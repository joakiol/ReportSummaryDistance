Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 46?54,Beijing, August 2010Handling Named Entities and Compound Verbs inPhrase-Based Statistical Machine TranslationSantanu Pal*, Sudip Kumar Naskar?, Pavel Pecina?,Sivaji Bandyopadhyay* and Andy Way?*Dept.
of Comp.
Sc.
& Engg.Jadavpur Universitysantanupersonal1@gmail.com, sivaji_cse_ju@yahoo.com?CNGL, School of ComputingDublin City University{snaskar, ppecina, away}@computing.dcu.ieAbstractData preprocessing plays a crucial role inphrase-based statistical machine transla-tion (PB-SMT).
In this paper, we showhow single-tokenization of two types ofmulti-word expressions (MWE), namelynamed entities (NE) and compoundverbs, as well as their prior alignmentcan boost the performance of PB-SMT.Single-tokenization of compound verbsand named entities (NE) provides sig-nificant gains over the baseline PB-SMTsystem.
Automatic alignment of NEssubstantially improves the overall MTperformance, and thereby the wordalignment quality indirectly.
For estab-lishing NE alignments, we transliteratesource NEs into the target language andthen compare them with the target NEs.Target language NEs are first convertedinto a canonical form before the com-parison takes place.
Our best systemachieves statistically significant im-provements (4.59 BLEU points absolute,52.5% relative improvement) on an Eng-lish?Bangla translation task.1 IntroductionStatistical machine translation (SMT) heavilyrelies on good quality word alignment andphrase alignment tables comprising translationknowledge acquired from a bilingual corpus.Multi-word expressions (MWE) are definedas ?idiosyncratic interpretations that cross wordboundaries (or spaces)?
(Sag et al, 2002).
Tradi-tional approaches to word alignment followingIBM Models (Brown et al, 1993) do not workwell with multi-word expressions, especiallywith NEs, due to their inability to handle many-to-many alignments.
Firstly, they only carry outalignment between words and do not considerthe case of complex expressions, such as multi-word NEs.
Secondly, the IBM Models only al-low at most one word in the source language tocorrespond to a word in the target language(Marcu, 2001, Koehn et al, 2003).In another well-known word alignment ap-proach, Hidden Markov Model (HMM: Vogel etal., 1996), the alignment probabilities depend onthe alignment position of the previous word.
Itdoes not explicitly consider many-to-manyalignment either.We address this many-to-many alignmentproblem indirectly.
Our objective is to see howto best handle the MWEs in SMT.
In this work,two types of MWEs, namely NEs and compoundverbs, are automatically identified on both sidesof the parallel corpus.
Then, source and targetlanguage NEs are aligned using a statisticaltransliteration method.
We rely on these auto-matically aligned NEs and treat them as transla-tion examples.
Adding bilingual dictionaries,which in effect are instances of atomic transla-tion pairs, to the parallel corpus is a well-knownpractice in domain adaptation in SMT (Eck etal., 2004; Wu et al, 2008).
We modify the paral-lel corpus by converting the MWEs into singletokens and adding the aligned NEs in the parallelcorpus in a bid to improve the word alignment,and hence the phrase alignment quality.
This46preprocessing results in improved MT quality interms of automatic MT evaluation metrics.The remainder of the paper is organized asfollows.
In section 2 we discuss related work.The System is described in Section 3.
Section 4includes the results obtained, together with someanalysis.
Section 5 concludes, and provides ave-nues for further work.2 Related WorkMoore (2003) presented an approach for si-multaneous NE identification and translation.
Heuses capitalization cues for identifying NEs onthe English side, and then he applies statisticaltechniques to decide which portion of the targetlanguage corresponds to the specified EnglishNE.
Feng et al (2004) proposed a MaximumEntropy model based approach for English?Chinese NE alignment which significantly out-performs IBM Model4 and HMM.
They consid-ered 4 features: translation score, transliterationscore, source NE and target NE's co-occurrencescore, and the distortion score for distinguishingidentical NEs in the same sentence.
Huang et al(2003) proposed a method for automatically ex-tracting NE translingual equivalences betweenChinese and English based on multi-feature costminimization.
The costs considered are translit-eration cost, word-based translation cost, and NEtagging cost.Venkatapathy and Joshi (2006) reported a dis-criminative approach of using the compositional-ity information about verb-based multi-wordexpressions to improve word alignment quality.
(Ren et al, 2009) presented log likelihood ratio-based hierarchical reducing algorithm to auto-matically extract bilingual MWEs, and investi-gated the usefulness of these bilingual MWEs inSMT by integrating bilingual MWEs into Moses(Koehn et al, 2007) in three ways.
They ob-served the highest improvement when they usedan additional feature to represent whether or nota bilingual phrase contains bilingual MWEs.This approach was generalized in Carpuat andDiab (2010).
In their work, the binary featurewas replaced by a count feature representing thenumber of MWEs in the source language phrase.Intuitively, MWEs should be both aligned inthe parallel corpus and translated as a whole.However, in the state-of-the-art PB-SMT, itcould well be the case that constituents of anMWE are marked and aligned as parts of con-secutive phrases, since PB-SMT (or any otherapproaches to SMT) does not generally treatMWEs as special tokens.
Another problem SMTsuffers from is that verb phrases are oftenwrongly translated, or even sometimes deleted inthe output in order to produce a target sentenceconsidered good by the language model.
More-over, the words inside verb phrases seldom showthe tendency of being aligned one-to-one; thealignments of the words inside source and targetverb phrases are mostly many-to-many, particu-larly so for the English?Bangla language pair.These are the motivations behind consideringNEs and compound verbs for special treatmentin this work.By converting the MWEs into single tokens,we make sure that PB-SMT also treats them as awhole.
The objective of the present work is two-fold; firstly to see how treatment of NEs andcompound verbs as a single unit affects theoverall MT quality, and secondly whether priorautomatic alignment of these single-tokenizedMWEs can bring about any further improvementon top of that.We carried out our experiments on an Eng-lish?Bangla translation task, a relatively hardtask with Bangla being a morphologically richerlanguage.3 System Description3.1 PB-SMTTranslation is modeled in SMT as a decisionprocess, in which the translation Ie1 = e1 .
.
.
ei .
.. eI of a source sentenceJf1 = f1 .
.
.
fj .
.
.
fJ ischosen to maximize (1):)().|(maxarg)|(maxarg 111,11, 11IIJeIJIeIePefPfePII=      (1)where )|( 11IJ efP  and )( 1IeP  denote respec-tively the translation model and the target lan-guage model (Brown et al, 1993).
In log-linearphrase-based SMT, the posterior probability)|( 11JI feP  is directly modeled as a log-linearcombination of features (Och and Ney, 2002),that usually comprise M translational features,and the language model, as in (2):47?==MmKIJmmJI sefhfeP111111 ),,()|(log ?
)(log 1ILM eP?+        (2)where kk sss ...11 =  denotes a segmentation of thesource and target sentences respectively into thesequences of phrases )?,...,?
( 1 kee  and )?,...,?
( 1 kffsuch that (we set i0 = 0) (3):,1 Kk ???
sk = (ik, bk, jk),kk iikeee ...?
11 +?= ,kk jbkfff ...?
= .
(3)and each feature mh?
in (2) can be rewritten as in(4):?==KkkkkmKIJm sefhsefh1111 ),?,?(?
),,(                  (4)where mh?
is a feature that applies to a singlephrase-pair.
It thus follows (5):?
?
?= ===KkKkkkkkkkmMmm sefhsefh1 11),?,?(?),?,?(??
(5)where mMmmhh ?
?1?== ?
.3.2 Preprocessing of the Parallel CorpusThe initial English?Bangla parallel corpus iscleaned and filtered using a semi-automaticprocess.
We employed two kinds of multi-wordinformation: compound verbs and NEs.
Com-pound verbs are first identified on both sides ofthe parallel corpus.
Chakrabarty et al (2008)analyzed and identified a category of V+V com-plex predicates called lexical compound verbsfor Hindi.
We adapted their strategy for identifi-cation of compound verbs in Bangla.
In additionto V+V construction, we also consider N+V andADJ+V structures.NEs are also identified on both sides of trans-lation pairs.
NEs in Bangla are much harder toidentify than in English (Ekbal and Bandyop-adhyay, 2009).
This can be attributed to the factthat (i) there is no concept of capitalization inBangla; and (ii) Bangla common nouns are oftenused as proper names.
In Bangla, the problem iscompounded by the fact that suffixes (casemarkers, plural markers, emphasizers, specifiers)are also added to proper names, just like to anyother common nouns.
As a consequence, the ac-curacy of Bangla NE recognizers (NER) is muchpoorer compared to that for English.
Once thecompound verbs and the NEs are identified onboth sides of the parallel corpus, they are con-verted into and replaced by single tokens.
Whenconverting these MWEs into single tokens, wereplace the spaces with underscores (?_?).
Sincethere are already some hyphenated words in thecorpus, we do not use hyphenation for this pur-pose; besides, the use of a special word separator(underscore in our case) facilitates the job ofdeciding which single-token (target language)MWEs to detokenize into words comprisingthem, before evaluation.3.3 Transliteration  Using Modified JointSource-Channel ModelLi et al (2004) proposed a generative frameworkallowing direct orthographical mapping of trans-literation units through a joint source-channelmodel, which is also called n-gram translitera-tion model.
They modeled the segmentation ofnames into transliteration units (TU) and theiralignment preferences using maximum likeli-hood via EM algorithm (Dempster et al, 1977).Unlike the noisy-channel model, the jointsource-channel model tries to capture howsource and target names can be generated simul-taneously by means of contextual n-grams of thetransliteration units.
For K aligned TUs, theydefine the bigram model as in (6):)...,,...,(),( 2121 KK bbbeeePBEP =),...,,,( 21 KbebebeP ><><><=?
><><= K=kk bebeP11-k1 ),|,(         (6)where E refers to the English name and B thetransliteration in Bengali, while ei and bi refer tothe ith English and Bangla segment (TU) respec-tively.Ekbal et al (2006) presented a modification tothe joint source-channel model to incorporatedifferent contextual information into the modelfor Indian languages.
They used regular expres-sions and language-specific heuristics based onconsonant and vowel patterns to segment namesinto TUs.
Their modified joint source-channelmodel, for which they obtained improvement48over the original joint source-channel model,essentially considers a trigram model for thesource language and a bigram model for the tar-get, as in (7).?
+><><= K=kkk ebebePBEP111-k ),,|,(),(   (7)Ekbal et al (2006) reported a word agreementratio of 67.9% on an English?Bangla translit-eration task.
In the present work, we use themodified joint source-channel model of (Ekbalet al, 2006) to translate names for establishingNE alignments in the parallel corpus.3.4 Automatic Alignment of NEs throughTransliterationWe first create an NE parallel corpus by extract-ing the source and target (single token) NEsfrom the NE-tagged parallel translations inwhich both sides contain at least one NE.
Forexample, we extract the NE translation pairsgiven in (9) from the sentence pair shown in (8),where the NEs are shown as italicized.
(8a) Kirti_Mandir , where Mahatma_Gandhiwas born , today houses a photo exhibition onthe life and times of the Mahatma , a library, aprayer hall and other memorabilia .
(8b) ??????_??n?
, ??????
???t?_??n?
??n?????
,????????
??????
???t??
????
o ?
?i ???????????????
e???
??tp?????????
, e???
??i?b??
oe???
p??????
??
e??
a?????
s ?????????
??????t???
?
(9a) Kirti_Mandir Mahatma_Gandhi Mahatma(9b) ??????_??n?
???t?_??n?
???t?
?Then we try to align the source and target NEsextracted from a parallel sentence, as illustratedin (9).
If both sides contain only one NE then thealignment is trivial, and we add such NE pairs toseed another parallel NE corpus that containsexamples having only one token in both side.Otherwise, we establish alignments between thesource and target NEs using transliteration.
Weuse the joint source-channel model of translitera-tion (Ekbal et al, 2006) for this purpose.If both the source and target side contains nnumber of NEs, and the alignments of n-1 NEscan be established through transliteration or bymeans of already existing alignments, then thenth alignment is trivial.
However, due to the rela-tive performance difference of the NERs for thesource and target language, the number of NEsidentified on the source and target sides is al-most always unequal (see Section 4).
Accord-ingly, we always use transliteration to establishalignments even when it is assumed to be trivial.Similarly, for multi-word NEs, intra-NE wordalignments are established through translitera-tion or by means of already existing alignments.For a multi-word source NE, if we can align allthe words inside the NE with words inside a tar-get NE, then we assume they are translations ofeach other.
Due to the relatively poor perform-ance of the Bangla NER, we also store the im-mediate left and right neighbouring words forevery NE in Bangla, just in case the left or theright word is a valid part of the NE but is notproperly tagged by the NER.As mentioned earlier, since the source sideNER is much more reliable than the target sideNER, we transliterate the English NEs, and tryto align them with the Bangla NEs.
For aligning(capitalized) English words to Bangla words, wetake the 5 best transliterations produced by thetransliteration system for an English word, andcompare them against the Bangla words.
BanglaNEs often differ in their choice of matras (vowelmodifiers).
Thus we first normalize the Banglawords, both in the target NEs and the transliter-ated ones, to a canonical form by dropping thematras, and then compare the results.
In effect,therefore, we just compare the consonant se-quences of every transliteration candidate withthat of a target side Bangla word; if they match,then we align the English word with the Banglaword.????
(?
+ ?
?+ ?
+ ?)
-- ?????
(?
+ ??
+ ?
+ ??
+ ?
)(10)The example in (10) illustrates the procedure.Assume, we are trying to align ?Niraj?
with???????.
The transliteration system produces??????
from the English word ?Niraj?
and wecompare ??????
with ???????.
Since the conso-nant sequences match in both words, ??????
isconsidered a spelling variation of ??????
?, andthe English word ?Niraj?
is aligned to theBangla word ??????
?.In this way, we achieve word-level align-ments, as well as NE-level alignments.
(11)shows the alignments established from (8).
Theword-level alignments help to establish new49word / NE alignments.
Word and NE alignmentsobtained in this way are added to the parallelcorpus as additional training data.
(11a) Kirti-Mandir  ?
??????-??n?
(11b) Kirti ?
??????
(11c) Mandir  ?
??n?
(11d) Mahatma-Gandhi ?
???t?-??n?
(11e) Mahatma ?
???t?
(11f) Gandhi ?
??n?
(11g) Mahatma ?
???t?
?3.5 Tools and Resources UsedA sentence-aligned English?Bangla parallelcorpus containing 14,187 parallel sentences froma travel and tourism domain was used in the pre-sent work.
The corpus was obtained from theconsortium-mode project ?Development of Eng-lish to Indian Languages Machine Translation(EILMT) System?
1.The Stanford Parser2 and the CRF chunker3were used for identifying compound verbs in thesource side of the parallel corpus.
The StanfordNER4 was used to identify NEs on the sourceside (English) of the parallel corpus.The sentences on the target side (Bangla)were POS-tagged by using the tools obtainedfrom the consortium mode project ?Develop-ment of Indian Languages to Indian LanguagesMachine Translation (ILILMT) System?.
NEs inBangla are identified using the NER system ofEkbal and Bandyopadhyay (2008).
We use theStanford Parser, Stanford NER and the NER forBangla along with the default model files pro-vided, i.e., with no additional training.The effectiveness of the MWE-aligned paral-lel corpus developed in the work is demonstratedby using the standard log-linear PB-SMT modelas our baseline system: GIZA++ implementationof IBM word alignment model 4, phrase-extraction heuristics described in (Koehn et al,2003), minimum-error-rate training (Och, 2003)on a held-out development set, target languagemodel with Kneser-Ney smoothing (Kneser and1 The EILMT and ILILMT projects are funded by the De-partment of Information Technology (DIT), Ministry ofCommunications and Information Technology (MCIT),Government of India.2 http://nlp.stanford.edu/software/lex-parser.shtml3 http://crfchunker.sourceforge.net/4 http://nlp.stanford.edu/software/CRF-NER.shtmlNey, 1995) trained with SRILM (Stolcke, 2002),and Moses decoder (Koehn et al, 2007).4 Experiments and ResultsWe randomly extracted 500 sentences each forthe development set and testset from the initialparallel corpus, and treated the rest as the train-ing corpus.
After filtering on maximum allow-able sentence length of 100 and sentence lengthratio of 1:2 (either way), the training corpus con-tained 13,176 sentences.
In addition to the targetside of the parallel corpus, a monolingual Banglacorpus containing 293,207 words from the tour-ism domain was used for the target languagemodel.
We experimented with different n-gramsettings for the language model and the maxi-mum phrase length, and found that a 4-gramlanguage model and a maximum phrase lengthof 4 produced the optimum baseline result.
Wetherefore carried out the rest of the experimentsusing these settings.English Bangla In training setT U T UCompound verbs 4,874 2,289 14,174 7,154Single-word NEs 4,720 1,101 5,068 1,1752-word NEs 4,330 2,961 4,147 3,417>2 word NEs 1,555 1,271 1,390 1,278Total NEs 10,605 5,333 10,605 5,870Total NE words 22,931 8,273 17,107 9,106Table 1.
MWE statistics (T - Total occur-rence, U ?
Unique).Of the 13,676 sentences in the training anddevelopment set, 13,675 sentences had at leastone NE on both sides, only 22 sentences hadequal number of NEs on both sides, and 13,654sentences had an unequal number of NEs.
Simi-larly, for the testset, all the sentences had at leastone NE on both sides, and none had an equalnumber of NEs on both sides.
It gives an indica-tion of the relative performance differences ofthe NERs.
6.6% and 6.58% of the source tokensbelong to NEs in the training and testset respec-tively.
These statistics reveal the high degree ofNEs in the tourism domain data that demandsspecial treatment.
Of the 225 unique NEs ap-pearing on the source side of the testset, only 65NEs are found in the training set.50Experiments Exp BLEU METEOR NIST WER PER TERBaseline 1 8.74 20.39 3.98 77.89 62.95 74.60NEs of any length as SingleToken (New-MWNEaST)2 9.15 18.19 3.88 77.81 63.85 74.61NEs of length >2 asSingle Tokens (MWNE-aST)3 8.76 18.78 3.86 78.31 63.78 75.15NEs as SingleTokens(NEaST)2-Word NEs as Single To-kens (2WNEaST)4 9.13 17.28 3.92 78.12 63.15 74.85Compound Verbs as  Single Tokens(CVaST) ?5 9.56 15.35 3.96 77.60 63.06 74.46Alignment of NEs of anylength (New-MWNEA) ?6 13.33 24.06 4.44 74.79 60.10 71.25Alignment of NEs of lengthupto 2 (New-2WNEA) ?7 10.35 20.93 4.11 76.49 62.20 73.05Alignment of NEs of length>2 (MWNEA) ?8 12.39 23.13 4.36 75.51 60.58 72.06NE Alignment(NEA)Alignment of NEs of length2 (2WNEA) ?9 11.2 23.14 4.26 76.13 60.72 72.57New-MWNEaST 10 8.62 16.64 3.73 78.41 65.21 75.47MWNEaST 11 8.74 14.68 3.84 78.40 64.05 75.40CVaST+NEaST 2WNEaST 12 8.85 16.60 3.86 78.17 63.90 75.33New-MWNEA?
13 11.22 21.02 4.16 75.99 61.96 73.06New-2WNEA?
14 10.07 17.67 3.98 77.08 63.35 74.18MWNEA?
15 10.34 16.34 4.07 77.12 62.38 73.88CVaST +NEA2WNEA?
16 10.51 18.92 4.08 76.77 62.28 73.56Table 2.
Evaluation results for different experimental setups (The ???
marked systems producestatistically significant improvements on BLEU over the baseline system).Table 1 shows the MWE statistics of theparallel corpus as identified by the NERs.
Theaverage NE length in the training corpus is2.16 for English and 1.61 for Bangla.
As canbe seen from Table 1, 44.5% and 47.8% of theNEs are single-word NEs in English andBangla respectively, which suggests that prioralignment of the single-word NEs, in additionto multi-word NE alignment, should also bebeneficial to word and phrase alignment.Of all the NEs in the training and develop-ment sets, the transliteration-based alignmentprocess was able to establish alignments of4,711 single-word NEs, 4,669 two-word NEsand 1,745 NEs having length more than two.It is to be noted that, some of the single-wordNE alignments, as well as two-word NEalignments, result from multi-word NE align-ment.We analyzed the output of the NE align-ment module and observed that longer NEswere aligned better than the shorter ones,which is quite intuitive, as longer NEs havemore tokens to be considered for intra-NEalignment.
Since the NE alignment process isbased on transliteration, the alignment methoddoes not work where NEs involve translationor acronyms.
We also observed that Englishmulti-word NEs are sometimes fused togetherinto single-word NEs.We performed three sets of experiments:treating compound verbs as single tokens,treating NEs as single tokens, and the combi-nation thereof.
Again for NEs, we carried outthree types of preprocessing: single-tokenization of (i) two-word NEs, (ii) morethan two-word NEs, and (iii) NEs of anylength.
We make distinctions among thesethree to see their relative effects.
The devel-opment and test sets, as well as the target lan-guage monolingual corpus (for language mod-eling), are also subjected to the same preproc-essing of single-tokenizing the MWEs.
ForNE alignment, we performed experiments us-ing 4 different settings: alignment of (i) NEsof length up to two, (ii) NEs of length two,51(iii) NEs of length greater than two, and (iv)NEs of any length.
Before evaluation, the sin-gle-token (target language) underscoredMWEs are expanded back to words compris-ing the MWEs.Since we did not have the gold-standardword alignment, we could not perform intrin-sic evaluation of the word alignment.
Insteadwe carry out extrinsic evaluation on the MTquality using the well known automatic MTevaluation metrics: BLEU (Papineni et al,2002), METEOR (Banerjee and Lavie, 2005),NIST (Doddington, 2002), WER, PER andTER (Snover et al, 2006).
As can be seenfrom the evaluation results reported in Table2, baseline Moses without any preprocessingof the dataset produces a BLEU score of 8.74.The low score can be attributed to the fact thatBangla, a morphologically rich language, ishard to translate into.
Moreover, Bangla beinga relatively free phrase order language (Ekbaland Bandyopadhyay, 2009) ideally requiresmultiple set of references for proper evalua-tion.
Hence using a single reference set doesnot justify evaluating translations in Bangla.Also the training set was not sufficiently largeenough for SMT.
Treating only longer than 2-word NEs as single tokens does not help im-prove the overall performance much, whilesingle tokenization  of two-word NEs as singletokens produces some improvements (.39BLEU points absolute, 4.5% relative).
Con-sidering compound verbs as single tokens(CVaST) produces a .82 BLEU point im-provement (9.4% relative) over the baseline.Strangely, when both compound verbs andNEs together are counted as single tokens,there is hardly any improvement.
By contrast,automatic NE alignment  (NEA) gives a hugeimpetus to system performance, the best ofthem (4.59 BLEU points absolute, 52.5% rela-tive improvement) being the alignment of NEsof any length that produces the best scoresacross all metrics.
When NEA is combinedwith CVaST, the improvements are substan-tial, but it can not beat the individual im-provement on NEA.
The (?)
marked systemsproduce statistically significant improvementsas measured by bootstrap resampling method(Koehn, 2004) on BLEU over the baselinesystem.
Metric-wise individual best scores areshown in bold in Table 2.5 Conclusions and Future WorkIn this paper, we have successfully shownhow the simple yet effective preprocessing oftreating two types of MWEs, namely NEs andcompound verbs, as single-tokens, in conjunc-tion with prior NE alignment can boost theperformance of PB-SMT system on an Eng-lish?Bangla translation task.
Treating com-pound verbs as single-tokens provides signifi-cant gains over the baseline PB-SMT system.Amongst the MWEs, NEs perhaps play themost important role in MT, as we have clearlydemonstrated through experiments that auto-matic alignment of NEs by means of translit-eration improves the overall MT performancesubstantially across all automatic MT evalua-tion metrics.
Our best system yields 4.59BLEU points improvement over the baseline,a 52.5% relative increase.
We compared asubset of the output of our best system withthat of the baseline system, and the output ofour best system almost always looks better interms of either lexical choice or word order-ing.
The fact that only 28.5% of the testsetNEs appear in the training set, yet prior auto-matic alignment of the NEs brings about somuch improvement in terms of MT quality,suggests that it not only improves the NEalignment quality in the phrase table, but wordalignment and phrase alignment quality musthave also been improved significantly.
At thesame time, single-tokenization of MWEsmakes the dataset sparser, but yet improvesthe quality of MT output to some extent.
Data-driven approaches to MT, specifically forscarce-resource language pairs for which verylittle parallel texts are available, should benefitfrom these preprocessing methods.
Datasparseness is perhaps the reason why single-tokenization of NEs and compound verbs,both individually and in collaboration, did notadd significantly to the scores.
However, asignificantly large parallel corpus can takecare of the data sparseness problem introducedby the single-tokenization of MWEs.The present work offers several avenues forfurther work.
In future, we will investigatehow these automatically aligned NEs can be52used as anchor words to directly influence theword alignment process.
We will look intowhether similar kinds of improvements can beachieved for larger datasets, corpora from dif-ferent domains and for other language pairs.We will also investigate how NE alignmentquality can be improved, especially whereNEs involve translation and acronyms.
Wewill also try to perform morphological analy-sis or stemming on the Bangla side before NEalignment.
We will also explore whether dis-criminative approaches to word alignment canbe employed to improve the precision of theNE alignment.AcknowledgementsThis research is partially supported by the Sci-ence Foundation Ireland (Grant 07/CE/I1142)as part of the Centre for Next Generation Lo-calisation (www.cngl.ie) at Dublin City Uni-versity, and EU projects PANACEA (Grant7FP-ITC-248064) and META-NET (GrantFP7-ICT-249119).ReferencesBanerjee, Satanjeev, and Alon Lavie.
2005.
AnAutomatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
Inproceedings of the ACL-2005 Workshop on In-trinsic and Extrinsic Evaluation Measures forMT and/or Summarization, pp.
65-72.
Ann Ar-bor, Michigan., pp.
65-72.Brown, Peter F., Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation:parameter estimation.
Computational Linguis-tics, 19(2):263-311.Carpuat, Marine, and Mona Diab.
2010.
Task-based Evaluation of Multiword Expressions: aPilot Study in Statistical Machine Translation.In Proceedings of Human Language Technologyconference and the North American Chapter ofthe Association for Computational Linguisticsconference (HLT-NAACL 2010), Los Angeles,CA, pp.
242-245.Chakrabarti, Debasri, Hemang Mandalia, RitwikPriya, Vaijayanthi Sarma, and Pushpak Bhat-tacharyya.
2008.
Hindi compound verbs andtheir automatic extraction.
In Proceedingsof  the 22nd International Conference on Com-putational Linguistics (Coling 2008), Postersand demonstrations, Manchester, UK, pp.
27-30.Dempster, A.P., N.M. Laird, and D.B.
Rubin.1977).
Maximum Likelihood from IncompleteData via the EM Algorithm.
Journal of theRoyal Statistical Society, Series B (Methodo-logical) 39 (1): 1?38.Doddington, George.
2002.
Automatic evaluationof machine translation quality using n-gramcooccurrence statistics.
In Proceedings of theSecond International Conference on HumanLanguage Technology Research (HLT-2002),San Diego, CA, pp.
128-132.Eck, Matthias, Stephan Vogel, and Alex Waibel.2004.
Improving statistical machine translationin the medical domain using the Unified Medi-cal Language System.
In Proceedings of  the20th International Conference on ComputationalLinguistics (COLING 2004), Ge-neva, Switzerland, pp.
792-798.Ekbal, Asif, and Sivaji Bandyopadhyay.
2009.Voted NER system using appropriate unlabeleddata.
In proceedings of the ACL-IJCNLP-2009Named Entities Workshop (NEWS 2009),Suntec, Singapore, pp.
202-210.Ekbal, Asif, and Sivaji Bandyopadhyay.
2008.Maximum Entropy Approach for Named EntityRecognition in Indian Languages.
InternationalJournal for Computer Processing of Lan-guages (IJCPOL), Vol.
21(3):205-237.Feng, Donghui, Yajuan Lv, and Ming Zhou.
2004.A new approach for English-Chinese named en-tity alignment.
In Proceedings of the 2004 Con-ference on Empirical Methods in Natural Lan-guage Processing (EMNLP-2004), Barcelona,Spain, pp.
372-379.Huang, Fei, Stephan Vogel, and Alex Waibel.2003.
Automatic extraction of named entitytranslingual equivalence based on multi-featurecost minimization.
In Proceedings of the ACL-2003 Workshop on Multilingual and Mixed-language Named Entity Recognition, 2003,Sapporo, Japan, pp.
9-16.Kneser, Reinhard, and Hermann Ney.
1995.
Im-proved backing-off for m-gram language model-ing.
In Proceedings of the IEEE InternationConference on Acoustics, Speech, and SignalProcessing (ICASSP), vol.
1, pp.
181-184.
De-troit, MI.Koehn, Philipp, Franz Josef Och, and DanielMarcu.
2003.
Statistical phrase-based transla-tion.
In Proceedings of HLT-NAACL 2003:53conference combining Human Language Tech-nology conference series and the North Ameri-can Chapter of the Association for Computa-tional Linguistics conference series,  Edmonton,Canada, pp.
48-54.Koehn, Philipp, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, Ni-cola Bertoldi, Brooke Cowan, Wade Shen,Christine Moran, Richard Zens, Chris Dyer,Ond?ej Bojar, Alexandra Constantin, and EvanHerbst.
2007.
Moses: open source toolkit forstatistical machine translation.
In Proceedings ofthe 45th Annual meeting of the Association forComputational Linguistics (ACL 2007): Pro-ceedings of demo and poster sessions, Prague,Czech Republic, pp.
177-180.Koehn, Philipp.
2004.
Statistical significance testsfor machine translation evaluation.
In  EMNLP-2004: Proceedings of the 2004 Conference onEmpirical Methods in Natural Language Proc-essing, 25-26 July 2004, Barcelona, Spain, pp.388-395.Marcu, Daniel.
2001.
Towards a Unified Approachto Memory- and Statistical-Based MachineTranslation.
In Proceedings of the 39th AnnualMeeting of the Association for ComputationalLinguistics (ACL 2001), Toulouse, France, pp.386-393.Moore, Robert C. 2003.
Learning translations ofnamed-entity phrases from parallel corpora.
InProceedings of 10th Conference of the Euro-pean Chapter of the Association for Computa-tional Linguistics (EACL 2003), Budapest,Hungary; pp.
259-266.Och, Franz J.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings ofthe 41st Annual Meeting of the Association forComputational Linguistics (ACL-2003), Sap-poro, Japan, pp.
160-167.Papineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a method forautomatic evaluation of machine translation.
InProceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics(ACL-2002), Philadelphia, PA, pp.
311-318.Ren, Zhixiang, Yajuan L?, Jie Cao, Qun Liu, andYun Huang.
2009.
Improving statistical ma-chine translation using domain bilingual multi-word expressions.
In Proceedings of the 2009Workshop on Multiword Expressions, ACL-IJCNLP 2009, Suntec, Singapore, pp.
47-54.Sag, Ivan A., Timothy Baldwin, Francis Bond,Ann Copestake and Dan Flickinger.
2002.
Mul-tiword expressions: A pain in the neck for NLP.In Proceedings of the 3rd International Confer-ence on Intelligent Text Processing and Compu-tational Linguistics (CICLing-2002), MexicoCity, Mexico, pp.
1-15.Snover, Matthew, Bonnie Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
2006.
Astudy of translation edit rate with targeted hu-man annotation.
In Proceedings of the 7th Con-ference of the Association for Machine Transla-tion in the Americas (AMTA 2006), Cambridge,MA, pp.
223-231.Vogel, Stephan, Hermann Ney, and ChristophTillmann.
1996.
HMM-based word alignment instatistical translation.
In Proceedings of the 16thInternational Conference on ComputationalLinguistics (COLING 1996), Copenhagen, pp.836-841.Venkatapathy, Sriram, and Aravind K. Joshi.
2006.Using information about multi-word expres-sions for the word-alignment task.
In Proceed-ings of Coling-ACL 2006: Workshop on Multi-word Expressions: Identifying and ExploitingUnderlying Properties, Sydney, pp.
20-27.Wu, Hua Haifeng Wang, and Chengqing Zong.2008.
Domain adaptation for statistical machinetranslation with domain dictionary and mono-lingual corpora.
In Proceedings of the 22nd In-ternational Conference on Computational Lin-guistics (COLING 2008),  Manchester, UK, pp.993-1000.54
