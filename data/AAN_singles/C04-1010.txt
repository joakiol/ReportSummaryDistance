Deterministic Dependency Parsing of English TextJoakim Nivre and Mario ScholzSchool of Mathematics and Systems EngineeringVa?xjo?
UniversitySE-35195 Va?xjo?Swedenjoakim.nivre@msi.vxu.seAbstractThis paper presents a deterministic dependencyparser based on memory-based learning, whichparses English text in linear time.
When trainedand evaluated on the Wall Street Journal sec-tion of the Penn Treebank, the parser achievesa maximum attachment score of 87.1%.
Unlikemost previous systems, the parser produces la-beled dependency graphs, using as arc labels acombination of bracket labels and grammaticalrole labels taken from the Penn Treebank II an-notation scheme.
The best overall accuracy ob-tained for identifying both the correct head andthe correct arc label is 86.0%, when restrictedto grammatical role labels (7 labels), and 84.4%for the maximum set (50 labels).1 IntroductionThere has been a steadily increasing interest in syn-tactic parsing based on dependency analysis in re-cent years.
One important reason seems to be thatdependency parsing offers a good compromise be-tween the conflicting demands of analysis depth,on the one hand, and robustness and efficiency, onthe other.
Thus, whereas a complete dependencystructure provides a fully disambiguated analysisof a sentence, this analysis is typically less com-plex than in frameworks based on constituent anal-ysis and can therefore often be computed determin-istically with reasonable accuracy.
Deterministicmethods for dependency parsing have now been ap-plied to a variety of languages, including Japanese(Kudo and Matsumoto, 2000), English (Yamada andMatsumoto, 2003), Turkish (Oflazer, 2003), andSwedish (Nivre et al, 2004).For English, the interest in dependency parsinghas been weaker than for other languages.
To someextent, this can probably be explained by the strongtradition of constituent analysis in Anglo-Americanlinguistics, but this trend has been reinforced by thefact that the major treebank of American English,the Penn Treebank (Marcus et al, 1993), is anno-tated primarily with constituent analysis.
On theother hand, the best available parsers trained on thePenn Treebank, those of Collins (1997) and Char-niak (2000), use statistical models for disambigua-tion that make crucial use of dependency relations.Moreover, the deterministic dependency parser ofYamada and Matsumoto (2003), when trained onthe Penn Treebank, gives a dependency accuracythat is almost as good as that of Collins (1997) andCharniak (2000).The parser described in this paper is similar tothat of Yamada and Matsumoto (2003) in that it usesa deterministic parsing algorithm in combinationwith a classifier induced from a treebank.
However,there are also important differences between the twoapproaches.
First of all, whereas Yamada and Mat-sumoto employs a strict bottom-up algorithm (es-sentially shift-reduce parsing) with multiple passesover the input, the present parser uses the algorithmproposed in Nivre (2003), which combines bottom-up and top-down processing in a single pass in orderto achieve incrementality.
This also means that thetime complexity of the algorithm used here is linearin the size of the input, while the algorithm of Ya-mada and Matsumoto is quadratic in the worst case.Another difference is that Yamada and Matsumotouse support vector machines (Vapnik, 1995), whilewe instead rely on memory-based learning (Daele-mans, 1999).Most importantly, however, the parser presentedin this paper constructs labeled dependency graphs,i.e.
dependency graphs where arcs are labeled withdependency types.
As far as we know, this makesit different from all previous systems for depen-dency parsing applied to the Penn Treebank (Eis-ner, 1996; Yamada and Matsumoto, 2003), althoughthere are systems that extract labeled grammati-cal relations based on shallow parsing, e.g.
Buch-holz (2002).
The fact that we are working with la-beled dependency graphs is also one of the motiva-tions for choosing memory-based learning over sup-port vector machines, since we require a multi-classclassifier.
Even though it is possible to use SVMfor multi-class classification, this can get cumber-some when the number of classes is large.
(For theThe ?DEPfinger-pointing ?NP-SBJhas already ?ADVPbegun ?VP.
? DEPFigure 1: Dependency graph for English sentenceunlabeled dependency parser of Yamada and Mat-sumoto (2003) the classification problem only in-volves three classes.
)The parsing methodology investigated here haspreviously been applied to Swedish, where promis-ing results were obtained with a relatively smalltreebank (approximately 5000 sentences for train-ing), resulting in an attachment score of 84.7% anda labeled accuracy of 80.6% (Nivre et al, 2004).1However, since there are no comparable resultsavailable for Swedish, it is difficult to assess the sig-nificance of these findings, which is one of the rea-sons why we want to apply the method to a bench-mark corpus such as the the Penn Treebank, eventhough the annotation in this corpus is not ideal forlabeled dependency parsing.The paper is structured as follows.
Section 2 de-scribes the parsing algorithm, while section 3 ex-plains how memory-based learning is used to guidethe parser.
Experimental results are reported in sec-tion 4, and conclusions are stated in section 5.2 Deterministic Dependency ParsingIn dependency parsing the goal of the parsing pro-cess is to construct a labeled dependency graph ofthe kind depicted in Figure 1.
In formal terms, wedefine dependency graphs as follows:1.
Let R = {r1, .
.
.
, rm} be the set of permissibledependency types (arc labels).2.
A dependency graph for a string of wordsW = w1?
?
?wn is a labeled directed graphD = (W,A), where(a) W is the set of nodes, i.e.
word tokens inthe input string,(b) A is a set of labeled arcs (wi, r, wj)(wi, wj ?
W , r ?
R),(c) for every wj ?
W , there is at most onearc (wi, r, wj) ?
A.1The attachment score only considers whether a word is as-signed the correct head; the labeled accuracy score in additionrequires that it is assigned the correct dependency type; cf.
sec-tion 4.3.
A graph D = (W,A) is well-formed iff it isacyclic, projective and connected.For a more detailed discussion of dependencygraphs and well-formedness conditions, the readeris referred to Nivre (2003).The parsing algorithm used here was first de-fined for unlabeled dependency parsing in Nivre(2003) and subsequently extended to labeled graphsin Nivre et al (2004).
Parser configurations are rep-resented by triples ?S, I,A?, where S is the stack(represented as a list), I is the list of (remaining)input tokens, and A is the (current) arc relationfor the dependency graph.
(Since in a dependencygraph the set of nodes is given by the input to-kens, only the arcs need to be represented explic-itly.)
Given an input string W , the parser is initial-ized to ?nil,W, ?
?2 and terminates when it reachesa configuration ?S,nil, A?
(for any list S and set ofarcs A).
The input string W is accepted if the de-pendency graph D = (W,A) given at terminationis well-formed; otherwise W is rejected.
Given anarbitrary configuration of the parser, there are fourpossible transitions to the next configuration (wheret is the token on top of the stack, n is the next inputtoken, w is any word, and r, r?
?
R):1.
Left-Arc: In a configuration ?t|S,n|I,A?, ifthere is no arc (w, r, t) ?
A, extend A with(n, r?, t) and pop the stack, giving the configu-ration ?S,n|I,A?
{(n, r?, t)}?.2.
Right-Arc: In a configuration ?t|S,n|I,A?, ifthere is no arc (w, r, n) ?
A, extend A with(t, r?, n) and push n onto the stack, giving theconfiguration ?n|t|S,I,A?
{(t, r?, n)}?.3.
Reduce: In a configuration ?t|S,I,A?, if thereis an arc (w, r, t)?A, pop the stack, giving theconfiguration ?S,I,A?.4.
Shift: In a configuration ?S,n|I,A?, pushn onto the stack, giving the configuration?n|S,I,A?.2We use nil to denote the empty list and a|A to denote a listwith head a and tail A.TH.POS ?T.DEP.
.
.
TL.POS ?TL.DEP.
.
.
T.POST.LEX ?TR.DEP.
.
.
TR.POS .
.
.
NL.POS ?NL.DEP.
.
.
N.POSN.LEXL1.POS L2.POS L3.POST = Top of the stackN = Next input tokenTL = Leftmost dependent of TTR = Rightmost dependent of TNL = Leftmost dependent of NLi = Next plus i input tokenX.LEX = Word form of XX.POS = Part-of-speech of XX.DEP = Dependency type of XFigure 2: Parser state featuresAfter initialization, the parser is guaranteed to ter-minate after at most 2n transitions, given an inputstring of length n (Nivre, 2003).
Moreover, theparser always constructs a dependency graph that isacyclic and projective.
This means that the depen-dency graph given at termination is well-formed ifand only if it is connected (Nivre, 2003).
Otherwise,it is a set of connected components, each of whichis a well-formed dependency graph for a substringof the original input.The transition system defined above is nondeter-ministic in itself, since several transitions can of-ten be applied in a given configuration.
To con-struct deterministic parsers based on this system,we use classifiers trained on treebank data in or-der to predict the next transition (and dependencytype) given the current configuration of the parser.In this way, our approach can be seen as a form ofhistory-based parsing (Black et al, 1992; Mager-man, 1995).
In the experiments reported here, weuse memory-based learning to train our classifiers.3 Memory-Based LearningMemory-based learning and problem solving isbased on two fundamental principles: learning is thesimple storage of experiences in memory, and solv-ing a new problem is achieved by reusing solutionsfrom similar previously solved problems (Daele-mans, 1999).
It is inspired by the nearest neighborapproach in statistical pattern recognition and arti-ficial intelligence (Fix and Hodges, 1952), as wellas the analogical modeling approach in linguistics(Skousen, 1989; Skousen, 1992).
In machine learn-ing terms, it can be characterized as a lazy learn-ing method, since it defers processing of input un-til needed and processes input by combining storeddata (Aha, 1997).Memory-based learning has been successfullyapplied to a number of problems in natural languageprocessing, such as grapheme-to-phoneme conver-sion, part-of-speech tagging, prepositional-phraseattachment, and base noun phrase chunking (Daele-mans et al, 2002).
Previous work on memory-basedlearning for deterministic parsing includes Veenstraand Daelemans (2000) and Nivre et al (2004).For the experiments reported in this paper, wehave used the software package TiMBL (TilburgMemory Based Learner), which provides a vari-ety of metrics, algorithms, and extra functions ontop of the classical k nearest neighbor classificationkernel, such as value distance metrics and distanceweighted class voting (Daelemans et al, 2003).The function we want to approximate is a map-ping f from configurations to parser actions, whereeach action consists of a transition and (except forShift and Reduce) a dependency type:f : Config ?
{LA,RA,RE,SH} ?
(R ?
{nil})Here Config is the set of all configurations and Ris the set of dependency types.
In order to make theproblem tractable, we approximate f with a func-tion f?
whose domain is a finite space of parserstates, which are abstractions over configurations.For this purpose we define a number of featuresthat can be used to define different models of parserstate.Figure 2 illustrates the features that are used todefine parser states in the present study.
The twocentral elements in any configuration are the tokenon top of the stack (T) and the next input token(N), the tokens which may be connected by a de-pendency arc in the next configuration.
For thesetokens, we consider both the word form (T.LEX,N.LEX) and the part-of-speech (T.POS, N.POS), asassigned by an automatic part-of-speech tagger ina preprocessing phase.
Next, we consider a selec-tion of dependencies that may be present in the cur-rent arc relation, namely those linking T to its head(TH) and its leftmost and rightmost dependent (TL,TR), and that linking N to its leftmost dependent(NL),3 considering both the dependency type (arclabel) and the part-of-speech of the head or depen-dent.
Finally, we use a lookahead of three tokens,considering only their parts-of-speech.We have experimented with two different statemodels, one that incorporates all the features de-picted in Figure 2 (Model 1), and one that ex-cludes the parts-of-speech of TH, TL, TR, NL (Model2).
Models similar to model 2 have been found towork well for datasets with a rich annotation of de-pendency types, such as the Swedish dependencytreebank derived from Einarsson (1976), where theextra part-of-speech features are largely redundant(Nivre et al, 2004).
Model 1 can be expected towork better for datasets with less informative de-pendency annotation, such as dependency trees ex-tracted from the Penn Treebank, where the extrapart-of-speech features may compensate for the lackof information in arc labels.The learning algorithm used is the IB1 algorithm(Aha et al, 1991) with k = 5, i.e.
classification basedon 5 nearest neighbors.4 Distances are measured us-ing the modified value difference metric (MVDM)(Stanfill and Waltz, 1986; Cost and Salzberg, 1993)for instances with a frequency of at least 3 (andthe simple overlap metric otherwise), and classifica-tion is based on distance weighted class voting withinverse distance weighting (Dudani, 1976).
Thesesettings are the result of extensive experiments par-tially reported in Nivre et al (2004).
For more infor-mation about the different parameters and settings,see Daelemans et al (2003).4 ExperimentsThe data set used for experimental evaluation isthe standard data set from the Wall Street Journalsection of the Penn Treebank, with sections 2?213Given the parsing algorithm, N can never have a head or aright dependent in the current configuration.4In TiMBL, the value of k in fact refers to k nearest dis-tances rather than k nearest neighbors, which means that, evenwith k = 1, the nearest neighbor set can contain several in-stances that are equally distant to the test instance.
This is dif-ferent from the original IB1 algorithm, as described in Aha etal.
(1991).used for training and section 23 for testing (Collins,1999; Charniak, 2000).
The data has been con-verted to dependency trees using head rules (Mager-man, 1995; Collins, 1996).
We are grateful to Ya-mada and Matsumoto for letting us use their rule set,which is a slight modification of the rules used byCollins (1999).
This permits us to make exact com-parisons with the parser of Yamada and Matsumoto(2003), but also the parsers of Collins (1997) andCharniak (2000), which are evaluated on the samedata set in Yamada and Matsumoto (2003).One problem that we had to face is that the stan-dard conversion of phrase structure trees to de-pendency trees gives unlabeled dependency trees,whereas our parser requires labeled trees.
Since theannotation scheme of the Penn Treebank does notinclude dependency types, there is no straightfor-ward way to derive such labels.
We have thereforeexperimented with two different sets of labels, noneof which corresponds to dependency types in a strictsense.
The first set consists of the function tags forgrammatical roles according to the Penn II annota-tion guidelines (Bies et al, 1995); we call this set G.The second set consists of the ordinary bracket la-bels (S, NP, VP, etc.
), combined with function tagsfor grammatical roles, giving composite labels suchas NP-SBJ; we call this set B.
We assign labels toarcs by letting each (non-root) word that heads aphrase P in the original phrase structure have its in-coming edge labeled with the label of P (modulothe set of labels used).
In both sets, we also includea default label DEP for arcs that would not other-wise get a label.
This gives a total of 7 labels in theG set and 50 labels in the B set.
Figure 1 shows aconverted dependency tree using the B labels; in thecorresponding tree with G labels NP-SBJ would bereplaced by SBJ, ADVP and VP by DEP.We use the following metrics for evaluation:1.
Unlabeled attachment score (UAS): The pro-portion of words that are assigned the correcthead (or no head if the word is a root) (Eisner,1996; Collins et al, 1999).2.
Labeled attachment score (LAS): The pro-portion of words that are assigned the correcthead and dependency type (or no head if theword is a root) (Nivre et al, 2004).3.
Dependency accuracy (DA): The proportionof non-root words that are assigned the correcthead (Yamada and Matsumoto, 2003).4.
Root accuracy (RA): The proportion of rootwords that are analyzed as such (Yamada andMatsumoto, 2003).5.
Complete match (CM): The proportion ofsentences whose unlabeled dependency struc-ture is completely correct (Yamada and Mat-sumoto, 2003).All metrics except CM are calculated as meanscores per word, and punctuation tokens are con-sistently excluded.Table 1 shows the attachment score, both unla-beled and labeled, for the two different state modelswith the two different label sets.
First of all, wesee that Model 1 gives better accuracy than Model2 with the smaller label set G, which confirms ourexpectations that the added part-of-speech featuresare helpful when the dependency labels are less in-formative.
Conversely, we see that Model 2 outper-forms Model 1 with the larger label set B, whichis consistent with the hypothesis that part-of-speechfeatures become redundant as dependency labels getmore informative.
It is interesting to note that thiseffect holds even in the case where the dependencylabels are mostly derived from phrase structure cate-gories.We can also see that the unlabeled attachmentscore improves, for both models, when the set ofdependency labels is extended.
On the other hand,the labeled attachment score drops, but it must beremembered that these scores are not really com-parable, since the number of classes in the classifi-cation problem increases from 7 to 50 as we movefrom the G set to the B set.
Therefore, we have alsoincluded the labeled attachment score restricted tothe G set for the parser using the B set (BG), and wesee then that the attachment score improves, espe-cially for Model 2.
(All differences are significantbeyond the .01 level; McNemar?s test.
)Table 2 shows the dependency accuracy, rootaccuracy and complete match scores for our bestparser (Model 2 with label set B) in comparisonwith Collins (1997) (Model 3), Charniak (2000),and Yamada and Matsumoto (2003).5 It is clear that,with respect to unlabeled accuracy, our parser doesnot quite reach state-of-the-art performance, evenif we limit the competition to deterministic meth-ods such as that of Yamada and Matsumoto (2003).We believe that there are mainly three reasons forthis.
First of all, the part-of-speech tagger usedfor preprocessing in our experiments has a loweraccuracy than the one used by Yamada and Mat-sumoto (2003) (96.1% vs. 97.1%).
Although thisis not a very interesting explanation, it undoubtedlyaccounts for part of the difference.
Secondly, since5The information in the first three rows is taken directlyfrom Yamada and Matsumoto (2003).our parser makes crucial use of dependency type in-formation in predicting the next action of the parser,it is very likely that it suffers from the lack of realdependency labels in the converted treebank.
Indi-rect support for this assumption can be gained fromprevious experiments with Swedish data, where al-most the same accuracy (85% unlabeled attachmentscore) has been achieved with a treebank whichis much smaller but which contains proper depen-dency annotation (Nivre et al, 2004).A third important factor is the relatively low rootaccuracy of our parser, which may reflect a weak-ness in the one-pass parsing strategy with respect tothe global structure of complex sentences.
It is note-worthy that our parser has lower root accuracy thandependency accuracy, whereas the inverse holds forall the other parsers.
The problem becomes evenmore visible when we consider the dependency androot accuracy for sentences of different lengths, asshown in Table 3.
Here we see that for really shortsentences (up to 10 words) root accuracy is indeedhigher than dependency accuracy, but while depen-dency accuracy degrades gracefully with sentencelength, the root accuracy drops more drastically(which also very clearly affects the complete matchscore).
This may be taken to suggest that some kindof preprocessing in the form of clausing may helpto improve overall accuracy.Turning finally to the assessment of labeled de-pendency accuracy, we are not aware of any strictlycomparable results for the given data set, but Buch-holz (2002) reports a labeled accuracy of 72.6%for the assignment of grammatical relations usinga cascade of memory-based processors.
This can becompared with a labeled attachment score of 84.4%for Model 2 with our B set, which is of about thesame size as the set used by Buchholz, although thelabels are not the same.
In another study, Blahetaand Charniak (2000) report an F-measure of 98.9%for the assignment of Penn Treebank grammaticalrole labels (our G set) to phrases that were correctlyparsed by the parser described in Charniak (2000).If null labels (corresponding to our DEP labels) areexcluded, the F-score drops to 95.7%.
The corre-sponding F-measures for our best parser (Model 2,BG) are 99.0% and 94.7%.
For the larger B set,our best parser achieves an F-measure of 96.9%(DEP labels included), which can be compared with97.0% for a similar (but larger) set of labels inCollins (1999).6 Although none of the previous re-sults on labeling accuracy is strictly comparable toours, it nevertheless seems fair to conclude that the6This F-measure is based on the recall and precision figuresreported in Figure 7.15 in Collins (1999).Model 1 Model 2G B BG G B BGUAS 86.4 86.7 85.8 87.1LAS 85.3 84.0 85.5 84.6 84.4 86.0Table 1: Parsing accuracy: Attachment score (BG = evaluation of B restricted to G labels)DA RA CMCharniak 92.1 95.2 45.2Collins 91.5 95.2 43.3Yamada & Matsumoto 90.3 91.6 38.4Nivre & Scholz 87.3 84.3 30.4Table 2: Comparison with related work (Yamada and Matsumoto, 2003)labeling accuracy of the present parser is close to thestate of the art, even if its capacity to derive correctstructures is not.5 ConclusionThis paper has explored the application of a data-driven dependency parser to English text, using datafrom the Penn Treebank.
The parser is deterministicand uses a linear-time parsing algorithm, guided bymemory-based classifiers, to construct labeled de-pendency structures incrementally in one pass overthe input.
Given the difficulty of extracting labeleddependencies from a phrase structure treebank withlimited functional annotation, the accuracy attainedis fairly respectable.
And although the structural ac-curacy falls short of the best available parsers, thelabeling accuracy appears to be competitive.The most important weakness is the limited ac-curacy in identifying the root node of a sentence,especially for longer sentences.
We conjecture thatan improvement in this area could lead to a boostin overall performance.
Another important issueto investigate further is the influence of differentkinds of arc labels, and in particular labels that arebased on a proper dependency grammar.
In thefuture, we therefore want to perform more experi-ments with genuine dependency treebanks like thePrague Dependency Treebank (Hajic, 1998) and theDanish Dependency Treebank (Kromann, 2003).We also want to apply dependency-based evaluationschemes such as the ones proposed by Lin (1998)and Carroll et al (1998).AcknowledgementsThe work presented in this paper has been supportedby a grant from the Swedish Research Council (621-2002-4207).
The memory-based classifiers used inthe experiments have been constructed using theTilburg Memory-Based Learner (TiMBL) (Daele-mans et al, 2003).
The conversion of the Penn Tree-bank to dependency trees has been performed usinghead rules kindly provided by Hiroyasu Yamada andYuji Matsumoto.ReferencesD.
W. Aha, D. Kibler, and M. Albert.
1991.Instance-based learning algorithms.
MachineLearning, 6:37?66.D.
Aha, editor.
1997.
Lazy Learning.
Kluwer.A.
Bies, M. Ferguson, K. Katz, and R. MacIn-tyre.
1995.
Bracketing guidelines for Treebank IIstyle, Penn Treebank project.
University of Penn-sylvania, Philadelphia.E.
Black, F. Jelinek, J. Lafferty, D. Magerman,R.
Mercer, and S. Roukos.
1992.
Towardshistory-based grammars: Using richer models forprobabilistic parsing.
In Proceedings of the 5thDARPA Speech and Natural Language Workshop.D.
Blaheta and E. Charniak.
2000.
Assigningfunction tags to parsed text.
In Proceedings ofNAACL, pages 234?240.S.
Buchholz.
2002.
Memory-Based GrammaticalRelation Finding.
Ph.D. thesis, University ofTilburg.J.
Carroll, E. Briscoe, and A. Sanfilippo.
1998.Parser evaluation: A survey and a new pro-posal.
In Proceedings of LREC, pages 447?454,Granada, Spain.E.
Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of NAACL.M.
Collins, J.
Hajic?, E. Brill, L. Ramshaw, andC.
Tillmann.
1999.
A Statistical Parser of Czech.In Proceedings of ACL, pages 505?512, Univer-sity of Maryland, College Park, USA.DA RA CM?
10 93.7 96.6 83.611?20 88.8 86.4 39.521?30 87.4 83.4 20.831?40 86.8 78.1 9.9?
41 84.6 74.9 1.8Table 3: Accuracy in relation to sentence length (number of words)M. Collins.
1996.
A new statistical parser based onbigram lexical dependencies.
In Proceedings ofACL, pages 184?191, Santa Cruz, CA.M.
Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings ofACL, pages 16?23, Madrid, Spain.M.
Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Uni-versity of Pennsylvania.S.
Cost and S. Salzberg.
1993.
A weighted near-est neighbor algorithm for learning with symbolicfeatures.
Machine Learning, 10:57?78.W.
Daelemans, A. van den Bosch, and J. Zavrel.2002.
Forgetting exceptions is harmful in lan-guage learning.
Machine Learning, 34:11?43.W.
Daelemans, J. Zavrel, K. van der Sloot, andA.
van den Bosch.
2003.
Timbl: Tilburg mem-ory based learner, version 5.0, reference guide.Technical Report ILK 03-10, Tilburg University,ILK.W.
Daelemans.
1999.
Memory-based languageprocessing.
Introduction to the special issue.Journal of Experimental and Theoretical Artifi-cial Intelligence, 11:287?292.S.
A. Dudani.
1976.
The distance-weighted k-nearest neighbor rule.
IEEE Transactions on Sys-tems, Man, and Cybernetics, SMC-6:325?327.J.
Einarsson.
1976.
Talbankens skriftspra?kskonkor-dans.
Lund University.J.
M. Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Pro-ceedings of COLING, Copenhagen, Denmark.E.
Fix and J. Hodges.
1952.
Discriminatory anal-ysis: Nonparametric discrimination: Consistencyproperties.
Technical Report 11, USAF School ofAviation Medicine, Randolph Field, Texas.J.
Hajic.
1998.
Building a syntactically annotatedcorpus: The prague dependency treebank.
In Is-sues of Valency and Meaning, pages 106?132.Karolinum.M.
T. Kromann.
2003.
The Danish dependencytreebank and the DTAG treebank tool.
In Pro-ceedings of the Second Workshop on Treebanksand Linguistic Theories, pages 217?220, Va?xjo?,Sweden.T.
Kudo and Y. Matsumoto.
2000.
Japanese depen-dency structure analysis based on support vec-tor machines.
In Proceedings of EMNLP/VLC,Hongkong.D.
Lin.
1998.
Dependency-based evaluation ofMINIPAR.
In Proceedings of LREC.D.
M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proceedings of ACL,pages 276?283, Boston, MA.M.
P. Marcus, B. Santorini, and M. A.Marcinkiewicz.
1993.
Building a large an-notated corpus of English: The Penn Treebank.Computational Linguistics, 19:313?330.J.
Nivre, J.
Hall, and J. Nilsson.
2004.
Memory-based dependency parsing.
In Proceedings ofCoNLL, pages 49?56.J.
Nivre.
2003.
An efficient algorithm for projectivedependency parsing.
In Proceedings of IWPT,pages 149?160, Nancy, France.K.
Oflazer.
2003.
Dependency parsing with an ex-tended finite-state approach.
Computational Lin-guistics, 29:515?544.R.
Skousen.
1989.
Analogical Modeling of Lan-guage.
Kluwer.R.
Skousen.
1992.
Analogy and Structure.
Kluwer.C.
Stanfill and D. Waltz.
1986.
Toward memory-based reasoning.
Communications of the ACM,29:1213?1228.V.
N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer-Verlag.J.
Veenstra and W. Daelemans.
2000.
A memory-based alternative for connectionist shift-reduceparsing.
Technical Report ILK-0012, Universityof Tilburg.H.
Yamada and Y. Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.In Proceedings of IWPT, pages 195?206, Nancy,France.
