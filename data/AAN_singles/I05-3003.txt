Chinese Deterministic Dependency Analyzer: Examining Effects ofGlobal Features and Root Node FinderYuchang CHENG, Masayuki ASAHARA and Yuji MATSUMOTONara Institute of Science and Technology8916-5 Takayama, Ikoma, Nara 630-0192, Japan{yuchan-c, masayu-a, matsu}@is.naist.jpAbstractWe present a method for improvingdependency structure analysis of Chi-nese.
Our bottom-up deterministic ana-lyzer adopt Nivre?s algorithm (Nivreand Scholz, 2004).
Support Vector Ma-chines (SVMs) are utilized to deter-mine the word dependency relations.We find that there are two problems inour analyzer and propose two methodsto solve them.
One problem is thatsome operations cannot be solved onlyusing local feature.
We utilize theglobal features to solve this.
The otherproblem is that this bottom-up analyzerdoesn?t use top-down information.
Wesupply the top-down information byconstructing SVMs based root nodefinder to solve this problem.
Experi-mental evaluation on the Penn ChineseTreebank Corpus shows that the pro-posed extensions improve the parsingaccuracy significantly.1 IntroductionMany syntactic analyzers for English have beenimplemented and have demonstrated good per-formance (Charniak, 2000; Collins, 1997; Rat-naparkhi, 1999).
However, implementation ofChinese syntactic structure analyzers is still lim-ited, since the structure of the Chinese languageis quite different from other languages.
There-fore the experience in processing western lan-guages cannot be guaranteed that it can apply toChinese language directly (Lee, 1991).
Chineselanguage has many special syntactic phenomenasubstantially different from western languages.Discussions about such characteristics of Chi-nese language can be found in the literature(Chao 1968; Li and Thompson 1981; Huang1982).About the previous work of Chinese depend-ency structure analysis, Zhou proposed a rulebased approach (Zhou, 2000).
Lai et al pro-posed a span-based statistical probability ap-proach (Lai, 2001).
Ma et al proposed a statisticdependency parser by using probabilistic model(Ma, 2004).
Using machine learning-based ap-proaches for dependency analysis of Chinese isstill limited.
In this paper, we propose a deter-ministic Chinese syntactic structure analyzer byusing global features and a root node finder.Our analyzer is a dependency structure ana-lyzer.
We utilize a deterministic method for de-pendency relation construction.
First, adependency relation matrix is constructed, inwhich each element corresponds to a pair of to-kens.
A likelihood value is assigned to the de-pendency relation of each pair of tokens.Second, the optimal dependency structure is es-timated using the likelihood of the whole sen-tence, provided there is no crossing betweendependencies.
A bottom-up algorithm proposedby (Nivre and Scholz, 2004) is use for a deter-ministic dependency structure analysis.
Our de-pendency relations are composed by machinelearners.
SVMs (Vapnik, 1998) deterministicallyestimate if there is a dependency relation be-tween a pair of words in the methods.However, this method has two problems.
First,some operations in the algorithm needs longdistance information.
However, the long dis-tance information cannot be available if we as-sume a context of a fixed size in all operations.17The second problem is that the top-down infor-mation isn?t used in the bottom-up approach.We use the global features to solve the firstproblem and we construct a SVM-based rootnode finder in our system to supplement the top-down information.Our analyzer is trained on the Penn ChineseTreebank 5.0 (Xue et al, 2002), which is a phrasestructure annotated corpus.
The phrase structureis converted into a dependency structure accord-ing to the head rules.
We perform experimentalevaluation in several settings on this corpus.In the next section, we describe our determi-nistic dependency structure analysis algorithm.Section 3 shows the global features and the two-step process.
Section 4 describes the use of theroot node finder.
Section 5 describes the ex-perimental setting and the results.
Finally, wesummarize our findings in the conclusion.2 Parsing methodThis chapter presents a basic parsing algorithmproposed by (Nivre and Scholz, 2004).
The al-gorithm is the base of our dependency analyzer.This algorithm is based on a deterministic ap-proach, in which the dependency relations areconstructed by a bottom-up deterministicschema.
While Nivre?s method uses memory-based learning, we use SVMs instead.
The algo-rithm consists of two major procedures:(i) Extract the surrounding features for thefocused node (or node pair).
(ii) Estimate the dependency relation opera-tion for the focused node by a machinelearning method.Example: ????????????
(The great triumph that Cheng Cheng-Kung recaptured Taiwan.)Fig.
1.
The operations of the Nivre algorithm??recapturedVV???
(name)NRS I??recapturedVV???
(name)NRS IRightS I??recapturedVV???
(name)NRS ILeftS I S IReduceS I S IShift??recapturedVV???(name)NR??TaiwanNR??TaiwanNR??recapturedVV???(name)NR??TaiwanNR??recapturedVV???(name)NR??TaiwanNR?DEDEG?DEDEG??recapturedVV???(name)NR??TaiwanNR??recapturedVV???(name)NR??TaiwanNR??greatVA?DEDEG??greatVA?DEDEG??TriumphNN??greatVA??greatVA?DEDEG?DEDEG??greatVA??TaiwanNR?
?TaiwanNRposition t-1 position n position n+1position tt-1 n n+1tt-1 n n+1tt-1 n n+1t t-1 n n+1tt-1 n n+1tt-1 n n+1tt-1 n n+1tA{    } A{???
->??
}A{???
->??}
A{???
->??
,??
->??
}A{???
->??
,??
->??
}A{???
->??
,??
->??
}A{???
->??
,??
->??
}A{???
->??
,??
->??
}182.1   AlgorithmWe utilize a bottom-up deterministic algorithmproposed by (Nivre and Scholz, 2004) in ouranalyzer.
In the algorithm, the states of analyzerare represented by a triple AIS ,, .
S and I arestacks, S keeps the words being in consideration,and I keeps the words to be processed.
A is a listof dependency relations decide during the algo-rithm.
Given an input word sequence W, theanalyzer is initialized by the triple ?,,Wnil .The analyzer estimates the dependency relationbetween two words (the top elements of stack Sand stack I).
The algorithm iterates until the listI becomes empty.
Then, the analyzer outputs theword dependency relations A.There are four possible operations for the con-figuration at hand:Right: Suppose the current triple isAInSt ,|,| (t and n are the top elements, S andI are the remaining elements in the stacks), ifthere is a dependency relation that the word tdepends on word n, add the new dependencyrelation ( )nt ?
into A, remove t from S. Theconfiguration now becomes ( ){ }ntAInS ?,|, .Left: In the current triple is AInSt ,|,|  , ifthere is a dependency relation that the word ndepends on the word t, adds the new dependencyrelation ( )tn ?
into A, push n onto the stack S.The configuration now becomes( ){ }tnAIStn ?,,|| .Suppose the current triple is AInSt ,|,| , ifthere is no dependency relation between n and t,check the following conditions.Reduce: If there are no more words 'n ( In ?'
)which may depend on t, and t has a parent on itsleft side, the analyzer removes t from the stack S.The configuration now becomes AInS ,|, .Shift: If there is no dependency between n and t,and the triple doesn?t satisfy the conditions forReduce, then push n onto the stack S. The con-figuration now becomes AIStn ,,|| .These operations are depicted in Fig.
1.
Givenan input sentence of length N (words), the ana-lyzer is guaranteed to terminate after at most 2Nactions.
The dependency structure given at thetermination is well-formed if and only if the re-lations in A constitute a single connected tree.This means that the algorithm produces a well-formed dependency graph.2.2   Machine learning methodA classification task usually involves with train-ing and testing data which consist of annotateddata instances.
Each instance in the training setcontains one ?target value?
(class label) andseveral ?attributes?
(features).
The goal of aclassifier is to produce a model which predictstarget value of data instances in the testing setwhich only give the attributes.SVMs are binary classifiers based on themaximal margin strategy.
Suppose we have a setof training data for a binary classification prob-lem: )y)...(y( nn11 ,, ZZ , where nR?iZ  is the fea-ture vector of the i-th sample in the training dataand }1,1{ ?+?iy is the class label of the sample.The goal is to find a decision function))(()( ?
?+=SViiibKyasignxf\i,\Z  for an input vec-tor Z .
The vectors SV?K\  are called supportvectors, which are representative examples.Support vectors and other constants are deter-mined by solving a quadratic programmingproblem.
)( zx,K is a kernel function which mapsvectors into a higher dimensional space.
We usethe polynomial kernel: dK )1()( zxzx, ?+= .
Theperformance of SVMs is better than using othermachine learning methods, such as memorybased learning or maximum entropy method, inour analyzer.
This is because that SVMs canadopt combining features automatically (usingthe polynomial kernel), whereas other methodcannot.
To extend binary classifiers to multi-class classifiers, we use the pair-wise method,which utilizes 2Cn  binary classifiers between allpairs of the classes (Kreel, 1998).
We useLibsvm (Lin et al, 2001) in our experiments.2.3   Features (Local features)It should be noted that we use a different ma-chine learner from the original method (Nivre,2004).
Nivre?s work used memory based learn-ing in their analyzer, we utilize SVMs in ouranalyzer.
Therefore, the features of our analyzerare different from the original Nivre?s method.In our method, the analyzer considers the de-pendency of two nodes (n,t) which are in current19triple.
The nodes include the word, the POS-tagand the information of its children.
The contextfeatures we use are 2 preceding nodes of node t(and t itself), 2 succeeding nodes of node n (andn itself), and their child nodes.
The distance be-tween nodes n and t is also used as a feature.We call these features as local features.3 Global features and two-step processIn the algorithm, the operation Reduce needsthe condition that the node n should have nochild in I.
However, it is difficult to check thiscondition.
In a long sentence, the modifier of thefocused node n may be far away from n. More-over, some non-local dependency may cause thiskind of error.
In this section, we will describethis problem and a solution to it.3.1   Global featuresThe analyzer selects features for deciding theoptimum operation, and then gives these fea-tures to machine learner.
The machine learneruses the same information to decide the opti-mum operation even when these operations es-sentially disagree.
However, the differentoperation consists of different condition.
In thedeterministic bottom-up dependency analysis,we can generally consider the process as twotasks:Task 1: Does the focused word depend on aneighbor node?Task 2: Does the focused word may have achild in the remaining token sequence?In the Task 1, the problem can be resolved byusing the information of the neighbor nodes.This information is possibly the same as the fea-tures that we described in section 2.3.
However,these features may not be able to resolve theproblem in task 2.
For resolving the problem intask 2, we need the information of long distancedependency.
In Fig.
2, for example, the analyzeris considering the relation between focusedwords ???
(tell)?
and ??
(he)?.
The featuresused in this original analysis are the informationof words ??
(please)?, ???
(tell)?, ??(he)?,???
(what time)?
and ???
(prepare)?.
Thesefeatures are ?local features?.
The correct answerin this situation is the operation ?Shift?.
It isbecause the word ???
(tell)?
has a child ???(start)?
which is not yet analyzed and the fo-cused words don?t depend on each other.
How-ever, the local features do not include theinformation of word ???
(start)?.
Therefore,the analyzer possibly estimates the answer as theoperation ?Reduce?.
The results make a mistakein this situation because of the lack of long dis-tance information.
To resolve this problem, weshould refer some information of long distancedependency in machine learning.
The informa-tion about long distance relations is defined as?global features?.
In this paper, we select thewords which remain in stack I but don?t be con-sider in local features as global features.Fig.
2.
An example of the ambiguity of deciding the long distance dependency relation and using two-steps classification dependency relation??prepare?please?you??tell?I?
?What time?
?start?HeS I(Please  tell me what time he will prepare to start.
)Classificationwith localfeaturesOutput :shiftLocal featuresGlobal featuresClassificationwith  globalfeaturesOutput :reduce203.2   two-step processTo use the global features, we cannot use themimmediately because the global features are noteffective in all operations.
For using global fea-tures efficiently, we propose a two-step processin our analyzer.
The analysis processes are di-vided to two processes.
First, the analyzer usesonly the local features (as described in Section2.3) to decide the optimum operation.
If the re-sult is ?Reduce?
or ?Shift?, it means that thefocused words do not have any dependency rela-tion.
The analyzer leaves the decision to anothermachine learner that makes use of global fea-tures.
The analyzer will select global features foranalyzing the Task 2.
Then the analyzer outputsthe final answer of this analysis process.Fig.
2 describes an example of using two-stepclassification for analyzing dependency relation.In this example, the focused words are ??
(I)?and ??
(He)?.
The word ??
(I)?
depends onthe word ???
(tell)?.
The local features aresurrounded by dotted line and the global featuresare surrounded by solid line.
The analyzer usedlocal features to analyze the operation of thissituation.
The result is the operation ?shift?.
Theanalyzer then selected the global features to ana-lyze again and the output is the operation ?re-duce?.
The final result of this situation is theoperation ?reduce?.4 The root node finderIn Isozaki?s work (Isozaki et.
al, 2004), theyadopted a root finder in their system to find theroot word of the input sentence.
Their methodused the information of the root word as a newfeature for machine learning.
Their experimentsshowed that information of root word was abeneficial feature.
However, we think the infor-mation of root word can be used not only as thefeature of machine learning, but also can be usedto divide the sentence.
Therefore, the complex-ity of the sentence can be alleviated by dividingthe input sentence.4.1   Root node and dividing sentence byusing root finderIn the fundamental definition of dependencystructure, there is one and only one head word ina dependency structure.
An element cannot havedependents lying on the other side of its owngovernor.These peculiarities imply that the head worddivides the phrase into two independent partsand each part does not cross the head word.
Asin Fig.
3, the original input sentence has a rootword (the head word of phrase) ??
(and)?.There are not any dependency relation whichcrosses the root word.
Therefore we can dividethis sentence into two sub-sentence ???
(exo-dus) / ?
(do) / ??
(study) / ?
(and)?
and ??
(and) / ?
(go) / ??
(foreign country) / ?
(do)/ ??
(visit)?.
Both these sub-sentences havetheir root word and the root word is ??
(and)?.We can conceive that to analyze the dependencystructure of the full sentence is to analyze thedependency structure of two sub-sentences.Combining structures of two sub-sentences, wecan get the full structure of original sentence.Our dependency analyzer is a bottom-up deter-ministic analyzer.
Instinctively, the accuracy ofanalyzing short sentence is significantly betterthan analyzing long sentence.
Thus the perform-ance of the dependency analyzer can be im-proved by this method.4.2   Constructing a root finderTo use the root node, we should construct theroot finder.
Similarly to Isozaki?s work, we usemachine learner (SVMs) to construct the rootfinder.
We refer to the features which are usedin Isozaki?s work and investigate other effectivefeatures.
The performance of our root nodefinder is 90.71%.
This is better than the root ac-curacy of our analyzer (86.22%, see Table 2).Fig.
3.
Dividing the phrase as two phrases by the rootword??
?
??
?
?
??
?
??
(To Leave native country to study and to visit other country.
)The root word??
?
??
?
?
?
??
?
?
?The root word The root wordOriginal inputsentence:Divide by theroot word:Part 1 Part 221Therefore, using the root finder can give the de-pendency analyzer more top-down information.The tags and features of the root finding areshown in Fig.
4.
We extract all root words in thetraining data and tagging every word to showthat it is root word or not.
For example, the rootword in Fig.
4 is ???
(get)?.
The root finderanalyzes each word in the sentence and gives thetag ?true?
or ?false?
to indicate the root word.The features for machine learning of root finderinclude the contextual features (the informationabout the focused word, the two precedingwords, and two succeeding words) and the wordrelation features (the words which are in the out-side of the window).
Other effectual featuresinclude the Boolean features ?root word isfound?
and ?the focus word is the first/last wordof sentence?.
For example, the contextual fea-tures of the word ???
(economic)?
includeinformation of the focused (n) word ???
(eco-nomic)?, the ?n-1?th word ???
(wide)?, the?n-2?th word ??
(DE)?, the ?n+1?th word?
??
(environment)?
and the ?n+2?th word ??(will)?.
The word relation features include thepreceding word set {??
(China)}, the suc-ceeding word set {?
?, ??
?, ?, ??}
andthe Boolean features are:?root_word_is_found=false?,?first_word=false?
,?last_word=false?.When we use the root finder to analyze theroot word of the sentence, we do not know thestructure of input sentence (either the phrasestructure or the dependency structure).
It maylook odd that the root finder can analyzes theroot word without any information of the struc-ture.
However, this analysis is practicable.
Natu-rally, the root word of a sentence is usually averb (about 61% of sentences have a verb as theroot word in our testing corpus).
For example, inthe example 1 of Fig.
5 ??
/ ?
/ ??
(I go toschool)?, we know the POS-tags are ?noun, verb,noun?
thus we can find that the root word is ??(go)?.
However, many sentences include morethen one verb or the root word is not verb (in NPor PP?etc.).
We can not only choose the verbsas root word directly.
To decide the root word ofcomplex sentences, there are some specialword/POS relations that can be used to estimatethe root node of a sentence.
Considering the rootfinder in Fig.
4, the root finder gives the root tagto each word of the sentence.The processes of analyzing the root word canbe thought as two tasks:Task 1: Does the focus word depend on aneighbor word?Task 2: Are there any special relation in the sen-tence?In Fig.
4, the contextual features (two pre-ceding words and two succeeding words) can beused to process the Task 1, and the word rela-tion features can be used to process the Task 2.If the focused word possibly depends onneighbor words, it is impossible that the focusedword is the root word.
Therefore these wordswill be tagged as ?false?.Alternately, considering the example 2 in Fig.5, the sentence has a verb ???
(recapture)?,but the special word ??
(DE)?
is in the rightside of the verb ???
(recapture)?.
Therefore,the verb ???
(recapture)?
is possibly in the?
(DE)-phrase and the verb cannot be the rootword.
The special word ??
(DE)?
resembles apreposition and it is always the last word of DE-phrase.
Therefore, although we do not know thestructure of sentence, we can identify whichwords can be the root word by the relation andposition of the features.
If the features of thefocused word include the special word relationsFig.
4.
The features and tag of root finderWord POS Tag??
NR false?
DEG false??
JJ false??
NN false??
NN false?
AD false??
VV true???
JJ false?
DEG false??
NN falseEOSPosition 0Position -1Position -2Position 1Position 2Focus wordContextualfeatureWordrelation Fig.
5.
The examples of analyzing the root wordof sentencesRoot???
??
??
?
??
?
?NR       VV       NR      DEG   VA        NN(The great triumph that Cheng Cheng-Kung recapturedTaiwan.
)?
?
?
?DT      VV      NN(I go school.)
RootExample 1:Example 2:22(for example, the focused word is in the preposi-tional phrase), it isn?t the root word.
The fea-tures ?word relations?
in Fig.
5 can consider thissituation.5 Experiments5.1 Corpus and estimationWe use Penn Chinese Treebank 5.0 (Xue et al,2002) in our experiments.
This Treebank is rep-resented by phrase structure and doesn?t includethe head information of each phrase.
The firststep of using Penn Chinese Treebank is to derivethe head rules for deciding the head word ofeach phrase.
Some examples of head rules areshown in Table 1.
We convert the Treebank byusing these head rules.
The training corpus in-cludes about 377,408 words for learning and63,886 words for testing.
It should be noted thatthe punctuation mark ???
marks the end of asentence in the Treebank.
However, the punc-tuation mark ???
also can be the end of a sen-tence.
It is hard to determine the dependencyrule of the clauses on the both side of comma.Therefore, to decide the dependency relationwhich crosses a punctuation mark ???
is difficult.We do not deal with the ambiguity of commasand divide the sentence by the punctuation mark??
?.Phrase The order of deciding the headof phrase (from left)ADJP CC PZ ADJP JJADVP CC PZ ADCLP PZ CLP M LCDP DP CLP QP DTDVP DEV DEC DEGVCP VC VVTable 1.
Some examples of head rulesThe performance of our dependency structureanalyzer is evaluated by the following threemeasures:Dependency Accuracy:relationsdependencyofnumberrelationsdependencyanalyzedcorrectlyofnumber=Root Accuracy:clausesofnumbernodesrootanalyzedcorrectlyofnumber=Sentence Accuracy:clausesofnumberclauseanalyzedcorrectlyfullyofnumber=5.2 Results and discussionOur experimental results are shown in Table.
2.First row in the table is the result of our basicanalyzer (Nivre algorithm with SVMs), secondand third row show the effects of the proposedextensions.
The last row is the result of combin-ing the two extensions.
We had used McNemartest to confirm the significance of the methods.The McNemar test proves that using the pro-posed methods improve the analyzers signifi-cantly.
Comparing the results of our basicanalyzer to related works, our analyzer (dep.Accuracy: 87.64) is better than (Ma et al, 2004,dep.
Accuracy: 80.38) and (Zhou, 2000, dep.Accuracy of newspaper: 67.7).
However, theseresearches used different corpus.
We cannotcompare the performances directly.According to the second row of Table.
2, di-viding the process of classification as two stepscan improve the performance of dependencyanalyzer.
However, the improvement of usingthis method is limited.
This is because that longdistance relations are not many in the corpus.The absence of global information does not oc-cur in the sentences without long distance rela-tions.
Another reason is the distribution ofoperations.
The instances of operations in ourexperimental corpus are not balanced.
The op-eration ?reduce?
is the least (7.8%) and it is farless than other operations.
Therefore the in-stances for creating the model of operation ?re-duce?
are not satisfactory.
These facts result inthat our experiment of using two step classifica-tion cannot improve the analyzer remarkably.About the experiment of utilizing root finderin our analyzer, we tried to adopt the root infor-mation to the analyzer (using the information asfeatures for machine learning).
However, theperformance is worse than the baseline (the fun-damental analyzer ?Nivre+SVMs?).
Therefore,we use our method to improve the analyzer byusing root information (dividing the sentenceaccording to root node).According to the third row of Table.
2, divid-ing the sentence into two sub-sentences can im-prove the performance of dependency analyzer.However, the sentence accuracy cannot increasereliably.
This result shows that using root finderand dividing sentence can reconstruct some mis-takes in sentences.
Certainly, the performance ofthe root finder influences the analyzer strongly.If we use a perfect root node finder into our ana-lyzer, the performance will improve signifi-cantly.23The last row of Table.
2 shows the results ofcombining the two proposed methods (usingglobal features and root node finder) to improveour analyzer.
Combining two methods can in-crease the dependency accuracy better than us-ing either one of the methods.
It means thatsome analysis errors of fundamental analyzercan be resolved by using both improvementmethods.
Therefore using combined methodcannot supply higher improvement.Dep.Acc.RootAcc.Sent.Acc.Baseline(Nivre withSVMs)85.25 86.18 59.98Baseline withtwo-stepprocess85.44 86.22 60.1Baseline withroot nodefinder86.13 90.94 61.33Baseline withtwo-stepprocess androot nodefinder86.18 90.94 61.33Table 2.
The experimental results6 Conclusion and future workIn this paper, we present two methods to im-prove a deterministic dependency structure ana-lyzer for Chinese.
This basic analyzerimplements a bottom-up deterministic algorithmwith SVMs.
We convert a phrase structure anno-tated corpus (Penn Chinese Treebank) to de-pendency tagged corpus by using head rules.According to the properties of Chinese languageand dependency structure, we try to add a rootfinder in our dependency analyzer to improvethe analyzer.
Moreover, considering the machinelearning process of our analyzer, we divide theprocess into two processes to improve the per-formance of analyzer.
The improving methods(using root finder and dividing machine learningprocess) showed to improve the analyzer.Future work includes three points.
First, weshould improve the performance of the rootfinder.
Second, we should construct a usefulprepositional phrase chunker, because theprepositional phrase is a major error source ofour basic analyzer.
The original analyzer tendsto let the preposition governing a partial subtreeof the full phrase.
According to the properties ofChinese language, the prepositional phrases inChinese are head-initial.
Intuitively, if we canextract the prepositional phrases from sentence,the complexity of the sentence will decrease.Thus an important task is how to chunk theprepositional phrase in the sentence.Finally, we should deal with the ambiguity ofthe meaning of punctuation mark ?,?.
The defi-nition of ?sentence?
is ambiguous in Chinese.
InChinese articles, the normal ending mark of asentence is the punctuation mark ???.
However,the mark ???
is often used at the end of a sen-tence.
To distinguish the meaning of the punc-tuation mark ???
is difficult.
Therefore, weshould adopt semantic analysis in our analyzer.References1.
Eugene Charniak, 2001.
Immediate-Head Parsingfor Language Models.
pages 124-131, NAACL-2001.2.
Yuen Ren Chao, 1968.
A Grammar of SpokenChinese.
Berkeley, CA: University of CaliforniaPress.3.
Michael Collins, Brian Roark, 2004, Incrementalparsing with the Perceptron algorithm.
Pages 112-119, ACL-2004.4.
J. Huang, 1982.
Logical relations in Chinese andthe theory of grammar Doctoral dissertation, Mas-sachusetts Institute of Technology, Cambridge.5.
Ulrich.
H.-G. Kre?el, 1998.
Pairwise classificationand support vector machines.
In Advances inKernel Methods, pages 255?268.
The MIT Press.6.
Chih Jen Lin, 2001.
A practical guide to supportvector classification, http://www.csie.ntu.edu.tw/~cjlin/libsvm/.7.
Lai, Bong Yeung Tom, Huang, Changning, 1994.Dependency Grammar and the Parsing of ChineseSentences.
PACLIC 19948.
Hideki Isozaki, Hideto Kazawa, Tsutomu Hirao,2004.
A Deterministic Word Dependency Ana-lyzer Enhanced With Preference Learning, pages275-281, COLING-20049.
Charles Li, and Thompson Sandra A., 1981.
Man-darin Chinese.
University of California Press.10.
Lin-Shan Lee, Long-Ji Lin, Keh-Jiann Chen, andJames Huang, 1991.
An Efficient Natural Lan-guage Processing System Specially Designed forthe Chinese Language.
ComputationaI Linguistics,Volume 17, Number 4.11.
Ma Jinshan, Zhang yu, Liu ting, and Li sheng,2004.
A Statistical Dependency Parser of Chinese-under Small Training Data.
IJCNLP 2004 Work-shop: Beyond shallow analyses, Formalisms andstatistical modeling for deep analyses.12.
Joakim Nivre and Mario Scholz, 2004.
Determi-nistic Dependency Parsing of English Text.
Pages64-70, COLING-2004.13.
Adwait Ratnaparkhi, 1999.
Learning to parsenatural language with maximum entropy models.Machine Learning, 34(1-3) pages151?175.14.
Vladimir N. Vapnik, 1998.
Statistical LearningTheory.
A Wiley-Interscience Publication.15.
Nianwen Xue, Fu-Dong Chiou, Martha StonePalmer, 2002.
Building a Large-Scale AnnotatedChinese Corpus.
COLING 200216.
Ming Zhou, 2000.
A block-based robust depend-ency parser for unrestricted Chinese text.
The sec-ond Chinese Language Processing Workshopattached to ACL-2000.24
