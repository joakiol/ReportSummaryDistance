Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 265?270,Dublin, Ireland, August 23-24, 2014.ECNU: Leveraging on Ensemble of Heterogeneous Features andInformation Enrichment for Cross Level Semantic Similarity EstimationTian Tian ZhuDepartment of Computer Science andTechnologyEast China Normal University51111201046@ecnu.cnMan Lan?Department of Computer Science andTechnologyEast China Normal Universitymlan@cs.ecnu.edu.cn?AbstractThis paper reports our submissions to theCross Level Semantic Similarity (CLSS)task in SemEval 2014.
We submittedone Random Forest regression system oneach cross level text pair, i.e., Paragraphto Sentence (P-S), Sentence to Phrase (S-Ph), Phrase to Word (Ph-W) and Wordto Sense (W-Se).
For text pairs on P-Slevel and S-Ph level, we consider them assentences and extract heterogeneous typesof similarity features, i.e., string features,knowledge based features, corpus basedfeatures, syntactic features, machine trans-lation based features, multi-level text fea-tures, etc.
For text pairs on Ph-W leveland W-Se level, due to lack of informa-tion, most of these features are not ap-plicable or available.
To overcome thisproblem, we propose several informationenrichment methods using WordNet syn-onym and definition.
Our systems rank the2nd out of 18 teams both on Pearson cor-relation (official rank) and Spearman rankcorrelation.
Specifically, our systems takethe second place on P-S level, S-Ph leveland Ph-W level and the 4th place on W-Selevel in terms of Pearson correlation.1 IntroductionSemantic similarity is an essential component ofmany applications in Natural Language Process-ing (NLP).
Previous works often focus on text se-mantic similarity on the same level, i.e., paragraphto paragraph or sentence to sentence, and many ef-fective text semantic measurements have been pro-posed (Islam and Inkpen, 2008), (B?ar et al., 2012),This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/(Heilman and Madnani, 2012).
However, in manyreal world cases, the two texts may not alwaysbe on the same level.
The Cross Level SemanticSimilarity (CLSS) task in SemEval 2014 providesa universal platform to measure the degree of se-mantic equivalence between two texts across dif-ferent levels.
For each text pair on four cross lev-els, i.e., Paragraph to Sentence (P-S), Sentence toPhrase (S-Ph), Phrase to Word (Ph-W) and Wordto Sense (W-Se), participants are required to re-turn a similarity score which ranges from 0 (norelation) to 4 (semantic equivalence).
We partici-pate in all the four cross levels and take the secondplace out of all 18 teams both on Pearson correla-tion (official) and Spearman correlation ranks.In this work, we present a supervised regres-sion system for each cross level separately.
ForP-S level and S-Ph level, we regard the paragraphof P-S as a long sentence, and the phrase of S-Ph as a short sentence.
Then we use various typesof text similarity features including string features,knowledge based features, corpus based features,syntactic features, machine translation based fea-tures, multi-level text features and so on, to cap-ture the semantic similarity between two texts.Some of these features are borrowed from our pre-vious system in the Semantic Textual Similarity(STS) task in?SEM Shared Task 2013 (Zhu andLan, 2013).
Others followed the previous workin (?Saric et al., 2012) and (Pilehvar et al., 2013).For Ph-W level and W-Se level, since the text pairslack contextual information, for example, word orsense alone no longer shares the property of sen-tence, most features used in P-S level and S-Phlevel are not applicable or available.
To overcomethe problem of insufficient information in wordand sense level, we propose several informationenrichment methods to extend information withthe aid of WordNet (Miller, 1995), which signif-icantly improved the system performance.The rest of this paper is organized as follows.265Section 2 describes the similarity features used onfour cross levels in detail.
Section 3 presents ex-periments and the results of four cross levels ontraining data and test data.
Conclusions and futurework are given in Section 4.2 Text Similarity MeasurementsTo estimate the semantic similarity on P-S leveland S-Ph level, we treat the text pairs on both lev-els as traditional semantic similarity computationon sentence level and adopt 7 types of features,i.e., string features, knowledge based features, cor-pus based features, syntactic features, machinetranslation based features, multi-level text featuresand other features.
All of them are borrowedfrom previous work due to their superior perfor-mance reported.
For Ph-W level and W-Se level,since word and sense alone cannot be treated assentence, we propose an information enrichmentmethod to extend original text with the help ofWordNet.
Once the word or sense is enriched withits synonym and its definition description, we canthus adopt the previous features as well.2.1 PreprocessingFor P-S level and S-Ph level, we perform text pre-processing before we extract semantic similarityfeatures.
Firstly, the Stanford parser1is used forsentence tokenization and parsing.
Specifically,the tokens n?t and ?m are replaced with not andam.
Secondly, the Stanford POS Tagger2is usedfor POS tagging.
Thirdly, we use Natural Lan-guage Toolkit3for WordNet based Lemmatiza-tion, which lemmatizes the word to its nearest baseform that appears in WordNet, for example, wasis lemmatized as is rather than be.2.2 Features on P-S Level and S-Ph LevelWe treat all text pairs of P-S level and S-Ph levelas sentences and then extract 7 types of similar-ity features as below.
Totally we get 52 similarityfeatures.
Generally, these similarity features arerepresented as numerical values.String features.
Intuitively, if two texts sharemore strings, they are considered to be more se-mantic similar.
We extract 13 string based featuresin consideration of the common sequence shared1http://nlp.stanford.edu/software/lex-parser.shtml2http://nlp.stanford.edu/software/tagger.shtml3http://nltk.org/by two texts.
We chose the Longest Common Se-quence (LCS) feature (Zhu and Lan, 2013), the N-gram Overlap feature (n=1,2,3) and the WeightedWord Overlap feature (?Saric et al., 2012).
Allthese features are computed from original textand from the processed text after lemmatizationas well.
Besides, we also computed the N-gramOverlap on character level, named Character N-gram (n=2,3,4).Knowledge based features.
Knowledge basedsimilarity estimation relies on the semantic net-work of words.
In this work we used the knowl-edge based features in our previous work (Zhu andLan, 2013), which include four word similaritymetrics based onWordNet: Path similarity (Baneaet al., 2012), WUP similarity (Wu and Palmer,1994), LCH similarity (Leacock and Chodorow,1998) and Lin similarity (Lin, 1998).
Then twostrategies, i.e., the best alignment strategy and theaggregation strategy, are employed to propagatethe word similarity to the text similarity.
Totallywe get 8 knowledge based features.Corpus based features.
Latent Semantic Analy-sis (LSA) (Landauer et al., 1997) is a widely usedcorpus based measure when evaluating text simi-larity.
In this work we use the Vector Space Sen-tence Similarity proposed by (?Saric et al., 2012),which represents each sentence as a single distri-butional vector by summing up the LSA vector ofeach word in the sentence.
Two corpora are usedto compute the LSA vector of words: New YorkTimes Annotated Corpus (NYT) and Wikipedia.Besides, in consideration of different weights fordifferent words, they also calculated the weightedLSA vector for each word.
In addition, we usethe Co-occurrence Retrieval Model (CRM) featurefrom our previous work (Zhu and Lan, 2013) asanother corpus-based feature.
The CRM is calcu-lated based on a notion of substitutability, that is,the more appropriate it is to substitute word w1in place of word w2in a suitable natural languagetask, the more semantically similar they are.
Atlast, 6 corpus based features are extracted.Syntactic features.
Dependency relations of sen-tences often contain semantic information.
In thiswork we follow two syntactic dependency similar-ity features presented in our previous work (Zhuand Lan, 2013), i.e., Simple Dependency Overlapand Special Dependency Overlap.
The Simple De-pendency Overlap measures all dependency rela-tions while the Special Dependency Overlap fea-266ture only focuses on the primary roles extractedfrom several special dependency relations, i.e.,subject, object and predict.Machine Translation based features.
Machinetranslation (MT) evaluation metrics are designedto assess whether the output of a MT system issemantically equivalent to a set of reference trans-lations.
This type of feature has been proved tobe effective in our previous work (Zhu and Lan,2013).
As a result, we extend the original 6 lexicallevel MT metrics to 10 metrics, i.e., WER, TER,PER, BLEU, NIST, ROUGE-L, GTM-1,GTM-2,GTM-3 and METEOR-ex.
All these metrics arecalculated using the Asiya Open Toolkit for Auto-matic Machine Translation (Meta-) Evaluation4.Multi-level text Features.
(Pilehvar et al., 2013)presented a unified approach to semantic similar-ity at multiple levels from word senses to textdocuments through the semantic signature repre-sentation of texts (e.g., sense, word or sentence).Given initial nodes (senses), they performed ran-dom walks on semantic network like WordNet,then the resulting frequency distribution over allnodes in WordNet served as semantic signature ofthe text.
By doing so the similarity of two textscan be computed as the similarity of two seman-tic signatures.
In this work, we borrowed theirsemantic signature method and adopted 3 similar-ity measures to estimate two semantic signatures,i.e., Cosine similarity, Weighted Overlap and Top-k Jaccard (k=250, 500).Other Features.
Besides, other simple surfacefeatures from texts, such as numbers, symbols andlength of texts, are extracted.
Following (?Saric etal., 2012) we adopt relative length difference, rela-tive information content difference, numbers over-lap, case match and stocks match.2.3 Features on Ph-W LevelFor Ph-W level, since word and phrase no longershare the property of sentence, most features usedfor sentence similarity estimation are not applica-ble for this level.
Therefore, we adopt the follow-ing features as the basic feature set for Ph-W level.String features.
This type contains two fea-tures.
The first is a boolean feature which recordswhether the word appears in the phrase.
The sec-ond is the Weighted Word Overlap feature men-tioned in Section 2.2.Knowledge based features.
As described in Sec-4http://nlp.lsi.upc.edu/asiya/tion 2.2, we compute the averaged score and themaximal score between word and phrase using thefour word similarity measures based on WordNet,i.e., Path, WUP, LCH and Lin.Corpus based features.
We adopt the VectorSpace Similarity described in Section 2.2.
Specif-ically, for word the single distributional vector isthe LSA vector of itself.Multi-level text Features.
As described in Sec-tion 2.2, since the semantic signatures are pro-posed for various kinds of texts (e.g., sense, wordor sentence), they serve as one basic feature.Obviously, the above features extracted fromthe phrase-word pair is significantly less than thefeatures used in P-S level and S-Ph level.
This isbecause the information contained in phrase-wordpair is much less than that in sentences and para-graphs.
To overcome this information insufficientproblem, we propose an information enrichmentmethod based on WordNet to extend the initialword in Ph-W level as below.Word Expansion with Definition.
For the wordpart in Ph-W level, we extract its definition interms of its most common concept inWordNet andthen replace the initial word with this definition.This gives a much richer set of initial single word.Since a word may have many senses, not all ofthis word definition expansion are correct.
But weshow below empirically that using this expandedset improves performance.
By doing so we treatthe phrase and the definition of the original wordas two sentences, and thus, all features describedin Section 2.2 are calculated.2.4 Features on W-Se LevelFor W-Se level, the information that a word anda sense carry is less than other levels.
Hence, thebasic features that can be extracted from the origi-nal word-sense pair are even less than Ph-W level.Therefore the basic features we use for W-Se levelare as follows.String features.
Two boolean string featuresare used.
One records whether the word-sensepair shares the same POS tag and another recordswhether the word-sense pair share the same word.Knowledge based features.
As described in Sec-tion 2.2, four knowledge-based word similaritymeasures based on WordNet are calculated.Multi-level text Features.
The multi-level textfeatures are the same as Ph-W level.In consideration of the lack of contextual infor-267mation between word-sense pair, we also proposethree information enrichment methods in order togenerate more effective information for word andsense with the aid of WordNet.Word Expansion with Synonyms.
For the wordpart in W-Se level, we extract its synonyms withthe help of WordNet, then update the valuesof above basic features if its synonyms achievehigher feature value than the original word itself.Sense Expansion with Definition.
For the sensein W-Se level, we directly use its definition inWordNet to enrich its information.
By doing sothe similarity estimation of W-Se level can be con-verted to that of word-phrase level, therefore weuse all basic features for Ph-W level described inSection2.3.Word-Sense Expansion with Definition.
Un-like the above two expansion methods which focusonly on one part of W-Se level, the third method isto enrich information for word and sense togetherby using their definitions in WordNet.
As beforewe extract the word definition in terms of its mostcommon concept in WordNet and then replace theinitial word with this definition.
Then we use allfeatures in Section 2.2.3 Experiment and ResultsWe adopt supervised regression model for eachcross level.
In order to compare the performanceof different regression algorithms, we perform 5-fold cross validation on training data for each crosslevel.
We used several regression algorithms in-cluding Support Vector Regression (SVR) with3 different kernels (i.e., linear, polynomial andrbf), Random Forest, Stochastic Gradient Descent(SGD) and Decision Tree implemented in thescikit-learn toolkit (Pedregosa et al., 2011).
Thesystem performance is evaluated in Pearson corre-lation (r) (official measure) and Spearman?s rankcorrelation (?
).3.1 Results on Training DataTable 1 and Table 2 show the averaged perfor-mance of different regression algorithms in termsof Pearson correlation (r) and Spearman?s rankcorrelation (?)
on the training data of P-S level andS-Ph level using 5-fold cross validation, where thestandard deviation is given in brackets.
The re-sults show that Random Forest performs the bestboth on P-S level and S-Ph level whether in (r) or(?).
We also find that the results of P-S level arebetter than that of S-Ph level, and the reason maybe that paragraph and sentence pair contain moreinformation than the sentence and phrase pair.Regression Algorithm r (%) ?
(%)SVR, ker=rbf 80.70 (?1.47) 79.90 (?1.66)SVR, ker=poly 73.78 (?1.57) 74.41 (?1.89)SVR, ker=linear 80.43 (?1.13) 79.46 (?1.51)Random Forest 80.92 (?1.40) 80.20 (?2.00)SGD 77.61 (?0.76) 77.14 (?1.49)Decision Tree 73.23 (?2.14) 71.84 (?2.55)Table 1: Results of different algorithms using 5-fold cross validation on training data of P-S levelRegression Algorithm r (%) ?
(%)SVR, ker=rbf 66.14 (?5.14) 65.76 (?5.93)SVR, ker=poly 58.93 (?2.29) 63.62 (?4.15)SVR, ker=linear 66.78 (?4.51) 66.34 (?4.90)Random Forest 73.18 (?5.23) 70.30 (?5.51)SGD 63.18 (?3.61) 64.80 (?4.21)Decision Tree 67.66 (?6.76) 66.03 (?6.64)Table 2: Results of different algorithms using 5-fold cross validation on training data of S-Ph levelTable 3 shows the results of different regressionalgorithms and different feature sets in terms ofr and ?
on the training data of Ph-W level us-ing 5-fold cross validation, where the basic fea-tures are denoted as Feature Set A and their com-bination with word definition expansion featuresare denoted as Feature Set B.
The results showthat almost all algorithms performance have beenimproved by using word definition expansion fea-ture except Decision Tree.
This proves the effec-tiveness of the information enrichment method weproposed in this level.
Besides, Random Forestachieves the best performance again with r=44%and ?=41%.
However, in comparison with P-Slevel and S-Ph level, all scores in Table 3 drop alot even with information enrichment method.
Thepossible reason may be two: the reduction of in-formation on Ph-W level and our information en-richment method brings in a certain noise as well.For W-Se level, in order to examine the perfor-mance of different information enrichment meth-ods, we perform experiments on 4 different fea-ture sets from A to D, where feature set A con-tains the basic features, feature set B, C and Dadd one information enrichment method based onformer feature set.
Table 4 and 5 present the rand ?
results of 4 feature sets using different re-gression algorithms.
From Table 4 and 5 we seethat most correlation scores are below 40% and268Regression Algorithm r (%) ?
(%)Feature Set A1Feature Set B2Feature Set A Feature Set BSVR, ker=rbf 34.67 (?4.34) 42.62 (?6.36) 33.26 (?4.24) 40.87 (?6.24)SVR, ker=poly 19.00 (?4.26) 24.06 (?5.55) 21.13 (?4.86) 28.35 (?6.11)SVR, ker=linear 34.87 (?4.65) 41.91 (?2.05) 35.42 (?5.05) 42.69 (?0.55)Random Forest 43.17 (?7.72) 44.00 (?6.88) 40.34 (?5.71) 41.80 (?6.76)SGD 26.20 (?3.37) 38.69 (?4.60) 23.55 (?5.01) 38.00 (?2.64)Decision Tree 39.22 (?7.54) 32.22 (?12.74) 38.90 (?6.03) 31.64 (?10.47)1Feature Set A = basic feature set2Feature Set B = Feature Set A + Word Definition Expansion FeaturesTable 3: Results of different algorithms using 5-fold cross validation on training data of Ph-W levelthe performance of W-Se level is the worst amongall these four levels.
This illustrates that the lessinformation the texts contain, the worse perfor-mance the model achieves.
Again the RandomForest algorithm performs the best among all algo-rithms.
Again almost all information enrichmentfeatures perform better than Feature set A.
This il-lustrates that these information enrichment meth-ods do help to improve performance.
When we ob-serve the three information enrichment methods,we find that feature set C performs the best.
Incomparison with feature set C, feature set B onlyused word synonyms to expand information andthis expansion is quite limited.
Feature set D per-forms better than B but still worse than C. The rea-son may be that when we extend sense with its def-inition, the definition is accurate and exactly repre-sents the meaning of sense.
However since a wordoften contains more than one concepts, and whenwe use the definition of the most common conceptto extend word, such extension may not be correctand the generated information may contain morenoise and/or change the original meaning of word.3.2 Results on Test DataAccording to the experiments on training data, weselect Random Forest as the final regression algo-rithm.
The number of trees in Random Forest n isoptimized to 50 and the rest parameters are set tobe default.
All features in Section 2.2 are used onP-S level, S-Ph level and Ph-W level.
For W-Selevel, we take all features except word-sense def-inition expansion feature which has been shownto impair the system performance.
For each level,all training examples are used to learn the corre-sponding regression model.
According to the offi-cial results released by organizers, Table 6 and Ta-ble 7 list the top 3 systems in terms of r (official)and ?.
Our final systems rank the second both interms of r and ?
and also achieve the second placeon P-S level, S-Ph level and Ph-W level, as wellas the 4th place on W-Se level in terms of officialPearson correlation.Team P-S S-Ph Ph-W W-Se r RankSimCompass 0.811 0.742 0.415 0.356 1ECNU 0.834 0.771 0.315 0.269 2UNAL-NLP 0.837 0.738 0.274 0.256 3Table 6: Pearson Correlation (official) on test dataTeam P-S S-Ph Ph-W W-Se ?
RankSimCompass 0.801 0.728 0.424 0.344 1ECNU 0.821 0.757 0.306 0.263 2UNAL-NLP 0.820 0.710 0.249 0.236 6Table 7: Spearman Correlation on test data4 ConclusionWe build a supervised Random Forest regressionmodel for each cross level.
For P-S and S-Ph level,we adopt the ensemble of heterogeneous similar-ity features, i.e., string features, knowledge basedfeatures, corpus based features, syntactic features,machine translation based features, multi-leveltext features and other features to capture the se-mantic similarity between two texts with distinc-tively different lengths.
For Ph-W and W-Se level,we propose information enrichment methods tolengthen original texts in order to generate moresemantic features, which has been proved to be ef-fective.
Our submitted final systems rank the 2ndout of 18 teams both on Pearson Rank (officialrank) and Spearman Rank, and also rank the sec-ond place on P-S level, S-Ph level and Ph-W level,as well as the 4th place on W-Se level in terms ofPearson correlation.
In future work we will focuson information enrichment methods which bringin more accurate information and less noises.AcknowledgmentsThis research is supported by grants from Na-tional Natural Science Foundation of China269Regression Algorithm Feature Set A1Feature Set B2Feature Set C3Feature Set D4SVR, ker=rbf 29.85 (?7.29) 34.49 (?5.55) 36.80 (?6.46) 22.19 (?6.49)SVR, ker=poly 24.62 (?3.63) 29.27 (?3.53) 26.55 (?1.27) 25.89 (?5.63)SVR, ker=linear 29.58 (?5.88) 34.87 (?3.97) 35.96 (?1.75) 34.57 (?3.75)Random Forest 22.87 (?5.59) 33.97 (?1.78) 40.43 (?3.00) 37.54 (?3.20)SGD 26.32 (?7.31) 27.36 (?6.44) 32.50 (?6.02) 18.00 (?6.13)Decision Tree 23.40 (?5.65) 26.33 (?3.86) 33.64 (?6.97) 31.86 (?3.95)1Feature Set A = basic feature set2Feature Set B = Feature Set A + Synonym Expansion3Feature Set C = Feature Set B + Sense Definition Expansion Features4Feature Set D = Feature Set C + Word-Sense Definition Expansion FeaturesTable 4: Results of different algorithms using 5-fold CV on training data of W-Se level (r (%))Regression Algorithm Feature Set A Feature Set B Feature Set C Feature Set DSVR, ker=rbf 28.41 (?8.99) 29.61 (?6.23) 34.18 (?6.36) 22.90 (?6.78)SVR, ker=poly 23.05 (?7.53) 22.47 (?4.47) 21.63 (?4.37) 25.37 (?7.25)SVR, ker=linear 27.29 (?7.02) 31.79 (?4.00) 34.75 (?3.55) 34.19 (?3.06)Random Forest 19.66 (?6.75) 31.98 (?3.21) 38.57 (?3.60) 37.56 (?3.15)SGD 24.12 (?7.98) 24.62 (?6.36) 29.27 (?5.86) 23.05 (?11.23)Decision Tree 22.30 (?5.25) 25.09 (?3.64) 31.99 (?7.81) 30.51 (?5.27)Table 5: Results of different algorithms using 5-fold CV on training data of W-Se level (?
(%))(No.60903093) and Shanghai Knowledge ServicePlatform Project (No.
ZF1213).ReferencesCarmen Banea, Samer Hassan, Michael Mohler, andRada Mihalcea.
2012.
Unt: A supervised synergis-tic approach to semantic text similarity.
pages 635?642.
First Joint Conference on Lexical and Compu-tational Semantics (*SEM).Daniel B?ar, Chris Biemann, Iryna Gurevych, andTorsten Zesch.
2012.
Ukp: Computing seman-tic textual similarity by combining multiple contentsimilarity measures.
pages 435?440.
First JointConference on Lexical and Computational Seman-tics (*SEM).Michael Heilman and Nitin Madnani.
2012.
Ets:Discriminative edit models for paraphrase scoring.pages 529?535.
First Joint Conference on Lexicaland Computational Semantics (*SEM).Aminul Islam and Diana Inkpen.
2008.
Semantic textsimilarity using corpus-based word similarity andstring similarity.
ACM Transactions on KnowledgeDiscovery from Data (TKDD), 2(2):10.Thomas K Landauer, Darrell Laham, Bob Rehder, andMissy E Schreiner.
1997.
How well can passagemeaning be derived without using word order?
acomparison of latent semantic analysis and humans.In Proceedings of the 19th annual meeting of theCognitive Science Society, pages 412?417.Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context and wordnet similarity for wordsense identification.
WordNet: An electronic lexicaldatabase, 49(2):265?283.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of the 15th in-ternational conference on Machine Learning, vol-ume 1, pages 296?304.
San Francisco.George A Miller.
1995.
Wordnet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.Fabian Pedregosa, Ga?el.
Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and?Edouard Duchesnay.
2011.Scikit-learn: Machine learning in Python.
Journalof Machine Learning Research, 12:2825?2830.Mohammad Taher Pilehvar, David Jurgens, andRoberto Navigli.
2013.
Align, disambiguate andwalk: A unified approach for measuring semanticsimilarity.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(ACL 2013).Frane?Saric, Goran Glava?s, Mladen Karan, Jan?Snajder,and Bojana Dalbelo Ba?sic.
2012.
Takelab: Systemsfor measuring semantic text similarity.
pages 441?448.
First Joint Conference on Lexical and Compu-tational Semantics (*SEM).Zhibiao Wu and Martha Palmer.
1994.
Verbs seman-tics and lexical selection.
In Proceedings of the 32ndannual meeting on Association for ComputationalLinguistics, pages 133?138.
Association for Com-putational Linguistics.Tian Tian Zhu and Man Lan.
2013.
Ecnucs: Measur-ing short text semantic equivalence using multiplesimilarity measurements.
Atlanta, Georgia, USA,page 124.270
