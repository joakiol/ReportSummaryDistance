Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 545?552Manchester, August 2008Comparative Parser Performance Analysis across Grammar Frameworksthrough Automatic Tree Conversion using Synchronous GrammarsTakuya Matsuzaki 1 Jun?ichi Tsujii 1,2,31.
Department of Computer Science, University of Tokyo, Japan2.
School of Computer Science, University of Manchester, UK3.
National Center for Text Mining, UK{matuzaki, tsujii}@is.s.u-tokyo.ac.jpAbstractThis paper presents a methodology for thecomparative performance analysis of theparsers developed for different grammarframeworks.
For such a comparison, weneed a common representation format ofthe parsing results since the representationof the parsing results depends on the gram-mar frameworks; hence they are not di-rectly comparable to each other.
We firstconvert the parsing result to a shallow CFGanalysis by using an automatic tree con-verter based on synchronous grammars.The use of such a shallow representation asa common format has the advantage of re-duced noise introduced by the conversionin comparison with the noise produced bythe conversion to deeper representations.We compared an HPSG parser with sev-eral CFG parsers in our experiment andfound that meaningful differences amongthe parsers?
performance can still be ob-served by such a shallow representation.1 IntroductionRecently, there have been advancement made inthe parsing techniques for large-scale lexicalizedgrammars (Clark and Curran, 2004; Ninomiya etal., 2005; Ninomiya et al, 2007), and it havepresumably been accelerated by the developmentof the semi-automatic acquisition techniques oflarge-scale lexicalized grammars from parsed cor-pora (Hockenmaier and Steedman, 2007; Miyaoc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.et al, 2005).
In many of the studies on lexical-ized grammar parsing, the accuracy of the pars-ing results is evaluated in terms of the accuracy ofthe semantic representations output by the parsers.Since the formalisms for the semantic representa-tion are different across the grammar frameworks,it has been difficult to directly compare the perfor-mance of the parsers developed for different gram-mar frameworks.Several researchers in the field of lexicalizedgrammar parsing have recently started to seek acommon representation of parsing results acrossdifferent grammar frameworks (Clark and Curran,2007; Miyao et al, 2007).
For example, Clarkand Curran (2007) developed a set of mappingrules from the output of a Combinatorial Catego-rial grammar parser to the Grammatical Relations(GR) (Carroll et al, 1998).
They found that themanual development of such mapping rules is not atrivial task; their mapping rules covered only 85%of the GRs in a GR-annotated corpus; i.e., 15% ofthe GRs in the corpus could not be covered by themapping from the gold-standard CCG analyses ofthose sentences.We propose another method for the cross-framework performance analysis of the parserswherein the output of parsers are first convertedto a CFG tree.
Specifically, we use CFG trees ofthe style used in the Penn Treebank (PTB) (Mar-cus et al, 1994), in which the non-terminal labelsare simple phrasal categories (i.e., we do not usefunction-tags, empty nodes, and co-indexing).
Wehereafter name such CFG trees, ?PTB-CFG trees.
?We use an automatic tree converter based on astochastic synchronous grammar in order to makethe PTB-CFG trees from the analyses based on alexicalized grammar.In such a shallow representation, some infor-545mation given by the lexicalized parsers is lost.For instance, long-distance dependency and con-trol/raising distinction cannot be directly repre-sented in the PTB-CFG tree.
From the viewpointof NLP-application developer, the parser evalua-tion based on such a shallow representation maybe not very informative because performance met-rics based on the shallow representation, e.g., la-beled bracketing accuracy, do not serve as a directindicator of the usefulness of the parser in theirapplications.
Nevertheless, we consider the parserperformance analysis based on the shallow repre-sentation is still very useful from the viewpointof parser developers because the accuracy of thestructure of the CFG-trees is, though not an idealone, a good indicator of the parsers?
structural dis-ambiguation performance.In addition, there are at least two advantages inusing the CFG-trees as the common representationfor the evaluation.
The first advantage is that theconversion from the parser?s output to the CFG-trees can be achieved with much higher accuracythan to deeper representations like GRs; we ob-tained a conversion accuracy of around 98% in ourexperiments using an HPSG grammar.
The accu-racy of the conversion is critical in the quantita-tive comparison of parsers that have similar per-formances because the difference in the parsers?ability would soon be masked by the errors intro-duced in the conversion process.
The second ad-vantage is that we can compare the converted out-put directly against the outputs of the well-studiedCFG-parsers derived from PTB.In the experiments, we applied the conversion toan HPSG parser, and compared the results againstseveral CFG parsers.
We found that the parsingaccuracy of the HPSG parser is a few points lowerthan state-of-the-art CFG parsers in terms of thelabeled bracketing accuracy.
By further investi-gating the parsing results, we have identified aportion of the reason for the discrepancy, whichcomes from the difference in the architecture of theparsers.2 BackgroundIn this section, we first give a brief overview ofthe semi-automatic acquisition framework of lex-icalized grammars.
Although our methodology isalso applicable to manually developed grammars,in this paper, we concentrate on the evaluation ofthe parsers developed for lexicalized grammars de-rived from a CFG treebank.
Next, we introducea specific instance of the treebank-derived lexical-ized grammars used in our experiment: the EnjuEnglish HPSG grammar.
Using the Enju grammaras a concrete example, we present the motivationsfor our tree conversion method based on a stochas-tic synchronous grammar.
We also provide a sum-mary of the basic concepts and terminologies ofthe stochastic synchronous grammar.2.1 Semi-automatic Acquisition ofLexicalized GrammarsA lexicalized grammar generally has two compo-nents: a small set of grammar rules and a largeset of lexical items.
The grammar rules representgeneric grammatical constraints while the lexicalitems represent word-specific characteristics.
Ananalysis of a sentence is created by iteratively com-bining lexical items assigned to to the words in thesentence by applying the grammar rules.Several researchers have suggested to extract thelexicon; i.e., the set of lexical items, from a tree-bank such as PTB.
Most of the lexicon acquisitionmethods proceed as follows:1.
Fix the the grammar rules and the basic de-sign of the lexical items.2.
Re-analyse the sentences in terms of the tar-get grammar framework, exploiting the anal-ysis given in the source treebank.
A re-analysis is generally represented as a deriva-tion of the sentence; i.e., a history of rule ap-plications.3.
Find a lexical item for each word in the sen-tences so that it matches the re-analysis of thesentence, and extract it.We used the pairs of the original trees and the re-analyses of the same sentence as a parallel tree-bank, from which we extract a synchronous gram-mar.2.2 The Enju HPSG GrammarWe used the Enju English HPSG grammar (Miyaoet al, 2005) 1 in the experiments.
The design ofthe grammar basically follows the definition in thetext by Pollard and Sag (1994).
A program calledMayz is distributed with the grammar, which was1Version 2.2., publicly available from http://www-tsujii.is.s.u-tokyo.ac.jp/enju546used to make the HPSG treebank (i.e., a set of re-analyses based on the HPSG grammar) from PTB;the lexicon was extracted from the HPSG treebank.We reproduced the HPSG treebank using the pro-gram.An analysis of a sentence in the HPSG for-malism is represented by a phrasal tree, in whicheach node is assigned a data structure calledtyped feature structure (TFS).
The TFS representssyntactic/semantic structures of the correspondingphrase.
To convert an HPSG analysis to a corre-sponding PTB-CFG trees, we first map the TFSs toatomic symbols like PP, NP, NX, etc.
(33 symbolsin total).
We hereafter name such HPSG trees af-ter the TFS-to-symbol mapping, ?simplified HPSGtrees.?
Similarly to the PTB-CFG trees, the simpli-fied HPSG trees do not include empty categories,co-indexing, and function-tags.
However, we can-not attain a PTB-CFG tree by simply mappingthose atomic symbols to the corresponding PTBnon-terminal symbols, because the analyses by thePTB-CFG and the HPSG yield different tree struc-tures for the same sentence.The conversion of the tree structure from HPSGtrees to PTB-CFG trees can be regarded as theinverse-mapping of the transformation from PTBtrees to HPSG trees implemented in the Mayz pro-gram.
A most notable transformation is the bina-rization of the PTB trees; all the branches in theHPSG treebank are unary or binary.
The binariza-tion scheme used in Mayz is similar to the head-centered binarization, which is often used for theextraction of ?Markovised?
PCFGs from the tree-bank.
Mayz identifies the head daughters by usinga modified version of Collins?
head finding rules(Collins, 1999).
It is also notable that the PTB-to-HPSG transformation by Mayz often makes abracketing in the HPSG analyses that crosses withthe original bracketing in the PTB.
Such a trans-formation is used, for instance, to change the at-tachment level of an article to a noun phrase witha post-modifier (Figure 1).The tree transformation by Mayz is achievedby sequentially applying many tree transformationrules to an input PTB tree.
Although each of therules operates on a relatively small region of thetree, the net result can be a very complex transfor-mation.
It is thus very difficult, if not impossible,to invert the transformation programmatically.NPthe NXcat PPon NPthe wallNPNPthe catPPon NPthe wallFigure 1: Different attachment level of the arti-cles: HPSG analysis (left) and PTB-CFG analysis(right).                   Figure 2: An example of synchronous CFG2.3 Stochastic Synchronous Tree-SubstitutionGrammar for Tree ConversionFor the purpose of the inverted transformationof simplified HPSG trees to PTB-CFG trees, weuse a statistical approach based on the stochasticsynchronous grammars.
Stochastic synchronousgrammars are a family of probabilistic models thatgenerate a pair of trees by recursively applyingsynchronous productions, starting with a pair ofinitial symbols.
See e.g., Eisner (2003) for a moreformal definition.
Figure 2 shows an example ofsynchronous CFG, which generates the pairs ofstrings of the form (abmc, cbma).
Each non-terminal symbol on the yields of the synchronousproduction is linked to a non-terminal symbol onthe other rule?s yield.
In the figure, the links arerepresented by subscripts.
A linked pair of the non-terminal symbols is simultaneously expanded byanother synchronous production.The probability of a derivation D of a tree pair?S, T ?
is defined as the product of the probabilityof the pair of initial symbols (i.e., the root nodes ofS and T ), and the probabilities of the synchronousproductions used in the derivation:P (D) = P(?R1, R2?)??t1i,t2i?
?DP(?t1i, t2i?
),where ?R1, R2?
is the pair of the symbols of theroot nodes of S and T , and ?t1i, t2i?
is a syn-chronous production.547fffiffiflffifl !#"ffffifl!#"   ff  fl fl    "   ff  fl fl    "   ff  fl  "   ff  fl  "fl$#%#&'$ffifl$#%&'$ fl$ % & $ fl$ % & $ fl$ % & $ fl$ % & $(ffi)(!(ffi)( ( )(  ( )(  ( )(  ( )( *,+-/./0*1*12 ++3*14ffi5ffi64++78.7*9+2*+-/.#0*+*12++3*14/5ffi64++7ffi8.7*9+!2* +- .
0* 1* 1 2 + + 3* 14 5 6 4* 14 5 6 4+ +7 8.7* + 2+ +7 8.7* + 2* +- .
0* +* 1 2+ + 3* 14 5 6 4* 14 5 6 4+ +7 8.7* + 2+ +7 8.7* + 2Figure 3: An example of synchronous TSG: syn-chronous productions (top) and a synchronousderivation (bottom).Stochastic synchronous grammars have beenused in several machine-translation systems toserve as a model of tree-to-tree translation (e.g.,(Eisner, 2003; Chiang, 2007)).
Our objectiveof automatic conversion between syntactic anal-yses is similar to the tree-to-tree machine trans-lation.
An important difference is that, for ourpurpose, the generated tree pair should have thesame yields since they are two analyses of the samesentence.
We also want the synchronous gram-mar to be able to generate a pair of trees whereinsome constituents in one tree cross with the con-stituents in the other tree; for example, such a tree-transformation is necessary to change the article-attachment levels.We show below that, by means of a simplealgorithm, we can obtain a synchronous tree-substitution grammar (STSG) that meets the abovetwo requirements.
Synchronous productions inSTSGs may include a local tree of depth greaterthan one.
Figure 3 shows a tiny STSG that gener-ates two different analyses for PP-modified NPs.Furthermore, a part of the derivation of the twoanalyses is presented; the dotted-lines indicate ap-plications of the synchronous productions.3 Tree Transformation Based on aStochastic Synchronous Grammar3.1 Extraction of Synchronous GrammarsWe created an STSG from a parallel treebank, inwhich a sentence is assigned with a PTB-CFG treeand a simplified HPSG tree.
The synchronous pro-ductions were obtained by splitting the tree pairs atseveral pairs of non-terminal nodes.
Specifically,for a tree pair (T1, T2) for a sentence s,1.
we select a set of ?common-span node pairs?
{(N1i, N2i)|N1i?
T1, N2i?
T2, i = 1, .
.
.
k},which is a set of pairs of non-terminal nodesthat dominate the same span of s, and then2.
split (T1, T2) at each (N 1i, N2i) for i =1, .
.
.
, k.For tree pairs that do not include unary produc-tions, the above procedure uniquely determinesa collection of pairs of tree-fragments (t1i, t2i) bysplitting the tree pair at all the common-span nodepairs.
For tree pairs including unary productions,we chose the split points {(N 1i, N2i)} so that theyyield a pair of the longest unary chains as a tree-fragment pair when both T1and T2include oneor more unary production dominating a commonspan.
When only T1(or T2) includes one or moreunary productions on a common span, we selectedthe upper-most node of the chain of the unary pro-ductions in T1, and the corresponding node in T2as the split points.A node on the yields of the resulting tree-fragments is either a terminal node or a node ina common-span node pair that is selected as a splitnode.
We make the synchronous productions byplacing links between the common-span node pairson the yields of the tree-fragment pairs.
We canshow that the grammar obtained as above only gen-erates pairs of identical sentences by noting thatthe terminal symbols on the yields of two localtrees in a synchronous production, if any, are al-ways identical.By regarding the division of tree-pairs into thetree-fragment pairs as a synchronous derivation,we get the maximum-likelihood estimates of theprobabilities of the synchronous productions:P (?t1, t2?)
= Count(?t1, t2?
)Count(?root of(t1), root of(t2)?
,where Count(?)
represents the number of the syn-chronous productions or the pair of their root sym-bols observed in the derivations.As is clear from the construction, certain typesof conversions can not be handled properly by theSTSG.
For instance, the apparent conversion rulebetween the right-branching analysis of the listing:(rats, (cows, (tigers, ... (and pigs))...) and left-branching analysis: (...((rats, cows,) tigers,) ... andpigs) can not be represented in its full generality bythe STSG.
We however expect that most of suchcases could be handled by combining the STSG-based conversion with a programmatic conversion.5483.2 Tree Conversion AlgorithmWe can define a conversion function f(S), whichtakes a tree S and returns the converted tree T , as:f(S) = argmaxTP (T |S) = argmaxTP (S, T ),where P (S, T ) is the marginal probability of thetree pair ?S, T ?.
The marginal probability is thesum of the probabilities of all the derivations thatgenerate the tree pair ?S, T ?.
Since the syn-chronous productions in a STSG may include a lo-cal tree of any depth, there are exponentially manyderivations that generate the same tree pair.
Toour knowledge, no polynomial time algorithm isknown for the above optimization problem.We have instead searched the max-probabilitysynchronous derivation of the tree pair of the form?S, ??
and taken the opponent tree as the conversionresult of the input tree S:f(S) = argmaxTargmaxD:deriv.of?S,T ?P (D).We used Eisner?s decoding algorithm (Eisner,2003), which is similar to the Viterbi algorithm forHMMs, to obtain the max-probability derivation.3.3 A Back-off Mechanism for theConversionIn our preliminary experiment, a certain number ofsource trees were not covered by the synchronousgrammar.
For the trees that are not covered by thesynchronous grammar, we used a set of ?back-offrules,?
which are synchronous productions consist-ing of two 1-level local trees:{?X ?
?, Y ?
??
| X ?
N1, ?
?
(N1?
?
)?,Y ?
N2, ?
?
(N2?
?)?
},where N1and N2are the sets of non-terminal sym-bols used in the two treebanks, and ?
is the set ofterminal symbols.
We assign small scores to theback-off rules as:score?X??,Y??
?= ?P (X ??
)?P (Bi|Ai),where ?
is a small constant, P (X ?
?)
is themaximum-likelihood estimate of the PCFG ruleprobability of X ?
?
in the source treebank, and?P (Bi|Ai) is the product of ?re-labeling proba-bilities?
for the pairs of linked non-terminal sym-bols in ?
and ?, defined as:P (B|A) = Count(?A,B?
)?B?Count(?A,B ??)
,where Count(?)
represents the number of common-span node pairs used as the split points.When a source tree is not covered by the origi-nal synchronous productions, we add the back-offrules to the synchronous productions, and searchfor the highest-scored derivation.
The score of aderivation including the back-off rules is defined asthe product of the probabilities of the original syn-chronous productions, and the scores of the back-off rules.
We set the value of ?
to be sufficientlysmall so that the highest-scored derivation includesa minimum number of back-off rules.4 Experiments4.1 Experiment SettingWe compared the performance of an HPSG parserwith several CFG parsers.
The HPSG parser isthe Enju parser (Ninomiya et al, 2007), whichhas been developed for parsing with the EnjuHPSG grammar.
A disambiguation module basedon a discriminative maximum-entropy model isused in the Enju parser.
We compared the Enjuparser with four CFG parsers: Stanford?s lexical-ized parser (Klein and Manning, 2003), Collins?parser (Collins, 1999), Charniak?s parser (Char-niak, 2000), and Charniak and Johnson?s rerank-ing parser (Charniak and Johnson, 2005).
The firstthree parsers are based on treebank PCFGs de-rived from PTB.
The last parser is a combinationof Charniak?s parser and a reranking module basedon a maximum-entropy model.
The Enju parserand Collins?
parser require POS-tagged sentencesas the input.
A POS tagger distributed with theEnju parser was used for the POS-tagging.We used a standard split of PTB for the train-ing/development/test data: sections 02-21 for theextraction of the synchronous grammar, section 22for the development, and section 23 for the evalu-ation of the parsers.
Some of the trees in PTB aremissing in the HPSG treebank because the Enjugrammar does not cover all sentences in PTB.
Sec-tion 23 of the HPSG treebank, which was usedas the gold-standard of the HPSG analyses in theexperiments, thus contains fewer sentences (2,278sentences) than the original PTB section 23 (2,416sentences).
We use a notation, ?section 23?,?
toindicate the portion of PTB section 23 covered bythe Enju grammar.Each parser?s output was evaluated in the fol-lowing three representations:?
PTB-CFG trees: We converted the Enju549parser?s output to PTB-CFG trees by using asynchronous grammar.
For the CFG parsers,we used their output as is.?
Simplified HPSG trees: We converted theCFG parsers?
output to simplified HPSG treesby using another synchronous grammar.
TheEnju parser?s output was mapped to simpli-fied HPSG trees.
This was achieved by sim-ply mapping the TFSs assigned to the non-terminal nodes to atomic symbols.?
Unlabeled word-word dependency: We ex-tracted head-modifier dependencies from thePTB-CFG trees by using Collins?
head find-ing rules.The evaluation in the unlabeled word-word depen-dency is motivated by the expectation that, by con-verting PTB-CFG trees to an even simpler repre-sentation, we can reduce the effect of the noiseintroduced in the conversion of the Enju parser?soutput to the PTB-CFG trees.4.2 Extraction of Synchronous grammarsThe stochastic synchronous grammars used for thetree-to-tree conversion were extracted from a par-allel treebank consisting of the PTB-CFG trees insections 02-21 of PTB and the simplified HPSGtrees mapped from the HPSG treebank created bythe Mayz.
We treated the POS tags, which arecommon to the PTB-CFG grammar and the EnjuHPSG grammar, as the terminal symbols.Although we can use a single synchronousgrammar for the conversion of both directions(i.e., from PTB-CFG trees to simplified HPSGtrees, and the opposite), we used two differentsynchronous grammars to achieve better conver-sion accuracies.
The two synchronous grammarswere created by applying different pre-processingto the parallel treebank.
Specifically, for theHPSG?PTB-CFG direction, 1) the PTB-CFGtrees in the parallel treebank were binarized in sucha way that maximized the number of common-span node pairs;2 2) commas in the PTB-CFG treeswere raised as high as possible, approximating thechange of the position of commas by the Mayzprogram; 3) the POS tags for ?not?
are changedfrom ?RB?
to ?RB-not,?
because in PTB, ?not?is treated differently from other adverbs; 4) base2The non-terminal nodes artificially introduced in the bi-narization process were labeled as ?A?
?, where ?A?
is the labelof the nearest ?non-artificial?
ancestor node.NPs in PTB-CFG trees are marked as ?NP-B.?
ThePTB-CFG trees converted from the Enju parser?soutput were post-processed so that the effect of thepre-processing was removed; i.e., artificially cre-ated non-terminal nodes like ?A??
were removed,?RB-not?
was changed to ?RB?, and ?NP-B?
waschanged to ?NP.
?3For the PTB-CFG?HPSG direction, 1) PTB-CFG trees in the parallel treebank are head-centered-binarized by using Collins?
head find-ing rules; 2) the same pre-processing as for thecomma-raising, ?not?
adverbs, and base NPs wasapplied to the PTB-CFG trees.
The same pre-processing was applied to the CFG parsers?
outputbefore the conversion to simplified HPSG trees.4.3 Accuracy of the Tree ConversionTo evaluate the accuracy of the tree conver-sion, we converted the trees in section 23?
ofthe HPSG/PTB treebank into the other format(target format) and compared the conversion re-sults against the corresponding trees in the gold-standard treebank of the target format.
Table 1presents the result of the evaluation.
With the ex-ception of the last column, the columns list thePARSEVAL metrics of the converted trees.
Thelast column headed ?back-off%?
shows the per-centages of the trees for which the back-off mech-anism described in Section 3 were used.
The ac-curacy of the word-word dependencies extractedfrom the PTB-CFG trees converted from the HPSGtreebank was 98.76%.4On average, a tree in section 23?
of the HPSGtreebank includes 0.89 brackets that cross with thebrackets in the corresponding tree in the PTB-CFGtreebank, and a tree in the PTB-CFG treebank in-cludes 0.78 brackets that cross with the brackets inthe corresponding tree in the HPSG treebank.
Thefigures in the column headed CBs (average numberof crossing brackets) show that most of such cross-ing brackets are ?corrected?
by the conversion.4.4 Comparative Parser Evaluation usingCommon RepresentationsWe measured the labeled bracketing accuracy ofthe parsers?
output in the PTB-CFG tree represen-tation (on section 23), and in the simplified HPSG3The positions of commas were left unchanged, becausewe ignored commas in the evaluation, just as in the standardway of evaluating CFG parsers based on the PARSEVAL met-rics.4Dependency relations involving a punctuation mark asthe modifier were not counted in the evaluation.550Conversion direction LP LR F1CBs 0 CBs ?
2 CBs back-off%HPSG ?
PTB-CFG 98.41 98.08 98.24 0.01 99.34 100.00 2.59PTB-CFG ?
HPSG 97.54 97.45 97.49 0.13 93.99 98.33 10.80Table 1: Accuracy of the tree conversionParser LP LR F1Charniak and Johnson 91.79 90.88 91.33Charniak 89.49 88.78 89.13Collins (model 3) 88.62 88.28 88.45Collins (model 2) 88.48 88.15 88.31Collins (model 1) 87.94 87.51 87.72Stanford lexicalized 86.36 86.47 86.41Enju + tree conv.
87.18 86.47 86.82Table 2: PTB-CFG tree evaluationParser (+ tree conv.)
LP LR F1Charniak and Johnson 91.44 91.31 91.37Charniak 90.07 89.97 90.02Collins (model 3) 88.79 88.38 88.58Collins (model 2) 88.74 88.33 88.53Collins (model 1) 88.62 88.48 88.54Stanford lexicalized 88.04 88.12 88.08Enju 90.79 90.30 90.54Table 3: Simplified HPSG tree Evaluationtree representation (on section 23?).
The resultsare shown in Table 2 and Table 3.
The accu-racy of word-word dependencies extracted fromthe PTB-CFG representation is shown in Table 4.As shown in the previous section, approximately2% of the brackets are wrong and approximately1% of the word-word dependencies are wrong af-ter the conversion in both direction even if a parsergives 100% correct output.
Taking this into con-sideration, we might conclude, utilizing the resultsshown in the three tables, that the performanceof the Enju parser is roughly the same level asCollins?
parser and Stanford?s lexicalized parser.Another notable fact depicted in the tables is thatCharniak and Johnson?s reranking parser outper-forms the Enju parser even in the simplified HPSGtree representation.4.5 Comparative Error AnalysisTo closely examine the difference in the parsers?performances, we conducted a comparative erroranalysis using the word-word dependency repre-sentation of the parsing results.
Specifically, wetested the difference in the ability of resolving spe-cific types of syntactic ambiguities between twoparsers by using McNemar?s test.
To make thesamples for the McNemar?s test, we identified asubset of tokens of type tmin the test set that haveParser accuracyCharniak and Johnson 93.66Charniak 92.50Collins (model 3) 91.15Collins (model 2) 91.12Collins (model 1) 90.66Stanford lexicalized 90.91Enju + tree conv.
90.77Table 4: Unlabeled dependency evaluationModifier Head Enju Charinak p-valuetmtctwX (Z) Y (W)VB ROOT NN 20 (20) 1 (1) 8.57e-5NN VB NN 69 (94) 30 (55) 1.34e-4NN VB ROOT 14 (14) 0 (0) 5.12e-4CD CD IN 0 (0) 11 (11) 2.57e-3CD NN CD 12 (12) 1 (2) 5.55e-3VB VB NN 17 (26) 4 (9) 8.83e-3DT NN VB 10 (15) 1 (4) 1.59e-2DT NN JJ 7 (10) 0 (3) 2.33e-2VB VB MD 7 (9) 0 (1) 2.33e-2TO VB NN 24 (31) 10 (19) 2.58e-2Table 5: Error types with smallest p-valuesa true head of type tcand another word of typetwthat is confusable as the head.
For example, asubset of tokens where (tm, tc, tw) = (preposition,noun, verb) can be used to test the difference inthe parsers?
ability to resolve PP-attachment am-biguities, by extracting the pairs of the predictedhead words for the prepositions from two parsers?output, and using them as the sample set for theMcNemar?s test.
When comparing two parsers Aand B, we approximated such a subset of tokens bycollecting the tokens of type tm, which have truehead of type tc, and for which either A or B pre-dicted a wrong head of type tw.We show the results of the comparison betweenthe Enju parser and the Charinak?s parser in Ta-ble 5.
We used section 22 of PTB for this ex-periment.
The table lists the type of the ambigu-ities (tm, tc, tw) for which the accuracies by thetwo parsers differ with the smallest p-values.
Notethat the POS types tm, tc, and tware determinedfrom the gold-standard POS tags, not the POS tagsgiven by parsers or the POS tagger.
In the table, Xis the number of the tokens of type (tm, tc, tw) forwhich only the Enju parser outputs wrong heads;Z is the total number of wrong predictions of thattype made by the Enju parser; Y and W are de-551fined similarly for the Charinak?s parser.The results indicate, for example, that there isa significant difference between the two parsersin the ability to identify the root of a sentence(the first and third row).
Investigation of the Enjuparser?s output including the root identification er-rors revealed that almost all of the errors of thistype were caused by POS-tagging error; for ex-ample, when the matrix verb of a sentence, like?costs,?
is mistakenly tagged as a noun, the matrixverb is not identified as the root (as in the case withthe first row), and the subject is often mistakenlyidentified as the root (as in the case with the thirdrow).
This weak-point of the Enju parser is a con-sequence of the architecture of the parser, whereinthe POS-tagging phase is completely separatedfrom the parsing phase.
Although this weak-pointhas already been pointed out (Yoshida et al, 2007),we expect that we can identify other reasons for thedifference in the parsers?
performances by investi-gating the other types of errors with small p-valuesas well.5 ConclusionWe have proposed the use of shallow CFG analy-ses as the common representation for the compar-ative performance analysis of parsers based on dif-ferent grammar frameworks.
We have presented amethod to convert the parsers?
output to the shal-low CFG analyses that is based on synchronousgrammars.
The experimental results showed thatour method gave a conversion accuracy of around98% in terms of the labeled bracketing accuracy,which was sufficiently high for extracting a mean-ingful conclusion from a quantitative comparisonof the parsers?
performance.
Furthermore, we con-ducted a comparative analysis of the parsing re-sults represented in word-word dependencies ex-tracted from the shallow CFG analyses.
As a re-sult, we could identified a weak-point of the HPSGparser that comes from the parser?s architecture.AcknowledgementsThis work was partially supported by Grant-in-Aidfor Specially Promoted Research (MEXT, Japan).ReferencesCarroll, J., T. Briscoe, and A. Sanlippo.
1998.
Parserevaluation : A survey and a new proposal.
In In Pro-ceedings First Conference on Linguistic Resources,pages 447?455.Charniak, E. and M. Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.
InProc.
ACL, pages 173?180.Charniak, E. 2000.
A maximum-entropy-inspiredparser.
In Proc.
NAACL, pages 132?139.Chiang, D. 2007.
Hierarchical phrase-based transla-tion.
Comput.
Linguist., 33(2):201?228.Clark, S. and J. R. Curran.
2004.
The importance of su-pertagging for wide-coverage ccg parsing.
In Proc.COLING, pages 282?288.Clark, S. and J. Curran.
2007.
Formalism-independentparser evaluation with ccg and depbank.
In Proc.ACL, pages 248?255.Collins, M. 1999.
Head-driven statistical models fornatural language parsing.
Ph.D. thesis, Universityof Pennsylvania.Eisner, J.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In Proc.
ACL, Com-panion Volume, pages 205?208.Hockenmaier, J. and M. Steedman.
2007.
Ccgbank:A corpus of ccg derivations and dependency struc-tures extracted from the penn treebank.
Computa-tional Linguistics, 33(3):355?396.Klein, D. and C. D. Manning.
2003.
A parsing: fastexact viterbi parse selection.
In Proc.
NAACL, pages40?47.Marcus, M. P., B. Santorini, and M. A. Marcinkiewicz.1994.
Building a large annotated corpus of en-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330.Miyao, Y., T. Ninomiya, and J. Tsujii.
2005.Corpus-oriented grammar development for acquir-ing a Head-driven Phrase Structure Grammar fromthe Penn Treebank.
In Natural Language Processing- IJCNLP 2004, volume 3248 of LNAI, pages 684?693.
Springer-Verlag.Miyao, Y., K. Sagae, and J. Tsujii.
2007.
Towardsframework-independent evaluation of deep linguisticparsers.
In Proc.
GEAF, pages 238?258.Ninomiya, T., Y. Tsuruoka, Y. Miyao, and J. Tsujii.2005.
Efficacy of beam thresholding, unification fil-tering and hybrid parsing in probabilistic hpsg pars-ing.
In Proc.
IWPT, pages 103?114.Ninomiya, T., T. Matsuzaki, Y. Miyao, and J. Tsujii.2007.
A log-linear model with an n-gram refer-ence distribution for accurate hpsg parsing.
In Proc.IWPT, pages 60?68.Pollard, C. and I.
A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press.Yoshida, K., Y. Tsuruoka, Y. Miyao, and J. Tsujii.2007.
Ambiguous part-of-speech tagging for im-proving accuracy and domain portability of syntacticparsers.
In Proc.
IJCAI, pages 1783?1788.552
