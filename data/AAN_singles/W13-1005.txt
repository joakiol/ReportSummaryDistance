Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 32?41,Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational LinguisticsThe (Un)expected Effects of Applying Standard Cleansing Models toHuman Ratings on CompositionalityStephen Roller??
Sabine Schulte im Walde ?
Silke Scheible ?
?Department of Computer Science ?Institut fu?r Maschinelle SprachverarbeitungThe University of Texas at Austin Universita?t Stuttgartroller@cs.utexas.edu {schulte,scheible}@ims.uni-stuttgart.deAbstractHuman ratings are an important source forevaluating computational models that predictcompositionality, but like many data sets ofhuman semantic judgements, are often fraughtwith uncertainty and noise.
However, despitetheir importance, to our knowledge there hasbeen no extensive look at the effects of cleans-ing methods on human rating data.
This paperassesses two standard cleansing approaches ontwo sets of compositionality ratings for Ger-man noun-noun compounds, in their abilityto produce compositionality ratings of higherconsistency, while reducing data quantity.
Wefind (i) that our ratings are highly robustagainst aggressive filtering; (ii) Z-score filter-ing fails to detect unreliable item ratings; and(iii) Minimum Subject Agreement is highlyeffective at detecting unreliable subjects.1 IntroductionCompounds have long been a reoccurring focus ofattention within theoretical, cognitive, and compu-tational linguistics.
Recent manifestations of inter-est in compounds include the Handbook of Com-pounding (Lieber and Stekauer, 2009) on theoreticalperspectives, and a series of workshops1 and spe-cial journal issues with respect to the computationalperspective (Journal of Computer Speech and Lan-guage, 2005; Language Resources and Evaluation,2010; ACM Transactions on Speech and LanguageProcessing, to appear).
Some work has focusedon modeling meaning and compositionality for spe-cific classes, such as particle verbs (McCarthy et al1www.multiword.sourceforge.net2003; Bannard, 2005; Cook and Stevenson, 2006);adjective-noun combinations (Baroni and Zampar-elli, 2010; Boleda et al 2013); and noun-noun com-pounds (Reddy et al 2011b; Reddy et al 2011a).Others have aimed at predicting the compositional-ity of phrases and sentences of arbitrary type andlength, either by focusing on the learning approach(Socher et al 2011); by integrating symbolic mod-els into distributional models (Coecke et al 2011;Grefenstette et al 2013); or by exploring the arith-metic operations to predict compositionality by themeaning of the parts (Widdows, 2008; Mitchell andLapata, 2010).An important resource in evaluating composition-ality has been human compositionality ratings, inwhich human subjects are asked to rate the degree towhich a compound is transparent or opaque.
Trans-parent compounds, such as raincoat, have a meaningwhich is an obvious combination of its constituents,e.g., a raincoat is a coat against the rain.
Opaquecompounds, such as hot dog, have little or no rela-tion to one or more of their constituents: a hot dogneed not be hot, nor is it (hopefully) made of dog.Other words, such as ladybug, are transparent withrespect to just one constituent.
As many words donot fall clearly into one category or the other, sub-jects are typically asked to rate the compositionalityof words or phrases on a scale, and the mean of sev-eral judgements is taken as the gold standard.Like many data sets of human judgements, com-positionality ratings can be fraught with large quan-tities of uncertainty and noise.
For example, partici-pants typically agree on items that are clearly trans-parent or opaque, but will often disagree about the32gray areas in between.
Such uncertainty representsan inherent part of the semantic task and is the majorreason for using the mean ratings of many subjects.Other types of noise, however, are undesirable,and should be eliminated.
In particular, we wishto examine two types of potential noise in our data.The first type of noise (Type I noise: uncertainty),comes from when a subject is unfamiliar or un-certain about particular words, resulting in sporad-ically poor judgements.
The second type of noise(Type II noise: unreliability), occurs when a sub-ject is consistently unreliable or uncooperative.
Thismay happen if the subject misunderstands the task,or if a subject simply wishes to complete the taskas quickly as possible.
Judgements collected viacrowdsourcing are especially prone to this secondkind of noise, when compared to traditional pen-and-paper experiments, since participants aim tomaximize their hourly wage.2In this paper, we apply two standard cleans-ing methods (Ben-Gal, 2005; Maletic and Marcus,2010), that have been used on similar rating data be-fore (Reddy et al 2011b), on two data sets of com-positionality ratings of German noun-noun com-pounds.
We aim to address two main points.
Thefirst is to assess the cleansing approaches in theirability to produce compositionality ratings of higherquality and consistency, while facing a reduction ofdata mass in the cleansing process.
In particular, welook at the effects of removing outlier judgementsresulting from uncertainty (Type I noise) and drop-ping unreliable subjects (Type II noise).
The secondissue is to assess the overall reliability of our tworating data sets: Are they clean enough to be usedas gold standard models in computational linguisticsapproaches?2 Compositionality RatingsOur focus of interest is on German noun-noun com-pounds (see Fleischer and Barz (2012) for a detailedoverview), such as Ahornblatt ?maple leaf?
andFeuerwerk ?fireworks?, and Obstkuchen ?fruit cake?where both the head and the modifier are nouns.We rely on a subset of 244 noun-noun compounds2See Callison-Burch and Dredze (2010) for a collection ofpapers on data collected with AMT.
While the individual ap-proaches deal with noise in individual ways, there is no generalapproach to clean crowdsourcing data.collected by von der Heide and Borgwaldt (2009),who created a set of 450 concrete, depictable Ger-man noun compounds according to four composi-tionality classes (transparent+transparent, transpar-ent+opaque, opaque+transparent, opaque+opaque).We are interested in the degrees of composition-ality of the German noun-noun compounds, i.e., therelation between the meaning of the whole com-pound (e.g., Feuerwerk) and the meaning of its con-stituents (e.g., Feuer ?fire?
and Werk ?opus?).
Wework with two data sets of compositionality rat-ings for the compounds.
The first data set, theindividual compositionality ratings, consists ofparticipants rating the compositionality of a com-pound with respect to each of the individual con-stituents.
These judgements were collected withina traditional controlled, pen-and-paper setting.
Foreach compound-constituent pair, 30 native Germanspeakers rated the compositionality of the com-pound with respect to its constituent on a scalefrom 1 (opaque/non-compositional) to 7 (transpar-ent/compositional).
The subjects were allowed toomit ratings for unfamiliar words, but very few did;of the 14,640 possible ratings judgements, only 111were left blank.
Table 1 gives several examples ofsuch ratings.
We can see that Fliegenpilz ?toadstool?is an example of a very opaque (non-compositional)word with respect to Fliege ?housefly/bow tie?
; it haslittle to do with either houseflies or bow ties.
Onthe other hand Teetasse ?teacup?
is highly composi-tional: it is a Tasse ?cup?
intended for Tee ?tea?.The second data set, the whole compositional-ity ratings consists of participants giving a singlerating for the entire compound.
These ratings, pre-viously unpublished, reflect a very different viewof the same compounds.
Rather than rating com-pounds with respect to their constituents, subjectswere asked to give a single rating for the entire com-pound using the same 1-7 scale as before.
The rat-ings were collected via Amazon Mechanical Turk(AMT).
The data was controlled for spammers byremoving subjects who failed to identify a numberof fake words.
Subjects who rated less than 10 com-pounds or had a low AMT reputation were also re-moved.
The resulting data represents 150 differ-ent subjects with roughly 30 ratings per compound.Most participants rated only a few dozen items.
Wecan see examples of these ratings in Table 2.33Compound W.R.T.
Subject 1 Subject 2 Subject 3 Subject 4 Mean Comb.Fliegenpilz ?toadstool?
Fliege ?housefly/bow tie?
3 1 1 2 1.753.37Fliegenpilz ?toadstool?
Pilz ?mushroom?
5 7 7 7 6.50Sonnenblume ?sunflower?
Sonne ?sun?
4 3 1 2 2.504.11Sonnenblume ?sunflower?
Blume ?flower?
7 7 7 6 6.75Teetasse ?teacup?
Tee ?tea?
6 6 4 2 4.504.50Teetasse ?teacup?
Tasse ?cup?
7 6 4 1 4.50Table 1: Sample compositionality ratings for three compounds with respect to their constituents.
We list the mean rat-ing for only these 4 subjects to facilitate examples.
The Combined column is the geometric mean of both constituents.Compound Subject 1 Subject 2 Subject 3 Subject 4 MeanFliegenpilz ?toadstool?
- 2 1 2 2.67Sonnenblume ?sunflower?
3 3 1 2 2.75Teetasse ?teacup?
7 7 7 6 6.75Table 2: Example whole compositionality ratings for three compounds.
Note that Subject 1 chose not to rate Fliegen-pilz, so the mean is computed using only the three available judgements.3 MethodologyIn order to check on the reliability of composition-ality judgements in general terms as well as with re-gard to our two specific collections, we applied twostandard cleansing approaches3 to our rating data: Z-score filtering is a method for filtering Type I noise,such as random guesses made by individuals when aword is unfamiliar.
Minimum Subject Agreement isa method for filtering out Type II noise, such as sub-jects who seem to misunderstand the rating task orrarely agree with the rest of the population.
We thenevaluated the original vs. cleaned data by one intrin-sic and one extrinsic task.
Section 3.1 presents thetwo evaluations and the unadulterated, baseline mea-sures for our experiments.
Sections 3.2.1 and 3.2.2describe the cleansing experiments and results.3.1 Evaluations and BaselinesFor evaluating the cleansing methods, we proposetwo metrics, an intrinsic and an extrinsic measure.3.1.1 Intrinsic Evaluation:Consistency between Rating Data SetsThe intrinsic evaluation measures the consistencybetween our two ratings sets individual and whole.Assuming that the compositionality ratings for acompound depend heavily on both constituents, weexpect a strong correlation between the two datasets.
For a compound to be rated transparent as a3See Ben-Gal (2005) or Maletic and Marcus (2010) foroverviews of standard cleansing approaches.whole, it should be transparent with respect to bothof its constituents.
Compounds which are highlytransparent with respect to only one of their con-stituents should be penalized appropriately.In order to compute a correlation between thewhole ratings (which consist of one average ratingper compound) and the individual ratings (whichconsist of two average ratings per compound, one foreach constituent), we need to combine the individualratings to arrive at a single value.
We use the geo-metric mean to combine the ratings, which is effec-tively identical to the multiplicative methods in Wid-dows (2008), Mitchell and Lapata (2010) and Reddyet al(2011b).
4 For example, using our means listedin Table 1, we may compute the combined rating forSonnenblume as?6.75 ?
2.50 ?
4.11.
These com-bined ratings are computed for all compounds, aslisted in the ?Comb.?
column of Table 1.
We thencompute our consistency measure as the Spearman?s?
rank correlation between these combined individ-ual ratings with the whole ratings (?Mean?
in Table2).
The original, unadulterated data sets have a con-sistency measure of 0.786, indicating that, despitethe very different collection methodologies, the tworatings sets largely agree.3.1.2 Extrinsic Evaluation:Correlation with Association NormsThe extrinsic evaluation compares the consistency4We also tried the arithmetic mean, but the multiplicativemethod always performs better.34Word Example AssociationsFliegenpilz ?toadstool?
giftig ?poisonous?, rot ?red?, Wald ?forest?Fliege ?housefly/bow tie?
nervig ?annoying?, summen ?to buzz?, Insekt ?insect?Pilz ?mushroom?
Wald ?forest?, giftig ?poisonous?, sammeln ?to gather?Sonnenblume ?sunflower?
gelb ?yellow?, Sommer ?summer?, Kerne ?seeds?Sonne ?sun?
Sommer ?summer?, warm ?warm?, hell ?bright?Blume ?flower?
Wiese ?meadow?, Duft ?smell?, Rose ?rose?Table 3: Example association norms for two German compounds and their constituents.between our two rating sets individual and wholewith evidence from a large collection of associa-tion norms.
Association norms have a long traditionin psycholinguistic research to investigate semanticmemory, making use of the implicit notion that asso-ciates reflect meaning components of words (Deese,1965; Miller, 1969; Clark, 1971; Nelson et al 1998;Nelson et al 2000; McNamara, 2005; de Deyne andStorms, 2008).
They are collected by presenting astimulus word to a subject and collecting the firstwords that come to mind.We rely on association norms that were collectedfor our compounds and constituents via both a largescale web experiment and Amazon Mechanical Turk(Schulte im Walde et al 2012) (unpublished).
Theresulting combined data set contains 85,049/34,560stimulus-association tokens/types for the compoundand constituent stimuli.
Table 3 gives examples ofassociations from the data set for some stimuli.The guiding intuition behind comparing our rat-ing data sets with association norms is that a com-pound which is compositional with respect to a con-stituent should have similar associations as its con-stituent (Schulte im Walde et al 2012).To measure the correlation of the rating data withthe association norms, we first compute the Jac-card similarity that measures the overlap in two sets,ranging from 0 (perfectly dissimilar) to 1 (perfectlysimilar).
The Jaccard is defined for two sets, A andB, asJ(A,B) =|A ?B||A ?B|.For example, we can use Table 3 to compute theJaccard similarity between Sonnenblume and Sonne:|{Sommer}||{gelb, Sommer,Kerne,warm, hell}|= 0.20.After computing the Jaccard similarity betweenall compounds and constituents across the associ-ation norms, we correlate this association overlapwith the average individual ratings (i.e., column?Mean?
in Table 1) using Spearman?s ?.
This cor-relation ?Assoc Norm (Indiv)?
reaches ?
= 0.638for our original data.
We also compute a combinedJaccard similarity using the geometric mean, e.g.
?J(Fliegenpilz, F liege) ?
J(Fliegenpilz, P ilz),and calculate Spearman?s ?
with the whole ratings(i.e., column ?Mean?
in Table 2).
This correlation?Assoc Norm (Whole)?
reaches ?
= 0.469 for ouroriginal data.3.2 Data CleansingWe applied the two standard cleansing approaches,Z-score Filtering and Minimum Subject Agreement,to our rating data, and evaluated the results.3.2.1 Z-score FilteringZ-score filtering is a method to filter out Type Inoise, such as random guesses made by individu-als when a word is unfamiliar.
It makes the sim-ple assumption that each item?s ratings should beroughly normally distributed around the ?true?
rat-ing of the item, and throws out all outliers whichare more than z?
standard deviations from the item?smean.
With regard to our compositionality ratings,for each item i (i.e., a compound in the whole data,or a compound?constituent pair in the individualdata) we compute the mean x?i and standard devia-tion ?i of the ratings for the given item.
We thenremove all values from xi where|xi ?
x?i| > ?iz?,with the parameter z?
indicating the maximum al-lowed Z-score of the item?s ratings.
For example, ifa particular item has ratings of xi = (1, 2, 1, 6, 1, 1),then the mean x?i = 2 and the standard deviation35llllllllllllllllllllllllllllllllll0.720.730.740.750.760.770.780.790.80N/A 4.0 3.0 2.0 1.0Maximum Z?score of JudgementsConsistency between ratings(Spearman's rho)l l lCleaned Indiv Cleaned Whole Cleaned Indiv & Whole(a) Intrinsic Evaluation of Z?score Filteringlllllllllllllll lllllllllllll0.400.450.500.550.600.65N/A 4.0 3.0 2.0 1.0Maximum Z?score of JudgementsCorrelationwith AssociationNormOverlap(Spearman's rho)l lAssoc Norms (Indiv) Assoc Norms (Whole)(b) Extrinsic Evaluation of Z?score FilteringFigure 1: Intrinsic and Extrinsic evaluation of Z-score fil-tering.
We see that Z-score filtering makes a minimal dif-ference when filtering is strict, and is slightly detrimentalwith more aggressive filtering.
?i = 2.
If we use a z?
of 1, then we would filter rat-ings outside of the range [2?
1 ?
2, 2 + 1 ?
2].
Thus,the resulting new xi would be (1, 2, 1, 1, 1) and thenew mean x?i would be 1.2.Filtering Outliers Figure 1a shows the results forthe intrinsic evaluation of Z-score filtering.
Thesolid black line represents the consistency of the fil-tered individual ratings with the unadulterated wholeratings.
The dotted orange line shows the consis-tency of the filtered whole ratings with the unadul-terated individual ratings, and the dashed purple lineshows the consistency between the data sets whenboth are filtered.
In comparison, the consistency be-tween the unadulterated data sets is provided by thehorizontal gray line.
We see that Z-score filteringoverall has a minimal effect on the consistency ofllllllllllllllll0.00.10.20.30.40.50.60.70.80.91.0N/A 4.0 3.0 2.0 1.0Maximum Z?score of JudgementsFraction DataRetainedl l lIndiv Whole BothData Retention with Z?score FilteringFigure 2: The data retention rate of Z-score filtering.
Dataretention drops rapidly with aggressive filtering.the two data sets.
It provides very small improve-ments with high Z-scores, but is slightly detrimentalat more aggressive levels.Figure 1b shows the effects of Z-score filteringwith our extrinsic evaluation of correlation with as-sociation norms.
At all levels of filtering, we see thatcorrelation with association norms remains mostlyindependent of the level of filtering.An important factor to consider when evaluatingthese results is the amount of data dropped at eachof the filtering levels.
Figure 2 shows the data re-tention rate for the different data sets and levels.
Asexpected, more aggressive filtering results in a sub-stantially lower data retention rate.
Comparing thiscurve to the consistency ratings gives a clear picture:the decrease in consistency is probably mostly due tothe decrease in available data but not due to filteringoutliers.
As such, we believe that Z-score filteringdoes not substantially improve data quality, but maybe safely applied with a conservative maximum al-lowed Z-score.Filtering Artificial Noise Z-score filtering has lit-tle impact on the consistency of the data, but wewould like to determine whether this is due becauseour data being very clean, so the filtering does notapply, or Z-score filtering not being able to detect theType I noise.
To test these two possibilities, we arti-ficially introduce noise into our data sets: we create100 variations of the original ratings matrices, wherewith 0.25 probability, each entry in the matrix was36l l l l l l l ll ll ll l l ll ll0.650.700.750.80N/A 4.0 3.0 2.0 1.0Maximum Z?score of JudgementsConsistency between ratings(Spearman's rho)l lCleaned Indiv Noisy Indiv(a) Removing Indiv Judgements with Uniform Noisel ll ll ll ll ll ll ll l ll ll ll llll0.650.700.750.80N/A 4.0 3.0 2.0 1.0Maximum Z?score of JudgementsConsistency between ratings(Spearman's rho)l lCleaned Whole Noisy Whole(b) Removing Whole Judgements with Uniform NoiseFigure 3: Ability of Z-score filtering at removing artificial noise added in the (a) individual and (b) whole judgements.The orange lines represent the consistency of the data with the noise, but no filtering, while the black lines indicatethe consistency after Z-score filtering.
Z-score filtering appears to be unable to find uniform random noise in eithersituation.l l l l l lllll ll lllll l ll0.720.730.740.750.760.770.780.790.800.1 0.2 0.3 0.4 0.5 0.6Minimum Subject?Average Correlation(Spearman's rho)Consistency between ratings(Spearman's rho)l l lCleaned Indiv Cleaned Whole Cleaned Indiv & Whole(a) Intrinsic Evaluation of MSA Filteringl l l l l l l l llll l l l l l l l l l l0.400.450.500.550.600.650.1 0.2 0.3 0.4 0.5 0.6Minimum Subject?Average Correlation(Spearman's rho)Correlationwith AssociationNormOverlap(Spearman's rho)l lAssoc Norms (Indiv) Assoc Norms (Whole)(b) Extrinsic Evaluation of MSA FilteringFigure 4: Intrinsic and Extrinsic evaluation of Minimum Subject Agreement filtering.
We see virtually no gains usingsubject filtering, and the individual judgements are quite hindered by aggressive filtering.37replaced with a uniform random integer between 1and 7.
That is, roughly 1 in 4 of the entries in theoriginal matrix were replaced with random, uniformnoise.
We then apply Z-score filtering on each ofthese noisy matrices and report their average con-sistency with its companion, unadulterated matrix.That is, we add noise to the individual ratings ma-trix, and then compare its consistency with the orig-inal whole ratings matrix, and vice versa.
Thus if weare able to detect and remove the artificial noise, weshould see higher consistencies in the filtered matrixover the noisy matrix.Figure 3 shows the results of adding noise to theoriginal data sets.
The lines indicate the averagesover all 100 matrix variations, while the shaded ar-eas represent the 95% confidence intervals.
Surpris-ingly, even though 1/4 entries in the matrix were re-placed with random values, the decrease in consis-tency is relatively low in both settings.
This likelyindicates our data already has high variance.
Fur-thermore, in both settings, we do not see any in-crease in consistency from Z-score filtering.
Wemust conclude that Z-score appears ineffective at re-moving Type I noise in compositionality ratings.We also tried introducing artificial noise in a sec-ond way, where judgements were not replaced with auniformly random value, but a fixed offset of either+3 or -3, e.g., 4?s became either 1?s or 7?s.
Again,the values were changed with probability of 0.25.The results were remarkably similar, so we do notinclude them here.3.2.2 Minimum Subject AgreementMinimum Subject Agreement is a method for fil-tering out subjects who seem to misunderstand therating task or rarely agree with the rest of the pop-ulation.
For each subject in our data, we computethe average ratings for each item excluding the sub-ject.
The subject?s rank agreement with the exclu-sive averages is computed using Spearman?s ?.
Wecan then remove subjects whose rank agreement isbelow a threshold, or remove the n subjects with thelowest rank agreement.Filtering Unreliable Subjects Figure 4 shows theeffect of subject filtering on our intrinsic and extrin-sic evaluations.
We can see that mandating mini-mum subject agreement has a strong, negative im-llllllllllllllllllllllllllllllllllllllllll0.20.30.40.50.60.70.80 5 10 15 20 25Number of Subjects Randomized/RemovedConsistency between ratings(Spearman's rho)l lCleaned Indiv Noisy Indiv(a) Removing Indiv Subjects with Artificial Noiselllllllllllllllllllllllllllllllllllllllllllll0.650.700.750.800 5 10 15 20 25Number of Subjects Randomized/RemovedConsistency between ratings(Spearman's rho)l lCleaned Whole Noisy Whole(b) Removing Whole Subjects with Artificial NoiseFigure 5: Ability of subject filtering at detecting highlydeviant subjects.
We see that artificial noise stronglyhurts the quality of the individual judgements, while hav-ing a much weaker effect on the whole judgements.
Theprocess is effective at identifying deviants in both set-tings.pact on the individual ratings after a certain thresh-old is reached, but virtually no effect on the wholeratings.
When we consider the corresponding dataretention curve in Figure 6, the result is not surpris-ing: the dip in performance for the individual ratingscomes with a data retention rate of roughly 25%.
Inthis way, it?s actually surprising that it does so well:with only 25% of the original data, consistency isonly 5 points lower.
The effects are more dramaticin the extrinsic evaluation.On the other hand, subject filtering has almost noeffect on the whole ratings.
This is not surprising, asmost subjects have only rated at most a few dozenitems, so removing subjects corresponds to a smallerreduction in data, as seen in Figure 6.
Furthermore,the subjects with the highest deviations tend to be38l l l l l l l lllll l l l l l l l l lll l llll0.00.10.20.30.40.50.60.70.80.91.00.1 0.2 0.3 0.4 0.5 0.6Minimum Subject?Average Correlation(Spearman's rho)Fraction DataRetainedl l lIndiv Whole BothData Retention with MSA FilteringFigure 6: Data retention rates for various levels of mini-mum subject agreement.
The whole ratings remain rela-tively untouched by mandating high levels of agreement,but individual ratings are aggressively filtered after a sin-gle breaking point.the subjects who rated the fewest items since theiragreement is more sensitive to small changes.
Assuch, the subjects removed tend to be the subjectswith the least influence on the data set.Removing Artificial Subject-level Noise To testthe hypothesis that minimum subject agreement fil-tering is effective at removing Type II noise, we in-troduce artificial noise at the subject level.
For theseexperiments, we create 100 variations of our ma-trices where n subjects have all of their ratings re-placed with random, uniform ratings.
We then applysubject-level filtering where we remove the n sub-jects who agree least with the overall averages.Figure 5a shows the ability of detecting Type IInoise in the individual ratings.
The results are un-surprising, but encouraging.
We see that increasingthe number of randomized subjects rapidly lowersthe consistency with the whole ratings.
However, thecleaned whole ratings matrix maintains a fairly highconsistency, indicating that we are doing a nearlyperfect job at identifying the noisy individuals.Figure 5b shows the ability of detecting Type IInoise in the whole ratings.
Again, we see that thecleaned noisy ratings have a higher consistency thanthe noisy ratings, indicating the efficacy of subjectagreement filtering at detecting unreliable subjects.The effect is less pronounced in the whole ratingsthan the individual ratings due to the lower propor-tion of subjects being randomized.Identification of Spammers Removing subjectswith the least agreement lends itself to another sortof evaluation: predicting subjects rejected duringdata collection.
As discussed in Section 2, subjectswho failed to identify the fake words or had an over-all low reputability were filtered from the data beforeany analysis.
To test the quality of minimum sub-ject agreement, we reconstructed the data set wherethese previously rejected users were included, ratherthan removed.
Subjects who rated fewer than 10items were still excluded.The resulting data set had a total of 242 users: 150(62.0%) which were included in the original data,and 92 (38.0%) which were originally rejected.
Af-ter constructing the modified data set, we sorted thesubjects by their agreement.
Of the 92 subjects withthe lowest agreement, 75 of them were rejected inthe original data set (81.5%).
Of the 150 subjectswith the highest agreement, only 17 of them wererejected from the original data set (11.3%).
The typ-ical precision-recall tradeoff obviously applies.Curiously, we note that the minimum subjectagreement at this 92nd subject was 0.457.
Compar-ing with the curves for the individual ratings in Fig-ures 4a and 6, we see this is the point where intrinsicconsistency and data retention both begin droppingrapidly.
While this may be a happy coincidence, itdoes seem to suggest that the ideal minimum sub-ject agreement is roughly where the data retentionrate starts rapidly turning.Regardless, we can definitely say that minimumsubject agreement is a highly effective way of root-ing out spammers and unreliable participants.4 ConclusionIn this paper, we have performed a thorough anal-ysis of two sets of compositionality ratings to Ger-man noun-noun compounds, and assessed their reli-ability from several perspectives.
We conclude thatasking for ratings of compositionality of compoundwords is reasonable and that such judgements arenotably reliable and robust.
Even when composi-tionality ratings are collected in two very differentsettings (laboratory vs. AMT) and with different dy-namics, the produced ratings are highly consistent.This is shown by the high initial correlation of thetwo sets of compositionality ratings.
We believe this39provides strong evidence that human judgements ofcompositionality, or at least these particular datasets, are reasonable as gold standards for other com-putational linguistic tasks.We also find that such ratings can be highly ro-bust against large amounts of data loss, as in thecase of aggressive Z-score and minimum subjectagreement filtering: despite data retention rates of10-70%, consistency between our data sets neverdropped more than 6 points.
In addition, we find thatthe correlation between compositionality ratings andassociation norms is substantial, but generally muchlower and less sensitive than internal consistency.We generally find Type I noise to be very diffi-cult to detect, and Z-score filtering is mostly inef-fective at eliminating unreliable item ratings.
Thisis confirmed by both our natural and artificial exper-iments.
At the same time, Z-score filtering seemsfairly harmless at conservative levels, and probablycan be safely applied in moderation with discretion.On the other hand, we have confirmed that mini-mum subject agreement is highly effective at filter-ing out incompetent and unreliable subjects, as evi-denced by both our artificial and spammer detectionexperiments.
We conclude that, as we have definedit, Type II noise is easily detected, and removing thisnoise produces much higher quality data.
We recom-mend using subject agreement as a first-pass identi-fier of likely unreliable subjects in need of manualreview.We would also like to explore other types ofcompounds, such as adjective-noun compounds (e.g.Gro?eltern ?grandparents?
), and compounds withmore than two constituents (e.g.
Bleistiftspitzma-chine ?automatic pencil sharpener?
).AcknowledgmentsWe thank the SemRel group, Alexander Fraser, andthe reviewers for helpful comments and feedback.The authors acknowledge the Texas Advanced Com-puting Center (TACC) for providing grid resourcesthat have contributed to these results.55http://www.tacc.utexas.eduReferencesCollin Bannard.
2005.
Learning about the Meaning ofVerb?Particle Constructions from Corpora.
ComputerSpeech and Language, 19:467?478.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages1183?1193, Cambridge, MA, October.Irad Ben-Gal.
2005.
Outlier detection.
In O. Maimonand L. Rockach, editors, Data Mining and KnowledgeDiscobery Handbook: A Complete Guide for Practi-tioners and Researchers.
Kluwer Academic Publish-ers.Gemma Boleda, Marco Baroni, Nghia The Pham, andLouise McNally.
2013.
On adjective-noun compo-sition in distributional semantics.
In Proceedings ofthe 10th International Conference on ComputationalSemantics, Potsdam, Germany.Chris Callison-Burch and Mark Dredze, editors.
2010.Proceedings of the NAACL/HLT Workshop on Creat-ing Speech and Language Data with Amazon?s Me-chanical Turk, Los Angeles, California.Herbert H. Clark.
1971.
Word Associations and Lin-guistic Theory.
In John Lyons, editor, New Horizon inLinguistics, chapter 15, pages 271?286.
Penguin.Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.2011.
Mathematical foundations for a compositionaldistributional model of meaning.
Linguistic Analysis,36(1-4):345?384.Paul Cook and Suzanne Stevenson.
2006.
ClassifyingParticle Semantics in English Verb-Particle Construc-tions.
In Proceedings of the ACL/COLING Workshopon Multiword Expressions: Identifying and ExploitingUnderlying Properties, Sydney, Australia.Simon de Deyne and Gert Storms.
2008.
Word associ-ations: Norms for 1,424 dutch words in a continuoustask.
Behavior Research Methods, 40(1):198?205.James Deese.
1965.
The Structure of Associations inLanguage and Thought.
The John Hopkins Press, Bal-timore, MD.Wolfgang Fleischer and Irmhild Barz.
2012.
Wortbil-dung der deutschen Gegenwartssprache.
de Gruyter.Edward Grefenstette, G. Dinu, Y. Zhang, MeernooshSadrzadeh, and Marco Baroni.
2013.
Multi-step re-gression learning for compositional distributional se-mantics.
In Proceedings of the 10th InternationalConference on Computational Semantics, Potsdam,Germany.Rochelle Lieber and Pavol Stekauer, editors.
2009.
TheOxford Handbook of Compounding.
Oxford Univer-sity Press.40Jonathan I. Maletic and Adrian Marcus.
2010.
Datacleansing: A prelude to knowledge discovery.
InO.
Maimon and L. Rokach, editors, Data Miningand Knowledge Discovery Handbook.
Springer Sci-ence and Business Media, 2 edition.Diana McCarthy, Bill Keller, and John Carroll.
2003.Detecting a Continuum of Compositionality in PhrasalVerbs.
In Proceedings of the ACL-SIGLEX Workshopon Multiword Expressions: Analysis, Acquisition andTreatment, Sapporo, Japan.Timothy P. McNamara.
2005.
Semantic Priming: Per-spectives from Memory and Word Recognition.
Psy-chology Press, New York.George Miller.
1969.
The Organization of Lexical Mem-ory: Are Word Associations sufficient?
In George A.Talland and Nancy C. Waugh, editors, The Pathol-ogy of Memory, pages 223?237.
Academic Press, NewYork.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin Distributional Models of Semantics.
Cognitive Sci-ence, 34:1388?1429.Douglas L. Nelson, Cathy L. McEvoy, and Thomas A.Schreiber.
1998.
The University of South FloridaWord Association, Rhyme, and Word FragmentNorms.Douglas L. Nelson, Cathy L. McEvoy, and Simon Den-nis.
2000.
What is Free Association and What does itMeasure?
Memory and Cognition, 28:887?899.Siva Reddy, Ioannis P. Klapaftis, Diana McCarthy, andSuresh Manandhar.
2011a.
Dynamic and Static Pro-totype Vectors for Semantic Composition.
In Pro-ceedings of the 5th International Joint Conference onNatural Language Processing, pages 705?713, ChiangMai, Thailand.Siva Reddy, Diana McCarthy, and Suresh Manandhar.2011b.
An Empirical Study on Compositionality inCompound Nouns.
In Proceedings of the 5th Interna-tional Joint Conference on Natural Language Process-ing, pages 210?218, Chiang Mai, Thailand.Sabine Schulte im Walde, Susanne Borgwaldt, andRonny Jauch.
2012.
Association Norms of GermanNoun Compounds.
In Proceedings of the 8th Interna-tional Conference on Language Resources and Evalu-ation, pages 632?639, Istanbul, Turkey.Richard Socher, Eric H. Huang, Jeffrey Pennington, An-drew Y. Ng, and Christopher D. Manning.
2011.
Dy-namic Pooling and Unfolding Recursive Autoencodersfor Paraphrase Detection.
In Advances in Neural In-formation Processing Systems 24.Claudia von der Heide and Susanne Borgwaldt.
2009.Assoziationen zu Unter-, Basis- und Oberbegriffen.Eine explorative Studie.
In Proceedings of the 9thNorddeutsches Linguistisches Kolloquium, pages 51?74.Dominic Widdows.
2008.
Semantic Vector Products:Some Initial Investigations.
In Proceedings of the 2ndConference on Quantum Interaction, Oxford, UK.41
