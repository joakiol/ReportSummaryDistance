HIDDEN UNDERSTANDING MODELSOF NATURAL LANGUAGEScott MillerCollege o f  Computer ScienceNortheastern UniversityBoston, MA 02115millers@ccs.neu.eduRobert Bobrow, Robert Ingria,Richard SchwartzBBN Systems and Technologies70 Fawcett St.,Cambridge, MA 02138rusty, ingria, schwartz@BBN.COMAbstractWe describe and evaluate hidden understanding models, astatistical learning approach to natural languageunderstanding.
Given a string of words, hiddenunderstanding models determine the most likely meaning forthe string.
We discuss 1) the problem of representingmeaning in this framework, 2) the structure of the statisticalmodel, 3) the process of training the model, and 4) theprocess of understanding using the model.
Finally, we giveexperimental results, including results on an ARPAevaluation.1 IntroductionHidden understanding models are an innovative class ofstatistical mechanisms that, given a string of words,determines the most likely meaning for the string.
Theoverall approach represents a substantial departure fromtraditional techniques by replacing hand-crafted grammarsand rules with statistical models that are automaticallylearned from examples.
Hidden understanding models wereprimarily motivated by techniques that have been extremelysuccessful in speech recognition, especially hidden Markovmodels \[Baum, 72\].
Related techniques have previouslybeen applied to the problem of identifying conceptsequences within a sentence \[Pieraccini et aL, 91\].
Inaddition, the approach contains elements of other naturallanguage processing techniques including semanticgrammars \[Waltz, 78; Hen&ix, 78\], augmented transitionnetworks (ATNs) \[Woods, 70\], probabilistic parsing\[Fujisaki et al, 89; Chitrao and Grishman, 90; Seneff, 92\],and automatic grammar induction \[Pereira and Schabes, 92\].Hidden understanding models are capable of learning avariety of meaning representations, ranging from simpledomain-specific representations, to ones at a level of detailand sophistication comparable to current natural anguagesystems.
In fact, a hidden understanding model can be usedto produce a representation with essentially the sameinformation content as the semantic graph used by theDelphi system \[-Bobrow et al, 90\], a general purpose NLPsystem, which utilizes a modified Definite Clause Grammarformalism.
This fact made it possible to interface ahiddenunderstanding system to the discourse processing and data-base retrieval components of Delphi to produce a complete"end to end" system.
This hybrid system participated in the1993 ATIS natural language valuation.
Although only fourmonths old, the scores achieved by the combined systemwere quite respectable.Because of differences between language understanding andspeech recognition, significant changes are required in thehidden Markov model methodology.
Unlike speech, whereeach phoneme results in a local sequence of spectra, therelation between the meaning of a sentence and the sequenceof words is not a simple linear sequential model.
Languageis inherently nested, with subgroups of concepts withinother concepts.A statistical system for understanding language must takethis and other differences into account in its overall design.In principle, we have the following requirements for ahidden understanding system:?
A notational system for expressing meanings.?
A statistical model that is capable of representingmeanings and the association between meanings andwords.?
An automatic training program which, given pairs ofmeanings and word sequences, can estimate theparameters ofa statistical model.?
An understanding program that can search thestatistical model to fred the most likely meaning ivena word sequence.L,sentences 17 progr~a ~ expressionsFigure 1.
The Main Components of a HiddenUnderstanding System.25Below, we describe solutions for each of these requirements,and describe the relationship of these solutions to other workin stochastic grammars and probabilistic parsing.
Finally,we will report on initial experiments with hiddenunderstanding models.2 Expressing MeaningsOne of the key requirements for a hidden understandingmodel is that the meaning representation must be bothprecise and appropriate for automatic learning techniques.Specifically, we require a meaning representation that is:?
Expressive.
It must be able to express meanings overthe entire range of utterances that are likely to occur inan application.?
Annotatable.
It must be possible to produce accurateannotations for a sufficiently large corpus with anacceptable vel of human effort.?
Trainable.
It must be possible to estimate the modelparameters from a reasonable number of trainingexamples.?
Tractable.
There must be a computationally tractable'algorithm capable of searching the meaning space.In order to facilitate annotation of a training corpus, meaningexpressions should be as simple as possible.
Frame basedrepresentations, such as the example shown in figure 2, havethe advantage that they are relatively simple to understand.A difficulty with this style of representation is that theframes do not align directly to the words of the sentences.
Inparticular, a meaning flame contains few explicit clues as tohow the words of a sentence imply the structuralcharacteristics of the frame.
Tree structured meaningrepresentations, discussed in the next section, have theadvantage that they can be fully aligned to the words of asentence.
The cost is that these tree structuredrepresentations are more detailed than their flame basedcounterparts, thereby requiring greater annotation effort.Fortunately, the techniques developed for tree structuredrepresentations can be extended to simpler framerepresentations as well.SHOW:FLIGHTS:TIME:PART-OF-DAY: morningORIGIN:CITY: BostonDEST:CITY: San FranciscoDATE:DAY-OF-WEEK: TuesdayPlease show me morning flights from Boston to SanFrancisco on Tuesday.Figure 2.
A Frame Based Meaning Representation.2.1 Tree Structured Meaning RepresentationsThe central characteristic of a tree structured representationis that individual concepts appear as nodes in a tree, withcomponent concepts appearing as nodes attached irectlybelow them.
For example, the concept of a flight in theATIS domain has component concepts including airline,flight number, origin, and destination.
These could thenform part of the representation for the phrase: United flight203 from Dallas to Atlanta.
The use of a hierarchicalrepresentation is one characteristic that distinguishes hiddenunderstanding models from earlier work in which meaningis represented by a linear sequence of concepts \[Pieraccini etai., 91\].A requirement for tree structured representations is that theorder of the component concepts must match the order of thewords they correspond to.
Thus, the representation f thephrase flight 203 to Atlanta from Dallas on United includesthe same nodes as the earlier example, but in a differentorder.
For both examples, however, the interpretation isidentical.At the leaves of a meaning tree are the words of theFigure 3.
A Tree Structured Meaning Representation.26sentence.
We distinguish between odes that appear aboveother nodes, and those that appear directly above the words.These will be referred to as nonterminal nodes and terminalnodes respectively, forming two disjoint sets.
No node hasboth words and other nodes appearing directly below it.Figure 3 shows an example of a typical meaning tree.
In thisexample, theflight node represents he abstract concept of aflight, which is a structured entity that may contain anorigin, a destination, and other component concepts.Appearing directly above the word "flight" is a terminalnode, which we call aflight indicator.
This name is chosento distinguish it from the flight node, and also because theword flight, in some sense, indicates the presence of a flightconcept.
Similarly, there are airline indicators, originindicators, and destination i dicators.One view of these tree structured representations is that theyare parse trees produced according to a semantic grammar.In this view, the dominance relations of the grammar arepredetermined by the annotation schema, while theprecedence r lations are learned from the training examples.2.2 Alternative Tree RepresentationsTree structured meaning expressions can range incomplexity from simple special purpose sublanguagerepresentations to the structural equivalent of detailedsyntactic parse trees.
The possibilities are limited only bytwo fundamental requirements: (I) semantic oncepts mustbe hierarchically nested within a tree structure, and (2) thesets of terminal and nonterminal nodes must remaindisjoint.
Both of these requirements can be satisfied bytrees possessing most of the structural characteristics ofconventional syntactic parse trees.
Since our objective is tomodel meaning, the nodes must still be labeled to reflectsemantic ategories.
However, additional and augmentedlabels may be introduced to reflect syntactic ategories aswell.Representations of this form contain significantly moreinternal structure than specialized sublanguage models.This can be seen in the example in figure 4.
The specializedsublanguage r presentation requires only seven nodes, whilea full syntactically motivated analysis requires fifteen.
Theadditional nodes are used to distinguish what is being shownto whom, to reflect the fact that the stopover phrase is partof a relative clause, and to determine the internal structureof the relative clause.One interesting characteristic of these more elaborate treesis their similarity to those produced by classical,linguistically motivated, natural anguage systems.
Thus, ahidden understanding model can serve to replace the part-of-speech tagger, parser, and semantic interpreter of a classicalsystem.
Instead of writing grammar and semanticinterpretation rules by hand, the training programautomatically constructs a statistical model from examplesof meaning trees.Regardless of the details of the tree structure and labels, thecomponents comprising a hidden understanding systemremain unchanged.
The only difference is in how the systemis trained.Figure 4.
A Specialized Sublanguage Analysis and a Full Syntactic Analysis.272.3 Frame Based RepresentationsOne way to think of a frame based meaning is as a partiallyspecified tree in which some words are not accounted for.Nevertheless, a flame representation is a complete meaningrepresentation in the sense that it fully specifies the conceptsand structure comprising the meaning.
In terms of a treestructured representation, the set of nonterminal nodes isfully specified, while some of the terminal nodes may beomitted.The missing terminal nodes are said to be hidden, in thesense that every word is required to align to some terminalnode, but the alignment is not necessarily given by themeaning frame.
These hidden nodes must later be alignedas part of the training process.
The general idea is to assigna small number of free terminal nodes (typically one or two)beneath every nonterminal node.
These are then free to alignto any unassigned words, provided that the overall treestructure is not violated.
An EM algorithm (Estimate-Maximize) is used to organize the unassigned terminalnodes into classes that correspond to individual words andphrases, and that bind to particular abstract concepts.Figure 5 shows the complete meaning tree with hiddennodes corresponding to the flame in figure 2.If we consider tree structured meaning expressions as parsetrees which are generated according to some incompletelyspecified grammar, then the problem of aligning the hiddennodes can be considered as a grammar induction problem.In this way, the problem of aligning the hidden nodes givenonly a partially specified set of trees is analogous to theproblem of fully parsing a training corpus given only apartial bracketing.
The difference is that while a partialbracketing determines constituent boundaries that cannot becrossed, a partially specified tree determines structure thatmust be preserved.3 The Statistical ModelOne central characteristic ofhidden understanding models isthat they are generative.
From this viewpoint, language isproduced by a two component s atistical process.
The firstcomponent chooses the meaning to be expressed, effectivelydeciding "what o say".
The second component selects wordsequences to express that meaning, effectively deciding"how to say it".
The first phase is referred to as the semanticlanguage model, and can be thought of as a stochasticprocess that produces meaning expressions selected from auniverse of meanings.
The second phase is referred to as thelexical realization model, and can be thought of as astochastic process that generates words once a meaning isgiven.By analogy with hidden Markov models, we refer to thecombination of these two models as a hidden understandingmodel.
The word "hidden" refers to the fact that only wordscan be observed.
The internal states of each of the twomodels are unseen and must be inferred from the words.The problem of language understanding, then, is to recoverthe most likely meaning structure given a sequence ofwords.
More formally, understanding a word sequence W isaccomplished by searching among all possible meanings forsome meaning M such that P(MI W) is maximized.
ByBayes Rule, P(M \[ W) can be rewritten as:P(WIM)P(M) P( MIW) =P(W)Now, since P(W) does not depend on M, maximizingP(M \[ W) is equivalent to maximizing the product P(W \[ M)P(M).
However, P(M I W) is simply our lexical realizationmodel, and P(M) is simply our semantic language model.Thus, by searching a combination of these models it ispossible to find the maximum likelihood meaning M givenword sequence W. Considering the statistical model as astochastic grammar, the problem of determining M given iVis analogous to the problem of finding the most likelyderivation for W according to that grammar."?
'Figure 5.
A Tree Structure Corresponding to a Frame Representation.283.1 Semantic Language ModelFor tree structured meaning representations, individualnonterminal nodes determine particular abstract semanticconcepts.
In the semantic language model, each abstractconcept corresponds to a probabilistic state transitionnetwork.
All such networks are then combined into a singleprobabilistic recursive transition network, forming theentire semantic language model.The network corresponding to a particular abstract conceptconsists of states for each of its component concepts,together with two extra states that define the entry and exitpoints.
Every component concept is fully connected to everyother component concept, with additional paths leading fromthe entry state to each component concept, and from eachcomponent concept o the exit state.
Figure 6 shows asample network corresponding to the flight concept.
Ofcourse, there are many more flight component concepts inthe ATIS domain than actually appear in this example.Associated with each arc is a probability value, in a similarfashion to the TINA system \[Seneff, 92\].
Theseprobabilities have the form P(Staten I Staten.l,, Context),which is the probability of a taking transition from one stateto another within a particular context.
Thus, the arc fromorigin to dest has probability P(dest \[ origin, flight),meaning the probability of entering dest from origin withinthe context of the flight network.
Presumably, thisprobability is relatively high, since people usually mentionthe destination of a flight directly after mentioning its origin.Conversely, P(origin I dest, flight) is probably low becausepeople don't usually express concepts in that order.
Thus,while all paths through the state space are possible, somehave much higher probabilities than others.Within a concept network, component concept states existfor both nonterminal concepts, such as origin, as well asterminal concepts, uch as flight indicator.
Arrows pointinginto nonterminal states indicate entries into other networks,while arrows pointing away indicate exits out of thosenetworks.
Terminal states correspond to networks as well,although these are determined by the lexical realizationmodel and have a different internal structure.
Thus, everymeaning tree directly corresponds directly to some particularpath through the state space.
Figure 7 shows a meaning treeand its corresponding path through state space.Viewed as a grammar, the semantic language model isexpressed irectly as a collection of networks rather than asa collection of production rules.
These networks representgrammatical constraints in a somewhat different fashionthan do grammars based on production rules, In this model,constituents may appear beneath nonterminal nodes in anyarbitrary order, while preferences for some orderings aredetermined through the use of probabilities.
By contrast,most grammars limit the ordering of constituents to anexplicit set which is specified by the grammar rules.
Theapproach taken in the TINA system eliminates manyordering constraints while retaining the local state transitionconstraints determined by its grammar.
We believe that anunconstrained ordering of constraints increases parsingrobustness, while the preferences determined by the arcprobabilities help minimize overgeneration.3.2 Lexicai Realization ModelJust as nonterminal tree nodes correspond to networks in thesemantic language model, terminal nodes correspond tonetworks in the lexical realization model.
The difference isthat semantic language networks specify transitionFigure 6.
A Partial Network Corresponding to the ATIS Flight Concept.29probabilities between states, while lexical realizationnetworks specify transition probabilities between words.Lexical realization probabilities have the formP(word,\[word,.1 , context), which is the probability oftaking a transition from one word to another given aparticular context.
Thus, P(show Iplease, show-indicator) isthe probability that the word show follows the word pleasewithin the context of a show indicator phrase.
In addition,there are two pseudo-words, *begin* and *end*, whichindicate the beginning and ending of phrases.
Thus, wehave probabilities such as P(please \[ *begin*,show-indicator), which is the probability that please is thefirst word of a show indicator phrase, andP( *end* \[ me, show-indicator) , which is the probability ofexiting a show indicator phrase given that the previous wordwas/t ie .4 The Understanding ComponentAs we have seen, understanding a word string W requiresfinding a meaning M such that the probability P(W \[ lvl)P(M) is maximized.
Since, the semantic language modeland the lexical realization model are both probabilisticnetworks, P(W I M) P(M) is the probability of a particularpath through the combined network.
Thus, the problem ofunderstanding is to fmd the highest probability path amongall possible paths, where the probability of a path is theproduct of all the transition probabilities along that path.rP(state nIstate~_ l,context) if t in Semantic Language Model 1P(Path):tle~a~LP(word~lwordn_t,context ) if t in Lexical Realization ModelJThus far, we have discussed the need to search among allmeanings for one with a maximal probability.
In fact, if itwere necessary to search every path through the combinednetwork individually, the algorithm would requireexponential time with respect to sentence length.Fortunately, this can be drastically reduced by combining theprobability computation of common subpaths throughdynamic programming.
In particular, because our meaningrepresentation aligns to the words, the search can beefficiently performed using the well-known Viterbi \[Viterbi,67\] algorithm.Since our underlying model is a reeursive transitionnetwork, the states for the Viterbi search must be allocateddynamically as the search proceeds, In addition, it isnecessary to prune very low probability paths in order tokeep the computation tractable.
We have developed anelegant algorithm that integrates tate allocation, Viterbisearch, and pruning all within a single traversal of a tree-like data structure.
In this algorithm, each of the set ofcurrently active states is represented as a node in a tree,New nodes are added to the tree as the computation pushesinto new subnetworks that are not currently active.
Stored ateach node is the probability of the most likely path reachingthat state, together with a backpointer sufficient o recreatethe path later if needed.
Whenever the probability of allstates in a subtree falls below the threshold specified by thebeam width, the entire subtree is pruned away.5 The Training ComponentIn order to train the statistical model, we must estimatetransition probabilities for the semantic language model midlexical realization model.
In the case of fully specifiedmeaning trees, each meaning tree can be straightforwardlyconverted into a path through state space.
Then, by countingoccurrence and transition frequencies along those paths, it ispossible to form simple estimates of the transitionprobabilities.
Let C(statem, context,) denote the number oftimes state,, has occurred in contexts, and let C(state, \]state=, context,) denote the number of times that thiscondition has led to a transition to state state.. Similarly,defme counts C(wordm, context1) and C(word.
\] word,.,contextt).
Then, a direct estimate of the probabilities isgiven by:Show flights to AtlantaFigure 7.
A Meaning Tree and its Corresponding Path Through State Space.30andP(statenlstatem,context ) = C(statenlstate=,c?ntext) ,C( stca% ,context)P( word n Iword= ,context ) = C( word nlword m ,context )C ( wordm , context )In order to obtain robust estimates, these simple estimatesare smoothed with backed-off estimates \[Good, 53\], usingtechniques similar to those used in speech recognition \[Katz,^87; Placeway et al, 93\].
Thus, P(state, I state,,, context) issmoothed with P(staten \] context), and P(wordn \] wordm,^context) is smoothed with P(word, I context).
Robustness ifurther increased through word classes.
For example,Boston and San Francisco are both members of the class ofcities.In the case of frame based representations, it is not alwayspossible to construct an exact path through the state spacecorresponding to a meaning representation.
Nevertheless,since frames are treated as partially specified trees, most ofthe path can be reconstructed, with some portionsundetermined.
Then, the partial path can be used toconstrain a gradient descent search, called the forward-backward algorithm \[13aura, 72\] for estimating the modelparameters.
This algorithm is an iterative procedure foradjusting the model parameters o as to increase thelikelihood of generating the training data, and is an instanceof the well-known class called EM (Estimate-Maximize)algorithms.6 Experimental ResultsWe have implemented a hidden understanding system andperformed a variety of experiments.
In addition, weparticipated in the 1993 ARPA ATIS NL evaluation.One experiment involved a 1000 sentence ATIS corpus,annotated according to a simple specialized sublanguagemodel.
The annotation effort was split between twoannotators, one of whom was a system developer, while theother was not.
To annotate the training data, we used abootstrapping process in which only the first 100 sentenceswere annotated strictly by hand.Thereafter, we worked in cycles of."1.
Running the training program using all availableannotated data.2.
Running the understanding component to annotate newsentences.3.
Hand correcting the new annotations.Annotating in this way, we found that a single annotatorcould produce 200 sentences per day.
We then extracted thefirst 100 sentences as a test set, and trained the system onthe remaining 900 sentences.
The results were as follows:?
61% matched exactly.?
21% had correct meanings, but did not match exactly.?
28% had the wrong meaning.Another experiment involved a 6000 sentence ATIS corpus,annotated according to a more sophisticated meaning model.In this experiment, the Delphi system automaticallyproduced the annotation by printing out its own internalrepresentation for each sentence, converted into a morereadable form.
In order to maintain high qualityannotations, we used only sentences for which Delphiproduced a complete parse, and for which it also retrieved acorrect answer from the database.
We then removed 300sentences as a test set, and trained the system on theremaining 5700.
The results were as follows:?
85% matched exactly.?
8% had correct meanings, but did not match exactly.?
7% had the wrong meaning.For the ARPA evaluation, we coupled our hiddenunderstanding system to the discourse and backendcomponents of the Delphi.
Using the entire 6000 sentencecorpus described above as training data, the systemproduced a score of 26% simple error on the ATIS NLevaluation.
By examining the errors, we have reached theconclusion that nearly half are due to simple programmingissues, especially in the interface between Delphi and thehidden understanding system.
In fact, the interface was stillincomplete at the time of the evaluation.We have just begun a series of experiments using framebased annotations, and are continuing to refme ourtechniques.
In a preliminary test involving a small corpus of588 ATIS sentences, the system correctly aligned the hiddenstates for over 95% of the sentences in the corpus.7 LimitationsSeveral limitations to our current approach are worth noting.In a small number of cases, linguistic movement phenomenamake it difficult to align the words of a sentence to any treestructured meaning expression without introducingcrossings.
In most cases, we have been able to work aroundthis problem by introducing minor changes in our annotationsuch that the tree structure is maintained.
A secondlimitation, due to the local nature of the model, is aninability to handle nonlocal phenomena such as coreference.Finally, in some cases the meaning of a sentence dependsstrongly upon the discourse state, which is beyond the scopeof the current model.8 ConclusionsWe have demonstrated the possibility of automaticallylearning semantic representations directly from a trainingcorpus through the application of statistical techniques.Empirical results, including the results of an ARPA31evaluation, indicate that these techniques are capable ofrelatively high levels of performance.While hidden understanding models are based primarily onthe concepts of hidden Markov models, we have also showntheir relationship to other work in stochastic grammars andprobabilistic parsing.Finally, we have noted some limitations to our currentapproach.
We view each of these limitations as opportunitiesfor fta~er esearch and exploration.AcknowledgmentsThe work reported here was supported in part by theDefense Advanced Research Projects Agency under ARPAContract No.
N00014-92-C-0035.
The views andconclusions contained in this document are those of theauthors and should not be interpreted as necessarilyrepresenting the official policies, either expressed orimplied, of the Defense Advanced Research Projects Agencyor the United States Government.References1.
E. Baum, "An Inequality and Associated MaximizationTechnique in Statistical Estimation of ProbabilisticFunctions of Markov Processes," Inequalities 3:1-8,19722.
Bobrow, R. Ingria, and D. StaUard, "Syntactic andSemantic Knowledge in the DELPHI UnificationGrammar," Proceedings, Speech and Natural LanguageWorkshop, p. 230-236, June 19903.
Chitrao, and R. Grishman, "Statistical Parsing ofMessages," Proceedings, Speech and NaturalLanguage Workshop, p. 263-276, Morgan KanfmamaPublishers, June 19904.
Fujisaki, F. Jelinek, J. Cocke, E. Black, T. Nishino, "AProbabilistic Parsing Method for SentenceDisambiguation," International Parsing Workshop, p.85-90, 1989.
Good, "The Population Frequencies of Species and theEstimation of Population Parameters," Biometrika 40,pp.237-264, 19536.
G.G Hendrix, "Semantic Aspects of Translation,"Understanding Spoken Language, pp.
193-226, NewYork, Elsevier, 19787.
Katz, "Estimation of Probabilities from Sparse Data forthe Language Model Component of a SpeechRecognizer," IEEE Transactions on Acoustics, Speech,and Signal Processing, Vol.
ASSP-35, pp.
400-401,19878, Pereira and Y. Schabes, "Inside-Outside Reestimationfrom Partially Bracketed Corpora," Proceedings of the30th Annual MeetT"ng of the Association forComputational Linguistics, pp.128-135, Newark,Delaware, 19929.
R. Pieraecini, E. Levin, and C. Lee, StochasticRepresentation f Conceptual Structure in the ATISTasic.
DARPA Speech and Natural Language Workshop,pp.
121-124, Feb. 1991.10.
Placeway, R. Schwartz, P. Fung, L. Nguyen, "TheEstimation of Powerful Language Models from Smalland Large Corpora," IEEE ICASSP, II:33-3611.12.13.14.Seneff, "'TINA: A Natural Language System for SpokenLanguage Applications," Computational LinguisticsVol.
18, Number 1, pp.
61-86, March 1992J.
Viterbi, "Error Bounds for Convolutional Codes andan Asympotically Optimum Decoding Algorithm,"IEEE Transactions on Information Theory IT-13(2):260-269, April 1967D.L Waltz, "An English Language Question AnsweringSystem for a Large Relational Database,"Communications of the ACM 21(7):526-39, 1978.W.A Woods, "Transition Network Grammars forNatural Language Analysis," Communications of theACM 13(10):591-606, 197032
