Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 65?72,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsStudying Feature Generation from Various Data Representations forAnswer ExtractionDan Shen??
Geert-Jan M. Kruijff?
Dietrich Klakow??
Department of Computational LinguisticsSaarland UniversityBuilding 17,Postfach 15 11 5066041 Saarbruecken, Germany?
Lehrstuhl Sprach Signal VerarbeitungSaarland UniversityBuilding 17, Postfach 15 11 5066041 Saarbruecken, Germany{dshen,gj}@coli.uni-sb.de{dietrich.klakow}@lsv.uni-saarland.deAbstractIn this paper, we study how to generatefeatures from various data representations,such as surface texts and parse trees, foranswer extraction.
Besides the featuresgenerated from the surface texts, wemainly discuss the feature generation inthe parse trees.
We propose and comparethree methods, including feature vector,string kernel and tree kernel, to representthe syntactic features in Support VectorMachines.
The experiment on the TRECquestion answering task shows that thefeatures generated from the more struc-tured data representations significantlyimprove the performance based on thefeatures generated from the surface texts.Furthermore, the contribution of the indi-vidual feature will be discussed in detail.1 IntroductionOpen domain question answering (QA), as definedby the TREC competitions (Voorhees, 2003),represents an advanced application of natural lan-guage processing (NLP).
It aims to find exact an-swers to open-domain natural language questionsin a large document collection.
For example:Q2131: Who is the mayor of San Francisco?Answer: Willie BrownA typical QA system usually consists of threebasic modules: 1.
Question Processing (QP) Mod-ule, which finds some useful information from thequestions, such as expected answer type and keywords.
2.
Information Retrieval (IR) Module,which searches a document collection to retrieve aset of relevant sentences using the question keywords.
3.
Answer Extraction (AE) Module, whichanalyzes the relevant sentences using the informa-tion provided by the QP module and identify theanswer phrase.In recent years, QA systems trend to be moreand more complex, since many other NLP tech-niques, such as named entity recognition, parsing,semantic analysis, reasoning, and external re-sources, such as WordNet, web, databases, are in-corporated.
The various techniques and resourcesmay provide the indicative evidences to find thecorrect answers.
These evidences are further com-bined by using a pipeline structure, a scoring func-tion or a machine learning method.In the machine learning framework, it is criticalbut not trivial to generate the features from thevarious resources which may be represented assurface texts, syntactic structures and logic forms,etc.
The complexity of feature generation stronglydepends on the complexity of data representation.Many previous QA systems (Echihabi et al, 2003;Ravichandran, et al, 2003; Ittycheriah and Roukos,2002; Ittycheriah, 2001; Xu et al, 2002) have wellstudied the features in the surface texts.
In thispaper, we will use the answer extraction module ofQA as a case study to further explore how to gen-erate the features for the more complex sentencerepresentations, such as parse tree.
Since parsinggives the deeper understanding of the sentence, thefeatures generated from the parse tree are expectedto improve the performance based on the featuresgenerated from the surface text.
The answer ex-65traction module is built using Support Vector Ma-chines (SVM).
We propose three methods to rep-resent the features in the parse tree: 1. features aredesigned by domain experts, extracted from theparse tree and represented as a feature vector; 2.the parse tree is transformed to a node sequenceand a string kernel is employed; 3. the parse tree isretained as the original representation and a treekernel is employed.Although many textual features have been usedin the others?
AE modules, it is not clear that howmuch contribution the individual feature makes.
Inthis paper, we will discuss the effectiveness ofeach individual textual feature in detail.
We fur-ther evaluate the effectiveness of the syntactic fea-tures we proposed.
Our experiments using TRECquestions show that the syntactic features improvethe performance by 7.57 MRR based on the textualfeatures.
It indicates that the new features basedon a deeper language understanding are necessaryto push further the machine learning-based QAtechnology.
Furthermore, the three representationsof the syntactic features are compared.
We findthat keeping the original data representation byusing the data-specific kernel function in SVMmay capture the more comprehensive evidencesthan the predefined features.
Although the featureswe generated are specific to the answer extractiontask, the comparison between the different featurerepresentations may be helpful to explore the syn-tactic features for the other NLP applications.2 Related WorkIn the machine learning framework, it is crucial tocapture the useful evidences for the task and inte-grate them effectively in the model.
Many re-searchers have explored the rich textual featuresfor the answer extraction.IBM (Ittycheriah and Roukos, 2002; Ittycheriah,2001) used a Maximum Entropy model to integratethe rich features, including query expansion fea-tures, focus matching features, answer candidateco-occurrence features, certain word frequencyfeatures, named entity features, dependency rela-tion features, linguistic motivated features and sur-face patterns.
ISI?s (Echihabi et al 2003; Echihabiand Marcu, 2003) statistical-based AE module im-plemented a noisy-channel model to explain how agiven sentence tagged with an answer can be re-written into a question through a sequence of sto-chastic operations.
(Ravichandran et al, 2003)compared two maximum entropy-based QA sys-tems, which view the AE as a classification prob-lem and a re-ranking problem respectively, basedon the word frequency features, expected answerclass features, question word absent features andword match features.
BBN (Xu et al 2002) used aHMM-based IR system to score the answer candi-dates based on the answer contexts.
They furtherre-ranked the scored answer candidates using theconstraint features, such as whether a numericalanswer quantifies the correct noun, whether theanswer is of the correct location sub-type andwhether the answer satisfies the verb arguments ofthe questions.
(Suzuki et al 2002) explored theanswer extraction using SVM.However, in the previous statistical-based AEmodules, most of the features were extracted fromthe surface texts which are mainly based on thekey words/phrases matching and the key word fre-quency statistics.
These features only capture thesurface-based information for the proper answersand may not provide the deeper understanding ofthe sentences.
In addition, the contribution of theindividual feature has not been evaluated by them.As for the features extracted from the structuredtexts, such as parse trees, only a few works ex-plored some predefined syntactic relation featuresby partial matching.
In this paper, we will explorethe syntactic features in the parse trees and com-pare the different feature representations in SVM.Moreover, the contributions of the different fea-tures will be discussed in detail.3 Answer ExtractionGiven a question Q and a set of relevant sentencesSentSet which is returned by the IR module, weconsider all of the base noun phrases and the wordsin the base noun phrases as answer candidates aci.For example, for the question ?Q1956: What coun-try is the largest in square miles?
?, we extract theanswer candidates { Russia, largest country, larg-est, country, world, Canada, No.2.}
in the sentence?I recently read that Russia is the largest countryin the world, with Canada No.
2.?
The goal of theAE module is to choose the most probable answerfrom a set of answer candidates 1 2{ , ,... }mac ac acfor the question Q.We regard the answer extraction as a classifica-tion problem, which classify each question and66answer candidate pair <Q, aci> into the positiveclass (the correct answer) and the negative class(the incorrect answer), based on some features.The predication for each <Q, aci> is made inde-pendently by the classifier, then, the ac with themost confident positive prediction is chosen as theanswer for Q.  SVM have shown the excellent per-formance for the binary classification, therefore,we employ it to classify the answer candidates.Answer extraction is not a trivial task, since itinvolves several components each of which isfairly sophisticated, including named entity recog-nition, syntactic / semantic parsing, question analy-sis, etc.
These components may provide someindicative evidences for the proper answers.
Be-fore generating the features, we process the sen-tences as follows:1. tag the answer sentences with named entities.2.
parse the question and the answer sentences us-ing the Collins?
parser (Collin, 1996).3. extract the key words from the questions, suchas the target words, query words and verbs.In the following sections, we will briefly intro-duce the machine learning algorithm.
Then, wewill discuss the features in detail, including themotivations and representations of the features.4 Support Vector MachinesSupport Vector Machines (SVM) (Vapnik, 1995)have strong theoretical motivation in statisticallearning theory and achieve excellent generaliza-tion performance in many language processingapplications, such as text classification (Joachims,1998).SVM constructs a binary classifier that predictwhether an instance x ( n?w R ) is positive( ( ) 1f =x ) or negative ( ( ) 1f = ?x ), where, aninstance may be represented as a feature vector ora structure like sequence of characters or tree.
Inthe simplest case (linearly separable instances), thedecision f( ) sgn( b )?
+x = w x is made basedon a separating hyperplane 0b?
+ =w x  ( n?w R ,b?R ).
All instances lying on one side of the hy-perplane are classified to a positive class, whileothers are classified to a negative class.Given a set of labeled training instances( ) ( ) ( ){ }1 1 2 2, , , ,..., ,m mD y y y= x x x , where ni ?x Rand { }1, 1iy = ?
, SVM is to find the optimal hy-perplane that separates the positive and negativetraining instances with a maximal margin.
Themargin is defined as the distance from the separat-ing hyperplane to the closest positive (negative)training instances.
SVM is trained by solving adual quadratic programming problem.Practically, the instances are non-linearly sepa-rable.
For this case, we need project the instancesin the original space Rn to a higher dimensionalspace RN based on the kernel function1 2 1 2( , ) ( ), ( )K =<?
?
>x x x x ,where, ( ): n N?
?x R R  isa project function of the instance.
By this means, alinear separation will be made in the new space.Corresponding to the original space Rn, a non-linear separating surface is found.
The kernelfunction has to be defined based on the Mercer?scondition.
Generally, the following kernel func-tions are widely used.Polynomial kernel: ( , ) ( 1) pi j i jk = ?
+x x x xGaussian RBF kernel:2 22( , ) i j-i jk e?
?= x xx x5 Textual FeaturesSince the features extracted from the surface textshave been well explored by many QA systems(Echihabi et al, 2003; Ravichandran, et al, 2003;Ittycheriah and Roukos, 2002; Ittycheriah, 2001;Xu et al, 2002), we will not focus on the textualfeature generation in this paper.
Only four types ofthe basic features are used:1.
Syntactic Tag Features: the features capturethe syntactic/POS information of the words inthe answer candidates.
For the certain ques-tion, such as ?Q1903: How many time zonesare there in the world?
?, if the answer candi-date consists of the words with the syntactictags ?CD NN?, it is more likely to be theproper answer.2.
Orthographic Features: the features capturethe surface format of the answer candidates,such as capitalization, digits and lengths, etc.These features are motivated by the observa-tions, such as, the length  of the answers areoften less than 3 words for the factoid ques-tions; the answers may not be the subse-quences of the questions; the answers oftencontain digits for the certain questions.3.
Named Entity Features: the features capturethe named entity information of the answer67candidates.
They are very effective for thewho, when and where questions, such as, For?Q1950: Who created the literary characterPhineas Fogg?
?, the answer ?Jules Verne?
istagged as a PERSON name in the sentences?Jules Verne 's Phileas Fogg made literaryhistory when he traveled around the world in80 days in 1873.?.
For the certain question tar-get, if the answer candidate is tagged as thecertain type of named entity, one feature fires.4.
Triggers: some trigger words are collected forthe certain questions.
For examples, for?Q2156: How fast does Randy Johnsonthrow?
?, the trigger word ?mph?
for the ques-tion words ?how fast?
may help to identify theanswer ?98-mph?
in ?Johnson throws a 98-mph fastball?.6 Syntactic FeaturesIn this section, we will discuss the feature genera-tion in the parse trees.
Since parsing outputs thehighly structured data representation of the sen-tence, the features generated from the parse treesmay provide the more linguistic-motivated expla-nation for the proper answers.
However, it is nottrivial to find the informative evidences from aparse tree.The motivation of the syntactic features in ourtask is that the proper answers often have the cer-tain syntactic relations with the question key words.Table 1 shows some examples of the typical syn-tactic relations between the proper answers (a) andthe question target words (qtarget).
Furthermore,the syntactic relations between the answers and thedifferent types of question key words vary a lot.Therefore, we capture the relation features for thedifferent types of question words respectively.
Thequestion words are divided into four types:z Target word, which indicates the expected an-swer type, such as ?city?
in ?Q: What city isDisneyland in?
?.z Head word, which is extracted from how ques-tions and indicates the expected answer head,such as ?dog?
in ?Q210: How many dogspull ??
?z Subject words, which are the base noun phrasesof the question except the target word and thehead word.z Verb, which is the main verb of the question.To our knowledge, the syntactic relation fea-tures between the answers and the question keywords haven?t been explored in the previous ma-chine learning-based QA systems.
Next, we willpropose three methods to represent the syntacticrelation features in SVM.6.1 Feature VectorIt is the commonly used feature representation inmost of the machine learning algorithms.
We pre-define a set of syntactic relation features, which isan enumeration of some useful evidences of theanswer candidates (ac) and the question key wordsin the parse trees.
20 syntactic features are manu-ally designed in the task.
Some examples of thefeatures are listed as follows,z if the ac node is the same of the qtarget node,one feature fires.z if the ac node is the sibling of the qtarget node,one feature fires.z if the ac node the child of the qsubject node,one feature fires.The limitation of the manually designed features isthat they only capture the evidences in the localcontext of the answer candidates and the questionkey words.
However, some question words, suchas subject words, often have the long range syntac-1.
a node is the same as the qtarget node and qtarget is the hypernym of a.Q: What city is Disneyland in?S: Not bad for a struggling actor who was working at Tokyo Disneyland a few years ago.2.
a node is the parent of qtarget node.Q: What is the name of the airport in Dallas Ft. Worth?S: Wednesday morning, the low temperature at the Dallas-Fort Worth International Airport was 81 degrees.3.
a node is the sibling of the qtarget node.Q: What book did Rachel Carson write in 1962?S: In her 1962 book Silent Spring, Rachel Carson, a marine biologist, chronicled DDT 's poisonous effects, ?.Table 1: Examples of the typical relations between answer and question target word.
In Q, the italic word isquestion target word.
In S, the italic word is the question target word which is mapped in the answer sentence;the underlined word is the proper answer for the question Q.68Figure 1: An example of the path from the answercandidate node to the question subject word nodetic relations with the answers.
To overcome thelimitation, we will propose some special kernelswhich may keep the original data representationinstead of explicitly enumerate the features, to ex-plore a much larger feature space.6.2 String KernelThe second method represents the syntactic rela-tion as a linked node sequence and incorporates astring kernel in SVM to handle the sequence.We extract a path from the node of the answercandidate to the node of the question key word inthe parse tree.
The path is represented as a nodesequence linked by symbols indicating upward ordownward movement through the tree.
For exam-ple, in Figure 1, the path from the answer candi-date node ?211,456 miles?
to the question subjectword node ?the moon?
is?
NPB ADVP VP S NPB?
?
?
?
?, where ?
?
?
and?
?
?
indicate upward movement and downwardmovement in the parse tree.
By this means, werepresent the object from the original parse tree tothe node sequence.
Each character of the sequenceis a syntactic/POS tag of the node.
Next, a stringkernel will be adapted to our task to calculate thesimilarity between two node sequences.
(Haussler, 1999) first described a convolutionkernel over the strings.
(Lodhi et al, 2000) appliedthe string kernel to the text classification.
(Leslieet al, 2002) further proposed a spectrum kernel,which is simpler and more efficient than the previ-ous string kernels, for protein classification prob-lem.
In their tasks, the string kernels achieved thebetter performance compared with the human-defined features.The string kernel is to calculate the similaritybetween two strings.
It is based on the observationthat the more common substrings the strings have,the more similar they are.
The string kernel weused is similar to (Leslie et al, 2002).
It is definedas the sum of the weighted common substrings.The substring is weighted by an exponentially de-caying factor ?
(set 0.5 in the experiment) of itslength k.  For efficiency, we only consider the sub-strings which length are less than 3.
Differentfrom (Leslie et al, 2002), the characters (syntac-tic/POS tag) of the string are linked with eachother.
Therefore, the matching between two sub-strings will consider the linking information.
Twoidentical substrings will not only have the samesyntactic tag sequences but also have the samelinking symbols.
For example, for the node se-quences NP VP VP S NP?
?
?
?
and NP NP VP NP?
?
?
,there is a matched substring (k = 2): NP VP?
.6.3 Tree KernelThe third method keeps the original representationof the syntactic relation in the parse tree and incor-porates a tree kernel in SVM.Tree kernels are the structure-driven kernels tocalculate the similarity between two trees.
Theyhave been successfully accepted in the NLP appli-cations.
(Collins and Duffy, 2002) defined a ker-nel on parse tree and used it to improve parsing.
(Collins, 2002) extended the approach to POS tag-ging and named entity recognition.
(Zelenko et al,2003; Culotta and Sorensen, 2004) further ex-plored tree kernels for relation extraction.We define an object (a relation tree) as thesmallest tree which covers one answer candidatenode and one question key word node.
Supposethat a relation tree T has nodes 0 1{ , , ..., }nt t t  andeach node it is attached with a set of attrib-utes 0 1{ , , ..., }ma a a , which represents the local char-acteristics of ti .
In our task, the set of theattributes includes Type attributes, Orthographicattributes and Relation Role attributes, as shown inTable 2.
The core idea of the tree kernel ( , )1 2K T Tis that the similarity between two trees T1 and T2 isPUNC.
away 221,456 milesSPP NPB VPVBZ ADVPNPB RBthe moonisQ1980: How far is the moon from Earth in miles?S: At its perigee, the closest approach to Earth , themoon is 221,456 miles away.?
?69T1_ac#targetT2_ac#targetQ1897: What is the name of the airport in Dallas Ft. Worth?S: Wednesday morning, the low temperature at the Dallas-FortWorth International Airport was 81 degrees.t4t3 t2T: BNPO: nullR1: trueR2: falset1Dallas-FortT: NNPO: CAPALLR1: falseR2: falseInternationalT: JJO: CAPALLR1: falseR2: falseAirportT: NNPO: CAPALLR1: falseR2: trueQ35: What is the name of the highest mountain in Africa?S: Mount Kilimanjaro, at 19,342 feet, is Africa's highest moun-tain, and is 5,000 feet higher than ?.MountT: NNPO: CAPALLR1: falseR2: trueKilimanjaroT: NNPO: CAPALLR1: falseR2: falseT: BNPO: nullR1: trueR2: falset0w0w1 w2WorthT: NNPO: CAPALLR1: falseR2: falsethe sum of the similarity between their subtrees.
Itis calculated by dynamic programming and cap-tures the long-range syntactic relations betweentwo nodes.
The kernel we use is similar to (Ze-lenko et al, 2003) except that we define a task-specific matching function and similarity function,which are two primitive functions to calculate thesimilarity between two nodes in terms of their at-tributes.Matching function1 if .
.
and .
.
( , )0 otherwisei j i ji jt type t type t role t rolem t t= == ??
?Similarity function0{ ,..., }( , ) ( .
, .
)i j i jma a as t t f t a t a?= ?where, ( .
, .
)i jf t a t a  is a compatibility function be-tween two feature values.
.
( .
, .
)1   if0   otherwisei ji jt a t af t a t a ==??
?Figure 2 shows two examples of the relation treeT1_ac#targetword and T2_ac#targetword.
Thekernel we used matches the following pairs of thenodes <t0, w0>, <t1, w2>, <t2, w2> and <t4, w1>.Attributes ExamplesPOS tag CD, NNP, NN?Typesyntactic tag NP, VP, ?Is Digit?
DIG, DIGALLIs Capitalized?
CAP, CAPALLOrtho-graphiclength of phrase LNG1, LNG2#3,LNGgt3Role1 Is answer candidate?
true, falseRole2 Is question key words?
true, falseTable 2: Attributes of the nodes7 ExperimentsWe apply the AE module to the TREC QA task.To evaluate the features in the AE module inde-pendently, we suppose that the IR module has got100% precision and only passes those sentencescontaining the proper answers to the AE module.The AE module is to identify the proper answersfrom the given sentence collection.We use the questions of TREC8, 9, 2001 and2002 for training and the questions of TREC2003for testing.
The following steps are used to gener-ate the data:1.
Retrieve the relevant documents for each ques-tion based on the TREC judgments.2.
Extract the sentences, which match both theproper answer and at least one question key word,from these documents.3.
Tag the proper answer in the sentences based onthe TREC answer patternsFigure 2: Two objects representing the relations be-tween answer candidates and target words.In TREC 2003, there are 413 factoid questionsin which 51 questions (NIL questions) are not re-turned with the proper answers by TREC.
Accord-ing to our data generation process, we cannotprovide data for those NIL questions because wecannot get the sentence collections.
Therefore, theAE module will fail on all of the NIL questionsand the number of the valid questions should be362 (413 ?
51).
In the experiment, we still test themodule on the whole question set (413 questions)to keep consistent with the other?s work.
Thetraining set contains 1252 questions.
The perform-ance of our system is evaluated using the meanreciprocal rank (MRR).
Furthermore, we also listthe percentages of the correct answers respectively70in terms of the top 5 answers and the top 1 answerreturned.
We employ the SVMLight (Joachims,1999) to incorporate the features and classify theanswer candidates.
No post-processes are used toadjust the answers in the experiments.Firstly, we evaluate the effectiveness of the tex-tual features, described in Section 5.
We incorpo-rate them into SVM using the three kernelfunctions: linear kernel, polynomial kernel andRBF kernel, which are introduced in Section 4.Table 3 shows the performance for the differentkernels.
The RBF kernel (46.24 MRR) signifi-cantly outperforms the linear kernel (33.72 MRR)and the polynomial kernel (40.45 MRR).
There-fore, we will use the RBF kernel in the rest ex-periments.Top1 Top5 MRRlinear 31.28 37.91 33.72polynomial 37.91 44.55 40.45RBF 42.67 51.58 46.24Table 3: Performance for kernelsIn order to evaluate the contribution of the indi-vidual feature, we test out module using differentfeature combinations, as shown in Table 4.
Sev-eral findings are concluded:1.
With only the syntactic tag features Fsyn., themodule achieves a basic level MRR of 31.38.
Thequestions ?Q1903: How many time zones are therein the world??
is correctly answered from the sen-tence ?The world is divided into 24 time zones.?.2.
The orthographic features Forth.
show the posi-tive effect with 7.12 MRR improvement based onFsyn..
They help to find the proper answer ?GroverCleveland?
for the question ?Q2049: What presi-dent served 2 nonconsecutive terms??
from thesentence ?Grover Cleveland is the forgotten two-term American president.
?, while Fsyn.
wronglyidentify ?president?
as the answer.3.
The named entity features Fne are also benefi-cial as they make the 4.46 MRR increase based onFsyn.+Forth.
For the question ?Q2076: What com-pany owns the soft drink brand "Gatorade"?
?, Fnefind the proper answer ?Quaker Oats?
in the sen-tence ?Marineau , 53 , had distinguished himselfby turning the sports drink Gatorade into a massconsumer brand while an executive at Quaker OatsDuring his 18-month?
?, while Fsyn.+Forth.
returnthe wrong answer ?Marineau?.4.
The trigger features Ftrg lead to an improve-ment of 3.28 MRR based on Fsyn.+Forth+Fne.
Theycorrectly answer more questions.
For the question?Q1937: How fast can a nuclear submarinetravel?
?, Ftrg return the proper answer ?25 knots?from the sentence ?The submarine , 360 feet( 109.8 meters ) long , has 129 crew members andtravels at 25 knots.
?, but the previous features failon it.Fsyn Forth.
Fne Ftrg Top1 Top5 MRR?
26.50 38.92 31.38?
?
34.69 43.61 38.50?
?
?
39.85 47.82 42.96?
?
?
?
42.67 51.58 46.24Table 4: Performance for feature combinationsNext, we will evaluate the effectiveness of the syn-tactic features, described in Section 6.
Table 5compares the three feature representation methods,FeatureVector, StringKernel and TreeKernel.z FeatureVector (Section 6.1).
We predefinesome features in the syntactic tree and presentthem as a feature vector.
The syntactic fea-tures are added with the textual features andthe RBF kernel is used to cope with them.z StringKernel (Section 6.2).
No features arepredefined.
We transform the syntactic rela-tions between answer candidates and questionkey words to node sequences and a string ker-nel is proposed to cope with the sequences.Then we add the string kernel for the syntacticrelations and the RBF kernel for the textualfeatures.z TreeKernel (Section 6.3).
No features arepredefined.
We keep the original representa-tions of the syntactic relations and propose atree kernel to cope with the relation trees.Then we add the tree kernel and the RBF ker-nel.Top1 Top2 MRRFsyn.+Forth.+Fne+Ftrg 42.67 51.58 46.24FeatureVector 46.19 53.69 49.28StringKernel 48.99 55.83 52.29TreeKernel 50.41 57.46 53.81Table 5: Performance for syntactic feature repre-sentationsTable 5 shows the performances of FeatureVec-tor, StringKernel and TreeKernel.
All of them im-prove the performance based on the textualfeatures (Fsyn.+Forth.+Fne+Ftrg) by 3.04 MRR, 6.05MRR and 7.57 MRR respectively.
The probablereason may be that the features generated from thestructured data representation may capture the71more linguistic-motivated evidences for the properanswers.
For example, the syntactic features helpto find the answer ?nitrogen?
for the question?Q2139: What gas is 78 percent of the earth 's at-mosphere??
in the sentence ?One thing they have-n't found in the moon's atmosphere so far isnitrogen, the gas that makes up more than three-quarters of the Earth's atmosphere.
?, while thetextual features fail on it.
Furthermore, the String-Kernel (+3.01MRR) and TreeKernel (+4.53MRR)achieve the higher performance than FeatureVec-tor, which may be explained that keeping theoriginal data representations by incorporating thedata-specific kernels in SVM may capture themore comprehensive evidences than the predefinedfeatures.
Moreover, TreeKernel slightly outper-forms StringKernel by 1.52 MRR.
The reason maybe that when we transform the representation of thesyntactic relation from the tree to the node se-quence, some information may be lost, such as thesibling node of the answer candidates.
Sometimesthe information is useful to find the proper answers.8 ConclusionIn this paper, we study the feature generation basedon the various data representations, such as surfacetext and parse tree, for the answer extraction.
Wegenerate the syntactic tag features, orthographicfeatures, named entity features and trigger featuresfrom the surface texts.
We further explore the fea-ture generation from the parse trees which providethe more linguistic-motivated evidences for thetask.
We propose three methods, including featurevector, string kernel and tree kernel, to representthe syntactic features in Support Vector Machines.The experiment on the TREC question answeringtask shows that the syntactic features significantlyimprove the performance by 7.57MRR based onthe textual features.
Furthermore, keeping theoriginal data representation using a data-specifickernel achieves the better performance than theexplicitly enumerated features in SVM.ReferencesM.
Collins.
1996.
A New Statistical Parser Based onBigram Lexical Dependencies.
In Proceedings ofACL-96, pages 184-191.M.
Collins.
2002.
New Ranking Algorithms for Parsingand Tagging: Kernel over Discrete Structures, andthe Voted Perceptron.
In Proceedings of ACL-2002.M.
Collins and N. Duffy.
2002.
Convolution Kernelsfor Natural Language.
Advances in Neural Informa-tion Processing Systems 14, Cambridge, MA.
MITPress.A.
Culotta and J. Sorensen.
2004.
Dependency TreeKernels for Relation Extraction.
In Proceedings ofACL-2004.A.
Echihabi, U. Hermjakob, E. Hovy, D. Marcu, E.Melz, D. Ravichandran.
2003.
Multiple-EngineQuestion Answering in TextMap.
In Proceedings ofthe TREC-2003 Conference, NIST.A.
Echihabi, D. Marcu.
2003.
A Noisy-Channel Ap-proach to Question Answering.
In Proceedings of theACL-2003.D.
Haussler.
1999.
Convolution Kernels on DiscreteStructures.
Technical Report UCS-CRL-99-10, Uni-versity of California, Santa Cruz.A.
Ittycheriah and S. Roukos.
2002.
IBM?s StatisticalQuestion Answering System ?
TREC 11.
In Pro-ceedings of the TREC-2002 Conference, NIST.A.
Ittycheriah.
2001.
Trainable Question AnsweringSystem.
Ph.D. Dissertation, Rutgers, The State Uni-versity of New Jersey, New Brunswick, NJ.T.
Joachims.
1999.
Making large-Scale SVM Learn-ing Practical.
Advances in Kernel Methods - Sup-port Vector Learning, MIT-Press, 1999.T.
Joachims.
1998.
Text Categorization with SupportVector Machines: Learning with Many Relevant Fea-tures.
In Proceedings of the European Conference onMachine Learning, Springer.C.
Leslie, E. Eskin and W. S. Noble.
2002.
The spec-trum kernel: A string kernel for SVM protein classi-fication.
Proceedings of the Pacific BiocomputingSymposium.H.
Lodhi, J. S. Taylor, N. Cristianini and C. J. C. H.Watkins.
2000.
Text Classification using StringKernels.
In NIPS, pages 563-569.D.
Ravichandran, E. Hovy and F. J. Och.
2003.
Statis-tical QA ?
Classifier vs. Re-ranker: What?s the dif-ference?
In Proceedings of Workshop on MulingualSummarization and Question Answering, ACL 2003.J.
Suzuki, Y. Sasaki, and E. Maeda.
2002.
SVM AnswerSelection for Open-domain Question Answering.
InProc.
of COLING 2002, pages 974?980.V.
N. Vapnik.
1998.
Statistical Learning Theory.Springer.E.M.
Voorhees.
2003.
Overview of the TREC 2003Question Answering Track.
In Proceedings of theTREC-2003 Conference, NIST.J.
Xu, A. Licuanan, J.
May, S. Miller and R. Weischedel.2002.
TREC 2002 QA at BBN: Answer Selectionand Confidence Estimation.
In Proceedings of theTREC-2002 Conference, NIST.D.
Zelenko, C. Aone and A. Richardella.
2003.
KernelMethods for Relation Extraction.
Journal of Ma-chine Learning Research, pages 1083-1106.72
