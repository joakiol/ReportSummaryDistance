Proceedings of the 25th International Conference on Computational Linguistics, pages 46?53,Dublin, Ireland, August 23-29 2014.Key Event Detection in Video using ASR and Visual DataNiraj Shrestha Aparna N. VenkitasubramanianKU Leuven, Belgium{niraj.shrestha, Aparna.NuraniVenkitasubramanian,Marie-Francine.Moens}@cs.kuleuven.beMarie-Francine MoensAbstractMultimedia data grow day by day which makes it necessary to index them automatically and effi-ciently for fast retrieval, and more precisely to automatically index them with key events.
In thispaper, we present preliminary work on key event detection in British royal wedding videos usingautomatic speech recognition (ASR) and visual data.
The system first automatically acquireskey events of royal weddings from an external corpus such as Wikipedia, and then identifiesthose events in the ASR data.
The system also models name and face alignment to identify thepersons involved in the wedding events.
We compare the results obtained with the ASR outputwith results obtained with subtitles.
The error is only slightly higher when using ASR output inthe detection of key events and their participants in the wedding videos compared to the resultsobtained with subtitles.1 IntroductionWith the increase of multimedia data widely available on the Web and in social media, it becomesnecessary to automatically index the multimedia resources with key events for information search andmining.
For instance, it is not possible to manually index all the frames of a video.
Automaticallyindexing multimedia data with key events makes the retrieval and mining effective and efficient.Event detection is an important and current research problem in the field of multimedia informationretrieval.
Most of the event detection in video is done by analyzing the visual features using manuallytranscribed data.
In this paper, we propose key event detection in British royal wedding videos usingautomatic speech recognition (ASR) data and where possible also to recognize the actors involved in therecognized events using visual and textual data.
An event is something that happens at a certain momentin time and at a certain location possibly involving different actors.
Events can be quite specific as in thiscase the key events are the typical events that make up a royal wedding scenario.
For example, events like?design of cake/dress/bouquet?, ?couple heading to Buckingham palace?, ?appearing on balcony?
etc.
arekey events in British royal wedding video.
Figure 1 shows an example of a frame containing an event withits actors, together with the associated subtitle and ASR output.
While most works in this domain havefocussed on clean textual content such as manual transcripts or subtitles, which are difficult to acquire, weuse the output of an ASR system.
While the event detection and name-face alignment problem by itselfis already quite difficult, the nature of the ASR text adds an additional complexity.
ASR data is noisyand inaccurate, it does not contain some parts of the actual spoken text, and does not contain sentenceboundaries.
Figure 2 illustrates this problem.
For the key events, the system first acquires the necessaryknowledge from external corpora - in our case Wikipedia articles associated with royal weddings.
Thenthe system identifies the key events in the ASR data.
The system also models name and face alignmentto identify the persons involved in the wedding events.
We perform named entity recognition in the textassociated with a window of frames to first generate a noisy label for the faces occurring in the framesand this rough alignment is refined using an Expectation-Maximization (EM) algorithm.
We comparethe results obtained with the ASR output with results obtained with subtitles.
The error is only slightlyThis work is licenced under a Creative Commons Attribution 4.0 International License.
License details: http://creativecommons.org/licenses/by/4.0/46Sub-title: ?Outside, fully 150,000 people with unbounded enthusiasm acclaimed Princess Margaret andher husband when they appeared on the balcony.. .
.
?ASR: ?outside only a hundred and 50 people on TV and using it as a .
.
.
?Figure 1: An example of a frame containing an event with associated subtitle and ASR outputhigher when using ASR output in the detection of key events and their participants in the wedding videoscompared to the results obtained with subtitles.
The methodology that we propose can be applied for thedetection of many different types of video events.2 Related workEvent detection has some relationship with Topic Detection and Tracking (TDT) and with concept de-tection.
TDT regards the detection and tracking over time of the main event of a news story and is achallenging problem in the field of text and visual analysis.
Although widely studied in text (Allan,2002), (Allan et al., 2005), (Mei and Zhai, 2005), (Wang et al., 2007), (Zhao et al., 2007), topic detectionin video is still not well studied.
An event in this context is usually broader in scope than the eventswe want to recognize in wedding videos in this paper.
In the multimedia research community, most ofthe works focus on concept detection like in (Liu et al., 2008), (Yang et al., 2007), (Snoek et al., 2006)rather than event detection.
A concept detection task is different from event detection as a concept canbe defined as any object or specific configuration of objects.
Any frame then can be labelled with someconcept descriptor (e.g., church, cake, etc.).
While in an event, there is a start and end time in betweenwhich something happens, and in video, an event is represented by a sequence of frames.Event detection is a challenging problem which is not well studied.
Only few event detection systemsthat process video exist.
They recognize events such as goal, run, tackle in a soccer game, or recognizespecific actions in news video (e.g., meeting of two well-known people) or in a surveillance video (e.g.,unusual event).
Event detection in video is often related to sports like basketball (Saur et al., 1997),soccer (Yow et al., 1995) and baseball (Kawashima et al., 1998) (Rui et al., 2000).
(Wang et al., 2008)developed a model based on a multi-resolution, multi-source and multi-modal bootstrapping frameworkthat exploits knowledge of sub-domains for concept detection in news video.
(Adam et al., 2008) de-veloped an algorithm based on multiple local monitors which collect low-level statistics to detect certaintypes of unusual events in surveillance video.Most of these works rely only on visual analysis (e.g., detection of certain motion patterns) to identifyevents in video and the event detection is performed with a supervised learning method, where a modelis trained on manually annotated examples of known events.
In this paper, we propose a novel idea inwhich the system learns events from an external corpus like Wikipedia and identifies those events in theASR or subtitle data of the video.
In addition, we identify the persons involved in an event based on theanalysis of visual and textual data.47Figure 2: An example showing sub-title vs. ASR data3 MethodologyThe main objective of this work is to identify and index key events in videos using ASR data along withkey actors involved in the event.
We start by identifying key events related to a certain domain, usingexternal corpora.
In addition, the proposed method involves pre-processing of the textual and visual data.At 11.30, Elizabeth entered the abbey on her father?s arm, but they did not head straight down the aisleas expected.Figure 3: Sequence of frames showing the anchor talking about an event of the wedding, but there is novisual appearance of the event.3.1 Acquiring background knowledgeOur approach for identifying key events in weddings exploits external text corpora.
We use two corpora:1.
A genre-specific corpus: a set of pages specific to the topic, for example, from Wikipedia - toidentify events associated with the topic.482.
A generic corpus, used to weigh the events identified in the genre-specific corpus.The process is as follows.
We first collect content from Wikipedia articles relevant for Britain?s royalweddings1in the form of nine documents.
These articles include both pages related to weddings, such asthese of Diana and Charles, that were mentioned in our test videos as well as pages about other Britishroyal weddings not shown in the videos, such as the wedding of Kate and William.
This set of articlesformed our wedding corpus for learning typical wedding events.
The generic corpus is formed by allEnglish Wikipedia articles.From each document of this corpus we extract events together with their arguments (subject and objectarguments) using a state-of-the-art event annotator2.
This tool uses linguistic features such as the resultsof a dependency parse of a sentence, to detect the events and their arguments in a text.
Next, we usea data mining technique to find frequent word patterns that signal the event and its arguments in thewedding articles, we keep each event that has sufficient support in the wedding articles and weigh it bya factor that is inversely proportional to the frequency of this event in the more general corpus.
We keepthe N highest weighted events from the obtained ranked list, where N is determined by whether we wantto keep the most common wedding events or include also more rare events.
The list obtained has itemssuch as ?to announce engagement?, ?to make dress?, ?to make cake?
etc, which are typical for weddings.We report here on preliminary work and acknowledge that the methodology can be largely refined.3.2 Detecting person namesIn royal wedding videos, there are many persons who appear in the video like anchor, interviewee, thepersons married or to be married, the dress designer, the bouquet designer, the cake maker, the friendsetc.
As in this preliminary work we are only interested in the brides and bridegrooms (which are also themost important persons when indexing the video) we use a gazetteer with their names for recognizingthe names in the texts.3.3 Detecting the faces of personsIn the video key frames are extracted at the rate of 1 frame per second using (ffmpeg, 2012), whichensures that no faces appearing in the video are omitted.
To detect the faces in the video, a face detectortool from (Bradski, 2000) is used.
Next, we extract the features from the faces detected in the video.Although there are several dedicated facial feature extraction methods such as (Finkel et al., 2005),(Strehland Ghosh, 2003), in this implementation, we use a simple bag-of-visual-words model (Csurka et al.,2004).Once feature vectors are built, clustering of the bounding boxes of the detected faces is performed.Each object is, then, compared to the cluster centers obtained and is replaced with the closest center.The clustering is done using Elkan?s k-means algorithm (Jain and Obermayer, 2010) which produces thesame results as the regular k-means algorithm, but is computationally more efficient.
This acceleratedalgorithm eliminates some distance calculations by applying the triangle inequality and by keeping trackof lower and upper bounds for distances between points and centers.
This algorithm, however, needs thenumber k of clusters present in the data.
Since we are primarily interested in the brides and bridegroomsand since there are seven weddings shown in the video, we experiment with values of k equal to 7*2 = 14.Although this approach very likely introduces errors in the clustering as we do not know beforehand howmany persons apart from the couple appear in the chosen key frames, it showed to be a better strategythan trying to align all persons mentioned in the texts.
The clustering is performed using an Euclideandistance metric.3.4 Name and face alignmentIf a key frame contains a face, then we identify the corresponding ASR or subtitle data that co-occur in afixed time window with this frame.
Further, the names occurring in the textual data are listed as possiblenames for the frame.
As a result, it is possible that an entity mentioned in the text is suggested for several1http://en.wikipedia.org/wiki/Category:British royal weddings2http://ariadne.cs.kuleuven.be/TERENCEStoryService/49Table 1: Names and faces alignment results on subtitle vs. ASR data on eventsSubtitle ASRP R F1P R F1Textual 38.095 21.622 27.586 36.585 17.857 24EM 41.304 25.676 31.667 40.426 22.619 29.008Table 2: WinDiff score on event identification on subtitle vs. ASR data on the union settingSubtitle ASR11.06 13.80key frames.
However, when there is no corresponding text, or when the text does not contain personentities, no name is suggested for the key frame.Name and face alignment in royal wedding video is difficult and complicated since the video containsmany other faces of persons mentioned above.
Sometimes the anchor or designer talks about the coupleinvolved in the wedding, but there is no appearance of this couple in the corresponding video key frameas shown in figure 3.We minimize this problem of name and face alignment by using the EM algorithm cited in (Pham etal., 2010).
Alignment is the process of mapping the faces in the video to the names mentioned in thetextual data.
For each frame, the most probable alignment scheme has to be chosen from all possibleschemes.
The EM algorithm has an initialization step followed by the iterative E- and M-steps.
TheE-step estimates the likelihood of each alignment scheme for a frame, while the M-step updates theprobability distribution based on the estimated alignments over all key frames of the video.3.5 Event identification in subtitle and ASR data with person involvement (if any)Once the system has learned the events from the Wikipedia data, it identifies the events from the subtitles.The process is as follow: the system scans each subtitle for the key words from the event list.
If the keyword appears in the subtitle data, then it is treated as the occurrence of the event and stores the set offrames that co-occur with that subtitle.
The name and face alignment module already might have yieldeda list of names present in this subtitle if there is any person involved.
If that is the case, then the namesare assigned to the events identified.The same process is repeated using ASR data.4 Experimental setupIn this section, we describe the dataset, experimental setup and the metrics used for evaluation.4.1 Datasets and ground truth annotationsThe data used in our experiments is the DVD on Britain?s Royal Weddings published by the BBC.
Theduration of this video is 116 minutes at a frame rate of 25 frames per second, and the frame resolutionis 720x576 pixels.
Frames are extracted at the rate of one frame per second using the ffmpeg tool(ffmpeg, 2012).
Faces in the frames are annotated manually using the Picasa tool for building the groundtruth for evaluation.
This tool is very handy and user-friendly to tag the faces.
We have found thatthere are 69 persons including British wedding couples in the video.
The subtitles came along with theDVD which are already split into segments of around 3 seconds.
We use the (FBK, 2013) system toobtain the ASR data of the videos.
Since the (FBK, 2013) system takes only sound (.mp3 file) as input,we have converted the video into a mp3 file using (ffmpeg, 2012).
The obtained ASR data is then inXML format without any sentence boundaries so we have converted the ASR data into segments in therange of three seconds, which is standard when presenting subtitles in video.
It is clear that the ASRtranscription contains many words that are incorrectly transcribed.
It is also visible that the ASR systemdoes not recognize or misspells many words from the actual speech.
As mentioned above, we have built50a gazetteer of the couples?
names.
A set of events are recognized by our system as being important inthe context of weddings.
To evaluate the quality of these events, the events in the video were annotatedby two annotators independently.
This annotation includes the actual event, and the start and end timesof the event.
These two sets with annotations form the groundtruth.
To be able to compare the systemgenerated events with the ground truth events, we adopt a two-step approach.
First, we combine thecorresponding ground truth entries from different annotators into one sequence of frames.
Suppose oneentry in a ground truth file (GT (a)) by one annotator contains the following start (xa) and end (ya)time range: GT (a) : [xa, ya], and the corresponding entry in the other ground truth file (GT (b)) (by thesecond annotator) contains the following start (xb) and end (yb) time range: GT (b) : [xb, yb].
Mergingof the ground truth event ranges can be done in different ways, but we report here on the union of thetwo ranges.GT (a) ?GT (b) = [min(xa, xb),max(ya, yb)] (1)4.2 Evaluation MetricsLet FL be the final list of name and face alignment retrieved by our system for all the faces detected inall frames, and GL the complete ground truth list.
To evaluate the name and face alignment task, we usestandard precision (P ), recall (R) and F1scores for evaluation:P =|FL ?GL||FL|R =|FL ?GL||GL|F1= 2 ?P ?RP +RTo evaluate correctness of event segment boundaries, precision and recall are too strict since theypenalize boundaries placed very close to the ground truth boundaries.
We use the WindowDiff (Pevznerand Hearst, 2002) metric that measures the difference between the ground truth segment GT and thesegment SE found by the machine originally designed for text segmentation.
For our scenario, thismetric is defined as follows:WD(GT, SE) =1M ?
kM?k?i=1(|b(GTi, GTi+k)?
b(SEi, SEi+k)| > 0) (2)where M = 7102, is the number of frames extracted, k = 1, is the window size and b(i, j) representsthe number of boundaries between frame indices i and j.5 ResultsFigure 4: Events learned from the Wikipedia data and their identification in the subtitles and ASR by thesystem515.1 Evaluation of the extraction of wedding events from WikipediaFigure 4 shows which key events typical for royal weddings the system has learned from Wikipedia dataand how it found these events in the subtitles and the ASR data.
It is seen from figure 4 that the systemcould not learn many events that are important to wedding events, but the system recognized the eventsthat it has learned quite accurately in the subtitles and ASR data.5.2 Evaluation of the event segmentation and recognitionTable 2 shows the results of WinDiff score obtained on subtitles versus ASR data on the union settingdiscussed in 4.1.
Though the error rate is more or less the same, it degrades in ASR data which isobviously due to the different ASR errors.
The error rate is increased by 2.74% in ASR data using awindow size of 1.
Here a window size 1 is equivalent to one second so it corresponds to one frame.In this case the system tries to find the event boundaries in each frame and evaluates these against theground truth event boundaries.5.3 Evaluation of the name-face alignmentsTable 1 shows the result of the name and face alignment given the detected events.
Though the resultis not quite satisfactory even after applying the EM algorithm, there are many bottlenecks that need tobe tackled.
Many parts of the video contain interviews.
Interviewees and anchors mostly talk about thecouples that are married or are to be married, but the couples are not shown which might cause errors inthe name and face alignment.6 Conclusion and future workIn this paper, we have presented ongoing research on event detection in video using ASR and visualdata.
To some extent the system is able to learn key events from relevant external corpora.
The eventidentification process is quite satisfactory as the system learns from external corpora.
If the system wouldhave learnt the events from external corpora good enough, it might identify events very well from subtitleor ASR data.
We are interested in improving the learning process from external corpora in further work.Finding event boundaries in the frame sequence corresponding to a subtitle or ASR data where the eventis mentioned is still a challenging problem because an event key word might be identified in a subtitlesegment or in a sentence which actually may not correspond to what is shown the aligned frames.
Wehave also tried to implement name and face alignment techniques to identify persons involved in theevent.
As a further improvement of our system, we need to find how to deal with the many interviews inthis type of videos which might improve the alignment of names and faces.ReferencesA.
Adam, E. Rivlin, I. Shimshoni, and D. Reinitz.
2008.
Robust real-time unusual event detection using multiplefixed-location monitors.
Pattern Analysis and Machine Intelligence, IEEE Transactions on, 30(3):555?560,March.James Allan, Stephen Harding, David Fisher, Alvaro Bolivar, Sergio Guzman-Lara, and Peter Amstutz.
2005.Taking topic detection from evaluation to practice.
In Proceedings of the 38th Annual Hawaii InternationalConference on System Sciences (HICSS?05) - Track 4 - Volume 04, HICSS ?05, pages 101.1?, Washington, DC,USA.
IEEE Computer Society.James Allan, editor.
2002.
Topic Detection and Tracking: Event-based Information Organization.
Kluwer Aca-demic Publishers, Norwell, MA, USA.G.
Bradski.
2000.
Opencv face detector tool.
Dr. Dobb?s Journal of Software Tools.
Available at http://opencv.org/downloads.html.Gabriella Csurka, Christopher R. Dance, Lixin Fan, Jutta Willamowski, and Cdric Bray.
2004.
Visual categoriza-tion with bags of keypoints.
In Workshop on Statistical Learning in Computer Vision, ECCV, pages 1?22.FBK.
2013.
FBK ASR transcription.
Available at https://hlt-tools.fbk.eu/tosca/publish/ASR/transcribe.52ffmpeg.
2012. ffmpeg audio/video tool.
Available at http://www.ffmpeg.org.Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning.
2005.
Incorporating non-local information intoinformation extraction systems by Gibbs sampling.
In Proceedings of ACL, pages 363?370.Brijnesh J. Jain and Klaus Obermayer.
2010.
Elkan?s k-means algorithm for graphs.
In Proceedings of the 9thMexican International Conference on Artificial Intelligence: Conference on Advances in Soft Computing: PartII, MICAI?10, pages 22?32, Berlin, Heidelberg.
Springer-Verlag.Toshio Kawashima, Kouichi Tateyama, Toshimasa Iijima, and Yoshinao Aoki.
1998.
Indexing of baseball telecastfor content-based video retrieval.
In ICIP (1), pages 871?874.Ken-Hao Liu, Ming-Fang Weng, Chi-Yao Tseng, Yung-Yu Chuang, and Ming-Syan Chen.
2008.
Association andtemporal rule mining for post-filtering of semantic concept detection in video.
Trans.
Multi., 10(2):240?251,February.Qiaozhu Mei and ChengXiang Zhai.
2005.
Discovering evolutionary theme patterns from text: An exploration oftemporal text mining.
In Proceedings of the Eleventh ACM SIGKDD International Conference on KnowledgeDiscovery in Data Mining, KDD ?05, pages 198?207, New York, NY, USA.
ACM.Lev Pevzner and Marti A. Hearst.
2002.
A critique and improvement of an evaluation metric for text segmentation.Computational Linguistics, 28(1):19?36.Phi The Pham, M. F. Moens, and T. Tuytelaars.
2010.
Cross-media alignment of names and faces.
IEEE Transac-tions on Multimedia, 12(1):13?27, January.Yong Rui, Anoop Gupta, and Alex Acero.
2000.
Automatically extracting highlights for tv baseball programs.
InProceedings of the Eighth ACM International Conference on Multimedia, MULTIMEDIA ?00, pages 105?115.Drew D. Saur, Yap-Peng Tan, Sanjeev R. Kulkarni, and Peter J. Ramadge.
1997.
Automated analysis and annota-tion of basketball video.
In Storage and Retrieval for Image and Video Databases (SPIE), pages 176?187.Cees G. M. Snoek, Marcel Worring, Jan C. van Gemert, Jan-Mark Geusebroek, and Arnold W. M. Smeulders.2006.
The challenge problem for automated detection of 101 semantic concepts in multimedia.
In Proceedingsof the 14th Annual ACM International Conference on Multimedia, MULTIMEDIA ?06, pages 421?430, NewYork, NY, USA.Alexander Strehl and Joydeep Ghosh.
2003.
Cluster ensembles ?
a knowledge reuse framework for combiningmultiple partitions.
J. Mach.
Learn.
Res., 3:583?617, March.Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard Sproat.
2007.
Mining correlated bursty topic patternsfrom coordinated text streams.
In Proceedings of the 13th ACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?07, pages 784?793.Gang Wang, Tat-Seng Chua, and Ming Zhao.
2008.
Exploring knowledge of sub-domain in a multi-resolutionbootstrapping framework for concept detection in news video.
In Proceedings of the 16th ACM InternationalConference on Multimedia, MM ?08, pages 249?258, New York, NY, USA.
ACM.Jun Yang, Rong Yan, and Alexander G. Hauptmann.
2007.
Cross-domain video concept detection using adaptiveSVMs.
In Proceedings of the 15th International Conference on Multimedia, MULTIMEDIA ?07, pages 188?197, New York, NY, USA.
ACM.Dennis Yow, Boon lock Yeo, Minerva Yeung, and Bede Liu.
1995.
Analysis and presentation of soccer highlightsfrom digital video.
pages 499?503.Qiankun Zhao, Prasenjit Mitra, and Bi Chen.
2007.
Temporal and information flow based event detection fromsocial text streams.
In Proceedings of the 22nd National Conference on Artificial Intelligence - Volume 2,AAAI?07, pages 1501?1506.53
