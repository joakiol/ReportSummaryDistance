Automatic Detection of Poor Speech Recognitionat the Dialogue LevelDiane J. Litman, Marilyn A. Walker and Michael S. KearnsAT&T Labs Research180 Park Ave, Bldg 103Florham Park, N.J. 07932{diane, walker, mkearns}@research, att.
comAbstractThe dialogue strategies used by a spoken dialoguesystem strongly influence performance and user sat-isfaction.
An ideal system would not use a singlefixed strategy, but would adapt o the circumstancesat hand.
To do so, a system must be able to identifydialogue properties that suggest adaptation.
Thispaper focuses on identifying situations where thespeech recognizer is performing poorly.
We adopta machine learning approach to learn rules froma dialogue corpus for identifying these situations.Our results how a significant improvement over thebaseline and illustrate that both lower-level acousticfeatures and higher-level dialogue features can af-fect the performance of the learning algorithm.1 IntroductionBuilders of spoken dialogue systems face a numberof fundamental design choices that strongly influ-ence both performance and user satisfaction.
Ex-amples include choices between user, system, ormixed initiative, and between explicit and implicitconfirmation of user commands.
An ideal systemwouldn't make such choices a priori, but ratherwould adapt to the circumstances at hand.
For in-stance, a system detecting that a user is repeatedlyuncertain about what to say might move from user tosystem initiative, and a system detecting that speechrecognition performance is poor might switch toa dialogUe strategy with more explicit prompting,an explicit confirmation mode, or keyboard inputmode.
Any of these adaptations might have beenappropriate in dialogue D1 from the Annie sys-tem (Kamm et al, 1998), shown in Figure 1.In order to improve performance through suchadaptation, a system must first be able to identify, inreal time, salient properties of an ongoing dialoguethat call for some useful change in system strategy.In other words, adaptive systems hould try to auto-matically identify actionable properties of ongoingdialogues.Previous work has shown that speech recognitionperformance is an important predictor of user satis-faction, and that changes in dialogue behavior im-pact speech recognition performance (Walker et al,1998b; Litman et al, 1998; Kamm et al, 1998).Therefore, in this work, we focus on the task of au-tomatically detecting poor speech recognition per-formance in several spoken dialogue systems devel-oped at AT&T Labs.
Rather than hand-crafting rulesthat classify speech recognition performance in anongoing dialogue, we take a machine learning ap-proach.
We begin with a collection of system logsfrom actual dialogues that were labeled by humansas having had "good" or "bad" speech recognition(the training set).
We then apply standard machinelearning algorithms to this training set in the hopeof discovering, in a principled manner, classifiersthat can automatically detect poor speech recogni-tion during novel dialogues.In order to train such classifiers, we must providethem with a number of "features" of dialogues de-rived from the system logs that might allow the sys-tem to automatically identify poor recognition per-formance.
In addition to identifying features thatprovide the best quantitative solutions, we are alsointerested in comparing the performance of classi-fiers derived solely from acoustic features or from"high-level" dialogue features, and from combina-tions of these and other feature types.
Note that weare free to invent as many features as we like, aslong as they can be computed in real time from theraw system logs.Since the dialogue systems we examine use au-tomatic speech recognition (ASR), one obvious fea-ture available in the system log is a per-utterancescore from the speech recognizer representing its"confidence" in its interpretation f the user's utter-ance (Zeljkovic, 1996).
For dialogue D1, the recog-nizer's output and the associated confidence scores309Actual DialogueAI: Annie here.U1: Hello Annie, could you give me the information on a meeting I have tomorrow?A2: What was that?U2: I need to find out uh, a meeting time and place.
Kim sent me a message on it.A3: For all options ay help me out.U3: Help me out.A4: Annie here.U4: Help.A5: Help.
Say call or say send a message, followed by a label or a number.Or say get my messages...U5: Get messages.
(user interrupts A5)A6: Speech and Image Processing Lab.U6: Help.A7: Help.
Say the first and last name ...U7: Kim.ASR Interpretation ASR ConfidenceREJECT -4.84REJECT -4.20REJECT -6.35help - 1.74get me sips -3.33help - 1.78annie -3.81Figure 1: Example dialogue xcerpt D1 with Annie.are in the last two columns of Figure 1.
These con-fidence measures are based on the recognizer's lan-guage and acoustic models.
The confidence scoresare typically used by the system to decide whetherit believes it has correctly understood the user's ut-terance.
When the confidence score falls below athreshold efined for each system, the utterance isconsidered a rejection (e.g., utterances U1, U2, andU3 in D1).
Note that since our classification prob-lem is defined by speech recognition performance,it might be argued that this confidence feature (orfeatures derived from it) suffices for accurate classi-fication.However, an examination of the transcript in D1suggests that other useful features might be derivedfrom global or high-level properties of the dialoguehistory, such as features representing the system'srepeated use of diagnostic error messages (utter-ances A2 and A3), or the user's repeated requestsfor help (utterances U4 and U6).Although the work presented here focuses ex-clusively on the problem of automatically detectingpoor speech recognition, a solution to this problemclearly suggests ystem reaction, such as the strat-egy changes mentioned above.
In this paper, we re-port on our initial experiments, with particular at-tention paid to the problem definition and method-ology, the best performance we obtain via a machinelearning approach, and the performance differencesbetween classifiers based on acoustic and higher-level dialogue features.2 Systems, Data, MethodsThe learning experiments that we describe hereuse the machine learning program RIPPER (Co-hen, 1996) to automatically induce a "poor speechrecognition performance" classification model froma corpus of spoken dialogues.
1 RIPPER (like otherlearning programs, such as c5.0 and CART) takesas input the names of a set of classes to be learned,the names and possible values of a fixed set of fea-tures, training data specifying the class and featurevalues for each example in a training set, and out-puts a classification model for predicting the classof future examples from their feature representation.In RIPPER, the classification model is learned usinggreedy search guided by an information gain metric,and is expressed as an ordered set of if-then rules.We use RIPPER for our experiments because it sup-ports the use of "set-valued" features for represent-ing text, and because if-then rules are often easierfor people to understand than decision trees (Quin-lan, 1993).
Below we describe our corpus of dia-logues, the assignment of classes to each dialogue,the extraction of features from each dialogue, andour learning experiments.Corpus: Our corpus consists of a set of 544 di-alogues (over 40 hours of speech) between humansand one of three dialogue systems: ANNIE (Kammet al, 1998), an agent for voice dialing and mes-saging; ELVIS (Walker et al, 1998b), an agentfor accessing email; and TOOT (Litman and Pan,1999), an agent for accessing online train sched-ules.
Each agent was implemented using a general-purpose platform for phone-based spoken dialoguesystems (Kamm et al, 1997).
The dialogues wereobtained in controlled experiments designed to eval-uate dialogue strategies for each agent.
The exper-~We also ran experiments using the machine l arning pro-gram BOOSTEXTER (Schapire and Singer, To appear), with re-sults imilar to those presented below.310iments required users to complete a set of applica-tion tasks in conversations with a particular versionof the agent.
The experiments resulted in both a dig-itized recording and an automatically produced sys-tem log for each dialogue.Class Assignment: Our corpus is used to con-struct he machine learning classes as follows.
First,each utterance that was not rejected by automaticspeech recognition (ASR) was manually labeled asto whether it had been semantically misrecognizedor not.
2 This was done by listening to the record-ings while examining the corresponding system log.If the recognizer's output did not correctly capturethe task-related information in the utterance, it waslabeled as a misrecognition.
For example, in Fig-ure 1 U4 and U6 would be labeled as correct recog-nitions, while U5 and U7 would be labeled as mis-recognitions.
Note that our labeling is semanticallybased; if U5 had been recognized as "play mes-sages" (which invokes the same application com-mand as "get messages"), then U5 would have beenlabeled as a correct recognition.
Although this la-beling needs to be done manually, the labeling isbased on objective criteria.Next, each dialogue was assigned a class of ei-ther good or bad, by thresholding on the percentageof user utterances that were labeled as ASR seman-tic misrecognitions.
We use a threshold of 11% tobalance the classes in our corpus, yielding 283 goodand 261 bad dialogues.
3 Our classes thus reflect rel-ative goodness with respect o a corpus.
DialogueD1 in Figure 1 would be classified as "bad", be-cause U5 and U7 (29% of the user utterances) aremisrecognized.Feature Extraction: Our corpus is used to con-struct the machine learning features as follows.Each dialogue is represented in terms of the 23primitive features in Figure 2.
In RIPPER, fea-ture values are continuous (numeric), set-valued, orsymbolic.
Feature values were automatically com-puted from system logs, based on five types ofknowledge sources: acoustic, dialogue efficiency,dialogue quality, experimental parameters, and lexi-cal.
Previous work correlating misrecognition ratewith acoustic information, as well as our own2These utterance labelings were produced uring a previousset of experiments investigating the performance evaluation ofspoken dialogue systems (Walker et al, 1997; Walker et al,1998a; Walker et al, 1998b; Kamm et al, 1998; Litman et al,1998; Litman and Pan, 1999).3This threshold is consistent with a threshold inferred fromhuman judgements (Litman, 1998).?
Acoustic Features-mean confidence, pmisrecs%l, pmisrecs%2, pmis-recs%3, pmisrecs%4?
Dialogue Efficiency Features- elapsed time, system turns, user turns?
Dialogue Quality Features- rejections, timeouts, helps, cancels, bargeins (raw)- rejection%, timeout%, help%, cancel%, bargein% (nor-malized)?
Experimental Parameters Features- system, user, task, condition?
Lexical Features- ASR textFigure 2: Features for spoken dialogues.hypotheses about the relevance of other types ofknowledge, contributed to our features.The acoustic, dialogue efficiency, and dialoguequality features are all numeric-valued.
The acous-tic features are computed from each utterance'sconfidence (log-likelihood) scores (Zeljkovic,1996).
Mean confidence represents the averagelog-likelihood score for utterances not rejected ur-ing ASR.
The four pmisrecs% (predicted percent-age of misrecognitions) features represent differ-ent (coarse) approximations to the distribution oflog-likelihood scores in the dialogue.
Each pmis-recs% feature uses a fixed threshold value to predictwhether a non-rejected utterance is actually a mis-recognition, then computes the percentage of userutterances in the dialogue that correspond to thesepredictedmisrecognitions.
(Recall that our dialogueclassifications were determined by thresholding onthe percentage of actual misrecognitions.)
For in-stance, pmisrecs%1 predicts that if a non-rejectedutterance has a confidence score below -2  then itis a misrecognition.
Thus in Figure 1, utterances U5and U7 would be predicted as misrecognitions u ingthis threshold.
The four thresholds used for the fourpmisrecs% features are -2 , -3 , -4 , -5 ,  and werechosen by hand from the entire dataset to be infor-mative.The dialogue efficiency features measure howquickly the dialogue is concluded, and includeelapsed time (the dialogue length in seconds), andsystem turns and user turns (the number of turns foreach dialogue participant).311mean confidence pmisrecs%1 pmisrecs%2 pmisrecs%3 pmisrecs%4 elapsed time system turns user turns-2.7 29 29 0 0 300 7 7rejections timeouts helps cancels bargeins rejection% timeout% help%3 0 2 0 1 43 0 29cancel% bargein% system user task condition0 14 annie mike day 1 novices without utorialASR textREJECT REJECT REJECT help get me sips help annieFigure 3: Feature representation f dialogue D1.The dialogue quality features attempt to captureaspects of the naturalness of the dialogue.
Rejec-tions represents he number of times that the sys-tem plays special rejection prompts, e.g., utterancesA2 and A3 in dialogue D1.
This occurs wheneverthe ASR confidence score falls below a thresholdassociated with the ASR grammar for each systemstate (where the threshold was chosen by the systemdesigner).
The rejections feature differs from thepmisrecs% features in several ways.
First, the pmis-recs% thresholds are used to determine misrecogni-tions rather than rejections.
Second, the pmisrecs%thresholds are fixed across all dialogues and are notdependent on system state.
Third, a system rejectionevent directly influences the dialogue via the rejec-tion prompt, while the pmisrecs% thresholds haveno corresponding behavior.Timeouts represents he number of times that thesystem plays special timeout prompts because theuser hasn't responded within a pre-specified timeframe.
Helps represents he number of times that hesystem responds to a user request with a (context-sensitive) help message.
Cancels represents thenumber of user's requests to undo the system's pre-vious action.
Bargeins represents the number ofuser attempts to interrupt he system while it isspeaking.
4 In addition to raw counts, each featureis represented in normalized form by expressing thefeature as a percentage.
For example, rejection%represents he number of rejected user utterances di-vided by the total number of user utterances.In order to test the effect of having the maxi-mum amount of possibly relevant information avail-able, we also included a set of features describ-ing the experimental parameters for each dialogue(even though we don't expect rules incorporatingsuch features to generalize).
These features capturethe conditions under which each dialogue was col-4Since the system automatically detects when a bargein oc-curs, this feature could have been automatically ogged.
How-ever, because our system did not log bargeins, we had to hand-label them.lected.
The experimental parameters features eachhave a different set of user-defined symbolic values.For example, the value of the feature system is either"annie", "elvis", or "toot", and gives RIPPER the op-tion of producing rules that are system-dependent.The lexical feature ASR text is set-valued, andrepresents he transcript of the user's utterances asoutput by the ASR component.Learning Experiments: The final input forlearning is training data, i.e., a representation f aset of dialogues in terms of feature and class values.In order to induce classification rules from a varietyof feature representations our training data is rep-resented ifferently in different experiments.
Ourlearning experiments can be roughly categorized asfollows.
First, examples are represented using all ofthe features in Figure 2 (to evaluate the optimal levelof performance).
Figure 3 shows how DialogueD1 from Figure 1 is represented using all 23 fea-tures.
Next, examples are represented using only thefeatures in a single knowledge source (to compara-tively evaluate the utility of each knowledge sourcefor classification), as well as using features fromtwo or more knowledge sources (to gain insight intothe interactions between knowledge sources).
Fi-nally, examples are represented using feature setscorresponding to hypotheses in the literature (to em-pirically test theoretically motivated proposals).The output of each machine learning experimentis a classification model learned from the trainingdata.
To evaluate these results, the error rates of thelearned classification models are estimated usingthe resampling method of cross-validation (Weissand Kulikowski, 1991).
In 25-fold cross-validation,the total set of examples is randomly divided into25 disjoint est sets, and 25 runs of the learning pro-gram are performed.
Thus, each run uses the exam-pies not in the test set for training and the remain-ing examples for testing.
An estimated error rate isobtained by averaging the error rate on the testingportion of the data from each of the 25 runs.312Features Used Accuracy (Standard Error)BASELINE 52%REJECTION% 54.5 % (2.0)EFFICIENCY 61.0 % (2.2)EXP-PARAMS 65.5 % (2.2)DIALOGUE QUALITY (NORMALIZED) 65.9 % (1.9)MEAN CONFIDENCE 68.4 % (2.0)EFFICIENCY + NORMALIZED QUALITY 69.7 % (1.9)ASR TEXT 72.0 % (1.7)PMISRECS%3 72.6 % (2.0)EFFICIENCY + QUALITY + EXP-PARAMS 73.4 % (1.9)ALL FEATURES 77.4 % (2.2)Figure 4: Accuracy rates for dialogue classifiers using different feature sets, 25-fold cross-validation  544dialogues.
We use SMALL CAPS to indicate feature sets, and ITALICS to indicate primitive features listed inFigure 2.3 ResultsFigure 4 summarizes our most interesting experi-mental results.
For each feature set, we report accu-racy rates and standard errors resulting from cross-validation.
5 It is clear that performance depends onthe features that the classifier has available.
TheBASELINE accuracy rate results from simply choos-ing the majority class, which in this case means pre-dicting that the dialogue is always "good".
Thisleads to a 52% BASELINE accuracy.The REJECTION% accuracy rates arise from aclassifier that has access to the percentage of dia-logue utterances in which the system played a re-jection message to the user.
Previous research sug-gests that this acoustic feature predicts misrecogni-tions because users modify their pronunciation iresponse to system rejection messages in such a wayas to lead to further misunderstandings (Shriberg etal., 1992; Levow, 1998).
However, despite our ex-pectations, the REJECTION% accuracy rate is notbetter than the BASELINE at our desired level of sta-tistical significance.Using the EFFICIENCY features does improve theperformance of the classifier significantly above theBASELINE (61%).
These features, however, tendto reflect the particular experimental tasks that theusers were doing.The EXP-PARAMS (experimental parameters)features are even more specific to this dialoguecorpus than the efficiency features: these featuresconsist of the name of the system, the experimen-5Accuracy rates are statistically significantly different whenthe accuracies plus or minus twice the standard error do notoverlap (Cohen, 1995), p. 134.tal subject, the experimental task, and the experi-mental condition (dialogue strategy or user exper-tise).
This information alone allows the classifierto substantially improve over the BASELINE clas-sifter, by identifying particular experimental condi-tions (mixed initiative dialogue strategy, or noviceusers without utorial) or systems that were run withparticularly hard tasks (TOOT) with bad dialogues,as in Figure 5.
Since with the exception of the ex-perimental condition these features are specific tothis corpus, we wouldn't expect hem to generalize.if (condition =mixed) then badif (system =toot) then badif (condition =novices without utorial) then baddefault is goodFigure 5: EXP-PARAMS rules.The normalized DIALOGUE QUALITY featuresresult in a similar improvement in performance(65.9%).
6 However, unlike the efficiency and ex-perimental parameters features, the normalizationof the dialogue quality features by dialogue lengthmeans that rules learned on the basis of these fea-tures are more likely to generalize.Adding the efficiency and normalized quality fea-ture sets together (EFFICIENCY + NORMALIZEDQUALITY) results in a significant performance im-provement (69.7%) over EFFICIENCY alone.
Fig-ure 6 shows that this results in a classifier withthree rules: one based on quality alone (per-centage of cancellations), one based on efficiency6The normalized versions of the quality features did betterthan the raw versions.313alone (elapsed time), and one that consists of aboolean combination of efficiency and quality fea-tures (elapsed time and percentage of rejections).The learned ruleset says that if the percentage ofcancellations i greater than 6%, classify the dia-logue as bad; if the elapsed time is greater than 282seconds, and the percentage of rejections i  greaterthan 6%, classify it as bad; if the elapsed time is lessthan 90 seconds, classify it as badT; otherwise clas-sify it as good.
When multiple rules are applicable,RIPPER resolves any potential conflict by using theclass that comes first in the ordering; when no rulesare applicable, the default is used.i f  (cancel% > 6) then badif (elapsed time > 282 secs) A (rejection% > 6) then badif (elapsed time < 90 secs) then baddefault is goodfor the MEAN CONFIDENCE classifier (68.4%) isnot statistically different han that for the PMIS-RECS%3 classifier.
Furthermore, since the featuredoes not rely on picking an optimal threshold, itcould be expected to better generalize to new dia-logue situations.The classifier trained on (noisy) ASR lexical out-put (ASR TEXT) has access only to the speech rec-ognizer's interpretation f the user's utterances.
TheASR TEXT classifier achieves 72% accuracy, whichis significantly better than the BASELINE, REJEC-TION% and EFFICIENCY classifiers.
Figure 7 showsthe rules learned from the lexical feature alone.
Therules include lexical items that clearly indicate thata user is having trouble e.g.
help and cancel.
Theyalso include lexical items that identify particulartasks for particular systems, e.g.
the lexical itemp-m identifies atask in TOOT.Figure 6: EFFICIENCY + NORMALIZED QUALITYrules.We discussed our acoustic REJECTION% resultsabove, based on using the rejection thresholds thateach system was actually run with.
However, aposthoc analysis of our experimental data showedthat our systems could have rejected substantiallymore misrecognitions with a rejection threshold thatwas lower than the thresholds picked by the sys-tem designers.
(Of course, changing the thresh-olds in this way would have also increased the num-ber of rejections of correct ASR outputs.)
Re-call that the PMISRECS% experiments explored theuse of different hresholds to predict misrecogni-tions.
The best of these acoustic thresholds wasPMISRECS%3, with accuracy 72.6%.
This classi-fier learned that if the predicted percentage of mis-recognitions using the threshold for that feature wasgreater than 8%, then the dialogue was predicted tobe bad, otherwise it was good.
This classifier per-forms significantly better than the BASELINE, RE-JECTION% and EFFICIENCY classifiers.Similarly, MEAN CONFIDENCE is anotheracoustic feature, which averages confidence scoresover all the non-rejected utterances in a dialogue.Since this feature is not tuned to the applications,we did not expect it to perform as well as the bestPMISRECS% feature.
However, the accuracy rate7This rule indicates dialogues too short for the user to havecompleted the task.
Note that this role could not be appliedto adapting the system's behavior during the course of the dia-logue.if (ASR text contains cance l )  then badif (ASR text contains the)  A (ASR text contains get )  A (ASR textcontains TIMEOUT) then badif (ASR text contains today)  ^  (ASR text contains on) then badif (ASR text contains the)  A (ASR text contains p-m) then badif (ASR text contains to) then badif (ASR text contains he lp )  ^  (ASR text contains the)  ^  (ASR textcontains read)  then badif (ASR text contains he lp )  A (ASR text contains prev ious)  thenbadif (ASR text contains about )  then badif (ASR text contains change-s trategy) then baddefault is goodFigure 7: ASR TEXT rules.Note that the performance of many of the classi-fiers is statistically indistinguishable, e.g.
the per-formance of the ASR TEXT classifier is virtuallyidentical to the classifier PMISRECS%3 and the EF-FICIENCY + QUALITY + EXP-PARAMS classifier.The similarity between the accuracies for a rangeof classifiers uggests that the information providedby different feature sets is redundant.
As discussedabove, each system and experimental condition re-suited in dialogues that contained lexical items thatwere unique to it, making it possible to identify ex-perimental conditions from the lexical items alone.Figure 8 shows the rules that RIPPER learned whenit had access to all the features except for the lexicaland acoustic features.
In this case, RIPPER learnssome rules that are specific to the TOOT system.Finally, the last row of Figure 4 suggests that aclassifier that has access to ALL FEATURES may dobetter (77.4% accuracy) than those classifiers that314if (cancel% > 4) ^  (system = toot) then badif (system turns _> 26) ^  (rejection% _> 5 ) then badi f  (condition =mixed) ^  (user turns > 12 ) then badi f  (system = toot)/x (user turns > 14 ) then badi f  (cancels > 1) A (timeout% _> 11 ) then badif (elapsed time _< 87 secs) then baddefault is goodFigure 8: EFF IC IENCY + QUAL ITY  + EXP-PARAMSrules.have access to acoustic features only (72.6%) or tolexical features only (72%).
Although these dif-ferences are not statistically significant, they showa trend (p < .08).
This supports the conclusionthat different feature sets provide redundant infor-mation, and could be substituted for each other toachieve the same performance.
However, the ALLFEATURES classifier does perform significantly bet-ter than the EXP-PARAMS, DIALOGUE QUALITY(NORMALIZED), and MEAN CONFIDENCE clas-sifiers.
Figure 9 shows the decision rules that theALL FEATURES classifier learns.
Interestingly, thisclassifier does not find the features based on experi-mental parameters tobe good predictors when it hasother features to choose from.
Rather it combinesfeatures representing acoustic, efficiency, dialoguequality and lexical information.if (mean confidence _< -2.2) ^  (pmisrecs%4 _>6 ) then badif (pmisrecs%3 >_ 7 ) A (ASR text contains yes )  A (mean confidence_< -1.9) then badif (cancel% _> 4) then badif (system turns _> 29 ) ^  (ASR text contains message)  then badi f  (elapsed time <_ 90) then baddefault is goodFigure 9: ALL FEATURES rules.4 DiscussionThe experiments presented here establish severalfindings.
First, it is possible to give an objective def-inition for poor speech recognition at the dialoguelevel, and to apply machine learning to build clas-sifiers detecting poor recognition solely from fea-tures of the system log.
Second, with appropri-ate sets of features, these classifiers ignificantlyoutperform the baseline percentage of the majorityclass.
Third, the comparable performance of clas-sifiers constructed from rather different feature sets(such as acoustic and lexical features) suggest thatthere is some redundancy between these feature sets(at least with respect o the task).
Fourth, the factthat the best estimated accuracy was achieved usingall of the features uggests that even problems thatseem inherently acoustic may best be solved by ex-ploiting higher-level information.This work differs from previous work in focusingon behavior at the (sub)dialogue l vel, rather thanon identifying single misrecognitions at the utter-ance level (Smith, 1998; Levow, 1998; van Zanten,1998).
The rationale is that a single misrecognitionmay not warrant a global change in dialogue strat-egy, whereas a user's repeated problems communi-cating with the system might warrant such a change.While we are not aware of any other work that hasapplied machine learning to detecting patterns ug-gesting that the user is having problems over thecourse of a dialogue, (Levow, 1998) has appliedmachine learning to identifying single misrecogni-tions.
We are currently extending our feature setto include acoustic-prosodic features uch as thoseused by Levow, in order to predict misrecognitionsat both the dialogue level as well as the utterancelevel.We are also interested in the extension and gen-eralization of our findings in a number of additionaldirections.
In other experiments, we demonstratedthe utility of allowing the user to dynamically adaptthe system's dialogue strategy at any point(s) duringa dialogue.
Our results how that dynamic adapta-tion clearly improves ystem performance, with thelevel of improvement sometimes a function of thesystem's initial dialogue strategy (Litman and Pan,1999).
Our next step is to incorporate classifierssuch as those presented in this paper into a systemin order to support dynamic adaptation according torecognition performance.
Another area for futurework would be to explore the utility of using alter-native methods for classifying dialogues as good orbad.
For example, the user satisfaction measures wecollected in a series of experiments u ing the PAR-ADISE evaluation framework (Walker et al, 1998c)could serve as the basis for such an alternative clas-sification scheme.
More generally, in the same waythat learning methods have found widespread use inspeech processing and other fields where large cor-pora are available, we believe that the constructionand analysis of spoken dialogue systems is a ripedomain for machine learning applications.5 AcknowledgementsThanks to J. Chu-Carroll, W. Cohen, C. Kamm, M.Kan, R. Schapire, Y.
Singer, B. Srinivas, and S.315Whittaker for help with this research and/or paper.ReferencesPaul R. Cohen.
1995.
Empirical Methods for Arti-ficial Intelligence.
MIT Press, Boston.William Cohen.
1996.
Learning trees and ruleswith set-valued features.
In 14th Conference ofthe American Association of Artificial Intelli-gence, AAAI.C.
Kamm, S. Narayanan, D. Dutton, and R. Rite-nour.
1997.
Evaluating spoken dialog systemsfor telecommunication services.
In 5th EuropeanConference on Speech Technology and Commu-nication, EUROSPEECH 97.Candace Kamm, Diane Litman, and Marilyn A.Walker.
1998.
From novice to expert: The ef-fect of tutorials on user expertise with spoken di-alogue systems.
In Proceedings of the Interna-tional Conference on Spoken Language Process-ing, ICSLP98.Gina-Anne Levow.
1998.
Characterizing and rec-ognizing spoken corrections in human-computerdialogue.
In Proceedings of the 36th AnnualMeeting of the Association of Computational Lin-guistics, COLING/ACL 98, pages 736-742.Diane J. Litman and Shimei Pan.
1999.
Empiricallyevaluating an adaptable spoken dialogue system.In Proceedings of the 7th International Confer-ence on User Modeling (UM).Diane J. Litman, Shimei Pan, and Marilyn A.Walker.
1998.
Evaluating Response Strategies ina Web-Based Spoken Dialogue Agent.
In Pro-ceedings of ACL/COLING 98: 36th Annual Meet-ing of the Association of Computational Linguis-tics, pages 780-787.Diane J. Litman.
1998.
Predicting speech recog-nition performance from dialogue phenomena.Presented at the American Association for Arti-ficial Intelligence Spring Symposium Series onApplying Machine Learning to Discourse Pro-cessing.J.
Ross Quinlan.
1993.
C4.5: Programs for Ma-chine Learning.
San Mateo, CA: Morgan Kauf-mann.Robert E. Schapire and Yoram Singer.
To appear.Boostexter: A boosting-based system for text cat-egorization.
Machine Learning.Elizabeth Shriberg, Elizabeth Wade, and Patti Price.1992.
Human-machine problem solving usingspoken language systems (SLS): Factors affect-ing performance and user satisfaction.
In Pro-316ceedings of the DARPA Speech and NL Workshop,pages 49-54.Ronnie W. Smith.
1998.
An evaluation of strate-gies for selectively verifying utterance meaningsin spoken atural language dialog.
InternationalJournal of Human-Computer Studies, 48:627-647.G.
Veldhuijzen van Zanten.
1998.
Adaptive mixed-initiative dialogue management.
Technical Re-port 52, IPO, Center for Research on User-System Interaction.Marilyn Walker, Donald Hindle, Jeanne Fromer,Giuseppe Di Fabbrizio, and Craig Mestel.
1997.Evaluating competing agent strategies for a voiceemail agent.
In Proceedings of the EuropeanConference on Speech Communication a d Tech-nology, EUROSPEECH97.M.
Walker, J. Fromer, G. Di Fabbrizio, C. Mestel,and D. Hindle.
1998a.
What can I say: Evaluat-ing a spoken language interface to email.
In Pro-ceedings of the Conference on Computer HumanInteraction ( CH198).Marilyn A. Walker, Jeanne C. Fromer, andShrikanth Narayanan.
1998b.
Learning optimaldialogue strategies: A case study of a spokendialogue agent for email.
In Proceedings of the36th Annual Meeting of the Association of Com-putational Linguistics, COLING/ACL 98, pages1345-1352.Marilyn.
A. Walker, Diane J. Litman, Candace.
A.Kamm, and Alicia Abella.
1998c.
Evaluatingspoken dialogue agents with PARADISE: Twocase studies.
Computer Speech and Language,12(3).S.
M. Weiss and C. Kulikowski.
1991.
ComputerSystems That Learn: Classification and Predic-tion Methods from Statistics, Neural Nets, Ma-chine Learning, and Expert Systems.
San Mateo,CA: Morgan Kaufmann.Ilija Zeljkovic.
1996.
Decoding optimal state se-quences with smooth state likelihoods.
In Inter-national Conference on Acoustics, Speech, andSignal Processing, ICASSP 96, pages 129-132.
