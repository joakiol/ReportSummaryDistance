Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1272?1280,Beijing, August 2010Grouping Product Features Using Semi-Supervised Learningwith Soft-Constraints*Zhongwu Zhai?, Bing Liu?, Hua Xu?
and Peifa Jia?
?State Key Lab of Intelligent Tech.
& Sys.Tsinghua National Lab for Info.
Sci.
and Tech.Dept.
of Comp.
Sci.
& Tech., Tsinghua Univ.zhaizhongwu@gmail.com?Dept.
of Comp.
Sci.University of Illinois at Chicagoliub@cs.uic.eduAbstractIn opinion mining of product reviews, one of-ten wants to produce a summary of opinionsbased on product features/attributes.
Howev-er, for the same feature, people can express itwith different words and phrases.
To producea meaningful summary, these words andphrases, which are domain synonyms, need tobe grouped under the same feature group.This paper proposes a constrained semi-supervised learning method to solve the prob-lem.
Experimental results using reviews fromfive different domains show that the proposedmethod is competent for the task.
It outper-forms the original EM and the state-of-the-artexisting methods by a large margin.1 Introduction*One form of opinion mining in product reviewsis to produce a feature-based summary (Hu andLiu, 2004a; Liu, 2010).
In this model, productfeatures are first identified, and positive and neg-ative opinions on them are aggregated to producea summary on the features.
Features of a productare attributes, components and other aspects ofthe product, e.g., ?picture quality?, ?battery life?and ?zoom?
of a digital camera.In reviews (or any writings), people often usedifferent words and phrases to describe the sameproduct feature.
For example, ?picture?
and?photo?
refer to the same feature for cameras.Grouping such synonyms is critical for effectiveopinion summary.
Although WorldNet and other*Supported by National Natural Science Foundation of Chi-na (Grant No: 60875073).This work was done when the first author was visitingBing Liu?s group at the University of Illinois at Chicago.thesaurus dictionaries can help to some extent,they are far from sufficient due to a few reasons.First, many words and phrases that are not syn-onyms in a dictionary may refer to the same fea-ture in an application domain.
For example, ?ap-pearance?
and ?design?
are not synonymous, butthey can indicate the same feature, design.Second, many synonyms are domain dependent.For example, ?movie?
and ?picture?
are syn-onyms in movie reviews, but they are not syn-onyms in camera reviews as ?picture?
is morelikely to be synonymous to ?photo?
while ?mov-ie?
to ?video?.
Third, determining which expres-sions indicate the same feature can be dependenton the user?s application need.
For example, incar reviews, internal design and external designcan be regarded as two separate features, but canalso be regarded as one feature, called ?design?,based to the level of details that the user needs tostudy.
In camera reviews, one may want to studybattery as a whole (one feature), or as more thanone feature, e.g., battery weight, and battery life.Due to this reason, in applications the user needsto be involved in synonym grouping.Before going further, let us introduce two con-cepts, feature group and feature expression.
Fea-ture group (or feature for short) is the name of afeature (given by the user), while a feature ex-pression of a feature is a word or phrase that ac-tually appears in a review to indicate the feature.For example, a feature group could be named?picture quality?, but there are many possibleexpressions indicating the feature, e.g., ?picture?,?photo?, ?image?, and even the ?picture quality?itself.
All the feature expressions in a featuregroup signify the same feature.Grouping feature expressions manually intosuitable groups is time consuming as there are1272often hundreds of feature expressions.
This paperhelps the user to perform the task more efficient-ly.
To focus our research, we assume that featureexpressions have been discovered from a reviewcorpus by an existing system such as those in(Hu and Liu, 2004b; Popescu and Etzioni, 2005;Kim and Hovy, 2006; Kobayashi et al, 2007;Mei et al, 2007; Stoyanov and Cardie, 2008; Jinet al, 2009; Ku et al, 2009).To reflect the user needs, he/she can manuallylabel a small number of seeds for each featuregroup.
The feature groups are also provided bythe user based on his/her application needs.
Thesystem then assigns the rest of the feature ex-pressions to suitable groups.
To the best of ourknowledge, this problem has not been studied inopinion mining (Pang and Lee, 2008).The problem can be formulated as semi-supervised learning.
The small set of seeds la-beled by the user is the labeled data, and the restof the discovered feature expressions are the un-labeled data.
This is the transductive setting(Joachims, 1999) because the unlabeled set isused in learning and also in testing since our ob-jective is to assign unlabeled expressions to theright feature groups.Any semi-supervised learning method can beapplied to tackle the problem.
In this work, weuse the Expectation-Maximization (EM) algo-rithm (Dempster et al, 1977).
Specifically, weuse the na?ve Bayesian EM formulation in(Nigam et al, 2000), which runs a Bayesian clas-sifier iteratively on the labeled and unlabeleddata until the probabilities for the unlabeled dataconverge.
When the algorithm ends, each unla-beled example is assigned a posterior probabilityof belonging to each group.However, we can do better since the EM algo-rithm only achieves local optimal.
What localoptimal it achieves depends on the initialization,i.e., the initial seeds.
We show that some priorknowledge can help provide a better initialization,and consequently generate better grouping results.Thus, we propose to create another set of dataextracted from the unlabeled set based on twopieces of natural language knowledge:1.
Feature expressions sharing some commonwords are likely to belong to the same group,e.g., ?battery life?
and ?battery power?.2.
Feature expressions that are synonyms in adictionary are likely to belong to the samegroup, e.g., ?movie?
and ?picture?.We call these two pieces of prior knowledge softconstraints because they constrain the featureexpressions to be in the same feature group.
Theconstraints are soft (rather than hard) as they canbe relaxed in the learning process.
This relaxa-tion is important because the above two con-straints can result in wrong groupings.
The EMalgorithm is allowed to re-assign them to othergroups in the learning process.We call the proposed framework constrainedsemi-supervised learning.
Since we use EM andsoft constraints, we call the proposed method SC-EM.
Clearly, the problem can also be attemptedusing some other techniques, e.g., topic modeling(e.g, LDA (Blei et al, 2003)), or clustering usingdistributional similarity (Pereira et al, 1993; Lin,1998; Chen et al, 2006; Sahami and Heilman,2006).
However, our results show that these me-thods do not perform as well.The input to the proposed algorithm consistsof: a set of reviews R, and a set of discoveredfeature expressions F from R (using an existingalgorithm).
The user labels a small set of featureexpressions, i.e., assigning them to the user-specified feature groups.
The system then assignsthe rest of the discovered features to the featuregroups.
EM is run using the distributional (orsurrounding words) contexts of feature expres-sions in review set R to build a na?ve Bayesianclassifier in each iteration.Our evaluation was conducted using reviewsfrom 5 different domains (insurance, mattress,vacuum, car and home-theater).
The results showthat the proposed method outperforms differentvariations of the topic modeling method LDA, k-means clustering, and the recent unsupervisedfeature grouping method mLSA.In summary, this paper makes three main con-tributions:1.
It proposes a new sub-problem of opinionmining, i.e., grouping feature expressions inthe context of semi-supervised learning.
Al-though there are existing methods for solvingthe problem based on unsupervised learning,we argue that for practical use some form ofsupervision from the user is necessary to letthe system know what the user wants.2.
An EM formulation is used to solve the prob-lem.
We augment EM with two soft con-straints.
These constraints help guide EM to1273produce better solutions.
We note that theseconstraints can be relaxed in the process tocorrect the imperfection of the constraints.3.
It is shown experimentally the new methodoutperforms the main existing state-of-the-artmethods that can be applied to the task.2 Related WorkThis work is mainly related to existing researchon synonyms grouping, which clusters words andphrases based on some form of similarity.The methods for measuring word similaritycan be classified into two main types (Agirre etal., 2009): those relying on pre-existing know-ledge resources (e.g., thesauri, or taxonomies)(Yang and Powers, 2005; Alvarez and Lim, 2007;Hughes and Ramage, 2007), and those based ondistributional properties (Pereira et al, 1993;Lin, 1998; Chen et al, 2006; Sahami andHeilman, 2006; Pantel et al, 2009).In the category that relies on existing know-ledge sources, the work of Carenini et al (2005)is most related to ours.
The authors proposed amethod to map feature expressions to a givendomain feature taxonomy, using several similari-ty metrics on WordNet.
This work does not usethe word distribution information, which is itsmain weakness because many expressions of thesame feature are not synonyms in WordNet asthey are domain/application dependent.
Dictiona-ries do not contain domain specific knowledge,for which a domain corpus is needed.Another related work is distributional similari-ty, i.e., words with similar meaning tend to ap-pear in similar contexts (Harris, 1968).
As such,it fetches the surrounding words as context foreach term.
Similarity measures such as Cosine,Jaccard, Dice, etc (Lee, 1999), can be employedto compute the similarities between the seeds andother feature expressions.
To suit our need, wetested the k-means clustering with distributionalsimilarity.
However, it does not perform as wellas the proposed method.Recent work also applied topic modeling (e.g.,LDA) to solve the problem.
Guo et al (2009)proposed a multilevel latent semantic associationtechnique (called mLSA) to group product featureexpressions, which runs LDA twice.
However,mLSA is an unsupervised approach.
For our eval-uation, we still implemented the method andcompared it with our SC-EM method.Our work is also related to constrained cluster-ing (Wagstaff et al, 2001), which uses two formsof constraints, must-link and cannot-link.
Must-links state that some data points must be in thesame cluster, and cannot-links state that somedata points cannot be in the same cluster.
In(Andrzejewski et al, 2009), the two constraintsare added to LDA, called DF-LDA.
We showthat both these methods do not perform as well asour semi-supervised learning method SC-EM.3 The Proposed AlgorithmSince our problem can be formulated as semi-supervised learning, we briefly describe the set-ting in our context.
Given a set C of classes (ourfeature groups), we use L to denote the small setof labeled examples (labeled feature expressionsor seeds), and U the set of unlabeled examples(unlabeled feature expressions).
A classifier isbuilt using L and U to classify every example inU to a class.
Several existing algorithms can beapplied.
In this work, we use EM as it is efficientand it allows prior knowledge to be used easily.Below, we first introduce the EM algorithm thatwe use, and then present our augmented EM.
Theconstraints and their conflict handling are dis-cussed in Section 4.3.1 Semi-Supervised Learning Using EMEM is a popular iterative algorithm for maximumlikelihood estimation in problems with missingdata.
In our case, the group memberships of theunlabeled expressions are considered missingbecause they come without group labels.We use the EM algorithm based on na?veBayesian classification (Nigam et al, 2000).
Al-though it is involved to derive, using it is simple.First, a classifier f is learned using only the la-beled data L (Equations 1 and 2).
Then, f is ap-plied to assign a probabilistic label to each unla-beled example in U (see Equation 3).
Next, anew classifier f is learned using both L and thenewly probabilistically labeled unlabeled exam-ples in UPL, again using Equations 1 and 2.
Theselast two steps iterate until convergence.We now explain the notations in the Equations.Given a set of training documents D, each docu-ment di in D is considered as an ordered list ofwords.
?????
denotes the kth word in di, whereeach word is from the vocabulary V={w1, w2,?,w|V|}.
C={c1, c2,?, c|C|} is the set of pre-defined1274classes or groups.
Nti is the number of times theword wt occurs in document di.For our problem, the surrounding words con-texts of the labeled seeds form L, while the sur-rounding words of the non-seed feature expres-sions form U.
When EM converges, the classifi-cation labels of the unlabeled feature expressionsgive us the final grouping.
Surrounding wordscontexts will be discussed in Section 5.3.2 Proposed Soft-Constrained EMAlthough EM can be directly applied to deal withour problem, we can do better.
As we discussedearlier, EM only achieves local optimal based onthe initialization, i.e., the labeled examples orseeds.
We show that natural languages con-straints can be used to provide a better initializa-tion, i.e., to add more seeds that are likely to becorrect, called soft-labeled examples or soft seeds(SL).
Soft-labeled examples are handled diffe-rently from the original labeled examples in L.With the soft seeds, we have the proposed soft-constrained EM (called SC-EM).Compared with the original EM, SC-EM hastwo main differences:y Soft constraints are applied to L and U to pro-duce a set SL of soft-labeled examples (or softseeds) to initialize EM in addition to L. SL isthus a subset of U.
The training set size is in-creased, which helps produce better results asour experimental results show.y In the first iteration of EM, soft-labeled ex-amples SL are treated in the same way as thelabeled examples in L. Thus both SL and L areused as labeled examples to learn the initialclassifier f0.
However, in the subsequent itera-tions, SL is treated in the same way as any ex-amples in U.
That is, the classifier fx fromeach iteration x (including f0) will predict U.After that, a new classifier is built using bothL and UPL (which is U with probabilistic la-1 Laplace smoothing is used to prevent zero probabilities forinfrequently occurring words.bels).
Clearly, this implies that the class labelsof the examples in SL are allowed to change.That is also why we call SL the soft-labeledset in contrast to the hard-labeled set L, i.e.,the examples in L will not change labels inEM.
The reason that SL is allowed to changelabels/classes is because the constraints canmake mistakes.
EM may be able to correctsome of the mistakes.The detailed algorithm is given in Figure 1.
Theconstraints are discussed in Section 4.4 Generating SL Using ConstraintsAs mentioned earlier, two forms of constraintsare used to induce the soft-labeled set SL.
Foreasy reference, we reproduce them here:1.
Feature expressions sharing some commonwords are likely to belong to the same group.2.
Feature expressions that are synonyms in adictionary are likely to belong to one group.According to the number of words, feature ex-pressions can be categorized into single-wordexpressions and phrase expressions.
They arehandled differently.
The detailed algorithm isgiven in Figure 2.
In the algorithm, L is the la-beled set and U is the unlabeled set.
L, in fact,consists of a set of sets, L = {L1, L2, ?, L|L|}.Each Li contains a set of labeled examples (fea-ture expressions) of the ith class (feature group).Similarly, the output set SL (the soft-labeled set)also consists of a set of sets, i.e., SL = {SL1,SL2, ?, SL|L|}.
Each SLi is a set of soft-labeledexamples (feature expressions) of the ith class????
???
??
?
?
????
???????????????
?
?
?
????
??????????????????(11)?
???
??
?
?
?
???????????????
?
???(21)?
??????
??
????
???????
???????????
??????????
?
?????????????????
(3)Input:- Labeled examples L- Unlabeled examples U1 Extract SL from U using constraints (Section 4);2 Learn an initial na?ve Bayesian classifier f0 using L?
SL and Equations 1 and 2;3 repeat4 // E-Step5 for each example di in U (including SL) do6 Using the current classifier fx to computeP(cj|di) using Equation 3.7 end8 // M-Step9 Learn a new na?ve Bayesian classifier fx from Land U by computing P(wt|cj) and P(cj) usingEquations 1 and 2.10 until the classifier parameters stabilizeOutput: the classifier fx from the last iteration.Figure 1.
The proposed SC-EM algorithm1275(feature group).
Thus Li and SLi correspond toeach other as they represent the original labeledexamples and the newly soft-labeled examples ofthe ith class (or feature group) respectively.The algorithm basically compares each fea-ture expression u in U (line 1) with each featureexpression e (line 4) in every labeled subset Li(line 2) based on the above two constraints.
Ifany of the constraints is satisfied (lines 5-17), itmeans that u is likely to belong to Li (or the ithclass or feature group), and it is added to SLi.There are conflict situations that need to be re-solved.
That is, u may satisfy a constraint ofmore than one labeled sub-set Li.
For example, ifu is a single word, it may be synonyms of featureexpressions from more than one feature groups.The question is which group it is likely to belong.Further, u may be synonyms of a few single-word feature expressions in Li.
Clearly, u being asynonym of more than one word in Li is betterthan it is only the synonym of one word in Li.Similar problems also occur when u is an ele-ment of a feature expression phrase e.To match u and e, there are a few possibilities.If both u and e are single words (lines 5-6), thealgorithm checks if they are synonyms (line 7).The score in line 8 is discussed below.
When oneof u and e is a phrase, or both of them are phrases,we see whether they have shared words.
Again,conflict situations can happen with multipleclasses (feature groups) as discussed above.
Notethat in these cases, we do not use the synonymconstraint, which does not help in our test.Given these complex cases, we need to decidewhich class that u should be assigned to orshould not be assigned to any class (as it does notmeet any constraint).
We use a score to recordthe level of satisfaction.
Once u is compared witheach e in every class, the accumulated score isused to determine which class Li has the strong-est association with u.
The class j with the high-est score is assigned to u.
In other words, u isadded to SLj.
Regarding the score value, syn-onyms gets the score of 1 (line 8), and intersec-tion (shared words) gets the score equal to thesize of the intersection (lines 10-17).5 Distributional Context ExtractionTo apply the proposed algorithm, a document dineeds to be prepared for each feature expressionei for na?ve Bayesian learning.
di is formed byaggregating the distributional context of eachsentence sij in our corpus that contains the ex-pression ei.
The context of a sentence is the sur-rounding words of ei in a text window of [-t, t],including the words in ei.
Given a relevant cor-pus R, the document di for each feature expres-sion ei in L (or U) is generated using the algo-rithm in Figure 3.
Stopwords are removed.1 for each feature expression ei in L (or U) do2       Si ?
all sentences containing ei in R;3       for each sentence sij ?
Si do4            dij ?
words in a window of [-t, t] on the leftand right (including the words in ei);5       di ?
words from all dij, j = 1, 2, ?, |Si|;// duplicates are kept as it is not unionFigure 3.
Distributional context extractionFor example, a feature expression from L (orU) is ei = ?screen?
and there are two sentences inour corpus R that contain ?screen?si1 = ?The LCD screen gives clear picture?.si2 = ?The picture on the screen is blur?We use the window size of [-3, 3].
Sentence si1,gives us di1 = <LCD, screen, give, clear, picture>as a bag of words.
?the?
and ?is?
are removed asstopwords.
si2 gives us di2 = <picture, screen,blur>.
?on?, ?the?
and ?is?
are removed as stop-words.
Finally, we obtain the document di forfeature expression ei as a bag of words:di = <LCD, screen, give, clear, picture,picture, screen, blur>6 Empirical EvaluationThis section evaluates the SC-EM algorithm andcompares it with the main existing methods thatcan be applied to solve the problem.1  for each feature expression u ?
U do2 for each feature group Li ?
L do3 score(Li) ?
0;4 for each feature expression e ?
Li do5 if u is a single word expression then6 if e is a single word expression then7 if u and e are synonyms then8 score(Li) ?
score(Li) + 1;9 else if w ?
e then  // e is a phrase10 score(Li) ?
score(Li) + 111 else  // u is a phrase12 if e is a single word expression then13 if e ?
u then  // u is a phrase14 score(Li) ?
score(Li) + 115 else16 s ?
e ?
u;17 score(Li) ?
score(Li) + |s|18 u is added to SLj s.t.
????????
????????
?Figure 2.
Generating the soft-labeled set SL12766.1 Review Data Sets and Gold StandardsTo demonstrate the generality of the proposedmethod, experiments were conducted using re-views from five domains: Hometheater, Insur-ance, Mattress, Car and Vacuum.
All the datasets and the gold standard feature expressionsand groups were from a company that providesopinion mining services.
The details of the datasets and the gold standards are given in Table 1.Hometheater Insurance Mattress Car Vacuum#Sentences 6355 12446 12107 9731 8785#Reviews 587 2802 933 1486 551#Featureexpressions 237 148 333 317 266#Featuregroups 15 8 15 16 28Table 1.
Data sets and gold standards6.2 Evaluation MeasuresSince SC-EM is based on semi-supervised learn-ing, we can use classification accuracy to eva-luate it.
We can also see it as clustering with ini-tial seeds.
Thus we also use clustering evaluationmethods.
Given gold standards, two popularclustering evaluation measures are Entropy andPurity (Liu, 2006).
As accuracy is fairly standard,we will not discuss it further.
Below, we brieflydescribe entropy and purity.Given a data set DS, its gold partition is G ={??,?,??,?,??
}, where k is the known numberof clusters.
The groups partition DS into k dis-joint subsets, DS1,?, DSi, ?, DSk.Entropy: For each resulting cluster, we canmeasure its entropy using Equation 4, wherePi(??)
is the proportion of ??
data points in DSi.The total entropy of the clustering (consideringall clusters) is calculated by Equation 5.?????????
???
?
??
??????????????????(4)????????????
????
????????????????
???????
(5)Purity: Purity measures the extent that a clus-ter contains only data from one gold-partition.Each cluster?s purity is computed by Equation 6,and the total purity of the whole clustering iscomputed with Equation 7.????????
???
?
????
????
? (6)???????????
????
???????????????
??????
? (7)In testing, the unlabeled set U is also our testset.
This is justified because our purpose is toassign unlabeled data to appropriate groups.6.3 Baseline Methods and SettingsThe proposed SC-EM method is compared witha set of existing methods, which can be catego-rized into unsupervised and semi-supervised me-thods.
We list the unsupervised methods first.LDA: LDA is a popular topic modeling me-thod (see Section 2).
Given a set of documents, itoutputs groups of terms of different topics.
In ourcase, each feature expression is a term, and thedocuments refer to the distributional contexts ofeach feature expressions (see Section 5).mLSA: This is a state-of-the-art unsupervisedmethod for solving the problem.
It is based onLDA, and has been discussed in related work.Kmeans: This is the k-means clustering me-thod (MacQueen, 1966) based on distributionalsimilarity with cosine as the similarity measure.In the semi-supervised category, the methodsare further classified into un-constrained, hard-constrained, and soft-constrained methods.For the un-constrained subclass (no con-straints are used), we have the following:LDA(L, H): This method is based on LDA,but the labeled examples L are used as seeds foreach group/topic.
All examples in L will alwaysstay in the same topic.
We call this hard initiali-zation (H).
L is handled similarly below.DF-LDA(L, H).
DF-LDA is the LDA method(Andrzejewski et al, 2009) that takes must-linksand cannot-links.
Our L set can be expressed as acombination of must-links and cannot-links.
Un-fortunately, only must-links can be used becausethe number of cannot-links is huge and crashesthe system.
For example, for the car data, thenumber of cannot-links is 194,400 for 10% la-beled data (see Section 6.4) and for 20% it is466,560,000.
DF-LDA also has a parameter ?controlling the link strength, which is set veryhigh (=1000) to reflect the hard initialization.
Wedid not use DF-LDA in the unsupervised subclassabove as without constraints it reduces to LDA.Kmeans(L, H): This method is based onKmeans, but the clusters of the labeled seeds arefixed at the initiation and remain unchanged.EM(L, H): This is the original EM for semi-supervised learning.
Only the labeled examplesare used as the initial seeds.For the hard-constrained (H) subclass (our1277two constraints are applied and cannot be vi-olated), we have the following methods (LC is Lplus SL produced by the constraints (C):Rand(LC, H): This is an important baseline.
Itshows whether the constraints alone are suffi-cient to produce good results.
That is, the finalresult is the expanded seeds SL plus the rest of Uassigned randomly to different groups.LDA(LC, H): It is similar to LDA(L,H), butboth the initial seeds L and the expanded seedsSL are considered as labeled examples.
They alsostay in the same topics/groups in the process.Note that although SL is called a set of soft-labeled examples (seeds) in the proposed algo-rithm, they are treated as hard-labeled exampleshere just for experimental comparison.DF-LDA(LC, H): This is DF-LDA with bothL and SL expressed as must-links.
Again, a large?
(= 1000) is used to make sure that must-linksfor L and SL will not be violated.Kmeans(LC,H): It is similar to Kmeans(L,H),but both L and SL stay in their assigned clusters.EM(LC, H): It is similar to SC-EM, but SL isadded to the labeled set L, and their classes arenot allowed to change in the EM iterations.For the soft-constrained (S) subclass, our twoconstraints can be violated.
Initially, both theinitial seeds L and the expanded seeds SL areconsidered as labeled data, but subsequently, on-ly L is taken as the labeled data (i.e., staying inthe same classes).
The algorithm will re-estimatethe label of each feature expression in SL.
Thissubclass has the following methods:LDA(LC, S): This is in contrast to LDA(LC,H).
It allows the SL set to change topics/groups.Kmeans(LC, S): This is in contrast toKmeans(LC, H).A soft DF-LDA is not included here becausedifferent ?
values give different results, and theyare generally worse than DF-LDA(LC, H).For all LDA based methods, the topic model-ing parameters were set to their default values.The number of iteration is 1000.
We used theLDA in MALLET2, and modified it to suit differ-ent LDA-based methods except DF-LDA, whichwas downloaded from its authors?
website3.
Weimplemented mLSA, Kmeans and changed EM4to take soft seeds.
For all Kmeans based methods,the distance function is the cosine similarity.2 http://mallet.cs.umass.edu/3 http://pages.cs.wisc.edu/~andrzeje/research/df_lda.html4 http://alias-i.com/lingpipe/6.4 Evaluation ResultsWe now compare the results of SC-EM and the14 baseline methods.
To see the effects of differ-ent numbers of labeled examples (seeds), we ex-perimented with 10%, 20%, 30%, 40%, and 50%of the feature expressions from the gold standarddata as the labeled set L, and the rest as the unla-beled set U.
All labeled data were selected ran-domly.
For each setting, we run the algorithms30 times and report the average results.
Due tospace limitations, we can only show the detailedpurity (Pur), entropy (Ent) and accuracy (Acc)results for 30% as the labeled data (70% as unla-beled) in Table 2.
For the other proportions oflabeled data, we summarize them in Table 3.Each result in Table 3 is thus the average of the 5data sets.
All the results were obtained from theunlabeled set U, which was our test set.
For en-tropy, the smaller the value is the better, but forpurity and accuracy, the larger the better.
Forthese experiments, we used the window size t = 5.Section 6.5 studies the effects of window sizes.Tables 2 and 3 clearly show that the proposedalgorithm (SC-EM) outperforms all 14 baselinemethods by a large margin on every dataset.
Indetail, we observe the following:?
LDA, mLSA and Kmeans with no seeds (la-beled data) perform the worst.
Seeds help toimprove the results, which is intuitive.
With-out seeds, DF-LDA is the same as LDA.?
LDA based methods seems to be the weakest.Kmeans based methods are slightly better, butEM based methods are the best.
This clearlyindicates that classification (EM) performsbetter than clustering.
Comparing DF-LDAand Kmeans, their results are similar.?
For LDA, and Kmeans, hard-constrained me-thods (i.e., LDA(L, H), and Kmeans(L, H))perform better than soft-constrained methods(i.e., LDA(LC, S) and Kmeans(LC, S)).
Thisindicates that soft-constrained versions maychange some correctly constrained expres-sions into wrong groups.
However, for theEM based methods, the soft-constrained me-thod (SC-EM) performs markedly better thanthe hard-constrained version (EM(LC, H)).This indicates that Bayesian classifier used inEM can take advantage of the soft constraintsand correct some wrong assignments made byconstraints.
Much weaker results of Rand(LC,H) than SC-EM in different settings show that1278constraints alone (i.e., synonyms and sharingof words) are far from sufficient.
EM can im-prove it considerably.?
Comparing EM based methods, we can seethat soft seeds in SL make a big difference forall data sets.
SC-EM is clearly the best.?
As the number of labeled examples increases(from 10% to 50%), the results improve forevery method (except those for DF-LDA,which does not change much).6.5 Varying the Context Window SizeWe varied the text window size t from 1 to 10 tosee how it impacts on the performance of SC-EM.The results are given in Figure 4 (they are aver-ages of the 5 datasets).
Again for purity and ac-curacy, the greater the value the better, while forentropy it is the opposite.
It is clear that the win-dow sizes of 2~6 produce similar good results.All evaluations reported above used t = 5.7 ConclusionThis paper proposed the task of feature groupingin a semi-supervised setting.
It argued that someform of supervision is needed for the problembecause its solution depends on the user applica-tion needs.
The paper then proposed to use theEM algorithm to solve the problem, which wasimproved by considering two soft constraints.Empirical evaluations using 5 real-life data setsshow that the proposed method is superior to 14baselines.
In our future work, we will focus onfurther improving the accuracy.MethodsHometheater Insurance Mattress Car VacuumAcc Pur Ent Acc Pur Ent Acc Pur Ent Acc Pur Ent Acc Pur EntLDA 0.06 0.31 2.54 0.11 0.36 2.24 0.05 0.32 2.57 0.06 0.37 2.39 0.03 0.36 2.09mLSA 0.06 0.31 2.53 0.14 0.38 2.19 0.06 0.34 2.55 0.09 0.37 2.40 0.03 0.37 2.11Kmeans 0.21 0.42 2.14 0.25 0.45 1.90 0.15 0.39 2.32 0.25 0.44 2.16 0.24 0.47 1.78LDA(L, H) 0.10 0.32 2.50 0.16 0.37 2.22 0.10 0.34 2.57 0.19 0.39 2.36 0.10 0.39 2.09DF-LDA(L, H) 0.27 0.37 2.32 0.25 0.41 2.00 0.19 0.39 2.35 0.28 0.45 2.15 0.31 0.40 1.98Kmeans(L, H) 0.20 0.42 2.12 0.25 0.43 1.92 0.17 0.42 2.26 0.27 0.48 2.04 0.20 0.48 1.76EM(L, H) 0.48 0.50 1.93 0.50 0.53 1.69 0.52 0.56 1.87 0.56 0.58 1.80 0.49 0.52 1.79Rand(CL, H) 0.41 0.46 2.07 0.40 0.46 1.94 0.40 0.47 2.07 0.34 0.41 2.31 0.39 0.52 1.59LDA(CL, H) 0.44 0.50 1.96 0.42 0.48 1.89 0.42 0.49 1.97 0.44 0.52 1.87 0.43 0.55 1.48DF-LDA(CL, H) 0.35 0.49 1.86 0.33 0.49 1.71 0.23 0.39 2.26 0.34 0.51 1.88 0.37 0.52 1.58Kmeans(CL, H) 0.49 0.55 1.70 0.48 0.55 1.62 0.44 0.51 1.91 0.47 0.54 1.80 0.44 0.58 1.42EM(CL, H) 0.59 0.60 1.62 0.58 0.60 1.46 0.56 0.59 1.74 0.62 0.64 1.54 0.55 0.60 1.44LDA(CL, S) 0.24 0.35 2.44 0.27 0.40 2.14 0.23 0.37 2.44 0.27 0.41 2.33 0.23 0.41 2.01Kmeans(CL, S) 0.33 0.46 2.04 0.34 0.45 1.90 0.25 0.43 2.20 0.29 0.47 2.07 0.37 0.50 1.68SC-EM 0.67 0.68 1.30 0.66 0.68 1.18 0.68 0.70 1.27 0.70 0.71 1.24 0.67 0.68 1.18Table 2.
Comparison results (L = 30% of the gold standard data)MethodsAcc Pur Ent10% 20% 30% 40% 50% 10% 20% 30% 40% 50% 10% 20% 30% 40% 50%LDA 0.07 0.07 0.06 0.06 0.08 0.33 0.33 0.34 0.35 0.38 2.50 2.44 2.37 2.28 2.11mLSA 0.07 0.07 0.08 0.07 0.07 0.34 0.35 0.35 0.37 0.38 2.48 2.42 2.36 2.26 2.12Kmeans 0.22 0.23 0.22 0.22 0.22 0.42 0.43 0.44 0.44 0.46 2.16 2.11 2.06 1.98 1.86LDA(L, H) 0.10 0.10 0.13 0.14 0.15 0.34 0.34 0.36 0.37 0.39 2.48 2.43 2.35 2.25 2.11DF-LDA(L, H) 0.23 0.25 0.26 0.27 0.30 0.41 0.40 0.41 0.41 0.44 2.23 2.23 2.16 2.10 1.94Kmeans(L, H) 0.13 0.16 0.22 0.24 0.28 0.42 0.43 0.45 0.45 0.48 2.15 2.11 2.02 1.95 1.79EM(L, H) 0.35 0.44 0.51 0.55 0.58 0.43 0.49 0.54 0.57 0.61 2.22 1.99 1.81 1.65 1.49Rand(CL, H) 0.28 0.35 0.39 0.42 0.45 0.39 0.43 0.47 0.50 0.54 2.33 2.15 2.00 1.82 1.63LDA(CL, H) 0.31 0.38 0.43 0.46 0.49 0.43 0.47 0.51 0.54 0.58 2.16 1.99 1.83 1.69 1.49DF-LDA(CL, H) 0.32 0.33 0.33 0.34 0.36 0.49 0.50 0.48 0.48 0.48 1.90 1.85 1.86 1.83 1.82Kmeans(CL, H) 0.33 0.41 0.46 0.49 0.52 0.47 0.51 0.55 0.57 0.61 1.98 1.82 1.69 1.56 1.42EM(CL, H) 0.44 0.54 0.58 0.61 0.64 0.49 0.57 0.61 0.64 0.67 1.98 1.72 1.56 1.40 1.25LDA(CL, S) 0.17 0.21 0.25 0.30 0.34 0.34 0.36 0.39 0.42 0.46 2.47 2.37 2.27 2.09 1.87Kmeans(CL, S) 0.23 0.28 0.32 0.36 0.42 0.43 0.44 0.46 0.48 0.51 2.15 2.08 1.98 1.86 1.70SC-EM 0.45 0.58 0.68 0.75 0.81 0.50 0.61 0.69 0.76 0.82 1.95 1.56 1.24 0.94 0.69Table 3.
Influence of the seeds?
proportion (which reflects the size of the labeled set L)Figure 4.
Influence of context window size1.01.11.21.31.41.562%64%66%68%70%1 2 3 4 5 6 7 8 9 10EntropyPurity/AccuracyWindow Size tSC-EMPurityAccuracyEntropy1279ReferencesAgirre E., E. Alfonseca, K. Hall, J. Kravalova, M. Paca and A. Soroa 2009.
A study on similarity andrelatedness using distributional and WordNet-based approaches.
Proceedings of ACL.Alvarez M. and S. Lim 2007.
A Graph Modeling ofSemantic Similarity between Words.
Proceeding ofthe Conference on Semantic Computing.Andrzejewski D., X. Zhu and M. Craven 2009.Incorporating domain knowledge into topicmodeling via Dirichlet forest priors.
Proceedingsof ICML.Blei D., A. Y. Ng and M. I. Jordan 2003.
"LatentDirichlet Allocation."
JMLR 3: 993-1022.Carenini G., R. Ng and E. Zwart 2005.
Extractingknowledge from evaluative text.
Proceedings ofInternational Conference on Knowledge Capture.Chen H., M. Lin and Y. Wei 2006.
Novel associationmeasures using web search with double checking.Proceedings of ACL.Dempster A., N. Laird and D. Rubin 1977.
"Maximumlikelihood from incomplete data via the EMalgorithm."
Journal of the Royal Statistical Society39(1): 1-38.Guo H., H. Zhu, Z. Guo, X. Zhang and Z. Su 2009.Product feature categorization with multilevellatent semantic association.
Proc.
of CIKM.Harris Z. S. 1968.
Mathematical structures oflanguage.
New York, Interscience Publishers.Hu M. and B. Liu 2004a.
Mining and summarizingcustomer reviews.
Proceedings of SIGKDD.Hu M. and B. Liu 2004b.
Mining Opinion Features inCustomer Reviews.
Proceedings of AAAI.Hughes T. and D. Ramage 2007.
Lexical semanticrelatedness with random graph walks.
EMNLP.Jin W., H. Ho and R. Srihari 2009.
OpinionMiner: anovel machine learning system for web opinionmining and extraction.
Proceedings of KDD.Joachims T. 1999.
Transductive inference for textclassification using support vector machines.Proceedings of ICML.Kim S. and E. Hovy 2006.
Extracting opinions,opinion holders, and topics expressed in onlinenews media text.
Proceedings of EMNLP.Kobayashi N., K. Inui and Y. Matsumoto 2007.Extracting aspect-evaluation and aspect-ofrelations in opinion mining.
Proceedings ofEMNLP.Ku L., H. Ho and H. Chen 2009.
"Opinion mining andrelationship discovery using CopeOpi opinionanalysis system."
Journal of the American Societyfor Information Science and Technology 60(7):1486-1503.Lee L. 1999.
Measures of distributional similarity,Proceedings of ACL.Lin D. 1998.
Automatic retrieval and clustering ofsimilar words, Proceedings of ACL.Liu B.
2006.
Web data mining; Exploring hyperlinks,contents, and usage data, Springer.Liu B.
2010.
Sentiment Analysis and Subjectivity.Handbook of Natural Language Processing N.Indurkhya and F. J. Damerau.MacQueen J.
1966.
Some methods for classificationand analysis of multivariate observations.
Proc.
ofSymposium on Mathematical Statistics andProbability.Mei Q., X. Ling, M. Wondra, H. Su and C. Zhai 2007.Topic sentiment mixture: modeling facets andopinions in weblogs.
Proceedings of WWW.Nigam K., A. McCallum, S. Thrun and T. Mitchell2000.
"Text classification from labeled andunlabeled documents using EM."
MachineLearning 39(2).Pang B. and L. Lee 2008.
"Opinion mining andsentiment analysis."
Foundations and Trends inInformation Retrieval 2(1-2): 1-135.Pantel P., E. Crestan, A. Borkovsky, A. Popescu andV.
Vyas 2009.
Web-scale distributional similarityand entity set expansion.
EMNLP.Pereira F., N. Tishby and L. Lee 1993.
Distributionalclustering of English words.
Proceedings of ACL.Popescu A.-M. and O. Etzioni 2005.
ExtractingProduct Features and Opinions from Reviews.EMNLP.Sahami M. and T. Heilman 2006.
A web-based kernelfunction for measuring the similarity of short textsnippets.
Proceedings of WWW.Stoyanov V. and C. Cardie 2008.
Topic identificationfor fine-grained opinion analysis.
COLING.Wagstaff K., C. Cardie, S. Rogers and S. Schroedl2001.
Constrained k-means clustering withbackground knowledge.
In Proceedings of ICML.Yang D. and D. Powers 2005.
Measuring semanticsimilarity in the taxonomy of WordNet,Proceedings of the Australasian conference onComputer Science.1280
