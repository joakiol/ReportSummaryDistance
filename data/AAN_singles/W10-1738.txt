Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 262?270,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsJane: Open Source Hierarchical Translation, Extended with Reorderingand Lexicon ModelsDavid Vilar, Daniel Stein, Matthias Huck and Hermann NeyLehrstuhl fu?r Informatik 6RWTH Aachen UniversityAachen, Germany{vilar,stein,huck,ney}@cs.rwth-aachen.deAbstractWe present Jane, RWTH?s hierarchicalphrase-based translation system, whichhas been open sourced for the scientificcommunity.
This system has been in de-velopment at RWTH for the last two yearsand has been successfully applied in dif-ferent machine translation evaluations.
Itincludes extensions to the hierarchical ap-proach developed by RWTH as well asother research institutions.
In this paperwe give an overview of its main features.We also introduce a novel reorderingmodel for the hierarchical phrase-basedapproach which further enhances transla-tion performance, and analyze the effectsome recent extended lexicon models haveon the performance of the system.1 IntroductionWe present a new open source toolkit for hi-erarchical phrase-based translation, as describedin (Chiang, 2007).
The hierarchical phrase modelis an extension of the standard phrase model,where the phrases are allowed to have ?gaps?.
Inthis way, long-distance dependencies and reorder-ings can be modelled in a consistent way.
As innearly all current statistical approaches to machinetranslation, this model is embedded in a log-linearmodel combination.RWTH has been developing this tool duringthe last two years and it was used success-fully in numerous machine translation evalua-tions.
It is developed in C++ with special at-tention to clean code, extensibility and efficiency.The toolkit is available under an open sourcenon-commercial license and downloadable fromhttp://www.hltpr.rwth-aachen.de/jane.In this paper we give an overview of the mainfeatures of the toolkit and introduce two new ex-tensions to the hierarchical model.
The first oneis an additional reordering model inspired by thereordering widely used in phrase-based transla-tion systems and the second one comprises twoextended lexicon models which further improvetranslation performance.2 Related WorkJane implements many features presented in pre-vious work developed both at RWTH and othergroups.
As we go over the features of the systemwe will provide the corresponding references.Jane is not the first system of its kind, al-though it provides some unique features.
Thereare other open source hierarchical decoders avail-able.
These include?
SAMT (Zollmann and Venugopal, 2006):The original version is not maintained anymore and we had problems working on bigcorpora.
A new version which requiresHadoop has just been released, however thedocumentation is still missing.?
Joshua (Li et al, 2009): A decoder writtenin Java by the John Hopkins University.
Thisproject is the most similar to our own, how-ever both were developed independently andeach one has some unique features.
A briefcomparison between these two systems is in-cluded in Section 5.1.?
Moses (Koehn et al, 2007): The de-factostandard phrase-based translation decoderhas now been extended to support hierarchi-cal translation.
This is still in an experimentalbranch, however.3 FeaturesIn this section we will only give a brief overviewof the features implemented in Jane.
For de-tailed explanation of previously published algo-262rithms and methods, we refer to the given litera-ture.3.1 Search AlgorithmsThe search for the best translation proceeds in twosteps.
First, a monolingual parsing of the inputsentence is carried out using the CYK+ algorithm(Chappelier and Rajman, 1998), a generalizationof the CYK algorithm which relaxes the require-ment for the grammar to be in Chomsky normalform.
From the CYK+ chart we extract a hyper-graph representing the parsing space.In a second step the translations are generated,computing the language model scores in an inte-grated fashion.
Both the cube pruning and cubegrowing algorithms (Huang and Chiang, 2007) areimplemented.
For the latter case, the extensionsconcerning the language model heuristics similarto (Vilar and Ney, 2009) have also been included.3.2 Language ModelsJane supports four formats for n-gram languagemodels:?
The ARPA format for language models.
Weuse the SRI toolkit (Stolcke, 2002) to supportthis format.?
The binary language model format supportedby the SRI toolkit.
This format allows for amore efficient language model storage, whichreduces loading times.
In order to reducememory consumption, the language modelcan be reloaded for every sentence, filteringthe n-grams that will be needed for scoringthe possible translations.
This format is spe-cially useful for this case.?
Randomized LMs as described in (Talbot andOsborne, 2007), using the open source im-plementation made available by the authorsof the paper.
This approach uses a space ef-ficient but approximate representation of theset of n-grams in the language model.
Inparticular the probability for unseen n-gramsmay be overestimated.?
An in-house, exact representation formatwith on-demand loading of n-grams, usingthe internal prefix-tree implementation whichis also used for phrase storage (see also Sec-tion 3.9).Several language models (also of mixed formats)can be used during search.
Their scores are com-bined in the log-linear framework.3.3 Syntactic FeaturesSoft syntactic features comparable to (Vilar et al,2008) are implemented in the extraction step ofthe toolkit.
In search, they are considered as ad-ditional feature functions of the translation rules.The decoder is able to handle an arbitrary num-ber of non-terminal symbols.
The extraction hasbeen extended so that the extraction of SAMT-rules is included (Zollmann and Venugopal, 2006)but this approach is not fully supported (theremay be empty parses due to the extended num-ber of non-terminals).
We instead opted to sup-port the generalization presented in (Venugopal etal., 2009), where the information about the newnon-terminals is included as an additional featurein the log-linear model.In addition, dependency information in thespirit of (Shen et al, 2008) is included.
Jane fea-tures models for string-to-dependency languagemodels and computes various scores based on thewell-formedness of the resulting dependency tree.Jane supports the Stanford parsing format,1 butcan be easily extended to other parsers.3.4 Additional Reordering ModelsIn the standard formulation of the hierarchicalphrase-based translation model two additionalrules are added:S ?
?S?0X?1, S?0X?1?S ?
?X?0, X?0?
(1)This allows for a monotonic concatenation ofphrases, very much in the way monotonic phrase-based translation is carried out.It is a well-known fact that for phrase-basedtranslation, the use of additional reordering mod-els is a key component, essential for achievinggood translation quality.
In the hierarchical model,the reordering is already integrated in the transla-tion formalism, but there are still cases where therequired reorderings are not captured by the hier-archical phrases alone.The flexibility of the grammar formalism allowsus to add additional reordering models without theneed to explicitely modify the code for supportingthem.
The most straightforward example would1http://nlp.stanford.edu/software/lex-parser.shtml263be to include the ITG-Reorderings (Wu, 1997), byadding following ruleS ?
?S?0S?1, S?1S?0?
(2)We can also model other reordering constraints.As an example, phrase-level IBM reordering con-straints with a window length of 1 can be includedsubstituting the rules in Equation (1) with follow-ing rulesS ?
?M?0,M?0?S ?
?M?0S?1,M?0S?1?S ?
?B?0M?1,M?1B?0?M ?
?X?0, X?0?M ?
?M?0X?1,M?0X?1?B ?
?X?0, X?0?B ?
?B?0X?1, X?1B?0?
(3)In these rules we have added two additional non-terminals.
The M non-terminal denotes a mono-tonic block and the B non-terminal a back jump.Actually both of them represent monotonic trans-lations and the grammar could be simplified byusing only one of them.
Separating them allowsfor more flexibility, e.g.
when restricting the jumpwidth, where we only have to restrict the maxi-mum span width of the non-terminal B. Theserules can be generalized for other reordering con-straints or window lengths.Additionally distance-based costs can be com-puted for these reorderings.
To the best of ourknowledge, this is the first time such additionalreorderings have been applied to the hierarchicalphrase-based approach.3.5 Extended Lexicon ModelsWe enriched Jane with the ability to score hy-potheses with discriminative and trigger-basedlexicon models that use global source sentencecontext and are capable of predicting context-specific target words.
This approach has recentlybeen shown to improve the translation results ofconventional phrase-based systems.
In this sec-tion, we briefly review the basic aspects of theseextended lexicon models.
They are similar to(Mauser et al, 2009), and we refer there for a moredetailed exposition on the training procedures andresults in conventional phrase-based decoding.Note that the training for these models is notdistributed together with Jane.3.5.1 Discriminative Word LexiconThe first of the two lexicon models is denoted asdiscriminative word lexicon (DWL) and acts as astatistical classifier that decides whether a wordfrom the target vocabulary should be included ina translation hypothesis.
For that purpose, it con-siders all the words from the source sentence, butdoes not take any position information into ac-count, i.e.
it operates on sets, not on sequences oreven trees.
The probability of a word being partof the target sentence, given a set of source words,are decomposed into binary features, one for eachsource vocabulary entry.
These binary features arecombined in a log-linear fashion with correspond-ing feature weights.
The discriminative word lex-icon is trained independently for each target wordusing the L-BFGS (Byrd et al, 1995) algorithm.For regularization, Gaussian priors are utilized.DWL model probabilities are computed asp(e|f) =?e?VEp(e?|f) ?
?e?ep(e+|f)p(e?|f)(4)with VE being the target vocabulary, e the set oftarget words in a sentence, and f the set of sourcewords, respectively.
Here, the event e+ is usedwhen the target word e is included in the targetsentence and e?
if not.
As the left part of the prod-uct in Equation (4) is constant given a source sen-tence, it can be dropped, which enables us to scorepartial hypotheses during search.3.5.2 Triplet LexiconThe second lexicon model we employ in Jane,the triplet lexicon model, is in many aspects re-lated to IBM model 1 (Brown et al, 1993), butextends it with an additional word in the con-ditioning part of the lexical probabilities.
Thisintroduces a means for an improved representa-tion of long-range dependencies in the data.
LikeIBM model 1, the triplets are trained iterativelywith the Expectation-Maximization (EM) algo-rithm (Dempster et al, 1977).
Jane implementsthe so-called inverse triplet model p(e|f, f ?
).The triplet lexicon model score t(?)
of the ap-plication of a rule X ?
?
?, ??
where (?, ?)
isa bilingual phrase pair that may contain symbolsfrom the non-terminal set is computed ast(?, ?, fJ0 ) = (5)??elog?
?2J ?
(J + 1)?j?j?>jp(e|fj , fj?)?
?264with e ranging over all terminal symbols in the tar-get part ?
of the rule.
The second sum selects allwords from the source sentence fJ0 (including theempty word that is denoted as f0 here).
The thirdsum incorporates the rest of the source sentenceright of the first triggering word.
The order ofthe triggers is not relevant because per definitionp(e|f, f ?)
= p(e|f ?, f), i.e.
the model is symmet-ric.
Non-terminals in ?
have to be skipped whenthe rule is scored.In Jane, we also implemented scoring for a vari-ant of the triplet lexicon model called the path-constrained (or path-aligned) triplet model.
Thecharacteristic of path-constrained triplets is thatthe first trigger f is restricted to the aligned targetword e. The second trigger f ?
is allowed to movealong the whole remaining source sentence.
Forthe training of the model, we use word alignmentinformation obtained by GIZA++ (Och and Ney,2003).
To be able to apply the model in search,Jane has to be run with a phrase table that con-tains word alignment for each phrase, too, with theexception of phrases which are composed purelyof non-terminals.
Jane?s phrase extraction can op-tionally supply this information from the trainingdata.
(Hasan et al, 2008) and (Hasan and Ney, 2009)employ similar techniques and provide some morediscussion on the path-aligned variant of themodel and other possible restrictions.3.6 Forced AlignmentsJane has also preliminary support for forced align-ments between a given source and target sentence.Given a sentence in the source language and itstranslation in the target language, we find the bestway the source sentence can be translated intothe given target sentence, using the available in-ventory of phrases.
This is needed for more ad-vanced training approaches like the ones presentedin (Blunsom et al, 2008) or (Cmejrek et al, 2009).As reported in these papers, due to the restrictionsin the phrase extraction process, not all sentencesin the training corpus can be aligned in this way.3.7 Optimization MethodsTwo method based on n-best for minimum errorrate training (MERT) of the parameters of the log-linear model are included in Jane.
The first oneis the procedure described in (Och, 2003), whichhas become a standard in the machine translationcommunity.
We use an in-house implementationof the method.The second one is the MIRA algorithm, firstapplied for machine translation in (Chiang et al,2009).
This algorithm is more adequate when thenumber of parameters to optimize is large.If the Numerical Recipes library (Press et al,2002) is available, an additional general purposeoptimization tool is also compiled.
Using thistool a single-best optimization procedure based onthe downhill simplex method (Nelder and Mead,1965) is included.
This method, however, can beconsidered deprecated in favour of the above men-tioned methods.3.8 Parallelized operationIf the Sun Grid Engine2 is available, all operationsof Jane can be parallelized.
For the extraction pro-cess, the corpus is split into chunks (the granular-ity being user-controlled) which are distributed inthe computer cluster.
Count collection, marginalcomputation and count normalization all happensin an automatic and parallel manner.For the translation process a batch job is startedon a number of computers.
A server distributes thesentences to translate to the computers that havebeen made available to the translation job.The optimization process also benefits fromthe parallelized optimization.
Additionally, forthe minimum error rate training methods, randomrestarts may be performed on different computersin a parallel fashion.The same client-server infrastructure used forparallel translation may also be reused for inter-active systems.
Although no code in this directionis provided, one would only need to implement acorresponding frontend which communicates withthe translation server (which may be located on an-other machine).3.9 ExtensibilityOne of the goals when implementing the toolkitwas to make it easy to extend it with new features.For this, an abstract class was created which wecalled secondary model.
New models need only toderive from this class and implement the abstractmethods for data reading and costs computation.This allows for an encapsulation of the computa-tions, which can be activated and deactivated ondemand.
The models described in Sections 3.32http://www.sun.com/software/sge/265through 3.5 are implemented in this way.
We thustry to achieve loose coupling in the implementa-tion.In addition a flexible prefix tree implementationwith on-demand loading capabilities is included aspart of the code.
This class has been used for im-plementing on-demand loading of phrases in thespirit of (Zens and Ney, 2007) and the on-demandn-gram format described in Section 3.2, in addi-tion to some intermediate steps in the phrase ex-traction process.
The code may also be reused inother, independent projects.3.10 CodeThe main core of Jane has been implemented inC++.
Our guideline was to write code that wascorrect, maintainable and efficient.
We tried toachieve correctness by means of unit tests inte-grated in the source as well as regression tests.
Wealso defined a set of coding guidelines, which wetry to enforce in order to have readable and main-tainable code.
Examples include using descriptivevariable names, appending an underscore to pri-vate members of classes or having each class namestart with an uppercase letter while variable namesstart with lowercase letters.The code is documented at great length usingthe doxygen system,3 and the filling up of themissing parts is an ongoing effort.
Every toolcomes with an extensive help functionality, andthe main tools also have their own man pages.As for efficiency we always try to speed up thecode and reduce memory consumption by imple-menting better algorithms.
We try to avoid ?darkmagic programming methods?
and hard to followoptimizations are only applied in critical parts ofthe code.
We try to document every such occur-rence.4 Experimental ResultsIn this section we will present some experimentalresults obtained using Jane.
We will pay specialattention to the performance of the new reorderingand lexicon models presented in this paper.
Wewill present results on three different large-scaletasks and language pairs.Additionally RWTH participated in this year?sWMT evaluation, where Jane was one of the sub-mitted systems.
We refer to the system descriptionfor supplementary experimental results.3http://www.doxygen.orgdev testSystem BLEU TER BLEU TERJane baseline 24.2 59.5 25.4 57.4+ reordering 25.2 58.2 26.5 56.1Table 1: Results for Europarl German-Englishdata.
BLEU and TER results are in percentage.4.1 Europarl DataThe first task is the Europarl as defined in theQuaero project.
The main part of the corpus inthis task consists of the Europarl corpus as used inthe WMT evaluation (Callison-Burch et al, 2009),with some additional data collected in the scope ofthe project.We tried the reordering approach presented inSection 3.4 on the German-English language pair.The results are shown in Table 1.
As can be seenfrom these results, the additional reorderings ob-tain nearly 1% improvement both in BLEU andTER scores.
Regrettably for this corpus the ex-tended lexicon models did not bring any improve-ments.Table 2 shows the results for the French-Englishlanguage pair of the Europarl task.
On this taskthe extended lexicon models yield an improve-ment over the baseline system of 0.9% in BLEUand 0.9% in TER on the test set.4.2 NIST Arabic-EnglishWe also show results on the Arabic-EnglishNIST?08 task, using the NIST?06 set as develop-ment set.
It has been reported in other work thatthe hierarchical system is not competitive with aphrase-based system for this language pair (Birchet al, 2009).
We report the figures of our state-of-the-art phrase-based system as comparison (de-noted as PBT).As can be seen from Table 3, the baselineJane system is in fact 0.6% worse in BLEU and1.0% worse in TER than the baseline PBT sys-tem.
When we include the extended lexicon mod-els we see that the difference in performance is re-duced.
For Jane the extended lexicon models givean improvement of up to 1.9% in BLEU and 1.7%in TER, respectively, bringing the system on parwith the PBT system extended with the same lex-icon models, and obtaining an even slightly betterBLEU score.266dev testBLEU TER BLEU TERBaseline 30.0 52.6 31.1 50.0DWL 30.4 52.2 31.4 49.6Triplets 30.4 52.0 31.7 49.4path-constrained Triplets 30.3 52.1 31.6 49.3DWL + Triplets 30.7 52.0 32.0 49.1DWL + path-constrained Triplets 30.8 51.7 31.6 49.3Table 2: Results for the French-English task.
BLEU and TER results are in percentage.dev (MT?06) test (MT?08)Jane PBT Jane PBTBLEU TER BLEU TER BLEU TER BLEU TERBaseline 43.2 50.8 44.1 49.4 44.1 50.1 44.7 49.1DWL 45.3 48.7 45.1 48.4 45.6 48.4 45.6 48.4Triplets 44.4 49.1 44.6 49.2 45.3 48.8 44.9 49.0path-constrained Triplets 44.3 49.4 44.7 49.1 44.9 49.3 45.3 48.7DWL + Triplets 45.0 48.9 45.1 48.5 45.3 48.6 45.5 48.5DWL + path-constrained Triplets 45.2 48.8 45.1 48.6 46.0 48.5 45.8 48.3Table 3: Results for the Arabic-English task.
BLEU and TER results are in percentage.5 DiscussionWe feel that the hierarchical phrase-based transla-tion approach still shares some shortcomings con-cerning lexical selection with conventional phrase-based translation.
Bilingual lexical context be-yond the phrase boundaries is barely taken intoaccount by the base model.
In particular, if onlyone generic non-terminal is used, the selection ofa sub-phrase that fills the gap of a hierarchicalphrase is not affected by the words composing thephrase it is embedded in ?
except for the languagemodel score.
This shortcoming is one of the issuessyntactically motivated models try to address.The extended lexicon models analyzed in thiswork also try to address this issue.
One can con-sider that they complement the efforts that are be-ing made on a deep structural level within the hi-erarchical approach.
Though they are trained onsurface forms only, without any syntactic informa-tion, they still operate at a scope that exceeds thecapability of common feature sets of standard hi-erarchical phrase-based SMT systems.As the experiments in Section 4 show, the ef-fect of these extended lexicon models is more im-portant for the hierarchical phrase-based approachthan for the phrase-based approach.
In our opinionthis is probably mainly due to the higher flexibil-ity of the hierarchical system, both because of itsintrinsic nature and because of the higher numberof phrases extracted by the system.
The scoringof the phrases is still carried out by simple relativefrequencies, which seem to be insufficient.
Theadditional lexicon models seem to help in this re-spect.5.1 Short Comparison with JoshuaAs mentioned in Section 2, Joshua is the mostsimilar decoder to our own.
It was developed inparallel at the Johns Hopkins University and it is267System words/secJoshua 11.6Jane cube prune 15.9Jane cube grow 60.3Table 4: Speed comparison Jane vs. Joshua.
Wemeasure the translated words per second.currently used by a number of groups around theworld.Jane was started separately and independently.In their basic working mode, both systems imple-ment parsing using a synchronous grammar andinclude language model information.
Each of theprojects then progressed independently, most ofthe features described in Section 3 being onlyavailable in Jane.Efficiency is one of the points where we thinkJane outperforms Joshua.
One of the reasons canwell be the fact that it is written in C++ whileJoshua is written in Java.
In order to compare run-ning times we converted a grammar extracted byJane to Joshua?s format and adapted the parame-ters accordingly.
To the best of our knowledge weconfigured both decoders to perform the same task(cube pruning, 300-best generation, same pruningparameters).
Except for some minor differences4the results were equal.We tried this setup on the IWSLT?08 Arabic toEnglish translation task.
The speed results (mea-sured in translated words per second) can be seenin Table 4.
Jane operating with cube prune isnearly 50% faster than Joshua, at the same levelof translation performance.
If we switch to cubegrow, the speed difference is even bigger, witha speedup of nearly 4 times.
However this usu-ally comes with a penalty in BLEU score (nor-mally under 0.5% BLEU in our experience).
Thisincreased speed can be specially interesting forapplications like interactive machine translationor online translation services, where the responsetime is critical and sometimes even more impor-tant than a small (and often hardly noticeable) lossin translation quality.Another important point concerning efficiencyis the startup time.
Thanks to the binary formatdescribed in Section 3.9, there is virtually no delay4E.g.
the OOVs seem to be handled in a slightly differentway, as the placement was sometimes different.in the loading of the phrase table in Jane.5 In factJoshua?s long phrase table loading times were themain reason the performance measures were doneon a small corpus like IWSLT instead of one of thelarge tasks described in Section 4.We want to make clear that we did not go intogreat depth in the workings of Joshua, just stayedat the basic level described in the manual.
Thistool is used also for large-scale evaluations andhence there certainly are settings for dealing withthese big tasks.
Therefore this comparison has tobe taken with a grain of salt.We also want to stress that we explicitly choseto leave translation results out of this comparison.Several different components have great impacton translation quality, including phrase extraction,minimum error training and additional parametersettings of the decoder.
As we pointed out wedo not have the expertise in Joshua to perform allthese tasks in an optimal way, and for that reasonwe did not include such a comparison.
However,both JHU and RWTH participated in this year?sWMT evaluation, where the systems, applied bytheir respective authors, can be directly compared.And in no way do we see Joshua and Jane as?competing?
systems.
Having different systemsis always enriching, and particularly as systemcombination shows great improvements in trans-lation quality, having several alternative systemscan only be considered a positive situation.6 LicensingJane is distributed under a custom open sourcelicense.
This includes free usage for non-commercial purposes as long as any changes madeto the original software are published under theterms of the same license.
The exact formulationis available at the download page for Jane.7 ConclusionWith Jane, we release a state-of-the-art hi-erarchical toolkit to the scientific communityand hope to provide a good starting point forfellow researchers, allowing them to have asolid system even if the research field is newto them.
It is available for download fromhttp://www.hltpr.rwth-aachen.de/jane.
Thesystem in its current state is stable and efficientenough to handle even large-scale tasks such as5There is, however, still some delay when loading the lan-guage model for some of the supported formats.268the WMT and NIST evaluations, while producinghighly competitive results.Moreover, we presented additional reorderingand lexicon models that further enhance the per-formance of the system.And in case you are wondering, Jane is Just anAcronym, Nothing Else.
The name comes fromthe character in the Ender?s Game series (Card,1986).AcknowledgmentsSpecial thanks to the people who have contributedcode to Jane: Markus Freitag, Stephan Peitz, Car-men Heger, Arne Mauser and Niklas Hoppe.This work was partly realized as part of theQuaero Programme, funded by OSEO, FrenchState agency for innovation, and also partly basedupon work supported by the Defense AdvancedResearch Projects Agency (DARPA) under Con-tract No.
HR001-06-C-0023.
Any opinions,findings and conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the views of the DARPA.ReferencesAlexandra Birch, Phil Blunsom, and Miles Osborne.2009.
A Quantitative Analysis of Reordering Phe-nomena.
In Proc.
of the Workshop on Statistical Ma-chine Translation, pages 197?205, Athens, Greece,March.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A Discriminative Latent Variable Model for Statis-tical Machine Translation.
In Proc.
of the AnnualMeeting of the Association for Computational Lin-guistics (ACL), pages 200?208, Columbus, Ohio,June.Peter F. Brown, Stephan A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation: Pa-rameter Estimation.
Computational Linguistics,19(2):263?311, June.Richard H. Byrd, Peihuang Lu, Jorge Nocedal, andCiyou Zhu.
1995.
A Limited Memory Algorithmfor Bound Constrained Optimization.
SIAM Journalon Scientific Computing, 16(5):1190?1208.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProc.
of the Workshop on Statistical Machine Trans-lation, pages 1?28, Athens, Greece, March.Orson Scott Card.
1986.
Speaker for the Dead.
TorBooks.Jean-Ce?dric Chappelier and Martin Rajman.
1998.
AGeneralized CYK Algorithm for Parsing Stochas-tic CFG.
In Proc.
of the First Workshop on Tab-ulation in Parsing and Deduction, pages 133?137,Paris, France, April.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new Features for Statistical Machine Trans-lation.
In Proc.
of the Human Language TechnologyConference / North American Chapter of the Associ-ation for Computational Linguistics (HLT-NAACL),pages 218?226, Boulder, Colorado, June.David Chiang.
2007.
Hierarchical Phrase-basedTranslation.
Computational Linguistics, 33(2):201?228, June.Martin Cmejrek, Bowen Zhou, and Bing Xiang.
2009.Enriching SCFG Rules Directly From EfficientBilingual Chart Parsing.
In Proc.
of the Interna-tional Workshop on Spoken Language Translation(IWSLT), pages 136?143, Tokyo, Japan.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum Likelihood from IncompleteData via the EM Algorithm.
Journal of the RoyalStatistical Society Series B, 39(1):1?22.Sas?a Hasan and Hermann Ney.
2009.
Comparison ofExtended Lexicon Models in Search and Rescoringfor SMT.
In Proc.
of the Annual Meeting of the As-sociation for Computational Linguistics (ACL), vol-ume short papers, pages 17?20, Boulder, CO, USA,June.Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, andJesu?s Andre?s-Ferrer.
2008.
Triplet Lexicon Mod-els for Statistical Machine Translation.
In Proc.
ofthe Conference on Empirical Methods for NaturalLanguage Processing (EMNLP), pages 372?381.Liang Huang and David Chiang.
2007.
Forest Rescor-ing: Faster Decoding with Integrated LanguageModels.
In Proc.
of the Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages144?151, Prague, Czech Republic, June.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proc.
of the Annual Meeting of the Association forComputational Linguistics (ACL), pages 177?180,Prague, Czech Republic, June.Zhifei Li, Chris Callison-Burch, Chris Dyer, San-jeev Khudanpur, Lane Schwartz, Wren Thornton,Jonathan Weese, and Omar Zaidan.
2009.
Joshua:An Open Source Toolkit for Parsing-Based MachineTranslation.
In Proc.
of the Workshop on Statisti-cal Machine Translation, pages 135?139, Athens,Greece, March.269Arne Mauser, Sas?a Hasan, and Hermann Ney.
2009.Extending Statistical Machine Translation with Dis-criminative and Trigger-Based Lexicon Models.
InProc.
of the Conference on Empirical Methodsfor Natural Language Processing (EMNLP), pages210?218, Singapore, August.John A. Nelder and Roger Mead.
1965.
The DownhillSimplex Method.
Computer Journal, 7:308.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum Error Rate Train-ing for Statistical Machine Translation.
In Proc.
ofthe Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 160?167, Sapporo,Japan, July.William H. Press, Saul A. Teukolsky, William T. Vet-terling, and Brian P. Flannery.
2002.
NumericalRecipes in C++.
Cambridge University Press, Cam-bridge, UK.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.A New String-to-Dependency Machine TranslationAlgorithm with a Target Dependency LanguageModel.
In Proc.
of the Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages577?585, Columbus, Ohio, June.Andreas Stolcke.
2002.
SRILM ?
an Extensible Lan-guage Modeling Toolkit.
In Proc.
of the Interna-tional Conference on Spoken Language Processing(ICSLP), volume 3, pages 901?904, Denver, Col-orado, September.David Talbot and Miles Osborne.
2007.
SmoothedBloom Filter Language Models: Tera-scale LMs onthe Cheap.
In Proc.
of the Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 468?476, Prague, CzechRepublic, June.Ashish Venugopal, Andreas Zollmann, N.A.
Smith,and Stephan Vogel.
2009.
Preference Grammars:Softening Syntactic Constraints to Improve Statis-tical Machine Translation.
In Proc.
of the HumanLanguage Technology Conference / North Ameri-can Chapter of the Association for ComputationalLinguistics (HLT-NAACL), pages 236?244, Boulder,Colorado, June.David Vilar and Hermann Ney.
2009.
On LM Heuris-tics for the Cube Growing Algorithm.
In Proc.
ofthe Annual Conference of the European Associationfor Machine Translation (EAMT), pages 242?249,Barcelona, Spain, May.David Vilar, Daniel Stein, and Hermann Ney.
2008.Analysing Soft Syntax Features and Heuristics forHierarchical Phrase Based Machine Translation.In Proc.
of the International Workshop on Spo-ken Language Translation (IWSLT), pages 190?197,Waikiki, Hawaii, October.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Richard Zens and Hermann Ney.
2007.
EfficientPhrase-Table Representation for Machine Transla-tion with Applications to Online MT and SpeechTranslation.
In Proc.
of the Annual Meeting of theAssociation for Computational Linguistics (ACL),pages 492?499, Rochester, New York, April.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax Augmented Machine Translation via Chart Pars-ing.
In Proc.
of the Human Language TechnologyConference / North American Chapter of the Associ-ation for Computational Linguistics (HLT-NAACL),pages 138?141, New York, June.270
