Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133?137,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsJoshua 2.0: A Toolkit for Parsing-Based Machine Translationwith Syntax, Semirings, Discriminative Training and Other GoodiesZhifei Li, Chris Callison-Burch, Chris Dyer,?
Juri Ganitkevitch,Ann Irvine, Sanjeev Khudanpur, Lane Schwartz,?
Wren N.G.
Thornton,Ziyuan Wang, Jonathan Weese and Omar F. ZaidanCenter for Language and Speech Processing, Johns Hopkins University, Baltimore, MD?
Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD?
Natural Language Processing Lab, University of Minnesota, Minneapolis, MNAbstractWe describe the progress we have made inthe past year on Joshua (Li et al, 2009a),an open source toolkit for parsing basedmachine translation.
The new functional-ity includes: support for translation gram-mars with a rich set of syntactic nonter-minals, the ability for external modules toposit constraints on how spans in the in-put sentence should be translated, latticeparsing for dealing with input uncertainty,a semiring framework that provides a uni-fied way of doing various dynamic pro-gramming calculations, variational decod-ing for approximating the intractable MAPdecoding, hypergraph-based discrimina-tive training for better feature engineering,a parallelized MERT module, document-level and tail-based MERT, visualizationof the derivation trees, and a cleanerpipeline for MT experiments.1 IntroductionJoshua is an open-source toolkit for parsing-basedmachine translation that is written in Java.
Theinitial release of Joshua (Li et al, 2009a) was are-implementation of the Hiero system (Chiang,2007) and all its associated algorithms, includ-ing: chart parsing, n-gram language model inte-gration, beam and cube pruning, and k-best ex-traction.
The Joshua 1.0 release also includedre-implementations of suffix array grammar ex-traction (Lopez, 2007; Schwartz and Callison-Burch, 2010) and minimum error rate training(Och, 2003; Zaidan, 2009).
Additionally, it in-cluded parallel and distributed computing tech-niques for scalability (Li and Khudanpur, 2008).This paper describes the additions to the toolkitover the past year, which together form the 2.0 re-lease.
The software has been heavily used by theauthors and several other groups in their daily re-search, and has been substantially refined since thefirst release.
The most important new functions inthe toolkit are:?
Support for any style of synchronous contextfree grammar (SCFG) including syntax aug-ment machine translation (SAMT) grammars(Zollmann and Venugopal, 2006)?
Support for external modules to posit transla-tions for spans in the input sentence that con-strain decoding (Irvine et al, 2010)?
Lattice parsing for dealing with input un-certainty, including ambiguous output fromspeech recognizers or Chinese word seg-menters (Dyer et al, 2008)?
A semiring architecture over hypergraphsthat allows many inference operations to beimplemented easily and elegantly (Li andEisner, 2009)?
Improvements to decoding through varia-tional decoding and other approximate meth-ods that overcome intractable MAP decoding(Li et al, 2009b)?
Hypergraph-based discriminative training forbetter feature engineering (Li and Khudan-pur, 2009b)?
A parallelization of MERT?s computations,and supporting document-level and tail-basedoptimization (Zaidan, 2010)?
Visualization of the derivation trees and hy-pergraphs (Weese and Callison-Burch, 2010)?
A convenient framework for designing andrunning reproducible machine translation ex-periments (Schwartz, under review)The sections below give short descriptions foreach of these new functions.1332 Support for Syntax-based TranslationThe initial release of Joshua supported onlyHiero-style SCFGs, which use a single nontermi-nal symbol X.
This release includes support for ar-bitrary SCFGs, including ones that use a rich setof linguistic nonterminal symbols.
In particularwe have added support for Zollmann and Venu-gopal (2006)?s syntax-augmented machine trans-lation.
SAMT grammar extraction is identical toHiero grammar extraction, except that one side ofthe parallel corpus is parsed, and syntactic labelsreplace the X nonterminals in Hiero-style rules.Instead of extracting this Hiero rule from the bi-text[X]?
[X,1] sans [X,2] | [X,1] without [X,2]the nonterminals can be labeled according towhich constituents cover the nonterminal span onthe parsed side of the bitext.
This constrains whattypes of phrases the decoder can use when produc-ing a translation.[VP]?
[VBN] sans [NP] | [VBN] without [NP][NP]?
[NP] sans [NP] | [NP] without [NP]Unlike GHKM (Galley et al, 2004), SAMT hasthe same coverage as Hiero, because it allowsnon-constituent phrases to get syntactic labels us-ing CCG-style slash notation.
Experimentally, wehave found that the derivations created using syn-tactically motivated grammars exhibit more coher-ent syntactic structure than Hiero and typically re-sult in better reordering, especially for languageswith word orders that diverge from English, likeUrdu (Baker et al, 2009).3 Specifying Constraints on TranslationIntegrating output from specialized modules(like transliterators, morphological analyzers, andmodality translators) into the MT pipeline canimprove translation performance, particularly forlow-resource languages.
We have implementedan XML interface that allows external modulesto propose alternate translation rules (constraints)for a particular word span to the decoder (Irvineet al, 2010).
Processing that is separate fromthe MT engine can suggest translations for someset of source side words and phrases.
The XMLformat allows for both hard constraints, whichmust be used, and soft constraints, which competewith standard extracted translation rules, as wellas specifying associated feature weights.
In ad-dition to specifying translations, the XML formatallows constraints on the lefthand side of SCFGrules, which allows constraints like forcing a par-ticular span to be translated as an NP.
We modi-fied Joshua?s chart-based decoder to support theseconstraints.4 Semiring ParsingIn Joshua, we use a hypergraph (or packed forest)to compactly represent the exponentially manyderivation trees generated by the decoder for aninput sentence.
Given a hypergraph, we may per-form many atomic inference operations, such asfinding one-best or k-best translations, or com-puting expectations over the hypergraph.
Foreach such operation, we could implement a ded-icated dynamic programming algorithm.
How-ever, a more general framework to specify thesealgorithms is semiring-weighted parsing (Good-man, 1999).
We have implemented the in-side algorithm, the outside algorithm, and theinside-outside speedup described by Li and Eis-ner (2009), plut the first-order expectation semir-ing (Eisner, 2002) and its second-order version (Liand Eisner, 2009).
All of these use our newly im-plemented semiring framework.The first- and second-order expectation semi-rings can also be used to compute many interestingquantities over hypergraphs.
These quantities in-clude expected translation length, feature expec-tation, entropy, cross-entropy, Kullback-Leiblerdivergence, Bayes risk, variance of hypothesislength, gradient of entropy and Bayes risk, covari-ance and Hessian matrix, and so on.5 Word Lattice InputWe generalized the bottom-up parsing algorithmthat generates the translation hypergraph so thatit supports translation of word lattices instead ofjust sentences.
Our implementation?s runtime andmemory overhead is proportional to the size of thelattice, rather than the number of paths in the lat-tice (Dyer et al, 2008).
Accepting lattice-basedinput allows the decoder to explore a distributionover input sentences, allowing it to select the besttranslation from among all of them.
This is es-pecially useful when Joshua is used to translatethe output of statistical preprocessing components,such as speech recognizers or Chinese word seg-menters, which can encode their alternative analy-ses as confusion networks or lattices.1346 Variational DecodingStatistical models in machine translation exhibitspurious ambiguity.
That is, the probability of anoutput string is split among many distinct deriva-tions (e.g., trees or segmentations) that have thesame yield.
In principle, the goodness of a stringis measured by the total probability of its manyderivations.
However, finding the best string dur-ing decoding is then NP-hard.
The first version ofJoshua implemented the Viterbi approximation,which measures the goodness of a translation us-ing only its most probable derivation.The Viterbi approximation is efficient, but it ig-nores most of the derivations in the hypergraph.We implemented variational decoding (Li et al,2009b), which works as follows.
First, given a for-eign string (or lattice), the MT system produces ahypergraph, which encodes a probability distribu-tion p over possible output strings and their deriva-tions.
Second, a distribution q is selected that ap-proximates p as well as possible but comes froma family of distributions Q in which inference istractable.
Third, the best string according to q(instead of p) is found.
In our implementation,the q distribution is parameterized by an n-grammodel, under which the second and third steps canbe performed efficiently and exactly via dynamicprogramming.
In this way, variational decodingconsiders all derivations in the hypergraph but stillallows tractable decoding.7 Hypergraph-based DiscriminativeTrainingDiscriminative training with a large number offeatures has potential to improve the MT perfor-mance.
We have implemented the hypergraph-based minimum risk training (Li and Eisner,2009), which minimizes the expected loss of thereference translations.
The minimum-risk objec-tive can be optimized by a gradient-based method,where the risk and its gradient can be computedusing a second-order expectation semiring.
Foroptimization, we use both L-BFGS (Liu et al,1989) and Rprop (Riedmiller and Braun, 1993).We have also implemented the average Percep-tron algorithm and forest-reranking (Li and Khu-danpur, 2009b).
Since the reference translationmay not be in the hypergraph due to pruning or in-herent defficiency of the translation grammar, weneed to use an oracle translation (i.e., the transla-tion in the hypergraph that is most simmilar to thereference translation) as a surrogate for training.We implemented the oracle extraction algorithmdescribed by Li and Khudanpur (2009a) for thispurpose.Given the current infrastructure, other trainingmethods (e.g., maximum conditional likelihood orMIRA as used by Chiang et al (2009)) can also beeasily supported with minimum coding.
We planto implement a large number of feature functionsin Joshua so that exhaustive feature engineering ispossible for MT.8 Minimum Error Rate TrainingJoshua?s MERT module optimizes parameterweights so as to maximize performance on a de-velopment set as measuered by an automatic eval-uation metric, such as Bleu (Och, 2003).We have parallelized our MERT module intwo ways: parallelizing the computation of met-ric scores, and parallelizing the search over pa-rameters.
The computation of metric scores isa computational concern when tuning to a met-ric that is slow to compute, such as translationedit rate (Snover et al, 2006).
Since scoring acandidate is independent from scoring any othercandidate, we parallelize this computation using amulti-threaded solution1.
Similarly, we parallelizethe optimization of the intermediate initial weightvectors, also using a multi-threaded solution.Another feature is the module?s awareness ofdocument information, and the capability to per-form optimization of document-based variants ofthe automatic metric (Zaidan, 2010).
For example,in document-based Bleu, a Bleu score is calculatedfor each document, and the tuned score is the aver-age of those document scores.
The MERT modulecan furthermore be instructed to target a specificsubset of those documents, namely the tail subset,where only the subset of documents with the low-est document Bleu scores are considered.2More details on the MERT method and the im-plementation can be found in Zaidan (2009).31Based on sample code by Kenneth Heafield.2This feature is of interest to GALE teams, for instance,since GALE?s evaluation criteria place a lot of focus on trans-lation quality of tail documents.3The module is also available as a standalone applica-tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/?ozaidan/zmert.
)1359 VisualizationWe created tools for visualizing two of themain data structures used in Joshua (Weese andCallison-Burch, 2010).
The first visualizer dis-plays hypergraphs.
The user can choose from aset of input sentences, then call the decoder tobuild the hypergraph.
The second visualizer dis-plays derivation trees.
Setting a flag in the con-figuration file causes the decoder to output parsetrees instead of strings, where each nonterminal isannotated with its source-side span.
The visual-izer can read in multiple n-best lists in this format,then display the resulting derivation trees side-by-side.
We have found that visually inspecting thesederivation trees is useful for debugging grammars.We would like to add visualization tools formore parts of the pipeline.
For example, a chartvisualizer would make it easier for researchers totell where search errors were happening duringdecoding, and why.
An alignment visualizer foraligned parallel corpora might help to determinehow grammar extraction could be improved.10 Pipeline for Running MTExperimentsReproducing other researchers?
machine transla-tion experiments is difficult because the pipeline istoo complex to fully detail in short conference pa-pers.
We have put together a workflow frameworkfor designing and running reproducible machinetranslation experiments using Joshua (Schwartz,under review).
Each step in the machine transla-tion workflow (data preprocessing, grammar train-ing, MERT, decoding, etc) is modeled by a Makescript that defines how to run the tools used in thatstep, and an auxiliary configuration file that de-fines the exact parameters to be used in that stepfor a particular experimental setup.
Workflowsconfigured using this framework allow a completeexperiment to be run ?
from downloading data andsoftware through scoring the final translated re-sults ?
by executing a single Makefile.This framework encourages researchers to sup-plement research publications with links to thecomplete set of scripts and configurations thatwere actually used to run the experiment.
TheJohns Hopkins University submission for theWMT10 shared translation task was implementedin this framework, so it can be easily and exactlyreproduced.AcknowledgementsResearch funding was provided by the NSF un-der grant IIS-0713448, by the European Commis-sion through the EuroMatrixPlus project, and bythe DARPA GALE program under Contract No.HR0011-06-2-0001.
The views and findings arethe authors?
alone.ReferencesKathy Baker, Steven Bethard, Michael Bloodgood,Ralf Brown, Chris Callison-Burch, Glen Copper-smith, Bonnie Dorr, Wes Filardo, Kendall Giles,Anni Irvine, Mike Kayser, Lori Levin, Justin Mar-tineau, Jim Mayfield, Scott Miller, Aaron Phillips,Andrew Philpot, Christine Piatko, Lane Schwartz,and David Zajic.
2009.
Semantically informed ma-chine translation (SIMT).
SCALE summer work-shop final report, Human Language TechnologyCenter Of Excellence.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine transla-tion.
In NAACL, pages 218?226.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Christopher Dyer, Smaranda Muresan, and PhilipResnik.
2008.
Generalizing word lattice transla-tion.
In Proceedings of ACL-08: HLT, pages 1012?1020, Columbus, Ohio, June.
Association for Com-putational Linguistics.Jason Eisner.
2002.
Parameter estimation for proba-bilistic finite-state transducers.
In ACL.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In HLT-NAACL.Joshua Goodman.
1999.
Semiring parsing.
Computa-tional Linguistics, 25(4):573?605.Ann Irvine, Mike Kayser, Zhifei Li, Wren Thornton,and Chris Callison-Burch.
2010.
Integrating out-put from specialized modules in machine transla-tion: Transliteration in joshua.
The Prague Bulletinof Mathematical Linguistics, 93:107?116.Zhifei Li and Jason Eisner.
2009.
First- and second-order expectation semirings with applications tominimum-risk training on translation forests.
InEMNLP, Singapore.Zhifei Li and Sanjeev Khudanpur.
2008.
A scalabledecoder for parsing-based machine translation withequivalent language model state maintenance.
InACL SSST, pages 10?18.Zhifei Li and Sanjeev Khudanpur.
2009a.
Efficientextraction of oracle-best translations from hyper-graphs.
In Proceedings of NAACL.136Zhifei Li and Sanjeev Khudanpur.
2009b.
Forestreranking for machine translation with the percep-tron algorithm.
In GALE book chapter on ?MTFrom Text?.Zhifei Li, Chris Callison-Burch, Chris Dyer, JuriGanitkevitch, Sanjeev Khudanpur, Lane Schwartz,Wren Thornton, Jonathan Weese, and Omar.
Zaidan.2009a.
Joshua: An open source toolkit for parsing-based machine translation.
In WMT09.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.2009b.
Variational decoding for statistical machinetranslation.
In ACL.Dong C. Liu, Jorge Nocedal, Dong C. Liu, and JorgeNocedal.
1989.
On the limited memory bfgsmethod for large scale optimization.
MathematicalProgramming, 45:503?528.Adam Lopez.
2007.
Hierarchical phrase-based trans-lation with suffix arrays.
In EMNLP-CoNLL.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In ACL.Martin Riedmiller and Heinrich Braun.
1993.
Adirect adaptive method for faster backpropagationlearning: The rprop algorithm.
In IEEE INTER-NATIONAL CONFERENCE ON NEURAL NET-WORKS, pages 586?591.Lane Schwartz and Chris Callison-Burch.
2010.
Hier-archical phrase-based grammar extraction in joshua.The Prague Bulletin of Mathematical Linguistics,93:157?166.Lane Schwartz.
under review.
Reproducible results inparsing-based machine translation: The JHU sharedtask submission.
In WMT10.Matthew Snover, Bonnie J. Dorr, and RichardSchwartz.
2006.
A study of translation edit ratewith targeted human annotation.
In AMTA.Jonathan Weese and Chris Callison-Burch.
2010.
Vi-sualizing data structures in parsing-based machinetranslation.
The Prague Bulletin of MathematicalLinguistics, 93:127?136.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.Omar F. Zaidan.
2010.
Document- and tail-based min-imum error rate training of machine translation sys-tems.
In preparation.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart pars-ing.
In Proceedings of the NAACL-2006 Workshopon Statistical Machine Translation (WMT-06), NewYork, New York.137
