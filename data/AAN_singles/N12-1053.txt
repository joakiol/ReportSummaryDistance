2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 488?497,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsTraining Dependency Parser Using Light FeedbackAvihai MejerDepartment of Electrical EngineeringTechnion-Israel Institute of TechnologyHaifa 32000, Israelamejer@tx.technion.ac.ilKoby CrammerDepartment of Electrical EngineeringTechnion-Israel Institute of TechnologyHaifa 32000, Israelkoby@ee.technion.ac.ilAbstractWe introduce lightly supervised learning fordependency parsing.
In this paradigm, the al-gorithm is initiated with a parser, such as onethat was built based on a very limited amountof fully annotated training data.
Then, the al-gorithm iterates over unlabeled sentences andasks only for a single bit of feedback, ratherthan a full parse tree.
Specifically, given anexample the algorithm outputs two possibleparse trees and receives only a single bit indi-cating which of the two alternatives has morecorrect edges.
There is no direct informationabout the correctness of any edge.
We showon dependency parsing tasks in 14 languagesthat with only 1% of fully labeled data, andlight-feedback on the remaining 99% of thetraining data, our algorithm achieves, on av-erage, only 5% lower performance than whentraining with fully annotated training set.
Wealso evaluate the algorithm in different feed-back settings and show its robustness to noise.1 IntroductionSupervised learning is a dominant paradigm in ma-chine learning in which a prediction model is builtbased on examples, each of which is composed of in-puts and a corresponding full annotation.
In the taskof parsing, examples are composed of sentences insome language and associated with full parse trees.These parse trees are often generated by human an-notators.
The annotation process is complex, slowand prone to mistakes as for each sentence a full cor-rect feedback is required.We describe light-feedback learning which suitslearning problems with complex or structured out-put, like parsing.
After building an initial classi-fier, our algorithm reduces the work of the annota-tor from a full annotation of the input sentence toa single bit of information.
Specifically, it providesthe annotator with two alternative parses of the in-put sentence and asks for the single bit indicatingwhich of the alternatives is better.
In 95% of thesentences both alternatives are identical except for asingle word.
See Fig.
2 for an illustration.
Thus,the work of the annotator boils down to decidingfor some specific word in the sentence which of twopossible words should be that word?s head.We show empirically, through simulation, that us-ing only 1% of the training set with full annotation,and the remaining 99% with light annotation, our al-gorithm achieves an average accuracy of about 80%,only 5% less than a parser built with full annotatedtraining data.
These results are averaged over 14languages.
With additional simple relaxations, ouralgorithm achieves average accuracy of 82.5%, notfar from the performance of an algorithm observingfull annotation of the data.
We also evaluate our al-gorithm under few noise settings, showing that it isresistant to noise, with a decrease of only 1.5% inaccuracy under about 10% feedback noise.
We defera discussion of related work to the end of the paper.2 Dependency Parsing and ParsersDependency parsing of a sentence is an intermediatebetween shallow-parsing, in which a given sentenceis annotated with its part-of-speech, and between afull structure over the sentence, such as the ones de-488fined using context-free grammar.
Given a sentencewith n words a parse tree is defined by constructinga single directed edge outgoing from each word toits head, that is the word it depends on according tosyntactic or semantic rules.
Additionally, one of thewords of the sentence must be labeled as the root ofthe tree.
The choice of edges is restricted to inducetrees, i.e.
graphs with no loops.Dependency parsers, such as the MSTParser of(McDonald et al, 2005), construct directed edgesbetween words of a given sentence to their argu-ments.
We focus on non-projective parsing withnon-typed (unlabeled) edges.
MSTParser producesa parse tree for a sentence by constructing a full di-rected graph over the words of the sentence withweighted edges, and then outputting the maximalspanning tree (MST) of the graph.
Given a true parsetree (aka as gold labeling) and a predicted parse treey?, we evaluate the latter by counting the number ofwords that are in agreement with the true labeling.The MSTParser maintains a linear model for set-ting the weights of the edges of the full graph.
Giventhe input sentence x the parser sets the weight ofthe edge between words xi and xj to be s(i, j) =w?f(x, i, j) using a feature function f that maps theinput x and a pair of possibly connected words intoRd.
Example features are the distance between thetwo words, words identity and words part-of-speech.The goal of the learning algorithm is to choose aproper value of w such that the induced tree for eachsentence x will have high accuracy.Online Learning: MSTParser is training a modelby processing one example at a time using onlinelearning.
On each round the algorithm receives anew sentence x and the set of correct edges y. Itthen computes the score-value of all possible di-rected edges, s(i, j) = w ?
f (x, i, j) for words i, jusing the current parameters w. The algorithm iscomputing the best dependency tree y?
of this inputx defined to be the MST of the weighted completedirected graph induced from the matrix {s(i, j)}.
Itthen uses the discrepancy between the true parse treey and the predicted parse tree y?
to modify the weightvector.MSTParser specifically employs the MIRA algo-rithm (Crammer et al, 2006) to update the weightvector w using a linear update,w?w+????
(i,j)?yf(x, i, j)??
(i,j)?y?f(x, i, j)??
(1)for input-dependent scalar ?
that is defined by thealgorithm.
By construction, correct edges (i, j), thatappear both in the true parse tree y and the predictedparse tree y?, are not affecting the update, as theterms in the two sums of Eq.
(1) cancel each other.3 Online Learning with Light FeedbackSupervised learning is a very common paradigm inmachine learning, where we assume having accessto the correct full parse tree of every input sentence.Many algorithms, including MSTParser, explicitlyassume this kind of feedback.
Supervised learn-ing algorithms achieve good performance in depen-dency parsing, but they come with a price.
Humanannotators are required to fully parse each and ev-ery sentence in the corpora, a long, tedious and ex-pensive process, which is also prone to mistakes.For example, the first phase of the famous penn treebank project (Marcus et al, 1993) lasted three years,in which annotators corrected outputs of automatedmachines in a rate of 475 words per hour.
For super-vised learning to be successful, typically a large setof thousands instances is required, which translatesto a long and expensive annotation phase.Binary or multi-class prediction tasks, such asspam filtering or document classification, are sim-ple in the sense that the label associated with eachinstance or input is simple.
It is either a single bitindicating whether the input email is spam or not,or one of few values from a fixed predefined set iftopics.
Dependency parsing is more complex as adecision is required for every word of the sentence,and additionally there is a global constraint of theparse being a tree.In binary classification or multi-class problems itis only natural to either annotate (or label) an exam-ple, or not, since the labels are atomic, they cannotbe decomposed to smaller components.
The situa-tion is different in structured tasks such as depen-dency parsing (or sequence labeling) where each in-stance is constructed of many elements that eachneeds to be annotated.
While there are relations and489coupling between the elements it is possible to anno-tate an instance only partially, such as provide a de-pendency edge only to several words in a sentence.We take this approach to the extreme, and con-sider (for now) that for each sentence only a singlebit of labeling will be provided.
The choice of whatbit to require is algorithm and example dependent.We propose using a light feedback scheme in orderto significantly reduce annotation effort for depen-dency parsing.
First, a base or initial model will belearned from a very small set of fully annotated ex-amples, i.e.
sentences with full dependency infor-mation known.
Then, in a second training stage thealgorithm works in rounds.
On each round the al-gorithm is provided with a new non-annotated sen-tence which it annotates, hopefully making the rightdecision for most of the words.
Then the algorithmchooses subset of the words (or segments) to be an-notated by humans.
These words are the ones that al-gorithm estimates to be the hardest, or that their truelabel would resolve any ambiguity that is currentlyexisting with the parsing of the input sentence.Although such partial annotation task may be eas-ier and faster to annotate, we realize that even partialannotation if not limited enough can require eventu-ally similar effort as annotating the entire sentence.For example, if for a 25-words sentence annotationis requested for 5 words scattered over the entire sen-tence, providing this annotation may require the an-notator to basically parse the entire sentence.We thus further restrict the possible feedback re-quested from the annotator.
Specifically, given anew sentence our algorithm outputs two possible an-notations, or parse trees, y?A and y?B , and asks fora single bit from the annotator, indicating whetherparse A is better or parse B is better.
We do notask the annotator to parse the actual sentence, or de-cide what is the correct parse, but only to state whichof the parses is quantitatively better.
Formally, wesay that parse y?A is better if it contains more correctedges than y?B .
The annotator is asked for a singlebit, and thus must state one of the two parses, evenif both parses are equally good.
We denote this la-beling paradigm as binary, as the annotator providesbinary feedback.The two parses our algorithms presents to the an-notator are the highest ranking parse and the sec-ond highest ranking parse according to the currentmodel.
That is, the parse it would output for x andthe best alternative.
The feedback required from theannotator is only which of the two parses is better,the annotator does not explicitly indicate which ofthe edges are labeled correctly or incorrectly, andfurthermore, the annotator does not provide any ex-plicit information about the correct edge of any ofthe words.In general, the two alternative parses presentedto the annotator may be significantly different fromeach other; they may disagree on the edges of manywords.
In this case the task of deciding which ofthem is better may be as hard as annotating the en-tire sentence, and then comparing the resulting an-notation to both alternatives.
In practice, however,due to our choice of features (as functions of the twowords) and model (linear), and since our algorithmchooses the two parse-trees ranked highest and sec-ond highest, the difference between the two alterna-tives is very small.
In fact, we found empiricallythat, on average, in 95% of the sentences, they differin the labeling of only a single word.
That is, bothy?A and y?B agree on all words, except some word xi,for which the first alternative assigns to some wordxj and the second alternative assign to other wordxk.
This is due to the fact that the score of the parsesare additive in the edges.
Therefore, the parse treeranked second highest is obtained from the highest-ranked parse tree, where for a single word the edge isreplaced, such that the difference between scores isminimized.
For the remaining 5% of the sentences,replacing an edge as described causes a loop in thegraph induced over words, and thus more than a sin-gle edge is modified.
To minimize the potential laborof the annotator we simply ignore these cases, andpresent the annotator only two alternatives which aredifferent in a single edge.
We refer to this setting orscenario as single.To conclude, given a new non-annotated sentencex the algorithm uses its current model w to out-put two annotations y?A and y?B which are differentonly on a single word and ask the annotator whichis better.
The annotator should decide to which oftwo possible words xj and xk to connect the wordxi in question.
The annotator then feeds the algo-rithm a single bit, i.e.
a binary labeling, which rep-resents which alternative is better, and the algorithmupdates its internal model w. Although it may be the490Input data A set of n unlabeled sentences {xi}ni=1Input parameters Initial weight vector learned fromfully annotated data u; Number of Iterations over the un-labeled data TInitialize w ?
uFor t = 1, .
.
.
, T?
For i = 1, .
.
.
, n?
Compute the two configurations y?A and y?Bwith highest scores of xi using w?
Ask for feedback : y?A vs. y?B?
Get feedback ?
?
{+1,?1}(or ?
?
{+1, 0,?1} in Sec.
5)?
Compute the value of ?
using the MIRA algo-rithm ( or just set ?
= 1 for simplicity)?
Updatew+???(i,j)?
(y?A/y?B?y?B/y?A)(?1)[[(i,j)?y?B ]]f(x, i, j)Output: Weight vector wFigure 1: The Light-Feedback learning algorithmcase that both alternatives are equally good (or bad),which occurs only when both assign the wrong wordto xi, that is not xj nor xk are the correct dependentsof xi, the annotator is still required to respond withone alternative, even though a wrong edge is recom-mended.
Although this setting may induce noise, weconsider it since a human annotator, that is asked toprovide a quick light feedback, will tend to chooseone of the two proposed options, the one that seemsmore reasonable, even if it is not correct.
We refer tothis combined setting of receiving a binary feedbackonly about a single word as Binary-Single.
Belowwe discuss alternative models where the annotatormay provide additional information, which we hy-pothesize, would be for the price of labor.Finally, given the light-feedback ?
from the anno-tator, where ?
= +1 if the first parse y?A is preferredover the second parse y?B , and ?
= ?1 otherwise,we employ a single online update,w ?w + ?????
(i,j)?y?Af(x, i, j)??
(i,j)?y?Bf(x, i, j)?
?Pseudocode of the algorithm appears in Fig.
1.From the last equation we note that the update de-pends only on the edges that are different betweenFigure 2: Example of single edge feedback.
The solid blue ar-rows describe the proposed parse and the two dashed red arrowsare the requested light feedback.the two alternatives.
This provides us the flexibil-ity of what to show the annotator.
One extreme isto provide the annotator with (almost) a full depen-dency parse tree, that both alternatives agree on, aswell as the dilemma.
This provides the annotatorsome context to assist of making a right decision andfast.
The other extreme, is to provide the annotatoronly the edges for which the algorithm is not sureabout, omitting any edges both alternatives agree on.This may remove labeling noise induced by erro-neous edges both alternatives mistakenly agree on.Formally, these options are equivalent, and the de-cision which to use may even be dependent on theindividual annotator.An example of a light-feedback request is shownin Fig.
2.
The sentence is 12 words long andthe parser succeeded to assign correct edges for 11words.
It was uncertain whether there was a ?sale byfirst boston corps?
- having the edge ?by?sale?
(in-correct), or there was an ?offer by first boston corps?- having the edge ?by?offered?
(correct).
In thisexample, a human annotator can easily clarify thedilemma.4 EvaluationWe evaluated the light feedback model using 14 lan-guages: English (the Penn Tree Bank) and the re-maining 13 were used in CoNLL 2006 shared task1.The number of training sentences in the trainingdatasets is ranging is between about 1.5?57K, withan average of about 14K sentences and 50K?700Kwords.
The test sets contain an average of ?
590sentences and ?10K words for all datasets.
The av-erage number of words per sentence vary from 6 inChinese to 37 in Arabic.1Arabic, Bulgarian, Chinese, Czech, Danish, Dutch, Ger-man, Japanese, Portuguese, Slovene, Spanish, Swedish andTurkish .
See http://nextens.uvt.nl/?conll/491Experimental Setup For each of the languageswe split the data into two parts of relative fraction ofp and 1?p for p = 10%, 5% and 1% and performedtraining in two stages.
First, we used the smaller setto build a parser using standard supervised learningprocedure.
Specifically, we used MSTParser and ranthe MIRA online learning algorithm for 5 iterations.This process yielded our initial parser.
Second, thelarger portion, which is the remaining of the trainingset, was used to improve the initial parser using thelight feedback algorithm described above.
Our algo-rithm iterates over the sentences of the larger subsetand each sentence was parsed by the current parser(parameterized by w) and asked for a preference be-tween two specific parses for that sentence.
Giventhis feedback, the algorithm updated its model andproceeded for the next sentence.
The true parse ofthese sentences was only used to simulate light feed-back and it was never provided to the algorithm.
Theperformance of all the trained parsers was evaluatedon a fixed test set.
We performed five iterations ofthe larger subset during the light feedback training.4.1 ResultsThe results of the light-feedback training after onlya single iteration are given in the two left plots ofFig.
3.
One plot shows the performance averagedover all languages, and second plot show the resultsfor English.
The black horizontal line shows the ac-curacy achieved by training the parser on the entireannotated data using the MIRA algorithm for 10 it-erations.
The predicted edge accuracy of the parsertrained on the entire annotated dataset ranges from77% on Turkish to 93% on Japanese, with an aver-age of 85%.
This is our skyline.The blue bars in each plot shows the accuracy ofa parser trained with only a fraction of the dataset- 10% (left group), via 5% (middle) to 1% (rightgroup).
As expected reducing the amount of trainingdata causes degradation in performance, from an ac-curacy of about 76.3% (averaged over all languages)via 75% to 70.1% when training only with 1% of thedata.
These performance levels are our baselines,one per specific amount of fully annotated data andlightly annotated data.The red bar in each pair, shows the contributionof a single training epoch with light-feedback on theperformance.
We see that training with light feed-back improves the performance of the final parser.Most noticeably, is when using only 1% of the fullyannotated data for initial training, and the remaining99% of the training data with light feedback.
Theaccuracy on test set improves from 70.1% to 75.6%,an absolute increase of 5.5%.
These results are av-eraged over all languages, individual results for En-glish are also shown.
In most languages, includingthose not shown, these trends remain: when reduc-ing the fraction of data used for fully supervisedtraining the performance decreases, and light feed-back improves it, most substantially for the smallestfraction of 1%.We also evaluated the improvement in accuracyon the test set by allowing more than a single itera-tion over the larger fraction of the training set.
Theresults are summarized in two right plots of Fig.
3,accuracy averaged over all languages (left), and forEnglish (right).
Each line refers to a different ratioof split between full supervised learning and lightfeedback learning - blue for 90%, green for 95% andred for 99%.
The x-axis is the number of light feed-back iterations, from zero up to five.
The y-axis isthe accuracy.
In general more iterations translates toimprovement in performance.
For example, build-ing a parser with only 1% of the training data yields70.1% accuracy on the test set, a single iteration oflight-feedback on the remaining 99% improves theperformance to 75.6%, each of the next iterationsimproves the accuracy by about 1?
2% up to an ac-curacy of about 80%, which is only 5% lower thanthe skyline.
We note again, that the skyline was ob-tained by using full feedback on the entire trainingset, while our parser used at most five bits of feed-back per sentence from the annotator, one bit per it-eration.As noted above, on each sentence, and each it-eration, our algorithm presents a parsing query or?dilemma?
: should word a be assigned to word bor word c. These queries are generated indepen-dently of the previous queries shown, and in fact thesame query may be presented again in a later iter-ation although already shown in an early one.
Wethus added a memory storage of all queries to thealgorithm.
When a query is generated by the algo-rithm, it first checks if an annotation of it alreadyexists in memory.
If this is the case, then no query isissued to the annotator, and the algorithm simulates49290 95 99707274767880828486Average%LightFeedbackAccuracy on test setw/o light feedbackw light feedbackwith ALL data 90 95 9978808284868890English%LightFeedbackAccuracy on test setw/o light feedbackw light feedbackwith ALL data 0 1 2 3 4 5707274767880828486AverageIterationAccuracy on testset909599with ALL data 0 1 2 3 4 578808284868890EnglishIterationAccuracy on testset909599with ALL dataFigure 3: Two left plots: Evaluation in Binary-Single light feedback setting.
Averaged accuracy over all languages (left) and forEnglish.
The horizontal black line shows the accuracy when training the parser on the entire annotated training data - ?skyline?.Each pair of bars shows the results for a parser trained with small amount of fully annotated data (left blue bar) and a parser thatwas then trained with a single iteration of light feedback on the remaining training data (right red bar).
Two right plots: Evaluationof training with up to five iterations of binary-single light feedback.
The plots show the average accuracy (left), and for English.Each line refers to a different ratio of split between full supervised learning and light feedback learning.
The x-axis is the numberof iterations of light feedback, from zero to five.a query and response using the stored information.The fraction of new queries, that were actuallypresented to the annotator, when light-training with99% of the training set, is shown in the left panelof Fig.
4.
Each line corresponds to one language.The languages are ordered in the legend accordingto the average number of words per sentence: fromChinese (6) to Arabic (37).
Each point shows thefraction of new queries (from the total number ofsentences with light-feedback) (y-axis) vs. the itera-tion (x-axis).
Two trends are observed.
First, in lateriterations there are less and less new queries (or needfor an actual interaction with the annotator).
By def-inition, all queries during the first iteration are new,and the fraction of new queries after five iterationranges from about 20% (Japanese and Chinese) to abit less than 80% (Arabic).The second trend is across the average number ofwords per sentence, the larger this number is, themore new queries there are in multiple iterations.For example, in Arabic (37 words per sentence) andSpanish (28) about 80% of the light-training sen-tences induce new queries in the fifth iteration, whilein Chinese (6) and Japanese (8) only about 20%.As expected, longer sentences require, on average,more queries before getting their parse correctly.We can also compare the performance improve-ment achieved by light feedbacks with the per-formance achieved by using the same amount oflabeled-edges using fully annotated sentences instandard supervised training.
The average sentencelength across all languages is 18 words.
Thus, re-ceiving feedback regarding a single word in a sen-tence equals to about 1/18 ?
5.5% of the informa-tion provided by a fully annotated sentence.
There-fore, we may view the light-feedback provided for99% of the dataset as about equal to additional 5.5%of fully annotated data.From the second plot from the right of Fig.
3, wesee that by training with 1% of fully annotated dataand a single iteration of light feedback over the re-maining 99% of the data, the parser performanceis 75.6% (square markers at x = 1), compared to75% obtained by training with 5% of fully anno-tated data (diamond markers at x = 0).
A seconditeration of light feedback on 99% of the datasetcan be viewed as additional .
5% of labeled data(accounting for repeating queries).
After the sec-ond light feedback iteration, the parser performanceis 77.8% (square markers at x = 2), compared to76.3% achieved when training with 10% of fully an-notated data (circle markers at x = 0).
Similar rela-tions can be observed for English in the right plot ofFig.
3.
From these observations, we learn that on av-erage, for about the same amount of labeled edges,light feedback learning gains equal, or even better,performance compared with fully labeled sentences.5 Light Feedback VariantsOur current model is restrictive in two ways: first,the algorithm does not pass to the annotators exam-ples for which the disagreements is larger than oneword; and second, the annotator must prefer one ofthe two alternatives.
Both restrictions were set tomake the annotators?
work easier.
We now describethe results of experiments in which one or even both4931 2 3 4 500.10.20.30.40.50.60.70.80.91IterationsFractionof New Edges Queried bythe Annotatorchinese ( 6)japanese ( 8)turkish (12)dutch (14)bulgarian (15)swedish (15)slovene (16)danish (18)portuguese (20)english (23)spanish (28)arabic (37) 90 95 990.780.790.80.810.820.830.840.85% of Light feedback dataAccuracy ontest setBinary?SingleBinary?MultiTernary?SingleTernary?Multi 0 5 10 15 20 25 300.70.720.740.760.780.80.820.84% of feedback noiseAccuracy on test setBinary?MultiTernary?MultiFigure 4: Left: the fraction of new queries presented to the annotator after each of the five iterations (x-axis) for all 14 languages,when light-training with 99% of the entire training data.
Middle: comparison of the accuracy achieved using the four light feedbackmodels using different fraction of the data for light feedback stage.
The results are averaged over all the languages.
Right: Effect oflight-feedback noise on the accuracy of the trained model.
Results are averaged over all languages for two light feedback settings,the ternary-multi and binary-multi.
The plots show the performance measured on test set according the amount of feedback noiseadded.
The black line is the baseline of the initial parser trained on 1% of annotated data.restrictions are relaxed, which may make the workof the annotator harder, but as we shall see, improvesperformance.Our first modification is to allow the algorithm topass the annotator also queries on two alternativesy?A and y?B that differ on more than a single edge.As mentioned before, we found empirically that thisarises in only ?5% of the instances.
In most casesthe two alternatives differ in two edges, but in somecases the alternatives differ in up to five edges.
Typ-ically when the alternatives differ on more than asingle edge, the words in question are close to eachother in the sentence (in terms of word-distance)and are syntactically related to each other.
For ex-ample, if changing the edge (i, j) to (i, k) forms aloop in the dependency graph then also another edge(k, l) must be changed to resolve the loop, so thetwo edges different between the alternatives are re-lated.
Nevertheless, even if the two alternatives arefar from being similar, the annotator is still requiredto provide only a binary feedback, indicating a strictpreference between the two alternatives.
We refer tothis model as Binary-Multi, for binary feedback andpossibly multiple different edge between the alter-natives.Second, we enrich the number of possible re-sponses of the annotator from two to three, givingthe annotator the option to respond that the two al-ternatives y?A and y?B are equally good (or bad), andno one should be preferred by the other.
In this casewe set ?
= 0 in the algorithm of Fig.
1, and as canbe seen in the pseudocode, this case does not modifythe weight vector w associated with the parser.
Suchfeedback will be received when both parses have thesame number of errors.
(We can also imagine a hu-man annotator using the equal feedback to indicate?don?t know?).
For the common case of single edgedifference between the two parses, this means thatboth proposed edges are incorrect.
Since there arethree possible responds we call this setting ternary.This setting can be combined with the previous oneand thus we have in fact two new settings.
The thirdsetting is when only single edges are presented to theannotator, yet three possible responds are optional.We call this setting Ternary-Single .
The fourth, iswhen the two alternatives may differ in more than asingle edge and three possible responds are optional- Ternary-Multi setting.The accuracy, averaged over all 14 languages, af-ter 5 light feedback iterations, for all four settingsis shown in the middle plate of Fig.
4.
Each ofthe three groups summarizes the results for differ-ent split of the training set to full training and light-training: 90%, 95% and 99% (left to right; portionof light training).
The horizontal black line showsthe accuracy skyline (85% obtained by using all thetraining set in full supervised learning).
Each bar ineach group shows the results for one of the four set-tings: Binary-Single, Binary-Multi, Ternary-Singleand Ternary-Multi.
We focus our discussion in the99% split.
The averaged accuracy using Binary-Single feedback setting is about 80% (left bar).
Re-laxing the type of input to include alternatives thatdiffer on more than one edge, improves accuracy by1.4% (second bar from left).
Slightly greater im-provement is shown when relaxing the type of feed-494back, from binary to ternary (third bar from left).Finally, relaxing both constraints yields an improve-ment of additional 1% to an averaged accuracy of82.5% which is only 2.5% lower than the skyline.Moving to the other splits of 95, 90% we observethat relaxing the feedback from binary to ternary im-proves the accuracy more than requiring to provide apreference of parses that differ on more than a singleword.6 Noisy Light FeedbackIn the last section we discussed relaxations that re-quires slightly more effort from the annotator to gainhigher test accuracy.
The intent of the light feed-back is to build a high-accuracy parser, yet fasterand with less human effort compared with full su-pervised learning, or alternatively, allow collectingfeedbacks from non-experts.
We now evaluate theeffect of light-feedback noise, which may be a con-sequence of asking the annotator to perform quick(and rough) light-feedback.
We experiment with twosettings in which the feedback of the annotator iseither binary or ternary, in the multi settings, when99% of the training-data is used for light-feedback.These settings refer to the second and fourth bar inthe right group of the middle plate of Fig.
4.We injected independent feedback errors to a frac-tion of  of the queries, where  is ranging between0?
30%.
In the Binary-Multi setting, we flipped thebinary preference with probability .
For example, ify?A is better than y?B then with probability  the lightfeedback was the other way around.
In the Ternary-Multi setting we changed the correct feedback to oneof the other two possible feedbacks with probabil-ity , the specific alternative chosen was chosen uni-formly.
E.g., if indeed y?A is preferred over y?B , thenwith probability /2 the feedback was that y?B is pre-ferred and with probability /2 that both are equal.The accuracy vs. noise level for both settings ispresented in the right panel of Fig.
4.
The black lineshows the baseline performance after training an ini-tial parser on 1% of annotated data.
Performanceof the parser trained using the Binary-Multi settingdrops by only 1% from 81.4% to 80.4% at error rateof 5% and eventually as the feedback noise increaseto 30% the performance drops to 70% - the perfor-mance level achieved by the initial trained model.The accuracy of the parser trained in the richerTernary-Multi setting suffers only 1% performancedecrease at error rate of 10%, and eventually 5% de-crease from 82.5% to 77.5% as the feedback noiseincrease to 30%, still a 7.5% improvement over theinitial trained parser.We hypothesize that learning with ternary feed-back is more robust to noise, as in half of the noisyfeedbacks when there is a strict preference betweenthe alternatives, the effect of the noise is not toupdate the model and practically ignore the input.Clearly, this is preferable than the other outcomeof the noise, that forces the algorithm to make thewrong update with respect to the true preference.We also experimented with sentence dependednoise by training a secondary parser on a subset ofthe training set, and emulating the feedback-bit us-ing its output.
Its averaged test error (=noise level)is 22%.
Yet, the accuracy obtained by our algo-rithm with it is 77%, about the same as achievingwith 30% random annotation noise.
We hypothesizethis is since the light-feedbacks are requested specif-ically on the edges harder to predict, where the errorrate is higher than the 22% average error rate of thesecondary parser.7 Related workWeak-supervision, semi-supervised and activelearning (e.g.
(Chapelle et al, 2006), (Tong andKoller, 2001)) are general approaches related to thelight-feedback approach.
These approaches buildon access to a small set of labeled examples and alarge set of unlabeled examples.The work of Hall et al (2011) is the most simi-lar to the light feedback settings we propose.
Theyapply an automatic implicit feedback approach forimproving the performance of dependency parsers.The parser produces the k-best parse trees and anexternal system that uses these parse trees providesfeedback as a score for each of the parses.
In ourwork, we focus on minimal updates by both restrict-ing the number of compared parses to two, and hav-ing them being almost identical (up to a single edge).Hwa (1999) investigates training a phrase struc-ture parser using partially labeled data in several set-tings.
In one of the settings, a parser is first trainedusing a large fully labeled dataset from one domain495and then adapted to another domain using partial la-beling.
The parts of the data that are labeled are se-lected in one of two approaches.
In the first approachphrases are randomly selected to be annotated.
Inthe second approach the phrases are selected accord-ing to their linguistic categories based on predefinedrules.
In both cases, the true phrases are provided.
Inour work, we train the initial parser on small subsetof the data from the same domain.
Additionally, thefeedback queries are selected dynamically accordingto the edges estimated to be hardest for the parser.Finally, we request only limited feedback and thetrue parse is never provided directly.Chang et al (2007) use a set of domain specificrules as automatic implicit feedback for training in-formation extraction system.
For example, they usea set of 15 simple rules to specify the expected for-mats of fields to be extracted from advertisements.The light feedback regarding a prediction is thenumber of rules that are broken.
That feedback isused to update the prediction model.Baldridge and Osborne (2004) learns an HPSGparser using active learning to choose sentences tobe annotated from a large unlabeled pool.
Then,like our algorithm the annotator is presented with aproposed parse with several local alternatives sub-parse-trees.
Yet, the annotator manually providesthe correct parse, if it is not found within the pro-posed alternatives.
Kristjansson et al (2004) em-ploy similar approach of combining active learningwith corrective feedback for information extraction.Instances with lowest confidence using the currentmodel are chosen to be annotated.
Few alternativelabels are shown to the user, yet again, the correctlabeling is added manually if needed.
The alterna-tives shown to the user are intended to reduce theeffort of obtaining the right label, but eventually thealgorithm receives the correct prediction.
Our al-gorithm is passive about examples (and active onlyabout subset of the labels), while their algorithmuses active learning to also choose examples.
Weplan to extend our work in this direction.
Addition-ally, in these works, the feedback requests involvemany alternatives and providing the true annotation,in oppose to the limited binary or ternary feedback.Yet our results show that despite of these limitationsthe trained parser achieved performance nor far fromthe performance of a parser training using the entireannotated dataset.Finally, our setting is related to bandits (Cesa-Bianchi and Lugosi, 2006) where the feedback isextremely limited, a binary success-failure bit.8 SummaryWe showed in a series of experimental simulationsthat using light-feedback it is possible to train a de-pendency parser that achieves parsing performancenot far from standard supervised training.
Further-more, very little amount of fully annotated data,even few tens of sentences, is sufficient for build-ing an initial parser which can then be significantlyimproved using light-feedbacks.While light-feedback training and standard super-vised training with about the same number of to-tal annotated edges may achieve close performance,we still view it as a possible alternative trainingframework.
The reduction of the general annota-tion task into focused and small feedback requests,opens possibilities for receiving these feedbacks be-yond expert labeling.
In our ongoing work we studyfeedbacks from a large group of non-experts, andpossibly even automatically.
Additionally, we inves-tigate methods for selecting light-feedback queriesthat are not necessarily derived from the highestscoring parse and the best alternative parse.
For ex-ample, selecting queries that would be easy to an-swer by non-experts.496References[Baldridge and Osborne2004] J. Baldridge and M. Os-borne.
2004.
Active learning and the total cost ofannotation.
In EMNLP.
[Cesa-Bianchi and Lugosi2006] N. Cesa-Bianchi andG.
Lugosi.
2006.
Prediction, Learning, and Games.Cambridge University Press, New York, NY, USA.
[Chang et al2007] Ming-Wei Chang, Lev Ratinov, andDan Roth.
2007.
Guiding semi-supervision withconstraint-driven learning.
In In Proceedings of the45th Annual Meeting of the Association for Computa-tional Linguistics.
[Chapelle et al2006] O. Chapelle, B. Scho?lkopf, andA.
Zien, editors.
2006.
Semi-Supervised Learning.MIT Press, Cambridge, MA.
[Crammer et al2006] K. Crammer, O. Dekel, J. Keshet,S.
Shalev-Shwartz, and Y.
Singer.
2006.
Onlinepassive-aggressive algorithms.
JMLR, 7:551?585.
[Hall et al2011] Keith Hall, Ryan McDonald, JasonKatz-Brown, and Michael Ringgaard.
2011.
Trainingdependency parsers by jointly optimizing multiple ob-jectives.
In In Proceedings of the 45th Annual Meetingof the Association for Computational Linguistics.
[Hwa1999] Rebecca Hwa.
1999.
Supervised grammarinduction using training data with limited constituentinformation.
CoRR, cs.CL/9905001.
[Kristjansson et al2004] T. Kristjansson, A. Culotta,P.
Viola, and A. McCallum.
2004.
Interactive infor-mation extraction with constrained conditional randomfields.
In AAAI, pages 412?418.
[Marcus et al1993] Mitchell P. Marcus, Mary AnnMarcinkiewicz, and Beatrice Santorini.
1993.
Build-ing a large annotated corpus of english: the penn tree-bank.
Comput.
Linguist., 19:313?330, June.
[McDonald et al2005] R. McDonald, F. Pereira, K. Rib-arov, and J. Hajic.
2005.
Non-projective depen-dency parsing using spanning tree algorithms.
InHLT/EMNLP.
[Tong and Koller2001] S. Tong and D. Koller.
2001.Support vector machine active learning with applica-tions to text classification.
In JMLR, pages 999?1006.497
