Detecting Subject Boundaries Within Text: A LanguageIndependent Statistical ApproachKor in  R ichmond(kor : i .n?cogsc?.
ed.
ac.
uk)Andrew Smi th(aj s?cogsci, ed.
ac.
uk)(Joint Authorship)Centre for Cognitive Science2 Buccleuch PlaceEdinburgh EH8 9LWSCOTLANDE inat  Ami tay(e inat@cogsc i .
ed.
ac.
uk)Abst rac tWe describe here an algorithm for detect-ing subject boundaries within text basedon a statistical lexical similarity measure.Hearst has already tackled this problemwith good results (Hearst, 1994).
One ofher main assumptions i that a change insubject is accompanied by a change in vo-cabulary.
Using this assumption, but byintroducing a new measure of word signif-icance, we have been able to build a ro-bust and reliable algorithm which exhibitsimproved accuracy without sacrificing lan-guage independency.1 In t roduct ionAutomatic detection of subject divisions within atext is considered to be a very difficult task even forhumans, let alne machines.
But such subject di-visions are used in more complex tasks in text pro-cessing such as text summarisation.
An automaticmethod for marking subject boundaries i highly de-sirable.
Hearst (Hearst, 1994) addresses this prob-lem by applying a statistical method for detectingsubjects within text.Hearst describes an algorithm for what she callsText Tiling, which is a method for detecting subjectboundaries within a text.
The underlying assump-tion of this algorithm is that there is a high proba-bility that words which are related to a certain sub-ject will be repeated whenever that subject is men-tioned.
Another basic assumption is that when anew subject emerges the choice of vocabulary willchange, and will stay consistent within the subjectboundaries until the next change in subject.
Thesebasic notions of vocabulary consistency within sub-ject boundaries lead to a method for dividing textbased on calculating vocabulary similarity betweentwo adjacent windows of text.Each potential subject boundary is identified andassigned a correspondence value based on the lexicalsimilarity between two windows of text, one on ei-ther side of the subject boundary.
The values for allpotential boundaries are plotted on a graph, creatingpeaks and troughs.
The troughs represent changesin vocabulary use and therefore, according to theunderlying assumption, a change in subject.
A divi-sion mark is inserted where a significant local min-imum is detected on the graph.
Hearst measuredapproximately 80% success in detection of subjectboundaries on some texts.We decided to adopt Hearst's underlying assump-tion that a change in subject will entail a change invocabulary.
Our aim was to make the algorithm aslanguage independent and computationally expedi-ent as possible , while also improving accuracy andreliability.47I Preprocessing } STAGE I?ICalculatc  significance 1 STAGE 2value for each word JVICalculate biased lexical\] STAGE 3correspondenfes J-v -I Smooth results 1 STAGE 4?I Insert breaks \] STAGE 5Figure 1: Algorithm Structure.2 Des ignThe algorithm is divided into five distinct stages.Figure 1 shows the sequential, modular structure ofthe algorithm.
Each stage of the algorithm is de-scribed in more detail below.2.1 P reprocess ing  (stage 1)In her implementation of the TextTiling algorithmHearst ignores preprocessing, claiming it does not af-fect the results (Hearst, 1994).
By preprocessing wemean lemmatizing, stemming, converting upper tolower case etc.
Testing this assumption on her algo-rithm indeed seems not to change the results.
How-ever, using preprocessing in conjunction with stage2 of our algorithm, does improve results.
It is impor-tant for our algorithm that morphological differencesbetween semantically related words are resolved, sothat words like "bankrupt" and "bankruptcy", forexample, are identified as the same word.2.2 Ca lcu la t ing  a s ignif icance value foreach word  (stage 2)Hearst treats a text more or less as a bag of wordsin its statistical analysis.
But natural languageis no doubt more structured than this.
Differentwords have differing semantic functions and rela-tionships with respect o the topic of discourse.
Wecan broadly distinguish two extreme categories ofwords; content words versus function words.
Con-tent words introduce concepts, and are the meansfor the expression of ideas and facts, for examplenouns, proper nouns, adjectives and so on.
Functionwords (for example determiners, auxiliary verbs etc.
)support and coordinate the combination of contentwords into meaningful sentences.
Obviously, bothare needed to form meaningful sentences, but, intu-itively, it is the content words that carry most weightin defining the actual topic of discourse.
Based onthis intuition, we believe it would be advantageousto identify these content words in a text.
It wouldthen be possible to bias the calculation of lexicalcorrespondences (stage 3) taking into account thehigher significance of these words relative to func-tion words.We would ideally like firstly to reduce the effectof noisy non-content words on the algorithm's per-formance, and secondly to pay more attention towords with a high semantic ontent.
In her imple-mentation, Hearst attempts to do this by having afinite list of problematic words that are filtered outfrom the text before the statistical analysis takesplace (Hearst, 1994).
These problematic words areprimarily function words and low semantic ontentwords, such as determiners, conjunctions, preposi-tions and very common ouns.Church and Gale (Church and Gale, 1995) men-tion the correlation between a word's semanticcontent and various measures of its distributionthroughout corpora.
They show that: "Word ratesvary from genre to genre, topic to topic, authorto author, document o document, section to sec:tion, paragraph to paragraph.
These factors tendto decrease the entropy and increase the other testvariables".
One of these other test variables men-tioned by Church and Gale is burstiness.
They at-tribute the innovation of the notion of burstinessto Slava Katz, who, pertaining to this topic, writes(Katz, 1996): "The notion of burstiness.., will beused for the characterisation of two closely relatedbut distinct phenomena: (a) document-level bursti-ness, i.e.
multiple occurrence of a content word orphrase in a single text document, which is contrastedwith the fact that most other documents contain noother instances of this word or phrase at all; and (b)within-document burstiness (or burstiness proper),i.e.
close proximity of all or some individual in-stances of a content word or phrase within a doc-ument exhibiting multiple occurrence."
Katz hashighlighted many interesting features of the distri-bution of content words, which do not conform tothe predictions of statistical models uch as the Pois-son.
Katz (Katz, 1996) states that, when a conceptnamed by a content word is topical for the document,then that content word tends to be characterisedby multiple and bursty occurrence.
He claims that,while a single occurrence of a topically used content48word or phrase is possible, it is more likely that anewly introduced topical entity, will be repeated, "ifnot for breaking the monotonous effect of pronounuse, then for emphasis or clarity".
He also claimso that, unlike function words, the number of instancesEof a specific content word is not directly associatedwith the document length, but is rather a function ~of how much the document is about the concept ex- i~5 pressed by that word.zTherefore, the characteristic distribution patternof topical content words, which contrasts markedlywith that of non-topical and non-content words,could provide a useful aid in identifying the seman-tically relevant words within a text.
Brief mentionshould be made of the work done by Justeson andKatz (Justeson and Katz, 1995), which, to a certaindegree, relates to the requirements of our task.
Intheir paper, Justeson and Katz describe some lin-guistic properties of technical terminology, and usethem to formulate an algorithm to identify the tech-nical terms in a given document.
However, their al-gorithm deals with complex noun phrases only, and,although the technical terms identified by their al-gorithm are generally highly topical, the algorithmdoes not provide the context sensitive informationof how topical each incidence of a given meaning-ful phrase is, relative to its direct environment.
Itis precisely this information that is needed to judgethe content of a particular segment of text.Although Katz (Katz, 1996) acknowledges whathe calls two distinct, but closely related, formsof burstiness, he concentrates on modelling theinter-document distributions of content words andphrases.
He then uses the inter-document distri-butions to make inferences about probabilities ofthe repeat occurrences of content words and phraseswithin a single document.
Another divergence be-tween what Katz has done so far and what the taskof subject boundary insertion requires, is that hedecides to ignore the issues of coincidental repeti-tions of non-topically used content words and sim-ply equates "single occurrence with non-topical oc-currence, and multiple occurrence with topical occur-rence.
"100642i i t i t i t t i0 O.Oi 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1Propol'fi~i Freiltlerlcy of WordFigure 2: Calculation of number of nearest neigh-bours.nsignificance(x) = -1 ?
Z arctan ( Dx,in i= l  tWicd\]  (1)where x is an individual word in the document andDx,i is the distance between word x and its ith near-est neighbour.
The 1st nearest neighbour of word xis the nearest occurrence of the same word.
The 2ndnearest neighbour of x is the nearest occurrence ofthe same word ignoring the 1st nearest neighbour.
Ingeneral, the ith nearest neighbour of x is the near-est occurrence of the same word ignoring the 1st,2nd, 3rd, .
.
.
, ( i -  1)th nearest neighbours.
W is thetotal number of words in the text.
w is the numberof occurrences of the word like x. n is the numberof nearest neighbours to include in the calculationand depends on the overall frequency of the word inthe text.
This formula will yield a significance scorethat lies within the range 0 to ~ (high significanceto low significance).
This number is then normalisedto between 0 and 1, with 0 indicating a very low sig-nificance, and 1 indicating a very high significance.The exact value of n is calculated separately for eachdistinct word, using the following formula:( 8 )n-- - -  1 + e-2?~-(~ -?
?2) + 2 (2)We have implemented a method which assigns anestimated significance score based on a measure oftwo context dependent properties; local burstinessand global frequency.
The heart of our solutionto the problem of assigning context-based values oftopical significance to all words in a text, can besummed up in the following formula:This is essentially a sigmoid function with therange varying between two and ten, as shown in Fig-ure 2.
The constants scale and translate the functionto yield the desired behaviour, which was derivedempirically.
The number of nearest neighbours toconsider in equation 1 increases with the word's fre-quency.
For example, when calculating the signif-490.9i0.80.70.60.50.40.30.20.IQ O2he  ?
?CII0 Oo & oo2?
?o t,$ ?
?i i i i i i~00 400 600 800 1000 1200 1400Word Posi~on in TextFigure 3: Significance Values.icance of the least frequent words, only two near-est neighbours are considered.
But for the mostfrequently occurring words, the number of nearestneighbours i  ten.
Figure 3 shows the main featuresof the performance of this significance assignment al-gorithm when tested on a sample text.
The resultsfor three very different words are shown.Two general trends are the most important fea-tures of this graph.
Firstly, elevated significancescores are associated with local clusters of a word.For example the cluster of three occurrences of "soft-ware" (a content word) at the end of the documenthave high significance scores.
This contrasts withthe relatively isolated occurrences of the word "soft-ware" in the middle of the document, which aredeemed to be little more significant han several oc-currences of the word "the" (a function word).
Sec-ondly, frequent words tend to receive lower signifi-cance scores.
For example, even local clusters of theword "the" only receive relatively low significancescores, simply because the word has a high frequencythroughout the document.
Conversely, "McNealy"(a high semantic ontent word), which only occurs ina cluster of three, receives a high significance value.The important result shown by the graph is thatcontent words (real names such as "McNealy") re-ceive higher significance values than function words("the").We found that an optimal solution to the problemof balancing local density against global frequencywas rather elusive.
For example, the words at thecentre of a cluster automatically receive a higherscore, whereas it may be more desirable to have allthe members of a cluster assigned a score lying in anarrower ange.
There are many other contentiousissues which need to be investigated, such as the useof the ratio of all the occurrences ofa word in a giventext to the total length of that text in order to calcu-late the relative significance measure.
Based on in-tuition, partly derived from Katz's discussion (Katz,1996) of the relationship between document lengthand word frequency, the exact nature of this rela-tionship across various document lengths may notbe reliable enough.
It may be more consistent oconsider this ratio within a constant window size,e.g.
1000 words.The advantage of this simple statistical methodof distinguishing significant content words from non-content words is that no words need to be removedbefore allowing the algorithm to proceed.
The out-put of this stage is a normalised significance score(0-1) for each word in the text.
This significancescore can then be taken into account when analysingthe text for subject boundaries.2.3 Ca lcu late  B iased LexicalCor respondences  (stage 3)Let us consider two sets of words, set A and setB.
The main aim of this stage of the processing isconcerned with calculating a correspondence mea-sure between two such sets depending on how similarthey are, where similarity is defined as a measure offlexical correspondence.
If many words are shared byboth set A and B, then the lexical correspondencebetween the two sets is high.
If the two sets do notshare many words, then the correspondence is low.Now let A t be the subset of A that contains onlythose words that occur somewhere in B.
And let B'be the subset of B that contains only those wordsthat occur somewhere in A.
The lexical correspon-dence between sets A and B can then be calculatedusing the simple formula:Correspondence- I~ + ~L 2This yields a value within the range 0 to 1.
IAI canbe re-written as 1+1+1+1+1 .... by adding a 1 forevery word in A.
Each word has already been givena significance value as described in stage 2 of thealgorithm and this information is taken into accountby re-defining IAI as sl+s2+s3+..,  where sl is thesignificance value assigned to the first word in A, s2the second and so on.
The same can be done for A ',B and B ~.
The formula now takes the average ofthe biased ratios.
All this means is that instead ofeach word counting for '1' in a set, it counts for itssignificance value (a value between 0 (insignificant)and 1 (highly significant)).
The result is that eachword affects the correspondence measure accordingto its significance in the text.50Set A Set BFigure 4: Word Sets.So far, a word that occurs only in A and not in B,contributes zero to JAn\[.
This means that a highlysignificant word occurring only in A has exactly thesame effect as an insignificant word occurring onlyin A.
In other words the significance biasing is onlytaking place for words that appear in both A and B.Therefore, the formula actually used is:Correspondence= L~I ~-~ "k I-~P-~I 2where A" is the subset of A which contains onlythose words that occur in A and not in B. Sim-ilarly, B ~ is the subset of B which contains onlythose words that occur in B and not in A.
This isshown in Figure 4.Recall that \[A\[, \[m'\[, \[A"\[, \[B\], \[B'\] and IS"\[ arenot calculated by adding one for each word in eachset, but by summing the significance values of thewords in each set.This stage of the processing looks at the outputfrom the significance calculation stage and considersevery sentence break in turn - starting at the topof the document and working down.
The algorithmassigns a correspondence measure to each sentencebreak as follows: Firstly, set A is generated by tak-ing all the words in the previous fifteen sentences.Next, set B is generated by taking all the wordsin the following fifteen sentences.
1 Now sets A p,A ~, B ~ and B ~ are generated as described and thenthe formula above is applied which assigns a cor-respondence value to the sentence break currentlyunder consideration.
The algorithm then moves tothe next sentence break and repeats the process.The output from this stage of the algorithm is alist of sentence break numbers (1..n, with n = num-ber of sentences in the document) and a lexical cor-respondence measure.
These numbers provide theinput for stage four - smoothing.1Fifteen sentences  turns  out  to be the  opt imum win-dow size for the  vast major i ty  of texts.
Th is  is becauseit is about  the  same as the average segment  size.2.4 Smooth ing  (stage 4)A graph can be plotted with lexical correspondencealong the y-axis and sentence number along the x-axis.
In order to distinguish the significant peaksand troughs from the many minor fluctuations, asimple smoothing algorithm is used.
Taking threeneighbouring points on the graph, P1, P2, P3:P3o A...~......~:~..................................................................... ......... i iUi:::~ XFigure 5: Smoothing.The line P1P3 is bisected and this point is labelledA.
P2 is perturbed by a constant amount (not dee-pendent on the distance between A and P2) towardsA.
This new point is labelled B and becomes thenew P2.
This is performed simultaneously on ev-ery point on the graph.
The process is k then iterateda fixed number of times.
The result is that noiseis flattened out while the larger peaks and troughsremain (although slightly smaller).The output from this stage is simply the sentencebreak numbers and their new, smoothed correspon-dence values.2.5 Inser t ing  sub jec t  boundar ies  (stage 5)Considering the graph described in the previous ec-tion, generating subject boundaries i simply a mat-ter of identifying local minima on the graph.
Theconfidence of a boundary is calculated from the'depth' of the local minimum.
This depth is calcu-lated simply by taking the average of the heights ofthe 'peak' (relative to the height of the minimum) oneither side of the minimum.
This now yields a listof candidate subject boundaries and an associatedconfidence measure for each one.
Breaks are then in-serted into the original text at the places correspond-ing to the local minima if their confidence value sat-isfies a 'minimum confidence' criterion.
This cut-offcriterion is arbitrary, and in our implementation canbe specified at run time.3 Resu l tsFigure 6 shows the result of processing the first800 sentences from an edition of The Times newspa-per.
The sentence number (x-axis) is plotted againstthe correspondence (y-axis) between the two win-dows of text on either side of that sentence.51.
i100-10-20-30-40-50-50-70-aO-gO0100 ?-10-20-30-40-60-50-70-50"g~O0-10-20-30-40-50-50-70-50-90400-4O-50-90500|5O|1 O0SentenceI | I25O 30q 35O~entence|460i l600  550Sentence| i |550 700 750SentenceFigure 6:800 Sentences from The Times newspaper.52Actual  subjectboundaries3661779109146165175203244278304333356376Boundaries found Errorby algorithm36 060 179 0109 0134 +145 1165 0174 1203 0214 +244 0278 0304 0332 1355 1375 1Table 1: The TimesA large negative value indicates a low degree ofcorrespondence and a small negative value or a pos-itive value indicates a high degree of correspondence.The vertical ines mark actual article boundaries.The advantage of using a text such as this is thatthere can be no doubt from any human judge as towhere the boundaries occur, i.e.
between articles.The local minima on the graph signify the bound-aries as determined by the algorithm.
The verticalbars signify the actual article boundaries.
The re-sults of the first 400 sentences are summarised intable 1.The algorithm located 53% of the article bound-aries precisely and 95% of the boundaries to withinan accuracy of a single sentence.
Every articleboundary was identified to within an accuracy oftwo sentences.
The algorithm made no use of end-of-paragraph markers.
It also found some additionalsubject boundaries in the middle of articles.
Theseare denoted by a '+ '  in the error column.
Many ex-tra subject boundaries were found in the long article(starting at sentence 430).
It is worth noting thatthe minima occurring within this article are not aspronounced as the actual article boundaries them-selves.
This section of the graph reflects a long arti-cle which contains a number of different subtopics.A newspaper is an easy test for such an algorithmthough.
Figure 7 shows a graph for an expositorytext - a 200 sentence psychology paper written bya fellow student.
Again the local minima indicatewhere the algorithm considers a subject boundaryto occur and the vertical ines are the obvious breaksin the text (mainly before new headings) as judgedby the author.
The results are summarised in table2.This time the algorithm precisely located 50% ofthe boundaries.
It found 63% of the boundaries towithin an accuracy of a single sentence and 88% toActual subjectboundaries722597296121162184Boundaries found Errorby algorithm77 022 042 +58 1772 077 +92 4118 3137 +156 +161 1177 +184 0191 +Table 2: Expository Textwithin an accuracy of two sentences.
This level ofaccuracy was obtained consistently for a variety ofdifferent exts.
Again, it should be mentioned thatthe algorithm found more breaks than were immedi-ately obvious to a human judge.
However, it shouldbe noted that these extra breaks were usually de-noted by smaller minima, and on inspection the vastmajority of them were in sensible places.The algorithm has a certain resolving power.
Asthe subject matter becomes more and more homoge-neous, the number of subject breaks the algorithmfinds decreases.
For some texts, this results in veryfew divisions being made.
By taking a smaller win-dow size (the number of sentences to look at eitherside of each possible sentence break), the resolvingpower 'of the algorithm can be increased making itmore sensitive to changes in the vocabulary.
How-ever, the reliability of the algorithm decreases withthe increased resolving power.
The default windowsize is fifteen sentences and this works well for allbut the most homogeneous of texts.
In this case awindow size of around six is more effective.
A lowerwindow size increases the resolving power, but de-creases the accuracy of the algorithm.
The windowsize was a parameter of our implementation.o-1o-3o-70~o 1?o  1soFigure 7: Expository Text.534 SummaryBased on our investigation, we believe that Hearst'soriginal intuition that lexical correspondences an beexploited to identify subject boundaries i a soundone.
The addition of the significance measure repre-sents an improvement on Hearst's algorithm imple-mented by the Berkeley Digital Library Project.Furthermore, this algorithm is language indepen-dent except for the preprocessing stage (which canbe omitted with only a modest degradation i per-formance).
In order to improve accuracy, languagedependent methods could be considered.
Such meth-ods might include the insertion of conventional dis-course markers in order to detect preferred breakingpoints (e.g.
repetition of the same syntactic struc-ture, and conventional paragraph openings uch as:"On the other hand...", "The above...", etc.).
An-other method would be to make use of a thesaurus,since we have found that human judgement is oftenbased on synonymous information such as real syn-onyms or anaphora.
The above issues are discussedin various articles (Morris and Hirst, 1991); (Mor-ris, 1988) and (Givon, 1983) which study discoursemarkers and synonymous information.Another interesting line of research would be touse the information from stage two of the algorithmto discover the significant words of a section, andthereby attach a label to it.
This would be particu-larly useful for information retrieval applications.5 AcknowledgementsThis problem was set as an assignment on the DataIntensive Linguistics course organised by Chris Brewat the HCRC, Edinburgh University.
Thanks to him,Jo Calder and Marc Moens for guidance and advicethroughout the project.
Thanks to ESRC and EP-SRC for funding.Katz, S. M. 1996.
Distribution of context words andphrases in text and language modelling.
NaturalLanguage Engineering, 2(1):15-59.Morris, J.
1988.
Lexical cohesion, the thesaurus,and the structure of text.
Technical Report CSRI-219, Computer Systems Research Institute, Uni-versity of Toronto.Morris, J. and G. Hirst.
1991.
Lexical cohesioncomputed by thesaural relations as an indicatorof the structure of text.
Computational Linguis-tics, 17(1):21-48.ReferencesChurch, K. W. and W. A. Gale.
1995.
Poisson mix-tures.
Natural Language Engineering, 1(2):163-190.Givon, T. 1983.
Topic Continuity in Discourse:A Quantitative Cross-Language Study.
Philadel-phia: John Benjamins Publishing Company.Hearst, M. A.
1994.
Multi-paragraph segmentationof expository text.
In ACL '94, Las Cruces, NM.Justeson, J. S. and S. M. Katz.
1995.
Technical ter-minology: some linguistic properties and an algo-rithm for identification i text.
Natural LanguageEngineering, 1(1) :9-27.54
