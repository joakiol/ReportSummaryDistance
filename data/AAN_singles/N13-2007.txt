Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 47?53,Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational LinguisticsReversing Morphological Tokenization in English-to-Arabic SMTMohammad Salameh?
Colin Cherry?
Grzegorz Kondrak?
?Department of Computing Science ?National Research Council CanadaUniversity of Alberta 1200 Montreal RoadEdmonton, AB, T6G 2E8, Canada Ottawa, ON, K1A 0R6, Canada{msalameh,gkondrak}@ualberta.ca Colin.Cherry@nrc-cnrc.gc.caAbstractMorphological tokenization has been usedin machine translation for morphologicallycomplex languages to reduce lexical sparsity.Unfortunately, when translating into a mor-phologically complex language, recombiningsegmented tokens to generate original wordforms is not a trivial task, due to morpho-logical, phonological and orthographic adjust-ments that occur during tokenization.
We re-view a number of detokenization schemes forArabic, such as rule-based and table-based ap-proaches and show their limitations.
We thenpropose a novel detokenization scheme thatuses a character-level discriminative stringtransducer to predict the original form of asegmented word.
In a comparison to a state-of-the-art approach, we demonstrate slightlybetter detokenization error rates, without theneed for any hand-crafted rules.
We alsodemonstrate the effectiveness of our approachin an English-to-Arabic translation task.1 IntroductionStatistical machine translation (SMT) relies on to-kenization to split sentences into meaningful unitsfor easy processing.
For morphologically complexlanguages, such as Arabic or Turkish, this may in-volve splitting words into morphemes.
Through-out this paper, we adopt the definition of tokeniza-tion proposed by Habash (2010), which incorpo-rates both morphological segmentation as well asorthographic character transformations.
To use anEnglish example, the word tries would be morpho-logically tokenized as ?try + s?, which involvesorthographic changes at morpheme boundaries tomatch the lexical form of each token.
When trans-lating into a tokenized language, the tokenizationmust be reversed to make the generated text read-able and evaluable.
Detokenization is the processof converting tokenized words into their original or-thographically and morphologically correct surfaceform.
This includes concatenating tokens into com-plete words and reversing any character transforma-tions that may have taken place.For languages like Arabic, tokenization can facil-itate SMT by reducing lexical sparsity.
Figure 1shows how the morphological tokenization of theArabic word ???
J?J??
?and he will prevent them?simplifies the correspondence between Arabic andEnglish tokens, which in turn can improve the qual-ity of word alignment, rule extraction and decoding.When translating from Arabic into English, the to-kenization is a form of preprocessing, and the out-put translation is readable, space-separated English.However, when translating from English to Arabic,the output will be in a tokenized form, which cannotbe compared to the original reference without detok-enization.
Simply concatenating the tokenized mor-phemes cannot fully reverse this process, because ofcharacter transformations that occurred during tok-enization.The techniques that have been proposed for thedetokenization task fall into three categories (Badret al 2008).
The simplest detokenization approachconcatenates morphemes based on token markerswithout any adjustment.
Table-based detokenizationmaps tokenized words into their surface form with alook-up table built by observing the tokenizer?s in-47Figure 1: Alignment between tokenized form of?wsymn?hm?
???
J?J??
and its English translation.put and output on large amounts of text.
Rule-baseddetokenization relies on hand-built rules or regularexpressions to convert the segmented form into theoriginal surface form.
Other techniques use combi-nations of these approaches.
Each approach has itslimitations: rule-based approaches are language spe-cific and brittle, while table-based approaches fail todeal with sequences outside of their tables.We present a new detokenization approach thatapplies a discriminative sequence model to predictthe original form of the tokenized word.
Liketable-based approaches, our sequence model re-quires large amounts of tokenizer input-output pairs;but instead of building a table, we use these pairsas training data.
By using features that considerlarge windows of within-word input context, we areable to intelligently transition between rule-like andtable-like behavior.Our experimental results on Arabic text demon-strate an improvement in terms of sentence errorrate1 of 11.9 points over a rule-based approach, and1.1 points over a table-based approach that backsoff to rules.
More importantly, we achieve a slightimprovement over the state-of-the-art approach ofEl Kholy and Habash (2012), which combines rulesand tables, using a 5-gram language model to dis-ambiguate conflicting table entries.
In addition, ourdetokenization method results in a small BLEU im-provement over a rule-based approach when appliedto English-to-Arabic SMT.1Sentence error rate is the percentage of sentences contain-ing at least one error after detokenization.2 Arabic MorphologyCompared to English, Arabic has rich and complexmorphology.
Arabic base words inflect to eight fea-tures.
Verbs inflect for aspect, mood, person andvoice.
Nouns and adjectives inflect for case andstate.
Verbs, nouns and adjectives inflect for bothgender and number.
Furthermore, inflected basewords can attract various optional clitics.
Cliticalprefixes include determiners, particle proclitics, con-junctions and question particles in strict order.
Clit-ical suffixes include pronominal modifiers.
As a re-sult of clitic attachment, morpho-syntactic interac-tions sometimes cause changes in spelling or pro-nunciations.Several tokenization schemes can be defined forArabic, depending on the clitical level that the to-kenization is applied to.
In this paper, we usePenn Arabic Treebank (PATB) tokenization scheme,which El Kholy and Habash (2012) report as pro-ducing the best results for Arabic SMT.
The PATBscheme detaches all clitics except for the definite ar-ticle Al ?@.
Multiple prefix clitics are treated as onetoken.Some Arabic letters present further ambiguity intext.2 For example, the different forms of HamzatedAlif ?
@ @?
are usually written without the Hamza ?Z?.Likewise, when the letter Ya ?Y?
?is present at theend of the word, it is sometimes written in the formof ?Alif Maqsura?
letter ???
?.
Also, short vow-els in Arabic are represented using diacritics, whichare usually absent in written text.
In order to dealwith these ambiguities in SMT, normalization is of-ten performed as a preprocessing step, which usu-ally involves converting different forms of Alif andYa to a single form.
This decreases Arabic?s lexicalsparsity and improves SMT performance.3 Related WorkSadat and Habash (2006) address the issue of lex-ical sparsity by presenting different preprocessingschemes for Arabic-to-English SMT.
The schemesinclude simple tokenization, orthographic normal-ization, and decliticization.
The combination ofthese schemes results in improved translation out-2We use Habash-Soudi-Buckwalter transliteration scheme(Habash, 2007) for all Arabic examples.48put.
This is one of many studies on normalizationand tokenization for translation from Arabic, whichwe will not attempt to review completely here.Badr et al(2008) show that tokenizing Arabicalso has a positive influence on English-to-ArabicSMT.
They apply two tokenization schemes onArabic text, and introduce detokenization schemesthrough a rule-based approach, a table-based ap-proach, and a combination of both.
The combina-tion approach detokenizes words first using the ta-ble, falling back on rules for sequences not found inthe table.El Kholy and Habash (2012) extend Badr?s workby presenting a larger number of tokenization anddetokenization schemes, and comparing their effectson SMT.
They introduce an additional detokeniza-tion schemes based on the SRILM disambig util-ity (Stolcke, 2002), which utilizes a 5-gram untok-enized language model to decide among different al-ternatives found in the table.
They test their schemeson naturally occurring Arabic text and SMT output.Their newly introduced detokenization scheme out-performs the rule-based and table-based approachesintroduced by Badr et al(2008), establishing thecurrent state-of-the-art.3.1 Detokenization Schemes in DetailRule-based detokenization involves manually defin-ing a set of transformation rules to convert a se-quence of segmented tokens into their surface form.For example, the noun ?llry?ys?
?KQ??
?to the pres-ident?
is tokenized as ?l+ Alry?ys?
( l+ ?to?
Alry?ys?the president?)
in the PATB tokenization scheme.Note that the definite article ?Al?
?
@ is kept attachedto the noun.
In this case, detokenization requiresa character-level transformation after concatenation,which we can generalize using the rule:l+Al ?
ll.Table 1 shows the rules provided by El Kholy andHabash (2012), which we employ throughout thispaper.There are two principal problems with the rule-based approach.
First, rules fail to account for un-usual cases.
For example, the above rule mishandlescases where ?Al?
?
@ is a basic part of the stem andnot the definite article ?the?.
Thus, ?l+ Al?Ab?
(l+?to?
Al?Ab ?games?)
is erroneously detokenized toRule Input Outputl+Al+l?
?
ll l+ Alry?ys llry?ys?+(pron) ?
t(pron) Abn?+hA AbnthAy+(pron) ?
A(pron) Alqy+h AlqAh?+(pron) ?
y?
AntmA?+hm AntmAy?hmy+y ?
y ?yny+y ?ynyn+n ?
n mn+nA mnAmn+m ?
mm mn+mA mmA?n+m ?
?m ?n+mA ?mAAn+lA ?
AlA An+lA AlATable 1: Detokenization rules of El Kholy and Habash(2012), with examples.
pron stands for pronominal clitic.llEAb H.
A???
instead of the correct form is ?lAl?Ab?H.
A??B.
Second, rules may fail to handle sequencesproduced by tokenization errors.
For example, theword ?bslT??
????
.
?with power?
can be erro-neously tokenized as ?b+slT+h?, while the correcttokenizations is ?b+slT??.
The erroneous tokeniza-tion will be incorrectly detokenized as ?bslTh?.The table-based approach memorizes mappingsbetween words and their tokenized form.
Such atable is easily constructed by running the tokenizeron a large amount of Arabic text, and observing theinput and output.
The detokenization process con-sults this table to retrieve surface forms of tokenizedwords.
In the case where a tokenized word has sev-eral observed surface forms, the most frequent formis selected.
This approach fails when the sequenceof tokenized words is not in the table.
In morpholog-ically complex languages like Arabic, an inflectedbase word can attrract many optional clitics, and ta-bles may not include all different forms and inflec-tions of a word.The SRILM-disambig scheme introduced byEl Kholy and Habash (2012) extends the table-basedapproach to use an untokenized Arabic languagemodel to disambiguate among the different alter-natives.
Hence, this scheme can make context-dependent detokenization decisions, rather than al-ways producing the most frequent surface form.Both the SRILM-disambig scheme and the table-based scheme have the option to fall back on eitherrules or simple concatenation for sequences missingfrom the table.494 Detokenization as String TransductionWe propose to approach detokenization as a stringtransduction task.
We train a discriminative trans-ducer on a set of tokenized-detokenized word pairs.The set of pairs is initially aligned on the charac-ter level, and the alignment pairs become the opera-tions that are applied during transduction.
For deto-kenization, most operations simply copy over char-acters, but more complex rules such as l+ Al ?
llare learned from the training data as well.The tool that we use to perform the transduction isDIRECTL+, a discriminative, character-level stringtransducer, which was originally designed for letter-to-phoneme conversion (Jiampojamarn et al 2008).To align the characters in each training example.DIRECTL+ uses an EM-based M2M-ALIGNER (Ji-ampojamarn et al 2007).
After alignment is com-plete, MIRA training repeatedly decodes the train-ing set to tune the features that determine when eachoperation should be applied.
The features includeboth n-gram source context and HMM-style targettransitions.
DIRECTL+ employs a fully discrimina-tive decoder to learn character transformations andwhen they should be applied.
The decoder resem-bles a monotone phrase-based SMT decoder, but isbuilt to allow for hundreds of thousands of features.The following example illustrates how stringtransduction applies to detokenization.
The seg-mented and surface forms of bbrA?thm ??
D?
@Q.
K.?with their skill?
constitute a training instance:b+_brA?
?_+hm ?
bbrA?thmThe instance is aligned during the training phase as:b+ _b r A ?
?_ + h m| | | | | | | | |b b r A ?
t  h mThe underscore ?_?
indicates a space, while ??
de-notes an empty string.
The following operations areextracted from the alignment:b+ ?
b, _b ?
b, r ?
r, A ?
A, E ?
E, p_ ?
t,+ ?
, h ?
h, m ?
mDuring training, weights are assigned to features thatassociate operations with context.
In our running ex-ample, the weight assigned to the b+ ?
b operationaccounts for the operation itself, for the fact that theoperation appears at the beginning of a word, and forthe fact that it is followed by an underscore; in fact,we employ a context window of 5 characters to theleft or right of the source substring ?b+?, creating afeature for each n-gram within that window.Modeling the tokenization problem as stringtransduction has several advantages.
The approachis completely language-independent.
The context-sensitive rules are learned automatically from ex-amples, without human intervention.
The rulesand features can be represented in a more com-pact way than the full mapping table required bytable-based approaches, while still elegantly han-dling words that were not seen during training.Also, since the training data is generalized moreefficiently than in simple memorization of com-plete tokenized-detokenized pairs, less training datashould be needed to achieve good accuracy.5 ExperimentsThis section presents two experiments that evaluatethe effect of the detokenization schemes on both nat-urally occurring Arabic and SMT output.5.1 DataTo build our data-driven detokenizers, we use theArabic part of 4 Arabic-English parallel datasetsfrom the Linguistic Data Consortium as train-ing data.
The data sets are: Arabic News(LDC2004T17), eTIRR (LDC2004E72), Englishtranslation of Arabic Treebank (LDC2005E46), andUmmah (LDC2004T18).
The training data has107K sentences.
The Arabic part of the training dataconstitutes around 2.8 million words, 3.3 million to-kens after tokenization, and 122K word types afterfiltering punctuation marks, Latin words and num-bers (refer to Table 2 for detailed counts).For training the SMT system?s translation and re-ordering models, we use the same 4 datasets fromLDC.
We also use 200 Million words from LDCArabic Gigaword corpus (LDC2011T11) to gener-ate a 5-gram language model using SRILM toolkit(Stolcke, 2002).We use NIST MT 2004 evaluation set for tun-ing (1075 sentences), and NIST MT 2005 evalua-tions set for testing (1056 sentences).
Both MT04and MT05 have multiple English references in or-der to evaluate Arabic-to-English translation.
As weare translating into Arabic, we take the first English50Data set Before Aftertraining set 122,720 61,943MT04 8,201 2,542MT05 7,719 2,429Table 2: Type counts before and after tokenization.translation to be our source in each case.
We alsouse the Arabic halves of MT04 and MT05 as devel-opment and test sets for our experiments on natu-rally occurring Arabic.
The tokenized Arabic is ourinput, with the original Arabic as our gold-standarddetokenization.The Arabic text of the training, development, test-ing set and language model are all tokenized usingMADA 3.2 (Habash et al 2009) with the Penn Ara-bic Treebank tokenization scheme.
The English textin the parallel corpus is lower-cased and tokenizedin the traditional sense to strip punctuation marks.5.2 Experimental SetupTo train the detokenization systems, we generate atable of mappings from tokenized forms to surfaceforms based on the Arabic part of our 4 paralleldatasets, giving us complete coverage of the out-put vocabulary of our SMT system.
In the table-based approaches, if a tokenized form is mapped tomore than one surface form, we use the most fre-quent surface form.
For out-of-table words, we fallback on concatenation (in T) or rules (in T+R).
ForSRILM-Disambig detokenization, we maintain am-biguous table entries along with their frequencies,and we introduce a 5-gram language model to dis-ambiguate detokenization choices in context.
Likethe table-based approaches, the Disambig approachcan back off to either simple concatenation (T+LM)or rules (T+R+LM) for missing entries.
The latteris a re-implementation of the state-of-the-art systempresented by El Kholy and Habash (2012).We train our discriminative string transducer us-ing word types from the 4 LDC catalogs.
Weuse M2M-ALIGNER to generate a 2-to-1 charac-ter alignments between tokenized forms and surfaceforms.
For the decoder, we set Markov order to one,joint n-gram features to 5, n-gram size to 11, andcontext size to 5.
This means the decoder can uti-lize contexts up to 11 characters long, allowing it toDetokenization WER SER BLEUBaseline 1.710 34.3 26.30Rules (R) 0.590 14.0 28.32Table (T) 0.192 4.9 28.54Table + Rules (T+R) 0.122 3.2 28.55Disambig (T+LM) 0.164 4.1 28.53Disambig (T+R+LM) 0.094 2.4 28.54DIRECTL+ 0.087 2.1 28.55Table 3: Word and sentence error rate of detokenizationschemes on the Arabic reference text of NIST MT05.BLEU score refers to English-Arabic SMT output.effectively memorize many words.
We found thesesettings using grid search on the development set,NIST MT04.For the SMT experiment, we use GIZA++ forthe alignment between English and tokenized Ara-bic, and perform the translation using Moses phrase-based SMT system (Hoang et al 2007), with a max-imum phrase length of 5.
We apply each detokeniza-tion scheme on the SMT tokenized Arabic outputtest set, and evaluate using the BLEU score (Pap-ineni et al 2002).5.3 ResultsTable 3 shows the performance of several detok-enization schemes.
For evaluation, we use the sen-tence and word error rates on naturally occurringArabic text, and BLEU score on tokenized Arabicoutput of the SMT system.
The baseline scheme,which is a simple concatenation of morphemes, in-troduces errors in over a third of all sentences.
Thetable-based approach outperforms the rule-based ap-proach, indicating that there are frequent excep-tions to the rules in Table 1 that require memoriza-tion.
Their combination (T+R) fares better, lever-aging the strengths of both approaches.
The addi-tion of SRILM-Disambig produces further improve-ments as it uses a language model context to disam-biguate the correct detokenized word form.
Our sys-tem outperforms SRILM-Disambig by a very slightmargin, indicating that the two systems are roughlyequal.
This is interesting, as it is able to do so byusing only features derived from the tokenized worditself; unlike SRILM-Disambig, it has no access tothe surrounding words to inform its decisions.
In ad-51dition, it is able to achieve this level of performancewithout any manually constructed rules.Improvements in detokenization do contribute tothe BLEU score of our SMT system, but only toa point.
Table 3 shows three tiers of performance,with no detokenization being the worst, the rules be-ing better, and the various data-driven approachesperforming best.
After WER dips below 0.2, furtherimprovements seem to no longer affect SMT quality.Note that BLEU scores are much lower overall thanone would expect for the translation in the reversedirection, because of the morphological complexityof Arabic, and the use of one (as opposed to four)references for evaluation.5.4 AnalysisThe sentence error rate of 2.1 represents only 21errors that our approach makes.
Among those 21,11 errors are caused by changing p to h and viceversa.
This is due to writing p and h interchange-ably.
For example, ?AjmAly+h?
was detokenizedas ?AjmAly??
?J?A?g.
@ instead of ?AjmAlyh?
?J?A?g.
@.Another 4 errors are caused by the lack of dia-critization, which affects the choice of the Hamzaform.
For example,?bnAw?h?
?
?A 	JK.
, ?bnAy?h?
?KA 	JK.and ?bnA?h?
?ZA 	JK.
(?its building?)
are 3 differentforms of the same word where the choice of HamzaZ is dependent on its diacritical mark or the markof the character that precedes it.
Another 3 errorsare attributed to the case of the nominal which it in-flects for.
The case is affected by the context of thenoun which DIRECTL+ has no access to.
For ex-ample, ?mfkry+hm?
(?thinkers/Dual-Accusative?
)was detokenized as ?mfkrAhm?
??@Q?
??
(Dual-Nominative) instead of ?mfkryhm?
??EQ?
??.
Thelast 3 errors are special cases of ?An +y?
whichcan be detokenized correctly as either ?Any?
?G @ or?Anny?
??
K @.The table-based detokenization scheme fails in54 cases.
Among these instances, 44 cases are notin the mapping table, hence resolving back to sim-ple concatenation ended with an error.
Our trans-duction approach succeeds in detokenizing 42 casesout of the 54.
The majority of these cases involveschanging p to h and vice versa, and changing l+Alto ll.
The only 2 instances where the tokenizedword is in the mapping table but DIRECTL+ incor-rectly detokenizes it are due to hamza case and pto h case described above.
There are 4 instancesof the same word/case where both the table schemeand DIRECTL+ fails due to error of tokenizationby MADA, where the proper name qwh ???
is er-roneously tokenized as qw+p.
This shows that DI-RECTL+ handles the OOV words correctly.The Disambig(T+R+LM) erroneously detok-enizes 27 instances, where 21 out of them are cor-rectly tokenized by DIRECTL+.
Most of the er-rors are due to the Hamza and p to h reasons.
Itseems that even with a large size language model,the SRILM utility needs a large mapping table toperform well.
Only 4 instances were erroneouslydetokenized by both Disambig and DIRECTL+ dueto Hamza and the case of the nominal.The analysis shows that using small size trainingdata, DIRECTL+ can achieve slightly better accu-racy than SRILM scheme.
The limitations of usingtable and rules are handled with DIRECTL+ as it isable to memorize more rules.6 Conclusion and Future WorkIn this paper, we addressed the detokenization prob-lem for Arabic using DIRECTL+, a discriminativetraining model for string transduction.
Our systemperforms the best among the available systems.
Itmanages to solve problems caused by limitations oftable-based and rule-based systems.
This allows usto match the performance of the SRILM-disambigapproach without using a language model or hand-crafted rules.
In the future, we plan to test our ap-proach on other languages that have morphologicalcharacteristics similar to Arabic.ReferencesIbrahim Badr, Rabih Zbib, and James Glass.
2008.
Seg-mentation for English-to-Arabic statistical machinetranslation.
In Proceedings of ACL, pages 153?156.Ahmed El Kholy and Nizar Habash.
2012.
Orthographicand morphological processing for English-Arabic sta-tistical machine translation.
Machine Translation,26(1-2):25?45, March.Nizar Habash, Owen Rambow, and Ryan Roth.
2009.Mada+tokan: A toolkit for Arabic tokenization, dia-critization, morphological disambiguation, POS tag-ging, stemming and lemmatization.
In Proceedings of52the Second International Conference on Arabic Lan-guage Resources and Tools.Nizar Habash.
2007.
Arabic morphological represen-tations for machine translation.
In Arabic Computa-tional Morphology: Knowledge-based and EmpiricalMethods.Nizar Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Synthesis Lectures on HumanLanguage Technologies.
Morgan & Claypool Publish-ers.Hieu Hoang, Alexandra Birch, Chris Callison-burch,Richard Zens, Rwth Aachen, Alexandra Constantin,Marcello Federico, Nicola Bertoldi, Chris Dyer,Brooke Cowan, Wade Shen, Christine Moran, and On-drej Bojar.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Annual Meeting ofthe Association for Computational Linguistics (ACL),demonstration session, pages 177?180.Sittichai Jiampojamarn, Grzegorz Kondrak, and TarekSherif.
2007.
Applying many-to-many alignmentsand HMMs to letter-to-phoneme conversion.
In Pro-ceedings of NAACL-HLT, pages 372?379.Sittichai Jiampojamarn, Colin Cherry, and GrzegorzKondrak.
2008.
Joint processing and discriminativetraining for letter-to-phoneme conversion.
In Proceed-ings of ACL-08: HLT, pages 905?913.Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318.Fatiha Sadat and Nizar Habash.
2006.
Combination ofArabic preprocessing schemes for statistical machinetranslation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the 44thannual meeting of the Association for ComputationalLinguistics, pages 1?8.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Intl.
Conf.
Spoken Language Pro-cessing, pages 901?904.53
