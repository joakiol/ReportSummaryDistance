Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 1?9,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsMulti-modal Sensing and Analysis of Poster Conversationstoward Smart PosterboardTatsuya KawaharaKyoto University, Academic Center for Computing and Media StudiesSakyo-ku, Kyoto 606-8501, Japanhttp://www.ar.media.kyoto-u.ac.jp/crest/AbstractConversations in poster sessions in academicevents, referred to as poster conversations,pose interesting and challenging topics onmulti-modal analysis of multi-party dialogue.This article gives an overview of our projecton multi-modal sensing, analysis and ?under-standing?
of poster conversations.
We fo-cus on the audience?s feedback behaviors suchas non-lexical backchannels (reactive tokens)and noddings as well as joint eye-gaze eventsby the presenter and the audience.
We inves-tigate whether we can predict when and whowill ask what kind of questions, and also inter-est level of the audience.
Based on these anal-yses, we design a smart posterboard which cansense human behaviors and annotate interac-tions and interest level during poster sessions.1 IntroductionAs a variety of spoken dialogue systems have beendeveloped and deployed in the real world, the fron-tier of spoken dialogue research, with engineeringapplications in scope, has been extended from theconventional human-machine speech interface.
Onedirection is a multi-modal interface, which includesnot only graphics but also humanoid robots.
An-other new direction is a multi-party dialogue sys-tem that can talk with multiple persons as an as-sistant agent (D.Bohus and E.Horvitz, 2009) or acompanion robot (S.Fujie et al, 2009).
While theseare extensions of the human-machine speech in-terface, several projects have focused on human-human interactions such as meetings (S.Renals etal., 2007) and free conversations (K.Otsuka et al,2008; C.Oertel et al, 2011), toward ambient systemssupervising the human communications.We have been conducting a project which focuseson conversations in poster sessions, hereafter re-ferred to as poster conversations.
Poster sessionshave become a norm in many academic conventionsand open laboratories because of the flexible and in-teractive characteristics.
Poster conversations havea mixture characteristics of lectures and meetings;typically a presenter explains his/her work to a smallaudience using a poster, and the audience gives feed-back in real time by nodding and verbal backchan-nels, and occasionally makes questions and com-ments.
Conversations are interactive and also multi-modal because people are standing and moving un-like in meetings.
Another good point of poster con-versations is that we can easily make a setting fordata collection, which is controlled in terms of fa-miliarity with topics or other participants and yet is?natural and real?.The goal of the project is signal-level sensingand high-level ?understanding?
of human interac-tions, including speaker diarization and annotationof comprehension and interest level of the audience.These will realize a new indexing scheme of speecharchives.
For example, after a long session of posterpresentation, we often want to get a short review ofthe question-answers and what looked difficult foraudience to follow.
The research will also providea model of intelligent conversational agents that canmake autonomous presentation.As opposed to the conventional content-based in-dexing approach which focuses on the presenter?s1Figure 1: Overview of multi-modal interaction analysisspeech by conducting speech recognition and nat-ural language analysis, we adopt an interaction-oriented approach which looks into the audience?sreaction.
Specifically we focus on non-linguistic in-formation such as backchannel, nodding and eye-gaze information, because we assume the audiencebetter understands the key points of the presentationthan the current machines.
An overview of the pro-posed scheme is depicted in Figure 1.Therefore, we set up an infrastructure for multi-modal sensing and analysis of multi-party interac-tions.
Its process overview is shown in Figure 2.From the audio channel, we detect utterances aswell as laughters and backchannels.
We also de-tect eye-gaze, nodding, and pointing information.Special devices such as a motion-capturing systemand eye-tracking recorders are used to make a ?gold-standard?
corpus, but only video cameras and distantmicrophones will be used in the practical system.Our goal is then annotation of comprehension andinterest level of the audience by combining these in-formation sources.
This annotation will be usefulin speech archives because people would be inter-ested in listening to the points other people wereinterested in.
Since this is apparently difficult tobe well-defined, however, we set up several mile-stones that can be formulated in objective mannersand presumably related with the above-mentionedgoal.
They are introduced in this article after de-scription of the sensing environment and the col-lected corpus in Section 2.
In Section 3, annota-tion of interest level is addressed through detectionof laughters and non-lexical kinds of backchannels,referred to as reactive tokens.
In Section 4 and 5,eye-gaze and nodding information is incorporatedto predict when and who in the audience will askquestions, and also what kind of questions.
WithFigure 2: Flow of multi-modal sensing and analysisthese analyses, we expect that we can get clues tohigh-level ?understanding?
of the conversations, forexample, whether the presentation is understood orliked by the audience.2 Multi-modal Corpus of PosterConversations2.1 Recording EnvironmentWe have designed a special environment (?IMADERoom?)
to record audio, video, human mo-tion, and eye-gaze information in poster conversa-tions (T.Kawahara et al, 2008).
An array of micro-phones (8 to 19) has been designed to be mounted ontop of the posterboard, while each participant useda wireless head-set microphone for recording voicefor the ?gold-standard?
corpus annotation.
A set ofcameras (6 or 8) has also been designed to cover allparticipants and the poster, while a motion captur-ing system was used for the ?gold-standard?
annota-tion.
Each participant was equipped with a dozen ofmotion-capturing markers as well as an eye-trackingrecorder and an accelerometer, but all devices areattached with a cap or stored in a compact belt bag,so they can be naturally engaged in the conversation.An outlook of session recording is given in Figure 3.2.2 Corpus Collection and AnnotationWe have recorded a number of poster conversations(31 in total) using this environment, but for some ofthem, failed to collect all sensor data accurately.
Inthe analyses of the following sections, we use fourposter sessions, in which the presenters and audi-ences are different from each other.
They are allin Japanese, although we recently recorded sessionsin English as well.
In each session, one presenter(labeled as ?A?)
prepared a poster on his/her own2Figure 3: Outlook of poster session recordingacademic research, and there was an audience oftwo persons (labeled as ?B?
and ?C?
), standing infront of the poster and listening to the presentation.They were not familiar with the presenter and hadnot heard the presentation before.
The duration ofeach session was 20-30 minutes.All speech data, collected via the head-set mi-crophones, were segmented into IPUs (Inter-PausalUnit) with time and speaker labels, and transcribedaccording to the guideline of the Corpus of Sponta-neous Japanese (CSJ) (K.Maekawa, 2003).
We alsomanually annotated fillers, verbal backchannels andlaughters.Eye-gaze information is derived from the eye-tracking recorder and the motion capturing systemby matching the gaze vector against the position ofthe other participants and the poster.
Noddings areautomatically detected with the accelerometer at-tached with the cap.3 Detection of Interesting Level withReactive Tokens of AudienceWe hypothesize that the audience signals their in-terest level with their feedback behaviors.
Specif-ically, we focus on the audience?s reactive tokensand laughters.
By reactive tokens (Aizuchi inJapanese), we mean the listener?s verbal short re-sponse, which expresses his/her state of the mindduring the conversation.
The prototypical lexical en-tries of backchannels include ?hai?
in Japanese and?yeah?
or ?okay?
in English, but many of them arenon-lexical and used only for reactive tokens, suchas ?hu:n?, ?he:?
in Japanese and ?wow?, ?uh-huh?in English.
We focus on the latter kind of reactivetokens, which are not used for simple acknowledg-ment.We also investigate detection of laughters and itsrelationship with interesting level.
The detectionmethod and performance were reported in (K.Sumiet al, 2009).3.1 Relationship between Prosodic Patterns ofReactive Tokens and Interest LevelIn this subsection, we hypothesize that the audienceexpresses their interest with specific syllabic andprosodic patterns.
Generally, prosodic features playan important role in conveying para-linguistic andnon-verbal information.
In previous works (F.Yanget al, 2008; A.Gravano et al, 2007), it was re-ported that prosodic features are useful in identi-fying backchannels.
Ward (N.Ward, 2004) madean analysis of pragmatic functions conveyed by theprosodic features in English non-lexical tokens.In this study, we designed an experiment to iden-tify the syllabic and prosodic patterns closely relatedwith interest level.
For this investigation, we selectthree syllabic patterns of ?hu:N?, ?he:?
and ?a:?,which are presumably related with interest level andalso most frequently observed in the corpus, exceptlexical tokens.We computed following prosodic features foreach reactive token: duration, F0 (maximum andrange) and power (maximum).
The prosodic fea-tures are normalized for every person; for each fea-ture, we compute the mean, and this mean is sub-tracted from the feature values.For each syllabic kind of reactive token and foreach prosodic feature, we picked up top-ten andbottom-ten samples, i.e.
samples that have thelargest/smallest values of the prosodic feature.
Foreach of them, an audio segment was extracted tocover the reactive token and its preceding utterances.Then, we had five subjects to listen to the audio seg-ments and evaluate the audience?s state of the mind.We prepared twelve items to be evaluated in a scaleof four (?strongly feel?
to ?do not feel?
), amongwhich two items are related to interest level and3Table 1: Significant combinations of syllabic andprosodic patterns of reactive tokensinterest surprisehu:N duration * *F0 maxF0 rangepowerhe: duration * *F0 max * *F0 range *power * *a: durationF0 max *F0 rangepower *other two items are related to surprise level 1.
Ta-ble 1 lists the results (marked by ?*?)
that have a sta-tistically significant (p < 0.05) difference betweentop-ten and bottom-ten samples.
It is observed thatprolonged ?hu:N?
means interest and surprise while?a:?
with higher pitch or larger power means inter-est.
On the other hand, ?he:?
can be emphasized inall prosodic features to express interest and surprise.The tokens with larger power and/or a longer du-ration is apparently easier to detect than indistincttokens, and they are more related with interest level.It is expected that this rather simple prosodic infor-mation is useful for indexing poster conversations.3.2 Third-party Evaluation of Hot SpotsIn this subsection, we define those segments whichinduced (or elicited) laughters or non-lexical reac-tive tokens as hot spots, 2 and investigate whetherthese hot spots are really funny or interesting to thethird-party viewers of the poster session.We had four subjects, who had not attended thepresentation nor listened the recorded audio content.They were asked to listen to each of the segmentedhot spots in the original time sequence, and to makeevaluations on the questionnaire, as below.1We used different Japanese wording for interest and for sur-prise to enhance the reliability of the evaluation; we adopt theresult if the two matches.2Wrede et al(B.Wrede and E.Shriberg, 2003; D.Gatica-Perez et al, 2005) defined ?hot spots?
as the regions where twoor more participants are highly involved in a meeting.
Our def-inition is different from it.Q1: Do you understand the reason why the reactivetoken/laughter occurred?Q2: Do you find this segment interesting/funny?Q3: Do you think this segment is necessary or use-ful for listening to the content?The percentage of ?yes?
on Question 1 was 89%for laughters and 95% for reactive tokens, confirm-ing that a large majority of the hot spots are appro-priate.The answers to Questions 2 and 3 are more sub-jective, but suggest the usefulness of the hot spots.It turned out that only a half of the spots associatedwith laughters are funny for the subjects (Q2), andthey found 35% of the spots not funny.
The resultsuggests that feeling funny largely depends on theperson.
And we should note that there are not manyfunny parts in poster sessions by nature.On the other hand, more than 90% of the spotsassociated with reactive tokens are interesting (Q2),and useful or necessary (Q3) for the subjects.
Theresult supports the effectiveness of the hot spots ex-tracted based on the reaction of the audience.4 Prediction of Turn-taking with Eye-gazeand Backchannel InformationTurn-taking is an elaborate process especially inmulti-party conversations.
Predicting whom the turnis yielded to or who will take the turn is significantfor an intelligent conversational agent handling mul-tiple partners (D.Bohus and E.Horvitz, 2009; S.Fujieet al, 2009) as well as an automated system to beam-form microphones or zoom in cameras on the speak-ers.
There are a number of previous studies on turn-taking behaviors in dialogue, but studies on com-putational modeling to predict turn-taking in multi-party interactions are very limited (K.Laskowski etal., 2011; K.Jokinen et al, 2011).
Conversationsin poster sessions are different from those in meet-ings and free conversations addressed in the previ-ous works, in that presenters hold most of turns andthus the amount of utterances is very unbalanced.However, the segments of audiences?
questions andcomments are more informative and should not bemissed.
Therefore, we focus on prediction of turn-taking by the audience in poster conversations, and,if that happens, which person in the audience willtake the turn to speak.4Table 2: Duration (sec.)
of eye-gaze and its relationshipwith turn-takingturn held by turn taken bypresenter A B CA gazed at B 0.220 0.589 0.299A gazed at C 0.387 0.391 0.791B gazed at A 0.161 0.205 0.078C gazed at A 0.308 0.215 0.355We also presume that turn-taking by the audienceis related with their interest level because they wantto know more and better when they are more at-tracted to the presentation.It is widely-known that eye-gaze informationplays a significant role in turn-taking (A.Kendon,1967; B.Xiao et al, 2011; K.Jokinen et al, 2011;D.Bohus and E.Horvitz, 2009).
The existence ofposters, however, requires different modeling inposter conversations as the eye-gaze of the partici-pants are focused on the posters in most of the time.This is true to other kinds of interactions using somematerials such as maps and computers.
Moreover,we investigate the use of backchannel informationby the audience during the presenter?s utterances.4.1 Relationship between Eye-gaze andTurn-takingWe identify the object of the eye-gaze of all partic-ipants at the end of the presenter?s utterances.
Thetarget object can be either the poster or other partic-ipants.
Then, we measure the duration of the eye-gaze within the segment of 2.5 seconds before theend of the presenter?s utterances because the major-ity of the IPUs are less than 2.5 seconds.
It is listedin Table 2 in relation with the turn-taking events.
Wecan see the presenter gazed at the person right beforeyielding the turn to him/her significantly longer thanother cases.
However, there is no significant differ-ence in the duration of the eye-gaze by the audienceaccording to the turn-taking events.4.2 Relationship between Joint Eye-gazeEvents and Turn-takingNext, we define joint eye-gaze events by the presen-ter and the audience as shown in Table 3.
In thistable, we use notation of ?audience?, but actuallythese events are defined for each person in the audi-Table 3: Definition of joint eye-gaze events by presenterand audiencewho presentergazes at audience poster(I) (P)audience presenter (i) Ii Piposter (p) Ip PpTable 4: Statistics of joint eye-gaze events by presenterand audience in relation with turn-taking#turn held #turn taken totalby presenter by audience(self) (other)Ii 125 17 3 145Ip 320 71 26 417Pi 190 11 9 210Pp 2974 147 145 3266ence.
Thus, ?Ii?
means the mutual gaze by the pre-senter and a particular person in the audience, and?Pp?
means the joint attention to the poster object.Statistics of these events at the end of the presen-ter?s utterances are summarized in Table 4.
Here,the counts of the events are summed over the twopersons in the audience.
They are classified accord-ing to the turn-taking events, and turn-taking by theaudience is classified into two cases: the person in-volved in the eye-gaze event actually took the turn(self), and the other person took the turn (other).The mutual gaze (?Ii?)
is expected to be related withturn-taking, but its frequency is not so high.
Thefrequency of ?Pi?
is not high, either.
The most po-tentially useful event is ?Ip?, in which the presentergazes at the person in the audience before giving theturn.
This is consistent with the observation in theprevious subsection.4.3 Relationship between Backchannels andTurn-takingAs shown in Section 3, verbal backchannels suggestthe listener?s interest level.
Nodding is regarded asa non-verbal backchannel, and it is more frequentlyobserved in poster conversations than in simple spo-ken dialogues.The occurrence frequencies of these events arecounted within the segment of 2.5 seconds beforethe end of the presenter?s utterances.
They areshown in Figure 4 according to the joint eye-gaze5Figure 4: Statistics of backchannels and their relationshipwith turn-takingevents.
It is observed that the person in the audi-ence who takes the turn (=turn-taker) made morebackchannels both in verbal and non-verbal man-ners, and the tendency is more apparent in the par-ticular eye-gaze events of ?Ii?
and ?Ip?
which areclosely related with the turn-taking events.4.4 Prediction of Turn-taking by AudienceBased on the analyses in the previous subsections,we conduct an experiment to predict turn-taking bythe audience.
The prediction task is divided into twosub-tasks: detection of speaker change and identifi-cation of the next speaker.
In the first sub-task, wepredict whether the turn is given from the presen-ter to someone in the audience, and if that happens,then we predict who in the audience takes the turnin the second sub-task.
Note that these predictionsare done at every end-point of the presenter?s utter-ance (IPU) using the information prior to the speakerchange or the utterance by the new speaker.For the first sub-task of speaker change predic-tion, prosodic features are adopted as a baseline.Specifically, we compute F0 (mean, max, min, andrange) and power (mean and max) of the presenter?sutterance prior to the prediction point.
Backchan-nel features are defined by taking occurrence countsprior to the prediction point for each type (verbalbackchannel and non-verbal nodding).
Eye-gazefeatures are defined in terms of eye-gaze objectsand joint eye-gaze events, as described in previoussubsections, and are parameterized with occurrencecounts and duration.
These parameterizations, how-ever, show no significant difference nor synergeticTable 5: Prediction result of speaker changefeature recall precision F-measureprosody 0.667 0.178 0.280backchannel (BC) 0.459 0.113 0.179eye-gaze (gaze) 0.461 0.216 0.290prosody+BC 0.668 0.165 0.263prosody+gaze 0.706 0.209 0.319prosody+BC+gaze 0.678 0.189 0.294effect in terms of prediction performance.SVM is adopted to predict whether speakerchange happens or not by using these features.
Theresult is summarized in Table 5.
Here, we computerecall, precision and F-measure for speaker change,or turn-taking by the audience.
This case accountsfor only 11.9% and its prediction is very challeng-ing, while we can easily get an accuracy of over 90%for prediction of turn-holding by the presenter.
Weare particularly concerned on the recall of speakerchange, considering the nature of the task and appli-cation scenarios.Among the individual features, the prosodic fea-tures obtain the best recall while the eye-gaze fea-tures achieve the best precision and F-measure.Combination of these two is effective in improvingboth recall and precision.
On the other hand, thebackchannel features get the lowest performance,and its combination with the other features is not ef-fective, resulting in degradation of the performance.Next, we conduct the second sub-task of speakerprediction.
Predicting the next speaker in a multi-party conversation (before he/she actually speaks) isalso challenging, and has not been addressed in theprevious work (K.Jokinen et al, 2011).
For this sub-task, the prosodic features of the current speaker arenot usable because it does not have information sug-gesting who the turn will be yielded to.
Therefore,we adopt the backchannel features and eye-gaze fea-tures.
Note that these features are computed for in-dividual persons in the audience, instead of takingthe maximum or selecting among them.The result is summarized in Table 6.
In this exper-iment, the backchannel features have some effect,and by combining them with the eye-gaze features,the accuracy reaches almost 70%.6Table 6: Prediction result of the next speakerfeature accuracyeye-gaze (gaze) 66.4%backchannel (BC) 52.6%gaze+BC 69.7%5 Relationship between FeedbackBehaviors and Question TypeNext, we investigate the relationship between feed-back behaviors of the audience and the kind of ques-tions they ask after they take a turn.
In this work,questions are classified into confirming questionsand substantive questions.
The confirming questionsare asked to make sure of the understanding of thecurrent explanation, thus they can be answered sim-ply by ?Yes?
or ?No?.3 The substantive questions,on the other hand, are asking about what was notexplained by the presenter, thus they cannot be an-swered by ?Yes?
or ?No?
only; an additional expla-nation is needed.This annotation together with the preceding ex-planation segment is not so straightforward when theconversation got into the QA phase after the presen-ter went through an entire poster presentation.
Thus,we exclude the QA phase and focus on the questionsasked during the explanation phase.
In this section,we analyze the behaviors during the explanation seg-ment that precedes the question by merging all con-secutive IPUs of the presenter.
This is a reasonableassumption once turn-taking is predicted in the pre-vious section.
These are major differences from theanalysis of the previous section.5.1 Relationship between Backchannels andQuestion TypeThe occurrence frequencies of verbal backchannelsand non-verbal noddings, normalized by the dura-tion of the explanation segment (seconds), are listedaccording to the question type in Tables 7 and 8.In these tables, statistics of the person who actu-ally asked questions are compared with those of theperson who did not.
We can observe the turn-takermade significantly more verbal backchannels whenasking substantive questions.
On the other hand,3This does not mean the presenter actually answered simplyby ?Yes?
or ?No?.Table 7: Frequencies (per sec.)
of verbal backchannelsand their relationship with question typeconfirming substantiveturn-taker 0.034 0.063non-turn-taker 0.041 0.038Table 8: Frequencies (per sec.)
of non-verbal noddingsand their relationship with question typeconfirming substantiveturn-taker 0.111 0.127non-turn-taker 0.109 0.132Table 9: Duration (ratio) of joint eye-gaze events andtheir relationship with question typeconfirming substantiveIi 0.053 0.015Ip 0.116 0.081Pi 0.060 0.035Pp 0.657 0.818there is no significant difference in the frequency ofnon-verbal noddings among the audience and amongthe question types.5.2 Relationship between Eye-gaze Events andQuestion TypeWe also investigate the relationship between eye-gaze events and the question type.
Among severalparameterizations introduced in the previous sec-tion, we observe a significant tendency in the du-ration of the joint eye-gaze events, which is normal-ized by the duration of the presenter?s explanationsegment.
It is summarized in Table 9.
We can seethe increase of ?Ip?
(and decrease of ?Pp?
accord-ingly) in confirming questions.
By combining withthe analysis in the previous section, we can reasonthe majority of turn-taking signaled by the presen-ter?s gazing is attributed to confirmation.6 Smart PosterboardWe have designed and implemented a smart poster-board, which can record a poster session, sense hu-man behaviors and annotate interactions.
Since itis not practical to ask every participant to wear spe-cial devices such as a head-set microphone and aneye-tracking recorder and also to set up any devicesattached to a room, all sensing devices are attached7Figure 5: Outlook of smart posterboardto the posterboard, which is actually a 65-inch LCDdisplay.
An outlook of the posterboard is given inFigure 5.It is equipped with a 19-channel microphone arrayon the top, and attached with six cameras and twoKinect sensors.
Speech separation and enhancementhas been realized with Blind Spatial Subtraction Ar-ray (BSSA), which consists of the delay-and-sum(DS) beamformer and a noise estimator based on in-dependent component analysis (ICA) (Y.Takahashiet al, 2009).
In this step, the audio input is separatedto the presenter and the audience, but discriminationamong the audience is not done.
Visual informationshould be combined to annotate persons in the au-dience.
Voice activity detection (VAD) is conductedon each of the two channels to make speaker diariza-tion.
Localization of the persons in the audience andestimation of their head direction, which approxi-mates their eye-gaze, are conducted using the videoinformation captured by the six cameras.Although high-level annotations addressed in theprevious sections have not been yet implemented inthe current system, the above-mentioned processingrealizes a browser of poster sessions which visual-izes the interaction.The Kinect sensors are used for a portable and on-line version, in which speech enhancement, speakerlocalization and head direction estimation are per-formed in real time.We made a demonstration of the system in IEEE-ICASSP 2012 as shown in Figure 5, and plan furtherimprovements and trials in the future.7 ConclusionsThis article has given an overview of our multi-modal data collection and analysis of poster conver-sations.
Poster conversations provide us with a num-ber of interesting topics in spoken dialogue researchas they are essentially multi-modal and multi-party.By focusing on the audience?s feedback behaviorsand joint eye-gaze events, it is suggested that we canannotate interest level of the audience and hot spotsin the session.Nowadays, presentation using a poster is one ofthe common and important activities in academicand business communities.
As large LCD displaysbecome ubiquitous, its style will be more interac-tive.
Accordingly, sensing and archiving functionsintroduced in the smart posterboard will be useful.AcknowledgmentsThe work presented in this article was conductedjointly with Hisao Setoguchi, Zhi-Qiang Chang,Takanori Tsuchiya, Takuma Iwatate, and KatsuyaTakanashi.
The smart posterboard system has beendeveloped by a number of researchers in Kyoto Uni-versity and Nara Institute of Science and Technol-ogy (NAIST).This work was supported by JST CREST andJSPS Grant-in-Aid for Scientific Research.ReferencesA.Gravano, S.Benus, J.Hirschberg, S.Mitchell, andI.Vovsha.
2007.
Classification of discourse functionsof affirmative words in spoken dialogue.
In Proc.
IN-TERSPEECH, pages 1613?1616.A.Kendon.
1967.
Some functions of gaze direction insocial interaction.
Acta Psychologica, 26:22?63.B.Wrede and E.Shriberg.
2003.
Spotting ?hot spots?
inmeetings: Human judgments and prosodic cues.
InProc.
EUROSPEECH, pages 2805?2808.B.Xiao, V.Rozgic, A.Katsamanis, B.R.Baucom,P.G.Georgiou, and S.Narayanan.
2011.
Acous-tic and visual cues of turn-taking dynamics indyadic interactions.
In Proc.
INTERSPEECH, pages2441?2444.C.Oertel, S.Scherer, and N.Campbell.
2011.
On the useof multimodal cues for the prediction of degrees of in-volvement in spontaneous conversation.
In Proc.
IN-TERSPEECH, pages 1541?1545.8D.Bohus and E.Horvitz.
2009.
Models for multipartyengagement in open-world dialog.
In Proc.
SIGdial.D.Gatica-Perez, I.McCowan, D.Zhang, and S.Bengio.2005.
Detecting group interest-level in meetings.
InProc.
IEEE-ICASSP, volume 1, pages 489?492.F.Yang, G.Tur, and E.Shriberg.
2008.
Exploiting dialogact tagging and prosodic information for action itemidentification.
In Proc.
IEEE-ICASSP, pages 4941?4944.K.Jokinen, K.Harada, M.Nishida, and S.Yamamoto.2011.
Turn-alignment using eye-gaze and speech inconversational interaction.
In Proc.
INTERSPEECH,pages 2018?2021.K.Laskowski, J.Edlund, and M.Heldner.
2011.
A single-port non-parametric model of turn-taking in multi-party conversation.
In Proc.
IEEE-ICASSP, pages5600?5603.K.Maekawa.
2003.
Corpus of Spontaneous Japanese: Itsdesign and evaluation.
In Proc.
ISCA & IEEE Work-shop on Spontaneous Speech Processing and Recogni-tion, pages 7?12.K.Otsuka, S.Araki, K.Ishizuka, M.Fujimoto, M.Heinrich,and J.Yamato.
2008.
A realtime multimodal systemfor analyzing group meetings by combining face posetracking and speaker diarization.
In Proc.
ICMI, pages257?262.K.Sumi, T.Kawahara, J.Ogata, and M.Goto.
2009.Acoustic event detection for spotting hot spots in pod-casts.
In Proc.
INTERSPEECH, pages 1143?1146.N.Ward.
2004.
Pragmatic functions of prosodic featuresin non-lexical utterances.
In Speech Prosody, pages325?328.S.Fujie, Y.Matsuyama, H.Taniyama, and T.Kobayashi.2009.
Conversation robot participating in and activat-ing a group communication.
In Proc.
INTERSPEECH,pages 264?267.S.Renals, T.Hain, and H.Bourlard.
2007.
Recognitionand understanding of meetings: The AMI and AMIDAprojects.
In Proc.
IEEE Workshop Automatic SpeechRecognition & Understanding.T.Kawahara, H.Setoguchi, K.Takanashi, K.Ishizuka, andS.Araki.
2008.
Multi-modal recording, analysis andindexing of poster sessions.
In Proc.
INTERSPEECH,pages 1622?1625.Y.Takahashi, T.Takatani, K.Osako, H.Saruwatari, andK.Shikano.
2009.
Blind spatial subtraction ar-ray for speech enhancement in noisy environment.IEEE Trans.
Audio, Speech & Language Process.,17(4):650?664.9
