Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 48?53,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsA Graph-based Cross-lingual Projection Approach forWeakly Supervised Relation ExtractionSeokhwan KimHuman Language Technology Dept.Institute for Infocomm ResearchSingapore 138632kims@i2r.a-star.edu.sgGary Geunbae LeeDept.
of Computer Science and EngineeringPohang University of Science and TechnologyPohang, 790-784, Koreagblee@postech.ac.krAbstractAlthough researchers have conducted exten-sive studies on relation extraction in the lastdecade, supervised approaches are still limitedbecause they require large amounts of trainingdata to achieve high performances.
To builda relation extractor without significant anno-tation effort, we can exploit cross-lingual an-notation projection, which leverages parallelcorpora as external resources for supervision.This paper proposes a novel graph-based pro-jection approach and demonstrates the mer-its of it by using a Korean relation extrac-tion system based on projected dataset froman English-Korean parallel corpus.1 IntroductionRelation extraction aims to identify semantic rela-tions of entities in a document.
Although manysupervised machine learning approaches have beensuccessfully applied to relation extraction tasks (Ze-lenko et al, 2003; Kambhatla, 2004; Bunescu andMooney, 2005; Zhang et al, 2006), applications ofthese approaches are still limited because they re-quire a sufficient number of training examples to ob-tain good extraction results.
Several datasets thatprovide manual annotations of semantic relation-ships are available from MUC (Grishman and Sund-heim, 1996) and ACE (Doddington et al, 2004)projects, but these datasets contain labeled trainingexamples in only a few major languages, includ-ing English, Chinese, and Arabic.
Although thesedatasets encourage the development of relation ex-tractors for these major languages, there are few la-beled training samples for learning new systems inother languages, such as Korean.
Because manualannotation of semantic relations for such resource-poor languages is very expensive, we instead con-sider weakly supervised learning techniques (Riloffand Jones, 1999; Agichtein and Gravano, 2000;Zhang, 2004; Chen et al, 2006) to learn the rela-tion extractors without significant annotation efforts.But these techniques still face cost problems whenpreparing quality seed examples, which plays a cru-cial role in obtaining good extractions.Recently, some researchers attempted to use ex-ternal resources, such as treebank (Banko et al,2007) and Wikipedia (Wu and Weld, 2010), thatwere not specially constructed for relation extractioninstead of using task-specific training or seed exam-ples.
We previously proposed to leverage parallelcorpora as a new kind of external resource for rela-tion extraction (Kim et al, 2010).
To obtain trainingexamples in the resource-poor target language, thisapproach exploited a cross-lingual annotation pro-jection by propagating annotations that were gener-ated by a relation extraction system in a resource-rich source language.
In this approach, projectedannotations were determined in a single pass pro-cess by considering only alignments between entitycandidates; we call this action direct projection.In this paper, we propose a graph-based projec-tion approach for weakly supervised relation extrac-tion.
This approach utilizes a graph that is con-stucted with both instance and context informationand that is operated in an iterative manner.
The goalof our graph-based approach is to improve the ro-bustness of the extractor with respect to errors thatare generated and accumulated by preprocessors.48fE (<Barack Obama, Honolulu>) = 1fK  ( <  ??
zj  ,   ?
?F>  > ) = 1??
zj(beo-rak-o-ba-ma)&r(e-seo)?(neun)??F>(ho-nol-rul-ru)???(ha-wa-i)2:.(tae-eo-nat-da)?
(ui)Barack Obama was born in Honolulu Hawaii, .
(beo-rak-o-ba-ma) (ho-nol-rul-ru)Figure 1: An example of annotation projection for rela-tion extraction of a bitext in English and Korean2 Cross-lingual Annotation Projection forRelation ExtractionRelation extraction can be considered to be a classi-fication problem by the following classifier:f(ei, ej)={1 if ei and ej have a relation,?1 otherwise.
,where ei and ej are entities in a sentence.Cross-lingual annotation projection intends tolearn an extractor ft for good performance with-out significant effort toward building resources fora resource-poor target language Lt. To accomplishthat goal, the method automatically creates a set ofannotated text for ft, utilizing a well-made extractorfs for a resource-rich source language Ls and a par-allel corpus of Ls and Lt.
Figure 1 shows an exam-ple of annotation projection for relation extractionwith a bi-text in Lt Korean and Ls English.
Given anEnglish sentence, an instance ?Barack Obama, Hon-olulu?
is extracted as positive.
Then, its translationalcounterpart ?beo-rak-o-ba-ma, ho-nol-rul-ru?
in theKorean sentence also has a positive annotation byprojection.Early studies in cross-lingual annotation projec-tion were accomplished for various natural lan-guage processing tasks (Yarowsky and Ngai, 2001;Yarowsky et al, 2001; Hwa et al, 2005; Zitouni andFlorian, 2008; Pado and Lapata, 2009).
These stud-ies adopted a simple direct projection strategy thatpropagates the annotations in the source languagesentences to word-aligned target sentences, and atarget system can bootstrap from these projected an-notations.For relation extraction, the direct projection strat-egy can be formularized as follows: ft(eit, ejt)=fs(A(eit), A(ejt )), where A(et) is the aligned entityof et.
However, these automatic annotations can beunreliable because of source text mis-classificationand word alignment errors; thus, it can cause a criti-cal falling-off in the annotation projection quality.Although some noise reduction strategies for pro-jecting semantic relations were proposed (Kim et al,2010), the direct projection approach is still vulner-able to erroneous inputs generated by submodules.We note two main causes for this limitation: (1)the direct projection approach considers only align-ments between entity candidates, and it does notconsider any contextual information; and, (2) it isperformed by a single pass process.
To solve both ofthese problems at once, we propose a graph-basedprojection approach for relation extraction.3 Graph ConstructionThe most crucial factor in the success of graph-based learning approaches is how to construct agraph that is appropriate for the target task.
Dasand Petrov (Das and Petrov, 2011) proposed a graph-based bilingual projection of part-of-speech taggingby considering the tagged words in the source lan-guage as labeled examples and connecting them tothe unlabeled words in the target language, while re-ferring to the word alignments.
Graph constructionfor projecting semantic relationships is more com-plicated than part-of-speech tagging because the unitinstance of projection is a pair of entities and not aword or morpheme that is equivalent to the align-ment unit.3.1 Graph VerticesTo construct a graph for a relation projection, wedefine two types of vertices: instance vertices V andcontext vertices U .Instance vertices are defined for all pairs of en-tity candidates in the source and target languages.Each instance vertex has a soft label vector Y =[ y+ y?
], which contains the probabilities thatthe instance is positive or negative, respectively.
Thelarger the y+ value, the more likely the instance hasa semantic relationship.
The initial label values of aninstance vertex vijs ?
Vs for the instance?eis, ejs?inthe source language are assigned based on the con-fidence score of the extractor fs.
With respect to thetarget language, every instance vertex vijt ?
Vt has49the same initial values of 0.5 in both y+ and y?.The other type of vertices, context vertices, areused for identifying relation descriptors that are con-textual subtexts that represent semantic relationshipsof the positive instances.
Because the characteristicsof these descriptive contexts vary depending on thelanguage, context vertices should be defined to belanguage-specific.
In the case of English, we definethe context vertex for each trigram that is located be-tween a given entity pair that is semantically related.If the context vertices Us for the source languagesentences are defined, then the units of context inthe target language can also be created based on theword alignments.
The aligned counterpart of eachsource language context vertex is used for generat-ing a context vertex uit ?
Ut in the target language.Each context vertex us ?
Us and ut ?
Ut also hasy+ and y?, which represent how likely the contextis to denote semantic relationships.
The probabilityvalues for all of the context vertices in both of thelanguages are initially assigned to y+ = y?
= 0.5.3.2 Edge WeightsThe graph for our graph-based projection is con-structed by connecting related vertex pairs byweighted edges.
If a given pair of vertices is likely tohave the same label, then the edge connecting thesevertices should have a large weight value.We define three types of edges according to com-binations of connected vertices.
The first type ofedges consists of connections between an instancevertex and a context vertex in the same language.For a pair of an instance vertex vi,j and a contextvertex uk, these vertices are connected if the contextsequence of vi,j contains uk as a subsequence.
Ifvij is matched to uk, the edge weight w(vi,j , uk))is assigned to 1.
Otherwise, it should be 0.Another edge category is for the pairs of contextvertices in a language.
Because each context vertexis considered to be an n-gram pattern in our work,the weight value for each edge of this type representsthe pattern similarity between two context vertices.The edge weight w(uk, ul) is computed by Jaccard?scoefficient between uk and ul.While the previous two categories of edges areconcerned with monolingual connections, the othertype addresses bilingual alignments of context ver-tices between the source language and the target lan-guage.
We define the weight for a bilingual edgeconnecting uks and ult as the relative frequency ofalignments, as follows:w(uks , ult) = count(uks , ult)/?umtcount(uks , umt),where count (us, ut) is the number of alignmentsbetween us and ut across the whole parallel corpus.4 Label PropagationTo induce labels for all of the unlabeled vertices onthe graph constructed in Section 3, we utilize thelabel propagation algorithm (Zhu and Ghahramani,2002), which is a graph-based semi-supervisedlearning algorithm.First, we construct an n ?
n matrix T that rep-resents transition probabilities for all of the vertexpairs.
After assigning all of the values on the ma-trix, we normalize the matrix for each row, to makethe element values be probabilities.
The other inputto the algorithm is an n ?
2 matrix Y , which indi-cates the probabilities of whether a given vertex vi ispositive or not.
The matrix T and Y are initializedby the values described in Section 3.For the input matrices T and Y , label propagationis performed by multiplying the two matrices, to up-date the Y matrix.
This multiplication is repeateduntil Y converges or until the number of iterationsexceeds a specific number.
The Y matrix, after fin-ishing its iterations, is considered to be the result ofthe algorithm.5 ImplementationTo demonstrate the effectiveness of the graph-basedprojection approach for relation extraction, we de-veloped a Korean relation extraction system that wastrained with projected annotations from English re-sources.
We used an English-Korean parallel cor-pus 1 that contains 266,892 bi-sentence pairs in En-glish and Korean.
We obtained 155,409 positive in-stances from the English sentences using an off-the-shelf relation extraction system, ReVerb 2 (Fader etal., 2011).1The parallel corpus collected is available in our website:http://isoft.postech.ac.kr/?megaup/acl/datasets2http://reverb.cs.washington.edu/50Table 1: Comparison between direct and graph-basedprojection approaches to extract semantic relationshipsfor four relation typesType Direct Graph-basedP R F P R FAcquisition 51.6 87.7 64.9 55.3 91.2 68.9Birthplace 69.8 84.5 76.4 73.8 87.3 80.0Inventor Of 62.4 85.3 72.1 66.3 89.7 76.3Won Prize 73.3 80.5 76.7 76.4 82.9 79.5Total 63.9 84.2 72.7 67.7 87.4 76.3The English sentence annotations in the parallelcorpus were then propagated into the correspond-ing Korean sentences.
We used the GIZA++ soft-ware 3 (Och and Ney, 2003) to obtain the word align-ments for each bi-sentence in the parallel corpus.The graph-based projection was performed by theJunto toolkit 4 with the maximum number of itera-tions of 10 for each execution.Projected instances were utilized as training ex-amples to learn the Korean relation extractor.
Webuilt a tree kernel-based support vector machinemodel using SVM-Light 5 (Joachims, 1998) andTree Kernel tools 6 (Moschitti, 2006).
In our model,we adopted the subtree kernel method for the short-est path dependency kernel (Bunescu and Mooney,2005).6 EvaluationThe experiments were performed on the manu-ally annotated Korean test dataset.
The datasetwas built following the approach of Bunescu andMooney (Bunescu and Mooney, 2007).
The datasetconsists of 500 sentences for four relation types: Ac-quisition, Birthplace, Inventor of, and Won Prize.
Ofthese, 278 sentences were annotated as positive in-stances.The first experiment aimed to compare two sys-tems constructed by the direct projection (Kim et al,2010) and graph-based projection approach.
Table 1shows the performances of the relation extraction ofthe two systems.
The graph-based system achievedbetter performances in precision and recall than the3http://code.google.com/p/giza-pp/4http://code.google.com/p/junto/5http://svmlight.joachims.org/6http://disi.unitn.it/ moschitt/Tree-Kernel.htmTable 2: Comparisons of our projection approach toheuristic and Wikipedia-based approachesApproach P R FHeuristic-based 92.31 17.27 29.09Wikipedia-based 66.67 66.91 66.79Projection-based 67.69 87.41 76.30system with direct projection for all of the four re-lation types.
It outperformed the baseline system byan F-measure of 3.63.To demonstrate the merits of our work againstother approaches based on monolingual external re-sources, we performed comparisons with the fol-lowing two baselines: heuristic-based (Banko etal., 2007) and Wikipedia-based approaches (Wu andWeld, 2010).
The heuristic-based baseline was builton the Sejong treebank corpus (Kim, 2006) and theWikipedia-based baseline used Korean Wikipediaarticles 7.
Table 2 compares the performances of thetwo baseline systems and our method.
Our proposedprojection-based approach obtained better perfor-mance than the other systems.
It outperformed theheuristic-based system by 47.21 and the Wikipedia-based system by 9.51 in the F-measure.7 ConclusionsThis paper presented a novel graph-based projectionapproach for relation extraction.
Our approach per-formed a label propagation algorithm on a proposedgraph that represented the instance and context fea-tures of both the source and target languages.
Thefeasibility of our approach was demonstrated by ourKorean relation extraction system.
Experimental re-sults show that our graph-based projection helped toimprove the performance of the cross-lingual anno-tation projection of the semantic relations, and oursystem outperforms the other systems, which incor-porate monolingual external resources.In this work, we operated the graph-based pro-jection under very restricted conditions, because ofhigh complexity of the algorithm.
For future work,we plan to relieve the complexity problem for deal-ing with more expanded graph structure to improvethe performance of our proposed approach.7We used the Korean Wikipedia database dump as of June2011.51AcknowledgmentsThis research was supported by the MKE(TheMinistry of Knowledge Economy), Korea, un-der the ITRC(Information Technology ResearchCenter) support program (NIPA-2012-(H0301-12-3001)) supervised by the NIPA(National IT IndustryPromotion Agency) and Industrial Strategic technol-ogy development program, 10035252, developmentof dialog-based spontaneous speech interface tech-nology on mobile platform, funded by the Ministryof Knowledge Economy(MKE, Korea).ReferencesE.
Agichtein and L. Gravano.
2000.
Snowball: Ex-tracting relations from large plain-text collections.
InProceedings of the fifth ACM conference on Digital li-braries, pages 85?94.M.
Banko, M. J Cafarella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open information extrac-tion from the web.
In Proceedings of the 20th In-ternational Joint Conference on Artificial Intelligence,pages 2670?2676.R.
Bunescu and R. Mooney.
2005.
A shortest path de-pendency kernel for relation extraction.
In Proceed-ings of the conference on Human Language Technol-ogy and Empirical Methods in Natural Language Pro-cessing, pages 724?731.R.
Bunescu and R. Mooney.
2007.
Learning to extractrelations from the web using minimal supervision.
InProceedings of the 45th annual meeting of the Associ-ation for Computational Linguistics, volume 45, pages576?583.J.
Chen, D. Ji, C. L Tan, and Z. Niu.
2006.
Relation ex-traction using label propagation based semi-supervisedlearning.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the 44thannual meeting of the Association for ComputationalLinguistics, pages 129?136.D.
Das and S. Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projections.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 600?609.G.
Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,S.
Strassel, and R. Weischedel.
2004.
The auto-matic content extraction (ACE) program?tasks, data,and evaluation.
In Proceedings of LREC, volume 4,pages 837?840.A.
Fader, S. Soderland, and O. Etzioni.
2011.
Identify-ing relations for open information extraction.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 1535?1545.R.
Grishman and B. Sundheim.
1996.
Message under-standing conference-6: A brief history.
In Proceedingsof the 16th conference on Computational linguistics,volume 1, pages 466?471.R.
Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Ko-lak.
2005.
Bootstrapping parsers via syntactic projec-tion across parallel texts.
Natural language engineer-ing, 11(3):311?325.T.
Joachims.
1998.
Text categorization with support vec-tor machines: Learning with many relevant features.In Proceedings of the European Conference on Ma-chine Learning, pages 137?142.N.
Kambhatla.
2004.
Combining lexical, syntactic,and semantic features with maximum entropy mod-els for extracting relations.
In Proceedings of theACL 2004 on Interactive poster and demonstrationsessions, pages 22?25.S.
Kim, M. Jeong, J. Lee, and G. G Lee.
2010.
A cross-lingual annotation projection approach for relation de-tection.
In Proceedings of the 23rd International Con-ference on Computational Linguistics, pages 564?571.H.
Kim.
2006.
Korean national corpus in the 21st cen-tury sejong project.
In Proceedings of the 13th NIJLInternational Symposium, pages 49?54.A.
Moschitti.
2006.
Making tree kernels practical fornatural language learning.
In Proceedings of the 11thConference of the European Chapter of the Associa-tion for Computational Linguistics, volume 6, pages113?120.F.
J Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
Computationallinguistics, 29(1):19?51.S.
Pado and M. Lapata.
2009.
Cross-lingual annotationprojection of semantic roles.
Journal of Artificial In-telligence Research, 36(1):307?340.E.
Riloff and R. Jones.
1999.
Learning dictionaries forinformation extraction by multi-level bootstrapping.In Proceedings of the National Conference on Artifi-cial Intelligence, pages 474?479.F.
Wu and D. Weld.
2010.
Open information extractionusing wikipedia.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 118?127.D.
Yarowsky and G. Ngai.
2001.
Inducing multilingualPOS taggers and NP bracketers via robust projectionacross aligned corpora.
In Proceedings of the SecondMeeting of the North American Chapter of the Associ-ation for Computational Linguistics, pages 1?8.D.
Yarowsky, G. Ngai, and R. Wicentowski.
2001.
In-ducing multilingual text analysis tools via robust pro-jection across aligned corpora.
In Proceedings of the52First International Conference on Human LanguageTechnology Research, pages 1?8.D.
Zelenko, C. Aone, and A. Richardella.
2003.
Kernelmethods for relation extraction.
The Journal of Ma-chine Learning Research, 3:1083?1106.M.
Zhang, J. Zhang, J. Su, and G. Zhou.
2006.
A com-posite kernel to extract relations between entities withboth flat and structured features.
In Proceedings of the21st International Conference on Computational Lin-guistics and the 44th annual meeting of the Associa-tion for Computational Linguistics, pages 825?832.Z.
Zhang.
2004.
Weakly-supervised relation classifica-tion for information extraction.
In Proceedings of thethirteenth ACM international conference on Informa-tion and knowledge management, pages 581?588.X.
Zhu and Z. Ghahramani.
2002.
Learning from labeledand unlabeled data with label propagation.
SchoolComput.
Sci., Carnegie Mellon Univ., Pittsburgh, PA,Tech.
Rep. CMU-CALD-02-107.I.
Zitouni and R. Florian.
2008.
Mention detection cross-ing the language barrier.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 600?609.53
