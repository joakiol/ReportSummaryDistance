INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 90?94,Utica, May 2012. c?2012 Association for Computational LinguisticsRich Morphology GenerationUsing Statistical Machine TranslationAhmed El Kholy and Nizar HabashCenter for Computational Learning Systems, Columbia University475 Riverside Drive New York, NY 10115{akholy,habash}@ccls.columbia.eduAbstractWe present an approach for generation of mor-phologically rich languages using statisticalmachine translation.
Given a sequence of lem-mas and any subset of morphological features,we produce the inflected word forms.
Testingon Arabic, a morphologically rich language,our models can reach 92.1% accuracy startingonly with lemmas, and 98.9% accuracy if allthe gold features are provided.1 IntroductionMany natural language processing (NLP) applica-tions, such as summarization and machine trans-lation (MT), require natural language generation(NLG).
Generation for morphologically rich lan-guages, which introduce a lot of challenges for NLPin general, has gained a lot of attention recently, es-pecially in the context of statistical MT (SMT).
Thecommon wisdom for handling morphological rich-ness is to reduce the complexity in the internal ap-plication models and then generate complex wordforms in a final step.In this paper,1 we present a SMT-based approachfor generation of morphologically rich languages.Given a sequence of lemmas and any subset of mor-phological features, we produce the inflected wordforms.
The SMT model parameters are derived froma parallel corpus mapping lemmas and morphologi-cal features to the inflected word forms.As a case study, we focus on Arabic, a mor-phologically rich language.
Our models can reach92.1% accuracy starting only with tokenized lem-mas and predicting some features, up from 55.0%accuracy without inflecting the lemmas.
If all of thegold morphological features are provided as input,our best model achieves 98.9% accuracy.1This work was funded by a Google research award.2 Related WorkIn the context of morphological generation for MT,the state-of-the-art factored machine translation ap-proach models morphology using generation factorsin the translation process (Koehn et al, 2007).
Oneof the limitations of factored models is that gen-eration is based on the word level not the phraselevel and the context is only captured through a lan-guage model.
Minkov et al (2007) and Toutanova etal.
(2008) model translation and morphology inde-pendently for English-Arabic and English-RussianMT.
They use a maximum entropy model to predictinflected word forms directly.
Clifton and Sarkar(2011) use a similar approach for English-FinnishMT where they predict morpheme sequences.
Un-like both approaches, we generate the word formsfrom the deeper representation of lemmas and fea-tures.As for using SMT in generation, there are manyprevious efforts.
Wong and Mooney (2007) useSMT methods for tactical NLG.
They learn throughSMT to map meaning representations to natural lan-guage.
Quirk et al (2004) apply SMT tools to gen-erate paraphrases of input sentences in the same lan-guage.
Both of these efforts target English, a mor-phologically poor language.
Our work is conceptu-ally closer to Wong and Mooney (2007), except thatwe focus on the question of morphological genera-tion and our approach includes an optional featureprediction component.
In a related publication, weintegrate our generation model as part of end-to-endEnglish-Arabic SMT (El Kholy and Habash, 2012).In that work, we make use of English features in theArabic morphology prediction component, e.g., En-glish POS and parse trees.903 Arabic ChallengesArabic is a morphologically complex language.
Oneaspect of Arabic?s complexity is its orthographywhich often omits short vowel diacritics.
As a re-sult, ambiguity is rampant.
Another aspect is thevarious attachable clitics which include conjunctionproclitics, e.g., +?
w+ ?and?, particle proclitics, e.g.,+?
l+ ?to/for?, the definite article +?
@ Al+ ?the?, andthe class of pronominal enclitics, e.g., ?
?+ +hm?their/them?.
Beyond these clitics, Arabic wordsinflect for person (PER), gender (GEN), number(NUM), aspect (ASP), mood (MOD), voice (VOX),state (STT) and case (CAS).
Arabic inflectionalfeatures are realized as affixes as well as templaticchanges, e.g., broken plurals.2These three phenomena, optional diacritics, at-tachable clitics and the large inflectional space, leadto thousands of inflected forms per lemma and a highdegree of ambiguity: about 12 analyses per word,typically corresponding to two lemmas on average(Habash, 2010).
The Penn Arabic Treebank (PATB)tokenization scheme (Maamouri et al, 2004), whichwe use in all our experiments, separates all cliticsexcept for the determiner clitic Al+ (DET).
As suchwe consider the DET as an additional morphologicalfeature.Arabic has complex morpho-syntactic agreementrules in terms of GEN, NUM and definiteness.
Ad-jectives agree with nouns in GEN and NUM but plu-ral irrational nouns exceptionally take feminine sin-gular adjectives.
Moreover, verbs agree with sub-jects in GEN only in VSO order while they agreein GEN and NUM in SVO order (Alkuhlani andHabash, 2011).
The DET in Arabic is used to dis-tinguish different syntactic constructions such as thepossessive or adjectival modification.
These agree-ment rules make the generation of correctly inflectedforms in context a challenging task.4 ApproachIn this section, we discuss our approach in gener-ating Arabic words from Arabic lemmas (LEMMA)using a pipeline of three steps.1.
(Optional)Morphology Prediction of linguis-tic features to inflect LEMMAs.2The Arabic NLP tools we use in this paper do not model alltemplatic inflectional realizations.Tokens w+ s+ yktbwn +hAPOS conj fut part verb pronLemma wa sa katab hAFeatures na,na,na, na,na,na, 3rd,masc,pl, 3rd,fem,sg,na,na,na, na,na,na, imp,act,ind, na,na,na,na,na,na, na,na,na, na,na,na, na,na,na,Figure 1: An example A?+ 	K?J.J?K+?+?
w+s+yktbwn +hA ?andthey will write it?.
Features?
order of presentation is: PER, GEN,NUM, ASP, VOX, MOD, DET, CAS, and STT.
The value ?na?
isfor ?not-applicable?.2.
Morphology Generation of inflected Arabictokens from LEMMAs and any subset of Ara-bic linguistic features.3.
Detokenization of inflected Arabic tokens intosurface Arabic words.Morphology generation is the main contributionof this paper which in addition to detokenization rep-resents an end-to-end inflection generator.
The mor-phology prediction step is an optional step that com-plements the whole process by enriching the inputof the morphology generation step with one or morepredicted morphological features.We follow numerous previously published effortson the value of tokenization for Arabic NLP tasks(Badr et al, 2008; El Kholy and Habash, 2010).
Weuse the best performing tokenization scheme (PATB)in machine translation in all our experiments and fo-cus on the question of how to generate Arabic in-flected words from LEMMAs and features.
Figure 1shows an example of a tokenized word and its de-composition into a LEMMA and morphological fea-tures.Morphology Prediction This optional step takesa sequence of LEMMAs and tries to enrich themby predicting one or more morphological features.It is implemented using a supervised discriminativelearning model, namely Conditional Random Fields(CRF) (Lafferty et al, 2001).
Table 1 shows theaccuracy of the CRF module on a test set of 1000sentences compared to using the most common fea-ture value baseline.
Some features, such as CAS andSTT are harder to predict but they also have very lowbaseline values.
GEN, DET and NUM have a mod-erate prediction accuracy while ASP, PER, VOX andMOD have high prediction accuracy (but also veryhigh baselines).
This task is similar to POS tagging91Predicted Baseline PredictionFeature Accuracy% Accuracy%Case (CAS) 42.87 70.39State (STT) 42.85 76.93Gender (GEN) 67.42 84.17Determiner (DET) 59.71 85.41Number (NUM) 70.61 87.31Aspect (ASP) 90.38 92.10Person (PER) 85.71 92.80Voice (VOX) 90.38 93.70Mood (MOD) 90.38 93.80Table 1: Accuracy (%) of feature prediction starting from Ara-bic lemmas (LEMMA).
The second column shows the baselinefor prediction using the most common feature value.
The thirdcolumn is the prediction accuracy using CRF.except that it starts with lemmas as opposed to in-flected forms (Habash and Rambow, 2005; Alkuh-lani and Habash, 2012).
As such, we expect it toperform worse than a comparable POS tagging task.For example, Habash and Rambow (2005) report98.2% and 98.8% for GEN and NUM, respectively,compared to our 84.2% and 87.3%.In the context of a specific application, the per-formance of the prediction could be improved us-ing information other than the context of providedLEMMAs.
For example, in MT, source language lex-ical, syntactic and morphological information couldbe used in the prediction module (El Kholy andHabash, 2012).The morphology prediction step produces a lat-tice with all the possible feature values each havingan associated confidence score.
We filter out optionswith very low confidence scores to control the expo-nential size of the lattice when combining more thanone feature.
We tried some experiments using onlyone or two top values but got lower performance.The morphology generation step takes the lattice anddecides on the best target inflection.Morphology Generation This step is imple-mented using a SMT model that translates from adeeper linguistic representation to a surface repre-sentation.
The model parameters are derived from aparallel corpus mapping LEMMAs plus morphologi-cal features to Arabic inflected forms.
The model ismonotonic and there is neither reordering nor worddeletion/addition.
We plan to consider these varia-tions in the future.
The main advantage of this ap-proach is that it only needs monolingual data whichis abundant.The morphology generation step can take a se-quence of LEMMAs and a subset of morphologicalfeatures directly or after enriching the sequence withone or more morphological features using the mor-phology prediction step.Detokenization Since we work on tokenized Ara-bic, we use a detokenization step which simplystitches the words and clitics together as a post-processing step after morphology generation.
Weuse the best detokenization technique presented byEl Kholy and Habash (2010).5 EvaluationEvaluation Setup All of the training data we useis available from the Linguistic Data Consortium(LDC).3 For SMT training and language modeling(LM), we use 200M words from the Arabic Giga-word corpus (LDC2007T40).
We use 5-grams forall LMs implemented using the SRILM toolkit (Stol-cke, 2002).MADA+TOKAN (Habash and Rambow, 2005;Habash et al, 2009) is used to preprocess theArabic text for generation and language modeling.MADA+TOKAN tokenizes, lemmatizes and selectsall morphological features in context.All generation experiments are conducted usingthe Moses phrase-based SMT system (Koehn et al,2007).
The decoding weight optimization is doneusing a set of 300 Arabic sentences from the 2004NIST MT evaluation test set (MT04).
The tuning isbased on tokenized Arabic without detokenization.We use a maximum phrase length of size 4.
Wereport results on the Arabic side of the 2005 NISTMT evaluation set (MT05), our development set.We use the Arabic side of MT06 NIST data set forblind test.
We evaluate using BLEU-1 and BLEU-4(Papineni et al, 2002).
BLEU is a precision-basedevaluation metric commonly used in MT research.Given the way we define our generation task to ex-clude reordering and word deletion/addition, BLEU-1 can be interpreted as a measure of word accuracy.BLEU-4 is the geometric mean of unigram, bigram,trigram and 4-gram precision.4 Since Arabic text3http://www.ldc.upenn.edu4n-gram precision is the number of test n-word sequencesthat appear in the reference divided by the number of all possi-ble n-word sequences in the test.92is generally written without diacritics, we evaluateon undiacritized text only.
In the future, we planto study generation into diacritized Arabic, a morechallenging goal.Baseline We conducted two baseline experiments.First, as a degenerate baseline, we only used deto-kenization to generate the inflected words fromLEMMAs.
Second, we used a generation step be-fore detokenization to generate the inflected tokensfrom LEMMAs.
The BLEU-1/BLEU-4 scores ofthe two baselines on the MT05 set are 55.04/24.51and 91.54/82.19.
We get a significant improvement(?35% BLEU-1 & ?58% BLEU-4) by using thegeneration step before detokenization.Generation with Gold Features We built severalSMT systems translating from LEMMAs plus one ormore morphological features to Arabic inflected to-kens.
We then use the detokenization step to recom-bine the tokens and produce the surface words.Table 2 shows the BLEU scores for MT05 set asLEMMAs plus different morphological features andtheir combinations.
We greedily combined the fea-tures based on the performance of each feature sep-arately.
Features with higher performance are com-bined first.
As expected, the more features are in-cluded the better the results.
Oddly, when we addthe POS to the feature combination, the performancedrops.
That could be explained by the redundancy ininformation provided by the POS given all the otherfeatures and the added sparsity.Although STT and MOD features hurt the per-formance when added incrementally to the featurecombination, removing them from the complete fea-ture set led to a drop in performance.
We suspectthat there are synergies in combining different fea-tures.
We plan to investigate this point extensivelyin the future.
BLEU scores are very high becausethe input is golden in terms of word order, lemmachoice and features.
These scores should be seen asthe upper limit of our model?s performance.
Most ofthe errors are detokenization and word form under-specification errors.Generation with Predicted Features We com-pare results of generation with a variety of predictedfeatures (see Table 3).
The results show that us-ing predicted features can help improve the gener-ation quality over the baseline in some cases, e.g.,Gold Generation Input BLEU-1% BLEU-4%LEMMA 91.54 82.19LEMMA+MOD 91.70 82.44LEMMA+ASP 92.09 83.26LEMMA+PER 92.09 83.34LEMMA+VOX 92.33 83.70LEMMA+CAS 92.71 84.34LEMMA+STT 93.92 86.55LEMMA+DET 93.97 86.62LEMMA+NUM 93.91 86.89LEMMA+GEN 94.33 87.32LEMMA+GEN+NUM 95.67 91.16++DET 97.88 95.76++STT 97.73 95.39++CAS 98.13 96.35++VOX 98.19 96.47++PER 98.24 96.59++ASP 98.85 98.08++MOD 98.85 98.06LEMMA + All Features + POS 98.82 98.01Table 2: Results of generation from gold Arabic lemmas(LEMMA) plus different sets of morphological features.
Resultsare in (BLEU-1 & BLEU-4) on our MT05 set.
?++?
means thefeature is added to the set of features in the previous row.when the LEMMAs are enriched with CAS, ASP,PER, VOX or MOD features.
Our best performer isLEMMA+MOD.
Unlike gold features, greedily com-bining predicted features hurts the performance andthe more features added the worse the performance.One explanation is that each feature is predicted in-dependently which may lead to incompatible featurevalues.
In the future, we plan to investigate waysof combining features that could help performancesuch predicting more than one feature together andfiltering out bad feature combinations.
The featureprediction accuracy (Table 1) does not always cor-relate with the generation performance, e.g., CAShas lower accuracy than GEN, but has a relativelyhigher BLEU score.
This may be due to the fact thatsome features are mostly realized as diacritics (CAS)which are ignored in our evaluation.Blind Test Set To validate our results, we ap-plied different versions of our system to a blindtest set (MT06).
Our results are as follows(BLEU-1/BLEU-4): detokenization without inflec-tion (55.64/24.92), generation from LEMMAs only(86.70/72.69), generation with gold MOD feature(87.00/73.31), and generation with predicted MODfeature (86.96/73.29).
These numbers are generally93Generation Input BLEU-1% BLEU-4%Baseline (LEMMA) 91.54 82.19LEMMA+GEN 89.23 78.37LEMMA+NUM 91.14 81.35LEMMA+STT 91.16 81.70LEMMA+DET 91.18 81.78LEMMA+CAS 91.60 82.43LEMMA+ASP 91.94 83.07LEMMA+PER 91.97 83.10LEMMA+VOX 91.99 83.18LEMMA+MOD 92.05 83.26LEMMA+MOD+VOX 91.76 82.73++PER 91.57 82.32++ASP 91.07 81.32++CAS 89.71 78.68Table 3: Results of generation from LEMMA plus different setsof predicted morphological features.
Results are in (BLEU-1 &BLEU-4) on our MT05 set.
?++?
means the feature is added tothe set of features in the previous row.lower than our development set, but the trends andconclusions are consistent.6 Conclusion and Future WorkWe present a SMT-based approach to generation ofmorphologically rich languages.
We evaluate ourapproach under a variety of settings for Arabic.
Inthe future, we plan to improve the quality of featureprediction and test our approach on other languages.ReferencesSarah Alkuhlani and Nizar Habash.
2011.
A Corpusfor Modeling Morpho-Syntactic Agreement in Arabic:Gender, Number and Rationality.
In Proc.
of ACL?11,Portland, OR.Sarah Alkuhlani and Nizar Habash.
2012.
IdentifyingBroken Plurals, Irregular Gender, and Rationality inArabic Text.
In Proc.
of EACL?12, Avignon, France.Ibrahim Badr, Rabih Zbib, and James Glass.
2008.
Seg-mentation for English-to-Arabic Statistical MachineTranslation.
In Proc.
of ACL?08, Columbus, OH.Ann Clifton and Anoop Sarkar.
2011.
Combin-ing morpheme-based machine translation with post-processing morpheme prediction.
In Proc.
of ACL?11,Portland, OR.Ahmed El Kholy and Nizar Habash.
2010.
Orthographicand Morphological Processing for English-Arabic Sta-tistical Machine Translation.
In Proc.
of TALN?10,Montre?al, Canada.Ahmed El Kholy and Nizar Habash.
2012.
Translate,Predict or Generate: Modeling Rich Morphology inStatistical Machine Translation.
In Proc.
of EAMT?12,Trento, Italy.Nizar Habash and Owen Rambow.
2005.
Arabic To-kenization, Part-of-Speech Tagging and Morphologi-cal Disambiguation in One Fell Swoop.
In Proc.
ofACL?05, Ann Arbor, MI.Nizar Habash, Owen Rambow, and Ryan Roth.
2009.MADA+TOKAN: A toolkit for Arabic tokenization,diacritization, morphological disambiguation, pos tag-ging, stemming and lemmatization.
Proc.
of MEDAR,Cairo, Egypt.Nizar Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Morgan & Claypool Publish-ers.Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-pher Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Christopher Dyer, Ondrej Bo-jar, Alexandra Constantin, and Evan Herbst.
2007.Moses: open source toolkit for statistical machinetranslation.
In Proc.
of ACL?07, Prague, Czech Re-public.J.
Lafferty, A. McCallum, and F.C.N.
Pereira.
2001.Conditional random fields: Probabilistic models forsegmenting and labeling sequence data.
In Proc.
ofthe International Conference on Machine Learning.Mohamed Maamouri, Ann Bies, Tim Buckwalter, andWigdan Mekki.
2004.
The Penn Arabic Treebank:Building a Large-Scale Annotated Arabic Corpus.
InProc.
of NEMLAR?04, Cairo, Egypt.Einat Minkov, Kristina Toutanova, and Hisami Suzuki.2007.
Generating complex morphology for machinetranslation.
In Proc.
of ACL?07, Prague, Czech Re-public.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for Automatic Eval-uation of Machine Translation.
In Proc.
of ACL?02,Philadelphia, PA.Chris Quirk, Chris Brockett, and William Dolan.
2004.Monolingual machine translation for paraphrase gen-eration.
In Dekang Lin and Dekai Wu, editors, Proc.of EMNLP?04, Barcelona, Spain.Andreas Stolcke.
2002.
SRILM - an Extensible Lan-guage Modeling Toolkit.
In Proc.
of ICSLP?02, Den-ver, CO.Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.2008.
Applying morphology generation models tomachine translation.
In Proc.
of ACL?08, Columbus,OH.Yuk Wah Wong and Raymond Mooney.
2007.
Gen-eration by inverting a semantic parser that uses sta-tistical machine translation.
In Proc.
of NAACL?07,Rochester, NY.94
