Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012?1017,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsFast and Accurate Preordering for SMT using Neural NetworksAdri`a de Gispert Gonzalo Iglesias Bill ByrneSDL ResearchEast Road, Cambridge CB1 1BH, U.K.{agispert|giglesias|bbyrne}@sdl.comAbstractWe propose the use of neural networks tomodel source-side preordering for faster andbetter statistical machine translation.
The neu-ral network trains a logistic regression modelto predict whether two sibling nodes of thesource-side parse tree should be swapped inorder to obtain a more monotonic parallelcorpus, based on samples extracted from theword-aligned parallel corpus.
For multiplelanguage pairs and domains, we show that thisyields the best reordering performance againstother state-of-the-art techniques, resulting inimproved translation quality and very fast de-coding.1 IntroductionPreordering is a pre-processing task in translationthat aims to reorder the source sentence so that itbest resembles the order of the target sentence.
Ifdone correctly, it has a doubly beneficial effect: itallows a better estimation of word alignment andtranslation models which results in higher transla-tion quality for distant language pairs, and it speedsup decoding enormously as less word movement isrequired.Preordering schemes can be automatically learntfrom source-side parsed, word-aligned parallel cor-pora.
Recently Jehl et al(2014) described a schemebased on a feature-rich logistic regression modelthat predicts whether a pair of sibling nodes in thesource-side dependency tree need to be permuted.Based on the node-pair swapping probability pre-dictions of this model, a branch-and-bound searchreturns the best ordering of nodes in the tree.We propose using a neural network (NN) to es-timate this node-swapping probability.
We find thatthis straightforward change to their scheme has mul-tiple advantages:1.
The superior modeling capabilities of NNsachieve better performance at preordering andoverall translation quality when using the sameset of features.2.
There is no need to manually define which featurecombinations are to be considered in training.3.
Preordering is even faster as a result of the previ-ous point.
Our results in translating from Englishto Japanese, Korean, Chinese, Arabic and Hindisupport these findings by comparing against twoother preordering schemes.1.1 Related WorkThere is a strong research and commercial in-terest in preordering, as reflected by the exten-sive previous work on the subject (Collins et al,2005; Xu et al, 2009; DeNero and Uszkor-eit, 2011; Neubig et al, 2012).
We are inter-ested in practical, language-independent preorder-ing approaches that rely only on automatic source-language parsers (Genzel, 2010).
The most recentwork in this area uses large-scale feature-rich dis-riminative models, effectively treating preorderingeither as a learning to rank (Yang et al, 2012), multi-classification (Lerner and Petrov, 2013) or logisticregression (Jehl et al, 2014) problem.
In this paperwe incorporate NNs into the latter approach.Lately an increasing body of work that uses NNsfor various NLP tasks has been published, includ-ing language modeling (Bengio et al, 2003), POS1012tagging (Collobert et al, 2011), or dependency pars-ing (Chen and Manning, 2014).
In translation, NNshave been used for improved word alignment (Yanget al, 2013; Tamura et al, 2014; Songyot and Chi-ang, 2014), to model reordering under an ITG gram-mar (Li et al, 2013), and to define additional featurefunctions to be used in decoding (Sundermeyer etal., 2014; Devlin et al, 2014).
End-to-end transla-tion systems based on NNs have also been proposed(Kalchbrenner and Blunsom, 2013; Sutskever et al,2014; Bahdanau et al, 2014).Despite the gains reported, only the approachesthat do not dramatically affect decoding times canbe directly applied to today?s commercial SMT sys-tems.
Our paper is a step towards this direction, andto the best of our knowledge, it is the first one todescribe the usage of NNs in preordering for SMT.2 Preordering as node-pair swappingJehl et al(2014) describe a preordering schemebased on a logistic regression model that predictswhether a pair of sibling nodes in the source-side de-pendency tree need to be permuted in order to havea more monotonically-aligned parallel corpus.
Theirmethod can be briefly summarised by the pseudo-code of Figure 1.LetN be the set of nodes in the source tree, and letCnbe the set of children nodes of node n. For eachnode with at least two children, first extract the nodefeatures (lines 1-2).
Then, for each pair of its chil-dren nodes: extract their respective features (lines4-5), produce all relevant feature combinations (line6), and store the node-pair swapping probability pre-dicted by a logistic regression model based on allavailable features (line 7).
Once all pair-wise proba-bilities are stored, search for the best global permu-tation and sort Cnaccordingly (lines 9-10).As features, Jehl et al(2014) use POS tags and de-pendency labels, as well as the identity and class ofthe head word (for the parent node) or the left/right-most word (for children nodes).
These are combinedinto conjunctions of 2 or 3 features to create newfeatures.
For logistic regression, they train a L1-regularised linear model using LIBLINEAR (Fanet al, 2008).
The training samples are either pos-itive/negative depending on whether swapping thenodes reduces/increases the number of crossed linksPREORDERPARSETREE1 for each node n ?
N, |Cn| > 12 F ?
GETFEATURES(n)3 for each pair of nodes i, j ?
Cn, i 6= j4 F ?
F ?
GETFEATURES(i)5 F ?
F ?
GETFEATURES(j)6 Fc?
FEATURECOMBINATIONS(F )7 pn(i, j) = LOGREGPREDICT(F, Fc)8 end for9 pin?
SEARCHPERMUTATION(pn)10 SORT(Cn, pin)Figure 1: Pseudocode for the preordering scheme of Jehlet al(2014)in the aligned parallel corpus.2.1 Applying Neural Networks?A (feedforward) neural network is a series of logis-tic regression models stacked on top of each other,with the final layer being either another logistic re-gression model or a linear regression model?
(Mur-phy, 2012).Given this, we propose a straightforward alterna-tive to the above framework: replace the linear logis-tic regression model by a neural network (NN).
Thisway a superior modeling performance of the node-swapping phenomenon is to be expected.
Addition-ally, feature combination need not be engineeredanymore because that is learnt by the NN in train-ing (line 6 in Figure 1 is skipped).Training the neural network requires the same la-beled samples that were used by Jehl et al(2014).We use the NPLM toolkit out-of-the-box (Vaswaniet al, 2013).
The architecture is a feed-forward neu-ral network (Bengio et al, 2003) with four layers.The first layer i contains the input embeddings.
Thenext two hidden layers (h1, h2) use rectified linearunits; the last one is the softmax layer (o).
We didnot experiment with deeper NNs.For our purposes, the input vocabulary of the NNis the set of all possible feature indicator namesthat are used for preordering1.
There are no OOVs.Given the sequence of ?
20 features seen by the1Using a vocabulary of the 5K top-frequency English words,50 word classes, approximately 40 POS tags and 50 depen-dency labels, the largest input vocabulary in our experimentsis roughly 30,000.1013preorderer, the NN is trained to predict whether thenodes should be reordered or not, i.e.
|o| = 2.
Forthe rest of the layers, we use |i| = 50, |h1| = 100,|h2| = 50.
We set the learning rate to 1, the mini-batch size to 64 and the number of epochs to 20.3 Experiments3.1 Data and setupWe report translation results in English intoJapanese, Korean, Chinese, Arabic and Hindi.
Foreach language pair, we use generic parallel data ex-tracted from the web.
The number of words is about100M for Japanese and Korean, 300M for Chinese,200M for Arabic and 9M for Hindi.We use two types of dev/test sets: in-domain andmix-domain.
The in-domain sets have 2K sentenceseach and were created by randomly extracting par-allel sentences from the corpus, ensuring no repe-titions remained.
The mix-domain sets have about1K sentences each and were created to evenly rep-resent 10 different domains, including world news,chat/SMS, health, sport, science, business, and oth-ers.Additionally, we report results on the English-to-Hindi WMT 2014 shared task (Bojar et al, 2014a)using the data provided2.
The dev and test setshave 520 and 2507 sentences each.
All dev andtest sets have one single reference.
We use SVM-Tool (Gim?enez and M`arquez, 2004) for POS Tag-ging, and MaltParser (Nivre et al, 2007) for depen-dency parsing.3.2 Intrinsic reordering evaluationWe evaluate the intrinsic preordering task on a ran-dom 5K-sentence subset of the training data which isexcluded from model estimation.
We report the nor-malized crossing score c/s, where c is the numberof crossing links (Genzel, 2010; Yang et al, 2012)in the aligned parallel corpus, and s is the numberof source (e.g.
English) words.
Ideally we wouldlike this metric to be zero, meaning a completelymonotonic parallel corpus3; the more monotonic the2HindEndCorp v0.5 (Bojar et al, 2014b)3However this may not be achievable given the alignmentlinks and parse tree available.
In this approach, only permuta-tions of sibling nodes in the single source parse tree are permit-ted.Figure 2: Normalized crossing score for English intoJapanese, Korean, Hindi, Chinese, Arabic, Spanish andPortuguese.corpus, the better the translation models will be andthe faster decoding will run as less distortion will beneeded.Normalizing over the number of source wordsallows us to compare this metric across languagepairs, and so the potential impact of preordering intranslation performance becomes apparent.
See Fig-ure 2 for results across several language pairs.
In allcases our proposed NN-based preorderer achievesthe lowest normalized crossing score among all pre-ordering schemes.3.3 Translation performanceFor translation experiments, we use a phrase-baseddecoder that incorporates a set of standard featuresand a hierarchical reordering model (Galley andManning, 2008).
The decoder stack size is set to1000.
Weights are tuned using MERT to optimizeBLEU on the dev set.
In English-to-Japanese andChinese we use character-BLEU instead.
To min-imise optimization noise, we tune all our systemsfrom flat parameters three times and report averageBLEU score and standard deviation on the test set.Table 1 contrasts the performance obtained bythe system when using no preordering capabilities(baseline), and when using three alternative pre-ordering schemes: the rule-based approach of Gen-zel (2010), the linear-model logistic-regression ap-proach of Jehl et al(2014) and our NN-based pre-orderer.
We report two baselines: one with distortionlimit d = 10 and another with d = 3.
For systems1014d system speed eng-jpn eng-korratio in mixed in mixed10 baseline 1x 54.5 ?0.2 26.2 ?0.2 33.5 ?0.3 9.7 ?0.23 baseline 3.2x 50.9 ?0.2 25.0 ?0.2 28.7 ?0.1 8.2 ?0.13 Genzel (2010) 2.7x 54.0 ?0.1 26.4 ?0.2 30.5 ?0.2 9.8 ?0.23 Jehl et al(2014) 2.3x 55.0 ?0.1 26.9 ?0.2 33.1 ?0.1 10.4 ?0.13 this work 2.7x 55.6 ?0.2 27.2 ?0.1 33.4 ?0.1 10.6 ?0.2d system eng-chi eng-ara eng-hinin mixed in mixed mixed wmt1410 baseline 46.9 ?0.5 18.4 ?0.6 25.1 ?0.1 22.7 ?0.2 10.1 ?0.3 11.7 ?0.13 baseline 44.8 ?0.7 18.3 ?0.4 24.6 ?0.1 21.9 ?0.2 8.3 ?0.2 9.3 ?0.33 Genzel (2010) 45.4 ?0.2 17.9 ?0.2 24.8 ?0.1 21.6 ?0.3 9.6 ?0.2 11.4 ?0.33 Jehl et al(2014) 45.8 ?0.1 18.5 ?0.3 25.1 ?0.2 22.4 ?0.2 10.0 ?0.1 12.7 ?0.33 this work 46.5 ?0.4 19.2 ?0.2 25.5 ?0.2 22.6 ?0.1 10.6 ?0.1 12.6 ?0.3best WMT14 constrained system 11.1Table 1: Translation performance for various language pairs using no preordering (baseline), and three alternativepreordering systems.
Average test BLEU score and standard deviation across 3 independent tuning runs.
Speed ratiois calculated with respect to the speed of the slower baseline that uses a d = 10.
Stack size is 1000.
For eng-jpn andeng-chi, character-based BLEU is used.with preordering we only report d = 3, as increasingd does not improve performance.As shown, our preorderer obtains the best BLEUscores for all reported languages and domains, prov-ing that the neural network is modeling the depen-dency tree node-swapping phenomenon more accu-rately, and that the reductions in crossing score re-ported in the previous section have a positive impactin the final translation performance.The bottom right-most column reports results onthe WMT 2014 English-to-Hindi task.
Our systemachieves better results than the best score reportedfor this task4.
In this case, the two logistic regressionpreorderers perform similarly, as standard deviationis higher, possibly due to the small size of the devset.3.4 Decoding SpeedThe main purpose of preordering is to find a bettertranslation performance in fast decoding conditions.In other words, by preordering the text we expect tobe able to decode with less distortion or phrase re-ordering, resulting in faster decoding.
This is shownin Table 1, which reports the speed ratio betweeneach system and the speed of the top-most baseline,4Details at matrix.statmt.org/matrix/systems list/1749as measured in English-to-Japanese in-domain5.
Wefind that decoding with a d = 3 is about 3 timesfaster than for d = 10.We now take this further by reducing the stacksize from 1000 to 50; see results in Table 2.
As ex-pected, all systems accelerate with respect to our ini-tial baseline.
However, this usually comes at a costin BLEU with respect to using a wider beam, unlesspreordering is used.
In fact, the logistic regressionpreorderers achieve the same performance while de-coding over 60 times faster than the baseline.Interestingly, the NN-based preorderer turns outto be slightly faster than any of the other preorderingapproaches.
This is because there is no need to ex-plicitly create thousands of feature combinations foreach node pair; simply performing the forward ma-trix multiplications given the input sequence of ?20features is more efficient.
Similar observations havebeen noted recently in the context of dependencyparsing with NNs (Chen and Manning, 2014).
Notealso that, on average, only 25 pair-wise probabili-ties are queried to the logistic regression model persource sentence.
Overall, we incorporate the bene-fits of neural networks for preordering at no compu-5Similar speed ratios were observed for other language pairs(not reported here for space)1015d system speed eng-jpn eng-kor eng-chiw/ stack=50 ratio in mixed in mixed mixed10 baseline 22x 53.6 (-0.9) 25.4 (-0.8) 32.8 (-0.7) 9.3 (-0.4) 17.9 (-0.5)3 baseline 66x 50.5 (-0.4) 24.8 (-0.2) 28.8 (+0.1) 8.1 (-0.1) 18.0 (-0.3)3 Genzel (2010) 64x 53.8 (-0.2) 26.3 (-0.1) 30.4 (-0.1) 9.8 (0.0) 18.1 (+0.2)3 Jehl et al(2014) 61x 55.0 (0.0) 26.5 (-0.4) 33.0 (-0.1) 10.4 (0.0) 18.3 (-0.2)3 this work 65x 55.7 (+0.1) 27.2 (0.0) 33.2 (-0.2) 10.8 (+0.2) 19.1 (-0.1)Table 2: Translation performance for maximum stack size of 50.
The figures in parentheses indicate the differencein BLEU scores due to using a smaller stack size, that is, compared to the same systems in Table 1.
Speed ratio iscalculated with respect to the speed of the slower baseline that uses a stack of 1000, eg.
the first row in Table 1.tational cost.Currently, our preorderer takes 6.3% of the to-tal decoding time (including 2.6% for parsing and3.7% for actually preordering).
We believe that fur-ther improvements in preordering will result in moretranslation gains and faster decoding, as the distor-tion limit is lowered.4 ConclusionsTo the best of our knowledge, this is the first paper todescribe the usage of NNs in preordering for SMT.We show that simply replacing the logistic regres-sion node-swapping model with an NN model im-proves both crossing scores and translation perfor-mance across various language pairs.
Feature com-bination engineering is avoided, which also resultsin even faster decoding times.ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
CoRR, abs/1409.0473.Yoshua Bengio, Rjean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Machine Learning Research, 3:1137?1155.Ondrej Bojar, Christian Buck, Christian Federmann,Barry Haddow, Philipp Koehn, Johannes Leveling,Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale?s Tam-chyna.
2014a.
Findings of the 2014 workshop onstatistical machine translation.
In Proceedings of theNinth Workshop on Statistical Machine Translation,pages 12?58.Ond?rej Bojar, Vojt?ech Diatka, Pavel Rychl?y, PavelStra?n?ak, V?
?t Suchomel, Ale?s Tamchyna, and DanielZeman.
2014b.
HindEnCorp - Hindi-English andHindi-only Corpus for Machine Translation.
In Pro-ceedings of LREC, pages 3550?3555.Danqi Chen and Christopher Manning.
2014.
A fast andaccurate dependency parser using neural networks.
InProceedings of EMNLP, pages 740?750.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause Restructuring for Statistical MachineTranslation.
In Proceedings of ACL, pages 531?540.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.Journal of Machine Learning Research, 12:2493?2537.John DeNero and Jakob Uszkoreit.
2011.
Inducing Sen-tence Structure from Parallel Corpora for Reordering.In Proceedings of EMNLP, pages 193?203.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and robust neural network joint models for sta-tistical machine translation.
In Proceedings of ACL,pages 1370?1380.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A Li-brary for Large Linear Classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Michel Galley and Christopher D. Manning.
2008.
ASimple and Effective Hierarchical Phrase ReorderingModel.
In Proceedings of EMNLP, pages 847?855.Dmitriy Genzel.
2010.
Automatically learning source-side reordering rules for large scale machine transla-tion.
In Proceedings of COLING, pages 376?384.Jes?us Gim?enez and Llu?
?s M`arquez.
2004.
SVMTool: Ageneral POS tagger generator based on Support VectorMachines.
In Proceedings of LREC, pages 43?46.Laura Jehl, Adri`a de Gispert, Mark Hopkins, and BillByrne.
2014.
Source-side preordering for translationusing logistic regression and depth-first branch-and-bound search.
In Proceedings of EACL, pages 239?248.1016Nal Kalchbrenner and Phil Blunsom.
2013.
Recur-rent continuous translation models.
In Proceedings ofEMNLP, pages 1700?1709.Uri Lerner and Slav Petrov.
2013.
Source-Side ClassifierPreordering for Machine Translation.
In Proceedingsof EMNLP, pages 513?523.Peng Li, Yang Liu, and Maosong Sun.
2013.
Recursiveautoencoders for ITG-based translation.
In Proceed-ings of EMNLP, pages 567?577.Kevin P. Murphy.
2012.
Machine Learning: A Proba-bilistic Perspective.
MIT Press, Cambridge, MA.Graham Neubig, Taro Watanabe, and Shinsuke Mori.2012.
Inducing a Discriminative Parser to OptimizeMachine Translation Reordering.
In Proceedings ofEMNLP-CoNLL, pages 843?853.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Glsen Eryigit, Sandra K?ubler, Svetoslav Marinov,and Erwin Marsi.
2007.
Maltparser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Theerawat Songyot and David Chiang.
2014.
Improvingword alignment using word similarity.
In Proceedingsof EMNLP, pages 1840?1845.Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,and Hermann Ney.
2014.
Translation modeling withbidirectional recurrent neural networks.
In Proceed-ings of EMNLP, pages 14?25.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural networks.CoRR, abs/1409.3215.Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.2014.
Recurrent neural networks for word alignmentmodel.
In Proceedings of ACL, pages 1470?1480.Ashish Vaswani, Yinggong Zhao, Victoria Fossum, andDavid Chiang.
2013.
Decoding with large-scale neu-ral language models improves translation.
In Proceed-ings of EMNLP, pages 1387?1392.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a Dependency Parser to ImproveSMT for Subject-Object-Verb Languages.
In Pro-ceedings of HTL-NAACL, pages 245?253.Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.2012.
A ranking-based approach to word reorderingfor statistical machine translation.
In Proceedings ofACL, pages 912?920.Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and NenghaiYu.
2013.
Word alignment modeling with context de-pendent deep neural network.
In Proceedings of ACL,pages 166?175.1017
