Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 280?287,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsGuiding Semi-Supervision with Constraint-Driven LearningMing-Wei Chang Lev Ratinov Dan RothDepartment of Computer ScienceUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801{mchang21, ratinov2, danr}@uiuc.eduAbstractOver the last few years, two of the mainresearch directions in machine learning ofnatural language processing have been thestudy of semi-supervised learning algo-rithms as a way to train classifiers when thelabeled data is scarce, and the study of waysto exploit knowledge and global informationin structured learning tasks.
In this paper,we suggest a method for incorporating do-main knowledge in semi-supervised learn-ing algorithms.
Our novel framework unifiesand can exploit several kinds of task specicconstraints.
The experimental results pre-sented in the information extraction domaindemonstrate that applying constraints helpsthe model to generate better feedback duringlearning, and hence the framework allowsfor high performance learning with signif-icantly less training data than was possiblebefore on these tasks.1 IntroductionNatural Language Processing (NLP) systems typi-cally require large amounts of knowledge to achievegood performance.
Acquiring labeled data is a dif-ficult and expensive task.
Therefore, an increasingattention has been recently given to semi-supervisedlearning, where large amounts of unlabeled data areused to improve the models learned from a smalltraining set (Collins and Singer, 1999; Thelen andRiloff, 2002).
The hope is that semi-supervised oreven unsupervised approaches, when given enoughknowledge about the structure of the problem, willbe competitive with the supervised models trainedon large training sets.
However, in the generalcase, semi-supervised approaches give mixed re-sults, and sometimes even degrade the model per-formance (Nigam et al, 2000).
In many cases, im-proving semi-supervised models was done by seed-ing these models with domain information takenfrom dictionaries or ontology (Cohen and Sarawagi,2004; Collins and Singer, 1999; Haghighi and Klein,2006; Thelen and Riloff, 2002).
On the other hand,in the supervised setting, it has been shown thatincorporating domain and problem specific struc-tured information can result in substantial improve-ments (Toutanova et al, 2005; Roth and Yih, 2005).This paper proposes a novel constraints-basedlearning protocol for guiding semi-supervised learn-ing.
We develop a formalism for constraints-basedlearning that unifies several kinds of constraints:unary, dictionary based and n-ary constraints, whichencode structural information and interdependenciesamong possible labels.
One advantage of our for-malism is that it allows capturing different levels ofconstraint violation.
Our protocol can be used inthe presence of any learning model, including thosethat acquire additional statistical constraints fromobserved data while learning (see Section 5.
In theexperimental part of this paper we use HMMs as theunderlying model, and exhibit significant reductionin the number of training examples required in twoinformation extraction problems.As is often the case in semi-supervised learning,the algorithm can be viewed as a process that im-proves the model by generating feedback through280labeling unlabeled examples.
Our algorithm pushesthis intuition further, in that the use of constraintsallows us to better exploit domain information as away to label, along with the current learned model,unlabeled examples.
Given a small amount of la-beled data and a large unlabeled pool, our frame-work initializes the model with the labeled data andthen repeatedly:(1) Uses constraints and the learned model to labelthe instances in the pool.
(2) Updates the model by newly labeled data.This way, we can generate better ?training?
ex-amples during the semi-supervised learning process.The core of our approach, (1), is described in Sec-tion 5.
The task is described in Section 3 and theExperimental study in Section 6.
It is shown therethat the improvement on the training examples viathe constraints indeed boosts the learned model andthe proposed method significantly outperforms thetraditional semi-supervised framework.2 Related WorkIn the semi-supervised domain there are two mainapproaches for injecting domain specific knowledge.One is using the prior knowledge to accurately tailorthe generative model so that it captures the domainstructure.
For example, (Grenager et al, 2005) pro-poses Diagonal Transition Models for sequential la-beling tasks where neighboring words tend to havethe same labels.
This is done by constraining theHMM transition matrix, which can be done also forother models, such as CRF.
However (Roth and Yih,2005) showed that reasoning with more expressive,non-sequential constraints can improve the perfor-mance for the supervised protocol.A second approach has been to use a small high-accuracy set of labeled tokens as a way to seed andbootstrap the semi-supervised learning.
This wasused, for example, by (Thelen and Riloff, 2002;Collins and Singer, 1999) in information extraction,and by (Smith and Eisner, 2005) in POS tagging.
(Haghighi and Klein, 2006) extends the dictionary-based approach to sequential labeling tasks by prop-agating the information given in the seeds with con-textual word similarity.
This follows a conceptuallysimilar approach by (Cohen and Sarawagi, 2004)that uses a large named-entity dictionary, where thesimilarity between the candidate named-entity andits matching prototype in the dictionary is encodedas a feature in a supervised classifier.In our framework, dictionary lookup approachesare viewed as unary constraints on the output states.We extend these kinds of constraints and allow formore general, n-ary constraints.In the supervised learning setting it has been es-tablished that incorporating global information cansignificantly improve performance on several NLPtasks, including information extraction and semanticrole labeling.
(Punyakanok et al, 2005; Toutanovaet al, 2005; Roth and Yih, 2005).
Our formalismis most related to this last work.
But, we develop asemi-supervised learning protocol based on this for-malism.
We also make use of soft constraints and,furthermore, extend the notion of soft constraints toaccount for multiple levels of constraints?
violation.Conceptually, although not technically, the most re-lated work to ours is (Shen et al, 2005) that, ina somewhat ad-hoc manner uses soft constraints toguide an unsupervised model that was crafted formention tracking.
To the best of our knowledge,we are the first to suggest a general semi-supervisedprotocol that is driven by soft constraints.We propose learning with constraints - a frame-work that combines the approaches described abovein a unified and intuitive way.3 Tasks, Examples and DatasetsIn Section 4 we will develop a general frameworkfor semi-supervised learning with constraints.
How-ever, it is useful to illustrate the ideas on concreteproblems.
Therefore, in this section, we give a briefintroduction to the two domains on which we testedour algorithms.
We study two information extrac-tion problems in each of which, given text, a set ofpre-defined fields is to be identified.
Since the fieldsare typically related and interdependent, these kindsof applications provide a good test case for an ap-proach like ours.1The first task is to identify fields from citations(McCallum et al, 2000) .
The data originally in-cluded 500 labeled references, and was later ex-tended with 5,000 unannotated citations collectedfrom papers found on the Internet (Grenager et al,2005).
Given a citation, the task is to extract the1The data for both problems is available at:http://www.stanford.edu/ grenager/data/unsupie.tgz281(a) [ AUTHOR Lars Ole Andersen . ]
[ TITLE Program analysis and specialization for the C programming language . ]
[TECH-REPORT PhD thesis , ] [ INSTITUTION DIKU , University of Copenhagen , ] [ DATE May 1994 .
](b) [ AUTHOR Lars Ole Andersen .
Program analysis and ] [TITLE specialization for the ] [EDITOR C ] [ BOOKTITLEProgramming language ] [ TECH-REPORT .
PhD thesis , ] [ INSTITUTION DIKU , University of Copenhagen , May ] [ DATE1994 .
]Figure 1: Error analysis of a HMM model.
The labels are annotated by underline and are to the right ofeach open bracket.
The correct assignment was shown in (a).
While the predicted label assignment (b) isgenerally coherent, some constraints are violated.
Most obviously, punctuation marks are ignored as cuesfor state transitions.
The constraint ?Fields cannot end with stop words (such as ?the?)?
may be also good.fields that appear in the given reference.
See Fig.
1.There are 13 possible fields including author, title,location, etc.To gain an insight to how the constraints can guidesemi-supervised learning, assume that the sentenceshown in Figure 1 appears in the unlabeled datapool.
Part (a) of the figure shows the correct la-beled assignment and part (b) shows the assignmentlabeled by a HMM trained on 30 labels.
However,if we apply the constraint that state transition canoccur only on punctuation marks, the same HMMmodel parameters will result in the correct labeling(a).
Therefore, by adding the improved labeled as-signment we can generate better training samplesduring semi-supervised learning.
In fact, the punc-tuation marks are only some of the constraints thatcan be applied to this problem.
The set of constraintswe used in our experiments appears in Table 1.
Notethat some of the constraints are non-local and arevery intuitive for people, yet it is very difficult toinject this knowledge into most models.The second problem we consider is extractingfields from advertisements (Grenager et al, 2005).The dataset consists of 8,767 advertisements forapartment rentals in the San Francisco Bay Areadownloaded in June 2004 from the Craigslist web-site.
In the dataset, only 302 entries have been la-beled with 12 fields, including size, rent, neighbor-hood, features, and so on.
The data was prepro-cessed using regular expressions for phone numbers,email addresses and URLs.
The list of the con-straints for this domain is given in Table 1.
We im-plement some global constraints and include unaryconstraints which were largely imported from thelist of seed words used in (Haghighi and Klein,2006).
We slightly modified the seedwords due todifference in preprocessing.4 Notation and DefinitionsConsider a structured classification problem, wheregiven an input sequence x = (x1, .
.
.
, xN ), the taskis to find the best assignment to the output variablesy = (y1, .
.
.
, yM ).
We denote X to be the space ofthe possible input sequences and Y to be the set ofpossible output sequences.We define a structured output classifier as a func-tion h : X ?
Y that uses a global scoring functionf : X ?Y ?
R to assign scores to each possible in-put/output pair.
Given an input x, a desired functionf will assign the correct output y the highest scoreamong all the possible outputs.
The global scoringfunction is often decomposed as a weighted sum offeature functions,f(x, y) =M?i=1?ifi(x, y) = ?
?
F (x, y).This decomposition applies both to discriminativelinear models and to generative models such asHMMs and CRFs, in which case the linear sumcorresponds to log likelihood assigned to the in-put/output pair by the model (for details see (Roth,1999) for the classification case and (Collins, 2002)for the structured case).
Even when not dictated bythe model, the feature functions fi(x, y) used arelocal to allow inference tractability.
Local featurefunction can capture some context for each input oroutput variable, yet it is very limited to allow dy-namic programming decoding during inference.Now, consider a scenario where we have a setof constraints C1, .
.
.
, CK .
We define a constraintC : X ?
Y ?
{0, 1} as a function that indicateswhether the input/output sequence violates some de-sired properties.
When the constraints are hard, thesolution is given byargmaxy?1C(x)?
?
F (x, y),282(a)-Citations1) Each field must be a consecutive list of words, and canappear at most once in a citation.2) State transitions must occur on punctuation marks.3) The citation can only start with author or editor.4) The words pp., pages correspond to PAGE.5) Four digits starting with 20xx and 19xx are DATE.6) Quotations can appear only in titles.7) The words note, submitted, appear are NOTE.8) The words CA, Australia, NY are LOCATION.9) The words tech, technical are TECH REPORT.10) The words proc, journal, proceedings, ACM are JOUR-NAL or BOOKTITLE.11) The words ed, editors correspond to EDITOR.
(b)-Advertisements1) State transitions can occur only on punctuation marks orthe newline symbol.2) Each field must be at least 3 words long.3) The words laundry, kitchen, parking are FEATURES.4) The words sq, ft, bdrm are SIZE.5) The word $, *MONEY* are RENT.6) The words close, near, shopping are NEIGHBORHOOD.7) The words laundry kitchen, parking are FEATURES.8) The (normalized) words phone, email are CONTACT.9) The words immediately, begin, cheaper are AVAILABLE.10) The words roommates, respectful, drama are ROOM-MATES.11) The words smoking, dogs, cats are RESTRICTIONS.12) The word http, image, link are PHOTOS.13) The words address, carlmont, st, cross are ADDRESS.14) The words utilities, pays, electricity are UTILITIES.Table 1: The list of constraints for extracting fieldsfrom citations and advertisements.
Some constraints(represented in the first block of each domain) areglobal and are relatively difficult to inject into tradi-tional models.
While all the constraints hold for thevast majority of the data, some of them are violatedby some correct labeled assignments.where 1C(x) is a subset of Y for which all Ci as-sign the value 1 for the given (x, y).When the constraints are soft, we want to in-cur some penalty for their violation.
Moreover, wewant to incorporate into our cost function a mea-sure for the amount of violation incurred by vi-olating the constraint.
A generic way to capturethis intuition is to introduce a distance functiond(y, 1Ci(x)) between the space of outputs that re-spect the constraint,1Ci(x), and the given output se-quence y.
One possible way to implement this dis-tance function is as the minimal Hamming distanceto a sequence that respects the constraint Ci, that is:d(y, 1Ci(x)) = min(y?
?1C(x)) H(y, y?).
If the penaltyfor violating the soft constraint Ci is ?i, we write thescore function as:argmaxy?
?
F (x, y) ?K?i=1?id(y, 1Ci(x)) (1)We refer to d(y, 1C(x)) as the valuation of theconstraint C on (x, y).
The intuition behind (1) is asfollows.
Instead of merely maximizing the model?slikelihood, we also want to bias the model usingsome knowledge.
The first term of (1) is used tolearn from data.
The second term biases the modeby using the knowledge encoded in the constraints.Note that we do not normalize our objective functionto be a true probability distribution.5 Learning and Inference with ConstraintsIn this section we present a new constraint-drivenlearning algorithm (CODL) for using constraints toguide semi-supervised learning.
The task is to learnthe parameter vector ?
by using the new objectivefunction (1).
While our formulation allows us totrain also the coefficients of the constraints valua-tion, ?i, we choose not to do it, since we view this asa way to bias (or enforce) the prior knowledge intothe learned model, rather than allowing the data tobrush it away.
Our experiments demonstrate that theproposed approach is robust to inaccurate approxi-mation of the prior knowledge (assigning the samepenalty to all the ?i ).We note that in the presence of constraints, theinference procedure (for finding the output y thatmaximizes the cost function) is usually done withsearch techniques (rather than Viterbi decoding,see (Toutanova et al, 2005; Roth and Yih, 2005) fora discussion), we chose beamsearch decoding.The semi-supervised learning with constraints isdone with an EM-like procedure.
We initialize themodel with traditional supervised learning (ignoringthe constraints) on a small labeled set.
Given an un-labeled set U , in the estimation step, the traditionalEM algorithm assigns a distribution over labeled as-signmentsY of each x ?
U , and in the maximizationstep, the set of model parameters is learned from thedistributions assigned in the estimation step.However, in the presence of constraints, assigningthe complete distributions in the estimation step isinfeasible since the constraints reshape the distribu-tion in an arbitrary way.
As in existing methods fortraining a model by maximizing a linear cost func-tion (maximize likelihood or discriminative maxi-283mization), the distribution over Y is represented asthe set of scores assigned to it; rather than consid-ering the score assigned to all y?s, we truncate thedistribution to the top K assignments as returnedby the search.
Given a set of K top assignmentsy1, .
.
.
, yK , we approximate the estimation step byassigning uniform probability to the top K candi-dates, and zero to the other output sequences.
Wedenote this algorithm top-K hard EM.
In this pa-per, we use beamsearch to generate K candidatesaccording to (1).Our training algorithm is summarized in Figure 2.Several things about the algorithm should be clari-fied: the Top-K-Inference procedure in line 7, thelearning procedure in line 9, and the new parameterestimation in line 9.The Top-K-Inference is a procedure that returnsthe K labeled assignments that maximize the newobjective function (1).
In our case we used the top-K elements in the beam, but this could be appliedto any other inference procedure.
The fact that theconstraints are used in the inference procedure (inparticular, for generating new training examples) al-lows us to use a learning algorithm that ignores theconstraints, which is a lot more efficient (althoughalgorithms that do take the constraints into accountcan be used too).
We used maximum likelihood es-timation of ?
but, in general, perceptron or quasi-Newton can also be used.It is known that traditional semi-supervised train-ing can degrade the learned model?s performance.
(Nigam et al, 2000) has suggested to balance thecontribution of labeled and unlabeled data to the pa-rameters.
The intuition is that when iteratively esti-mating the parameters with EM, we disallow the pa-rameters to drift too far from the supervised model.The parameter re-estimation in line 9, uses a similarintuition, but instead of weighting data instances, weintroduced a smoothing parameter ?
which controlsthe convex combination of models induced by the la-beled and the unlabeled data.
Unlike the techniquementioned above which focuses on naive Bayes, ourmethod allows us to weight linear models generatedby different learning algorithms.Another way to look the algorithm is from theself-training perspective (McClosky et al, 2006).Similarly to self-training, we use the current modelto generate new training examples from the unla-Input:Cycles: learning cyclesTr = {x, y}: labeled training set.U : unlabeled datasetF : set of feature functions.
{?i}: set of penalties.
{Ci}: set of constraints.?
: balancing parameter with the supervised model.learn(Tr, F ): supervised learning algorithmTop-K-Inference:returns top-K labeled scored by the cost function (1)CODL:1.
Initialize ?0 = learn(Tr, F ).2. ?
= ?0.3.
For Cycles iterations do:4.
T = ?5.
For each x ?
U6.
{(x, y1), .
.
.
, (x, yK)} =7.
Top-K-Inference(x, ?, F, {Ci}, {?i})8.
T = T ?
{(x, y1), .
.
.
, (x, yK)}9. ?
= ?
?0 + (1 ?
?
)learn(T, F )Figure 2: COnstraint Driven Learning (CODL).
InTop-K-Inference, we use beamsearch to find the K-best solution according to Eq.
(1).beled set.
However, there are two important differ-ences.
One is that in self-training, once an unlabeledsample was labeled, it is never labeled again.
Inour case all the samples are relabeled in each iter-ation.
In self-training it is often the case that onlyhigh-confidence samples are added to the labeleddata pool.
While we include all the samples in thetraining pool, we could also limit ourselves to thehigh-confidence samples.
The second difference isthat each unlabeled example generates K labeled in-stances.
The case of one iteration of top-1 hard EMis equivalent to self training, where all the unlabeledsamples are added to the labeled pool.There are several possible benefits to using K > 1samples.
(1) It effectively increases the training setby a factor of K (albeit by somewhat noisy exam-ples).
In the structured scenario, each of the top-Kassignments is likely to have some good componentsso generating top-K assignments helps leveragingthe noise.
(2) Given an assignment that does not sat-isfy some constraints, using top-K allows for mul-tiple ways to correct it.
For example, consider theoutput 11101000 with the constraint that it shouldbelong to the language 1?0?.
If the two top scoringcorrections are 11111000 and 11100000, consider-ing only one of those can negatively bias the model.2846 Experiments and ResultsIn this section, we present empirical results of ouralgorithms on two domains: citations and adver-tisements.
Both problems are modeled with a sim-ple token-based HMM.
We stress that token-basedHMM cannot represent many of our constraints.
Thefunction d(y, 1C(x)) used is an approximation of aHamming distance function, discussed in Section 7.For both domains, and all the experiments, ?
wasset to 0.1.
The constraints violation penalty ?
is setto ?
log 10?4 and ?
log 10?1 for citations and ad-vertisements, resp.2 Note that all constraints sharethe same penalty.
The number of semi-supervisedtraining cycles (line 3 of Figure 2) was set to 5.
Theconstraints for the two domains are listed in Table 1.We trained models on training sets of size vary-ing from 5 to 300 for the citations and from 5 to100 for the advertisements.
Additionally, in all thesemi-supervised experiments, 1000 unlabeled exam-ples are used.
We report token-based3 accuracy on100 held-out examples (which do not overlap neitherwith the training nor with the unlabeled data).
Weran 5 experiments in each setting, randomly choos-ing the training set.
The results reported below arethe averages over these 5 runs.To verify our claims we implemented severalbaselines.
The first baseline is the supervised learn-ing protocol denoted by sup.
The second baselinewas a traditional top-1 Hard EM also known astruncated EM4 (denoted by H for Hard).
In the thirdbaseline, denoted H&W, we balanced the weightof the supervised and unsupervised models as de-scribed in line 9 of Figure 2.
We compare these base-lines to our proposed protocol, H&W&C, where weadded the constraints to guide the H&W protocol.We experimented with two flavors of the algorithm:the top-1 and the top-K version.
In the top-K ver-sion, the algorithm uses K-best predictions (K=50)for each instance in order to update the model as de-scribed in Figure 2.The experimental results for both domains are ingiven Table 2.
As hypothesized, hard EM sometimes2The guiding intuition is that ?F (x, y) corresponds to a log-likelihood of a HMM model and ?
to a crude estimation of thelog probability that a constraint does not hold.
?
was tuned ona development set and kept fixed in all experiments.3Each token (word or punctuation mark) is assigned a state.4We also experimented with (soft) EM without constraints,but the results were generally worse.
(a)- CitationsN Inf.
sup.
H H&W H&W&C H&W&C(Top-1) (Top-K)5 no I 55.1 60.9 63.6 70.6 71.0I 66.6 69.0 72.5 76.0 77.810 no I 64.6 66.8 69.8 76.5 76.7I 78.1 78.1 81.0 83.4 83.815 no I 68.7 70.6 73.7 78.6 79.4I 81.3 81.9 84.1 85.5 86.220 no I 70.1 72.4 75.0 79.6 79.4I 81.1 82.4 84.0 86.1 86.125 no I 72.7 73.2 77.0 81.6 82.0I 84.3 84.2 86.2 87.4 87.6300 no I 86.1 80.7 87.1 88.2 88.2I 92.5 89.6 93.4 93.6 93.5(b)-AdvertisementsN Inf.
sup.
H H&W H&W&C H&W&C(Top-1) (Top-K)5 no I 55.2 61.8 60.5 66.0 66.0I 59.4 65.2 63.6 69.3 69.610 no I 61.6 69.2 67.0 70.8 70.9I 66.6 73.2 71.6 74.7 74.715 no I 66.3 71.7 70.1 73.0 73.0I 70.4 75.6 74.5 76.6 76.920 no I 68.1 72.8 72.0 74.5 74.6I 71.9 76.7 75.7 77.9 78.125 no I 70.0 73.8 73.0 74.9 74.8I 73.7 77.7 76.6 78.4 78.5100 no I 76.3 76.2 77.6 78.5 78.6I 80.4 80.5 81.2 81.8 81.7Table 2: Experimental results for extracting fieldsfrom citations and advertisements.
N is the numberof labeled samples.
H is the traditional hard-EM andH&W weighs labeled and unlabeled data as men-tioned in Sec.
5.
Our proposed model is H&W&C,which uses constraints in the learning procedure.
Irefers to using constraints during inference at eval-uation time.
Note that adding constraints improvesthe accuracy during both learning and inference.degrade the performance.
Indeed, with 300 labeledexamples in the citations domain, the performancedecreases from 86.1 to 80.7.
The usefulness of in-jecting constraints in semi-supervised learning is ex-hibited in the two right most columns: using con-straints H&W&C improves the performance overH&W quite significantly.We carefully examined the contribution of us-ing constraints to the learning stage and the testingstage, and two separate results are presented: test-ing with constraints (denoted I for inference) andwithout constraints (no I).
The I results are consis-tently better.
And, it is also clear from Table 2,that using constraints in training always improves285the model and the amount of improvement dependson the amount of labeled data.Figure 3 compares two protocols on the adver-tisements domain: H&W+I, where we first run theH&W protocol and then apply the constraints dur-ing testing stage, and H&W&C+I, which uses con-straints to guide the model during learning and usesit also in testing.
Although injecting constraints inthe learning process helps, testing with constraints ismore important than using constraints during learn-ing, especially when the labeled data size is large.This confirms results reported for the supervisedlearning case in (Punyakanok et al, 2005; Roth andYih, 2005).
However, as shown, our proposed al-gorithm H&W&C for training with constraints iscritical when the amount labeled data is small.Figure 4 further strengthens this point.
In the cita-tions domain, H&W&C+I achieves with 20 labeledsamples similar performance to the supervised ver-sion without constraints with 300 labeled samples.
(Grenager et al, 2005) and (Haghighi and Klein,2006) also report results for semi-supervised learn-ing for these domains.
However, due to differ-ent preprocessing, the comparison is not straight-forward.
For the citation domain, when 20 labeledand 300 unlabeled samples are available, (Grenageret al, 2005) observed an increase from 65.2% to71.3%.
Our improvement is from 70.1% to 79.4%.For the advertisement domain, they observed no im-provement, while our model improves from 68.1%to 74.6% with 20 labeled samples.
Moreover, wesuccessfully use out-of-domain data (web data) toimprove our model, while they report that this datadid not improve their unsupervised model.
(Haghighi and Klein, 2006) also worked on one ofour data sets.
Their underlying model, Markov Ran-dom Fields, allows more expressive features.
Nev-ertheless, when they use only unary constraints theyget 53.75%.
When they use their final model, alongwith a mechanism for extending the prototypes toother tokens, they get results that are comparable toour model with 10 labeled examples.
Additionally,in their framework, it is not clear how to use smallamounts of labeled data when available.
Our modeloutperforms theirs once we add 10 more examples.0.650.70.750.80.85100252015105H+N+IH+N+C+IFigure 3: Comparison between H&W+I andH&W&C+I on the advertisements domain.
Whenthere is a lot of labeled data, inference with con-straints is more important than using constraints dur-ing learning.
However, it is important to train withconstraints when the amount of labeled data is small.0.70.750.80.850.90.95100252015105sup.
(300)H+N+C+IFigure 4: With 20 labeled citations, our algorithmperforms competitively to the supervised versiontrained on 300 samples.7 Soft ConstraintsThis section discusses the importance of using softconstraints rather than hard constraints, the choiceof Hamming distance for d(y, 1C(x)) and how weapproximate it.
We use two constraints to illustratethe ideas.
(C1): ?state transitions can only occur onpunctuation marks or newlines?, and (C2): ?the fieldTITLE must appear?.First, we claim that defining d(y, 1C(x)) to bethe Hamming distance is superior to using a binaryvalue, d(y, 1C(x)) = 0 if y ?
1C(x) and 1 other-wise.
Consider, for example, the constraint C1 inthe advertisements domain.
While the vast majorityof the instances satisfy the constraint, some violateit in more than one place.
Therefore, once the binarydistance is set to 1, the algorithm looses the ability todiscriminate constraint violations in other locations286of the same instance.
This may hurt the performancein both the inference and the learning stage.Computing the Hamming distance exactly canbe a computationally hard problem.
Further-more, it is unreasonable to implement the ex-act computation for each constraint.
Therefore,we implemented a generic approximation for thehamming distance assuming only that we aregiven a boolean function ?C(yN ) that returnswhether labeling the token xN with state yN vio-lates constraint with respect to an already labeledsequence (x1, .
.
.
, xN?1, y1, .
.
.
, yN?1).
Thend(y, 1C(x)) =?Ni=1 ?C(yi).
For example,consider the prefix x1, x2, x3, x4, which con-tains no punctuation or newlines and was labeledAUTH, AUTH, DATE, DATE.
This labelingviolates C1, the minimal hamming distance is 2, andour approximation gives 1, (since there is only onetransition that violates the constraint.
)For constraints which cannot be validated basedon prefix information, our approximation resorts tobinary violation count.
For instance, the constraintC2 cannot be implemented with prefix informationwhen the assignment is not complete.
Otherwise, itwould mean that the field TITLE should appear asearly as possible in the assignment.While (Roth and Yih, 2005) showed the signif-icance of using hard constraints, our experimentsshow that using soft constraints is a superior op-tion.
For example, in the advertisements domain,C1 holds for the large majority of the gold-labeledinstances, but is sometimes violated.
In supervisedtraining with 100 labeled examples on this domain,sup gave 76.3% accuracy.
When the constraint vio-lation penalty ?
was innity (equivalent to hard con-straint), the accuracy improved to 78.7%, but whenthe penalty was set to ?log(0.1), the accuracy of themodel jumped to 80.6%.8 Conclusions and Future WorkWe proposed to use constraints as a way to guidesemi-supervised learning.
The framework devel-oped is general both in terms of the representationand expressiveness of the constraints, and in termsof the underlying model being learned ?
HMM inthe current implementation.
Moreover, our frame-work is a useful tool when the domain knowledgecannot be expressed by the model.The results show that constraints improve notonly the performance of the final inference stage butalso propagate useful information during the semi-supervised learning process and that training withthe constraints is especially significant when thenumber of labeled training data is small.Acknowledgments: This work is supported by NSF SoD-HCER-0613885 and by a grant from Boeing.
Part of this workwas done while Dan Roth visited the Technion, Israel, sup-ported by a Lady Davis Fellowship.ReferencesW.
Cohen and S. Sarawagi.
2004.
Exploiting dictionaries innamed entity extraction: Combining semi-markov extractionprocesses and data integration methods.
In Proc.
of the ACMSIGKDD.M.
Collins and Y.
Singer.
1999.
Unsupervised models fornamed entity classification.
In Proc.
of EMNLP.M.
Collins.
2002.
Discriminative training methods for hiddenMarkov models: Theory and experiments with perceptronalgorithms.
In Proc.
of EMNLP.T.
Grenager, D. Klein, and C. Manning.
2005.
Unsupervisedlearning of field segmentation models for information extrac-tion.
In Proc.
of the Annual Meeting of the ACL.A.
Haghighi and D. Klein.
2006.
Prototype-driven learning forsequence models.
In Proc.
of HTL-NAACL.A.
McCallum, D. Freitag, and F. Pereira.
2000.
Maximumentropy markov models for information extraction and seg-mentation.
In Proc.
of ICML.D.
McClosky, E. Charniak, and M. Johnson.
2006.
Effectiveself-training for parsing.
In Proceedings of HLT-NAACL.K.
Nigam, A. McCallum, S. Thrun, and T. Mitchell.
2000.
Textclassification from labeled and unlabeled documents usingEM.
Machine Learning, 39(2/3):103?134.V.
Punyakanok, D. Roth, W. Yih, and D. Zimak.
2005.
Learn-ing and inference over constrained output.
In Proc.
of IJCAI.D.
Roth and W. Yih.
2005.
Integer linear programming infer-ence for conditional random fields.
In Proc.
of ICML.D.
Roth.
1999.
Learning in natural language.
In Proc.
of IJCAI,pages 898?904.W.
Shen, X. Li, and A. Doan.
2005.
Constraint-based entitymatching.
In Proc.
of AAAI).N.
Smith and J. Eisner.
2005.
Contrastive estimation: Traininglog-linear models on unlabeled data.
In Proc.
of the AnnualMeeting of the ACL.M.
Thelen and E. Riloff.
2002.
A bootstrapping method forlearning semantic lexicons using extraction pattern contexts.In Proc.
of EMNLP.K.
Toutanova, A. Haghighi, and C. D. Manning.
2005.
Jointlearning improves semantic role labeling.
In Proc.
of theAnnual Meeting of the ACL.287
