Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 365?372,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsQuality Estimation for Machine Translation Using the Joint Methodof Evaluation Criteria and Statistical ModelingAaron Li-Feng Hanhanlifengaaron@gmail.comYi Lumb25435@umac.moDerek F. Wongderekfw@umac.moLidia S. Chaolidiasc@umac.moLiangye Hewutianshui0515@gmail.comJunwen Xingmb15470@umac.moNatural Language Processing & Portuguese-Chinese Machine Translation LaboratoryDepartment of Computer and Information ScienceUniversity of Macau, Macau S.A.R.
ChinaAbstractThis paper is to introduce our participation inthe WMT13 shared tasks on Quality Estima-tion for machine translation without using ref-erence translations.
We submitted the resultsfor Task 1.1 (sentence-level quality estima-tion), Task 1.2 (system selection) and Task 2(word-level quality estimation).
In Task 1.1,we used an enhanced version of BLEU metricwithout using reference translations to evalu-ate the translation quality.
In Task 1.2, we uti-lized a probability model Na?ve Bayes (NB) asa classification algorithm with the featuresborrowed from the traditional evaluation met-rics.
In Task 2, to take the contextual infor-mation into account, we employed a discrimi-native undirected probabilistic graphical mod-el Conditional random field (CRF), in additionto the NB algorithm.
The training experimentson the past WMT corpora showed that the de-signed methods of this paper yielded promis-ing results especially the statistical models ofCRF and NB.
The official results show thatour CRF model achieved the highest F-score0.8297 in binary classification of Task 2.1 IntroductionDue to the fast development of Machine transla-tion, different automatic evaluation methods forthe translation quality have been proposed in re-cent years.
One of the categories is the lexicalsimilarity based metric.
This kind of metrics in-cludes the edit distance based method, such asWER (Su et al 1992), Multi-reference WER(Nie?en et al 2000), PER (Tillmann et al1997), the works of (Akiba, et al 2001),(Leusch et al 2006) and (Wang and Manning,2012); the precision based method, such asBLEU (Papineni et al 2002), NIST (Doddington,2002), and SIA (Liu and Gildea, 2006); recallbased method, such as ROUGE (Lin and Hovy2003); and the combination of precision and re-call, such as GTM (Turian et al 2003), METE-OR (Lavie and Agarwal, 2007), BLANC (Lita etal., 2005), AMBER (Chen and Kuhn, 2011),PORT (Chen et al 2012b), and LEPOR (Han etal., 2012).Another category is the using of linguistic fea-tures.
This kind of metrics includes the syntacticsimilarity, such as the POS information used byTESLA (Dahlmeier et al 2011), (Liu et al2010) and (Han et al 2013), phrase informationused by (Povlsen, et al 1998) and (Echizen-yaand Araki, 2010), sentence structure used by(Owczarzak et al 2007); the semantic similarity,such as textual entailment used by (Mirkin et al2009) and (Castillo and Estrella, 2012), Syno-nyms used by METEOR (Lavie and Agarwal,2007), (Wong and Kit, 2012), (Chan and Ng,2008); paraphrase used by (Snover et al 2009).The traditional evaluation metrics tend toevaluate the hypothesis translation as comparedto the reference translations that are usually of-fered by human efforts.
However, in the practice,there is usually no golden reference for the trans-lated documents, especially on the internet works.How to evaluate the quality of automaticallytranslated documents or sentences without usingthe reference translations becomes a new chal-lenge in front of the NLP researchers.365ADJ ADP ADV CONJ DET NOUN NUM PRON PRT VERB X .ADJ PREP,PREP/DELADV,NEGCC,CCAD,CCNEG,CQUE,CSUBF,CSUBI,CSUBXART NC,NMEA,NMON,NP,PERCT,UMMXCARD,CODE,QUDM,INT,PPC,PPO,PPX,RELSE VCLIger,VCLIinf,VCLIfin,VEadj,VEfin,VEger,VEinf,VHadj,VHfin,VHger,VHinf,VLadj,VLfin,VLger,VLinf,VMadj,VMfin,VMger,VMinf,VSadj,VSfin,VSger,VSinfACRNM,ALFP,ALFS,FO, ITJN,ORD,PAL,PDEL,PE, PNC,SYMBACKSLASH,CM, COLON,DASH, DOTS,FS, LP, QT,RP, SEMICO-LON, SLASHTable 1: Developed POS mapping for Spanish and universal tagset2 Related WorksGamon et al(2005) perform a research aboutreference-free SMT evaluation method on sen-tence level.
This work uses both linear and non-linear combinations of language model and SVMclassifier to find the badly translated sentences.Albrecht and Hwa (2007) conduct the sentence-level MT evaluation utilizing the regressionlearning and based on a set of weaker indicatorsof fluency and adequacy as pseudo references.Specia and Gimenez (2010) use the ConfidenceEstimation features and a learning mechanismtrained on human annotations.
They show thatthe developed models are highly biased by diffi-culty level of the input segment, therefore theyare not appropriate for comparing multiple sys-tems that translate the same input segments.
Spe-cia et al(2010) discussed the issues between thetraditional machine translation evaluation and thequality estimation tasks recently proposed.
Thetraditional MT evaluation metrics require refer-ence translations in order to measure a score re-flecting some aspects of its quality, e.g.
theBLEU and NIST.
The quality estimation ad-dresses this problem by evaluating the quality oftranslations as a prediction task and the featuresare usually extracted from the source sentencesand target (translated) sentences.
They also showthat the developed methods correlate better withhuman judgments at segment level as comparedto traditional metrics.
Popovi?
et al(2011) per-form the MT evaluation using the IBM modelone with the information of morphemes, 4-gramPOS and lexicon probabilities.
Mehdad et al(2012) use the cross-lingual textual entailment topush semantics into the MT evaluation withoutusing reference translations.
This evaluationwork mainly focuses on the adequacy estimation.Avramidis (2012) performs an automatic sen-tence-level ranking of multiple machine transla-tions using the features of verbs, nouns, sentenc-es, subordinate clauses and punctuation occur-rences to derive the adequacy information.
Otherdescriptions of the MT Quality Estimation taskscan be gained in the works of (Callison-Burch etal., 2012) and (Felice and Specia, 2012).3 Tasks InformationThis section introduces the different sub-tasks weparticipated in the Quality Estimation task ofWMT 13 and the methods we used.3.1 Task 1-1 Sentence-level QETask 1.1 is to score and rank the post-editingeffort of the automatically translated English-Spanish sentences without offering the referencetranslation.Firstly, we develop the English and SpanishPOS tagset mapping as shown in Table 1.
The 75Spanish POS tags yielded by the Treetagger(Schmid, 1994) are mapped to the 12 universaltags developed in (Petrov et al 2012).
The Eng-lish POS tags are extracted from the parsed sen-tences using the Berkeley parser (Petrov et al2006).Secondly, the enhanced version of BLEU(EBLEU) formula is designed with the factors ofmodified length penalty (   ), precision, andrecall, the   and   representing the lengths ofhypothesis (target) sentence and source sentencerespectively.
We use the harmonic mean of pre-cision and recall, i.e.
(       ).
We assignthe weight values     and    , i.e.
higherweight value is assigned to precision, which isdifferent with METEOR (the inverse values).(?
( (       ))) (1){(2)(3)(4)366Lastly, the scoring for the post-editing effortof the automatically translated sentences is per-formed on the extracted POS sequences of thesource and target languages.
The evaluated per-formance of EBLEU on WMT 12 corpus isshown in Table 2 using the Mean-Average-Error(MAE), Root-Mean-Squared-Error (RMSE).Precision Recall MLP EBLEUMAE 0.17 0.19 0.25 0.16RMSE 0.22 0.24 0.30 0.21Table 2: Performance on the WMT12 corpusThe official evaluation scores of the testing re-sults on WMT 13 corpus are shown in Table 3.The testing results show similar scores as com-pared to the training scores (the MAE score isaround 0.16 and the RMSE score is around 0.22),which shows a stable performance of the devel-oped model EBLEU.
However, the performanceof EBLEU is not satisfactory currently as shownin the Table 2 and Table 3.
This is due to the factthat we only used the POS information as lin-guistic feature.
This could be further improvedby the combination of lexical information andother linguistic features such as the sentencestructure, phrase similarity, and text entailment.MAE RMSE DeltaAvgSpearmanCorrEBLEU 16.97 21.94 2.74 0.11BaselineSVM14.81 18.22 8.52 0.46Table 3: Performance on the WMT13 corpus3.2 Task 1-2 System SelectionTask 1.2 is the system selection task on EN-ESand DE-EN language pairs.
Participants are re-quired to rank up to five alternative translationsfor the same source sentence produced by multi-ple translation systems.Firstly, we describe the two variants ofEBLEU method for this task.
We score the fivealternative translation sentences as compared tothe source sentence according to the closeness oftheir POS sequences.
The German POS is alsoextracted using Berkeley parser (Petrov et al2006).
The mapping of German POS to universalPOS tagset is using the developed one in thework of (Petrov et al 2012).
When we convertthe absolute scores into the corresponding rankvalues, the variant EBLEU-I means that we usefive fixed intervals (with the span from 0 to 1) toachieve the alignment as shown in Table 4.
[1,0.4) [0.4, 0.3) [0.3, 0.25) [0.25, 0.2) [0.2, 0]5 4 3 2 1Table 4: Convert absolute scores into ranksThe alignment work from absolute scores torank values shown in Table 4 is empirically de-termined.
We have made a statistical work on theabsolute scores yielded by our metrics, and eachof the intervals shown in Table 4 covers the simi-lar number of sentence scores.On the other hand, in the metric EBLEU-A,?A?
means average.
The absolute sentence editscores are converted into the five rank valueswith the same number (average number).
Forinstance, if there are 1000 sentence scores in to-tal then each rank level (from 1 to 5) will gain200 scores from the best to the worst.Secondly, we introduce the NB-LPR modelused in this task.
NB-LPR means the Na?veBayes classification algorithm using the featuresof Length penalty (introduced in previous sec-tion), Precision, Recall and Rank values.
NB-LPR considers each of its features independently.Let?s see the conditional probability that is alsoknown as Bayes?
rule.
If the  ( | )  is given,then the  ( | ) can be calculated as follows:( | )( | ) ( )( )(5)Given a data point identified as(          ) and the classifications(          ), Bayes?
rule can be applied tothis statement:(  |          )(         |  ) (  )(         )(6)As in many practical applications, parameterestimation for NB-LPR model uses the methodof maximum likelihood.
For details of Na?veBayes algorithm, see the works of (Zhang, 2004)and (Harrington, 2012).Thirdly, the SVM-LPR model means the sup-port vector machine classification algorithm us-ing the features of Length penalty, Precision,Recall and Rank values, i.e.
the same features asin NB-LPR.
SVM solves the nonlinear classifica-tion problem by mapping the data from a lowdimensional space to a high dimensional spaceusing the Kernel methods.
In the projected highdimensional space, the problem usually becomesa linear one, which is easier to solve.
SVM isalso called maximum interval classifier becauseit tries to find the optimized hyper plane that367separates different classes with the largest mar-gin, which is usually a quadratic optimizationproblem.
Let?s see the formula below, we shouldfind the points with the smallest margin to thehyper plane and then maximize this margin.
{    (      ())?
?
}(7)where   is normal to the hyper plane, || || isthe Euclidean norm of  , and | | || ||  is theperpendicular distance from the hyper plane tothe origin.
For details of SVM, see the works of(Cortes and Vapnik, 1995) and (Burges, 1998).EN-ESNB-LPR SVM-LPRMAE RMSE Time MAE RMSE Time.315 .399 .40s .304 .551 60.67sDE-ENNB-LPR SVM-LPRMAE RMSE Time MAE RMSE Time.318 .401 .79s .312 .559 111.7sTable 5: NB-LPR and SVM-LPR trainingIn the training stage, we used all the officiallyreleased data of WMT 09, 10, 11 and 12 for theEN-ES and DE-EN language pairs.
We used theWEKA (Hall et al 2009) data mining softwareto implement the NB and SVM algorithms.
Thetraining scores are shown in Table 5.
The NB-LPR performs lower scores than the SVM-LPRbut faster than SVM-LPR.DE-EN EN-ESMethodsTau(tiespenalized)|Tau|(tiesignored)Tau(tiespenalized)|Tau|(tiesignored)EBLEU-I -0.38 -0.03 -0.35 0.02EBLEU-A N/A N/A -0.27 N/ANB-LPR -0.49 0.01 N/A 0.07Baseline  -0.12 0.08 -0.23 0.03Table 6: QE Task 1.2 testing scoresThe official testing scores are shown in Table6.
Each task is allowed to submit up to two sys-tems and we submitted the results using themethods of EBLEU and NB-LPR.
The perfor-mance of NB-LPR on EN-ES language pairshows higher Tau score (0.07) than the baselinesystem score (0.03) when the ties are ignored.Because of the number limitation of submittedsystems for each task, we did not submit theSVM-LPR results.
However, the training exper-iments prove that the SVM-LPR model performsbetter than the NB-LPR model though SVM-LPR takes more time to run.3.3 Task 2 Word-level QETask 2 is the word-level quality estimation ofautomatically translated news sentences fromEnglish to Spanish without given reference trans-lations.
Participants are required to judge eachtranslated word by assigning a two- or multi-class labels.
In the binary classification, a goodor a bad label should be judged, where ?bad?indicates the need for editing the token.
In themulti-class classification, the labels include?keep?, ?delete?
and ?substitute?.
In addition tothe NB method, in this task, we utilized a dis-criminative undirected probabilistic graphicalmodel, i.e.
Conditional Random Field (CRF).CRF is early employed by Lefferty (Leffertyet al 2001) to deal with the labeling problems ofsequence data, and is widely used later by otherresearchers.
As the preparation for CRF defini-tion, we assume that   is a variable representingthe input sequence, and   is another variable rep-resenting the corresponding labels to be attachedto  .
The two variables interact as conditionalprobability  ( | )  mathematically.
Then thedefinition of CRF: Let a graph model   (   )comprise a set   of vertices or nodes togetherwith a set   of edges or lines and      |, such that   is indexed by the vertices of  ,then (   ) shapes a CRF model.
This set meetsthe following form:( | )(?
(   |   )       ?
(   |   )     )(8)where   and   represent the data sequence andlabel sequence respectively;    and    are thefeatures to be defined;    and    are the parame-ters trained from the datasets.
We used the toolCRF++1 to implement the CRF algorithm.
Thefeatures we used for the NB and CRF are shownin Table 7.
We firstly trained the CRF and NBmodels on the officially released training corpus(produced by Moses and annotated by computingTER with some tweaks).
Then we removed thetruth labels in the training corpus (we call itpseudo test corpus) and labeled each word usingthe derived training models.
The test results onthe pseudo test corpus are shown in Table 8,1 https://code.google.com/p/crfpp/368which specifies CRF performs better than NBalgorithm.
(    )Unigram, from antecedent 4thto subsequent 3rd token(    )Bigram, from antecedent 2ndto subsequent 2nd tokenJump bigram, antecedent andsubsequent token(    )Trigram, from antecedent 2ndto subsequent 2nd tokenTable 7: Developed featuresBinaryCRF NBTraining Accuracy Training AccuracyItera=108Time=2.48s0.944 Time=0.59s 0.941Multi-classesCRF NBTraining Accuracy Training AccuracyItera=106Time=3.67s0.933 Time=0.55s 0.929Table 8: Performance on pseudo test corpusThe official testing scores of Task 2 are shownin Table 9.
We include also the results of otherparticipants (CNGL and LIG) and their ap-proaches.Binary MulticlassMethods Pre Recall F1 AccCNGL-dMEMM0.7392 0.9261 0.8222 0.7162CNGL-MEMM0.7554 0.8581 0.8035 0.7116LIG-All N/A N/A N/A 0.7192LIG-FS 0.7885 0.8644 0.8247 0.7207LIG-BOOSTING0.7779 0.8843 0.8276 N/ANB 0.8181 0.4937 0.6158 0.5174CRF 0.7169 0.9846 0.8297 0.7114Table 9: QE Task 2 official testing scoresThe results show that our method CRF yieldsa higher recall score than other systems in binaryjudgments task, and this leads to the highest F1score (harmonic mean of precision and recall).The recall value reflects the loyalty to the truthdata.
The augmented feature set designed in thispaper allows the CRF to take the contextual in-formation into account, and this contributesmuch to the recall score.
On the other hand, theaccuracy score of CRF in multiclass evaluation islower than LIG-FS method.4 ConclusionsThis paper describes the algorithms and featureswe used in the WMT 13 Quality Estimation tasks.In the sentence-level QE task (Task 1.1), we de-velop an enhanced version of BLEU metric, andthis shows a potential usage for the traditionalevaluation criteria.
In the newly proposed systemselection task (Task 1.2) and word-level QE task(Task 2), we explore the performances of severalstatistical models including NB, SVM, and CRF,of which the CRF performs best, the NB per-forms lower than SVM but much faster thanSVM.
The official results show that the CRFmodel yields the highest F-score 0.8297 in binaryclassification judgment of word-level QE task.AcknowledgmentsThe authors are grateful to the Science andTechnology Development Fund of Macau andthe Research Committee of the University ofMacau for the funding support for our research,under the reference No.
017/2009/A andRG060/09-10S/CS/FST.
The authors also wish tothank the anonymous reviewers for many helpfulcomments.ReferencesAkiba, Yasuhiro, Kenji Imamura, and Eiichiro Sumita.2001.
Using Multiple Edit Distances to Automati-cally Rank Machine Translation Output.
In Pro-ceedings of the MT Summit VIII, Santiago deCompostela, Spain.Albrecht, Joshua, and Rebecca Hwa.
2007.
Regres-sion for sentence-level MT evaluation with pseudoreferences.
ACL.
Vol.
45.
No.
1.Avramidis, Eleftherios.
2012.
Comparative qualityestimation: Automatic sentence-level ranking ofmultiple machine translation outputs.
In Proceed-ings of 24th International Conference onComputational Linguistics (COLING), pages115?132, Mumbai, India.Burges, Christopher J. C. 1998.
A Tutorial on SupportVector Machines for Pattern Recognition.
J. DataMin.
Knowl.
Discov.
Volume 2 Issue 2, June1998, 121-167.
Kluwer Academic PublishersHingham, MA, USA.Callison-Burch, Chris, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the Seventh369Workshop on Statistical Machine Translation,pages 10?51, Montr?al, Canada, June.Castillo, Julio and Paula Estrella.
2012.
SemanticTextual Similarity for MT evaluation, Proceed-ings of the 7th Workshop on Statistical Ma-chine Translation (WMT2012), pages 52?58,Montre a?l, Canada, June 7-8.
Association forComputational Linguistics.Chan, Yee Seng and Hwee Tou Ng.
2008.
MAXSIM:A maximum similarity metric for machine transla-tion evaluation.
In Proceedings of ACL 2008:HLT, pages 55?62.
Association for ComputationalLinguistics.Chen, Boxing and Roland Kuhn.
2011.
Amber: Amodified bleu, enhanced ranking metric.
In Pro-ceedings of the Sixth Workshop on StatisticalMachine translation of the Association forComputational Linguistics(ACL-WMT), pages71-77, Edinburgh, Scotland, UK.Chen, Boxing, Roland Kuhn and Samuel Larkin.
2012.PORT: a Precision-Order-Recall MT EvaluationMetric for Tuning, Proceedings of the 50th An-nual Meeting of the Association for Computa-tional Linguistics, pages 930?939, Jeju, Republicof Korea, 8-14 July.Cortes, Corinna and Vladimir Vapnik.
1995.
Support-Vector Networks, J.
Machine Learning, Volume20, issue 3, pp 273-297.
Kluwer Academic Pub-lishers, Boston.
Manufactured in The Netherlands.Dahlmeier, Daniel, Chang Liu, and Hwee Tou Ng.2011.
TESLA at WMT2011: Translation evalua-tion and tunable metric.
In Proceedings of theSixth Workshop on Statistical Machine Trans-lation, Association for Computational Linguis-tics (ACL-WMT), pages 78-84, Edinburgh, Scot-land, UK.Doddington, George.
2002.
Automatic evaluation ofmachine translation quality using n-gram co-occurrence statistics.
In Proceedings of the sec-ond international conference on Human Lan-guage Technology Research (HLT '02).
MorganKaufmann Publishers Inc., San Francisco, CA,USA, 138-145.Echizen-ya, Hiroshi and Kenji Araki.
2010.
Automat-ic evaluation method for machine translation usingnoun-phrase chunking.
In Proceedings of ACL2010, pages 108?117.
Association for Computa-tional Linguistics.Gamon, Michael, Anthony Aue, and Martine Smets.2005.
Sentence-level MT evaluation without refer-ence translations: Beyond language modeling.Proceedings of EAMT.Hall, Mark, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, Ian H. Witten.
2009.The WEKA data mining software: An update.SIGKDD Explorations, 11.Han, Aaron Li-Feng, Derek F. Wong and Lidia S.Chao.
2012.
LEPOR: A Robust Evaluation Metricfor Machine Translation with Augmented Factors.Proceedings of the 24th International Confer-ence on Computational Linguistics (COLING2012: Posters), Mumbai, India.Han, Aaron Li-Feng, Derek F. Wong, Lidia S. Chao,Liangye He, Yi Lu, Junwen Xing and XiaodongZeng.
2013.
Language-independent Model for Ma-chine Translation Evaluation with Reinforced Fac-tors.
Proceedings of the 14th InternationalConference of Machine Translation Summit(MT Summit 2013), Nice, France.Harrington, Peter.
2012.
Classifying with probabilitytheory: na?ve bayes.
Machine Learning in Ac-tion, Part 1 Classification.
Page 61-82.
Publisher:Manning Publications.
April.Lafferty, John, McCallum Andrew, and Pereira C.N.Ferando.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling se-quence data.
In Proceeding of 18th Internation-al Conference on Machine Learning.
282-289.Lavie, Alon and Abhaya Agarwal.
2007.
METEOR:An Automatic Metric for MT Evaluation with HighLevels of Correlation with Human Judgments,Proceedings of the ACL Second Workshop onStatistical Machine Translation, pages 228-231,Prague, June.Leusch, Gregor, Nicola Ueffing, and Hermann Ney.2006.
CDer: Efficient MT Evaluation Using BlockMovements.
In Proceedings of the 13th Confer-ence of the European Chapter of the Associa-tion for Computational Linguistics (EACL-06),241-248.Lin, Chin-Yew and Eduard Hovy.
2003.
AutomaticEvaluation of Summaries Using N-gram Co-occurrence Statistics.
In Proceedings of 2003Language Technology Conference (HLT-NAACL 2003), Edmonton, Canada, May 27 - June1.Lita, Lucian Vlad, Monica Rogati and Alon Lavie.2005.
BLANC: Learning Evaluation Metrics forMT, Proceedings of Human Language Tech-nology Conference and Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP), pages 740?747, Vancouver, Oc-tober.
Association for Computational Linguistics.Liu, Chang, Daniel Dahlmeier and Hwee Tou Ng.2010.
TESLA: Translation evaluation of sentences370with linear-programming-based analysis.
In Pro-ceedings of the Joint Fifth Workshop on Statis-tical Machine Translation and MetricsMATR.Liu, Ding and Daniel Gildea.
2006.
Stochastic itera-tive alignment for machine translation evaluation.Sydney.
ACL06.Mariano, Felice and Lucia Specia.
2012.
LinguisticFeatures for Quality Estimation.
Proceedings ofthe 7th Workshop on Statistical MachineTranslation, pages 96?103.Mehdad, Yashar, Matteo Negri, and Marcello Federi-co. 2012.
Match without a referee: evaluating MTadequacy without reference translations.
Proceed-ings of the Seventh Workshop on StatisticalMachine Translation.
Association for Compu-tational Linguistics.Mirkin, Shachar, Lucia Specia, Nicola Cancedda, IdoDagan, Marc Dymetman, and Idan Szpektor.
2009.Source-Language Entailment Modeling for Trans-lating Unknown Terms, Proceedings of the JointConference of the 47th Annual Meeting of theACL and the 4th International Joint Confer-ence on Natural Language Processing of theAFNLP, pages 791?799, Suntec, Singapore, 2-7.ACL and AFNLP.Nie?en, Sonja, Franz Josef Och, Gregor Leusch, andHermann Ney.
2000.
A Evaluation Tool for Ma-chine Translation: Fast Evaluation for MT Re-search.
In Proceedings of the 2nd InternationalConference on Language Resources and Eval-uation (LREC-2000).Owczarzak, Karolina, Josef van Genabith and AndyWay.
2007.
Labelled Dependencies in MachineTranslation Evaluation, Proceedings of the ACLSecond Workshop on Statistical MachineTranslation, pages 104-111, Prague.Papineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a method for automat-ic evaluation of machine translation.In Proceedings of the 40th Annual Meeting onAssociation for Computational Linguis-tics (ACL '02).
Association for ComputationalLinguistics, Stroudsburg, PA, USA, 311-318.Petrov, Slav, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proceedings of the21st International Conference on Computa-tional Linguistics and the 44th annual meetingof the Association for Computational Linguis-tics (ACL-44).
Association for ComputationalLinguistics, Stroudsburg, PA, USA, 433-440.Popovic, Maja, David Vilar, Eleftherios Avramidis,Aljoscha Burchardt.
2011.
Evaluation without ref-erences: IBM1 scores as evaluation metrics.
InProceedings of the Sixth Workshop on Statisti-cal Machine Translation, Association forComputational Linguistics (ACL-WMT), pages99-103, Edinburgh, Scotland, UK.Povlsen, Claus, Nancy Underwood, Bradley Music,and Anne Neville.
1998.
Evaluating Text-TypeSuitability for Machine Translation a Case Studyon an English-Danish System.
Proceedings of theFirst Language Resources and EvaluationConference, LREC-98, Volume I.
27-31.
Grana-da, Spain.Schmid, Helmut.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In Proceedings ofInternational Conference on New Methods inLanguage Processing, Manchester, UK.Snover, Matthew G., Nitin Madnani, Bonnie Dorr,and Richard Schwartz.
2009.
TER-Plus: paraphrase,semantic, and alignment enhancements to Transla-tion Edit Rate.
J.
Machine Tranlslation, 23: 117-127.Specia, Lucia and Gimenez, J.
2010.
Combining Con-fidence Estimation and Reference-based Metricsfor Segment-level MT Evaluation.
The NinthConference of the Association for MachineTranslation in the Americas (AMTA).Specia, Lucia, Dhwaj Raj, and Marco Turchi.
2010.Machine Translation Evaluation Versus QualityEstimation.
Machine Translation, 24:39?50.Su, Keh-Yih, Wu Ming-Wen and Chang Jing-Shin.1992.
A New Quantitative Quality Measure forMachine Translation Systems.
In Proceedings ofthe 14th International Conference on Compu-tational Linguistics, pages 433?439, Nantes,France, July.Tillmann, Christoph, Stephan Vogel, Hermann Ney,Arkaitz Zubiaga, and Hassan Sawaf.
1997.
Accel-erated DP Based Search For Statistical Translation.In Proceedings of the 5th European Confer-ence on Speech Communication and Technol-ogy (EUROSPEECH-97).Turian, Joseph P., Luke Shen, and I. Dan Melamed.2003.
Evaluation of Machine Translation and itsEvaluation.
In Machine Translation Summit IX,pages 386?393.
International Association for Ma-chine Translation.Wang, Mengqiu and Christopher D. Manning.
2012.SPEDE: Probabilistic Edit Distance Metrics forMT Evaluation, WMT2012, 76-83.Wong, Billy T. M. and Chunyu Kit.
2012.
ExtendingMachine Translation Evaluation Metrics with Lex-ical Cohesion to Document Level.
Proceedings ofthe 2012 Joint Conference on Empirical371Methods in Natural Language Processing andComputational Natural Language Learning,pages 1060?1068, Jeju Island, Korea, 12?14 July.Association for Computational Linguistics.Zhang, Harry.
2004.
The Optimality of Naive Bayes.Proceedings of the Seventeenth InternationalFlorida Artificial Intelligence Research Socie-ty Conference, Miami Beach, Florida, USA.AAAI Press.372
