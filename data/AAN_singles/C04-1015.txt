Example-based Machine Translation Based on Syntactic Transferwith Statistical ModelsKenji Imamura, Hideo Okuma, Taro Watanabe, and Eiichiro SumitaATR Spoken Language Translation Research Laboratories2-2-2 Hikaridai, ?Keihanna Science City?Kyoto, 619-0288, Japan{kenji.imamura,hideo.okuma,taro.watanabe,eiichiro.sumita}@atr.jpAbstractThis paper presents example-based machinetranslation (MT) based on syntactic trans-fer, which selects the best translation by us-ing models of statistical machine translation.Example-based MT sometimes generates in-valid translations because it selects similar ex-amples to the input sentence based only onsource language similarity.
The method pro-posed in this paper selects the best transla-tion by using a language model and a trans-lation model in the same manner as statisti-cal MT, and it can improve MT quality overthat of ?pure?
example-based MT.
A featureof this method is that the statistical modelsare applied after word re-ordering is achievedby syntactic transfer.
This implies that MTquality is maintained even when we only ap-ply a lexicon model as the translation model.In addition, translation speed is improved bybottom-up generation, which utilizes the treestructure that is output from the syntactictransfer.1 IntroductionIn response to the ongoing expansion of bilingualcorpora, many machine translation (MT) meth-ods have been proposed that automatically ac-quire their knowledge or models from the cor-pora.
Recently, two major approaches to such ma-chine translation have emerged: example-basedmachine translation and statistical machine trans-lation.Example-based MT (Nagao, 1984) regards abilingual corpus as a database and retrieves exam-ples that are similar to an input sentence.
Then,a translation is generated by modifying the tar-get part of the examples while referring to trans-lation dictionaries.
Most example-based MT sys-tems employ phrases or sentences as the unit forexamples, so they can translate while consider-ing case relations or idiomatic expressions.
How-ever, when some examples conflict during re-E =J =A =NULL0 show1 me2 the3 one4 in5 the6 window7uindo1 no2 shinamono3 o4 mise5 telidasai67 0 4 0 1 1( )Figure 1: Example of Word Alignment betweenEnglish and Japanese (Watanabe and Sumita,2003)trieval, example-based MT selects the best exam-ple scored by the similarity between the input andthe source part of the example.
This implies thatexample-based MT does not check whether thetranslation of the given input sentence is corrector not.On the other hand, statistical MT employingIBM models (Brown et al, 1993) translates an in-put sentence by the combination of word transferand word re-ordering.
Therefore, when it is ap-plied to a language pair in which the word order isquite different (e.g., English and Japanese, Figure1), it becomes difficult to find a globally optimalsolution due to the enormous search space (Watan-abe and Sumita, 2003).Statistical MT could generate high-qualitytranslations if it succeeded in finding a globallyoptimal solution.
Therefore, the models employedby statistical MT are superior indicators of thequality of machine translation.
Using this feature,Akiba et al (2002) achieved selection of the besttranslation among those output by multiple MTengines.This paper presents an example-based MTmethod based on syntactic transfer, which selectsthe best translation by using models of statisti-cal MT.
This method is roughly structured usingtwo modules (Figure 2).
One is an example-basedsyntactic transfer module.
This module constructsInput Sentence Output SentenceExample-basedSyntactic TransferThesaurusPreprocessing PostprocessingStatisticalGenerationTranslationDictionaryTransferRulesTranslationModelLanguageModelFigure 2: Structure of Proposed Methodtree structures of the target language by parsingand mapping the input sentence while referring totransfer rules.
The other is a statistical generationmodule, which selects the best word sequence ofthe target language in the same manner as statis-tical MT.
Therefore, this method is sequentiallycombined example-based and statistical MT.The proposed method has the following advan-tages.?
From the viewpoint of example-based MT, thequality of machine translation improves by se-lecting the best translation not only from thesimilarity judgment between the input sen-tence and the source part of the examples butalso from the scoring of translation correctnessrepresented by the word transfer and word con-nection.?
From the viewpoint of statistical MT, an ap-propriate translation can be obtained even ifwe use simple models because a global searchis applied after word re-ordering by syntac-tic transfer.
In addition, the search spacebecomes smaller because the example-basedtransfer generates syntactically correct candi-dates for the most appropriate translation.The rest of this paper is organized as follows:Section 2 describes the example-based syntactictransfer, Section 3 describes the statistical gen-eration, Section 4 evaluates an experimental sys-tem that uses this method, and Section 5 comparesother hybrid methods of example-based and statis-tical MT.2 Example-based Syntactic TransferThe example-based syntactic transfer used in thispaper is a revised version of the HierarchicalPhrase Alignment-based Translator (HPAT, re-fer to (Imamura, 2002)).
This section gives anoverview with an example of Japanese-to-Englishmachine translation.2.1 Transfer RulesTransfer rules are automatically acquired frombilingual corpora by using hierarchical phrasealignment (HPA; (Imamura, 2001)).
HPA parsesbilingual sentences and acquires correspondingsyntactic nodes of the source and target sentences.The transfer rules are created from their node cor-respondences.
Figure 3 shows an example of thetransfer rules.
Variables, such as X and Y in Fig-ure 3, denote non-terminal symbols that corre-spond between source and target grammar.
Theset of transfer rules is regarded as synchronizedcontext-free grammar.The difference between this approach and con-ventional synchronized context-free grammar isthat source examples are added to each transferrule.
The source example is an instance (i.e., aheadword) of the variables that appeared in thetraining corpora.
For example, the source exam-ple of Rule 1 in Figure 3 is obtained from a phrasepair of the Japanese verb phrase ?furaito (flight)wo yoyaku-suru (reserve)?
and the English verbphrase ?make a reservation for the flight.
?2.2 Syntactic Transfer ProcessWhen an input sentence is given, the target treestructure is constructed in the following threesteps.1.
The input sentence is parsed by using thesource grammar of the transfer rules.2.
The nodes in the source tree are mapped to thetarget nodes by using transfer rules.3.
If non-terminal symbols remain in the leaves ofthe target tree, candidates of translated wordsare inserted by referring to the translation dic-tionary.An example of the syntactic transfer process isshown in Figure 4 for the input sentence ?basuwa 11 ji ni de masu (The bus will leave at 11o?clock).?
There are two points worthy of notice inthis figure.
First, nodes in which the word order isinverted are generated after transfer (cf.
VP noderepresented by a bold frame).
Word re-orderingis achieved by syntactic transfer.
Second, wordsNo.
Source Grammar Target Grammar Source Example1 VP ?
XPPYVP?
VP ?
YVPXPP((furaito (flight), yoyaku-suru (reserve)) ..)2 VP ?
YVPXADVP((soko (there), yuku (go)) ..)3 VP ?
YBEVPXNP((hashi (bridge), aru (be)) ..)4 S ?
XNPwa YVPmasu ?
S ?
XNPYVP((kare (he), enso-suru (play)) ..)5 S ?
XNPwill YVP((basu (bus), tomaru (stop)) ..)Figure 3: Example of Transfer Rulesbusbathgoleavestart11NP -> a X3NP -> the X3NP -> X3VP -> Y2 X2VP -> X5 PP -> at X4ADVP -> X4NP -> X6 o?clockNP -> X6basu(bus)11deru(leave)NPX3NPX6 ji(o?clock)PPX4 niVPX5VPX2 Y2SX1 wa Y1 masuX1Y1Y2 X2Japanese EnglishS -> X1 will Y1Figure 4: Example of Syntactic Transfer Process(Bold frames are syntactic nodes mentioned in text)that do not correspond between the source and tar-get sentences (e.g., the determiner ?a?
or ?the?
)are automatically inserted or eliminated by the tar-get grammar (cf.
NP node represented by a boldframe).
Namely, transfer rules work in a mannersimilar to the functions of distortion, fertility, andNULL in IBM models.2.3 Usage of Source ExamplesExample-based transfer utilizes the source exam-ples for disambiguation of mapping and parsing.Specifically, the semantic distance (Sumita andIida, 1991) is calculated between the source exam-ples and the headwords of the input sentence, andthe transfer rules that contain the nearest exam-ple are used to construct the target tree structure.The semantic distance between words is definedas the distance from the leaf node to the most spe-cific common abstraction (MSCA) in a thesaurus(Ohno and Hamanishi, 1984).For example, if the input phrase ?ie (home) nikaeru (return)?
is given, Rules 1 to 3 in Figure 3are used for the syntactic transfer, and three targetnodes are generated without any disambiguation.However, when we compare the source exampleswith the headword of the variables X (ie) and Y(kaeru), only Rule 2 is used for the transfer be-cause the semantic distance of the example (soko(there), yuku (go)) is the nearest.
In the currentimplementation, all rules that contain examples ofthe same distance are used.Consequently, example-based transfer achievestranslation while considering case relations or id-iomatic expressions based on the semantic dis-tance from the source examples.3 Statistical Generation3.1 Translation Model and Language ModelStatistical generation searches for the most ap-propriate sequence of target words from the tar-get tree output from the example-based syntactictransfer.
The most appropriate sequence is deter-mined from the product of the translation modeland the language model in the same manner as sta-tistical MT.
In other words, when F and E denotethe channel target and channel source sequence,respectively, the output word sequence E?
that sat-isfies the following equation is searched for.E?
= argmaxEP (E|F )= argmaxEP (E)P (F |E).
(1)We only utilize the lexicon model as the trans-lation model in this paper, similar to the modelsproposed by Vogel et al (2003).
Namely, when fand e denote the channel target and channel sourceword, respectively, the translation probability iscomputed by the following equation.P (F |E) =?j?it(fj|ei).
(2)The IBM models include other models, suchas fertility, NULL, and distortion models.
As wedescribed in Section 2.2, the quality of machinetranslation is maintained using only the lexiconmodel because syntactical correctness is alreadypreserved by example-based transfer.For the language model, we utilize a standardword n-gram model.3.2 Bottom-up GenerationWe can construct word graphs by serializing thetarget tree structure, which allows us to select thebest word sequence from the graphs.
However,the tree structure already shares nodes transferredfrom the same input sub-sequence.
The cost ofcalculating probabilities is equivalent if we cal-culate the probabilities while serializing the treestructure.
We call this method bottom-up genera-tion in this paper.Figure 5 shows a partial example of bottom-up generation when the target tree in Figure 4is given.
For each node, word sub-sequencesand their probabilities (language and translation)are obtained from child nodes.
Then, the newprobabilities of the word sequence combinationare calculated, and the n-best sequences are se-lected.
These n-best sequences and their prob-abilities are reused to calculate the probabilitiesof parent nodes.
When the translation probabil-ity is calculated, the source word sub-sequence isobtained by tracing transfer mapping, and the ap-plied translation model is restricted to the sourcesub-sequence.
In other words, the translationprobability is locally calculated between the cor-responding phrases.Set Name Item English JapaneseTraining # of Sentences 152,170# of Words 886,708 1,007,484Test # of Sentences 510# of Words 2,973 3,340Table 1: Corpus SizeWhen the generation reaches the top node, thelanguage probability is re-calculated with marksfor start-of-sentence and end-of-sentence, and then-best list is re-sorted.
As a result, the translation?The bus will leave at 11 o?clock?
is obtained fromthe tree of Figure 4.Bottom-up generation calculates the probabili-ties of shared nodes only once, so it effectivelyuses tree information.4 EvaluationIn order to evaluate the effect when models of sta-tistical MT are integrated into example-based MT,we compared various methods that changed thestatistical generation module.4.1 Experimental SettingBilingual Corpus The corpus used in the fol-lowing experiments is the Basic Travel ExpressionCorpus (Takezawa et al, 2002; Kikui et al, 2003).This is a collection of Japanese sentences and theirEnglish translations based on expressions that areusually found in phrasebooks for foreign tourists.We divided it into subsets for training and testingas shown in Table 1.Transfer Rules Transfer rules were acquiredfrom the training set using hierarchical phrasealignment, and low-frequency rules that appearedless than twice were removed.
The number ofrules was 24,310.Translation Model and Language Model Weused a lexicon model of IBM Model 4 learned byGIZA++ (Och and Ney, 2003) and word bigramand trigram models learned by CMU-CambridgeStatistical Language Modeling Toolkit (Clarksonand Rosenfeld, 1997).Compared Methods We compared the follow-ing four methods.?
Baseline (Example-based Transfer only)The best translation that had the same seman-tic distance was randomly selected from thethe bus TM: -0.07LM: -1.94bus TM: -0.07LM: -0.0XNP n-bestn-best n-bestwillYVPleave at 11 o?clock TM: -2.72LM: -4.58start at 11 o?clock TM: -3.62LM: -4.17leaves at 11 o?clock TM: -2.72LM: -3.11YVPleave TM: -1.88LM: -0.0start TM: -2.78LM: -0.0leaves TM: -1.88LM: -0.0XPPat 11 o?clock TM: -0.84LM: -2.79at 11 TM: -4.91LM: -2.26a bus TM: -0.07LM: -2.11Sbus will start at 11 o?clockthe bus will leave at 11 o?clockbus will leave at 11 o?clock TM: -7.13LM: -14.30TM: -8.03LM: -13.84TM: -7.13LM: -13.54<s> </s>Figure 5: Example of Bottom-up Generation(TM and LM denote log probabilities of the translation and language models, respectively)tree that was output from the example-basedtransfer module.
The translation words wereselected in advance as those having the highestfrequency in the training corpus.
This is thebaseline for translating a sentence when usingonly the example-based transfer.?
Bottom-upThe bottom-up generation selects the besttranslation from the outputs of the example-based transfer.
We used a 100-best criterionin this experiment.?
All SearchFor all combinations that can be generatedfrom the outputs of the example-based trans-fer, we calculated the translation and languageprobabilities and selected the best translation.Namely, a globally optimal solution was se-lected when the search space was restricted bythe example-based transfer.?
LM OnlyIn the same way as All Search, the best trans-lation was searched for, but only the languagemodel was used for calculating probabilities.The purpose of this experiment is to measurethe influence of the translation model.Evaluation Metrics From the test set, 510 sen-tences were evaluated by the following automaticand subjective evaluation metrics.
The numberof reference translations for automatic evaluationwas 16 per sentence.BLEU: Automatic evaluation by BLEU score(Papineni et al, 2002).NIST: Automatic evaluation by NIST score(Doddington, 2002).mWER: The mean rate by calculating the worderror rates between the MT results and all ref-erence translations, where the lowest rate is se-lected.Subjective Evaluation: Subjective evaluationby an English native speaker into the four ranksof A: Perfect, B: Fair, C: Acceptable, and D:Nonsense.Automatic Evaluation Subjective Evaluation Translation SpeedMethod BLEU NIST mWER A A+B A+B+C Mean Worst(sec./sent.)
(sec.
)Baseline 0.410 9.06 0.423 51.6% 64.3% 70.4% 0.180 10.82Bottom-up 0.491 9.99 0.366 62.2% 72.5% 80.4% 0.211 5.03All Search 0.498 10.04 0.353 62.9% 73.1% 80.8% 1.23 171.31LM Only 0.491 9.11 0.385 57.6% 66.9% 72.0% 1.624 220.69Table 2: MT Quality and Translation Speed vs. Generation Methods4.2 ResultsTable 2 shows the results of the MT quality andtranslation speed among each method.First, comparing the baseline with the statisti-cal generations (Bottom-up and All Search), theMT quality of statistical generation improved inall evaluation metrics.
Accordingly, the models ofstatistical MT are effective for improving the MTquality of example-based MT.Next, comparing Bottom-up with All Search,the MT quality of bottom-up generation wasslightly low.
Bottom-up generation locally appliesthe translation model to a partial tree.
In otherwords, the probability is calculated without wordalignment linked to the outside of the tree.
This re-sult indicates that the results of bottom-up genera-tion are not equal to the global optimal solution.Comparing LM Only with the statistical gener-ations, the MT quality of ranks A+B+C by subjec-tive evaluation significantly decreased.
This is be-cause the n-gram language model used here doesnot consider output length, and shorter translationsare preferred.
Although the language model waseffective to some degree, it could not evaluate theequivalence of the translation and the input sen-tence.
Therefore, we concluded that the transla-tion model is necessary for improving MT quality.Finally, focusing on translation speed, the worsttime for Bottom-up generation was dramaticallyfaster than that for All Search.
Bottom-up gen-eration effectively uses shared nodes of the targettree, so it can improve translation speed.
There-fore, bottom-up generation is suitable for tasksthat require real-time processing, such as spokendialogue translation.5 DiscussionWe incorporated example-based MT in modelsof statistical MT.
However, some methods to ob-tain initial solutions of statistical MT by example-based MT have already been proposed.
Forexample, Marcu (2001) proposed a method inwhich initial translations are constructed by com-bining bilingual phrases from translation mem-ory, which is followed by modifying the transla-tions by greedy decoding (Germann et al, 2001).Watanabe and Sumita (2003) proposed a decodingalgorithm in which translations that are similar tothe input sentence are retrieved from bilingual cor-pora and then modified by greedy decoding.The difference between our method and thesemethods involves whether modification is applied.Our approach simply selects the best translationfrom candidates that are output from example-based MT.
Even though example-based MT canoutput appropriate translations to some degree,our method assumes that the candidates containa globally optimal solution.
This means thatthe upper bound of MT quality is limited by theexample-based transfer, so we have to improvethis stage in order to further improve MT quality.For instance, example-based MT can be improvedby applying an optimization algorithm that usesan automatic evaluation of MT quality (Imamuraet al, 2003).6 ConclusionsThis paper demonstrated that example-based MTcan be improved by incorporating it in models ofstatistical MT.
The example-based MT used in thispaper is based on syntactic transfer, so word re-ordering is achieved in the transfer module.
Us-ing this feature, the best translation was selectedby using only a lexicon model and an n-gram lan-guage model.
In addition, bottom-up generationachieved faster translation speed by using the treestructure of the target sentence.AcknowledgementsThe authors would like to thank Kadokawa Pub-lishers, who permitted us to use the hierarchy ofRuigo-shin-jiten.The research reported here is supported in partby a contract with the Telecommunications Ad-vancement Organization of Japan entitled, ?Astudy of speech dialogue translation technologybased on a large corpus.
?ReferencesYasuhiro Akiba, Taro Watanabe, and EiichiroSumita.
2002.
Using language and transla-tion models to select the best among outputsfrom multiple MT systems.
In Proceedings ofCOLING-2002, pages 8?14.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of machine translation: Pa-rameter estimation.
Computational Linguistics,19(2):263?311.Philip Clarkson and Ronald Rosenfeld.
1997.Statistical language modeling using the CMU-Cambridge toolkit.
In Proceedings of Eu-roSpeech 97, pages 2707?2710.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gramco-occurrence statistics.
In Proceedings of theHLT Conference, San Diego, California.Ulrich Germann, Michael Jahr, Kevin Knight,Daniel Marcu, and Kenji Yamada.
2001.
Fastdecoding and optimal decoding for machinetranslation.
In Proceedings of 39th AnnualMeeting of the Association for ComputationalLinguistics, pages 228?235.Kenji Imamura, Eiichiro Sumita, and Yuji Mat-sumoto.
2003.
Feedback cleaning of machinetranslation rules using automatic evaluation.
InProceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics(ACL 2003), pages 447?454.Kenji Imamura.
2001.
Hierarchical phrase align-ment harmonized with parsing.
In Proceed-ings of the 6th Natural Language ProcessingPacific Rim Symposium (NLPRS 2001), pages377?384.Kenji Imamura.
2002.
Application of transla-tion knowledge acquired by hierarchical phrasealignment for pattern-based MT.
In Proceed-ings of the 9th Conference on Theoretical andMethodological Issues in Machine Translation(TMI-2002), pages 74?84.Genichiro Kikui, Eiichiro Sumita, ToshiyukiTakezawa, and Seiichi Yamamoto.
2003.
Cre-ating corpora for speech-to-speech translation.In Proceedings of EuroSpeech 2003, pages381?384.Daniel Marcu.
2001.
Towards a unified approachto memory- and statistical-based machine trans-lation.
In Proceedings of 39th Annual Meetingof the Association for Computational Linguis-tics, pages 386?393.Makoto Nagao.
1984.
A framework of mechani-cal translation between Japanese and English byanalogy principle.
In Artificial and Human In-telligence, pages 173?180, Amsterdam: North-Holland.Franz Josef Och and Hermann Ney.
2003.
Asystematic comparison of various statisticalalignment models.
Computational Linguistics,29(1):19?51.Susumu Ohno and Masato Hamanishi.
1984.Ruigo-Shin-Jiten.
Kadokawa, Tokyo.
inJapanese.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a method for au-tomatic evaluation of machine translation.
InProceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 311?318.Eiichiro Sumita and Hitoshi Iida.
1991.
Experi-ments and prospects of example-based machinetranslation.
In Proceedings of the 29th ACL,pages 185?192.Toshiyuki Takezawa, Eiichiro Sumita, FumiakiSugaya, Hirofumi Yamamoto, and Seiichi Ya-mamoto.
2002.
Toward a broad-coverage bilin-gual corpus for speech translation of travel con-versations in the real world.
In Proceedingsof the Third International Conference on Lan-guage Resources and Evaluation (LREC 2002),pages 147?152.Stephan Vogel, Ying Zhang, Fei Huang, AliciaTribble, Ashish Venugopal, Bing Zhao, andAlex Waibel.
2003.
The CMU statistical ma-chine translation system.
In Proceedings of the9th Machine Translation Summit (MT SummitIX), pages 402?409.Taro Watanabe and Eiichiro Sumita.
2003.Example-based decoding for statistical machinetranslation.
In Proceedings of Machine Trans-lation Summit IX, pages 410?417.
