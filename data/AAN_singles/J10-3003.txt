Generating Phrasal and SententialParaphrases: A Survey ofData-Driven MethodsNitin Madnani?University of Maryland, College ParkBonnie J.
Dorr?
?University of Maryland, College ParkThe task of paraphrasing is inherently familiar to speakers of all languages.
Moreover, the task ofautomatically generating or extracting semantic equivalences for the various units of language?words, phrases, and sentences?is an important part of natural language processing (NLP)and is being increasingly employed to improve the performance of several NLP applications.In this article, we attempt to conduct a comprehensive and application-independent survey ofdata-driven phrasal and sentential paraphrase generation methods, while also conveying anappreciation for the importance and potential use of paraphrases in the field of NLP research.Recent work done in manual and automatic construction of paraphrase corpora is also examined.We also discuss the strategies used for evaluating paraphrase generation techniques and brieflyexplore some future trends in paraphrase generation.1.
IntroductionAlthough everyone may be familiar with the notion of paraphrase in its most funda-mental sense, there is still room for elaboration on how paraphrases may be automat-ically generated or elicited for use in language processing applications.
In this survey,we make an attempt at such an elaboration.
An important outcome of this survey isthe discovery that there are a large variety of paraphrase generation methods, eachwith widely differing sets of characteristics, in terms of performance as well as easeof deployment.
We also find that although many paraphrase methods are developedwith a particular application in mind, all methods share the potential for more generalapplicability.
Finally, we observe that the choice of the most appropriate method foran application depends on proper matching of the characteristics of the producedparaphrases with an appropriate method.It could be argued that it is premature to survey an area of research that has shownpromise but has not yet been tested for a long enough period (and in enough systems).However, we believe this argument actually strengthens the motivation for a survey?
Department of Computer Science and Institute for Advanced Computer Studies, A.V.
Williams Bldg,University of Maryland, College Park, MD 20742, USA.
E-mail: nmadnani@umiacs.umd.edu.??
Department of Computer Science and Institute for Advanced Computer Studies, A.V.
Williams Bldg,University of Maryland, College Park, MD 20742, USA.
E-mail: bonnie@umiacs.umd.edu.Submission received: 16 December 2008; revised submission received: 30 November 2009; accepted forpublication: 7 March 2010.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 3that can encourage the community to use paraphrases by providing an application-independent, cohesive, and condensed discussion of data-driven paraphrase generationtechniques.
We should also acknowledge related work that has been done on furtheringthe community?s understanding of paraphrases.
Hirst (2003) presents a comprehensivesurvey of paraphrasing focused on a deep analysis of the nature of a paraphrase.
Thecurrent survey focuses instead on delineating the salient characteristics of the variousparaphrase generation methods with an emphasis on describing how they could beused in several different NLP applications.
Both these treatments provide different butvaluable perspectives on paraphrasing.The remainder of this section formalizes the concept of a paraphrase, scopes outthe coverage of this survey?s discussion, and provides broader context and motivationby discussing applications in which paraphrase generation has proven useful, alongwith examples.
Section 2 briefly describes the tasks of paraphrase recognition andtextual entailment and their relationship to paraphrase generation and extraction.
Sec-tion 3 forms the major contribution of this survey by examining various corpora-basedtechniques for paraphrase generation, organized by corpus type.
Section 4 examinesrecent work done to construct various types of paraphrase corpora and to elicit humanjudgments for such corpora.
Section 5 considers the task of evaluating the performanceof paraphrase generation and extraction techniques.
Finally, Section 6 provides a briefglimpse of the future trends in paraphrase generation and Section 7 concludes thesurvey with a summary.1.1 What is a Paraphrase?The concept of paraphrasing is most generally defined on the basis of the principle ofsemantic equivalence: A paraphrase is an alternative surface form in the same languageexpressing the same semantic content as the original form.
Paraphrases may occur atseveral levels.Individual lexical items having the same meaning are usually referred to as lexicalparaphrases or, more commonly, synonyms, for example, ?hot, warm?
and ?eat, consume?.However, lexical paraphrasing cannot be restricted strictly to the concept of synonymy.There are several other forms such as hyperonymy, where one of the words in theparaphrastic relationship is either more general or more specific than the other, forexample, ?reply, say?
and ?landlady, hostess?.The term phrasal paraphrase refers to phrasal fragments sharing the same semanticcontent.
Although these fragments most commonly take the form of syntactic phrases(?work on, soften up?
and ?take over, assume control of ?)
they may also be patterns withlinked variables, for example, ?Y was built by X, X is the creator of Y?.Two sentences that represent the same semantic content are termed sententialparaphrases, for example, ?I finished my work, I completed my assignment?.
Although it ispossible to generate very simple sentential paraphrases by simply substituting wordsand phrases in the original sentence with their respective semantic equivalents, it issignificantly more difficult to generate more interesting ones such as ?He needed to makea quick decision in that situation, The scenario required him to make a split-second judgment?.Culicover (1968) describes some common forms of sentential paraphrases.1.2 Scope of DiscussionThe idea of paraphrasing has been explored in conjunction with, and employed in, alarge number of natural language processing applications.
Given the difficulty inherent342Madnani and Dorr Generating Phrasal and Sentential Paraphrasesin surveying such a diverse task, an unfortunate but necessary remedy is to imposecertain limits on the scope of our discussion.
In this survey, we will be restricting ourdiscussion to only automatic acquisition of phrasal paraphrases (including paraphrasticpatterns) and on generation of sentential paraphrases.
More specifically, this entails theexclusion of certain categories of paraphrasing work.
However, as a compromise forthe interested reader, we do include a relatively comprehensive list of references in thissection for the work that is excluded from the survey.For one, we do not discuss paraphrasing techniques that rely primarily onknowledge-based resources such as dictionaries (Wallis 1993; Fujita et al 2004), hand-written rules (Fujita et al 2007), and formal grammars (McKeown 1979; Dras 1999;Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005).
We also refrain from dis-cussing work on purely lexical paraphrasing which usually comprises various ways tocluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira,Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al 2001; Glickman andDagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasingmethods obviously implies that other lexical methods developed just for specificapplications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, andCollin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006).
Methods atthe other end of the spectrum that paraphrase supra-sentential units such as paragraphsand entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami2001; Hallett and Scott 2005; Power and Scott 2005).
Finally, we also do not discuss thenotion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002).1.3 Applications of Paraphrase GenerationBefore describing the techniques used for paraphrasing, it is essential to examine thebroader context of the application of paraphrases.
For some of the applications wediscuss subsequently, the use of paraphrases in the manner described may not yet bethe norm.
However, wherever applicable, we cite recent research that promises gainsin performance by using paraphrases for these applications.
Also note that we onlydiscuss those paraphrasing techniques that can generate the types of paraphrases underexamination in this survey: phrasal and sentential.1.3.1 Query and Pattern Expansion.
One of the most common applications of paraphrasingis the automatic generation of query variants for submission to information retrievalsystems or of patterns for submission to information extraction systems.
Culicover(1968) describes one of the earliest theoretical frameworks for query keyword expansionusing paraphrases.
One of the earliest works to implement this approach (Spa?rck-Jones and Tait 1984) generates several simple variants for compound nouns in queriessubmitted to a technical information retrieval system.
For example:Original : circuit detailsVariant 1 : details about the circuitVariant 2 : the details of circuits1 Inferring words to be similar based on similar contexts can be thought of as the most common instanceof employing distributional similarity.
The concept of distributional similarity also turns out to be quiteimportant for phrasal paraphrase generation and is discussed in more detail in Section 3.1.343Computational Linguistics Volume 36, Number 3In fact, in recent years, the information retrieval community has extensively exploredthe task of query expansion by applying paraphrasing techniques to generate similar orrelated queries (Beeferman and Berger 2000; Jones et al 2006; Sahami and Hellman 2006;Metzler, Dumais, and Meek 2007; Shi and Yang 2007).
The generation of paraphrases inthese techniques is usually effected by utilizing the query log (a log containing the recordof all queries submitted to the system) to determine semantic similarity.
Jacquemin(1999) generates morphological, syntactic, and semantic variants for phrases in theagricultural domain.
For example:Original : simultaneous measurementsVariant : concurrent measuresOriginal : development areaVariant : area of growthRavichandran and Hovy (2002) use semi-supervised learning to induce several para-phrastic patterns for each question type and use them in an open-domain questionanswering system.
For example, for the INVENTOR question type, they generate:Original : X was invented by YVariant 1 : Y?s invention of XVariant 2 : Y, inventor of XRiezler et al (2007) expand a query by generating n-best paraphrases for the query(via a pivot-based sentential paraphrasing model employing bilingual parallel corpora,detailed in Section 3) and then using any new words introduced therein as additionalquery terms.
For example, for the query how to live with cat allergies, they may generatethe following two paraphrases.
The novel words in the two paraphrases are highlightedin bold and are used to expand the original query:P1 : ways to live with feline allergyP2 : how to deal with cat allergensFinally, paraphrases have also been used to improve the task of relation extraction(Romano et al 2006).
Most recently, Bhagat and Ravichandran (2008) collect paraphras-tic patterns for relation extraction by applying semi-supervised paraphrase inductionto a very large monolingual corpus.
For example, for the relation of ?acquisition,?
theycollect:Original : X agreed to buy YVariant 1 : X completed its acquisition of YVariant 2 : X purchased Y1.3.2 Expanding Sparse Human Reference Data for Evaluation.
A large percentage of NLPapplications are evaluated by having human annotators or subjects carry out the same344Madnani and Dorr Generating Phrasal and Sentential Paraphrasestask for a given set of data and using the output so created as a reference against whichto measure the performance of the system.
The two applications where comparisonagainst human-authored reference output has become the norm are machine translationand document summarization.In machine translation evaluation, the translation hypotheses output by a ma-chine translation system are evaluated against reference translations created by humantranslators by measuring the n-gram overlap between the two (Papineni et al 2002).However, it is impossible for a single reference translation to capture all possibleverbalizations that can convey the same semantic content.
This may unfairly penalizetranslation hypotheses that have the same meaning but use n-grams that are not presentin the reference.
For example, the given system output S will not have a high scoreagainst the reference R even though it conveys precisely the same semantic content:S: We must consider the entire community.R: We must bear in mind the community as a whole.One solution is to use multiple reference translations, which is expensive.
An alternativesolution, tried in a number of recent approaches, is to address this issue by allowing theevaluation process to take into account paraphrases of phrases in the reference trans-lation so as to award credit to parts of the translation hypothesis that are semantically,even if not lexically, correct (Owczarzak et al 2006; Zhou, Lin, and Hovy 2006).In evaluation of document summarization, automatically generated summaries(peers) are also evaluated against reference summaries created by human authors(models).
Zhou et al (2006) propose a new metric called ParaEval that leverages anautomatically extracted database of phrasal paraphrases to inform the computation ofn-gram overlap between peer summaries and multiple model summaries.1.3.3 Machine Translation.
Besides being used in evaluation of machine translation sys-tems, paraphrasing has also been applied to directly improve the translation process.Callison-Burch, Koehn, and Osborne (2006) use automatically induced paraphrases toimprove a statistical phrase-based machine translation system.
Such a system works bydividing the given sentence into phrases and translating each phrase individually bylooking up its translation in a table.
The coverage of the translation system is improvedby allowing any source phrase that does not have a translation in the table to usethe translation of one of its paraphrases.
For example, if a given Spanish sentencecontains the phrase presidente de Brazil but the system does not have a translation forit, another Spanish phrase such as presidente brasilen?o may be automatically detected asa paraphrase of presidente de Brazil; then if the translation table contains a translation forthe paraphrase, the system can use the same translation for the given phrase.
Therefore,paraphrasing allows the translation system to properly handle phrases that it does nototherwise know how to translate.Another important issue for statistical machine translation systems is that ofreference sparsity.
The fundamental problem that translation systems have to face isthat there is no such thing as the correct translation for any sentence.
In fact, any givensource sentence can often be translated into the target language in many valid ways.Because there can be many ?correct answers,?
almost all models employed by SMTsystems require, in addition to a large bitext, a held-out development set comprisingmultiple high-quality, human-authored reference translations in the target language inorder to tune their parameters relative to a translation quality metric.
However, given345Computational Linguistics Volume 36, Number 3the time and cost implications of such a process, most such data sets usually haveonly a single reference translation.
Madnani et al (2007, 2008b) generate sententialparaphrases and use them to expand the available reference translations for such setsso that the machine translation system can learn a better set of system parameters.2.
Paraphrase Recognition and Textual EntailmentA problem closely related to, and as important as, generating paraphrases is that ofassigning a quantitative measurement to the semantic similarity of two phrases (Fujitaand Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner andKatz 2005).
A more complex formulation of the task would be to detect or recognizewhich sentences in the two texts are paraphrases of each other (Brockett and Dolan2005; Marsi and Krahmer 2005a; Wu 2005; Joa`o, Das, and Pavel 2007a, 2007b; Das andSmith 2009; Malakasiotis 2009).
Both of these task formulations fall under the categoryof paraphrase detection or recognition.
The latter formulation of the task has becomepopular in recent years (Dolan and Dagan 2005) and paraphrase generation techniquesthat require monolingual parallel or comparable corpora (discussed in Section 3) canbenefit immensely from this task.
In general, paraphrase recognition can be very helpfulfor several NLP applications.
Two examples of such applications are text-to-text gener-ation and information extraction.Text-to-text generation applications rely heavily on paraphrase recognition.
For amulti-document summarization system, detecting redundancy is a very important con-cern because two sentences from different documents may convey the same semanticcontent and it is important not to repeat the same information in the summary.
Onthis note, Barzilay and McKeown (2005) exploit the redundancy present in a given setof sentences by detecting paraphrastic parts and fusing them into a single coherentsentence.
Recognizing similar semantic content is also critical for text simplificationsystems (Marsi and Krahmer 2005b).Information extraction enables the detection of regularities of informationstructure?events which are reported many times, about different individuals and indifferent forms?and making them explicit so that they can be processed and used inother ways.
Sekine (2006) shows how to use paraphrase recognition to cluster togetherextraction patterns to improve the cohesion of the extracted information.Another recently proposed natural language processing task is that of recognizingtextual entailment: A piece of text T is said to entail a hypothesis H if humans readingT will infer that H is most likely true.
The observant reader will notice that, in this sense,the task of paraphrase recognition can simply be formulated as bidirectional entailmentrecognition.
The task of recognizing entailment is an application-independent task andhas important ramifications for almost all other language processing tasks that canderive benefit from some form of applied semantic inference.
For this reason, the taskhas received noticeable attention in the research community and annual community-wide evaluations of entailment systems have been held in the form of the RecognizingTextual Entailment (RTE) Challenges (Dagan, Glickman, and Magnini 2006; Bar-Haimet al 2007; Sekine et al 2007; Giampiccolo et al 2008).Looking towards the future, Dagan (2008) suggests that the textual entailment taskprovides a comprehensive framework for semantic inference and argues for buildinga concrete inference engine that not only recognizes entailment but also searches forall entailing texts given an entailment hypothesis H and, conversely, generates allentailed statements given a text T. Given such an engine, Dagan claims that paraphrase346Madnani and Dorr Generating Phrasal and Sentential Paraphrasesgeneration is simply a matter of generating all entailed statements given any sentence.Although this is a very attractive proposition that defines both paraphrase generationand recognition in terms of textual entailment, there are some important caveats.
Forexample, textual entailment cannot guarantee that the entailed hypothesis H capturesall of the same meaning as the given text T. Consider the following example:T: Yahoo?s buyout of Overture was finalized.H1: Yahoo bought Overture.H2: Overture is now owned by Yahoo.Although both H1 and H2 are entailed by T, they are not strictly paraphrases of Tbecause some of the semantic content has not been carried over.
This must be animportant consideration when building the proposed entailment engine.
Of course,even these approximately semantically equivalent constructions may prove useful inan appropriate downstream application.The relationship between paraphrasing and entailment is more tightly entwinedthan it might appear.
Entailment recognition systems sometimes rely on the use ofparaphrastic templates or patterns as inputs (Iftene 2009) and might even use para-phrase recognition to improve their performance (Bosma and Callison-Burch 2007).In fact, examination of some RTE data sets in an attempt to quantitatively determinethe presence of paraphrases has shown that a large percentage of the set consists ofparaphrases rather than typical entailments (Bayer et al 2005; Garoufi 2007).
It hasalso been observed that, in the entailment challenges, it is relatively easy for submittedsystems to recognize constructions that partially overlap in meaning (approximatelyparaphrastic) from those that are actually bound by an entailment relation.
On theflip side, work has also been done to extend entailment recognition techniques for thepurpose of paraphrase recognition (Rus, McCarthy, and Lintean 2008).Detection of semantic similarity and, to some extent, that of bidirectional entailmentis usually an implicit part of paraphrase generation.
However, given the interestingand diverse work that has been done in both these areas, we feel that any significantdiscussion beyond the treatment above merits a separate, detailed survey.3.
Paraphrasing with CorporaIn this section, we explore in detail the data-driven paraphrase generation approachesthat have emerged and have become extremely popular in the last decade or so.
Thesecorpus-based methods have the potential of covering a much wider range of paraphras-ing phenomena and the advantage of widespread availability of corpora.We organize this section by the type of corpora used to generate the paraphrases:a single monolingual corpus, monolingual comparable corpora, monolingual parallelcorpora, and bilingual parallel corpora.
This form of organization, in our opinion, isthe most instructive because most of the algorithmic decisions made for paraphrasegeneration will depend heavily on the type of corpus used.
For instance, it is reasonableto assume that a different set of considerations will be paramount when using a largesingle monolingual corpus than when using bilingual parallel corpora.However, before delving into the actual paraphrasing methods, we believe thatit would be very useful to explain the motivation behind distributional similarity, anextremely popular technique that can be used for paraphrase generation with severaldifferent types of corpora.
We devote the following section to such an explanation.347Computational Linguistics Volume 36, Number 33.1 Distributional SimilarityThe idea that a language possesses distributional structure was first discussed at lengthby Harris (1954).
The term represents the notion that one can describe a language interms of relationships between the occurrences of its elements (words, morphemes,phonemes) relative to the occurrence of other elements.
The name for the phenomenonis derived from an element?s distribution?sets of elements in particular positions thatthe element occurs with to produce an utterance or a sentence.More specifically, Harris presents several empirical observations to support thehypothesis that such a structure exists naturally for language.
Here, we closely quotethese observations: Utterances and sentences are not produced by arbitrarily putting togetherthe elements of the language.
In fact, these elements usually occur only incertain positions relative to certain other elements. The empirical restrictions on the co-occurrents of a class are respected foreach and every one of its members and are not disregarded for arbitraryreasons. The occurrence of a member of a class relative to another member of adifferent class can be computed as a probabilistic measure, defined interms of the frequency of that occurrence in some sample or corpus. Not every member of every class can occur with every member of anotherclass (think nouns and adjectives).
This observation can be used as ameasure of difference in meaning.
For example, if the pair of words teacherand instructor is considered to be more semantically equivalent than, say,the pair teacher and musician, then the distributions of the first pair willalso be more alike than that of the latter pair.Given these observations, it is relatively easy to characterize the concept of distrib-utional similarity: words or phrases that share the same distribution?the same set ofwords in the same context in a corpus?tend to have similar meanings.Figure 1 shows the basic idea behind phrasal paraphrase generation techniques thatleverage distributional similarity.
The input corpus is usually a single or set of mono-lingual corpora (parallel or non-parallel).
After preprocessing?which may includetagging the parts of speech, generating parse trees, and other transformations?the nextstep is to extract pairs of words or phrases (or patterns) that occur in the same context inthe corpora and hence may be considered (approximately) semantically equivalent.
Thisextraction may be accomplished by several means (e.g., by using a classifier employingcontextual features or by finding similar paths in dependency trees).
Although it ispossible to stop at this point and consider this list as the final output, the list usuallycontains a lot of noise and may require additional filtering based on other criteria,such as collocations counts from another corpus (or the Web).
Finally, some techniquesmay go even further and attempt to generalize the filtered list of paraphrase pairsinto templates or rules which may then be applied to other sentences to generate theirparaphrases.
Note that generalization as a post-processing step may not be necessary ifthe induction process can extract distributionally similar patterns directly.One potential disadvantage of relying on distributional similarity is that itemsthat are distributionally similar may not necessarily end up being paraphrastic: Both348Madnani and Dorr Generating Phrasal and Sentential ParaphrasesFigure 1A general architecture for paraphrasing approaches leveraging the distributional similarityhypothesis.elements of the pairs ?boys, girls?, ?cats, dogs?, ?high, low?
can occur in similar contextsbut are not semantically equivalent.3.2 Paraphrasing Using a Single Monolingual CorpusIn this section, we concentrate on paraphrase generation methods that operate on asingle monolingual corpus.
Most, if not all, such methods usually perform paraphraseinduction by employing the idea of distributional similarity as outlined in the previoussection.
Besides the obvious caveat discussed previously regarding distributional sim-ilarity, we find that the other most important factor affecting the performance of thesemethods is the choice of distributional ingredients?that is, the features used to formu-late the distribution of the extracted units.
We consider three commonly used techniquesthat generate phrasal paraphrases (or paraphrastic patterns) from a single monolingualcorpus but use very different distributional features in terms of complexity.
The firstuses only surface-level features and the other two use features derived from additionalsemantic knowledge.
Although the latter two methods are able to generate more so-phisticated paraphrases by virtue of more specific and more informative ingredients,we find that doing so usually has an adverse effect on their coverage.Pas?ca and Dienes (2005) use as their input corpus a very large collection of Webdocuments taken from the repository of documents crawled by Google.
Although usingWeb documents as input data does require a non-trivial pre-processing phase since suchdocuments tend to be noisier, there are certainly advantages to the use of Web docu-ments as the input corpus: It does not require parallel (or even comparable) documents349Computational Linguistics Volume 36, Number 3and can allow leveraging of even larger document collections.
In addition, the extractedparaphrases are not tied to any specific domain and are suitable for general application.Algorithm 1 shows the details of the induction process.
Steps 3?6 extract all n-gramsof a specific kind from each sentence: Each n-gram has Lc words at the beginning,between M1 to M2 words in the middle, and another Lc words at the end.
Steps 7?13can intuitively be interpreted as constructing a textual anchor A?by concatenating afixed number of words from the left and the right?for each candidate paraphrase Cand storing the ?anchor, candidate?
tuple in H. These anchors are taken to constitutethe distribution of the words and phrases under inspection.
Finally, each occurrence ofa pair of potential paraphrases, that is, a pair sharing one or more anchors, is counted.The final set of phrasal paraphrastic pairs is returned.This algorithm embodies the spirit of the hypothesis of distributional similarity: Itconsiders all words and phrases that are distributionally similar?those that occur withthe same sets of anchors (or distributions)?to be paraphrases of each other.
Addition-ally, the larger the set of shared anchors for two candidate phrases, the stronger the like-lihood that they are paraphrases of each other.
After extracting the list of paraphrases,less likely phrasal paraphrases are filtered out by using an appropriate count threshold.Pas?ca and Dienes (2005) attempt to make their anchors even more informative byattempting variants where they extract the n-grams only from sentences that includespecific additional information to be added to the anchor.
For example, in one variant,they only use sentences where the candidate phrase is surrounded by named entitiesAlgorithm 1 (Pas?ca and Dienes 2005).
Induce a set of phrasal paraphrase pairs H withassociated counts from a corpus of pre-processed Web documents.Summary.
Extract all n-grams from the corpus longer than a pre-stipulated length.Compute a lexical anchor for each extracted n-gram.
Pairs of n-grams that share lexicalanchors are then construed to be paraphrases.1: Let N represent a set of n-grams extracted from the corpus2: N ?
{?
}, H ?
{?
}3: for each sentence E in the corpus do4: Extract the set of n-grams NE = {e?i s.t (2Lc + M1) ?
|e?i| ?
(2Lc + M2)}}, whereM1, M2, and Lc are all preset constants and M1 ?
M25: N ?
N ?
NE6: end for7: for each n-gram e?
in N do8: Extract the subsequence C, such that Lc ?
|C| ?
(|e?| ?
Lc ?
1)9: Extract the subsequence AL, such that 0 ?
|AL| ?
(Lc ?
1)10: Extract the subsequence AR, such that (|e?| ?
Lc) ?
|AR| ?
(|e?| ?
1)11: A ?
AL + AR12: Add the pair (A, C) to H13: end for14: for each subset of H with the same anchor A do15: Exhaustively compare each pair of tuples (A, Ci) and (A, Cj) in this subset16: Update the count of the candidate paraphrase pair (Ci, Cj) by 117: Update the count of the candidate paraphrase pair (Cj, Ci) by 118: end for19: Output H containing paraphrastic pairs and their respective counts350Madnani and Dorr Generating Phrasal and Sentential Paraphraseson both sides and they attach the nearest pair of entities to the anchor.
As expected, theparaphrases do improve in quality as the anchors become more specific.
However, theyalso report that as anchors are made more specific by attaching additional information,the likelihood of finding a candidate pair with the same anchor is reduced.The ingredients for measuring distributional similarity in a single corpus can cer-tainly be more complex than simple phrases used by Pas?ca and Dienes.
Lin and Pantel(2001) discuss how to measure distributional similarity over dependency tree paths inorder to induce generalized paraphrase templates such as:2X found answer to Y ?
X solved YX caused Y ?
Y is blamed on XWhereas single links between nodes in a dependency tree represent direct semanticrelationships, a sequence of links, or a path, can be understood to represent an indirectrelationship.
Here, a path is named by concatenating the dependency relationships andlexical items along the way but excluding the lexical items at the end.
In this way, apath can actually be thought of as a pattern with variables at either end.
Consider thefirst dependency tree in Figure 2.
One dependency path that we could extract would bebetween the node John and the node problem.
We start at John and see that the first itemin the tree is the dependency relation subject that connects a noun to a verb and so weappend that information to the path.3 The next item in the tree is the word found andwe append its lemma (find) to the path.
Next is the semantic relation object connecting averb to a noun and we append that.
The process continues until we reach the other slot(the word problem) at which point we stop.4 The extracted path is shown below the tree.Similarly, we can extract a path for the second dependency tree.
Let?s briefly mentionthe terminology associated with such paths: The relations on either end of a path are referred to as SlotX and SlotY. The tuples (SlotX, John) and (SlotY, problem) are known as the two featuresof the path. The dependency relations inside the path that are not slots are termedinternal relations.Intuitively, one can imagine a path to be a complex representation of the pattern X findsanswer to Y, where X and Y are variables.
This representation for a path is a perfect fit foran extended version of the distributional similarity hypothesis: If similar sets of wordsfill the same variables for two different patterns, then the patterns may be considered tohave similar meaning, which is indeed the case for the paths in Figure 2.Lin and Pantel (2001) use newspaper text as their input corpus and create depen-dency parses for all the sentences in the corpus in the pre-processing step.
Algorithm 2provides the details of the rest of the process: Steps 1 and 2 extract the paths andcompute their distributional properties, and Steps 3?14 extract pairs of paths which are2 Technically, these templates represent inference rules, such that the right-hand side can be inferred fromthe left-hand side but is not semantically equivalent to it.
This form of inference is closely related to thatexhibited in textual entailment.
This work is primarily concerned with inducing such rules rather thanstrict paraphrases.3 Although the first item is the word John, the words at either end are, by definition, considered slots andnot included in the path.4 Any relations not connecting two content words, such as determiners and auxiliaries, are ignored.351Computational Linguistics Volume 36, Number 3Figure 2Two different dependency tree paths (a and b) that are considered paraphrastic because the samewords ( John and problem) are used to fill the corresponding slots (shown co-indexed) in both thepaths.
The implied meaning of each dependency path is also shown.similar, insofar as such properties are concerned.5 At the end, we have sets of paths (orinference rules) that are considered to have similar meanings by the algorithm.The performance of their dependency-path based algorithm depends heavily on theroot of the extracted path.
For example, whereas verbs frequently tend to have severalmodifiers, nouns tend to have no more than one.
However, if a word has any fewer thantwo modifiers, no path can go through it as the root.
Therefore, the algorithm tends toperform better for paths with verbal roots.
Another issue is that this algorithm, despitethe use of more informative distributional features, can generate several incorrect or im-plausible paraphrase patterns (inference rules).
Recent work has shown how to filter outincorrect inferences when using them in a downstream application (Pantel et al 2007).Finally, there is no reason for the distributional features to be in the same languageas the one in which the paraphrases are desired.
Wu and Zhou (2003) describe a5 A demo of the algorithm is available online at http://demo.patrickpantel.com/Content/LexSem/paraphrase.htm.352Madnani and Dorr Generating Phrasal and Sentential ParaphrasesAlgorithm 2 (Lin and Pantel 2001).
Produce inference rules from a parsed corpus.Summary.
Adapt Harris?s (1954) hypothesis of distributional similarity for paths independency trees: If two tree paths have similar distributions such that they tend tolink the same set of words, then they likely mean the same thing and together generatean inference rule.1: Extract paths of the form described above from the parsed corpus2: Initialize a hash H that stores, for each tuple of the form (p, s, w)?where p is a path,s is one of the two slots in p, and w is a word that appears in that slot?the followingtwo quantities:(a) A count C(p, s, w) indicating how many times word w appeared in slots in path p(b) The mutual information I(p, s, w) indicating the strength of associationbetween slot s and word w in path p:I(p, s, w) = log(C(p, s, w)?p?,w?
C(p?, s, w?)?w?
C(p, s, w?)?p?
C(p?, s, w))3: for each extracted path p do4: Find all instances (p, w1, w2) such that p connects the words w1 and w25: for each such instance do6: Update C(p, SlotX, w1) and I(p, SlotX, w1) in H7: Update C(p, SlotY, w2) and I(p, SlotY, w2) in H8: end for9: end for10: for each extracted path p do11: Create a candidate set C of similar paths by extracting all paths from H that shareat least one feature with p12: Prune candidates from C based on feature overlap with p13: Compute the similarity between p and the remaining candidates in C. The simi-larity is defined in terms of the various values of mutual information I betweenthe paths?
two slots and all the words that appear in those slots14: Output all paths in C sorted by their similarity to p15: end forbilingual approach to extract English relation-based paraphrastic patterns of the form?w1, R, w2?, where w1 and w2 are English words connected by a dependency link withthe semantic relation R. Figure 3 shows a simple example based on their approach.
First,instances of one type of pattern are extracted from a parsed monolingual corpus.
In thefigure, for example, a single instance of the pattern ?verb, IN, pobj?
has been extracted.Several new, potentially paraphrastic, English candidate patterns are then induced byreplacing each of the English words with its synonyms in WordNet, one at a time.
Thefigure shows the list of induced patterns for the given example.
Next, each of the Englishwords in each candidate pattern is translated to Chinese, via a bilingual dictionary.66 The semantic relation R is deemed to be invariant under translation.353Computational Linguistics Volume 36, Number 3Figure 3Using Chinese translations as the distributional elements to extract a set of English paraphrasticpatterns from a large English corpus.Given that the bilingual dictionary may contain multiple Chinese translations for agiven English word, several Chinese patterns may be created for each English candidatepattern.
Each Chinese pattern is assigned a probability value via a simple bag-of-wordstranslation model (built from a small bilingual corpus) and a language model (trainedon a Chinese collocation database); all translated patterns, along with their probabilityvalues, are then considered to be features of the particular English candidate pattern.Any English pattern can subsequently be compared to another by computing cosinesimilarity over their shared features.
English collocation pairs whose similarity valueexceeds some threshold are construed to be paraphrastic.The theme of a trade-off between the precision of the generated paraphrase set?byvirtue of the increased informativeness of the distributional features?and its coverageis seen in this work as well.
When using translations from the bilingual dictionary, aknowledge-rich resource, the authors report significantly higher precision than compa-rable methods that rely only on monolingual information to compute the distributionalsimilarity.
Predictably, they also find that recall values obtained with their dictionary-based method are lower than those obtained by other methods.Paraphrase generation techniques using a single monolingual corpus have to relyon some form of distributional similarity because there are no explicit clues availablethat indicate semantic equivalence.
In the next section, we look at paraphrasing methodsoperating over data that does contain such explicit clues.3.3 Paraphrasing Using Monolingual Parallel CorporaIt is also possible to generate paraphrastic phrase pairs from a parallel corpus whereeach component of the corpus is in the same language.
Obviously, the biggest advantageof parallel corpora is that the sentence pairs are paraphrases almost by definition; theyrepresent different renderings of the same meaning created by different translatorsmaking different lexical choices.
In effect, they contain pairs (or sets) of sentences354Madnani and Dorr Generating Phrasal and Sentential Paraphrasesthat are either semantically equivalent (sentential paraphrases) or have significant se-mantic overlap.
Extraction of phrasal paraphrases can then be effected by extractingphrasal correspondences from a set of sentences that represent the same (or similar)semantic content.
We present four techniques in this section that generate paraphrasesby finding such correspondences.
The first two techniques attempt to do so by relying,again, on the paradigm of distributional similarity: one by positing a bootstrappingdistributional similarity algorithm and the other by simply adapting the previouslydescribed dependency path similarity algorithm to work with a parallel corpus.
Thenext two techniques rely on more direct, non-distributional methods to compute therequired correspondences.Barzilay and McKeown (2001) align phrasal correspondences by attempting tomove beyond a single-pass distributional similarity method.
They propose a bootstrap-ping algorithm that allows for the gradual refinement of the features used to determinesimilarity and yields improved paraphrase pairs.
As their input corpus, they use mul-tiple human-written English translations of literary texts such as Madame Bovary andTwenty Thousand Leagues Under the Sea that are expected to be rich in paraphrastic ex-pressions because different translators would use their own words while still preservingthe meaning of the original text.
The parallel components are obtained by performingsentence alignment (Gale and Church 1991) on the corpora to obtain sets of parallelsentences that are then lemmatized, part-of-speech tagged and chunked in order toidentify all the verb and noun phrases.
The bootstrapping algorithm is then employedto incrementally learn better and better contextual features that are then leveraged togenerate semantically similar phrasal correspondences.Figure 4 shows the basic steps of the algorithm.
To seed the algorithm, some fakeparaphrase examples are extracted by using identical words from either side of thealigned sentence pair.
For example, given the following sentence pair:S1: Emma burst into tears and he tried to comfort her.S2: Emma cried and he tried to console her.Figure 4A bootstrapping algorithm to extract phrasal paraphrase pairs from monolingual parallel corpora.355Computational Linguistics Volume 36, Number 3?tried, tried?, ?her, her?
may be extracted as positive examples and ?tried, Emma?, ?tried,console?
may be extracted as negative examples.
Once the seeding examples are ex-tracted, the next step is to extract contextual features for both the positive and thenegative examples.
These features take the form of aligned part-of-speech sequences ofa given length from the left and the right of the example.
For instance, we can extract thecontextual feature [?L1 : PRP1, R1 : TO1?, ?L2 : PRP1, R2 : TO1?]
of length 1 for the positiveexample ?tried, tried?.
This particular contextual feature contains two tuples, one foreach sentence.
The first tuple ?L1 : PRP1, R1 : TO1?
indicates that, in the first sentence, thePOS tag sequence to the left of the word tried is a personal pronoun (he) and the POStag sequence to the right of tired is the preposition to.
The second tuple is identical forthis case.
Note that the tags of identical tokens are indicated as such by subscripts on thePOS tags.
All such features are extracted for both the positive and the negative examplesfor all lengths less than or equal to some specified length.
In addition, a strength valueis calculated for each positive (negative) contextual feature f using maximum likelihoodestimation as follows:strength( f ) =Number of positive (negative) examples surrounded by fTotal occurrences of fThe extracted list of contextual features is thresholded on the basis of this strengthvalue.
The remaining contextual rules are then applied to the corpora to obtain addi-tional positive and negative paraphrase examples that, in turn, lead to more refinedcontextual rules, and so on.
The process is repeated for a fixed number of iterations oruntil no new paraphrase examples are produced.
The list of extracted paraphrases atthe end of the final iteration represents the final output of the algorithm.
In total, about9, 000 phrasal (including lexical) paraphrases are extracted from 11 translations of fiveworks of classic literature.
Furthermore, the extracted paraphrase pairs are also gener-alized into about 25 patterns by extracting part-of-speech tag sequences correspondingto the tokens of the paraphrase pairs.Barzilay and McKeown also perform an interesting comparison with another tech-nique that was originally developed for compiling translation lexicons from bilingualparallel corpora (Melamed 2001).
This technique first compiles an initial lexicon usingsimple co-occurrence statistics and then uses a competitive linking algorithm (Melamed1997) to improve the quality of the lexicon.
The authors apply this technique to theirmonolingual parallel data and observe that the extracted paraphrase pairs are of muchlower quality than the pairs extracted by their own method.
We present similar obser-vations in Section 3.5 and highlight that although more recent translation techniques?specifically ones that use phrases as units of translation?are better suited to the taskof generating paraphrases than the competitive linking approach, they continue tosuffer from the same problem of low precision.
On the other hand, such techniquescan take good advantage of large bilingual corpora and capture a much larger varietyof paraphrastic phenomena.Ibrahim, Katz, and Lin (2003) propose an approach that applies a modified versionof the dependency path distributional similarity algorithm proposed by Lin and Pantel(2001) to the same monolingual parallel corpus (multiple translations of literary works)used by Barzilay and McKeown (2001).
The authors claim that their technique is moretractable than Lin and Pantel?s approach since the sentence-aligned nature of the inputparallel corpus obviates the need to compute similarity over tree paths drawn fromsentences that have zero semantic overlap.
Furthermore, they also claim that theirtechnique exploits the parallel nature of a corpus more effectively than Barzilay and356Madnani and Dorr Generating Phrasal and Sentential ParaphrasesMcKeown?s approach simply because their technique uses tree paths and not just lexicalinformation.
Specifically, they propose the following modifications to Lin and Pantel?salgorithm:1.
Extracting tree paths with aligned anchors.
Rather than using a singlecorpus and comparing paths extracted from possibly unrelated sentences,the authors leverage sentence-aligned monolingual parallel corpora; thesame as used in Barzilay and McKeown (2001).
For each sentence in analigned pair, anchors are identified.
The anchors from both sentences arebrought into alignment.
Once anchor pairs on either side have beenidentified and aligned, a breadth-first search algorithm is used to find theshortest path between the anchor nodes in the dependency trees.
All pathsfound between anchor pairs for a sentence pair are taken to bedistributionally?and, hence, semantically?similar.2.
Using a sliding frequency measure.
The original dependency-basedalgorithm (Lin and Pantel 2001) weights all subsequent occurrences of thesame paraphrastic pair of tree paths as much as the first one.
In thisversion, every successive induction of a paraphrastic pair using the sameanchor pair is weighted less than the previous one.
Specifically, inducingthe same paraphrase pair using an anchor pair that has already been seenonly counts for 12n , where n is the number of times the specific anchor pairhas been seen so far.
Therefore, induction of a path pair using new anchorsis better evidence that the pair is paraphrastic, as opposed to the repeatedinduction of the path pair from the same anchor over and over again.Despite the authors?
claims, they offer no quantitative evaluation comparing theirparaphrases with those from Lin and Pantel (2001) or from Barzilay and McKeown(2001).It is also possible to find correspondences between the parallel sentences using amore direct approach instead of relying on distributional similarity.
Pang, Knight, andMarcu (2003) propose an algorithm to align sets of parallel sentences driven entirelyby the syntactic representations of the sentences.
The alignment algorithm outputs amerged lattice from which lexical, phrasal, and sentential paraphrases can simply beread off.
More specifically, they use the Multiple-Translation Chinese corpus that wasoriginally developed for machine translation evaluation and contains 11 human-writtenEnglish translations for each sentence in a news document.
Using several sentencesexplicitly equivalent in semantic content has the advantage of being a richer sourcefor paraphrase induction.As a pre-processing step, any group (of 11 sentences) that contains sentences longerthan 45 words is discarded.
Next, each sentence in each of the groups is parsed.
Allthe parse trees are then iteratively merged into a shared forest.
The merging algo-rithm proceeds top-down and continues to recursively merge constituent nodes thatare expanded identically.
It stops upon reaching the leaves or upon encountering thesame constituent node expanded using different grammar rules.
Figure 5(a) showshow the merging algorithm would work on two simple parse trees.
In the figure, itis apparent that the leaves of the forest encode paraphrasing information.
However,the merging only allows identical constituents to be considered as paraphrases.
Inaddition, keyword-based heuristics need to be employed to prevent inaccurate mergingof constituent nodes due to, say, alternations of active and passive voices among the357Computational Linguistics Volume 36, Number 3Figure 5The merging algorithm.
(a) How the merging algorithm works for two simple parse trees toproduce a shared forest.
Note that for clarity, not all constituents are expanded fully.
Leaf nodeswith two entries represent paraphrases.
(b) The word lattice generated by linearizing the forestin (a).sentences in the group.
Once the forest is created, it is linearized to create the wordlattice by traversing the nodes in the forest top-down and producing an alternativepath in the lattice for each merged node.
Figure 5(b) shows the word lattice generatedfor the simple two-tree forest.
The lattices also require some post-processing to removeredundant edges and nodes that may have arisen due to parsing errors or limitations inthe merging algorithm.
The final output of the paraphrasing algorithm is a set of wordlattices, one for each sentence group.These lattices can be used as sources of lexical as well as phrasal paraphrases.
Allalternative paths between any pair of nodes can be considered to be paraphrases ofeach other.
For example, besides the obvious lexical paraphrases, the paraphrase pair?ate at cafe, chowed down at bistro?
can also be extracted from the lattice in Figure 5(b).In addition, each path between the START and the END nodes in the lattice represents asentential paraphrase of the original 11 sentences used to create the lattice.The direct alignment approach is able to leverage the sheer width (number ofparallel alternatives per sentence position; 11 in this case) of the input corpus to doaway entirely with any need for measuring distributional similarity.
In general, it hasseveral advantages.
It can capture a very large number of paraphrases: Each lattice hason the order of hundreds or thousands of paths depending on the average length ofthe sentence group that it was generated from.
In addition, the paraphrases producedare of better quality than other approaches employing parallel corpora for paraphraseinduction discussed so far.
However, the approach does have a couple of drawbacks: No paraphrases for unseen data.
The lattices cannot be applied to newsentences for generating paraphrases because no form of generalizationis performed to convert lattices into patterns.358Madnani and Dorr Generating Phrasal and Sentential Paraphrases Requirement of a large number of human-written translations.
Each ofthe lattices described is built using 11 manually written translations of thesame sentence, each by a different translator.
There are very few corporathat provide such a large number of human translations.
In recent years,most MT corpora have had no more than four references, which wouldcertainly lead to much sparser word lattices and smaller numbers ofparaphrases that can be extracted.
In fact, given the cost and amount ofeffort required for humans to translate a relatively large corpus, it iscommon to encounter corpora with only a single human translation.With such a corpus, of course, this technique would be unable to produceany paraphrases.
One solution might be to augment the relatively fewhuman translations with translations obtained from automatic machinetranslation systems.
In fact, the corpus used (Huang, Graff, andDoddington 2002) also contains, besides the 11 human translations,6 translations of the same sentence by machine translation systemsavailable on the Internet at the time.
However, no experiments areperformed with the automatic translations.Finally, an even more direct method to align equivalences in parallel sentence pairscan be effected by building on word alignment techniques from the field of statisticalmachine translation (Brown et al 1990).
Current state-of-the-art SMT methods rely onunsupervised induction of word alignment between two bilingual parallel sentences toextract translation equivalences that can then be used to translate a given sentence inone language into another language.
The same methods can be applied to monolingualparallel sentences without any loss of generality.
Quirk, Brockett, and Dolan (2004)use one such method to extract phrasal paraphrase pairs.
Furthermore, they use theseextracted phrasal pairs to construct sentential paraphrases for new sentences.Mathematically, Quirk, Brockett, and Dolan?s approach to sentential paraphrasegeneration may be expressed in terms of the typical channel model equation forstatistical machine translation:E?p = arg maxEpP(Ep|E) (1)The equation denotes the search for the optimal paraphrase Ep for a given sentence E.We may use Bayes?
Theorem to rewrite this as:E?p = arg maxEpP(Ep) P(E|Ep)where P(Ep) is an n-gram language model providing a probabilistic estimate of thefluency of a hypothesis Ep and P(E|Ep) is the translation model, or more appropriatelyfor paraphrasing, the replacement model, providing a probabilistic estimate of what isessentially the semantic adequacy of the hypothesis paraphrase.
Therefore, the optimalsentential paraphrase may loosely be described as one that fluently captures most, ifnot all, of the meaning contained in the input sentence.It is important to provide a brief description of the parallel corpus used here becauseunsupervised induction of word alignments typically requires a relatively large numberof parallel sentence pairs.
The monolingual parallel corpus (or more accurately, quasi-parallel, since not all sentence pairs are fully semantically equivalent) is constructedby scraping on-line news sites for clusters of articles on the same topic.
Such clusters359Computational Linguistics Volume 36, Number 3contain the full text of each article and the dates and times of publication.
After re-moving the mark-up, the authors discard any pair of sentences in a cluster where thedifference in the lengths or the edit distance is larger than some stipulated value.
Thismethod yields a corpus containing approximately 140, 000 quasi-parallel sentence pairs{(E1, E2)}, where E1 = e11e21 .
.
.
em1 , E2 = e12e22 .
.
.
en2.
The following examples show that theproposed method can work well:S1: In only 14 days, U.S. researchers have created an artificial bacteria-eating virusfrom synthetic genes.S2: An artificial bacteria-eating virus has been made from synthetic genes in therecord time of just two weeks.S1: The largest gains were seen in prices, new orders, inventories, and exports.S2: Sub-indexes measuring prices, new orders, inventories, and exports increased.For more details on the creation of this corpus, we refer the reader to Dolan, Quirk,and Brockett (2004) and, more specifically, to Section 4.
Algorithm 3 shows how toAlgorithm 3 (Quirk, Dolan, and Brockett 2004).
Generate a set M of phrasal para-phrases with associated likelihood values from a monolingual parallel corpus C.Summary.
Estimate a simple English to English phrase translation model from C usingword alignments.
Use this model to create sentential paraphrases as explained later.1: M ?
{?
}2: Compute lexical replacement probabilities P(e1|e2) from all sentence pairs in C viaIBM Model 1 estimation3: Compute a set of word alignments {a} such that for each sentence pair (E1, E2)a = a1a2 .
.
.
amwhere ai ?
{0 .
.
.
n}, m = |E1|, n = |E2|4: for each word-aligned sentence pair (E1, E2)a in C do5: Extract pairs of contiguous subsequences (e?1, e?2) such that:(a) |e?1| ?
5, |e?2| ?
5(b) ?i ?
{1, .
.
.
, |e?1|} ?j ?
{1, .
.
.
, |e?2|}, e1,ia?
e2,j(c) ?i ?
{1, .
.
.
, |e?2|} ?j ?
{1, .
.
.
, |e?1|}, e2,ia?
e1,j6: Add all extracted pairs to M7: end for8: for each paraphrase pair (e?1, e?2) in M do9: Compute P(e?1|e?2) =?e j1?e?1?ek2?e?2P(e j1|ek2)10: end for11: Output M containing paraphrastic pairs and associated probabilities360Madnani and Dorr Generating Phrasal and Sentential Paraphrasesgenerate a set of phrasal paraphrase pairs and compute a probability value for eachsuch pair.
In Step 2, a simple parameter estimation technique (Brown et al 1993) is usedto compute, for later use, the probability of replacing any given word with another.Step 3 computes a word alignment (indicated by a) between each pair of sentences.
Thisalignment indicates for each word ei in one string that word ej in the other string fromwhich it was most likely produced (denoted here by eia?
ej).
Steps 4?7 extract, from eachpair of sentences, pairs of short contiguous phrases that are aligned with each otheraccording to this alignment.
Note that each such extracted pair is essentially a phrasalparaphrase.
Finally, a probability value is computed for each such pair by assuming thateach word of the first phrase can be replaced with each word of the second phrase.
Thiscomputation uses the lexical replacement probabilities computed in Step 2.Now that a set of scored phrasal pairs has been extracted, these pairs can be used togenerate paraphrases for any unseen sentence.
Generation proceeds by creating a latticefor the given sentence.
Given a sentence E, the lattice is populated as follows:1.
Create |E| + 1 vertices q0, q1 .
.
.
q|E|.2.
Create N edges between each pair of vertices qj and qk ( j < k) such that N =the number of phrasal paraphrases for the input phrase e( j+1)e( j+2) .
.
.
ek.Label each edge with the phrasal paraphrase string itself and itsprobability value.
Each such edge denotes a possible paraphrasing of theabove input phrase by the replacement model.3.
Add the edges {(qj?1, qj)} and label each edge with the token sj and aconstant u.
This is necessary to handle words from the sentence that donot occur anywhere in the set of paraphrases.Figure 6 shows an example lattice.
Once the lattice has been constructed, it is straight-forward to extract the 1-best paraphrase by using a dynamic programming algorithmsuch as Viterbi decoding and extracting the optimal path from the lattice as scored by theproduct of an n-gram language model and the replacement model.
In addition, as withSMT decoding, it is also possible to extract a list of n-best paraphrases from the latticeby using the appropriate algorithms (Soong and Huang 1990; Mohri and Riley 2002).Quirk, Brockett, and Dolan (2004) borrow from the statistical machine translationliterature so as to align phrasal equivalences as well as to utilize the aligned phrasalequivalences to rewrite new sentences.
The biggest advantage of this method is itsSMT inheritance: It is possible to produce multiple sentential paraphrases for any newFigure 6A paraphrase generation lattice for the sentence He ate lunch at a cafe near Paris.
Alternate pathsbetween various nodes represent phrasal replacements.
The probability values associated witheach edge are not shown for the sake of clarity.361Computational Linguistics Volume 36, Number 3sentence, and there is no limit on the number of sentences that can be paraphrased.7However, there are certain limitations: Monotonic Translation.
It is assumed that a phrasal replacement willoccur in the exact same position in the output sentence as that of theoriginal phrase in the input sentence.
In other words, reorderings ofphrasal units are disallowed. Naive Parameter Estimation.
Using a bag-of-words approach toparameter estimation results in a relatively uninformative probabilitydistribution over the phrasal paraphrases. Reliance on edit distance.
Relying on edit distance to build the trainingcorpus of quasi-parallel sentences may exclude sentences that do exhibit aparaphrastic relationship but differ significantly in constituent orderings.All of these limitations combined lead to paraphrases that, although grammaticallysound, contain very little variety.
Most sentential paraphrases that are generated involvelittle more than simple substitutions of words and short phrases.
In Section 3.5, we willdiscuss other approaches that also find inspiration from statistical machine translationand attempt to circumvent the above limitations by using a bilingual parallel corpusinstead of a monolingual parallel corpus.3.4 Paraphrasing Using Monolingual Comparable CorporaWhereas it is clearly to our advantage to have monolingual parallel corpora, suchcorpora are usually not very readily available.
The corpora usually found in the realworld are comparable instead of being truly parallel: Parallelism between sentences isreplaced by just partial semantic and topical overlap at the level of documents.
There-fore, for monolingual comparable corpora, the task of finding phrasal correspondencesbecomes harder because the two corpora may only be related by way of describingevents under the same topic.
In such a scenario, possible paraphrasing methods either(a) forgo any attempts at directly finding such correspondences and fall back to thedistributional similarity workhorse or, (b) attempt to directly induce a form of coarse-grained alignment between the two corpora and leverage this alignment.In this section, we describe three methods that generate paraphrases from suchcomparable corpora.
The first method falls under category (a): Here the elements whosedistributional similarity is being measured are paraphrastic patterns and the distri-butions themselves are the named entities with which the elements occur in varioussentences.
In contrast, the next two methods fall under category (b) and attempt todirectly discover correspondences between two comparable corpora by leveraging anovel alignment algorithm combined with some similarity heuristics.
The differencebetween the two latter methods lies only in the efficacy of the alignment algorithm.Shinyama et al (2002) use two sets of 300 news articles from two different Japanesenewspapers from the same day as their source of paraphrases.
The comparable nature ofthe articles is ensured because both sets are from the same day.
During pre-processing,7 However, if no word in the input sentence has been observed in the parallel corpus, the paraphrasersimply reproduces the original sentence as the paraphrase.362Madnani and Dorr Generating Phrasal and Sentential Paraphrasesall named entities in each article are tagged and dependency parses are created for eachsentence in each article.
The distributional similarity driven algorithm then proceeds asfollows:1.
For each article in the first set, find the most ?similar?
article from theother set, based on a similarity measure computed over the namedentities appearing in the two articles.2.
From each sentence in each such pair of articles, extract all dependencytree paths that contain at least one named entity and generalize theminto patterns wherein the named entities have been replaced withvariables.
Each class of named-entity (e.g., Organization, Person,Location) gets its own variable.
For example, the following sentence:8Vice President Kuroda of Nihon Yamamura Glass Corp. was promoted toPresident.may give us the following two patterns, among others:?PERSON?
of ?ORGANIZATION?
was promoted?PERSON?
was promoted to ?POST?3.
Find all sentences in the two newswire corpora that match thesepatterns.
When a match is found, attach the pattern to the sentenceand link all variables to the corresponding named entities in thesentences.4.
Find all sentences that are most similar to each other (above some presetthreshold), again based on the named entities they share.5.
For each pair of similar sentences, compare their respective attachedpatterns.
If the variables in the patterns link to the same or comparablenamed entities (based on the entity text and type), then consider thepatterns to be paraphrases of each other.At the end, the output is a list of generalized paraphrase patterns with named entitytypes as variables.
For example, the algorithm may generate the following two patternsas paraphrases:?PERSON?
is promoted to ?POST?the promotion of ?PERSON?
to ?POST?
is decidedAs a later refinement, Sekine (2005) makes a similar attempt at using distributionalsimilarity over named entity pairs in order to produce a list of fully lexicalized phrasalparaphrases for specific concepts represented by keywords.The idea of enlisting named entities as proxies for detecting semantic equivalence isinteresting and has certainly been explored before (see the discussion regarding Pas?caand Dienes [2005] in Section 3.2).
However, it has some obvious disadvantages.
Theauthors manually evaluate the technique by generating paraphrases for two specific8 Although the authors provide motivating examples in Japanese (transliterated into romaji) in their paper,we choose to use English here for the sake of clarity.363Computational Linguistics Volume 36, Number 3domains (arrest events and personnel hirings) and find that while the precision isreasonably good, the coverage is very low primarily due to restrictions on the patternsthat may be extracted in Step 2.
In addition, if the average number of entities insentences is low, the likelihood of creating incorrect paraphrases is confirmed to behigher.Let us now consider the altogether separate idea of deriving coarse-grained corre-spondences by leveraging the comparable nature of the corpora.
Barzilay and Lee (2003)attempt to do so by generating compact sentence clusters in template form (stored asword lattices with slots) separately from each corpora and then pairing up templatesfrom one corpus with those from the other.
Once the templates are paired up, a newincoming sentence that matches one member of a template pair gets rendered as theother member, thereby generating a paraphrase.
They use as input a pair of corpora:the first (C1) consisting of clusters of news articles published by Agence France Presse(AFP) and the second (C2) consisting of those published by Reuters.
The two corporamay be considered comparable since the articles in each are related to the same topicand were published during the same time frame.Algorithm 4 shows some details of how their technique works.
Steps 3?18 showhow to cluster topically related sentences, construct a word lattice from such a cluster,and convert that into a slotted lattice?basically a word lattice with certain nodes recastas variables or empty slots.
The clustering is done so as to bring together sentencespertaining to the same topics and having similar structure.
The word lattice is the prod-uct of an algorithm that computes a multiple-sequence alignment (MSA) for a clusterof sentences (Step 6).
A very brief outline of such an algorithm, originally developedto compute an alignment for a set of three or more protein or DNA sequences, is asfollows:91.
Find the most similar pair of sentences in the cluster according to asimilarity scoring function.
For this approach, a simplified version ofthe edit-distance measure (Barzilay and Lee 2002) is used.2.
Align this sentence pair and replace the pair with this single alignment.3.
Repeat until all sentences have been aligned together.The word lattice so generated now needs to be converted into a slotted lattice to allowits use as a paraphrase template.
Slotting is performed based on the following intuition:Areas of high variability between backbone nodes, that is, several distinct parallel paths,may correspond to template arguments and can be collapsed into one slot that can befilled by these arguments.
However, multiple parallel paths may also appear in thelattice because of simple synonymy and those paths must be retained for paraphrasegeneration to be useful.
To differentiate between the two cases, a synonymy threshold sof 30% is used, as shown in Steps 8?14.
The basic idea behind the threshold is that as thenumber of sentences increases, the number of different arguments will increase fasterthan the number of synonyms.
Figure 7 shows how a very simple word lattice may begeneralized into a slotted lattice.Once all the slotted lattices have been constructed for each corpus, Steps 19?24try to match the slotted lattices extracted from one corpus to those extracted from theother by referring back to the sentence clusters from which the original lattices were9 For more details on MSA algorithms, refer to Gusfield (1997) and Durbin et al (1998).364Madnani and Dorr Generating Phrasal and Sentential ParaphrasesAlgorithm 4 (Barzilay and Lee 2003).
Generate set M of matching lattice pairs given apair of comparable corpora C1 and C2.Summary.
Gather topically related sentences from C1 into clusters.
Do the same for C2.Convert each sentence cluster into a slotted lattice using a multiple-sequence alignment(MSA) algorithm.
Compare all lattice pairs and output those likely to be paraphrastic.1: Let WC1 and WC2 represent word lattices obtained from C1 and C2, respectively2: M ?
{?
}, WC1 ?
{?
}, WC2 ?
{?
}3: for each input corpus Ci ?
{C1, C2} do4: Create a set of clusters GCi = {GCi,k} of sentences based on n-gram overlap suchthat all sentences in a cluster describe the same kinds of events and share similarstructure5: for each cluster GCi,k do6: Compute an MSA for all sentences in GCi,k by using a pre-stipulated scoringfunction and represent the output as a word lattice WCi,k7: Compute the set of backbone nodes Bk for WCi,k, that is, the nodes that areshared by a majority (?50%) of the sentences in GCi,k8: for each backbone node b ?
Bk do9: if no more than 30% of all the edges from b lead to the same node then10: Replace all nodes adjacent to b with a single slot11: else12: Delete any node with < 30% of the edges from b leading to it and preservethe rest13: end if14: end for15: Merge any consecutive slot nodes into a single slot16: WCi ?
WCi ?
{WCi,k}17: end for18: end for19: for each lattice pair (WC1,j, WC2,k) ?
WC1 ?
WC2 do20: Inspect clusters GC1,j and GC2,k and compare slot fillers in the cross-corpussentence pairs written on the same day21: if comparison score > a pre-stipulated threshold ?
then22: M ?
M ?
{(WC1,j, WC2,k)}23: end if24: end for25: Output M containing paraphrastic lattice pairs with linked slotsgenerated, comparing the sentences that were written on the same day and computinga comparison score based on overlap between the sets of arguments that fill the slots.
Ifthis computed score is greater than some fixed threshold value ?, then the two lattices(or patterns) are considered to be paraphrases of each other.Besides generating pairs of paraphrastic patterns, the authors go one step furtherand actually use the patterns to generate paraphrases for new sentences.
Given sucha sentence S, the first step is to find an existing slotted lattice from either corpus thataligns best with S, in terms of the previously mentioned alignment scoring function.If some lattice is found as a match, then all that remains is to take all correspondinglattices from the other corpus that are paired with this lattice and use them to create365Computational Linguistics Volume 36, Number 3Figure 7An example showing the generalization of the word lattice (a) into a slotted lattice (b).
The wordlattice is produced by aligning seven sentences.
Nodes having in-degrees > 1 occur in more thanone sentence.
Nodes with thick incoming edges occur in all sentences.multiple rewritings (paraphrases) for S. Rewriting in this context is a simple matter ofsubstitution: For each slot in the matching lattice, we know not only the argument fromthe sentence that fills it but also the slot in the corresponding rewriting lattice.As far as the quality of acquired paraphrases is concerned, this approach easily out-performs almost all other sentential paraphrasing approaches surveyed in this article.However, a paraphrase is produced only if the incoming sentence matches some existingtemplate, which leads to a strong bias favoring quality over coverage.
In addition,the construction and generalization of lattices may become computationally expensivewhen dealing with much larger corpora.We can also compare and contrast Barzilay and Lee?s work and the work fromSection 3.3 that seems most closely related: that of Pang, Knight, and Marcu (2003).Both take sentences grouped together in a cluster and align them into a lattice using aparticular algorithm.
Pang, Knight, and Marcu have a pre-defined size for all clusterssince the input corpus is an 11-way parallel corpus.
However, Barzilay and Lee have toconstruct the clusters from scratch because their input corpus has no pre-defined notionof parallelism at the sentence level.
Both approaches use word lattices to represent andinduce paraphrases since a lattice can efficiently and compactly encode n-gram similar-ities (sets of shared overlapping word sequences) between a large number of sentences.However, the two approaches are also different in that Pang, Knight, and Marcu use theparse trees of all sentences in a cluster to compute the alignment (and build the lattice),whereas Barzilay and Lee use only surface level information.
Furthermore, Barzilayand Lee can use their slotted lattice pairs to generate paraphrases for novel and unseensentences, whereas Pang, Knight, and Marcu cannot paraphrase new sentences at all.366Madnani and Dorr Generating Phrasal and Sentential ParaphrasesShen et al (2006) attempt to improve Barzilay and Lee?s technique by trying toinclude syntactic constraints in the cluster alignment algorithm.
In that way, it is doingsomething similar to what Pang, Knight, and Marcu do but with a comparable corpusinstead of a parallel one.
More precisely, whereas Barzilay and Lee use a relativelysimple alignment scoring function based on purely lexical features, Shen et al try tobring syntactic features into the mix.
The motivation is to constrain the relatively freenature of the alignment generated by the MSA algorithm?which may lead to the gen-eration of grammatically incorrect sentences?by using informative syntactic features.In their approach, even if two words are a lexical match?as defined by Barzilay andLee (2003)?they are further inspected in terms of certain pre-defined syntactic features.Therefore, when computing the alignment similarity score, two lexically matched wordsacross a sentence pair are not considered to fully match unless their score on syntacticfeatures also exceeds a preset threshold.The syntactic features constituting the additional constraints are defined in termsof the output of a chunk parser.
Such a parser takes as input the syntactic treesof the sentences in a topic cluster and provides the following information for eachword: Part-of-speech tag IOB tag.
This is a notation denoting the constituent covering a wordand its relative position in that constituent (Ramshaw and Marcus 1995).If a word has the tag I-NP, we can infer that the word is covered by anNP and located inside that NP.
Similarly, B denotes that the word is atthe beginning and O denotes that the word is not covered by anyconstituent. IOB chain.
A concatenation of all IOB tags going from the root of the treeto the word under consideration.With this information and a heuristic to compute the similarity between two wordsin terms of their POS and IOB tags, the alignment similarity score can be calculatedas the sum of the heuristic similarity value for the given two words and the heuristicsimilarity values for each corresponding node in the two IOB chains.
If this score ishigher than some threshold and the two words have similar positions in their respectivesentences, then the words are considered to be a match and can be aligned.
Given thisalignment algorithm, the word lattice representing the global alignment is constructedin an iterative manner similar to the MSA approach.Shen et al (2006) present evidence from a manual evaluation that sentences sam-pled from lattices constructed via the syntactically informed alignment method receivehigher grammaticality scores as compared to sentences from the lattices constructed viathe purely lexical method.
However, they present no analysis of the actual paraphrasingcapacity of their, presumably better aligned, lattices.
Indeed, they explicitly mention thattheir primary goal is to measure the correlation between the syntax-augmented scoringfunction and the correctness of the sentences being generated from such lattices, evenif the sentences do not bear a paraphrastic relationship to the input.
Even if one wereto assume that the syntax-based alignment method would result in better paraphrases,it still would not address the primary weakness of Barzilay and Lee?s method: Para-phrases are only generated for new sentences that match an already existing lattice,leading to lower coverage.367Computational Linguistics Volume 36, Number 33.5 Paraphrasing Using Bilingual Parallel CorporaIn the last decade, there has been a resurgence in research on statistical machine transla-tion.
There has also been an accompanying dramatic increase in the number of availablebilingual parallel corpora due to the strong interest in SMT from both the public andprivate sectors.
Recent research in paraphrase generation has attempted to leveragethese very large bilingual corpora.
In this section, we look at such approaches that relyon the preservation of meaning across languages and try to recover said meaning byusing cues from the second language.Using bilingual parallel corpora for paraphrasing has the inherent advantage thatsentences in the other language are exactly semantically equivalent to sentences inthe intended paraphrasing language.
Therefore, the most common way to generateparaphrases with such a corpus exploits both its parallel and bilingual natures: Alignphrases across the two languages and consider all co-aligned phrases in the intendedlanguage to be paraphrases.
The bilingual phrasal alignments can simply be generatedby using the automatic techniques developed for the same task in the SMT literature.Therefore, arguably the most important factor affecting the performance of thesetechniques is usually the quality of the automatic bilingual phrasal (or word) alignmenttechniques.One of the most popular methods leveraging bilingual parallel corpora is thatproposed by Bannard and Callison-Burch (2005).
This technique operates exactly asdescribed above by attempting to infer semantic equivalence between phrases in thesame language indirectly with the second language as a bridge.
Their approach buildson one of the initial steps used to train a phrase-based statistical machine translationsystem (Koehn, Och, and Marcu 2003).
Such systems rely on phrase tables?a tabulationof correspondences between phrases in the source language and phrases in the targetlanguage.
These tables are usually extracted by inducing word alignments betweensentence pairs in a training corpus and then incrementally building longer phrasalcorrespondences from individual words and shorter phrases.
Once such a tabulation ofbilingual phrasal correspondences is available, correspondences between phrases in onelanguage may be inferred simply by using the phrases in the other language as pivots.Algorithm 5 shows how monolingual phrasal correspondences are extracted froma bilingual corpus C by using word alignments.
Steps 3?7 extract bilingual phrasalcorrespondences from each sentence pair in the corpus by using heuristically inducedbidirectional word alignments.
Figure 8 illustrates this extraction process for two exam-ple sentence pairs.
For each pair, a matrix shows the alignment between the Chineseand the English words.
Element (i, j) of the matrix is filled if there is an alignment linkbetween the ith Chinese word and the jth English word ej.
All phrase pairs consistentwith the word alignment are then extracted.
A consistent phrase pair can intuitivelybe thought of as a sub-matrix where all alignment points for its rows and columns areinside it and never outside.
Next, Steps 8?11 take all English phrases that correspondto the same foreign phrase and infer them all to be paraphrases of each other.10 Forexample, the English paraphrase pair ?effectively contained, under control?
is obtainedfrom Figure 8 by pivoting on the Chinese phrase , shown underlined for bothmatrices.10 Note that it would have been equally easy to pivot on the English side and generate paraphrases in theother language instead.368Madnani and Dorr Generating Phrasal and Sentential ParaphrasesAlgorithm 5 (Bannard and Callison-Burch 2005).
Generate set M of monolingual para-phrase pairs given a bilingual parallel corpus C.Summary.
Extract bilingual phrase pairs from C using word alignments and standardSMT heuristics.
Pivot all pairs of English phrases on any shared foreign phrases andconsider them paraphrases.
The alignment notation from Algorithm 3 is employed.1: Let B represent the bilingual phrases extracted from C2: B ?
{?
}, M ?
{?
}3: Compute a word alignment a for each sentence pair (E, F) ?
C4: for each aligned sentence pair (E, F)a do5: Extract the set of bilingual phrasal correspondences {(e?, f? )}
such that:(a) ?ei ?
e?
: eia?
fj ?
fj ?
f?
, and(a) ?fj ?
f?
: fja?
ei ?
ei ?
e?6: B ?
B ?
{(e?, f?
)}7: end for8: for each member of the set {?
(e?j, f?k), (e?l, f?m)?
s.t.
(e?j, f?k) ?
B?
(e?l, ?fm) ?
B?
f?k = ?fm} do9: M ?
M ?
{(e?j, e?l)}10: Compute p(e?j|e?l) =?f?
p(e?j| f?
)p( f?
|e?l)11: end for12: Output M containing paraphrastic pairs and associated probabilitiesUsing the components of a phrase-based SMT system also makes it easy to assign aprobability value to any of the inferred paraphrase pairs as follows:p(e?j|e?k) =?f?p(e?j, f?
|e?k) ?
?f?p(e?j| f?
)p( f?
|e?k)where both p(e?j| f? )
and p( f?
|e?k) can be computed using maximum likelihood estimationas part of the bilingual phrasal extraction process:p(e?j| f? )
=number of times f?
is extracted with e?jnumber of times f?
is extracted with any e?Once the probability values are obtained, the most likely paraphrase can be chosen forany phrase.Bannard and Callison-Burch (2005) are able to extract millions of phrasal para-phrases from a bilingual parallel corpus.
Such an approach is able to capture a largevariety of paraphrastic phenomena in the inferred paraphrase pairs but is seriouslylimited by the bilingual word alignment technique.
Even state-of-the-art alignmentmethods from SMT are known to be notoriously unreliable when used for aligningphrase pairs.
The authors find via manual evaluation that the quality of the phrasal369Computational Linguistics Volume 36, Number 3Figure 8Extracting consistent bilingual phrasal correspondences from the shown sentence pairs.
(i1, j1) ?
(i2, j2) denotes the correspondence ?
fi1 .
.
.
fj1 , ei2 .
.
.
ej2?.
Not all extractedcorrespondences are shown.paraphrases obtained via manually constructed word alignments is significantly betterthan that of the paraphrases obtained from automatic alignments.It has been widely reported that the existing bilingual word alignment techniquesare not ideal for use in translation and, furthermore, improving these techniques doesnot always lead to an improvement in translation performance.
(Callison-Burch, Talbot,and Osborne 2004; Ayan and Dorr 2006; Lopez and Resnik 2006; Fraser and Marcu2007).
More details on the relationship between word alignment and SMT can be foundin the comprehensive SMT survey recently published by Lopez (2008) (particularlySection 4.2).
Paraphrasing done via bilingual corpora relies on the word alignmentsin the same way as a translation system would and, therefore, would be equallysusceptible to the shortcomings of the word alignment techniques.
To determine hownoisy automatic word alignments affect paraphrasing done via bilingual corpora, weinspected a sample of paraphrase pairs that were extracted when using Arabic?alanguage significantly different from English?as the pivot language.11 In this study, wefound that the paraphrase pairs in the sample set could be grouped into the followingthree broad categories:(a) Morphological variants.
These pairs only differ in the morphologicalform of one of the words in the phrases and cannot really be consideredparaphrases.
Examples: ?ten ton, ten tons?, ?caused clouds, causing clouds?.11 The bilingual Arabic?English phrases were extracted from a million sentences of Arabic newswire datausing the freely available and open source Moses SMT toolkit (http://www.statmt.org/moses/).
Thedefault Moses parameters were used.
The English paraphrases were generated by simply applying thepivoting process described herein to the bilingual phrase pairs.370Madnani and Dorr Generating Phrasal and Sentential Paraphrases(b) Approximate Phrasal Paraphrases.
These are pairs that only share partialsemantic content.
Most paraphrases extracted by the pivot method usingautomatic alignments fall into this category.
Examples: ?were exiled, wentabroad?, ?accounting firms, auditing firms?.
(c) Phrasal Paraphrases.
Despite unreliable alignments, there were indeed alarge number of truly paraphrastic pairs in the set that were semanticallyequivalent.
Examples: ?army roadblock, military barrier?
?staff walked out,team withdrew?.Besides there being obvious linguistic differences between Arabic and English, theprimary reason for the generation of phrase pairs that lie in categories (a) and (b)is incorrectly induced alignments between the English and Arabic words, and hence,phrases.
Therefore, a good portion of subsequent work on paraphrasing using bilingualcorpora, as discussed below focuses on using additional machinery or evidence to copewith the noisy alignment process.
Before we continue, we believe it would be usefulto draw a connection between Bannard and Callison-Burch?s (2005) work and that ofWu and Zhou (2003) as discussed in Section 3.2.
Note that both of these techniquesrely on a secondary language to provide the cues for generating paraphrases in theprimary language.
However, Wu and Zhou rely on a pre-compiled bilingual dictionaryto discover these cues whereas Bannard and Callison-Burch have an entirely data-driven discovery process.In an attempt to address some of the noisy alignment issues, Callison-Burch (2008)recently proposed an improvement that places an additional syntactic constraint on thephrasal paraphrases extracted via the pivot-based method from bilingual corpora andshowed that using such a constraint leads to a significant improvement in the qual-ity of the extracted paraphrases.12 The syntactic constraint requires that the extractedparaphrase be of the same syntactic type as the original phrase.
With this constraint,estimating the paraphrase probability now requires the incorporation of syntactic typeinto the equation:p(e?j|e?k, s(ek)) ?
?f?p(e?j| f?
, s(ek))p( f?
|e?k, s(ek))where s(e) denotes the syntactic type of the English phrase e. As before, maximumlikelihood estimation is employed to compute the two component probabilities:p(e?j| f?
, s(ek)) =number of times f?
is extracted with e?j and type s(ek)number of times f?
is extracted with any e?
and type s(ek)If the syntactic types are restricted to be simple constituents (NP, VP, etc.
), thenusing this constraint will actually exclude some of the paraphrase pairs that couldhave been extracted in the unconstrained approach.
This leads to the familiar precision-recall tradeoff: It only extracts paraphrases that are of higher quality, but the approachhas a significantly lower coverage of paraphrastic phenomena that are not necessarilysyntactically motivated.
To increase the coverage, complex syntactic types such as those12 The software for generating these phrasal paraphrases along with a large collection of already extractedparaphrases is available at http://www.cs.jhu.edu/?ccb/howto-extract-paraphrases.html.371Computational Linguistics Volume 36, Number 3used in Combinatory Categorial Grammars (Steedman 1996) are employed, which canhelp denote a syntactic constituent with children missing on the left and/or right handsides.
An example would be the complex type VP/(NP/NNS) which denotes a verbphrase missing a noun phrase to its right which, in turn, is missing a plural noun to itsright.
The primary benefit of using complex types is that less useful paraphrastic phrasepairs from different syntactic categories such as ?accurately, precise?, that would havebeen allowed in the unconstrained pivot-based approach, are now disallowed.The biggest advantage of this approach is the use of syntactic knowledge as oneform of additional evidence in order to filter out phrase pairs from categories (a) and(b) as defined in the context of our manual inspection of pivot-based paraphrasesabove.
Indeed, the authors conduct a manual evaluation to show that the syntacticallyconstrained paraphrase pairs are better than those produced without such constraints.However, there are two additional benefits of this technique:1.
The constrained approach might allow induction of some new phrasalparaphrases in category (c) since now an English phrase only has tocompete with other pivoted phrases of similar syntactic type and not all ofthem.2.
The effective partitioning of the probability space for a given paraphrasepair by syntactic types can be exploited: Overly specific syntactic typesthat occur very rarely can be ignored and a less noisy paraphraseprobability estimate can be computed, which may prove more usefulin a downstream application than its counterpart computed via theunconstrained approach.We must also note that requiring syntactic constraints for pivot-based paraphrase ex-traction restricts the approach to those languages where a reasonably good parser isavailable.An obvious extension of the Callison-Burch style approach is to use the collectionof pivoted English-to-English phrase pairs to generate sentential paraphrases for newsentences.
Madnani et al (2008a) combine the pivot-based approach to paraphraseacquisition with a well-defined English-to-English translation model that is then used inan (unmodified) SMT system, yielding sentential paraphrases by means of ?translating?input English sentences.
However, instead of fully lexicalized phrasal correspondencesas in (Bannard and Callison-Burch 2005), the fundamental units of translation (andparaphrasing) are hierarchical phrase pairs.
The latter can be extracted from the formerby replacing aligned sub-phrases with non-terminal symbols.
For example, given theinitial phrase pair , growth rate has been effectively contained?, thehierarchical phrase pair ?X1 X2, X1 has been X2?
can be formed.13 Each hierarchicalphrase pair can also have certain features associated with it that are estimated viamaximum likelihood estimation during the extraction process.
Such phrase pairs canformally be considered the rules of a bilingual synchronous context-free grammar(SCFG).
Translation with SCFGs is equivalent to parsing the string in the source lan-guage using these rules to generate the highest-scoring tree and then reading off thetree in target order.
For the purposes of this survey, it is sufficient to state that efficient13 The process of converting an initial phrase into a hierarchical one is subject to several additionalconstraints on the lengths of the initial and hierarchical phrases and the number and position ofnon-terminals in the hierarchical phrase.372Madnani and Dorr Generating Phrasal and Sentential Paraphrasesmethods to extract such rules, to estimate their features, and to translate with them arenow well established.
For more details on building SCFG-based models and translatingwith them, we refer the reader to (Chiang 2006, 2007).Once a set of bilingual hierarchical rules has been extracted along with associatedfeatures, the pivoting trick can be applied to infer monolingual hierarchical paraphrasepairs (or paraphrastic patterns).
However, the patterns are not the final output andare actually used as rules from a monolingual SCFG grammar in order to define anEnglish-to-English translation model.
Features for each monolingual rule are estimatedin terms of the features of the bilingual pairs that the rule was inferred from.
A sententialparaphrase can then be generated for any given sentence by using this model along withan n-gram language model and a regular SMT decoder to paraphrase (or monolinguallytranslate) any sentence just as one would translate bilingually.The primary advantage of this approach is the ability to produce good qualitysentential paraphrases by leveraging the SMT machinery to address the noise issue.However, although the decoder and the language model do serve to counter the noisyword alignment process, they do so only to a degree and not entirely.Again, we must draw a connection between this work and that of Quirk, Brockett,and Dolan (2004) (discussed in Section 3.3) because both treat paraphrasing asmonolingual translation.
However, as outlined in the discussion of that work, Quirk,Brockett, and Dolan use a relatively simplistic translation model and decoder whichleads to paraphrases with little or no lexical variety.
In contrast, Madnani et al use amore complex translation model and an unmodified state-of-the-art SMT decoder toproduce paraphrases that are much more diverse.
Of course, the reliance of the latterapproach on automatic word alignments does inevitably lead to much noisier sententialparaphrases than those produced by Quirk, Brockett, and Dolan.Kok and Brockett (2010) present a novel take on generating phrasal paraphraseswith bilingual corpora.
As with most approaches based on parallel corpora, they alsostart with phrase tables extracted from such corpora along with the correspondingphrasal translation probabilities.
However, instead of performing the usual pivotingstep with the bilingual phrases in the table, they take a graphical approach and representeach phrase in the table as a node, leading to a bipartite graph.
Two nodes in thegraph are connected to each other if they are aligned to each other.
In order to extractparaphrases, they sample random paths in the graph from any English node to another.Note that the traditional pivot step is equivalent to a path of length two: one Englishphrase to the foreign pivot phrase and then to the potentially paraphrastic Englishphrase.
By allowing paths of lengths longer than two, this graphical approach can findmore paraphrases for any given English phrase.Furthermore, instead of restricting themselves to a single bilingual phrase table,they take as input a number of phrase tables, each corresponding to a different pair ofsix languages.
Similar to the single-table case, each phrase in each table is representedas a node in a graph that is no longer bipartite in nature.
By allowing edges to existbetween nodes of all the languages if they are aligned, the pivot can now even be a setof nodes rather than a single node in another language.
For example, one could easilyfind the following path in such a graph:ate lunch ?
a?en zu ittag (German) ?
aten een hapje (Dutch) ?
had a biteIn general, each edge is associated with a weight corresponding to the bilingual phrasetranslation probability.
Random walks are then sampled from the graph in such a waythat only paths of high probability end up contributing to the extracted paraphrases.373Computational Linguistics Volume 36, Number 3Obviously, the alignment errors discussed in the context of simple pivoting will alsohave an adverse effect on this approach.
In order to prevent this, the authors add specialfeature nodes to the graph in addition to regular nodes.
These feature nodes representdomain-specific knowledge of what would make good paraphrases.
For example, nodesrepresenting syntactic equivalence classes of the start and end words of the Englishphrases are added.
This indicates that phrases that start and end with the same kind ofwords (interrogatives or articles) are likely to be paraphrases.
Astute readers will makethe following observations about the syntactic feature nodes used by the authors: Such nodes can be seen as an indirect way of incorporating a limited formof distributional similarity. By including such nodes?essentially based on lexical equivalenceclasses?the authors are, in a way, imposing weaker forms of syntacticconstraints described in Callison-Burch (2008) without requiring a parser.The authors extract paraphrases for a small set of input English paraphrases andshow that they are able to generate a larger percentage of correct paraphrases comparedto the syntactically constrained approach proposed by Callison-Burch (2008).
They con-duct no formal evaluation of the coverage of their approach but show that, in a limitedsetting, it is higher than that for the syntactically constrained pivot-based approach.However, they perform no comparisons of their coverage with the original pivot-basedapproach (Bannard and Callison-Burch 2005).4.
Building Paraphrase CorporaBefore we present some specific techniques from the literature that have been employedto evaluate paraphrase generation methods, it is important to examine some recentwork that has been done on constructing paraphrase corpora.
As part of this work, hu-man subjects are generally asked to judge whether two given sentences are paraphrasesof each other.
We believe that a detailed examination of this manual evaluation taskprovides an illuminating insight into the nature of a paraphrase in a practical, ratherthan a theoretical, context.
In addition, it has obvious implications for any method,whether manual or automatic, that is used to evaluate the performance of a paraphrasegenerator.Dolan and Brockett (2005) were the first to attempt to build a paraphrase corpuson a large scale.
The Microsoft Research Paraphrase (MSRP) Corpus is a collection of5, 801 sentence pairs, each manually labeled with a binary judgment as to whether itconstitutes a paraphrase or not.
As a first step, the corpus was created using a heuristicextraction method in conjunction with an SVM-based classifier that was trained toselect likely sentential paraphrases from a large monolingual corpus containing newsarticle clusters.
However, the more interesting aspects of the task were the subsequentevaluation of these extracted sentence pairs by human annotators and the set of issuesencountered when defining the evaluation guidelines for these annotators.It was observed that if the human annotators were instructed to mark only thesentence pairs that were strictly semantically equivalent or that exhibited bidirectionalentailment as paraphrases, then the results were limited to uninteresting sentence pairssuch as the following:S1: The euro rose above US$1.18, the highest price since its January 1999 launch.S2: The euro rose above $1.18, the highest level since its launch in January 1999.374Madnani and Dorr Generating Phrasal and Sentential ParaphrasesS1: However, without a carefully controlled study, there was little clear proof thatthe operation actually improves people?s lives.S2: But without a carefully controlled study, there was little clear proof that theoperation improves people?s lives.Instead, they discovered that most of the complex paraphrases?ones with alter-nations more interesting than simple lexical synonymy and local syntactic changes?exhibited varying degrees of semantic divergence.
For example:S1: Charles O.
Prince, 53, was named as Mr. Weill?s successor.S2: Mr. Weill?s longtime confidant, Charles O.
Prince, 53, was named as his successor.S1: David Gest has sued his estranged wife Liza Minelli for beating him when she wasdrunk.S2: Liza Minelli?s estranged husband is taking her to court after saying she threw alamp at him and beat him in drunken rages.Therefore, in order to be able to create a richer paraphrase corpus, one with manycomplex alternations, the instructions to the annotators had to be relaxed; the degree ofmismatch accepted before a sentence pair was judged to be fully semantically divergent(or ?non-equivalent?)
was left to the human subjects.
It is also reported that, given theidiosyncratic nature of each sentence pair, only a few formal guidelines were generaliz-able enough to take precedence over the subjective judgments of the human annotators.Despite the somewhat loosely defined guidelines, the inter-annotator agreement for thetask was 84%.
However, a kappa score of 62 indicated that the task was overall a difficultone (Cohen 1960).
At the end, 67% of the sentence pairs were judged to be paraphrasesof each other and the rest were judged to be non-equivalent.14Although the MSRP Corpus is a valuable resource and its creation provided valu-able insight into what constitutes a paraphrase in the practical sense, it does have someshortcomings.
For example, one of the heuristics used in the extraction process wasthat the two sentences in a pair must share at least three words.
Using this constraintrules out any paraphrase pairs that are fully lexically divergent but still semanticallyequivalent.
The small size of the corpus, when combined with this and other suchconstraints, precludes the use of the corpus as training data for a paraphrase generationor extraction system.
However, it is fairly useful as a freely available test set to evaluateparaphrase recognition methods.On a related note, Fujita and Inui (2005) take a more knowledge-intensive ap-proach to building a Japanese corpus containing sentence pairs with binary paraphrasejudgments and attempt to focus on variety and on minimizing the human annotationcost.
The corpus contains 2, 031 sentence pairs each with a human judgment indicatingwhether the paraphrase is correct or not.
To build the corpus, they first stipulate atypology of paraphrastic phenomena (rewriting light-verb constructions, for example)and then manually create a set of morpho-syntactic paraphrasing rules and patternsdescribing each type of paraphrasing phenomenon.
A paraphrase generation system14 The MSR paraphrase corpus is available at http://research.microsoft.com/en-us/downloads/607d14d9-20cd-47e3-85bc-a2f65cd28042.375Computational Linguistics Volume 36, Number 3using these rules (Fujita et al 2004) is then applied to a corpus containing Japanesenews articles, and example paraphrases are generated for the sentences in the corpus.These paraphrase pairs are then handed to two human annotators who create binaryjudgments for each pair indicating whether or not the paraphrase is correct.
Using aclass-oriented approach is claimed to have a two-fold advantage:1.
Exhaustive Collection of Paraphrases.
Creating specific paraphrasingrules for each class manually is likely to increase the chance of thecollected examples accurately reflecting the distribution of occurrencesin the real world.2.
Low Annotation Cost.
Partitioning the annotation task into classes isexpected to make it easier (and faster) to arrive at a binary judgment giventhat an annotator is only concerned with a specific type of paraphrasingwhen creating said judgment.The biggest disadvantage of this approach is that only two types of paraphrastic phe-nomena are used: light-verb constructions and transitivity alternations (using intransi-tive verbs in place of transitive verbs).
The corpus indeed captures almost all examplesof both types of paraphrastic phenomena and any that are absent can be easily coveredby adding one or two more patterns to the class.
The claim of reduced annotation cost isnot necessarily borne out by the observations.
Despite partitioning the annotation taskby types, it was still difficult to provide accurate annotation guidelines.
This led to asignificant difference in annotation time?with some annotations taking almost twiceas long as others.
Given the small size of the corpus, it is unlikely that it may be usedas training data for corpus-based paraphrase generation methods and, like the MSRPcorpus, would be best suited to the evaluation of paraphrase recognition techniques.Most recently, Cohn, Callison-Burch, and Lapata (2008) describe a different takeon the creation of a monolingual parallel corpus containing 900 sentence pairs withparaphrase annotations that can be used for both development and evaluation of para-phrase systems.
These paraphrase annotations take the form of alignments between thewords and sequences of words in each sentence pair; these alignments are analogousto the word- and phrasal-alignments induced in SMT systems that were illustrated inSection 3.5.
As is the case with SMT alignments, the paraphrase annotations can beof different forms: one-word-to-one-word, one-word-to-many-words, as well as fullyphrasal alignments.15The authors start from a sentence-aligned paraphrase corpus compiled from threecorpora that we have already described elsewhere in this survey: (1) the sentencepairs judged equivalent from the MSRP Corpus: (2) the Multiple Translation Chinese(MTC) corpus of multiple human-written translations of Chinese news stories usedby Pang, Knight, and Marcu (2003); and (3) two English translations of the French novelTwenty Thousand Leagues Under the Sea, a subset of the monolingual parallel corpus usedby Barzilay and McKeown (2001).
The words in each sentence pair from this corpusare then aligned automatically to produce the initial paraphrase annotations that arethen refined by two human annotators.
The annotation guidelines required that theannotators judge which parts of a given sentence pair were in correspondence and to in-dicate this by creating an alignment between those parts (or correcting already existing15 The paraphrase-annotated corpus can be found at http://www.dcs.shef.ac.uk/?tcohn/paraphrase corpus.html.376Madnani and Dorr Generating Phrasal and Sentential Paraphrasesalignments, if present).
Two parts were said to correspond if they could be substitutedfor each other within the specific context provided by the respective sentence pair.
Inaddition, the annotators were instructed to classify the created alignments as either sure(the two parts are clearly substitutable) or possible (the two parts are slightly divergenteither in terms of syntax or semantics).
For example, given the following paraphrasticsentence pair:S1: He stated the convention was of profound significance.S2: He said that the meeting could have very long-term effects.the phrase pair ?the convention, the meeting?
will be aligned as a sure correspondencewhereas the phrase pair ?was of profound significance, could have very long-term effects?
willbe aligned as a possible correspondence.
Other examples of possible correspondencescould include the same stem expressed as different parts-of-speech (such as ?significance,significantly?)
or two non-synonymous verbs (such as ?this is also, this also marks?).
Formore details on the alignment guidelines that were provided to the annotators, we referthe reader to (Callison-Burch, Cohn, and Lapata 2006).Extensive experiments are conducted to measure inter-annotator agreements andobtain good agreement values but they are still low enough to confirm that it is difficultfor humans to recognize paraphrases even when the task is formulated differently.Overall, such a paraphrase corpus with detailed paraphrase annotations is much moreinformative than a corpus containing binary judgments at the sentence level such asthe MSRP corpus.
As an example, because the corpus contains paraphrase annotationsat the word as well as phrasal levels, it can be used to build systems that can learnfrom these annotations and generate not only fully lexicalized phrasal paraphrases butalso syntactically motivated paraphrastic patterns.
To demonstrate the viability of thecorpus for this purpose, a grammar induction algorithm (Cohn and Lapata 2007) isapplied?originally developed for sentence compression?to the parsed version of theirparaphrase corpus and the authors show that they can learn paraphrastic patterns suchas those shown in Figure 9.In general, building paraphrase corpora, whether it is done at the sentence level orat the sub-sentential level, is extremely useful for the fostering of further research anddevelopment in the area of paraphrase generation.5.
Evaluation of Paraphrase GenerationWhereas other language processing tasks such as machine translation and docu-ment summarization usually have multiple annual community-wide evaluations usingFigure 9An example of syntactically motivated paraphrastic patterns that can be extracted from theparaphrase corpus constructed by Cohn, Callison-Burch, and Lapata (2008).377Computational Linguistics Volume 36, Number 3standard test sets and manual as well as automated metrics, the task of automatedparaphrasing does not.
An obvious reason for this disparity could be that paraphrasingis not an application in and of itself.
However, the existence of similar evaluations forother tasks that are not applications, such as dependency parsing (Buchholz and Marsi2006; Nivre et al 2007) and word sense disambiguation (Senseval), suggests otherwise.We believe that the primary reason is that, over the years, paraphrasing has been em-ployed in an extremely fragmented fashion.
Paraphrase extraction and generation areused in different forms and with different names in the context of different applications(for example: synonymous collocation extraction, query expansion).
This usage patterndoes not allow researchers in one community to share the lessons learned with thosefrom other communities.
In fact, it may even lead to research being duplicated acrosscommunities.However, more recent work?some of it discussed in this survey?on extractingphrasal paraphrases (or patterns) does include direct evaluation of the paraphrasingitself: The original phrase and its paraphrase are presented to multiple human judges,along with the contexts in which the phrase occurs in the original sentence, whoare asked to determine whether the relationship between the two phrases is indeedparaphrastic (Barzilay and McKeown 2001; Barzilay and Lee 2003; Ibrahim, Katz, andLin 2003; Pang, Knight, and Marcu 2003).
A more direct approach is to substitute theparaphrase in place of the original phrase in its sentence and present both sentencesto judges who are then asked to judge not only their semantic equivalence but alsothe grammaticality of the new sentence (Bannard and Callison-Burch 2005; Callison-Burch 2008).
Motivation for such substitution-based evaluation is discussed in Callison-Burch (2007): the basic idea being that items deemed to be paraphrases may behave assuch only in some contexts and not others.
Szpektor, Shnarch, and Dagan (2007) posita similar form of evaluation for textual entailment wherein the human judges are notonly presented with the entailment rule but also with a sample of sentences that matchits left-hand side (called instances), and then asked to assess whether the rule holdsunder each specific instance.Sentential paraphrases may be evaluated in a similar fashion without the need forany surrounding context (Quirk, Brockett, and Dolan 2004).
An intrinsic evaluation ofthis form must employ the usual methods for avoiding any bias and for maximizinginter-judge agreement.
In addition, we believe that, given the difficulty of this task evenfor human annotators, adherence to strict semantic equivalence may not always be asuitable guideline and intrinsic evaluations must be very carefully designed.
A numberof these approaches also perform extrinsic evaluations, in addition to the intrinsicone, by utilizing the extracted or generated paraphrases to improve other applicationssuch as machine translation (Callison-Burch, Koehn, and Osborne 2006) and others asdescribed in Section 1.Another option when evaluating the quality of a paraphrase generation method isthat of using automatic measures.
The traditional automatic evaluation measures of pre-cision and recall are not particularly suited to this task because, in order to use them, alist of reference paraphrases has to be constructed against which these measures may becomputed.
Given that it is extremely unlikely that any such list will be exhaustive, anyprecision and recall measurements will not be accurate.
Therefore, other alternativesare needed.
Since the evaluation of paraphrases is essentially the task of measuringsemantic similarity or of paraphrase recognition, all of those metrics, including the onesdiscussed in Section 2, can be employed here.Most recently, Callison-Burch, Cohn, and Lapata (2008) discuss ParaMetric, anotherautomatic measure that may be used to evaluate paraphrase extraction methods.
This378Madnani and Dorr Generating Phrasal and Sentential Paraphraseswork follows directly from the work done by the authors to create the paraphrase-annotated corpus described in the previous section.
Recall that this corpus containsparaphrastic sentence pairs with annotations in the form of alignments between theirrespective words and phrases.
It is posited that to evaluate any paraphrase generationmethod, one could simply have it produce its own set of alignments for the sentencepairs in the corpus and precision and recall could then be computed over alignmentsinstead of phrase pairs.
These alignment-oriented precision (Palign) and recall (Ralign)measures are computed as follows:Palign =??s1,s2?
|NP(s1, s2) ?
NM(s1, s2)|??s1,s2?
|NP(s1, s2)|Ralign =??s1,s2?
|NP(s1, s2) ?
NM(s1, s2)|??s1,s2?
|NM(s1, s2)|where ?s1, s2?
denotes a sentence pair, NM(s1, s2) denotes the phrases extracted via themanual alignments for the pair ?s1, s2?, and NP(s1, s2) denotes the phrases extracted viathe automatic alignments induced using the paraphrase method P that is to be evalu-ated.
The phrase extraction heuristic used to compute NP and NM from the respectivealignments is the same as that employed by Bannard and Callison-Burch (2005) andillustrated in Figure 8.Although using alignments as the basis for computing precision and recall is aclever trick, it does require that the paraphrase generation method be capable of produc-ing alignments between sentence pairs.
For example, the methods proposed by Pang,Knight, and Marcu (2003) and Quirk, Brockett, and Dolan (2004) for generating sen-tential paraphrases from monolingual parallel corpora and described in Section 3.3 doproduce alignments as part of their respective algorithms.
Indeed, Callison-Burch et alprovide a comparison of their pivot-based approach?operating on bilingual parallelcorpora?with the two monolingual approaches just mentioned in terms of ParaMetric,since all three methods are capable of producing alignments.However, for other approaches that do not necessarily operate at the level ofsentences and cannot produce any alignments, falling back on estimates of traditionalformulations of precision and recall is suggested.There has also been some preliminary progress toward using standardized test setsfor intrinsic evaluations.
A test set containing 20 AFP articles (484 sentences) aboutviolence in the Middle East that was used for evaluating the lattice-based paraphrasetechnique in (Barzilay and Lee 2003) has been made freely available.16 In addition tothe original sentences for which the paraphrases were generated, the set alo containsthe paraphrases themselves and the judgments assigned by human judges to theseparaphrases.
The paraphrase-annotated corpus discussed in the previous section wouldalso fall under this category of resources.As with many other fields in NLP, paraphrase generation also lacks serious extrinsicevaluation (Belz 2009).
As described herein, many paraphrase generation techniquesare developed in the context of a host NLP application and this application usuallyserves as one form of extrinsic evaluation for the quality of the paraphrases generated16 The corpus is available at http://www.cs.cornell.edu/Info/Projects/NLP/statpar.html.379Computational Linguistics Volume 36, Number 3by that technique.
However, as yet there is no widely agreed-upon method of extrinsi-cally evaluating paraphrase generation.
Addressing this deficiency should be a crucialconsideration for any future community-wide evaluation effort.An important dimension for any area of research is the availability of fora wheremembers of the community may share their ideas with their colleagues and receivevaluable feedback.
In recent years, a number of such fora have been made available tothe automatic paraphrasing community (Inui and Hermjakob 2003; Tanaka et al 2004;Dras and Yamamoto 2005; Sekine et al 2007), which represents an extremely importantstep toward countering the fragmented usage pattern described previously.6.
Future TrendsIt is important for any survey to provide a look to the future of the surveyed task andgeneral trends for the corresponding research methods.
We identify several such trendsin the area of paraphrase generation that are gathering momentum.The Influence of the Web.
The Web is rapidly becoming one of the most importantsources of data for natural language processing applications, which should not be sur-prising given its phenomenal rate of growth.
The (relatively) freely available Web data,massive in scale, has already had a definite influence over data-intensive techniquessuch as those employed for paraphrase generation (Pas?ca and Dienes 2005).
However,the availability of such massive amounts of Web data comes with serious concerns forefficiency and has led to the development of efficient methods that can cope with suchlarge amounts of data.
Bhagat and Ravichandran (2008) extract phrasal paraphrases bymeasuring distributional similarity over a 150GB monolingual corpus (25 billion words)via locality sensitive hashing, a randomized algorithm that involves the creation offingerprints for vectors in space (Broder 1997).
Because vectors that are more similarare more likely to have similar fingerprints, vectors (or distributions) can simply becompared by comparing their fingerprints, leading to a more efficient distributionalsimilarity algorithm (Charikar 2002; Ravichandran, Pantel, and Hovy 2005).
We alsobelieve that the influence of the Web will extend to other avenues of paraphrase genera-tion such as the aforementioned extrinsic evaluation or lack thereof.
For example, Fujitaand Sato (2008b) propose evaluating phrasal paraphrase pairs, automatically generatedfrom a monolingual corpus, by querying the Web for snippets related to the pairs andusing them as features to compute the pair?s paraphrasability.Combining Multiple Sources of Information.
Another important trend in para-phrase generation is that of leveraging multiple sources of information to determinewhether two units are paraphrastic.
For example, Zhao et al (2008) improve the sen-tential paraphrases that can be generated via the pivot method by leveraging five othersources in addition to the bilingual parallel corpus itself: (1) a corpus of Web queriessimilar to the phrase, (2) definitions from the Encarta dictionary, (3) a monolingual par-allel corpus, (4) a monolingual comparable corpus, and (5) an automatically constructedthesaurus.
Phrasal paraphrase pairs are extracted separately from all six models andthen combined in a log-linear paraphrasing-as-translation model proposed by Madnaniet al (2007).
A manual inspection reveals that using multiple sources of informationyields paraphrases with much higher accuracy.
We believe that such exploitation ofmultiple types of resources and their combinations is an important development.
Zhaoet al (2009) further increase the utility of this combination approach by incorporatingapplication specific constraints on the pivoted paraphrases.
For example, if the outputparaphrases need to be simplified versions of the input sentences, then only thosephrasal paraphrase pairs are used where the output is shorter than the input.380Madnani and Dorr Generating Phrasal and Sentential ParaphrasesUse of SMT Machinery.
In theory, statistical machine translation is very closelyrelated to paraphrase generation since the former also relies on finding semantic equiv-alence, albeit in a second language.
Hence, there have been numerous paraphrasing ap-proaches that have relied on different components of an SMT pipeline (word alignment,phrase extraction, decoding/search) as we saw in the preceding pages of this survey.Despite the obvious convenience of using SMT components for the purpose of mono-lingual translation, we must consider that doing so usually requires additional work todeal with the added noise due to the nature of such components.
We believe that SMTresearch will continue to influence research in paraphrasing; both by providing ready-to-use building blocks and by necessitating development of methods to effectively usesuch components for the unintended task of paraphrase generation.Domain-Specific Paraphrasing.
Recently, work has been done to generate phrasalparaphrases in specialized domains.
For example, in the field of health literacy, itis well known that documents for health consumers are not very well-targeted totheir purported audience.
Recent research has shown how to generate a lexicon ofsemantically equivalent phrasal (and lexical) pairs of technical and lay medical termsfrom monolingual parallel corpora (Elhadad and Sutaria 2007) as well as monolingualcomparable corpora (Dele?ger and Zweigenbaum 2009).
Examples include pairs such as?myocardial infarction, heart attack?
and ?leucospermia, increased white cells in the sperm?.In another domain, Max (2008) proposes an adaptation of the pivot-based method togenerate rephrasings of short text spans that could help a writer revise a text.
Becausethe goal is to assist a writer in making revisions, the rephrasings do not always needto bear a perfect paraphrastic relationship to the original, a scenario suited for thepivot-based method.
Several variants of such adaptations are developed that generatecandidate rephrasings driven by fluency, semantic equivalence, and authoring value,respectively.We also believe that a large-scale annual community-wide evaluation should be-come a trend since it is required to foster further research in, and use of, paraphraseextraction and generation.
Although there have been recent workshops and tasks onparaphrasing and entailment as discussed in Section 5, this evaluation would be muchmore focused, providing sets of shared guidelines and resources, in the spirit of therecent NIST MT Evaluation Workshops (NIST 2009).7.
SummaryOver the last two decades, there has been much research on paraphrase extraction andgeneration within a number of research communities in natural language processing,in order to improve the specific application with which that community is concerned.However, a large portion of this research can be easily adapted for more widespread useoutside its particular host and can provide significant benefits to the whole field.
Onlyrecently have there been serious efforts to conduct research on the topic of paraphrasingby treating it as an important natural language processing task independent of a hostapplication.In this article, we have presented a comprehensive survey of the task of paraphraseextraction and generation motivated by the fact that paraphrases can help in a multi-tude of applications such as machine translation, text summarization, and informationextraction.
The aim was to provide an application-independent overview of paraphrasegeneration, while also conveying an appreciation for the importance and potential useof paraphrasing in the field of NLP research.
We show that there are a large variety381Computational Linguistics Volume 36, Number 3of paraphrase generation methods and each such method has a very different set ofcharacteristics, in terms of both its performance and its ease of deployment.
We alsoobserve that whereas most of the methods in this survey can be used in multipleapplications, the choice of the most appropriate method depends on how well thecharacteristics of the produced paraphrases match the requirements of the downstreamapplication in which the paraphrases are being utilized.ReferencesAyan, Necip Fazil and Bonnie Dorr.
2006.Going beyond AER: An extensive analysisof word alignments and their impact onMT.
In Proceedings of ACL/COLING,pages 9?16, Sydney.Bangalore, Srinivas and Owen Rambow.2000.
Corpus-based lexical choice innatural language generation.
InProceedings of ACL, pages 464?471,Hong Kong.Bannard, Colin and Chris Callison-Burch.2005.
Paraphrasing with bilingualparallel corpora.
In Proceedings of ACL,pages 597?604, Ann Arbor, MI.Bar-Haim, Roy, Ido Dagan, Bill Dolan, LisaFerro, Danilo Giampiccolo, BernardoMagnini, and Idan Szpektor, editors.
2007.Proceedings of the Second PASCAL ChallengesWorkshop on Recognizing Textual Entailment,Venice.Barzilay, Regina and Lillian Lee.
2002.Bootstrapping lexical choice viamultiple-sequence alignment.
InProceedings of EMNLP, pages 164?171,Philadelphia, PA.Barzilay, Regina and Lillian Lee.
2003.Learning to paraphrase: An unsupervisedapproach using multiple-sequencealignment.
In Proceedings of HLT-NAACL2003, pages 16?23, Edmonton.Barzilay, Regina and Kathleen McKeown.2001.
Extracting paraphrases from aparallel corpus.
In Proceedings of ACL,pages 50?57, Toulouse.Barzilay, Regina and Kathleen R. McKeown.2005.
Sentence fusion for multidocumentnews summarization.
ComputationalLinguistics, 31(3):297?328.Bayer, Samuel, John Burger, Lisa Ferro, JohnHenderson, and Alexander Yeh.
2005.MITREs submissions to the EU PascalRTE Challenge.
In Proceedings of thePASCAL Challenges Workshop onRecognizing Textual Entailment,pages 41?44, Southampton, U.K.Beeferman, Doug and Adam Berger.
2000.Agglomerative clustering of a searchengine query log.
In Proceedings of the ACMSIGKDD International Conference onKnowledge Discovery and Data mining,pages 407?416, Boston, MA.Belz, Anja.
2009.
That?s nice...what can youdo with it?
Computational Linguistics,35(1):111?118.Bhagat, Rahul and Deepak Ravichandran.2008.
Large scale acquisition ofparaphrases for learning surfacepatterns.
In Proceedings of ACL,pages 674?682, Columbus, OH.Bosma, Wauter and Chris Callison-Burch.2007.
Paraphrase substitution forrecognizing textual entailment.In Evaluation of Multilingual andMultimodal Information Retrieval,Lecture Notes in Computer Science,Volume 4730, Springer-Verlag,pages 502?509.Brockett, Chris and William B. Dolan.
2005.Support vector machines for paraphraseidentification and corpus construction.
InProceedings of the Third InternationalWorkshop on Paraphrasing, pages 1?8,Jeju Island.Broder, Andrei.
1997.
On the resemblanceand containment of documents.
InProceedings of the Compression andComplexity of Sequences, pages 21?29,Salemo.Brown, Peter F., John Cocke, Stephen DellaPietra, Vincent J. Della Pietra, FrederickJelinek, John D. Lafferty, Robert L. Mercer,and Paul S. Roossin.
1990.
A StatisticalApproach to Machine Translation.Computational Linguistics, 16(2):79?85.Brown, Peter F., Stephen A. Della Pietra,Vincent J. Della Pietra, and Robert L.Mercer.
1993.
The mathematics ofstatistical machine translation: Parameterestimation.
Computational Linguistics,19(2):263?311.Buchholz, Sabine and Erwin Marsi.
2006.CoNLL-X shared task on multilingualdependency parsing.
In Proceedings of theConference on Computational NaturalLanguage Learning (CoNLL-X),pages 149?164, New York, NY.Callison-Burch, Chris.
2007.
Paraphrasing andTranslation.
Ph.D. thesis, School ofInformatics, University of Edinburgh.382Madnani and Dorr Generating Phrasal and Sentential ParaphrasesCallison-Burch, Chris.
2008.
Syntacticconstraints on paraphrases extracted fromparallel corpora.
In Proceedings of EMNLP,pages 196?205, Waikiki, HI.Callison-Burch, Chris, Trevor Cohn, andMirella Lapata.
2006.
Annotationguidelines for paraphrase alignment.Technical report, University of Edinburgh.http://www.dcs.shef.ac.uk/?tcohn/paraphrase guidelines.pdf.Callison-Burch, Chris, Trevor Cohn, andMirella Lapata.
2008.
ParaMetric: Anautomatic evaluation metric forparaphrasing.
In Proceedings of COLING,pages 97?104, Manchester.Callison-Burch, Chris, Philipp Koehn, andMiles Osborne.
2006.
Improved statisticalmachine translation using paraphrases.
InProceedings of NAACL, pages 17?24,New York, NY.Callison-Burch, Chris, David Talbot,and Miles Osborne.
2004.
Statisticalmachine translation with word- andsentence-aligned parallel corpora.
InProceedings of ACL, pages 176?183,Barcelona.Charikar, Moses.
2002.
Similarity estimationtechniques from rounding algorithms.
InProceedings of the 34th Annual ACMSymposium on Theory of Computing,pages 380?388, Montre?al.Chiang, David.
2006.
An Introduction toSynchronous Grammars.
Part of a tutorialgiven at ACL.
Sydney, Australia.Chiang, David.
2007.
Hierarchicalphrase-based translation.
ComputationalLinguistics, 33(2):201?228.Cohen, J.
1960.
A coefficient of agreement fornominal scales.
Educational andPsychological Measurement, 20:3746.Cohn, Trevor, Chris Callison-Burch, andMirella Lapata.
2008.
Constructing corporafor the development and evaluation ofparaphrase systems.
ComputationalLinguistics, 34:597?614.Cohn, Trevor and Mirella Lapata.
2007.
Largemargin synchronous generation and itsapplication to sentence compression.
InProceedings of EMNLP-CoNLL, pages 73?82,Prague.Corley, Courtney and Rada Mihalcea.2005.
Measuring the semantic similarityof texts.
In Proceedings of the ACL Workshopon Empirical Modeling of SemanticEquivalence and Entailment, pages 13?18,Ann Arbor, MI.Crouch, Carolyn J. and Bokyung Yang.1992.
Experiments in automatic statisticalthesaurus construction.
In Proceedingsof the ACM SIGIR conference on Researchand Development in InformationRetrieval, pages 77?88, Copenhagen,Denmark.Culicover, P. W. 1968.
Paraphrase generationand information retrieval from stored text.Mechanical Translation and ComputationalLinguistics, 11(1?2):78?88.Dagan, Ido.
2008.
Invited Talk: It?s time fora semantic engine!
In Proceedings of theNSF Symposium on Semantic KnowledgeDiscovery, Organization and Use,pages 20?28, New York, NY.Dagan, Ido, Oren Glickman, and BernardoMagnini.
2006.
The PASCAL RecognisingTextual Entailment Challenge.
In MachineLearning Challenges, Lecture Notes inComputer Science, Volume 3944,Springer-Verlag, pages 177?190.Das, Dipanjan and Noah A. Smith.
2009.Paraphrase identification as probabilisticquasi-synchronous recognition.
InProceedings of ACL/IJCNLP, pages 468?476,Singapore.Dele?ger, Louise and Pierre Zweigenbaum.2009.
Extracting lay paraphrases ofspecialized expressions from monolingualcomparable medical corpora.
InProceedings of the ACL Workshop on Buildingand Using Comparable Corpora, pages 2?10,Singapore.Dolan, Bill and Ido Dagan, editors.
2005.Proceedings of the ACL Workshop onEmpirical Modeling of Semantic Equivalenceand Entailment, Ann Arbor, MI.Dolan, William, Chris Quirk, and ChrisBrockett.
2004.
Unsupervised constructionof large paraphrase corpora: Exploitingmassively parallel news sources.
InProceedings of COLING, pages 350?356,Geneva.Dolan, William B. and Chris Brockett.
2005.Automatically constructing a corpus ofsentential paraphrases.
In Proceedingsof the Third International Workshop onParaphrasing, pages 9?16, Jeju Island.Dras, Mark.
1999.
A Meta-level grammar:Redefining synchronous TAG fortranslation and paraphrase.
In Proceedingsof ACL, pages 80?88, College Park, MD.Dras, Mark and Kazuhide Yamamoto,editors.
2005.
Proceedings of the ThirdInternational Workshop on Paraphrasing,Jeju Island.Duclaye, Florence, Franc?ois Yvon, andOlivier Collin.
2003.
Learning paraphrasesto improve a question-answering system.In Proceedings of the EACL Workshop onNatural Language Processing for383Computational Linguistics Volume 36, Number 3Question-Answering, pages 35?41,Budapest.Durbin, Richard, Sean R. Eddy, AndersKrogh, and Graeme Mitchison.
1998.Biological Sequence Analysis: ProbabilisticModels of Proteins and Nucleic Acids.Cambridge University Press, Cambridge.Edmonds, Philip and Graeme Hirst.
2002.Near-synonymy and lexical choice.Computational Linguistics, 28(2):105?144.Elhadad, Noemie and Komal Sutaria.
2007.Mining a lexicon of technical terms and layequivalents.
In Proceedings of the ACLBioNLP Workshop, pages 49?56, Prague.Fraser, Alexander and Daniel Marcu.
2007.Measuring word alignment quality forstatistical machine translation.Computational Linguistics, 33(3):293?303.Fujita, Atsushi, Kentaro Furihata, KentaroInui, Yuji Matsumoto, and KoichiTakeuchi.
2004.
Paraphrasing of Japaneselight-verb constructions based on lexicalconceptual structure.
In Proceedings of theACL Workshop on Multiword Expressions:Integrating Processing, pages 9?16,Barcelona.Fujita, Atsushi and Kentaro Inui.
2005.
AClass-oriented approach to buildinga paraphrase corpus.
In Proceedingsof the Third International Workshop onParaphrasing, pages 25?32, Jeju Island.Fujita, Atsushi, Shuhei Kato, Naoki Kato,and Satoshi Sato.
2007.
A compositionalapproach toward dynamic phrasalthesaurus.
In Proceedings of theACL-PASCAL Workshop on TextualEntailment and Paraphrasing,pages 151?158, Prague.Fujita, Atsushi and Satoshi Sato.
2008a.
Aprobabilistic model for measuringgrammaticality and similarity ofautomatically generated paraphrases ofpredicate phrases.
In Proceedings ofCOLING, pages 225?232, Manchester.Fujita, Atsushi and Satoshi Sato.
2008b.Computing paraphrasability of syntacticvariants using Web snippets.
In Proceedingsof IJCNLP, pages 537?544, Hyderabad.Gale, William A. and Kenneth W. Church.1991.
A program for aligning sentences inbilingual corpora.
In Proceedings of ACL,pages 177?184, Berkeley, CA.Gardent, Claire, Marilisa Amoia, andEvelyne Jacquey.
2004.
Paraphrasticgrammars.
In Proceedings of the SecondWorkshop on Text Meaning andInterpretation, pages 73?80, Barcelona.Gardent, Claire and Eric Kow.
2005.Generating and selecting grammaticalparaphrases.
In Proceedings of the EuropeanWorkshop on Natural Language Generation,pages 49?57, Abderdeen.Garoufi, Konstantina.
2007.
Towards a BetterUnderstanding of Applied Textual Entailment:Annotation and Evaluation of the RTE-2Dataset.
Master?s thesis, Language Scienceand Technology, Saarland University.Gasperin, Caroline, P. Gamallo, A. Agustini,G.
Lopes, and Vera de Lima.
2001.Using syntactic contexts for measuringword similarity.
In Proceedings of theWorkshop on Knowledge Acquisition andCategorization, ESSLLI, pages 18?23,Helsinki.Giampiccolo, Danilo, Hoa Dang, Ido Dagan,Bill Dolan, and Bernardo Magnini, editors.2008.
Proceedings of the Text AnalysisConference (TAC): Recognizing TextualEntailment Track, Gaithersburg, MD.Glickman, Oren and Ido Dagan.
2003.Identifying lexical paraphrases from asingle corpus: A case study for verbs.
InRecent Advantages in Natural LanguageProcessing (RANLP?03), pages 81?90,Borovets.Grefenstette, G. 1994.
Explorations inAutomatic Thesaurus Discovery.
KluwerAcademic Press, Dordrecht.Gusfield, D. 1997.
Algorithms on Strings, Trees,and Sequences: Computational Science andComputational Biology.
CambridgeUniversity Press, Cambridge.Hallett, Catalina and Donia Scott.
2005.Structural variation in generated healthreports.
In Proceedings of the ThirdInternational Workshop on Paraphrasing,pages 33?40, Jeju Island.Harris, Zellig.
1954.
Distributional structure.Word, 10(2):3.146?162.Hearst, Graeme.
1995.
Near-synonymy andthe structure of lexical knowledge.
InWorking notes of the AAAI SpringSymposium on Representation and Acquisitionof Lexical Knowledge, Stanford, CA.Hirst, Graeme.
2003.
Paraphrasingparaphrased.
Unpublished invited talk atthe ACL International Workshop onParaphrasing, Sapporo, Japan.Hovy, Eduard H. 1988.
Generating NaturalLanguage under Pragmatic Constraints.Lawrence Erlbaum Associates, Inc.,Mahwah, NJ.Huang, Shudong, David Graff, and GeorgeDoddington.
2002.
Multiple-translationchinese corpus.
Linguistic DataConsortium.
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2002T01384Madnani and Dorr Generating Phrasal and Sentential ParaphrasesIbrahim, Ali, Boris Katz, and Jimmy Lin.2003.
Extracting structural paraphrasesfrom aligned monolingual corpora.
InProceedings of the International Workshop onParaphrasing, pages 57?64, Sapporo.Iftene, Adrian.
2009.
Textual Entailment.
Ph.D.thesis, Faculty of Computer Science,University of Ias?i.Inoue, Naomi.
1991.
Automatic nounclassification by using Japanese-Englishword pairs.
In Proceedings of ACL,pages 201?208, Berkeley, CA.Inui, Kentaro and Ulf Hermjakob, editors.2003.
Proceedings of the Second InternationalWorkshop on Paraphrasing.
Association forComputational Linguistics, Sapporo.Inui, Kentaro and Masaru Nogami.
2001.
Aparaphrase-based exploration ofcohesiveness criteria.
In Proceedings of theEuropean Workshop on Natural LanguageGeneration (ENLG?01), pages 1?10,Toulouse.Jacquemin, Christian.
1999.
Syntagmatic andparadigmatic representations of termvariation.
In Proceedings of ACL,pages 341?348, College Park, MD.Joa?o, Cordeiro, Gae?l Dias, and Brazdil Pavel.2007a.
A metric for paraphrase detection.In Proceedings of the Second InternationalMulti-Conference on Computing in theGlobal Information Technology, page 7,Guadeloupe.Joa?o, Cordeiro, Gae?l Dias, and Brazdil Pavel.2007b.
New functions for unsupervisedasymmetrical paraphrase detection.Journal of Software, 2(4):12?23.Jones, Rosie, Benjamin Rey, Omid Madani,and Wile Greiner.
2006.
Generating querysubstitutions.
In Proceedings of the WorldWide Web Conference, pages 387?396,Edinburgh.Kauchak, David and Regina Barzilay.
2006.Paraphrasing for automatic evaluation.
InProceedings of HLT-NAACL, pages 455?462,New York, NY.Koehn, Philipp, Franz Josef Och, and DanielMarcu.
2003.
Statistical phrase-basedtranslation.
In Proceedings of HLT-NAACL,pages 48?54, Edmonton.Kok, Stanley and Chris Brockett.
2010.Hitting the right paraphrases in goodtime.
In Proceedings of NAACL, LosAngeles, CA.Lin, Dekang.
1998.
Automatic Retrieval andClustering of Similar Words.
In Proceedingsof ACL-COLING, pages 768?774, Montre?al.Lin, Dekang and Lin Pantel.
2001.
DIRT -Discovery of Inference Rules from Text.
InProceedings of ACM SIGKDD Conference onKnowledge Discovery and Data Mining,pages 323?328, San Francisco, CA.Lopez, Adam.
2008.
Statistical machinetranslation.
ACM Computing Surveys,40(3):1?49.Lopez, Adam and Philip Resnik.
2006.Word-based alignment, phrase-basedtranslation: What?s the link?
In Proceedingsof AMTA, pages 90?99, Boston, MA.Madnani, Nitin, Necip Fazil Ayan, PhilipResnik, and Bonnie J. Dorr.
2007.
Usingparaphrases for parameter tuning instatistical machine translation.
InProceedings of the Workshop on StatisticalMachine Translation, pages 120?127, Prague.Madnani, Nitin, Philip Resnik, Bonnie J.Dorr, and Richard Schwartz.
2008a.Applying automatically generatedsemantic knowledge: A case study inmachine translation.
In Proceedingsof the NSF Symposium on SemanticKnowledge Discovery, Organization and Use,pages 60?61, New York, NY.Madnani, Nitin, Philip Resnik, Bonnie J.Dorr, and Richard Schwartz.
2008b.
Aremultiple reference translations necessary?Investigating the value of paraphrasedreference translations in parameteroptimization.
In Proceedings of the EighthConference of the Association for MachineTranslation in the Americas (AMTA),pages 143?152, Waikiki, HI.Malakasiotis, Prodromos.
2009.
Paraphraserecognition using machine learning tocombine similarity measures.
InProceedings of the ACL-IJCNLP 2009 StudentResearch Workshop, pages 27?35, Singapore.Marsi, Erwin and Emiel Krahmer.
2005a.Classification of semantic relations byhumans and machines.
In Proceedings of theACL Workshop on Empirical Modeling ofSemantic Equivalence and Entailment,pages 1?6, Ann Arbor, MI.Marsi, Erwin and Emiel Krahmer.
2005b.Explorations in sentence fusion.In Proceedings of the European Workshopon Natural Language Generation,pages 109?117, Aberdeen.Max, Aure?lien.
2008.
Local rephrasingsuggestions for supporting the work ofwriters.
In Proceedings of GoTAL,pages 324?335, Gothenburg.McKeown, Kathleen R. 1979.
Paraphrasingusing given and new information in aquestion-answer system.
In Proceedings ofACL, pages 67?72, San Diego, CA.Melamed, Dan.
2001.
Empirical Methods forExploiting Parallel Texts.
MIT Press,Cambridge, MA.385Computational Linguistics Volume 36, Number 3Melamed, I. Dan.
1997.
A word-to-wordmodel of translational equivalence.
InProceedings of ACL, pages 490?497, Madrid.Metzler, Donald, Susan Dumais, andChristopher Meek.
2007.
Similaritymeasures for short segments of text.
InProceedings of the European Conference onInformation Retrieval (ECIR), pages 16?27,Rome.Mohri, Mehryar and Michael Riley.
2002.
Anefficient algorithm for the n-best-stringsproblem.
In Proceedings of the 7thInternational Conference on Spoken LanguageProcessing (ICSLP?02), pages 1313?1316,Denver, CO.Murakami, Akiko and Tetsuya Nasukawa.2004.
Term aggregation: Miningsynonymous expressions using personalstylistic variations.
In Proceedings ofCOLING, pages 806?812, Geneva.NIST.
2009.
NIST Open Machine Translation(MT) Evaluation.
Information AccessDivision.
http://www.nist.gov/speech/tests/mt/.Nivre, Joakim, Johan Hall, Sandra Ku?bler,Ryan McDonald, Jens Nilsson, SebastianRiedel, and Deniz Yuret.
2007.
TheCoNLL 2007 shared task on dependencyparsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007,pages 915?932, Prague.Owczarzak, Karolina, Declan Groves, JosefVan Genabith, and Andy Way.
2006.Contextual bitext-derived paraphrases inautomatic MT evaluation.
In Proceedings onthe Workshop on Statistical MachineTranslation, pages 86?93, New York, NY.Pang, Bo, Kevin Knight, and Daniel Marcu.2003.
Syntax-based alignment of multipletranslations: Extracting paraphrasesand generating new sentences.
InProceedings of HLT-NAACL, pages 102?109,Edmonton.Pantel, Patrick, Rahul Bhagat, BonaventuraCoppola, Timothy Chklovski, and EduardHovy.
2007.
ISP: Learning inferentialselectional preferences.
In Proceedings ofNAACL, pages 564?571, Rochester, NY.Papineni, K., S. Roukos, T. Ward, and W.-J.Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
InProceedings of ACL, pages 311?318,Philadelphia, PA.Pas?ca, Marius and Pe?ter Dienes.
2005.Aligning needles in a haystack: Paraphraseacquisition across the Web.
In Proceedingsof IJCNLP, pages 119?130, Jeju Island.Pereira, Fernando, Naftali Tishby, and LillianLee.
1993.
Distributional clustering ofEnglish words.
In Proceedings of ACL,pages 183?190, Columbus, OH.Power, Richard and Donia Scott.
2005.Automatic generation of large-scaleparaphrases.
In Proceedings of the ThirdInternational Workshop on Paraphrasing,pages 57?64, Jeju Island.Quirk, Chris, Chris Brockett, and WilliamDolan.
2004.
Monolingual machinetranslation for paraphrase generation.
InProceedings of EMNLP, pages 142?149,Barcelona.Ramshaw, Lance and Mitch Marcus.
1995.Text chunking using transformation-basedlearning.
In Proceedings of the ThirdWorkshop on Very Large Corpora,pages 82?94, Cambridge, MA.Ravichandran, Deepak and Eduard Hovy.2002.
Learning surface text patterns for aquestion answering system.
In Proceedingsof ACL, pages 41?47, Philadelphia, PA.Ravichandran, Deepak, Patrick Pantel, andEduard H. Hovy.
2005.
Randomizedalgorithms and NLP: Using localitysensitive hash function for high speednoun clustering.
In Proceedings of ACL,pages 622?629, Ann Arbor, MI.Riezler, Stefan, Alexander Vasserman,Ioannis Tsochantaridis, Vibhu O. Mittal,and Yi Liu.
2007.
Statistical machinetranslation for query expansion inanswer retrieval.
In Proceedings of ACL,pages 464?471, Prague.Romano, Lorenza, Milen Kouylekov,Idan Szpektor, Ido Dagan, and AlbertoLavelli.
2006.
Investigating a genericparaphrase-based approach for relationextraction.
In Proceedings of EACL,pages 409?416, Trento.Rus, Vasile, Philip M. McCarthy, andMihai C. Lintean.
2008.
Paraphraseidentification with lexico-syntactic graphsubsumption.
In Proceedings of the 21stInternational FLAIRS Conference,pages 201?206, Coconut Grove, FL.Sahami, Mehran and Timothy D. Heilman.2006.
A Web-based kernel function formeasuring the similarity of short textsnippets.
In Proceedings of the WorldWide Web Conference, pages 377?386,Edinburgh.Sekine, Satoshi.
2005.
Automatic paraphrasediscovery based on context and keywordsbetween NE pairs.
In Proceedings of theInternational Workshop on Paraphrasing,pages 80?87, Jeju Island, South Korea.Sekine, Satoshi.
2006.
On-demandinformation extraction.
In Proceedings ofCOLING-ACL, pages 731?738, Sydney.386Madnani and Dorr Generating Phrasal and Sentential ParaphrasesSekine, Satoshi, Kentaro Inui, Ido Dagan,Bill Dolan, Danilo Giampiccolo, andBernardo Magnini, editors.
2007.Proceedings of the ACL-PASCAL Workshopon Textual Entailment and Paraphrasing.Association for ComputationalLinguistics, Prague.Shen, Siwei, Dragomir R. Radev, AgamPatel, and Gu?nes?
Erkan.
2006.
Addingsyntax to dynamic programming foraligning comparable texts for thegeneration of paraphrases.
In Proceedingsof ACL-COLING, pages 747?754,Sydney.Shi, Xiaodong and Christopher C. Yang.2007.
Mining related queries from Websearch engine query logs using animproved association rule mining model.JASIST, 58(12):1871?1883.Shimohata, Mitsuo and Eiichiro Sumita.2005.
Acquiring synonyms frommonolingual comparable texts.
InProceedings of IJCNLP, pages 233?244,Jeju Island.Shinyama, Y., S. Sekine, K. Sudo, andR.
Grishman.
2002.
Automatic paraphraseacquisition from news articles.
InProceedings of HLT, pages 313?318,San Diego, CA.Soong, Frank K. and Eng-Fong Huang.1990.
A tree-trellis based fast search forfinding the n-best sentence hypotheses incontinuous speech recognition.
InProceedings of the HLT workshop on Speechand Natural Language, pages 12?19,Hidden Valley, PA.Spa?rck-Jones, Karen and J. I. Tait.
1984.Automatic search term variantgeneration.
Journal of Documentation,40(1):50?66.Steedman, Mark, editor.
1996.
SurfaceStructure and Interpretation (LinguisticInquiry Monograph No.
30).
MIT Press,Cambridge, MA.Szpektor, Idan, Eyal Shnarch, and Ido Dagan.2007.
Instance-based evaluation ofentailment rule acquisition.
In Proceedingsof ACL, pages 456?463, Prague.Tanaka, Takaaki, Aline Villavicencio, FrancisBond, and Anna Korhonen, editors.
2004.Proceedings of the Workshop on MultiwordExpressions: Integrating Processing.Association for Computational Linguistics,Barcelona.Uzuner, O?zlem and Boris Katz.
2005.Capturing expression using linguisticinformation.
In Proceedings of AAAI,pages 1124?1129, Pittsburgh, PA.Wallis, Peter.
1993.
Information retrievalbased on paraphrase.
In Proceedings of the3rd Conference of the Pacific Association forComputational Linguistics (PACLING),pages 118?126, Vancouver.Wu, Dekai.
2005.
Recognizing paraphrasesand textual entailment using inversiontransduction grammars.
In Proceedings ofthe ACL Workshop on Empirical Modeling ofSemantic Equivalence and Entailment,pages 25?30, Ann Arbor, MI.Wu, Hua and Ming Zhou.
2003.
Synonymouscollocation extraction using translationinformation.
In Proceedings of the ACLWorkshop on Multiword Expressions:Integrating Processing, pages 120?127,Sapporo.Zhao, Shiqi, Xiang Lan, Ting Liu, and ShengLi.
2009.
Application-driven statisticalparaphrase generation.
In Proceedings ofACL/AFNLP, pages 834?842, Singapore.Zhao, Shiqi, Cheng Niu, Ming Zhou, TingLiu, and Sheng Li.
2008.
Combiningmultiple resources to improve SMT-basedparaphrasing model.
In Proceedings ofACL-08: HLT, pages 1021?1029,Columbus, OH.Zhou, Liang, Chin-Yew Lin, and EduardHovy.
2006.
Re-evaluating machinetranslation results with paraphrasesupport.
In Proceedings of EMNLP,pages 77?84, Sydney.Zhou, Liang, Chin-Yew Lin, Dragos StefanMuntenau, and Eduard Hovy.
2006.ParaEval: Using paraphrases toevaluate summaries automatically.
InProceedings of HLT-NAACL, pages 447?454,New York, NY.387
