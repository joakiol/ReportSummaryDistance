Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 85?91,October 29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsExploring System Combination approaches for Indo-Aryan MT SystemsKaran Singla1, Nishkarsh Shastri2, Megha Jhunjhunwala2, Anupam Singh3,Srinivas Bangalore4, Dipti Misra Sharma11LTRC IIIT Hyderabad,2IIT-Kharagpur,3NIT-Durgapur,4AT&T Labs-ResearchAbstractStatistical Machine Translation (SMT)systems are heavily dependent on the qual-ity of parallel corpora used to train transla-tion models.
Translation quality betweencertain Indian languages is often poor dueto the lack of training data of good qual-ity.
We used triangulation as a techniqueto improve the quality of translations incases where the direct translation modeldid not perform satisfactorily.
Triangula-tion uses a third language as a pivot be-tween the source and target languages toachieve an improved and more efficienttranslation model in most cases.
We alsocombined multi-pivot models using linearmixture and obtained significant improve-ment in BLEU scores compared to the di-rect source-target models.1 IntroductionCurrent SMT systems rely heavily on large quan-tities of training data in order to produce goodquality translations.
In spite of several initiativestaken by numerous organizations to generate par-allel corpora for different language pairs, train-ing data for many language pairs is either notyet available or is insufficient for producing goodSMT systems.
Indian Languages Corpora Initia-tive (ILCI) (Choudhary and Jha, 2011) is currentlythe only reliable source for multilingual parallelcorpora for Indian languages however the numberof parallel sentences is still not sufficient to createhigh quality SMT systems.This paper aims at improving SMT systemstrained on small parallel corpora using various re-cently developed techniques in the field of SMTs.Triangulation is a technique which has been foundto be very useful in improving the translationswhen multilingual parallel corpora are present.Triangulation is the process of using an interme-diate language as a pivot to translate a source lan-guage to a target language.
We have used phrasetable triangulation instead of sentence based tri-angulation as it gives better translations (Utiyamaand Isahara, 2007).
As triangulation technique ex-plores additional multi parallel data, it providesus with separately estimated phrase-tables whichcould be further smoothed using smoothing meth-ods (Koehn et al.
2003).
Our subsequent approachwill explore the various system combination tech-niques through which these triangulated systemscan be utilized to improve the translations.The rest of the paper is organized as follows.We will first talk about the some of the relatedworks and then we will discuss the facts about thedata and also the scores obtained for the baselinetranslation model.
Section 3 covers the triangu-lation approach and also discusses the possibilityof using combination approaches for combiningtriangulated and direct models.
Section 4 showsresults for the experiments described in previoussection and also describes some interesting obser-vations from the results.
Section 5 explains theconclusions we reached based on our experiments.We conclude the paper with a section about our fu-ture work.2 Related WorksThere are various works on combining the tri-angulated models obtained from different pivotswith the direct model resulting in increased con-fidence score for translations and increased cov-erage by (Razmara and Sarkar, 2013; Ghannay etal., 2014; Cohn and Lapata, 2007).
Among thesetechniques we explored two of the them.
The firstone is the technique based on the confusion ma-trix (dynamic) (Ghannay et al., 2014) and the otherone is based on mixing the models as exploredby (Cohn and Lapata, 2007).
The paper also dis-cusses the better choice of combination technique85among these two when we have limitations ontraining data which in our case was small and re-stricted to a small domain (Health & Tourism).As suggested in (Razmara and Sarkar, 2013),we have shown that there is an increase in phrasecoverage when combining the different systems.Conversely we can say that out of vocabularywords (OOV) always decrease in the combinedsystems.3 Baseline Translation ModelIn our experiment, the baseline translation modelused was the direct system between the source andtarget languages which was trained on the sameamount of data as the triangulated models.
Theparallel corpora for 4 Indian languages namelyHindi (hn), Marathi (mt), Gujarati (gj) and Bangla(bn) was taken from Indian Languages CorporaInitiative (ILCI) (Choudhary and Jha, 2011) .
Theparallel corpus used in our experiments belongedto two domains - health and tourism and the train-ing set consisted of 28000 sentences.
The develop-ment and evaluation set contained 500 sentenceseach.
We used MOSES (Koehn et al., 2007) totrain the baseline Phrase-based SMT system for allthe language pairs on the above mentioned paral-lel corpus as training, development and evaluationdata.
Trigram language models were trained usingSRILM (Stolcke and others, 2002).
Table 1 belowshows the BLEU score for all the trained pairs.Language Pair BLEU Scorebn-mt 18.13mt-bn 21.83bn-gj 22.45gj-mt 23.02gj-bn 24.26mt-gj 25.5hn-mt 30.01hn-bn 32.92bn-hn 34.99mt-hn 36.82hn-gj 40.06gj-hn 43.48Table 1: BLEU scores of baseline models4 Triangulation: Methodology andExperimentWe first define the term triangulation in our con-text.
Each source phrase s is first translated to anintermediate (pivot) language i, and then to a tar-get language t. This two stage translation processis termed as triangulation.Our basic approach involved making triangu-lated models by triangulating through differentpivots and then interpolating triangulated modelswith the direct source-target model to make ourcombined model.In line with various previous works, we willbe using multiple translation models to overcomethe problems faced due to data sparseness and in-crease translational coverage.
Rather than usingsentence translation (Utiyama and Isahara, 2007)from source to pivot and then pivot to target, aphrase based translation model is built.Hence the main focus of our approach is onphrases rather than on sentences.
Instead of usingcombination techniques on the output of severaltranslation systems, we constructed a combinedphrase table to be used by the decoder thus avoid-ing the additional inefficiencies observed whilemerging the output of various translation systems.Our method focuses on exploiting the availabilityof multi-parallel data, albeit small in size, to im-prove the phrase coverage and quality of our SMTsystem.Our approach can be divided into different stepswhich are presented in the following sections.4.1 Phrase-table triangulationOur emphasis is on building an enhanced phrasetable that incorporates the translation phrase tablesof different models.
This combined phrase tablewill be used by the decoder during translation.Phrase table triangulation depends mainly onphrase level combination of the two differentphrase based systems mainly source (src) - pivot(pvt) and pivot (pvt) - target (tgt) using pivot lan-guage as a basis for combination.
Before statingthe mathematical approach for triangulation, wepresent an example.4.1.1 Basic methodologySuppose we have a Bengali-Hindi phrase-table(TBH) and a Hindi-Marathi phrase-table (THM).From these tables, we have to construct a Bengali-Marathi phrase-table (TBM).
For that we need86TriangulatedSystemFull-Triangulation(phrase-table length)Triangulation with top 40(Length of phrase table)Full Triangulation(BLEU Score)Triangulation with top 40(BLEU SCORE)gj - hn - mt 3,585,450 1,086,528 24.70 24.66gj - bn - mt 7,916,661 1,968,383 20.55 20.04Table 2: Comparison between triangulated systems in systems with full phrase table and the other havingtop 40 phrase-table entriesto estimate four feature functions: phrase trans-lation probabilities for both directions ?(?b|m?
)and ?
(m?|?b), and lexical translation probabilitiesfor both directions lex(?b|m?)
and lex(m?|?b) where?b and m?
are Bengali and Marathi phrases thatwill appear in our triangulated Bengali-Marathiphrase-table TBM.?(?b|m?)
=??h?TBH?THM?(?b|?h)?(?h|m?)
(1)?
(m?|?b) =??h?TBH?THM?(m?|?h)?
(?h|?b) (2)lex(?b|m?)
=??h?TBH?THMlex(?b|?h)lex(?h|m?)
(3)lex(m?|?b) =?
?h?TBH?THMlex(m?|?h)lex(?h|?b) (4)In these equations a conditional independenceassumption has been made that source phrase?band target phrase m?
are independent given theircorresponding pivot phrase(s)?h.
Thus, we canderive ?(?b|m?
), ?
(m?|?b), lex(?b|m?
), lex(m?|?b) by as-suming that these probabilities are mutually inde-pendent given a Hindi phrase?h.The equation given requires that all phrases inthe Hindi-Marathi bitext must also be present inthe Bengali-Hindi bitext.
Clearly there would bemany phrases not following the above require-ment.
For this paper we completely discarded themissing phrases.
One important point to note isthat although the problem of missing contextualphrases is uncommon in multi-parallel corpora, asit is in our case, it becomes more evident when thebitexts are taken out from different sources.In general, wider range of possible translationsare found for any source phrase through triangula-tion.
We found that in the direct model, a sourcephrase is aligned to three phrases then there ishigh possibility of it being aligned to three phrasesin intermediate language.
The intermediate lan-guage phrases are further aligned to three or morephrases in target language.
This results in increasein number of translations of each source phrase.4.1.2 Reducing the size of phrase-tableWhile triangulation is intuitively appealing, it suf-fers from a few problems.
First, the phrasal trans-lation estimates are based on noisy automatic wordalignments.
This leads to many errors and omis-sions in the phrase-table.
With a standard source-target phrase-table these errors are only encoun-tered once, however with triangulation they are en-countered twice, and therefore the errors are com-pounded.
This leads to much noisier estimatesthan in the source-target phrase-table.
Secondly,the increased exposure to noise means that trian-gulation will omit a greater proportion of large orrare phrases than the standard method.
An align-ment error in either of the source-intermediate bi-text or intermediate-target bitext can prevent theextraction of a source-target phrase pair.As will be explained in the next section, the sec-ond kind of problem can be ameliorated by usingthe triangulated phrase-based table in conjunctionwith the standard phrase based table referred to asdirect src-to-pvt phrase table in our case.For the first kind of problem, not only the com-pounding of errors leads to increased complex-ity but also results in an absurdly large triangu-lated phrase based table.
To tackle the problem ofunwanted phrase-translation, we followed a novelapproach.A general observation is that while triangulat-ing between src-pvt and pvt-tgt systems, the re-sultant src-tgt phrase table formed will be verylarge since for a translation s?
to?i in the src-to-pvt table there may be many translations from?i to?t1,?t2...?tn.
For example, the Bengali-Hindiphrase-table(TBH) consisted of 846,106 transla-tions and Hindi-Marathi phrase-table(THM) con-sisted of 680,415 translations and after triangu-lating these two tables our new Bengali-Marathitriangulated table(TBM) consisted of 3,585,450translations as shown in Table 2.
Tuning withsuch a large phrase-table is complex and time-consuming.
To reduce the complexity of thephrase-table, we used only the top-40 transla-87tions (translation with 40 maximum values ofP (?f |e?)
for every source phrase in our triangulatedphrase-table(TBM) which reduced the phrase tableto 1,086,528 translations.We relied on P (?f |e?
)(inverse phrase translationprobability) to choose 40 phrase translations foreach phrase, since in the direct model, MERTtraining assigned the most weight to this param-eter.It is clearly evident from Table 2 that we havegot a massive reduction in the length of the phrase-table after taking in our phrase table and still theresults have no significant difference in our outputmodels.4.2 Combining different triangulated modelsand the direct modelCombining Machine translation (MT) systems hasbecome an important part of Statistical MT in thepast few years.
There have been several works by(Rosti et al., 2007; Karakos et al., 2008; Leuschand Ney, 2010);We followed two approaches1.
A system combination based on confusionnetwork using open-source tool kit MANY(Barrault, 2010), which can work dynami-cally in combining the systems2.
Combine the models by linearly interpolatingthem and then using MERT to tune the com-bined system.4.2.1 Combination based on confusionmatrixMANY tool was used for this and initially it wasconfigured to work with TERp evaluation matrix,but we modified it to work using METEOR-Hindi(Gupta et al., 2010), as it has been shown by(Kalyani et al., 2014), that METEOR evaluationmetric is closer to human evaluation for morpho-logically rich Indian Languages.4.2.2 Linearly Interpolated ModelsWe used two different approaches while mergingthe different triangulated models and direct src-tgtmodel and we observed that both produced com-parable results in most cases.
We implemented thelinear mixture approach, since linear mixtures of-ten outperform log-linear ones (Cohn and Lapata,2007).
Note that in our combination approachesthe reordering tables were left intact.1.
Our first approach was to use linear interpola-tion to combine all the three models (Bangla-Hin-Marathi, Bangla-Guj-Marathi and di-rect Bangla-Marathi models) with uniformweights, i.e 0.3 each in our case.2.
In the next approach, the triangulated phrasetables are combined first into a single trian-gulated phrase-table using uniform weights.The combined triangulated phrase-table anddirect src-tgt phrase table is then combinedusing uniform weights.
In other words, wecombined all the three systems, Ban-Mar,Ban-Hin-Mar, and Ban-Guj-Mar with 0.5,0.25 and 0.25 weights respectively.
Thisweight distribution reflects the intuition thatthe direct model is less noisy than the trian-gulated models.In the experiments below, both weight settingsproduced comparable results.
Since we performedtriangulation only through two languages, wecould not determine which approach would per-form better.
An ideal approach will be to train theweights for each system for each language pairusing standard tuning algorithms such as MERT(Zaidan, 2009).4.2.3 Choosing Combination ApproachIn order to compare the approaches on our data,we performed experiments on Hindi-Marathi pairfollowing both approaches discussed in Section4.2.1 and 4.2.2.
We also generated triangulatedmodels through Bengali and Gujarati as pivot lan-guages.Also, the approach presented in section 4.2.1depends heavily on LM (Language Model).In or-der to study the impact of size, we worked ontraining Phrase-based SMT systems with subsetsof data in sets of 5000, 10000, 150000 sentencesand LM was trained for 28000 sentences for com-paring these approaches.
The combination resultswere compared following the approach mentionedin 4.2.1 and 4.2.2.Table 3, shows that the approach discussed in4.2.1 works better if there is more data for LMbut we suffer from the limitation that there is noother in-domain data available for these languages.From the Table, it can also be seen that combin-ing systems with the approach explained in 4.2.2can also give similar or better results if there isscarcity of data for LM.
Therefore we followed the88#Training #LM Data Comb-1 Comb-25000 28000 21.09 20.2710000 28000 24.02 24.2715000 28000 27.10 27.63Table 3: BLEU scores for Hindi-Marathi Modelcomparing approaches described in 3.2.1(Comb-1) and 3.2.2(Comb-2)approach from Section 4.2.2 for our experimentson other language pairs.5 Observation and ResusltsTable 4, shows the BLEU scores of triangulatedmodels when using the two languages out of the4 Indian languages Hin, Guj, Mar, Ban as sourceand target and the remaining two as the pivot lan-guage.
The first row mentions the BLEU scoreof the direct src-tgt model for all the languagepairs.
The second and third rows provide the tri-angulated model scores through pivots which havebeen listed.
The fourth and fifth rows show theBLEU scores for the combined models (triangu-lated+direct) with the combination done using thefirst and second approach respectively that havebeen elucidated in the Section 4.2.2As expected, both the combined models haveperformed better than the direct models in allcases.Figure 1: Phrase-table coverage of the evaluationset for all the language pairsFigure 1, shows the phrase-table coverage of theevaluation set for all the language pairs.
Phrase-table coverage is defined as the percentage of un-igrams in the evaluation set for which translationsare present in the phrase-table.
The first bar cor-responds to the direct model for each languagepair, the second and third bars show the cover-age for triangulated models through the 2 piv-ots, while the fourth bar is the coverage for thecombined model (direct+triangulated).
The graphclearly shows that even though the phrase tablecoverage may increase or decrease by triangula-tion through a single pivot the combined model(direct+triangulated) always gives a higher cover-age than the direct model.Moreover, there exists some triangulation mod-els whose coverage and subsequent BLEU scoresfor translation is found to be better than that of thedirect model.
This is a particularly interesting ob-servation as it increases the probability of obtain-ing better or at least comparable translation mod-els even when direct source-target parallel corpusis absent.6 DiscussionDravidian languages are different from Indo-aryanlanguages but they are closely related amongstthemselves.
So we explored similar experimentswith Malayalam-Telugu pair of languages withsimilar parallel data and with Hindi as pivot.The hypothesis was that the direct model forMalayalam-Telegu would have performed betterdue to relatedness of the two languages.
Howeverthe results via Hindi were better as can be seen inTable 5.As Malayalam-Telegu are comparatively closerthan compared to Hindi, so the results via Hindishould have been worse but it seems more like abiased property of training data which considersthat all languages are closer to Hindi, as the trans-lation data was created from Hindi.7 Future WorkIt becomes increasingly important for us to im-prove these techniques for such languages havingrare corpora.
The technique discussed in the paperis although efficient but still have scope for im-provements.As we have seen from our two approaches ofcombining the phrase tables and subsequent in-terpolation with direct one, the best combinationamong the two is also not fixed.
If we can find the89BLEU scores gj-mt mt-gj gj-hn hn-gj hn-mt mt-hnDirect model 23.02 25.50 43.48 40.06 30.01 36.82Triangulatedthrough pivotshn 24.66 hn 27.09 mt 36.76 mt 33.69 gj 29.27 gj 33.86bn 20.04 bn 22.02 bn 35.07 bn 32.66 bn 26.72 bn 31.34Mixture-1 26.12 27.46 43.23 39.99 33.09 38.50Mixture-2 26.25 27.32 44.04 41.45 33.36 38.44(a)BLEU scores bn-gj gj-bn bn-hn hn-bn mt-bn bn-mtDirect model 22.45 24.26 34.99 32.92 21.83 18.13Triangulatedthrough pivotshn 23.97 hn 26.26 gj 31.69 gj 29.60 hn 23.80 hn 21.04mt 20.70 mt 22.32 mt 28.96 mt 27.95 gj 22.41 gj 18.15Mixture-1 25.80 27.45 35.14 34.77 24.99 22.16Mixture-2 24.66 27.39 35.02 34.85 24.86 22.75(b)Table 4: Table (a) & (b) show results for all language pairs after making triangulated models and thencombining them with linear interpolation with the two approaches described in 3.2.2.
In Mixture-1,uniform weights were given to all three models but in Mixture-2, direct model is given 0.5 weight relativeto the other models (.25 weight to each)System Blue ScoreDirect Model 4.63Triangulated via Hindi 14.32Table 5: Results for Malayalam-Telegu Pair forsame data used for other languagesbest possible weights to be assigned to each table,then we can see improvement in translation.
Thiscan be implemented by making the machine learnfrom various iterations of combining and adjustingthe scores accordingly.
(Nakov and Ng, 2012) haveindeed shown that results show significant devia-tions associated with different weights assigned tothe tables.ReferencesLo?
?c Barrault.
2010.
Many: Open source machinetranslation system combination.
The Prague Bul-letin of Mathematical Linguistics, 93:147?155.Narayan Choudhary and Girish Nath Jha.
2011.
Cre-ating multilingual parallel corpora in indian lan-guages.
In Proceedings of Language and Technol-ogy Conference.Trevor Cohn and Mirella Lapata.
2007.
Ma-chine translation by triangulation: Making ef-fective use of multi-parallel corpora.
In AN-NUAL MEETING-ASSOCIATION FOR COMPU-TATIONAL LINGUISTICS, volume 45, page 728.Citeseer.Sahar Ghannay, France Le Mans, and Lo?c Barrault.2014.
Using hypothesis selection based features forconfusion network mt system combination.
In Pro-ceedings of the 3rd Workshop on Hybrid Approachesto Translation (HyTra)@ EACL, pages 1?5.Ankush Gupta, Sriram Venkatapathy, and Rajeev San-gal.
2010.
Meteor-hindi: Automatic mt evaluationmetric for hindi as a target language.
In Proceed-ings of ICON-2010: 8th International Conferenceon Natural Language Processing.Aditi Kalyani, Hemant Kumud, Shashi Pal Singh, andAjai Kumar.
2014.
Assessing the quality of mt sys-tems for hindi to english translation.
arXiv preprintarXiv:1404.3992.Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,and Markus Dreyer.
2008.
Machine translationsystem combination using itg-based alignments.
InProceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics on HumanLanguage Technologies: Short Papers, pages 81?84.Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 177?180.
Association for Computational Lin-guistics.Gregor Leusch and Hermann Ney.
2010.
The rwthsystem combination system for wmt 2010.
In Pro-ceedings of the Joint Fifth Workshop on Statistical90Machine Translation and MetricsMATR, pages 315?320.
Association for Computational Linguistics.Preslav Nakov and Hwee Tou Ng.
2012.
Improv-ing statistical machine translation for a resource-poor language using related resource-rich lan-guages.
Journal of Artificial Intelligence Research,44(1):179?222.Majid Razmara and Anoop Sarkar.
2013.
Ensembletriangulation for statistical machine translation.
InProceedings of the Sixth International Joint Confer-ence on Natural Language Processing, pages 252?260.Antti-Veikko I Rosti, Spyridon Matsoukas, andRichard Schwartz.
2007.
Improved word-level sys-tem combination for machine translation.
In AN-NUAL MEETING-ASSOCIATION FOR COMPU-TATIONAL LINGUISTICS, volume 45, page 312.Citeseer.Andreas Stolcke et al.
2002.
Srilm-an extensible lan-guage modeling toolkit.
In INTERSPEECH.Masao Utiyama and Hitoshi Isahara.
2007.
A compari-son of pivot methods for phrase-based statistical ma-chine translation.
In HLT-NAACL, pages 484?491.Omar Zaidan.
2009.
Z-mert: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.91
