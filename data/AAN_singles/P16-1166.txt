Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1757?1768,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsSituation entity types:automatic classification of clause-level aspectAnnemarie Friedrich1Alexis Palmer2Manfred Pinkal11Department of Computational Linguistics, Saarland University, Germany2Leibniz ScienceCampus, Dept.
of Computational Linguistics, Heidelberg University, Germany{afried,pinkal}@coli.uni-saarland.de palmer@cl.uni-heidelberg.deAbstractThis paper describes the first robust ap-proach to automatically labeling clauseswith their situation entity type (Smith,2003), capturing aspectual phenomena atthe clause level which are relevant forinterpreting both semantics at the clauselevel and discourse structure.
Previouswork on this task used a small data setfrom a limited domain, and relied mainlyon words as features, an approach whichis impractical in larger settings.
We pro-vide a new corpus of texts from 13 genres(40,000 clauses) annotated with situationentity types.
We show that our sequencelabeling approach using distributional in-formation in the form of Brown clusters,as well as syntactic-semantic features tar-geted to the task, is robust across genres,reaching accuracies of up to 76%.1 IntroductionClauses in text have different aspectual properties,and thus contribute to the discourse in differentways.
Distinctions that have been made in thelinguistic and semantic theory literature includethe classification of states, events and processes(Vendler, 1957; Bach, 1986), and whether clausesintroduce particular eventualities or report regular-ities generalizing either over events or membersof a kind (Krifka et al, 1995).
Such aspectualdistinctions are relevant to natural language pro-cessing tasks requiring text understanding such asinformation extraction (Van Durme, 2010) or tem-poral processing (Costa and Branco, 2012).In this paper, we are concerned with automat-ically identifying the type of a situation entity(SE), which we assume to be expressed by aclause.
Specifically, we present a system for au-tomatically labeling clauses using the inventory ofSTATE: The colonel owns the farm.EVENT: John won the race.REPORT: ?...
?, said Obama.GENERIC SENTENCE: Generalizations overkinds.
The lion has a bushy tail.GENERALIZING SENTENCE:Generalizations over events (habituals).Mary often fed the cat last year.QUESTION: Who wants to come?IMPERATIVE: Hand me the pen!Figure 1: SE types, adapted from Smith (2003).SE types shown in Figure 1 (Smith, 2003, 2005;Palmer et al, 2007).
The original motivation forthe above inventory of SE types is the observationthat different modes of discourse, a classificationof linguistic properties of text at the passage level,have different distributions of SE types.
For ex-ample, EVENTs and STATEs are predominant innarrative passages, while GENERIC SENTENCEsoccur frequently in information passages.A previous approach to automatically labelingSE types (Palmer et al, 2007) ?
referred to here asUT07 ?
captures interesting insights, but is trainedand evaluated on a relatively small amount of text(about 4300 clauses), mainly from one rather spe-cialized subsection of the Brown corpus.
The datashows a highly skewed distribution of SE typesand was annotated in an intuitive fashion with onlymoderate agreement.
In addition, the UT07 sys-tem relies mostly on part-of-speech tags and wordsas features.
The latter are impractical when deal-ing with larger data sets and capture most of thecorpus vocabulary, overfitting the model to thedata set.
Despite this overfitting, the system?s ac-curacy is only around 53%.We address these shortcomings, developing arobust system that delivers high performance com-pared to the human upper bound across a range ofgenres.
Our approach uses features which increase1757robustness: Brown clusters and syntactic-semanticfeatures.
Our models for labeling texts with theaspectual properties of clauses in the form of SEtypes reach accuracies of up to 76%.In an oracle experiment, Palmer et al (2007)show that including the gold labels of the previ-ous clauses as features into their maximum en-tropy model is beneficial.
We implement the firsttrue sequence labeling model for SE types, us-ing conditional random fields to find the globally-best sequence of labels for the clauses in a docu-ment.
Performance increases by around 2% abso-lute compared to predicting labels for clauses sep-arately; much of this effect stems from the fact thatGENERIC SENTENCEs often occur together.Moving well beyond the single-domain setting,our models are trained and evaluated on a multi-genre corpus of approximately 40,000 clausesfrom MASC (Ide et al, 2008) and Wikipediawhich have been annotated with substantial agree-ment.
We train and test our models both withingenres and across genres, highlighting differencesbetween genres and creating models that are ro-bust across genres.
Both the corpus and the codefor an SE type labeler are freely available.1Theseform the basis for future research on SE types andrelated aspectual phenomena and will enable theinclusion of SE type information as a preprocess-ing step into various NLP tasks.2 Linguistic backgroundThe inventory of SE types proposed by Smith(2003) consists of three high-level categories, eachwith two subtypes.
Eventualities comprise EVENTand STATE, categories for clauses representing ac-tual happenings, states of the world, or attributesof entities or situations.
General Statives includeGENERIC SENTENCE and GENERALIZING SEN-TENCE and reflect regularities in the world or gen-eral information predicated over classes or kinds.Finally, Abstract Entities (Figure 2) have the sub-types FACT and PROPOSITION.
Although Ab-stract Entities are part of the label inventory forUT07, we treat them in a separate identificationstep, for reasons discussed in Section 7.
The in-ventory was expanded by Palmer et al (2007) toinclude three additional types: REPORT, QUES-TION and IMPERATIVE.
The latter two categorieswere added to accommodate exhaustive annota-1Corpora, annotation manual and code available atwww.coli.uni-saarland.de/projects/sitentFACT: Objects of knowledge.I know that Mary refused the offer.PROPOSITION: Objects of belief.I believe that Mary refused the offer.Figure 2: Abstract Entity SE types.tion of text; REPORT is a subtype of event for at-tributions of quoted speech.Two parts of a clause provide important in-formation for determining the SE type (Friedrichand Palmer, 2014b): a clause?s main verb andits main referent.
The latter is loosely definedas the main entity that the segment is about; inEnglish this is usually the subject.
For example,main referents of GENERIC SENTENCEs are kindsor classes as in ?Elephants are huge?, while themain referents of Eventualities and GENERALIZ-ING SENTENCEs are particular individuals (?Johnis short?).
For English, the main verb is thenon-auxiliary verb ranked highest in the depen-dency parse (e.g.
?kiss?
in ?John has kissed Joe?
).STATEs and EVENTs differ in the fundamental lex-ical aspectual class (Siegel and McKeown, 2000)of their main verbs (e.g.
dynamic in ?She filledthe glass with water?
vs. stative in ?Water fillsthe glass?).
While fundamental lexical aspectualclass is a word-sense level attribute of the clause?smain verb, habituality is a property of the entireclause which is helpful to determine the clause?sSE type.
For example, EVENT and GENERALIZ-ING SENTENCE differ in habituality (e.g.
episodicin ?John cycled to work yesterday?
vs. habitual in?John cycles to work?
).Like habituality, SE types are a categorizationat the clause level.
Properties of the clause suchas modals, negation, or the perfect influence theSE type: for instance, ?John might win?
is treatedas a STATE as it describes a possible state of theworld rather than an EVENT.
Such coercions hap-pen only for clauses which, without the trigger foraspectual shift, would be EVENTs; other SEs re-tain their type even under coercions such as nega-tion, e.g., ?Elephants are not small?
is a GENERICSENTENCE.
SE types aim to capture how clausesbehave in discourse, and the types STATE andEVENT are aspectual rather than ontological cat-egories.
The types reflect not so much semanticcontent of a clause as its manner of presentation,and all parts of a clause contribute to determiningits SE type.17583 Related workSE types model aspect at the clause level; thus theyare most closely related to other works performingautomatic classification for various aspect-relatedphenomena of the verb or the clause.
For exam-ple, Vendler classes (Vendler, 1957) ascribe fourcategories as lexical properties of verbs, distin-guishing states from three types of events (ac-complishment, achievement, and activity), differ-ing according to temporal and aspectual proper-ties (e.g.
telicity and punctuality).
The work ofSiegel and McKeown (2000) is a major inspirationin computational work on modeling these linguis-tic phenomena, introducing the use of linguisticindicators (see Section 5.1).
Hermes et al (2015)model Vendler classes computationally on a verb-type level for 95 different German verbs, combin-ing distributional vectors with supervised classifi-cation.
Zarcone and Lenci (2008) investigate bothsupervised and unsupervised classification frame-works for occurrences of 28 Italian verbs, andFriedrich and Palmer (2014a) predict lexical as-pectual class for English verbs in context.The only previous approach to automatic clas-sification of SE types comes from Palmer et al(2007).
This system (UT07) uses word and POStag features as well as a number of lexical featuresadopted from theoretical work on aspectual classi-fication.
The model is described in Section 6.1.Another related body of work has to do withdetermining event class as a precursor to tempo-ral relation classification.
The inventory of eventclasses, described in detail in the TimeML anno-tation guidelines (Saur??
et al, 2006), combinessemantic (REPORTING, PERCEPTION), aspectual(ASPECTUAL, STATE, OCCURRENCE), and inten-sional (I ACTION, I STATE) properties of events.Finally, there are close connections to systemswhich predict genericity of noun phrases (Reiterand Frank, 2010; Friedrich and Pinkal, 2015a),and habituality of clauses (Mathew and Katz,2009; Friedrich and Pinkal, 2015b).4 Data setsThe experiments presented in this paper make useof two data sets labeled with SE types.Brown data.
This data set consists of 20 textsfrom the popular lore section of the Brown corpus(Francis and Ku?cera, 1979), manually segmentedinto 4391 clauses and marked by two annotators incorpus tokens SEs Fleiss?
?MASC 357078 30333 0.69Wikipedia 148040 10607 0.66Table 1: SE-labeled corpora: size and agreement.SE type MASC Wiki Fleiss ?
*STATE 49.8 24.3 0.67EVENT 24.3 18.9 0.74REPORT 4.8 0.9 0.80GENERIC 7.3 49.7 0.68GENERALIZING 3.8 2.5 0.43QUESTION 3.3 0.1 0.91IMPERATIVE 3.2 0.2 0.94undecided 2.4 2.1 -Table 2: Distribution of SE types in gold stan-dard (%).
*Krippendorff?s diagnostics.an intuitive way with ?=0.52 (Cohen, 1960).
Fi-nal labels were created via adjudication.
The textsare essays and personal stories with topics rangingfrom maritime stories to marriage advice.MASC and Wikipedia.
Our main data set con-sists of documents from MASC (Ide et al, 2010)and Wikipedia.
The MASC data covers 12 ofthe written genres (see Table 11).
Texts are splitinto clauses using SPADE (Soricut and Marcu,2003) with some heuristic post-processing, andthe clauses are labeled by three annotators inde-pendently.
Annotators, all student assistants withbasic linguistic training, were given an extensiveannotation manual.
Table 1 reports agreementover types in terms of Fleiss?
?
(Fleiss, 1971).
Aswe do not force annotators to assign a label to eachclause, we compute ?
using all pairings of labelswhere both annotators assigned an SE type.
Thegold standard is constructed via majority voting.Table 2 shows the distribution of SE types.The largest proportion of segments in MASC areSTATEs, while the largest proportion in Wikipediaare GENERIC SENTENCEs.
The Wikipedia datawas collected to supplement MASC, which con-tains few generics and no data from an encyclo-pedic genre.
Within MASC, the various genres?distributions of SE types differ as well, and agree-ment scores also vary: some genres contain manyinstances of easily classifiable SE types, while oth-ers (e.g., essays or journal) are more difficult toannotate (more details in Section 6.6).The rightmost column of Table 2 shows the val-ues for Krippendorff?s diagnostics (Krippendorff,1980), a tool for determining which categories hu-1759group explanation examplesmv features describing the SE?s mainverb & its argumentstense, lemma, lemma of object, auxiliary, WordNet sense and hypernymsense, progressive, POS, perfect, particle, voice, linguistic indicatorsmr features describing the main refer-ent, i.e., the NP denoting the mainverb?s subjectlemma, determiner type, noun type, number, WordNet sense and super-sense, dependency relations linked to this token, person, countability, barepluralcl features describing entire clause thatinvokes the SEpresence of adverbs / prepositional clauses, conditional, modal, whethersubject before verb, negated, verbs embedding the clauseTable 3: Overview of feature set B.
The full and detailed list is available (together with the implementa-tion) at http://www.coli.uni-saarland.de/projects/sitent.mans had most difficulties with.
For each cate-gory, one computes ?
for an artificial set-up inwhich all categories except one are collapsed intoan artificial OTHER category.
A high value indi-cates that annotators can distinguish this SE typewell from others.
GENERALIZING SENTENCEsare most difficult to agree upon.
For all frequentlyoccurring types as well as QUESTIONs and IM-PERATIVEs, agreement is substantial.Agreement on QUESTION and IMPERATIVEs isnot perfect even for humans, as identifying themrequires recognizing cases in reported speech,which is not a trivial task (e.g., Brunner, 2013).To illustrate another difficult case, consider the ex-ample ?You must never confuse faith?, which wasmarked as both IMPERATIVE and GENERIC SEN-TENCE, by different annotators.5 MethodThis section describes the feature sets and classifi-cation methods used in our approach, which mod-els SE type labeling as a supervised sequence la-beling task.5.1 Feature setsOur feature sets are designed to work well on largedata sets, across genres and domains.
Features aregrouped into two sets: A consists of standard NLPfeatures including POS tags and Brown clusters.Set B targets SE labeling, focusing on syntactic-semantic properties of the main verb and main ref-erent, as well as properties of the clause which in-dicate its aspectual nature.
Texts are pre-processedwith Stanford CoreNLP (Manning et al, 2014),including tokenization, POS tagging (Toutanovaet al, 2003) and dependency parsing (Klein andManning, 2002) using the UIMA-based DKProframework (Ferrucci and Lally, 2004; Eckart deCastilho and Gurevych, 2014).A-pos: part-of-speech tags.
These featurescount how often each POS tag occurs in a clause.A-bc: Brown cluster features.
UT07 reliesmostly on words and word/POS tag pairs.
Thesesimple features work well on the small Brown dataset, but the approach quickly becomes impracti-cal with increasing corpus size.
We instead turnto distributional information in the form of Brownclusters (Brown et al, 1992), which can be learnedfrom raw text and represent word classes in a hi-erarchical way.
Originally developed in the con-text of n-gram language modeling, they aim to as-sign words to classes such that the average mutualinformation of the words in the clusters is maxi-mized.
We use existing, freely-available clusterstrained on news data by Turian et al (2010) usingthe implementation by Liang (2005).2Clusteringswith 320 and 1000 Brown clusters work best forour task.
We use one feature per cluster, countinghow often a word in the clause was assigned to thiscluster (0 for most clusters).B-mv: main verb.
Using dependency parses,we extract the verb ranked highest in the clause?sparse as the main verb, and extract the set of fea-tures listed in Table 3 for that token.
Featuresbased on WordNet (Fellbaum, 1998) use the mostfrequent sense of the lemma.
Tense and voiceinformation is extracted from sequences of POStags using a set of rules (Loaiciga et al, 2014).Linguistic indicators (Siegel and McKeown, 2000)are features collected per verb type over a largeparsed background corpus, encoding how often averb type occurred with each linguistic marker,e.g., in past tense or with an in-PP.
We use val-ues collected from Gigaword (Graff et al, 2003);these are freely available at our project web site(Friedrich and Palmer, 2014a).B-mr: main referent.
We extract the gram-matical subject of the main verb (i.e., nsubj ornsubjpass) as the clause?s main referent.
Whilethe main verb must occur within the clause, the2http://metaoptimize.com/projects/wordreprs1760main referent may be a token either within or out-side the clause.
In the latter case, it still functionsas the clause?s main referent, as in most cases itcan be considered an implicit argument within theclause.
Table 3 lists the features extracted for themain referent.B-cl: clause.
These features (see also Table 3)describe properties at the clause level, capturingboth grammatical phenomena such as word orderand lexical phenomena including presence of par-ticular adverbials or prepositional phrases, as wellas semantic information such as modality.
If theclause?s main verb is embedded in a ccomp rela-tion, we also use features describing the respectivegoverning verb.5.2 Classification / sequence labeling modelOur core modeling assumption is to view a doc-ument as a sequence of SE type labels, each as-sociated with a clause; this motivates the choiceof using a conditional random field (CRF, Laffertyet al (2001)) for label prediction.
The conditionalprobability of label sequence ~y given an observa-tion sequence ~x is given by:P (~y|~x) =1Z(~x)exp(n?j=1m?i=1?ifi(yj?1, yj, ~x, j)),with Z(~x) being a normalization constant (seealso Klinger and Tomanek (2007)).
?i, the weightsof the feature functions, are learned via L-BGFS(Wright and Nocedal, 1999).We create a linear chain CRF using theCRF++ toolkit3with default parameters, apply-ing two forms of feature functions: fi(yj, xj) andfi(yj?1, yj).
The former consists of indicatorfunctions for combinations of SE type labels andeach of the features listed above.
The latter type offeature function (also called ?bigram?
features inCRF++ terminology) gets instantiated as indicatorfunctions for each combination of labels, therebyenabling the model to take sequence informationinto account.
When using only the former type offeature function, our classifier is equivalent to amaximum entropy (MaxEnt) model.Side remark: pipeline approach.
Feature set Bis inspired by previous work on two subtasks ofassigning an SE type to a clause (see Section 3):(a) identifying the genericity of a noun phrase inits clausal context, and (b) identifying whether aclause is episodic, habitual or static.
This informa-3https://code.google.com/p/crfpption can in turn be used to determine a clause?s SEtype label in a rule-based way, e.g., GENERALIZ-ING SENTENCEs are habitual clauses with a non-generic main referent.
As our corpus is also anno-tated with this information, we also trained sepa-rate models for these subtasks and assigned the SEtype label accordingly.
However, such a pipelineapproach is not competitive with the model traineddirectly on SE types (see Section 6.3).6 ExperimentsHere we present our experiments on SE type clas-sification, beginning with a (near) replication ofthe UT07 system, and moving on to evaluate ournew approach from multiple perspectives.6.1 Replication and extension of UT07As a first step, we implement a system similar toUT07, which relies on the features summarized inTable 4.
For W and T features, we set a frequencythreshold of 7 occurrences.
Feature set L com-prises sets of predicates assumed to correlate withparticular SE types, and whether or not the clausecontains a modal or finite verb.
Set G includesall verbs of the clause and their POS-tags.
UT07additionally uses CCG supertags and grammaticalfunction information.
The UT07 system approx-imates a sequence labeling model by adding thepredicted labels of previous clauses as lookback(LB) features.
To parallel their experiments, wetrain both MaxEnt and CRF models, as explainedin Section 5.2.
Results on the Brown data, with thesame training/test split, appear in Table 5.
Unlikethe LB model, our CRF predicts the label sequencejointly and outperforms UT07 on the Brown databy up to 7% accuracy.
We assume that the per-formance boost in the MaxEnt setting is at leastpartially due to having better parses.In sum, on the small Brown data set, a CRF ap-proach successfully leverages sequence informa-tion, and a simple set of features works well.
Pre-liminary experiments applying our new featureson Brown data yield no improvements, suggestingthat word-based features overfit this domain.W wordsT POS tags, word/POS tag combinationsL linguistic cuesG grammatical cuesTable 4: Features used in baseline UT07.1761Palmer et al (2007) our implementationfeatures MaxEnt LB MaxEnt CRFW 45.4 46.6 48.8 47.0WT 49.9 52.4 52.9 53.7WTL 48.9 50.5 51.6 55.8WTLG 50.6 53.1 55.8 60.0Table 5: Accuracy on Brown.
Test set majorityclass (STATE) is 35.3%.
LB = results for best look-back settings in MaxEnt.
787 test instances.6.2 Experimental settingsWe develop our models using 10-fold cross val-idation (CV) on 80% (counted in terms of thenumber of SEs) of the MASC and Wikipedia data(a total of 32,855 annotated SEs), keeping the re-maining 20% as a held-out test set.
Developmentand test sets each contain distinct sets of docu-ments; the documents of each MASC genre andof Wikipedia are distributed over the folds.
We re-port results in terms of macro-average precision,macro-average recall, macro-average F1-measure(harmonic mean of macro-average precision andmacro-average recall), and accuracy.
We applyMcNemar?s test (McNemar, 1947) with p < 0.01to test significance of differences in accuracy.
Inthe following tables, we mark numerically-closescores with the same symbols if they are found tobe significantly different.Upper bound: human performance.
Labelingclauses with their SE types is a non-trivial taskeven for humans, as there are many borderlinecases (see Sections 4 and 8).
We compute an upperbound for system performance by iterating over allclauses: for each pair of human annotators, twoentries are added to a co-occurrence matrix (simi-lar to a confusion matrix), with each label servingonce as ?gold standard?
and once as the ?predic-tion.?
From this matrix, we can compute scores inthe same manner as for system predictions.
Preci-sion and recall scores are symmetric in this case,and accuracy corresponds to observed agreement.6.3 Impact of feature setsWe now compare various configurations of ourCRF-based SE type labeler, experimenting withthe feature sets as described in Section 5.1.
Ta-ble 6 shows the results for 10-fold CV on the devportion of the MASC+Wiki corpus.Each feature set on its own outperforms themajority class baseline.
Of the individual fea-ture groups, bc and mv have the highest predic-feature set P R F acc.maj.
class (STATE) 6.4 14.3 8.8 45.0A 70.1 61.4 65.4 ?
?72.1pos 49.3 40.3 44.3 58.7bc 67.5 55.8 61.1 ?70.6B 69.5 62.7 66.9 ?
?72.8mr 36.4 26.8 30.9 51.7mv 62.3 52.4 56.9 ?70.8cl 53.3 41.2 46.6 52.8A+B 74.1 68.6 71.2 ?
?76.4upper bound (humans) 78.6 78.6 78.6 79.6Table 6: Impact of different feature sets.Wiki+MASC dev set, CRF, 10-fold CV.feature set P R F acc.maj.
class (STATE) 6.4 14.3 8.8 44.7A 67.6 60.6 63.9 ?69.8B 69.9 61.7 65.5 ?71.4A+B 73.4 65.5 69.3 ?
?74.7Table 7: Results on MASC+Wiki held-out test set(7937 test instances).tive power; both capture lexical information of themain verb.
Using sets A and B individually re-sults in similar scores; their combination increasesaccuracy on the dev set by an absolute 3.6-4.3%.Within A and B, each subgroup contributes to theincrease in performance (not shown in table).Finally, having developed exclusively on thedev set, we run the system on the held-out test set,training on the entire dev set.
Results (in Table 7)show the same tendencies as for the dev set: eachfeature set contributes to the final score, and thesyntactic-semantic features targeted at classifyingSE types (i.e.
B) are helpful.Ablation.
To gain further insight, we ablate eachfeature subgroup from the full system, see Ta-ble 8.
Again, bc features and mv features areidentified as the most important ones.
The otherfeature groups partially carry redundant informa-tion when combining A and B.
Next, we rank fea-tures by their information gain with respect to theSE types.
In Table 3, the features of each groupare ordered by this analysis.
Ablating single fea-tures from the full system does not result in signif-icant performance losses.
However, using only se-lected, top features for our system decreased per-formance, possibly because some features coverrare but important cases, and because the featureselection algorithm does not take into account theinformation features may provide regarding tran-1762feature set P R F acc.A+B 74.1 68.6 71.2 76.4- bc 71.3 65.7 68.4 74.5- pos 73.4 67.4 70.2 76.0- mr 73.7 67.4 70.4 75.9- mv 72.3 64.2 68.0 73.6- cl 73.1 67.6 70.2 76.0Table 8: Impact of feature groups: ablationWiki+MASC dev set, CRF, 10-fold CV.
All accu-racies for ablation settings are significantly differ-ent from A+B.sitions (Klinger and Friedrich, 2009).
In addition,CRFs are known to be able to deal with a largenumber of potentially dependent features.Side remark: pipeline approach.
We here usethe subset of SEs labeled as EVENT, STATE,GENERIC SENTENCE or GENERALIZING SEN-TENCE because noun phrase genericity and habit-uality is not labeled for IMPERATIVE and QUES-TION, and REPORT is identified lexically based onthe main verb rather than these semantic features.Models for subtasks of SE type identification, i.e.,(a) genericity of noun phrases and (b) habitual-ity reach accuracies of (a) 86.8% and (b) 83.6%(on the relevant subset).
Applying the labels out-put by these two systems as (the only) features ina rule-based way using a J48 decision tree (Wit-ten et al, 1999) results in an accuracy of 75.5%,which is lower than 77.2%, the accuracy of theCRF which directly models SE types (when usingonly the above four types).6.4 Impact of amount of training dataNext we test how much training data is required toget stable results for SE type classification.
Fig-ure 3 shows accuracy and F1 for 10-fold CV us-ing A+B, with training data downsampled to dif-ferent extents in each run by randomly removingdocuments.
Up to the setting which uses about60% of the training data, performance increasessteadily.
Afterwards, the curves start to level off.We conclude that robust models can be learnedfrom our corpus.
Adding training data, especiallyin-domain data, will, as always, be beneficial.6.5 Impact of sequence labeling approachPalmer et al (2007) suggest that SE types ofnearby clauses are a useful source of information.We further test this hypothesis by comparing oursequence labeling model (CRF) to two additional0 20 40 60 80 1004050607080llllllll l ll l l l l l4050607080lAccuracyF1% training dataFigure 3: Learning curve for MASC+Wiki dev.models: (1) a MaxEnt model, which labels clausesin isolation, and (2) a MaxEnt model including thecorrect label of the preceding clause (seq-oracle),simulating an upper bound for the impact of se-quence information on our system.Table 9 shows the results.
Scores for GENER-ALIZING SENTENCE are the lowest as this class isvery infrequent in the data set.
The most strik-ing improvement of the two sequence labelingsettings over MaxEnt concerns the identificationof GENERIC SENTENCEs.
These often ?cluster?in texts (Friedrich and Pinkal, 2015b) and hencetheir identification profits from using sequence in-formation.
The results for seq-oracle show thatthe sequence information is useful for STATE,GENERIC and GENERALIZING SENTENCEs, butthat no further improvement is to be expected fromthis method for the other SE types.
We concludethat the CRF model is to be preferred over theMaxEnt model; in almost all of our experimentsit performs significantly better or equally well.SE type MaxEnt CRF seq-oracleSTATE 79.1 80.6 81.7EVENT 77.5 78.6 78.3REPORT 78.2 78.9 78.3GENERIC 61.3 68.3 73.5GENERALIZING 25.0 29.4 38.1IMPERATIVE 72.3 75.3 74.7QUESTION 84.4 84.4 83.8macro-avg P 71.5 74.1 75.5macro-avg R 66.1 68.6 70.4macro-avg F1 68.7 71.2 73.9accuracy ?74.1 ?
?76.4 ?77.9Table 9: Impact of sequence information: (F1by SE type): CRF, Masc+Wiki, 10-fold CV.6.6 Impact of genreIn this section, we test to what extent our mod-els are robust across genres.
Table 10 shows F1-scores for each SE type for two settings: the 10-1763SE type genre-CV 10-fold CV humansSTATE 78.2 80.6 82.8EVENT 77.0 78.6 80.5REPORT 76.8 78.9 81.5GENERIC 44.8 68.3 75.1GENERALIZING 27.4 29.4 45.8IMPERATIVE 70.8 75.3 93.6QUESTION 81.8 84.4 90.7macro-avg F1 66.6 71.2 78.6accuracy ?71.8 ?76.4 79.6Table 10: Impact of in-genre training data.
F1-score by SE type, CRF, MASC+Wiki dev.fold CV setting as explained in section 6.2, and agenre-CV setting, simulating the case where noin-genre training data is available, treating eachgenre as one cross validation fold.
As expected, inthe latter setting, both overall accuracy and macro-average F1 are lower compared to the case whenin-genre training data is available.
Nevertheless,our model is able to capture the nature of SE typesacross genres: the prediction of STATE, EVENT,REPORT and QUESTION is relatively stable evenin the case of not having in-genre training data.An EVENT seems to be easily identifiable regard-less of the genre.
GENERIC SENTENCE is a prob-lematic case; in the genre-CV setting, its F1-scoredrops by 23.5%.
The main reason for this is thatthe distribution of SE types in Wikipedia differscompletely from the other genres (see section 4).Precision for GENERIC SENTENCE is at 70.5%in the genre-CV setting, but recall is only 32.8%(compared to 70.1% and 66.6% in the 10-fold CVsetting).
Genericity seems to work differently inthe various genres: most generics in Wikipediaclearly refer to kinds (e.g., lions or plants), whilemany generics in essays or letters are instances ofmore abstract concepts or generic you.Results by genre.
Next, we drill down in theevaluation of our system by separately inspect-ing results for individual genres.
Table 11 showsthat performance differs greatly depending on thegenre.
In some genres, the nature of SE typesseems clearer to our annotators than in others, andthis is reflected in the system?s performance.
Themajority class is GENERIC SENTENCE in wiki,and STATE in all other genres.
In the ?same genre?setting, 10-fold CV was performed within eachgenre.
Adding out-of-genre training data improvesmacro-average F1 especially for genres with lowscores in the ?same genre?
setting.
This boost istraining datamaj.
sameclass genre all humansgenre % F1 F1 F1 ?blog 57.6 57.3 64.9 72.9 0.62email 68.6 63.6 66.4 67.0 0.65essays 49.4 33.5 62.1 64.6 0.54ficlets 44.7 60.2 65.7 81.7 0.80fiction 45.8 63.0 66.0 76.7 0.77govt-docs 60.9 26.6 67.6 72.6 0.57jokes 34.9 66.2 69.8 82.0 0.77journal 59.3 35.8 59.8 63.7 0.52letters 57.3 51.9 65.1 68.0 0.66news 52.2 54.6 64.1 78.6 0.75technical 57.7 31.4 59.4 54.7 0.55travel 25.9 39.9 58.1 48.9 0.59wiki 51.6 53.1 63.0 69.2 0.66Table 11: Macro-avg.
F1 by genre, CRF, 10-foldCV.
Majority class given in % of clauses.due to adding training data for types that are infre-quent in that genre.
Accuracy (not shown in table)improves significantly for blog, essays, govt-docs,jokes, and journal, and does not change for the re-maining genres.
We conclude that it is extremelybeneficial to use our full corpus for training, as ro-bustness of the system is increased, especially forSE types occurring infrequently in some genres.7 Identification of Abstract EntitiesOur system notably does not address one ofSmith?s main SE categories: Abstract Entities, in-troduced in Section 2.
These SEs are expressedas clausal arguments of certain predicates such as(canonically) know or believe.
Note that the Ab-stract Entity subtypes FACT and PROPOSITION donot imply that a clause?s propositional content istrue or likely from an objective point of view, theyrather indicate that the clause?s content is intro-duced to the discourse as an object of knowledgeor belief, respectively.
Following Smith (2003),we use PROPOSITION in a different sense thanthe usual meaning of ?proposition?
in semantics -naturally situation entities of any type may havepropositional content.
Smith?s use of the term(and thus ours too) contrasts PROPOSITION withFACT - our PROPOSITIONs are simply sentencespresented as a belief (or with uncertain evidentialstatus) of the writer or speaker.
This use of ?propo-sition?
also occurs in linguistic work by Peterson(1997) on factive vs. propositional predicates.During the creation of the corpus, annotators1764were asked to give one label out of the SE typesincluded in our classification task, and to markthe clause with one of the Abstract Entity sub-types in addition if applicable.
Analysis of thedata shows that our annotators frequently for-got to mark clauses as Abstract Entities, whichmakes it difficult to model these categories cor-rectly.
As a first step toward resolving this issue,we implement a filter which automatically identi-fies Abstract Entities by looking for clausal com-plements of certain predicates.
The list of pred-icates is compiled using WordNet synonyms ofknow, think, and believe, as well as predicatesextracted from FactBank (Sauri and Pustejovsky,2009) and TruthTeller (Lotan et al, 2013).
Manyof the clauses automatically identified as AbstractEntities are cases that annotators missed duringannotation.
We thus performed a post-hoc evalua-tion, presenting these clauses in context to annota-tors and asking whether the clause is an AbstractEntity.
The so-estimated precision of our filter is85.8% (averaged over 3 annotators).
Agreementfor this annotation task is ?
= 0.54, with an ob-served agreement of 88.7%.
Our filter finds 80%of the clauses labeled as Abstract Entity by at leastone annotator in the gold standard; this is approx-imately its recall.8 ConclusionWe have presented a system for automatically la-beling clauses with their SE type which is mostlyrobust to changes in genre and which reaches ac-curacies of up to 76%, comparing favorably to thehuman upper bound of 80%.
The system benefitsfrom capturing contexual effects by using a linearchain CRF with label bigram features.
In addition,the distributional and targeted syntactic-semanticfeatures we introduce enable SE type predictionfor large and diverse data sets.
Our publicly avail-able system can readily be applied to any writtenEnglish text, making it easy to explore the utilityof SE types for other NLP tasks.Discussion.
Our annotation scheme and guide-lines for SE types (Friedrich and Palmer, 2014b)follow established traditions in linguistics and se-mantic theory.
When applying these to a largenumber of natural texts, though, we came acrossa number of borderline cases where it is not easyto select just one SE type label.
As we have re-ported before (Friedrich et al, 2015), the most dif-ficult case is the identification of GENERIC SEN-TENCEs, which are defined as making a statementabout a kind or class.
We find that making thistask becomes particularly difficult for argumenta-tive essays (Becker et al, to appear).Future work.
A next major step in our researchagenda is to integrate SE type information intovarious applications, including argument mining,temporal reasoning, and summarization.
Togetherwith the mode of progression through the text,e.g., temporal or spatial, SE type distribution is akey factor for a reader or listener?s intuitive recog-nition of the discourse mode of a text passage.Therefore the automatic labeling of clauses withtheir SE type is a prerequisite for automaticallyidentifying a passage?s discourse mode, which inturn has promising applications in many areas ofNLP, as the mode of a text passage has implica-tions for the linguistic phenomena to be found inthe passage.
Examples include temporal process-ing of text (Smith, 2008), summarization, or ma-chine translation (for work on genres see van derWees et al, 2015).
Here we focus on the automaticidentification of SE types, leaving the identifica-tion of discourse modes to future work.The present work, using the SE type inventoryintroduced by Smith (2003), is also the basis forresearch on more fine-grained aspectual type in-ventories.
Among others, we plan to create sub-types of the STATE label, which currently sub-sumes clauses stativized by negation, modals, lex-ical information or other aspectual operators.
Dis-tinguishing these is relevant for temporal relationprocessing or veridicality recognition.AcknowledgmentsWe thank the anonymous reviewers and AndreaHorbach for their helpful comments related to thiswork, and our annotators Melissa Peate S?rensen,Christine Bocionek, Kleo-Isidora Mavridou, Fer-nando Ardente, Damyana Gateva, Ruth K?uhn andAmbika Kirkland.
This research was supportedin part by the MMCI Cluster of Excellence of theDFG, and the first author is supported by an IBMPhD Fellowship.
The second author is fundedby the Leibniz ScienceCampus Empirical Linguis-tics and Computational Language Modeling, sup-ported by Leibniz Association grant no.
SAS-2015-IDS-LWC and by the Ministry of Science,Research, and Art of Baden-W?urttemberg.1765ReferencesEmmon Bach.
1986.
The algebra of events.
Lin-guistics and philosophy 9(1):5?16.Maria Becker, Alexis Palmer, and Anette Frank.
toappear.
Argumentative texts and clause types.In Proceedings of the 3rd Workshop on Argu-ment Mining, ACL 2016.Peter F Brown, Peter V Desouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.Class-based n-gram models of natural language.Computational Linguistics 18(4):467?479.Annelen Brunner.
2013.
Automatic recognition ofspeech, thought, and writing representation ingerman narrative texts.
Literary and linguisticcomputing 28(4):563?575.Jacob Cohen.
1960.
A coefficient of agreement fornominal scales.
Educational and PsychologicalMeasurement 20(1):37?46.Francisco Costa and Ant?onio Branco.
2012.
As-pectual type and temporal relation classifica-tion.
In Proceedings of the 13th Conferenceof the European Chapter of the Association forComputational Linguistics (EACL).Richard Eckart de Castilho and Iryna Gurevych.2014.
A broad-coverage collection of portableNLP components for building shareable analy-sis pipelines.
In Proceedings of the Workshopon Open Infrastructures and Analysis Frame-works for HLT .
Dublin, Ireland.Christiane Fellbaum.
1998.
WordNet.
Wiley On-line Library.David Ferrucci and Adam Lally.
2004.
UIMA: anarchitectural approach to unstructured informa-tion processing in the corporate research envi-ronment.
Natural Language Engineering 10(3-4):327?348.Joseph L Fleiss.
1971.
Measuring nominal scaleagreement among many raters.
Psychologicalbulletin 76(5):378.W.
Nelson Francis and Henry Ku?cera.
1979.
AStandard Corpus of Present-Day Edited Amer-ican English, for use with Digital Computers.Brown University.Annemarie Friedrich and Alexis Palmer.
2014a.Automatic prediction of aspectual class of verbsin context.
In Proceedings of the 52nd AnnualMeeting of the Association for ComputationalLinguistics (ACL).
Baltimore, USA.Annemarie Friedrich and Alexis Palmer.
2014b.Situation entity annotation.
In Proceedings ofthe Linguistic Annotation Workshop VIII .Annemarie Friedrich, Alexis Palmer,Melissa Peate Srensen, and Manfred Pinkal.2015.
Annotating genericity: a survey, ascheme, and a corpus.
In Proceedings of the9th Linguistic Annotation Workshop (LAW IX).Denver, Colorado, US.Annemarie Friedrich and Manfred Pinkal.
2015a.Automatic recognition of habituals: a three-wayclassification of clausal aspect.
In Proceedingsof Conference on Empirical Methods in NaturalLanguage Processing (EMNLP).
Lisbon, Portu-gal.Annemarie Friedrich and Manfred Pinkal.
2015b.Discourse-sensitive Automatic Identification ofGeneric Expressions.
In Proceedings ofthe 53rd Annual Meeting of the Associationfor Computational Linguistics (ACL).
Beijing,China.David Graff, Junbo Kong, Ke Chen, and KazuakiMaeda.
2003.
English gigaword.
LinguisticData Consortium, Philadelphia .J?urgen Hermes, Michael Richter, and ClaesNeuefind.
2015.
Automatic induction of Ger-man aspectual verb classes in a distributionalframework.
In Proceedings of the InternationalConference of the German Society for Compu-tational Linguistics and Language Technology(GSCL).Nancy Ide, Collin Baker, Christiane Fellbaum, andCharles Fillmore.
2008.
MASC: The manuallyannotated sub-corpus of American English .Nancy Ide, Christiane Fellbaum, Collin Baker, andRebecca Passonneau.
2010.
The manually an-notated sub-corpus: A community resource forand by the people.
In Proceedings of the ACL2010 conference short papers.Dan Klein and Christopher D Manning.
2002.
Fastexact inference with a factored model for nat-ural language parsing.
In Advances in NeuralInformation Processing Systems.Roman Klinger and Christoph M Friedrich.
2009.Feature subset selection in conditional randomfields for named entity recognition.
In Proceed-ings of Recent Advances in Natural LanguageProcessing (RANLP).1766Roman Klinger and Katrin Tomanek.
2007.
Clas-sical probabilistic models and conditional ran-dom fields.
TU Dortmund Algorithm Engineer-ing Report .Manfred Krifka, Francis Jeffrey Pelletier, Gre-gory N. Carlson, Alice ter Meulen, GodehardLink, and Gennaro Chierchia.
1995.
Generic-ity: An Introduction.
The Generic Book pages1?124.Klaus Krippendorff.
1980.
Content analysis: Anintroduction to its methodology.
Sage.John D. Lafferty, Andrew McCallum, and Fer-nando C. N. Pereira.
2001.
Conditional Ran-dom Fields: Probabilistic Models for Segment-ing and Labeling Sequence Data.
In Proceed-ings of the Eighteenth International Conferenceon Machine Learning (ICML).Percy Liang.
2005.
Semi-supervised learning fornatural language.
Ph.D. thesis, University ofCalifornia Berkeley.Sharid Loaiciga, Thomas Meyer, and AndreiPopescu-Belis.
2014.
English-French VerbPhrase Alignment in Europarl for Tense Trans-lation Modeling.
In The Ninth Language Re-sources and Evaluation Conference (LREC).Amnon Lotan, Asher Stern, and Ido Dagan.
2013.TruthTeller: Annotating Predicate Truth.
InProceedings of NAACL 2013.Christopher D. Manning, Mihai Surdeanu, JohnBauer, Jenny Finkel, Steven J. Bethard, andDavid McClosky.
2014.
The Stanford CoreNLPNatural Language Processing Toolkit.
In As-sociation for Computational Linguistics (ACL)System Demonstrations.Thomas A. Mathew and Graham E. Katz.
2009.Supervised categorization for habitual versusepisodic sentences.
In Sixth Midwest Computa-tional Lingustics Colloquium.
Indiana Univer-sity, Bloomington, Indiana.Quinn McNemar.
1947.
Note on the sampling er-ror of the difference between correlated propor-tions or percentages.
Psychometrika 12(2):153?157.Alexis Palmer, Elias Ponvert, Jason Baldridge, andCarlota Smith.
2007.
A sequencing model forsituation entity classification.
In Proceedings ofthe 45th Annual Meeting of the Association forComputational Linguistics.Philip L Peterson.
1997.
On representing eventreference.
In Fact Proposition Event, Springer,pages 65?90.Nils Reiter and Anette Frank.
2010.
Identify-ing Generic Noun Phrases.
In Proceedings ofthe 48th Annual Meeting of the Association forComputational Linguistics.
Uppsala, Sweden.Roser Saur?
?, Jessica Littman, Bob Knippen, RoberGaizauskas, Andrea Setzer, and James Puste-jovsky.
2006.
TimeML Annotation Guidelines,Version 1.2.1.
Technical report.Roser Sauri and James Pustejovsky.
2009.
Fact-bank 1.0 ldc2009t23.
Web Download.
Philadel-phia: Linguistic Data Consortium.Eric V Siegel and Kathleen R McKeown.
2000.Learning methods to combine linguistic indica-tors: Improving aspectual classification and re-vealing linguistic insights.
Computational Lin-guistics 26(4):595?628.Carlota S Smith.
2003.
Modes of discourse: Thelocal structure of texts, volume 103.
CambridgeUniversity Press.Carlota S Smith.
2005.
Aspectual entities andtense in discourse.
In Aspectual inquiries,Springer, pages 223?237.Carlota S Smith.
2008.
Time with and withouttense.
In Time and modality, Springer, pages227?249.Radu Soricut and Daniel Marcu.
2003.
Sentencelevel discourse parsing using syntactic and lex-ical information.
In Proceedings of the 2003Conference of the North American Chapter ofthe Association for Computational Linguisticson Human Language Technology.Kristina Toutanova, Dan Klein, Christopher DManning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic depen-dency network.
In Proceedings of the 2003Conference of the North American Chapter ofthe Association for Computational Linguisticson Human Language Technology.Joseph Turian, Lev Ratinov, and Yoshua Bengio.2010.
Word representations: a simple and gen-eral method for semi-supervised learning.
InProceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics.Marlies van der Wees, Arianna Bisazza, WouterWeerkamp, and Christof Monz.
2015.
What?s1767in a Domain?
Analyzing Genre and Topic dif-ferences in Statistical Machine Translation.
InProceedings of the 53rd Meeting of the Associ-ation for Computational Linguistics (ACL).Benjamin D Van Durme.
2010.
Extracting implicitknowledge from text.
Ph.D. thesis, University ofRochester.Zeno Vendler.
1957.
Verbs and times.
The philo-sophical review pages 143?160.Ian H Witten, Eibe Frank, Len Trigg, Mark Hall,Geoffrey Holmes, and Sally Jo Cunningham.1999.
Weka: Practical machine learning toolsand techniques with java implementations .Stephen Wright and Jorge Nocedal.
1999.
Numer-ical optimization.
Springer Science 35:67?68.Alessandra Zarcone and Alessandro Lenci.
2008.Computational models of event type classifica-tion in context.
In Proceedings of LREC2008.1768
