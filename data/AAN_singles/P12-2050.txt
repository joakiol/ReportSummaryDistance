Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 253?258,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsCoarse Lexical Semantic Annotation with Supersenses:An Arabic Case StudyNathan Schneider?
Behrang Mohit?
Kemal Oflazer?
Noah A. Smith?School of Computer Science, Carnegie Mellon University?Doha, Qatar ?Pittsburgh, PA 15213, USA{nschneid@cs.,behrang@,ko@cs.,nasmith@cs.}cmu.eduAbstract?Lightweight?
semantic annotation of textcalls for a simple representation, ideally with-out requiring a semantic lexicon to achievegood coverage in the language and domain.In this paper, we repurpose WordNet?s super-sense tags for annotation, developing specificguidelines for nominal expressions and ap-plying them to Arabic Wikipedia articles infour topical domains.
The resulting corpushas high coverage and was completed quicklywith reasonable inter-annotator agreement.1 IntroductionThe goal of ?lightweight?
semantic annotation oftext, particularly in scenarios with limited resourcesand expertise, presents several requirements for arepresentation: simplicity; adaptability to new lan-guages, topics, and genres; and coverage.
Thispaper describes coarse lexical semantic annotationof Arabic Wikipedia articles subject to these con-straints.
Traditional lexical semantic representationsare either narrow in scope, like named entities,1 ormake reference to a full-fledged lexicon/ontology,which may insufficiently cover the language/domainof interest or require prohibitive expertise and ef-fort to apply.2 We therefore turn to supersense tags(SSTs), 40 coarse lexical semantic classes (25 fornouns, 15 for verbs) originating in WordNet.
Previ-ously these served as groupings of English lexicon1Some ontologies like those in Sekine et al (2002) and BBNIdentifinder (Bikel et al, 1999) include a large selection ofclasses, which tend to be especially relevant to proper names.2E.g., a WordNet (Fellbaum, 1998) sense annotation effortreported by Passonneau et al (2010) found considerable inter-annotator variability for some lexemes; FrameNet (Baker etal., 1998) is limited in coverage, even for English; and Prop-Bank (Kingsbury and Palmer, 2002) does not capture semanticrelationships across lexemes.
We note that the Omega ontol-ogy (Philpot et al, 2003) has been used for fine-grained cross-lingual annotation (Hovy et al, 2006; Dorr et al, 2010).Q.J?KconsidersH.
AJ?book?JJk.Guinness?AP?C?for-records?J?AJ??
@the-standardCOMMUNICATION?@that???Ag.university?@?Q??
@Al-KaraouineARTIFACT??in?A?FezH.
Q???
@MoroccoLOCATION?Y?@oldest???Ag.universityGROUP??in??A??
@the-worldLOCATIONIJkwhere??'wasA?D?J?AKestablishedACT?
?in?J?year859 ?XCJ?ADTIME.
?The Guinness Book of World Records considers theUniversity of Al-Karaouine in Fez, Morocco, establishedin the year 859 AD, the oldest university in the world.
?Figure 1: A sentence from the article ?Islamic GoldenAge,?
with the supersense tagging from one of two anno-tators.
The Arabic is shown left-to-right.entries, but here we have repurposed them as targetlabels for direct human annotation.Part of the earliest versions of WordNet, thesupersense categories (originally, ?lexicographerclasses?)
were intended to partition all English nounand verb senses into broad groupings, or semanticfields (Miller, 1990; Fellbaum, 1990).
More re-cently, the task of automatic supersense tagging hasemerged for English (Ciaramita and Johnson, 2003;Curran, 2005; Ciaramita and Altun, 2006; Paa?
andReichartz, 2009), as well as for Italian (Picca et al,2008; Picca et al, 2009; Attardi et al, 2010) andChinese (Qiu et al, 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be-lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail-ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap-plied to an Arabic sentence in figure 1.SSTs both refine and relate lexical items: theycapture lexical polysemy on the one hand?e.g.,3Note that work in supersense tagging used text with fine-grained sense annotations that were then coarsened to SSTs.4The noun/verb distinction might prove problematic in somelanguages.253Crusades ?Damascus ?
Ibn Tolun Mosque ?
Imam Hussein Shrine ?
Islamic Golden Age ?
Islamic History ?Ummayad Mosque 434s 16,185t 5,859mAtom ?
Enrico Fermi ?
Light ?
Nuclear power ?
Periodic Table ?
Physics ?
Muhammad al-Razi 777s 18,559t 6,477m2004 Summer Olympics ?Christiano Ronaldo ?Football ?FIFA World Cup ?Portugal football team ?Rau?l Gonza?les ?Real Madrid 390s 13,716t 5,149mComputer ?
Computer Software ?
Internet ?
Linux ?
Richard Stallman ?
Solaris ?
X Window System 618s 16,992t 5,754mTable 1: Snapshot of the supersense-annotated data.
The 7 article titles (translated) in each domain, with total countsof sentences, tokens, and supersense mentions.
Overall, there are 2,219 sentences with 65,452 tokens and 23,239mentions (1.3 tokens/mention on average).
Counts exclude sentences marked as problematic and mentions marked ?.disambiguating PERSON vs.
POSSESSION for thenoun principal?and generalize across lexemes onthe other?e.g., principal, teacher, and student canall be PERSONs.
This lumping property might beexpected to give too much latitude to annotators; yetwe find that in practice, it is possible to elicit reason-able inter-annotator agreement, even for a languageother than English.
We encapsulate our interpreta-tion of the tags in a set of brief guidelines that aimsto be usable by anyone who can read and understanda text in the target language; our annotators had noprior expertise in linguistics or linguistic annotation.Finally, we note that ad hoc categorizationschemes not unlike SSTs have been developed forpurposes ranging from question answering (Li andRoth, 2002) to animacy hierarchy representation forcorpus linguistics (Zaenen et al, 2004).
We believethe interpretation of the SSTs adopted here can serveas a single starting point for diverse resource en-gineering efforts and applications, especially whenfine-grained sense annotation is not feasible.2 Tagging ConventionsWordNet?s definitions of the supersenses are terse,and we could find little explicit discussion of thespecific rationales behind each category.
Thus,we have crafted more specific explanations, sum-marized for nouns in figure 2.
English examplesare given, but the guidelines are intended to belanguage-neutral.
A more systematic breakdown,formulated as a 43-rule decision list, is includedwith the corpus.5 In developing these guidelineswe consulted English WordNet (Fellbaum, 1998)and SemCor (Miller et al, 1993) for examples andsynset definitions, occasionally making simplifyingdecisions where we found distinctions that seemedesoteric or internally inconsistent.
Special cases(e.g., multiword expressions, anaphora, figurative5For example, one rule states that all man-made structures(buildings, rooms, bridges, etc.)
are to be tagged as ARTIFACTs.language) are addressed with additional rules.3 Arabic Wikipedia AnnotationThe annotation in this work was on top of a smallcorpus of Arabic Wikipedia articles that had al-ready been annotated for named entities (Mohit etal., 2012).
Here we use two different annotators,both native speakers of Arabic attending a universitywith English as the language of instruction.Data & procedure.
The dataset (table 1) consists ofthe main text of 28 articles selected from the topicaldomains of history, sports, science, and technology.The annotation task was to identify and categorizementions, i.e., occurrences of terms belonging tonoun supersenses.
Working in a custom, browser-based interface, annotators were to tag each relevanttoken with a supersense category by selecting the to-ken and typing a tag symbol.
Any token could bemarked as continuing a multiword unit by typing <.If the annotator was ambivalent about a token theywere to mark it with the ?
symbol.
Sentences werepre-tagged with suggestions where possible.6 Anno-tators noted obvious errors in sentence splitting andgrammar so ill-formed sentences could be excluded.Training.
Over several months, annotators alter-nately annotated sentences from 2 designated arti-cles of each domain, and reviewed the annotationsfor consistency.
All tagging conventions were deve-loped collaboratively by the author(s) and annotatorsduring this period, informed by points of confusionand disagreement.
WordNet and SemCor were con-sulted as part of developing the guidelines, but notduring annotation itself so as to avoid complicatingthe annotation process or overfitting to WordNet?sidiosyncracies.
The training phase ended once inter-annotator mention F1 had reached 75%.6Suggestions came from the previous named entity annota-tion of PERSONs, organizations (GROUP), and LOCATIONs, aswell as heuristic lookup in lexical resources?Arabic WordNetentries (Elkateb et al, 2006) mapped to English WordNet, andnamed entities in OntoNotes (Hovy et al, 2006).254O NATURAL OBJECT natural feature or nonliving object innature barrier reef nest neutron starplanet sky fishpond metamorphic rock Mediterranean cavestepping stone boulder Orion ember universeA ARTIFACT man-made structures and objects bridgerestaurant bedroom stage cabinet toaster antidote aspirinL LOCATION any name of a geopolitical entity, as well asother nouns functioning as locations or regionsCote d?Ivoire New York City downtown stage left IndiaNewark interior airspaceP PERSON humans or personified beings; names of socialgroups (ethnic, political, etc.)
that can refer to an individ-ual in the singular Persian deity glasscutter motherkibbutznik firstborn worshiper Roosevelt Arab consumerappellant guardsman Muslim American communistG GROUP groupings of people or objects, including: orga-nizations/institutions; followers of social movementscollection flock army meeting clergy Mennonite Churchtrumpet section health profession peasantry People?s PartyU.S.
State Department University of California populationconsulting firm communism Islam (= set of Muslims)$ SUBSTANCE a material or substance krypton mochaatom hydrochloric acid aluminum sand cardboard DNAH POSSESSION term for an entity involved in ownership orpayment birthday present tax shelter money loanT TIME a temporal point, period, amount, or measurement10 seconds day Eastern Time leap year 2nd millenium BC2011 (= year) velocity frequency runtime latency/delaymiddle age half life basketball season words per minutecurfew industrial revolution instant/moment August= RELATION relations between entities or quantitiesratio scale reverse personal relation exponential functionangular position unconnectedness transitivityQ QUANTITY quantities and units of measure, includingcardinal numbers and fractional amounts 7 cm 1.8 million12 percent/12% volume (= spatial extent) volt real numbersquare root digit 90 degrees handful ounce halfF FEELING subjective emotions indifference wondermurderousness grudge desperation astonishment sufferingM MOTIVE an abstract external force that causes someoneto intend to do something reason incentiveC COMMUNICATION information encoding and transmis-sion, except in the sense of a physical objectgrave accent Book of Common Prayer alphabetCree language onomatopoeia reference concert hotel billbroadcast television program discussion contract proposalequation denial sarcasm concerto software?
COGNITION aspects of mind/thought/knowledge/belief/perception; techniques and abilities; fields of academicstudy; social or philosophical movements referring to thesystem of beliefs Platonism hypothesislogic biomedical science necromancy hierarchical structuredemocracy innovativeness vocational program woodcraftreference visual image Islam (= Islamic belief system) dreamscientific method consciousness puzzlement skepticismreasoning design intuition inspiration muscle memory skillaptitude/talent method sense of touch awarenessS STATE stable states of affairs; diseases and their symp-toms symptom reprieve potencypoverty altitude sickness tumor fever measles bankruptcyinfamy opulence hunger opportunity darkness (= lack of light)@ ATTRIBUTE characteristics of people/objects that can bejudged resilience buxomness virtue immaterialityadmissibility coincidence valence sophistication simplicitytemperature (= degree of hotness) darkness (= dark coloring)!
ACT things people do or cause to happen; learned pro-fessions meddling malpractice faith healing dismountcarnival football game acquisition engineering (= profession)E EVENT things that happens at a given place and timebomb blast ordeal miracle upheaval accident tideR PROCESS a sustained phenomenon or one marked bygradual changes through a series of statesoscillation distillation overheating aging accretion/growthextinction evaporationX PHENOMENON a physical force or something that hap-pens/occurs electricity suction tailwind tornado effect+ SHAPE two and three dimensional shapesD FOOD things used as food or drinkB BODY human body parts, excluding diseases and theirsymptomsY PLANT a plant or fungusN ANIMAL non-human, non-plant lifeScience chemicals, molecules, atoms, and subatomicparticles are tagged as SUBSTANCESports championships/tournaments are EVENTs(Information) Technology Software names, kinds, andcomponents are tagged as COMMUNICATION (e.g.
kernel,version, distribution, environment).
A connection is a RE-LATION; project, support, and a configuration are taggedas COGNITION; development and collaboration are ACTs.Arabic conventions Masdar constructions (verbalnouns) are treated as nouns.
Anaphora are not tagged.Figure 2: Above: The complete supersense tagset for nouns; each tag is briefly described by its symbol, NAME,short description, and examples.
Some examples and longer descriptions have been omitted due to space constraints.Below: A few domain- and language-specific elaborations of the general guidelines.255Figure 3: Distribution of supersense mentions bydomain (left), and counts for tags occurring over800 times (below).
(Counts are of the union of theannotators?
choices, even when they disagree.
)tag num tag numACT (!)
3473 LOCATION (G) 1583COMMUNICATION (C) 3007 GROUP (L) 1501PERSON (P) 2650 TIME (T) 1407ARTIFACT (A) 2164 SUBSTANCE ($) 1291COGNITION (?)
1672 QUANTITY (Q) 1022Main annotation.
After training, the two annota-tors proceeded on a per-document basis: first theyworked together to annotate several sentences fromthe beginning of the article, then each was inde-pendently assigned about half of the remaining sen-tences (typically with 5?10 shared to measure agree-ment).
Throughout the process, annotators were en-couraged to discuss points of confusion with eachother, but each sentence was annotated in its entiretyand never revisited.
Annotation of 28 articles re-quired approximately 100 annotator-hours.
Articlesused in pilot rounds were re-annotated from scratch.Analysis.
Figure 3 shows the distribution of SSTs inthe corpus.
Some of the most concrete tags?BODY,ANIMAL, PLANT, NATURAL OBJECT, and FOOD?were barely present, but would likely be frequentin life sciences domains.
Others, such as MOTIVE,POSSESSION, and SHAPE, are limited in scope.To measure inter-annotator agreement, 87 sen-tences (2,774 tokens) distributed across 19 of the ar-ticles (not including those used in pilot rounds) wereannotated independently by each annotator.
Inter-annotator mention F1 (counting agreement over en-tire mentions and their labels) was 70%.
Excludingthe 1,397 tokens left blank by both annotators, thetoken-level agreement rate was 71%, with Cohen?s?
= 0.69, and token-level F1 was 83%.7We also measured agreement on a tag-by-tag ba-sis.
For 8 of the 10 most frequent SSTs (fig-ure 3), inter-annotator mention F1 ranged from 73%to 80%.
The two exceptions were QUANTITY at63%, and COGNITION (probably the most heteroge-neous category) at 49%.
An examination of the con-fusion matrix reveals four pairs of supersense cate-gories that tended to provoke the most disagreement:COMMUNICATION/COGNITION, ACT/COGNITION,ACT/PROCESS, and ARTIFACT/COMMUNICATION.7Token-level measures consider both the supersense labeland whether it begins or continues the mention.The last is exhibited for the first mention in figure 1,where one annotator chose ARTIFACT (referring tothe physical book) while the other chose COMMU-NICATION (the content).
Also in that sentence, an-notators disagreed on the second use of university(ARTIFACT vs. GROUP).
As with any sense anno-tation effort, some disagreements due to legitimateambiguity and different interpretations of the tags?especially the broadest ones?are unavoidable.A ?soft?
agreement measure (counting as matchesany two mentions with the same label and at leastone token in common) gives an F1 of 79%, show-ing that boundary decisions account for a major por-tion of the disagreement.
E.g., the city Fez, Mo-rocco (figure 1) was tagged as a single LOCATIONby one annotator and as two by the other.
Furtherexamples include the technical term ?thin client?,for which one annotator omitted the adjective; and?World Cup Football Championship?, where one an-notator tagged the entire phrase as an EVENT whilethe other tagged ?football?
as a separate ACT.4 ConclusionWe have codified supersense tags as a simple an-notation scheme for coarse lexical semantics, andhave shown that supersense annotation of Ara-bic Wikipedia can be rapid, reliable, and robust(about half the tokens in our data are coveredby a nominal supersense).
Our tagging guide-lines and corpus are available for download athttp://www.ark.cs.cmu.edu/ArabicSST/.AcknowledgmentsWe thank Nourhen Feki and Sarah Mustafa for assistancewith annotation, as well as Emad Mohamed, CMU ARKmembers, and anonymous reviewers for their comments.This publication was made possible by grant NPRP-08-485-1-083 from the Qatar National Research Fund (amember of the Qatar Foundation).
The statements madeherein are solely the responsibility of the authors.256ReferencesGiuseppe Attardi, Stefano Dei Rossi, Giulia Di Pietro,Alessandro Lenci, Simonetta Montemagni, and MariaSimi.
2010.
A resource and tool for super-sensetagging of Italian texts.
In Nicoletta Calzolari,Khalid Choukri, Bente Maegaard, Joseph Mariani,Jan Odijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh Interna-tional Conference on Language Resources and Evalu-ation (LREC?10), Valletta, Malta, May.
European Lan-guage Resources Association (ELRA).Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceed-ings of the 36th Annual Meeting of the Associationfor Computational Linguistics and 17th InternationalConference on Computational Linguistics (COLING-ACL ?98), pages 86?90, Montreal, Quebec, Canada,August.
Association for Computational Linguistics.D.
M. Bikel, R. Schwartz, and R. M. Weischedel.
1999.An algorithm that learns what?s in a name.
MachineLearning, 34(1).Massimiliano Ciaramita and Yasemin Altun.
2006.Broad-coverage sense disambiguation and informationextraction with a supersense sequence tagger.
In Pro-ceedings of the 2006 Conference on Empirical Meth-ods in Natural Language Processing, pages 594?602,Sydney, Australia, July.
Association for Computa-tional Linguistics.Massimiliano Ciaramita and Mark Johnson.
2003.
Su-persense tagging of unknown nouns in WordNet.
InProceedings of the 2003 Conference on EmpiricalMethods in Natural Language Processing, pages 168?175, Sapporo, Japan, July.James R. Curran.
2005.
Supersense tagging of unknownnouns using semantic similarity.
In Proceedings ofthe 43rd Annual Meeting on Association for Computa-tional Linguistics (ACL?05), pages 26?33, Ann Arbor,Michigan, June.Bonnie J. Dorr, Rebecca J. Passonneau, David Farwell,Rebecca Green, Nizar Habash, Stephen Helmreich,Eduard Hovy, Lori Levin, Keith J. Miller, TerukoMitamura, Owen Rambow, and Advaith Siddharthan.2010.
Interlingual annotation of parallel text corpora:a new framework for annotation and evaluation.
Nat-ural Language Engineering, 16(03):197?243.Sabri Elkateb, William Black, Horacio Rodr?
?guez, MusaAlkhalifa, Piek Vossen, Adam Pease, and ChristianeFellbaum.
2006.
Building a WordNet for Arabic.In Proceedings of The Fifth International Conferenceon Language Resources and Evaluation (LREC 2006),pages 29?34, Genoa, Italy.Christiane Fellbaum.
1990.
English verbs as a semanticnet.
International Journal of Lexicography, 3(4):278?301, December.Christiane Fellbaum, editor.
1998.
WordNet: an elec-tronic lexical database.
MIT Press, Cambridge, MA.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:the 90% solution.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL (HLT-NAACL), pages 57?60, New York City, USA, June.Association for Computational Linguistics.Paul Kingsbury and Martha Palmer.
2002.
From Tree-Bank to PropBank.
In Proceedings of the Third In-ternational Conference on Language Resources andEvaluation (LREC-02), Las Palmas, Canary Islands,May.Xin Li and Dan Roth.
2002.
Learning question classi-fiers.
In Proceedings of the 19th International Con-ference on Computational Linguistics (COLING?02),pages 1?7, Taipei, Taiwan, August.
Association forComputational Linguistics.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T. Bunker.
1993.
A semantic concordance.
InProceedings of the Workshop on Human LanguageTechnology (HLT ?93), HLT ?93, pages 303?308,Plainsboro, NJ, USA, March.
Association for Compu-tational Linguistics.George A. Miller.
1990.
Nouns in WordNet: a lexicalinheritance system.
International Journal of Lexicog-raphy, 3(4):245?264, December.Behrang Mohit, Nathan Schneider, Rishav Bhowmick,Kemal Oflazer, and Noah A. Smith.
2012.Recall-oriented learning of named entities in ArabicWikipedia.
In Proceedings of the 13th Conference ofthe European Chapter of the Association for Computa-tional Linguistics (EACL 2012), pages 162?173, Avi-gnon, France, April.
Association for ComputationalLinguistics.Gerhard Paa?
and Frank Reichartz.
2009.
Exploitingsemantic constraints for estimating supersenses withCRFs.
In Proceedings of the Ninth SIAM InternationalConference on Data Mining, pages 485?496, Sparks,Nevada, USA, May.
Society for Industrial and AppliedMathematics.Rebecca J. Passonneau, Ansaf Salleb-Aoussi, VikasBhardwaj, and Nancy Ide.
2010.
Word sense anno-tation of polysemous words by multiple annotators.In Nicoletta Calzolari, Khalid Choukri, Bente Mae-gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,Mike Rosner, and Daniel Tapias, editors, Proceed-ings of the Seventh International Conference on Lan-guage Resources and Evaluation (LREC?10), Valletta,Malta, May.
European Language Resources Associa-tion (ELRA).257Andrew G. Philpot, Michael Fleischman, and Eduard H.Hovy.
2003.
Semi-automatic construction of a generalpurpose ontology.
In Proceedings of the InternationalLisp Conference, New York, NY, USA, October.Davide Picca, Alfio Massimiliano Gliozzo, and Mas-similiano Ciaramita.
2008.
Supersense Taggerfor Italian.
In Nicoletta Calzolari, Khalid Choukri,Bente Maegaard, Joseph Mariani, Jan Odjik, SteliosPiperidis, and Daniel Tapias, editors, Proceedings ofthe Sixth International Language Resources and Eval-uation (LREC?08), pages 2386?2390, Marrakech, Mo-rocco, May.
European Language Resources Associa-tion (ELRA).Davide Picca, Alfio Massimiliano Gliozzo, and SimoneCampora.
2009.
Bridging languages by SuperSenseentity tagging.
In Proceedings of the 2009 NamedEntities Workshop: Shared Task on Transliteration(NEWS 2009), pages 136?142, Suntec, Singapore, Au-gust.
Association for Computational Linguistics.Likun Qiu, Yunfang Wu, Yanqiu Shao, and AlexanderGelbukh.
2011.
Combining contextual and struc-tural information for supersense tagging of Chineseunknown words.
In Computational Linguistics and In-telligent Text Processing: Proceedings of the 12th In-ternational Conference on Computational Linguisticsand Intelligent Text Processing (CICLing?11), volume6608 of Lecture Notes in Computer Science, pages 15?28.
Springer, Berlin.Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.2002.
Extended named entity hierarchy.
In Proceed-ings of the Third International Conference on Lan-guage Resources and Evaluation (LREC-02), Las Pal-mas, Canary Islands, May.Annie Zaenen, Jean Carletta, Gregory Garretson, JoanBresnan, Andrew Koontz-Garboden, Tatiana Nikitina,M.
Catherine O?Connor, and Tom Wasow.
2004.
An-imacy encoding in English: why and how.
In Bon-nie Webber and Donna K. Byron, editors, ACL 2004Workshop on Discourse Annotation, pages 118?125,Barcelona, Spain, July.
Association for ComputationalLinguistics.258
