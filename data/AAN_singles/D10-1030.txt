Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 304?314,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsJoint Inference for Bilingual Semantic Role LabelingTao Zhuang and Chengqing ZongNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences{tzhuang, cqzong}@nlpr.ia.ac.cnAbstractWe show that jointly performing semantic rolelabeling (SRL) on bitext can improve SRLresults on both sides.
In our approach, weuse monolingual SRL systems to produce ar-gument candidates for predicates in bitext atfirst.
Then, we simultaneously generate SRLresults for two sides of bitext using our jointinference model.
Our model prefers the bilin-gual SRL result that is not only reasonable oneach side of bitext, but also has more consis-tent argument structures between two sides.To evaluate the consistency between two argu-ment structures, we also formulate a log-linearmodel to compute the probability of aligningtwo arguments.
We have experimented withour model on Chinese-English parallel Prop-Bank data.
Using our joint inference model,F1 scores of SRL results on Chinese and En-glish text achieve 79.53% and 77.87% respec-tively, which are 1.52 and 1.74 points higherthan the results of baseline monolingual SRLcombination systems respectively.1 IntroductionIn recent years, there has been an increasing inter-est in SRL on several languages.
However, littleresearch has been done on how to effectively per-form SRL on bitext, which has important applica-tions including machine translation (Wu and Fung,2009).
A conventional way to perform SRL on bi-text is performing SRL on each side of bitext sep-arately, as has been done by Fung et al (2007) onChinese-English bitext.
However, it is very difficultto obtain good SRL results on both sides of bitextin this way.
The reason is that even the state-of-the-art SRL systems do not have very high accuracyon both English text (Ma`rquez et al, 2008; Pradhanet al, 2008; Punyakanok et al, 2008; Toutanova etal., 2008), and Chinese text (Che et al, 2008; Xue,2008; Li et al, 2009; Sun et al, 2009).On the other hand, the semantic equivalence be-tween two sides of bitext means that they shouldhave consistent predicate-argument structures.
Thisbilingual argument structure consistency can guideus to find better SRL results.
For example, in Fig-ure 1(a), the argument structure consistency canguide us to choose a correct SRL result on Chineseside.
Consistency between two argument structuresis reflected by sound argument alignments betweenthem, as shown in Figure 1(b).
Previous research hasshown that bilingual constraints can be very help-ful for parsing (Burkett and Klein, 2008; Huang etal., 2008).
In this paper, we show that the bilingualargument structure consistency can be leveraged tosubstantially improve SRL results on both sides ofbitext.Formally, we present a joint inference model topreform bilingual SRL.
Using automatic word align-ment on bitext, we first identify a pair of predicatesthat align with each other.
And we use monolin-gual SRL systems to produce argument candidatesfor each predicate.
Then, our model jointly generateSRL results for both predicates from their argumentcandidates, using integer linear programming (ILP)technique.
An overview of our approach is shown inFigure 2.Our joint inference model consists of three com-ponents: the source side, the target side, and the ar-304In recent years the pace of opening up to the outside of China `s construction market    has   further    accelerated[    AM-TMP    ]  [                                                        A1                                                 ]          [   A2  ]   [   Pred   ]R1:    [                A1                  ]    [ AM-TMP ]    [            C-A1           ]     [ AM-ADV ]    [Pred]R2:    [                                                     A1                                           ]     [ AM-ADV ]    [Pred]??
??
??
??
?
?
?
??
??
???
?
?zhongguo jianzhu shichang      jinnian lai        dui wai kaifang bufa         jinyibu         jiakuai[    AM-TMP   ] [                                                        A1                                                  ]         [   A2   ]  [    Pred    ]In recent years the pace of opening up to the outside of China `s construction market    has   further    accelerated??
??
??
??
?
?
?
??
??
???
??
[ A1 ] [ AM-TMP ] [ C-A1          ]    [AM-ADV]    [Pred](a) Word alignment and SRL results for a Chinese-English predicate pair.
(b) Argument alignments for a Chinese-English predicate pair.Figure 1: An example from Chinese-English parallel PropBank.
In (a), the SRL results are generated by the state-of-the-art monolingual SRL systems.
The English SRL result is correct.
But it is to more difficult to get correctSRL result on Chinese side, because the AM-TMP argument embeds into a discontinuous A1 argument.
The ChineseSRL result in the row marked by ?R1?
is correct and consistent with the result on English side.
Whereas the result inthe row marked by ?R2?
is incorrect and inconsistent with the result on English side, with the circles showing theirinconsistency.
The argument structure consistency can guide us to choose the correct Chinese SRL result.MonolingualSRL SystemMonolingualSRL SystemOur JointInferenceModelSource-sidePredidateTarget sidePredicateSource-sideSRLCandidatesTarget-sideSRLCandidates BilingualSRL ResultFigure 2: Overview of our approach.gument alignment between two sides.
These threecomponents correspond to three interrelated factors:the quality of the SRL result on source side, the qual-ity of the SRL result on target side, and the argu-ment structure consistency between the SRL resultson both sides.
To evaluate the consistency betweenthe two argument structures in our joint inferencemodel, we formulate a log-linear model to computethe probability of aligning two arguments.
Experi-ments on Chinese-English parallel PropBank showsthat our model significantly outperforms monolin-gual SRL combination systems on both Chinese andEnglish sides.The rest of this paper is organized as follows: Sec-tion 2 introduces related work.
Section 3 describeshow we generate SRL candidates on each side of bi-text.
Section 4 presents our joint inference model.Section 5 presents our experiments.
And Section 6concludes our work.2 Related WorkSome existing work on monolingual SRL combina-tion is related to our work.
Punyakanok et al (2004;2008) formulated an ILP model for SRL.
Koomenet al (2005) combined several SRL outputs usingILP method.
Ma`rquez et al (2005) and Pradhan etal.
(2005) proposed combination strategies that arenot based on ILP method.
Surdeanu et al (2007)did a complete research on a variety of combinationstrategies.
Zhuang and Zong (2010) proposed a min-imum error weighting combination strategy for Chi-nese SRL combination.Research on SRL utilizing parallel corpus is alsorelated to our work.
Pado?
and Lapata (2009) didresearch on cross-lingual annotation projection onEnglish-German parallel corpus.
They performedSRL only on the English side, and then mappedthe English SRL result to German side.
Fung etal.
(2007) did pioneering work on studying argu-ment alignment on Chinese-English parallel Prop-Bank.
They performed SRL on Chinese and En-glish sides separately.
Then, given the SRL resulton both sides, they automatically induced the argu-ment alignment between two sides.The major difference between our work and allexisting research is that our model performs SRL in-ference on two sides of bitext simultaneously.
In our305model, we jointly consider three interrelated factors:SRL result on the source side, SRL result on the tar-get side, and the argument alignment between them.3 Generating Candidates for Inference3.1 Monolingual SRL SystemAs shown in Figure 2, we need to use a monolin-gual SRL system to generate candidates for our jointinference model.
We have implemented a monolin-gual SRL system which utilize full phrase-structureparse trees to perform SRL.
In this system, the wholeSRL process is comprised of three stages: pruning,argument identification, and argument classification.In the pruning stage, the heuristic pruning methodin (Xue, 2008) is employed.
In the argument iden-tification stage, a number of argument locations areidentified in a sentence.
In the argument classifica-tion stage, each location identified in the previousstage is assigned a semantic role label.
Maximumentropy classifier is employed for both the argumentidentification and classification tasks.
And ZhangLe?s MaxEnt toolkit1 is used for implementation.We use the monolingual SRL system describedabove for both Chinese and English SRL tasks.
Forthe Chinese SRL task, the features used in this paperare the same with those used in (Xue, 2008).
Forthe English SRL task, the features used are the samewith those used in (Pradhan et al, 2008).3.2 Output of the Monolingual SRL SystemThe maximum entropy classifier in our monolingualSRL system can output classification probabilities.We use the classification probability of the argumentclassification stage as an argument?s probability.
Asillustrated in Figure 3, in an individual system?s out-put, each argument has three attributes: its locationin sentence loc, represented by the number of its firstword and last word; its semantic role label l; and itsprobability p.So each argument outputted by a system is a triple(loc, l, p).
For example, the A0 argument in Figure 3is ((0, 2),A0, 0.94).
Because these outputs are to becombined, we call such triple a candidate.1http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.htmlSent: 	??]????
?I	n?
?O:Args: [ A0 ] [Pred] [ A1 ]loc: (0, 2) (4, 7)l: A0 A1p: 0.94 0.92Figure 3: Three attributes of an output argument: locationloc, label l, and probability p.3.3 Generating and Merging CandidatesTo generate candidates for joint inference, we needto have multiple SRL results on each side of bi-text.
Therefore, for both Chinese and English SRLsystems, we use the 3-best parse trees of Berkeleyparser (Petrov and Klein, 2007) and 1-best parsetrees of Bikel parser (Bikel, 2004) and Stanfordparser (Klein and Manning, 2003) as inputs.
All thethree parsers are multilingual parsers.
The secondand third best parse trees of Berkeley parser are usedfor their good quality.
Therefore, each monolingualSRL system produces 5 different outputs.Candidates from different outputs may have thesame loc and l but different p. So we merge allcandidates with the same loc and l into one by av-eraging their probabilities.
For a merged candidate(loc, l, p), we say that p is the probability of assign-ing l to loc.4 Joint Inference ModelOur model can be conceptually decomposed to threecomponents: the source side, the target side, and theargument alignment.
The objective function of ourjoint inference model is the weighted sum of threesub-objectives:max Os + ?1Ot + ?2Oa (1)where Os and Ot represent the quality of the SRLresults on source and target sides, and Oa representsthe soundness of the argument alignment betweenthe SRL results on two sides, ?1, ?2 are positiveweights corresponding to the importance of Ot andOa respectively.4.1 Components of Source and Target Sides4.1.1 Source Side ComponentThe source side component aims to improve theSRL result on source side.
This is equivalent to a306monolingual SRL combination problem.For convenience, we denote the whole semanticrole label set for source language as {ls1, ls2, .
.
.
, lsLs},in which ls1 ?
ls6 stand for the key argument labelsA0 ?
A5 respectively.
Suppose there are Ns differ-ent locations, denoted as locs1, .
.
.
, locsNs , among allcandidates on the source side.
The probability of as-signing lsj to locsi is psij .
An indicator variable xij isdefined as:xij = [locsi is assigned label lsj ].Then the source side sub-objective Os in equation(1) is the sum of arguments?
probabilities on sourceside:Os =Ns?i=1Ls?j=1(psij ?
Ts)xij (2)where Ts is a bias to prevent including too many can-didates in solution (Surdeanu et al, 2007).We consider the following two linguistically mo-tivated constraints:1.
No duplication: There is no duplication for keyarguments: A0 ?
A5.2.
No overlapping: Arguments cannot overlapwith each other.In (Punyakanok et al, 2004), several more con-straints are considered.
According to (Surdeanuet al, 2007), however, no significant performanceimprovement can be obtained by considering moreconstraints than the two above.
So we do not con-sider other constraints.The inequalities in (3) make sure that each locsi isassigned at most one label.
?1 ?
i ?
Ns :Ls?j=1xij ?
1 (3)The inequalities in (4) satisfy the No duplicationconstraint.
?1 ?
j ?
6 :Ns?i=1xij ?
1 (4)For any source side location locsi , let Ci denotethe index set of the locations that overlap with it.Then the No overlapping constraint means that iflocsi is assigned a label, i.e.,?Nsj=1 xij = 1, thenfor any u ?
Ci, locsu cannot be assigned any label,i.e.,?Nsj=1 xuj = 0.
A common technique in ILPmodeling to form such a constraint is to use a suf-ficiently large auxiliary constant M .
And the con-straint is formulated as:?1 ?
i ?
Ns :?u?CiLs?j=1xuj ?
(1?Ls?j=1xij)M (5)In this case,M only needs to be larger than the num-ber of candidates to be combined.
In this paper,M = 500 is large enough.4.1.2 Target Side ComponentIn principle, the target side component of our jointinference model is the same with the source sidecomponent.The whole semantic role label set for target lan-guage is denoted by {lt1, lt2, .
.
.
, ltLt}.
There areNt different locations, denoted as loct1, .
.
.
, loctNt ,among all candidates in the target side.
And lt1 ?
lt6stand for the key argument labels A0 ?
A5 respec-tively.
The probability of assigning ltj to loctk is ptkj .An indicator variable ykj is defined as:ykj = [loctk is assigned label ltj ].Then the target side sub-objective Ot in equation (1)is:Ot =Nt?k=1Lt?j=1(ptkj ?
Tt)ykj (6)The constraints on target side are as follows:Each loctk is assigned at most one label:?1 ?
k ?
Nt :Lt?j=1ykj ?
1 (7)The No duplication constraint:?1 ?
j ?
6 :Nt?k=1ykj ?
1 (8)The No overlapping constraint:?1 ?
k ?
Nt :?v?CkLt?j=1yvj ?
(1?Lt?j=1ykj)M (9)In (9), Ck denotes the index set of the locations thatoverlap with loctk, and the constant M is set to 500in this paper.3074.2 Argument AlignmentThe argument alignment component is the core ofour joint inference model.
It gives preference to thebilingual SRL results that have more consistent ar-gument structures.For a source side argument argsi = (locsi , ls) anda target side argument argtk = (loctk, lt), let zik bethe following indicator variable:zik = [argsi aligns with argtk].We use paik to represent the probability that argsi andargtk align with each other, i.e., paik = P (zik = 1).We call paik the argument alignment probabilitybetween argsi and argtk.4.2.1 Argument Alignment Probability ModelWe use a log-linear model to compute the argu-ment alignment probability paik between argsi andargtk.
Let (s, t) denote a bilingual sentence pair andwa denote the word alignment on (s, t).
Our log-linear model defines a distribution on zik given thetuple tup = (argsi , argtk, wa, s, t):P (zik|tup) ?
exp(wT?
(tup))where ?
(tup) is the feature vector.
With this model,paik can be computed as paik = P (zik = 1|tup).In order to study the argument alignment in cor-pus and to provide training data for our log-linearmodel, we have manually aligned the arguments in60 files (chtb 0121.fid to chtb 0180.fid) of Chinese-English parallel PropBank.
On this data set, we getthe argument alignment matrix in Table 1.Ch\En A0 A1 A2 A3 A4 AM* NULA0 492 30 4 0 0 0 46A1 98 853 43 2 0 0 8A2 9 57 51 1 0 47 0A3 1 0 2 6 0 0 0A4 0 0 2 0 3 0 0AM* 0 2 39 0 0 895 221NUL 53 14 27 0 0 45 0Table 1: The argument alignment matrix on manuallyaligned corpus.Each entry in Table 1 is the number of times forwhich one type of Chinese argument aligns with onetype of English argument.
AM* stands for all ad-juncts types like AM-TMP, AM-LOC, etc., and NULmeans that the argument on the other side cannot bealigned with any argument on this side.
For exam-ple, the number 46 in the A0 row and NUL columnmeans that Chinese A0 argument cannot be alignedwith any argument on English side for 46 times inour manually aligned corpus.We use the following features in our model.Word alignment feature: If there are many word-to-word alignments between argsi and argtk, thenit is very probable that argsi and argtk would alignwith each other.
We adopt the method used in (Pado?and Lapata, 2009) to measure the word-to-wordalignments between argsi and argtk.
And the wordalignment feature is defined as same as the wordalignment-based word overlap in (Pado?
and Lapata,2009).
Note that this is a real-valued feature.Head word alignment feature: The head wordof an argument is usually more representative thanother words.
So we use whether the head words ofargsi and argtk align with each other as a binary fea-ture.
The use of this feature is inspired by the workin (Burkett and Klein, 2008).Semantic role labels of two arguments: From Ta-ble 1, we can see that semantic role labels of two ar-guments are a good indicator of whether they shouldalign with each other.
For example, a Chinese A0argument aligns with an English A0 argument mostof the times, and never aligns with an English AM*argument in Table 1.
Therefore, the semantic rolelabels of argsi and argtk are used as a feature.Predicate verb pair: Different predicate pairs havedifferent argument alignment patterns.
Let?s take theChinese predicate O/zengzhang and the Englishpredicate grow as an example.
The argument align-ment matrix for all instances of the Chinese-Englishpredicate pair (zengzhang, grow) in our manuallyaligned corpus is shown in Table 2.CH \EN A0 A1 A2 AM* NULA0 0 16 0 0 0A1 0 0 12 0 0AM* 0 0 4 7 10NUL 0 0 0 2 0Table 2: The argument alignment matrix for the predicatepair (zengzhang, grow).From Table 2 we can see that all A0 arguments ofzengzhang align with A1 arguments of grow.
This308is very different from the results in Table 1, where aChinese A0 argument tends to align with an EnglishA0 argument.
This phenomenon shows that a pred-icate pair can determine which types of argumentsshould align with each other.
Therefore, we use thepredicate pair as a feature.4.2.2 Argument Alignment ComponentThe argument alignment sub-objective Oa inequation (1) is the sum of argument alignment prob-abilities:Oa =Ns?i=1Nt?k=1(paik ?
Ta)zik (10)where Ta is a bias to prevent including too manyalignments in final solution, and paik is computedusing the log-linear model described in subsec-tion 4.2.1.Oa reflects the consistency between argumentstructures on two sides of bitext.
Larger Oa meansbetter argument alignment between two sides, thusindicates more consistency between argument struc-tures on two sides.The following constraints are considered:1.
Conformity with bilingual SRL result.
Forall candidates on both source and target sides, onlythose that are chosen to be arguments on each sidecan be aligned.2.
One-to-many alignment limit.
Each argumentcan not be aligned with more than 3 arguments.3.
Complete argument alignment.
Each argumenton source side must be aligned with at least one ar-gument on target side, and vice versa.The Conformity with bilingual SRL result con-straint is necessary to validly integrate the bilingualSRL result with the argument alignment.
This con-straint means that if argsi and argtk align with eachother, i.e., zik = 1, then locsi must be assigneda label on source side, i.e.,?Lsj=1 xij = 1, andloctk must be assigned a label on target side, i.e.,?Ltj=1 ykj = 1.
So this constraint can be representedas:?1 ?
i ?
Ns, 1 ?
k ?
Nt :Ls?j=1xij ?
zik (11)?1 ?
k ?
Nt, 1 ?
i ?
Ns :Lt?j=1ykj ?
zik (12)The One-to-many alignment limit constraintcomes from our observation on manually alignedcorpus.
We have found that no argument aligns withmore than 3 arguments in our manually aligned cor-pus.
This constraint can be represented as:?1 ?
i ?
Ns :Nt?k=1zik ?
3 (13)?1 ?
k ?
Nt :Ns?i=1zik ?
3 (14)The Complete argument alignment constraintcomes from the semantic equivalence between twosides of bitext.
For each source side location locsi ,if it is assigned a label, i.e.,?Lsj=1 xij = 1, then itmust be aligned with some arguments on target side,i.e.,?Ntk=1 zik ?
1.
This can be represented as:?1 ?
i ?
Ns :Nt?k=1zik ?Ls?j=1xij (15)Similarly, each target side argument must be alignedto at least one source side argument.
This can berepresented as:?1 ?
k ?
Nt :Ns?i=1zik ?Lt?j=1ykj (16)4.3 Complete Argument Alignment as a SoftConstraintAlthough the hard Complement argument alignmentconstraint is ideally reasonable, in real situations thisconstraint does not always hold.
The manual argu-ment alignment result shown in Table 1 indicatesthat in some cases an argument cannot be alignedwith any argument on the other side (see the NULrow and column in Table 1).
Therefore, it wouldbe reasonable to change the hard Complement argu-ment alignment constraint to a soft one.
To do so,we need to remove the hard Complement argumentalignment constraint and add penalty for violation ofthis constraint.If an argument does not align with any argumenton the other side, we say it aligns with NUL.
And wedefine the following indicator variables:zi,NUL = [argsi aligns with NUL], 1 ?
i ?
Ns.309zNUL,k = [argtk aligns with NUL], 1 ?
k ?
Nt.Then?Nsi=1 zi,NUL is the number of source side ar-guments that align with NUL.
And?Ntk=1 zNUL,k isthe number of target side arguments that align withNUL.
For each argument that aligns with NUL, weadd a penalty ?3 to the argument alignment sub-objective Oa.
Therefore, the sub-objective Oa inequation (10) is changed to:Oa =Ns?i=1Nt?k=1(paik ?
Ta)zik?
?3(Ns?i=1zi,NUL +Nt?k=1zNUL,k) (17)From the definition of zi,NUL, it is obvious that,for any 1 ?
i ?
Ns, zi,NUL and zik(1 ?
k ?
Nt)have the following relationship: If?Ntk=1 zik ?
1,i.e., argsi aligns with some arguments on target side,then zi,NUL = 0; Otherwise, zi,NUL = 1.
Theserelationships can be captured by the following con-straints:?1 ?
i ?
Ns, 1 ?
k ?
Nt : zi,NUL ?
1?zik (18)?1 ?
i ?
Ns :Nt?k=1zik + zi,NUL ?
1 (19)Similarly, for any 1 ?
k ?
Nt, zNUL,k andzik(1 ?
i ?
Ns) observe the following constraints:?1 ?
k ?
Nt, 1 ?
i ?
Ns : zNUL,k ?
1?
zik(20)?1 ?
k ?
Nt :Ns?i=1zik + zNUL,k ?
1 (21)4.4 Models SummarySo far, we have presented two versions of our jointinference model.
The first version treats Comple-ment argument alignment as a hard constraint.
Wewill refer to this version as Joint1.
The objectivefunction of Joint1 is defined by equations (1, 2, 6,10).
And the constraints of Joint1 are defined byequations (3-5, 7-9, 11-16).The sencond version treats Complement argumentalignment as a soft constraint.
We will refer to thisversion as Joint2.
The objective function of Joint2is defined by equations (1, 2, 6, 17).
And the con-straints of Joint2 are defined by equations (3-5, 7-9,11-14, 18-21).Our baseline models are monolingual SRL com-bination models.
We will refer to the source sidecombination model as SrcCmb.
The objective of Sr-cCmb is to maximize Os, which is defined in equa-tion (2).
And the constraints of SrcCmb are definedby equations (3-5).
Similarly, we will refer to the tar-get side combination model as TrgCmb.
The objec-tive of TrgCmb is to maximize Ot defined in equa-tion (6).
And the constraints of TrgCmb are definedby equations (7-9).
In this paper, we employ lp-solve2 to solve all ILP models.5 Experiments5.1 Experimental SetupIn our experiments, we use the Xinhua News por-tion of Chinese and English data in LDC OntoNotesRelease 3.0.
This data is a Chinese-English parallelproposition bank described in (Palmer et al, 2005).It contains parallel proposition annotations for 325files (chtb 0001.fid to chtb 0325.fid) from Chinese-English parallel Treebank.
The English part of thisdata contains proposition annotations only for ver-bal predicates.
Therefore, we only consider verbalpredicates in this paper.We employ the GIZA++ toolkit (Och and Ney,2003) to perform automatic word alignment.
Be-sides the parallel PropBank data, we use additional4,500K Chinese-English sentence pairs3 to induceword alignments for both directions, with the defaultGIZA++ settings.
The alignments are symmetrizedusing the intersection heuristic (Och and Ney, 2003),which is known to produce high-precision align-ments.We use 80 files (chtb 0001.fid to chtb 0080.fid)as test data, and 40 files (chtb 0081.fid tochtb 0120.fid) as development data.
Although ourjoint inference model needs no training, we stillneed to train a log-linear argument alignment prob-ability model, which is used in the joint inferencemodel.
As specified in subsection 4.2.1, the train-2http://lpsolve.sourceforge.net/3These data includes the following LDC corpus:LDC2002E18, LDC2003E07, LDC2003E14, LDC2005T06,LDC2004T07, LDC2000T50.310ing set for the argument alignment probability modelconsists of 60 files (chtb 0121.fid to chtb 0180.fid)with manual argument alignment.
Unfortunately,the quality of automatic word alignment on one-to-many Chinese-English sentence pairs is usuallyvery poor.
So we only include one-to-one Chinese-English sentence pairs in all data.
And not all predi-cates in a sentence pair can be included.
Only bilin-gual predicate pairs are included.
A bilingual pred-icate pair is defined to be a pair of predicates in bi-text which align with each other in automatic wordalignment.
Table 3 shows how many sentences andpredicates are included in each data set.Test Dev TrainArticles 1-80 81-120 121-180Chinese Sentences 1067 578 778English Sentences 1182 620 828Bilingual pairs 821 448 614Chinese Predicates 3792 2042 2572English Predicates 2864 1647 1860Bilingual pairs 1476 790 982Table 3: Sentence and predicate counts.Our monolingual SRL systems are trained sep-arately.
Our Chinese SRL system is trained on640 files (chtb 0121.fid to chtb 0931.fid) in ChinesePropbank 1.0.
Because Xinhua News is a quite dif-ferent domain from WSJ, the training set for our En-glish SRL system includes not only Sections 02?21of WSJ data in English Propbank, but also 205 files(chtb 0121.fid to chtb 0325.fid) in the English partof parallel PropBank.
For Chinese, the syntacticparsers are trained on 640 files (chtb 0121.fid tochtb 0931.fid) plus the broadcast news portion ofChinese Treebank 6.0.
For English, the syntacticparsers are trained on the following data: Sections02?21 of WSJ data in English Treebank, 205 files(chtb 0121.fid to chtb 0325.fid) of Xinhua Newsdata in OntoNotes 3.0, and the Sinorama data inOntoNotes 3.0.
We treat discontinuous and corefer-ential arguments in accordance to the CoNLL-2005shared task (Carreras and Ma`rquez, 2005).
The firstpart of a discontinuous argument is labeled as it is,and the second part is labeled with a prefix ?C-?.All coreferential arguments are labeled with a prefix?R-?.5.2 Tuning Parameters in ModelsThe models Joint1, Joint2, SrcCmb, and TrgCmbhave different parameters.
For each model, we haveautomatically tuned its parameters on developmentset using Powell?s Mothod (Brent, 1973).
Powell?sMethod is a heuristic optimization algorithm thatdoes not require the objective function to have an ex-plicit analytical formula.
For a monolingual modellike SrcCmb or TrgCmb, our objective is to maxi-mize the F1 score of the model?s result on develop-ment set.
But a joint model, like Joint1 or Joint2,generates SRL results on both sides of bitext.
Soour objective is to maximize the sum of the two F1scores of the model?s results for both Chinese andEnglish on development set.
For all models, we re-gard the parameters to be tuned as variables.
Thenwe optimize our objective using Powell?s Method.The solution of this optimization is the values of pa-rameters.
To avoid finding poor local optimum, weperform the optimization 30 times with different ini-tial parameter values, and choose the best solutionfound.
The final parameter values are listed in Ta-ble 4.Model Ts Tt Ta ?1 ?2 ?3SrcCmb 0.21 - - - - -TrgCmb - 0.32 - - - -Joint1 0.17 0.22 0.36 0.96 1.04 -Joint2 0.15 0.26 0.42 1.02 1.21 0.15Table 4: Parameter values in models.5.3 Individual SRL Outputs?
PerformanceAs specified in subsection 3.3, the monoligual SRLsystem uses different parse trees to generate multi-ple SRL outputs.
The performance of these outputson test set is shown in Table 5.
In Table 5, O1?O3are the outputs using 3-best parse trees of Berkeleyparser respectively, O4 and O5 are the outputs us-ing the best parse trees of Stanford parser and Bikelparser respectively.As specified in subsection 5.1, only a small partof English SRL training data is in the same domainwith test data.
Therefore, the English SRL result inTable 5 is not very impressive.
But the Chinese SRLresult is pretty good.311Side Outputs P (%) R(%) F1O1 79.84 71.95 75.69O2 78.53 70.32 74.20Chinese O3 78.41 69.99 73.96O4 73.21 67.13 70.04O5 75.32 63.78 69.07O1 77.13 70.42 73.62O2 75.88 69.06 72.31English O3 75.74 68.65 72.02O4 71.57 66.11 68.73O5 73.12 68.04 70.49Table 5: The results of individual monolingual SRL out-puts on test set.5.4 Effects of Different ConstraintsThe One-to-many limit and Complete argumentalignment constraints in subsection 4.2.2 comesfrom our empirical knowledge.
To investigate theeffect of these two constraits, we remove them fromour joint inference models one by one, and observethe performance variations on test set.
The resultsare shown in Table 6.
In Table 6, ?c2?
refers to theOne-to-many limit constraint, ?c3?
refers to the Com-plete argument alignment constraint, and ?-?
meansremoving.
For example, ?Joint1 - c2?
means remov-ing the constraint ?c2?
from the model Joint1.
Recallthat the only difference between Joint1 and Joint2 isthat ?c3?
is a hard constraint in Joint1, but a soft con-straint in Joint2.
Therefore, ?Joint2 - c3?
and ?Joint2- c2 - c3?
do not appear in Table 6, because they arethe same with ?Joint1 - c3?
and ?Joint1 - c2 - c3?respectively.Model Side P (%) R(%) F1Joint1Chinese82.95 75.21 78.89Joint1 - c2 81.46 75.97 78.62Joint1 - c3 82.36 74.68 78.33Joint1 - c2 - c3 82.04 74.67 78.18Joint2 83.35 76.04 79.53Joint2 - c2 82.41 76.03 79.09Joint1English79.38 75.16 77.21Joint1 - c2 78.51 75.22 76.83Joint1 - c3 78.66 74.55 76.55Joint1 - c2 - c3 78.37 74.37 76.32Joint2 79.64 76.18 77.87Joint2 - c2 78.41 75.89 77.13Table 6: Results of different joint models on test set.From Table 6, we can see that the constraints ?c2?and ?c3?
both have positive effect in our joint in-ference model, because removing any one of themcauses performance degradation.
And removing?c3?
from Joint1 causes more performance degrada-tion than removing ?c2?.
This means that ?c3?
playsa more important role than ?c2?
in our joint inferencemodel.
Indeed, by treating ?c3?
as a soft constraint,the model Joint2 has the best performance on bothsides of bitext.5.5 Final ResultsWe use Joint2 as our final joint inference model.And as specified in subsection 4.4, our baselines aremonolingual SRL combination models: SrcCmb forChinese, and TrgCmb for English.
Note that SrcCmband TrgCmb are basically the same as the state-of-the-art combination model in (Surdeanu et al, 2007)with No overlapping and No duplication constraints.The final results on test set are shown in Table 7.Side Model P (%) R(%) F1ChineseSrcCmb 82.58 73.92 78.01Joint2 83.35 76.04 79.53EnglishTrgCmb 79.02 73.44 76.13Joint2 79.64 76.18 77.87Table 7: Comparison between monolingual combinationmodel and our joint inference model on test set.From Table 5 and Table 7, we can see that SrcCmband TrgCmb improve F1 scores over the best indi-vidual SRL outputs by 2.32 points and 2.51 pointson Chinese and English seperately.
Thus they formstrong baselines for our joint inference model.
Evenso, our joint inference model still improves F1 scoreover SrcCmb by 1.52 points, and over TrgCmb by1.74 points.From Table 7, we can see that, despite only part oftraining data for English SRL system is in-domain,our joint inference model still produces good En-glish SRL result.
And the F1 score of Chinese SRLresult reaches 79.53%, which represents the state-of-the-art Chinese SRL performance to date.6 ConclusionsIn this paper, we propose a joint inference modelto perform bilingual SRL.
Our joint inferencemodel incorporates not only linguistic constraints on312source and target sides of bitext, but also the bilin-gual argument structure consistency requirement onbitext.
Experiments on Chinese-English parallelPropBank show that our joint inference model isvery effective for bilingual SRL.
Compared to state-of-the-art monolingual SRL combination baselines,our joint inference model substantially improvesSRL results on both sides of bitext.
In fact, the so-lution of our joint inference model contains not onlythe SRL results on bitext, but also the optimal argu-ment alignment between two sides of bitext.
Thismakes our model especially suitable for applicationin machine translation, which needs to obtain the ar-gument alignment.AcknowledgmentsThe research work has been partially funded bythe Natural Science Foundation of China underGrant No.
60975053 and 60736014, the NationalKey Technology R&D Program under Grant No.2006BAH03B02.
We would like to thank JiajunZhang for helpful discussions and the anonymousreviewers for their valuable comments.ReferencesDaniel Bikel.
2004.
Intricacies of Collins Parsing Model.Computational Linguistics, 30(4):480-511.Richard P. Brent.
1973.
Algorithms for Minimizationwithout Derivatives.
Prentice-Hall, Englewood Cliffs,NJ.David Burkett, and Dan Klein.
2008.
Two Languagesare Better than One (for Syntactic Parsing).
In Pro-ceedings of EMNLP-2008, pages 877-886.Xavier Carreras, and Llu?
?s Ma`rquez.
2005.
Introductionto the CoNLL-2005 shared task: semantic role label-ing.
In Proceedings of CoNLL-2005, pages 152-164.Wanxiang Che, Min Zhang, Ai Ti Aw, Chew Lim Tan,Ting Liu, and Sheng Li.
2008.
Using a Hybrid Convo-lution Tree Kernel for Semantic Role Labeling.
ACMTransactions on Asian Language Information Process-ing, 2008, 7(4): 1-23.Pascale Fung, Zhaojun Wu, Yongsheng Yang and DekaiWu.
2007.
Learning Bilingual Semantic Frames:Shallow Semantic Parsing vs. Semantic Role Projec-tion.
In Proceedings of the 11th Conference on The-oretical and Methodological Issues in Machine Trans-lation, pages 75-84.Liang Huang, Wenbin Jiang, Qun Liu.
2009.Bilingually-Constrained (Monolingual) Shift-ReduceParsing.
In Proceedings of EMNLP-2009, pages 1222-1231.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL-2003,pages 423-430.Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2005.
Generalized Inference with MultipleSemantic Role Labeling Systems.
In Proceedings ofCoNLL-2005 shared task, pages 181-184.Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu,and Peide Qian.
2009.
Improving Nominal SRL inChinese Language with Verbal SRL Information andAutomatic Predicate Recognition.
In Proceedings ofEMNLP-2009, pages 1280-1288.Llu?
?s Ma`rquez, Xavier Carreras, Kenneth C. Litkowski,Suzanne Stevenson.
2008.
Semantic Role Labeling:An Introduction to the Special Issue.
ComputationalLinguistics, 34(2):145-159.Llu?
?s Ma`rquez, Mihai Surdeanu, Pere Comas, and JordiTurmo.
2005.
A Robust Combination Strategy forSemantic Role Labeling.
In Proceedings of EMNLP-2005, pages 644-651.Frans J. Och, and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29:19-51.Sebastian Pado?, and Mirella Lapata.
2009.
Cross-lingualAnnotation Projection of Semantic Roles.
Journal ofArtificial Intelligence Research (JAIR), 36:307-340.Martha Palmer, Nianwen Xue, Olga Babko-Malaya, Jiny-ing Chen, Benjamin Snyder.
2005.
A Parallel Propo-sition Bank II for Chinese and English.
In Frontiersin Corpus Annotation, Workshop in conjunction withACL-05, pages 61-67.Slav Petrov and Dan Klein.
2007.
Improved Inferencefor Unlexicalized parsing.
In Proceedings of ACL-2007, pages 46-54.Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu, JamesH.
Martin, and Daniel Jurafsky.
2005.
Semantic RoleLabeling Using Different Syntactic Views.
In Pro-ceedings of ACL-2005, pages 581-588.Sameer S. Pradhan, Wayne Ward, James H. Martin.2008.
Towards Robust Semantic Role Labeling.
Com-putational Linguistics, 34(2):289-310.Vasin Punyakanok, Dan Roth, Wen-tauYih.
2008.
TheImportance of Syntactic Parsing and Inference in Se-mantic Role Labeling.
Computational Linguistics,34(2):257-287.Vasin Punyakanok, Dan Roth, Wen-tau Yih, and DavZimak.
2004.
Semantic Role Labeling via IntegerLinear Programming Inference.
In Proceedings ofCOLING-2004, pages 1346-1352.Weiwei Sun, Zhifang Sui, Meng Wang, and Xin Wang.2009.
Chinese Semantic Role Labeling with Shallow313Parsing.
In Proceedings of EMNLP-2009, pages 1475-1483.Mihai Surdeanu, Llu?
?s Ma`rquez, Xavier Carreras, andPere R. Comas.
2007.
Combination Strategies forSemantic Role Labeling.
Journal of Artificial Intel-ligence Research (JAIR), 29:105-151.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A Global Joint Model for Seman-tic Role Labeling.
Computational Linguistics, 34(2):145-159.Dekai Wu, and Pascale Fung.
2009.
Semantic Roles forSMT: A Hybrid Two-Pass Model.
In Proceedings ofNAACL-2009, pages 13-16.Nianwen Xue.
2008.
Labeling Chinese Predicates withSemantic Roles.
Computational Linguistics, 34(2):225-255.Tao Zhuang, and Chengqing Zong.
2010.
A MinimumError Weighting Combination Strategy for Chinese Se-mantic Role Labeling.
In Proceedings of COLING-2010, pages 1362-1370.314
