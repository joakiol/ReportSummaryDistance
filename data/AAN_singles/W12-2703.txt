NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model?
On the Future of Language Modeling for HLT, pages 20?28,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsDeep Neural Network Language ModelsEbru Ar?soy, Tara N. Sainath, Brian Kingsbury, Bhuvana RamabhadranIBM T.J. Watson Research CenterYorktown Heights, NY, 10598, USA{earisoy, tsainath, bedk, bhuvana}@us.ibm.comAbstractIn recent years, neural network language mod-els (NNLMs) have shown success in bothpeplexity and word error rate (WER) com-pared to conventional n-gram language mod-els.
Most NNLMs are trained with one hid-den layer.
Deep neural networks (DNNs) withmore hidden layers have been shown to cap-ture higher-level discriminative informationabout input features, and thus produce betternetworks.
Motivated by the success of DNNsin acoustic modeling, we explore deep neuralnetwork language models (DNN LMs) in thispaper.
Results on a Wall Street Journal (WSJ)task demonstrate that DNN LMs offer im-provements over a single hidden layer NNLM.Furthermore, our preliminary results are com-petitive with a model M language model, con-sidered to be one of the current state-of-the-arttechniques for language modeling.1 IntroductionStatistical language models are used in many naturallanguage technologies, including automatic speechrecognition (ASR), machine translation, handwrit-ing recognition, and spelling correction, as a crucialcomponent for improving system performance.
Astatistical language model represents a probabilitydistribution over all possible word strings in a lan-guage.
In state-of-the-art ASR systems, n-grams arethe conventional language modeling approach dueto their simplicity and good modeling performance.One of the problems in n-gram language modelingis data sparseness.
Even with large training cor-pora, extremely small or zero probabilities can beassigned to many valid word sequences.
Therefore,smoothing techniques (Chen and Goodman, 1999)are applied to n-grams to reallocate probability massfrom observed n-grams to unobserved n-grams, pro-ducing better estimates for unseen data.Even with smoothing, the discrete nature of n-gram language models make generalization a chal-lenge.
What is lacking is a notion of word sim-ilarity, because words are treated as discrete enti-ties.
In contrast, the neural network language model(NNLM) (Bengio et al, 2003; Schwenk, 2007) em-beds words in a continuous space in which proba-bility estimation is performed using single hiddenlayer neural networks (feed-forward or recurrent).The expectation is that, with proper training of theword embedding, words that are semantically or gra-matically related will be mapped to similar loca-tions in the continuous space.
Because the prob-ability estimates are smooth functions of the con-tinuous word representations, a small change in thefeatures results in a small change in the probabil-ity estimation.
Therefore, the NNLM can achievebetter generalization for unseen n-grams.
Feed-forward NNLMs (Bengio et al, 2003; Schwenkand Gauvain, 2005; Schwenk, 2007) and recur-rent NNLMs (Mikolov et al, 2010; Mikolov et al,2011b) have been shown to yield both perplexity andword error rate (WER) improvements compared toconventional n-gram language models.
An alternatemethod of embedding words in a continuous spaceis through tied mixture language models (Sarikayaet al, 2009), where n-grams frequencies are mod-eled similar to acoustic features.To date, NNLMs have been trained with one hid-20den layer.
A deep neural network (DNN) with mul-tiple hidden layers can learn more higher-level, ab-stract representations of the input.
For example,when using neural networks to process a raw pixelrepresentation of an image, lower layers might de-tect different edges, middle layers detect more com-plex but local shapes, and higher layers identify ab-stract categories associated with sub-objects and ob-jects which are parts of the image (Bengio, 2007).Recently, with the improvement of computationalresources (i.e.
GPUs, mutli-core CPUs) and bettertraining strategies (Hinton et al, 2006), DNNs havedemonstrated improved performance compared toshallower networks across a variety of pattern recog-nition tasks in machine learning (Bengio, 2007;Dahl et al, 2010).In the acoustic modeling community, DNNshave proven to be competitive with the well-established Gaussian mixture model (GMM) acous-tic model.
(Mohamed et al, 2009; Seide et al, 2011;Sainath et al, 2012).
The depth of the network (thenumber of layers of nonlinearities that are composedto make the model) and the modeling a large numberof context-dependent states (Seide et al, 2011) arecrucial ingredients in making neural networks com-petitive with GMMs.The success of DNNs in acoustic modeling leadsus to explore DNNs for language modeling.
In thispaper we follow the feed-forward NNLM architec-ture given in (Bengio et al, 2003) and make the neu-ral network deeper by adding additional hidden lay-ers.
We call such models deep neural network lan-guage models (DNN LMs).
Our preliminary experi-ments suggest that deeper architectures have the po-tential to improve over single hidden layer NNLMs.This paper is organized as follows: The next sec-tion explains the architecture of the feed-forwardNNLM.
Section 3 explains the details of the baselineacoustic and language models and the set-up usedfor training DNN LMs.
Our preliminary results aregiven in Section 4.
Section 5 summarizes the relatedwork to our paper.
Finally, Section 6 concludes thepaper.2 Neural Network Language ModelsThis section describes a general framework for feed-forward NNLMs.
We will follow the notations given    !
"#"$Figure 1: Neural network language model architecture.in (Schwenk, 2007).Figure 1 shows the architecture of a neural net-work language model.
Each word in the vocabu-lary is represented by a N dimensional sparse vectorwhere only the index of that word is 1 and the restof the entries are 0.
The input to the network is theconcatenated discrete feature representations of n-1previous words (history), in other words the indicesof the history words.
Each word is mapped to itscontinuous space representation using linear projec-tions.
Basically discrete to continuous space map-ping is a look-up table with N x P entries where Nis the vocabulary size and P is the feature dimen-sion.
i?th row of the table corresponds to the contin-uous space feature representation of i?th word in thevocabulary.
The continuous feature vectors of thehistory words are concatenated and the projectionlayer is performed.
The hidden layer has H hiddenunits and it is followed by hyperbolic tangent non-linearity.
The output layer has N targets followedby the softmax function.
The output layer posteriorprobabilities, P (wj = i|hj), are the language modelprobabilities of each word in the output vocabularyfor a specific history, hj .Let?s c represents the linear activations in the pro-jection layer, M represents the weight matrix be-tween the projection and hidden layers and V rep-resents the weight matrix between the hidden andoutput layers, the operations in the neural network21are as follows:dj = tanh??
(n?1)?P?l=1Mjlcl + bj??
?j = 1, ?
?
?
,Hoi =H?j=1Vijdj + ki ?i = 1, ?
?
?
, Npi =exp(oi)?Nr=1 exp(or)= P (wj = i|hj)where bj and ki are the hidden and output layer bi-ases respectively.The computational complexity of this model isdominated by HxN multiplications at the outputlayer.
Therefore, a shortlist containing only the mostfrequent words in the vocabulary is used as the out-put targets to reduce output layer complexity.
SinceNNLM distribute the probability mass to only thetarget words, a background language model is usedfor smoothing.
Smoothing is performed as describedin (Schwenk, 2007).
Standard back-propagation al-gorithm is used to train the model.Note that NNLM architecture can also be con-sidered as a neural network with two hidden layers.The first one is a hidden layer with linear activationsand the second one is a hidden layer with nonlin-ear activations.
Through out the paper we refer thefirst layer the projection layer and the second layerthe hidden layer.
So the neural network architec-ture with a single hidden layer corresponds to theNNLM, and is also referred to as a single hiddenlayer NNLM to distinguish it from DNN LMs.Deep neural network architecture has several lay-ers of nonlinearities.
In DNN LM, we use the samearchitecture given in Figure 1 and make the networkdeeper by adding hidden layers followed by hyper-bolic tangent nonlinearities.3 Experimental Set-up3.1 Baseline ASR systemWhile investigating DNN LMs, we worked on theWSJ task used also in (Chen 2008) for model M lan-guage model.
This set-up is suitable for our initialexperiments since having a moderate size vocabu-lary minimizes the effect of using a shortlist at theoutput layer.
It also allows us to compare our pre-liminary results with the state-of-the-art performingmodel M language model.The language model training data consists of900K sentences (23.5M words) from 1993 WSJtext with verbalized punctuation from the CSR-IIIText corpus, and the vocabulary is the union of thetraining vocabulary and 20K-word closed test vo-cabulary from the first WSJ CSR corpus (Paul andBaker, 1992).
For speech recognition experiments,a 3-gram modified Kneser-Ney smoothed languagemodel is built from 900K sentences.
This modelis pruned to contain a total of 350K n-grams usingentropy-based pruning (Stolcke, 1998) .Acoustic models are trained on 50 hoursof Broadcast news data using IBM Attilatoolkit (Soltau et al, 2010).
We trained across-word quinphone system containing 2,176context-dependent states and a total of 50,336Gaussians.From the verbalized punctuation data from thetraining and test portions of the WSJ CSR corpus,we randomly select 2,439 unique utterances (46,888words) as our evaluation set.
From the remainingverbalized punctuation data, we select 977 utter-ances (18,279 words) as our development set.We generate lattices by decoding the develop-ment and test set utterances with the baseline acous-tic models and the pruned 3-gram language model.These lattices are rescored with an unpruned 4-gramlanguage model trained on the same data.
Afterrescoring, the baseline WER is obtained as 20.7%on the held-out set and 22.3% on the test set.3.2 DNN language model set-upDNN language models are trained on the baselinelanguage model training text (900K sentences).
Wechose the 10K most frequent words in the vocabu-lary as the output vocabulary.
10K words yields 96%coverage of the test set.
The event probabilities forwords outside the output vocabulary were smoothedas described in (Schwenk, 2007).
We used the un-pruned 4-gram language model as the backgroundlanguage model for smoothing.
The input vocabu-lary contains the 20K words used in baseline n-grammodel.
All DNN language models are 4-gram mod-els.
We experimented with different projection layersizes and numbers of hidden units, using the samenumber of units for each hidden layer.
We trainedDNN LMs up to 4 hidden layers.
Unless otherwisenoted, the DNN LMs are not pre-trained, i.e.
the22weights are initialized randomly, as previous workhas shown deeper networks have more impact on im-proved performance compared to pre-training (Seideet al, 2011).The cross-entropy loss function is used duringtraining, also referred to as fine-tuning or backprop-agation.
For each epoch, all training data is random-ized.
A set of 128 training instances, referred to asa mini-batch, is selected randomly without replace-ment and weight updates are made on this mini-batch.
After one pass through the training data, lossis measured on a held-out set of 66.4K words andthe learning rate is annealed (i.e.
reduced) by a fac-tor of 2 if the held-out loss has not improved suf-ficiently over the previous iteration.
Training stopsafter we have annealed the weights 5 times.
Thistraining recipe is similar to the recipe used in acous-tic modeling experiments (Sainath et al, 2012).To evaluate our language models in speech recog-nition, we use lattice rescoring.
The lattices gener-ated by the baseline acoustic and language modelsare rescored using 4-gram DNN language models.The acoustic weight for each model is chosen to op-timize word error rate on the development set.4 Experimental ResultsOur initial experiments are on a single hidden layerNNLM with 100 hidden units and 30 dimensionalfeatures.
We chose this configuration for our ini-tial experiments because this models trains in oneday of training on an 8-core CPU machine.
How-ever, the performance of this model on both theheld-out and test sets was worse than the baseline.We therefore increased the number of hidden unitsto 500, while keeping the 30-dimensional features.Training a single hidden layer NNLM with this con-figuration required approximately 3 days on an 8-core CPU machine.
Adding additional hidden lay-ers does not have as much an impact in the train-ing time as increased units in the output layer.
Thisis because the computational complexity of a DNNLM is dominated by the computation in the outputlayer.
However, increasing the number of hiddenunits does impact the training time.
We also experi-mented with different number of dimensions for thefeatures, namely 30, 60 and 120.
Note that thesemay not be the optimal model configurations for our1 2 3 41919.52020.7Number of hidden layersHeld?out setWER(%)4?gram LMDNN LM: h=500, d=30DNN LM: h=500, d=60DNN LM: h=500, d=120Figure 2: Held-out set WERs after rescoring ASR latticeswith 4-gram baseline language model and 4-gram DNNlanguage models containing up to 4 hidden layers.set-up.
Exploring several model configurations canbe very expensive for DNN LMs, we chose theseparameters arbitrarily based on our previous experi-ence with NNLMs.Figure 2 shows held-out WER as a function of thenumber of hidden layers for 4-gram DNN LMs withdifferent feature dimensions.
The same number ofhidden units is used for each layer.
WERs are ob-tained after rescoring ASR lattices with the DNNlanguage models only.
We did not interpolate DNNLMs with the 4-gram baseline language model whileexploring the effect of additional layers on DNNLMs.
The performance of the 4-gram baseline lan-guage model after rescoring (20.7%) is shown witha dashed line.
h denotes the number of hidden unitsfor each layer and d denotes the feature dimensionat the projection layer.
DNN LMs containing only asingle hidden layer corresponds to the NNLM.
Notethat increasing the dimension of the features im-proves NNLM performance.
The model with 30 di-mensional features has 20.3% WER, while increas-ing the feature dimension to 120 reduces the WER to19.6%.
Increasing the feature dimension also shiftsthe WER curves down for each model.
More im-portantly, Figure 2 shows that using deeper networkshelps to improve the performance.
The 4-layer DNNLM with 500 hidden units and 30 dimensional fea-tures (DNN LM: h = 500 and d = 30) reducesthe WER from 20.3% to 19.6%.
For a DNN LMwith 500 hidden units and 60 dimensional features(DNN LM: h = 500 and d = 60), the 3-layer modelyields the best performance and reduces the WERfrom 19.9% to 19.4%.
For DNN LM with 500 hid-23den units and 120 dimensional features (DNN LM:h = 500 and d = 120), the WER curve plateausafter the 3-layer model.
For this model the WERreduces from 19.6% to 19.2%.We evaluated models that performed best on theheld-out set on the test set, measuring both perplex-ity and WER.
The results are given in Table 1.
Notethat perplexity and WER for all the models were cal-culated using the model by itself, without interpolat-ing with a baseline n-gram language model.
DNNLMs have lower perplexities than their single hid-den layer counterparts.
The DNN language modelsfor each configuration yield 0.2-0.4% absolute im-provements in WER over NNLMs.
Our best resulton the test set is obtained with a 3-layer DNN LMwith 500 hidden units and 120 dimensional features.This model yields 0.4% absolute improvement inWER over the NNLM, and a total of 1.5% absoluteimprovement in WER over the baseline 4-gram lan-guage model.Table 1: Test set perplexity and WER.Models Perplexity WER(%)4-gram LM 114.4 22.3DNN LM: h=500, d=30with 1 layer (NNLM) 115.8 22.0with 4 layers 108.0 21.6DNN LM: h=500, d=60with 1 layer (NNLM) 109.3 21.5with 3 layers 105.0 21.3DNN LM: h=500, d=120with 1 layer (NNLM) 104.0 21.2with 3 layers 102.8 20.8Model M (Chen, 2008) 99.1 20.8RNN LM (h=200) 99.8 -RNN LM (h=500) 83.5 -Table 1 shows that DNN LMs yield gains on topof NNLM.
However, we need to compare deep net-works with shallow networks (i.e.
NNLM) with thesame number of parameters in order to concludethat DNN LM is better than NNLM.
Therefore, wetrained different NNLM architectures with varyingprojection and hidden layer dimensions.
All of thesemodels have roughly the same number of parameters(8M) as our best DNN LM model, 3-layer DNN LMwith 500 hidden units and 120 dimensional features.The comparison of these models is given in Table 2.The best WER is obtained with DNN LM, showingthat deep architectures help in language modeling.Table 2: Test set perplexity and WER.
The models have8M parameters.Models Perplexity WER(%)NNLM: h=740, d=30 114.5 21.9NNLM: h=680, d=60 108.3 21.3NNLM: h=500, d=140 103.8 21.2DNN LM: h=500, d=120with 3 layers 102.8 20.8We also compared our DNN LMs with a model MLM and a recurrent neural network LM (RNNLM)trained on the same data, considered to be cur-rent state-of-the-art techniques for language model-ing.
Model M is a class-based exponential languagemodel which has been shown to yield significant im-provements compared to conventional n-gram lan-guage models (Chen, 2008; Chen et al, 2009).
Be-cause we used the same set-up as (Chen, 2008),model M perplexity and WER are reported directlyin Table 1.
Both the 3-layer DNN language modeland model M achieve the same WER on the test set;however, the perplexity of model M is lower.The RNNLM is the most similar model to DNNLMs because the RNNLM can be considered to havea deeper architecture thanks to its recurrent connec-tions.
However, the RNNLM proposed in (Mikolovet al, 2010) has a different architecture at the in-put and output layers than our DNN LMs.
First,RNNLM does not have a projection layer.
DNNLM has N ?
P parameters in the look-up table anda weight matrix containing (n ?
1) ?
P ?
H pa-rameters between the projection and the first hid-den layers.
RNNLM has a weight matrix containing(N + H)?H parameters between the input and thehidden layers.
Second, RNNLM uses the full vo-cabulary (20K words) at the output layer, whereas,DNN LM uses a shortlist containing 10K words.
Be-cause of the number of output targets in RNNLM, itresults in more parameters even with the same num-ber of hidden units with DNN LM.
Note that the ad-ditional hidden layers in DNN LM will introduce ex-tra parameters.
However, these parameters will have24a little effect compared to 10, 000 ?
H additionalparameters introduced in RNNLM due to the use ofthe full vocabulary at the output layer.We only compared DNN and RNN languagemodels in terms of perplexity since we can not di-rectly use RNNLM in our lattice rescoring frame-work.
We trained two models using the RNNLMtoolkit1, one with 200 hidden units and one with500 hidden units.
In order to speed up training,we used 150 classes at the output layer as describedin (Mikolov et al, 2011b).
These models have 8Mand 21M parameters respectively.
RNNLM with200 hidden units has the same number of parameterswith our best DNN LM model, 3-layer DNN LMwith 500 hidden units and 120 dimensional features.The results are given in Table 1.
This model resultsin a lower perplexity than DNN LMs.
RNNLM with500 hidden units results in the best perplexity in Ta-ble 1 but it has much more parameters than DNNLMs.
Note that, RNNLM uses the full history andDNN LM uses only the 3-word context as the his-tory.
Therefore, increasing the n-gram context canhelp to improve the performance for DNN LMs.We also tested the performance of NNLMand DNN LM with 500 hidden units and 120-dimensional features after linearly interpolating withthe 4-gram baseline language model.
The interpola-tion weights were chosen to minimize the perplexityon the held-out set.
The results are given Table 3.After linear interpolation with the 4-gram baselinelanguage model, both the perplexity and WER im-prove for NNLM and DNN LM.
However, the gainwith 3-layer DNN LM on top of NNLM diminishes.Table 3: Test set perplexity and WER with the interpo-lated models.Models Perplexity WER(%)4-gram LM 114.4 22.34-gram + DNN LM:(h=500, d=120)with 1 layer (NNLM) 93.1 20.6with 3 layers 92.6 20.5One problem with deep neural networks, espe-cially those with more than 2 or 3 hidden lay-ers, is that training can easily get stuck in local1http://www.fit.vutbr.cz/?imikolov/rnnlm/minima, resulting in poor solutions.
Therefore,it may be important to apply pre-training (Hintonet al, 2006) instead of randomly initializing theweights.
In this paper we investigate discrimina-tive pre-training for DNN LMs.
Past work in acous-tic modeling has shown that performing discrimina-tive pre-training followed by fine-tuning allows forfewer iterations of fine-tuning and better model per-formance than generative pre-training followed byfine-tuning (Seide et al, 2011).In discriminative pre-training, a NNLM (one pro-jection layer, one hidden layer and one output layer)is trained using the cross-entropy criterion.
Af-ter one pass through the training data, the outputlayer weights are discarded and replaced by anotherrandomly initialized hidden layer and output layer.The initially trained projection and hidden layersare held constant, and discriminative pre-trainingis performed on the new hidden and output layers.This discriminative training is performed greedy andlayer-wise like generative pre-training.After pre-training the weights for each layer, weexplored two different training (fine-tuning) scenar-ios.
In the first one, we initialized all the lay-ers, including the output layer, with the pre-trainedweights.
In the second one, we initialized all thelayers, except the output layer, with the pre-trainedweights.
The output layer weights are initializedrandomly.
After initializing the weights for eachlayer, we applied our standard training recipe.Figure 3 and Figure 4 show the held-out WER asa function of the number of hidden layers for thecase of no pre-training and the two discriminativepre-training scenarios described above using modelswith 60- and 120-dimensional features.
In the fig-ures, pre-training 1 refers to the first scenario andpre-training 2 refers to the second scenario.
As seenin the figure, pre-training did not give consistentgains for models with different number of hiddenlayers.
We need to investigate discriminative pre-training and other pre-training strategies further forDNN LMs.5 Related WorkNNLM was first introduced in (Bengio et al, 2003)to deal with the challenges of n-gram language mod-els by learning the distributed representations of251 2 3 41919.52020.7Number of hidden layersHeld?out setWER(%)4?gram LMDNN LM: h=500, d=60DNN LM: h=500, d=60 (with disc.
pre?training 1)DNN LM: h=500, d=60 (with disc.
pre?training 2)Figure 3: Effect of discriminative pre-training for DNNLM: h=500, d=60.1 2 3 41919.52020.7Number of hidden layersHeld?out setWER(%)4?gram LMDNN LM: h=500, d=120DNN LM: h=500, d=120 (with disc.
pre?training 1)DNN LM: h=500, d=120 (with disc.
pre?training 2)Figure 4: Effect of discriminative pre-training for DNNLM: h=500, d=120.words together with the probability function of wordsequences.
This NNLM approach is extended tolarge vocabulary speech recognition in (Schwenkand Gauvain, 2005; Schwenk, 2007) with somespeed-up techniques for training and rescoring.Since the input structure of NNLM allows for usinglarger contexts with a little complexity, NNLM wasalso investigated in syntactic-based language mod-eling to efficiently use long distance syntactic infor-mation (Emami, 2006; Kuo et al, 2009).
Significantperplexity and WER improvements over smoothedn-gram language models were reported with theseefforts.Performance improvement of NNLMs comes atthe cost of model complexity.
Determining theoutput layer of NNLMs poses a challenge mainlyattributed to the computational complexity.
Us-ing a shortlist containing the most frequent severalthousands of words at the output layer was pro-posed (Schwenk, 2007), however, the number ofhidden units is still a restriction.
Hierarchical de-composition of conditional probabilities has beenproposed to speed-up NNLM training.
This decom-position is performed by partitioning output vocab-ulary words into classes or by structuring the outputlayer to multiple levels (Morin and Bengio, 2005;Mnih and Hinton, 2008; Son Le et al, 2011).
Theseapproaches provided significant speed-ups in train-ing and make the training of NNLM with full vo-cabularies computationally feasible.In the NNLM architecture proposed in (Bengioet al, 2003), a feed-forward neural network with asingle hidden layer was used to calculate the lan-guage model probabilities.
Recently, a recurrentneural network architecture was proposed for lan-guage modelling (Mikolov et al, 2010).
In con-trast to the fixed content in feed-forward NNLM, re-current connections allow the model to use arbitrar-ily long histories.
Using classes at the output layerwas also investigated for RNNLM to speed-up thetraining (Mikolov et al, 2011b).
It has been shownthat significant gains can be obtained on top of avery good state-of-the-art system after scaling upRNNLMs in terms of data and model sizes (Mikolovet al, 2011a).There has been increasing interest in using neu-ral networks also for acoustic modeling.
HiddenMarkov Models (HMMs), with state output distri-butions given by Gaussian Mixture Models (GMMs)have been the most popular methodology for acous-tic modeling in speech recognition for the past 30years.
Recently, deep neural networks (DNNs) (Hin-ton et al, 2006) have been explored as an alternativeto GMMs to model state output distributions.
DNNswere first explored on a small vocabulary phoneticrecognition task, showing a 5% relative improve-ment over a state-of-the-art GMM/HMM baselinesystem (Dahl et al, 2010).
Recently, DNNs havebeen extended to large vocabulary tasks, showing a10% relative improvement over a GMM/HMM sys-tem on an English Broadcast News task (Sainath etal., 2012), and a 25% relative improvement on a con-versational telephony task (Seide et al, 2011).As summarized, recent NNLM research has fo-cused on making NNLMs more efficient.
Inspiredby the success of acoustic modeling with DNNs,we applied deep neural network architectures to lan-guage modeling.
To our knowledge, DNNs have26not been investigated before for language modeling.RNNLMs are the closest to our work since recurrentconnections can be considered as a deep architecturewhere weights are shared across hidden layers.6 Conclusion and Future WorkIn this paper we investigated training language mod-els with deep neural networks.
We followed thefeed-forward neural network architecture and madethe network deeper with the addition of several lay-ers of nonlinearities.
Our preliminary experimentson WSJ data showed that deeper networks can alsobe useful for language modeling.
We also com-pared shallow networks with deep networks with thesame number of parameters.
The best WER wasobtained with DNN LM, showing that deep archi-tectures help in language modeling.
One impor-tant observation in our experiments is that perplex-ity and WER improvements are more pronouncedwith the increased projection layer dimension inNNLM than the increased number of hidden layersin DNN LM.
Therefore, it is important to investigatedeep architectures with larger projection layer di-mensions to see if deep architectures are still useful.We also investigated discriminative pre-training forDNN LMs, however, we do not see consistent gains.Different pre-training strategies, including genera-tive methods, need to be investigated for languagemodeling.Since language modeling with DNNs has not beeninvestigated before, there is no recipe for buildingDNN LMs.
Future work will focus on elaboratingtraining strategies for DNN LMs, including investi-gating deep architectures with different number ofhidden units and pre-training strategies specific forlanguage modeling.
Our results are preliminary butthey are encouraging for using DNNs in languagemodeling.Since RNNLM is the most similar in architectureto our DNN LMs, it is important to compare thesetwo models also in terms of WER.
For a fair com-parison, the models should have similar n-gram con-texts, suggesting a longer context for DNN LMs.The increased depth of the neural network typi-cally allows learning more patterns from the inputdata.
Therefore, deeper networks can allow for bet-ter modeling of longer contexts.The goal of this study was to analyze the behav-ior of DNN LMs.
After finding the right trainingrecipe for DNN LMs in WSJ task, we are going tocompare DNN LMs with other language modelingapproaches in a state-of-the-art ASR system wherethe language models are trained with larger amountsof data.
Training DNN LMs with larger amountsof data can be computationally expensive, however,classing the output layer as described in (Mikolov etal., 2011b; Son Le et al, 2011) may help to speedup training.ReferencesYoshua Bengio, Rejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Research,3:1137?1155.Yoshua Bengio.
2007.
Learning Deep Architectures forAI.
Technical report, Universit e de Montreal.S.
F. Chen and J. Goodman.
1999.
An empirical study ofsmoothing techniques for language modeling.
Com-puter Speech and Language, 13(4).Stanley F. Chen, Lidia Mangu, Bhuvana Ramabhadran,Ruhi Sarikaya, and Abhinav Sethy.
2009.
Scalingshrinkage-based language models.
In Proc.
ASRU2009, pages 299?304, Merano, Italy, December.Stanley F. Chen.
2008.
Performance prediction for expo-nential language models.
Technical Report RC 24671,IBM Research Division.George E. Dahl, Marc?Aurelio Ranzato, Abdel rah-man Mohamed, and Geoffrey E. Hinton.
2010.Phone Recognition with the Mean-Covariance Re-stricted Boltzmann Machine.
In Proc.
NIPS.Ahmad Emami.
2006.
A neural syntactic languagemodel.
Ph.D. thesis, Johns Hopkins University, Bal-timore, MD, USA.Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh.2006.
A Fast Learning Algorithm for Deep BeliefNets.
Neural Computation, 18:1527?1554.H-K. J. Kuo, L. Mangu, A. Emami, I. Zitouni, and Y-S. Lee.
2009.
Syntactic features for Arabic speechrecognition.
In Proc.
ASRU 2009, pages 327 ?
332,Merano, Italy.Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cer-nocky, and Sanjeev Khudanpur.
2010.
Recurrent neu-ral network based language model.
In Proc.
INTER-SPEECH 2010, pages 1045?1048.Tomas Mikolov, Anoop Deoras, Daniel Povey, LukasBurget, and Jan Cernocky.
2011a.
Strategies for train-ing large scale neural network language models.
InProc.
ASRU 2011, pages 196?201.27Tomas Mikolov, Stefan Kombrink, Lukas Burget, JanCernocky, and Sanjeev Khudanpur.
2011b.
Exten-sions of recurrent neural network language model.
InProc.
ICASSP 2011, pages 5528?5531.Andriy Mnih and Geoffrey Hinton.
2008.
A scalable hi-erarchical distributed language model.
In Proc.
NIPS.Abdel-rahman Mohamed, George E. Dahl, and GeoffreyHinton.
2009.
Deep belief networks for phone recog-nition.
In Proc.
NIPS Workshop on Deep Learning forSpeech Recognition and Related Applications.Frederic Morin and Yoshua Bengio.
2005.
Hierarchicalprobabilistic neural network language model.
In Proc.AISTATS05, pages 246?252.Douglas B. Paul and Janet M. Baker.
1992.
The de-sign for the wall street journal-based csr corpus.
InProc.
DARPA Speech and Natural Language Work-shop, page 357362.Tara N. Sainath, Brian Kingsbury, and Bhuvana Ramab-hadran.
2012.
Improvements in Using Deep BeliefNetworks for Large Vocabulary Continuous SpeechRecognition.
Technical report, IBM, Speech and Lan-guage Algorithms Group.Ruhi Sarikaya, Mohamed Afify, and Brian Kingsbury.2009.
Tied-mixture language modeling in continuousspace.
In HLT-NAACL, pages 459?467.Holger Schwenk and Jean-Luc Gauvain.
2005.
Trainingneural network language models on very large corpora.In Proc.
HLT-EMNLP 2005, pages 201?208.Holger Schwenk.
2007.
Continuous space languagemodels.
Comput.
Speech Lang., 21(3):492?518, July.Frank Seide, Gang Li, Xie Chen, and Dong Yu.
2011.Feature Engineering in Context-Dependent Deep Neu-ral Networks for Conversational Speech Transcription.In Proc.
ASRU.Hagen Soltau, George.
Saon, and Brian Kingsbury.
2010.The IBM Attila speech recognition toolkit.
In Proc.IEEE Workshop on Spoken Language Technology,pages 97?102.Hai Son Le, Ilya Oparin, Alexandre Allauzen, Jean-LucGauvain, and Francois Yvon.
2011.
Structured out-put layer neural network language model.
In Pro-ceedings of IEEE International Conference on Acous-tic, Speech and Signal Processing, pages 5524?5527,Prague, Czech Republic.Andreas Stolcke.
1998.
Entropy-based pruning ofbackoff language models.
In Proceedings of DARPABroadcast News Transcription and UnderstandingWorkshop, pages 270 ?
274, Lansdowne, VA, USA.28
