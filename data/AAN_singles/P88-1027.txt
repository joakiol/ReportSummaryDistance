PARSING VS.
TEXT PROCESSINGIN THE ANALYSIS OF DICTIONARY DEFINITIONSThomas Ahlswede and Martha EvensComputer Science Dept.Illinois Institute of TechnologyChicago, 11.
60616312-567-5153ABSTRACTWe have analyzed efinitions from Webster'sSeventh New Collegiate Dictionary using Sager'sLinguistic String Parser and again using basic UNIXtext processing utilities uch as grep and awk.
Tiffspaper evaluates both procedures, compares theirresults, and discusses possible future lines of researchexploiting and combining their respective strengths.IntroductionAs natural language systems grow moresophisticated, they need larger and more d~ledlexicons.
Efforts to automate the process ofgenerating lexicons have been going on for years,and have often been combined with the analysis ofmachine-readable dictionaries.Since 1979, a group at HT under theleadership of Manha Evens has been using themachine-readable version of Webster' s Seventh NewCollegiate Dictionary (W7) in text generation,information retrieval, and the theory of lexical-semantic relations.
This paper describes some of ourrecent work in extracting semantic information fromWT, primarily in the form of word pairs linked bylexical-semantic relations.
We have used twomethods: parsing definitions with Sager's LinguisticString Parser (LSP) and text processing with acombination ofUNIX utilities and interactive editing.We will use the terms "parsing" and "textprocessing" here primarily with reference to our ownuse of the LSP and UNIX utilities respectively, butwill also use them more broadly.
"Parsing" in thismore general sense will mean a computationaltechnique of text analysis drawing on an extensivedatabase of linguistic knowledge, e.g., the lexicon,syntax and/or semantics of English; "text processing"will refer to any computational technique thatinvolves little or no such knowledge.This research is supported by National ScienceFoundation grant IST 87-03580.
Our thanks also tothe G & C Merriam Company for permission to usethe dictionary tapes.Our model of the lexicon emphasizes lexicaland semantic relations between words.
Some ofthese relationships axe fan~iliar.
Anyone who hasused a dictionary or thesaurus has encounteredsynonymy, and perhaps also antonymy.
W7 aboundsin synonyms (the capitalized words in the examplesbelow):(1) funny 1 la aj affording light mirth andlaughter : AMUSING(2) funny 1 lb aj seeking or intended to amuse: FACETIOUSOur notation for dictionary definitions consists of: (1)the entry (word or phrase being defined); (2) thehomograph number (multiple homographs are givensepmaw entries in W7); (3) the sense number, whichmay include a subsense letter and even a sub-subseuse number (e.g.
263); (4) the text of thedefinition.We commonly express a relation betweenwords through atriple consisting of Wordl, Relation,Word2:(3) funny SYN amusing(4) funny SYN facetiousA third relation, particularly important in W7and in dictionaries generally, is taxonomy, thespecies-genus relation or (in artificial intelligence)the IS-A relation.
Consider the entries:(5) dodecahedron 0 0 n a solid having 12 planefaces(6) build 1 1 vt to form by ordering and unitingmaterials...These definitions yield the taxonomy Iriples(7) dodecahedron TAX solid(8) build TAX formTaxonomy is not explicit in definitions, as issynonymy, but is implied in their very structure.Some other elations have been frequently observed,e.g.
:(9) driveshaft PART engine(10) wood COMES-FROM tree217The usefulness of relations in informationretrieval is demonstrated in Wang et al \[1985\] aswell as in Fox \[1980\].
Relations are also important ingiving coherence to text, as shown by Halliday andHasan \[1977\].
They are abundant in a typicalEnglish language dictionary, us we will see later.We have recognized, however, that word-relation-word triples are not adequate, or at least notoptimal, for expressing all the useful informationassociated with words.
Some information is bestexpressed us unary attributes or feauLres.
We havealso recognized that phrases and even largerstructures may on one hand be in some waysequivalent to single words, as pointed out by Becker\[1975\], or may on the other hand express complexfacts that cannot be reduced to any combination ofword-to- word links.ParsingRecognizing the vastness of the task ofparsing a whole dictionary, most computationallexicologists have preferred approaches lesscomp,,t~tionally intensive and more specificallysuited to their immediate goals.
A partial exceptionis Amsler \[1980\], who proposed a simple ATNgrammar for some definitions in the Merriam.Webster Pocket D/ctionary.
More recently, Jensenand her coworkers at IBM have also parseddefinitions.
But the record shows that dictionaryresearchers have avoided parsing.
One of ourquestions was, how justified is this avoidance?
Howmuch harder is parsing, and what rewards, ff any,will the effort yield7We used Sager's Linguistic String Parser, aswe have clone for several years.
It has beencontinuously developed since the 1970s and by nowhas a very extensive and powerful user interface uswell as a large English grammar and a vocabulary(the LSP Dictionary) of over 10,000 words.
It is notexceptionally fast - -  a fact which should be takeninto account in evaluating the performance ofparsersgenerally in dictionary analysis.Our efforts to parse W7 definitions beganwith simple LSP grammars for small sets of adjective\[Ahlswede, 1985\] and adverb \[Klick, 1981\]definitions.
These led evenm, lly to a large grammarof noun, verb and adjective definitions \[Ahlswede,1988\], based on the Linguistic Siring Project's fullEnglish grammar \[Sager, 1981\], and using the LSP'sfull set of resources, including restrictions,transformations, and special output generationroutines.
All of these grammars have been used notonly to create parse trees but also (and primarily) togenerate relational triples linking defined words tothe major words used in their definitions.The large definition grammar is describedmore fully in Ahlswede \[1988\].
We are concernedhere with its performance: its success in parsingdefinitions with a minimum of incorrect orimprobable parses, its success in identifyingrelational triples, and its speed.Input to the parser was a set of 8,832definition texts from the machine-readable WT,chosen because their vocabulary permitted them to beparsed without enlarging the LSP's vocab-I~ry.For parsing, the 8,832-definition subset wassorted by part of speech and broken into 100-definition blocks of nouns, transitive verbs,imransitive verbs, and adjectives.
Limiting theselection to nouns, verbs and adjectives reduced thesubset to 8,211, including 2,949 nouns, 1,451adjectives, 1,272 intransitive verbs, and 2,549transitive verbs.We were able to speed up the parsing processconsiderably by automatically extractingsubvocabularies from the LSP vocabulary, so thatfor a IO0-definition input sample, for inslance, theparser would only have to search tln'ough about 300words instead of I0,000.Parsing the subset eventually required a littleunder 180 hours of CPU time on two machines, aVax 8300 and a Vax 750.
Total clock time required "was very little more than this, however, since almostall the parsing was done at night when the systemswere otherwise idle.
Table 1 compares the LSP'sperformance in the four part of speech categories.Part ofspeech ofdefd.
wordnounsadjectivesinL verbs~'.
verbsaverageTablePet.
of Avg.
no.
Time (see.)
Triplesclefs, of parses per parse generatedparsed per success per success77.63 1.70 11.05 11.4668.15 1.85 10.59 5.4564.62 1.59 11.96 6.6260.29 1.50 43.33 9.1568.65 1.66 18.89 9.06I.
Performance time and parsingefficiency of LSP by part of speech of words defined(adapted from Fox et ul., 1988)In most cases, there is little variation amongthe parts of speech.
The most obvious discrepancy isthe slow parsing time for wansifive verbs.
We are notyet sure why this is, but we suspect i  has to do withW7"s practice of representing the defined verb'sdirect object by an empty slot in the definition:(11) madden 0 2 vt to make intensely angry218(12) magnetize 0 2 vt to communicate magneticproperties toThe total number of triples generated was51,115 and the number of unique triples was 25,178.The most common triples were 5,086 taxonomles and7,971 modification relations.
(Modification involvedany word or phrase in the definition that modified theheadword; thus a definition such as "cube: a regularsolid .
.
. "
would yield the modification triple (cubeMOD regular)).We also identified 125 other relations, in threecategories: (1) "traditional" relmions, identified byprevious researchers, which we hope to associatewith axioms for making inferences; (2) syntacticrelations between the defined word and variousdefining words, such as (in a verb definition) thedirect object of the head verb, which we willinvestigate for possible consistent semanticsignificance; and (3) syntactic relations within thebody of the definition, such as modifier-head, verb-object, etc, The relations in this last category werebuilt into our grammar;, we were simply collectings_t~_ti$~ics on their occurrence, which we hopeeven.rally to test for the existence of dictionary-specific selectional categories above and beyond thegeneral English selectional categories already presentin the LSP grammar.Figure 1 shows a sample definition and thetriples the parser found in it.ABDOMEN 0 1 N THE PART OF THE BODYBETWEEN THE THORAX AND THEPELVIS(THE) pmod (PART)(ABDOMEN 0 1 N) lm (THE)(ABDOMEN 0 1 N) t (PART)(ABDOMEN 0 1 N) rm (OF THE BODY BETWEENTHE THORAX AND THE PELVIS)(THE) pmod (BODY)(THE) pmod (PELVIS)(THE) pmod (THORAX)(BETWEEN) pobj (THORAX)(BETWEEN) pobj (PELVIS)(ABDOMEN 0 1 N) part (BODY)Figure 1.
A definition and its relational triplesIn this definition, "part" is a typical category1 relation, recognized by virtually all students ofrelations, though they may disagree about its exactnature.
"Ira" and "rm" are left and rightmodification.
As can be seen, "rm" does not involveanalysis of the long posmominal modifier phrase.
"pmod" and "pobj" are permissible modifier andpermissible object, respectively; these are among themost common category 3 relations.We began with a list of about fifty relations,intending to generate plain parse trees and thenexamine them for relational triples in a separate step.It soon became clear, however, that the LSP itselfwas the best tool available for extracting informationfrom parse trees, especially its own parse trees.Therefore we added a section to the grammarconsisting of routines for identifying relations andprinting out triples.
The LSP's Restriction Languagepermitted us to keep this section physically separatefrom the rest of the grammar and thus to treat it as anindependent piece of code.
Having done this, wewere able to add new relations in the com~e ofdeveloping the grammar.Approximately a third of the definitions in thesample could not be parsed with this grammar.During development of the grammar, we uncovered agreat many reasons why definitions failed to parse;there remains no one fix which will add more than afew definitions to the success list.
However, somegeneral problem areas can be identified.One common cause of failure is the inabilityof the grammar to deal with all the nuances ofadjective comparison:(13) accelerate 0 1 vt to bring about at an earlierpoint of timeIdiomatic ,~es  of common words are a frequentsource of failure:(14) accommodnto.
0 3c vt to make room forThere are some errors in the input, for example aninlransitive verb definition labeled as transitive:(15) ache 1 2 vt to become fi l l~ with painfulyearningAs column 3 of Table 1 indicates, manydefinitions yielded multiple parses.
Multiple parseswere responsible for most of the duplicate relationaltriples.Finding relational triples by text processingAs the performance statistics above show,parsing is painfully slow.
For the simple business offinding and writing relational triples, it turns out to bemuch less efficient than a combination of textprocessing with interactive editing.We first used straight text processing toidentify synonym references indefinitions and reducethem to triples.
Our next essay in the textprocessing/editing method began as a casualexperiment.
We extracted the set of intransitive verbdefinitions, uspecting that these would be the easiestto work with.
This is the smallest of the four major219W7 part of speech categories (the others being nouns,adjectives, and Iransitive verbs) with 8,883 texts.Virtually all verb definition texts begin withto followed by a head verb, or a set of conjoined headverbs.
The most common words in the secondposition in inwansitive verb definitions, along withtheir typical complements, were:become + noun or adj.
phrase(774 occurrences in 8,482 definitions)mate + noun phrase \[+ adj.
phrase\](526 occurrences)be + various(408 occurrences)mow + adverbial phrase(388 occurrences)Definitions in become, make and move hadsuch consistent forms that the core word or words inthe object or complement phrase were easy toidentify.
Occasional prepositional phrases or otherposmominal constructions were easy to edit out byhand.
From these, and from some definitions in serveas, we were able to generate triples representing fiverelations.
(16) age 2 2b vi to become mellow or mature(17) (age 2 2b vi) va-incep (mature)(18) (age 2 2b vi) va-incep (mellow)(19) add 0 2b vi to make an addition(20) (add 0 2b vi) vn-canse (addition)(21) accelerate 0 I vi to move faster(22) (accelerate 0 1 vi) move (faster)(23) add 0 2a vi to serve as an addition(24) (add 0 2a vi) vn-be (addition)(25) annotate 0 0 vi to make or furnish critical orexplanatory notes(26) (annotate 0 0 vi) va-cause (critical)(27) (annotate 0 0 vi) va-cause (explanatory)We also al~empted to generate taxonomictriples for inwansitive verbs.
In verb definitions, weidentified conjoined headwords, and otherwisedeleted everything to the right of the last headword.This was straightforward and gave us almost 1O,000triples.These triples are of mixed quality, however.Those representing very common headwords such asbe or become are vacuous; worse, our lexically dumbalgorithm could not recognize phrasal verbs, so that aphrasal head term such as take place appears as astake, with misleading results.The vacuous triples can easily be removedfrom the total, however, and the incorrect riplesresulting from broken phrasal head terms arerelatively few.
We therefore felt we had been highlysuccessful, and were inspired to proceed with nouns.As with verbs, we are primarily interested inrelationsother than taxonomy, and these are most commonlyfound in the often lengthy postoheadword part of thedefinitions.The problems we encountered with nounswere generally the same as with inlransitive verbs,but accentuated by the much larger number (80,022)of noun definition texts.
Also, as Chodorow et al\[1985\] .have noted, the boundary between theheadword and the postnominal part of the definitionis much harder to identify in noun definitions than inverb definitions.
Our first algorithm, which had nolexical knowledge xcept of prepositions, was about88% correct in finding the boundary.In order to get better esults, we needed analgorithm comparable to Chodorow's Head Finder,which uses part of speech information.
Our strategyis first to tag each word in each definition with all itspossible parts of s ix ,h ,  then to step through thedefinitions, using Chodorow's heuristics (plus anyothers we can find or invent) to mark prenonn-nounand nunn-posmoun boundaries.The first step in tagging is to generate atagged vocabulary.
We nsed an awk program to stepthrough the entries and nm-ons, appending to eachone its part or parts of speech.
(A run-on is asubentry, giving information about a word or phrasederived from the entry word or phrase; for instance,the verb run has the run-ons run across, run ~fter,and run a temperature among others; the noun runehas the run-on adjective runic.)
Archaic, obsolete, ordialect forms were marked as such by W7 and couldbe excluded.Turning to W7's defining vocabulary, thewords (and/or phrases) actually employed indefinitions, we used Mayer's morphological nalyzer\[1988\] to identify regular noun plurals, adjectivecomparatives and superlatives, and verb tense forms.Following suggestions by Peterson \[1982\], weassumed that words ending in -/a and -ae (virt~mllyall appearing in scientific names) were nouns.We then added to our tagged vocabularythose irregular noun plurals and verb tense formsexpressly given in W7.
Unforumately, neither W7nor Mayer's program provides for derivedcompounds with irregular plurals; for instance, W7indicates men as the plural of man but there are over300 nouns ending in -man for which no plural isshown.
Most of these (e.g., salesman, trencherman)take plurals in -men but others (German, shaman) donot.
These had to be identified by hand.
Another220group of nouns, whose plurals we found convenientrather than absolutely necessary to treat by hand, isthe 200 or so ending in -ch.
(Those with a hard -ch(patriarch, loch) take plurals in -chs; the rest takeplurals in -ches.)
We could have exploited W7'spronunciation i formation to distinguish these, butthe work would have been well out of proportion tothe scale of the task.After some more of this kind of work, we hada tagged vocabulary of 46,566 words used in W7definitions.
For the next step, we chose to generatetagged blocks of definitions (rather than performtagging on the fly).
We wrote a C program to read atext file and replac~ each word with its taggedcounterpart.
(We are not yet attempting todeal withphrases.
)Head finding on noun definitions was donewith an awk program which examines consecutivepairs of words (working from right to left) and marksprenoun-noun and nonn-posmoun boundaries.
Itrecognizes certain kinds of word sequences asbeyond its ability to disambiguate, e.g.
:(28) alarm 1 2a n a \[ signal }?
warning } ofdanger(29) aitatus 0 0 n a { divine }7 imparting } ofknowledge or powerThe result of all this effort is a rudimentaryparsing system, in which the tagged vocabulary is thelexicon, the tagging program is the lexical analyzer,and the head finder is a syntax analyzer using a verysimple finite state grammar of about ten rules.Despite its lack of linguistic sophistication, this is aclear step in the direction of parsing.And the effort seems to be justified.Development took about four weeks, most of it spenton the lexicon.
(And, to be sure, mote work is stillneeded.)
This is more than we expected, butconsiderably less than the eight man-months spentdeveloping and testing the LSP definition grammar.Tagging and head finding were performed ona sample of 2157 noun definition texts, covering thenouns from a through anode.
170 were flagged asambiguous; of the remaining 1987, all but 58 werecorrect for a success rate of 97.1 percent.In 37 of the 58 failures, the head findermistakenly identified a noun (or polysemousadjective/noun) modifying the head as anindependent oun:(30) agiotage 0 1 n ( exchange } business(3 I) alpha 1 3 n the { chief ) or brightest star ofa constellationThere were 5 cases of misidenfification of afollowing adjective (parsable as a noun) as the headnoun:(32) air mile 0 0 n a unit { equal } to 6076.1154feetThe remaining failures resulted from errors in thecreation of the tagged vocabulary (5), non-definitiendictionary lines incorrectly labeled as definition texts(53, and non-noun definitions inconecfly labeled asnoun definitions (6).
The last two categories arosefrom errors in our original W7 tape.Among the 170 definitions flagged asambiguous, there were two mislabeled efinitionsand one vocabulary en~r.
There were 128 cases ofnoun followed by an -/n& form; in 116 of these the-/ng form was a participle, otherwise it was the headnoun.
(The other case flagged as ambiguous was of apossible head followed by a preposition also parsableas an adjective.
This flag turned out to beunnecessary.)
There were also seven instances ofmiscellaneous misidentification f a modifying nounas the head.
Thus the "success rate" among thesedefinitions was 148/170 or 87.1 percent.We are still working on improving the headfinder, as well as developing similar "grammars" forposmominal phrases and for the major phrasestr~tures of other definition types.
In the course ofthis work we expect o solve the major "problem inthis parficnl~ grammar, that of prenominal modifiersidentified as heads.Parsing, againSimple text processing, even without suchlexical knowledge as parts of speech, is about asaccurate as parsing in terms of correct vs. incorrectrelational triples identified.
(It should be noted thatboth methods require hand checking of the output,and it seems unlikely that we will ever completelyeliminate this step.)
The text processing strategy canbe applied to the entire corpus of definitions, withoutthe labor of enlarging a parser lexicon such as theLSP Dictionary.
And it is much faster.This way of looking at our results may makeit appear that parsing was a waste of time and effort,of value only as a lesson in how not to go aboutdictionary analysis.
Before coming to any suchconclusion, however, we should consider some otherfactors.It has been suggested that a more "modem"parser than the LSP could give much faster parsingtimes.
At least part of the slowness of the LSP is dueto the completeness of its associated Englishgrammar, perhaps the most detailed grammarassociated with any natural anguage parser.
Thus a221probable tradcoff for greater speed would be a lowerpercentage ofdefinitions successfully parsed.Nonetheless, it appears that the immediatefuture of parsing in the analysis of dictionarydefinitions or of any other large text corpus lies in asimpler, less computationally intensive parsingtechnique.
In addition, a parser for definitionanalysis needs to be able to return partial parses ofdifficult definitions.
As we have seen, even theLSP's detailed grammar failed to parse about a thirdof the definitions it was given.
A partial parsecapability would facilitate the use of simplergrammars.For further work with the machine-~Jul~bleW7, another valuable feature would be the ability tohandle ill-formed input.
This is perhaps tartling,since a dictionary is supposed to be the epitome ofwellftxmedness, by definition as it were.
However,Peterson \[1982\] counted 903 typographical andspelling en~rs in the machine-readable W7(including ten errors carried over from the printedWT), and my experience suggests that his count wasconservative.
Such errors are probably little or noproblem in more recent MRDs, which are used astypesetter input and are therefore xacdy as correctas the printed dictionary; exrots creep into thesedictionaries in other places, as Boguraev \[1988\]discovered in his study of the grammar codes in theLongman Dictionary of Contemporary English.Before choosing or designing the best parserfor the m~k, it is worthwhile to define an appropriatetask: to determine what sort of information one canget from parsing that is impossible or impractical toget by easier means.One obvious approach is to use parsing as abackup.
For instance, one category of definitiuns thathas steadfastly resisted our text processing analysis isthat of verb definitions whose headword is a verbplus separable particle, e.g.
give up.
A textprocessing program using part-of-sgw.~h taggedinput can, however, flag these and other troublesomedefinitions for further analysis.It still seems, though, that we should be ableto use parsing more ambitiously than this.
It isintrinsically more powerful; the techniques we referto here as "text processing" mostly only extractsingle, stereotyped fragments of information.
Themost powerful of them, the head finder, still performsonly one simple grammatical operation: finding thenuclei of noun phrases.
In conwast, a "real" parsergenerates a parse tree containing a wealth ofstructural and relational information that cannot beadequately represented by a fcenn~li~m such asword-relation-word t iples, feature lists, etc.Only in the simplest definitions does ourpresent set of relations give us a complete analysis.In most definitions, we are forced to throw awayessential information.
The definition(33) dodecahedron 0 0 n a solid having 12 planefacesgives us two relational triples:(34) (dodecahedron 0 0 n) t (solid)(35) (dodecahedron 0 0 n) nn-aUr (face)The first triple is straightforward.
The second tripletells us that the noun dodecahedron has the (noun)auribute face, i.e.
that a dodecahedron has faces.But the relational triple structme, by itself, cannotcapture the information that the dodecahedron hasspecifically 12 faces.
We could add another triple(36) (face) nn-atlr (12)i.e., saying that faces have the anribute of (acardinality of) 12, but this Iriple is correct only in thecontext of the definition of a dodecahedron.
It is notpermanendy orgenerically true, as are (28) and (29).The information is present, however, in theparse Iree we get from the LSP.
It can be madesomewhat more accessible by putting it into adependency form such as(37) (soild (a) (having (face (plural) (12)(plane))))which indicates not only that face is an attribute ofthat solid which is a dodecahedron, but that the~ t y  12 is an attribute of face in this particularcase, as is also plane.In order to be really useful, a structure such asthis must have conjunctionphrases expanded,passives inverted, inflected forms analyzed, and othermodifications of the kind often brought under therubric of "transformations."
The LSP can do this sortof thing very welL The defining words also need tobe disambiguated.
We do not hope for any fullyautomatic way to do this, but co-?r.currence ofdefining words, perhaps weighted according to theirposition in the dependency slructure, would reducethe human di~mbiguator's task to one of post-editing.
This might perhaps be further simplified bya customized interactive editing facility.We do not need to set up an elaboratenetwork data structure, though; the Lisp-like treestructure, once it is transformed and its elementsdisambiguated, constitutes a set of implicit pointersto the definitions of the various words.Even with all this work done, however, a biggap remains between words and ideal semantic222concepts.
Let us consider the ways in which W7 hasdefined all five basic polyhedrons:(38) dodecahedron 0 0 n a solid having 12 planefaces(39) cube 1 1 n the regular solid of six equalsquare sides(40) icosahedmn 0 0 n a polyhedron having 20faces(41) octahedron 0 0 n a solid bounded by eightplane faces(42) tetrahedron 0 0 n a polyhedron of four faces(43) polyhedron 0 0 n a solid formed by planefacesThe five polyhedrons differ only in theirnumber of faces, apart from the cube's additionalattribute of being regular.
There is no reason why asingle syntactic/semantic s ructure could not be usedto define all five polyhedrons.
Despite this, no two ofthe definitions have the same structure.
Thesedefinitions illaslrate that, even though W7 is fairlystereotyped in its language, it is not nearly asstereotyped as it needs to be for large scale,automatic semantic analysis.
We are going to need agreat deal of sophistication i  synonymy and movingaround the taxonomic hierarchy.
(It is worthrepeating, however, that in building our lexicon, wehave no intention of relying exclusively on theinformation contained in W7).Figure 2 shows a small part of a possiblenetwork.
In this sample, the definitions have beenparsed into a Lisp-like dependency slructure, withsome wansformations such as inversion of passives,but no attempt to fit the polyhedron definitions into asingle semantic format.
(cube 1 1) T (solid 3 1 (the) (regular)(of (side 1 6b (PL) (six)?
(equal) (square}) ) )(dodecahedron 0 0) T (solid 3 1 (a)(have (OBJ (face 1 5a5 (PL)(12) (plane)))))( icosahedron 0 0) T (polyhedron (a)(have (OBJ (face 1 5a5 (PL)(20) )  ) ) )(octahedron 0 O) T (solid 3 1 (a)(bound (SUBJ (face 1 5a5 (PL)(eight) (plane)) ) ) )(tetrahedron 0 0) T (polyhedron (a) (of(face 1 5a5 (PL) (four)) ) )(polyhedron 0 0) T (solid 3 1 (a) (form(SUBJ (face 1 5a5 (PL)(plane)) ) ) )(solid 3 1) T (figure (a) (geometrical)(have (OBJ (dimension- (PL)(three)) ) ) )(face 1 5a5) T (surface 1 2 (plane)(bound (OBJ (solid 3 1 (a)(geometric)) ) ) )(side 1 6a) T (line (a) (bound (OBJ(NULL)) ) (of (figure (a)(geometrical)) ) )(side 1 6b) T (surface 1 2 (delimit(OBJ (solid (a)))))(surface 1 2) T (locus (a) (or (plane)(curved)) (two-dimensional)(of (point (PL)) .
.
.
))Figure 2.
Part of a "network" of parsed efinitionsIf this formalism does not look much like anetwork, imagine each word in each definition (thepart of the node to the right of the taxonomy marker'W") serving as a pointer to its own defining node.The resulting network is quite dense.
We simplify byleaving out other parts of the lexical entry, and byincluding only a few disambignations, just to give theflavor of their presence.
Disambignation fa word isindicated by the inclusion of its homograph and sensenumbers (see examples 1and 2, above).SummaryIn the process of developing techniques ofdictionary analysis, we have learned a variety oflessons.
In particular, we have learned (as manydictionary researchers had suspected but none hadattempted to establish) that full namral-langnageparsing is not an efficient procedure for gatheringlexical information in a simple form such asrelational Iriples.
This realization stimulated us to dotwo things.F'n~'t, we needed to develop faster and morereliable techniques for extracting triples.
We foundthat many Iriples could be found using UNIX textprocessing utilities combined with the recognition ofa few structural patterns in definitions..Theseprocedures are subject to further development andrefinement, but have already yielded thousands oftriples.Second, we were inspired to look for a formof data representation that would allow our lexicald-tabase to exploit he power of full natural-languageparsing more effectively than it can through triples.We are now in the early stages of investigating sucha representation.REFERENCESAhlswede, Thomas E., 1985.
"A Linguistic StringGrammar for Adjective Definitions."
In S.Williams, ed., Humans and Machines: theInterface through Language.
Ablex,Norwood, NJ, pp.
101-127.Ahlswede, Thomas E., 1988.
"Syntactic and223Semantic Analysis of Definitions in aMachine-Readable Dictionary."
Ph.D. Thesis,Illinois Institute of Technology.Amsler, Robert A., 1980.
"The Structure of TheMerriam-Webster Pocket Dictionary."
Ph.D.Dissertation, Computer Science.
University ofTexas, Austin.Amsler, Robert A., 1981.
"A Taxonomy for EnglishNouns and Verbs."
Proceedings of the 19thAnnual Meeting of the ACL, pp.
133-138.Apresyan, Yu.
D., I.
A. Mel'~uk and A. IC~olkovsky, 1970.
"Semantics andLexicography:.
Towards a New Type ofUnilingual Dictionary."
In Kiefer, F., exl.Studies in Syntax.
Reidel, Dordrecht, Holland,pp.
1-33.Becker, Joseph D., 1975.
"The Phrasal I..~xicon."
InSchank, R. C. and B. Nash-Webber, eds.,Theoretical Issues in Natural LanguageProcessing, ACL Annual Meeting,Cambridge, MA, June, 1975, pp.
38-41.Boguraev, Branimir, 1987.
"Experiences with aMachine-Re~'~d~ble Dictionary."
Proceedingsof the Third Annual Conference of the UWCentre for the New OF_D, University ofWaterloo, Waterloo, Ontario, November1987, pp.
37-50.Chodorow, Martin S., Roy J. Byrd, and George E.Heidom, 1985.
"Extracting SemanticHierarchies from a Large On-lineDictionary."
Proceedings of the 23rd AnnualMeeting of the ACL, pp.
299-304.Evens, Martha W., Bonnie C. Litowitz, Judith A.Markowitz, Raoul N. Smith, and OswaldWerner, 1980.
Lexical-Semantic Relations: AComparative Survey.
Linguistic Research,Inc., Edmonton, Alberta.Fox, Edward A., 1980.
~..exical Relations:Enhancing Effectiveness of InformationRetrieval Systems."
ACM SIGIR Forum, Vol.15, No.
3, pp.
5-36.Fox, Edward A., J. Terry Nutter, Thomas Ahlswede,Martha Evens, and Judith Markowitz,forthcoming.
"Building a Large Thesaurusfor Information Retrieval."
To be presented atthe ACL Conference on Appfied NaturalLanguage Processing, February, 1988.Mayer, Gleam, 1988.
Program for morphologicalanalysis, nT, unpublished.Halliday, Michael A. IC and Ruqaiya Hs~n, 1976.Cohesion in English.
Longman, London.Klick, Vicki, 1981.
LSP grammar of adverbdefinitions.
Illinois Institute of Technology,unpublished.Peterson, James L., 1982.
Webstex's Seventh NewCollegiate Dictionary: A Computer-ReadableFile Format.
Technical Report TR-196,University of Texas, Austin, TX, May, 1982.Sager, Naomi, 1981.
Natural Language InformationProcessing.
Addison-Wesley.
New York.Wang, Yih-Chen, James Vandendorpe, and MarthaEvens, 1985.
"Relational Thesauri inInformation Retrieval."
./ournal of theAmerican Society for Information Science,voL 36, no.
1,pp.
15-27.224
