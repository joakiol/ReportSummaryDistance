Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1552?1561,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsPhrase-based Statistical Language Generation usingGraphical Models and Active LearningFranc?ois Mairesse, Milica Gas?ic?, Filip Jurc??
?c?ek,Simon Keizer, Blaise Thomson, Kai Yu and Steve Young?Cambridge University Engineering Department, Trumpington Street, Cambridge, CB2 1PZ, UK{f.mairesse, mg436, fj228, sk561, brmt2, ky219, sjy}@eng.cam.ac.ukAbstractMost previous work on trainable languagegeneration has focused on two paradigms:(a) using a statistical model to rank aset of generated utterances, or (b) usingstatistics to inform the generation deci-sion process.
Both approaches rely onthe existence of a handcrafted generator,which limits their scalability to new do-mains.
This paper presents BAGEL, a sta-tistical language generator which uses dy-namic Bayesian networks to learn fromsemantically-aligned data produced by 42untrained annotators.
A human evalua-tion shows that BAGEL can generate nat-ural and informative utterances from un-seen inputs in the information presentationdomain.
Additionally, generation perfor-mance on sparse datasets is improved sig-nificantly by using certainty-based activelearning, yielding ratings close to the hu-man gold standard with a fraction of thedata.1 IntroductionThe field of natural language generation (NLG) isone of the last areas of computational linguistics toembrace statistical methods.
Over the past decade,statistical NLG has followed two lines of research.The first one, pioneered by Langkilde and Knight(1998), introduces statistics in the generation pro-cess by training a model which reranks candi-date outputs of a handcrafted generator.
Whiletheir HALOGEN system uses an n-gram languagemodel trained on news articles, other systems haveused hierarchical syntactic models (Bangalore andRambow, 2000), models trained on user ratings of?This research was partly funded by the UK EPSRC un-der grant agreement EP/F013930/1 and funded by the EUFP7 Programme under grant agreement 216594 (CLASSiCproject: www.classic-project.org).utterance quality (Walker et al, 2002), or align-ment models trained on speaker-specific corpora(Isard et al, 2006).A second line of research has focused on intro-ducing statistics at the generation decision level,by training models that find the set of genera-tion parameters maximising an objective function,e.g.
producing a target linguistic style (Paiva andEvans, 2005; Mairesse and Walker, 2008), gener-ating the most likely context-free derivations givena corpus (Belz, 2008), or maximising the expectedreward using reinforcement learning (Rieser andLemon, 2009).
While such methods do not sufferfrom the computational cost of an overgenerationphase, they still require a handcrafted generator todefine the generation decision space within whichstatistics can be used to find an optimal solution.This paper presents BAGEL (Bayesian networksfor generation using active learning), an NLG sys-tem that can be fully trained from aligned data.While the main requirement of the generator is toproduce natural utterances within a dialogue sys-tem domain, a second objective is to minimise theoverall development effort.
In this regard, a majoradvantage of data-driven methods is the shift ofthe effort from model design and implementationto data annotation.
In the case of NLG systems,learning to produce paraphrases can be facilitatedby collecting data from a large sample of annota-tors.
Our meaning representation should therefore(a) be intuitive enough to be understood by un-trained annotators, and (b) provide useful gener-alisation properties for generating unseen inputs.Section 2 describes BAGEL?s meaning represen-tation, which satisfies both requirements.
Sec-tion 3 then details how our meaning representationis mapped to a phrase sequence, using a dynamicBayesian network with backoff smoothing.Within a given domain, the same semanticconcept can occur in different utterances.
Sec-tion 4 details how BAGEL exploits this redundancy1552to improve generation performance on sparsedatasets, by guiding the data collection processusing certainty-based active learning (Lewis andCatlett, 1994).
We train BAGEL in the informa-tion presentation domain, from a corpus of utter-ances produced by 42 untrained annotators (seeSection 5.1).
An automated evaluation metric isused to compare preliminary model and trainingconfigurations in Section 5.2, while Section 5.3shows that the resulting system produces naturaland informative utterances, according to 18 hu-man judges.
Finally, our human evaluation showsthat training using active learning significantly im-proves generation performance on sparse datasets,yielding results close to the human gold standardusing a fraction of the data.2 Phrase-based generation fromsemantic stacksBAGEL uses a stack-based semantic representa-tion to constrain the sequence of semantic con-cepts to be searched.
This representation can beseen as a linearised semantic tree similar to theone previously used for natural language under-standing in the Hidden Vector State model (Heand Young, 2005).
A stack representation providesuseful generalisation properties (see Section 3.1),while the resulting stack sequences are relativelyeasy to align (see Section 5.1).
In the context ofdialogue systems, Table 1 illustrates how the inputdialogue act is first mapped to a set of stacks ofsemantic concepts, and then aligned with a wordsequence.
The bottom concept in the stack willtypically be a dialogue act type, e.g.
an utteranceproviding information about the object under dis-cussion (inform) or specifying that the requestof the user cannot be met (reject).
Other con-cepts include attributes of that object (e.g., food,area), values for those attributes (e.g., Chinese,riverside), as well as special symbols for negat-ing underlying concepts (e.g., not) or specifyingthat they are irrelevant (e.g., dontcare).The generator?s goal is thus finding themost likely realisation given an unorderedset of mandatory semantic stacks Sm derivedfrom the input dialogue act.
For example,s =inform(area(centre)) is a mandatory stackassociated with the dialogue act in Table 1 (frame8).
While mandatory stacks must all be conveyedin the output realisation, Sm does not contain theoptional intermediary stacks Si that can refer to(a) general attributes of the object under discus-sion (e.g., inform(area) in Table 1), or (b) toconcepts that are not in the input at all, which areassociated with the singleton stack inform (e.g.,phrases expressing the dialogue act type, or clauseaggregation operations).
For example, the stacksequence in Table 1 contains 3 intermediary stacksfor t = 2, 5 and 7.BAGEL?s granularity is defined by the semanticannotation in the training data, rather than externallinguistic knowledge about what constitutes a unitof meaning, i.e.
contiguous words belonging tothe same semantic stack are modelled as an atomicobservation unit or phrase.1 In contrast with word-level models, a major advantage of phrase-basedgeneration models is that they can model long-range dependencies and domain-specific idiomaticphrases with fewer parameters.3 Dynamic Bayesian networks for NLGDynamic Bayesian networks have been used suc-cessfully for speech recognition, natural languageunderstanding, dialogue management and text-to-speech synthesis (Rabiner, 1989; He and Young,2005; Lefe`vre, 2006; Thomson and Young, 2010;Tokuda et al, 2000).
Such models provide aprincipled framework for predicting elements in alarge structured space, such as required for non-trivial NLG tasks.
Additionally, their probabilisticnature makes them suitable for modelling linguis-tic variation, i.e.
there can be multiple valid para-phrases for a given input.BAGEL models the generation task as findingthe most likely sequence of realisation phrasesR?
= (r1...rL) given an unordered set of manda-tory semantic stacks Sm, with |Sm| ?
L. BAGELmust thus derive the optimal sequence of semanticstacks S?
that will appear in the utterance givenSm, i.e.
by inserting intermediary stacks if neededand by performing content ordering.
Any num-ber of intermediary stacks can be inserted betweentwo consecutive mandatory stacks, as long as alltheir concepts are included in either the previousor following mandatory stack, and as long as eachstack transition leads to a different stack (see ex-ample in Table 1).
Let us define the set of possi-ble stack sequences matching these constraints asSeq(Sm) ?
{S = (s1...sL) s.t.
st ?
Sm ?
Si}.We propose a model which estimates the dis-1The term phrase is thus defined here as any sequence ofone or more words.1553Charlie Chan is a Chinese restaurant near Cineworld in the centre of townCharlie Chan Chinese restaurant Cineworld centrename food type near near area areainform inform inform inform inform inform inform informt = 1 t = 2 t = 3 t = 4 t = 5 t = 6 t = 7 t = 8Table 1: Example semantic stacks aligned with an utterance for the dialogue actinform(name(Charlie Chan) type(restaurant) area(centre) food(Chinese) near(Cineworld)).
Mandatorystacks are in bold.tribution P (R|Sm) from a training set of real-isation phrases aligned with semantic stack se-quences, by marginalising over all stack sequencesin Seq(Sm):P (R|Sm) =?S?Seq(Sm)P (R,S|Sm)=?S?Seq(Sm)P (R|S,Sm)P (S|Sm)=?S?Seq(Sm)P (R|S)P (S|Sm) (1)Inference over the model defined in (1) requiresthe decoding algorithm to consider all possible or-derings over Seq(Sm) together with all possiblerealisations, which is intractable for non-trivial do-mains.
We thus make the additional assumptionthat the most likely sequence of semantic stacksS?
given Sm is the one yielding the optimal reali-sation phrase sequence:P (R|Sm) ?
P (R|S?
)P (S?|Sm) (2)with S?
= argmaxS?Seq(Sm)P (S|Sm) (3)The semantic stacks are therefore decoded firstusing the model in Fig.
1 to solve the argmaxin (3).
The decoded stack sequence S?
is thentreated as observed in the realisation phase, inwhich the model in Fig.
2 is used to find the real-isation phrase sequence R?
maximising P (R|S?
)over all phrase sequences of length L = |S?| inour vocabulary:R?
= argmaxR=(r1...rL)P (R|S?
)P (S?|Sm) (4)= argmaxR=(r1...rL)P (R|S?)
(5)In order to reduce model complexity, we fac-torise our model by conditioning the realisationphrase at time t on the previous phrase rt?1,and the previous, current, and following semanticstacks.
The semantic stack st at time t is assumedlast mandatorystackstack setvalidatorfirst framesemanticstack sstack set trackerrepeated frame final frameFigure 1: Graphical model for the semantic decod-ing phase.
Plain arrows indicate smoothed proba-bility distributions, dashed arrows indicate deter-ministic relations, and shaded nodes are observed.The generation of the end semantic stack symboldeterministically triggers the final frame.to depend only on the previous two stacks and thelast mandatory stack su ?
Sm with 1 ?
u < t:P (S|Sm) =???
?Tt=1 P (st|st?1, st?2, su)if S ?
Seq(Sm)0 otherwise(6)P (R|S?)
=T?t=1P (rt|rt?1, s?t?1, s?t , s?t+1) (7)While dynamic Bayesian networks typicallytake sequential inputs, mapping a set of seman-tic stacks to a sequence of phrases is achievedby keeping track of the mandatory stacks thatwere visited in the current sequence (see stack settracker variable in Fig.
1), and pruning any se-quence that has not included all mandatory inputstacks on reaching the final frame (see observedstack set validator variable in Fig.
1).
Since thenumber of intermediary stacks is not known at de-coding time, the network is unrolled for a fixednumber of frames T defining the maximum num-ber of phrases that can be generated (e.g., T =50).
The end of the stack sequence is then deter-mined by a special end symbol, which can onlybe emitted within the T frames once all mandatorystacks have been visited.
The probability of the re-sulting utterance is thus computed over all framesup to the end symbol, which determines the length1554L of S?
and R?.
While the decoding constraintsenforce that L > |Sm|, the search for S?
requirescomparing sequences of different lengths.
A con-sequence is that shorter sequences containing onlymandatory stacks are likely to be favoured.
Whilefuture work should investigate length normalisa-tion strategies, we find that the learned transitionprobabilities are skewed enough to favour stacksequences including intermediary stacks.Once the topology and the decoding constraintsof the network have been defined, any inference al-gorithm can be used to search for S?
and R?.
Weuse the junction tree algorithm implemented in theGraphical Model ToolKit (GMTK) for our exper-iments (Bilmes and Zweig, 2002), however bothproblems can be solved using a standard Viterbisearch given the appropriate state representation.In terms of computational complexity, it is impor-tant to note that the number of stack sequencesSeq(Sm) to search over increases exponentiallywith the number of input mandatory stacks.
Nev-ertheless, we find that real-time performance canbe achieved by pruning low probability sequences,without affecting the quality of the solution.3.1 Generalisation to unseen semantic stacksIn order to generalise to semantic stacks whichhave not been observed during training, the re-alisation phrase r is made dependent on under-specified stack configurations, i.e.
the tail land the head h of the stack.
For example, thelast stack in Table 1 is associated with the headcentre and the tail inform(area).
As a re-sult, BAGEL assigns non-zero probabilities to re-alisation phrases in unseen semantic contexts, bybacking off to the head and the tail of the stack.A consequence is that BAGEL?s lexical realisa-tion can generalise across contexts.
For exam-ple, if reject(area(centre)) was never ob-served at training time, P (r = centre of town|s =reject(area(centre))) will be estimated bybacking off to P (r = centre of town|h =centre).
BAGEL can thus generate ?there areno venues in the centre of town?
if the phrase?centre of town?
was associated with the con-cept centre in a different context, such asinform(area(centre)).
The final realisationmodel is illustrated in Fig.
2:realisation phrase rrepeated frame final framefirst framestack head hsemanticstack sstack tail lFigure 2: Graphical model for the realisationphase.
Dashed arrows indicate deterministic re-lations, and shaded node are observed.!
"#$%&& '(")*+11111 ,,,,,,,| +?+??
ttttttttt sssllrlhrttttttt sllrlhr ,,,,,| 111 +?
?111 ,,,,| +??
tttttt llrlhrttt lhr ,|21,| ??
ttt sssuttt ssss ,,| 21 ?
?tt hr |1| ?tt sstrtsFigure 3: Backoff graphs for the semantic decod-ing and realisation models.P (R|S?)
=L?t=1P (rt|rt?1, ht, lt?1, lt, lt+1,s?t?1, s?t , s?t+1) (8)Conditional probability distributions are repre-sented as factored language models smoothed us-ing Witten-Bell interpolated backoff smoothing(Bilmes and Kirchhoff, 2003), according to thebackoff graphs in Fig.
3.
Variables which are thefurthest away in time are dropped first, and par-tial stack variables are dropped last as they are ob-served the most.It is important to note that generating unseen se-mantic stacks requires all possible mandatory se-mantic stacks in the target domain to be prede-fined, in order for all stack unigrams to be assigneda smoothed non-zero probability.3.2 High cardinality concept abstractionWhile one should expect a trainable generatorto learn multiple lexical realisations for low-cardinality semantic concepts, learning lexicalrealisations for high-cardinality database entries(e.g., proper names) would increase the number ofmodel parameters prohibitively.
We thus dividepre-terminal concepts in the semantic stacks intotwo types: (a) enumerable attributes whose val-ues are associated with distinct semantic stacks in1555our model (e.g., inform(pricerange(cheap))),and (b) non-enumerable attributes whose valuesare replaced by a generic symbol before train-ing in both the utterance and the semantic stack(e.g., inform(name(X)).
These symbolic valuesare then replaced in the surface realisation by thecorresponding value in the input specification.
Aconsequence is that our model can only learn syn-onymous lexical realisations for enumerable at-tributes.4 Certainty-based active learningA major issue with trainable NLG systems is thelack of availability of domain-specific data.
It istherefore essential to produce NLG models thatminimise the data annotation cost.BAGEL supports the optimisation of the datacollection process through active learning, inwhich the next semantic input to annotate is de-termined by the current model.
The probabilis-tic nature of BAGEL allows the use of certainty-based active learning (Lewis and Catlett, 1994),by querying the k semantic inputs for which themodel is the least certain about its output real-isation.
Given a finite semantic input space Irepresenting all possible dialogue acts in our do-main (i.e., the set of all sets of mandatory seman-tic stacks Sm), BAGEL?s active learning trainingprocess iterates over the following steps:1.
Generate an utterance for each semantic input Sm ?
Iusing the current model.22.
Annotate the k semantic inputs {S1m...Skm} yieldingthe lowest realisation probability, i.e.
for q ?
(1..k)Sqm = argminSm?I\{S1m...Sq?1m }(maxRP (R|Sm)) (9)with P (R|Sm) defined in (2).3.
Retrain the model with the additional k data points.The number of utterances to be queried k shoulddepend on the flexibility of the annotators and thetime required for generating all possible utterancesin the domain.5 Experimental methodBAGEL?s factored language models are trained us-ing the SRILM toolkit (Stolcke, 2002), and de-coding is performed using GMTK?s junction treeinference algorithm (Bilmes and Zweig, 2002).2Sampling methods can be used if I is infinite or toolarge.Since each active learning iteration requires gen-erating all training utterances in our domain, theyare generated using a larger clique pruning thresh-old than the test utterances used for evaluation.5.1 Corpus collectionWe train BAGEL in the context of a dialoguesystem providing information about restaurantsin Cambridge.
The domain contains two dia-logue act types: (a) inform: presenting infor-mation about a restaurant (see Table 1), and (b)reject: informing that the user?s constraints can-not be met (e.g., ?There is no cheap restaurantin the centre?).
Our domain contains 8 restau-rant attributes: name, food, near, pricerange,postcode, phone, address, and area, out ofwhich food, pricerange, and area are treatedas enumerable.3 Our input semantic space is ap-proximated by the set of information presentationdialogue acts produced over 20,000 simulated di-alogues between our statistical dialogue manager(Young et al, 2010) and an agenda-based usersimulator (Schatzmann et al, 2007), which resultsin 202 unique dialogue acts after replacing non-enumerable values by a generic symbol.
Each di-alogue act contains an average of 4.48 mandatorysemantic stacks.As one of our objectives is to test whetherBAGEL can learn from data provided by a largesample of untrained annotators, we collected acorpus of semantically-aligned utterances usingAmazon?s Mechanical Turk data collection ser-vice.
A crucial aspect of data collection forNLG is to ensure that the annotators under-stand the meaning of the semantics to be con-veyed.
Annotators were first asked to providean utterance matching an abstract descriptionof the dialogue act, regardless of the order inwhich the constraints are presented (e.g., Offerthe venue Taj Mahal and provide the informationtype(restaurant), area(riverside), food(Indian),near(The Red Lion)).
The order of the constraintsin the description was randomised to reduce theeffect of priming.
The annotators were then askedto align the attributes (e.g., Indicate the region ofthe utterance related to the concept ?area?
), andthe attribute values (e.g., Indicate only the wordsrelated to the concept ?riverside?).
Two para-phrases were collected for each dialogue act inour domain, resulting in a total of 404 aligned ut-3With the exception of areas defined as proper nouns.1556rt st ht lt<s> START START STARTThe Rice Boat inform(name(X)) X inform(name)is a inform inform EMPTYrestaurant inform(type(restaurant)) restaurant inform(type)in the inform(area) area informriverside inform(area(riverside)) riverside inform(area)area inform(area) area informthat inform inform EMPTYserves inform(food) food informFrench inform(food(French)) French inform(food)food inform(food) food inform</s> END END ENDTable 2: Example utterance annotation used to estimate the conditional probability distributions of themodels in Figs.
1 and 2 ( rt=realisation phrase, st=semantic stack, ht=stack head, lt=stack tail).terances produced by 42 native speakers of En-glish.
After manually checking and normalisingthe dataset,4 the layered annotations were auto-matically mapped to phrase-level semantic stacksby splitting the utterance into phrases at annotationboundaries.
Each annotated utterance is then con-verted into a sequence of symbols such as in Ta-ble 2, which are used to estimate the conditionalprobability distributions defined in (6) and (8).The resulting vocabulary consists of 52 distinct se-mantic stacks and 109 distinct realisation phrases,with an average of 8.35 phrases per utterance.5.2 BLEU score evaluationWe first evaluate BAGEL using the BLEU auto-mated metric (Papineni et al, 2002), which mea-sures the word n-gram overlap between the gen-erated utterances and the 2 reference paraphrasesover a test corpus (with n up to 4).
While BLEUsuffers from known issues such as a bias towardsstatistical NLG systems (Reiter and Belz, 2009), itprovides useful information when comparing sim-ilar systems.
We evaluate BAGEL for differenttraining set sizes, model dependencies, and activelearning parameters.
Our results are averaged overa 10-fold cross-validation over distinct dialogueacts, i.e.
dialogue acts used for testing are not seenat training time,5 and all systems are tested on thesame folds.
The training and test sets respectivelycontain an average of 181 and 21 distinct dialogueacts, and each dialogue act is associated with twoparaphrases, resulting in 362 training utterances.4The normalisation process took around 4 person-hour for404 utterances.5We do not evaluate performance on dialogue acts usedfor training, as the training examples can trivially be used asgeneration templates.!"#$!"%!"%$!"#!"#$%&'()%*+,-"!"$!"$$!"#$%&'()%*+,-"!"#$%&'()%*+,-"&'(()*+,-(!".!".$!"#$%&'()%*+,-"/+)01234)5234+66/+)01234)5234+667)8+)6'1'9-)0-*281:30!
";$ <!
=!
.!
#!
>!
<!!
<=!
<$!
=!!
=$!
;!!
;#=!"#$%&'()%*+,-".-#/$/$0%*"1%*/2"!"#$%&'()%*+,-"!
"#$%&'()%*+,-"Figure 4: BLEU score averaged over a 10-foldcross-validation for different training set sizes andnetwork topologies, using random sampling.Results: Fig.
4 shows that adding a dependencyon the future semantic stack improves perfor-mances for all training set sizes, despite the addedmodel complexity.
Backing off to partial stacksalso improves performance, but only for sparsetraining sets.Fig.
5 compares the full model trained usingrandom sampling in Fig.
4 with the same modeltrained using certainty-based active learning, fordifferent values of k. As our dataset only con-tains two paraphrases per dialogue act, the samedialogue act can only be queried twice during theactive learning procedure.
A consequence is thatthe training set used for active learning convergestowards the randomly sampled set as its size in-creases.
Results show that increasing the train-ing set one utterance at a time using active learn-ing (k = 1) significantly outperforms randomsampling when using 40, 80, and 100 utterances(p < .05, two-tailed).
Increasing the number ofutterances to be queried at each iteration to k = 10results in a smaller performance increase.
A possi-1557!"#!"##!"$!"$#!"%!"%#!"#$%&'()%*+,-"&'()*+,-'+./0(1!"2#!"3!"3#4!
5!
3!
$!
6!
4!!
45!
4#!
5!!
5#!
2!!
2$5!
"#$%&'()%*+,-".-#/$/$0%*"1%*/2"7890:;,/;'<(0(1,=>47890:;,/;'<(0(1,=>4!Figure 5: BLEU score averaged over a 10-foldcross-validation for different numbers of queriesper iteration, using the full model with the queryselection criterion (9).!"#!"##!"$!"$#!"%!"%#!"#$%&'()%*+,-"&'(()(*+,-*.!"/#!"0!"0#1!
2!
0!
$!
3!
1!!
12!
1#!
2!!
2#!
/!!
/$2!"#$%&'()%*+,-".-#/$/$0%*"1%*/2"4*+,-*.
),5-)6-7854*9+5:;)<9,';)6<-:;Figure 6: BLEU score averaged over a 10-foldcross-validation for different query selection cri-teria, using the full model with k = 1.ble explanation is that the model is likely to assignlow probabilities to similar inputs, thus any valueabove k = 1 might result in redundant querieswithin an iteration.As the length of the semantic stack sequenceis not known before decoding, the active learn-ing selection criterion presented in (9) is biasedtowards longer utterances, which tend to have alower probability.
However, Fig.
6 shows thatnormalising the log probability by the number ofsemantic stacks does not improve overall learn-ing performance.
Although a possible explanationis that longer inputs tend to contain more infor-mation to learn from, Fig.
6 shows that a base-line selecting the largest remaining semantic inputat each iteration performs worse than the activelearning scheme for training sets above 20 utter-ances.
The full log probability selection criteriondefined in (9) is therefore used throughout the restof the paper (with k = 1).5.3 Human evaluationWhile automated metrics provide useful informa-tion for comparing different systems, human feed-back is needed to assess (a) the quality of BAGEL?soutputs, and (b) whether training models using ac-tive learning has a significant impact on user per-ceptions.
We evaluate BAGEL through a large-scale subjective rating experiment using Amazon?sMechanical Turk service.For each dialogue act in our domain, partici-pants are presented with a ?gold standard?
humanutterance from our dataset, which they must com-pare with utterances generated by models trainedwith and without active learning on a set of 20, 40,100, and 362 utterances (full training set), as wellas with the second human utterance in our dataset.See example utterances in Table 3.
The judges arethen asked to evaluate the informativeness and nat-uralness of each of the 8 utterances on a 5 pointlikert-scale.
Naturalness is defined as whether theutterance could have been produced by a human,and informativeness is defined as whether it con-tains all the information in the gold standard utter-ance.
Each utterance is taken from the test folds ofthe cross-validation experiment presented in Sec-tion 5.2, i.e.
the models are trained on up to 90%of the data and the training set does not contain thedialogue act being tested.Results: Figs.
7 and 8 compare the naturalnessand informativeness scores of each system aver-aged over all 202 dialogue acts.
A paired t-testshows that models trained on 40 utterances orless produce utterances that are rated significantlylower than human utterances for both naturalnessand informativeness (p < .05, two-tailed).
How-ever, models trained on 100 utterances or more donot perform significantly worse than human utter-ances for both dimensions, with a mean differencebelow .10 over 202 comparisons.
Given the largesample size, this result suggests that BAGEL cansuccessfully learn our domain using a fraction ofour initial dataset.As far as the learning method is concerned, apaired t-test shows that models trained on 20 and40 utterances using active learning significantlyoutperform models trained using random sam-pling, for both dimensions (p < .05).
The largestincrease is observed using 20 utterances, i.e.
thenaturalness increases by .49 and the informative-ness by .37.
When training on 100 utterances, theeffect of active learning becomes insignificant.
In-1558Input inform(name(the Fountain) near(the Arts Picture House) area(centre) pricerange(cheap))Human There is an inexpensive restaurant called the Fountain in the centre of town near the Arts Picture HouseRand-20 The Fountain is a restaurant near the Arts Picture House located in the city centre cheap price rangeRand-40 The Fountain is a restaurant in the cheap city centre area near the Arts Picture HouseAL-20 The Fountain is a restaurant near the Arts Picture House in the city centre cheapAL-40 The Fountain is an affordable restaurant near the Arts Picture House in the city centreFull set The Fountain is a cheap restaurant in the city centre near the Arts Picture HouseInput reject(area(Barnwell) near(Saint Mary?s Church))Human I am sorry but I know of no venues near Saint Mary?s Church in the Barnwell areaFull set I am sorry but there are no venues near Saint Mary?s Church in the Barnwell areaInput inform(name(the Swan)area(Castle Hill) pricerange(expensive))Human The Swan is a restaurant in Castle Hill if you are seeking something expensiveFull set The Swan is an expensive restaurant in the Castle Hill areaInput inform(name(Browns) area(centre) near(the Crowne Plaza) near(El Shaddai) pricerange(cheap))Human Browns is an affordable restaurant located near the Crowne Plaza and El Shaddai in the centre of the cityFull set Browns is a cheap restaurant in the city centre near the Crowne Plaza and El ShaddaiTable 3: Example utterances for different input dialogue acts and system configurations.
AL-20 = activelearning with 20 utterances, Rand = random sampling.!
"## !"$%!"&'!
"(% !
")* *"%% *"%#*"%'++"$!!"$**"$$!
"#$%$#&'(#)$"**%*+,(",-./01##"$ +% *% #%% !(+!"#$%$#&'(#)$"**%*+,("-(#.$.$/%*"&%*.0"234567897-:.5.
;<=1-.8=447:-.378>8*"%'Figure 7: Naturalness mean opinion scores for dif-ferent training set sizes, using random samplingand active learning.
Differences for training setsizes of 20 and 40 are all significant (p < .05).terestingly, while models trained on 100 utterancesoutperform models trained on 40 utterances usingrandom sampling (p < .05), they do not signifi-cantly outperform models trained on 40 utterancesusing active learning (p = .15 for naturalness andp = .41 for informativeness).
These results sug-gest that certainty-based active learning is benefi-cial for training a generator from a limited amountof data given the domain size.Looking back at the results presented in Sec-tion 5.2, we find that the BLEU score correlateswith a Pearson correlation coefficient of .42 withthe mean naturalness score and .35 with the meaninformativeness score, over all folds of all systemstested (n = 70, p < .01).
This is lower thanprevious correlations reported by Reiter and Belz(2009) in the shipping forecast domain with non-expert judges (r = .80), possibly because our do-main is larger and more open to subjectivity.!
"## !"$$#"%&!
"'& !
"()#"%$ #"%##"&!**"+!!"+##"++!"#$%&$'()*#+&,"$"--%-.
()",-./01&&"+ *% #% &%% !)*!"#$%&$'()*#+&,"$"--%-.()"/)#&$&$0%-"+%-&1"234567897-:.5.
;<=1-.8=447:-.378>8#"&!Figure 8: Informativeness mean opinion scores fordifferent training set sizes, using random samplingand active learning.
Differences for training setsizes of 20 and 40 are all significant (p < .05).6 Related workWhile most previous work on trainable NLG re-lies on a handcrafted component (see Section 1),recent research has started exploring fully data-driven NLG models.Factored language models have recently beenused for surface realisation within the OpenCCGframework (White et al, 2007; Espinosa et al,2008).
More generally, chart generators fordifferent grammatical formalisms have beentrained from syntactic treebanks (White et al,2007; Nakanishi et al, 2005), as well as fromsemantically-annotated treebanks (Varges andMellish, 2001).
However, a major difference withour approach is that BAGEL uses domain-specificdata to generate a surface form directly from se-mantic concepts, without any syntactic annotation(see Section 7 for further discussion).1559This work is strongly related to Wong andMooney?s WASP?1 generation system (2007),which combines a language model with an in-verted synchronous CFG parsing model, effec-tively casting the generation task as a translationproblem from a meaning representation to natu-ral language.
WASP?1 relies on GIZA++ to alignutterances with derivations of the meaning repre-sentation (Och and Ney, 2003).
Although earlyexperiments showed that GIZA++ did not performwell on our data?possibly because of the coarsegranularity of our semantic representation?futurework should evaluate the generalisation perfor-mance of synchronous CFGs in a dialogue systemdomain.Although we do not know of any work on ac-tive learning for NLG, previous work has usedactive learning for semantic parsing and informa-tion extraction (Thompson et al, 1999; Tang et al,2002), spoken language understanding (Tur et al,2003), speech recognition (Hakkani-Tu?r et al,2002), word alignment (Sassano, 2002), and morerecently for statistical machine translation (Blood-good and Callison-Burch, 2010).
While certainty-based methods have been widely used, future workshould investigate the performance of committee-based active learning for NLG, in which examplesare selected based on the level of disagreement be-tween models trained on subsets of the data (Fre-und et al, 1997).7 Discussion and conclusionThis paper presents and evaluates BAGEL, a sta-tistical language generator that can be trained en-tirely from data, with no handcrafting required be-yond the semantic annotation.
All the requiredsubtasks?i.e.
content ordering, aggregation, lex-ical selection and realisation?are learned fromdata using a unified model.
To train BAGEL in a di-alogue system domain, we propose a stack-basedsemantic representation at the phrase level, whichis expressive enough to generate natural utterancesfrom unseen inputs, yet simple enough for data tobe collected from 42 untrained annotators with aminimal normalisation step.
A human evaluationover 202 dialogue acts does not show any differ-ence in naturalness and informativeness betweenBAGEL?s outputs and human utterances.
Addition-ally, we find that the data collection process canbe optimised using active learning, resulting in asignificant increase in performance when trainingdata is limited, according to ratings from 18 hu-man judges.6 These results suggest that the pro-posed framework can largely reduce the develop-ment time of NLG systems.While this paper only evaluates the most likelyrealisation given a dialogue act, we believe thatBAGEL?s probabilistic nature and generalisationcapabilities are well suited to model the linguis-tic variation resulting from the diversity of annota-tors.
Our first objective is thus to evaluate the qual-ity of BAGEL?s n-best outputs, and test whethersampling from the output distribution can improvenaturalness and user satisfaction within a dialogue.Our results suggest that explicitly modellingsyntax is not necessary for our domain, possi-bly because of the lack of syntactic complexitycompared with formal written language.
Never-theless, future work should investigate whethersyntactic information can improve performance inmore complex domains.
For example, the reali-sation phrase can easily be conditioned on syntac-tic constructs governing that phrase, and the recur-sive nature of syntax can be modelled by keepingtrack of the depth of the current embedded clause.While syntactic information can be included withno human effort by using syntactic parsers, theirrobustness to dialogue system utterances must firstbe evaluated.Finally, recent years have seen HMM-basedsynthesis models become competitive with unit se-lection methods (Tokuda et al, 2000).
Our longterm objective is to take advantage of those ad-vances to jointly optimise the language genera-tion and the speech synthesis process, by combin-ing both components into a unified probabilisticconcept-to-speech generation model.ReferencesS.
Bangalore and O. Rambow.
Exploiting a probabilistic hi-erarchical model for generation.
In Proceedings of the18th International Conference on Computational Linguis-tics (COLING), pages 42?48, 2000.A.
Belz.
Automatic generation of weather forecast texts us-ing comprehensive probabilistic generation-space models.Natural Language Engineering, 14(4):431?455, 2008.J.
Bilmes and K. Kirchhoff.
Factored language models andgeneralized parallel backoff.
In Proceedings of HLT-NAACL, short papers, 2003.J.
Bilmes and G. Zweig.
The Graphical Models ToolKit: Anopen source software system for speech and time-seriesprocessing.
In Proceedings of ICASSP, 2002.6The full training corpus and the generatedutterances used for evaluation are available athttp://mi.eng.cam.ac.uk/?farm2/bagel.1560M.
Bloodgood and C. Callison-Burch.
Bucking the trend:Large-scale cost-focused active learning for statistical ma-chine translation.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguistics(ACL), 2010.D.
Espinosa, M. White, and D. Mehay.
Hypertagging: Su-pertagging for surface realization with CCG.
In Proceed-ings of the 46th Annual Meeting of the Association forComputational Linguistics (ACL), 2008.Y.
Freund, H. S. Seung, E.Shamir, and N. Tishby.
Selectivesampling using the query by committee algorithm.
Ma-chine Learning, 28:133?168, 1997.D.
Hakkani-Tu?r, G. Riccardi, and A. Gorin.
Active learn-ing for automatic speech recognition.
In Proceedings ofICASSP, 2002.Y.
He and S. Young.
Semantic processing using the HiddenVector State model.
Computer Speech & Language, 19(1):85?106, 2005.A.
Isard, C. Brockmann, and J. Oberlander.
Individuality andalignment in generated dialogues.
In Proceedings of the4th International Natural Language Generation Confer-ence (INLG), pages 22?29, 2006.I.
Langkilde and K. Knight.
Generation that exploits corpus-based statistical knowledge.
In Proceedings of the 36thAnnual Meeting of the Association for Computational Lin-guistics (ACL), pages 704?710, 1998.F.
Lefe`vre.
A DBN-based multi-level stochastic spoken lan-guage understanding system.
In Proceedings of the IEEEWorkshop on Spoken Language Technology (SLT), 2006.D.
D. Lewis and J. Catlett.
Heterogeneous uncertainty am-pling for supervised learning.
In Proceedings of ICML,1994.F.
Mairesse and M. A. Walker.
Trainable generation of Big-Five personality styles through data-driven parameter esti-mation.
In Proceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics (ACL), 2008.H.
Nakanishi, Y. Miyao, , and J. Tsujii.
Probabilistic methodsfor disambiguation of an HPSG-based chart generator.
InProceedings of the IWPT, 2005.F.
J. Och and H. Ney.
A systematic comparison of variousstatistical alignment models.
Computational Linguistics,29(1):19?51, 2003.D.
S. Paiva and R. Evans.
Empirically-based control of nat-ural language generation.
In Proceedings of the 43rd An-nual Meeting of the Association for Computational Lin-guistics (ACL), pages 58?65, 2005.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
BLEU: amethod for automatic evaluation of machine translation.
InProceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics (ACL), 2002.L.
R. Rabiner.
Tutorial on Hidden Markov Models and se-lected applications in speech recognition.
Proceedings ofthe IEEE, 77(2):257?285, 1989.E.
Reiter and A. Belz.
An investigation into the validityof some metrics for automatically evaluating natural lan-guage generation systems.
Computational Linguistics, 25:529?558, 2009.V.
Rieser and O.
Lemon.
Natural language generation asplanning under uncertainty for spoken dialogue systems.In Proceedings of the Annual Meeting of the EuropeanChapter of the ACL (EACL), 2009.M.
Sassano.
An empirical study of active learning with sup-port vector machines for japanese word segmentation.
InProceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics (ACL), 2002.J.
Schatzmann, B. Thomson, K. Weilhammer, H. Ye, andS.
Young.
Agenda-based user simulation for bootstrap-ping a POMDP dialogue system.
In Proceedings of HLT-NAACL, short papers, pages 149?152, 2007.A.
Stolcke.
SRILM ?
an extensible language modelingtoolkit.
In Proceedings of the International Conferenceon Spoken Language Processing, 2002.M.
Tang, X. Luo, and S. Roukos.
Active learning for statis-tical natural language parsing.
In Proceedings of the 40thAnnual Meeting of the Association for Computational Lin-guistics (ACL), 2002.C.
Thompson, M. E. Califf, and R. J. Mooney.
Active learn-ing for natural language parsing and information extrac-tion.
In Proceedings of ICML, 1999.B.
Thomson and S. Young.
Bayesian update of dialogue state:A POMDP framework for spoken dialogue systems.
Com-puter Speech & Language, 24(4):562?588, 2010.Y.
Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi, andT.
Kitamura.
Speech parameter generation algorithms forHMM-based speech synthesis.
In Proceedings of ICASSP,2000.G.
Tur, R. E. Schapire, and D. Hakkani-Tu?r.
Active learn-ing for spoken language understanding.
In Proceedings ofICASSP, 2003.S.
Varges and C. Mellish.
Instance-based natural languagegeneration.
In Proceedings of the Annual Meeting of theNorth American Chapter of the ACL (NAACL), 2001.M.
A. Walker, O. Rambow, and M. Rogati.
Training a sen-tence planner for spoken dialogue using boosting.
Com-puter Speech and Language, 16(3-4), 2002.M.
White, R. Rajkumar, and S. Martin.
Towards broad cov-erage surface realization with CCG.
In Proceedings of theWorkshop on Using Corpora for NLG: Language Genera-tion and Machine Translation, 2007.Y.
W. Wong and R. Mooney.
Generation by inverting a se-mantic parser that uses statistical machine translation.
InProceedings of HLT-NAACL, 2007.S.
Young, M.
Gas?ic?, S. Keizer, F. Mairesse, J. Schatzmann,B.
Thomson, and K. Yu.
The Hidden Information Statemodel: a practical framework for POMDP-based spokendialogue management.
Computer Speech and Language,24(2):150?174, 2010.1561
