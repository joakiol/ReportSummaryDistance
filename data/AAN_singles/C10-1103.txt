Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 913?921,Beijing, August 2010The Bag-of-Opinions Method for Review Rating Prediction from SparseText PatternsLizhen QuMax-Planck Institutefor Informaticslqu@mpii.mpg.deGeorgiana IfrimBioinformatics ResearchCentreifrim@birc.au.dkGerhard WeikumMax-Planck Institutefor Informaticsweikum@mpii.mpg.deAbstractThe problem addressed in this paper is topredict a user?s numeric rating in a prod-uct review from the text of the review.
Un-igram and n-gram representations of textare common choices in opinion mining.However, unigrams cannot capture impor-tant expressions like ?could have been bet-ter?, which are essential for predictionmodels of ratings.
N-grams of words, onthe other hand, capture such phrases, buttypically occur too sparsely in the train-ing set and thus fail to yield robust pre-dictors.
This paper overcomes the limita-tions of these two models, by introducinga novel kind of bag-of-opinions represen-tation, where an opinion, within a review,consists of three components: a root word,a set of modifier words from the same sen-tence, and one or more negation words.Each opinion is assigned a numeric scorewhich is learned, by ridge regression,from a large, domain-independent cor-pus of reviews.
For the actual test caseof a domain-dependent review, the re-view?s rating is predicted by aggregat-ing the scores of all opinions in the re-view and combining it with a domain-dependent unigram model.
The paperpresents a constrained ridge regression al-gorithm for learning opinion scores.
Ex-periments show that the bag-of-opinionsmethod outperforms prior state-of-the-arttechniques for review rating prediction.1 Introduction1.1 MotivationOpinion mining and sentiment analysis has be-come a hot research area (Pang and Lee, 2008).There is ample work on analyzing the sentimentsof online-review communities where users com-ment on products (movies, books, consumer elec-tronics, etc.
), implicitly expressing their opinionpolarities (positive, negative, neutral), and alsoprovide numeric ratings of products (Titov andMcDonald, 2008b; Lerman et al, 2009; Hu andLiu, 2004; Titov and McDonald, 2008a; Pangand Lee, 2005; Popescu and Etzioni, 2005a).
Al-though ratings are more informative than polari-ties, most prior work focused on classifying textfragments (phrases, sentences, entire reviews) bypolarity.
However, a product receiving mostly 5-star reviews exhibits better customer purchase be-havior compared to a product with mostly 4-starreviews.
In this paper we address the learning andprediction of numerical ratings from review texts,and we model this as a metric regression problemover an appropriately defined feature space.Formally, the input is a set of rated documents(i.e., reviews), {xi, yi}Ni=1, where xi is a sequenceof word-level unigrams (w1, ..., wl) and yi ?
R isa rating.
The goal is to learn a function f(x) thatmaps the word vector x into a numerical rating y?,which indicates both the polarity and strength ofthe opinions expressed in a document.Numerical review rating prediction is harderthan classifying by polarity.
Consider the follow-ing example from Amazon book reviews:The organization of the book is hard to followand the chapter titles are not very helpful, so go-ing back and trying to find information is quite913difficult.We note that there are many subjective words(hard, helpful, difficult) modified by opinion mod-ifiers such as (very, quite) and negation words like(not).
For rating prediction, considering opin-ion modifiers is crucial; very helpful is a muchstronger sentiment than helpful.
Negation wordsalso need attention.
As pointed out by Liu andSeneff (2009) we cannot simply reverse the polar-ity.
For example, if we assign a higher positivescore to very helpful than to helpful, simply re-versing the sign of the scores would incorrectlysuggest that not helpful is less negative than notvery helpful.The widely used unigram (bag-of-words)model (Pang and Lee, 2005; Snyder and Barzilay,2007; Goldberg and Zhu, 2006; Ganu et al, 2009)cannot properly capture phrase patterns.
Con-sider the following example: not so helpful vs.not so bad.
In a unigram-based regression modeleach unigram gets a weight indicating its polarityand strength.
High positive/negative weights arestrongly positive/negative clues.
It is reasonableto assign a positive weight to helpful and a nega-tive weight to bad.
The fundamental problem ofunigrams arises when assigning a weight to not.If not had a strongly negative weight, the posi-tive weight of helpful would be strongly reducedwhile the negative weight of bad would be ampli-fied (by combining weights).
This clearly fails tocapture the true intentions of the opinion phrases.The same problem holds for so, which is an inten-sifier that should keep the same sign as the wordit modifies.
We refer to this limitation of the uni-gram model as polarity incoherence.A promising way of overcoming this weaknessis to include n-grams, generalizing the bag-of-words model into a bag-of-phrases model (Bac-cianella et al, 2009; Pang and Lee, 2008).
How-ever, regression models over the feature spaceof all n-grams (for either fixed maximal n orvariable-length phrases) are computationally ex-pensive in their training phase.
Moreover andmost importantly for our setting, including n-grams in the model results in a very high dimen-sional feature space: many features will then oc-cur only very rarely in the training data.
There-fore, it is difficult if not impossible to reliablylearn n-gram weights from limited-size trainingsets.
We refer to this problem as the n-gram spar-sity bottleneck.
In our experiments we inves-tigate the effect of using bigrams and variable-length ngrams for improving review rating predic-tion.1.2 ContributionTo overcome the above limitations of unigram andn-gram features, we have developed a novel kindof bag-of-opinions model, which exploits domain-independent corpora of opinions (e.g., all Amazonreviews), but is finally applied for learning predic-tors on domain-specific reviews (e.g., movies asrated in IMDB or Rottentomatoes).
A documentis represented as a bag of opinions each of whichhas three components: a root word, a set of modi-fier words and one or more negation words.
In thephrase not very helpful, the opinion root is help-ful, one (of potentially many) opinion modifier(s)is very, and a negation word is not.
We enforce po-larity coherence by the design of a learnable func-tion that assigns a score to an opinion.Our approach generalizes the cumulative linearoffset model (CLO) presented in (Liu and Seneff,2009).
The CLO model makes several restrictiveassumptions, most notably, that all opinion scoreswithin one document are the same as the overalldocument rating.
This assumption does not holdin practice, not even in reviews with extremelypositive/negative ratings.
For example, in a 5-star Amazon review the phrases most impressivebook and it helps explain should receive differentscores.
Otherwise, the later transfer step to dif-ferent domains would yield poor predictions.
Dueto this restriction, CLO works well on particulartypes of reviews that have pro/con entries listingcharacteristic major opinions about the object un-der review.
For settings with individual reviewswhose texts do not exhibit any specific structure,the CLO model faces its limitations.In our bag-of-opinions method, we address thelearning of opinion scores as a constrained ridgeregression problem.
We consider the opinionscores in a given review to be drawn from anunknown probability distribution (so they do nothave to be the same within a document).
We es-timate the review rating based on a set of statis-914tics (e.g., expectation, variance, etc.)
derived fromthe scores of opinions in a document.
Thus, ourmethod has a sound statistical foundation and canbe applied to arbitrary reviews with mixed opin-ion polarities and strengths.
We avoid the n-gramsparsity problem by the limited-size structuredfeature space of (root,modifiers,negators) opin-ions.We treat domain-independent and domain-dependent opinions differently in our system.
Inthe first step we learn a bag-of-opinions model ona large dataset of online reviews to obtain scoresfor domain-independent opinions.
Since the po-larity of opinions is not bound to a topic, onecan learn opinion scores from a pooled corpusof reviews for various categories, e.g., movies,books, etc., and then use these scored opinionsfor predicting the ratings of reviews belongingto a particular category.
In order to also capturedomain-dependent information (possibly comple-mentary to the opinion lexicon used for learn-ing domain-independent opinions), we combinethe bag-of-opinions model with an unigram modeltrained on the domain-dependent corpus.
Sincedomain-dependent training is typically limited,we model it using unigram models rather thanbag-of-opinions.
By combining the two models,even if an opinion does not occur in the domain-dependent training set but it occurs in a test re-view, we can still accurately predict the review rat-ing based on the globally learned opinion score.
Insome sense our combined learning scheme is sim-ilar to smoothing in standard learning techniques,where the estimate based on a limited trainingset is smoothed using a large background corpus(Zhai and Lafferty, 2004).In summary, the contributions of this paper arethe following:1.
We introduce the bag-of-opinions model, forcapturing the influence of n-grams, but in astructured way with root words, modifiers,and negators, to avoid the explosion of thefeature space caused by explicit n-gram mod-els.2.
We develop a constrained ridge regressionmethod for learning scores of opinions fromdomain-independent corpora of rated re-views.3.
For transferring the regression model tonewly given domain-dependent applications,we derive a set of statistics over opinionscores in documents and use these as fea-tures, together with standard unigrams, forpredicting the rating of a review.4.
Our experiments with Amazon reviews fromdifferent categories (books, movies, music)show that the bag-of-opinions method out-performs prior state-of-the-art techniques.2 Bag-of-Opinions ModelIn this section we first introduce the bag-of-opinions model, followed by the method forlearning (domain-independent) model parameters.Then we show how we annotate opinions and howwe adapt the model to domain-dependent data.2.1 Model RepresentationWe model each document as a bag-of-opinions{opk}Kk=1, where the number of opinionsK variesamong documents.
Each opinion opk consistsof an opinion root wr, r ?
SR, a set of opin-ion modifiers {wm}Mm=1, m ?
SM and a set ofnegation words {wz}Zz=1, z ?
SZ , where the setsSR, SM , SZ are component index sets of opinionroots, opinion modifiers and negation words re-spectively.
The union of these sets forms a globalcomponent index set S ?
Nd, where d is the di-mension of the index space.
The opinion root de-termines the prior polarity of the opinion.
Modi-fiers intensify or weaken the strength of the priorpolarity.
Negation words strongly reduce or re-verse the prior polarity.
For each opinion, theset of negation words consists of at most a nega-tion valence shifter like not (Kennedy and Inkpen,2006) and its intensifiers like capitalization of thevalence shifter.
Each opinion component is asso-ciated with a score.
We assemble the scores ofopinion elements into an opinion-score by usinga score function.
For example, in the opinion notvery helpful, the opinion root helpful determinesthe prior polarity positive say with a score 0.9, themodifier very intensifies the polarity say with a915score 0.5.
The prior polarity is further strongly re-duced by the negation word not with e.g., a score-1.2.
Then we sum up the scores to get a score of0.2 for the opinion not very helpful.Formally, we define the function score(op) asa linear function of opinion components, whichtakes the formscore(op) = sign(r)?rxr+M?m=1sign(r)?mxm+Z?z=1sign(r)?zxz (1)where {xz, xm, xr} are binary variables denotingthe presence or absence of negation words, modi-fiers and opinion root.
{?z, ?m, ?r} are weights ofeach opinion elements.
sign(r) : wr ?
{?1, 1}is the opinion polarity function of the opinion rootwr.
It assigns a value 1/-1 if an opinion root ispositive/negative.
Due to the semantics of opin-ion elements, we have constraints that ?r ?
0and ?z ?
0.
The sign of ?m is determined in thelearning phase, since we have no prior knowledgewhether it intensifies or weakens the prior polar-ity.Since a document is modeled as a bag-of-opinions, we can simply consider the expec-tation of opinion scores as the document rat-ing.
If we assume the scores are uniformly dis-tributed, the prediction function is then f(x) =1K?Kk=1 score(opk) which assigns the average ofopinion scores to the document x.2.2 Learning Regression ParametersWe assume that we can identify the opinion rootsand negation words from a subjectivity lexicon.
Inthis work we use MPQA (Wilson et al, 2005).
Inaddition, the lexicon provides the prior polarity ofthe opinion roots.
In the training phase, we aregiven a set of documents with ratings {xi, yi}Ni=1,and our goal is to find an optimal function f?whose predictions {y?i}Ni=1 are as close as possi-bile to the original ratings {yi}Ni=1.
Formally, weaim to minimize the following loss function:L = 12NN?i=1(f(xi)?
yi)2 (2)where f(xi) is modeled as the average score ofopinions in review xi.First, we rewrite score(op) as the dotproduct ??,p?
between a weight vector?
= [?z,?m, ?r] and a feature vectorp = [sign(r)xz, sign(r)xm, sign(r)xr].In order to normalize the vectors, werewrite the weight and feature vectors inthe d dimensional vector space of all rootwords, modifiers and negation words.
Then?
= [..,?z, 0, ..,?m, 0, .., ?r, 0..] ?
Rd and p =[sign(r)xz, 0, .., sign(r)xm, 0, .., sign(r)xr, ...] ?Rd.
The function f(xi) can then be written asthe dot product ?
?,vi?, where vi = 1Ki?Kik=1 pk,with Ki the number of opinions in review xi.By using this feature representation, the learningproblem is equivalent to:min?L(?)
= 12NN?i=1(?
?,vi?+ ?0 ?
yi)2s.t.
?z ?
0 z ?
SZ?r ?
0 r ?
SR (3)where ?
?
Rd, ?
= [?z,?m,?r].
?0 is the inter-cept of the regression function, which is estimatedas the mean of the ratings in the training set.
Wedefine a new variable y?i = yi ?
?0.In order to avoid overfitting, we add an l2 normregularizer to the loss function with the parameter?
> 0.LR(?)
= 12NN?i=1(??,vi?
?
y?i)2 +?2 ?
?
?22s.t.
?z ?
0 z ?
SZ?r ?
0 r ?
SR (4)We solve the above optimization problem by Al-gorithm 1 using coordinate descent.
The proce-dure starts with ?0 = 0, ?0 ?
Rd.
Then it up-dates iteratively every coordinate of the vector ?until convergence.
Algorithm 1 updates every co-ordinate ?j , j ?
{1, 2, ..., d} of ?
by solving thefollowing one-variable sub-problem:minlj?
?j?cjLR(?1, ..., ?j , ..., ?d)916where lj and cj denote the lower and upperbounds of ?j .
If j ?
SZ , lj = ??
and cj = 0.If j ?
SR, lj = 0 and cj = ?.
Otherwise bothbounds are infinity.According to (Luo and Tseng, 1992), the solu-tion of this one-variable sub-problem is?
?j = max{lj ,min{cj , gj}}wheregj =1N?Ni=1 vij(y?i ?
?l 6=j ?lvl)1N?Ni=1 v2ij + ?Here gj is the close form solution of standardridge regression at coordinate j (for details see(Friedman et al, 2008)).
We prove the conver-gence of Algorithm 1, by the following theoremusing techniques in (Luo and Tseng, 1992).Theorem 1 A sequence of ?
generated by Algo-rithm 1 globally converges to an optimal solution??
?
??
of problem (4), where ??
is the set ofoptimal solutions.Proof: Luo and Tseng (1992) show that coordi-nate descent for constrained quadratic functionsin the following form converges to one of its globaloptimal solutions.min?
h(?)
= ??,Q?
?/2 + ?q,??s.t.
ET?
?
bwhere Q is a d?d symmetric positive-definite ma-trix, E is a d?
d matrix having no zero column, qis a d-vector and b is a d-vector.We rewrite LR in matrix form as12N (y?
?V?
)T (y?
?V?)
+ ?2?T?= 12N (V?
)T (V?)
+ ?2?T?
?
12N ((V?
)T y??
12N y?T (V?))
+ 12N y?T y?= ??,Q?
?/2 + ?q,?
?+ constantwhereQ = BTB,B =[ ?1NV?
?Id?d],q = ?1N (VT y?
)where Id?d is the identity matrix.
Because ?
>0, all columns of B are linearly independent.
AsQ = BTB and symmetric, Q is positive definite.We define E as a d ?
d diagonal matrix withall entries on the main diagonal equal to 1 excepteii = ?1, i ?
SZ and b is a d-vector with allentries equal to ??
except bi = 0, for i ?
SZ ori ?
SR.Because the almost cyclic rule is applied togenerate the sequence {?t}, the algorithm con-verges to a solution ??
?
?
?.Algorithm 1 Constrained Ridge Regression1: Input: ?
and {vn, y?n}Nn=12: Output: optimal ?3: repeat4: for j = 1, ..., d do5: gj =1NPNi=1 vij(y?i?Pl 6=j ?lvl)1NPNi=1 v2ij+?6:?
?j =??
?0, if j ?
SR and gj < 00, if j ?
SZ and gj > 0gj , else7: end for8: until Convergence condition is satisfied2.3 Annotating OpinionsThe MPQA lexicon contains separate lexicons forsubjectivity clues, intensifiers and valence shifters(Wilson et al, 2005), which are used for identify-ing opinion roots, modifiers and negation words.Opinion roots are identified as the positive andnegative subjectivity clues in the subjectivity lex-icon.
In the same manner, intensifiers and va-lence shifters of the type {negation, shiftneg} aremapped to modifiers and negation words.
Othermodifier candidates are adverbs, conjunctions andmodal verbs around opinion roots.
We considernon-words modifiers as well, e.g., punctuations,capitalization and repetition of opinion roots.
Ifthe opinion root is a noun, adjectives are also in-cluded into modifier sets.The automatic opinion annotation starts withlocating the continous subjectivity clue sequence.Once we find such a sequence and at least oneof the subjectivity clue is positive or negative, wesearch to the left up to 4 words for negation wordsand modifier candidates, and stop if encounteringanother opinion root.
Similarly, we search to the917right up to 3 unigrams for modifiers and stop ifwe find negation words or any other opinion roots.The prior polarity of the subjectivity sequence isdetermined by the polarity of the last subjectivityclue with either positive or negative polarity in thesequence.
The other subjectivity clues in the samesequence are treated as modifiers.2.4 Adaptation to Domain-Dependent DataThe adaptation of the learned (domain-independent) opinion scores to the targetdomain and the integration of domain-dependentunigrams is done in a second ridge-regressiontask.
Note that this is a simpler problem thantypical domain-adaptation, since we already knowfrom the sentiment lexicon which are the domain-independent features.
Additionally, its relativelyeasy to obtain a large mixed-domain corpus forreliable estimation of domain-independent opin-ion scores (e.g., use all Amazon product reviews).Furthermore, we need a domain-adaptation stepsince domain-dependent and domain-independentdata have generally different rating distributions.The differences are mainly reflected in theintercept of the regression function (estimatedas the mean of the ratings).
This means thatwe need to scale the positive/negative mean ofthe opinion scores differently before using itfor prediction on domain-dependent reviews.Moreover, other statistics further characterize theopinion score distribution.
We use the varianceof opinion scores to capture the reliability ofthe mean, multiplied by the negative sign of themean to show how much it strengthens/weakensthe estimation of the mean.
The mean score ofthe dominant polarity (major exp) is also usedto reduce the influence of outliers.
Becausepositive and negative means should be scaleddifferently, we represent positive and negativevalues of the mean and major exp as 4 differentfeatures.
Together with variance, they are the 5statistics of the opinion score distribution.
Thesecond learning step on opinion score statisticsand domain-dependent unigrams as features,re-weights the importance of domain-independentand domain-dependent information according tothe target domain bias.3 Experimental SetupWe performed experiments on three target do-mains of Amazon reviews: books, movies(DVDs), and music (CDs).
For each domain,we use ca.
8000 Amazon reviews for evalua-tion; an additional set of ca.
4000 reviews arewithheld for parameter tuning (regularization pa-rameter, etc.).
For learning weights for domain-independent opinions, we use a mixed-domaincorpus of ca.
350,000 reviews from Amazon(electronics, books, dvds, etc.
); this data is dis-joint from the test sets and contains no reviewsfrom the music domain.
In order to learn un-biased scores, we select about the same numberof positive and negative reviews (where reviewswith more/less than 3 stars are regarded as posi-tive/negative).
The regularization parameters usedfor this corpus are tuned on withheld data with ca.6000 thematically mixed reviews.1.We compare our method, subsequently referredto as CRR-BoO (Constrained Ridge Regressionfor Bag-of-Opinions), to a number of alternativestate-of-the-art methods.
These competitors arevaried along two dimensions: 1) feature space,and 2) training set.
Along the first dimension,we consider a) unigrams coined uni, b) unigramsand bigrams together, coined uni+bi, c) variable-length n-grams coined n-gram, d) the opinionmodel by (Liu and Seneff, 2009) coined CLO (cu-mulative linear offset model).
As learning pro-cedure, we use ridge regression for a), b), andd), and bounded cyclic regression, coined BCR,for c).
Along the second - orthogonal - di-mension, we consider 3 different training sets:i) domain-dependent training set coined DD, ii)the large mixed-domain training set coined MD,iii) domain-dependent training set and the largemixed-domain training set coined DD+MD.
Forthe DD+MD training set, we apply our two stageapproach for CRR-BoO and CLO, i.e., we usethe mixed-domain corpus for learning the opinionscores in the first stage, and integrate unigramsfrom DD in a second domain-adaptation stage.We train the remaining feature models directly onthe combination of the whole mixed-domain cor-1All datasets are available fromhttp://www.mpi-inf.mpg.de/?lqu918feature models uni uni+bi n-gram CLO CRR-BoODDbook 1.004 0.961 0.997 1.469 0.942dvd 1.062 1.018 1.054 1.554 0.946music 0.686 0.672 0.683 0.870 0.638MDbook 1.696 1.446 1.643 1.714 1.427dvd 1.919 1.703 1.858 1.890 1.565music 2.395 2.160 2.340 2.301 1.731DD+MDbook 1.649 1.403 1.611 1.032 0.884dvd 1.592 1.389 1.533 1.086 0.928music 1.471 1.281 1.398 0.698 0.627Table 1: Mean squared error for rating prediction methods on Amazon reviews.pus and the training part of DD.The CLO model is adapted as follows.
Sincebags-of-opinions generalize CLO, adjectives andadverbs are mapped to opinion roots and modi-fiers, respectively; negation words are treated thesame as CLO.
Subsequently we use our regressiontechnique.
As Amazon reviews do not contain proand con entries, we learn from the entire review.For BCR, we adapt the variable-length n-gramsmethod of (Ifrim et al, 2008) to elastic-net-regression (Friedman et al, 2008) in order to ob-tain a fast regularized regression algorithm forvariable-length n-grams.
We search for signifi-cant n-grams by incremental expansion in back-ward direction (e.g., expand bad to not bad).
BCRpursues a dense solution for unigrams and a sparsesolution for n-grams.
Further details on the BCRlearning algorithm will be found on a subsequenttechnical report.As for the regression techniques, we showonly results with ridge regression (for all fea-ture and training options except BCR).
It outper-formed -support vector regression (SVR) of lib-svm (Chang and Lin, 2001), lasso (Tibshirani,1996), and elastic net (Zou and Hastie, 2005) inour experiments.4 Results and DiscussionTable 1 shows the mean square error (MSE) fromeach of the three domain-specific test sets.
The er-ror is defined as MSE = 1N?Ni=1(f(xi) ?
yi)2.The right most two columns of the table show re-sults for the full-fledge two-stage learning for ourmethod and CLO, with domain-dependent weightlearning and the domain adaptation step.
Theother models are trained directly on the giventraining sets.
For the DD and DD+MD train-ing sets, we use five-fold cross-validation on thedomain-specific sets.
For the MD training set, wetake the domain-specific test sets as hold-out datafor evaluation.Table 1 clearly shows that our CRR-BoOmethod outperforms all alternative methods by asignificant margin.
Most noteworthy is the mu-sic domain, which is not covered by the mixed-domain corpus.
As expected, unigrams only per-form poorly, and adding bigrams leads only tomarginal improvements.
BCR pursues a densesolution for unigrams and a sparse solution forvariable-length n-grams, but due to the sparsityof occurence of long n-grams, it filters out manyinteresting-but-infrequent ngrams and thereforeperforms worse than the dense solution of theuni+bi model.
The CLO method of (Liu and Sen-eff, 2009) shows unexpectedly poor performance.Its main limitation is the assumption that opinionscores are identical within one document.
Thisdoes not hold in documents with mixed opinionpolarities.
It also results in conflicts for opinioncomponents that occur in both positive and nega-tive documents.
In contrast, CRR-BoO naturallycaptures the mixture of opinions as a bag of pos-itive/negative scores.
We only require that themean of opinion scores equals the overall docu-ment rating.The right most column of Table 1 shows thatour method can be improved by learning opinionscores from the large mixed-domain corpus.
How-919opinion scoregood 0.18recommend 1.64most difficult -1.66but it gets very good!
2.37would highly recommend 2.73would not recommend -1.93Table 2: Example opinions learned from the Ama-zon mixed-domain corpus.ever, the high error rates of the models learned di-rectly on the MD corpus show that direct trainingon the mixed-domain data can introduce a signifi-cant amount of noise into the prediction models.Although the noise can be reduced by learningfrom MD and DD together, the performance isstill worse than when learning directly from thedomain-dependent corpora.
Additionally, whenthe domain is not covered by the mixed-domaincorpus (e.g., music), the results are even worse.Thus, the two stages of our method (learningdomain-independent opinion scores plus domain-adaptation) are decisive for a good performance,and the sentiment-lexicon-based BoO model leadsto robust learning of domain-independent opinionscores.Another useful property of BoO is its high in-terpretability.
Table 2 shows example opinionscores learned from the mixed-domain corpus.We observe that the scores corelate well with ourintuitive interpretation of opinions.Our CRR-BoO method is highly scalable.Excluding the preprocessing steps (same forall methods), the learning of opinion compo-nent weights from the ca.
350,000 domain-independent reviews takes only 11 seconds.5 Related WorkRating prediction is modeled as an ordinal re-gression problem in (Pang and Lee, 2005; Gold-berg and Zhu, 2006; Snyder and Barzilay, 2007).They simply use the bag-of-words model with re-gression algorithms, but as seen previously thiscannot capture the expressive power of phrases.The resulting models are not highly interpretable.Baccianella et al (2009) restrict the n-grams tothe ones having certain POS patterns.
However,the long n-grams matching the patterns still sufferfrom sparsity.
The same seems to hold for sparsen-gram models (BCR in this paper) in the spiritof Ifrim et al (2008).
Although sparse n-grammodels can explore arbitrarily large n-gram fea-ture spaces, they can be of little help if the n-gramsof interests occur sparsely in the datasets.Since our approach can be regarded as learninga domain-independent sentiment lexicon, it is re-lated to the area of automatically building domain-independent sentiment lexicons (Esuli and Sebas-tiani, 2006; Godbole et al, 2007; Kim and Hovy,2004).
However, this prior work focused mainlyon the opinion polarity of opinion words, neglect-ing the opinion strength.
Recently, the lexiconbased approaches were extended to learn domain-dependent lexicons (Kanayama and Nasukawa,2006; Qiu et al, 2009), but these approachesalso neglect the aspect of opinion strength.
Ourmethod requires only the prior polarity of opinionroots and can thus be used on top of those meth-ods for learning the scores of domain-dependentopinion components.
The methods proposed in(Hu and Liu, 2004; Popescu and Etzioni, 2005b)can also be categorized into the lexicon basedframework because their procedure starts with aset of seed words whose polarities are propagatedto other opinion bearing words.6 Conclusion and Future WorkIn this paper we show that the bag-of-opinions(BoO) representation is better suited for captur-ing the expressive power of n-grams while at thesame time overcoming their sparsity bottleneck.Although in this paper we use the BoO represen-tation to model domain-independent opinions, webelieve the same framework can be extended todomain-dependent opinions and other NLP appli-cations which can benefit from modelling n-grams(given that the n-grams are decomposable in someway).
Moreover, the learned model can be re-garded as a domain-independent opinion lexiconwith each entry in the lexicon having an associatedscore indicating its polarity and strength.
This inturn has potential applications in sentiment sum-marization, opinionated information retrieval andopinion extraction.920ReferencesBaccianella, S., A. Esuli, and F. Sebastiani.
2009.Multi-facet rating of product reviews.
In ECIR.Springer.Chang, C.C.
and C.J.
Lin, 2001.
LIBSVM: alibrary for support vector machines.
Soft-ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Esuli, A. and F. Sebastiani.
2006.
Sentiwordnet: Apublicly available lexical resource for opinion min-ing.
In LREC, pages 417?422.Friedman, J., T. Hastie, and R. Tibshirani.
2008.Regularization paths for generalized linear modelsvia coordinate descent.
Technical report, Techni-cal Report, Available at http://www-stat.
stanford.edu/jhf/ftp/glmnet.
pdf.Ganu, G., N. Elhadad, and A. Marian.
2009.
Beyondthe stars: Improving rating predictions using reviewtext content.
In 12th International Workshop on theWeb and Databases.Godbole, Namrata, Manjunath Srinivasaiah, andSteven Skiena.
2007.
Large-scale sentiment anal-ysis for news and blogs.
In ICWSM.Goldberg, A.
B. and X.J.
Zhu.
2006.
See-ing stars when there aren?t many stars: Graph-based semi-supervised learning for sentiment cat-egorization.
In HLT-NAACL 2006 Workshop onTextgraphs: Graph-based Algorithms for NaturalLanguage Processing.Hu, M.Q.
and B. Liu.
2004.
Mining and summarizingcustomer reviews.
In CIKM, pages 168?177.
ACMNew York,USA.Ifrim, G., G. Bakir, and G. Weikum.
2008.
Fast logis-tic regression for text categorization with variable-length n-grams.
In KDD, pages 354?362, NewYork,USA.
ACM.Kanayama, H. and T. Nasukawa.
2006.
Fully auto-matic lexicon expansion for domain-oriented senti-ment analysis.
In EMNLP, pages 355?363.Kennedy, A. and D. Inkpen.
2006.
Sentiment classi-fication of movie reviews using contextual valenceshifters.
Computational Intelligence, 22(2):110?125.Kim, S.M.
and E. Hovy.
2004.
Determining the senti-ment of opinions.
In COLING, pages 1367?1373.Lerman, K., S. Blair-Goldensohn, and R. McDonald.2009.
Sentiment summarization: Evaluating andlearning user preferences.
In EACL, pages 514?522.ACL.Liu, J.J. and S. Seneff.
2009. Review SentimentScoring via a Parse-and-Paraphrase Paradigm.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages161?169.
ACL.Luo, Z.Q.
and Q. Tseng.
1992.
On the convergence ofthe coordinate descent method for convex differen-tiable minimization.
Journal of Optimization The-ory and Applications, 72(1):7?35.Pang, B. and L. Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
In ACL, page 124.
ACL.Pang, B. and L. Lee.
2008.
Opinion mining and senti-ment analysis.
Foundations and Trends in Informa-tion Retrieval, 2(1-2):1?135.Popescu, A.M. and O. Etzioni.
2005a.
Extractingproduct features and opinions from reviews.
InHLT/EMNLP, volume 5, pages 339?346.
Springer.Popescu, A.M. and O. Etzioni.
2005b.
Extractingproduct features and opinions from reviews.
In Pro-ceedings of HLT/EMNLP, volume 5, pages 339?346.
Springer.Qiu, G., B. Liu, J.J. Bu, and C. Chen.
2009.
Ex-panding Domain Sentiment Lexicon through Dou-ble Propagation.
In IJCAI.Snyder, B. and R. Barzilay.
2007.
Multiple as-pect ranking using the good grief algorithm.
InNAACL/HLT, pages 300?307.Tibshirani, R. 1996.
Regression shrinkage and selec-tion via the lasso.
Journal of the Royal StatisticalSociety.
Series B (Methodological), 58(1):267?288.Titov, I. and R. McDonald.
2008a.
A joint model oftext and aspect ratings for sentiment summarization.In HLT/ACL, pages 308?316.Titov, I. and R. McDonald.
2008b.
Modeling onlinereviews with multi-grain topic models.
In WWW,pages 111?120.
ACM.Wilson, T., J. Wiebe, and P. Hoffmann.
2005.
Recog-nizing contextual polarity in phrase-level sentimentanalysis.
In HLT/ACL, pages 347?354.Zhai, C. X. and J. Lafferty.
2004.
A study of smooth-ing methods for language models applied to infor-mation retrieval.
ACM Transactions on InformationSystems, 22(2):179?214.Zou, H. and T. Hastie.
2005.
Regularization andvariable selection via the elastic net.
Journal ofthe Royal Statistical Society Series B(StatisticalMethodology), 67(2):301?320.921
