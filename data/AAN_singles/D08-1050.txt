Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 475?484,Honolulu, October 2008. c?2008 Association for Computational LinguisticsAdapting a Lexicalized-Grammar Parser to Contrasting DomainsLaura Rimell and Stephen ClarkOxford University Computing LaboratoryWolfson Building, Parks RoadOxford, OX1 3QD, UK{laura.rimell,stephen.clark}@comlab.ox.ac.ukAbstractMost state-of-the-art wide-coverage parsersare trained on newspaper text and suffer aloss of accuracy in other domains, makingparser adaptation a pressing issue.
In thispaper we demonstrate that a CCG parser canbe adapted to two new domains, biomedicaltext and questions for a QA system, by us-ing manually-annotated training data at thePOS and lexical category levels only.
This ap-proach achieves parser accuracy comparableto that on newspaper data without the needfor annotated parse trees in the new domain.We find that retraining at the lexical categorylevel yields a larger performance increase forquestions than for biomedical text and analyzethe two datasets to investigate why differentdomains might behave differently for parseradaptation.1 IntroductionMost state-of-the-art wide-coverage parsers arebased on the Penn Treebank (Marcus et al, 1993),making such parsers highly tuned to newspaper text.A pressing question facing the parsing communityis how to adapt these parsers to other domains, suchas biomedical research papers and web pages.
A re-lated question is how to improve the performanceof these parsers on constructions that are rare in thePenn Treebank, such as questions.
Questions areparticularly important since a question parser is acomponent in most Question Answering (QA) sys-tems (Harabagiu et al, 2001).In this paper we investigate parser adaptation inthe context of lexicalized grammars, by using aparser based on Combinatory Categorial Grammar(CCG) (Steedman, 2000).
A key property of CCG isthat it is lexicalized, meaning that each word in asentence is associated with an elementary syntacticstructure.
In the case of CCG this is a lexical cate-gory expressing subcategorization information.
Weexploit this property of CCG by performing manualannotation in the new domain, but only up to thislevel of representation, where the annotation can becarried out relatively quickly.
Since CCG lexical cat-egories are so expressive, many of the syntactic char-acteristics of a domain are captured at this level.The two domains we consider are the biomedicaldomain and questions for a QA system.
We use theterm ?domain?
somewhat loosely here, since ques-tions are best described as a particular set of syn-tactic constructions, rather than a set of documentsabout a particular topic.
However, we consider ques-tion data to be interesting in the context of domainadaptation for the following reasons: 1) there arefew examples in the Penn Treebank (PTB) and soPTB parsers typically perform poorly on them; 2)questions form a fairly homogeneous set with re-spect to the syntactic constructions employed, andit is an interesting question how easy it is to adapt aparser to such data; and 3) QA is becoming an impor-tant example of NLP technology, and question pars-ing is an important task for QA systems.The CCG parser we use (Clark and Curran, 2007b)makes use of three levels of representation: one, aPOS tag level based on the fairly coarse-grained POStags in the Penn Treebank; two, a lexical categorylevel based on the more fine-grained CCG lexical cat-egories, which are assigned to words by a CCG su-475pertagger; and three, a hierarchical level consistingof CCG derivations.
A key idea in this paper, follow-ing a pilot study in Clark et al (2004), is to performmanual annotation only at the first two levels.
Sincethe lexical category level consists of sequences oftags, rather than hierarchical derivations, the anno-tation can be performed relatively quickly.For the biomedical and question domains wemanually annotated approximately 1,000 and 2,000sentences, respectively, with CCG lexical categories.We also created a gold standard set of grammati-cal relations (GR) in the Stanford format (de Marn-effe et al, 2006), using 500 of the questions.
Forthe biomedical domain we used the BioInfer corpus(Pyysalo et al, 2007a), an existing gold-standard GRresource also in the Stanford format.
We evaluatedthe parser on both lexical category assignment andrecovery of GRs.The results show that the domain adaptation ap-proach used here is successful in two very differentdomains, achieving parsing accuracy comparable tostate-of-the-art accuracy for newspaper text.
The re-sults also show, however, that the two domains havedifferent profiles with regard to the levels of repre-sentation used by the parser.
We find that simply re-training the POS tagger used by the parser leads to alarge improvement in performance for the biomed-ical domain, and that retraining the CCG supertag-ger on the annotated biomedical data improves theperformance further.
For the question data, retrain-ing just the POS tagger also improves parser perfor-mance, but retraining the supertagger has a muchgreater effect.
We perform some analysis of the twodatasets in order to explain the different behaviourswith regard to porting the CCG parser.2 The CCG ParserThe CCG parser is described in detail in Clark andCurran (2007b) and so we provide only a brief de-scription.
The stages in the CCG parsing pipeline areas follows.
First, a maximum entropy POS taggerassigns a single POS tag to each word in a sentence.POS tags are fairly coarse-grained grammatical la-bels indicating part-of-speech; the Penn Treebankset, used here, contains approximately 50 labels.Second, a maximum entropy supertagger assignsCCG lexical categories to the words in the sentence.Lexical categories can be thought of as fine-grainedPOS tags expressing subcategorization information,i.e.
information about the argument frame of theword.
There are 425 categories in the set used by theCCG parser.
Supertagging was originally developedfor Lexicalized Tree Adjoining Grammar (Banga-lore and Joshi, 1999), but has been particularly suc-cessful for wide-coverage CCG parsing (Clark andCurran, 2007b).
Rather than assign a single categoryto each word, the supertagger operates as a multi-tagger, sometimes assigning more than one categoryif the context is not sufficiently discriminating tosuggest a single tag (Curran et al, 2006).
Sincethe taggers have linear time complexity, the first twostages can be performed extremely quickly.Finally, the parsing stage combines the lexical cat-egories, using a small set of combinatory rules thatare part of the grammar of CCG, and builds a packedchart representation containing all the derivationswhich can be built from the lexical categories.
TheViterbi algorithm efficiently finds the highest scor-ing derivation from the packed chart, using a log-linear model to score the derivations.
The grammarand training data for the newspaper version of theCCG parser are obtained from CCGbank (Hocken-maier and Steedman, 2007), a CCG version of thePenn Treebank.The aspect of the pipeline which is most relevantto this paper is the supertagging phase.
Figure 1gives an example sentence from each target domain,with the CCG lexical category assigned to each wordshown below the word, and the POS tag to the right.Note that the categories contain a significant amountof grammatical information, in particular subcatego-rization information.
The verb acts in the biomedi-cal sentence, for example, looks for a prepositionalphrase (PP, as a linkage protein) to its right and anoun phrase (NP, Talin) to its left, with the resultingcategory a declarative sentence (S[dcl]).Bangalore and Joshi (1999) refer to supertaggingas almost parsing, because once the correct lexicalcategories have been assigned, the parser is left withmuch less work to do.
The CCG supertagger is notable to assign a single category to each word withextremely high accuracy ?
hence the need for it tooperate as a multi-tagger ?
but even in multi-taggermode it dramatically reduces the ambiguity passedthrough to the parser (Clark and Curran, 2007b).476Talin|NN perhaps|RB acts|VBZ as|IN a|DT linkage|NN protein|NN .|.NP (S\NP)/(S\NP) (S [dcl ]\NP)/PP PP/NP NP [nb]/N N /N N .What|WDT king|NN signed|VBD the|DT Magna|NNP Carta|NNP ?|.
(S [wq ]/(S [dcl ]\NP))/N N (S [dcl ]\NP)/NP NP [nb]/N N /N N .Figure 1: Example sentences with lexical category assignment.The parser has been evaluated on DepBank (Kinget al, 2003), using the GR scheme of Briscoe etal.
(2006), and it scores 82.4% labelled precisionand 81.2% labelled recall overall (Clark and Curran,2007a).
Section 4.4 describes how the CCG depen-dencies can be mapped into the Stanford GR scheme(de Marneffe et al, 2006) and gives the results ofevaluating the parser on biomedical and question GRresources.The CCG parser is particularly well suited to thebiomedical and question domains.
First, use of CCGallows recovery of long-distance dependencies.
Inthe sentence What does target heart rate mean?, theword What is an underlying object of the verb mean.The parser recovers this information despite the dis-tance between the two words.
This capability iscrucial for question parsing, and also useful in thebiomedical field for extraction of relationships be-tween biological entities.
Additionally, the speed ofthe parser (tens of sentences per second) is usefulfor the large volumes of biomedical data that requireprocessing for biomedical text mining.3 ApproachOur approach to domain adaptation is to target thecoarser-grained, less syntactically complex, levels ofrepresentation used by the parser, and to train newmodels with manually annotated data at these levels.The motivation for this approach is twofold.
First,accuracy at each stage of the pipeline depends on ac-curacy at the earlier stages.
If the POS tagger assignsincorrect tags, it is unlikely that the supertagger willbe able to recover and produce the correct lexicalcategories, since it relies heavily on POS tags as fea-tures.
Without the correct categories, the parser inturn will be unable to find a correct parse.In the sentence What year did the Vietnam Warend?, the newspaper-trained POS tagger incorrectlyassigns the POS tag NN (common noun) to the verbend, since verb-final sentences are atypical for thePTB.
As a result, the supertagger is virtually cer-tain (greater than 99% probability) that the correctCCG lexical category for end is N (noun).
The parserthen assigns the Vietnam War end the structure of anoun phrase, and chooses an unusual subcategoriza-tion frame for did in which it takes three arguments:What, year, and the Vietnam War end.In the sentence How many siblings doesshe have?, on the other hand, the supertag-ger assigns an incorrect category to the wordHow despite it having the correct POS tag(WRB for wh-adverb).
The correct category is((S [wq ]/(S [q ]/NP))/N )/(NP/N ), which takesmany (category NP/N ) and siblings (category N )as arguments.
Instead it is tagged as S [wq ]/S [q ],the category for a sentential adverb (i.e.
the man-ner reading of how), which prevents a correct parse.Our intention was that creating new training data atthe lower levels of representation would improve theaccuracy of the POS tagger and supertagger in thetarget domains, thereby improving the accuracy oflater stages in the pipeline as well.The second motivation for our approach is to re-duce annotation overhead.
Full syntactic deriva-tions are costly to produce by hand.
POS tags, how-ever, are relatively easy to annotate; even an out-of-domain tagger will provide a good starting point,and manual correction is quick, especially in a do-main without much unfamiliar vocabulary.
CCG lex-ical categories require more expertise, but our ex-perience shows that an out-of-domain supertaggercan again provide a starting point for correction, andsince the annotation is flat rather than hierarchical,we hypothesize that it is not as difficult or time-consuming as annotation of full derivations.Our adaptation approach has been partially ex-plored in previous work which targets one or anotherof the different levels of representation.477Lease and Charniak (2005) obtained an improve-ment in the accuracy of the Charniak (2000) parser,as well as POS tagging accuracy, when applied tothe biomedical domain, by training a new POS tag-ger model with a combination of newspaper andbiomedical data.
The parser improvement was duesolely to the new POS tagger, without retraining theparser model.
Since the Charniak parser does notuse a lexicalized grammar with an intermediate levelof representation, any further improvements wouldhave to come from the parser model itself.Clark et al (2004) obtained an improvement inCCG supertagging accuracy for What-questions bytraining a new supertagger model with a combina-tion of newspaper and question data annotated withCCG lexical categories.
Because a question resourceannotated with GRs was not available, they did notperform a parser evaluation, and the effects of thePOS tagging level were not compared to the lexi-cal category level.
In this paper, we extend the pi-lot experiments performed by Clark et al (2004) infour ways.
First, we use a larger corpus of TRECquestions covering additional question types, thusextending the experiments to the question domainmore broadly, as well as to the biomedical domain.Second, we create a gold standard GR resource en-abling a full parser evaluation on question data.Third, we show that the POS level is important foradaptation, reinforcing the work of Lease and Char-niak (2005).
A key finding of the present paper isthat the combination of retraining at the POS tag andlexical category levels provides additional improve-ments beyond those gained by retraining at a singlelevel.
Finally, we provide analysis comparing theadaptation methodology for question and biomedi-cal data.Hara et al (2007) followed a similar approach toClark et al (2004), using the parser of Ninomiyaet al (2006), a version of the Enju parser (Miyaoand Tsujii, 2005).
Enju is based on HPSG, a lex-icalized grammar formalism.
They obtained an im-provement in parsing accuracy in the biomedical do-main by training a new probabilistic model of lexi-cal entry assignments on a combination of newspa-per and biomedical data without changing the orig-inal newspaper-trained parsing model.
Hara et al(2007) did not consider the role of POS tagging.
Thelexical category data in Hara et al (2007) was de-rived from a gold standard treebank, while the an-notation of lexical categories in this paper was per-formed without reference to gold standard syntacticderivations.Judge et al (2006) produced a corpus of 4,000questions annotated with syntactic trees, and ob-tained an improvement in parsing accuracy forBikel?s reimplementation of the Collins parser(Collins, 1997) by training a new parser model witha combination of newspaper and question data.
Ourapproach differs in retraining only at the levels ofrepresentation below parse trees.4 Experiments and Results4.1 ResourcesWe have used a combination of existing resourcesand new, manually annotated data.
The baseline POStagger, supertagger, and parser are trained on WSJSections 02-21 of CCGbank.
The baseline perfor-mance at each level of representation is on WSJ Sec-tion 00 of CCGbank, which contains 1913 sentencesand approximately 45,000 words.For the biomedical domain, we trained the POStagger on gold-standard POS tags from GENIA (Kimet al, 2003), a corpus of 2,000 MEDLINE abstractscontaining a total of approximately 18,500 sentencesand 440,000 words.
We also annotated the first1,000 sentences of GENIA with CCG lexical cate-gories.
This set of 1,000 sentences, containing ap-proximately 27,000 words, was used for POS taggerevaluation and for development and evaluation of anew supertagger model.
For parser evaluation, weused BioInfer (Pyysalo et al, 2007a), a corpus ofMEDLINE abstracts (on a different topic from thosein GENIA) containing 1,100 sentences, and with syn-tactic dependencies encoded as grammatical rela-tions in the Stanford GR format.
We used the sameevaluation set of 500 sentences as in Pyysalo et al(2007b), and the remaining 600 for development ofthe mapping to Stanford format.
Two parsers havealready been evaluated on BioInfer, which makes ita useful resource for comparative evaluation.For the question domain, we extended the datasetdescribed in Clark et al (2004).
That dataset con-tained 1,171 questions beginning with the wordWhat, from the TREC 9-12 competitions (2000-2003), manually POS tagged and annotated with478CCG lexical categories.
We annotated all the addi-tional TREC question types and improved the exist-ing annotation, for a total of 1,828 sentences.
We ad-ditionally annotated a random subset of 500 of thesewith GRs in the Stanford format.
This subset servedas our evaluation set at all levels of representation.
Itcontains approximately 4,000 words, fewer than theother domains because of the significantly shortersentence lengths of typical questions.
The remain-ing 1,328 sentences were used as training data.
Aset of about a dozen sentences from the evaluationand training sets were used to develop the mappingto Stanford format for lexical categories not occur-ring in the biomedical data.4.2 POS taggerWe began by training new models at the POS taglevel of representation.
All datasets use the PTBtagset.
As a baseline, we used the original WSJ 02-21 model on the biomedical and question datasets.For comparison we also evaluated on Section 00 us-ing the WSJ-trained model.For the question data, the new POS tagger wastrained on CCGbank Sections 02-21 plus ten copiesof the 1,328 training sentences.
The WSJ data pro-vides additional robustness and wide grammaticalcoverage, and the weighting factor of ten was chosenin preliminary experiments to prevent the newspaperdata from ?overwhelming?
the question data.
Forthe biomedical data, the new POS tagger was trainedon the full GENIA corpus, minus the first 1,000 sen-tences.
GENIA is large enough that combination withthe newspaper data was not needed.Table 1 gives the results.
For both of the new do-mains the performance of the WSJ model decreasedcompared to Section 00, but the retrained model per-formed at least as well as the WSJ model did on 00.1Improving the POS tagger performance has a posi-tive effect on the performance of the supertagger andparser, which will be discussed in Sections 4.3-4.4.1Since GENIA does not use the proper noun tag, NNP, fornames of genes and other biomedical entities, all figures inthis paper collapse the NNP-NN distinction where relevant forbiomedical data.
The question data uses NNP and the distinc-tion is not collapsed.WSJ 02-21 RetrainedSec.
00 96.7 ?Qus 92.2 97.1Bio 93.4 98.7Table 1: POS tagger accuracy (%) for original and re-trained models.OrigpipelineRetrainedPOSRetrainedPOS andsuperSec.
00 91.5 ?
?Qus 71.6 74.0 92.1Bio 89.0 91.2 93.0Table 2: Supertagging accuracy (%) and the effect of re-training the POS model and the supertagger model.4.3 SupertaggerWe next trained new models at the CCG lexical cat-egory level.
The training data consisted of manu-ally annotated biomedical and question sentences;specifically, lexical categories were automaticallyassigned by the original parsing pipeline and thenmanually corrected.
Whenever possible we usedcategories from the parser?s original set of 425, al-though occasionally it was necessary to use a newcategory for a syntactic construction not occurringin CCGbank Sections 02-21.
(The parser can be con-figured to recognize additional categories.)
Questiondata in particular requires the use of categories thatare rare or unseen in CCGbank.For the questions, the new supertagger model,like the POS tagger, was trained on WSJ 02-21 plusten copies of the 1,328 training sentences.
For thebiomedical data, a ten-fold cross-validation was per-formed, training each supertagger model on WSJ 02-21 plus ten copies of 90% of the 1,000 annotatedsentences.
Table 2 gives the supertagger accuracywith and without the retrained POS and supertaggermodels.
The figure for the retrained biomedical su-pertagger is the average of the ten-fold split.The results show an improvement in accuracy oflexical category assignment solely from retrainingthe POS tagger, and an additional improvement fromretraining the supertagger.
Supertagger accuracy forthe two domains with a retrained supertagger wascomparable, and in both cases was at least as high479What car company invented the Edsel?
(nsubj invented company)(det Edsel the)(dobj invented Edsel)(det company What)(nn company car)Figure 2: Example of grammatical relations in the Stan-ford grammatical relation format.as for the original pipeline on Section 00.
The ques-tion data started from a much lower baseline figure,however.4.4 ParserWe evaluated the parser on the 500 questions anno-tated with Stanford GRs and on the 500 evaluationsentences from the BioInfer corpus.
We used theoriginal newspaper pipeline, a pipeline with a re-trained POS tagger, and a pipeline with both a re-trained POS tagger and supertagger.In order to perform these evaluations we devel-ooped a mapping from the parser?s native CCG syn-tactic dependencies to GRs in the Stanford format.The mapping was based on the same principles asthe mapping that produces GR output in the styleof Briscoe et al (2006).
These principles are dis-cussed in detail in Clark and Curran (2007a); insummary, the argument slots in the CCG dependen-cies are mapped to argument slots in Stanford GRs,a fairly complex, many-to-many mapping.
An ad-ditional post-processing script applies some manu-ally developed rules to bring the output closer to theStanford format.
Figure 2 gives an example of Stan-ford GRs, where the label of the relation is followedby two arguments, head and dependent.Table 3 gives the results of the parser evaluationon GRs.
Since the parser model was not retrained,the improvements in accuracy are due solely to thenew POS and supertaggers.
The results are given asan F-score over labelled GRs.2The F-scores given in Table 3 are only for sen-tences for which a parse was found.
However, therewere also improvements in coverage with the re-trained models.
For the question data, parser cov-2Only GRs at the lowest level of the Stanford hierarchy wereconsidered in the evaluation; more generic relations such as de-pendent were not considered.Orig POSand superNew POS New POSand superQus 64.4 69.4 86.6BioInfer 76.0 80.4 81.5Table 3: Parser F-score on grammatical relations and theeffect of retraining the POS and supertagger models.erage was 94% for the original pipeline and thepipeline with just the retrained POS tagger, and99.6% with the retrained POS and supertaggers.
Forthe biomedical data, coverage was 97.2% for theoriginal pipeline, 99.0% for the pipeline with the re-trained POS tagger, and 99.8% for the pipeline withthe retrained POS and supertaggers.The final accuracy for both domains is in the samerange as that of the original parser on newspaperdata (81.8%) (Clark and Curran, 2007b), althoughthe results are not directly comparable, since thenewspaper resource uses a different GR scheme.
Forthe BioInfer corpus, the final accuracy is also inline with results reported in the literature for otherparsers (Pyysalo et al, 2007b).
(No comparable GRresults are available for questions.)
A score in thisrange is thought to be near the upper bound whenevaluating a CCG parser on GRs, since some loss isinherent in the mapping to GRs (Clark and Curran,2007a).5 AnalysisAlthough domain adaptation was successful for bothof our target domains, the impact of the differentlevels of representation on parsing accuracy was notuniform.
Table 3 shows that retraining the POS tag-ger accounted for a greater proportion of the im-provement on biomedical data, while retraining thesupertagger accounted for a much greater proportionon questions.
In this section we discuss some of thedifferences between the domains which may havecontributed to their behaviour in this regard, withthe intention of highlighting attributes that may berelevant for domain adaptation in general.Informally, we believe that the main differencebetween newspaper and biomedical text is vocabu-lary, and that their syntactic structures are essentiallysimilar (with some isolated exceptions, such as morefrequent use of parentheses and comma-separated480Tag Errors Freq confusedQusWDT 129 WPVB 46 NN, VBPNNP 33 JJ, NNNN 32 JJ, NNPBioNN 801 JJ, CDJJ 268 NN, VBNVBN 113 JJ, VBDFW 95 NN, INTable 4: Tags with the most frequent errors by thenewspaper-trained POS tagger and the tags they weremost frequently confused with.lists in biomedical text).
Once the POS tagger hadbeen retrained for biomedical text, accounting forunfamiliar vocabulary, the original supertagger al-ready performed well.
The main difference betweennewspaper and question data, on the other hand, issyntactic.
Retraining the POS tagger for questionstherefore had less effect; even with the correct POStags the supertagger was unable to assign the correctlexical categories.
Since lexical categories encodesyntactic information, the domain with the more di-vergent syntax is likely to benefit most from newtraining data at the lexical category level.5.1 POS taggerTable 1 showed that the accuracy of the newspaper-trained POS tagger was in the same range for bothbiomedical and question data.
However, the distri-bution of errors was different.
Table 4 shows the tagswith the most frequent errors, accounting for about75% of all POS tag errors in each domain, and thetags that they were most frequently confused with.For the question data, the most frequent error wastagging a wh-determiner (WDT) as a wh-pronoun(WP).
A determiner combines with a noun to forma noun phrase, as in the sentence What Liverpoolclub spawned the Beatles?.
A pronoun, on the otherhand, is a noun phrase in its own right, as in Whatare the colors of the German flag?.
This tagger er-ror arises from the fact that the word What occursonly once in WSJ 02-21 with a WDT tag.
The sec-ond most common error was on bare verbs (VB), be-cause the newspaper model gives a low probabilityof bare verbs occurring in sentence-final position, ornot directly following an auxiliary.Unknown wordrateUnknownword-POS rateSec.
00 3.8 4.4Qus 7.5 8.3Bio 23.6 25.3Table 5: Unknown word rate and word-POS tag pair rate(%) compared to WSJ 02-21 (by token).For the biomedical data, the most frequent errorsby far were confusions of noun (NN) and adjective(JJ).
This is most likely due to the prevalence of longnoun phrases in the biomedical data, such as majorhistocompatibility complex class II molecules.
Al-though the words preceding the head noun are rec-ognized as nominal modifiers, the classification intonoun and adjective is difficult, especially when theword is previously unseen.
There were also prob-lems distinguishing verbal past participles (VBN)from adjectives (JJ) and identifying foreign words(FW), for example the phrase in vitro.The fact that the newspaper-trained POS taggerperformed comparably in the two target domains(Table 1) is surprising, since their lexical profilesare quite different.
Lease and Charniak (2005) dis-cussed unknown word rate as a predictor of POStagger accuracy.
However, the unknown word ratecompared with WSJ 02-21 is much higher for thebiomedical data than for the question data, as seenin Table 5.
(The unknown word rate for the questiondata is still higher than that for WSJ 00, which maybe due to the high proportion of proper nouns in thequestion data.
)Some POS tagging errors can be attributed, notto an unknown word, but to the use of a knownword with an unfamiliar tag (as in the WDT exam-ple above).
However, it is not the case that the ques-tion data contains many known words with unknowntags, since the rate of unknown word-tag pairs is alsomuch higher for biomedical than for question data,as seen in the rightmost column of Table 5.We do know that the newspaper-trained POS tag-ger performs better on unknown words for biomedi-cal (84.7%) than for question data (80.4%).
We hy-pothesize that the syntactic context of the biomed-ical data, being more similar to newspaper data,provides more information for the POS tagger in481WSJ 02-21 New train-ing sets3-gramsSec.
00 0.4 ?Qus 3.6 0.7Bio 0.7 0.55-gramsSec.
00 12.1 ?Qus 22.0 7.4Bio 10.9 9.2Table 6: Unknown POS n-gram rate (%) compared to WSJ02-21, and when in-domain data is added (by token).biomedical than in question data.
Syntactic differ-ences are discussed in the next section.5.2 SupertaggerTo quantify the syntactic distance between domains,we propose using the unknown POS n-gram ratecompared to WSJ Sections 02-21.
In the absence ofparse trees, POS n-grams can serve as a rough proxyfor the syntactic characteristics of a domain, reflect-ing local word order configurations.
POS n-gramshave been used in document modeling for text cate-gorization (Baayen et al, 1996; Argamon-Engelsonet al, 1998), but we believe our proposed use of theunknown POS n-gram rate is novel.The leftmost column of Table 6 gives the un-known POS trigram and 5-gram rates compared toWSJ Sections 02-21.
The rates for the biomedicaldata are quite similar to those for Section 00.
Thequestion data, however, shows higher rates of un-known POS n-grams.For both biomedical and question data, adding in-domain data to the training set makes its syntacticprofile more like that of the evaluation set.
The right-most column of Table 6 shows the unknown POS n-gram rates compared to the datasets used for trainingthe new supertagger models, consisting of WSJ 02-21 plus annotated question or biomedical data.
(Forthe biomedical data, the figures are averages of thesame ten-fold split used for evaluation).
It can beseen that adding in-domain data reduces the rate ofunknown POS n-grams to about the same level ob-served for newspaper text.The unknown POS n-gram rate requires POStagged data for a new domain and thus cannot be3-grams 5-gramsSec.
00 18 19Qus 8 5Bio 16 13Table 7: Number of the 20 most frequent POS n-gramsthat are also in the 20 most frequent POS n-grams of WSJSections 02-21.WSJ 02-21 Bio Qus.
?
?
JJ NN NN ?
?
WPIN DT NN IN JJ NN ?
WP VBZNN .
?
NN IN JJ ?
?
WDTDT JJ NN NNS IN NN WP VBZ DTTable 8: Four most frequent POS trigrams for WSJ 02-21; four most frequent POS trigrams for biomedical andquestion data that are not in the 20 most frequent for WSJ02-21.
The dash represents the sentence boundary.used with unlabelled data.
However, since POS tag-ging is relatively inexpensive, it might be possible touse this rate as one measure of syntactic distance be-tween a training corpus and a target domain, prior toundertaking parser domain adaptation.
The measuredoes not capture all aspects of syntactic distance,however.
As pointed out by an anonymous reviewer,if the syntactic tree structures are similar across do-mains but lexical distributions are different ?
e.g.
alarge number of words with unfamiliar categories inthe new domain ?
this measure will not be sensitiveto the difference.Another useful measure for comparing domainadaptation in the biomedical and question domainsis frequent POS n-grams.
Table 7 shows how manyof the 20 most frequent POS n-grams in each datasetoverlap with the 20 most frequent POS n-grams inWSJ 02-21.
It can be seen that the overlap is thehighest for Section 00, but much lower for the ques-tion data than for the biomedical data, again demon-strating that the question data makes frequent use ofsyntactic constructions which are rare in the PTB.Table 8 shows the four most frequent POS tri-grams in WSJ Sections 02-21,3 and the four mostfrequent POS trigrams in the biomedical and ques-tion data that are not among the 20 most frequent3Collapsing the NNP-NN distinction yields a slightly differ-ent set.482for WSJ 02-21.
The frequent question trigrams in-clude two sentence-initial question words as well asthe pattern ?
WP VBZ, occurring in sentences be-ginning with e.g.
What is or Who is.
Though notamong the top four, the pattern VB .
?, represent-ing a sentence-final bare verb, is also frequent.
Themost frequent biomedical POS trigrams are not dra-matically different from the newspaper trigrams, butdo appear to reflect the prevalence of NPs and PPsin the data.One final measure of syntactic distance is thefrequency with which CCG lexical categories thatare rare or unseen in CCGbank are used in a do-main.
It is typical to use a few such categories,even for in-domain data, for unusual syntactic con-structions, but each one is usually used only a hand-ful of times.
The question data is unique in thefrequency with which previously rare or unseencategories are required.
For example, the unseencategory (S [wq ]/S [q ])/N , representing the wordWhat in a question such as What day did Nintendo64 come out?
is used 11 times in the evaluationset; the rare category (S [wq ]/(S [dcl ]\NP))/N ,used in subject questions like Which river runsthrough Dublin?, is used 61 times; and the rare cat-egory (S [q ]/(S [pss]\NP))/NP , representing pas-sive verbs in sentences like What is Jane Goodallknown for?, is used 59 times.6 ConclusionWe have targeted lower levels of representation inorder to adapt a lexicalized-grammar parser to twonew domains, biomedical text and questions.
Al-though each of the lower levels has been targeted in-dependently in previous work, this is the first studythat examines both levels together to determine howthey affect parsing accuracy.
We achieved an accu-racy on grammatical relations in the same range asthat of the original parser for newspaper text, with-out requiring costly annotation of full parse trees.Both biomedical and question data are domains inwhich there is an immediate need for accurate pars-ing.
The question dataset is in some ways an ex-treme example for domain adaptation, since the sen-tences are syntactically uniform; on the other hand,it is of interest as a set of constructions where theparser initially performed poorly, and is a realisticparsing challenge in the context of QA systems.Interestingly, although an increase in accuracy ateach stage of the pipeline did yield an increase atthe following stage, these increases were not uni-form across the two domains.
The new POS taggermodel was responsible for most of the improvementin parsing for the biomedical domain, while the newsupertagger model was necessary to see a large im-provement in the question domain.
We attribute thisto the fact that question syntax is significantly differ-ent from newspaper syntax.
We expect these consid-erations to apply to any lexicalized-grammar parser.Of course, it would be useful to have a way ofpredicting which level of annotation would be mosteffective for adapting to a new domain before the an-notation begins.
The utility of measures such as un-known word rate (which can be performed with un-labelled data) and unknown POS n-gram rate (whichcan be performed with only POS tags) is not yet suffi-ciently clear to rely on them as predictive measures,but it seems a fruitful avenue for future work to in-vestigate the importance of such measures for parserdomain adaptation.AcknowledgmentsWe would like to thank Marie-Catherine de Marn-effe for advice on the use of the Stanford GR for-mat, Sampo Pyysalo for sharing information aboutthe BioInfer corpus, and Mark Steedman for adviceon encoding question data in CCG.
We would alsolike to thank three anonymous reviewers for theirsuggestions.
This work was supported by EPSRCgrant EP/E035698/1: Accurate and Efficient Parsingof Biomedical Text.ReferencesShlomo Argamon-Engelson, Moshe Koppel, and GalitAvneri.
1998.
Style-based text categorization: Whatnewspaper am I reading?
In Proceedings of AAAIWorkshop on Learning for Text Categorization, pages1?4.Harald Baayen, Hans Van Halteren, and Fiona Tweedie.1996.
Outside the cave of shadows: Using syntacticannotation to enhance authorship attribution.
Literaryand Linguistic Computing, 11(3):121?131.Srinivas Bangalore and Aravind Joshi.
1999.
Supertag-ging: An approach to almost parsing.
ComputationalLinguistics, 25(2):237?265.483Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Proceed-ings of the Interactive Demo Session of the Joint Con-ference of the International Committee on Computa-tional Linguistics and the Association for Computa-tional Linguistics (COLING/ACL-06), Sydney, Aus-trailia.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 1st Meeting of theNAACL, pages 132?139, Seattle, WA.Stephen Clark and James R. Curran.
2007a.
Formalism-independent parser evaluation with CCG and Dep-Bank.
In Proceedings of the 45th Meeting of the ACL,pages 248?255, Prague, Czech Republic.Stephen Clark and James R. Curran.
2007b.
Wide-coverage efficient statistical parsing with CCG andlog-linear models.
Computational Linguistics,33(4):493?552.Stephen Clark, Mark Steedman, and James R. Curran.2004.
Object-extraction and question-parsing usingCCG.
In Proceedings of the EMNLP Conference,pages 111?118, Barcelona, Spain.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of the35th Meeting of the ACL, pages 16?23, Madrid, Spain.James R. Curran, Stephen Clark, and David Vadas.2006.
Multi-tagging for lexicalized-grammar pars-ing.
In Proceedings of the Joint Conference of theInternational Committee on Computational Linguis-tics and the Association for Computational Linguis-tics (COLING/ACL-06), pages 697?704, Sydney, Aus-trailia.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of the 5th LREC Conference, pages 449?454,Genoa, Italy.Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.2007.
Evaluating impact of re-training a lexical dis-ambiguation model on domain adaptation of an HPSGparser.
In Proceedings of IWPT, pages 11?22, Prague,Czech Republic.Sanda Harabagiu, Dan Moldovan, Marius Pasca, RadaMihalcea, Mihai Surdeanu, Razvan Bunescu, RoxanaGirju, Vasile Rus, and Paul Morarescu.
2001.
Therole of lexico-semantic feedback in open-domain tex-tual question-answering.
In Proceedings of the 39thMeeting of the ACL, pages 274?281, Toulose, France.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: a corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Compu-tational Linguistics, 33(3):355?396.John Judge, Aoife Cahill, and Josef van Genabith.
2006.Questionbank: Creating a corpus of parse-annotatedquestions.
In Proceedings of the Joint Conference ofthe International Committee on Computational Lin-guistics and the Association for Computational Lin-guistics, pages 497?504, Sydney, Australia.Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and Jun?ichiTsujii.
2003.
GENIA corpus ?
a semantically an-notated corpus for bio-textmining.
Bioinformatics,19:i180?i182.Tracy H. King, Richard Crouch, Stefan Riezler, MaryDalrymple, and Ronald M. Kaplan.
2003.
The PARC700 Dependency Bank.
In Proceedings of the 4thInternational Workshop on Linguistically InterpretedCorpora, Budapest, Hungary.Matthew Lease and Eugene Charniak.
2005.
Parsingbiomedical literature.
In Proceedings of the SecondInternational Joint Conference on Natural LanguageProcessing (IJCNLP-05), Jeju Island, Korea.Mitchell Marcus, Beatrice Santorini, and MaryMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Yusuke Miyao and Jun?ichi Tsujii.
2005.
Probabilisticdisambiguation models for wide-coverage HPSG pars-ing.
In Proceedings of the 43rd meeting of the ACL,pages 83?90, University of Michigan, Ann Arbor.Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-ruoka, Yusuke Miyao, and Jun?ichi Tsujii.
2006.
Ex-tremely lexicalized models for accurate and fast HPSGparsing.
In Proceedings of the EMNLP Conference.Sampo Pyysalo, Filip Ginter, Juho Heimonen, JariBjo?rne, Jorma Boberg, Jouni Ja?rvinen, and TapioSalakoski.
2007a.
BioInfer: A corpus for informationextraction in the biomedical domain.
BMC Bioinfor-matics, 8:50.Sampo Pyysalo, Filip Ginter, Veronika Laippala, Ka-tri Haverinen, Juho Heimonen, and Tapio Salakoski.2007b.
On the unification of syntactic annotations un-der the stanford dependency scheme: A case study onBioInfer and GENIA.
In ACL?07 workshop on Biolog-ical, translational, and clinical language processing,pages 25?32, Prague, Czech Republic.Mark Steedman.
2000.
The Syntactic Process.
The MITPress, Cambridge, MA.484
