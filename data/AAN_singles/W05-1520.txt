Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 186?187,Vancouver, October 2005. c?2005 Association for Computational LinguisticsStatistical Shallow Semantic Parsing despite Little Training DataRahul BhagatInformation SciencesInstituteUniversity of SouthernCaliforniaMarina del Rey,CA, 90292, USArahul@isi.eduAnton LeuskiInstitute for CreativeTechnologiesUniversity of SouthernCaliforniaMarina del Rey,CA, 90292, USAleuski@ict.usc.eduEduard HovyInformation SciencesInstituteUniversity of SouthernCaliforniaMarina del Rey,CA, 90292, USAhovy@isi.edu1 Introduction and Related WorkNatural language understanding is an essential mod-ule in any dialogue system.
To obtain satisfac-tory performance levels, a dialogue system needsa semantic parser/natural language understandingsystem (NLU) that produces accurate and detaileddialogue oriented semantic output.
Recently, anumber of semantic parsers trained using eitherthe FrameNet (Baker et al, 1998) or the Prop-Bank (Kingsbury et al, 2002) have been reported.Despite their reasonable performances on generaltasks, these parsers do not work so well in spe-cific domains.
Also, where these general purposeparsers tend to provide case-frame structures, thatinclude the standard core case roles (Agent, Patient,Instrument, etc.
), dialogue oriented domains tendto require additional information about addressees,modality, speech acts, etc.
Where general-purposeresources such as PropBank and Framenet provideinvaluable training data for general case, it tends tobe a problem to obtain enough training data in a spe-cific dialogue oriented domain.We in this paper propose and compare a num-ber of approaches for building a statistically traineddomain specific parser/NLU for a dialogue system.Our NLU is a part of Mission Rehearsal Exercise(MRE) project (Swartout et al, 2001).
MRE is alarge system that is being built to train experts, inwhich a trainee interacts with a Virtual Human usingvoice input.
The purpose of our NLU is to convertthe sentence strings produced by the speech recog-nizer into internal shallow semantic frames com-posed of slot-value pairs, for the dialogue module.2 Parsing Methods2.1 Voting ModelWe use a simple conditional probability modelP (f | W ) for parsing.
The model represents theprobability of producing slot-value pair f as an out-put given that we have seen a particular word orn-gram W as input.
Our two-stage procedure forgenerating a frame for a given input sentence is: (1)Find a set of all slot-value that correspond with eachword/ngram (2) Select the top portion of these can-didates to form the final frame (Bhagat et al, 2005;Feng and Hovy, 2003).2.2 Maximum EntropyOur next approach is the Maximum Entropy (Bergeret al, 1996) classification approach.
Here, we castour problem as a problem of ranking using a classi-fier where each slot-value pair in the training data isconsidered a class and feature set consists of the un-igrams, bigrams and trigrams in the sentences (Bha-gat et al, 2005).2.3 Support Vector MachinesWe use another commonly used classifier, SupportVector Machine (Burges, 1998), to perform thesame task (Bhagat et al, 2005).
Approach is sim-ilar to Section 2.2.2.4 Language ModelAs a fourth approach to the problem, we use the Sta-tistical Language Model (Ponte and Croft, 1997).We estimate the language model for the slot-valuepairs, then we construct our target interpretation as186Method Precison Recall F-scoreV oting 0.82 0.78 0.80ME 0.77 0.80 0.78SVM 0.79 0.72 0.75LM1 0.80 0.84 0.82LM2 0.82 0.84 0.83Table 1: Performance of different systems on testdata.a set of the most likely slot-value pairs.
We useunigram-based and trigram-based language mod-els (Bhagat et al, 2005).3 Experiments and ResultsWe train all our systems on a training set of 477sentence-frame pairs.
The systems are then tested onan unseen test set of 50 sentences.
For the test sen-tences, the system generated frames are comparedagainst the manually built gold standard frames, andPrecision, Recall and F-scores are calculated foreach frame.Table 1 shows the average Precision, Recall andF-scores of the different systems for the 50 test sen-tences: Voting based (Voting), Maximum Entropybased (ME), Support Vector Machine based (SVM),Language Model based with unigrams (LM1) andLanguage Model based with trigrams (LM2).
TheF-scores show that the LM2 system performs thebest though the system scores in general for all thesystems are very close.
To test the statistical signifi-cance of these scores, we conduct a two-tailed pairedStudent?s t test (Manning and Schtze, 1999) on theF-scores of these systems for the 50 test cases.
Thetest shows that there is no statistically significant dif-ference in their performances.4 ConclusionsThis work illustrates that one can achieve fair suc-cess in building a statistical NLU engine for a re-stricted domain using relatively little training dataand surprisingly using a rather simple voting model.The consistently good results obtained from all thesystems on the task clearly indicate the feasibility ofusing using only word/ngram level features for pars-ing.5 Future WorkHaving successfully met the initial challenge ofbuilding a statistical NLU with limited training data,we have identified multiple avenues for further ex-ploration.
Firstly, we wish to build an hybrid systemthat will combine the strengths of all the systems toproduce a much more accurate system.
Secondly,we wish to see the effect that ASR output has oneach of the systems.
We want to test the robustnessof systems against an increase in the ASR word er-ror rate.
Thirdly, we want to build a multi-clauseutterance chunker to integrate with our systems.
Wehave identified that complex multi-clause utteranceshave consistently hurt the system performances.
Tohandle this, we are making efforts along with ourcolleagues in the speech community to build a real-time speech utterance-chunker.
We are eager to dis-cover any performance benefits.
Finally, since wealready have a corpus containing sentence and theircorresponding semantic-frames, we want to explorethe possibility of building a Statistical Generator us-ing the same corpus that would take a frame as inputand produce a sentence as output.
This would takeus a step closer to the idea of building a ReversibleSystem that can act as a parser when used in onedirection and as a generator when used in the other.ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998.
The berkeleyframenet project.
In Proceedings of COLING/ACL, page 8690, Montreal,Canada.Adam L. Berger, Stephen Della Pietra, and Vincent J. Della Pietra.
1996.
Amaximum entropy approach to natural language processing.
ComputationalLinguistics, 22(1):39?71.Rahul Bhagat, Anton Leuski, and Eduard Hovy.
2005.
Statistical shallowsemantic parsing despite little training data.
Technical report available athttp://www.isi.edu/?rahul.Christopher J. C. Burges.
1998.
A tutorial on support vector machines for patternrecognition.
Data Mining and Knowledge Discovery, 2(2):121?167.Donghui Feng and Eduard Hovy.
2003.
Semantics-oriented language understand-ing with automatic adaptability.
In Proceedings of Natural Language Process-ing and Knowledge Engineering.Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002.
Adding semantic an-notation to the penn treebank.
In Proceedings of HLT Conference.Christopher D. Manning and Hinrich Schtze.
1999.
Foundations of StatisticalNatural Language Processing.
The MIT Press, Cambridge, MA.Jay M. Ponte and W. Bruce Croft.
1997.
Text segmentation by topic.
In Proceed-ings of the First European Conference on Research and Advanced Technologyfor Digital Libraries, pages 120?129.W.
Swartout, R. Hill, J. Gratch, W. Johnson, C. Kyriakakis, C. LaBore, R. Lind-heim, S. Marsella, D. Miraglia, B. Moore, J. Morie, J. Rickel, M. Thiebaux,L.
Tuch, R. Whitney, and J. Douglas.
2001.
Toward the holodeck: Integratinggraphics, sound, character and story.
In Proceedings of Autonomous Agents.187
