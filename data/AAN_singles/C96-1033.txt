FeasPar - A Feature Structure Parser Learning to ParseSpoken LanguageF inn  Dag Bu?
and  A lex  Waibe lInteractive Systems Laborator iesUniversity of Karlsruhe, Germanyand Carnegie Mellon University, USA{finndag\[waibel}~ira.
uka.
deAbst ractWe describe and experimentally evalu-ate a system, FeasPar, that learns pars-ing spontaneous speech.
To train andrun FeasPar (Feature Structure Parser),only limited handmodeled knowledge isrequired.The FeasPar architecture consists of neu-ral networks and a search.
The networksspilt the incoming sentence into chunks,which are labeled with feature values andchunk relations.
Then, the search findsthe most probable and consistent featurestructure.FeasPar is trained, tested and evaluatedwith the Spontaneous Schednling Task,and compared with a handmodeled LR-parser.
The handmodeling effort for Fea-sPar is 2 weeks.
The handmodeling ef-fort for the LR-parser was 4 months.FeasPar performed better than the LR-parser in all six comparisons that aremade.1 I n t roduct ionWhen building a speech parsing component forsmall domains, an important goal is to get goodperformance.
If low hand labor is involved, thenit's even better.Unification based formalisms, e.g.
(Gazdar etal., 1985; Kaplan and Bresnan, 1982; Pollard andSag, \]987), have been very successful for analyz-ing written language, because they have providedparses with rich and detailed linguistic informa-tion.
However, these approaches have two majordrawbacks: first, they require hand-designed sym-bolic knowledge like lexica and grammar ules,and second, this knowledge is too rigid, causingproblems with ungranlmaticality and other devi-ations from linguistic rules.
These deviations aremanageable and low in number, when analyzingwritten language, but not for spoken language.The latter also contains spontaneous effects andspeech recognition errors.
(On the other hand, thegood thing is that spoken language tend to containless complex structures than written language.
)Several methods have been suggested compensatefor these speech related problems: e.g.
score andpenalties, probabilistic rules, and skipping words(Dowding et al, 1993; Seneff, 1992; Lavie andTomita, 1993; Issar and Ward, 1993).A small community have experimented with ei-ther purely statistical approaches(Brown et al,1990; Schiitze, 1993) or connectionist based ap-proaches (Berg, 1991; Miikkulainen and Dyer,1991; Jain, 1991; Wermter and Weber, 1994).The main problem when using statistical ap-proaches for spoken language processing, is thelarge amounts of data required to train these mod-els.
All connectionist approaches to our knowl-edge, have suffered from one or more of the fol-lowing problems: One, parses contains none or toofew linguistic attributes to be used in translationor understanding, and/or it is not shown how touse their parse formalism in a total NLP system.Two, no clear and quantitative statement aboutoverall performance is made.
Three, the approachhas not been evaluated with real world data, butwith highly regular sentences.
Four, millions oftraining sentences are required.In this paper, we present a parser that producescomplex feature structures, as known from e.g.GPSG(Gazdar et al, 1985).
This parser requiresonly minor hand labeling, and learns the parsingtask itself.
It generalizes well, and is robust to-wards spontaneous effects and speech recognitionerrors.The parser is trained and evaluated with theSpontaneous Scheduling Task, which is a nego-tiation situation, in which two subjects have todecide on time and place for a meeting.
The sub-jects' calendars have conflicts, so that a few sug-188gestions have to go back and tbrth before findinga time slot suitable for both.
The data sets arereal-world ata, containing spontaneous speech ef-fects.
3?he training set consists of 560 sentences,the deveJopment test set of 65 sentences, andthe unseen evaluation set of 120 sentences.
Forclarity, tile examl)le sentences in this paper areamong the simpler in the training set.
The parseris trained with transcribed ata only, but eval-uated with transcribed and speech data (includ-ing speech recognition errors).
The parser pro-duces feature structures, holding semantic infor-mation.
Feature structures are used as interlinguain the JANUS speech-to-speech translation sys-tem(Woszczyna el;al., 1994).
Within our researchteam, the design of the interlingua ILT was deter-mined by the needs of uniticatkm based parser andgenerator writers.
Consequently, the ILT designwas ,lot tuned towards connectkmist ysteins.
Onthe contrary, our parser must learn the form of tileoutput provided by a unitication based parser.This paper is organized as follows: First, a shorttutorial on feature structures, and how to buildthem.
Second, we describe the parser architec~ture and how it works.
Third, we describe thelexicon.
Fourth, we describe the tmrser's neuralaspects.
Fifth, a search algorithm is motivated.Then results and conclusion follow.2 Feature  S t ructuresFeature structures(Gazdar et al, 1985; Pollardand Sag, 1987) are used as output fbrmalism forl,basPar.
Their core.
syntactic properties and ter-minology are:1.
A feature structure is a set of none, one orseveral feature pairs.2.
A featurepair, e.g.
(frame *c la r i fy ) ,  con-sists of a feature, e.g.
frame or top ic ,  and afeature value.3.
A feature value is either:(a) an atomic value, e.g.
*c la r i fy(b) a complex value4.
A complex value is a feature structure.3 The  Chunk 'n 'Labe l  Pr inc ip leIn contrast o tim standard feature structure deti-nition of Section 2, an alternative view-point is tolook at a feature structure as a tree 1, where setstThis assumes that structure sharing is not possi-ble, see Section 3.1.2.
((speech-a-6-t *confirm)(sentence-type *state)(frame *clarify)(topic ((frame *simple-time)(day-of-week monday)))(adverb perhaps)(clarified ((frame *simple-time)(day-of-week monday)__  (day 27))))Figure 1: Feature structure with the meaning "bymonday i assume you mean monday the twenty sev-enth"of feature pairs with atomic wdues make up tilebraimhes, and the ln'anches are connected withrelations.
Atomic feature pairs belonging to thesame branches, have the same relation to all otherbranches.
Further, when comparing the sentencewith its feature structure, it appears that thereis a correspondence b tween fl'agments of the fea-ture structure, and specific ctmnks of the sentence.In the example feature structure of Figure 1, thefollowing observations about feature pairs and re-lations apply:?
feature pairs:\ [ feature  pairs:  cor responds  to:~-( (f rame *s imple - t ime)\ [ (day-o f -week  monday) "monday theL_ (day 27)) twenty seventh""the twenty seventh"?
re lat ions:  tile coinplex value of the tbaturetopic corresponds to the chunk "by mon-day", and tile complex value of the featurec la r i f ied  corresponds to "you mean mondaythe twenty seventh".Manually aligniug the sentence with fragmentsof the feature structure, gives a structure as shownin Figure 2.
A few coinments apply to this figure:?
The sentence is hierarchically split intochunks.?
Feature pairs are listed with their correspond-ing chunk.?
Relations are shown in square brackets, andexpress how a chunk relates to its parentchunk.
Relations may contain more than oneelement.
This allows several nesting levels.Once having obtained the information in Fig-ure 2, producing a feature structure is straightforward, using the algorithm of Figure 3.
Sum-ruing up, we can define this procedure as thechunk'n'label principle of parsing:189( \ [ \ ] ( (speech-act  * onfirm)(sentence-type *state)(frame *clarify))(\[\](\[topic\]((frame *simple-time))(\[\] by)(\[\]((day-of-week monday)) monday))(C\] (\[\] i))( \ [ \ ] ( (adverb perhaps))(\[\] assume)))( \ [ c la r i f i ed \ ](\[\] (\[\] you))(\[\] (\[\] mean))(\[\]((frame *simple-time))(\[\]((day-of-week monday)) monday)(\[\] the)(,\[\]((day 27)) (\[rego\] twenty seventh)))))Figure 2: Chunk parse: Sentence aligned with its feature structure (see text for explanation).1.
Split the incoming sentence into hierarchicalchunks.2.
Label each chuck with feature pairs and fea-ture relations.3.
Convert this into a feature structure, usingthe algorithm of Figure 3.FUNCTION convert()VARS: set ;C: chunk;BEGINS := empty set ;assign(S,top_level_chunk);return(S);END;PROCEDURE assign(VAR S: set;C: chunk);BEGINP := chunk_relation(C);F0R each relation element PE in PBEGINS' := empty set;include (PE,S') in S;S := S';END;FOR each feature pair FP in Cinclude FP in S;F0R each chunk C' in Cassign(S,C);END;Figure 3: Algorithm for converting a parse to afeature structure3.1 Theoretical LimitationsThe chunk'n'label principle has a few theoreticallimitations compared with the feature structureformalisms commonly used in unification-basedparsing, e.g.
(Gazdar et al, 1985).3.1.1 DepthWith the chunk'n'label principle, the featurestructure has a maximum nesting depth.
Onecould expect he maximal nesting depth to causelimitations.
However, these limitations are onlytheoretical, because very deep nesting is hardlyneeded in practice for spoken language.
Due tothe ability to model relations of more than length1, no nesting depth problems occurred while mod-eling over 600 sentences from the English Sponta-neous Scheduling Task (ESST).3.1.2 Structure SharingMany unification formalisms allow feature val-ues to be shared.
The chunk'n'label principle doesnot incorporate any mechanism for this.
However,all work with ESST and ILT empirically showedthat there is no need for structure sharing.
Thisobservation suggests that for semantic analysls,structure sharing is statistically insignificant, evenif its existence is theoretically present.4 Base l ine  ParserThe chunk'n'label principle is the basis for thedesign and implementation f the FeasPar parser.FeasPar uses neural networks to learn to producechunk parses.
It has two modes: learn modeand run mode.
In learn mode, manually mod-eled chunk parses are split into several separatetraining sets; one per neural network.
Then, thenetworks are trained independently of each other,allowing for parallel training on several CPU's.
Inrun mode, the input sentence is processed throughall networks, giving a chunk parse, which is passed190(((speech-act *state-constraint)(sentence-type *state))(((frame *booked))(((frame : * i ) )( i))(( h~ve))(((frame =*meeting))(((specifier indefinite)) a)( meeting))(((frame *simple-time)( .
.
/ f rame *interval)(../incl-excl inclusive))( till)(((hour =12)) (\[regc\] twelve)))))((speech-act *state-constraint)(sentence-type *state)(frame *booked)(who ((frame *i)))(what ((frame *meeting)(specifier indefinite)))(when ((incl-excl inclusive)(frame *interval)(end ((frame *simple-time)(hour 12))))))Figure 6: Feature structure parseFigure 4: Chunked and labeled sentence (labelsshown in boldface)(\[/((speech-act *state-constraint)(sentence-type *state))(~((frame *booked))(\[who\] ((frame ----*i))(8 i))(\[I(8 ha~e))(\[what\](( frame ----*meeting))(\[\]((specifier indefinite)) a)(\[\] meeting))(\[when/end\]((frame *simple-time)(../frame *interwd)(../incl-excl inclusive))(fl till)(fl((hour =12)) (\[regc\] twelve)))))Figure 5: Chunk parse (chunk relations hown inboldface)on to the converting algorithm shown in Figure 3.In the following, tile three main modules re-quired to produce a chunk parse are described:The Chunker splits an input sentence intochunks.
It consists of three neural networks.
Thefirst network finds numbers.
They are classified asbeing ordinal or cardinal numbers, and are pre-sented as words to the following networks.
Thenext network groups words together to phrases.The third network groups phrases together intoclauses.
In total, there are four levels of chunks:word/numbers, phrases, clauses and sentence.The Linguistic Feature Labeler attaches featuresand atomic feature values (if applicable) to thesechunks.
For each feature, there is a network,which finds one or zero atomic values.
Since thereare many features, each chunk may get no, one orseveral pairs of features and atomic values.
Sincea feature normally only occurs at a certain ctmnklevel, the network is tailored to decide on a par-ticular feature at a particular chunk level.
Thisspecialization is there to prevent he learning taskfrom becoming too complex.
A special atomic fea-ture value is called lexical feature value.
It is in-dicated by '=' and means that the neural networkonly detects the occurrence of a value, whereas thevalue itself is found by a lexicon lookup.
The lex-ical feature values are a true hybrid mechanism,where symbolic knowledge is included when theneural network signals so.
Furthermore, featuresmay be marked as up-features (e .g .
.
.
/ inc l -exc lin Figure 4 and 5).
An up-feature is propagatedup to its parent branch when building the featurestructure (see Figure 6).The Chunk Relation Finder determines how achunk relates to its parent chunk.
It has one net-work per chunk level and chunk relation element.The following example illustrates in detail howthe three parts work.
\]~br clarity, this exampleassumes that all networks perform perfectly.
Theparser gets the English sentence:"i have a meeting till twelve"The Chunker segments the sentence before pass-ing it to the Linguistic Feature Labeler, whichadds semantic labels (see Figure 4).
The ChunkRelation Finder then adds relations, where appro-priate, and we get the chunk parse as shown inFigure 5.
Finally, processing it by the algorithmin Figure 3, gives the final parse, the feature struc-ture, as shown in Figure 6.4.1 Lex iconFeasPar uses a full word form lexicon.
The lexiconconsists of three parts: one, a syntactic and se-mantic microfeature vector per word, second, lex-ical feature values, and three, statistical microfea-tures.Syntactic and semantic microfeatures are repre-sented for each word as a vector of binary vahles.These vectors are used as input to the neural net-works.
As the neural networks learn their tasksbased on the microfeatures, and not based on dis-tinct words, adding new words using the same mi-crofeatures i easy and does not degrade general-191ization performance.
The number and selection ofmicrofeatures are domain dependent and must bemade manually.
For ESST, the lexicon containsdomain independent syntactic and domain depen-dent semantic microfcatures.
To manually modela 600 word ESST vocabulary requires 3 lull days.Lexical feature values are stored in look-uptables, which are accessed when the LinguisticFeature Labeler indicates a lexical feature value.These tables are generated automatically from thetraining data, and can easily be extended by handfor more generality and new words.
An auto-matie ambiguity checker warns if similar words orphrases map to ambiguous lexical feature values.Statistical microfeatures are represented foreach word as a vector of continuous values Vstat.These microfeatures, each of them representing afeature pair, are extracted automatically.
For ev-ery feature value at a certain chunk level, if thereexists a word such that, given this word in thetraining data, the feature value occurs in morethan 50 % of tim cases.
One continuous microfea-ture value v~t,t for a word w is set automaticallyto the percentage of feature value occurrence giventhat word w.4.2  Neura l  Arch i tec ture  and  Tra in ingAll neural networks have one hidden layer, and areconventional feed-forward networks.
The learn-ing is done with standard back-propagation, com~bined with the constructive learning algorithmPCL(Jain, 1991), where learning starts using asmall context, which is increased later in the learn-ing process.
This causes local dependencies to belearned first.Generalization performance is increased bysparse connectivity.
This connection principle isbased on the microfeatures in the lexicon that arerelevant o a particular network.
The Chunkernetworks are only connected to the syntactic mi-crofeatures, because chunking is a syntactic task.With ESST, the Linguistic Feature Labeler andChunk Relation Finder networks are connectedonly to the semantic microfeatures, and to rel-evant statistical microfeatures.
All connectivitysetup is automatic.
Further techniques for im-proving performance are described in (Buo, 1996).For the neural networks, the average test set per-formance is 95.4 %5 SearchThe complete parse depends on many neural net-works.
Most networks have a certain error rate;only a few networks are perfect.
When buildingcomplete feature structures, these network errorsmultiply up, resulting in not only that many fea-ture structures are erroneous, but also inconsis-tent and making no sense.To compensate for this, we wrote a search al-gorithm.
It's based on two information sources:First, scores that originates from the network out-put activations; second, a formal feature struc-ture specification, stating what mixture of featurepairs are consistent.
This specification was al-ready available as an interlingua specification doc-ument.Using these two information sources, the searchfinds the feature structure with the highest score,under the constraint of being consistent.
Thesearch is described in more detail in (Bu0 andWaibel, 1996; Bu0, 1996).6 Resu l tsFeasPar GLR* ParserPM1 - T 71.8 % 51.6 %PM1 - S 52.3 % 30.3 %PM2E-  T 74 % 63 %PM2E-  S 49 % 28 %PM3G - T 49 % 42 %PM2G - S 36 % 17 %Figure 7: ResultsFeasPar is compared with a handmodeled LR-parser.
The handmodeling effort for FeasPar is 2weeks.
The handmodeling effort tbr the LR-parserwas 4 months.The evaluation environment is the JANUSspeech translation system for the SpontaneousScheduling Task.
The system have one parser andone generator per language.
All parsers and gen-erators are written using CMU's GLR/GLR* sys-tem(Lavie and Tomita, 1993).
They all share thesame interlingua, ILT, which is a special case ofLFG or feature structures.All Performance measures are run with tran-scribed (T) sentences and with speech (S) sen-tences containing speech recognition errors.
Per-formance measure 1 is the feature accuracy, whereall features of a parser-nmde f ature structure arecompared with feature of the correct handmodeledfeature structure.
Performance measure 2 is theend-to-end translation ratio for acceptable non-trivial sentences achieved when LR-generators areused as back-ends of the parsers.
Performancemeasure 2 uses an English LR-generator (hand-modeled for 2 years), providing results for English-to-English translation, whereas performance mea-sure 3 uses a German LR-generator (handmodeled192for 6 months), hence providing results for English-to-German translations.
Results for an unseen,independent evaluation set are shown in Figure 7.As we see, FeasPar is better than the LR-parserin all six comparison perforInance measures made.7 Conclus ionWe described and experimentally evaluated a sys-tem, FeasPar, that learns parsing spontaneousspeech.
To train and run FeasPar (Feature Struc-ture Parser), only limited handmodeled knowl-edge is required (chunk parses and a lexicon).l~5;asPar is based on a principle of chunks, theirfeatures and relations.
The FeasPar architectureconsists of two n'tajor parts: A neural network col-lection and a search.
The neural networks firstspilt the incoming sentence into chunks.
Theneach chunk is labeled with feature values andchunk relations.
Finally, the search uses a formalfeature structure specification as constraint, andoutputs the most probable and consistent featurestructure.FeasPar was trained, tested and evaluated withthe Spontaneous Scheduling Task, and comparedwith a handmodeled LR-parser.
FeasPar per-tbrmed better than the LR-parser in all six com-parison performance measures that were made.ReferencesGeorge Berg.
1991.
Learifing Recursive PhraseStructure: Combining the St, rengths of PDPand X-Bar Syntax.
Technical report TR 91-5,Dept.
of Computer Science, University at Al-bany, State University of New York.Peter F. Brown, John Cocke, Stephen A. DellaPietra, Vincent J. Della Pietra, Fredrick JelinekJohn D. Lafferty, Robert L. Mercer, and Paul S.Roossin.
1990.
A Statistical Approach To Ma-chine ~lYanslation.
Computational Linguistics,16(2):79-85, June.Finn Dag Bu0 and Alex Waibel.
1996.
Search in aLearnable Spoken Language Parser.
In Proceed-ings of the 12th European Conference on Arti-ficial Intelligence, August.Finn Dag Bu0.
1996.
FeasPar - A 1%atureStructure Parser Learning to Parse Sponta-neous Speech.
Ph.D. thesis, University of Karl-sruhe, upcoming.J.
Dowding, J. M. Gawron, D. Appelt, J. Bear,L.
Cherny, R. Moore, and D. Moran.
1993.Gemini: A Natural Language System forSpoken-Language Understanding.
In Proceed-ings ARPA Workshop on Human LanguageTechnology, pages 43-48, Princeton, New Jer-sey, March.
Morgan Kaufmann Publisher.G.
Gazdar, E. Klein, G. K. Pullum, and I. A.Sag.
1985.
A theory of syntactic features.
InGeneralized Phrase Structure Grammar, chap-ter 2.
Blackwell Publishing, Oxford, Englandand Itarvard University Press, Cambridge, MA,USA.Sunil Issar and Wayne Ward.
1993.
CMU's ro-bust spoken language understanding system.
InProceedings of Eurospeech.Ajay N. Jain.
1991.
A Connectionist Learning Ar-chitecture for Parsing Spoken Language.
Ph.D.thesis, School of Computer Science, CarnegieMellon University, Dec.R.
Kaplan and J. Bresnan.
1982.
Lexical-Functional Grammar: A Formal System forGrammatical Representation.
In J. Bresnan,editor, The Mental Representation of Gram-matical Relations, pages 173-281.
The MITPress, Cambridge, MA.A.
Lavie and M. Tomita.
1993.
GLR* - AnEfficient Noise-skipping Parsing Algorithm forContext-free Grammars.
In Proceedings ofThird Intcrnational Workshop on Parsing Tech-nologies, pages 123 134.R.
Miikkulainen and M. Dyer.
1991.
NaturalLanguage Processing With Modular PDP Net-works and Distributed Lexicon.
Cognitive Sci-enec, 15:343 399.C.
Pollard and I.
Sag.
1987.
Formal Foundations.In An In/ormation-Based Syntax and Seman-tics, chapter 2.
CSLI Lecture Notes No.13.tIinrich Schiitze.
1993. rDanslation by Confusion.In Spring Symposium on Machinc Translation.AAAI.Stephanie Seneff.
1992.
TINA: A Natural Lan-guage System for Spoken Language Applica-tions.
Computational linguistics, 18(1).Stefan Wermter and Volker Weber.
1994.Learning Fault-tolerant Spreech Parsing witt~SCREEN.
In Proceedings of Twelfth NationalConference on Artificial InteUigence, Seattle.M.
Woszczyna, N. Aoki-Waibel, F. D. Bu0,N.
Coccaro, K. Horiguchi, T. Kemp, A. Lavie,A.
McNair, T. Polzin, I. Rogina, C.P.Rose, T. Schultz, B. Suhm, M. Tomita, andA.
Waibel.
1994.
JANUS 93: Towards Spon-taneous Speech Translation.
In InternationalConference on Acoustics, Speech '~ Signal Pro-cessing, pages 345--348, vol.
1, Adelaide, Aus-tralia, April.
IEEE.193
