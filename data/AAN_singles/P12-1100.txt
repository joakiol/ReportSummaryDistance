Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950?958,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsHierarchical Chunk-to-String Translation?Yang Feng?
Dongdong Zhang?
Mu Li?
Ming Zhou?
Qun Liu??
Department of Computer Science ?
Microsoft Research AsiaUniversity of Sheffield dozhang@microsoft.comSheffield, UK muli@microsoft.comy.feng@shef.ac.uk mingzhou@microsoft.com?Key Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of Sciencesliuqun@ict.ac.cnAbstractWe present a hierarchical chunk-to-stringtranslation model, which can be seen as acompromise between the hierarchical phrase-based model and the tree-to-string model,to combine the merits of the two models.With the help of shallow parsing, our modellearns rules consisting of words and chunksand meanwhile introduce syntax cohesion.Under the weighed synchronous context-freegrammar defined by these rules, our modelsearches for the best translation derivationand yields target translation simultaneously.Our experiments show that our model signif-icantly outperforms the hierarchical phrase-based model and the tree-to-string model onEnglish-Chinese Translation tasks.1 IntroductionThe hierarchical phrase-based model (Chiang, 2007)makes an advance of statistical machine translationby employing hierarchical phrases, which not onlyuses phrases to learn local translations but also useshierarchical phrases to capture reorderings of wordsand subphrases which can cover a large scope.
Be-sides, this model is formal syntax-based and doesnot need to specify the syntactic constituents ofsubphrases, so it can directly learn synchronouscontext-free grammars (SCFG) from a parallel textwithout relying on any linguistic annotations or as-sumptions, which makes it used conveniently andwidely.
?This work was done when the first author visited MicrosoftResearch Asia as an intern.However, it is often desirable to consider syntac-tic constituents of subphrases, e.g.
the hierarchicalphraseX ?
?X 1 for X 2 , X 2 de X 1 ?can be applied to both of the following strings inFigure 1?A request for a purchase of shares?
?filed for bankruptcy?,and get the following translation, respectively?goumai gufen de shenqing?
?pochan de shenqing?.In the former, ?A request?
is a NP and this rule actscorrectly while in the latter ?filed?
is a VP and thisrule gives a wrong reordering.
If we specify the firstX on the right-hand side to NP, this kind of errorscan be avoided.The tree-to-string model (Liu et al, 2006; Huanget al, 2006) introduces linguistic syntax via sourceparse to direct word reordering, especially long-distance reordering.
Furthermore, this model is for-malised as Tree Substitution Grammars, so it ob-serves syntactic cohesion.
Syntactic cohesion meansthat the translation of a string covered by a subtreein a source parse tends to be continuous.
Fox (2002)shows that translation between English and Frenchsatisfies cohesion in the majority cases.
Many pre-vious works show promising results with an as-sumption that syntactic cohesion explains almostall translation movement for some language pairs(Wu, 1997; Yamada and Knight, 2001; Eisner, 2003;Graehl and Knight, 2004; Quirk et al, 2005; Cherry,2008; Feng et al, 2010).950But unfortunately, the tree-to-string model re-quires each node must be strictly matched duringrule matching, which makes it strongly dependenton the relationship of tree nodes and their roles inthe whole sentence.
This will lead to data sparse-ness and being vulnerable to parse errors.In this paper, we present a hierarchical chunk-to-string translation model to combine the merits of thetwo models.
Instead of parse trees, our model intro-duces linguistic information in the form of chunks,so it does not need to care the internal structures andthe roles in the main sentence of chunks.
Based onshallow parsing results, it learns rules consisting ofeither words (terminals) or chunks (nonterminals),where adjacent chunks are packed into one nonter-minal.
It searches for the best derivation through theSCFG-motivated space defined by these rules andget target translation simultaneously.
In some sense,our model can be seen as a compromise betweenthe hierarchical phrase-based model and the tree-to-string model, specifically?
Compared with the hierarchical phrase-basedmodel, it integrates linguistic syntax and sat-isfies syntactic cohesion.?
Compared with the tree-to-string model, it onlyneeds to perform shallow parsing which intro-duces less parsing errors.
Besides, our modelallows a nonterminal in a rule to cover severalchunks, which can alleviate data sparseness andthe influence of parsing errors.?
we refine our hierarchical chunk-to-stringmodel into two models: a loose model (Section2.1) which is more similar to the hierarchicalphrase-based model and a tight model (Section2.2) which is more similar to the tree-to-stringmodel.The experiments show that on the 2008 NISTEnglish-Chinese MT translation test set, both theloose model and the tight model outperform the hi-erarchical phrase-based model and the tree-to-stringmodel, where the loose model has a better perfor-mance.
While in terms of speed, the tight modelruns faster and its speed ranking is between the tree-to-string model and the hierarchical phrase-basedmodel.NP IN NP IN NP VBD VPA request for a purchase of shares was madegoumai gufen de shenqing bei dijiao??
??
?
??
?
??
(a)NP VBZ VBN IN NPThe bank has filed for bankruptcygai yinhang yijing shenqing pochan?
??
??
??
??
(b)Figure 1: A running example of two sentences.
For eachsentence, the first row gives the chunk sequence.SNPDTTheNNbankVPVBZhasVPVBNfiledPPINforNPNNbankruptcy(a) A parse treeB-NP I-NP B-VBZ B-VBN B-IN B-NPThe bank has filed for bankruptcy(b) A chunk sequence got from the parse treeFigure 2: An example of shallow parsing.2 ModelingShallow parsing (also chunking) is an analysis ofa sentence which identifies the constituents (noungroups, verbs, verb groups, etc), but neither spec-ifies their internal structures, nor their roles in themain sentence.
In Figure 1, we give the chunk se-quence in the first row for each sentence.
We treatshallow parsing as a sequence label task, and a sen-tence f can have many possible different chunk la-bel sequences.
Therefore, in theory, the conditionalprobability of a target translation e conditioned onthe source sentence f is given by taking the chunklabel sequences as a latent variable c:p(e|f) =?cp(c|f)p(e|f , c) (1)951In practice, we only take the best chunk label se-quence c?
got byc?
= argmaxcp(c|f) (2)Then we can ignore the conditional probabilityp(c?|f) as it holds the same value for each transla-tion, and get:p(e|f) = p(c?|f)p(e|f , c?
)= p(e|f , c?)
(3)We formalize our model as a weighted SCFG.In a SCFG, each rule (usually called production inSCFGs) has an aligned pair of right-hand sides ?the source side and the target side, just as follows:X ?
?
?, ?,?
?where X is a nonterminal, ?
and ?
are both strings ofterminals and nonterminals, and ?
denotes one-to-one links between nonterminal occurrences in ?
andnonterminal occurrences in ?.
A SCFG produces aderivation by starting with a pair of start symbolsand recursively rewrites every two coindexed non-terminals with the corresponding components of amatched rule.
A derivation yields a pair of stringson the right-hand side which are translation of eachother.In a weighted SCFG, each rule has a weight andthe total weight of a derivation is the productionof the weights of the rules used by the derivation.A translation may be produced by many differentderivations and we only use the best derivation toevaluate its probability.
With d denoting a deriva-tion and r denoting a rule, we havep(e|f) = maxdp(d,e|f , c?
)= maxd?r?dp(r,e|f , c?)
(4)Following Och and Ney (2002), we frame our modelas a log-linear model:p(e|f) = exp?k ?kHk(d,e, c?,f)exp?d?,e?,k ?kHk(d?,e?, c?,f)(5)where Hk(d,e, c?,f) =?rhk(f , c?, r)So the best translation is given bye?
= argmaxe?k?kHk(d,e, c?,f) (6)We employ the same set of features for the log-linear model as the hierarchical phrase-based modeldoes(Chiang, 2005).We further refine our hierarchical chunk-to-stringmodel into two models: a loose model which is moresimilar to the hierarchical phrase-based model anda tight model which is more similar to the tree-to-string model.
The two models differ in the form ofrules and the way of estimating rule probabilities.While for decoding, we employ the same decodingalgorithm for the two models: given a test sentence,the decoders first perform shallow parsing to get thebest chunk sequence, then apply a CYK parsing al-gorithm with beam search.2.1 A Loose ModelIn our model, we employ rules containing non-terminals to handle long-distance reordering whereboundary words play an important role.
So for thesubphrases which cover more than one chunk, wejust maintain boundary chunks: we bundle adjacentchunks into one nonterminal and denote it as the firstchunk tag immediately followed by ?-?
and next fol-lowed by the last chunk tag.
Then, for the string pair<filed for bankruptcy, shenqing pochan>, we canget the ruler1 : X ?
?VBN 1 for NP 2 , VBN 1 NP 2 ?while for the string pair <A request for a purchaseof shares, goumai gufen de shenqing>, we can getr2 : X ?
?NP 1 for NP-NP 2 , NP-NP 2 de NP 1 ?.The rule matching ?A request for a purchase ofshares was?
will ber3 : X ?
?NP-NP 1 VBD 2 , NP-NP 1 VBD 2 ?.We can see that in contrast to the method of rep-resenting each chunk separately, this representationform can alleviate data sparseness and the influenceof parsing errors.952?S 1 , S 1 ?
?
?S 2 X 3 , S 2 X 3 ??
?X 4 X 3 , X 4 X 3 ??
?NP-NP 5 VBD 6 X 3 , NP-NP 5 VBD 6 X 3 ??
?NP 7 for NP-NP 8 VBD 6 X 3 , NP-NP 8 de NP 7 VBD 6 X 3 ??
?A request for NP-NP 8 VBD 6 X 3 , NP-NP 8 de shenqing VBD 6 X 3 ??
?A request for a purchase of shares VBD 6 X 3 , goumai gufen de shenqing VBD 6 X 3 ??
?A request for a purchase of shares was X 3 , goumai gufen de shenqing bei X 3 ??
?A request for a purchase of shares was made, goumai gufen de shenqing bei dijiao?
(a) The loose model?NP-VP 1 , NP-VP 1 ?
?
?NP-VBD 2 VP 3 , NP-VBD 2 VP 3 ??
?NP-NP 4 VBD 5 VP 3 , NP-NP 4 VBD 5 VP 3 ??
?NP 6 for NP-NP 7 VBD 5 VP 3 , NP-NP 7 de NP 6 VBD 5 VP 3 ??
?A request for NP-NP 7 VBD 5 VP 3 , NP-NP 7 de shenqing VBD 5 VP 3 ??
?A request for a purchase of shares VBD 5 VP 3 , goumai gufen de shenqing VBD 5 VP 3 ??
?A request for a purchase of shares was VP 3 , goumai gufen de shenqing bei VP 3 ??
?A request for a purchase of shares was made, goumai gufen de shenqing bei dijiao?
(b) The tight modelFigure 3: The derivations of the sentence in Figure 1(a).In these rules, the left-hand nonterminal symbol Xcan not match any nonterminal symbol on the right-hand side.
So we need a set of rules such asNP ?
?X 1 , X 1 ?NP-NP ?
?X 1 , X 1 ?and so on, and set the probabilities of these rules to1.
To simplify the derivation, we discard this kind ofrules and assume that X can match any nonterminalon the right-hand side.Only with r2 and r3, we cannot produce anyderivation of the whole sentence in Figure 1 (a).
Inthis case we need two special glue rules:r4 : S ?
?S 1 X 2 , S 1 X 2 ?r5 : S ?
?X 1 , X 1 ?Together with the following four lexical rules,r6 : X ?
?a request, shenqing?r7 : X ?
?a purchase of shares, goumai gufen?r8 : X ?
?was, bei?r9 : X ?
?made, dijiao?Figure 3(a) shows the derivation of the sentence inFigure 1(a).2.2 A Tight ModelIn the tight model, the right-hand side of each ruleremains the same as the loose model, but the left-hand side nonterminal is not X but the correspond-ing chunk labels.
If a rule covers more than onechunk, we just use the first and the last chunk la-bels to denote the left-hand side nonterminal.
Therule set used in the tight model for the example inFigure 1(a) corresponding to that in the loose modelbecomes:r2 : NP-NP ?
?NP 1 for NP-NP 2 , NP-NP 2 de NP 1 ?r3 : NP-VBD ?
?NP-NP 1 VBD 2 , NP-NP 1 VBD 2 ?.r6 : NP ?
?a request, shenqing?r7 : NP-NP ?
?a purchase of shares, goumai gufen?r8 : VBD ?
?was, bei?r9 : VP ?
?made, dijiao?During decoding, we first collect rules for eachspan.
For a span which does not have any matchingrule, if we do not construct default rules for it, therewill be no derivation for the whole sentence, then weneed to construct default rules for this kind of spanby enumerating all possible binary segmentation ofthe chunks in this span.
For the example in Figure1(a), there is no rule matching the whole sentence,953so we need to construct default rules for it, whichshould beNP-VP ?
?NP-VBD 1 VP 2 , NP-VBD 1 VP 2 ?.NP-VP ?
?NP-NP 1 VBD-VP 2 , NP-NP 1 VBD-VP 2 ?.and so on.Figure 3(b) shows the derivation of the sentencein Figure 1(a).3 Shallow ParsingIn a parse tree, a chunk is defined by a leaf node oran inner node whose children are all leaf nodes (SeeFigure 2 (a)).
In our model, we identify chunks bytraversing a parse tree in a breadth-first order.
Oncea node is recognized as a chunk, we skip its children.In this way, we can get a sole chunk sequence givena parse tree.
Then we label each word with a labelindicating whether the word starts a chunk (B-) orcontinues a chunk (I-).
Figure 2(a) gives an example.In this method, we get the training data for shallowparsing from Penn Tree Bank.We take shallow Parsing (chunking) as a sequencelabel task and employ Conditional Random Field(CRF)1 to train a chunker.
CRF is a good choice forlabel tasks as it can avoid label bias and use morestatistical correlated features.
We employ the fea-tures described in Sha and Pereira (2003) for CRF.We do not introduce CRF-based chunkier in this pa-per and more details can be got from Hammersleyand Clifford (1971), Lafferty et al (2001), Taskar etal.
(2002), Sha and Pereira (2003).4 Rule ExtractionIn what follows, we introduce how to get the ruleset.
We learn rules from a corpus that first is bi-directionally word-aligned by the GIZA++ toolkit(Och and Ney, 2000) and then is refined using a?final-and?
strategy.
We generate the rule set in twosteps: first, we extract two sets of phrases, basicphrases and chunk-based phrases.
Basic phrases aredefined using the same heuristic as previous systems(Koehn et al, 2003; Och and Ney, 2004; Chiang,2005).
A chunk-based phrase is such a basic phrasethat covers one or more chunks on the source side.1We use the open source toolkit CRF++ got inhttp://code.google.com/p/crfpp/ .We identity chunk-based phrases ?cj2j1 ,fj2j1 ,ei2i1?
asfollows:1.
A chunk-based phrase is a basic phrase;2. cj1 begins with ?B-?;3.
fj2 is the end word on the source side or cj2+1does not begins with ?I-?.Given a sentence pair ?f ,e,?
?, we extract rules forthe loose model as follows1.
If ?f j2j1 ,ei2i1?
is a basic phrase, then we can havea ruleX ?
?f j2j1 ,ei2i1?2.
Assume X ?
?
?, ??
is a rule with ?
=?1f j2j1 ?2 and ?
= ?1ei2i1?2, and ?fj2j1 ,ei2i1?
isa chunk-based phrase with a chunk sequenceYu ?
?
?Yv, then we have the following ruleX ?
?
?1Yu-Yv k ?2, ?1Yu-Yv k ?2?.We evaluate the distribution of these rules in thesame way as Chiang (2007).We extract rules for the tight model as follows1.
If ?f j2j1 ,ei2i1?
is a chunk-based phrase with achunk sequence Ys ?
?
?Yt, then we can have aruleYs-Yt ?
?f j2j1 ,ei2i1?2.
Assume Ys-Yt ?
?
?, ??
is a rule with ?
=?1f j2j1 ?2 and ?
= ?1ei2i1?2, and ?fj2j1 ,ei2i1?
isa chunk-based phrase with a chunk sequenceYu ?
?
?Yv, then we have the following ruleYs-Yt ?
?
?1Yu-Yv k ?2, ?1Yu-Yv k ?2?.We evaluate the distribution of rules in the same wayas Liu et al (2006).For the loose model, the nonterminals must be co-hesive, while the whole rule can be noncohesive: ifboth ends of a rule are nonterminals, the whole ruleis cohesive, otherwise, it may be noncohesive.
Incontrast, for the tight model, both the whole rule andthe nonterminal are cohesive.Even with the cohesion constraints, our modelstill generates a large number of rules, but not all954of the rules are useful for translation.
So we followthe method described in Chiang (2007) to filter therule set except that we allow two nonterminals to beadjacent.5 Related WorksWatanabe et al (2003) presented a chunk-to-stringtranslation model where the decoder generates atranslation by first translating the words in eachchunk, then reordering the translation of chunks.Our model distinguishes from their model mainlyin reordering model.
Their model reorders chunksresorting to a distortion model while our model re-orders chunks according to SCFG rules which retainthe relative positions of chunks.Nguyen et al (2008) presented a tree-to-stringphrase-based method which is based on SCFGs.This method generates SCFGs through syntac-tic transformation including a word-to-phrase treetransformation model and a phrase reordering modelwhile our model learns SCFG-based rules fromword-aligned bilingual corpus directlyThere are also some works aiming to introducelinguistic knowledge into the hierarchical phrase-based model.
Marton and Resnik (2008) took thesource parse tree into account and added soft con-straints to hierarchical phrase-based model.
Cherry(2008) used dependency tree to add syntactic cohe-sion.
These methods work with the original SCFGdefined by hierarchical phrase-based model and uselinguistic knowledge to assist translation.
Instead,our model works under the new defined SCFG withchunks.Besides, some other researchers make efforts onthe tree-to-string model by employing exponentiallyalternative parses to alleviate the drawback of 1-bestparse.
Mi et al (2008) presented forest-based trans-lation where the decoder translates a packed forestof exponentially many parses instead of i-best parse.Liu and Liu (2010) proposed to parse and to trans-late jointly by taking tree-based translation as pars-ing.
Given a source sentence, this decoder producesa parse tree on the source side and a translation onthe target side simultaneously.
Both the models per-form in the unit of tree nodes rather than chunks.6 Experiments6.1 Data PreparationData for shallow parsing We got training data andtest data for shallow parsing from the standard PennTree Bank (PTB) English parsing task by splittingthe sections 02-21 on the Wall Street Journal Portion(Marcus et al, 1993) into two sets: the last 1000sentences as the test set and the rest as the trainingset.
We filtered the features whose frequency waslower than 3 and substituted ??
and ??
with ?
tokeep consistent with translation data.
We used L2algorithm to train CRF.Data for Translation We used the NIST trainingset for Chinese-English translation tasks excludingthe Hong Kong Law and Hong Kong Hansard2 as thetraining data, which contains 470K sentence pairs.For the training data set, we first performed wordalignment in both directions using GIZA++ toolkit(Och and Ney, 2000) then refined the alignmentsusing ?final-and?.
We trained a 5-gram languagemodel with modified Kneser-Ney smoothing on theXinhua portion of LDC Chinese Gigaword corpus.For the tree-to-string model, we parsed English sen-tences using Stanford parser and extracted rules us-ing the GHKM algorithm (Galley et al, 2004).We used our in-house English-Chinese data setas the development set and used the 2008 NISTEnglish-Chinese MT test set (1859 sentences) as thetest set.
Our evaluation metric was BLEU-4 (Pap-ineni et al, 2002) based on characters (as the tar-get language is Chinese), which performed case-insensitive matching of n-grams up to n = 4 andused the shortest reference for the brevity penalty.We used the standard minimum error-rate training(Och, 2003) to tune the feature weights to maximizethe BLEU score on the development set.6.2 Shallow ParsingThe standard evaluation metrics for shallow parsingare precision P, recall R, and their harmonic meanF1 score, given by:P = number of exactly recognized chunksnumber of output chunksR = number of exactly recognized chunksnumber of reference chunks2The source side and target side are reversed.955Word number Chunk number Accuracy %23861 12258 94.48Chunk type P % R % F1 % FoundAll 91.14 91.35 91.25 12286One 90.32 90.99 90.65 5236NP 93.97 94.47 94.22 5523ADVP 82.53 84.30 83.40 475VP 93.66 92.04 92.84 284ADJP 65.68 69.20 67.39 236WHNP 96.30 95.79 96.04 189QP 83.06 80.00 81.50 183Table 1: Shallow parsing result.
The collum Found givesthe number of chunks recognized by CRF, the row Allrepresents all types of chunks, and the row One representsthe chunks that consist of one word.F1 =2 ?
P ?
RP +RBesides, we need another metric, accuracy A, toevaluate the accurate rate of individual labeling de-cisions of every word asA = number of exactly labeled wordsnumber of wordsFor example, given a reference sequenceB-NP I-NP I-NP B-VP I-VP B-VP, CRF out-puts a sequence O-NP I-NP I-NP B-VP I-VP I-NP,then P = 33.33%, A = 66.67%.Table 1 summaries the results of shallow parsing.For ??
and ??
were substituted with ?
, the perfor-mance was slightly influenced.The F1 score of all chunks is 91.25% and the F1score of One and NP, which in number account forabout 90% of chunks, is 90.65% and 94.22% respec-tively.
F score of NP chunking approaches 94.38%given in Sha and Pereira (2003).6.3 Performance ComparisonWe compared our loose decoder and tight decoderwith our in-house hierarchical phrase-based decoder(Chiang, 2007) and the tree-to-string decoder (Liu etal., 2006).
We set the same configuration for all thedecoders as follows: stack size = 30, nbest size = 30.For the hierarchical chunk-based and phrase-baseddecoders, we set max rule length to 5.
For the tree-to-string decoder, we set the configuration of ruleSystem Dev NIST08 Speedphrase 0.2843 0.3921 1.163tree 0.2786 0.3817 1.107tight 0.2914 0.3987 1.208loose 0.2936 0.4023 1.429Table 2: Performance comparison.
Phrase representsthe hierarchical phrase-based decoder, tree represents thetree-to-string decoder, tight represents our tight decoderand loose represents our loose decoder.
The speed is re-ported by seconds per sentence.
The speed for the tree-to-string decoder includes the parsing time (0.23s) and thespeed for the tight and loose models includes the shallowparsing time, too.extraction as: the height up to 3 and the number ofleaf nodes up to 5.We give the results in Table 2.
From the results,we can see that both the loose and tight decodersoutperform the baseline decoders and the improve-ment is significant using the sign-test of Collins etal.
(2005) (p < 0.01).
Specifically, the loose modelhas a better performance while the tight model has afaster speed.Compared with the hierarchical phrase-basedmodel, the loose model only imposes syntactic cohe-sion cohesion to nonterminals while the tight modelimposes syntax cohesion to both rules and nonter-minals which reduces search space, so it decodersfaster.
We can conclude that linguistic syntax canindeed improve the translation performance; syntac-tic cohesion for nonterminals can explain linguis-tic phenomena well; noncohesive rules are useful,too.
The extra time consumption against hierarchi-cal phrase-based system comes from shallow pars-ing.By investigating the translation result, we find thatour decoder does well in rule selection.
For exam-ple, in the hierarchical phrase-based model, this kindof rules, such asX ?
?X of X, ?
?, X ?
?X for X, ?
?and so on, where ?
stands for the target component,are used with a loose restriction as long as the ter-minals are matched, while our models employ morestringent constraints on these rules by specifying thesyntactic constituent of ?X?.
With chunk labels, ourmodels can make different treatment for differentsituations.956System Dev NIST08 Speedcohesive 0.2936 0.4023 1.429noncohesive 0.2937 0.3964 1.734Table 3: Influence of cohesion.
The row cohesive rep-resents the loose system where nonterminals satisfy co-hesion, and the row noncohesive represents the modifiedversion of the loose system where nonterminals can benoncohesive.Compared with the tree-to-string model, the re-sult indicates that the change of the source-side lin-guistic syntax from parses to chunks can improvetranslation performance.
The reasons should be ourmodel can reduce parse errors and it is enough to usechunks as the basic unit for machine translation.
Al-though our decoders and tree-to-string decoder allrun in linear-time with beam search, tree-to-stringmodel runs faster for it searches through a smallerSCFG-motivated space.6.4 Influence of CohesionWe verify the influence of syntax cohesion via theloose model.
The cohesive model imposes syntaxcohesion on nonterminals to ensure the chunk is re-ordered as a whole.
In this experiment, we introducea noncohesive model by allowing a nonterminal tomatch part of a chunk.
For example, in the nonco-hesive model, it is legal for a rule with the sourceside?NP for NP-NP?to match?request for a purchase of shares?in Figure 1 (a), where ?request?
is part of NP.
Aswell, the rule with the source side?NP for a NP-NP?can match?request for a purchase of shares?.In this way, we can ensure all the rules used in thecohesive system can be used in the noncohesive sys-tem.
Besides cohesive rules, the noncohesive systemcan use noncohesive rules, too.We give the results in Table 3.
From the results,we can see that cohesion helps to reduce searchspace, so the cohesive system decodes faster.
Thenoncohesive system decoder slower, as it employsSystem Number Dev NIST08 Speedloose two 0.2936 0.4023 1.429loose three 0.2978 0.4037 2.056tight two 0.2914 0.3987 1.208tight three 0.2954 0.4026 1.780Table 4: The influence of the number of nonterminals.The column number lists the number of nonterminalsused at most in a rule.more rules, but this does not bring any improvementof translation performance.
As other researches saidin their papers, syntax cohesion can explain linguis-tic phenomena well.6.5 Influence of the number of nonterminalsWe also tried to allow a rule to hold three nonter-minals at most.
We give the result in Table 4.
Theresult shows that using three nonterminals does notbring a significant improvement of translation per-formance but quite more time consumption.
So weonly retain two nonterminals at most in a rule.7 ConclusionIn this paper, we present a hierarchical chunk-to-string model for statistical machine translationwhich can be seen as a compromise of the hierarchi-cal phrase-based model and the tree-to-string model.With the help of shallow parsing, our model learnsrules consisting of either words or chunks and com-presses adjacent chunks in a rule to a nonterminal,then it searches for the best derivation under theSCFG defined by these rules.
Our model can com-bine the merits of both the models: employing lin-guistic syntax to direct decoding, being syntax co-hesive and robust to parsing errors.
We refine the hi-erarchical chunk-to-string model into two models: aloose model (more similar to the hierarchical phrase-based model) and a tight model (more similar to thetree-to-string model).Our experiments show that our decoder can im-prove translation performance significantly over thehierarchical phrase-based decoder and the tree-to-string decoder.
Besides, the loose model gives a bet-ter performance while the tight model gives a fasterspeed.9578 AcknowledgementsWe would like to thank Trevor Cohn, Shujie Liu,Nan Duan, Lei Cui and Mo Yu for their help,and anonymous reviewers for their valuable com-ments and suggestions.
This work was supportedin part by EPSRC grant EP/I034750/1 and in partby High Technology R&D Program Project No.2011AA01A207.ReferencesColin Cherry.
2008.
Cohesive phrase-based decoding forstatistical machine translation.
In Proc.
of ACL, pages72?80.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
of ACL,pages 263?270.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33:201?228.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proc.
of ACL, pages 531?540.Jason Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In Proc.
of ACL, pages205?208.Yang Feng, Haitao Mi, Yang Liu, and Qun Liu.
2010.
Anefficient shift-reduce decoding algorithm for phrased-based machine translation.
In Proc.
of Coling:Posters,pages 285?293.Heidi Fox.
2002.
Phrasal cohesion and statistical ma-chine translation.
In Proc.
of EMNLP, pages 304?3111.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proc ofNAACL, pages 273?280.Jonathan Graehl and Kevin Knight.
2004.
Training treetransducers.
In Proc.
of HLT-NAACL, pages 105?112.J Hammersley and P Clifford.
1971.
Markov fields onfinite graphs and lattices.
In Unpublished manuscript.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of AMTA.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proc.
of HLT-NAACL, pages 127?133.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proc.
ofICML, pages 282?289.Yang Liu and Qun Liu.
2010.
Joint parsing and transla-tion.
In Proc.
of COLING, pages 707?715.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proc.
of COLING-ACL, pages 609?616.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics, 19:313?330.Yuval Marton and Philip Resnik.
2008.
Soft syntacticconstraints for hierarchical phrased-based translation.In Proc.
of ACL, pages 1003?1011.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proc.
of ACL, pages 192?199.Thai Phuong Nguyen, Akira Shimazu, Tu Bao Ho,Minh Le Nguyen, and Vinh Van Nguyen.
2008.
Atree-to-string phrase-based model for statistical ma-chine translation.
In Proc.
of CoNLL, pages 143?150.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proc.
of ACL.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In Proc.
of ACL, pages 295?302.Frans J. Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30:417?449.Frans J. Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proc.
of ACL, pages160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalu-ation of machine translation.
In Proceedings of ACL,pages 311?318.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal smt.
In Proceedings of ACL, pages 271?279.Fei Sha and Fernando Pereira.
2003.
Shallow pars-ing with conditional random fields.
In Proc.
of HLT-NAACL, pages 134?141.Ben Taskar, Pieter Abbeel, and Daphne Koller.
2002.Discriminative probabilistic models for relational data.In Eighteenth Conference on Uncertainty in ArtificialIntelligence.Taro Watanabe, Eiichiro Sumita, and Hiroshi G. Okuno.2003.
Chunk-based statistical translation.
In Proc.
ofACL, pages 303?310.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23:377?403.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proc.
of ACL, pages523?530.958
