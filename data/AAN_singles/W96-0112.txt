A Probabilistic Disambiguation Method Based on PsycholinguisticPrinciplesHang LiC&C Research Laboratories, NEC Corporationl ihang?sbl, cl.
nec.
co. jpAbst rac tWe address the problem of structural disambiguation i  syntactic parsing.
In psycholinguistics, anumber of principles of disambiguation have been proposed, notably the Lexical Preference Rule(LPR), the Right Association Principle (RAP), and the Attach Low and Parallel Principle (ALPP).We argue that in order to improve disambiguation results it is necessary to implement these prin-ciples on the basis of a probabilistic methodology.
We define a 'three-word probability' for im-plementing LPR, and a 'length probability' for implementing RAP and ALPP.
Furthermore, weadopt the 'back-off' method to combine these two types of probabilities.
Our experimental resultsindicate our method to be effective, attaining an accuracy of 89.2%.1 In t roduct ionStructural disambiguation is still a central problem in natural anguage processing.
To completelyresolve ambiguities, we would need to construct a human-like language understanding system(c.f.
(Altmann and Steedman, 1988; Johnson-Laird, 1983)).
The construction of such a system isextremely difficult, however, and we need to adopt a more realistic approach.
In psycholinguistics,a number of principles have been proposed which attempt o modelize the human disambiguationprocess.
The Lexical Preference Rule (LPR) (Ford et al, 1982), the Right Association Principle(RAP) (Kimball, 1973), and the Attach Low and Parallel Principle (ALPP, an extension of RAP)(Hobbs and Bear, 1990) have been proposed, and it is thought hat we might resolve ambiguitiesquite satisfactorily if we could implement these principles ufficiently (Hobbs and Bear, 1990; Whit-temore et al, 1990).
Methods of implementing these principles have also been proposed (e.g.,(Shieber, 1983; Wermter, 1989; Wilks et al, 1985)).
An alternative approach is to view languageas a stochastic phenomenon, particularly from the viewpoint of information theory and statistics.If we could properly define a probability model 1and calculate the likelihood value of each interpre-tation using the model, we might also resolve ambiguities quite well.
There have been a number ofmethods proposed to perform structural disambiguation using probability models, many of whichhave proved to be quite effective (Alshawi and Carter, 1995; Black et al, 1992; Briscoe and Carroll.1993; Chang et al, 1992; Collins and Brooks, 1995; Fujisaki, 1989; Hindle and Rooth, 1991; Hindleand Rooth, 1993; Jelinek et al, 1990; Magerman and Marcus, 1991; Magerman, 1995; Ratnaparkhiet al, 1994; Resnik, 1993; Su and Chang, 1988).Although each of the disambiguation methods proposed to date has its merits, none resolvesthe disambiguation problem completely satisfactorily.
We feel that it is necessary to devise a newmethod that unifies the above two approaches, i.e., to implement psycholinguistic principles ofdisambiguation  the basis of a probabilistic methodology.
Most psycholinguistic principles have1A representat ion of a probabi l i ty distr ibut ion is called a 'probabi l i ty model, '  or simplely a 'model.
'141been developed on the basis of a vast data base of actual observations, and thus a method based onthem is expected to achieve good disambiguation results.
Probabilistic methods of implementingthese principles have the merit of being able to handle noisy data, as well as being able to employa principled methodology for acquiring the knowledge necessary for disambiguation.LPR, RAP and ALPP are known to be effective for disambiguation, and these are the oneswhose implementation we consider in the present paper.
Thus our problem involves the followingthree subproblems: (a) resolving structural ambiguities based on LPR in terms of probabilisticrepresentations, (b) resolving structural ambiguities based on RAP and ALPP in terms of prob-abilistic representations, and (c) combining the two.
For subproblem (a), we have devised a newmethod, based on LPR, which has some good properties not shared by the methods proposed sofar (Alshawi and Carter, 1995; Chang et al, 1992; Collins and Brooks, 1995; Hindle and Rooth,1991; Ratnaparkhi et al, 1994; Resnik, 1993).
In (Li and Abe, 1995), we have described this methodin detail.
In the present paper, we mainly describe our solutions to subproblems (b) and (c).
Forsubproblem (b), we point out that the notion of the 'length' of a syntactic ategory 2is important,and propose to use a 'length probability' to perform structural disambiguation.
For subproblem(c), we propose to adopt the 'back-off' method, i.e., to make use first of a lexical likelihood basedon LPR, and then a syntactic likelihood based on RAP and ALPP.
Experiments conducted to testthe effectiveness of our method demonstrate an encouraging accuracy of 89.2%.2 Psycholinguistic Principles of DisambiguationIn this section, we introduce the psycholinguistic principles of disambiguation.
Kimball has pro-posed the Right Association Principle (RAP) (Kimball, 1973), which states that (in English) aphrase on the right should be attached to the nearest phrase on the left if possible.
Hobbs & Bearhave generalized RAP to the Attach Low and Parallel Principle (ALPP) (Hobbs and Bear, 1990).ALPP states that a phrase on the right should be attached to the nearest phrase on the left ifpossible, and that phrases hould be attached to a phrase in parallel if possible.
(When we referto ALPP.
we ordinarily mean just the part concerning attachments in parallel. )
Ford et ah haveproposed the Lexica\] Preference Rule (LPR) which states that an interpretation is to be preferredwhose case frame assumes more semantically consistent values (Ford et al, 1982).
Classically, lexi-cal preference is realized by checking consistencies between 'semantic features' of slots and those ofslot vMues, namely the 'selectionM restrictions' (Katz and Fodor, 1963).
The realization of lexicalpreference in terms of selectional restrictions has some disadvantages, however.
Interpretationsobtained in an analysis cannot, for example, be ranked in their preferential order.
Thus one cannotadopt a strategy of retaining the N most plausible interpretations in an analysis, which is the mostwidely accepted practice at present.
In fact it is more appropriate to treat the lexical preference asa kind of score representing the association between slots and their values.
In the present paper,we refer to this kind of score as 'lexical preference.'
For the same reason, we also treat 'syntacticpreference' as a kind of score.LPR is a lexical semantic principle, while RAP and ALPP are syntactic ones, and in psycholin-guistics it is commonly claimed that LPR overrides RAP and ALPP (Hobbs and Bear, 1990).
Letus consider some examples of LPR and RAP in this regard.
For the sentenceI ate ice cream with a spoon, (1)there are two interpretations; one is 'I ate ice cream using a spoon' and the other 'I ate ice cream anda spoon.'
In this sentence, a human speaker would certainly assume the former interpretation ver2The length of a syntactic ategory in simply defined as the number of words contained in that category.142the latter.
From the psycholinguistic perspective, this can be explained in the following way: theformer interpretation has a stronger lexical preference than the latter, and thus is to be preferredaccording to LPR.
Moreover, since LPR overrides RAP, the preference is solely determined by LPR.For the sentenceJohn phoned a man in Chicago, (2)there are two interpretations; one is 'John phoned a man who is in Chicago' and the other 'John,while in Chicago, phoned a man.'
In this sentence, a human speaker would probably assume theformer interpretation over the latter.
The two interpretations have an equal exical preference value,and thus the preference of the two cannot be determined by LPR.
After LPR fails to work, theformer interpretation is to be preferred according to RAP, because 'a man' is closer to 'in Chicago'than 'phone' in the sentence.LPR implies that (in natural anguage) one should communicate asrelevantly as possible, whileRAP and ALPP implies that one should communicate asefficiently as possible.
Although the phe-nomena governed by these principles vary from language to language, the principles themselves,we think, are language independent, and thus can be regarded as fundamental principles of humancommunication.
According to Whittemore t al.
and Hobbs & Bear, nearly all of the ambiguitiescan be resolved by first applying LPR and then RAP and ALPP (Hobbs and Bear, 1990; Whit-temore et al, 1990).
These observations motivate us strongly to implement these principles fordisambiguation purposes.While there are also other principles proposed in the literature, including the Minimal Attach-ment Principle (Frazier and Fodor, 1979), they are generally either not highly functional or arecovered by the above three principles in any case (Hobbs and Bear, 1990; Whittemore t al., 1990).The necessity of developing a disambiguation method with learning ability has recently cometo be widely recognized.
The realization of such a method would make it possible to (a) save thecost of defining knowledge by hand (b) do away with the subjectivity inherent in human definition(c) make it easier to adapt a natural anguage analysis system to a new domain.
We think that aprobabilistic approach is especially attractive because it is able to employ a principled methodologyfor acquiring the knowledge necessary for disambiguation.
In our research, we implement LPR, RAPand ALPP by means of a probabilistic methodology.3 LPR and Lexical  L ikel ihoodIn this section, we briefly describe our LPR-based probabilistic disambiguation method.3.1 The  three-word  probab i l i tyWe refer to a syntactic tree and its corresponding case frame, as obtained in an analysis, 'aninterpretation.
'3 After analyzing the sentence in (1), for example, we obtain the case frames of theinterpretations:eat:\[argl I, arg2 ice_cream, with spoon\], (3)andeat:\[argl I, arg2 ice_cream: \[with spoon\]\].
(4)The value assumed by a case slot of a case frame of a verb can be viewed as being generatedaccording a conditional probability distribution:P(nlv, s),3We do not take into account ambiguities caused by word senses.
(5)143where random variable v takes on a value of a set of verbs, n a value of a set of nouns, and .s avalue of a set of slot names.
Similarly, the value assumed by a case slot of a case frame of a nouncan be viewed as being generated by a conditional probability distribution: P(nln , s).
We call thiskind of conditional probability the 'three-word probability.'
Moreover, we assume that the three-word probabilities in the case frame of an interpretation are mutually independent, and define thegeometric mean of the three-word probabilities as the 'lexical ikelihood' of the interpretation:mpt x(x) = (H (6)i=1where Pi is the ith three-word probability in the case frame of interpretation I,  and m the numberof three-word probabilities in it.
The lexical likelihood values of the two interpretations in (3) and(4) are thus calculated asPt~x(I1) = (P(Ileat, argl) x P(ice_creamleat,arg2 ) ?
P(spoonleat,with))!/3, (7)andPzex(I2) = (P(Ileat, argl) ?
P(ice_creamleat , rg2) ?
P(spoon\]ice_cream, with)) 1/3.
(8)In disambiguation, we simply rank the interpretations according to their lexical ikelihood values.
Ifa verb (or a noun) has a strong tendency to require a certain noun as the value of its case frame slot,the estimated three-word probability for such a co-currence will be very high.
To prefer an inter-pretation with a higher lexical likelihood value, then, is to prefer it based on its lexical preference.Specifically, in order to perform pp-attachment disambiguation i  analysis of sentences like (1), weneed only calculate and compare the values of P(spoonleat, with ) and P(spoonlice_cream,with ).In sentences likeA number of companies ell and buy by computer, (9)the number of three-word probabilities in each of its respective case frames will be different.
If wewere to define a lexical likelihood as the product of the three-word probabilities in the case frame ofan interpretation, an interpretation with fewer case slots would be preferred.
We use the definitionof lexical likelihood described above to avoid this problem.
43.2 The data sparseness problemHindle & Rooth have previously proposed resolving pp-attachment ambiguities with 'two-wordprobabilities' (Hindle and Rooth, 1991), e.g., P(withlice_cream),P(withleat), but these are notaccurate nough to represent lexical preference.
For example, in the sentences,Britain reopened the embassy in December,Britain reopened the embassy in Teheran, (10)the pp-attachment sites of the two prepositional phrases are different.
The attachment sites wouldbe determined to be the same, however, if we were to use two-word probabilities (c:f.(Resnik, 1993)),and thus the ambiguity of only one of the sentences can be resolved.
It is very likely, however, thatthis kind of ambiguity could be resolved satisfactorily by using the three-word probabilities.The number of para.meters that need to be estimated increases drastically when we use three-word probabilities, and the data available for estimation of the probability parameters usually are4An alternative for resolving coordinate structure ambiguities i to employ a method which examines the similaritythat exists between conjuncts (c.f.
(Kutohashi and Na~ao, 1994; Resnik, 1993)).144not sufficient in practice.
If we employ the Maximum Likelihood Estimator, we may find most ofthe parameters are estimated to be 0: a problem often referred to, in statistical natural anguageprocessing, as the 'data sparseness problem.'
(the motivation for using the two-word probabilities in(Hindle and Rooth, 1991) appears to be a desire to avoid the data sparseness problem. )
One mayexpect his problem to be less severe in the future, when more data are available.
However, as datasize increases, new words may appear, and the number of parameters that need to be estimatedmay increase as well.
Thus, the data sparseness problem is unlikely to be resolved.
A number ofmethods have been proposed, however, to cope with the data sparseness problem.
Chang et al,for instance, have proposed replacing words with word classes and using class-based co-occurrenceprobabilities (Chang et al, 1992).
However, forcibly replacing words with certain word classesis too loose an approximation, which, in practice, could seriously degrade disambiguation results.Resnik has defined a probabilistic measure called 'selectional association' in terms of the wordclasses existing in a given thesaurus.
While Resnik's method is based on an interesting intuition,the justification of this method from the viewpoint of statistics is still not clear.
We have deviseda method of estimating the three-word probabilities in an efficient and theoretically sound way(Li and Abe, 1995).
Our method selects optimal word classes according to the distribution ofgiven data, and smoothes the three-word probabilities using the selected classes.
Experimentalresults indicate that our method improves upon or is at least as effective as existing methods.Using our method of estimating (smoothing) probabilities, we can cope with the data sparsenessproblem.
However, for the same reason as described above, the data sparseness problem cannot beresolved completely.
We propose combining the use of three-word probabilities and that of two-word probabilities.
Specifically, we first use the lexical likelihood value calculated as the geometricmean of the three-word probabilities of an interpretation, and when the lexical likelihood values ofobtained interpretations are equal, including the case in which all of them are 0, we use the lexicallikelihood value calculated as the geometric mean of the two-word probabilities of an interpretation.4 RAP,ALPP,  and Syntactic LikelihoodIn this section, we describe our probabilistic disambiguation method based on RAP and ALPP.4.1 The deterministic approachShieber has previously proposed incorporating RAP into the mechanism of a shift-reduce parser(Shieber, 1983).
When RAP is implemented, the parser prefers shift to reduce whenever a 'shift-reduce conflict' occurs.
The advantage of this deterministic approach is its simple mechanism,while the disadvantage is that although it can output the most preferred interpretation, it cannotrank interpretations in their preferential order.
In order to be able to rank interpretations in thisway, it is necessary to construct a parser which operates tochastically, not deterministically.4.2 Formalizing a syntactic preferenceIn this subsection, we formalize a syntactic preference based on RAP and ALPP.
While we borrowfrom the terminology of HPSG (Pollard and Sag, 1987) in our reference to 'head' categories, wealso use the single term 'modifier' categories to refer to categories which HPSG would classify asbeing either 'complements' or 'adjuncts.'
We refer to that word which exhibits the subcategoryfeature of a category to be that category's 'head word.
'Let us consider a simple case in which we are dealing with a modifier category M, a headcategory H, and the head word of H, w. We first apply CFG rule L --  H, M to H and M, yieldingcategory L (see Figure l(a)).
We refer to the number of words in a given sequence as 'distance.
'145L L(a) (b)Figure 1: RAP, ALPP and lengthAs may be seen in Figure l(a), the distance between M and w is d. RAP prefers an interpretationwith a smaller d. Thus, syntactic preference can be represented by a monotonically decreasingfunction of d. Since in English the head word w of category H tends to locate near its left corner,we can approximate d as l, the number of words contained in H. In this paper, we call the numberof words contained in a category the 'length' of that category.
In addition, syntactic preferencealso depends on type of head category and modifier category.
Assuming that 1 is known to be 5, ifH is a verb phrase and M is a prepositional phrase, the preference value is likely to be high, but ifH is a noun phrase and M is a prepositional phrase, it is likely to be low.
Since category type canbe specified within a CFG rule, syntactic preference can be defined as a function of a CFG rule.Syntactic preference based on RAP can be formalized, then, as a function of CFG rule L -- H, Mand length l, namely,S(l, (L ~ H, M)).
(11)Suppose that categories R1 and R2 form a coordinate structure, and 11 and 12 are the lengthsof R1 and R2, respectively.
ALPP prefers categories forming a coordinate structure to be of equallength (see Figure l(b)).
Preference value will be high when ll equals 12, and syntactic preferencebased on ALPP 's can be defined asS(ll,12,(L ~ Ri,C, R2)).
(12)Further, suppose that categories R1, R2, .
.
.
,  Rk are combined into category A, and 11,12,..., lkare the lengths of R1, R2,... ,  Rk, respectively.
Syntactic preference of the attachment can then bedefined asS(ll, I2,..., Ik, (L ~ R1, R2, .
.
.
,  Rk)).
(13)Note that (13) contains (11) and (12).
Furthermore, we assume that the attachments in thesyntactic tree of an interpretation are mutually independent, and we define the product (or thesum, depending on the preference function) of the syntactic preference values of the attachmentsin the syntactic tree of the interpretation as the syntactic preference of the interpretation:ms .n(I) = (14)i= lwhere Si denotes the syntactic preference value of the ith attachment in the syntactic tree ofinterpretation I, and m the number of attachments in it.5This kind of syntactic preference requires that the CFG rules for coordinate structures have the form LRi, C, R2, C,.
.
.
,  C, Rk.1464.3 The  length  probab i l i tyWe now consider how to specify the syntactic preference function in (13).
As there are any numberof ways to formulate the function (note the fact that syntactic preference is also a function of aCFG rule.
), it is nearly impossible to find the most suitable formula experimentally.
To cope withthis problem, we used machine learning techniques (recall the merits of using machine learningtechniques in disambiguation, as described in Section 2).
Specifically, we have defined a probabilitymodel to calculate syntactic preference.
Suppose that attachments represented by CFG rules andlengths are extracted from the correct syntactic trees in training data, and the frequency of eachkind of attachment is obtained asf( l l ,  12, .
.
.
.
4, (L ~ R1, R2,.
?
?, Rk )), (15)where L ~ R1, R2,.
.
.
,  Rk denotes a CFG rule, and 11,12,..., Ik denote the lengths of R1, R2,.
?
?, Rk~respectively.
RAP prefers an interpretation attached to a nearer phrase, while ALPP prefers inter-pretations with attachments hat are low and in parallel.
Many such attachments may be observedin the training data, and we can formulate the frequencies of attachments (15) as a syntactic pref-erence.
Considering the fact that individual rules will be applied with different frequency, it isdesirable to modify the syntactic preference tof( l l ,  12,..., lk, (L ~ R1, R2 , .
.
.
,  Rk )) (16)f((L -- R1, R2,.
.
.
,  Rk)) 'where f ((L -+ R1, R2,.
.
.
,  Rk)) denotes the frequence of application of CFG rule L --+ R1, R~,.
.
.
,  Rk.This is precisely the 'length probability' we propose in this paper.Let us now define the length probability more formally.
Suppose that an attachment is obtainedafter the application of C FG rule L -+ R1, R2,.
?
?, Rk, the lengths of R1, R2, .
.
.
,  Rk are 11,12,.
?., 4,respectively.
The attachment can be viewed as being generated by the following conditional distri-bution:P(li, 12,..., lk\[(L -+ Ra, R2, .
.
.
,  Rk)).
(17)We call this kind of conditional probability the 'length probability.'
6 Furthermore, the syntacticlikelihood of an interpretation is defined as the geometric mean of the length probabilities of theattachments in the syntactic tree of the interpretation, assuming that the attachments are mutuallyindependent:mPsyn(I) (1~ Pi)?
= m, (18)i=1where Pi is the ith length probability in the syntactic tree of interpretation I, and m the numberof length probabilities in it.
We define syntactic likelihood as the geometric mean of the lengthprobabilities, rather than as the product of the length probabilities, in order to factor out the effectof the different number of attachments in the syntactic trees of individual interpretations.
Whentraining the length probabilities, the parameters in (16) may be estimated using the frequences in(15).Next, let us consider a simple example illustrating how the operation of this model indicatesthe functioning of RAP.
For the phrase shown in Figure 2(a), there are two interpretations; RAP6The number of parameters in a length probabil ity model depends on k - the number of categories on the right-hand side of a CFG rule, and N - the maximum value of lengths of a category on the left-hand side of the rule:~i=k-1 k - 1 - 1 = k - 1.
As k is very small (in our case k < 3), the number of parameters in a lengthprobabil ity model is of N 's  polynomial order.147would necessarily prefer the former.
The difference between the syntactic likelihood values of thetwo interpretations is solely determined byandP(1,5I(PP -- P, NP))  ?
P(2,6I(NP - .
NP, PP)), (19)P(1,21(PP -- P, NP))  ?
P(5,3I(NP -- NP, PP)).
(20)First, let us compare the left-hand length probabilities of (19) and (20).
Both represent an attach-ment of NP to P, and the length of P is 1 in both terms.
Thus the two estimated probabilities maynot differ so greatly.
Next, compare the right-hand length probabilities in (19) and (20).
Whileboth represent an attachment of PP  to NP, the length of NP of the former is 2 and that of thelatter is 5.
Thus the second length probability in (19) is likely to be higher than that in (20), as intraining data there are more phrases attached to nearby phrases than are attached to distant ones.Therefore, when we use only the syntactic likelihood to perform disambiguation, we can expect heformer interpretation i Figure 2(a) to be preferred, i.e., we have an indication of the functioningof RAP.Let us consider another example illustrating how the operation of the length probability modelindicates the functioning of ALPP.
For the sentence shown in Figure 2(b), there are two interpre-tations; ALPP would necessarily prefer the former.
The difference between the syntactic likelihoodvalues of the two interpretations is solely determined byP(3,21(VP ---+ VP, PP)) x P(1,1,11(VP ---~ VP, C, VP)), (21)andP(1,21(VP -- VP, PP)) x P(1,1,3I(VP -- VP, C, VP)).
(22)First, let us compare the left-hand length probabilities in (21) and (22).
Both represent an at-tachment ofPP to VP, but the length of VP of the former is 3 and that of the latter is I. Theleft-hand probability in (21) is likely to be lower than that in (22).
Next, compare the right-handlength probabilities in (21) and (22).
Both represent a coordinate structure consisting of VPs.
Thelengths of VPs in the latter are equal, while the lengths of VPs in the former are not.
Thus theright-hand probability in (21) is likely to be higher than that in (22).
Moreover, the differencebetween the right-hand probabilities i likely to be higher than that between the left-hand prob-abilities, and thus the syntactic likelihood value of the former interpretation will be higher thanthat of the latter.
Therefore, when we use only the syntactic likelihood to perform disambiguation,we can expect he former interpretation in Figure 2(b) to be preferred.4.4 Re la ted  workAnother approach to disambiguation is to define a probability model and to rank interpretations onthe basis of syntactic parsing.
One method of this type employs the well-known PCFG (ProbabilisticContext Free Grammar) model (Fujisaki, 1989; Jelinek et al, 1990; Lari and Young, 1990).
InPCFG, a CFG rule having the form of a, -- 3 is associated with a conditional probability P(~Ia),and the likelihood of a syntactic tree is defined as the product of the conditional probabilities ofthe rules which are applied in the derivation of that tree.
Other methods have also been proposed.Magerman ~ Marcus, for instance, have proposed making use of a conditional probability modelspecifying a conditional probability of a CFG rule, given the part-of-speech trigram it dominatesand its parent rule (Magerman and Marcus, 1991).
Black et al have defined a richer model toutilize all the information in the top-down derivation of a non-terminal (Black et al, 1992).
Briscoe& Carroll have proposed using a probabilistic model specific to LR parsing (Briscoe and Carroll,1993).148the  b lock  on  the  tab le  in  the  roomthe  b lock  P: I  N P :5II IIthe  tab le  in  the  roomNP:~NP:2  PP :3  PP :3Ii iithe  b lock  P: I  NP :2  in  the  roomil flon the  tab leNon- termina l :  l engthA number  of  compan ies  se l l  end  buy by computerby computerIt'se l l  and  buyse l l  andbuy by computerNon-terminel :  l ength(a) (b)Figure 2: Examples of syntactic parsingThe advantage of the syntactic parsing approach is that it mGv embody heuristics (principles)effective in disambiguation, which would not have been thought of by humans, but it also risks notembodying heuristics (principles) already known to be effective in disambiguation.
For example,the two interpretations of the noun phrase shown in Figure 2(a) have an equal likelihood value, ifwe employ PCFG, although the former would be preferred according to RAP.5 The Back-Off MethodHaving defined a lexical likelihood based on LPR and a syntactic likelihood based on RAP andALPP, we may next consider how to combine the two kinds of likelihood in disambiguation.
Onechoice is to calculate total preference as a weighted average of likelihood values, as proposed in(Alshawi and Carter, 1995).
However since LPR overrides RAP and ALPP, a simpler approach isto adopt the back-off method, i.e., to rank interpretations/1 and I2 as follows:1. if Plex( I1)-  Pl=(Is) > r/ then /1 >/22.
else if Plex(I2) - Plex(I1) > 7/ then Is >/13.
else if P~yn(I1)-  P~yn(Is) > r then /1 > Is4.
else if P~yn( Is) -  P~yn(I1) > r then /2 >/1(23)where/1 and/2 denote any two interpretations, Pl=() denotes the lexical likelihood of an interpre-tation, and Psyn() the syntactic likelihood of an interpretation.
~ > 0 and r > 0 are thresholds (inthe experiment described later, both are set to 0).
Note that in lines 3 and 4, IPtex(I1)-Pzex(I2)l < r Iholds.
Further note that the preferential order cannot be determined (or can only be determinedat random) when IPi=(I1) - Plex(Is)\] _< ~ and IPsyn(I1) - Psyn(Is)\] ~ 7".6 Exper imenta l  ResultsWe have conducted experiments to test the effectiveness of our proposed method.
This sectiondescribes the results.
In the experiments, we considered only resolving pp-attachment ambigui-149ties and coordinate structure ambiguities.
These two kinds of ambiguities are typical, and otherambiguities can be resolved in the same way (Hobbs and Bear, 1990).We first defined 12 CFG rules as our grammar to be used by a parser which calculates a prefer-ence for each partial interpretation, and always retains the N most preferable partial interpretations 7.We have not yet actually constructed such a parser, however, and use a parser called 'SAX,' pre-viously developed by Matsumoto & Sugimura (Matsumoto and Sugimura, 1986), which calculatesa preference for each interpretation after it obtains each interpretation.We then trained the parameters of probability models.
We extracted 181,250 case frames fromthe WSJ (Wall Street Journal) bracketed corpus of the Penn Tree Bank (Marcus et al, 1993).We used these data to estimate three-word probabilities and two-word probabilities?
Furthermore,we extracted 963 sentences from the WSJ tagged corpus of the Penn Tree Bank.
We used SAXto analyze the sentences and selected the correct syntactic trees by hand.
We then employedthe Maximum Likelihood Estimator to estimate length probabilities using the selected syntactictrees, e.g., if CFG rule NP ~ NP, PP  is applied x times, and among the attachments obtained byapplying this rule, xi of them have the lengths of 2 and 3, then the length probability P(2, 31(NPNP, PP)) is estimated as ~.
It is known, in statistics, that the number of samples required foraccurate stimation of a probabilistic model is roughly proportional to the number of parametersin the target model, and thus the data used for training length probabilities were nearly sufficient.Figure 3 plots the estimated length probabilities versus the lengths, for two CFG rules.
The resultindicates that there are more attachments attached to nearby phrases than are attached to distantones in the training data.
Moreover, the length probabilities for CFG rule VP ~ VP, PP  andthose for CFG rule NP ~ NP, PP  show different distribution patterns, suggesting that syntacticpreference is a function of a CFG rule.0.30.250.20.150.10.0500.3,0.250.20.150.10.052 0 ~ 5 g ?
(a) (b)Figure 3: Length probability versus lengthWe then extracted 249 sentences from a part of thein training as our test data and analyzed the sentences.obtained interpretations a  follows:ifelse ifelse ifelse ifelse ifelse iftagged WSJ corpus which was not usedWhen analizing a sentence, we rank thetit is necessary to do so, as the numberincreases (Church and Patil, 1982).Ptex3(I1) > P/~x3(I2) thenPlex3(I2) > Pl~x3(I1) thenPte,2(I1) > Pzex2(I2) thenPzex2(I2) > Pt~2(I1) thenPsyn(I1) > Psyn(I2) thenPsyn(I2) > Psyn(I1) thenII > I2f2 > f l11>12I2>I1X1>~2I2>I1(24)of ambiguities will increase drastically when the length of an input, sentence150where /1 and I2 denote any two interpretations.
Ptex3() denotes the lexical likelihood value ofan interpretation calculated as the geometric mean of three-word probabilities, Pte,2() the lexicallikelihood value of an interpretation calculated as the geometric mean of two-word probabilities, andPsyn() the syntactic likelihood value of an interpretation.
The average number of interpretationsobtained in the analysis of a sentence was 2.4.Table 1: Disambiguation resultsMethod Accuracy(%)Lex3+Lex2+Syn 89.2Lex3+Lex2+PCFG 86.7Lex3 (Lex2) ?
Syn 87.1Table 2: Breakdown of \[Lex3+Lex2+SynJLex3Lex2SynTotalCorrect Incorrect112 594 1416 i 8222 27Total117108242490,9509085080.750 .70.65'0.0 .
~ :  :=0.5,5 ~0.5 2 ~ 4Figure 4: The top 5 accuraciesThe number i accuracy obtained was 89.2% (Table 1 represents his result as 'Lex3+Lex2+Syn'),where the number n accuracy is defined as the fraction of the test sentences whose preferred inter-pretation is successfully ranked in the first n candidates.
We feel that this result is very encouraging.Table 2 shows the breakdown of the result, in which 'Lex3' stands for the proportion determined byusing lexical likelihood Pzex3, 'Lex2' by using lexical likelihood Pl~x2, and 'Syn' by using syntacticlikelihood Psyn.
The accuracies of 'Lex3,' 'Lex2,' and 'Syn' were 95.7%, 87.0%, and 66.7%, respec-tively.
Furthermore, 'Lex3,' 'Lex2,' and 'Syn' formed 47.0%, 43.4%, and 9.6% of the disambiguationresults, respectively.We further examined the types of mistakes made by our method.
First, there were somemistakes by 'Syn.'
For example, inRain washes the fertilizers off the land, (25)151there are two interpretations.
The lexical likelihood values Pt?~3 of the two interpretations werecalculated as 0, and the lexical ikelihood values Pt~x2 of the two interpretations were calculated as0, as well.
The interpretations were ranked by the syntactic likelihood Psy~, and the interpretationof attaching the 'off' phrase to 'fertilizer' was mistakenly preferred.
We also found some mistakesby 'Lex2.'
For example, inThe parents reclaimed the child under the circumstances, (26)there are two interpretations.
The lexical likelihood values Pt~.~3 of the two interpretations werecalculated as 0.
The lexical likelihood value Pl~2 of the interpretation f attaching 'under' phraseto 'child' was higher than that of attaching it to 'reclaimed,' as there were many expressions like 'achild under five' observed in the training data.
And thus the former interpretation was mistakenlypreferred.
It is obvious that these kinds of mistakes could be avoided if more data were available.We conclude that the most effective way of improving disambiguation results is to increase datafor training lexical preference.We further checked the disambiguation decisions made by 'Syn' when 'Lex3' and 'Lex2' fail towork, and found that all of the prepositional phrases in these sentences were attached to nearbyphrases by 'Syn,' indicating that using syntactic likelihood can help to achieve a functioning ofRAP.
One may argue that we could obtain the same number 1 accuracy if we were to employa deterministic approach in implementing RAP.
As we pointed out earlier, however, if we are toobtain the N most preferred interpretations, we need to use syntactic likelihood.
To verify thatthe syntactic likelihood is indeed useful, we conducted the following additional experiment.
Weranked the interpretations of each of the 249 test sentences using only syntactic likelihood.
We alsoselected the interpretation with phrases always attached to nearby phrases as the most preferredones, and randomly selected interpretations from what remain as the nth most preferred ones.
Weevaluated the results on the basis of the number n accuracy.
Figure 4 shows the top 5 accuraciesof the stochastic approach and the deterministic approach.
The results indicate that the formeroutperforms the latter.
(The number 2 accuracy for both methods increases drastically, as manytest sentences have only two interpretations.}
The improvement is not significant, however.
Weexpect the effect of the use of the syntactic likelihood to become more significant when longersentences are used in future analyses.In place of a length probability model, we used PCFG for calculating syntactic preference.
Weemployed the Maximum Likelihood Estimator to estimate the parameters of PCFG (we did notuse the so-called 'inside-outside algorithm' (Jelinek et al, 1990; Lari and Young, 1990)), makinguse of the same training data as those used for the length probability model.
Table 1 representsthis result as 'Lex3+Lex2+PCFG.'
Our experimental results indicate that our method of using alength probability model outperforms that of using PCFG.Instead of the back-off method, we used the product of lexical likelihood values and syntacticlikelihood values to rank interpretations.
When using lexical likelihood, we use a lexical likeli-hood value calculated fl'om three-word probabilities, provided that it is not 0, otherwise we usea lexical ikelihood value calculated from two-word probabilities.
Table 1 represents his result as'Lex3(Lex2)x Syn.'
When the preference values of all of the interpretations obtained are calculatedas 0, we rank the interpretations at random.
Our results indicate that it is preferable to employthe back-off method.7 Concluding RemarksWe have proposed a probabilistic method of disambiguation based on psycholinguistic principles.Our main proposals are: (a) to unify the psycholinguistic approach and the probabilistic approach,152specifically, to implement psycholinguistic principles on the basis of probabilistic methodology.
(b)to use the notion of 'length' in defining a probabilistic model for the implementation f RAP andALPP, and (c) to employ the back-off method to combine the use of lexical likelihood with that ofsyntactic likelihood.
Our experimental results indicate that our method is quite effective.AcknowledgementI thank greatly Mr. K. Nakamura, Mr. T. Fujita, and Dr. K. Kobayashi of NEC for their constantencouragement.
I also thank greatly Dr. N. Abe of NEC, and Dr. Y. Den of ATR for their valuablecomments  and suggestions.ReferencesHiyan Alshawi and David Carter.
1995.
Training and scaling preference functions for disambigua-tion.
Computational Linguistics, 20(4):635-648.Gerry Altmann and Mark Steedman.
1988.
Interaction with context during human sentenceprocessing.
Cognition, 30:191-238.Ezra Black, Fred Jelinek, John Lafferty, and David M. Magerman.
1992.
Towards history-basedgrammars: Using richer models for probabilistic parsing.
Proceedings of the 30th ACL, pages31-37.Ted Briscoe and John Carroll.
1993.
Generalized probabilistic LR parsing of natural language(corpora) with unification-based grammars.
Computational Linguistics, 19(1):25-59.Jing-Shin Chang, Yih-Fen Luo, and Keh-Yih Su.
1992.
GPSM: A generalized probabilistic semanticmodel for ambiguity resolution.
Proceedings off the 30th ACL, pages 177-184.Kenth.
W. Church and Ramesh Patil.
1982.
Coping with syntactic amgiguity or how to put theblock in the box on the table.
American Journal of Computational Linguistics, 8(3-4):139-149.Michael Collins and James Brooks.
1995.
Prepositional phrase attachment through a backed-offmodel.
Proceedings of the 3rd Workshop on Very Large Corpora.Marylyn Ford, Joan Bresnan, and Ronald Kaplan.
1982.
A competence based theory of syntacticclosure.
In J. Bresnan Ed.
The Mental Representation f Grammatical Relations.Lyn Frazier and Janet Fodor.
1979.
The sausage machine: A new two-stage parsing model.Cognition, 6:291-325.Fujisaki.
1989.
A probabilistic parsing method for sentence disambiguation.
Proceedings of Inter-national Workshop on Parsing Technology'89, pages 85-94.Donald Hindle and Mats Rooth.
1991.
Structural ambiguity and lexical relations.
Proceedings ofthe 29th A CL, pages 229-236.Donald Hindle and Mats Rooth.
1993.
Structural ambiguity and lexical relations.
ComputationalLinguistics, 19(1):103-120.Jerry R. Hobbs and John Bear.
1990.
Two principles of parse preference.
Proceedings of the 13thCOLING, pages 162-167.153Jelinek, Laggerty, and Mercer.
1990.
Basic methods of probabilistic ontext fl'ee grammars.
IBMResearch Report, RC 16374.P.
N. Johnson-Laird.
1983.
Mental Model: Towards a Cognitive Science of Language, Inference,and Consciousness.
Harvard Univ.
Press.J.
J. Katz and J.
A. Fodor.
1963.
The structure of semantic theory.
Language, 39:170-210.John Kimball.
1973.
Seven principles of surface structure parsing in natural anguage.
Cognition,2(1):15-47.Sadao Kurohashi and Makoto Nagao.
1994.
A syntactic analysis method of long japanese sentencesbased on the detection of conjunctive structures.
Computational Linguistics, 20(4):507-534.K.
Lari and S.J.
Young.
1990.
The estimation of stochastic ontext free grammars using theinside-outside algorithm.
Computer Speech and Language, 4:35-56.Hang Li and Naoki Abe.
1995.
Generalizing case frames using a thesaurus and the MDL principle.Proceedings of Recent Advances in Natural Language Processing, pages 239-248.David M. Magerman and Mitchell P. Marcus.
1991.
Pearl:A probabilistic hart parser.
Proceedingsof the International Workshop on Parsing Technology, pages 193-199.David M. Magerman.
199.5.
Statistical decision-tree models for parsing.
Proceedings of the 33thACL.Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
1993.
Building a largeannotated corpus of english: The penn treebank.
Computational Linguistics, 19(1):313-330.Yuji Matsumoto and Ryoichi Sugimura.
1986.
SAX: A parsing system based on logic programminglanguages.
Computer Software ('in Japanese), 13(4):4-11.C.
Pollard and I.
A.
Sag.
1987.
Information-based Syntax and Semantics.
Volume 1: Syntax.
CSLILecture Notes 13.
Chicago Univ.
Press.Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994.
A maximum entropy model for prepo-sitional phrase attachment.
Proceedings of ARPA Workshop on Human Language Technology,pages 250-255.Philip Resnik.
1993.
Semantic lasses and syntactic ambiguity.
Proceedings of ARPA Workshopon Human Language Technology.Stuart M. Shieber.
1983.
Sentence disambiguation by shift-reduce parsing technique.
Proceedingsof the 21st ACL, pages 113-118.Keh-Yih Su and Jing-Shin Chang.
1988.
Semantic and syntactic aspects of score function.
Pro-ceedings of the 12th COLING, pages 642-644.Stefan Wermter.
1989.
Integration of semantic and syntactic onstraints for structural noun phrasedisambiguation.
Proceedings of the IJCAI'89, pages 1486-1491.Greg Whittemore, Kathleen Ferrara, and Hans Brunner.
1990.
Empirical study of predictivepowers of simple attachment schemes for post-modifier prepositional phrases.
Proceedings ofthe 28th A CL, pages 23-30.Yorick Wilks, Xiuming Huang, and Dan Fass.
1985.
Syntax, preference and right attachment.Proceedings of the IJCAI'85, pages 779-784.154
