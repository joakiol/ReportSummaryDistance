What Is Not in the Bag of Words forWhy-QA?Suzan Verberne?Radboud University NijmegenLou Boves?
?Radboud University NijmegenNelleke Oostdijk?Radboud University NijmegenPeter-Arno Coppen?Radboud University NijmegenWhile developing an approach towhy-QA, we extended a passage retrieval system that uses off-the-shelf retrieval technology with a re-ranking step incorporating structural information.
Weget significantly higher scores in terms of MRR@150 (from 0.25 to 0.34) and success@10.
The23% improvement that we reach in terms of MRR is comparable to the improvement reached ondifferent QA tasks by other researchers in the field, although our re-ranking approach is basedon relatively lightweight overlap measures incorporating syntactic constituents, cue words, anddocument structure.1.
IntroductionAbout 5% of all questions asked to QA systems are why-questions (Hovy, Hermjakob,and Ravichandran 2002).
Why-questions need a different approach than factoid ques-tions, because their answers are explanations that usually cannot be stated in a singlephrase.
Recently, research (Verberne 2006; Higashinaka and Isozaki 2008) has beendirected at QA forwhy-questions (why-QA).
In earlier work on answeringwhy-questionson the basis of Wikipedia, we found that the answers to most why-questions are pas-sages of text that are at least one sentence and at most one paragraph in length (Verberneet al 2007b).
Therefore, we aim at developing a system that takes as input a why-question and gives as output a ranked list of candidate answer passages.In the current article, we propose a three-step setup for a why-QA system: (1) aquestion-processing module that transforms the input question to a query; (2) an off-the-shelf retrieval module that retrieves and ranks passages of text that share content?
Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands.E-mail: s.verberne@let.ru.nl.??
Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands.E-mail: l.boves@let.ru.nl.?
Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands.E-mail: n.oostdijk@let.ru.nl.?
Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands.E-mail: p.a.coppen@let.ru.nl.Submission received: 30 July 2008; revised submission received: 18 February 2009; accepted for publication:4 September 2009.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 2with the input query; and (3) a re-ranking module that adapts the scores of the re-trieved passages using structural information from the input question and the retrievedpassages.In the first part of this article, we focus on step 2, namely, passage retrieval.
Theclassic approach to finding passages in a text collection that share content with aninput query is retrieval using a bag-of-words (BOW) model (Salton and Buckley 1988).BOWmodels are based on the assumption that text can be represented as an unorderedcollection of words, disregarding grammatical structure.
Most BOW-based models usestatistical weights based on term frequency, document frequency, passage length, andterm density (Tellex et al 2003).Because BOW approaches disregard grammatical structure, systems that rely ona BOW model have their limitations in solving problems where the syntactic relationbetweenwords or word groups is crucial.
The importance of syntax for QA is sometimesillustrated by the sentence Ruby killed Oswald, which is not an answer to the questionWho did Oswald kill?
(Bilotti et al 2007).
Therefore, a number of researchers in the fieldinvestigated the use of structural information on top of a BOW approach for answerretrieval and ranking (Tiedemann 2005; Quarteroni et al 2007; Surdeanu, Ciaramita, andZaragoza 2008).
These studies show that although the BOW model makes the largestcontribution to the QA system results, adding structural (syntactic information) can givea significant improvement.In the current article, we hypothesize that for the relatively complex problem ofwhy-QA, a significant improvement?at least comparable to the improvement gainedfor factoidQA?can be gained from the addition of structural information to the rankingcomponent of the QA system.
We first evaluate a passage retrieval system for why-QAbased on standard BOW ranking (step 1 and 2 in our set-up).
Then we perform ananalysis of the strengths and weaknesses of the BOW model for retrieving and rankingcandidate answers.
In view of the observed weaknesses of the BOW model, we chooseour feature set to be applied to the set of candidate answer passages in the re-rankingmodule (step 3 in our set-up).The structural features that we propose are based on the idea that some parts of thequestion and the answer passage are more important for relevance ranking than otherparts.
Therefore, our re-ranking features are overlap-based: They tell us which parts ofa why-question and its candidate answers are the most salient for ranking the answers.We evaluate our initial and adapted ranking strategies using a set of why-questions anda corpus of Wikipedia documents, and we analyze the contribution of both the BOWmodel and the structural features.The main contributions of this article are: (1) we address the relatively new problemof why-QA and (2) we analyze the contribution of overlap-based structural informationto the problem of answer ranking.The paper is organized as follows.
In Section 2, related work is discussed.
Section 3presents the BOW-based passage retrieval method forwhy-QA, followed by a discussionof the strengths andweaknesses of the approach in Section 4.
In Section 5, we extend oursystem with a re-ranking component based on structural overlap features.
A discussionof the results and our conclusions are presented in Sections 6 and 7, respectively.2.
Related WorkWedistinguish relatedwork in two directions: research into the development of systemsfor why-QA (Section 2.1), and research into combining structural and BOW features forQA (Section 2.2).230Verberne et al What Is Not in the Bag of Words forWhy-QA?2.1 Research intoWhy-QAIn related work (Verberne et al 2007a), we focused on selecting and ranking explanatorypassages for why-QA with the use of rhetorical structures.
We developed a system thatemploys the discourse relations in a manually annotated document collection: the RSTTreebank (Carlson, Marcu, and Okurowski 2003).
This system matches the input ques-tion to a text span in the discourse tree of the document and it retrieves as answer thetext span that has a specific discourse relation to this question span.
We evaluated ourmethod on a set of 336 why-questions formulated to seven texts from the WSJ corpus.We concluded that discourse structure can play an important role in why-QA, but thatsystems relying on these structures can only work if candidate answer passages havebeen annotated with discourse structure.
Automatic parsers for creating full rhetoricalstructures are currently unavailable.
Therefore, a more practical approach appears tobe necessary for work in why-QA, namely, one which is based on automatically createdannotations.Higashinaka and Isozaki (2008) focus on the problem of ranking candidate answerparagraphs for Japanese why-questions.
They assume that a document retrieval modulehas returned the top 20 documents for a given question.
They extract features for contentsimilarity, causal expressions, and causal relations from two annotated corpora and adictionary.
Higashinaka and Isozaki evaluate their ranking method using a set of 1,000why-questions that were formulated to a newspaper corpus by a text analysis expert.70.3% of the reference answers for these questions are ranked in the top 10 by theirsystem, and MRR1 was 0.328.Although the approach of Higashinaka and Isozaki is very interesting, their eval-uation collection has the same flaw as the one used by Verberne et al (2007a): Bothcollections consist of questions formulated to a pre-selected answer text.
Questionselicited in response to newspaper texts tend to be unrepresentative of questions askedin a real QA setting.
In the current work, therefore, we work with a set of questionsformulated by users of an online QA system (see Section 3.1).2.2 Combining Structural and Bag-of-Words Features for QATiedemann (2005) investigates syntactic information from dependency structures inpassage retrieval for Dutch factoid QA.
He indexes his corpus at different text layers(BOW, part-of-speech, dependency relations) and uses the same layers for questionanalysis and query creation.
He optimizes the query parameters for the passage retrievaltask by having a genetic algorithm apply the weights to the query terms.
Tiedemannfinds that the largest weights are assigned to the keywords from the BOW layer andto the keywords related to the predicted answer type (such as ?person?).
The baselineapproach, using only the BOW layer, gives an MRR of 0.342.
Using the optimized IRsettings with additional layers, MRR improves to 0.406.Quarteroni et al (2007) consider the problem of answering definition questions.They use predicate?argument structures (PAS) for improved answer ranking.
They findthat PAS as a stand-alone representation is inferior to parse tree representations, butthat together with the BOW it yields higher accuracy.
Their results show a significant1 The reciprocal rank (RR) for a question is 1 divided by the rank ordinal of the highest ranked relevantanswer.
The Mean RR is obtained by averaging RR over all questions.231Computational Linguistics Volume 36, Number 2improvement of PAS?BOW compared to parse trees (F-scores 70.7% vs. 59.6%) but PASmakes only a very small contribution compared to BOW only (which gives an F-scoreof 69.3%).Recent work by Surdeanu, Ciaramita, and Zaragoza (2008) addresses the problemof answer ranking for how-to-questions.
From Yahoo!
Answers,2 they extract a corpusof 140,000 answers with 40,000 questions.
They investigate the usefulness of a largeset of question and answer features in the ranking task.
They conclude that the linguisticfeatures ?yield a small, yet statistically significant performance increase on top of thetraditional BOW and n-gram representation (page 726).
?All these authors conclude that the addition of structural information in QAgives a small but significant improvement compared to using a BOW-model only.
Forwhy-questions, we also expect to gain improvement from the addition of structuralinformation.3.
Passage Retrieval forWhy-QA Using a BOWModelAs explained in Section 1, our system comprises three modules: question2query, passageretrieval, and re-ranking.
In the current section, we present the first two system mod-ules, and the re-ranking module, including a description of the structural features thatwe consider, is presented in Section 5.
First, however, we describe our data collectionand evaluation method.3.1 Data and Evaluation Set-upFor our experiments, we use the Wikipedia INEX corpus (Denoyer and Gallinari 2006).This corpus consists of all 659,388 articles from the online Wikipedia in the summer of2006 in XML format.For development and testing purposes, we exploit the Webclopedia questionset (Hovy, Hermjakob, and Ravichandran 2002), which contains questions asked tothe online QA system answers.com.
Of these questions, 805 (5% of the total set) arewhy-questions.
For 700 randomly selected why-questions, we manually searched for ananswer in the Wikipedia XML corpus, saving the remaining 105 questions for futuretesting purposes.
186 of these 700 questions have an answer in the corpus.3 Extractionof one relevant answer for each of these questions resulted in a set of 186 why-questionsand their reference answers.4 Two examples illustrate the type of data we are workingwith:1.
?Why didn?t Socrates leave Athens after he was convicted??
?
?Socratesconsidered it hypocrisy to escape the prison: he had knowingly agreed tolive under the city?s laws, and this meant the possibility of being judgedguilty of crimes by a large jury.
?2 See http://answers.yahoo.com/.3 Thus, about 25% of our questions have an answer in the Wikipedia corpus.
The other questions are eithertoo specific (Why do ceiling fans turn counter-clockwise but table fans turn clockwise?)
or too trivial (Why dohotdogs come in packages of 10 and hotdog buns in packages of 8?)
for the coverage of Wikipedia in 2006.4 Just like factoid questions, most why-questions generally have one correct answer that can be formulatedin different ways.232Verberne et al What Is Not in the Bag of Words forWhy-QA?2.
?Why do most cereals crackle when you add milk??
?
?They are made ofa sugary rice mixture which is shaped into the form of rice kernels andtoasted.
These kernels bubble and rise in a manner which forms very thinwalls.
When the cereal is exposed to milk or juices, these walls tend tocollapse suddenly, creating the famous ?Snap, crackle and pop?
sounds.
?To be able to do fast evaluation without elaborate manual assessments, we manuallycreated one answer pattern for each of the questions in our set.
The answer pattern is aregular expression that defines which of the retrieved passages are considered a relevantanswer to the input question.
The first version of the answer patterns was directlybased on the corresponding reference answer, but in the course of the developmentand evaluation process, we extended the patterns in order to cover as many as possibleof the Wikipedia passages that contain an answer.
For example, for question 1, wedeveloped the following answer pattern based on two variants of the correct answerthat occur in the corpus: /(Socrates.
* opportunity.
* escape.
* Athens.
* considered.
*hypocrisy | leave.
* run.
* away.
* community.
* reputation)/.5In fact, answer judgment is a complex task due to the presence of multiple answervariants in the corpus.
It is a time-consuming process because of the large number ofcandidate answers that need to be judged when long lists of answers are retrieved perquestion.
In future work, we will come back to the assessment of relevant and irrelevantanswers.After applying our answer patterns to the passages retrieved, we count the ques-tions that have at least one relevant answer in the top n results.
This number divided bythe total number of questions in a test set gives the measure success@n. In Section 3.2,we explain the levels for n that we use for evaluation.
For the highest ranked relevantanswer per question, we determine the RR.
Questions for which the system did notretrieve an answer in the list of 150 results get an RR of 0.
Over all questions, we calculatethe mean reciprocal rank MRR.3.2 Method and ResultsIn the question2query module of our system we convert the input question to a queryby removing stop words6 and punctuation, and simply list the remaining content wordsas query terms.The second module of our system performs passage retrieval using off-the-shelfretrieval technology.
In Khalid and Verberne (2008), we compared a number of settingsfor our passage retrieval task.
We considered two different retrieval engines (Lemur7and Wumpus8), four different ranking models, and two types of passage segmentation:disjoint and sliding passages.
In each setting, 150 results were obtained by the retrievalengine and ranked by the retrieval model.
We evaluated all retrieval settings in terms of5 Note that the vertical bar separates the two alternatives.6 To this end we use the stop word list that can be found at http://marlodge.supanet.com/museum/funcword.html.We use all items except the numbers and the word why.7 Lemur is an open source toolkit for information retrieval that provides flexible support for different typesof retrieval models.
See http://www.lemurproject.org.8 Wumpus is an information retrieval system mainly geared at XML retrieval.
See http://www.wumpus-search.org/.233Computational Linguistics Volume 36, Number 2MRR@n9 and success@n for levels n = 10 and n = 150.
For the evaluation of the retrievalmodule, we were mainly interested in the scores for success@150 because re-rankingcan only be successful if at least one relevant answer was returned by the retrievalmodule.We found that the best-scoring passage retrieval setting in terms of success@150 isLemur on an index of sliding passages with TF-IDF (Zhai 2001) as ranking model.
Weobtained the following results with this passage retrieval setting: success@150 is 78.5%,success@10 is 45.2%, and MRR@150 is 0.25.
We do not include the results obtained withthe other retrieval settings here because the differences were small.The results show that for 21.5% of the questions in our set, no answer was retrievedin the top-150 results.
We attempted to increase this coverage by retrieving 250 or500 answers per question but this barely increased the success score at maximumn.
The main problems for the questions that we miss are infamous retrieval prob-lems such as the vocabulary gap between a question and its answer.
For example,the answer to Why do chefs wear funny hats?
contains none of the words from thequestion.4.
The Strengths and Weaknesses of the BOWModelIn order to understand how answer ranking is executed by the passage retrieval mod-ule, we first take a closer look at the TF-IDF algorithm as it has been implemented inLemur.
TF-IDF is a pure BOW model: Both the query and the passages in the corpusare represented by the term frequencies (numbers of occurrences) for each of the wordsthey contain.
The terms are weighted using their inverse document frequency (IDF),which puts a higher weight on terms that occur in few passages than on terms thatoccur in many passages.
The term frequency (TF) functions for the query and the doc-ument, and the parameter values chosen for these functions in Lemur can be found inZhai (2001).As explained in the previous section, we consider success@150 to be the mostimportant measure for the retrieval module of our system.
However, for the system as awhole, success@10 is a more important evaluation measure.
This is because users tendto pay much more attention to the top 10 results of a retrieval system than to results thatare ranked lower (Joachims et al 2005).
Therefore, it is interesting to investigate whichquestions are answered in the top 150 and not in the top 10 by our passage retrievalmodule.
This is the set of questions for which the BOW model is not effective enoughand additional (more specific) overlap information is needed for ranking a relevantanswer in the top 10.We analyzed the set of questions that get a relevant answer at a rank between 10 and150 (62 questions), which belowwewill refer to as our focus set.
We compared our focusset to the questions for which a relevant answer is in the top 10 (84 questions).
Althoughthese numbers are too small to do a quantitative error analysis, a qualitative analysisprovides valuable insights into the strengths and weaknesses of a BOW representationsuch as TF-IDF.
In Sections 4.1 to 4.4 we discuss four different aspects of why-questionsthat present problems for the BOWmodel.9 Note that MRR is often used without the explicit cut-off point (n).
We add it to clarify that RR is 0 for thequestions without a correct answer in the top-n.234Verberne et al What Is Not in the Bag of Words forWhy-QA?4.1 Short QuestionsTen questions in our focus set contain only one or two content words.
We can see theeffect of short queries if we compare three questions that contain only one semanticallyrich content word.10 The rank of the highest ranked relevant answer is given betweenparentheses; the last of these three questions is in our focus set.1.
Why do people hiccup?
(2)2.
Why do people sneeze?
(4)3.
Why do we dream?
(76)We found that the rank of the relevant answer is related to the corpus frequency ofthe single semantically rich word, which is 64 for hiccup, 220 for sneeze, and 13,458 fordream.
This means that many passages are retrieved for question 3, making the chancesfor the relevant answer to be ranked in the top 10 smaller.
One way to overcome theproblem of long result lists for short queries is by adding words to the query that makeit more specific.
In the case of why-QA, we know that we are not simply searchingfor information on dreaming but for an explanation for dreaming.
Thus, in the rankingprocess, we can extend the query with explanatory cue words such as because.11 Weexpect that the addition of explanatory cue phrases will give an improvement in rankingperformance.4.2 The Document Context of the AnswerThere are many cases where the context of the candidate answer gives useful infor-mation.
Consider, for example, the question Why does a snake flick out its tongue?, thecorrect answer to which was ranked 29.
A human searcher expects to find the answerin a Wikipedia article about snakes.
Within the Snake article he or she may search forthe words flick and/or tongue in order to find the answer.
This suggests that in somecases there is a direct relation between a specific part of the question and the context(document and/or section) of the candidate answer.
In cases like this, the answerdocument and the question apparently share the same topic (snake).
By analogy withlinguistically motivated approaches to factoid QA (Ferret et al 2002), we introduce theterm question focus for this topic.In the example question flick is the word with the lowest corpus frequency (556),followed by tongue (4,925) and snake (6,809).
Using a BOW approach to document titlematching, candidate answers from documents with flick or tongue in their title wouldbe ranked higher than answers from documents with snake in their title.
Thus, forquestions for which there is overlap between the question focus and the title of theanswer documents (two thirds of the questions in our set), we can improve the rankingof candidate answers by correctly predicting the question focus.
In Section 5.1.2, wemake concrete suggestions for achieving this.10 The word people in subject position is a semantically poor content word.11 The addition of cue words can also be considered to be applied in the retrieval step.
We come back to thisin Section 6.3.235Computational Linguistics Volume 36, Number 24.3 Multi-Word TermsA very important characteristic of the BOWmodel is that words are considered separateterms.
One of the consequences is that multi-word terms such as multi-word nounphrases (mwNPs) are not treated as a single term.
Here, three examples of questionsare shown in which the subject is realized by a mwNP (underlined in the examples; therank of the relevant answer is shown between brackets):1.
Why are hush puppies called hush puppies?
(1)2.
Why is the coral reef disappearing?
(29)3.
Why is a black hole black?
(31)We investigated the corpus frequencies for the separate parts of each mwNP.
We foundthat these are quite high for coral (3,316) and reef (2,597) compared to the corpusfrequency of the phrase coral reef (365).
The numbers are even more extreme for black(103,550) and hole (9,734) versus black hole (1,913).
On the other hand, the answer tothe hush puppies question can more easily be ranked because the corpus frequenciesfor the separate terms hush (594) and puppies (361) are relatively low.
This shows thatmulti-word terms do not necessarily give problems for the BOW model as long as thedocument frequencies for the constituent words are relatively low.
If (one of) the wordsin the phrase is/are frequent, it is very difficult to rank the relevant answer high in theresult list with use of word overlap only.In our focus set, 36 of the 62 questions contain a mwNP.
For these questions, we canexpect improved ranking from the addition of NPs to our feature set.4.4 Syntactic StructureThe BOW model does not take into account sentence structure.
The potential impor-tance of sentence structure for improved ranking can be exemplified by the followingtwo questions from our set.
Note that both examples contain a subordinate clause (finiteor non-finite):1.
Why do baking soda and vinegar explode when you mix them together?
(4)2.
Why are there 72 points to the inch when discussing fonts and printing?
(36)In both cases, the contents of the subordinate clause are less important to the goal of thequestion than the contents of themain clause.
In the first example, this is (coincidentally)reflected by the corpus frequencies of the words in both clauses: mix (12,724) andtogether (83,677) have high corpus frequencies compared to baking (832), soda (1,620),vinegar (871), and explode (1,285).
As a result, the reference answer containing theseterms is ranked in the top-10 by TF-IDF.
In the second example, however, the corpusfrequencies do not reflect the importance of the terms.
Fonts and printing have lowercorpus frequencies (1,243 and 6,978, respectively) than points (43,280) and inch (10,046).Thus, fonts and printing are weighted heavier by TF-IDF although these terms are onlyperipheral to the goal of the query, the core of which isWhy are there 72 points to the inch?This cannot be derived from the corpus frequencies, but can only be inferred from thesyntactic function (adverbial) of when discussing fonts and printing in the question.Thus, the lack of information about sentence structure in the BOW model doesnot necessarily give rise to problems as long as the importance of the question termsis reflected by their frequency counts.
If term importance does not align with corpus236Verberne et al What Is Not in the Bag of Words forWhy-QA?frequency, grammatical structure becomes potentially useful.
Therefore, we expect thatsyntactic structure can make a contribution to cases where the importance of the termsis not reflected by their corpus frequencies but can be derived from their syntacticfunction.4.5 What Can We Expect from Structural Information?In Sections 4.1 to 4.4 we discussed four aspects of why-questions that are problematicfor the BOW model.
We expect contributions from the inclusion of information on cuephrases, question focus and the document context of the answer, noun phrases, andthe syntactic structure of the question.
We think that it is possible to achieve improvedranking performance if features based on structural overlap are taken into accountinstead of global overlap information.5.
Adding Overlap-Based Structural InformationFrom our analyses in Section 4, we found a number of question and answer aspectsthat are potentially useful for improving the ranking performance of our system.
Inthis section, we present the re-ranking module of our system.
We define a feature setthat is inspired by the findings from Section 4 and aims to find out which structuralfeatures of a question?answer pair contribute the most to better answer ranking.
Weaim to weigh these features in such a way that we can optimize ranking performance.The input data for our re-ranking experiments is the output of the passage retrievalmodule.
A success@150 score of 78.5% for passage retrieval (see Section 3.2) means thatthe maximum success@10 score that we can achieve by re-ranking is 78.5%.5.1 Features for Re-rankingThe first feature in our re-ranking method is the score that was assigned to a candidateanswer by Lemur/TF-IDF in the retrieval module (f0).
In the following sections weintroduce the other features that we implemented.
Each feature represents the overlapbetween two item bags:12 a bag of question items (for example: all the question?s nounphrases, or the question?s main verb) and a bag of answer items (for example: all answerwords, or all verbs in the answer).
The value that is assigned to a feature is a function ofthe overlap between these two bags.
We used the following overlap function:S(Q,A) =QA + AQQ+ A(1)in whichQA is the number of question items that occur at least once in the bag of answeritems, AQ is the number of answer items that occur at least once in the bag of questionitems, and Q+ A is the number of items in both bags of items joined together.5.1.1 The Syntactic Structure of the Question.
In Section 4.4, we argued that some syntacticparts of the question may be more important for answer ranking than others.
Becausewe have no quantitative evidence yet which syntactic parts of the question are the mostimportant, we created overlap features for each of the following question parts: phrase12 Note that a ?bag?
is a set in which duplicates are counted as distinct items.237Computational Linguistics Volume 36, Number 2heads (f1), phrase modifiers (f2); the subject (f3), main verb (f4), nominal predicate (f5),and direct object (f6) of the main clause; and all noun phrases (f11).
For each of thesequestion parts, we calculated its word overlap with the bag of all answer words.
For thefeatures f3?f6, we added a variant where as answer items only words/phrases with thesame syntactic function as the question token were included (f7, f8, f9, f10).Consider for example question 1 from Section 3.1: Why didn?t Socrates leave Athensafter he was convicted?, and the reference answer as the candidate answer for which weare determining the feature values: Socrates considered it hypocrisy to escape the prison: hehad knowingly agreed to live under the city?s laws, and this meant the possibility of being judgedguilty of crimes by a large jury.From the parser output, our feature extraction script extracts Socrates as subject,leave as main verb, and Athens as direct object.
Neither leave nor Athens occur in theanswer passage, thus f4, f6, f8, and f10 are all given a value of 0.
So are f5 and f9,because the question has no nominal predicate.
For the subject Socrates, our script findsthat it occurs once in the bag of answer words.
The overlap count for the feature f3 isthus calculated as 1+11+18 = 0.105.13 For the feature f7, our script extracts the grammaticalsubjects Socrates, he, and this from the parser?s representation of the answer passage.Because the bag of answer subjects for f7 contains three items, the overlap is calculatedas 1+11+3 = 0.5.5.1.2 The Semantic Structure of the Question.
In Section 4.2, we saw that often there is alink between the question focus and the title of the document in which the referenceanswer is found.
In those cases, the answer document and the question share the sametopic.
For most questions, the focus is the syntactic subject: Why do cats sleep so much?Judging from our data, there are two exceptions to this general rule: (1) If the subjectis semantically poor, the question focus is the (verbal or nominal) predicate: Why dopeople sneeze?, and (2) in case of etymology questions (which cover about 10% ofwhy-questions), the focus is the subject complement of the passive sentence: Why arechicken wings called Buffalo Wings?We included a feature (f12) for matching words from the question focus to wordsfrom the document title and a feature (f13) for the relation between question focuswordsand all answer words.
We also include a feature (f14) for the other, non-focus questionwords.5.1.3 The Document Context of the Answer.
Not only is the document title in relation tothe question focus potentially useful for answer ranking, but also other aspects of theanswer context.
We include four answer context features in our feature set: overlapbetween the question words and the title of the Wikipedia document (f15), overlap be-tween question words and the heading of the answer section (f16), the relative positionof the answer passage in the document (f17), and overlap between a fixed set of wordsthat we selected as explanatory cues when they occur in a section heading and the setof words that occur in the section heading of the passage (f18).1413 The bag of question subjects contains one item (Socrates, the 1 in the denominator) and one item fromthis bag occurs in the bag of answer words (the left 1 in the numerator).
Without stopwords, the bag ofall answer words contains 18 items, one of which occurs in the bag of question subjects (the right 1 inthe numerator).14 We found these section heading cues by extracting all section headings from the Wikipedia corpus,sorting them by frequency, and then manually marking those section heading words that we expectto occur with explanatory sections.
The result is a small set of heading cues (history, origin, origins,background, etymology, name, source, sources) that is independent of the test set we work with.238Verberne et al What Is Not in the Bag of Words forWhy-QA?5.1.4 Synonyms.
For each of the features f1 to f10 and f12 to f16 we add an alternativefeature (f19 to f34) covering the set of all WordNet synonyms for all question terms inthe original feature.
For synonyms, we apply a variant of Equation (1) in which QA isinterpreted as the number of question items that have at least one synonym in the bagof answer items and AQ as the number of answer items that occur in at least one of thesynonym sets of the question items.5.1.5 WordNet Relatedness.
Additionally, we included a feature representing the related-ness between the question and the candidate answer using the WordNet Relatednesstool (Pedersen, Patwardhan, and Michelizzi 2004) (f35).
As a measure of relatedness,we choose the Lesk measure, which incorporates information fromWordNet glosses.5.1.6 Cue Phrases.
Finally, as proposed in Section 4.1, we added a closed set of cue phrasesthat are used to introduce an explanation (f36).
We found these explanatory phrases in away that is commonly used for finding answer cues and that is independent of our ownset of question?answer pairs.
We queried the key answer words to the most frequentwhy-question on the Web Why is the sky blue?
(blue sky rayleigh scattering) to the MSNSearch engine15 and crawled the first 250 answer fragments that are retrieved by theengine.
From these, we manually extracted all phrases that introduce the explanation.This led to a set of 47 cue phrases such as because, as a result of, which explains why,and so on.5.2 Extracting Feature Values from the DataFor the majority of features we needed the syntactic structure of the input question,and for some of the features also of the answer.
We experimented with two differentsyntactic parsers for these tasks: the Charniak parser (Charniak 2000) and a develop-ment version of the Pelican parser.16 Of these, Pelican has a more detailed descriptivemodel and gives better accuracy but Charniak is at present more robust for parsinglong sentences and large amounts of text.
We parsed the questions with Pelican becausewe need accurate parsings in order to correctly extract all constituents.
We parsed allanswers (186 ?
150 passages) with Charniak because of its speed and robustness.For feature extraction, we used the following external components: A stop wordlist,17 the sets of cue phrases as described in Sections 5.1.3 and 5.1.6, the CELEX Lemmalexicon (Burnage et al 1990), the WordNet synonym sets, the WordNet Similaritytool (Pedersen, Patwardhan, and Michelizzi 2004), and a list of pronouns and semanti-cally poor nouns.18 We used a Perl script for extracting feature values for each question?answer pair.
For each feature, the script composes the required bags of question itemsand answer items.
All words are lowercased and punctuation is removed.
For termsin the question set that consist of multiple words (for example, a multi-word subject),spaces are replaced by underscores before stop words are removed from the questionand the answer.
Then the script calculates the similarity between the two sets for eachfeature following Equation (1).1915 http://www.live.com.16 See http://lands.let.ru.nl/projects/pelican/.17 See Section 3.1.18 Semantically poor nouns that we came across in our data set are the nouns humans and people.19 A multi-word term from the question is counted as one item.239Computational Linguistics Volume 36, Number 2Table 1Results for the why-QA system: the complete system including re-ranking compared againstplain Lemur/TF-IDF for 187 why-questions.Success@10 Success@150 MRR@150Lemur/TF-IDF?sliding 45.2% 78.5% 0.25TF-IDF + Re-ranking using 37 structural features 57.0% 78.5% 0.34Whether or not to lemmatize the terms before matching them is open to debate.In the literature, there is some discussion on the benefit of lemmatization for questionanswering (Bilotti, Katz, and Lin 2004).
Lemmatization can be especially problematicin the case of proper names (which are not always recognizable by capitalization).Therefore, we decided only to lemmatize verbs (for features f4 and f8) in the currentversion of our system.5.3 Re-ranking MethodFeature extraction led to a vector consisting of 37 feature values for each of the 27,900items in the data set.
We normalized the feature values over all 150 answer candidatesfor the same question to a number between 0 and 1 using the L1 vector norm.
Eachinstance (representing one question?answer pair) was automatically labeled 1 if thecandidate answer matched the answer pattern for the question and 0 if it did not.On average, a why-question had 1.6 correct answers among the set of 150 candidateanswers.In the process of training our re-ranking module, we aim at combining the 37features in a ranking function that is used for re-ordering the set of candidate answers.The task of finding the optimal ranking function for ranking a set of items is referred toas ?learning to rank?
in the information retrieval literature (Liu et al 2007).
In Verberneet al (2009), we compared several machine learning techniques20 for our learning-to-rank problem.
We evaluated the results using 5-fold cross validation on the ques-tion set.5.4 Results from Re-rankingThe results for the complete system compared with passage retrieval with Lemur/TF-IDF only are in Table 1.We show the results in terms of success@10, success@150, andMRR@150.
We only present the results obtained using the best-performing learning-to-rank technique: logistic regression.21 A more detailed description of our machinelearning method and a discussion of the results obtained with other learning techniquescan be found in Verberne et al (2009).20 Naive Bayes, Support Vector Classification, Support Vector Regression, Logistic regression, RankingSVM, and a genetic algorithm, all with several optimization functions.21 We used the lrm function from the Design package in R (http://cran.r-project.org/web/packages/Design) for training and evaluating models based on logistic regression.240Verberne et al What Is Not in the Bag of Words forWhy-QA?After applying our re-ranking module, we found a significant improvement overbare TF-IDF in terms of success@10 and MRR@150 (z = ?4.29, p < 0.0001 using theWilcoxon Signed-Rank test for paired reciprocal ranks).5.5 Which Features Made the Improvement?In order to evaluate the importance of our features, we rank them according to thecoefficient that was assigned to them in the logistic regression model (See Table 2).
Weonly consider features that are significant at the p = 0.05 level.
We find that all eightsignificant features are among the top nine features with the highest coefficient.The feature ranking is discussed in Section 6.1.6.
DiscussionIn the following sections, we discuss the feature ranking (Section 6.1), make a compari-son to other re-ranking approaches (Section 6.2), and explain the attempts that we madeat solving the remaining problems (Section 6.3).6.1 Discussion of the Feature RankingTable 2 shows that only a small subset (8) of our 37 features significantly contribute tothe re-ranking score.
The highest ranked feature is TF-IDF (the bag of words), which isnot surprising since TF-IDF alone already reaches an MRR@150 of 0.25 (see Section 3.2).In Section 4.5, we predicted a valuable contribution from the addition of cue phrases,question focus, noun phrases, and the document context of the answer.
This is partlyconfirmed by Table 2, which shows that among the significant features are the featurethat links question focus to document title and the cue phrases feature.
The nounphrases feature (f11) is actually in the top nine features with the highest coefficient butits contribution was not significant at the 0.05 level (p = 0.068).The importance of question focus for why-QA is especially interesting because itis a question feature that is specific to why-questions and does not similarly applyTable 2Features that significantly contribute to the re-ranking score (p < 0.05), ranked by theircoefficient in the logistic regression model (representing their importance).Feature CoefficientTF-IDF (f0) 0.39**Overlap between question focus synonyms and document title (f30) 0.25**Overlap between question object synonyms and answer words (f28) 0.22Overlap between question object and answer objects (f10) 0.18*Overlap between question words and document title synonyms (f33) 0.17Overlap between question verb synonyms and answer words (f24) 0.16WordNet Relatedness (f35) 0.16*Cue phrases (f36) 0.15*Asterisks on coefficients denote the level of significance for the feature: ** p < 0.001; * 0.001 <p < 0.01; no asterisk means 0.01 < p < 0.05.241Computational Linguistics Volume 36, Number 2to factoids or other question types.
Moreover, the link from the question focus to thedocument title shows that Wikipedia as an answer source can provide QA systems withmore information than a collection of plain texts with less discriminative documenttitles does.The significance of cue phrases is also an important finding.
In fact, including cuephrases in the why-QA process is the only feasible way of specifying which passagesare likely to contain an explanation (i.e., an answer to a why-question).
In earlier work(Verberne et al 2007a), we pointed out that higher-level annotation such as discoursestructure can give useful information in the why-answer selection process.
However,the development of systems that incorporate discourse structure suffers from the lackof tools for automated annotation.
The current results show that surface patterns (theliteral presence of items from a fixed set of cue words) are a step in the direction ofanswer selection.The significant features in Table 2 also show us which question constituents arethe most salient for answer ranking: focus, main verb, and direct object.
We think thatfeatures incorporating the question?s subject are not found to be significant because, ina subset of the questions, the subject is semantically poor.
Moreover, because for mostquestions the subject is the question focus, the subject features and the focus features arecorrelated.
In our data, the question focus apparently is the more powerful predictor.6.2 Comparison to Other ApproachesThe 23% improvement that we reach in terms of MRR@150 (from 0.25 to 0.34) is com-parable to that reached by Tiedemann in his work on improving factoid QA with theuse of structural information.In order to see whether the improvement that we achieved with re-ranking ison account of structural information or just the benefit of using word sequences, weexperimented with a set of re-ranking features based on sequences of question wordsthat are not syntactically defined.
In this re-ranking experiment, we included TF-IDF,word bigrams, and word trigrams as features.
The resulting performance was aroundbaseline level (MRR = 0.25), significantly worse than re-ranking with structural overlapfeatures.
This is still true if we add the cue word feature (which, in isolation, only givesa small improvement to baseline performance) to the n-gram features.6.3 Solving the Remaining ProblemsAlthough the results in terms of success@10 and MRR@150 are satisfactory, there is stilla substantial proportion of why-questions that is not answered in the top 10 result list.In this section, we discuss a number of attempts that we made to further improve oursystem.First, after we found that for some question parts synonym expansion leads toimprovement (especially the main verb and direct object), we experimented with theaddition of synonyms for these constituents in the retrieval step of our system (Lemur).We found, however, that it does not improve the results due to the large synonym setsof many verbs and nouns which add much noise and lead to very long queries.
Thesame holds for the addition of cue words in the retrieval step.Second, although our re-ranking module incorporates expansion to synonym sets,there are many question?answer pairs where the vocabulary gap between the question242Verberne et al What Is Not in the Bag of Words forWhy-QA?and the answer is still a problem.
There are cases where semantically related terms inthe question and the answer are of different word classes (e.g., hibernate?hibernation),and there are cases of proper nouns that are not covered by WordNet (e.g., B.B.
King).We considered using dynamic stemming for verb?noun relations such as the hibernationcase but research has shown that stemming hurts as many queries as it helps (Bilotti,Katz, and Lin 2004).
Therefore, we experimented with a number of different semanticresources, namely, the nominalization dictionary Nomlex (Meyers et al 1998) and thewikiOntology by Ponzetto and Strube (2007).
However, in their current state of develop-ment these semantic resources cannot improve our system because their coverage is toolow to make a contribution to our re-ranking module.
Moreover, the present version ofthe wikiOntology is very noisy and requires a large amount of cleaning up and filtering.Third, we considered that the use of cue phrases may not be sophisticated enoughfor finding explanatory relations between question and answer.
Therefore, we exper-imented with the addition of cause?effect pairs from the English version of the EDRConcept Dictionary (Yokoi 1995) ?
as suggested by Higashinaka and Isozaki (2008).Unfortunately, the list appeared to be extremely noisy, proving it not useful as a sourcefor answer ranking.7.
Conclusions and Directions for Future ResearchIn the current research, we extended a passage retrieval system for why-QA using off-the-shelf retrieval technology (Lemur/TF-IDF) with a re-ranking step incorporatingstructural information.
We get significantly higher scores in terms of MRR@150 (from0.25 to 0.34) and success@10.
The 23% improvement that we reach in terms of MRRis comparable to that reached on various other QA tasks by other researchers in thefield (see Section 6.3).
This confirms our hypothesis in Section 1 that for the relativelycomplex problem of why-QA, a significant improvement can be gained by the additionof structural information to the ranking component of the QA system.Most of the features that we implemented for answer re-ranking are based on wordoverlap between part of the question and part of the answer.
As a result of this set-up,our features identify the parts of why-questions and their candidate answers that are themost powerful/effective for ranking the answers.
The question constituents that appearto be the most important are the question focus, the main verb, and the direct object.
Onthe answer side, most important are the title of the document in which the candidateanswer is embedded and knowledge on the presence of cue phrases.Because our features are overlap-based, they are relatively easy to implement.
Forimplementation of some of the significant features, a form of syntactic parsing is neededthat can identify subject, verb, and direct object from the question and sentences inthe candidate answers.
An additional set of rules is needed for finding the questionfocus.
Finally, we need a fixed list for identifying cue phrases.
Exploiting the title ofanswer documents in the feature set is only feasible if the documents that may containthe answers have titles and section headings similar to Wikipedia.In conclusion, we developed a method for significantly improving a BOW-basedapproach to why-QA that can be implemented without extensive semantic knowledgesources.
Our series of experiments suggest that we have reached the maximum per-formance that can be obtained using a knowledge-poor approach.
Experiments withmore complex types of information (discourse structure, cause?effect relations) showthat these information sources have not as yet developed sufficiently to be exploited ina QA system.243Computational Linguistics Volume 36, Number 2ReferencesBilotti, M. W., B. Katz, and J. Lin.
2004.What works better for questionanswering: Stemming or morphologicalquery expansion.
In Proceedings of theWorkshop on Information Retrieval forQuestion Answering (IR4QA) at SIGIR 2004,Sheffield.Bilotti, M. W., P. Ogilvie, J. Callan, andE.
Nyberg.
2007.
Structured retrieval forquestion answering.
In Proceedings of the30th Annual International ACM SIGIRConference on Research and Developmentin Information Retrieval, pages 351?358,Amsterdam.Burnage, G., R. H. Baayen, R. Piepenbrock,and H. van Rijn.
1990.
CELEX: A Guide forUsers.
CELEX, University of Nijmegen,the Netherlands.Carlson, Lynn, Daniel Marcu, andMary Ellen Okurowski.
2003.
Buildinga discourse-tagged corpus in theframework of Rhetorical Structure Theory.In Jan van Kuppevelt and Ronnie Smith,editors, Current Directions in Discourse andDialogue.
Kluwer Academic Publishers,Dordrecht, pages 85?112.Charniak, E. 2000.
A maximum-entropy-inspired parser.
ACM InternationalConference Proceeding Series, 4:132?139.Denoyer, L. and P. Gallinari.
2006.
TheWikipedia XML corpus.
ACM SIGIRForum, 40(1):64?69.Ferret, O., B. Grau, M. Hurault-Plantet,G.
Illouz, L. Monceaux, I. Robba, andA.
Vilnat.
2002.
Finding an answerbased on the recognition of the questionfocus.
NIST Special Publication,pages 362?370.Higashinaka, R. and H. Isozaki.
2008.Corpus-based question answering forwhy-questions.
In Proceedings of IJCNLP,pages 418?425, Hyderabad.Hovy, E. H., U. Hermjakob, andD.
Ravichandran.
2002.
A question/answer typology with surface textpatterns.
In Proceedings of the HumanLanguage Technology conference (HLT),pages 247?251, San Diego, CA.Joachims, T., L. Granka, B. Pan,H.
Hembrooke, and G. Gay.
2005.Accurately interpreting clickthrough dataas implicit feedback.
In Proceedings of the28th Annual International ACM SIGIRConference on Research and Development inInformation Retrieval, pages 154?161,Salvador, Brazil.Khalid, M. and S. Verberne.
2008.
Passageretrieval for question answering usingSliding Windows.
In Proceedings of theCOLING 2008 Workshop IR4QA,Manchester, UK.Liu, T. Y., J. Xu, T. Qin, W. Xiong, and H. Li.2007.
Letor: Benchmark dataset forresearch on learning to rank forinformation retrieval.
In Proceedings ofthe Workshop on Learning to Rank forInformation Retrieval (LR4IR) at SIGIR 2007,pages 3?10, Amsterdam.Meyers, A., C. Macleod, R. Yangarber,R.
Grishman, L. Barrett, and R. Reeves.1998.
Using NOMLEX to producenominalization patterns for informationextraction.
In Proceedings: TheComputational Treatment of Nominals,volume 2, pages 25?32, Montreal.Pedersen, T., S. Patwardhan, andJ.
Michelizzi.
2004.
WordNet::Similarity ?measuring the relatedness of concepts.
InProceedings of the National Conference onArtificial Intelligence, pages 1024?1025,San Jose, CA.Ponzetto, S. P. and M. Strube.
2007.
Derivinga large scale taxonomy fromWikipedia.In Proceedings of the National Conference onArtificial Intelligence, pages 1440?1445,Vancouver, BC.Quarteroni, S., A. Moschitti, S. Manandhar,and R. Basili.
2007.
Advanced structuralrepresentations for question classificationand answer re-ranking.
In Proceedings ofECIR 2007, pages 234?245, Rome.Salton, G. and C. Buckley.
1988.Term-weighting approaches in automatictext retrieval.
Information Processing andManagement, 24(5):513?523.Surdeanu, M., M. Ciaramita, andH.
Zaragoza.
2008.
Learning to rankanswers on large online QA collections.
InProceedings of ACL 2008, pages 719?727,Columbus, OH.Tellex, S., B. Katz, J. Lin, A. Fernandes, andG.
Marton.
2003.
Quantitative evaluationof passage retrieval algorithms forquestion answering.
In Proceedings of the26th Annual International ACM SIGIRConference on Research and Developmentin Information Retrieval, pages 41?47,Toronto.Tiedemann, J.
2005.
Improving passageretrieval in question answering usingNLP.
In Progress in Artificial Intelligence,volume 3808.
Springer, Berlin /Heidelberg, pages 634?646.Verberne, S. 2006.
Developing an approachfor why-question answering.
In ConferenceCompanion of the 11th Conference of theEuropean Chapter of the Association for244Verberne et al What Is Not in the Bag of Words forWhy-QA?Computational Linguistics (EACL 2006),pages 39?46, Trento.Verberne, S., L. Boves, N. Oostdijk, andP.
A. Coppen.
2007a.
Discourse-basedanswering of why-questions.
TraitementAutomatique des Langues (TAL), special issueon ?Discours et document: traitementsautomatiques?, 47(2):21?41.Verberne, S., L. Boves, N. Oostdijk, andP.
A. Coppen.
2007b.
Evaluatingdiscourse-based answer extraction forwhy-question answering.
In Proceedings ofthe 30th Annual International ACM SIGIRConference on Research and Developmentin Information Retrieval, pages 735?736,Amsterdam.Verberne, S., H. Van Halteren, D. Theijssen,S.
Raaijmakers, and L. Boves.
2009.Learning to rank QA data.
In Proceedingsof the Workshop on Learning to Rank forInformation Retrieval (LR4IR) at SIGIR 2009,pages 41?48, Boston, MA.Yokoi, T. 1995.
The EDR electronic dictionary.Communications of the ACM, 38(11):42?44.Zhai, C. 2001.
Notes on the Lemur TFIDFmodel.
Technical report, School ofComputer Science, Carnegie MellonUniversity.245
