Effects of Variable Initiative on LinguisticBehavior in Human-Computer SpokenNatural Language DialogueRonnie W. Smith*East Carolina UniversitySteven A. Gordon*East Carolina UniversityThis paper presents an analysis of the dialogue structure of actual human-computer interactions.The 141 dialogues analyzed were produced from experiments with a variable initiative spokennatural anguage dialogue system organized around the paradigm of the Missing Axiom Theoryfor language use.
Results about utterance classification i to subdialogues, frequency of user-initiated subdialogue transitions, regularity of subdialogue transitions, frequency of linguisticcontrol shifts, and frequency of user-initiated error corrections are presen ted.
These results indicatethere are differences in user behavior and dialogue structure as a function of the computer's levelof initiative.
Furthermore, they provide vidence that a spoken atural anguage dialogue systemmust be capable of varying its level of initiative in order to facilitate effective interaction withusers of varying levels of expertise and experience.1.
Modeling Human-Computer DialogueIt is generally acknowledged that developing a successful computational model ofinteractive natural anguage (NL) dialogue requires extensive analysis of sample dia-logues.
Previous work has included analyses of (1) human-human dialogues in rele-vant task domains; (2) Wizard-of-Oz dialogues in which a human (the Wizard) simu-lates the role of the computer as a way of testing out an initial model; and (3) human-computer dialogues based on initial implementations of computational models.
Eachof these dialogue types has advantages as a model for system building, in terms ofthe relevance of the data to the final model.
However, each also has particular disad-vantages when researchers attempt o generalize from the findings of previous work.For example, much analysis of human-human i teractions has been done, suchas Walker and Whittaker's (1990) analysis of mixed initiative in dialogue, or Oviattand Cohen's (1991) comparison of interactive and non-interactive spoken modalities.Analyses of human-human dialogues are a good basis for an initial task model and alexicon, but it is difficult to determine which aspects of these analyses will generalizeto human-computer dialogues and which ones will not.
Fraser and Gilbert (1991) notethat "although it is certainly better to rely on analyses of human-human i teractionsthan to rely on intuitions alone, the fact remains that human-human i teractions arenot the same as human-computer interactions and it would be surprising if they fol-lowed precisely the same rules" (p. 81).
In addition, Oviatt and Cohen (1991) say that"... to model discourse accurately for interactive systems further research clearly willbe needed on the extent o which human-computer speech differs from that betweenhumans.
At present, there is no well developed model of human-machine communi-* Department of Mathematics, Greenville, NC 27858, USA.
First author's e-mail: rws@cs.ecu.edu(~ 1997 Association for Computational LinguisticsComputational Linguistics Volume 23, Number 1cation .
.
. "
(p. 323).
The dilemma of researchers i nicely summarized by Fraser andGilbert: "The designer is caught in a vicious circle it is necessary to know the char-acteristics of dialogues between people and automata in order to be able to build thesystem, but it is impossible to know what such dialogues would be like until such asystem has been built" (p. 81).Wizard-of-Oz (WOZ) dialogues result from an experimental technique that is oneway of addressing this dilemma.
In this methodology, human subjects are told theyare interacting with a computer when they are really interacting with another human(the Wizard) who simulates the performance of the computer system.
In some sim-ulations (e.g., Whittaker and Stenton \[1989\]), the Wizard simulates the entire systemwhile in other cases (e.g., Dahlb~ick, J6nsson, and Ahrenberg \[1993\]), the Wizard makesuse of partially implemented systems to assist in responding.
Consequently, manyinitial models can be prototyped and tested before implementation, and researchersneed not have a fully developed natural anguage interface.
As other researchers havenoted (Whittaker and Stenton 1989; Dahlback, J6nsson, and Ahrenberg 1993; Fraserand Gilbert 1991), when the WOZ simulations are convincing, they obtain data that area more accurate predictor of actual human-computer interaction than human-humandialogues because speakers adapt to the perceived characteristics of their conversa-tional partners.
Consequently, WOZ studies can provide an indication of the typesof adaptations that humans will make in human-computer interaction.
WOZ studiessuch as the ones cited above have been particularly useful in obtaining data on dis-course structure and contextual references.
The WOZ study of Moody (1988) on theeffects of restricted vocabulary on interactive spoken dialogue provided the data thatinfluenced the development of our own system.While much knowledge can be gained from WOZ studies, they are not an adequatemeans of studying all elements of human-computer natural language dialogue.
Asimulation is feasible as long as humans can use their own problem-solving skills incarrying out the simulation, but when it requires mimicking a proposed algorithm, theWOZ technique becomes impractical.
For example, it is difficult to simulate and testthe computer's error recovery strategies for speech recognition or natural languageunderstanding errors, because the natural language understanding of the computeris only a simulation.
If we wish to test an actual computational model for naturallanguage processing, its complexity demands the construction of a computer programto execute it.
Furthermore, an important feature of dialogue that is difficult to simulatevia the WOZ paradigm is that of initiative.
Depending on the interaction environment,dialogue initiative may reside with the computer, with the user, or may change duringthe interaction.
Lacking any formal models of initiative, it would be very difficult fora Wizard to accurately simulate the response patterns a computerized conversationalparticipant would produce in a mixed-initiative dialogue for a nontrivial domain thatwould be consistent from subject o subject.Unfortunately, we can also have difficulties generalizing from analyses of human-computer dialogues, because parameters of the particular system with which the di-alogues were collected may have significantly affected the resulting dialogues.
Forexample, if a particular system is always run with a particular speech recognizer, itmay be difficult to determine what the outcome would have been with a better speechrecognizer.
Similarly, most human-computer dialogues are collected from systems witha particular dialogue model.
Since it is well known that users adapt to the system, itwill be unclear how the results from a particular set of human-computer dialoguesgeneralize to a model of interaction based on a different dialogue model.This paper reports work that attempts to address both of these dilemmas throughthe analysis of human-computer dialogues collected in an environment in which142Smith and Gordon Human-Computer Dialogueaspects of the system are parameterizable.
We have built an integrated ialogue-processing system, the Circuit Fix-It Shop, which is parameterized for a key systembehavior: initiative.
1We have tested the system in 141 dialogues totaling 2,840 userutterances while varying levels of system initiative.
The paper discusses our model ofinitiative and presents quantitative results from an analysis of our corpus on the effectof the computer's level of initiative on aspects of human-computer dialogue structuresuch as (1) utterance classification into subdialogues, (2) frequency of user-initiatedsubdialogue transitions, (3) regularity of subdialogue transitions, (4) frequency of lin-guistic control shifts, and (5) frequency of user-initiated error corrections.
The resultsindicate there are differences in user behavior and dialogue structure as a function ofthe computer's level of initiative.
Furthermore, they provide evidence that a spokennatural anguage dialogue system must be capable of varying its level of initiative inorder to facilitate effective interaction with users of varying levels of expertise andexperience.2.
A Theory of Variable Initiative DialogueIn this section we review a theory presented in Smith and Hipp (1994).
It is important tonote that our focus is on task-oriented dialogues, that is, dialogues whose purpose is todiscuss a task whose completion is being carried out at the same time as the dialogue.Consequently during the discussion, we make the distinction between linguistic goalsand task goals.
Linguistic goals relate to speaker intentions in making statements (e.g.,to inform, command, or request), while task goals relate to specific actions that need tobe carried out in the domain of interest in order to complete the task (e.g., performinga voltage measurement).
As will be seen from the discussion, we take the approachthat task initiative is assigned to the participant whose current ask goals have priority,and the purpose of dialogue initiative is to indicate who has the task initiative.2.1 Defining Variable Initiative and Dialogue ModeVariable initiative dialogue is dialogue in which: (1) either dialogue participant canhave control of the dialogue, (2) control can vary between participants during thedialogue, and (3) intermediate l vels of control are allowed.A variable initiative dialogue system contrasts with other NL dialogue systemssuch as those described in Section 3.1 in which the dialogue is either purely user-controlled or purely computer-controlled.
In user-controlled dialogue systems the com-puter acts as a passive agent responding to user queries.
Question-answering systemsare examples of user-controlled dialogue systems.
2 In computer-controlled dialoguesystems, the user is totally dependent on the computer for accomplishment of thetask.The need for variable initiative dialogue arises because at some points during taskcompletion a user may have sufficient knowledge to take control of the dialogue andaccomplish several goals without much computer assistance while at other times, auser may need detailed assistance.
Thus, user initiative is characterized by giving pri-ority to the user's goals of carrying out steps uninterrupted while computer initiativeis characterized by giving priority to the specific goals of the computer.
In general,we have observed that the level of initiative that the computer has in the dialogue is1 Smith and Hipp (1994) presents details about the overall computational model that forms the basis ofthe system.2 While some question-answering systems can initiate clarifications todisambiguate user queries, theuser emains in control of the overall interaction.143Computational Linguistics Volume 23, Number 1primarily reflected in the degree to which it allows the user to interrupt he currentsubdialogue in order to discuss another task goal.
When the user has control, the in-terrupt is allowed, but when the computer has control it is not.
3 However, initiativeis not an all-or-nothing control mechanism.
Either the user or the computer may havethe initiative without having complete control of the dialogue.
Based on these obser-vations, four dialogue modes were identified that characterize the level of initiativethat the computer can have in a dialogue.
These are described below.....Directive: The computer has complete dialogue control.
It recommends atask goal for completion and will use whatever dialogue is necessary tocomplete this goal.
No interruptions to other subdialogues are allowed.Suggestive: The computer still has dialogue control, but not as strongly.The computer will recommend a task goal for completion, but will allowminor interruptions to closely related subdialogues.
4Declarative: The user has dialogue control and can interrupt to anydesired subdialogue at any time.
However, the computer is free tomention relevant facts as a response to the user's statements.Passive: The user has complete dialogue control.
Consequently, thecomputer will provide domain information only as a direct response to auser question.2.2 Response Formulation in Variable Initiative DialogueSince the degree of interruptibility allowed by the computer increases from directive topassive mode, dialogue mode has a critical effect on the computer's choice of response.As illustrated in Figure 1, response topic selection is a function of the computer's goal,the user focus (i.e., the task goal that the computer believes the user currently wantscompleted), and the dialogue mode.
When the user focus differs from the computer'sgoal (i.e., an interrupt), the dialogue mode becomes the decisive factor in the selectionprocess, as described in Figure 2.
The mechanics of the response selection algorithmcan be illustrated via the following situation: suppose that the user initially states,"the light is off," and suppose the computer knows that in order for the light to be lit,the switch must be turned up (i.e., state(switch, up) ~ state(light, on)).
Consequently,the computer goal of highest priority is to put the switch up, while the user focus isassumed to be on the light.
5 The selection process as a function of mode would be asfollows:.2.Directive: User focus is ignored with no interrupts permitted.
Theselected goal is to put the switch up.Suggestive: User focus is seen to be related to the computer goal via thedomain fact relating the switch and the light.
The selected goal is to"observe the switch position when the light is off.
"3 Note that we do not consider clarification subdialogues tobe interrupts, as the overall task goalremains unchanged.4 Subdialogues about different task goals are considered closely related if the different objects of interestshare a sufficiently close common ancestor in the domain-knowledge hi rarchy.5 A complete plan recognition process for inferring the user's exact goal has tended to be a very costlycomputational process and not feasible in a system designed for real-time interaction.
Consequently,our system uses the user focus as determined from the previous utterance as a basis for its beliefsabout the user's current goals.144Smith and Gordon Human-Computer DialogueINPUTS OUTP\ [ .q  ~Current Computer  GoalCurrent User FocusDialog ModeComputerResponseSelectionAlgorithmSelected Task GoalFigure 1Flow diagram ofthe computerresponse selection process.IF Mode = directive THENselect the computer goal without any regard for the user's focusELSE IF Mode = suggestive THENsearch the domain knowledge hierarchy for a common relationshipbetween the computer goal and the user focusIF such a relationship exists THENselect this as the next goalELSEselect the computer's original goalELSE IF Mode = declarative THENsearch the domain knowledge hierarchy for a common relationshipbetween the computer goal and the user focusIF such a relationship exists THENselect as the next goal that the user learn about thisrelationshipELSEselect as the next goal an uncommunicated fact relevant tothe user focusELSE IF Mode = passive THENselect as a goal that the user learn the computer has processedthe user's last utteranceFigure 2Computerresponse selection algorithm...Declarative: User now has control.'
Consequently, the selected goal mustbe a relevant fact.
The previous goal is converted to "user learn that thelight is on when the switch is up.
"Passive: User has complete control.
Computer simply acknowledgesprocessing the last user utterance.This response selection process has been implemented as part of the previouslymentioned Circuit Fix-It Shop.
The two dialogues of Figure 3, obtained from actualusage of the implemented system, illustrate differences between the two modes in145Computational Linguistics Volume 23, Number 1which the system was experimentally evaluated: directive and declarative.
Note thefollowing phenomena in these dialogues...In the directive mode dialogue, the subject is performing task goalsunder the close guidance of the computer.
There is language interactionabout each task goal.
6In the declarative mode dialogue, the subject independently carries outseveral task goals, known to be necessary, without any interaction.
Byallowing the user to arbitrarily change subdialogues, the computer isable to provide relevant assistance when a potential problem is reportedwithout requiring language interaction for the task goals alreadycompleted.A variable initiative dialogue system is just the first step toward the more impor-tant objective of a mixed-initiative dialogue system.
In a mixed-initiative interaction,initiative can vary between the participants hroughout the dialogue.
Given that ourfirst priority in experimentally evaluating the system was to demonstrate that behaviorvaried as a function of initiative, it was necessary to fix the level of initiative for theduration of a session.
We next review the work of others who have examined issuesin mixed-initiative interaction.2.3 Prior Theoret ica l  Work  on  Mixed- In i t ia t iveWhittaker and Stenton (1988) propose a definition for dialogue control based on theutterance type of the speaker (question, assertion, command, or prompt) as follows:?
Question: The speaker has control unless the question directly followed aquestion or command by the other conversant.?
Assertion: The speaker has control unless the assertion was a response toa question.?
Command: The speaker has control.?
Prompt: The listener has control because the speaker is abdicatingcontrol.Walker and Whittaker (1990) apply these dialogue control rules to analyze howinitiative affects discourse structure.
Their analysis distinguishes between advisorydialogues and task-oriented dialogues, but they do not allow for the possibility thatthe novice in a task-oriented dialogue can gain knowledge over time and want morecontrol of the dialogue.
Their key observations include the following:Transfer of control is often a collaborative phenomenon.
Since anoncontrolling participant has the option of seizing control at anymoment, then the controlling participant must have control because thenoncontroller allows it.6 For brevity, dialogue 1represents one of the simplest directive mode interactions that could occur.
Formore complex examples, the transcripts of the dialogues collected during the experiment are availableby anonymous FTP.
Contact the first author for details.146Smith and Gordon Human-Computer DialogueDialogue i (Directive Mode)i.
C: This is the Circuit Fix-lt Shop.
How may I help you?2.
U: I want to fix a circuit.3.
C: What is the ID of the circuit?4.
U: RSl11.5.
C: I am familiar with that circuit.
The LED is supposed to be displayingalternately flashing one and seven.6.
U: The LED is off.7.
C: What is the switch at when the LED is off?8.
U: The switch is down.9.
C: Put the switch up.10.
U: Okay.II.
C: Put the knob to one zero.12.
U: Okay.13.
C: What is the LED displaying?14.
U: LED is displaying only a flashing seven.15.
C: Is there a wire between connector eight four and connector nine nine?16.
U: No.17.
C: Add a w i re  between connector  e ight  four  and connector  n ine  n ine .18.
U: Done.19.
C: What i s  the  LED d isp lay ing?20.
U: A l te rnate ly  f lash ing  one and  seven.21.
C: I s  the  one on the  LED d isp lay ing  fo r  a longer  per iod  of  t ime?22.
U: No.23.
C: I s  the  seven on the  LED d isp lay ing  fo r  a longer  per iod  of  t ime?24.
U: No.25.
C: Put  the  knob to  zero .26.
U: Okay.27.
C: What i s  the  LED d isp lay ing?28.
U: A l te rnate ly  d i sp lay ing  one and seven.29.
C: I s  the  one on the  LED d isp lay ing  fo r  a longer  per iod  of  t ime?30.
U: Yes.31.
C: Put  the  sw i tch  down.32.
U: The swi tch  i s  down.38.
C: What i s  the  LED d isp lay ing?34.
U; Noth ing .35.
C: The c i rcu i t  i s  work ing  cor rect ly .
Good-bye .D ia logue  2 (Dec larat ive  Mode)1.
C: Th is  i s  the  C i rcu i t  F ix - I t  shop .
How may I he lp  you?2.
U: RS l l l  i s  not  work ing .3.
C: I am fami l ia r  w i th  that  c i rcu i t .
The LED i s  supposed to  he d isp lay inga l te rnate ly  f lash ing  one and seven.NOTE: Sub ject  put  the  sw i tch  up and set  the  knob to  one zero ,  observedthe  LED d isp lay ,  and  noted  the  potent ia l  p rob lem wi thoutrequ i r ing  any  ass i s tance  f rom the  computer .4.
U: There  i s  no w i re  f rom connector  n ine  n ine  to  connector  e ight  four .5.
C: There  i s  supposed to  be a w i re  between connector  n ine  n ine  andconnector  e ight  four .NOTE: Subject then added the missing wire, and manually performed allnecessary checks to verify the circuit was functioning properly.6.
U: The circuit is working.7.
C: Glad to have been of assistance.
Good-bye.Figure 3Sample dialogues: directive and declarative.147Computational Linguistics Volume 23, Number 1?
The expert retains more control in the task-oriented dialogues, but thereare still occasional control changes when the novice has to describeproblems that are occurring while completing the task.t Summaries are more frequent in advisory dialogues due to the need forboth participants to verify that they do share the mutual beliefs neededto develop the necessary plan.In Section 6.5 we investigate the relationship of this notion of dialogue controlbased on linguistic goals to our task goal notion of control.Kitano and Van Ess-Dykema (1991) extend the plan recognition model of Litmanand Allen (1987) to consider mixed-initiative dialogue.
Their key insight is the obser-vation that the two participants may have different domain plans that can be activatedat any point in the dialogue.
Thus, there are speaker-specific plans instead of simplyjoint plans as in the Litman and Allen model.
This separation of plans permits greaterflexibility in the plan recognition process.
Furthermore, they extend the initiative con-trol rules proposed by Whittaker and Stenton to consider the utterance content byobserving that a speaker has control when the speaker makes an utterance relevantto his or her speaker-specific domain plan.
Although they do not consider a compu-tational model for participating in mixed-initiative dialogues, their observation thatthere are speaker-specific plans or goals underlies the model that we propose.2.4 Theory EvaluationWhile WOZ simulation of directive and passive modes is feasible, the requirements foralgorithmically determining the relationship between user focus and the computer goalmake WOZ simulations of suggestive and declarative modes very difficult, especiallygiven the fast response time necessary for spoken interaction.
Before the constructionof the Circuit Fix-It Shop, Moody (1988) conducted a Wizard-of-Oz study on the effectsof restricted vocabulary on interactive spoken dialogue.
Her data were the basis for theformulation of the experimental Circuit Fix-It Shop system.
Although she attemptedto acquire information concerning user behavior when users were given the initiative,she was unable to provide much information because her subjects did not interactwith the system enough to evolve from novices to experts.
Her attempts to yield theinitiative to users still led to statements that guided users step-by-step through thetask.
By direct testing of a computer system that implements our proposed model ofvariable initiative dialogue, we could more rigorously control the system performanceand more easily run repeated tests with subjects and allow them to gain task expertise.Simultaneously, we could more readily monitor the effects of the change in initiativesetting while holding other system features constant.In testing our theory of variable initiative dialogue, there were two main types ofphenomena we wished to examine: (1) general aspects of task efficiency, such as timeto completion and number of utterances spoken; and (2) the nature of the dialoguestructure.
Results on task efficiency are reported in detail in Smith and Hipp (1994) andare briefly reviewed in Section 6.1.
The primary contribution of this paper is to presentan analysis of how the dialogue structure varies according to the computer's level ofinitiative.
After reviewing some details about the overall dialogue-processing modeland its implementation, in Section 3, and a review of the experimental environment,in Section 4, the remainder of the paper focuses on the results of this analysis, a reviewof some related analyses, and some concluding remarks about the usefulness of theanalysis and the role of experimental natural anguage dialogue systems in modeling'human-computer dialogue.148Smith and Gordon Human-Computer Dialogue3.
Dialogue-Processing Model: An Integrated Approach3.1 Motivation and OverviewMost prior work on natural anguage dialogue has either focused on individual sub-problems uch as quantification, presuppositions, ellipsis, anaphoric reference, anduser modeling, or else focused on dialogue-processing ssues in database query ap-plications.
Examples of such dialogue systems are described in Allen, Frisch, andLitman (1982), Bobrow et al (1977), Carberry (1988), Frederking (1988), Hafner andGodden (1985), Hendrix et al (1978), Hoeppner et al (1983), Jullien and Marty (1989),Kaplan (1982), Levine (1990), Peckham (1991), Seneff (1992), Waltz (1978), Wilenskyet al (1988), Young et al (1989), and Young and Proctor (1989).
However, there hasbeen little work on integrating the various aspects of dialogue processing into a uni-fied whole (exceptions are Allen et al \[1995\] and Young et al \[1989\]).
Consequently,we developed a dialogue-processing model for task-oriented dialogues that when im-plemented in an electronic repair domain exhibits a number of important behaviorsincluding: (1) problem-solving; (2) coherent subdialogue movement; (3) user modelusage; (4) expectation usage; and (5) variable initiative behavior.
We summarize thekey features of the model below.?
Theorem proving is used as the reasoning mechanism for determiningwhen task goals are completed.?
Consequently, the purpose for language during the dialogue is to acquirethe missing axioms needed for proving task goal completion (i.e., TheMissing Axiom Theory \[Smith 1992\]).?
User model information is maintained as a set of axioms acquired frominferences based on user input.
The axioms may then be used by thetheorem prover.?
Finally, integration of theorems, the utterances relevant to thesetheorems, and the expectations for responses that supply missing axiomsyields a constructive method for creating and using a discourse modelfirst proposed by Grosz and Sidner (1986), but for which they did notoffer a method of dynamic onstruction during the course of a dialogue.Furthermore, the model enables the system to engage in variable initiative dialogueas outlined in Section 2.
The interested reader is referred to Smith, Hipp, and Biermann(1995) for further details about the overall model.3.2 System ImplementationWe constructed the Circuit Fix-It Shop based on the details of our dialogue-processingmodel.
The system was originally implemented on a Sun 4 workstation with the ma-jority of the code written in Quintus Prolog and the parser in C. The system assistsusers in the repair of a Radio Shack 160 in One Electronic Proiect Kit.
The system candetect errors caused by missing wires as well as a dead battery.Speech recognition is performed by a Verbex 6000 running on an IBM PC.
Toimprove speech recognition performance, we restrict he vocabulary to 125 words.
ADECtalk DTCO1 text-to-speech onverter is used to provide spoken output by thecomputer.An important feature of any spoken atural language dialogue system is the abilityto perform robust parsing.
Spoken inputs are frequently ungrammatical but must still149Computational Linguistics Volume 23, Number 1be interpreted correctly.
The main source of ungrammatical inputs in our experimentswas the misrecognition of the user's input.
An error-correcting parser was developedthat finds the minimal cost set of insertions, deletions, and substitutions to transformthe input into grammatical input (Smith and Hipp 1994).
During our formal exper-iment, the system was able to find the correct meaning for 81.5% of the more than2,800 input utterances even though only 50% of these inputs were correctly recognizedword for word.
An overview of the experimental design is presented next.4.
Experimental DesignThe experimental design is discussed in great detail in Smith and Hipp (1994) andSmith (1991).
Here we present an overview of the experiment sufficient for under-standing the environment in which the data were collected.4.1 Subject PoolThe eight subjects were Duke University undergraduates who met the following cri-teria.?
They had demonstrated problem-solving skills by having successfullycompleted one computer science course and had taken or were takinganother.?
They did not have excessive familiarity with AI and natural anguageprocessing.
In particular, they had not taken a class in AI and they hadnot interacted with a natural anguage system.?
None were majoring in electrical engineering.
Such individuals couldprobably fix the circuit without any assistance.The subject pool consisted of six male and two female subjects.
In addition, twopilot subjects, one female and one male, were run using the proposed experimentaldesign before the formal experiment began.4.2 Session Overview and Problem SelectionSubjects participated in the experiment in three sessions.
The first and third sessionsoccurred a week apart, and the second session normally occurred three or four daysafter the first session.
7 The first session consisted of: (1) the primary speech training,lasting approximately 60 to 75 minutes; (2) approximately 20 minutes of instructionon using the system; and (3) practice using the system by attempting to solve four"warmup" problems with the system operating in directive mode, the mode where thecomputer has maximal control.
A maximum of two and one-half hours was spent onthe first session.
The second and third sessions each consisted of: (1) review work withthe speech recognizer; (2) a review of the instructions; and (3) usage of the system onup to 10 problems depending on how rapidly the problems were solved.
One groupof subjects worked with the system in directive mode during the second session andin declarative mode during the third session while the other group worked with thesame modes, but in opposite sessions.
The time allowed for the second and thirdsessions was two hours each.7 The only exception was the last subject, where the second session occurred two days after the firstsession, and the third session occurred one week after the second session.150Smith and Gordon Human-Computer DialogueThe particular circuit being repaired is supposed to cause the LED to alternatelydisplay a 1 and a 7, and the implemented domain problem-solving component coulddetect errors caused by missing wires as well as a dead battery.
The basic debuggingprocess consists of the following steps:.2..Determine if the LED display is correct.If it is not correct, perform zero or more diagnostic steps to further isolatethe problem.
Possible diagnostic steps are voltage measurements or anLED observation under a different physical configuration of the circuit.Check for the absence of one or more wires until a missing wire isidentified.The wires are attached to metal spring-like connectors, which are identified bynumbers on the circuit board.
Thus, a wire is identified by the numbers of the twoconnectors to which it is connected.
In order to balance the difficulty of the prob-lems between the second and third sessions, the wires were classified according tothe number and type of diagnostic steps required to detect he error.
Based on thisclassification, the assignment of missing wires to problems in each session was madeas follows:?
Four wires were used in the four warmup problems of the first session.?
From a set of 10 other wires, 5 were used for the first five problems ofsession 2 and the other 5 were used for the first five problems ofsession 3.
Each of these problems was balanced for difficulty.
Forexample, problem 1 of both sessions was a power subcircuit problem,while problem 5 of both sessions was an LED subcircuit problem.Problems 2 through 4 were similarly balanced.?
Problems 6 through 8 of sessions 2 and 3 consisted of 2 missing wires foreach problem.
The 2 missing wires were selected from the 5 missingwires used during the first five problems of the session.
Each ofproblems 6 through 8 differed by one missing wire.
These problems werealso balanced for difficulty.?
Problems 9 and 10 of each session consisted of a missing wire that wasalso used during the warmup problems of session 1.
Each of these 4wires was assigned to a different problem.
Consequently, sessions 2and 3 are balanced for difficulty only through the first eight problems.4.3 Experimental SetupFigure 4 provides a rough sketch of the room layout.
The subject was seated facingthe desk containing the circuit board.
Communication with the speech recognizer wasperformed through a telephone handset.
The experimenter was seated in front of thecomputer console.
Thus, the subject's back was to the experimenter.
The experimenterhad a copy of the raw data form for the session, a copy of the word list, and aguide describing the allowed experimenter interaction with the subject.
Data collectionmechanisms consisted of the following:.
Automatic logging of the words received from the speech recognizer(subject input) and the words sent to the DECtalk (computer output).151Computational Linguistics Volume 23, Number 1EXPERIMENTAL SETUPTapeFigure 4Room setup.DECtalkP(\ \ ExperimenterCircuit BoardVoltmeterVocabulary Chart..This logging information included the time the words were received orsent.
In addition, time information was recorded for when the parserfinished its processing of the input and when the computation of theinput interpretation was complete.The interaction was tape recorded in order to make a transcript thatincluded the actual words used by the subject and the interactions thatoccurred between the subject and the experimenter.The experimenter made notes about the interaction on the raw data formas well as marked occurrences of subject-experimenter interactionaccording to the category into which the interaction could be classified.In order to assist he experimenter in determining when a misrecognitionoccurred, the experimenter monitored the file where automatic loggingoccurred.4.4 Experimenter InteractionAn important issue in experiments such as this, as has been observed elsewhere (Bier-mann, Fineman, and Heidlage 1992), is the problem of giving the subject sufficienterror messages to enable satisfactory progress.
One major source of difficulty in thisexperiment were misrecognitions by the Verbex speech recognizer.
These miscommu-nications created various problems for the dialogue interaction, ranging from repetitivedialogue to experimenter intervention to occasional failure of the dialogue.
Whenevera serious misrecognition caused the computer to interpret the utterance in a way thatcontradicted what was meant, the experimenter was allowed to (1) tell the subject hata misrecognition had occurred, and (2) tell the subject he interpretation made by thecomputer, but could say nothing else.
For example, when one subject said, "the circuit is152Smith and Gordon Human-Computer Dialogueworking," the speech recognizer returned the words "faster it is working."
This wasinterpreted as the phrase faster.
Consequently, the experimenter told the subject, "Dueto misrecognition, your words came out as faster."
It is important o note that whenan utterance was misunderstood, the experimenter did not tell the subject what to do,but merely described what happened.
In this way, the interaction was restricted tobeing between the computer and the subject as much as possible, given the quality ofcommercial, real-time, continuous peech recognition devices at the time of the exper-iment.
Such error messages from the experimenter occurred, on average, once every15 user-utterances throughout the experiment.The other main source of difficulty in using the system was the enforcement ofthe single utterance, turn-taking protocol of the interaction.
This required the user tosignal the beginning of an utterance by speaking the sentinel word verbie and end theutterance with the word over.
Users would sometimes forget to use the sentinel wordsor else would not wait for the system's response that would occasionally be delayedup to 30 seconds (normal response time was 5 to 10 seconds).
In cases where theinteraction protocol was violated, the experimenter would issue a warning statementsuch as, "Please be patient.
The system is taking a long time to respond," or "Pleaseremember to start utterances with verbie."
These types of experimenter interactionsoccurred, on average, once every 33 user-utterances.4.5 The Nature of the Spoken DialogueThe limitations of real-time continuous peech recognition at the time of the experi-ments had an impact on the nature of the spoken human-computer interaction thatwas observed in comparison to what might be expected in a spoken human-humaninteraction.
In particular, the restrictive 125-word vocabulary meant that speech re-pairs and disfluencies that are prevalent in human-human spoken interaction and animportant area of study (Oviatt 1995; Heeman and Allen 1994) could not be processedby the system.
Whenever a person misspoke, they could start over by issuing the sen-tinel word cancel, rather than over at the end of their utterance.
To prevent his fromhappening often, subjects were instructed at the start of their participation to plantheir utterance completely before speaking.
Consequently, there were only 11 cancelsissued in the production of the 2,840 user-utterances.
Furthermore, in exit interviewsconducted after they had completed participation, none of the subjects indicated anydifficulty with, or dislike of, planning utterances in advance.To summarize, the results in Section 6 on the structure of spoken natural lan-guage dialogue are based for the most part on planned speech, a consequence ofthe technological limitations of speech recognizers at the time.
Nevertheless, we be-lieve it represents the first widely reported and analyzed spoken human-computerco-operative problem-solving dialogue, and that it is representative of such dialoguefor the forseeable future.5.
Classifying Dialogue Utterances5.1 Major Subdialogues in Repair AssistanceFor task-oriented dialogues Grosz (1978) has noted that the structure of a dialogue mirrorsthe structure of the underlying task.
Moody (1988) conducted a Wizard-of-Oz study onthe effects of restricted vocabulary on interactive spoken dialogue.
Her data were thebasis for the formulation of the experimental Circuit Fix-It Shop system.
For repair153Computational Linguistics Volume 23, Number 1Table 1Utterance classification i to major subdialogues.Subdialogue Type Directive Dialogue Utterances Declarative Dialogue UtterancesIntroduction 1--4 1-2Assessment 5-14 3Diagnosis 15-16 4-5Repair 17-18 --Test 19-35 6-7tasks, she identified five primary task subdialogues:?
Introduction Subdialogue (I): Establish the purpose of the task (e.g., tofix the circuit with ID number RSl11).?
Assessment Subdialogue (A): Establish the current behavior.?
Diagnosis Subdialogue (D): Establish the cause for the errant behavior.?
Repair Subdialogue (R): Establish that the correction for the errantbehavior has been made.?
Test Subdialogue (T): Establish that the behavior is now correct.Table 1 shows the classification into the various subdialogues of the utterancesfrom the sample dialogues of Figure 3.5.2 Subdialogue TransitionAnother important aspect of the dialogue structure is the nature of the transitionsbetween subdialogues.
The model we present is derived from Moody's (1988) study,mentioned above.
In the absence of errors in completing task actions, the naturaltransition from subdialogue to subdialogue is described by the following regular ex-pression:I+A+(D+R*T+)nFwhere "+" denotes that one or more utterances will be spoken in the given subdia-logue, "," denotes that zero or more utterances will be spoken in the given subdia-logue, and n represents the number of individual repairs in the problem.
8 The letterscorrespond to the abbreviations given in Section 5.1, and F represents the finishedstate (i.e., completion of the dialogue).
This transition model is also depicted in thefinite-state network of Figure 5.
For clarity, loop arcs (i.e., transitions from a subdia-logue back into itself) are omitted.
We see from this model that dialogues normallybegin with the Introduction and Assessment phases.
Once the errant system behavioris described, the dialogue goes through one or more cycles of Diagnosis, Repair, andTest, until the system behavior is correct.8 In our domain, n represents he number of missing wires in the problem.
For example, when there aretwo missing wires, the first DRT iteration will cause one missing wire to be added, but the Test phasewill show that the circuit is still not working.
A second DRT iteration is required to detect and add themissing wire that completes the repair.154Smith and Gordon Human-Computer DialogueFigure 5Subdialogue transition as a finite-state network.This model was helpful in classifying each utterance into the appropriate subdia-logue.
As discussed in Section 6.4, however, not all dialogues followed this model, dueto user initiative and dialogue miscommunication.
Nevertheless, it provides a goodfirst approximation of the nature of subdialogue movement.5.3 Transcript CodingThe two authors each coded the transcripts independently.
Every utterance (those spo-ken by the computer as well as those spoken by the human subject) was classified intoone of these five subdialogue categories, according to two perspectives: the speaker'sperspective (i.e., the task subdialogue that the speaker of the utterance believed wasrelevant o the statement) and the global perspective (i.e., the task subdialogue thatis relevant o the utterance, based on omniscient knowledge of the task status).
Nor-mally these were the same, but not always.
In situations where the user carried outa repair without explicitly notifying the computer, the computer might think the taskwas still in one phase, when the user had actually moved the task into another phase.In the results to be presented, the current subdialogue is based on global, rather thanspeaker, perspective.
Overall, there was a difference between speaker and global per-spective in 6.7% of the declarative mode utterances and in 1.7% of the directive modeutterances.5.4 Coding ReliabilityThe two authors compared their coding results as the transcripts for each one of theeight subjects were completed, in order to resolve differences and, hopefully, improveagreement as more transcripts were coded.
The first author was a principal designerof the system, while the second author had only watched a videotape of the systemin operation and read some of the previous papers about the project.
Consequently,many of the initial disagreements in coding were due to a lack of familiarity with whattranspired uring the experiment.
For example, in situations where the Repair subdi-alogue was not explicitly verbalized, it was not clear whether subsequent descriptionsof the circuit behavior indicated that the current subdialogue was Test or Assessment.Proper coding in these situations required familiarity with what had actually occurredduring the experiment, familiarity that only the first author had.
For all dialogues,initial interrater agreement on both speaker and global perspective of the current sub-dialogue was 87.2%.
That is, for 12.8% of the utterances, there was a disagreementbetween the coders over either speaker perspective of the current subdialogue, globalperspective, or both.
The kappa coefficient (Isard and Carletta 1995) for the level ofagreement is 0.82.
When the coding process was completed, all discrepancies wereresolved to the satisfaction of both authors.155Computational Linguistics Volume 23, Number 16.
Results6.1 Data Inclusion and Statistical AnalysisSubjects attempted a total of 141 dialogues, of which 118 or 84% were completedsuccessfully.
9 The average speech rate by subjects was 2.9 sentences per minute, andthe average task completion time for successful dialogues was 6.5 minutes.
The systemhad an average response time of 8.1 seconds during the formal experiment.
Later, afaster parsing algorithm was implemented and the system was ported to a SPARC IIworkstation from the Sun 4 used during the experiment.
During test dialogues usingthe enhanced system, average response time was 2.2 seconds.In general, differences in user behavior depending on the level of computer initia-tive were observed.
When the computer operated in declarative mode--y ie ld ing theinitiative to human users, who could then take advantage of their acquired expert ise--the dialogues:?
were completed faster (4.5 minutes versus 8.5 minutes).?
had fewer user-utterances per dialogue (10.7 versus 27.6).?
had users speaking longer utterances (63% of the user-utterances weremultiword versus 40% in directive mode).While users given the initiative in the final session were somewhat more efficientat completing the dialogues than users given the initiative in the second session (com-pleting dialogues approximately 1.5 minutes faster and speaking on average 2.7 fewerutterances), the large standard eviations, which ranged from 50% to 90% of the asso-ciated sample means, and the small number of subjects tested indicate that we shoulduse caution in generalizing from our results.Unless explicitly noted, the results on human subjects' linguistic behavior that willbe reported throughout this section are based only on the 118 dialogues that were suc-cessfully completed.
While the 23 incomplete dialogues also contain interesting phe-nomena, we chose to focus the analysis on the completed ialogues, as they representthe linguistic record of successful interactions with the system.
In reality, there are onlyslight differences in the results when the unsuccessful dialogues are included.
Further-more, a valid statistical analysis could only be performed on the completed ialogues.Reporting data values from only the successful dialogues maintains consistency withthe reported statistical values.6.2 Utterance Classification into Subdialogues6.2.1 Hypotheses.
For users to take the initiative in the task domain, they must havesome expertise in the domain.
Once this expertise is gained, and the computer yieldstask control to the human user, it is expected that users will exploit the situation torestrict the dialogue to specific issues of interest.
Presumably, such users have sub-stantial knowledge about the general behavior of the circuit, how to determine when9 Due to time constraints, not all subjects were able to attempt all possible dialogues.
Only three of theeight subjects successfully completed all possible dialogues.
Of the 23 dialogues not completed, 22 wereterminated prematurely due to excessive time being spent on the dialogue.
Misunderstandings due tomisrecognition were the cause in 13 of these failures.
Misunderstandings due to inadequate grammarcoverage occurred in 3 of the failures.
In 4 of the failures, the subject misconnected a wire.
In one failurethere was confusion by the subject about when the circuit was working, and in another failure therewere problems with the system software.
A hardware failure caused termination ofthe final dialogue.156Smith and Gordon Human-Computer DialogueTable 2Utterance breakdown i to major subdialogues.Subdialogue Type Directive Mode Declarative ModeAverage Percent Average PercentIntroduction 2.9 5.2% 2.6 11.6%Assessment 15.4 27.4% 7.9 35.1%Diagnosis 11.8 21.0% 6.7 29.8%Repair 2.9 5.2% 0.3 1.3%Test 23.2 41.2% 5.0 22.2%Total 56.2 22.5it is working, and the basic nature of repairs, but will need some assistance with di-agnosing specific problems.
Consequently, we would expect he following differencesbetween modes for users who are able to take the initiative:?
Introduction Subdialogue: The number of utterances will change little,since problem introduction seems independent of initiative.?
Assessment Subdialogue: The number of utterances will be reducedslightly in declarative mode, as users who take the initiative may exploittheir control of the dialogue to carry out some preliminary steps withoutverbal interaction.?
Diagnosis Subdialogue: The number of utterances will change little, sinceall users presumably need the computer's assistance in problemdiagnosis.?
Repair Subdialogue: The change should be dependent on the taskdomain.
If the repair process is basically the same once the error isdiagnosed, few utterances will be required as repairs can be donewithout discussion.
If the repair process is highly dependent on the typeof error (e.g., debugging a program), even the skilled user may requiresignificant advice from the system.
For our domain, we expect areduction in the number of utterances spoken in declarative mode, sincethe repair process (adding a wire) is similar across the different problemtypes.?
Test Subdialogue: The number of utterances i  significantly reduced (i.e.,users who take the initiative can verify the circuit behavior withoutdialogue).6.2.2 Overall Averages.
Table 2 shows the average and relative number of utterancesspoken per dialogue in each of the main task subdialogues.
The reported ata com-bine both computer and user utterances.
Note that virtually no utterances were everspoken during the Repair phase of declarative mode dialogues.
This is because therepair process was always the addition of a missing wire to the circuit, a process thatusers quickly became able to do without explicit guidance.
However, since not manyutterances were spoken in the Repair phase of the directive mode dialogues either,the major source of the reduction in the absolute number of utterances spoken perdialogue occurred in the Assessment, Diagnosis, and Test phases, especially the Test157Computational Linguistics Volume 23, Number 1Table 3Statistical results on utterance classification for the first five problems.Mode Effect Subdialogue Effect Interaction EffectF Value p F Value p F Value pBy-subjects F(1,7) = 24.93 = 0.002 F(3,21) = 17.77 < 0.001 F(3,21) = 4.93 = 0.01By-items F(1,4) = 32.26 = 0.005 F(3,12) = 13.99 < 0.001 F(3,12) = 9.70 = 0.002phase.
Although we originally expected little change in the number of utterances asa function of initiative for the Diagnosis phase, the large increase in the number ofutterances poken for that phase for problem 6, during directive mode interactionshad a major impact on the overall averages.
Excluding problem 6, the average num-ber of utterances spoken in the Diagnosis phase was 9.4 in directive mode and 7.2 indeclarative mode.6.2.3 Statistical Analysis.
While the first eight problems in each of the two experi-mental sessions are balanced (Section 4.2), we must distinguish between the first fiveproblems of each session, where there was a single missing wire in each problem andproblems 6 through 8 in each session, which have two missing wires.
Not all subjectscompleted the same number of dialogues for problems 6 through 8 in the two ex-perimental sessions.
Consequently, including them in the computation of the averagenumber of utterances spoken in a given subdialogue phase would distort the averagesused in a statistical analysis} ?Therefore, we apply the statistical technique of analysis of variance (ANOVA) tothe data from the first five problems of each session, the single-missing-wire problems.This represents a total of 60 completed ialogues.
A 2 X 4 design (mode X subdialoguephase) was used (the Introduction phase was omitted).
11Table 3 summarizes the results of the statistical analysis.
The analysis was con-ducted using the averages by subjects as well as by items (problems).
The individualmain effects showed very strong statistical significance under both forms of analysiswhile the interaction effect of mode and subdialogue phase also appears to be statis-tically significant, but not quite as strongly as the main effects individually.
We nowturn our attention to the order effect.
Did the order in which subjects were given theinitiative affect their performance?6.2.4 The Effects of Experience.
As mentioned in Section 4.2, we balanced the experi-ment problems according to type, such that problem k of both sessions 2 and 3 was thesame type of problem.
Furthermore, we balanced the subjects also.
Half the subjectsused the system when it was operating in directive mode for session 2 while the otherhalf used the system when it was operating in declarative mode for session 2.
Themode was, of course, reversed for session 3 for both groups.
One of our claims hasbeen that as users gain experience and are given the initiative by the system, they10 As shown in the transition model of Figure 5, the Diagnosis, Repair, and Test subdialogue phases couldoccur twice in a dialogue with two missing wires.11 As discussed in the next section, the order in which subjects were given the initiative did not show asignificant effect and is omitted from the current analysis.158Smith and Gordon Human-Computer Dialoguewill take advantage of that.
We might expect hen, that subjects given the initiativein session 3 would behave differently than subjects given the initiative in session 2.Furthermore, we might expect difficulties for subjects given the initiative in session 2who then had to work with the system in directive mode in session 3.
What do wefind in the results?We conducted a paired t-test on the paired differences ~2 in the average number ofutterances spoken per dialogue between the two modes, as a function of the problemnumber.
Computing this test statistic for the two subdialogue phases in the domainwhere we would expect additional experience to have the most effect, Assessment andDiagnosis, yields the following results.
For the Assessment phase, the test statistic is0.854 with a corresponding p value of 0.42 for 7 degrees of freedom.
For the Diagnosisphase, the test statistic is 0.556 with a corresponding p value of 0.60.
Consequently, wedo not find that the order in which a subject was given the initiative has a significanteffect on the number of utterances spoken in a given subdialogue phase.
We do notfind this result surprising because:Some expertise was gained during the preliminary training session, sosome subjects were ready to be given initiative in session 2.
In fact, thetwo subjects who struggled with using declarative mode in session 2only contribute 5 of the 48 declarative mode data points used incomputing the averages.Some subjects, as part of their expertise, developed a somewhatritualistic style of interaction with the machine, which may havelengthened their interactions.6.3 User Initiation of Subdialogue TransitionsWhen the computer has total control of the dialogue, in directive mode, it is expectedthat the computer will initiate the transitions between subdialogues.
How will thischange when the computer operates in declarative mode and control is given back tothe user?While user control means the user's goals have priority, it does not necessarilymean the user will initiate every transition from one subdialogue to the next.
Theuser controls the dialogue but still requires computer assistance.
Consequently, it isexpected that the computer will still initiate many of the transitions to the Assessmentand Diagnosis phases in order to provide assistance in these areas, but that the userwill be able to transition to other subdialogues as deemed appropriate.
In particular,it is expected that the user will initiate most of the transitions to the final Test phasefor confirming circuit behavior, since an experienced user would have learned howthe circuit should function.These hypotheses are generally supported by the results in Table 4.
When thecomputer had the initiative (the directive mode dialogues), very few subdialoguetransitions were ever initiated by the user other than to the final Test phase whenthe repair would cause the circuit to begin to function normally.
When the computeryielded the initiative (the declarative mode dialogues), users initiated the transition12 For example, the value of 12 for problem 3 in the Assessment phase for subjects who operated indeclarative mode in session 2 and directive mode in session 3 is obtained by subtracting the declarativemode average for the number of Assessment u terances spoken per dialogue, 9, from the directivemode average, 21.
This value would be paired with the value 8 (18 - 10) also for problem 3 in theAssessment phase, but for subjects who operated in directive mode in session 2 and declarative modein session 3.159Computational Linguistics Volume 23, Number 1Table 4Subdialogues initiated by each participant.Assessment Subdialogues Directive Mode Declarative ModeSystem-initiated 70 83User-initiated 9 20Diagnosis Subdialogues Directive Mode Declarative ModeSystem-initiated 96 96User-initiated 0 7Repair Subdialogues Directive Mode Declarative ModeSystem-initiated 78 3User-initiated 0 5Test Subdialogues Directive Mode Declarative ModeSystem-initiated 71 6User-initiated 22 77to the final stage of the dialogue almost every time.
In the intermediate stages, thecomputer still initiated most subdialogues, but users occasionally felt compelled tocause a change to a different phase.
This rarely happened when the computer had theinitiative.
Not counting the Introduction, which had to be initiated by the computer,only 9% of all subdialogues in directive mode were initiated by the user while 37% ofthe subdialogues in declarative mode were user-initiated.6.4 General Subdialogue TransitionsAs described in Section 5.2, the natural course of transition from subdialogue to sub-dialogue is described by the following regular expression:I+A+(D+R*T+)nFwhere n represents the number of individual repairs in the problem (i.e., numberof missing wires in our domain).
If every dialogue followed this model, then wewould expect o see all transitions out of the Introduction phase go to the Assessmentphase, all transitions out of the Assessment phase go to the Diagnosis phase, and alltransitions out of the Repair phase go to the Test phase.
However, with the potentialfor miscommunication as well as the potential for users to exploit their expertise andcontrol of the dialogue to skip discussion of some task steps, it is highly unlikely thatthe actual results will follow the idealized model.
Where might we see differences?Table 5 shows the actual breakdown in percentages.
The row value representsthe initial subdialogue phase and the column represents the new subdialogue.
TheF column represents the finished state (i.e., dialogue completion).
For example, thepercentage of all transitions out of the Diagnosis phase that went to the Assessmentphase is 18.8% in directive mode and 38.8% in declarative mode.
The X entries alongthe main diagonal represent impossible xit transitions (i.e., there cannot be a transitionfrom Diagnosis to Diagnosis).
The "--" entries represent values of less than 5%.
13 If thedialogues follow the transition model, then the largest entries hould be in the values13 Consequently, he numerical values in each row will not necessarily add up to 100%.160Smith and Gordon Human-Computer DialogueTab le  5Subdialogue transition breakdown as a function of dialogue mode.Directive Mode Declarative ModeI A D R T F I A D R T FI X 100  .
.
.
.
I X 96 .8  .
.
.
.A - -  X 91 .1  - -  7 .6  - -  A - -  X 79 .6  - -  19 .4  - -D - -  18 .8  X 68 .7  12.5  - -  D - -  38 .8  X 7 .8  53 .4  - -R - -  - -  - -  X 96 .2  - -  R - -  12 .5  12.5  X 75 .0  - -T - -  - -  24 .7  12 .9  X 62 .4  T - -  - -  24 .1  - -  X 72 .3in the diagonal just above the main diagonal.
The resulting largest entry in each rowis noted in boldface.For the most part, the percentages are consistent with the model, especially in theearly phase transitions and in the transitions out of the Test subdialogue.
Based on therelative number of completed ialogues that required the repair of two missing wires(17 in directive mode, 21 in declarative mode), the expected percentage of transitionsfrom Test-to-Diagnosis would be 22.7% in directive mode and 25.9% in declarativemode.
TM The actual values of 24.7% and 24.1% compare favorably with the expectedresults.
The large relative difference in percentages for transitions from Diagnosis toeither Repair or Test in the two modes is also expected, given that users who takethe initiative can make the repair themselves without discussing it with the computer.The transition percentages that are most surprising are the Diagnosis-to-Assessmenttransitions in both modes and the Test-to-Repair t ansitions in directive mode.
TheDiagnosis-to-Assessment transitions are indicative of attempts at error correction.
Thatis, at some point during Diagnosis either the computer or the user becomes uspiciousof the initial problem assessment and consequently moves back to Assessment to besure that the erroneous circuit behavior is properly understood.
The Test-to-Repairtransition is common when the user makes the repair without mentioning it.
That is,the user has prematurely moved from Repair to Test without notifying tile computerthat the repair has actually been made.
In directive mode dialogues, the computer willrequire verbal verification of the repair before transitioning to the Test phase.In general, 64% of the dialogues in directive mode have no "unusual" transitions(where we define unusual as a transition ot described by our model).
In contrast, only33% of the declarative mode dialogues had no unusual transitions, again demonstrat-ing how users felt free to skip steps without discussion.
This particularly increased asusers gained more experience, with only 26% of the 35 declarative dialogues of thefinal session containing no unusual transitions.6.5  Task  Cont ro l  versus  L ingu is t i c  Cont ro lAs described in Section 2.1, our view of initiative concerns which participant's taskgoals currently have priority.
Walker and Whittaker's (1990) study of mixed-initiativedialogue used a notion of control based on linguistic goals as specified in the controlrules first presented in Section 2.3 and repeated below.
These rules are a function of14 These Test-to-Diagnosis transitions occur because after repairing one of the missing wires, the Testphase would show that the circuit is still not working due to the other missing wire, causing atransition back to the Diagnosis phase to discover the other problem.161Computational Linguistics Volume 23, Number 1the classification of the linguistic goal of the current utterance (Assertion, Command,Question, or Prompt) and reflect he status of initiative after the utterance was made...3..Assertion: The speaker has the initiative unless the utterance is aresponse to a Question.Command: The speaker has the initiative.Question: The speaker has the initiative unless the utterance is aresponse to a question or command.Prompt: The hearer has the initiative.We analyzed our dialogues using this notion of control with one modification--assertions that were a continuation of the current opic left the initiative unchanged.Consider the following dialogue xcerpt:C: The LED is supposed to be displaying an alternately flashingone and seven.U: The LED is of f .C: The power is on when the switch is up.U: The switch is up.C: The switch is connecting to the battery when there is a wirebetween connectors 111 and 120.In both cases the user's assertions continue the topic introduced by the computer anddo not cause a change of control.
Contrast his with the following:C: The LED is supposed to be displaying an alternately f lashingone and seven.U: There is no wire between connector eight four and connectornine nine.C: There is supposed to be a wire between connector 84 andconnector 99.In this case the user's assertion does change control, as it is a change of topic.
Ourrule modification reflects this issue.6.5.1 Hypotheses.
The two primary measures reported by Walker and Whittaker areaverage number of utterances between control shifts and percent of total utterancescontrolled by the computer.
Their results for task-oriented dialogues about construct-ing a water pump showed that experts had control of the dialogue about 90% of thetime.
In contrast, their results for advisory dialogues where clients talked to an ex-pert over the phone to obtain assistance in diagnosing and repairing various softwarefaults showed that experts had control only about 50% of the time.
While our problemdomain is more similar to the advisory dialogues, the nature of our dialogues is moresimilar to the task-oriented dialogues as the task of circuit repair is being completedconcurrently with the dialogue.
Therefore, we expect he computer to show stronglinguistic ontrol when it has task initiative.
Conversely, when users control the taskinitiative, we expect more assertions by the user concerning the user's own task goals,rather than direct responses to computer questions or commands.
Nevertheless, be-cause the computer is the ultimate xpert, we still expect it to respond with assertionsof facts designed to assist he user that take a linguistic form that would be classifiedas continuing or regaining linguistic ontrol (e.g., "The power is on when the switch162Smith and Gordon Human-Computer DialogueTable 6Differences in linguistic ontrol as a function of initiative.Directive Mode Declarative ModePercentage of total utterances 97.6controlled by the computerAverage number of utterances 15.8between control shifts85.73.3Table 7Differences in average number of utterances between control shifts.Problem Number of Paired Differences Mean Standard Deviation1 5 21.2 24.52 5 17.1 10.83 4 39.8 13.04 4 31.4 12.25 4 16.7 19.56 7 23.3 22.67 5 16.2 6.98 4 31.7 13.0is up," from the first excerpt).
The net effect should be that user task control in declar-ative mode will lead to more frequent linguistic control shifts although the computerwill still have overall control of most utterances.6.5.2 Resulting Comparison.
Table 6 gives the results.
Although the user has linguis-tic control only 14.3% of the time in declarative mode, this is much more often than indirective mode.
Correspondingly, the average number of utterances between controlshifts is reduced by a factor of almost 4.8.
A detailed examination shows that 79%of the 248 control shifts were caused either by the user attempting to correct a com-puter misunderstanding (Section 6.6.2) or by the user initiating a task topic changeby asserting new task information.
These types of control shifts occurred once ev-ery 4.4 user-utterances in declarative mode, but only once every 32.0 user-utterancesin directive mode.
The remaining control shifts were due to requests for repetition ofthe previous utterance or requests for other information.
Table 7 presents the meandifference in the average number of utterances between control shifts for each of thebalanced problems.
Thus, the value 21.2 for problem i means that the difference in theaverage number of utterances between control shifts was greater by 21.2 utterances indirective mode over declarative mode.
These results show that there is a relationshipbetween our notion of task control and the Whittaker and Stenton (1988) notion oflinguistic control evaluated by Walker and Whittaker (1990)--namely, that as usersexploit their task expertise, linguistic control shifts occur much more frequently.
Thisresult may prove useful as a possible cue for when the system needs to release taskinitiative to the user during a mixed-initiative dialogue--as linguistic control shifts be-gin to occur more frequently, it may be an indicator that a user is gaining experienceand can take more overall control of the dialogue.
Further development and testingof this hypothesis are needed.163Computational Linguistics Volume 23, Number 16.6 The Impact of MiscommunicationOne important phenomenon of interactive dialogue that has recently begun to receiveattention in the computational linguistics community is the handling of miscommu-nication (e.g., McRoy and Hirst \[1995\], Brennan and Hulteen \[1995\], and Lambertand Carberry \[1992\]).
In the Circuit Fix-It Shop the computer misunderstood user-utterances 18.5% of the time.
The primary cause of these misunderstandings was themisrecognition of the words spoken by the user--only 50% of the user's utteranceswere correctly recognized word for word.
Consequently, misunderstanding occurredmore often in declarative mode (24.7% of user-utterances) than directive mode (15.0%of user-utterances).
This is due to the fact that, on average, users spoke longer utter-ances in declarative mode.
Speech recognition technology has improved dramaticallysince this system was tested, but the need for handling miscommunication is still rel-evant as users and designers will continually test the performance limits of availabletechnology.
Human-human communication frequently contains miscommunication, sowe should expect it in human-computer dialogue as well.
For the current system, howdid miscommunication impact on the dialogue structure?6.6.1 Frequency of Experimenter Interaction.
As mentioned in Section 4.4, when thecomputer made a serious misinterpretation the experimenter was allowed to tell theuser about the computer's erroneous interpretation without telling the user what todo.
Computer misinterpretation of the user's utterances due to misrecognition ofwords can cause confusion between the user and computer, and ultimately, failureof the dialogue.
With the computer unning in declarative mode, the experimenterchose to make such statements once every 8.5 user-utterances, but only once every26.5 user-utterances in directive mode.
Not all misrecognitions required experimenterinteraction.
156.6.2 User-Initiated Corrections.
The previously mentioned procedure for notifyingthe user of a serious misrecognition leaves the responsibility with the user to try to cor-rect the computer's misunderstanding.
It is hypothesized that when the computer hasyielded the initiative, users are more likely to attempt to redirect he computer's focuswhen an error situation occurs.
Conversely, users will tend to give up trying to redirectthe computer's attention when the computer has the initiative because the machinewill proceed on its own line of reasoning, ignoring what it perceives as user interruptseven when these interrupts are actually attempts at resolving previous miscommuni-cations.
This is borne out by the results.
Overall, while the computer was operating indirective mode, the user attempted to correct only 24% of the misunderstandings forwhich the user received notification.
In contrast, while the computer was operating indeclarative mode, the user attempted to correct 52% of the misunderstandings.15 As reported in Smith and Gordon (1996), there were a total of 250 misunderstandings in declarativemode, 215 for which the experimenter was allowed to notify the user.
The experimenter chose tointervene in 118 of these or 54% of the time.
In contrast, here were a total of 276 misunderstandings indirective mode, 226 for which the experimenter was allowed to notify the user.
In only 69 or 30.5% ofthese misunderstandings did the experimenter notify the user.
The difference in the relative number ofnotifications i largely due to the fact that, in directive mode, the computer frequently ignored thestatements it misunderstood, as the misunderstandings often were in conflict with the computer'scurrent ask goal.
Consequently, it was unnecessary for the experimenter to notify the user about suchmisunderstandings since they would not cause a problem.
On the other hand, confusion betweencomputer and user was much more likely in declarative mode because the computer would morefrequently formulate a response based on its erroneous interpretation f the user's input.
In thesecases, there was a greater need for the experimenter to notify the user of the misunderstanding.164Smith and Gordon Human-Computer Dialogue6.7 Summary of ResultsWhat general conclusions can we draw from this analysis?
Based on the evaluationof the Circuit Fix-It Shop at two different levels of initiative, we have observed thefollowing phenomena:?
Directive mode dialogues tend to follow an orderly pattern consistinglargely of computer-initiated subdialogue transitions, terse userresponses, and predictable subdialogue transitions.
However, theinflexibility of this mode is a severe drawback in the presence ofuser-correctable miscommunications.?
Declarative mode dialogues are shorter but less orderly, consisting ofmore user-initiated subdialogue transitions.
There is evidence that usersare willing to modify their behavior as they gain expertise, provided thecomputer allows it.
The ability to yield the initiative as users gainexperience is essential if a dialogue system is to be useful in practicalapplications involving repeat users.?
The small number of subjects and the design of the experiment make itdifficult to observe differences within a given level of initiative assubjects gain additional expertise.
Nevertheless, in a practicalenvironment we believe the capacity to change initiative during adialogue is essential for obtaining the most effective interaction betweenrepeat users and a system.
It is our conjecture that being able to varyinitiative between dialogues is insufficient, but further study of this issueis needed.After reviewing other empirical studies in the next section, we will address theimpact of these results on future research in Section 8.7.
Recent Empirical Studies Relevant to Human-Computer Mixed-InitiativeDialogue StructureDanieli and Gerbino (1995) also look at dialogues with an implemented computersystem.
This system answers user queries about rain schedules and services.
The fo-cus of the paper is on a few objective and several subjective performance measuresof two interaction strategies similar to the directive and declarative modes describedin this paper.
Their paper concludes that the mode similar to our directive mode ismore robust and more likely to succeed, but the mode similar to our declarative modeis faster and less frustrating to experienced users.
The general performance r sultsobtained uring our testing of the Circuit Fix-It Shop (Section 6.1) lend support otheir claim, as 88% of our attempted dialogues in directive mode were completed suc-cessfully, compared to 80% in declarative mode, and experimenter interaction (of anykind) occurred only once every 18 user-utterances in directive mode, but once every 6user-utterances in declarative mode.
While their dialogue control algorithms are notidentical to ours, their results are complementary, as they show that performance dif-ferences as a function of the computer's level of control may be prevalent in databasequery interactions as well.Guinn (1996) reports on the utility of computer-computer dialogue simulations ofthe Collaborative Algorithm, an extension of our Missing Axiom Theory (Section 3.1)for modeling dialogue processing.
Guinn has implemented the model and run ex-tensive simulations of computer-computer dialogues in order to explore the dynamic165Computational Linguistics Volume 23, Number 1setting of initiative as the dialogue ensues.
The model attaches an initiative level toeach task goal, and a competency evaluation, based on user model information, is usedto decide who should be given the initiative for a given task goal.
There is ongoingwork in implementing and testing the Collaborative Algorithm in human-computerinteractive nvironments.8.
ConclusionsWhile there is ample analysis of dialogue structure based on human-human and sim-ulated human-computer dialogue, there is very little information on the structureof actual human-computer dialogue.
In this paper we have reviewed an integratedapproach to dialogue processing that allows a system to support variable initiativebehavior in its interactions with human users.
Furthermore, we have reported theresults from the analysis of 141 dialogues collected during experimental use with asystem based on the overall dialogue-processing model.
These results indicate differ-ences in both dialogue structure and user behavior as a function of the computer'slevel of initiative.
An important open question is the degree to which the parametersof the transition model for task-oriented ialogues for repair assistance are domaindependent.
For example, the relative amount of time spent in each subdialogue phaseis likely to be highly dependent on the domain.
Furthermore, the model does not fullytake into account different ypes of miscommunication a d their repair.
These issuesrequire further study.The next step in extending the dialogue-processing model is to incorporate theknowledge gained from this study in addressing two of the most significant unre-solved problems in human-computer dialogue: (1) automatic switching of initiativeduring dialogue; and (2) automatic detection and repair of miscommunication.
Thecurrent dialogue-processing model considers ubdialogues at the lower level of ba-sic domain actions.
Extending the model to describe dialogue structure at the moreabstract level of task phases would allow the system to track the excessive and un-usual subdialogue transitions observed in this study.
Such tracking can be used forrecognizing evolving user expertise as well as detecting a lack of mutual understand-ing about the current situation.
Normally, implemented ialogue systems tend to bebased on processing models that are rich in domain information, but are deficient inone or more areas of knowledge about dialogue.
Incorporating more metaknowledgeabout dialogue structure into the model should lead to more human-like performancein handling initiative changes and miscommunication problems.The observations reported in this paper are an initial step on the long road toa comprehensive model of actual human-computer dialogue structure.
It is hopedthat these results will encourage other researchers to construct experimental NL dia-logue systems, test these systems, and then analyze and report the results so that amore comprehensive iew of human-computer dialogue structure can be obtained.
Ingeneral, we believe that the natural life cycle of experimental NL dialogue systemsshould be one of analyzing, modeling, building, and testing so that the analysis ofactual human-computer dialogues can lead to the development of more effective sys-tems.
Such a methodology allows us to gain clearer insight into the evolving natureof human-computer dialogues.AcknowledgmentsThe authors wish to express their thanks toRobert M. Hoekstra, Lynn Eudey, and theanonymous reviewers who advised usconcerning the appropriateness of variousstatistical tests and data displays.
We arefurther grateful to the anonymous reviewersof Computational Linguistics for their helpful166Smith and Gordon Human-Computer Dialoguecomments about previous drafts of thispaper.
A special note of thanks goes toguest editor Marilyn Walker, who providednumerous constructive suggestions aboutthe form of the introductory material.
Otherresearchers who contributed to thedevelopment of the experimental systeminclude Alan W. Biermann, RobertD.
Rodman, Ruth S. Day, D. Richard Hipp,Dania Egedi, and Robin Gambill.
Thewriting of this paper has been supported byNational Science Foundation GrantNSF-IRI-9501571.ReferencesAllen, James E, Alan M. Frisch, and Diane J.Litman.
1982.
ARGOT: The Rochesterdialogue system.
In Proceedings ofthe 2ndNational Conference on Artificial Intelligence,pages 66--70.Allen, James F., Lenhart K. Schubert, GeorgeFerguson, Peter Heeman, Chung HeeHwang, Tsuneaki Kato, Marc Light,Nathaniel Martin, Bradford Miller,Massimo Poesio, and David R. Traum.1995.
The TRAINS project: A case studyin building a conversational planningagent.
Journal of Experimental ndTheoretical Artificial Intelligence, 7:7-48.Biermann, Alan W., Linda Fineman, andJ.
Francis Heidlage.
1992.
A voice- andtouch-driven atural anguage ditor andits performance.
International Journal ofMan-Machine Studies, 37:1-21.Bobrow, D. G., R. M. Kaplan, M. Kay,D.
A. Norman, H. Thompson, andT.
Winograd.
1977.
GUS, a frame drivendialog system.
Artificial Intelligence,8:155-173.Brennan, Susan E. and Eric A. Hulteen.1995.
Interaction and feedback in aspoken language system: A theoreticalframework.
Knowledge-Based Systems,8:143-151.Carberry, Sandra.
1988.
Modeling the user'splans and goals.
Computational Linguistics,14(3):23-37.Dahlb/ick, Nils, Arne JOnsson, and LarsAhrenberg.
1993.
Wizard of Ozstudies--why and how.
Knowledge-BasedSystems, 6(4):258-266.Danieli, Morena and Elisabetta Gerbino.1995.
Metrics for evaluating dialoguestrategies in a spoken language system.
InProceedings ofthe 1995 AAAI SpringSymposium on Empirical Methods inDiscourse Interpretation a d Generation,pages 34-39.Fraser, Norman M. and G. Nigel Gilbert.1991.
Simulating speech systems.Computer Speech and Language, 5:81-99.Frederking, Robert E. 1988.
Integrated NaturalLanguage Dialogue: A Computational Model.Kluwer Academic Publishers, Boston.Grosz, Barbara J.
1978.
Discourse analysis.In D. E. Walker, editor, UnderstandingSpoken Language.
North-Holland, NewYork, pages 235-268.Grosz, Barbara J. and Candace L. Sidner.1986.
Attentions, intentions, and thestructure of discourse.
ComputationalLinguistics, 12(3):175-204.Guinn, Curry I.
1996.
Mechanisms formixed-initiative human-computercollaborative discourse.
In Proceedings ofthe 34th Annual Meeting, pages 278-285.Association for ComputationalLinguistics.Hafner, Carole D. and Kurt Godden.
1985.Portability of syntax and semantics inDatalog.
ACM Transactions on OfficeInformation Systems, pages 141-164, April.Heeman, Peter and James Allen.
1994.Detecting and correcting speech repairs.In Proceedings ofthe 32nd Annual Meeting,pages 295-302.
Association forComputational Linguistics.Hendrix, Gary G., Earl D. Sacerdoti, DanielSagalowicz, and Jonathan Slocum.
1978.Developing a natural anguage interfaceto complex data.
ACM Transactions onDatabase Systems, pages 105-147, June.Hoeppner, W., T. Christaller, H. Marburger,K.
Morik, B. Nebel, M. O'Leary, andW.
Wahlster.
1983.
Beyonddomain-independence: Experience withthe development of a German languageaccess ystem to highly diversebackground systems.
In Proceedings ofthe8th International Joint Conference on ArtificialIntelligence, pages 588-594.Isard, Amy and Jean Carletta.
1995.Replicability of transaction and actioncoding in the map task corpus.
InProceedings ofthe 1995 AAAI SpringSymposium on Empirical Methods inDiscourse Interpretation a d Generation,pages 60-66.Jullien, C. and J. Marty.
1989.
Plan revisionin person-machine dialogue.
InProceedings ofthe Fourth Conference oftheEuropean Chapter of the Assoc&tion forComputational Linguistics, pages 153-160.Kaplan, S. J.
1982.
Cooperative responsesfrom a portable natural anguage querysystem.
Artificial Intelligence, 19(2):165-187.Kitano, Hiroaki and Carol Van Ess-Dykema.1991.
Toward a plan-based understandingmodel for mixed-initiative dialogues.
InProceedings ofthe 29th Annual Meeting,pages 25-32.
Association for167Computational Linguistics Volume 23, Number 1Computational Linguistics.Lambert, Lynn and Sandra Carberry.
1992.Modeling negotiation subdialogues.
InProceedings ofthe 30th Annual Meeting,pages 193-200.
Association forComputational Linguistics.Levine, J. M. 1990.
PRAGMA--A flexiblebidirectional dialogue system.
InProceedings ofthe 8th National Conference onArtif@ial Intelligence, pages 964--969.Litman, Diane J. and James E Allen.
1987.
Aplan recognition model for subdialoguesin conversations.
Cognitive Science,11(2):163-200.McRoy, Susan and Graeme Hirst.
1995.
Therepair of speech act misunderstandingsby abductive inference.
ComputationalLinguistics, 21(4):435-478.Moody, Terry S. 1988.
The Effects of RestrictedVocabulary Size on Voice Interactive DiscourseStructure.
Ph.D. thesis, North CarolinaState University.Oviatt, Sharon.
1995.
Predicting spokendisfluencies during human-computerinteraction.
Computer Speech and Language,9:19-35.Oviatt, Sharon L. and Philip R. Cohen.
1991.Discourse structure and performanceefficiency in interactive andnon-interactive spoken modalities.Computer Speech and Language, 5:297-326.Peckham, Jeremy.
1991.
Speechunderstanding and dialogue over thetelephone: An overview of progress in theSUNDIAL project.
In Proceedings ofthe 2ndEuropean Conference on SpeechCommunication a d Technology, pages1469-1472.Seneff, Stephanie.
1992.
TINA: A naturallanguage system for spoken languageapplications.
Computational Linguistics,8(1):61-86.Smith, Ronnie W. 1991.
A ComputationalModel of Expectation-Driven Mixed-InitiativeDialog Processing.
Ph.D. thesis, DukeUniversity.Smith, Ronnie W. 1992.
Integration ofdomain problem solving with naturallanguage dialog: The missing axiomtheory.
In Proceedings ofApplications orAlX: Knowledge-Based Systems, pages 270-278.Smith, Ronnie W. and Steven A. Gordon.1996.
Pragmatic issues in handlingmiscommunication: Observations of aspoken natural language dialog system.In Proceedings ofthe AAAI '96 Workshop onDetecting, Repairing, and PreventingHuman-Machine Miscommunication, pages21-28.Smith, Ronnie W. and D. Richard Hipp.1994.
Spoken Natural Language DialogSystems: A Practical Approach.
OxfordUniversity Press, New York.Smith, Ronnie W., D. Richard Hipp, andAlan W. Biermann.
1995.
An architecturefor voice dialog systems based onProlog-style theorem-proving.Computational Linguistics, 21(3):281-320.Walker, Marilyn and Steve Whittaker.
1990.Mixed initiative in dialogue: Aninvestigation i to discourse segmentation.In Proceedings ofthe 28th Annual Meeting,pages 70-78.
Association forComputational Linguistics.Waltz, David L. 1978.
An English languagequestion answering system for a largerelational database.
Communications of theACM, pages 526-539, July.Whittaker, Steve and Phil Stenton.
1988.Cues and control in expert-clientdialogues.
In Proceedings ofthe 26th AnnualMeeting, pages 123-130.
Association forComputational Linguistics.Whittaker, Steve and Phil Stenton.
1989.User studies and the design of naturallanguage systems.
In Proceedings oftheFourth Conference ofthe European Chapter ofthe Association for Computational Linguistics,pages 116-123.Wilensky, Robert, David N. Chin, MarcLuria, James Martin, James Mayfield, andDekai Wu.
1988.
The Berkeley UNIXconsultant project.
ComputationalLinguistics, 14(4):35-84.Young, Sheryl R., Alexander G. Hauptmann,Wayne H. Ward, Edward T. Smith, andPhilip Werner.
1989.
High levelknowledge sources in usable speechrecognition systems.
Communications of theACM, pages 183-194, February.Young, S. J. and C. E. Proctor.
1989.
Thedesign and implementation f dialoguecontrol in voice operated atabase inquirysystems.
Computer Speech and Language,3:329-353.168
