Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 904?911,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsWords and Echoes: Assessing and Mitigatingthe Non-Randomness Problem in Word Frequency Distribution ModelingMarco BaroniCIMeC (University of Trento)C.so Bettini 3138068 Rovereto, Italymarco.baroni@unitn.itStefan EvertIKW (University of Osnabru?ck)Albrechtstr.
2849069 Osnabru?ck, Germanystefan.evert@uos.deAbstractFrequency distribution models tuned towords and other linguistic events can pre-dict the number of distinct types and theirfrequency distribution in samples of arbi-trary sizes.
We conduct, for the first time,a rigorous evaluation of these models basedon cross-validation and separation of train-ing and test data.
Our experiments revealthat the prediction accuracy of the modelsis marred by serious overfitting problems,due to violations of the random sampling as-sumption in corpus data.
We then proposea simple pre-processing method to allevi-ate such non-randomness problems.
Furtherevaluation confirms the effectiveness of themethod, which compares favourably to morecomplex correction techniques.1 IntroductionLarge-Number-of-Rare-Events (LNRE) models(Baayen, 2001) are a class of specialized statisticalmodels that allow us to estimate the characteristicsof the distribution of type probabilities in type-richlinguistic populations (such as words) from limitedsamples (our corpora).
They also allow us toextrapolate quantities such as vocabulary size (thenumber of distinct types) and the number of hapaxes(types occurring just once) beyond a given corpus ormake predictions for completely unseen data fromthe same underlying population.LNREmodels have applications in theoretical lin-guistics, e.g.
for comparing the type richness of mor-phological or syntactic processes that are attested todifferent degrees in the data (Baayen, 1992).
Con-sider for example a very common prefix such as re-and a rather rare prefix such as meta-.
With LNREmodels we can answer questions such as: If wecould obtain as many tokens of meta- as we haveof re-, would we also see as many distinct types?In other words, is the prefix meta- as productive asthe prefix re-?
Practical NLP applications, on theother hand, include estimating how many out-of-vocabulary words we will encounter given a lexiconof a certain size, or making informed guesses abouttype counts in very large data sets (e.g., how manytypos are there on the Internet?
)In this paper, after introducing LNRE models(Section 2), we present an evaluation of their per-formance based on separate training and test dataas well as cross-validation (Section 3).
As far aswe know, this is the first time that such a rigorousevaluation has been conducted.
The results showhow evaluating on the training set, a common strat-egy in LNRE research, favours models that overfitthe training data and perform poorly on unseen data.They also confirm the observation by Evert and Ba-roni (2006) that current LNRE models achieve onlyunsatisfactory prediction accuracy, and this is the is-sue we turn to in the second part of the paper (Sec-tion 4).
Having identified the violation of the ran-dom sampling assumption by real-world data as oneof the main factors affecting the quality of the mod-els, we present a new approach to alleviating non-randomness problems.
Further evaluation shows oursolution to outperform Baayen?s (2001) partition-adjustment method, the former state-of-the-art innon-randomness correction.
Section 5 concludes by904pointing out directions for future work.2 LNRE modelsBaayen (2001) introduces a family of models forZipf-like frequency distributions of linguistic pop-ulations, referred to as LNRE models.
Such a lin-guistic population is formally described by a finiteor countably infinite set of types ?i and their occur-rence probabilities pii.
Word frequency models arenot concerned with the probabilities (i.e., relativefrequencies) of specific individual types, but ratherthe overall distribution of these probabilities.Numbering the types in order of decreasing prob-ability (pi1 ?
pi2 ?
pi3 ?
.
.
., called a popula-tion Zipf ranking), we can specify a LNRE modelfor their distribution as a function that computes piifrom the Zipf rank i of ?i.
For instance, the Zipf-Mandelbrot law1 is defined by the equationpii =C(i + b)a(1)with parameters a > 1 and b > 0.
It is mathemati-cally more convenient to formulate LNRE models interms of a type density function g(pi) on the intervalpi ?
[0, 1], such that?
BAg(pi) dpi (2)is the (approximate) number of types ?i with A ?pii ?
B. Evert (2004) shows that Zipf-Mandelbrotcorresponds to a type density of the formg(pi) :={C ?
pi??
?1 A ?
pi ?
B0 otherwise(3)with parameters 0 < ?
< 1 and 0 ?
A < B.2Models that are formulated in terms of such a typedensity g have many direct applications (e.g.
using gas a Bayesian prior), and we refer to them as properLNRE models.Assuming that a corpus of N tokens is a randomsample from such a population, we can make pre-dictions about lexical statistics such as the number1The Zipf-Mandelbrot law is an extension of Zipf?s law(which has a = 1 and b = 0).
While the latter originally refersto type frequencies in a given sample, the Zipf-Mandelbrot lawis formulated for type probabilities in a population.2In this equation, C is a normalizing constant required inorder to ensureR 10 pig(pi) dpi = 1, the equivalent ofPi pii = 1.V (N) of different types in the corpus (the vocab-ulary size), the number V1(N) of hapax legomena(types occurring just once), as well as the further dis-tribution of type frequencies Vm(N).
Since the pre-cise values would be different from sample to sam-ple, the model predictions are given by expectationsE[V (N)] and E[Vm(N)], which can be computedwith relative ease from the type density function g.By comparing expected and observed values of Vand Vm (for the lowest frequency ranks, usually upto m = 15), the parameters of a LNRE model canbe estimated (we refer to this as training the model),allowing inferences about the population (such asthe total number of types in the population) as wellas further applications of the estimated type density(e.g.
for Good-Turing smoothing).
Since we can cal-culate expected values for samples of arbitrary sizeN , we can use the trained model to predict howmany new types would be seen in a larger corpus,how many hapaxes there would be, etc.
This kind ofvocabulary growth extrapolation has become one ofthe most important applications of LNRE models inlinguistics and NLP.A detailed account of the mathematics of LNREmodels can be found in Baayen (2001, Ch.
2).Baayen describes two LNRE models, lognormaland GIGP, as well as several other approaches (in-cluding a version of Zipf?s law and the Yule-Simonmodel) that are not based on a type density andhence do not qualify as proper LNRE models.
TwoLNRE models based on Zipf?s law, ZM and fZM, areintroduced by Evert (2004).In the following, we will only consider properLNRE models because of their considerably greaterutility, and because their performance in extrapo-lation tasks appears to be better than, or at leastcomparable to, the other models (Evert and Baroni,2006).
In addition, we exclude the lognormal modelbecause of its computational complexity and numer-ical instability.3 In initial evaluation experiments,the performance of lognormal was also inferior tothe remaining three models (ZM, fZM and GIGP).Note that ZM is the most simplistic model, with only2 parameters and assuming an infinite populationvocabulary, while fZM and GIGP have 3 parameters3There are no closed form equations for the expectations ofthe lognormal model, which have to be calculated by numericalintegration.905and can model populations of different sizes.3 Evaluation of LNRE modelsLNRE models are traditionally evaluated by look-ing at how well expected values generated by themfit empirical counts extracted from the same data-set used for parameter estimation, often by visualinspection of differences between observed and pre-dicted data in plots.
More rigorously, Baayen (2001)and Evert (2004) compare the frequency distribu-tion observed in the training set to the one predictedby the model with a multivariate chi-squared test.As we will show below, evaluating standard LNREmodels on the same data that were used to estimatetheir parameters favours overfitting, which results inpoor performance on unseen data.Evert and Baroni (2006) attempt, for the first time,to evaluate LNRE models on unseen data.
However,rather than splitting the data into separate trainingand test sets, they evaluate the models in an extra-polation setting, where the parameters of the modelare estimated on a subset of the data used for testing.Evert and Baroni do not attempt to cross-validate theresults, and they do not provide a quantitative evalu-ation, relying instead on visual inspection of empir-ical and observed vocabulary growth curves.3.1 Data and procedureWe ran our experiments with three corpora in differ-ent languages and representing different textual ty-pologies: the British National Corpus (BNC), a ?bal-anced?
corpus of British English of about 100 mil-lion tokens illustrating different communicative set-tings, genres and topics; the deWaC corpus, a Web-crawled corpus of about 1.5 billion German words;and the la Repubblica corpus, an Italian newspapercorpus of about 380 million words.4From each corpus, we extracted 20 non-overlapping samples of randomly selected docu-ments, amounting to a total of 4 million tokens each(punctuation marks and entirely non-alphabetical to-kens were removed before sampling, and all wordswere converted to lowercase).
Each of these sam-ples was then split into a training set of 1 million to-kens (the training size N0) and a test set of 3 million4See www.natcorp.ox.ac.uk, http://wacky.sslmit.unibo.it and http://sslmit.unibo.it/repubblicatokens.
The documents in the la Repubblica sam-ples were ordered chronologically before splitting,to simulate a typical scenario arising when workingwith newspaper data, where the data available fortraining precede, chronologically, the data one wantsto generalize to.We estimate parameters of the ZM, fZM andGIGP models on each training set, using the zipfRtoolkit.5 The models are then used to predict theexpected number of distinct types, i.e., vocabularysize V , at sample sizes of 1, 2 and 3 million tokens,equivalent to 1, 2 and 3 times the size of the trainingset (we refer to these as the prediction sizesN0, 2N0and 3N0, respectively).
Finally, the expected vo-cabulary size E[V (N)] is compared to the observedvalue V (N) in the test set for N = N0, N = 2N0and N = 3N0.
We also look at V1(N), the numberof hapax legomena, in the same way.Our main focus is V prediction, since this is byfar the most useful measure in practical applica-tions, where we are typically interested in knowinghow many types (or how many types belonging toa certain category) we will see as our sample sizeincreases (How many typos are there on the Web?How many types with prefix meta- would we seeif we had as many types of meta- as we have ofre-?)
Hapax legomena counts, on the other hand,play a central role in quantifying morphological pro-ductivity (Baayen, 1992) and they give us a first in-sight into how good the models are at predicting fre-quency distributions, besides vocabulary size (as wewill see, a model?s success in predicting V does notnecessary imply that the model is also capturing theright frequency distribution).For all models, corpora and prediction sizes,goodness-of-fit of the model on the training setis measured with a multivariate chi-squared test(Baayen, 2001, 118-122).
Performance of the mod-els in prediction of V is assessed via relative error,computed for each of the 20 samples from a corpusand the 3 prediction sizes as follows:e =E[V (N)]?
V (N)V (N)where N = k ?
N0 is the prediction size (for k =1, 2, 3), V (N) is the observed V in the relevant test5http://purl.org/stefan.evert/zipfR906set at size N , and E[V (N)] is the corresponding ex-pected V predicted by a model.6For each corpus and prediction size we obtain 20values ei (viz., e1, .
.
.
, e20).
As a summary measure,we report the square root of the mean square relativeerror (rMSE) calculated according to?rMSE =????
120?20?i=1(ei)2This gives us an overall assessment of prediction ac-curacy (we take the square root to obtain values onthe same scale as relative errors, and thus easier tointerpret).
We complement rMSEs with reports onthe average relative error (indicating whether thereis a systematic under- or overestimation bias) and itsasymptotic 95% confidence intervals, based on theempirical standard deviation of the ei across the 20trials (the confidence intervals are usually somewhatlarger than the actual range of values found in theexperiments, so they should be seen as ?pessimisticestimates?
of the actual variance).3.2 ResultsThe panels of Figure 1 report rMSE values for the3 corpora and for each prediction size.
For now,we focus on the first 3 histograms of each panel,that present rMSEs for the 3 LNRE models intro-duced above: ZM, fZM and GIGP (the remaininghistograms will be discussed later).7For all corpora and all extrapolation sizes beyondN0, the simple ZM model outperforms the more so-phisticated fZM and GIGP models (which seem tobe very similar to each other).
Even at the largestprediction size of 3N0, ZM?s rMSE is well below10%, whereas the other models have, in the worstcase (BNC 3N0), a rMSE above 15%.
Figure 2presents plots of average relative error and its em-pirical confidence intervals (again, focus for now onthe ZM, fZM and GIGP results; the rest of the figureis discussed later).
We see that the poor performance6We normalize by V (N) rather than (a function of)E[V (N)] because in the latter case we would favour modelsthat overestimate V , compared to ones that are equally ?close?to the correct value but underestimate V .7A table with the full numerical results is available uponrequest; we find, however, that graphical summaries such asthose presented in this paper make the results easier to interpret.of fZM and GIGP is due to their tendency to under-estimate the true vocabulary size V , while varianceis comparable across models.The rMSEs of V1 prediction are reported in Fig-ure 3.
V1 prediction performance is poorer acrossthe board, and ZM is no longer outperforming theother models.
For space reasons, we do not presentrelative error and variance plots for V1, but the gen-eral trends are the same observed for V , except thatthe bias of ZM towards V1 overestimation is muchclearer than for V .Interestingly, goodness-of-fit on the training datais not a good predictor of V and V1 prediction per-formance on unseen data.
This is shown in Figure4, which plots rMSE for prediction of V againstgoodness-of-fit (quantified by multivariate X2 onthe training set, as discussed above) for all corporaand LNREmodels at the 3N0 prediction size (but thesame patterns emerge at other prediction sizes andwith V1).
The larger X2, the poorer the training setfit; the larger rMSE, the worse the prediction.
Thus,ideally, we should see a positive correlation betweenX2 and rMSE.
Focusing for now on the circles (pin-pointing the ZM, fZM and GIGP models), we seethat there is instead a negative correlation betweengoodness of fit on the training set and quality of pre-diction on unseen data.8First, these results indicate that, if we take good-ness of fit on the training set as a criterion for choos-ing the best model (as done by Baayen and Evert),we end up selecting the worst model for actual pre-diction tasks.
This is, we believe, a very strongcase for applying the split train-test cross-validationmethod used in other areas of statistical NLP to fre-quency distribution modeling.
Second, the data sug-gest that the more sophisticated models are overfit-ting the training set, leading to poorer performancethan the simpler ZM on unseen data.
We turn now towhat we think is the main cause for this overfitting.4 Non-randomness and echoesThe results in the previous section indicate that theV s predicted by LNRE models are at best ?ballparkestimates?
(and V1 predictions, with a relative errorthat is often above 20%, do not even qualify as plau-8With correlation coefficients of r < ?.8, significant at the0.01 level despite the small sample size.907ZM fZM GIGP fZMecho GIGPecho GIGPpartitionN02N03N0rMSE for E[V] vs. V on test set (BNC)rMSE(%)05101520ZM fZM GIGP fZMecho GIGPecho GIGPpartitionN02N03N0rMSE for E[V] vs. V on test set (DEWAC)rMSE(%)05101520ZM fZM GIGP fZMecho GIGPecho GIGPpartitionN02N03N0rMSE for E[V] vs. V on test set (REPUBBLICA)rMSE(%)05101520Figure 1: rMSEs of predicted V on the BNC, deWaC and la Repubblica data-setsZM fZM GIGP fZMecho GIGPecho GIGPpartitionRelative error: E[V] vs. V on test set (BNC)relative error(%)?40?2002040ll ll l ll N02N03N0ZM fZM GIGP fZMecho GIGPecho GIGPpartitionRelative error: E[V] vs. V on test set (DEWAC)relative error(%)?20?1001020ll ll lll N02N03N0ZM fZM GIGP fZMecho GIGPecho GIGPpartitionRelative error: E[V] vs. V on test set (REPUBBLICA)relative error(%)?20?1001020ll ll lll N02N03N0Figure 2: Average relative errors and asymptotic 95% confidence intervals of V prediction on BNC, deWaCand la Repubblica data-setsZM fZM GIGP fZMecho GIGPecho GIGPpartitionN02N03N0rMSE for E[V1] vs. V1 on test set (BNC)rMSE(%)01020304050ZM fZM GIGP fZMecho GIGPecho GIGPpartitionN02N03N0rMSE for E[V1] vs. V1 on test set (DEWAC)rMSE(%)01020304050ZM fZM GIGP fZMecho GIGPecho GIGPpartitionN02N03N0rMSE for E[V1] vs. V1 on test set (REPUBBLICA)rMSE(%)01020304050Figure 3: rMSEs of predicted V1 on the BNC, deWaC and la Repubblica data-sets9080 5000 10000 15000051015Accuracy for V on test set (3N0)X2rMSE(%)llllllllll standardechomodelpartition?adjustedFigure 4: Correlation between X2 and V predictionrMSE across corpora and modelssible ballpark estimates).
Although such rough esti-mates might be more than adequate for many practi-cal applications, is it possible to further improve thequality of LNRE predictions?A major factor hampering prediction quality isthat real texts massively violate the randomness as-sumption made in LNRE modeling: words, ratherobviously, are not picked at random on the basisof their population probability (Evert and Baroni,2006; Baayen, 2001).
The topic-driven ?clumpi-ness?
of low frequency content words reduces thenumber of hapax legomena and other rare eventsused to estimate the parameters of LNRE models,leading the models to underestimate the type rich-ness of the population.
Interestingly (but unsurpris-ingly), ZM with its assumption of an infinite pop-ulation, is less prone to this effect, and thus it hasa better prediction performance than the more so-phisticated fZM and GIGP models, despite its poorgoodness-of-fit.The effect of non-randomness is illustrated veryclearly for the BNC (but the same could be shownfor the other corpora) by Figure 5, a comparisonof rMSE for prediction of V from our experimentsabove to results obtained on versions of the BNCsamples with words scrambled in random order,thus forcibly removing non-randomness effects.
Wesee from this figure that the performance of bothfZM and GIGP improves dramatically when theyare trained and tested on randomized sequences ofwords.
Interestingly, randomization has instead anegative effect on ZM performance.ZM fZM GIGP ZMrandom fZMrandom GIGPrandomN02N03N0rMSE for E[V] vs. V on test set (BNC)rMSE(%)05101520Figure 5: rMSEs of predicted V on unmodifiedvs.
randomized versions of the BNC sets4.1 Previous approaches to non-randomnessWhile non-randomness is widely acknowledged asa serious problem for the statistical analysis of cor-pus data, very few authors have suggested correc-tion strategies.
The key problem of non-random dataseems to be that the occurrence frequencies of a typein different documents do not follow the binomialdistribution assumed by random sampling models.One approach is therefore to model this distribu-tion explicitly, replacing the binomial with its sin-gle parameter pi by a more complex distribution thathas additional parameters (Church and Gale, 1995;Katz, 1996).
However, these distributions are cur-rently not applicable to LNRE modeling, which isbased on the overall frequencies of types in a cor-pus rather than their frequencies in individual doc-uments.
The overall frequencies can only be calcu-lated by summation over all documents in the cor-pus, resulting in a mathematically and numericallyintractable model.
In addition, the type density g(pi)would have to be extended to a multi-dimensionalfunction, requiring a large number of parameters tobe estimated from the data.Baayen (2001) suggests a different approach,which partitions the population into ?normal?
typesthat satisfy the random sampling assumption, and?totally underdispersed?
types, which are assumedto concentrate all occurrences in the corpus into a909single ?burst?.
Using a standard LNRE model forthe normal part of the population and a simple lin-ear growth model for the underdispersed part, ad-justed values for E[V ] and E[Vm] can easily be cal-culated.
These so-called partition-adjusted models(which introduce one additional parameter) are thusthe only viable models for non-randomness correc-tion in LNRE modeling and have to be consideredthe state of the art.4.2 Echo adjustmentRather than making more complex assumptionsabout the population distribution or the samplingmodel, we propose that non-randomness should betackled as a pre-processing problem.
The issue, weargue, is really with the way we count occurrencesof types.
The fact that a rare topic-specific word oc-curs, say, four times in a single document does notmake it any less a hapax legomenon for our purposesthan if the word occurred once (this is the case, forexample, of the word chondritic in the BNC, whichoccurs 4 times, all in the same scientific document).We operationalize our intuition by proposing that,for our purposes, each content word (at least eachrare, topic-specific content word) occurs maximallyonce in a document, and all other instances of thatword in the document are really instances of a spe-cial ?anaphoric?
type, whose function is that of?echoing?
the content words in the document.
Thus,in the BNC document mentioned above, the wordchondritic is counted only once, whereas the otherthree occurrences are considered as tokens of theecho type.
Thus, we are counting what in the in-formation retrieval literature is known as documentfrequencies.
Intuitively, these are less susceptible totopical clumpiness effects than plain token frequen-cies.
However, by replacing repeated words withecho tokens, we can stick to a sampling model basedon random word token sampling (rather than docu-ment sampling), so that the LNRE models can beapplied ?as is?
to echo-adjusted corpora.Echo-adjustment does not affect the sample sizeN nor the vocabulary size V , making the interpre-tation of results obtained with echo-adjusted mod-els entirely straightforward.
N does not change be-cause repeated types are replaced with echo tokens,not deleted.
V does not change because only re-peated types are replaced.
Thus, no type present inthe original corpus disappears (more precisely, V in-creases by 1 because of the addition of the echo type,but given the large size of V this can be ignored forall practical purposes).
Thus, the expected V com-puted for a specified sample size N with a modeltrained on an echo-adjusted corpus can be directlycompared to observed values at N , and to predic-tions made for the same N by models trained on anunprocessed corpus.
The same is not true for the pre-diction of the frequency distribution, where, for thesame N , echo-based models predict the distributionof document frequencies.We are proposing echoes as a model for the us-age of (rare) content words.
It would be diffi-cult to decide where the boundary is between top-ical words that are inserted once in a discourseand then anaphorically modulated and ?general-purpose?
words that constitute the frame of the dis-course and can occur multiple times.
Luckily, wedo not have to make this decision when estimatinga LNRE model, since model fitting is based on thedistribution of the lowest frequencies.
For example,with the default zipfR model fitting setting, only thelowest 15 spectrum elements are used to fit the mod-els.
For any reasonably sized corpus, it is unlikelythat function words and common content words willoccur in less than 16 documents, and thus their dis-tribution will be irrelevant for model fitting.
Thus,we can ignore the issue of what is the boundary be-tween topical words to be echo-adjusted and generalwords, as long as we can be confident that the setof lowest frequency words used for model fitting be-long to the topical set.9 This makes practical echo-adjustment extremely simple, since all we have todo is to replace all repetitions of a word in the samedocument with echo tokens, and estimate the param-eters of a plain LNRE model with the resulting ver-sion of the training corpus.4.3 Experiments with echo adjustmentUsing the same training and test sets as in Sec-tion 3.1, we train the partition-adjusted GIGP model9The issue becomes more delicate if we want to predictthe frequency spectrum rather than V , since a model trainedon echo-adjusted data will predict echo-adjusted frequenciesacross the board.
However, in many theoretical and practicalsettings only the lowest frequency spectrum elements are of in-terest, where, again, it is safe to assume that words are highlytopic-dependent, and echo-adjustment is appropriate.910implemented in the LEXSTATS toolkit (Baayen,2001).
We estimate the parameters of echo-adjustedZM, fZM and GIGP models on versions of the train-ing corpora that have been pre-processed as de-scribed above.
The performance of the models isevaluated with the same measures as in Section 3.1(for prediction of V1, echo-adjusted versions of thetest data are used).Figure 1 reports the performance of the echo-adjusted fZM and GIGP models and of partition-adjusted GIGP (echo-adjusted ZM performed sys-tematically much worse than the other echo-adjustedmodels and typically worse than uncorrected ZM,and it is not reported in the figure).
Both correctionmethods lead to a dramatic improvement, bringingthe prediction performance of fZM and GIGP to lev-els comparable to ZM (with the latter outperformingthe corrected models on the BNC, but being outper-formed on la Repubblica).
Moreover, echo-adjustedGIGP is as good as partitioned GIGP on la Repub-blica, and better on both BNC and deWaC, suggest-ing that the much simpler echo-adjustment methodis at least as good and probably better than Baayen?spartitioning.
The mean error and confidence intervalplots in Figure 2 show that the echo-adjusted modelshave a much weaker underestimation bias than thecorresponding unadjusted models, and are compara-ble to, if not better than, ZM (although they mighthave a tendency to display more variance, as clearlyillustrated by the performance of echo-adjusted fZMon la Repubblica at 3N0 prediction size).
Finally,the echo-adjusted models clearly stand out with re-spect to ZM when it comes to V1 prediction (Fig-ure 3), indicating that echo-adjusted versions of themore sophisticated fZM and GIGP models shouldbe the focus of future work on improving predic-tion of the full frequency distribution, rather thanplain ZM.
Moreover, echo-adjusted GIGP is outper-forming partitioned GIGP, and emerging as the bestmodel overall.10 Reassuringly, for the echoed mod-els there is a very strong positive correlation betweengoodness-of-fit on the training set and quality of pre-diction, as illustrated for V prediction at 3N0 bythe triangles in Figure 4 (again, the patterns in this10In looking at the V1 data, it must be kept in mind, how-ever, that V1 has a different interpretation when predicted byecho-adjusted models, i.e., it is the number of document-basedhapaxes, the number of types that occur in one document only.figure represent the general trend for echo-adjustedmodels found in all settings).11 This indicates thatthe over-fitting problem has been resolved, and forecho-adjusted models goodness-of-fit on the train-ing set is a reliable indicator of prediction accuracy.5 ConclusionDespite the encouraging results we reported, muchwork, of course, remains to be done.
Even withthe echo-adjusted models, prediction of V1 suffersfrom large errors and prediction of V quickly deteri-orates with increasing prediction sizeN .
If the mod-els?
estimates for 3 times the size of the training sethave acceptable errors of around 5%, for many ap-plications we might want to extrapolate to 100N0 ormore (recall the example of estimating type countsfor the entire Web).
Moreover, echo-adjusted mod-els make predictions pertaining to the distribution ofdocument frequencies, rather than plain token fre-quencies.
The full implications of this remain tobe investigated.
Finally, future work should system-atically explore to what extent different textual ty-pologies are affected by the non-randomness prob-lem (notice, e.g., that non-randomness seems to be agreater problem for the BNC than for the more uni-form la Repubblica corpus).ReferencesBaayen, Harald.
1992.
Quantitative aspects of morpho-logical productivity.
Yearbook of Morphology 1991,109-150.Baayen, Harald.
2001.
Word frequency distributions.Dordrecht: Kluwer.Church, KennethW.
andWilliam A. Gale.
1995.
Poissonmixtures.
Journal of Natural Language Engineering1, 163-190.Evert, Stefan.
2004.
A simple LNRE model for randomcharacter sequences.
Proceedings of JADT 2004, 411-422.Evert, Stefan and Marco Baroni.
2006.
Testing the ex-trapolation quality of word frequency models.
Pro-ceedings of Corpus Linguistics 2005.Katz, Slava M. 1996.
Distribution of content words andphrases in text and language modeling.
Natural Lan-guage Engineering, 2(2) 15-59.11With significant correlation coefficients of r = .76 for 2N0(p < 0.05) and r = .94 for 3N0 (p 0.01).911
