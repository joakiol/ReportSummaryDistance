Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 270?280,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsHubness and Pollution:Delving into Cross-Space Mapping for Zero-Shot LearningAngeliki Lazaridou Georgiana Dinu Marco BaroniCenter for Mind/Brain SciencesUniversity of Trento{angeliki.lazaridou|georgiana.dinu|marco.baroni}@unitn.itAbstractZero-shot methods in language, vision andother domains rely on a cross-space map-ping function that projects vectors fromthe relevant feature space (e.g., visual-feature-based image representations) to alarge semantic word space (induced inan unsupervised way from corpus data),where the entities of interest (e.g., objectsimages depict) are labeled with the wordsassociated to the nearest neighbours of themapped vectors.
Zero-shot cross-spacemapping methods hold great promise as away to scale up annotation tasks well be-yond the labels in the training data (e.g.,recognizing objects that were never seenin training).
However, the current perfor-mance of cross-space mapping functionsis still quite low, so that the strategy isnot yet usable in practical applications.In this paper, we explore some generalproperties, both theoretical and empirical,of the cross-space mapping function, andwe build on them to propose better meth-ods to estimate it.
In this way, we attainlarge improvements over the state of theart, both in cross-linguistic (word trans-lation) and cross-modal (image labeling)zero-shot experiments.1 IntroductionIn many supervised problems, the parameters ofa classification function are estimated on (x, y)pairs, where x is a vector representing a traininginstance in some feature space, and y is the labelassigned to the instance.
For example, in imagelabeling x contains visual features extracted froma picture and y is the name of the object depictedin the picture (Grauman and Leibe, 2011).
Sinceeach label is treated as an unanalyzed primitive,this approach requires ad-hoc annotation for eachlabel of interest, and it will not scale up to chal-lenges where the potential label set is vast (for ex-ample, bilingual dictionary induction, where thelabel set corresponds to the full vocabulary of thetarget language).Zero-shot methods (Palatucci et al, 2009) ad-dress the scalability problem by building on theobservation that the labels of interest are oftenwords (or longer linguistic expressions), whichstand in a semantic similarity relation to eachother.
Moreover, distributional approaches allowus to estimate very large semantic word spacesin an efficient and unsupervised manner, usingjust unannotated text corpora as input (Turney andPantel, 2010).
Extensive evidence has shown thatthe similarity estimates obtained by representingwords as vectors in such corpus-induced seman-tic spaces are extremely accurate (Baroni et al,2014).
Under the assumption that the domain ofinterest (e.g., objects in pictures, words in a sourcelanguage) exhibits comparable similarity structureto that manifested in language, we can rephrase thelearning task, from inducing multiple functionsfrom the source feature space onto independentatomic labels, to that of estimating a single cross-space mapping function from vectors in the sourcefeature space onto vectors for the correspondingword labels in distributional semantic space.
Theinduced function can then also be applied to adata-point whose label was not used for training.The word corresponding to the nearest neighbourof the mapped vector in the latter space is usedas the label of the data point.
Zero-shot learn-ing using distributional semantic spaces was origi-nally proposed for brain signal decoding (Mitchellet al, 2008), but it has since been extensively ap-plied in other domains, including image labeling(Frome et al, 2013; Lazaridou et al, 2014; Socheret al, 2013) and bilingual dictionary/phrase tableinduction (Dinu and Baroni, 2014; Mikolov et al,2702013a), the two applications we focus on here.Effective zero-shot learning by cross-spacemapping could get us through the manual anno-tation bottleneck that hampers many applications.However, in practice, the accuracy in label re-trieval with current mapping methods is still toolow for practical uses.
In image labeling, whena search space of realistic size is considered, ac-curacy is just above 1% (which is still well abovechance for large search spaces).
In bilingual lex-icon induction, accuracy reaches values around30% (across words of varying frequency), whichare definitely more encouraging, but still indicatethat only 1 word in 3 will be translated correctly.In this article, we look at some general prop-erties of the linear cross-modal mapping functionstandardly used for zero-shot learning, in orderto achieve a better understanding of its shortcom-ings, and improve its quality by devising meth-ods to overcome them.
First, when the mappingfunction is estimated with least-squares error tech-niques, we observe a systematic increase in hub-ness (Radovanovi?c et al, 2010b), that is, in thetendency of some vectors (?hubs?)
to appear in thetop neighbour lists of many test items.
We connecthubness to least-squares estimation, and we showhow it is greatly mitigated when the mapping func-tion is estimated with a max-margin ranking lossinstead.
Still, switching to max-margin greatlyimproves accuracy in the cross-linguistic context,but not for vision-to-language mapping.
In thecross-modal setting, we observe indeed a differ-ent problem, that we name (training instance) pol-lution: The neighbourhoods of mapped test itemsare ?polluted?
by the target vectors used in train-ing.
This suggests that cross-modal mappingsuffers from overfitting issues, and consequentlyfrom poor generalization power.
Taking inspi-ration from domain adaptation, which addressessimilar generalization concerns, and self-learning,we propose a technique to augment the trainingdata with automatically constructed examples thatforce the function to generalize better.
Havingshown the advantages of a ranking loss, our fi-nal contribution is the adaptation of some insightsfrom the max-margin literature to our setting, inparticular concerning the choice of negative ex-amples.
This leads to further accuracy improve-ments.
We thus conclude the paper by reportingzero-shot performances in both cross-modal andcross-language settings that are well above the cur-cross-linguistic cross-modalformer state of art 33.0 0.5standard mapping 29.7 1.1max-margin - ?3 39.4 1.9data augmentation - ?4 NA 3.7negative evidence - ?5 40.2 5.6Table 1: Roadmap.
Proposed changes to cross-space mapping training and resulting percentagePrecision @1 in our two experimental setups.rent state of the art.
Table 1 provides a roadmapand summary of our results.2 Experimental SetupCross-linguistic experiments In the cross-linguistic experiments, we learn a mapping fromthe semantic space of language A to the semanticspace of language B, which can then be used fortranslating words outside the training set.
Specifi-cally, given the vector representation of a word inlanguage A, we apply the mapping to obtain anestimate of the vector representation of its mean-ing in language B, returning the nearest neigh-bour of the mapped vector in the B space as can-didate translation.
We focus on translating fromEnglish to Italian and adopt the setup (word vec-tors, training and test data) of Dinu et al (2015).For a set of 200K words, 300-dimensional vectorswere built using the word2vec toolkit,1choosingthe CBOW method.2CBOW, which learns to pre-dict a target word from the ones surrounding it,produces state-of-the-art results in many linguis-tic tasks (Baroni et al, 2014).
The word vectorswere induced from corpora of 2.8 and 1.6 billiontokens, respectively, for English and Italian.3Thetrain and test English-to-Italian translation pairswere extracted from a Europarl-derived dictionary(Tiedemann, 2012).4The 5K most frequent trans-lation pairs were used for training, while the testset includes 1.5K English words equally split into5 frequency bins.
The search for the correct trans-lation is performed in a semantic space of 200K1https://code.google.com/p/word2vec/2Other hyperparameters, which we adopted without fur-ther tuning, include a context window size of 5 words toeither side of the target, setting the sub-sampling option to1e-05 and estimating the probability of target words by neg-ative sampling, drawing 10 samples from the noise distribu-tion (Mikolov et al, 2013b).3Corpus sources: http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk4http://opus.lingfil.uu.se/271Italian words.5Cross-modal experiments In the cross-modalexperiments, we induce a mapping from visualto linguistic space.
Specifically, given an image,we apply the mapping to its visual vector repre-sentation to obtain an estimate of its representa-tion in linguistic space, where the word associatedto the nearest neighbour is retrieved as the imagelabel.
Similarly to translation pairs in the cross-linguistic setup, we create a list of ?visual transla-tion?
pairs between images and their correspond-ing noun labels.
Our starting point are the 5.1Klabels in ImageNet (Deng et al, 2009) that oc-cur at least 500 times in our English corpus andhave concreteness score ?5, according to Turneyet al (2011).
For each label, we sample 100 pic-tures from its ImageNet entry, and associate eachpicture with the 4094-dimensional layer (fc7) atthe top of the pre-trained convolutional neural net-work model of Krizhevsky et al (2012), using theCaffe toolkit (Jia et al, 2014).
The target wordspace is identical to the English space used in thecross-linguistic experiment.
Finally, we use 75%of the labels (and the respective images) for train-ing and the remaining 25% of the labels for test-ing.6From the 127.5K images corresponding totest labels, we sample 1K images as our test set.For zero-shot evaluation purposes, the search forthe correct label is performed in the space of 5.1Kpossible labels, unless otherwise specified.
How-ever, when quantifying hubness and pollution, inorder to have a setting comparable to that of cross-language mapping, we use the full set of 200K En-glish words as search space.Learning objectives We assume that we havecross-space ?translation?
pairs available for a setof |Tr| items (xi,yi) = {xi?
Rd1,yi?
Rd2}.Moreover, following previous work, we assumethat the mapping function is linear.
For estimat-ing its parameters W ?
Rd1?d2, we consider twoobjectives.
The first is L2-penalized least squares5Faithful to the zero-shot setup, in our experiments thereis never any overlap between train and test words; however,to make the task more challenging, we include the train wordsin the search space, except where expressly indicated.6At training time, we average the 100 vectors associatedto a label into a single representation, to reduce training setsize while minimizing information loss.
At test time, as nor-mally done, we present the model with single image visualvectors.
(ridge):?W = argminW?Rd1?d2?XW ?Y?+ ?
?W?,which has an analytical solution.The second objective is a margin-based rank-ing loss (max-margin) similar in spirit to the oneused in similar cross-modal experiments with WS-ABIE (Weston et al, 2011) and DeViSE (Fromeet al, 2013).
The loss for a given pair of train-ing items (xi,yi) and the corresponding mapping-based prediction y?i= Wxiis defined ask?j 6=imax{0, ?
+ dist(y?i,yi)?
dist(y?i,yj)},where dist is a distance measure, in our case theinverse cosine, and ?
and k are tunable hyperpa-rameters denoting the margin and the number ofnegative examples, respectively.
Intuitively, thegoal of the max-margin objective is to rank thecorrect translation yiof xihigher than any otherpossible translation yj.
In theory, the summationin the equation could range over all possible la-bels, but in practice this is too expensive (e.g., inthe cross-linguistic experiments the search spacecontains 200K candidate labels!
), and it is usuallycomputed over just a portion of the label space.In Weston et al (2011), the authors propose anefficient way of selecting negative examples, inwhich they randomly sample, for each trainingitem, labels from the complete set, and pick asnegative sample the first label violating the mar-gin.
This guarantees that there will be exactly asmany weight updates as training items.
Anotherpossibility is proposed in Mikolov et al (2013b),where negative samples are picked from a non-item specific distribution (e.g., the uniform distri-bution).7For the experiments in Sections 3 and4, we follow a more general setup in which thesize of the margin and number of negative sam-ples is tuned for each task.
In this way, for asufficiently large margin and number of negativesamples, we increase the probability of perform-ing a weight update per training item.
We estimatethe mapping parameters W with stochastic gradi-ent descent and per-parameter learning rates tunedwith Adagrad (Duchi et al, 2011).
The tuning ofhyperparameters ?
and k is performed on a ran-dom 25% subset of the training data.7The notion of negative samples is not unique to margin-based learning; in Mikolov et al (2013b), the authors used itto efficiently estimate a word probability distribution.2720 10 20 30 40 5000.0010.0020.0030.0040.0050.0060.0070.0080.0090.01 Hubness in Cross?lingual ExperimentN20 valuesPr(N20)ridgemax?margingold5 10 15 20 25 30 35 4000.0010.0020.0030.0040.0050.0060.0070.0080.0090.01 Hubness in Cross?modal ExperimentN20 valuesPr(N20)ridgemax?margingoldFigure 1: Hubness distribution in cross-linguistic (left) and cross-modal (right) search spaces.
Thehubness score (N20) is computed on the top-20 neighbour lists of the test items, using their original(gold), ridge- or max-margin-mapped vectors as query terms.3 HubnessHigh-dimensional spaces are often affected byhubness (Radovanovi?c et al, 2010b; Radovanovi?cet al, 2010a), that is, they contain certain ele-ments ?
hubs ?
that are near many other pointsin space without being similar to the latter in anymeaningful way.
As recently noted by Dinu etal.
(2015), the hubness problem is greatly exacer-bated when one looks at the nearest neighbours ofvectors that have been mapped across spaces withridge.8Given a set of query vectors with the cor-responding top-k nearest neighbour lists, we canquantify the degree of hubness of an item in thesearch space (parameterized by k) by the numberof lists in which it occurs.
Nk(y), the hubness at kof an item y, is computed as follows:Nk(y) = |{x ?
T|y ?
NNk(x, S)}|,where S denotes the search space, T denotes theset of query items and NNk(x,S) denotes the knearest neighbors of x in S.Figure 1 reports N20distributions across thecross-linguistic and cross-modal search spaces,using the respective test items as query vectors.The blue line shows the distributions for the?gold?
vectors (that is, the vectors in the targetspace we would like to approximate).
The red lineshows the same distributions when neighbours are8Dinu et al (2015) observe, but do not attempt to under-stand hubness, as we do here.
They propose to address it withmethods to re-rank neighbour lists, which are less general andshould be largely complementary to our effort to improve es-timation of the cross-mapping function.Cross-linguistic Cross-modalblockmonthon (50) smilodon (40)hashim (28) pintle (33)akayev (27) knurled (27)autogiustificazione (27) handwheel (24)limassol (26) circlip (23)regulars (26) black-footed (23)18 (25) flatbread (22)Table 2: Top ridge hubs, together with N20scores.
Note that cross-linguistic hubs are sup-posed to be Italian words.queried for the ridge-mapped test vectors (ignoreblack lines for now).
In both spaces, when thequery vectors are mapped, hubness increases dra-matically.
The largest hubs for the original testitems occur in 15 neighbour lists or less.
Withthe mapped vectors, we find hubs occurring in40 lists or more.
The figure also shows that, inboth spaces, we observe more points with smallerbut non-negligible N20(e.g., around 10) whenmapped vectors are queried.
In both spaces, thedifference in hubness is very significant accordingto a cross-tab test (p<10?30).
Finally, as Table 2shows, the largest hubs are by no means terms thatwe might expect to occur as neighbours of manyother items on semantic grounds (e.g., very gen-eral terms), but rather very specific and rare wordswhose high hubness cannot possibly be a genuinesemantic property.Causes of hubness Why should the mappingfunction lead to an increase in hubness?
We con-jecture that this is due to an intrinsic property ofleast-squares estimation.
Given the training ma-273trices X and Y, and the projection matrix W ob-tained by minimizing squared error, each columny?
?,iof?Y = XW is the orthogonal projection ofy?,i, the corresponding Y column onto the col-umn space of X (Strang, 2003, Ch.
4).
Conse-quently, y?,i= i+ y?
?,i, where the ierror vectoris orthogonal to y??,i.
It follows that ||y?,i||2?=||y??,i||2.
Since y?,iand y?
?,ihave equal means (be-cause the error terms in imust sum to 0), it imme-diately follows from the squared length inequalitythat y?
?,ihas lower or equal variance to y?,i.
Sincethis holds for all columns of?Y, it follows in turnthat the set of mapped vectors in?Y has lower orequal variance to the corresponding set of origi-nal vectors in Y.
Coming back to hubness, a setof lower variance points (such as the mapped vec-tors) will result in higher hubness since the pointswill on average be closer to each other.
The prob-lem is likely to be further exacerbated by the prop-erty of least-squares to ignore relative distancesbetween points (the objective only aims at mak-ing predicted and observed vectors look like eachother),Strictly, the theoretical result only holds for thetraining points.
However, to the extent that thetraining set is representative of what will be en-countered in the test set, it should also extendto test data (and if training and testing data arevery different, the mapping function will gener-alize very poorly anyway).
Moreover, the resultholds for a pure least-squares solution, without theridge L2 regularization term.
Whether it also ap-plies to ridge-based estimates will depend on therelative impact of the least-squares and L2 termson the final solution (and it is not excluded thatthe L2 term might also independently reduce vari-ance, of course).
Empirically, we find that, in-deed, lower variance also characterizes test vectorsmapped with a ridge-estimated function.Interestingly, in the literature on cross-spacemapping we find that authors choose a differentcost function than ridge, without motivating thechoice.
Socher et al (2014) mention in pass-ing that max-margin outperforms a least-squared-error cost for cross-modal mapping.Max-margin as a solution to hubness Re-ferring back to Figure 1, we see that whenridge estimation is replaced by max-margin (blackline), there is a considerable decrease in hub-ness in both settings.
This is directly reflectedin a large increase in performance in our cross-linguistic (English-to-Italian) zero-shot task (lefttwo columns of Table 3), with the largest im-provement for the all important P@1 measure(equivalent to accuracy).9These results are wellabove the current best cross-language accuracy forcross-modal mapping without added orthographiccues (33%), attained by Mikolov et al (2013a).10The absolute performance figures are low in thechallenging cross-modal setting, but here too weobserve a considerable improvement in accuracywhen max-margin is applied.
Indeed, we are al-ready above the cross-modal zero-shot mappingstate of the art for a search space of similar size(0.5% accuracy in Frome et al (2013)).
Still, theimprovement over ridge (while present) is not aslarge for the less strict (higher ranks) performancescores.Table 4 confirms that the improvement broughtabout by max-margin is indeed (at least partially)due to hubness reduction.
A large proportionof vectors retrieved as top-1 predictions (trans-lations/labels) are hubs when mapping is trainedwith ridge, but the proportion drops dramaticallywith max-margin.
Still, more than 1/5 top predic-tions for cross-modal mapping with max-marginare hubs (vs. less than 1/10 for the original vec-tors).
Now, the mathematical properties we re-viewed above suggest that, for least-squares es-timation, hubness is caused by general reducedvariance of the space after mapping.
Thus, hubsshould be vectors that are near the mean of thespace.
The first row of Table 5 confirms thatthe hubs found in the neighbourhoods of ridge-mapped query terms are items that tend to becloser to the search space mean vector, and thatthis effect is radically reduced with max-marginestimation.
However, the second row of the tableshows another factor at play, that has a major rolein the cross-modal setting, and it is only partiallyaddressed by max-margin estimation: Namely, invision-to-language mapping, there is a strong ten-dency for hubs (that, recall, have an important ef-fect on performance, as they enter many nearestneighbour lists) to be close to a training data point.9We have no realistic upper-bound estimate, but due todifferent word senses, synonymy, etc., it is certainly not100%.10Although the numbers are not fully comparable becauseof different language pairs and various methodological de-tails, their method is essentially equivalent to our ridge ap-proach we are clearly outperforming.274Cross-linguistic Cross-modalridge max-margin ridge max-marginP@1 29.7 38.4 1.1 1.9P@5 44.2 54.2 4.8 5.4P@10 49.1 60.4 7.9 9.0Table 3: Ridge vs. max-margin in zero-shot experiments.
Precision @N results cross-linguistically (test items: 1.5K, search space:200K) and cross-modally (test items: 1K, searchspace: 5.1K).Cross-linguistic Cross-modalridge max-margin gold ridge max-margin gold19.6 9.8 0.6 55.8 21.6 7.8Table 4: Hubs as top predictions.
Percentage oftop-1 neighbours of test vectors in zero-shot ex-periments of Table 3 with N20> 5.Cross-linguistic Cross-modalcosine with ridge max-margin ridge max-marginfull-space mean 0.21 0.06 0.13 -0.01training point 0.15 0.12 0.34 0.24Table 5: Properties of hubs.
Spearman ?
ofN20scores with cosines to mean vector of fullsearch space (top) and nearest training item (bot-tom), across all search space elements.
All corre-lations significant (p<0.001) except cross-modalmax-margin hubness/full-space mean.4 PollutionThe quantitative results and post-hoc analysis ofhubs in Section 3 suggest that cross-modal map-ping is facing a serious generalization problem.
Toget a better grasp of the phenomenon, we define abinary measure of (training data) pollution for aqueried item x and parameterized by k, such thatpollution is 1 if x has a (target) training item yamong its k nearest neighbours, 0 otherwise.
For-mally:Npolk,S(x) = [[?y ?
YTr: y ?
NNk,S(x)]],where YTris the matrix of target vectors used intraining, NNk,S(y) denotes the top k neighbors ofy in search space S, and [[z]] is an indicator func-tion.1111Pollution is of course an effect of overfitting, but we usethis more specific term to refer to the tendency of trainingvectors to ?pollute?
nearest neighbour lists of mapped vec-tors.The average pollution Npol1,Sof all test items inthe cross-modal experiment, when |S|=200K is18%, which indicates that in 1/5 of cases the re-turned label is that of a training point.
The equiv-alent statistic in the cross-linguistic experimentdrops to 8.7% (words tend to be more varied thanthe set of concrete, imageable concepts used forimage annotation tasks, and so the cross-linguistictraining set is probably less uniform than the oneused in the vision-to-language setting).The real extent of the generalization problemin the cross-modal setup becomes more obviousif we restrict the search space to labels effectivelyassociated to an image in our data set (|S|=5.1K).In this case, the average pollution Npol1,Sacross alltest items jumps to 88%, that is, the vast major-ity of test images are annotated with a label com-ing from the training data.
Clearly, there is a seri-ous problem of overfitting to the training subspace.While we came to this observation by inspectingthe properties of hubs, other work in zero-shotfor image labeling has indirectly noted the same.Frome et al (2013) empirically showed that theperformance of the system is higher when remov-ing training labels from the search space, whileNorouzi et al (2014) proposed a zero-shot methodthat avoids explicit cross-modal mapping.Adapting to the full search space by dataaugmentation High training-data pollution in-dicates that cross-modal mapping does not gener-alize well beyond the kind of data points it encoun-tered in learning.
This is a special case of the data-set bias problem (Torralba and Efros, 2011) and,given that the latter has been addressed as a do-main adaptation problem (Gong et al, 2012; Don-ahue et al, 2013), we adopt here a similar view.Self-training has been successfully used for do-main adaptation in NLP, e.g., in syntactic parsing.Given the limited amount of syntactically anno-tated data coming from monotonous sources (e.g.,the Wall Street Journal), parsers show a big dropin performance when applied to different domains(e.g., reviews), since training and test domains dif-fer dramatically, thus affecting their generalizationperformance.
In a nutshell, the idea behind self-training (McClosky et al, 2006; Reichart and Rap-poport, 2007) is to use manually annotated data(xAi, .., xAN, yAi, .., yAN) from domain A to train aparser, feed the trained parser with data xBi, .., xBKfrom domain B in order to obtain their automatedannotations y?Bi, .., y?BKand then retrain the parser275dolphin tarantula highlandwhale anteater whiskyorca arachnid lowlandporpoise spider bagpipecetacean opossum glenshark scorpion distilleryTable 6: Visual chimeras for dolphin, tarantulaand highland.with a combination of ?clean?
data from domainA and ?noisy?
data from domain B.In our setup, self-training would be applied bylabeling a larger set of images with a cross-modalmapping function estimated on the initial train-ing data, and then using both sources of labeleddata to retrain the function.
Although the ideaof self-training for inducing cross-modal map-ping functions is appealing, especially given thevast amount of unlabeled data available out there,the very low performance of current cross-modalmapping functions makes the effort questionable.We would like to exploit unannotated data repre-sentative of the search space, without relying onthe output of cross-modal mapping for their an-notation.
One way to achieve this is to use dataaugmentation techniques that are representative ofthe search space.
Data augmentation is popularin computer vision, where it is performed (amongothers) by data jittering, visual sampling or imageperturbations.
It has proven beneficial for both?deep?
(Krizhevsky et al, 2012; Zeiler and Fer-gus, 2014) and ?shallow?
(Chatfield et al, 2014)systems, and it was recently introduced to NLPtasks (Zhang and LeCun, 2015).Specifically, in order to train the mapping func-tion using both annotated data and points that arerepresentative of the full search space, we rely ona form of data augmentation that we call visualchimera creation.
For every item yi/?
YTrin thesearch space S, we use linguistic similarity as aproxy of visual similarity, and create its visual vec-tor x?iby averaging the visual vectors correspond-ing to the nearest words in language space that dooccur as labels in the training set.
Table 6 presentssome examples of visual chimeras.
For yi=dol-phin, the visual vectors of other cetacean mam-none chimera-5 chimera-10P@1 1.9 3.7 3.2P@5 5.4 10.9 10.5P@10 9.0 15.8 15.9Table 7: Cross-modal zero-shot experimentwith data augmentation.
Labeling precision @Nwith no data augmentation (none) and when us-ing top 5 (chimera-5) and top 10 (chimera-10) near-est neighbors from training set of each item in thesearch space to build the corresponding chimeras(1K test items, 5.1K search space).mals are averaged to create the chimera x?i.
Sincelinguistic similarity is not always determined byvisual factors, the method also produces noisy datapoints.
For yi=tarantula, opossums enter the pic-ture, while for yi=highland images of ?topically?similar concepts are used (e.g., bagpipe).Table 7 reports cross-modal zero-shot labelingwhen training with max-margin and data augmen-tation.
We experiment with visual chimeras con-structed using 5 vs. 10 nearest neighbours.
Whilethe examples above suggest that the process injectssome noise in the training data, we also observe adecrease of pollution Npol1,Sfrom 88% when usingthe ?clean?
training data, to 71% and 73% whenexpanding them with chimeras (for chimera-5 andchimera-10, respectively).
Reflecting this drop inpollution, we see large improvements in precisionat all levels, when chimeras are used (no big dif-ferences between 5 or 10 neighbours).The improvements brought about by thechimera method are robust.
First, Table 8 reportsperformance when the search space excludes thetraining labels, showing that data augmentation isbeneficial beyond mitigating the bias in favor ofthe latter.
In this setup, chimera-5 is clearly out-performing chimera-10 (longer neighbour lists willinclude more noise), and we focus on it from hereon.All experiments up to here follow the stan-dard cross-modal zero-shot protocol, in which thesearch space is given by the union of the test andtraining labels, or a subset thereof.
Next, we makethe task more challenging by increasing it with 1Kextra elements acting as distractors.
The distrac-tors are either randomly sampled from our usual200K English word space, or, in the most chal-lenging scenario, picked among those words, inthe same space, that are among the top-5 near-276none chimera-5 chimera-10P@1 6.7 9.3 8.3P@5 21.7 25.2 21.3P@10 29.9 34.3 29.7Table 8: Cross-modal zero-shot experimentwith data augmentation, disjoint train/searchspaces.
Same setup as Table 8, but search spaceexcludes training elements (1K test items, 1Ksearch space).random relatednone chimera-5 none chimera-5P@1 0.8 3.3 1.9 2.8P@5 5.3 9.0 4.8 8.8P@10 8.8 13.3 7.9 12.6Table 9: Cross-modal zero-shot experimentwith data augmentation, enlarged search space.Labeling precision @N with no data augmenta-tion (none) and when using top 5 (chimera-5) near-est neighbors from training set of each item in thesearch space to build the corresponding chimeras.Test items: 1K.
Search space: 5.1K+1K extra dis-tractors from a 200K word space, either randomlypicked (random), or related to the training items.est neighbours of a training element.
Again, wecreate one visual chimera for each label in thesearch space.
Results are presented in Table 9.As expected, performance is negatively affectedwith both plain and data-augmented models, butthe latter is still better in absolute terms.
Whilechimera-5 undergoes a larger drop when the searchcontains many elements similar to the training data(?related?
column), which is explained by the factthat visual chimeras will often include the distrac-tor items of this setup, it appears to be more resis-tant against random labels, which in many casesare words that bear no resemblance to the trainingdata (e.g., naushad, yamato, 13-14).
The picturewhen using no data augmentation is exactly theopposite, with the model being more harmed, atP@1, by the random labels.Finally, Table 10 presents results in the cross-linguistic setup, when applying the same data aug-mentation technique.
In this case, we augmentthe 5K training elements with 11.5K chimeras, forthe 1.5K test elements and 10K randomly sam-pled distractors.
For these 11.5K elements, we as-sociate their Italian (target space) label yiwith anone chimera-5P@1 38.4 31.1P@5 54.2 46.1P@10 60.4 51.3Table 10: Cross-linguistic zero-shot experimentwith data augmentation.
Translation precision@N when learning with max-margin and no dataaugmentation (none) or data augmentation usingthe top 5 (chimera-5) nearest neighbors of 11.5Kitems in the 200K-word search space (1.5K testitems).
catdog truckFigure 2: Looking for intruders.
We pick truckrather than dog as negative example for cat.?pseudo-translation?
vector x?iobtained by averag-ing the vectors of the English (source space) trans-lations of the nearest Italian words to yiincludedin the training set.
Results, in Table 10, show thatin this case our data augmentation method is ac-tually hampering performance.
We saw that pol-lution affects the cross-linguistic setup much lessthan it affects the cross-modal one, and we con-jecture that, consequently, in the translation task,there is not a large-enough generalization gain tomake up for the extra noise introduced by augmen-tation.5 Picking informative negative examplesAn interesting feature of the ranking max-marginobjective lies in its active use of negative exam-ples.
While previous work in cross-space map-ping has paid little attention to the properties thatnegative samples should possess, this has not goneunnoticed in the NLP literature on structured pre-diction tasks.
Smith and Eisner (2005) propose acontrastive estimation framework in the context ofPOS-tagging, in which positive evidence derivedfrom gold sentence annotations is extended withnegative evidence derived by various neighbour-hood functions that corrupt the data in particularways (e.g., by deleting 1 word).Having shown the effectiveness of max-marginestimation in the previous sections, we now take277Cross-linguistic Cross-modalrandom intruder random intruderP@1 38.4 40.2 3.7 5.6P@5 54.2 55.5 10.9 12.4P@10 60.4 61.8 15.8 17.8Table 11: Random vs. intruding negative exam-ples.
Zero-shot precision @N results when cross-space function is estimated using max-margin withrandom or ?intruder?
negative examples, cross-linguistically (test items: 1.5K, search space:200K) and cross-modally (test items: 1K, searchspace: 5.1K).a first step towards engineering the negative evi-dence exploited by this method, in the context ofinducing cross-space mapping functions.
In par-ticular, our idea is that, given a training instancexi, an informative negative example would be nearthe mapped vector y?i, but far from the actual goldtarget space vector yi.
Intuitively, such ?intruders?correspond to cases where the mapping functionis getting the predictions seriously wrong, and thusthey should be very informative in ?correcting?
thefunction mapping trajectories.
This can seen as avector-space interpretation of the max-loss updateprotocol (Crammer et al, 2006) that picks nega-tive samples expected to harm performance more.Figure 2 illustrates the idea with a cartoon exam-ple.
If cat is the gold target vector yiand y?ithecorresponding mapped vector, then we are goingto pick truck as negative example, since it is an in-truder (near the mapped vector, far from the goldone).More formally, at each step of stochastic gra-dient descent, given a source space vector xi, itstarget gold label/translation yiin YTrand themapped vector y?i, we compute sj= cos(y?i, yj) ?cos(yi, yj), for all vectors yjin YTrs.t.
j 6= i, andpick as negative example for xithe vector with thelargest sj.Table 11 presents zero-shot mapping resultswhen intruding negative examples are used formax-margin estimation.
For cross-modal map-ping, we apply data augmentation as described inthe previous section.
While the absolute perfor-mance increase is relatively small (less than 2% inboth setups), it is consistent.
Furthermore, the pro-posed protocol results in lower Npol1,Spollution inthe cross-modal setup (from 71% to 63%).
Finally,we observe that the learning behaviour of the twoNumber of Epochs0 5 10 15 20 25 30 35 40 45 50Precision@10.150.20.250.30.350.40.45 randomintruderFigure 3: Learning curve with random or in-truding negative samples in the cross-linguisticexperiment.protocols (intruders vs. random) is different; theintruder approach is already achieving good perfor-mance after just few training epochs, since it canrely on more informative negative samples (seeFigure 3).6 ConclusionWe have considered some general mathemati-cal and empirical properties of linear cross-spacemapping functions, suggesting one well-known(max-margin estimation) and two new (chimeraaugmentation and ?intruder?
negative sample ad-justment) methods to improve their performance.With them, we achieve results well above the stateof the art in both the cross-linguistic and the cross-modal setting.
Both chimera and the intrudermethods are flexible, and we plan to explore themfurther in future research.
In particular, we wantto devise more semantically-motivated methods toselect chimera components and negative samples.AcknowledgmentsWe thank Adam Liska, Yoav Goldberg and theanonymous reviewers for useful comments.
Weacknowledge ERC 2011 Starting Independent Re-search Grant n. 283554 (COMPOSES).ReferencesMarco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014.
Don?t count, predict!
asystematic comparison of context-counting vs.context-predicting semantic vectors.
In Proceedingsof ACL, pages 238?247, Baltimore, MD.278Ken Chatfield, Karen Simonyan, Andrea Vedaldi, andAndrew Zisserman.
2014.
Return of the devil in thedetails: Delving deep into convolutional nets.
arXivpreprint arXiv:1405.3531.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
The Journal of Ma-chine Learning Research, 7:551?585.Jia Deng, Wei Dong, Richard Socher, Lia-Ji Li, andLi Fei-Fei.
2009.
Imagenet: A large-scale hierarchi-cal image database.
In Proceedings of CVPR, pages248?255, Miami Beach, FL.Georgiana Dinu and Marco Baroni.
2014.
How tomake words with vectors: Phrase generation in dis-tributional semantics.
In Proceedings of ACL, pages624?633, Baltimore, MD.Georgiana Dinu, Angeliki Lazaridou, and Marco Ba-roni.
2015.
Improving zero-shot learning by miti-gating the hubness problem.
In Proceedings of ICLRWorkshop Track, San Diego, CA.
Published on-line: http://www.iclr.cc/doku.php?id=iclr2015:main.Jeff Donahue, Judy Hoffman, Erik Rodner, KateSaenko, and Trevor Darrell.
2013.
Semi-superviseddomain adaptation with instance constraints.
In InProceedings of CVPR, pages 668?675.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-gio, Jeff Dean, Marc?Aurelio Ranzato, and TomasMikolov.
2013.
DeViSE: A deep visual-semanticembedding model.
In Proceedings of NIPS, pages2121?2129, Lake Tahoe, NV.Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grau-man.
2012.
Geodesic flow kernel for unsuperviseddomain adaptation.
In In Proceedings of CVPR,pages 2066?2073.Kristen Grauman and Bastian Leibe.
2011.
Visual Ob-ject Recognition.
Morgan & Claypool, San Fran-cisco.Yangqing Jia, Evan Shelhamer, Jeff Donahue, SergeyKarayev, Jonathan Long, Ross Girshick, SergioGuadarrama, and Trevor Darrell.
2014.
Caffe: Con-volutional architecture for fast feature embedding.arXiv preprint arXiv:1408.5093.Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.2012.
ImageNet classification with deep convolu-tional neural networks.
In Proceedings of NIPS,pages 1097?1105, Lake Tahoe, Nevada.Angeliki Lazaridou, Elia Bruni, and Marco Baroni.2014.
Is this a wampimuk?
cross-modal map-ping between distributional semantics and the visualworld.
In Proceedings of ACL, pages 1403?1414,Baltimore, MD.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of HLT-NAACL, pages 152?159.Tomas Mikolov, Quoc V Le, and Ilya Sutskever.2013a.
Exploiting similarities among lan-guages for machine translation.
arXiv preprintarXiv:1309.4168.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In Proceedings of NAACL,pages 746?751, Atlanta, Georgia.Tom Mitchell, Svetlana Shinkareva, Andrew Carlson,Kai-Min Chang, Vincente Malave, Robert Mason,and Marcel Just.
2008.
Predicting human brain ac-tivity associated with the meanings of nouns.
Sci-ence, 320:1191?1195.Mohammad Norouzi, Tomas Mikolov, Samy Bengio,Yoram Singer, Jonathon Shlens, Andrea Frome,Greg S Corrado, and Jeffrey Dean.
2014.
Zero-shotlearning by convex combination of semantic embed-dings.
In Proceedings of ICLR.Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,and Tom Mitchell.
2009.
Zero-shot learning withsemantic output codes.
In Proceedings of NIPS,pages 1410?1418, Vancouver, Canada.Milo?s Radovanovi?c, Alexandros Nanopoulos, and Mir-jana Ivanovi?c.
2010a.
Hubs in space: Popular near-est neighbors in high-dimensional data.
Journal ofMachine Learning Research, 11:2487?2531.Milo?s Radovanovi?c, Alexandros Nanopoulos, andMirjana Ivanovi?c.
2010b.
On the existence of obsti-nate results in vector space models.
In Proceedingsof SIGIR, pages 186?193, Geneva, Switzerland.Roi Reichart and Ari Rappoport.
2007.
Self-trainingfor enhancement and domain adaptation of statisticalparsers trained on small datasets.
In In Proceedingsof ACL, pages 616?623.Noah A Smith and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proceedings of ACL, pages 354?362.Richard Socher, Milind Ganjoo, Christopher Manning,and Andrew Ng.
2013.
Zero-shot learning throughcross-modal transfer.
In Proceedings of NIPS, pages935?943, Lake Tahoe, NV.Richard Socher, Quoc Le, Christopher Manning, andAndrew Ng.
2014.
Grounded compositional se-mantics for finding and describing images with sen-tences.
Transactions of the Association for Compu-tational Linguistics, 2:207?218.Gilbert Strang.
2003.
Introduction to linear algebra,3d edition.
Wellesley-Cambridge Press, Wellesley,MA.279J?org Tiedemann.
2012.
Parallel data, tools and in-terfaces in OPUS.
In Proceedings of LREC, pages2214?2218.Antonio Torralba and Alexei A Efros.
2011.
Unbiasedlook at dataset bias.
In In Proceedings of CVPR,pages 1521?1528.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-hen.
2011.
Literal and metaphorical sense identi-fication through concrete and abstract context.
InProceedings of EMNLP, pages 680?690, Edinburgh,UK.Jason Weston, Samy Bengio, and Nicolas Usunier.2011.
Wsabie: Scaling up to large vocabulary imageannotation.
In Proceedings of IJCAI, pages 2764?2770.Matthew Zeiler and Rob Fergus.
2014.
Visualizingand understanding convolutional networks.
In Pro-ceedings of ECCV (Part 1), pages 818?833, Zurich,Switzerland.Xiang Zhang and Yann LeCun.
2015.
Text understand-ing from scrath.
arXiv preprint arXiv:1502.01710.280
