Linguistic Knowledge GeneratorSatoshi SEKINE *Tokyo  In format ion  and  Communicat ions  Research  LaboratoryMatsush i ta  E lec t r i c  Indust r ia l  Co .
,L td .So f ia  ANANIADOU Je remy J .CARROLL  Jun ' i ch i  TSUJ I ICent re  for Computat iona l  L ingu is t i csUn ivers i ty  of Manchester  Ins t i tu te  of Sc ience and  Techno logyPO Box 88, Manchester M60 1QD, United Kingdom1 IntroductionThe difficulties in current NLP applications are sel-dom due to the lack of appropriate frameworks forencoding our linguistic or extra-linguistic knowledge,hut rather to the fact that we do not know in advancewhat actual znstances of knowledge should be, eventhough we know in advance what types of knowledgeare required.It normally takes a long time and requires painfultrial and error processes to adapt knowledge, for ex-ample, in existing MT systems in order to translatedocuments of a new text-type and of a new subjectdomain.
Semantic classification schemes for words,for example, usually reflect ontologies of subject do-mains so that we cannot expect a single classifica-tion scheme to be effective across different domains.To treat different suhlanguages requires different wordclassification schemes.
We have to construct appro-priate schemes for given sublanguages from scratch\[1\].It has also been reported that not only knowledgeconcerned with extra-linguistic domains but also syn-tactic knowledge, such as subcategorization frames ofverbs (which is usually conceived as a part of generallanguage knowledge), often varies from one sublan-guage to another \[2\].Though re-usability of linguistic knowledge is cur-rently and intensively prescribed \[3\], our contentionis that the adaptation of existing knowledge requiresprocesses beyond mere re-use.
That is,1.
There are some types of knowledge which wehave to discover from scratch, and which shouldbe integrated with already existing knowledge.2.
It is often the case that knowledge, which is nor-mally conceived as valid regardless of subject do-mains, text types etc., should be revised signifi-cantly.In practical projects, the ways of achieving suchadaptation and discovery of knowledge rely heavily*SEKINE is currently a visitor at U.M.I.S T.*eki~eOccl.umist.
ac.ukon human introspection.
In the adaptation of exist-ing MT systems, linguists add and revise the knowl-edge by inspecting a large set of system translationresults, and then try to translate another set of sen-tences from given domains, and so on.
The very factthat this trial and error process is time consumingand not always satisfactory indicates that human in-trospection alone cannot effectively reveal regularitiesor closure properties of sublanguages.There have been some proposals to aid this pro-cedure by using programs in combination with hugecorpora \[4\] \[51 \[13\] \[7\].
But the acquisition prog .
.
.
.
inthese reports require huge amounts of sample texts ingiven domains which often makes these methods un-realistic in actual application environments.
Further-more, the input corpora to such learning programsare often required to be properly tagged or anno-tated, which demands enormous manual effort, mak-ing them far less useful.In order to overcmne the difficulties of these meth-ods, we propose a Linguistic Knowledge Generator(LKG) which working on the principle of "GradualApproximation" involving both human introspectionand discovery programs.In the following section, we will explain the Grad-ual Approximation approach.
Then a scenario whichembodies the idea and finally we describe an experi-ment which illustrates its use.2 Gradual ApproximationSome of the traditional learning programs which areto discover linguistic regularities in a certain dimen-sion requires some amount of training corpus to berepresented or structured in a certain way.
For ex-ample, a program which learns disambiguation rulesfor parts-of-speech may require training data to berepresented as a sequence of words with their correctparts-of-speech.
It may count frequencies of tr igramof parts-of-speech in corpora to learn rules for disam-biguation.
On the other hand, a program to discoverthe semautic lasses of nouns may require input data(sentences) to be accompanied by their correct syn-AcrEs DE COLING-92, NANTES, 23-28 Ao~r 1992 5 6 0 PROC.
OF COLING-92, NANTES, AUG. 23-28.
1992tactic structures, and so on.This is also the case for statistical programs.
Mean-ingful statistics can be obtained only when they areapplied to appropriate units of data.
Frequencies ofcharacters or trigrams of characters, for example~ arcunlikely to be useful for capturing the structures ofthe semantic domains of a given sublanguage.
Inshort, discovery processes can be effectively assistedor carried out, if corpora are appropriately repre-sented for the purpose.
However, to represent or tagcorpora ppropriately requires other sorts of linguisticor extra-linguistic knowledge or even the very knowl-edge whicb is to he discovered by the program.For example, though corpora amtotated with syn-tactic structures are usefld tbr discovering semanticclasses, to assign correct syntactic structures to cor-pora requires emantic knowledge in order to preventproliferation of Imssible syntactic structures.One possible way of avoiding this chicken-and-eggsituation is 1o use roughly approximated, imperfectknowledge of semantic domains in order to hypothe-mac correct syntactic structures of sentences ill cor-pora.
Because such approximated semantic knowl-edge will contain errors or lack necessary information,syntact i c  s t ruc tures  &ssiglled to sentences ii1 corporamay contain errors or imperfections.Ilowever, if a program or Imnran expert could pro-duce more accurate, less imperfect knowledge of se-mantic domains from descriptions of corpora (as-signed syntactic structures), we could use it to pro-duce more accurate, less erroneous syntactic descrip-tions of corpora, and repeat the same process againto gain fllrtber zmprovcment botb in knowledge of se-mantic domains and in syntactic descriptions of cor-pora.
Thus, we may be able to converge graduallyto botb correct syntactic descriptions of corpora, andsemantic lassifications of words.In order to support such convergence processes,I,KG has to maintain the following two types of data.1.
knowledge sets of various dimensions (morphol-ogy, syntax, semantics, pragmatics/ontology ofextra-linguistic domains etc.
), which are hypoth-esis, eel by humans or by discovery programs, andall of which are imperfect in the sense that theycontain erroneous generalizations, lack specificinformation, etc.2.
descriptions of corpora at diverse levels, whichare based on the hypothesised knowledge in 1.Because of the hypothetical nature of the knowl-edge in 1, descriptions based on it inevitably con-tain errors or lack precision.Based on these two types of data, both of whichcontain imperfect, the whole process of discoveringregularities in sublanguage will be performed as a re-laxation process or a gradual repetitive approxima-tion process.
That is,1.
hunmn specialists or discovery programs makehypotheses based on mrperfect descriptions ofcorpora2.
hypotheses thus proposed result in more accu-rate, less imperfect knowledge3.
tbe more accurate, less imperfect knowledge in2., results m a more accurate description of thecorporaThe same process will be repeated from 1., but thistime, based on the more accurate descriptions of co lpora than the previous cycle.
It will yield further,more accurate hypothesized knowledge and descrip-tions of corpora and so on.3 AlgorithmIn this sec.tkm, we describe a scenario to illustratehow our idea of the "Gradual Approximation" worksto obtain knowledge front actual corpora.
The goal ofthe scenario is to discover semantic lasses of nounswhich are effective for determining (disambiguating)internal structures of compound nouns, which con-sist of sequences of nmms.
Note that, because thereis no clear distinction in Japanese between nounphrases and conll lOUlld uonns  consisting of sequencesof nouns, we refer to them collectively as compoundnouns.
The scenario is comprised of three programs,ie.
Japanese tagging program, Automatic LearningProgram of Semantic Collocations and clustering pro-gram.There is a phase of human intervention which accel-erates the calculation, but in this scenario, we try tominimize it.
In the following, we first give an overviewof the scenario, then explain each program briefly, andtlnally report on an experiment that fits this scenario.Note that, though we use this simple scenario as anillustrative xample, the same learning program canbe used in another inore complex scenario whose aimis, for example, to discow~r semantic ollocation be-tween verbs aml noun/prepositional phrases.3.1 Scenar ioThis scenario Lakes a corpus without any significantannotation as tile input data, and generates, ms theresult, plausibility values of collocational relations be-tween two words and word clusters, based oll the cal-culated semantic distances between words.The diagram illustrating this scenario is shown inFigure 1.
The first program to be applied is the"Japanese tagging program" which divides a sentenceinto words and generates lists of possible parts-of-speech for each word.Sequences of words with parts-of-speeches are thenused to extract candidates for compound nouns (ornoun phrases consisting of noun sequences), whichare the input for the next program, the "Auto-matic Learning Program for Semantic Collocations"(ALPSC).
This program constitutes the main part ofAergs DE COLING-92, NANTES, 23-28 AOf/'r 1992 5 6 1 PROC.
OF COLING-92, NANTES, AUO.
23-28, 1992the scenario and produces tile ahove-ment, ioned out-putTbe output of the program contain errors.
Errorshere mean that the plausibility values assigned to coblocations may lead to wrong determinations of com-pound noun structures.
Such errors are contained inthe results, because of the errors in the tagged ata,the insufficient quality of the corpus and inevitableimperfections in tile learning system.From the word distance results, word clusters arecomputed by the next program, the "Clustering Pro-gram".
Because of tile errors in tile word distancedata, the computed clusters may be counter-intuitive.We expect human intervention at this stage to formu-late more intnitively reasonable clusters of nouns.After revision of the clusters by human specialists,the scenario enters a second trial.
That is, the ALPSCre-computes plausibilit.y values of collocations andword distances based on tile revised clusters, the"Chlstering Program" generates the next generationof dusters, and humans intervene to formulate morereasonable clusters, and so on, and so forth.
It is ex-pected that word clusters after the (i+l)-th trial be-comes more intuitively understandable than that ofthe i-th trial and that the repetition eventually con-verges towards ideal clusters of nouns and plausibilityvalues, in the sense that they are consistent both withhuman introspection and the actual corpus.It should be noted that, while the overall processworks as gradual approximation, the key program inthe scenario, the ALPSC also works in tile mode ofgradual approximation asexplained in Section 3.2.2.3.2 P rograms and  I luman in terven-t ionsWe will explain each program briefly.
Ifowevcr theALPSC is crucial and tmique, so it will be explainedin greater detail.3,2.1 Program: J apanese  tagging programThis program takes Japanese sentences as an input,finds word boundaries arid puts all possible parts-of-speech for each word under adjacency constraints.From the tagged sentences, sequences of nouns areextracted for input to the next program.3.2.2 Program: Antomat ic  Learn ing Programof Semant ic  Col locations (ALPSC)This is the key program which computes plausibil-ity values and word distances.
In this scenario, theALPSC treats only sequences of nouns, but it cangenerally applied for any structure of syntactic rela-tionships.
It is an unique program with the followingpoints \[8\]:l. it does not need a training corpus, which is one ofthe bottle necks of some other learning programs2.
it learns by using a combination of linguisticknowledge and statistical analysis3.
it uses a parser which produces all possible anal-yses4 it works as a relaxation processWhile it is included as a part of a larger repetitiveloop, this program itself contains a repetitive loop.Overv iewBefore formally describing the algorithm, the %l-lowing shni)le example illustrates its working.A parser produces all possible syntactic descrip-tions aluong words in the form of syntactic depen-dency structures, The description is represented by aset of tupies, for example, \[head uord, syntact i cre la t?on,  argument\].
The only syntactic relationm a tuple is MOD for this scenario, but it can be ei-ther a grammatical relation like MOD, SUB,J, OBJ,etc.
or a surface preposition like BY, WITH, etc.When two or more tupies share tile same argumentand tile same syntactic-relation, but have differenthead-words, there is an ambiguity.For example, tile description of a compound noun:"F i le  t rans fer  operat ion"  contains three tuples:\[transfer, MOO, file\]\[operation, MOD, file\]\[operation, HOD, transfer\]The first two tup\]es are redundant, because oneword can only be an argument in one of tile tuples.As repeatedly clahned in tile literature of natural an-gnage understanding, in order to resolve this ambi-gmty, a system may have to be able to infer extra-linguistic knowledge.
A practical problem here isthat there is no systematic way of accumulating suchextradinguistic knowledge for given subject fields.That is, mdess a system ha-s a full range of contex-tual understanding abilities it cannot reject either ofthe possihle interpretations a 'hnpossible'.
The besta system can do, without full understanding abilities,is to select more plausible ones or reject less plausi-ble ones.
This implies that we have to introduce ameasure by which we can judge plausibility of :inter-pretations'.The algorithm we propose computes such measuresfrom given data.
It gives a plausibility value to eachpossible tuple, based on the sample corpus.
For exam-pie, when tile tuples ( t rans fer ,  ROD, f i l e )  and(opera~:ion, MOD, f i l e )  are assigned 0.5 and 0.82as their plausibility, this would show the latter tupleto be more plausible than the former.The algorithm is based on the assumption thatthe ontological characteristics of the objects and ac-tions denoted by words (or linguistic expressions ingeneral), and the nature of the ontological relationsAcrEs DE COLING-92, NANTES, 23-28 AOOT 1992 5 6 2 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992among tbein, are exhibited, though implicitly in sam-pie texts.
For example, uouns denoting objects whichbelong to tile same ontological classes tend to appearm similar linguistic contexts.Note that we talk about extra-linguistic 'ontology'for the sake of explaining the basic idea bebind theactual algorithm, ttowever, as you will see, we donot represent such things as ontological entities inthe actual algorithm.
The algorithm sinrply countsfrequencies of co-occurrences among words, and wordsimilarity algorithms interpret such co-occurrences mscontexts.The algoritbm in this progranr computes the plan-sibility values of hypothesis-tuples like (operat ion ,M0D, I i l e ) ,  etc., basically by counting frequencies ofinstance-tuples \ [operat ion ,  ROD, f?
le \ ] ,  etc.
gencrated from the input data.Termino logy  and  notat ioninstance-.tuple {h, r, a\] : a token of a dependency rc-lat, ion; part of the analysis of a sentence in acorpus.hypothesis-tuple (h , r ,a )  : a dependency relation;an abstraction or type over identical instance-tuples}:yale-- repeat time of the relaxation cycle.
(;'r,, : Gredit of instance tuple 7' with identificationmmfl, er i, {0, 1\]V,,~ : Plausibility value of a hypothesis-tuple T incycle 9.
\[0, 1\]D~ (w.,rvb) : distance between words, w~ and wb mcycle 9.
TO, 11A lgor i thmThe following explanation of the algorithm assumesthat the i l l ln l t s  are sentences.1 For a sentence we use a simple grammar  to findall tuples lmssibly used.
Each instance-tuple isthen given credit in proportion to the number ofconlpeting tuples.1 (~ = (1)number of  competing tuplesThis credit sbows which rules are suitable for thissentence.
On the first iteration the split of thecredit between ambiguous analyses is uniform asshown above, but on subsequent i erations plau-sibility values of tire hypothesis-tuples VT a-1 be-fore the iteration are used to give preference tocredit for some analyses over others.
The formulafor this will be given later.2.
tlypotbesis-tuples have a plausibility value whichindicates their reliability by a number between0 and 1.
If an instance-tuple occurs frequentlyin the corpus or if it occurs where there are noalternative tuples, the plausibility value for thecorresponding hypothesis must be large.
Afteranalysing all the sentences of tile corpus, wc geta set of sentences with weighted instanee-tuples.Each instauee-tuple invokes a hypothesis-tuple.For each hypothesis-tulflC, we define the plausi-bility value by the following formula.
This for-mula is designed so that tile value does not ex-ceed 1.v4 = 1.
I \ ]  I1- v,,,,) (2)iAt this stage, the word-distances can be used tomodify the plausibility wdnes of the hypotbesiu-tupies.
The word-dist>tnces are either definedexternally using human mtuitiou or calculatedin the previous cycle with a formula given laterDisl`ance between words induces a distance be-tween bypothesis-tuples, 'lk~ speed up the cal-culation and to get, better resull`s, we use sim-ilar hypothesis eft~cts.
The plausibility valueof a hypothesis-tuple is modified based on theword distance and plausibility value of a simi-lar hypothesis.
For each bypotbesis-tuple, theplausibility-vMue is increased only as a conse-quence of the similar hypothesis-tuple which hasthe greatest ellect.
The new plausibility valuewith similar hypothesis-tulfie effect is calculated10y tile following formula.
(3)llere, the hypotbesis-tuple 7" is the hypothesis-tuple which has the greatest effect on thehypothesis-tuple "/' (origmM one).
I lypotbems-tuple T and T'  }lave all the same elements ex-cept one, "Fbe distance between ~" and 2/" isthe distance betweei1 the different elements, w aand wb.
Ordinarily the dilierence is in the heador argument element, but when the relation isa preposition, it is possible to consider distancefront another preposition.Distances between words are calculated on thebasis of similarity between hypothesis-tuplesabout them.
Tbe formula is as follows:D~ ( .
.
.
.
.
~) Z,~, (v4 - V4,) e (4)n7' and 7" are bypothesis-tuldes whose argumentsare w~ and wb, respectively and whose heads andrelations are the same fl is a constant parame-terACRES DE COLING-92, NANTES, 23-28 AO~" 1992 5 6 3 PROC.
OF COLING-92, NANTES, AUG. 23-28, 19925.
Tbis  procedure will be repeated from the begin-ning, but modify ing the credits of instance-tupiesbetween ambiguous analyses using tim plausibil-ity values of hypothesis-tuples.
This  will hope-fully be more accurate than the previous cycle.On the first iteration, we used just a constantfigure for the credits of instanee-tuples.
Butthis t ime we can use the plausibil ity value of thehypothesis-tuple which was deduced in the pre-vious iteration.
Hence with each iteration we ex-pect more reliable figures, q.
'o cMcuLate the newcredit of instance-tuple T, we use:c~, - vT~' (s)Ilere, V.r J m the numerator,  is the plausibil ityvalue of a hypothesis-tuple which is the same tu-pie as the instance-tuple T V,~ in the denom-inator are the plausibil ity values of competinghypothesis tnples in the sentence and the plau-sibility value of the same hyl)otbesis-tuple itself.ct is a coustant paranleter6.
Iterate step 1 to 5 several times, until the infor-matiou is saturated.3,2.3 Program: Clus ter ing  i ) rogramWord clusters are produced based on the word dis-tance data which are comlmted in the previous pro-g ram A non-overlapping clusters algorithm with themax imum method was used.
The level of the clusterswas adjusted experimental ly to get suitable sizes torbu Foau intervention.3.2.4 Human zT~lerventzon.
Select: c lus tersThe clusters may mbcrit  errors contained in the worddistance data  The errors can be classified into thefollowing two types.1 A Correct cluster overlaps with two or more geu-crated clusters.2 A generated duster  overlaps with two or morecorrect chlstersNote that 'correcU bere means that it is correctm terms of truman intuition.
To ease the laboriousjob of correcting these errors by band, we ignore thefirst type of error, which is much harder to removethan the second one.
It is not ditlieult to remove thesecond type of error, because the number of wordsin a single cluster ranges from two to about thirty,and this number is manageable for hmnans.
We tryto extract purely 'correcU clusters or a subset of acorrect cluster, from a generated cluster.It is our contention that, thougll chlsters containerrors, and are mixtures of clusters based on humanintuition and clusters computed by process, we willgradual ly converge on correct clusters by repeatingtiffs approximation.At this stage, some correct clusters in the producedclusters are extracted.
This  information will be aninput of the next trial of ALPSC.4 ExperimentWe conducted an experiment using compound nounsfrom a computer manual  according to the scenario.The resnlt for other relations, for example preposi-tional attachment,  would be not so differeut from thisresult.The corpus consisted of 8304 sentences.
As theresult of Japanese tagging program, 1881 candidates,616 kinds of compound nouns were extracted.Then ALPSC took these compound nouns as an in-put.
Tuple relations were supposed between all wordsof all compound nouns with the syntactic relation'MODIFY ' .
A tuple has to have a preceding argumentand a following head  For example, from a compoundnoun with 4 words, 5 ambiguous tuples and 1 firm tu-pie can be extracted, because each element can be theargument  m only one tuple.
An initial credit of 1/3was set for each instance-tuple whose arguments  arethe first word of the compound noun.
Similarly, acredit i /2  was set for each instance-tuple in whichthe second word is an argument.No word distance information was introduced inthe first trial.
Then the learning process was started.We have shown the results of first trial in Table 1and examples m Figure 2The results were classified as correct or incorrectetc.. 'Correct'  means that a hypotbesis-tuple whichhas the highest plausibil ity value is the correct tu-pie within ambiguous tuples. '
Incorrect'  means itisn't. '
ludefinite' means that plausibil ity values ofsome hypothesis-tuples have the same value.
'Un-certain' means that it is impossible to declare whichhypothesis tuple is the best without context.j 4 tl 41 \[ r /  s r 1 II 5 II 4 I o I o I 2 ITable I: Results of experiment after first ALPSCTile clustering program produced 44 clusters basedon the word distance data.
a sample of the clusters isshown in Figure 3.
The average number of words ina cluster was 3.43, Each produced cluster containedone to twenty five words.
This  is good number totreat manually.
The human intervention to extractcorrect clusters resulted in 26 clusters being selectedfrom 44 produced clusters.
The average number ofACRES DE COLING-92, NANTES, 23-28 Aot'rr 1992 5 6 4 Paoc.
oi: COLING-92, NAYrES, AUO.
23-28, 1992words in a cluster is 2.96, It took a linguist who isfamiliar with computer 15 minutes.
A sample of theselected clusters is shown in Figure 4.These clusters were used for the second trial ofALPSC, The results of second trial are shown in Ta-ble 2.a0 I 3 !
f~ Is I ~ /  i t0 L - -  ?
-1  ~ J \[ t~ l -U - i~- I -TF -  ~-  -\[---TC--/\ [ _~L( r4 .1 )  L (21.1) I (- 4.8 ) _~_ (-)_ATable 2: Results of experiment after second trial5 Discuss ionThe scenario described above embodies a part of ourideas.
Several other experiments have already beenconducted, based on other scenarios uch a.q a seenario for finding clusters of nouns by which we can re-solve ambiguities caused by prepositional ttachmentin English.
Though this works in a similar fashionas the one we discussed, it has to treat more serfous structural ambiguities and store diverse syntacticstructures.Though we have not compared them in detail, itcall be expected that the organization of semanticclusters of nouns tbat emerge in these two scenarioswill be different from each other.
One reflects colloca-tional relations among nouns, while the other reflectstlmse between ouns and verbs.
By merging these twoscenarios into one larger scenario, we may be able toobtain more accurate or intuitively reasonable notnrclusters.
We are planning to accumulate a number ofSUCh scenarios and larger scenarios.
We hope we canreport it.
soon.As t\)r tim result of the particular experiment in theprevious ection, one eighth of the incorrect resultshave progressed after one trial of the gradual approx-iruatiou.
This is significant progress in the processing.For humans it wmdd be a tremendously laborious jobas they would be required to examine all the results.What bunlans did in the experiment is simply dividethe produced clasters.Although the clusters are produced by a no-overlapping clustering algorithm in this experiment,we are developing an overlapping clustering program.l\[opefulty it will produce clusters which involve theconcept of word sense ambiguity.
It will mean thata word can belong to several clusters at a time.
3"hemethod to produce overlapping clusters is one of ourcurrent research topics.Examining the results, we can say that the clus-ter effect is not enough to explain word relatious ofcompound nouns, q'here might be some structuraland syntactic restrictions.
This feature of compoundnouns made it hard to get a higher percentage of cor-rect answers in our experiment.
Extra-processing toaddress these problems can be introduced into oursystem.Because tile process concerns huge amount of lin-guistic data which also has ambiguity, it is inevitableto be experimental.
A sort of repetitive progress isneeded to make the system smarter.
We will need toperform a lot of experiments in order to determinethe type of the human intervention required, as thereseems to be no means of deternfining this theoreti-cally.This system is aiming not to sinmlate humanlinguists who conveutionally have derived linguis-tic knowledge by computer, but to discover a newparadigm where automatic knowledge acquisitionprograrns and human effort are effectively combinedto generate linguistic knowledge.6 AcknowledgementsWc wouht like to thank our colleagues at theCCIo and Matsushita, in particular, Mr.a.Phillips,MrK.Kageura, Mr.P.Olivier and Mr.Y.Kamm, whosecomments have been very usefifl.t{cferences\[1\] Ralph Grishman: Discovery Procedures for Sub-language Selectional Patterns: Initial Experi-ments Comp.
Linguistics Vol.
12 No.3 (1986)\[2\] Sofia Auauiadou: Sublanguage studies as the Ba-sis for Conqmter Support for Multilingua} Com-munication t'roceedin9 s of 7'ermplarL '90, KualaLumpur (1990)\[3\] A Zampolli: Reusable Linguistic Resources (In-vited paper) 5th Conference of the E.A.C.L.
(199i)\[4\] Kenneth Ward Church: A Stochastic Parts Pro-gram and Noun Phrase Parser for UnrestrictedText 2nd Uon/erer~ce on A.N.L.P (1988)\[5\] Donald llindle anti Mats Rooth: Structural Am-biguity and l,exical Relations ?~9th Conference ofthe A.C.L.
(1991)\[6\] Smaja and McKeown: Automatically Extractingand Representing Collocations for language gen-eration 28th (7onfeve,~ce of tt*e A.C.L.
(1991)\[7\] Uri Zernik and Paul aacobs: Tagging for Learn-rag: Collecting Thematic Relations from Corpus13th UOLING-90 (1990)\[8\] S.Sekine, J.J.Carroll, S. Ananiadou, a.Tsujii:Automatic Learning for Semantic Collocation3rd Conference on A.N.L.P.
(1992)ACRES DE COL1NG-92, NANKS, 23-28 AoGr 1992 5 6 5 Pgoc.
OF COLING-92, NAI, rtEs, AUG. 23-28, 1992~_Sentenees )\[-?
.....
I(c,.,o.
)(,,,oo.,,on)Figure i.
Diagram of the scenario{Tr4~(file).a~(high speed),:~(discource).~t,--7"(loop),~--7"(group).~-~&(table)._Zl~(duplieation).~(reverse).~l~surplus).x~-~(schedule).8~(pliority).~(paper).~(float).iE~(regular).
:~-V(charaeter).t~(sentence-structure).~(words-and-phrase).~-~2~, supposition).~(KkNk).~ll~(chinese-ehareter), -y~ ~e ~ 0 (directory).,~'.~T'/~(pop-up), ~y--(letter), ~.~9 (back-),~(white-)}{~4~H(wild),~l-~(calculation)){3:~-(error),~i~(infinite).~(wait-)){c,Iggi(analysis),i'CJl~(object)){:~-~(hard),~(main-),~t~(assistance).14T(finish)}{~;~(access),f'~A~(usage)}{~g4~-(exeeute),~POT~(refer))I~(middZe-)}{~(connection))(~(retrieval)}{~Zl~(regulation)}IiPl(for~ard-), 3W-(copy)\]{~/:i~(multiple).--~7~ (default)}{7~-~V(field)}{g_~(return).:Yr~(direction).~(~idth).~(full-).~(half-)}{~T,(display).~(change)}{?
(upper-).~,~(management), ~,(delete)}{~(expression).
7,~V (font)}{~g(internal-),:~(grammar)}{~(specification),~(input).~2(output)}{-Y----:7'(tape)}{~#'-~(modification),~(creation),~bW(final-),~-~(mode)}{~(ite~)}(~(right).~(left)}Figure 3.
Sample of Produced Clusters in first trial\[yJl~--7": group\] \[~-?
: execu tel\[l~:pe~ission\] \[3~:chareter\](~'n,--~" ~)  O.
000(~%--7" i~)  O.
002(~L,-7" 3L~) O.
997 X(~  I~*q) ~.
000 o(~l~tY ~t'-l~) O.
000\[J:v-:error\] [~Lw~:management\] \[n,-~-7:routin\](~-  ~)  0.505 o(~J~---~-7) 0.494\[~1~--7": loop\] \[147:finish\] \[-7-x t- : test\]Or,--v" 147") O.
02~(147 -Y-~, ~) O.
976 xFigure 2.
Sample of the Results{:~:r(eharacter),~(words-and-phrases),qi~(KANA),~(chinese--character), ~y-(letter)}I:~:l~(discourse),~(sentence-structure)}{vw~/~(file),-7~ ~y~U (directory)}t~(maiu-),~PJ(assistanee)}{g(t~(execute),~(refer)}{~r~(direction),~(width)}{~(full-),~(half)}{~-~,(display).~(change)}I~Lu~(management).~J~,(delete)}{~l~(expression).
7~b(font)}1~,2(input).~2(output)}I~(modifieation).f~(creation)}(~i(right).~(left)}Figure 4.
Sample of Selected clustersAcrEs DE COLING-92, NANTES, 23-28 AO~" 1992 5 6 6 PROCI OF COLING-92, NANTES, AUG. 23-28, 1992
