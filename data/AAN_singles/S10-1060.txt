Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 268?271,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsCambridge: Parser Evaluation using Textual Entailment byGrammatical Relation ComparisonLaura Rimell and Stephen ClarkUniversity of CambridgeComputer Laboratory{laura.rimell,stephen.clark}@cl.cam.ac.ukAbstractThis paper describes the Cambridge sub-mission to the SemEval-2010 Parser Eval-uation using Textual Entailment (PETE)task.
We used a simple definition of en-tailment, parsing both T and H with theC&C parser and checking whether the coregrammatical relations (subject and object)produced for H were a subset of those forT.
This simple system achieved the topscore for the task out of those systems sub-mitted.
We analyze the errors made by thesystem and the potential role of the task inparser evaluation.1 IntroductionSemEval-2010 Task 12, Parser Evaluation usingTextual Entailment (PETE) (Yuret et al, 2010),was designed as a new, formalism-independenttype of parser evaluation scheme.
The task isbroadly Recognizing Textual Entailment (RTE),but unlike typical RTE tasks, its intention is to fo-cus on purely syntactic entailments, assuming nobackground knowledge or reasoning ability.
Forexample, given a text (T) The man with the hatwas tired., the hypothesis (H) The man was tired.is entailed, but The hat was tired.
is not.
A cor-rect decision on whether H is entailed can be usedas a diagnostic for the parser?s analysis of (someaspect of) T. By requiring only a binary decisionon the entailment, instead of a full syntactic anal-ysis, a parser can be evaluated while its underlyingformalism remains a ?black box?.Our system had two components: a parser, andan entailment system which decided whether T en-tails H based on the parser?s output.
We distin-guish two types of evaluation.
Task evaluation,i.e.
the official task scoring, indicates whether theentailment decisions ?
made by the parser and en-tailment system together ?
tally with the gold stan-dard dataset.
Entailment system evaluation, on theother hand, indicates whether the entailment sys-tem is an appropriate parser evaluation tool.
Inthe PETE task the parser is not evaluated directlyon the dataset, since the entailment system actsas intermediary.
Therefore, for PETE to be a vi-able parser evaluation scheme, each parser mustbe coupled with an entailment system which accu-rately reflects the parser?s analysis of the data.2 SystemWe used the C&C parser (Clark and Curran,2007), which can produce output in the form ofgrammatical relations (GRs), i.e.
labelled head-dependencies.
For example, (nsubj tiredman) for the example in Section 1 represents thefact that the NP headed by man is the subject ofthe predicate headed by tired.
We chose to use theStanford Dependency GR scheme (de Marneffe etal., 2006), but the same approach should work forother schemes (and other parsers producing GRs).Our entailment system was very simple, andbased on the assumption that H is a simplified ver-sion of T (true for this task though not for RTE ingeneral).
We parsed both T and H with the C&Cparser.
Let grs(S) be the GRs the parser producesfor a sentence S. In principle, if grs(H) ?
grs(T),then we would consider H an entailment.
In prac-tice, a few refinements to this rule are necessary.We identified three exceptional cases.
First,syntactic transformations between T and H maychange GR labels.
The most common transforma-tion in this dataset was passivization, meaning thata direct object in T could be a passive subject in H.Second, H could contain tokens not present in T.Auxiliary verbs were introduced by passivization.Pronouns such as somebody and something wereintroduced into some H sentences to indicate anNP or other phrase not targeted for evaluation.
De-terminers were sometimes introduced or changed,e.g.
prices to the prices.
Expletive subjects werealso sometimes introduced.268Third, the parses of T and H might be incon-sistent in an incidental way.
Consider the pair Ireached into that funny little pocket that is high upon my dress.
?
The pocket is high up on some-thing.
The intended focus of the evaluation (as in-dicated by the content word pair supplied as a sup-plement to the gold standard development data)is (pocket, high).
As long as the parser analyzespocket as the subject of high, we want to avoidpenalizing it for, say, treating the PP up on X dif-ferently in T and H.To address these issues we used a small set ofheuristics.
First, we ignored any GR in grs(H) con-taining a token not in T. This addressed the pas-sive auxiliaries, pronouns, determiners, and exple-tive subjects.
Second, we equated passive subjectswith direct objects.
Similar rules could be definedfor other transformations, but we implementedonly this one based on the prevalence of passiviza-tion in the development data.
Third, when check-ing whether grs(H) ?
grs(T), we considered onlythe core relations subject and object.
The intentionwas that incidental differences between the parsesof T and H would not be counted as errors.
Wechose these GR types based on the nature of the en-tailments in the development data, but the systemcould easily be reconfigured to focus on other rela-tion types.
Finally, we required grs(H) ?
grs(T) tobe non-empty (no vacuous positives), but did notrestrict this criterion to subjects and objects.We used a PTB tokenizer1for consistency withthe parser?s training data.
We used the morphalemmatizer (Minnen et al, 2000), which is builtinto the C&C tools, to match tokens across T andH; and we converted all tokens to lowercase.
If theparser failed to find a spanning analysis for eitherT or H, the entailment decision was NO.
The fullpipeline is shown in Figure 1.3 ResultsA total of 19 systems were submitted.
The base-line score for ?always YES?
was 51.8% accuracy.Our system achieved 72.4% accuracy, which wasthe highest score among the submitted systems.Table 1 shows the results for our system, as wellas SCHWA (University of Sydney), also based onthe C&C parser and the next-highest scorer (seeSection 6 for a comparison), and the median andlowest scores.
The parser found an analysis for1http://www.cis.upenn.edu/?treebank/tokenizer.sed.Tokenize T and H with PTB tokenizer?Parse T and H with C&C parser?Lowercase and lemmatize all tokens?Discard any GR in grs(H) containing a token not in T?YES if core(H) ?
core(T) and grs(H) ?
grs(T) 6= ?,NO otherwiseFigure 1: Full pipeline for parser and entailmentsystem.
core(S): the set of core (subject and ob-ject) GRs in grs(S).99.0% of T sentences and 99.7% of H sentencesin the test data.4 Error AnalysisTable 2 shows the results for our system on the de-velopment data (66 sentences).
The parser foundan analysis for 100% of sentences and the overallaccuracy was 66.7%.
In the majority of cases theparser and entailment system worked together tofind the correct answer as expected.
For example,for Trading in AMR shares was suspended shortlyafter 3 p.m. EDT Friday and didn?t resume.
?Trading didn?t resume., the parser produced threeGRs for H (tokens are shown lemmatized and low-ercase): (nsubj resume trading), (negdo n?t), and (aux resume do).
All ofthese were also in grs(T), and the correct YESdecision was made.
For Moreland sat brood-ing for a full minute, during which I made eachof us a new drink.
?
Minute is made., theparser produced two GRs for H. One, (auxpassmake be), was ignored because the passiveauxiliary be is not in T. The second, pas-sive subject GR(nsubjpass make minute)was equated with a direct object (dobj makeminute).
This GR was not in grs(T), so the cor-rect NO decision was made.In some cases a correct YES answer wasreached via arguably insufficient positive evi-dence.
For He would wake up in the middle ofthe night and fret about it.
?
He would wake up.,the parser produces incorrect analyses for the VPwould wake up for both T and H. However, theseGRs are ignored since they are non-core (not sub-ject or object), and a YES decision is based onthe single GR match (nsubj would he).
This269Score on YES entailments Score on NO entailments OverallSystem correct incorrect accuracy (%) correct incorrect accuracy (%) accuracy (%)Cambridge 98 58 62.8 120 25 82.8 72.4SCHWA 125 31 80.1 87 58 60.0 70.4Median 71 85 45.5 88 57 60.7 52.8Low 68 88 43.6 76 69 52.4 47.8Table 1: Results on the test data.Score on YES entailments Score on NO entailments OverallSystem correct incorrect accuracy (%) correct incorrect accuracy (%) accuracy (%)Cambridge 22 16 57.9 22 6 78.6 66.7Table 2: Results on the development data.Type FN FP TotalUnbounded dependency 8 1 9Other parser error 6 2 8Entailment system 1 3 4Difficult entailment 1 0 1Total 16 6 22Table 3: Error breakdown on the developmentdata.
FN: false negative, FP: false positive.is not entirely a lucky guess, since the entailmentsystem has correctly ignored the odd analyses ofwould wake up and focused on the role of he as thesubject of the sentence.
However, especially sincethe target content word pair was (he, wake), morepositive evidence would be desirable.
Of the 22correct YES decisions, only two were truly luckyguesses in that the single match was a determiner;others had at least one core match.Table 3 shows the breakdown of errors.
Thelargest category was false negatives due to un-bounded dependencies not recovered by theparser, for example It required an energy he nolonger possessed to be satirical about his father.?
Somebody no longer possessed the energy..Here the parser fails to recover the direct object re-lation between possess and energy in T. It is knownthat parsers have difficulty with unbounded depen-dencies (Rimell et al, 2009, from which the un-bounded examples in this dataset were obtained),so this result is not surprising.The next category was other parser errors.
Thisis a miscellaneous category including e.g.
errorson coordination, parenthetical elements, identify-ing the head of a clausal subject, and one due tothe POS tagger.
For example, for Then at least hewould have a place to hang his tools and some-thing to work on.
?
He would have something towork on., the parser incorrectly coordinated toolsand something for T. As a result (dobj havesomething) was in grs(H) but not grs(T), yield-ing an incorrect NO.Four errors were due to the entailment systemrather than the parser; these will be dicsussed inSection 5.
We also identified one sentence wherethe gold standard entailment appears to rely onextra-syntactic information, or at least informa-tion that is difficult for a parser to recover.
Thisis Index-arbitrage trading is ?something we wantto watch closely,?
an official at London?s Stock Ex-change said.
?We want to watch index-arbitragetrading.
Recovering the entailment would requireresolving the reference of something, arguably therole of a semantic rather than syntactic module.5 Entailment System EvaluationWe now consider whether our entailment systemwas an appropriate tool for evaluating the C&Cparser on the PETE dataset.
It is easy to imag-ine a poor entailment system that makes incorrectguesses in spite of good parser output, or con-versely one that uses additional reasoning to sup-plement the parser?s analysis.
To be an appropri-ate parser evaluation tool, the entailment systemmust decide whether the information in H is alsocontained in the parse of T, without ?introducing?or ?correcting?
any errors.Assuming our GR-based approach is valid, thengiven gold-standard GRs for T and H, we expect anappropriate entailment system to result in 100%accuracy on the task evaluation.
To perform thisoracle experiment we annotated the development270data with gold-standard GRs.
Using our entailmentsystem with the gold GRs we achieved 90.9% taskaccuracy.
Six incorrect entailment decisions weremade, of which one was on the arguably extra-syntactic entailment discussed in Section 4.Three errors were due to transformations be-tween T and H which changed the GR label orhead.
For example, consider Occasionally, thechildren find steamed, whole-wheat grains for ce-real which they call ?buckshot?.
?
Grains aresteamed..
In T, steamed is a prenominal adjective,with grains as its head; while in H, it is a passive,with grains as its subject.
The entailment systemdid not account for this transformation, althoughin principle it could have.
The other two errorsoccurred because GRs involving a non-core rela-tion or a pronoun introduced in H, both of whichour system ignored, were crucial for the correctentailment decision.Table 3 shows that with automatically-generated GRs, four errors on the task evaluationwere attributable to the entailment system.
Threeof these were also found in the oracle experiment.The fourth resulted from a POS change between Tand H for There was the revolution in Tibet whichwe pretended did not exist.
?
The pretended didnot exist..
The crucial GR was (nsubj existpretended) in grs(H), but the entailmentsystem ignored it because the lemmatizer didnot give pretend as the lemma for pretended as anoun.
This type of error might be prevented byanswering NO if the POS of any word changesbetween T and H, but the implementation isnon-trivial since word indices may also change.There were eight POS changes in the developmentdata, most of which did not result in errors.
Wealso observed two cases where the entailmentsystem ?corrected?
parser errors, yielding acorrect entailment decision despite the parser?sincorrect analysis of T. When compared with amanual analysis of whether T entailed H basedon automatically-generated GRs, the entailmentsystem achieved 89.4% overall accuracy.6 ConclusionWe achieved a successful result on the PETE taskusing a state-of-the-art parser and a simple entail-ment system, which tested syntactic entailmentsby comparing the GRs produced by the parser forT and H. We also showed that our entailment sys-tem had accuracy of approximately 90% as a toolfor evaluating the C&C parser (or potentially anyparser producing GR-style output) on the PETEdevelopment data.
This latter result is perhapseven more important than the task score since itsuggests that PETE is worth pursuing as a viableapproach to parser evaluation.The second-highest scoring system, SCHWA(University of Sydney), was also based on theC&C parser and used a similar approach (thoughusing CCG dependency output rather than GRs).It achieved almost identical task accuracy to theCambridge system, but interestingly with higheraccuracy on YES entailments, while our systemwas more accurate on NO entailments (Table 1).We attribute this difference to the decision crite-ria: both systems required at least one matchingrelation between T and H for a YES answer; butwe additionally answered NO if any core GR ingrs(H) was not in grs(T).
This difference showsthat a GR-based entailment system can be tuned tofavour precision or recall.Finally, we note that although this was a sim-ple entailment system with some dataset-specificcharacteristics ?
such as a focus on subject andobject relations rather than, say, PP-attachment ?these aspects should be amenable to customizationor generalization for other related tasks.AcknowledgmentsThe authors were supported by EPSRC grantEP/E035698/1.
We thank Matthew Honnibal forhis help in producing the gold-standard GRs.ReferencesStephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of LREC, Genoa, Italy.Guido Minnen, John Carroll, and Darren Pearce.
2000.Robust, applied morphological generation.
In Pro-ceedings of INLG, Mitzpe Ramon, Israel.Laura Rimell, Stephen Clark, and Mark Steedman.2009.
Unbounded dependency recovery for parserevaluation.
In Proceedings of EMNLP, Singapore.Deniz Yuret, Ayd?n Han, and Zehra Turgut.
2010.Semeval-2010 task 12: Parser evaluation using tex-tual entailments.
In Proceedings of SemEval-2010,Uppsala, Sweden.271
