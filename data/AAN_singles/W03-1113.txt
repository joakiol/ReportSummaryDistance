Dynamic Programming Matching for Large Scale Information RetrievalEiko YamamotoCommunications Research Laboratory, Kyoto Japaneiko@crl.go.jpMasahiro Kishida Yoshinori TakenamiSumitomo Electric Information Systems Co., Ltd., Osaka Japan{kishida-masahiro, takenami-yoshinori}@sei.co.jpYoshiyuki Takeda Kyoji UmemuraToyohashi University of Technology, Aichi Japan{take@ss.ics, umemura@tutics}.tut.ac.jpAbstractThough dynamic programming matchingcan carry out approximate string matchingwhen there may be deletions or insertionsin a document, its effectiveness andefficiency are usually too poor to use it forlarge-scale information retrieval.
In thispaper, we propose a method of dynamicprogramming matching for informationretrieval.
This method is as effective as aconventional information retrieval system,even though it is capable of approximatematching.
It is also as efficient as aconventional system.Keywords: Dynamic programming,Corpus-based, Japanese.1 IntroductionThe dynamic programming method is well-knownfor its ability to calculate the edit distance betweenstrings.
The method can also be applied to informa-tion retrieval.
Dynamic programming matching canmeasure the similarity between documents, even ifthere are partial deletions or insertions.
However,there are two problems in applying this method toinformation retrieval.
One problem is search effec-tiveness.
It is poor because dynamic programmingmatching lacks an adequate weighting schema.
Thesecond problem is computational efficiency.
Also,lack of an adequate indexing schema means that dy-namic programming matching usually has to processthe entire document.Yamamoto et al proposed a method of dynamicprogramming matching with acceptable search ef-fectiveness (Yamamoto et al, 2000; Yamamoto,Takeda, and Umemura, 2003).
They report thatthe effectiveness of dynamic programming match-ing improves by introducing an IDF (Inverse Doc-ument Frequency) weighting schema for all stringsthat contribute similarity.
They calculate matchingweights not only for words but also for all strings.Although they report that effectiveness is improved,the speed of their method is slower than that ofconventional dynamic programming matching, andmuch slower than that of a typical information re-trieval system.In this paper, we aim to improve the retrieval ef-ficiency of the dynamic programming method whilekeeping its search effectiveness.
From a mathemat-ical point of view, we have only changed the defini-tion of the weighting.
The mathematical structure ofsimilarity remains the same as that of the dynamicprogramming method proposed by (Yamamoto etal., 2000; Yamamoto, Takeda, and Umemura, 2003).Although it has the same definition, the new weight-ing method makes it possible to build a more effi-cient information retrieval system by creating the in-dex in advance.
To our surprise, we have observedthat our proposed method is not only more efficientbut also more effective.2 Similarities Based on DynamicProgramming MatchingIn this section, we introduce several similaritiesproposed by (Yamamoto et al, 2000; Yamamoto,Takeda, and Umemura, 2003).
All of them are aform of dynamic programming matching.
Thesesimilarities include translation of the edit distance.This distance has been described by several authors.We have adopted Korfhage?s definition: ?the editdistance is the minimum number of edit operations,such as insertion and deletion, which are required tomap one string into the other?
(Korfhage, 1997).There are three related similarities.
The first is dy-namic programming matching, which is simply con-version of the edit distance.
The second similarityis an extension of the first similarity, introducing acharacter weighting for each contributing character.The third and proposed similarity is an extension ofthe second one, using string weight instead of char-acter weight.2.1 Dynamic Programming MatchingAs stated above, dynamic programming (DP)matching is a conversion of edit distance.
We callthis similarity SIM1.
While the edit distance (ED) isa measure of difference, counting different charac-ters between two strings , SIM1 is a measure of sim-ilarity, counting matching characters between twostrings.
ED and SIM1 are defined as follows:Definition 2.1 Edit Distance (Korfhage, 1997)Let ?
and ?
be strings, x and y be a character, and??
be empty string.?
If both strings are empty thenED(?
?, ??)
= 0.0?
If x 6= y thenED(x, y) = 1.0?
If their first characters are the same thenED(x?, x?)
=MIN(ED(?, x?
), ED(x?, ?
),ED(?, ?)
+ 1.0)?
OtherwiseED(x?, y?)
=MIN(ED(?, y?
), ED(x?, ?
),ED(?, ?
))Definition 2.2 SIM1Let ?
and ?
be strings, x and y be a character, and??
be empty string.?
If both strings are empty thenSIM1(?
?, ??)
= 0.0?
If x 6= y thenSIM1(x, y) = 0.0?
If their first characters are the same thenSIM1(x?, x?)
=MAX(SIM1(?, x?
), SIM1(x?, ?
),SIM1(?, ?)
+ 1.0)?
OtherwiseSIM1(x?, y?)
=MAX(SIM1(?, y?
), SIM1(x?, ?
),SIM1(?, ?
))2.2 Character Weight DP SimilaritySIM1 adds 1.0 to the similarity between two stringsfor every matching character, and this value is con-stant for all the time.
Our assumption for the newfunction is that different characters make differentcontributions.
For example, in Japanese informa-tion retrieval, Hiragana characters are usually usedfor functional words and make a different contribu-tion than Kanji characters, which are usually usedfor content words.
Thus, it is natural to assign a dif-ferent similarity weight according to the nature ofthe character.
The below method of defining Charac-ter Weight DP Similarity adds not 1.0 but a specificweight depending on the matching character.
Wecall this similarity SIM2.
It resembles Ukkonen?sEnhanced Dynamic Programming ASM (Approxi-mate String Matching) (Berghel and Roach, 1996).The weight is expressed by a function called Score.SIM2 is defined as follows:Definition 2.3 SIM2Let ?
and ?
be strings, x and y be a character, and??
be empty string.?
If both strings are empty thenSIM2(?
?, ??)
= 0.0?
If x 6= y thenSIM2(x, y) = 0.0?
If their first characters are the same thenSIM2(x?, x?)
=MAX(SIM2(?, x?
), SIM2(x?, ?
),SIM2(?, ?)
+ Score(x))?
OtherwiseSIM2(x?, y?)
=MAX(SIM2(?, y?
), SIM2(x?, ?
),SIM2(?, ?
))2.3 String Weight DP SimilarityDP procedure usually considers just a single char-acter at a time, but since some long substrings canreceive good scores, it is natural to consider all pre-fixes of the longest common prefix, not just the nextcharacter.While SIM2 uses a character weight whenever acharacter matches between strings, a single char-acter may not be enough.
In some cases, evenwhen each character has a low weight, the stringas a whole may be a good clue for information re-trieval.
For example, ?chirimenjyako?
is a Japaneseword that could be a retrieval key word.
This word,which means ?boiled and dried baby sardines,?
con-sists only of Hiragana characters ?chi-ri-me-n-jya-ko?
but each character would make a small contri-bution in SIM2.The proposed similarity is called String WeightDP Similarity, which is a generalization of SIM2.We call this similarity SIM3.
It considers the weightof all matching strings and is defined as follows:Definition 2.4 SIM3Let ?
and ?
be strings, x and y be a character, and??
be empty string.?
If both strings are empty thenSIM3(?
?, ??)
= Score(??)
= 0.0?
OtherwiseSIM3(?, ?)
=MAX(SIM3s(?, ?
), SIM3g(?, ?))?
SIM3s(?
?, ??)
=MAX(Score(?)
+ SIM3(?
?, ??
))where ?
(= ??)
is the maximum lengthstring matching from the first character.?
SIM3g(x?, y?)
=MAX(SIM3(?, y?
), SIM3(x?, ?
),SIM3(?, ?
))2.4 Weighting FunctionYamamoto et al have used IDF (Inverse DocumentFrequency) as a weight for each string.
The weightis computed using a Score function as follows:Definition 2.5 Yamamoto et al?s Score functionLet ?
be string, df(?)
the frequency of documentsincluding ?
in the document set for retrieval, and Nbe the number of documents in the set.Score(?)
= IDF (?)
= ?log(df(?
)/N)The standard one-character-at-a-time DP methodassumes that long matches cannot receive exception-ally good scores.
In other words, it regards Score(?
)as 0 if the length of ?
is greater than one.
If theScore function obeys the inequality, Score(??)
<Score(?)
+ Score(?)
for all substrings ?
and ?,the best path would consist of a sequence of sin-gle characters, and we would not need to considerlong phrases.
However, we are proposing a differentScore function.
It sometimes assigns good scores tolong phrases, and therefore SIM2 has to be extendedinto SIM3 to establish a DP procedure that considersmore than just one character at a time.3 Proposed Weighting FunctionAlthough SIM3, as shown in Section 2.3, has rea-sonable effectiveness, its computation is harder thanthat of the edit distance, and much harder than thatof the similarity used in a conventional informationretrieval system.
In this paper, we have modifiedthe weighting function so that it keeps its effective-ness while improving efficiency.
To achieve this im-provement, we use the SIM3 with the same defini-tion but with a different score function.3.1 Proposed String WeightingWe reduce the computational cost by limiting stringsthat have positive scores.
First, we select bigrams assuch strings.
In other words, we assign a score ofzero if the length of the string does not equal to 2.Several language systems use Kanji characters (e.g.Chinese and Japanese), and bigram is an effectiveindexing unit for information retrieval for these lan-guage systems (Ogawa and Matsuda, 1997).
In addi-tion, we may assume that the contribution of a longerstring is approximated by the total bigram weight-ing.
We have also restricted our attention to infre-quent bigrams.
Thus, we have restricted the weight-ing function Score as follows, where K is the num-ber decided by the given query.?
If string length is 2 and cf(?)
< K thenScore(?)
= ?log(df(?)/N)?
Otherwise Score(?)
= 0.03.2 Using a Suffix Array for IndexingSince we have restricted the number of match-ing strings, and all the matching strings appear ina query, we can collect all the positions of suchstrings.
To make it possible, we need some index-ing in advance.
We have used a suffix array for thisindex.
Below we summarize our proposed algorithmusing a suffix array:I.
Make a suffix array of the document set.II.
For each query,A.
Make a set of substrings consisting of twocharacters (bigram).B.
For a given number n, extract the total n ofless frequent bigrams, calculating corpusfrequency.C.
For each bigram from step B,i.
Record all positions in which the bi-gram appears in the query and docu-ment set,ii.
Record all documents that contain thebigram.D.
For each document recorded,i.
Compute the similarity between thequery and the document with SIM3,using the recorded position of the cor-responding bigram.ii.
Assign the similarity to the document.E.
Extract the most similar 1000 documentsfrom the recorded documents as a retrievalresult for the query.We call the retrieval method described above FastDynamic Programming (FDP).
In general, retrievalsystems use indexes to find documents.
FDP alsouses an index as a usual method.
However, unlikeconventional methods, FDP requires information notonly on the document identification but also on theposition of bigrams.Manber and Myers proposed a data structurecalled ?suffix array.?
(Manber and Myers, 1993)Figure 1 shows an example of suffix array.
Eachsuffix is expressed by one integer corresponding toits position.
We use this suffix array to find out theposition of selected bigrams.
A suffix array can becreated in O(N log(N)) time because we need tosort all suffixes in alphabetical order.
We can getthe position of any string in O(log(N)) time by abinary search of suffixes and by then obtaining itscorresponding position.4 ExperimentIn the experiment, we compared the proposed FDPmethod with SIM1, SIM2, and SIM3, which weredescribed in Section 2.
We measured three values:Figure 1: Suffix Arraysearch effectiveness, memory usage, and executiontime.We used the NTCIR1 collection (NTCIR Project,1999).
This collection consists of 83 retrieval topicsand roughly 330,000 documents of Japanese tech-nical abstracts.
The 83 topics include 30 trainingtopics (topic01-30); the rest are for testing (topic31-83).
The testing topics were more difficult than thetraining topics.
Each topic contains five parts, ?TI-TLE?, ?DESCRIPTION?, ?NARRATIVE?, ?CON-CEPT?, and ?FIELD.?
We retrieved using ?DE-SCRIPTION,?
which is retrieval query and a shortsentence.All the experiments reported in this section wereconducted using a dual AMD Athlon MP 1900+with 3GB of physical memory, running TurboLinux7.0.4.1 Search EffectivenessThe proposed FDP method restricts the number ofbigrams that can contribute to string matching.
Thatis, only a small number of strings are considered.
Itwas not clear whether FDP maintains its effective-ness like SIM3.
To verify it, we compared the effec-tiveness of FDP with that of SIM1, SIM2, and SIM3.We also needed to know how the effectiveness mightvary by the number of bigrams.
We set number nat 5, 10, 15, 20, 30, 50, and 500.
They were namedFDP5, FDP10, FDP15, FDP20, FDP30, FDP50, andFDP500, respectively.Table 1: Statistical Significant Test for difference of MAP (?
= 0.005, ?
= 83?
1)SIM2 SIM3 FDP5 FDP10 FDP15 FDP20 FDP30 FDP50 FDP500SIM1 << << << << << << << << <<SIM2 << = < << << << << <<SIM3 = = < << << << <<FDP5 = << << << << <<FDP10 = << < < <FDP15 < = = =FDP20 = = =FDP30 = =FDP50 =Table 2: Search Effectiveness for Topic01-30Method 11 pt.
average R-precisionSIM1 0.1349 0.1790SIM2 0.1948 0.2296SIM3 0.2691 0.3024FDP5 0.2547 0.2649FDP10 0.2948 0.3089FDP15 0.3109 0.3446FDP20 0.3207 0.3574FDP30 0.3176 0.3421FDP50 0.3131 0.3377FDP500 0.3172 0.3419The NTCIR1 collection also contains a relevancejudgment.
We obtained the 11-point average pre-cision and R-precision using standard tools calledTRECEVAL.
And we tested about statistical signif-icance for difference of MAP (Mean Average Preci-sion) (Kishida et al, 2002).Tables 2 and 3 show the search effectiveness forall methods.
We found that FDP20 is the most ef-fective.
Table 1 shows the results of one-sided t-testfor difference of MAP x?i ?
y?i, where x?i and y?i areMAP of i-th method in the first row and MAP ofi-th method in the first column, respectively.
Thelevel of significance ?
is 0.005 and the degree offreedom ?
is 83 ?
1.
The Symbols <<,<,= rep-resent ?much less than ?
?, ?less than ?, and ?notless than ?
?, respectively.
We found that except forFDP5 and FDP10, the other FDPs are significantlymore effective than SIM3 at a level of significance0.005.
In additional, this shows that FDP30, FDP50,and FDP500 are not significantly more effective thanFDP20.
These have demonstrated our proposed FDPTable 3: Search Effectiveness for Topic31-83Method 11 pt.
average R-precisionSIM1 0.0545 0.0845SIM2 0.1245 0.1596SIM3 0.1807 0.2083FDP5 0.1277 0.1505FDP10 0.1766 0.2013FDP15 0.2144 0.2280FDP20 0.2398 0.2621FDP30 0.2353 0.2485FDP50 0.2354 0.2488FDP500 0.2350 0.2477method maintains its effectiveness, even though thestrings that contribute similarity are restricted to asmall number of bigrams.
Also, it is interesting thatthe FDP with 20 bigrams is significantly more effec-tive than the one with many more bigrams.4.2 Memory UsageThe proposed method needs to record all the posi-tions considered bigrams.
A memory area is there-fore required to hold position information; in theworst case, the memory size required is the prod-uct of the number of documents and the number ofsubstrings in a query.
This means the memory re-quirement could be very large.
However, using FDP,we have found that the amount of memory requestedis of a reasonable size.In other words, the size of the memory area is thetotal sum of collection frequency for all strings thatcontribute similarity.
We examined the amount ofmemory used by comparison for the total sum of col-lection frequency.0200000004000000060000000800000001000000001 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81QueryTotal Collection FrequencyAllNgram20BigramAllBigramFigure 2: Memory Usage (Total Number of Collection Frequency for Each String)0500000100000015000002000000250000030000003500000400000045000001 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81QueryTotal CollectionFrequency20BigramAllBigramFigure 3: Memory Usage for Different Number of Restricted BigramsFigure 2 shows the total sum of collection fre-quency for three kinds of string sets.
In the fig-ure, AllNgram is for sets of all substrings consid-ered by SIM3, AllBigram is for sets of all bigrams,and 20Bigram is for sets of 20 bigrams consideredby FDP20.
The field surrounded by the plot lineand the horizontal axis represents the total sum ofcollection frequency.
As the figure shows, AllBi-gram and 20Bigram occupy a much smaller fieldthan AllNgram.
This means the memory require-ment of FDP is much smaller than that of SIM3.This result shows that FDP is possible to efficientlyperform large-scale information retrieval on a com-puter with a reasonable amount of memory.Figure 3 shows enlarged graphs of AllBigram and20Bigram from Figure 2.
The figure shows that20Bigram equals AllBigram for most queries, butnot always.
However, as shown in Table 2 and Ta-ble 3, FDP20 actually has the highest precision in allFDPs.
This means that considering more bigrams isnot necessarily an advantage.
Probably, by choosingsubstrings with a high contribution, we manage toget rid of noisy strings.4.3 Execution TimeWe measured execution time under the same con-ditions as described in Section 4.1.
Notice we im-plemented SIM1, SIM2, and SIM3 in C language.On the other hand, FDP is implemented in Java(JDK1.3.1.04).
When we noted the time requiredto make a suffix array, we found that FDP took 1.5times as long as SIM in Figure 4.
Thus, for the samealgorithm, the execution speed of Java is generallyslower than that of C language.Figures 5 and 6 show the time taken to retrievefor each topic01-30 and topic31-83.
In the figures,the vertical axis is the number of documents, and thehorizontal axis is the execution time.
We found thatall SIMs took much longer than FDPs.
This demon-strates that our algorithm in Section 3 sharply im-proves execution speed.
Moreover, we found thatexecution time did not increase exponentially evenif the candidate documents for retrieval increased;instead, the retrieval collection becomes larger andlarger.
This suggests that FDP is an effective DPtechnique for large-scale information retrieval.5 Related WorkOur proposed technique is a type of DP matching.The most typical application of DP matching is geneinformation research, because DP is effective forgene information matching.
However, this systemhas a very slow processing speed.In recent years, advances in this field of re-search have meant that high-speed systems havebeen required for gene information retrieval.
Ahigh-speed gene information retrieval system calledBLAST was developed (Setubal and Meidanis,2001).
BLAST has achieved higher processingspeed by using heuristics that specify characteristicgene arrangements, rather than using DP matching.In contrast, we have managed to achieve fast match-ing using the DP technique.Moreover, in music information retrieval, an errorin copying a tune corresponds to a deficit (deletion)and insertion of data.
For this reason, a music searchengine has been built based on the DP technique (Huand Dannenberg, 2002).
Since there is a great dealof music information available these days, scalabil-ity is also an important problem for music informa-tion retrieval systems.
Our proposed DP method isscalable and can cope with deficits.
It therefore haspotential applications in music information retrieval.6 ConclusionIn this study, we proposed a DP matching methodfor large-scale information retrieval.
To improveits efficiency, this method selects the strings thatcontribute more to retrieval.
This selection processreduces the memory requirement and frequency ofmemory access.
We conclude that our method issuitable for large-scale information retrieval whereapproximate matching is required.AcknowledgementThis work was supported in The 21st Century COEProgram ?Intelligent Human Sensing,?
from theMinistry of Education, Culture, Sports, Science, andTechnology.ReferencesHal Berghel and David Roach.
1996.
An extension ofUkkonen?s enhanced dynamic programming ASM al-gorithm.
Journal of ACM TOIS, 4(1):94?106.Kazuaki Kishida, Makoto Iwayama, and Koji Eguchi.2002.
Methodology and Pragmatics of Retrieval Ex-periments at NTCIR Workshop.
Pre-meeting Lectureat the NTCIR-3 Workshop.Ning Hu and Roger B. Dannenberg.
2002.
Comparisonof Melodic Database Retrieval Techniques Using SungQueries.
Proceedings of JCDL 2002, 301?307.Robert R. Korfhage.
1997.
Information Storage andRetrieval.
WILEY COMPUTER PUBLISHING, 291?303.Udi Manber and Gene Myers.
1993.
Suffix arrays: anew method for on-line string searches.
SIAM Journalof Computing, 22(5):935?948.NTCIR Project.
http://research.nii.ac.jp/ntcir/.Yasushi Ogawa and Toru Matsuda.
1997.
Overlappingstatistical word indexing: A new indexing method forJapanese text.
Proceedings of SIGIR97, 226?234.Joao Carlos Setubal and Joao Meidanis.
2001.Introduction to Computational Molecular Biology.BrooksCole Publishing Company.Eiko Yamamoto, Mikio Yamamoto, Kyoji Umemura, andKenneth W. Church.
2000.
Dynamic Prgramming:A Method for Taking Advantage of Technical Ter-minology in Japanese Documents.
Proceedings ofIRAL2000, 125?131.Eiko Yamamoto, Yoshiyuki Takeda, and Kyoji Umemura.2003.
An IR Similarity Measure which is Tolerantfor Morphological Variation.
Journal of Natural Lan-guage Processing, 10(1):63?80.
(in Japanese).7.9317.31 23.3757.63 108.17185.42 263.57391.11 541.42 698.15856.31 1512.8810.3123.2450.68 81.12151.05 272.29354.78 561.57788.54 1031.59 1279.31 1530.431101001000100005000 10000 20000 30000 50000 80000 100000 150000 200000 250000 300000 allThe Number of DocumentsMaking Time[sec]SIMFDPFigure 4: Suffix Array Generation Time1101001000100005000 10000 20000 30000 50000 80000 100000 150000 200000 250000 300000 allThe Number of DocumentsExecusion Time[sec]SIM1 SIM2 SIM3 FDP5 FDP10FDP15 FDP20 FDP30 FDP50Figure 5: Execution Time for Topic01-301101001000100005000 10000 20000 30000 50000 80000 100000 150000 200000 250000 300000 allThe Number of DocumentsExecusionTime [sec]SIM1 SIM2 SIM3 FDP5 FDP10FDP15 FDP20 FDP30 FDP50Figure 6: Execution Time for Topic31-83
