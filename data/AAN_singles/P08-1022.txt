Proceedings of ACL-08: HLT, pages 183?191,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsHypertagging: Supertagging for Surface Realization with CCGDominic Espinosa and Michael White and Dennis MehayDepartment of LinguisticsThe Ohio State UniversityColumbus, OH, USA{espinosa,mwhite,mehay}@ling.osu.eduAbstractIn lexicalized grammatical formalisms, it ispossible to separate lexical category assign-ment from the combinatory processes thatmake use of such categories, such as pars-ing and realization.
We adapt techniquesfrom supertagging ?
a relatively recent tech-nique that performs complex lexical taggingbefore full parsing (Bangalore and Joshi,1999; Clark, 2002) ?
for chart realizationin OpenCCG, an open-source NLP toolkit forCCG.
We call this approach hypertagging, asit operates at a level ?above?
the syntax, tag-ging semantic representations with syntacticlexical categories.
Our results demonstratethat a hypertagger-informed chart realizer canachieve substantial improvements in realiza-tion speed (being approximately twice as fast)with superior realization quality.1 IntroductionIn lexicalized grammatical formalisms such as Lex-icalized Tree Adjoining Grammar (Schabes et al,1988, LTAG), Combinatory Categorial Grammar(Steedman, 2000, CCG) and Head-Driven Phrase-Structure Grammar (Pollard and Sag, 1994, HPSG),it is possible to separate lexical category assign-ment ?
the assignment of informative syntactic cat-egories to linguistic objects such as words or lex-ical predicates ?
from the combinatory processesthat make use of such categories ?
such as pars-ing and surface realization.
One way of performinglexical assignment is simply to hypothesize all pos-sible lexical categories and then search for the bestcombination thereof, as in the CCG parser in (Hock-enmaier, 2003) or the chart realizer in (Carroll andOepen, 2005).
A relatively recent technique for lex-ical category assignment is supertagging (Bangaloreand Joshi, 1999), a preprocessing step to parsing thatassigns likely categories based on word and part-of-speech (POS) contextual information.
Supertaggingwas dubbed ?almost parsing?
by these authors, be-cause an oracle supertagger left relatively little workfor their parser, while speeding up parse times con-siderably.
Supertagging has been more recently ex-tended to a multitagging paradigm in CCG (Clark,2002; Curran et al, 2006), leading to extremely ef-ficient parsing with state-of-the-art dependency re-covery (Clark and Curran, 2007).We have adapted this multitagging approach tolexical category assignment for realization using theCCG-based natural language toolkit OpenCCG.1 In-stead of basing category assignment on linear wordand POS context, however, we predict lexical cat-egories based on contexts within a directed graphstructure representing the logical form (LF) of aproposition to be realized.
Assigned categories areinstantiated in OpenCCG?s chart realizer where, to-gether with a treebank-derived syntactic grammar(Hockenmaier and Steedman, 2007) and a factoredlanguage model (Bilmes and Kirchhoff, 2003), theyconstrain the English word-strings that are chosen toexpress the LF.
We have dubbed this approach hy-pertagging, as it operates at a level ?above?
the syn-tax, moving from semantic representations to syn-tactic categories.We evaluate this hypertagger in two ways: first,1http://openccg.sourceforge.net.183we evaluate it as a tagger, where the hypertaggerachieves high single-best (93.6%) and multitagginglabelling accuracies (95.8?99.4% with category perlexical predication ratios ranging from 1.1 to 3.9).2Second, we compare a hypertagger-augmented ver-sion of OpenCCG?s chart realizer with the pre-existing chart realizer (White et al, 2007) that sim-ply instantiates the chart with all possible CCG cat-egories (subject to frequency cutoffs) for each in-put LF predicate.
The hypertagger-seeded realizerruns approximately twice as fast as the pre-existingOpenCCG realizer and finds a larger number ofcomplete realizations, resorting less to chart frag-ment assembly in order to produce an output withina 15 second per-sentence time limit.
Moreover, theoverall BLEU (Papineni et al, 2002) and METEOR(Lavie and Agarwal, 2007) scores, as well as num-bers of exact string matches (as measured against tothe original sentences in the CCGbank) are higherfor the hypertagger-seeded realizer than for the pre-existing realizer.This paper is structured as follows: Section 2 pro-vides background on chart realization in OpenCCGusing a corpus-derived grammar.
Section 3 de-scribes our hypertagging approach and how it is in-tegrated into the realizer.
Section 4 describes ourresults, followed by related work in Section 5 andour conclusions in Section 6.2 Background2.1 Surface Realization with OpenCCGThe OpenCCG surface realizer is based on Steed-man?s (2000) version of CCG elaborated withBaldridge and Kruijff?s multi-modal extensions forlexically specified derivation control (Baldridge,2002; Baldridge and Kruijff, 2003) and hybridlogic dependency semantics (Baldridge and Kruijff,2002).
OpenCCG implements a symbolic-statisticalchart realization algorithm (Kay, 1996; Carroll et al,1999; White, 2006b) combining (1) a theoreticallygrounded approach to syntax and semantic composi-tion with (2) factored language models (Bilmes andKirchhoff, 2003) for making choices among the op-tions left open by the grammar.In OpenCCG, the search for complete realizations2Note that the multitagger is ?correct?
if the correct tag isanywhere in the multitag set.he h2aa1heh3<Det><Arg0> <Arg1><TENSE>pres<NUM>sg<Arg0>w1 want.01m1<Arg1><GenRel><Arg1><TENSE>presp1pointh1have.03make.03Figure 1: Semantic dependency graph from the CCGbankfor He has a point he wants to make [.
.
.
]makes use of n-gram language models over wordsrepresented as vectors of factors, including surfaceform, part of speech, supertag and semantic class.The search proceeds in one of two modes, anytimeor two-stage (packing/unpacking).
In the anytimemode, a best-first search is performed with a con-figurable time limit: the scores assigned by the n-gram model determine the order of the edges onthe agenda, and thus have an impact on realizationspeed.
In the two-stage mode, a packed forest ofall possible realizations is created in the first stage;in the second stage, the packed representation is un-packed in bottom-up fashion, with scores assignedto the edge for each sign as it is unpacked, muchas in (Langkilde, 2000).
Edges are grouped intoequivalence classes when they have the same syn-tactic category and cover the same parts of the in-put logical form.
Pruning takes place within equiv-alence classes of edges.
Additionally, to realize awide range of paraphrases, OpenCCG implementsan algorithm for efficiently generating from disjunc-tive logical forms (White, 2006a).To illustrate the input to OpenCCG, consider thesemantic dependency graph in Figure 1, which istaken from section 00 of a Propbank-enhanced ver-sion of the CCGbank (Boxwell and White, 2008).In the graph, each node has a lexical predica-tion (e.g.
make.03) and a set of semantic features(e.g.
?NUM?sg); nodes are connected via depen-dency relations (e.g.
?ARG0?).
Internally, such184graphs are represented using Hybrid Logic Depen-dency Semantics (HLDS), a dependency-based ap-proach to representing linguistic meaning developedby Baldridge and Kruijff (2002).
In HLDS, hy-brid logic (Blackburn, 2000) terms are used to de-scribe dependency graphs.
These graphs have beensuggested as representations for discourse structure,and have their own underlying semantics (White,2006b).To more robustly support broad coverage surfacerealization, OpenCCG has recently been enhancedto greedily assemble fragments in the event that therealizer fails to find a complete realization.
The frag-ment assembly algorithm begins with the edge forthe best partial realization, i.e.
the one that coversthe most elementary predications in the input logi-cal form, with ties broken according to the n-gramscore.
(Larger fragments are preferred under theassumption that they are more likely to be gram-matical.)
Next, the chart and agenda are greedilysearched for the best edge whose semantic coverageis disjoint from those selected so far; this process re-peats until no further edges can be added to the setof selected fragments.
In the final step, these frag-ments are concatenated, again in a greedy fashion,this time according to the n-gram score of the con-catenated edges: starting with the original best edge,the fragment whose concatenation on the left or rightside yields the highest score is chosen as the one toconcatenate next, until all the fragments have beenconcatenated into a single output.2.2 Realization from an Enhanced CCGbankWhite et al (2007) describe an ongoing effort to en-gineer a grammar from the CCGbank (Hockenmaierand Steedman, 2007) ?
a corpus of CCG deriva-tions derived from the Penn Treebank ?
suitable forrealization with OpenCCG.
This process involvesconverting the corpus to reflect more precise anal-yses, where feasible, and adding semantic represen-tations to the lexical categories.
In the first step, thederivations in the CCGbank are revised to reflect thedesired syntactic derivations.
Changes to the deriva-tions are necessary to reflect the lexicalized treat-ment of coordination and punctuation assumed bythe multi-modal version of CCG that is implementedin OpenCCG.
Further changes are necessary to sup-port semantic dependencies rather than surface syn-tactic ones; in particular, the features and unifica-tion constraints in the categories related to semanti-cally empty function words such complementizers,infinitival-to, expletive subjects, and case-markingprepositions are adjusted to reflect their purely syn-tactic status.In the second step, a grammar is extracted fromthe converted CCGbank and augmented with logi-cal forms.
Categories and unary type changing rules(corresponding to zero morphemes) are sorted byfrequency and extracted if they meet the specifiedfrequency thresholds.A separate transformation then uses around twodozen generalized templates to add logical formsto the categories, in a fashion reminiscent of (Bos,2005).
The effect of this transformation is illustratedbelow.
Example (1) shows how numbered seman-tic roles, taken from PropBank (Palmer et al, 2005)when available, are added to the category of an ac-tive voice, past tense transitive verb, where *pred*is a placeholder for the lexical predicate; examples(2) and (3) show how more specific relations are in-troduced in the category for determiners and the cat-egory for the possessive ?s, respectively.
(1) s1 :dcl\np2/np3 =?s1 :dcl,x1\np2 :x2/np3 :x3 : @x1(*pred* ?
?TENSE?pres ?
?ARG0?x2 ?
?ARG1?x3)(2) np1/n1 =?np1 :x1/n1 :x1 : @x1(?DET?
(d ?
*pred*))(3) np1/n1\np2 =?np1 :x1/n1 :x1\np2 :x2 : @x1(?GENOWN?x2)After logical form insertion, the extracted andaugmented grammar is loaded and used to parse thesentences in the CCGbank according to the gold-standard derivation.
If the derivation can be success-fully followed, the parse yields a logical form whichis saved along with the corpus sentence in order tolater test the realizer.
The algorithm for followingcorpus derivations attempts to continue processing ifit encounters a blocked derivation due to sentence-internal punctuation.
While punctuation has beenpartially reanalyzed to use lexical categories, manyproblem cases remain due to the CCGbank?s re-liance on punctuation-specific binary rules that arenot supported in OpenCCG.185Currently, the algorithm succeeds in creating log-ical forms for 97.7% of the sentences in the devel-opment section (Sect.
00) of the converted CCG-bank, and 96.1% of the sentences in the test section(Sect.
23).
Of these, 76.6% of the development log-ical forms are semantic dependency graphs with asingle root, while 76.7% of the test logical formshave a single root.
The remaining cases, with multi-ple roots, are missing one or more dependencies re-quired to form a fully connected graph.
These miss-ing dependencies usually reflect inadequacies in thecurrent logical form templates.2.3 Factored Language ModelsFollowing White et al (2007), we use factored tri-gram models over words, part-of-speech tags andsupertags to score partial and complete realiza-tions.
The language models were created using theSRILM toolkit (Stolcke, 2002) on the standard train-ing sections (2?21) of the CCGbank, with sentence-initial words (other than proper names) uncapital-ized.
While these models are considerably smallerthan the ones used in (Langkilde-Geary, 2002; Vell-dal and Oepen, 2005), the training data does havethe advantage of being in the same domain andgenre (using larger n-gram models remains for fu-ture investigation).
The models employ interpolatedKneser-Ney smoothing with the default frequencycutoffs.
The best performing model interpolates aword trigrammodel with a trigrammodel that chainsa POS model with a supertag model, where the POSmodel conditions on the previous two POS tags, andthe supertag model conditions on the previous twoPOS tags as well as the current one.Note that the use of supertags in the factored lan-guage model to score possible realizations is distinctfrom the prediction of supertags for lexical categoryassignment: the former takes the words in the localcontext into account (as in supertagging for parsing),while the latter takes features of the logical form intoaccount.
It is this latter process which we call hyper-tagging, and to which we now turn.3 The Approach3.1 Lexical Smoothing and Search ErrorsIn White et al?s (2007) initial investigation of scal-ing up OpenCCG for broad coverage realization,test set grammar completeoracle / bestdev (00) dev 49.1% / 47.8%train 37.5% / 22.6%Table 1: Percentage of complete realizations using an or-acle n-gram model versus the best performing factoredlanguage model.all categories observed more often than a thresh-old frequency were instantiated for lexical predi-cates; for unseen words, a simple smoothing strategybased on the part of speech was employed, assign-ing the most frequent categories for the POS.
Thisapproach turned out to suffer from a large numberof search errors, where the realizer failed to find acomplete realization before timing out even in caseswhere the grammar supported one.
To confirm thatsearch errors had become a significant issue, Whiteet al compared the percentage of complete realiza-tions (versus fragmentary ones) with their top scor-ing model against an oracle model that uses a simpli-fied BLEU score based on the target string, which isuseful for regression testing as it guides the best-firstsearch to the reference sentence.
The comparisoninvolved both a medium-sized (non-blind) grammarderived from the development section and a largegrammar derived from the training sections (the lat-ter with slightly higher thresholds).
As shown inTable 1, with the large grammar derived from thetraining sections, many fewer complete realizationsare found (before timing out) using the factored lan-guage model than are possible, as indicated by theresults of using the oracle model.
By contrast, thedifference is small with the medium-sized grammarderived from the development section.
This result isnot surprising when one considers that a large num-ber of common words are observed to have manypossible categories.In the next section, we show that a supertag-ger for CCG realization, or hypertagger, can reducethe problem of search errors by focusing the searchspace on the most likely lexical categories.3.2 Maximum Entropy HypertaggingAs supertagging for parsing involves studying agiven input word and its local context, the concep-186tual equivalent for a lexical predicate in the LF is tostudy a given node and its local graph structure.
Ourimplementation makes use of three general types offeatures: lexicalized features, which are simply thenames of the parent and child elementary predica-tion nodes, graph structural features, such as thetotal number of edges emanating from a node, thenumber of argument and non-argument dependents,and the names of the relations of the dependentnodes to the parent node, and syntactico-semanticattributes of nodes, such as the tense and number.For example, in the HLDS graph shown in Figure 1,the node representing want has two dependents, andthe relational type of make with respect to want isARG1.Clark (2002) notes in his parsing experiments thatthe POS tags of the surrounding words are highly in-formative.
As discussed below, a significant gain inhypertagging accuracy resulted from including fea-tures sensitive to the POS tags of a node?s parent, thenode itself, and all of its arguments and modifiers.Predicting these tags requires the use of a separatePOS tagger, which operates in a manner similar tothe hypertagger itself, though exploiting a slightlydifferent set of features (e.g., including features cor-responding to the four-character prefixes and suf-fixes of rare logical predication names).
Follow-ing the (word) supertagging experiments of (Cur-ran et al, 2006) we assigned potentially multiplePOS tags to each elementary predication.
The POStags assigned are all those that are some factor ?of the highest ranked tag,3 giving an average of 1.1POS tags per elementary predication.
The values ofthe corresponding feature functions are the POS tagprobabilities according to the POS tagger.
At thisambiguity level, the POS tagger is correct ?
92% ofthe time.Features for the hypertagger were extracted fromsemantic dependency graphs extracted from sections2 through 21 of the CCGbank.
In total, 37,168dependency graphs were derived from the corpus,yielding 468,628 feature parameters.The resulting contextual features and gold-standard supertag for each predication were thenused to train a maximum entropy classifier model.3I.e., all tags t whose probabilities p(t) ?
?
?
p?, where p?is the highest ranked tag?s probability.Maximum entropy models describe a set of proba-bility distributions of the form:p(o | x) =1Z(x)?
exp( n?i=1?ifi(o, x))where o is an outcome, x is a context, the fi arefeature functions, the ?i are the respective weightsof the feature functions, and Z(x) is a normalizingsum over all competing outcomes.
More concretely,given an elementary predication labeled want (as inFigure 1), a feature function over this node could be:f(o, x) ={ 1, if o is (s[dcl]\np)/(s[adj]\np) andnumber of LF dependents(x) = 20, otherwise.We used Zhang Le?s maximum entropy toolkit4for training the hypertagging model, which uses animplementation of Limited-memory BFGS, an ap-proximate quasi-Newton optimization method fromthe numerical optimization literature (Liu and No-cedal, 1989).
Using L-BFGS allowed us to includecontinuous feature function values where appropri-ate (e.g., the probabilities of automatically-assignedPOS tags).
We trained each hypertagging model to275 iterations and our POS tagging model to 400 it-erations.
We used no feature frequency cut-offs, butrather employed Gaussian priors with global vari-ances of 100 and 75, respectively, for the hypertag-ging and POS tagging models.3.3 Iterative ?-Best RealizationDuring realization, the hypertagger serves to prob-abilistically filter the categories assigned to an ele-mentary predication, as well as to propose categoriesfor rare or unseen predicates.
Given a predication,the tagger returns a ?-best list of supertags in orderof decreasing probability.
Increasing the number ofcategories returned clearly increases the likelihoodthat the most-correct supertag is among them, but ata corresponding cost in chart size.
Accordingly, thehypertagger begins with a highly restrictive value for?, and backs off to progressively less-restrictive val-ues if no complete realization could be found usingthe set of supertags returned.
The search is restarted4http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.187Table 2: Hypertagger accuracy on Sections 00 and 23.Results (in percentages) are for per-logical-predication(PR) and per-whole-graph (GRPH) tagging accurcies.Difference between best-only and baselines (b.l.)
is sig-nificant (p < 2 ?
10?16) by McNemar?s ?2 test.Sect00 Sect23?
TagsPred PR GRPH PR GRPHb.l.
1 1 68.7 1.8 68.7 2.3b.l.
2 2 84.3 9.9 84.4 10.91.0 1 93.6 40.4 93.6 38.20.16 1.1 95.8 55.7 96.2 56.80.05 1.2 96.6 63.8 97.3 66.00.0058 1.5 97.9 74.8 98.3 76.91.75e-3 1.8 98.4 78.9 98.7 81.86.25e-4 2.2 98.7 82.5 99.0 84.31.25e-4 3.2 99.0 85.7 99.3 88.55.8e-5 3.9 99.1 87.2 99.4 89.9from scratch with the next ?
value, though in prin-ciple the same chart could be expanded.
The iter-ative, ?-best search for a complete realization usesthe realizer?s packing mode, which can more quicklydetermine whether a complete realization is possi-ble.
If the halfway point of the overall time limitis reached with no complete realization, the searchswitches to best-first mode, ultimately assemblingfragments if no complete realization can be foundduring the remaining time.4 Results and DiscussionSeveral experiments were performed in training andapplying the hypertagger.
Three different modelswere created using 1) non-lexicalized features only,2) all features excluding POS tags, 3) all, 3) allfeatures except syntactico-semantic attributes suchas tense and number and 4) all features available.Models trained on these feature subsets were testedagainst one another on Section 00, and then the bestperforming model was run on both Section 00 and23.4.1 Feature Ablation TestingThe the whole feature set was found in feature abla-tion testing on the development set to outperform allother feature subsets significantly (p < 2.2 ?
10?16).These results listed in Table 3.
As we can see, takingTable 3: Hypertagger feature ablation testing results onSection 00.
The full feature set outperforms all others sig-nificantly (p < 2.2 ?
10?16).
Results for per-predication(PR) and per-whole-graph (GRPH) tagging percentageaccuracies are listed.
(Key: no-POS=no POS features;no-attr=no syntactico-semantic attributes such as tenseand number; non-lex=non-lexicalized features only (nopredication names).FEATURESET PR GRPHfull 93.6 40.37no-POS 91.3 29.5no-attr 91.8 31.2non-lex 91.5 28.7away any one class of features leads to drop in per-predication tagging accuracy of at least 1.8% and adrop per-whole-graph accuracy of at least 9.2%.
Asexpected from previous work in supertagging (forparsing), POS features resulted in a large improve-ment in overall accuracy (1.8%).
Although the POStagger by itself is only 92% accurate (as a multi-tagger of 1.1 POSword average ambiguity) ?
well be-low the state-of-the-art for the tagging of words ?its predictions are still quite valuable to the hyper-tagger.4.2 Best Model Hypertagger AccuracyThe results for the full feature set on Sections 00and 23 are outlined in Table 2.
Included in thistable are accuracy data for a baseline dummy tag-ger which simply assigns the most-frequently-seentag(s) for a given predication and backs off to theoverall most frequent tag(s) when confronted withan unseen predication.
The development set (00)was used to tune the ?
parameter to obtain reason-able hypertag ambiguity levels; the model was nototherwise tuned to it.
The hypertagger achieves highper-predication and whole-graph accuracies even atsmall ambiguity levels.4.3 Realizer PerformanceTables 4 and 5 show how the hypertagger improvesrealization performance on the development and testsections of the CCGbank.
As Table 4 indicates, us-ing the hypertagger in an iterative beta-best fash-ion more than doubles the number of grammati-cally complete realizations found within the time188Table 5: Realization quality metrics exact match, BLEU and METEOR, on complete realizations only and overall,with and without hypertagger, on Sections 00 and 23.Sec- Hyper- Complete Overalltion tagger BLEU METEOR Exact BLEU METEOR00 with 0.8137 0.9153 15.3% 0.6567 0.8494w/o 0.6864 0.8585 11.3% 0.5902 0.820923 with 0.8149 0.9162 16.0% 0.6701 0.8557w/o 0.6910 0.8606 12.3% 0.6022 0.8273Table 4: Percentage of grammatically complete realiza-tions, runtimes for complete realizations and overall run-times, with and without hypertagger, on Sections 00 and23.Sec- Hyper- Percent Complete Overalltion tagger Complete Time Time00 with 47.4% 1.2s 4.5sw/o 22.6% 8.7s 9.5s23 with 48.5% 1.2s 4.4sw/o 23.5% 8.9s 9.6slimit; on the development set, this improvement eli-mates more than the number of known search errors(cf.
Table 1).
Additionally, by reducing the searchspace, the hypertagger cuts overall realization timesby more than half, and in the cases where completerealizations are found, realization times are reducedby a factor of four, down to 1.2 seconds per sentenceon a desktop Linux PC.Table 5 shows that increasing the number of com-plete realizations also yields improved BLEU andMETEOR scores, as well as more exact matches.
Inparticular, the hypertagger makes possible a morethan 6-point improvement in the overall BLEU scoreon both the development and test sections, and amore than 12-point improvement on the sentenceswith complete realizations.As the effort to engineer a grammar suitable forrealization from the CCGbank proceeds in paral-lel to our work on hypertagging, we expect thehypertagger-seeded realizer to continue to improve,since a more complete and precise extracted gram-mar should enable more complete realizations to befound, and richer semantic representations shouldsimplify the hypertagging task.
Even with the cur-rent incomplete set of semantic templates, the hy-pertagger brings realizer performance roughly up tostate-of-the-art levels, as our overall test set BLEUscore (0.6701) slightly exceeds that of Cahill andvan Genabith (2006), though at a coverage of 96%instead of 98%.
We caution, however, that it remainsunclear how meaningful it is to directly comparethese scores when the realizer inputs vary consider-ably in their specificity, as Langkilde-Geary?s (2002)experiments dramatically illustrate.5 Related WorkOur approach follows Langkilde-Geary (2002) andCallaway (2003) in aiming to leverage the PennTreebank to develop a broad-coverage surface re-alizer for English.
However, while these earlier,generation-only approaches made use of convertersfor transforming the outputs of Treebank parsers toinputs for realization, our approach instead employsa shared bidirectional grammar, so that the input torealization is guaranteed to be the same logical formconstructed by the parser.
In this regard, our ap-proach is more similar to the ones pursued more re-cently by Carroll, Oepen and Velldal (2005; 2005;2006), Nakanishi et al (2005) and Cahill and vanGenabith (2006) with HPSG and LFG grammars.While we consider our approach to be the first toemploy a supertagger for realization, or hypertagger,the approach is clearly reminiscent of the LTAG treemodels of Srinivas and Rambow (2000).
The maindifference between the approaches is that ours con-sists of a multitagging step followed by the bottom-up construction of a realization chart, while theirsinvolves the top-down selection of the single mostlikely supertag for each node that is grammatically189compatible with the parent node, with the proba-bility conditioned only on the child nodes.
Notethat although their approach does involve a subse-quent lattice construction step, it requires makingnon-standard assumptions about the TAG; in con-trast, ours follows the chart realization tradition ofworking with the same operations of grammaticalcombination as in parsing, including a well-definednotion of semantic composition.
Additionally, asour tagger employs maximum entropy modeling, itis able to take into account a greater variety of con-textual features, including those derived from parentnodes.In comparison to other recent chart realization ap-proaches, Nakanishi et al?s is similar to ours in thatit employs an iterative beam search, dynamicallychanging the beam size in order to cope with thelarge search space.
However, their log-linear selec-tion models have been adapted from ones used inparsing, and do not condition choices based on fea-tures of the input semantics to the same extent.
Inparticular, while they employ a baseline maximumlikelihood model that conditions the probability ofa lexical entry upon its predicate argument struc-ture (PAS) ?
that is, the set of elementary predi-cations introduced by the lexical item ?
this prob-ability does not take into account other elements ofthe local context, including parents and modifiers,and their lexical predicates.
Similarly, Cahill andvan Genabith condition the probability of their lex-ical rules on the set of feature-value pairs linked tothe RHS of the rule, but do not take into account anyadditional context.
Since their probabilistic mod-els involve independence assumptions like those ina PCFG, and since they do not employ n-grams forscoring alternative realizations, their approach onlykeeps the single most likely edge in an equivalenceclass, rather than packing them into a forest.
Car-roll, Oepen and Velldal?s approach is like Nakanishiet al?s in that they adapt log-linear parsing modelsto the realization task; however, they employ manu-ally written grammars on much smaller corpora, andperhaps for this reason they have not faced the needto employ an iterative beam search.6 ConclusionWe have introduced a novel type of supertagger,which we have dubbed a hypertagger, that assignsCCG category labels to elementary predications ina structured semantic representation with high accu-racy at several levels of tagging ambiguity in a fash-ion reminiscent of (Bangalore and Rambow, 2000).To our knowledge, we are the first to report tag-ging results in the semantic-to-syntactic direction.We have also shown that, by integrating this hy-pertagger with a broad-coverage CCG chart real-izer, considerably faster realization times are possi-ble (approximately twice as fast as compared witha realizer that performs simple lexical look-ups)with higher BLEU, METEOR and exact string matchscores.
Moreover, the hypertagger-augmented real-izer finds more than twice the number of completerealizations, and further analysis revealed that therealization quality (as per modified BLEU and ME-TEOR) is higher in the cases when the realizer findsa complete realization.
This suggests that furtherimprovements to the hypertagger will lead to morecomplete realizations, hence more high-quality re-alizations.
Finally, further efforts to engineer agrammar suitable for realization from the CCGbankshould provide richer feature sets, which, as our fea-ture ablation study suggests, are useful for boostinghypertagging performance, hence for finding betterand more complete realizations.AcknowledgementsThe authors thank the anonymous reviewers, ChrisBrew, Detmar Meurers and Eric Fosler-Lussier forhelpful comments and discussion.ReferencesJason Baldridge and Geert-Jan Kruijff.
2002.
CouplingCCG and Hybrid Logic Dependency Semantics.
InProc.
ACL-02.Jason Baldridge and Geert-Jan Kruijff.
2003.
Multi-Modal Combinatory Categorial Grammar.
In Proc.ACL-03.Jason Baldridge.
2002.
Lexically Specified DerivationalControl in Combinatory Categorial Grammar.
Ph.D.thesis, School of Informatics, University of Edinburgh.Srinivas Bangalore and Aravind K. Joshi.
1999.
Su-190pertagging: An Approach to Almost Parsing.
Com-putational Linguistics, 25(2):237?265.Srinivas Bangalore and Owen Rambow.
2000.
Exploit-ing a probabilistic hierarchical model for generation.In Proce.
COLING-00.Jeff Bilmes and Katrin Kirchhoff.
2003.
Factored lan-guage models and general parallelized backoff.
InProc.
HLT-03.Patrick Blackburn.
2000.
Representation, reasoning, andrelational structures: a hybrid logic manifesto.
LogicJournal of the IGPL, 8(3):339?625.Johan Bos.
2005.
Towards wide-coverage semantic in-terpretation.
In Proc.
IWCS-6.Stephen Boxwell and Michael White.
2008.
ProjectingPropbank roles onto the CCGbank.
In Proc.
LREC-08.To appear.Aoife Cahill and Josef van Genabith.
2006.
RobustPCFG-based generation using automatically acquiredLFG approximations.
In Proc.
COLING-ACL ?06.Charles Callaway.
2003.
Evaluating coverage for largesymbolic NLG grammars.
In Proc.
IJCAI-03.John Carroll and Stefan Oepen.
2005.
High efficiencyrealization for a wide-coverage unification grammar.In Proc.
IJCNLP-05.John Carroll, Ann Copestake, Dan Flickinger, and Vic-tor Poznan?ski.
1999.
An efficient chart generator for(semi-) lexicalist grammars.
In Proc.
ENLG-99.Stephen Clark and James Curran.
2007.
Wide-coverageefficient statistical parsing with CCG and log-linearmodels.
Computational Linguistics, 33(4).Stephen Clark.
2002.
Supertagging for combinatorycategorial grammar.
In Proceedings of the 6th Inter-national Workshop on Tree Adjoining Grammars andRelated Frameworks (TAG+6), pages 19?24, Venice,Italy.James R. Curran, Stephen Clark, and David Vadas.2006.
Multi-tagging for lexicalized-grammar pars-ing.
In Proceedings of the Joint Conference of theInternational Committee on Computational Linguis-tics and the Association for Computational Linguis-tics (COLING/ACL-06), pages 697?704, Sydney, Aus-tralia.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and DependencyStructures Extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.Julia Hockenmaier.
2003.
Data and Models for Sta-tistical Parsing with Combinatory Categorial Gram-mar.
Ph.D. thesis, University of Edinburgh, Edin-burgh, Scotland.Martin Kay.
1996.
Chart generation.
In Proc.
ACL-96.Irene Langkilde-Geary.
2002.
An empirical verificationof coverage and correctness for a general-purpose sen-tence generator.
In Proc.
INLG-02.Irene Langkilde.
2000.
Forest-based statistical sentencegeneration.
In Proc.
NAACL-00.Alon Lavie and Abhaya Agarwal.
2007.
METEOR: Anautomatic metric for MT evaluation with high levelsof correlation with human judgments.
In Proceedingsof Workshop on Statistical Machine Translation at the45th Annual Meeting of the Association of Computa-tional Linguistics (ACL-2007), Prague.D C Liu and Jorge Nocedal.
1989.
On the limited mem-ory method for large scale optimization.
MathematicalProgramming B, 45(3).Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic methods for disambiguation of anHPSG-based chart generator.
In Proc.
IWPT-05.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The proposition bank: A corpus annotated with se-mantic roles.
Computational Linguistics, 31(1).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics (ACL), Philadelphia, PA.Carl J Pollard and Ivan A Sag.
1994.
Head-DrivenPhrase Structure Grammar.
University Of ChicagoPress.Yves Schabes, Anne Abeille?, and Aravind K. Joshi.1988.
Parsing strategies with ?lexicalized?
grammars:Application to tree adjoining grammars.
In Proceed-ings of the 12th International Conference on Compu-tational Linguistics (COLING-88), Budapest.Mark Steedman.
2000.
The Syntactic Process.
MITPress, Cambridge, Massachusetts, USA.Andreas Stolcke.
2002.
SRILM ?
An extensible lan-guage modeling toolkit.
In Proc.
ICSLP-02.Erik Velldal and Stephan Oepen.
2005.
Maximum en-tropy models for realization ranking.
In Proc.
MTSummit X.Erik Velldal and Stephan Oepen.
2006.
Statistical rank-ing in tactical generation.
In Proceedings of the 2006Conference on Empirical Methods in Natural Lan-guage Processing, Sydney, Australia, July.Michael White, Rajakrishnan Rajkumar, and Scott Mar-tin.
2007.
Towards broad coverage surface realiza-tion with CCG.
In Proc.
of the Workshop on UsingCorpora for NLG: Language Generation and MachineTranslation (UCNLG+MT).Michael White.
2006a.
CCG chart realization from dis-junctive inputs.
In Proceedings, INLG 2006.Michael White.
2006b.
Efficient realization of coordi-nate structures in Combinatory Categorial Grammar.Research on Language and Computation, 4(1):39?75.191
