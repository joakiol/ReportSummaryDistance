Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 148?156,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsPseudo-word for Phrase-based Machine TranslationXiangyu Duan Min Zhang Haizhou LiInstitute for Infocomm Research, A-STAR, Singapore{Xduan, mzhang, hli}@i2r.a-star.edu.sgAbstractThe pipeline of most Phrase-Based StatisticalMachine Translation (PB-SMT) systems startsfrom automatically word aligned parallel cor-pus.
But word appears to be too fine-grainedin some cases such as non-compositionalphrasal equivalences, where no clear wordalignments exist.
Using words as inputs to PB-SMT pipeline has inborn deficiency.
This pa-per proposes pseudo-word as a new start pointfor PB-SMT pipeline.
Pseudo-word is a kindof basic multi-word expression that character-izes minimal sequence of consecutive words insense of translation.
By casting pseudo-wordsearching problem into a parsing framework,we search for pseudo-words in a monolingualway and a bilingual synchronous way.
Ex-periments show that pseudo-word significantlyoutperforms word for PB-SMT model in bothtravel translation domain and news translationdomain.1 IntroductionThe pipeline of most Phrase-Based StatisticalMachine Translation (PB-SMT) systems startsfrom automatically word aligned parallel corpusgenerated from word-based models (Brown et al,1993), proceeds with step of induction of phrasetable (Koehn et al, 2003) or synchronous gram-mar (Chiang, 2007) and with model weights tun-ing step.
Words are taken as inputs to PB-SMT atthe very beginning of the pipeline.
But there is adeficiency in such manner that word is too fine-grained in some cases such as non-compositionalphrasal equivalences, where clear word align-ments do not exist.
For example in Chinese-to-English translation, ???
and ?would like to?constitute a 1-to-n phrasal equivalence, ?????
and ?how much is it?
constitute a m-to-nphrasal equivalence.
No clear word alignmentsare there in such phrasal equivalences.
Moreover,should basic translational unit be word or coarse-grained multi-word is an open problem for opti-mizing SMT models.Some researchers have explored coarse-grained translational unit for machine translation.Marcu and Wong (2002) attempted to directlylearn phrasal alignments instead of word align-ments.
But computational complexity is prohibi-tively high for the exponentially large number ofdecompositions of a sentence pair into phrasepairs.
Cherry and Lin (2007) and Zhang et al(2008) used synchronous ITG (Wu, 1997) andconstraints to find non-compositional phrasalequivalences, but they suffered from intractableestimation problem.
Blunsom et al (2008; 2009)induced phrasal synchronous grammar, whichaimed at finding hierarchical phrasal equiva-lences.Another direction of questioning word as basictranslational unit is to directly question wordsegmentation on languages where word bounda-ries are not orthographically marked.
In Chinese-to-English translation task where Chinese wordboundaries are not marked, Xu et al (2004) usedword aligner to build a Chinese dictionary to re-segment Chinese sentence.
Xu et al (2008) useda Bayesian semi-supervised method that com-bines Chinese word segmentation model andChinese-to-English translation model to derive aChinese segmentation suitable for machine trans-lation.
There are also researches focusing on theimpact of various segmentation tools on machinetranslation (Ma et al 2007; Chang et al 2008;Zhang et al 2008).
Since there are many 1-to-nphrasal equivalences in Chinese-to-English trans-lation (Ma and Way.
2009), only focusing onChinese word as basic translational unit is notadequate to model 1-to-n translations.
Ma andWay (2009) tackle this problem by using wordaligner to bootstrap bilingual segmentation suit-able for machine translation.
Lambert andBanchs (2005) detect bilingual multi-word ex-148pressions by monotonically segmenting a givenSpanish-English sentence pair into bilingualunits, where word aligner is also used.IBM model 3, 4, 5 (Brown et al, 1993) andDeng and Byrne (2005) are another kind of re-lated works that allow 1-to-n alignments, butthey rarely questioned if such alignments exist inword units level, that is, they rarely questionedword as basic translational unit.
Moreover, m-to-n alignments were not modeled.This paper focuses on determining the basictranslational units on both language sides withoutusing word aligner before feeding them into PB-SMT pipeline.
We call such basic translationalunit as pseudo-word to differentiate with word.Pseudo-word is a kind of multi-word expression(includes both unary word and multi-word).Pseudo-word searching problem is the same todecomposition of a given sentence into pseudo-words.
We assume that such decomposition is inthe Gibbs distribution.
We use a measurement,which characterizes pseudo-word as minimalsequence of consecutive words in sense of trans-lation, as potential function in Gibbs distribution.Note that the number of decomposition of onesentence into pseudo-words grows exponentiallywith sentence length.
By fitting decompositionproblem into parsing framework, we can findoptimal pseudo-word sequence in polynomialtime.
Then we feed pseudo-words into PB-SMTpipeline, and find that pseudo-words as basictranslational units improve translation perform-ance over words as basic translational units.
Fur-ther experiments of removing the power ofhigher order language model and longer maxphrase length, which are inherent in pseudo-words, show that pseudo-words still improvetranslational performance significantly overunary words.This paper is structured as follows: In section2, we define the task of searching for pseudo-words and its solution.
We present experimentalresults and analyses of using pseudo-words inPB-SMT model in section 3.
The conclusion ispresented at section 4.2 Searching for Pseudo-wordsPseudo-word searching problem is equal to de-composition of a given sentence into pseudo-words.
We assume that the distribution of suchdecomposition is in the form of Gibbs distribu-tion as below:)exp(1)|( ?= ySigXYPwhere X denotes the sentence, Y denotes a de-composition of X. Sig function acts as potentialfunction on each multi-word yk, and ZX acts aspartition function.
Note that the number of yk isnot fixed given X because X can be decomposedinto various number of multi-words.Given X, ZX is fixed, so searching for optimaldecomposition is as below:?==kyYY kKSigARGMAXXYPARGMAXY1)|(?
(2)where Y1K denotes K multi-word units from de-composition of X.
A multi-word sequence withmaximal sum of Sig function values is the searchtarget ?
pseudo-word sequence.
From (2) wecan see that Sig function is vital for pseudo-wordsearching.
In this paper Sig function calculatessequence significance which is proposed to char-acterize pseudo-word as minimal sequence ofconsecutive words in sense of translation.
Thedetail of sequence significance is described in thefollowing section.2.1 Sequence SignificanceTwo kinds of definitions of sequence signifi-cance are proposed.
One is monolingual se-quence significance.
X and Y are monolingualsentence and monolingual multi-words respec-tively in this monolingual scenario.
The other isbilingual sequence significance.
X and Y are sen-tence pair and multi-word pairs respectively inthis bilingual scenario.2.1.1 Monolingual Sequence SignificanceGiven a sentence w1, ?, wn, where wi denotesunary word, monolingual sequence significanceis defined as:1,1,,+?=jijiji FreqFreqSig   (3)where Freqi, j (i?j) represents frequency of wordsequence wi, ?, wj in the corpus, Sigi, j  repre-sents monolingual sequence significance of aword sequence wi, ?, wj.
We also denote wordsequence wi, ?, wj as span[i, j], whole sentenceas span[1, n].
Each span is also a multi-word ex-pression.Monolingual sequence significance of span[i, j]is proportional to span[i, j]?s frequency, while isinversely proportion to frequency of expandedspan (span[i-1, j+1]).
Such definition character-izes minimal sequence of consecutive wordswhich we are looking for.
Our target is to findpseudo-word sequence which has maximal sumof spans?
significances: kX kZ(1)149k (4) ?
== Kk spanspanK K SigARGMAXpw 11 1where pw denotes pseudo-word, K is equal to orless than sentence?s length.
spank is the kth spanof K spans span1K.
Equation (4) is the rewrite ofequation (2) in monolingual scenario.
Searchingfor pseudo-words pw1K is the same to findingoptimal segmentation of a sentence into K seg-ments span1K (K is a variable too).
Details ofsearching algorithm are described in section2.2.1.We firstly search for monolingual pseudo-words on source and target side individually.Then we apply word alignment techniques tobuild pseudo-word alignments.
We argue thatword alignment techniques will work fine if non-existent word alignments in such as non-compositional phrasal equivalences have beenfiltered by pseudo-words.2.1.2 Bilingual Sequence SignificanceBilingual sequence significance is proposed tocharacterize pseudo-word pairs.
Co-occurrenceof sequences on both language sides is used todefine bilingual sequence significance.
Given abilingual sequence pair: span-pair[is, js, it, jt](source side span[is, js] and target side span[it, jt]),bilingual sequence significance is defined as be-low:1k,1,1,1,,,,,,+?+?=ttssttssttssjijijijijiji FreqFreqSig   (5)where Freq denotes the frequency of a span-pair.Bilingual sequence significance is an extensionof monolingual sequence significance.
Its valueis proportional to frequency of span-pair[is, js, it,jt], while is inversely proportional to frequencyof expanded span-pair[is-1, js+1, it-1, jt+1].Pseudo-word pairs of one sentence pair are suchpairs that maximize the sum of span-pairs?
bilin-gual sequence significances:?
= ?
?= Kk pairspanpairspanK K SigARGMAXpwp 11 1  (6)pwp represents pseudo-word pair.
Equation (6) isthe rewrite of equation (2) in bilingual scenario.Searching for pseudo-word pairs pwp1K is equalto bilingual segmentation of a sentence pair intooptimal span-pair1K.
Details of searching algo-rithm are presented in section 2.2.2.2.2 Algorithms of Searching for Pseudo-wordsPseudo-word searching problem is equal to de-composition of a sentence into pseudo-words.But the number of possible decompositions ofthe sentence grows exponentially with the sen-tence length in both monolingual scenario andbilingual scenario.
By casting such decomposi-tion problem into parsing framework, we canfind pseudo-word sequence in polynomial time.According to the two scenarios, searching forpseudo-words can be performed in a monolin-gual way and a synchronous way.
Details of thetwo kinds of searching algorithms are describedin the following two sections.2.2.1 Algorithm of Searching for Monolin-gual Pseudo-words (SMP)Searching for monolingual pseudo-words isbased on the computation of monolingual se-quence significance.
Figure 1 presents the searchalgorithm.
It is performed in a way similar toCKY (Cocke-Kasami-Younger) parser.Initialization: Wi, i = Sigi, i;Wi, j = 0,  (i?j);1:  for d = 2 ?
n do2:      for all i, j s.t.
j-i=d-1 do3:          for k = i ?
j ?
1 do4:              v = Wi, k + Wk+1, j5:              if v > Wi, j then6:                  Wi, j = v;7:          u = Sigi, j8:          if u > Wi, j then9:              Wi, j = u;Figure 1.
Algorithm of searching for monolingualpseudo-words (SMP).In this algorithm, Wi, j records maximal sum ofmonolingual sequence significances of sub spansof span[i, j].
During initialization, Wi, i is initial-ized as Sigi,i (note that this sequence is word wionly).
For all spans that have more than oneword (i?j), Wi, j is initialized as zero.In the main algorithm, d represents span?slength, ranging from 2 to n, i represents start po-sition of a span, j represents end position of aspan, k represents decomposition position ofspan[i,j].
For span[i, j], Wi, j is updated if highersum of monolingual sequence significances isfound.The algorithm is performed in a bottom-upway.
Small span?s computation is first.
Aftermaximal sum of significances is found in smallspans, big span?s computation, which uses smallspans?
maximal sum, is continued.
Maximal sumof significances for whole sentence (W1,n, n issentence?s length)  is guaranteed in this way, andoptimal decomposition is obtained correspond-ingly.150The method of fitting the decomposition prob-lem into CKY parsing framework is located atsteps 7-9.
After steps 3-6, all possible decompo-sitions of span[i, j] are explored and Wi, j of op-timal decomposition of span[i, j] is recorded.Then monolingual sequence significance Sigi,j ofspan[i, j] is computed at step 7, and it is com-pared to Wi, j at step 8.
Update of Wi, j is taken atstep 9 if Sigi,j is bigger than Wi, j, which indicatesthat span[i, j] is non-decomposable.
Thuswhether span[i, j] should be non-decomposableor not is decided through steps 7-9.2.2.2 Algorithm of Synchronous Searchingfor Pseudo-words (SSP)Synchronous searching for pseudo-words utilizesbilingual sequence significance.
Figure 2 pre-sents the search algorithm.
It is similar to ITG(Wu, 1997), except that it has no productionrules and non-terminal nodes of a synchronousgrammar.
What it cares about is the span-pairsthat maximize the sum of bilingual sequence sig-nificances.Initialization:  if is = js or it = jt thenttssttssttssjijijiji SigW ,,,,,, = ;else0,,, =jijiW ;1:  for ds = 2 ?
ns, dt = 2 ?
nt do2:      for all  is, js, it, jt s.t.
js-is=ds-1 and jt-it=dt-1 do3:             for ks = is ?
js ?
1, kt = it ?
jt ?
1 do4:                    v = max{ ,ttssttss jkjkkikiWW ,1,,1,,, +++ttssttjiji ,,,tj,,,tj,,,jiji ,,,tss kijkjkkiWW ,,,1,1,, ++ + }5:                    if v > W  thentss6:                           W = v;tss iji7:              u =ttss jijiSig ,,,8:              if u > W  thentss iji9:                    W = u;ttssFigure 2.
Algorithm of Synchronous Searching forPseudo-words(SSP).In the algorithm, records maximalsum of bilingual sequence significances of subspan-pairs of span-pair[ittss jijiW ,,,s, js, it, jt].
For 1-to-mspan-pairs, Ws are initialized as bilingual se-quence significances of such span-pairs.
Forother span-pairs, Ws are initialized as zero.In the main algorithm, ds/dt denotes the lengthof a span on source/target side, ranging from 2 tons/nt (source/target sentence?s length).
is/it is thestart position of a span-pair on source/target side,js/jt is the end position of a span-pair onsource/target side, ks/kt is the decomposition po-sition of a span-pair[is, js, it, jt] on source/targetside.Update steps in Figure 2 are similar to that ofFigure 1, except that the update is about span-pairs, not monolingual spans.
Reversed and non-reversed alignments inside a span-pair are com-pared at step 4.
For span-pair[is, js, it, jt],is updated at step 6 if higher sum ofbilingual sequence significances is found.ttss jijiW ,,,Fitting the bilingually searching for pseudo-words into ITG framework is located at steps 7-9.Steps 3-6 have explored all possible decomposi-tions of span-pair[is, js, it, jt] and have recordedmaximalttssof these decompositions.
Thenbilingual sequence significance of span-pair[ijijiW ,,,s, js,it, jt] is computed at step 7.
It is compared tottssat step 8.
Update is taken at step 9 ifbilingual sequence significance of span-pair[ijijiW ,,,s, js,it, jt] is bigger thanttss, which indicates thatspan-pair[ijijiW ,,,s, js, it, jt] is non-decomposable.Whether the span-pair[is, js, it, jt] should be non-decomposable  or not is decided through steps 7-9.In addition to the initialization step, all span-pairs?
bilingual sequence significances are com-puted.
Maximal sum of bilingual sequence sig-nificances for one sentence pair is guaranteedthrough this bottom-up way, and the optimal de-composition of the sentence pair is obtained cor-respondingly.z Algorithm of Excluded SynchronousSearching for Pseudo-words (ESSP)The algorithm of SSP in Figure 2 explores allspan-pairs, but it neglects NULL alignments,where words and ?empty?
word are aligned.
Infact, SSP requires that all parts of a sentence pairshould be aligned.
This requirement is too strongbecause NULL alignments are very common inmany language pairs.
In SSP, words that shouldbe aligned to ?empty?
word are programmed tobe aligned to real words.Unlike most word alignment methods (Ochand Ney, 2003) that add ?empty?
word to ac-count for NULL alignment entries, we propose amethod to naturally exclude such NULL align-ments.
We call this method as Excluded Syn-chronous Searching for Pseudo-words (ESSP).The main difference between ESSP and SSP isin steps 3-6 in Figure 3.
We illustrate Figure 3?sspan-pair configuration in Figure 4.151Initialization:  if is = js or it = jt thenttssttss jijijiji ,,,,,,,,, jijiWSigW = ;else0=ttss;1:  for ds = 2 ?
ns, dt = 2 ?
nt do2:        for all  is, js, it, jt s.t.
js-is=ds-1 and jt-it=dt-1 do3:              for ks1=is+1 ?
js, ks2=ks1-1 ?
js-1kt1=it+1 ?
jt, kt2=kt1-1 ?
jt-1 do4:                    v = max{W ,ttssttss jkjkkikiW ,1,,11,,1, 2211 ++??
+1,,,1,1, 122 ?++ + ttsstt kijkjk Wtt j,,,tj,,,Sigtt ji ,,,ttss jiji ,,,1, 1?ss kiW }5:                    if v > W  thenss iji6:                           W = v;tss iji7:               u =ttss jiji ,,,8:               if u > W  thenss ji9:                    W = u;Figure 3.
Algorithm of Excluded SynchronousSearching for Pseudo-words (ESSP).The solid boxes in Figure 4 represent excludedparts of span-pair[is, js, it, jt] in ESSP.
Note that,in SSP, there is no excluded part, that is, ks1=ks2and kt1=kt2.We can see that in Figure 4, each monolingualspan is configured into three parts, for example:span[is, ks1-1], span[ks1, ks2] and span[ks2+1, js]on source language side.
ks1 and ks2 are two newvariables gliding between is and js, span[ks1, ks2]is source side excluded part of span-pair[is, js, it,jt].
Bilingual sequence significance is computedonly on pairs of blank boxes, solid boxes are ex-cluded in this computation to represent NULLalignment cases.Figure 4.
Illustration of excluded configuration.Note that, in Figure 4, solid box on either lan-guage side can be void (i.e., length is zero) ifthere is no NULL alignment on its side.
If allsolid boxes are shrunk into void, algorithm ofESSP is the same to SSP.Generally, span length of NULL alignment isnot very long, so we can set a length thresholdfor NULL alignments, eg.
ks2-ks1?EL, where ELdenotes Excluded Length threshold.
Computa-tional complexity of the ESSP remains the sameto SSP?s complexity O(ns3.nt3), except multiply aconstant EL2.There is one kind of NULL alignments thatESSP can not consider.
Since we limit excludedparts in the middle of a span-pair, the algorithmwill end without considering boundary parts of asentence pair as NULL alignments.3 Experiments and ResultsIn our experiments, pseudo-words are fed intoPB-SMT pipeline.
The pipeline uses GIZA++model 4 (Brown et al, 1993; Och and Ney, 2003)for pseudo-word alignment, uses Moses (Koehnet al, 2007) as phrase-based decoder, uses theSRI Language Modeling Toolkit to train lan-guage model with modified Kneser-Ney smooth-ing (Kneser and Ney 1995; Chen and Goodman1998).
Note that MERT (Och, 2003) is still onoriginal words of target language.
In our experi-ments, pseudo-word length is limited to no morethan six unary words on both sides of the lan-guage pair.We conduct experiments on Chinese-to-English machine translation.
Two data sets areadopted, one is small corpus of IWSLT-2008BTEC task of spoken language translation intravel domain (Paul, 2008), the other is largecorpus in news domain, which consists HongKong News (LDC2004T08), Sinorama Magazine(LDC2005T10), FBIS (LDC2003E14), Xinhua(LDC2002E18), Chinese News Translation(LDC2005T06), Chinese Treebank(LDC2003E07), Multiple Translation Chinese(LDC2004T07).
Table 1 lists statistics of thecorpus used in these experiments.is ks1 ks2 jsit kt1 kt2 jtis ks1 ks2 jsit kt1 kt2 jta) non-reversedb) reversedsmall largeCh ?
En Ch ?
EnSent.
23k 1,239kword 190k 213k 31.7m 35.5mASL 8.3 9.2 25.6 28.6Table 1.
Statistics of corpora, ?Ch?
denotes Chinese,?En?
denotes English, ?Sent.?
row is the number ofsentence pairs, ?word?
row is the number of words,?ASL?
denotes average sentence length.152For small corpus, we use CSTAR03 as devel-opment set, use IWSLT08 official test set for test.A 5-gram language model is trained on Englishside of parallel corpus.
For large corpus, we useNIST02 as development set, use NIST03 as testset.
Xinhua portion of the English Gigaword3corpus is used together with English side of largecorpus to train a 4-gram language model.Experimental results are evaluated by case-insensitive BLEU-4 (Papineni et al, 2001).Closest reference sentence length is used forbrevity penalty.
Additionally, NIST score (Dod-dington, 2002) and METEOR (Banerjee and La-vie, 2005) are also used to check the consistencyof experimental results.
Statistical significance inBLEU score differences was tested by pairedbootstrap re-sampling (Koehn, 2004).3.1 Baseline PerformanceOur baseline system feeds word into PB-SMTpipeline.
We use GIZA++ model 4 for wordalignment, use Moses for phrase-based decoding.The setting of language model order for eachcorpus is not changed.
Baseline performances ontest sets of small corpus and large corpus are re-ported in table 2.small LargeBLEU 0.4029 0.3146NIST 7.0419 8.8462METEOR 0.5785 0.5335Table 2.
Baseline performances on test sets of smallcorpus and large corpus.3.2 Pseudo-word UnpackingBecause pseudo-word is a kind of multi-wordexpression, it has inborn advantage of higherlanguage model order and longer max phraselength over unary word.
To see if such inbornadvantage is the main contribution to the per-formance or not, we unpack pseudo-word intowords after GIZA++ aligning.
Aligned pseudo-words are unpacked into m?n word alignments.PB-SMT pipeline is executed thereafter.
The ad-vantage of longer max phrase length is removedduring phrase extraction, and the advantage ofhigher order of language model is also removedduring decoding since we use language modeltrained on unary words.
Performances of pseudo-word unpacking are reported in section 3.3.1 and3.4.1.
Ma and Way (2009) used the unpackingafter phrase extraction, then re-estimated phrasetranslation probability and lexical reorderingmodel.
The advantage of longer max phraselength is still used in their method.3.3 Pseudo-word Performances on SmallCorpusTable 3 presents performances of SMP, SSP,ESSP on small data set.
pwchpwen denotes thatpseudo-words are on both language side of train-ing data, and they are input strings during devel-opment and testing, and translations are alsopseudo-words, which will be converted to wordsas final output.
wchpwen/pwchwen denotes thatpseudo-words are adopted only on Eng-lish/Chinese side of the data set.We can see from table 3 that, ESSP attains thebest performance, while SSP attains the worstperformance.
This shows that excluding NULLalignments in synchronous searching for pseudo-words is effective.
SSP puts overly strong align-ment constraints on parallel corpus, which im-pacts performance dramatically.
ESSP is superiorto SMP indicating that bilingually motivatedsearching for pseudo-words is more effective.Both SMP and ESSP outperform baseline consis-tently in BLEU, NIST and METEOR.There is a common phenomenon among SMP,SSP and ESSP.
wchpwen always performs betterthan the other two cases.
It seems that Chineseword prefers to have English pseudo-wordequivalence which has more than or equal to oneword.
pwchpwen in ESSP performs similar to thebaseline, which reflects that our direct pseudo-word pairs do not work very well with GIZA++alignments.
Such disagreement is weakened byusing pseudo-words on only one language side(wchpwen or pwchwen), while the advantage ofpseudo-words is still leveraged in the alignments.Best ESSP (wchpwen) is significantly betterthan baseline (p<0.01) in BLEU score, best SMP(wchpwen) is significantly better than baseline(p<0.05) in BLEU score.
This indicates thatpseudo-words, through either monolingualsearching or synchronous searching, are moreeffective than words as to being basic transla-tional units.Figure 5 illustrates examples of pseudo-wordsof one Chinese-to-English sentence pair.
Goldstandard word alignments are shown at the bot-tom of figure 5.
We can see that ?front desk?
isrecognized as one pseudo-word in ESSP.
Be-cause SMP performs monolingually, it can notconsider ????
and ?front desk?
simultaneously.SMP only detects frequent monolingual multi-words as pseudo-words.
SSP has a strong con-straint that all parts of a sentence pair should bealigned, so source sentence and target sentencehave same length after merging words into153Table 3.
Performance of using pseudo-words on small data.pseudo-words.
We can see that too many pseudo-words are detected by SSP.Figure 5.
Outputs of the three algorithms ESSP,SMP and SSP on one sentence pair and gold standardword alignments.
Words in one pseudo-word are con-catenated by ?_?.3.3.1 Pseudo-word Unpacking Perform-ances on Small CorpusWe test pseudo-word unpacking in ESSP.
Table4 presents its performances on small corpus.unpackingESSPpwchpwen wchpwen pwchwenbaselineBLEU 0.4097 0.4182 0.4031 0.4029NIST 7.5547 7.2893 7.2670 7.0419METEOR 0.5951 0.5874 0.5846 0.5785Table 4.
Performances of pseudo-word unpacking onsmall corpus.We can see that pseudo-word unpacking sig-nificantly outperforms baseline.
wchpwen is sig-nificantly better than baseline (p<0.04) in BLEUscore.
Unpacked pseudo-word performs com-paratively with pseudo-word without unpacking.There is no statistical difference between them.
Itshows that the improvement derives frompseudo-word itself as basic translational unit,does not rely very much on higher languagemodel order or longer max phrase length setting.3.4 Pseudo-word Performances on LargeCorpusTable 5 lists the performance of using pseudo-words on large corpus.
We apply SMP on thistask.
ESSP is not applied because of its highcomputational complexity.
Table 5 shows that allthree configurations (pwchpwen, wchpwen, pwchwen)of SMP outperform the baseline.
If we go back tothe definition of sequence significance, we cansee that it is a data-driven definition that utilizescorpus frequencies.
Corpus scale has an influ-ence on computation of sequence significance inlong sentences which appear frequently in newsdomain.
SMP benefits from large corpus, andwchpwen is significantly better than baseline(p<0.01).
Similar to performances on small cor-pus, wchpwen always performs better than theother two cases, which indicates that Chineseword prefers to have English pseudo-wordequivalence which has more than or equal to oneword.SMPpwchpwen wchpwen pwchwenbaselineBLEU 0.3185 0.3230 0.3166 0.3146NIST 8.9216 9.0447 8.9210 8.8462METEOR 0.5402 0.5489 0.5435 0.5335Table 5.
Performance of using pseudo-words on largecorpus.3.4.1 Pseudo-word Unpacking Perform-ances on Large CorpusTable 6 presents pseudo-word unpacking per-formances on large corpus.
All three configura-tions improve performance over baseline afterpseudo-word unpacking.
pwchpwen attains thebest BLEU among the three configurations, andis significantly better than baseline (p<0.03).wchpwen is also significantly better than baseline(p<0.04).
By comparing table 6 with table 5, wecan see that unpacked pseudo-word performscomparatively with pseudo-word without un-packing.
There is no statistical difference be-SMP SSP ESSPpwchpwen wchpwen pwchwen pwchpwen wchpwen pwchwen pwchpwen wchpwen pwchwenbaselineBLEU 0.3996 0.4155 0.4024 0.3184 0.3661 0.3552 0.3998 0.4229 0.4147 0.4029NIST 7.4711 7.6452 7.6186 6.4099 6.9284 6.8012 7.1665 7.4373 7.4235 7.0419METEOR 0.5900 0.6008 0.6000 0.5255 0.5569 0.5454 0.5739 0.5963 0.5891 0.5785??
?
??
?
?
??
?The guy at the front desk is pretty rude .??
?
??
?
?
??
?The guy_at the front_desk is pretty_rude .??
?
??
?
?
??
?The guy at the front_desk is pretty rude .ESSP??
?
??
?
?
??
?The guy at the front desk is pretty rude  .Gold standard word alignmentsSMPSSP154tween them.
It shows that the improvement de-rives from pseudo-word itself as basic transla-tional unit, does not rely very much on higherlanguage model order or longer max phraselength setting.
In fact, slight improvement inpwchpwen and pwchwen is seen after pseudo-wordunpacking, which indicates that higher languagemodel order and longer max phrase length im-pact the performance in these two configurations.UnpackingSMPpwchpwen wchpwen pwchwenBaselineBLEU 0.3219 0.3192 0.3187 0.3146NIST 8.9458 8.9325 8.9801 8.8462METEOR 0.5429 0.5424 0.5411 0.5335Table 6.
Performance of pseudo-word unpacking onlarge corpus.3.5 Comparison to English ChunkingEnglish chunking is experimented to comparewith pseudo-word.
We use FlexCRFs (Xuan-Hieu Phan et al, 2005) to get English chunks.Since there is no standard Chinese chunking dataand code, only English chunking is executed.The experimental results show that Englishchunking performs far below baseline, usually 8absolute BLEU points below.
It shows that sim-ple chunks are not suitable for being basic trans-lational units.4 ConclusionWe have presented pseudo-word as a novel ma-chine translational unit for phrase-based machinetranslation.
It is proposed to replace too fine-grained word as basic translational unit.
Pseudo-word is a kind of basic multi-word expressionthat characterizes minimal sequence of consecu-tive words in sense of translation.
By castingpseudo-word searching problem into a parsingframework, we search for pseudo-words in poly-nomial time.
Experimental results of Chinese-to-English translation task show that, in phrase-based machine translation model, pseudo-wordperforms significantly better than word in bothspoken language translation domain and newsdomain.
Removing the power of higher orderlanguage model and longer max phrase length,which are inherent in pseudo-words, shows thatpseudo-words still improve translational per-formance significantly over unary words.ReferencesS.
Banerjee, and A. Lavie.
2005.
METEOR: Anautomatic metric for MT evaluation with im-proved correlation with human judgments.
InProceedings of the ACL Workshop on Intrinsic andExtrinsic Evaluation Measures for Machine Trans-lation and/or Summarization (ACL?05).
65?72.P.
Blunsom, T. Cohn, C. Dyer, M. Osborne.
2009.
AGibbs Sampler for Phrasal SynchronousGrammar Induction.
In Proceedings of ACL-IJCNLP, Singapore.P.
Blunsom, T. Cohn, M. Osborne.
2008.
Bayesiansynchronous grammar induction.
In Proceed-ings of NIPS 21, Vancouver, Canada.P.
Brown, S. Della Pietra, V. Della Pietra, and R.Mercer.
1993.
The mathematics of machinetranslation: Parameter estimation.
Computa-tional Linguistics, 19:263?312.P.-C. Chang, M. Galley, and C. D. Manning.
2008.Optimizing Chinese word segmentation formachine translation performance.
In Proceed-ings of the 3rd Workshop on Statistical MachineTranslation (SMT?08).
224?232.Chen, Stanley F. and Joshua Goodman.
1998.
Anempirical study of smoothing techniques forlanguage modeling.
Technical Report TR-10-98,Harvard University Center for Research in Com-puting Technology.C.
Cherry, D. Lin.
2007.
Inversion transductiongrammar for joint phrasal translation model-ing.
In Proc.
of the HLTNAACL Workshop onSyntax and Structure in Statistical Translation(SSST 2007), Rochester, USA.D.
Chiang.
2007.
Hierarchical phrase-basedtranslation.Computational Linguistics, 33(2):201?228.Y.
Deng and W. Byrne.
2005.
HMM word andphrase alignment for statistical machine trans-lation.
In Proc.
of HLT-EMNLP, pages 169?176.G.
Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram cooc-currence statistics.
In Proceedings of the 2nd In-ternational Conference on Human Language Tech-nology (HLT?02).
138?145.Kneser, Reinhard and Hermann Ney.
1995.
Improvedbacking-off for M-gram language modeling.
InProceedings of the IEEE International Conferenceon Acoustics, Speech, and Signal Processing,pages 181?184, Detroit, MI.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.Federico, N. Bertoldi, B. Cowan,W.
Shen, C.Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,E.
Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proc.
of the15545th Annual Meeting of the ACL (ACL-2007),Prague.P.
Koehn, F. J. Och, D. Marcu.
2003.
Statisticalphrasebased translation.
In Proc.
of the 3rd In-ternational conference on Human Language Tech-nology Research and 4th Annual Meeting of theNAACL (HLT-NAACL 2003), 81?88, Edmonton,Canada.P.
Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proceed-ings of EMNLP.P.
Lambert and R. Banchs.
2005.
Data InferredMulti-word Expressions for Statistical Ma-chine Translation.
In Proceedings of MT SummitX.Y.
Ma, N. Stroppa, and A.
Way.
2007.
Bootstrap-ping word alignment via word packing.
In Pro-ceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics (ACL?07).304?311.Y.
Ma, and A.
Way.
2009.
Bilingually MotivatedWord Segmentation for Statistical MachineTranslation.
In ACM Transactions on Asian Lan-guage Information Processing, 8(2).D.
Marcu,W.Wong.
2002.
A phrase-based, jointprobability model for statistical machinetranslation.
In Proc.
of the 2002 Conference onEmpirical Methods in Natural Language Process-ing (EMNLP-2002), 133?139, Philadelphia.
Asso-ciation for Computational Linguistics.F.
J. Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL,pages 160?167.F.
J. Och and H. Ney.
2003.
A systematic compari-son of various statistical alignment models.Computational Linguistics, 29(1):19?51.Xuan-Hieu Phan, Le-Minh Nguyen, and Cam-TuNguyen.
2005.
FlexCRFs: Flexible ConditionalRandom Field Toolkit, http://flexcrfs.sourceforge.netK.
Papineni, S. Roukos, T. Ward, W. Zhu.
2001.
Bleu:a method for automatic evaluation of machinetranslation, 2001.M.
Paul, 2008.
Overview of the IWSLT 2008evaluation campaign.
In Proc.
of InternationaWorkshop on Spoken Language Translation, 20-21October 2008.A.
Stolcke.
(2002).
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings ofICSLP, Denver, Colorado.D.
Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallelcorpora.
Computational Linguistics, 23(3):377?403.J.
Xu, Zens., and H. Ney.
2004.
Do we need Chi-nese word segmentation for statistical ma-chine translation?
In Proceedings of the ACLWorkshop on Chinese Language ProcessingSIGHAN?04).
122?128.J.
Xu, J. Gao, K. Toutanova, and H. Ney.
2008.Bayesian semi-supervised chinese word seg-mentation for statistical machine translation.In Proceedings of the 22nd International Confer-ence on Computational Linguistics (COLING?08).1017?1024.H.
Zhang, C. Quirk, R. C. Moore, D. Gildea.
2008.Bayesian learning of non-compositionalphrases with synchronous parsing.
In Proc.
ofthe 46th Annual Conference of the Association forComputational Linguistics: Human LanguageTechnologies (ACL-08:HLT), 97?105, Columbus,Ohio.R.
Zhang, K. Yasuda, and E. Sumita.
2008.
Improvedstatistical machine translation by multipleChinese word segmentation.
In Proceedings ofthe 3rd Workshop on Statistical Machine Transla-tion (SMT?08).
216?223.156
