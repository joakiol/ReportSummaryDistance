Composing Questions throughConceptual AuthoringCatalina Hallett?The Open UniversityRichard Power?The Open UniversityDonia Scott?The Open UniversityThis article describes a method for composing fluent and complex natural language ques-tions, while avoiding the standard pitfalls of free text queries.
The method, based on ConceptualAuthoring, is targeted at question-answering systems where reliability and transparencyare critical, and where users cannot be expected to undergo extensive training in questioncomposition.
This scenario is found in most corporate domains, especially in applications thatare risk-averse.
We present a proof-of-concept system we have developed: a question-answeringinterface to a large repository of medical histories in the area of cancer.
We show that the methodallows users to successfully and reliably compose complex queries with minimal training.1.
IntroductionWhere early attempts to build natural language question-answering systems focused onaccessing and presenting information held in (closed domain) databases (e.g., Hendrixet al 1978; Templeton and Burger 1983; Kaplan 1984; Hafner and Godden 1985), theadvent of the World Wide Web has led to a shift towards (open domain) collectionsof texts.
However, despite significant advances in open domain question answeringsince the simple pattern-matching systems of the first TREC competition in 1999,current systems are still largely restricted to simple questions.
They can, for example,successfully find answers to questions like Which is the highest peak in Africa?
or Who firstclimbed Kilimanjaro?
but they cannot correctly answer more complex questions like:What is the median height of the top twelve highest peaks in Africa?Which explorer who climbed Kilimanjaro but not Everest between 1960 and 1995 died in the lastthree years before the age of 55?How many of the explorers who climbed Kilimanjaro but not Everest between 1960 and 1995did so more than three times during that period??
Department of Computing, The Open University, Walton Hall, Milton Keynes, Buckinghamshire,MK7 6AA, UK.
E-mail: D.Scott@open.ac.uk; C.Hallett@open.ac.uk; R.Power@open.ac.uk.The research presented here was supported in part by Medical Research Council grant G0100852under the e-Science GRID Initiative.
Special thanks are due our colleagues on CLEF (Alan Rector,Jeremy Rogers, and James McKay) and to the CLEF clinical collaborators at the Royal Marsden andRoyal Free hospitals?see www.clinical-escience.org.Submission received: 17 July 2005; revised submission received: 3 May 2006; accepted for publication:28 July 2006.?
2007 Association for Computational LinguisticsComputational Linguistics Volume 33, Number 1There are many reasons why such queries are unlikely to be successful.
For example,although the first question is very simple to interpret, a correct answer is unlikely to beavailable (in a retrievable form) in any individual document in the target collection.A question-answering system would thus have to first retrieve the heights of eachof the top twelve highest peaks, probably from different documents, and apply somecalculations to obtain their median height, and then generate a response that aggregatesanswers from multiple documents.
The answer to the second question, on the otherhand, is very simple and likely to be found in a small number of documents, butthe question itself is not trivial to interpret and would require (among other things)resolving the temporal information, correctly assuming that 55 refers to age at the timeof death, and interpreting the negation but not as referring to the climbing of Everestonly within the specified time span.
For the third question, the difficulty comes froma combination of complex question and complex answer.
Retrieving aggregated resultsfrom the World Wide Web also introduces issues of reliability because the sources maynot all be trusted, and there is no guarantee that a different selection of sources wouldnot yield a contrary result.For many applications of question answering, the need for complex questionsand trusted answers is paramount?for example, in the medical, legal, and financialdomains, or indeed in any research area?and it is to this scenario that the work wepresent here applies.
Our goal is to develop a general and intuitive method by whichusers can pose complex queries to data repositories; we are particularly concernedwith scenarios where the users are domain experts (i.e., clinicians, lawyers, financiers,etc.)
rather than database experts, where reliability of the answer is critical, wherethe method of posing questions should be easy to learn, and where the questionsthemselves should be transparent (i.e., clear and unambiguous) to both user andsystem.Current methods for querying databases typically make use of formal querylanguages such as SQL.
These languages are highly technical and require a great dealof training to achieve the level of proficiency required to pose the kinds of complexqueries shown in the previous example.
Successful query composition requires the userto be proficient in the query language and have detailed knowledge of the structureof the database to which the queries are being addressed.
Users also need to befluent in any formal codes employed to refer to entities in the domain (e.g., diseaseclassifications, laws, bank codes).
For example, in the medical domain alone there are alarge number of clinical terminologies and classifications, used for different purposes:Some classifications, such as ICD-9, ICD-10, and OPCS-4, are employed in summarizingthe incidence of diseases and operations on a national or worldwide level; others,such as CPT4 or ICD-9CM, manage the process of billing patients.
Each covers a largenumber of terms and associated codes: SNOMED-CT alone, to name the most widelyused medical terminology, currently contains some 365,000 individual concepts, and isbeing updated continuously (College of American Pathologists 2004).
Finally, becausedatabase languages are not transparent, mistakes in query formulation can be difficultto spot; so even where the system itself may be highly reliable, there is a reasonablechance that?except for very highly experienced database programmers?the returnedanswer may not be an accurate response to the intended question.A well-known alternative to formal database languages is available in visual querysystems, which make use of graphical devices such as forms, diagrams, menus, andpointers to communicate the content of a database to the user.
They are also widelyused in commercial applications, and research shows that they are much preferred overtextual query languages like SQL, especially by casual and non-expert users (Capindale106Hallett, Scott, and Power Composing Questions through Conceptual Authoringand Crawford 1990; Bell and Rowe 1992; Catarci and Santucci 1995).
However, visualinterfaces are also problematic: empirical studies report high error rates by domainexperts using visual object-oriented modeling tools (Kim 1990), and a clear advantageof text over graphics for understanding nested conditional structures (Petre 1995).Natural language clearly provides a more intuitive means for users to pose theirquestions, but this is also highly problematic because queries expressed in free naturallanguage are obviously very sensitive to errors of composition (e.g., misspellings,ungrammaticalities) or processing (at the lexical, syntactic, or semantic level).2.
Natural Language InterfacesIn a typical natural language interface to a database (henceforth NLIDB), the userrequests database records through a query expressed in natural language.
The ques-tion is first parsed and analyzed semantically by a linguistic front-end, which trans-lates it into an intermediate meaning representation language (typically, some formof logic).
The intermediate language expression is then translated into a databaselanguage (usually SQL) that is supported by the underlying database managementsystem.A large number of NLIDBs have been developed in the past 30 years, featuring awide range of techniques.
The general drawback of these systems1 is that they normallyunderstand only a subset of natural language.
Casual users cannot always discernwhich constructions are valid or whether the lack of response from the system is due tothe unavailability of an answer or to an unaccepted input construction.
On the positiveside, natural language is far more expressive than formal database query languagessuch as SQL, so it is generally easier to ask complex questions using natural language(NL) than a database language (a single natural language query will have to be trans-lated into multiple SQL statements).
Natural language queries are not only more user-friendly for the non-expert user, but they also allow easier manipulation of temporalconstructions.Broadly, research in NLIDBs has addressed the following issues:2 domain knowledge acquisition (Frank et al 2005) interpretation of the NL input query, including parsing and semanticdisambiguation, semantic interpretation, and transformation of the queryto an intermediate logical form (Hendrix et al 1978; Zhang et al 1999;Tang and Mooney 2001; Popescu, Etzioni, and Kautz 2003; Kate, Wong,and Mooney 2005) translation to a database query language (Lowden et al 1991;Androutsopoulos 1992) portability (Templeton and Burger 1983; Kaplan 1984; Hafner and Godden1985; Androutsopoulos, Ritchie, and Thanitsch 1993; Popescu, Etzioni, andKautz 2003)1 Leaving aside here the possibility of errors in parsing and interpretation.2 The extent of NLIDBs research is such that it is beyond the scope of this article to reference acomprehensive list of projects in this area.
For a critical review of various NLIDBs, the reader isreferred to Androutsopoulos, Ritchie, and Thanisch (1995).107Computational Linguistics Volume 33, Number 1In order to recover from errors in any of these steps, most advanced NLIDB systems alsoincorporate some sort of cooperative user feedback module that will inform users whenthe system cannot construct their query, and ask for clarification.2.1 Our Solution: A Quasi-NL InterfaceThe solution that we propose partially overlaps with previous research in NLIDBs, inthat a logical representation is constructed using a NL interface, and then mappedinto the database query language.
The difference lies in the nature of the NL interface,which in our case uses a method that we call Conceptual Authoring; this replaces thetraditional method of free text entry followed by automatic interpretation.There are two key ideas to Conceptual Authoring.
The first (captured by ?Con-ceptual?)
is that all editing operations are defined directly on an underlying logicalrepresentation, governed by a predefined ontology.
Instead of typing in text, the userbuilds the logical representation directly, so no problem of interpretation arises.
Thesecond key idea (captured by ?Authoring?)
is that the user interface presents thedeveloping logical representation, and the options for editing it, in a way that istransparent to users?namely, natural language text, possibly supplemented by otherfamiliar media; users therefore feel that they are performing a familiar activity, a kindof guided writing, rather than an unfamiliar activity akin to programming.In general, then, Conceptual Authoring requires that some kind of formal knowl-edge encoding is edited by direct manipulation of a familiar presentation, the presen-tation being generated automatically from the underlying knowledge encoding, andupdated every time knowledge is added (or removed) through an editing operation.The user need not be aware of the underlying formalism any more than a person usinga text editor need be aware of ASCII codes.
Conceptual Authoring therefore dependsentirely on language generation technology; it does not use language interpretationat all.Various applications of Conceptual Authoring are possible, depending on thenature of the underlying knowledge and the presentational medium.
In the queryeditor described in this article, the underlying knowledge is a set of assertions (i.e., anA-box), and the presentational medium is natural language text.
Elsewhere, we haveused the term WYSIWYM (What You See Is What You Meant) for various systems of thiskind that we have developed (Power and Scott 1998): as well as query interfaces theyinclude programs that generate technical documentation in multiple languages.
We use?Conceptual Authoring?
as a more general term that would also cover applications inwhich the underlying knowledge included conceptual definitions and rules as well asassertions, and the presentation medium included diagrams as well as text?provided,of course, that the diagrams were familiar to the relevant subject-matter experts (e.g., amolecular structure diagram if the user were an organic chemist).The basic idea of Conceptual Authoring is that a special kind of natural languagetext is generated in order to present successive states of the underlying logicalrepresentation.
This text includes generic phrases, called ?place-holders?, which markattributes that currently have no value.
Place-holders serve as the locations where newobjects may be added.
By opening a pop-up menu on a place-holder, the user obtainsa list of short (generated) phrases describing the types of objects that are permissiblevalues of the attribute; when one of these options is selected, a new object of thespecified type is added.
New text is then generated to present the modified logicalrepresentation, including the attributes of the new object.
As more information is added108Hallett, Scott, and Power Composing Questions through Conceptual Authoringabout an object, it will be presented by longer spans of text, comprising sentences orperhaps even paragraphs.
These spans of text are also mouse-sensitive, so that theassociated semantic material can be cut or copied.
The cutting operation removes thelogical fragment that was previously the value of an attribute, and stores it in a buffer,where it remains available for pasting into another suitable location.
The text associatedwith the fragment may or may not remain the same, depending on the context of thenew location.As an illustration, suppose that the user wishes to define an event that mightnaturally be expressed by the sentence The doctor examined the patient with a stethoscope.The underlying logical structure could be an event object of type examined, withattributes for actor, actee, and instrument; this will of course be only one amongmany events allowed by the ontology.
To define this content using a ConceptualAuthoring interface, the user begins from a text containing a place-holder for any kindof event.
By clicking on this place-holder, the user obtains a list of event patterns, eachshown as a short phrase corresponding to a specific event type from the ontology:FEEDBACK TEXT[Some event]MENU OF OPTIONS.....consultedexaminedtreatedvisited.....When the user selects examined from this list, an event object of type examined is addedto the underlying semantic model, and the feedback text is regenerated to express thenew event and its attributes (as yet unspecified), which are shown by short phrases insquare brackets (the place-holders).
A color code on place-holders indicates whetheran attribute is obligatory (it must be specified) or optional (it can be left unspecified);here for convenience we use boldface for obligatory, and italics for optional.
By clickingon a place-holder, say the first, the user can now obtain options for specifying thecorresponding attribute (in this case the actor).FEEDBACK TEXT[Some person] examined [some person] [in some way]MENU OF OPTIONS.....doctornursepatient.....109Computational Linguistics Volume 33, Number 1By making successive choices in this way, the user will complete the desired proposi-tion, perhaps through the following sequence:[Some event].
[Some person] examined [some person] [in some way].The doctor examined [some person] [in some way].The doctor examined the patient [in some way].The doctor examined the patient by using a stethoscope.Note that because the feedback text is always generated by the system, we can try todesign feedback texts in a way that minimizes ambiguity.
We might, for instance, preferto avoid the more natural phrase ?with a stethoscope?, which introduces the well-knownPP-attachment ambiguity, in favor of the slightly clumsy but unambiguous alternativeemployed herein.As well as introducing new objects on place-holders, the user can select a spanrepresenting a filled slot and perform Cut or Copy.
For instance, from the feedbacksentence reached in the last example, the user could select the span ?the patient?
andchoose Cut, thus emptying the slot and reinstalling the place-holder:The doctor examined [some person] by using a stethoscope.Having freed up the slot in this way, the user might next select ?The doctor?, chooseCopy, select the place-holder ?
[some person]?, and choose Paste.
The Copy operationhere applies to the actual instance selected: It does not create a new instance of the sametype.
Therefore, after the Paste operation, the doctor instance fills two slots, both theactor and the actee of the event.
The resulting coreference is shown by the wording ofthe feedback text:The doctor examined himself by using a stethoscope.Conceptual Authoring (or WYSIWYM) has been applied as a tool for creatingknowledge content for multilingual generation of instruction manuals (Power and Scott1998; Power, Scott, and Evans 1998; Scott, Power, and Evans 1998) and pharmaceuticalleaflets (Bouayad-Agha et al 2002).
It has also been applied in a tool for posing queriesto a knowledge base of legal and regulatory information about maritime shipping(Piwek et al 2000; Piwek 2002; Evans et al in press).
In some ways the interfaceresembles early menu-based techniques like Tennant, Ross, and Thompson (1983) andMueckstein (1985); however, this resemblance is only superficial, because in thesetechniques the user edits a linguistic structure, whereas in Conceptual Authoring allediting operations are defined on an underlying logical structure.3.
A Test Application: Electronic Health RecordsTypically, an individual?s medical record is a collection of documents held in his or herdoctor?s office; most people will also have other records held at other sites, such as110Hallett, Scott, and Power Composing Questions through Conceptual Authoringhospitals or clinics they have attended, or specialists they have seen.
These records areprimarily textual, and the record of the average hospital patient will consist of a largenumber of documents?around 100 narratives, plus hundreds of items of structureddata derived from laboratory, pharmacy, or other hospital subsystems.
There is asignificant move?not just by medical providers, but by governments (e.g., the NationalProgramme for Information Technology in Medicine [NPfIT] in the UK, and variouse-Health initiatives in other countries)?to replace or supplement the current form ofpatient records with electronic records; these are intended to be not simply electronictext files of the existing records, but collections of ?messages?
held in databases andaccessible at the point of care.
One of the disadvantages of text-only health records isthat the information contained within them, because it is ?locked into?
the text, is notavailable for statistical manipulation and cannot be easily interrogated.Previous studies (Gorman and Helfand 1995; Ely et al 2000; Jerome et al 2001;Estrella et al 2004; Koonce, Giuse, and Todd 2004), as well as our own preliminaryanalysis, show that free text queries written by medical professionals are mostlycomplex and often highly ambiguous.
From this we conclude that when queryingmedical databases, such users need to be able to construct queries that are complex,both in volume of material and in the organization of this material (e.g., into temporalor conditional constructions).
Traditionally, user interfaces to medical databases havebeen complex visual interfaces that are unsuitable for use by a casual user (Nadkarniand Brandt 1998; Shahar and Cheng 1999).Electronic health records provide a good example of the kind of applicationfor which question-answering systems are required for accessing large collections oftrusted closed-domain data.
Not only is there a requirement for complex queries ofthe sort that are extremely difficult to achieve in current natural language question-answering systems but, for obvious reasons, the veracity of the results of any query iscritical, making it doubly important that queries put to the system, and their resultingresponses, are unambiguous and clearly understandable to the user.
Because the userswill be medical professionals, with great demands on their time, the ease of useof the question-answering system is also extremely important.
We have applied ourConceptual Authoring question-answering method to one such application: the ClinicalE-Science Framework (CLEF).3.1 The Clinical E-Science FrameworkThe Clinical E-Science Framework (CLEF) aims at providing a repository of well-organized clinical histories that can be queried and summarized both for biomedicalresearch and clinical care (Rector et al 2003).
In this context, the purpose of the queryinterface is to provide efficient access to aggregated data for performing a variety oftasks: assisting in diagnosis or treatment, identifying patterns in treatment, selectingsubjects for clinical trials, and monitoring the participants in clinical trials.
Althoughthe CLEF architecture is largely independent of any particular area of medicine, it iscurrently being applied to cancer, in collaboration with the Royal Marsden Hospital inLondon, one of the primary centers for the treatment of cancer in Britain.The current CLEF database repository contains 22,500 patient records,3 containinga total of more than 400,000 database entries, some 3.5 million record components3 At present, the repository contains records of deceased patients only.
In the near future, it will growsignificantly with the addition of live patient records.111Computational Linguistics Volume 33, Number 1and more than 5 gigabytes of data, implemented as a relational database that storespatient records modeled on an archetype for cancer developed by Kalra et al (2001).The information on each patient comes from hundreds of documents, and a single careepisode or clinical problem is likely to be mentioned repeatedly in several documents.Within CLEF, a patient record is organized as a collection of individual entries, eachentry representing an instance of the cancer archetype.3.2 Users and ExtentThe CLEF query system is designed to answer questions relating to patterns in medicalhistories over sets of patients in the repository.
At this point, the system supportsattribute-centric queries asking for aggregated results, such as:{Absolute count/percentage/statistical measure} of patients with certain characteristics.The answers to such queries can be produced by simple interrogation of thedatabase, because they do not require inferences over the repository of patient records.However, the query interface is also coupled with a data-mining module to provideanswers to more complex queries, such asGiven certain conditions, what is the treatment with the highest chance of success for a patientwith certain characteristics?The query interface can also be used for accessing information about individualpatients.The interface is designed for casual and moderate users who are familiar with thesemantic domain of the repository (but not with its actual structure or encoding) andwho require queries of little variance but with relatively high structural complexity.Under this description come three primary types of users, each having a different goalin interrogating the repository: clinicians, who use it for assisting in diagnosis or treatment medical researchers, who use it for identifying patterns in treatment,selecting subjects for clinical trials, or monitoring the participants inclinical trials hospital administrators, who use it for collecting information aboutpatterns of treatment, frequency of tests, hospital admissions, and so onAmong these, we expect those users with little or no knowledge of formal databaselanguages (e.g., SQL) to be the main beneficiaries of the query interface, although in theEvaluation (Section 6) we will show that even for SQL-aware users, the query interfacerepresents an improved alternative to standard SQL.
We also target the interface at userswho are unfamiliar with medical encoding schemes, such as SNOMED or ICD, or whoprefer to use natural language expressions instead of medical codes.3.3 Previous Work on Querying Clinical DatabasesThere are a number of query systems for clinical databases, mostly designed forformulating patient-centric queries and typically using visual interfaces.
For example:112Hallett, Scott, and Power Composing Questions through Conceptual AuthoringFigure 1Architecture of the CLEF query interface.The Columbia-Presbyterian Medical Center Query Builder works off the medicalcenter?s data repository and generates both HL7 and Arden Syntax4 for severalcategories of data; for example, patient demographics, laboratory values, medi-cations, and diagnoses (Wilcox, Hripcsak, and Chen 1997).
The user interface is asimple HTML-based application that allows users to select the type of data theywant to query and to specify constraints on it.TrialDB (formerly ACT/DB) is a clinical study data management system that providesa complex visual interface for formulating attribute-centric temporal queries(Nadkarni and Brandt 1998; Deshpande, Brandt, and Nadkarni 2001, 2003).
Theinterface allows for attributes to be searched and selected by specifying key wordsor part of the attribute name/caption.
Once an attribute is selected, the user mayoptionally specify that an aggregate value for that attribute be returned.
Searchingcriteria can be combined using boolean operators.KNAVE is a visualization and navigation model that enables clinicians to query a specificpatient record for time-oriented raw data, external interventions, abstractions, andtemporal patterns, and to visualize the results of the temporal query (Shahar andCheng 1999).Natural language query interfaces have been used far less extensively and for morerestricted tasks.
For example, the HERMES system (Rivera and Cercone 1998) allowsthe formulation and interpretation of ad hoc queries relating to doctor and patientdemographic information, patients?
personal details, visit information, and insurancecoverage information.
It is designed as an aid to hospital administration, and not toclinical care.4.
The CLEF Query InterfaceThe CLEF Query Tool has four components which are invoked in sequence whenever aquery is posed by the user (Figure 1).
The first is the Query Editor, a natural languageinterface which guides the user in building a clear and valid query.
The output of4 HL7 is an internationally adopted communication language used for healthcare data.
It covers the wholescope of healthcare communication (http://www.hl7.org).
Arden Syntax is a standard specification ofdefining and sharing modular health knowledge bases, providing procedural representations of medicalknowledge and explicit definitions.113Computational Linguistics Volume 33, Number 1this component is a logical representation underlying the query text that the user hascreated.
The second component, the Query Transcoder, converts this logical represen-tation to a Java encoding accepted by the CLEF database management system (DBMS).In this form, the query is sent to the DBMS, which recodes it again into SQL andsubmits it to the database.
The result of the query, usually a list of records for relevantpatients, returns to the third component of the Query Tool, the Result Processor, whichtransforms the raw data into an aggregated representation defining the content of theanswer.
This representation then passes to the fourth and final component, the AnswerRenderer, which configures a convenient display for the user by combining fluent textwith diagrams (tables and charts).We now describe these components in more detail.4.1 Query EditorThe Query Editor allows the user to create a logical representation of the query by meansof Conceptual Authoring.
When beginning a new query, the user is shown a minimallyspecified feedback text based on a model of query structure in this domain; this modelis described in Section 5.
By inserting content in the initial place-holders, the user canbuild up the full text of a query in a few dozen choices, a process that takes a fewminutes once the user has become accustomed to the editing process (for details, seeSection 6).
A query is potentially complete when all obligatory slots have been filled.This is easy for the user to verify because obligatory place-holders are shown in red:When no red text remains, the query is complete.
At this point, the user can hit theSubmit button, whereupon the current A-box is passed to the Query Transcoder.4.2 Query TranscoderThe Query Transcoder takes as input an A-box from the Query Editor, and recodes itin the format expected by the DBMS.
This conversion depends on a mapping betweenthe ontology (or T-box) employed by the Query Editor, and the concepts of the databasearchetype.
The T-box cannot be exactly the same as the archetype, because it has to servea different purpose?that of providing logical representations suitable for generatinglinguistic structures like clauses and nominals.4.3 Result ProcessorThe Result Processor receives the data returned by the DBMS, normally a set of recordsfor relevant patients, and constructs the logical representation of an answer for the user.A typical result set received from the DBMS would list the patients that fulfilled therequirements of the query, and specify, for each patient, the features AGE and GENDERalong with values for each of the query elements.
For example, the queryHow many patients between 30 and 70 years of age, who had a clinical diagnosis of malignantneoplasm of breast and underwent surgery, had a haematoma after surgery?may yield the result set shown in Figure 2.From such data, the Result Processor plans aggregate presentations in whichpatients are grouped according to the age/gender breakdown and the individual queryterms.
For each query term, the data are split into a dynamically determined number ofage groups, and for each age group the patients are further subdivided by gender.114Hallett, Scott, and Power Composing Questions through Conceptual AuthoringFigure 2Example of a result set.4.4 Answer RendererThe data thus organized are presented to the user in three types of formats: tables, chartsand text.
Each individual chart is accompanied by an automatically generated captionthat explains its content.Captions are generated using template-based techniques, where fillers are providedby the same data that were used for generating the chart.
For example, the results wesaw in Figure 2 are presented as an answer that includes the bar chart in Figure 3.This is accompanied by a textual explanation in the form of a caption, a fragment ofwhich reads:Your query has returned 965 patients between 30 and 70 years of age who had a clinicaldiagnosis of malignant neoplasm of breast and underwent surgery.
This chart displaysthe distribution of patients in five age groups according to their gender and time ofhaematoma after surgery.?
In the 30?39 years age group there were 163 patients (2 men and 161 women):151 patients did not have haematoma after surgery, 12 patients had haematomaafter surgery.?
In the 40?49 years age group there were 326 patients (no men and 326 women):304 patients did not have haematoma after surgery, 22 patients had haematomaafter surgery.?
In the 50?59 years age group there were 363 patients (8 men and 355 women):337 patients did not have haematoma after surgery, 26 patients had haematomaafter surgery.?
In the 60?69 years age group there were 110 patients (2 men and 108 women):97 patients did not have haematoma after surgery, 13 patients had haematomaafter surgery.?
In the 70?79 years age group there were 3 patients (one man and two women):two patients did not have haematoma after surgery, one patient had haematomaafter surgery.5.
Query ModelA controlled editing environment is most effective when based on a model of the kindsof queries that users will wish to make.
There is a trade-off here between flexibility andease of use.
If we have no preconceptions about the general nature of queries, we haveto provide users with a wide set of possible patterns, leaving them to search for theparticular pattern they happen to want.
If instead we can assume that the query willbelong to a known set of patterns, the editor can help the user to get started by offeringa manageable list of alternatives, so avoiding the ?blank page?
problem.Investigations on a taxonomy of queries posed by general practitioners in anoutpatient setting has shown that in primary care, queries are relatively simple and115Computational Linguistics Volume 33, Number 1Figure 3Chart generated as part of a response displaying the distribution of patients who developed anddid not develop haematoma according to their age and gender.generally ask for evidence-based advice for treatment decisions (Ely et al 2000).
Forexample, of 64 generic question types, the three most common are:What is the drug of choice for condition X?What is the cause of symptom X?What test is indicated in situation X?In contrast to these findings, our consultation with cancer clinicians revealed thatquestions posed in a clinical research setting tend to have a more complex natureand to be directed at groups of patients, searching for relationships rather than simplevalues:What is the average time of relapse in Acute Myeloid Leukaemia for patients with a completeresponse after two cycles of treatment?Can this time be linked to the cytogenetic findings in the original blood sample?What is the median time between first drug treatment for metastatic breast cancer and death?116Hallett, Scott, and Power Composing Questions through Conceptual AuthoringOur policy in CLEF has been to aim first at a relatively specific model, under theguidance of the relevant experts?clinicians and medical researchers in the area ofcancer.5.1 Elements of a Basic QueryThe structure of a typical query (according to our experts) is shown by the followingrelatively simple example:For all patients with cancer of the pancreas, what is the percentage alive at five years for thosewho had a course of gemcitabine?As can be seen, this query breaks down into three elements: the set of relevant patients,defined by a problem; the partition of this set according to treatment; and the furtherpartition according to outcome, from which the percentage can be calculated.
Formaximum clarity, the Query Editor can format the query so that these three elementsare marked explicitly and presented separately:Relevant subjects: Patients with cancer of the pancreas.Treatment profile: Patients who received a course of gemcitabine.Outcome measure: Percentage of patients alive after five years.Generalizing from this example, we can identify the following basic query pattern:Relevant subjects: Patients with [some diagnosis].Treatment profile: Patients who received [some treatment].Outcome measure: [Measure] of patients [with some status] [at some point in time].An important requirement on this formulation of the query is that it shouldbe unambiguous?namely, that users should understand correctly how the outcomemeasure will be calculated.
We assume that the calculation will proceed through thefollowing steps.
First, retrieve all the patients in the database who satisfy the conditionsin the first two paragraphs (Relevant subjects and Treatment profile)?in this example,all patients with cancer of the pancreas who received a course of gemcitabine.
Call thisset S and let its cardinality (i.e., the number of patients in the set) be C(S).
Next, findthe subset of S also satisfying the outcome condition?in this example, the patients stillalive after five years.
Call this set M and its cardinality C(M).
Finally, divide C(M) byC(S) and express the result as a percentage.With a slight elaboration of this basic pattern, we can obtain a second kind of query,which requests a comparison rather than a single value:For all patients with cancer of the pancreas, compare the percentage alive at five years for thosewho had a course of gemcitabine with those who didn?t.Again this can be presented to the user using a separate paragraph for each element:Relevant subjects: Patients with cancer of the pancreas.Treatment profile: Patients who received a course of gemcitabine, compared withpatients who did not.Outcome measure: Percentage of patients alive after five years.117Computational Linguistics Volume 33, Number 1For a comparison question we need to compute two outcome measures, so the steps inthe calculation have to be elaborated as follows.
First, retrieve two sets of patients, S1and S2, satisfying the conditions that we want to compare.
In the example, S1 will bethe set of patients with cancer of the pancreas who had a course of gemcitabine; S2 willbe the set of patients with the same type of cancer but no gemcitabine treatment.5 Next,for each set, find the subset of patients still alive after five years: Call these subsets M1and M2.
Finally, compute the measures to be compared by dividing C(M1) by C(S1), andC(M2) by C(S2), and expressing the resulting ratios as percentages.5.2 Complex QueriesEach element of a query can be made more complex in two ways.
First, it can be replacedby a conjunction or disjunction, so that the query in a sense becomes several queriesrequiring several answers.
Second, the content of the description can be elaborated, forexample by adding more qualifications.
Here is an example of the first kind:For all patients with a brain glioma, what percentages are still alive at 1, 2, and 5 years if theytake Imatinib Mesylate every day?This can be analyzed as a single relevance group, single treatment, and multipleoutcome measures (survival at 1, 2, and 5 years).
Separate answers for these measureswill be needed.
An example of the second kind is the following:For all patients with cancer of the vulva that is locally advanced and/or metastatic or recurrent,and where this cannot be treated with either surgery or radiotherapy of any kind, what is thesurvival rate for those given Taxol only?We assume this is a single rather than a multiple query, and that separate answers arenot needed for the various conjunctions and disjunctions.
The treatment profile (Taxol)and the outcome measure (survival rate) have a content that can be easily specified?asingle choice from a menu would suffice.
However, the set of relevant patients requiresa very elaborate description because there are so many qualifications.5.3 Multiple Relevance SetsWhen the phrase describing a relevance set includes a conjunction or disjunction, theremay be ambiguity over whether the intended query is single or multiple.
Compare thesethree patterns:(1) For all patients with lung cancer, and for all patients with breast cancer .
.
.
(2) For all patients with lung cancer and breast cancer .
.
.
(3) For all patients with lung cancer or breast cancer .
.
.Here (1) seems a clear case of a multiple query, whereas the others are ambiguousbut tending to a single-query interpetation.
It is hard to eliminate such ambiguitiesaltogether while wording the query in a way that is reasonably natural, but at least wecan impose consistency by using different realization devices for the two cases?for5 These sets are obviously disjoint.118Hallett, Scott, and Power Composing Questions through Conceptual Authoringexample, bulleted lists for conjunctions (or disjunctions) that imply multiple queries,and discourse connectives (and, or) for ones that imply single queries.
For example:Relevant subjects:?
Patients younger than 60 years of age who have had bad prognosismyelodysplastic syndrome only for at least six months?
Patients younger than 60 years of age who have had acutemyelogenous leukaemia caused by bad prognosis myelodysplasticsyndrome for at least six monthsversusRelevant subjects:?
Patients younger than 60 years of age who have either had badprognosis myelodysplastic syndrome only for at least six months oracute myelogenous leukaemia caused by bad prognosis myelodysplasticsyndrome for at least six monthsIn the first we have two relevance sets; in the second we have only one.5.4 Multiple Treatment ProfilesA similar ambiguity is found when several treatment profiles are mentioned.
Eitherthere are several queries, or there is a single query concerning a logical combination ofthe treatments.
The following query text (written as an example by a medical researcher)could be interpreted either way:For all patients younger than 60 years of age who have either had bad prognosis myelodysplasticsyndrome only for at least six months or acute myelogenous leukaemia caused by bad prognosismyelodysplastic syndrome for at least six months, what is the survival rate if you give themintensified remission induction chemotherapy followed by either an autologous or allogeneicbone marrow transplant?Perhaps the researcher?s aim is to compare autologous bone marrow transplantswith allogeneic ones.
Alternatively, it might not matter whether the transplant isautologous or allogeneic provided that it is one or the other, as suggested by the singularverb (?what is the survival?).
In the query interface, the ambiguity can be avoided in thesame way as before, by using bullets to mark separate queries.
For example:Treatment profiles:?
Intensified remission induction chemotherapy followed by anautologous bone marrow transplant?
Intensified remission induction chemotherapy followed by anallogeneic bone marrow transplantversusTreatment profiles:?
Intensified remission induction chemotherapy followed by anautologous or allogeneic bone marrow transplantIn the first we have two treatment profiles and hence separate queries; in the second wehave only one.119Computational Linguistics Volume 33, Number 15.5 Multiple Outcome MeasuresThere are several examples in which survival rates are requested at, say, one year,two years, and five years.
It makes no sense to combine these into a single query, sothey are always interpreted as separate queries.5.6 Comparison QueriesComparison queries are those that ask for certain outcomes of separate groups ofpatients that do not share common diagnosis or treatment profiles.Compare survival rates at 5 years after diagnosis for patients with adenocarcinoma who receivedchemotherapy and patients with invasive ductal carcinoma who received radiotherapy.We represent such queries as two independent queries, with separate profiles.5.7 Elaborate DescriptionsDescriptions are boolean combinations of properties.
A description can be elaborateeither because it contains many boolean operators, or because the properties arethemselves complicated.
The following description of a reference set is elaborate inboth ways:For all patients younger than 60 years of age who have either had bad prognosis myelodysplasticsyndrome only for at least six months or acute myelogenous leukaemia caused by bad prognosismyelodysplastic syndrome for at least six months, what is the survival rate.
.
.
?Complex boolean combinations of this kind often cannot be presented in runningprose without the scopes of the boolean operators becoming unclear.
To avoid thisproblem, the feedback text generator formats complex boolean combinations usinghierarchical layout:Relevant subjects:?
Patients with the following properties:a.
They are younger than 60 years of ageANDb.
They have one of these properties:b1.
They have had bad prognosis myelodysplastic syndrome onlyfor at least six monthsORb2.
They have had acute myelogenous leukaemia caused bybad prognosis myelodysplastic syndrome for at least sixmonthsDescriptions of treatment profiles can be elaborate in the same ways.
The followingexcerpt has two treatment profiles, the first using a combination of AND and NOT, thesecond using AND combined with temporal sequence (marked by THEN).. .
.
compare the survival rates over time for those who had no surgery but did have mitomycin Cinjected into the bladder once a month, with those who had transurethral resection of the tumorand then a single one-time injection of mitomycin C into the bladder.120Hallett, Scott, and Power Composing Questions through Conceptual AuthoringThe corresponding part of the feedback text is laid out as follows:Treatment profiles:?
a.
NO surgeryANDb.
Mitomycin C injected into the bladder once a month?
a. Transurethral resection of the tumorTHENb.
A single one-time injection of mitomycin C into the bladder5.8 Representing TimeQueries about patient records contain many references to events occurring at particulartimes: diagnoses, tests, treatments, deaths, and so forth.
These time specifications arecrucial.
To deal with them effectively, the query tool must meet two requirements.First, the feedback text should express temporal relations naturally and unambiguously,using familiar devices like tenses and adverbials.
Second, the resulting conceptualrepresentation (in the A-box) must be aligned with the fields through which time isrepresented in the database.Like most databases, the CLEF medical records employ several types of time stamp.First of all, an event has a valid time, the moment when it actually took place.
Forexample, if a mastectomy was performed on 29th January 2000, the valid time would besome representation of this day, perhaps ?29-01-2000?, assuming a granularity calibratedin days (rather than hours, weeks, etc.).
Next, an event has a recording time, themoment when it was written down.
Obviously this might differ from the valid time,although they would be the same if the doctor kept prompt records.
We also use aconcept of query time, the moment when a query was formulated by the CLEF user:this is needed in order to interpret deictic time references in the feedback text, basedfor example on tenses or on phrases like ?after 1995?, which can be interpreted to mean?from 1995 until now?.For some events the valid time can be a single moment, specified for example by adate.
For events that last for longer intervals, like a whole course of treatment, two validtime stamps have to be given, one for the start time and one for the end time.
Of coursethis distinction is related to granularity.
With a granularity based on days, a week hasto be treated as an interval with a start date and an end date; with a granularity basedon weeks, the same week could be identified by a single time stamp (e.g., ?W40-2000?,meaning the 40th week of the year 2000).To model time effectively in queries, we need to provide a range of natural andclear options, and map them to the time stamps used in the database.
At present, thetemporal modifiers offered during query editing are as follows: between [date 1] and [date 2]: interpreted as a closed interval [date 1, date 2] after [some date]:interpreted as a closed interval [this date, query time] before [some date] in [this year]: interpreted as [01/01/this year, 31/12/this year] any of the above, where instead of a specific date the user enters an indexevent after the surgery; in this case, the implied time will be computed bythe DBMS instead of explicitly entered by the user121Computational Linguistics Volume 33, Number 1 event1 {while/at the same time as/during event2} : will be interpreted as twooverlapping time intervals corresponding to the two eventsFor example, such time expressions cover queries like: patients diagnosed with cancerbefore 1999, or patients who received chemotherapy within 5 months of surgery.
The interfaceallows Allen?s 13 basic interval relationships to be expressed in natural language (Allen1984).In principle we could require users to associate a time stamp with every eventmentioned in a query; however, by imposing this further requirement on users wewould pay a high price in usability, virtually doubling the number of operations neededin order to complete the query, and damaging the transparency of the resulting text.In the CLEF query interface we have decided instead to associate default values totime descriptions and to make the time stamp anchors visible only on demand in thefeedback text.
The output text will contain all the time stamps, with the values eitherentered by the user or defaulted, so allowing the user to review the query and amend itwhere necessary.6.
EvaluationThe best evaluation of any question-answering system is one which looks at real usersmaking information-seeking requests in real-life contexts.
Because the complete CLEFsystem is not yet ready for deployment, this is impractical at this stage.
However, wehave been able to perform usability tests on the query interface in isolation from thefull system, and this is what we report on here.
Our current study does not coverthe Query to SQL Translation and the Answer Retrieval components, which are part ofthe server components side of the query interface.
This separation is not always possiblein practice.
For example, we cannot at this stage test the full range of queries that canbe constructed in the interface, because some are not yet supported by the back-end.Similarly, we can only assess the time necessary for editing queries, not for retrievinganswers, because this is almost entirely dependent on the communication procedureand on the speed of the SQL translator.We have thus far conducted two formal experiments, to address the followingquestions: Are users able to successfully compose complex queries using the system? Can the system be used with minimal training? Are the queries, as presented in the interface, easily understandable?6.1 Experiment 1: Query CompositionAs mentioned earlier (Section 1), one of the main desiderata behind the design of ourquerying method is that it should be intuitive.
With respect to the system we haveimplemented for CLEF, what this means is that medics and bio-informaticians shouldbe able to pose the kind of complex queries that they require, without the need forextensive training, or for knowledge of the structure or language of the underlyingrepository.
This experiment tests the extent to which our querying method fulfills theserequirements.122Hallett, Scott, and Power Composing Questions through Conceptual AuthoringSubjects.
Fifteen medics and bio-informaticians participated in the experiment.
All hadpreviously been granted clearance6 to see the information in the confidential repositoryof patient records.
All subjects were knowledgeable in the domain of cancer, and allbut two had no knowledge of the representation language of the repository (SQL), orof how the data contained therein were structured; none had any prior experience withthe query-formulation interface.Methodology.
Each subject was given a short (5?10 minute) introduction to the interface,which included a demonstration of the construction of a fairly simple query.
Subjectswere then given a set of four queries, which they were asked to compose using theinterface.
To increase the difficulty of the task, the questions presented to the subjectsavoided, where possible, the wording required by the user interface, so that users wereobliged to think about the meaning rather than to aim for particular target phrases.
Toavoid effects of practice, we varied randomly the order in which the questions in theset were presented to subjects.
Subjects were allowed as much time as they needed tocompose each query.For each subject, we measured the time taken to build each query, and recorded thenumber of operations used for constructing it.Materials.
The materials for the experiment consisted of the following set of four queries:How many patients who received surgical treatment for malignant neoplasm of the centralportion of the breast had no curative radiotherapy?How many patients between the ages of 40 and 60 when they were first diagnosed with lungcancer (malignant neoplasm of bronchus or lung, unspec) received radiotherapy and hada platelet count higher than 300 and a leukocytes count lower than 3?What percentage of patients under the age of 60 treated for breast cancer (malignant neoplasmof breast, unspec) died within 5 years of a mastectomy?How many patients with acute lymphoid leukaemia have been given chemotherapy?These are representative of the query types that emerged from an earlier requirementsanalysis with oncologists and cancer bio-informaticians.
They also vary in their levels ofstructural complexity and in the number of interface operations required to successfullycomplete them.As can be seen, these questions are far more complex than the queries standardlyposed to search engines or to most other interactive query engines (as described,for example in [Hovy, Hermjakob, and Ravichandran 2002]; [Soricut and Brill 2004];[TREC 2005]).Results.
The main finding of this experiment is the achievement of 100% success insubjects?
ability to use the interface for the purpose for which it was intended: Allsubjects successfully composed all queries.
The mean completion time per query was3.9 minutes (noting that subjects were under no time pressure to complete the individual6 By the UK Medical Research Ethics Committee (MREC).123Computational Linguistics Volume 33, Number 1Figure 4Mean completion time for queries in order of occurrence.queries).7 Figure 4, which gives the average time to completion across all subjects, showsthat subjects learned to use the interface quickly: they take much longer on their firstquery, and their performance asymptotes by the time they get to the second query.This effect is confirmed by an analysis of variance (ANOVA)8, which shows a highlysignificant effect of order of presentation (F = 9.8427; p< .0001).
Furthermore, significantdifferences were found between subjects?
performance on the first query they composedcompared to the second, third, and the fourth (each at p < .01 on the Tukey HSDtest).
However, application of the same test showed no significant difference in subjects?performance on the second versus third, second versus fourth, or the third versus fourthcomposed query.Because the queries vary in structural complexity, some will require the user toperform more interface actions than others, and so one would predict a difference insubjects?
performance (i.e., time to completion) on the individual queries; this was borneout by the analysis (ANOVA, F = 5.5015; p < .0028).If the method is easy to learn, one would predict that subjects?
proficiency with theinterface will increase fairly quickly as they move from the first query they encounterto the last, irrespective of complexity.
This can be tested by measuring subjects?performance on the interface in terms of the number of interface operations (mouseclicks and selections) they perform, normalized for complexity: a value of 1 wouldmean that subjects perform twice as many operations as are required; a value of 0 wouldmean that subjects perform the minimal number of required operations (i.e., perfectperformance).
The result of such an analysis is shown in Figure 5.
The picture thatemerges from this is one where, overall, subjects are very efficient, achieving an averagescore of 0.19 over their first four encounters with the method.
They make a fair numberof false starts when composing their first query, but become extremely proficient bythe time they get to their second query, and near perfect by the time they get to thefourth.
Analysis of variance9 shows a highly significant effect of order of presentation(F = 7.4993; p < .0004).
Once again, the Tukey HSD Test shows a significant differencebetween the first query encountered and each of the subsequent ones (p <.
01), and thatthe differences between the second and third, the second and fourth, and the third andfourth, were nonsignificant.7 For the last 5 subjects, all of whom used a version of the interface that had been improved to respondfaster to interface actions, this average went down to 2.7 minutes.8 One-way ANOVA for correlated samples.9 One-way ANOVA for correlated samples.124Hallett, Scott, and Power Composing Questions through Conceptual AuthoringFigure 5Proficiency with the interface as a function of experience.6.2 Experiment 2: Clarity of the QueriesInterfaces to databases based on natural language interpretation inevitably suffer fromthe ambiguity and imprecision of the input texts, unless users can be trained in acontrolled language.
Our method of composing queries avoids this problem altogether:because the natural language feedback text is generated by the system rather than theuser, there is no need for the system to choose among alternative interpretations.
Ofcourse, this does not guarantee that the query text is equally transparent to the user: thiswill depend on the efficacy of our feedback text design?the point we wish to evaluatein the present experiment, which explores the extent to which composed queries, aspresented in the feedback texts, can be clearly understood.Subjects.
Fifteen subjects participated in the experiment.
Of these, ten had previouslyparticipated in Experiment 1; the new subjects had the same profile as those previouslyseen.Methodology.
Subjects were given a paper-based questionnaire containing 24 trials, eachshowing a completed complex query as presented in the interface (i.e., as a ?feedbacktext?).
Each query was associated with three alternative interpretations, presented as fullnatural language questions: only one of these represented the correct meaning; the othertwo represented plausible but incorrect meanings.
Subjects were given a forced-choicetask to identify which of the three alternatives corresponded to the meaning of the givenfeedback text.The queries were presented to all subjects in the same (random) order.
We devisedfive presentation sets, each containing a different ordering of the options for each query,and these were randomly assigned to subjects.
We suggested to subjects that a usefulstrategy might be to read the alternatives before looking at the associated feedback text.There was no time limit.Materials.
The materials comprised four examples, each of six patterns of ambiguity:Type 1: Attachment of temporal expression.
Most events can have a temporal expressionassociated.
When there is more than one event that could be subsumed by a temporalexpression, the text may become ambiguous.
For example:Relevant subjects: patients with a clinical diagnosis of breast cancerTreatment: patients who did not receive adjuvant chemotherapy in the past yearTests: [ ]Outcome: absolute number of patients125Computational Linguistics Volume 33, Number 1Options:10 How many patients diagnosed with breast cancer had no adjuvant chemotherapyin the past year? How many patients treated for breast cancer in the past year had noadjuvant chemotherapy? How many patients diagnosed with breast cancer in the past year had noadjuvant chemotherapy?Type 2: Scope of conjunctions.
Whenever a complex expression contains a combinationof conjunctions and disjunctions, potential ambiguities may occur, especially whencombined with negations or prepositional phrases.
For example:Relevant subjects: patients with a clinical diagnosis of invasive ductal carcinomaTreatment: patients who received breast conservation surgery, no auxillary surgery,and radiotherapyTests:[ ]Outcome: absolute number of patientsOptions: How many patients diagnosed with invasive ductal carcinoma underwent breastconservation surgery, did not undergo auxillary surgery, and receivedradiotherapy? How many patients diagnosed with invasive ductal carcinoma underwentbreast conservation surgery, did not undergo auxillary surgery, and didnot receive radiotherapy? How many patients diagnosed with invasive ductal carcinoma did notundergo breast conservation surgery, did not undergo auxillary surgery,and received radiotherapy?Type 3: Scope of conjunctions plus attachment of temporal expression.
This is an extension ofthe first two cases, where a temporal expression post-modifies an expression that is partof a conjunction of events.
For example:Relevant subjects: patients with a clinical diagnosis of malignant neoplasm,unspecifiedTreatment: patients who received radiotherapy and chemotherapy within 1 yearof the diagnosisTests:[ ]Outcome:absolute number of patientsOptions: How many patients diagnosed with cancer had radiotherapy and chemotherapyboth within 1 year of diagnosis? How many patients diagnosed with cancer had radiotherapy within1 year of diagnosis and also had chemotherapy at any time?10 In the examples that follow, the correct interpretations are indicated with italics.126Hallett, Scott, and Power Composing Questions through Conceptual Authoring How many patients diagnosed with cancer had radiotherapy andchemotherapy and received any kind of treatment within 1 year ofdiagnosis?Type 4: Combination of various query components.
Events in a query can be linked to eachother by various means, including temporal expressions, conjunctions, and disjunc-tions.
Complex combinations may render the feedback text ambiguous.
For example:Relevant subjects: patients with a clinical diagnosis of breast cancer and who hadnausea within 1 year of the chemotherapyTreatment: patients who received [some surgical procedure] [at some point in time]and chemotherapy but no radiotherapy within 1 year of the diagnosisTests:[ ]Outcome: percentage of patients who were alive after 5 years of the diagnosisOptions: What percentage of patients diagnosed with breast cancer who underwent asurgical procedure at any time, received chemotherapy within 1 year of thediagnosis, had nausea within 1 year of the chemotherapy, and received noradiotherapy within 1 year of the diagnosis, survived more than 5 yearsafter diagnosis? What percentage of patients diagnosed with breast cancer who underwenta surgical procedure at any time, received chemotherapy at any time, hadnausea at any time after chemotherapy, and received no radiotherapywithin 1 year of the diagnosis, survived more than 5 years after diagnosis? What percentage of patients diagnosed with breast cancer who underwenta surgical procedure at any time, received chemotherapy within 1 year ofthe diagnosis, had nausea after chemotherapy but within 1 year of thediagnosis, and received no radiotherapy within 1 year of the diagnosis,survived more than 5 years after diagnosis?Type 5: Complex queries, non-ambiguous components.
We introduced this category in orderto test the readability of complex queries that do not necessarily contain ambiguouscomponents.
Because most queries in the medical domain are likely to be very complex,can the sheer number of query components render the query ambiguous to the users?For example:Relevant subjects: patients under the age of 50 at the time of diagnosis, with aclinical diagnosis of breast cancerTreatment: patients who received [some surgical procedure] [at some point in time]and no chemotherapy within 1 year of the diagnosisTests: [ ]Outcome: absolute number of patientsOptions How many patients with breast cancer, under the age of 50, had a surgicalprocedure at any time and did not have chemotherapy within 1 year of thediagnosis?127Computational Linguistics Volume 33, Number 1 How many patients with breast cancer, under the age of 50, had a surgicalprocedure within one year of the diagnosis and did not havechemotherapy within one year of the diagnosis? How many patients with breast cancer, below the age of 50, had a surgicalprocedure within one year of the diagnosis and had chemotherapy afterone year of the diagnosis?Type 6: Attachment/interpretation of outcome.
The outcome section generally describes acondition holding between a reference and a target set of patients.
If the query containsmultiple features describing the patient set, it may be difficult to differentiate betweenfeatures that contribute to the reference set and features that contribute to the target set.For example:Relevant subjects: patients with a clinical diagnosis of breast cancer and who hadanaemia after chemotherapyTreatment: patients who received chemotherapyTests:[ ]Outcome: percentage of patients who were alive after 5 years from the diagnosisOptions: Of the patients diagnosed with breast cancer who developed anaemia afterchemotherapy, what percentage survived 5 years after diagnosis? Of the patients in the database, what percentage were diagnosed withbreast cancer, developed anaemia after chemotherapy, and survived5 years after diagnosis? Of the patients diagnosed with breast cancer, what percentage developedanaemia after chemotherapy and survived 5 years after diagnosis?Results.
If the presented feedback text is incomprehensible, the probability that subjectswill select the correct interpretation will be 0.33 (i.e., they will get the right answer onlya third of the time).
Our results show that subjects?
precision is 0.84; that is, on average,they select the intended interpretation 84% of the time, rather than 33% as would be predicted iftheir selections were random.
Statistical analysis of these results, using a one-sample t-test,shows this effect to be highly significant (mean = 0.8361, d = 0.5028, t = 16.76, p < .0001).The breakdown by type of ambiguity is shown in Table 1.Table 1Interpretation of feedback texts.Ambiguity Total correct PercentType 1 51 85Type 2 54 90Type 3 50 83Type 4 48 80Type 5 47 78Type 6 51 85128Hallett, Scott, and Power Composing Questions through Conceptual Authoring6.3 Summarizing the EvaluationThe true test of any system comes with its use in situ by the users for which it wasdesigned.
Normally, this would be preceded by a bank of formal empirical studiesunder more controlled conditions.
For a question-answering system like the one we areaddressing in this article (which is but a small part of a much larger system), a formalcontrolled evaluation would ideally cover a large number of exemplars of each type ofquery supported by the system, and a large number of subjects.
Given our constraintson the number of available subjects (and the concomitant effect this has on the possibledesign of any experiments), the evaluation reported here is necessarily more limited inscope.
This is not an unusual situation in system development, where evaluation mustproceed by gradual refinement through the application of rigor, wherever possible,but also applying along the way intuition, common sense, and past experience.
Theevaluation we have presented here shows what can be done during the early phasesof the development of a large and complex system whose components are in differentstages of completion, and where access to representative users is limited.Given these caveats, the picture that emerges from this study is nonetheless veryencouraging.
Our results suggest that our target users (medical researchers) can quicklylearn to construct queries of the type and complexity that they have identified asrelevant.
Specifically: They are able to use the Conceptual Authoring method successfully tocompose complex queries, with no prior exposure to the method andwith the benefit of only minimal training. They become quickly proficient with the system, achieving near perfectperformance by their fourth attempt at query composition.The study also indicates that the feedback texts employed to construct queries of ahigh degree of structural complexity are not difficult to understand.
This is extremelyimportant, as it means that users can be confident that they are obtaining an answerthat pertains to the question that they think they are asking, as opposed to an answer tosome other similar question.Additionally, in a separate, informal study, we have found suggestive evidencethat the Conceptual Authoring method of query composition may be much moreuser-friendly than the traditional method of direct SQL editing, even for extremelyskilled SQL coders with a high level of familiarity with the database and the domain(Hallett, Scott, and Power 2006).
Our tests showed that (an albeit small sample of) suchexperts, even in a situation that is heavily biased towards optimal performance ofSQL codes, found it much easier to compose queries with the Conceptual Authoringinterface than in SQL.
Not only did it take them more than three times longer, onaverage, to compose the query in SQL, but they were not able to produce the completeSQL in that time.7.
ConclusionMost question-answering systems make use of natural language understanding andallow users to pose simple questions to textual repositories.
We have presentedhere a generic method for composing natural language questions within a question-answering system that avoids the well-know pitfalls of natural language understanding129Computational Linguistics Volume 33, Number 1while allowing users to pose complex questions to data repositories.
The method,Conceptual Authoring, involves no natural language interpretation?only generation?and is particularly well-suited to query interfaces to closed-domain systems.
We haveelucidated the method through its use in the CLEF query tool, which has been designedto meet the requirements of a particular context: the querying of large repositoriesof electronic health records by doctors and medical researchers.
Similar requirementsalmost certainly apply in other fields of expertise (e.g., engineering, genomics, law,finance), as data are increasingly available in machine-usable electronic form; they canbe summarized as follows: Users: The query tool must be usable by the relevant domainexperts?doctors, lawyers, or whomever?with no training in databasequery languages. Training: The users must be able to use the query tool after minimallearning time (minutes rather than hours). Time: After training, users must be able to construct complex queries ina time comparable to writing the query down on paper?that is, a fewminutes. Reliability: The query tool must be close to 100% reliable, in the sense thatany query correctly formed by the user will be correctly transcoded intothe database query language and therefore answered by the system. Transparency: Queries must be presented to users in a form that is clearand unambiguous, so that they know exactly what question they haveasked.Although not exactly a requirement, a practical consideration is that the queries shouldbe frequent and important enough to justify the effort needed to meet the very stringentrequirements on usability and transparency.
There is no point investing in a naturallanguage interface like the CLEF query tool except in contexts where the query resultsare highly valuable.In our view, transparent communication with expert users depends first andforemost on using a familiar medium?the medium the experts use in theirnormal work, which in this case means natural language.
However, as argued inSection 2, traditional natural language interfaces to database systems cannot meet therequirements on reliability and training, because reliable interpretation of an inputtext can be achieved only if the text conforms strictly to a controlled language (whichour users would not have time to learn).
We therefore proposed a modification ofthe traditional approach, in which the semantic representation of the query is editeddirectly, through an interactive feedback text generated by the system.
Otherwise theapproaches are the same: once obtained, the semantic representation is transcoded tothe database query language and passed to the database management system; whenthe answer is returned, it is organized to suit the purposes of users, and presented in afamiliar display such as a text or diagram.Ultimately, the value of such a tool must be proved in everyday use, but ourevaluation study provides some evidence that our approach can meet the requirements.First, our studies were performed with the relevant users, in this case medical experts.The training required for reaching a reliable level of performance was a matter ofminutes?usually a single demonstration followed by a single trial.
Thereafter, most130Hallett, Scott, and Power Composing Questions through Conceptual Authoringusers could formulate fairly complex queries in a reasonably short time (3?4 minutes); incontrast, we have found in informal tests that expert SQL coders take at least three timesas long, while often failing to achieve a complete query (Hallett, Scott, and Power 2006).The reliability achieved over 60 queries was 100%, in the sense that all users managed toformulate all their targets.
Finally, a further experiment showed that the formulationsof the queries in the feedback texts were transparent, with accuracy rates of 80?90% on amultiple-choice comprehension task with a random baseline of 33%.These evaluation results offer strong support for Conceptual Authoring as anapproach to this class of problems: we know of no alternative approach that canachieve similar success in meeting these requirements.
However, there are several areaswhere improvement should be possible.
First, we need to find ways of facilitatingthe development process: building and maintaining a system like the CLEF query toolrequires, at present, the hand-coding of linguistic and conceptual resources; as a step inthis direction, we have developed a method of automatically inferring relevant typesof queries for any database, and automatically constructing resources that inform aConceptual Authoring interface (Hallett 2006).
Second, we cannot be sure yet that thewording of feedback texts is optimal?perhaps with more research the comprehensionrates can be pushed higher.
Finally, we have only begun to explore the possibilities forimproving the GUI for Conceptual Authoring.ReferencesAllen, James.
1984.
Towards a general theoryof action and time.
Artificial Intelligence,23(2):123?154.Androutsopoulos, I.
1992.
Interfacing anatural language front end to a relationaldatabase.
Masters thesis, Department ofArtificial Intelligence, University ofEdinburgh.Androutsopoulos, I., G. Ritchie, and P.Thanitsch.
1993.
An efficient and portablenatural language query interface forrelational databases.
In Proceedings of the6th International Conference on IndustrialEngineering Applications of ArtificialIntelligence and Expert Systems Edinburgh,pages 327?330, Edinburgh.Androutsopoulos, I., G. D. Ritchie, andP.
Thanisch.
1995.
Natural languageinterfaces to databases?an introduction.Natural Language Engineering, 2(1):29?81.Bell, J. E. and L. A. Rowe.
1992.
Anexploratory study of ad hoc querylanguages to databases.
In Proceedings ofthe 8th International Conference on DataEngineering, pages 606?613, Tempe, AZ.Bouayad-Agha, Nadjet, Richard Power,Donia Scott, and Anja Belz.
2002.PILLS: Multilingual generation ofmedical information documents withoverlapping content.
In Proceedingsof the Third International Conference onLanguage Resources and Evaluation,pages 2111?2114, Las Palmas, Spain.Capindale, R. A. and R. G. Crawford.
1990.Using a natural language interface withcasual users.
International Journal ofMan?Machine Studies, 32(3):341?361.Catarci, T. and G. Santucci.
1995.
Are visualquery languages easier to use thantraditional ones?
An experimental proof.In Proceedings of the International Conferenceon Human-Computer Interaction (HCI95).College of American Pathologists, 2004.SNOMED Clinical Terms User Guide.July 2004 release.Deshpande, A., C. Brandt, and P. Nadkarni.2001.
Ad hoc query of patient data:Meeting the needs of clinical studies.Journal of the American Medical InformaticsAssociation, 9(4):369?382.Deshpande, A., C. Brandt, and P. Nadkarni.2003.
Temporal query of attribute-valuepatient data: Utilizing the constraints ofclinical studies.
International Journal ofMedical Informatics, 70:59?77.Ely, John W., Jerome A. Osheroff, Paul N.Gorman, Mark H. Ebell, M. Lee Chambliss,Eric A. Pifer, and P. Zoe Stavri.
2000.
Ataxonomy of generic clinical questions:Classification study.
British Medical Journal,321:429?432.Estrella, Florida, Chiara del Frate, TamasHauer, Richard McClatchey, MohammedOdeh, Dmitry Rogulin, Salvator RobertoAmendolia, David Schottlander,Tony Solomonides, and Ruth Warren.2004.
Resolving clinicians queries131Computational Linguistics Volume 33, Number 1across a grids infrastructure.
InProceedings of the 2nd InternationalHealthGRID Conference.Evans, Roger, Paul Piwek, Lynne Cahill, andNeil Tipper.
In press.
Natural languageprocessing in CLIME, a multilingual legaladvisory system.
Natural LanguageEngineering.Frank, Annette, Hans-Ulrich Krieger, FeiyuXu, Hans Uszkoreit, Berthold Crysmann,Brigitte Jorg, and Ulrich Schafer.
2005.Querying structured knowledge sources.In AAAI-05 Workshop on QuestionAnswering in Restricted Domains, pages10?19, Pittsburgh, Pennsylvania.Gorman, P. N. and M. Helfand.
1995.Information seeking in primary care:How physicians choose which clinicalquestions to pursue and which to leaveunanswered.
Medical Decision Making,(15):113?119.Hafner, Carole D. and Kurt Godden.
1985.Portability of syntax and semantics indatalog.
ACM Transactions on InformationSystems, 3(2):141?164.Hallett, Catalina.
2006.
Generic querying ofrelational databases using naturallanguage generation techniques.
InProceedings of the 4th International NaturalLanguage Generation Conference (INLG?06),pages 95?102, Sydney, Australia.Hallett, Catalina, Donia Scott, and RichardPower.
2006.
Evaluation of the CLEFquery interface.
Technical Report 2006/01,Centre for Research in Computing, TheOpen University.Hendrix, Gary G., Earl D. Sacerdoti, DanielSagalowicz, and Jonathan Slocum.
1978.Developing a natural language interface tocomplex data.
ACM Transactions onDatabase Systems, 3(2):105?147.Hovy, E. H., U. Hermjakob, andD.
Ravichandran.
2002.
A question/answer typology with surface textpatterns.
In Proceedings of the DARPAHuman Language Technology Conference,pages 247?250, San Diego, CA.Jerome, R. N., N. B. Giuse, K. W. Gish,N.
A. Sathe, and M. S. Dietrich.
2001.Information needs of clinical teams:Analysis of questions received by theclinical informatics consult service.Bulletin of the Medical Library Association,89(2):177?184.Kalra, Dipak, Anthony Austin, A. O?Connor,D.
Patterson, David Lloyd, and DavidIngram.
2001.
Design and Implementationof a Federated Health Record Server,pages 1?13.
Medical Records Institutefor the Centre for Advancement ofElectronic Records Ltd.Kaplan, S. Jerrold.
1984.
Designing a portablenatural language database query system.ACM Transactions on Database Systems,9(1):1?19.Kate, R. J., Y. W. Wong, and R. J. Mooney.2005.
Learning to transform natural toformal languages.
In Proceedings of theTwentieth National Conference on ArtificialIntelligence (AAAI-05), pages 1062?1068,Pittsburgh, PA.Kim, Y.
1990.
Effects of conceptual datamodelling formalisms on user validation andanalyst modelling of information requirements.Ph.D.
thesis, University of Minnesota.Koonce, Taneya Y., Nunzia Bettinsoli Giuse,and Pauline Todd.
2004.
Evidence-baseddatabases versus primary medicalliterature: An in-house investigation ontheir optimal use.
Bulletin of the MedicalLibrary Association, 92(4):407?411.Lowden, B. G. T., B. R. Walls, A.
De Roeck,C.
J.
Fox, and R. Turner.
1991.
A formalapproach to translating English into SQL.In Proceedings of the 9th British NationalConference on Databases, pages 110?127,Wolverhampton, UK.Mueckstein, Eva-Martin.
1985.
Controllednatural language interfaces (extendedabstract): The best of three worlds.
In CSC?85: Proceedings of the 1985 ACM thirteenthannual conference on Computer Science,pages 176?178, New York, NY.Nadkarni, P. and C. Brandt.
1998.
Dataextraction and ad hoc query of anentity-attribute-value database.
Journalof the American Medical InformaticsAssociation, 5(6):511?527.Petre, Marian.
1995.
Why looking isn?talways seeing: Readership skills andgraphical programming.
Communicationsof the ACM, 38(6):33?44.Piwek, Paul.
2002.
Requirements definition,validation, verification and evaluationof the clime interface and languageprocessing technology.
Technical ReportITRI-02-03, ITRI, University of Brighton.Piwek, Paul, Roger Evans, Lynne Cahill,and Neil Tipper.
2000.
Natural languagegeneration in the MILE system.
InProceedings of the IMPACTS in NLGWorkshop, pages 33?42, Schloss Dagstuhl,Germany.Popescu, Ana-Maria, Oren Etzioni, andHenry Kautz.
2003.
Towards a theoryof natural language interfaces todatabases.
In IUI ?03: Proceedings ofthe 8th international conference on132Hallett, Scott, and Power Composing Questions through Conceptual AuthoringIntelligent user interfaces, pages 149?157,New York, NY.Power, Richard and Donia Scott.
1998.Multilingual authoring using feedbacktexts.
In Proceedings of 17th InternationalConference on Computational Linguisticsand 36th Annual Meeting of theAssociation for Computational Linguistics(COLING-ACL 98), pages 1053?1059,Montreal, Canada.Power, Richard, Donia Scott, and RogerEvans.
1998.
What you see is what youmeant: direct knowledge editing withnatural language feedback.
In Proceedingsof the 13th Biennial European Conference onArtificial Intelligence, pages 675?681,Brighton, UK.Rector, Alan, Jeremy Rogers, Adel Taweel,David Ingram, Dipak Kalra, Jo Milan,Robert Gaizauskas, Mark Hepple,Donia Scott, and Richard Power.
2003.CLEF?joining up healthcare withclinical and post-genomic research.
InSecond UK E-Science ?All Hands Meeting?,Nottingham, UK.Rivera, Carlos and Nick Cercone.
1998.Hermes: Natural language access to amedical database.
Technical reportCS-98-03, University of Regina, Canada.Scott, Donia, Richard Power, and RogerEvans.
1998.
Generation as a solutionto its own problem.
In Proceedingsof the 9th International Workshop onNatural Language Generation,pages 256?265, Niagara-on-the-Lake,Canada.Shahar, Yuval and Cleve Cheng.
1999.Intelligent visualization and exploration oftime-oriented clinical data.
In Proceedingsof HICSS, pages 4019?4030, Maui, HI.Soricut, R. and E. Brill.
2004.
Automaticquestion answering: Beyond the factoid.In Proceedings of the HLT/NAACL 2004,pages 57?64, Boston, MA.Tang, Lappoon R. and Raymond J. Mooney.2001.
Using multiple clause constructorsin inductive logic programmingfor semantic parsing.
In EMCL ?01:Proceedings of the 12th European Conferenceon Machine Learning, pages 466?477,London, UK.Templeton, Marjorie and John Burger.
1983.Problems in natural-language interface toDBMs with examples from EUFID.
InProceedings of the First Conference on AppliedNatural Language Processing, pages 3?16,Morristown, NJ.Tennant, Harry R., Kenneth M. Ross, andCraig W. Thompson.
1983.
Usable naturallanguage interfaces through menu-basednatural language understanding.
In CHI?83: Proceedings of the SIGCHI Conferenceon Human Factors in Computing Systems,pages 154?160, New York, NY.TREC.
2005.
Question answering data.http://trec.nist.gov/data/qa/t2005qadata.html.Wilcox, Adam, George Hripcsak, andCynthia Chen.
1997.
Creating anenvironment for linking knowledge-basedsystems to a clinical database: A suite oftools.
In Proceedings of AMIA Annual FallSymposium, pages 303?307, Nashville, TN.Zhang, Guogen, Wesley W. Chu, FrankMeng, and Gladys Kong.
1999.
Queryformulation from high-level conceptsfor relational databases.
In UIDIS ?99:Proceedings of the 1999 User Interfacesto Data Intensive Systems, page 64,Washington, DC.133
