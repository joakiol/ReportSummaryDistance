Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546?556,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsModeling Term Translation for Document-informed Machine TranslationFandong Meng1, 2Deyi Xiong3Wenbin Jiang1, 2Qun Liu4, 11Key Laboratory of Intelligent Information ProcessingInstitute of Computing Technology, Chinese Academy of Sciences2University of Chinese Academy of Sciences{mengfandong,jiangwenbin,liuqun}@ict.ac.cn3School of Computer Science and Technology, Soochow Universitydyxiong@suda.edu.cn4Centre for Next Generation Localisation, Dublin City UniversityAbstractTerm translation is of great importance forstatistical machine translation (SMT), es-pecially document-informed SMT.
In thispaper, we investigate three issues of termtranslation in the context of document-informed SMT and propose three cor-responding models: (a) a term trans-lation disambiguation model which se-lects desirable translations for terms in thesource language with domain information,(b) a term translation consistency modelthat encourages consistent translations forterms with a high strength of translationconsistency throughout a document, and(c) a term bracketing model that rewardstranslation hypotheses where bracketablesource terms are translated as a wholeunit.
We integrate the three models intohierarchical phrase-based SMT and eval-uate their effectiveness on NIST Chinese-English translation tasks with large-scaletraining data.
Experiment results showthat all three models can achieve sig-nificant improvements over the baseline.Additionally, we can obtain a furtherimprovement when combining the threemodels.1 IntroductionA term is a linguistic expression that is used asthe designation of a defined concept in a language(ISO 1087).
As terms convey concepts of a text,term translation becomes crucial when the text istranslated from its original language to anotherlanguage.
The translations of terms are often af-fected by the domain in which terms are used andthe context that surrounds terms (Vasconcellos etal., 2001).
In this paper, we study domain-specificand context-sensitive term translation for SMT.In order to achieve this goal, we focus on threeissues of term translation: 1) translation ambigu-ity, 2) translation consistency and 3) bracketing.First, term translation ambiguity is related to trans-lations of the same term in different domains.
Asource language term may have different transla-tions when it occurs in different domains.
Second,translation consistency is about consistent trans-lations for terms that occur in the same document.Usually, it is undesirable to translate the same termin different ways as it occurs in different parts ofa document.
Finally, bracketing concerns whethera multi-word term is bracketable during transla-tion.
Normally, a multi-word term is translated asa whole unit into a contiguous target string.We study these three issues in the contextof document-informed SMT.
We use document-informed information to disambiguate term trans-lations in different documents and maintain con-sistent translations for terms that occur in the samedocument.
We propose three different models forterm translation that attempt to address the threeissues mentioned above.
In particular,?
Term Translation Disambiguation Model: Inthis model, we condition the translations ofterms in different documents on correspond-ing per-document topic distributions.
In do-ing so, we enable the decoder to favor trans-lation hypotheses with domain-specific termtranslations.?
Term Translation Consistency Model: Thismodel encourages the same terms with a highstrength of translation consistency that occurin different parts of a document to be trans-lated in a consistent fashion.
We calculatethe translation consistency strength of a termbased on the topic distribution of the docu-ments where the term occurs in this model.?
Term Bracketing Model: We use the brack-eting model to reward translation hypothe-546ses where bracketable multi-word terms aretranslated as a whole unit.We integrate the three models into hierarchicalphrase-based SMT (Chiang, 2007).
Large-scaleexperiment results show that they are all able toachieve significant improvements of up to 0.89BLEU points over the baseline.
When simulta-neously integrating the three models into SMT,we can gain a further improvement, which outper-forms the baseline by up to 1.16 BLEU points.In the remainder of this paper, we begin witha brief overview of related work in Section 2,and bilingual term extraction in Section 3.
Wethen elaborate the proposed three models for termtranslation in Section 4.
Next, we conduct experi-ments to validate the effectiveness of the proposedmodels in Section 5.
Finally, we conclude and pro-vide directions for future work in Section 6.2 Related WorkIn this section, we briefly introduce related workand highlight the differences between our workand previous studies.As we approach term translation disambigua-tion and consistency via topic modeling, our mod-els are related to previous work that explores thetopic model (Blei et al., 2003) for machine trans-lation (Zhao and Xing, 2006; Su et al., 2012;Xiao et al., 2012; Eidelman et al., 2012).
Zhaoand Xing (2006) employ three models that enableword alignment process to leverage topical con-tents of document-pairs with topic model.
Su et al.
(2012) establish the relationship between out-of-domain bilingual corpus and in-domain monolin-gual corpora via topic mapping and phrase-topicdistribution probability estimation for translationmodel adaptation.
Xiao et al.
(2012) propose atopic similarity model for rule selection.
Eidel-man et al.
(2012) use topic models to adapt lexicalweighting probabilities dynamically during trans-lation.
In these studies, the topic model is not usedto address the issues of term translation mentionedin Section 1.Our work is also related to document-levelSMT in that we use document-informed informa-tion for term translation.
Tiedemann (2010) pro-pose cache-based language and translation mod-els, which are built on recently translated sen-tences.
Gong et al.
(2011) extend this by furtherintroducing two additional caches.
They employa static cache to store bilingual phrases extractedfrom documents in training data that are similar tothe document being translated and a topic cachewith target language topic words.
Recently wehave also witnessed efforts that model lexical co-hesion (Hardmeier et al., 2012; Wong and Kit,2012; Xiong et al., 2013a; Xiong et al., 2013b)as well as coherence (Xiong and Zhang, 2013)for document-level SMT.
Hasler et al.
(2014a)use topic models to learn document-level transla-tion probabilities.
Hasler et al.
(2014b) use topic-adapted model to improve lexical selection.
Thesignificant difference between our work and thesestudies is that term translation has not been inves-tigated in these document-level SMT models.Itagaki and Aikawa (2008) employ bilingualterm bank as a dictionary for machine-aided trans-lation.
Ren et al.
(2009) propose a binary featureto indicate whether a bilingual phrase contains aterm pair.
Pinis and Skadins (2012) investigate thatbilingual terms are important for domain adapta-tion of machine translation.
These studies do notfocus on the three issues of term translation asdiscussed in Section 1.
Furthermore, domain anddocument-informed information is not used to as-sist term translation.Itagaki et al.
(2007) propose a statistical methodto calculate translation consistency for terms withexplicit domain information.
Partially inspiredby their study, we introduce a term translationconsistency metric with document-informed infor-mation.
Furthermore, we integrate the proposedterm translation consistency model into an actualSMT system, which has not been done by Itagakiet al.
(2007).
Ture et al.
(2012) use IR-inspiredtf-idf scores to encourage consistent translationchoice.
Guillou (2013) investigates what kind ofwords should be translated consistently.
Termtranslation consistency has not been investigatedin these studies.Our term bracketing model is also relatedto Xiong et al.
(2009)?s syntax-driven bracket-ing model for phrase-based translation, which pre-dicts whether a phrase is bracketable or not usingrich syntactic constraints.
The difference is thatwe construct the model with automatically createdbilingual term bank and do not depend on any syn-tactic knowledge.3 Bilingual Term ExtractionBilingual term extraction is to extract terms fromtwo languages with the purpose of creating or ex-547tending a bilingual term bank, which in turn canbe used to improve other tasks such as informationretrieval and machine translation.
In this paper, wewant to automatically build a bilingual term bankso that we can model term translation to improvetranslation quality of SMT.
Our interest is to ex-tract multi-word terms.Currently, there are mainly two strategies toconduct bilingual term extraction from parallelcorpora.
One of them is to extract term candi-dates separately for each language according tomonolingual term metrics, such as C-value/NC-value (Frantzi et al., 1998; Vu et al., 2008), orother common cooccurrence measures such asLog-Likelihood Ratio, Dice coefficient and Point-wise Mutual Information (Daille, 1996; Piao etal., 2006).
The extracted monolingual terms arethen paired together (Hjelm, 2007; Fan et al.,2009; Ren et al., 2009).
The other strategy is toalign words and word sequences that are transla-tion equivalents in parallel corpora and then clas-sify them into terms and non-terms (Merkel andFoo, 2007; Lefever et al., 2009; Bouamor et al.,2012).
In this paper, we adopt the first strategy.In particular, for each sentence pair, we collect allsource phrases which are terms and find alignedtarget phrases for them via word alignments.
Ifthe target side is also a term, we store the sourceand target term as a term pair.We conduct monolingual term extraction usingthe C-value/NC-value metric and Log-LikelihoodRatio (LLR) measure respectively.
We then com-bine terms extracted according to the two metricsmentioned above.
For the C-value/NC-value met-ric based term extraction, we implement it in thesame way as described in Frantzi et al.
(1998).This extraction method recognizes linguistic pat-terns (mainly noun phrases) listed as follows.((Adj|Noun)+|((Adj|Noun)?(NounPrep)?)(Adj|Noun)?
)NounIt captures the linguistic structures of terms.
Forthe LLR metric based term extraction, we imple-ment it according to Daille (1996), who estimatethe propensity of two words to appear together as amulti-word expression.
We then adopt LLR-basedhierarchical reducing algorithm proposed by Renet al.
(2009) to extract terms with arbitrary lengths.Since the C-value/NC-value metric based extrac-tion method can obtain terms in strict linguisticpatterns while the LLR measure based method ex-tracts more flexible terms, these two methods arecomplementary to each other.
Therefore, we usethese two methods to extract monolingual multi-word terms and then combine the extracted terms.4 ModelsThis section presents the three models of termtranslation.
They are the term translation dis-ambiguation model, term translation consistencymodel and term bracketing model respectively.4.1 Term Translation Disambiguation ModelThe most straightforward way to disambiguateterm translations in different domains is to cal-culate the conditional translation probability ofa term given domain information.
We use thetopic distribution of a document obtained by atopic model to represent the domain informationof the document.
Since Latent Dirichlet Alloca-tion (LDA) (Blei et al., 2003) is the most widely-used topic model, we exploit it for inferring topicdistributions of documents.
Xiao et al.
(2012)proposed a topic similarity model for rule selec-tion.
Different from their work, we take an eas-ier strategy that estimates topic-conditioned termtranslation probabilities rather than rule-topic dis-tributions.
This makes our model easily scalableon large training data.With the bilingual term bank created from thetraining data, we calculate the source-to-targetterm translation probability for each term pair con-ditioned on the topic distribution of the sourcedocument where the source term occurs.
We main-tain a K-dimension (K is the number of topics)vector for each term pair.
The k-th componentp(te|tf, z = k) measures the conditional transla-tion probability from source term tfto target termtegiven the topic k.We calculate p(te|tf, z = k) via maximumlikelihood estimation with counts from trainingdata.
When the source part of a bilingual termpair occurs in a document D with topic distribu-tion p(z|D) estimated via LDA tool, we collectan instance (tf, te, p(z|D), c), where c is the frac-tion count of the instance as described in Chiang(2007).
After collection, we get a set of instancesI = {(tf, te, p(z|D), c)}with different document-topic distributions for each bilingual term pair.
Us-ing these instances, we calculate the probability548p(te|tf, z = k) as follows:p(te|tf, z = k)=?i?I,i.tf=tf,i.te=tei.c ?
p(z = k|D)?i?I,i.tf=tfi.c ?
p(z = k|D)(1)We associate each extracted term pair in ourbilingual term bank with its corresponding topic-conditioned translation probabilities estimated inthe Eq.
(1).
When translating sentences of docu-ment D?, we first get the topic distribution of D?using LDA tool.
Given a sentence which containsT terms {tfi}T1in D?, our term translation disam-biguation model TermDis can be denoted asTermDis =T?i=1Pd(tei|tfi, D?)
(2)where the conditional source-to-target term trans-lation probability Pd(tei|tfi, D?)
given the docu-ment D?is formulated as follows:Pd(tei|tfi, D?
)=K?k=1p(tei|tfi, z = k) ?
p(z = k|D?)
(3)Whenever a source term tfiis translated into tei,we check whether the pair of tfiand its translationteican be found in our bilingual term bank.
If itcan be found, we calculate the conditional transla-tion probability from tfito teigiven the documentD?according to Eq.
(3).The term translation disambiguation model isintegrated into the log-linear model of SMT as afeature.
Its weight is tuned via minimum error ratetraining (MERT) (Och, 2003).
Through the fea-ture, we can enable the decoder to favor translationhypotheses that contain target term translations ap-propriate for the domain represented by the topicdistribution of the corresponding document.4.2 Term Translation Consistency ModelThe term translation disambiguation model helpsthe decoder select appropriate translations forterms that are in accord with their domains.
Yetanother translation issue related to the domain-specific term translation is to what extent a termshould be translated consistently given the domainwhere it occurs.
Term translation consistency in-dicates the translation stability that a source termis translated into the same target term (Itagaki etal., 2007).
When translating a source term, if thetranslation consistency strength of the source termis high, we should take the corresponding targetterm as the translation for it.
Otherwise, we mayneed to create a new translation for it according toits context.
In particular, we want to enable thedecoder to choose between: 1) translating a givensource term into the extracted corresponding tar-get term or 2) translating it in another way accord-ing to the strength of its translation consistency.In doing so, we can encourage consistent transla-tions for terms with a high translation consistencystrength throughout a document.Our term translation consistency model can ex-actly measure the strength of term translation con-sistency in a document.
Since the essential com-ponent of our term translation consistency modelis the translation consistency strength of the sourceterm estimated under the topic distribution, we de-scribe how to calculate it before introducing thewhole model.With the bilingual term bank created fromtraining data, we first group each source termand all its corresponding target terms into a 2-tuple G?tf, Set(te)?, where tfis the source termand Set(te) is the set of tf?s corresponding tar-get terms.
We maintain a K-dimension (K isthe number of topics) vector for each 2-tupleG?tf, Set(te)?.
The k-th component measures thetranslation consistency strength cons(tf, k) of thesource term tfgiven the topic k.We calculate cons(tf, k) for eachG?tf, Set(te)?
with counts from training data asfollows:cons(tf, k) =M?m=1Nm?n=1(qmn?
p(k|m)Qk)2(4)Qk=M?m=1Nm?n=1qmn?
p(k|m) (5)where M is the number of documents in whichthe source term tfoccurs, Nmis the number ofunique corresponding term translations of tfin themth document, qmnis the frequency of the nthtranslation of tfin the mth document, p(k|m) isthe conditional probability of the mth documentover topic k, and Qkis the normalization factor.All translations of tfare from Set(te).
We adaptItagaki et al.
(2007)?s translation consistency met-ric for terms to our topic-based translation consis-tency measure in the Eq.
(4).
This equation cal-culates the translation consistency strength of thesource term tfgiven the topic k according to thedistribution of tf?s translations in each document549where they occur.
According to Eq.
(4), the trans-lation consistency strength is a score between 0and 1.
If a source term only occurs in a documentand all its translations are the same, the translationconsistency strength of this term is 1.We reorganize our bilingual term bank into alist of 2-tuples G?tf, Set(te)?s, each of which isassociated with a K-dimension vector storing thetopic-conditioned translation consistency strengthcalculated in the Eq.
(4).
When translating sen-tences of document D, we first get the topic dis-tribution of D via LDA tool.
Given a sentencewhich contains T terms {tfi}T1in D, our termtranslation consistency model TermCons can bedenoted asTermCons =T?i=1exp(Sc(tfi|D)) (6)where the strength of translation consistency fortfigiven the document D is formulated as fol-lows:Sc(tfi|D) = log(K?k=1cons(tfi, k) ?
p(k|D)) (7)During decoding, whenever a hypothesis justtranslates a source term tfiinto te, we checkwhether the translation tecan be found in Set(te)of tfifrom the reorganized bilingual term bank.
Ifit can be found, we calculate the strength of trans-lation consistency for tfigiven the document Daccording to Eq.
(7) and take it as a soft con-straint.
If the Sc(tfi|D) of tfiis high, the decodershould translate tfiinto the extracted correspond-ing target terms.
Otherwise, the decoder will se-lect translations from outside of Set(te) for tfi.
Indoing so, we encourage terms to be translated ina topic-dependent consistency pattern in the testdata similar to that in the training data so that wecan control the translation consistency of terms inthe test data.The term translation consistency model is alsointegrated into the log-linear model of SMT as afeature.
Through the feature, we can enable thedecoder to translate terms with a high translationconsistency in a document into corresponding tar-get terms from our bilingual term bank rather thanother translations in a consistent fashion.4.3 Term Bracketing ModelThe term translation disambiguation model andconsistency model concern the term translation ac-curacy with domain information.
We further pro-pose a term bracketing model to guarantee the in-tegrality of term translation.
Xiong et al.
(2009)proposed a syntax-driven bracketing model forphrase-based translation, which predicts whethera phrase is bracketable or not using rich syntac-tic constraints.
If a source phrase remains con-tiguous after translation, they refer to this type ofphrase as bracketable phrase, otherwise unbrack-etable phrase.
For multi-word terms, it is alsodesirable to be bracketable since a source termshould be translated as a whole unit and its trans-lation should be contiguous.In this paper, we adapt Xiong et al.
(2009)?sbracketing approach to term translation and builda classifier to measure the probability that a sourceterm should be translated in a bracketable man-ner.
For all source parts of the extracted bilingualterm bank, we find their target counterparts in theword-aligned training data.
If the correspondingtarget counterpart remains contiguous, we take thesource term as a bracketable instance, otherwisean unbracketable instance.
With these bracketableand unbracketable instances, we train a maximumentropy binary classifier to predict bracketable (b)probability of a given source term tfwithin par-ticular contexts c(tf).
The binary classifier is for-mulated as follows:Pb(b|c(tf)) =exp(?j?jhj(b, c(tf)))?b?exp(?j?jhj(b?, c(tf)))(8)where hj?
{0, 1} is a binary feature function and?jis the weight of hj.
We use the following fea-tures: 1) the word sequence of the source term, 2)the first word of the source term, 3) the last wordof the source term, 4) the preceding word of thefirst word of the source term, 5) the succeedingword of the last word of the source term, and 6)the number of words in the source term.Given a source sentence which contains T terms{tfi}T1, our term bracketing model TermBrackcan be denoted asTermBrack =T?i=1Pb(b|c(tfi)) (9)Whenever a hypothesis just covers a source termtfi, we calculate the bracketable probability of tfiaccording to Eq.
(8).The term bracketing model is integrated into thelog-linear model of SMT as a feature.
Through thefeature, we want the decoder to translate sourceterms with a high bracketable probability as awhole unit.550Source Target D MF?angy`u X`?t?ong defence mechanismsF?angy`u X`?t?ong defence systemsF?angy`u X`?t?ong defense programmes 470 56F?angy`u X`?t?ong prevention systems... ...Zh`anlu`e D?aod`an F?angy`u X`?t?ong strategic missile defense system 7 0Table 1: Examples of bilingual terms extracted from the training data.
?D?
means the total number ofdocuments in which the corresponding source term occurs and ?M?
denotes the number of documents inwhich the corresponding source term is translated into different target terms.
The source side is ChinesePinyin.
To save space, we do not list all the 23 different translations of the source term ?F?angy`u X`?t?ong?.5 ExperimentsIn this section, we conducted experiments to an-swer the following three questions.1.
Are our term translation disambiguation,consistency and bracketing models able toimprove translation quality in BLEU?2.
Does the combination of the three modelsprovide further improvements?3.
To what extent do the proposed models affectthe translations of test sets?5.1 SetupOur training data consist of 4.28M sentence pairsextracted from LDC1data with document bound-aries explicitly provided.
The bilingual trainingdata contain 67,752 documents, 124.8M Chinesewords and 140.3M English words.
We choseNIST MT05 as the MERT (Och, 2003) tuning set,NIST MT06 as the development test set, and NISTMT08 as the final test set.
The numbers of docu-ments/sentences in NIST MT05, MT06 and MT08are 100/1082, 79/1664 and 109/1357 respectively.The word alignments were obtained by runningGIZA++ (Och and Ney, 2003) on the corpora inboth directions and using the ?grow-diag-final-and?
balance strategy (Koehn et al., 2003).
Weadopted SRI Language Modeling Toolkit (Stol-cke and others, 2002) to train a 4-gram languagemodel with modified Kneser-Ney smoothing onthe Xinhua portion of the English Gigaword cor-pus.
For the topic model, we used the open source1The corpora include LDC2003E07, LDC2003E14,LDC2004T07, LDC2004E12, LDC2005E83, LDC2005T06,LDC2005T10, LDC2006E24, LDC2006E34, LDC2006E85,LDC2006E92, LDC2007E87, LDC2007E101,LDC2008E40, LDC2008E56, LDC2009E16 andLDC2009E95.LDA tool GibbsLDA++2with the default settingfor training and inference.
We performed 100 it-erations of the L-BFGS algorithm implemented inthe MaxEnt toolkit3with both Gaussian prior andevent cutoff set to 1 to train the term bracketingprediction model (Section 4.3).We performed part-of-speech tagging for mono-lingual term extraction (C-value/NC-vaule methodin Section 3) of the source and target languageswith the Stanford NLP toolkit4.
The bilingual termbank was extracted based on the following param-eter settings of term extraction methods.
Empiri-cally, we set the maximum length of a term to 6words5.
For both the C-value/NC-value and LLR-based extraction methods, we set the context win-dow size to 5 words, which is a widely-used set-ting in previous work.
And we set C-value/NC-value score threshold to 0 and LLR score thresholdto 10 according to the training corpora.We used the case-insensitive 4-gram BLEU6asour evaluation metric.
In order to alleviate the im-pact of the instability of MERT (Och, 2003), weran it three times for all our experiments and pre-sented the average BLEU scores on the three runsfollowing the suggestion by Clark et al.
(2011).We used an in-house hierarchical phrase-baseddecoder to verify our proposed models.
Althoughthe decoder translates a document in a sentence-by-sentence fashion, it incorporates document-informed information for sentence translation viathe proposed term translation models trained ondocuments.2http://sourceforge.net/projects/gibbslda/3http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html4http://nlp.stanford.edu/software/tagger.shtml5We determine the maximum length of a term by testing{5, 6, 7, 8} in our preliminary experiments.
We find thatlength 6 produces a slightly better performance than othervalues.6ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl551Zh?y?u W?iyu?nhu?
Ch?ngyu?n C?i  K?
C?nji?
W?iyu?nhu?
Sh?ny?
?Only members of the commission shall take part  in the commission deliberations .
?He these proposalsT?
Ji?ng Zh?xi?
Ji?ny?
Ji?o Y?u Y?
G?
B?zh?ngj?
W?iyu?nhu?
Sh?ny?submit for approval to a committee of ministers .
(a)(b)Figure 1: An example of unbracketable source term in the training data.
In (a), ?W?eiyu?anhu`?
Sh?eny`??
isbracketable while in (b) it is unbracketable.
The solid lines connect bilingual phrases.
The source side isChinese Pinyin.5.2 Bilingual Term BankBefore reporting the results of the proposed mod-els, we provide some statistics of the bilingualterm bank extracted from the training data.According to our statistics, about 1.29M bilin-gual terms are extracted from the training data.65.07% of the sentence pairs contain bilingualterms in the training data.
And on average, asource term has about 1.70 different translations.These statistics indicate that terms are frequentlyused in real-world data and that a source term canbe translated into different target terms.We also present some examples of bilingualterms extracted from the training data in Table 1.Accordingly, we show the total number of doc-uments in which the corresponding source termoccurs and the number of documents in whichthe corresponding source term is translated intodifferent target terms.
The source term ?F?angy`uX`?t?ong?
has 23 different translations in total.
Theyare distributed in 470 documents in the trainingdata.
In 414 documents, ?F?angy`u X`?t?ong?
hasonly one single translation.
However, in the other56 documents it has different translations.
Thisindicates that ?F?angy`u X`?t?ong?
is not consistentlytranslated in these 56 documents.
Different fromthis, the source term ?Zh`anlu`e D?aod`an F?angy`uX`?t?ong?
only has one translation.
And it is trans-lated consistently in all 7 documents where it oc-curs.
In fact, according to our statistics, there areabout 5.19% source terms whose translations arenot consistent even in the same document.These examples and statistics suggest 1) thatsource terms have domain-specific translationsand 2) that source terms are not necessarily trans-lated in a consistent manner even in the same doc-ument.
These are exactly the reasons why we pro-pose the term translation disambiguation and con-sistency model based on domain information rep-resented by topic distributions.Actually, 36.13% of the source terms are notnecessarily translated into target strings as a wholeunit.
We show an example of such terms in Fig-ure 1.
In Figure 1-(a), ?W?eiyu?anhu`?
Sh?eny`??
is aterm, and is translated into ?commission deliber-ations?
as a whole unit.
Therefore ?W?eiyu?anhu`?Sh?eny`??
is bracketable in this sentence.
How-ever, in Figure 1-(b), ?W?eiyu?anhu`??
and ?Sh?eny`?
?are translated separately.
Therefore ?W?eiyu?anhu`?Sh?eny`??
is an unbracketable term in this sentence.This is the reason why we propose a bracketingmodel to predict whether a source term is brack-etable or not.5.3 Effect of the Proposed ModelsIn this section, we validate the effectiveness of theproposed term translation disambiguation model,consistency model and bracketing model respec-tively.
In addition to the traditional hiero (Chi-ang, 2007) system, we also compare against the?CountFeat?
method in Ren et al.
(2009) who usea binary feature to indicate whether a bilingualphrase contains a term pair.
Although Ren et al.
(2009)?s experiments are conducted in a phrase-based system, the idea can be easily applied to ahierarchical phrase-based system.We carried out experiments to investigate the ef-fect of the term translation disambiguation model(Dis-Model) and report the results in Table 2.
Inorder to find the topic number setting with whichour model has the best performance, we ran exper-iments using the MT06 as the development test set.From Table 2, we observe that the Dis-Model ob-tains steady improvements over the baseline and?CountFeat?
method with the topic number K552Models MT06 MT08 AvgBaseline 32.43 24.14 28.29CountFeat 32.77 24.29 28.53Dis-ModelK = 50 32.94* 24.53 28.74K = 100 33.10* 24.57 28.84K = 150 33.16* 24.67* 28.92K = 200 33.08* 24.55 28.81Cons-ModelK = 50 33.09* 24.59 28.84K = 100 33.13* 24.74* 28.94K = 150 33.32*+ 24.84*+ 29.08K = 200 33.02* 24.73* 28.88Brack-Model 33.09* 24.66* 28.88Combined-Model 33.59*+ 24.99*+ 29.29Table 2: BLEU-4 scores (%) of the term translation disambiguation model (Dis-Model), the term transla-tion consistency model (Cons-Model), the term bracketing model (Brack-Model), and the combination ofthe three models, on the development test set MT06 and the final test set MT08.
K ?
{50, 100, 150, 200}which is the number of topics for the Dis-Model and the Cons-Model.
?Combined-Model?
is the combi-nation of the three single modes with topic number 150 for the Dis-Model and the Cons-Model.
?Base-line?
is the traditional hierarchical phrase-based system.
?CountFeat?
is the method that adds a countingfeature to reward translation hypotheses containing bilingual term pairs.
The ?*?
and ?+?
denote that theresults are significantly (Clark et al., 2011) better than those of the baseline system and the CountFeatmethod respectively (p<0.01).ranging from 50 to 150.
However, when we set Kto 200, the performance drops.
The highest BLEUscores 33.16 and 24.67 are obtained at the topicsetting K = 150.
In fact, our Dis-Model gainshigher performance in BLEU than both the tradi-tional hiero baseline and the ?CountFeat?
methodwith all topic settings.
The ?CountFeat?
methodrewards translation hypotheses containing bilin-gual term pairs.
However it does not explore anydomain information.
Our Dis-Model incorporatesdomain information to conduct translation disam-biguation and achieves higher performance.
Whenthe topic number is set to 150, we gain the high-est BLEU score, which is higher than that of thebaseline by 0.73 and 0.53 BLEU points on MT06and MT08, respectively.
The final gain over thebaseline is on average 0.63 BLEU points.We conducted the second group of experimentsto study whether the term translation consistencymodel (Cons-Model) is able to improve the per-formance in BLEU, as well as to investigate theimpact of different topic numbers on the Cons-Model.
Results are shown in Table 2, from whichwe observe the similar phenomena to what wehave found in the Dis-Model.
Our Cons-Modelgains higher BLEU scores than the baseline sys-tem and the ?CountFeat?
method with all topicsettings.
Setting topic number to 150 achieves thehighest BLEU score, which is higher than base-line by 0.89 BLEU points and 0.70 BLEU pointson MT06 and MT08 respectively, and on average0.79 BLEU points.We also conducted experiments to verify the ef-fectiveness of the term bracketing model (Brack-Model), which conducts bracketing prediction forsource terms.
Results in Table 2 show thatour Brack-Model gains higher BLEU scores thanthose of the baseline system and the ?CountFeat?method.
The final gain of Brack-Model over thebaseline is 0.66 BLEU points and 0.52 points onMT06 and MT08 respectively, and on average0.59 BLEU points.5.4 Combination of the Three ModelsAs shown in the previous subsection, the termtranslation disambiguation model, consistencymodel and bracketing model substantially outper-form the baseline.
Now, we investigate whetherusing these three models simultaneously can leadto further improvements.
The last row in Table 2shows that the combination of the three models(Combined-Model) achieves higher BLEU scorethan all single models, when we set the topic num-ber to 150 for the term translation disambigua-tion model and consistency model.
The final gain553Models MT06 MT08Best-Dis-Model 30.89 30.14Best-Cons-Model 38.04 36.70Brack-Model 60.46 55.78Combined-Model 54.39 50.85Table 3: Percentage (%) of 1-best translationswhich are generated by the Combined-Model andthe three single models with best settings on thedevelopment test set MT06 and the final test setMT08.
The topic number is 150 for Best-Dis-Model and Best-Cons-Model.of the Combined-Model over the baseline is 1.16BLEU points and 0.85 points on MT06 and MT08respectively, and on average 1.00 BLEU points.5.5 AnalysisIn this section, we investigate to what extent theproposed models affect the translations of test sets.In Table 3, we show the percentage of 1-best trans-lations affected by the Combined-Model and thethree single models with best settings on test setsMT06 and MT08.
For single models, if the corre-sponding feature (disambiguation, consistency orbracketing) is activated in the 1-best derivation,the corresponding model has impact on the 1-besttranslation.
For the Combined-Model, if any ofthe corresponding features is activated in the 1-best derivation, the Combined-Model affects the1-best translation.From Table 3, we can see that 1-best transla-tions of source sentences affected by any of theproposed models account for a high proportion(30%?60%) on both MT06 and MT08.
This in-dicates that all proposed models play an importantrole in the translation of both test sets.
Amongthe three proposed models, the Brack-Model is theone that affects the largest number of 1-best trans-lations in both test sets.
And the percentage is60.46% and 55.78% on MT06 and MT08 respec-tively.
The Brack-Model only considers sourceterms during decoding, while the Dis-Model andCons-Model need to match both source and targetterms.
The Brack-Model is more likely to be acti-vated.
Hence the percentage of 1-best translationsaffected by this model is higher than those of theother two models.
Since we only investigate the1-best translations generated by the Combined-Model and single models, the translations gener-ated by some single models (e.g., Brack-Model)may not be generated by the Combined-Model.Therefore it is hard to say that the numbers of 1-best translations affected by the Combined-Modelmust be greater than those of single models.6 Conclusion and Future WorkWe have studied the three issues of term trans-lation and proposed three different term trans-lation models for document-informed SMT.
Theterm translation disambiguation model enablesthe decoder to favor the most suitable domain-specific translations with domain information forsource terms.
The term translation consistencymodel encourages the decoder to translate sourceterms with a high domain translation consistencystrength into target terms rather than other newstrings.
Finally, the term bracketing model re-wards hypotheses that translate bracketable termsinto continuous target strings as a whole unit.We integrate the three models into a hierarchicalphrase-based SMT system7and evaluate their ef-fectiveness on the NIST Chinese-English transla-tion task with large-scale training data.
Experi-ment results show that all three models achievesignificant improvements over the baseline.
Ad-ditionally, combining the three models achieves afurther improvement.
For future work, we wouldlike to evaluate our models on term translationacross a range of different domains.AcknowledgmentsThis work was supported by National Key Tech-nology R&D Program (No.
2012BAH39B03) andCAS Action Plan for the Development of WesternChina (No.
KGZD-EW-501).
Deyi Xiong?s workwas supported by Natural Science Foundation ofJiangsu Province (Grant No.
BK20140355).
QunLiu?s work was partially supported by ScienceFoundation Ireland (Grant No.
07/CE/I1142) aspart of the CNGL at Dublin City University.
Sin-cere thanks to the anonymous reviewers for theirthorough reviewing and valuable suggestions.
Thecorresponding author of this paper, according tothe meaning given to this role by University ofChinese Academy of Sciences and Soochow Uni-versity, is Deyi Xiong.7Our models are not limited to hierarchical phrase-basedSMT.
They can be easily applied to other SMT formalisms,such as phrase- and syntax-based SMT.554ReferencesDavid M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet allocation.
the Journal of ma-chine Learning research, 3:993?1022.Houda Bouamor, Aur?elien Max, and Anne Vilnat.2012.
Validation of sub-sentential paraphrases ac-quired from parallel monolingual corpora.
In Pro-ceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 716?725.
Association for Computa-tional Linguistics.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah ASmith.
2011.
Better hypothesis testing for statisticalmachine translation: Controlling for optimizer insta-bility.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies: short papers-Volume2, pages 176?181.B?eatrice Daille.
1996.
Study and implementation ofcombined techniques for automatic extraction of ter-minology.
Journal of The balancing act: Combin-ing symbolic and statistical approaches to language,1:49?66.Vladimir Eidelman, Jordan Boyd-Graber, and PhilipResnik.
2012.
Topic models for dynamic translationmodel adaptation.
In Proceedings of the 50th An-nual Meeting of the Association for ComputationalLinguistics: Short Papers-Volume 2, pages 115?119.Association for Computational Linguistics.Xiaorong Fan, Nobuyuki Shimizu, and Hiroshi Nak-agawa.
2009.
Automatic extraction of bilin-gual terms from a chinese-japanese parallel corpus.In Proceedings of the 3rd International UniversalCommunication Symposium, pages 41?45.
ACM.Katerina T Frantzi, Sophia Ananiadou, and JunichiTsujii.
1998.
The c-value/nc-value method of au-tomatic recognition for multi-word terms.
In Re-search and Advanced Technology for Digital Li-braries, pages 585?604.
Springer.Zhengxian Gong, Min Zhang, and Guodong Zhou.2011.
Cache-based document-level statistical ma-chine translation.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 909?919.Liane Guillou.
2013.
Analysing lexical consistencyin translation.
In Proceedings of the Workshop onDiscourse in Machine Translation, pages 10?18.Christian Hardmeier, Joakim Nivre, and J?org Tiede-mann.
2012.
Document-wide decoding for phrase-based statistical machine translation.
In Proceed-ings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1179?1190.Eva Hasler, Phil Blunsom, Philipp Koehn, and BarryHaddow.
2014a.
Dynamic topic adaptation forphrase-based mt.
In Proceedings of the 14th Confer-ence of the European Chapter of the Association forComputational Linguistics, Gothenburg, Sweden.Eva Hasler, Barry Haddow, and Philipp Koehn.
2014b.Dynamic topic adaptation for smt using distribu-tional profiles.
In Proceedings of the Ninth Work-shop on Statistical Machine Translation, pages 445?456, Baltimore, Maryland, USA, June.
Associationfor Computational Linguistics.Hans Hjelm.
2007.
Identifying cross language termequivalents using statistical machine translation anddistributional association measures.
In Proceedingsof 16th Nordic Conference of Computational Lin-guistics Nodalida, pages 97?104.Masaki Itagaki and Takako Aikawa.
2008.
Post-mtterm swapper: Supplementing a statistical machinetranslation system with a user dictionary.
In LREC.Masaki Itagaki, Takako Aikawa, and Xiaodong He.2007.
Automatic validation of terminology trans-lation consistency with statistical method.
Proceed-ings of MT summit XI, pages 269?274.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InProceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 48?54.Els Lefever, Lieve Macken, and Veronique Hoste.2009.
Language-independent bilingual terminologyextraction from a multilingual parallel corpus.
InProceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 496?504.Magnus Merkel and Jody Foo.
2007.
Terminologyextraction and term ranking for standardizing termbanks.
In Proceedings of 16th Nordic Conferenceof Computational Linguistics Nodalida, pages 349?354.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics-Volume 1, pages 160?167.Scott SL Piao, Guangfan Sun, Paul Rayson, andQi Yuan.
2006.
Automatic extraction of chi-nese multiword expressions with a statistical tool.In Workshop on Multi-word-expressions in a Mul-tilingual Context held in conjunction with the 11thEACL, Trento, Italy, pages 17?24.555Pinis and Skadins.
2012.
Mt adaptation for under-resourced domains?what works and what not.
InHuman Language Technologies?The Baltic Perspec-tive: Proceedings of the Fifth International Confer-ence Baltic HLT 2012, volume 247, page 176.
IOSPress.Zhixiang Ren, Yajuan L?u, Jie Cao, Qun Liu, and YunHuang.
2009.
Improving statistical machine trans-lation using domain bilingual multiword expres-sions.
In Proceedings of the Workshop on MultiwordExpressions: Identification, Interpretation, Disam-biguation and Applications, pages 47?54.Andreas Stolcke et al.
2002.
Srilm-an extensible lan-guage modeling toolkit.
In Proceedings of the inter-national conference on spoken language processing,volume 2, pages 901?904.Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,Xiaodong Shi, Huailin Dong, and Qun Liu.
2012.Translation model adaptation for statistical machinetranslation with monolingual topic information.
InProceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics: LongPapers-Volume 1, pages 459?468.J?org Tiedemann.
2010.
Context adaptation in statisti-cal machine translation using models with exponen-tially decaying cache.
In Proceedings of the 2010Workshop on Domain Adaptation for Natural Lan-guage Processing, pages 8?15.Ferhan Ture, Douglas W Oard, and Philip Resnik.2012.
Encouraging consistent translation choices.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 417?426.
Association for Computational Lin-guistics.Muriel Vasconcellos, Brian Avey, Claudia Gdaniec,Laurie Gerber, Marjorie Le?on, and Teruko Mita-mura.
2001.
Terminology and machine translation.Handbook of Terminology Management, 2:697?723.Thuy Vu, Ai Ti Aw, and Min Zhang.
2008.
Term ex-traction through unithood and termhood unification.In Proceedings of the third international joint con-ference on natural language processing.Billy Wong and Chunyu Kit.
2012.
Extending ma-chine translation evaluation metrics with lexical co-hesion to document level.
In Proceedings of the2012 Joint Conference on Empirical Methods inNatural Language Processing and ComputationalNatural Language Learning, pages 1060?1068.Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, andShouxun Lin.
2012.
A topic similarity model for hi-erarchical phrase-based translation.
In Proceedingsof the 50th Annual Meeting of the Association forComputational Linguistics: Long Papers-Volume 1,pages 750?758.Deyi Xiong and Min Zhang.
2013.
A topic-basedcoherence model for statistical machine translation.In Proceedings of the Twenty-Seventh AAAI Confer-ence on Artificial Intelligence (AAAI-13), Bellevue,Washington, USA, July.Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.2009.
A syntax-driven bracketing model for phrase-based translation.
In Proceedings of the Joint Con-ference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on NaturalLanguage Processing of the AFNLP, pages 315?323.Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan L?u,and Qun Liu.
2013a.
Modeling lexical cohesion fordocument-level machine translation.
In Proceedingsof the Twenty-Third international joint conferenceon Artificial Intelligence, pages 2183?2189.
AAAIPress.Deyi Xiong, Yang Ding, Min Zhang, and Chew LimTan.
2013b.
Lexical chain based cohesion mod-els for document-level statistical machine transla-tion.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1563?
?1573.Bing Zhao and Eric P Xing.
2006.
Bitam: Bilingualtopic admixture models for word alignment.
In Pro-ceedings of the COLING/ACL on Main conferenceposter sessions, pages 969?976.556
