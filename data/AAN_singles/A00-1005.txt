PartslD: A Dialogue-Based System for Identifying Parts for MedicalSystemsAmit BAGGA, Tomek STRZALKOWSKI, and G. Bowden WISEInformation Technology LaboratoryGE Corporate Research and Development1 Research CircleNiskayuna, USA, NY 12309{ bagga, strzalkowski, wisegb } @crd.ge.comAbstractThis paper describes a system thatprovides customer service by allowingusers to retrieve identification umbers ofparts for medical systems using spokennatural language dialogue.
The paper alsopresents an evaluation of the systemwhich shows that the system successfullyretrieves the identification numbers ofapproximately 80% of the parts.IntroductionCurrently people deal with customer servicecenters either over the phone or on the worldwide web on a regular basis.
These servicecenters upport a wide variety of tasks includingchecking the balance of a bank or a credit cardaccount, transferring money from one account oanother, buying airline tickets, and filing one'sincome tax returns.
Most of these customerservice centers use interactive voice response(IVR) systems on the front-end for determiningthe user's need by providing a list of options thatthe user can choose from, and then routing thecall appropriately.
The IVRs also gatheressential information like the user's bankaccount number, social security number, etc.For back-end support, the customer servicecenters use either specialized computer systems(example: a system that retrieves the accountbalance from a database), or, as in most cases,human operators.However, the IVR systems are unwieldyto use.
Often a user's needs are not covered bythe options provided by the system forcing theuser to hit 0 to transfer to a human operator.
Inaddition, frequent users often memorize thesequence of options that will get them thedesired information.
Therefore, any change inthe options greatly inconveniences these users.Moreover, there are users that always hit 0 tospeak to a live operator because they prefer todeal with a human instead of a machine.Finally, as customer service providers continueto rapidly add functionality to their IVRsystems, the size and complexity of thesesystems continues to grow proportionally.
Insome popular systems like the IVR system thatprovides customer service for the InternalRevenue Service (IRS), the user is initiallybombarded with 10 different options with eachoption leading to sub-menus offering a further 3-5 options, and so on.
The total number of nodesin the tree corresponding to the IRS' IVR systemis quite large (approximately 100) making itextremely complex to use.Some customer service providers havestarted to take advantage of the recent advancesin speech recognition technology.
Therefore,some of the IVR systems now allow users to saythe option number (1, 2, 3 .
.
.
.
.
etc.)
instead ofpressing the corresponding button.
In addition,some providers have taken this a step further byallowing users to say a keyword or a phrasefrom a list of keywords and/or phrases.
Forexample, AT&T, the long distance company,provides their users the following options:"Please say information for information onplacing a call, credit for requesting credit, oroperator to speak to an operator.
"However, given the improved speechrecognition technology, and the research done innatural anguage dialogue over the last decade,there exists tremendous potential in enhancing29these customer service centers by allowing usersto conduct a more natural human-like dialoguewith an automated system to provide acustomer-friendly s stem.
In this paper wedescribe a system that uses natural languagedialogue to provide customer service for amedical domain.
The system allows fieldengineers to call and obtain identificationnumbers of parts for medical systems usingnatural language dialogue.
We first describesome work done previously in using naturallanguage dialogue for customer serviceapplications.
Next, we present he architectureof our system along with a description of each ofthe key components.
Finally, we conclude byproviding results from an evaluation of thesystem.1.
Previous WorkAs mentioned earlier, some customer servicecenters now allow users to say either the optionnumber or a keyword from a list ofoptions/descriptions.
However, the only knownwork which automates part of a customer servicecenter using natural language dialogue is the oneby Chu-Carroll and Carpenter (1999).
Thesystem described here is used as the front-end ofa bank's customer service center.
It routes callsby extracting key phrases from a user utteranceand then by statistically comparing these phrasesto phrases extracted from utterances in a trainingcorpus consisting of pre-recorded calls wherethe routing was done by a human.
The call isrouted to the destination of the utterance fromthe training corpus that is most "similar" to thecurrent utterance.
On occasion, the system willinteract with the user to clarify the user's requestby asking a question.
For example, if the userwishes to reach the loan department, the systemwill ask if the loan is for an automobile, or ahome.
Other related work is (Georgila et al,1998).While we are aware of the work beingdone by speech recognition companies likeNuance (www.nuance.com) and Speechworks(www.speechworks.com) in the area ofproviding more natural anguage dialogue-basedcustomer service, we are not aware of anyconference or journal publications from them.Some magazine articles which mention theirwork are (Rosen 1999; Rossheim 1999;Greenemeier 1999 ; Meisel 1999).
In addition,when we tried out a demo of Nuance's ystems,we found that their systems had a very IVRishfeel to them.
For example, if one wanted totransfer $50 from one account o another, thesystem would first ask the account that themoney was coming from, then the account hatthe money was going to, and finally, the amountto be transferred.
Therefore, a user could notsay "I want to transfer $50 from my savingsaccount o my checking account" and have thesystem conduct that transaction.In addition to the works mentioned above,there have been several classic projects in thearea of natural language dialogue likeTRAINS/TRIPS project at Rochester (Allen etal., 1989, 1995, 1996), Duke's Circuit-Fixit-Shoppe and Pascal Tutoring System (Biermannet al, 1997; 1995), etc.
While the Circuit-Fixit-Shoppe system helps users fix a circuit through adialogue with the system, the TRIPS and theTRAINS projects allow users to plan theiritineraries through dialogue.
Duke's Pascaltutoring system helps students in an introductoryprogramming class debug their programs byallowing them to analyze their syntax errors, getadditional information on the error, and learn thecorrect syntax.
Although these systems havebeen quite successful, they use detailed modelsof the domain and therefore cannot be used fordiverse applications uch as the ones requiredfor customer service centers.
Other related workon dialogue include (Carberry, 1990; Grosz andSidner, 1986; Reichman, 1981).2.
PartslD: A System for Identificationof Parts for Medical SystemsInitially, we were approached by the medicalsystems business of our company for help inreducing the number of calls handled by humanoperators at their call center.
An analysis of thetypes of customer service provided by their callcenter showed that a large volume of callshandled by their operators were placed by fieldengineers requesting identification umbers ofparts for various medical systems.
The IDnumbers were most often used for ordering thecorresponding parts using an automated IVRsystem.
Therefore, the system we have built30Figure 1.
PartslD System ArchitectureWI Parser l~ UserDia logue  ManagerF .
, .pros entetionhelps automate some percentage of these callsby allowing the engineer to describe a part usingnatural language.
The rest of this sectiondescribes our system in detail.2.1 DataThe database we used for our system was thesame as the one used by the operators at the callcenter.
This database consists of the mostcommon parts and was built by the operatorsthemselves.
However, the data contained in thedatabase is not clean and there are several typesof errors including mis-spellings, use of non-standard abbreviations, use of several differentabbreviations for the same word, etc.The database consists of approximately7000 different parts.
For each part, the databasecontains its identification umber, a description,and the product (machine type) that it is used in.The descriptions consist of approximately60,000 unique words of which approximately3,000 are words which either are non-standardabbreviations or are unique to the medicaldomain (example: collimator).Due to the large size of the database, wedid not attempt to clean the data.
However, wedid build several data structures based on thedatabase which were used by the system.
Theprimary data structures built were two invertedhash tables corresponding to the product, and thepart description fields in the database.
Theinverted hash tables were built as follows:1) Each product and part description fieldwas split into words.2) Stop-words (words containing noinformation like: a, the, an, etc.)
werefiltered.3) Each remaining word was inserted as theindex of the appropriate hash table withthe identification number of the partbeing the value corresponding to theindex.Therefore, for each non-stop-word word used indescribing a part, the hash table contains a list ofall the parts whose descriptions contained thatword.
Similarly, the products hash tablecontains a list of all parts corresponding to eachproduct word.2.2 System ArchitectureThe architecture of the system is shown inFigure 1.
The system was designed in a mannersuch that it could be easily ported from oneapplication to another with minimal effort otherthan providing the domain-specific knowledgeregarding the new application.
Therefore, wedecided to abstract away the domain-specificinformation into self-contained modules whilekeeping the other modules completelyindependent.
The domain-specific modules areshown in the dark shaded boxes in Figure I.The remainder of this section discusses each ofthe modules hown in the system architecture.2.2.1 The Speech Recognition System (ASR)Since customer service centers are meant o beused by a variety of users, we needed a user-independent speech recognition system.
In31addition, since the system could not restrict hemanner in which a user asked for service, thespeech recognition system could not begrammar-based.
Therefore, we used a generalpurpose dictation engine for the system.
Thedictation system used was Lernout & Hauspie'sVoiceXPress ystem (www.lhs.com).
Althoughthe system was general purpose, we did provideto it the set of keywords and phrases that arecommonly used in the domain thereby enablingit to better recognize these domain-specifickeywords and phrases.
The keywords andphrases used were simply the list of descriptionsand product names corresponding to each part inthe database.
It should be noted that the set ofdomain-specific keywords and phrases wasprovided to the speech recognition system as atext document.
In other words, the training wasnot done by a human speaking the keywords andphrases into the speech recognition system.
Inaddition, the speech recognition system is farfrom perfect.
The recognition rates hoveraround 50%, and the system has additionaldifficulty in identifying product names whichare most often words not found in a dictionary(examples: 3MlaserCam, 8000BUCKY, etc.
).2.2.2 Parser and the LexiconThe parser is domain-driven i the sense that ituses domain-dependent information produced bythe lexicon to look for information, in a userutterance, that is useful in the current domain.However, it does not attempt to understand fullyeach user utterance.
It is robust enough tohandle ungrammatical sentences, hort phrases,and sentences that contain mis-recognized text.The lexicon, in addition to providingdomain-dependent keywords and phrases to theparser, also provides the semantic knowledgeassociated with each keyword and phrase.Therefore, for each content word in the invertedhash tables, the lexicon contains entries whichhelp the system determine whether the word wasused in a part description, or a product name.
Inaddition, the lexicon also provides the semanticknowledge associated with the pre-specifiedactions which can be taken by the user like"operator" which allows the user to transfer toan operator, and "stop," or "quit" which allowthe user to quit the system.
Some sample ntriesare:collimator => (description_word, collimator)camera => (product_word, camera)operator => (user action, operator)etc.The parser scans a user utterance andreturns, as output, a list of semantic tuplesassociated with each keyword/phrase containedin the utterance.
It is mainly interested in "keywords" (words that are contained in product andpart descriptions, user action words, etc.)
and itignores all the other words in the user utterance.The parser also returns a special tuple containingthe entire input string which may be used laterby the context-based parser for sub-stringmatching specially in cases when the DM hasasked a specific question to the user and isexpecting a particular kind of response.2.2.3 The Filler and Template ModulesThe filler takes as input the set of tuplesgenerated by the parser and attempts to checkoff templates contained in the templates moduleusing these tuples, The set of templates in thetemplates module contains most of remainingdomain-specific knowledge required by thesystem.
Each template is an internalrepresentation of a part in the database.
Itcontains for each part, its ID, its description, andthe product which contains it.
In addition, thereare several additional templates corresponding topre-specified user actions like "operator," and"quit."
A sample template follows:tl__I = ('product' = > 'SFD','product__ids' = > 2229005"'product_descriptions' => 'IR RECEIVER PCBOARD CI104 BISTABLE MEMORY')For each tuple input from the parser, thefiller checks off the fields which correspond tothe tuple.
For example, if the filler gets as input(description_word, collimator), it checks off thedescription fields of those templates containingcollimator as a word in the field.
A template ischecked off iff one or more of its fields ischecked off.
In addition, the filler alsomaintains a list of all description and productwords passed through the tuples (i.e.
these words32have been uttered by the user).
These two listsare subsequently passed to the dialoguemanager.Although the filler does not appear to bevery helpful for the current application domain,it is an important part of the architecture forother application domains.
For example, thecurrent PartslD system is a descendant from anearlier system which allowed users to processfinancial transactions where the filler wasinstrumental in helping the dialogue managerdetermine the type of transaction being carriedout by the user (Bagga et al, 2000).2.2.4 The Dialogue Manager (DM)The DM receives as input from the filler the setof templates which are checked off.
In addition,it also receives two lists containing the list ofdescription words, and product word uttered bythe user.
The DM proceeds using the followingalgorithm:1) It first checks the set of checked offtemplates input from the filler.
If there isexactly one template in this set, the DM asksthe user to confirm the part that the templatecorresponds to.
Upon receipt of theconfirmation from the user, it returns theidentification number of the part to the user.2) Otherwise, for each description word utteredby the user, the DM looks up the set of parts(or templates) containing the word from thedescriptions inverted hash table.
It thencomputes the intersection of these sets.
Ifthe intersection is empty, the DM computesthe union of these sets and proceeds treatingthe union as the intersection.3) If the intersection obtained from (2) abovecontains exactly one template, the DM asksthe user to confirm the part corresponding tothe template as in (1) above.4) Otherwise, the DM looks at the set ofproduct words uttered by the user.
If this setis empty, the DM queries the user for theproduct name.
Since the DM is expecting aproduct name here, the input provided by theuser is handled by the context-based parser.Since most product names consist of non-standard words consisting of alpha-numericcharacters (examples: AMX3,8000BUCKY, etc.
), the recognition qualityis quite poor.
Therefore, the context-basedparser anks the input received from the userusing a sub-string matching algorithm thatuses character-based unigram and bigramcounts (details are provided in the nextsection).
The sub-string matching algorithmgreatly enhances the performance of thesystem (as shown in the sample dialoguebelow).5) If the set of product words is non-empty, orif the DM has successfully queried the userfor a product name, it extracts the set ofparts (templates) containing each productword from the product words inverted hashtable.
It then computes an intersection ofthese sets with the intersection set ofdescription words obtained from (2) above.The resulting intersection is the joint productand description i tersection.6) If the joint intersection has exactly onetemplate, the DM proceeds as in (1) above.Alternatively, if the number of templates inthe joint intersection is less than 4, the DMlists the parts corresponding toeach of theseand asks the user to confirm the correct one.7) If there are more than 4 templates in thejoint intersection, the DM ranks thetemplates based upon word overlap with thedescription words uttered by the user.
If thenumber of resulting top-ranked templates iless than 4, the DM proceeds as in thesecond half of (6) above.8) If the joint intersection is empty, or in thehighly unlikely case of there being morethan 4 top-ranked templates in (7), the DMasks the user to enter additionaldisambiguating information.The goal of the DM is to hone in on the part(template) desired by the user, and it has todetermine this from the set of templates input toit by the filler.
It has to be robust enough to dealwith poor recognition quality, inadequateinformation input by the user, and ambiguousdata.
Therefore, the DM is designed to handlethese issues.
For example, description wordsthat are mis-recognized as other descriptionwords usually cause the intersection of the setsof parts corresponding to these words to beempty.
The DM, in this case, takes a union ofthe sets of parts corresponding to the description333333words thereby ensuring that the templatecorresponding tothe desired part is in the union.The DM navigates the space of possibilitiesby first analyzing the intersection of the sets ofparts corresponding to the description wordsuttered by the user.
If no unique part emerges,the DM then checks to see if the user hasprovided any information about the product hatthe part is going to be used in.
If no product wasmentioned by the user, the DM queries the userfor the product name.
Once this is obtained, theDM then checks to see if a unique partcorresponds to the product name and the partdescription provided by the user.
If no uniquepart emerges, then the DM backs off and asksthe user to re-enter the part description.Alternatively, if more than one part correspondsto the specified product and part description,then the DM ranks the parts based upon thenumber of words uttered by the user.Obviously, since the DM in this case uses aheuristic, it asks the user to confirm the part thatranks the highest.
If more than one (althoughless than 4) parts have the same rank, then theDM explicitly lists these parts and asks the userto specify the desired part.
It should be notedthat the DM has to ensure that the information itreceives is actually what the user meant.
This isespecially true when the DM uses heuristics, andsub-string matches (as in the case of productnames).
Therefore, the DM occasionally asksthe user to confirm input it has received.2.2.5 The Sub-String Matching AlgorithmWhen the dialogue manager is expecting acertain type of input (examples : product names,yes/no responses) from the user, the userresponse is processed by the context-basedparser.
Since the type of input is known, thecontext-based parser uses a sub-string matchingalgorithm that uses character-based unigram andbigram counts to match the user input with theexpectation of the dialogue manager.
Therefore,the sub-string matching module takes as input auser utterance string along with a list ofexpected responses, and it ranks the list ofexpected responses based upon the userresponse.
Listed below are the details of thealgorithm :1) The algorithm first concatenates the wordsof the user utterance into one long string.This is needed because the speechrecognition system often breaks up theutterance into words even though a singleword is being said.
For example, theproduct name AMXl l0  is often broken upinto the string 'Amex 110'.2) Next, the algorithm goes through the stringformed in (1) and compares this character bycharacter with the list of expected responses.It assigns one point for every commoncharacter.
Therefore, the expected response'AMX3' gets three points for the utterance'Amex110'.3) The algorithm then compares the userutterance with the list of expected responsesusing 2 characters (bigrams) at a time.
Itassigns 2 points for each bigram match.
Forthe example shown in (2), there are twobigram matches: the first is that theutterance starts with an 'A' (the previouscharacter is this case is the null character),and the second is the bigram 'AM'.4) The algorithm now compares the length ofthe user utterance string and the expectedresponse.
If the length of the two strings isthe same, then it assigns 2 points to theexpected response.5) Finally, the algorithm calculates the numberof unique characters in the expectedresponse, and the user utterance string.
Ifthese characters are the same, then it assigns4 points to the expected response.The expected response which has the highestnumber of points is the most likely one.
If twoor more expected responses have the samenumber of points, then the system asks the userto confh'm the correct one.While we have not evaluated this sub-string matching algorithm independently, a briefevaluation in the context of the system resultedin about 90% accuracy.2.2.6 The Presentation ModuleThe presentation module works in one of twopossible modes: over the phone, and over theweb.
This module takes as input a stringgenerated by the question-generation moduleand presents this string to the user in theappropriate mode of communication.
If thespeech option for the system is turned on, thespeech-based output is generated using Lernout"~ld.
34and Hauspie's RealSpeak text-to-speech system.Although the system currently cannot use bothmodes of communication simultaneously, weplan to incorporate this feature sometime in thefuture.2.2.
7 Robustness of  the SystemAs with any dialogue system, it is extremelyimportant for the system to be robust.
Oursystem has the following two features whichmake it extremely robust:1) The user can, at any given moment,say operator to transfer to anoperator, quit~stop to exit, andback~restart to start afresh.2) When expecting a response from theuser, if the system does not receive anexpected input, it repeats the questionat most twice before transferringcontrol to an operator.3.
Sample DialoguesThis section gives examples of two sampledialogues that occur during the testing of thesystem.
The system's response is indicated by"S>", the user's response by "U>", and therecognition of the user's response by "V>".S> Welcome to the PartslD Center.
Please tellme the part that you want the ID for.U> I need the ID for  PCB Control.V> I need the I the for PCB home thrown.S> What product do you need this for?U> COMPAX 40EV> home fax for a O0 ES> Is COMPAX 40E the product name that yousaid?U> yesV> gasS> Do you want the ID for PCB Display SID orthe PCB Control?U> The second one.V> The second home.S> Do you need the ID for PCB Control used inproduct COMPAX 40E?U> yesV> yesS> The ID for PCB Control used in productCOMPAX 40E is 2168983.
Do you wan to exitor f ind the ID for another part ?U> look up another part.S> Welcome to the PartslD Center.
Please tellme the part that you want the ID for.U> I need cable data for cradle 2M long.V> I needed cable date them for cradle to beloan.S> Do you need this part for a ANGIXM2004,or a AFM ?U> AFMV> a fanS> Is AFM the product name that you said?U> yesV> yesS> Do you want the ID for Cable Data forCradle 2M long ?U> yesV> yesS> The ID for  Cable Data for Cradle 2M longused in product AFM is 45438628.
Do you wanto exit or f ind the ID for another part?U> look up another part.4.
Evaluation and ResultsThe goal of our evaluation was to ensure that thesystem helped a user successfully identify partsirrespective of the performance of the speechrecognition engine for the user.
In other words,we wanted to see if the system was robustenough to conduct transactions with a diversemix of users.
We tested the system with 4different users two of whom had foreign accents.For each user, we randomly selected 20 partsfrom the database.
The results are summarizedin Table 1.These results show that the system wasquite successful in handling requests from userswith a variety of accents achieving varyingrecognition rates.
Out of the 80 parts tested,only twice did the user feel that he/she had totransfer to an operator.
The system successfullyretrieved the identification umbers of 79% ofthe parts while transferring 19% of the cases to ahuman operator because of extremely bad:$5User Partssuccessfullyidentified15Calls systemtransfers tooperator3Calls usertransfers tooperator2Systemprompts percall3.7Relevant wordsrecognized perpart2.518 2 0 3 2.3513 7 0 2.5 1.6517 3 0 2.9 2.7Table 1: Summary of Resultsrecognition.
We are planning on conducting amore elaborate test which a larger set of users.ConclusionsIn this paper we have described a robust systemthat provides customer service for a medicalparts application.
The preliminary results areextremely encouraging with the system beingable to successfully process approximately 80%of the requests from users with diverse accents.AcknowledgementsWe wish to thank the GE Medical Systems teamof Todd Reinke, Jim Tierney, and LisaNaughton for providing support and funding forthis project.
In addition, we also wish to thankDong Hsu of Lernout and Hauspie for his helpon the ASR and the text-to-speech systems.Finally, we wish to thank the InformationTechnology Laboratory of GE CRD forproviding additional funding for this project.ReferencesAllen, J. F. et al (1995) The TRAINS Project: Acase study in building a conversational p anningagent.
Journal of Experimental nd Theoretical AI,(7) 7-48.Allen, J. F., Miller, B. W.; Ringer, E. K.; andSikorski, T. (1996) A Robust System for NaturalSpoken Dialogue.
34th Annual Meeting of theACL, Santa Cruz, 62-70.Bagga, A., Stein G. C., and Strzalkowski, T. (2000)FidelityXPress: A Multi-Modal System forFinancial Transactions.
Proceedings of the 6 a~Conference on Content-Based MultimediaInformation Access (RIAO'00).Biermann, A.W.
; Rodman, R.; Rubin, D.; andHeidlage, J.R. (1985) Natural language withdiscrete speech as a mode for human to machinecommunication.
Communication of the ACM18(6): 628-636.Biermann, Alan W.; Guinn, Curry I.; Fulkerson, M.:Keim, G.A.
; Liang, Z.; Melamed, D.M.
; andRajagopalan, K. (1997) Goal-orientedMultimediaDialogue with Variable Initiative.
Lecture Notes inArtificial Intelligence 1325; Springer-Verlag, NewYork; pp.
1-16.Carberry, S. (1990) Plan Recognition in NaturalLanguage Dialogue.
Cambridge, Mass.
: The MITPress.Chu-Carroll, J, and R. Carpenter.
(1999) Vector-Based Natural Language Call Routing.
Journal ofComputational Linguistics, 25(30), pp.
361-388.Georgila, K., A.Tsopanoglou, N.Fakotakis andG.Kokkinakis.
(1998) An Integrated DialogueSystem for the Automation of Call Centre Services.ICLSP'98, 5th International Conference on SpokenLanguage Processing, Sydney, Australia.Grosz, B.J.
and Sidner, C.L.
(1986) Attentions,intentions, and the structure of discourse.Computational Linguistics 12(3): 175-204.Greenemeier, L. (1999) Voice-RecognitionTechnology Builds a Following.
InformationWeek, December 13.Meisel, W. (1999) Can Speech Recognition GiveTelephones a New Face?
BusinessCommunications Review, November 1.Reichman, R.. (1981) Plain-speaking: A theory andgrammar of spontaneous discourse.
PhD thesis,Department of Computer Science, HarvardUniversity, Cambridge, Massachusetts.Rosen, C. (1999) Speech Has Industry Talking.Business Travel News, November.Rossheim, J.
(1999) Giving Voice to CustomerService.
Datamation, November 1.36
