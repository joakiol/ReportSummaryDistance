Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 18?26,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsWordNet Based Features for Predicting BrainActivity associated with meanings of nounsAhmad Babaeian Jelodar, Mehrdad Alizadeh, and Shahram KhadiviComputer Engineering Department, Amirkabir University of Technology424 Hafez Avenue, Tehran, Iran{ahmadb_jelodar, mehr.alizadeh, khadivi}@aut.ac.irAbstractDifferent studies have been conducted forpredicting human brain activity associatedwith the semantics of nouns.
Corpus basedapproaches have been used for deriving fea-ture vectors of concrete nouns, to model thebrain activity associated with that noun.
Inthis paper a computational model is proposedin which, the feature vectors for each concretenoun is computed by the WordNet similarityof that noun with the 25 sensory-motor verbssuggested by psychologists.
The feature vec-tors are used for training a linear model topredict functional MRI images of the brain as-sociated with nouns.
The WordNet extractedfeatures are also combined with corpus basedsemantic features of the nouns.
The combinedfeatures give better results in predicting hu-man brain activity related to concrete nouns.1 IntroductionThe study of human brain function has receivedgreat attention in recent years from the advent offunctional Magnetic Resonance Imaging (fMRI).fMRI is a 3D imaging method, that gives the abili-ty to perceive brain activity in human subjects.
Athree dimensional fMRI image contains approx-imately 15000 voxels (3D pixels).
Since its advent,fMRI has been used to conduct hundreds of studiesthat identify specific regions of the brain that areactivated on average when a human performs aparticular cognitive function (e.g., reading, mentalimagery).
A great body of these publications showthat averaging together fMRI data collected overmultiple time intervals, while the subject respondsto some kind of repeated stimuli (reading words),can present descriptive statistics of brain activity(Mitchell et al, 2004).Conceptual meanings of different words and pic-tures trigger different brain activity.
The represen-tation of conceptual knowledge in the human brainhas been studied by different science communitiessuch as psychologists, neuroscientists, linguists,and computational linguists.
Some of these ap-proaches focus on visual features of picture stimulito analyze fMRI activation associated with viewingthe picture (O?Toole et al 2005) (Hardoon et al,2007).
Recent work (Kay et al, 2008) has shownthat it is possible to predict aspects of fMRI activa-tion based on visual features of arbitrary scenesand to use this predicted activation to identifywhich of a set of candidate scenes an individual isviewing.
Studies of neural representations in thebrain have mostly focused on just cataloging thepatterns of fMRI activity associated with specificcategories of words.
Mitchell et alpresent a ma-chine learning approach that is able to predict thefMRI activity for arbitrary words (Mitchell et al,2008).In this paper a computational model similar tothe computational model in (Mitchell et al, 2008)is proposed for predicting the neural activation of agiven stimulus word.
Mitchell et alperforms pre-diction of the neural fMRI activation based on afeature vector for each noun.
The feature vector isextracted by the co-occurrences of each individualconcrete noun with each of the 25 sensory-motorverbs, gathered from a huge google corpus (Brants,2006).
The feature vector of each noun is used to18Figure 1 - Structure of the model for predicting fMRI activation for arbitrary stimuli word wpredict the  activity  of  each voxel  in  the brain,by assuming a weighted linear model (Figure 1).The activity of a voxel is defined as a conti-nuous value that is assigned to it in the functionalimaging1 procedure.
Mitchell et alapplied a linearmodel based on its high consistency with the wide-spread use of linear models in fMRI analysis.
Inthis paper focus is on using WordNet based fea-tures (in comparison to co-occurrence based fea-tures), therefore the linear model proposed andjustified by Mitchell et alis used and other modelslike SVM are not even considered.
Mitchell et alsuggests that the trained model is able to predictbrain activity  even for unseen concepts and there-fore notes that a great step forward in modelingbrain activity is taken in comparison to the pre-vious cataloging approaches for brain activity.
Thismodel does not work well in case of ambiguity inmeaning, for example a word like saw has twomeanings, as a noun and as a verb, making it diffi-cult to construct the suitable feature vector for thisword.
We try to alleviate this problem in this paperand achieve better models by combining differentmodels in case of ambiguity.In our work, we use the sensory-motor verbswhich are suggested by psychologists and are alsoused by (Mitchell et al, 2008), to extract the fea-1 Functional images were acquired on a Siemens (Erlangen,Germany) Allegra 3.0T scanner at the Brain Imaging Re-search Center of Carnegie Mellon University and the Univer-sity of Pittsburgh (supporting online material of Mitchell et al2008).ture vectors.
But, instead of using a corpus to ex-tract the co-occurrences of concrete nouns withthese verbs we use WordNet to find the similaritiesof each noun with the 25 sensory-motor verbs.
Wealso combine the WordNet extracted model withthe corpus based model, and achieve better resultsin matching predicted fMRI images (from themodel) to their own observed images.This paper is organized as follows: in section 2 abrief introduction to WordNet measures is de-scribed.
In section 3, the WordNet approaches ap-plied in the experiments and the Mitchell et allinear model are explained.
The results of the expe-riments are discussed in section 4 and finally insection 5 the results and experiments are con-cluded.2 WordNet-based Similarity2.1 WordNetWorNet is a semantic lexicon database for Englishlanguage and is one of the most important andwidely used lexical resources for natural languageprocessing tasks (Fellbaum, 1998), such as wordsense disambiguation, information retrieval, auto-matic text classification, and automatic text sum-marization.WordNet is a network of concepts in the form ofword nodes organized by semantic relations be-tween words according to meaning.
Semantic rela-tion is a relation between concepts, and each nodeconsists of a set of words (synsets) representing the19real world concept associated with that node.
Se-mantic relations are like pointers between synsets.The synsets in WordNet are divided into four dis-tinct categories, each corresponding to four of theparts of speech ?
nouns, verbs, adjectives and ad-verbs (Pathwarden, 2003).WordNet is a lexical inheritance system.
The re-lation between two nodes show the level of gene-rality in an is?a hierarchy of concepts.
Forexample the relation between horse and mammalshows the inheritance of horse is-a mammal.2.2 SimilarityMany attempts have investigated to approximatehuman judgment of similarity between objects.Measures of similarity use information found in is?a hierarchy of concepts (or synsets), and quantifyhow much concept A is like concept B (Pedersen,2004).
Such a measure might show that a horse ismore like a cat than it is like a window, due to thefact that horse and cat share mammal as an ances-tor in the WordNet noun hierarchy.Similarity is a fundamental and widely usedconcept and refers to relatedness between two con-cepts in WordNet.
Many similarity measures havebeen proposed for WordNet?based measures ofsemantic similarity, such as information content(Resnik, 1995), JCN (Jiang and Conrath, 1997),LCH (Leacock and Chodorow, 1998), and Lin(Lin, 1998).These measures have limited the part of speech(POS) of words, for example it is not defined tomeasure the similarity between verb see and nouneye.
There is another set of similarity measureswhich work beyond this boundary of POS limita-tion.
These measures are called semantic related-ness measures; such as Lesk (Banerjee andPedersen, 2003), and Vector (Patwardhan, 2003).The simple idea behind the LCH method is tocompute the shortest path of two concepts in aWordNet unified hierarchy tree.
The LCH measureis defined as follows (Leacock and Chodorow,1998):(1)Similarity is measured between concepts c1 andc2, and D is the maximum depth of taxonomy;therefore the longest path is at most 2D.Statistical information from large corpora isused to estimate the information content of con-cepts.
Information content of a concept measuresthe specificity or the generality of that concept.IC(c)= - log ( freq(c)freq(root) )                                   (2)freq(c) is defined as the sum of frequencies of allconcepts in subtree of concept c. The frequency ofeach concept is counted in a large corpus.
There-fore freq(root) includes frequency count of all con-cepts.The LCS (Longest Common Subsummer) ofconcepts A and B is the most specific concept thatis an ancestor of both A and B. Resnik defined thesimilarity of two concepts as follows (Resnik,1995):relatednessres(c1,c2)=IC(lcs(c1,c2))                    (3)IC(lcs(c1,c2)) is the information content of LongestCommon Subsummer of concepts c1 and c2.The Lin measure, augment the information con-tent of the LCS with the sum of the informationcontent of concepts c1 and c2.
The Lin measurescales the information content of the LCS by thissum.
The similarity measure proposed by Lin, isdefined as follows (Lin, 1998):relatednesslin(c1,c2)= 2.IC(lcs(c1,c2))IC(c1)+IC(c2)                      (4)IC(c1) and IC(c2) are information content of con-cepts c1 and c2, respectively.Jiang and Conrath proposed another formulanamed JCN as a similarity measure which is shownbelow (Jiang and Conrath, 1997):relatednessjcn(c1,c2)= 1IC(c1)+IC(c2)-2.IC(lcs(c1,c2))   (5)The Lesk is a measure of semantic relatednessbetween concepts that is based on the number ofshared words (overlaps) in their definitions(glosses).
This measure extends the glosses of theconcepts under consideration to include theglosses of other concepts to which they are re-lated according to a given concept hierarchy (Ba-nerjee and Pedersen, 2003).
This method makes itpossible to measure similarity between nouns andverbs.The Vector measure creates a co?occurrencematrix for each word used  in theWordNet glossesfrom a given corpus, and then represents eachgloss/concept with a vector that is the average ofthese co?occurrence vectors (Patwardhan, 2003).203 ApproachesAs mentioned in the previous section, differentWordNet measures can be used to compute thesimilarities between two concepts.
The WordNetsimilarity measures are used to compute the verb-concept similarities.
The feature matrix comprisesof the similarities between 25 verbs (features) and60 concrete nouns (instances).
In this section thecomputational model proposed by (Mitchell et al,2008), WordNet-based models, and combinatorymodels are briefly described.3.1 Mitchell et alBaseline ModelIn our paper we used the Mitchell et alregressionmodel for predicting human brain actively as ourbaseline.
In all of the experiments in this paper, weuse the fMRI data gathered byMitchell et al The fMRI data were collected fromnine healthy, college age participants who viewed60 different word-picture pairs presented six timeseach (Mitchell et al 2008).
In Mitchell et al foreach concept, a feature vector containing norma-lized co-occurrences with 25 sensory-motor verbs,gathered from a huge google corpus (Brants,2006), is constructed.
The computational modelwas evaluated using the collected fMRI data ga-thered by Mitchell et al Mean fMRI images wereconstructed from the primary fMRI images, beforetraining.
A linear regression model was trained,using 58 (from 60) average brain images for eachsubject that maps these features to the correspond-ing brain image.
For testing the model, the two leftout brain images were compared with their corres-ponding predicted images, obtained from thetrained model.
The Pearson correlation (Equation6) was used for comparing whether each predictedimage has more similarity with its own observedimage (match1) or the other left out observed im-age (match2).match1(p1=i1 & p2=i2) =pearsonCorrelation(p1,i1)+pearsonCorrelation(p2,i2)              (6)p1 ,p2 are predicted images, and i1, i2 are corres-ponding observed images.For calculating the accuracy we check whetherthe classification is done correctly or not.
By se-lecting two arbitrary concepts (of sixty concepts)as test, there would be 1770 different classifica-tions.
The overall percentage of correct classifica-tion represents the accuracy of the model.We tried to use the same implementations in(Mitchell et al, 2008) as our baseline.
We imple-mented the training and test models as described inthe supporting online material of Mitchell et alspaper, but due to some probable unseen differencesfor example in the voxel selection, the classifica-tion accuracies achieved by our replicated baselineof Mitchell et als is in average less than the accu-racies attained by (Mitchell et al, 2008).
In the testphase we used 500 selected voxels for comparison.The training is done for all 9 participants.This procedure is used in all the other approach-es mentioned in this section.
We have contactedthe authors of the paper and we are trying to re-solve the problem of our baseline.3.2 WordNet based ModelsAs mentioned in section 2, several WordNet?basedsimilarity measures have been proposed (Pedersen,2004).
We apply some of the known measures toconstruct the feature matrix, and use them to trainthe models of 9 participants.WordNet::Similarity is a utility program availa-ble on web2 to compute information content val-ues.
WordNet::Similarity implements measures ofsimilarity and relatedness that are all in some waybased on the structure and content of WordNet(Resnik, 2004).As mentioned in section 2, every concept inWordNet consists of a set of words (synsets).
Thesimilarity between two concepts is defined as aseries of similarities between synsets of the firstconcept and synsets of the second concept.
In thispaper the maximum similarity between synsets oftwo concepts is considered as the candidate simi-larity between two concepts.In contrary to relatedness measures, similaritymeasures have the limitation of POS of words.
Inour case the verb-noun pair similarity is not de-fined when using similarity measures.
To solve thisproblem the sense (POS) of verb features are as-sumed to be free (verb, noun, adjective and ad-verb).
For most cases the meaning of a verb senseof a word is close to the non-verb senses of that2 http://wn-similarity.sourceforge.net/21word.
For example the verb clean can be seen as anoun, adjective, and adverb which all have closemeanings.
Some problems arise by this assump-tion.
For example the verb Watch has a far mean-ing of the noun Watch or some verbs like eat donot have a non-verb sense.
To handle these issuesthe combination of the relatedness measures andsimilarity measures is used.
This approach is dis-cussed in section 3.3 to make a more suitable fea-ture matrix.The two leave out cross-validation accuraciesof regression models trained by feature matrices(computed from WordNet similarities) are depictedin Figure 2.
The results helped us to select twomeasures for a final feature construction.
The re-sults are discussed and analyzed in the next sec-tion.3.3 Lin/Lesk and JCN/Lesk based featuresThe experiments show that, JCN similarity meas-ure gives the best results on extracting the featurevectors for predicting brain activity.
Unfortunately,some similarity measures like JCN and Lin featurematrices are to some extent sparse.
In some cases,the feature (sensory-motor verb) or even a conceptis represented by a null vector.
The null input datado not affect the linear regression training, but leadto less data for training the model.
This anomaly isoriginated from the fact that some verbs do nothave related non-verb senses (POS).On the other hand, relatedness measures (likeLesk) do not limit the POS of words.
In conse-quence, we have non-zero values for every elementof the feature matrix.
This motivates us to combineLesk similarity measure with Lin to alleviate thedefect mentioned above.Combination is based on finding a better featurematrix from the two Lin (JCN) and Lesk featurematrices.
For this, a linear averaging is consideredbetween Lin (JCN) and Lesk feature matrices.3.4 Combinatory SchemesIn this paper, a new approach for extracting thefeature matrix using WordNet is presented and dif-ferent similarity measures for representing this fea-ture matrix are investigated.In this section, we propose new combinatoryapproaches for combining Mitchell et als corpusbased approach with our WordNet based approach.We assume that we have two feature matrices, onebased on the corpus-based (baseline) method andthe other based on a WordNet-based (Lin/Lesksimilarity measure) method.3.4.1 Linear combinationThe first approach for combining WordNet andbaseline models, is based on assigning weights(?,1- ?)
to the models, for calculation of match1and match2.
match1 of baseline model is assignedweight ?, and match1 of WordNet model is as-signed weight (1- ?
), for calculating the finalmatch1 of the system (Equation 7).match1=?.(match1Baseline)+(1-?).
(match1WordNet) (7)match2 is calculated in the same way.
Classifica-tion is assumed to be correct when match1 gets agreater value than match2.
The parameter ?
needsto be tuned.
Different values of ?
were tested andtheir output accuracies are depicted in Figure 2.Figure 2 ?
accuracies of different ?
values3.4.2 Concept based combinationThe performance of computational models can beanalyzed from a different view.
We are looking fora combination mechanism based on model accura-cies for classifying a concept pair.
This combina-tion mechanism estimates weights for WordNetand baseline models for testing a left out pair.
Tohave a system with the ability to work properly onunseen nouns, we leave out all the concept pairsthat have concepts c1 or c2 (117 pairs).
This guar-antees that the trained model is blind to concepts c1and c2.
The remaining concept pairs are used for22Table 1- Voting mechanismtraining (1653 pairs).The accuracies of WordNet and baseline modelsfor the training set are derived and weight of base-line model is calculated as follows:?
= Accuracy(Base)Accuracy(Base) + Accuracy(WordNet)                    (8)weight of WordNet model is calculated in a similarway.
Relation 7 is used for calculating match1 andmatch2.
For calculating the accuracy we checkwhether the classification is done correctly or not.This procedure is repeated for each arbitrary pair(1770 iterations) to calculate the overall accuracyof the combinatory system.3.4.3 Voting based combination schemesIn many intelligent combinatory systems, the ma-jority voting scheme is an approach for determin-ing the final output.
Mitchell et alcollected datafor 9 participants.
In this approach a voting is per-formed on the models of 8 participants (participantj=1:9, j?i) for each concept pair (the two left outconcepts), to select the better model amongstWordNet and baseline models.
The better model isthe model that leads to higher accuracy in classify-ing the left out concepts of 8 participants (partici-pant j=1:9, j?i).
The selected model is used to testthe model for pi (participant i).Votes for selecting the better model for eachparticipant is calculated as shown in Table 1.match1Base and match1WordNet represent match1for baseline and WordNet models.match1= voteBase8 (match1Base) +voteWordNet8 (match1WordNet)        (9)Another approach is linear voting combination.This approach is based on calculating match1 andmatch2 for a model, based on a weighted linearcombination (relation 9).
The weights for a combi-natory model are calculated by a voting mechan-ism (Table 1).4 Results and DiscussionAs mentioned in section 2, it is possible to con-struct the feature matrix based on WordNet simi-larity measures.
Seven different measures weretested and models for 9 participants were trainedusing a 2-leave out cross validation.
Four similaritymeasures (Lin, JCN, LCH, and Resnik), two simi-larity relatedness measures (Lesk and Vector), acombination of Lin/ Lesk and a combination ofJCN/ Lesk are compared to the baseline.
The re-sults based on accuracies of these tests are shownin Table 3.
The accuracies are calculated fromcounts of match scores.
The match score betweenthe two predicted and the two observed fMRI im-ages was determined by which match (match1 ormatch2) had a higher Pearson correlation, eva-luated over 500 voxels with the most stable res-ponses across training presentations.The results of WordNet-based models are shownin Table 3.
As described in section 2 the similaritymeasures have limitation of POS.
The JCN meas-ure has the best accuracy among all single similari-ty measures.
The JCN measure has a better averageaccuracy (0.65) in comparison to the Lin measure(0.63).
The relatedness similarity does not have thelimitation of POS.
In spite of this advantage theLesk and Vector measures do not provide a betteraccuracy than the JCN similarity measure.
TheVector average accuracy (0.529) is worse thanLesk (0.622) and therefore just Lesk is consideredas a candidate of combination with other similaritymeasures like JCN and Lin.
In section 3 the idea ofcombining Lin (JCN) and Lesk measures was men-tioned.
These combinatory schemes led to better23accuracies among all single measures (Table 3).Despite the lower average accuracy of the Lin me-thod, the combination of Lin/Lesk achieved a bet-ter average accuracy in comparison to JCN/Leskcombination.
This is probably because of the lowercorrelation between Lin/Lesk feature vectors incomparison to JCN/Lesk feature vectors.
The cor-relation between different pairs of feature matricesextracted by WordNet-based similarity measuresare shown in Table 2.
The result shows that Leskfeature matrix has minimum correlations with allother WordNet-based feature matrices.
This is agood motivation to have the Lesk measure as acandidate to mix with other measures to extract amore informative feature matrix.
The Lesk featurematrix has the least correlation with Lin featurematrix among all WordNet-based feature matrices.Therefore as noted before, results of Table 3 showbetter accuracy for Lin/Lesk in comparison toJCN/Lesk.
But these accuracies are less than theaccuracies attained by the base method proposedby Mitchell et alMeasure 1 Measure 2 CorrelationLesk Lin 0.3929Lesk Resnik 0.4528Lesk JCN 0.5129Lesk LCH 0.5556Lin LCH 0.6182JCN Res 0.6357JCN Lin 0.7175JCN LCH 0.7234Lin Res 0.7400LCH Res 0.7946Table 2?
correlation between different pairs of Word-Net-based similarity (relatedness) measuresOne important reason of this shortage can be thedifference in sense (POS) between concepts (withnoun POS) and features (with verb POS).
Thisleads to limitation of WorldNet-based measures forconstructing better feature matrices.
Investigatingnew features of the same sense of POS betweenconcepts and features (associated with sensory-motor verbs) might lead to even better results.The Base and WordNet use ultimately differentapproaches to compute the similarity of each pairof concepts.
Several experiments like the union offeatures and the combination of system outputswas designed.
The union of the two feature matric-es (baseline feature matrix and Lin/Lesk featurematrix) does not lead to a better result (0.646).
Incontrary to the united features the combination ofthese systems gives a better performance.
Threedifferent schemes of combinatory systems are pro-posed in section 4.
The first scheme (linear combi-nation) uses a fixed ratio (?)
for combining theoutput match of the two systems.
As depicted inFigure 2 the ?
value is tuned and an optimum valueof ?=0.64 achieved an average accuracy of 0.775(Table 4).Figure 3- Improvement of linear combinatory schemeThe accuracies of participants P1 and P5 for ourimplemented baseline are almost the same as theaccuracies of P1 and P5 in Mitchell et al A com-parison of the accuracies for P1 and P5 attained bythe baseline model and the linear combinationscheme is illustrated in Figure 3.
The results showconsiderable improvement in accuracies when thecombinatory model is used.Figure 4- Comparison of linear Combinatory schemewith Baseline and WordNet24Measure/ Partic-ipant P1 P2 P3 P4 P5 P6 P7 P8 P9 AverageBaseline 0.828 0.845 0.752 0.798 0.776 0.658 0.705 0.615 0.680 0.740Lin 0.73 0.624 0.739 0.727 0.591 0.507 0.64 0.501 0.632 0.632Lesk 0.725 0.629 0.668 0.688 0.601 0.519 0.604 0.584 0.580 0.622Vector 0.603 0.599 0.551 0.553 0.567 0.451 0.509 0.446 0.476 0.529LCH 0.685 0.613 0.671 0.617 0.574 0.468 0.577 0.506 0.587 0.589RES 0.610 0.558 0.594 0.622 0.505 0.555 0.603 0.449 0.490 0.554JCN 0.797 0.638 0.765 0.713 0.671 0.525 0.504 0.568 0.642 0.647Lin/Lesk 0.807 0.677 0.767 0.812 0.672 0.645 0.690 0.502 0.697 0.697JCN/Lesk 0.790 0.604 0.718 0.789 0.641 0.593 0.593 0.514 0.667 0.656Table 3- Results of Different similarity measures compared to baselineApproach/ Par-ticipant P1 P2 P3 P4 P5 P6 P7 P8 P9 AverageLinear 0.877 0.847 0.827 0.862 0.798 0.696 0.734 0.605 0.728 0.775Concept-based 0.887 0.832 0.836 0.87 0.793 0.687 0.736 0.588 0.734 0.774Binary voting 0.894 0.837 0.829 0.858 0.796 0.684 0.758 0.612 0.736 0.778Weighted voting 0.905 0.840 0.861 0.882 0.808 0.710 0.761 0.614 0.755 0.793Table 4- Accuracies of different combinatory approachesThe improvement of this combinatory schemecan be viewed from another aspect.
Concept accu-racy, defined as classification   accuracy   of theconcept paired with each of the other 59 concepts,shows the performance of the system for each con-cept (Figure 4).
The concept accuracies of the li-near combinatory scheme are compared with theBaseline   and  WordNet   systems  and  resultsare illustrated in Figure 4.
The accuracy of someambiguous concrete nouns like ?saw?
are improvedin WordNet-based model and this improvement ismaintained by linear combinatory model.
Im-provements have been seen in combinatory model.The second scheme uses a cross validation ofthe remaining 58 concepts to train the system, fordeciding on each pair of concepts.
After training,each system (WordNet and Base) is assigned aweight according to its accuracy.
Decision on thetest pair is based on a weighted combination of thesystems.
The results of this scheme are shown inTable 4.
It has an improvement of 3.4% in compar-ison to the baseline model.The third scheme chooses another combinatorystrategy to decide on each test pair of concepts forparticipant Pi.
This scheme gathers votes from theother 8 participants as described in section 3.
Theresults are shown in Table 4.
Improvement of bi-nary voting scheme to baseline is almost as muchas the Improvement of linear and concept-basedschemes to baseline.
The weighted voting used amore flexible combination scheme, and led to animprovement of about 5.3% in comparison to base-line.A result is called statistically significant if it isimprobable to have occurred by   chance.
T-testParticipant H-value P-valueP1 1 7.73e-12P2 0 0.6610P3 1 5.55e-17P4 1 2.61e-12P5 1 0.0051P6 1 0.0004P7 1 8.28e-05P8 0 0.5275P9 1 3.95e-07Table 5- t-test of baseline and weighted voting outputvalues for 9 participants25was used to show whether the improvementachieved in this paper is statistically significant ornot.
The t-test was tested on output accuracies ofbaseline (with average accuracy 0.74) andweighted voting combinatory scheme (with aver-age accuracy 0.793) for 9 participants.
The resultsare shown in Table 5.
The weighted voting schemedoes not have improvement on P2  and P8  andresults  are  almost similar to baseline, thereforethe null hypothesis of equal mean is not rejected(H-value=0) at 0.05 confidence level.
For all par-ticipants with improvement on results, null hypo-thesis of equal mean is rejected (H-value=1) at0.05 confidence level.
This rejection shows that theimprovements are approved to be statistically sig-nificant for all participants with improvement.
Thet-test on overall 9 participants rejected null hypo-thesis with a P-value of almost zero.
This experi-ment shows the improvement achieved in thispaper is statistical significant.5 ConclusionIn this work, a new WordNet-based similarity ap-proach for deriving the sensory-motor feature vec-tors associated with the concrete nouns wasintroduced.
A correlation based combination ofWordNet measures is used to attain more informa-tive feature vectors.
The computational modeltrained by these feature vectors are combined withthe computational model trained with feature vec-tors extracted by a corpus based method.The combinatory scheme achieves a better aver-age accuracy in predicting the brain activity asso-ciated with the meaning of concrete nouns.Investigating new features of the same sense (POS)between concepts and non-verb features (asso-ciated with sensory-motor verbs) might lead toeven better results for WordNet-based Models.AcknowledgementsThe authors would like to thank the anonymousreviewers for their thorough review and their con-structive comments.ReferencesBanerjee, S., and Pedersen, T. 2003.
Extended glossoverlaps as a measure of semantic relatedness.
In Pro-ceedings of the Eighteenth International Joint Confe-rence on Artificial Intelligence, 805?810.Brants T., and Franz A., 2006,www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13.
LinguisticData Consortium, Philadelphia.Fellbaum C., 1998.
WordNet: An Electronic LexicalDatabase.
The MIT Press, Cambridge, MA.Hardoon D. R., Mourao-Miranda J., M. Brammer,Shawe-Taylor J.
2007. unsupervised analysis of fMRIdata using kernel canonical correlation.
Neuroimage,pp.
1250-1259.Kay K. N., Naselaris T., Prenger R. J., Gallant J. L.2008.
Identifying Natural Images from Human BrainActivity, Nature, pp.
352-355.Leacock C. and Chodorow M. 1998.
Combining lo-cal context andWordNet similarity for word sense iden-tification.
In C. Fellbaum, editor, WordNet: Anelectronic lexical database, pages 265?283.
MIT Press.Lin D. 1998.
An information-theoretic definition ofsimilarity.
In Proceedings of the International Confe-rence on Machine Learning, Madison.Mitchell T. M., et al 2008.
Predicting Human BrainActivity Associated with the Meanings of Nouns, Ameri-can Association for the Advancement of Science.Mitchell T. M., Hutchinson R. A., Niculescu R. S.,Pereira F., and Wang X..  2004.
Learning to DecodeCognitive States from Brain Images, Machine Learning,pp.
145-175.O?Toole A. J., Jiang F., Abdi H., and Haxby J. V..2005.
Partially distributed representations of objectsand faces in ventral temporal cortex.
Journal of Cogni-tive Neuroscience, pp.
580-590.Patwardhan S. 2003.
Incorporating dictionary andcorpus information into a context vector measure ofsemantic relatedness.
Master?s thesis, University ofMinnesota, Duluth.Pedersen T., Patwardhan S., and Michelizzi J.
2004.WordNet::Similarity - Measuring the relatedness ofConcepts.
Proceedings of Fifth Annual Meeting of theNorth American Chapter of the Association for Compu-tational Linguistics (NAACL-04), pp.
38-41.Resnik.
P. 1995.
Using information content to eva-luate semantic similarity in taxonomy.
In Proceedings ofthe 14th International Joint Conference on ArtificialIntelligence, pages 448?453.26
