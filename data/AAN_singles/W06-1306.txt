Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 37?45,Sydney, July 2006. c?2006 Association for Computational LinguisticsMultidimensional Dialogue ManagementSimon Keizer and Harry BuntDepartment of Language and Information ScienceFaculty of Arts, Tilburg UniversityP.O.
Box 90153, 5000 LE Tilburg, The Netherlands{s.keizer,harry.bunt}@uvt.nlAbstractIn this paper we present an approach todialogue management that supports thegeneration of multifunctional utterances.It is based on the multidimensional dia-logue act taxonomy and associated con-text model as developed in Dynamic Inter-pretation Theory (DIT).
The multidimen-sional organisation of the taxonomy re-flects that there are various aspects that di-alogue participants have to deal with si-multaneously during a dialogue.
Besidesperforming some underlying task, a par-ticipant also has to pay attention to vari-ous aspects of the communication processitself, including social conventions.Therefore, a multi-agent approach is pro-posed, in which for each of the dimensionsin the taxonomy a specialised dialogue actagent is designed, dedicated to the gener-ation of dialogue acts from that particulardimension.
These dialogue act agents op-erate in parallel on the information state ofthe system.
For a simplified version of thetaxonomy, a dialogue manager has beenimplemented and integrated into an inter-active QA system.1 IntroductionDuring (task-oriented) dialogues, the participantshave to deal with many different aspects of com-munication simultaneously.
Besides some under-lying task that may be performed through the dia-logue, there are also various aspects of managingthe communicative process itself, including deal-ing with social obligations.
Therefore, speakersoften use utterances that are multifunctional.We will present an approach to dialogue man-agement that accounts for the generation of multi-functional utterances.
The approach is based on adialogue theory involving a multidimensional dia-logue act taxonomy and associated context model.In this theory, called Dynamic Interpretation The-ory (DIT) (Bunt, 1996; Bunt, 2000a), a dialogue ismodelled as a sequence of (sets of) dialogue actsoperating on the Information State of each of theparticipants.
The dialogue acts are organised in ataxonomy that is multidimensional, i.e., each ut-terance may involve dialogue acts of at most onetype from each dimension.
The taxonomy has di-mensions for aspects like feedback, interaction-management, social obligations management andmanaging the underlying task.In a dialogue system developed according tothe principles of DIT, the information state is rep-resented through a context model, containing allinformation considered relevant for interpretinguser utterances an generating system utterances interms of dialogue acts.
Hence, given the multidi-mensionality of the taxonomy, the input interpre-tation components of the system result in severaldialogue acts for each utterance, at most one fromeach of the dimensions.
Using these recogniseduser dialogue acts, the context model is updated.On the other hand, the ultimate task for a di-alogue manager component of a dialogue systemis deciding which dialogue acts to generate.
So,again with the multidimensional organisation ofthe taxonomy in mind, we argue for a multi-agentapproach, in which the dialogue act generationtask is divided over several agents that operate inparallel on the context model, each agent beingdedicated to the generation of dialogue acts fromone particular dimension in the taxonomy.
Thisleads to the design of a number of so-called Di-37alogue Act Agents, including e.g.
a task-orientedagent, two feedback agents and an agent dealingwith social obligations management.The multi-agent approach to dialogue manage-ment itself is not new: JASPIS (Turunen andHakulinen, 2000; Salonen et al, 2004) is a multi-agent framework for dialogue systems which al-lows for implementations of several agents for thesame tasks, varying from input interpretation andoutput presentation to dialogue management.
De-pending on the situation, the agent that is mostappropriate for a given task is selected in a pro-cess involving several so-called ?evaluators?.
InJASPIS the multi-agent approach is aimed at flex-ibility and adaptiveness, while our approach fo-cuses more on supporting multidimensionality incommunication.In a very general sense, our dialogue manage-ment approach follows an information state updateapproach similar to the dialogue managers that aredeveloped within the TRINDI framework (Lars-son and Traum, 2000).
For example, Mathesonet al (2000) describe the implementation of a di-alogue management system focusing in the con-cepts of grounding and discourse obligations.An approach to dialogue management whichidentifies several simultaneous processes in thegeneration of system utterances, is described in(Stent, 2002).
In this approach, which is imple-mented in the TRIPS dialogue system, dialoguecontributions are generated through three corecomponents operating independently and concur-rently, using a system of conversation acts or-ganised in several levels (Traum and Hinkelman,1992).Although there are apparent similarities be-tween our approach and that of the TRINDI baseddialogue managers and the TRIPS system, thereare clear differences as well, which for an impor-tant part stem from the system of dialogue actsused and the way the information state is organ-ised.
More particularly, the way in which mech-anisms for generating dialogue acts along multi-ple dimensions are modelled and implemented bymeans of multiple agents, differs from existing ap-proaches.This paper is organised as follows.
First we ex-plain the closely connected DIT notions of dia-logue act and information state, and the multi-dimensional dialogue act taxonomy and contextmodel (Sections 2 and 3).
We then introducethe multi-agent approach to dialogue management(Section 4) and illustrate it by a description ofthe current implementation (Section 4.1).
Thisimplementation is carried out in the PARADIMEproject (PARallel Agent-based DIalogue Manage-ment Engine), which is part of the multiprojectIMIX (Interactive Multimodal Information Ex-traction).
The PARADIME dialogue manager isintegrated into an interactive question-answeringsystem that is developed in a collaboration be-tween several projects participating in IMIX.
Thepaper ends with conclusions and directions for fu-ture research (Section 5).2 The DIT dialogue act taxonomyBased on studies of a variety of dialogues fromseveral dialogue corpora, a dialogue act taxonomywas developed consisting of a number of dimen-sions, reflecting the idea that during a dialogue,several aspects of the communication need to beattended to by the dialogue participants (Bunt,2006).
Even within single utterances, several as-pects are dealt with at the same time, i.e., in gen-eral, utterances are multifunctional.
The multidi-mensional organisation of the taxonomy supportsthis multifunctionality in that it allows several di-alogue acts to be performed in each utterance, atmost one from each dimension.
The 11 dimen-sions of the taxonomy are listed below, with briefdescriptions and/or specific dialogue act types inthat dimension.
For convenience, the dimensionsare further grouped into so-called layers.
At thetop level are two layers: one for dialogue con-trol acts and one coinciding with the task-domaindimension.
Dialogue control is further dividedinto 3 layers: Feedback (2 dimensions), Interac-tion Management (7 dimensions), and a layer co-inciding with the Social Obligations Managementdimension.?
Dialogue Control?
Feedback1.
Auto-Feedback: acts dealing with thespeaker?s processing of the addressee?sutterances; contains positive and nega-tive feedback acts on the levels of per-ception, interpretation, evaluation, andexecution;2.
Allo-Feedback: acts dealing with theaddressee?s processing of the speaker?sprevious utterances (as viewed by the38speaker); contains positive and negativefeedback-giving acts and feedback elic-itation acts, both on the levels of per-ception, interpretation, evaluation, andexecution;?
Interaction management3.
Turn Management: turn accepting,giving, grabbing, keeping;4.
Time Management: stalling, pausing;5.
Dialogue Structuring: opening,preclosing, closing, dialogue act an-nouncement;6.
Partner Processing Management:completion, correct-misspeaking;7.
Own Processing Management: errorsignalling, retraction, self-correction;8.
Contact Management: contact check,contact indication;9.
Topic Management: topic introduction,closing, shift, shift announcement;10.
Social Obligations Management: saluta-tion, self-introduction, gratitude, apology,valediction;11.
Task/domain: acts that concern the specificunderlying task and/or domain.Formally, a dialogue act in DIT consists of aSemantic Content and a Communicative Function,the latter specifying how the information stateof the addressee is to be updated with the for-mer.
A dialogue act in a particular dimensionmay have either a dimension-specific communica-tive function, or a General-Purpose communica-tive function with a content type (type of semanticcontent) in that dimension.
The general-purposecommunicative functions are hierarchically or-ganised into the branches of Information Trans-fer and Action Discussion functions, InformationTransfer consisting of information-seeking (e.g.,WH-QUESTION, YN-QUESTION, CHECK) andinformation-providing functions (e.g., INFORM,WH-ANSWER, YN-ANSWER, CONFIRM), andAction Discussion consisting of commissives(e.g., OFFER, PROMISE, ACCEPT-REQUEST) anddirectives (e.g., INSTRUCT, REQUEST, DECLINE-OFFER).The taxonomy is currently being evaluated inannotation experiments, involving several anno-tators and several dialogue corpora.
Measuringinter-annotator agreement will give an indicationof the usability of the taxonomy and annotationscheme.
A first analysis has resulted in promisingscores (Geertzen and Bunt, 2006).3 The DIT context modelThe Information State according to DIT is repre-sented by a Context Model, containing all infor-mation considered relevant for interpreting userutterances (in terms of dialogue acts) and gener-ating system dialogue acts (leading to system ut-terances).
The contents of the context model aretherefore very closely related to the dialogue acttaxonomy; in (Bunt and Keizer, 2005) it is ar-gued that the context model serves as a formal se-mantics for dialogue annotation, such an annota-tion being a kind of underspecified semantic rep-resentation.
In combination with additional gen-eral conceptual considerations, the context modelhas evolved into a five component structure:1.
Linguistic Context: linguistic informationabout the utterances produced in the dialogueso far (a kind of ?extended dialogue history?
);information about planned system dialogueacts (a ?dialogue future?);2.
Semantic Context: contains current infor-mation about the task/domain, including as-sumptions about the dialogue partner?s infor-mation;3.
Cognitive Context: the current processingstates of both participants (on the levels ofperception, interpretation, evaluation, andtask execution), as viewed by the speaker;4.
Physical and Perceptual Context: the percep-tible aspects of the communication processand the task/domain;5.
Social Context: current communicative pres-sures.In Figure 1, a feature structure representation ofthe context model is given, in which the five com-ponents have been specified in further detail.
Thisspecification forms the basis for the dialogue man-ager being implemented in the PARADIME project.The Linguistic Context contains features forstoring dialogue acts performed in the dialogue sofar: user utts and system utts, having lists of di-alogue act representations as values.
It also hasfeatures for information about topics and conver-sational structure: topic struct and conv state re-spectively.
Finally, there are two features that39????????????????????????????????????
?LingContext :????????
?user utts : ?last user dial act = uda0 , uda?1 , uda?2 , .
.
.
?system utts : ?last system dial act = sda0 , sda?1 , sda?2 , .
.
.
?topic struct : ?referents?conv state : opening |body |closingcandidate dial acts : .
.
.dial acts pres : .
.
.????????
?SemContext :???
?task progress : comp quest |quest qa|answ eval |user satuser info needs : ?.
.
.
,[question : .
.
.satisfied : +|?
], .
.
.
?qa answers : ?.
.
.????
?CogContext :[own proc state : [proc problem : perc|int |eval |exec|none]partner proc state : [proc problem : perc|int |eval |exec|none]]PhysPercContext :[ ]SocContext :[reactive pressures : none|grt |apo|thk |valedinteractive pressures : none|grt |apo|thk |valed]????????????????????????????????????
?Figure 1: Feature structure representation of the PARADIME context model.are related to the actual generation of system di-alogue acts: candidate dial acts stores the dia-logue acts generated by the dialogue act agents,and dial acts pres stores combined dialogue actsfor presentation as system output; in Section 4,this will be discussed in more detail.The specification of the Semantic Context isdetermined by the character of the task-domain.In Section 4.1, the task-domain of interactivequestion-answering on encyclopedic medical in-formation will be discussed and from that, thespecification of the Semantic Context for this pur-pose.The Cognitive Context is specified by means oftwo features, representing the processing states ofthe system (own proc state) and the user (part-ner proc state).
Both features indicate whether ornot a processing problem was encountered, and ifso, on which level of processing this happened.The Physical and Perceptual Context is consid-ered not to be relevant for the current system func-tionality.The Social Context is specified in terms of re-active and interactive pressures; the correspond-ing features indicate whether or not a pressure ex-ists and if so, for which social obligations manage-ment act it is a pressure (e.g., reactive pressures:grt indicates a pressure for the system to respondto a greeting).4 Dialogue Act AgentsHaving discussed the dialogue act taxonomy andcontext model in DIT, we can now move on to thedialogue management approach that is also closelyconnected to these concepts.
Having 11 dimen-sions of dialogue acts that each attend to a dif-ferent aspect of communication, the generation of(system) dialogue acts should also happen alongthose 11 dimensions.
As a dialogue act in a di-mension can be selected independent of the otherdimensions, we propose to divide the generationprocess over 11 Dialogue Act Agents operating inparallel on the information state of the system,each agent dedicated to generating dialogue actsfrom one particular dimension.All of the dialogue act agents continuouslymonitor the context model and, if appropriate, tryto generate candidate dialogue acts from their as-sociated dimension.
This process of monitoringand act generation is modelled through a trigger-ing mechanism: if the information state satisfiesthe agent?s triggering conditions, i.e., if there isa motivation for generating a dialogue act from aparticular dimension, the corresponding agent getstriggered and tries to generate such a dialogue act.For example, the Auto-Feedback Agent gets trig-gered if a processing problem is recorded in theOwn Processing State of the Cognitive Context.The agent then tries to generate a negative auto-feedback act in order to solve the processing prob-40lem (e.g., ?Could you repeat that please??
or ?Didyou say ?five???).
The Auto-Feedback Agent mayalso be triggered if it has reason to believe that theuser is not certain that the system has understooda previous utterance, or simply if it has not givenany explicit positive feedback for some time.
Inthese cases of triggering, the agent tries to gener-ate a positive auto-feedback act.Hence the dialogue management process in-volves 11 dialogue act agents that operate in par-allel on the context model.
The dialogue acts gen-erated by these agents are kept in the linguisticcontext as candidates.
The selection of dialogueacts from different dimensions may happen inde-pendently, but for their order of performance andtheir combination, the relative importance of thedimensions at the given point in the dialogue hasto be taken into account.An additional Evaluation Agent monitors thelist of candidates and decides which of them canbe combined into a multifunctional system utter-ance for generation, and when.
Some of the dia-logue act candidates may have higher priority andshould be generated at once, some may be storedfor possible generation in later system turns, andsome will already be implicitly performed throughthe performance of other candidate acts.4.1 A dialogue manager for interactive QAThe current implementation of the PARADIMEdialogue manager is integrated in an interactivequestion-answering (QA) system, as developedthe IMIX multiproject.
The task-domain at handconcerns encyclopedic information in the medicaldomain, in particular RSI (Repetitive Strain In-jury).
The system consists of several input anal-ysis modules (ASR, syntactic analysis in termsof dependency trees, and shallow semantic tag-ging), three different QA modules that take self-contained domain questions and return answersretrieved from several electronic documents withtext data in the medical domain, and a presentationmodule that takes the output from the dialoguemanager, possibly combining any QA-answers tobe presented, into a multimodal system utterance.The dialogue management module providessupport for more interactive, coherent dialogues,in which problems can be solved about both com-munication and question-answering processes.
Ininteraction with the user, the system should playthe role of an Information Search Assistant (ISA).This HCI metaphor posits that the dialogue systemis not an expert on the domain, but merely assiststhe user in formulating questions about the domainthat will lead to QA answers from the QA mod-ules satisfying the user?s information need (Akkeret al, 2005).In the context model for this dialogue manager,as represented by the feature structure in Figure 1,the Semantic Context has been further specifiedaccording to this underlying task.
It contains astate variable for keeping track of the question-answering process (the feature task progress withvalues to distinguish between the states of com-posing a self-contained question to send to the QAmodules, waiting for the QA results in case a QA-question has been sent, evaluating the QA results,and discussing the results with the user).
Also, theSemantic Context keeps a record of user?s infor-mation need, by means of a list user info needsof ?information need?
specifications in terms ofsemantic descriptions of domain questions andwhether or not these info-needs have been satis-fied.For the first version of the dialogue managerwe have defined a limited system functionality,and following from that a simplified version ofthe dialogue act taxonomy.
This simplificationmeans for example that Social Obligations Man-agement (SOM) and the various dimensions inthe Interaction Management (IM) layer have beenmerged into one dimension, following the obser-vation that utterances with a SOM function veryoften also have a function in the IM layer, es-pecially in human-computer dialogue; see (Bunt,2000b).
Also several general-purpose commu-nicative functions have been clustered into singletypes.
Table 1 lists the dialogue acts that the dia-logue act recogniser is able to identify from userutterances.GP AUF IM-SOMYN-Question PosAutoFb Init-OpenWH-Question NegAutoFb-Int Init-CloseH-Question NegAutoFb-EvalRequestInstructTable 1: Dialogue act types for interpreting userutterances.Table 2 lists the dialogue acts that can be gen-erated by the dialogue manager.
Task-domainacts, generally answers to questions about the do-41main, consist of a general-purpose function (eithera WH-ANSWER or UNC-WH-ANSWER; the latterreflecting that the speaker is uncertain about the in-formation provided) with a semantic content con-taining the answers obtained from QA.AUF ALF IM-SOMNegAutoFb-Int Fb-Elicit React-OpenNegAutoFb-Exe React-CloseTable 2: Dialogue act types for generating systemresponses.The above considerations have resulted in a di-alogue manager containing 4 dialogue act agentsthat operate on a slightly simplified version of thecontext model as specified in Figure 1: a Task-Oriented (TO) Agent, an Auto-Feedback (AUF)Agent, an Allo-Feedback (AUF) Agent, and anInteraction Management and Social ObligationsManagement (IMSOM) Agent.
In addition, a (cur-rently very simple) Evaluation Agent takes care ofmerging candidate dialogue acts for output presen-tation.In Appendices A.1 and A.2, two example di-alogues with the IMIX demonstrator system aregiven, showing system responses based on can-didate dialogue acts from several dialogue actagents.
The ISA metaphor is reflected in the sys-tem behaviour especially in the way in which QAresults are presented to the user.
In system utter-ances S2 and S3 in Appendix A.1, for example,the answer derived from the retrieved QA results isisolated from the first part of the system utterance,showing that the system has a neutral attitude con-cerning that answer.4.1.1 The Task-Oriented AgentThe TO-Agent is dedicated to the generation oftask-specific dialogue acts, which in practice in-volves ANSWER dialogue acts intended to satisfythe user?s information need about the (medical)domain as indicated through his/her domain ques-tions.
The agent is triggered if a new informationneed is recorded in the Semantic Context.
Once ithas been triggered, the agent sends a request to theQA modules to come up with answers to a ques-tion asked, and evaluates the returned results.
Thisevaluation is based on the number of answers re-ceived and the confidence scores of the answers;the confidence scores are also part of the output ofthe QA modules.
If the QA did not find any an-swers or if the answers produced had confidencescores that were all below some lower threshold,the TO-Agent will not generate a dialogue act, butwrite an execution problem in the Own Process-ing State of the Cognitive Context (which causesthe Auto-Feedback Agent to be triggered, see Sec-tion 4.1.2; an example can be found in the dia-logue in Appendix A.2).
Otherwise, the TO-Agenttries to make a selection from the QA answersto be presented to the user.
If this selection willend up containing extremely many answers, again,an execution problem is written in the CognitiveContext (the question might have been too gen-eral to be answerable).
Otherwise, the selectionwill be included in an answer dialogue act, eithera WHANSWER, or UNCWHANSWER (uncertainwh-answer) in case the confidence scores are be-low some upper threshold.
System utterances S1and S2 in the example dialogue in Appendix A.1illustrate this variation.
The selection is narroweddown further if there is a subselection of answerswith confidences that are significantly higher thanthose of the other answers in the selection.4.1.2 The Auto-Feedback-AgentThe AUF-Agent is dedicated to the generationof auto-feedback dialogue acts.
It currently pro-duces negative auto-feedback acts on the levelsof interpretation (?I didn?t understand what yousaid?
), evaluation (?I do not know what to do withthis?)
and execution (?I could not find any answersto your question?).
It may also decide to occa-sionally give positive feedback to the user.
In thefuture, we would also like this agent to be ableto generate articulate feedback acts, for examplewith the purpose of resolving reference resolutionproblems, as in:U: what is RSI?S: RSI (repetitive strain injury) is a pain ordiscomfort caused by small repetitive move-ments or tensions.U: how can it be prevented?S: do you mean ?RSI?
or ?pain?
?4.1.3 The Allo-Feedback AgentThe ALF-Agent is dedicated to the generationof allo-feedback dialogue acts.
For example, itmay generate a feedback-elicitation act if it hasreason to believe that the user might not be sat-isfied with an answer (?Was this an answer to yourquestion??
).424.1.4 Interaction Management and SocialObligations Management AgentThe IM-SOM Agent is dedicated to the gener-ation of social obligations management acts, pos-sibly also functioning as dialogue structuring acts(opening resp.
closing a dialogue through a greet-ing resp.
valediction act).
It gets triggered ifcommunicative pressures are recorded in the So-cial Context.
Currently it only responds to re-active pressures as caused by initiative greetingsand goodbyes.
The example dialogues in Appen-dices A.1 and A.2 illustrate this type of social be-haviour.4.1.5 Multi-agent Architecture of theDialogue ManagerIn Figure 2, a schematic overview of the multi-agent dialogue manager is given.
It shows thecontext model with four components (for now, thePhysical and Perceptual Context is considered tobe of minor importance and is therefore ignored),a set of dialogue act agents, and an EvaluationAgent.
The dialogue act agents each monitor thecontext model and may be triggered if certain con-ditions are satisfied.
The TO-agent may also writeto the Cognitive Context (particularly in case ofexecution problems).
All agents may constructa dialogue act and write it in the candidates listin the Linguistic Context.
The Evaluation Agentmonitors this candidates list and selects one ormore dialogue acts from it for presentation as sys-tem output.
In this way, a control module maydecide to take this combination of dialogue act forpresentation anytime and send it to the presenta-tion module to produce a system utterance.With this initial design of a multi-agent dia-logue manager, the system is able to support mul-tifunctional output.
The beginning of the exampledialogue in Appendix A.1 illustrates multifunc-tionality, both in input interpretation and outputgeneration.
The system has recognised two dia-logue acts in processing U1 (a conventional open-ing and a domain question), and S1 is generatedon the basis of two candidate dialogue acts gen-erated by different dialogue act agents: the IM-SOM-Agent (generated the react-greeting act) andthe TO-Agent (generated the answer act).5 Conclusions and future workWe have presented a dialogue management ap-proach supporting the generation of multifunc-candidatedialogue actsIM?SOM?AgentTO?AgentAUF?AgentALF?AgentSemantic ContextCognitive ContextSocial ContextLinguistic Contextcandidatedialogue actsEval?Agentdialogue actsfor presentationDIALOGUE ACT AGENTSCONTEXT MODELFigure 2: Architecture of the PARADIME dialoguemanager.tional utterances.
The approach builds on a di-alogue theory involving a multidimensional dia-logue act taxonomy and an information state onwhich the dialogue acts operate.
Several dialogueacts from different dimensions are generated bydialogue act agents associated with these dimen-sions, and can thus be combined into multifunc-tional system utterances.A first implementation of a dialogue managerfollowing this multi-agent approach has been in-tegrated into an interactive QA system and sup-ports a limited range of dialogue acts from theDIT taxonomy, both for interpreting user utter-ances and generating system utterances.
The sys-tem is able to attend to different aspects of thecommunication simultaneously, involving reactivesocial behaviour, answering domain questions andgiving feedback about utterance interpretation andthe question-answering process.Future development will involve extending therange of dialogue acts to be covered by the dia-logue manager, for a part following from the def-inition of an extended system functionality, andconsequently, extending the set of dialogue actagents.
This also has consequences for the Eval-uation Agent: the process of combination and se-lection will be more complex if more dialogue acttypes can be expected and if the dialogue acts havea semantic content that is more than just a collec-tion of QA-answers.In terms of system functionality we aim at sup-43port for generating articulate feedback, i.e., feed-back acts that are not merely signalling processingsuccess or failure, but (in case of negative feed-back) also contain a further specification of theprocessing problem at hand.
For example, the sys-tem may have encountered problems in processingcertain parts of a user utterance, or in resolving ananaphor; then it should be able to ask the user aspecific question in order to obtain the informa-tion required to solve the processing problem (seethe example in Section 4.1.2).
The articulate feed-back acts may also involve dealing with problemsin the question answering process, where the sys-tem should be able to give specific instructions tothe user to reformulate his question or give addi-tional information about his information need.In addition to supporting generation of articu-late feedback acts, we also aim at dialogues be-tween user and system that are more coherent andnatural, i.e., the system should be more aware ofthe conversational structure, and display more re-fined social behaviour.
Not only should it gener-ate simple reactions to greetings, apologies, andgoodbyes; it should also be able to generate initia-tive social acts, for example, apologies after sev-eral cases of negative auto-feedback.The extended set of dialogue acts will also leadto an extended context model.
Related to thecontext model and updating mechanism is on-going work on belief dynamics and groundingin DIT (Morante and Bunt, 2005).
The definedmechanisms for the creation, strengthening, adop-tion, and cancelling of beliefs and goals in thecontext model are currently being implementedin a demonstrator tool and will also be integratedin the information state update mechanism of thePARADIME dialogue manager.AcknowledgementThis work is part of PARADIME (Parallel Agent-based Dialogue Management Engine), which is asubproject of IMIX (Interactive Multimodal Infor-mation eXtraction), a multiproject on Dutch lan-guage and speech technology, funded by the Dutchnational science foundation (NWO).We would like to thank the reviewers for theirvaluable comments, which really helped us to im-prove our paper.ReferencesR.
op den Akker, H. Bunt, S. Keizer, and B. vanSchooten.
2005.
From question answering to spo-ken dialogue: Towards an information search assis-tant for interactive multimodal information extrac-tion.
In Proceedings of the 9th European Confer-ence on Speech Communication and Technology, In-terspeech 2005, pages 2793?2796.H.
Bunt and S. Keizer.
2005.
Dialogue semanticslinks annotation to context representation.
In JointTALK/AMI Workshop on Standards for MultimodalDialogue Context.
http://homepages.inf.ed.ac.uk/olemon/standcon-SOI.html.H.
Bunt.
1996.
Dynamic interpretation and dia-logue theory.
In M.M.
Taylor, F. Ne?el, and D.G.Bouwhuis, editors, The Structure of Multimodal Di-alogue, Volume 2, pages 139?166.
John Benjamins.H.
Bunt.
2000a.
Dialogue pragmatics and contextspecification.
In H. Bunt and W. Black, editors,Abduction, Belief and Context in Dialogue, Studiesin Computational Pragmatics, pages 81?150.
JohnBenjamins.H.
Bunt.
2000b.
Non-problems and social obliga-tions in human-computer conversation.
In Proceed-ings of the 3rd International Workshop on Human-Computer Conversation, pages 36?41.H.
Bunt.
2006.
Dimensions in dialogue act anno-tation.
In Proceedings Fifth International Confer-ence on Language Resources and Evaluation (LREC2006).J.
Geertzen and H. Bunt.
2006.
Measuring annotatoragreement in a complex hierarchical dialogue act an-notation scheme.
In 7th SIGdial Workshop on Dis-course and Dialogue.S.
Larsson and D. Traum.
2000.
Information stateand dialogue management in the TRINDI dialoguemove engine toolkit.
Natural Language Engineer-ing: Special Issue on Best Practice in Spoken Lan-guage Dialogue Systems, 6(3?4):323?340.C.
Matheson, M. Poesio, and D. Traum.
2000.
Mod-eling grounding and discourse obligations using up-date rules.
In Proceedings of the First Annual Meet-ing of the North American Chapter of the ACL.R.
Morante and H. Bunt.
2005.
Micro-analysis of thebelief transfer in information dialogues.
In C. Gar-dent and B. Gaiffe, editors, DIALOR?05 Proceed-ings of the Ninth Workshop on the Semantics andPragmatics of Dialogue, pages 135?138.E.P.
Salonen, M. Hartikainen, M. Turunen, J. Hakuli-nen, and J.A.
Funk.
2004.
Flexible dialogue man-agement using distributed and dynamic dialoguecontrol.
In Proceedings of ICSLP 2004.A.J.
Stent.
2002.
A conversation acts model for gen-erating spoken dialogue contributions.
Computer44Speech and Language, Special Issue on Spoken Lan-guage Generation, 16(3?4):313?352.D.R.
Traum and E.R.
Hinkelman.
1992.
Conversa-tion acts in task-oriented spoken dialogue.
Compu-tational Intelligence, 3(8):575?599.M.
Turunen and J. Hakulinen.
2000.
Jaspis ?
a frame-work for multilingual adaptive speech applications.In Sixth International Conference on Spoken Lan-guage Processing (ICSLP 2000), volume 2, pages719?722.A Example dialoguesA.1 Example 1: greetings, answers andunc-answersU1: hallo, ik wilde graag weten wat rsi is(hello, I wanted to know what is rsi)[INIT-OPEN; INDWHQ, task-domain]S1: Hallo.
Ik heb de volgende informatie gevonden:(repetitive strain injury) Algemene benaming voor(blijvende) beschadigingen aan het lichaam ten gevolgevan langdurige, eentonige werkzaamheden.(Hello.
I found the following information: ...)[REACT-OPEN; ANSWER, task-domain]U2: wat moet ik doen als ik denk rsi te hebben(what should I do if I think I have RSI)[WHQUESTION, task-domain]S2: De volgende informatie levert mogelijk een antwoordop uw vraag:Ontspanning betekent minder stress, minder aange-spannen spieren en dus minder kans op RSI.
(The following information might answer your ques-tion: ...)[UNCANSWER, task-domain]U3: helpt ontspanning ook als ik eenmaal rsi heb(does relaxing help as well, once I have RSI)[YNQUESTION, task-domain]S3: Ik heb de volgende informatie gevonden:Behandelmethoden bij RSI Er zijn vele wegen die naarRome leiden.
Verbetering van de doorbloeding ...(I found the following information: ...)[ANSWER, task-domain]U4: oke, bedankt(okay, thanks)[INIT-CLOSE]S4: tot ziens.
U kunt de buttons gebruiken om te stoppenof om een nieuwe dialoog te beginnen.(Goodbye.
You may use the buttons to stop or to start anew dialogue.
)[REACT-CLOSE]A.2 Example 2: negative execution-feedbackU1: hallo!(hello!
)[INIT-OPEN]S1: Hallo.
Stel uw vraag.(Hello.
Ask your question.
)[REACT-OPEN]U2: wat kan ik als beelschermwerker tegen rugpijn doen(what can I, a person working behind a computerscreen all the time, do against pain in the back)[WHQUESTION, task-domain]S2: Ik heb helaas geen antwoorden kunnen vinden; probeerde vraag te herformuleren.
(Unfortunately, I could not find any answers; please tryto reformulate the question)[NEGAUTOFB-EXEC]U3: wat kan men tegen rugpijn doen(what can be done against pain in the back)[WHQUESTION, task-domain]S3: ik heb de volgende informatie gevonden:Deze is afhankelijk van de oorzaak.
Rugpijn die doorinstabiliteit of door tekortschietende spieren en ban-den ontstaat, kan behandeld worden met houdingscor-rigerende oefentherapie.
(I found the following information: ...)[ANSWER, task-domain]U4: bedankt!(thanks!
)[INIT-CLOSE]S4: Tot ziens.
U kunt de buttons gebruiken om te stoppenof om een nieuwe dialoog te beginnen.(Goodbye.
U may use the buttons to stop or to start anew dialogue.
)[REACT-CLOSE]45
