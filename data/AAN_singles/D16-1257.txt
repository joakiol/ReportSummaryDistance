Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2331?2336,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsParsing as Language ModelingDo Kook ChoeBrown UniversityProvidence, RIdc65@cs.brown.eduEugene CharniakBrown UniversityProvidence, RIec@cs.brown.eduAbstractWe recast syntactic parsing as a languagemodeling problem and use recent advances inneural network language modeling to achievea new state of the art for constituency PennTreebank parsing ?
93.8 F1 on section 23, us-ing 2-21 as training, 24 as development, plustri-training.
When trees are converted to Stan-ford dependencies, UAS and LAS are 95.9%and 94.1%.1 IntroductionRecent work on deep learning syntactic parsingmodels has achieved notably good results, e.g., Dyeret al (2016) with 92.4 F1 on Penn Treebank con-stituency parsing and Vinyals et al (2015) with92.8 F1.
In this paper we borrow from the ap-proaches of both of these works and present aneural-net parse reranker that achieves very good re-sults, 93.8 F1, with a comparatively simple architec-ture.In the remainder of this section we outline the ma-jor difference between this and previous work ?viewing parsing as a language modeling problem.Section 2 looks more closely at three of the mostrelevant previous papers.
We then describe our ex-act model (Section 3), followed by the experimentalsetup and results (Sections 4 and 5).
(a)SVPNPcatschaseNPdogs(b)(S (NP dogs )NP (VP chase (NP cats )NP )VP )SFigure 1: A tree (a) and its sequential form (b).There is a one-to-one mapping between a tree and itssequential form.
(Part-of-speech tags are not used.
)1.1 Language ModelingFormally, a language model (LM) is a probabilitydistribution over strings of a language:P (x) = P (x1, ?
?
?
, xn)=n?t=1P (xt|x1, ?
?
?
, xt?1), (1)where x is a sentence and t indicates a word posi-tion.
The efforts in language modeling go into com-puting P (xt|x1, ?
?
?
, xt?1), which as described nextis useful for parsing as well.1.2 Parsing as Language ModelingA generative parsing model parses a sentence (x)into its phrasal structure (y) according toargmaxy?
?Y(x)P (x,y?
),where Y(x) lists all possible structures of x.
If wethink of a tree (x,y) as a sequence (z) (Vinyals et2331al., 2015) as illustrated in Figure 1, we can define aprobability distribution over (x,y) as follows:P (x,y) = P (z) = P (z1, ?
?
?
, zm)=m?t=1P (zt|z1, ?
?
?
, zt?1), (2)which is equivalent to Equation (1).
We havereduced parsing to language modeling and canuse language modeling techniques of estimatingP (zt|z1, ?
?
?
, zt?1) for parsing.2 Previous WorkWe look here at three neural net (NN) models clos-est to our research along various dimensions.
Thefirst (Zaremba et al, 2014) gives the basic languagemodeling architecture that we have adopted, whilethe other two (Vinyals et al, 2015; Dyer et al, 2016)are parsing models that have the current best resultsin NN parsing.2.1 LSTM-LMThe LSTM-LM of Zaremba et al (2014) turns(x1, ?
?
?
, xt?1) into ht, a hidden state of anLSTM (Hochreiter and Schmidhuber, 1997; Gers etal., 2003; Graves, 2013), and uses ht to guess xt:P (xt|x1, ?
?
?
, xt?1) = P (xt|ht)= softmax(Wht)[xt],where W is a parameter matrix and [i] indexes ithelement of a vector.
The simplicity of the modelmakes it easily extendable and scalable, which hasinspired a character-based LSTM-LM that workswell for many languages (Kim et al, 2016) andan ensemble of large LSTM-LMs for English withastonishing perplexity of 23.7 (Jozefowicz et al,2016).
In this paper, we build a parsing model basedon the LSTM-LM of Zaremba et al (2014).2.2 MTPVinyals et al (2015) observe that a phrasal struc-ture (y) can be expressed as a sequence and builda machine translation parser (MTP), a sequence-to-sequence model, which translates x into y using aconditional probability:P (y|x) = P (y1, ?
?
?
, yl|x)=l?t=1P (yt|x, y1, ?
?
?
, yt?1),where the conditioning event (x, y1, ?
?
?
, yt?1) ismodeled by an LSTM encoder and an LSTM de-coder.
The encoder maps x into he, a set of vectorsthat represents x, and the decoder obtains a sum-mary vector (h?t) which is concatenation of the de-coder?s hidden state (hdt ) and weighted sum of wordrepresentations (?ni=1 ?ihei ) with an alignment vec-tor (?).
Finally the decoder predicts yt given h?t.Inspired by MTP, our model processes sequentialtrees.2.3 RNNGRecurrent Neural Network Grammars (RNNG), agenerative parsing model, defines a joint distributionover a tree in terms of actions the model takes to gen-erate the tree (Dyer et al, 2016):P (x,y) = P (a) =m?t=1P (at|a1, ?
?
?
, at?1), (3)where a is a sequence of actions whose output pre-cisely matches the sequence of symbols in z, whichimplies Equation (3) is the same as Equation (2).RNNG and our model differ in how they computethe conditioning event (z1, ?
?
?
, zt?1): RNNG com-bines hidden states of three LSTMs that keep trackof actions the model has taken, an incomplete treethe model has generated and words the model hasgenerated whereas our model uses one LSTM?s hid-den state as shown in the next section.3 ModelOur model, the model of Zaremba et al (2014) ap-plied to sequential trees and we call LSTM-LM fromnow on, is a joint distribution over trees:P (x,y) = P (z) =m?t=1P (zt|z1, ?
?
?
, zt?1)=m?t=1P (zt|ht)=m?t=1softmax(Wht)[zt],2332where ht is a hidden state of an LSTM.
Due to lackof an algorithm that searches through an exponen-tially large phrase-structure space, we use an n-bestparser to reduce Y(x) to Y ?
(x), whose size is poly-nomial, and use LSTM-LM to find y that satisfiesargmaxy?
?Y ?
(x)P (x,y?).
(4)3.1 Hyper-parametersThe model has three LSTM layers with 1,500 unitsand gets trained with truncated backpropagationthrough time with mini-batch size 20 and step size50.
We initialize starting states with previous mini-batch?s last hidden states (Sutskever, 2013).
Theforget gate bias is initialized to be one (Jozefowiczet al, 2015) and the rest of model parameters aresampled from U(?0.05, 0.05).
Dropout is appliedto non-recurrent connections (Pham et al, 2014)and gradients are clipped when their norm is big-ger than 20 (Pascanu et al, 2013).
The learningrate is 0.25 ?
0.85max(?15, 0) where  is an epochnumber.
For simplicity, we use vanilla softmax overan entire vocabulary as opposed to hierarchical soft-max (Morin and Bengio, 2005) or noise contrastiveestimation (Gutmann and Hyva?rinen, 2012).4 ExperimentsWe describe datasets we use for evaluation, detailtraining and development processes.14.1 DataWe use the Wall Street Journal (WSJ) of the PennTreebank (Marcus et al, 1993) for training (2-21),development (24) and testing (23) and millions ofauto-parsed ?silver?
trees (McClosky et al, 2006;Huang et al, 2010; Vinyals et al, 2015) for tri-training.
To obtain silver trees, we parse the en-tire section of the New York Times (NYT) of thefifth Gigaword (Parker et al, 2011) with a prod-uct of eight Berkeley parsers (Petrov, 2010)2 andZPar (Zhu et al, 2013) and select 24 million treeson which both parsers agree (Li et al, 2014).
We donot resample trees to match the sentence length dis-tribution of the NYT to that of the WSJ (Vinyals et1The code and trained models used for experiments areavailable at github.com/cdg720/emnlp2016.2We use the reimplementation by Huang et al (2010).Figure 2: Perplexity and F1 on the development setat each epoch during training.n Oracle Final Exact10 94.0 91.2 39.850 96.7 91.7 40.051o 100 93.9 49.7100 96.3 91.7 39.9500 97.0 91.8 40.0Table 1: The performance of LSTM-LM (G) withvarying n-best parses on the dev set.
Oracle refersto Charniak parser?s oracle F1.
Final and Exact re-port LSTM-LM (G)?s F1 and exact match percent-age respectively.
To simulate an optimal scenario,we include gold trees to 50-best trees and rerankthem with LSTM-LM (G) (51o).al., 2015) because in preliminary experiments Char-niak parser (Charniak, 2000) performed better whentrained on all of 24 million trees than when trainedon resampled two million trees.Given x, we produce Y ?
(x), 50-best trees, withCharniak parser and find y with LSTM-LM as Dyeret al (2016) do with their discriminative and gener-ative models.34.2 Training and Development4.2.1 SupervisionWe unk words that appear fewer than 10 timesin the WSJ training (6,922 types) and drop activa-tions with probability 0.7.
At the beginning of eachepoch, we shuffle the order of trees in the trainingdata.
Both perplexity and F1 of LSTM-LM (G) im-prove and then plateau (Figure 2).
Perplexity, the3Dyer et al (2016)?s discriminative model performs compa-rably to Charniak (89.8 vs. 89.7).2333Base FinalVinyals et al (2015) 88.3 90.5Dyer et al (2016) 89.8 92.4LSTM-LM (G) 89.7 92.6Table 2: F1 of models trained on WSJ.
Base refersto performance of a single base parser and Final thatof a final parser.model?s training objective, nicely correlates with F1,what we care about.
Training takes 12 hours (37epochs) on a Titan X.
We also evaluate our modelwith varying n-best trees including optimal 51-besttrees that contain gold trees (51o).
As shown in Ta-ble 1, the LSTM-LM (G) is robust given sufficientlylarge n, i.e.
50, but does not exhibit its full capac-ity because of search errors in Charniak parser.
Weaddress this problem in Section 5.3.4.2.2 Semi-supervisionWe unk words that appear at most once in thetraining (21,755 types).
We drop activations withprobability 0.45, smaller than 0.7, thanks to manysilver trees, which help regularization.
We trainLSTM-LM (GS) on the WSJ and a different set of400,000 NYT trees for each epoch except for thelast one during which we use the WSJ only.
Trainingtakes 26 epochs and 68 hours on a Titan X. LSTM-LM (GS) achieves 92.5 F1 on the development.5 Results5.1 SupervisionAs shown in Table 2, with 92.6 F1 LSTM-LM (G)outperforms an ensemble of five MTPs (Vinyals etal., 2015) and RNNG (Dyer et al, 2016), both ofwhich are trained on the WSJ only.5.2 Semi-supervisionWe compare LSTM-LM (GS) to two very strongsemi-supervised NN parsers: an ensemble of fiveMTPs trained on 11 million trees of the high-confidence corpus4 (HC) (Vinyals et al, 2015); andan ensemble of six one-to-many sequence models4The HC consists of 90,000 gold trees, from the WSJ, En-glish Web Treebank and Question Treebank, and 11 million sil-ver trees, whose sentence length distribution matches that of theWSJ, parsed and agreed on by Berkeley parser and ZPar.trained on the HC and 4.5 millions of English-German translation sentence pairs (Luong et al,2016).
We also compare LSTM-LM (GS) tobest performing non-NN parsers in the literature.Parsers?
parsing performance along with their train-ing data is reported in Table 3.
LSTM-LM (GS) out-performs all the other parsers with 93.1 F1.5.3 Improved Semi-supervisionDue to search errors ?
good trees are missing in50-best trees ?
in Charniak (G), our supervised andsemi-supervised models do not exhibit their full po-tentials when Charniak (G) provides Y ?(x).
To mit-igate the search problem, we tri-train Charniak (GS)on all of 24 million NYT trees in addition to theWSJ, to yield Y ?(x).
As shown in Table 3, bothLSTM-LM (G) and LSTM-LM (GS) are affectedby the quality of Y ?(x).
A single LSTM-LM (GS)together with Charniak (GS) reaches 93.6 and anensemble of eight LSTM-LMs (GS) with Charniak(GS) achieves a new state of the art, 93.8 F1.
Whentrees are converted to Stanford dependencies,5 UASand LAS are 95.9% and 94.1%,6 more than 1%higher than those of the state of the art dependencyparser (Andor et al, 2016).
Why an indirect method(converting trees to dependencies) is more accu-rate than a direct one (dependency parsing) remainsunanswered (Kong and Smith, 2014).6 ConclusionThe generative parsing model we presented in thispaper is very powerful.
In fact, we see that a gen-erative parsing model, LSTM-LM, is more effec-tive than discriminative parsing models (Dyer et al,2016).
We suspect building large models with char-acter embeddings would lead to further improve-ment as in language modeling (Kim et al, 2016;Jozefowicz et al, 2016).
We also wish to de-velop a complete parsing model using the LSTM-LM framework.AcknowledgmentsWe thank the NVIDIA corporation for its dona-tion of a Titan X GPU, Tstaff of Computer Science5Version 3.3.0.6We use the CoNLL evaluator available through the CoNLLwebsite: ilk.uvt.nl/conll/software/eval.pl.
Following the con-vention, we ignore punctuation.2334Base Oracle Final Gold SilverHuang et al (2010) - - 92.8 WSJ (40K) BLLIP (1.8M)Shindo et al (2012) - - 92.4 WSJ (40K) -Choe et al (2015) - - 92.6 WSJ (40K) NYT (2M)Vinyals et al (2015) - - 92.8 HC (90K) HC (11M)Luong et al (2016) - - 93.0 HC (90K) HC (11M)Charniak (G) + LSTM-LM (G) 89.7 96.7 92.6 WSJ (40K) -Charniak (G) + LSTM-LM (GS) 89.7 96.7 93.1 WSJ (40K) NYT (0/10M)Charniak (GS) + LSTM-LM (G) 91.2 97.1 92.9 WSJ (40K) NYT (24M/0)Charniak (GS) + LSTM-LM (GS) 91.2 97.1 93.6 WSJ (40K) NYT (24M/10M)Charniak (GS) + E(LSTM-LMs (GS)) 91.2 97.1 93.8 WSJ (40K) NYT (24M/11.2M)Table 3: Evaluation of models trained on the WSJ and additional resources.
Note that the numbers of Vinyalset al (2015) and Luong et al (2016) are not directly comparable as their models are evaluated on OntoNotes-style trees instead of PTB-style trees.
E(LSTM-LMs (GS)) is an ensemble of eight LSTM-LMs (GS).
X/Yin Silver column indicates the number of silver trees used to train Charniak parser and LSTM-LM.
For theensemble model, we report the maximum number of trees used to train one of LSTM-LMs (GS).at Brown University for setting up GPU machinesand David McClosky for helping us train Charniakparser on millions trees.ReferencesDaniel Andor, Chris Alberti, David Weiss, AliakseiSeveryn, Alessandro Presta, Kuzman Ganchev, SlavPetrov, and Michael Collins.
2016.
Globally normal-ized transition-based neural networks.
Proceedings ofthe 54th Annual Meeting of the Association for Com-putational Linguistics.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In 1st Meeting of the North American Chapterof the Association for Computational Linguistics.Do Kook Choe, David McClosky, and Eugene Charniak.2015.
Syntactic parse fusion.
In Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing.Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, andNoah A Smith.
2016.
Recurrent neural network gram-mars.
In Proceedings of the 2016 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies.Felix A Gers, Nicol N Schraudolph, and Ju?rgen Schmid-huber.
2003.
Learning precise timing with lstm re-current networks.
The Journal of Machine LearningResearch.Alex Graves.
2013.
Generating sequences with recurrentneural networks.
arXiv preprint arXiv:1308.0850.Michael U. Gutmann and Aapo Hyva?rinen.
2012.Noise-contrastive estimation of unnormalized statisti-cal models, with applications to natural image statis-tics.
The Journal of Machine Learning Research.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation.Zhongqiang Huang, Mary Harper, and Slav Petrov.
2010.Self-training with products of latent variable gram-mars.
In Proceedings of the 2010 Conference on Em-pirical Methods in Natural Language Processing.Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever.2015.
An empirical exploration of recurrent networkarchitectures.
In Proceedings of the 32nd Interna-tional Conference on Machine Learning.Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, NoamShazeer, and Yonghui Wu.
2016.
Exploringthe limits of language modeling.
arXiv preprintarXiv:1602.02410.Yoon Kim, Yacine Jernite, David Sontag, and Alexan-der M. Rush.
2016.
Character-aware neural languagemodels.
In Proceedings of the Thirtieth AAAI Confer-ence on Artificial Intelligence.Lingpeng Kong and Noah A Smith.
2014.
An empiricalcomparison of parsing methods for stanford dependen-cies.
arXiv preprint arXiv:1404.4314.Zhenghua Li, Min Zhang, and Wenliang Chen.2014.
Ambiguity-aware ensemble training for semi-supervised dependency parsing.
In Proceedings of the52nd Annual Meeting of the Association for Computa-tional Linguistics.Minh-Thang Luong, Quoc V Le, Ilya Sutskever, OriolVinyals, and Lukasz Kaiser.
2016.
Multi-task se-2335quence to sequence learning.
International Confer-ence on Learning Representations.Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of english: The penn treebank.
Computational lin-guistics.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of the Human Language Technology Conferenceof the NAACL.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InProceedings of the Tenth International Workshop onArtificial Intelligence and Statistics.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2011.
English gigaword fifth edition.Linguistic Data Consortium, LDC2011T07.Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.2013.
On the difficulty of training recurrent neural net-works.
In Proceedings of the 30th International Con-ference on Machine Learning.Slav Petrov.
2010.
Products of random latent variablegrammars.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics.Association for Computational Linguistics.Vu Pham, The?odore Bluche, Christopher Kermorvant,and Je?ro?me Louradour.
2014.
Dropout improves re-current neural networks for handwriting recognition.In 2014 14th International Conference on Frontiers inHandwriting Recognition.Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, andMasaaki Nagata.
2012.
Bayesian symbol-refined treesubstitution grammars for syntactic parsing.
In Pro-ceedings of the 50th Annual Meeting of the Associationfor Computational Linguistics.Ilya Sutskever.
2013.
Training recurrent neural net-works.
Ph.D. thesis, University of Toronto.Oriol Vinyals, ?ukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2015.
Grammaras a foreign language.
In Advances in Neural Informa-tion Processing Systems 28.Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.2014.
Recurrent neural network regularization.
arXivpreprint arXiv:1409.2329.Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, andJingbo Zhu.
2013.
Fast and accurate shift-reduce con-stituent parsing.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Linguis-tics.2336
