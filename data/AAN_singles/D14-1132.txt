Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250?1260,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsLarge-scale Expected BLEU Training of Phrase-based Reordering ModelsMichael Auli, Michel Galley, Jianfeng GaoMicrosoft ResearchRedmond, WA, USA{michael.auli,mgalley,jfgao}@microsoft.comAbstractRecent work by Cherry (2013) has shownthat directly optimizing phrase-based re-ordering models towards BLEU can leadto significant gains.
Their approach is lim-ited to small training sets of a few thou-sand sentences and a similar number ofsparse features.
We show how the ex-pected BLEU objective allows us to traina simple linear discriminative reorderingmodel with millions of sparse features onhundreds of thousands of sentences re-sulting in significant improvements.
Acomparison to likelihood training demon-strates that expected BLEU is vastly moreeffective.
Our best results improve a hi-erarchical lexicalized reordering baselineby up to 2.0 BLEU in a single-referencesetting on a French-English WMT 2012setup.1 IntroductionModeling reordering for phrase-based machinetranslation has been a long standing problem.Contrary to synchronous context free grammar-based translation models (Wu, 1997; Galley et al.,2004; Galley et al., 2006; Chiang, 2007), phrase-based models (Koehn et al., 2003; Och and Ney,2004) have no in-built notion of reordering beyondwhat is captured in a single phrase pair, and thefirst phrase-based decoders simply scored inter-phrase reorderings using a restricted linear dis-tortion feature, which scores a phrase reorderingproportionally to the length of its displacement.While phrase-based models allow in theory com-pletely unrestricted reordering patterns, move-ments are generally limited to a finite distance forcomplexity reasons.
To address this limitation,extensive prior work focused on richer featuresets, in particular on lexicalized reordering mod-els trained with maximum likelihood-based ap-proaches (Tillmann, 2003; Xiong et al., 2006; Gal-ley and Manning, 2008; Nguyen et al.,2009;?2).More recently, Cherry (2013) proposed a veryeffective sparse ordering model relying on a setof only a few thousand indicator features whichare trained towards a task-specific metric such asBLEU (Papineni et al., 2002).
These featuresare simply added to the log-linear framework oftranslation that is trained with the Margin InfusedRelaxed Algorithm (MIRA; Chiang et al., 2009)on a small development set of a few thousandsentences.
While simple, the approach outper-forms the state-of-the-art hierarchical reorderingmodel of Galley and Manning (2008), a maximumlikelihood-based model trained on millions of sen-tences to fit millions of parameters.Ideally, we would like to scale sparse reorder-ing models to similar dimensions but recent at-tempts to increase the amount of training data forMIRA was met with little success (Eidelman etal., 2013).
In this paper we propose much largersparse ordering models that combine the scalabil-ity of likelihood-based approaches with the higheraccuracy of maximum BLEU training (?3).
Wetrain on the output of a hierarchical reorderingmodel-based system and scale to millions of fea-tures learned on hundreds of thousands of sen-tences (?4).
Specifically, we use the expectedBLEU objective function (Rosti et al., 2010; Rostiet al., 2011; He and Deng, 2012; Gao and He,2013; Gao et al., 2014; Green et al., 2014) whichallows us to train models that use training data andfeature sets that are two to three orders of magni-tudes larger than in previous work (?5).Our models significantly outperform thestate-of-the-art hierarchical lexicalized reorderingmodel on two language pairs and we demonstratethat richer feature sets result in significantlyhigher accuracy than with a feature set similarto Cherry (2013).
We also demonstrate that our1250approach greatly benefits from more trainingdata than is typically used for maximum BLEUtraining.
Previous work concluded that sparsereordering models perform better than maximumentropy models, however, the two approachesdo not only differ in the objective function butalso the type of training data (Cherry, 2013).
Ouranalysis isolates the objective function and showsthat expected BLEU optimization is the mostimportant factor to train accurate ordering models.Finally, we compare expected BLEU training topair-wise ranked optimization (PRO) on a featureset similar to Cherry (2013; ?7).2 Reordering ModelsReordering models for phrase-based translationare typically part of the log-linear frameworkwhich forms the basis of many statistical machinetranslation systems (Och and Ney, 2004).Formally, we are given K training pairs D =(f(1), e(1))...(f(K), e(K)), where each f(i)?
Fis drawn from a set of possible foreign sentences,and each English sentence e(i)?
E(f(i)) is drawnfrom a set of possible English translations of f(i).The log-linear model is parameterized by m pa-rameters ?
where each ?k?
?
is the weight ofan associated feature hk(f, e) such as a languagemodel or a reordering model.
Function h(f, e)maps foreign and English sentences to the vectorh1(f, e)...hm(f, e), and we usually choose trans-lations e?
according to the following decision rule:e?
= arg maxe?E(f)?Th(f, e) (1)In practice, computing e?
exactly is intractable andwe resort to an approximate but more efficientbeam search (Och and Ney, 2004).Early phrase-based models simply relied on alinear distortion feature, which measures the dis-tance between the first word of the current sourcephrase and the last word of the previous sourcephrase (Koehn et al., 2003; Och and Ney, 2004).Unfortunately, this approach is agnostic to the ac-tual phrases being reordered, and does not takeinto account that certain phrases are more likelyto be reordered than others.
This shortcoming ledto a range of lexicalized reordering models thatcapture exactly those preferences for individualphrases (Tillmann, 2003; Koehn et al., 2007).Reordering models generally assume a se-quence of English phrases e = {e?1, .
.
.
, e?n} cur-rently hypothesized by the decoder, a phrase align-ment a = {a1, .
.
.
, an} that defines a foreignphrase?faifor each English phrase e?i, and an ori-entation oiwhich describes how a phrase pairshould be reordered with respect to the previousphrases.
There are typically three orientation typesand the exact definition depends on the specificmodels which we describe below.
Orientations canbe determined during decoding and from word-aligned training corpora.
Most models estimatea probability distribution p(oi|ppi, a1, .
.
.
, ai) forthe i-th phrase pair ppi= ?e?i,?fai?
and the align-ments a1, .
.
.
, aiof the previous target phrases.Lexicalized Reordering.
This model defines thethree orientation types based only on the posi-tion of the current and previously translated sourcephrase aiand ai?1, respectively (Tillmann, 2003;Koehn et al., 2007).
The orientation types gen-erally are: monotone (M), indicating that ai?1isdirectly followed by ai.
swap (S) assumes that aiprecedes ai?1, i.e., the two phrases swap places.Finally, discontinuous (D) indicates that aiis notadjacent to ai?1.
The probability distribution overthese reordering events is based on a maximumlikelihood estimate:p(o|pp, ai?1, ai) =cnt(o, pp)cnt(pp)(2)where o ?
{M,S,D} and cnt returns smoothedfrequency counts over a word-aligned corpus.Hierarchical Reordering.
An extension of thelexicalized reordering model better handles long-distance reordering by conditioning the orientationof the current phrase on a context larger than justthe previous phrase (Galley and Manning, 2008).In particular, the hierarchical reordering modeldoes so by building a compact representationsof the preceding context using an efficient shift-reduce parser.
During translation new phrases getmoved on a stack and are then combined with anyprevious phrase if they are adjacent.
Figure 1shows an illustrative example: when the decodershifts phrase pp8onto the stack, this phrase is thenmerged with pp7(reduce operation), which thencan be merged with previous phrases to finallyform a hierarchical block h1.
These merge opera-tions stop once we reach a phrase (here, pp3) thatis not contiguous with the current block.
Then, asanother phrase (pp9) is hypothesized, the decoderuses the hierarchical block at the top of the stack(h1) to determine the orientation of the current1251         therussiansidehopestoholdconsultationswithiranonthisissueinthenearfuture................................................................................................................................................................................................h1pp9pp8pp7pp3decodedoutputinput sentenceFigure 1: The hierarchical reordering model(HRM) analyzes a non-local context to determinethe orientation of the current phrase.
For exam-ple, the phrase pair pp9has a swap orientation(o9= S) with respect to a hierarchical block (h1)that comprises the five preceding phrase pairs.phrase pp9, which in this case is a swap (S) orien-tation.1The model has the advantage that the ori-entations computed are more robust to derivationalambiguity of the underlying translation model.
Agiven surface translation may be derived throughdifferent phrases but the shift-reduce parser com-bines them into a single representation which ismore consistent with the orientations observed inthe word-aligned training data.Maximum Entropy-based models.
The statis-tics used to estimate the lexicalized and the hierar-chical reordering models are based on very sparseestimates, simply because certain phrases are notvery frequent.
Maximum entropy models addressthis problem by estimating Eq.
2 through sparseindicator features over phrase pairs instead, butprior work with such models still relies on wordaligned corpora for estimation (Xiong et al., 2006;Nguyen et al., 2009).
However, recent evalua-tions of the approach show little gain over the sim-pler frequency-based estimation method (Cherry,2013).Sparse Hierarchical Reordering model.
All ofthe models so far are trained to maximize the like-lihood of reordering decisions observed in wordaligned corpora.
Cherry (2013) argues that itis probably too difficult to learn human reorder-ing patterns through noisy word alignments that1Galley and Manning (2008) provide a more formal ex-planation.were generated by unsupervised methods.
Instead,he proposes to learn a discriminative reorderingmodel based on the outputs of the actual machinetranslation system, adjusting the feature weightsto maximize a task-specific objective, which isBLEU in their case.
Their model is based on aset of sparse features derived from the hierarchi-cal reordering model which we scale to millionsof features (?6).3 A Simple Linear Reordering ModelOur reordering model is defined as a simple linearmodel over the basic orientation types, similar toCherry (2013).
In particular, our model definesscore s?
(o, e, f) over orientations o = {M,S,D},and a sentence pair {e, f, a} with alignment a as alinear combination of weighted indicator features:s?
(o, e, f, a) = ?Tu(o, e, f, a)=I?i=1?Tu(o, ppi, ci)=I?i=1s?
(o, ppi, ci) (3)where ?
is a vector of weights, {ppi}Ii=1is aset of phrases that decompose the sentence pair{e, f, a}, and u(o, ppi, ci) is a function that mapsorientation o, phrase pair ppiand local context cito a sparse vector of indicator features.
The lo-cal context cirepresents information used by themodel that is in addition to the phrase pair.
Forexample, the features of Cherry (2013) conditionon the top-stack of the hierarchical shift reduceparser, information that is non-local with respectto the phrase pair.
In our experiments, we use fea-tures that go beyond the top-stack, in order to con-dition on various parts of the source and target sidecontexts (?7).4 Model TrainingOptimization of our model is based on standardstochastic gradient descent (SGD; Bottou, 2004)with an expected BLEU loss l(?)
which we detailnext (?5).
The update is:?t= ?t?1?
??l(?t?1)?
?t?1(4)where ?tand ?t?1are model weights at time t andt?
1 respectively, and ?
is a learning rate.We add the model as a small number of densefeatures to the log-linear framework of translation1252(Eq.
1).
Specifically, we extend the m baselinefeatures by a set of new features hm+1, .
.
.
, hm+j,where each represents a linear combination ofsparse indicator features corresponding to one ofthe orientation types.
Exposing each orientationas a separate dense feature within the log-linearmodel is common practice for lexicalized reorder-ing models (Koehn et al., 2005):hm+j= s?
(oj, e, f, a)where oj?
{M,S,D}.The translation model is then parameterized byboth ?, the log-linear weights of the baseline fea-tures, as well as ?, the weights of the reorderingmodel.
The reordering model is learned as follows(Gao and He, 2013; Gao et al., 2014):1.
We first train a baseline translation system tolearn ?, without the discriminative reorderingmodel, i.e., we set ?m+1= 0, .
.
.
, ?m+j= 0.2.
Using these weights, we generate n-best listsfor the foreign sentences in the training datausing the setup described in the experimentalsection (?7).
The n-best lists serve as an ap-proximation to E(f), the set of possible trans-lations of f , used in the next step for expectedBLEU training of the reordering model (?5).3.
Next, we fix ?, set ?m+1= 1, .
.
.
?m+j= 1and optimize ?
with respect to the loss func-tion on the training data using stochastic gra-dient descent.24.
Finally, we fix ?
and re-optimize ?
in thepresence of the discriminative reorderingmodel using Minimum Error Rate Training(MERT; Och 2003; ?7).We found that re-optimizing ?
after a few iter-ations of stochastic gradient descent in step 3 didnot improve accuracy.5 Expected BLEU Objective FunctionThe expected BLEU objective (Gao and He, 2013;Gao et al., 2014) allows us to efficiently optimizea large scale discriminative reordering model to-wards the desired task-specific metric, which inour setting is BLEU.2We tuned ?m+1, .
.
.
?m+jon the development set butfound that setting them uniformly to one resulted in fastertraining and equal accuracy.Formally, we define our loss function l(?)
asthe negative expected BLEU score, denoted asxBLEU(?
), for a given foreign sentence f and alog-linear parameter set ?:l(?)
=?
xBLEU(?)=??e?E(f)p?,?
(e|f) sBLEU(e, e(i)) (5)where sBLEU(e, e(i)) is a smoothed sentence-level BLEU score with respect to the referencetranslation e(i), and E(f) is the generation set ap-proximated by an n-best list.
In our experimentswe use n-best lists with unique entries and there-fore our definitions do not take into account mul-tiple derivations of the same translation.
Specif-ically, our n-best lists are generated by choosingthe highest scoring derivation e?
amongst stringidentical translations e for f .
We use a sentence-level BLEU approximation similar to Gao et al.
(2014).3Finally, p?,?
(e|f) is the normalized prob-ability of translation e given f , defined as:p?,?
(e|f) =exp{?
?Th(f, e)}?e??E(f)exp{?
?Th(f, e?
)}(6)where ?Th(f, e) includes the discriminative re-ordering model hm+1(e, f), .
.
.
, hm+j(e, f) pa-rameterized by ?, and ?
?
[0, inf) is a tuned scal-ing factor that flattens the distribution for ?
< 1and sharpens it for ?
> 1 (Tromble et al., 2008).4Next, we define the gradient of the expectedBLEU loss function l(?).
To simplify our notationwe omit the local context c in s?
(o, pp, c) (Eq.
3)from now on and assume it to be part of pp.
Us-ing the observation that the loss does not explicitlydepend on ?, we get:?l(?)??=?o,pp?l(?)?s?
(o, pp)?s?
(o, pp)??=?o,pp?
?o,ppu(o, pp)where ?o,ppis the error term for orientation o ofphrase pair pp:?o,pp= ??l(?)?s?
(o, pp)3We found in early experiments that the BLEU+1 approx-imation used by Liang et al.
(2006) and Nakov et.
al (2012)worked equally well in our setting.4?
is only used during expected BLEU training.1253The error term indicates how the expected BLEUloss changes with the reordering score which wederive in the next section.Finally, the gradient of the reordering scores?
(o, pp) with respect to ?
is simply given by this:?s?
(o, pp)??=?
?Tu(o, pp)?
?= u(o, pp)5.1 Derivation of the Error Term ?o,ppWe rewrite the loss function (Eq.
5) using Eq.
6and separate it into two terms G(?)
and Z(?):l(?)
= ?xBLEU(?)
= ?G(?)Z(?
)(7)= ??e?E(f)exp{?
?Th(f, e)} sBLEU(e, e(i))?e??E(f)exp{?
?Th(f, e?
)}Next, we apply the quotient rule of differentiation:?o,pp=?xBLEU(?)?s?
(o, pp)=?(G(?)/Z(?))?s?
(o, pp)=1Z(?)(?G(?)?s?
(o, pp)??Z(?)?s?
(o, pp)xBLEU(?
))The gradients for G(?)
and Z(?)
with respect tos?
(o, pp) are:?G(?)?s?
(o, pp)=?e?E(f)sBLEU(e, e(i))?
exp{?
?Th(f, e)}?s?
(o, pp)?Z(?)?s?
(o, pp)=?e?E(f)?
exp{?
?Th(f, e)}?s?
(o, pp)By using the following definition:U(?, e) = sBLEU(e, e(i))?
xBLEU(?
)together with the chain rule, Eq.
6 and Eq.
7, wecan rewrite ?o,ppas follows:?o,pp=1Z(?)?e?E(f)(?
exp{?
?Th(f, e)}?s?
(o, pp)U(?, e))=?e?E(f)(p?,?(e|f)??
?Th(f, e)?s?
(o, pp)U(?, e))Because ?
is only relevant to the reorderingmodel, represented by hm+1, .
.
.
, hm+j, we have:??
?Th(f, e)?s?
(o, pp)= ?
?k?hk(e, f)?s?
(o, pp)= ?
?kN (o, pp, e, f)1: function TRAINSGD(D, ?
)2: t?
03: for all (f(i), e(i)) in D do4: xBLEU = 0 .
Compute xBLEU5: for all e in E(f(i)) do6: wBLEU?
p?,?t(e|f) sBLEU(e, e(i))7: xBLEU?
xBLEU + wBLEU8: end for9: for all e in E(f(i)) do10: D = sBLEU(e, e(i))?
xBLEU11: for all o, pp in ?e, f(i)?
do12: N = N (o, pp, e, f)13: ?o,pp= p?,?t(e|f(i))?
?kND14: ?t+1= ?t?
?
?o,ppu(o, pp))15: end for16: end for17: t?
t+ 118: end for19: end functionFigure 2: Algorithm for computing the expectedBLEU loss with SGD updates (Eq.
4) based ontraining data D and learning rate ?.where m + 1 ?
k ?
m + j and N (o, pp, e, f) isthe number of times pp with orientation o occursin the current sentence pair.This simplifies the error term to:?o,pp=?e?E(f)p?,?(e|f)?
?kN (o, pp, e, f)U(?, e)(8)where ?kis the weight of the dense feature sum-marizing orientation o in the log-linear model.
Weuse Eq.
8 in a simple algorithm to train our model(Figure 2).
Our SGD trainer uses a mini-batch sizeof a single sentence (?7) which entails all hypoth-esis in the n-best list for this sentence and the pa-rameters are updated after each mini-batch.6 Feature SetsOur features are inspired by Cherry (2013)who bases his features on the local phrase-pairpp = ?e?,?f?
as well as the top stack of the shift re-duce parser of the baseline hierarchical orderingmodel.
We experiment with these variants and ex-tensions:?
SparseHRMLocal: This feature set is exclu-sively based on the local phrase-pair and1254consists of features over the first and lastword of both the source and target phrase.5We use four different word representations:The word identity itself, but only for the80 most common source and target languagewords.
The three other word representationsare based on Brown clustering with either 20,50 or 80 classes (Brown et al., 1992).
Thereis one feature for every orientation type.?
SparseHRM: The main feature set of Cherry(2013).
This is an extension of SparseHRM-Local adding features based on the first andlast word of both the source and the target ofthe hierarchical block at the top of the stack.There are also features based on the sourcewords in-between the current phrase and thehierarchical block at the top of the stack.?
SparseHRM+UncommonWords: This set isidentical to SparseHRM, except that word-identity features are not restricted to the 80most frequent words, but can be instantiatedfor all words, regardless of frequency.?
SparseHRM+BiPhrases: This augmentsSparseHRM by phrase-identity features re-sulting in millions of instances compared toonly a few thousand for SparseHRM.
We addthree features for each possible phrase pair:the source phrase, the target phrase, and thewhole phrase pair.The baseline hierarchical lexicalized reorder-ing model is most similar to SparseHRM+BiPhrasesfeature set since both have parameters for phrase,orientation pairs.6The feature set closest toCherry (2013) is SparseHRM.
However, whileCherry had to severely restrict his features forbatch lattice MIRA-based training, our maximumexpected BLEU approach can handle millions offeatures.7 ExperimentsBaseline.
We experiment with a phrase-basedsystem similar to Moses (Koehn et al., 2007),5Phrase-local features allow pre-computation which re-sults in significant speed-ups at run-time.
Cherry (2013)shows that local features are responsible for most of his gains.6Although, our model is likely to learn significantly fewerparameters since many phrase, orientation pairs will only beseen in the word-aligned data but not in actual machine trans-lation output.scoring translations by a set of common fea-tures including maximum likelihood estimatesof source given target phrases pMLE(e|f) andvice versa, pMLE(f |e), lexically weighted esti-mates pLW(e|f) and pLW(f |e), word and phrase-penalties, as well as a linear distortion feature.The baseline uses a hierarchical reordering modelwith five orientation types, including monotoneand swap, described in ?2, as well as two discon-tinuous orientations, distinguishing if the previousphrase is to the left or right of the current phrase.Finally, monotone global indicates that all previ-ous phrases can be combined into a single hier-archical block.
The baseline includes a modifiedKneser-Ney word-based language model trainedon the target-side of the parallel data, which is de-scribed below.
Log-linear weights are estimatedwith MERT (Och, 2003).
We regard the 1-bestoutput of the phrase-based decoder with the hierar-chical reordering model as the baseline accuracy.Evaluation.
We use training and test data fromthe WMT 2012 campaign and report results onFrench-English and German-English translation(Callison-Burch et al., 2012).
Translation mod-els are estimated on 102M words of parallel datafor French-English and 91M words for German-English; between 7.5-8.2M words are newswire,depending on the language pair, and the remainderare parliamentary proceedings.
All discrimina-tive reordering models are trained on the newswiresubset since we found this portion of the data to bemost useful in initial experiments.
We evaluate onsix newswire domain test sets from 2008, 2010 to2013 as well as the 2010 system combination testset containing between 2034 to 3003 sentences.Log-linear weights are estimated on the 2009 dataset comprising 2525 sentences.
We evaluate usingBLEU with a single reference.Discriminative Reordering Model.
We use 100-best lists generated by the phrase-based decoderto train the discriminative reordering model.
Then-best lists are generated by ten systems, eachtrained on 90% of the available data in order to de-code the remaining 10%.
The purpose of this pro-cedure is to avoid a bias introduced by generatingn-best lists for sentences on which the translationmodel was previously trained.7Unless otherwise7Later, we found that the bias has only a negligible effecton end-to-end accuracy since we obtained very similar resultswhen decoding with a system trained on all data.
This settingincreased the training data BLEU score from 27.5 to 37.8.
Weused a maximum source and target phrase length of 7 words.1255dev 2008 2010 sc2010 2011 2012 2013 AllTest FeatTypesnoRM 23.37 20.18 24.24 24.18 24.83 24.23 24.85 23.93 -HRM (baseline) 24.11 20.85 24.92 24.83 25.68 25.11 25.76 24.72 -SparseHRMLocal 25.24 21.26 25.99 25.93 26.98 26.34 26.77 25.77 4,407SparseHRM 25.29 21.43 26.17 26.14 26.99 26.63 27.01 25.95 9,463+UncommonWords 25.32 21.76 26.30 26.29 27.15 26.77 27.18 26.12 897,537+BiPhrases 25.46 21.67 26.19 26.19 27.55 27.07 27.41 26.26 3,043,053Table 1: French-English results of expected BLEU trained sparse reordering models compared to noreordering model at all (noRM) and the likelihood trained baseline hierarchical reordering model (HRM)on WMT test sets; sc2010 is the 2010 system combination test set.
FeatTypes is the number of differenttypes and AllTest is the average BLEU score over all the test sets, weighted by corpus size.
All resultsfor our sparse reordering models include a likelihood-trained hierarchical reordering model.dev 2008 2010 sc2010 2011 2012 2013 AllTest FeatTypesnoRM 18.54 19.28 20.14 20.01 18.90 18.87 21.60 19.81 -HRM (baseline) 19.35 19.96 20.87 20.66 19.60 19.80 22.48 20.58 -SparseHRMLocal 19.89 19.86 21.11 20.84 20.04 20.21 22.93 20.88 4,410SparseHRM 19.83 20.27 21.26 21.05 20.22 20.44 23.17 21.11 9,477+UncommonWords 20.06 20.35 21.45 21.31 20.28 20.55 23.30 21.24 1,136,248+BiPhrases 20.09 20.33 21.62 21.47 20.66 20.75 23.27 21.40 3,640,693Table 2: German-English results of expected BLEU trained sparse reordering models (cf.
Table 1).mentioned, we train our reordering model on thenews portion of the parallel data, corresponding to136K-150K sentences, depending on the languagepair.
We tuned the various hyper-parameters on aheld-out set, including the learning rate, for whichwe found a simple setting of 0.1 to be useful.
Toprevent overfitting, we experimented with `2regu-larization, but found that it did not improve test ac-curacy.
We also tuned the probability scaling pa-rameter ?
(Eq.
6) but found ?
= 1 to be very goodamong other settings.
We evaluate the perfor-mance on a held-out validation set during trainingand stop whenever the objective changes less thana factor of 0.0003.
For our PRO experiments, wetuned three hyper-parameters controlling `2reg-ularization, sentence-level BLEU smoothing, andlength.
The latter is important to eliminate PRO?stendency to produce too short translations (Nakovet al., 2012).7.1 Scaling the Feature SetWe first compare our baseline, a likelihood trainedhierarchical reordering model (HRM; Galley &Manning, 2008), to various expected BLEUtrained models, starting with SparseHRMLocal,inspired by Cherry (2013) and compare it toSparseHRM+BiPhrases, a set that is three orders ofmagnitudes larger.Our results on French-English translation (Ta-ble 1) and German-English translation (Table 2)show that the expected BLEU trained models scaleto millions of features and that we outperform thebaseline by up to 2.0 BLEU on newstest2012 forFrench-English and by up to 1.1 BLEU on new-stest2011 for German-English.8Increasing thesize of the feature set improves accuracy acrossthe board: The average accuracy over all test setsimproves from 1.0 BLEU for the most basic fea-ture set to 1.5 BLEU for the largest feature seton French-English and from 0.3 BLEU to 0.8BLEU on German-English.9The most compa-rable setting to Cherry (2013) is the feature setSparseHRM, which we outperform by up to 0.5BLEU on French-English and by 0.3 BLEU on av-erage on both language pairs, demonstrating thebenefit of being able to effectively train large fea-ture sets.
Furthermore, the increase in the num-ber of features does not affect runtime, since most8Different to the setups of Galley & Manning (2008) andCherry (2013) our WMT evaluation framework uses only oneinstead of four references, which makes our BLEU score im-provements not directly comparable.9We attribute smaller improvements on German-Englishto the low distortion limit of only six words of our system andthe more difficult reordering patterns when translating fromGerman which may require more elaborate features.1256features can be pre-computed and stored in thephrase-table, only requiring a constant time table-lookup, similar to traditional reordering models.Another appeal of our approach is that train-ing is very fast given a set of n-best lists for thetraining data.
The SparseHRM model with 4,407features is trained in only 26 minutes, while theSparseHRM+BiPhrases model with over three mil-lion parameters can be trained in just over twohours (136K sentences and 100 epochs in bothcases).
We attribute this to the training regime(?4), which does not iteratively re-decode thetraining data for expected BLEU training.107.2 Varying Training Set SizePrevious work on sparse reordering models wasrestricted to small data sets (Cherry, 2013) dueto the limited ability of standard machine trans-lation optimizers to handle more than a few thou-sand sentences.
In particular, recent attempts toscale the margin-infused relaxation algorithm, avariation which was also used by Cherry (2013),to larger data sets showed that more data does notnecessarily help to improve test set accuracy forlarge feature sets (Eidelman et al., 2013).In the next set of experiments, we shed light onthe advantage of training discriminative reorderingmodels with expected BLEU on large training sets.Specifically, we start off by estimating a reorder-ing model on only 2,000 sentences, similar to thesize of the development set used by Cherry (2013),and incrementally increase the amount of trainingdata to nearly three hundred thousand sentences.To avoid overfitting to small data sets we experi-ment with our most basic feature set SparseHRM-Local, comprising of just over 4,400 types.For this experiment only, we measure accuracyin a re-ranking framework for faster experimen-tation where we use the 100-best output of thebaseline system relying on a likelihood-based hi-erarchical reordering model.
We re-estimate thelog-linear weights by running a further iteration ofMERT on the n-best list of the development setwhich is augmented by scores corresponding tothe discriminative reordering model.
The weightsof those features are initially set to one and weuse 20 random restarts for MERT.
At test time werescore the 100-best list of the test set using thenew set of log-linear weights learned previously.10We would expect better accuracy when iteratively decod-ing the training data but did not do so in this study for effi-ciency reasons.24.424.624.825.025.22K 4K 8K 16K 32K 64K 136K 272KBLEUTraining set sizedev26.6 Training set size25.625.826.026.226.426.62K 4K 8K 16K 32K 64K 136K 272KBLEUTraining set sizenews2011Figure 3: Effect of increasing the training set sizefrom 2,000 to 272,000 sentences measured on thedev set (top) and news2011 (bottom) in an n-bestlist rescoring setting.Figure 3 confirms that more training data in-creases accuracy and that the best model requiresa substantially larger amount of training data thanwhat is typically used for maximum BLEU train-ing.
We expect an even steeper curve for largerfeature sets where more parameters need to be es-timated and where the amount of training data islikely to have an even larger effect.7.3 Likelihood versus BLEU OptimizationPrevious research has shown that directly traininga reordering model for BLEU can vastly outper-form a likelihood trained maximum entropy re-ordering model (Cherry, 2013).
However, the twoapproaches do not only differ in the objectivesused, but also in the type of training data.
Themaximum entropy reordering model is trained ona word-aligned corpus, trying to learn human re-ordering patterns, whereas the sparse reorderingmodel is trained on machine translation output,trying to learn from the mistakes made by the ac-tual system.
It is therefore not clear how mucheither one contributes to good accuracy.Our next experiment teases those two aspectsapart and clearly shows the effect of the objec-tive function.
Specifically, we compare the tra-ditionally used conditional log-likelihood (CLL)objective to expected BLEU on the French-English translation task in a small feature con-dition (SparseHRM) of about 9K features and1257dev 2008 2010 sc2010 2011 2012 2013 AllTestnoRM 23.37 20.18 24.24 24.18 24.83 24.23 24.85 23.93HRM (baseline) 24.11 20.85 24.92 24.83 25.68 25.11 25.76 24.72SparseHRM (CLL) 24.28 21.02 25.11 25.10 25.92 25.24 25.76 24.88SparseHRM (xBLEU) 25.29 21.43 26.17 26.14 26.99 26.63 27.01 25.95SparseHRM+BiPhrases (CLL) 24.42 21.17 25.12 25.00 25.86 25.36 26.18 24.98SparseHRM+BiPhrases (xBLEU) 25.46 21.67 26.19 26.19 27.55 27.07 27.41 26.26Table 3: French-English results comparing the baseline hierarchical reordering model (HRM) to sparsereordering model trained towards conditional log-likelihood (CLL) and expected BLEU (xBLEU).dev 2008 2010 sc2010 2011 2012 2013 AllTestPRO 24.05 20.90 25.42 25.28 25.79 25.09 26.07 24.94xBLEU 25.24 21.26 25.99 25.93 26.98 26.34 26.77 25.77Table 4: French-English results on the SparseHRMLocal feature set when when trained with pair-wiseranked optimization (PRO) and expected BLEU (xBLEU).a large feature setting of over 3M features(SparseHRM+BiPhrases).
In the CLL setting, wemaximize the likelihood of the hypothesis with thehighest BLEU score in the n-best list of each train-ing sentence.Our results (Table 3) show that CLL trainingachieves only a fraction of the gains yielded bythe expected BLEU objective.
For SparseHRM,CLL improves the baseline by less than 0.2 BLEUon average across all test sets, whereas expectedBLEU achieves 1.2 BLEU.
Increasing the numberof features to 3M (SparseHRM+BiPhrases) resultsin a slightly better average gain of 0.3 BLEU forCLL but but expected BLEU still achieves a muchhigher improvement of 1.5 BLEU.
Because ourgains with likelihood training are similar to whatCherry (2013) reported for his maximum entropymodel, we conclude that the objective function isthe most important factor to achieving good accu-racy.7.4 Comparison to PROIn our final experiment we compare expectedBLEU training to pair-wise ranked optimization(PRO), a popular off the shelf trainer for ma-chine translation models with large feature sets(Hopkins and May, 2011).11Previous work hasshown that PRO does not scale to truly large fea-ture sets with millions of types (Yu et al., 2013)and we therefore restrict ourselves to our smallest11MIRA is another popular optimizer but as previouslymentioned, even the best publicly available implementationdoes not scale to large training sets (Eidelman et al., 2013).set (SparseHRMLocal) of just over 4.4K features.We train PRO on the development set compris-ing of 2,525 sentences, a setup that is commonlyused by standard machine translation optimizers.In this setting, PRO directly learns weights for thebaseline features (?7) as well as the 4.4K indica-tor features corresponding to the sparse reorderingmodel.
For expected BLEU training we use thefull 136K sentences from the training data.
Theresults (Table 4) demonstrate that expected BLEUoutperforms a typical setup commonly used totrain large feature sets.8 Conclusion and Future WorkThe expected BLEU objective is a simple and ef-fective approach to train large-scale discriminativereordering models.
We have demonstrated thatit scales to millions of features, which is ordersof magnitudes larger than other modern machinetranslation optimizers can currently handle.Empirically, our sparse reordering model im-proves machine translation accuracy across theboard, outperforming a strong hierarchical lexi-calized reordering model by up to 2.0 BLEU ona French to English WMT2012 setup, where thebaseline was trained on over two million sentencepairs.
We have shown that scaling to large train-ing sets is crucial to good performance and thatthe best performance is reached when hundredsof thousands of training sentences are used.
Fur-thermore, we demonstrate that task-specific train-ing towards expected BLEU is much more effec-tive than optimizing conditional log-likelihood as1258is usually done.
We attribute this to the fact thatlikelihood is a strict zero-one loss that does not as-sign credit to partially correct solutions, whereasexpected BLEU does.In future work we plan to extend expectedBLEU training to lattices and to evaluate the ef-fect of estimating weights for the dense baselinefeatures as well.
Our current training procedure(Gao and He, 2013; Gao et al., 2014) decodesthe training data only once.
In future work, wewould like to compare this to repeated decodingas done by conventional optimization methods aswell as other large-scale discriminative trainingapproaches (Yu et al., 2013).
We expect this toyield additional accuracy gains.AcknowledgementsWe would like to thank Arul Menezes and Xi-aodong He for helpful discussion related to thiswork and the three anonymous reviewers for theircomments.ReferencesL?eon Bottou.
2004.
Stochastic learning.
InOlivier Bousquet and Ulrike von Luxburg, edi-tors, Advanced Lectures in Machine Learning, Lec-ture Notes in Artificial Intelligence, pages 146?168.Springer Verlag, Berlin.Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):467?479,Dec.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proc.
of WMT, pages 10?51.Association for Computational Linguistics, June.Colin Cherry.
2013.
Improved Reordering for Phrase-Based Translation using Sparse Features.
In Proc.
ofNAACL, pages 9?14.
Association for ComputationalLinguistics, June.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 New Features for Statistical Machine Trans-lation.
In Proc.
of NAACL, pages 218?226.
Associ-ation for Computational Linguistics, June.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Vladimir Eidelman, Ke Wu, Ferhan Ture1, PhilipResnik, and Jimmy Lin.
2013.
Mr. MIRA:Open-Source Large-Margin Structured Learning onMapReduce.
In Proc.
of ACL, pages 199?204.
As-sociation for Computational Linguistics, August.Michel Galley and Christopher D. Manning.
2008.
ASimple and Effective Hierarchical Phrase Reorder-ing Model.
In Proc.
of EMNLP, pages 848?856.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proc.
of HLT-NAACL, pages 273?280, Boston,MA, USA, May.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable Inference and Trainingof Context-Rich Syntactic Translation Models.
InProc.
of ACL, pages 961?968, Sydney, Australia,June.Jianfeng Gao and Xiaodong He.
2013.
Training MRF-Based Phrase Translation Models using GradientAscent.
In Proc.
of NAACL-HLT, pages 450?459.Association for Computational Linguistics, June.Jianfeng Gao, Xiaodong He, Scott Wen tau Yih, andLi Deng.
2014.
Learning Continuous Phrase Rep-resentations for Translation Modeling.
In Proc.of ACL.
Association for Computational Linguistics,June.Spence Green, Daniel Cer, and Christopher Manning.2014.
An Empirical Comparison of Features andTuning for Phrase-based Machine Translation.
InProc.
of WMT.
Association for Computational Lin-guistics, June.Xiaodong He and Li Deng.
2012.
Maximum ExpectedBLEU Training of Phrase and Lexicon TranslationModels.
In Proc.
of ACL, pages 8?14.
Associationfor Computational Linguistics, July.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proc.
of EMNLP.
Association for Com-putational Linguistics, July.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
InProc.
of HLT-NAACL, pages 127?133, Edmonton,Canada, May.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh System Descriptionfor the 2005 IWSLT Speech Translation Evaluation.In Proc.
of IWSLT.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proc.
of ACL Demo and Poster Sessions, pages177?180, Prague, Czech Republic, Jun.Percy Liang, Alexandre Bouchard-C?ot?e, Ben Taskar,and Dan Klein.
2006.
An end-to-end discriminativeapproach to machine translation.
In Proc.
of ACL-COLING, pages 761?768, Jul.1259Preslav Nakov, Francisco Guzman, and Stephan Vo-gel.
2012.
Optimizing for Sentence-Level BLEU+1Yields Short Translations.
In Proc.
of COLING.
As-sociation for Computational Linguistics.Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen,and Thai Phuong Nguyen.
2009.
Improving A Lex-icalized Hierarchical Reordering Model Using Max-imum Entropy.
In MT Summit XII.
Association forComputational Linguistics, August.Franz Josef Och and Hermann Ney.
2004.
Thealignment template approach to machine translation.Computational Linguistics, 30(4):417?449, June.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proc.
of ACL,pages 160?167, Sapporo, Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
of ACL,pages 311?318, Philadelphia, PA, USA, Jul.Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2010.
BBN System De-scription for WMT10 System Combination Task.In Proc.
of WMT, pages 321?326.
Association forComputational Linguistics, July.Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2011.
Expected BLEUTraining for Graphs: BBN System Description forWMT11 System Combination Task.
In Proc.
ofWMT, pages 159?165.
Association for Computa-tional Linguistics, July.Christoph Tillmann.
2003.
A Unigram OrientationModel for Statistical Machine Translation.
In Proc.of NAACL, pages 106?108.
Association for Compu-tational Linguistics, June.Roy W. Tromble, Shankar Kumar, Franz Och, andWolfgang Macherey.
2008.
Lattice MinimumBayes-Risk Decoding for Statistical Machine Trans-lation.
In Proc.
of EMNLP, pages 620?629.
Associ-ation for Computational Linguistics, October.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Max-imum entropy based phrase reordering model forstatistical machine translation.
In Proc.
of ACL-COLING, pages 521?528, Sydney, Jul.Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.2013.
Max-Violation Perceptron and Forced Decod-ing for Scalable MT Training.
In Proc.
of EMNLP,pages 1112?1123.
Association for ComputationalLinguistics, October.1260
