Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 17?26,Baltimore, Maryland USA, June 27, 2014.c?2014 Association for Computational LinguisticsComparison of different feature sets for identification of variants inprogressive aphasiaKathleen C. Fraser1, Graeme Hirst1, Naida L. Graham2, Jed A. Meltzer3,Sandra E. Black4, and Elizabeth Rochon21Dept.
of Computer Science, University of Toronto2Dept.
of Speech-Language Pathology, University of Toronto, & Toronto Rehabilitation Institute3Rotman Research Institute, Baycrest Centre, Toronto4LC Campbell Cognitive Neurology Research Unit, Sunnybrook Health Sciences Centre, Toronto{kfraser,gh}@cs.toronto.edu, {naida.graham,elizabeth.rochon}@utoronto.cajmeltzer@research.baycrest.org, sandra.black@sunnybrook.caAbstractWe use computational techniques to ex-tract a large number of different featuresfrom the narrative speech of individualswith primary progressive aphasia (PPA).We examine several different types of fea-tures, including part-of-speech, complex-ity, context-free grammar, fluency, psy-cholinguistic, vocabulary richness, andacoustic, and discuss the circumstancesunder which they can be extracted.
Weconsider the task of training a machinelearning classifier to determine whether aparticipant is a control, or has the fluent ornonfluent variant of PPA.
We first evaluatethe individual feature sets on their classifi-cation accuracy, then perform an ablationstudy to determine the optimal combina-tion of feature sets.
Finally, we rank thefeatures in four practical scenarios: givenaudio data only, given unsegmented tran-scripts only, given segmented transcriptsonly, and given both audio and segmentedtranscripts.
We find that psycholinguis-tic features are highly discriminative inmost cases, and that acoustic, context-freegrammar, and part-of-speech features canalso be important in some circumstances.1 IntroductionIn some types of dementia, such as primary pro-gressive aphasia, language deficit is a core symp-tom, and the analysis of narrative or conversa-tional speech is important for assessing the extentof an individual?s language impairment.
Analy-sis of connected speech has been limited in thepast because it is time-consuming and requires ex-pert annotation.
However, studies have shown thatit is possible for machine learning classifiers toachieve high accuracy on some diagnostic taskswhen trained on features which were automati-cally extracted from speech transcripts.In this paper, we summarize previous researchon the automatic analysis of speech samples fromindividuals with dementia, focusing in particularon primary progressive aphasia.
We discuss in de-tail different types of features and compare theireffectiveness in the classification task.
We sug-gest some benefits and drawbacks of these differ-ent features.
We also examine the interactions be-tween different feature sets, and discuss the rela-tive importance of individual features across fea-ture sets.
Because we examine a large numberof features on a relatively small data set, we em-phasize that this work is exploratory in nature;nonetheless, our results are consistent with, andextend, previous work in the field.2 BackgroundIn recent years, there has been growing interest inusing computer techniques to automatically detectdementia from speech and language features de-rived from a sample of narrative speech.
Some re-searchers have explored ways to use methods suchas part-of-speech tagging, statistical parsing, andspeech signal analysis to detect disorders such asdementia of the Alzheimer?s type (DAT) (Bucks etal., 2000; Singh et al., 2001; Thomas et al., 2005;Jarrold et al., 2010) and mild cognitive impairment(MCI) (Roark et al., 2011).Here, we focus on a type of dementia calledprimary progressive aphasia (PPA).
PPA is a sub-type of frontotemporal dementia (FTD) which ischaracterized by progressive language impairmentwithout other notable cognitive impairment.
Thereare three subtypes of PPA: semantic dementia(SD), progressive nonfluent aphasia (PNFA), andlogopenic progressive aphasia (LPA).
SD, some-times called ?fluent?
progressive aphasia, is typi-cally marked by fluent but empty speech, anomia,17deficits in comprehension, and spared grammarand syntax (Gorno-Tempini et al., 2011).
Incontrast, PNFA is characterized by halting andsometimes agrammatic speech, reduced syntac-tic complexity, word-finding difficulties, and rela-tively spared single-word comprehension (Gorno-Tempini et al., 2011).
The third subtype, LPA, ischaracterized by slow speech and frequent wordfinding difficulties; this subtype is not included inthe current analysis.Although clear diagnostic criteria for PPA havebeen established (Gorno-Tempini et al., 2011),there is no one test which can provide a diagno-sis.
Classification of PPA into subtypes requiresevaluation of spoken output, as well as neuropsy-chological assessment and brain imaging.
Quali-tative evaluation of speech often can be done accu-rately by clinicians or researchers, but the abilityto do this evaluation can require years of trainingand experience.
Some researchers have performeddetailed quantitative characterization of speech inPPA, but the precise characteristics of speech arenot yet fully understood and this process is tootime-consuming for most clinicians.Peintner et al.
(2008) conducted one of the earli-est automatic analyses of speech from individualswith FTD, including SD and PNFA as well as abehavioural variant.
They considered psycholin-guistic features as well as phoneme duration fea-tures extracted from the audio samples.
Althoughthey were fairly successful in classifying partici-pants according to their subtype, they did not re-port many details regarding the specific featureswhich were useful or how those features might re-flect the underlying impairment of the speakers.Pakhomov et al.
(2010a) examined FTD speechfrom an information-theoretic approach.
Theyconstructed a language model of healthy controlspeech, and then calculated the perplexity and out-of-vocabulary rate for each of the patient groupsrelative to that model.
In another study, Pakhomovet al.
(2010b) extracted speech and language fea-tures from samples of FTD speech.
In a principalcomponents analysis, they discovered four com-ponents which accounted for most of the variancein their data: speech length, hesitancy, empty con-tent, and grammaticality.
However, they did notperform any classification experiments.Fraser et al.
(2013a) attempted to classify par-ticipants as either SD patients, PNFA patients, orhealthy controls using a large number of languageSD(N = 11)PNFA(N = 13)Control(N = 16)Male/Female 8/3 7/6 9/7Age (yrs) 65.9 (7.1) 64.5 (10.4) 67.8 (8.2)Education (yrs) 17.5 (5.8) 14.0 (3.5) 16.8 (4.3)Table 1: Demographic information.
Numbers aregiven in the form: mean (standard deviation).features extracted from manually-transcribed tran-scripts.
They distinguished between SD and con-trol participants with very high accuracy, and werealso successful at distinguishing between PNFAand control participants.
However, their methoddid not perform as well on the task of classify-ing SD vs. PNFA speakers.
In subsequent work(Fraser et al., 2013b), they expanded their featureset to include acoustic features extracted directlyfrom the audio file.3 Methods3.1 DataTwenty-four patients with PPA were recruitedthrough three Toronto memory clinics, and 16 age-and education-matched healthy controls were re-cruited through a volunteer pool.
All participantswere native speakers of English, or had completedsome of their education in English.
Exclusion cri-teria included a known history of drug or alcoholabuse and a history of neurological or major psy-chiatric illness.
Each patient was diagnosed by abehavioural neurologist and all met current crite-ria for PPA (Gorno-Tempini et al., 2011).
Table 1shows demographic information for each group.To elicit a sample of narrative speech, partici-pants were asked to tell the well-known story ofCinderella.
They were given a wordless picturebook to remind them of the story; then the bookwas removed and they were asked to tell the storyin their own words.
This procedure, described infull by Saffran et al.
(1989), is commonly used instudies of connected speech in aphasia.The narrative samples were transcribed bytrained research assistants.
The transcriptions in-clude filled pauses, repetitions, and false starts,and were annotated with the total speech time.Sentence boundaries were marked according to se-mantic, syntactic, and prosodic cues.3.2 Classification frameworkGiven the audio files and transcripts, we can thencalculate our features (described in detail below)18and use them to train a support vector machine(SVM) classifier.
We use a leave-one-out cross-validation framework and report the average ac-curacy (i.e.
proportion of correctly classified in-stances) across folds.
We optimize the complexityparameter and the kernel type in a nested cross-validation loop over the training set.
For compar-ison, we also tested a na?
?ve Bayes classifier; how-ever we found that the results were consistentlypoorer and we do not report them here.3.3 FeaturesIn the following sections we will describe each ofthe feature sets that we use and explain how thefeatures are computed, and we will discuss someof the potential advantages and disadvantages as-sociated with each set.
In particular, we discusswhat types of data are necessary for the extractionof these features.
The data types are: unsegmentedtranscripts, segmented transcripts, and audio.3.3.1 Part-of-speech featuresDifferent categories of words may be selectivelyimpaired in different types of dementia.
In PPA,individuals with SD tend to be more impairedwith respect to nouns than verbs, and may replacenouns with pronouns or circumlocutory phrases.In contrast, individuals with PNFA may have moredifficulty with verbs and may even demonstrateagrammatism, which can result in the omissionof grammatical morphemes and function words.Thus, it is often useful to compare the relative fre-quencies with which words representing the differ-ent parts-of-speech (POS) are produced in a sam-ple, as in Table 2.
Similar features have been re-ported in computational studies of MCI (Roark etal., 2011), FTD (Pakhomov et al., 2010b), andDAT (Bucks et al., 2000).
Numerous POS taggersexist, although we use the Stanford tagger here(Toutanova et al., 2003).3.3.2 Complexity featuresChanges in linguistic complexity may accompanythe onset of dementia, although some studies havefound a decrease in complexity (e.g.
Kemper et al.
(2001)) while others have found an increase (e.g.Le et al.
(2011)).The features in Table 3 vary in their ease ofcomputation.
Mean word length can be calculatedfrom an unsegmented transcript, while mean sen-tence length requires only sentence boundary seg-mentation.
Other measures, such as Yngve depthNouns # nouns / # wordsVerbs # verbs / # wordsNoun-verb ratio # nouns / # verbsNoun ratio # nouns / (# nouns + # verbs)Inflected verbs # inflected verbs / # verbsDeterminers # determiners / # wordsDemonstratives # demonstratives / # wordsPrepositions # prepositions / # wordsAdjectives # adjectives / # wordsAdverbs # adverbs / # wordsPronoun ratio # pronouns / (# nouns + # pronouns)Function words # function words / # wordsInterjections # interjections / # wordsTable 2: Part-of-speech features.Max depth maximum Yngve depth of each parse tree,averaged over all sentencesMean depth mean Yngve depth of each node in theparse tree, averaged over all sentencesTotal depth total sum of the Yngve depths of each nodein the parse tree, averaged over all sentencesTree height height of each parse tree, averaged overall sentencesMLS mean length of sentenceMLC mean length of clauseMLT mean length of T-unitSubordinate conjunctions number of subordinateconjunctionsCoordinate conjunctions number of coordinate con-junctionsSubordinate:coordinate ratio ratio of number of sub-ordinate conjunctions to number of coordinateconjunctionsMean word length mean length, in letters, of eachword in the sampleTable 3: Complexity features.
(Yngve, 1960), require full parses of the sentences(we use the Stanford parser (Klein and Manning,2003) and Lu?s Syntactic Complexity Analyzer(Lu, 2010)).3.3.3 CFG featuresAlthough many of the complexity features aboveare derived from parse trees, in this section wepresent a set of features that take into accountthe context-free grammar (CFG) labels on eachof the nodes.
CFG features have been previouslyused to assess the grammaticality of sentences inan artificial error corpus (Wong and Dras, 2010)and to distinguish human from machine transla-tions (Chae and Nenkova, 2009).
However, thisis the first time such features have been applied tospeech from participants with dementia.In Table 4 we list a few examples of our 134CFG features, as well as the three phrase-level fea-tures (calculated for noun phrases, verb phrases,and prepositional phrases).19NP ?
NNS Noun phrases consisting of only a pluralnounVP ?
VBN PP Verb phrases consisting of a past-participle verb and a prepositional phraseROOT?
INTJ Trees consisting of only an interjec-tionPhrase type proportion Length of each phrase type(noun phrase, verb phrase, or prepositionalphrase), divided by total narrative lengthAverage phrase type length Total number of words ina phrase type, divided by the number of phrasesof that typePhrase type rate Number of phrases of a given type,divided by total narrative lengthTable 4: CFG features.Um Frequency of filled pause umUh Frequency of filled pause uhNID Frequency of words Not In Dictionary (e.g.
para-phasias, neologisms)Verbal rate Number of words per minuteTotal words Total number of words producedTable 5: Fluency features.3.3.4 Fluency featuresPark et al.
(2011) found that listeners?
judgementsof fluency were affected by a number of differentvariables, and the three most discriminative fea-tures were ?speech rate, speech productivity, andaudible struggle.?
For our list of fluency features(Table 5), we include only those features whichcould be extracted from the transcripts alone (as-suming the total speech time is given).
We countpauses filled by um and uh separately, as researchhas suggested that they may indicate different cog-nitive processes (Clark and Fox Tree, 2002).The number of words in a sample could be eas-ily generated using the word count feature in mosttext-editing software (although we first excludefilled pauses and NID tokens), and the verbal ratecan subsequently be calculated directly.
The otherthree features are easily calculated using stringmatching and an electronic dictionary.3.3.5 Psycholinguistic featuresSome types of dementia are characterized by im-pairments in semantic access.
Such impairmentsmay be sensitive to psycholinguistic features suchas lexical frequency, familiarity, imageability, andage of acquisition (Table 6).
We use the SUBTLfrequency norms (Brysbaert and New, 2009) andthe combined Bristol and Gilhooly-Logie norms(Stadthagen-Gonzalez and Davis, 2006; Gilhoolyand Logie, 1980) for familiarity, imageability, andFrequency Frequency with which a word occurs insome corpus of natural languageFamiliarity Subjective rating of how familiar a wordseemsImageability Subjective rating of how easily a wordgenerates an image in the mindAge of acquisition Subjective rating of how old a per-son is when they first learn that wordLight verbs Number of occurrences of be, have, come,go, give, take, make, do, get, move, and put,normalized by total number of verbsTable 6: Psycholinguistic features.age of acquisition (see Table 6).
We compute theaverage of each of these measures for all contentwords, as well as for nouns and verbs separately.Another measure that fits into this category isthe frequency of occurrence of light verbs, whichan impaired speaker may use to replace a morespecific verb.
We use the same list of light verbsas Breedin et al.
(1998), given in Table 6.One challenge associated with psycholinguis-tic features is finding norms which provide ade-quate coverage for the given data.
Fraser et al.
(2013a) reported that the SUBTL frequency normshad a coverage of above 90% on their data, but theBristol-Gilhooly-Logie norms had a coverage ofonly around 30%.3.3.6 Vocabulary richness featuresIndividuals experiencing semantic difficulty mayuse a limited range of vocabulary.
We can mea-sure the vocabulary richness or lexical diversityof a narrative sample using a number of differentmetrics (see Table 7).
Type-token ratio has beena popular choice, perhaps due to its simplicity;however it is sensitive to the length of the sample.Bucks et al.
(2000) were the first to apply Honor?e?sstatistic and Brun?et?s index to the study of demen-tia, and found significant differences between in-dividuals with DAT and healthy controls.
Cov-ington and McFall (2010) proposed a new mea-sure called the moving-average type-token ratio(MATTR), which is independent of text length.This feature was later applied to aphasic speech ina study by Fergadiotis and Wright (2011), and wasfound to be one of the most unbiased indicators oflexical diversity in impaired speakers.The measures given in Table 7 are easily com-puted from their respective formulae.
In this work,we lemmatize each word using NLTK (Bird etal., 2009) before calculating the features.
ForMATTR, we consider w = 10,20,30,40,50.20Honor?e?s statistic NV?0.165/ where V is the number ofword types and N is the number of word tokens.Brun?et?s index 100logN/(1?V1/V ) where V1is thenumber of words used only once, V is the totalnumber of word types, and N is the number ofword tokens.Type-token ratio (TTR) V/N where V is the num-ber of word types and N is the number of wordtokens.Moving-average type-token ratio (MATTRw) TTRcalculated over a moving window of size w,and averaged over all windows.Table 7: Vocabulary richness features.3.3.7 Acoustic featuresWhat we call acoustic features are extracted di-rectly from the audio file.
We consider the fea-tures given in Table 8.
Acoustic features have beenshown to be useful when automatically detectingconditions such as Parkinson?s disease, in whichchanges in speech are common (Little et al., 2009;Tsanas et al., 2012).
Acoustic features have alsobeen examined in studies of DAT (Meil?an et al.,2014), FTD (Pakhomov et al., 2010b), and PPA(Fraser et al., 2013b, whose software we use here).An obvious benefit to acoustic features is thatthey do not require a transcription, and can be cal-culated immediately given an audio sample.
Thecorresponding drawback is that they tell us noth-ing about the linguistic content of the narrative.4 ExperimentsWe report the results of three experiments explor-ing the discriminative power of the different fea-tures.
We first compare the classification accura-cies using each individual feature set.
We then per-form an ablation study to determine which com-bination of feature sets leads to the highest clas-sification accuracy.
We also look at individualfeatures across sets and discuss which ones arethe most discriminative, particularly in situationswhere data might be limited.4.1 Individual comparison of accuracy by setThe accuracies which result from using each fea-ture set individually are given in Table 9.
Thehighest accuracy across the three tasks is achievedin distinguishing SD participants from controls.An accuracy of .963 can be achieved using allthe features together, or using the psycholinguis-tic or POS features alone.
This is consistent withthe semantic impairments that are observed in SD.Total duration of speech Total length of all non-silentsegmentsPhonation rate Total duration of speech / total dura-tion of the sample (including pauses)Mean pause duration Mean length of pauses > 0.15msShort pause count # Pauses > 0.15 ms and < 0.4 msLong pause count # Pauses > 0.4 msPause:word ratio Ratio of silent segments longer than150 ms to non-silent segmentsF0:3mean Mean of the fundamental frequency and thefirst three formant frequenciesF0:3variance Variance of the fundamental frequencyand the first three formant frequenciesMean instantaneous power Measure related to theloudness of the signalMean 1st ACF Mean first autocorrelation functionMax 1st ACF Maximum first autocorrelation functionSkewness Measure of lack of symmetry, associatedwith tense or creaky voiceKurtosis Measure of the peakedness of the signalZCR Zero-crossing rate, can be used to distinguishbetween voiced and unvoiced regionsMRPDE Mean recurrence period density entropy, ameasure of periodicityJitter Measure of the short-term variation in the pitch(frequency) of a voiceShimmer Measure of the short-term variation in theloudness (amplitude) of a voiceTable 8: Acoustic features.The measures of vocabulary richness do not distin-guish between the SD and control groups, suggest-ing it is the words themselves, and not the numberof different words being used, that is important.In the case of PNFA participants vs. controls,we find that the highest accuracy is achieved us-ing all the features, and the second highest by us-ing only acoustic features.
This is not surprising,considering that the acoustic features include mea-sures of pausing and phonation rate, which candetect the characteristic halting speech of PNFA.The third best accuracy is achieved using the flu-ency features, which also fits with this explana-tion.
However, we might have expected that thecomplexity and CFG features would be more sen-sitive to the grammatical impairments of PNFA.Finally, the best accuracy for SD vs. PNFAis lower than in the previous two cases, and isachieved using only CFG features.
This sug-gests that there are some grammatical construc-tions which occur with different frequencies inthe two groups.
These differences do not appearto be captured by the complexity features, whichcould explain why Fraser et al.
(2013a) did not findsyntactic differences between the SD and PNFAgroups.
Interestingly, the results using CFG fea-21Feature setSD vs.controlsPNFA vs.controlsSD vs.PNFAAll .963 .931 .708Acoustic .778 .862 .167Psycholinguistic .963 .724 .708POS .963 .690 .375Complexity .852 .621 .667Fluency .667 .828 .500Vocab.
richness .481 .586 .583CFG .630 .690 .792Table 9: Classification accuracies for each featureset individually using a SVM classifier.
Bold indi-cates the highest accuracy for each task.tures are actually higher than the results using allfeatures.
This demonstrates that classifier perfor-mance can be adversely affected by the presenceof irrelevant features, especially in small data sets.4.2 Combining feature setsIn the previous section we examined the featuresets individually; however, one type of featuremay complement the information contained in an-other feature set, or it may contain redundant in-formation.
To examine the interactions betweenthe feature sets, we perform an ablation study.Starting with all the features, we remove each fea-ture set one at a time and measure the accuracyof the classifier.
The feature set whose removalcauses the smallest decrease in accuracy is then re-moved permanently from the experiment, the rea-soning being that the most important feature setswill cause the greatest decrease in accuracy whenremoved.
In some cases, we observe that the clas-sification accuracy actually increases when a setis removed, which suggests that those features arenot relevant to the classification (at least in combi-nation with the other sets).
In the case of a tie, weremove the feature set whose individual classifica-tion accuracy on that task is lowest.
The procedureis then repeated on the remaining feature sets, con-tinuing until only one set remains.The results for SD vs. controls are given in Ta-ble 10a.
The best result, 1.00, is achieved bycombining the psycholinguistic and POS features.This is unsurprising, since each of those featuresets perform well individually.
Curiously, thesame result can also be achieved by also includingthe complexity, vocabulary richness, and CFG fea-tures, but not in the intermediate stages betweenthose two optimal sets.
We attribute this to the in-teractions between features and the small data set.For PNFA vs. controls, shown in Table 10b, the(a) SD vs. controls.Removed Remaining Features AccuracyA+P+POS+C+F+VR+CFG .963F A+P+POS+C+VR+CFG .963A P+POS+C+VR+CFG 1.00VR P+POS+C+CFG .926CFG P+POS+C .926C P+POS 1.00POS P .963(b) PNFA vs. controls.Removed Remaining Features AccuracyA+P+POS+C+F+VR+CFG .931VR A+P+POS+C+F+CFG .931C A+P+POS+F+CFG .931POS A+P+F+CFG .931CFG A+P+F .966F A+P .966P A .862(c) SD vs. PNFA.Removed Remaining Features AccuracyA+P+POS+C+F+VR+CFG .708POS A+P+C+F+VR+CFG .750VR A+P+C+F+CFG .833F A+P+C+CFG .833A P+C+CFG .792C P+CFG .917P CFG .792Table 10: A=acoustic, P=psycholinguistic,POS=part-of-speech, C=complexity, F=fluency,VR=vocabulary richness, CFG=CFG productionrule features.
Bold indicates the highest accuracywith the fewest feature sets.best result of .966 is achieved using a combina-tion of acoustic and psycholinguistic features.
Inthis case the removal of the fluency features, whichgave the second highest individual accuracy, doesnot make a difference to the accuracy.
This sug-gests that the fluency features contain similar in-formation to one of the remaining sets, presum-ably the acoustic set.In the case of SD vs. PNFA, we again see thatthe best accuracy can be achieved by combiningtwo feature sets, as shown in Table 10c.
Us-ing psycholinguistic and CFG features, we canachieve an accuracy of .917, a substantial im-provement over the best accuracy for this task inTable 9.
In fact, in all three cases we see that us-ing a carefully selected combination of feature setscan result in better accuracy than using all the fea-ture sets together or using any one set individually.4.3 Best features for incomplete dataUp to this point, we have examined complete fea-ture sets.
We now briefly explore which individual22features are the most discriminative across featuresets.
We approach this as a practical consideration:given the data that a researcher has, and limited re-sources, what are the best features to measure?
Weconsider the following four scenarios:1.
Given audio files only.
This scenario oftenarises because it is relatively easy to recordspeech, but difficult to have it transcribed.Only acoustic features can be extracted.2.
Given basic transcriptions only (no audio).We assume there is no sentence segmentationand the time is not marked (e.g.
as in the out-put of automatic speech recognition).
Thus,we can measure psycholinguistic, POS, andvocabulary measures.
We can also measurethe fluency features except for verbal rate,as well as mean word length and subordi-nate/coordinate conjunctions from the com-plexity set.
Without sentence boundaries, wecannot parse the transcripts.3.
Given fully segmented transcripts (no audio).We can measure all features except for acous-tic features.4.
Given audio and fully segmented transcripts.We can measure all features.For each scenario, we rank the available fea-tures by their ?2value and choose the top 10 onlyas input to the SVM classifier (see Manning et al.
(2008) for a complete explanation of ?2feature se-lection).
We only include features if ?2> 0, so incases where there are very few relevant features,fewer than 10 features may be selected.
Becausewe perform cross-validation, the selected featuresmay vary across different folds.
In the tables thatfollow, we present the features ranked by the num-ber of folds in which they appear (i.e.
a featurewith the value 1.00 was selected in every fold).Due to space constraints, only the top 10 rankedfeatures are shown.The results for Scenario 1 are given in Ta-ble 11a.
For the SD vs. controls and PNFA vs.controls, the most highly ranked features tend tobe related to fluency and rate of speech, as wellas voice quality (skewness and MRPDE).
How-ever, when distinguishing between the two patientgroups, the acoustic features are essentially use-less.
In most cases, we see that none of the acous-tic features had a non-zero ?2value, and thus theclassifier could not be properly trained.For Scenario 2 (Table 11b), the results for SDvs.
controls show that within the psycholinguisticand POS feature sets, features relating to familiar-ity and frequency are very important, as well asnouns and demonstratives.
In the PNFA vs. con-trols case, we see that a number of the vocabularyrichness features are selected, which is in contrastto the previous two experiments.
However, it ap-pears that only the MATTR feature is important(with varying window lengths), so when we con-sidered only full feature sets, that information wasobscured by the other, irrelevant features in thatset.
The SD vs. PNFA case shows a mix of fea-tures from the previous two cases.For Scenario 3 (Table 11c), we add the com-plexity and CFG features.
These features do nothave a large effect in the SD vs. controls case, buta few CFG features are selected in the PNFA vs.controls and SD vs. PNFA cases.In Scenario 4 (Table 11d), we consider all fea-tures.
In the SD vs. controls case this increasesthe accuracy.
However, for PNFA vs. controls andSD vs. PNFA, the classification accuracy actuallydecreases, relative to Scenario 3.
When the num-ber of features increases, the potential to overfit tothe training data fold also increases, and it seemslikely that that is occurring here.
Nonetheless, weexpect that the features which are selected in everyfold are still highly relevant.
These features areunchanged between Scenarios 3 and 4 in the SDvs.
controls and SD vs. PNFA case, however in thePNFA vs. controls case, the acoustic features arenow ranked more highly than some of the vocabu-lary richness and CFG features from Scenario 3.5 DiscussionWhile it may be tempting to calculate as manyfeatures as possible and use them all in a classi-fier, we have shown here that better results can beachieved by choosing a small, relevant subset offeatures.
In particular, psycholinguistic featuressuch as frequency and familiarity were useful in allthree classification tasks.
Acoustic features wereuseful in discriminating patients from controls, butnot for discriminating between the two PPA sub-types.
We also found that MATTR was relevantin some cases, although the other vocabulary rich-ness features were not, and that the CFG featureswere more useful than traditional measures of syn-tactic complexity.
POS features were useful onlyin distinguishing between SD and controls.One of the biggest challenges in this typeof work is the small amount of data available.23(a) Scenario 1: audio only.SD vs. control, Acc: .852 PNFA vs. control, Acc: .793 SD vs. PNFA, Acc: .5001.00 skewness 1.00 long pause count .083 max 1st ACF1.00 phonation rate 1.00 phonation rate .042 mean F31.00 MRPDE 1.00 short pause count1.00 mean duration of pauses 1.00 MRPDE.037 long pause count 1.00 mean duration of pauses.037 mean 1st ACF .966 pause:word ratio.037 kurtosis .793 skewness.793 ZCR.345 mean inst.
power.035 jitter(b) Scenario 2: unsegmented transcripts.SD vs. control, Acc: .926 PNFA vs. control, Acc: .621 SD vs. PNFA, Acc: .7921.00 familiarity 1.00 MATTR 50 1.00 familiarity1.00 noun frequency 1.00 MATTR 40 1.00 noun frequency1.00 noun familiarity 1.00 MATTR 30 1.00 noun familiarity1.00 frequency 1.00 frequency 1.00 MATTR 201.00 verb frequency 1.00 MATTR 20 .708 MATTR 101.00 nouns .931 total words .208 MATTR 301.00 demonstratives .759 light verbs .042 MATTR 50.778 pronoun ratio .690 adjectives .042 MATTR 40.667 noun imageability .241 age of acquisition .042 light verbs.630 Honor?e?s statistic .241 MATTR 10 .042 verbs(c) Scenario 3: segmented transcripts.SD vs. control, Acc: .926 PNFA vs. control, Acc: .897 SD vs. PNFA, Acc: .7921.00 word length 1.00 MATTR 50 1.00 WHADVP?WRB1.00 familiarity 1.00 MATTR 40 1.00 familiarity1.00 noun frequency 1.00 WHNP?WP 1.00 noun familiarity1.00 noun familiarity 1.00 frequency 1.00 noun frequency1.00 frequency 1.00 MATTR 20 1.00 MATTR 201.00 demonstratives 1.00 verbal rate 1.00 NP?
NNS.889 nouns .966 MATTR 30 1.00 SBAR?WHADVP S.852 verb frequency .827 S1?
INTJ .667 MATTR 10.630 MLS .483 total words .500 NP?
DT JJ NNS.630 total Yngve depth .414 word length .458 SQ?
AUX NP VP(d) Scenario 4: segmented transcripts + audio.SD vs. control, Acc: .963 PNFA vs. control, Acc: .793 SD vs. PNFA, Acc: .7501.00 word length 1.00 frequency 1.00 WHADVP?WRB1.00 familiarity 1.00 phonation rate 1.00 familiarity1.00 noun frequency 1.00 MRPDE 1.00 noun familiarity1.00 noun familiarity 1.00 verbal rate 1.00 noun frequency1.00 frequency 1.00 mean duration of pauses 1.00 MATTR 201.00 demonstratives .897 MATTR 50 1.00 NP?
NNS.963 phonation rate .897 WHNP?WP 1.00 SBAR?WHADVP S.741 verb frequency .897 MATTR 20 .625 MATTR 10.593 nouns .690 MATTR 40 .500 NP?
DT JJ NNS.333 MLS .690 MATTR 30 .458 SQ?
AUX NP VPTable 11: Classification accuracies and top 10 features for four different data scenarios.Psychological studies are typically on the or-der of only tens to possibly hundreds of partic-ipants, while machine learning researchers oftentackle problems with thousands to millions of datapoints.
We have chosen techniques appropriate forsmall data sets, but acknowledging the potentialweaknesses of machine learning methods whentraining data are limited, these findings must beconsidered preliminary.
However, we also believethat this is a promising approach for future ap-plications, including automated screening for lan-guage impairment, support for clinical diagnosis,tracking severity of symptoms over time, and eval-uating therapeutic interventions.AcknowledgmentsThis research was supported by the Natural Sciences and En-gineering Research Council of Canada and the Canadian In-stitutes of Health Research (grant #MOP-8277).
Thanks toFrank Rudzicz for the acoustic features software.24ReferencesSteven Bird, Ewan Klein, and Edward Loper.
2009.
Naturallanguage processing with Python.
O?Reilly Media, Inc.Sarah D. Breedin, Eleanor M. Saffran, and Myrna F.Schwartz.
1998.
Semantic factors in verb retrieval: Aneffect of complexity.
Brain and Language, 63:1?31.Marc Brysbaert and Boris New.
2009.
Moving beyondKu?cera and Francis: A critical evaluation of current wordfrequency norms and the introduction of a new and im-proved word frequency measure for American English.Behavior Research Methods, 41(4):977?990.R.S.
Bucks, S. Singh, J.M.
Cuerden, and G.K. Wilcock.2000.
Analysis of spontaneous, conversational speech indementia of Alzheimer type: Evaluation of an objectivetechnique for analysing lexical performance.
Aphasiol-ogy, 14(1):71?91.Jieun Chae and Ani Nenkova.
2009.
Predicting the fluencyof text with shallow structural features: case studies of ma-chine translation and human-written text.
In Proceedingsof the 12th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 139?147.Association for Computational Linguistics.Herbert H. Clark and Jean E. Fox Tree.
2002.
Using uh andum in spontaneous speaking.
Cognition, 84(1):73?111.Michael A. Covington and Joe D. McFall.
2010.
Cuttingthe Gordian knot: The moving-average type?token ratio(MATTR).
Journal of Quantitative Linguistics, 17(2):94?100.Gerasimos Fergadiotis and Heather Harris Wright.
2011.Lexical diversity for adults with and without apha-sia across discourse elicitation tasks.
Aphasiology,25(11):1414?1430.Kathleen C. Fraser, Jed A. Meltzer, Naida L. Graham, CarolLeonard, Graeme Hirst, Sandra E. Black, and ElizabethRochon.
2013a.
Automated classification of primaryprogressive aphasia subtypes from narrative speech tran-scripts.
Cortex.Kathleen C. Fraser, Frank Rudzicz, and Elizabeth Rochon.2013b.
Using text and acoustic features to diagnose pro-gressive aphasia and its subtypes.
In Proceedings of Inter-speech.K.J.
Gilhooly and R.H. Logie.
1980.
Age-of-acquisition, im-agery, concreteness, familiarity, and ambiguity measuresfor 1,944 words.
Behavior Research Methods, 12:395?427.M.L.
Gorno-Tempini, A.E.
Hillis, S. Weintraub, A. Kertesz,M.
Mendez, S.F.
Cappa, J.M.
Ogar, J.D.
Rohrer, S. Black,B.F.
Boeve, F. Manes, N.F.
Dronkers, R. Vandenberghe,K.
Rascovsky, K. Patterson, B.L.
Miller, D.S.
Knopman,J.R.
Hodges, M.M.
Mesulam, and M. Grossman.
2011.Classification of primary progressive aphasia and its vari-ants.
Neurology, 76:1006?1014.William Jarrold, Bart Peintner, Eric Yeh, Ruth Krasnow,Harold Javitz, and Gary Swan.
2010.
Language ana-lytics for assessing brain health: Cognitive impairment,depression and pre-symptomatic Alzheimer?s disease.
InYiyu Yao, Ron Sun, Tomaso Poggio, Jiming Liu, NingZhong, and Jimmy Huang, editors, Brain Informatics, vol-ume 6334 of Lecture Notes in Computer Science, pages299?307.
Springer Berlin / Heidelberg.Susan Kemper, Marilyn Thompson, and Janet Marquis.2001.
Longitudinal change in language production: Ef-fects of aging and dementia on grammatical complex-ity and propositional content.
Psychology and Aging,16(4):600?614.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st Meetingof the Association for Computational Linguistics, pages423?430.Xuan Le, Ian Lancashire, Graeme Hirst, and Regina Jokel.2011.
Longitudinal detection of dementia through lex-ical and syntactic changes in writing: a case study ofthree British novelists.
Literary and Linguistic Comput-ing, 26(4):435?461.Max A.
Little, Patrick E. McSharry, Eric J.
Hunter, JenniferSpielman, and Lorraine O. Ramig.
2009.
Suitabilityof dysphonia measurements for telemonitoring of Parkin-son?s disease.
Biomedical Engineering, IEEE Transac-tions on, 56(4):1015?1022.Xiaofei Lu.
2010.
Automatic analysis of syntactic complex-ity in second language writing.
International Journal ofCorpus Linguistics, 15(4):474?496.Christopher D. Manning, Prabhakar Raghavan, and HinrichSch?utze.
2008.
Introduction to Information Retrieval.Cambridge University Press.Juan Jos?e G. Meil?an, Francisco Mart?
?nez-S?anchez, JuanCarro, Dolores E. L?opez, Lymarie Millian-Morell, andJos?e M. Arana.
2014.
Speech in Alzheimer?s disease:Can temporal and acoustic parameters discriminate de-mentia?
Dementia and Geriatric Cognitive Disorders,37(5-6):327?334.Serguei V.S.
Pakhomov, Glen E. Smith, Susan Marino, An-gela Birnbaum, Neill Graff-Radford, Richard Caselli,Bradley Boeve, and David D. Knopman.
2010a.
A com-puterized technique to asses language use patterns in pa-tients with frontotemporal dementia.
Journal of Neurolin-guistics, 23:127?144.S.V.
Pakhomov, G.E.
Smith, D. Chacon, Y. Feliciano,N.
Graff-Radford, R. Caselli, and D. S. Knopman.
2010b.Computerized analysis of speech and language to identifypsycholinguistic correlates of frontotemporal lobar degen-eration.
Cognitive and Behavioral Neurology, 23:165?177.Hyejin Park, Yvonne Rogalski, Amy D. Rodriguez, ZvinkaZlatar, Michelle Benjamin, Stacy Harnish, Jeffrey Ben-nett, John C. Rosenbek, Bruce Crosson, and Jamie Reilly.2011.
Perceptual cues used by listeners to discriminatefluent from nonfluent narrative discourse.
Aphasiology,25(9):998?1015.Bart Peintner, William Jarrold, Dimitra Vergyri, ColleenRichey, Maria Luisa Gorno Tempini, and Jennifer Ogar.2008.
Learning diagnostic models using speech and lan-guage measures.
In Engineering in Medicine and Biol-ogy Society, 2008.
EMBS 2008.
30th Annual InternationalConference of the IEEE, pages 4648?4651.Brian Roark, Margaret Mitchell, John-Paul Hosom, KristyHollingshead, and Jeffery Kaye.
2011.
Spoken languagederived measures for detecting mild cognitive impairment.IEEE Transactions on Audio, Speech, and Language Pro-cessing, 19(7):2081?2090.25Eleanor M. Saffran, Rita Sloan Berndt, and Myrna F.Schwartz.
1989.
The quantitative analysis of agrammaticproduction: procedure and data.
Brain and Language,37:440?479.Sameer Singh, Romola S. Bucks, and Joanne M. Cuerden.2001.
Evaluation of an objective technique for analysingtemporal variables in DAT spontaneous speech.
Aphasiol-ogy, 15(6):571?583.Hans Stadthagen-Gonzalez and Colin J. Davis.
2006.
TheBristol norms for age of acquisition, imageability, and fa-miliarity.
Behavior Research Methods, 38(4):598?605.Calvin Thomas, Vlado Keselj, Nick Cercone, Kenneth Rock-wood, and Elissa Asp.
2005.
Automatic detection andrating of dementia of Alzheimer type through lexical anal-ysis of spontaneous speech.
In Proceedings of the IEEEInternational Conference on Mechatronics and Automa-tion, pages 1569?1574.Kristina Toutanova, Dan Klein, Christopher Manning, andYoram Singer.
2003.
Feature-rich part-of-speech taggingwith a cyclic dependency network.
In Proceedings of the2003 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Lan-guage Technologies, pages 252?259.Athanasios Tsanas, Max A.
Little, Patrick E. McSharry, Jen-nifer Spielman, and Lorraine O. Ramig.
2012.
Novelspeech signal processing algorithms for high-accuracyclassification of Parkinson?s disease.
IEEE Transactionson Biomedical Engineering, 59(5):1264?1271.Sze-Meng Jojo Wong and Mark Dras.
2010.
Parser featuresfor sentence grammaticality classification.
In Proceed-ings of the Australasian Language Technology AssociationWorkshop, pages 67?75.Victor Yngve.
1960.
A model and hypothesis for languagestructure.
Proceedings of the American Physical Society,104:444?466.26
