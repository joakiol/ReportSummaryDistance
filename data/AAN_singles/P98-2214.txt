General-to-Specific Model Selectionfor Subcategorization Preference*Takeh i to  Utsuro  and Takash i  M iyata  and Yu j i  MatsumotoGraduate  School of  In fo rmat ion  Science, Nara  Ins t i tu te  of  Science and Technology8916-5, Takayama-cho, Ikoma-shi, Nara, 630-0101, JAPANE-mail: utsuro@is, a ist -nara,  ac.
jp, URL: h t tp : / / c l ,  a ist -nara,  ac.
jp / -utsuro/Abst rac tThis paper proposes a novel method for learningprobability models of subcategorization preference ofverbs.
We consider the issues of case dependenciesand noun class generalization i  a uniform way by em-ploying the maximum entropy modeling method.
Wealso propose a new model selection algorithm whichstarts from the most general model and gradually ex-amines more specific models.
In the experimentalevaluation, it is shown that both of the case depen-dencies and specific sense restriction selected by theproposed method contribute to improving the perfor-mance in subcategorization preference resolution.1 In t roduct ionIn empirical approaches to parsing, lexi-cal/semantic collocation extracted from corpushas been proved to be quite useful for rankingparses in syntactic analysis.
For example, Mager-man (1995), Collins (1996), and Charniak (1997)proposed statistical parsing models which incor-porated lexical/semantic information.
In theirmodels, syntactic and lexical/semantic featuresare dependent on each other and are combinedtogether.
This paper also proposes a methodof utilizing lexical/semantic features for the pur-pose of applying them to ranking parses in syn-tactic analysis.
However, unlike the models ofMagerman (1995), Collins (1996), and Char-niak (1997), we assume that syntactic and lex-ical/semantic features are independent.
Then,we focus on extracting lexical/semantic colloca-tional knowledge of verbs which is useful in syn-tactic analysis.More specifically, we propose a novel methodfor learning a probability model of subcategoriza-tion preference of verbs.
In general, when learn-ing lexical/semantic collocational knowledge ofverbs from corpus, it is necessary to considerthe two issues of 1) case dependencies, and 2)noun class generalization.
When considering 1),we have to decide which cases are dependent oneach other and which cases are optional and in-* This research was partially supported by the Ministryof Education, Science, Sports and Culture, Japan, Grant-in-Aid for Encouragement of Young Scientists, 09780338,1998.
An extended version of this paper is available fromthe above URL.dependent of other cases.
When considering 2),we have to decide which superordinate class gen-erates each observed leaf class in the verb-nouncollocation.
So far, there exist several workswhich worked on these two issues in learning col-locational knowledge of verbs and also evaluatedthe results in terms of syntactic disambiguation.Resnik (1993) and Li and Abe (1995) studied howto find an optimal abstraction level of an argu-ment noun in a tree-structured thesaurus.
Theirworks are limited to only one argument.
Li andAbe (1996) also studied a method for learning de-pendencies between case slots and reported thatdependencies were discovered only at the slot-level and not at the class-level.Compared with these previous works, this pa-per proposes to consider the above two issuesin a uniform way.
First, we introduce a modelof generating a collocation of a verb and argu-ment/adjunct nouns (section 2) and then viewthe model as a probability model (section 3).
Asa model learning method, we adopt the max-imum entropy model learning method (DellaPietra et al, 1997; Berger et al, 1996).
Casedependencies and noun class generalization arerepresented as features in the maximum entropyapproach.
Features are allowed to have overlapand this is quite advantageous when we considercase dependencies and noun class generalizationin parameter estimation.
An optimal model is se-lected by searching for an optimal set of features,i.e, optimal case dependencies and optimal nounclass generalization levels.
As the feature selec-tion process, this paper proposes a new featureselection algorithm which starts from the mostgeneral model and gradually examines more spe-cific models (section 4).
As the model evalua-tion criterion during the model search from gen-eral to specific ones, we employ the descriptionlength of the model and guide the search processso as to minimize the description length (Ris-sanen, 1984).
Then, after obtaining a sequenceof subcategorization preference models which aretotally ordered from general to specific, we se-lect an approximately optimal subcategorizationpreference model according to the accuracy ofsubcategorization preference test.
In the exper-imental evaluation of performance of subcatego-1314rization preference, it is shown that  both  of thecase dependencies and specific sense restrictionselected by the proposed method contr ibute toimproving the performance in subcategorizat ionpreference resolution (section 5).2 A Mode l  o f  Generat ing  a Verb -NounCo l locat ion  f rom Subcategor i za t ionF rame(s )This section introduces a model of generatinga verb-noun collocation from subcategorizat ionframe(s).2.1 Data  S t ructureVerb -Noun Co l locat ion  Verb-noun col loca-t ion is a data structure for the collocation of averb and all of its a rgument /ad junct  nouns.
Averb-noun collocation e is represented by a fea-ture structure which consists of the verb v andall the pairs of co-occurring case-markers p andthesaurus classes e of case-marked nouns:Fred : vPl : cxe = .
(1 )Pk : CkWe assume that  a thesaurus  is a tree-structuredtype hierarchy in which each node representsa semantic class, and each thesaurus class0, .
.
.
,  Ck in a verb-noun collocation is a leaf classin the thesaurus.
We also introduce ~c as thesuperordinate-subordinate relation of classes ina thesaurus: cl ___e c2 means that  cl is subordi-1 nate to c2.Subcategor i za t ion  F rame A subcategor iza-t ion f rame s is represented by a feature structurewhich consists of a verb v and the pairs of case-markers p and sense restriction c of case-markedargument /ad junct  nouns:Fred : vpl : cls = .
(2)Pl : clSense restriction cl, ?
?
?, ct of case-marked argu-ment /ad junct  nouns are represented by classesat arb i t rary levels of the thesaurus.Subsumpt ion  Re la t ion  We introduce thesubsumpt ion  re la t ion  "~s$ of a verb-noun col lo-1Although we ignore sense ambiguities of case-markednouns in the definitions of this section, in the currentimplementation, we deal with sense ambiguities of case-marked nouns by deciding that a class c is superordinateto an ambiguous leaf class Cl if c is superordinate o atleast one of the possible unambiguous classes of Ct.cat ion  e and a subcategor i za t ion  f rame s:e --sl s iff.
for each case-marker Pi in s and itsnoun class csi, there exists the samecase-marker pi in e and its nounclass cei is subordinate to c~i, i.e.Cei  "<c CsiThe subsumpt ion relation "~s$ is applicable alsoas a subsumpt ion relation of two subcategoriza-tion frames.2.2 Generat ing  a Verb -Noun Co l locat ionf rom Subcategor i za t ion  F rame(s )Suppose a verb-noun collocation e is given as:Fred : vPl : Cele = .
(3)Pk : CekThen, let us consider a tuple (sl, .
.
.
, sn )  ofpar t ia l  subcategor i za t ion  f rames  which satisfiesthe following requirements: i) the unifications l  A .
.
.
Asn  of all the part ial  subcategor izat ionframes has exactly the same case-markers as ehas as in (4), ii) each semantic class Csi of acase-marked noun of the part ial  subcategoriza-tion frames is superordinate to the correspond-ing leaf semantic class eei of e as in (5), and iii)any pair si and si, (i 7?
i I) do not have commoncase-markers as in (6):S 1 A ? "
" A S n ~-wed : vP l  : Cs lPk : Cskcsi  ( i= l , .
.
.
, k )(4)pred : v \]J v jv j '  pi j  # pi,j, si = ' ( i , i '= l , .
.
,n ,  i# i ' )  (6) P i j  : C i jWhen a tuple (Sl, .
.
.
, sn )  satisfies the abovethree requirements, we assume that  the tuple (Sl,.
.
.
,  sn) can generate  the verb-noun collocation eand denote as below:(~,..., ~.)
, e (7)As we will describe in section 3.2, we assume thatthe part ial  subcategor izat ion frames Sl, .
.
.
,  Snare regarded as events occurring i ndependent lyof each other and each of them is assigned anindependent parameter .2.3 ExampleThis section shows how we can incorporate casedependenc ies  and noun class genera l i za t ion  intothe model of generating a verb-noun collocationfrom a tuple of part ial  subcategorizat ion frames?1315The Ambigu i ty  of  Case DependenciesThe problem of the ambiguity of case dependen-cies is caused by the fact that, only by observingeach verb-noun collocation in corpus, it is not de-cidable which cases are dependent on each otherand which cases are optional and independent ofother cases.
Consider the following example:Example 1Kodomo-ga kouen-de juusu-wo nomu.child-NOM park-at juice-A CC drink(A child drinks juice at the park.
)The verb-noun collocation is represented as afeature structure below:pred : nomu \]ga : Cc \] e = wo : cj (8)de : cpwhere co, cp, and cj represent the leaf classes(in the thesaurus) of the nouns "kodomo(child)","kouen(park)", and '~uusu(juice)'.Next, we assume that the concepts "hu-man", "place", and "beverage" are superordi-hate to "kodomo(child)", "kouen(park)", and'~uusu(juice)", respectively, and introduce thecorresponding classes Chum, Cplc, and Cbe v as senserestriction in subcategorization frames.
Then,according to the dependencies of cases, we canconsider several patterns of subcategorizationframes each of which can generate the verb-nouncollocation e.If the three cases "ga(NOM)",  "wo(ACC)",and "de(at)" are dependent on each other andit is not possible to find any division into severalindependent subcategorization frames, e can beregarded as generated from a subcategorizationframe containing all of the three cases:ga : Chum :' e (9)WO : Cbevde : CptcOtherwise, if only the two cases "ga(NOM)"and "wo(A CC)"  are dependent on each other andthe "de(at)" case is independent of those twocases, e can be regarded as generated from thefollowing two subcategorization frames indepen-dently:ga : Chu  m ' de : Cpl c ~ eWO : Cbe vThe Ambigu i ty  of Noun Class General iza-t ion The problem of the ambiguity of nounclass generalization is caused by the fact that,only by observing each verb-noun collocation incorpus, it is not decidable which superordinateclass generates each observed leaf class in theverb-noun collocation.
Let us again consider Ex-ample 1.
We assume that the concepts "mam-mal" and "liquid" are superordinate o "human"and "beverage", respectively, and introduce thecorresponding classes Cma m and Ctiq.
If we addi-tionally allow these superordinate classes as senserestriction in subcategorization frames, we canconsider several additional patterns of subcate-gorization frames which can generate the verb-noun collocation e.Suppose that only the two cases "ga(NOM)"and "wo(ACC)"  are dependent on each otherand the "de(at)" case is independent ofthose twocases as in the formula (10).
Since the leaf classcc ("child") can be generated from either Chumor cream, and also the leaf class cj ( '~uice') canbe generated from either Cbe v or  Cliq, e can beregarded as generated according to either of thefour formulas (10) and (11),~(13):ga : Cma m ~ de : Cpl c ) eWO : Cbe vga : Chum ' de : Cpl c >WO : Cliqga : c .
.
.
.
de : %to , e (13)WO : Cliq3 Max imum Ent ropy  Mode l ing  o fSubcategor i za t ion  P re ferenceThis section describes how we apply the maxi-mum entropy modeling approach of Della Pietraet al (1997) and Berger et al (1996) to modellearning of subcategorization preference.3.1 Max imum Ent ropy  Mode l ingGiven the training sample C of the events (x, y),our task is to estimate the conditional probabil-ity p(y I x) that, given a context x, the processwill output y.
In order to express certain featuresof the whole event (x, y), a binary-valued indica-tor function is introduced and called a featurefunction.
Usually, we suppose that there exists alarge collection F of candidate features, and in-clude in the model only a subset S of the full setof candidate features .T.
We call S the set of ac-tive features.
Now, we assume that S contains nfeature functions.
For each feature f i (E S),  thesets Vzi and Vyi indicate the sets of the valuesof x and y for that feature.
According to thosesets, each feature function fi will be defined asfollows:1 i fxE  V~iandyEVy if i (x,y) = 0 otherwiseThen, in the maximum entropy modeling ap-proach, the model with the maximum entropyis selected among the possible models.
With thisconstraint, he conditional probability of the out-put y given the context x can be estimated as thefollowing p~(y \[ x) of the form of the exponen-tial family, where a parameter Ai is introduced1316for each feature fi.
exp(~-'~ )qfi(x,y))p Cy I x) = ~ (14)y iThe parameter values )?i are estimated by analgorithm called Improved Iterative Scaling (IIS)algorithm.Feature  Select ion by One-by-one FeatureAdd ing  The feature selection process pre-sented in Della Pietra et al (1997) and Bergeret al (1996) is an incremental procedure thatbuilds up S by successively adding features one-by-one.
It starts with S as empty, and, at eachstep, selects the candidate feature which, whenadjoined to the set of active features S, pro-duces the greatest increase in log-likelihood ofthe training sample.3.2 Mode l ing  Subcategor i zat ion  Prefer -enceEvents  In our task of model learning of sub-categorization preference, each event (x,y) inthe training sample is a verb-noun collocation e,which is defined in the formula (1).
A verb-nouncollocation e can be divided into two parts: oneis the verbal part ev containing the verb v whilethe other is the nominal part ep containing all thepairs of case-markers p and thesaurus leaf classesc of case-marked nouns:Pk CkThen, we define the context x of an event (x, y)as the verb v and the output y as the nominal part& of e, and each event in the training sample isdenoted as (v, %):x = v, y -~ epFeatures  We represent each partial subcatego-rization frame as a feature in the maximum en-tropy modeling.
According to the possible vari-ations of case dependencies and noun class gen-eralization, we consider every possible patternsof subcategorization frames which can generatea verb-noun collocation, and then construct hefull set ~- of candidate features.
Next, for thegiven verb-noun collocation e, tuples of partialsubcategorization frames which can generateare collected into the set SF(e) as below:Then, for each partial subcategorization frames, a binary-valued feature function fs(V, ep) is de-fined to be true if and only if at least one elementof the set SF(e) is a tuple ( s l , .
.
.
, s , .
.
.
, sn )  thatcontains : 1 if 3(s l , .
.
.
,s , .
.
.
,sn)f,(v, ep) = ?
SF(e=(\[pred : v\] A %))0 otherwise1317In the maximum entropy modeling approach,each feature is assigned an independent param-eter, i.e., each (partial) subcategorization frameis assigned an independent parameter.Parameter  Es t imat ion  Suppose that the setS(C_ ~') of active features is found by the pro-cedure of the next section.
Then, the param-eters of subcategorization frames are estimatedaccording to IIS Algorithm and the conditionalprobability distribution ps(& \[ v) is given as::,~s (15)% f~ E S4 Genera l - to -Spec i f i c  Feature  Selec-t ionThis section describes the new feature selectionalgorithm which utilizes the subsumption rela-tion of subcategorization frames.
It starts fromthe most general model, i.e., a model with nocase dependency as well as the most generalsense restrictions which correspond to the high-est classes in the thesaurus.
This starting modelhas high coverage of the test data.
Then, the al-gorithm gradually examines more specific mod-els with case dependencies a well as more spe-cific sense restrictions which correspond to lowerclasses in the thesaurus.
The model search pro-cess is guided by a model evaluation criterion.4.1 Par t ia l l y -Ordered  Feature  SpaceIn section 2.1, we introduced subsumption rela-tion ~sl of two subcategorization frames.
All thesubcategorization frames are partially orderedaccording to this subsumption relation, and el-ements of the set .T of candidate features consti-tute a partially ordered feature space.Const ra in t  on Act ive Feature  SetThroughout the feature selection process,we put the following constraint on the activefeature set S:Case Covering Constraint: for each verb-nouncollocation in the training set C, each case p (andthe leaf class marked by p) of e has to be coveredby at least one feature in S.In it ia l  Act ive Feature  Set Initial set So ofactive features is constructed by collecting fea-tures which are not subsumed by any other can-didate features in ~-:So = ( f s lV fs , (   fs)  E ~,s  7~sf S t } (16)This constraint on the initial active feature setmeans that each feature in So has only one caseand the sense restriction of the case is (one of)the most general class(es).Candidate Non-active Features  for  Re-p lacement  At each step of feature selection,one of the active features is replaced with sev-eral non-active features.
Let G be a set of non-active features which have never been active untilthat step.
Then, for each active feature fs(E S),the set DI, (C ~) of candidate non-active featureswith which fs is replaced has to satisfy the fol-lowing two requirements 2 3.1.
Subsumption with s: for each element fs' of DI.
,s' has to be subsumed by s.2.
Upper Bound of ~: for each element fs, of DI, ,and for each element ft of G, t does not subsumes', i.e., DI, is a subset of the upper bound ofwith respect o the subsumption relation ~sI-Among all the possible replacements, the mostappropriate one is selected according to a modelevaluation criterion.4.2 Mode l  Eva luat ion  CriterionAs the model evaluation criterion during featureselection, we consider the following two types.4.2.1 MDL PrincipleThe MDL (Minimum Description Length) prin-ciple (Rissanen, 1984) is a model selection crite-rion.
It is designed so as to "select he model thathas as much fit to a given data as possible andthat is as simple as possible."
The MDL princi-ple selects the model that  minimizes the follow-ing description length l( M, D) of the probabil i tymodel M for the data D: 1Nl(M,D) = -logLM(D) + ~ MloglO I (17)where logLM(D) is the log-likelihood of themodel M to the data D, NM is the number ofthe parameters in the model 21I, and IDI is thesize of the data D.Description Length  o f  SubcategorizationPre ference  Mode l  The description lengthl(ps, ?)
of the probabil i ty model Ps (of (15)) forthe training data set C is given as below: 4l(ps,C) = - ~ logps(% I v )+ llsIloglCI (18)(v,e,.
)~2The general-to-specific feature selection considers onlya small portion of the non-active f atures as the next can-didate for the active feature, while the feature selection byone-by-one f ature adding considers all the non-active f a-tures as the next candidate.
Thus, in terms of efficiency,the general-to-specific feature selection has an advantageover the one-by-one f ature adding algorithm, especiallywhen the number of the candidate features is large.3As long as the case covering constraint is satisfied, theset Df, of candidate non-active features with which f, isreplaced could be an empty set 0.4More precisely, we slightly modify the probabilitymodel ps by multiplying the probability of generating theverb-noun collocation e from the (partial) subcategoriza-tion frames that correspond to active features evaluatingto true for e, and then apply the MDL principle to thismodified model.
The probability of generating a verb-noun collocation from (partial) subcategorization framesis simply estimated as the product of the probabilities4.2.2 Subcategorization Preference Testusing Posit ive/Negative ExamplesThe other type of the model evaluation criterionis the performance in the subcategorizat ion pref-erence test presented in Utsuro and Matsumoto(1997), in which the goodness of the model ismeasured according to how many of the posi-tive examples can be judged as more appropriatethan the negative xamples.
This subcategoriza-tion preference test can be regarded as modelingthe subcategorization ambiguity of an argumentnoun in a Japanese sentence with more than oneverbs like the one in Example 2.Example 2TV-de mouketa shounin-wo mitaTV-by/on earn money merchant-A CC see(If the phrase "TV-de'(by/on TV) modifies the verb"mouketa'(earn money), the sentence means that"(Somebody) saw a merchant who earned money by(selling) TV."
On the other hand, if the phrase "TV-de"(by/on TV) modifies the verb "mita'(see), thesentence means that "On TV, (somebody) saw a mer-chant who earned money.
")Negative examples are artificially generated fromthe positive examples by choosing a case elementin a positive example of one verb at random andmoving it to a positive example of another verb.Compared with the calculation of the descrip-tion length l(ps, C) in (18), the calculation of theaccuracy of subcategorizat ion preference test re-quires comparison of probabil ity values for suffi-cient number of positive and negative data andits computat ional  cost is much higher than thatof calculating the description length.
There-fore, at present, we employ the description lengthl(ps,C) in (18) as the model evaluation crite-rion during the general-to-specific feature selec-tion procedure, which we will describe in the nextsection in detail.
After obtaining a sequence ofactive feature sets (i.e., subcategorization pref-erence models) which are total ly ordered fromgeneral to specific, we select an optimal subcate-gorization preference model according to the ac-curacy of subcategorization preference test, as wewill describe in section 4.4.4.3 Feature Selection Algor i thmThe following gives the details of the general-to-specific feature selection algorithm, where the de-of generating each leaf-class in the verb-noun collocationfrom the corresponding superordinate class in the subcat-egorization frame.
With this generation probability, themore general the sense restriction of the subcategoriza-tion frames is, the less fit the model has to the data, andthe greater the data description length (the first term of(18)) of the model is.
Thus, this modification causes thefeature selection process to be more sensitive to the senserestriction of the model.1318scription length l(ps, g) in (18) is employed asthe model evaluation criterion: 5General-to-Specific Feature SelectionInput: Training data set E;collection ~- of candidate f aturesOutput: Set `S of active features;model Ps incorporating these features1.
Start with ,S = ,So of the definition (16) and withg =~' -&2.
Do for each active feature f E `S and every pos-sible replacement D I C G:Compute the model PSuD/-U} usingIIS Algorithm.Compute the decrease in the descrip-tion length of (18).3.
Check the termination condition s4.
Select the feature j and its replacement D\] withmaximum decrease in the description length5.
S , - - - -SuD\ ] -{ \ ]} ,  G~- - -G-D\ ]6.
Compute ps using IIS Algorithm7.
Go to step 24.4 Select ing a Mode l  w i th  Approx-imate ly  Opt ima l  Subcategor i zat ionPre ference  AccuracySuppose that we are constructing subcategoriza-tion preference models for the verbs Vl,...,Vm.By the general-to-specific eature selection algo-rithm in the previous section, for each verb vi,a totally ordered sequence of ni active featuresets Si0,...  ,'-"?ini (i.e., subcategorization prefer-ence models) are obtained from the training sam-ple g. Then, using another training sample C ~which is different from C and consists of positiveas well as negative data, a model with optimalsubcategorization preference accuracy is approx-imately selected by the following procedure.
Let~ , .
.
.
,  7-m denote the current sets of active fea-tures for verbs Vl , .
.
.
,  Vm, respectively:1.
Initially, for each verb vi, set ~ as the most gen-eral one `sis of the sequence `sio,...,  `sire.2.
For each verb vi, from the sequence `sn, .
.
.
,  `sire,search for an active feature set which gives amaximum subcategorization preference accuracyfor g~, then set Ti as it.3.
Repeat he same procedure as 2.4.
Return the current sets ~, .
.
.
,  7-m as the approx-imately optimal active feature sets 'S1,.--,'~r~for verbs Vl,..., vm, respectively.5Note that this feature selection algorithm is a hill-climbing one and the model selected here may have a de-scription length greater than the global minimum.6In the present implementation, the feature selectionprocess is terminated after the description length of themodel stops decreasing and then certain umber of activefeatures are replaced.5 Exper iment  and  Eva luat ion5.1 Corpus  and ThesaurusAs the training and test corpus, we used theEDR Japanese bracketed corpus (EDR, 1995),which contains about 210,000 sentences collectedfrom newspaper and magazine articles.
Weused 'Bunrui Goi Hyou'(BGH) (NLRI, 1993)as the Japanese thesaurus.
BGH has a seven-layered abstraction hierarchy and more than60,000 words are assigned at the leaves and itsnominal part contains about 45,000 words.5.2 T ra in ing /Test  Events  and FeaturesWe conduct he model earning experiment underthe following conditions: i) the noun class gener-alization level of each feature is limited to abovethe level 5 from the root node in the thesaurus,ii) since verbs are independent of each other inour model learning framework, we collect verb-noun collocations of one verb into a training dataset and conduct he model earning procedure foreach verb separately.For the experiment, seven Japanese verbs 7 areselected so that the difficulty of the subcatego-rization preference test is balanced among verbpairs.
The number of training events for eachverb varies from about 300 to 400, while thenumber of candidate features for each verb variesfrom 200 to 1,350.
From this data, we constructthe following three types of data set, each pairof which has no common element: i) the trainingdata ~: which consists of positive data only, andis used for selecting a sequence of active featuresets by the general-to-specific eature selectionalgorithm in section 4.3, ii) the training data g'which consists of positive and negative data andis used in the procedure of section 4.4, and iii) thetest data C ts which consists of positive and neg-ative data and is used for evaluating the selectedmodels in terms of the performance of subcate-gorization preference test.
The sizes of the datasets g, g', and g ts are 2,333, 2,100, and 2,100.5.3 ResultsTable 1 shows the performance of subcategoriza-tion preference t st described insection 4.2.2, forthe approximately optimal models elected by theprocedure in section 4.4 (the "Optimal" mode\]of "General-to-Specific" method), as well as forseveral other models including baseline models.Coverage is the rate of test instances which sat-isfy the case covering constraint of section 4.1.Accuracy is measured with the following heuris-tics: i) verb-noun collocations which satisfy ther"Agaru (rise)", "kau (buy)", "motoduku (base)","oujiru (respond)", "sumu (live)", "tigau (differ)", and"tsunagaru (connect)".1319Table 1: Comparison of Coverage and Accuracyof Optimal and Other Models (%)General-to-Specific(Initial)(Independent Cases)(General Classes)(Optimal)(MDL)One-by-one Feature Adding(Optimal)Coverage84.884.877.575.415.960.8Accuracy81.382.279.587.170.579.0case covering constraint are preferred, it) eventhose verb-noun collocations which do not satisfythe case covering constraint are assigned the con-ditional probabilities in (15) by neglecting caseswhich are not covered by the model.
With theseheuristics, subcategorization preference can bejudged for all the test instances, and test set cov-erage becomes 100%.In Table 1, the "Initial" model is the oneconstructed according to the description in sec-tion 4.1, in which cases are independent of eachother and the sense restriction of each case is(one of) the most general class(es).
The "Inde-pendent Cases" model is the one obtained by re-moving all the case dependencies from the "Op-timal" model, while the "General Classes" modelis the one obtained by generalizing all the senserestriction of the "Optimal" model to the mostgeneral classes.
The "MDL" model is the onewith the minimum description length.
This isfor evaluating the effect of the MDL principle inthe task of subcategorization preference modellearning.
The "Optimal" model of "One-by-oneFeature Adding" method is the one selected fromthe sequence of one-by-one feature adding in sec-tion 3.1 by the procedure in section 4.4.The "Optimal" model of 'General-to-Specific"method performs best among all the models inTable 1.
Especially, it outperforms the "Op-timal" model of "One-by-one Feature Adding"method both in coverage and accuracy.
As forthe size of the optimal model, the average num-ber of the active feature set is 126 for "General-to-Specific" method and 800 for "One-by-oneFeature Adding" method.
Therefore, general-to-specific feature selection algorithm achieves ig-nificant improvements over the one-by-one fea-ture adding algorithm with much smaller num-ber of active features.
The "Optimal" model of"General-to-Specific" method outperforms boththe "Independent Cases" and "General Classes"models, and thus both of the case dependenciesand specific sense restriction selected by the pro-posed method have much contribution to improv-ing the performance in subcategorization prefer-1320ence test.
The "MDL" model performs worsethan the "Optimal" model, because the featuresof the "MDL" model have much more specificsense restriction than those of the "Optimal"model, and the coverage of the "MDL" modelis much lower than that of the "Optimal" model.6 Conc lus ionThis paper proposed a novel method for learn-ing probability models of subcategorization pref-erence of verbs.
Especially, we proposed a newmodel selection algorithm which starts from themost general model and gradually examines morespecific models.
In the experimental evaluation,it is shown that both of the case dependenciesand specific sense restriction selected by the pro-posed method contribute to improving the per-formance in subcategorization preference resolu-tion.
As for future works, it is important to eval-uate the performance of the learned subcatego-rization preference model in the real parsing task.Re ferencesA.
L. Berger, S. A. Della Pietra, and V. J. DellaPietra.
1996.
A Maximum Entropy Approach to Nat-ural Language Processing.
Computational Linguistics,22(1):39-71.E.
Charniak.
1997.
Statistical Parsing with a Context-free Grammar and Word Statistics.
In Proceedings ofthe 14th AAAI, pages 598-603.M.
Collins.
1996.
A New Statistical Parser Based on Bi-gram Lexical Dependencies.
In Proceedings of the 34thAnnual Meeting of ACL, pages 184-191.S.
Della Pietra, V. Della Pietra, and J. Lafferty.
1997.Inducing Features of Random Fields.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,19(4):380-393.EDR (Japan Electronic Dictionary Research Institute,Ltd.).
1995.
EDR Electronic Dictionary TechnicalGuide.H.
Li and N. Abe.
1995.
Generalizing Case Frames Usinga Thesaurus and the MDL Principle.
In Proceedings ofInternational Conference on Recent Advances in Natu-ral Language Processing, pages 239-248.H.
Li and N. Abe.
1996.
Learning Dependencies betweenCase Frame Slots.
In Proceedings of the 16th COLING,pages 10-15.D.
M. Magerman.
1995.
Statistical Decision-Tree Modelsfor Parsing.
In Proceedings of the 33rd Annual Meetingof A CL, pages 276-283.NLRI (National Language Research Institute).
1993.Word List by Semantic Principles.
Syuei Syuppan.
(inJapanese).P.
Resnik.
1993.
Semantic Classes and Syntactic Ambigu-ity.
In Proceedings of the Human Language TechnologyWorkshop, pages 278-283.J.
Rissanen.
1984.
Universal Coding, Information, Pre-diction, and Estimation.
IEEE Transactions on Infor-mation Theory, IT-30(4):629-636.T.
Utsuro and Y. Matsumoto.
1997.
Learning Probabilis-tic Subcategorization Preference by Identifying CaseDependencies and Optimal Noun Class GeneralizationLevel.
In Proceedings of the 5th ANLP, pages 364-371.
