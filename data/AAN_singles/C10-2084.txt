Coling 2010: Poster Volume, pages 730?738,Beijing, August 2010Improved Discriminative ITG Alignment usingHierarchical Phrase Pairs and Semi-supervised Training?Shujie Liu*, ?Chi-Ho Li and ?Ming Zhou?
School of Computer Science and TechnologyHarbin Institute of Technologyshujieliu@mtlab.hit.edu.cn?Microsoft Research Asia{chl, mingzhou}@microsoft.comAbstractWhile ITG has many desirable propertiesfor word alignment, it still suffers fromthe limitation of one-to-one matching.While existing approaches relax this li-mitation using phrase pairs, we propose aITG formalism, which even handles unitsof non-contiguous words, using bothsimple and hierarchical phrase pairs.
Wealso propose a parameter estimation me-thod, which combines the merits of bothsupervised and unsupervised learning,for the ITG formalism.
The ITG align-ment system achieves significant im-provement in both word alignment quali-ty and translation performance.1 IntroductionInversion transduction grammar (ITG) (Wu,1997) is an adaptation of CFG to bilingualparsing.
It does synchronous parsing of twolanguages with phrasal and word-level alignmentas by-product.
One of the merits of ITG is that itis less biased towards short-distance reorderingcompared with other word alignment modelssuch as HMM.
For this reason ITG has gainedmore and more attention recently in the wordalignment community (Zhang et al, 2005;Cherry et al, 2006; Haghighi et al, 2009)1.The basic ITG formalism suffers from the ma-jor drawback of one-to-one matching.
This limi-tation renders ITG unable to produce certainalignment patterns (such as many-to-many* This work has been done while the first author was visit-ing Microsoft Research Asia.alignment for idiomatic expression).
For thisreason those recent approaches to ITG alignmentintroduce the notion of phrase (or block), de-fined as sequence of contiguous words, into theITG formalism (Cherry and Lin, 2007; Haghighiet al, 2009; Zhang et al, 2008).
However, thereare still alignment patterns which cannot be cap-tured by phrases.
A simple example is connec-tive in Chinese/English.
In English, two clausesare connected by merely one connective (like"although", "because") but in Chinese we needtwo connectives (e.g.
There is a sentence pattern"??
??
?
although   ", whereand     are variables for clauses).
The Englishconnective should then be aligned to two non-contiguous Chinese connectives, and suchalignment pattern is not available in either word-level or phrase-level ITG.
As hierarchicalphrase-based SMT (Chiang, 2007) is proved tobe superior to simple phrase-based SMT, it isnatural to ask, why don?t we further incorporatehierarchical phrase pairs (henceforth h-phrasepairs) into ITG?
In this paper we propose a ITGformalism and parsing algorithm using h-phrasepairs.The ITG model involves much more parame-ters.
On the one hand, each phrase/h-phrase pairhas its own probability or score.
It is not feasibleto learn these parameters through discrimina-tive/supervised learning since the repertoire ofphrase pairs is much larger than the size of hu-man-annotated alignment set.
On the other hand,there are also a few useful features which cannotbe estimated merely by unsupervised learninglike EM.
Inspired by Fraser et al (2006), wepropose a semi-supervised learning algorithmwhich combines the merits of both discrimina-730tive training (error minimization) and approx-imate EM (estimation of numerous parameters).The ITG model augmented with the learningalgorithm is shown by experiment results to im-prove significantly both alignment quality andtranslation performance.In the following, we will explain, step-by-step,how to incorporate hierarchical phrase pairs intothe ITG formalism (Section 2) and in ITG pars-ing (Section 3).
The semi-supervised trainingmethod is elaborated in Section 4.
The merits ofthe complete system are illustrated with the ex-periments described in Section 5.2 ITG Formalisms2.1 W-ITG : ITG with only word pairsThe simplest formulation of ITG contains threetypes of rules: terminal unary rules  ?
,where e and f represent words (possibly a nullword, ?)
in the English and foreign languagerespectively, and the binary rules  ?
and?
, which refer to that the componentEnglish and foreign phrases are combined in thesame and inverted order respectively.
From theviewpoint of word alignment, the terminal unaryrules provide the links of word pairs, whereasthe binary rules represent the reordering factor.Note also that the alignment between two phrasepairs is always composed of the alignmentbetween word pairs (c.f.
Figure 1(a) and (b)).The Figure 1 also shows ITG can handle thecases where two languages share the same(Figure 1(a)) and different (Figure 1(b)) wordorder??
XX,?X  (b)]f,e[?X  (c) f1Xf3][e1Xe3,?X  (d)X][X,?X  ( )e2e1f1 f2 f1 f2e2e1e2e1f1 f2e2e1f1 f2 f3e3Figure 1.
Four ways in which ITG can analyze amulti-word span pair.Such a formulation has two drawbacks.
Firstof all, the simple ITG leads to redundancy ifword alignment is the sole purpose of applyingITG.
For instance, there are two parses for threeconsecutive word pairs, viz.and                   .
The problem of re-dundancy is fixed by adopting ITG normal form.The ITG normal form grammar as used in thispaper is described in Appendix A.The second drawback is that ITG fails toproduce certain alignment patterns.
Its constraintthat a word is not allowed to align to more thanone word is indeed a strong limitation as noidiom or multi-word expression is allowed toalign to a single word on the other side.Moreover, its reordering constraint makes itunable to produce the ?inside-out?
alignmentpattern (c.f.
Figure 2).f1      f2      f3      f4e1     e2      e3      e4Figure 2.
An example of inside-out alignment.2.2 P-ITG : ITG with Phrase PairsA single word in one language is not always on apar with a single word in another language.
Forexample, the Chinese word "??"
is equivalentto two words in English ("white house").
Thisproblem is even worsened by segmentation er-rors (i.e.
splitting a single word into more thanone word).
The one-to-one constraint in W-ITGis a serious limitation as in reality there are al-ways segmentation or tokenization errors as wellas idiomatic expressions.
Therefore, researcheslike Cherry and Lin (2007), Haghighi et al(2009) and Zhang et al (2009) tackle this prob-lem by enriching ITG, in addition to word pairs,with pairs of phrases (or blocks).
That is, a se-quence of source language word can be aligned,as a whole, to one (or a sequence of more thanone) target language word.These methods can be subsumed under theterm phrase-based ITG (P-ITG), which enhancesW-ITG by altering the definition of a terminalproduction to include phrases:   ?
(c.f.Figure 1(c)).
stands for English phrase andstands for foreign phrase.
As an example, ifthere is a simple phrase pair <white house, ?731?>, then it is transformed into the ITG rule?
white house   ??
.An important question is how these phrasepairs can be formulated.
Marcu and Wong (2002)propose a joint probability model which searchesthe phrase alignment space, simultaneouslylearning translations lexicons for words andphrases without consideration of potentially sub-optimal word alignments and heuristic for phraseextraction.
This method suffers from computa-tional complexity because it considers all possi-ble phrases and all their possible alignments.Birch et al (2006) propose a better and moreefficient method of constraining the search spacewhich does not contradict a given high confi-dence word alignment for each sentence.
Our P-ITG collects all phrase pairs which are consistentwith a word alignment matrix produced by asimpler word alignment model.2.3 HP-ITG : P-ITG with H-Phrase pairsP-ITG is the first enhancement of ITG to capturethe linguistic phenomenon that more than oneword of a language may function as a single unit,so that these words should be aligned to a singleunit of another language.
But P-ITG can onlytreat contiguous words as a single unit, andtherefore cannot handle the single units of non-contiguous words.
Apart from sentenceconnectives as mentioned in Section 1, there isalso the example that the single word ?since?
inEnglish corresponds to two non-adjacent words "?"
and "??"
as shown the following sentencepair:?
???
??
?
?
??
?
??
.I have been ill since last weekend .No matter whether it is P-ITG or phrase-basedSMT, the very notion of phrase pair is not help-ful because this example is simply handled byenumerating all possible contiguous sequencesinvolving the words "?"
and "??
", and thussubject to serious data sparseness.
The lessonlearned from hierarchical phrase-based SMT isthat the modeling of non-contiguous word se-quence can be very simple if we allow rules in-volving h-phrase pairs, like:?
since    ?
?
?where   is a placeholder for substituting aphrase pair like "??
?/last weekend".H-phrase pairs can also perform reordering, asillustrated by the well-known example fromChiang (2007),  ?
have    with        ??
, for the following bilingual sentencefragment:?
??
?
?
?have diplomatic relations with North KoreaThe potential of intra-phrase reordering may alsohelp us to capture those alignment patterns likethe ?inside-out?
pattern.All these merits of h-phrase pairs motivate aITG formalism, viz.
hierarchical phrase-basedITG (HP-ITG), which employs not only simplephrase pairs but also hierarchical ones.
The ITGgrammar is enriched with rules of the format:?
where   and    refer to either a phraseor h-phrase (c.f.
Figure 1(d)) pair in English andforeign language respectively 2 .
Note that, al-though the format of HP-ITG is similar to P-ITG,it is much more difficult to handle rules with h-phrase pairs in ITG parsing, which will be elabo-rated in the next section.It is again an important question how to for-mulate the h-phrase pairs.
Similar to P-ITG, theh-phrase pairs are obtained by extracting the h-phrase pairs which are consistent with a wordalignment matrix produced by some simplerword alignment model.3 ITG ParsingBased on the rules, W-ITG word alignment isdone in a similar way to chart parsing (Wu,1997).
The base step applies all relevant terminalunary rules to establish the links of word pairs.The word pairs are then combined into spanpairs in all possible ways.
Larger and larger spanpairs are recursively built until the sentence pairis built.Figure 3(a) shows one possible derivation fora toy example sentence pair with three words ineach sentence.
Each node (rectangle) representsa pair, marked with certain phrase category, of2 Haghighi et al (2009) impose some rules which look likeh-phrase pairs, but their rules are essentially h-phrase pairswith at most one ?
?
only, added with the constraint thateach ?
?
covers only one word.732foreign span (F-span) and English span (E-span)(the upper half of the rectangle) and the asso-ciated alignment hypothesis (the lower half).Each graph like Figure 3(a) shows only one de-rivation and also only one alignment hypothesis.The various derivations in ITG parsing can becompactly represented in hypergraph (Klein etal., 2001) like Figure 3(b).
Each hypernode (rec-tangle) comprises both a span pair (upper half)and the list of possible alignment hypotheses(lower half) for that span pair.
The hyperedgesshow how larger span pairs are derived fromsmaller span pairs.
Note that hypernode mayhave more than one alignment hypothesis, sincea hypernode may be derived through more thanone hyperedge (e.g.
the topmost hypernode inFigure 3(b)).
Due to the use of normal form, thehypotheses of a span pair are different from eachother.In the case of P-ITG parsing, each span pairdoes not only examine all possible combinationsof sub-span pairs using binary rules, but alsochecks if the yield of that span pair is exactly thesame as that phrase pair.
If so, then this span pairis treated as a valid leaf node in the parse tree.Moreover, in order to enable the parse tree pro-duce a complete word aligned matrix as by-product, the alignment links within the phrasepair (which are recorded when the phrase pair isextracted from a word aligned matrix producedby a simpler model) are taken as an alternativealignment hypothesis of that span pair.In the case of HP-ITG parsing, an ITG rulelike  ?
have    with      ?
?
(ori-ginated from the hierarchical rule like  ?
<??
, have    with   >), is processed in thefollowing manner: 1) Each span pair checks if itcontains the lexical anchors: "have", "with","?
"and "?
"; 2) each span pair checks if the remain-ing words in its yield can form two sub-spanpairs which fit the reordering constraint amongand    (Note that span pairs of any categoryin the ITG normal form grammar can substitutefor    or   ).
3) If both conditions hold, then thespan pair is assigned an alignment hypothesiswhich combines the alignment links among thelexical anchors and those links among the sub-span pairs.C:[e3,e3]/[f3,f3]{e3/f3}C:[e1,e2]/[f1,f2]{e1/f2,e1/f1,e2/f1,e2/f2}A:[e1,e3]/[f1,f3]{e1/f2,e1/f1,e2/f1,e2/f2,e3/f3} ,{e1/f1,e1/f3,e3/f1,e3/f3,e2,f2}{e2/f2}e1Xe3/f1Xf3:[e1Xe3]/[f1Xf3]{e1/f3,e1/f1,e3/f3,e3/f1}C:[e2,e2]/[f2,f2](c)e1               e2              e3f1                f2               f3(a) (b)e1               e2              e3f1                f2               f3A?
[C,C] A?
[e1Xe3/f1Xf3,C]Figure 4.
Phrase/h-phrase in hypergraph.Figure 4(c) shows an example how to usephrase pair and h-phrase pairs in hypergraph.Figure 4(a) and  Figure 4(b) refer to alignmentmatrixes which cannot be generated by W-ITG,because of the one-to-one assumption.
Figure4(c) shows how the span pair [e1,e3]/[f1,f3] canbe generated in two ways: one is combining aphrase pair and a word pair directly, and the oth-er way is replacing the X in the h-phrase pairwith a word pair.
Here we only show how h-phrase pairs with one variable be used during theB:[e1,e2]/[f1,f2]{e1/f2,e2/f1}C:[e1,e1]/[f2,f2]{e1/f2}C:[e2,e2]/[f1,f1]{e2/f1}C:[e3,e3]/[f3,f3]{e3/f3}A:[e1,e3]/[f1,f3]{e1/f2,e2/f1,e3/f3}(a)C:[e2,e2]/[f2,f2]{e2/f2}C:[e1,e1]/[f1,f1]{e1/f1}C:[e3,e3]/[f3,f3]{e3/f3}C:[e2,e2]/[f1,f1]{e2/f1}C:[e1,e1]/[f2,f2]{e1/f2}B:[e1,e2]/[f1,f2]{e1/f2}A:[e1,e2]/[f1,f2]{e2/f2}A:[e1,e3]/[f1,f3]{e1/f2,e2/f1,e3/f3} ,{e1/f1,e2/f2,e3,f3}(b)B?<C,C> A?[C,C]A?[A,C]A?
[B,C]Figure 3.
Example ITG parses in graph (a) and hypergraph (b).733parsing, and h-phrase pairs with more than onevariable can be used in a similar way.The original (unsupervised) ITG algorithmhas complexity of O(n6).
When extended to su-pervised/discriminative framework, ITG runseven more slowly.
Therefore all attempts to ITGalignment come with some pruning method.Zhang and Gildea (2005) show that Model 1(Brown et al, 1993) probabilities of the wordpairs inside and outside a span pair are useful.Tic-tac-toe pruning algorithm (Zhang and Gildea,2005) uses dynamic programming to computeinside and outside scores for a span pair in O(n4).Tic-tac-toe pruning method is adopted in thispaper.4 Semi-supervised TrainingThe original formulation of ITG (W-ITG) is agenerative model in which the ITG tree of a sen-tence pair is produced by a set of rules.
The pa-rameters of these rules are trained by EM.
Cer-tainly it is difficult to add more non-independentfeatures in such a generative model, and there-fore Cherry et al (2006) and Haghighi et al(2009) used a discriminative model to incorpo-rate features to achieve state-of-art alignmentperformance.4.1 HP-DITG : Discriminative HP-ITGWe also use a discriminative model to assignscore to an alignment candidate for a sentencepair (     ) as probability from a log-linear model(Liu et al, 2005; Moore, 2006):(1)where each           is some feature about thealignment matrix, and each ?
is the weight of thecorresponding feature.
The discriminativeversion of W-ITG, P-ITG, and HP-ITG are thencalled W-DITG, P-DITG, and HP-DITGrespectively.There are two kinds of parameters in (1) to belearned.
The first is the values of the features ?.Most features are indeed about the probabilitiesof the phrase/h-phrase pairs and there are toomany of them to be trained from a labeled dataset of limited size.
Thus the feature values aretrained by approximate EM.
The other kind ofparameters is feature weights ?, which aretrained by an error minimization method.
Thediscriminative training of ?
and the approximateEM training of ?
are integrated into a semi-supervised training framework similar to EMD3(Fraser and Marcu, 2006).4.2 Discriminative Training of ?MERT (Och, 2003) is used to train featureweights ?.
MERT estimates model parameterswith the objective of minimizing certain measureof translation errors (or maximizing certainperformance measure of translation quality) for adevelopment corpus.
Given an SMT systemwhich produces, with model parameters, theK-best candidate translationsfor asource sentence   , and an error measureof a particular candidate      withrespect to the reference translation   , theoptimal parameter values will be:MERT for DITG applies the same equationfor parameter tuning, with different interpreta-tion of the components in the equation.
Insteadof a development corpus with reference transla-tions, we have a collection of training samples,each of which is a sentence pair with annotatedalignment result.
The ITG parser outputs foreach sentence pair a K-best list of alignment re-sultbased on the current parametervalues.
The MERT module for DITG takesalignment F-score of a sentence pair as the per-formance measure.
Given an input sentence pairand the reference annotated alignment, MERTaims to maximize the F-score of DITG-producedalignment.4.3 Approximate EM Training of ?Three kinds of features (introduced in section4.5 and 4.6) are calculated from training corpusgiven some initial alignment result: conditionalprobability of word pairs and two types ofconditional probabilities for phrase/h-phrase.3 For simplicity, we will also call our semi-supervisedframework as EMD.734The initial alignment result is far from perfectand so the feature values thus obtained are notoptimized.
There are too many features to betrained in supervised way.
So, unsupervisedtraining like EM is the best solution.When EM is applied to our model, the E-stepcorresponds to calculating the probability for allthe ITG trees, and the M-step corresponds to re-estimate the feature values.
As it is intractable tohandle all possible ITG trees, instead we use theViterbi parse to update the feature values.
Inother words, the training is a kind of approx-imate EM rather than EM.Word pairs are collected over Viterbi align-ment and their conditional probabilities are esti-mated by MLE.
As to phrase/h-phrase, if theyare handled in a similar way, then there will bedata sparseness (as there are much fewerphrase/h-phrase pairs in Viterbi parse tree thanneeded for reliable parameter estimation).
Thus,we collect all phrase/h-phrase pairs which areconsistent with the alignment links.
The condi-tional probabilities are then estimated by MLE.4.4 Semi-supervised trainingAlgorithm EMD (semi-supervised training)input development data dev, test data test, trainingdata with initial alignment (train, align_train)output feature weights   and features .1: estimate initial features    with (train, align_train)2: get an initial weights    by MERT with the initialfeatures   on dev.3: get the F-Measure    for          on test.4: for( =1;;  ++)5:  get the Viterbi alignment align_train for trainusing      and6:  estimate    with (train, align_train)7:  get new feature weights    by MERT withon dev.8:  get the F-Measure    for          on test.9:  if             then10:   break11: end for12: return      andFigure 5.
Semi-supervised training for HP-DITG.The discriminative training (error minimiza-tion) of feature weights   and the approximateEM learning of feature values  are integrated ina single semi-supervised framework.
Given aninitial estimation of  (estimated from an initialalignment matrix by some simpler word align-ment model) and an initial estimation of  , thediscriminative training process and the approx-imate EM learning process are alternatively ite-rated until there is no more improvement.
Thesketch of the semi-supervised training is shownin Figure 5.4.5 Features for word pairsThe following features about alignment link areused in W-DITG:1) Word pair translation probabilitiestrained from HMM model (Vogel et al,1996) and IBM model 4 (Brown et al,1993).2) Conditional link probability (Moore,2006).3) Association score rank features (Moore etal., 2006).4) Distortion features: counts of inversionand concatenation.4.6 Features for phrase/h-phrase pairsFor our HP-DITG model, the rule probabilitiesin both English-to-foreign and foreign-to-English directions are estimated and taken asfeatures, in addition to those features in W-DITG, in the discriminative model of alignmenthypothesis selection:1)           : The conditional probability ofEnglish phrase/h-phrase given foreignphrase/h-phrase.2)           : The conditional probability offoreign phrase/h-phrase given Englishphrase/h-phrase.The features are calculated as described insection 4.3.5 EvaluationOur experiments evaluate the performance ofHP-DITG in both word alignment and transla-tion in a Chinese-English setting, taking GI-ZA++, BerkeleyAligner (henceforth BERK)(Haghighi, et al, 2009), W-ITG as baselines.Word alignment quality is evaluated by recall,precision, and F-measure, while translation per-formance is evaluated by case-insensitiveBLEU4.5.1 Experiment DataThe small human annotated alignment set fordiscriminative training of feature weights is thesame as that in Haghighi et al (2009).
The 491735sentence pairs in this dataset are adapted to ourown Chinese word segmentation standard.
250sentence pairs are used as training data and theother 241 are test data.
The large, un-annotatedbilingual corpus for approximate EM learning offeature values is FBIS, which is also the trainingset for our SMT systems.In SMT experiments, our 5-gram languagemodel is trained from the Xinhua section of theGigaword corpus.
The NIST?03 test set is usedas our development corpus and the NIST?05 andNIST?08 test sets are our test sets.
We use twokinds of state-of-the-art SMT systems.
One is aphrase-based decoder (PBSMT) with a MaxEnt-based distortion model (Xiong, et al, 2006), andthe other is an implementation of hierarchicalphrase-based model (HPBSMT) (Chiang, 2007).The phrase/rule table for these two systems isnot generated from the terminal node of HP-DITG tree directly, but extracted from wordalignment matrix (HP-DITG generated) usingthe same criterion as most phrase-based systems(Chiang, 2007).5.2 HP-DITG without EMDOur first experiment isolates the contribution ofthe various DITG alignment models from that ofsemi-supervised training.
The feature values ofthe DITG models are estimated simply fromIBM Model 4 using GIZA++.
Apart from DITG,P-ITG, and HP-ITG as introduced in Section 2,we also include a variation, known as H-DITG,which covers h-phrase pairs but no simplephrase pairs at all.
The experiment results areshown in Table 1.Precision Recall F-MeasureGIZA++ 0.826 0.807 0.816BERK 0.917 0.814 0.862W-DITG 0.912 0.745 0.820P-DITG 0.913 0.788 0.846H-DITG 0.913 0.781 0.842HP-DITG 0.915 0.795 0.851Table 1.
Performance gains with features forHP-DITG.It is obvious that any form of ITG achievesbetter F-Measure than GIZA++.
Without semi-supervised training, however, our various DITGmodels cannot compete with BERK.
Among theDITG models, it can be seen that precision isroughly the same in all cases, while W-ITG hasthe lowest recall, due to the limitation of one-to-one matching.
The improvement by (simple)phrase pairs is roughly the same as that by h-phrase pairs.
And it is not surprising that thecombination of both kinds of phrases achieve thebest result.Even HP-DITG does not achieve as high recallas BERK, it does produce promising alignmentpatterns that BERK fails to produce.
For in-stance, for the following sentence pair:?
???
??
?
?
??
?
??
.I have been ill since last weekend .Both GIZA++ and BERK produce the patternin Figure 6(a), while HP-DITG produces the bet-ter pattern in Figure 6(b) as it learns the h-phrasepair  since     ?
??
.
(b): HP-DITG?
???
?
?since          last       weekend?
???
?
?since          last       weekend(a): BERK/Giza++Figure 6.
Partial alignment results.5.3 Alignment Quality of HP-DITG withEMDPrecision Recall F- MeasureGIZA++ 0.826 0.807 0.816BERK 0.917 0.814 0.862EMD0 0.915 0.795 0.851EMD1 0.923 0.814 0.865EMD2 0.930 0.821 0.872EMD3 0.935 0.819 0.873Table 2.
Semi-supervised Training Task on F-Measure.The second experiment evaluates how thesemi-supervised method of EMD improves HP-DITG with respect to word alignment quality.The results are shown in Table 2.
In the table,EMD0 refers to the HP-DITG model before anyEMD training; EMD1 refers to the model afterthe first iteration of training, and so on.
It is em-pirically found that F-Measure is not improvedafter the third EMD iteration.It can be observed that EMD succeeds to helpHP-DITG improves feature value and weightestimation iteratively.
When semi-supervised736training converges, the new HP-DITG model isbetter than before training by 2%, and better thanBERK by 1%.5.4 Translation Quality of HP-DITG withEMDThe third experiment evaluates the samealignment models in the last experiment but withrespect to translation quality, measured by case-insensitive BLEU4.
The results are shown inTable 3.
Note that the differences betweenEMD3 and the two baselines are statisticallysignificant.PBSMT HPBSMT05 08 05 08GIZA++ 33.43 23.89 33.59 24.39BERK 33.76 24.92 34.22 25.18EMD0 34.02 24.50 34.30 24.90EMD1 34.29 24.80 34.77 25.25EMD2 34.25 25.01 35.04 25.43EMD3 34.42 25.19 34.82 25.56Table 3.
Semi-supervised Training Task onBLEU.It can be observed that EMD improves SMTperformance in most iterations in most cases.EMD does not always improve BLEU score be-cause the objective function of the discrimina-tive training in EMD is about alignment F-Measure rather than BLEU.
And it is wellknown that the correlation between F-Measureand BLEU (Fraser and Marcu, 2007) is itself anintriguing problem.The best HP-DITG leads to more than 1BLEU point gain compared with GIZA++ on alldatasets/MT models.
Compared with BERK,EMD3 improves SMT performance significantlyon NIST05 and slightly on NIST08.6 Conclusion and Future WorkIn this paper, we propose an ITG formalismwhich employs the notion of phrase/h-phrasepairs, in order to remove the limitation of one-to-one matching.
The formalism is proved to enablean alignment model to capture the linguistic factthat a single concept is expressed in several non-contiguous words.
Based on the formalism, wealso propose a semi-supervised training methodto optimize feature values and feature weights,which does not only improve the alignment qual-ity but also machine translation performancesignificantly.
Combining the formalism andsemi-supervised training, we obtain betteralignment and translation than the baselines ofGIZA++ and BERK.A fundamental problem of our current frame-work is that we fail to obtain monotonic incre-ment of BLEU score during the course of semi-supervised training.
In the future, therefore, wewill try to take the BLEU score as our objectivefunction in discriminative training.
That is tocertain extent inspired by Deng et al (2008).Appendix A.
The Normal Form GrammarTable 4 lists the ITG rules in normal form asused in this paper, which extend the normal formin Wu (1997) so as to handle the case ofalignment to null.1     ?2     ?3     ?
?4     ?5     ?6    ?7     ?
?8    ?
?9    ?
?Table 4.
ITG Rules in Normal Form.In these rules,   is the Start symbol;   is thecategory for concatenating combination whereasfor inverted combination.
Rules (2) and (3) areinherited from Wu (1997).
Rules (4) divide theterminal category   into subcategories.
Ruleschema (6) subsumes all terminal unary rules forsome English word   and foreign word  , andrule schemas (7) are unary rules for alignment tonull.
Rules (8) ensure all words linked to null arecombined in left branching manner, while rules(9) ensure those words linked to null combinewith some following, rather than preceding,word pair.
(Note: Accordingly, all sentencesmust be ended by a special token      , other-wise the last word(s) of a sentence cannot belinked to null.)
If there are both English and for-eign words linked to null, rule (5) ensures thatthose English words linked to null precede thoseforeign words linked to null.737ReferencesBirch, Alexandra, Chris Callison-Burch, Miles Os-borne and Phillipp Koehn.
2006.
Constraining thePhrase-Based, Joint Probability Statistical Transla-tion Model.
Proceedings of the Workshop on Sta-tistical Machine Translation.Brown, Peter F. Brown, Stephen A. Della Pietra,Vincent J. Della Peitra, Robert L. Mercer.
1993.The Mathematics of Statistical Machine Transla-tion: Parameter Estimation.
Computational Lin-guistics, 19(2):263-311.Cherry, Colin and Dekang Lin.
2006.
Soft SyntacticConstraints for Word Alignment through Discri-minative Training.
Proceedings of the 21st Interna-tional Conference on Computational Linguisticsand 44th Annual Meeting of the Association forComputational Linguistics.Cherry, Colin and Dekang Lin.
2007.
InversionTransduction Grammar for Joint Phrasal Transla-tion Modeling.
Proceedings of the Second Work-shop on Syntax and Structure in Statistical Trans-lation, Pages:17-24.Chiang, David.
2007.
Hierarchical Phrase-basedTranslation.
Computational Linguistics, 33(2).Deng, Yonggang, Jia Xu and Yuqing Gao.
2008.Phrase Table Training For Precision and Recall:What Makes a Good Phrase and a Good PhrasePair?.
Proceedings of the 7th International Confe-rence on Human Language Technology Researchand 46th Annual Meeting of the Association forComputational Linguistics, Pages:1017-1026.Fraser, Alexander, Daniel Marcu.
2006.
Semi-Supervised Training for StatisticalWord Align-ment.
Proceedings of the 21st International Confe-rence on Computational Linguistics and 44th An-nual Meeting of the Association for ComputationalLinguistics, Pages:769-776.Fraser, Alexander, Daniel Marcu.
2007.
MeasuringWord Alignment Quality for Statistical MachineTranslation.
Computational Linguistics, 33(3).Haghighi, Aria, John Blitzer, John DeNero, and DanKlein.
2009.
Better Word Alignments with Super-vised ITG Models.
Proceedings of the Joint Confe-rence of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on NaturalLanguage, Pages: 923-931.Klein, Dan and Christopher D. Manning.
2001.
Pars-ing and Hypergraphs.
Proceedings of the 7th In-ternational Workshop on Parsing Technologies,Pages:17-19Liu, Yang, Qun Liu and Shouxun Lin.
2005.
Log-linear models for word alignment.
Proceedings ofthe 43rd Annual Meeting of the Association forComputational Linguistics, Pages: 81-88.Marcu, Daniel, William Wong.
2002.
A Phrase-Based,Joint Probability Model for Statistical MachineTranslation.
Proceedings of 2002 Conference onEmpirical Methods in Natural LanguageProcessing, Pages:133-139.Moore, Robert, Wen-tau Yih, and Andreas Bode.2006.
Improved Discriminative Bilingual WordAlignment.
Proceedings of the 44rd Annual Meet-ing of the Association for Computational Linguis-tics, Pages: 513-520.Och, Franz Josef.
2003.
Minimum error rate trainingin statistical machine translation.
Proceedings ofthe 41rd Annual Meeting of the Association forComputational Linguistics, Pages:160-167.Och, Franz Josef and Hermann Ney.
2004.
TheAlignment Template Approach to Statistical Ma-chine Translation.
Computational Linguistics,30(4) : 417-449.Vogel, Stephan, Hermann Ney, and Christoph Till-mann.
1996.
HMM-based word alignment in sta-tistical translation.
Proceedings of 16th Interna-tional Conference on Computational Linguistics,Pages: 836-841.Wu, Dekai.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Cor-pora.
Computational Linguistics, 23(3).Xiong, Deyi, Qun Liu and Shouxun Lin.
2006.
Max-imum entropy based phrase reordering model forstatistical machine translation.
Proceedings of the44rd Annual Meeting of the Association for Com-putational Linguistics, Pages: 521-528.Zhang, Hao and Daniel Gildea.
2005.
Stochastic Lex-icalized Inversion Transduction Grammar forAlignment.
Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics.Zhang, Hao, Chris Quirk, Robert Moore, and DanielGildea.
2008.
Bayesian learning of non-compositional phrases with synchronous parsing.Proceedings of the 46rd Annual Meeting of the As-sociation for Computational Linguistics, Pages:314-323.738
