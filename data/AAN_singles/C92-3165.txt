Interactive Speech UnderstandingIliroaki SaitoDept.
of MathemattiesKeio UniversityYokohama, 223, JAPANE-mail: hxs@nak.math.keio.ae.j  pAbst ractThis paper introduces at robust interactivemethod for speech understatnding.
The gener-atlized LR patrsing is enhanced ill this approach.Patrsing proceeds fl'om left to right correcting mi-nor errors.
When at very noisy portion is detected,the patrser skips that portion using a .fake non-terminal symbol.
The unidentified portion is re-solved by re-utterance of thatt portion which isparsed very efliciently by using the parse recordof the first utterance.
The user does not have tospeak the whole sentence again.
This method isalso catpatble of hatndling unknown words, whichis imlmrtatnt in pra.ctical systems.
1)erected un-known words earn I)e incrementatlly incorporattedinto the dictionary after the interatction with tileuser.
A pilot system has shown great elfectivenessof this atpproach.1 In t roduct ionIt has been continuously mentioned thatt somekind of latnguage knowledge is essential in good-quality speech understanding.
Until recently,however, most research has focused mainly oilword recognition atnd one of the excellent recogni-tion systems built to date is Sphinx developed byLee \[7\].
Although SI)hinx atttained atn excellentword accuracy of 96 % on at 997-word task, itssentence recognition accuracy drops slgnificatntlyclue to its use of only at stattisticaJ trigra~l gratm-i i l a l ' .There hatve been at few atttempts to integratte atspeech recognition device with a nattural languageunderstanding syste,n, ltatyes el al.
\[3\] adoptedtechnique of case fi'ame instantiation to patrse atcontinuously spoken English sentence in the formof at word lattice (a set of word catndldattes hy-pothesized by at st)eech recognition module) andproduce at frame representation f the utterance.The case frame patrsing hats been pursued by Poe-sio et al \[8\] and Giatchin et al \[2\] for instance.Meanwhile, at compiler-oriented shift-reduceLR parsing technique hats been used for speechrecognition recently due to its no-batcktrackingtatl)le-drlven ei\[iciency \[12, iII, 6\].
Becatuse theparsing proceeds from left to right pruning low-l)robatl)ility t)atrtiatl-parses, the correct parse catnnot be obtained if the parsing fails to find thecorrect path in the beginning.
Moreover, it issometimes difficult to handle tim very noisy input,esl)ecially the input with missing words.
Thus anLll.
parser sometimes yields totally incorrect butsyntactically-sound hypotheses or no hypothesesatt all.
This weakness is occasionally cited todemonstrate superiority of the pa.rsing methodnsing much simI)ler bigram or trigratm grammarsin which the re.covery in the middle of the in-put earn be done at eatse.
In this paper, we de-scribe at method of enllatncing the generalized 1,R(GLR) parsing towatrds interactive speech under-standing.Section 2 describes the enhatnced GLR parrs-lug.
Section 3 describes the rol)ustness of theparser and presents an interatctive method to re-solve the unclcatr I)ortion of the input and un-known words.
Section 4 experiments the effec-tiveness of the technique in parsing spoken sen-tences.
Finally the concluding rematrks atre givenin Section 5.2 Enhanced  GLR Pars ing  fo rSpeech  Unders tand ingIll this section, tile GI,R patrsing method is de-scribed first.
Then some techniques which en-hatnce the robustness are described.AcrEs DE COLING-92.
NANTES, 23.28 ^ ot~'r 1992 1 0 S 3 Paoc.
OF COLING-92, NAtcrras.
Aoo.
23-28, 19922.1 Background:  GLR Pars ingThe LR parsing technique was originally devel-oped for the compilers of programming languages\[1\] and has been extended for natural anguageprocessing \[11\].
The GLI\[ parsing analyzes the in-put sequence from left to right with no backtrack-ing by looking at the parsing table constructedfrom the context-flee grammar ules in advance.An example grammar and its parsing table areshown in Figure l and Figure 2 respectively.Entries "s n" in the action table (the left partof the table) indicate the action "shift one wordfrom the input 1)uffcr onto the stack and go tostate n".
Entries "r n" indicate tile action "re-duce constituents on the stack usiug rule n".
Theentry "ace" stands for the action "accept", andt)lank spaces represent "error".
"$" in the actiontable is the end-of-inl)ut symbol.
The goto table(the right part of the table) decides to which statethe parser shouhl go after a reduce action.
TheLR parsing table in Figure 2 is different fi'om reg-ular LR tables utilized by the compilers in thatthere are multiple ntries, called conflicts, on therows of state 11 and 12.
While the encounteredentry has only one aztion, parsing proceeds ex-actly the same way as the regular LR parsing.In case there are multiple actions in an entry, allthe actions are executed with the graph-structuredstack \[11\].
(1 )  S - ->  NP VP(2) S --> S PP(3 )  NP - ->  n(4) NP --> det n(5) NP --> NP PP(6) PP - -> prep NP(7) VP --> v NPFigure 1: Example CFG Rules2.2 GLR Pars ing  for E r roneous  Sen-tencesThe original GLR parsing method was not de-signed to handle ungrammatical sentences.
Thisfeature is acceptable if the domain is strictly de-fined and input sentences are correct at all times.Unfortunately, accuracy of speech recognition isnot 100%.
Common errors in speech recognitionare insertions, deletions (missing words), and sub-<Act ion Table> I <Goto Table>det n v prep $ I NP PP VP S. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.s3 s4s6s7 s6slO012345678910t l12s3 s4s3 s4r3 r3r2 r2r l  r lr5 r5 r5r4 r4 r4r6 r6,s6 r6rT,s6 r72 1ace 59 8r3111299Figure 2: GI, R Parsing Tablestitntions.
Some techniques have been developedto handle erroneous entences for the GLR pars-ing \[12, 10\].?
The action table can be looked up in apredictive way to handle a missing word.Namely, a set of possible terminal symbols{Ti} at State i can be missing word candi-dates.?
This way of using the action table is also use-ful to handle substitution and insertion er-rors.
I.e., the table can tell which part of theinput should be replaced by a specific symbolor ignored.
'\['he parser explores every possibility inparalleP.2.3 Gap- f i l l i ng  Techn iqueThe techniques described in tile previous ectioncan not handle such a big noise as two consecutivemissing words.
To cope with this, the gap-fillingtechnique \[9\] is presented here.In tile gap-filling GLR parsing, the goto tableis consulted just the same way as the action table,in addition to its regular usage.
Namely, at statesi which is expecting shift action(s), the parseralso consults the gore table.
If an entry m ex-ists along the row of state sl under the columnlie practice, pruning is incorporated to reduce searchby using the likelihood attached to each word in the speechhy potheses,ACIT~ DE COLING-92.
NANTES.
23-28 ^OI~T 1992 1 0 5 4 PROC.
OF COL1NG-92.
NAN'r~s, AU6.23-28, 1992labeled with nontel'nlinM 1), the parser shifts Donto the stack an(l goes to state m. Note that no .
:zINP\] -~input is scanned when this action is performed.
..~):\]~-+~When the input is in<:omplete, the parser pro 0%\ " , .
wo cut Bad duces hyl)otheses with a fake nonterminal  at tile +"+ ~n .+~v".
adjnoisy position.
::, ...." +"....We show an example of l)arsing an incorrectly NO 2 Wp \] -B INP\] ~2recognized sentence "we cut sad with a kuife" us-ing the grammar in I:igure 12 and the LI?
tablein Figure 2. :~ At the initial state 0, the got() ta-Ill( +, tells that the nonterminals NP and S can I>eshifte(1 using the gap-fi l l ing technique.
Althoughthe first wor<t "we" (noun) is expected at state0, these fake+ nonterminals are ere+areal (\]"igure 3)in ca+se "we" is an incorrectly recognized word.Tile new states for the fake tlonterminals NP andS are 2 and 1, resi>ectively, q'he goto table tellsthat fake nonterminals PP and VP can be place(\[at state 2.
In this case, however, we do not createthese  nonter l l t ina l s~ l )eeause  two fake l / o I l te l ' l l t i -nals r+u'ely need to I>e I)\[a(:e<\] adjacently in prac|ice.
No further fake nonterminal  is att, a.ched to ii itile fake nonterminal S for the same reason.
~!
v+,/\[NP) 2O%'",, we OOl had with a knilon v ad i prop el l  nl:igure 3: I'arse TraceIu parsing the third word "sad", a fake nonter-minal \[NP\] to word "cut" keeps the correct path(Figure ,1).l 'arsing continues in this way and the linal situ-ation is shown in Figure 5.
As a result, the parsertinds two snccessfifl parses:(n  (v  ( \ [NP \ ]  (p rep  (det  n ) ) ) ) )( (n  (v \ [NP \ ] ) )  (p rep  (de |  n ) ) )Namely, the \])arser Jinds <rot that  the thirdword is incorrect and must be the word(s) in NPcategory.2'J'he terminal symbols of this grammar are grammati-cal category names called prcterminals.
A lexicon shouldbe prepared to map all actual word to its pretcrmina\].
:~'l'hc techniques in the previous ection arc enough forparsing this erroneous e/dencc.
We use this eXaml>le onlyfor describing Ihe gal~ |iliing techJ,illue.with a knMepe~ ~1 nFigure 4: Parse Trace (cont'd)+,"\[NPl~";',.
~ eel ~?+ wi~h / .
'\ n ," v ~-, ad I .,' I)t ~+*;~ ", dol~i!
".. \ - -  k~!!!
\' VPk.mn ioi1S.r",,,"1ppFigure 5: Parse Trace (conq)lete)3 Interactive Speech Under-standingIn this section, the rot)tLstness <)f tlw ( ;LR parserwith various error-recovery techniques (esl)ecia.llythe gap-fi l l ing te(:htdque) aga.inst a noisy input isdescribed.
Then an interactive way to resolve theunidentif ied portion is I(reseld.ed.3.1  Reso lv ing  Un ident i f ied  Por t ionThe gap-fi l l ing teehniqtm enhances the robustnessof the (HAl parsing in handl ing a noisy int)ut asfolk>ws:?
A fake nonterminals fills big missing con-st i tuents of the input which would yiehl nohylmtheses without the gap-.tilling func+tion.
* The gap+filling fiHtction enables an LR parserto perform reduce actions only when the ac-tion creates a definite high-score nontermi-hal.
The fake nonterminal is likely to I)e ci-t l l e l  +111 il lSel'ti<')i i  o f  a l l  t l l lk l lo~,vi i  word ,ACI'ES DE COLING-92, NANTES, 23-28 AO(ff 1992 1 0 5 5 PROC.
Ol: COLING-92, NANTES.
AUG. 2,3+28, 1992A gap filled with a fake nonterminal can beresolved by reanalysis of the input under theconstraint that that portion of the input shouldyield the specific nonterminal.
This top-down re-analysis would be effective against he genuinelybottom-up GLR parsing.
In practice, however, amore reliable way is to ask the user to speak onlythe missed portion.
In the previous example, onlythe portion of \[NP\] shouhl ie st)oken again.The parser can analyze the re-utterance effi-ciently ,as follows:1.
The parser keeps the parse record of the firstinput.2.
The parser starts parsing the new input justwhere the fake nonterminal was created.3.
The parsing ends when tim same-name r alnonterminal symbol is created out of the re-utterance.3.2 Hand l ing  Unknown WordsIf the reutterance cau not be parsed correctlyeven by the reutteraime, the unidentified portionis likely to contain an unknown word.
Finding anunknown word by a specific nonterminal symbolenables the interactive grammar augmentation asthe following, for instance.The parser can not identily the ~ollowingportion of your input.We cut \[NP\] with a knifeIf this is a new word in the category of \[NP\]a ru leNP --> (recog.
result of  the 2nd utterance)will be added to the grammar.
Is this ok?Handling unknown words is important in natu-ral language processing.
For example, Kainioka etal.
\[5\] proposed a mechanisnl which parses a sen-tencc with unknown words nsing Delinite C, lauseGralumars.
The efficient gap-filling technique ofhandling unknown words is quite useful in prac-tical systems and enhances the robustness of theGLR parsing greatly.When an unknown word W,,~, is detected, theword should be incorporated into the system.
Ifthe grammar is separated from the lexicon, theword can be easily added to the dictionary.
Ifthe grammar contains the lexicon, the LR tableshould be augmented incrementally in the follow-ing way.1.
For each state si which has an entry underthe column of the nonterminal D($~k,) in thegoto table, add shift action "s m" (m is thenew state number) for W .
.
.
.
(If Wnew con-sists of such multiple words as "get rid of', anew state should be created for each elementof the words.
)2.
Add reduce action "r p" (p is the new rulenumber) for all the terminals on the row ofstate nl.Before we close this section, wc should considerside etfects of the gap-tilling technique.
It is truethat putting fake nonterminals expands earch.Thus, some side effect might appear if the accu-racy of input is not good.
Namely, input shouldbe good enough to produce distinct fake nonter-minals and real nonterminals.
Although it is dif-ficult to analyze this phenomenon theoretically,the following natural heuristics can minimize thesearch growth.o Two consecutive fake uontermiuals are notallowed as shown in the previous ection.?
When a word (Wi) can be shifted to botha fake nonternfinal D.fake and a same-namereal nonterminal D~e,z, only D~,t should bevalid.?
When D: ,~ and D,.~l (:an be bundled usingthe local ambiguity packing \[111 tecbnique,discard l) f (,k,,.4 Exper iments :  Pars ing  Spo-ken  SentencesWe evaluated effectiveness of tlle enhanced GLRparsing by spoken input.
We used a device whichrecognizes a .lapanese utterance and produces itsphoneme sequence \[4\].
The parser we used is1)ased on the (-HA/ parser exploring the possi-bilities of substituted/inserted/deleted phonemes\[10\] by looking up the eonfilsion mntrix, whichwas constructed from the large vocabulary data.The confusion matrix is also used to mssign thescore to each explored phoneme, because therecogldtion device gives neither the alternativephoneme candidates nor the likelihood of hypoth-esized phonemes.
The gap-filling fimction is in-corporated iuto the parser in the following experi-ments.
Parsing a l>honeme seqnence might soundless pot>ular than I)arsing a word lattice in speechAcrEs DE COUNG-92, NANTES, 23-28 Ao(:r 1992 1 0 S 6 PROC.
OF COLING-92, NANTES, AU6.23-28, 1992recognition.
Because the parser builds a latticedynamically in parsing the sequence from left toright using a CFG which contains the dictionary,no static lattice is necessary.125 sentences (five speakers pronounced 25 sen-tences) were tested in tim domain called "conver-sation between doctors and patients."
111 sen-tences were parsed correctly \[88.8 %\] (the correctsentence was obtained as the top-scored hypoth-esis).
14 failed sentences can be classified intothree groups:(i) 4 sentences were parsed as the top-scoredhypotlmses with fake nonterminals.
Thus theparser asked the user to speak the unidentitiedportion again.
(ii) 6 sentences were parsed incorrectly in thatthe correct sentence did not get the highest scoremainly because the incorrect nonterminal had aslightly higher score than the correct one.
In thiscase, both the closely-scored correct and incofrect nontermin~s are packed into one nouterllli-nal using the local ambiguity packing technique inan efficient implementation.
In this situation theparser should ask the user to speak only that un-clear portion in the same way as in (i) instead ofproducing a barely top-scored hypothesis.
In thecurrent implementation the parser asks the userwhich word is the correct one.
(iii) 4 sentences were pronounced very I)adly.The user has to speak the whole sentence again.5 sentences with unknown words were alsotested, in all eases, the unknown word was de-tected.This result shows that interactive partial re-utterance is very effective both for error-recoveryand for detection of unknown words.5 Conc lud ing  RemarksWe presented a robust interactive apl)roach forspeech understanding.
The GLR parsing methodWaS enl la l iced to recover errors and to skip a verynoisy portion.
These techniques remedy ",dl-or-nothing-imss of the CF(Lbased LR t)arsing.
Theskipped portion is represented by a fake non-termimd which is resolved l)y re-utterance.
Anunknown word is also detected by a fake non-terminal and is incorporated into the dictionaryincrementally through interaction with the user.Exl)eriments in t)arsing a Jal)anese l)honeme se-quence have shown a great effectiveness of thisinteractive approach.References\[1\] Sethi R. Aho, A. V. and J. D. Ullman.
Compilers.Addison Wesley, 1986.\[2\] E. Giachin and C. l{,ullent.
Robust Pars-ing of Severely Corrupted Spoken Utterances.In Proceedings, 12th lnte~mational Conferenceon Computational Linguistics (COLING), Bu-dapest, ltungary, August 1988.\[3\] llauptmmm A. G. Carbonell J. G. Hayes, P. J.and M. Tomita.
Parsing spoken language: Asemantic caseframc approach.
In Proceedings,l l th lnleT~aalional Conference on ComputationalLinguistics (COLING), West Germany, August1986.
Bonn.\[4\] Morii S. lloshimi M. Hiraoka, S. and K. Niyada.Compact isolated word recognition system forlarge vocabulary.
In Proceedings, IEEF-IEC'EJ-ASJ International Conference ou Acoustics,Speech, and Signal Processin 9 (ICASSP), Tokyo,April 1986.\[5\] T. Kamioka and Y. Anzai.
Analysis of sentencesincluding unknown words by hypothesis genera-tion mechanism.
Journal of Japanese Society forArtificial Intelligence, Vol.3 No.5, pages 627-638,September 1!)88.
\[In Japanese\].\[0\] Kawabat, a T. Kits, K. and I1.
Saito.
tlMM Con-tinuous Speech Recognition Using Predictive LRParsing.
In ICASSP, May 1989.\[7\] K.F.
Lee.
Large- Vocabulary Speaker-htdependenlContinuous Speech Recognition: The SPHINXSystem.
Phi) thesis, Computer Science Depart-ment, Carnegie Mellon Uniw;rsity, April 1988.\[8\] M. Poesio and C. lhlllent.
Modified Cm'~eframeParsing for Speech Understanding Systems.
InProceedings, lOth International Joint Conferenceon Artificial Intelligence (IJCAI), Milan, August1987.\[9\] II.
Saito.
Gap-tilling LR Parsing for Noisy SpokenInput: "Ibwards Interactive Speech Hx~eognition.In Proceedings, httcTmattonal Conference on Spo-ken Language Processing (ICSLP), Kobe, Japan,November 1990.\[10\] II.
Saito and M. Tomita.
Parsing Noisy Sen-tences.
In Proceedings, 12th International Con-ference on G'omputalional Linguistics (COL-IN(;), Budapest, lhmgary, August 1988.\[11\] M. Tomita.
l';Jlieient Parsing for Natural Lan.guage, l(luwer Academic Publishers, Boston,MA, 1985.M.
qbmita.
An efficient word lattice parsing algo-rithm for continuous speech recognition.
In Pro-ceedin.qs, IEEE-1ECEJ-ASJ htternational Con-ference on Acoustics, Speech, and Signal Process-ing (ICASSP), 2bkyo, April 1986.\[12\]ACRES DE COLING-92, NANTES, 23-28 AO(rr 1992 1 0 5 7 PRoc.
OF COL1NG-92, NANTES, AUO.
23-28, 1992
