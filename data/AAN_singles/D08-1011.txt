Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 98?107,Honolulu, October 2008. c?2008 Association for Computational LinguisticsIndirect-HMM-based Hypothesis Alignment for Combining Outputsfrom Machine Translation SystemsXiaodong He?, Mei Yang?
*, Jianfeng Gao?, Patrick Nguyen?, and Robert Moore?
?Microsoft Research ?Dept.
of Electrical EngineeringOne Microsoft Way University of WashingtonRedmond, WA 98052 USA Seattle, WA 98195, USA{xiaohe,jfgao, panguyen,bobmoore}@microsoft.comyangmei@u.washington.eduAbstractThis paper presents a new hypothesis alignment methodfor combining outputs of multiple machine translation(MT) systems.
An indirect hidden Markov model(IHMM) is proposed to address the synonym matchingand word ordering issues in hypothesis alignment.Unlike traditional HMMs whose parameters are trainedvia maximum likelihood estimation (MLE), theparameters of the IHMM are estimated indirectly from avariety of sources including word semantic similarity,word surface similarity, and a distance-based distortionpenalty.
The IHMM-based method significantlyoutperforms the state-of-the-art TER-based alignmentmodel in our experiments on NIST benchmarkdatasets.
Our combined SMT system using theproposed method achieved the best Chinese-to-Englishtranslation result in the constrained training track of the2008 NIST Open MT Evaluation.1 Introduction*System combination has been applied successfullyto various machine translation tasks.
Recently,confusion-network-based system combinationalgorithms have been developed to combineoutputs of multiple machine translation (MT)systems to form a consensus output (Bangalore, etal.
2001, Matusov et al, 2006, Rosti et al, 2007,Sim et al, 2007).
A confusion network comprises asequence of sets of alternative words, possiblyincluding null?s, with associated scores.
Theconsensus output is then derived by selecting oneword from each set of alternatives, to produce thesequence with the best overall score, which couldbe assigned in various ways such as by voting, by* Mei Yang performed this work when she was an intern withMicrosoft Research.using posterior probability estimates, or by using acombination of these measures and other features.Constructing a confusion network requireschoosing one of the hypotheses as the backbone(also called ?skeleton?
in the literature), and otherhypotheses are aligned to it at the word level.
Highquality hypothesis alignment is crucial to theperformance of the resulting system combination.However, there are two challenging issues thatmake MT hypothesis alignment difficult.
First,different hypotheses may use differentsynonymous words to express the same meaning,and these synonyms need to be aligned to eachother.
Second, correct translations may havedifferent word orderings in different hypothesesand these words need to be properly reordered inhypothesis alignment.In this paper, we propose an indirect hiddenMarkov model (IHMM) for MT hypothesisalignment.
The HMM provides a way to modelboth synonym matching and word ordering.
Unliketraditional HMMs whose parameters are trainedvia maximum likelihood estimation (MLE), theparameters of the IHMM are estimated indirectlyfrom a variety of sources including word semanticsimilarity, word surface similarity, and a distance-based distortion penalty, without using largeamount of training data.
Our combined SMTsystem using the proposed method gave the bestresult on the Chinese-to-English test in theconstrained training track of the 2008 NIST OpenMT Evaluation (MT08).2 Confusion-network-based MT systemcombinationThe current state-of-the-art is confusion-network-based MT system combination as described by98Rosti and colleagues (Rosti et al, 2007a, Rosti etal., 2007b).
The major steps are illustrated inFigure 1.
In Fig.
1 (a), hypotheses from differentMT systems are first collected.
Then in Fig.
1 (b),one of the hypotheses is selected as the backbonefor hypothesis alignment.
This is usually done by asentence-level minimum Bayes risk (MBR)method which selects a hypothesis that has theminimum average distance compared to allhypotheses.
The backbone determines the wordorder of the combined output.
Then as illustrated inFig.
1 (c), all other hypotheses are aligned to thebackbone.
Note that in Fig.
1 (c) the symbol ?denotes a null word, which is inserted by thealignment normalization algorithm described insection 3.4.
Fig.
1 (c) also illustrates the handlingof synonym alignment (e.g., aligning ?car?
to?sedan?
), and word re-ordering of the hypothesis.Then in Fig.
1 (d), a confusion network isconstructed based on the aligned hypotheses,which consists of a sequence of sets in which eachword is aligned to a list of alternative words(including null) in the same set.
Then, a set ofglobal and local features are used to decode theconfusion network.E1 he have good carargmin ( , )B E EE TER E E??
?
??
?E EE2 he has nice sedanE3 it a nice car        e.g., EB = E1 E4 a sedan he has(a)  hypothesis set                    (b) backbone selectionEB he have ?
good car      he  have   ?
good   carhe   has    ?
nice    sedanit     ?
a   nice    carE4 a  ?
sedan  he   has      he   has    a     ?
sedan(c)  hypothesis alignment        (d) confusion networkFigure 1: Confusion-network-based MT systemcombination.3 Indirect-HMM-based HypothesisAlignmentIn confusion-network-based system combinationfor SMT, a major difficulty is aligning hypothesesto the backbone.
One possible statistical model forword alignment is the HMM, which has beenwidely used for bilingual word alignment (Vogel etal., 1996, Och and Ney, 2003).
In this paper, wepropose an indirect-HMM method for monolingualhypothesis alignment.3.1 IHMM for hypothesis alignmentLet1 1( ,..., )I Ie e e?
denote the backbone,1 1( ,..., )J Je e e?
?
??
a hypothesis to be aligned to 1Ie ,and1 1( ,..., )J Ja a a?
the alignment that specifiesthe position of the backbone word aligned to eachhypothesis word.
We treat each word in thebackbone as an HMM state and the words in thehypothesis as the observation sequence.
We use afirst-order HMM, assuming that the emissionprobability( | )jj ap e e?depends only on thebackbone word, and the transition probability1( | , )j jp a a I?depends only on the position of thelast state and the length of the backbone.
Treatingthe alignment as hidden variable, the conditionalprobability that the hypothesis is generated by thebackbone is given by11 1 11( | ) ( | , ) ( | )jJJJ Ij j j ajap e e p a a I p e e???
??
??
?
???
(1)As in HMM-based bilingual word alignment(Och and Ney, 2003), we also associate a null witheach backbone word to allow generatinghypothesis words that do not align to any backboneword.In HMM-based hypothesis alignment, emissionprobabilities model the similarity between abackbone word and a hypothesis word, and will bereferred to as the similarity model.
The transitionprobabilities model word reordering, and will becalled the distortion model.3.2 Estimation of the similarity modelThe similarity model, which specifies the emissionprobabilities of the HMM, models the similaritybetween a backbone word and a hypothesis word.Since both words are in the same language, thesimilarity model can be derived based on bothsemantic similarity and surface similarity, and theoverall similarity model is a linear interpolation ofthe two:( | ) ( | ) (1 ) ( | )j i sem j i sur j ip e e p e e p e e?
??
?
??
?
?
?
?
(2)99where ( | )sem j ip e e?and ( | )sur j ip e e?reflect thesemantic and surface similarity betweenje?andie , respectively, and ?
is the interpolation factor.Since the semantic similarity between twotarget words is source-dependent, the semanticsimilarity model is derived by using the sourceword sequence as a hidden layer:0( | )( | ) ( | , )sem j iKk i j k ikp e ep f e p e f e????
?0( | ) ( | )K k i j kkp f e p e f????
(3)where1 1( ,..., )K Kf f f?
is the source sentence.Moreover, in order to handle the case that twotarget words are synonyms but neither of them hascounter-part in the source sentence, a null isintroduced on the source side, which is representedby f0.
The last step in (3) assumes that first eigenerates all source words including null.
Then ej?is generated by all source words including null.In the common SMT scenario where a largeamount of bilingual parallel data is available, wecan estimate the translation probabilities from asource word to a target word and vice versa viaconventional bilingual word alignment.
Then both( | )k ip f e  and ( | )j kp e f?in (3) can be derived:2( | ) ( | )j k s t j kp e f p e f?
?
?where2 ( | )s t j kp e f?is the translation model fromthe source-to-target word alignment model, and( | )k ip f e  , which enforces the sum-to-1 constraintover all words in the source sentence, takes thefollowing form,220( | )( | )( | )t s k ik i Kt s k ikp f ep f ep f e??
?where2 ( | )t s k ip f e  is the translation model fromthe  target-to-source word alignment model.
In ourmethod,2 ( | )t s ip null e  for all target words issimply a constant pnull, whose value is optimizedon held-out data 1.The surface similarity model can be estimatedin several ways.
A very simple model could bebased on exact match: the surface similarity model,( | )sur j ip e e?, would take the value 1.0 if e?= e, and0 otherwise 2 .
However, a smoothed surfacesimilarity model is used in our method.
If the targetlanguage uses alphabetic orthography, as Englishdoes, we treat words as letter sequences and thesimilarity measure can be the length of the longestmatched prefix (LMP) or the length of the longestcommon subsequence (LCS) between them.
Then,this raw similarity measure is transformed to asurface similarity score between 0 and 1 throughan exponential mapping,?
?
( | ) exp ( , ) 1sur j i j ip e e s e e??
??
??
?
??
?
(4)where ( , )j is e e?is computed as( , )( , ) max(| |,| |)j ij ij iM e es e e e e??
?
?and ( , )j iM e e?is the raw similarity measure of ej?ei, which is the length of the LMP or LCS of ej?and ei.
and ?
is a smoothing factor thatcharacterizes the mapping, Thus as ?
approachesinfinity, ( | )sur j ip e e?backs off to the exact matchmodel.
We found the smoothed similarity model of(4) yields slightly better results than the exactmatch model.
Both LMP- and LCS- based methodsachieve similar performance but the computationof LMP is faster.
Therefore, we only report resultsof the LMP-based smoothed similarity model.3.3 Estimation of the distortion modelThe distortion model, which specifies the transitionprobabilities of the HMM, models the first-orderdependencies of word ordering.
In bilingualHMM-based word alignment, it is commonlyassumed that transition probabilities1  The other direction,2 ( | )s t ip e null?
, is available from thesource-to-target translation model.2 Usually a small back-off value is assigned instead of 0.1001( | , )?
??
?j jp a i a i Idepend only on the jumpdistance (i - i')  (Vogel et al, 1996):1( )( | , )( )Ilc i ip i i Ic l i????
????
(5)As suggested by Liang et al (2006), we cangroup the distortion parameters {c(d)}, d= i - i',into a few buckets.
In our implementation, 11buckets are used for c(?-4),  c(-3), ... c(0), ..., c(5),c(?6).
The probability mass for transitions withjump distance larger than 6 and less than -4 isuniformly divided.
By doing this, only a handful ofc(d) parameters need to be estimated.
Although itis possible to estimate them using the EMalgorithm on a small development set, we foundthat a particularly simple model, described below,works surprisingly well in our experiments.Since both the backbone and the hypothesis arein the same language, It seems intuitive that thedistortion model should favor monotonicalignment and only allow non-monotonicalignment with a certain penalty.
This leads us touse a distortion model of the following form,where K is a tuning factor optimized on held-outdata.?
?
?
?1 1c d d ???
?
?, d= ?4, ?, 6   (6)As shown in Fig.
2, the value of distortion scorepeaks at d=1, i.e., the monotonic alignment, anddecays for non-monotonic alignments dependingon how far it diverges from the monotonicalignment.Figure 2, the distance-based distortion parameterscomputed according to (6), where K=2.Following Och and Ney (2003), we use a fixedvalue p0 for the probability of jumping to a nullstate, which can be optimized on held-out data, andthe overall distortion model becomes00if     state( | , ) (1 ) ( | , )  otherwisep i nullp i i I p p i i I???
?
?
??
??
?3.4 Alignment normalizationGiven an HMM, the Viterbi alignment algorithmcan be applied to find the best alignment betweenthe backbone and the hypothesis,11 11?
argmax ( | , ) ( | )jJJJj j j aa ja p a a I p e e???
???
?
??
(7)However, the alignment produced by thealgorithm cannot be used directly to build aconfusion network.
There are two reasons for this.First, the alignment produced may contain 1-Nmappings between the backbone and thehypothesis whereas 1-1 mappings are required inorder to build a confusion network.
Second, ifhypothesis words are aligned to a null in thebackbone or vice versa, we need to insert actualnulls into the right places in the hypothesis and thebackbone, respectively.
Therefore, we need tonormalize the alignment produced by Viterbisearch.EB ?
e2  ?2   ??
?
e2        ?
?
?e1'    e2'    e3'   e4'Eh e1'    e2'    e3'   e4'(a) hypothesis words are aligned to the backbone nullEB e1  ?1  e2  ?2  e3  ?3?
e1     e2        e3      ?e2'    ?
e1'Eh e1'    e2'    ?
(b) a backbone word is aligned to no hypothesis wordFigure 3: illustration of alignment normalizationFirst, whenever more than one hypothesiswords are aligned to one backbone word, we keepthe link which gives the highest occupationprobability computed via the forward-backwardalgorithm.
The other hypothesis words originally-4                     1                      61.00.0c(d)d101aligned to the backbone word will be aligned to thenull associated with that backbone word.Second, for the hypothesis words that arealigned to a particular null on the backbone side, aset of nulls are inserted around that backbone wordassociated with the null such that no links crosseach other.
As illustrated in Fig.
3 (a), if ahypothesis word e2?
is aligned to the backboneword e2, a null is inserted in front of the backboneword e2 linked to the hypothesis word e1?
thatcomes before e2?.
Nulls are also inserted for otherhypothesis words such as e3?
and e4?
after thebackbone word e2.
If there is no hypothesis wordaligned to that backbone word, all nulls areinserted after that backbone word .3For a backbone word that is aligned to nohypothesis word, a null is inserted on thehypothesis side, right after the hypothesis wordwhich is aligned to the immediately precedingbackbone word.
An example is shown in Fig.
3 (b).4 Related workThe two main hypothesis alignment methods forsystem combination in the previous literature areGIZA++ and TER-based methods.
Matusov et al(2006) proposed using GIZA++ to align wordsbetween different MT hypotheses, where allhypotheses of the test corpus are collected to createhypothesis pairs for GIZA++ training.
Thisapproach uses the conventional HMM modelbootstrapped from IBM Model-1 as implementedin GIZA++, and heuristically combines resultsfrom aligning in both directions.
Systemcombination based on this approach gives animprovement over the best single system.However, the number of hypothesis pairs fortraining is limited by the size of the test corpus.Also, MT hypotheses from the same sourcesentence are correlated with each other and thesehypothesis pairs are not i.i.d.
data samples.Therefore, GIZA++ training on such a data set maybe unreliable.Bangalore et al (2001) used a multiple string-matching algorithm based on Levenshtein editdistance, and later Sim et al (2007) and Rosti et al(2007) extended it to a TER-based method forhypothesis alignment.
TER (Snover et al, 2006)3  This only happens if no hypothesis word is aligned to abackbone word but some hypothesis words are aligned to thenull associated with that backbone word.measures the minimum number of edits, includingsubstitution, insertion, deletion, and shift of blocksof words, that are needed to modify a hypothesis sothat it exactly matches the other hypothesis.
Thebest alignment is the one that gives the minimumnumber of translation edits.
TER-based confusionnetwork construction and system combination hasdemonstrated superior performance on variouslarge-scale MT tasks (Rosti.
et al 2007).
However,when searching for the optimal alignment, theTER-based method uses a strict surface hard matchfor counting edits.
Therefore, it is not able tohandle synonym matching well.
Moreover,although TER-based alignment allows phraseshifts to accommodate the non-monotonic wordordering, all non-monotonic shifts are penalizedequally no matter how short or how long the moveis, and this penalty is set to be the same as that forsubstitution, deletion, and insertion edits.Therefore, its modeling of non-monotonic wordordering is very coarse-grained.In contrast to the GIZA++-based method, ourIHMM-based method has a similarity modelestimated using bilingual word alignment HMMsthat are trained on a large amount of bi-text data.Moreover, the surface similarity information isexplicitly incorporated in our model, while it isonly used implicitly via parameter initialization forIBM Model-1 training by Matusov et al (2006).On the other hand, the TER-based alignmentmodel is similar to a coarse-grained, non-normalized version of our IHMM, in which thesimilarity model assigns no penalty to an exactsurface match and a fixed penalty to allsubstitutions, insertions, and deletions, and thedistortion model simply assigns no penalty to amonotonic jump, and a fixed penalty to all otherjumps, equal to the non-exact-match penalty in thesimilarity model.There have been other hypothesis alignmentmethods.
Karakos, et al (2008) proposed an ITG-based method for hypothesis alignment, Rosti et al(2008) proposed an incremental alignment method,and a heuristic-based matching algorithm wasproposed by Jayaraman and Lavie (2005).5 EvaluationIn this section, we evaluate our IHMM-basedhypothesis alignment method on the Chinese-to-English (C2E) test in the constrained training track102of the 2008 NIST Open MT Evaluation (NIST,2008).
We compare to the TER-based method usedby Rosti et al (2007).
In the followingexperiments, the NIST BLEU score is used as theevaluation metric (Papineni et al, 2002), which isreported as a percentage in the following sections.5.1 Implementation detailsIn our implementation, the backbone is selectedwith MBR.
Only the top hypothesis from eachsingle system is considered as a backbone.
Auniform posteriori probability is assigned to allhypotheses.
TER is used as loss function in theMBR computation.Similar to (Rosti et al, 2007), each word in theconfusion network is associated with a wordposterior probability.
Given a system S, each of itshypotheses is assigned with a rank-based score of1/(1+r)?, where r is the rank of the hypothesis, and?
is a rank smoothing parameter.
The systemspecific rank-based score of a word w for a givensystem S is the sum of all the rank-based scores ofthe hypotheses in system S that contain the word wat the given position (after hypothesis alignment).This score is then normalized by the sum of thescores of all the alternative words at the sameposition and from the same system S to generatethe system specific word posterior.
Then, the totalword posterior of w over all systems is a sum ofthese system specific posteriors weighted bysystem weights.Beside the word posteriors, we use languagemodel scores and a word count as features forconfusion network decoding.Therefore, for an M-way system combinationthat uses N LMs, a total of M+N+1 decodingparameters, including M-1 system weights, onerank smoothing factor, N language model weights,and one weight for the word count feature, areoptimized using Powell?s method (Brent, 1973) tomaximize BLEU score on a development set4 .Two language models are used in ourexperiments.
One is a trigram model estimatedfrom the English side of the parallel training data,and the other is a 5-gram model trained on theEnglish GigaWord corpus from LDC using theMSRLM toolkit (Nguyen et al 2007).4 The parameters of IHMM are not tuned by maximum-BLEUtraining.In order to reduce the fluctuation of BLEUscores caused by the inconsistent translation outputlength, an unsupervised length adaptation methodhas been devised.
We compute an expected lengthratio between the MT output and the sourcesentences on the development set after maximum-BLEU training.
Then during test, we adapt thelength of the translation output by adjusting theweight of the word count feature such that theexpected output/source length ratio is met.
In ourexperiments, we apply length adaptation to thesystem combination output at the level of thewhole test corpus.5.2  Development and test dataThe development (dev) set used for systemcombination parameter training contains 1002sentences sampled from the previous NIST MTChinese-to-English test sets: 35% from MT04,55% from MT05, and 10% from MT06-newswire.The test set is the MT08 Chinese-to-English?current?
test set, which includes 1357 sentencesfrom both newswire and web-data genres.
Bothdev and test sets have four references per sentence.As inputs to the system combination, 10-besthypotheses for each source sentence in the dev andtest sets are collected from each of the eight singlesystems.
All outputs on the MT08 test set weretrue-cased before scoring using a log-linearconditional Markov model proposed by Toutanovaet al (2008).
However, to save computation effort,the results on the dev set are reported in caseinsensitive BLEU (ciBLEU) score instead.5.3  Experimental resultsIn our main experiments, outputs from a total ofeight single MT systems were combined.
As listedin Table 1, Sys-1 is a tree-to-string systemproposed by Quirk et al, (2005); Sys-2 is a phrase-based system with fast pruning proposed by Mooreand Quirk (2008); Sys-3 is a phrase-based systemwith syntactic source reordering proposed byWang et al (2007a); Sys-4 is a syntax-based pre-ordering system proposed by Li et.
al.
(2007); Sys-5 is a hierarchical system proposed by Chiang(2007); Sys-6 is a lexicalized re-ordering systemproposed by Xiong et al (2006); Sys-7 is a two-pass phrase-based system with adapted LMproposed by Foster and Kuhn (2007); and  Sys-8 is103a hierarchical system with two-pass rescoringusing a parser-based LM proposed by Wang et al,(2007b).
All systems were trained within theconfines of the constrained training condition ofNIST MT08 evaluation.
These single systems areoptimized with maximum-BLEU training ondifferent subsets of the previous NIST MT testdata.
The bilingual translation models used tocompute the semantic similarity are from the word-dependent HMMs proposed by He (2007), whichare trained on two million parallel sentence-pairsselected from the training corpus allowed by theconstrained training condition of MT08.5.3.1 Comparison with TER alignmentIn the IHMM-based method, the smoothingfactor for surface similarity model is set to ?
= 3,the interpolation factor of the overall similaritymodel is set to ?
= 0.3, and the controlling factor ofthe distance-based distortion parameters is set toK=2.
These settings are optimized on the dev set.Individual system results and system combinationresults using both IHMM and TER alignment, onboth the dev and test sets, are presented in Table 1.The TER-based hypothesis alignment tool used inour experiments is the publicly available TER Javaprogram, TERCOM (Snover et al, 2006).
Defaultsettings of TERCOM are used in the followingexperiments.On the dev set, the case insensitive BLEU scoreof the IHMM-based 8-way system combinationoutput is about 5.8 points higher than that of thebest single system.
Compared to the TER-basedmethod, the IHMM-based method is about 1.5BLEU points better.
On the MT08 test set, theIHMM-based system combination gave a casesensitive BLEU score of 30.89%.
It outperformedthe best single system by 4.7 BLEU points and theTER-based system combination by 1.0 BLEUpoints.
Note that the best single system on the devset and the test set are different.
The differentsingle systems are optimized on different tuningsets, so this discrepancy between dev set and testset results is presumably due to differing degreesof mismatch between the dev and test sets and thevarious tuning sets.Table 1.
Results of single and combined systemson the dev set and the MT08 test setSystem DevciBLEU%MT08BLEU%System 1 34.08 21.75System 2 33.78 20.42System 3 34.75 21.69System 4 37.85 25.52System 5 37.80 24.57System 6 37.28 24.40System 7 32.37 25.51System 8 34.98 26.24TER 42.11 29.89IHMM 43.62 30.89In order to evaluate how well our methodperforms when we combine more systems, wecollected MT outputs on MT08 from sevenadditional single systems as summarized in Table2.
These systems belong to two groups.
Sys-9 toSys-12 are in the first group.
They are syntax-augmented hierarchical systems similar to thosedescribed by Shen et al (2008) using differentChinese word segmentation and language models.The second group has Sys-13 to Sys-15.
Sys-13 isa phrasal system proposed by Koehn et al (2003),Sys-14 is a hierarchical system proposed byChiang (2007), and Sys-15 is a syntax-basedsystem proposed by Galley et al (2006).
All sevensystems were trained within the confines of theconstrained training condition of NIST MT08evaluation.We collected 10-best MT outputs only on theMT08 test set from these seven extra systems.
NoMT outputs on our dev set are available from themat present.
Therefore, we directly adopt systemcombination parameters trained for the previous 8-way system combination, except the systemweights, which are re-set by the followingheuristics: First, the total system weight mass 1.0 isevenly divided among the three groups of singlesystems: {Sys-1~8}, {Sys-9~12}, and {Sys-13~15}.
Each group receives a total system weightmass of 1/3.
Then the weight mass is furtherdivided in each group: in the first group, theoriginal weights of systems 1~8 are multiplied by1/3; in the second and third groups, the weightmass is evenly distributed within the group, i.e.,1/12 for each system in group 2, and 1/9 for each104system in group 35.
Length adaptation is applied tocontrol the final output length, where the sameexpected length ratio of the previous 8-way systemcombination is adopted.The results of the 15-way system combinationare presented in Table 3.
It shows that the IHMM-based method is still about 1 BLEU point betterthan the TER-based method.
Moreover, combining15 single systems gives an output that has a NISTBLEU score of 34.82%, which is 3.9 points betterthan the best submission to the NIST MT08constrained training track (NIST, 2008).
To ourknowledge, this is the best result reported on thistask.Table 2.
Results of seven additional single systemson the NIST MT08 test setSystem MT08BLEU%System 9 29.59System 10 29.57System 11 29.64System 12 29.85System 13 25.53System 14 26.04System 15 29.70Table 3.
Results of the 15-way system combinationon the NIST MT08 C2E test setSys.
Comb.
MT08BLEU%TER 33.81IHMM 34.825.3.2 Effect of the similarity modelIn this section, we evaluate the effect of thesemantic similarity model and the surfacesimilarity model by varying the interpolationweight ?
of (2).
The results on both the dev andtest sets are reported in Table 4.
In one extremecase, ?
= 1, the overall similarity model is basedonly on semantic similarity.
This gives a caseinsensitive BLEU score of 41.70% and a casesensitive BLEU score of 28.92% on the dev andtest set, respectively.
The accuracy is significantlyimproved to 43.62% on the dev set and 30.89% ontest set when ?
= 0.3.
In another extreme case, ?
=5 This is just a rough guess because no dev set is available.
Webelieve a better set of system weights could be obtained if MToutputs on a common dev set were available.0, in which only the surface similarity model isused for the overall similarity model, theperformance degrades by about 0.2 point.Therefore, the surface similarity information seemsmore important for monolingual hypothesisalignment, but both sub-models are useful.Table 4.
Effect of the similarity modelDevciBLEU%TestBLEU%?
= 1.0 41.70 28.92?
= 0.7 42.86 30.50?
= 0.5 43.11 30.94?
= 0.3 43.62 30.89?
= 0.0 43.35 30.735.3.3 Effect of the distortion modelWe investigate the effect of the distance-baseddistortion model by varying the controlling factorK in (6).
For example, setting K=1.0 gives a linear-decay distortion model, and setting K=2.0 gives aquadratic smoothed distance-based distortionmodel.
As shown in Table 5, the optimal result canbe achieved using a properly smoothed distance-based distortion model.Table 5.
Effect of the distortion modelDevciBLEU%TestBLEU%K=1.0 42.94 30.44K=2.0 43.62 30.89K=4.0 43.17 30.30K=8.0 43.09 30.016 ConclusionSynonym matching and word ordering are twocentral issues for hypothesis alignment inconfusion-network-based MT system combination.In this paper, an IHMM-based method is proposedfor hypothesis alignment.
It uses a similarity modelfor synonym matching and a distortion model forword ordering.
In contrast to previous methods, thesimilarity model explicitly incorporates bothsemantic and surface word similarity, which iscritical to monolingual word alignment, and asmoothed distance-based distortion model is usedto model the first-order dependency of wordordering, which is shown to be better than simplerapproaches.105Our experimental results show that the IHMM-based hypothesis alignment method gave superiorresults on the NIST MT08 C2E test set comparedto the TER-based method.
Moreover, we show thatour system combination method can scale up tocombining more systems and produce a betteroutput that has a case sensitive BLEU score of34.82, which is 3.9 BLEU points better than thebest official submission of MT08.AcknowledgementThe authors are grateful to Chris Quirk, ArulMenezes, Kristina Toutanova, William Dolan, MuLi, Chi-Ho Li, Dongdong Zhang, Long Jiang,Ming Zhou, George Foster, Roland Kuhn, JingZheng, Wen Wang, Necip Fazil Ayan, DimitraVergyri, Nicolas Scheffer, Andreas Stolcke, KevinKnight, Jens-Soenke Voeckler, Spyros Matsoukas,and Antti-Veikko Rosti for assistance with the MTsystems and/or for the valuable suggestions anddiscussions.ReferencesSrinivas Bangalore, German Bordel, and GiuseppeRiccardi.
2001.
Computing consensus translationfrom multiple machine translation systems.
In Proc.of IEEE ASRU, pp.
351?354.Richard Brent, 1973.
Algorithms for Minimizationwithout Derivatives.
Prentice-Hall, Chapter 7.David Chiang.
2007.
Hierarchical phrase-basedtranslation.
Computational Linguistics, 33(2):201?228.George Foster and Roland Kuhn.
2007.
Mixture-ModelAdaptation for SMT.
In Proc.
of the Second ACLWorkshop on Statistical Machine Translation.
pp.128 ?
136.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang and IgnacioThayer.
2006.
Scalable Inference and Training ofContext-Rich Syntactic Translation Models.
In Proc.of COLING-ACL, pp.
961?968.Xiaodong He.
2007.
Using Word-Dependent TransitionModels in HMM based Word Alignment forStatistical Machine Translation.
In Proc.
of theSecond ACL Workshop on Statistical MachineTranslation.Shyamsundar Jayaraman and Alon Lavie.
2005.
Multi-engine machine translation guided by explicit wordmatching.
In Proc.
of EAMT.
pp.
143 ?
152.Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,and Markus Dreyer.
2008.
Machine TranslationSystem Combination using ITG-based Alignments.In Proc.
of ACL-HLT, pp.
81?84.Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, MingZhou, Yi Guan.
2007.
A Probabilistic Approach toSyntax-based Reordering for Statistical MachineTranslation.
In Proc.
of ACL.
pp.
720 ?
727.Percy Liang, Ben Taskar, and Dan Klein.
2006.Alignment by Agreement.
In Proc.
of NAACL.
pp104 ?
111.Evgeny Matusov, Nicola Ueffing, and Hermann Ney.2006.
Computing consensus translation frommultiple machine translation systems using enhancedhypotheses alignment.
In Proc.
of EACL, pp.
33?40.Robert Moore and Chris Quirk.
2007.
Faster Beam-Search Decoding for Phrasal Statistical MachineTranslation.
In Proc.
of MT Summit XI.Patrick Nguyen, Jianfeng Gao and Milind Mahajan.2007.
MSRLM: a scalable language modelingtoolkit.
Microsoft Research Technical Report MSR-TR-2007-144.NIST.
2008.
The 2008 NIST Open Machine TranslationEvaluation.
www.nist.gov/speech/tests/mt/2008/doc/Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
of ACL,pp.
311?318.Koehn, Philipp, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase based translation.
In Proc.
ofNAACL.
pp.
48 ?
54.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntacticallyinformed phrasal SMT.
In Proc.
of ACL.
pp.
271?279.Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,Richard Schwartz, Necip Fazil Ayan, and Bonnie J.Dorr.
2007a.
Combining outputs from multiplemachine translation systems.
In Proc.
of NAACL-HLT, pp.
228?235.Antti-Veikko I. Rosti, Spyros Matsoukas, and RichardSchwartz.
2007b.
Improved Word-Level SystemCombination for Machine Translation.
In Proc.
ofACL, pp.
312?319.Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2008.
Incremental HypothesisAlignment for Building Confusion Networks withApplication to Machine Translation SystemCombination, In Proc.
of the Third ACL Workshopon Statistical Machine Translation, pp.
183?186.Libin Shen, Jinxi Xu, Ralph Weischedel.
2008.
A NewString-to-Dependency Machine TranslationAlgorithm with a Target Dependency LanguageModel.
In Proc.
of ACL-HLT, pp.
577?585.106Khe Chai Sim, William J. Byrne, Mark J.F.
Gales,Hichem Sahbi, and Phil C. Woodland.
2007.Consensus network decoding for statistical machinetranslation system combination.
In Proc.
of ICASSP,vol.
4. pp.
105?108.Matthew Snover, Bonnie Dorr, Rich Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proc.
of AMTA.Kristina Toutanova, Hisami Suzuki and Achim Ruopp.2008.
Applying Morphology Generation Models toMachine Translation.
In Proc.
of ACL.
pp.
514 ?
522.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based Word Alignment In StatisticalTranslation.
In Proc.
of COLING.
pp.
836-841.Chao Wang, Michael Collins, and Philipp Koehn.2007a.
Chinese Syntactic Reordering for StatisticalMachine Translation.
In Proc.
of EMNLP-CoNLL.pp.
737-745.Wen Wang, Andreas Stolcke, Jing Zheng.
2007b.Reranking Machine Translation Hypotheses WithStructured and Web-based Language Models.
InProc.
of IEEE ASRU.
pp.
159 ?
164.Deyi Xiong, Qun Liu and Shouxun Lin.
2006.Maximum Entropy Based Phrase Reordering Modelfor Statistical Machine Translation.
In Proc.
of ACL.pp.
521 ?
528.107
