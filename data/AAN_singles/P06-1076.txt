Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 601?608,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Comparison of Document, Sentence, and Term Event SpacesCatherine BlakeSchool of Information and Library ScienceUniversity of North Carolina at Chapel HillNorth Carolina, NC 27599-3360cablake@email.unc.eduAbstractThe trend in information retrieval sys-tems is from document to sub-documentretrieval, such as sentences in a summari-zation system and words or phrases inquestion-answering system.
Despite thistrend, systems continue to model lan-guage at a document level using the in-verse document frequency (IDF).
In thispaper, we compare and contrast IDF withinverse sentence frequency (ISF) and in-verse term frequency (ITF).
A directcomparison reveals that all languagemodels are highly correlated; however,the average ISF and ITF values are 5.5and 10.4 higher than IDF.
All languagemodels appeared to follow a power lawdistribution with a slope coefficient of1.6 for documents and 1.7 for sentencesand terms.
We conclude with an analysisof IDF stability with respect to random,journal, and section partitions of the100,830 full-text scientific articles in ourexperimental corpus.1 IntroductionThe vector based information retrieval modelidentifies relevant documents by comparingquery terms with terms from a document corpus.The most common corpus weighting scheme isthe term frequency (TF) x inverse document fre-quency (IDF), where TF is the number of times aterm appears in a document, and IDF reflects thedistribution of terms within the corpus (Saltonand Buckley, 1988).
Ideally, the system shouldassign the highest weights to terms with the mostdiscriminative power.One component of the corpus weight is thelanguage model used.
The most common lan-guage model is the Inverse Document Fre-quency (IDF), which considers the distributionof terms between documents (see equation (1)).IDF has played a central role in retrieval systemssince it was first introduced more than thirtyyears ago (Sparck Jones, 1972).IDF(ti)=log2(N)?log2(ni)+1   (1)N is the total number of corpusdocuments; ni is the number of docu-ments that contain at least one oc-currence of the term ti; and ti is aterm, which is typically stemmed.Although information retrieval systems aretrending from document to sub-document re-trieval, such as sentences for summarization andwords, or phrases for question answering, sys-tems continue to calculate corpus weights on alanguage model of documents.
Logic suggeststhat if a system identifies sentences rather thandocuments, it should use a corpus weightingscheme based on the number of sentences ratherthan the number documents.
That is, the systemshould replace IDF with the Inverse SentenceFrequency (ISF), where N in (1) is the totalnumber of sentences and ni is the number of sen-tences with term i.
Similarly, if the system re-trieves terms or phrases then IDF should be re-placed with the Inverse Term Frequency (ITF),where N in (1) is the vocabulary size, and ni isthe number of times a term or phrases appears inthe corpus.
The challenge is that although docu-ment language models have had unprecedentedempirical success, language models based on asentence or term do not appear to work well(Robertson, 2004).Our goal is to explore the transition from thedocument to sentence and term spaces, such thatwe may uncover where the language models start601to break down.
In this paper, we explore this goalby answering the following questions: How cor-related are the raw document, sentence, and termspaces?
How correlated are the IDF, ISF, andITF values?
How well does each language mod-els conform to Zipf?s Law and what are the slopecoefficients?
How sensitive is IDF with respectto sub-sets of a corpus selected at random, fromjournals, or from document sections includingthe abstract and body of an article?This paper is organized as follows: Section 2provides the theoretical and practical implica-tions of this study; Section 3 describes the ex-perimental design we used to study document,sentence, and term, spaces in our corpora ofmore than one-hundred thousand full-text docu-ments; Section 4 discusses the results; and Sec-tion 5 draws conclusions from this study.2 Background and MotivationThe transition from document to sentence toterm spaces has both theoretical and practicalramifications.
From a theoretical standpoint, thesuccess of TFxIDF is problematic because themodel combines two different event spaces ?
thespace of terms in TF and of documents in IDF.
Inaddition to resolving the discrepancy betweenevent spaces, the foundational theories in infor-mation science, such as Zipf?s Law (Zipf, 1949)and Shannon?s Theory (Shannon, 1948) consideronly a term event space.
Thus, establishing a di-rect connection between the empirically success-ful IDF and the theoretically based ITF may en-able a connection to previously adopted informa-tion theories.05101520250 5 10 15 20 25log(Vocababulary Size (n))log(CorpusSize(N))SSSMSLMSMMMLLSLMLLfirst IDFpaperthispaperDocument space dominatesVocabulary space dominatesthe webover time ?Figure 1.
Synthetic data showing IDF trendsfor different sized corpora and vocabulary.Understanding the relationship among docu-ment, sentence and term spaces also has practicalimportance.
The size and nature of text corporahas changed dramatically since the first IDF ex-periments.
Consider the synthetic data shown inFigure 1, which reflects the increase in both vo-cabulary and corpora size from small (S), to me-dium (M), to large (L).
The small vocabularysize is from the Cranfield corpus used in SparckJones (1972), medium is from the 0.9 millionterms in the Heritage Dictionary (Pickett 2000)and large is the 1.3 million terms in our corpus.The small number of documents is from theCranfield corpus in Sparck Jones (1972), me-dium is 100,000 from our corpus, and large is 1millionAs a document corpus becomes sufficientlylarge, the rate of new terms in the vocabularydecreases.
Thus, in practice the rate of growth onthe x-axis of Figure 1 will slow as the corpus sizeincreases.
In contrast, the number of documents(shown on the y-axis in Figure 1) remains un-bounded.
It is not clear which of the two compo-nents in equation (1), the log2(N), which re-flects the number of documents, or thelog2(ni),which reflects the distribution ofterms between documents within the corpus willdominate the equation.
Our strategy is to explorethese differences empirically.In addition to changes in the vocabulary sizeand the number of documents, the average num-ber of terms per document has increased from7.9, 12.2 and 32 in Sparck Jones (1972), to 20and 32 in Salton and Buckley (1988), to 4,981 inour corpus.
The transition from abstracts to full-text documents explains the dramatic differencein document length; however, the impact withrespect to the distribution of terms and motivatesus to explore differences between the languageused in an abstract, and that used in the body of adocument.One last change from the initial experiments isa trend towards an on-line environment, wherecalculating IDF is prohibitively expensive.
Thissuggests a need to explore the stability of IDF sothat system designers can make an informed de-cision regarding how many documents should beincluded in the IDF calculations.
We explore thestability of IDF in random, journal, and docu-ment section sub-sets of the corpus.3 Experimental DesignOur goal in this paper is to compare and contrastlanguage models based on a document with thosebased on a sentence and term event spaces.
Weconsidered several of the corpora from the TextRetrieval Conferences (TREC, trec.nist.gov);however, those collections were primarily news602articles.
One exception was the recently addedgenomics track, which considered full-text scien-tific articles, but did not provide relevance judg-ments at a sentence or term level.
We also con-sidered the sentence level judgments from thenovelty track and the phrase level judgmentsfrom the question-answering track, but thosewere news and web documents respectively andwe had wanted to explore the event spaces in thecontext of scientific literature.Table 1 shows the corpus that we developedfor these experiments.
The American ChemistrySociety provided 103,262 full-text documents,which were published in 27 journals from 2000-20041.
We processed the headings, text, and ta-bles using Java BreakIterator class to identifysentences and a Java implementation of the Por-ter Stemming algorithm (Porter, 1980) to identifyterms.
The inverted index was stored in an Ora-cle 10i database.Docs Avg TokensJournal # % Length Million   %ACHRE4 548 0.5 4923 2.7 1ANCHAM 4012 4.0 4860 19.5 4BICHAW 8799 8.7 6674 58.7 11BIPRET 1067 1.1 4552 4.9 1BOMAF6 1068 1.1 4847 5.2 1CGDEFU 566 0.5 3741 2.1 <1CMATEX 3598 3.6 4807 17.3 3ESTHAG 4120 4.1 5248 21.6 4IECRED 3975 3.9 5329 21.2 4INOCAJ 5422 5.4 6292 34.1 6JACSAT 14400  14.3 4349 62.6 12JAFCAU 5884 5.8 4185 24.6 5JCCHFF 500 0.5 5526 2.8 1JCISD8 1092 1.1 4931 5.4 1JMCMAR 3202 3.2 8809 28.2 5JNPRDF 2291 2.2 4144 9.5 2JOCEAH 7307 7.2 6605 48.3 9JPCAFH 7654 7.6 6181 47.3 9JPCBFK 9990 9.9 5750 57.4 11JPROBS 268 0.3 4917 1.3 <1MAMOBX 6887 6.8 5283 36.4 7MPOHBP 58 0.1 4868 0.3 <1NALEFD 1272 1.3 2609 3.3 1OPRDFK 858 0.8 3616 3.1 1ORLEF7 5992 5.9 1477 8.8 2Total 100,830    526.6Average 4,033 4.0 4,981 21.1Std Dev 3,659 3.6 1,411 20.3Table 1.
Corpus summary.1 Formatting inconsistencies precluded two journals andreduced the number of documents by 2,432.We made the following comparisons betweenthe document, sentence, and term event spaces.
(1) Raw term comparisonA set of well-correlated spaces would enablean accurate prediction from one space to thenext.
We will plot pair-wise correlations betweeneach space to reveal similarities and differences.This comparison reflects a previous analysiscomprising a random sample of 193 words froma 50 million word corpus of 85,432 news articles(Church and Gale 1999).
Church and Gale?sanalysis of term and document spaces resulted ina p value of -0.994.
Our work complements theirapproach by considering full-text scientific arti-cles rather than news documents, and we con-sider the entire stemmed term vocabulary in a526 million-term corpus.
(2) Zipf Law comparisonInformation theory tells us that the frequencyof terms in a corpus conforms to the power lawdistribution K/j?
(Baeza-Yates and Ribeiro-Neto1999).
Zipf?s Law is a special case of the powerlaw, where ?
is close to 1 (Zipf, 1949).
To pro-vide another perspective of the alternativespaces, we calculated the parameters of Zipf?sLaw, K and ?
for each event space and journalusing the binning method proposed in (Adamic2000).
By accounting for K, the slope as definedby ?
will provide another way to characterizedifferences between the document, sentence andterm spaces.
We expect that all event spaces willconform to Zipf?s Law.
(3) Direct IDF, ISF, and ITF comparisonThe log2(N) and  log2(ni) should allow adirect comparison between IDF, ISF and ITF.Our third experiment was to provide pair-wisecomparisons among these the event spaces.
(4) Abstract versus full-text comparisonLanguage models of scientific articles oftenconsider only abstracts because they are easier toobtain than full-text documents.
Although his-torically difficult to obtain, the increased avail-ability of full-text articles motivates us to under-stand the nature of language within the body of adocument.
For example, one study found thatfull-text articles require weighting schemes thatconsider document length (Kamps, et al 2005).However, controlling the weights for documentlengths may hide a systematic difference be-tween the language used in abstracts and the lan-guage used in the body of a document.
For ex-ample, authors may use general language in an603abstract and technical language within a docu-ment.Transitioning from abstracts to full-text docu-ments presents several challenges including howto weigh terms within the headings, figures, cap-tions, and tables.
Our forth experiment was tocompare IDF between the abstract and full textof the document.
We did not consider text fromheadings, figures, captions, or tables.
(5) IDF SensitivityIn a dynamic environment such as the Web, itwould be desirable to have a corpus-basedweight that did not change dramatically with theaddition of new documents.
An increased under-standing of IDF stability may enable us to makespecific system recommendations such as if thecollection increases by more than n% then up-date the IDF values.To explore the sensitivity we compared theamount of change in IDF values for various sub-sets of the corpus.
IDF values were calculatedusing samples of 10%, 20%, ?, 90% and com-pared with the global IDF.
We stratified sam-pling such that the 10% sample used term fre-quencies in 10% of the ACHRE4 articles, 10%of the BICHAW articles, etc.
To control forvariations in the corpus, we repeated each sample10 times and took the average from the 10 runs.To explore the sensitivity we compared theglobal IDF in Equation 1 with the local sample,where N was the average number of documentsin the sample and ni was the average term fre-quency for each stemmed term in the sample.In addition to exploring sensitivity with re-spect to a random subset, we were interested inlearning more about the relationship between theglobal IDF and the IDF calculated on a journalsub-set.
To explore these differences, we com-pared the global IDF with local IDF where Nwas the number of documents in each journaland ni was the number of times the stemmedterm appears in the text of that journal.4 Results and DiscussionThe 100830 full text documents comprised2,001,730 distinct unstemmed terms, and1,391,763 stemmed terms.
All experiments re-ported in this paper consider stemmed terms.4.1 Raw frequency comparisonThe dimensionality of the document, sentence,and terms spaces varied greatly, with 100830documents, 16.5 million sentences, and 2.0 mil-lion distinct unstemmed terms (526.0 million intotal), and 1.39 million distinct stemmed terms.Figure 2A shows the correlation between the fre-quency of a term in the document space (x) andthe average frequency of the same set of terms inthe sentence space (y).
For example, the averagenumber of sentences for the set of terms that ap-pear in 30 documents is 74.6.
Figure 2B com-pares the document (x) and average term freq-FrequencyA - Document vs. Sentence1.0E+01.0E+11.0E+21.0E+31.0E+41.0E+51.0E+61.0E+71.0E+81.0E+00 1.0E+01 1.0E+02 1.0E+03 1.0E+04 1.0E+05 1.0E+06Document Frequency (Log scale)AverageSentenceFrequency(Logscale)B - Document vs. Term1.0E+01.0E+11.0E+21.0E+31.0E+41.0E+51.0E+61.0E+71.0E+81.00E+00 1.00E+01 1.00E+02 1.00E+03 1.00E+04 1.00E+05 1.00E+06Document Frequency (Log scale)AverageTermFrequency(Logscale)C - Sentence vs.Term1.0E+01.0E+11.0E+21.0E+31.0E+41.0E+51.0E+61.0E+71.0E+81.0E+00 1.0E+01 1.0E+02 1.0E+03 1.0E+04 1.0E+05 1.0E+06 1.0E+07Sentence Frequency (Log scale)AverageTermFrequency(Logscale)Standard Deviation ErrorD - Document vs. Sentence1.0E+01.0E+11.0E+21.0E+31.0E+41.0E+51.0E+61.0E+0 1.0E+1 1.0E+2 1.0E+3 1.0E+4 1.0E+5Document Frequency (Log scale)SentenceStandardDeviation(Logscale)E - Document vs. Term1.0E+01.0E+11.0E+21.0E+31.0E+41.0E+51.0E+61.0E+0 1.0E+1 1.0E+2 1.0E+3 1.0E+4 1.0E+5Document Frequency (Log scale)TermStandardDeviation(Logscale)F - Sentence vs. Term1.0E+01.0E+11.0E+21.0E+31.0E+41.0E+51.0E+61.0E+0 1.0E+1 1.0E+2 1.0E+3 1.0E+4 1.0E+5Sentence Frequency (Log scale)TermStandardDeviation(Logscale)Figure 2.
Raw frequency correlation between document, sentence, and term spaces.604A ?
JACSAT Document Space1.0E+01.0E+11.0E+21.0E+31.0E+41.0E+51.0E+61.E+0 1.E+1 1.E+2 1.E+3 1.E+4 1.E+5 1.E+6 1.E+7 1.E+8Word Rank (log scale)WordFrequency(logscale)ActualPredicted(K=89283, m=1.6362)B ?
JACSAT Sentence Space1.0E+01.0E+11.0E+21.0E+31.0E+41.0E+51.0E+61.E+0 1.E+1 1.E+2 1.E+3 1.E+4 1.E+5 1.E+6 1.E+7 1.E+8Word Rank (log scale)WordFrequency(logscale) ActualPredicted (K=185818, m=1.7138)C ?
JACSAT Term Space1.0E+01.0E+11.0E+21.0E+31.0E+41.0E+51.0E+61.E+0 1.E+1 1.E+2 1.E+3 1.E+4 1.E+5 1.E+6 1.E+7 1.E+8Word Rank (log scale)WordFrequency(logscale)ActualPredicted(K=185502, m=1.7061)D - Slope Coefficients between document, sen-tence, and term spaces for each journal, when fitto the power law K=jm-1.85-1.80-1.75-1.70-1.65-1.60-1.55-1.80 -1.70 -1.60 -1.50Document SlopeSentenceorTermSlopeSentenceTerm JACSATFigure 3.
Zipf?s Law comparison.
A through C show the power law distribution for the journal JAC-SAT in the document (A), sentence (B), and term (C) event spaces.
Note the predicted slope coeffi-cients of 1.6362, 1.7138 and 1.7061 respectively).
D shows the document, sentence, and term slopecoefficients for each of the 25 journals when fit to the power law K=jm, where j is the rank.quency (y) These figures suggest that the docu-ment space differs substantially from the sen-tence and term spaces.
Figure 2C shows the sen-tence frequency (x) and average term frequency(y), demonstrating that the sentence and termspaces are highly correlated.Luhn proposed that if terms were ranked bythe number of times they occurred in a corpus,then the terms of interest would lie within thecenter of the ranked list (Luhn 1958).
Figures2D, E and F show the standard deviation be-tween the document and sentence space, thedocument and term space and the sentence andterm space respectively.
These figures suggestthat the greatest variation occurs for importantterms.4.2 Zipf?s Law comparisonZipf?s Law states that the frequency of termsin a corpus conforms to a power law distributionK/j?
where ?
is close to 1 (Zipf, 1949).
We calcu-lated the K and ?
coefficients for each journaland language model combination using thebinning method proposed in (Adamic, 2000).Figures 3A-C show the actual frequencies, andthe power law fit for the each language model injust one of the 25 journals (jacsat).
These and theremaining 72 figures (not shown) suggest thatZipf?s Law holds in all event spaces.Zipf Law states that ?
should be close to -1.
Inour corpus, the average ?
in the document spacewas -1.65, while the average ?
in both the sen-tence and term spaces was -1.73.Figure 3D compares the document slope (x)coefficient for each of the 25 journals with thesentence and term spaces coefficients (y).
Thesefindings are consistent with a recent study thatsuggested ?
should be closer to 2 (Cancho 2005).Another study found that term frequency rankdistribution was a better fit Zipf?s Law when theterm space comprised both words and phrases(Ha et al 2002).
We considered only stemmedterms.
Other studies suggest that a Poisson mix-ture model would better capture the frequencyrank distribution than the power model (Churchand Gale, 1995).
A comprehensive overview ofusing Zipf?s Law to model language can befound in (Guiter and Arapov, 1982).6054.3 Direct IDF, ISF, and ITF comparisonOur third experiment was to compare the threelanguage models directly.
Figure 4A shows theaverage, minimum and maximum ISF value foreach rounded IDF value.
After fitting a regres-sion line, we found that ISF correlates well withIDF, but that the average ISF values are 5.57greater than the corresponding IDF.
Similarly,ITF correlates well with IDF, but the ITF valuesare 10.45 greater than the corresponding IDF.Ay = 1.0662x + 5.5724R2 = 0.99740510152025301 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18IDFISFAvgMinMaxBy = 1.0721x + 10.452R2 = 0.99720510152025301 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18IDFITFAvgMinMaxCy = 1.0144x + 4.6937R2 = 0.99960510152025301 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25ISFITFAvgMinMaxFigure 4.
Pair-wise IDF, ISF, and ITF com-parisons.It is little surprise that Figure 4C reveals astrong correlation between ITF and ISF, giventhe correlation between raw frequencies reportedin section 4.1.
Again, we see a high correlationbetween the ISF and ITF spaces but that the ITFvalues are on average 4.69 greater than theequivalent ISF value.
These findings suggeststhat simply substituting ISF or ITF for IDFwould result in a weighting scheme where thecorpus weights would dominate the weights as-signed to query in the vector based retrievalmodel.
The variation appears to increase athigher IDF values.Table 2 (see over) provides example stemmedterms with varying frequencies, and their corre-sponding IDF, ISF and ITF weights.
The mostfrequent term ?the?, appears in 100717 docu-ments, 12,771,805 sentences and 31,920,853times.
In contrast, the stemmed term ?electro-chem?
appeared in only six times in the corpus,in six different documents, and six different sen-tences.
Note also the differences between ab-stracts, and the full-text IDF (see section 4.4).4.4 Abstract vs full text comparisonAlthough abstracts are often easier to obtain, theavailability of full-text documents continues toincrease.
In our fourth experiment, we comparedthe language used in abstracts with the languageused in the full-text of a document.
We com-pared the abstract and non-abstract terms in eachof the three language models.Not all of the documents distinguished the ab-stract from the body.
Of the 100,830 documents,92,723 had abstracts and 97,455 had sectionsother than an abstract.
We considered only thosedocuments that differentiated between sections.Although the number of documents did not differgreatly, the vocabulary size did.
There were214,994 terms in the abstract vocabulary and1,337,897 terms in the document body, suggest-ing a possible difference in the distribution ofterms, the log(ni) component of IDF.Figure 5 suggests that language used in an ab-stract differs from the language used in the bodyof a document.
On average, the weights assignedto stemmed terms in the abstract were higherthan the weights assigned to terms in the body ofa document (space limitations preclude the inclu-sion of the ISF and ITF figures).0246810121416181 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18Global IDFAverageabstract/Non-abstractIDFAbstractNon-AbstractFigure 5.
Abstract and full-text IDF comparedwith global IDF.606Document (IDF) Sentence (ISF) Term (ITF)Word Abs NonAbs All Abs NonAbs All Abs NonAbs Allthe 1.014 1.004 1.001 1.342 1.364 1.373 4.604 9.404 5.164chemist 11.074 5.957 5.734 13.635 12.820 12.553 22.838 17.592 17.615synthesis 14.331 11.197 10.827 17.123 18.000 17.604 26.382 22.632 22.545eletrochem 17.501 15.251 15.036 20.293 22.561 22.394 29.552 26.965 27.507Table 2.
Examples of IDF, ISF and ITF for terms with increasing IDF.4.5 IDF sensitivityThe stability of the corpus weighting scheme isparticularly important in a dynamic environmentsuch as the web.
Without an understanding ofhow IDF behaves, we are unable to make a prin-cipled decision regarding how often a systemshould update the corpus-weights.To measure the sensitivity of IDF we sampledat 10% intervals from the global corpus as out-lined in section 3.
Figure 6 compares the globalIDF with the IDF from each of the 10% samples.The 10% samples are almost indiscernible fromthe global IDF, which suggests that IDF valuesare very stable with respect to a random subset ofarticles.
Only the 10% sample shows any visibledifference from the global IDF values, and eventhen, the difference is only noticeable at higherglobal IDF values (greater than 17 in our cor-pus).0246810121416181 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18IDF of Total CorpusAverageIDFof StemmedTerms102030405060708090% of Total CorpusFigure 6 ?
Global IDF vs random sample IDF.In addition to a random sample, we comparedthe global based IDF with IDF values generatedfrom each journal (in an on-line environment, itmay be pertinent to partition pages into academicor corporate URLs or to calculate term frequen-cies for web pages separately from blog andwikis).
In this case, N in equation (1) was thenumber of documents in the journal and ni wasthe distribution of terms within a journal.If the journal vocabularies were independent,the vocabulary size would be 4.1 million for un-stemmed terms and 2.6 million for stemmedterms.
Thus, the journals shared 48% and 52% oftheir vocabulary for unstemmed and stemmedterms respectively.Figure 7 shows the result of this comparisonand suggests that the average IDF within a jour-nal differed greatly from the global IDF value,particularly when the global IDF value exceedsfive.
This contrasts sharply with the randomsamples shown in Figure 6.0510151 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18Global IDFAverageLocalIDFACHRE4ANCHAMBICHAWBIPRETBOMAF6CGDEFUCMATEXESTHAGIECREDINOCAJJACSATJAFCAUJCCHFFJCISD8JMCMARJNPRDFJOCEAHJPCAFHJPCBFKJPROBSMAMOBXMPOHBPNALEFDOPRDFKORLEF7Figure 7 ?
Global IDF vs local journal IDF.At first glance, the journals with more articlesappear to correlated more with the global IDFthan journals with fewer articles.
For example,JACSAT has 14,400 documents and is most cor-related, while MPOHBP with 58 documents isleast correlated.
We plotted the number of arti-cles in each journal with the mean squared error(figure not shown) and found that journals withfewer than 2,000 articles behave differently tojournals with more than 2,000 articles; however,the relationship between the number of articles inthe journal and the degree to which the languagein that journal reflects the language used in theentire collection was not clear.5 ConclusionsWe have compared the document, sentence, andterm spaces along several dimensions.
Resultsfrom our corpus of 100,830 full-text scientificarticles suggest that the difference between thesealternative spaces is both theoretical and practi-607cal in nature.
As users continue to demand in-formation systems that provide sub-documentretrieval, the need to model language at the sub-document level becomes increasingly important.The key findings from this study are:(1) The raw document frequencies are con-siderably different to the sentence andterm frequencies.
The lack of a directcorrelation between the document andsub-document raw spaces, in particulararound the areas of important terms, sug-gest that it would be difficult to performa linear transformation from the docu-ment to a sub-document space.
In con-trast, the raw term frequencies correlatewell with the sentence frequencies.
(2) IDF, ISF and ITF are highly correlated;however, simply replacing IDF with theISF or ITF would result in a weightingscheme where the corpus weight domi-nated the weights assigned to query anddocument terms.
(3) IDF was surprisingly stable with respectto random samples at 10% of the totalcorpus.
The average IDF values based ononly a 20% random stratified samplecorrelated almost perfectly to IDF valuesthat considered frequencies in the entirecorpus.
This finding suggests that sys-tems in a dynamic environment, such asthe Web, need not update the global IDFvalues regularly (see (4)).
(4) In contrast to the random sample, thejournal based IDF samples did not corre-late well to the global IDF.
Further re-search is required to understand thesefactors that influence language usage.
(5) All three models (IDF, ISF and ITF) sug-gest that the language used in abstracts issystematically different from the lan-guage used in the body of a full-text sci-entific document.
Further research is re-quired to understand how well the ab-stract tested corpus-weighting schemeswill perform in a full-text environment.ReferencesLada A. Adamic 2000 Zipf, Power-laws, and Pareto -a ranking tutorial.
[Available fromhttp://www.parc.xerox.com/istl/groups/iea/papers/ranking/ranking.html]Ricardo Baeza-Yates, and Berthier Ribeiro-Neto 1999Modern Information Retrieval: Addison Wesley.Cancho, R. Ferrer 2005 The variation of Zipfs Law inhuman language.
The European Physical Journal B44 (2):249-57.Kenneth W Church and William A. Gale 1999 Inversedocument frequency: a measure of deviations fromPoisson.
NLP using very large corpora, KluwerAcademic Publishers.Kenneth W Church.and William A. Gale 1995 Pois-son mixtures.
Natural Language Engineering, 1(2):163-90.H.
Guiter and M Arapov 1982.
Editors Studies onZipf's Law.
Brochmeyer, Bochum.Jaap Kamps, Maarten De Rijke, and BorkurSigurbjornsson 2005 The Importance of lenghtnormalization for XML retrieval.
Information Re-trieval 8:631-54.Le Quan Ha, E.I.
Sicilia-Garcia, Ji Ming, and F.J.Smith 2002 Extension of Zipf's Law to words andphrases.
19th International Conference on Compu-tational linguistics.Hans P. Luhn 1958 The automatic creation of litera-ture abstracts IBM Journal of Research and Devel-opment 2 (1):155-64.Joseph P Pickett et al 2000 The American Heritage?Dictionary of the English Language.
Fourth edi-tion.
Edited by H. Mifflin.Martin F. Porter 1980 An Algorithm for Suffix Strip-ping.
Program, 14 (3).
130-137.Stephen Robertson 2004 Understanding inversedocument frequency: on theoretical arguments forIDF.
Journal of Documentation 60 (5):503-520.Gerard Salton and Christopher Buckley 1988 Term-weighting approaches in automatic text retrieval.Information Processing & Management, 24(5):513-23.Claude E. Shannon 1948 A Mathematical Theory ofCommunication Bell System Technical Journal.
27379?423 & 623?656.Karen Sparck Jones, Steve Walker, and StephenRobertson 2000 A probabilistic model of informa-tion retrieval: development and comparative ex-periments Part 1.
Information Processing & Man-agement, 36:779-808.Karen Sparck Jones 1972 A statistical interpretationof term specificity and its application in retrieval.Journal of Documentation, 28:11-21.George Kingsley Zipf 1949 Human behaviour and theprinciple of least effort.
An introduction to humanecology, 1st edn.
Edited by Addison-Wesley.
Cam-bridge, MA.608
