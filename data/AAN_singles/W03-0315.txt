Efficient Optimization for Bilingual Sentence AlignmentBased on Linear RegressionBing ZhaoLanguage TechnologiesInstituteCarnegie Mellon Universitybzhao@cs.cmu.eduKlaus ZechnerEducational Testing ServiceRosedale Road, Princeton,NJ 08541kzechner@ets.orgStephan VogelLanguage TechnologiesInstituteCarnegie Mellon Universityvogel+@cs.cmu.eduAlex WaibelLanguage TechnologiesInstituteCarnegie Mellon Universityahw@cs.cmu.eduAbstractThis paper presents a study on optimizing sen-tence pair alignment scores of a bilingual sen-tence alignment module.
Five candidatescores based on perplexity and sentencelength are introduced and tested.
Then a linearregression model based on those candidates isproposed and trained to predict sentence pairs?alignment quality scores solicited from humansubjects.
Experiments are carried out on dataautomatically collected from Internet.
Thecorrelation between the scores generated bythe linear regression model and the scoresfrom human subjects is in the range of the in-ter-subject agreement score correlations.
Pear-son's correlation ranges from 0.53 up to 0.72in our experiments.1 IntroductionIn many instances, multilingual natural languagesystems like machine translation systems are developedand trained on parallel corpora.
When faced with a dif-ferent, unseen text genre, however, translation perform-ance usually drops noticeably.
One way to remedy thissituation is to adapt and retrain the system parametersbased on bilingual data from the same source or at leasta closely related source.
A bilingual sentence alignmentprogram (Gale and Church, 1991, and Brown et al,1991) is the crucial part in this adaptation procedure, inthat it collects bilingual document pairs from the Inter-net, and identifies sentence pairs, which should have ahigh likelihood of being correct translations of eachother.
The set of identified bilingual parallel sentencepairs is then added to the training set for parameter re-estimation.As is well known, text mined from the Internet isvery noisy.
Even after careful html parsing and filteringfor text size and language, the text from comparablehtml-page pairs still contains mismatches of content ornon-parallel junk text, and the sentence order can be toodifferent to be aligned.
Together with a large mismatchof vocabulary, the aligned sentence pairs, which areextracted from these collected comparable html-pagepairs, contain a number of low translation qualityalignments.
These need to be removed before the re-training of the MT system.In this paper, we present an approach to automati-cally optimizing the alignment scores of such a bilingualsentence alignment program.
The alignment score is acombination (by linear regression) of two word transla-tion lexicon scores and three sentence length scores andpredicts the translation quality scores from a set of hu-man annotators.
We also present experiments analyzinghow many different human scorers are needed for goodprediction and also how many sentence pairs should bescored per human annotator.The paper is structured as follows: in section 2, thetext mining system is briefly described.
In section 3,five sentence alignment models based on lexical infor-mation and sentence length are explained.
In section 4, aregression model is proposed to combine the five mod-els to get further improvement in predicting alignmentquality.
We describe alignment experiments in section5, focusing on the correlation between the alignmentscores predicted by the sentence alignment models andby humans.
Conclusions are given in section 6.2 System of Mining Parallel TextOne crucial component of statistical machine trans-lation (SMT) system is the parallel text mining fromInternet.
Several processing modules are applied to col-lect, extract, convert, and clean the text from Internet.The components in our system include:?
A web crawler, which collects potential parallelhtml documents based on link information follow-ing (Philip Resnik 1999);?
A bilingual html parser (based on flex for effi-ciency), which is designed for both Chinese andEnglish html documents.
The paragraphs?
bounda-ries within the html structure are kept.?
A character encoding detector, which judges if theChinese html document is GB2312 encoding orBIG5 encoding.?
An encoding converter, which converts the BIG5documents to GB2312 encoding.?
A language identifier to ensure that source and tar-get documents are both of the proper language.
(Noord?s Implementation).?
A Chinese word segmenter, which parses the Chi-nese strings into Chinese words.?
A document alignment program, which judges ifthe document pair is close translation candidates,and filters out those non-translation pairs.?
A sentence boundary detector, which is based onpunctuation and capitalized characters;?
And the key component, a sentence alignment pro-gram, which aligns and extracts potential parallelsentence pairs from the candidate document pairs.After sentence alignment, each candidate of a par-allel sentence pair is then re-scored by the regressionmodels (to be described in section 5).
These scores areused to judge the quality of the aligned sentences.
Thusone can select the aligned sentence pairs, which havehigh alignment quality scores, to re-estimate the sys-tem?s parameters.2.1 Sentence AlignmentOur sentence alignment program uses IBM Model-1based perplexity (section 2.2) to calculate the similarityof each sentence pair.
Dynamic programming is appliedto find Viterbi path for sentence alignments of the bilin-gual comparable document pair.
In our dynamic pro-gramming implementation, we allow for sevenalignment types between English and Chinese sentences:?
1:1 ?
exact match, where one sentence is the trans-lation of the other one;?
2:2 ?
the break point between two sentences in thesource document is different from the segmentationin the target document.
E.g.
part of sentence one inthe source might be translated as part of the secondsentence in the target;?
2:1, 1:2, and 3:1 ?
these cases are similar to thecase before: they handle differences in how a text issplit into sentences.
The case 1:3 has not been usedin the final configuration of the system, as this typedid not occur in any significant number;?
1:0 (deletion) and (0:1) insertion ?
a sentence in thesource document is missing in the translation orvice versa.The deletion and insertion types are discarded, andthe remaining types are extracted to be used as potentialparallel data.
In general, one Chinese sentence corre-sponds to several English sentences.
In (Bing andStephan, 2002), experiments on a 10-year XinHua newsstory collection from the Linguistic Data Consortium(LDC) show that alignment types like (2:1) and (3:1)are common, and this 7-type alignment is shown to bereliable for English-Chinese sentence alignment.
How-ever, only a small part of the whole 10-year collectionwas pre-aligned (Xiaoyi, 1999) and extracted for sen-tence alignment.The picture can be very different when directly min-ing the data from Internet.
Due to the mismatch betweenthe training data and the data collected from Internet,the vocabulary coverage can be very low; the data isvery noisy; and the data aligned is not strictly parallel.The percentage of alignment types of insertion (0:1) anddeletion (1:0) become very high as shown in section 5.The aligned sentence pairs are subject to many align-ment errors.
The alignment errors are not desired in there-training of the system, and need to be removed.Though the sentence alignment outputs a score fromViterbi path for each of the aligned sentence pairs, thisscore is only a rough estimation of the alignment quality.A more reliable re-scoring of the data is desirable toestimate the alignment quality as a post processing stepto filter out the errors and noise from the aligned data.2.2 Statistical Translation LexiconWe use a statistical translation lexicon known as IBMModel-1 in (Brown et al, 1993) for both efficiency andsimplicity.In our approach, Model-1 is the conditional probabil-ity that a word f in the source language is translatedgiven word e in the target language, t(f|e).
This prob-ability can be reliably estimated using the expectation-maximization (EM) algorithm (Cavnar, W. B. and J. M.Trenkle, 1994).Given training data consisting of parallel sen-tences: }..1),,{( )()( Sief ii = , our Model-1 training fort(f|e) is as follows:?=?=Sssse efefceft1)()(1 ),;|()|( ?Where 1?e?
is a normalization factor such that0.1)|( =?jj eft),;|( )()( ss efefc denotes the expected number of timesthat word e connects to word f.??
?====liimjjlkkss eeffefteftefefc111)()( ),(),()|()|(),;|( ?
?With the conditional probability t(f|e), the probabilityfor an alignment of foreign string F given English stringE is in (1):?
?= =+=mjniijm eftlEFP 1 0)|()1(1)|(  (1)The probability of alignment F given E: )|( EFP isshown to achieve the global maximum under this EMframework as stated in (Brown et al,1993).In our approach, equation (1) is further normalizedso that the probability for different lengths of F is com-parable at the word level:mmjniijm eftlEFP/11 0)|()1(1)|(?????
?+= ?
?= =(2)The alignment models described in (Brown et al,1993) are all based on the notion that an alignmentaligns each source word to exactly one target word.This makes this type of alignment models asymmetric.Thus by using the conditional probability t(e|f) trans-lation lexicon trained from English (source) to Chinese(target), different aspects of the bilingual lexicalinformation can be captured.
A similar probability to (2)can be defined based on this reverse translation lexicon:nminjjim fetlFEP/11 0)|()1(1)|(?????
?+= ?
?= =(3)Starting from the Hong Kong news corpora providedby LDC, we trained the translation lexicons to be usedin the parallel sentence alignment.
Each sentence pairhas a perplexity, which is calculated using the minus logof the probability eg.
equation (2).3 Alignment ModelsThe alignment model is aimed at automatically pre-dicting the alignment scores of a bilingual sentencealignment program.
By scoring the alignment quality ofthe sentence pairs, we can filter out those mis-alignedsentence pairs, and save our SMT system from beingcorrupted by mis-aligned data.3.1 Lexicon Based ModelsIt is necessary to include lexical features in thealigned quality evaluation.
One way is to use the trans-lation lexicon based perplexity as in our sentencealignment program.For each of the aligned sentence pairs, the sentencealignment generated a score, which is solely based onequation (2).
Using this score only, we can do a simplefiltering by setting a threshold of perplexity.
The sen-tence pairs which have a higher perplexity than thethreshold will be removed.
However the perplexitybased on (2) is definitely not discriminative enough toevaluate the quality of aligned sentence pairs.In our experiment, it showed that perplexity (3) hasmore discriminative power in judging the quality of thealigned sentence pairs for Chinese-English sentencealignment.
It is also possible that equation (2) is moresuitable for other language pairs.
Both (2) and (3) areapplied in our sentence alignment quality judgment,which is to be explained in section 4.3.2 Sentence Length ModelsAs was shown in the sentence alignment literature(Church, K.W.
1993), the sentence length ratio is also avery good indication of the alignment of a sentence pairfor languages from a similar family such as French andEnglish.
For language pairs from very different familiessuch as Chinese and English, the sentence length ratio isalso a good indication of alignment quality as shown inour experiments.For the language pair of Chinese and English, thesentence length can be defined in several different ways.3.2.1 Sentence LengthIn general, a Chinese sentence does not have wordboundary information; so one way to define Chinesesentence length is to count the number of bytes of thesentence.
Another way is to first segment the Chinesesentence into words (section 3.2.2) and count how manywords are in the sentence.
For English sentences, wecan similarly define the length in bytes and in words.The length ratio is assumed to be a Gaussian distri-bution.
The mean and variance are calculated from theparallel training corpus, which, in our case, is the HongKong parallel corpus with 290K parallel sentence pairs.3.2.2 A Chinese Word SegmenterThe word segmenter for Chinese is to parse the Chi-nese string into words.
Different word segmenters cangenerate different numbers of words for the same Chi-nese sentence.There are many word segmenters publicly available.In our experiments, we applied a two-pass strategy tosegment the word according to the dictionary of theLDC bilingual dictionary of Chinese-English.
The two-pass started first from left to right, and then from rightback to left, to calculate the maximum word frequencyand select one best path to segment the words.In general, the sentence length is not sensitive to thesegmenters used.
But for reliability, we want each seg-mented word can have an English translation, thus weused the LDC bilingual dictionary as a reference wordlist for segmentation.3.2.3 Sentence Length ModelAssume the alignment probability of ),|( tsAP  isonly related to the length of source sentence s and targetsentence t:|))||,(|(~|)||(|~||)||,|||||(|~),||||(|),|(tsPtsPtstsPtstsPtsAP?=?=?=?=where || s and || t are the sentence lengths of s and t.The difference of the length |)||,(| ts?
is assumedto be a Gaussian distribution (Church, K.W.
1993) andcan be normalized as follows:)1,0(~)1|(|||||2Nscst?
?+?=  (4)where c is a constant indicating the mean length ratiosbetween source and target sentences and 2?
is the vari-ance of the length ratios.In our case, we applied three length models de-scribed in the following Table 1:Table 1.
Three Length Models descriptionL-1 Both English and Chinese sentence are meas-ured in bytesL-2 Both English and Chinese sentence are meas-ured in wordsL-3 English sentence is measured in words andChinese sentence is measured in bytesThe means and 2?
of the length ratios for each of thelength models are calculated from Hong Kong newsparallel corpus.
The statistics of the three sentencelength models are shown in Table 2.Table 2.
Sentence length ratio statisticsL-1  L-2 L-3:Mean 1.59 1.01 0.33Var 3.82 0.79 0.71In general, the smaller the variance, the better thesentence length model can be.
From Table 2 we observethat the bytes based length ratio model has significantlylarger variance (3.82) than the other two models (L-2:0.79, L-3: 0.71).
This means L1 is not as reliable as L2and L3.
Both L2 and L3 have similar variance, whichindicates measuring English sentences in words willentail smaller variance in length model; measuring Chi-nese sentences in bytes or words entails only a slightdifference in variance.
This also indicates that the lengthmodel is not so sensitive to the Chinese word segmenterapplied.
L-1, L-2 and L-3 capture the length relationshipof parallel sentence in different views.
Their modelingpower has overlap, but they also compensate each otherin capturing the parallel characteristics of good transla-tion quality.
A combination of these models can poten-tially bring further improvement, which is shown in ourexperiment in section 6.4 Regression ModelRather than doing a binary decision (classification) thatthe aligned sentence pair is either good or not, the re-gression can give a confidence score indicating howgood the alignment can be, thus offering more flexibil-ity in decisions.
Predicting the alignment quality usingthe candidate models is considered as a regression prob-lem in that different scores are combined together.There are many ways such as genetic programming,to combine the candidate models, and regression is oneof the straight forward and efficient ones.
So in thiswork, we explored linear regression.4.1 Candidate ModelsWe have five candidate models described in section3.
They are: PP1, the perplexity based on the word pairconditional probability p(f|e) in equation (2); PP2, theperplexity based on the reverse word pair conditionalprobability p(e|f) in equation (3); L-1, Length ratiomodel measured in bytes (mean=1.59, var=3.82); L-2,length ratio model measured in words (mean=1.01,var=0.79); L-3, length ratio model, where the Englishsentence is measured in words and the Chinese sentenceis measured in bytes (mean=0.33, var=0.71).
These fivemodels capture different aspects of the aligned qualityof the sentence pair.
The idea is to combine these fivemodels together to get better prediction of the alignedquality.Linear regression is applied to combine these fivemodels.
It is trained from the observation of the fivemodels together with the label of human judgment on atraining set.4.2 Regression Model TrainingThe linear regression model tries to discover theequation for a line that most nearly fits the given data(Trevor Hastie et al 2001).
That linear equation is thenused to predict values for the data.Now given human subject judgment of the alignedtranslation quality of sentence pairs, we can train a re-gression model based on the five models we describedin section 4.1 under the objective of least square errors.The human evaluation is measures translation qual-ity of aligned pairs on a discrete 6-point scale between 1(very bad) and 5 (perfect translation).
The score 0 wasused for alignments that were not genuine translatione.g., both sentences were from the same language.
Wewill use n for the number of total sentence pairs labeledby humans and used in training.Let A= [PP1, PP2, L-1, L-2, L-3] be the machine-generated scores for each of the sentence pairs.
In ourcase, A is a 5?n  matrix.Let H= [Human-Judgment-Score] be the humanevaluation of the sentence pairs on a 6-point scale.
Inour case, H is a 1?n  matrix.In linear regression modeling, a linear transforma-tion matrix W should satisfy the least square error crite-rion:||}{||min* HAWWw?=  (5)where W is in fact a 5x1 weight matrix.
The equationcan be solved as:HAAAW TT 1* )( ?=  (6)The inverse of matrix AAT  is usually calculated usingsingular vector decomposition (SVD).
After W is calcu-lated, the predicted score from the regression model is:*' AWH =  (7)where 'H  is the final predicted alignment quality scoreof the regression model.
We can also view 'H  as aweighted sum of the five models shown in section 4.1.The calculation of 'H  reduces to a linear weightedsummation, which is very efficient to compute.5 Experiments1500 pairs of comparable html document pairs wereobtained from bilingual web pages crawled from Inter-net.
After preprocessing, filtering, and sentence align-ment, the alignment types were distributed as shown inTable 3.
Ignoring the alignment type of insertion (0:1)and deletion (1:0), we extracted around 5941 parallelsentences.Table 3.
Alignment types?
distribution of mineddata from noisy web data crawled1:0 0:1 1:1 2:1 1:2 2:2 3:1% 23.7 41.9 29.4 1.99 0.01 0.02 2.79From Table 3, we see the data is very noisy, con-taining a large portion of insertions (23.7%) and dele-tions (41.9%).
This is very different from the LDCXinHua pre-aligned collection provided by LDC, whichis relatively clean.For this set of English-Chinese bilingual sentences,we randomly selected 200 sentence pairs, focusing onViterbi alignment scores below 12.0 from sentencealignment, which was an empirically determinedthreshold (The alignment scores here were purely re-flecting the Model-1 parameters using equation (2)).Three human subjects then had to score the 'translationquality' of every sentence pair, using a 6 point scaledescribed in section 4.2.
We further excluded very shortsentences from consideration and evaluated 168 remain-ing sentences.Pearson R correlation is applied to calculate the mag-nitude of the association between two variables (human-human or human-machine in our case) that are on aninterval or ratio scale.
The correlation coefficients(Pearson R) between human subjects were in Table 4(all are statistically significant):Table 4.
Correlation between Human SubjectsH2 H3H1 0.786 0.615H2 ---- 0.568Overall, more than 2/3 of the human scores are identicalor differ by only 1 (between subjects).For the automatic score prediction, the five compo-nent scores described in section 4.1 are used, which arethen combined using a standard Linear Regression asdescribed in section 4.2.
Table 5 shows the correlationbetween alignment scores based on Model X and humansubjects' predicted quality scores:Table 5.
Correlation between optimization modelsand human subjectsModel human-1 human -2 human -3PP-1 .57 .53 .32PP-2 .60 .58 .46L-1 .42 .41 .30L-2 .46 .41 .40L-3 .40 .38 .29Na?ve .58 .56 .38Regression  .72 .68 .53The data we used in our training of the lexicon is HongKong news parallel data from LDC.
There are 290Kparallel sentence pairs, with 7 million words of Englishand 7.3 million Chinese words after segmentation.
TheIBM Model-1 for PP-1 and PP-2 are both trained using5 EM iterations.
The other three length models are alsocalculated from the same 290K sentence pairs.
Punctua-tion is removed before the calculation of all automaticscore prediction models.The regression model here is the standard linear re-gression using the observations from three human sub-jects as described in section 4.1.
The averageperformance of the regression model is shown in thebottom line of the above Table 5.
The average correla-0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5-0.500.511.522.533.544.5Average of 3-human judgement 5-scale score (5-best, 0-non)Pearson r Correlationtion varies from 0.53 upto 0.72, which shows that theregression model has a very strong positive correlationwith the human judgment.Also from Table 5, we see both lexicon based mod-els: PP-1 and PP-2 are better than the length models interm of correlation with human scorer.
Model PP-2 hasthe largest correlation, and is slightly better than PP-1.PP-2 is based on the conditional probability of p(e|f),which models the generation of an English word from aChinese word.
The vocabulary size of Chinese is usu-ally smaller than English vocabulary size, so this modelcan be more reliably estimated than the reverse directionof p(f|e).
This explains why PP-2 is slightly better thanPP-1.For sentence length models, we see L-2, for whichthe lengths of both the English sentence and the Chinesesentence are measured in words, has the best perform-ance among the three settings of a sentence lengthmodel.
This indicates that the length model measured inwords is more reliable.Also shown in Table 5, the na?ve interpolation ofthese different models, i.e.
just using each model withequal weight, resulted in lower correlation than the bestsingle alignment model.We also performed correlation experiments withvaried numbers of training sentences from either Hu-man-1/Human-2/Human-3 or from all of the three hu-man subjects.
We picked the first 30/60/90/120 labeledsentence pairs for training and saved the last 48 sen-tence pairs for testing.
The average performance of theregression model is as follows:Table 6.
Correlation between different training setsizes and human scorers.Trainingset sizeHuman-1 Human -2 Human ?330 .686 .639 .44760 .750 .707 .45290 .765 .721 .456120 .760 .721 .464The average correlation of the regression modelsshowed here increased noticeably when the training setwas increased from 30 sentence pairs to 90 sentencepairs.
More sentence pairs caused no or only marginalimprovements (esp.
for the third human subject).Figure 1 shows a scatter plot, which illustrates agood correlation (here: Pearson R=0.74) between ourregression model predictors and the human scorers.6 ConclusionIn this paper, we have demonstrated ways to effi-ciently optimize a sentence alignment module, such thatit is able to select aligned sentence pairs of high transla-tion quality automatically.
This procedure of alignmentscore optimization requires (a) a small number of hu-man subjects who annotate a set of about 100 sentencepairs each for translation quality; and (b) a set of align-ment scores, based on perplexity and sentence lengthratio, to be able to learn to predict the human scores.Based on the learned predictions, by means of linearregression, the alignment program can choose the bestsentence pair candidates to be included in the trainingdata for the SMT system re-estimation.Our experiments showed that, for Chinese-Englishlanguage pair, perplexity based on the reverse word pairconditional probability p(e|f) (PP-2) gives the most reli-able prediction among the five models proposed in thispaper; the regression model, which combines those fivemodels, give the best correlation between human scoreand automatic predictions.
Our approach needs only afairly limited number of human labeled sentences pairs,and is an efficient optimization of the sentencealignment system.Figure 1.
Correlation between regression model andhuman scorers, Pearson R=0.74.ReferencesBing Zhao, Stephan Vogel.
2002.
Adaptive ParallelSentences Mining from Web Bilingual News Collec-tion.
IEEE International Conference on Data Mining(ICDM 02) , pp.
745-748.
Japan.Brown, P., Lai, J. C., and Mercer, R. 1991.
AligningSentences in Parallel Corpora.
In Proceedings ofACL-91, Berkeley CA.
1991Cavnar, W. B. and J. M. Trenkle.
1994.
N-Gram-BasedText Categorization.
Proceedings of Third AnnualSymposium on Document Analysis and InformationRetrieval, Las Vegas, NV, UNLV Publica-tions/Reprographics, pp.
161-175, 11-13.Stanley Chen.
1993.
Aligning sentences in Bilingualcorpora using lexical information.
In proceedings ofthe 31st Annual Conference of the Association forcomputational linguistics, pages 9-16, Columbus,Ohio, June 1993Church, K. W. 1993.
Char_align: A Program for Align-ing Parallel Texts at the Character Level.
Proceed-ings of ACL-93, Columbus OH.Gale, W. A. and Church, K. W.  1991.
A Program forAligning Sentences in Bilingual Corpora.
In Pro-ceedings of ACL-91, Berkeley CA.
1991.Melamed, I.D.
1996.
A Geometric Approach to Map-ping Bitext Correspondence.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, Philadelphia, PA. 1996Noord?s Implementation of Textcat: http://odur.let.rug.nl/~vannoord/TextCat/index.htmlPeter F. Brown, Stephan A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
TheMathmatics of Statistical Machine Translation: Pa-rameter estimation.
Computational Linguistics, vol19, no.2 , pp.263-311.Philip Resnik.
1999.
Mining the Web for Bilingual Text.37th Annual Meeting of the Association for Computa-tional Linguistics (ACL'99), University of Maryland,College Park, Maryland.Trevor Hastie, Robert Tibshirani, Jerome Friedman.2001.
The Elements of Statistical Learning: DataMining, Inference and Prediction.
Springer Publisher.Xiaoyi Ma, Mark Y. Liberman, ?BITS: A Method forBilingual Text Search over the Web?.
MachineTranslation Summit VII, 1999
