Proceedings of the 6th Workshop on Statistical Machine Translation, pages 198?206,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsWider Context by Using Bilingual Language Models in Machine TranslationJan Niehues1, Teresa Herrmann1, Stephan Vogel2 and Alex Waibel1,21Institute for Anthropomatics, KIT - Karlsruhe Institute of Technology, Germany2 Language Techonolgies Institute, Carnegie Mellon University, USA1firstname.lastname@kit.edu 2lastname@cs.cmu.eduAbstractIn past Evaluations for Machine Translation ofEuropean Languages, it could be shown thatthe translation performance of SMT systemscan be increased by integrating a bilingual lan-guage model into a phrase-based SMT system.In the bilingual language model, target wordswith their aligned source words build the to-kens of an n-gram based language model.
Weanalyzed the effect of bilingual language mod-els and show where they could help to bet-ter model the translation process.
We couldshow improvements of translation quality onGerman-to-English and Arabic-to-English.
Inaddition, for the Arabic-to-English task, train-ing an extra bilingual language model on thePOS tags instead of the surface word formsled to further improvements.1 IntroductionIn many state-of-the art SMT systems, the phrase-based (Koehn et al, 2003) approach is used.
Inthis approach, instead of building the translation bytranslating word by word, sequences of source andtarget words, so-called phrase pairs, are used as thebasic translation unit.
A table of correspondencesbetween source and target phrases forms the transla-tion model in this approach.
Target language fluencyis modeled by a language model storing monolin-gual n-gram occurrences.
A log-linear combinationof these main models as well as additional featuresis used to score the different translation hypotheses.Then the decoder searches for the translation withthe highest score.A different approach to SMT is to use a stochas-tic finite state transducer based on bilingual n-grams (Casacuberta and Vidal, 2004).
This ap-proach was for example successfully applied by Al-lauzen et al (2010) on the French-English trans-lation task.
In this so-called n-gram approach thetranslation model is trained by using an n-gram lan-guage model of pairs of source and target words,called tuples.
While the phrase-based approach cap-tures only bilingual context within the phrase pairs,in the n-gram approach the n-gram model trained onthe tuples is used to capture bilingual context be-tween the tuples.
As in the phrase-based approach,the translation model can also be combined with ad-ditional models like, for example, language modelsusing log-linear combination.Inspired by the n-gram-based approach, we in-troduce a bilingual language model that extendsthe translation model of the phrase-based SMT ap-proach by providing bilingual word context.
In ad-dition to the bilingual word context, this approachenables us also to integrate a bilingual context basedon part of speech (POS) into the translation model.When using phrase pairs it is complicated to usedifferent kinds of bilingual contexts, since the con-text of the POS-based phrase pairs should be biggerthan the word-based ones to make the most use ofthem.
But there is no straightforward way to inte-grate phrase pairs of different lengths into the trans-lation model in the phrase-based approach, while itis quite easy to use n-gram models with differentcontext lengths on the tuples.
We show how we canuse bilingual POS-based language models to capturelonger bilingual context in phrase-based translation198systems.This paper is structured in the following way: Inthe next section, we will present some related work.Afterwards, in Section 3, a motivation for using thebilingual language model will be given.
In the fol-lowing section the bilingual language model is de-scribed in detail.
In Section 5, the results and ananalysis of the translation results is given, followedby a conclusion.2 Related WorkThe n-gram approach presented in Mari?o et al(2006) has been derived from the work of Casacu-berta and Vidal (2004), which used finite state trans-ducers for statistical machine translation.
In this ap-proach, units of source and target words are used asbasic translation units.
Then the translation model isimplemented as an n-gram model over the tuples.
Asit is also done in phrase-based translations, the dif-ferent translations are scored by a log-linear combi-nation of the translation model and additional mod-els.Crego and Yvon (2010) extended the approach tobe able to handle different word factors.
They usedfactored language models introduced by Bilmes andKirchhoff (2003) to integrate different word factorsinto the translation process.
In contrast, we use alog-linear combination of language models on dif-ferent factors in our approach.A first approach of integrating the idea presentedin the n-gram approach into phrase-based machinetranslation was described in Matusov et al (2006).In contrast to our work, they used the bilingual unitsas defined in the original approach and they did notuse additional word factors.Hasan et al (2008) used lexicalized triplets to in-troduce bilingual context into the translation pro-cess.
These triplets include source words from out-side the phrase and form and additional probabilityp(f |e, e?)
that modifies the conventional word prob-ability of f given e depending on trigger words e?
inthe sentence enabling a context-based translation ofambiguous phrases.Other approaches address this problem by inte-grating word sense disambiguation engines into aphrase-based SMT system.
In Chan and Ng (2007)a classifier exploits information such as local col-locations, parts-of-speech or surrounding words todetermine the lexical choice of target words, whileCarpuat and Wu (2007) use rich context featuresbased on position, syntax and local collocations todynamically adapt the lexicons for each sentenceand facilitate the choice of longer phrases.In this work we present a method to extend thelocally limited context of phrase pairs and n-gramsby using bilingual language models.
We keep thephrase-based approach as the main SMT frameworkand introduce an n-gram language model trained in asimilar way as the one used in the finite state trans-ducer approach as an additional feature in the log-linear model.3 MotivationTo motivate the introduction of the bilingual lan-guage model, we will analyze the bilingual contextthat is used when selecting the target words.
In aphrase-based system, this context is limited by thephrase boundaries.
No bilingual information outsidethe phrase pair is used for selecting the target word.The effect can be shown in the following examplesentence:Ein gemeinsames Merkmal aller extremenRechten in Europa ist ihr Rassismusund die Tatsache, dass sie das Einwan-derungsproblem als politischen Hebel be-nutzen.Using our phrase-based SMT system, we get thefollowing segmentation into phrases on the sourceside: ein gemeinsames, Merkmal, aller, extremenRechten.
That means, that the translation of Merk-mal is not influenced by the source words gemein-sames or aller.However, apart from this segmentation, otherphrases could have been conceivable for building atranslation:ein, ein gemeinsames, ein gemeinsames Merk-mal, gemeinsames, gemeinsames Merkmal, Merk-mal aller, aller, extremen, extremen Rechten andRechten.As shown in Figure 1 the translation for thefirst three words ein gemeinsames Merkmal into acommon feature can be created by segmenting itinto ein gemeinsames and Merkmal as done by the199Figure 1: Alternative Segmentationsphrase-based system or by segmenting it into ein andgemeinsames Merkmal.
In the phrase-based system,the decoder cannot make use of the fact that bothsegmentation variants lead to the same translation,but has to select one and use only this informationfor scoring the hypothesis.Consequently, if the first segmentation is cho-sen, the fact that gemeinsames is translated to com-mon does effect the translation of Merkmal only bymeans of the language model, but no bilingual con-text can be carried over the segmentation bound-aries.To overcome this drawback of the phrase-basedapproach, we introduce a bilingual language modelinto the phrase-based SMT system.
Table 1 showsthe source and target words and demonstrates howthe bilingual phrases are constructed and how thesource context stays available over segment bound-aries in the calculation of the language model scorefor the sentence.
For example, when calculating thelanguage model score for the word feature P ( fea-ture_Merkmal | common_gemeinsames) we can seethat through the bilingual tokens not only the previ-ous target word but also the previous source word isknown and can influence the translation even thoughit is in a different segment.4 Bilingual Language ModelThe bilingual language model is a standard n-gram-based language model trained on bilingual tokens in-stead of simple words.
These bilingual tokens aremotivated by the tuples used in n-gram approachesto machine translation.
We use different basic unitsfor the n-gram model compared to the n-gram ap-proach, in order to be able to integrate them into aphrase-based translation system.In this context, a bilingual token consists of a tar-get word and all source words that it is aligned to.More formally, given a sentence pair eI1 = e1...eIand fJ1 = f1...fJ and the corresponding word align-ment A = {(i, j)} the following tokens are created:tj = {fj} ?
{ei|(i, j) ?
A} (1)Therefore, the number of bilingual tokens in asentence equals the number of target words.
If asource word is aligned to two target words like theword aller in the example sentence, two bilingual to-kens are created: all_aller and the_aller.
If, in con-trast, a target word is aligned to two source words,only one bilingual token is created consisting of thetarget word and both source words.The existence of unaligned words is handled inthe following way.
If a target word is not alignedto any source word, the corresponding bilingual to-ken consists only of the target word.
In contrast, if asource word is not aligned to any word in the targetlanguage sentence, this word is ignored in the bilin-gual language model.Using this definition of bilingual tokens the trans-lation probability of source and target sentence andthe word alignment is then defined by:p(eI1, fJ1 , A) =J?j=1P (tj |tj?1...tj?n) (2)This probability is then used in the log-linear com-bination of a phrase-based translation system as anadditional feature.
It is worth mentioning that al-though it is modeled like a conventional languagemodel, the bilingual language model is an extensionto the translation model, since the translation for thesource words is modeled and not the fluency of thetarget text.To train the model a corpus of bilingual tokens canbe created in a straightforward way.
In the genera-tion of this corpus the order of the target words de-fines the order of the bilingual tokens.
Then we canuse the common language modeling tools to trainthe bilingual language model.
As it was done forthe normal language model, we used Kneser-Neysmoothing.4.1 Comparison to TuplesWhile the bilingual tokens are motivated by the tu-ples in the n-gram approach, there are quite somedifferences.
They are mainly due to the fact that the200Source Target Bi-word LM Probein a a_ein P(a_ein | <s>)gemeinsames common common_gemeinsames P(common_gemeinsames | a_ein, <s>)Merkmal feature feature_Merkmal P(feature_Merkmal | common_gemeinsames)of of_ P(of_ | feature_Merkmal)aller all all_aller P(all_aller | of_)aller the the_aller P(the_aller | all_aller, of_)extremen extreme extreme_extremen P(extreme_extremen)Rechten right right_Rechten P(right_Rechten | extreme_extremen)Table 1: Example Sentence: Segmentation and Bilingual Tokenstuples are also used to guide the search in the n-gramapproach, while the search in the phrase-based ap-proach is guided by the phrase pairs and the bilin-gual tokens are only used as an additional feature inscoring.While no word inside a tuple can be aligned toa word outside the tuple, the bilingual tokens arecreated based on the target words.
Consequently,source words of one bilingual token can also bealigned to target words inside another bilingual to-ken.
Therefore, we do not have the problems of em-bedded words, where there is no independent trans-lation probability.Since we do not create a a monotonic segmenta-tion of the bilingual sentence, but only use the seg-mentation according to the target word order, it isnot clear where to put source words, which have nocorrespondence on the target side.
As mentioned be-fore, they are ignored in the model.But an advantage of this approach is that we haveno problem handling unaligned target words.
Wejust create bilingual tokens with an empty sourceside.
Here, the placing order of the unaligned tar-get words is guided by the segmentation into phrasepairs.Furthermore, we need no additional pruning ofthe vocabulary due to computation cost, since this isalready done by the pruning of the phrase pairs.
Inour phrase-based system, we allow only for twentytranslations of one source phrase.4.2 Comparison to Phrase PairsUsing the definition of the bilingual language model,we can again have a look at the introductory examplesentence.
We saw that when translating the phraseein gemeinsames Merkmal using a phrase-based sys-tem, the translation of gemeinsames into commoncan only be influenced by either the preceeding ein# a or by the succeeding Merkmal # feature, butnot by both of them at the same time, since eitherthe phrase ein gemeinsames or the phrase gemein-sames Merkmal has to be chosen when segmentingthe source sentence for translation.
If we now lookat the context that can be used when translating thissegment applying the bilingual language model, wesee that the translation of gemeinsames into com-mon is on the one hand influenced by the translationof the token ein # a within the bilingual languagemodel probability P (common_gemeinsames | a_ein,<s>).On the other hand, it is also influenced by thetranslation of the word Merkmal into feature en-coded into the probability P (feature_Merkmal |common_gemeinsames).
In contrast to the phrase-based translation model, this additional model is ca-pable of using context information from both sidesto score the translation hypothesis.
In this way,when building the target sentence, the informationof aligned source words can be considered even be-yond phrase boundaries.4.3 POS-based Bilingual Language ModelsWhen translating with the phrase-based approach,the decoder evaluates different hypotheses with dif-ferent segmentations of the source sentence intophrases.
The segmentation depends on availablephrase pair combinations but for one hypothesistranslation the segmentation into phrases is fixed.This leads to problems, when integrating parallelPOS-based information.
Since the amount of differ-201ent POS tags in a language is very small compared tothe number of words in a language, we could man-age much longer phrase pairs based on POS tagscompared to the possible length of phrase pairs onthe word level.In a phrase-based translation system the averagephrase length is often around two words.
For POSsequences, in contrast, sequences of 4 tokens canoften be matched.
Consequently, this informationcan only help, if a different segmentation could bechosen for POS-based phrases and for word-basedphrases.
Unfortunately, there is no straightforwardway to integrate this into the decoder.If we now look at how the bilingual languagemodel is applied, it is much easier to integrate thePOS-based information.
In addition to the bilin-gual token for every target word we can generate abilingual token based on the POS information of thesource and target words.
Using this bilingual POStoken, we can train an additional bilingual POS-based language model and apply it during transla-tion.
In this case it is no longer problematic if thecontext of the POS-based bilingual language modelis longer than the one based on the word informa-tion, because word and POS sequences are scoredseparately by two different language models whichcover different n-gram lengths.The training of the bilingual POS language modelis straightforward.
We can build the corpus of bilin-gual POS tokens based on the parallel corpus ofPOS tags generated by running a POS tagger overboth source and target side of the initial parallel cor-pus and the alignment information for the respectivewords in the text corpora.During decoding, we then also need to know thePOS tag for every source and target word.
Sincewe build the sentence incrementally, we cannot usethe tagger directly.
Instead, we store also the POSsource and target sequences during the phrase ex-traction.
When creating the bilingual phrase pairwith POS information, there might be different pos-sibilities of POS sequences for the source and targetphrases.
But we keep only the most probable one foreach phrase pair.
For the Arabic-to-English trans-lation task, we compared the generated target tagswith the tags created by the tagger on the automatictranslations.
They are different on less than 5% ofthe words.Using the alignment information as well as thesource and target POS sequences we can then createthe POS-based bilingual tokens for every phrase pairand store it in addition to the normal phrase pairs.At decoding time, the most frequent POS tags in thebilingual phrases are used as tags for the input sen-tence and the translation is done based on the bilin-gual POS tokens built from these tags together withtheir alignment information.5 ResultsWe evaluated and analyzed the influence of the bilin-gual language model on different languages.
Onthe one hand, we measured the performance of thebilingual language model on German-to-English onthe News translation task.
On the other hand, weevaluated the approach on the Arabic-to-English di-rection on News and Web data.
Additionally, wepresent the impact of the bilingual language modelon the English-to-German, German-to-English andFrench-to-English systems with which we partici-pated in the WMT 2011.5.1 System DescriptionThe German-to-English translation system wastrained on the European Parliament corpus, NewsCommentary corpus and small amounts of addi-tional Web data.
The data was preprocessed andcompound splitting was applied.
Afterwards the dis-criminative word alignment approach as describedin (Niehues and Vogel, 2008) was applied to gener-ate the alignments between source and target words.The phrase table was built using the scripts from theMoses package (Koehn et al, 2007).
The languagemodel was trained on the target side of the paral-lel data as well as on additional monolingual Newsdata.
The translation model as well as the languagemodel was adapted towards the target domain in alog-linear way.The Arabic-to-English system was trained us-ing GALE Arabic data, which contains 6.1M sen-tences.
The word alignment is generated usingEMDC, which is a combination of a discriminativeapproach and the IBM Models as described in Gaoet al (2010).
The phrase table is generated usingChaski as described in Gao and Vogel (2010).
Thelanguage model data we trained on the GIGAWord202V3 data plus BBN English data.
After splitting thecorpus according to sources, individual models weretrained.
Then the individual models were interpo-lated to minimize the perplexity on the MT03/MT04data.For both tasks the reordering was performed as apreprocessing step using POS information from theTreeTagger (Schmid, 1994) for German and usingthe Amira Tagger (Diab, 2009) for Arabic.
For Ara-bic the approach described in Rottmann and Vogel(2007) was used covering short-range reorderings.For the German-to-English translation task the ex-tended approach described in Niehues et al (2009)was used to cover also the long-range reorderingstypical when translating between German and En-glish.For both directions an in-house phrase-based de-coder (Vogel, 2003) was used to generate the transla-tion hypotheses and the optimization was performedusing MER training.
The performance on the test-sets were measured in case-insensitive BLEU andTER scores.5.2 German to EnglishWe evaluated the approach on two different test setsfrom the News Commentary domain.
The first con-sists of 2000 sentences with one reference.
It willbe referred to as Test 1.
The second test set consistsof 1000 sentences with two references and will becalled Test 2.5.2.1 Translation QualityIn Tables 2 and 3 the results for translation per-formance on the German-to-English translation taskare summarized.As it can been seen, the improvements of transla-tion quality vary considerably between the two dif-ferent test sets.
While using the bilingual languagemodel improves the translation by only 0.15 BLEUand 0.21 TER points on Test 1, the improvement onTest 2 is nearly 1 BLEU point and 0.5 TER points.5.2.2 Context LengthOne intention of using the bilingual languagemodel is its capability to capture the bilingual con-texts in a different way.
To see, whether additionalbilingual context is used during decoding, we ana-lyzed the context used by the phrase pairs and bythe n-gram bilingual language model.However, a comparison of the different contextlengths is not straightforward.
The context of an n-gram language model is normally described by theaverage length of applied n-grams.
For phrase pairs,normally the average target phrase pair length (avg.Target PL) is used as an indicator for the size of thecontext.
And these two numbers cannot be com-pared directly.To be able to compare the context used by thephrase pairs to the context used in the n-gram lan-guage model, we calculated the average left contextthat is used for every target word where the worditself is included, i.e.
the context of a single wordis 1.
In case of the bilingual language model thescore for the average left context is exactly the aver-age length of applied n-grams in a given translation.For phrase pairs the average left context can be cal-culated in the following way: A phrase pair of length1 gets a left context score of 1.
In a phrase pair oflength 2, the first word has a left context score of 1,since it is not influenced by any target word to theleft.
The second word in that phrase pair gets a leftcontext count of 2, because it is influenced by thefirst word in the phrase.
Correspondingly, the leftcontext score of a phrase pair of length 3 is 6 (com-posed of the score 1 for the first word, score 2 forthe second word and score 3 for the third word).
Toget the average left context for the whole translation,the context scores of all phrases are summed up anddivided by the number of words in the translation.The scores for the average left contexts for the twotest sets are shown in Tables 2 and 3.
They are calledavg.
PP Left Context.
As it can be seen, the con-text used by the bilingual n-gram language model islonger than the one by the phrase pairs.
The averagen-gram length increases from 1.58 and 1.57, respec-tively to 2.21 and 2.18 for the two given test sets.If we compare the average n-gram length of thebilingual language model to the one of the targetlanguage model, the n-gram length of the first is ofcourse smaller, since the number of possible bilin-gual tokens is higher than the number of possiblemonolingual words.
This can also be seen whenlooking at the perplexities of the two language mod-els on the generated translations.
While the perplex-ity of the target language model is 99 and 101 onTest 1 and 2, respectively, the perplexity of the bilin-203gual language model is 512 and 538.Metric No BiLM BiLMBLEU 30.37 30.52TER 50.27 50.06avg.
Target PL 1.66 1.66avg.
PP Left Context 1.57 1.58avg.
Target LM N-Gram 3.28 3.27avg.
BiLM N-Gram 2.21Table 2: German-to-English results (Test 1)Metric No BiLM BiLMBLEU 44.16 45.09TER 41.02 40.52avg.
Target PL 1.65 1.65avg.
PP Left Context 1.56 1.57avg.
Target LM N-Gram 3.25 3.23avg.
BiLM N-Gram 2.18Table 3: German-to-English results (Test 2)5.2.3 Overlapping ContextAn additional advantage of the n-gram-based ap-proach is the possibility to have overlapping con-text.
If we would always use phrase pairs of length2 only half of the adjacent words would influenceeach other in the translation.
The others are onlyinfluenced by the other target words through the lan-guage model.
If we in contrast would have a bilin-gual language model which uses an n-gram lengthof 2, this means that every choice of word influencesthe previous and the following word.To analyze this influence, we counted how manyborders of phrase pairs are covered by a bilingualn-gram.
For Test 1, 16783 of the 27785 bordersbetween phrase pairs are covered by a bilingual n-gram.
For Test 2, 9995 of 16735 borders are cov-ered.
Consequently, in both cases at around 60 per-cent of the borders additional information can beused by the bilingual n-gram language model.5.2.4 Bilingual N-Gram LengthFor the German-to-English translation task weperformed an additional experiment comparing dif-ferent n-gram lengths of the bilingual languageBiLM Length aNGL BLEU TERNo 30.37 50.271 1 29.67 49.732 1.78 30.36 50.053 2.11 30.47 50.084 2.21 30.52 50.065 2.23 30.52 50.076 2.24 30.52 50.07Table 4: Different N-Gram Lengths (Test 1)BiLM Length aNGL BLEU TERNo 44.16 41.021 1 44.22 40.532 1.78 45.11 40.383 2.09 45.18 40.514 2.18 45.09 40.525 2.21 45.10 40.526 2.21 45.10 40.52Table 5: Different N-Gram Lengths (Test 2)model.
To ensure comparability between the exper-iments and avoid additional noise due to differentoptimization results, we did not perform separateoptimization runs for for each of the system vari-ants with different n-gram length, but used the samescaling factors for all of them.
Of course, the sys-tem using no bilingual language model was trainedindependently.
In Tables 4 and 5 we can see that thelength of the actually applied n-grams as well as theBLEU score increased until the bilingual languagemodel reaches an order of 4.
For higher order bilin-gual language models, nearly no additional n-gramscan be found in the language models.
Also the trans-lation quality does not increase further when usinglonger n-grams.5.3 Arabic to EnglishThe Arabic-to-English system was optimized on theMT06 data.
As test set the Rosetta in-house test setDEV07-nw (News) and wb (Web Data) was used.The results for the Arabic-to-English translationtask are summarized in Tables 6 and 7.
The perfor-mance was tested on two different domains, transla-tion of News and Web documents.
On both tasks,the translation could be improved by more than 1204BLEU point.
Measuring the performance in TERalso shows an improvement by 0.7 and 0.5 points.By adding a POS-based bilingual languagemodel, the performance could be improved further.An additional gain of 0.2 BLEU points and decreaseof 0.3 points in TER could be reached.
Conse-quently, an overall improvement of up to 1.7 BLEUpoints could be achieved by integrating two bilin-gual language models, one based on surface wordforms and one based on parts-of-speech.SystemDev TestBLEU TER BLEUNoBiLM 48.42 40.77 52.05+ BiLM 49.29 40.04 53.51+ POS BiLM 49.56 39.85 53.71Table 6: Results on Arabic to English: Translation ofNewsSystemDev TestBLEU TER BLEUNoBiLM 48.42 47.14 41.90+ BiLM 49.29 46.66 43.12+ POS BiLM 49.56 46.40 43.28Table 7: Results on Arabic to English: Translation ofWeb documentsAs it was done for the German-to-English system,we also compared the context used by the differentmodels for this translation direction.
The results aresummarized in Table 8 for the News test set and inTable 9 for the translation of Web data.
It can be seenlike it was for the other language pair that the contextused in the bilingual language model is bigger thanthe one used by the phrase-based translation model.Furthermore, it is worth mentioning that shorterphrase pairs are used, when using the POS-basedbilingual language model.
Both bilingual languagemodels seem to model the context quite good, so thatless long phrase pairs are needed to build the trans-lation.
Instead, the more frequent short phrases canbe used to generate the translation.5.4 Shared Translation Task @ WMT2011The bilingual language model was included in 3systems built for the WMT2011 Shared TranslationMetric No BiLM POS BiLMBLEU 52.05 53.51 53.71avg.
Target PL 2.12 2.03 1.79avg.
PP Left Context 1.92 1.85 1.69avg.
BiLM N-Gram 2.66 2.65avg.
POS BiLM 4.91Table 8: Bilingual Context in Arabic-to-English results(News)Metric No BiLM POS BiLMBLEU 41.90 43.12 43.28avg.
Target PL 1.82 1.80 1.57avg.
PP Left Context 1.72 1.69 1.53avg.
BiLM N-Gram 2.33 2.31avg.
POS BiLM 4.49Table 9: Bilingual Context in Arabic-to-English results(Web data)Task evaluation.
A phrase-based system similar tothe one described before for the German-to-Englishresults was used.
A detailed system description canbe found in Herrmann et al (2011).
The results aresummarized in Table 10.
The performance of com-petitive systems could be improved in all three lan-guages by up to 0.4 BLEU points.Language Pair No BiLM BiLMGerman-English 24.12 24.52English-German 16.89 17.01French-English 28.17 28.34Table 10: Preformance of Bilingual language model atWMT20116 ConclusionIn this work we showed how a feature of the n-gram-based approach can be integrated into a phrase-based statistical translation system.
We performeda detailed analysis on how this influences the scor-ing of the translation system.
We could show im-provements on a variety of translation tasks cover-ing different languages and domains.
Furthermore,we could show that additional bilingual context in-formation is used.Furthermore, the additional feature can easily be205extended to additional word factors such as part-of-speech, which showed improvements for the Arabic-to-English translation task.AcknowledgmentsThis work was realized as part of the Quaero Pro-gramme, funded by OSEO, French State agency forinnovation.ReferencesAlexandre Allauzen, Josep M. Crego, I?lknur Durgar El-Kahlout, and Fran?ois Yvon.
2010.
LIMSI?s Statisti-cal Translation Systems for WMT?10.
In Fifth Work-shop on Statistical Machine Translation (WMT 2010),Uppsala, Sweden.Jeff A. Bilmes and Katrin Kirchhoff.
2003.
Fac-tored language models and generalized parallel back-off.
In Proceedings of the 2003 Conference of theNorth American Chapter of the Association for Com-putational Linguistics on Human Language Technol-ogy, pages 4?6, Stroudsburg, PA, USA.Marine Carpuat and Dekai Wu.
2007.
Improving Statis-tical Machine Translation using Word Sense Disam-biguation.
In In The 2007 Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning.Francisco Casacuberta and Enrique Vidal.
2004.
Ma-chine Translation with Inferred Stochastic Finite-StateTransducers.
Comput.
Linguist., 30:205?225, June.Yee Seng Chan and Hwee Tou Ng.
2007.
Word SenseDisambiguation improves Statistical Machine Trans-lation.
In In 45th Annual Meeting of the Associationfor Computational Linguistics (ACL-07, pages 33?40.Josep M. Crego and Fran?ois Yvon.
2010.
Factoredbilingual n-gram language models for statistical ma-chine translation.
Machine Translation, 24, June.Mona Diab.
2009.
Second Generation Tools (AMIRA2.0): Fast and Robust Tokenization, POS tagging, andBase Phrase Chunking.
In Proc.
of the Second Interna-tional Conference on Arabic Language Resources andTools, Cairo, Egypt, April.Qin Gao and Stephan Vogel.
2010.
Training Phrase-Based Machine Translation Models on the Cloud:Open Source Machine Translation Toolkit Chaski.
InThe Prague Bulletin of Mathematical Linguistics No.93.Qin Gao, Francisco Guzman, and Stephan Vogel.2010.
EMDC: A Semi-supervised Approach for WordAlignment.
In Proc.
of the 23rd International Confer-ence on Computational Linguistics, Beijing, China.Sa?a Hasan, Juri Ganitkevitch, Hermann Ney, and Jes?sAndr?s-Ferrer.
2008.
Triplet Lexicon Models for Sta-tistical Machine Translation.
In Proc.
of Conferenceon Empirical Methods in NLP, Honolulu, USA.Teresa Herrmann, Mohammed Mediani, Jan Niehues,and Alex Waibel.
2011.
The Karlsruhe Institute ofTechnology Translation Systems for the WMT 2011.In Sixth Workshop on Statistical Machine Translation(WMT 2011), Edinbugh, U.K.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology, pages 48?54, Edmonton, Canada.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In ACL2007, Demonstration Session, Prague, Czech Repub-lic, June 23.Jos?
B. Mari?o, Rafael E. Banchs, Josep M. Crego, Adri?de Gispert, Patrik Lambert, Jos?
A. R. Fonollosa, andMarta R. Costa-juss?.
2006.
N-gram-based machinetranslation.
Comput.
Linguist., 32, December.Evgeny Matusov, Richard Zens, David Vilar, ArneMauser, Maja Popovic?, Sa?a Hasan, and HermannNey.
2006.
The rwth machine translation system.
InTC-STAR Workshop on Speech-to-Speech Translation,pages 31?36, Barcelona, Spain, June.Jan Niehues and Stephan Vogel.
2008.
DiscriminativeWord Alignment via Alignment Matrix Modeling.
InProc.
of Third ACL Workshop on Statistical MachineTranslation, Columbus, USA.Jan Niehues, Teresa Herrmann, Muntsin Kolss, and AlexWaibel.
2009.
The Universit?t Karlsruhe TranslationSystem for the EACL-WMT 2009.
In Fourth Work-shop on Statistical Machine Translation (WMT 2009),Athens, Greece.Kay Rottmann and Stephan Vogel.
2007.
Word Reorder-ing in Statistical Machine Translation with a POS-Based Distortion Model.
In TMI, Sk?vde, Sweden.Helmut Schmid.
1994.
Probabilistic Part-of-Speech Tag-ging Using Decision Trees.
In International Con-ference on New Methods in Language Processing,Manchester, UK.Stephan Vogel.
2003.
SMT Decoder Dissected: WordReordering.
In Int.
Conf.
on Natural Language Pro-cessing and Knowledge Engineering, Beijing, China.206
