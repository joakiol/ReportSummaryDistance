Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 33?38,Portland, OR, USA, 24 June 2011. c?2011 Association for Computational LinguisticsAutomatic linguistic annotation of historical language:ToTrTaLe and XIX century SloveneToma?
ErjavecDepartment of Knowledge Technologies,Jo?ef Stefan InstituteJamova cesta 39, 1000 LjubljanaSloveniatomaz.erjavec@ijs.siAbstractThe paper describes a tool developed toprocess historical (Slovene) text, which an-notates words in a TEI encoded corpuswith their modern-day equivalents, mor-phosyntactic tags and lemmas.
Such a toolis useful for developing historical corporaof highly-inflecting languages, enablingfull text search in digital libraries of histor-ical texts, for modernising such texts fortoday's readers and making it simpler tocorrect OCR transcriptions.1 IntroductionBasic processing of written language, in particulartokenisation, tagging and lemmatisation, is usefulin a number of applications, such as enabling full-text search, corpus-linguistic studies, and addingfurther layers of annotation.
Support for lemmati-sation and morphosyntactic tagging is well-advanced for modern-day languages, however, thesituation is very different for historical languagevarieties, where much less ?
if any ?
resources ex-ist to train high-quality taggers and lemmatisers.Historical texts also bring with them a number ofchallenges not present with modern language:?
due to the low print quality, optical characterrecognition (OCR) produces much worse re-sults than for modern day texts; currently, suchtexts must be hand-corrected to arrive at ac-ceptable quality levels;?
full-text search is difficult, as the texts are notlemmatised and use different orthographicconventions and archaic spellings, typicallynot familiar to non-specialists;?
comprehension can also be limited, esp.
whenthe text uses an alphabet different from thecontemporary norm.This paper describes a tool to help alleviate theabove problems.
The tool implements a pipeline,where it first tokenises the text and then attemptsto transcribe the archaic words to their modern dayequivalents.
For here on, the text is tagged andlemmatised using the models for modern Slovene.Such an approach is not new, as it straightforward-ly follows from a situation where good languagemodels are available for contemporary language,but not for its historical variants.The focus of the research in such cases is on themapping from historical words to modern ones,and such approaches have already been attemptedfor other languages, e.g.
for English (Rayson et al2007), German (Pilz et al 2008), Spanish(S?nchez-Marco et al 2010) and Icelandic (R?gn-valdsson and Helgad?ttir, 2008).
These studieshave mostly concentrated on mapping historicalvariants to modern words or evaluating PoS tag-ging accuracy and have dealt with Germanic andRomance languages.
This paper discusses thecomplete annotation process, including lemmatisa-tion, and treats a Slavic language, which has sub-stantially different morphology; in Slovene, wordsbelong to complex inflectional paradigms, whichmakes tagging and lemmatisation models quitecomplex, esp.
for unknown words.The paper also discusses structural annotationssupported by the tool, which takes as input a doc-ument encoded according to (a subset of) the TextEncoding Initiative Guidelines, TEI P5 (Burnardand Bauman, 2007) and also produces output inthis format.An example of the tool input fragment and the cor-responding output is given in Figure 1.332 The ToTrTaLe toolThe annotation tool implements a pipeline archi-tecture and is essentially a wrapper program thatcalls a number of further processing modules.
Thetool is based on the ToTaLe tool (Erjavec et al,2005), which performs Tokenisation, Tagging andLemmatisation on modern text; as the present toolextends this with Transcription, it is called To-TrTaLe, and comprises the following modules:1. extracting processing chunks from source TEI2.
tokenisation3.
extracting text to be annotated4.
transcription to modern word-forms5.
part-of-speech tagging6.
lemmatisation7.
TEI outputWhile the tool and its modules make some lan-guage specific assumption, they are rather broad,such as that text tokens are (typically) separated byspace; otherwise, the tool relies on external lan-guage resources, so it could be made to work withmost European languages, although it is especiallysuited for the highly-inflecting ones.The tool is written in Perl and is reasonably fast,i.e.
it processes about 100k words per minute on aLinux server.
The greatest speed bottleneck is thetool start-up, mostly the result of the lemmatisationmodule, which for Slovene contains thousands ofrules and exceptions.
In the rest of this section wepresent the modules of ToTrTaLe, esp.
as they re-late to processing of historical language.2.1 Extracting chunksIn the first step, the top-level elements of the TEIfile that contain text to be processed in one chunkare identified and passed on for linguistic pro-cessing.
This step serves two purposes.
CertainTEI elements, in particular the <teiHeader>, whichcontains the meta-data of the document, should notbe analysed but simply passed on to the output(except for recording the fact that the text has beenlinguistically annotated).
Second, the processors incertain stages keep the text and annotations inmemory.
As a TEI document can be arbitrarilylarge the available physical memory can be ex-hausted, leading to severe slow-down or even out-of-memory errors.
It is therefore possible to speci-fy which elements (such as <body> or <div>)should be treated as chunks to be processed in oneannotation run.2.2 The tokenisation moduleThe multilingual tokenisation module mlToken 1is written in Perl and in addition to splitting theinput string into tokens has also the following fea-tures:?
assigns to each token its token type, e.g.
XMLtag, sentence final punctuation, digit, abbrevia-tion, URL, etc.?
preserves (subject to a flag) white-space, sothat the input can be reconstituted from theoutput.The tokeniser can be fine-tuned by putting punctu-ation into various classes (e.g.
word-breaking vs.non-breaking) and also uses several language-dependent resource files, in particular a list of ab-breviations (?words?
ending in period, which is apart of the token and does not necessarily end asentence), list of multi-word units (tokens consist-ing of several space-separated ?words?)
and a listof (right or left) clitics, i.e.
cases where one ?word?should be treated as several tokens.
These resourcefiles are esp.
important in the context of processinghistorical language, as it often happens that wordsthat used to be written apart and now written to-gether or vice-versa.
Such words are put in the ap-propriate resource file, so that their tokenisation isnormalised.
Examples of multi-word and split to-kens are given in Figure 1.2.3 Text extractionA TEI encoded text can contain a fair amount ofmarkup, which we, as much as possible, aim topreserve in the output.
However, most of themarkup should be ignored by the annotation mod-ules, or, in certain cases, even the content of anelement should be ignored; this goes esp.
formarkup found in text-critical editions of historicaltexts.
For example, the top and bottom of the pagecan contain a running header, page number andcatch-words (marked up in <fw> ?forme work?elements), which should typically not be annotatedas they are not linguistically interesting and wouldfurthermore break the continuity of the text.
Thetext might also contain editorial corrections(marked up as <choice> <sic>mistyped text</sic><corr>corrected text</corr> </choice>), where,arguably, only the corrected text should be taken1 mlToken was written in 2005 by Camelia Ignat, then work-ing at the EU Joint Research Centre  in Ispra, Italy.34into account in the linguistic annotation.
Thismodule extracts the text that should be passed onto the annotation modules, where the elements tobe ignored are specified in a resource file.This solution does take care of most situations en-countered so far in our corpora2 but is not com-pletely general.
As discussed in Bennet et al(2010), there are many cases where adding token(and sentence) tags to existing markup breaksXML well-formedness or TEI validity, such assentences crossing structural boundaries or word-internal TEI markup.A general ?solution?
to the problem is stand-offmarkup, where the annotated text is kept separatefrom the source TEI, but that merely postpones theproblem of how to treat the two as a unit.
Andwhile TEI does offer solutions to such problems,implementing processing of arbitrary TEI in-placemarkup would, however, require much further re-search.
So ToTrTaLe adds the linguistic mark-upin-place, but does so correctly only for a restricted,although still useful, set of TEI element configura-tions.2.4 TranscriptionThe transcription of archaic word-forms to theirmodern day equivalents is the core module whichdistinguishes our processing of historical languageas opposed to its contemporary form.
The tran-scription process relies on three resources:?
a lexicon of modern-day word-forms;?
a lexicon of historical word-forms, with asso-ciated modern-day equivalent word-form(s);3?
a set of transcription patterns.In processing historical texts, the word-form to-kens are first normalised, i.e.
de-capitalised anddiacritic marks over vowels removed; the latter ismost likely Slovene specific, as modern-day Slo-vene, unlike the language of the 19th century, doesnot use vowel diacritics.2 The notable exception is <lb/>, line break, which, giv-en the large font size and small pages, often occurs inthe middle of a word in historical texts.
We move suchline breaks in the source documents to the start of theword and mark their displacement in lb/@n.3 The two lexica have in fact a somewhat more compli-cated structure.
For example, many archaic words donot have a proper modern day equivalent; for these, thelexicon gives the word in its modern spelling but also itsmodern near synonyms.To determine the modern-day word-form, the his-torical lexicon is checked first.
If the normalizedword-form is an entry of the historical lexicon, theequivalent modern-day word-form has also beenidentified; if not, it is checked against the modern-day lexicon.
This order of searching the lexica isimportant, as the modern lexicon can containword-forms which have an incorrect meaning inthe context of historical texts, so the historical lex-icon also serves to block such meanings.If neither lexicon contains the word, the transcrip-tion patterns are tried.
Many historical spellingvariants can be traced to a set of rewrite rules or?patterns?
that locally explain the difference be-tween the contemporary and the historical spelling.For Slovene, a very prominent pattern is e.g.
r?eras exemplified by the pair br?
?ber?, where theleft side represents the modern and the right thehistorical spelling.Such patterns are operationalized by the finite-state?Variant aware approximate matching?
tool Vaam,(Gotscharek et al 2009; Reffle, 2011), which takesas input a historical word-form, the set of patters,and a modern-day lexicon and efficiently returnsthe modern-day word-forms that can be computedfrom the archaic one by applying one or more pat-terns.
The output list is ranked, preferring candi-dates where a small number of pattern applicationsis needed for the rewrite operation.4It should be noted that the above process of tran-scription is non-deterministic.
While this rarelyhappens in practice, the historical word-form canhave several modern-day equivalents.
More im-portantly, the Vaam module will typically returnseveral possible alternative modernisations, ofwhich only one is correct for the specific use of theword in context.
We currently make use of fre-quency based heuristics to determine the ?best?transcription, but more advanced models are possi-ble, which would postpone the decision of the bestcandidate until the tagging and lemmatization hasbeen performed.We currently use a set of about 100 transcriptionpatterns, which were obtained by corpus inspec-tion, using a dedicated concordancer.4 Vaam also supports approximate matching based onedit distance, useful for identifying (and correcting)OCR errors; we have, however, not yet made use of thisfunctionality.352.5 TaggingFor tagging words in the text with their contextdisambiguated morphosyntactic annotations we useTnT (Brants, 2000), a fast and robust tri-gram tag-ger.
The tagger has been trained on jos1M, the 1million word JOS corpus of contemporary Slovene(Erjavec and Krek, 2008), and is also given a largebackground lexicon extracted from the 600 millionword FidaPLUS reference corpus of contemporarySlovene (Arhar and Gorjanc, 2007).2.6 LemmatisationAutomatic lemmatisation is a core application formany language processing tasks.
In inflectionallyrich languages assigning the correct lemma (baseform) to each word in a running text is not trivial,as, for instance, Slovene adjectives inflect for gen-der, number and case (3x3x6) with a complex con-figuration of endings and stem modifications.For our lemmatiser we use CLOG (Manandhar etal., 1998, Erjavec and D?eroski, 2004), which im-plements a machine learning approach to the au-tomatic lemmatisation of (unknown) words.
CLOGlearns on the basis of input examples (pairs word-form/lemma, where each morphosyntactic tag islearnt separately) a first-order decision list, essen-tially a sequence of if-then-else clauses, where thedefined operation is string concatenation.
Thelearnt structures are Prolog programs but in orderto minimise interface issues we made a converterfrom the Prolog program into one in Perl.An interesting feature of CLOG is that it does notsucceed in lemmatising just any word-form.
Withhistorical texts it almost invariably fails in lemma-tising truly archaic words, making it a good selec-tor for new entries in the historical lexicon.The lemmatiser was trained on a lexicon extractedfrom the jos1M corpus, and the lemmatisation ofcontemporary language is quite accurate, with 92%on unknown words.
However, as mentioned, thelearnt model, given that there are 2,000 separateclasses, is quite large: the Perl rules have about2MB, which makes loading the lemmatiser slow.2.7 TEI outputThe final stage of processing is packing the origi-nal file with the added annotations into a valid TEIdocument.
This is achieved by combining Perl pro-cessing with XSLT scripts.
The last step in theprocessing is the validation of the resulting XMLfile against a TEI schema expressed in Relax NG.A validation failure indicates that the input docu-ment breaks some (possibly implicit) mark-up as-sumptions ?
in this case either the input documentmust be fixed, or, if the encoding choices were val-id, the program should be extended to deal alsowith such cases.3 ConclusionsThe paper gave an overview of the ToTrTaLe tool,which performs basic linguistic annotation on TEIencoded historical texts.
Some future work on thetool has already been mentioned, in particular ex-ploring ways of flexibly connecting transcription totagging and lemmatisation, as well as supportingmore complex TEI encoded structures.While the tool itself is largely language independ-ent, it does need substantial language resources tooperationalize it for a language.
Specific for histor-ical language processing are a corpus of tran-scribed historical texts, a lexicon of historical wordforms and a pattern set.
The paper did not discussthese language resources, although it is here thatmost work will be invested in the future.The corpus we have used so far for Slovene lexi-con building comes from the AHLib digital library(Prun?, 2007; Erjavec 2005), which contains 2 mil-lion words of 19th century texts; we now plan toextend this with older material, predominantlyfrom the 18th century.The on-going process of creating the Slovene his-torical lexicon is described in Erjavec et al,(2010), while the model of a TEI encoded lexiconcontaining not only historical word-forms, but alsoall the other lexical items needed to feed the tool(such as multi-word units) is presented in Erjavecet al (2011).
As we extend the corpus, we will alsoobtain new words, which will be automaticallyannotated with ToTrTaLe and then manually cor-rected, feeding into the lexicon building process.For the patterns, the extension of the corpus will nodoubt show the need to extend also the pattern set.Most likely this will be done by corpus inspection,via a dedicated concordancer, although alternativemethods of pattern identification are possible.
Inparticular, once when a substantial list of pairs his-torical word-form / contemporary word-form be-comes available, automatic methods can be used toderive a list of patterns, ranked by how productivethey are (Pilz et al, 2008; Oravecz et al 2010).36AcknowledgementsThe author thanks the anonymous reviewers for theiruseful comments and suggestions.
The work presentedin this paper has been supported by the EU IMPACTproject ?Improving Access to Text?
and the GoogleDigital Humanities Research Award ?Language modelsfor historical Slovenian?.ReferencesPaul Bennett, Martin Durrell, Silke Scheible, and Rich-ard J. Whitt, 2010.
Annotating a historical corpus ofGerman: A case study.
Proceedings of the LREC2010 workshop on Language Resources and Lan-guage Technology Standards.
Valletta, Malta, 18May 2010.
64-68.Lou Burnard and Syd Bauman, 2007.
Guidelines forElectronic Text Encoding and Interchange (TEI P5).Text Encoding Initiative Consortium.
Oxford, 2007.http://www.tei-c.org/release/doc/tei-p5-doc/Toma?
Erjavec.
2007.
Architecture for Editing ComplexDigital Documents.
Proceedings of the Conferenceon Digital Information and Heritage.
Zagreb.
pp.105-114.Toma?
Erjavec and Sa?o D?eroski.
2004.
MachineLearning of Language Structure: Lemmatising Un-known Slovene Words.
Applied Artificial Intelli-gence, 18(1):17?41.Toma?
Erjavec, Simon Krek, 2008.
The JOS morpho-syntactically tagged corpus of Slovene.
In Proceed-ings of the Sixth International Conference onLanguage Resources and Evaluation, LREC?08, Par-is, ELRA.Toma?
Erjavec, Camelia Ignat, Bruno Pouliquen, andRalf Steinberger.
Massive Multi-Lingual CorpusCompilation: Acquis Communautaire and ToTaLe.In Proceedings of the 2nd Language & TechnologyConference, April 21-23, 2005, Poznan, Poland.2005, pp.
32-36.Toma?
Erjavec, Christoph Ringlstetter, Maja ?orga, andAnnette Gotscharek, 2010.
Towards a Lexicon ofXIXth Century Slovene.
In Proceedings of the Sev-enth Language Technologies Conference, October14th-15th, 2010, Ljubljana, Slovenia.
Jo?ef StefanInstitute.Toma?
Erjavec, Christoph Ringlstetter, Maja ?orga, andAnnette Gotscharek, (submitted).
A lexicon for pro-cessing archaic language: the case of XIXth centurySlovene.
ESSLLI Workshop on Lexical Resourcesworkshop, WoLeR?11.
Ljubljana, Slovenia.Annette Gotscharek, Andreas Neumann, Ulrich Reffle,Christoph Ringlstetter and Klaus U. Schulz.
2009.Enabling Information Retrieval on Historical Docu-ment Collections - the Role of Matching Proceduresand Special Lexica.
Proceedings of the ACM SIGIR2009 Workshop on Analytics for Noisy UnstructuredText Data (AND09), Barcelona.Suresh Manandhar, Sa?o D?eroski and Toma?
Erjavec1998.
Learning Multilingual Morphology withCLOG.
In Proceedings of Inductive Logic Program-ming; 8th International Workshop ILP-98 (LectureNotes in Artificial Intelligence 1446) (pp.
135-144).Springer-Verlag, Berlin.Csaba Oravecz, B?lint Sass and Eszter Simon.
2010.Semi-automatic Normalization of Old HungarianCodices.
Proceedings of the ECAI 2010 Workshopon Language Technology for Cultural Heritage, So-cial Sciences, and Humanities (LaTeCH 2010), Au-gust 16, 2010, Lisbon, Portugal.Thomas Pilz, Andrea Ernst-Gerlach, Sebastian Kemp-ken, Paul Rayson and Dawn Archer, 2008.
The Iden-tification of Spelling Variants in English and GermanHistorical Texts: Manual or Automatic?
Literary andLinguistic Computing, 23/1, pp.
65-72.Erich Prun?.
2007.
Deutsch-slowenische/kroatische?bersetzung 1848-1918 [German-Slovene/Croatiantranslation, 1848-1918].
Ein Werkst?ttenbericht.Wiener Slavistisches Jahrbuch 53/2007.
AustrianAcademy of Sciences Press, Vienna.
pp.
163-176.Paul Rayson, Dawn Archer, Alistair Baron, JonathanCulpeper, and Nicolas Smith, 2007.
Tagging theBard: Evaluating the accuracy of a modern POS tag-ger on Early Modern English corpora.
In Proceedingsof Corpus Linguistics 2007.
University of Birming-ham, UK.Ulrich Reffle, Efficiently generating correction sugges-tions for garbled tokens of historical language, Jour-nal of Natural Language Engineering, Special Issueon Finite State Methods and Models in Natural Lan-guage  Processing, 2011.Eir?kur R?gnvaldsson and Sigr?n Helgad?ttir, 2008.Morphological tagging of Old Norse texts and its usein studying syntactic variation and change.
In Pro-ceedings of the LREC 2008 Workshop on LanguageTechnology for Cultural Heritage Data (LaTeCH2008).
ELRA, Paris.Cristina S?nchez-Marco, Gemma Boleda, Josep MariaFontana and Judith Domingo.
2010.
Annotation andRepresentation of a Diachronic Corpus of Spanish.Proceedings of the Seventh conference on Interna-tional Language Resources and Evaluation(LREC'10).
ELRA, Paris.37TEI input fragment:<p xml:id="p.401">Nekiga bogatiga kneza z nja <lb/><pb n="93" facs="#FPG00012.097" xml:id="pb.97"/>dru?ino, ki v mes nemore  <lb n="3"/><gap/></p>ToTrTaLe output:<p xml:id="p.401"><s><w subtype="lexicon" nform="nekiga" mform="nekega" lemma="nek" ctag="Pi-msg">Nekiga</w><c> </c><w subtype="pattern" pattern="[ega@?iga@]" mform="bogatega" lemma="bogat"ctag="Agpmsg">bogatiga</w><c> </c><w lemma="knez" ctag="Npmsg">kneza</w><c> </c><w lemma="z" ctag="Si">z</w><c> </c><w subtype="lexicon" mform="njegova" lemma="njegov" ctag="Ps3fsnsm">nja</w><c> </c><lb/><pb n="93" facs="#FPG00012.097" xml:id="pb.97"/><w lemma="dru?ina" ctag="Ncfsa">dru?ino</w><pc ctag=",">,</pc><c> </c><w lemma="ki" ctag="Cs">ki</w><c> </c><w type="multiw" subtype="pattern" pattern="[@v?
@v_]" mform="vmes" lemma="vmes" ctag="Rgp"n="mw_jeGx2">v</w><c> </c><w type="multiw" subtype="pattern" pattern="[@v?
@v_]" mform="vmes" lemma="vmes" ctag="Rgp"n="mw_jeGx2">mes</w><c> </c><w type="split" mform="ne_more" lemma="ne_mo?i" ctag="Q_Vmpr3s">nemore</w><c>  </c><lb n="3"/><gap/></s></p>Figure 1.
An example of ToTrTaLe input paragraph and the equivalent output.Paragraphs, page and line breaks are preserved, and the program adds elements for words, punctuation symbols andwhite-space.
Both punctuation and words are assigned a corpus tag and lemma, and, where different from the de-fault, the type and subtype of the word, its normalised and modernised form, and possibly the used pattern(s).
Incases of multi-words, each part is given its own word tag, which have identical analyses and are joined together bythe unique value of @n; this approach allows also modelling discontinuous multi-word units, such as separableverbs in Germanic languages.
Split words forms, on the other hand, are modelled by one word token, but with aportmanteau analysis.38
