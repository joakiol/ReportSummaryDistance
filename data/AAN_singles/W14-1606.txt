Proceedings of the Eighteenth Conference on Computational Language Learning, pages 49?57,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsInducing Neural Models of Script KnowledgeAshutosh ModiMMCI,Saarland University, Germanyamodi@mmci.uni-saarland.deIvan TitovILLC,University of Amsterdam, Netherlandstitov@uva.nlAbstractInduction of common sense knowledgeabout prototypical sequence of events hasrecently received much attention (e.g.,Chambers and Jurafsky (2008); Regneriet al.
(2010)).
Instead of inducing thisknowledge in the form of graphs, as inmuch of the previous work, in our method,distributed representations of event real-izations are computed based on distributedrepresentations of predicates and their ar-guments, and then these representationsare used to predict prototypical event or-derings.
The parameters of the composi-tional process for computing the event rep-resentations and the ranking componentof the model are jointly estimated.
Weshow that this approach results in a sub-stantial boost in performance on the eventordering task with respect to the previousapproaches, both on natural and crowd-sourced texts.1 IntroductionIt is generally believed that natural language un-derstanding systems would benefit from incorpo-rating common-sense knowledge about prototyp-ical sequences of events and their participants.Early work focused on structured representationsof this knowledge (called scripts (Schank andAbelson, 1977)) and manual construction of scriptknowledge bases.
However, these approaches donot scale to complex domains (Mueller, 1998;Gordon, 2001).
More recently, automatic induc-tion of script knowledge from text have startedto attract attention: these methods exploit ei-ther natural texts (Chambers and Jurafsky, 2008,2009) or crowdsourced data (Regneri et al., 2010),and, consequently, do not require expensive ex-pert annotation.
Given a text corpus, they ex-tract structured representations (i.e.
graphs), forexample chains (Chambers and Jurafsky, 2008)or more general directed acyclic graphs (Regneriet al., 2010).
These graphs are scenario-specific,nodes in them correspond to events (and associ-ated with sets of potential event mentions) and arcsencode the temporal precedence relation.
Thesegraphs can then be used to inform NLP applica-tions (e.g., question answering) by providing in-formation whether one event is likely to precedeor succeed another.
Note that these graphs en-code common-sense knowledge about prototypi-cal ordering of events rather than temporal orderof events as described in a given text.Though representing the script knowledge asgraphs is attractive from the human interpretabilityperspective, it may not be optimal from the appli-cation point of view.
More specifically, these rep-resentations (1) require a model designer to choosean appropriate granularity of event mentions (e.g.,whether nodes in the graph should be associatedwith verbs, or also their arguments); (2) do notprovide a mechanism for deciding which scenarioapplies in a given discourse context and (3) oftendo not associate confidence levels with informa-tion encoded in the graph (e.g., the precedence re-lation in Regneri et al.
(2010)).Instead of constructing a graph and using it toprovide information (e.g., prototypical event or-dering) to NLP applications, in this work we ad-vocate for constructing a statistical model which iscapable to ?answer?
at least some of the questionsthese graphs can be used to answer, but doing thiswithout explicitly representing the knowledge as agraph.
In our method, the distributed representa-tions (i.e.
vectors of real numbers) of event real-izations are computed based on distributed repre-sentations of predicates and their arguments, andthen the event representations are used in a rankerto predict the prototypical ordering of events.
Boththe parameters of the compositional process forcomputing the event representation and the rank-49ing component of the model are estimated fromtexts (either relying on unambiguous discourseclues or natural ordering in text).
In this way webuild on recent research on compositional distri-butional semantics (Baroni and Zamparelli, 2011;Socher et al., 2012), though our approach specif-ically focuses on embedding predicate-argumentstructures rather than arbitrary phrases, and learn-ing these representation to be especially informa-tive for prototypical event ordering.In order to get an intuition why the embeddingapproach may be attractive, consider a situationwhere a prototypical ordering of events the busdisembarked passengers and the bus drove awayneeds to be predicted.
An approach based on fre-quency of predicate pairs (Chambers and Jurafsky,2008) (henceforth CJ08), is unlikely to make aright prediction as driving usually precedes disem-barking.
Similarly, an approach which treats thewhole predicate-argument structure as an atomicunit (Regneri et al., 2010) will probably fail aswell, as such a sparse model is unlikely to be ef-fectively learnable even from large amounts of un-labeled data.
However, our embedding methodwould be expected to capture relevant features ofthe verb frames, namely, the transitive use for thepredicate disembark and the effect of the particleaway, and these features will then be used by theranking component to make the correct prediction.In previous work on learning inferencerules (Berant et al., 2011), it has been shownthat enforcing transitivity constraints on theinference rules results in significantly improvedperformance.
The same is likely to be true forthe event ordering task, as scripts have largelylinear structure, and observing that a ?
b andb ?
c is likely to imply a ?
c. Interestingly, inour approach we learn the model which satisfiestransitivity constraints, without the need for anyexplicit global optimization on a graph.
Thisresults in a significant boost of performance whenusing embeddings of just predicates (i.e.
ignoringarguments) with respect to using frequencies ofordered verb pairs, as in CJ08 (76% vs. 61% onthe natural data).Our model is solely focusing on the orderingtask, and admittedly does not represent all the in-formation encoded by a script graph structure.
Forexample, it cannot be directly used to predict amissing event given a set of events (the narrativecloze task (Chambers and Jurafsky, 2009)).
Nev-disembarked passengersbuspredicate embeddingevent embeddingarg embeddingTa1RpTa2f(e)a1= C(bus) a2= C(passenger)p = C(disembark)arg embeddinghidden layerhAhFigure 1: Computation of an event representationfor a predicate with two arguments (the bus disem-barked passengers), an arbitrary number of argu-ments is supported by our approach.ertheless, we believe that the framework (a proba-bilistic model using event embeddings as its com-ponent) can be extended to represent other aspectsof script knowledge by modifying the learning ob-jective, but we leave this for future work.
In thispaper, we show how our model can be used to pre-dict if two event mentions are likely paraphrasesof the same event.The approach is evaluated in two set-ups.
First,we consider the crowdsourced dataset of Regneriet al.
(2010) and demonstrate that using our modelresults in the 13.5% absolute improvement in F1on event ordering with respect to their graph in-duction method (84.1% vs. 70.6%).
Secondly,we derive an event ordering dataset from the Gi-gaword corpus, where we also show that the em-bedding method beats the frequency-based base-line (i.e.
reimplementation of the scoring compo-nent of CJ08) by 22.8% in accuracy (83.5% vs.60.7%).2 ModelIn this section we describe the model we use forcomputing event representations as well as theranking component of our model.2.1 Event RepresentationLearning and exploiting distributed word repre-sentations (i.e.
vectors of real values, also knownas embeddings) have been shown to be benefi-cial in many NLP applications (Bengio et al.,2001; Turian et al., 2010; Collobert et al., 2011).These representations encode semantic and syn-tactic properties of a word, and are normally50learned in the language modeling setting (i.e.learned to be predictive of local word context),though they can also be specialized by learningin the context of other NLP applications such asPoS tagging or semantic role labeling (Collobertet al., 2011).
More recently, the area of dis-tributional compositional semantics have startedto emerge (Baroni and Zamparelli, 2011; Socheret al., 2012), they focus on inducing represen-tations of phrases by learning a compositionalmodel.
Such a model would compute a represen-tation of a phrase by starting with embeddings ofindividual words in the phrase, often this composi-tion process is recursive and guided by some formof syntactic structure.In our work, we use a simple compositionalmodel for representing semantics of a verb framee (i.e.
the predicate and its arguments).
We willrefer to such verb frames as events.
The model isshown in Figure 1.
Each word ciin the vocabu-lary is mapped to a real vector based on the cor-responding lemma (the embedding function C).The hidden layer is computed by summing linearlytransformed predicate and argument1embeddingsand passing it through the logistic sigmoid func-tion.
We use different transformation matrices forarguments and predicates, T and R, respectively.The event representation f(e) is then obtained byapplying another linear transform (matrix A) fol-lowed by another application of the sigmoid func-tion.
Another point to note in here is that, as inprevious work on script induction, we use lemmasfor predicates and specifically filter out any tensemarkers as our goal is to induce common-senseknowledge about an event rather than propertiespredictive of temporal order in a specific discoursecontext.We leave exploration of more complex andlinguistically-motivated models for future work.2These event representations are learned in the con-text of event ranking: the transformation parame-ters as well as representations of words are forcedto be predictive of the temporal order of events.In our experiments, we also consider initializationof predicate and arguments with the SENNA wordembeddings (Collobert et al., 2011).1Only syntactic heads of arguments are used in this work.If an argument is a coffee maker, we will use only the wordmaker.2In this study, we apply our model in two very differ-ent settings, learning from crowdsourced and natural texts.Crowdsourced collections are relatively small and require notover-expressive models.2.2 Learning to OrderThe task of learning stereotyped order of eventsnaturally corresponds to the standard ranking set-ting.
We assume that we are provided with se-quences of events, and our goal is to capture thisorder.
We discuss how we obtain this learning ma-terial in the next section.
We learn a linear ranker(characterized by a vectorw) which takes an eventrepresentation and returns a ranking score.
Eventsare then ordered according to the score to yieldthe model prediction.
Note that during the learn-ing stage we estimate not only w but also theevent representation parameters, i.e.
matrices T ,R and A, and the word embedding C. Note thatby casting the event ordering task as a global rank-ing problem we ensure that the model implicitlyexploits transitivity of the relation, the propertywhich is crucial for successful learning from finiteamount of data, as we argued in the introductionand will confirm in our experiments.At training time, we assume that each trainingexample k is a list of events e(k)1, .
.
.
, e(k)n(k)pro-vided in the stereotypical order (i.e.
e(k)i?
e(k)jifi < j), n(k)is the length of the list k. We mini-mize the L2-regularized ranking hinge loss:?k?i<j?n(k)max(0, 1?wTf(e(k)i;?)+wTf(e(k)j;?
))+ ?
(?w?2+ ??
?2),where f(e;?)
is the embedding computedfor event e, ?
are all embedding parame-ters corresponding to elements of the matrices{R,C, T,A}.
We use stochastic gradient descent,gradients w.r.t.
?
are computed using back propa-gation.3 ExperimentsWe evaluate our approach in two different set-ups.First, we induce the model from the crowdsourceddata specifically collected for script induction byRegneri et al.
(2010), secondly, we consider anarguably more challenging set-up of learning themodel from news data (Gigaword (Parker et al.,2011)), in the latter case we use a learning sce-nario inspired by Chambers and Jurafsky (2008).33Details about downloading the data and models are at:http://www.coli.uni-saarland.de/projects/smile/docs/nmReadme.txt51Precision (%) Recall (%) F1 (%)BL EEverbMSA BS EE BL EEverbMSA BS EE BL EEverbMSA BS EEBus 70.1 81.9 80.0 76.0 85.1 71.3 75.8 80.0 76.0 91.9 70.7 78.8 80.0 76.0 88.4Coffee 70.1 73.7 70.0 68.0 69.5 72.6 75.1 78.0 57.0 71.0 71.3 74.4 74.0 62.0 70.2Fastfood 69.9 81.0 53.0 97.0 90.0 65.1 79.1 81.0 65.0 87.9 67.4 80.0 64.0 78.0 88.9Return 74.0 94.1 48.0 87.0 92.4 68.6 91.4 75.0 72.0 89.7 71.0 92.8 58.0 79.0 91.0Iron 73.4 80.1 78.0 87.0 86.9 67.3 69.8 72.0 69.0 80.2 70.2 69.8 75.0 77.0 83.4Microw.
72.6 79.2 47.0 91.0 82.9 63.4 62.8 83.0 74.0 90.3 67.7 70.0 60.0 82.0 86.4Eggs 72.7 71.4 67.0 77.0 80.7 68.0 67.7 64.0 59.0 76.9 70.3 69.5 66.0 67.0 78.7Shower 62.2 76.2 48.0 85.0 80.0 62.5 80.0 82.0 84.0 84.3 62.3 78.1 61.0 85.0 82.1Phone 67.6 87.8 83.0 92.0 87.5 62.8 87.9 86.0 87.0 89.0 65.1 87.8 84.0 89.0 88.2Vending 66.4 87.3 84.0 90.0 84.2 60.6 87.6 85.0 74.0 81.9 63.3 84.9 84.0 81.0 88.2Average 69.9 81.3 65.8 85.0 83.9 66.2 77.2 78.6 71.7 84.3 68.0 79.1 70.6 77.6 84.1Table 1: Results on the crowdsourced data for the verb-frequency baseline (BL), the verb-only embed-ding model (EEverb), Regneri et al.
(2010) (MSA), Frermann et al.
(2014)(BS) and the full model (EE).3.1 Learning from Crowdsourced Data3.1.1 Data and taskRegneri et al.
(2010) collected descriptions (calledevent sequence descriptions, ESDs) of varioustypes of human activities (e.g., going to a restau-rant, ironing clothes) using crowdsourcing (Ama-zon Mechanical Turk), this dataset was also com-plemented by descriptions provided in the OMICScorpus (Gupta and Kochenderfer, 2004).
Thedatasets are fairly small, containing 30 ESDs peractivity type in average (we will refer to differentactivities as scenarios), but in principle the col-lection can easily be extended given the low costof crowdsourcing.
The ESDs list events formingthe scenario and are written in a bullet-point style.The annotators were asked to follow the prototyp-ical event order in writing.
As an example, con-sider a ESD for the scenario prepare coffee :{go to coffee maker} ?
{fill water in coffeemaker} ?
{place the filter in holder} ?
{placecoffee in filter} ?
{place holder in coffee maker}?
{turn on coffee maker}Regneri et al.
also automatically extracted pred-icates and heads of arguments for each event, asneeded for their MSA system and our composi-tional model.Though individual ESDs may seem simple, thelearning task is challenging because of the limitedamount of training data, variability in the used vo-cabulary, optionality of events (e.g., going to thecoffee machine may not be mentioned in a ESD),different granularity of events and variability inthe ordering (e.g., coffee may be put in the filterbefore placing it in the coffee maker).
Unlike ourwork, Regneri et al.
(2010) relies on WordNet toprovide extra signal when using the Multiple Se-quence Alignment (MSA) algorithm.
As in theirwork, each description was preprocessed to extracta predicate and heads of argument noun phrases tobe used in the model.The methods are evaluated on human anno-tated scenario-specific tests: the goal is to classifyevent pairs as appearing in a stereotypical order ornot (Regneri et al., 2010).4The model was estimated as explained in Sec-tion 2.2 with the order of events in ESDs treatedas gold standard.
We used 4 held-out scenariosto choose model parameters, no scenario-specifictuning was performed, and the 10 test scripts werenot used to perform model selection.
The selectedmodel used the dimensionality of 10 for event andword embeddings.
The initial learning rate and theregularization parameter were set to 0.005 and 1.0,respectively and both parameters were reduced bythe factor of 1.2 every epoch the error functionwent up.
We used 2000 epochs of stochastic gradi-ent descent.
Dropout (Hinton et al., 2012) with therate of 20% was used for the hidden layers in allour experiments.
When testing, we predicted thatthe event pair (e1,e2) is in the stereotypical order(e1?
e2) if the ranking score for e1exceeded theranking score for e2.3.1.2 Results and discussionWe evaluated our event embedding model (EE)against baseline systems (BL , MSA and BS).
MSAis the system of Regneri et al.
(2010).
BS is ahierarchical Bayesian model by Frermann et al.(2014).
BL chooses the order of events based onthe preferred order of the corresponding verbs inthe training set: (e1, e2) is predicted to be in the4The event pairs are not coming from the same ESDsmaking the task harder as the events may not be in any tem-poral relation.52stereotypical order if the number of times the cor-responding verbs v1and v2appear in this orderin the training ESDs exceeds the number of timesthey appear in the opposite order (not necessary atadjacent positions); a coin is tossed to break ties(or if v1and v2are the same verb).
This frequencycounting method was previously used in CJ08.5We also compare to the version of our modelwhich uses only verbs (EEverbs).
Note thatEEverbsis conceptually very similar to BL, as it es-sentially induces an ordering over verbs.
However,this ordering can benefit from the implicit transi-tivity assumption used in EEverbs(and EE), as wediscussed in the introduction.
The results are pre-sented in Table 1.The first observation is that the full model im-proves substantially over the baseline and the pre-vious method (MSA) in F1 (13.5% improvementover MSA and 6.5% improvement over BS).
Notealso that this improvement is consistent across sce-narios: EE outperforms MSA and BS on 9 scenar-ios out of 10 and 8 out of 10 scenarios in case ofBS.
Unlike MSA and BS, no external knowledge(i.e.
WordNet) was exploited in our method.We also observe a substantial improvement inall metrics from using transitivity, as seen by com-paring the results of BL and EEverb(11% improve-ment in F1).
This simple approach already sub-stantially outperforms the pipelined MSA system.These results seem to support our hypothesis inthe introduction that inducing graph representa-tions from scripts may not be an optimal strategyfrom the practical perspective.We performed additional experiments using theSENNA embeddings (Collobert et al., 2011).
In-stead of randomly initializing arguments and pred-icate embeddings (vectors), we initialized themwith pre-trained SENNA embeddings.
We havenot observed any significant boost in performancefrom using the initialization (average F1 of 84.0%for EE).
We attribute the lack of significant im-provement to the following three factors.
Firstof all, the SENNA embeddings tend to placeantonyms / opposites near each other (e.g., comeand go, or end and start).
However, ?opposite?predicates appear in very different positions inscripts.
Additionally, the SENNA embeddingshave dimensionality of 50 which appears to be5They scored permutations of several events by summingthe logarithmed differences of the frequencies of ordered verbpairs.
However, when applied to event pairs, their approachwould yield exactly the same prediction rule as BL.too high for small crowd-sourced datasets, as itforces us to use larger matrices T and R. More-over, the SENNA embeddings are estimated fromWikipedia, and the activities in our crowdsourceddomain are perhaps underrepresented there.3.1.3 ParaphrasingRegneri et al.
(2010) additionally measure para-phrasing performance of the MSA system by com-paring it to human annotation they obtained: a sys-tem needs to predict if a pair of event mentions areparaphrases or not.
The dataset contains 527 eventpairs for the 10 test scenarios.
Each pair consistsof events from the same scenario.
The dataset isfairly balanced containing from 47 to 60 examplesper scenario.This task does not directly map to any statisti-cal inference problem with our model.
Instead weuse an approach inspired by the interval algebra ofAllen (1983).Our ranking model maps event mentions to po-sitions on the time line (see Figure 2).
However,it would be more natural to assume that events areintervals rather than points.
In principle, these in-tervals can be overlapping to encode a rich set oftemporal relations (see (Allen, 1983)).
However,we make a simplifying assumption that the inter-vals do not overlap and every real number belongsto an interval.
In other words, our goal is to inducea segmentation of the line: event mentions corre-sponding to the same interval are then regarded asparaphrases.One natural constraint on this segmentation isthe following: if two event mentions are from thesame training ESD, they cannot be assigned to thesame interval (as events in ESD are not supposedto be paraphrases).
In Figure 2 arcs link eventmentions from the same ESD.
We look for a seg-mentation which produces the minimal number ofsegments and satisfy the above constraint for eventmentions appearing in training data.Though inducing intervals given a set of tem-poral constraints is known to be NP-hard in gen-eral (see, e.g., (Golumbic and Shamir, 1993)), forour constraints a simple greedy algorithm finds anoptimal solution.
We trace the line from the leftmaintaining a set of event mentions in the currentunfinished interval and create a boundary when theconstraint is violated; we repeat the process un-til we processed all mentions.
In Figure 2, wewould create the first boundary between arrivein a restaurant and order beverages: order bev-53EnterarestaurantArriveinarestaurant...OrderbeveragesBrowseamenuReviewoptionsinamenuFigure 2: Events on the time line, dotted arcs linkevents from the same ESD.erages and enter a restaurant are from the sameESD and continuing the interval would violatethe constraint.
It is not hard to see that this re-sults in an optimal segmentation.
First, the seg-mentation satisfies the constraint by construction.Secondly, the number of segments is minimal asthe arcs which caused boundary creation are non-overlapping, each of these arcs needs to be cut andour algorithm cuts each arc exactly once.This algorithm prefers to introduce a bound-ary as late as possible.
For example, it wouldintroduce a boundary between browse a menuand review options in a menu even though thecorresponding points are very close on the line.We modify the algorithm by moving the bound-aries left as long as this move does not resultin new constraint violations and increases mar-gin at boundaries.
In our example, the boundarywould be moved to be between order beveragesand browse a menu, as desired.The resulting performance is reported in Ta-ble 2.
We report results of our method, as well asresults for MSA, BS and a simple all-paraphrasebaseline which predict that all mention pairs in atest set are paraphrases (APBL).6We can see thatinterval induction technique results in a lower F1than that of MSA or BS.
This might be partiallydue to not using external knowledge (WordNet) inour method.We performed extra analyses on the develop-ment scenario doorbell.
The analyses revealed thatthe interval induction approach is not very robustto noise: removing a single noisy ESD results in adramatic change in the interval structure inducedand in a significant increase of F1.
Consequently,soft versions of the constraint would be beneficial.Alternatively, event embeddings (i.e.
continuousvectors) can be clustered directly.
We leave this6The results for the random baseline are lower: F1 of40.6% in average.Scenario F1 (%)APBL MSA BS EETake bus 53.7 74.0 47.0 63.5Make coffee 42.1 65.0 52.0 63.5Order fastfood 37.0 59.0 80.0 62.6Return food back 64.8 71.0 67.0 81.1Iron clothes 43.3 67.0 60.0 56.7Microwave cooking43.2 75.0 82.0 57.8Scrambled eggs57.6 69.0 76.0 53.0Take shower 42.1 78.0 67.0 55.7Answer telephone 71.0 89.0 81.0 79.4Vending machine56.1 69.0 77.0 69.3Average 51.1 71.6 68.9 64.5Table 2: Paraphrasing results on the crowdsourceddata for Regneri et al.
(2010) (MSA), Frermannet al.
(2014)(BS) and the all-paraphrase baseline(APBL) and using intervals induced from ourmodel (EE).investigation for future work.3.2 Learning from Natural TextIn the second set of experiments we consider amore challenging problem, inducing knowledgeabout the stereotyped ordering of events from nat-ural texts.
In this work, we are largely inspiredby the scenario of CJ08.
The overall strategy isthe following: we process the Gigaword corpuswith a high precision rule-based temporal classi-fier relying on explicit clues (e.g., ?then?, ?after?
)to get ordered pairs of events and then we trainour model on these pairs (note that clues used bythe classifier are removed from the examples, sothe model has to rely on verbs and their argu-ments).
Conceptually, the difference between ourapproach and CJ08 is in using a different tempo-ral classifier, not enforcing that event pairs havethe same protagonist, and learning an event em-bedding model instead of scoring event sequencesbased on verb-pair frequencies.We also evaluate our system on examples ex-tracted using the same temporal classifier (but val-idated manually) which allows us to use muchlarger tests set, and, consequently, provide moredetailed and reliable error analysis.3.2.1 Data and taskThe Gigaword corpus consists of news data fromdifferent news agencies and newspapers.
For test-ing and development we took the AFP (AgenceFrance-Presse) section, as it appeared most differ-ent from the rest when comparing sets of extractedevent pairs (other sections correspond mostly toUS agencies).
The AFP section was not used for54Accuracy (%)BL 60.7CJ08 60.1EEverb75.9EE 83.5Table 3: Results on the Gigaword data for theverb-frequency baseline (BL), the verb-only em-bedding model (EEverb), the full model (EE) andCJ08 rules.training.
This selection strategy was chosen to cre-ate a negative bias for our model which is moreexpressive than the baseline methods and, conse-quently, better at memorizing examples.As a rule-based temporal classifier, we usedhigh precision ?happens-before?
rules from theVerbOcean system (Chklovski and Pantel, 2004).Consider ?to ?verb-x?
and then ?verb-y??
as oneexample of such rule.
We used predicted collapsedStanford dependencies (de Marneffe et al., 2006)to extract arguments of the verbs, and used onlya subset of dependents of a verb.7This prepro-cessing ensured that (1) clues which form part ofa pattern are not observable by our model both attrain and test time; (2) there is no systematic dif-ference between both events (e.g., for collapseddependencies, the noun subject is attached to bothverbs even if the verbs are conjoined); (3) no in-formation about the order of events in text is avail-able to the models.
Applying these rules resultedin 22,446 event pairs for training, and we splitadditional 1,015 pairs from the AFP section into812 for final testing and 203 for development.
Wemanually validated random 50 examples and all 50of them followed the correct temporal order, so wechose not to hand correct the test set.We largely followed the same training and eval-uation regime as for the crowdsourced data.
Weset the regularization parameter and the learningrate to 0.01 and 5.e ?
4 respectively.
The modelwas trained for 600 epochs.
The embedding sizeswere 30 and 50 dimensions for words and events,respectively.3.2.2 Results and discussionIn our experiments, as before, we use BL as abaseline, and EEverbas a verb-only simplifiedversion of our approach.
We used another baseline7The list of dependencies not considered: aux, auxpass,attr, appos, cc, conj, complm, cop, dep, det, punct, mwe.consisting of the verb pair ordering counts pro-vided by Chambers and Jurafsky (2008).8We re-fer this baseline as CJ08.
Note also that BL can beregarded as a reimplementation of CJ08 but witha different temporal classifier.
We report results inTable 3.The observations are largerly the same as be-fore: (1) the full model substantially outperformsall other approaches (p-level < 0.001 with the per-mutation test); (2) enforcing transitivity is veryhelpful (75.9 % for EEverbvs.
60.1% for BL).Surprisingly CJ08 rules produce as good resultsas BL, suggesting that maybe our learning set-upsare not that different.However, an interesting question is in which sit-uations using a more expressive model, EE, is ben-eficial.
If these accuracy gains have to do withmemorizing the data, it may not generalize wellto other domains or datasets.
In order to test thishypothesis we divided the test examples in threefrequency bands according to the frequency of thecorresponding verb pairs in the training set (to-tal, in both orders).
There are 513, 249 and 50event pairs in the bands corresponding to unseenpairs of verbs, frequency ?
10 and frequency >10, respectively.
These counts emphasize that cor-rect predictions on unseen pairs are crucial andthese are exactly where BL would be equivalentto a random guess.
Also, this suggest, even beforelooking into the results, that memorization is irrel-evant.
The results for BL, CJ08, EEverband EEare shown in Figure 3.One observation is that most gains for EE andEEverbare due to an improvement on unseen pairs.This is fairly natural, as both transitivity and in-formation about arguments are the only sourcesof information available.
In this context it is im-portant to note that some of the verbs are light,in the sense that they have little semantic contentof their own (e.g., take, get) and the event seman-tics can only be derived from analyzing their argu-ments (e.g., take an exam vs. take a detour).
Onthe high frequency verb pairs all systems performequally well, except for CJ08 as it was estimatedfrom somewhat different data.In order to understand how transitivity works,we considered a few unseen predicate pairs wherethe EEverbmodel was correctly predicting theirorder.
For many of these pairs there were no infer-8These verb pair frequency counts are available atwww.usna.edu/Users/cs/nchamber/data/schemas/acl09/verb-pair-orders.gz55?????????????????????
??????
???
?50.057.271.082.462.777.881.883.181.296.094.196.0CJ08BLEEverbEEFigure 3: Results for different frequency bands:unseen, medium frequency (between 1 and 10)and high frequency (> 10) verb pairs.ence chains of length 2 (e.g., chain of length 2 wasfound for the pair accept ?
carry: accept ?
getand get ?
carry but not many other pairs).
Thisobservation suggest that our model captures somenon-trivial transitivity rules.4 Related WorkAdditionally to the work on script inductiondiscussed above (Chambers and Jurafsky, 2008,2009; Regneri et al., 2010), other methods forunsupervised learning of event semantics havebeen proposed.
These methods include unsu-pervised frame induction techniques (O?Connor,2012; Modi et al., 2012).
Frames encode situa-tions (or objects) along with their participants andproperties (Fillmore, 1976).
Events in these un-supervised approaches are represented with cate-gorical latent variables, and they are induced rely-ing primarily on the selectional preferences?
sig-nal.
The very recent work of Cheung et al.
(2013)can be regarded as their extension but Cheung etal.
also model transitions between events withMarkov models.
However, neither of these ap-proaches considers (or directly optimizes) the dis-criminative objective of learning to order events,and neither of them uses distributed representa-tions to encode semantic properties of events.As we pointed out before, our embedding ap-proach is similar (or, in fact, a simplification of)the phrase embedding methods studied in the re-cent work on distributional compositional seman-tics (Baroni and Zamparelli, 2011; Socher et al.,2012).
However, they have not specifically lookedinto representing script information.
Approacheswhich study embeddings of relations in knowledgebases (e.g., Riedel et al.
(2013)) bear some similar-ity to the methods proposed in this work but theyare mostly limited to binary relations and deal withpredicting missing relations rather than with tem-poral reasoning of any kind.Identification of temporal relations within a textis a challenging problem and an active area of re-search (see, e.g., the TempEval task (UzZamanet al., 2013)).
Many rule-based and supervised ap-proaches have been proposed in the past.
How-ever, integration of common sense knowledge in-duced from large-scale unannotated resources stillremains a challenge.
We believe that our approachwill provide a powerful signal complementary toinformation exploited by most existing methods.5 ConclusionsWe have developed a statistical model for rep-resenting common sense knowledge about proto-typical event orderings.
Our model induces dis-tributed representations of events by composingpredicate and argument representations.
Theserepresentations capture properties relevant to pre-dicting stereotyped orderings of events.
We learnthese representations and the ordering componentfrom unannotated data.
We evaluated our modelin two different settings: from crowdsourced dataand natural news texts.
In both set-ups our methodoutperformed baselines and previously proposedsystems by a large margin.
This boost in perfor-mance is primarily caused by exploiting transitiv-ity of temporal relations and capturing informationencoded by predicate arguments.The primary area of future work is to exploitour method in applications such as question an-swering.
Another obvious applications is discov-ery of temporal relations within documents (Uz-Zaman et al., 2013) where common sense knowl-edge implicit in script information, induced fromlarge unannotated corpora, should be highly ben-eficial.
Our current model uses a fairly naive se-mantic composition component, we plan to extendit with more powerful recursive embedding meth-ods which should be especially beneficial whenconsidering very large text collections.6 AcknowledgementsThanks to Lea Frermann, Michaela Regneri andManfred Pinkal for suggestions and help with thedata.
This work is partially supported by theMMCI Cluster of Excellence at the Saarland Uni-versity.56ReferencesJames F Allen.
1983.
Maintaining knowledgeabout temporal intervals.
Communications ofthe ACM, 26(11):832?843.Marco Baroni and Robert Zamparelli.
2011.Nouns are vectors, adjectives are matrices: Rep-resenting adjective-noun constructions in se-mantic space.
In Proceedings of EMNLP.Yoshua Bengio, R?ejean Ducharme, and PascalVincent.
2001.
A neural probabilistic languagemodel.
In Proceedings of NIPS.Jonathan Berant, Ido Dagan, and Jacob Gold-berger.
2011.
Global learning of typed entail-ment rules.
In Proceedings of ACL.Nathanael Chambers and Dan Jurafsky.
2009.
Un-supervised learning of narrative schemas andtheir participants.
In Proceedings of ACL.Nathanael Chambers and Daniel Jurafsky.
2008.Unsupervised learning of narrative event chains.In Proceedings of ACL.Jackie Chi Kit Cheung, Hoifung Poon, and LucyVanderwende.
2013.
Probabilistic frame induc-tion.
In Proceedings of NAACL.Timothy Chklovski and Patrick Pantel.
2004.
Ver-bocean: Mining the web for fine-grained se-mantic verb relations.
In Proceedings ofEMNLP.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Naturallanguage processing (almost) from scratch.Journal of Machine Learning Research,12:2493?2537.Marie-Catherine de Marneffe, Bill MacCartney,and Christopher D. Manning.
2006.
Generatingtyped dependency parses from phrase structureparses.
In Proceedings of LREC.Charles Fillmore.
1976.
Frame semantics and thenature of language.
Annals of the New YorkAcademy of Sciences, 280(1):20?32.Lea Frermann, Ivan Titov, and Manfred Pinkal.2014.
A hierarchical bayesian model for un-supervised induction of script knowledge.
InEACL, Gothenberg, Sweden.Martin Charles Golumbic and Ron Shamir.
1993.Complexity and algorithms for reasoning abouttime: A graph-theoretic approach.
Journal ofACM, 40(5):1108?1133.Andrew Gordon.
2001.
Browsing image collec-tions with representations of common-sense ac-tivities.
JAIST, 52(11).Rakesh Gupta and Mykel J. Kochenderfer.
2004.Common sense data acquisition for indoor mo-bile robots.
In Proceedings of AAAI.Geoffrey E. Hinton, Nitish Srivastava, AlexKrizhevsky, Ilya Sutskever, and RuslanSalakhutdinov.
2012.
Improving neural net-works by preventing co-adaptation of featuredetectors.
arXiv: CoRR, abs/1207.0580.Ashutosh Modi, Ivan Titov, and Alexandre Kle-mentiev.
2012.
Unsupervised induction offrame-semantic representations.
In Proceedingsof the NAACL-HLT Workshop on Inducing Lin-guistic Structure.
Montreal, Canada.Erik T. Mueller.
1998.
Natural Language Process-ing with Thought Treasure.
Signiform.Brendan O?Connor.
2012.
Learning frames fromtext with an unsupervised latent variable model.CMU Technical Report.Robert Parker, David Graff, Junbo Kong,Ke Chen, and Kazuaki Maeda.
2011.
En-glish gigaword fifth edition.
Linguistic DataConsortium.Michaela Regneri, Alexander Koller, and ManfredPinkal.
2010.
Learning script knowledge withweb experiments.
In Proceedings of ACL.Sebastian Riedel, Limin Yao, Andrew McCal-lum, and Benjamin Marlin.
2013.
Relation ex-traction with matrix factorization and universalschemas.
TACL.R.
C Schank and R. P Abelson.
1977.
Scripts,Plans, Goals, and Understanding.
LawrenceErlbaum Associates, Potomac, Maryland.Richard Socher, Brody Huval, Christopher D.Manning, and Andrew Y. Ng.
2012.
Seman-tic compositionality through recursive matrix-vector spaces.
In Proceedings of EMNLP.Joseph Turian, Lev Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and gen-eral method for semi-supervised learning.
InProceedings of ACL.Naushad UzZaman, Hector Llorens, Leon Der-czynski, James Allen, Marc Verhagen, andJames Pustejovsky.
2013.
Semeval-2013 task1: Tempeval-3: Evaluating time expressions,events, and temporal relations.
In Proceedingsof SemEval.57
