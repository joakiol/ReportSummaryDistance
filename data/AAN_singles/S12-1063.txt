First Joint Conference on Lexical and Computational Semantics (*SEM), pages 461?466,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUTDHLT: COPACETIC System for Choosing Plausible AlternativesTravis Goodwin, Bryan Rink, Kirk Roberts, Sanda M. HarabagiuHuman Language Technology Research InstituteUniversity of Texas DallasRichardson TX, 75080{travis,bryan,kirk,sanda}@hlt.utdallas.eduAbstractThe Choice of Plausible Alternatives (COPA)task in SemEval-2012 presents a series offorced-choice questions wherein each questionprovides a premise and two viable cause or ef-fect scenarios.
The correct answer is the causeor effect that is the most plausible.
This paperdescribes the COPACETIC system developedby the University of Texas at Dallas (UTD) forthis task.
We approach this task by casting itas a classification problem and using featuresderived from bigram co-occurrences, TimeMLtemporal links between events, single-word po-larities from the Harvard General Inquirer, andcausal syntactic dependency structures withinthe gigaword corpus.
Additionally, we showthat although each of these components im-proves our score for this evaluation, the dif-ference in accuracy between using all of thesefeatures and using bigram co-occurrence infor-mation alone is not statistically significant.1 The Problem?The surfer caught the wave.?
This statement, al-though almost tautological for human understanding,requires a considerable depth of semantic reasoning.What is a surfer?
What does it mean to ?catch awave??
How are these concepts related?
What ifwe want to ascertain, given that the surfer caught thewave, whether the most likely next event is that ?thewave carried her to the shore?
or that ?she paddled herboard into the ocean??
This type of causal and tempo-ral reasoning requires a breadth of world-knowledge,often called commonsense understanding.Question 15 (Find the EFFECT)Premise: I poured water on my sleeping friend.Alternative 1: My friend awoke.Alternative 2: My friend snored.Question 379 (Find the CAUSE)Premise: The man closed the umbrella.Alternative 1: He got out of the car.Alternative 2: He approached the building.Figure 1: An example of each type of question, one target-ing an effect, and another targeting a cause.The seventh task of SemEval-2012 evaluates pre-cisely this type of cogitation.
COPA: Choice of Plau-sible Alternatives presents 1,0001 sets of two-choicequestions (presented as a premise and two alterna-tives) provided in simple English sentences.
Thegoal for each question is to choose the most plausiblecause or effect entailed by the premise (the datasetprovided an equal distribution of cause and effecttargetting questions).
Additionally, each question islabeled so as to describe whether the answer shouldbe a cause or an effect, as indicated in Figure 1.The topics of these questions were drawn from twosources:1.
Randomly selected accounts of personal storiestaken from a collection of Internet weblogs (Gor-don and Swanson, 2009).2.
Randomly selected subject terms from the Li-brary of Congress Thesaurus for Graphic Mate-rials (of Congress.
Prints et al, 1980).Additionally, the incorrect alternatives were authored1This data set was split into a 500 question development (ortraining) set and a 500 question test set.461Figure 2: Architecture of the COPACETIC Systemwith the intent of impeding ?purely associative meth-ods?
(Roemmele et al, 2011).
The task aims toevaluate the state of commonsense causal reasoning(Roemmele et al, 2011).2 System ArchitectureGiven a question, such as Question 15 (as shownin Figure 1), our system selects the most plausiblealternative by using the output of an SVM classifier,trained on the 500 provided development questionsand tested on the 500 provided test questions.
Theclassifier operates with features describing informa-tion extracted from the processing of the question?spremise and alternatives.
As illustrated by Figure 2,the preprocessing involves part of speech (POS) tag-ging, and syntactic dependency parsing providedby the Stanford parser (Klein and Manning, 2003;Toutanova et al, 2003), multi-word expression detec-tion using Wikipedia, automatic TimeML annotationusing TARSQI (Verhagen et al, 2005; Pustejovskyet al, 2003), and Brown clustering as provided in(Turian, 2010).The architecture of the COPACETIC system is di-vided into offline (independent of any question) andonline (question dependent) processing.
The onlineaspect of our system inspects each question usingan SVM and selects the most likely alternative.
Oursystem?s offline functions focus on pre-processingresources so that they may be used by componentsof the online aspect of our system.
In the next sec-tion, we describe the offline processing upon whichour system is built, and in the following section, theonline manner in which we evaluate each question.2.1 Offline ProcessingBecause the questions presented in this task requirea wealth of commonsense knowledge, we first ex-tracted commonsense and temporal facts.
This sub-section describes the process of mining this informa-tion from the fourth edition of the English Gigawordcorpus2 (Parker et al, 2009).We collected commonsense facts by extractingcause and effect pairs using twenty-four hand-craftedpatterns.
Rather than lexical patterns, we used pat-terns over syntactic dependency structures in orderto capture the syntactic role each word plays.
Fig-ure 3 illuminates two examples of the dependencystructures encoded by our causal patterns.
CausalPattern 1 captures all cases of causality indicated bythe verb causes, while Causal Pattern 2 illustrates amore sophisticated pattern, in which the phrasal verbbrought on indicates causality.In order to extract this information, we first parsedthe syntactic dependence structure of each sentenceusing the Stanford parser (Klein and Manning, 2003).Next, we loaded each sentence?s dependence tree2The LDC Catalog number of the English Gigaword FourthEdition corpus is LDC2009T13.462CAUSAL PATTERN 1:"causes"?causensubj?effectdobjCAUSAL PATTERN 2:"causenb"?causejdsco"uj"a?effectucoFigure 3: The dependency structures associated withthe causal patterns: ?cause ?causes?
?effect, and?cause ?brought on?
?effect.into the RDF3X (Neumann and Weikum, 2008)implementation of an RDF3 database.
Then, werepresented our dependency structures using in theSPARQL4query language and extracted cause andeffect pairs by issuing SPARQL queries against theRDF3X database.
We used SPARQL and RDF repre-sentations because they allowed us to easily representand reason over graphical structures, such as those ofour dependency trees.It has been shown that causality often manifests asa temporal relation (Bethard, 2008; Bethard and Mar-tin, 2008).
The questions presented in this task areno exception: many of the alternative-premise pairsnecessitate temporal understanding.
For example,consider question 63 provided in Figure 4.Question 63 (Find the EFFECT)Premise: The man removed his coat.Alternative 1: He entered the house.Alternative 2: He loosened his tie.Figure 4: Example question 63, which illustrates the ne-cessity for temporal reasoning.3The Resource Description Framework (RDF) is is a spec-ification from the W3C.
Information on RDF is available athttp://www.w3.org/RDF/.3The SPARQL Query Language is defined at http://www.w3.org/TR/rdf-sparql-query/.
An examples of theWHERE clause for a SPARQL query associated with the broughton pattern from Figure 3 is provided below:{ ?a <nsubj> ?cause ;<token> "brought" ;<prep> ?b .
?b <token> "on" ;<pobj> ?effect .
}In order to extract this temporal information, weautomatically annotated our corpus with TimeMLannotations using the TARSQI Toolkit (Verhagenet al, 2005).
Unfortunately, the events representedin this corpus were too sparse to use directly.
Tomitigate this sparsity, we clustered events using the3,200 Brown clusters5 described in (Turian, 2010).After all such offline processing has been com-pleted, we incorporate the knowledge encoded bythis processing in the online components of our sys-tem (online preprocessing, and feature extraction) asdescribed in the following section.2.2 Online ProcessingWe cast the task of selecting the most plausible al-ternative as a classification problem, using a supportvector machine (SVM) supervised classifier (usinga linear kernel).
To this end, we pre-process eachquestion for lexical information.
We extract partsof speech (POS) and syntactic dependencies usingthe Stanford CoreNLP parser (Klein and Manning,2003; Toutanova et al, 2003).
Stopwords are re-moved using a manually curated list of one hundredand one common stopwords; non-content words (de-fined as words whose POS is not a noun, verb, oradjective) are also discarded.
Additionally, we ex-tract multi-word expressions (noun collocations6 andphrasal verbs7).
Finally, in order to utilize our of-fline TimeML annotations, we extract events usingPOS.
Examples of the retained content words areunderlined in Figures 5, 6, 7 and 8.After preprocessing each question, we convertit into two premise-alternative pairs (PREMISE-ALTERNATIVE1, and PREMISE-ALTERNATIVE2).For each of these pairs, we attempt to form a bridgefrom the causal sentence to the effect sentence, with-out distinction over whether the cause or effect origi-nated from the premise or the alternative.
This bridgeis provided by four measures, or features, describedin the following section.5These clusters are available at http://metaoptimize.com/projects/wordreprs/.6These were detected using a list of English Wikipedia ar-ticle titles available at http://dumps.wikimedia.org/backup-index.html.7Phrasal verbs were determined using a list avail-able at http://www.learn-english-today.com/phrasal-verbs/phrasal-verb-list.htm.4633 The Features of the COPACETICSystemIn determining the causal relatedness between a causeand an effect sentence, we utilize four features.
Eachfeature calculates a value indicating the perceivedstrength of the causal relationship between a causeand an effect using a different measure of causality.The four features used by our COPACETIC systemare described in the following subsections.3.1 Bigram RelatednessOur first feature measures the degree of relatednessbetween all pairs of bigrams (at the token level) in thecause and effect pair.
We do this by calculating thepoint-wise mutual Information (PMI) (Fano, 1961)for all bigram combinations between the candidatealternative and its premise in the English Gigawordcorpus (Parker et al, 2009) as shown in Equation 1.PMI(x; y) ?
logp(x, y)p(x)p(y)(1)Under the assumption that distance words are un-likely to causally influence each other, we only con-sider co-occurrences within a window of one hundredtokens when calculating the joint probability of thePMI.
Additionally, we allow for up to two tokensto occur within a single bigram?s occurrence (e.g.the phrase pierced her ears would be considered amatch for the bigram pierced ears ).
Although theserelaxations skew the values of our calculated PMIsby artificially lowering the joint probability, we areonly concerned with how the values compare to eachother.
Note that because we employ no smoothing,the PMI of an unseen bigram is set to zero.
The max-imum PMI over all pairs of bigrams is retained as thevalue for this feature.
Figure 5 illustrates this featurefor Question 495.3.2 Temporal RelatednessAlthough most of the questions in this task focus oncausal relationships, for many questions, the natureof this causal relationship manifests instead as a tem-poral one (Bethard and Martin, 2008; Bethard, 2008).We use temporal link information from TimeML(Pustejovsky et al, 2005; Pustejovsky et al, 2003)annotations on our corpus to determine how tempo-rally related a given cause and effect sentence are.Question 495 (Find the EFFECT)Premise: The girl wanted to wear earrings.Alternative 1: She got her ears pierced.Alternative 2: She got a tattoo.Alternative 1 Alternative 2PMI(wear earrings, pierced ears) = -10.928 PMI(wear earrings, tattoo) = -12.77PMI(wanted wear, pierced ears) = -13.284 PMI(wanted wear, tattoo) = -14.284PMI(girl wanted, pierced ears) = -13.437 PMI(girl wanted, tattoo) = -14.762PMI(girl, pierced ears) = -15.711 PMI(girl, tattoo) = -14.859Maximum PMI = -10.928 Maximum PMI = -12.77Figure 5: Example PMI values for bigrams and unigrams(with content words underlined).
Alternative 1 is correctlychosen as it has largest maxi mum PMI.This is accomplished by using the point-wise mutualinformation (PMI) between all pairs of events fromthe cause to the effect (see Equation 1).
We definethe relevant probabilities as follows:?
The joint probability (P (x, y)) of a cause andeffect event is defined as the number of timesthe cause event participates in a temporal linkending with the effect event.?
The probability of a cause event (P (x)) is de-fined as the number of times the cause eventprecipitates a temporal link to any event.?
The probability of an effect event (P (y)) is de-fined as the number of times the effect eventends a temporal link begun by any event.We define the PMI to be zero for any unseen pair ofevents (and for any pairs involving an unseen event).The summation of all pairs of PMIs is used as thevalue of this feature.
Figure 6 shows how this featurebehaves.Question 468 (Find the CAUSE)Premise: The dog barked.Alternative 1: The cat lounged on the couch.Alternative 2: A knock sounded at the door.Alternative 1 Alternative 2PMI(lounge, bark) = 5.60436 PMI(knock, bark) = 5.77867PMI(sound, bark) = 5.26971Figure 6: Example temporal PMI values (with contentwords underlined).
Alternative 2 is correctly chosen as ithas the highest summation.3.3 Causal Dependency StructuresWe attempted to capture the degree of direct causal re-latedness between a cause sentence and an effect sen-tence.
To determine the strength of this relationship,464we considered how often phrases from the cause andeffect sentences occur within a causal dependencystructure.
We detect this through the use of twenty-four8 manually crafted causal patterns (described inSection 2.1).
The alternative that has the maximumnumber of matched dependency structures with thepremise is retained as the correct choice.
Figure 7illustrates this feature.Question 490 (Find the EFFECT)Premise: The man won the lottery.Alternative 1: He became rich.Alternative 2: He owed money.Alternative 1 Alternative 2won?
rich = 15 won?
owed = 5Figure 7: Example casual dependency matches (with con-tent words underlined).
Alternative 1 is correctly selectedbecause more patterns extracted ?won?
causing ?rich?
than?won?
causing ?owed?.3.4 Polarity ComparisonWe observed that many of the questions involve thedilemma of determining whether a positive premiseis more related to a positive or negative alternative(and vice-versa).
This differs from sentiment analysisin that rather than determining if a sentence expressesa negative statement or view, we instead desire theoverall sentimental connotation of a sentence (andthus of each word).
For example, the premise fromQuestion 494 (Figure 8) is ?the woman became fa-mous.?
Although this sentence makes no positive ornegative claims about the woman, the word ?famous??
when considered on its own ?
implies positive con-notations.We capture this information using the HarvardGeneral Inquirer (Stone et al, 1966).
Originally de-veloped in 1966, the Harvard General Inquirer pro-vides a mapping from English words to their polarity(POSITIVE, or NEGATIVE).
For example, it de-notes the word ?abandon?
as NEGATIVE, and theword ?abound?
as POSITIVE.
We use this informa-tion by summing the score for all words in a sen-tence (assigning POSITIVE words a score of 1.0,NEGATIVE words a score of -1.0, and NEUTRAL orunseen words a score of 0.0).
The difference between8Twenty-four patterns was deemed sufficient due to timeconstraints.these scores between the cause sentence and the ef-fect sentence is used as the value of this feature.
Thisfeature is illustrated in Figure 8.Question 494 (Find the CAUSE)Premise: The woman became famous.Alternative 1: Photographers followed her.Alternative 2: Her family avoided her.Premise Alternative 1 Alternative 2famous POSITIVE 1.0 follow NEUTRAL 0.0 avoid NEGATIVE?1.0photographer NEUTRAL 0.0 family NEUTRAL 0.0Sum 1.0 Sum 0.0 Sum ?1.0Figure 8: Example polarity comparison (with contentwords underlined).
Alternative 1 is correctly chosen as ithas the least difference from the score of the premise.4 ResultsThe COPA task of SemEval-2012 provided partici-pants with 1,000 causal questions, divided into 500questions for development or training, and 500 ques-tions for testing.
We submitted two systems to theCOPA Evaluation for SemEval-2012, both of whichare trained on the 500 development questions.
Ourfirst system uses only the bigram PMI feature and isdenoted as bigram pmi.
Our second system usesall four features and is denoted as svm combined.The accuracy of our two systems on the 500 providedtest questions is provided in Table 1 (Gordon et al,2012).
On this task, accuracy is defined as the quo-tient of dividing the number of questions for whichthe correct alternative was chosen by the number ofquestions.
Although multiple groups registered, ourswere the only submitted results.
Note that the differ-ence in performance between our two systems is notstatistically significant (p = 0.411) (Gordon et al,2012).Team ID System ID ScoreUTDHLT bigram pmi 0.618UTDHLT svm combined 0.634Table 1: Accuracy of submitted systemsThe primary hindrance to our approach is in com-bining each feature ?
that is, determining the con-fidence of each feature?s judgement.
Because thequestions vary significantly in their subject matterand the nature of the causal relationship betweengiven causes and effects, a single approach is unlikely465to satisfy all scenarios.
Unfortunately, the problemof determining which feature best applies to a givequestion requires non-trivial reasoning over implicitsemantics between the premise and alternatives.5 ConclusionThis evaluation has shown that although common-sense causal reasoning is trivial for humans, it beliesdeep semantic reasoning and necessitates a breadth ofworld knowledge.
Additional progress towards cap-turing world knowledge by leveraging a large numberof cross-domain knowledge resources is necessary.Moreover, distilling information not specific to anydomain ?
that is, a means of inferring basic and fun-damental information about the world ?
is not onlynecessary but paramount to the success of any fu-ture system desiring to build chains of commonsenseor causal reasoning.
At this point, we are merelyapproximating such possible distillation.6 AcknowledgementsWe would like to thank the organizers of SemEval-2012 task 7 for their work constructing the datasetand overseeing the task.References[Bethard and Martin2008] S. Bethard and J.H.
Martin.2008.
Learning semantic links from a corpus of paralleltemporal and causal relations.
Proceedings of the 46thAnnual Meeting of the ACL-HLT.
[Bethard2008] S Bethard.
2008.
Building a corpus oftemporal-causal structure.
Proceedings of the SixthLREC.
[Fano1961] RM Fano.
1961.
Transmission of Information:A Statistical Theory of Communication.
[Gordon and Swanson2009] A. Gordon and R. Swanson.2009.
Identifying personal stories in millions of weblogentries.
In Third International Conference on Weblogsand Social Media, Data Challenge Workshop, San Jose,CA.
[Gordon et al2012] Andrew Gordon, Zornitsa Kozareva,and Melissa Roemmele.
2012.
(2012) SemEval-2012Task 7: Choice of Plausible Alternatives: An Evalua-tion of Commonsense Causal Reasoning.
In Proceed-ings of the 6th International Workshop on SemanticEvaluation (SemEval 2012), Montreal.
[Klein and Manning2003] D. Klein and C.D.
Manning.2003.
Accurate unlexicalized parsing.
In Proceed-ings of the 41st Annual Meeting on Association forComputational Linguistics-Volume 1, pages 423?430.Association for Computational Linguistics.
[Neumann and Weikum2008] Thomas Neumann and Ger-hard Weikum.
2008.
RDF-3X: a RISC-style engine forRDF.
Proceedings of the VLDB Endowment.
[of Congress.
Prints et al1980] Libraryof Congress.
Prints, Photographs Division, andE.B.
Parker.
1980.
Subject headings used in the libraryof congress prints and photographs division.
Prints andPhotographs Division, Library of Congress.
[Parker et al2009] Robert Parker, David Graff, JunboKong, Ke Chen, and Kazuaki Maeda.
2009.
EnglishGigaword Fourth Edition.
[Pustejovsky et al2003] J Pustejovsky, J Castano, andR Ingria.
2003.
TimeML: Robust specification ofevent and temporal expressions in text.
AAAI SpringSymposium on New Directions in Question-Answering.
[Pustejovsky et al2005] J Pustejovsky, Bob Ingria, RoserSauri, Jose Castano, Jessica Littman, Rob Gaizauskas,Andrea Setzer, G. Katz, and I. Mani.
2005.
The speci-fication language TimeML.
The Language of Time: AReader.
[Roemmele et al2011] Melissa Roemmele, Cos-min Adrian Bejan, and Andrew S. Gordon.
2011.Choice of Plausible Alternatives: An Evaluation ofCommonsense Causal Reasoning.
2011 AAAI SpringSymposium Series.
[Stone et al1966] P. J.
Stone, D.C. Dunphy, and M. S.Smith.
1966.
The General Inquirer: A ComputerApproach to Content Analysis.
MIT Press.
[Toutanova et al2003] K. Toutanova, D. Klein, C.D.
Man-ning, and Y.
Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Proceed-ings of the 2003 Conference of NAACL-HLT, pages173?180.
Association for Computational Linguistics.
[Turian2010] J Turian.
2010.
Word representations: a sim-ple and general method for semi-supervised learning.Proceedings of the 48th Annual Meeting of the ACL,pages 384?394.
[Verhagen et al2005] M Verhagen, I Mani, and R Sauri.2005.
Automating Temporal Annotation with TARSQI.In Proceedings of the ACL 2005, pages 81?84.466
