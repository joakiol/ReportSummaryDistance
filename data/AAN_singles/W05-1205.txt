Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 25?30,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsRecognizing Paraphrases and Textual Entailment usingInversion Transduction GrammarsDekai Wu1Human Language Technology CenterHKUSTDepartment of Computer ScienceUniversity of Science and Technology, Clear Water Bay, Hong Kongdekai@cs.ust.hkAbstractWe present first results using paraphrase as well astextual entailment data to test the language univer-sal constraint posited by Wu?s (1995, 1997) Inver-sion Transduction Grammar (ITG) hypothesis.
Inmachine translation and alignment, the ITG Hypoth-esis provides a strong inductive bias, and has beenshown empirically across numerous language pairsand corpora to yield both efficiency and accuracygains for various language acquisition tasks.
Mono-lingual paraphrase and textual entailment recogni-tion datasets, however, potentially facilitate closertests of certain aspects of the hypothesis than bilin-gual parallel corpora, which simultaneously exhibitmany irrelevant dimensions of cross-lingual varia-tion.
We investigate this using simple generic Brack-eting ITGs containing no language-specific linguis-tic knowledge.
Experimental results on the MSRParaphrase Corpus show that, even in the absenceof any thesaurus to accommodate lexical variationbetween the paraphrases, an uninterpolated aver-age precision of at least 76% is obtainable fromthe Bracketing ITG?s structure matching bias alone.This is consistent with experimental results on thePascal Recognising Textual Entailment ChallengeCorpus, which show surpisingly strong results for anumber of the task subsets.1 IntroductionThe Inversion Transduction Grammar or ITG formalism,which historically was developed in the context of trans-lation and alignment, hypothesizes strong expressivenessrestrictions that constrain paraphrases to vary word or-der only in certain allowable nested permutations of ar-guments (Wu, 1997).
The ITG Hypothesis has been moreextensively studied across different languages, but newlyavailable paraphrase datasets provide intriguing opportu-1The author would like to thank the Hong Kong Re-search Grants Council (RGC) for supporting this researchin part through grants RGC6083/99E, RGC6256/00E, andDAG03/04.EG09, and Marine Carpuat and Yihai Shen for in-valuable assistance in preparing the datasets and stoplist.nities for meaningful analysis of the ITG Hypothesis in amonolingual setting.The strong inductive bias imposed by the ITG Hypoth-esis has been repeatedly shown empirically to yield bothefficiency and accuracy gains for numerous language ac-quisition tasks, across a variety of language pairs andtasks.
For example, Zens and Ney (2003) show thatITG constraints yield significantly better alignment cov-erage than the constraints used in IBM statistical ma-chine translation models on both German-English (Verb-mobil corpus) and French-English (Canadian Hansardscorpus).
Zhang and Gildea (2004) find that unsuper-vised alignment using Bracketing ITGs produces signif-icantly lower Chinese-English alignment error rates thana syntactically supervised tree-to-string model (Yamadaand Knight, 2001).
With regard to translation rather thanalignment accuracy, Zens et al (2004) show that decod-ing under ITG constraints yields significantly lower worderror rates and BLEU scores than the IBM constraints.We are conducting a series of investigations motivatedby the following observation: the empirically demon-strated suitability of ITG paraphrasing constraints acrosslanguages should hold, if anything, even more stronglyin the monolingual case.
The monolingual case allows insome sense closer testing of various implications of theITG hypothesis, without irrelevant dimensions of varia-tion arising from other cross-lingual phenomena.Asymmetric textual entailment recognition (RTE)datasets, in particular the Pascal Recognising Textual En-tailment Challenge Corpus (Dagan et al, 2005), providetestbeds that abstract over many tasks, including infor-mation retrieval, comparable documents, reading com-prehension, question answering, information extraction,machine translation, and paraphrase acquisition.At the same time, the emergence of paraphrasingdatasets presents an opportunity for complementary ex-periments on the task of recognizing symmetric bidirec-tional entailment rather than asymmetric directional en-tailment.
In particular, for this study we employ the MSRParaphrase Corpus (Quirk et al, 2004).252 Inversion Transduction GrammarsFormally, ITGs can be defined as the restricted subset ofsyntax-directed transduction grammars or SDTGs Lewisand Stearns (1968) where all of the rules are either ofstraight or inverted orientation.
Ordinary SDTGs allowany permutation of the symbols on the right-hand side tobe specified when translating from the input language tothe output language.
In contrast, ITGs only allow two outof the possible permutations.
If a rule is straight, the or-der of its right-hand symbols must be the same for bothlanguage.
On the other hand, if a rule is inverted, then theorder is left-to-right for the input language and right-to-left for the output language.
Since inversion is permittedat any level of rule expansion, a derivation may intermixproductions of either orientation within the parse tree.The ability to compose multiple levels of straight and in-verted constituents gives ITGs much greater expressive-ness than might seem at first blush.A simple example may be useful to fix ideas.
Considerthe following pair of parse trees for sentence translations:[[[The Authority]NP [will [[be accountable]VV [to[the [[Financial Secretary]NN ]NNN ]NP ]PP ]VP]VP ]SP .
]S[[[??
]NP [R?
[[5 [[[cu?
]NN ]NNN ]NP ]PP[?
]VV ]VP ]VP ]SP ]SEven though the order of constituents under the innerVP is inverted between the languages, an ITG can cap-ture the common structure of the two sentences.
This iscompactly shown by writing the parse tree together forboth sentences with the aid of an ??
angle bracket no-tation marking parse tree nodes that instantiate rules ofinverted orientation:[[[The/?Authority/ ?
?
]NP [will/R ??[be/?accountable/?
]VV [to/5 [the/?[[Financial/cuSecretary/?
]NN ]NNN ]NP ]PP?VP ]VP ]SP./]SIn a weighted or stochastic ITG (SITG), a weight or aprobability is associated with each rewrite rule.
Follow-ing the standard convention, we use a and b to denoteprobabilities for syntactic and lexical rules, respectively.For example, the probability of the rule NN 0.4?
[A N] isaNN?
[A N] = 0.4.
The probability of a lexical rule A0.001?x/y is bA(x, y) = 0.001.
Let W1,W2 be the vocabularysizes of the two languages, and N = {A1, .
.
.
, AN} bethe set of nonterminals with indices 1, .
.
.
, N .Wu (1997) also showed that ITGs can be equivalentlybe defined in two other ways.
First, ITGs can be definedas the restricted subset of SDTGs where all rules are ofrank 2.
Second, ITGs can also be defined as the restrictedsubset of SDTGs where all rules are of rank 3.Polynomial-time algorithms are possible for varioustasks including translation using ITGs, as well as bilin-gual parsing or biparsing, where the task is to build thehighest-scored parse tree given an input bi-sentence.For present purposes we can employ the special case ofBracketing ITGs, where the grammar employs only onesingle, undistinguished ?dummy?
nonterminal categoryfor any non-lexical rule.
Designating this category A, aBracketing ITG has the following form (where, as usual,lexical transductions of the form A ?
e/f may possiblybe singletons of the form A ?
e/ or A ?
/f ).A ?
[AA]A ?
?AA?A ?
, A ?
e1/f1.
.
.A ?
ei/fjThe simplest class of ITGs, Bracketing ITGs, areparticularly interesting in applications like paraphras-ing, because they impose ITG constraints in language-independent fashion, and in the simplest case do not re-quire any language-specific linguistic grammar or train-ing.
In Bracketing ITGs, the grammar uses only asingle, undifferentiated non-terminal (Wu, 1995).
Thekey modeling property of Bracketing ITGs that is mostrelevant to paraphrase recognition is that they assignstrong preference to candidate paraphrase pairs in whichnested constituent subtrees can be recursively alignedwith a minimum of constituent boundary violations.
Un-like language-specific linguistic approaches, however, theshape of the trees are driven in unsupervised fashion bythe data.
One way to view this is that the trees arehidden explanatory variables.
This not only providessignificantly higher robustness than more highly con-strained manually constructed grammars, but also makesthe model widely applicable across languages in econom-ical fashion without a large investment in manually con-structed resources.Moreover, for reasons discussed by Wu (1997), ITGspossess an interesting intrinsic combinatorial property ofpermitting roughly up to four arguments of any frame tobe transposed freely, but not more.
This matches supris-ingly closely the preponderance of linguistic verb frametheories from diverse linguistic traditions that all allowup to four arguments per frame.
Again, this propertyemerges naturally from ITGs in language-independentfashion, without any hardcoded language-specific knowl-edge.
This further suggests that ITGs should do wellat picking out paraphrase pairs where the order of upto four arguments per frame may vary freely betweenthe two strings.
Conversely, ITGs should do well at re-jecting pairs where (1) too many words in one sentence26find no correspondence in the other, (2) frames do notnest in similar ways in the candidate sentence pair, or(3) too many arguments must be transposed to achieve analignment?all of which would suggest that the sentencesprobably express different ideas.As an illustrative example, in common similarity mod-els, the following pair of sentences (found in actual dataarising in our experiments below) would receive an inap-propriately high score, because of the high lexical simi-larity between the two sentences:Chinese president Jiang Zemin arrived in Japantoday for a landmark state visit .T?
R 4 t ?? )
)/6?
{ D?
?))ff??
.
(Jiang Zemin will be the first Chinese nationalpresident to pay a state vist to Japan.
)However, the ITG based model is sensitive enoughto the differences in the constituent structure (reflectingunderlying differences in the predicate argument struc-ture) so that our experiments show that it assigns a lowscore.
On the other hand, the experiments also show thatit successfully assigns a high score to other candidate bi-sentences representing a true Chinese translation of thesame English sentence, as well as a true English transla-tion of the same Chinese sentence.We investigate a model for the paraphrase recognitionproblem that employ simple generic Bracketing ITGs.The experimental results show that, even in the absenceof any thesaurus to accommodate lexical variation be-tween the two strings, the Bracketing ITG?s structurematching bias alone produces a significant improvementin average precision.3 Scoring MethodAll words of the vocabulary are included among the lex-ical transductions, allowing exact word matches betweenthe two strings of any candidate paraphrase pair.Each candidate pair of the test set was scored via theITG biparsing algorithm, which employs a dynamic pro-gramming approach as follows.Let the input English sen-tence be e1, .
.
.
, eT and the corresponding input Chinesesentence be c1, .
.
.
, cV .
As an abbreviation we write es..tfor the sequence of words es+1, es+2, .
.
.
, et, and simi-larly for cu..v; also, es..s =  is the empty string.
It isconvenient to use a 4-tuple of the form q = (s, t, u, v)to identify each node of the parse tree, where the sub-strings es..t and cu..v both derive from the node q. De-note the nonterminal label on q by `(q).
Then for anynode q = (s, t, u, v), define?q(i) = ?stuv(i) = maxsubtrees ofqP [subtree ofq, `(q) = i, i ??
es..t/cu..v]as the maximum probability of any derivation from i thatsuccessfully parses both es..t and cu..v .
Then the bestparse of the sentence pair has probability ?0,T,0,V (S).The algorithm computes ?0,T,0,V (S) using the follow-ing recurrences.
Note that we generalize argmax to thecase where maximization ranges over multiple indices,by making it vector-valued.
Also note that [ ] and ??
aresimply constants, written mnemonically.
The condition(S?
s)(t?S)+ (U ?u)(v?U) 6= 0 is a way to specifythat the substring in one but not both languages may besplit into an empty string  and the substring itself; thisensures that the recursion terminates, but permits wordsthat have no match in the other language to map to an instead.1.
Initialization?t?1,t,v?1,v(i) = bi(et/cv),1 ?
t ?
T1 ?
v ?
V?t?1,t,v,v(i) = bi(et/),1 ?
t ?
T0 ?
v ?
V?t,t,v?1,v(i) = bi(/cv),0 ?
t ?
T1 ?
v ?
V2.
Recursion For all i, s, t, u, v such that{1?i?N0?s<t?T0?u<v?Vt?s+v?u>2?stuv(i) = max[?
[ ]stuv(i), ??
?stuv(i)]?stuv(i) ={[ ] if ?
[ ]stuv(i) ?
???stuv(i)??
otherwisewhere?
[ ]stuv(i) = max1?j?N1?k?Ns?S?tu?U?v(S?s)(t?S)+(U?u)(v?U) 6=0ai?
[jk] ?sSuU (j) ?StUv(k)??????
[ ]stuv(i)?
[ ]stuv(i)?
[ ]stuv(i)?
[ ]stuv(i)????
?= argmax1?j?N1?k?Ns?S?tu?U?v(S?s)(t?S)+(U?u)(v?U) 6=0ai?
[jk] ?sSuU (j) ?StUv(k)??
?stuv(i) = max1?j?N1?k?Ns?S?tu?U?v(S?s)(t?S)+(U?u)(v?U) 6=0ai??jk?
?sSUv(j) ?StuU (k)????????stuv(i)???stuv(i)???stuv(i)???stuv(i)????
?= argmax1?j?N1?k?Ns?S?tu?U?v(S?s)(t?S)+(U?u)(v?U) 6=0ai??jk?
?sSUv(j) ?StuU (k)273.
Reconstruction Initialize by setting the root of theparse tree to q1 = (0, T, 0, V ) and its nonterminal la-bel to `(q1) = S. The remaining descendants in theoptimal parse tree are then given recursively for anyq = (s, t, u, v) by:LEFT(q) =????
?NIL if t?s+v?u?2(s, ?
[ ]q (`(q)), u, ?
[ ]q (`(q))) if ?q(`(q)) = [ ](s, ??
?q (`(q)), ??
?q (`(q))) if ?q(`(q)) = ?
?RIGHT(q) =????
?NIL if t?s+v?u?2(?
[ ]q (`(q)), t, ?
[ ]q (`(q)), v) if ?q(`(q)) = [ ](??
?q (`(q)), t, u, ??
?q (`(q))) if ?q(`(q)) = ?
?`(LEFT(q)) = ?
?q(`(q))q (`(q))`(RIGHT(q)) = ?
?q(`(q))q (`(q))As mentioned earlier, biparsing for ITGs can be ac-complished efficiently in polynomial time, rather than theexponential time required for classical SDTGs.
The re-sult in Wu (1997) implies that for the special case ofBracketing ITGs, the time complexity of the algorithmis ?
(T 3V 3)where T and V are the lengths of the twosentences.
This is a factor of V 3 more than monolingualchart parsing, but has turned out to remain quite practicalfor corpus analysis, where parsing need not be real-time.The ITG scoring model can also be seen as a variantof the approach described by Leusch et al (2003), whichallows us to forego training to estimate true probabilities;instead, rules are simply given unit weights.
The ITGscores can be interpreted as a generalization of classi-cal Levenshtein string edit distance, where inverted blocktranspositions are also allowed.
Even without probabilityestimation, Leusch et al found excellent correlation withhuman judgment of similarity between translated para-phrases.4 Experimental Results?ParaphraseRecognitionOur objective here was to isolate the effect of the ITGconstraint bias.
No training was performed with the avail-able development sets.
Rather, the aim was to establishfoundational baseline results, to see in this first round ofparaphrase recognition experiments what results could beobtained with the simplest versions of the ITG models.The MSR Paraphrase Corpus test set consists of 1725candidate paraphrase string pairs, each annotated for se-mantic equivalence by two or three human collectors.Within the test set, 66.5% of the examples were annotatedas being semantically equivalent.
The corpus was origi-nally generated via a combination of automatic filteringmethods, making it difficult to make specific claims aboutdistributional neutrality, due to the arbitrary nature of theexample selection process.The ITG scoring model produced an uninterpolatedaverage precision (also known as confidence weightedscore) of 76.1%.
This represents an improvement ofroughly 10% over the random baseline.
Note that thisimprovement can be achieved with no thesaurus or lexi-cal similarity model, and no parameter training.5 Experimental Results?TextualEntailment RecognitionThe experimental procedure for the monolingual textualentailment recognition task is the same as for paraphraserecognition, except that one string serves as the Text andthe other serves as the Hypothesis.Results on the textual entailment recognition task areconsistent with the above paraphrase recognition results.For the PASCAL RTE challenge datasets, across all sub-sets overall, the model produced a confidence-weightedscore of 54.97% (better than chance at the 0.05 level).
Allexamples were labeled, so precision, recall, and f-scoreare equivalent; the accuracy was 51.25%.For the RTE task we also investigated a second variantof the model, in which a list of 172 words from a stoplistwas excluded from the lexical transductions.
The moti-vation for this model was to discount the effect of wordssuch as ?the?
or ?of?
since, more often than not, theycould be irrelevant to the RTE task.Surprisingly, the stoplisted model produced worseresults.
The overall confidence-weighted score was53.61%, and the accuracy was 50.50%.
We discuss thereasons below in the context of specific subsets.As one might expect, the Bracketing ITG models per-formed better on the subsets more closely approximat-ing the tasks for which Bracketing ITGs were designed:comparable documents (CD), paraphrasing (PP), and in-formation extraction (IE).
We will discuss some impor-tant caveats on the machine translation (MT) and readingcomprehension (RC) subsets.
The subsets least close tothe Bracketing ITG models are information retrieval (IR)and question answering (QA).5.1 Comparable Documents (CD)The CD task definition can essentially be characterized asrecognition of noisy word-aligned sentence pairs.
Amongall subsets, CD is perhaps closest to the noisy word align-ment task for which Bracketing ITGs were originally de-veloped, and indeed produced the best results for bothof the Bracketing ITG models.
The basic model pro-duced a confidence-weighted score of 79.88% (accuracy71.33%), while the stoplisted model produced an essen-tially unchanged confidence-weighted score of 79.83%28(accuracy 70.00%).The results on the RTE Challenge datasets closely re-flect the larger-scale findings of Wu and Fung (2005),who demonstrate that an ITG based model yields farmore accurate extraction of parallel sentences from quasi-comparable non-parallel corpora than previous state-of-the-art methods.
Wu and Fung?s results also use the eval-uation metric of uninterpolated average precision (i.e.,confidence-weighted score).Note also that we believe the results here are artificiallylowered by the absence of any thesaurus, and that signifi-cantly further improvements would be seen with the addi-tion of a suitable thesaurus, for reasons discussed belowunder the MT subsection.5.2 Paraphrase Acquisition (PP)The PP task is also close to the task for which Brack-eting ITGs were originally developed.
For the PP task,the basic model produced a confidence-weighted score of57.26% (accuracy 56.00%), while the stoplisted modelproduced a lower confidence-weighted score of 51.65%(accuracy 52.00%).
Unlike the CD task, the greaterimportance of function words in determining equivalentmeaning between paraphrases appears to cause the degra-dation in the stoplisted model.The effect of the absence of a thesaurus is muchstronger for the PP task as opposed to the CD task.
In-spection of the datasets reveals much more lexical vari-ation between paraphrases, and shows that cases wherelexis does not vary are generally handled accurately bythe Bracketing ITG models.
The MT subsection belowdiscusses why a thesaurus should produce significant im-provement.5.3 Information Extraction (IE)The IE task presents a slight issue of misfit for theBracketing ITG models, but yielded good results any-how.
The basic Bracketing ITG model attempts to alignall words/collocations between the two strings.
However,for the IE task in general, only a substring of the Textshould be aligned to the Hypothesis, and the rest shouldbe disregarded as ?noise?.
We approximated this by al-lowing words to be discarded from the Text at little cost,by using parameters that impose only a small penalty onnull-aligned words from the Text.
(As a reasonable firstapproximation, this characterization of the IE task ig-nores the possibility of modals, negation, quotation, andthe like in the Text.
)Despite the slight modeling misfit, the Bracketing ITGmodels produced good results for the IE subset.
The basicmodel produced a confidence-weighted score of 59.92%(accuracy 55.00%), while the stoplisted model produceda lower confidence-weighted score of 53.63% (accuracy51.67%).
Again, the lower score of the stoplisted modelappears to arise from the greater importance of functionwords in ensuring correct information extraction, as com-pared with the CD task.5.4 Machine Translation (MT)One exception to expectations is the machine translationsubset, a task for which Bracketing ITGs were devel-oped.
The basic model produced a confidence-weightedscore of 34.30% (accuracy 40.00%), while the stoplistedmodel produced a comparable confidence-weighted scoreof 35.96% (accuracy 39.17%).However, the performance here on the machine trans-lation subset cannot be directly interpreted, for two rea-sons.First, the task as defined in the RTE Challenge datasetsis not actually crosslingual machine translation, but ratherevaluation of monolingual comparability between an au-tomatic translation and a gold standard human transla-tion.
This is in fact closer to the problem of defining agood MT evaluation metric, rather than MT itself.
Leuschet al (2003 and personal communication) found thatBracketing ITGs as an MT evaluation metric show ex-cellent correlation with human judgments.Second, no translation lexicon or equivalent was usedin our model.
Normally in translation models, includ-ing ITG models, the translation lexicon accommodateslexical ambiguity, by providing multiple possible lexi-cal choices for each word or collocation being translated.Here, there is no second language, so some substitutemechanism to accommodate lexical ambiguity would beneeded.The most obvious substitute for a translation lexiconwould be a monolingual thesaurus.
This would allowmatching synonomous words or collocations between theText and the Hypothesis.
Our original thought was to in-corporate such a thesaurus in collaboration with teams fo-cusing on creating suitable thesauri, but time limitationsprevented completion of these experiments.
Based on ourown prior experiments and also on Leusch et al?s expe-riences, we believe this would bring performance on theMT subset to excellent levels as well.5.5 Reading Comprehension (RC)The reading comprehension task is similar to the infor-mation extraction task.
As such, the Bracketing ITGmodel could be expected to perform well for the RC sub-set.
However, the basic model produced a confidence-weighted score of just 49.37% (accuracy 47.14%), andthe stoplisted model produced a comparable confidence-weighted score of 47.11% (accuracy 45.00%).The primary reason for the performance gap betweenthe RC and IE domains appears to be that RC is lessnews-oriented, so there is less emphasis on exact lexicalchoices such as named entities.
This puts more weight on29the importance of a good thesaurus to recognize lexicalvariation.
For this reason, we believe the addition of athesaurus would bring performance improvements simi-lar to the case of MT.5.6 Information Retrieval (IR)The IR task diverges significantly from the tasks forwhich Bracketing ITGs were developed.
The basic modelproduced a confidence-weighted score of 43.14% (ac-curacy 46.67%), while the stoplisted model produced acomparable confidence-weighted score of 44.81% (accu-racy 47.78%).Bracketing ITGs seek structurally parallelizable sub-strings, where there is reason to expect some degree ofgeneralization between the frames (heads and arguments)of the two substrings from a lexical semantics standpoint.In contrast, the IR task relies on unordered keywords, sothe effect of argument-head binding cannot be expectedto be strong.5.7 Question Answering (QA)The QA task is extremely free in the sense that ques-tions can differ significantly from the answers in bothsyntactic structure and lexis, and can also require asignificant degree of indirect complex inference us-ing real-world knowledge.
The basic model pro-duced a confidence-weighted score of 33.20% (accuracy40.77%), while the stoplisted model produced a signifi-cantly better confidence-weighted score of 38.26% (ac-curacy 44.62%).Aside from adding a thesaurus, to properly model theQA task, at the very least the Bracketing ITG modelswould need to be augmented with somewhat more lin-guistic rules that include a proper model for wh- words inthe Hypothesis, which otherwise cannot be aligned to theText.
In the Bracketing ITG models, the stoplist appearsto help by normalizing out the effect of the wh- words.6 ConclusionThe most serious omission in our experiments withBracketing ITG models was the absence of any thesaurusmodel, allowing zero lexical variation between the twostrings of a candidate paraphrase pair (or Text and Hy-pothesis, in the case of textual entailment recognition).This forced the models to rely entirely on the BracketingITG?s inherent tendency to optimize structural match be-tween hypothesized nested argument-head substructures.What we find highly interesting is the perhaps surpris-ingly large effect obtainable from this structure matchingbias alone, which already produces good results on para-phrasing as well as a number of the RTE subsets.We plan to remedy the absence of a thesaurus as theobvious next step.
This can be expected to raise perfor-mance significantly on all subsets.Wu and Fung (2005) also discuss how to obtain anydesired tradeoff between precision and recall.
This wouldbe another interesting direction to pursue in the context ofrecognizing paraphrases or textual entailment.Finally, using the development sets to train the param-eters of the Bracketing ITG model would improve per-formance.
It would only be feasible to tune a few basicparameters, however, given the small size of the develop-ment sets.ReferencesIdo Dagan, Oren Glickman, and Bernardo Magnini.
The pascalrecognising textual entailment challenge.
In PASCAL Pro-ceedings of the First Challenge Workshop?Recognizing Tex-tual Entailment, pages 1?8, Southampton, UK, April 2005.Gregor Leusch, Nicola Ueffing, and Hermann Ney.
A novelstring-to-string distance measure with applications to ma-chine translation evaluation.
In Machine Translation Summit,New Orleans, 2003.P.
M. Lewis and R. E. Stearns.
Syntax-directed transduc-tion.
Journal of the Association for Computing Machinery,15:465?488, 1968.C.
Quirk, C. Brockett, and W. B. Dolan.
Monolingual ma-chine translation for paraphrase generation.
In Proceed-ings of the 2004 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP-2004), Barcelona, June2004.
SIGDAT, Association for Computational Linguistics.Dekai Wu and Pascale Fung.
Inversion Transduction Gram-mar constraints for mining parallel sentences from quasi-comparable corpora.
In Forthcoming, 2005.Dekai Wu.
An algorithm for simultaneously bracketing paralleltexts by aligning words.
In 33rd Annual Meeting of the Asso-ciation for Computational Linguistics Conference (ACL-95),Cambridge, MA, Jun 1995.
Association for ComputationalLinguistics.Dekai Wu.
Stochastic inversion transduction grammars andbilingual parsing of parallel corpora.
Computational Linguis-tics, 23(3), Sep 1997.Kenji Yamada and Kevin Knight.
A syntax-based statisticaltranslation model.
In 39th Annual Meeting of the Associ-ation for Computational Linguistics Conference (ACL-01),Toulouse, France, 2001.
Association for Computational Lin-guistics.Richard Zens and Hermann Ney.
A comparative study on re-ordering constraints in statistical machine translation.
pages192?202, Hong Kong, August 2003.Richard Zens, Hermann Ney, Taro Watanabe, and EiichiroSumita.
Reordering constraints for phrase-based statisticalmachine translation.
In Proceedings of COLING, Geneva,August 2004.Hao Zhang and Daniel Gildea.
Syntax-based alignment: Super-vised or unsupervised?
In Proceedings of COLING, Geneva,August 2004.30
