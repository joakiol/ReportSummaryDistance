Search Result Clustering Using Label Language ModelYeha Lee Seung-Hoon Na Jong-Hyeok LeeDiv.
of Electrical and Computer EngineeringPohang University of Science and Technology (POSTECH)Advanced Information Technology Research Center (AITrc)San 31, Hyoja-Dong, Pohang, Republic of Korea, 790-784{sion,nsh1979,jhlee}@postech.ac.krAbstractSearch results clustering helps users tobrowse the search results and locate whatthey are looking for.
In the search resultclustering, the label selection which anno-tates a meaningful phrase for each clusterbecomes the most fundamental issue.
In thispaper, we present a new method of usingthe language modeling approach over Dmozfor label selection, namely label languagemodel.
Experimental results show that ourmethod is helpful to obtain meaningful clus-tering labels of search results.1 IntroductionMost contemporary search engines generate a longflat list in response to a user query.
This result canbe ranked by using criteria such as PageRank (Brinand Page, 1998) or relevancy to the query.
However,this long flat list is uncomfortable to users, since itforces users to examine each page one by one, andto spend significant time and effort for finding thereally relevant information.
Most users only lookinto top 10 web pages in the list (Kummamuru et al,2004).
Thus many other relevant information can bemissed out as a result.
Clustering method is pro-posed in order to remedy the problem.
Instead ofthe flat list, it groups the search results to clusters,and annotates a label with a representative words orphrases to each cluster.
Then, these labeled clustersof search results are presented to users.
Users canbenefit from labeled clusters because the size of in-formation presented is significantly reduced.Search result clustering has several specific re-quirements that may not be required by other clusteralgorithms.
First, search result clustering should al-low fast clustering and fast generation of a label onthe fly, since it is an online process.
This require-ment can be met by adopting ?snippets?
1 rather thanentire documents of a search result set.
Second, la-bels annotated for clusters should be meaningful tousers because they are presented to users as a generalview of results.
For this reason, recent search resultclustering researches focus on selecting meaningfullabels.
This differs from general clustering whichfocuses on the similarity of documents.
In Zamirand Etzioni (Zamir and Etzioni, 1998), a few otherkey requirements of search result clustering are pre-sented.In this paper, we present a language modelingapproach with Dmoz for search result clustering.Dmoz 2 is an Open Directory Project, and containsmanually tagged categories for web-sites.
Sincethese categories are built by human, they provide agood basis to build labels for clusters.
We can viewthe problem of label selection for clusters as a prob-lem of label generation by Dmoz.We define a language model for each Dmoz-category and select labels for clusters according tothe probability that this language model would gen-erate candidate labels.Thus, our method can select more meaningful la-bels for clusters because we use labels generated byhuman-tagged categories of Dmoz.
The selected la-1The term ?snippet?
is used here to denote fragment of aWeb page returned by certain search engines2Open Directory Project, http://www.dmoz.com/637bels enable users to quickly identify the desired in-formation.The paper is organized as follows.
The next sec-tion introduces related works.
In Section 3, we for-mulate the problem and show the detail of our ap-proach.
The experiment results and evaluations arepresented in Section 4.
Finally, we conclude thepaper and discuss future works in Section 5.2 RELATED WORKSMany approaches have been suggested for organiz-ing search results to improve browsing effectiveness.Previous researches such as scatter/Gather (Hearstand Pedersen, 1996) and Leuski (Leuski and Allan,2000), Leouski (Leouski and Croft, 1996), clusterdocuments using document-similarity, and generaterepresentative terms or phrases as labels.
However,these labels are often not meaningful, which compli-cates user relevance judgment.
They are also slowin generating clusters and labels because they useentire document contents in the process.
Thus it isdifficult to apply these approaches to search engineapplications.Due to the problems mentioned above, researchin search result clustering has focused on choosingmeaningful labels which is not usually addressedin general document clustering.
Zeng et al pre-sented salient phrase ranking problem for label se-lection, which ranks labels scored by a combinationof some properties of labels and documents (Zenget al, 2004).
Kummamuru regarded label se-lection as a problem making a taxonomy of thesearch result, and proposed a label selecting crite-rion based on taxonomy likelihood (Kummamuru etal., 2004).
Zamir presented a Suffix Tree Cluster-ing (STC) which identifies sets of documents thatshare common phrases, and clusters according tothese phrases (Zamir and Etzioni, 1998).
Maareket al and Osinski presented a singular value de-composition of the term-document matrix for searchresult clustering (Maarek et al, 2000), (Osinskiand Weiss, 2004).
The problem of these methodsis that SVD is extremely time-consuming when ap-plied to a large number of snippets.
Ferragina pro-posed a method for generating hierarchical labels bywhich entire search results are hierarchically clus-tered (Ferragina and Gulli, 2005).
This method pro-duces a hierarchy of labeled clusters by constructinga sequence of labeled and weighted bipartite graphsrepresenting the individual snippets on one side anda set of labeled clusters on the other side.3 LABEL LANGUAGE MODELThe main purpose of Label Language Model(LLM)is to generate meaningful labels on-the-fly fromsearch results, specifically snippets, for web-users.The generated labels provide a view of the search re-sult to users, and allow the users to navigate throughthem for their search needs.Our algorithm is composed of the four phases:1.
Search result fetching2.
Candidate Labels Generation3.
Label Score Calculation4.
Post-processingSearch result fetching.
LLM operates as a meta-search engine on top of established search engines.Our engine first retrieves results from dedicatedsearch engines in response to user queries.
Thesearch results are parsed through HTML parser,and snippets are obtained as a result.
We assumethat these snippets contain enough informationto provide user-relevance judgment.
Hence, wecan generate meaningful labels using only thosesnippets rather than the entire document contents ofthe search result set.Candidate Labels Generation.
Candidate labelsare generated using the snippets obtained by searchresult fetching.
Snippets are processed by Porter?salgorithm for stemming and stopword removing,then every n-grams becomes a candidate label.
Eachcandidate label is tagged with a score calculatedby the Label Language Model.
Finally, top Ncandidate labels with highest scores are displayedto users as labels for clusters of search result.Label Score Calculation.
Our model utilizesDmoz to select meaningful labels.
Dmoz is thelargest, most comprehensive human-edited directoryof the Web and classifies more than 3,500,000 sitesin more than 460,000 categories.
It is used for rank-ing and retrieval by many search engines, such as638Google (Rerragina and Gulli, 2005).Language model ranks documents according tothe probability that the language model of each doc-ument would generate the user query.Dmoz is a human-edited directory, which containsmeaningful categories.
We can use the probabilitythat categories of Dmoz would generate candidatelabels as criteria to rank labels.In our approach, the user query and the documentcorrespond to the candidate label and the Dmoz?scategory, respectively.
We can obtain the probabilitythat LLM of each category would generate a label bylanguage model.
We assume that the probability ofcertain candidate label being generated can be esti-mated by the maximum value of the probability thatLLM of each category would generate the candidatelabel.Let labeli be ith label, wij be jth word of labeli,and Ck be kth category of Dmoz, respectively.
If weassume that the labels are drawn independently fromthe distribution, then we can express the probabilitythat Dmoz generates labels as follows:p(labeli|Dmoz) = maxkp(labeli|Ck) (1)p(labeli|Ck) =?p(wij |Ck) (2)We use two smoothing methods, Jelinek-Mercersmoothing and Dirichlet Priors smoothing (Zhai andLafferty, 2001), in order to handle unseen words.The score of labeli is calculated as follows:Si = maxk?jlog(1 + ?p(wij |Ck)(1 ?
?
)p(wij |Call))(3)Si = maxk?jlog#(wkij) + ?p(wij |Call)#(Ck) + ?
(4)To solve the equation, p(wij |Ck) and p(wij |Call)should be estimated.
Let #(Ck) and #(Call)3 bethe number of words in kth category and the numberof words in Dmoz.
Further, let #(wkij) and #(wallij )be the number of word, wij , in kth category and thenumber of word, wij , in Dmoz.
Then p(wij |Ck) isestimated as #(wkij)#(Ck) , and p(wij |Call) as#(wallij )#(Call) .3Call denotes all categories of DmozIn Candidate Labels Generation phase, all can-didate labels are scored.
After post-processing,candidate labels are shown in a descending order.Post-processing.
In post processing phase, labelsare refined through several rules.
First, labels com-posed of only query words are removed because theydo not provide better clues for users.
Second, la-bels that are contained in another label are removed.Since every possible n gram is eligible for candidatelabels, multiple labels that differ only at the eitherends, i.e., one label contained in another, can be as-signed a high score.
In such cases, longer labelsare more specific and meaningful than shorter ones,therefore shorter ones are removed.
Users can ben-efit from a more specific and meaningful label thatclarifies what a cluster contains.
Finally, Top N La-bels with highest scores produced by post processingare presented to users.4 EXPERIMENTSWe conducted several experiments with varyingsmoothing parameter values, ?, ?.
We investigatedthe influence of the smoothing parameter on the la-bel selection procedure.4.1 Experiment SetupDespite heavy researches on search result cluster-ing, a standard test-set or evaluation measurementdoes not exist.
This paper adopts the methodologyof (Zeng et al, 2004) in order to evaluate the ex-pressiveness of selected label and LLM4.1.1 Test Data SetWe obtained Google?s search results that corre-spond to fifty queries.
The fifty queries are com-prised of top 25 queries to Google and 25 from(Zeng et al, 2004).
For each query of the fifty, 200snippets from Google are obtained.
Table 1 summa-rizes the query used in our experiment.Search results obtained from Google are parsedto remove html-tag and stopword, and stemming isapplied to obtain the snippets.
Every n-gram of thesnippets, where n ?
3, becomes candidate labels.Labels that do not occur more than 3 times are re-moved from candidate set in order to reduce noise.639Type Queries2005 GoogleTop queryMyspace, Ares, Baidu, orkut,iTumes, Sky News, World of War-craft, Green Day, Leonardo daVinci, Janet Jackson, HurricaneKatrina, tsunami, xbox 360, BradPitt, Michael Jackson, AmericanIdol, Britney Spears, AngelinaJolie, Harry Potter, ipod, digi-tal camera, psp, laptop, computerdesk(Zeng et al,2004) queryjaguar, apple, saturn, jobs, jordan,tiger, trec, ups, quotes, matrix, su-san dumais, clinton, iraq, dell, dis-ney, world war 2, ford, health, yel-low pages, maps, flower, music,chat, games, radio, jokes, graphicdesign, resume, time zones, travelTable 1: Queries used in experiment4.1.2 Answer Label Set for EvaluationIn order to evaluate LLM, we manually createdlabels for each query which are desired as outputs ofour test, and we refer them as answer labels.
Theremight be a case where an answer label and label se-lected by our model are semantically equivalent butlexically different; for example, car and automobile.To mitigate the problem, we used Wordnet to han-dle two different words with the same semantic.
Weexplain the use of Wordnet further in section 4.1.3.4.1.3 Evaluation Measure & MethodWe used precision at top N labels to evaluate themodel.
Precision at top N is defined as P@N =M@NN , where is M@N is the number of relevant la-bels among the top N generated labels to the answerset.
As explained in section 4.1.2, the labels gen-erated by our model might not be equal to answerlabels even when they have the same semantic mean-ing.
It might be very time consuming for a humanto manually compare the two label set where one setcan vary due to the varying smoothing parameter ifsemantic meaning also has to be considered.We used WordNet?s synonyms and hypernymsrelationships in order to mitigate the problem ad-dressed above.
We regard a test label to be equalto an answer label when WordNet?s synonyms orhypernyms relationship allows them.
Only the firstlisted sense in Wordnet is used to prevent over-generation.We evaluated the overall effectiveness of LLMwith P@N and the effect of smoothing parameteron P@N .4.2 Experimental ResultWe used P@5, P@10 and P@20 to evaluate the ef-fectiveness of our model because most users disre-gard snippets beyond 20.First, for each query, we obtained each label?sMAP4 for two smoothing methods.
Figures 1 and2 depicts MAP of Jelinke-Mercer smoothing andDirichlet Priors smoothing.0 0.2 0.4 0.6 0.8 10.40.450.50.550.60.650.70.750.8LambdaPrecisionP@5P@10P@20Figure 1: Jelinek-Mercer Smoothing0 1000 2000 3000 4000 50000.20.30.40.50.60.70.80.91MuPrecisionP@5P@10P@20Figure 2: Dirichlet priors SmoothingIn figures 1 and 2, X-axis denotes smoothing pa-rameter, and Y -axis denotes MAP.
The figures showthat the smaller the value of the smoothing is , the4Mean Average Precision640higher the precision is.
This indicates that a betterlabel is selected when the probability that a specificcategory would generate the label is high.
In ourtest result, when using Dirichlet smoothing, the pre-cision of top 5 and 10 labels are 82% and 80%, thususers can benefit in browsing from our model using5 or 10 labels.
However, the precision rapidly dropsto 60% at P@20.
The low precision at P@20 showsthe vulnerability of our model, indicating that ourmodel needs a refinement.Figure 3 shows individual precisions of labels forrandomly selected five queries.
The labels were gen-erated by using Dirichlet priors smoothing.Baidu tiger apple jaguar travel0.10.20.30.40.50.60.70.80.91QueriesPrecisionP@20P@5P@10Figure 3: Using Dirichlet priors SmoothingAs shown in figure 1 and 2, the general order ofresult precisions is as follows: P@20 ?
P@10 ?P@5.
However, figure 3 shows that the precisionfor query ?travel?
is the lowest at P@5.
This re-sult indicates that words that appear many times in aspecific category of Dmoz might have higher proba-bility regardless of snippet?s contents.Average Baidu apple jaguar tiger travel00.10.20.30.40.50.60.70.80.91QueriesCoverageTop 5Top 10Top 20Figure 4: CoverageFigure 4 shows the average coverage of labelsgenerated by our model.
The coverage of the labelsis about 0.32%, 0.51% and 0.66% at top 5, 10 and 20labels respectively.
This means that the labels allowbrowsing over only 60% of the entire search results.The lack of coverage is another pitfall of our model,and further refining is needed.Finally, in Table 2, we list top 10 labels for fivequeries.5 CONCLUSION & FUTURE WORKSWe proposed a LLM for label selection of search re-sults, and analyzed the smoothing parameter?s effecton the label selection.
Experimental results showedthat LLM can pick up meaningful labels, and aidusers in browsing web search results.
Experimentalresults also validated our assumption that the highprobability that Dmoz categories generate a label in-dicates meaningful labels.
Further research direc-tions remain as future works.Our model is sensitive to Dmoz because we usethe language model based on Dmoz.
Our model mayresult in poor performance for labels that are not rep-resented or over-represented in Dmoz.
Therefore, itis meaningful to study how sensitive to Dmoz theperformance of the LLM is, and how to mitigate sen-sitivity.
We used Google?s search results as an inputto our system.
However, multiple engines offer abetter coverage of the web because of the low over-lap of current search engines (Bharat and Broder,1998).
Further work can utilize multiple engines togenerate input to our system.
In our test, snippet?stitle and content were assigned the same weight, andtitles and descriptions of Dmoz?s category were alsoassigned the same weight.
Future work might bene-fit from varying the weights to them.
We did not uti-lize the information buried in the documents, suchas tf ?
idf , but used only knowledge provided by theexternal system, Dmoz.
We believe that this also af-fected LLM?s poor performance on over-representedterms.
Future work will benefit from incorporatingthe information derivable from the documents.6 ACKNOWLEDGMENTSThis work was supported by the Korea Scienceand Engineering Foundation (KOSEF) through theAdvanced Information Technology Research Center641Queries LabelsBaidu language search set, Chinesesearch engine, search engine com-pany, Baidu.com, MP3 Search,Baidu engine, Japanese SearchEngine, IPO, search market,Mobileapple Mac OS X, iPod, Apple Mac-intosh, Apple products, languagecharacter set, Music Store, Appledevelops, Apple Support, informa-tion, San Franciscojaguar Mac OS X, Jaguar Cars, LandRover, Jaguar XJ, Jaguar XK,largest cat, Leopard, Photostagged jaguar, Jaguar dealer,Jaguar Clubstiger Mac OS X, Tiger Woods, TigerCats, Detroit Tigers, Securitytool, Parts PC Components, PaperTiger, Adventure Tour, NationalZoo, Tiger Beattravel Car Rental, airline tickets, dis-count hotels, Plan trip, Airfares,package holidays, Visa, TravelCheap, Destination guides, TravelnewsTable 2: Queries used in experiment(AITrc), also in part by the BK 21 Project and MIC& IITA through IT Leading R&D Support Project in2007.ReferencesP.
Rerragina and A. Gulli.
2005.
A personalized searchengine based on web-snippet hierarchical clustering.In Special Interest Tracks and Poster Proceedingsof WWW-05, International Conference on the WorldWide Web, 801-810H.
Zeng, Q.
He, Z. Chen, W. Ma and J. Ma 2004.
Learn-ing to cluster web search results.
In Proceedings of the27th ACM SIGIR Conference on Research and Devel-opment of Information RetrievalM.
A. Hearst and J. O. Pedersen.
1996.
Reexamining thecluster hypothesis: Scatter/Gather on retrieval results.In Proceedings of 19th ACM SIGIR Conference on Re-search and Development in Information Retrieval, 76-84K.
Kummamuru, R. Lotlikar, S. Roy, K. Signal and R.Krishnapuram 2004.
A hierarchical monothetic docu-ment clustering algorithm for summarization browsingsearch results.
In Proceedings of 13th InternationalConference on World Wide Web, 658-665A.
Leuski and J. Allan.
2000.
Improving Interactive Re-trieval by Combining Ranked List and Clustering.
InProceedings of RIAOI, College de France, 665-681A.
V. Leouski and W. B. Croft.
1996.
An Evaluationof Techniques for Clustering Search Results.
In Tech-nical Report IR-76, Department of Computer Science,University of Massachusetts, AmherstO.
Zamir and O. Etzioni.
1998.
Web Document Clus-tering: A Feasibility Demonstration.
In Proceedingsof the 21th ACM SIGIR Conference on Research andDevelopment of Information Retrieval, 46-54Y.
Maarek, R. Fagin, I. Ben-Shaul and D. Pelleg.
2000.Ephemeral document clustering for Web applications.Technical Report RJ 10186, IBM, San Jose, USS.
Osinski and D. Weiss.
2004.
Conceptual clusteringusing Lingo algorithm: Evaluation on Open DirectoryProject data In Proceedings of IIPWM-04, 5th Confer-ence on Intelligent Information Processing and WebMining, 369-377P.
Ferragina and A. Gulli.
2005.
A personalized searchengine based on Web-snippet hierarchical clustering.In Special Interest Tracks and Poster Proceedings ofWWW-05, International conference on the World WideWeb, 801-810S.
Brin and L. Page 1998.
The anatomy of a large-scalehypertextual(Web) Search Engine.
In Proceedings ofthe 7th International Conference on World Wide Web,107-117C.
Zhai and J. Lafferty 2001.
A study of smoothingmethods for language models applied to ad hoc infor-mation retrieval.
In Proceedings of the 24th ACM SI-GIR Conference on Research and Development of In-formation Retrieval, 334-342K.
Bharat and A. Broder.
1998.
A technique for measur-ing the relative size and overlap of public web searchengines.
In Proceedings of the 7th International Con-ference on World Wide Web642
