Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 752?762,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsDemographic Factors Improve Classification PerformanceDirk HovyCenter for Language TechnologyUniversity of Copenhagen, DenmarkNjalsgade 140dirk@cst.dkAbstractExtra-linguistic factors influence languageuse, and are accounted for by speakersand listeners.
Most natural language pro-cessing (NLP) tasks to date, however,treat language as uniform.
This assump-tion can harm performance.
We investi-gate the effect of including demographicinformation on performance in a varietyof text-classification tasks.
We find thatby including age or gender information,we consistently and significantly improveperformance over demographic-agnosticmodels.
These results hold across threetext-classification tasks in five languages.1 IntroductionWhen we use language, we take demographicfactors of the speakers into account.
In otherwords, we do have certain expectations as to whouses ?super cute,?
?rather satisfying,?
or ?rad,dude.?
Sociolinguistics has long since studied theinterplay between demographic factors and lan-guage use (Labov, 1964; Milroy and Milroy, 1992;Holmes, 1997; Macaulay, 2001; Macaulay, 2002;Barbieri, 2008; Wieling et al, 2011; Rickford andPrice, 2013, inter alia).1These factors greatly in-fluence word choice, syntax, and even semantics.In natural language processing (NLP), however,we have largely ignored demographic factors, andtreated language as a uniform medium.
It was ir-relevant, (and thus not modeled) whether a textwas produced by a middle-aged man, an elderlylady, or a teenager.
These three groups, how-ever, differ along a whole host of demographicaxes, and these differences are reflected in theirlanguage use.1Apart from the demographic factors, other factors suchas mood, interpersonal relationship, authority, language atti-tude, etc.
contribute to our perception of language.A model that is agnostic to demographic dif-ferences will lose these distinctions, and perfor-mance suffers whenever the model is applied to anew demographic.
Historically, the demograph-ics of training and test data (newswire) were rela-tively homogenous, language was relatively uni-form, and information the main objective.
Un-der these uniform conditions, the impact of demo-graphics on performance was small.Lately, however, NLP is increasingly appliedto other domains, such as social media, wherelanguage is less canonical, demographic informa-tion about the author is available, and the authors?goals are no longer purely informational.
The in-fluence of demographic factors in this medium isthus much stronger than on the data we have tra-ditionally used to induce models.
The resultingperformance drops have often been addressed viavarious domain adaptation approaches (Blitzer etal., 2006; Daume III and Marcu, 2006; Reichartand Rappoport, 2007; Chen et al, 2009; Daum?e etal., 2010; Chen et al, 2011; Plank and Moschitti,2013; Plank et al, 2014; Hovy et al, 2015b, interalia).
However, the authors and target demograph-ics of social media differ radically from those innewswire text, and domain might in some case bea secondary effect to demographics.
In this paper,we thus ask whether we also need demographicadaptation.Concretely, we investigate1.
how we can encode demographic factors, and2.
what effect they have on the performance oftext-classification tasksWe focus on age and gender, and similarlyto Bamman et al (2014a), we use distributedword representations (embeddings) conditionedon these demographic factors (see Section 2.1) toincorporate the information.We evaluate the effect of demographic informa-tion on classification performance in three NLP752tasks: sentiment analysis (Section 2.2), topic de-tection (Section 2.3), and author attribute classifi-cation (Section 2.4).2We compare F1-performance of classifiers a)trained with access to demographic information,or b) under agnostic conditions.
We find thatdemographic-aware models consistently outper-form their agnostic counterparts in all tasks.Our contributionsWe investigate the effect of demographic fac-tors on classification performance.
We show thatNLP systems benefit from demographic aware-ness, i.e., that information about age and gendercan lead to significant performance improvementsin three different NLP tasks across five differentlanguages.2 DataWe use data from an international user reviewwebsite, Trustpilot.
It contains information bothabout the review (text and star rating), as well asthe reviewer, in form of a profile.
The profile in-cluded a screen name, and potentially informationabout gender and birth year.Since demographic factors are extra-linguistic,we assume that the same effects hold irrespectiveof language.
To investigate this hypothesis, we usedata from several languages (Danish, French, andGerman) and varieties (American English, BritishEnglish).We use data from the countries with most users,i.e., Great Britain, Denmark, Germany, France,and the US.
The selection was made based on theavailability of sufficient amounts of training data(see Table 1 for more details).
The high number ofusers in Denmark (one tenth of the country?s pop-ulation) might be due to the fact that Trustpilot isa Danish company and thus existed there longerthan in other countries.
Danish users also provide(in relative terms) more information about them-selves than users of any other country, so that evenin absolute numbers, there is oftentimes more in-formation available than for larger countries likeFrance or Germany, where users are more reluc-tant to disclose information.While most of this profile information is vol-untary, we have good coverage for both age and2We selected these tasks to represent a range of text-classification applications, and based on the availability ofsuitable data with respect to target and demographic vari-ables.USERS AGE GENDER PLACE ALLUK 1,424k 7% 62% 5% 4%France 741k 3% 53% 2% 1%Denmark 671k 23% 87% 17% 16%US 648k 8% 59% 7% 4%Germany 329k 8% 47% 6% 4%Table 1: Number of users and % per variable percountry (after applying augmentations).gender.
In case of missing gender values, we basea guess on the first name (if given), by choosingthe gender most frequently associated with thatname in the particular language.
We do requirethat one gender is prevalent (accounting for 95%of all mentions), and that there is enough support(at least 3 attributed instances), though.
For age,coverage is less dense, so the resulting data setsare smaller, but still sufficient.For more information on Trustpilot as a re-source, see Hovy et al (2015a).We split each review into sentences, tokenize,replace numbers with a 0, lowercase the data, andjoin frequent bigrams with an underscore to forma single token.For each language, we collect four sub-corpora,namely two for gender (male and female) andtwo for age (under 35 and over 45).
The sub-corpora for the discrete variable gender are rela-tively straightforward (although see (Bamman etal., 2014b)), but the split for the continuous agevariable are less clear.
While the effect of age onlanguage use is undisputed (Barke, 2000; Barbieri,2008; Rickford and Price, 2013), providing a clearcut-off is hard.
We therefore use age ranges thatresult in roughly equally sized data sets for bothgroups, and that are not contiguous.For each independent variable (age and gender),we induce embeddings for the two sub-groups (seesection 2.1), as well as a ?mixed?
setting.
Wealso extract labeled data for each task (see sections2.2, 2.3, and 2.4).
Each of these data sets is ran-domly split into training and test data, 60:40.
Notethat we do not set any parameters on developmentdata, but instead use off-the-shelf software withdefault parameters for classification.
Table 2 givesan overview of the number of training and test in-stances for each task and both variables (genderand age).Note that this setup is somewhat artificial: thevocabulary of the embeddings can subsume the753GENDER AGETASK COUNTRY TRAIN TEST TRAIN TESTTOPICDenmark 72.48k 48.32k 26.89k 17.93kFrance 33.34k 22.23k 3.67k 2.45kGermany 18.35k 12.23k 4.82k 3.22kUK 110.40k 73.60k 13.26k 8.84kUS 36.95k 24.63k 7.25k 4.84kSENTIMENTDenmark 150.29k 100.19k 45.18k 30.12kFrance 40.38k 26.92k 3.94k 2.63kGermany 17.35k 11.57k 3.52k 2.35kUK 93.98k 62.65k 15.80k 10.53kUS 43.36k 28.91k 3.90k 2.60kATTRIBUTESDenmark 180.31k 120.20k 180.31k 120.20kFrance 10.69k 7.12k 10.69k 7.12kGermany 11.47k 7.64k 11.47k 7.64kUK 70.87k 47.25k 70.87k 47.25kUS 28.10k 18.73k 28.10k 18.73ktotal 918.32k 612.20k 429.66k 286.43kTable 2: Number of sentences per task for gender and age as independent variablevocabulary of the tasks (there is some loss dueto frequency cut-offs in word2vec).
The out-of-vocabulary rate on the tasks is thus artificially lowand can inflate results.
In a standard ?improve-ment over baseline?-setup, this would be problem-atic.
However, the results should not be interpretedwith respect to their absolute value on the respec-tive tasks, but with respect to the relative differ-ences.2.1 Conditional EmbeddingsCOUNTRY AGE GENDERDenmark 495k 1.6mFrance 36k 490kGermany 47k 211kUK 232k 1.63mUS 70k 576ktotal 880k 4.51mTable 3: Number of sentences used to induce em-beddingsEmbeddings are distributed representations ofwords in a vector space, capturing syntactic andsemantic regularities among the words.
Welearn our word embeddings by using word2vec3(Mikolov et al, 2013) on unlabeled review data.Our corpora are relatively small, compared to thelanguage modeling tasks the tool was developedfor (see Table 3 for the number of instances usedfor each language and variable).
We thus followthe suggestions in the word2vec documentationand use the skip-gram model and hierarchical soft-max rather than the standard continuous-bag-of-words model.
This setting penalizes low-frequentwords less.
All out-of-vocabulary (OOV) wordsare replaced with an ?unknown?
token, which isrepresented as the averaged vector over all otherwords.In this paper, we want to use embeddings tocapture group-specific differences.
We thereforetrain embeddings on each of the sub-corpora(e.g., male, female, and U35, O45) separately.
Ascomparison, we create a mixed setting.
For eachvariable, we combine half of both sub-corpora(say, men and women) to form a third corpuswith no demographic distinction.
We also trainembeddings on this data.
This setting assumesthat there are no demographic differences, whichis the common approach in NLP to date.Since embeddings depend crucially on the3https://code.google.com/p/word2vec/754size of the available training data, and since wewant to avoid modeling size effects, we balancethe three corpora we use to induce embeddingssuch that all three contain the same number ofinstances.4Note that while we condition the embeddings ondemographic variables, they are not task-specific.While general-purpose embeddings are widelyused in the NLP community, task-specific embed-dings are known to lead to better results for var-ious tasks, including sentiment analysis (Tang etal., 2014).
Inducing task-specific embeddings car-ries the risk of overfitting to a task and data set,though, and would make it harder to attribute per-formance differences to demographic factors.Since we are only interested in the relative dif-ference between demographic-aware and unawaresystems, not in the absolute performance on thetasks, we do not use task-specific embeddings.2.2 Sentiment AnalysisSentiment analysis is the task of determining thepolarity of a document.
In our experiments, weuse three polarity values: positive, negative, andneutral.
To collect data for the sentiment analysistask, we select all reviews that contain the targetvariable (gender or age), and a star-rating.
Fol-lowing previous work on similar data (Blitzer etal., 2007; Hardt and Wulff, 2012; Elming et al,2014), we use one, three, or five star ratings, cor-responding to negative, neutral, and positive senti-ment, respectively.We balance the data sets so that both trainingand test set contain equal amounts of all three la-bels.
We do this in order to avoid demographic-specific label distributions (women and peopleover 45 tend to give more positive ratings than menand people under 35, see Section 3.1).2.3 Topic IdentificationTopic identification is the task of assigning a high-level concept to a document that captures its con-tent.
In our case, the topic labels are taken fromthe Trustpilot taxonomy for companies (e.g., Elec-tronics, Pets, etc.).
Again, there is a strong genderbias: the most common topic for men is Computer& Accessories, the most common topic amongwomen is Pets.
There is thus considerably lessoverlap between the groups than for the other4Note, however, that the vocabulary sizes still vary amonglanguages and between age and gender.tasks.
In order not to model gender-specific topicbias and to eliminate topic frequency as a con-founding factor, we restrict ourselves to the fivemost frequent labels that occur in both groups.
Wealso ensure that we have the same number of ex-amples for each label in both groups.
However,in the interest of data size, we do not enforce auniform distribution over the five labels (i.e., theclasses are not balanced).2.4 Author Attribute IdentificationAuthor attribute identification is the task of infer-ring demographic factors from linguistic features(Alowibdi et al, 2013; Ciot et al, 2013; Liu andRuths, 2013).
It is often used in author profiling(Koppel et al, 2002) and stylometrics (Goswamiet al, 2009; Sarawgi et al, 2011).
Rosenthal andMcKeown (2011) have shown that these attributesare correlated.In this paper, we restrict ourselves to using gen-der to predict age, and age to predict gender.
Thisserves as an additional test case.
Again, we bal-ance the class labels to minimize the effect of anyconfounding factors.3 Experiments3.1 Data AnalysisBefore we analyze the effect of demographicdifferences on NLP performance, we investigatewhether there is an effect on the non-linguistic cor-relates, i.e., ratings and topics.
To measure the in-fluence of demographic factors on these values, wequantify the distributions over the three sentimentlabels and the five topic labels.
We analyze bothgender and age groups separately, but in the inter-est of space average across all languages.negativeneutralpositive020406080100malefemaleFigure 1: Label distribution for gender755negativeneutralpositive0102030405060708090U35O45Figure 2: Label distribution for age groupsFigures 1 and 2 show the distributions oversentiment labels.
We note that men give morenegative and fewer positive ratings than women.The same holds for people in the younger group,who are more skewed towards negative ratingsthan people in the older group.
While the differ-ences are small, they suggest that demographicscorrelate with rating behavior have a measurableeffect on model performance.The gender distributions over categories ex-hibit a very different tendency.
Table 3 showsthat the review categories (averaged over alllanguages) are highly gender-specific.
With theexception of Hotels and Fashion Accessories, thetwo distributions are almost bimodal opposites.However, they are still significantly correlated(Spearman ?
is 0.49 at p < 0.01).The difference in the two distributions illus-trates why we need to control for topic frequencyin our experiments.3.2 ModelsClassifiers For all tasks, we use logistic regres-sion models5with standard parameter settings.
Inorder to isolate the effect of demographic dif-ferences on performance in all text classificationtasks, we need to represent variable length doc-uments based only upon the embeddings of thewords they contain.We follow Tang et al (2014) in using convo-lutional layers over word embeddings (Collobertet al, 2011) to generate fixed-length input repre-sentations.
Figure 4 schematically shows the pro-cedure for the minimum of a 4-dimensional toy5http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.htmlexample.
For each instance, we collect five N -dimensional statistics over the t by N input ma-trix, where N is the dimensionality of the embed-dings (here: 100), and t is the sentence length inwords.From the matrix representation, we compute thedimension-wise minimum, maximum, and meanrepresentation, as well as one standard deviationabove and below the mean.
We then concate-nate those five 100-dimensional vectors to a 500-dimensional vector thats represents each instance(i.e., review) as input to the logistic regressionclassifier.Taking the maximum and minimum across allembedding dimensions is equivalent to represent-ing the exterior surface of the ?instance manifold?
(the volume in embedding space within which allwords in the instance reside).
Adding the meanand standard deviation summarizes the densityper-dimension within the manifold.
This way, wecan represent any input sentence solely based onthe embeddings, and with the same feature vectordimensionality.that was cool0.10.80.20.40.40.50.60.30.90.60.70.20.10.50.20.2min()0.90.80.70.4max()0.460.630.50.3mean()0.140.50.280.22-std()0.80.760.720.38+std()Figure 4: Example for deriving embedding statis-tics from sentence in 4-dimensional space.
Mini-mum shadedThe approach is the same for all three tasks, andwe did not tune any parameters to maximize per-formance.
The results are thus maximally compa-rable to each other, albeit far from state-of-the-art.Overall performance could be improved with task-specific features and more sophisticated models,but it would make the results less comparable, andcomplicate identifying the source of performancedifferences.
We leave this for future research.Comparison In order to compare demographic-aware and agnostic models, we use the followingsetup for each task and language:1.
In the ?agnostic?
setting, we train a logistic-regression model using the joint embeddings(i.e., embeddings induced on the corpus con-taining both sub-groups, e.g.
male and fe-756PetsClothes_&_FashionBeauty_and_WellnessGiftsFlowersDecoration_and_Interior_DesignBooksDrugs_&_PharmacyFood_&_BeverageContact_LensesTravel_AggregatorCell_phone_RecyclingDomestic_AppliancesFlightsHome_&_GardenOnline_MarketplaceAirport_ParkingArt_SuppliesElectrical_goodsInk_CartridgesFashion_AccessoriesBathroomHotelsElectronicsWineFitness_&_NutritionCar_RentalTiresComputer_&_AccessoriesCar_lights0.000.020.040.060.080.100.120.14malefemaleFigure 3: Distribution of the 30 most frequent categories per gender over all languagesmale) and group-agnostic training data (i.e.,data that contains an equal amount of in-stances from either sub-group).2.
In the demographic-aware setting, we train alogistic-regression model for each of the twosub-groups (e.g., male and female).
For eachsub-group, we use the group-specific embed-dings (i.e., embeddings induced on, say, maledata) and group-specific training data (i.e.,instances collected from male data).We measure F1-performance for both settings(agnostic and demographic-aware) on the test set.The test data contains an equal amount of in-stances from both sub-groups (say, male and fe-male).
We use the demographic-aware classifierappropriate for each instance (e.g., male classi-fier for male instances), i.e., we assume that themodel has access to this information.
For manyuser-generated content settings, this is realistic,since demographic information is available.
How-ever, we only predict the target variable (senti-ment, topic, or author attribute).
We do not requirethe model to predict the sub-group (age or gendergroup).We assume that demographic factors holdirrespective of language.
We thus compute amacro-F1over all languages.
Micro-F1wouldfavor languages for which there is more dataavailable, i.e., performance on those languageswould dominate the average performance.
Sincewe do not want to ascribe more importanceto any particular language, macro-F1is moreappropriate.Even if there is a difference in performancebetween the agnostic and aware settings, this dif-ference could still be due to the specific data set.In order to test whether the difference is also sta-tistically significant, we use a bootstrap-samplingtest.
In a bootstrap-sampling test, we samplesubsets of the predictions of both settings (withreplacement) 10,000 times.
For each sample,we measure F1of both systems, and comparethe winning system of the sample to the winningsystem on the entire data set.
The number of times757SENTIMENT ANALYSIS TOPIC CLASSIFICATION AGE CLASSIFICATIONCOUNTRY AGNOSTIC AWARE AGNOSTIC AWARE AGNOSTIC AWAREDenmark 61.75?62.00 49.19?50.08 59.94?60.22France 61.21 61.09 38.45?39.33 53.85 54.21Germany 60.50 61.36 60.45 61.11 60.19 60.20UK 65.22 65.12 66.02 66.26 59.78?60.35US 60.94 61.24 65.64 65.37 61.97 62.68avg 61.92 62.16 55.95 56.43 59.15 59.53Table 4: F1for gender-aware and agnostic models on tasks.
Averages are macro average.?
: p < 0.05the sample winner differs from the entire dataset, divided by 10, 000, is the reported p-value.Bootstrap-sampling essentially simulates runs ofthe two systems on different data sets.
If onesystem outperforms the other under most of theseconditions (i.e., the test returns a low p-value), wecan be reasonably sure that the difference is notdue to chance.As discussed in Berg-Kirkpatrick et al (2012)and S?gaard et al (2014), this test is the most ap-propriate for NLP data, since it does not make anyassumptions about the underlying distributions,and directly takes performance into account.
Notethat the test still depends on data size, though,so that small differences in performance on largerdata sets can be significant, while larger differ-ences on small sets might not.We test for significance with the standard cutoffof p < 0.05.
However, even under a bootstrap-sampling test, we can only limit the number oflikely false positives.
If we run enough tests, weincrease the chance of reporting a type-I error.
Inorder to account for this effect, we use Bonferronicorrections for each of the tasks.4 ResultsFor each task, we compare the demographic-awaresetting to an agnostic setting.
The latter is equiva-lent to the currently common approach in NLP.
Foreach task and language, the setting with the higherperformance is marked in bold.
Statistically sig-nificant differences (at p < 0.05) are marked witha star (?).
Note that for the macro-averaged scores,we cannot perform bootstrap significance testing.4.1 GenderTable 4 shows the F1scores for the different tasks.In the left column of each task (labeled AGNOS-TIC), the system is trained on embeddings and datafrom both genders, in the same ratios as in the testdata.
This column is similar to the configurationnormally used in NLP to date, where ?
at least intheory ?
data comes from a uniformly distributedsample.In the right column (labeled AWARE), theclassification is based on the classifier trained onembeddings and data from the respective gender.While the improvements are small, they areconsistent.
We do note some variance in consis-tency across tasks.The largest average improvement among thethree tasks is on topic classification.
This improve-ment is interesting, since we have seen stark dif-ferences for the topic distribution between gen-ders.
Note, however that we controlled for thisfactor in our experiments (cf.
Table 3).
The re-sults thus show that taking gender into accountimproves topic classification performance even af-ter controlling for prior topic distribution as a con-founding factor.The improvements in age classification are themost consistent.
This consistency is likely dueto the fact that author attributes are often corre-lated.
The fact that the attributes are related canbe exploited in stacking approaches, where the at-tributes are predicted together.Analyzing the errors, the misclassifications forsentiment analysis (the weakest task) seem to besystem-independent.
Mistakes are mainly due tothe simplicity of the system.
Since we do not ex-plicitly model negation, we incur errors such as ?Iwill never order anywhere else again?
classified asnegative, even though it is in fact rather positive.758SENTIMENT ANALYSIS TOPIC CLASSIFICATION GENDER CLASSIFICATIONCOUNTRY AGNOSTIC AWARE AGNOSTIC AWARE AGNOSTIC AWAREDenmark 58.74 59.12 45.11 46.00 58.82 58.97France 53.50 53.40 43.54 42.64 54.64 54.24Germany 51.91 52.83?56.91 55.41 54.04 54.51UK 59.72?60.83 59.40?60.88 57.69?58.25US 55.57 56.00 61.14 61.38 60.05 60.97avg 55.89 56.44 53.22 53.26 57.05 57.59Table 5: F1for age-aware and agnostic models on tasks.
Averages are macro average.?
: p < 0.054.2 AgeTable 5 presents the results for systems with ageas independent demographic variable.
Again, weshow the difference between the agnostic andage-aware setting in parallel columns for eachtask.The improvements are similar to the onesfor gender.
The smaller magnitude across tasksindicates that knowledge of age offers less dis-criminative power than knowledge of gender.
Thisin itself is an interesting result, suggesting that theage gap is much smaller than the gender gap whenit comes to language variation (i.e., older people?slanguage is more similar to younger people thanthe language of men is to women).
The differencebetween groups could be a domain-effect, though,caused by the fact that all subjects are using aform of ?reviewese?
when leaving their feedback.Why this effect would be more prevalent acrossages than across genders is not obvious from thedata.When averaged over all languages, the age-aware setup again consistently outperforms the ag-nostic setup, as it did for gender.
While the finalnumbers are lower than in the gender setting, av-erage improvements tend to be just as decisive.5 Related WorkMost work in NLP that has dealt with demo-graphic factors has either a) looked at the corre-lation of socio-economic attributes with linguis-tic features (Eisenstein et al, 2011; Eisenstein,2013a; Eisenstein, 2013b; Doyle, 2014; Bammanet al, 2014a; Eisenstein, to appear), or b) used lin-guistic features to infer socio-economic attributes(Rosenthal and McKeown, 2011; Nguyen et al,2011; Alowibdi et al, 2013; Ciot et al, 2013; Liuand Ruths, 2013; Bergsma et al, 2013; Volkova etal., 2015).Our approach is related to the work by Eisen-stein (2013a) and Doyle (2014), in that we in-vestigate the influence of extralinguistic factors.Both of them work on Twitter and use geocodinginformation, whereas we focus on age and gen-der.
Also, rather than correlating with census-levelstatistics, as in (Eisenstein et al, 2011; Eisenstein,2013a; Eisenstein, to appear), we take individualinformation of each author into account.Volkova et al (2013) also explore the influenceof gender and age on text-classification.
Theyinclude demographic-specific features into theirmodel and show improvements on sentiment anal-ysis in three languages.
Our work extends to morelanguages and three different text-classificationtasks.
We also use word representations trainedon corpora from the various demographic groups,rather than incorporating the differences explicitlyas features in our model.Recently, Bamman et al (2014a) have shownhow regional lexical differences (i.e., situated lan-guage) can be learned and represented via dis-tributed word representations (embeddings).
Theyevaluate the conditional embeddings intrinsically,to show that the regional representatives of sportsteams, parks, etc.
are more closely associated withthe respective hypernyms than other representa-tives.
We also use embeddings conditioned on de-mographic factors (age and gender instead of loca-tion), but evaluate their effect on performance ex-trinsically, when used as input to an NLP system,rather than intrinsically (i.e., for discovering cor-relations between language use and demographicstatistics).Tang et al (2014) learn embeddings for senti-ment analysis by splitting up their data by rating.759We follow their methodology in using embeddingsto represent variable length inputs for classifica-tion.The experiments on author attribute identifi-cation are inspired by a host of previous work(Rosenthal and McKeown, 2011; Nguyen et al,2011; Alowibdi et al, 2013; Ciot et al, 2013;Liu and Ruths, 2013; Volkova et al, 2015, in-ter alia).
The main difference is that we use em-beddings trained on another demographic variablerather than n-gram based features, and that ourgoal is not to build a state-of-the-art system.6 DiscussionThe results in Section 4 have shown that incor-porating information on age and gender improvesperformance across a host of text-classificationtasks.
Even though the improvements are smalland vary from task to task, they hold consistentlyacross three tasks and languages.
The magnitudeof the improvements could be improved by usingtask-specific embeddings, additional features, andmore sophisticated models.
This would obscurethe influence of the individual factors, though.The observed improvements are solely due tothe fact that different demographic groups use lan-guage quite differently.
Sociolinguistic researchsuggests that younger people and women tendto be more creative in their language use thanmen and older groups.
The former are thus of-ten the drivers of language change (Holmes, 2013;Nguyen et al, 2014).
Modeling language as uni-form loses these distinctions, and thus causes per-formance drops.As NLP systems are increasingly used for busi-ness intelligence and decision making, systematicperformance differences carry the danger of dis-advantaging minority groups whose language usediffers from the norm.7 ConclusionIn this paper, we investigate the influence of ageand gender on topic identification, sentiment anal-ysis, and author attribute identification.
We induceembeddings conditioned on the respective demo-graphic variable and use those embeddings as soleinput to classifiers to build both demographic-agnostic and aware models.
We evaluate our mod-els on five languages.Our results show that the models using de-mographic information perform on average betterthan the agnostic models.
The improvements aresmall, but consistent, and in 8/30 cases, also statis-tically significant at p < 0.05, according to boot-strap sampling tests.The results indicate that NLP systems can im-prove classification performance by incorporat-ing demographic information, where available.
Inmost of situated texts (social media, etc.
), this isthe case.
While the improvements vary amongtasks, the results suggest that similar to domainadaptation, we should start addressing the problemof demographic adaptation in NLP.AcknowledgementsThanks to?Zeljko Agi?c, David Bamman, JacobEisenstein, Stephan Gouws, Anders Johannsen,Barbara Plank, Anders S?gaard, and SvitlanaVolkova for their invaluable feedback, as well asto the anonymous reviewers, whose commentshelped improve the paper.
The author was sup-ported under ERC Starting Grant LOWLANDSNo.
313695.ReferencesJalal S Alowibdi, Ugo A Buy, and Philip Yu.
2013.Empirical evaluation of profile characteristics forgender classification on twitter.
In Machine Learn-ing and Applications (ICMLA), 2013 12th Interna-tional Conference on, volume 1, pages 365?369.IEEE.David Bamman, Chris Dyer, and Noah A. Smith.2014a.
Distributed representations of geographi-cally situated language.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics, pages 828?834.
Proceedings ofACL.David Bamman, Jacob Eisenstein, and Tyler Schnoe-belen.
2014b.
Gender identity and lexical varia-tion in social media.
Journal of Sociolinguistics,18(2):135?160.Federica Barbieri.
2008.
Patterns of age-based lin-guistic variation in American English.
Journal ofsociolinguistics, 12(1):58?88.Andrew J Barke.
2000.
The Effect of Age on theStyle of Discourse among Japanese Women.
In Pro-ceedings of the 14th Pacific Asia Conference on Lan-guage, Information and Computation, pages 23?34.Taylor Berg-Kirkpatrick, David Burkett, and DanKlein.
2012.
An empirical investigation of statisti-cal significance in NLP.
In Proceedings of EMNLP.Shane Bergsma, Mark Dredze, Benjamin Van Durme,Theresa Wilson, and David Yarowsky.
2013.760Broadly improving user classification viacommunication-based name and location clusteringon twitter.
In HLT-NAACL, pages 1010?1019.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of EMNLP.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, Bollywood, Boom-boxes andBlenders: Domain Adaptation for Sentiment Clas-sification.
In Proceedings of ACL.Bo Chen, Wai Lam, Ivor Tsang, and Tak-Lam Wong.2009.
Extracting discriminative concepts for do-main adaptation in text mining.
In KDD.Minmin Chen, Killiang Weinberger, and John Blitzer.2011.
Co-training for domain adaptation.
In NIPS.Morgane Ciot, Morgan Sonderegger, and Derek Ruths.2013.
Gender inference of twitter users in non-english contexts.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, Seattle, Wash, pages 18?21.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Hal Daum?e, Abhishek Kumar, and Avishek Saha.2010.
Frustratingly easy semi-supervised domainadaptation.
In ACL Workshop on Domain Adapta-tion for NLP.Hal Daume III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research, 26:101?126.Gabriel Doyle.
2014.
Mapping dialectal variation byquerying social media.
In EACL.Jacob Eisenstein, Noah Smith, and Eric Xing.
2011.Discovering sociolinguistic associations with struc-tured sparsity.
In Proceedings of ACL.Jacob Eisenstein.
2013a.
Phonological factors in so-cial media writing.
In Workshop on Language Anal-ysis in Social Media, NAACL.Jacob Eisenstein.
2013b.
What to do about bad lan-guage on the internet.
In Proceedings of NAACL.Jacob Eisenstein.
to appear.
Systematic patterningin phonologically-motivated orthographic variation.Journal of Sociolinguistics.Jakob Elming, Barbara Plank, and Dirk Hovy.
2014.Robust cross-domain sentiment analysis for low-resource languages.
In Proceedings of the 5th Work-shop on Computational Approaches to Subjectivity,Sentiment and Social Media Analysis, pages 2?7,Baltimore, Maryland, June.
Association for Compu-tational Linguistics.Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.2009.
Stylometric analysis of bloggers?
age andgender.
In Third International AAAI Conference onWeblogs and Social Media.Daniel Hardt and Julie Wulff.
2012.
What is the mean-ing of 5*?s?
an investigation of the expression andrating of sentiment.
In Empirical Methods in Natu-ral Language Processing, page 319.Janet Holmes.
1997.
Women, language and identity.Journal of Sociolinguistics, 1(2):195?223.Janet Holmes.
2013.
An introduction to sociolinguis-tics.
Routledge.Dirk Hovy, Anders Johannsen, and Anders S?gaard.2015a.
User review-sites as a source for large-scalesociolinguistic studies.
In Proceedings of WWW.Dirk Hovy, Barbara Plank, H?ector Mart?
?nez Alonso,and Anders S?gaard.
2015b.
Mining for unambigu-ous instances to adapt pos taggers to new domains.In Proceedings of NAACL-HLT.Moshe Koppel, Shlomo Argamon, and Anat RachelShimoni.
2002.
Automatically categorizing writ-ten texts by author gender.
Literary and LinguisticComputing, 17(4):401?412.William Labov.
1964.
The social stratification of En-glish in New York City.
Ph.D. thesis, Columbia uni-versity.Wendy Liu and Derek Ruths.
2013.
What?s in a name?using first names as features for gender inference intwitter.
In Analyzing Microtext: 2013 AAAI SpringSymposium.Ronald Macaulay.
2001.
You?re like ?why not??
thequotative expressions of glasgow adolescents.
Jour-nal of Sociolinguistics, 5(1):3?21.Ronald Macaulay.
2002.
Extremely interesting, veryinteresting, or only quite interesting?
adverbs andsocial class.
Journal of Sociolinguistics, 6(3):398?417.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Lesley Milroy and James Milroy.
1992.
Social net-work and social class: Toward an integrated soci-olinguistic model.
Language in society, 21(01):1?26.Dong Nguyen, Noah A Smith, and Carolyn P Ros?e.2011.
Author age prediction from text using lin-ear regression.
In Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cul-tural Heritage, Social Sciences, and Humanities,pages 115?123.
Association for Computational Lin-guistics.761Dong Nguyen, Dolf Trieschnigg, A. Seza Dogru?oz, Ri-lana Gravel, Mariet Theune, Theo Meder, and Fran-ciska De Jong.
2014.
Predicting Author Genderand Age from Tweets: Sociolinguistic Theories andCrowd Wisdom.
In Proceedings of COLING 2014.Barbara Plank and Alessandro Moschitti.
2013.
Em-bedding semantic similarity in tree kernels for do-main adaptation of relation extraction.
In Proceed-ings of ACL.Barbara Plank, Dirk Hovy, Ryan McDonald, and An-ders S?gaard.
2014.
Adapting taggers to twitterwith not-so-distant supervision.
In Proceedings ofCOLING.
COLING.Roi Reichart and Ari Rappoport.
2007.
Self-trainingfor enhancement and domain adaptation of statisticalparsers trained on small datasets.
In Proceedings ofACL.John Rickford and Mackenzie Price.
2013.
Girlz iiwomen: Age-grading, language change and stylisticvariation.
Journal of Sociolinguistics, 17(2):143?179.Sara Rosenthal and Kathleen McKeown.
2011.
Ageprediction in blogs: A study of style, content, andonline behavior in pre-and post-social media genera-tions.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies-Volume 1, pages 763?772.
Association for Computational Linguistics.Ruchita Sarawgi, Kailash Gajulapalli, and Yejin Choi.2011.
Gender attribution: tracing stylometric evi-dence beyond topic and genre.
In Proceedings ofthe Fifteenth Conference on Computational NaturalLanguage Learning, pages 78?86.
Association forComputational Linguistics.Anders S?gaard, Anders Johannsen, Barbara Plank,Dirk Hovy, and H?ector Mart?
?nez Alonso.
2014.What?s in a p-value in nlp?
In Proceedings of theEighteenth Conference on Computational NaturalLanguage Learning, pages 1?10, Ann Arbor, Michi-gan, June.
Association for Computational Linguis-tics.Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, TingLiu, and Bing Qin.
2014.
Learning sentiment-specific word embedding for twitter sentiment clas-sification.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguis-tics, pages 1555?1565.Svitlana Volkova, Theresa Wilson, and DavidYarowsky.
2013.
Exploring demographic languagevariations to improve multilingual sentiment anal-ysis in social media.
In Proceedings of EMNLP,pages 1815?1827.Svitlana Volkova, Yoram Bachrach, Michael Arm-strong, and Vijay Sharma.
2015.
Inferring latentuser properties from texts published in social media(demo).
In Proceedings of the Twenty-Ninth Confer-ence on Artificial Intelligence (AAAI), Austin, TX,January.Martijn Wieling, John Nerbonne, and R HaraldBaayen.
2011.
Quantitative social dialectology: Ex-plaining linguistic variation geographically and so-cially.
PloS one, 6(9):e23613.762
