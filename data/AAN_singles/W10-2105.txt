Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 25?33,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsGrammar-driven versus Data-driven: Which Parsing System is MoreAffected by Domain Shifts?Barbara PlankUniversity of GroningenThe Netherlandsb.plank@rug.nlGertjan van NoordUniversity of GroningenThe NetherlandsG.J.M.van.Noord@rug.nlAbstractIn the past decade several parsing systemsfor natural language have emerged, whichuse different methods and formalisms.
Forinstance, systems that employ a hand-crafted grammar and a statistical disam-biguation component versus purely sta-tistical data-driven systems.
What theyhave in common is the lack of portabil-ity to new domains: their performancemight decrease substantially as the dis-tance between test and training domain in-creases.
Yet, to which degree do they suf-fer from this problem, i.e.
which kind ofparsing system is more affected by domainshifts?
Intuitively, grammar-driven sys-tems should be less affected by domainchanges.
To investigate this hypothesis,an empirical investigation on Dutch is car-ried out.
The performance variation ofa grammar-driven versus two data-drivensystems across domains is evaluated, and asimple measure to quantify domain sensi-tivity proposed.
This will give an estimateof which parsing system is more affectedby domain shifts, and thus more in needfor adaptation techniques.1 IntroductionMost modern Natural Language Processing (NLP)systems are subject to the wellknown problem oflack of portability to new domains: there is a sub-stantial drop in their performance when the sys-tem gets input from another text domain (Gildea,2001).
This is the problem of domain adapta-tion.
Although the problem exists ever since theemergence of supervised Machine Learning, it hasstarted to get attention only in recent years.Studies on supervised domain adaptation(where there are limited amounts of annotatedresources in the new domain) have shown thatstraightforward baselines (e.g.
models based onsource only, target only, or the union of the data)achieve a relatively high performance level and are?surprisingly difficult to beat?
(Daume?
III, 2007).In contrast, semi-supervised adaptation (i.e.
noannotated resources in the new domain) is a muchmore realistic situation but is clearly also consid-erably more difficult.
Current studies on semi-supervised approaches show very mixed results.Dredze et al (2007) report on ?frustrating?
re-sults on the CoNLL 2007 semi-supervised adap-tation task for dependency parsing, i.e.
?no teamwas able to improve target domain performancesubstantially over a state-of-the-art baseline?.
Onthe other hand, there have been positive results aswell.
For instance, McClosky et al (2006) im-proved a statistical parser by self-training.
Struc-tural Correspondence Learning (Blitzer et al,2006) was effective for PoS tagging and SentimentAnalysis (Blitzer et al, 2006; Blitzer et al, 2007),while only modest gains were obtained for struc-tured output tasks like parsing.For parsing, most previous work on do-main adaptation has focused on data-driven sys-tems (Gildea, 2001; McClosky et al, 2006;Dredze et al, 2007), i.e.
systems employing (con-stituent or dependency based) treebank gram-mars.
Only few studies examined the adaptation ofgrammar-based systems (Hara et al, 2005; Plankand van Noord, 2008), i.e.
systems employinga hand-crafted grammar with a statistical disam-biguation component.
This may be motivated bythe fact that potential gains for this task are inher-ently bound by the grammar.
Yet, domain adap-tation poses a challenge for both kinds of pars-ing systems.
But to what extent do these differ-ent kinds of systems suffer from the problem?
Wetest the hypothesis that grammar-driven systemsare less affected by domain changes.
We empir-ically investigate this in a case-study on Dutch.252 Related workMost previous work has focused on a single pars-ing system in isolation (Gildea, 2001; Hara etal., 2005; McClosky et al, 2006).
However,there is an observable trend towards combiningdifferent parsing systems to exploit complemen-tary strengths.
For instance, Nivre and McDon-ald (2008) combine two data-driven systems to im-prove dependency accuracy.
Similarly, two studiessuccessfully combined grammar-based and data-driven systems: Sagae et al (2007) incorporatedata-driven dependencies as soft-constraint in aHPSG-based system for parsing the WallstreetJournal.
In the same spirit (but the other di-rection), Zhang and Wang (2009) use a deep-grammar based backbone to improve data-drivenparsing accuracy.
They incorporate features fromthe grammar-based backbone into the data-drivensystem to achieve better generalization across do-mains.
This is the work most closest to ours.However, which kind of system (hand-craftedversus purely statistical) is more affected by thedomain, and thus more sensitive to domain shifts?To the best of our knowledge, no study has yet ad-dressed this issue.
We thus assess the performancevariation of three dependency parsing systems forDutch across domains, and propose a simple mea-sure to quantify domain sensitivity.3 Parsing SystemsThe parsing systems used in this study are: agrammar-based system for Dutch (Alpino) andtwo data-driven systems (MST and Malt), all de-scribed next.
(1) Alpino is a parser for Dutch which hasbeen developed over the last ten years, on the ba-sis of a domain-specific HPSG-grammar that wasused in the OVIS spoken dialogue system.
TheOVIS parser was shown to out-perform a statisti-cal (DOP) parser, in a contrastive formal evalua-tion (van Zanten et al, 1999).
In the ten years af-ter this evaluation, the system has developed into ageneric parser for Dutch.
Alpino consists of morethan 800 grammar rules in the tradition of HPSG,and a large hand-crafted lexicon.
It produces de-pendency structures as ouput, where more than asingle head per token is allowed.
For words thatare not in the lexicon, the system applies a largevariety of unknown word heuristics (van Noord,2006), which deal with number-like expressions,compounds, proper names, etc.
Coverage of thegrammar and lexicon has been extended over theyears by paying careful attention to the results ofparsing large corpora, by means of error miningtechniques (van Noord, 2004; de Kok et al, 2009).Lexical ambiguity is reduced by means of aPOS-tagger, described in (Prins and van No-ord, 2003).
This POS-tagger is trained on largeamounts of parser output, and removes unlikelylexical categories.
Some amount of lexical am-biguity remains.
A left-corner parser constructsa parse-forest for an input sentence.
Based onlarge amounts of parsed data, the parser considersonly promising parse step sequences, by filteringout sequences of parse steps which were not pre-viously used to construct a best parse for a givensentence.
The parse step filter improves efficiencyconsiderably (van Noord, 2009).A best-first beam-search algorithm retrieves thebest parse(s) from that forest by consulting a Max-imum Entropy disambiguation component.
Fea-tures for the disambiguation component includenon-local features.
For instance, there are featuresthat can be used to learn a preference for local ex-traction over long-distance extraction, and a pref-erence for subject fronting rather than direct ob-ject fronting, and a preference for certain types oforderings in the ?mittelfeld?
of a Dutch sentence.The various features that we use for disambigua-tion, as well as the best-first algorithm is describedin (van Noord, 2006).
The model now also con-tains features which implement selection restric-tions, trained on the basis of large parsed corpora(van Noord, 2007).
The maximum entropy dis-ambiguation component is trained on the Alpinotreebank, described below.To illustrate the role of the disambiguation com-ponent, we provide some results for the first 536sentences of one of the folds of the training data(of course, the model used in this experiment istrained on the remaining folds of training data).In this setup, the POS-tagger and parse step filteralready filter out many, presumably bad, parses.This table indicates that a very large amount ofparses can be constructed for some sentences.
Fur-thermore, the maximum entropy disambiguationcomponent does a good job in selecting goodparses from those.
Accuracy is given here in termsof f-score of named dependencies.sents parses oracle arbitrary model536 45011 95.74 76.56 89.39(2) MST Parser (McDonald et al, 2005) is a26data-driven graph-based dependency parser.
Thesystem couples a minimum spanning tree searchprocedure with a separate second stage classifierto label the dependency edges.
(3) MALT Parser (Nivre et al, 2007) is a data-driven transition-based dependency parser.
Maltparser uses SVMs to learn a classifier that predictsthe next parsing action.
Instances represent parserconfigurations and the label to predict determinesthe next parser action.Both data-driven parsers (MST and Malt) arethus not specific for the Dutch Language, however,they can be trained on a variety of languages giventhat the training corpus complies with the column-based format introduced in the 2006 CoNLLshared task (Buchholz and Marsi, 2006).
Ad-ditionally, both parsers implement projective andnon-projective parsing algorithms, where the latterwill be used in our experiments on the relativelyfree word order language Dutch.
Despite that, wetrain the data-driven parsers using their default set-tings (e.g.
first order features for MST, SVM withpolynomial kernel for Malt).4 Datasets and experimental setupThe source domain on which all parsers are trainedis cdb, the Alpino Treebank (van Noord, 2006).For our cross-domain evaluation, we considerWikipedia and DPC (Dutch Parallel Corpus) astarget data.
All datasets are described next.Source: Cdb The cdb (Alpino Treebank) con-sists of 140,000 words (7,136 sentences) from theEindhoven corpus (newspaper text).
It is a col-lection of text fragments from 6 Dutch newspa-pers.
The collection has been annotated accord-ing to the guidelines of CGN (Oostdijk, 2000) andstored in XML format.
It is the standard treebankused to train the disambiguation component of theAlpino parser.
Note that cdb is a subset of thetraining corpus used in the CoNLL 2006 sharedtask (Buchholz and Marsi, 2006).
The CoNLLtraining data additionally contained a mix of non-newspaper text,1 which we exclude here on pur-pose to keep a clean baseline.Target: Wikipedia and DPC We use theWikipedia and DPC subpart of the LASSY cor-1Namely, a large amount of questions (from CLEF,roughly 4k sentences) and hand-crafted sentences used dur-ing the development of the grammar (1.5k).Wikipedia Example articles #a #w ASLLOC (location) Belgium, Antwerp (city) 31 25259 11.5KUN (arts) Tervuren school 11 17073 17.1POL (politics) Belgium elections 2003 16 15107 15.4SPO (sports) Kim Clijsters 9 9713 11.1HIS (history) History of Belgium 3 8396 17.9BUS (business) Belgium Labour Federation 9 4440 11.0NOB (nobility) Albert II 6 4179 15.1COM (comics) Suske and Wiske 3 4000 10.5MUS (music) Sandra Kim, Urbanus 3 1296 14.6HOL (holidays) Flemish Community Day 4 524 12.2Total 95 89987 13.4DPC Description/Example #a #words ASLScience medicine, oeanography 69 60787 19.2Institutions political speeches 21 28646 16.1Communication ICT/Internet 29 26640 17.5Welfare state pensions 22 20198 17.9Culture darwinism 11 16237 20.5Economy inflation 9 14722 18.5Education education in Flancers 2 11980 16.3Home affairs presentation (Brussel) 1 9340 17.3Foreign affairs European Union 7 9007 24.2Environment threats/nature 6 8534 20.4Finance banks (education banker) 6 6127 22.3Leisure various (drugscandal) 2 2843 20.3Consumption toys from China 1 1310 22.6Total 186 216371 18.5Table 1: Overview Wikipedia and DPC corpus (#aarticles, #w words, ASL average sentence length)pus2 as target domains.
These corpora contain sev-eral domains, e.g.
sports, locations, science.
Onoverview of the corpora is given in Table 1.
Notethat both consist of hand-corrected data labeled byAlpino, thus all domains employ the same anno-tation scheme.
This might introduce a slight biastowards Alpino, however it has the advantage thatall domains employ the same annotation scheme ?which was the major source of error in the CoNLLtask on domain adaptation (Dredze et al, 2007).CoNLL2006 This is the testfile for Dutch thatwas used in the CoNLL 2006 shared task on multi-lingual dependency parsing.
The file consistsof 386 sentences from an institutional brochure(about youth healthcare).
We use this file to checkour data-driven models against state-of-the-art.Alpino to CoNLL format In order to train theMST and Malt parser and evaluate it on the var-ious Wikipedia and DPC articles, we needed toconvert the Alpino Treebank format into the tab-ular CoNLL format.
To this end, we adapted thetreebank conversion software developed by ErwinMarsi for the CoNLL 2006 shared task on multi-lingual dependency parsing.
Instead of using thePoS tagger and tagset used in the shared task (towhich we did not have access to), we replaced thePoS tags with more fine-grained tags obtained by2LASSY (Large Scale Syntactic Annotation of writtenDutch), ongoing project.
Corpus version 17905, obtainedfrom http://www.let.rug.nl/vannoord/Lassy/corpus/27parsing the data with the Alpino parser.3 At testingtime, the data-driven parsers are given PoS taggedinput, while Alpino gets plain sentences.Evaluation In all experiments, unless otherwisespecified, performance is measured as LabeledAttachment Score (LAS), the percentage of to-kens with the correct dependency edge and label.To compute LAS, we use the CoNLL 2007 eval-uation script4 with punctuation tokens excludedfrom scoring (as was the default setting in CoNLL2006).
We thus evaluate all parsers using the sameevaluation metric.
Note that the standard metricfor Alpino would be a variant of LAS, which al-lows for a discrepancy between expected and re-turned dependencies.
Such a discrepancy can oc-cur, for instance, because the syntactic annotationof Alpino allows words to be dependent on morethan a single head (?secondary edges?)
(van No-ord, 2006).
However, such edges are ignored inthe CoNLL format; just a single head per tokenis allowed.
Furthermore, there is another simpli-fication.
As the Dutch tagger used in the CoNLL2006 shared task did not have the concept of multi-words, the organizers chose to treat them as a sin-gle token (Buchholz and Marsi, 2006).
We herefollow the CoNLL 2006 task setup.
To determinewhether results are significant, we us the Approx-imate Randomization Test (see Yeh (2000)) with1000 random shuffles.5 Domain sensitivityThe problem of domain dependence poses a chal-lenge for both kinds of parsing systems, data-driven and grammar-driven.
However, to what ex-tent?
Which kind of parsing system is more af-fected by domain shifts?
We may rephrase ourquestion as: Which parsing system is more robustto different input texts?
To answer this question,we will examine the robustness of the differentparsing systems in terms of variation of accuracyon a variety of domains.A measure of domain sensitivity Given a pars-ing system (p) trained on some source domainand evaluated on a set of N target domains, themost intuitive measure would be to simply calcu-3As discussed later (Section 6, cf.
Table 2), using Alpinotags actually improves the performance of the data-drivenparsers.
We could perform this check as we recently got ac-cess to the tagger and tagset used in the CoNLL shared task(Mbt with wotan tagset; thanks to Erwin Marsi).4http://nextens.uvt.nl/depparse-wiki/SoftwarePagelate mean (?)
and standard deviation (sd) of theperformance on the target domains:LASip = accuracy of parser p on target domain i?targetp =?Ni=1 LASipN, sdtargetp =?
?Ni=1(LASip ?
?targetp )2N ?
1However, standard deviation is highly influencedby outliers.
Furthermore, this measure does nottake the source domain performance (baseline)into consideration nor the size of the target domainitself.
We thus propose to measure the domainsensitivity of a system, i.e.
its average domainvariation (adv), as weighted average differencefrom the baseline (source) mean, where weightsrepresents the size of the various domains:adv =?Ni=1wi ?
?ip?Ni=1wi, with?ip = LASip?LASbaselinep and wi =size(wi)?Ni=1 size(wi)In more detail, we measure average domainvariation (adv) relative to the baseline (source do-main) performance by considering non-squareddifferences from the out-of-domain mean andweigh it by domain size.
The adv measure canthus take on positive or negative values.
Intu-itively, it will indicate the average weighted gainor loss in performance, relative to the source do-main.
As alternative, we may want to just cal-culate a straight, unweighted average: uadv =?Ni=1 ?ip/N .
However, this assumes that domainshave a representative size, and a threshold mightbe needed to disregard domains that are presum-ably too small.We will use adv in the empirical result sectionto evaluate the domain sensitivity of the parsers,where sizewill be measured in terms of number ofwords.
We additionally provide values for the un-weighted version using domains with at least 4000words (cf.
Table 1).6 Empirical resultsFirst of all, we performed several sanity checks.We trained the MST parser on the entire originalCoNLL training data as well as the cdb subpartonly, and evaluated it on the original CoNLL testdata.
As shown in Table 2 (row 1-2) the accura-cies of both models falls slightly below state-of-the-art performance (row 5), most probably due tothe fact that we used standard parsing settings (e.g.28no second-order features for MST).
More impor-tantly, there was basically no difference in perfor-mance when trained on the entire data or cdb only.Model LAS UASMST (original CoNLL) 78.35 82.89MST (original CoNLL, cdb subpart) 78.37 82.71MST (cdb retagged with Alpino) 82.14 85.51Malt (cdb retagged with Alpino) 80.64 82.66MST (Nivre and McDonald, 2008) 79.19 83.6Malt (Nivre and McDonald, 2008) 78.59 n/aMST (cdb retagged with Mbt) 78.73 82.66Malt (cdb retagged with Mbt) 75.34 78.29Table 2: Performance of data-driven parsers ver-sus state-of-the-art on the CoNLL 2006 testset (inLabeled/Unlabeled Attachment Score).We then trained the MST and Malt parser onthe cdb corpus converted into the retagged CoNLLformat, and tested on CoNLL 2006 test data (alsoretagged with Alpino).
As seen in Table 2, byusing Alpino tags the performance level signifi-cantly improves (with p < 0.002 using Approx-imate Randomization Test with 1000 iterations).This increase in performance can be attributed totwo sources: (a) improvements in the Alpino tree-bank itself over the course of the years, and (b) themore fine-grained PoS tagset obtained by parsingthe data with the deep grammar.
To examine thecontribution of each source, we trained an addi-tional MST model on the cdb data but tagged withthe same tagger as in the CoNLL shared task (Mbt,cf.
Table 2 last row): the results show that themajor source of improvement actually comes fromusing the more fine-grained Alpino tags (78.73?82.14 = +3.41 LAS), rather than the changes inthe treebank (78.37 ?
78.73 = +0.36 LAS).Thus, despite the rather limited training data anduse of standard training settings, we are in linewith, and actually above, current results of data-driven parsing for Dutch.Baselines To establish our baselines, we per-form 5-fold cross validation for each parser on thesource domain (cdb corpus, newspaper text).
Thebaselines for each parser are given in Table 3.
Thegrammar-driven parser Alpino achieves a baselinethat is significantly higher (90.75% LAS) com-pared to the baselines of the data-driven systems(around 80-83% LAS).Cross-domain results As our goal is to assessperformance variation across domains, we evalu-ate each parser on the Wikipedia and DPC corporaModel Alpino MST MaltBaseline (LAS) 90.76 83.63 79.95Baseline (UAS) 92.47 88.12 83.31Table 3: Baseline (5-fold cross-validation).
Alldifferences are significant at p < 0.001.that cover a variety of domains (described in Ta-ble 1).
Figure 1 and Figure 2 summarizes the re-sults for each corpus, respectively.
In more detail,the figures depict for each parser the baseline per-formance as given in Table 3 (straight lines) andthe performance on every domain (bars).
Note thatdomains are ordered by size (number of words), sothat the largest domains appear as bars on the left.Similar graphs come up if we replace labeled at-tachment score with its unlabeled variant.Figure 1 depicts parser performance on theWikipedia domains with respect to the sourcedomain baseline.
The figure indicates that thegrammar-driven parser does not suffer much fromdomain shifts.
Its performance falls even abovebaseline for several Wikipedia domains.
In con-trast, the MST parser suffers the most from thedomain changes; on most domains a substantialperformance drop can be observed.
The transition-based parser scores on average significantly lowerthan the graph-based counterpart and Alpino, butseems to be less affected by the domain shifts.We can summarize this findings by our pro-posed average domain variation measure (un-weighted scores are given in the Figure): On av-erage (over all Wikipedia domains), Alpino suf-fers the least (adv = +0.81), followed by Malt(+0.59) and MST (?2.2), which on average loses2.2 absolute LAS.
Thus, the graph-based data-driven dependency parser MST suffers the most.We evaluate the parsers also on the more var-ied DPC corpus.
It contains a broader set of do-mains, amongst others science texts (medical textsfrom the European Medicines Agency as well astexts about oceanography) and articles with moretechnical vocabulary (Communication, i.e.
Inter-net/ICT texts).
The results are depicted in Fig-ure 2.
Both Malt (adv = 0.4) and Alpino (adv =0.22) achieve on average a gain over the baseline,with this time Malt being slightly less domain af-fected than Alpino (most probably because Maltscores above average on the more influential/largerdomains).
Nevertheless, Alpino?s performancelevel is significantly higher compared to both data-driven counterparts.
The graph-based data-driven29LabeledAttachment Score (LAS)Alpinoadv= 0.81 (+/?
3.7 )uadv (>4k)= 2 (+/?
2.1 )757677787980818283848586878889909192939495969798LOCKUNPOLSPO HIS BUSNOBCOMMUSHOLLOCKUNPOLSPO HIS BUSNOBCOMMUSHOLMSTadv = ?2.2 (+/?
9 )uadv (>4k)= ?1.8 (+/?
4 )LOCKUNPOLSPO HIS BUSNOBCOMMUSHOLMaltadv = 0.59 (+/?
9.4 )uadv(>4k)= 1.3 (+/?
3 )AlpinoMSTMaltFigure 1: Performance on Wikipedia domains with respect to the source baseline (newspaper text) in-cluding average domain variation (adv) score and its unweighted alternative (uadv).
Domains are orderedby size (largest on left).
Full-colored bars indicate domains where performance lies below the baseline.parser MST is the most domain-sensitive parseralso on DPC (adv = ?0.27).In contrast, if we would take only the deviationon the target domains into consideration (with-out considering the baseline, cf.
Section 5), wewould get a completely opposite ranking on DPC,where the Malt parser would actually be consid-ered the most domain-sensitive (here higher sdmeans higher sensitivity): Malt (sd = 1.20), MST(sd = 1.14), Alpino (sd = 1.05).
However, bylooking at Figure 2, intuitively, MST suffers morefrom the domain shifts than Malt, as most bars liebelow the baseline.
Moreover, the standard devia-tion measure neither gives a sense of whether theparser on average suffers a loss or gain over thenew domains, nor incorporates the information ofdomain size.
We thus believe our proposed aver-age domain variation is a better suited measure.To check whether the differences in perfor-mance variation are statistically significant, weperformed an Approximate Randomization Testover the performance differences (deltas) on the23 domains (DPC and Wikipedia).
The resultsshow that the difference between Alpino and MSTis significant.
The same goes for the differencebetween MST and Malt.
Thus Alpino is signifi-cantly more robust than MST.
However, the dif-ference between Alpino and Malt is not signif-icant.
These findings hold for differences mea-sured in both labeled and unlabeled attachmentsscores.
Furthermore, all differences in absoluteperformance across domains are significant.To summarize, our empirical evaluation showsthat the grammar-driven system Alpino is ratherrobust across domains.
It is the best perform-ing system and it is significantly more robust thanMST.
In constrast, the transition-based parser Maltscores the lowest across all domains, but its vari-ation turned out not to be different from Alpino.Over all domains, MST is the most domain-sensitive parser.30LabeledAttachmentScore(LAS)787980818283848586878889909192939495ScienceInstitutionsCommunicationWelfare_stateCultureEconomyEducationHome_affairsForeign_affairsEnvironmentFinanceLeisureConsumptionAlpinoadv = 0.22 (+/?
0.823 )uadv (>4k)= 0.4 (+/?
0.8 )ScienceInstitutionsCommunicationWelfare_stateCultureEconomyEducationHome_affairsForeign_affairsEnvironmentFinanceLeisureConsumptionMSTadv = ?0.27 (+/?
0.56 )uadv (>4k)= ?0.21 (+/?
1 )ScienceInstitutionsCommunicationWelfare_stateCultureEconomyEducationHome_affairsForeign_affairsEnvironmentFinanceLeisureConsumptionMaltadv = 0.4 (+/?
0.54 )uadv (>4k)= 0.41 (+/?
0.9 )AlpinoMSTMaltFigure 2: Performance on DPC domains with respect to the source baseline (newspaper text).Excursion: Lexical information Both kindsof parsing systems rely on lexical information(words/stems) when learning their parsing (orparse disambiguation) model.
However, howmuch influence does lexical information have?To examine this issue, we retrain all parsing sys-tems by excluding lexical information.
As all pars-ing systems rely on a feature-based representa-tion, we remove all feature templates that includewords and thus train models on a reduced fea-ture space (original versus reduced space: Alpino24k/7k features; MST 14M/1.9M features; Malt17/13 templates).
The result of evaluating theunlexicaled models on Wikipedia are shown inFigure 3.
Clearly, performance drops for for allparsers in all domains.
However, for the data-driven parsers to a much higher degree.
For in-stance, MST loses on average 11 absolute pointsin performance (adv = ?11) and scores belowbaseline on all Wikipedia domains.
In contrast,the grammar-driven parser Alpino suffers far less,still scores above baseline on some domains.5 TheMalt parser lies somewhere in between, also suf-fers from the missing lexical information, but to alesser degree than the graph-based parser MST.7 Conclusions and Future workWe examined a grammar-based system cou-pled with a statistical disambiguation component(Alpino) and two data-driven statistical parsingsystems (MST and Malt) for dependency parsingof Dutch.
By looking at the performance variationacross a large variety of domains, we addressedthe question of how sensitive the parsing systemsare to the text domain.
This, to gauge which kind5Note that the parser has still access to its lexicon here;for now we removed lexicalized features from the trainablepart of Alpino, the statistical disambiguation component.31LabeledAttachment Score (LAS)Alpinoadv= ?0.63 (+/?
3.6 )uadv (>4k)= 0.1 (+/?
2 )66687072747678808284868890929496LOCKUNPOLSPO HIS BUSNOBCOMMUSHOLLOCKUNPOLSPO HIS BUSNOBCOMMUSHOLMSTadv = ?11 (+/?
11 )uadv (>4k)= ?11 (+/?
2.1 )LOCKUNPOLSPO HIS BUSNOBCOMMUSHOLMaltadv = ?4.9 (+/?
9 )uadv (>4k)= ?4.8 (+/?
3 )AlpinoMSTMaltFigure 3: Performance of unlexical parsers on Wikipedia domains with respect to the source baseline.of system (data-driven versus grammar-driven) ismore affected by domain shifts, and thus more inneed for adaptation techniques.
We also proposeda simple measure to quantify domain sensitivity.The results show that the grammar-based sys-tem Alpino is the best performing system, and itis robust across domains.
In contrast, MST, thegraph-based approach to data-driven parsing is themost domain-sensitive parser.
The results for Maltindicate that its variation across domains is lim-ited, but this parser is outperformed by both othersystems on all domains.
In general, data-drivensystems heavily rely on the training data to esti-mate their models.
This becomes apparent whenwe exclude lexical information from the train-ing process, which results in a substantial perfor-mance drop for the data-driven systems, MST andMalt.
The grammar-driven model was more robustagainst the missing lexical information.
Grammar-driven systems try to encode domain independentlinguistic knowledge, but usually suffer from cov-erage problems.
The Alpino parser successfullyimplements a set of unknown word heuristics anda partial parsing strategy (in case no full parse canbe found) to overcome this problem.
This makesthe system rather robust across domains, and, asshown in this study, significantly more robust thanMST.
This is not to say that domain dependencedoes not consitute a problem for grammar-drivenparsers at all.
As also noted by Zhang and Wang(2009), the disambiguation component and lexi-cal coverage of grammar-based systems are stilldomain-dependent.
Thus, domain dependence is aproblem for both types of parsing systems, though,as shown in this study, to a lesser extent for thegrammar-based system Alpino.
Of course, theseresults are specific for Dutch; however, it?s a firststep.
As the proposed methods are indepedent oflanguage and parsing system, they can be appliedto another system or language.In future, we would like to (a) perform an erroranalysis (e.g.
why for some domains the parsersoutperform their baseline; what are typical in-domain and out-domain errors), (a) examine whythere is such a difference in performance variationbetween Malt and MST, and (c) investigate whatpart(s) of the Alpino parser are responsible for thedifferences with the data-driven parsers.32ReferencesJohn Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Conference on Empirical Meth-ods in Natural Language Processing, Sydney.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In ACL, Prague, Czech Republic.Sabine Buchholz and Erwin Marsi.
2006.
Conll-xshared task on multilingual dependency parsing.
InIn Proc.
of CoNLL, pages 149?164.Hal Daume?
III.
2007.
Frustratingly easy domain adap-tation.
In ACL, Prague, Czech Republic.Danie?l de Kok, Jianqiang Ma, and Gertjan van Noord.2009.
A generalized method for iterative error min-ing in parsing results.
In Proceedings of the 2009Workshop on Grammar Engineering Across Frame-works (GEAF 2009), pages 71?79, Suntec, Singa-pore, August.Mark Dredze, John Blitzer, Pratha Pratim Taluk-dar, Kuzman Ganchev, Joao Graca, and FernandoPereira.
2007.
Frustratingly hard domain adaptationfor parsing.
In Proceedings of the CoNLL SharedTask Session, Prague, Czech Republic.Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Proceedings of the 2001 Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP).Tadayoshi Hara, Miyao Yusuke, and Jun?ichi Tsu-jii.
2005.
Adapting a probabilistic disambiguationmodel of an hpsg parser to a new domain.
In Pro-ceedings of the International Joint Conference onNatural Language Processing.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of the Human Language TechnologyConference of the NAACL, Main Conference, pages152?159, New York City.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In In Proceed-ings of Human Language Technology Conferenceand Conference on Empirical Methods in NaturalLanguage Processing, pages 523?530.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In Proceedings of ACL-08: HLT, pages950?958, Columbus, Ohio, June.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, Gu?lsen Eryigit, Sandra Ku?bler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13:95?135.Nelleke Oostdijk.
2000.
The Spoken Dutch Corpus:Overview and first evaluation.
In Proceedings ofLREC, pages 887?894.Barbara Plank and Gertjan van Noord.
2008.
Ex-ploring an auxiliary distribution based approach todomain adaptation of a syntactic disambiguationmodel.
In Proceedings of the Workshop on Cross-Framework and Cross-Domain Parser Evaluation(PE), Manchester, August.Robbert Prins and Gertjan van Noord.
2003.
Reinforc-ing parser preferences through tagging.
TraitementAutomatique des Langues, 44(3):121?139.Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii.
2007.Hpsg parsing with shallow dependency constraints.In Proceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, pages 624?631, Prague, Czech Republic, June.Gertjan van Noord.
2004.
Error mining for wide-coverage grammar engineering.
In ACL2004,Barcelona.
ACL.Gertjan van Noord.
2006.
At Last Parsing Is NowOperational.
In TALN 2006 Verbum Ex Machina,Actes De La 13e Conference sur Le TraitementAutomatique des Langues naturelles, pages 20?42,Leuven.Gertjan van Noord.
2007.
Using self-trained bilexicalpreferences to improve disambiguation accuracy.
InProceedings of the International Workshop on Pars-ing Technology (IWPT), ACL 2007 Workshop, pages1?10, Prague.
ACL.Gertjan van Noord.
2009.
Learning efficient parsing.In EACL 2009, The 12th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 817?825, Athens, Greece.Gert Veldhuijzen van Zanten, Gosse Bouma, KhalilSima?an, Gertjan van Noord, and Remko Bonnema.1999.
Evaluation of the NLP components of theOVIS2 spoken dialogue system.
In Frank vanEynde, Ineke Schuurman, and Ness Schelkens, ed-itors, Computational Linguistics in the Netherlands1998, pages 213?229.
Rodopi Amsterdam.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In ACL,pages 947?953, Morristown, NJ, USA.Yi Zhang and Rui Wang.
2009.
Cross-domain depen-dency parsing using a deep linguistic grammar.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP, pages 378?386, Suntec, Singapore,August.33
