Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 578?584,Dublin, Ireland, August 23-24, 2014.SNAP: A Multi-Stage XML-Pipeline for Aspect Based Sentiment AnalysisClemens Schulze Wettendorf and Robin Jegan and Allan K?orner and Julia Zercheand Nataliia Plotnikova and Julian Moreth and Tamara Schertl and Verena Obermeyerand Susanne Streil and Tamara Willacker and Stefan EvertFriedrich-Alexander-Universit?at Erlangen-N?urnbergDepartment Germanistik und KomparatistikProfessur f?ur KorpuslinguistikBismarckstr.
6, 91054 Erlangen, Germany{clemens.schulze.wettendorf, robin.jegan, allan.koerner, julia.zerche,nataliia.plotnikova, julian.moreth, tamara.schertl, verena.obermeyer,susanne.streil, tamara.willacker, stefan.evert}@fau.deAbstractThis paper describes the SNAP system,which participated in Task 4 of SemEval-2014: Aspect Based Sentiment Analysis.We use an XML-based pipeline that com-bines several independent components toperform each subtask.
Key resourcesused by the system are Bing Liu?s senti-ment lexicon, Stanford CoreNLP, RFTag-ger, several machine learning algorithmsand WordNet.
SNAP achieved satisfactoryresults in the evaluation, placing in the tophalf of the field for most subtasks.1 IntroductionThis paper describes the approach of the SemaN-tic Analyis Project (SNAP) to Task 4 of SemEval-2014: Aspect Based Sentiment Analysis (Pontikiet al., 2014).
SNAP is a team of undergraduatestudents at the Corpus Linguistics Group, FAUErlangen-N?urnberg, who carried out this work aspart of a seminar in computational linguistics.Task 4 was divided into the four subtasks As-pect term extraction (1), Aspect term polarity (2),Aspect category detection (3) and Aspect categorypolarity (4), which were evaluated in two phases(A: subtasks 1/3; B: subtasks 2/4).
Subtasks 1 and3 were carried out on two different datasets, oneof laptop reviews and one of restaurant reviews.Subtasks 2 and 4 only made use of the latter.This work is licensed under a Creative Commons Attribution4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/Task Dataset Rank Score Best1 Lap 10 of 21 0.624 0.7461 Res 20 of 21 0.465 0.8403 Res 6 of 15 0.782 0.8862 Lap 7 of 23 0.641 0.7052 Res 12 of 24 0.708 0.8104 Res 11 of 18 0.696 0.829Table 1: Ranking among constrained systems.The developed system consists of one moduleper subtask, in addition to a general infrastruc-ture and preprocessing module, All modules ac-cept training and test data in the XML format spec-fied by the task organizers.
The modules can becombined into a pipeline, where each step addsnew annotation corresponding to one of the foursubtasks.Table 1 shows our ranking among all con-strained systems (counting only the best run fromeach team), the score achieved by SNAP (accu-racy or F-score, depending on subtask), and thescore achieved by the best system in the respec-tive subtask.
Because of a preprocessing mistakethat was only discovered after phase A of the eval-uation had ended, results for subtasks 1 and 3 aresignificantly lower than the results achieved dur-ing development of the system.2 Sentiment lexiconEarly on in the project it was decided that a com-prehensive, high-quality sentiment lexicon wouldplay a crucial role in building a successful sys-tem.
After a review of several existing lexica, Bing578Liu?s sentiment word list (Hu and Liu, 2004) wastaken as a foundation and expanded with extensivemanual additions.The first step was an exhaustive manual web-search to find additional candidates for the lexicon.The candidates were converted to a common for-mat, and redundant entries were discareded.
Thenext step consisted of further expansion with thehelp of online thesauri, from which large num-ber of synonyms and antonyms for existing entrieswere obtained.
Since the coverage of the lexiconwas still found to be insufficient, it was furthercomplemented with entries from two other exist-ing sentiment lexica, AFINN (Nielsen, 2011) andMPQA (Wilson et al., 2005).Finally the augmented lexicon was comparedwith the original word lists from AFINN, MPQAand Bing Liu in order to measure the reliabilty ofthe entries.
The reliability score of each entry isthe number of sources in which it is found.3 Infrastructure and preprocessingWithin the scope of Task 4 ?
but not one of theofficial subtasks ?
the goal of the infrastructuremodule was (i) to support the other modules witha set of project-specific tools and (ii) to providea common API to the training and test data aug-mented with several layers of linguistic annota-tion.
In order to roll out the required data asquick as possible, the Stanford CoreNLP suite1was used as an off-the-shelf tool.
The XML filesprovided by the task organizers were parsed withthe xml.etree.ElementTree API, which is part ofthe standard library of Python 2.7.Since the module for subtask 1 was pursuing anIOB-tagging approach for aspect term identifica-tion, the part-of-speech tags provided by CoreNLPhad to be extended.
During the process of mergingthe original XML files with the CoreNLP annota-tions, IOB tags were generated indicating whethereach token is part of an aspect term or not.
SeeSection 4 for further information.For determining the polarity of an aspect term,the subtask 2 module made use of syntactic depen-dencies between words (see Section 5 for details).For this purpose, the dependency trees producedby CoreNLP were converted into a more accessi-ble format with the help of the Python softwarepackage NetworkX.2.1http://nlp.stanford.edu/software/corenlp.shtml2http://networkx.github.io/4 Aspect term extractionThe approach chosen by the aspect term extrac-tion module (subtask 1) was to treat aspect termextraction as a tagging task.
We used a standardIOB tagset indicating whether each token is at thebeginning of an aspect term (ATB), inside an as-pect term (ATI), or not part of an aspect term atall (ATX).First experiments were carried out with uni-gram, bigram and trigram taggers implemented inNLTK (Bird et al., 2009), which were trained onIOB tags derived from the annotations in the Task4 gold standard (comprising both trial and trainingdata).
We also tested higher-order n-gram taggersand the NLTK implementation of the Brill tagger(Brill, 1992).For a more sophisticated approach we used RF-Tagger (Schmid and Laws, 2008), which extendsthe standard HMM tagging model with complexhidden states that consist of features correspond-ing to different pieces of information.
RFTaggerwas developed for morphological tagging, wherecomplex tags such as N.Reg.Nom.Sg.Neutare decomposed into the main syntactic category(N) and additional morpho-syntactic features rep-resenting case (Nom), number (Sg), etc.In our case, the tagger was used for joint an-notation of part-of-speech tags and IOB tags forthe aspect term boundaries, based on the ratio-nale that the additional information encoded inthe hidden states (compared to a simple IOB tag-ger) would allow RFTagger to learn more mean-ingful aspect term patterns.
We decided to en-code the IOB tags as the main category and thepart-of-speech tags as additional features, sincechanging these categories, meaning POS tags asthe main category and IOB tags as additional fea-tures, had resulted in lesser performance.
Thetraining data were thus converted into word-annotation pairs such as screen_AT/ATB.NNor beautiful/ATX.JJ.
Note that known as-pect terms from the gold standard (as well as ad-ditional candidates that were generated throughcomparisons of known aspect terms with lists fromWordNet) were extended with the suffix _AT in apreprocessing step.
Our intention was to enablethe tagger to learn directly that tokens with thissuffix are likely to be aspect terms.Table 2 shows tagging accuracy for different al-gorithms, computed by ten-fold cross-validationover a gold standard comprising the training and579Tagger Rest.
LaptopsUnigram 83.41% 83.91%Bigram (backoff: U) 85.37% 85.74%Trigram (backoff: U+Bi) 85.41% 86.33%Brill (backoff: U+Bi+T) 85.48% 86.47%RFTagger 95.20% 96.47%Table 2: Accuracy of different aspect term taggers.trial data sets.
The table shows that the bigram, tri-gram and Brill taggers achieve only marginal im-provements over a simplistic unigram tagger, evenwhen they are combined through back-off linking.The RFTagger achieved by far the best accuracyon both data sets.4.1 Results and debuggingOur final results for the full aspect term extractionprocedure are shown in Table 3.Score Rest.
LaptopsPrecision 57.14% 64.54%Recall 39.15% 60.40%F1-Score 46.47% 62.40%Table 3: Aspect term extraction results.The huge difference between tagging accuracyachieved in the development phase and the aspectterm extraction quality obtained on the SemEval-2014 test set is caused by different factors.
First,Table 2 shows the tagging accuracy across all to-kens, not limited to aspect terms.
A tagger thatworks particularly well for many irrelevant tokens(punctuation, verbs, etc.
), correctly marking themATX, may achieve high accuracy even if it has lowrecall on tokens belonging to aspect terms.
Sec-ond, the official scores only consider an aspectterm candidate to be a true positive if it coversexactly the same tokens as the gold standard an-notation.
If the tagger disagrees with the humanannotators on whether an adjective or determinershould be considered part of an aspect term, thiswill be counted as a mistake despite the overlap.Thus, even a relatively small number of taggingmistakes near aspect term boundaries will be pun-ished severly in the evaluation.
Unseen words aswell as long or unusual noun phrases turned out tobe particularly difficult.Table 3 indicates a serious problem with therestaurant data, which has surprisingly low recall,resulting in an F1-score almost 16 percent pointslower than for the laptop data.
A careful exami-nation of the trial, training and test data revealedan early mistake in the preprocessing code as themain culprit.
Once this mistake was corrected, therecall score for restaurants was similar to the scorefor laptops.5 Aspect term polaritySubtask 2 is concerned with opinion sentences,i.e.
sentences that contain one or more aspectterms and express subjective opinions about (someof) these aspects.
Such opinions are expressedthrough opinion words; common opinion wordswith their corresponding confidence values (nu-meric values from 1 to 6 expressing the level ofcertainty that a word is positive or negative, cf.Sec.
2) are collected in sentiment lexica.The preprocessing stage in this subtask startswith a sentence segmentation step that uses theoutput of the Stanford CoreNLP parser.3All de-pendencies map onto a directed graph represen-tation where words of each sentence are nodesin the graph and grammatical relations are edgelabels.
All aspect terms (Sec.
2) are marked ineach dependency graph.
When processing such agraph we extract all positive and negative opinionwords occurring in each sentence by comparingthem with word lists contained in our sentimentlexica.
A corresponding confidence value fromlexica is assigned for each opinion word, the num-ber of positive and negative aspect terms occurringin each sentence are counted and their confidencevalues are summed up.
These values serve as fea-tures for supervised machine learning using algo-rithms implemented in scikit-learn (Pedregosa etal., 2011).All opinion words that build a dependency withan aspect term are stored for each sentence.
Adominant word of each dependency is stored as agovernor, whereas a subordinate one is stored as adependent.
Both direct and indirect dependenciesare processed.
If there are several indirect depen-dencies to an aspect term, they are processed re-cursively.
Using lists of extracted dependenciesbetween opinion words and aspect terms hand-writen rules assign corresponding confidence val-ues to aspect terms.3nlp.stanford.edu/software/dependencies manual.pdf5805.1 Features based on a sentiment lexicaThe extended sentiment dictionaries were used toextract five features: I) tokens expressing a pos-itive sentiment belonging to one aspect term, II)tokens expressing a negative sentiment, III) con-fidence values of positive tokens, IV) confidencevalues of negative tokens, V) a sum of all confi-dence values for all positive and all negative opin-ion words occurring in a sentence.5.2 Features based on hand-written rulesWe made use of direct and indirect negation mark-ers, so that all opinion words belonging to anegated aspect term swap their polarity signs.
Weadded rules for negative particles not and no thatdirectly precede an opinion word, for adverbsbarely, scarcely and too, for such constructionsas could have been and wish in the subjunctivemood.
After swapping polarity signs of opinionwords, a general set of hand-written rules was ap-plied to the graph dependencies.
The rules fol-low the order of importance of dependencies scal-ing from least important up to most important.We placed the dependencies in the following or-der: acomp, advmod, nsubjpass, conj and, amod,prep of, prep worth, prep on, prep in, nsubj, inf-mod, dobj, xcomp, rcmod, conj or, appos.
All de-pendencies can be grouped into three categoriesbased on the direction of the polarity assignment.The first group (acomp, advmod, amod, rcmod,prep in) includes dependencies where a governorof a dependency takes over polarity of a dependentif the latter is defined.
The second group (infmod,conj or, prep on, prep worth, prep of, conj and)covers dependencies in which a dependent ele-ment takes over polarity of a governor if the lat-ter is defined.
The third group (dobj, xcomp) isfor cases when both governor and dependent aredefined.
Here a governor takes over polarity of adependent.5.3 ExperimentsIn this section we compare two approches to as-pect term polarity detection.
The first approachsimply counts all positive and negative words ineach sentence and then assigns a label based onwhich of the two counts is larger.
It does not makeuse of machine learning techniques and its accu-racy is only about 54%.
Results improve signifi-cantly with supervised machine learning based onthe feature sets described above.
We experimentedwith different classifiers (Maximum Entropy, Lin-ear SVM and SVMs with RBF kernel) and var-ious subsets of features.
By default, we workedon the level of single opinion words that expressa positive or negative polarity (sg).
We addedthe following features in different combinations:an extended list of opinion words (ex) obtainedfrom a distribution semantic model, based on near-est neighbours of known opinion words (Proisl etal., 2013); potential misspellings of know opin-ion words, within a maximal Levenshtein distanceof 1 (lv); word combinations and fixed phrases(ml) containing up to 3 words (e.g., good man-nered, put forth, tried and true, up and down);and the sums of positive and negative opinionwords in the whole sentence (st).
The best resultsfor the laptops data were achieved with a Max-imum Entropy classifier, excluding misspellings(lv) and word combinations (ml); the correspond-ing line in Table 4 is highlighted in bold font.Even though MaxEnt achieved the best results dur-ing development, we decided to use SVM with aRBF kernel for the test set, assuming that it wouldbe able to exploit interdependencies between fea-tures.
The accuracy achieved by the submittedsystem is highlighted in italics in the table.
Thetraining test data provided for restaurants and lap-tops categories were split equally into two setswhere the first set (first half) was used for traininga model and the second set was used for the testand evaluation stages.
Experiments on the restau-rants data produced similar results.classifier sg ex lv ml st AccMaxEnt + + ?
?
?
0.5589MaxEnt + + + ?
?
0.4905MaxEnt + + ?
+ ?
0.5479MaxEnt + + ?
?
+ 0.6506MaxEnt + + ?
+ + 0.5742SVMrbf+ + ?
?
?
0.5581SVMrbf+ + + ?
?
0.4905SVMrbf+ + ?
+ ?
0.5479SVMrbf+ + ?
?
+ 0.6402SVMrbf+ + ?
+ + 0.5717Table 4: Results for laptops category on train set.6 Aspect category detectionSubtask 3 deals with determining which aspectcategories out of a predefined set occur in a givensentence.
The developed module consists of two581independent parts ?
one based on machine learn-ing, the other on similarities between WordNetsynsets (?synonym sets?, roughly correspondingto concepts).
While both approaches achievedsimilar performance during development, combin-ing them resulted in overall better scores.
How-ever, the success of this method crucially dependson accurate indentification of aspect terms.6.1 A WordNet-based approach4The WordNet-based component operates on previ-ously identified aspect terms (from the gold stan-dard in the evaluation, but from the module de-scribed in Sec.
4 in a real application setting).For each term, it finds all synsets and comparesthem to a list of ?key synsets?
that characterizethe different aspect categories (e.g.
the categoryfood is characterized by the key synset meal.n.01,among others).
The best match is chosen andadded to an internal lexicon, which maps eachunique phrase appearing as an aspect term to ex-actly one aspect category.
As a similarity measurefor synsets we used path similarity, which deter-mines the length of the shortest path between twosynsets in the WordNet hypernym/hyponym tax-onomy.
Key synsets were extracted from a list ofhigh frequency terms and tested manually to cre-ate an accurate represenation for each category.In the combined approach this component wastaken as a foundation and was augmented by high-confidence suggestions from the machine learningcomponent (see below).Additional extensions include a high-confidence lexicon based on nearest neighboursfrom a distributional semantic model, a rudi-mentary lexicon of international dishes, andthe application of a spellchecker; together, theyaccounted only for a small increase in F-score onthe development data (from 0.758 to 0.768).6.2 A machine learning approach5The machine learning component is essentially abasic bag-of-words model.
It employs a multino-mial Naive Bayes classifier in a one-vs-all setupto achieve multi-label classification.
In additionto tuning of the smoothing parameters, a probabil-ity threshold was introduced that every predictedcategory has to pass to be assigned to a sentence.4We used the version of WordNet included in NLTK 2.0.4(Bird et al., 2009), accessed through the NLTK API.5We used machine learning algorithms implemented inscikit-learn 0.14.1 (Pedregosa et al., 2011).Test Train AT Mode F1SE14 Dev Sub1 All 0.782SE14 Dev* Sub1 WN 0.666SE14 Dev Sub1* ML 0.788SE14 Dev Gold All 0.848SE14 Dev* Gold WN 0.829SE14 Dev Gold* ML 0.788Dev (cv) Dev Gold All 0.800Dev (cv) Dev* Gold WN 0.768Dev (cv) Dev Gold* ML 0.769*indicates data sets not used by a given componentTable 5: Aspect category detection results.Different thresholds were used for the stand-alonecomponent (th = 0.625) and the combined ap-proach (th = 0.9).
In the latter case all predici-tions of the WordNet-based component were ac-cepted, but only high-confidence predictions fromthe Naive Bayes classifier were added.6.3 ResultsTable 5 summarizes the results of different exper-iments with aspect category detection.
In all casesthe training data consisted of the combined officialtrain and trial sets (Dev).
The last three rows showresults obtained by ten-fold cross-validation in thedevelopment phase, the other rows show the cor-responding results on the official test set (SE14).The first three rows are based on automatically de-tected aspect terms from the module described inSec.
4 (Sub 1), the other rows used gold standardaspect terms.
Separate results are provided for thecombined approach (Mode: All) as well as for thetwo individual components (WN = WordNet, ML= machine learning).
Note that the WN compo-nent does not require any training data, while theML component does not make use of aspect termsmarked in the input.With gold standard aspect terms, the WordNet-based approach is equal to or better than the NaiveBayes classifier, and best results are achived by acombination of the two components.
However, thepoor accuracy of the automatic aspect term extrac-tion (cf.
Table 3) has a disastrous effect: even thecombined approach used in the official submis-sion performs less well than the ML componentalone.
Nevertheless the experiment with gold stan-dard aspect terms suggests that the matching fromaspect term to category works quite well, witha small additional improvement from the Naive582Bayes bag-of-words model.7 Aspect category polarityThe general approach was to allocate each aspectterm to the corresponding aspect categories.
Asimple rule set was then used to determine the po-larity of each aspect category based on the polari-ties of the aligned aspect terms.
In cases where noaspect terms are marked (but sentences are still la-belled with aspect categories), the idea was to fallback on the sentiment values for the entire sen-tences provided by the CoreNLP suite.67.1 Term / category alignmentTo establish a basis for creating the mapping rules,the first step was to work out the distribution ofaspect terms and aspect categories in the train-ing data.
The most common case is that an as-pect category aligns with a single aspect term(1476?
); there are also many aspect categorieswith multiple aspect terms (1179?)
and some as-pect categories without any aspect terms.
Sincethe WordNet-Approach from Sec.
6 showed rela-tively good results (especially if gold standard as-pect terms are available, which is the case here),a modified version was used to assign each aspectterm to one of the annotated categories.7.2 Polarity allocationAfter the assignment of aspect terms to their ac-cording aspect category ?
if needed ?
the aspectcategory polarity can be determined.
For this, thepolarity values of all aspect terms assigned to thiscategory were collected, and duplicates were re-moved in order to produce a unique set (e.g.
1,1, -1, 0, 0 would be reduced to 1, -1, 0).
A setwith both negative and positive polarity values in-dicates a conflict for the corresponding aspect cat-egory, while a neutral polarity value would be ig-nored, if positive or negative polarity values occur.Our method achieved an accuracy of 89.16% forsentences annotated with just a single aspect cat-egory.
In cases where only one aspect term hadbeen assigned to a aspect category the accuracywas unsuprisingly high (96.61%), whereas the ac-curacy decreased in cases of multiple assigned as-pect terms (78.44%).
For aspect categories with-out aligned aspect terms, as well as the categoryanecdotes/miscellaneous, the sentiment values ofthe CoreNLP sentiment analysis tool had to be6http://nlp.stanford.edu/software/corenlp.shtmlused, which led to a poor accuracy in those cases,namely 52.74%.7.3 ResultsOn the official test set, the module for subtask 4achieved an accuracy of 69.56%.
An importantfactor is the very low accuracy in cases where theCoreNLP sentiment value for the entire sentencehad to be used.
We expect a considerable improve-ment from using a modified version of the subtask2 module (Sec.
5) to compute overall sentence po-larity.8 ConclusionWe have shown a modular system working as apipeline that modifies the input sentences step bystep by adding new information as XML tags.Aspect term extraction was handled as a taggingtask that utilized an IOB tagset to find aspectterms with the final version relying on Schmid?sRFTagger.
Determination of aspect term polar-ity was achieved through a machine learning ap-proach that uses SVMs with RBF kernel.
Thiswas supported by an augmented sentiment lexiconbased on several different sources, which was ex-panded manually by a team of students.
Aspectcategory detection in turn employs a combinationapproach of an algorithm depending on WordNetsynsets and a bag-of-words Naive Bayes classifier.Finally aspect category polarity was calculated bycombining the results from the last two modules.Overall results were satisfactory, being mostlyin the top half of submitted systems.
During phaseA of testing (subtasks 1 and 3), a preprocessing er-ror caused a massive drop in performance in aspectterm extraction.
This carried over to the other sub-task, because the module uses aspect terms amongother features to identify aspect categories.
Scoresfor phase B (subtasks 2 and 4) were very close totest results during development with the exceptionof cases where the CoreNLP sentiment value foran entire sentence had to be used.ReferencesSteven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media.Eric Brill.
1992.
A simple rule-based part of speechtagger.
In Proceedings of the Third Conference onApplied Natural Language Processing, pages 152?155, Trento, Italy.583Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the 10thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining (KDD ?04), pages168?177, Seattle, WA.Finn?Arup Nielsen.
2011.
A new ANEW: Evaluationof a word list for sentiment analysis in microblogs.In Proceedings of the ESWC2011Workshop onMak-ing Sense of Microposts: Big things come in smallpackages, number 718 in CEUR Workshop Proceed-ings, pages 93?98, Heraklion, Greece, May.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and?Edouard Duchesnay.
2011.Scikit-learn: Machine learning in Python.
Journalof Machine Learning Research, 12:2825?2830.Maria Pontiki, Dimitrios Galanis, John Pavlopou-los, Harris Papageorgiou, Ion Androutsopoulos, andSuresh Manandhar.
2014.
SemEval-2014 task 4:Aspect based sentiment analysis.
In Proceedings ofthe 8th International Workshop on Semantic Evalu-ation (SemEval 2014), Dublin, Ireland.Thomas Proisl, Paul Greiner, Stefan Evert, and BesimKabashi.
2013.
KLUE: Simple and robust meth-ods for polarity classification.
In Second JointConference on Lexical and Computational Seman-tics (*SEM), Volume 2: Proceedings of the Sev-enth International Workshop on Semantic Evalua-tion (SemEval 2013), pages 395?401, Atlanta, Geor-gia, USA, June.
Association for Computational Lin-guistics.Helmut Schmid and Florian Laws.
2008.
Estimationof conditional probabilities with decision trees andan application to fine-grained POS tagging.
In Pro-ceedings of the 22nd International Conference onComputational Linguistics, volume 1, pages 777?784.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the Hu-man Language Technology Conference and Confer-ence on Empirical Methods in Natural LanguageProcessing (HLT-EMNLP 2005), pages 347?354,Vancouver, BC, Canada.584
