Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 502?507,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAutomatically Predicting Peer-Review HelpfulnessWenting XiongUniversity of PittsburghDepartment of Computer SciencePittsburgh, PA, 15260wex12@cs.pitt.eduDiane LitmanUniversity of PittsburghDepartment of Computer Science &Learning Research and Development CenterPittsburgh, PA, 15260litman@cs.pitt.eduAbstractIdentifying peer-review helpfulness is an im-portant task for improving the quality of feed-back that students receive from their peers.
Asa first step towards enhancing existing peer-review systems with new functionality basedon helpfulness detection, we examine whetherstandard product review analysis techniquesalso apply to our new context of peer reviews.In addition, we investigate the utility of in-corporating additional specialized features tai-lored to peer review.
Our preliminary resultsshow that the structural features, review uni-grams and meta-data combined are useful inmodeling the helpfulness of both peer reviewsand product reviews, while peer-review spe-cific auxiliary features can further improvehelpfulness prediction.1 IntroductionPeer reviewing of student writing has been widelyused in various academic fields.
While existingweb-based peer-review systems largely save instruc-tors effort in setting up peer-review assignments andmanaging document assignment, there still remainsthe problem that the quality of peer reviews is of-ten poor (Nelson and Schunn, 2009).
Thus to en-hance the effectiveness of existing peer-review sys-tems, we propose to automatically predict the help-fulness of peer reviews.In this paper, we examine prior techniques thathave been used to successfully rank helpfulness forproduct reviews, and adapt them to the peer-reviewdomain.
In particular, we use an SVM regression al-gorithm to predict the helpfulness of peer reviewsbased on generic linguistic features automaticallymined from peer reviews and students?
papers, plusspecialized features based on existing knowledgeabout peer reviews.
We not only demonstrate thatprior techniques from product reviews can be suc-cessfully tailored to peer reviews, but also show theimportance of peer-review specific features.2 Related WorkPrior studies of peer review in the Natural Lan-guage Processing field have not focused on help-fulness prediction, but instead have been concernedwith issues such as highlighting key sentences in pa-pers (Sandor and Vorndran, 2009), detecting impor-tant feedback features in reviews (Cho, 2008; Xiongand Litman, 2010), and adapting peer-review assign-ment (Garcia, 2010).
However, given some simi-larity between peer reviews and other review types,we hypothesize that techniques used to predict re-view helpfulness in other domains can also be ap-plied to peer reviews.
Kim et al (2006) used re-gression to predict the helpfulness ranking of prod-uct reviews based on various classes of linguisticfeatures.
Ghose and Ipeirotis (2010) further exam-ined the socio-economic impact of product reviewsusing a similar approach and suggested the useful-ness of subjectivity analysis.
Another study (Liuet al, 2008) of movie reviews showed that helpful-ness depends on reviewers?
expertise, their writingstyle, and the timeliness of the review.
Tsur andRappoport (2009) proposed RevRank to select themost helpful book reviews in an unsupervised fash-ion based on review lexicons.
However, studies ofAmazon?s product reviews also show that the per-502Class Label FeaturesStructural STR review length in terms of tokens, number of sentences, percentage of sentencesthat end with question marks, number of exclamatory sentences.Lexical UGR, BGR tf-idf statistics of review unigrams and bigrams.Syntactic SYNpercentage of tokens that are nouns, verbs, verbs conjugated in thefirst person, adjectives / adverbs and open classes, respectively.SemanticTOP, counts of topic words,posW, negW counts of positive and negative sentiment words.Meta-data METthe overall ratings of papers assigned by reviewers, and the absolutedifference between the rating and the average score given by all reviewers.Table 1: Generic features motivated by related work of product reviews (Kim et al, 2006).ceived helpfulness of a review depends not only onits review content, but also on social effects such asproduct qualities, and individual bias in the presenceof mixed opinion distribution (Danescu-Niculescu-Mizil et al, 2009).Nonetheless, several properties distinguish ourcorpus of peer reviews from other types of reviews:1) The helpfulness of our peer reviews is directlyrated using a discrete scale from one to five insteadof being defined as a function of binary votes (e.g.the percentage of ?helpful?
votes (Kim et al, 2006));2) Peer reviews frequently refer to the related stu-dents?
papers, thus review analysis needs to take intoaccount paper topics; 3) Within the context of edu-cation, peer-review helpfulness often has a writingspecific semantics, e.g.
improving revision likeli-hood; 4) In general, peer-review corpora collectedfrom classrooms are of a much smaller size com-pared to online product reviews.
To tailor existingtechniques to peer reviews, we will thus proposenew specialized features to address these issues.3 Data and FeaturesIn this study, we use a previously annotated peer-review corpus (Nelson and Schunn, 2009; Patchanet al, 2009), collected using a freely available web-based peer-review system (Cho and Schunn, 2007)in an introductory college history class.
The corpusconsists of 16 papers (about six pages each) and 267reviews (varying from twenty words to about twohundred words).
Two experts (a writing instructorand a content instructor) (Patchan et al, 2009) wereasked to rate the helpfulness of each peer reviewon a scale from one to five (Pearson correlationr = 0.425, p < 0.01).
For our study, we considerthe average ratings given by the two experts (whichroughly follow a normal distribution) as the goldstandard of review helpfulness.
Two example ratedpeer reviews (shown verbatim) follow:A helpful peer review of average-rating 5:The support and explanation of the ideas could usesome work.
broading the explanations to include allgroups could be useful.
My concerns come from someof the claims that are put forth.
Page 2 says that the13th amendment ended the war.
is this true?
was thereno more fighting or problems once this amendment wasadded?
...The arguments were sorted up into paragraphs,keeping the area of interest clear, but be careful aboutbringing up new things at the end and then simply leavingthem there without elaboration (ie black sterilization atthe end of the paragraph).An unhelpful peer review of average-rating 1:Your paper and its main points are easy to find and tofollow.As shown in Table 1, we first mine genericlinguistic features from reviews and papers basedon the results of syntactic analysis of the texts,aiming to replicate the feature sets used by Kim etal.
(2006).
While structural, lexical and syntacticfeatures are created in the same way as suggestedin their paper, we adapt the semantic and meta-datafeatures to peer reviews by converting the mentionsof product properties to mentions of the historytopics and by using paper ratings assigned by peersinstead of product scores.11We used MSTParser (McDonald et al, 2005) for syntacticanalysis.
Topic words are automatically extracted from all stu-503In addition, the following specialized features aremotivated by an empirical study in cognitive sci-ence (Nelson and Schunn, 2009), which suggeststhat students?
revision likelihood is significantly cor-related with certain feedback features, and by ourprior work (Xiong and Litman, 2010; Xiong etal., 2010) for detecting these cognitive science con-structs automatically:Cognitive-science features (cogS): For a givenreview, cognitive-science constructs that are signifi-cantly correlated with review implementation likeli-hood are manually coded for each idea unit (Nel-son and Schunn, 2009) within the review.
Note,however, that peer-review helpfulness is rated forthe whole review, which can include multiple ideaunits.2 Therefore in our study, we calculate the dis-tribution of feedbackType values (praise, problem,and summary) (kappa = .92), the percentage ofproblems that have problem localization ?the pres-ence of information indicating where the problem islocalized in the related paper?
(kappa = .69), andthe percentage of problems that have a solution ?the presence of a solution addressing the problemmentioned in the review?
(kappa = .79) to modelpeer-review helpfulness.
These kappa values (Nel-son and Schunn, 2009) were calculated from a sub-set of the corpus for evaluating the reliability of hu-man annotations3.
Consider the example of the help-ful review presented in Section 3 which was manu-ally separated into two idea units (each presented ina separate paragraph).
As both ideas are coded asproblem with the presence of problem localizationand solution, the cognitive-science features of thisreview are praise%=0, problem%=1, summary%=0,localization%=1, and solution%=1.Lexical category features (LEX2): Ten cate-gories of keyword lexicons developed for automat-ically detecting the previously manually annotatedfeedback types (Xiong et al, 2010).
The categoriesare learned in a semi-supervised way based on syn-tactic and semantic functions, such as suggestiondents?
papers using topic signature (Lin and Hovy, 2000) soft-ware kindly provided by Annie Louis.
Positive and negativesentiment words are extracted from the General Inquirer Dic-tionaries (http://www.wjh.harvard.edu/ inquirer/homecat.htm).2Details of different granularity levels of annotation can befound in (Nelson and Schunn, 2009).3These annotators are not the same experts who rated thepeer-review helpfulness.modal verbs (e.g.
should, must, might, could, need),negations (e.g.
not, don?t, doesn?t), positive and neg-ative words, and so on.
We first manually createda list of words that were specified as signal wordsfor annotating feedbackType and problem localiza-tion in the coding manual; then we supplementedthe list with words selected by a decision tree modellearned using a Bag-of-Words representation of thepeer reviews.
These categories will also be helpfulfor reducing the feature space size as discussed be-low.Localization features (LOC): Five features de-veloped in our prior work (Xiong and Litman, 2010)for automatically identifying the manually codedproblem localization tags, such as the percentage ofproblems in reviews that could be matched with alocalization pattern (e.g.
?on page 5?, ?the sectionabout?
), the percentage of sentences in which topicwords exist between the subject and object, etc.4 Experiment and ResultsFollowing Kim et al (2006), we train our helpful-ness model using SVM regression with a radial ba-sis function kernel provided by SVMlight (Joachims,1999).
We first evaluate each feature type in iso-lation to investigate its predictive power of peer-review helpfulness; we then examine them togetherin various combinations to find the most useful fea-ture set for modeling peer-review helpfulness.
Per-formance is evaluated in 10-fold cross validationof our 267 peer reviews by predicting the absolutehelpfulness scores (with Pearson correlation coeffi-cient r) as well as by predicting helpfulness rank-ing (with Spearman rank correlation coefficient rs).Although predicted helpfulness ranking could be di-rectly used to compare the helpfulness of a given setof reviews, predicting helpfulness rating is desirablein practice to compare helpfulness between existingreviews and new written ones without reranking allpreviously ranked reviews.
Results are presented re-garding the generic features and the specialized fea-tures respectively, with 95% confidence bounds.4.1 Performance of Generic FeaturesEvaluation of the generic features is presented inTable 2, showing that all classes except syntac-tic (SYN) and meta-data (MET) features are sig-504nificantly correlated with both helpfulness rating(r) and helpfulness ranking (rs).
Structural fea-tures (bolded) achieve the highest Pearson (0.60)and Spearman correlation coefficients (0.59) (al-though within the significant correlations, the dif-ference among coefficients are insignificant).
Notethat in isolation, MET (paper ratings) are not sig-nificantly correlated with peer-review helpfulness,which is different from prior findings of product re-views (Kim et al, 2006) where product scores aresignificantly correlated with product-review help-fulness.
However, when combined with other fea-tures, MET does appear to add value (last row).When comparing the performance between predict-ing helpfulness ratings versus ranking, we observer ?
rs consistently for our peer reviews, while Kimet al (2006) reported r < rs for product reviews.4Finally, we observed a similar feature redundancyeffect as Kim et al (2006) did, in that simply com-bining all features does not improve the model?s per-formance.
Interestingly, our best feature combina-tion (last row) is the same as theirs.
In sum ourresults verify our hypothesis that the effectivenessof generic features can be transferred to our peer-review domain for predicting review helpfulness.Features Pearson r Spearman rsSTR 0.60?
0.10* 0.59?
0.10*UGR 0.53?
0.09* 0.54?
0.09*BGR 0.58?
0.07* 0.57?
0.10*SYN 0.36?
0.12 0.35?
0.11TOP 0.55?
0.10* 0.54?
0.10*posW 0.57?
0.13* 0.53?
0.12*negW 0.49?
0.11* 0.46?
0.10*MET 0.22?
0.15 0.23?
0.12All-combined 0.56?
0.07* 0.58?
0.09*STR+UGR+MET0.61?
0.10* 0.61?
0.10*+TOPSTR+UGR+MET 0.62?
0.10* 0.61?
0.10*Table 2: Performance evaluation of the generic featuresfor predicting peer-review helpfulness.
Significant resultsare marked by * (p ?
0.05).4.2 Analysis of the Specialized FeaturesEvaluation of the specialized features is shown inTable 3, where all features examined are signifi-4The best performing single feature type reported (Kim etal., 2006) was review unigrams: r = 0.398 and rs = 0.593.cantly correlated with both helpfulness rating andranking.
When evaluated in isolation, althoughspecialized features have weaker correlation coeffi-cients ([0.43, 0.51]) than the best generic features,these differences are not significant, and the special-ized features have the potential advantage of beingtheory-based.
The use of features related to mean-ingful dimensions of writing has contributed to va-lidity and greater acceptability in the related area ofautomated essay scoring (Attali and Burstein, 2006).When combined with some generic features, thespecialized features improve the model?s perfor-mance in terms of both r and rs compared tothe best performance in Section 4.1 (the baseline).Though the improvement is not significant yet, wethink it still interesting to investigate the potentialtrend to understand how specialized features cap-ture additional information of peer-review helpful-ness.
Therefore, the following analysis is also pre-sented (based on the absolute mean values), wherewe start from the baseline feature set, and graduallyexpand it by adding our new specialized features:1) We first replace the raw lexical unigram features(UGR) with lexical category features (LEX2), whichslightly improves the performance before roundingto the significant digits shown in row 5.
Note thatthe categories not only substantially abstract lexicalinformation from the reviews, but also carry simplesyntactic and semantic information.
2) We then addone semantic class ?
topic words (row 6), which en-hances the performance further.
Semantic featuresdid not help when working with generic lexical fea-tures in Section 4.1 (second to last row in Table 2),but they can be successfully combined with the lexi-cal category features and further improve the perfor-mance as indicated here.
3) When cognitive-scienceand localization features are introduced, the predic-tion becomes even more accurate, which reaches aPearson correlation of 0.67 and a Spearman correla-tion of 0.67 (Table 3, last row).5 DiscussionDespite the difference between peer reviews andother types of reviews as discussed in Section 2,our work demonstrates that many generic linguisticfeatures are also effective in predicting peer-reviewhelpfulness.
The model?s performance can be alter-505Features Pearson r Spearman rscogS 0.43?
0.09 0.46?
0.07LEX2 0.51?
0.11 0.50?
0.10LOC 0.45?
0.13 0.47?
0.11STR+MET+UGR0.62?
0.10 0.61?
0.10(Baseline)STR+MET+LEX2 0.62?
0.10 0.61?
0.09STR+MET+LEX2+0.65?
0.10 0.66?
0.08TOPSTR+MET+LEX2+0.66?
0.09 0.66?
0.08TOP+cogSSTR+MET+LEX2+0.67?
0.09 0.67?
0.08TOP+cogS+LOCTable 3: Evaluation of the model?s performance (all sig-nificant) after introducing the specialized features.natively achieved and further improved by addingauxiliary features tailored to peer reviews.
Thesespecialized features not only introduce domain ex-pertise, but also capture linguistic information at anabstracted level, which can help avoid the risk ofover-fitting.
Given only 267 peer reviews in ourcase compared to more than ten thousand productreviews (Kim et al, 2006), this is an important con-sideration.Though our absolute quantitative results arenot directly comparable to the results of Kim etal.
(2006), we indirectly compared them by ana-lyzing the utility of features in isolation and com-bined.
While STR+UGR+MET is found as the bestcombination of generic features for both types ofreviews, the best individual feature type is differ-ent (review unigrams work best for product reviews;structural features work best for peer reviews).
Moreimportantly, meta-data, which are found to signif-icantly affect the perceived helpfulness of productreviews (Kim et al, 2006; Danescu-Niculescu-Mizilet al, 2009), have no predictive power for peer re-views.
Perhaps because the paper grades and otherhelpfulness ratings are not visible to the reviewers,we have less of a social dimension for predictingthe helpfulness of peer reviews.
We also found thatSVM regression does not favor ranking over predict-ing helpfulness as in (Kim et al, 2006).6 Conclusions and Future WorkThe contribution of our work is three-fold: 1) Ourwork successfully demonstrates that techniques usedin predicting product review helpfulness ranking canbe effectively adapted to the domain of peer reviews,with minor modifications to the semantic and meta-data features.
2) Our qualitative comparison showsthat the utility of generic features (e.g.
meta-datafeatures) in predicting review helpfulness varies be-tween different review types.
3) We further showthat prediction performance could be improved byincorporating specialized features that capture help-fulness information specific to peer reviews.In the future, we would like to replace the man-ually coded peer-review specialized features (cogS)with their automatic predictions, since we have al-ready shown in our prior work that some impor-tant cognitive-science constructs can be successfullyidentified automatically.5 Also, it is interesting toobserve that the average helpfulness ratings assignedby experts (used as the gold standard in this study)differ from those given by students.
Prior work onthis corpus has already shown that feedback fea-tures of review comments differ not only betweenstudents and experts, but also between the writingand the content experts (Patchan et al, 2009).
WhilePatchan et al (2009) focused on the review com-ments, we hypothesize that there is also a differencein perceived peer-review helpfulness.
Therefore, weare planning to investigate the impact of these dif-ferent helpfulness ratings on the utilities of featuresused in modeling peer-review helpfulness.
Finally,we would like to integrate our helpfulness modelinto a web-based peer-review system to improve thequality of both peer reviews and paper revisions.AcknowledgementsThis work was supported by the Learning Researchand Development Center at the University of Pitts-burgh.
We thank Melissa Patchan and Christian D.Schunn for generously providing the manually an-notated peer-review corpus.
We are also grateful toChristian D. Schunn, Janyce Wiebe, Joanna Drum-mond, and Michael Lipschultz who kindly gave usvaluable feedback while writing this paper.5The accuracy rate is 0.79 for predicting feedbackType, 0.78for problem localization, and 0.81 for solution on the same his-tory data set.506ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-rater v.2.
In Michael Russell, editor,The Journal of Technology, Learning and Assessment(JTLA), volume 4, February.Kwangsu Cho and Christian D. Schunn.
2007.
Scaf-folded writing and rewriting in the discipline: A web-based reciprocal peer review system.
In Computersand Education, volume 48, pages 409?426.Kwangsu Cho.
2008.
Machine classification of peercomments in physics.
In Proceedings of the First In-ternational Conference on Educational Data Mining(EDM2008), pages 192?196.Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,Jon Kleinber g, and Lillian Lee.
2009.
How opin-ions are received by online communities: A case studyon Amazon .com helpfulness votes.
In Proceedings ofWWW, pages 141?150.Raquel M. Crespo Garcia.
2010.
Exploring documentclustering techniques for personalized peer assessmentin exploratory courses.
In Proceedings of Computer-Supported Peer Review in Education (CSPRED) Work-shop in the Tenth International Conference on Intelli-gent Tutoring Systems (ITS 2010).Anindya Ghose and Panagiotis G. Ipeirotis.
2010.
Esti-mating the helpfulness and economic impact of prod-uct reviews: Mining text and reviewer characteristics.In IEEE Transactions on Knowledge and Data Engi-neering, volume 99, Los Alamitos, CA, USA.
IEEEComputer Society.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In B. Scho?lkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods - Sup-port Vector Learning.
MIT Press, Cambridge, MA,USA.Soo-Min Kim, Patrick Pantel, Tim Chklovski, and MarcoPennacchiotti.
2006.
Automatically assessing reviewhelpfulness.
In Proceedings of the 2006 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP2006), pages 423?430, Sydney, Australia,July.Chin-Yew Lin and Eduard Hovy.
2000.
The auto-mated acquisition of topic signatures for text summa-rization.
In Proceedings of the 18th conference onComputational linguistics, volume 1 of COLING ?00,pages 495?501, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Yang Liu, Xiangji Guang, Aijun An, and Xiaohui Yu.2008.
Modeling and predicting the helpfulness of on-line reviews.
In Proceedings of the Eighth IEEE Inter-national Conference on Data Mining, pages 443?452,Los Alamitos, CA, USA.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 91?98, Stroudsburg, PA, USA.
Association forComputational Linguistics.Melissa M. Nelson and Christian D. Schunn.
2009.
Thenature of feedback: how different types of peer feed-back affect writing performance.
In Instructional Sci-ence, volume 37, pages 375?401.Melissa M. Patchan, Davida Charney, and Christian D.Schunn.
2009.
A validation study of students?
endcomments: Comparing comments by students, a writ-ing instructor, and a content instructor.
In Journal ofWriting Research, volume 1, pages 124?152.
Univer-sity of Antwerp.Agnes Sandor and Angela Vorndran.
2009.
Detect-ing key sentences for automatic assistance in peer-reviewing research articles in educational sciences.
InProceedings of the 47th Annual Meeting of the Associ-ation for Computational Linguistics and the 4th Inter-national Joint Conference on Natural Language Pro-cessing of the Asian Federation of Natural LanguageProcessing (ACL-IJCNLP), pages 36?44.Oren Tsur and Ari Rappoport.
2009.
Revrank: A fullyunsupervised algorithm for selecting the most helpfulbook reviews.
In Proceedings of the Third Interna-tional AAAI Conference on Weblogs and Social Media(ICWSM2009), pages 36?44.Wenting Xiong and Diane J. Litman.
2010.
Identifyingproblem localization in peer-review feedback.
In Pro-ceedings of Tenth International Conference on Intelli-gent Tutoring Systems (ITS2010), volume 6095, pages429?431.Wenting Xiong, Diane J. Litman, and Christian D.Schunn.
2010.
Assessing reviewers performancebased on mining problem localization in peer-reviewdata.
In Proceedings of the Third International Con-ference on Educational Data Mining (EDM2010),pages 211?220.507
