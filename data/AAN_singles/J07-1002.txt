Squibs and DiscussionsIdentifying Sources of Disagreement:Generalizability Theory in ManualAnnotation StudiesPetra Saskia Bayerl?University of OklahomaKarsten Ingmar Paul?University of Erlangen-NurembergMany annotation projects have shown that the quality of manual annotations often is not asgood as would be desirable for reliable data analysis.
Identifying the main sources responsiblefor poor annotation quality must thus be a major concern.
Generalizability theory is a valuabletool for this purpose, because it allows for the differentiation and detailed analysis of factors thatinfluence annotation quality.
In this article we will present basic concepts of GeneralizabilityTheory and give an example for its application based on published data.1.
IntroductionManual annotations are still a major source of information in many small- and large-scale projects in diverse areas of corpus and computational linguistics.
Often, however,manual annotations are not reliable enough for a given application.
Measures mustbe taken to increase and secure the consistency of linguistic annotations, if analysesand applications are not to suffer from low data quality.
Because a multitude of fac-tors may be responsible for inadequate reliability, a method is needed that is able tosimultaneously consider a variety of probable factors and indicate those that are mainlyresponsible for low reliability in a given case.
Generalizability Theory, or G-Theory(Cronbach et al 1972), is a methodological framework specifically designed for thispurpose.
Because it is not restricted to any type of data or study design, it can be ofgreat use in any kind of manual annotation project that needs to systematically identifysources of annotator disagreement.
In this article we provide an outline of the approachand its basic assumptions and demonstrate its application based on an annotation studydone by Shriberg and Lof (1991).1?
Department of Psychology, Tulsa Graduate College, 4502 East 41st Street, Tulsa, OK 74135, USA.?
Chair of Psychology, especially Organizational and Social Psychology (Prof. Dr. Moser), Lange Gasse 20,90403 Nuremberg, Germany.1 A more comprehensive introduction to G-Theory is provided in a longer version of this article, which isavailable from the authors.The work for this article was done at the Department for Applied and Computational Linguistics,Justus-Liebig-University Giessen, Germany.?
2007 Association for Computational LinguisticsComputational Linguistics Volume 33, Number 12.
The G-Theory ApproachReliability in G-Theory is defined by the amount of variation or variance observed inannotations; the lower the total variance in the data, the higher is its reliability.
G-Theoryfurther assumes that data reliability is influenced by several independent factors orfacets, which are, individually as well as in interaction, responsible for the observedvariation.2 Sources of variation might be idiosyncratic behaviors of individual annota-tors or external influences like alterations in the tools used for annotations, increasingtime pressure, removal or adding of rewards, or changes in the annotation scheme.
Eachof these influences can lead to systematic changes in an annotator?s behavior and soto higher disagreement among annotators.
According to G-Theory each possible facet,annotator, tools, rewards, and so on, will have its own independent impact on the quality,that is, reliability, of annotations.
The task of a G-study is to isolate the influence ofsingle facets and determine the degree of their impact.2.1 Basic G-Study DesignsThe main distinction with respect to G-study designs is the choice between a crossedand a (partially) nested design.
In crossed designs measurements are obtained foreach possible combination of facet values.
Given two facets, items and coders, eachindividual item (phrase, phone, gesture, etc.)
is annotated by all possible coders, sothat each value of the item facet is measured on every value of the coders facet.
Nesteddesigns, in contrast, only measure a subset of possible combinations of facet values,for instance, when limited resources determine that only some of the coders annotatethe same objects on more than one occasion.
In general, fully crossed designs requirea higher number of observations, but also provide more information.
To obtain a fullpicture of possible influences crossed designs should therefore be preferred.
For adetailed discussion of G-study designs, including unbalanced designs or missing data,and random and fixed facets, see, for example, Brennan (2001).2.2 Estimating Variance ComponentsIn fully crossed designs the total variance in the data is a result of individual facets aswell as their interactions.
Because G-Theory assumes independence of facets, effects ofcomponents are additive.
Given three facets a, b, c, the total variance ?2(Xabc) thereforeis calculated as?2(Xabc) = ?2a + ?2b + ?2c + ?2ab + ?2ac + ?2bc + ?2abc,e (1)where ?2 refers to variance and the subscripts to the name of one or more facets.The subscript e in the last variance component denotes error variance.
In nested de-signs some facets cannot be determined as independent terms due to their confoundingwith other facets.
For instance, in a nested design with three factors a, b, c, different2 This definition of reliability differs from the traditional true-score-model of classical reliability theory(Spearman 1904) and can be considered a modern approach to the question of consistency or?dependability?
of data measurement.
A discussion of the conceptual differences is beyond the scopeof this article.
Information on this topic may be found in Thompson (2002) or Matt (2001).4Bayerl and Paul Identifying Sources of Disagreementvalues of c may be associated with different values of b.
Here the effect for c will beconfounded both with bc and the residual term abc,e so that no independent term forthe c facet can be obtained.
Instead of the seven variance components in the crosseddesign only five variance components can be calculated, again stressing the fact thatnested designs provide less information than fully crossed designs?2(Xabc) = ?2a + ?2b + ?2ab + ?2c,cb + ?2ac,abc,e (2)For information on the mathematical foundation of G-Theory and the derivation ofestimates see Cronbach et al (1972) and Brennan (2001).2.3 Interpreting Variance ComponentsBased on the assumption that the total variance is a sum of single variance components,the total variance is 100%.
The relative magnitude of each component with respect tothe total variance is an indicator of the individual contribution of this component withrespect to overall (un)reliability.
A facet explaining 60% of the total variance would thusbe considered a major source of variation in contrast to a minor facet explaining only 5%of the variance.
For instance, given that the coder facet is the largest facet, variation canbe explained through systematic differences in the annotation behavior of individualcoders?for example, annotators differ in their tendency to set prosodic boundaries inutterances leading to systematic differences in the number of boundaries placed.
In thiscase retraining of annotators to reach a more comparable behavior would be advisable.A high schema component indicates that there is systematic variation in the use ofcategories, whereas a high coder?schema interaction indicates systematic differences inannotators?
use of these categories; for example, coders annotating rhetorical (RST)relations could differ in the frequency with which they use individual relations such as?background?, ?concession?, ?evidence?, and so forth, pointing to possible problems withthe interpretation of rhetorical relations and their application.
Variation mainly due tothe item facet indicates that certain materials are harder to annotate than others.
Such aresult would imply retraining or elimination of overly difficult material.
In consequence,the identification of distinct sources of variation should lead to specifically designedsteps for improvement.3.
A Re-Analysis of Shriberg and Lof (1991)As an illustration for the application of G-Theory we reanalyzed data provided byShriberg and Lof (1991), who studied the accuracy of broad and narrow phonetic tran-scriptions.
In Set A of their study they investigated four facets: annotation scheme (typeof consonant, C), granularity (broad vs. narrow transcription G), material (continuousspeech vs. articulation test, M), and annotation team (T).
Data in Set A were given asagreement percentages.
Our G-study results are shown in Table 1.Traditionally, reliability concerns focus on disagreements among individual anno-tators assuming that variation is due to incommensurable annotator behavior.
In ourcase, however, the team facet explains only a very small percentage of variance bothas an individual factor and in interaction with other factors.
This suggests that thefour annotation teams are comparable in their annotation quality.
The major factorsresponsible for the observed variance are granularity and type of consonants.
Material5Computational Linguistics Volume 33, Number 1Table 1G-Study results for Shriberg and Lof (1991), Table 8, Set A.Effect df Variance Percentage ofcomponents estimates total varianceConsonant (C) 23 234.86877 25.70Granularity (G) 1 312.80278 34.23Team (T) 3 3.70906 0.41Material (M) 1 0.0 [?3.08672]?
?CG 23 99.18526 10.85CT 69 0.0 [?8.25984]?
?CM 23 45.80498 5.01GT 3 0.0 [?1.12263]?
?GM 1 0.0 [?6.05138]?
?TM 3 0.0 [?1.74740]?
?CGT 69 3.84207 0.42CGM 23 111.61108 12.12CTM 69 57.64646 6.31GTM 3 6.04318 0.66CGTM,e 36 38.23065 4.18913.74429 99.99?
Values set zero, original negative estimates in brackets.For the analysis the GENOVA program as described in Brennan (2001) was used.does not exhibit a substantial individual influence on reliability, but becomes relevantin the CGM-interaction.
Our G-study therefore reveals that unreliability in Shriberg andLof?s data is caused not by idiosyncrasies of individuals, but due to the characteristicsof the task, namely, granularity and scheme.Having identified the critical facets, it might now be interesting to look at the valuesof these facets that are especially prone to produce disagreement.
Because we operatedwith agreement data, this information can be easily obtained from the data entered intothe analysis.
Because neither team nor material are major sources for variance, we onlyhave to examine the values for granularity and consonants.
Due to the same reasonwe can base the comparison on mean values over teams and material types.
For thegranularity facet we find overall lower agreement in narrow transcriptions (64.15%)compared to broad transcriptions (89.46%).
On the consonant facet we can differentiatecritical phonemes such as /D/ or /S/ from uncritical ones (e. g., /j/, /b/).
Interpretingthe CG-interaction in this light, disagreement on consonants in narrow transcriptionsseems to be comparably higher than in broad transcriptions.
Implications from thisstudy would be that the selection of annotators and the training of annotation teams aresuccessful in producing comparable results.
For high reliability, however, transcriptionsshould be done on a broad level with specific training for difficult consonants and somespecial care for material from articulation tests (see CGM-interaction).4.
Practical Considerations in Planning G-StudiesIn planning and conducting a G-study some deliberation is necessary to achieve inter-pretable results.
Foremost, the overall quality of the G-study depends on the choice offactors that completely and accurately represent the situation of the annotators.
As it is6Bayerl and Paul Identifying Sources of Disagreementquite easy to overlook relevant but rather inconspicuous factors like minor changes inthe annotation tool or increasing time pressure due to upcoming project deadlines, thechoice of correct facets relies heavily on the experience and knowledge of the researcher.The statistical results, however, will give indications for likely misspecifications of facetsby showing a high error or rest variance ?e for the tested model.
Theoretically, thenumber of facets that can be included in a G-study is unlimited.
Having more than fouror five facets in one study might make the final interpretation overly complex, however.Even though there is no minimum necessary number of observations, missing data dueto a low number of observations pose a problem for model interpretation.
Approachesto deal with such unbalanced designs are given by Brennan (2001) and Chiu and Wolfe(2003).
Additionally, there is no clear-cut rule when a component might be considered?too small?
to be of importance.
As a rule of thumb a component of less than 8% mightbe considered ?small?, but the decision remains one of ?relative importance?
dependingon the distribution of explained variance across components.5.
G-Theory and Agreement IndicesTwo well-known measures for capturing the quality of manual annotations are agree-ment percentages and the kappa statistic (Cohen 1960; Carletta 1996; Eugenio andGlass 2004).
Both measures provide a ?summary index?
(Agresti 1992) that expressesthe degree of (dis)agreement among coders.
Where the calculation of percentages andkappa provides a measure for overall reliability (or reliability indices for individualfacets), G-Theory has been designed to analyze multiple possible influencing factors ina single run and to compare the relative importance of components among each other.Shriberg and Lof (1991), for instance, compare narrow and broad transcriptions usinggraphics that show the agreement percentages for each consonant for both transcrip-tion types, arguing that overall narrow transcriptions seem unreliable.
Based on theirexperience with the study context they further assumed that these differences were notdue to annotator training, behaviors, or experience.
They did not provide any directevidence, however.
The G-study presented in this article could prove both assumptionsin a single run.
It further allows us to investigate the interactions and mutual influencesof these factors, thus clearly exceeding the possibilities of summary statistics.
As wehave seen in the example, G-Theory, however, does not provide answers as to whichvalues of a facet are responsible for higher or lower reliability.
This information mustbe obtained by a review of the data.
Agreement indices and G-Theory should thusnot be seen as competing, but rather as complementary, approaches.
Kappa can serveas a first approximation to the degree of disagreement present in the data, whereasG-Theory in a second step investigates the underlying reasons of inadequate reliabilityand subsequently guides efforts to improve reliability.6.
Final RemarksGeneralizability theory is a valuable approach for identifying problematic areas inannotation projects.
The investigation of multiple facets at the same time can provide aclearer understanding of reasons underlying insufficient annotation quality and subse-quently offer avenues to its improvement.
In this article we could not give more thana passing glance over the possibilities provided by the G-Theory approach.
For theinterested reader, Shavelson and Webb (1981) give a good introduction into the material.Further references are provided throughout the article and in the reference section.7Computational Linguistics Volume 33, Number 1ReferencesAgresti, Alan.
1992.
Modelling patternsof agreement and disagreement.Statistical Methods in Medical Research,1(2):201?218.Brennan, Robert L. 2001.
GeneralizabilityTheory.
Statistics for Social Science andPublic Policy.
Springer Verlag, New York.Carletta, Jean.
1996.
Assessing agreementon classification tasks: The kappastatistic.
Computational Linguistics,22(2):249?254.Chiu, Christopher W. T. and Edward W.Wolfe.
2003.
A method for analyzingsparse data matrices in the generalizabilitytheory framework.
Applied PsychologicalMeasurement, 26(3):321?338.Cohen, Jacob.
1960.
A coefficient ofagreement for nominal scales.
Educationaland Psychological Measurement, 20(1):37?46.Cronbach, Lee J., Goldine C. Gleser, HarinderNanda, and Nageswari Rajaratnam.1972.
The Dependability of BehavioralMeasurements: Theory of Generalizabilityfor Scores and Profiles.
John Wiley & Sons,Inc., New York.Eugenio, Barbara Di and Michael Glass.2004.
The kappa statistic: A second look.Computational Linguistics, 30(1):95?101.Matt, Georg E. 2001.
Generalizabilty theory.In Neil J. Smelser and Paul B. Baltes,editors, International Encyclopedia of theSocial and Behavioral Sciences.
Elsevier,Oxford.Shavelson, Richard J. and Noreen M. Webb.1981.
Generalizability Theory: A Primer.
SagePublications, Newbury Park, London,New Delhi.Shriberg, Lawrence D. and Gregory L. Lof.1991.
Reliability studies in broad andnarrow phonetic transcription.
ClinicalLinguistics and Phonetics, 5(3):225?279.Spearman, Charles.
1904.
The proof andmeasurement of association between twothings.
American Journal of Psychology,15:72?101.Thompson, Bruce.
2002.
A brief introductionto Generalizability Theory.
In BruceThompson, editor, Score Reliability:Contemporary Thinking on Reliability Issues.Sage Publication, Thousand Oaks, CA,pages 43?58.Statistical Software GENOVA, urGENOVA, mGENOVA: Available online athttp://www.education.uiowa.edu/casma/GenovaPrograms.htm8
