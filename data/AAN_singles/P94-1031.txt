Tr ico lo r  DAGs  fo r  Mach ine  Trans la t ionKo ich i  TakedaTokyo  Research  Laboratory ,  IBM Research1623-14 Sh imotsuruma,  Yamato ,  Kanagawa 242, JapanPhone:  81-462-73-4569,  81-462-73-7413 (FAX)t akeda@tr l .vnet .
ibm.comAbstractMachine translation (MT) has recently been for-mulated in terms of constraint-based knowledgerepresentation and unification theories~ but it isbecoming more and more evident that it is notpossible to design a practical MT system withoutan adequate method of handling mismatches be-tween semantic representations in the source andtarget languages.
In this paper, we introduce theidea of "information-based" MT, which is consid-erably more flexible than interlingual MT or theconventional transfer-based MT.In t roduct ionWith the intensive exploration of contemporarytheories on unification grammars\[6, 15, 13\] andfeature structures\[7, 19\] in the last decade, theold image of machine translation (MT) as a bru-tal form of natural anguage processing has givenway to that of a process based on a uniform andreversible architecture\[16~ 1, 27\].The developers of MT systems based on theconstraint-based formalism found a serious prob-lem in "language mismatching," namely, the dif-ference between semantic representations in thesource and target languages.
1 Attempts to de-sign a pure interlingual MT system were thereforeabandoned, 2 and the notion of "semantic trans-fer"\[24, 22\] came into focus as a practical so-lution to the problem of handling the languagemismatching.
The constraint-based formalism\[2\]seemed promising as a formal definition of trans-fer, but pure constraints are too rigid to be pre-cisely imposed on target-language s ntences.Some researchers(e.g., Russell\[14\]) introduced1For example, Yasuhara\[26\] reported there was anoverlap of only 10% between his group's English andJapanese concept dictionaries, which covered 0.2 mil-lion concepts.2Even an MT system with a controlled inputlanguage\[12\] does not claim to be a pure interlingualsystem.the concept of defeasible r asoning in order to for-malize what is missing from a pure constraint-based approach, and control mechanisms for suchreasoning have also been proposed\[5.
3\].
Withthis additional mechanism, we can formulate the"transfer" process as a mapping from a set of con-straints into another set of mandatory and defen-sible constraints.
This idea leads us further to theconcept of "information-based" MT, which meansthat, with an appropriate representation scheme,a source sentence can be represented by a set ofconstraints that it implies and that, given a targetsentence, the set Co of constraints can be dividedinto three disjoint subsets:?
The subset Co of constraints that is also impliedby the target sentence?
The subset C+ of constraints that is not im-plied by, but is consistent with, the translatedsentence?
The subset C-  of constraints that is violated bythe target sentenceThe target sentence may also imply another setC~eto of constraints, none of which is in Ca.
Thatis~ the set Ct of constraints implied by the tar-get sentences i a union of C0 and C~e~o, whileCs = CoUC+UC_.
When Ca = Co = Ct, we havea fully interlingual translation of the source sen-tence.
If C+ ?
?, C_ = ?, and Chew = ?, the tar-get sentence is said to be under-generated~ while itis said to be over-generated when C+ = ?, C-  = ?,and Cacao y~ ?.s In either case, C-  must be emptyif a consistent translation is required.
Thus, thegoal of machine translation is to find an optimalpair of source and target sentences that minimizesC+~C-, and C~w.
Intuitively, Co correspondsto essential information, and C+ and Cneto canbe viewed as language-dependent supportive in-formation.
C_ might be the inconsistency be-ZThe notions of completeness and coherence inLFG\[6\] have been employed by Wedekind\[25\] to avoidover- and under-generation.226tween the assumptions of the source- and target-language speakers.In this paper~ we introduce tricolor DAGsto represent he above constraints, and discusshow tricolor DAGs are used for practical MT sys-tems.
In particular, we give a generation algo-rithm that incorporates the notion of semantictransfer by gradually approaching the optimal tar-get sentence through the use of tricolor DAGs,when a fully interlingual translation fails.
TricolorDAGs give a graph-algorithmic interpretation ofthe constraints, and the distinctions between thetypes of constraint mentioned above allow us toadjust the margin between the current and opti-mal solution effectively.Tricolor DAGsA tricolor DAG (TDAG, for short) is a rooted,directed, acyclic 4 graph with a set of three colors(red, yellow, and g'reen) for nodes and directedarcs.
It is used to represent a feature structure ofa source or target sentence.
Each node representseither an atomic value or a root of a DAG, andeach arc is labeled with a feature name.
The onlydifference between the familiar usage of DAGs inunification grammars and that of TDAGs is thatthe color of a node or "arc represents its degree ofimportance:1.
Red shows that a node (arc) is essential.2.
Yellow shows that a node (arc) may be ignored,but must not be violated.3.
Green shows that a node (arc) may be violated.For practical reasons, the above distinctions areinterpreted as follows:1.
Red shows that a node (arc) is derived fromlexicons and grammatical constraints.2.
Yellow shows that a node (arc) may be inferredfrom a source or a target sentence by using do-main knowledge, common sense, and so on.3.
Green shows that a node (arc) is defeasibly in-ferred, specified as a default, or heuristicallyspecified.When all the nodes and arcs of TDAGs are red,TDAGs are basically the same as the feature struc-tures 5 of grammar-based translation\[25, 17\].
ATDAG is well-formed iff the following conditionsare satisfied:4Acyclicity is not crucial to the results in this pa-per, but it significantly simplifies the definition of thetricolor DAGs and semantic transfer.SWe will only consider the semantic portion of thefeature structure although the theory of tricolor DAGSfor representing entire feature structures i an interest-ing topic.1.
The root is a red node.2.
Each red arc connects two red nodes.3.
Each red node is reachable from the rootthrough the red arcs and red nodes.4.
Each yellow node is reachable from the rootthrough the arcs and nodes that are red and/oryellow.5.
Each yellow arc connects red and/or yellownodes.6.
No two arcs start from the same node, and havethe same feature name.Conditions 1 to 3 require that all the red nodesand red arcs between them make a single, con-nected DAG.
Condition 4 and 5 state that a de-feasible constraint must not be used to derive animposed constraint.
In the rest of this paper, wewill consider only well-formed TDAGs.
Further-more, since only the semantic portions of TDAGsare used for machine translation, we will not dis-cuss syntactic features.The subsurnption relationship among theTDAGs is defined a~ the usual subsumption overDAGs, with the following extensions.?
A red node (arc) subsumes only a red node(arc).?
A yellow node (arc) subsumes a red node (arc)and a yellow node (arc).?
A green node (arc) subsumes a node (arc) withany color.The unification of TDAGs is similarly defined.The colors of unified nodes and arcs are specifiedas follows:?
Unification of a red node (arc) with anothernode (arc) makes a red node (arc).?
Unification of a yellow node (arc) with a yellowor green node (arc) makes a yellow node (arc).?
Unification of two green nodes (arcs) makes agreen node (arc).Since the green nodes and arcs represent defensibleconstraints, unification of a green node (either aroot of a TDAG or an atomic node) with a redor yellow node always succeeds~ and results in ared or yellow node.
When two conflicting greennodes are to be unified, the result is indefinite, ora single non-atomic green node.
6Now, the problem is that a red node/arc in asource TDAG (the TDAG for a source sentence)6An alternative definition is that one green nodehas precedence over the other\[14\].
Practically, sucha conflicting unification should be postponed until noother possibility is found.227sgWISH _ n u m ~ ~"JOHNSource T-DAG1sg ,anum?
"WISH o ~"JOHN?
agent"WALKTarget T-DAG2sg"WISH num ..
.
?...-"" "JOHNagenttherne~ "WALK"WISH~ "JOHN"WALKTarget T-DAG4 Source T-DAG3red node ~ red arcyellow node m m ~ yellow arcO green ode .
.
.
.
.
.
.
.
.
, - -  green arcFigure h Sample TDAGsmay not always be a red node/arc in the targetTDAG (the TDAG for a target sentence).
Forexample, the functional control of the verb "wish"in the English sentenceJohn ~ished to walkmay produce the TDAGI in Figure 1, but thered arc corresponding to the agent of the *WALKpredicate may not be preserved in a targetTDAG2.
7 This means that the target sentencea\]one cannot convey the information that it isJohn who wished to walk, even if this informationcan be understood from the context.
Hence thered arc is relaxed into a yellow one, and any tar-get TDAG must have an agent of *WALK that isconsistent with *JOHN.
This relaxation will helpthe sentence generator in two ways.
First, it canprevent generation failure (or non-termination ithe worst case).
Second, it retains important in-formation for a choosing correct translation of theverb "walk".
srFor example, the Japanese counterpart "~"  forthe verb "wish" only takes a sentential complement,and no functional control is observed.SWhether or not the subject of the verb is humanis often crucial information for making an appropriatechoice between the verb's two Japanese counterparts"~ <" and "~?~7o".Another example is the problem of iden-tifying number and determiner in Japanese-to-English translation.
This type of information israrely available from a syntactic representationof a Japanese noun phrase, and a set of heuris-tic rules\[ll\] is the only known basis for makinga reasonable guess.
Even if such contextual pro-cessing could be integrated into a logical inferencesystem, the obtained information should be defea-sible, and hence should be represented by greennodes and arcs in the TDAGs.
Pronoun resolu-tion can be similarly represented by using greennodes and arcs.It is worth looking at the source and tar-get TDAGs in the opposite direction.
From theJapanese sentence,John +subj walk +nom +obj wishedwe get the source TDAG3 in Figure I, where func-tional control and number information are miss-ing.
With the help of contextual processing, weget the target TDAG4, which can be used to gen-erate the English sentence "John wished to walk.
;"Semantic TransferAs illustrated in the previous section, it is oftenthe case that we have to solve mismatches betweensource and target TDAGs in order to obtain suc-cessful translations.
Syntactic/semantic transferhas been formulated by several researchers\[18, 27\]as a means of handling situations in which fullyinterlingual translation does not work.
It is notenough, however, to capture only the equivalentrelationship between source and target semanticrepresentations: this is merely a mapping amongred nodes and arcs in TDAGs.
What is missing inthe existing formulation is the provision of somemargin between what is said and what is trans-lated.
The semantic transfer in our framework isdefined as a set of successive operations on TDAGsfor creating a sequence of TDAGs to, tl, .
.
.
,  tksuch that to is a source TDAG and tk is a targetTDAG that is a successful input to the sentencegenerator.A powerful contextual processing and a do-main knowledge base can be used to infer addi-tional facts and constraints, which correspond tothe addition of yellow nodes and arcs.
Default in-heritance, proposed by Russell et al\[14\], providesan efficient way of obtaining further informationnecessary for translation, which corresponds to theaddition of green nodes and arcs.
A set of well-known heuristic rules, which we will describe laterin the "Implementation" Section, can also be usedto add green nodes and arcs.
To complete themodel of semantic transfer, we have to introduce228a "painter."
A painter maps a red node to ei-ther a yellow or a green node, a yellow node toa green node, and so on.
It is used to loosen theconstraints imposed by the TDAGs.
Every appli-cation of the painter monotonically loses some in-formation in a TDAG, and only a finite number ofapplications of the painter are possible before theTDAG consists entirely of green nodes and arcsexcept for a red root node.
Note that the painternever removes a node or an arc from a TDAG,it simply weakens the constraints imposed by thenodes and arcs.Formally, semantic transfer is defined as a se-quence of the following operations on TDAGs:?
Addition of a yellow node (and a yellow arc) toa given TDAG.
The node must be connected toa node in the TDAG by a yellow arc.?
Addition of a yellow arc to a given TDAG.
Thearc must connect wo red or yellow nodes in theTDAG.?
Addition of a green node (and a green arc) to agiven TDAG.
The node must be connected to anode in the TDAG by the green arc.?
Addition of a green arc to a given TDAG.
Thearc can connect two nodes of any color in theTDAG.?
Replacement of a red node (arc) with a yellowone, as long as the well-formedness is preserved.?
Replacement of a yellow node (arc) with a greenone, as long as the well-formedness is preserved.The first two operations define the logical impli-cations (possibly with common sense or domainknowledge) of a given TDAG.
The next two op-erations define the defensible (or heuristic) infer-ence from a given TDAG.
The last two operationsdefine the painter.
The definition of the painterspecifies that it can only gradually relax the con-straints.
That is, when a red or yellow node (orarc) X has other red or yellow nodes that are onlyconnected through X, X cannot be "painted" un-til each of the connected red and yellow nodes ispainted yellow or green to maintain the reachabil-ity through X.In the sentence analysis phase, the first fouroperations can be applied for obtaining a sourceTDAG as a reasonable semantic interpretation ofa sentence.
The application of these operationscan be controlled by "weighted abduction"\[5\], de-fault inheritance, and so on.
These operations canalso be applied at semantic transfer for augment-ing the TDAG with a common sense knowledge ofthe target language.
On the other hand, these op-erations are not applied to a TDAG in the gener-ation phase, as we will explain in the next section.This is because the lexicon and grammatical con-straints are only applied to determine whether ednodes and arcs are exactly derived.
If they are notexactly derived, we will end up with either over- orunder-generation beyond the permissible margin.Semantic transfer is applied to a source TDAG asmany times 9 as necessary until a successful gen-eration is made.
Recall the sample sentence inFigure 1~ where two painter calls were made tochange two red arcs in TDAG1 into yellow onesin TDAG2.
These are examples of the first sub-stitution operation shown above.
An addition ofa green node and a green arc, followed by an ad-dition of a green arc, was applied to TDAG3 toobtain TDAG4.
These additions are examples ofthe third and fourth addition operations.Sentence  Generat ion  A lgor i thmBefore describing the generation algorithm, let uslook at the representation of lexicons and gram-mars for machine translation.
A lexical rule isrepresented by a set of equations, which intro-duce red nodes and arcs into a source TDAG.
l?
Aphrasal rule is similarly defined by a set of equa-tions, which also introduce red nodes and arcs fordescribing a syntactic head and its complements.For example, if we use Shieber's PATR-II\[15\]notation~ the lexical rule for "wished" can be rep-resented as follows:V "-~ wished(V cat) ---- v(V form) - past(V subj cat} = np(V obj cat) = v(V obj form) = infinitival(V wed) -- *WISH(V pred agent) = (V subj pred)(V pred theme) = (V obj pred)(V pred theme agent) = (V subj pred)The last four equations are semantic equa-tions.
Its TDAG representation is shown in Fig-ure 2.
It would be more practical to further as-sume that such a lexicai rule is obtained froma type inference system, 11 which makes use of asyntactic lass hierarchy so that each lexical classcan inherit general properties of its superclasses.Similarly, semantic oncepts uch as *WISH and*WALK should be separately defined in an onto-logical hierarchy together with necessary domainknowledge (e.g., selectional constraints on case9The iteration is bounded by the number of nodesand arcs in the TDAG, although the number of possi-ble sequences of operations could be exponential.1?For simplicity, we will only consider semanticequations to form the TDAGs.11as in Shieber\[15\], Pollard and Sag\[13\], and Russellet al\[14\]229*WISHp r e d J ~  Qnpcat~ o~ ~prme:  ~agentv c .
,  vFigure 2: TDAG representation of the verb"wished" (embedded in the entire feature struc-ture)caller ?
-.. work-for?
"-.
*OFF!CF~BoSTON * C A L ' ~definite singularFigure 3: Source TDAG for the sentence "TheBoston Office called.
"fillers and part-of relationships.
See KBMT-8918\].
)A unification grammar is used for both analysisand generation.
Let us assume that we have twounification grammars for English and Japanese.Analyzing a sentence yields a source TDAG withred nodes and arcs.
Semantic interpretation re-solves possible ambiguity and the resulting TDAGmay include all kinds of nodes and arcs.
For ex-ample, the sentence 12The Boston office calledwould give the source TDAG in Figure 3.
Byutilizing the domain knowledge, the node labeled*PERSON is introduced into the TDAG as a realcaller of the action *CALL, and two arcs repre-senting *PERSON work-for *OFFICE and *OF-FICE in *BOSTON are abductively inferred.Our generation algorithm is based onWedekind's DAG traversal algorithm\[25\] forLFG.
la The algorithm runs with an input TDAGby traversing the nodes and arcs that were derivedfrom the lexicon mand grammar ules.
The termi-nation conditions are as follows:12in Hobbs et al\[5\]13It would be identical to Wedekind's algorithm ifan input TDAG consisted of only red nodes and arcs.. *PERSONcaller ?
".. work-for?
",,.
*OFFICE *BOSTON _ (~ *CALL= "~ A m. .
.
.
.
/(~ - -npm~~r~~ (~definite singularFigure 4: Target TDAG for the sentence "TheBoston Office called."?
Every red node and arc in the TDAG was de-rived.?
No new red node (arc) is to be introduced intothe TDAG if there is no corresponding node(arc) of any color in the TDAG.
That is, thegenerator can change the color of a node (arc)to red, but cannot add a new node (arc).?
For each set of red paths (i.e., the sequence ofred arcs) that connects the same pair of nodes,the reentrancy was also derived.These conditions are identical to those ofWedekind except hat yellow (or green) nodes andarcs may or may not be derived.
For example, thesentence "The Boston Office called" in Figure 3can  be translated into Japanese by the followingsequence of semantic transfer and sentence gener-ation.1.
Apply the painter to change the yellow of thedefinite node and the def arc to green.2.
Apply the painter to change the yellow of thesingular node and the hum arc to green.
Theresulting TDAG is shown in Figure 4.3.
Run the sentence generator with an input fea-ture structure, which has a root and an arc predconnecting to the given TDAG.
(See the nodemarked "1" in Figure 4.)4.
The generator applies a phrasal rule, say S ---*NP VP, which derives the subj arc connectingto the subject NP (marked "2"), and the agentarc.5.
The generator applies a phrasal rule, say NP ---+MOD NP, TM which derives the npmod arc to the14There are several phrasal rules for deriving thisLHS NP in Japanese: (1) A noun-noun compound, (2)a noun, copula, and a noun, and (3) a noun, postposi-tional particle, and a noun.
These three rules roughlycorrespond to the forms (1) Boston Office, (2) officeof Boston, and (3) office in Boston.
Inference of the"*OFFICE in *BOSTON" relation is easiest if rule (3)230modifier of the NP (marked "3") and the roodarc .6.
Lexical rules are applied and all the semanticnodes, *CALL, *OFFICE, and *BOSTON arederived.The annotated sample run of the sentence gen-erator is shown in Figure 5.
The input TDAG inthe sample run is embedded in the input featurestructure as a set of PRED values, but the seman-tic arcs are not shown in the figure.
The inputfeature structure has syntactic features that werespecified in the lexical rules.
The feature value*UNDEFINED* is used to show that the node hasbeen traversed by the generator.The basic property of the generation algo-rithm is as follows:Let t be a given TDAG, tmi~ be the connectedsubgraph including all the red nodes and arcsin t, and t ,~ ,  be the connected subgraph oft obtained by changing all the colors of thenodes and arcs to red.
Then, any successfulgeneration with the derived TDAG tg satisfiesthe condition that t,,i~ subsumes ta, and t asubsumes trnaz.The proof is immediately obtained from the defini-tion of successful generation and the fact that thegenerator never introduces a new node or a newarc into an input TDAG.
The TDAGs can alsobe employed by the semantic head-driven genera-tion algorithm\[17\] while retaining the above prop-erty.
Semantic monotonicity always holds for aTDAG, since red nodes must be connected.
It hasbeen shown by Takeda\[21\] that semantically non-monotonic representations can also be handled byintroducing a functional semantic lass.Imp lementat ionWe have been developing a prototype English-to-Japanese MT system, called Shalt2122\], witha lexicon for a computer-manual domain includ-ing about 24,000 lexemes each for English andJapanese, and a general exicon including about50,000 English words and their translations.
Asample set of 736 sentences was collected fromthe "IBM AS/400 Getting Started" manual, andwas tested with the above semantic transfer andgeneration algorithmJ s The result of the syntac-tic analysis by the English parser is mapped toa TDAG using a set of semantic equations 16 oh-is used, but the noun-noun compound is probably thebest translation.
!15We used McCord's English parser based on hisEnglish Slot Grammar\[10\], which covered more than93% of the sentences.l~We call such a set of semantic equations mappingrules (see Shalt2\[20\] or KBMT-8918\]).
; ;  run the generator with input f-structureO> *J-GG-START called with((PRED "~")  (CAT V) (VTYPE V-bDAN-B)(SUBCAT TRANS) (ASP-TYPE SHUNKAN)(:MOOD ((PKED "@dec")))(AUX ((PRED "@aux") (:TIME ((PRED "@past")))(:PASSIVE ((PRED "@minus")))))(SUBJ ((CAT N) (PRED "~i~;~")(XADJL1BCT ((XCOP ,,'C'Cr),,) (CAT N)(PRED ",~?5~ ~ ~"))))))?
.
.3> *J-GG-S ca l led  ; ;<star t> ->.
.
.
->  <S>4> *J-GG-XP called with ;;subj-filler((CASE (.0,'I* "~ ....
%?"))
(CAT N)(NEG *UNDEFINED*) (PRED "~P~")(\](ADJUNCT ((COP -) (CAT N)(PRED "~,  }" > '" ) ) ) )5> *J-GG-NP called ;;head NP of subj10< *GG-N-ROOT returns ;;np mod" ,~?~ ~ M"  ; ;"Boston"9> *J-GG-N called ; ;head np10< *GG-N-ROOT returns"~"  ;;"office"7< *9 (<SS> <NP>) returns ;;mod+NP5< .i (<NP> <P>) returns ;;NP+case-marker'~A I- >z~$~I$ ,4< *J-GG-XP returns "~?A b > '7~69~&~"4> *J-GG-S called with ;;VP part5> *J-GG-VP called ;;stem +6> *J-GG-V called ; ;function word chains( (SUBJ *UNDEFINED*)(ADVADJUBCT *UNDEFINED*)(PPAD JUNCT *UNDEFINED*)( :MOOD *UNDEFINED*)(AUX ((:TIME ((PRED "@past")))(:PASSIVE((PRED (*OR* *UNDEFINED* "@minus"))) )(PRED "@aux") ))(CAT V) (TYPE FINAL) (ASP-TYPE SHUNKAN)(VTYPE V-bDAN-B) (SUBCAT TRIIlIS)(PKED "l~2g" ) )7> *J-GG-RENTAI-PAST called ; ;past - form14< *GG-V-ROOT returns "~" ; ;stem?
.
.6< *J-GG-V returns "~\ [~ b~C"5< *J-GG-VP returns "~\ [~ ~fC"4< *J-GG-S returns "~\ [~ ~"3< *J-GG-S returnsO< *J-GG-START returnsFigure 5: Sentence generation from the TDAG for"The Boston Office called.
"231tained from the lexicons.
We have a very shal-low knowledge base for the computer domain,and no logical inference system was used to de-rive further constraints from the given source sen-tences.
The Japanese grammar is similar to theone used in KBMT-89, which is written inpseudo-unification\[23\] equations, but we have added sev-eral new types of equation for handling coordi-nated structures.
The Japanese grammar can gen-erate sentences from all the successful TDAGs forthe sample English sentences.It turned out that there were a few collectionsof semantic transfer sequences which contributedvery strongly to the successful generation.
Thesesequences include?
Painting the functional control arcs in yellow.?
Painting the gaps of relative clauses in yellow.?
Painting the number and definiteness featuresin yellow.?
Painting the passivization feature in green.
~7Other kinds of semantic transfer are rather id-iosyncratic, and are usually triggered by a par-ticular lexical rule.
Some of the sample sentencesused for the translations are as follows: ~sMake sure you are using the proper editionfor the level of the product.~-+f -  ~ ~ ?
p~<m ~ ~t~user +subj product +pos level +for properedition +obj use +prog +nom +objconfirm +impPublications are not stocked at the addresspublication +subj following +loc provideaddress +loc stock +passive +negThis publication could contain technicalinaccuracies or typographical errors.th i s  pub l i ca t ion  +subj techn ica l  inaccuracyor typographical error +objconta in  +ab i l i ty  +past17We decided to include the passivization feature inthe semantic representation in order to determine theproper word ordering in Japanese.1s Japanese translation reflects the errors made inEnglish analysis.
For example, the auxiliary verb"could" is misinterpreted in the last sample sentence.The overall accuracy of the translated sen-tences was about 63%.
The main reason for trans-lation errors was the occurrence of errors in lexi-cal and structural disambiguation by the syntac-tic/semantic analyzer.
We found that the accu-racy of semantic transfer and sentence generationwas practically acceptable.Though there were few serious errors, someoccurred when a source TDAG had to be com-pletely "paraphrased" into a different TDAG.
Forexample, the sentenceLet's get started.was very hard to translate into a natural Japanesesentence.
Therefore, a TDAG had to be para-phrased into a totally different TDAG, which is an-other important role of semantic transfer.
Otherserious errors were related to the ordering of con-stituents in the TDAG.
It might be generally ac-ceptable to assume that the ordering of nodes in aDAG is immaterial.
However, the different order-ing of adjuncts ometimes resulted in a misleadingtranslation, as did the ordering of members in acoordinated structure.
These subtle issues have tobe taken into account in the framework of seman-tic transfer and sentence generation.Conc lus ionsIn this paper, we have introduced tricolor DAGsto represent various degrees of constraint, and de-fined the notions of semantic transfer and sen-tence generation as operations on TDAGs.
Thisapproach proved to be so practical that nearlyall of the source sentences that were correctlyparsed were translated into readily acceptable sen-tences.
Without semantic transfer, the translatedsentences would include greater numbers of incor-rectly selected words, or in some cases the gener-ator would simply fail 19Extension of TDAGs for disjunctive informa-tion and a set of feature structures must be fullyincorporated into the framework.
Currently onlya limited range of the cases are implemented.
Op-timal control of semantic transfer is still unknown.Integration of the constraint-based formalism, de-feasible reasoning, and practical heuristic rules arealso important for achieving high-quality transla-tion.
The ability to process and represent variouslevels of knowledge in TDAGs by using a uniformarchitecture is desirable, but there appears to besome efficient procedural knowledge that is veryhard to represent declaratively.
For example, thenegative determiner "no" modifying a noun phrasein English has to be procedurally transferred into~gThe Essential Arguments Algorithm\[9\] might bean alternative method for finding a successful genera-tion path.232the negation of the verb governing the noun phrasein 3 apanese.
Translation of "any", "yet", "only",and so on involves similar problems.While TDAGs reflect three discrete types ofconstraints, it is possible to generalize the typesinto continuous, numeric values such as potentialenergy\[4\].
This approach will provide a consider-ably more flexible margin that defines a set of per-missible translations, but it is not clear whetherwe can successfully define a numeric value for eachlexical rule in order to obtain acceptable transla-tions.AcknowledgmentsThe idea of the tricolor DAGs grew from discus-sions with Shiho 0gino on the design and im-plementation of the sentence generator.
I wouldalso like to thank the members of the NL group- Naohiko Uramoto, Tetsuya Nasukawa, HiroshiMaruyama, Hiroshi Nomiyama, Hideo Watanabe,Masayuki Morohashi, and Taijiro Tsutsumi - fo rstimulating comments and discussions that di-rectly and indirectly contributed to shaping thepaper.
Michael McDonald, who has always beenthe person I turn to for proofreading, helped mewrite the final version.Re ferences\[1\] M. Dymetman.
"Inherently Reversible Grammars, LogicProgramming and Computability".
In Proc.
of ACLWorkshop on Reversible Grammar in Natural Lan-guage Processing, pages 20-30, Berkeley, California,June 1991.\[2\] M. Emele, U.
Held, S. Momma, and R. Zajac.
"Inter-actions between Linguistic Constraints: Procedural vs.Declarative Approaches".
Machine Translation, 7(1-2):61-98, 1992.\[3\] K. Hasida.
"Common Heuristics for Parsing, Genera-tion, and Whatever, .
.
.".
In Proc.
of a Workshop onReversible Grammar in Natural Language Processing,pages 81-90, June 1991.\[4\] K. Haslda.
"Dynamics of Symbol Systems - An Inte-grated Architecture of Cognition -".
In Proc.
of Interna-tional Conference on Fifth Generation Computer Sys-tems 1994 pages 1141-1148, June 1992.\[5\] J. R. Hobbs, M. E. Sticke\], D. E. Appelt, and P.
Martin.
"Interpretation as abduction".
Artificial Intelligence,63:69-142, 1993.\[el R. Kaplan and J. Bresnan.
"Lexlcal-b'~nctional Gram-mar: A Formal System for Generalized Grammaticall~epresentatlon".
In J. Bresnan, editor, "Mental Rep-resentation of Grammatical Relations", pages 173-281.MIT Press, Cambridge, Mass., 1982.\[7\] R. Kasper and W.  C. Rounds.
"A Logical Semantics forFeature Structures".
In Proc.
of the ?.~th Annual Meet-ing of the Aasociation for Computational Linguistics,Columbia University, New York, NY, June 1986.\[8\] KBMT89.
"Special Issue on Knowlege-based MachineTranslation I and II".
Machine Translatlon~ 4(2-3),March-June 1989.\[9\] M. Martinovic and T. Strzalkowski.
"Comparing TwoGra~nmar-Bued Generation Algorithms: A Case Study".In Proc.
of the 30th Annual Meeting of ACL, pages 81-88, June 1992,233\[1O\] M. McCord.
"Slot Grammar: A System for SimplerConstruction of Practical Natural Language Grammars(Ed:Studsr, R.)", pages 118-145.
Springer-Verlag, 1990.\[11\] M. Murata and M. Naga~.
"Determination f ReferentialProperty and Number of Nouns in Japanese Sentences forMachine Translation into English".
In Prac.
of the 5thInternational Conference on Theoretical and Method-ological lssues in Machine Translation, pages 218-225,Kyoto, Japan, July 1993.\[12\] E. H. Nyberg, 3rd and T. Mitamura.
"The KANT Sys-tem: Fast, Accurate, High-Quality Translation in Prac-tical Domains".
In Proc.
of the 14th International Con-ference on Computational Linguistics, pages 1069-1073,July 1992.\[131 C. Pollard and I.
A.
Sag.
"An Information-Based Syn-tax and Semantics, Vol.1 Fitndamentals".
CSLI LectureNotes, Number 18, 1987.\[14\] G. Russell, A. Bdlim, J. Carroll, and S. Warwick-Armstrong.
"A Practical Approach to Multiple DefaultInheritance for Unlficatlon-Based Lexicons".
Computa-tional Linguistics, 18(3):311-337, Sept. 1992.\[15\] S. M. Shleber.
"An Introduction to Unlficatlon-BasedApproaches to Grammar".
CSLI Lecture Notes, Number4, Stanford, CA, 1988.\[161 S. M. Shleber.
=A Uniform Architecture for Parsing andGeneration".
In Proc.
of the l~th International Con-ference on Computational Linguistics, pages 614-619,August 1988.\[17\] S. M. Shleber, P. C. N. Perelra, G. van Noord, and R. C.Moore.
"Semantic-Head-Drlven GenerationS.
Computa-tional Linguistics, 16(1):30--42, March 1990.\[18\] S. M. Shieber and Y. Schabes.
"Synchronous Tree-Adjoining Grammars".
In Proc.
of the 13th Interna-tional Conference on Computational Linguistics, pages253-258, August 1990.\[19\] 13.
Smolka.
"A Feature Logic with Subsorts'.
TechnicalReport LILOG-REPORT 33, IBM Deutschland GmbH,Stuttgart, West Germany, May 1988.\[20\] K. Takeda.
"An Object-Oriented Implementation f Ma-chine Translation Systems ~.
In Proc.
of the 5th Inter-national Conference on Theoretical and Methodologi-cal Issues in Machine Translation, pages 154-167, July1993.\[21\] K. Takeda.
"Sentence Generation from Partially Con-strained Feature Structures ~.
In Proc.
of the NaturalL~nguags Processing Pacific Rim Symposium, pages 7-16, Dec. 1993.\[22\] K. Takeda~ N. Uramoto, T. Nasukawa, and T.
Tsutsumi.
"Shall2 - A Symmetric Machine Translation System withConceptual Transfer".
In Proc.
of the l,~th InternationalConference on Computational Linguistics, pages 1034-1038, July 1992.\[23\] M. Tomlta and K. Knight.
=Pseudo Unification and FullUnification".
Technical Report CMU-CMT-88-MEMO,Center for Machine Translation, Carnegie Mellon Uni-verslty, November 1987.\[94\] H. Uchida~ "ATLAS Ih A Machine Translation SystemUsing Conceptual Structure as an Interlingua".
In Proc.of ~nd Intl.
Conf.
on Theoretical and MethodologicalIssues in Machine Translation of Natural Languages,pages 150-160, June 1988.\[25\] J. Wedeklnd.
"Generation as Structure Driven Deriva-tion".
In Proc.
of the 1J~th International Conference onComputational Liguistics, pages 732-737, August 1988.\[26\] H. Yasuhara.
"Conceptual Transfer in an InterlinguaMethod and Example Based MT".
In Proc.
of the Nat-ural Language Processing Pacific Rim Symposium '93,pages 376-379, Fukuoka, Japan, Dec. 1993.\[27\] I~,.
Zajac.
"A Uniform Architecture for Parsing, Gen-eration and Transfer".
In Proc.
of a Workshop onReversible Grammar in Natural Lan9uage Proceasing,pages 71--80, June 1991.
