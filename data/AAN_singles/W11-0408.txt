Proceedings of the Fifth Law Workshop (LAW V), pages 65?73,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsReducing the Need for Double AnnotationDmitriy DligachDepartment of Computer ScienceUniversity of Colorado at BoulderDmitriy.Dligach@colorado.eduMartha PalmerDepartment of LinguisticsUniversity of Colorado at BoulderMartha.Palmer@colorado.eduAbstractThe quality of annotated data is crucial forsupervised learning.
To eliminate errors insingle annotated data, a second round of an-notation is often used.
However, is it abso-lutely necessary to double annotate every ex-ample?
We show that it is possible to reducethe amount of the second round of annotationby more than half without sacrificing the per-formance.1 IntroductionSupervised learning has become the dominantparadigm in NLP in recent years thus making thecreation of high-quality annotated corpora a top pri-ority in the field.
A corpus where each instance isannotated by a single annotator unavoidably con-tains errors.
To improve the quality of the data, onemay choose to annotate each instance twice and ad-judicate the disagreements thus producing the goldstandard.
For example, the OntoNotes (Hovy et al,2006) project opted for this approach.However, is it absolutely necessary to double an-notate every example?
In this paper, we demonstratethat it is possible to double annotate only a subset ofthe single annotated data and still achieve the samelevel of performance as with full double annotation.We accomplish this task by using the single anno-tated data to guide the selection of the instances tobe double annotated.We propose several algorithms that accept sin-gle annotated data as input.
The algorithms selecta subset of this data that they recommend for an-other round of annotation and adjudication.
The sin-gle annotated data our algorithms work with can po-tentially come from any source.
For example, it canbe the single annotated output of active learning orthe data that had been randomly sampled from somecorpus and single annotated.
Our approach is ap-plicable whenever a second round of annotation isbeing considered to improve the quality of the data.Our approach is similar in spirit to active learn-ing but more practical in a double annotation multi-tagger environment.
We evaluate this approach onOntoNotes word sense data.
Our best algorithm de-tects 75% of the errors, while the random samplingbaseline only detects less than a half of that amount.We also show that this algorithm can lead to a 54%reduction in the amount of annotation needed for thesecond round of annotation.The rest of this paper is structured as follows: wediscuss the relevant work in section 2, we explainour approach in section 3, we evaluate our approachin section 4, we discuss the results and draw a con-clusion in section 5, and finally, we talk about ourplans for future work in section 6.2 Related WorkActive Learning (Settles, 2009; Olsson, 2009) hasbeen the traditional avenue for reducing the amountof annotation.
However, in practice, serial activelearning is difficult in a multi-tagger environment(Settles, 2009) when many annotators are workingin parallel (e.g.
OntoNotes employs tens of tag-gers).
At the same time, several papers recently ap-peared that used OntoNotes data for active learningexperiments (Chen et al, 2006; Zhu, 2007; Zhong etal., 2008).
These works all utilized OntoNotes goldstandard labels, which were obtained via double an-notation and adjudication.
The implicit assumption,therefore, was that the same process of double anno-65tation and adjudication could be reproduced in theprocess of active learning.
However, this assumptionis not very realistic and in practice, these approachesmay not bring about the kind of annotation cost re-duction that they report.
For example, an instancewould have to be annotated by two taggers (and eachdisagreement adjudicated) on each iteration beforethe system can be retrained and the next instance se-lected.
Active learning tends to select ambiguous ex-amples (especially at early stages), which are likelyto cause an unusually high number of disagreementsbetween taggers.
The necessity of frequent manualadjudication would slow down the overall process.Thus, if the scenarios of (Chen et al, 2006; Zhu,2007; Zhong et al, 2008) were used in practice, thetaggers would have to wait on each other, on the ad-judicator, and on the retraining, before the systemcan select the next example.
The cost of annotatorwaiting time may undermine the savings in annota-tion cost.The rationale for our work arises from these dif-ficulties: because active learning is not practicalin a double annotation scenario, the data is singleannotated first (with the instances selected via ac-tive learning, random sampling or some other tech-nique).
After that, our algorithms can be applied toselect a subset of the single annotated data for thesecond round of annotation and adjudication.
Ouralgorithms select the data for repeated labeling in asingle batch, which means the selection can be doneoff-line.
This should greatly simplify the applicationof our approach in a real life annotation project.Our work also borrows from the error detectionliterature.
Researchers have explored error detec-tion for manually tagged corpora in the contextof pos-tagging (Eskin, 2000; Kve?ton?
and Oliva,2002; Nova?k and Raz?
?mova?, 2009), dependencyparsing (Dickinson, 2009), and text-classification(Fukumoto and Suzuki, 2004).
The approaches toerror detection include anomaly detection (Eskin,2000), finding inconsistent annotations (van Hal-teren, 2000; Kve?ton?
and Oliva, 2002; Nova?k andRaz?
?mova?, 2009), and using the weights assignedby learning algorithms such as boosting (Abney etal., 1999; Luo et al, 2005) and SVM (Nakagawaand Matsumoto, 2002; Fukumoto and Suzuki, 2004)by exploiting the fact that errors tend to concentrateamong the examples with large weights.
Some ofthese works eliminate the errors (Luo et al, 2005).Others correct them automatically (Eskin, 2000;Kve?ton?
and Oliva, 2002; Fukumoto and Suzuki,2004; Dickinson, 2009) or manually (Kve?ton?
andOliva, 2002).
Several authors also demonstrate en-suing performance improvements (Fukumoto andSuzuki, 2004; Luo et al, 2005; Dickinson, 2009).All of these researchers experimented with singleannotated data such as Penn Treebank (Marcus et al,1993) and they were often unable to hand-examineall the data their algorithms marked as errors be-cause of the large size of their data sets.
Instead,to demonstrate the effectiveness of their approaches,they examined a selected subset of the detected ex-amples (e.g.
(Abney et al, 1999; Eskin, 2000; Nak-agawa and Matsumoto, 2002; Nova?k and Raz??mova?,2009)).
In this paper, we experiment with fully dou-ble annotated and adjudicated data, which allows usto evaluate the effectiveness of our approach moreprecisely.
A sizable body of work exists on us-ing noisy labeling obtained from low-cost annota-tion services such as Amazon?s Mechanical Turk(Snow et al, 2008; Sheng et al, 2008; Hsueh etal., 2009).
Hsueh et al (2009) identify several cri-teria for selecting high-quality annotations such asnoise level, sentiment ambiguity, and lexical uncer-tainty.
(Sheng et al, 2008) address the relationshipsbetween various repeated labeling strategies and thequality of the resulting models.
They also proposea set of techniques for selective repeated labelingwhich are based on the principles of active learn-ing and an estimate of uncertainty derived from eachexample?s label multiset.
These authors focus onthe scenario where multiple (greater than two) labelscan be obtained cheaply.
This is not the case with thedata we experiment with: OntoNotes data is doubleannotated by expensive human experts.
Also, unfor-tunately, Sheng et al simulate multiple labeling (thenoise is introduced randomly).
However, human an-notators may have a non-random annotation bias re-sulting from misreading or misinterpreting the direc-tions, or from genuine ambiguities.
The data we usein our experiments is annotated by humans.3 AlgorithmsIn the approach to double annotation we are propos-ing, the reduction in annotation effort is achieved by66double annotating only the examples selected by ouralgorithms instead of double annotating the entiredata set.
If we can find most or all the errors madeduring the first round of labeling and show that dou-ble annotating only these instances does not sacri-fice performance, we will consider the outcome ofthis study positive.
We propose three algorithms forselecting a subset of the single annotated data for thesecond round of annotation.Our machine tagger algorithm draws on error de-tection research.
Single annotated data unavoidablycontains errors.
The main assumption this algorithmmakes is that a machine learning classifier can forma theory about how the data should be labeled froma portion of the single annotated data.
The classifiercan be subsequently applied to the rest of the data tofind the examples that contradict this theory.
In otherwords, the algorithm is geared toward detecting in-consistent labeling within the single annotated data.The machine tagger algorithm can also be viewed asusing a machine learning classifier to simulate thesecond human annotator.
The machine tagger al-gorithm accepts single annotated data as input andreturns the instances that it believes are labeled in-consistently.Our ambiguity detector algorithm is inspired byuncertainty sampling (Lewis and Gale, 1994), a kindof active learning in which the model selects theinstances for which its prediction is least certain.Some instances in the data are intrinsically ambigu-ous.
The main assumption the ambiguity detectoralgorithm makes is that a machine learning classifiertrained using a portion of the single annotated datacan be used to detect ambiguous examples in therest of the single annotated data.
The algorithm isgeared toward finding hard-to-classify instances thatare likely to cause problems for the human annota-tor.
The ambiguity detector algorithm accepts singleannotated data as input and returns the instances thatare potentially ambiguous and thus are likely to becontroversial among different annotators.It is important to notice that the machine taggerand ambiguity detector algorithms target two differ-ent types of errors in the data: the former detectsinconsistent labeling that may be due to inconsistentviews among taggers (in a case when the single an-notated data is labeled by more than one person) orthe same tagger tagging inconsistently.
The latterfinds the examples that are likely to result in dis-agreements when labeled multiple times due to theirintrinsic ambiguity.
Therefore, our goal is not tocompare the performance of the machine tagger andambiguity detector algorithms, but rather to providea viable solution for reducing the amount of annota-tion on the second round by detecting as much noisein the data as possible.
Toward that goal we alsoconsider a hybrid approach, which is a combinationof the first two.Still, we expect some amount of overlap in theexamples detected by the two approaches.
For ex-ample, the ambiguous instances selected by the sec-ond algorithm may also turn out to be the ones thatthe first one will identify because they are harderto classify (both by human annotators and machinelearning classifiers).
The three algorithms we exper-iment with are therefore (1) the machine tagger, (2)the ambiguity detector, and (3) the hybrid of the two.We will now provide more details about how each ofthem is implemented.3.1 General FrameworkAll three algorithms accept single annotated data asinput.
They output a subset of this data that they rec-ommend for repeated labeling.
All algorithms be-gin by splitting the single annotated data into N setsof equal size.
They proceed by training a classifieron N ?
1 sets and applying it to the remaining set,which we will call the pool1.
The cycle repeats Ntimes in the style of N -fold cross-validation.
Uponcompletion, each single annotated instance has beenexamined by the algorithm.
A subset of the singleannotated data is selected for the second round of an-notation based on various criteria.
These criteria arewhat sets the algorithms apart.
Because of the timeconstraints, for the experiments we describe in thispaper, we set N to 10.
A larger value will increasethe running time but may also result in an improvedperformance.1Notice that the term pool in active learning research typi-cally refers to the collection of unlabeled data from which theexamples to be labeled are selected.
In our case, this term ap-plies to the data that is already labeled and the goal is to selectdata for repeated labeling.673.2 Machine Tagger AlgorithmThe main goal of the machine tagger algorithm isfinding inconsistent labeling in the data.
This al-gorithm operates by training a discriminative clas-sifier and making a prediction for each instance inthe pool.
Whenever this prediction disagrees withthe human-assigned label, the instance is selectedfor repeated labeling.For classification we choose a support vector ma-chine (SVM) classifier because we need a high-accuracy classifier.
The state-of-the art system weuse for our experiments is SVM-based (Dligach andPalmer, 2008).
The specific classification softwarewe utilize is LibSVM (Chang and Lin, 2001).
Weaccept the default settings (C = 1 and linear ker-nel).3.3 Ambiguity Detector AlgorithmThe ambiguity detector algorithm trains a proba-bilistic classifier and makes a prediction for eachinstance in the pool.
However, unlike the previousalgorithm, the objective in this case is to find the in-stances that are potentially hard to annotate due totheir ambiguity.
The instances that lie close to thedecision boundary are intrinsically ambiguous andtherefore harder to annotate.
We hypothesize that ahuman tagger is more likely to make a mistake whenannotating these instances.We can estimate the proximity to the class bound-ary using a classifier confidence metric such as theprediction margin, which is a simple metric oftenused in active learning (e.g.
(Chen et al, 2006)).
Foran instance x, we compute the prediction margin asfollows:Margin(x) = |P (c1|x)?
P (c2|x)| (1)Where c1 and c2 are the two most probable classesof x according to the model.
We rank the singleannotated instances by their prediction margin andselect selectsize instances with the smallest margin.The selectsize setting can be manipulated to increasethe recall.
We experiment with the settings of select-size of 20% and larger.While SVM classifiers can be adapted to producea calibrated posterior probability (Platt and Platt,1999), for simplicity, we use a maximum entropyclassifier, which is an intrinsically probabilistic clas-sifier and thus has the advantage of being able tooutput the probability distribution over the class la-bels right off-the-shelf.
The specific classificationsoftware we utilize is the python maximum entropymodeling toolkit (Le, 2004) with the default options.3.4 Hybrid AlgorithmWe hypothesize that both the machine tagger andambiguity detector algorithms we just described se-lect the instances that are appropriate for the secondround of human annotation.
The hybrid algorithmsimply unions the instances selected by these twoalgorithms.
As a result, the amount of data selectedby this algorithm is expected to be larger than theamount selected by each individual algorithm.4 EvaluationFor evaluation we use the word sense data annotatedby the OntoNotes project.
The OntoNotes data waschosen because it is fully double-blind annotated byhuman annotators and the disagreements are adjudi-cated by a third (more experienced) annotator.
Thistype of data allows us to: (1) Simulate single anno-tation by using the labels assigned by the first an-notator, (2) Simulate the second round of annotationfor selected examples by using the labels assignedby the second annotator, (3) Evaluate how well ouralgorithms capture the errors made by the first anno-tator, and (4) Measure the performance of the cor-rected data against the performance of the doubleannotated and adjudicated gold standard.We randomly split the gold standard data into tenparts of equal size.
Nine parts are used as a poolof data from which a subset is selected for repeatedlabeling.
The rest is used as a test set.
Before pass-ing the pool to the algorithm, we ?single annotate?it (i.e.
relabel with the labels assigned by the firstannotator).
The test set alays stays double anno-tated and adjudicated to make sure the performanceis evaluated against the gold standard labels.
The cy-cle is repeated ten times and the results are averaged.Since our goal is finding errors in single anno-tated data, a brief explanation of what we count asan error is appropriate.
In this evaluation, the er-rors are the disagreements between the first anno-tator and the gold standard.
The fact that our data68Sense Definition Sample ContextAccept as true withoutverificationI assume his train waslateTake on a feature, po-sition, responsibility,rightWhen will the newPresident assume of-fice?Take someone?s soulinto heavenThis is the day whenMary was assumedinto heavenTable 1: Senses of to assumeis double annotated allows us to be reasonably surethat most of the errors made by the first annotatorwere caught (as disagreements with the second an-notator) and resolved.
Even though other errors maystill exist in the data (e.g.
when the two annotatorsmade the same mistake), we assume that there arevery few of them and we ignore them for the pur-pose of this study.4.1 TaskThe task we are using for evaluating our approachis word sense disambiguation (WSD).
Resolution oflexical ambiguities has for a long time been viewedas an important problem in natural language pro-cessing that tests our ability to capture and representsemantic knowledge and and learn from linguisticdata.
More specifically, we experiment with verbs.There are fewer verbs in English than nouns but theverbs are more polysemous, which makes the taskof disambiguating verbs harder.
As an example, welist the senses of one of the participating verbs, toassume, in Table 1.The goal of WSD is predicting the sense of an am-biguous word given its context.
For example, givena sentence When will the new President assume of-fice?, the task consists of determining that the verbassume in this sentence is used in the Take on a fea-ture, position, responsibility, right, etc.
sense.4.2 DataWe selected the 215 most frequent verbs in theOntoNotes data and discarded the 15 most frequentones to make the size of the dataset more manage-able (the 15 most frequent verbs have roughly asmany examples as the next 200 frequent verbs).
WeInter-annotator agreement 86%Annotator1-gold standard agreement 93%Share of the most frequent sense 71%Number of classes (senses) per verb 4.44Table 2: Evaluation data at a glanceended up with a dataset containing 58,728 instancesof 200 frequent verbs.
Table 2 shows various impor-tant characteristics of this dataset averaged acrossthe 200 verbs.Observe that even though the annotator1-goldstandard agreement is high, it is not perfect: about7% of the instances are the errors the first annota-tor made.
These are the instances we are target-ing.
OntoNotes double annotated all the instancesto eliminate the errors.
Our goal is finding them au-tomatically.4.3 SystemOur word sense disambiguation system (Dligach andPalmer, 2008) includes three groups of features.Lexical features include open class words from thetarget sentence and the two surrounding sentences;two words on both sides of the target verb and theirPOS tags.
Syntactic features are based on con-stituency parses of the target sentence and includethe information about whether the target verb has asubject/object, what their head words and POS tagsare, whether the target verb has a subordinate clause,and whether the target verb has a PP adjunct.
Thesemantic features include the information about thesemantic class of the subject and the object of thetarget verb.
The system uses Libsvm (Chang andLin, 2001) software for classification.
We train asingle model per verb and average the results acrossall 200 verbs.4.4 Performance MetricsOur objective is finding errors in single annotateddata.
One way to quantify the success of error de-tection is by means of precision and recall.
We com-pute precision as the ratio of the number of errorsin the data that the algorithm selected and the to-tal number of instances the algorithm selected.
Wecompute recall as the ratio of the number of errorsin the data that the algorithm selected to the total69number of errors in the data.
To compute baselineprecision and recall for an algorithm, we count howmany instances it selected and randomly draw thesame number of instances from the single annotateddata.
We then compute precision and recall for therandomly selected data.We also evaluate each algorithm in terms of clas-sification accuracy.
For each algorithm, we measurethe accuracy on the test set when the model is trainedon: (1) Single annotated data only, (2) Single anno-tated data with a random subset of it double anno-tated2 (of the same size as the data selected by thealgorithm), (3) Single annotated data with the in-stances selected by the algorithm double annotated,and (4) Single annotated data with all instances dou-ble annotated.4.5 Error Detection PerformanceIn this experiment we evaluate how well the threealgorithms detect the errors.
We split the data foreach word into 90% and 10% parts as described atthe beginning of section 4.
We relabel the 90% partwith the labels assigned by the first tagger and use itas a pool in which we detect the errors.
We pass thepool to each algorithm and compute the precisionand recall of errors in the data the algorithm returns.We also measure the random baseline performanceby drawing the same number of examples randomlyand computing the precision and recall.
The resultsare in the top portion of Table 3.Consider the second column, which shows theperformance of the machine tagger algorithm.
Thealgorithm identified as errors 16.93% of the totalnumber of examples that we passed to it.
These se-lected examples contained 60.32% of the total num-ber of errors found in the data.
Of the selected ex-amples, 23.81% were in fact errors.
By drawing thesame number of examples (16.93%) randomly werecall only 16.79% of the single annotation errors.The share of errors in the randomly drawn examplesis 6.82%.
Thus, the machine tagger outperforms therandom baseline both with respect to precision andrecall.The ambiguity detector algorithm selected 20% ofthe examples with the highest value of the prediction2Random sampling is often used as a baseline in the activelearning literature (Settles, 2009; Olsson, 2009).margin and beat the random baseline both with re-spect to precision and recall.
The hybrid algorithmalso beat the random baselines.
It recalled 75% oferrors but at the expense of selecting a larger set ofexamples, 30.48%.
This is the case because it selectsboth the data selected by the machine tagger and theambiguity detector.
The size selected, 30.48%, issmaller than the sum, 16.93% + 20.01%, becausethere is some overlap between the instances selectedby the first two algorithms.4.6 Model PerformanceIn this experiment we investigate whether doubleannotating and adjudicating selected instances im-proves the accuracy of the models.
We use the samepool/test split (90%-10%) as was used in the previ-ous experiment.
The results are in the bottom por-tion of Table 3.Let us first validate empirically an assumption thispaper makes: we have been assuming that full dou-ble annotation is justified because it helps to correctthe errors the first annotator made, which in turnleads to a better performance.
If this assumptiondoes not hold, our task is pointless.
In general re-peated labeling does not always lead to better per-formance (Sheng et al, 2008), but it does in ourcase.
We train a model using only the single an-notated data and test it.
We then train a model usingthe double annotated and adjudicated version of thesame data and evaluate its performance.As expected, the models trained on fully doubleannotated data perform better.
The performance ofthe fully double annotated data, 84.15%, is the ceil-ing performance we can expect to obtain if we detectall the errors made by the first annotator.
The perfor-mance of the single annotated data, 82.84%, is thehard baseline.
Thus, double annotating is beneficial,especially if one can avoid double annotating every-thing by identifying the single annotated instanceswhere an error is suspected.All three algorithms beat both the hard and therandom baselines.
For example, by double annotat-ing the examples the hybrid algorithm selected weachieve an accuracy of 83.82%, which is close to thefull double annotation accuracy, 84.15%.
By doubleannotating the same number of randomly selectedinstances, we reach a lower accuracy, 83.36%.
Thedifferences are statistically significant for all three70Metric Machine Tagger, % Ambiguity Detector, % Hybrid, %Actual size selected 16.93 20.01 30.48Error detection precision 23.81 10.61 14.70Error detection recall 60.32 37.94 75.14Baseline error detection precision 6.82 6.63 6.86Baseline error detection recall 16.79 19.61 29.06Single annotation only accuracy 82.84 82.84 82.84Single + random double accuracy 83.23 83.09 83.36Single + selected double accuracy 83.58 83.42 83.82Full double annotation accuracy 84.15 84.15 84.15Table 3: Results of performance evaluation.
Error detection performance is shown at the top part of the table.
Modelperformance is shown at the bottom.algorithms (p < 0.05).Even though the accuracy gains over the randombaseline are modest in absolute terms, the readershould keep in mind that the maximum possible ac-curacy gain is 84.15% - 82.84% = 1.31% (when allthe data is double annotated).
The hybrid algorithmcame closer to the target accuracy than the othertwo algorithms because of a higher recall of errors,75.14%, but at the expense of selecting almost twiceas much data as, for example, the machine taggeralgorithm.4.7 Reaching Double Annotation AccuracyThe hybrid algorithm performed better than thebaselines but it still fell short of reaching the accu-racy our system achieves when trained on fully dou-ble annotated data.
However, we have a simple wayof increasing the recall of error detection.
One wayto do it is by increasing the number of instances withthe smallest prediction margin the ambiguity detec-tor algorithm selects, which in turn will increase therecall of the hybrid algorithm.
In this series of exper-iments we measure the performance of the hybrid al-gorithm at various settings of the selection size.
Thegoal is to keep increasing the recall of errors until theperformance is close to the double annotation accu-racy.Again, we split the data for each word into 90%and 10% parts.
We relabel the 90% part with thelabels assigned by the first tagger and pass it to thehybrid algorithm.
We vary the selection size settingbetween 20% and 50%.
At each setting, we com-pute the precision and recall of errors in the datathe algorithm returns as well as in the random base-line.
We also measure the performance of the mod-els trained on on the single annotated data with itsrandomly and algorithm-selected subsets double an-notated.
The results are in Table 4.As we see at the top portion of the Table 4, as weselect more and more examples with a small predic-tion margin, the recall of errors grows.
For exam-ple, at the 30% setting, the hybrid algorithm selects37.91% of the total number of single annotated ex-amples, which contain 80.42% of all errors in thesingle annotated data (more than twice as much asthe random baseline).As can be seen at the bottom portion of the Ta-ble 4, with increased recall of errors, the accuracyon the test set alo grows and nears the double an-notation accuracy.
At the 40% setting, the algorithmselects 45.80% of the single annotated instances andthe accuracy with these instances double annotatedreaches 84.06% which is not statistically different(p < 0.05) from the double annotation accuracy.5 Discussion and ConclusionWe proposed several simple algorithms for reducingthe amount of the second round of annotation.
Thealgorithms operate by detecting annotation errorsalong with hard-to-annotate and potentially error-prone instances in single annotated data.
We evalu-ate the algorithms using OntoNotes word sense data.Because OntoNotes data is double annotated and ad-judicated we were able to evaluate the error detec-tion performance of the algorithms as well as theiraccuracy on the gold standard test set.
All three al-71Metric Selection Size20% 30% 40% 50%Actual size selected 30.46 37.91 45.80 54.12Error detection precision 14.63 12.81 11.40 10.28Error detection recall 75.65 80.42 83.95 87.37Baseline error detection precision 6.80 6.71 6.78 6.77Baseline error detection recall 29.86 36.23 45.63 53.30Single annotation only accuracy 83.04 83.04 83.04 83.04Single + random double accuracy 83.47 83.49 83.63 83.81Single + selected double accuracy 83.95 83.99 84.06 84.10Full double annotation accuracy 84.18 84.18 84.18 84.18Table 4: Performance at various sizes of selected data.gorithms outperformed the random sampling base-line both with respect to error recall and model per-formance.By progressively increasing the recall of errors,we showed that the hybrid algorithm can be usedto replace full double annotation.
The hybrid algo-rithm reached accuracy that is not statistically dif-ferent from the full double annotation accuracy withapproximately 46% of data double annotated.
Thus,it can potentially save 54% of the second pass of an-notation effort without sacrificing performance.While we evaluated the proposed algorithms onlyon word sense data, the evaluation was performedusing 200 distinct word type datasets.
These wordseach have contextual features that are essentiallyunique to that word type and consequently, 200distinct classifiers, one per word type, are trained.Hence, these could loosely be considered 200 dis-tinct annotation and classification tasks.
Thus, it islikely that the proposed algorithms will be widelyapplicable whenever a second round of annotationis being contemplated to improve the quality of thedata.6 Future WorkToward the same goal of reducing the cost of the sec-ond round of double annotation, we will explore sev-eral research directions.
We will investigate the util-ity of more complex error detection algorithms suchas the ones described in (Eskin, 2000) and (Naka-gawa and Matsumoto, 2002).
Currently our algo-rithms select the instances to be double annotatedin one batch.
However it is possible to frame theselection more like batch active learning, where thenext batch is selected only after the previous one isannotated, which may result in further reductions inannotation costs.AcknowledgementsWe gratefully acknowledge the support of the Na-tional Science Foundation Grant NSF-0715078,Consistent Criteria for Word Sense Disambiguation,and the GALE program of the Defense AdvancedResearch Projects Agency, Contract No.
HR0011-06-C-0022, a subcontract from the BBN-AGILETeam.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflectthe views of the National Science Foundation.ReferencesSteven Abney, Robert E. Schapire, and Yoram Singer.1999.
Boosting applied to tagging and pp attachment.In Proceedings of the Joint SIGDAT Conference onEmpirical Methods in Natural Language Processingand Very Large Corpora, pages 38?45.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: alibrary for support vector machines.Jinying Chen, Andrew Schein, Lyle Ungar, and MarthaPalmer.
2006.
An empirical study of the behaviorof active learning for word sense disambiguation.
InProceedings of the main conference on Human Lan-guage Technology Conference of the North AmericanChapter of the Association of Computational Linguis-tics, pages 120?127, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.72Markus Dickinson.
2009.
Correcting dependency anno-tation errors.
In EACL ?09: Proceedings of the 12thConference of the European Chapter of the Associ-ation for Computational Linguistics, pages 193?201,Morristown, NJ, USA.
Association for ComputationalLinguistics.Dmitriy Dligach and Martha Palmer.
2008.
Novel se-mantic features for verb sense disambiguation.
InHLT ?08: Proceedings of the 46th Annual Meetingof the Association for Computational Linguistics onHuman Language Technologies, pages 29?32, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Eleazar Eskin.
2000.
Detecting errors within a corpususing anomaly detection.
In Proceedings of the 1stNorth American chapter of the Association for Com-putational Linguistics conference, pages 148?153, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.Fumiyo Fukumoto and Yoshimi Suzuki.
2004.
Correct-ing category errors in text classification.
In COLING?04: Proceedings of the 20th international conferenceon Computational Linguistics, page 868, Morristown,NJ, USA.
Association for Computational Linguistics.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:the 90% solution.
In NAACL ?06: Proceedings ofthe Human Language Technology Conference of theNAACL, Companion Volume: Short Papers on XX,pages 57?60, Morristown, NJ, USA.
Association forComputational Linguistics.Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.2009.
Data quality from crowdsourcing: a study ofannotation selection criteria.
In HLT ?09: Proceedingsof the NAACL HLT 2009 Workshop on Active Learningfor Natural Language Processing, pages 27?35, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Pavel Kve?ton?
and Karel Oliva.
2002.
(semi-)automaticdetection of errors in pos-tagged corpora.
In Proceed-ings of the 19th international conference on Compu-tational linguistics, pages 1?7, Morristown, NJ, USA.Association for Computational Linguistics.Zhang Le, 2004.
Maximum Entropy Modeling Toolkit forPython and C++.David D. Lewis and William A. Gale.
1994.
A sequentialalgorithm for training text classifiers.
In SIGIR ?94:Proceedings of the 17th annual international ACM SI-GIR conference on Research and development in in-formation retrieval, pages 3?12, New York, NY, USA.Springer-Verlag New York, Inc.Dingsheng Luo, Xinhao Wang, Xihong Wu, andHuisheng Chi.
2005.
Learning outliers to refine a cor-pus for chinese webpage categorization.
In ICNC (1),pages 167?178.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of english: the penn treebank.
Comput.
Linguist.,19(2):313?330.Tetsuji Nakagawa and Yuji Matsumoto.
2002.
Detect-ing errors in corpora using support vector machines.In Proceedings of the 19th international conferenceon Computational linguistics, pages 1?7, Morristown,NJ, USA.
Association for Computational Linguistics.Va?clav Nova?k and Magda Raz??mova?.
2009.
Unsu-pervised detection of annotation inconsistencies usingapriori algorithm.
In ACL-IJCNLP ?09: Proceedingsof the Third Linguistic Annotation Workshop, pages138?141, Morristown, NJ, USA.
Association for Com-putational Linguistics.Fredrik Olsson.
2009.
A literature survey of activemachine learning in the context of natural languageprocessing.
In Technical Report, Swedish Institute ofComputer Science.John C. Platt and John C. Platt.
1999.
Probabilistic out-puts for support vector machines and comparisons toregularized likelihood methods.
In Advances in LargeMargin Classifiers, pages 61?74.
MIT Press.Burr Settles.
2009.
Active learning literature survey.
InComputer Sciences Technical Report 1648 Universityof Wisconsin-Madison.Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeiro-tis.
2008.
Get another label?
improving data qual-ity and data mining using multiple, noisy labelers.
InKDD ?08: Proceeding of the 14th ACM SIGKDD in-ternational conference on Knowledge discovery anddata mining, pages 614?622, New York, NY, USA.ACM.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In EMNLP ?08: Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, pages 254?263, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Hans van Halteren.
2000.
The detection of inconsistencyin manually tagged text.
In Proceedings of LINC-00,Luxembourg.Z.
Zhong, H.T.
Ng, and Y.S.
Chan.
2008.
Word sensedisambiguation using OntoNotes: An empirical study.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 1002?1010.
Association for Computational Linguistics.Jingbo Zhu.
2007.
Active learning for word sense disam-biguation with methods for addressing the class imbal-ance problem.
In In Proceedings of ACL, pages 783?790.73
