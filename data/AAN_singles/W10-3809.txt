Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 66?74,COLING 2010, Beijing, August 2010.A Discriminative Approach for Dependency BasedStatistical Machine TranslationSriram VenkatapathyLTRC, IIIT-Hyderabadsriram@research.iiit.ac.in sangal@mail.iiit.ac.inRajeev SangalLTRC, IIIT-HyderabadAravind JoshiUniversity of Pennsylvaniajoshi@seas.upenn.edu karthik.gali@gmail.comKarthik Gali1TalenticaAbstractIn this paper, we propose a dependencybased statistical system that uses discrim-inative techniques to train its parameters.We conducted experiments on an English-Hindi parallel corpora.
The use of syntax(dependency tree) allows us to address thelarge word-reorderings between Englishand Hindi.
And, discriminative trainingallows us to use rich feature sets, includ-ing linguistic features that are useful in themachine translation task.
We present re-sults of the experimental implementationof the system in this paper.1 IntroductionSyntax based approaches for Machine Translation(MT) have gained popularity in recent times be-cause of their ability to handle long distance re-orderings (Wu, 1997; Yamada and Knight, 2002;Quirk et al, 2005; Chiang, 2005), especially fordivergent language pairs such as English-Hindi(or English-Urdu).
Languages such as Hindi arealso known for their rich morphology and longdistance agreement of features of syntactically re-lated units.
The morphological richness can behandled by employing techniques that factor thelexical items into morphological factors.
Thisstrategy is also useful in the context of English-Hindi MT (Bharati et al, 1997; Bharati et al,1This work was done at LTRC, IIIT-Hyderabad, when hewas a masters student, till July 20082002; Ananthakrishnan et al, 2008; Ramanathanet al, 2009) where there is very limited paral-lel corpora available, and breaking words intosmaller units helps in reducing sparsity.
In or-der to handle phenomenon such as long-distanceword agreement to achieve accurate generation oftarget language words, the inter-dependence be-tween the factors of syntactically related wordsneed to be modelled effectively.Some of the limitations with the syntax basedapproaches such as (Yamada and Knight, 2002;Quirk et al, 2005; Chiang, 2005) are, (1) Theydo not offer flexibility for adding linguisticallymotivated features, and (2) It is not possible touse morphological factors in the syntax based ap-proaches.
In a recent work (Shen et al, 2009), lin-guistic and contextual information was effectivelyused in the framework of a hierarchical machinetranslation system.
In their work, four linguisticand contextual features are used for accurate se-lection of translation rules.
In our approach incontrast, linguistically motivated features can bedefined that directly effect the prediction of var-ious elements in the target during the translationprocess.
This features use syntactic labels and col-location statistics in order to allow effective train-ing of the model.Some of the other approaches related to ourmodel are the Direct Translation Model 2 (DTM2)(Ittycheriah and Roukos, 2007), End-to-End Dis-criminative Approach to MT (Liang et al, 2006)and Factored Translation Models (Koehn andHoang, 2007).
In DTM2, a discriminative trans-66lation model is defined in the setting of a phrasebased translation system.
In their approach, thefeatures are optimized globally.
In contrast totheir approach, we define a discriminative modelfor translation in the setting of a syntax based ma-chine translation system.
This allows us to useboth the power of a syntax based approach, aswell as, the power of a large feature space duringtranslation.
In our approach, the weights are op-timized in order to achieve an accurate predictionof the individual target nodes, and their relativepositions.We propose an approach for syntax based sta-tistical machine translation which models the fol-lowing aspects of language divergence effectively.?
Word-order variation including long-distance reordering which is prevalentbetween language pairs such as English-Hindi and English-Japanese.?
Generation of word-forms in the target lan-guage by predicting the word and its factors.During prediction, the inter-dependence offactors of the target word form with the fac-tors of syntactically related words is consid-ered.To accomplish this goal, we visualize the prob-lem of MT as transformation from a morpho-logically analyzed source syntactic structure to atarget syntactic structure1 (See Figure 1).
Thetransformation is factorized into a series of mini-transformations, which we address as features ofthe transformation.
The features denote the vari-ous linguistic modifications in the source structureto obtain the target syntactic structure.
Some ofthe examples of features are lexical translation ofa particular source node, the ordering at a particu-lar source node etc.
These features can be entirelylocal to a particular node in the syntactic structureor can span across syntactically related entities.More about the features (or mini-transformations)is explained in section 3.
The transformation ofa source syntactic structure is scored by taking aweighted sum of its features 2.
Let ?
represent1Note that target structure contains only the target fac-tors.
An accurate and deterministic morphological generatorcombines these factors to produce the target word form.2The features can be either binary-values or real-valuedthe transformation of source syntactic structure s,the score of transformation is computed as repre-sented in Equation 1.score(?
|s) =?iwi ?
fi(?, s) (1)In Equation 1, f ?is are the various features oftransformation and w?is are the weights of the fea-tures.
The strength of our approach lies in the flex-ibility it offers in incorporating linguistic featuresthat are useful in the task of machine translation.These features are also known as prediction fea-tures as they map from source language informa-tion to information in the target language that isbeing predicted.During decoding a source sentence, the goalis to choose a transformation that has the high-est score.
The source syntactic structure is tra-versed in a bottom-up fashion and the target syn-tactic structure is simultaneously built.
We useda bottom-up traversal while decoding because itbuilds a contiguous sequence of nodes for the sub-trees during traversal enabling the application of awide variety of language models.In the training phase, the task is to learn theweights of features.
We use an online large-margin training algorithm, MIRA (Crammer etal., 2005), for learning the weights.
The weightsare locally updated at every source node duringthe bottom-up traversal of the source structure.For training the translation model, automaticallyobtained word-aligned parallel corpus is used.
Weused GIZA++ (Och and Ney, 2003) along with thegrowing heuristics to word-align the training cor-pus.The basic factors of the word used in our exper-iments are root, part-of-speech, gender, numberand person.
In Hindi, common nouns and verbshave gender information whereas, English doesn?tcontain that information.
Apart from the basicfactors, we also consider the role information pro-vided by labelled dependency parsers.
For com-puting the dependency tree on the source side, Weused stanford parser (Klein and Manning, 2003)in the experiments presented in this chapter3.3Stanford parser gives both the phrase-structure tree aswell as dependency relations for a sentence.67root=mila,   tense=PASTgnp=m3sgroot=segnp=x3sgroot=raamgnp=m1sgroot=shyaamgnp=m1sgroot=pay,   tense=PASTgnp=x3sg,  role=Xpaid/VBDroot=Ram,  gnp=x1sgRam/NNProle=subjvisit/NNroot=visit,  gnp=x3sgrole=obj role=vmodroot=to,     gnp=x3sgto/TOroot=Shyam,  gnp=x1sgrole=pmodShyam/NNProot=a,    gnp=x3sgrole=nmoda/DTFigure 1: Transformation from source structure to target languageThe function words such as prepositions andauxiliary verbs largely express the grammaticalroles/functions of the content words in the sen-tence.
In fact, in many agglutinative languages,these words are commonly attached to the con-tent word to form one word form.
In this pa-per, we also conduct experiments where we beginby grouping the function words with their corre-sponding function words.
These groups of wordsare called local-word groups.
In these cases, thefunction words are considered as factors of thecontent words.
Section 2 explains more about thelocal word groups in English and Hindi.2 Local Word GroupsLocal word groups (LWGs) (Bharati et al, 1998;Vaidya et al, 2009) consist of a content word andits associated function words.
Local word group-ing reduces a sentence to a sequence of contentwords with the case-markers and tense-markersacting as their factors.
For example, consideran English sentence ?People of these island haveadopted Hindi as a means of communication?.
?have adopted?
is a LWG with root ?adopt?
andtense markers being ?have ed?.
Another examplefor the LWG will be ?of communication?
where?communication?
is the root, and ?of?
is the case-marker.
It is to be noted that Local word groupingis different from chunking, where more than onecontent word can be part of a chunk.
We obtain lo-cal word groups in English by processing the out-put of the stanford parser.
In Hindi, the functionwords always appear immediately after the con-tent word4, and it requires simple patternmatching to obtain the LWGs.
The rules ap-plied are, (1) VM (RB|VAUX)+, and (2) N.* IN.3 FeaturesThere are three types of transformation featuresexplored by us, (1) Local Features, (2) SyntacticFeatures and, (3) Contextual Features.
In this sec-tion, we describe each of these categories of fea-tures representing different aspects of transforma-tion with examples.3.1 Local FeaturesThe local features capture aspects of local trans-formation of an atomic treelet in the sourcestructure to an atomic treelet in the target lan-guage.
Atomic treelet is a semantically non-decomposible group of one or more nodes in thesyntactic structure.
It usually contains only onenode, except for the case of multi-word expres-sions (MWEs).
Figure 2 presents the examples oflocal transformation.Some of the local features used by us in our ex-periments are (1) dice coefficient, (2) dice coeffi-cient of roots, (3) dice coefficient of null transla-tions, (4) treelet translation probability, (5) gnp-gnp pair, (5) preposition-postposition pair, (6)tense-tense pair, (7) part-of-speech fertility etc.Dice coefficients and treelet translation probabil-ities are measures that express the statistical co-occurrence of the atomic treelets.4case-markers are called postpositions68root=Ram,  gnp=x1sgRam/NNProle=subjroot=pay,   tense=PASTgnp=x3sg,  role=Xpaid/VBDvisit/NNroot=visit,  gnp=x3sgrole=objroot=mila,   tense=PASTgnp=m3sgroot=raamgnp=m1sgFigure 2: Local transformations3.2 Syntactic FeaturesThe syntactic features are used to model the differ-ence in the word orders of the two languages.
Atevery node of the source syntactic structure, thesefeatures define the changes in the relative orderof children during the process of transformation.They heavily use source information such as part-of-speech tags and syntactic roles of the sourcenodes.
One of the features used is reorderPostags.This feature captures the change in relative po-sitions of children with respect to their parentsduring the tree transformation.
An example fea-ture for the transformation given in Figure 1 isshown in Figure 3.IN   NNPVBNNPVBTOFigure 3: Syntactic feature - reorder postagsThe feature reorderPostags is in the form of acomplete transfer rule.
To handle cases, where theleft-hand side of ?reorderPostags?
does not matchthe syntactic structure of the source tree, the sim-pler feature functions are used to qualify variousreorderings.
Instead of using POS tags, featurefunctions can be defined that use syntactic roles.Apart from the above feature functions, we canalso have features that compute the score of a par-ticular order of children using syntactic languagemodels (Gali and Venkatapathy, 2009; Guo et al,2008).
Different features can be defined that usedifferent levels of information pertaining to theatomic treelet and its children.3.3 Contextual FeaturesContextual features model the inter-dependenceof factors of nodes connected by dependency arcs.These features are used to enable access to globalinformation for prediction of target nodes (wordsand its factors).One of the features diceCoeffParent, relates theparent of a source node to the corresponding targetnode (see figure 4.x1x2x3 x4ydiceFigure 4: Use of Contextual (parent) informationof x2 for generation of yThe use of this feature is expected to address ofthe limitations of using ?atomic treelets?
as the ba-sic units in contrast to phrase based systems whichconsider arbitrary sequences of words as units toencode the local contextual information.
In mycase, We relate the target treelet with the contex-tual information of the source treelet using featurefunctions rather than using larger units.
Similarfeatures are used to connect the context of a sourcenode to the target node.Various feature functions are defined to han-dle interaction between the factors of syntacti-cally related treelets.
The gender-number-personagreement is a factor that is dependent of gender-number-person factors of the syntactically relatedtreelets in Hindi.
The rules being learnt hereare simple.
However, more complex interac-tions can also be handled though features such asprep Tense where, the case-marker in the target islinked to the tense of parent verb.4 DecodingThe goal is to compute the most probable targetsentence given a source sentence.
First, the sourcesentence is analyzed using a morphological ana-lyzer5, local word grouper (see section 2) and adependency parser.
Given the source structure,the task of the decoding algorithm is to choose thetransformation that has the maximum score.5http://www.cis.upenn.edu/?xtag/69The dependency tree of the source languagesentence is traversed in a bottom-up fashion forbuilding the target language structure.
At everysource node during the traversal, the local trans-formation is first computed.
Then, the relative or-der of its children is then computed using the syn-tactic features.
This results in a target structureassociated with the subtree rooted at the particularnode.
The target structure associated with the rootnode of the source structure is the result of the besttransformation of the entire source structure.Hence, the task of computing the best transfor-mation of the entire source structure is factorizedinto the tasks of computing the best transforma-tions of the source treelets.
The equation for com-puting the score of a transformation, Equation 1,can be modified as Equation 2 given below.score(?
|s) =?r|r| ?
?iwi ?
fi(?r, r) (2)where, ?j is the local transformation of thesource treelet r. The best transformation ??
ofsource sentence s is,??
= argmax?
score(?
|s) (3)5 Training AlgorithmThe goal of the training algorithm is to learn thefeature weights from the word aligned corpus.
Forword-alignment, we used the IBM Model 5 imple-mented in GIZA++ along with the growing heuris-tics (Koehn et al, 2003).
The gold atomic treeletsin the source and their transformation is obtainedby mapping the source node to the target using theword-alignment information.
This information isstored in the form of transformation tables that isused for the prediction of target atomic treelets,prepositions and other factors.
The transformationtables are pruned in order to limit the search andeliminate redundant information.
For each sourceelement, only the top few entries are retained inthe table.
This limit ranges from 3 to 20.We used an online-large margin algorithm,MIRA (McDonald and Pereira, 2006; Crammeret al, 2005), for updating the weights.
Duringparameter optimization, it is sometimes impossi-ble to achieve the gold transformation for a nodebecause the pruned transformation tables may notlead to the target gold prediction for the sourcenode.
In such cases where the gold transforma-tion is unreachable, the weights are not updatedat all for the source node as it might cause erro-neous weight updates.
We conducted our exper-iments by considering both the cases, (1) Identi-fying source nodes with unreachable transforma-tions, and (2) Updating weights for all the sourcenodes (till a maximum iteration limit).
The num-ber of iterations on the entire corpus can also befixed.
Typically, two iterations have been found tobe sufficient to train the model.The dependency tree is traversed in a bottom-upfashion and the weights are updated at each sourcenode.6 Experiments and ResultsThe important aspects of the translation modelproposed in this paper have been implemented.Some of the components that handle word in-sertions and non-projective transformations havenot yet been implemented in the decoder, andshould be considered beyond the scope of thispaper.
The focus of this work has been tobuild a working syntax based statistical machinetranslation system, which can act as a plat-form for further experiments on similar lines.The system would be available for downloadat http://shakti.iiit.ac.in/?sriram/vaanee.html.
Toevaluate this experimental system, a restrictedset of experiments are conducted.
The experi-ments are conducted on the English-Hindi lan-guage pair using a corpus in tourism domain con-taining 11300 sentence pairs6.6.1 Training6.1.1 ConfigurationFor training, we used DIT-TOURISM-ALIGN-TRAIN dataset which is the word-aligned datasetof 11300 sentence pairs.
The word-alignment isdone using GIZA++ (Och and Ney, 2003) toolkitand then growing heuristics are applied.
Forour experiments, we use two growing heuristics,GROW-DIAG-FINAL-AND and GROW-DIAG-FINAL as they cover most number of words inboth the sides of the parallel corpora.6DIT-TOURISM corpus70Number of Training Sentences 500Iterations on Corpus 1-2Parameter optimization algorithm MIRABeam Size 1-20Maximum update attempts at source node 1-4Unreachable updates FalseSize of transformation tables 3Table 1: Training ConfigurationThe training of the model can be performed un-der different configurations.
The configurationsthat we used for the training experiments are givenin Table 6.1.1.6.2 ResultsFor the complete training, the number of sen-tences that should be used for the best perfor-mance of the decoder should be the complete set.In the paper, we have conducted experiments byconsidering 500 training sentences to observe thebest training configuration.At a source node, the weight vector is itera-tively updated till the system predicts the goldtransformation.
We conducted experiments by fix-ing the maximum number of update attempts.
Asource node, where the gold transformation is notachieved even after the maximum updates limit,the update at this source node is termed a updatefailure.
The source nodes, where the gold trans-formation is achieved even without making anyupdates is known as the correct prediction.At some of the source nodes, it is not possibleto arrive at the gold target transformation becauseof limited size of the training corpus.
At suchnodes, we have avoided doing any weight update.As the desired transformation is unachievable, anyattempt to update the weight vector would causenoisy weight updates.We observe various parameters to check the ef-fectiveness of the training configuration.
One ofthe parameters (which we refer to as ?updateHits?
)computes the number of successful updates (S)performed at the source nodes in contrast to num-ber of failed updates (F ).
Successful updates re-sult in the prediction of the transformation that issame as the reference transformation.
A failed up-date doesn?t result in the achievement of the cor-rect prediction even after the maximum iterationlimit (see section 6.1.1) is reached.
At some of thesource nodes, the reference transformations areunreachable (U ).
The goal is to choose the con-figuration that has least number of average failedupdates (F ) because it implies that the model hasbeen learnt effectively.UpdateHitK m P S F U1.
1 4 1680 2692 84 40812.
5 4 1595 2786 75 40813.
10 4 1608 2799 49 40814.
20 4 1610 2799 47 4081Table 2: Training Statistics - Effect of Beam SizeFrom Table 2, we can see that the bigger beamsize leads to a better training of the model.
Thebeam size was varied between 1 and 20, and thenumber of update failures (F ) was observed to beleast at K=20.UpdateHitK m P S F U1.
20 1 1574 2724 158 40812.
20 2 1598 2767 91 40813.
20 4 1610 2799 47 4081Table 3: Training Statistics - Effect of maximumupdate attemptsIn Table 3, we can see that an higher limit onthe maximum number of update attempts resultsin less number of update attempts as expected.
Amuch higher value of m is not preferable becausethe training updates makes noisy updates in caseof difficult nodes i.e., the nodes where target trans-formation is reachable in theory, but is unreach-able given the set of features.UpdateHitK i P S F U1.
1 1 1680 2692 84 40812.
1 2 1679 2694 83 4081Table 4: Training Statistics - Effect of number ofiterationsNow, we examine the effect of number of it-71erations on the quality of the model.
In table 4,we can observe that the number of iterations onthe data has no effect on the quality of the model.This implies, that the model is adequately learntafter one pass through the data.
This is possiblebecause of the multiple number of update attemptsallowed at every node.
Hence, the weights are up-dated at a node till the model prediction is consis-tent with the gold transformation.Based on the above observations, we considerthe configuration 4 in Table 2 for the decoding ex-periments.Now, we present some of the top featuresweights leant by the best configuration.
Theweights convey that important properties of trans-formation are being learnt well.
Table 5 presentsthe weights of the features ?diceRoot?, ?dice-RootChildren?
and ?diceRootParent?.Feature Weightdice 75.67diceChildren 540.31diceParent 595.94treelet translation probability (ttp) 1 0.77treelet translation probability (ttp) 2 389.62Table 5: Weights of dice coefficient based featuresWe see that the dice coefficient based local andcontextual features have a positive impact on theselection of correct transformations.
A featurethat uses a syntactic language model to computethe perplexity per word has a negative weight of-1.115.Table 6 presents the top-5 entries of contex-tual features that describe the translation of sourceargument ?nsubj?
using contextual information(?tense?
of its parent).Feature WeightroleTenseVib:nsubj+NULL NULL 44.194196513246roleTenseVib:nsubj+has VBN ne 14.4541356715382roleTenseVib:nsubj+VBD ne 10.9241093097953roleTenseVib:nsubj+VBP meM 6.14149937079584roleTenseVib:nsubj+VBP NULL 4.76795730621754Table 6: Top weights of a contextual feature :preposition+Tense-postpositionTable 7 presents the top-10 ordering relative po-sition feature where the head word is a verb.
Inthis feature, the relative position (left or right) ofthe head and the child is captured.
For example, afeature ?relPos:amod-NN?, if active, conveys thatan argument with the role ?amod?
is at the left ofa head word with POS tag ?NN?.Feature WeightrelPos:amod-NN 6.70relPos:NN-appos 1.62relPos:lrb-NN 1.62Table 7: Top weights of relPos feature6.3 DecodingWe computed the translation accuracies using twometrics, (1) BLEU score (Papineni et al, 2002),and (2) Lexical Accuracy (or F-Score) on a testset of 30 sentences.
We compared the accuracyof the experimental system (Vaanee) presented inthis paper, with Moses (state-of-the-art translationsystem) and Shakti (rule-based translation system7) under similar conditions (with using a develop-ment set to tune the models).
The rule-based sys-tem considered is a general domain system tunedto the tourism domain.
The best BLEU score forMoses on the test set is 0.118, and the best lexi-cal accuracy is 0.512.
The best BLEU score forShakti is 0.054, and the best lexical accuracy is0.369.In comparison, the best BLEU score of Vaaneeis 0.067, while the best lexical accuracy is 0.445.As observed, the decoding results of the experi-mental system mentioned here are not yet compa-rable to the state-of-art.
The main reasons for thelow translation accuracies are,1.
Poor Quality of the datasetThe dataset currently available for English-Hindi language pair is noisy.
This is anextremely large limiting factor for a modelwhich uses rich linguistic information withinthe statistical framework.2.
Low Parser accuracy7http://shakti.iiit.ac.in/72The parser accuracy on the English-Hindidataset is low, the reasons being, (1) Noise,(2) Length of sentences, and (3) Wide scopeof the tourism domain.3.
Word insertions not implemented yet4.
Non-projectivity not yet handled5.
BLEU is not an appropriate metricBLEU is not an appropriate metric (Anan-thakrishnan et al, ) for measuring the trans-lation accuracy into Indian languages.6.
Model is context free as far as targets wordsare concerned.
Selection depends on chil-dren but not parents and siblingsThis point concerns the decoding algorithm.The current algorithm is greedy while chos-ing the best translation at every source node.It first explores the K-best local transforma-tions at a source node.
It then makes a greedyselection of the predicted subtree based onit?s overall score after considering the predic-tions at the child nodes, and the relative posi-tion of the local transformation with respectthe predictions at the child nodes.The problem in this approach is that, an er-ror once made at a lower level of the treeis propogated to the top, causing more mis-takes.
A computationally reasonable solutionto this problem is to maintain a K-best listof predicted subtrees corresponding to everysource node.
This allows rectification of amistake made at any stage.The system, however, performs better than therule based system.
As observed earlier, the righttype of information is being learnt by the model,and the approach looks promising.
The limitationsexpressed here shall be addressed in the future.7 ConclusionIn this work, we presented a syntax based de-pendency model to effectively handle problems intranslation from English to Indian languages suchas, (1) Large word order variation, and (2) Ac-curate generation of word-forms in the target lan-guage by predicted the word and its factors.
Themodel that we have proposed, has the flexibility ofadding rich linguistic features.An experimental version of the system has beenimplemented, which is available for download athttp://shakti.iiit.ac.in/?sriram/vaanee.html.
Thiscan facilitate as a platform for future research insyntax based statistical machine translation fromEnglish to Indian languages.
We also plan to per-form experiments using this system between Eu-ropean languages in future.The performance of the implemented transla-tion system, is not yet comparable to the state-of-art results primarily for two reasons, (1) Poorquality of available data, because of which ourmodel which uses rich linguistic informationdoesn?t perform as expected, and (2) Componentsfor word insertion and non-projectivity handlingare yet to be implemented in this version of thesystem.ReferencesAnanthakrishnan, R, B Pushpak, M Sasikumar, andRitesh Shah.
Some issues in automatic evaluationof english-hindi mt: more blues for bleu.
ICON-2007.Ananthakrishnan, R., Jayprasad Hegde, Pushpak Bhat-tacharyya, and M. Sasikumar.
2008.
Simple syntac-tic and morphological processing can help english-hindi statistical machine translation.
In Proceedingsof IJCNLP-2008.
IJCNLP.Bharati, Akshar, Vineet Chaitanya, Amba P Kulkarni,and Rajeev Sangal.
1997.
Anusaaraka: Machinetranslation in stages.
A Quarterly in Artificial Intel-ligence, NCST, Bombay (renamed as CDAC, Mum-bai).Bharati, Akshar, Medhavi Bhatia, Vineet Chaitanya,and Rajeev Sangal.
1998.
Paninian grammarframework applied to english.
South Asian Lan-guage Review, (3).Bharati, Akshar, Rajeev Sangal, Dipti M Sharma, andAmba P Kulkarni.
2002.
Machine translation activ-ities in india: A survey.
In Proceedings of workshopon survey on Research and Development of MachineTranslation in Asian Countries.Chiang, David.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL?05), pages263?270, Ann Arbor, Michigan, June.
Associationfor Computational Linguistics.73Crammer, K., R. McDonald, and F. Pereira.
2005.Scalable large-margin online learning for structuredclassification.
Technical report, University of Penn-sylvania.Gali, Karthik and Sriram Venkatapathy.
2009.
Sen-tence realisation from bag of words with depen-dency constraints.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, Companion Volume:Student Research Workshop and Doctoral Consor-tium, pages 19?24, Boulder, Colorado, June.
Asso-ciation for Computational Linguistics.Guo, Yuqing, Josef van Genabith, and Haifeng Wang.2008.
Dependency-based n-gram models for gen-eral purpose sentence realisation.
In Proceedingsof the 22nd International Conference on Compu-tational Linguistics (Coling 2008), pages 297?304,Manchester, UK, August.
Coling 2008 OrganizingCommittee.Ittycheriah, Abraham and Salim Roukos.
2007.
Di-rect translation model 2.
In Human Language Tech-nologies 2007: The Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics; Proceedings of the Main Conference,pages 57?64, Rochester, New York, April.
Associa-tion for Computational Linguistics.Klein, Dan and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Com-putational Linguistics, pages 423?430, Sapporo,Japan, July.
Association for Computational Linguis-tics.Koehn, Philipp and Hieu Hoang.
2007.
Factoredtranslation models.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 868?876.Koehn, P., F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proceedings of the Hu-man Language Technology Conference 2003 (HLT-NAACL 2003), Edmonton, Canada, May.Liang, P., A. Bouchard-Cote, D. Klein, and B. Taskar.2006.
An end-to-end discriminative approach tomachine translation.
In International Conferenceon Computational Linguistics and Association forComputational Linguistics (COLING/ACL).McDonald, R. and F. Pereira.
2006.
Online learningof approximate dependency parsing algorithms.
InEACL.Och, F.J. and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.Papineni, Kishore, Salim Roukos, Todd Ward, and W.J.Zhu.
2002.
Bleu: A method for automatic eval-uation of machine translation.
In Proceedings of40th Annual Meeting of the Association of Compu-tational Linguistics, pages 313?318, Philadelphia,PA, July.Quirk, Chris, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal SMT.
In Proceedings of the 43rdAnnual Meeting of the Association for Computa-tional Linguistics (ACL?05), pages 271?279, AnnArbor, Michigan, June.
Association for Computa-tional Linguistics.Ramanathan, Ananthakrishnan, Hansraj Choudhary,Avishek Ghosh, and Pushpak Bhattacharyya.
2009.Case markers and morphology: Addressing the cruxof the fluency problem in english-hindi smt.
In Pro-ceedings of ACL-IJCNLP 2009.
ACL-IJCNLP.Shen, Libin, Jinxi Xu, Bing Zhang, Spyros Matsoukas,and Ralph Weischedel.
2009.
Effective use of lin-guistic and contextual information for statistical ma-chine translation.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 72?80, Singapore, August.
Asso-ciation for Computational Linguistics.Vaidya, Ashwini, Samar Husain, Prashanth Reddy, andDipti M Sharma.
2009.
A karaka based annotationscheme for english.
In Proceedings of CICLing ,2009.Wu, Dekai.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Cor-pora.
Computational Linguistics, 23(3):377?404.Yamada, Kenji and Kevin Knight.
2002.
A decoderfor syntax-based statistical mt.
In Proceedings of40th Annual Meeting of the Association for Compu-tational Linguistics, pages 303?310, Philadelphia,Pennsylvania, USA, July.
Association for Computa-tional Linguistics.74
