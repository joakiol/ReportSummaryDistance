Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 93?96,New York, June 2006. c?2006 Association for Computational LinguisticsNER Systems that Suit User?s Preferences: Adjusting the Recall-PrecisionTrade-off for Entity ExtractionEinat Minkov, Richard C. WangLanguage TechnologiesInstituteCarnegie Mellon Universityeinat,rcwang@cs.cmu.eduAnthony TomasicInst.
for Software ResearchInternationalCarnegie Mellon Universitytomasic@cs.cmu.eduWilliam W. CohenMachine Learning Dept.Carnegie Mellon Universitywcohen@cs.cmu.eduAbstractWe describe a method based on ?tweak-ing?
an existing learned sequential classi-fier to change the recall-precision tradeoff,guided by a user-provided performancecriterion.
This method is evaluated onthe task of recognizing personal names inemail and newswire text, and proves to beboth simple and effective.1 IntroductionNamed entity recognition (NER) is the task of iden-tifying named entities in free text?typically per-sonal names, organizations, gene-protein entities,and so on.
Recently, sequential learning methods,such as hidden Markov models (HMMs) and con-ditional random fields (CRFs), have been used suc-cessfully for a number of applications, includingNER (Sha and Pereira, 2003; Pinto et al, 2003; Mc-callum and Lee, 2003).
In practice, these methodsprovide imperfect performance: precision and re-call, even for well-studied problems on clean well-written text, reach at most the mid-90?s.
Whileperformance of NER systems is often evaluated interms of F1 measure (a harmonic mean of preci-sion and recall), this measure may not match userpreferences regarding precision and recall.
Further-more, learned NER models may be sub-optimal alsoin terms of F1, as they are trained to optimize othermeasures (e.g., loglikelihood of the training data forCRFs).Obviously, different applications of NER havedifferent requirements for precision and recall.
Asystem might require high precision if it is designedto extract entities as one stage of fact-extraction,where facts are stored directly into a database.
Onthe other hand, a system that generates candidate ex-tractions which are passed to a semi-automatic cu-ration system might prefer higher recall.
In somedomains, such as anonymization of medical records,high recall is essential.One way to manipulate an extractor?s precision-recall tradeoff is to assign a confidence score to eachextracted entity and then apply a global threshold toconfidence level.
However, confidence thresholdingof this sort cannot increase recall.
Also, while confi-dence scores are straightforward to compute in manyclassification settings, there is no inherent mecha-nism for computing confidence of a sequential ex-tractor.
Culotta and McCallum (2004) suggest sev-eral methods for doing this with CRFs.In this paper, we suggest an alternative simplemethod for exploring and optimizing the relation-ship between precision and recall for NER systems.In particular, we describe and evaluate a techniquecalled ?extractor tweaking?
that optimizes a learnedextractor with respect to a specific evaluation met-ric.
In a nutshell, we directly tweak the threasholdterm that is part of any linear classifier, including se-quential extractors.
Though simple, this approachhas not been empirically evaluated before, to ourknowledge.
Further, although sequential extractorssuch as HMMs and CRFs are state-of-the-art meth-ods for tasks like NER, there has been little prior re-search about tuning these extractors?
performance tosuit user preferences.
The suggested algorithm op-timizes the system performance per a user-provided93evaluation criterion, using a linear search procedure.Applying this procedure is not trivial, since the un-derlying function is not smooth.
However, we showthat the system?s precision-recall rate can indeed betuned to user preferences given labelled data usingthis method.
Empirical results are presented for aparticular NER task?recognizing person names, forthree corpora, including email and newswire text.2 Extractor tweakingLearning methods such as VP-HMM and CRFs op-timize criteria such as margin separation (implicitlymaximized by VP-HMMs) or log-likelihood (ex-plicitly maximized by CRFs), which are at best indi-rectly related to precision and recall.
Can such learn-ing methods be modified to more directly reward auser-provided performance metric?In a non-sequential classifier, a threshold on confi-dence can be set to alter the precision-recall tradeoff.This is nontrivial to do for VP-HMMs and CRFs.Both learners use dynamic programming to find thelabel sequence y = (y1, .
.
.
, yi, .
.
.
, yN ) for a wordsequence x = (x1, .
.
.
, xi, .
.
.
, xN ) that maximizesthe function W ?
?i f(x, i, yi?1, yi) , where W isthe learned weight vector and f is a vector of fea-tures computed from x, i, the label yi for xi, and theprevious label yi?1.
Dynamic programming findsthe most likely state sequence, and does not outputprobability for a particular sub-sequence.
(Culottaand McCallum, 2004) suggest several ways to gen-erate confidence estimation in this framework.
Wepropose a simpler approach for directly manipulat-ing the learned extractor?s precision-recall ratio.We will assume that the labels y include one labelO for ?outside any named entity?, and let w0 be theweight for the feature f0, defined as follows:f0(x, i, yi?1, yi) ?
{1 if yi = O0 elseIf no such feature exists, then we will create one.The NER based on W will be sensitive to the valueof w0: large negative values will force the dynamicprogramming method to label tokens as inside enti-ties, and large positive values will force it to labelfewer entities1.1We clarify that w0 will refer to feature f0 only, and not toother features that may incorporate label information.We thus propose to ?tweak?
a learned NER byvarying the single parameter w0 systematically so asto optimize some user-provided performance metric.Specifically, we tune w0 using a a Gauss-Newtonline search, where the objective function is itera-tively approximated by quadratics.2 We terminatethe search when two adjacent evaluation results arewithin a 0.01% difference3.A variety of performance metrics might be imag-ined: for instance, one might wish to optimize re-call, after applying some sort of penalty for pre-cision below some fixed threshold.
In this paperwe will experiment with performance metrics basedon the (complete) F-measure formula, which com-bines precision and recall into a single numeric valuebased on a user-provided parameter ?
:F (?, P,R) = (?2 + 1)PR?2P +RA value of ?
> 1 assigns higher importance to re-call.
In particular, F2 weights recall twice as muchas precision.
Similarly, F0.5 weights precision twiceas much as recall.We consider optimizing both token- and entity-level F?
?
awarding partial credit for partially ex-tracted entities and no credit for incorrect entityboundaries, respectively.
Performance is optimizedover the dataset on which W was trained, and testedon a separate set.
A key question our evaluationshould address is whether the values optimized forthe training examples transfer well to unseen test ex-amples, using the suggested approximate procedure.3 Experiments3.1 Experimental SettingsWe experiment with three datasets, of both emailand newswire text.
Table 1 gives summary statis-tics for all datasets.
The widely-used MUC-6 datasetincludes news articles drawn from the Wall StreetJournal.
The Enron dataset is a collection of emailsextracted from the Enron corpus (Klimt and Yang,2004), where we use a subcollection of the mes-sages located in folders named ?meetings?
or ?cal-endar?.
The Mgmt-Groups dataset is a second email2from http://billharlan.com/pub/code/inv.3In the experiments, this is usually within around 10 itera-tions.
Each iteration requires evaluating a ?tweaked?
extractoron a training set.94collection, extracted from the CSpace email cor-pus, which contains email messages sent by MBAstudents taking a management course conducted atCarnegie Mellon University in 1997.
This data wassplit such that its test set contains a different mix ofentity names comparing to training exmaples.
Fur-ther details about these datasets are available else-where (Minkov et al, 2005).# documents # namesTrain Test # tokens per doc.MUC-6 347 30 204,071 6.8Enron 833 143 204,423 3.0Mgmt-Groups 631 128 104,662 3.7Table 1: Summary of the corpora used in the experimentsWe used an implementation of Collins?
voted-percepton method for discriminatively trainingHMMs (henceforth, VP-HMM) (Collins, 2002) aswell as CRF (Lafferty et al, 2001) to learn a NER.Both VP-HMM and CRF were trained for 20 epochson every dataset, using a simple set of features suchas word identity and capitalization patterns for awindow of three words around each word being clas-sified.
Each word is classified as either inside or out-side a person name.43.2 Extractor tweaking ResultsFigure 1 evaluates the effectiveness of the optimiza-tion process used by ?extractor tweaking?
on theEnron dataset.
We optimized models for F?
withdifferent values of ?, and also evaluated each op-timized model with different F?
metrics.
The topgraph shows the results for token-level F?
, and thebottom graph shows entity-level F?
behavior.
Thegraph illustates that the optimized model does in-deed roughly maximize performance for the target?
value: for example, the token-level F?
curve forthe model optimized for ?
= 0.5 indeed peaks at?
= 0.5 on the test set data.
The optimization isonly roughly accurate5 for several possible reasons:first, there are differences between train and test sets;in addition, the line search assumes that the perfor-mance metric is smooth and convex, which neednot be true.
Note that evaluation-metric optimiza-tion is less successful for entity-level performance,4This problem encoding is basic.
However, in the context ofthis paper we focus on precision-recall trade-off in the generalcase, avoiding settings?
optimization.5E.g, the token-level F2 curve peaks at ?
= 5.5055606570758085905.02.01.00.50.2F(Beta)Beta0.20.51.02.05.05055606570758085905.02.01.00.50.2F(Beta)Beta0.20.51.02.05.0Figure 1: Results of token-level (top) and entity-level (bot-tom) optimization for varying values of ?, for the Enron dataset,VP-HMM.
The y-axis gives F in terms of ?.
?
(x-axis) is givenin a logarithmic scale.which behaves less smoothly than token-level per-formance.Token Entity?
Prec Recall Prec RecallBaseline 93.3 76.0 93.6 70.60.2 100 53.2 98.2 57.00.5 95.3 71.1 94.4 67.91.0 88.6 79.4 89.2 70.92.0 81.0 83.9 81.8 70.95.0 65.8 91.3 69.4 71.4Table 2: Sample optimized CRF results, for the MUC-6dataset and entity-level optimization.Similar results were obtained optimizing baselineCRF classifiers.
Sample results (for MUC-6 only,due to space limitations) are given in Table 2, opti-mizing a CRF baseline for entity-level F?
.
Note thatas ?
increases, recall monotonically increases andprecision monotonically falls.The graphs in Figure 2 present another set of re-sults with a more traditional recall-precision curves.The top three graphs are for token-level F?
opti-mization, and the bottom three are for entity-leveloptimization.
The solid lines show the token-leveland entity-level precision-recall tradeoff obtained by95MUC-6 Enron M.Groups506070809010050  60  70  80  90  100PrecisionRecallToken-levelEntity-levelToken-level baselineEntity-level baseline506070809010050  60  70  80  90  100Recall506070809010050  60  70  80  90  100Recall506070809010050  60  70  80  90  100PrecisionRecallToken-levelEntity-levelToken-level baselineEntity-level baseline506070809010050  60  70  80  90  100Recall506070809010050  60  70  80  90  100RecallFigure 2: Results for the evaluation-metric model optimization.
The top three graphs are for token-level F (?)
optimization,and the bottom three are for entity-level optimization.
Each graph shows the baseline learned VP-HMM and evaluation-metricoptimization for different values of ?, in terms of both token-level and entity-level performance.varying6 ?
and optimizing the relevant measure forF?
; the points labeled ?baseline?
show the precisionand recall in token and entity level of the baselinemodel, learned by VP-HMM.
These graphs demon-strate that extractor ?tweaking?
gives approximatelysmooth precision-recall curves, as desired.
Again,we note that the resulting recall-precision trade-off for entity-level optimization is generally lesssmooth.4 ConclusionWe described an approach that is based on mod-ifying an existing learned sequential classifier tochange the recall-precision tradeoff, guided by auser-provided performance criterion.
This approachnot only allows one to explore a recall-precisiontradeoff, but actually allows the user to specify aperformance metric to optimize, and optimizes alearned NER system for that metric.
We showedthat using a single free parameter and a Gauss-Newton line search (where the objective is itera-tively approximated by quadratics), effectively op-timizes two plausible performance measures, token-6We varied ?
over the values 0.2, 0.5, 0.8, 1, 1.2, 1.5, 2, 3and 5level F?
and entity-level F?
.
This approach is infact general, as it is applicable for sequential and/orstructured learning applications other than NER.ReferencesM.
Collins.
2002.
Discriminative training methods for hiddenmarkov models: Theory and experiments with perceptron al-gorithms.
In EMNLP.A.
Culotta and A. McCallum.
2004.
Confidence estimation forinformation extraction.
In HLT-NAACL.B.
Klimt and Y. Yang.
2004.
Introducing the Enron corpus.
InCEAS.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Conditionalrandom fields: Probabilistic models for segmenting and la-beling sequence data.
In ICML.A.
Mccallum and W. Lee.
2003. early results for named entityrecognition with conditional random fields, feature inductionand web-enhanced lexicons.
In CONLL.E.
Minkov, R. C. Wang, and W. W. Cohen.
2005.
Extractingpersonal names from emails: Applying named entity recog-nition to informal text.
In HLT-EMNLP.D.
Pinto, A. Mccallum, X. Wei, and W. B. Croft.
2003. tableextraction using conditional random fields.
In ACM SIGIR.F.
Sha and F. Pereira.
2003.
Shallow parsing with conditionalrandom fields.
In HLT-NAACL.96
