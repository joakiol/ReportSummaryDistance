Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 182?190,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsPhrase-Based Query Degradation Modeling forVocabulary-Independent Ranked Utterance RetrievalJ.
Scott OlssonHLT Center of ExcellenceJohns Hopkins UniversityBaltimore, MD 21211, USAsolsson@jhu.eduDouglas W. OardCollege of Information StudiesUniversity of MarylandCollege Park, MD 15213, USAoard@umd.eduAbstractThis paper introduces a new approach to rank-ing speech utterances by a system?s confi-dence that they contain a spoken word.
Multi-ple alternate pronunciations, or degradations,of a query word?s phoneme sequence are hy-pothesized and incorporated into the rankingfunction.
We consider two methods for hy-pothesizing these degradations, the best ofwhich is constructed using factored phrase-based statistical machine translation.
We showthat this approach is able to significantly im-prove upon a state-of-the-art baseline tech-nique in an evaluation on held-out speech.We evaluate our systems using three differ-ent methods for indexing the speech utter-ances (using phoneme, phoneme multigram,and word recognition), and find that degrada-tion modeling shows particular promise for lo-cating out-of-vocabulary words when the un-derlying indexing system is constructed withstandard word-based speech recognition.1 IntroductionOur goal is to find short speech utterances whichcontain a query word.
We accomplish this goalby ranking the set of utterances by our confidencethat they contain the query word, a task known asRanked Utterance Retrieval (RUR).
In particular,we are interested in the case when the user?s queryword can not be anticipated by a Large VocabularyContinuous Speech Recognizer?s (LVCSR) decod-ing dictionary, so that the word is said to be Out-Of-Vocabulary (OOV).Rare words tend to be the most informative, butare also most likely to be OOV.
When words areOOV, we must use vocabulary-independent tech-niques to locate them.
One popular approach is tosearch for the words in output from a phoneme rec-ognizer (Ng and Zue, 2000), although this suffersfrom the low accuracy typical of phoneme recogni-tion.
We consider two methods for handling this in-accuracy.
First, we compare an RUR indexing sys-tem using phonemes with two systems using longerrecognition units: words or phoneme multigrams.Second, we consider several methods for handlingthe recognition inaccuracy in the utterance rank-ing function itself.
Our baseline generative modelhandles errorful recognition by estimating term fre-quencies from smoothed language models trainedon phoneme lattices.
Our new approach, which wecall query degradation, hypothesizes many alternate?pronunciations?
for the query word and incorpo-rates them into the ranking function.
These degra-dations are translations of the lexical phoneme se-quence into the errorful recognition language, whichwe hypothesize using a factored phrase-based statis-tical machine translation system.Our speech collection is a set of oral historyinterviews from the MALACH collection (Byrneet al, 2004), which has previously been used forad hoc speech retrieval evaluations using one-bestword level transcripts (Pecina et al, 2007; Olsson,2008a) and for vocabulary-independent RUR (Ols-son, 2008b).
The interviews were conducted withsurvivors and witnesses of the Holocaust, who dis-cuss their experiences before, during, and after theSecond World War.
Their speech is predominatelyspontaneous and conversational.
It is often alsoemotional and heavily accented.
Because the speechcontains many words unlikely to occur within a gen-eral purpose speech recognition lexicon, it repre-182sents an excellent collection for RUR evaluation.We were graciously permitted to use BBN Tech-nology?s speech recognition system Byblos (Prasadet al, 2005; Matsoukas et al, 2005) for our speechrecognition experiments.
We train on approximately200 hours of transcribed audio excerpted from about800 unique speakers in the MALACH collection.
Toprovide a realistic set of OOV query words, we usean LVCSR dictionary previously constructed for adifferent topic domain (broadcast news and conver-sational telephone speech) and discard all utterancesin our acoustic training data which are not coveredby this dictionary.
New acoustic and language mod-els are trained for each of the phoneme, multigramand word recognition systems.The output of LVCSR is a lattice of recogni-tion hypotheses for each test speech utterance.
Alattice is a directed acyclic graph that is used tocompactly represent the search space for a speechrecognition system.
Each node represents a point intime and arcs between nodes indicates a word oc-curs between the connected nodes?
times.
Arcs areweighted by the probability of the word occurring,so that the so-called ?one-best?
path through the lat-tice (what a system might return as a transcription)is the path through the lattice having highest proba-bility under the acoustic and language models.
EachRUR model we consider is constructed using the ex-pected counts of a query word?s phoneme sequencesin these recognition lattices.
We consider three ap-proaches to producing these phoneme lattices, usingstandard word-based LVCSR, phoneme recognition,and LVCSR using phoneme multigrams.
Our wordsystem?s dictionary contains about 50,000 entries,while the phoneme system contains 39 phonemesfrom the ARPABET set.Originally proposed by Deligne and Bimbot(1997) to model variable length regularities instreams of symbols (e.g., words, graphemes, orphonemes), phoneme multigrams are short se-quences of one or more phonemes.
We produce aset of ?phoneme transcripts?
by replacing transcriptwords with their lexical pronunciation.
The set ofmultigrams is learned by then choosing a maximum-likelihood segmentation of these training phonemetranscripts, where the segmentation is viewed as hid-den data in an Expectation-Maximization algorithm.The set of all continuous phonemes occurring be-tween segment boundaries is then chosen as ourmultigram dictionary.
This multigram recognitiondictionary contains 16,409 entries.After we have obtained each recognition lat-tice, our indexing approach follows that of Olsson(2008b).
Namely, for the word and multigram sys-tems, we first expand lattice arcs containing multi-ple phones to produce a lattice having only singlephonemes on its arcs.
Then, we compute the ex-pected count of all phoneme n-grams n ?
5 in thelattice.
These n-grams and their counts are insertedin our inverted index for retrieval.This paper is organized as follows.
In Section 2we introduce our baseline RUR methods.
In Sec-tion 3 we introduce our query degradation approach.We introduce our experimental validation in Sec-tion 4 and our results in Section 5.
We find thatusing phrase-based query degradations can signifi-cantly improve upon a strong RUR baseline.
Finally,in Section 6 we conclude and outline several direc-tions for future work.2 Generative BaselineEach method we present in this paper ranks the ut-terances by the term?s estimated frequency withinthe corresponding phoneme lattice.
This generalapproach has previously been considered (Yu andSeide, 2005; Saraclar and Sproat, 2004), on the ba-sis that it provides a minimum Bayes-risk rankingcriterion (Yu et al, Sept 2005; Robertson, 1977) forthe utterances.
What differs for each method is theparticular estimator of term frequency which is used.We first outline our baseline approach, a generativemodel for term frequency estimation.Recall that our vocabulary-independent indicescontain the expected counts of phoneme sequencesfrom our recognition lattices.
Yu and Seide (2005)used these expected phoneme sequence counts to es-timate term frequency in the following way.
For aquery term Q and lattice L, term frequency t?fG isestimated as t?fG(Q,L) = P (Q|L) ?NL, where NLis an estimate for the number of words in the utter-ance.
The conditional P (Q|L) is modeled as an or-der M phoneme level language model,P?
(Q|L) =l?i=1P?
(qi|qi?M+1, .
.
.
, qi?1,L), (1)183so that t?fG(Q,L) ?
P?
(Q|L) ?
NL.
The probabil-ity of a query phoneme qj being generated, giventhat the phoneme sequence qj?M+1, .
.
.
, qj?1 =qj?1j?M+1 was observed, is estimated asP?
(qj |qj?1j?M+1,L) =EPL [C(qjj?M+1)]EPL [C(qj?1j?M+1)].Here, EPL [C(qj?1j?M+1)] denotes the expected countin lattice L of the phoneme sequence qj?1j?M+1.
Wecompute these counts using a variant of the forward-backward algorithm, which is implemented by theSRI language modeling toolkit (Stolcke, 2002).In practice, because of data sparsity, the languagemodel in Equation 1 must be modified to includesmoothing for unseen phoneme sequences.
We use abackoff M -gram model with Witten-Bell discount-ing (Witten and Bell, 1991).
We set the phonemelanguage model?s order to M = 5, which gave goodresults in previous work (Yu and Seide, 2005).3 Incorporating Query DegradationsOne problem with the generative approach is thatrecognition error is not modeled (apart from the un-certainty captured in the phoneme lattice).
The es-sential problem is that while the method hopes tomodel P (Q|L), it is in fact only able to model theprobability of one degradation H in the lattice, thatis P (H|L).
We define a query degradation as anyphoneme sequence (including the lexical sequence)which may, with some estimated probability, occurin an errorful phonemic representation of the audio(either a one-best or lattice hypothesis).
Because ofspeaker variation and because recognition is error-ful, we ought to also consider non-lexical degrada-tions of the query phoneme sequence.
That is, weshould incorporate P (H|Q) in our ranking function.It has previously been demonstrated that allow-ing for phoneme confusability can significantly in-crease spoken term detection performance on one-best phoneme transcripts (Chaudhari and Picheny,2007; Schone et al, 2005) and in phonemic lat-tices (Foote et al, 1997).
These methods work byallowing weighted substitution costs in minimum-edit-distance matching.
Previously, these substitu-tion costs have been maximum-likelihood estimatesof P (H|Q) for each phoneme, where P (H|Q) iseasily computed from a phoneme confusion matrixafter aligning the reference and one-best hypothesistranscript under a minimum edit distance criterion.Similar methods have also been used in other lan-guage processing applications.
For example, in (Ko-lak, 2005), one-for-one character substitutions, in-sertions and deletions were considered in a genera-tive model of errors in OCR.In this work, because we are focused on construct-ing inverted indices of audio files (for speed andto conserve space), we must generalize our methodof incorporating query degradations in the rankingfunction.
Given a degradation model P (H|Q), wetake as our ranking function the expectation of thegenerative baseline estimate NL ?
P?
(H|L) with re-spect to P (H|Q),t?fG(Q,L) =?H?H[P?
(H|L) ?NL]?P (H|Q), (2)where H is the set of degradations.
Note that, whilewe consider the expected value of our baseline termfrequency estimator with respect to P (H|Q), thisgeneral approach could be used with any other termfrequency estimator.Our formulation is similar to approaches takenin OCR document retrieval, using degradations ofcharacter sequences (Darwish and Magdy, 2007;Darwish, 2003).
For vocabulary-independent spo-ken term detection, perhaps the most closely re-lated formulation is provided by (Mamou and Ram-abhadran, 2008).
In that work, they ranked ut-terances by the weighted average of their match-ing score, where the weights were confidences froma grapheme to phoneme system?s first several hy-potheses for a word?s pronunciation.
The match-ing scores were edit distances, where substitutioncosts were weighted using phoneme confusability.Accordingly, their formulation was not aimed at ac-counting for errors in recognition per se, but ratherfor errors in hypothesizing pronunciations.
We ex-pect this accounts for their lack of significant im-provement using the method.Since we don?t want to sum over all possiblerecognition hypotheses H , we might instead sumover the smallest setH such that?H?H P (H|Q) ??.
That is, we could take the most probable degra-dations until their cumulative probability exceedssome threshold ?.
In practice, however, because184degradation probabilities can be poorly scaled, weinstead take a fixed number of degradations andnormalize their scores.
When a query is issued,we apply a degradation model to learn the top fewphoneme sequences H that are most likely to havebeen recognized, under the model.
In the machinetranslation literature, this process is commonly re-ferred to as decoding.We now turn to the modeling of query degrada-tions H given a phoneme sequence Q, P (H|Q).First, we consider a simple baseline approach in Sec-tion 3.1.
Then, in Section 3.2, we propose a morepowerful technique, using state-of-the-art machinetranslation methods to hypothesize our degradations.3.1 Baseline Query DegradationsSchone et al (2005) used phoneme confusion ma-trices created by aligning hypothesized and refer-ence phoneme transcripts to weight edit costs for aminimum-edit distance based search in a one-bestphoneme transcript.
Foote et al (1997) had previ-ously used phoneme lattices, although with ad hocedit costs and without efficient indexing.
In thiswork, we do not want to linearly scan each phonemelattice for our query?s phoneme sequence, preferringinstead to look up sequences in the inverted indicescontaining phoneme sequences.Our baseline degradation approach is related tothe edit-cost approach taken by (Schone et al,2005), although we generalize it so that it may beapplied within Equation 2 and we consider speechrecognition hypotheses beyond the one-best hypoth-esis.
First, we randomly generate N traversals ofeach phonemic recognition lattice.
These traver-sals are random paths through the lattice (i.e., westart at the beginning of the lattice and move to thenext node, where our choice is weighted by the out-going arcs?
probabilities).
Then, we align each ofthese traversals with its reference transcript using aminimum-edit distance criterion.
Phone confusionmatrices are then tabulated from the aggregated in-sertion, substitution, and deletion counts across alltraversals of all lattices.
From these confusion ma-trices, we compute unsmoothed estimates of P (h|r),the probability of a phoneme h being hypothesizedgiven a reference phoneme r.Making an independence assumption, our base-line degradation model for a query with mAY K M AA NVowel Consonant Semi-vowel Vowel Semi-vowelDipthong Voiceless plosive Nasal Back vowel NasalFigure 1: Three levels of annotation used by the factoredphrase-based query degradation model.phonemes is then P (H|Q) = ?mi=1 P (hi|ri).
Weefficiently compute the most probable degradationsfor a query Q using a lattice of possible degrada-tions and the forward backward algorithm.
We callthis baseline degradation approach CMQD (Confu-sion Matrix based Query Degradation).3.2 Phrase-Based Query DegradationOne problem with CMQD is that we only allow in-sertions, deletions, and one-for-one substitutions.
Itmay be, however, that certain pairs of phonemesare commonly hypothesized for a particular refer-ence phoneme (in the language of statistical machinetranslation, we might say that we should allow somenon-zero fertility).
Second, there is nothing to dis-courage query degradations which are unlikely un-der an (errorful) language model?that is, degrada-tions that are not observed in the speech hypothe-ses.
Finally, CMQD doesn?t account for similaritiesbetween phoneme classes.
While some of these de-ficiencies could be addressed with an extension toCMQD (e.g., by expanding the degradation latticesto include language model scores), we can do bet-ter using a more powerful modeling framework.
Inparticular, we adopt the approach of phrase-basedstatistical machine translation (Koehn et al, 2003;Koehn and Hoang, 2007).
This approach allowsfor multiple-phoneme to multiple-phoneme substi-tutions, as well as the soft incorporation of addi-tional linguistic knowledge (e.g., phoneme classes).This is related to previous work allowing higher or-der phoneme confusions in bigram or trigram con-texts (Chaudhari and Picheny, 2007), although theyused a fuzzy edit distance measure and did not in-corporate other evidence in their model (e.g., thephoneme language model score).
The reader is re-ferred to (Koehn and Hoang, 2007; Koehn et al,2007) for detailed information about phrase-basedstatistical machine translation.
We give a brief out-line here, sufficient only to provide background forour query degradation application.Statistical machine translation systems work by185converting a source-language sentence into the mostprobable target-language sentence, under a modelwhose parameters are estimated using example sen-tence pairs.
Phrase-based machine translation is onevariant of this statistical approach, wherein multiple-word phrases rather than isolated words are thebasic translation unit.
These phrases are gener-ally not linguistically motivated, but rather learnedfrom co-occurrences in the paired example transla-tion sentences.
We apply the same machinery to hy-pothesize our pronunciation degradations, where wenow translate from the ?source-language?
referencephoneme sequence Q to the hypothesized ?target-language?
phoneme sequence H .Phrase-based translation is based on the noisychannel model, where Bayes rule is used to refor-mulate the translation probability for translating areference query Q into a hypothesized phoneme se-quence H asargmaxHP (H|Q) = argmaxHP (Q|H)P (H).Here, for example, P (H) is the language modelprobability of a degradation H and P (Q|H) is theconditional probability of the reference sequence Qgiven H .
More generally however, we can incorpo-rate other feature functions of H and Q, hi(H,Q),and with varying weights.
This is implemented us-ing a log-linear model for P (H|Q), where the modelcovariates are the functions hi(H,Q), so thatP (H|Q) = 1Z expn?i=1?ihi(H,Q)The parameters ?i are estimated by MLE and thenormalizing Z need not be computed (because wewill take the argmax).
Example feature functions in-clude the language model probability of the hypoth-esis and a hypothesis length penalty.In addition to feature functions being defined onthe surface level of the phonemes, they may also bedefined on non-surface annotation levels, called fac-tors.
In a word translation setting, the intuition isthat statistics from morphological variants of a lex-ical form ought to contribute to statistics for othervariants.
For example, if we have never seen theword houses in language model training, but haveexamples of house, we still can expect houses are tobe more probable than houses fly.
In other words,factors allow us to collect improved statistics onsparse data.
While sparsity might appear to be lessof a problem for phoneme degradation modeling(because the token inventory is comparatively verysmall), we nevertheless may benefit from this ap-proach, particularly because we expect to rely onhigher order language models and because we haverather little training data: only 22,810 transcribedutterances (about 600k reference phonemes).In our case, we use two additional annotation lay-ers, based on a simple grouping of phonemes intobroad classes.
We consider the phoneme itself, thebroad distinction of vowel and consonant, and a finergrained set of classes (e.g., front vowels, centralvowels, voiceless and voiced fricatives).
Figure 1shows the three annotation layers we consider for anexample reference phoneme sequence.
After map-ping the reference and hypothesized phonemes toeach of these additional factor levels, we train lan-guage models on each of the three factor levels ofthe hypothesized phonemes.
The language modelsfor each of these factor levels are then incorporatedas features in the translation model.We use the open source toolkit Moses (Koehnet al, 2007) as our phrase-based machine transla-tion system.
We used the SRI language model-ing toolkit to estimate interpolated 5-gram languagemodels (for each factor level), and smoothed ourestimates with Witten-Bell discounting (Witten andBell, 1991).
We used the default parameter settingsfor Moses?s training, with the exception of modi-fying GIZA++?s default maximum fertility from 10to 4 (since we don?t expect one reference phonemeto align to 10 degraded phonemes).
We used defaultdecoding settings, apart from setting the distortionpenalty to prevent any reorderings (since alignmentsare logically constrained to never cross).
For the restof this chapter, we refer to our phrase-based querydegradation model as PBQD.
We denote the phrase-based model using factors as PBQD-Fac.Figure 2 shows an example alignment learnedfor a reference and one-best phonemic transcript.The reference utterance ?snow white and the sevendwarves?
is recognized (approximately) as ?nowhite a the second walks?.
Note that the phrase-based system is learning not only acoustically plau-sible confusions, but critically, also confusions aris-186N OW W AY T AX DH AX S EH K AX N D W AO K SS N OW W AY T AE N D DH AX S EH V AX N D W OW R F Ssnow white and the seven dwarvesFigure 2: An alignment of hypothesized and reference phoneme transcripts from the multigram phoneme recognizer,for the phrase-based query degradation model.ing from the phonemic recognition system?s pe-culiar construction.
For example, while V andK may not be acoustically similar, they are stillconfusable?within the context of S EH?becausemultigram language model data has many exam-ples of the word second.
Moreover, while the worddwarves (D-W-OW-R-F-S) is not present in thedictionary, the words dwarf (D-W-AO-R-F) anddwarfed (D-W-AO-R-F-T) are present (N.B., thechange of vowel from AO to OW between the OOVand in vocabulary pronunciations).
While CMQDwould have to allow a deletion and two substitutions(without any context) to obtain the correct degrada-tion, the phrase-based system can align the completephrase pair from training and exploit context.
Here,for example, it is highly probable that the errorfullyhypothesized phonemes W AO will be followed byK, because of the prevalence of walk in languagemodel data.4 ExperimentsAn appropriate and commonly used measure forRUR is Mean Average Precision (MAP).
Given aranked list of utterances being searched through, wedefine the precision at position i in the list as the pro-portion of the top i utterances which actually containthe corresponding query word.
Average Precision(AP) is the average of the precision values computedfor each position containing a relevant utterance.
Toassess the effectiveness of a system across multi-ple queries, Mean Average Precision is defined asthe arithmetic mean of per-query average precision,MAP = 1n?n APn.
Throughout this paper, whenwe report statistically significant improvements inMAP, we are comparing AP for paired queries us-ing a Wilcoxon signed rank test at ?
= 0.05.Note, RUR is different than spoken term detec-tion in two ways, and thus warrants an evaluationmeasure (e.g., MAP) different than standard spokenterm detection measures (such as NIST?s actual termweighted value (Fiscus et al, 2006)).
First, STDmeasures require locating a term with granularityfiner than that of an utterance.
Second, STD mea-sures are computed using a fixed detection thresh-old.
This latter requirement will be unnecessary inmany applications (e.g., where a user might preferto decide themselves when to stop reading downthe ranked list of retrieved utterances) and unlikelyto be helpful for downstream evidence combination(where we may prefer to keep all putative hits andweight them by some measure of confidence).For our evaluation, we consider retrievingshort utterances from seventeen fully transcribedMALACH interviews.
Our query set contains allsingle words occurring in these interviews that areOOV with respect to the word dictionary.
Thisgives us a total of 261 query terms for evalua-tion.
Note, query words are also not present inthe multigram training transcripts, in any languagemodel training data, or in any transcripts used fordegradation modeling.
Some example query wordsinclude BUCHENWALD, KINDERTRANSPORT, andSONDERKOMMANDO.To train our degradation models, we used a heldout set of 22,810 manually transcribed utterances.We run each recognition system (phoneme, multi-gram, and word) on these utterances and, for each,train separate degradation models using the alignedreference and hypothesis transcripts.
For CMQD,we computed 100 random traversals on each lattice,giving us a total of 2,281,000 hypothesis and refer-ence pairs to align for our confusion matrices.5 ResultsWe first consider an intrinsic measure of the threespeech recognition systems we consider, namelyPhoneme Error Rate (PER).
Phoneme Error Rateis calculated by first producing an alignment of187the hypothesis and reference phoneme transcripts.The counts of each error type are used to computePER = 100 ?
S+D+IN , where S,D, I are the num-ber of substitutions, insertions, and deletions respec-tively, while N is the phoneme length of the refer-ence.
Results are shown in Table 1.
First, we see thatthe PER for the multigram system is roughly halfthat of the phoneme-only system.
Second, we findthat the word system achieves a considerably lowerPER than the multigram system.
We note, however,that since these are not true phonemes (but ratherphonemes copied over from pronunciation dictionar-ies and word transcripts), we must cautiously inter-pret these results.
In particular, it seems reasonablethat this framework will overestimate the strengthof the word based system.
For comparison, on thesame train/test partition, our word-level system hada word error rate of 31.63.
Note, however, that au-tomatic word transcripts can not contain our OOVquery words, so word error rate is reported only togive a sense of the difficulty of the recognition task.Table 1 shows our baseline RUR evaluation re-sults.
First, we find that the generative model yieldsstatistically significantly higher MAP using wordsor multigrams than phonemes.
This is almost cer-tainly due to the considerably improved phonemerecognition afforded by longer recognition units.Second, many more unique phoneme sequences typ-ically occur in phoneme lattices than in their wordor multigram counterparts.
We expect this will in-crease the false alarm rate for the phoneme system,thus decreasing MAP.Surprisingly, while the word-based recognitionsystem achieved considerably lower phoneme er-ror rates than the multigram system (see Table 1),the word-based generative model was in fact in-distinguishable from the same model using multi-grams.
We speculate that this is because the method,as it is essentially a language modeling approach,is sensitive to data sparsity and requires appropri-ate smoothing.
Because multigram lattices incor-porate smaller recognition units, which are not con-strained to be English words, they naturally producesmoother phoneme language models than a word-based system.
On the other hand, the multigramsystem is also not statistically significantly betterthan the word-based generative model, suggestingthis may be a promising area for future work.Table 1 shows results using our degradation mod-els.
Query degradation appears to help all sys-tems with respect to the generative baseline.
Thisagrees with our intuition that, for RUR, low MAP onOOV terms is predominately driven by low recall.1Note that, at one degradation, CMQD has the sameMAP as the generative model, since the most prob-able degradation under CMQD is almost always thereference phoneme sequence.
Because the CMQDmodel can easily hypothesize implausible degrada-tions, we see the MAP increases modestly with afew degradations, but then MAP decreases.
In con-trast, the MAP of the phrase-based system (PBQD-Fac) increases through to 500 query degradations us-ing multigrams.
The phonemic system appears toachieve its peak MAP with fewer degradations, butalso has a considerably lower best value.The non-factored phrase-based system PBQDachieves a peak MAP considerably larger than thepeak CMQD approach.
And, likewise, using addi-tional factor levels (PBQD-Fac) also considerablyimproves performance.
Note especially that, usingmultiple factor levels, we not only achieve a higherMAP, but also a higher MAP when only a few degra-dations are possible.To account for errors in phonemic recognition, wehave taken two steps.
First, we used longer recog-nition units which we found significantly improvedMAP while using our baseline RUR technique.
Asa second method for handling recognition errors,we also considered variants of our ranking func-tion.
In particular, we incorporated query degrada-tions hypothesized using factored phrase-based ma-chine translation.
Comparing the MAP for PBQD-Fac with MAP using the generative baseline for themost improved indexing system (the word system),we find that this degradation approach again statisti-cally significantly improved MAP.
That is, these twostrategies for handling recognition errors in RUR ap-pear to work well in combination.Although we focused on vocabulary-independentRUR, downstream tasks such as ad hoc speechretrieval will also want to incorporate evidencefrom in-vocabulary query words.
This makes1We note however that the preferred operating point in thetradeoff between precision and recall will be task specific.
Forexample, it is known that precision errors become increasinglyimportant as collection size grows (Shao et al, 2008).188Query DegradationsMethod Phone Source PER QD Model Baseline 1 5 50 500Degraded Model Phonemes 64.4 PBQD-Fac 0.0387 0.0479 0.0581 0.0614 0.0612Multigrams 32.1 CMQD 0.1258 0.1258 0.1272 0.1158 0.0991Multigrams 32.1 PBQD 0.1258 0.1160 0.1283 0.1347 0.1317Multigrams 32.1 PBQD-Fac 0.1258 0.1238 0.1399 0.1510 0.1527Words 20.5 PBQD-Fac 0.1255 0.1162 0.1509 0.1787 0.1753Table 1: PER and MAP results for baseline and degradation models.
The best result for each indexing approach isshown in bold.our query degradation approach which indexedphonemes from word-based LVCSR particularly at-tractive.
Not only did it achieve the best MAP inour evaluation, but this approach also allows us toconstruct recognition lattices for both in and out-of-vocabulary query words without running a second,costly, recognition step.6 ConclusionOur goal in this work was to rank utterances by ourconfidence that they contained a previously unseenquery word.
We proposed a new approach to thistask using hypothesized degradations of the queryword?s phoneme sequence, which we produced us-ing a factored phrase-based machine translationmodel.
This approach was principally motivated bythe mismatch between the query?s phonemes andthe recognition phoneme sequences due to errorfulspeech indexing.
Our approach was constructed andevaluated using phoneme-, multigram-, and word-based indexing, and significant improvements inMAP using each indexing system were achieved.Critically, these significant improvements were inaddition to the significant gains we achieved by con-structing our index with longer recognition units.While PBQD-Fac outperformed CMQD averag-ing over all queries in our evaluation, as expected,there may be particular query words for which thisis not the case.
Table 2 shows example degrada-tions using both the CMQD and PBQD-Fac degra-dation models for multigrams.
The query word isMengele.
We see that CMQD degradations are near(in an edit distance sense) to the reference pronun-ciation (M-EH-NX-EY-L-EH), while the phrase-based degradations tend to sound like commonly oc-CMQD Phrase-basedM-EH-NX-EY-L-EH M-EH-N-T-AX-LM-EH-NX-EY-L M-EH-N-T-AX-L-AA-TM-NX-EY-L-EH AH-AH-AH-AH-M-EH-N-T-AX-LM-EH-NX-EY-EH M-EH-N-DH-EY-L-EHM-EH-NX-L-EH M-EH-N-T-AX-L-IYTable 2: The top five degradations and associated proba-bilities using the CMQD and PBQD-Fac models, for theterm Mengele using multigram indexing.curring words (mental, meant a lot, men they.
.
.
,mentally).
In this case, the lexical phoneme se-quence does not occur in the PBQD-Fac degrada-tions until degradation nineteen.
Because delet-ing EH has the same cost irrespective of contextfor CMQD, both CMQD degradations 2 and 3 aregiven the same pronunciation weight.
Here, CMQDperforms considerably better, achieving an averageprecision of 0.1707, while PBQD-Fac obtains only0.0300.
This suggests that occasionally the phrase-based language model may exert too much influenceon the degradations, which is likely to increase theincidence of false alarms.
One solution, for futurework, might be to incorporate a false alarm model(e.g., down-weighting putative occurrences whichlook suspiciously like non-query words).
Second,we might consider training the degradation modelin a discriminative framework (e.g., training to op-timize a measure that will penalize degradationswhich cause false alarms, even if they are good can-didates from the perspective of MLE).
We hope thatthe ideas presented in this paper will provide a solidfoundation for this future work.189ReferencesW.
Byrne et al 2004.
Automatic Recognition of Spon-taneous Speech for Access to Multilingual Oral His-tory Archives.
IEEE Transactions on Speech and Au-dio Processing, Special Issue on Spontaneous SpeechProcessing, 12(4):420?435, July.U.V.
Chaudhari and M. Picheny.
2007.
Improvements inphone based audio search via constrained match withhigh order confusion estimates.
Automatic SpeechRecognition & Understanding, 2007.
ASRU.
IEEEWorkshop on, pages 665?670, Dec.Kareem Darwish and Walid Magdy.
2007.
Error cor-rection vs. query garbling for Arabic OCR documentretrieval.
ACM Trans.
Inf.
Syst., 26(1):5.Kareem M. Darwish.
2003.
Probabilistic Methods forSearching OCR-Degraded Arabic Text.
Ph.D. thesis,University of Maryland, College Park, MD, USA.
Di-rected by Bruce Jacob and Douglas W. Oard.S.
Deligne and F. Bimbot.
1997.
Inference of Variable-length Acoustic Units for Continuous Speech Recog-nition.
In ICASSP ?97: Proceedings of the IEEE Inter-national Conference on Acoustics, Speech, and SignalProcessing, pages 1731?1734, Munich, Germany.Jonathan Fiscus et al 2006.
English Spoken Term De-tection 2006 Results.
In Presentation at NIST?s 2006STD Eval Workshop.J.T.
Foote et al 1997.
Unconstrained keyword spot-ting using phone lattices with application to spokendocument retrieval.
Computer Speech and Language,11:207?224.Philipp Koehn and Hieu Hoang.
2007.
Factored Transla-tion Models.
In EMNLP ?07: Conference on Empiri-cal Methods in Natural Language Processing, June.Philipp Koehn et al 2003.
Statistical phrase-basedtranslation.
In NAACL ?03: Proceedings of the 2003Conference of the North American Chapter of the As-sociation for Computational Linguistics on HumanLanguage Technology, pages 48?54, Morristown, NJ,USA.
Association for Computational Linguistics.Philipp Koehn et al 2007.
Moses: Open Source Toolkitfor Statistical Machine Translation.
In ACL ?07: Pro-ceedings of the 2007 Conference of the Associationfor Computational Linguistics, demonstration session,June.Okan Kolak.
2005.
Rapid Resource Transfer for Mul-tilingual Natural Language Processing.
Ph.D. thesis,University of Maryland, College Park, MD, USA.
Di-rected by Philip Resnik.Jonathan Mamou and Bhuvana Ramabhadran.
2008.Phonetic Query Expansion for Spoken Document Re-trieval.
In Interspeech ?08: Conference of the Interna-tional Speech Communication Association.Spyros Matsoukas et al 2005.
The 2004 BBN 1xRTRecognition Systems for English Broadcast News andConversational Telephone Speech.
In Interspeech ?05:Conference of the International Speech Communica-tion Association, pages 1641?1644.K.
Ng and V.W.
Zue.
2000.
Subword-based approachesfor spoken document retrieval.
Speech Commun.,32(3):157?186.J.
Scott Olsson.
2008a.
Combining Speech Retrieval Re-sults with Generalized Additive Models.
In ACL ?08:Proceedings of the 2008 Conference of the Associationfor Computational Linguistics.J.
Scott Olsson.
2008b.
Vocabulary Independent Dis-criminative Term Frequency Estimation.
In Inter-speech ?08: Conference of the International SpeechCommunication Association.Pavel Pecina, Petra Hoffmannova, Gareth J.F.
Jones, Jian-qiang Wang, and Douglas W. Oard.
2007.
Overviewof the CLEF-2007 Cross-Language Speech RetrievalTrack.
In Proceedings of the CLEF 2007 Workshopon Cross-Language Information Retrieval and Evalu-ation, September.R.
Prasad et al 2005.
The 2004 BBN/LIMSI 20xRT En-glish Conversational Telephone Speech RecognitionSystem.
In Interspeech ?05: Conference of the Inter-national Speech Communication Association.S.E.
Robertson.
1977.
The Probability Ranking Princi-ple in IR.
Journal of Documentation, pages 281?286.M.
Saraclar and R. Sproat.
2004.
Lattice-Based Searchfor Spoken Utterance Retrieval.
In NAACL ?04: Pro-ceedings of the 2004 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology.P.
Schone et al 2005.
Searching Conversational Tele-phone Speech in Any of the World?s Languages.Jian Shao et al 2008.
Towards Vocabulary-IndependentSpeech Indexing for Large-Scale Repositories.
In In-terspeech ?08: Conference of the International SpeechCommunication Association.A.
Stolcke.
2002.
SRILM ?
an extensible language mod-eling toolkit.
In ICSLP ?02: Proceedings of 2002 In-ternational Conference on Spoken Language Process-ing.I.
H. Witten and T. C. Bell.
1991.
The Zero-FrequencyProblem: Estimating the Probabilities of Novel Eventsin Adaptive Text Compression.
IEEE Trans.
Informa-tion Theory, 37(4):1085?1094.Peng Yu and Frank Seide.
2005.
Fast Two-Stage Vocabulary-Independent Search In SpontaneousSpeech.
In ICASSP ?05: Proceedings of the 2005IEEE International Conference on Acoustics, Speech,and Signal Processing.P.
Yu et al Sept. 2005.
Vocabulary-Independent Index-ing of Spontaneous Speech.
IEEE Transactions onSpeech and Audio Processing, 13(5):635?643.190
