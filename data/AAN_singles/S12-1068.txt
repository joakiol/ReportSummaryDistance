First Joint Conference on Lexical and Computational Semantics (*SEM), pages 487?492,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsANNLOR: A Na?
?ve Notation-system for Lexical Outputs RankingAnne-Laure LigozatLIMSI-CNRS/ENSIIErue John von Neumann91400 Orsay, Franceannlor@limsi.frCyril GrouinLIMSI-CNRSrue John von Neumann91400 Orsay, Francecyril.grouin@limsi.frAnne Garcia-FernandezCEA-LISTNANO INNOV, Bt.
86191191 Gif-sur-Yvette cedex, Franceanne.garcia-fernandez@cea.frDelphine BernhardLiLPa, Universite?
de Strasbourg22 rue Rene?
Descartes, BP 8001067084 Strasbourg cedex, Francedbernhard@unistra.frAbstractThis paper presents the systems we developedwhile participating in the first task (EnglishLexical Simplification) of SemEval 2012.
Ourfirst system relies on n-grams frequenciescomputed from the Simple English Wikipediaversion, ranking each substitution term by de-creasing frequency of use.
We experimentedwith several other systems, based on term fre-quencies, or taking into account the context inwhich each substitution term occurs.
On theevaluation corpus, we achieved a 0.465 scorewith the first system.1 IntroductionIn this paper, we present the methods we used whileparticipating to the Lexical Simplification task at Se-mEval 2012 (Specia et al, 2012).
We experimentedwith several methods:?
using word frequencies or other statistical fig-ures from the BNC corpus, Google BooksNGrams, the Simple English Wikipedia, andresults from the Bing search engine (with/with-out lemmatization);?
using association measures for a word and itscontext based on language models (with/with-out inflection);?
making a combination of previous methodswith SVMRank.Depending on the results obtained on the trainingcorpus, we chose the methods that seemed to best fitthe data.2 Task description2.1 PresentationThe Lexical Simplification task aimed at determin-ing the degree of simplicity of words.
The inputsgiven were a short text, in which a target word waschosen, and several substitutes for the target wordthat fit the context.An example of a short text follows; the targetword is ?outdoor?, and other words of this text willbe considered as the context of this target word.< i n s t a n c e i d =?
270 ?><c o n t e x t>With t h e growing demand f o rt h e s e f i n e g a r de n f u r n i s h i n g s ,t h e y found i t n e c e s s a r y t o d e d i c a t ea p o r t i o n o f t h e i r b u s i n e s s t o<head>o u t d o o r< / head> l i v i n g andp a t i o f u r n i s h i n g s .< / c o n t e x t>< / i n s t a n c e>The substitutes given for this target word werethe following: ?alfresco;outside;open-air;outdoor;?.The objective was to order these words by descend-ing simplicity.2.2 CorporaTwo corpora were provided: the trial corpus withdevelopment examples, and the test corpus for eval-uation.In the trial corpus, a gold standard was also given.For the previous example, it stated that the substi-tutes had to be in the following order: ?outdooropen-air outside, alfresco?, ?outdoor?
being consid-ered as the simplest substitute, and ?outside?
and?alfresco?
being considered as the less simple ones.487Three baselines have been given by the organiz-ers: the first one is a simple randomization of thesubstitute list, the second one keeps the substitutelist as it is, and the third one (called ?simple fre-quency?)
relies on the use of the Google Web 1Tcorpus.3 Preprocessing3.1 Corpus constitutionIn order to use machine-learning based approaches,we produced two sub-corpora respectively for thetraining and evaluation stages from the trial corpus.The training sub-corpus is used to develop and tunethe systems we produced while the evaluation sub-corpus is used to evaluate the results of these sys-tems.For each set from the SemEval trial corpus, if theset is composed of at least eight lexical elements be-longing to the same morpho-syntactic category (e.g.,a set with at least eight instances of ?bright?
as anadjective), we extracted three instances from thisset for the evaluation sub-corpus, the remaining in-stances being part of the training sub-corpus.
If theset is composed of less than eight instances, all in-stances are used in the training sub-corpus.
We alsokept two complete sets of lexical elements for theevaluation sub-corpus in order to test the robustnessof our methods on new lexical elements that have notbeen studied yet.
This distribution allows us to bene-fit from a repartition between training and evaluationsub-corpora where the instances ratio is of 66/33%.3.2 Corpus cleaningWhile studying the trial corpus, we noticed that thetexts were not always in plain text, and in particularcontained HTML entities.
As some of our methodsused the context of target words, we decided to cre-ate a cleaner version of the corpora.
For the dash andquote HTML entities (&#8211; &#8220; etc.
), wereplaced each entity by its refering symbol.
Whenreplacing the apostrophe HTML entity (&apos;), wedecided to link the abbreviated token with the previ-ous one because all n-grams methods worked betterwith abbreviated terms of one token-length (don?t)than two token-length (do n?t) (see section 5).3.3 InflectionIn some sentences, the target words are inflected, butthe substitutes are given in their lemmatized forms.For example, one of the texts was the following :<c o n t e x t>In f a c t , d u r i n g a t l e a s t s i xd i s t i n c t p e r i o d s i n Army h i s t o r ys i n c e World War I , l a c k o f t r u s t andc o n f i d e n c e i n s e n i o r l e a d e r s c au se dt h e so?c a l l e d b e s t and<head>b r i g h t e s t< / head> t o l e a v e t h eArmy i n d r o v e s .< / c o n t e x t>For this text and target word, the proposed sub-stitutes were ?capable; most able; motivated; in-telligent; bright; clever; sharp; promising?, and ifwe want to test the simplicity of the words in con-text, for example with a 2-words left context, wewill obtain unlikely phrases such as ?best and capa-ble?
(which should be ?best and most capable?).
Wethus used several resources to get inflected forms ofwords: we used the Lingua::EN::Conjugate and Lin-gua::EN::Inflect Perl modules, which give inflectedforms of verbs and plural forms of nouns, as well asthe English dictionary of inflected forms DELA,1 tovalidate the Perl modules outputs if necessary, andget comparatives and superlatives of adjectives, anda list of irregular English verbs, also to validate thePerl modules outputs.4 Simple English Wikipedia based systemOur best system, called ANNLOR-simple, is basedon Simple English Wikipedia frequencies.
As thechallenge focused on substitutions performed bynon-native English speakers, we tried to use linguis-tic resources that best fit this kind of data.
In thisway, we made the hypothesis that training our sys-tem on documents written by or written for non-native English speakers would be useful.The use of the Simple English version fromWikipedia seems to be a good solution as it is tar-geted at people who do not have English as theirmother tongue.
Our hypothesis seems to be correctdue to the results we obtained.
Morevover, the Sim-ple English Wikipedia has been used previously inwork on automatic text simplification, e.g.
(Zhu etal., 2010).1http://infolingu.univ-mlv.fr/DonneesLinguistiques/Dictionnaires/telechargement.html488First, we produced a plain text version of the Sim-ple English Wikipedia.
We downloaded the dumpdated February 27, 2012 and extracted the textualcontents using the wikipedia2text tool.2 Thefinal plaintext file contains approximately 10 millionwords.We extracted word n-grams (n ranging from 1 to3) and their frequencies from this corpus thanks tothe Text-NSP Perl module 3 and its count.pl pro-gram, which produces the list of n-grams of a docu-ment, with their frequencies.
Table 1 gives the num-ber of n-grams produced.Table 1: Number of distinct n-grams extracted from theSimple English Wikipedian #n-grams1 301,7182 2,517,3943 6,680,9061 to 3 9,500,018Some of these n-grams are invalid, and resultfrom problems when extracting plain text fromWikipedia, such as ?27|ufc 1?, which correspondsto wiki syntax.
As we would not find these n-gramsin our substitution lists, we did not try to clean then-gram data.Then, we ranked the possible substitutes of a lex-ical item according to these frequencies, in descend-ing order.
For example, for the substitution list (in-telligent, bright, clever, smart), the respective fre-quencies in the Simple English Wikipedia are (206,475, 141, 201), and the substitutes will be ranked indescending frequencies: (bright, intelligent, smart,clever).Several tests were conducted, with varying pa-rameters.
We used the plain text version of the Sim-ple English Wikipedia, but also tried to lemmatize it,since substitutes are lemmatized.
We used the Tree-Tagger 4 (Schmid, 1994) and applied it on the whole2See http://www.polishmywriting.com/download/wikipedia2text\_rsm\_mods.tgzand http://blog.afterthedeadline.com/2009/12/04/generating-a-plain-text-corpus-from-wikipedia3http://search.cpan.org/?tpederse/Text-NSP-1.25/lib/Text/NSP.pm4http://www.ims.uni-stuttgart.de/corpus, before counting n-grams.
Moreover, sincebigrams and trigrams increase a lot, the size of n-gram data, we evaluated their influence on results.These tests are summed up in table 2.Table 2: Results obtained with the Simple EnglishWikipedia based system, on the trial and test corporareference lemmas score on score onn-grams trial corpus test corpus1-grams only no 0.333 ?1 and 2-grams no 0.371 ?1 to 3-grams no 0.381 0.4651 to 3-grams yes 0.380 0.462Simple Frequency0.398 0.471baselineWLV-SHEF-SimpLex(best system ?
0.496@SemEval2012)With unigrams only, 158 substitutes of the trialcorpus are absent of the reference dataset, 105 whenadding bigrams, and 91 when adding trigrams.
Mostof the missing n-grams (when using 1 to 3-grams)indeed seem to be very uncommon, such as ?undo-mesticated?
or ?telling untruths?.The small difference between the lemmatized andinflected versions of Wikipedia is due to two rea-sons: some substitutes are found in the lemmatizedversion because substitutes are given in the lemma-tized form (for example ?abnormal growth?
is onlypresent in its plural form ?abnormal growths?
inthe inflected Wikipedia); and some other substitutesare missing in the lemmatized version, mostly be-cause of errors from the TreeTagger (for example?be scared of?
becomes ?be scare of?
).We kept the system that obtained the best scoreson the trial corpus, that is with 1 to 3-grams and non-lemmatized n-grams, with a score of 0.381.
Thissystem obtained a score of 0.465 on the evalua-tion corpus, thus ranking second ex-aequo at the Se-mEval evaluation.projekte/corplex/TreeTagger/4895 Other frequency-based methodsWe tried several other reference corpora, alwayswith the idea that the more frequent a word is, thesimpler it is.
We used the BNC corpus,5 as wellas the Google Books NGrams.6 These NGramswere calculated on the books digitized by Google,and contain for each encountered n-gram, its num-ber of occurrences for a given year.
As the GoogleBooks NGrams are quite voluminous, we selected arandom year (2008), and kept only alphabetical n-grams with potential hyphens, and used n-grams forn ranging from 1 to 4.
The dataset used contains477,543,736 n-grams.We also used the Microsoft Web N-gram Service(more details on this service are given in the fol-lowing section) to rank substitutes in descending or-der.
The results of these methods on the trial corpusare given in table 3.
The result of the simple fre-quency baseline is also given: this baseline is alsofrequency-based, but words are ranked according tothe number of hits found when querying the GoogleWeb 1T corpus with each substitute.Table 3: Results obtained with frequency-based methods,on the trial corpusreference corpus scoreBNC 0.347Google Books NGrams 0.367Microsoft NGrams 0.383Simple Frequency baseline 0.398This table shows that all frequency-based meth-ods have lower scores than the Simple Frequencybaseline, although the score obtained with the Mi-crosoft NGrams is quite close to the baseline.
Theresults from Microsoft Ngrams and the Simple En-glish are very close.
We decided to submit the Sim-ple English Wikipedia-based system because it wasmore different from the simple frequency baseline.6 Contextual methodsWe also wanted to use contextual information, since,according to the contexts of the target word, dif-ferent substitutes can be used, or ranked differ-5http://www.natcorp.ox.ac.uk/6http://books.google.com/ngrams/datasetsently.
In the following two examples, the same word?film?
is targetted, and the same substitutes are pro-posed ?film;picture;movie;?
; yet, in the gold stan-dard, ?film?
is placed before ?movie?
in instance 19,and after it in instance 15.< i n s t a n c e i d =?
15 ?><c o n t e x t>Film Music L i t e r a t u r eC y b e r p l a c e ?
I n c l u d e s<head>f i l m< / head> r e v i e w s , messageb o a r d s , c h a t room , and imagesfrom v a r i o u s f i l m s .< / c o n t e x t>< / i n s t a n c e>( .
.
.
)< i n s t a n c e i d =?
19 ?><c o n t e x t>A f i n e s c o r e by George Fen ton( THE CRUCIBLE ) and b e a u t i f u lp h o t o g r a h y by Roger P r a t t addg r e a t l y t o t h e e f f e c t i v e n e s s o f t h e<head>f i l m< / head> .< / c o n t e x t>< / i n s t a n c e>Ranking substitutes thus depends on the contextof the target word.
We implemented two systemstaking the context of target words into account.6.1 Language model probabilitiesThe other system submitted (called ANNLOR-lmbing) relies on language models, which was themethod used by the organizers in their Simple Fre-quency baseline.
While the organizers used Googlen-grams to rank terms to be substituted by decreas-ing frequency of use, we used Microsoft Web n-grams in the same way.
Nevertheless, we also addedthe contexts of each term to be substituted.We used the Microsoft Web N-gram Service7 toobtain joint probability for text units, and moreprecisely its Python library.8 We used the bing-body/apr10/ ) N-Gram model.We considered a text unit composed of the lexi-cal item and a contextual window of 4 words to theleft and 4 words to the right (words being separatedby spaces).
For example, in the following sentence,we tested ?He brings an incredibly rich and diversebackground that?, and the same unit with the tar-get word replaced by substitutes, for example ?Hebrings an incredibly lush and diverse backgroundthat?.7http://research.microsoft.com/en-us/collaboration/focus/cs/web-ngram.aspx8http://web-ngram.research.microsoft.com/info/MicrosoftNgram-1.02.zip490< i n s t a n c e i d =?
118 ?><c o n t e x t>He b r i n g s an i n c r e d i b l y<head> r i c h< / head> and d i v e r s ebackground t h a t i n c l u d e s e v e r y t h i n gfrom e x e c u t i v e c o a c h i n g , l e a r n i n g&amp ; deve lopmen t and managementc o n s u l t i n g , t o s e n i o r o p e r a t i o n sr o l e s , mixed wi th a m a s t e r s i no r g a n i z a t i o n a ldeve lopmen t .< / c o n t e x t>< / i n s t a n c e>We performed several tests, with different N-Gram models, and different context sizes.
Some ofthese results for the trial corpus are given in table 4.Table 4: Results obtained with Microsoft Web N-gramService, on the trial corpusSize of left context Size of right context Score0 3 0.3623 0 0.3582 2 0.3653 3 0.3584 4 0.370For the evaluation, this system was our secondrun, with the parameters that obtained the best scoreson the training corpus (contexts of 4 words to theleft and to the right).
This method obtained a 0.370score on the trial corpus and a 0.396 score on the testcorpus.97 Combination of methodsAs each method seemed to have its own benefits, wetried to combine them using SVMRank 10(Joachims,2006).
The output of each system is converted intoa feature file.
For example, the output of the SimpleEnglish Wikipedia based system begins with:1 bright 475 11 intelligent 206 21 smart 201 31 clever 141 42 light 3241 12 clear 707 29This result is different from the official one, because anincorrect file was submitted at the time.10http://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html2 bright 475 32 luminous 14 42 well-lit 0 5The first column represent the instance id, the sec-ond one the considered substitute, the third one thefeature (in this case, the frequency of the substitutein the Simple English Wikipedia), and the last one,the substitute rank according to this method.
Then,we combined these files to include all features (afterbasic query-wise feature scaling).
For example, thetraining file begins with:1 qid:1 2:-0.004610613953259293:0.0345010535723618#intelligent2 qid:1 2:-0.004850107553253393:-0.0213467053270483 #clever3 qid:1 2:-0.004629036537874223:0.092640777900771 #smart4 qid:1 2:-0.003619478900975993:0.0489145618699556 #bright1 qid:4 2:-0.004610613953259293:0.0345010535723618#intelligentThe first column gives the gold standard rank forthe substitute (in training phase), the second one theinstance id, and then feature ids and values for eachsubstitute.
Default parameters were used.We used the division of the trial corpus into atraining corpus and a development corpus.
Table 5gives some examples of scores obtained by combin-ing two methods.
The scores are not exactly thosepresented earlier, since they correspond to a part ofthe trial corpus only.
Even though some improve-ment can be obtained by this combination, it wasquite small, and so we did not use it for the evalua-tion.Table 5: Results obtained with combination of methodswith SVMRank, on the trial corpusSimple English MicrosoftSVMWikipedia NGrams0.352 0.352 0.3544918 ConclusionIn this paper, we present several systems developedfor the English Lexical Simplification task of Se-mEval 2012.
The best results are obtained using fre-quencies from the Simple English Wikipedia.
Wefound the task quite hard to solve, since none ofour experiments significantly outperforms the Sim-ple Frequency baseline.
On the trial corpus, oursystem based upon the Simple English Wikipediaachieved a score of 0.381 (below the 0.399 base-line score); on the test corpus, we achieved a scoreof 0.465 with the Simple English Wikipedia systemwhile the baseline achieved a score of 0.471 score.All our systems using contextual information did notachieve high scores.ReferencesThorsten Joachims.
2006.
Training Linear SVMs in Lin-ear Time.
In Proceedings of the ACM Conference onKnowledge Discovery and Data Mining (KDD).Helmut Schmid.
1994.
Probabilistic Part-of-Speech Tag-ging Using Decision Trees.
In Proc.
of the Interna-tional Conference on New Methods in Language Pro-cessing, Manchester, UK.Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea.
2012.SemEval-2012 Task 1: English Lexical Simplification.In Proc.
of the 6th International Workshop on Seman-tic Evaluation (SemEval 2012), Montre?al, Canada.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A Monolingual Tree-based Translation Modelfor Sentence Simplification.
In Proceedings of the23rd International Conference on Computational Lin-guistics (COLING 2010), pages 1353?1361.492
