Proceedings of the Third Workshop on Statistical Machine Translation, pages 159?162,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsUsing syntactic coupling features for discriminating phrase-basedtranslations (WMT-08 Shared Translation Task)Vassilina Nikoulina and Marc DymetmanXerox Research Centre EuropeGrenoble, France{nikoulina,dymetman}@xrce.xerox.comAbstractOur participation in the shared translation taskat WMT-08 focusses on news translation fromEnglish to French.
Our main goal is to con-trast a baseline version of the phrase-basedMATRAX system, with a version that incor-porates syntactic ?coupling?
features in orderto discriminate translations produced by thebaseline system.
We report results comparingdifferent feature combinations.1 IntroductionOur goal is to try to improve the fluency and ad-equacy of a baseline phrase-based SMT system byusing a variety of ?syntactic coupling features?, ex-tracted from parses for the source and target strings.These features are used for reranking the n-best can-didates of the baseline system.The phrase-based SMT system MATRAX, devel-oped at XRCE, is used as the baseline in the experi-ments.
MATRAX is based on a fairly standard log-linear model, but one original aspect of the systemis the use of non-contiguous bi-phrases such as ne... plus / not ... anymore, where words in the sourceand target phrases may be separated by gaps, to befilled at translation time by lexical material providedby some other such pairs (Simard et al, 2005).For parsing, we use the Xerox Incremental ParserXIP (A?
?t-Mokhtar et al, 2002), which is a robustdependency parser developed at the Xerox ResearchCentre Europe.
XIP is fast (around 2000 words persecond for English) and is well adapted to a situ-ation, like the one we have here, were we need toparse on the order of a few hundred target candi-dates on the fly.
Also of interest to us is the fact thatXIP produces labelled dependencies, a feature thatwe use in some of our experiments.1.1 Decoding and TrainingWe resort to a standard reranking approach in whichwe produce an n-best list of MATRAX candidatetranslations (with n = 100 in our experiments), andthen rerank this list with a linear combination of ourparse-dependent features.
In order to train the fea-ture weights, we use an averaged structured percep-tron approach (Roark et al, 2004), where we try tolearn weights such that the first candidate to emergeis equal to the ?oracle?
candidate, that is, the candi-date that is closest to the reference in terms of NISTscore.1.2 Coupling FeaturesOur general approach to computing coupling fea-tures between the dependency structure of the sourceand that of a candidate translation produced by MA-TRAX is the following: we start by aligning thewords between the source and the candidate trans-lation, we parse both sides, and we count (possi-bly according to a weighting scheme) the number ofconfigurations (?rectangles?)
that are of the follow-ing type: ((s1, s12, s2), (t1, t12, t2)), where s12 is anedge between s1 and s2, t12 is an edge between t1and t2, s1 is aligned with t1 and s2 is aligned witht2.
We implemented several variants of this basicscheme.We start by describing different ?generic?
cou-pling functions derived from the basic scheme, as-159suming that word alignments have been already de-termined, then we describe the option of taking intoaccount specific dependency labels when countingrectangles, and finally we describe two options forcomputing the word alignments.1.2.1 Generic featuresThe first measure of coupling is based on sim-ple, non-weighted, word alignments.
Here we sim-ply consider that a word of the source and a wordof the target are aligned or not aligned, without anyintermediary degree, and consider that a rectangleexists on the quadruple of words s1, s2, t1, t2 iff siis aligned to ti, s1 and s2 have a dependency linkbetween them (in whatever direction) and similarlyfor t1 and t2.
The first feature that we introduce,Coupling-Count, is simply the count of all such rect-angles between the source and the target.We note that the value of this feature tends to becorrelated with the size of the source and target de-pendency trees.
We therefore introduce some nor-malized variants of the feature:?
Coupling-Recall.
We compute the number ofsource edges for which there exists a projec-tion in the target.
More formally, the number ofedges between two words s1, s2 such that thereexist two words t1, t2 with si aligned to ti andsuch that t1, t2 have an edge between them.
Wethen divide this number by the total number ofedges in the source.?
Coupling-Precision.
We do the same thing thistime starting from the target.?
Coupling-F-measure.
This is defined as theharmonic mean of the two previous features.1.2.2 Label-specific featuresThe features previously defined do not take intoaccount the labels associated with edges in the de-pendency trees.
However, while rectangles of theform ((s1, subj, s2), (t1, subj, t2)) may be rather sys-tematic between such languages as English andFrench, other rectangles may be much less so, dueon the one hand to actual linguistic divergences be-tween the two languages, but also, as importantlyin practice, to different representational conventionsused by different grammar developers for the twolanguages.1In order to control this problem, we introduce acollection of Label-Specific-Coupling features, eachfor a specific pair of source label and target label.The values of a label-specific feature are the num-ber of occurrences for this specific label pair.
Weuse only label pairs that have been observed to bealigned in the training corpus (that is, that partici-pate in observed rectangles).
In one version of thatapproach, we use all such pairs found in the corpus,in another version only the pairs above a certain fre-quency threshold in the corpus.1.2.3 AlignmentIn order to compute the features described above,a prerequisite is to be able to determine a word align-ment between the source and a candidate translation.Our first approach is to use GIZA++ (correspond-ing roughly to IBM Model 4) to create these align-ments, by producing for a given source and a givencandidate translation n-best alignment lists in bothdirections and applying standard techniques of sym-metrization to produce a bidirectional alignment.Another way to find word alignments is to use theinformation provided by the baseline system.
SinceMATRAX is a phrase-based system, it has access tothe bi-phrases (aligned by definition) that are used inorder to generate a candidate translation.
Howevernote that when we use a bi-phrase based alignment,there will be differences from the word alignmentthat we discussed before, and we need to adapt ourcoupling functions.1.2.4 Related approachesThere is a growing body of work on the use ofsyntax for improving the quality of SMT systems.Our approach is closest to the line taken in (Ochet al, 2003), where syntactic features are also usedfor discriminating between candidates produced bya phrase-based system, but here we introduce andcompare results for a wider variety of coupling fea-tures, taking into account different combinations in-volving normalization of the counts, symmetrizedfeatures between the source and target, labelled de-1Although the XIP formalism is shared between grammardevelopers of French and English, the grammars do sometimesfollow different conventions.160pendencies, and also consider several ways for com-puting the word alignment on the basis of whichedge couplings are determined.2 Experiments2.1 DescriptionOur participation concerns the English to FrenchNews translation task.
To train our baseline systemwe used the News Commentary corpus, namely thetraining (?
1M words) and development (1057 sen-tences) sets proposed for the shared translation task.The same development set was used for the MERTtraining procedure of the baseline system, as wellas for learning the parameters of the reranking pro-cedure.
Note that the test data on which we reportour experimental results here is the one proposed asdevelopment test set for the News translation task(1064 sentences, nc-devtest2007).Using MATRAX as the baseline system we gen-erate 100-best lists of candidate translations for allsource sentences of the test set, we rerank these can-didates using our features, and we output the topcandidate.
We present our results in Table 1, distin-guished according to the actual combination of fea-tures used in each experiment.?
The Baseline entry in the table corresponds toMATRAX results on the test set, without theuse of any of the coupling features.?
We distinguish two sub-tables, according towhether Giza-based alignments or phrase-based alignments were used.?
The Generic keyword corresponds to the cou-pling features introduced in section 1.2.1, basedon rectangle counts, independent of the labelsof the edges.?
The Matrax keyword corresponds to usingMATRAX ?internal?
features as reranking fea-tures, along with the coupling features.
TheseMATRAX features are pretty standard phrase-based features, apart from some features deal-ing explicitly with gapped phrases, and are de-scribed in detail in (Simard et al, 2005).?
The Labels and Frequent Labels keywords cor-responds to using label-specific features.
Inthe first case (Labels) we extracted all of thealigned label pairs (label pair associated witha coupling rectangle) found in a training set,while in the second case (Frequent Labels), weonly kept the most frequently observed amongthese label pairs.?
When several keywords appear on a line, weused the union of the corresponding features,and in the last line of the table, we show acombination involving at the same time somefeatures computed on the basis of Giza-basedalignments and of phrase-based alignments.?
Along with the NIST and BLEU scores of eachcombination, we also conducted an informalmanual assessment of the quality of the re-sults relative to the MATRAX baseline.
Wetook a random sample of 100 source sentencesfrom the test set and for each sentence, assessedwhether the first candidate produced by rerank-ing was better, worse, or indistinguishable interms of quality relative to the baseline trans-lation.
We report the number of improvements(+) and deteriorations (-) among these 100 sam-ples as well as their difference.23 DiscussionWhile the overall results in terms of Bleu and Nistdo not show major improvements relative to thebaseline, there are several interesting observationsto make.
First of all, if we focus on feature com-binations in which MATRAX features are included(shown in italics in the table), we see that there is ageneral tendency for the results, both in terms of au-tomatic and human evaluations, to be better than forthe same combination without the MATRAX fea-tures; the explanation seems to be that if we donot use the MATRAX features during reranking, butconsider the 100 candidates in the n-best list to beequally valuable from the viewpoint of MATRAXfeatures, we lose essential information that cannot2All the results reported here correspond to our own evalu-ations, prior to the WMT evaluations.
Given time constraints,we focussed more on contrasting the baseline with the baseline+ coupling features, than in tuning the baseline itself for thetask at hand.
After the submission deadline, we were able toimprove the baseline for this task.161NIST BLEU - + DiffBaseline 6.4093 0.2034 0 0 0Giza-based alignmentsGeneric 6.3383 0.2043 15 17 2Generic, Matrax 6.3782 0.2083 4 18 14Labels 6.3483 0.1963 12 18 6Labels, Generic 6.3514 0.2010 3 18 15Labels, Generic, Matrax 6.4016 0.2075 3 20 17Frequent Labels 6.3815 0.2054 7 11 4Frequent Labels, Generic 6.3826 0.2044 6 18 12Frequent Labels, Generic, Matrax 6.4177 0.2100 2 16 14Phrase-based alignmentsGeneric 6.2869 0.1964 12 14 2Generic, Matrax 6.3972 0.2031 4 11 7Labels 6.3677 0.1995 16 15 -1Labels, Generic 6.3567 0.1977 8 15 7Labels, Generic, Matrax 6.4269 0.2049 4 17 13Frequent Labels 6.3701 0.1998 3 15 12Frequent Labels, Generic 6.3846 0.2013 7 16 9Frequent Labels, Generic, Matrax 6.4160 0.2049 4 16 12Giza Generic, Phrase Generic, Giza Labels, Matrax 6.4351 0.2060 7 22 15Table 1: Reranking results.be recovered simply by appeal to the syntactic cou-pling features.If we now concentrate on the lines which do in-clude MATRAX features and compare their resultswith the baseline, we see a trend for these results tobe better than the baseline, both in terms of auto-matic measures as (more strongly) in terms of hu-man evaluation.
Taken individually, perhaps the im-provements are not very clear, but collectively, atrend does seem to appear in favor of syntactic cou-pling features generally, although we have not con-ducted formal statistical tests to validate this impres-sion.
A more detailed comparison between individ-ual lines, inside the class of combinations that in-clude MATRAX features, appears however difficultto make on the basis of the reported experiments.ReferencesSalah A?
?t-Mokhtar, Jean-Pierre Chanod, and ClaudeRoux.
2002.
Robustness beyond shallowness: incre-mental deep parsing.
Natural Language Engineering,8(3):121?144.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2003.
Syn-tax for Statistical Machine Translation: Final report ofJohn Hopkins 2003 Summer Workshop.
Technical re-port, John Hopkins University.B.
Roark, M. Saraclar, M. Collins, and M. Johnson.2004.
Discriminative language modeling with condi-tional random fields and the perceptron algorithm.
InProceedings of the 42nd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL?04), July.Michel Simard, Nicola Cancedda, Bruno Cavestro,Marc Dymetman, ?Eric Gaussier, Cyril Goutte,Kenji Yamada, Philippe Langlais, and Arne Mauser.2005.
Translating with non-contiguous phrases.
InHLT/EMNLP.162
