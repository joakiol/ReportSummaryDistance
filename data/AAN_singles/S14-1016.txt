Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 121?131,Dublin, Ireland, August 23-24 2014.Extracting Latent Attributes from Video Scenes Using Text asBackground KnowledgeAnh Tran, Mihai Surdeanu, Paul CohenUniversity of Arizona{trananh, msurdeanu, prcohen}@email.arizona.eduAbstractWe explore the novel task of identify-ing latent attributes in video scenes, suchas the mental states of actors, usingonly large text collections as backgroundknowledge and minimal information aboutthe videos, such as activity and actor types.We formalize the task and a measure ofmerit that accounts for the semantic re-latedness of mental state terms.
We de-velop and test several largely unsupervisedinformation extraction models that iden-tify the mental states of human partici-pants in video scenes.
We show that thesemodels produce complementary informa-tion and their combination significantlyoutperforms the individual models as wellas other baseline methods.1 Introduction?Labeling a narrowly avoided vehicularmanslaughter as approach(car, person) ismissing something.
?1The recognition of ac-tivities, participants, and objects in videos hasadvanced considerably in recent years (Li et al.,2010; Poppe, 2010; Weinland et al., 2011; Yangand Ramanan, 2011; Ng et al., 2012).
However,identifying latent attributes of scenes, such as themental states of human participants, has not beenaddressed.
Latent attributes matter: If a videosurveillance system detects one person chasinganother, the response from law enforcementshould be radically different if the people arehappy (e.g., children playing) or afraid and angry(e.g., a person running from an assailant).This work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1James Donlon, former manager of DARPA?s Mind?s Eyeprogram, personal communication.Attributes that are latent in visual representa-tions are often explicit in textual representations.This suggests a novel method for inferring latentattributes: Use explicit features of videos to querytext corpora, and from the resulting texts extractattributes that are latent in the videos, such as men-tal states.
The contributions of this work are:1: We formalize the novel task of latent attributeidentification from video scenes, focusing on theidentification of actors?
mental states.
The inputfor the task is contextual information about thescene, such as detections about the activity (e.g.,chase) and actor types (e.g., policeman or child),and the output is a distribution over mental statelabels.
We show that gold standard annotationsfor this task can be reliably generated using crowdsourcing.
We define a novel evaluation measure,called constrained weighted similarity-aligned F1score, that accounts for both the differences be-tween mental state distributions and the seman-tic relatedness of mental state terms (e.g., partialcredit is given for irate when the target is angry).2: We propose several robust and largely unsuper-vised information extraction (IE) models for iden-tifying the mental state labels of human partici-pants in a scene, given solely the activity and actortypes: a lexical semantic (LS) model that extractsmental state labels that are highly similar to thecontext of the scene in a latent, conceptual vectorspace; and an information retrieval (IR) model thatidentifies labels commonly appearing in sentencesrelated to the explicit scene context.
We show thatthese models are complementary and their combi-nation performs better than either model, alone.3: Furthermore, we show that an event-centricmodel that focuses on the mental state labels ofthe participants in the relevant event (identified us-ing syntactic patterns and coreference resolution)outperforms the above shallower models.1212 Related WorkAs far as we know, the task proposed here is novel.We can, however, review work relevant to eachpart of the problem and our solution.
Mentalstate inference is often formulated as a classifica-tion problem, where the goal is to predict targetmental state labels based on low-level sensory in-put data.
Most solutions try to learn classificationmodels based on large amounts of training data,while some require human engineering of domainknowledge.
Hidden Markov Models (HMMs) andDynamic Bayesian Networks (DBNs) are popularrepresentations because they can model the tem-poral evolution of mental states.
For instance, themental states of students can be inferred from un-intentional body gestures using a DBN (Abbasi etal., 2009).
Likewise, an HMM can also be usedto model the emotional states of humans (Liu andWang, 2011).
Some solutions combine HMMsand DBNs in a Bayesian inference framework toyield a multi-layer representation that can do real-time inference of complex mental and emotionalstates (El Kaliouby and Robinson, 2004; Baltru-saitis et al., 2011).
Our work differs from theseapproaches in several ways: It is mostly unsuper-vised, multi-modal, and requires little training.Relevant video processing technology includesobject detection (e.g., (Felzenszwalb et al., 2008)),person detection, and pose detection (e.g., (Yangand Ramanan, 2011)).
Many tracking algo-rithms have been developed, such as group track-ing (McKenna et al., 2000), tracking by learn-ing appearances (Ramanan et al., 2007), andtracking in 3D space (Giebel et al., 2004; Brauet al., 2013).
For human action recognition,current state-of-the-art techniques are capable ofachieving near perfect performance on the com-monly used KTH Actions dataset (Schuldt et al.,2004) and high performance rates on other morechallenging datasets (O?Hara and Draper, 2012;Sadanand and Corso, 2012).To extract mental state information from texts,one might use any or all of the technologies ofnatural language processing, so a complete reviewof relevant technologies is impossible, here.
Ofimmediate relevance is the work of de Marneffeet al.
(2010), which identified the latent meaningbehind scalar adjectives (e.g., which ages peoplehave in mind when talking about ?little kids?
).The authors learned these meanings by extract-ing scalars, such as children?s ages, that werecommonly collocated with phrases, such as ?lit-tle kids,?
in web documents.
Mohtarami et al.
(2011) tried to infer yes/no answers from indirectyes/no question-answer pairs (IQAPs) by predict-ing the uncertainty of sentiment adjectives in in-direct answers.
Their method employs antonyms,synonyms, word sense disambiguation as well asthe semantic association between the sentimentadjectives that appear in the IQAP to assign a de-gree of certainty to each answer.
Sokolova and La-palme (2011) further showed how to learn a modelfor predicting the opinions of users based on theirwritten contents, such as reviews and product de-scriptions, on the Web.
Gabbard et al.
(2011)found that coreference resolution can significantlyimprove the recall rate of relations extraction with-out much expense to the precision rate.Our work builds on these efforts by combininginformation retrieval, lexical semantics, and eventextraction to extract latent scene attributes.3 DataFor the experiments in this paper, we focus solelyon videos containing chase scenes.
Chases ofteninvoke clear mental state inferences, and depend-ing on context can suggest very different mentalstate distributions for the actors involved.3.1 Video CorpusWe compiled a video dataset of 26 chase videosfound on the Web.
Of these, five involve policeofficers, seven involve children, four show sports-related scenes, and twelve describe different chasescenarios involving civilian adults (two videos in-volve children playing sports).
The average dura-tion of the dataset is 8.8 seconds with a range of[4, 18].
Most videos involve a single chaser and asingle chasee (a person being chased) while a fewhave several chasers and/or chasees.For each video, we used Amazon MechanicalTurk (MTurk) to identify both the actors and theirmental states.
Each worker was asked to view avideo in its entirety before answering some ques-tions about the scene.
We give no prior training tothe workers.
The questions were carefully phrasedto apply to all participants of a particular role, forexample all chasers (if there are more than one).We also ask obvious validation questions about theparticipants in each role (e.g., are the chasers run-ning towards the camera?)
and use the answers tothese questions to filter out poor responses.
In gen-122eral, we found that most responses were good andonly a few incomplete submissions were rejected.In the first experiment, we asked MTurk work-ers to select the actor types and various other de-tections from a predefined list of tags.
This label-ing task is a proxy for a computer vision detectionsystem that functions at a human level of perfor-mance.
Indeed, we restricted the actor type labelsto a set that can be reasonably expected from auto-matic detection algorithms: person, police officer,child, and (non-human) object.
For instance, po-lice officers often wear distinctive color uniformsthat can be learned using the Felzenszwalb detec-tor (Felzenszwalb et al., 2008), whereas childrencan be reliably differentiated by their heights un-der a 3D-tracking model (Brau et al., 2013).
Eachvideo was annotated by three different workersand the union of their annotations is produced.The overall accuracy of the annotation was excel-lent.
The MTurk workers correctly identified theimportant actors in every video.Next, we collected a gold standard list of mentalstate labels for each video by asking MTurk work-ers to identify all applicable mental state adjec-tives for the actors involved.
We used a text-boxto allow for free-form input.
Studies have shownthat people of different cultures can perceive emo-tions very differently, and having forced choiceoptions cannot always capture their true percep-tion (Gendron et al., 2014).
Therefore, we did notrestrict the response of the workers in any way.Workers could abstain from answering if they feltthe video was too ambiguous.
Each video wasevaluated by ten different workers.
We convertedeach term provided to the closest adjective formif possible.
Terms with no equivalent adjectiveforms were left in place.
On rare occasions, work-ers provided sentence descriptions despite beingasked for single-word adjectives.
These sentenceswere either removed, or collapsed into a singleword if appropriate.
The overall quality of the an-notations was good and generally followed com-mon intuition.
Asides from the frequently usedterms, we also received some colorful (yet infor-mative) descriptions, like incredulous and vindic-tive.
In general, chases involving police scenar-ios often contained violent and angry states whilechases involving children received more cheerfullabels.
There were unexpected descriptions, suchas annoy for a playful chase between two children.Upon review of the video, we agreed that one childdid indeed look annoyed.
Thus, the resulting de-scriptions were subjective, but very few were hardto rationalize.
By aggregating the answers fromthe workers, we generated a gold standard distri-bution of mental state terms for each video.23.2 Text CorpusThe text corpus used for our models is the En-glish Gigaword 5th Edition corpus3, made avail-able by the Linguistics Data Consortium and in-dexed by Lucene4.
It is a comprehensive archiveof newswire text data (approximately 26 GB), ac-quired over several years.
It is in this corpus thatwe expect to find mental state terms cued by con-textual information from videos.4 Neighborhood ModelsWe developed several individual models based onthe neighborhood paradigm, that is, the hypoth-esis that relevant mental state labels will appear?near?
text cued by the visual features of a scene.The models take as input the context extractedfrom a video scene, defined simply as a list of ?ac-tivity and actor-type?
tuples (e.g., (chase, police)).Multiple actor types will result in multiple tuplesfor a video.
The actors can be either a person, apoliceman, a child, or a (non-human) object.
Ifthe detections describe the actor as both a personand a child, or a person and a policeman, we auto-matically remove the person label as it is a Word-Net (Miller, 1995) hypernym of both child and po-liceman.
For each human actor type, we furtherincrease our coverage by retrieving the synonymset (synset) of its most frequent sense (i.e., sense#1) from WordNet.
For example, a chase involv-ing a policeman would generate the following tu-ples: (chase, policeman) and (chase, officer).We call these query tuples because they are usedto query text for sentences that ?
if all goes well ?will contain relevant mental state labels.Given query tuples, our models use an initialseed set of 160 mental state adjectives to producea single distribution over mental state labels, re-ferred to as the response distribution, for eachvideo.
The seed set is compiled from popularmental and emotional state dictionaries, includ-ing the Profile of Mood States (POMS) (McNairet al., 1971) and Plutchik?s wheel of emotion.
We2All videos and annotations are available at:http://trananh.github.io/vlsa3Linguistics Data Consortium catalog no.
LDC2011T074Apache Lucene: http://lucene.apache.org123Source Example Mental State LabelsPOMSalert, annoyed, energetic, exhausted, helpful,sad, terrified, unworthy, weary, etc.Plutchikangry, disgusted, fearful, joyful/joyous,sad, surprised, trusting, etc.Othersagitated, competitive, cynical, disappointed,excited, giddy, happy, inebriated, violent, etc.Table 1: The initial seed set contains 160 mentalstate labels, compiled from different sources likethe popular Profile of Mood States dictionary andPlutchik?s wheel of emotion.also included frequently used labels gathered fromsynsets found in WordNet (see Table 1 for exam-ples).
Note that the gold standard annotations pro-duced by MTurk workers (Sec.
3) was not a sourcefor this set, nor was it restricted to these terms.4.1 Back-off Interpolation in Vector SpaceOur first model uses the recurrent neural net-work language model (RNNLM) of Mikolov etal.
(2013) to project both mental state labels andquery tuples into a latent conceptual space.
Simi-larity is then trivially computed as the cosine sim-ilarity between these vectors.
In all of our experi-ments, we used a RNNLM computed over the Gi-gaword corpus with 600-dimensional vectors.For this vector space (vec) model, we separatethe query tuples into different levels of back-offcontext.
The first level includes the set of activ-ity types as singleton context tuples, e.g., (chase),while the second level includes all (activity, actor)context tuples.
Hence, each query tuple will yieldtwo different context tuples, one for each back-offlevel.
For each context tuple with multiple terms,such as (chase, policeman), we find the vector rep-resentation for the context by aggregating the vec-tors representing the search terms:vec(chase, policeman) = vec(chase) +vec(policeman) .The vector representation for a singleton con-text tuple is just the vector of the single searchterm.
We then calculate the distance of each men-tal state labelm to the normalized vector represen-tation of the context tuple by computing the cosinesimilarity score between the two vectors:cos(?m) =vec(m) ?
vec(context tuple)||vec(m)|| ||vec(context tuple)||.The hypothesis here is that mental state labelsthat are related to the search context will have aRNNLM vector that is closer to the context tuplevector, resulting in a high cosine similarity score.Because the number of latent dimensions is rela-tively small (when compared to vocabulary size),cosine similarity scores in this latent space tend tobe close.
To further separate these scores, we raisethem to an exponential power:score(m) = ecos(?m)+1?
1 .The processing of each context tuple yields 160different scores, one for each mental state label.We normalize these scores to form a single distri-bution of scores for each context tuple.
The distri-butions are then integrated into a single distribu-tion representative of the complete activity as fol-lows: (a) the distributions at each context back-offlevel are averaged to generate a single distributionper level ?
for the second level (which includesactivity and actor types), it means distributions forall (activity, actor) tuples are averaged, whereasthe first level only has a single distribution fromthe singleton activity tuple (chase); and (b) distri-butions for the different levels are linearly interpo-lated, similar to the back-off strategy of (Collins,1997).
Let e1and e2represent the weights of somemental state label m from the average distributionat the first and second level, respectively.
Then theinterpolated distribution score e for m is:e = ?e1+ (1?
?
)e2.Compiling the distribution scores for each mproduces the final distribution representing the ac-tivity modeled.
We prune this final distribution bytaking the top ranked items that make up some ?proportion of the distribution.
We delay the dis-cussion of how ?
is tuned to Section 6.
The finalpruned distribution is normalized to produce theresponse distribution.4.2 Sentence Co-occurrence with DeletedInterpolationOur second model, the sent model, extracts mentalstate labels based on the likelihood that they ap-pear in sentences cued by query tuples.
For eachtuple, we estimate the conditional probability thatwe will see a mental state label m in a sentence,where m is from the seed set, given that we al-ready observed the desired activity and actor typein the same sentence: P (m|activity, actor).
In thiscase, we refer to the sentence length as the neigh-borhood window.
Furthermore, all terms must ap-pear as the correct part-of-speech (POS): m must124appear as an adjective or verb, the activity as averb, and the actor as a noun.
(Mental state adjec-tives are allowed to appear as verbs because someare often mis-tagged as verbs; e.g., agitated, deter-mined, welcoming.)
We used Stanford?s CoreNLPtoolkit for tokenization and POS tagging.5Note that this probability is similar to a trigramprobability in POS tagging, except the triples neednot form an ordered sequence but must appear inthe same sentence and under the correct POS tag.Unfortunately, we cannot always compute this tri-gram probability directly from the corpus becausethere might be too few instances of each trigramto compute a probability reliably.
As is common,we instead estimate it as a linear interpolation ofunigrams, bigrams, and trigrams.
We define themaximum likelihood probabilities?P , derived fromrelative frequencies f , for the unigrams, bigrams,and trigrams as follows:?P (m) =f(m)N?P (m|activity) =f(m, activity)f(activity)?P (m|activity, actor) =f(m, activity, actor)f(activity, actor)for all mental state labels m, activities, and actortypes in our queries.
N is the total number of to-kens in the corpus.
The aforementioned POS re-quirement is enforced: f(m) is the number of oc-currences of m as an adjective or verb.
We define?P = 0 if the corresponding numerator and denom-inator are zero.
The desired trigram probability isthen estimated as:P (m|activity, actor) = ?1?P (m) +?2?P (m|activity) + ?3?P (m|activity, actor) .As ?1+?2+?3= 1, P represents a probabilitydistribution.
We use the deleted interpolation algo-rithm (Brants, 2000) to estimate one set of lambdavalues for the model, based on all trigrams.For each query tuple generated in a video, 160different trigrams are computed, one for each men-tal state label in the seed set, resulting in 160 con-ditional probability scores.
We normalize thesescores into a single distribution ?
the mental statedistribution for that query tuple.
We then combine5http://nlp.stanford.edu/software/corenlp.shtml.all resulting distributions, one from each query tu-ple, and take the average to produce a single dis-tribution over mental state labels for the video.
Asbefore, we prune this distribution by taking thetop-ranked items that cover a large fraction ?
oftotal probability.
The pruned distribution is renor-malized to yield the final response distribution.4.3 Event-centric with Deleted InterpolationThe sent model has two limitations.
On one hand,it is too sparse: the single sentence neighborhoodwindow is too small to reliably estimate the fre-quencies of trigrams for the probabilities of men-tal state terms.
On the other hand, it may be toolenient, as it extracts all mental state mentions ap-pearing in the same sentence with the activity, orevent, under consideration, regardless if they ap-ply to this event or not.
We address these limita-tions next with an event-centric model (event).Intuitively, the event model focuses on the men-tal state labels of event participants.
Formally,these mental state terms are extracted as follows:1: We identify event participants (or actors).
Wedo this by analyzing the syntactic dependencies ofsentences containing the target verb (e.g., chase)to find the subject and object.
In most cases, thenominal subject of the verb chase is the chaser andthe direct object is the person being chased.
Weimplemented additional patterns to model passivevoice and other exceptions.
We used Stanford?sCoreNLP toolkit for syntactic dependency parsingand the downstream coreference resolution.2: Once the phrases that point to actors are iden-tified, we identify all mentions of these actors inthe entire document by traversing the coreferencechains containing the phrases extracted in the pre-vious step.
The sentences traversed in the chainsdefine the neighborhood area for this model.3: Lastly, we identify the mental state terms ofevent participants using a second set of syntac-tic patterns.
First, we inspect several copulativeverbs, such as to be and feel, and extract men-tal state labels from these structures if the corre-sponding subject is one of the mentions detectedabove.
Second, we search for mental states alongadjectival modifier relations, where the head is anactor mention.
For all patterns, we make sure tofilter for only mental state complements belong-ing to the initial seed list.
The same POS restric-tion as in the other models also applies.
We incre-ment the joint frequency f for the n-gram once for125each neighborhood that properly contain all searchterms from the n-gram in the correct POS.The event model addresses both limitations ofthe sent model: it avoids the lenient extraction ofmental state labels by focusing on labels associ-ated with event participants; it addresses sparsityby considering all mentions of event participantsin a document.To understand the impact of this model, wecompare it against two additional baselines.
Thefirst baseline investigates the importance of focus-ing on mental state terms associated with eventparticipants.
This model, called coref, implementsthe first two steps of the above algorithm, but in-stead of extracting only mental state terms associ-ated with event actors (last step), it considers allmentions appearing anywhere in the coreferenceneighborhood.
That is, all unique sentences tra-versed by the relevant coreference chains are firstpieced together to define a single neighborhood fora given document; then the relative joint frequen-cies of n-grams are computed by incrementing fonce for each neighborhood that contains all termswith correct POS tags.The second baseline analyzes the importance ofcoreference resolution to our problem.
This modelis similar to sent, with the modification that it in-creases the size of the neighborhood window to in-clude the immediate neighbors of target sentencesthat contain activity labels.
We call this the win-nmodel: The window around a target verb contains2n + 1 sentences.
We build the context neigh-borhood by concatenating all target sentences andtheir windows together for a given document.
Thisdefines a single neighborhood for each document.This contrasts with the sent model, in which theneighborhood is defined for each sentence con-taining the activity label in the document, resultingin several possible neighborhoods in a document.The joint frequency f for each n-gram ?
wheren > 1 ?
is computed similarly with the corefmodel: it is incremented once for each neighbor-hood that contains all the terms from the n-gramin the correct POS.
Frequencies for unigrams arecomputed similar to sent.As before, 160 different trigrams are generatedfor each query tuple, one for each mental state la-bel in the seed set, resulting in 160 conditionalprobability scores.
We similarly combine thesescores and generate a single pruned distribution asthe response for each of the model above.G (irate, 0.8), (afraid, 0.2)R1(angry, 0.6), (mad, 0.4)R2(irate, 0.2), (afraid, 0.8)R3(mad, 0.4), (irate, 0.4), (scared, 0.2)Table 2: We show an example gold standard dis-tribution G and several candidate response distri-butions to be matched against G. Here, R3bestmatches the shape and meaning of G, because(irate, mad) and (afraid, scared) are close syn-onyms.
R2appears to match G semantically, butmatches its shape poorly.
R1misses one of themental state labels, afraid, but contains labels thatare semantically close to the weightiest term in G.4.4 Ensemble ModelWe combined the results from the event andvec models to produce an ensemble model (ens)which, for a mental state label m, returns the aver-age of m?s scores according to the response distri-butions of the two individual models.5 Evaluation MeasuresLetR denote the response distribution over mentalstate labels produced for a single video by one ofthe models described in the previous section, andlet G denote the gold standard distribution pro-duced for the same video by MTurk workers.
IfR is similar to G then our models produce simi-lar mental state terms as the workers.
There aremany ways to compare distributions (e.g., KL dis-tance, chi-square statistics) but these give bad re-sults when distributions are sparse.
More impor-tantly, for our purposes, the measures that comparethe shapes of distributions do not allow semanticcomparisons at the level of distribution elements.Suppose R assigns high scores to angry and mad,only, while G assigns a high score to happy, only.Clearly, R is wrong.
But if insteadG had assigneda high score to irate, only, then R would be moreright than wrong because, at the level of the indi-vidual elements, angry and mad are similar to iratebut not similar to happy.We describe a series of measures, starting withthe familiar F1score, and discuss their applicabil-ity.
To illustrate the effectiveness of each measure,we will use the examples shown in Table 2.5.1 F1ScoreThe F1score measures the similarity between twosets of elements, R and G. F1= 1 when R = G126and F1= 0 when R and G share no elements.
F1is the harmonic mean of precision and recall:precision =|R ?G||R|, recall =|R ?G||G|,(1)F1= 2 ?precision ?
recallprecision+ recall.
(2)The F1score penalizes the responses in Table 3that include semantically similar labels to those inG, and fails to reflect the weights of the labels inG and R.5.2 Similarity-Aligned F1ScoreAlthough the standard F1does not immediately fitour needs, it is a good starting point.
We can in-corporate the semantic similarity of distribution el-ements by generalizing the formulas for precisionand recall as follows:precision =1|R|?r?Rmaxg?G?
(r, g) ,recall =1|G|?g?Gmaxr?R?
(r, g) ,(3)where ?
?
[0, 1] is a function that yields the simi-larity between two elements.
The standard F1has:?
(r, g) ={1 , if r = g0 , otherwise,but clearly ?
can be defined to take values pro-portional to the similarity of r and g. We canchoose from a wide range of semantic similarityand relatedness measures that are based on Word-Net (Pedersen et al., 2004).
The recent RNNLMof Mikolov opens the door to even more similar-ity measures based on vector space representationsof words (Mikolov et al., 2013).
After experi-mentations, we settled on one proposed by Hirstand St-Onge (1998).
It represents two lexicalizedconcepts as semantically close if their WordNetsynsets are connected by a path that is not toolong and that ?does not change direction too of-ten?
(Hirst and St-Onge, 1998).
We chose thismetric because it has a finite range, accommodatesnumerous POS pairs, and works well in practice.Given the generalized precision and recall for-mulas in Eq 3, our similarity-aligned (SA) F1score can be computed in the usual way, as theharmonic mean of precision and recall (Eq 2).SA-F1is inspired by the Constrained Entity-Aligned F-Measure (CEAF) metric proposedF1SA-F1CWSA-F1p r f1p r f1p r f1R10 0 0 1 .5231 .8 .89R21 1 1 1 1 1 .4 .4 .4R313.5 .4 1 1 1 1 1 1Table 3: The precision (p), recall (r), and F1(f1) scores under various evaluation models arepresented for the examples from Table 2.
Sup-pose that ?
(irate, angry) = ?
(irate,mad) =?
(afraid, scared) = 1, with ?
of any two identi-cal strings being 1, and ?
of all other pairs are 0.by (Luo, 2005) for coreference resolution.
CEAFcomputes an optimal one-to-one mapping betweensubsets of reference and system entities before itcomputes recall, precision and F. Similarly, SA-F1finds optimal mappings between the labels of thetwo sets based on ?
(this is what the max terms inEq 3 do).
Table 3 shows that SA-F1correctly re-wards the use of synonyms.
The high scores givento R2, however, indicate that it does not measurethe similarity between distribution shapes.5.3 Constrained Weighted Similarity-AlignedF1ScoreLet R(r) and G(r) be the probabilities of labelr in the R and G distributions, respectively.
Let?
?S(`) denote the best similarity score achievablewhen comparing elements from set S to ` us-ing the similarity function ?.
That is, ?
?S(`) =maxe?S?
(`, e).
We can easily weight ?
?S(`) bythe probability of `.
For example, we might re-define precision as?r?RR(r) ???G(r).
However,this would not account for the probability of r inthe gold standard distribution, G.An analogy might help here: Suppose we havean unknown ?mystery bag?
of 100 colored pen-cils that we will try to match with a ?responsebag?
of pencils.
If we fill our response bag with100 crimson pencils, while the mystery bag con-tains only 25 crimson pencils, then our precisionscore should get points only for the first 25 pen-cils, while the remaining 75 in the response bagshould not be rewarded.
For recall, the rewardgiven for each color in the mystery bag is cappedby the number of pencils of that color in the re-sponse bag.
The analogy is complete when weconsider that crimson pencils should perhaps bepartially rewarded when matched by cardinal, roseor cerise pencils.
In other words, a similarity mea-127sure should account for an accumulated mass ofsynonyms.
Let MS(`) denote the subset of termsfrom S that have the best similarity score to `:MS(`) = {e | ?
(`, e) = ?
?S(`), ?e ?
S} .We define new forms of precision and recall as:p =?r?Rmin??R(r),?e?MG(r)G(e)???
?G(r) ,r =?g?Gmin??G(g),?e?MR(g)R(e)???
?R(g) .
(4)The resulting constrained weighted similarity-aligned (CWSA) F1score is the harmonic meanof these new precision and recall scores.
Table 3shows that CWSA-F1yields the most intuitiveevaluation of the response distributions, down-weighting R2in favor of R3and R1.6 Experimental ProcedureAs described in Section 3, MTurk workers anno-tated 26 videos by identifying the actor types andmental state labels for each video.
The actor typesbecome query tuples of the form (activity, actor)and the mental state labels are compiled into oneprobability distribution over labels for each video,designated G. The query tuples were provided toour neighborhood models (Sec.
4), which returneda response distribution over mental state labels foreach video, designated R.We selected four videos of the 26 to calibratethe prune parameters ?
and the interpolation pa-rameters ?
(Sec.
4).
One of these videos containschildren, one has police involvement, and two con-tain adults.
We asked additional MTurk workers toannotate these videos, yielding an independent setof annotations to be used solely for calibration.The experimental question is, how well does Gmatch R for each video?7 Results & DiscussionsWe report the average performance of our mod-els along with two additional baseline methods inTable 4.
The na?
?ve baseline method unif simplybinds R to the initial seed set of 160 mental statelabels with uniform probability, while the strongerfreq baseline uses the occurrence frequency dis-tribution of the labels from the Gigaword corpus(note that only occurrences tagged as adjectives orF1CWSA-F1p r f1p r f1unif .107 .750 .187 .284 .289 .286freq .107 .750 .187 .362 .352 .355sent .194 .293 .227 .366 .376 .368vec .226 .145 .175 .399 .392 .393coref .264 .251 .253 .382 .461 .416event .231 .303 .256 .446 .488 .463ens .259 .296 .274 .488 .517 .500Table 4: The average evaluation performanceacross 26 different chase videos are shown against2 different baselines for all proposed models.
Boldfont indicates the best score in a given column.verbs were counted).
All average improvementsof the ensemble model over the baseline modelsare significant (p < 0.01).
Significance tests wereone-tailed and were based on nonparametric boot-strap resampling with 10, 000 iterations.Using the classical F1measure, the coref modelscored highest on precision, while the ensemblemethod did best on F1.
Not surprisingly, no modelcan top the baseline methods on recall as bothbaselines use the entire seed set of 160 terms.Even so, the average recall for the baselines wereonly .750, which means that the initial seed set didnot include words that were used by the MTurk an-notators.
As we?ve mentioned, the classical F1ismisleading because it does not credit synonyms.For example, in one movie, one of our modelswas rewarded once for matching the label angryand penalized six times for also reporting irate,enraged, raging, upset, furious, and mad.
Fre-quently, our models were penalized for using theterms scared and afraid instead of fearful.Under the CWSA-F1evaluation measure,which correctly accounts for both synonyms andlabel probabilities, our ensemble model performedbest.
The average CWSA-F1score of the ensem-ble model improves upon the simple uniform base-line unif by almost 75%, and over the strongerfreq baseline by over 40%.
The ensemble methodalso outperforms each individual method in allmeasured scores.
These improvements were alsofound to be significant.
This strongly suggeststhat the vec and event models are complementary,and not entirely redundant.
Furthermore, Table 4shows that the event model performs considerablybetter than coref.
This result emphasizes the im-portance of focusing on the mental state labels ofevent participants rather than considering all men-tal state terms collocated in the same sentence withan actor or action verb.128Models CWSA-F1 Versus coref p-valuewin-0 0.388682 ?0.027512 0.0067win-1 0.415328 ?0.000866 0.4629win-2 0.399777 ?0.016417 0.0311win-3 0.392832 ?0.023362 0.0029Table 5: The average CWSA-F1scores for thewin-n model with different window parameters areshown in comparison to the coref model.
Thecoref model outperformed all tested configura-tions, though the difference is not significant forn = 1.
The p-value based on the average differ-ences were obtained using one-tailed nonparamet-ric bootstrap resampling with 10, 000 iterations.Table 5 explores the effectiveness of corefer-ence resolution in expanding the neighborhoodarea.
The coref model outperformed the simplewindowing method under every tested configura-tion.
However, the improvement over windowingwith n = 1 is not significant.
This can be ex-plained by fact that immediately neighboring sen-tences are more likely to be related.
Moreover,since newswire articles tend to be short, the neigh-borhoods generated by win-1 tend to be similar tothose generated by coref.
In general, coref doesnot do worse than a simple windowing method andhas the bonus advantage of providing references tothe actors of interest for downstream processes.In Table 6, we show the performance resultsbased on the types of chase scenarios happening inthe videos.
The average scores under the uniformbaseline unif for chase videos involving childrenand sporting events are lower than for police andother chases.
This suggests that our seed set of160 mental state labels is biased towards the lattertypes of events, and is not as fit to describe chasesinvolving children.On average, videos involving police officersshow the biggest improvement in the CWSA-F1scores over the unif baseline (+0.2693), whereasvideos involving children received the lowest gain(+0.1517).
We believe this is the effect of theGigaword text corpus, which is a comprehensivearchive of newswire text, and thus is heavily bi-ased towards high-speed and violent chases in-volving the police.
The Gigaword corpus is notthe place to find children happily chasing eachother.
Similarly, sports-related chases, which arealso news-worthy, have a higher gain than chil-dren?s videos on average.Categories Unif Ensemble Gainchildren 0.2082 0.3599 +0.1517police 0.3313 0.6006 +0.2693sports 0.2318 0.4126 +0.1808others 0.3157 0.5457 +0.2300Table 6: The average CWSA-F1scores for the en-semble model are shown in comparison to the uni-form baseline method, categorize by video types.8 Conclusion and Future WorkWe introduced the novel task of identifying latentattributes in video scenes, specifically the men-tal states of actors in chase scenes.
We showedthat these attributes can be identified by using ex-plicit features of videos to query text corpora, andfrom the resulting texts extract attributes that arelatent in the videos.
We presented several largelyunsupervised methods for identifying distributionsof actors?
mental states in video scenes.
We de-fined a similarity measure, CWSA-F1, for com-paring distributions of mental state labels that ac-counts for both semantic relatedness of the labelsand their probabilities in the corresponding distri-butions.
We showed that very little informationfrom videos is needed to produce good results thatsignificantly outperform baseline methods.In the future, we plan to add more detectiontypes.
Additional contextual information fromvideos (e.g., scene locations) should help improveperformance, especially on tougher videos (e.g.,videos involving children chases).
Moreover, webelieve that the initial seed set of mental state la-bels can be learned simultaneously with the ex-traction patterns of the event model using a mutualbootstrapping method, similar to that of (Riloffand Jones, 1999).Currently, our experiments assume one distri-bution of mental state labels for each video.
Theydo not distinguish between the mental states of thechaser and chasee, while in reality these partici-pants may be in very different states of mind.
Ourevent model is capable of making this distinctionand we will test its performance on this task in thefuture.
We also plan to test the effectiveness of ourmodels with actual computer vision detectors.
Asa first approximation, we will simulate the noisynature of detectors by degrading the quality of an-notated data.
Using artificial noise on ground-truthdata, we can simulate the performance of real de-tectors and test the robustness of our models.129ReferencesAbdul Rehman Abbasi, Matthew N. Dailey, Nitin V.Afzulpurkar, and Takeaki Uno.
2009.
Student men-tal state inference from unintentional body gesturesusing dynamic Bayesian networks.
Journal on Mul-timodal User Interfaces, 3(1-2):21?31, December.Tadas Baltrusaitis, Daniel McDuff, NtombikayiseBanda, Marwa Mahmoud, Rana el Kaliouby, PeterRobinson, and Rosalind Picard.
2011.
Real-timeinference of mental states from facial expressionsand upper body gestures.
In Face and Gesture 2011,pages 909?914.
IEEE, March.Thorsten Brants.
2000.
TnT: A statistical part-of-speech tagger.
In Proceedings of the sixth confer-ence on Applied natural language processing, pages224?231, Morristown, NJ, USA.
Association forComputational Linguistics.Ernesto Brau, Jinyan Guan, Kyle Simek, Luca DelPero, Colin Reimer Dawson, and Kobus Barnard.2013.
Bayesian 3D Tracking from monocular video.In The IEEE International Conference on ComputerVision (ICCV), December.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of the35th annual meeting on Association for Computa-tional Linguistics -, pages 16?23, Morristown, NJ,USA.
Association for Computational Linguistics.R.
El Kaliouby and P. Robinson.
2004.
Real-Time In-ference of Complex Mental States from Facial Ex-pressions and Head Gestures.
In 2004 Conferenceon Computer Vision and Pattern Recognition Work-shop, pages 154?154.
IEEE.Pedro Felzenszwalb, David McAllester, and Deva Ra-manan.
2008.
A discriminatively trained, multi-scale, deformable part model.
In 2008 IEEE Confer-ence on Computer Vision and Pattern Recognition,pages 1?8.
IEEE, June.Ryan Gabbard, Marjorie Freedman, andRM Weischedel.
2011.
Coreference for learn-ing to extract relations: yes, Virginia, coreferencematters.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics:Human Language Technologies: short papers -Volume 2, pages 288?293.Maria Gendron, Debi Roberson, Jacoba Marietavan der Vyver, and Lisa Feldman Barrett.
2014.Cultural relativity in perceiving emotion from vo-calizations.
Psychological science, 25(4):911?20,April.J Giebel, DM Gavrila, and C Schn?orr.
2004.
Abayesian framework for multi-cue 3d object track-ing.
In Computer Vision-ECCV 2004, pages 241?252.Graeme Hirst and D St-Onge.
1998.
Lexical chains asrepresentations of context for the detection and cor-rection of malapropisms.
In Christiane Fellbaum,editor, WordNet: An Electronic Lexical Database(Language, Speech, and Communication), pages305?332.
The MIT Press.LJ Li, Hao Su, L Fei-Fei, and EP Xing.
2010.
Ob-ject bank: A high-level image representation forscene classification & semantic feature sparsifica-tion.
In Advances in Neural Information ProcessingSystems.Zhilei Liu and Shangfei Wang.
2011.
Emotion recog-nition using hidden Markov models from facial tem-perature sequence.
In ACII?11 Proceedings of the4th international conference on Affective computingand intelligent interaction - Volume Part II, pages240?247.Xiaoqiang Luo.
2005.
On coreference resolution per-formance metrics.
In Proceedings of the confer-ence on Human Language Technology and Empiri-cal Methods in Natural Language Processing - HLT?05, pages 25?32, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.MC De Marneffe, CD Manning, and Christopher Potts.2010.
?Was it good?
It was provocative.?
Learningthe meaning of scalar adjectives.
In Proceedings ofthe 48th Annual Meeting of the Association for Com-putational Linguistics, pages 167?176.Stephen J. McKenna, Sumer Jabri, Zoran Duric, AzrielRosenfeld, and Harry Wechsler.
2000.
TrackingGroups of People.
Computer Vision and Image Un-derstanding, 80(1):42?56, October.D M McNair, M Lorr, and L F Droppleman.
1971.Profile of Mood States (POMS).Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781, pages 1?12.George A. Miller.
1995.
WordNet: a lexicaldatabase for English.
Communications of the ACM,38(11):39?41, November.Mitra Mohtarami, Hadi Amiri, Man Lan, andChew Lim Tan.
2011.
Predicting the uncertaintyof sentiment adjectives in indirect answers.
In Pro-ceedings of the 20th ACM international conferenceon Information and knowledge management - CIKM?11, page 2485, New York, New York, USA.
ACMPress.CB Ng, YH Tay, and BM Goi.
2012.
Recognizing hu-man gender in computer vision: a survey.
PRICAI2012: Trends in Artificial Intelligence, 7458:335?346.S O?Hara and B.
A. Draper.
2012.
Scalable actionrecognition with a subspace forest.
In 2012 IEEEConference on Computer Vision and Pattern Recog-nition, pages 1210?1217.
IEEE, June.130Ted Pedersen, S Patwardhan, and J Michelizzi.
2004.WordNet::Similarity: measuring the relatedness ofconcepts.
In Proceedings of the Nineteenth Na-tional Conference on Artificial Intelligence (AAAI-04), pages 1024?1025, San Jose, CA.Ronald Poppe.
2010.
A survey on vision-based humanaction recognition.
Image and Vision Computing,28(6):976?990, June.Deva Ramanan, David a Forsyth, and Andrew Zisser-man.
2007.
Tracking people by learning their ap-pearance.
IEEE transactions on pattern analysisand machine intelligence, 29(1):65?81, January.E Riloff and R Jones.
1999.
Learning dictionaries forinformation extraction by multi-level bootstrapping.In Proceedings of the sixteenth national conferenceon Artificial intelligence (AAAI-1999), pages 474?479.S.
Sadanand and J. J. Corso.
2012.
Action bank: Ahigh-level representation of activity in video.
In2012 IEEE Conference on Computer Vision and Pat-tern Recognition, pages 1234?1241.
IEEE, June.C Schuldt, I Laptev, and B Caputo.
2004.
Recognizinghuman actions: a local SVM approach.
In Proceed-ings of the 17th International Conference on PatternRecognition, 2004.
ICPR 2004., pages 32?36 Vol.3.IEEE.M.
Sokolova and G. Lapalme.
2011.
Learning opin-ions in user-generated web content.
Natural Lan-guage Engineering, 17(04):541?567, March.Daniel Weinland, Remi Ronfard, and Edmond Boyer.2011.
A survey of vision-based methods for actionrepresentation, segmentation and recognition.
Com-puter Vision and Image Understanding, 115(2):224?241, February.Yi Yang and Deva Ramanan.
2011.
Articulated poseestimation with flexible mixtures-of-parts.
In CVPR2011, pages 1385?1392.
IEEE, June.131
