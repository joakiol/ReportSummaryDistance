Contextual Semantics for WSDERIC CRESTAN(1,2)(1) Sinequa SAS51-54, rue Ledru-Rollin92400 Ivry-sur-Seine, FranceCrestan@sinequa.com(2) Laboratoire Informatique d?AvignonB.P.
1228 Agroparc339 Chemin des Meinajaries84911 Avignon Cedex 9, FranceAbstractFor Sinequa?s second participation to theSenseval evaluation, two systems usingcontextual semantic have been proposed.Based on different approaches, they bothshare the same data preprocessing andenrichment.
The first system is acombined approach using semanticclassification trees and informationretrieval techniques.
For the secondsystem, the words from the context areconsidered as clues.
The final sense isdetermined by summing the weightassigned to each clue for a givenexample.1 IntroductionIn the framework of the Senseval-3 evaluationcampaign on Word Sense Disambiguation(WSD), we presented two systems relying ondifferent strategy.
The system SynLexEn is anevolution from the system used during theSenseval-2 campaign.
It is based on two steps.The first step uses semantic classification treeson a short context size.
A decision system basedon document similarity is used as second step.The novelty of this system resides in a newvision level on the context.
The semanticdictionary of Sinequa is extensively used in thisprocess.The second system, SynLexEn2, is based onweighted clues summation over a short contextsize.
From the training data, a score is computedfor each word in a short context size, for eachsense.In Section 2, the combined approach systemfor WSD is presented.
We first give an overviewof the data pre-processing that was applied(Section 2.1).
Then, a brief description ofSemantic Classification Trees is given (Section2.2) along with a description of additional dataused for semantic view of short and long context(Section 2.3 and Section 2.4).
Next, a semanticinformation retrieval system used in order toselect the appropriate sense is proposed (Section2.5).Finally, the SynLexEn2 system is presented inSection 3.
We then conclude with the evaluationresults for both systems in Section 4.2 Combined approachThe SinLexEn system is quite similar to thesystem used during the last Senseval-2 evaluationcampaign (Crestan et al, 2001).
It is based ontwo stages: the first stage uses three SemanticClassification Trees in parallel, trained ondifferent size of context.
Then, the second stagebrings in a decision system based on informationretrieval techniques.
The novelty of this approachdwells in the use of semantic resource asconceptual view on extended context in bothstages.2.1 Data pre-processingThe first step in order to get the most from thedata is to lemmatize and clean sentences.
Eachparagraph from the training and the test data arefirst passed though an internal tagger/lemmatizer.Then, some grammatical words are removed suchas articles and possessive pronouns.
Only oneword is not handled in this process, it is the wordto be disambiguated.
Indeed, previous works(Loupy et al, 1998) have shown that the form ofthis word could bring interesting clues about itsAssociation for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemspossible sense.
Other pronouns, such as subjectpronouns, are replaced by a generic PRP tag.2.2 Semantic Classification Trees for WSDThe Semantic Classification Trees (SCT) werefirst introduced by Kuhn and De Mori (1995).
Itcan be defined as simple binary decision trees.Training data are used in order to build one ormore trees for each word to be disambiguate.
AnSCT is made up of questions distributed alongthe tree nodes.
Then, each test sequence ispresented to the corresponding trees and followsa path along the SCT according to the way thequestions are answers.
When no more question isavailable (arrived at a leaf), the major sense isassigned to the test.In order to build the trees, the Gini impurity(Breiman et al, 1984) was used.
It is defined as:( )2/1)( ?
?=SsXsPXGwhere P(s/X) is the likelihood of sense s giventhe population X.At the first step of the tree building process,the Gini impurity is computed for each possiblequestions.
Then, the best question is selected andthe population made up of all the examples isdivided between the ones which answer thequestion (yes branch) and the others (no branch).The same process is recursively applied on eachbranch until the maximum tree depth is reached.In the framework of the SinLexEn system,three different trees have been built for eachword to be disambiguated.
They use differentcontext size, varying from one to three words oneach side of the target word.
Following is anexample of three training sequences usingrespectively 1, 2 and 3 words on each side of thetarget (0#sense):-1#make 0#sense 1#of-2#make -1#more 0#sense 1#to 2#annex-3#ceiling -2#add -1#to 0#sense 1#of 2#space 3#andThe number preceding the # character gives theposition of the word according to the target.
Theset of possible questions for the SCT buildingprocess is composed of all the words present inconsidered window width.
The tree shown inFigure 1 was built for the word ?sense?
on awindow width of 3 words.
Each nodecorresponds to a question, while leafs contain thesense to be assigned to the target.
The testsequence?1#make 0#sense 1#of will be assignedto sense%1:10:00:: sense (?the meaning of aword or expression?)
from WordNet (Miller etal., 1990).
For a more detail description of SCT,see (Crestan et al, 2003).2.3 Semantic in short contextWSD is much more easy to do for a humanthan for a machine.
By simply reading asentence, we can determine at a glance themeaning of a word in this particular context.However, we are not relying solely on what thewords look like.
The human brain is able to seethe correlation between ?open a box?
and ?open ajar?.
We have the ability to generalize oversimilar ?concepts?.
In order to follow thisscheme, we used the WordNet?s semantic classes(SC).
It enables a generalization over wordssharing the same high-level hyperonym.
Becausethe correct SC is not known for each word incontext, all the possible SC were included in theset of questions for a given word and position.The WordNet top ontology is separated in 26 SCfor the nouns and 15 for the verbs.
An extendeddescription of SC can be found in (Ciaramita andJohnson.
2003).
In the following example, thetwo first sequences share the same sense (?causeto open or to become open?)
whereas the lastFigure 1.
SCT example for the target word 'sense'-1#in ?1#of ?-1#of ?-1#senses ?
-1#make ?sense%1:10:00::sense%1:10:00:: sense%1:09:04:: sense%1:09:02::sense%1:09:05:: sense%1:00:00::y ny n y ny ny nsequence corresponds to another sense (?start tooperate or function or cause to start operating orfunctioning?
):0#open 1#box 1#06 1#20 1#23 1#25 1#350#open 1#jar 1#06 1#11 1#23 1#38 1#420#open 1#business 1#04  1#09 1#14Two of the five SC are common to both wordsbox and jar:?
06: nouns denoting man-made objects,?
23: nouns denoting quantities and units ofmeasure.However, they have nothing in common withthe word business.Although many wrong SC are proposed foreach word according to its context, we noticed a2% improvement on Senseval-2 data while usingthese ?high level information?.2.4 Semantic in full contextThe main improvement for this evaluation isthe use of semantic clues at a paragraph level.Sinequa has developed along the last 5 years alarge scale semantic dictionary of about 100.000entries.
All the word of the language areorganized across a semantic space composed of800 dimensions.
For example, a word such as?diary?
is present in the dimensions: calendar,story, book and newspaper.
It has been wildlyused in the information retrieval system Intuition(Manigot and Pelletier, 1997), (Loupy et al,2003).For each training sample, we summed thesemantic vectors of each word.
This step resultson a global semantic vector from which only the3 most representative dimensions (with highestscore) where kept.
That additional informationhas been used as possible question in the treebuilding process.
Then, the same semanticanalysis has been done on each test sentence.
Forexample, the major dimension represented in thenext sentence for the word material is?newspaper?
:?furthermore , nothing have yet be say about all theresearch that do not depend on the collection of datumby the sociologist ( primary datum ) but instead makeuse of secondary datum - the wealth of materialalready available from other source , such asgovernment statistics , personal diary , newspaper ,and other kind of information .
?This enables a new vision of the context on awider scale than the one we used with only shortcontext SCT.
Preliminary experiments carried onthe Sensenval-2 nouns have shown a 1%improvement.
Some nouns such as dyke, senseand spade have been dramatically improved(more than 5%).
Although, words such asauthority and post have had about 5% decrease inprecision.
A first hypothesis can be proposed toexplain the gain of some words while others havelost in precision:  the use of a wide contextsemantic is mostly benefic in the case ofhomonymy, while it is not when dealing withpolysemy.2.5 Semantic similarity for decision systemIn order to select the appropriate sense amongthe three senses proposed by the SCT, a decisionsystem was used.
It is based on the Intuitionsearch engine used on the Default mode: thewords and the semantic vectors of documents areused.
The final score is a combination betweenthe words score and the semantic score.Moreover, all the sentences linked to a givensense in the training data were concatenated inorder to form a unique document (pseudo-document).
Then, for a given test instance, thewhole paragraph was used to query the engine.The pseudo-document?s scores were then used inorder to select among the three senses proposedby the SCT.
A 2% improvement have beenobserved during the Senseval-2 evaluationcampaign while using this strategy.3 Maximum clues approachStarting from the same preprocessing used forthe combined approach, we implemented asimple approach based on Gini impurity.Considering a short context, the Gini impurity iscomputed for all the possible questions in thetraining data (including the questions aboutsemantic level).
For instance, if the question ?1#of appears 3 times with the sense S1, 1 timewith S2 and does not appear in 1 example ofsense S1 and 2 examples for sense S2, the finalscore for this question is:G(-1#of) = [1-(3/4)?-(1/4)?]
+ [1-(1/3)?-(2/3)?]
= 0.82Which corresponds to the Gini impurity of theexamples where ?1#of is present, plus the Giniimpurity for the examples where it is not.
Then, ascore is given for each sense according to eachquestion.
For the previous example, the score S1for the question ?1#of is:Score(S1, ?1#of) = P(S1/-1#of) * [G - G(?1#of)]Where G is the initial Gini impurity, minus theGini impurity of G(-1#of) and weighted by theprobability of S1 when ?1#of was observed.When disambiguating a test phrase, the scorefor each sense is computed by summing theindividual score for each question.
The highestscore gives the sense.This simple approach has shown similar resultsas those obtained with the combined approach onnouns.
Unlike the trees, this system is able tobenefit from all the clues in the training corpus.At the opposite, for the SCT, if two questions getrather good scores at first stage, only on questionwill be selected in order to build the node.
Thisprevents from using clues from the other questionbecause its population is (or might be) dividedbetween the two branches.4 Results and conclusionFor the third edition of the Senseval campaign,the sense repository for the verbs was different,using WordsMyth instead of WordNet.
Theproportion of nouns, verbs and adjectives wasalso different.
Because of these changes, it isdifficult to compare this evaluation results withthe previous ones.Fine CoarseSinLexEn 67.2% 74.3%SinLexEn2 66.8% 73.6%Table 1:  Precision for both systemsThe precisions of both systems are presented inTable 1.
The column named Fine corresponds tofine grain scores, while the column namedCoarse is for the coarse grained senses.Although we are using different strategies,both systems give approximately the same resultsfor fine grained precision.
According to theprevious evaluation, we can observe almost 5%increase in precision.
However, this increasecannot be taken as significant because of thedifferences between the evaluations.A comparative evaluation have to be carriednow in order to establish if a combination of bothsystem could improve the final score.ReferencesL.
Breiman, J. Friedman, R. Olshen, and C. Stone1984.
Classification and Regression Trees,Wadsworth.M.
Ciaramita and M. Johnson.
2003.
SupersenseTagging of Unknown Nouns in WordNet.
InProceedings of the Conference on EmpiricalMethods in Natural Language Processing.E.
Crestan, M. El-B?ze and C. de Loupy 2001.Improving WSD with Multi-Level View of ContextMonitored by Similarity Measure, Proceedings ofSenseval-2, 39th ACL.E.
Crestan, M. El-B?ze, C. de Loupy.
2003.
Peut-ontrouver la taille de contexte optimale end?sambigu?sation s?mantique ?, In Proceedings ofTALN 2003.C.
de Loupy, V. Combet and E. Crestan 2003.Linguistic resources for Information Retrieval,Proceedings ENABLER/ELSNET, InternationalRoadmap for Language Resources, Paris.R.
Kuhn and R. De Mori.
1995.
The Application ofSemantic Classification Trees to Natural LanguageUnderstanding, IEEE Transactions on PatternAnalysis and Machine Intelligence, 17(5),p 449-460.C.
de Loupy, M. El-B?ze and P.-F. Marteau.
1998.WSD based on three short context methods,SENSEVAL Workshop, Herstmontceux.L.
Manigot, B. Pelletier.
1997.
Intuition, une approchemath?matique et s?mantique du traitementd'informations textuelles.
Proceedings ofFractal'1997.
pp.
287-291.
Besan?on.G.
A. Miller, R. Beckwith, C. Fellbaum, D. Gross, andK.
Miller.
1990.
Introduction to WordNet: An on-line lexical database, International Journal ofLexicography, vol.
3(4), p 235-244.
