Proceedings of the Second Workshop on Statistical Machine Translation, pages 256?264,Prague, June 2007. c?2007 Association for Computational LinguisticsLinguistic Features for Automatic Evaluation of Heterogenous MT SystemsJesu?s Gime?nez and Llu?
?s Ma`rquezTALP Research Center, LSI DepartmentUniversitat Polite`cnica de CatalunyaJordi Girona Salgado 1?3, E-08034, Barcelona{jgimenez,lluism}@lsi.upc.eduAbstractEvaluation results recently reported byCallison-Burch et al (2006) and Koehn andMonz (2006), revealed that, in certain cases,the BLEU metric may not be a reliable MTquality indicator.
This happens, for in-stance, when the systems under evaluationare based on different paradigms, and there-fore, do not share the same lexicon.
Thereason is that, while MT quality aspects arediverse, BLEU limits its scope to the lex-ical dimension.
In this work, we suggestusing metrics which take into account lin-guistic features at more abstract levels.
Weprovide experimental results showing thatmetrics based on deeper linguistic informa-tion (syntactic/shallow-semantic) are able toproduce more reliable system rankings thanmetrics based on lexical matching alone,specially when the systems under evaluationare of a different nature.1 IntroductionMost metrics used in the context of Automatic Ma-chine Translation (MT) Evaluation are based onthe assumption that ?acceptable?
translations tend toshare the lexicon (i.e., word forms) in a predefinedset of manual reference translations.
This assump-tion works well in many cases.
However, severalresults in recent MT evaluation campaigns have castsome doubts on its general validity.
For instance,Callison-Burch et al (2006) and Koehn and Monz(2006) reported and analyzed several cases of strongdisagreement between system rankings provided byhuman assessors and those produced by the BLEUmetric (Papineni et al, 2001).
In particular, theynoted that when the systems under evaluation areof a different nature (e.g., rule-based vs. statistical,human-aided vs. fully automatical, etc.)
BLEU maynot be a reliable MT quality indicator.
The reason isthat BLEU favours MT systems which share the ex-pected reference lexicon (e.g., statistical systems),and penalizes those which use a different one.Indeed, the underlying cause is much simpler.
Ingeneral, lexical similarity is nor a sufficient neithera necessary condition so that two sentences conveythe same meaning.
On the contrary, natural lan-guages are expressive and ambiguous at differentlevels.
Consequently, the similarity between twosentences may involve different dimensions.
In thiswork, we hypothesize that, in order to ?fairly?
evalu-ate MT systems based on different paradigms, simi-larities at more abstract linguistic levels must be an-alyzed.
For that purpose, we have compiled a richset of metrics operating at the lexical, syntactic andshallow-semantic levels (see Section 2).
We presenta comparative study on the behavior of several met-ric representatives from each linguistic level in thecontext of some of the cases reported by Koehn andMonz (2006) and Callison-Burch et al (2006) (seeSection 3).
We show that metrics based on deeperlinguistic information (syntactic/shallow-semantic)are able to produce more reliable system rankingsthan those produced by metrics which limit theirscope to the lexical dimension, specially when thesystems under evaluation are of a different nature.2562 A Heterogeneous Metric SetFor our experiments, we have compiled a represen-tative set of metrics1 at different linguistic levels.We have resorted to several existing metrics, andwe have also developed new ones.
Below, we groupthem according to the level at which they operate.2.1 Lexical SimilarityMost of the current metrics operate at the lexicallevel.
We have selected 7 representatives from dif-ferent families which have been shown to obtainhigh levels of correlation with human assessments:BLEU We use the default accumulated score up tothe level of 4-grams (Papineni et al, 2001).NIST We use the default accumulated score up tothe level of 5-grams (Doddington, 2002).GTM We set to 1 the value of the e parame-ter (Melamed et al, 2003).METEOR We run all modules: ?exact?, ?porter-stem?, ?wn stem?
and ?wn synonymy?, in thatorder (Banerjee and Lavie, 2005).ROUGE We used the ROUGE-S* variant (skip bi-grams with no max-gap-length).
Stemming isenabled (Lin and Och, 2004a).mWER We use 1 ?
mWER (Nie?en et al, 2000).mPER We use 1 ?
mPER (Tillmann et al, 1997).Let us note that ROUGE and METEOR may con-sider stemming (i.e., morphological variations).
Ad-ditionally, METEOR may perform a lookup for syn-onyms in WordNet (Fellbaum, 1998).2.2 Beyond Lexical SimilarityModeling linguistic features at levels further thanthe lexical level requires the usage of more complexlinguistic structures.
We have defined what we call?linguistic elements?
(LEs).2.2.1 Linguistic ElementsLEs are linguistic units, structures, or relation-ships, such that a sentence may be partially seen as a?bag?
of LEs.
Possible kinds of LEs are: word forms,parts-of-speech, dependency relationships, syntacticphrases, named entities, semantic roles, etc.
Each1All metrics used in this work are publicly available insidethe IQMT Framework (Gime?nez and Amigo?, 2006).
http://www.lsi.upc.edu/?nlp/IQMTLE may consist, in its turn, of one or more LEs,which we call ?items?
inside the LE.
For instance, a?phrase?
LE may consist of ?phrase?
items, ?part-of-speech?
(PoS) items, ?word form?
items, etc.
Itemsmay be also combinations of LEs.
For instance, a?phrase?
LE may be seen as a sequence of ?word-form:PoS?
items.2.2.2 Similarity MeasuresWe are interested in comparing linguistic struc-tures, and linguistic units.
LEs allow for compar-isons at different granularity levels, and from dif-ferent viewpoints.
For instance, we might comparethe semantic structure of two sentences (i.e., whichactions, semantic arguments and adjuncts exist) orwe might compare lexical units according to the se-mantic role they play inside the sentence.
For thatpurpose, we use two very simple kinds of similaritymeasures over LEs: ?Overlapping?
and ?Matching?.We provide a general definition:Overlapping between items inside LEs, accordingto their type.
Formally:Overlapping(t) =Xi?itemst(hyp)count?hyp(i, t)Xi?itemst(ref)countref (i, t)where t is the LE type2, itemst(s) refers to theset of items occurring inside LEs of type t insentence s, countref(i, t) denotes the numberof times item i appears in the reference trans-lation inside a LE of type t, and count?hyp(i, t)denotes the number of times i appears in thecandidate translation inside a LE of type t, lim-ited by the number of times i appears in the ref-erence translation inside a LE of type t. Thus,?Overlapping?
provides a rough measure of theproportion of items inside elements of a cer-tain type which have been ?successfully?
trans-lated.
We also introduce a coarser metric, ?Over-lapping(*)?, which considers the uniformly aver-aged ?overlapping?
over all types:Overlapping(?)
= 1|T |Xt?TOverlapping(t)where T is the set of types.2LE types vary according to the specific LE class.
For in-stance, in the case of Named Entities types may be ?PER?
(i.e.,person), ?LOC?
(i.e., location), ?ORG?
(i.e., organization), etc.257Matching between items inside LEs, according totheir type.
Its definition is analogous to the?Overlapping?
definition, but in this case therelative order of the items is important.
Allitems inside the same element are considered asa single unit (i.e., a sequence in left-to-right or-der).
In other words, we are computing the pro-portion of ?fully?
translated elements, accord-ing to their type.
We also introduce a coarsermetric, ?Matching(*)?, which considers the uni-formly averaged ?Matching?
over all types.notes:?
?Overlapping?
and ?Matching?
operate on theassumption of a single reference translation.The extension to the multi-reference setting iscomputed by assigning the maximum value at-tained over all human references individually.?
?Overlapping?
and ?Matching?
are general met-rics.
We may apply them to specific scenariosby defining the class of linguistic elements anditems to be used.
Below, we instantiate thesemeasures over several particular cases.2.3 Shallow Syntactic SimilarityMetrics based on shallow parsing (?SP?)
analyzesimilarities at the level of PoS-tagging, lemmati-zation, and base phrase chunking.
Outputs andreferences are automatically annotated using state-of-the-art tools.
PoS-tagging and lemmatizationare provided by the SVMTool package (Gime?nez andMa`rquez, 2004), and base phrase chunking is pro-vided by the Phreco software (Carreras et al, 2005).Tag sets for English are derived from the Penn Tree-bank (Marcus et al, 1993).We instantiate ?Overlapping?
over parts-of-speechand chunk types.
The goal is to capture the propor-tion of lexical items correctly translated, accordingto their shallow syntactic realization:SP-Op-t Lexical overlapping according to the part-of-speech ?t?.
For instance, ?SP-Op-NN?
roughlyreflects the proportion of correctly translatedsingular nouns.
We also introduce a coarsermetric, ?SP-Op-*?
which computes averageoverlapping over all parts-of-speech.SP-Oc-t Lexical overlapping according to thechunk type ?t?.
For instance, ?SP-Oc-NP?
roughlyreflects the successfully translated proportionof noun phrases.
We also introduce a coarsermetric, ?SP-Oc-*?
which considers the averageoverlapping over all chunk types.At a more abstract level, we use the NISTmetric (Doddington, 2002) to compute accumu-lated/individual scores over sequences of:Lemmas ?
SP-NIST(i)l-nParts-of-speech ?
SP-NIST(i)p-nBase phrase chunks ?
SP-NIST(i)c-nFor instance, ?SP-NISTl-5?
corresponds to the accu-mulated NIST score for lemma n-grams up to length5, whereas ?SP-NISTip-5?
corresponds to the individ-ual NIST score for PoS 5-grams.2.4 Syntactic SimilarityWe have incorporated, with minor modifications,some of the syntactic metrics described by Liu andGildea (2005) and Amigo?
et al (2006) based on de-pendency and constituency parsing.2.4.1 On Dependency Parsing (DP)?DP?
metrics capture similarities between depen-dency trees associated to automatic and referencetranslations.
Dependency trees are provided by theMINIPAR dependency parser (Lin, 1998).
Similari-ties are captured from different viewpoints:DP-HWC(i)-l This metric corresponds to the HWCmetric presented by Liu and Gildea (2005).
Allhead-word chains are retrieved.
The fraction ofmatching head-word chains of a given length,?l?, is computed.
We have slightly modifiedthis metric in order to distinguish three differ-ent variants according to the type of items head-word chains may consist of:Lexical forms ?
DP-HWC(i)w -lGrammatical categories ?
DP-HWC(i)c-lGrammatical relationships ?
DP-HWC(i)r-lAverage accumulated scores up to a given chainlength may be used as well.
For instance,?DP-HWCiw-4?
retrieves the proportion of match-ing length-4 word-chains, whereas ?DP-HWCw -4?
retrieves average accumulated proportion ofmatching word-chains up to length-4.
Anal-ogously, ?DP-HWCc-4?, and ?DP-HWCr -4?
com-258pute average accumulated proportion of cate-gory/relationship chains up to length-4.DP-Ol|Oc|Or These metrics correspond exactly tothe LEVEL, GRAM and TREE metrics intro-duced by Amigo?
et al (2006).DP-Ol-l Overlapping between words hangingat level ?l?, or deeper.DP-Oc-t Overlapping between words directlyhanging from terminal nodes (i.e.
gram-matical categories) of type ?t?.DP-Or-t Overlapping between words ruledby non-terminal nodes (i.e.
grammaticalrelationships) of type ?t?.Node types are determined by grammatical cat-egories and relationships defined by MINIPAR.For instance, ?DP-Or-s?
reflects lexical overlap-ping between subtrees of type ?s?
(subject).
?DP-Oc-A?
reflects lexical overlapping between ter-minal nodes of type ?A?
(Adjective/Adverbs).?DP-Ol-4?
reflects lexical overlapping betweennodes hanging at level 4 or deeper.
Addition-ally, we consider three coarser metrics (?DP-Ol-*?, ?DP-Oc-*?
and ?DP-Or -*?)
which correspondto the uniformly averaged values over all lev-els, categories, and relationships, respectively.2.4.2 On Constituency Parsing (CP)?CP?
metrics capture similarities between con-stituency parse trees associated to automatic andreference translations.
Constituency trees are pro-vided by the Charniak-Johnson?s Max-Ent rerankingparser (Charniak and Johnson, 2005).CP-STM(i)-l This metric corresponds to the STMmetric presented by Liu and Gildea (2005).All syntactic subpaths in the candidate and thereference trees are retrieved.
The fraction ofmatching subpaths of a given length, ?l?, iscomputed.
For instance, ?CP-STMi-5?
retrievesthe proportion of length-5 matching subpaths.Average accumulated scores may be computedas well.
For instance, ?CP-STM-9?
retrieves av-erage accumulated proportion of matching sub-paths up to length-9.2.5 Shallow-Semantic SimilarityWe have designed two new families of metrics, ?NE?and ?SR?, which are intended to capture similari-ties over Named Entities (NEs) and Semantic Roles(SRs), respectively.2.5.1 On Named Entities (NE)?NE?
metrics analyze similarities between auto-matic and reference translations by comparing theNEs which occur in them.
Sentences are automati-cally annotated using the BIOS package (Surdeanuet al, 2005).
BIOS requires at the input shallowparsed text, which is obtained as described in Sec-tion 2.3.
See the list of NE types in Table 1.Type DescriptionORG OrganizationPER PersonLOC LocationMISC MiscellaneousO Not-a-NEDATE Temporal expressionsNUM Numerical expressionsANGLE QUANTITYDISTANCE QUANTITYSIZE QUANTITY QuantitiesSPEED QUANTITYTEMPERATURE QUANTITYWEIGHT QUANTITYMETHODMONEYLANGUAGE OtherPERCENTPROJECTSYSTEMTable 1: Named Entity types.We define two types of metrics:NE-Oe-t Lexical overlapping between NEs accord-ing to their type t. For instance, ?NE-Oe-PER?
re-flects lexical overlapping between NEs of type?PER?
(i.e., person), which provides a rough es-timate of the successfully translated proportionof person names.
The ?NE-Oe-*?
metric consid-ers the average lexical overlapping over all NEtypes.
This metric includes the NE type ?O?
(i.e., Not-a-NE).
We introduce another variant,?NE-Oe-**?, which considers only actual NEs.NE-Me-t Lexical matching between NEs accord-ing to their type t. For instance, ?NE-Me-LOC?reflects the proportion of fully translated NEsof type ?LOC?
(i.e., location).
The ?NE-Me-*?259metric considers the average lexical matchingover all NE types, this time excluding type ?O?.Other authors have measured MT quality overNEs in the recent literature.
In particular, the ?NE-Me-*?
metric is similar to the ?NEE?
metric definedby Reeder et al (2001).2.5.2 On Semantic Roles (SR)?SR?
metrics analyze similarities between auto-matic and reference translations by comparing theSRs (i.e., arguments and adjuncts) which occur inthem.
Sentences are automatically annotated usingthe SwiRL package (Ma`rquez et al, 2005).
Thispackage requires at the input shallow parsed text en-riched with NEs, which is obtained as described inSection 2.5.1.
See the list of SR types in Table 2.Type DescriptionA0A1A2 arguments associated with a verb predicate,A3 defined in the PropBank Frames scheme.A4A5AA Causative agentAM-ADV Adverbial (general-purpose) adjunctAM-CAU Causal adjunctAM-DIR Directional adjunctAM-DIS Discourse markerAM-EXT Extent adjunctAM-LOC Locative adjunctAM-MNR Manner adjunctAM-MOD Modal adjunctAM-NEG Negation markerAM-PNC Purpose and reason adjunctAM-PRD Predication adjunctAM-REC Reciprocal adjunctAM-TMP Temporal adjunctTable 2: Semantic Roles.We define three types of metrics:SR-Or-t Lexical overlapping between SRs accord-ing to their type t. For instance, ?SR-Or-A0?
re-flects lexical overlapping between ?A0?
argu-ments.
?SR-Or -*?
considers the average lexicaloverlapping over all SR types.SR-Mr-t Lexical matching between SRs accord-ing to their type t. For instance, the met-ric ?SR-Mr-AM-MOD?
reflects the proportion offully translated modal adjuncts.
The ?SR-Mr -*?metric considers the average lexical matchingover all SR types.SR-Or This metric reflects ?role overlapping?, i.e..overlapping between semantic roles indepen-dently from their lexical realization.Note that in the same sentence several verbs, withtheir respective SRs, may co-occur.
However, themetrics described above do not distinguish betweenSRs associated to different verbs.
In order to accountfor such a distinction we introduce a more restric-tive version of these metrics (?SR-Mrv-t?, ?SR-Orv-t?,?SR-Mrv -*?, ?SR-Orv -*?, and ?SR-Orv ?
), which requireSRs to be associated to the same verb.3 Experimental WorkIn this section, we study the behavior of someof the metrics described in Section 2, accordingto the linguistic level at which they operate.
Wehave selected a set of coarse-grained metric vari-ants (i.e., accumulated/average scores over linguis-tic units and structures of different kinds)3.
We ana-lyze some of the cases reported by Koehn and Monz(2006) and Callison-Burch et al (2006).
We distin-guish different evaluation contexts.
In Section 3.1,we study the case of a single reference translationbeing available.
In principle, this scenario shoulddiminish the reliability of metrics based on lexicalmatching alone, and favour metrics based on deeperlinguistic features.
In Section 3.2, we study the caseof several reference translations available.
This sce-nario should alleviate the deficiencies caused by theshallowness of metrics based on lexical matching.We also analyze separately the case of ?homoge-neous?
systems (i.e., all systems being of the samenature), and the case of ?heterogenous?
systems (i.e.,there exist systems based on different paradigms).As to the metric meta-evaluation criterion, the twomost prominent criteria are:Human Acceptability Metrics are evaluated on thebasis of correlation with human evaluators.Human Likeness Metrics are evaluated in terms ofdescriptive power, i.e., their ability to distin-guish between human and automatic transla-tions (Lin and Och, 2004b; Amigo?
et al, 2005).In our case, metrics are evaluated on the basis of?Human Acceptability?.
Specifically, we use Pear-son correlation coefficients between metric scores3When computing ?lexical?
overlapping/matching, we uselemmas instead of word forms.260and the average sum of adequacy and fluency as-sessments at the document level.
The reason isthat meta-evaluation based on ?Human Likeness?
re-quires the availability of heterogenous test beds (i.e.,representative sets of automatic outputs and humanreferences), which, unfortunately, is not the case ofall the tasks under study.
First, because most transla-tion systems are statistical.
Second, because in mostcases only one reference translation is available.3.1 Single-reference ScenarioWe use some of the test beds corresponding tothe ?NAACL 2006 Workshop on Statistical MachineTranslation?
(WMT 2006) (Koehn and Monz, 2006).Since linguistic features described in Section 2 areso far implemented only for the case of English be-ing the target language, among the 12 translationtasks available, we studied only the 6 tasks corre-sponding to the Foreign-to-English direction.
A sin-gle reference translation is available.
System out-puts consist of 2000 and 1064 sentences for the ?in-domain?
and ?out-of-domain?
test beds, respectively.In each case, human assessments on adequacy andfluency are available for a subset of systems and sen-tences.
Table 3 shows the number of sentences as-sessed in each case.
Each sentence was evaluatedby two different human judges.
System scores havebeen obtained by averaging over all sentence scores.in out sysFrench-to-English 2,247 1,274 11/14German-to-English 2,401 1,535 10/12Spanish-to-English 1,944 1,070 11/15Table 3: WMT 2006.
?in?
and ?out?
columnsshow the number of sentences assessed for the ?in-domain?
and ?out-of-domain?
subtasks.
The ?sys?column shows the number of systems counting onhuman assessments with respect to the total numberof systems which presented to each task.Evaluation of Heterogeneous SystemsIn four of the six translation tasks under study, allthe systems are statistical except ?Systran?, which isrule-based.
This is the case of the German/French-to-English in-domain/out-of-domain tasks.
Table 4shows correlation with human assessments for somemetric representatives at different linguistic levels.fr2en de2enLevel Metric in out in out1-PER 0.73 0.64 0.57 0.461-WER 0.73 0.73 0.32 0.38BLEU 0.71 0.87 0.60 0.67Lexical NIST 0.74 0.82 0.56 0.63GTM 0.84 0.86 0.12 0.70METEOR 0.92 0.95 0.76 0.81ROUGE 0.85 0.89 0.65 0.79SP-Op-* 0.81 0.88 0.64 0.71SP-Oc-* 0.81 0.89 0.65 0.75Shallow SP-NISTl-5 0.75 0.81 0.56 0.64Syntactic SP-NISTp-5 0.75 0.91 0.77 0.77SP-NISTc-5 0.73 0.88 0.71 0.54DP-HWCw-4 0.76 0.88 0.64 0.74DP-HWCc-4 0.93 0.97 0.88 0.72DP-HWCr-4 0.92 0.96 0.91 0.76Syntactic DP-Ol-* 0.87 0.94 0.84 0.84DP-Oc-* 0.91 0.95 0.88 0.87DP-Or-* 0.87 0.97 0.91 0.88CP-STM-9 0.93 0.95 0.93 0.87NE-Me-* 0.80 0.79 0.93 0.63NE-Oe-* 0.79 0.76 0.91 0.59NE-Oe-** 0.81 0.87 0.63 0.70SR-Mr-* 0.83 0.95 0.92 0.84Shallow SR-Or-* 0.89 0.95 0.88 0.90Semantic SR-Or 0.95 0.85 0.80 0.75SR-Mrv-* 0.77 0.92 0.72 0.85SR-Orv-* 0.81 0.93 0.76 0.94SR-Orv 0.84 0.93 0.81 0.92Table 4: WMT 2006.
Evaluation of HeterogeneousSystems.
French-to-English (fr2en) / German-to-English (de2en), in-domain and out-of-domain.Although the four cases are different, we haveidentified several regularities.
For instance, BLEUand, in general, all metrics based on lexical match-ing alone, except METEOR, obtain significantlylower levels of correlation than metrics based ondeeper linguistic similarities.
The problem with lex-ical metrics is that they are unable to capture the ac-tual quality of the ?Systran?
system.
Interestingly,METEOR obtains a higher correlation, which, inthe case of French-to-English, rivals the top-scoringmetrics based on deeper linguistic features.
The rea-son, however, does not seem to be related to its ad-ditional linguistic operations (i.e., stemming or syn-onymy lookup), but rather to the METEOR matchingstrategy itself (unigram precision/recall).Metrics at the shallow syntactic level are in thesame range of lexical metrics.
At the properlysyntactic level, metrics obtain in most cases highcorrelation coefficients.
However, the ?DP-HWCw-4?metric, which, although from the viewpoint of de-261pendency relationships, still considers only lexicalmatching, obtains a lower level of correlation.
Thisreinforces the idea that metrics based on rewardinglong n-grams matchings may not be a reliable qual-ity indicator in these cases.At the level of shallow semantics, while ?NE?metrics are not equally useful in all cases, ?SR?
met-rics prove very effective.
For instance, correlationattained by ?SR-Or-*?
reveals that it is important totranslate lexical items according to the semantic rolethey play inside the sentence.
Moreover, correlationattained by the ?SR-Mr-*?
metric is a clear indicationthat in order to achieve a high quality, it is impor-tant to ?fully?
translate ?whole?
semantic structures(i.e., arguments/adjuncts).
The existence of all thesemantic structures (?SR-Or?
), specially associated tothe same verb (?SR-Orv?
), is also important.Evaluation of Homogeneous SystemsIn the two remaining tasks, Spanish-to-Englishin-domain/out-of-domain, all the systems are sta-tistical.
Table 5 shows correlation with human as-sessments for some metric representatives.
In thiscase, BLEU proves very effective, both in-domainand out-of-domain.
Indeed, all metrics based on lex-ical matching obtain high levels of correlation withhuman assessments.
However, still metrics based ondeeper linguistic analysis attain in most cases highercorrelation coefficients, although not as significantlyhigher as in the case of heterogeneous systems.3.2 Multiple-reference ScenarioWe study the case reported by Callison-Burch etal.
(2006) in the context of the Arabic-to-Englishexercise of the ?2005 NIST MT Evaluation Cam-paign?4 (Le and Przybocki, 2005).
In this case allsystems are statistical but ?LinearB?, a human-aidedMT system (Callison-Burch, 2005).
Five referencetranslations are available.
System outputs consist of1056 sentences.
We obtained permission5 to use 7system outputs.
For six of these systems we counted4http://www.nist.gov/speech/tests/summaries/2005/mt05.htm5Due to data confidentiality, we contacted each participantindividually and asked for permission to use their data.
A num-ber of groups and companies responded positively: Univer-sity of Southern California Information Sciences Institute (ISI),University of Maryland (UMD), Johns Hopkins University &University of Cambridge (JHU-CU), IBM, University of Edin-burgh, MITRE and LinearB.es2enLevel Metric in out1-PER 0.82 0.781-WER 0.88 0.83BLEU 0.89 0.87Lexical NIST 0.88 0.84GTM 0.86 0.80METEOR 0.84 0.81ROUGE 0.89 0.83SP-Op-* 0.88 0.80SP-Oc-* 0.89 0.84Shallow SP-NISTl-5 0.88 0.85Syntactic SP-NISTp-5 0.85 0.86SP-NISTc-5 0.84 0.83DP-HWCw-4 0.94 0.83DP-HWCc-4 0.91 0.87DP-HWCr-4 0.91 0.88Syntactic DP-Ol-* 0.91 0.84DP-Oc-* 0.88 0.83DP-Or-* 0.88 0.84CP-STM-9 0.89 0.86NE-Me-* 0.75 0.76NE-Oe-* 0.71 0.71NE-Oe-** 0.88 0.80SR-Mr-* 0.86 0.82Shallow SR-Or-* 0.92 0.92Semantic SR-Or 0.91 0.92SR-Mrv-* 0.89 0.88SR-Orv-* 0.91 0.92SR-Orv 0.91 0.91Table 5: WMT 2006.
Evaluation of HomogeneousSystems.
Spanish-to-English (es2en), in-domainand out-of-domain.on a subjective manual evaluation based on ade-quacy and fluency for a subset of 266 sentences (i.e.,1596 sentences were assessed).
Each sentence wasevaluated by two different human judges.
Systemscores have been obtained by averaging over all sen-tence scores.Table 6 shows the level of correlation with hu-man assessments for some metric representatives(see ?ALL?
column).
In this case, lexical metricsobtain extremely low levels of correlation.
Again,the problem is that lexical metrics are unable to cap-ture the actual quality of ?LinearB?.
At the shallowsyntactic level, only metrics which do not considerany lexical information (?SP-NISTp-5?
and ?SP-NISTc-5?)
attain a significantly higher quality.
At the prop-erly syntactic level, all metrics attain a higher corre-lation.
At the shallow semantic level, again, while?NE?
metrics are not specially useful, ?SR?
metricsprove very effective.On the other hand, if we remove ?LinearB?
(see262ar2enLevel Metric ALL SMT1-PER -0.35 0.751-WER -0.50 0.69BLEU 0.06 0.83Lexical NIST 0.04 0.81GTM 0.03 0.92ROUGE -0.17 0.81METEOR 0.05 0.86SP-Op-* 0.05 0.84SP-Oc-* 0.12 0.89Shallow SP-NISTl-5 0.04 0.82Syntactic SP-NISTp-5 0.42 0.89SP-NISTc-5 0.44 0.68DP-HWCw-4 0.52 0.86DP-HWCc-4 0.80 0.75DP-HWCr-4 0.88 0.86Syntactic DP-Ol-* 0.51 0.94DP-Oc-* 0.53 0.91DP-Or-* 0.72 0.93CP-STM-9 0.74 0.95NE-Me-* 0.33 0.78NE-Oe-* 0.24 0.82NE-Oe-** 0.04 0.81SR-Mr-* 0.72 0.96Shallow SR-Or-* 0.61 0.87Semantic SR-Or 0.66 0.75SR-Mrv-* 0.68 0.97SR-Orv-* 0.47 0.84SR-Orv 0.46 0.81Table 6: NIST 2005.
Arabic-to-English (ar2en) ex-ercise.
?ALL?
refers to the evaluation of all systems.?SMT?
refers to the evaluation of statistical systemsalone (i.e., removing ?LinearB?).?SMT?
column), lexical metrics attain a much highercorrelation, in the same range of metrics based ondeeper linguistic information.
However, still met-rics based on syntactic parsing, and semantic roles,exhibit a slightly higher quality.4 ConclusionsWe have presented a comparative study on thebehavior of a wide set of metrics for automaticMT evaluation at different linguistic levels (lexical,shallow-syntactic, syntactic, and shallow-semantic)under different scenarios.
We have shown, throughempirical evidence, that linguistic features at moreabstract levels may provide more reliable systemrankings, specially when the systems under evalu-ation do not share the same lexicon.We strongly believe that future MT evaluationcampaigns should benefit from these results, by in-cluding metrics at different linguistic levels.
For in-stance, the following set could be used:{ ?DP-HWCr-4?, ?DP-Oc-*?, ?DP-Ol-*?, ?DP-Or-*?, ?CP-STM-9?, ?SR-Or-*?, ?SR-Orv?
}All these metrics are among the top-scoring in allthe translation tasks studied.
However, none of thesemetrics provides, in isolation, a ?global?
measure ofquality.
Indeed, all these metrics focus on ?partial?aspects of quality.
We believe that, in order to per-form ?global?
evaluations, different quality dimen-sions should be integrated into a single measure ofquality.
With that purpose, we are currently explor-ing several metric combination strategies.
Prelim-inary results, based on the QUEEN measure insidethe QARLA Framework (Amigo?
et al, 2005), indi-cate that metrics at different linguistic levels may berobustly combined.Experimental results also show that metrics re-quiring linguistic analysis seem very robust againstparsing errors committed by automatic linguisticprocessors, at least at the document level.
Thatis very interesting, taking into account that, whilereference translations are supposedly well formed,that is not always the case of automatic translations.However, it remains pending to test the behaviour atthe sentence level, which could be very useful for er-ror analysis.
Moreover, relying on automatic proces-sors implies two other important limitations.
First,these tools are not available for all languages.
Sec-ond, usually they are too slow to allow for massiveevaluations, as required, for instance, in the case ofsystem development.
In the future, we plan to incor-porate more accurate, and possibly faster, linguisticprocessors, also for languages other than English, asthey become publicly available.AcknowledgementsThis research has been funded by the Span-ish Ministry of Education and Science, projectsOpenMT (TIN2006-15307-C03-02) and TRAN-GRAM (TIN2004-07925-C03-02).
We are recog-nized as a Quality Research Group (2005 SGR-00130) by DURSI, the Research Department of theCatalan Government.
Authors are thankful to theWMT organizers for providing such valuable testbeds.
Authors are also thankful to Audrey Le (fromNIST), and to the 2005 NIST MT Evaluation Cam-paign participants who agreed to share their system263outputs and human assessments for the purpose ofthis research.ReferencesEnrique Amigo?, Julio Gonzalo, Anselmo Pen?as, and Fe-lisa Verdejo.
2005.
QARLA: a Framework for theEvaluation of Automatic Sumarization.
In Proceed-ings of the 43th Annual Meeting of the Association forComputational Linguistics.Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Llu??sMa`rquez.
2006.
MT Evaluation: Human-Like vs. Hu-man Acceptable.
In Proceedings of COLING-ACL06.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Pro-ceedings of ACL Workshop on Intrinsic and ExtrinsicEvaluation Measures for Machine Translation and/orSummarization.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the Role of BLEU in Ma-chine Translation Research.
In Proceedings of EACL.Chris Callison-Burch.
2005.
Linear B system descrip-tion for the 2005 NIST MT evaluation exercise.
InProceedings of the NIST 2005 Machine TranslationEvaluation Workshop.Xavier Carreras, Llu?
?s Ma?rquez, and Jorge Castro.
2005.Filtering-ranking perceptron learning for partial pars-ing.
Machine Learning, 59:1?31.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proceedings of ACL.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proceedings of the 2nd IHLT.C.
Fellbaum, editor.
1998.
WordNet.
An Electronic Lexi-cal Database.
The MIT Press.Jesu?s Gime?nez and Enrique Amigo?.
2006.
IQMT: AFramework for Automatic Machine Translation Eval-uation.
In Proceedings of the 5th LREC.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2004.
SVMTool: Ageneral POS tagger generator based on Support VectorMachines.
In Proceedings of 4th LREC.Philipp Koehn and Christof Monz.
2006.
Manual andAutomatic Evaluation of Machine Translation betweenEuropean Languages.
In Proceedings of the Workshopon Statistical Machine Translation, pages 102?121.Audrey Le and Mark Przybocki.
2005.
NIST 2005 ma-chine translation evaluation official results.
Technicalreport, NIST, August.Chin-Yew Lin and Franz Josef Och.
2004a.
Auto-matic Evaluation of Machine Translation Quality Us-ing Longest Common Subsequence and Skip-BigramStatics.
In Proceedings of ACL.Chin-Yew Lin and Franz Josef Och.
2004b.
ORANGE: aMethod for Evaluating Automatic Evaluation Metricsfor Machine Translation.
In Proceedings of COLING.Dekang Lin.
1998.
Dependency-based Evaluation ofMINIPAR.
In Proceedings of the Workshop on theEvaluation of Parsing Systems.Ding Liu and Daniel Gildea.
2005.
Syntactic Featuresfor Evaluation of Machine Translation.
In Proceed-ings of ACL Workshop on Intrinsic and Extrinsic Eval-uation Measures for Machine Translation and/or Sum-marization.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics, 19(2):313?330.I.
Dan Melamed, Ryan Green, and Joseph P. Turian.2003.
Precision and Recall of Machine Translation.In Proceedings of HLT/NAACL.Llu?
?s Ma`rquez, Mihai Surdeanu, Pere Comas, andJordi Turmo.
2005.
Robust Combination Strat-egy for Semantic Role Labeling.
In Proceedings ofHLT/EMNLP.S.
Nie?en, F.J. Och, G. Leusch, and H. Ney.
2000.
Eval-uation Tool for Machine Translation: Fast Evaluationfor MT Research.
In Proceedings of the 2nd LREC.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic evalu-ation of machine translation, rc22176, ibm.
Technicalreport, IBM T.J. Watson Research Center.Florence Reeder, Keith Miller, Jennifer Doyon, and JohnWhite.
2001.
The Naming of Things and the Con-fusion of Tongues: an MT Metric.
In Proceedingsof the Workshop on MT Evaluation ?Who did what towhom??
at MT Summit VIII, pages 55?59.Mihai Surdeanu, Jordi Turmo, and Eli Comelles.
2005.Named Entity Recognition from Spontaneous Open-Domain Speech.
In Proceedings of the 9th Inter-national Conference on Speech Communication andTechnology (Interspeech).C.
Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf.1997.
Accelerated DP based Search for StatisticalTranslation.
In Proceedings of European Conferenceon Speech Communication and Technology.264
