Application of Analogical Modelling to Example Based Machine TranslationChr i s tos  Ma lavazos  l' 2 Ste l ios P iper id is  !
'2%stitute for Language and Speech Processing, 2National Technical University of Athens6 Artemidos & Epidavrou, 151 25 Marousi, Athens, Greece{christos, pip}@ilsp.grAbstractThis paper describes a self-modelling, incremental gorithm for learning translation rules from existingbilingual corpora.
The notions of supracontext and subcontext are extended to encompass bilingualinformation through simultaneous analogy on both source and target sentences and juxtaposition ofcorresponding results.
Analogical modelling is performed uring the learning phase and translationpatterns are projected in a multi-dimensional analogical network.
The proposed fi'amework was evaluatedon a small training corpus providing promising results.
Suggestions to improve system performance are1.
IntroductionIdeally, an EBMT system must determinecorrespondences at a sub-sentence l vel if optimaladaptation of matching fragments i to be achieved(Collins, B., & Cunningham, P. 1995).
In practice,EBMT systems that operate at sub-sentence l velinvolve the dynamic derivation of the optimumlength of segments of the input sentence byanalysing the available parallel corpora.
Thisrequires a procedure for determining the best"cover" of an input text by segments of sentencescontained in the database (Nirenburg, S.Domashnev, C., Grannes, D. 1993), (Cranias, L. etal 1994), (Frederking, R., Nirenburg, S., 1994),(Sato, S. 1995).
What is needed is a procedure foraligning parallel texts at sub-sentence level,(Sadler, V., Vendehnans, R. 1990), (Boutsis, S.,Piperidis, S. 1998).
If sub-sentence alignment isavailable, the approach is fully automated but isquite vulnerable to the problem of low quality, aswell as to translational mbiguity problems whenthe produced segments are rather small.Several approaches aim at proceeding a stepfurther, by attempting to build a transfer-rule basein the form of abstract representations throughdifferent ypes of generalization processes appliedon the available corpora relying on different levelsof linguistic information and processing (Kaji et al92), (Juola, P. 1994), (Furuse, O., Iida, H. 1996),(Veale, T. and Way, A.
1997), (McTait, K., et al1999), thus providing more complete "context"information to the translation phase.
The deeper thelinguistic analysis involved in such a process, themore flexible the final translation structures will beand the better the quality of the results.
However,tiffs kind of analysis unquestionably leads to morecomputationally expensive and difficult to obtainsystems.
Our approach consists in a fully modularanalogical fiamework, which can cope with lack ofresources, and will perform even better when theseare available.Analogical Modelling (AM) has been proposed asan alternative model of language usage.
The mainassumption underlying this approach is that manyaspects of speaker performance are betteraccounted for in terms of "analogy", i.e.
theidentification of similarities and differences withforms in memory (the lexicon), than by referring toexplicit and inaccessible rules.
By "analogy" wemean the process of matching between an inputpattern and a database of stored examples(exemplars).
The result of this matching process isa collection of examples called the "analogical set"and classification of the input pattern is achievedthrough extrapolation fi'om this set.
At any giventime, the main source of knowledge consists in adatabase of stored translation examples.
Theseexamples themselves are used to classify newitems, without intermediate abstraction i the formof rules.
In order to achieve this exhaustivedatabase search is needed, and during this search,less relevant examples need to be discarded.
Alltext features are equally important initially, andserve to partition the database into several disjointclasses of examples.In contrast o most of the analogy-based systemsour approach applies tile same principles during thelearning phase in an attempt to extract appropriategeneralizations (translatiou rules) based onsimilarities and differences between inputexemplars.
In this way, analogy is treated as more516than simple pairwisc simila,ity between input anddatabase xemplars, rather it is conside,'ed as themain relation underlying a more complex networkof relations between database xemplars.2.
GeneralThe main idea behind otu" approach is based o,1 theobservation that given any source and targetlanguage sentence pair, any alteration of the sourcesentence will most likely result in one or morechanges in the respective target, while it is alsohighly likely that constant and variable units of thesource sentence correspond to constant and variabletarget units respectively.
Apart from cases of socalled "translational divergences" (Dorr, B.
1994)as well as cases of idiomatic expressions, in mosteases the above assumption hokts true.
Especiallyin the case of technical sublanguagcs, where ratherliteral and accurate translation is expected,?
y '~ "translational divergences are limited whileidiomatic expressions can be captured and finallyrejected fiom the main process, through certainconstraints, as this will be explained later on.The matching process as this is described by(Daelemans W., et al 1997) based on Skousen'sanalogical modelling algorithm (Skousen, R. 1989),consists of two subsequent s ages.
The first stage ofthe matching process is the construction of"subcontexts", these are sets o1' examples and theyare obtained by matching the input tmttern, featureby feature, to each database item on an equal/not-equal base, and classify the database examplesaccordingly.
Taking the input pattern ABC as anexample ight (=2 3) different and mutually disjointsubcontexts would be constructed:ABC, ~,BC, ABC, ABC, ABC, ABC, ABC, ABCwhere the macron denotes complementation.
Thusexemplars in the second class share only the secondand third feature with the input pattern.in the following stage "supraeontexts" areconstructed by generalising over specific featurevalues.
This is done by systematically discardingfeatures fi'om the input pattern and taking the unionof the subcontexts that are subsumed by this newpattern.
Supracontexts can be ordered with respectto generality, so that most specific supracontextcontains items that share all features with the inputpattern while the less specific ones those items thatshare at least one feature.
The most generalsupracontext contains all database exampleswhether or not they share any features with theinput pattern.Some exemplary supracontexts together with therespective subeontexts for the input pattern ABCare I)rcsented in the following table?A B - ABe, ABC,A-  C ABC, ABC- B C ABe, ~,BCA- -  ABC, ABC, AB(~, ABC1,1 addition, our approach introduces a seconddimension to tile above described process, that oflanguage, by simultaneously performing thematching process to target language equivalentsand aligning individual results, based on theprinciples described earlier.
Therefore, what we areultimately searching for, are source and targetsentence pairs for which evidence ofcorrespondence between any or all of respectivesubcontcxts within the available training corpora isavailable.
This will subsequently lead to linksbetween respective supracontexts.
For example :\[As BsCs\] o \[At Bt Ct \ ] - '~AND __~>-=> \[As Bs-\] ~ \[At Bt-\]\[As B~Cs\] o \[At Bt Ct \ ] .
.
f lSubcontexts Supracontexts(Where s = Source Language, t = Target Language )3.
The learning mechanism3.1 Translation TemplatesSupracontexts and translation templates can beviewed as two sides of the same coin.Generalization through unification on featurevalues of neigbbouring sentences, if these satisfy,certain criteria, leads to more abstract expressionsof bilingual pairs of "pseudo-sentences", consistingof sequences of constant and variable elements,517m~.+m~I(application)41- .
.
.
.
.
.
.
.
.
.
.
T .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ I ~?0appoy/l~u + ; ' b v I v+ l++ 1 +- ' -+  ) + ,  x,+, I( 1 .
.
.
.
.
.
.
.
.
.
;{  ?.+u - - I I '+ network ~.
.
.
.
.
.
.
.
.
.
.
.
I .
.
.
.
.
.
.
.
.
.Syntagmatic RelationsCustomizing I +(1):Customizing application settings <=:> Flpoaappoyq puepicr~tov ~(oaPllOg/lf, and(2): Customizing network settings ~> npo<~cxppoyq puBpicr~tov 51KrOOU is"where ?
\[ Xs l \ ]  ~ \[ Xt l \ ]  and,application ~ ~q)appoyfl(;network ~ 61KTt)OUFigure Iwhere variable elements are represented by specialsymbols ("Xi") and constant-fixed elements act asthe context in each case.3.2 Translation UnitsDiscarded features (represented by the "-" symbol)of corresponding supracontexts, rising fromvariable elements of the matching sentences,correspond to the translation units of the respectivetranslation patterns.
As a result, single or multi-word elements (translation units) of source andtarget language appearing within correspondingsupracontext positions, are linked and stored,comprising the bilingual translation unit lexicon.3.3 The Analogical NetworkThe main linguistic object for which matching isperformed is not the sentence but pairs of sourceand target sentences/exemplars.
Therefore,matching between linguistic objects is performed intwo dimensions imultaneously, that is betweensource and target sentences of matching pairsrespectively.
The result of the process, if certainconditions are met, are stored in an "analogicalnetwork" (Federici, S. & Pirrelli V., 1994) of inter-sentence and intrasentence r lations between theseexemplars and their generalizations.
A rathersimple example of this is presented ill Figure 1.Different parts of matching sentences are replacedby corresponding variables, and are consequentlyassigned the role of translation units, whilesimilar/constant parts are considered to be thecontext under which variable units are instantiated.The union of context and variables establishes the"generalized" translation (paradigmatic) patternsbetween source and target language.
The similar(constant) and different (variable) parts betweensource and target sentences are factored out andpresented as separate nodes in the above diagram.For each sentence we can view its constituentsingle or multi-word, constant or variable units asseparate nodes, where links between these nodesindicate the syntagmatic relations between them,that is, the way they actually appear and are orderedin the respective sentence.
The vertical axisrepresents the paradigmatic dimension of availablealternants, that is, the information concerningwhich substrings are in complementary distributionwith respect o the same syntagmatic context i.e.with respect o the same context '?Customizing __518settings".
Syntagmatic links constitute theintrasentence relations/links between sentenceconstituents Mille paradigmatic ones correspond tothe interscntential relations.
Furthermore, a thirddhnension is added to the whole framework, that ofthe "l'mguage", since all principles are appliedsimultaneously to both source sentences and theirtarget equivalents.
In case, linguistic annotationsare available, they are appropriately incorporated inthe respective nodes.At this point no conflicts are resolved.
All possiblepatterus are stored in the network includingconflicting as well as overlapping patterns.However, all links both paradigmatic andsyntagmatic are weighted by frequencyinformation.
Tiffs will eventually provide thenecessary informatiou to disable and even discardcertain false or useless variables or templates.3.4 The AlgorithmTranslation templates as well as translation unitsare treated as paradigmatic flexible structures thatdepend on the available evidence.
As new datacome into the system, rules can be extended or evenreplaced by other more general ones.
It is usuallyassulned that there is only one fixed way to assign astructural representation to a symbolic object eitherbe a translation unit or a translation template.14owever, it is obvious that in our approach there isno initial fixed definition of this particularstructure, rather it is left up to the training corpusand the learning mechanism.
As was expected,under this kind of analogy-based approach,linguistic objects were determined based on theparadiglnatic context hey appeared in, resulting ina more flexible and also corpus dependentdefinition of translation units.Search Space ReductionIn general, if sentence matching wereunconstrained and all resulting matches were storedin tile analogical network, then the number of alllinks (inter/intra-sentential) for N equal to thenumber of translation patterns learned through theprocess and L equal to the number of words in asentence (template) would be :while the complexity of the learning phase is alsoincreased by tile fact that each candidate rule needsto be verified against the available corpus,introducing an additional parameter S, that of thesize of tile training corpora (in number ofsentences).Moreover, if a rather straightforward approach inmatching was to be followed, the complexityinvolved for each individual candidate senteucewould be enormous.
In such an approach, for eachcandidate sentence, all corresponding subcontextswould have to be identified and verified against heavailable corpora.
For instance, a sentence of lengthL would generate 2 ~' subcontexts, thus resulting in0(2 L) required search actions against he availablecorpora.
Even if constraints would be set upon thelength of possible ignore (variable) areas, forexample = 5 words, the process would still be toocomplex.
For example for a sentence of length L =10 and for variables of length up to 5 words, thepossible subcontexts that have to be matchedagainst the corpus would be (,,,)+/:)_,_ (,;)+ (:)+ (,;)--,o+,,210 + 36 =- 421, where terms of tile previouscquation correspond to the subcontexts withvariables of length 1 to 5 respectively.The SSR methodology, depends on the specificneeds of the particular task.
Run-time pruning ofpossible matches can speed up the learning process,however it also reduces ystem recall & coverage.On the other hand, constraints on paradigmaticrelations are more reliable providing better esultsbut cannot contribute to the speed of the learningprocess.
SSR was based on an efficient indexingand retrieval mechanism (Wilhnan, N. 1994)allowing fast identification of "relevant" sentencesbased on comlnon single/multi-word units.
In thisway, the search space for each individual candidatewas significantly reduced to a smaller set ofpossible matching sentences.Distance MetricThe main objects of knowledge generated by thelearning process are the translation patterns and thebilingual lexicon of translation units.
During thelearning process, both sources are enriched whenpossible.
Sentences are analysed and encoded totwo-dimensional vectors based on the words (firstdimension) and the linguistic annotations (seconddimension) they might contain.
Then sentencevectors are compared on an equal - not equal basis519through a Levensthein or Edit distance algorithrn(Damerau, F. 1964), (Oflazer, K. 1996).
Thealgorithm, implemented through a dynamicprogramming framework (Stephen, G. 1992),computes the minimum number of required editingactions (insertions, deletions, substitutions,movements and transpositions) in order totransform one sentence into another through aninverse backtracking procedure.
The final similarityscore is computed by assigning appropriate weightsto these actions.
For the time being only insertionsand deletions were accounted for.
More complexactions, like transpositions or movements of wordsand their influence in the final translation patternwill be the focus of future work.Variable ElementsDiflbrences between matching sentences result incoupling of corresponding source and target words,as explained earlier in this section, thus enrichingthe lexicon with new information.
Coupling isrestricted to content words.
Content words canusually be replaced by other words of the samecategory acting as potential variables (Kaji, H. et al1992).
On the other hand fimctional words dopresent an "abnormal" translational behavior, sincethey sometimes act as optional units which do notappear in both source and target segments, othertimes have a one-to-one correspondence, yet it isnot rare that they affect the target pattern(especially when they participate in verbcomplementation).
"Exclusion lists" were used forthis purpose in order to reject functional wordsfrom acting as translation w~riables.WorkflowAll sentences are stored as vectors of constituentwords-annotations.
Functional words are marked assuch.
The process runs iteratively tbr all sentencesstarting l'rom sentences of length 1 to the maximumlength appearing in the training corpus.
The processterminates in case of an unsuccesslifl loop, meaningan iteration where no new information eithertranslation units or templates were extracted.
Thelearning process consisting of five subsequentphases, is depicted in detail in Figure 2 :Phasel Search Space Reduction : Extract aninitial set of possibly relevant sentences tbr thecurrent input sentence.Phase2 Sentence Matching : Match Input sentenceagainst he previous et.
Matching candidates areSearehS_pace Reduction \] ~;i(SSR)I Sentence Matchinq~ \](Edit Distance)LEARNING PHASEIdentif~AII Subcontexts 1 -~-~h Target EquivalentsResolve DifferencesIdentify Variables/Tunits_.
Enrich Bilingual LexiconIdenti_fffy_A/l_S u p raco ntextsUnify Variable Feature Values \]Extract Translation PatternsSup~x~a~ernsL Fnrinh/llndat~ Ana~nical N~.twnrkFigure.
2sorted based on distance score.
Matches with fewerdifferences are examined first.Phase3 Identification of Subcontexts :For eachnaatching candidate, identify the respectivesubcontext of the input sentence that it adheres to.Examine target language equivalents.
Resolvedifferences between source and target languagematching candidates based on already existingintbrmation contained in the bilingual exicon.During this process, the bilingual translation unitlexicon is enriched with any successfldly resolveddifference (even if the particular candidate will notfinally lead to a new translation pattern).520Phase4 Identification of Supracontcxts : Basedon tile ah'eady identified subcontextsproduce tile respective supraeontexts throughunification of respective variable feature values.Phase5 Extraction of Translation Patterns :Construct corresponding translation patterns fromexisting supraeontexts.
Update analogical network.In case a pattern ahcady exists, update the weightof its constituent links.At the end of the learning process the analogicalnetwork has been enriched with all possibletranslation patterns and variables/units extractedfi'om the available corpora.
Conflict resolution andnetwork refinement in general is performed on thefinal results, where all information is available asdescribed in the next section.3.5 Network RefinementAs mentioned earlier, tile analogical networkcontains all translation alternatives For individualtranslation units as well as all translation patternsresulting fiom the learning process, l lowever, linkweight information is also included in the aboveframework representing the validily of a particularrelation against he training corpus.Translation alternatives of individual units (inour case words) are implicitly classified throughtheir context, that is the constant part of thetranslation patterns they participate in.
These willconstitute the main selection criterion duringtranslation, l lowevcr, fi'equency inlimnation isalso used in order to disable and finally discardobsolete or erroneous translation unit alternatives.Translation templates are compared with respectto their source and target language constituentpatterns: (a) Conflicting templates, that istemplates haring only one of the two patterns aresubsequcntly checked in terms of weightinformation.
Templates of equivalent weights areconsidered equally effective.
This is usually tilecase where different translations are produced fi'omthe same source pattern due to semantic difliarenceson the variables it contains.
Conflicting templateswith significantly low weights (under a predefinedthreshold), are judged ineffective or "exceptional"(Nomiyama, It.
1992) and are flagged as such inorder to receive a special treatment during thetranslation phase (Watanabe, 1t.
1994).
These caneven be disabled or discarded fiom the networkdepending on their significance weight lhrough adynamic 'Torgetting and remembering" process(Streiter, O. et al 1999).
(b) Overlappingtemplates, where both source and target patterns ofone template can be generated from the other bycoupling words of the constant part of the templatethrough valid translation alternatives included inthe network, are identified and the more generalones are preferred.
A basic requirement is that tileset of all translation alternatives instantiating thevariables of the more general template is a supersetof those instantiating the less general one.
In anyother case, both templates are retained.
And finally,(c) complementary templates, are also identifiedand replaced by their union.4.
EvaluationThe training set consisted of a bilingual (EN-GR)technical corpus (automotive industry) of 5Ksentences, -20K wordl'orms on each language.
Theprocess resulted in ~550 translation rules, and 350translation units (~50 multi-word ones).
Theprecision estimated through manual evaluation was~75%.
More than 23% of the erroneous rules weredue to idiomatic expressions.
The rest of tile errorswas caused by imprecise translation patterns foundin the corpus.
However, these errors being ratherexceptional, received a very low weight ofeffectiveness atlhe end of the process.
No straightforward approach to measure the recall of thelearning process was devised, since it was not easyto a-priori determine the number off rules thatshould be extracted fiom tile training corpus.Howcvcr, coverage of the final translation rule setagainst the corpus was measured and found equal to38%.
More specifically, the set of 500 rules couldtlu'ough an inverse process generate 38% of tilecorpus sentences, subsequently interpreted in asignificant gain in terms of storage space.
Anotherobvious benefit is the subsentential alignmentinformation that is, the source and target ranslationunits learned at tile end of the process.5.
Conclusion & Future WorkWe have presented a self-modelling, incrementalanalogical algorithm for extracting translatioupatterns fi'om existing bilingual corpora s well as amethod for efficient storage and representation fextracted relations between various units of text.Not surprisingly, the quality of the results dependson the available information in terms of quantity as521well as quality and depth.
Lack of any kind oflinguistic information will consequently result intranslation rules based only owl "shallow" evidence.Similarly, information of low quality will generateerroneous rules.
However, this is a basicpresupposition of any EBMT system: "what yougive is what you get...".Tile proposed fi'amework was initially evaluatedbased only owl string form information.
However,tile model can easily take into account "deeper"linguistic knowledge during the learning phase,thus improving tile quality of the final results.Evaluation of learning performance in this case isthe main object of ctn'rent work.Another, interesting issue is how the currentfi'amework can constrain acceptable multi-wordvariables in order to reduce computationalcomplexity.
In present, accepting or rqiectingcandidate variables extracted from the sentencematching process, is based on a simple heuristic oflength in content words.
This type of approachwould presumably require some kind of clue onwhat could be an acceptable translation unit pattern(Juola, P. 1994), (Furuse, O., lida, 14.
1996).Finally, future work will mainly fbcus on how thesystem can invoke all existing information in orderto generate new translations, mainly aiming atautomatic and (senti-) automatic methods for"recursive" as well as "parallel" utilization ofmultiple translation rules towards optimal"coverage" of new incoming sentences.7.
References(Boutsis, S., Piperidis, S. t998) Aligning Clauses inParallel Texts.
3ld Conference on Empirical Methods inNatural Language Processing, June 1998(Collins, B., & Cunningham, P. 1995) A Methodologyth for EBMT.
4 International Conference on the CognitiveScience of Natural Language Processing, Dublin 1995.
(Cranias, L., Papageorgiou, H. and Piperidis, S. 1994).A matching technique in Example-Based MachineTranslation, Proc.
of COLING-94, pp 100-105,(Daelemans, W., Gillis, S. & Durieux, G., 1997)Skousen's analogical modelling algorithm: a comparisonwith lazy learning.
New Methods in LanguageProcessing: Edited by Daniel Jones & Harold Somers,UCL Press, p.3-15.
(Damerau, F. 1964) A Technique for Computer Detectionand Correction of Spelling Errors.
Communications of theACM 7, p. 171-176, 1964.
(Dorr, B.
1994) Machine Translation Divergences: AFormal Description and Proposed Solution.
Associationfor Computational Linguistics, Vol.
20, 1994.
(Federici, S. & Pirrelli, V. 1994).
The compilation oflarge pronunciation lexica: the elicitation of letter to soundpatterns through analogy based networks.
Papers inComputational Lexicography, Complex '94, Budapest,59-67.
(Frederking, R., Nirenburg, S., 1994)Three Heads areBetter then One.
Proceedings of the fourth Conferenceon Appfied Natural Language Processing, ANLP-94,Stuttgart, GernTany(Furuse, O., lida, H. 1996) Incremental TranslationUtilizing Constituent Boundary Patterns.
Proc.
Coling-96,pp 412-417.
(Juola, P. 1994) Self-Organizing Machine Translation:Example-Driven Induction of Transfer Functions.University of Colorado at Boulder, Technical Report CU-CS-722-94.
(Kaji, H., Kida, Y., and Morimoto, Y., 1992) LearningTranslation Templates from Bilingual TexL Proc.
Coling.,p.
672-678, 1992.
(McTait, K., Olohan, M., Trujillo, A.
1999) A BuildingBlocks Approach to Translation Memory.
Proc.
From the21 st ASLIB Conference, London, 1999.
(Nirenburg, S. Domashnev, C., Grannes, D. 1993) TwoApproaches to Matching in Example-Based MachineTranslation.
Proc.
of TMI-93, Kyoto, Japan, 1993.
(Nomiyama, H. 1992) Machine Translation by CaseGeneralization.
Proceedings of the \[sic\] InternationalConference on Computational Linguistics, COLING-92,Nantes, p.
714-720.
(Oflazer, K. t996) Error-tolerant Finite State Recognitionwith Applications to Morphological Analysis and SpellingCorrection.
Association for Computational Linguistics,Vol.
22, (1), 1996(Sadler, V., Vendelmans, R. 1990} Pilot Implementationof a Bilingual Knowledge Bank.
Proc.
of Coling, pp 449-451, 1990.
(Sato, S. 1995).
MBT2: A Method for CombiningFragments of Examples in Example-Based MachineTranslation.
Artificial Intelligence 75, 31-49.
(Skousen, R. 1989} Analogical Modelling of language.Dordrecht: Kluwe~.
(Stephen, G. 1992) String Search.
University College ofNorth Wales, Technical Report TR-92-gas-01.
(Streiter, O., Iomdin,L., Hong,M., Hauck, U., 1999)IAI CA T2 Publications, www.iai, uni-sb.de(Veale, T. and Way, A.
1997) Gaijin: A BootstrappingApproach to Example-Based Machine Translation.International Conf., Recent Advances in NaturalLanguage Processing, Tzigov Chark, Bulgaria, 239-244.
(Watanabe, H. 1994) A Method for DistinguishingExceptional or General Examples in Example-BasedTransfer Systems.
The 15 t~' International Conference onComputational LhTguistics, COLING-94, Kyoto, p.39-44.
(Willman, N. 1994) A Prototype Information RetrievalSystem to Perform a Best-Match Search for Names.Conference Proceeding of RIA 0 '94.522
