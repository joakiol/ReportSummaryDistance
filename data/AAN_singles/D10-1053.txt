Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 545?554,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsHierarchical Phrase-based Translation Grammars Extracted fromAlignment Posterior ProbabilitiesAdria` de Gispert, Juan Pino, William ByrneMachine Intelligence LaboratoryDepartment of Engineering, University of CambridgeTrumpington Street, CB2 1PZ, U.K.{ad465|jmp84|wjb31}@eng.cam.ac.ukAbstractWe report on investigations into hierarchi-cal phrase-based translation grammars basedon rules extracted from posterior distributionsover alignments of the parallel text.
Ratherthan restrict rule extraction to a single align-ment, such as Viterbi, we instead extract rulesbased on posterior distributions provided bythe HMM word-to-word alignment model.
Wedefine translation grammars progressively byadding classes of rules to a basic phrase-basedsystem.
We assess these grammars in termsof their expressive power, measured by theirability to align the parallel text from whichtheir rules are extracted, and the quality of thetranslations they yield.
In Chinese-to-Englishtranslation, we find that rule extraction fromposteriors gives translation improvements.
Wealso find that grammars with rules with onlyone nonterminal, when extracted from posteri-ors, can outperform more complex grammarsextracted from Viterbi alignments.
Finally, weshow that the best way to exploit source-to-target and target-to-source alignment modelsis to build two separate systems and combinetheir output translation lattices.1 IntroductionCurrent practice in hierarchical phrase-based trans-lation extracts regular phrases and hierarchical rulesfrom word-aligned parallel text.
Alignment modelsestimated over the parallel text are used to generatethese alignments, but these models are then typicallyused no further in rule extraction.
This is less thanideal because these alignment models, even if theyare not suitable for direct use in translation, can stillprovide a great deal of useful information beyond asingle best estimate of the alignment of the paralleltext.
Our aim is to use alignment models to generatethe statistics needed to build translation grammars.The challenge in doing so is to extend the currentprocedures, which are geared towards the use of asingle alignment, to make more of what can be pro-vided by alignment models.
The goal is to extract aricher and more robust set of translation rules.There are two aspects to hierarchical phrase-basedtranslation grammars which concern us.
The firstis expressive power, which we take as the abilityto generate known reference translations from sen-tences in the source language.
This is determinedby the degree of phrase movements and the trans-lations allowed by the rules of the grammar.
For agrammar with given types of rules, larger rule setswill yield greater expressive power.
This motivatesstudies of grammars based on the rules which are ex-tracted and the movement the grammar allows.
Thesecond aspect is of course translation accuracy.
Ifthe expressive power is adequate, then the desire isthat the grammar assigns a high score to a correcttranslation.We use posterior probabilities over parallel data toaddress both of these aspects.
These posteriors allowus to build larger rule sets with improved transla-tion accuracy.
Ideally, for a sentence pair we wish toconsider all possible alignments between all possi-ble source and target phrases within these sentences.Given a grammar allowing certain types of move-ment, we would then extract all possible parses thatare consistent with any alignments of these phrases.545To make this approach feasible, we consider onlyphrase-to-phrase alignments with a high posteriorprobability under the alignment models.
In this way,the alignment model probabilities guide rule extrac-tion.The paper is organized as follows.
Section 2 re-views related work on using posteriors to extractphrases, as well as other approaches that tightly in-tegrate word alignment and rule extraction.
Sec-tion 3 describes rule extraction based on word andphrase posterior distributions provided by the HMMword-to-word alignment model.
In Section 4 we de-fine translation grammars progressively by addingclasses of rules to a basic phrase-based system, mo-tivating each rule type by the phrase movement it isintended to achieve.
In Section 5 we assess thesegrammars in terms of their expressive power and thequality of the translations they yield in Chinese-to-English, showing that rule extraction from posteriorsgives translation improvements.
We also find thatthe best way to exploit source-to-target and target-to-source alignment models is to build two sepa-rate systems and combine their output translationlattices.
Section 6 presents the main conclusions ofthis work.2 Related WorkSome authors have previously addressed the limita-tion caused by decoupling word alignment modelsfrom grammar extraction.
For instance Venugopalet al (2008) extract rules from n-best lists of align-ments for a syntax-augmented hierarchical system.Alignment n-best lists are also used in Liu et al(2009) to create a structure called weighted align-ment matrices that approximates word-to-word linkposterior probabilities, from which phrases are ex-tracted for a phrase-based system.
Alignment pos-teriors have been used before for extracting phrasesin non-hierarchical phrase-based translation (Venu-gopal et al, 2003; Kumar et al, 2007; Deng andByrne, 2008).In order to simplify hierarchical phrase-basedgrammars and make translation feasible with rela-tively large parallel corpora, some authors discussthe need for various filters during rule extraction(Chiang, 2007).
In particular Lopez (2008) enforcesa minimum span of two words per nonterminal,Zollmann et al (2008) use a minimum count thresh-old for all rules, and Iglesias et al (2009) proposea finer-grained filtering strategy based on rule pat-terns.
Other approaches include insisting that target-side rules are well-formed dependency trees (Shen etal., 2008).We also note approaches to tighter coupling be-tween translation grammars and alignments.
Marcuand Wong (2002) describe a joint-probabilityphrase-based model for alignment, but the approachis limited due to excessive complexity as Viterbiinference becomes NP-hard (DeNero and Klein,2008).
More recently, Saers et al (2009) reportimprovement on a phrase-based system where wordalignment has been trained with an inversion trans-duction grammar (ITG) rather than IBM models.Pauls et al (2010) also use an ITG to directly alignphrases to nodes in a string-to-tree model.
Bayesianmethods have been recently developed to induce agrammar directly from an unaligned parallel corpus(Blunsom et al, 2008; Blunsom et al, 2009).
Fi-nally, Cmejrek et al (2009) extract rules directlyfrom bilingual chart parses of the parallel corpuswithout using word alignments.
We take a differ-ent approach in that we aim to start with very strongword alignment models and use them to guide gram-mar extraction.3 Rule Extraction from AlignmentPosteriorsThe goal of rule extraction is to generate a set ofgood-quality translation rules from a parallel cor-pus.
Rules are of the form X???,?,??
, where?, ?
?
{X ?
T}+ are the source and target sides ofthe rule, T denotes the set of terminals (words) and?
is a bijective function1 relating source and targetnonterminals X of each rule (Chiang, 2007).
Foreach ?, the probability over translations ?
is set byrelative frequency over the extracted examples fromthe corpus.We take a general approach to rule extraction, asdescribed by the following procedure.
For simplic-ity we discuss the extraction of regular phrases, thatis, rules of the form X?
?w,w?, where w ?
{T}+.Section 3.3 extends this procedure to rules with non-1This function is defined if there are at least two nontermi-nals, and for clarity of presentation will be omitted in this paper546terminal symbols.Given a sentence pair (fJ1 , eI1), the extraction al-gorithm traverses the source sentence and, for eachsequence of terminals f j2j1 , it considers all possibletarget-side sequences ei2i1 as translation candidates.Each target-side sequence that satisfies the align-ment constraints CA is ranked by the function fR.For practical reasons, a set of selection criteria CS isthen applied to these ranked candidates and definesthe set of translations of the source sequence that areextracted as rules.
Each extracted rule is assigned acount fC .In this section we will explore variations of thisrule extraction procedure involving alternative def-initions of the ranking and counting functions, fRand fC , based on probabilities over alignment mod-els.Common practice (Koehn et al, 2003) takes a setof word alignment links L and defines the alignmentconstraints CA so that there is a consistency betweenthe links in the (f j2j1 , ei2i1) phrase pair.
This is ex-pressed by ?
(j, i) ?
L : (j ?
[j1, j2]?
i ?
[i1, i2])?
(j 6?
[j1, j2] ?
i 6?
[i1, i2]).
If these constraintsare met, then alignment probabilities are ignored andfR = fC = 1.
We call this extraction Viterbi-based,as the set of alignment links is generally obtainedafter applying a symmetrization heuristic to source-to-target and target-to-source Viterbi alignments.In the following section we depart from this ap-proach and apply novel functions to rank and counttarget-side translations according to their quality inthe context of each parallel sentence, as defined bythe word alignment models.
We also depart fromcommon practice in that we do not use a set of linksas alignment constraints.
We thus find an increasein the number of extracted rules, and consequentlybetter relative frequency estimates over translations.3.1 Ranking and Counting FunctionsWe describe two alternative approaches to modifythe functions fR and fC so that they incorporate theprobabilities provided by the alignment models.3.1.1 Word-to-word Alignment PosteriorProbabilitiesWord-to-word alignment posterior probabilitiesp(lji|fJ1 , eI1) express how likely it is that the wordsin source position j and target position i are alignedgiven a sentence pair.
These posteriors can be effi-ciently computed for Model 1, Model 2 and HMM,as described in (Brown et al, 1993; Venugopal et al,2003; Deng and Byrne, 2008).We will use these posteriors in functions toscore phrase pairs.
For a simple non-disjoint case(f j2j1 , ei2i1) we use:fR(f j2j1 , ei2i1) =j2?j=j1i2?i=i1p(lji|fJ1 , eI1)i2 ?
i1 + 1(1)which is very similar to the score used for lexicalfeatures in many systems (Koehn, 2010), with thelink posteriors for the sentence pair playing the roleof the Model 1 translation table.For a particular source phrase, Equation 1 is nota proper conditional probability distribution over allphrases in the target sentence.
Therefore it cannot beused as such without further normalization.
Indeedwe find that this distribution is too sharp and over-emphasises short phrases, so we use fC = 1.
How-ever, it does allow us to rank target phrases as pos-sible translations.
In contrast to the common extrac-tion procedure described in the previous section, theranking approach described here can lead to a muchmore exhaustive extraction unless selection criteriaare applied.
These we describe in Section 3.2.We note that Equation 1 can be computed us-ing link posteriors provided by alignment modelstrained on either source-to-target or target-to-sourcetranslation directions.3.1.2 Phrase-to-phrase Alignment PosteriorProbabilitiesRather than limit ourselves to word-to-wordlink posteriors we can define alignment proba-bility distributions over phrase alignments.
Wedo this by defining the set of alignments A asA(j1, j2; i1, i2) = {aJ1 : aj ?
[i1, i2] iff j ?
[j1, j2]}, where aj is the random process that de-scribes word-to-word alignments.
These are thealignments from which the phrase pair (f j2j1 , ei2i1)would be extracted.The posterior probability of these alignmentsgiven the sentence pair is defined as follows:p(A|eI1, fJ1 ) =?aJ1?Ap(fJ1 , aJ1 |eI1)?aJ1p(fJ1 , aJ1 |eI1)(2)547G0 G1 G2 G3S??X,X?
X?
?w X,X w?
X?
?w X,X w?
X?
?w X,X w?S?
?S X,S X?
X?
?X w,w X?
X?
?X w,w X?
X?
?X w,w X?X??w,w?
X?
?w X,w X?
X?
?w X,w X?X?
?w X w,w X w?Table 1: Hierarchical phrase-based grammars containing different types of rules.
The grammar expressivity is greateras more types of rules are included.
In addition to the rules shown in the respective columns, G1, G2 and G3 alsocontain the rules of G0.With IBM models 1 and 2, the numerator and de-nominator in Equation 2 can be computed in termsof posterior link probabilities (Deng, 2005).
Withthe HMM model, the denominator is computed us-ing the forward algorithm while the numerator canbe computed using a modified forward algorithm(Deng, 2005).These phrase posteriors directly define a proba-bility distribution over the alignments of translationcandidates, so we use them both for ranking andscoring extracted rules, that is fR = fC = p. Thisapproach assigns a fractional count to each extractedrule, which allows finer estimation of the forwardand backward translation probability distributions.3.2 Alignment Constraints and SelectionCriteriaIn order to keep this process computationallytractable, some extraction constraints are needed.
Inorder to extract a phrase pair (f j2j1 , ei2i1), we definethe following:?
CA requires at least one pair of positions (j, i) :(j ?
[j1, j2] ?
i ?
[i1, i2]) with word-to-wordlink posterior probability p(lji|fJ1 , eI1) > 0.5,and that there is no pair of positions (j, i) : (j ?
[j1, j2]?i 6?
[i1, i2])?
(j 6?
[j1, j2]?i ?
[i1, i2])with p(lji|fJ1 , eI1) > 0.5?
CS allows only the k best translation candidatesto be extracted.
We use k = 3 for regularphrases, and k = 2 for hierarchical rules.Note that we do not discard rules according totheir scores fC at this point (unlike Liu et al(2009)), since we prefer to add all phrases fromall sentence pairs before carrying out such filteringsteps.Once all rules over the entire collection of paral-lel sentences have been extracted, we require eachrule to occur at least nobs times and with a forwardtranslation probability p(?|?)
> 0.01 to be used fortranslation.3.3 Extraction of Rules with NonterminalsExtending the procedure previously described tothe case of more complex hierarchical rules includ-ing one or even two nonterminals is conceptuallystraightforward.
It merely requires that we traversethe source and target sentences and consider possi-bly disjoint phrase pairs.
Optionally, the alignmentconstraints can also be extended to apply on the non-terminal X.Equation 1 is then only modified in the limitsof the product and summation, whereas Equation2 remains unchanged, as long as the set of validalignments A is redefined.
For example, for a ruleof the form X?
?w X w,w X w?, we use A ?A(j1, j2; j3, j4; i1, i2; i3, i4).4 Hierarchical Translation GrammarDefinitionIn this section we define the hierarchical phrase-based synchronous grammars we use for translationexperiments.
Each grammar is defined by the type ofhierarchical rules it contains.
The rule type can beobtained by replacing every sequence of terminalsby a single symbol ?w?, thus ignoring the identity ofthe words, but capturing its generalized structure andthe kind of reordering it encodes (this was defined asrule pattern in Iglesias et al (2009)).A monotonic phrase-based translation grammarG0 can be defined as shown in the left-most col-umn of Table 1; it includes all regular phrases, repre-sented by the rule type X?
?w,w?, and the two glue548(G0) R1: S??X,X?
(G0) R2: X?
?s2 s3,t2?
(G1) R3: X?
?s1 X,X t3?
(G1) R4: X?
?X s4,t1 X?
(G2) R5: X?
?s1 X,t7 X?
(G3) R6: X?
?s1 X s4,t5 X t6?Figure 1: Example of a hierarchical translation grammar and two parsing trees following alternative rule derivationsfor the input sentence s1s2s3s4.rules that allow concatenation.
Our approach is nowsimple: we extend this grammar by successively in-corporating sets of hierarchical rules.
The goal is toobtain a grammar with few rule types but which iscapable of generating a rich set of translation candi-dates for a given input sentence.With this in mind, we define the following threegrammars, also summarized in Table 1:?
G1 := G0?
{ X?
?w X,X w?
, X?
?X w,w X?
}.
Thisincorporates reordering capabilities with tworule types that place the unique nonterminalin an opposite position in each language; wecall these ?phrase swap rules?.
Since all non-terminals are of the same category X, nestedreordering is possible.
However, this needs tohappen consecutively, i.e.
a swap must applyafter a swap, or the rule is concatenated withthe glue rule.?
G2 := G1?
{ X?
?w X,w X?
}.
Thisadds monotonic concatenation capabilities tothe previous translation grammar.
The glue rulealready allows rule concatenation.
However, itdoes so at the S category, that is, it concate-nates phrases and rules after they have been re-ordered, in order to complete a sentence.
Withthis new rule type, G2 allows phrase/rule con-catenation before reordering with another hier-archical rule.
Therefore, nested reordering doesnot require successive swaps anymore.?
G3 := G2?
{ X?
?w X w,w X w?
}.
Thisadds single nonterminal rules with disjoint ter-minal sequences, which can encode a mono-tonic or reordered relationship between them,depending on what their alignment was in theparallel corpus.
Although one could expect themovement captured by this phrase-disjoint ruletype to be also present in G2 (via two swaps orone concatenation plus one swap), the terminalsequences w may differ.Figure 1 shows an example set of rules indicat-ing to which of the previous grammars each rule be-longs, and shows three translation candidates as gen-erated by grammars G1 (left-most tree), G2 (mid-dle tree) and G3 (right-most tree).
Note that themiddle tree cannot be generated with G1 as it re-quires monotonic concatenation before reorderingwith rule R4.The more rule types a hierarchical grammar con-tains, the more different rule derivations and thegreater the search space of alternative translationcandidates.
This is also connected to how manyrules are extracted per rule type.
Ideally we wouldlike the grammar to be able to generate the correcttranslation of a given input sentence, without over-generating too many other candidates, as that makesthe translation task more difficult.We will make use of the parallel data in measuringthe ability of a grammar to generate correct transla-tions.
By extracting rules from a parallel sentence,we translate them and observe whether the transla-tion grammar is able to produce the parallel targettranslation.
In Section 5.1 we evaluate this for aChinese-to-English task.5494.1 Reducing Grammar RedundancyLet us discuss grammar G2 in more detail.
As de-scribed in the previous section, the motivation for in-cluding rule type X?
?w X,w X?
is that the gram-mar be able to carry out monotonic concatenationbefore applying another hierarchical rule with re-ordering.
This movement is permitted by this ruletype, but the use of a single nonterminal category Xalso allows the grammar to apply the concatenationafter reordering, that is, immediately before the gluerule is applied.
This creates significant redundancyin rule derivations, as this rule type is allowed to actas a glue rule.
For example, given an input sentences1s2 and the following simple grammar:R0: S?
?X,X?R1: S?
?S X,S X?R2: X?
?s1,t1?R3: X?
?s2,t2?R4: X?
?s1 X,t1 X?two derivations are possible: R2,R0,R3,R1 andR3,R4,R0, and the translation result is identical.To avoid this situation we introduce a nonterminalM in the left-hand side of monotonic concatenationrules of G2.
All rules are allowed to use nontermi-nals X and M in their right-hand side, except theglue rules, which can only take X.
In the context ofour example, R4 is substituted by:R4a: M?
?s1 X,t1 X?R4b: M?
?s1 M ,t1 M?so that only the first derivation is possible:R2,R0,R3,R1, because applying R3,R4a yields a non-terminal M that cannot be taken by the glue rule R0.5 ExperimentsWe report experiments in Chinese-to-English trans-lation.
Our system is trained on a subset of theGALE 2008 evaluation parallel text;2 this is approx-imately 50M words per language.
We report trans-lation results on a development set tune-nw and atest set test-nw1.
These contain translations pro-duced by the GALE program and portions of thenewswire sections of MT02 through MT06.
Theycontain 1,755 sentences and 1,671 sentences respec-tively.
Results are also reported on a smaller held-2See http://projects.ldc.upenn.edu/gale/data/catalog.html.We excluded the UN material and the LDC2002E18,LDC2004T08, LDC2007E08 and CUDonga collections.304050607080V-stV-tsV-unionV-gdfV-mergeWP-stWP-tsWP-mergeG0G1G2G3Figure 2: Percentage of parallel sentences successfullyaligned for various extraction methods and grammars.out test set test-nw2, containing 60% of the NISTnewswire portion of MT06, that is, 369 sentences.The parallel texts for both language pairs arealigned using MTTK (Deng and Byrne, 2008).
Fordecoding we use HiFST, a lattice-based decoder im-plemented with Weighted Finite State Transducers(de Gispert et al, 2010).
Likelihood-based searchpruning is applied if the number of states in thelattice associated with each CYK grid cell exceeds10,000, otherwise the entire search space is ex-plored.
The language model is a 4-gram languagemodel estimated over the English side of the paral-lel text and the AFP and Xinhua portions of the En-glish Gigaword Fourth Edition (LDC2009T13), in-terpolated with a zero-cutoff stupid-backoff (Brantset al, 2007) 5-gram estimated using 6.6B words ofEnglish newswire text.
In tuning the systems, stan-dard MERT (Och, 2003) iterative parameter estima-tion under IBM BLEU3 is performed on the devel-opment sets.5.1 Measuring Expressive PowerWe measure the expressive power of the grammarsdescribed in the previous section by running thetranslation system in alignment mode (de Gispertet al, 2010) over the parallel corpus.
Conceptually,this is equivalent to replacing the language model bythe target sentence and seeing if the system is able tofind any candidate.
Here the weights assigned to the3See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl550Grammar Extraction # Rules tune-nw test-nw1 test-nw2time prune BLEU BLEU BLEUGH V-union 979149 3.7 0.3 35.1 35.6 37.6V-union 613962 0.4 0.0 33.6 34.6 36.4G1 WP-st 920183 0.9 0.0 34.3 34.8 37.5PP-st 893542 1.4 0.0 34.4 35.1 37.7V-union 734994 1.0 0.0 34.5 35.4 37.2G2 WP-st 1132386 5.8 0.5 35.1 36.0 37.7PP-st 1238235 7.8 0.7 35.5 36.4 38.2V-union 966828 1.2 0.0 34.9 35.3 37.0G3 WP-st 2680712 8.3 1.1 35.1 36.2 37.9PP-st 5002168 10.7 2.6 35.5 36.4 38.5Table 2: Chinese-to-English translation results with alternative grammars and extraction methods (lower-cased BLEUshown).
Time (secs/word) and prune (times/word) measurements done on tune-nw set.rules are irrelevant, as only the ability of the gram-mar to create a desired hypothesis is important.We compare the percentage of target sentencesthat can be successfully produced by grammars G0,G1, G2 and G3 for the following extraction meth-ods:?
Viterbi (V).
This is the standard extractionmethod based on a set of alignment links.
Wedistinguish four cases, depending on the modelused to obtain the set of links: source-to-target (V-st), target-to-source (V-ts), and twocommon symmetrization strategies: union (V-union) and grow-diag-final (V-gdf), describedin (Koehn et al, 2003).?
Word Posteriors (WP).
The extraction methodis based on word alignment posteriors de-scribed in Section 3.1.1.
These rules can be ob-tained either from the posteriors of the source-to-target (WP-st) or the target-to-source (WP-ts) alignment models.
We apply the alignmentconstraints and selection criteria described inSection 3.2.
We do not report alignment per-centages when using phrase posteriors (as de-scribed in Section 3.1.2) as they are roughlyidentical to the WP case.?
Finally, in both cases, we also report resultswhen merging the extracted rules in both direc-tions into a single rule set (V-merge and WP-merge).Figure 2 shows the results obtained for a randomselection of 10,000 parallel corpus sentences.
As ex-pected, we can see that for any extraction method,the percentage of aligned sentences increases whenswitching from G0 to G1, G2 and G3.
Posterior-based extraction is shown to outperform standardmethods based on a Viterbi set of alignments fornearly all grammars.
The highest alignment percent-ages are obtained when merging rules obtained un-der models trained in each direction (WP-merge),approximately reaching 80% for grammar G3.The maximum rule span in alignment was al-lowed to be 15 words, so as to be similar to transla-tion, where the maximum rule span is 10 words.
Re-laxing this in alignment to 30 words yields approxi-mately 90% coverage for WP-merge under G3.We note that if alignment constraints CA and se-lection criteria CS were not applied, that is k = ?,then alignment percentages would be 100% evenfor G0, but the extracted grammar would includemany noisy rules with poor generalization powerand would suffer from overgeneration.5.2 Translation ResultsIn this section we investigate the translation perfor-mance of each hierarchical grammar, as defined byrules obtained from three rule extraction methods:?
Viterbi union (V-union).
Standard rule extrac-tion from the union of the source-to-target andtarget-to-source alignment link sets.551?
Word Posteriors (WP-st).
Extraction basedon word posteriors as described in Section3.1.1.
The posteriors are provided by thesource-to-target algnment model.
Alignmentconstraints and selection criteria of Section 3.2are applied, with nobs = 2.?
Phrase Posteriors (PP-st).
Extraction basedon phrase alignment posteriors, as describedin Section 3.1.2, with fractional counts pro-portional to the phrase probability under thesource-to-target algnment model.
Alignmentconstraints and selection criteria of Section 3.2are applied, with nobs = 0.2.Table 2 reports the translation results, as well asthe number of extracted rules in each case.
It alsoshows the following decoding statistics as measuredon the tune-nw set: decoding time in seconds per in-put word, and number of instances of search pruning(described in Section 5) per input word.As a contrast, we extract rules according to theheuristics introduced in (Chiang, 2007) and applythe filters described in (Iglesias et al, 2009) to gen-erate a standard hierarchical phrase-based grammarGH .
This uses rules with up to two nonadjacent non-terminals, but excludes identical rule types such asX?
?w X,w X?
or X?
?w X1 w X2,w X1 w X2?,which were reported to cause computational difficul-ties without a clear improvement in translation (Igle-sias et al, 2009).Grammar expressivity.
As expected, for the stan-dard extraction method (see rows entitled V-union),grammar G1 is shown to underperform all othergrammars due to its structural limitations.
On theother hand, grammar G2 obtains much better scores,nearly generating the same translation quality asthe baseline grammar GH .
Finally, G3 does notprove able to outperform G2, which suggests thatthe phrase-disjoint rules with one nonterminal areredundant for the translation grammar.Rule extraction method.
For all grammars, wefind that the proposed extraction methods based onalignment posteriors outperform standard Viterbi-based extraction, with improvements ranging from0.5 to 1.1 BLEU points for test-nw1 (depending onthe grammar) and from 1.0 to 1.5 for test-nw2.
Inall cases, the use of phrase posteriors PP is the bestoption.
Interestingly, we find that G2 extracted withWP and PP methods outperforms the more complexGH grammar as obtained from Viterbi alignments.Rule set statistics.
For grammar G2 evaluatedon the tune-nw set, standard Viterbi-based extrac-tion produces 0.7M rules, whereas the WP and PPextraction methods yield 1.1M and 1.2M rules re-spectively.
We further analyse the sets of rulesX???,?,??
in terms of the number of distinctsource and target sequences ?
and ?
which are ex-tracted.
Viterbi extraction yields 82k distinct sourcesequences whereas the WP and PP methods yield116k and 146k sequences, respectively.
In termsof the average number of target sequences for eachsource sequence, Viterbi extraction yields an aver-age of 8.7 while WP and PP yield 9.7 and 8.4 ruleson average.
This shows that method PP yields widercoverage but with sharper forward rule translationprobability distributions than method WP, as the av-erage number of translations per rule is determinedby the p(?|?)
> 0.01 threshold mentioned in Sec-tion 3.2.Decoding time and pruning in search.
In connec-tion to the previous comments, we find an increasedneed for search pruning, and subsequently slowerdecoding speed, as the search space grows largerwith methods WP and PP.
A larger search space iscreated by the larger rule sets, which allows the sys-tem to generate new hypotheses of better quality.5.3 Rule Concatenation in Grammar G2In Section 4.1 we described a strategy to reducegrammar redundancy by introducing an additionalnonterminal M for monotonic concatenation rules.We find that without this distinction among nonter-minals, search pruning and decoding time are in-creased by a factor of 1.5, and there is a slight degra-dation in BLEU (?0.2) as more search errors are in-troduced.Another relevant aspect of this grammar is the ac-tual rule type selected for monotonic concatenation.We described using type X?
?w X,w X?
(con-catenation on the right), but one could also includeX?
?X w,X w?
(concatenation on the left), or both,for the same purpose.
We evaluated the three alter-natives and found that scores are identical when ei-ther including right or left concatenation types, butincluding both is harmful for performance, as theneed to prune and decoding time increase by a fac-552tor of 5 and 4, respectively, and we observe again aslight degradation in performance.Rule Extraction tune-nw test-nw1 test-nw2V-st 34.7 35.6 37.5V-ts 34.0 34.8 36.6V-union 34.5 35.4 37.2V-gdf 34.4 35.3 37.1WP-st 35.1 36.0 37.7WP-ts 34.5 35.0 37.0PP-st 35.5 36.4 38.2PP-ts 34.8 35.3 37.2PP-merge 35.5 36.4 38.4PP-merge-MERT 35.5 36.4 38.3LMBR(V-st) 35.0 35.8 38.4LMBR(V-st,V-ts) 35.5 36.3 38.9LMBR(PP-st) 36.1 36.8 38.8LMBR(PP-st,PP-ts) 36.4 36.9 39.3Table 3: Translation results under grammar G2 with indi-vidual rule sets, merged rule sets, and rescoring and sys-tem combination with lattice-based MBR (lower-casedBLEU shown)5.4 Symmetrizing Alignments of Parallel TextIn this section we investigate extraction from align-ments (and posterior distributions) over parallel textwhich are generated using alignment models trainedin the source-to-target (st) and target-to-source (ts)directions.
Our motivation is that symmetrizationstrategies have been reported to be beneficial forViterbi extraction methods (Och and Ney, 2003;Koehn et al, 2003).Results are shown in Table 3 for grammar G2.
Wefind that rules extracted under the source-to-targetalignment models (V-st, WP-st and PP-st) consis-tently perform better than the V-ts, WP-ts and PP-ts cases.
Also, for Viterbi extraction we find that thesource-to-target V-st case performs better than anyof the symmetrization strategies, which contradictsprevious findings for non-hierarchical phrase-basedsystems(Koehn et al, 2003).We use the PP rule extraction method to extracttwo sets of rules, under the st and ts alignment mod-els respectively.
We now investigate two ways ofmerging these sets into a single grammar for trans-lation.
The first strategy is PP-merge and mergesboth rule sets by assigning to each rule the maximumcount assigned by either alignment model.
We thenextend the previous strategy by adding three binaryfeature functions to the system, indicating whetherthe rule was extracted under the ?st?
model, the ?ts?model or both.
The motivation is that MERT canweight rules differently according to the alignmentmodel they were extracted from.
However, we donot find any improvement with either strategy.Finally, we use linearised lattice minimum Bayes-risk decoding (Tromble et al, 2008; Blackwood etal., 2010) to combine translation lattices (de Gis-pert et al, 2010) as produced by rules extractedunder each alignment direction (see rows namedLMBR(V-st,V-ts) and LMBR(PP-st,PP-ts)).
Gainsare consistent when comparing this to applyingLMBR to each of the best individual systems (rowsnamed LMBR(V-st) and LMBR(PP-st)).
Overall,the best-performing strategy is to extract two sets oftranslation rules under the phrase pair posteriors ineach translation direction, and then to perform trans-lation twice and merge the results.6 ConclusionRule extraction based on alignment posterior proba-bilities can generate larger rule sets.
This results ingrammars with more expressive power, as measuredby the ability to align parallel sentences.
Assign-ing counts equal to phrase posteriors produces bet-ter estimation of rule translation probabilities.
Thisresults in improved translation scores as the searchspace grows.This more exhaustive rule extraction method per-mits a grammar simplification, as expressed by thephrase movement allowed by its rules.
In particulara simple grammar with rules of only one nontermi-nal is shown to outperform a more complex gram-mar built on rules extracted from Viterbi alignments.Finally, we find that the best way to exploit align-ment models trained in each translation direction isto extract two rule sets based on alignment posteri-ors, translate the input independently with each ruleset and combine translation output lattices.AcknowledgmentsThis work was supported in part by the GALE pro-gram of the Defense Advanced Research Projects553Agency, Contract No.
HR0011- 06-C-0022.ReferencesGraeme Blackwood, Adria` de Gispert, and WilliamByrne.
2010.
Efficient Path Counting Transducersfor Minimum Bayes-Risk Decoding of Statistical Ma-chine Translation Lattices.
In Proceedings of ACL,Short Papers, pages 27?32.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.Bayesian Synchronous Grammar Induction.
In Ad-vances in Neural Information Processing Systems, vol-ume 21, pages 161?168.Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A gibbs sampler for phrasal synchronousgrammar induction.
In Proceedings of the ACL, pages782?790.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In Proceedings of EMNLP-ACL,pages 858?867.P.
Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.1993.
The mathematics of statistical machine trans-lation: Parameter estimation.
Computational Linguis-tics, 19(2):263?311.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Martin Cmejrek, Bowen Zhou, and Bing Xiang.
2009.Enriching SCFG Rules Directly From Efficient Bilin-gual Chart Parsing.
In Proceedings of IWSLT, pages136?143.Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,Eduardo R. Banga, and William Byrne.
2010.
Hier-archical phrase-based translation with weighted finitestate transducers and shallow-n grammars.
Computa-tional Linguistics, 36(3).John DeNero and Dan Klein.
2008.
The complexity ofphrase alignment problems.
In Proceedings of ACL-HLT, Short Papers, pages 25?28.Yonggang Deng and William Byrne.
2008.
HMM wordand phrase alignment for statistical machine transla-tion.
IEEE Transactions on Audio, Speech, and Lan-guage Processing, 16(3):494?507.Yonggang Deng.
2005.
Bitext Alignment for Statisti-cal Machine Translation.
Ph.D. thesis, Johns HopkinsUniversity.Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,and William Byrne.
2009.
Rule filtering by patternfor efficient hierarchical translation.
In Proceedings ofthe EACL, pages 380?388.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofHLT/NAACL, pages 48?54.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press.Shankar Kumar, Franz J. Och, and Wolfgang Macherey.2007.
Improving word alignment with bridge lan-guages.
In Proceedings of EMNLP-CoNLL, pages 42?50.Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu.
2009.Weighted alignment matrices for statistical machinetranslation.
In Proceedings of EMNLP, pages 1017?1026.Adam Lopez.
2008.
Tera-scale translation models viapattern matching.
In Proceedings of COLING, pages505?512.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In Proceedings of EMNLP, pages 133?139.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Franz J. Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proceedings of ACL,pages 160?167.Adam Pauls, Dan Klein, David Chiang, and KevinKnight.
2010.
Unsupervised syntactic alignment withinversion transduction grammars.
In Proceedings ofthe HLT-NAACL, pages 118?126.Markus Saers and Dekai Wu.
2009.
Improving phrase-based translation via word alignments from stochasticinversion transduction grammars.
In Proceedings ofthe HLT-NAACL Workshop on Syntax and Structure inStatistical Translation, pages 28?36.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL-HLT, pages 577?585.Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-gang Macherey.
2008.
Lattice Minimum Bayes-Riskdecoding for statistical machine translation.
In Pro-ceedings of EMNLP, pages 620?629.Ashish Venugopal, Stephan Vogel, and Alex Waibel.2003.
Effective phrase translation extraction fromalignment models.
In Proceedings of ACL, pages 319?326.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2008.
Wider pipelines: N-bestalignments and parses in mt training.
In Proceedingsof AMTA, pages 192?201.Andreas Zollmann, Ashish Venugopal, Franz J. Och, andJay Ponte.
2008.
A systematic comparison of phrase-based, hierarchical and syntax-augmented statisticalMT.
In Proceedings of COLING, pages 1145?1152.554
