SPEECH COMPARISON IN The Rosetta Stone rMJohn FAIRFIELDDepartment ofComputer ScienceJames Madison UniversityHarrisonburg, VA, 22807fairfijr@jmu.eduAbstractThe Rosetta Stone TM is a successful CD-ROM based interactive program for teachingforeign languages, that uses speechcomparison to help students improve theirpronunciation.
The input to a speechcomparison system is N+I digitisedutterances.
The output is a measure of thesimilarity of the last utterance to each of theN others.
Which language is being spokenis irrelevant.
This differs from classicalspeech recognition where the input dataincludes but one utterance, a set ofexpectations tuned to the particular languagein use (typically digraphs or similar), and agrammar of expected words or phrases, andthe output is recognition in the utterance ofone of the phrases in the grammar (orrejection).
This paper describes a speechcomparison system and its application inThe Rosetta Stone TM.IntroductionFunding for this research came from thedevelopers 1, of The Rosetta Stone TM (TRS), ahighly successful interactive multimediaprogram for teaching foreign languages.
Thedevelopers wanted to use speech recognitiontechnology to help students of foreign languagesimprove their pronunciation and their activevocabulary.
As of this writing TRS is availablein twenty languages, which was part of themotivation to develop a language independentapproach to speech recognition.
Classicalapproaches require extensive development perlanguage.1 FLT, 165 South Main St., Harrisonburg, VA 22801.540-432-6166 www.trstone.comTRS provides an immersion experience, whereimages, movies and sounds are used to buildknowledge of a language from scratch.
Sincethere is no concession to the native language ofthe learner, a German speaker and a Koreanspeaker both learning Vietnamese have the sameexperience--all in Vietnamese.The most recent release of TRS includes EAR,the speech comparison system described in thispaper.
The input to a speech comparison systemis N+I digitized utterances--in the case of TRS,that includes N utterances by native speakersrecorded in a studio with quality microphones,and one utterance by a student recorded in asometimes very noisy environment with a built-in or handheld microphone.
The output is ameasure of the similarity of the last utterance toeach of the N others.
Which language is beingspoken is irrelevant.Speech comparison differs from classical speechrecognition, where the input data includes oneutterance, a set of expectations tuned to theparticular language in use (typically digraphs orsimilar), and a grammar of expected words orphrases, and the output is recognition of theutterance as one of the phrases in the grammar,or rejection.The TRS CD-ROM contains tens of thousandsof utterances by native speakers.
Thus the TRSdata set aleady included the necessary input forspeech comparison, but not for classical speechrecognition.
The first application we developedwas a pronunciation guide (see Fig.
1).
The userclicks on a picture, hears a native speaker'sutterance, attempts to mimic that utterance, seesa display of two images visually portraying thetwo utterances, and observes a gauge whichshows a measure of the similarity between thetwo utterances.
The system normalizes bothvoices (native speaker's and student's) to a12Fig.
1.
TRS pronunciation evaluationClicking on an image brings up the speechcomparison panel, seen here imposed overthe lower two images.
The upper half ofthis panel displays a visualization of thenative speaker's phrase describing theimage.
The student then attempts to mimicthe pronunciation of the native speaker.The visualization of the student's utteranceis displayed in real time.
Each visualizationincludes pitch (the fine line at the top),emphasis (the line varying in thickness) andan image of highly processed spectralinformation of the normalized voice.The meter to the right gives an evaluation.common standard, and displays various abstractor at least highly processed features of thenormalized voices, so that differences irrelevantto speech (such as how deep your voice is, or thefrequency response curve of the microphone)hopefully do not play a role.The second application, currently underdevelopment, is active vocabulary building.
Theuser sees four pictures and hears four phrasessemantically related to the pictures.
This ismaterial they have already worked over in otherlearning modes designed to build passivevocabulary, i.e.
the ability to recognize themeaning of speech.
However in this exercise theuser must be able to generate the speech withless prompting.
The order of the pictures isscrambled, and they are flashed one at a time.The user must respond to each with the phrasethat was given for that picture.
The systemevaluates their success, i.e.
whether theyresponded with the correct phrase, one of theother phrases, or some unrelated utterance.
Onedifficulty for the system is that frequently thefour phrases are very similar, so that thedifference between them might hinge on a shortpiece in the middle of otherwise nearly identicalutterances (for example "the girl is cutting theblue paper", "the girl is cutting the red paper").EAR is written in C. Since TRS is written inMacroMedia Director TM, EAR is interfaced toTRS using Director's interface for extendingDirector with C code.
TRS is multithreaded, soEAR is able to do its work incrementally since itmust not take the CPU for extended periods oftime.
Indeed EAR itself contains multiplethreads of two kinds: description threads andcomparison threads.Since the system might load several prerecordedutterances of native speakers at once, it isdesirable that the work of computing thenormalized high-level description of eachutterance be done while the user is listening tothose utterances, in parallel.
Thus each streamof sound data (22050 Hz sound samples) isanalyzed by a separate description thread, with avisual display in real time being an option.Similarly, sound data from the microphone isanalyzed in real time while the student isspeaking by a description thread, and theresulting visual display is displayed in real time.Description threads are discussed in Section 1.Once the user has finished speaking, acomparison thread can be launched for each ofthe native speaker descriptions, which comparethose descriptions to the description of thestudent's utterance.
Comparison threads arediscussed in Section 2.1 Utterance DescriptionAn EAR utterance description is a vector offeature vectors.
Of these, only pitch, emphasisand a dozen spectral features are portrayed in thevisual display.
An utterance descriptioncontains one feature vector for each 1/100 of asecond of the utterance.131.1 FiltersDescription of a sound stream begins with 48tuned Danforth(1997) filters developing a mel-scale frequency domain spectrum.
They aretuned 6 per octave to cover 8 octaves, thehighest frequency of the highest octave being8820 Hz, well below the Nyquist limit for a22050 Hz sample rate.
Within each octave eachfilter is tuned to a frequency 2 TM times as high asthe next lower filter, so that they aregeometrically evenly spaced over the octave.1.2 Speech DetectionEvery 220 sound samples, i.e.
about 100 timesper second, the response of each of the filters issampled.
Call the resulting 48-value vector the"raw spectrum".
EAR automatically detects theonset and end of speech by the followingmethod.
Let S be the sum of the upper half ofthe raw spectrum.
If S is greater than five timesthe least S observed uring this utterance, EARconsiders that speech is occurring.
This methodmakes EAR insensitive to constant backgroundnoise, but not to varying background noise1.3 Voice NormalizationThe natural logarithm of the raw spectrumvalues are smoothed in the frequency domain,using kernel widths adequate to bridge thedistance between the voice harmonics of a child.This over-smoothes the signal for adults,especially males, but it makes the resultingspectral curve less dependent on the pitch of thevoice and more accurately reflect formants.The rain and max of the smoothed result aremapped to 0 and 1 respectively, and multipliedby the volume, to give a measure of thedistribution of energy in the spectrum.
This isthe data displayed in the voice panel in Fig.
1,and the data (combined with pitch and emphasis)used in the comparison discussed in thefollowing section.2 ComparisonThis section describes the dynamic templatematching approach used in EAR to match twoutterances.
The result of a comparison betweentwo utterance descriptions A and B is a mappingbetween the two, and a scalar that on the range0-1 gives a measure of similarity between thetwo utterances.
A threshold on the scalar can beused to accept or reject the hypothesis that thetwo utterances are the same.In a real-time thread, EAR dynamically matchesa pair (A,B) of descriptions by means of a zipperobject.
Remember that a description containsone feature vector for each .01 second ofutterance.
A zipper object implements amapping from description A to description B inpatches.
A patch is a segment (time-contiguousseries of feature vectors) of A that is mapped toa segment of identical length (duration) in B. Azipper is a series of compatible patches--nooverlaps, and the nth patch, timewise, in A ismapped to the nth patch in B.
In the gapsbetween patches, A is mapped to B byinterpolation.
If the gap in A is x times as longas the gap in B, then each feature vector in thegap in B is mapped to, on the average, xconsecutive f ature vectors in A, such that thetime discrepancy between the two patches ismade up incrementally asyou traverse the gap.Initially several identical zippers are made byinterpolating the two utterances onto each otherwholesale--beginning to beginning, end to end,and everything in between is time interpolated.EAR then goes about randomly improving themas will be described shortly.
When the zipperscease improving significantly, the best one istaken as the mapping between the twoutterances.A track(A,B) maps each feature vector ofdescription A onto a feature vector ofdescription B in a time non-decreasing fashion.A zipper object defines two compatible tracks,one from A to B and the other from B to A. Thegoodness of zipper z is defined as the leastgoodness of its two tracks.
The goodness of atrack(A,B) is the trackValue minus thetrackCost.The trackCost penalizes tracks where the timingof A relative to B is not uniform.
It accumulates?
cost whenever timing is advanced, then retarded,etc., but permits a smooth movement in onedirection without cost, so that an utterance thatis uniformly slower or faster than another is notpenalized.14The trackValue favors tracks which match betterthan would be expected--the null hypothesis.Since a track maps each feature vector of A ontoone of B, the trackValue is the sum of thevectorMatches of those pairs of vectors, dividedby the null hypothesis value of the match of A.The vectorMatch(Fa,Fb) of a pair of featurevectors Fa,Fb isvectorMatch(Fa,Fb) = Fa.infoWt*MAMI(Fa,Fb)Let feature vectors Fa and Fb be indexed by i toaccess their m individual features.
ThenOngoing research includes better automaticadaptation to different microphones' responsecurves without burdening the user with trainingsessions or stringent microphone requirements.ReferencesDanforth, Doug (1997), ftp downloadable fromwww.speech.cs.cmu.edu/comp.speech/Section6/Q6.3.html.SUM(i=I to m) { min(Fa\[i\],Fb\[i\]) }MAMI(Fa,Fb) ............................................. * l/mSUM(i=I to m) {max(Fa\[i\],Fb\[i\]) }Thus if the features are random uniformlydistributed random variables in the range 0 to 1,the expected (null hypothesis) value of MAMI isV2.ConclusionSpeech comparison in TRS enables tudents tofocus on those elements of pronunciation thatare deficient.
Pitch and emphasis are used quitedifferently in most languages.
For example, inEnglish, pitch is used to mark questions,responses, and place in a list, whereas inChinese there are very different words whoseonly distinguishing characteristic is pitch.Some users of TRS who could not hear thedifference between avowel sound produced by anative speaker and their own vowel, have beenhelped by the visual display drawing theirattention to the nature of the difference.15
