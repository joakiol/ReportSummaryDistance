Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 335?345,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsImproving Mention Detection Robustness to Noisy InputRadu Florian, John F. Pitrelli, Salim Roukos and Imed ZitouniIBM T.J. Watson Research CenterYorktown Heights, NY, U.S.A.{raduf,pitrelli,roukos,izitouni}us.ibm.comAbstractInformation-extraction (IE) research typicallyfocuses on clean-text inputs.
However, an IEengine serving real applications yields manyfalse alarms due to less-well-formed input.For example, IE in a multilingual broadcastprocessing system has to deal with inaccu-rate automatic transcription and translation.The resulting presence of non-target-languagetext in this case, and non-language mate-rial interspersed in data from other applica-tions, raise the research problem of makingIE robust to such noisy input text.
We ad-dress one such IE task: entity-mention de-tection.
We describe augmenting a statisticalmention-detection system in order to reducefalse alarms from spurious passages.
The di-verse nature of input noise leads us to pursuea multi-faceted approach to robustness.
Forour English-language system, at various missrates we eliminate 97% of false alarms on in-puts from other Latin-alphabet languages.
Inanother experiment, representing scenarios inwhich genre-specific training is infeasible, weprocess real financial-transactions text con-taining mixed languages and data-set codes.On these data, because we do not train on datalike it, we achieve a smaller but significant im-provement.
These gains come with virtuallyno loss in accuracy on clean English text.1 IntroductionInformation-extraction (IE) research is typically per-formed on clean text in a predetermined language.Lately, IE has improved to the point of being usablefor some real-world tasks whose accuracy require-ments are reachable with current technology.
Theseuses include media monitoring, topic alerts, sum-marization, population of databases for advancedsearch, etc.
These uses often combine IE with tech-nologies such as speech recognition, machine trans-lation, topic clustering, and information retrieval.The propagation of IE technology from isolateduse to aggregates with such other technologies, fromNLP experts to other types of computer scientists,and from researchers to users, feeds back to the IEresearch community the need for additional inves-tigation which we loosely refer to as ?information-extraction robustness?
research.
For example:1.
Broadcast monitoring demands that IE handleas input not only clean text, but also the tran-scripts output by speech recognizers.2.
Multilingual applications, and the imperfectionof translation technology, require IE to contendwith non-target-language text input (Pitrelli etal., 2008).3.
Naive users at times input to IE other materialwhich deviates from clean text, such as a PDFfile that ?looks?
like plain text.4.
Search applications require IE to deal withdatabases which not only possess clean text butat times exhibit other complications like mark-up codes particular to narrow, application-specific data-format standards, for example, theexcerpt from a financial-transactions data setshown in Figure 1.Legacy industry-specific standards, such as il-lustrated in this example, are part of long-established processes which are cumbersometo convert to a more-modern database format.Transaction data sets typically build up over aperiod of years, and as seen here, can exhibit335:54D://121000358BANK OF BOSTON:55D:/0148280005NEVADA DEPT.OF VET.94C RECOV.FD-5:MAC:E19DECA8CHK:641EB09B8968USING OF FIELD 59: ONLY /INS/ WHENFOLLOWED BY BCC CODE IN CASEOF QUESTIONS DONT HESITATE TOCONTACT US QUOTING REFERENCENON-STC CHARGES OR VIA E-MAIL:YOVANKA(UL)BRATASOVA(AT)BOA.CZ.BEST REGARDSBANKA OBCHODNIKA, A.S. PRAGUE, CZ:58E::ADTX//++ ADDITIONALINFORMATION ++ PLEASE BEINFORMED THAT AS A RESULT OFTHE PURCHASE OFFER ENDED ON 23MAR 2008 CALDRADE LTD. ISPOSSESSING WITH MORE THEN 90PER CENT VOTING RIGHT OF SLICE.THEREFOR CALDRADE LTD. ISEXERCISING PURCHASE RIGHTSFOR ALL SLICE SHARES WHICH ARECURRENTLY NOT INHIS OWN.PURCHASE PRICE: HUF 1.940 PERSHARE.
PLEASE :58E::ADTX//NOTETHAT THOSE SHARES WHICH WILLNOT BE PRESENTED TO THE OFFERWILL BE CANCELLED AND INVALID.
:58:SIE SELBSTTRN/REF:515220 035:78:RUECKGABE DES BETRAGES LT.ANZBA43 M ZWECKS RUECKGABE INAUD.
URSPR.
ZU UNSEREM ZA MITREF.
0170252313279065 UND IHRERUECKG.
:42:/BNF/UNSERE REF:Figure 1: Example application-specific text, in thiscase from financial transactions.peculiar mark-up interspersed with meaning-ful text.
They also suffer complications arisingfrom limited-size entry fields and a diversityof data-entry personnel, leading to effects likehaphazard abbreviation and improper spacing,as shown.
These issues greatly complicate theIE problem, particularly considering that adapt-ing IE to such formats is hampered by the exis-tence of a multitude of such ?standards?
and bylack of sufficient annotated data in each one.A typical state-of-the-art statistical IE engine willhappily process such ?noisy?
inputs, and will typ-ically provide garbage-in/garbage-out performance,embarrassingly reporting spurious ?information?
nohuman would ever mistake.
Yet it is also inappro-priate to discard such documents wholesale: evenpoor-quality inputs may have relevant informationinterspersed.
This information can include accuratespeech-recognition output, names which are recog-nizable even in wrong-language material, and cleantarget-language passages interleaved with the mark-up.
Thus, here we address methods to make IE ro-bust to such varied-quality inputs.
Specifically, ouroverall goals are?
to skip processing non-language material suchas standard or database-specific mark-up,?
to process all non-target-language text cau-tiously, catching interspersed target-languagetext as well as text which is compatible withthe target language, e.g.
person names whichare the same in the target- and non-target lan-guage, and?
to degrade gracefully when processing anoma-lous target-language material,while minimizing any disruption of the processingof clean, target-language text, and avoiding any ne-cessity for explicit pre-classification of the genre ofmaterial being input to the system.
Such explicitclassification would be impractical in the presenceof the interleaving and the unconstrained data for-mats from unpredetermined sources.We begin our robustness work by addressing animportant and basic IE task: mention detection(MD).
MD is the task of identifying and classifyingtextual references to entities in open-domain texts.Mentions may be of type ?named?
(e.g.
John, LasVegas), ?nominal?
(e.g.
engineer, dentist)or ?pronominal?
(e.g.
they, he).
A mention also336has a specific class which describes the type of en-tity it refers to.
For instance, consider the followingsentence:Julia Gillard, primeminister of Australia,declared she will enhancethe country?s economy.Here we see three mentions of one person en-tity: Julia Gillard, prime minister, andshe; these mentions are of type named, nominal,and pronominal, respectively.
Australia andcountry are mentions of type named and nominal,respectively, of a single geopolitical entity.
Thus, theMD task is a more general and complex task thannamed-entity recognition, which aims at identifyingand classifying only named mentions.Our approach to IE has been to use language-independent algorithms, in order to facilitate reuseacross languages, but we train them with language-specific data, for the sake of accuracy.
Therefore, in-put is expected to be predominantly in a target lan-guage.
However, real-world data genres inevitablyinclude some mixed-language/non-linguistic input.Genre-specific training is typically infeasible dueto such application-specific data sets being unanno-tated, motivating this line of research.
Therefore, thegoal of this study is to investigate schemes to make alanguage-specific MD engine robust to the types ofinterspersed non-target material described above.
Inthese initial experiments, we work with English asthe target language, though we aim to make our ap-proach to robustness as target-language-independentas possible.While our ultimate goal is a language-independent approach to robustness, in theseinitial experiments, English is the target language.However, we process mixed-language materialincluding real-world data with its own peculiarmark-up, text conventions including abbreviations,and mix of languages, with the goal of English MD.We approach robust MD using a multi-stage strat-egy.
First, non-target-character-set passages (here,non-Latin-alphabet) are identified and marked fornon-processing.
Then, following word-tokenization,we apply a language classifier to a sliding variable-length set of windows in order to generate fea-tures for each word indicative of how much the textaround that word resembles good English, primar-ily in comparison to other Latin-alphabet languages.These features are used in a separate maximum-entropy classifier whose output is a single feature toadd to the MD classifier.
Additional features, pri-marily to distinguish English from non-language in-put, are added to MD as well.
An example is theminimum of the number of letters and the number ofdigits in the ?word?, which when greater than zerooften indicates database detritus.
Then we run theMD classifier enhanced with these new robustness-oriented features.
We evaluate using a detection-error-trade-off (DET) (Martin et al, 1997) anal-ysis, in addition to traditional precision/recall/F -measure.This paper is organized as follows.
Section 2 dis-cusses previous work.
Section 3 describes the base-line maximum-entropy-based MD system.
Section 4introduces enhancements to the system to achieverobustness.
Section 5 describes databases used forexperiments, which are discussed in Section 6, andSection 7 draws conclusions and plots future work.2 Previous work on mention detectionThe MD task has close ties to named-entity recog-nition, which has been the focus of much recent re-search (Bikel et al, 1997; Borthwick et al, 1998;Tjong Kim Sang, 2002; Florian et al, 2003; Bena-jiba et al, 2009), and has been at the center of sev-eral evaluations: MUC-6, MUC-7, CoNLL?02 andCoNLL?03 shared tasks.
Usually, in computational-linguistics literature, a named entity represents aninstance of either a location, a person, an organi-zation, and the named-entity-recognition task con-sists of identifying each individual occurrence ofnames of such an entity appearing in the text.
Asstated earlier, in this paper we are interested inidentification and classification of textual referencesto object/abstraction mentions, which can be eithernamed, nominal or pronominal.
This task has beena focus of interest in ACE since 2003.
The recentACE evaluation campaign was in 2008.Effort to handle noisy data is still limited, espe-cially for scenarios in which the system at decodingtime does not have prior knowledge of the input datasource.
Previous work dealing with unstructureddata assumes the knowledge of the input data source.As an example, E. Minkov et al (Minkov et al,2005) assume that the input data is text from e-mails,and define special features to enhance the detectionof named entities.
Miller et al (Miller et al, 2000)assume that the input data is the output of a speechor optical character recognition system, and henceextract new features for better named-entity recog-nition.
In a different research problem, L. Yi et aleliminate the noisy text from the document before337performing data mining (Yi et al, 2003).
Hence,they do not try to process noisy data; instead, theyremove it.
The approach we propose in this paperdoes not assume prior knowledge of the data source.Also we do not want to eliminate the noisy data, butrather attempt to detect the appropriate mentions, ifany, that appear in that portion of the data.3 Mention-detection algorithmSimilarly to classical NLP tasks such as base phrasechunking (Ramshaw and Marcus, 1999) and named-entity recognition (Tjong Kim Sang, 2002), we for-mulate the MD task as a sequence-classificationproblem, by assigning to each word token in thetext a label indicating whether it starts a specificmention, is inside a specific mention, or is out-side any mentions.
We also assign to every non-outside label a class to specify entity type e.g.
per-son, organization, location, etc.
We are interestedin a statistical approach that can easily be adaptedfor several languages and that has the ability tointegrate easily and make effective use of diversesources of information to achieve high system per-formance.
This is because, similar to many NLPtasks, good performance has been shown to dependheavily on integrating many sources of informa-tion (Florian et al, 2004).
We choose a MaximumEntropy Markov Model (MEMM) as described pre-viously (Florian et al, 2004; Zitouni and Florian,2009).
The maximum-entropy model is trained us-ing the sequential conditional generalized iterativescaling (SCGIS) technique (Goodman, 2002), and ituses a Gaussian prior for regularization (Chen andRosenfeld, 2000)1.3.1 Mention detection: standard featuresThe featues used by our mention detection systemscan be divided into the following categories:1.
Lexical Features Lexical features are imple-mented as token n-grams spanning the currenttoken, both preceding and following it.
For atoken xi, token n-gram features will contain theprevious n?1 tokens (xi?n+1, .
.
.
xi?1) and thefollowing n?
1 tokens (xi+1, .
.
.
xi+n?1).
Set-ting n equal to 3 turned out to be a good choice.2.
Gazetteer-based Features The gazetteer-based features we use are computed on tokens.1Note that the resulting model cannot really be called amaximum-entropy model, as it does not yield the model whichhas the maximum entropy (the second term in the product), butrather is a maximum-a-posteriori model.The gazetteers consist of several class ofdictionaries: including person names, countrynames, company names, etc.
Dictionar-ies contain single names such as John orBoston, and also phrases such as BarackObama, New York City, or The UnitedStates.
During both training and decoding,when we encounter in the text a token or asequence of tokens that completely matches anentry in a dictionary, we fire its correspondingclass.The use of this framework to build MD systemsfor clean English text has given very competitive re-sults at ACE evaluations (Florian et al, 2006).
Try-ing other classifiers is always a good experiment,which we didn?t pursue here for two reasons: first,the MEMM system used here is state-of-the-art, asproven in evaluations and competitions ?
while it isentirely possible that another system might get betterresults, we don?t think the difference would be large.Second, we are interested in ways of improving per-formance on noisy data, and we expect any systemto observe similar degradation in performance whenpresented with unexpected input ?
showing resultsfor multiple classifier types might very well dilutethe message, so we stuck to one classifier type.4 Enhancements for robustnessAs stated above, our goal is to skip spans of charac-ters which do not lend themselves to target-languageMD, while minimizing impact on MD for target-language text, with English as the initial target lan-guage for our experiments.
More specifically, ourtask is to process data automatically in any unprede-termined format from any source, during which westrive to avoid outputting spurious mentions on:?
non-language material, such as mark-up tagsand other data-set detritus, as well as non-textdata such as code or binaries likely mistakenlysubmitted to the MD system,?
non-target-character-set material, here, non-Latin-alphabet material, such as Arabic andChinese in their native character sets, and?
target-character-set material not in the targetlanguage, here, Latin-alphabet languages otherthan English.It is important to note that this is not merelya document-classification problem; this non-targetdata is often interspersed with valid input text.338Mark-up is the obvious example of interspersing;however, other categories of non-target data can alsointerleave tightly with valid input.
A few examples:?
English text is sometimes infixed right in a Chi-nese sentence, such as?
some translation algorithms will leave un-changed an untranslatable word, or willtransliterate it into the target language using acharacter convention which may not be a stan-dard known to the MD engine, and?
some target-alphabet-but-non-target-languagematerial will be compatible with the targetlanguage, particularly people?s names.
Anexample with English as the target lan-guage is Barack Obama in the Spanishtext ...presidente de EstadosUnidos, Barack Obama, dijo elda 24 que ....Therefore, to minimize needless loss of process-able material, a robustness algorithm ideally does asliding analysis, in which, character-by-character orword-by-word, material may be deemed to be suit-able to process.
Furthermore, a variety of strategieswill be needed to contend with the diverse nature ofnon-target material and the patterns in which it willappear among valid input.Accordingly, the following is a summary of algo-rithmic enhancements to MD:1. detection of standard file formats, such asSGML, and associated detagging,2.
segmentation of the file into target- vs. non-target-character-set passages, such that the lat-ter not be processed further,3.
tokenization to determine word and sentenceunits, and4.
MD, augmented as follows:?
Sentence-level categorization of likeli-hood of good English.?
If ?clean?
English was detected, run thesame clean baseline model as described inSection 3.?
If the text is determined to be abad fit to English, run an alternatemaximum-entropy model that is heavilybased on gazetteers, using only context-independent (e.g.
primarily gazetteer-based) features, to catch isolated ob-vious English/English-compatible namesembedded in otherwise-foreign text.?
If in between ?clean?
and ?bad?, usea ?mixed?
maximum-entropy MD modelwhose training data and feature set areaugmented to handle interleaving of En-glish with mark-up and other languages.These MD-algorithm enhancements will be de-scribed in the following subsections.4.1 Detection and detagging for standard fileformatsSome types of mark-up are well-known standards,such as SGML (Warmer and van Egmond, 1989).Clearly the optimal way of dealing with them is toapply detectors of these specific formats, and associ-ated detaggers, as done previously (Yi et al, 2003).For this reason, standard mark-up is not a subject ofthe current study; rather, our concern is with mark-up peculiar to specific data sets, as described above,and so while this step is part of our overall strategy,it is not employed in the present experiments.4.2 Character-set segmentationSome entity mentions may be recognizable in a non-target language which shares the target-language?scharacter set, for example, a person?s name recog-nizable by English speakers in an otherwise-not-understandable Spanish sentence.
However, non-target character sets, such as Arabic and Chinesewhen processing English, represent pure noise foran IE system.
Therefore, deterministic character-set segmentation is applied, to mark non-target-character-set passages for non-processing by the re-mainder of the system, or, in a multilingual system,to be diverted to a subsystem suited to process thatcharacter set.
Characters which can be ambiguouswith regard to character set, such as some punctua-tion marks, are attached to target-character-set pas-sages when possible, but are not considered to breaknon-target-character-set passages surrounding themon both sides.4.3 TokenizationSubsequent processing is based on determination ofthe language of target-alphabet text.
The fundamen-tal unit of such processing is target-alphabet word,necessitating tokenization at this point into word-level units.
This step includes punctuation sepa-339ration as well as the detction of sentence bound-ary (Zimmerman et al, 2006).4.4 Robust mention detectionAfter preprocessing steps presented earlier, we de-tect mentions using a cascaded approach that com-bines several MD classifiers.
Our goal is to selectamong maximum-entropy MD classifiers trainedseparately to represent different degrees of ?nois-iness?
occurring in many genres of data, includ-ing machine-translation output, informal communi-cations, mixed-language material, varied forms ofnon-standard database mark-up, etc.
We somewhat-arbitrarily choose to employ three classifiers as de-scribed below.
We select a classifier based on asentence-level determination of the material?s fit tothe target language.
First, we build an n-gram lan-guage model on clean target-language training text.This language model is used to compute the perplex-ity (PP ) of each sentence during decoding.
ThePP indicates the quality of the text in the target-language (i.e.
English) (Brown et al, 1992); thelower the PP , the cleaner the text.
A sentencewith a PP lower than a threshold ?1 is considered?clean?
and hence the ?clean?
baseline MD modeldescribed in Section 3 is used to detect mentionsof this sentence.
The clean MD model has accessto standard features described in Section 3.1.
Inthe case where a sentence looks particularly badlymatched to the target language, defined as PP > ?2,we use a ?gazetteer-based?
model based on a dic-tionary look-up to detect mentions; we retreat toseeking known mentions in a context-independentmanner reflecting that most of the context consistsof out-of-vocabulary words.
The gazetteer-basedMD model has access only to gazetteer informationand does not look to lexical context during decod-ing, reflecting the likelihood that in this poor ma-terial, words surrounding any recognizable mentionare foreign and therefore unusable.
In the case of anin-between determination, that is, a sentence with?1 < PP < ?2, we use a ?mixed?
MD model, basedon augmenting the training data set and the featureset as described in the next section.
The values of ?1and ?2 are estimated empirically on a separate devel-opment data set that is also used to tune the Gaussianprior (Chen and Rosenfeld, 2000).
This set containsa mix of clean English and Latin-alphabet-but-non-English text that is not used for traning and evalua-tion.The advantage of this combination strategy is thatwe do not need pre-defined knowledge of the textsource in order to apply an appropriate model.
Theselection of the appropriate model to use for de-coding is done automatically based on PP value ofthe sentence.
We will show in the experiments sec-tion how this combination strategy is effective notonly in maintaining good performance on a cleanEnglish text but also in improving performance onnon-English data when compared to other source-specific MD models.4.5 Mixed mention detection modelThe mixed MD model is designed to process ?sen-tences?
mixing English with non-English, whetherforeign-language or non-language material.
Ourapproach is to augment model training comparedto the clean baseline by adding non-English,mixed-language, and non-language material, andto augment the model?s feature set with language-identification features more localized than thesentence-level perplexity described above, as well asother features designed primarily to distinguish non-language material such as mark-up codes.4.5.1 Language-identification featuresWe apply an n-gram-based language classi-fier (Prager, 1999) to variable-length sliding win-dows as follows.
For each word, we run 1- through6-preceding-word windows through the classifier,and 1- through 6-word windows beginning with theword, for a total of 12 windows, yielding for eachwindow a result like:0.235 Swedish0.148 English0.134 French...For each of the 12 results, we extract three fea-tures: the identity of the top-scoring language, here,Swedish; the confidence score in the top-scoringlanguage, here, 0.235; and the score difference be-tween the target language (English for these ex-periments) and the top-scoring non-target language,here, 0.148 ?
0.235 = ?0.087.
Thus we havea 36-feature vector for each word.
We bin theseand use them as input to a maximum-entropy clas-sifier (separate from the MD classifier) which out-puts ?English?
or ?Non-English?, and a confidencescore.
These scores in turn are binned into six cate-gories to serve as a ?how-English-is-it?
feature in theaugmented MD model.
The language-identificationclassifier and the maximum-entropy ?how-English?classifier are each trained on text data separate from340each other and from the training and test sets forMD.4.5.2 Additional featuresThe following features are designed to captureevidence of whether a ?word?
is in fact linguisticmaterial or not: number of alphabetic characters,number of characters, maximum consecutive rep-etitions of a character, numbers of non-alphabeticand non-alphanumeric characters, fraction of char-acters which are alphabetic, fraction alphanumeric,and number of vowels.
These features are part of theaugmentation of the mixed MD model relative to theclean MD model.5 Data setsFour data sets are used for our initial experiments.One, ?English?, consists of 367 documents total-ing 170,000 words, drawn from web news storiesfrom various sources and detagged to be plain text.This set is divided into 340 documents as a train-ing set and 27 for testing, annotated as described inmore detail elsewhere (Han, 2010).
These data av-erage approximately 21 annotated mentions per 100words.The second set, ?Latin?, consists of 23 detaggedweb news articles from 11 non-English Latin-alphabet languages totaling 31,000 words.
Of thesearticles, 12 articles containing 19,000 words areused as a training set, with the remaining used fortesting, and each set containing all 11 languages.They are annotated using the same annotation con-ventions as ?English?, and from the perspective ofEnglish; that is, only mentions which would be clearto an English speaker are labeled, such as BarackObama in the Spanish example in Section 4.
Forthis reason, these data average only approximately 5mentions per 100 words.The third, ?Transactions?, consists of approxi-mately 60,000 words drawn from a text data setlogging real financial transactions.
Figure 1 showsexample passages from this database, anonymizedwhile preserving the character of the content.This data set logs transactions by a staff ofcustomer-service representatives.
English is the pri-mary language, but owing to international clientele,occasionally representatives communicate in otherlanguages, such as the German here, or in Englishbut mentioning institutions in other countries, here, aCzech bank.
Interspersed among text are codes spe-cific to this application which delineate and identifyvarious information fields and punctuate long pas-sages.
The application also places constraints onlegal characters, leading to the unusual representa-tion of underline and the ?at?
sign as shown, mak-ing for an e-mail address which is human-readablebut likely not obvious to a machine.
Abbreviationsrepresent terms particularly common in this appli-cation area, though they may not be obvious with-out adapting to the application; these include stan-dards like HUF, a currency code which stands forHungarian forint, and financial-transaction peculiar-ities like BNF for ?beneficiary?
as seen in Figure 1.In short, good English is interspersed with non-language content, foreign-language text, and roughEnglish like data-entry errors and haphazard abbre-viations.
These data average 4 mentions per 100words.Data sets with peculiarities analogous to those inthis Transactions set are commonplace in a varietyof settings.
Training specific to data sets like this isoften infeasible due to lack of labeled data, insuffi-cient data for training, and the multitude of such dataformats.
For this reason, we do not train on Transac-tions, letting our testing on this data set serve as anexample of testing on such data formats unseen.6 ExperimentsMD systems were trained to recognize the 116entity-mention types shown in Table 1, annotated asdescribed previously (Han, 2010).
The clean-dataclassifier was trained on the English training data us-ing the feature set described in Section 3.1.
The clas-sifier for ?mixed?-quality data and the ?gazetteer?model were each trained on that set plus the ?Latin?training set and the supplemental set.
In addition,?mixed?
training included the additional features de-scribed in Section 4.5.
The framework used to buildthe baseline MD system is similar to the one we usedin the ACE evaluation2.
This system has achievedcompetitive results with an F -measure of 82.7 whentrained on the seven main types of ACE data withaccess to wordnet and part-of-speech-tag informa-tion as well as output of other MD and named-entityrecognizers (Zitouni and Florian, 2008).It is instructive to evaluate on the individual com-ponent systems as well as the combination, despitethe fact that the individual components are not well-suited to all the data sets, for example, the mixedand gazetteer systems being a poorer fit to the En-glish task than the baseline, and vice versa for the2NIST?s ACE evaluation plan:http://www.nist.gov/speech/tests/ace/index.htm341age event-custody facility people dateanimal event-demonstration food percent durationaward event-disaster geological-object person e-mail-addresscardinal event-legal geo-political product measuredisease event-meeting law substance moneyevent event-performance location title-of-a-work phone-numberevent-award event-personnel ordinal vehicle ticker-symbolevent-communication event-sports organ weapon timeevent-crime event-violence organization web-addressTable 1: Entity-type categories used in these experiments.
The eight in the right-most column are notfurther distinguished by mention type, while the remaining 36 are further classified as named, nominal orpronominal, for a total of 36 ?
3 + 8 = 116 mention labels.English Latin TransactionsP R F P R F P R FClean 78.7 73.6 76.1 16.0 40.0 22.9 19.5 32.2 24.3Mixed 77.9 69.7 73.6 78.5 55.9 65.3 37.1 47.8 41.7Gazetteer 76.9 66.2 71.1 77.8 55.5 64.8 36.5 47.5 41.3Combination 78.1 73.2 75.6 80.4 56.0 66.0 38.5 49.1 43.2Table 2: Performance of clean, mixed, and gazetteer-based mention detection systems as well as their com-bination.
Performance is presented in terms of Precision (P), Recall (R), and F -measure (F).non-target data sets.
Precision/recall/F -measure re-sults are shown in Table 2.
Not surprisingly, thebaseline system, intended for clean data, performspoorly on noisy data.
The mixed and gazetteer sys-tems, having a variety of noisy data in their train-ing set, perform much better on the noisy conditions,particularly on Latin-alphabet-non-English data be-cause that is one of the conditions included in itstraining, while Transactions remains a condition notcovered in the training set and so shows less im-provement.
However, because the mixed classifier,and moreso the gazetteer classifier, are oriented tonoisy data, on clean data they suffer in performanceby 2.5 and 5 F -measure points, respectively.
Butsystem combination serves us well: it recovers allbut 0.5 F -measure point of this loss, while also ac-tually performing better on the noisy data sets thanthe two classifiers specifically targeted toward them,as can be seen in Table 2.
It is important to notethat the major advantage of using the combinationmodel is the fact that we do not have to know thedata source in order to select the appropriate MDmodel to use.
We assume that the data source isunknown, which is our claim in this work, and weshow that we obtain better performance than usingsource-specific MD models.
This reflects the factthat a noisy data set will in fact have portions withvarying degrees of ?noise?, so the combination out-performs any single model targeted to a single par-ticular level of noise, enabling the system to con-tend with such variability without the need for pre-segregating sub-types of data for noise level.
Theobtained improvement from the system combinationover all other models is statistically significant basedon the stratified bootstrap re-sampling significancetest (Noreen, 1989).
We consider results statisticallysignificant when p < 0.05, which is the case in thispaper.
This approach was used in the named-entity-recognition shared task of CoNNL-20023.It should be noted that some completely-non-target types of data, such as non-target-character setdata, have been omitted from analysis here.
In-cluding them would make our system look compar-atively stronger, as they would have only spuriousmentions and so generate false alarms but no correctmentions in the baseline system, while our systemdeterministically removes them.As mentioned above, we view MD robustness pri-marily as an effort to eliminate, relative to a base-line system, large volumes of spurious ?mentions?detected in non-target input content, while minimiz-3http://www.cnts.ua.ac.be/conll2002/ner/342(a) DET plot for clean (baseline), mixed, gazetteer,and combination MD systems on the Latin-alphabet-non-English text.
The clean system (upper curve)performs far worse than the other three systems de-signed to provide robustness; these systems in turnperform nearly indistinguishably.
(b) DET plot for clean (baseline), mixed, gazetteer,and combination MD systems on the Transactionsdata set.
The clean system (upper/longer curve)reaches far higher false-alarm rates, while never ap-proaching the lower miss rates achievable by any ofthe other three systems, which in turn perform com-parably to each other.Figure 2: DET plots for Latin-alphabet-non-English and Transactions data setsing disruption of detection in target input.
A sec-ondary goal is recall in the event of occasional validmentions in such non-target material.
Thus, as in-put material degrades, precision increases in impor-tance relative to recall.
As such, we view precisionand recall asymmetrically on this task, and so ratherthan evaluating purely in terms of F -measure, weperform a detection-error-trade-off (DET) (Martinet al, 1997) analysis, in which we plot a curve ofmiss rate on valid mentions vs. false-alarm rate, withthe curve traced by varying a confidence thresholdacross its range.
We measure false-alarm and missrates relative to the number of actual mentions anno-tated in the data set:FA rate = # false alarms# annotated mentions (1)Miss rate = # misses# annotated mentions (2)where false alarms are ?mentions?
output by the sys-tem but not appearing in annotation, while missesare mentions which are annotated but do not ap-pear in the system output.
Each mention is treatedequally in this analysis, so frequently-recurring en-tity/mention types weigh on the results accordingly.Figure 2a shows a DET plot for the clean, mixed,gazetteer, and combination systems on the ?Latin?data set, while Figure 2b shows the analogous plotfor the ?Transactions?
data set.
The drastic gainsmade over the baseline system by the three experi-mental systems are evident in the plots.
For exam-ple, on Latin, choosing an operating point of a missrate of 0.6 (nearly the best achievable by the cleansystem), we find that the robustness-oriented sys-tems eliminate 97% of the false alarms of the cleanbaseline system, as the plot shows false-alarm ratesnear 0.07 compared to the baseline?s of 2.08.
Gainson Transaction data are more modest, owing to thiscase representing a data genre not included in train-ing.
It should be noted that the jaggedness of theTransaction curves traces to the repetitive nature ofsome of the terms in this data set.In making a system more oriented toward robust-ness in the face of non-target inputs, it is importantto quantify the effect of these systems being less-oriented toward clean, target-language text.
Figure 3shows the analogous DET plot for the English testset, showing that achieving robustness through thecombination system comes at a small cost to accu-racy on the text the original system is trained to pro-cess.7 ConclusionsFor information-extraction systems to be useful,their performance must degrade gracefully whenconfronted with inputs which deviate from idealand/or derive from unknown sources in unknownformats.
Imperfectly-translated, mixed-language,marked-up text and non-language material must not343Figure 3: DET plot for clean (baseline), mixed,gazetteer, and combination MD systems on clean Englishtext, verifying that performance by the clean system (low-est curve) is very closely approximated by the combina-tion system (second-lowest curve), while the mixed sys-tem performs somewhat worse and the gazetteer system(top curve), worse still, reflecting that these systems areincreasingly oriented toward noisy inputs.be processed in a garbage-in-garbage-out fashionmerely because the system was designed only tohandle clean text in one language.
Thus we have em-barked on information-extraction-robustness work,to improve performance on imperfect inputs whileminimizing disruption of processing of clean text.We have demonstrated that for one IE task, mentiondetection, a multi-faceted approach, motivated bythe diversity of input data imperfections, can elimi-nate a large proportion of the spurious outputs com-pared to a system trained on the target input, at arelatively small cost of accuracy on that target input.This outcome is achieved by a system-combinationapproach in which a perplexity-based measure ofhow well the input matches the target language isused to select among models designed to deal withsuch varying levels of noise.
Rather than relying onexplicit recognition of genre of source data, the ex-perimental system merely does its own assessmentof how much each sentence-sized chunk matches thetarget language, an important feature in the case ofunknown text sources.Chief among directions for further work is to con-tinue to improve performance on noisy data, and tostrengthen our findings via larger data sets.
Addi-tionally, we look forward to expanding analysis todifferent types of imperfect input, such as machine-translation output, different types of mark-up, anddifferent genres of real data.
Further work shouldalso explore the degree to which the approach toachieving robustness must vary according to the tar-get language.
Finally, robustness work should be ex-panded to other information-extraction tasks.AcknowledgementsThe authors thank Ben Han, Anuska Renta,Veronique Baloup-Kovalenko and Owais Akhtar fortheir help with annotation.
This work was supportedin part by DARPA under contract HR0011-08-C-0110.ReferencesY.
Benajiba, M. Diab, and P. Rosso.
2009.
Arabic namedentity recognition: A feature-driven study.
In the spe-cial issue on Processing Morphologically Rich Lan-guages of the IEEE Transaction on Audio, Speech andLanguage.D.
M. Bikel, S. Miller, R. Schwartz, and R. Weischedel.1997.
Nymble: a high-performance learning name-finder.
In Proceedings of ANLP-97, pages 194?201.A.
Borthwick, J.
Sterling, E. Agichtein, and R. Grishman.1998.
Exploiting diverse knowledge sources via max-imum entropy in named entity recognition.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, J. C.Lai, and R. L. Mercer.
1992.
An estimate of an up-per bound for the entropy of English.
ComputationalLinguistics, 18(1), March.S.
Chen and R. Rosenfeld.
2000.
A survey of smooth-ing techniques for ME models.
IEEE Transaction onSpeech and Audio Processing.R.
Florian, A. Ittycheriah, H. Jing, and T. Zhang.
2003.Named entity recognition through classifier combina-tion.
In Conference on Computational Natural Lan-guage Learning - CoNLL-2003, Edmonton, Canada,May.R.
Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-hatla, X. Luo, N Nicolov, and S Roukos.
2004.
Astatistical model for multilingual entity detection andtracking.
In Proceedings of HLT-NAACL 2004, pages1?8.R.
Florian, H. Jing, N. Kambhatla, and I. Zitouni.
2006.Factorizing complex models: A case study in men-tion detection.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Computa-tional Linguistics, pages 473?480, Sydney, Australia,July.
Association for Computational Linguistics.J.
Goodman.
2002.
Sequential conditional generalizediterative scaling.
In Proceedings of ACL?02.D.
B. Han.
2010.
Klue annotation guidelines - version2.0.
Technical Report RC25042, IBM Research, Au-gust.344A.
Martin, G. Doddington, T. Kamm, M. Ordowski, andM.
Przybocki.
1997.
The DET curve in assessmentof detection task performance.
In Proceedings of theEuropean Conference on Speech Communication andTechnology (Eurospeech), pages 1895?1898.
Rhodes,Greece.D.
Miller, S. Boisen, R. Schwartz, R. Stone, andR.
Weischedel.
2000.
Named entity extraction fromnoisy input: speech and OCR.
In Proceedings of thesixth conference on Applied natural language process-ing, pages 316?324, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.E.
Minkov, R. C. Wang, and W. W. Cohen.
2005.
Ex-tracting personal names from email: Applying namedentity recognition to informal text.
In Proceedings ofHuman Language Technology Conference and Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 443?450, Vancouver, British Columbia,Canada, October.
Association for Computational Lin-guistics.E.
W. Noreen.
1989.
Computer-Intensive Methods forTesting Hypotheses.
John Wiley Sons.J.
F. Pitrelli, B. L. Lewis, E. A. Epstein, M. Franz,D.
Kiecza, J. L. Quinn, G. Ramaswamy, A. Srivas-tava, and P. Virga.
2008.
Aggregating DistributedSTT, MT, and Information Extraction Engines: TheGALE Interoperability-Demo System.
In Interspeech.Brisbane, NSW, Australia.J.
M. Prager.
1999.
Linguini: Language identification formultilingual documents.
In Journal of ManagementInformation Systems, pages 1?11.L.
Ramshaw and M. Marcus.
1999.
Text chunking usingtransformation-based learning.
In S. Armstrong, K.W.Church, P. Isabelle, S. Manzi, E. Tzoukermann, andD.
Yarowsky, editors, Natural Language ProcessingUsing Very Large Corpora, pages 157?176.
Kluwer.E.
F. Tjong Kim Sang.
2002.
Introduction to the conll-2002 shared task: Language-independentnamed entityrecognition.
In Proceedings of CoNLL-2002, pages155?158.
Taipei, Taiwan.J.
Warmer and S. van Egmond.
1989.
The implementa-tion of the Amsterdam SGML parser.
Electron.
Publ.Origin.
Dissem.
Des., 2(2):65?90.L.
Yi, B. Liu, and X. Li.
2003.
Eliminating noisy in-formation in web pages for data mining.
In KDD ?03:Proceedings of the ninth ACM SIGKDD internationalconference on Knowledge discovery and data mining,pages 296?305, New York, NY, USA.
ACM.M.
Zimmerman, D. Hakkani-Tur, J. Fung, N. Mirghafori,L.
Gottlieb, E. Shriberg, and Y. Liu.
2006.
TheICSI+ multilingual sentence segmentation system.
InInterspeech, pages 117?120, Pittsburgh, Pennsylvania,September.I.
Zitouni and R. Florian.
2008.
Mention detectioncrossing the language barrier.
In Proceedings ofEMNLP?08, Honolulu, Hawaii, October.I.
Zitouni and R. Florian.
2009.
Cross-language informa-tion propagation for Arabic mention detection.
ACMTransactions on Asian Language Information Process-ing (TALIP), 8(4):1?21.345
