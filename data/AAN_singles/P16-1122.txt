Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1288?1297,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsInner Attention based Recurrent Neural Networks for Answer SelectionBingning Wang, Kang Liu, Jun ZhaoNational Laboratory of Pattern Recognition, Institute of AutomationChinese Academy of Sciences, Beijing, China{bingning.wang, kliu, jzhao}@nlpr.ia.ac.cnAbstractAttention based recurrent neural networkshave shown advantages in representingnatural language sentences (Hermann etal., 2015; Rockt?aschel et al, 2015; Tanet al, 2015).
Based on recurrent neuralnetworks (RNN), external attention infor-mation was added to hidden representa-tions to get an attentive sentence represen-tation.
Despite the improvement over non-attentive models, the attention mechanismunder RNN is not well studied.
In thiswork, we analyze the deficiency of tradi-tional attention based RNN models quanti-tatively and qualitatively.
Then we presentthree new RNN models that add attentioninformation before RNN hidden represen-tation, which shows advantage in repre-senting sentence and achieves new state-of-art results in answer selection task.1 IntroductionAnswer selection (AS) is a crucial subtask of theopen domain question answering (QA) problem.Given a question, the goal is to choose the an-swer from a set of pre-selected sentences (Heilmanand Smith, 2010; Yao et al, 2013).
TraditionalAS models are based on lexical features such asparsing tree edit distance.
Neural networks basedmodels are proposed to represent the meaning ofa sentence in a vector space and then comparethe question and answer candidates in this hiddenspace (Wang and Nyberg, 2015; Feng et al, 2015),which have shown great success in AS.
However,these models represent the question and sentenceseparately, which may ignore the information sub-ject to the question when representing the answer.For example, given a candidate answer:Michael Jordan abruptly retired from ChicagoBulls before the beginning of the 1993-94 NBAseason to pursue a career in baseball.For a question: When did Michael Jordanretired from NBA?
we should focus on the be-ginning of the 1993-94 in the sentence; how-ever, when we were asked: Which sports doesMichael Jordan participates after his retire-ment from NBA?
we should pay more attentionto pursue a career in baseball.Recent years, attention based models are pro-posed in light of this purpose and have showngreat success in many NLP tasks such as ma-chine translation (Bahdanau et al, 2014; Sutskeveret al, 2014), question answering (Sukhbaatar etal., 2015) and recognizing textual entailments(Rockt?aschel et al, 2015).
When building the rep-resentation of a sentence, some attention informa-tion is added to the hidden state.
For example,in attention based recurrent neural networks mod-els (Bahdanau et al, 2014) each time-step hiddenrepresentation is weighted by attention.
Inspiredby the attention mechanism, some attention-basedRNN answer selection models have been proposed(Tan et al, 2015) in which the attention when com-puting answer representation is from question rep-resentation.However, in the RNN architecture, at each timestep a word is added and the hidden state is up-dated recurrently, so those hidden states near theend of the sentence are expected to capture moreinformation1.
Consequently, after adding the at-tention information to the time sequence hiddenrepresentations, the near-the-end hidden variableswill be more attended due to their comparativelyabundant semantic accumulation, which may re-sult in a biased attentive weight towards the latercoming words in RNN.In this work, we analyze this attention bias1so in many previous RNN-based model use the last hid-den variable as the whole sentence representation1288problem qualitatively and quantitatively, and thenpropose three new models to solve this prob-lem.
Different from previous attention based RNNmodels in which attention information is added af-ter RNN computation, we add the attention be-fore computing the sentence representation.
Con-cretely, the first one uses the question attention toadjust word representation (i.e.
word embedding)in the answer directly, and then we use RNN tomodel the attentive word sequence.
However, thismodel attends a sentence word by word which mayignore the relation between words.
For example,if we were asked: what is his favorite food?
oneanswer candidate is: He likes hot dog best.
hotor dog may be not relate to the question by itself,but they are informative as a whole in the context.So we propose the second model in which everyword representation in answer is impacted by notonly question attention but also the context repre-sentation of the word (i.e.
the last hidden state).In our last model, inspired by previous work onadding gate into inner activation of RNN to con-trol the long and short term information flow, weembed the attention to the inner activation gate ofRNN to influence the computation of RNN hid-den representation.
In addition, inspired by recentwork called Occam?s Gate in which the activationof input units are penalized to be as less as pos-sible, we add regulation to the summation of theattention weights to impose sparsity.Overall, in this work we make three contribu-tions: (1) We analyze the attention bias problemin traditional attention based RNN models.
(2) Wepropose three inner attention based RNN modelsand achieve new state-of-the-art results in answerselection.
(3) We use Occam?s Razor to regulatethe attention weights which shows advantage inlong sentence representation.2 Related WorkRecent years, many deep learning framework hasbeen developed to model the text in a vector space,and then use the embedded representations in thisspace for machine learning tasks.
There are manyneural networks architectures for this represen-tation such as convolutional neural networks(Yinet al, 2015), recursive neural networks(Socher etal., 2013) and recurrent neural networks(Mikolovet al, 2011).
In this work we propose InnerAttention based RNN (IARNN) for answer selec-tion, and there are two main works which we arerelated to.2.1 Attention based ModelsMany recent works show that attention techniquescan improve the performance of machine learningmodels (Mnih et al, 2014; Zheng et al, 2015).
Inattention based models, one representation is builtwith attention (or supervision) from other repre-sentation.
Weston et al(2014) propose a neuralnetworks based model called Memory Networkswhich uses an external memory to store the knowl-edge and the memory are read and written on thefly with respect to the attention, and these attentivememory are combined for inference.
Since then,many variants have been proposed to solve ques-tion answering problems (Sukhbaatar et al, 2015;Kumar et al, 2015).
Hermann (2015) and manyother researchers (Tan et al, 2015; Rockt?aschel etal., 2015) try to introduce the attention mechanisminto the LSTM-RNN architecture.
RNN modelsthe input sequence word-by-word and updates itshidden variable recurrently.
Compared with CNN,RNN is more capable of exploiting long-distancesequential information.
In attention based RNNmodels, after computing each time step hiddenrepresentation, attention information is added toweight each hidden representation, then the hid-den states are combined with respect to that weightto obtain the sentence (or document) representa-tion.
Commonly there are two ways to get atten-tion from source sentence, either by the whole sen-tence representation (which they call attentive) orword by word attention (called impatient).2.2 Answer SelectionAnswer selection is a sub-task of QA and manyother tasks such as machine comprehension.Given a question and a set of candidate sentences,one should choose the best sentence from a can-didate sentence set that can answer the question.Previous works usually stuck in employing fea-ture engineering, linguistic tools, or external re-sources.
For example, Yih et al (2013) use se-mantic features from WordNet to enhance lexicalfeatures.
Wang and Manning (2007) try to com-pare the question and answer sentence by theirsyntactical matching in parse trees.
Heilman andSmith (Heilman and Smith, 2010) try to fulfill thematching using minimal edit sequences betweentheir dependency parse trees.
Severyn and Mos-chitti (2013) automate the extraction of discrimi-native tree-edit features over parsing trees.1289While these methods show effectiveness, theymight suffer from the availability of additional re-sources and errors of many NLP tools such asdependency parsing.
Recently there are manyworks use deep learning architecture to representthe question and answer in a same hidden space,and then the task can be converted into a classi-fication or learning-to-rank problem (Feng et al,2015; Wang and Nyberg, 2015).
With the develop-ment of attention mechanism, Tan et.al(2015) pro-pose an attention-based RNN models which intro-duce question attention to answer representation.3 Traditional Attention based RNNModels and Their DeficiencyThe attention-based models introduce the atten-tion information into the representation process.In answer selection, given a question Q ={q1, q2, q3, ..., qn} where qiis i-th word, n is thequestion length, we can compute its representationin RNN architecture as follows:X = D[q1, q2, ..., qn]ht= ?
(Wihxt+ Whhht?1+ bh)yt= ?
(Whoht+ bo)(1)where D is an embedding matrix that projectsword to its embedding space in Rd; Wih, Whh,Whoare weight matrices and bh, boare bias vec-tors; ?
is active function such as tanh.
Usually wecan ignore the output variables and use the hiddenvariables.
After recurrent process, the last hiddenvariable hnor all hidden states average1n?nt=1htis adopted as the question representation rq.When modeling the candidate answer sentencewith length m:S = {s1, s2, s3, ..., sm} in attentionbased RNN model,instead of using the last hidden state or averagehidden states, we use attentive hidden states thatare weighted by rq:Ha= [ha(1),ha(2), ...,ha(m)]st?
fattention(rq,ha(t))?ha(t) = ha(t)stra=m?t=1?ha(t)(2)where ha(t) is hidden state of the answer at timet.
In many previous work (Hermann et al, 2015;Rockt?aschel et al, 2015; Tan et al, 2015), the at-QuestionRNNAnswerave|maxrq Attention SUM racosineFigure 1: Traditional attention based RNN answerselection model.
Dark blue rectangles representhidden virable, ?
means gate opperation.tention function fattentionwas computed as:m(t) = tanh(Whmha(t) + Wqmrq)fattention(rq,ha(t)) = exp(wTmsm(t))(3)Whmand Wqmare attentive weight matrices andwmsis attentive weight vector.
So we can ex-pect that the candidate answer sentence represen-tation ramay be represented in a question-guidedway: when its hidden state ha(t) is irrelevant tothe question (determined by attention weight st),it will take less part in the final representation; butwhen this hidden state is relavent to the question,it will contribute more in representing ra.
We callthis type of attention based RNN model OARNNwhich stands for Outer Attention based RNN mod-els because this kind of model adds attention in-formation outside the RNN hidden representationcomputing process.
An illustration of traditionalattention-based RNN model is in Figure 1.However, we know in the RNN architecture, theinput words are processed in time sequence andthe hidden states are updated recurrently, so thecurrent hidden state htis supposed to contain allthe information up to time t, when we add ques-tion attention information, aiming at finding theuseful part of the sentence, these near-the-end hid-den states are prone to be selected because theycontains much more information about the wholesentence.
In other word, if the question pays atten-tion to the hidden states at time t , then it shouldalso pay attention to those hidden states after t(i.e {ht?|t?> t}) as they contain the informationat least as much as ht, but in answer selectionfor a specific candidate answer, the useful partsto answer the question may be located anywherein a sentence, so the attention should also dis-tribute uniformly around the sentence.
Traditionalattention-based RNN models under attention afterrepresentation mechanism may cause the attentionto bias towards the later coming hidden states.
Wewill analyze this attention bias problem quantita-1290tively in the experiments.4 Inner Attention based RecurrentNeural NetworksIn order to solve the attention bias problem, wepropose an intuition:Attention before representationInstead of adding attention information after en-coding the answer by RNN, we add attention be-fore computing the RNN hidden representations.Based on this intuition, we propose three inner at-tention based RNN models detailed below.4.1 IARNN-WORDAs attention mechanism aims at finding useful partof a sentence, the first model applies the aboveintuition directly.
Instead of using the originalanswer words to the RNN model, we weight thewords representation according to question atten-tion as follows:?t= ?
(rTqMqixt)?xt= ?t?
xt(4)where Mqiis an attention matrix to transform aquestion representaion into the word embeddingspace.
Then we use the dot value to determine thequestion attention strength, ?
is sigmoid functionto normalize the weight ?tbetween 0 and 1.The above attention process can be understoodas sentence distillation where the input words aredistilled (or filtered) by question attention.
Then,we can represent the whole sentence based on thisdistilled input using traditional RNN model.
Inthis work, we use GRU instead of LSTM as build-ing block for RNN because it has shown advan-tages in many tasks and has comparatively lessparameter(Jozefowicz et al, 2015) which is for-mulated as follows:zt= ?
(Wxz?xt+ Whzht?1)ft= ?
(Wxf?xt+ Whfht?1)?ht= tanh(Wxh?xt+ Whh(ftht?1))ht= (1?
zt) ht?1+ zt?ht(5)where Wxz,Whz,Wxf,Whh,Wxhare weightmatrices and  stands for element-wise multipli-cation.
Finally, we get candidate answer represen-tation by average pooling all the hidden state ht.we call this model IARNN-WORD as the atten-tion is paid to the original input words.
This modelis shown in Figure 2.rqAttentionraave??????
?GRUFigure 2: IARNN-WORD architecture.
rqis ques-tion representation.rqAttentionraave??????
?GRU h 0Figure 3: IARNN-CONTEXT architecture forbuilding candidate answer sentence representa-tion.
h0is added for completeness.4.2 IARNN-CONTEXTIABRNN-WORD attend input word embeddingdirectly.
However, the answer sentence may con-sist of consecutive words that are related to thequestion, and a word may be irrelevant to ques-tion by itself but relevant in the context of answersentence.So the above word by word attention mech-anism may not capture the relationship betweenmultiple words.
In order to import contextual in-formation into attention process, we modify the at-tention weights in Equation 4 with additional con-text information:wC(t) = Mhcht?1+ Mqcrq?tC= ?
(wTC(t)xt)?xt= ?tC?
xt(6)where we use ht?1as context, Mhcand Mqcareattention weight matrices, wC(t) is the attentionrepresentation which consists of both question andword context information.
This additional con-text attention endows our model to capture rele-vant part in longer text span.
We show this modelin Figure 3.12914.3 IARNN-GATEInspired by the previous work of LSTM (Hochre-iter and Schmidhuber, 1997) on solving the gra-dient exploding problem in RNN and recent workon building distributed word representation withtopic information(Ghosh et al, 2016), instead ofadding attention information to the original input,we can apply attention deeper to the GRU inneractivation (i.e ztand ft).
Because these inner ac-tivation units control the flow of the informationwithin the hidden stage and enables informationto pass long distance in a sentence, we add atten-tion information to these active gates to influencethe hidden representation as follows:zt= ?
(Wxzxt+ Whzht?1+Mqzrq)ft= ?
(Wxfxt+ Whfht?1+Mqfrq)?ht= tanh(Wxhxt+ Whh(ftht?1))ht= (1?
zt) ht?1+ zt?ht(7)where Mqzand Mhzare attention weight matrices.In this way, the update and forget units in GRU canfocus on not only long and short term memory butalso the attention information from the question.The architecture is shown in Figure 4.4.4 IARNN-OCCAMIn answer selection, the answer sentence mayonly contain small number of words that are re-lated to the question.
In IARNN-WORD andIARNN-CONTEXT, we calculate each word at-tention weight without considering total weights.Similar with Raiman(2015) who adds regulationto the input gate, we punish the summation of theattention weights to enforce sparsity.
This is anapplication of Occam?s Razor: Among the wholewords set, we choose those with fewest numberthat can represent the sentence.
However, assign-ing a pre-defined hyper-parameter for this regula-tion2is not an ideal way because it punishes allquestion attention weights with same strength.
Fordifferent questions there may be different numberof snippets in candidate answer that are required.For example, when the question type is When orWho, answer sentence may only contains a littlerelavant words so we should impose more sparsityon the summation of the attention.
But when the2For example, in many machine learning problem theoriginal objective sometimes followed with a L1or L2regulation with hyper-parameter ?1 or ?2 to control thetradeoff between the original objective J and the sparsitycriterion:J?= J + (?1|?2)?
(L1|L2norm)rqh txth t - 1xth t - 1z t f t??
?1 -AttentionG RUFigure 4: IABRNN-GATE architecture.
We showone time step GRU inner state process within theblue dotted line.question type is Why or How, there may be muchmore words on the sentence that are relevant tothe question so we should set the regulation valuesmall accordingly.
In this work, this attention reg-ulation is added as follows: for the specific ques-tion Qiand its representation riq, we use a vectorwqpto project it into scalar value nip, and then weadd it into the original objective Jias follows:nip= max{wTqpriq, ?q}J?i= Ji+ nipmc?t=1?it(8)where ?itis attention weights in Equation 4 andEquation 6.
?qis a small positive hyper-parameter.It needs to mention that we do not regulateIARNN-GATE because the attention has been em-bedded to gate activation.5 Experiments5.1 Quantify Traditional Attention basedModel Bias ProblemIn order to quantify the outer attention based RNNmodel?s attention bias problem in Section 3, webuild an outer attention based model similar withTan (2015).
First of all, for the question we buildits representation by averaging its hidden states inLSTM, then we build the candidate answer sen-tence representation in an attentive way introducedin Section 3.
Next we use the cosine similarity tocompare question and answer representation simi-larity.
Finally, we adopt max-margin hinge loss asobjective:L = max{0,M ?
cosine(rq, ra+)+ cosine(rq, ra?
)}(9)1292position in a sentence0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000attentionweight#10-40.80.911.11.21.31.4Bi-directional OARNNBi-directional IARNN-WORDstart endFigure 5: One directional OARNN attention dis-tribution, the horizontal axis is position of wordin a sentence that has been normalized from 1 to10000.position in a sentence0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000attentionweight#10-40.80.911.11.21.31.4Bi-directional OARNNBi-directional IARNN-WORDstart endFigure 6: Bi-directional OARNN attention distri-bution, the horizontal axis is the postion of theword in a sentence that has been normalized from1 to 10000.where a+is ground truth answer candidate anda?stands for negative one, the scalar M is a pre-defined margin.
When training result saturates af-ter 50 epoches, we get the attention weight distri-bution (i.e.
sqin Equation 2).
The experiment isconducted on two answer selection datasets: Wik-iQA (Yang et al, 2015) and TrecQA (Wang et al,2007).
The normalized attention weights is re-ported in Figure 5.However, the above model use only forwardLSTM to build hidden state representation, the at-tention bias problem may attribute to the biasedanswer distribution: the useful part of the an-swer to the question sometimes may located at theend of the sentence.
So we try OARNN in bidi-rectional architecture, where the forward LSTMand backward LSTM are concatenated for hiddenrepresentation, The bidirectional attention basedLSTM attention distribution is shown in Figure 6.Analysis: As is shown in Figure 5 and 6, forone-directional OARNN, as we move from begin-ning to the end in a sentence, the question atten-tion gains continuously; when we use bidirectionalOARNN, the hidden representations near two endsof a sentence get more attention.
This is consistentwith our assumption that for a hidden representa-tion in RNN, the closer to the end of a sentence,the more attention it should drawn from question.But the relevant part may be located anywhere in aanswer.
As a result, when the sample size is largeenough3, the attention weight should be unformlydistributed.
The traditional attention after repre-sentation style RNN may suffer from the biasedattention problem.
Our IARNN models are freefrom this problem and distribute nearly uniform(orange line) in a sentence.5.2 IARNN evaluationCommon Setup: We use the off-the-shelf 100-dimension word embeddings from word2vec4, andinitiate all weights and attention matrices by fixingtheir largest singular values to 1 (Pascanu et al,2013).
IARNN-OCCAM base regulation hyper-parameter ?qis set to 0.05, we addL2penalty witha coefficient of 10?5.
Dropout (Srivastava et al,2014) is further applied to every parameters withprobability 30%.
We use Adadelta(Zeiler, 2012)with ?
= 0.90 to update parameters.We choose three datasets for evaluation: Insur-anceQA, WikiQA and TREC-QA.
These datasetscontain questions from different domains.
Table 1presents some statistics about these datasets.
Weadopt a max-margin hinge loss as training objec-tive.
The results are reported in terms of MAP andMRR in WikiQA and TREC-QA and accuracy inInsuranceQA.We use bidirectional GRU for all models.
Weshare the GRU parameter between question andanswer which has shown significant improvementon performance and convergency rate (Tan et al,2015; Feng et al, 2015).There are two common baseline systems forabove three datasets:?
GRU: A non-attentive GRU-RNN that mod-els the question and answer separately.?
OARNN: Outer attention-based RNN mod-els (OARNN) with GRU which is detailed inSection 5.1.WikiQA (Yang et al, 2015) is a recentlyreleased open-domain question answering310000 for WikiQA and 5000 for TrecQA in experiment.4https://code.google.com/archive/p/word2vec/1293Dataset(train / test / dev) InsuranceQA WikiQA TREC-QA# of questions 12887 / 1800x2 /1000 873 / 243 / 126 78 / 68 / 65# of sentences 24981(ALL) 20360 / 6165 / 2733 5919 / 1442 / 1117Ave length of question 7.16 7.16 / 7.26 / 7.23 11.39 / 8.63 / 8.00Ave length of sentence 49.5 25.29 / 24.59 / 24.59 30.39 / 25.61 / 24.9Table 1: The statistics of three answer selection datasets.
For the TREC-QA, we use the cleaned datasetthat has been edit by human.
For WikiQA and TREC-QA we remove all the questions that has no rightor wrong answers.System MAP MRR(Yang et al, 2015) 0.652 0.6652(Yin et al, 2015) 0.6921 0.7108(Santos et al, 2016) 0.6886 0.6957GRU 0.6581 0.6691OARNN 0.6881 0.7013IARNN-word 0.7098 0.7234IARNN-Occam(word) 0.7121 0.7318IARNN-context 0.7182 0.7339IARNN-Occam(context) 0.7341 0.7418IARNN-Gate 0.7258 0.7394Table 2: Performances on WikiQAdataset in which all answers are collected fromWikipedia.
In addition to the original (ques-tion,positive,negative) triplets, we randomlyselect a bunch of negative answer candidatesfrom answer sentence pool and finally we get arelatively abundant 50,298 triplets.
We use cosinesimilarity to compare the question and candidateanswer sentence.
The hidden variable?s lengthis set to 165 and batch size is set to 1.
We usesigmoid as GRU inner active function, we keepword embedding fixed during training.
Margin Mwas set to 0.15 which is tuned in the developmentset.
We adopt three additional baseline systemsapplied to WikiQA: (1) A bigram CNN modelswith average pooling(Yang et al, 2015).
(2)An attention-based CNN model which uses aninteractive attention matrix for both question andanswer(Yin et al, 2015)5(3) An attention basedCNN models which builds the attention matrixafter sentence representation(Santos et al, 2016).The result is shown in Table 2.InsuranceQA (Feng et al, 2015) is a domainspecific answer selection dataset in which all ques-tions is related to insurance.
Its vocabulary sizeis comparatively small (22,353), we set the batchsize to 16 and the hidden variable size to 145,hinge loss margin M is adjusted to 0.12 by evalu-ation behavior.
Word embeddings are also learnedduring training.
We adopt the Geometric mean ofEuclidean and Sigmoid Dot (GESD) proposed in(Feng et al, 2015) to measure the similarity be-5In their experiment some extra linguistic features wasalso added for better performance.System Dev Test1 Test2(Feng et al, 2015) 65.4 65.3 61.0(Santos et al, 2016) 66.8 67.8 60.3GRU 59.4 53.2 58.1OARNN 65.4 66.1 60.2IARNN-word 67.2125 67.0651 61.5896IARNN-Occam(word) 69.9130 69.5923 63.7317IARNN-context 67.1025 66.7211 63.0656IARNN-Occam(context) 69.1125 68.8651 65.1396IARNN-Gate 69.9812 70.1128 62.7965Table 3: Experiment result in InsuranceQA, (Fenget al, 2015) is a CNN architecture without atten-tion mechanism.System MAP MRR(Wang and Nyberg, 2015) ?
0.7134 0.7913(Wang and Ittycheriah, 2015) ?
0.7460 0.8200(Santos et al, 2016) ?
0.7530 0.8511GRU 0.6487 0.6991OARNN 0.6887 0.7491IARNN-word 0.7098 0.7757IARNN-Occam(word) 0.7162 0.7916IARNN-context 0.7232 0.8069IARNN-Occam(context) 0.7272 0.8191IARNN-Gate 0.7369 0.8208Table 4: Result of different systems in Trec-QA.
(Wang and Ittycheriah, 2015) propose a ques-tion similarity model to extract features from wordalignment between two questions which is suitableto FAQ based QA.
It needs to mention that the sys-tem marked with ?
are learned on TREC-QA orig-inal full training data.tween two representations:GESD(x, y) =11 + ||x?
y||?11 + exp(??
(xyT+ c))(10)which shows advantage over cosine similarity inexperiments.We report accuracy instead of MAP/MRR be-cause one question only has one right answers inInsuranceQA.
The result is shown in Table 3.TREC-QA was created by Wang et al(2007)based on Text REtrieval Conference (TREC) QAtrack (8-13) data.
The size of hidden variable wasset to 80, M was set to 0.1.
This dataset is com-paratively small so we set word embedding vector1294Q: how old was monica lewinsky during the affair ?Monica Samille Lewinsky ( born July 23 , 1973 ) is an American woman with whom United States President Bill Clinton admitted to having had an ``  improper relations hip '' while she worked at the White House in 1995 and 1996 .Monica Samille Lewinsky ( born July 23 , 1973 ) is an American woman with whom United States President Bill Clinton admitted to having had an `` improper relationship '' while she worked at the White House in 1995 and 1996 .OARNN:IARNN-CONTEXT:Figure 7: An example demonstrates the advantage of IARNN in capturing the informed part of a sentencecompared with OARNN.The effects of relativistic self focusing and preformed plasma channel guiding are analyzed.Q: what did gurgen askaryan research when he entered the moscow state university?IARNN - CONTEXT:IARNN - WORD:Answer:Figure 8: An example illustrates the IARNN-CONTEXT could attend the consecutive words in a sen-tence.size to 50 and update it during training.
It needs tomention that we do not use the original TREC-QAtraining data but the smaller one which has beenedited by human.
The result is shown in Table 4.6 Result and AnalysisWe can see from the result tables that the atten-tion based RNN models achieve better results thanthe non-attention RNN models (GRU).
OARNNand IARNN beat the non-attentive GRU in ev-ery datasets by a large margin, which proves theimportance of attention mechanism in represent-ing answer sentence in AS.
For the non-attentivemodels, the fixed width of the hidden vectors isa bottleneck for interactive information flow, sothe informative part of the question could onlypropagate through the similarity score which isblurred for the answer representation to be prop-erly learned.
But in attention based models, thequestion attention information is introduced to in-fluence the answer sentence representation explic-itly, in this way we can improve sentence repre-sentation for the specific target (or topic (Ghosh etal., 2016)).The inner attention RNN models outperformouter attention model in three datasets, this iscorresponds to our intuition that the bias atten-tion problem in OARNN may cause a biasd sen-tence representation.
An example of the attentionheatmap is shown in Figure7.
To answer the ques-tion, we should focus on ?born July 23 , 1973?which is located at the beginning of the sentence.But in OARNN, the attention is biases towards thelast few last words in the answer.
In IARNN-CONTEXT, the attention is paid to the relevantpart and thus results in a more relevant representa-tion.The attention with context information couldalso improves the result, we can see that IARNN-CONTEXT and IARNN-GATE outperformIARNN-WORD in three experiments.
IARNN-WORD may ignore the importance of somewords because it attends answer word by word,for example in Figure8, the specific word self orfocusing may not be related to the question byitself, but their combination and the previous wordrelativistic is very informative for answering thequestion.
In IARNN-CONTEXT we add attentioninformation dynamically in RNN process, thus itcould capture the relationship between word andits context.In general, we can see from table3-5 that theIARNN-GATE outperforms IARNN-CONTEXTand IARNN-WORD.
In IARNN-WORD andIARNN-CONTEXT, the attention is added to im-pact each word representation, but the recur-rent process of updating RNN hidden state rep-resentations are not influenced.
IARNN-GATEembeds the attention into RNN inner activa-tion, the attentive activation gate are more ca-pable of controlling the attention information inRNN.
This enlights an important future work:we could add attention information as an individ-ual activation gate, and use this additional gateto control attention information flow in RNN.The regulation of the attention weights (Oc-cam?s attention) could also improve the represen-tation.
We also conduct an experiment on Wik-iQA (training process) to measure the Occam?s at-tention regulation on different type of questions.We use rules to classify question into 6 types1295who why how when where what0.131 0.065 0.052 0.103 0.118 0.08900 .
0 20 .
0 40 .
0 60 .
0 80.10 .
1 20 .
1 4whowhyhowwhenwherewhatFigure 9: The Occam?s attention regulation on dif-ferent types of question.(i.e.
who,why,how,when,where,what), and each ofthem has the same number of samples to avoiddata imbalance.
We report the Occam?m regula-tion (nipin Equation.8) in Figure 9.
As we can seefrom the radar graph, who and where are regulizedseverely compared with other types of question,this is correspond to their comparetively less infor-mation in the answer candidate to answer the ques-tion.
This emphasize that different types questionshould impose different amount of regulation onits candidate answers.
The experiment result onthree AS datasets shows that the improvement ofOccam?s attention is significant in WikiQA and in-suranceQA.
Because most of the sentence are rel-atively long in these two datasets, and the longerthe sentence, the more noise it may contain, sowe should punish the summation of the attentionweights to remove some irrelevant parts.
Ourquestion-specific Occam?s attention punishes thesummation of attention and thus achieves a bet-ter result for both IARNN-WORD and IARNN-CONTEXT.7 Conclusion and Future WorkIn this work we present some variants of tradi-tional attention-based RNN models with GRU.The key idea is attention before representation.We analyze the deficiency of traditional outerattention-based RNN models qualitatively andquantitatively.
We propose three models where at-tention is embedded into representation process.Occam?s Razor is further implemented to this at-tention for better representation.
Our results onanswer selection demonstrate that the inner atten-tion outperforms the outer attention in RNN.
Ourmodels can be further extended to other NLP taskssuch as recognizing textual entailments where at-tention mechanism is important for sentence rep-resentation.
In the future we plan to apply ourinner-attention intuition to other neural networkssuch as CNN or multi-layer perceptron.AcknowledgmentsThe work was supported by the Natural ScienceFoundation of China (No.61533018), the NationalHigh Technology Development 863 Program ofChina (No.2015AA015405) and the National Nat-ural Science Foundation of China (No.61272332).And this research work was also supported byGoogle through focused research awards program.ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
arXiv preprintarXiv:1409.0473.Minwei Feng, Bing Xiang, Michael R Glass, LidanWang, and Bowen Zhou.
2015.
Applying deeplearning to answer selection: A study and an opentask.
IEEE Automatic Speech Recognition and Un-derstanding Workshop (ASRU).Shalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy,Tom Dean, and Larry Heck.
2016.
Contextuallstm (clstm) models for large scale nlp tasks.
arXivpreprint arXiv:1602.06291.Michael Heilman and Noah A Smith.
2010.
Tree editmodels for recognizing textual entailments, para-phrases, and answers to questions.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 1011?1019.Association for Computational Linguistics.Karl Moritz Hermann, Tomas Kocisky, EdwardGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-leyman, and Phil Blunsom.
2015.
Teaching ma-chines to read and comprehend.
In Advances in Neu-ral Information Processing Systems, pages 1684?1692.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Rafal Jozefowicz, Wojciech Zaremba, and IlyaSutskever.
2015.
An empirical exploration of re-current network architectures.
In Proceedings of the32nd International Conference on Machine Learn-ing (ICML-15), pages 2342?2350.Ankit Kumar, Ozan Irsoy, Jonathan Su, James Brad-bury, Robert English, Brian Pierce, Peter Ondruska,Ishaan Gulrajani, and Richard Socher.
2015.Ask me anything: Dynamic memory networksfor natural language processing.
arXiv preprintarXiv:1506.07285.1296Tom?a?s Mikolov, Stefan Kombrink, Luk?a?s Burget,Jan Honza?Cernock`y, and Sanjeev Khudanpur.2011.
Extensions of recurrent neural network lan-guage model.
In Acoustics, Speech and Signal Pro-cessing (ICASSP), 2011 IEEE International Confer-ence on, pages 5528?5531.
IEEE.Volodymyr Mnih, Nicolas Heess, Alex Graves, et al2014.
Recurrent models of visual attention.
In Ad-vances in Neural Information Processing Systems,pages 2204?2212.Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.2013.
On the difficulty of training recurrent neuralnetworks.
In Proceedings of the 30th InternationalConference on Machine Learning (ICML-13), pages1310?1318.Jonathan Raiman and Szymon Sidor.
2015.
Occam?sgates.
arXiv preprint arXiv:1506.08251.Tim Rockt?aschel, Edward Grefenstette, Karl MoritzHermann, Tom?a?s Ko?cisk`y, and Phil Blunsom.
2015.Reasoning about entailment with neural attention.arXiv preprint arXiv:1509.06664.Cicero dos Santos, Ming Tan, Bing Xiang, and BowenZhou.
2016.
Attentive pooling networks.
arXivpreprint arXiv:1602.03609.Aliaksei Severyn and Alessandro Moschitti.
2013.
Au-tomatic feature engineering for answer selection andextraction.
In EMNLP, pages 458?467.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of the conference onempirical methods in natural language processing(EMNLP), volume 1631, page 1642.
Citeseer.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
The Journal of Machine LearningResearch, 15(1):1929?1958.Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al2015.
End-to-end memory networks.
In Advancesin Neural Information Processing Systems, pages2431?2439.Ilya Sutskever, Oriol Vinyals, and Quoc V Le.
2014.Sequence to sequence learning with neural net-works.
In Advances in neural information process-ing systems, pages 3104?3112.Ming Tan, Bing Xiang, and Bowen Zhou.
2015.
Lstm-based deep learning models for non-factoid answerselection.
arXiv preprint arXiv:1511.04108.Zhiguo Wang and Abraham Ittycheriah.
2015.
Faq-based question answering via word alignment.arXiv preprint arXiv:1507.02628.Di Wang and Eric Nyberg.
2015.
A long short-term memory model for answer sentence selectionin question answering.
ACL, July.Mengqiu Wang, Noah A Smith, and Teruko Mita-mura.
2007.
What is the jeopardy model?
a quasi-synchronous grammar for qa.
In EMNLP-CoNLL,volume 7, pages 22?32.Jason Weston, Sumit Chopra, and Antoine Bor-des.
2014.
Memory networks.
arXiv preprintarXiv:1410.3916.Yi Yang, Wen-tau Yih, and Christopher Meek.
2015.Wikiqa: A challenge dataset for open-domain ques-tion answering.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing.
Citeseer.Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark.
2013.
Answer extractionas sequence tagging with tree edit distance.
In HLT-NAACL, pages 858?867.
Citeseer.Wen-tau Yih, Ming-Wei Chang, Christopher Meek, andAndrzej Pastusiak.
2013.
Question answering usingenhanced lexical semantic models.
In Proceedingsof ACL.Wenpeng Yin, Hinrich Sch?utze, Bing Xiang, andBowen Zhou.
2015.
Abcnn: Attention-based con-volutional neural network for modeling sentencepairs.
arXiv preprint arXiv:1512.05193.Matthew D Zeiler.
2012.
Adadelta: an adaptive learn-ing rate method.
arXiv preprint arXiv:1212.5701.Yin Zheng, Richard S Zemel, Yu-Jin Zhang, and HugoLarochelle.
2015.
A neural autoregressive approachto attention-based recognition.
International Jour-nal of Computer Vision, 113(1):67?79.1297
