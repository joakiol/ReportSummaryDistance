How to thematically segment texts by using lexical cohesion?Ol iv ie r  Fer re tLIMSI-CNRSBP 133F-91403 Orsay Cedex, FRANCEferret~limsi.
frAbst rac tThis article outlines a quantitative methodfor segmenting texts into thematically coherentunits.
This method relies on a network of lexicalcollocations to compute the thematic oherenceof the different parts of a text from the lexicalcohesiveness of their words.
We also present heresults of an experiment about locating bound-aries between a series of concatened texts.1 In t roduct ionSeveral quantitative methods exist for themati-cally segmenting texts.
Most of them are basedon the following assumption: the thematic o-herence of a text segment finds expression atthe lexical level.
Hearst (1997) and Nomoto andNitta (1994) detect his coherence through pat-terns of lexical cooccurrence.
Morris and Hirst(1991) and Kozima (1993) find topic boundariesin the texts by using lexical cohesion.
The firstmethods are applied to texts, such as expositorytexts, whose vocabulary is often very specific.As a concept is always expressed by the sameword, word repetitions are thematically signifi-cant in these texts.
The use of lexical cohesionallows to bypass the problem set by texts, suchas narratives, in which a concept is often ex-pressed by different means.
However, this sec-ond approach requires knowledge about the co-hesion between words.
Morris and Hirst (1991)extract his knowledge from a thesaurus.
Koz-ima (1993) exploits a lexical network built froma machine readable dictionary (MRD).This article presents a method for thematicallysegmenting texts by using knowledge about lex-ical cohesion that has been automatically built.This knowledge takes the form of a network oflexical collocations.
We claim that this networkis as suitable as a thesaurus or a MRD for seg-menting texts.
Moreover, building it for a spe-cific domain or for another language is quick.2 MethodThe segmentation algorithm we propose in-cludes two steps.
First, a computation of thecohesion of the different parts of a text is doneby using a collocation etwork.
Second, we lo-cate the major breaks in this cohesion to detectthe thematic shifts and build segments.2.1 The  col locat ion networkOur collocation network has been built from24 months of the French Le Monde newspa-per.
The size of this corpus is around 39 mil-lion words.
The cohesion between words hasbeen evaluated with the mutual informationmeasure, as in (Church and Hanks, 1990).
Alarge window, 20 words wide, was used to takeinto account he thematic links.
The texts werepre-processed with the probabilistic POS taggerTreeTagger (Schmid, 1994) in order to keep onlythe lemmatized form of their content words, i.e.nouns, adjectives and verbs.
The resulting net-work is composed of approximatively 31 thou-sand words and 14 million relations.2.2 Computat ion  of  text  cohes ionAs in Kozima's work, a cohesion value is com-puted at each position of a window in a text (af-ter pre-processing) from the words in this win-dow.
The collocation network is used for de-termining how close together these words are.We suppose that if the words of the window arestrongly connected in the network, they belongto the same domain and so, the cohesion in thispart of text is high.
On the contrary, if they arenot very much linked together, we assume thatthe words of the window belong to two differentdomains.
It means that the window is locatedacross the transition from one topic to another.1481Pw2XO.21+Pw3XO.lO = 0.31 0.48 = Pw3XO.Ig+Pw4XO.130 Q +pw5xo'I70 1/\010 /i.,0,71.14 1.14 1.0 1.0 1.0wl w2 w3 w4 w50.31Q word from the collocation network (with its computed weight)O word from the text (with its computed weight1.0 ex.
for the first word: Pwl+PwlXO.14 = 1.14)0.14 link in the collocation network (with its cohesion value)Pwi initial weight of the word of the window wi (equal to 1.0 here}Figure 1: Computation of word weightIn practice, the cohesion inside the windowis evaluated by the sum of the weights of thewords in this window and the words selectedfrom the collocation etwork common to at leasttwo words of the window.
Selecting words fromthe network linked to those of the texts makesexplicit words related to the same topic as thetopic referred by the words in the window andproduces a more stable description of this topicwhen the window moves.As shown in Figure 1, each word w (from thewindow or from the network) is weighted by thesum of the contributions of all the words of thewindow it is linked to.
The contribution of sucha word is equal to its number of occurrences inthe window modulated by the cohesion measureassociated to its link with w. Thus, the more thewords belong to a same topic, the more they arelinked together and the higher their weights are.Finally, the value of the cohesion for one posi-tion of the window is the result of the followingweighted sum:coh(p) = Y~i sign(wi) .
wght(wi), withwght(wi), the resulting weight of the word wi,sign(wi), the significance of wi, i.e.
the normal-ized information of wi in the Le Monde corpus.Figure 2 shows the smoothed cohesion graph forten texts of the experiment.
Dotted lines aretext boundaries (see 3.1).2.3 Segment ing  the  cohes ion graphFirst, the graph is smoothed to more easily de-tect the main minima and maxima.
This op-eration is done again by moving a window onthe text.
At each position, the cohesion associ-!
t  ~:356252O15 ilO i 50 I00  150, i l :  ui:I 1 1 ~ l200 250 300 350Position of  the wordsFigure 2: The cohesion graph of a series of textsated to the window center is re-evaluated as themean of all the cohesion values in the window.After this smoothing, the derivative of thegraph is calculated to locate the maxima andthe minima.
We consider that a minimummarks a thematic shift.
So, a segment is char-acterized by the following sequence: minimum- maximum - minimum.
For making the delim-itation of the segments more precise, they arestopped before the next (or the previous) mini-mum if there is a brutal break of the graph andafter this, a very slow descent.
This is done bydetecting that the cohesion values fall under agiven percentage of the maximum value.3 Resu l tsA first qualitative valuation of the method hasbeen done with about 20 texts but without a for-mal protocol as in (Hearst, 1997).
The resultsof these tests are rather stable when parameterssuch as the size of the cohesion computing win-dow or the size of the smoothing window arechanged (from 9 to 21 words).
Generally, thebest results are obtained with a size of 19 wordsfor the first window and 11 for the second one.3.1 D iscover ing document  breaksIn order to have a more objective valuation, themethod has been applied to the "classical" taskof discovering boundaries between concatenedtexts.
Results are shown in Table 1.
As in(Hearst, 1997), boundaries found by the methodare weighted and sorted in decreasing order.Document breaks are supposed to be the bound-aries that have the highest weights.
For the firstNb boundaries, Nt is the number of boundariesthat match with document breaks.
Precision is148210 5 0.520 10 0.530 17 0.5838 19 0.540 20 0.550 24 0.4860 26 0.4367(Nbmax) 26 0.390.130.260.450.50.530.630.680.68Table 1: Results of the experimentgiven by Nt/Nb and recall, by Nt/N, where Nis the number of document breaks.
Our evalu-ation has been performed with 39 texts comingfrom the Le Monde newspaper, but not takenfrom the corpus used for building the collocationnetwork.
Each text was 80 words long on aver-age.
Each boundary, which is a minimum of thecohesion graph, was weighted by the sum of thedifferences between its value and the values ofthe two maxima around it, as in (Hearst, 1997).The match between a boundary and a documentbreak was accepted if the boundary was no fur-ther than 9 words (after pre-processing).Globally, our results are not as good as Hearst's(with 44 texts; Nb: 10, P: 0.8, R: 0.19; Nb: 70,P: 0.59, R: 0.95).
The first explanation for sucha difference is the fact that the two methods donot apply to the same kind of texts.
Hearstdoes not consider texts smaller than 10 sen-tences long.
All the texts of this evaluation areunder this limit.
In fact, our method, as Koz-ima's, is more convenient for closely trackingthematic evolutions than for detecting the ma-jor thematic shifts.
The second explanation forthis difference is related to the way the docu-ment breaks are found, as shown by the preci-sion values.
When Nb increases, precision de-creases as it generally does, but very slowly.The decrease actually becomes ignificant onlywhen Nb becomes larger than N. It means thatthe weights associated to the boundaries are notvery significant.
We have validated this hypoth-esis by changing the weighting policy of theboundaries without having significant changesin the results.One way for increasing the performance wouldbe to take as text boundary not the position of aminimum in the cohesion graph but the nearestsentence boundary from this position.4 Conc lus ion  and  fu ture  workWe have presented a method for segmentingtexts into thematically coherent units that re-lies on a collocation network.
This collocationnetwork is used to compute a cohesion value forthe different parts of a text.
Segmentation isthen done by analyzing the resulting cohesiongraph.
But such a numerical value is a roughcharacterization f the current opic.For future work we will build a more preciserepresentation f the current opic based on thewords selected from the network.
By computinga similarity measure between the representationof the current opic at one position of the win-dow and this representation at a further one,it will be possible to determine how themati-cally far two parts of a text are.
The minima ofthe measure will be used to detect he thematicshifts.
This new method is closer to Hearst'sthan the one presented above but it relies ona collocation network for finding relations be-tween two parts of a text instead of using theword recurrence.Re ferencesK.
W. Church and P. Hanks.
1990.
Wordassociation orms, mutual information, andlexicography.
Computational Linguistics,16(1):22-29.M.
A. Hearst.
1997.
Texttiling: Segmentingtext into multi-paragraph subtopic passages.Computational Linguistics, 23 (1) :33-64.H.
Kozima.
1993.
Text segmentation basedon similarity between words.
In 31th AnnualMeeting of the Association for ComputationalLinguistics (Student Session), pages 286-288.J.
Morris and G. Hirst.
1991.
Lexical cohesioncomputed by thesaural relations as an indi-cator of the structure of text.
ComputationalLinguistics, 17(1):21-48.T.
Nomoto and Y. Nitta.
1994.
A grammatico-statistical approach to discourse partitioning.In 15th International Conference on Compu-tational Linguistics (COLING), pages 1145-1150.H.
Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In InternationalConference on New Methods in LanguageProcessing.1483
