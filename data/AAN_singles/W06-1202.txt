Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 2?11,Sydney, July 2006. c?2006 Association for Computational LinguisticsMeasuring MWE Compositionality Using Semantic AnnotationScott S. L. Piao1, Paul Rayson1, Olga Mudraya2, Andrew Wilson2 and Roger Garside11Computing DepartmentLancaster UniversityLancaster, UK{s.piao, p.rayson, r.garside}@lancaster.ac.uk2Dept.
of Linguistics and ELLancaster UniversityLancaster, UK{o.mudraya, a.wilson}@lancaster.ac.ukAbstractThis paper reports on an experiment inwhich we explore a new approach to theautomatic measurement of multi-wordexpression (MWE) compositionality.
Wepropose an algorithm which ranks MWEsby their compositionality relative to asemantic field taxonomy based on theLancaster English semantic lexicon (Piaoet al, 2005a).
The semantic informationprovided by the lexicon is used for meas-uring the semantic distance between aMWE and its constituent words.
The al-gorithm is evaluated both on 89 manuallyranked MWEs and on McCarthy et als(2003) manually ranked phrasal verbs.We compared the output of our tool withhuman judgments using Spearman?srank-order correlation coefficient.
Ourevaluation shows that the automatic rank-ing of the majority of our test data(86.52%) has strong to moderate correla-tion with the manual ranking while widediscrepancy is found for a small numberof MWEs.
Our algorithm also obtained acorrelation of 0.3544 with manual rank-ing on McCarthy et als test data, whichis comparable or better than most of themeasures they tested.
This experimentdemonstrates that a semantic lexicon canassist in MWE compositionality meas-urement in addition to statistical algo-rithms.1 IntroductionOver the past few years, compositionality anddecomposability of MWEs have become impor-tant issues in NLP research.
Lin (1999) arguesthat ?non-compositional expressions need to betreated differently than other phrases in manystatistical or corpus?based NLP methods?.
Com-positionality means that ?the meaning of thewhole can be strictly predicted from the meaningof the parts?
(Manning & Sch?tze, 2000).
On theother hand, decomposability is a metric of thedegree to which the meaning of a MWE can beassigned to its parts (Nunberg, 1994; Riehemann,2001; Sag et al, 2002).
These two concepts areclosely related.
Venkatapathy and Joshi (2005)suggest that ?an expression is likely to be rela-tively more compositional if it is decomposable?.While there exist various definitions forMWEs, they are generally defined as cohesivelexemes that cross word boundaries (Sag et al,2002; Copestake et al, 2002; Calzolari et al,2002; Baldwin et al, 2003), which includenominal compounds, phrasal verbs, idioms, col-locations etc.
Compositionality is a critical crite-rion cutting across different definitions for ex-tracting and classifying MWEs.
While semanticsof certain types of MWEs are non-compositional,like idioms ?kick the bucket?
and ?hot dog?,some others can have highly compositional se-mantics like the expressions ?traffic light?
and?audio tape?.Automatic measurement of MWE composi-tionality can have a number of applications.
Oneof the often quoted applications is for machinetranslation (Melamed, 1997; Hwang & Sasaki,2005), in which non-compositional MWEs needspecial treatment.
For instance, the translation ofa highly compositional MWE can possibly beinferred from the translations of its constituentwords, whereas it is impossible for non-compositional MWEs, for which we need toidentify the translation equivalent for the MWEsas a whole.In this paper, we explore a new method ofautomatically estimating the compositionality ofMWEs using lexical semantic information,sourced from the Lancaster semantic lexicon(Piao et al, 2005a) that is employed in theUSAS1 tagger (Rayson et al, 2004).
This is a1 UCREL Semantic Analysis System2large lexical resource which contains nearly55,000 single-word entries and over 18,800MWE entries.
In this lexicon, each MWE2 andthe words it contains are mapped to their poten-tial semantic categories using a semantic fieldtaxonomy of 232 categories.
An evaluation oflexical coverage on the BNC corpus showed thatthe lexical coverage of this lexicon reaches98.49% for modern English (Piao et al, 2004).Such a large-scale semantic lexical resource al-lows us to examine the semantics of manyMWEs and their constituent words convenientlywithout resorting to large corpus data.
Our ex-periment demonstrates that such a lexical re-source provides an additional approach for auto-matically estimating the compositionality ofMWEs.One may question the necessity of measuringcompositionality of manually selected MWEs.The truth is, even if the semantic lexicon underconsideration was compiled manually, it does notexclusively consist of non-compositional MWEslike idioms.
Built for practical discourse analysis,it contains many MWEs which are highly com-positional but depict certain entities or semanticconcepts.
This research forms part of a largereffort to extend lexical resources for semantictagging.
Techniques are described elsewhere(e.g.
Piao et al, 2005b) for finding new candi-date MWE from corpora.
The next stage of thework is to semi-automatically classify these can-didates using an existing semantic field taxon-omy and, to assist this task, we need to investi-gate patterns of compositionality.2 Related WorkIn recent years, various approaches have beenproposed to the analysis of MWE compositional-ity.
Many of the suggested approaches employstatistical algorithms.One of the earliest studies in this area was re-ported by Lin (1999) who assumes that ?non-compositional phrases have a significantly dif-ferent mutual information value than the phrasesthat are similar to their literal meanings?
andproposed to identify non-compositional MWEsin a corpus based on distributional characteristicsof MWEs.
Bannard et al (2003) tested tech-niques using statistical models to infer the mean-ing of verb-particle constructions (VPCs), focus-2 In this lexicon, many MWEs are encoded as templates,such as driv*_* {Np/P*/J*/R*} mad_JJ,  which representvariational forms of a single MWE, For further details, seeRayson et al, 2004.ing on prepositional particles.
They tested fourmethods over four compositional classificationtasks, reporting that, on all tasks, at least one ofthe four methods offers an improvement in preci-sion over the baseline they used.McCarthy et al (2003) suggested that compo-sitional phrasal verbs should have similarneighbours as for their simplex verbs.
Theytested various measures using the nearestneighbours of phrasal verbs and their simplexcounterparts, and reported that some of themeasures produced results which show signifi-cant correlation with human judgments.
Baldwinet al (2003) proposed a LSA-based model formeasuring the decomposability of MWEs by ex-amining the similarity between them and theirconstituent words, with higher similarity indicat-ing the greater decomposability.
They evaluatedtheir model on English noun-noun compoundsand verb-particles by examining the correlationof the results with similarities and hyponymyvalues in WordNet.
They reported that the LSAtechnique performs better on the low-frequencyitems than on more frequent items.
Venkatapathyand Joshi (2005) measured relative composition-ality of collocations having verb-noun patternusing a SVM (Support Vector Machine) basedranking function.
They integrated seven variouscollocational and contextual features using theirranking function, and evaluated it against manu-ally ranked test data.
They reported that the SVMbased method produces significantly better re-sults compared to methods based on individualfeatures.The approaches mentioned above invariablydepend on a variety of statistical contextual in-formation extracted from large corpus data.
In-evitably, such statistical information can be af-fected by various uncontrollable ?noise?, andhence there is a limitation to purely statisticalapproaches.In this paper, we contend that a manuallycompiled semantic lexical resource can have animportant part to play in measuring the composi-tionality of MWEs.
While any approach based ona specific lexical resource may lack generality, itcan complement purely statistical approaches byimporting human expert knowledge into theprocess.
Particularly, if such a resource has ahigh lexical coverage, which is true in our case,it becomes much more useful for dealing withgeneral English.
It should be emphasized that wepropose our semantic lexical-based approach notas a substitute for the statistical approaches.3Rather we propose it as a potential complementto them.In the following sections, we describe our ex-periment and explore this approach to the issueof automatic estimation of MWE compositional-ity.3 Measuring MWE compositionalitywith semantic field informationIn this section, we propose an algorithm forautomatically measuring MWE compositionalitybased on the Lancaster semantic lexicon.
In thislexicon, the semantic field of each word andMWE is encoded in the form of semantic tags.We contend that the compositionality of a MWEcan be estimated by measuring the distance be-tween semantic fields of an MWE and its con-stituent words based on the semantic field infor-mation available from the lexicon.The lexicon employs a taxonomy containing21 major semantic fields which are further di-vided into 232 sub-categories.
3  Tags are de-signed to denote the semantic fields using lettersand digits.
For instance, tag N3.2 denotes thecategory of {SIZE} and Q4.1 denotes {media:Newspapers}.
Each entry in the lexicon maps aword or MWE to its potential semantic fieldcategory/ies.
More often than not, a lexical itemis mapped to multiple semantic categories, re-flecting its potential multiple senses.
In suchcases, the tags are arranged by the order of like-lihood of meanings, with the most prominent oneat the head of the list.
For example, the word?mass?
is mapped to tags N5, N3.5, S9, S5 andB2, which denote its potential semantic fields of{QUANTITIES},  {MEASUREMENT:WEIGHT}, {RELIGION AND SUPERNATU-RAL}, {GROUPS AND AFFILIATION} and{HEALTH AND DISEASE}.The lexicon provides direct access to the se-mantic field information for large number ofMWEs and their constituent words.
Furthermore,the lexicon was analysed and classified manuallyby a team of linguists based on the analysis ofcorpus data and consultation of printed and elec-tronic corpus-based dictionaries, ensuring a highlevel of consistency and accuracy of the semanticanalysis.In our context, we interpret the task of measur-ing the compositionality of MWEs as examiningthe distance between the semantic tag of a MWEand the semantic tags of its constituent words.3 For the complete semantic tagset, see website:http://www.comp.lancs.ac.uk/ucrel/usas/Given a MWE M and its constituent words wi (i= 1, .., n), the compositionality D can be meas-ured by multiplying the semantic distance SDbetween M and each of its constituent words wi.In practice, the square root of the product is usedas the score in order to reduce the range of actualD-scores, as shown below:(1)   ?==niiwMSDMD1),()(where D-score ranges between [0, 1], with 1 in-dicating the strongest compositionality and 0 theweakest compositionality.In the semantic lexicon, as the semantic in-formation of function words is limited, they areclassified into a single grammatical bin (denotedby tag Z5).
In our algorithm, they are excludedfrom the measuring process by using a stop wordlist.
Therefore, only the content constituentwords are involved in measuring the composi-tionality.
Although function words may form animportant part of many MWEs, such as phrasalverbs, because our algorithm solely relies on se-mantic field information, we assume they can beignored.The semantic distance between a MWE andany of its constituent words is calculated byquantifying the similarity between their semanticfield categories.
In detail, if the MWE and a con-stituent word do not share any of the major 21semantic domains, the SD is assigned a smallvalue ?.4 If they do, three possible cases are con-sidered:Case a.
If they share the same tag, and the con-stituent word has only one tag, then SDis one.Case b.
If they share a tag or tags, but the con-stituent words have multiple candidatetags, then SD is weighted using a vari-able ?
based on the position of thematched tag in the candidate list as wellas the number of candidate tags.Case c. If they share a major category, but theirtags fall into different sub-categories(denoted by the trailing digits followinga letter), SD is further weighted using a4 We avoid using zero here in order to avoid producing se-mantic distance of zero indiscriminately when any one ofthe constituent words produces zero distance regardless ofother constituent words.4variable ?
which reflects the differenceof the sub-categories.With respect to weight ?, suppose L is thenumber of candidate tags of the constituent wordunder consideration, N is the position of the spe-cific tag in the candidate list (the position startsfrom the top with N=1), then the weight ?
is cal-culated as(2)21LNL +?=?
,where N=1, 2, ?, n and N<=L.
Ranging between[1, 0), ?
takes into account both the location ofthe matched tag in the candidate tag list and thenumber of candidate tags.
This weight penalisesthe words having more candidate semantic tagsby giving a lower value for their higher degree ofambiguity.
As either L or N increases, the ?-value decreases.Regarding the case c), where the tags share thesame head letter but different digit codes, i.e.they are from the same major category but indifferent sub-categories, the weight ?
is calcu-lated based on the number of sub-categories theyshare.
As we mentioned earlier, a semantic tagconsists of an initial letter and some trailing dig-its divided by points, e.g.
S1.1.2 {RECIPROC-ITY}, S1.1.3 {PARTICIPATION}, S1.1.4 {DE-SERVE} etc.
If we let T1, T2 be a pair of semantictags with the same initial letters, which have kiand kj trailing digit codes (denoting the numberof sub-division layers) respectively and share ndigit codes from the left, or from the top layer,then ?
is calculated as follows:(3)kn=?
;(4)   .
),max( ji kkk =where ?
ranges between (0, 1).
In fact, the cur-rent USAS taxonomy allows only the maximumthree layers of sub-division, therefore ?
has oneof three possible scores: 0.500 (1/2), 0.333 (1/3)and 0.666 (2/3).
In order to avoid producing zeroscores, if the pair of tags do not share any digitcodes except the head letter, then n is given asmall value of 0.5.Combining all of the weighting scores, thesemantic distance SD in formula (1) is calculatedas follows:(5)  ( )?????????=??==.
then   c), if;  then   b), if1;  then   a), if;   then   matches,   tagno if,11niiiniiiwMSD???
?where ?
is given a small value of 0.001 for ourexperiment5.Some MWEs and single words in the lexiconare assigned with combined semantic categorieswhich are considered to be inseparable, as shownbelow:petrol_NN1 station_NN1 M3/H1where the slash means that this MWE falls underthe categories of M3 {VEHICLES AND TRANS-PORTS ON LAND} and H1 {ARCHITECTUREAND KINDS OF HOUSES AND BUILDINGS}at the same time.
For such cases, criss-crosscomparisons between all possible tag pairs arecarried out in order to find the optimal matchbetween the tags of the MWE and its constituentwords.By way of further explanation, the word?brush?
as a verb has candidate semantic tags ofB4 {CLEANING AND PERSONAL CARE} andA1.1.1 {GENERAL ACTION, MAKING} etc.
Onthe other hand, the phrasal verb ?brush down?may fall under either B4 category with the senseof cleaning or G2.2 category {ETHICS} with thesense of reprimand.
When we apply our algo-rithm to it, we get the D-score of 1.0000 for thesense of cleaning, indicating a high degree ofcompositionality, whereas we get a low D-scoreof 0.0032 for the sense of reprimand, indicatinga low degree of compositionality.
Note that theword ?down?
in this MWE is filtered out as it isa functional word.The above example has only a single constitu-ent content word.
In practice, many MWEs havemore complex structures than this example.
Inorder to test the performance of our algorithm,we compared its output against human judgmentsof compositionality, as reported in the followingsection.4 Manually Ranking MWEs forEvaluationIn order to evaluate the performance of ourtool against human judgment, we prepared a list5 As long as ?
is small enough, it does not affect the rankingof D-scores.5of 89 MWEs6 and asked human raters to rankthem via a website.
The list includes six MWEswith multiple senses, and these were treated asseparate MWE.
The Lancaster MWE lexicon hasbeen compiled manually by expert linguists,therefore we assume that every item in this lexi-con is a true MWE, although we acknowledgethat some errors may exist.Following McCarthy et al?s approach, weasked the human raters to assign each MWE anumber ranging between 0 (opaque) and 10(fully compositional).
Both native and non-nativespeakers are involved, but only the data fromnative speakers are used in this evaluation.
As aresult, three groups of raters were involved in theexperiment.
Group 1 (6 people) rated MWEswith indexes of 1-30, Group 2 (4 people) ratedMWEs with indexes of 31-59 and Group 3 (fivepeople) rated MWEs with indexes of 6-89.In order to test the level of agreement betweenthe raters, we used the procedures provided inthe 'irr' package for R (Gamer, 2005).
With thistool, the average intraclass correlation coefficient(ICC) was calculated for each group of ratersusing a two-way agreement model (Shrout &Fleiss, 1979).
As a result, all ICCs exceeded 0.7and were significant at the 95% confidence level,indicating an acceptable level of agreement be-tween raters.
For Group 1, the ICC was 0.894(95% ci = 0.807 < ICC < 0.948), for Group 2 itwas 0.9 (95% ci=0.783<ICC<0.956) and forGroup 3 it was 0.886 (95% ci =  0.762 < ICC <0.948).Based on this test, we conclude that the man-ual ranking of the MWEs is reliable and is suit-able to be used in our evaluation.
Source data forthe human judgements is available from ourwebsite in spreadsheet form7.5 EvaluationIn our evaluation, we focused on testing theperformance of the D-score against human rat-ers?
judgment on ranking different MWEs bytheir degree of compositionality, as well as dis-tinguishing the different degrees of composition-ality for each sense in the case of multiple tags.The first step of the evaluation was to imple-ment the algorithm in a program and run the toolon the 89 test MWEs we prepared.
Fig.
1 illus-trates the D-score distribution in a bar chart.
Asshown by the chart, the algorithm produces awidely dispersed distribution of D-scores across6 Selected at random from the Lancaster semantic lexicon.7 http://ucrel.lancs.ac.uk/projects/assist/the sample MWEs, ranging from 0.000032 to1.000000.
For example, the tool assigned thescore of 1.0 to the FOOD sense and 0.001 to theTHIEF senses of ?tea leaf?
successfully distin-guishing the different degrees of compositional-ity of these two senses.MWE Compositionality Distribution00.10.20.30.40.50.60.70.80.911 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 8689 MWEsD-scoreFig 1: D-score distribution across 89 sampleMWEsAs shown in Fig.
1, some MWEs share thesame scores, reflecting the limitation of the num-ber of ranks that our algorithm can produce aswell as the limited amount of semantic informa-tion available from a lexicon.
Nonetheless, thealgorithm produced 45 different scores whichranked the MWEs into 45 groups (see the stepsin the figure).
Compared to the eleven scoresused by the human raters, this provides a fine-grained ranking of the compositionality.The primary issue in our evaluation is the ex-tent to which the automatic ranking of the MWEscorrelates with the manual ranking of them.
Asdescribed in the previous section, we created alist of 89 manually ranked MWEs for this pur-pose.
Since we are mainly interested in the ranksrather than the actual scores, we examined thecorrelation between the automatic and manualrankings using Spearman?s correlation coeffi-cient.
(For the full ranking list, see Appendix).In the manually created list, each MWE wasranked by 3-6 human raters.
In order to create aunified single test data of human ranking, wecalculated the average of the human ranks foreach MWE.
For example, if two human ratersgive ranks 3 and 4 to a MWE, then its rank is(3+4)/2=3.5.
Next, the MWEs are sorted by theaveraged ranks in descending order to obtain thecombined ranks of the MWEs.
Finally, we sortedthe MWEs by the D-score in the same way toobtain a parallel list of automatic ranks.
For thecalculation of Spearman?s correlation coefficient,if n MWEs are tied to a score (either D-score orthe average manual ranks), their ranks were ad-6justed by dividing the sum of their ranks by thenumber of MWEs involved.
Fig.
2 illustrates thecorrespondence between the adjusted automaticand manual rankings.Auto vs. Manual Ranks Comparison(n=89, rho=0.2572)0204060801000 20 40 60 80 100auto ranksmanual ranksFig.
2: Scatterplot of automatic vs. manualranking.As shown in Fig.
2, the overall correlation seemsquite weak.
In the automatic ranking, quite a fewMWEs are tied up to three ranks, illustrated bythe vertically aligned points.
The precise correla-tion between the automatic and manual rankingswas calculated using the function provided in Rfor Windows 2.2.1.
Spearman's rank correlation(rho) for these data was 0.2572 (p=0.01495),indicating a significant though rather weak posi-tive relationship.In order to find the factors causing this weakcorrelation, we tested the correlation for thoseMWEs whose rank differences were less than 20,30, 40 and 50 respectively.
We are interested tofind out how many of them fall under each of thecategories and which of their features affectedthe performance of the algorithm.
As a result, wefound 43, 54, 66 and 77 MWEs fall under thesecategories respectively, which yield differentcorrelation scores, as shown in Table 1.numb ofMWEsPercent(%)Rankdiffrho-scoreSig.43 48.31 <20 0.9149 P<0.00154 60.67 <30 0.8321 P<0.00166 74.16 <40 0.7016 P<0.00177 86.52 <50 0.5084 P<0.00189 (total) 100.00 <=73 0.2572 P<0.02Table 1: Correlation coefficients correspondingdifferent rank differences.As we expected, the rho decreases as the rankdifference increases, but all of the four categoriescontaining a total of 77 MWEs (86.52%) showreasonably high correlations, with the minimumscore of 0.5084.
8 In particular, 66 of them(74.16%), whose ranking differences are lessthan 40, demonstrate a strong correlation withrho-score 0.7016, as illustrated by Fig.
3ScatterPlot of Auto vs. Man Ranks for 66 MWEs(rank_diff < 40)0204060801000 20 40 60 80 10auto ranksmanranks0Fig 3: ScatterPlot for 66 MWEs (rank_diff <40) which shows a strong correlationOur manual examination shows that the algo-rithm generally pushes the highly compositionaland non-compositional MWEs towards oppositeends of the spectrum of the D-score.
For example,those assigned with score 1 include ?aid worker?,?audio tape?
and ?unemployment figure?.
On theother hand, MWEs such as ?tea leaf?
(meaningthief), ?kick the bucket?
and ?hot dog?
are givena low score of 0.001.
We assume these twogroups of MWEs are generally treated as highlycompositional and opaque MWEs respectively.However, the algorithm could be improved.
Amajor problem found is that the algorithm pun-ishes longer MWEs which contain functionwords.
For example, ?make an appearance?
isscored 0.000114 by the algorithm, but when thearticle ?an?
is removed, it gets a higher score0.003608.
Similarly, when the preposition ?up?is removed from ?keep up appearances?, it gets0.014907 compared to the original 0.000471,which would push up their rank much higher.
Toaddress this problem, the algorithm needs to berefined to minimise the impact of the functionwords to the scoring process.Our analysis also reveals that 12 MWEs withrank differences (between automatic and manualranking) greater than 50 results in a degradedoverall correlation.
Table 2 lists these words, inwhich the higher ranks indicate higher composi-tionality.8 Salkind (2004: 88) suggests that r-score ranges 0.4~0.6,0.6~0.8 and 0.8~1.0 indicate moderate, strong and verystrong correlations respectively.7MWE Sem.
Tag9 AutorankManualrankplough into A9- 53.5 3Bloody Mary F2 53.5 2pillow fight K6 26 80.5lollipop lady M3/S2 70 15cradle snatcher S3.2/T3/S2 73.5 17.5go bananas X5.2+++ 65 8.5make an appearance S1.1.3+ 2 58.5keep up appearances A8/S1.1.1 4 61sandwich course P1 69 11.5go bananas B2-/X1 68 10Eskimo roll M4 71.5 5in other words Z4 12.5 83Table 2: Twelve MWEs having rank differencesgreater than 50.Let us take ?pillow fight?
as an example.
Thewhole expression is given the semantic tag K6,whereas neither ?pillow?
nor ?fight?
as individ-ual word is given this tag.
In the lexicon, ?pil-low?
is classified as H5 {FURNITURE ANDHOUSEHOLD FITTINGS} and ?fight?
is as-signed to four semantic categories including S8-{HINDERING}, X8+ {HELPING}, E3- {VIO-LENT/ANGRY}, and K5.1 {SPORTS}.
For thisreason, the automatic score of this MWE is aslow as 0.003953 on the scale of [0, 1].
On thecontrary, human raters judged the meaning ofthis expression to be fairly transparent, giving ita high score of 8.5 on the scale of [0, 10].
Similarcontrasts occurred with the majority of theMWEs with rank differences greater than 50,which are responsible for weakening the overallcorrelation.Another interesting case we noticed is theMWE ?pass away?.
This MWE has two majorsenses in the semantic lexicon L1- {DIE} andT2- {END} which were ranked separately.
Re-markably, they were ranked in the opposite orderby human raters and the algorithm.
Human ratersfelt that the sense DIE is less idiomatic, or morecompositional, than END, while the algorithmindicated otherwise.
The explanation of thisagain lies in the semantic classification of thelexicon, where ?pass?
as a single word containsthe sense T2- but not L1-.
Consequently, theautomatic score for ?pass away?
with the sense9 Semantic tags occurring in Table 2: A8 (seem), A9 (givingpossession), B2 (health and disease), F2 (drink), K6 (chil-dren?s games and toys), M3 (land transport), M4 (swim-ming), P1 (education), S1.1.1 (social actions), S1.1.3 (par-ticipation), S2 (people), S3.2 (relationship), T3 (time: age),X1 (psychological actions), X5.2 (excited), Z4 (discoursebin)L1- is much lower (0.001) than that with thesense of T2- (0.007071).In order to evaluate our algorithm in compari-son with previous work, we also tested it on themanual ranking list created by McCarthy et al(2003).10 We found that 79 of the 116 phrasalverbs in that list are included in the Lancastersemantic lexicon.
We applied our algorithm onthose 79 items to compare the automatic ranksagainst the average manual ranks using theSpearman?s rank correlation coefficient (rho).
Asa result, we obtained rho=0.3544 with signifi-cance level of p=0.001357.
This result is compa-rable with or better than most measures reportedby McCarthy et al(2003).6 DiscussionThe algorithm we propose in this paper is dif-ferent from previous proposed statistical methodsin that it employs a semantic lexical resource inwhich the semantic field information is directlyaccessible for both MWEs and their constituentwords.
Often, typical statistical algorithms meas-ure the semantic distance between MWEs andtheir constituent words by comparing their con-texts comprising co-occurrence words in nearcontext extracted from large corpora, such asBaldwin et als algorithm (2003).When we consider the definition of the com-positionality as the extent to which the meaningof the MWE can be guessed based on that of itsconstituent words, a semantic lexical resourcewhich maps MWEs and words to their semanticfeatures provides a practical way of measuringthe MWE compositionality.
The Lancaster se-mantic lexicon is one such lexical resourcewhich allows us to have direct access to semanticfield information of large number of MWE andsingle words.
Our experiment demonstrates thepotential value of such semantic lexical resourcesfor the automatic measurement of MWE compo-sitionality.
Compared to statistical algorithmswhich can be affected by a variety of un-controllable factors, such as size and domain ofcorpora, etc., an expert-compiled semantic lexi-cal resource can provide much more reliable and?clean?
lexical semantic information.However, we do not suggest that algorithmsbased on semantic lexical resources can substi-tute corpus-based statistical algorithms.
Rather,we suggest it as a complement to existing statis-tical algorithms.
As the errors of our algorithm10This list is available at website:http://mwe.stanford.edu/resources/8reveal, the semantic information provided by thelexicon alone may not be rich enough for a veryfine-grained distinction of MWE compositional-ity.
In order to obtain better results, this algo-rithm needs to be combined with statistical tech-niques.A limitation of our approach is language-dependency.
In order to port our algorithm tolanguages other than English, one needs to buildsimilar semantic lexicon in those languages.However, similar semantic lexical resources arealready under construction for some other lan-guages, including Finnish and Russian (L?fberget al, 2005; Sharoff et al, 2006), which will al-low us to port our algorithm to those languages.7 ConclusionIn this paper, we explored an algorithm basedon a semantic lexicon for automatically measur-ing the compositionality of MWEs.
In ourevaluation, the output of this algorithm showedmoderate correlation with a manual ranking.
Weclaim that semantic lexical resources provideanother approach for automatically measuringMWE compositionality in addition to the exist-ing statistical algorithms.
Although our resultsare not yet conclusive due to the moderate scaleof the test data, our evaluation demonstrates thepotential of lexicon-based approaches for thetask of compositional analysis.
We foresee, bycombining our approach with statistical algo-rithms, that further improvement can be ex-pected.8 AcknowledgementThe work reported in this paper was carriedout within the UK-EPSRC-funded ASSIST Pro-ject (Ref.
EP/C004574).ReferencesTimothy Baldwin, Colin Bannard, Takaaki Tanaka,and Dominic Widdows.
2003.
An Empirical Modelof Multiword Expression Compositionality.
InProc.
of the ACL-2003 Workshop on MultiwordExpressions: Analysis, Acquisition and Treatment,pages 89-96, Sapporo, Japan.Colin Bannard, Timothy Baldwin, and Alex Las-carides.
2003.
A statistical approach to the seman-tics of verb-particles.
In Proc.
of the ACL2003Workshop on Multiword Expressions: Analysis,Acquisition and Treatment, pages 65?72, Sapporo.Nicoletta Calzolari, Charles Fillmore, Ralph Grish-man, Nancy Ide, Alessandro Lenci, CatherineMacLeod, and Antonio Zampolli.
2002.
Towardsbest practice for multiword expressions in compu-tational lexicons.
In Proc.
of the Third Interna-tional Conference on Language Resources andEvaluation (LREC 2002), pages 1934?1940, LasPalmas, Canary Islands.Ann Copestake, Fabre Lambeau, Aline Villavicencio,Francis Bond, Timothy Baldwin, Ivan A.
Sag, andDan Flickinger.
2002.
Multiword expressions: Lin-guistic precision and reusability.
In Proc.
of theThird International Conference on Language Re-sources and Evaluation (LREC 2002), pages 1941?1947, Las Palmas, Canary Islands.Matthias  Gamer.
2005.
The irr Package: Various Co-efficients of Interrater Reliability and Agreement.Version 0.61 of 11 October 2005.
Available from:cran.r-project.org/src/contrib/Descriptions/irr.htmlDekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proc.
of the 37th AnnualMeeting of the ACL, pages 317?324, College Park,USA.Laura L?fberg, Scott Piao, Paul Rayson, Jukka-PekkaJuntunen, Asko Nyk?nen, and Krista Varantola.2005.
A semantic tagger for the Finnish language.In Proc.
of the Corpus Linguistics 2005 conference,Birmingham, UK.Christopher D. Manning and Hinrich Sch?tze.
2000.Foundations of Statistical Natural Language Proc-essing.
The MIT Press, Cambridge, Massachusetts.Diana McCarthy, Bill Keller, and John Carroll.
2003.Detecting a continuum of compositionality inphrasal verbs.
In Proc.
of the ACL-2003 Workshopon Multiword Expressions: Analysis, Acquisitionand Treatment, pages 73?80, Sapporo, Japan.Dan Melamed.
1997.
Automatic discovery of non-compositional compounds in parallel data.
In Proc.of the 2nd Conference on Empirical Methods inNatural Language Processing , Providence, USA.Geoffrey Nunberg, Ivan A.
Sag, and Tom Wasow.1994.
Idioms.
Language, 70: 491?538.Scott S.L.
Piao, Paul Rayson, Dawn Archer and TonyMcEnery.
2004.
Evaluating Lexical Resources fora Semantic Tagger.
In Proc.
of LREC-04, pages499?502, Lisbon, Portugal.Scott S.L.
Piao, Dawn Archer, Olga Mudraya, PaulRayson, Roger Garside, Tony McEnery and An-drew Wilson.
2005a.
A Large Semantic Lexiconfor Corpus Annotation.
In Proc.
of the Corpus Lin-guistics Conference 2005, Birmingham, UK.Scott S.L.
Piao., Paul Rayson, Dawn Archer, TonyMcEnery.
2005b.
Comparing and combining a se-mantic tagger and a statistical tool for MWE ex-traction.
Computer Speech and Language, 19, 4:378?397.9Paul Rayson, Dawn Archer, Scott Piao, and TonyMcEnery.
2004.
The UCREL Semantic AnalysisSystem.
In Proc.
of LREC-04 Workshop: BeyondNamed Entity Recognition Semantic Labeling forNLP Tasks, pages 7?12, Lisbon, Portugal.Susanne Riehemann.
2001.
A Constructional Ap-proach to Idioms and Word Formation.
Ph.D. the-sis, Stanford University, Stanford.Ivan A.
Sag, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
MultiwordExpressions: A Pain in the Neck for NLP.
In Proc.of the 3rd International Conference on IntelligentText Processing and Computational Linguistics(CICLing-2002), pages 1?15, Mexico City, Mexico.Neil J. Salkind.
2004.
Statistics for People Who HateStatistics.
Sage: Thousand Oakes, US.Serge Sharoff, Bogdan Babych, Paul Rayson, OlgaMudraya and Scott Piao.
2006.
ASSIST: Auto-mated semantic assistance for translators.
Proceed-ings of EACL 2006, pages 139?142, Trento, Italy.Patrick E. Shrout and Joseph L. Fleiss.
1979.
Intra-class Correlations: Uses in Assessing Rater Reli-ability.
Psychological Bulletin (2), 420?428.Sriram Venkatapathy and Aravind K. Joshi.
2005.Measuring the relative compositionality of verb-noun (V-N) collocations by integrating features.
InProc.
of Human Language Technology Conferenceand Conference on Empirical Methods in NaturalLanguage Processing (HLT/EMNLP 2005), pages899?906, Vancouver, Canada.Appendix: Manual vs. Automatic Ranksof Sample MWEsThe table below shows the human and auto-matic rankings of 89 sample MWEs.
The MWEsare sorted in ascending order by manual averageranks.
The top items are supposed to be the mostcompositional ones.
For example, according tothe manual ranking, facial expression is the mostcompositional MWE while tea leaf is the mostopaque one.
This table also shows that someMWEs are tied up with the same ranks.
For thedefinitions of the full semantic tagset, see web-site http://www.comp.lancs.ac.uk/ucrel/usas/.MWE Tag Sem tag ManrankAuto.rankfacial expression B1 1 9aid worker S8/S2 2 4audio tape K3 3.5 4leisure activities K1 3.5 36.5advance warning T4/Q2.2 5 36.5living space H2 6 51in other words Z4 7 77.5unemployment fig-uresI3.1/N5 8 4camera angle Q4.3 9.5 45pillow fight K6 9.5 64youth club S5/T3 11.5 4petrol station M3/H1 11.5 36.5palm tree L3 13 9rule book G2.1/Q4.1 14 4ball boy K5.1/S2.2 15 13goal keeper K5.1/S2 16.5 4kick in E3- 16.5 36.5ventilation shaft H2 18 47directory enquiries Q1.3 19 14phone box Q1.3/H1 21 18.5lose balance M1 21 53bend the rules A1.7 21 54.5big nose X7/X2.4 23 67quantity control N5/A1.7 24 11.5act of God S9 25 36.5air bag A15/M3 26 62.5mind stretching A12 27 59plain clothes B5 28 36.5keep up appearances A8/S1.1.1 29 86examining board P1 30 23open mind X6 31.5 49make an appearance S1.1.3+ 31.5 88cable television Q4.3 33 15king size N3.2 34 36.5action point X7 35 61keep tight rein on A1.7 36 28noughts and crosses K5.2 37 77.5tea leaf L3/F2 38 4single minded X5.1 39.5 77.5window dressing I2.2 39.5 77.5street girl G1.2/S5 42 36.5just over the horizon S3.2/S2.1 42 60pressure group T1.1.3 42 16.5air proof O4.1 44.5 57.5heart of gold S1.2.2 44.5 77.5lose heart X5.2 46 26food for thought X2.1/X5.1 47 89play part S8 48 68look down on S1.2.3 49 77.5arm twisting Q2.2 50 36.5take into account A1.8 51 69kidney bean F1 52 9come alive A3+ 53 52break new ground T3/T2 54 54make up to S1.1.2 55 65by virtue of C1 56.5 36.5snap shot A2.2 56.5 27pass away L1- 58 77.5long face E4.1 59 77.5bossy boots S1.2.3/S2 60 77.5plough into M1/A1.1.2 61 11.5kick in T2+ 62 50animal magnetism S1.2 63 55.5sixth former P1/S2 64 77.5pull the strings S7.1 65 62.5couch potato A1.1.1/S2 66 77.5think tank S5/X2.1 67 36.5come alive X5.2+ 68 24hot dog F1 69 77.5cheap shot G2.2-/Q2.2 70 6610rock and roll K2 71 48bright as a button S3.2/T3/S2 72.5 87cradle snatcher X9.1+ 72.5 16.5alpha wave B1 74 77.5lollipop lady M3/S2 75 20pass away X5.2+ 76.5 57.5plough into T2- 76.5 36.5piece of cake P1 78.5 77.5sandwich course A12 78.5 21go bananas B2-/X1 80 22go bananas X5.2+++ 81.5 36.5go bananas E3- 81.5 25kick the bucket L1 83 77.5on the wagon F2 84 36.5Eskimo roll M4 85 18.5acid house K2 86 46plough into A9- 87 36.5Bloody Mary F2 88 36.5tea leaf G2.1-/S2mf 89 77.511
