Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 601?612,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsA Constrained Latent Variable Model for Coreference ResolutionKai-Wei Chang Rajhans Samdani Dan RothUniversity of Illinois at Urbana-Champaign{kchang10|rsamdan2|danr}@illinois.eduAbstractCoreference resolution is a well known clus-tering task in Natural Language Processing.
Inthis paper, we describe the Latent Left Linkingmodel (L3M), a novel, principled, and linguis-tically motivated latent structured predictionapproach to coreference resolution.
We showthat L3M admits efficient inference and can beaugmented with knowledge-based constraints;we also present a fast stochastic gradient basedlearning.
Experiments on ACE and Ontonotesdata show that L3M and its constrained ver-sion, CL3M, are more accurate than severalstate-of-the-art approaches as well as somestructured prediction models proposed in theliterature.1 IntroductionCoreference resolution is a challenging task, that in-volves identification and clustering of noun phrasesmentions that refer to the same real-world entity.Most machine learning approaches to coreferenceresolution learn a scoring function to estimate thecompatibility between two mentions or two sets ofpreviously clustered mentions.
Then, a decoding al-gorithm is designed to aggregate these scores andfind an optimal clustering assignment.The most popular of these frameworks is the pair-wise mention model (Soon et al 2001; Ng andCardie, 2002; Bengtson and Roth, 2008), whichlearns a compatibility score of mention-pairs anduses these pairwise scores to obtain a global cluster-ing.
Recently, efforts have been made (Haghighi andKlein, 2010; Rahman and Ng, 2011b; Rahman andNg, 2011c) to consider models that capture higherorder interactions, in particular, between mentionsand previously identified entities (that is, betweenmentions and clusters).
While such models are po-tentially more expressive, they are largely based onheuristics to achieve computational tractability.This paper focuses on a novel and principled ma-chine learning framework that pushes the state-of-the-art while operating at a mention-pair granularity.We present two models ?
the Latent Left-LinkingModel (L3M), and a version of that is augmentedwith domain knowledge-based constraints, the Con-strained Latent Left-Linking Model (CL3M).
L3Madmits efficient inference, linking each mention to apreviously occurring mention to its left, much likethe existing best-left-link inference models (Ng andCardie, 2002; Bengtson and Roth, 2008).
How-ever, unlike previous best-link techniques, learningin our case is performed jointly with decoding ?
wepresent a novel latent structural SVM approach, op-timized using a fast stochastic gradient-based tech-nique.
Furthermore, we present a probabilistic gen-eralization of L3M that is more expressive in thatit is capable of considering mention-entity interac-tions using scores at the mention-pair granularity.We augment this model with a temperature-like pa-rameter (Samdani et al 2012) to provide additionalflexibility.CL3M augments L3M with knowledge-basedconstraints following (Roth and Yih, 2004; Denisand Baldridge, 2007).
This capability is very de-sirable as shown by the success of the rule-based de-terministic approach of Raghunathan et al(2010)in the CoNLL shared task 2011 (Pradhan et al2011).
In L3M, domain-specific constraints are in-corporated into learning and inference in a straight-forward way.
CL3M scores a mention?s contributionto its cluster by combining the corresponding score601of the underlying L3M model with that from a set ofconstraints.Most importantly, in our experiments on bench-mark coreference datasets, we show that CL3M,with just five constraints, compares favorably withother, more complicated, state-of-the-art algorithmson a variety of evaluation metrics.
Over-all, the main contribution of this paper is aprincipled machine learning model operating atmention-pair granularity, using easy to implementconstraint-augmented inference and learning, thatyields competitive results on coreference resolutionon Ontonotes-5.0 (Pradhan et al 2012) and ACE2004 (NIST, 2004).2 Related WorkThe idea of Latent Left-linking Model (L3M) is in-spired by a popular inference approach to corefer-ence which we call the Best-Left-Link approach (Ngand Cardie, 2002; Bengtson and Roth, 2008).
In thebest-left-link strategy, each mention i is connectedto the best antecedent mention j with j < i (i.e.
amention occurring to the left of i, assuming a left-to-right reading order), thereby creating a left-link.The ?best?
antecedent mention is the one with thehighest pairwise score, wij ; furthermore, if wij isbelow some threshold, say 0, then i is not connectedto any antecedent mention.
The final clustering isa transitive closure of these ?best?
links.
The intu-ition behind best-left-link strategy is based on howhumans read and decipher coreference links ?
theymostly rely on information to the left of the men-tion when deciding whether to add it to a previouslyconstructed cluster or not.
This strategy has beensuccessful and commonly used in coreference res-olution (Ng and Cardie, 2002; Bengtson and Roth,2008; Stoyanov et al 2009).
However, most workshave developed ad-hoc approaches to implement thisidea.
For instance, Bengtson and Roth (2008) traina model w on binary training data generated by tak-ing for each mention, the closest antecedent corefer-ent mention as a positive example, and all the othermentions as negative examples.
Similar approachesto training and, additionally, decoupling the trainingstage from the clustering stage were used by othersystems.
In this paper, we formalize the learningproblem of the best-left-link model as a structuredprediction problem and analyze our system with de-tailed experiments.
Furthermore, we generalize thisapproach by considering multiple pairwise left-linksinstead of just the best link, efficiently capturing thenotion of a mention-to-cluster link.Many techniques in the coreference literaturebreak away from the mention pair-based, best-left-link paradigm.
Denis and Baldridge (2008) and Ng(2005) learn a local ranker to rank the mentionpairs based on their compatibility.
While these ap-proaches achieve decent empirical performance, itis unclear why these are the right ways to train themodel.
Some techniques consider a more expres-sive model by using features defined over mention-cluster or cluster-cluster (Rahman and Ng, 2011c;Stoyanov and Eisner, 2012; Haghighi and Klein,2010).
For these models, the inference and learn-ing algorithms are usually complicated.
Very re-cently, Durrett et al(2013) propose a probabilis-tic model which enforces structural agreement con-straints between specified properties of mentioncluster when using a mention-pair model.
This ap-proach is very related to the probabilistic extensionof our method as both models attempt to leverageentity-level information from mention-pair features.However, our approach is simpler because it directlyconsiders the probabilities of multiple links.
Fur-thermore, while their model performs only slightlybetter than the Stanford rule-based system (Lee etal., 2011), we significantly outperform this system.Most importantly, our model obtains state-of-the-artperformance on OntoNotes-5.0 while still operatingat the mention-pair granularity.
We believe that thisis due to our novel and principled structured predic-tion framework which results in accurate (and effi-cient) training.Several structured prediction techniques havebeen applied to coreference resolution in the ma-chine learning literature.
For example, McCallumand Wellner (2003) and Finley and Joachims (2005)model coreference as a correlational clustering prob-lem (Bansal et al 2002) on a complete graph overthe mentions with edge weights given by the pair-wise classifier.
However, correlational clustering isknown to be NP Hard (Bansal et al 2002); nonethe-less, an ILP solver or an approximate inference algo-rithm can be used to solve this problem.
Another ap-proach proposed by Yu and Joachims (2009) formu-602lates coreference with latent spanning trees.
How-ever, their approach has no directionality betweenmentions, whereas our latent structure captures thenatural left-to-right ordering of mentions.
In ourexperiments (Sec.
5), we show that our techniquevastly outperforms both the spanning tree and thecorrelational clustering techniques.
We also com-pare with (Fernandes et al 2012) and the pub-licly available Stanford coreference system (Raghu-nathan et al 2010; Lee et al 2011), a state-of-the-art rule-based system.Finally, some research (Ratinov and Roth, 2012;Bansal and Klein, 2012; Rahman and Ng, 2011a)has tried to integrate world knowledge from web-based statistics or knowledge bases into a corefer-ence system.
World knowledge is potentially use-ful for resolving coreference and can be injectedinto our system in a straightforward way via theconstraints framework.
We will show an exampleof incorporating our system with name-entity andWordNet-based similarity metric (Q.
Do, 2009) inSec.
5.
Including massive amount of informationfrom knowledge resources is not the focus of thispaper and may distort the comparison with otherrelevant models but our results indicate that this isdoable in our model, and may provide significantimprovements.3 Latent Left-Linking Model withConstraintsIn this section, we describe our Constrained LatentLeft-Linking Model (CL3M).
CL3M is inspired bya few ideas from the literature: (a) the popular Best-Left-Link inference approach to coreference (Ng andCardie, 2002; Bengtson and Roth, 2008), and (b) theinjection of domain knowledge-based constraints forstructured prediction (Roth and Yih, 2004; Clarkeand Lapata, 2006; Chang et al 2012b; Ganchev etal., 2010; Koo et al 2010; Pascal and Baldridge,2009).We first introduce the notion of a pairwisemention-scorer, then introduce our Left-LinkingModel (L3M), and finally describe how to inject con-straints into our model.Let d be a document with md mentions.
Mentionsare denoted solely using their indices, ranging from1 to md.
A coreference clustering C for documentd is a collection of disjoint sets partitioning the set{1, .
.
.
, md}.
We represent C as a binary functionwith C(i, j) = 1 if mentions i and j are coreferent,otherwise C(i, j) = 0.
Let s(C;w, d) be the scoreof a given clustering C for a given document and agiven pairwise weight vector w. Then, during infer-ence, a clustering C is predicted by maximizing thescoring function s(C;w, d), over all valid (i.e.
sat-isfying symmetry and transitivity) clustering binaryfunctions C : {1, .
.
.
, md}?
{1, .
.
.
, md} ?
{0, 1}.3.1 Mention Pair ScorerWe model the task of coreference resolution using apairwise scorer which indicates the compatibility ofa pair of mentions.
The inference routine then pre-dicts the final clustering ?
a structured predictionproblem ?
using these pairwise scores.Specifically, for any two mentions i and j (w.l.o.g.j < i), we produce a pairwise compatibility scorewji using extracted features ?
(j, i) aswji = w ?
?
(j, i) , (1)where w is a weight parameter that is learned.3.2 Latent Left-Linking ModelOur inference algorithm is inspired by the best-left-link approach.
In particular, the score s(C; d,w) isdefined so that each mention links to the antecedentmention (to its left) with the highest score (as longas the score is above some threshold, say, 0).
Specif-ically:s(C; d,w) =md?i=1max0?j<i,C(i,j)=1w ?
?
(j, i) .
(2)In order to simplify the notation, we introduce adummy mention with index 0, which is to the left(i.e.
appears before) of all other mentions and hasw0i = 0 for all actual mentions i > 0.
For a givenclustering C, if a mention i is not co-clustered withany previous actual mention j, 0 < j < i, then weassume that i links to 0 and C(i, 0) = 1.
In otherwords, C(i, 0) = 1 iff i is the first actual item of acluster in C. However, such an item i is not consid-ered to be co-clustered with 0 and for any valid clus-tering, item 0 is always in a singleton dummy clus-ter, which is eventually discarded.
The importantproperty of the score s is that it is exactly maximized603by the best-left-link inference, as it maximizes indi-vidual left link scores and the creation of one left-link does not affect the creation of other left-links.3.3 LearningWe use a max-margin approach to learn w. We aregiven a training set D of documents where for eachdocument d ?
D, Cd refers to the annotated groundtruth clustering.
Then we learn w by minimizingL(w) =?2?w?2 + 1|D|?d?D1md(maxC(s(C; d,w)+ ?
(C, Cd))?
s(Cd; d,w)),where ?
(C, Cd) is a loss function used in corefer-ence.
In order to achieve tractable loss-augmentedminimization ?
something not possible with stan-dard loss functions used in coreference (e.g.B3 (Bagga and Baldwin, 1998)) ?
we use a de-composable loss function that just counts the num-ber of mention pairs on which C and Cd disagree:?
(C, Cd) =?mdi,j=0,j<i IC(i,j)=Cd(i,j), where I isa binary indicator function.
This loss functionis equivalent to the numerator of the Rand indexloss (Rand, 1971).
With this form of loss functionand using the scoring function in Eq.
(2), we canwrite L(w) as?2?w?2 + 1|D|?d?D1mdmd?i=1(max0?j<i(w ?
?
(j, i)+ ?
(Cd, i, j))?
max0?j<i,C(i,j)=1(w ?
?
(j, i))),(3)where ?
(Cd, i, j) = 1 ?
Cd(i, j) is the loss-basedmargin that is 1 if i and j are not coreferent in Cd,and is 0 otherwise.
In the above objective function,the left-links remain latent while we get to observethe clustering.
This objective function is related tolatent structural SVMs (Yu and Joachims, 2009).However Yu and Joachims (2009) use a spanningtree based latent structure which does not have theleft-to-right directionality we exploit.
We can mini-mize the above function using Concave Convex Pro-cedure (Yuille and Rangarajan, 2003), which is guar-anteed to reach the local minima.
However, such aprocedure is costly as it requires doing inference onall the documents to compute a single gradient up-date.
Consequently, we choose a faster stochasticsub-gradient descent (SGD) approach.
Since L(w)in Eq.
(3) decomposes not only over training doc-uments, but also over individual mentions in eachdocument, we can perform SGD on a per-mentionbasis.
The stochastic sub-gradient w.r.t.
mention iin document d is given by?L(w)id ?
?
(j?, i) ?
?(j?
?, i) + ?w, where (4)j?
= arg max0?j<i(w ?
?
(j, i) + 1 ?
Cd(i, j))j??
= arg max0?j<i,C(i,j)=1w ?
?
(j, i)While SGD has no theoretically convergence guar-antee, it works excellently in our experiments.Specifically, we observe that SGD achieves similartraining performance to CCCP with a speed-up ofaround 10,000.3.4 Incorporating ConstraintsNext, we show how to incorporate domainknowledge-based constraints into L3M and gener-alize it to CL3M.
In CL3M, we obtain a cluster-ing by maximizing a constraint-augmented scoringfunction f given bys(C; d,w) +nc?p=1?p?p(d, C),where the second term on the R.H.S.
is thescore contributed by domain specific constraints?1, .
.
.
, ?nc with their respective scores ?1, .
.
.
, ?nc .In particular, ?p(d, C) measures the extent to whicha given clustering C satisfies the pth constraint.
Notethat this framework is general and can be applied toinject mention-to-cluster or cluster-to-cluster levelconstraints too.
However, for simplicity, we con-sider here only constraints between mention pairs.This allows us derive fast greedy algorithm to solvethe inference problem.
The details of our constraintsare presented in Sec.
5.All of our constraints can be categorized into twogroups: ?must-link?
and ?cannot-link?.
?Must-link?constraints encourage a pair of mentions to connect,while ?cannot-link?
constraints discourage mentionpairs from being linked.
Consequently, the coeffi-cients ?p associated with ?must-link?
constraints arepositive while ?p for ?cannot-link?
constraints arenegative.
In the following, we briefly discuss how to604solve the inference problem with these two types ofconstraints.We slightly abuse notations and use ?p(j, i) to in-dicate the pth constraint on a pair of mentions (i, j).
?p(j, i) is a binary function that is 1 iff two mentionsi and j satisfy the conditions specified in constraintp.
Chang et al(2011) shows that best-left-link in-ference can be formulated as an ILP problem.
Whenwe add constraints, the ILP becomes:arg maxB,C?
{0,1}?i,j:j<iwjiBji +?i,j?p?p(j, i)Cijs.t Ckj ?
Cij + Cki ?
1, ?i, j, k,?i?1j=0Bji = 1, ?iBji < Cji, Cji = Cji,?i, j,(5)where Cij ?
C(i, j) is a binary variable indicatingwhether i and j are in the same cluster or not andBji is an auxiliary variable indicating the best-left-link for mention i.
The first set of inequality con-straints in (5) enforces the transitive closure of theclustering.
The constraints Bji < Cji,?i, j enforcethe consistency between these two sets of variables.One can use an off-the-shelf solver to solve Eq.(5).
However, when the absolute values of the con-straint scores (|?p|) are high (the hard constraintcase), then the following greedy algorithm approxi-mately solves the inference efficiently.
We scan thedocument from left-to-right (or in any other arbitraryorder).
When processing mention i, we findj?
= arg maxj<iwji +?k:C?
(k,j)=1?p?p?p(k, i),(6)where C?
is the current clustering obtained from theprevious inference steps.
Then, we add a link be-tween mention i and j?.
The rest of the infer-ence process is the same as in the original best-left-link inference.
Specifically, this inference procedurecombines the classifier score for mention pair i, j,with the constraints score of all mentions currentlyco-clustered with j.
We discuss this further in Sec-tion 5.4 Probabilistic Latent Left-Linking ModelIn this section, we extend and generalize our left-linking model approach to a probabilistic model,Probabilistic Latent Left-Linking Model (PL3M),that allows us to naturally consider mention-to-entity (or mention-to-cluster) links.
While in L3M,we assumed that each mention links determinis-tically to the max-scoring mention on its left, inPL3M, we assume that mention i links to mentionj, j ?
i, with probability given byPr[j ?
i; d,w] = e1?
(w??
(i,j))Zi(w, ?).
(7)Here Zi(w, ?)
=?0?k<i e1?
(w??
(i,k)) is a normal-izing constant and ?
?
(0, 1] is a constant tem-perature parameter that is tuned on a developmentset (Samdani et al 2012).
We assume that the eventthat mention i links to a mention j is independent ofthe event that mention i?
links to j?
for i 6= i?.Inference with PL3M: Given the probability of alink as in Eq.
(7), the probability that mention i joinsan existing cluster c, Pr[c ?
i; d,w], is simply thesum of the probabilities of i linking to the mentionsinside c:Pr[c ?
i; d,w] =?j?c,0?j<iPr[j ?
i; d,w]=?j?c,0?j<ie1?
(w??
(i,j))Zi(d,w, ?).
(8)Based on Eq.
(8) and making use of the indepen-dence assumption of left-links, we follow a simplegreedy clustering (or inference) algorithm: sequen-tially add each mention i to a previously formedcluster c?, where c?
= arg maxc Pr[c ?
i; d,w].If the arg max cluster is the singleton cluster withthe dummy mention 0 (i.e.
the score of all otherclusters is below the threshold of 0), then i starts anew cluster and is not included in the dummy clus-ter.
Note that we link a mention to a cluster tak-ing into account all the mentions inside that cluster,mimicking the notion of a mention-to-cluster link.This provides more expressiveness than the Best-Left-Link inference, where a mention connects toa cluster solely based on a single pairwise link tosome antecedent mention (the best-link mention) inthat cluster.The case of ?
= 0: As ?
approaches zero, it iseasy to show that the probability P [j ?
i; d, w]605in Eq.
(7) approaches a Kronecker delta functionthat puts probability 1 on the max-scoring mentionj = arg max0?k<i w??
(i, j) (assuming no ties), and0 everywhere else (Pletscher et al 2010; Samdani etal., 2012).
Consequently, as ?
?
0, Pr[c ?
i; d,w]in Eq.
8 approaches a Kronecker delta function cen-tered on the cluster containing the max-scoring men-tion, thus reducing to the best-link case of L3M.Thus, PL3M, when tuning the value of ?, is a strictlymore general model than L3M.Learning with PL3M We use a likelihood-basedapproach to learning with PL3M, and first computethe probability Pr[C; d,w] of generating a cluster-ing C, given w. We then learn w by minimizingthe regularized negative log-likelihood of the data,augmenting the partition function with a loss-basedmargin (Gimpel and Smith, 2010).
We omit the de-tails of likelihood computation due to lack of space.With PL3M, we again follow a stochastic gradi-ent descent technique instead of CCCP for the samereasons mentioned in Sec.
3.3.
The stochastic gra-dient (subgradient when ?
= 0) w.r.t.
mention i indocument d is given by?LL(w)id ??0?j<ipj?
(i, j) ??0?j<ip?j?
(i, j) + ?w,where pj and p?j , j = 0, .
.
.
, i ?
1, are non-negativeweights that sum to one and are given bypj =e1?
(w??(i,j)+?
(Cd,i,j))?0?k<i e1?
(w??(i,k)+?
(Cd,i,k))andp?j =Cd(i, j)Zi(d,w, ?
)Zi(Cd; d,w, ?
)Pr[j ?
i; d,w] .Interestingly, the above update rule generalizes theone for L3M, as we are incorporating a weightedsum of all previous mentions in the update rule.With ?
?
0, the SGD in Eq.
(4) converges to theSGD update in L3M (Eq.
(4)).
Finally, in the pres-ence of constraints, we can fold them inside the pair-wise link probabilities as in Eq.
(6).5 Experiments and ResultsIn this section, we present our experiments on thetwo commonly used benchmarks for coreference?
Ontonotes-5.0 (Pradhan et al 2012) and ACE2004 (NIST, 2004).
Table 1 exhibits our bottom lineresults: CL3M achieves the best result reported onOntonotes-5.0 development set and essentially tieswith (Fernandes et al 2012) on the test set.
Asshown in Table 3, CL3M is also the best algorithmon ACE and when evaluated on the gold mentionsof Ontonotes.
We show that CL3M performs partic-ularly well on clusters containing named entity men-tions, which are more important for many informa-tion extraction applications.
In the rest of this sec-tion, after describing our experimental setting, weprovide careful analysis of our algorithms and com-pare them to competitive coreference approaches inthe literature.5.1 Experimental SetupDatasets: ACE 2004 contains 443 documents ?we used a standard split of these documents into268 training, 68 development, and 106 testing doc-uments used by Culotta et al(2007) and Bengt-son and Roth (2008).
OntoNotes-5.0 dataset, re-leased for the CoNLL 2012 Shared Task (Pradhan etal., 2012), is by far the largest annotated corpus oncoreference.
It contains 3,145 annotated documentsdrawn from a wide variety of sources ?
newswire,bible, broadcast transcripts, magazine articles, andweb blogs.
We report results on both developmentset and test set.
To test on the development set, wefurther split the training data into training and devel-opment sets.Classifier details: For each of the pairwise ap-proaches, we assume the pairwise score is given byw??
(?, ?
)+t where ?
are the features, w is the weightvector learned by the approach, and t is a thresholdwhich we set to 0 during learning (as in Eq.
(1)), butuse a tuned value (tuned on a development set) dur-ing testing.
For learning with L3M, we do stochasticgradient descent with 5 passes over the data.
Empir-ically, we observe that this is enough to generate astable model.
For PL3M (Sec.
4), we tune the valueof ?
using the development set picking the best ?from {0.0, 0.2, .
.
.
, 1.0}.
Recall that when ?
= 0,PL3M is the same as L3M.
We refer to L3M andPL3M with incorporating constraints during infer-ence as CL3M and CPL3M (Sec.
3.4), respectively.Metrics: We compare the systems using threepopular metrics for coreference ?
MUC (Vilain etal., 1995), BCUB (Bagga and Baldwin, 1998), and606Entity-based CEAF (CEAFe) (Luo, 2005).
Follow-ing, the CoNLL shared tasks (Pradhan et al 2012),we use the average F1 scores of these three metricsas the main metric of comparison.Features: We build our system on the publiclyavailable Illinois-Coref system1 primarily because itcontains a rich set of features presented in Bengtsonand Roth (2008) and Chang et al(2012a) (the latteradds features for pronominal anaphora resolution).We also compare with the Best-Left-Link approachdescribed by Bengtson and Roth (2008).Constraints: We consider the following con-straints in CL3M and CPL3M.?
SameSpan: two mentions must be linked toeach other if they share the same surface textspan and the number of words in the text spanis larger than a threshold (set as 5 in our imple-mentation).?
SameDetNom: two mentions must be linkedto each other if both mentions start with a de-terminer and the [0,1] wordnet-based similarityscore between the mention head words is abovea threshold (set to 0.8).?
SameProperName: two mentions must belinked if they are both proper names and thesimilarity score measured by a named entity-based similarity metric, Illinois NESim2, arehigher than a threshold (set to 0.8).
For a per-son entity we add additional rules to extract thefirst name, last name and professional title asproperties.?
ModifierMismatch: the constraint prevents twomentions to be linked if the head modifiersconflict.
For example, the constraint prevents?northern Taiwan?
from linking to ?southernTaiwan?.
We gather a list of mutual exclusivemodifiers from the training data.?
PropertyMismatch: the constraint prevents twomentions to be linked if their properties con-flict.
For example, it prevents male pronounsto link to female pronouns and ?Mr.
Clinton?to link to ?Mrs.
Clinton?
by checking the gen-der property.
The properties we consider aregender, number, professional title and the na-1The system is available at http://cogcomp.cs.illinois.edu/page/software_view/Coref/2http://cogcomp.cs.illinois.edu/page/software_view/NESimMUC BCUB CEAFe AVGDev SetStanford 64.30 70.46 46.35 60.37(Chang et al 2012a) 65.75 70.25 45.30 60.43(Martschat et al 2012) 66.76 71.91 47.52 62.06(Bjo?rkelund and Farkas, ) 67.12 71.18 46.84 61.71(Chen and Ng, 2012) 66.4 71.8 48.8 62.3(Fernandes et al 2012) 69.46 71.93 48.66 63.35L3M 67.88 71.88 47.16 62.30CL3M 69.20 72.89 48.67 63.59Test SetStanford 63.83 68.52 45.36 59.23(Chang et al 2012a) 66.38 69.34 44.81 60.18(Martschat et al 2012) 66.97 70.36 46.60 61.31(Bjo?rkelund and Farkas, ) 67.58 70.26 45.87 61.24(Chen and Ng, 2012) 63.7 69.0 46.4 59.7(Fernandes et al 2012) 70.51 71.24 48.37 63.37L3M 68.31 70.81 46.73 61.95CL3M 69.64 71.93 48.32 63.30Table 1: Performance on OntoNotes-5.0 with predictedmentions.
We report the F1 scores (%) on various coref-erence metrics (MUC, BCUB, CEAF).
The column AVGshows the average scores of the three.
We observe thatPL3M and CPL3M (see Sec.
4) yields the same perfor-mance as L3M and CL3M, respectively as the tuned ?
forall the datasets turned out to be 0.tionality.While the ?must-link?
constraints described in thepaper can be treated as features, due to their highprecision, treating them as hard constraints (set ?
toa high value) is a safe and direct way to inject hu-man knowledge into the learning model.
Moreover,our framework allows a constraint to use informa-tion from previous decisions (such as ?cannot-link?constraints).
Treating such constraints as featureswill complicate the learning model.5.2 Performance of the End-to-End SystemWe compare our system with the top systems re-ported in the CoNLL shared task 2012 as well aswith the Stanford?s publicly released rule-based sys-tem (Lee et al 2013; Lee et al 2011), which wonthe CoNLL 2011 Shared Task (Pradhan et al 2011).Note that all the systems use the same annotations(e.g., gender prediction, part-of-speech tags, nameentity tags) provided by the shared task organizers.607However, each system implements its own mentiondetector and pipelines the identified mentions intothe coreference clustering component.
Moreover,different systems use a different set of features.
Inorder to partially control for errors on mention de-tection and better evaluate the clustering componentin our coreference system, we will also present re-sults on correct (gold) mentions in the next section.Table 1 shows the end-to-end results.
On thedevelopment set, only the best performing systemof Fernandes et al(2012) is better than L3M, but thisdifference disappears when we use our system withconstraints, CL3M.
Although our system is muchsimple, it achieves the best B3 score on the test setand is competitive with the best system participatedin the CoNLL shared task 2012.Performance on named entities: The corefer-ence annotation in Ontonotes 5.0 includes varioustypes of mentions.
However, not all mention typesare equally interesting.
In particular, clusters whichcontain at least one proper name or a named entitymention are more important for information extrac-tion tasks like Wikification (Mihalcea and Csomai,2007; Ratinov et al 2011), cross-document coref-erence resolution (Bagga and Baldwin, 1998), andentity linking and knowledge based population (Jiand Grishman, 2011).Inspired by this, we compare our system to thebest systems in the CoNLL shared task of 2011(Stanford (Lee et al 2011)) and 2012 (Fernan-des (Fernandes et al 2012)) on the following spe-cific tasks on Ontonotes-5.0.?
ENT-C: Evaluate the system on clusters thatcontain at least one proper name mention.
Wegenerate the gold annotation and system out-puts by using the gold and predicted name en-tity tag annotations provided by the CoNLLshard task 2012.
That is, if a cluster does notinclude any name entity mention, then it willbe removed from the final clustering.?
PER-C: As in the construction of ENT-C, buthere we only consider clusters which contain atleast one ?Person (PER)?
entity.?
ORG-C: As in the construction of Entity-C, buthere we only consider clusters which contain atleast one ?Organization (ORG)?
entity.Typically, the clusters that get ignored in the abovedefinitions contain only first and second personTask Stanford Fernandes L3M CL3MENT-C 44.06 47.05 46.63 48.02PER-C 34.04 36.43 37.01 37.57ORG-C 25.02 26.23 26.22 27.01Table 2: Performance on named entities for OntoNotes-5.0 data.
We compare our system to Fernandes (Fernan-des et al 2012) and Stanford (Lee et al 2013) systems.pronouns (which often happens in transcribed dis-course.)
Also note that all the systems are trainedwith the same name entity tags, provided by theshared task organizers, and we use the same nameentity tags to construct the specific clustering.
Also,in order to further ensure fairness, we do not tuneour system to favor the evaluation of these specifictypes of clusters.
We chose to do so because we onlyhave access to the system output of Fernandes et al(2012).Table 2 shows the results.
The performance ofall systems degrades when considering only clustersthat contain name entities, indicating that ENT-C isactually a harder task than the original coreferenceresolution problem.
In particular, resolving ORGcoreferent clusters is hard, because names of organi-zations are sometimes confused with person names,and they can be referred to using a range of pronouns(including ?we?
and ?it?).
Overall, CL3M outper-forms all the competing systems on the clusters thatcontain at least one specific type of entity by a mar-gin larger than that for the overall coreference.5.3 Analysis on Gold MentionsTo better understand the contribution of our jointlearning and clustering model, we present experi-ments assuming that gold mentions are given.
Thedefinitions of gold mentions in ACE and Ontonotesare different because Ontonotes-5.0 excludes single-ton clusters in the annotation.
In addition, Ontonotesincludes longer mentions; for example, it includesNP and appositives in the same mention.
We com-pare with the publicly available Stanford (Lee et al2011) and IllinoisCoref (Chang et al 2012a) sys-tems; the system of Fernandes et al(2012) is notpublicly available.
In addition, we also comparewith the following two structured prediction base-lines that use the same set of features as L3M andPL3M.608MUC BCUB CEAFe AVGACE 2004 Gold Ment.All-Link-Red.
77.45 81.10 77.57 78.71Spanning 73.31 79.25 74.66 75.74IllinoisCoref 76.02 81.04 77.6 78.22Stanford 75.04 80.45 76.75 77.41(Stoyanov and Eisner, 2012) 80.1 81.8 - -L3M 77.57 81.77 78.15 79.16PL3M 78.18 82.09 79.21 79.83CL3M 78.17 81.64 78.45 79.42CPL3M 78.29 82.20 79.26 79.91Ontonotes 5.0 Gold Ment.All-Link-Red.
83.72 75.59 64.00 74.44Spanning 83.64 74.83 61.07 73.18IllinoisCoref 80.84 74.29 65.96 73.70Stanford 82.26 76.82 61.69 73.59L3M 83.44 78.12 64.56 75.37PL3M 83.97 78.25 65.69 75.97CL3M 84.10 78.30 68.74 77.05CPL3M 84.80 78.74 68.75 77.43Table 3: Performance on ACE 2004 and OntoNotes-5.0.All-Link-Red.
is based on correlational clustering; Span-ning is based on latent spanning forest based clustering(see Sec.
2).
Our proposed approach is L3M (Sec.
3) andPL3M (sec.
4).
CL3M and CPL3M are the version withincorporating constraints.1.
All-Link-Red: a reduced and faster alterna-tive to the correlational clustering based ap-proach (Finley and Joachims, 2005).
We im-plemented this algorithm as an ILP and dropedone of the three transitivity constraints for eachtriplet of mention variables.
Following Pascaland Baldridge (2009) and Chang et al(2011)we observe that this slightly improves the ac-curacy over a pure correlation clustering ap-proach, in addition to speeding up inference.2.
Spanning: the latent spanning forest based ap-proach presented by Yu and Joachims (2009).We use the publicly available implementationprovided by the authors3 for the ACE data;since their CCCP implementation is slow, weimplemented our own stochastic gradient de-scent version to scale it to the much largerOntonotes data.3Available at http://www.cs.cornell.edu/ cnyu/latentssvm/Table 3 lists the results.
Although L3M is simpleand use only the features defined on pairwise men-tions, it compares favorably with all recently pub-lished results.
Moreover, the probabilistic general-ization of L3M, PL3M, achieves even better perfor-mance.
For example, L3M with ?
= 0.2 improvesL3M with ?
= 0 by 0.7 points in ACE 2004.
In par-ticular, This shows that considering more than a oneleft-links is helpful.
This is in contrast with the pre-dicted mentions where ?
= 0 performed best.
Wesuspect that this is because noisy mentions can hurtthe performance of PL3M that takes into accountnot just the best scoring links, but also weaker linkswhich are likely to be less reliable (more false pos-itives).
Also, as opposed to what is reported by Yuand Joachims (2009), the correlation clustering ap-proach performs better than the spanning forest ap-proach.
We think that this is because we comparethe systems on different metrics than they did andalso because we use exact ILP inference for corre-lational clustering whereas Yu and Joachims (2009)used approximate greedy inference.Both L3M and PL3M can be benefit from usingconstraints.
However, The constraints improve onlymarginally on the ACE 2004 data because ACE usesshorter phrases as mentions.
Consequently, con-straints designed for leveraging information fromlong mention spans are less effective.
Overall, theexperiments show that L3M and PL3M perform wellon modeling coreference clustering.5.4 Ablation Study of ConstrainsFinally, we study the value of individual constraintsby adding one constraint at a time to the corefer-ence system starting with the simple L3M model.The system with all the constraints added is theCL3M model introduced in Table 1.
We then re-move individual constraints from CL3M to assessits contribution.
Table 4 shows the results on theOntonotes dataset with predicted mentions.
Overall,it is shown that each one of the constraints has a con-tribution, and that using all the constraints improvesthe performance of the system by 1.29% in the AVGF1 score.
In particular, most of this improvement(1.19%) is due to the must-link constraints (the firstfour constraints in the table).
The must-link con-straints are more useful for L3M as L3M achieveshigher precision than recall (e.g., the precision and609MUC BCUB CEAFe AVGL3M 67.88 71.88 47.16 62.30+SameSpan 68.27 72.27 47.73 62.75+SameDetNom 68.79 72.57 48.30 63.22+SameProperName 69.11 72.81 48.56 63.49+ModifierMismatch 69.11 72.81 48.58 63.50+PropertyMismatch 69.20 72.89 48.67 63.59(i.e.
CL3M)-SameSpan 68.91 72.66 48.36 63.31-SameDetNom 68.62 72.51 48.06 63.06-SameProperName 68.97 72.69 48.50 63.39-ModifierMismatch 69.12 72.80 48.63 63.52-PropertyMismatch 69.11 72.81 48.58 63.50Table 4: Ablation study on constraints.
We first showcumulative performance on OntoNotes-5.0 data with pre-dicted mentions as constraints are added one at a time intothe coreference system.
Then we demonstrate the valueof individual constraints by leaving out one constraint ateach time.recall of L3M are 78.38% and 67.96%, respectivelyin B3).
As a result, the must-link constraints, whichaim at improving the recall, do better when optimiz-ing F1.6 ConclusionsWe presented a principled yet simple framework forcoreference resolution.
Furthermore, we showedthat our model can be augmented in a straightfor-ward way with knowledge based constraints, to im-prove performance.
We also presented a probabilis-tic generalization of this model that can take intoaccount entity-mention links by considering mul-tiple possible coreference links.
We proposed afast stochastic gradient-based learning technique forour model.
Our model, while operating at men-tion pair granularity, obtains state-of-the-art resultson OntoNotes-5.0, and performs especially well onmention clusters containing named entities.
We pro-vided a detailed analysis of our experimental results.Acknowledgments Supported by the Intelligence AdvancedResearch Projects Activity (IARPA) via Department of Interior NationalBusiness Center contract number D11PC20155.
The U.S. Governmentis authorized to reproduce and distribute reprints for Governmental pur-poses notwithstanding any copyright annotation thereon.
Disclaimer:The views and conclusions contained herein are those of the authors andshould not be interpreted as necessarily representing the official policiesor endorsements, either expressed or implied, of IARPA, DoI/NBC, orthe U.S. Government.ReferencesA.
Bagga and B. Baldwin.
1998.
Algorithms for scoringcoreference chains.
In In The First International Con-ference on Language Resources and Evaluation Work-shop on Linguistics Coreference.M.
Bansal and D. Klein.
2012.
Coreference semanticsfrom web features.
In Proceedings of ACL, Jeju Island,South Korea, July.N.
Bansal, A. Blum, and S. Chawla.
2002.
Correlationclustering.
In Proceedings of the 43rd Symposium onFoundations of Computer Science.E.
Bengtson and D. Roth.
2008.
Understanding the valueof features for coreference resolution.
In EMNLP, 10.A.
Bjo?rkelund and R. Farkas.K.-W. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,M.
Sammons, and D. Roth.
2011.
Inference protocolsfor coreference resolution.
In CoNLL Shared Task.K.-W. Chang, R. Samdani, A. Rozovskaya, M. Sammons,and D. Roth.
2012a.
Illinois-coref: The UI systemin the CoNLL-2012 Shared Task.
In CoNLL SharedTask.M.
Chang, L. Ratinov, and D. Roth.
2012b.
Structuredlearning with constrained conditional models.
Ma-chine Learning, 88(3):399?431, 6.C.
Chen and V. Ng.
2012.
Combining the best of twoworlds: A hybrid approach to multilingual corefer-ence resolution.
In Joint Conference on EMNLP andCoNLL - Shared Task.J.
Clarke and M. Lapata.
2006.
Constraint-basedsentence compression: An integer programming ap-proach.
In Proceedings of the Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 144?151, Sydney, Australia, July.
ACL.A.
Culotta, M. Wick, R. Hall, and A. McCallum.
2007.First-order probabilistic models for coreference reso-lution.
In HLT/NAACL.P.
Denis and J. Baldridge.
2007.
Joint determination ofanaphoricity and coreference resolution using integerprogramming.
In Proceedings of the Annual Meetingof the North American Association of ComputationalLinguistics (NAACL).P.
Denis and J. Baldridge.
2008.
Specialized models andranking for coreference resolution.
In EMNLP, pages660?669.G.
Durrett, D. Hall, and D. Klein.
2013.
Decentral-ized entity-level modeling for coreference resolution.In Proceedings of ACL, August.610E.
R. Fernandes, C. N. dos Santos, and R. L. Milidiu?.2012.
Latent structure perceptron with feature induc-tion for unrestricted coreference resolution.
In JointConference on EMNLP and CoNLL - Shared Task.T.
Finley and T. Joachims.
2005.
Supervised cluster-ing with support vector machines.
In Proceedingsof the International Conference on Machine Learning(ICML).K.
Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.2010.
Posterior regularization for structured latentvariable models.
Journal of Machine Learning Re-search.K.
Gimpel and N. A. Smith.
2010.
Softmax-marginCRFs: Training log-linear models with cost functions.In NAACL.A.
Haghighi and D. Klein.
2010.
Coreference resolutionin a modular, entity-centered model.
In NAACL.H.
Ji and R. Grishman.
2011.
Knowledge base popula-tion: successful approaches and challenges.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies - Volume 1.T.
Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-tag.
2010.
Dual decomposition for parsing with non-projective head automata.
In EMNLP.H.
Lee, Y. Peirsman, A. Chang, N. Chambers, M. Sur-deanu, and D. Jurafsky.
2011.
Stanford?s multi-pass sieve coreference resolution system at the conll-2011 shared task.
In Proceedings of the CoNLL-2011Shared Task.H.
Lee, A. Chang, Y. Peirsman, N. Chambers, M. Sur-deanu, and D. Jurafsky.
2013.
Deterministic coref-erence resolution based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4).X.
Luo.
2005.
On coreference resolution performancemetrics.
In EMNLP.S.
Martschat, J. Cai, S. Broscheit, ?E.
Mu?jdricza-Maydt,and M. Strube.
2012.
A multigraph model for corefer-ence resolution.
In Joint Conference on EMNLP andCoNLL - Shared Task, July.A.
McCallum and B. Wellner.
2003.
Toward condi-tional models of identity uncertainty with applicationto proper noun coreference.
In The Conference onAdvances in Neural Information Processing Systems(NIPS).R.
Mihalcea and A. Csomai.
2007.
Wikify!
: linking doc-uments to encyclopedic knowledge.
In CIKM.V.
Ng and C. Cardie.
2002.
Improving machine learningapproaches to coreference resolution.
In ACL.Vincent Ng.
2005.
Supervised ranking for pronoun res-olution: Some recent improvements.
In AAAI, pages1081?1086.NIST.
2004.
The ACE evaluation plan.D.
Pascal and J. Baldridge.
2009.
Global joint models forcoreference resolution and named entity classification.In Procesamiento del Lenguaje Natural.P.
Pletscher, C. S. Ong, and J. M. Buhmann.
2010.
En-tropy and margin maximization for structured outputlearning.
In ECML PKDD.S.
Pradhan, L. Ramshaw, M. Marcus, M. Palmer,R.
Weischedel, and N. Xue.
2011.
Conll-2011 sharedtask: Modeling unrestricted coreference in ontonotes.In CoNLL.S.
Pradhan, A. Moschitti, N. Xue, O. Uryupina, andY.
Zhang.
2012.
CoNLL-2012 shared task: Modelingmultilingual unrestricted coreference in OntoNotes.
InCoNLL 2012.M.
Sammons Y. Tu V. Vydiswaran Q.
Do, D. Roth.
2009.Robust, light-weight approaches to compute lexicalsimilarity.
Technical report.K.
Raghunathan, H. Lee, S. Rangarajan, N. Chambers,M.
Surdeanu, D. Jurafsky, and C. Manning.
2010.A multi-pass sieve for coreference resolution.
InEMNLP.A.
Rahman and V. Ng.
2011a.
Coreference resolutionwith world knowledge.
In ACL, pages 814?824.A.
Rahman and V. Ng.
2011b.
Ensemble-based corefer-ence resolution.
In IJCAI.A.
Rahman and V. Ng.
2011c.
Narrowing the modelinggap: a cluster-ranking approach to coreference resolu-tion.
JAIR.W.M.
Rand.
1971.
Objective criteria for the evaluationof clustering methods.
Journal of the American Statis-tical Association, 66(336):846?850.L.
Ratinov and D. Roth.
2012.
Learning-based multi-sieve co-reference resolution with knowledge.
InEMNLP.L.
Ratinov, D. Roth, D. Downey, and M. Anderson.2011.
Local and global algorithms for disambiguationto wikipedia.
In Proc.
of the Annual Meeting of theAssociation for Computational Linguistics (ACL).Dan Roth and Wen Tau Yih.
2004.
A linear program-ming formulation for global inference in natural lan-guage tasks.
In Proceedings of CoNLL-04, pages 1?8.R.
Samdani, M. Chang, and D. Roth.
2012.
Unified ex-pectation maximization.
In NAACL.W.
M. Soon, H. T. Ng, and D. C. Y. Lim.
2001.
A ma-chine learning approach to coreference resolution ofnoun phrases.
Comput.
Linguist.V.
Stoyanov and J. Eisner.
2012.
Easy-first coreferenceresolution.
In COLING, pages 2519?2534.V.
Stoyanov, N. Gilbert, C. Cardie, and E. Riloff.
2009.Conundrums in noun phrase coreference resolution:making sense of the state-of-the-art.
In ACL.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreference611scoring scheme.
In Proceedings of the 6th conferenceon Message understanding.C.
Yu and T. Joachims.
2009.
Learning structural svmswith latent variables.
In Proceedings of the Interna-tional Conference on Machine Learning (ICML).A.
L. Yuille and A. Rangarajan.
2003.
The concave-convex procedure.
Neural Computation, 15(4).612
