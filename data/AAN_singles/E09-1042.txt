Proceedings of the 12th Conference of the European Chapter of the ACL, pages 363?371,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsWeakly Supervised Part-of-Speech Tagging for Morphologically-Rich,Resource-Scarce LanguagesKazi Saidul Hasan and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{saidul,vince}@hlt.utdallas.eduAbstractThis paper examines unsupervised ap-proaches to part-of-speech (POS) taggingfor morphologically-rich, resource-scarcelanguages, with an emphasis on Goldwa-ter and Griffiths?s (2007) fully-Bayesianapproach originally developed for En-glish POS tagging.
We argue that ex-isting unsupervised POS taggers unreal-istically assume as input a perfect POSlexicon, and consequently, we proposea weakly supervised fully-Bayesian ap-proach to POS tagging, which relaxes theunrealistic assumption by automaticallyacquiring the lexicon from a small amountof POS-tagged data.
Since such relaxationcomes at the expense of a drop in tag-ging accuracy, we propose two extensionsto the Bayesian framework and demon-strate that they are effective in improv-ing a fully-Bayesian POS tagger for Ben-gali, our representative morphologically-rich, resource-scarce language.1 IntroductionUnsupervised POS tagging requires neither man-ual encoding of tagging heuristics nor the avail-ability of data labeled with POS information.Rather, an unsupervised POS tagger operates byonly assuming as input a POS lexicon, which con-sists of a list of possible POS tags for each word.As we can see from the partial POS lexicon forEnglish in Figure 1, ?the?
is unambiguous with re-spect to POS tagging, since it can only be a deter-miner (DT), whereas ?sting?
is ambiguous, sinceit can be a common noun (NN), a proper noun(NNP) or a verb (VB).
In other words, the lexi-con imposes constraints on the possible POS tagsWord POS tag(s)... ...running NN, JJsting NN, NNP, VBthe DT...
...Figure 1: A partial lexicon for Englishof each word, and such constraints are then usedby an unsupervised tagger to label a new sentence.Conceivably, tagging accuracy decreases with theincrease in ambiguity: unambiguous words suchas ?the?
will always be tagged correctly; on theother hand, unseen words (or words not presentin the POS lexicon) are among the most ambigu-ous words, since they are not constrained at alland therefore can receive any of the POS tags.Hence, unsupervised POS tagging can present sig-nificant challenges to natural language processingresearchers, especially when a large fraction ofthe words are ambiguous.
Nevertheless, the de-velopment of unsupervised taggers potentially al-lows POS tagging technologies to be applied to asubstantially larger number of natural languages,most of which are resource-scarce and, in particu-lar, have little or no POS-tagged data.The most common approach to unsupervisedPOS tagging to date has been to train a hiddenMarkov model (HMM) in an unsupervised man-ner to maximize the likelihood of an unannotatedcorpus, using a special instance of the expectation-maximization (EM) algorithm (Dempster et al,1977) known as Baum-Welch (Baum, 1972).More recently, a fully-Bayesian approach to un-supervised POS tagging has been developed byGoldwater and Griffiths (2007) [henceforth G&G]as a viable alternative to the traditional maximum-likelihood-based HMM approach.
While unsuper-vised POS taggers adopting both approaches have363demonstrated promising results, it is important tonote that they are typically evaluated by assumingthe availability of a perfect POS lexicon.
This as-sumption, however, is fairly unrealistic in practice,as a perfect POS lexicon can only be constructedby having a linguist manually label each word ina language with its possible POS tags.1 In otherwords, the labor-intensive POS lexicon construc-tion process renders unsupervised POS taggers alot less unsupervised than they appear.
To makethese unsupervised taggers practical, one could at-tempt to automatically construct a POS lexicon, atask commonly known as POS induction.
How-ever, POS induction is by no means an easy task,and it is not clear how well unsupervised POS tag-gers work when used in combination with an au-tomatically constructed POS lexicon.The goals of this paper are three-fold.
First,motivated by the successes of unsupervised ap-proaches to English POS tagging, we aim to inves-tigate whether such approaches, especially G&G?sfully-Bayesian approach, can deliver similar per-formance for Bengali, our representative resource-scarce language.
Second, to relax the unrealis-tic assumption of employing a perfect lexicon asin existing unsupervised POS taggers, we proposea weakly supervised fully-Bayesian approach toPOS tagging, where we automatically construct aPOS lexicon from a small amount of POS-taggeddata.
Hence, unlike a perfect POS lexicon, our au-tomatically constructed lexicon is necessarily in-complete, yielding a large number of words thatare completely ambiguous.
The high ambiguityrate inherent in our weakly supervised approachsubstantially complicates the POS tagging pro-cess.
Consequently, our third goal of this paper isto propose two potentially performance-enhancingextensions to G&G?s Bayesian POS tagging ap-proach, which exploit morphology and techniquessuccessfully used in supervised POS tagging.The rest of the paper is organized as follows.Section 2 presents related work on unsupervisedapproaches to POS tagging.
Section 3 gives anintroduction to G&G?s fully-Bayesian approachto unsupervised POS tagging.
In Section 4, wedescribe our two extensions to G&G?s approach.Section 5 presents experimental results on BengaliPOS tagging, focusing on evaluating the effective-1When evaluating an unsupervised POS tagger, re-searchers typically construct a pseudo-perfect POS lexiconby collecting the possible POS tags of a word directly fromthe corpus on which the tagger is to be evaluated.ness of our two extensions in improving G&G?sapproach.
Finally, we conclude in Section 6.2 Related WorkWith the notable exception of Synder etal.
?s (2008; 2009) recent work on unsupervisedmultilingual POS tagging, existing approaches tounsupervised POS tagging have been developedand tested primarily on English data.
For instance,Merialdo (1994) uses maximum likelihood esti-mation to train a trigram HMM.
Schu?tze (1995)and Clark (2000) apply syntactic clustering anddimensionality reduction in a knowledge-freesetting to obtain meaningful clusters.
Haghighiand Klein (2006) develop a prototype-drivenapproach, which requires just a few prototypeexamples for each POS tag and exploits theselabeled words to constrain the labels of theirdistributionally similar words.
Smith and Eisner(2005) train an unsupervised POS tagger usingcontrastive estimation, which seeks to moveprobability mass to a positive example e fromits neighbors (i.e., negative examples are createdby perturbing e).
Wang and Schuurmans (2005)improve an unsupervised HMM-based tagger byconstraining the learned structure to maintainappropriate marginal tag probabilities and usingword similarities to smooth the lexical parameters.As mentioned before, Goldwater and Griffiths(2007) have recently proposed an unsupervisedfully-Bayesian POS tagging framework that op-erates by integrating over the possible parametervalues instead of fixing a set of parameter valuesfor unsupervised sequence learning.
Importantly,this Bayesian approach facilitates the incorpora-tion of sparse priors that result in a more practicaldistribution of tokens to lexical categories (John-son, 2007).
Similar to Goldwater and Griffiths(2007) and Johnson (2007), Toutanova and John-son (2007) also use Bayesian inference for POStagging.
However, their work departs from exist-ing Bayesian approaches to POS tagging in thatthey (1) introduce a new sparse prior on the dis-tribution over tags for each word, (2) extend theLatent Dirichlet Allocation model, and (3) explic-itly model ambiguity class.
While their taggingmodel, like Goldwater and Griffiths?s, assumes asinput an incomplete POS lexicon and a large unla-beled corpus, they consider their approach ?semi-supervised?
simply because of the human knowl-edge involved in constructing the POS lexicon.3643 A Fully Bayesian Approach3.1 MotivationAs mentioned in the introduction, the most com-mon approach to unsupervised POS tagging is totrain an HMM on an unannotated corpus using theBaum-Welch algorithm so that the likelihood ofthe corpus is maximized.
To understand what theHMM parameters are, let us revisit how an HMMsimultaneously generates an output sequence w= (w0, w1, ..., wn) and the associated hidden statesequence t = (t0, t1, ..., tn).
In the context of POStagging, each state of the HMM corresponds to aPOS tag, the output sequence w is the given wordsequence, and the hidden state sequence t is theassociated POS tag sequence.
To generate w andt, the HMM begins by guessing a state t0 and thenemitting w0 from t0 according to a state-specificoutput distribution over word tokens.
After that,we move to the next state t1, the choice of whichis based on t0?s transition distribution, and emitw1 according to t1?s output distribution.
This gen-eration process repeats until the end of the wordsequence is reached.
In other words, the parame-ters of an HMM, ?, are composed of a set of state-specific (1) output distributions (over word tokens)and (2) transition distributions, both of which canbe learned using the EM algorithm.
Once learningis complete, we can use the resulting set of param-eters to find the most likely hidden state sequencegiven a word sequence using the Viterbi algorithm.Nevertheless, EM sometimes fails to find goodparameter values.2 The reason is that EM tries toassign roughly the same number of word tokens toeach of the hidden states (Johnson, 2007).
In prac-tice, however, the distribution of word tokens toPOS tags is highly skewed (i.e., some POS cate-gories are more populated with tokens than oth-ers).
This motivates a fully-Bayesian approach,which, rather than committing to a particular setof parameter values as in an EM-based approach,integrates over all possible values of ?
and, mostimportantly, allows the use of priors to favor thelearning of the skewed distributions, through theuse of the term P (?|w) in the following equation:P (t|w) =?P (t|w, ?
)P (?|w)d?
(1)The question, then, is: which priors on ?
wouldallow the acquisition of skewed distributions?
To2When given good parameter initializations, however, EMcan find good parameter values for an HMM-based POS tag-ger.
See Goldberg et al (2008) for details.answer this question, recall that in POS tagging, ?is composed of a set of tag transition distributionsand output distributions.
Each such distribution isa multinomial (i.e., each trial produces exactly oneof some finite number of possible outcomes).
Fora multinomial with K outcomes, a K-dimensionalDirichlet distribution, which is conjugate to themultinomial, is a natural choice of prior.
For sim-plicity, we assume that a distribution in ?
is drawnfrom a symmetric Dirichlet with a certain hyper-parameter (see Teh et al (2006) for details).The value of a hyperparameter, ?, affects theskewness of the resulting distribution, as it as-signs different probabilities to different distribu-tions.
For instance, when ?
< 1, higher proba-bilities are assigned to sparse multinomials (i.e.,multinomials in which only a few entries are non-zero).
Intuitively, the tag transition distributionsand the output distributions in an HMM-basedPOS tagger are sparse multinomials.
As a re-sult, it is logical to choose a Dirichlet prior with?
< 1.
By integrating over all possible param-eter values, the probability that i-th outcome, yi,takes the value k, given the previous i ?
1 out-comes y?i= (y1, y2, ..., yi?1), isP (k|y?i, ?)
=?P (k|?
)P (?|y?i, ?)d?
(2)= nk + ?i?
1 + K?
(3)where nk is the frequency of k in y?i.
SeeMacKay and Peto (1995) for the derivation.3.2 ModelOur baseline POS tagging model is a standard tri-gram HMM with tag transition distributions andoutput distributions, each of which is a sparsemultinomial that is learned by applying a symmet-ric Dirichlet prior:ti | ti?1, ti?2, ?
(ti?1,ti?2) ?
Mult(?
(ti?1,ti?2))wi | ti, ?
(ti) ?
Mult(?(ti))?
(ti?1,ti?2) | ?
?
Dirichlet(?)?
(ti) | ?
?
Dirichlet(?
)where wi and ti denote the i-th word and tag.
Witha tagset of size T (including a special tag used assentence delimiter), each of the tag transition dis-tributions has T components.
For the output sym-bols, each of the ?
(ti) has Wti components, whereWti denotes the number of word types that can beemitted from the state corresponding to ti.365From the closed form in Equation 3, given pre-vious outcomes, we can compute the tag transitionand output probabilities of the model as follows:P (ti|t?i, ?)
=n(ti?2,ti?1,ti) + ?n(ti?2,ti?1) + T?
(4)P (wi|ti, t?i,w?i, ?)
=n(ti,wi) + ?nti + Wti?
(5)where n(ti?2,ti?1,ti) and n(ti,wi) are the frequen-cies of observing the tag trigram (ti?2, ti?1, ti)and the tag-word pair (ti, wi), respectively.
Thesecounts are taken from the i ?
1 tags and wordsgenerated previously.
The inference procedure de-scribed next exploits the property that trigrams(and outputs) are exchangeable; that is, the prob-ability of a set of trigrams (and outputs) does notdepend on the order in which it was generated.3.3 Inference ProcedureWe perform inference using Gibbs sampling (Ge-man and Geman, 1984), using the following pos-terior distribution to generate samples:P (t|w, ?, ?)
?
P (w|t, ?
)P (t|?
)Starting with a random assignment of a POS tagto each word (subject to the constraints in the POSlexicon), we resample each POS tag, ti, accord-ing to the conditional distribution shown in Figure2.
Note that the current counts of other trigramsand outputs can be used as ?previous?
observa-tions due to the property of exchangeability.Following G&G, we use simulated annealing tofind the MAP tag sequence.
The temperature de-creases by a factor of exp(log( ?2?1 )N?1 ) after each iter-ation, where ?1 is the initial temperature and ?2 isthe temperature after N sampling iterations.4 Two ExtensionsIn this section, we present two extensions toG&G?s fully-Bayesian framework to unsupervisedPOS tagging, namely, induced suffix emission anddiscriminative prediction.4.1 Induced Suffix EmissionFor morphologically-rich languages like Bengali,a lot of grammatical information (e.g., POS) is ex-pressed via suffixes.
In fact, several approaches tounsupervised POS induction for morphologically-rich languages have exploited the observation thatsome suffixes can only be associated with a smallnumber of POS tags (e.g., Clark (2003), Dasguptaand Ng (2007)).
To exploit suffixes in HMM-based POS tagging, one can (1) convert the word-based POS lexicon to a suffix-based POS lexicon,which lists the possible POS tags for each suffix;and then (2) have the HMM emit suffixes ratherthan words, subject to the constraints in the suffix-based POS lexicon.
Such a suffix-based HMM,however, may suffer from over-generalization.
Toprevent over-generalization and at the same timeexploit suffixes, we propose as our first exten-sion to G&G?s framework a hybrid approach toword/suffix emission: a word is emitted if it ispresent in the word-based POS lexicon; otherwise,its suffix is emitted.
In other words, our approachimposes suffix-based constraints on the tagging ofwords that are unseen w.r.t.
the word-based POSlexicon.
Below we show how to induce the suffixof a word and create the suffix-based POS lexicon.Inducing suffixes To induce suffixes, we rely onKeshava and Pitler?s (2006) method.
Assume that(1) V is a vocabulary (i.e., a set of distinct words)extracted from a large, unannotated corpus, (2) C1and C2 are two character sequences, and (3) C1C2is the concatenation of C1 and C2.
If C1C2 andC1 are found in V , we extract C2 as a suffix.However, this unsupervised suffix inductionmethod is arguably overly simplistic and hencemany of the induced affixes could be spurious.
Toidentify suffixes that are likely to be correct, weemploy a simple procedure: we (1) score each suf-fix by multiplying its frequency (i.e., the numberof distinct words in V to which each suffix at-taches) and its length3, and (2) select only thosewhose score is above a certain threshold.
In ourexperiments, we set this threshold to 50, and gen-erate our vocabulary from five years of articlestaken from the Bengali newspaper Prothom Alo.This enables us to induce 975 suffixes.Constructing a suffix-based POS lexiconNext, we construct a suffix-based POS lexicon.For each word w in the original word-basedPOS lexicon, we (1) use the induced suffix listobtained in the previous step to identify thelongest-matching suffix of w, and then (2) assignall the POS tags associated with w to this suffix.Incorporating suffix-based output distributionsFinally, we extend our trigram model by introduc-3The dependence on frequency and length is motivated bythe observation that less frequent and shorter affixes are morelikely to be erroneous (see Goldsmith (2001)).366P (ti|t?i,w, ?, ?)
?n(ti,wi) + ?nti + Wti?.n(ti?2,ti?1,ti) + ?n(ti?2,ti?1) + T?.n(ti?1,ti,ti+1) + I(ti?2 = ti?1 = ti = ti+1) + ?n(ti?1,ti) + I(ti?2 = ti?1 = ti) + T?.n(ti,ti+1,ti+2) + I(ti?2 = ti = ti+2, ti?1 = ti+1) + I(ti?1 = ti = ti+1 = ti+2) + ?n(ti,ti+1) + I(ti?2 = ti, ti?1 = ti+1) + I(ti?1 = ti = ti+1) + T?Figure 2: The sampling distribution for ti (taken directly from Goldwater and Griffiths (2007)).
All nxvalues are computed from the current values of all tags except for ti.
Here, I(arg) is a function thatreturns 1 if arg is true and 0 otherwise, and t?i refers to the current values of all tags except for ti.ing a state-specific probability distribution over in-duced suffixes.
Specifically, if the current word ispresent in the word-based POS lexicon, or if wecannot find any suffix for the word using the in-duced suffix list, then we emit the word.
Other-wise, we emit its suffix according to a suffix-basedoutput distribution, which is drawn from a sym-metric Dirichlet with hyperparameter ?
:si | ti, ?
(ti) ?
Mult(?(ti))?
(ti) | ?
?
Dirichlet(?
)where si denotes the induced suffix of the i-thword.
The distribution, ?
(ti), has Sti components,where Sti denotes the number of induced suffixesthat can be emitted from the state corresponding toti.
We compute the induced suffix emission prob-abilities of the model as follows:P (si|ti, t?i, s?i, ?)
=n(ti,si) + ?nti + Sti?
(6)where n(ti,si) is the frequency of observing thetag-suffix pair (ti, si).This extension requires that we slightly modifythe inference procedure.
Specifically, if the cur-rent word is unseen (w.r.t.
the word-based POSlexicon) and has a suffix (according to the inducedsuffix list), then we sample from a distribution thatis almost identical to the one shown in Figure 2,except that we replace the first fraction (i.e., thefraction involving the emission counts) with theone shown in Equation (6).
Otherwise, we simplysample from the distribution in Figure 2.4.2 Discriminative PredictionAs mentioned in the introduction, the (word-based) POS lexicons used in existing approachesto unsupervised POS tagging were created some-what unrealistically by collecting the possiblePOS tags of a word directly from the corpus onwhich the tagger is to be evaluated.
To make thelexicon formation process more realistic, we pro-pose a weakly supervised approach to BayesianPOS tagging, in which we automatically create theword-based POS lexicon from a small set of POS-tagged sentences that is disjoint from the test data.Adopting a weakly supervised approach has an ad-ditional advantage: the presence of POS-taggedsentences makes it possible to exploit techniquesdeveloped for supervised POS tagging, which isthe idea behind discriminative prediction, our sec-ond extension to G&G?s framework.Given a small set of POS-tagged sentences L,discriminative prediction uses the statistics col-lected from L to predict the POS of a word in adiscriminative fashion whenever possible.
Morespecifically, discriminative prediction relies ontwo simple ideas typically exploited by supervisedPOS tagging algorithms: (1) if the target word(i.e., the word whose POS tag is to be predicted)appears in L, we can label the word with its POStag in L; and (2) if the target word does not appearin L but its context does, we can use its context topredict its POS tag.
In bigram and trigram POStaggers, the context of a word is represented us-ing the preceding one or two words.
Nevertheless,since L is typically small in a weakly supervisedsetting, it is common for a target word not to sat-isfy any of the two conditions above.
Hence, if it isnot possible to predict a target word in a discrim-inative fashion (due to the limited size of L), weresort to the sampling equation in Figure 2.To incorporate the above discriminative deci-sion steps into G&G?s fully-Bayesian frameworkfor POS tagging, the algorithm estimates threetypes of probability distributions from L. First,to capture context, it computes (1) a distribu-tion over the POS tags following a word bi-gram, (wi?2, wi?1), that appears in L [henceforthD1(wi?2, wi?1)] and (2) a distribution over thePOS tags following a word unigram, wi?1, that ap-pears in L [henceforth D2(wi?1)].
Then, to cap-367Algorithm 1 Algorithm for incorporating discrim-inative predictionInput: wi: current wordwi?1: previous wordwi?2: second previous wordL: a set of POS-tagged sentencesOutput: Predicted tag, ti1: if wi ?
L then2: ti ?
Tag drawn from the distribution of wi?s candi-date tags3: else if (wi?2, wi?1) ?
L then4: ti ?
Tag drawn from the distribution of the POS tagsfollowing the word bigram (wi?2, wi?1)5: else if wi?1 ?
L then6: ti ?
Tag drawn from the distribution of the POS tagsfollowing the word unigram wi?17: else8: ti ?
Tag obtained using the sampling equation9: end ifture the fact that a word can have more than onePOS tag, it also estimates a distribution over POStags for each word wi that appears in L [hence-forth D3(wi)].Implemented as a set of if-else clauses, the al-gorithm uses these three types of distributions totag a target word, wi, in a discriminative manner.First, it checks whether wi appears in L (line 1).
Ifso, it tags wi according to D3(wi).
Otherwise, itattempts to label wi based on its context.
Specifi-cally, if (wi?2, wi?1), the word bigram precedingwi, appears in L (line 3), then wi is tagged accord-ing to D1(wi?2, wi?1).
Otherwise, it backs off toa unigram distribution: if wi?1, the word preced-ing wi, appears in L (line 5), then wi is taggedaccording to D2(wi?1).
Finally, if it is not possi-ble to tag the word discriminatively (i.e., if all theabove cases fail), it resorts to the sampling equa-tion (lines 7?8).
We apply simulated annealing toall four cases in this iterative tagging procedure.5 Evaluation5.1 Experimental SetupCorpus Our evaluation corpus is the one usedin the shared task of the IJCNLP-08 Workshop onNER for South and South East Asian Languages.4Specifically, we use the portion of the Bengalidataset that is manually POS-tagged.
IIIT Hy-derabad?s POS tagset5, which consists of 26 tagsspecifically developed for Indian languages, hasbeen used to annotate the data.
The corpus is com-posed of a training set and a test set with approxi-4The corpus is available from http://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5.5http://shiva.iiit.ac.in/SPSAL2007/iiit tagset guidelines.pdfmately 50K and 30K tokens, respectively.
Impor-tantly, all our POS tagging results will be reportedusing only the test set; the training set will be usedfor lexicon construction, as we will see shortly.Tagset We collapse the set of 26 POS tags into15 tags.
Specifically, while we retain the tags cor-responding to the major POS categories, we mergesome of the infrequent tags designed to captureIndian language specific structure (e.g., reduplica-tion, echo words) into a category called OTHERS.Hyperparameter settings Recall that our tag-ger consists of three types of distributions ?
tagtransition distributions, word-based output distri-butions, and suffix-based output distributions ?drawn from a symmetric Dirichlet with ?, ?,and ?
as the underlying hyperparameters, respec-tively.
We automatically determine the values ofthese hyperparameters by (1) randomly initializ-ing them and (2) resampling their values by usinga Metropolis-Hastings update (Gilks et al, 1996)at the end of each sampling iteration.
Details ofthis update process can be found in G&G.Inference Inference is performed by running aGibbs sampler for 5000 iterations.
The initial tem-perature is set to 2.0, which is gradually loweredto 0.08 over the iterations.
Owing to the random-ness involved in hyperparameter initialization, allreported results are averaged over three runs.Lexicon construction methods To better under-stand the role of a POS lexicon in tagging perfor-mance, we evaluate each POS tagging model byemploying lexicons constructed by three methods.The first lexicon construction method, arguablythe most unrealistic among the three, follows thatof G&G: for each word, w, in the test set, we (1)collect from each occurrence of w in the trainingset and the test set its POS tag, and then (2) insertw and all the POS tags collected for w into thePOS lexicon.
This method is unrealistic because(1) in practice, a human needs to list all possiblePOS tags for each word in order to construct thislexicon, thus rendering the resulting tagger con-siderably less unsupervised than it appears; and(2) constructing the lexicon using the dataset onwhich the tagger is to be evaluated implies thatthere is no unseen word w.r.t.
the lexicon, thus un-realistically simplifies the POS tagging task.
Tomake the method more realistic, G&G also createa set of relaxed lexicons.
Each of these lexiconsincludes the tags for only the words that appearat least d times in the test corpus, where d ranges3681 2 3 4 5 6 7 8 9 1030405060708090dAccuracy(%)(a) Lexicon 1MLHMMBHMMBHMM+IS1 2 3 4 5 6 7 8 9 1030354045505560657075dAccuracy(%)(b) Lexicon 2MLHMMBHMMBHMM+ISFigure 3: Accuracies of POS tagging models using (a) Lexicon 1 and (b) Lexicon 2from 1 to 10 in our experiments.
Any unseen (i.e.,out-of-dictionary) word is ambiguous among the15 possible tags.
Not surprisingly, both ambigu-ity and the unseen word rate increase with d. Forinstance, the ambiguous token rate increases from40.0% with 1.7 tags/token (d=1) to 77.7% with 8.1tags/token (d=10).
Similarly, the unseen word rateincreases from 16% (d=2) to 46% (d=10).
We willrefer to this set of tag dictionaries as Lexicon 1.The second method generates a set of relaxedlexicons, Lexicon 2, in essentially the same wayas the first method, except that these lexicons in-clude only the words that appear at least d timesin the training data.
Importantly, the words thatappear solely in the test data are not included inany of these relaxed POS lexicons.
This makesLexicon 2 a bit more realistic than Lexicon 1 interms of the way they are constructed.
As a result,in comparison to Lexicon 1, Lexicon 2 has a con-siderably higher ambiguous token rate and unseenword rate: its ambiguous token rate ranges from64.3% with 5.3 tags/token (d=1) to 80.5% with 8.6tags/token (d=10), and its unseen word rate rangesfrom 25% (d=1) to 50% (d=10).The third method, arguably the most realisticamong the three, is motivated by our proposedweakly supervised approach.
In this method, we(1) form ten different datasets from the (labeled)training data of sizes 5K words, 10K words, .
.
.,50K words, and then (2) create one POS lexiconfrom each dataset L by listing, for each word w inL, all the tags associated with w in L. This set oftag dictionaries, which we will refer to as Lexicon3, has an ambiguous token rate that ranges from57.7% with 5.1 tags/token (50K) to 61.5% with8.1 tags/token (5K), and an unseen word rate thatranges from 25% (50K) to 50% (5K).5.2 Results and Discussion5.2.1 Baseline SystemsWe use as our first baseline system G&G?sBayesian POS tagging model, as our goal is toevaluate the effectiveness of our two extensionsin improving their model.
To further gauge theperformance of G&G?s model, we employ anotherbaseline commonly used in POS tagging exper-iments, which is an unsupervised trigram HMMtrained by running EM to convergence.As mentioned previously, we evaluate each tag-ging model by employing the three POS lexiconsdescribed in the previous subsection.
Figure 3(a)shows how the tagging accuracy varies with dwhen Lexicon 1 is used.
Perhaps not surpris-ingly, the trigram HMM (MLHMM) and G&G?sBayesian model (BHMM) achieve almost identi-cal accuracies when d=1 (i.e., the complete lexi-con with a zero unseen word rate).
As d increases,both ambiguity and the unseen word rate increase;as a result, the tagging accuracy decreases.
Also,consistent with G&G?s results, BHMM outper-forms MLHMM by a large margin (4?7%).Similar performance trends can be observedwhen Lexicon 2 is used (see Figure 3(b)).
How-ever, both baselines achieve comparatively lowertagging accuracies, as a result of the higher unseenword rate associated with Lexicon 2.3695 10 15 20 25 30 35 40 45 504550556065707580Training data (K)Accuracy(%)Lexicon 3SHMMBHMMBHMM+ISBHMM+IS+DPFigure 4: Accuracies of the POS tagging modelsusing Lexicon 3Results using Lexicon 3 are shown in Figure4.
Owing to the availability of POS-tagged sen-tences, we replace MLHMM with its supervisedcounterpart that is trained on the available labeleddata, yielding the SHMM baseline.
The accuraciesof SHMM range from 48% to 67%, outperformingBHMM as the amount of labeled data increases.5.2.2 Adding Induced Suffix EmissionNext, we augment BHMM with our firstextension, induced suffix emission, yieldingBHMM+IS.
For Lexicon 1, BHMM+IS achievesthe same accuracy as the two baselines when d=1.The reason is simple: as all the test words arein the POS lexicon, the tagger never emits an in-duced suffix.
More importantly, BHMM+IS beatsBHMM and MLHMM by 4?9% and 10?14%, re-spectively.
Similar trends are observed for Lex-icon 2, where BHMM+IS outperforms BHMMand MLHMM by a larger margin of 5?10% and12?16%, respectively.
For Lexicon 3, BHMM+ISoutperforms SHMM, the stronger baseline, by 6?11%.
Overall, these results suggest that inducedsuffix emission is a strong performance-enhancingextension to G&G?s approach.5.2.3 Adding Discriminative PredictionFinally, we augment BHMM+IS with discrimi-native prediction, yielding BHMM+IS+DP.
Sincethis extension requires labeled data, it can only beapplied in combination with Lexicon 3.
As seenin Figure 4, BHMM+IS+DP outperforms SHMMby 10?14%.
Its discriminative nature proves to bePredicted Tag Correct Tag % of ErrorNN NNP 8.4NN JJ 6.9VM VAUX 5.9Table 1: Most frequent POS tagging errors forBHMM+IS+DP on the 50K-word training setstrong as it even beats BHMM+IS by 3?4%.5.2.4 Error AnalysisTable 1 lists the most common types of er-rors made by the best-performing tagging model,BHMM+IS+DP (50K-word labeled data).
As wecan see, common nouns and proper nouns (row1) are difficult to distinguish, due in part to thecase insensitivity of Bengali.
Also, it is difficultto distinguish Bengali common nouns and adjec-tives (row 2), as they are distributionally similarto each other.
The confusion between main verbs[VM] and auxiliary verbs [VAUX] (row 3) arisesfrom the fact that certain Bengali verbs can serveas both a main verb and an auxiliary verb, depend-ing on the role the verb plays in the verb sequence.6 ConclusionsWhile Goldwater and Griffiths?s fully-Bayesianapproach and the traditional maximum-likelihoodparameter-based approach to unsupervised POStagging have offered promising results for English,we argued in this paper that such results were ob-tained under the unrealistic assumption that a per-fect POS lexicon is available, which renders thesetaggers less unsupervised than they appear.
As aresult, we investigated a weakly supervised fully-Bayesian approach to POS tagging, which relaxesthe unrealistic assumption by automatically ac-quiring the lexicon from a small amount of POS-tagged data.
Since such relaxation comes at theexpense of a drop in tagging accuracy, we pro-posed two performance-enhancing extensions tothe Bayesian framework, namely, induced suffixemission and discriminative prediction, which ef-fectively exploit morphology and techniques fromsupervised POS tagging, respectively.AcknowledgmentsWe thank the three anonymous reviewers andSajib Dasgupta for their comments.
We also thankCRBLP, BRAC University, Bangladesh, for pro-viding us with Bengali resources and Taufiq HasanAl Banna for his MATLAB code.
This work wassupported in part by NSF Grant IIS-0812261.370ReferencesLeonard E. Baum.
1972.
An equality and associ-ated maximization technique in statistical estimationfor probabilistic functions of Markov processes.
In-equalities, 3:1?8.Alexander Clark.
2000.
Inducing syntactic categoriesby context distribution clustering.
In Proceedings ofCoNLL: Short Papers, pages 91?94.Alexander Clark.
2003.
Combining distributional andmorphological information for part-of-speech induc-tion.
In Proceedings of the EACL, pages 59?66.Sajib Dasgupta and Vincent Ng.
2007.
Unsupervisedpart-of-speech acquisition for resource-scarce lan-guages.
In Proceedings of EMNLP-CoNLL, pages218?227.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incompletedata via the EM algorithm.
Journal of the Royal Sta-tistical Society.
Series B (Methodological), 39:1?38.Stuart Geman and Donald Geman.
1984.
Stochas-tic relaxation, Gibbs distributions, and the Bayesianrestoration of images.
IEEE Transactions on PatternAnalysis and Machine Intelligence, 6:721?741.Walter R. Gilks, Sylvia Richardson, and DavidJ.
Spiegelhalter (editors).
1996.
Markov ChainMonte Carlo in Practice.
Chapman & Hall, Suffolk.Yoav Goldberg, Meni Adler, and Michael Elhadad.2008.
EM can find pretty good HMM POS-taggers(when given a good start).
In Proceedings of ACL-08:HLT, pages 746?754.John Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
ComputationalLinguistics, 27(2):153?198.Sharon Goldwater and Thomas L. Griffiths.
2007.A fully Bayesian approach to unsupervised part-of-speech tagging.
In Proceedings of the ACL, pages744?751.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofHLT-NAACL, pages 320?327.Mark Johnson.
2007.
Why doesn?t EM find goodHMM POS-taggers?
In Proceedings of EMNLP-CoNLL, pages 296?305.Samarth Keshava and Emily Pitler.
2006.
A simpler,intuitive approach to morpheme induction.
In PAS-CAL Challenge Workshop on Unsupervised Segmen-tation of Words into Morphemes.David J. C. MacKay and Linda C. Bauman Peto.
1995.A hierarchical Dirichlet language model.
NaturalLanguage Engineering, 1:289?307.Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational Linguistics,20(2):155?172.Hinrich Schu?tze.
1995.
Distributional part-of-speechtagging.
In Proceedings of EACL, pages 141?148.Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proceedings of the ACL, pages 354?362.Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,and Regina Barzilay.
2008.
Unsupervised multi-lingual learning for POS tagging.
In Proceedings ofEMNLP, pages 1041?1050.Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,and Regina Barzilay.
2009.
Adding more lan-guages improves unsupervised multilingual tagging.In Proceedings of NAACL-HLT.Yee Whye Teh, Michael Jordan, Matthew Beal, andDavid Blei.
2006.
Hierarchical Dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101(476):1527?1554.Kristina Toutanova and Mark Johnson.
2007.
ABayesian LDA-based model for semi-supervisedpart-of-speech tagging.
In Proceedings of NIPS.Qin Iris Wang and Dale Schuurmans.
2005.
Im-proved estimation for unsupervised part-of-speechtagging.
In Proceedings of the 2005 IEEE Interna-tional Conference on Natural Language Processingand Knowledge Engineering (IEEE NLP-KE), pages219?224.371
