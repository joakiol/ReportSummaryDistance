Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 557?567, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsExploiting Chunk-level Features to Improve Phrase ChunkingJunsheng Zhou    Weiguang Qu     Fen ZhangJiangsu Research Center of Information Security & Privacy TechnologySchool of Computer Science and TechnologyNanjing Normal University.
Nanjing, China, 210046Email:{zhoujs,wgqu}@njnu.edu.cn  zf9646@126.comAbstractMost existing systems solved the phrasechunking task with the sequence labelingapproaches, in which the chunk candidatescannot be treated as a whole during parsingprocess so that the chunk-level featurescannot be exploited in a natural way.
In thispaper, we formulate phrase chunking as ajoint segmentation and labeling task.
Wepropose an efficient dynamic programmingalgorithm with pruning for decoding,which allows the direct use of the featuresdescribing the internal characteristics ofchunk and the features capturing thecorrelations between adjacent chunks.
Arelaxed, online maximum margin trainingalgorithm is used for learning.
Within thisframework, we explored  a variety ofeffective feature representations forChinese phrase chunking.
Theexperimental results show that the use ofchunk-level features can lead to significantperformance improvement, and that ourapproach achieves state-of-the-artperformance.
In particular, our approach ismuch better at recognizing long andcomplicated phrases.1 IntroductionPhrase chunking is a Natural Language Processingtask that consists in dividing a text intosyntactically correlated parts of words.
Thesesphrases are non-overlapping, i.e., a word can onlybe a member of one chunk (Abney, 1991).Generally speaking, there are two phrase chunkingtasks, including text chunking (shallow parsing),and noun phrase (NP) chunking.
Phrase chunkingprovides a key feature that helps on moreelaborated NLP tasks such as parsing, semanticrole tagging and information extraction.There is a wide range of research work onphrase chunking based on machine learningapproaches.
However, most of the previous workreduced phrase chunking to sequence labelingproblems either by using the classification models,such as SVM (Kudo and Matsumoto, 2001),Winnow and voted-perceptrons (Zhang et al2002;Collins, 2002), or by using the sequence labelingmodels, such as Hidden Markov Models (HMMs)(Molina and Pla, 2002) and Conditional RandomFields (CRFs) (Sha and Pereira, 2003).
Whenapplying the sequence labeling approaches tophrase chunking, there exist two major problems.Firstly, these models cannot treat globally asequence of continuous words as a chunkcandidate, and thus cannot inspect the internalstructure of the candidate, which is an importantaspect of information in modeling phrase chunking.In particular, it makes impossible the use of localindicator function features of the type "the chunkconsists of POS tag sequence p1...,pk".
For example,the Chinese NP "??
/NN(agriculture) ?
?/NN(production) ?/CC(and) ?
?/NN(rural) ?
?/NN(economic) ??
/NN(development)" seemsrelatively difficult to be correctly recognized by asequence labeling approach due to its length.
But ifwe can treat the sequence of words as a whole anddescribe the formation pattern of POS tags of thischunk with a regular expression-like form"[NN]+[CC][NN]+", then it is more likely to becorrectly recognized, since this pattern might betterexpress the characteristics of its constituents.
Asanother example, consider the recognition ofspecial terms.
In Chinese corpus, there exists akind of NPs called special terms, such as "?
?
?557(Life) ??
(Forbidden Zone) ?
", which arebracketed with the particular punctuations like "?
, ?
, ?
, ?
, ?
, ?".
When recognizing thespecial terms, it is difficult for the sequencelabeling approaches to guarantee the matching ofparticular punctuations appearing at the startingand ending positions of a chunk.
For instance, thechunk candidate "?
??
(Life) ??(ForbiddenZone)?
is considered to be an invalid chunk.
Butit is easy to check this kind of punctuationmatching in a single chunk by introducing a chunk-level feature.Secondly, the sequence labeling models cannotcapture the correlations between adjacent chunks,which should be informative for the identificationof chunk boundaries and types.
In particular, wefind that some headwords in the sentence areexpected to have a stronger dependency relationwith their preceding headwords in precedingchunks than with their immediately precedingwords within the same chunk.
For example, in thefollowing sentence:" [?
?/PN(Bilateral)]_NP [?
?/NN(economicand trade) ?
?/NN(relations)]_NP [?/AD(just)?
?/AD(steadily) ?
?/VV(develop)]_VP "if we can find the three headwords "??
", "??
"and "??"
located in the three adjacent chunkswith some head-finding rules, then the headworddependency expressed by headword bigrams ortrigrams should be helpful to recognize thesechunks in this sentence.In summary, the inherent deficiency in applyingthe sequence labeling approaches to phrasechunking is that the chunk-level features onewould expect to be very informative cannot beexploited in a natural way.In this paper, we formulate phrase chunking as ajoint segmentation and labeling problem, whichoffers advantages over previous learning methodsby providing a natural formulation to exploit thefeatures describing the internal structure of a chunkand the features capturing the correlations betweenthe adjacent chunks.Within this framework, we explored  a variety ofeffective feature representations for Chinese phrasechunking.
The experimental results on Chinesechunking corpus as well as English chunkingcorpus show that the use of chunk-level featurescan lead to significant performance improvement,and that our approach performs better than otherapproaches based on the sequence labeling models.2 Related WorkIn recent years, many chunking systems based onmachine learning approaches have been presented.Some approaches rely on k-order generativeprobabilistic models, such as HMMs (Molina andPla, 2002).
However, HMMs learn a generativemodel over input sequence and labeled sequencepairs.
It has difficulties in modeling multiple non-independent features of the observation sequence.To accommodate multiple overlapping features onobservations, some other approaches view thephrase chunking as a sequence of classificationproblems, including support vector machines(SVMs) (Kudo and Matsumoto 2001) and a varietyof other classifiers (Zhang et al2002).
Since theseclassifiers cannot trade off decisions at differentpositions against each other, the best classifierbased shallow parsers are forced to resort toheuristic combinations of multiple classifiers.Recently, CRFs were widely employed for phrasechunking, and presented comparable or betterperformance than other state-of-the-art models(Sha and Pereira 2003; McDonald et al005).Further, Sun et al2008) used the latent-dynamicconditional random fields (LDCRF) to explicitlylearn the hidden substructure of shallow phrases,achieving state-of-the-art performance over theNP-chunking task on the CoNLL data.Some similar approaches based on classifiers orsequence labeling models were also used forChinese chunking (Li et al2003; Tan et al2004;Tan et al2005).
Chen et al2006) conducted anempirical study of Chinese chunking on a corpus,which was extracted from UPENN ChineseTreebank-4 (CTB4).
They compared theperformances of the state-of-the-art machinelearning models for Chinese chunking, andproposed some Tag-Extension and novel votingmethods to improve performance.In this paper, we model phrase chunking with ajoint segmentation and labeling approach, whichoffer advantages over previous learning methodsby explicitly incorporating the internal structuralfeature and the correlations between the adjacentchunks.
To some extent, our model is similar toSemi-Markov Conditional Random Fields (called aSemi-CRF), in which the segmentation and558labeling can also be done directly (Sarawagi andCohen, 2004).
However, Semi-CRF just modelslabel dependency, and it cannot capture morecorrelations between adjacent chunks, as is done inour approach.
The limitation of Semi-CRF leads toits relatively low performance.3 Problem Formulation3.1 Chunk TypesUnlike English chunking, there is not abenchmarking corpus for Chinese chunking.
Wefollow the studies in (Chen et al006) so that amore direct comparison with state-of-the-artsystems for Chinese chunking would be possible.There are 12 types of chunks: ADJP, ADVP, CLP,DNP, DP, DVP, LCP, LST, NP, PP, QP and VP inthe chunking corpus (Xue et al2000).
Thetraining and test corpus can be extracted fromCTB4 with a public tool, as depicted in (Chen et al2006).3.2 Sequence Labeling Approaches to PhraseChunkingThe standard approach to phrase chunking is to usetagging techniques with a BIO tag set.
Words inthe input text are tagged with one of B for thebeginning of a contiguous segment, I for the insideof a contiguous segment, or O for outside asegment.
For instance, the sentence (wordsegmented and POS tagged) "?/NR(He) ?
?/VV(reached) ?
?
/NR(Beijing) ?
?/NN(airport) ?/PU" will be tagged as follows:Example 1:S1: [NP ?
][VP ??
][NP ??/??
][O ?
]S2: ?/B-NP ?
?/B-VP ?
?/B-NP ?
?/I-NP ?/OHere S1 denotes that the sentence is tagged withchunk types, and S2 denotes that the sentence istagged with chunk tags based on the BIO-basedmodel.
With the data representation like the S2, theproblem of phrase chunking can be reduced to asequence labeling task.3.3 Phrase Chunking via a JointSegmentation and Labeling ApproachTo tackle the problems with the sequence labelingapproaches to phrase chunking, we formulate it asa joint problem, which maps a Chinese sentence xwith segmented words and POS tags to an output ywith tagged chunk types, like the S1 in Example 1.The joint model considers all possible chunkboundaries and corresponding chunk types in thesentence, and chooses the overall best output.
Thiskind of parser reads the input sentences from left toright, predicts whether current segment ofcontinuous words is some type of chunk.
After onechunk is found, parser move on and search for nextpossible chunk.Given a sentence x, let y denote an output taggedwith chunk types, and GEN a function thatenumerates a set of segmentation and labelingcandidates GEN(x) for x.
A parser is to solve thefollowing ?argmax?
problem:( )| |[1.. ]( ) 1?
arg max ( )arg max ( )Ty GEN xyTiy GEN x iy w yw yf??
== ?F= ?
?where F  and f  are global and local feature mapsand w is the parameter vector to learn.
The innerproduct [1.. ]( )T iw yf?
can be seen as the confidencescore of whether yi is a chunk.
The parser takes intoaccount confidence score of each chunk, by usingthe sum of local scores as its criteria.
Markovassumption is necessary for computation, so f  isusually defined on a limited history.The main advantage of the joint segmentationand labeling approach to phrase chunking is toallow for integrating both the internal structuralfeatures and the correlations between the adjacentchunks for prediction.
The two basic componentsof our model are decoding and learning algorithms,which are described in the following sections.4 DecodingThe inference technique is one of the mostimportant components for a joint segmentation andlabeling model.
In this section, we propose adynamic programming algorithm with pruning toefficiently produce the optimal output.4.1 Algorithm DescriptionGiven an input sentence x, the decoding algorithmsearches for the highest-scored output withrecognized chunks.
The search space of combinedcandidates in the joint segmentation and labelingtask is very large, which is an exponential growth(1)559in the number of possible candidates withincreasing sentence size.
The rate of growth isO(2nTn) for the joint system, where n is the lengthof the sentence and T is the number of chunk types.It is natural to use some greedy heuristic searchalgorithms for inference in some similar jointproblems (Zhang and Clark, 2008; Zhang andClark, 2010).
However, the greedy heuristic searchalgorithms only explore a fraction of the wholespace (even with beam search) as opposed todynamic programming.
Additionally, a specificadvantage of the dynamic programming algorithmis that constraints required in a valid predictionsequence can be handled in a principled way.
Weshow that dynamic programming is in fact possiblefor this joint problem, by introducing someeffective pruning schemes.To make the inference tractable, we first make afirst-order Markov assumption on the features usedin our model.
In other words, we assume that thechunk ci and the corresponding label ti are onlyassociated with the preceding chunk ci-1 and thelabel ti-1.
Suppose that the input sentence has nwords and the constant M is the maximum chunklength in the training corpus.
Let V(b,e,t) denotethe highest-scored segmentation and labeling withthe last chunk starting at word index b, ending atword index e and the last chunk type being t. Oneway to find the highest-scored segmentation andlabeling for the input sentence is to first calculatethe V(b,n-1,t) for all possible start position b?
(n-M)..n-1, and all possible chunk type t, respectively,and then pick the highest-scored one from thesecandidates.
In order to compute V(b,n-1,t), the lastchunk needs to be combined with all possibledifferent segmentations of words (b-M)..b-1 and allpossible different chunk types so that the highest-scored can be selected.
According to the principleof optimality, the highest-scored among thesegmentations of words (b-M)..b-1 and all possiblechunk types with the last chunk being word b?
..b-1 and the last chunk type being t ?
will also give the highest score when combined with the wordb..n-1 and tag t. In this way, the search task isreduced recursively into smaller subproblems,where in the base case the subproblems V(0,e,t) fore?0..M-1, and each possible chunk type t, aresolved in straightforward manner.
And the finalhighest-scored segmentation and labeling can befound by solving all subproblems in a bottom-upfashion.The pseudo code for this algorithm is shown inFigure 1.
It works by filling an n by n by T tablechart, where n is the number of words in the inputsentence sent, and T is the number of chunk types.chart[b,e,t] records the value of subproblemV(b,e,t).
chart[0, e, t] can be computed directly fore = 0..M-1 and for chunk type t=1..T. The finaloutput is the best among chart[b,n-1,t], with b=n-M..n-1, and t=1..T.Inputs: sentence sent (word segmented and POStagged)Variables:word index b for the start of chunk;word index e for the end of chunk;word index p for the start of the previous chunk.chunk type index t for the current chunk;chunk type index t ?
for the previous chunk;Initialization:for e = 0.. M-1:for t =1..T:chart[0,e,t] ?single chunk sent[0,e] and type tAlgorithm:for e = 0..n-1:for b = (e-M)..e:for t =1..T:chart[b,e,t]?the highest scored segmentationand labeling among those derived bycombining chart[p,b-1, t ? ]
with sent[b,e]and chunk type t, for p = (b-M)..b-1,t ?
=1..T.Outputs: the highest scored segmentation andlabeling among chart[b,n-1,t], for b=n-M..n-1, t=1..T.Figure 1: A dynamic-programming algorithm forphrase chunking.4.2 PruningThe time complexity of the above algorithm isO(M2T2n), where M is the maximum chunk size.
Itis linear in the length of sentence.
However, theconstant in the O is relatively large.
In practice, thesearch space contains a large number of invalidpartial candidates, which make the algorithm slow.In this section we describe three partial outputpruning schemes which are helpful in speeding upthe algorithm.560Firstly, we collect chunk type transitioninformation between chunk types by observingevery pair of adjacent chunks in the training corpus,and record a chunk type transition matrix.
Forexample, from the Chinese Treebank that we usedfor our experiments, a transition from chunk typeADJP to ADVP does not occur in the trainingcorpus, the corresponding matrix element is set tofalse, true otherwise.
During decoding, the chunktype transition information is used to pruneunlikely combinations between current chunk andthe preceding chunk by their chunk types.Secondly, a POS tag dictionary is used to recordPOS tags associated with each chunk type.Specifically, for each chunk type, we record allPOS tags appearing in this type of chunk in thetraining corpus.
During decoding, a segment ofcontinuous words that contains only allowed POStags according to the POS tag dictionary will beconsidered to be a valid chunk candidate.Finally, the system records the maximumnumber of words for each type of chunk in thetraining corpus.
For example, in the ChineseTreebank, most types of chunks have one to threewords.
The few chunk types that are seen withlength bigger than ten are NP, QP and ADJP.During decoding, the chunk candidate whoselength is greater than the maximum chunk lengthassociated with its chunk type will be discarded.For the above pruning schemes, developmenttests show that it improves the speed significantly,while having a very small negative influence onthe accuracy.5 Learning5.1 Discriminative Online TrainingBy defining features, a candidate output y ismapped into a global feature vector, in which eachdimension represents the count of a particularfeature in the sentence.
The learning task is to setthe parameter values w using the training examplesas evidence.Online learning is an attractive method for thejoint model since it quickly converges within a fewiterations (McDonald, 2006).
We focus on anonline learning algorithm called MIRA, which is arelaxed, online maximum margin trainingalgorithm with the desired accuracy and scalabilityproperties (Crammer, 2004).
Furthermore, MIRAis very flexible with respect to the loss function.Any loss function on the output is compatible withMIRA since it does not require the loss to factoraccording to the output, which enables our modelto be optimized with respect to evaluation metricsdirectly.
Figure 2 outlines the generic onlinelearning algorithm (McDonald, 2006) used in ourframework.MIRA updates the parameter vector w with twoconstraints: (1) the positive example must have ahigher score by a given margin, and (2) the changeto w should be minimal.
This second constraint isto reduce fluctuations in w. In particular, we use ageneralized version of MIRA (Crammer et al2005; McDonald, 2006) that can incorporate k-bestdecoding in the update procedure.Input: Training set 1{( , )}Tt t tS x y ==1: w(0) = 0; v = 0; i = 02: for iter = 1 to N do3:    for t = 1 to T do4:       w(i+1) = update w(i) according to (xt, yt)5:       v = v + w(i+1)6:       i = i + 17:    end for8: end for9: w = v/(N ?
T)Output: weight vector wFigure 2: Generic Online Learning AlgorithmIn each iteration, MIRA updates the weightvector w by keeping the norm of the change in theweight vector as small as possible.
Within thisframework, we can formulate the optimizationproblem as follows (McDonald, 2006):( 1) ( )( )argmin.
.
( ; ) :( ) ( ) ( , )i iwik tT Tt tw w ws t   y best x ww y w y L y y+ = -?"
??
?
?F - ?F ?where ( )( ; )ik tbest x w  represents a set of top k-bestoutputs for xt given the weight vector w(i).
In ourimplementation, the top k-best outputs are obtainedwith a straightforward k-best extension to thedecoding algorithm in section 4.1.
The abovequadratic programming (QP) problem can besolved using Hildreth?s algorithm (Yair Censor,1997).
Replacing Eq.
(2) into line 4 of thealgorithm in Figure 2, we obtain k-best MIRA.As shown in (McDonald, 2006), parameteraveraging can effectively avoid overfitting.
The(2)561final weight vector w is the average of the weightvectors after each iteration.5.2 Loss FunctionFor the joint segmentation and labeling task, thereare two alternative loss functions: 0-1 loss and F1loss.
0-1 loss gives credit only when the entireoutput sequence is correct: there is no notion ofpartially correct solutions.
The most common lossfunction for joint segmentation and labelingproblems is F1 measure over chunks.
This is thegeometric mean of precision and recall over the(properly-labeled) chunk identification task,defined as follows.2 | |?
( , ) 1 | | | |F y yL y y y y?
?- ?+?where the cardinality of y is simply the number ofchunks identified.
The cardinality of theintersection is the number of chunks in common.As can be seen in the definition, one is penalizedboth for identifying too many chunks (penalty inthe denominator) and for identifying too few(penalty in the numerator).In our experiments, we will compare theperformance of the systems with different lossfunctions.5.3 FeaturesTable 1 shows the feature templates for the jointsegmentation and labeling model.
In the row forfeature templates, c, t, w and p are used torepresent a chunk, a chunk type, a word and a POStag, respectively.
And c0 and c?1 represent thecurrent chunk and the previous chunk respectively.Similarly, w?1, w0 and w1 represent the previousword, the current word and the next word,respectively.Although it is slightly less natural to do so, partof the features used in the sequence labelingmodels can also be represented in our approach.Therefore the features employed in our model canbe divided into three types: the features similar tothose used in the sequence labeling models (calledSL-type features), the features describing internalstructure of a chunk (called Internal-type features),and the features capturing the correlations betweenthe adjacent chunks (called Correlation-typefeatures).Firstly, some features associated with a singlelabel (here refers to label "B" and "I") used in thesequence labeling models are also represented inour model.
In Table 1, templates 1-4 are SL-typefeatures, where label(w) denotes the labelindicating the position of the word w in the currentchunk; len(c) denotes the length of chunk c. Forexample, given an NP chunk "??
(Beijing) ??
(Airport)", which includes two words, the value oflabel("??")
is "B" and the value of label("??
")is "I".
Bigram(w) denotes the word bigrams formedby combining the word to the left of w and the oneto the right of w. And the same meaning is forbiPOS(w).
Template specitermMatch(c) is used tocheck the punctuation matching within chunk c forthe special terms, as illustrated in section 1.Secondly, in our model, we have a chance totreat the chunk candidate as a whole duringdecoding, which means that we can employ moreexpressive features in our model than in thesequence labeling models.
In Table 1, templates 5-13 concern the Internal-type features, wherestart_word(c) and end_word(c) represent the firstword and the last word of chunk c, respectively.Similarly, start_POS(c) and end_POS(c) representthe POS tags associated with the first word and thelast word of chunk c, respectively.
These featuresaim at expressing the formation patterns of thecurrent chunk with respect to words and POS tags.Template internalWords(c) denotes theconcatenation of words in chunk c, whileinternalPOSs(c) denotes the sequence of POS tagsin chunk c using regular expression-like form, asillustrated in section 1.Finally, in Table 1, templates 14-28 concern theCorrelation-type features, where head(c) denotesthe headword extracted from chunk c, andheadPOS(c) denotes the POS tag associated withthe headword in chunk c. These features take intoaccount various aspects of correlations betweenadjacent chunks.
For example, we extracted theheadwords located in adjacent chunks to formheadword bigrams to express semantic dependencybetween adjacent chunks.
To find the headwordwithin every chunk, we referred to the head-finding rules from (Bikel, 2004), and made asimple modification to them.
For instance, thehead-finding rule for NP in (Bikel, 2004) is asfollows:(NP (r NP NN NT NR QP) (r))Since the phrases are non-overlapping in our task,we simply remove the overlapping phrase tags NP(3)562and QP from the rule, and then the rule is modifiedas follows:(NP (r NN NT NR) (r))Additionally, the different bigrams formed bycombining the first word (or POS) and last word(or POS) located in two adjacent chunks can alsocapture some correlations between adjacent chunks,and templates 17-22 are designed to express thiskind of bigram information.ID Feature template1 wlabel(w) t0for all w in c02 bigram (w) label(w)t0for all w in c03 biPOS(w) label(w)t0for all w in c04 w-1w1label(w0) t0 ,  where len(c0)=15 start_word(c0)t06 start_POS(c0)t07 end_word(c0)t08 end_POS(c0)t09 wend_word (c0) t0where 0 0_ ( )w c  and w end word c?
?10 pend_POS (c0) t0where 0 0_ ( )p c  and p end POS c?
?11 internalPOSs(c0) t012 internalWords(c0) t013 specitermMatch(c0)14 t-1t015 head(c-1)t-1head(c0)t016 headPOS(c-1)t-1headPOS(c0)t017 end_word(c-1)t-1start_word(c0)t018 end_POS(c-1)t-1start_POS(c0)t019 end_word(c-1)t-1end_word(c0)t020 end_POS(c-1)t-1end_POS(c0)t021 start_word(c-1)t-1start_word(c0)t022 start_POS(c-1)t-1start_POS(c0)t023 end_word(c-1)t024 end_POS(c-1)t025 t-1t0start_word(c0)26 t-1t0start_POS(c0)27 internalWords(c-1) t-1 internalWords(c0) t028 internalPOSs(c-1) t-1 internalPOSs(c0) t0Table 1: Feature templates.6 Experiments6.1 Data Sets and EvaluationFollowing previous studies on Chinese chunking in(Chen et al2006), our experiments wereperformed on the CTB4 dataset.
The datasetconsists of 838 files.
In the experiments, we usedthe first 728 files (FID from chtb 001.fid to chtb899.fid) as training data, and the other 110 files(FID from chtb 900.fid to chtb 1078.fid) as testingdata.
The training set consists of 9878 sentences,and the test set consists of 5920 sentences.
Thestandard evaluation metrics for this task areprecision p (the fraction of output chunks matchingthe reference chunks), recall r (the fraction ofreference chunks returned), and the F-measuregiven by F = 2pr/(p + r).Our model has two tunable parameters: thenumber of training iterations N; the number of topk-best outputs.
Since we were interested in findingan effective feature representation at chunk-levelfor phrase chunking, we fixed N = 10 and k = 5 forall experiments.
In the following experiments, ourmodel has roughly comparable training time to thesequence labeling approach based on CRFs.6.2 Chinese NP chunkingNP is the most important phrase in Chinesechunking and about 47% phrases in the CTB4Corpus are NPs.
In this section, we present theresults of our approach to NP recognition.Table 2 shows the results of the two systemsusing the same feature representations as definedin Table 1, but using different loss functions forlearning.
As shown, learning with F1 loss canimprove the F-score by 0.34% over learning with0-1 loss.
It is reasonable that the model optimizedwith respect to evaluation metrics directly canachieve higher performance.Loss Function Precision Recall F10-1 loss 91.39 90.93 91.16F1 loss 92.03 90.98 91.50Table 2: Experimental results on Chinese NPchunking.6.3 Chinese Text ChunkingThere are 12 different types of phrases in thechunking corpus.
Table 3 shows the results from563two different systems with different loss functionsfor learning.
Observing the results in Table 3, wecan see that learning with F1 loss can improve theF-score by 0.36% over learning with 0-1 loss,similar to the case in NP recognition.
Morespecifically, learning with F1 loss provides muchbetter results for ADJP, ADVP, DVP, NP and VP,respectively.
And it yields equivalent orcomparable results to 0-1 loss in other categories.F1 loss 0-1 lossprecision recall F1 precision recall F1ADJP 87.86 87.09 87.47 86.74 86.55 86.64ADVP 90.66 78.73 84.27 91.91 76.68 83.61CLP 0.00 0.00 0.00 1.32 5.88 2.15DNP 99.42 99.93 99.68 99.42 99.95 99.69DP 99.46 99.76 99.61 99.46 99.76 99.61DVP 99.61 99.61 99.61 99.22 99.61 99.42LCP 99.74 99.96 99.85 99.74 99.93 99.84LST 87.50 52.50 65.63 87.50 52.50 65.63NP 91.87 91.01 91.44 91.34 90.52 90.93PP 99.57 99.77 99.67 99.57 99.77 99.67QP 96.45 96.64 96.55 96.45 97.07 96.76VP 90.14 90.39 90.26 89.92 89.79 89.85ALL 92.54 91.68 92.11 92.30 91.20 91.75Table 3: Experimental results on Chinese text chunking.6.4 Comparison with Other ModelsChen et al2006) compared the performance ofthe state-of-the-art machine learning models forChinese chunking, and found that the SVMsapproach yields higher accuracy than respectiveCRFs, Transformation-based Learning (TBL)(Megyesi, 2002), and Memory-based Learning(MBL) (Sang, 2002) approaches.In this section, we give a comparison andanalysis between our model and other state-of-the-art machine learning models for Chinese NPchunking and text chunking tasks.
Performance ofour model and some of the best results from thestate-of-the-art systems are summarized in Table 4.Row "Voting" refers to the phrase-based votingmethods based on four basic systems, which arerespectively SVMs, CRFs, TBL and MBL, asdepicted in (Chen et al2006).
Observing theresults in Table 4, we can see that for both NPchunking and text chunking tasks, our modelachieves significant performance improvementover those state-of-the-art systems in terms of theF1-score, even for the voting methods.
For textchunking task, our approach improves performanceby 0.65% over SVMs, and 0.43% over the votingmethod, respectively.Method F1NPchunkingCRFs 89.72SVMs 90.62Voting 91.13Ours 91.50TextchunkingCRFs 90.74SVMs 91.46Voting 91.68Ours 92.11Table 4: Comparisons of chunking performance forChinese NP chunking and text chunking.In particular, for NP chunking task, the F1-scoreof our approach is improved by 0.88% incomparison with SVMs, the best single system.Further, we investigated the likely cause forperformance improvement by comparing therecognized results from our system and SVMs564respectively.
We first sorted NPs by their length,and then calculated the F1-scores associated withdifferent lengths for the two systems respectively.Figure 3 shows the comparison of F1-scores of thetwo systems by the chunk length.
In the Chinesechunking corpus, the max NP length is 27, andthe mean NP length is 1.5.
Among all NPs, theNPs with the length 1 account for 81.22%.
For theNPs with the length 1, our system gives slightimprovement by 0.28% over SVMs.
From thefigure, we can see that the performance gap growsrapidly with the increase of the chunk length.
Inparticular, the gap between the two systems is27.73% when the length hits 4.
But the gap beginsto become smaller with further growth of thechunk length.
The reasons may include thefollowing two aspects.
First, the number of NPswith the greater length is relatively small in thecorpus.
Second, the NPs with greater length inChinese corpus often exhibit some typical rules.For example, an NP with length 8 is given asfollows."?
?/NN(cotton) ?/PU ?
?/NN(oil) ?/PU ?
?/NN(drug) ?/PU ?
?/NN(vegetable) ?/ETC(et alThe NP consists of a sequence of nouns simplyseparated by a punctuation "?".
So it is also easyto be recognized by the sequence labelingapproach based on SVMs.
In summary, the aboveinvestigation indicates that our system is better atrecognizing the long and complicated phrasescompared with the sequence labeling approaches.354555657585951 2 3 4 5 6 7 8 >8The length of NPF-scoreour systemSVMFigure 3: Comparison of F1-scores of NPrecognition on Chinese corpus by the chunk length.6.5 Impact of Different Types of FeaturesOur phrase chunking model is highly dependentupon chunk-level information.
To establish theimpact of each type of feature (SL-type, Internal-type, Correlation-type), we look at theimprovement in F1-score brought about by addingeach type of features.
Table 5 shows the accuracywith various features added to the model.First consider the effect of the SL-type features.If we use only the SL-type features, the systemachieves slightly lower performance than CRFs orSVMs, as shown in Table 4.
Since the SL-typefeatures consist of the features associated withsingle label, not including the features associatedwith label bigrams.
Then, adding the Internal-typefeatures to the system results in significantperformance improvement on NP chunking and ontext chunking, achieving 2.53% and 1.37%,respectively.
Further, if Correlation-type featuresare used, the F1-scores on NP chunking and on textchunking are improved by 1.01% and 0.66%,respectively.
The results show a significant impactdue to the use of Internal-type features andCorrelation-type features for both NP chunkingand text chunking.Task Type Feature Type F1NP chunkingSL-type 87.96+Internal-type 90.49+Correlation-type 91.50Text chunkingSL-type 90.08+Internal-type 91.45+Correlation-type 92.11Table 5: Test F1-scores for different types offeatures on Chinese corpus.6.6 Performance on Other LanguagesWe mainly focused on Chinese chunking in thispaper.
However, our approach is generallyapplicable to other languages including English,except that the definition of feature templates maybe language-specific.
To validate this point, weevaluated our system on the CoNLL 2000 data set,a public benchmarking corpus for Englishchunking (Sang and Buchholz 2000).
The trainingset consists of 8936 sentences, and the test setconsists of 2012 sentences.We conducted both the NP-chunking and textchunking experiments on this data set with ourapproach, using the same feature templates as inChinese chunking task excluding template 13.
Tofind the headword within every chunk, we referredto the head-finding rules from (Collins, 1999), andmade a simple modification to them in a similarway as in Chinese.
As we can see from Table 6,565our model is able to achieve better performancecompared with state-of-the-art systems.
Table 6also shows state-of-the-art performance for bothNP-chunking and text chunking tasks.
LDCRF'sresults presented in (Sun et al2008) are the state-of-the-art for the NP chunking task, and SVM'sresults presented in (Wu et al2006) are the state-of-the-art for the text chunking task.Moreover, the performance should be furtherimproved if some additional features tailored forEnglish chunking are employed in our model.
Forexample, we can introduce an orthographic featuretype called Token feature and the affix feature intothe model, as used in  (Wu et al2006).Method Precision Recall F1NPchunkingOurs 94.79 94.65 94.72LDCRF 94.65 94.03 94.34TextchunkingOurs 94.31 94.12 94.22SVMs 94.12 94.13 94.12Table 6: Performance on English corpus.7 Conclusions and Future WorkIn this paper we have presented a novel approachto phrase chunking by formulating it as a jointsegmentation and labeling problem.
One importantadvantage of our approach is that it provides anatural formulation to exploit chunk-level features.The experimental results on both Chinese chunkingand English chunking tasks show that the use ofchunk-level features can lead to significantperformance improvement and that our approachoutperforms the best in the literature.Future work mainly includes the following twoaspects.
Firstly, we will explore applying externalinformation, such as semantic knowledge, torepresent the chunk-level features, and thenincorporate them into our model to improve theperformance.
Secondly, we plan to apply ourapproach to other joint segmentation and labelingtasks, such as clause identification and namedentity recognition.AcknowledgmentsThis research is supported by Projects 61073119,60773173 under the National Natural ScienceFoundation of China, and project BK2010547under the Jiangsu Natural Science Foundation ofChina.
We would also like to thank the excellentand insightful comments from the threeanonymous reviewers.ReferencesSteven P. Abney.
1991.
Parsing by chunks.
In Robert C.Berwick, Steven P. Abney, and Carol Tenny, editors,Principle-Based Parsing , pages 257-278.
KluwerAcademic Publishers.Daniel M, Bikel.
2004.
On the Parameter Space ofGenerative Lexicalized Statistical Parsing Models.Ph.D.
thesis, University of Pennsylvania.Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.
2006.An empirical study of Chinese chunking.
InProceedings of the COLING/ACL 2006 MainConference Poster Sessions, pages 97-104.Michael Collins.
2002.
Discriminative training methodsfor hidden Markov models: Theory and experimentswith perceptron algorithms.
In Proc.
EMNLP-02.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Koby Crammer.
2004.
Online Learning of ComplexCategorial Problems.
Hebrew University ofJerusalem, PhD Thesis.Taku Kudo and Yuji Matsumoto.
2001.
Chunking withsupport vector machines.
In Proceedings ofNAACL01.Koby Crammer, Ryan McDonald, and Fernando Pereira.2005.
Scalable large-margin online learning forstructured classification.
In NIPS Workshop onLearning With Structured Outputs.Heng Li, Jonathan J. Webster, Chunyu Kit, andTianshun Yao.
2003.
Transductive hmm basedchinese text chunking.
In Proceedings of IEEENLPKE2003, pages 257-262, Beijing, China.Ryan McDonald, Femando Pereira, Kiril Ribarow, andJan Hajic.
2005.
Non-projective dependency parsingusing spanning tree algorithms.
In Proceedings ofHLT/EMNLP, pages 523-530.Ryan.
McDonald, K. Crammer, and F. Pereira, 2005.Flexible Text Segmentation with StructuredMultilabel Classification.
In ProceedingsHLT/EMNLP, pages 987- 994.Ryan McDonald.
2006.
Discriminative Training andSpanning Tree Algorithms for Dependency Parsing.University of Pennsylvania, PhD Thesis.Beata Megyesi.
2002.
Shallow parsing with pos taggersand linguistic features.
Journal of Machine LearningResearch, 2:639-668.566Antonio Molina and Ferran Pla.
2002.
Shallow parsingusing specialized hmms.
Journal of MachineLearning Research., 2:595- 613.E.F.T.K Sang and S. Buchholz.
2000.
Introduction tothe CoNLL-2000 shared task: Chunking.
InProceedings CoNLL-00, pages 127-132.Sunita Sarawagi and W. Cohen.
2004.
Semi-markovconditional random fields for information extraction.In Proceedings of NIPS 17, pages 1185?1192.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In Proceedings ofHLT-NAACL03.Xu Sun, Louis-Philippe Morency, Daisuke Okanohara,and Jun?ichi Tsujii.
2008.
Modeling Latent-Dynamicin Shallow Parsing: A Latent Conditional Model withImproved Inference.
In Proceedings of the 22ndInternational Conference on ComputationalLinguistics, pages 841?848.Yongmei Tan, Tianshun Yao, Qing Chen, and JingboZhu.
2004.
Chinese chunk identification using svmsplus sigmoid.
In IJCNLP, pages 527-536.Yongmei Tan, Tianshun Yao, Qing Chen, and JingboZhu.
2005.
Applying conditional random fields tochinese shallow parsing.
In Proceedings of CICLing-2005, pages 167-176.Erik F. Tjong Kim Sang.
2002.
Memory-based shallowparsing.
JMLR, 2(3):559-594.Yu-Chieh Wu, Chia-Hui Chang, and Yue-Shi Lee.
2006.A general and multi-lingual phrase chunking modelbased on masking method.
In Proceedings of 7thInternational Conference on Intelligent TextProcessing and Computational Linguistics, pages144-155.Nianwen Xue, Fei Xia, Shizhe Huang, and AnthonyKroch.
2000.
The bracketing guidelines for the pennchinese treebank.
Technical report, University ofPennsylvania.Stavros A. Zenios Yair Censor.
1997.
ParallelOptimization: Theory, Algorithms, and Applications.Oxford University Press.Tong Zhang, F. Damerau, and D. Johnson.
2002.
Textchunking based on a generalization of winnow.Journal of Machine Learning Research, 2:615-637.Yue Zhang and Stephen Clark.
2008.
Joint wordsegmentation and POS tagging using a singleperceptron.
In Proceedings of ACL/HLT, pages 888-896.Yue Zhang and Stephen Clark.
2010.
A fast decoder forjoint word segmentation and POS-tagging using asingle discriminative model.
In Proceedings ofEMNLP, pages 843-852.567
