Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 128?136,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsIntersecting multilingual data for faster and better statistical translationsYu Chen1,2, Martin Kay1,3, Andreas Eisele1,21: Universita?t des Saarlandes, Saarbru?cken, Germany2: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany3: Stanford University, CA, USA{yuchen,kay,eisele}@coli.uni-saarland.deAbstractIn current phrase-based SMT systems, moretraining data is generally better than less.However, a larger data set eventually intro-duces a larger model that enlarges the searchspace for the translation problem, and con-sequently requires more time and more re-sources to translate.
We argue redundant in-formation in a SMT system may not only de-lay the computations but also affect the qual-ity of the outputs.
This paper proposes an ap-proach to reduce the model size by filteringout the less probable entries based on com-patible data in an intermediate language, anovel use of triangulation, without sacrificingthe translation quality.
Comprehensive exper-iments were conducted on standard data sets.We achieved significant quality improvements(up to 2.3 BLEU points) while translating withreduced models.
In addition, we demon-strate a straightforward combination methodfor more progressive filtering.
The reductionof the model size can be up to 94% with thetranslation quality being preserved.1 IntroductionStatistical machine translation (SMT) applies ma-chine learning techniques to a bilingual corpus toproduce a translation system entirely automatically.Such a scheme has many potential advantages overearlier systems which relied on carefully craftedrules.
The most obvious is that it at dramaticallyreduces cost in human labor and it is able to reachmany critical translation rules that are easily over-looked by human being.SMT systems generally assemble translations byselecting phrases from a large candidate set.
Un-supervised learning often introduces a considerableamount of noise into this set as a result of which theselection process becomes more longer and less ef-fective.
This paper provides one approach to theseproblems.Various filtering techniques, such as (Johnson etal., 2007) and (Chen et al, 2008), have been ap-plied to eliminate a large portion of the translationrules that were judged unlikely to be of value forthe current translation.
However, these approacheswere only able to improve the translation qualityslightly.
In this paper, we describe a triangulationapproach (Kay, 1997) that incorporates multilingualdata to improve system efficiency and translationquality at the same time.
Most of the previous tri-angulation approaches (Kumar et al, 2007; Cohnand Lapata, 2007; Filali and Bilmes, 2005; Simard,1999; Och and Ney, 2001) add information obtainedfrom a third language.
In other words, they workwith the union of the data from the different lan-guages.
In contrast, we work with the intersection ofinformation acquired through a third language.
Thehope is that the intersection will be more precise andmore compact than the union, so that a better resultwill be obtained more efficiently.2 Noise in a phrase-based SMT systemThe phrases in a translation model are extractedheuristically from a word alignment between theparallel texts in two languages using machine learn-ing techniques.
The translation model feature valuesare stored in the form of a so-called phrase-table,128while the distortion model is in the reordering-table.As we have said models built in this way tend to con-tain a contains a considerable amount of noise.
Thephrase-table entries are far less reliable than the lex-icons and grammar rules handcrafted for rule-basedsystems.The main source of noise in the phrase table iserrors from the word alignment process.
For exam-ple, many function words occur so frequently thatthey are incorrectly mapped to translations of manyfunction words in the other language to which theyare, in fact, unrelated.
On the other hand, manywords remain unaligned on account of their very lowfrequency.
Another source noise comes from thephrase extraction algorithm itself.
The unalignedwords are usually attached to aligned sequences Inorder to achieve longer phrase pairs.The final selection of entries from the phrase ta-ble is based not only on the values assigned to themthere, but also to values coming from the languageand reordering models, so that entries that receive aninitially high value may end up not being preferred.
(1) SietheyliebenloveihretheirKinderchildrennicht.notThey don?t love their children.The frequently occurring German negative ?nicht?in (1).
is sometimes difficult for SMT systemsto translate into English because it may appear inmany positions of a sentence.
For instance, it oc-curs at the end of the sentence in (1).
The phrasepairs ?ihre kinder nicht ?
their children are not?and ?ihre kinder nicht ?
their children?
are bothlikely also to appear in the phrase table and the for-mer has greater estimated probability.
However, thelanguage model would preferred the latter in this ex-ample because the sentence ?They love their childrenare not.?
is unlikely to be attested.
Accordingly,SMT system may therefore produce the misleadingtranslation in (2).
(2) They love their children.The system would not produce translations with theopposite meanings if the noisy entries like ?ihrekinder nicht ?
their children?
were excluded fromthe translation candidates.
Eliminating the noiseshould help to improve the system?s performance,for both efficiency and translation quality.3 Triangulated filteringWhile direct translation and pivot translationthrough a bridge language presumably introducenoise, in substantially similar amounts, there is noreason to expect the noise in the two systems to cor-relate strongly.
In fact, the noise from such differ-ent sources, tends to be quite distinct, whereas themore useful information is often retained.
This en-courages us to hope that information gathered fromvarious sources will be more reliable overall.Our plan is to ameliorate the noise problem byconstructing a smaller phrase-table by taking theintersection of a number of sources.
We reason that atarget phrase is will appear as a candidate translationof a given source phrase, only if it also appears as acandidate translation for some word or phrase in thebridge language mapping to the source phrase.
Werefer to this triangulation approach as triangulatedphrase-table filtering.TargetTextSourceTextModelFilteredParallelCorpusExtractionAlignment,PhraseSMTDecoderTranslationModelLanguageModelMonolingualCorpusCountingSmoothingFilteringModelTarget?BridgeModelSource?BridgeFigure 1: Triangulated filtering in SMT systemsFigure 1 illustrates our triangulation approach.Two bridge models are first constructed: one fromthe source language to the bridge language, and an-other from the target language to the bridge lan-guage.
Then, we use these two models to filter theoriginal source-target model.
For each phrase pairin the original table, we try to find a common linkin these bridge models to connect both phrases.
Ifsuch links do not exist, we remove the entry fromthe table.
The probability values in the table remain129unchanged.
The reduced table can be used in placeof the original one in the SMT system.There are various forms of links that can be usedas our evidence for the filtering process.
One obvi-ous form is complete phrases in the bridge language,which means, for each phrase pair in the model tobe filtered, we should look for a third phrase in thebridge language that can relate the two phrases in thepair.This approach to filtering examines each phrasepair presented in the phrase-table one by one.
Foreach phrase pair, we collect the corresponding trans-lations using the models for translation into a thirdlanguage.
If both phrases can be mapped to somephrases in the bridge language, but to different ones,we should remove it from the model.
It is also possi-ble that neither of the phrases appear in correspond-ing bridge models.
In this case, we consider thebridge models insufficient for making the filteringdecision and prefer to keep the pair in the table.The way a decoder constructs translation hypothe-ses is directly related to the weights for differentmodel features in a SMT system, which are usuallyoptimized for a given set of models with minimumerror rate training (MERT) (Och, 2003) to achievebetter translation performance.
In other words, theweights obtained for a model do not necessarily ap-ply to another model.
Since the triangulated filter-ing method removes a part of the model, it is impor-tant to readjust the feature weights for the reducedphrase-table.4 Experimental designAll the text data used in our experiments arefrom Release v3 of ?European Parliament Proceed-ings Parallel Corpus 1996-2006?
(Europarl) cor-pus (Koehn, 2005).
We mainly investigated trans-lations from Spanish to English.
There are enoughstructural differences in these two language to in-troduce some noise in the phrase table.
French,Portuguese, Danish, German and Finnish were usedas bridge languages.
Portuguese is very similar toSpanish and French somewhat less so.
Finnish is un-related and fairly different typologically with Danishand German occupying the middle ground.
In addi-tion, we also present briefly the results on German-English translations with Dutch, Spanish and Danishas bridges.For the Spanish-English pair, three translationmodels were constructed over the same parallel cor-pora.
We acquired comparable data sets by draw-ing several subsets from the same corpus accordingto various maximal sentence lengths.
The subsetsTokensModel Sentences Spanish EnglishEP-20 410,487 5,220,142 5,181,452EP-40 964,687 20,820,067 20,229,833EP-50 1,100,813 26,731,269 25,867,370Europarl 1,304,116 37,870,751 36,429,274Table 1: Europarl subsets for building the Spanish-English SMT systemwe used in the experiments are presented by ?EP-20?, ?EP-40?
and ?EP-50?, in which the numbersindicate the maximal sentence length in respectiveEuroparl subsets.
Table 1 lists the characteristicsof the Spanish-English subsets.
Although the max-imal sentence length in these sets is far less thanthat of the whole corpus (880 tokens), EP-50 al-ready includes nearly 85% of Spanish-English sen-tence pairs from Europarl.The translations models, both the models to befiltered and the bridge models, were generatedfrom compatible Europarl subsets using the Mosestoolkit (Koehn et al, 2007) with the most basic con-figurations.
The feature weights for the Spanish-English translation models were optimized over adevelopment set of 500 sentences using MERT tomaximize BLEU (Papineni et al, 2001).The triangulated filtering algorithm was appliedto each combination of a translation model and athird language.
The reordering models were alsofiltered according to the phrase-table.
Only thosephrase pairs that appeared in the phrase-table re-mained in the reordering table.
We rerun the MERTprocess solely based on the remaining entries in thefiltered tables.
Each table is used to translate a set of2,000 sentences of test data (from the shared task ofthe third Workshop on Statistical Machine Transla-tion, 2008 1).
Both the test set and the developmentdata set have been excluded from the training data.We evaluated the proposed phrase-table filtering1For details, seehttp://www.statmt.org/wmt08/shared-task.html130method mainly from two points of view: the effi-ciency of systems with filtered tables and the qualityof output translations produced by the systems.5 Results5.1 System efficiencyOften the question of machine translation is not onlyhow to produce a good translation, but also howto produce it quickly.
To evaluate the system ef-ficiency, we measured both storage space and timeconsumption.
For recording the computation time,we run an identical of installation of the decoderwith different models and then measure the averageexecution time for the given translation task.In Table 2, we give the number of entries in eachphrase table (N ), and the physical file size of thephrase table (SPT ) and the reordering table (SRT )(without any compression or binarization), Tl, thetime for the program to load phrase tables and Tt thetime to translate the complete test set.
We also high-lighted the largest and the smallest reduction fromeach group.All filtered models showed significant reductionsin size.
The greatest reduction of model sizes, takingboth phrase-table and reordering table into account,is nearly 11 gigabytes for filtering the largest model(EP-50) with a Finnish bridge, which leads to themaximal time saving of 939 seconds, or almost 16minutes, for translating two thousand sentences.The reduction rates from two larger models arevery close to each other whereas the filtered tablescaled down the most significantly on the smallestmodel (EP-20), which was in fact constructed over amuch smaller subset of Europarl corpus, consistingof less than half of the sentences pairs in the othertwo Europarl subsets.
Compared to the larger Eu-roparl subsets, the small data set is expected to pro-duce more errors through training as there is muchless relevant data for the machine learning algorithmto correctly extract useful information from.
Conse-quently, there are more noisy entries in this smallmodel, and therefore more entries to be removed.
Inaddition, the filtering is done by exact matching ofcomplete phrases, which presumably happens muchless frequently even for correctly paired phrase pairsin the very limited data supplied by the smallesttraining set.
For the same reason, the distinction be-tween different bridge languages was less clear forthis small model.Due to hardware limitation, we are not able tofit the unfiltered phrase tables completely into thememory.
Every table was filtered based on the giveninput so only a small portion of each table wasloaded into memory.
This may diminish the differ-ence between the original and the filtered table to acertain degree.
The relative time consumptionnev-ertheless agrees with the reduction in size: phrasetables from the smallest model showed the most re-duction for both loading the models and processingthe translations.For loading time, we count the time it takes tostart and to load the bilingual phrase-tables plus re-ordering tables and the monolingual language modelinto the memory.
The majority of the loading timefor the smallest model, even before filtering, hasbeen used for loading language models and otherstart-up processes, could not be reduced as much asthe reduction on table size.5.2 Translation qualityBridge EP-20 EP-40 EP-50?
26.62 31.43 31.68pt 28.40 32.90 33.93fr 28.28 32.69 33.47da 28.48 32.47 33.88de 28.05 32.65 33.13fi 28.02 31.91 33.04Table 3: BLEU scores of translations using filtered phrasetablesEfficiency aside, a translation system should beable to produce useful translation.
It is importantto verify that the filtering approach does not affectthe translation quality of the system.
Table 3 showthe BLEU scores of each translation acquired in theexperiments.Between translation models of different sizes,there are obvious performance gaps.
Differentbridge languages can cause different effects on per-formance.
However, the translation qualities froma single model are fairly close to each other.
Wetherefore take it that the effect of the triangulationapproach is rather robust across translation modelsof different sizes.131Time Table SizeModel+Bridge Tl (s) Tt (s) N SPT (byte) SRT (byte)EP-20+ ?
55 3529 7,599,271 953M 717MEP-20+ pt 53 2826 1,712,508 (22.54%) 198M 149MEP-20+ fr 48 2702 1,536,056 (20.21%) 172M 131MEP-20+ da 52 2786 1,659,067 (21.83%) 186M 141MEP-20+ de 43 2732 1,260,524 (16.59%) 132M 101MEP-20+ fi 47 2670 1,331,323 (17.52%) 147M 111MEP-40+ ?
65 3673 19,199,807 2.5G 1.9GEP-40+ pt 50 3091 8,378,517 (43.64%) 1.1G 1.8GEP-40+ fr 46 3129 8,599,708 (44.79%) 1.1G 741MEP-40+ da 42 3050 6,716,304 (34.98%) 842M 568MEP-40+ de 46 3069 6,113,769 (31.84%) 725M 492MEP-40+ fi 40 2889 4,473,483 (23.30%) 533M 353MEP-50+ ?
140 4130 54,382,715 7.1G 5.4GEP-50+ pt 78 3410 13,225,654 (24.32%) 1.6G 1.3GEP-50+ fr 97 3616 24,057,849 (44.24%) 3.0G 2.3GEP-50+ da 81 3418 12,547,839 (23.07%) 1.5G 1.2GEP-50+ de 95 3488 15,938,151 (29.31%) 1.9G 1.5GEP-50+ fi 71 3191 7,691,904 (17.75%) 895M 677MTable 2: System efficiency: time consumption and phrase-table sizeIt is obvious that the best systems are usuallyNOT from the filtered tables that preserved the mostentries from the original.
All the filtered modelsshowed some improvement in quality with updatedmodel weights.
Mostly around 1.5 BLEU points, theincreases ranged from 0.36 to 2.25.
Table 4 gives aset of translations from the experiments.
The unfil-tered baseline system inserted the negative by mis-take while all the filtered systems are able to avoidthis.
It indicates that there are indeed noisy entriesaffecting translation quality in the original table.
Wewere able to achieve better translations by eliminat-ing noisy entries.The filtering methods indeed tend to remove en-tries composed of long phrases.
Table 5 lists theaverage length of phrases in several models.
Bothsource phrases and target phrases are taken into ac-count.
The best models have shortest phrases on av-erage.
Discarding such entries seems to be neces-sary.
This is consistent with the findings in (Koehn,2003) that phrases longer than three words improveperformance little for training corpora of up to 20million words.Quality gains appeared to converge in the resultsacross different bridge languages while the originalmodels became larger.
Translations generated us-ing large models filtered with different bridge lan-Bridge EP-20 EP-40 EP-50?
3.776 4.242 4.335pt 3.195 3.943 3.740fr 3.003 3.809 3.947da 3.005 3.74 3.453de 2.535 3.501 3.617fi 2.893 3.521 3.262Table 5: Average phrase lengthguages are less diverse.
Meanwhile, the degradationis less for a larger model.
It is reasonable to expectimprovements for extremely large models with arbi-trary bridge languages.
For relatively small models,the selection of bridge languages would be criticalfor the effect of our approach.5.3 Language clusteringTo further understand how the triangulated filter-ing approach worked and why it could work as itdid, we examined a randomly selected phrase tablefragment through the experiments.
The segment in-cluded 10 potential English translations of the sameSpanish word ?fabricantes?, the plural form of theword ?fabricante?
(manufacturer).Table 6 shows the filtering results on a randomlyselected segment from the original ?EP-40?
model,including 10 English translations of the same source132source As?
?, se van modificando poco a poco los principios habituales del Estado de derecho por influencia de unaconcepcin extremista de la lucha con tra las discriminaciones..ref thus , the usual principles of the rule of law are being gradually altered under the influence of an extremistapproach to combating discrimination.baseline we are not changing the usual principles of the rule of law from the influence of an extremist approach inthe fight against discrimination.pt so , are gradually changing normal principles of the rule of law by influence of an extremist conception ofthe fight against discrimination.fr so , we are gradually changing the usual principles of the rule of law by influence of an extremist conceptionof the fight against discrimination.da so , are gradually changing the usual principles of the rule of law by influence of an extremist conceptionof the fight against discrimination.de thus , we are gradually altering the usual principles of the rule of law by influence of an extremist concep-tion of the fight against discrimination.fi so , are gradually changing normal principles of the rule of law by influence of an extremist conception ofthe fight against discrimination.Table 4: Examplesfabricantes pt fr da de fia manufacturer X X X X 4battalions X X X 3car manufacturers have 0car manufacturers X X X X X 5makers X X X 3manufacturer X X X X X 5manufacturers X X X X X 5producers are X X X 3producers need 0producers X X X X X 5Table 6: Phrase-table entries before and after filtering amodel with different bridgesword ?fabricantes?.
X indicates that the corre-sponding English phrase remained in the table aftertriangulated filtering with the corresponding bridgelanguage.
We also counted the number of tables thatincluded each phrase pair.Regardless of the bridge language, the triangu-lated filtering approach had removed those entriesthat are clearly noise.
Meanwhile, entries whichare surely correct were always preserved in the fil-tered tables.
The results of using different bridgelanguages turned out to be consistent on these ex-treme cases.
The 5 filtering processes agreed on sixout of ten pairs.As for the other 4 pairs, the decisions were differ-ent using different bridge languages.
The remainingentries were always different when the bridge waschanged.
None of the languages led to the identi-cal eliminations.
None of the cases excludes all er-rors.
Apparently, the selection of bridge languageshad immediate effects on the filtering results.3131.231.431.631.83232.232.432.632.8334  6  8  10  12  14  16  18  20BLEU(%)Phrase-table Entries (Mil.
)PortugeseFrenchDanishGermanFinnishBaselineFigure 2: Clustering of bridge languagesWe compared two factors of these filtered tables:their sizes and the corresponding BLEU scores.
Fig-ure 2 shows interesting signs of language similar-ity/dissimilarity.
There are apparently two groupsof languages having extremely close performance,which happen to fall in two language groups: Ger-manic (German and Danish) and Romance (Frenchand Portuguese).
The Romance group was as-sociated with larger filtered tables that producedslightly better translations.
The filtered tables cre-ated with Germanic bridge languages contained ap-133proximately 2 million entries less than Romancegroups.
The translation quality difference betweenthese two groups was within 1 point of BLEU.Observed from this figure, it seems that the trans-lation quality was connected to the similarity be-tween the bridge language and the source language.The closer the bridge is to the source language, thebetter translations it may produce.
For instance, Por-tuguese led to a filtered table that produced the besttranslations.
On the other hand, the more differentthe bridge languages compared to the source, thelarger portion of the phrase-table the filtering algo-rithm will remove.
The table filtered with Germanwas the smallest in the four cases.Finnish, a language that is unrelated to others, wasassociated with distinctive results.
The size of thetable filtered with Finnish is only 23% of the orig-inal, almost half of the table generated with Por-tuguese.
Finnish has extremely rich morphology,hence a great many word-forms, which would makeexact matching in bridge models less likely to hap-pen.
Many more phrase pairs in the original tablewere removed for this reason even though some ofthese entries were beneficial for translations.
Eventhough the improvement on translation quality dueto the Finnish bridge was less significant than theothers, it is clear that triangulated filtering retainedthe useful information from the original model.5.4 Further filteringThe filtering decision with a bridge language on aparticular phrase pair is fixed: either to keep the en-try or to discard it.
It is difficult to adjust the systemto work differently.
However, as the triangulated fil-tering procedure does not consider probability distri-butions in the models, it is possible to further filterthe tables according to the probabilities.The phrase pairs are associated with values com-puted from the given set of feature weights andsorted, so that we can remove any portions of theremain entries based on the values.
Each generatedtable is used to translate the test set again.
Fig-ure 3 shows BLEU scores of the translation out-puts produced with tables derived from the ?EP-50?model with respect to their sizes.
We also includedthe curve of probability-based filtering alone as thebaseline.The difference between filtered tables at the same24262830320  10  20  30  40  50BLEU(%)Phrase-table Entries (Mil.
)BaselinePortugeseFrenchDanishGermanFinnishFigure 3: Combining probability-based filteringsize can be over 6 BLEU points, which is a re-markable advantage for the triangulated filtering ap-proach always producing better translations.
Thecurves of the triangulated filtered models are clearlymuch steeper than that of the naive pruned ones.Data in these filtered models are more compact thanthe original model before any filtering.
The triangu-lated filtered phrase-tables contain more useful in-formation than a normal phrase-table of the samesize.
The curves representing the triangulated filter-ing performance are always on the left of the originalcurves.We are able to use less than 6% of the originalphrase table (40% of the table filtered with Finnish)to obtain translations with the same quality as theoriginal.
The extreme case, using only 1.4% of theoriginal table, leads to a reasonable BLEU score, in-dicating that most of the output sentences shouldstill be understandable.
In this case, the overall sizeof the phrase table and the reordering table was lessthan 100 megabytes, potentially feasible for mobiledevices, whereas the original models took nearly12.5 gigabytes of disk space.5.5 Different source languageBridge EP-40 EP-50?
5.1G 26.92 6.5G 27.23Dutch 562M 27.11 1.3G 28.14Spanish 3.0G 27.28 3.6G 28.09Danish 505M 28.04 780M 28.21Table 7: Filtered German-English systems (Size andBLEU)134In addition to Spanish-English translation, wealso conducted experiments on German-Englishtranslation.
The results, shown in Table 7, appearconsistent with the results of Spanish-English trans-lation.
Translations in most cases have performanceclose to the original unfiltered models, whereas thereduction in phrase-table size ranged from 40% to85%.
Meanwhile, translation speed has been in-creased up to 17%.Due to German?s rich morphology, the unfil-tered German-English models contain many moreentries than the Spanish-English ones constructedfrom similar data sets.
Unlike the Spanish-Englishmodels, the difference between ?EP-40?
and ?EP-50?
was not significant.
Neither was the differencebetween the impacts of the filtering in terms of trans-lation quality.
In addition, German and English areso dissimilar that none of the three bridge languageswe chose turned out to be significantly superior.6 ConclusionsWe highlighted one problem of the state-of-the-artSMT systems that was generally neglected: thenoise in the translation models.
Accordingly, weproposed triangulated filtering methods to deal withthis problem.
We used data in a third language as ev-idence to locate the less probable items in the trans-lation models so as to obtain the intersection of in-formation extracted from multilingual data.
Onlythe occurrences of complete phrases were taken intoaccount.
The probability distributions of the phraseshave not been considered so far.Although the approach was fairly naive, our ex-periments showed it to be effective.
The approacheswere applied to SMT systems built with the Mosestoolkit.
The translation quality was improved at least1 BLEU for all 15 cases (filtering 3 different modelswith 5 bridge languages).
The improvement can beas much as 2.25 BLEU.
It is also clear that the besttranslations were not linked to the largest translationmodels.
We also sketched a simple extension to thetriangulated filtering approach to further reduce themodel size, which allows us to generate reasonableresults with only 1.4% of the entries from the origi-nal table.The results varied for different bridge languagesas well as different models.
For translation fromSpanish to English, Finnish, the most distinctivebridge language, appeared to be a more effectiveintermediate language which could remove morephrase pair entries while still improving the transla-tion quality.
Portuguese, the most close to the sourcelanguage, always leads to a filtered model that pro-duces the best translations.
The selection of bridgelanguages has more obvious impact on the perfor-mance of our approach when the size of the modelto filter was larger.The work gave one instance of the general ap-proach described in Section 3.
There are severalpotential directions for continuing this work.
Themost straightforward one is to use our approacheswith more different languages, such as Chinese andArabic, and incompatible corpora, for example, dif-ferent segments of Europarl.
The main focus of suchexperiments should be verifying the conclusions wehad in this paper.AcknowledgmentsThis work was supported by European Communitythrough the EuroMatrix project funded under theSixth Framework Programme and the EuroMatrixPlus project funded under the Seventh FrameworkProgramme for Research and Technological Devel-opment.ReferencesYu Chen, Andreas Eisele, and Martin Kay.
2008.
Im-proving Statistical Machine Translation Efficiency byTriangulation.
In the 6th International Conferenceon Language Resources and Evaluation (LREC ?08),May.Trevor Cohn and Mirella Lapata.
2007.
MachineTranslation by Triangulation: Making Effective Useof Multi-Parallel Corpora.
In the 45th Annual Meet-ing of the Association for Computational Linguistics,Prague, Czech, June.Karim Filali and Jeff Bilmes.
2005.
Leveraging Multi-ple Languages to Improve Statistical MT Word Align-ments.
In IEEE Automatic Speech Recognition andUnderstanding (ASRU), Cancun, Mexico, November.J.
Howard Johnson, Joel Martin, George Foster, andRoland Kuhn.
2007.
Improving Translation Qual-ity by Discarding Most of the Phrasetable.
In the2007 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational Natural135Language Learning (EMNLP-CoNLL), Prague, CzechRepublic, June.Martin Kay.
1997.
The proper place of men and ma-chines in language translation.
Machine Translation,12(1-2):3?23.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In the 45thAnnual Meeting of the Association for ComputationalLinguistics (ACL), Prague, Czech Republic, June.Philipp Koehn.
2003.
Noun Phrase Translation.
Ph.D.thesis, University of Southern California.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In MT Summit 2005.Shankar Kumar, Franz Josef Och, and WolfgangMacherey.
2007.
Improving word alignment withbridge languages.
In the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 42?50, Prague, Czech.Franz Josef Och and Hermann Ney.
2001.
Statisticalmulti-source translation.
In MT Summit VIII, Santiagode Compostela, Spain.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In ACL ?03: Pro-ceedings of the 41st Annual Meeting on Associationfor Computational Linguistics, pages 160?167, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a method for automatic eval-uation of machine translation.
In the 40th AnnualMeeting on Association for Computational Linguis-tics, pages 311?318, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.Michel Simard.
1999.
Text-translation alignment: Threelanguages are better than two.
In EMNLP/VLC-99,College Park, MD, June.136
