Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 126?131,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsExploiting CCG Structures with Tree Kernels for Speculation DetectionLiliana Mamani Sa?nchez, Baoli Li, Carl VogelComputational Linguistics GroupTrinity College DublinDublin 2, Ireland{mamanisl,baoli.li,vogel}@tcd.ieAbstractOur CoNLL-2010 speculative sentencedetector disambiguates putative keywordsbased on the following considerations: aspeculative keyword may be composed ofone or more word tokens; a speculativesentence may have one or more specula-tive keywords; and if a sentence containsat least one real speculative keyword, it isdeemed speculative.
A tree kernel classi-fier is used to assess whether a potentialspeculative keyword conveys speculation.We exploit information implicit in treestructures.
For prediction efficiency, onlya segment of the whole tree around a spec-ulation keyword is considered, along withmorphological features inside the segmentand information about the containing doc-ument.
A maximum entropy classifieris used for sentences not covered by thetree kernel classifier.
Experiments on theWikipedia data set show that our systemachieves 0.55 F-measure (in-domain).1 IntroductionSpeculation and its impact on argumentation hasbeen studied by linguists and logicians since atleast as far back as Aristotle (trans 1991, 1407a,1407b), and under the category of linguistic?hedges?
since Lakoff (1973).
Practical appli-cation of this research has emerged due to theefforts to create a biomedical database of sen-tences tagged with speculation information: Bio-Scope (Szarvas et al, 2008) and because of theassociation of some kinds of Wikipedia data withthe speculation phenomenon (Ganter and Strube,2009).
It is clear that specific words can be con-sidered as clues that can qualify a sentence asspeculative.
However, the presence of a specu-lative keyword not always conveys a speculationassertion which makes the speculation detection atough problem.
For instance, the sentences belowcontain the speculative keyword ?may?, but onlythe sentence (a) is speculative.
(a) These effects may be reversible.
(b) Members of an alliance may not attack each other.The CoNLL-2010 Shared Task (Farkas et al,2010), ?Learning to detect hedges and their scopein natural language text?
proposed two tasks re-lated to speculation research.
Task 1 aims to detectsentences containing uncertainty and Task 2 aimsto resolve the intra-sentential scope of hedge cues.We engaged in the first task in the biomedical andWikipedia domains as proposed by the organizers,but eventually we got to submit only Wikipediadomain results.
However, in this paper we includeresults in the biomedical domain as well.The BioScope corpus is a linguistically hand an-notated corpus of negation and speculation phe-nomena for medical free texts, biomedical articleabstracts and full biomedical articles.
The afore-said phenomena have been annotated at sentencelevel with keyword tags and linguistic scope tags.Some previous research on speculation detectionand boundary determination over biomedical datahas been done by Medlock & Briscoe (2007) andO?zgu?r & Radev (2009) from a computational viewusing machine learning methods.The Wikipedia speculation dataset was gener-ated by exploiting a weasel word marking.
Asweasel words convey vagueness and ambiguity byproviding an unsupported opinion, they are dis-couraged by Wikipedia editors.
Ganter & Strube(2009) proposed a system to detect hedges basedon frequency measures and shallow information,achieving a F-score of 0.691.We formulate the speculation detection prob-lem as a word disambiguation problem and de-veloped a system as a pipelined set of natural1They used different Wikipedia data.126language processing tools and procedures to pre-process the datasets.
A Combinatory CategorialGrammar parsing (CCG) (Steedman, 2000) tooland a Tree Kernel (TK) classifier constitute thecore of the system.The Section 2 of this paper describes the over-all architecture of our system.
Section 3 depictsthe dataset pre-processing.
Section 4 shows howwe built the speculation detection module, outlinesthe procedure of examples generation and the useof the Tree-kernel classifier.
Section 5 presentsthe experiments and results, we show that sentenceCCG derivation information helps to differentiatebetween apparent and real speculative words forspeculation detection.
Finally Section 6 gives ourconclusions.2 Speculation detection systemOur system for speculation detection is a machinelearning (ML) based system (Figure 1).
In the pre-processing module a dataset of speculative/non-speculative sentences goes through a process ofinformation extraction of three kinds: specula-tive word or keyword extraction,2 sentence extrac-tion and document feature extraction (i.e docu-ment section).
Later the extracted keywords areused to tag potential speculative sentences in thetraining/evaluation datasets and used as featuresby the classifiers.
The sentences are submitted tothe tokenization and parsing modules in order toprovide a richer set of features necessary for creat-ing the training/evaluation datasets, including thedocument features as well.In the ML module two types of dataset are built:one used by a TK classifier and other one by a bag-of-features based maximum entropy classifier.
Asthe first one processes only those sentences thatcontain speculative words, we use the second clas-sifier, which is able to process samples of all thesentences.The models built by these classifiers are com-bined in order to provide a better performance andcoverage for the speculation problem in the clas-sification module which finally outputs sentenceslabeled as speculative or non-speculative.
Usedtools are the GeniaTagger (Tsuruoka et al, 2005)for tokenization and lemmatization, and the C&CParser (Clark and Curran, 2004).
The next sec-tions explain in detail the main system compo-nents.2Extraction of keywords for the training stage.3 Dataset pre-processing for rich featureextractionThe pre-processing module extracts keywords,sentences and document information.All sentences are processed by the tok-enizer/lemmatizer and at the same time specific in-formation about the keywords is extracted.Speculative keywordsSpeculative sentences are evidenced by the pres-ence of speculation keywords.
We have the fol-lowing observations:?
A hedge cue or speculative keyword 3 may becomposed of one or more word tokens.?
In terms of major linguistic categories, theword tokens are heterogeneous: they may beverbs, adjectives, nouns, determiners, etc.
Astop-word removing strategy was dismissed,since no linguistic category can be elimi-nated.?
A keyword may be covered by another longerone.
For instance, the keyword most can beseen in keywords like most of all the heroesor the most common.Considering these characteristics for each sen-tence, in the training stage, the keyword extractionmodule retrieves the speculative/non-speculativeproperty of each sentence, the keyword occur-rences, number of keywords in a sentence, the ini-tial word token position and the number of wordtokens in the keyword.
We build a keyword lex-icon with all the extracted keywords and theirfrequency in the training dataset, this speculativekeyword lexicon is used to tag keyword occur-rences in non-speculative training sentences andin all the evaluation dataset sentences.The overlapping problem when tagging key-words is solved by maximal matching strategy.
Itis curious that speculation phrases come in de-grees of specificity; the approach adopted herefavors ?specific?
multi-word phrases over single-word expressions.Sentence processingOften, speculation keywords convey certain in-formation that can not be successfully expressedby morphology or syntactic relations provided byphrase structure grammar parsers.
On the other3Or just ?keyword?
for sake of simplicity.127Figure 1: Block diagram for the speculation detection system.hand, CCG derivations or dependencies providedeeper information, in form of predicate-argumentrelations.
Previous works on semantic role label-ing (Gildea and Hockenmaier, 2003; Boxwell etal., 2009) have used features derived from CCGparsings and obtained better results.C&C parser provides CCG predicate-argumentdependencies and Briscoe and Carroll (2006) stylegrammatical relations.
We parsed the tokenizedsentences to obtain CCG derivations which arebinary trees as shown in the Figure 2.
TheCCG derivation trees contain function categoryand part-of-speech labels; this information is con-tained in the tree structures to be used in buildinga subtree dataset for the TK classifier.4 Speculative sentence classifier4.1 Tree Kernel classificationThe subtree dataset is processed by a Tree Kernelclassifier (Moschitti, 2006) based on Support Vec-tor Machines.
TK uses a kernel function betweentwo trees, allowing a comparison between theirsubstructures, which can be subtrees (ST) or sub-set trees (SST).
We chose the comparison betweensubset trees since it expands the kernel calculationto those substructures with constituents that arenot in the leaves.
Our intuition is that real specula-tive sentences have deep semantic structures thatare particularly different from those ones in ap-parent speculative sentences, and consequently thecomparison between the structures of well identi-fied and potential speculative sentences may en-hance the identification of real speculative key-words.4.2 Extracting tree structuresThe depth of a CCG derivation tree is propor-tional to the number of word tokens in the sen-tence.
Therefore, the processing of a whole deriva-tion tree by the classifier is highly demanding andmany subtrees are not relevant for the classifica-tion of speculative/non-speculative sentences, inparticular when the scope of the speculation is asmall proportion of a sentence.In order to tackle this problem, a fragment ofthe CCG derivation tree is extracted.
This frag-ment or subtree spans the keyword together withneighbors terms in a fixed-size window of n wordtokens, (i.e.
n word tokens to the left and n wordtokens to the right of the keyword) and has as rootthe lower upper bound node of the first and lasttokens of this span.
After applying the subtree ex-traction, the subtree can contain more word tokensin addition to those contained in the n-span, whichare replaced by a common symbol.Potential speculative sentences are turned intotraining examples.
However, as described in Sec-tion 3, a speculative sentence can contain one ormore speculative keywords.
This can produce anoverlapping between their respective n-spans ofindividual keywords during the subtree extraction,producing subtrees with identical roots for bothkeywords.
For instance, in the following sen-tence(c), the spans for the keywords suggests andthought will overlap if n = 3.
(c) This suggests that diverse agents thought to ac-tivate NF-kappa B ...The overlapping interacts with the windows sizeand potential extraction of dependency relations128It was reported to have burned for a dayPRP VBD VBN TO VB VBN IN DT NNNP (S[dcl]\NP)/(S[pss]\NP) (S[pss]\NP)/(S[to]\NP) (S[to]\NP)/(S[b]\NP) (S[b]\NP)/(S[pt]\NP) S[pt]\NP ((S\NP)\(S\NP))/NP NP[nb]/N NNP[nb](S[X]\NP)\(S[X]\NP)S[pt]\NPS[b]\NPS[to]\NPS[pss]\NPS[dcl]\NPS[dcl]Figure 2: CCG derivations tree for It was reported to have burned for a day.shared by terms belonging to the two differentspans.
We deal with this issue by extracting onetraining example if two spans have a common rootand two different examples otherwise.4.3 Bag of features modelBy default, our system classifies the sentences notcovered by the TK model using a baseline clas-sifier that labels a sentence as speculative if thishas at least one keyword.
Alternatively, a bag offeatures classifier is used to complement the treekernel, aimed to provide a more precise methodthat might detect even speculative sentences withnew keywords in the evaluation dataset.
The set offeatures used to build this model includes:a) Word unigrams;b) Lemma unigrams;c) Word+POS unigrams;d) Lemma+POS unigrams;e) Word+Supertag unigrams;f) Lemma+Supertag unigrams;g) POS+Supertag unigrams;h) Lemma bigrams;i) POS bigrams;j) Supertag bigrams;k) Lemma+POS bigrams;l) Lemma+Supertag bigrams;m) POS+Supertag bigrams;n) Lemma trigrams;o) POS trigrams;p) Supertag trigrams;q) Lemma+POS trigrams;r) Lemma+Supertag trigrams;s) POS+Supertag trigrams;t) Number of tokens;u) Type of section in the document (Title, Text,Section);v) Name of section in the document;w) Position of the sentence in a section startingfrom beginning;Dataset Dev.
Train.
Eval.Biomedical 39 14541 5003Wikipedia 124 11111 9634Table 1: Datasets sizes.x) Position of the sentence in a section startingfrom end.Position of the sentence information, composed bythe last four features, represents the informationabout the sentence relative to a whole document.The bag of features model is generated using aMaximum Entropy algorithm (Zhang, 2004).5 Experiments and results5.1 DatasetsIn the CoNLL-2010 Task 1, biomedical andWikipedia datasets were provided for develop-ment, training and evaluation in the BioScopeXML format.
Development and training datasetsare tagged with cue labels and a certainty feature.4The number of sentences for each dataset 5 is de-tailed in Table 1.After manual revision of sentences not parsedby C&C parser, we found that they contain equa-tions, numbering elements (e.g.
(i), (ii).. 1),2) ), or long n-grams of named-entities, for in-stance: ...mannose-capped lipoarabinomannan (ManLAM ) of Mycobacterium tuberculosis ( M.tuberculosis )... that out of a biomedical domainappear to be ungrammatical.
Similarly, in theWikipedia datasets, some sentences have manynamed entities.
This suggests the need of a spe-cific pre-processor or a parser for this kind of sen-tences like a named entity tagger.In Table 2, we present the number of parsed sen-tences, processed sentences by the TK model andexamples obtained in the tree structure extraction.4certainty=?uncertain?
and certainty=?certain?.5The biomedical abstracts and biomedical articles trainingdatasets are processed as a single dataset.129Dataset Parsed Process.
SamplesBiomedical train.
14442 10852 23511Biomedical eval.
4903 3395 7826Wikipedia train.
10972 7793 13461Wikipedia eval.
9559 4666 8467Table 2: Count of processed sentences.5.2 Experimental resultsThe CoNLL-2010 organizers proposed in-domainand cross-domain evaluations.
In cross-domainexperiments, test datasets of one domain can beused with classifiers trained on the other or on theunion of both domains.
We report here our resultsfor the Wikipedia and biomedical datasets.So far, we mentioned two settings for our clas-sifier: a TK classifier complemented by a baselineclassifier (BL) and TK classifier complementedby a bag of features classifier (TK+BF).
Table3 shows the scores of our submitted system (in-domain Task 1) on the Wikipedia dataset, whereasTable 4 gives the scores of the baseline system.TP FP FN Precision Recall FOur system 1033 480 1201 0.6828 0.4624 0.5514Max.
1154 448 1080 0.7204 0.5166 0.6017Min.
147 9 2087 0.9423 0.0658 0.123Table 3: Comparative scores for our system withCoNLL official maximum and minimum scores inTask 1, Wikipedia dataset in-domain.TP FP FN Precision Recall FBiomedical 786 2690 4 0.2261 0.9949 0.3685Wikipedia 1980 2747 254 0.4189 0.8863 0.5689Table 4: Baseline results.Additionally, we consider a bag of features clas-sifier (BF) and a classifier that combines the base-line applied to the sentences that have at least onekeyword plus the BF classifier for the remainingsentences (BL+BF).
In Tables 5 to 10, results forthe four classifiers (TK, TK+BF, BF, BL+BF) withevaluations in-domain and cross-domain are pre-sented6.The baseline scores confirm that relying on justthe keywords is not enough to identify speculativesentences.
In the biomedical domain, the classi-fiers give high recall but too low precision result-ing in low F-scores.
Still, the TK, TK+BF and BF(in-domain configurations) gives much better re-sults than BL and BL+BF which indicates that theinformation from CCG improves the performance6It is worth to note that the keyword lexicons have beennot used in cross-domain way, so the TK and TK+BF modelshave not been tested in regards to keywords.TP FP FN Precision Recall FBL 1980 2747 254 0.4189 0.8863 0.5689TK 1033 480 1201 0.6828 0.4624 0.5514TK+BF 1059 516 1175 0.6729 0.4740 0.5560BF 772 264 1462 0.7452 0.3456 0.4722BL+BF 2028 2810 206 0.4192 0.9078 0.5735Table 5: Results for Wikipedia dataset in-domain.TP FP FN Precision Recall FBL 1980 2747 254 0.4189 0.8863 0.5689TK 1776 2192 458 0.4476 0.7950 0.5727TK+BF 1763 2194 471 0.4455 0.7892 0.5695BF 403 323 1831 0.5551 0.1804 0.2723BL+BF 1988 2772 246 0.4176 0.8899 0.5685Table 6: Wikipedia data classified with biomedicalmodel scores (cross-domain).TP FP FN Precision Recall FBL 1980 2747 254 0.4189 0.8863 0.5689TK 1081 624 1153 0.6340 0.4839 0.5489TK+BF 1099 636 1135 0.6334 0.4919 0.5538BF 770 271 1464 0.7397 0.3447 0.4702BL+BF 2017 2786 217 0.4199 0.9029 0.5733Table 7: Wikipedia data classified with biomedical+ Wikipedia model scores (cross-domain).TP FP FN Precision Recall FBL 786 2690 4 0.2261 0.9949 0.3685TK 759 777 31 0.4941 0.9606 0.6526TK+BF 751 724 39 0.5092 0.9506 0.6631BF 542 101 248 0.8429 0.6861 0.7565BL+BF 786 2695 4 0.2258 0.9949 0.3681Table 8: Biomedical data scores (in-domain).TP FP FN Precision Recall FBL 786 2690 4 0.2261 0.9949 0.3685TK 786 2690 4 0.2261 0.9949 0.3685TK+BF 771 2667 19 0.2243 0.9759 0.3647BF 174 199 616 0.4665 0.2206 0.2992BL+BF 787 2723 3 0.2242 0.9962 0.3660Table 9: Biomedical data classified withWikipedia model scores (cross-domain).TP FP FN Precision Recall FBL 786 2690 4 0.2261 0.9949 0.3685TK 697 357 93 0.6613 0.8823 0.7560TK+BF 685 305 105 0.6919 0.8671 0.7697BF 494 136 296 0.7841 0.6253 0.6958BL+BF 786 2696 4 0.2257 0.9949 0.3679Table 10: Biomedical data classified with biomed-ical + Wikipedia model scores (cross-domain).of the classifiers when compared to the baselineclassifier.Even though in the Wikipedia domain theTK+BF score is less than the baseline score, stillthe performance of the classifiers do not fall muchin any of the in-domain and cross-domain exper-iments.
On the other hand, BF does not have agood performance in 5 of 6 the experiments.
Tomake a more precise comparison between TK andBF, the TK and BL+BF scores show that BL+BFperforms better than TK in only 2 of the 6 ex-periments but the better performances achievedby BL+BF are very small.
This suggests that130the complex processing made by tree kernels ismore useful when disambiguating speculative key-words than BF.
Nonetheless, the bag-of-featuresapproach is also of importance for the task at handwhen combined with TK.
We observe that the TKclassifer and BF classifier perform well making usbelieve that the CCG derivations provide relevantinformation for speculation detection.
The use oftree kernels needs further investigations in order toevaluate the suitability of this approach.6 Concluding remarksSpeculation detection is found to be a tough taskgiven the high ambiguity of speculative keywords.We think these results can be improved by study-ing the influences of context on speculation asser-tions.This paper presents a new approach for disam-biguating apparent speculative keywords by us-ing CCG information in the form of supertags andCCG derivations.
We introduce the use of the treekernel approach for CCG derivations trees.
Theinclusion of other features like grammatical rela-tions provided by the parser needs to be studiedbefore incorporating this information into the cur-rent classifier and possibly to resolve the boundaryspeculation detection problem.AcknowledgmentsThis research is supported by the Trinity CollegeResearch Scholarship Program and the ScienceFoundation Ireland (Grant 07/CE/I1142) as partof the Centre for Next Generation Localisation(www.cngl.ie) at Trinity College of Dublin.ReferencesAristotle.
trans.
1991.
The Art of Rhetoric.
PenguinClassics, London.
Translated with an Introductionand Notes by H.C. Lawson-Tancred.Stephen Boxwell, Dennis Mehay, and Chris Brew.2009.
Brutus: A semantic role labeling system in-corporating CCG, CFG, and dependency features.In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP, pages 37?45, Suntec, Singapore.Ted Briscoe and John Carroll.
2006.
Evaluating theaccuracy of an unlexicalized statistical parser onthe PARC DepBank.
In Proceedings of the COL-ING/ACL on Main conference poster sessions, pages41?48, Morristown, NJ, USA.Stephen Clark and James R. Curran.
2004.
Parsingthe WSJ using CCG and log-linear models.
In ACL?04: Proceedings of the 42nd Annual Meeting on As-sociation for Computational Linguistics, page 103,Morristown, NJ, USA.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010 Shared Task: Learning to Detect Hedges andtheir Scope in Natural Language Text.
In Proceed-ings of the Fourteenth Conference on ComputationalNatural Language Learning (CoNLL-2010): SharedTask, pages 1?12, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.Viola Ganter and Michael Strube.
2009.
Findinghedges by chasing weasels: Hedge detection usingWikipedia tags and shallow linguistic features.
InProceedings of the ACL-IJCNLP 2009 ConferenceShort Papers, pages 173?176, Suntec, Singapore.Daniel Gildea and Julia Hockenmaier.
2003.
Identi-fying semantic roles using combinatory categorialgrammar.
In Proceedings of 2003 Conference onEmpirical Methods in Natural Language Processing(EMNLP), Sapporo, Japan.George Lakoff.
1973.
Hedges: A study in meaningcriteria and the logic of fuzzy concepts.
Journal ofPhilosophical Logic, 2(4):458?508.Ben Medlock and Ted Briscoe.
2007.
Weakly super-vised learning for hedge classification in scientificliterature.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 992?999, Prague, Czech Republic.AlessandroMoschitti.
2006.
Making tree kernels prac-tical for natural language learning.
In Proceedingsof the 11th Conference of the European Chapter ofthe Association for Computational Linguistics.Arzucan O?zgu?r and Dragomir R. Radev.
2009.
Detect-ing speculations and their scopes in scientific text.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages1398?1407, Singapore.Mark Steedman.
2000.
The syntactic process.
MITPress, Cambridge, MA, USA.Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, andJa?nos Csirik.
2008.
The BioScope corpus: anno-tation for negation, uncertainty and their scope inbiomedical texts.
In Proceedings of the Workshopon Current Trends in Biomedical Natural LanguageProcessing, pages 38?45, Columbus, Ohio.Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,Tomoko Ohta, John McNaught, Sophia Ananiadou,and Jun?ichi Tsujii.
2005.
Developing a robust part-of-speech tagger for biomedical text.
In Advances inInformatics, pages 382?392.Le Zhang.
2004.
Maximum entropy modeling toolkitfor Python and C++ (version 20041229).
In NaturalLanguage Processing Lab, Northeastern.131
