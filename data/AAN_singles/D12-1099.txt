Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1081?1092, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsBuilding a Lightweight Semantic Model for Unsupervised InformationExtraction on Short ListingsDoo Soon KimAccenture Technology Lab50 W San Fernando St.,San Jose, CA, 95113Kunal VermaAccenture Technology Lab50 W San Fernando St.,San Jose, CA, 95113{doo.soon.kim, k.verma, peter.z.yeh}@accenture.comPeter Z. YehAccenture Technology Lab50 W San Fernando St.,San Jose, CA, 95113AbstractShort listings such as classified ads or productlistings abound on the web.
If a computer canreliably extract information from them, it willgreatly benefit a variety of applications.
Shortlistings are, however, challenging to processdue to their informal styles.
In this paper, wepresent an unsupervised information extrac-tion system for short listings.
Given a cor-pus of listings, the system builds a seman-tic model that represents typical objects andtheir attributes in the domain of the corpus,and then uses the model to extract informa-tion.
Two key features in the system are a se-mantic parser that extracts objects and their at-tributes and a listing-focused clustering mod-ule that helps group together extracted tokensof same type.
Our evaluation shows that thesemantic model learned by these two modulesis effective across multiple domains.1 IntroductionShort listings such as classified ads or product list-ings are prevalent on the web.
These texts are gen-erally concise ?
around 10 words in length.
Fig.
1shows some example listings.
Due to the recent ex-plosive growth of such listings, extracting informa-tion from them becomes crucial for tasks such asfaceted search and reasoning.
For example, con-sider an online shopping site on which informationabout merchandises for sale is posted.
Detectingbrands/styles/features that are frequently mentionedin the postings would allow a company to design abetter marketing strategy.Most Information Extraction (IE) techniques de-veloped for formal texts, however, would be inap-plicable to listings because of their informal and id-iosyncratic styles.
For example, typos, abbreviationsand synonyms often appear and should be resolved(e.g., apartment/apt, bike/bicycle).
Symbols couldhave the special meanings (e.g., x in 2x2 in Fig.
1indicates number of bedrooms and bathrooms).
To-kenization based only on space is insufficient (e.g.,RawlingBaseball in Fig.
1).
Multiwords such asgranite top should also be detected.
Applying off-the-shelf parsers is infeasible because of unusualphrasal forms in most listings, such as a long se-quence of nouns/adjectives (e.g., ?New Paint WoodFloors New Windows Gated Complex?
)To address these challenges, several approacheshave applied machine learning algorithms (Ghani etal., 2006) (Putthividhya and Hu, 2011) or an externalknowledge base (Michelson and Knoblock, 2005).These approaches, however, commonly require hu-man supervision to produce training data or to builda knowledge base.
This is expensive, requiring re-peated manual effort whenever a new domain or anew set of information to be extracted is introduced.In this paper, we present an unsupervised IE sys-tem for listings.
The system extracts tokens froma corpus of listings and then clusters tokens ofthe same types, where each resulting cluster cor-responds to an information type (e.g., size, brand,etc.).
For formal texts, contexts (e.g., surroundingwords) have been a major feature for word cluster-ing (Turney and Pantel, 2010).
This feature alone,however, is insufficient for short listings because oflack of contextual clues in short listings.10812x2 Charming Condo ?
1515 Martin Av-enue near Downtown (from Craigslist)HousingType: Condo, BedroomNum: 2, BathroomNum: 2, Lo-cation: 1515 Martin Avenue, Neighborhood: DowntownRawlingsBaseball Gloves Pro PreferredBlack 12?
(from EBay)ProductType: Gloves, Brand: Rawlings, Sport: Baseball, Color:Black, Size: 12?, SeriesName: Pro PreferredLG 32?
1080p LCD TV ?
$329 @BestBuy (from FatWallet)Product Type: TV, Brand: Panasonic, Size: 32?, Resolution :1080P, DisplayTechnology: LCD, Price 329$, Seller: Best BuyFigure 1: Example short listings and the information extracted from them.To address this limitation of context-based clus-tering, we first identify common types of informa-tion (main objects and their attributes) representedin listings and apply customized clustering for thesetypes.
Specifically, we define a semantic modelto explicitly represent these information types, andbased on the semantic model, develop two compo-nents to improve clustering ?
a shallow semanticparser and listing-focused clustering module.
Thesemantic parser specifically focuses on extractingmain objects and the listing-focused clustering mod-ule helps group together extracted tokens of thesame type.Our evaluation shows that our two main con-tributions (shallow semantic parser and listing-focused clustering) significantly improve perfor-mance across three different domains ?
Craigslist,EBay, and FatWallet.
Our system achieves .50?.65F1-score for the EBay and the FatWallet datasetsbased on gold standards constructed by human an-notators.
For the Craigslist dataset, which is moredifficult than the other two, F1-score is .35.2 Related WorkIE on Listings.
(Ghani et al 2006) (Putthividhyaand Hu, 2011) propose semi-supervised approachesto extract product types and their attributes fromproduct listings.
(Ghani et al 2006) applies theEM (Expectation-Maximization) algorithm to incor-porate unlabelled data.
(Putthividhya and Hu, 2011)uses unlabelled data to build dictionaries of values tobe extracted (e.g., brand or model names), which arethen used as a feature for a machine learning system.
(Michelson and Knoblock, 2005) uses a manually-crafted knowledge base called reference set to de-fine standard forms of values to be extracted.
Usinga string edit function, the system then identifies to-kens in listings that have a low distance score withthe values defined in the reference set.
They alsopropose a semi-supervised method to building ref-erence sets (Michelson and Knoblock, 2009).Unlikethese systems, our approach is unsupervised.Unsupervised Information Extraction.
Most un-supervised IE systems produce clusters for tokensof same type extracted from a corpus of unlabelledtexts.
(Chambers and Jurafsky, 2011) (Poon andDomingos, 2010) (Chen et al 2011) focus on ex-tracting frame-like structures (Baker et al 1998)by defining two types of clusters, event clusters androle clusters.
Event clusters define an event (a sit-uation or a frame) such as BOMBING by clusteringverbs/nominalized verbs such as {kill, explosion}.Role clusters define the semantic roles of the event(e.g., {terrorist, gunman} for the Perpetrator role inBOMBING).
Similarly, our system defines two typesof clusters ?
the main concept clusters (e.g., TV orbook) and the attribute clusters (e.g., size, color).
(Chambers and Jurafsky, 2011) is similar to our ap-proach in that it learns a semantic model, called tem-plate, from unlabelled news articles and then usesthe template to extract information.Our system is different because it focuses on in-formal listings, which the components (such as aparser) used by these systems cannot handle.Field Segmentation (Sequence Modelling).
Thistask focuses on segmenting a short text, such asbibliographies or listings.
(Grenager et al 2005)presents an unsupervised HMM based on the obser-vation that the segmented fields tend to be of mul-tiple words length.
(Haghighi and Klein, 2006)exploits prototype words (e.g., close, near, shop-ing for the NEIGHBORHOOD attribute) in an un-supervised setting.
(Chang et al 2007) incorpo-rates domain specific constraints in semi-supervisedlearning.
Our task is different than these systems be-cause we focus on extracting information to enablea variety of automated applications such as business1082intelligence reporting, faceted search or automatedreasoning, rather than segmenting the text.
The seg-mented fields are often a long unstructured text (e.g.,2 bath 1 bed for size rather than 2 for BathroomNumand 1 for BedroomNum).IE on Informal Texts.
IE on informal texts is get-ting much attention because of the recent explosivegrowth of these texts.
The informal texts that areattempted for IE include online forums (Gruhl etal., 2009), SMS (Beaufort et al 2010), twitter mes-sages (Liu et al 2011).3 Our ApproachGiven a corpus of listings for a domain of interest,our system constructs a semantic model that repre-sents the types of information and their values to beextracted.
Our system then uses the resulting modelto extract both the type and value from the corpus 1.We first describe the semantic model and then thetwo key steps for creating this model ?
shallow se-mantic parsing and listing-focused clustering.
Fig.
3illustrates these two steps with example listings.3.1 Semantic ModelOur semantic model captures two important piecesof information ?
the main concept and its attributes.This representation is based on the observation thatmost listings (e.g.
rentals, products) describe theattributes of a single object (i.e.
the main conceptin our model).
Our system takes advantage of thisobservation by applying customized clustering foreach type of information in the model, which resultsin better performance compared to a one-size-fits-allalgorithm.
Moreover, this model is general enoughto be applicable across a wide range of domains.
Wequantitatively show both benefits in our evaluation.Fig.
2 illustrates our model along with an instanti-ation for rental listings.
The main concept is a clus-ter containing tokens referencing the main object inthe listing.
For example, in the rental listing, themain concept cluster includes tokens such as house,condo, and townhouse.Each attribute of the main concept (e.g.
Address,BedroomNum, etc.)
is also a cluster, and two types1To handle string variations (e.g., typos) during extraction,our system uses a string edit distance function, Jaro-Winklerdistance (Winkler, 1990) with a threshold, 0.9.
* Main Concept{house, condo, apt., apartment, townhouse, ?
}* Quantitative Attribute+{bedroom, bdrm, bd, bed,?
}* Qualitative Attribute+{washer, dryer, w/d, washer hookup, d hookup,?
}Figure 2: Semantic model and its instantiation for rentallistings.
+ indicates multiple clusters can be created.of attributes are defined in our model ?
quantita-tive attributes and qualitative ones.
Quantitative at-tributes capture numeric values (e.g.
1 bedroom, 150Hz, and 70 kg), and are generally a number followedby a token indicating the attribute (e.g., unit of mea-surement).
Hence, clusters for quantitative attributesinclude these indicator tokens (see Fig.
2).Qualitative attributes capture descriptions aboutthe main concept (e.g., address, shipping informa-tion, condition).
The values of these attributes gen-erally appear in listings without explicitly mention-ing the names of these attributes.
Hence, the clustersfor qualitative attributes include tokens correspond-ing to the values themselves (e.g.
washer hookup).3.2 Shallow Semantic ParserOur Shallow Semantic Parser (SSP) analyzes an in-put corpus to produce a partial semantic model.
SSPfirst performs preprocessing and multiword detec-tion.
SSP then identifies which resulting tokens arethe main concepts and which are their attributes.3.2.1 Preprocessing and Multiword DetectionSSP preprocesses the corpus through three steps.
(1) SSP cleans the corpus by removing duplicatelistings and HTML expressions/tags.
(2) SSP tok-enizes each listing based on spaces along with cus-tom heuristics ?
e.g., handling alpha-numeric to-kens starting with numbers (e.g., 3bedroom to 3 bed-room) and mixed case tokens (e.g., NikeShoes toNike Shoes).
(3) SSP performs POS tagging using anoff-the-shelf tagger (Tsuruoka and Tsujii, 2005).
Toimprove accuracy, SSP assigns to a token the mostfrequent POS across all occurrences of that token.This heuristic works well because most tokens in fo-cused domains, like listings, have only one POS.SSP then detects multiword tokens based on thefollowing rules:1083A corpus of listings Semantically analyzed listings Semantic model?
Brentwood Apt.
with 3 bedroom?
2 BD/ 2 BA +Den ?
Open Sun 2/12?
Affordable Rental Apartments-Come take a Look!?
[brentwood/ATTR] [apt./MC] with 3[bedroom/ATTR]?
2 [BD/ATTR]/ 2 [BA/ATTR] +[den/ATTR] ?
[opensun 2/12/ATTR]?
[affordable rental/ATTR] [apartments/MC]-come take a look!
* Main Concept{apt., apartment}* Location{brentwood}* BedroomNum{bedroom, bd}Shallow SemanticParserListing-FocusedClusteringFigure 3: Steps of our system: The parser tokenizes listings (multiword detection) and then identifies main conceptsand attributes (marked as MC and ATTR).
The clustering module then clusters tokens of the same type.
The tags (suchas Location, BedroomNum) are included to help understand the figure.
They are not produced by the system.1.
If a bigram (e.g., top floor) in a listing fre-quently appears as either a single or dashed token(e.g., TopFloor or top-floor) in other listings, thenthe bigram is regarded as a multiword.2.
For each bigram, w1 w2 (excluding symbolsand numbers), if the conditional probability of thebigram given either w1 or w2 (i.e., p(w1w2 |w1(or w2)) is high (over 0.75 in our system)), thebigram is considered as a candidate multiword.
Thisrule tests the tendency of two tokens appearing to-gether when either one appears.However, this test alone is insufficient, as it of-ten generates coarse-grained results ?
e.g., baseballglove, softball glove, and Hi-Def TV 2.
To preventthis problem, for each w2, we measure the entropyover the distribution of the tokens in the w1 position.Our intuition is that high variability in the w1 posi-tion (i.e., high entropy) indicates that the multiwordis likely a breakable phrase.
Hence, those candidateswith high entropy are removed.SPP repeatedly applies the above rules to acquiremultiwords of arbitrary length.
In our implementa-tion, we limit multiword detection up to four-gram.3.2.2 Main Concept IdentificationSSP then identifies the main concepts (mc words)and their attributes (attrs) to produce a partial se-mantic model.
This process is guided by the ob-servation that main concepts tend to appear as headnouns in a listing and attributes as the modifiers ofthese head nouns (see the examples in Fig.
3).2Even though these examples are legitimate multiwords,they overlook useful information such as baseball and softballare types of gloves and Hi-Def is an attribute of TV.Algorithm 1 describes the discovery process ofmc words and attrs.
First, SSP initializes attrs withtokens that are likely to be a modifier (line 2), bychoosing tokens that frequently appear as the objectof a preposition within the corpus ?
e.g., for rent,with washer and dryer, for baseball.SSP then iteratively performs two steps ?
PARSEand EXPANDMODEL (lines 3 ?
6) ?
in a boot-strap manner (see Fig.
4).
PARSE tags the noun to-kens in each listing as either head nouns or modi-fiers.
Specifically, PARSE first assesses if a listingis ?hard?
to parse (line 10) based on two criteria ?
(1)the listing contains a long sequence of nouns (sevenwords or longer in our system) without any prepo-sitions (e.g., worth shutout series 12?
womens fast-pitch softball fielders glove s0120 lefty); and (2) themajority of these nouns do not appear in mc wordsand attrs (e.g., over 70% in our system).
The listingsmeeting these criteria are generally difficult to rec-ognize the head noun without any semantic knowl-edge.
PARSE will revisit these listings in the nextround as more mc words and attrs are identified.If a listing does not meet these criteria, PARSEtags nouns appearing in mc words and attrs ashead nouns and modifiers respectively (line 11).
Ifthis step fails to recognize a head noun, a heuris-tic is used to identify the head noun ?
it identi-fies the first noun phrase by finding a sequence ofnouns/adjectives/numbers, and then tags as the headnoun the last noun in the phrase that is not tagged asa modifier (line 13).
For example, in the first listingof Fig.
3, brentwood apts.
is the first noun phrasethat meets the condition above; and hence apt.
istagged as the head noun.
The remaining untaggednouns in the listing are tagged as modifiers (line 15).1084Algorithm 1 Extracting main concepts1: Input: POS-tagged corpus, corp2: Initialize attrs3: repeat4: (hn,mod) = Parse(corp, mc words, attrs)5: (mc words,attrs) = ExpandModel(hn,mod)6: until mc words, attrs not changed7:8: function PARSE(mc words, attrs)9: for all each listing do10: if parsible then11: Parse with mc words, attrs12: if headnoun is not tagged then13: Tag the last noun in the first nounphrase that are not a modifier as hn14: end if15: Tag the other nouns as mod16: end if17: end for18: end function19:20: function EXPANDMODEL(corp)21: For each token, calculate a ratio of as a headnoun to as a modifier22: Add tokens with high ratio to mc words23: Add tokens with low ratio to attrs24: end functionEXPANDMODEL assigns tokens to eithermc words or attrs based on the tags generatedby PARSE.
For each token, EXPANDMODELcounts the frequency of the token being tagged as ahead noun and as a modifier.
If a token is predom-inately tagged as a head noun (or a modifier), thetoken is added to mc words (or attrs) 3.This bootstrap method is advantageous 4 be-cause SSP can initially focus on easy cases ?i.e., mc words and attrs that can be detected withhigh confidence, such as condo(mc words) and bed-room(attrs), which often appear as a head noun anda modifier in the easy-to-parse listings.
These re-sults can help the system to parse more difficult3In our system, if the ratio of the frequency of the head nounto the frequency of the modifier is over .55, the token is addedto mc words.
If less than .35, it is added to attrs.4The bootstrapping cycle generally ends within 3?4 itera-tions.?
mc_words?
attributesExpandModel?
Listings with head nounand modifiers detectedParseFigure 4: Bootstrapped PARSE and EXPANDMODELlistings.
For example, identifying condo(mc words)helps parsing the following more difficult listing,?2 bedroom 1 bathroom condo large patio washerdryer available?
?
condo would be tagged as a headnoun and the rest of the nouns as modifiers.The result of this step is a partial semantic modelthat contains a cluster for the main concept and a listof candidate attribute tokens.3.3 Listing-Focused ClusteringListing-Focused Clustering (LFC) further expandsthe partial semantic model (constructed by SSP) bygrouping the remaining candidate attribute tokensinto attribute clusters ?
i.e.
one cluster for each at-tribute of the main concept.
LFC may also add atoken to the main concept cluster if appropriate.For formal texts, distributional similarity iswidely used for clustering words because the con-textual clues in these texts are sufficiently discrimi-native (Lin and Pantel, 2001).
This feature alone,however, is insufficient for listings because theylack discriminative contexts due to the short length.Hence, our approach augments context-based sim-ilarity with the following rules (presented in orderof precedence), based on general properties we ob-served from listing data across various domains.?
Two quantitative attribute tokens cannot beplaced into the same cluster if they frequentlyappear together in a listing.
For example, bedand bath should not be clustered because theyfrequently appear together (e.g.
2 bed / 2bath).This rule is based on the observation that aquantitative attribute is likely to appear onlyonce in a listing.
To enforce this restriction, forall pairs of tokens, t1 and t2, LFC measures theconditional probability of the pair appearing to-gether in a listing given the appearance of either1085t1 and t2.
If any of these conditional probabili-ties are high, t1 and t2 are not clustered.?
Attribute types are strongly enforced by neverclustering together a quantitative attribute to-ken and a qualitative attribute token.
The typeof a token is determined by analyzing the im-mediate preceding tokens throughout the cor-pus.
If the preceding tokens are generally num-bers, then LFC regards the token as a quantita-tive attribute.
Otherwise, the token is regardedas a qualitative attribute.?
Two tokens are similar if the characters in onetoken appear in the other, preserving the order(e.g., bdrm and bedroom)If the above rules fail to determine the similaritybetween two tokens, LFC reverts to context-basedsimilarity.
For each token, LFC creates a contextvector containing frequencies of the context wordsaround the token with a window size of two.
Forexample, in the first sentence in Fig.
3, the contextwords around apts.
are l-start (beginning of the sen-tence), l-brentwood, r-with, and r-3 5.
The frequen-cies in these vectors are also weighted using PMIscores (pointwise mutual information) between a to-ken and its context words, as suggested by (Turneyand Pantel, 2010).
The intuition is that a high PMIindicates a context word is strongly associated with atoken and hence has high discriminative power.
Wealso apply a smoothing function suggested in (Tur-ney and Pantel, 2010) to mitigate PMI?s bias towardsinfrequent events.
The similarity score is based on acosine similarity between the two weighted vectors.Based on this similarity function, LFC applies ag-glomerative clustering (with average linkage) to pro-duce attribute clusters (or to expand the main con-cept cluster).
However, calculating similarity scoresfor all pairs of tokens is expensive.
To addressthis problem, LFC performs clustering in two steps.First, LFC performs agglomerative clustering on allpairs of tokens with high frequency 6.
LFC then cal-culates the similarity between each low-frequencytoken and the clusters resulting from the previousstep.
If the similarity score is over a user-specified5l and r indicates the left or right window.6The threshold for stopping clustering is determined withthe development dataset.Dataset Dev Test Avg wordRent Ad 8,950 9,400 9.44Glove 8,600 9,500 10.56TV Deal - 900 15.60Table 1: Dev/Test indicates the number of listings usedfor development/testing.
Avg word indicates the averagenumber of words in a listing.
The development dataset isused to tune the parameters.threshold, then LFC addes the token to the cluster.If the score is less than the threshold but the tokenstill appears relatively frequently, then LFC createsa new cluster for the token.4 EvaluationWe perform two evaluations to assess the perfor-mance of our approach.
First, we evaluate howwell our approach extracts the correct type (e.g.,BedroomNum) and value (e.g., 2) across multipledomains.
Next, we evaluate the contribution ofeach main component in our approach ?
i.e., shal-low semantic parsing and listing-focused clustering?
through an ablation study.
We also provide anin-depth error analysis along with how our system?sperformance is affected by the corpus size.4.1 Evaluation SetupWe first assemble three different listing datasets forour evaluation ?
housing rental advertisements fromCraigslist (Rent Ad), auction listings of baseballgloves from EBay (Glove), and hot deal postings ofTV/Projector from FatWallet (TV Deal) 7.
Table 1shows the size of each dataset, and example listingsare shown in Fig.
1 8.
We also include example ex-tractions for each domain in Table.
3.We then construct a gold standard by employingtwo independent human annotators.
To do this, wefirst define the information types (i.e.
main conceptand attribute) for each dataset (and hence the tar-gets for extraction).
We use attributes from EBay7The datasets are available at https://sites.google.com/site/2soonk/8The parameters were tuned by the development set.
Oursystem is sensitive to the similarity score threshold in agglom-erative clustering but less sensitive to the other parameters.Hence, we tuned the similarity threshold for each domain whilefixing the values for the other parameters across different do-mains.1086Rent Ad housing type (.70/.93), num bedroom (.94/.99), num bathroom (.97/1), location (-.64/.82),neighborhood (.55/.79), others(14 types)Glove product type (.71/1.00), brand (.58/.98), sport (.71/1.00), size (.87/.99), series name(.51/.77), others(10 types)TV Deal product type (.98/1.00), size (.85/1.00), display technology (.93/1.00), resolution (.89/.99),seller (.73/1.00), others(14 types)Table 2: Information types for each domain.
See Fig.
1 for the examples of each type.
Due to space limitation, weshow only top five types in terms of the number of extractions made by the annotators.
The parentheses indicate inter-annotator agreement based on exact match between two annotators (first number) and partial match (second number).Exact agreement for qualitative attributes (e.g., location, neighborhood, series name) is found to be difficult.Rent Adhousing type studio, townhome, condo, townhouse, cottage,num bedroom bd, bed, br, bedroom, bdrm, bedroomsneighborhood downtown, San Francisco, china townGloveproduct type mitt, base mitt, glove, glove mittsize ?, inch, inbrand rawlings, louisville slugger, mizuno, wilsonTV Dealproduct type hdtv, wall mount, monitorsize ?, inchseller wallmart, amazon, newegg, best buyTable 3: Examples of positive extractions for the main concept attributes (e.g., housing type, product type), thequalitative attributes (e.g., num bedroom, size) and the quantitative attributes (e.g., neighborhood, brand, seller)used in the experimentas the starting point for Glove and TV Deal; andattributes from Rent.com9 for Rent Ad.
We reviewthese attributes with two independent Subject Mat-ter Experts (SMEs) to identify (and include) addi-tional, missing attributes that are useful for analyt-ics and reporting.
In total, 19 types (Rent Ad), 15types (Glove) and 19 types (TV Deal) are defined.For each dataset, we randomly select 100 listingsfrom the Test listings, and instruct each annotatorto extract values from these listings, based on theinformation types for the dataset.
Table 2 showsthe defined attributes of each data set and the inter-annotator agreement across the attributes 10.Finally, we apply our system to the Test listingsin each dataset; and evaluate the extraction results9http://www.rent.com10We use Cohen?s kappa, P (a)?P (e)1?P (e) .
P(a), the agree-ment probability, is calculated by the number of list-ings in which the two annotators agree divided by 100.P(e), the chance agreement probability, is calculated by?
(Ti,vi)P1((Ti, vi)) ?
P2((Ti, vi)) in which Pj((Ti, vi)) de-notes a probability of extracting vi of the type Ti by the anno-tator j. P ((Ti, vi)) is calculated by the frequency of (Ti, vi)extracted divided by the frequency of Ti extracted.against the gold standards using the metrics of preci-sion (P), recall (R), and F1-score (a harmonic meanof precision and recall).
Each extraction result is atuple (Ti, vi) where Ti is the information type (e.g.,BedroomNum) and vi is the value (e.g.
2).P =# correct extractions by our systemTotal # extractions by our systemR =# correct extractions by our systemTotal # extractions by an annotatorWe say that an extraction result is correct if vimatches exactly the value extracted by the annotatorand Ti matches the information type assigned to viby the annotator.
To enforce this criteria, we matchthe attribute clusters produced by our system to theinformation types.
This matching step is needed be-cause our approach is unsupervised and hence theclusters are unlabelled.
We use two methods for thismatching step ?
many-to-one mapping and one-to-one mapping.
For many-to-one mapping (as usedin (Chen et al 2011)), we match an attribute clusterto the information type whose values (as extractedby the annotator) have the highest overlap with the1087values of the cluster.
Hence, many attribute clusterscan map to the same information type.
This method,however, has one major disadvantage: high perfor-mance can be easily achieved by creating small-sized clusters ?
e.g., singleton clusters in the extremecase.
To mitigate this problem, we also use a one-to-one mapping method ?
i.e.
at most one attributecluster (i.e.
the one with the best overlap) can bemapped to one information type.We report results for both methods.
We also re-port results for partial matches using the same met-rics above.
We say that an extraction result is par-tially correct if vi partially matches the value ex-tracted by the annotator (hardwood vs. hardwoodfloors) and Ti matches the information type assignedto vi by the annotator.
114.2 Performance Across Multiple DomainsTable 4 shows the system performance result acrossthe domains.
Considering our approach is unsuper-vised, the result is encouraging.
For the baseballglove and the TV dataset, F-score is .51 and .66 (inone-to-one mapping).
For the rent ad, which is moredifficult, F-score is .39.
We hypothesize that the lowF1-score in the rent ad dataset is the result of poorextraction due to spurious tokens (e.g., our commu-nity, this weather).
To test this hypothesis, we mea-sure the performance of our system only on the ex-traction task (i.e., excluding the information type as-signment task).
Table 5 shows that the performanceon extraction for the rent ad dataset is the lowest,confirming our hypothesis.We also measure the performance per each infor-mation type.
Fig.
5 shows the result, revealing sev-eral facts.
First, the main concept clusters (hous-ing type and product type) achieve a high F1-score,showing the benefit of our semantic parser.
Sec-11We could not compare our system to other systems for sev-eral reasons.
First, to the best of our knowledge, no unsuper-vised IE system has been built specifically for short listings.Second, semi-supervised systems such as (Putthividhya andHu, 2011) (Michelson and Knoblock, 2005) require domain-specific dictionaries, which are expensive to build and scale.Third, even developing a supervised IE system is non-trivial.
Inour preliminary evaluation with the (linear chain) conditionalrandom field (170/30 training/testing listings) using basic fea-tures (the lexemes and POS of the current word and words in thetwo left/right windows), precision/recall/F1-score are .5/.33/.4.This result is no better than our system.
More training dataand/or better features seem to be required.Full Full(Par)P R F P R FRent Ad 0.3 0.41 0.35 0.43 0.55 0.48(302/219) 0.34 0.46 0.39 0.65 0.69 0.67Glove 0.54 0.48 0.51 0.72 0.58 0.64(563/631) 0.57 0.51 0.54 0.81 0.65 0.72TV Deal 0.7 0.63 0.66 0.81 0.7 0.75(765/851) 0.74 0.67 0.7 0.86 0.74 0.79Table 4: Performance based on one-to-one (first row) andmany-to-one mappings (second row) combined acrossboth annotators.
Full indicates exact match between sys-tem?s extraction and annotators?
extraction.
(Par) indi-cates partial match.
The parentheses indicate the totalnumber of extraction made by our system and the anno-tators (averaged) respectively.Full Full(Par) Ext Ext(Par)Rent Ad 0.35 0.48 0.42 0.58Glove 0.51 0.64 0.62 0.69TV Deal 0.66 0.75 0.75 0.77Table 5: F1-score when considering only extraction task(Ext and Ext(Par)).
Ext(Par) is based on partial match.ond, the quantitative attributes (e.g., num bedroom,glove size, screen size) generally have a higher F1than the qualitative attributes (e.g., location, neigh-borhood, series name).
These qualitative attributes,in fact, have a low inter-annotator agreement (e.g., -.64, .55 for location and neighborhood in Rent Adand .51 for series name in Glove), indicating thedifficulty of exactly predicting the extractions madeby the annotators.
If we consider the partial match orthe extraction only match (Ext) for those qualitativeattributes, their F1-scores are significantly higherthan the exact match in the Full task.4.3 Ablation StudyTo evaluate our semantic parser and listing-focusedclustering module, we ablate these two componentsto create four versions of our system for compari-son ?
Baseline, Baseline+LFC, Baseline+SSP andFull System.
Baseline performs a space-based tok-enization followed by clustering based only on thecontext feature.
Baseline+LFC and Baseline+SSPadd listing-focused clustering and shallow semanticparser features respectively.
The Full system usesboth features.108800.10.20.30.40.50.60.70.80.91Full Full(Par) Ext Ext(Par)Figure 5: F1-score across the attribute types shown in Table 2.
The parentheses indicate the number of extractionsmade by our system and the annotators respectively.Rent Ad Glove TV DealP R F P R F P R FFullBaseline 0.10 0.27 0.14 0.26 0.34 0.29 0.42 0.51 0.46Baseline+LFC 0.11 0.30 0.16 0.30?
0.39?
0.34 0.42 0.51 0.46Baseline+SSP 0.21?
0.37?
0.26 0.46?
0.47?
0.46 0.65?
0.59?
0.62Full System 0.30 ?
0.41 ?
0.35 0.54 ?
0.48 ?
0.51 0.70 ?
0.63 ?
0.66Full(Par)Baseline 0.15 0.40 0.22 0.37 0.50 0.42 0.59 0.69 0.63Baseline+SSP 0.15 0.43 0.22 0.44?
0.56?
0.49 0.59 0.69 0.63Baseline+LFC 0.39?
0.55?
0.46 0.37 0.34 0.35 0.79?
0.69 0.73Full System 0.43 ?
0.55 ?
0.48 0.72 ?
0.58 ?
0.64 0.81 ?
0.70 0.75Table 6: Based on one-to-one mapping.
* indicates two-tail statistically significant difference (p < 0.05) againstBaseline in Fisher?s test.
?
indicates one-tail difference.
The Fisher?s test is inapplicable to F1-scores.Rent Ad Glove TV DealNOT IN MAPPING 0.42 NOT IN MAPPING 0.27 NOT IN MAPPING 0.33WRONG EXT 0.25 WRONG EXT 0.16 WRONG EXT 0.20TK-neighborhood 0.05 WRONG TYPE 0.15 TK-display technology 0.13TK-housing type 0.05 TK-series name 0.13 TK-shipping info 0.07TK-location 0.03 TK-dexterity 0.12 WRONG TYPE 0.07Table 7: The top five errors based on the one-to-one mapping.1089The results shown in Table 6 lead to the fol-lowing observations.
First, the use of SSP (Base-line+SSP) makes an improvement for all categoriesexcept Full(Par) in the glove dataset.
This is becauseSSP identifies main concepts accurately.
Second,while LFC by itself (Bseline+LFC) is effective onlyin the glove dataset, it has the best F1-score in allthree domains when combined with SSP (Full Sys-tem).
The result also shows that the simple space-based tokenization and the context-based clustering(Baseline) is insufficient for handling short listings.4.4 Error Analysis and Corpus Size EffectWe analyze the error types for the wrong extractionmade by our system.
Specifically, for each error, weassign it to one (or more) of the following causes:(1) a cluster (and hence attribute type) was excludeddue to the 1-to-1 mapping methodology describedabove (NOT IN MAPPING); (2) the value extractedby the system was not extracted by any of the anno-tators (WRONG EXT); (3) wrong information type ?i.e., the token belonged to a wrong cluster (WRONGTYPE); (4) incorrect tokenization for an informationtype (TK-<type name>).Table 7 shows the result.
In all three domains,NOT IN MAPPING is a major source of error, indi-cating the system?s clusters are too fine-grained ascompared to the gold standard.
WRONG EXT isanother source of error (especially in the housingrental), indicating the system should extract moreinformative tokens.
Tokenization on the qualitativeattributes (neighborhood, series name, display tech-nology in Table 7) should be improved also.Finally, we measure the effect of the corpus sizeon the system performance.
Fig.
6 shows how theF1-score varies with the corpus size 12.
It shows thata small corpus size is sufficient for achieving goodperformance.
We hypothesize that, for focused do-mains such as our dataset, only a couple of hundredlistings are sufficient to acquire meaningful statis-tics.5 Conclusion and Future WorkWe presented an unsupervised IE system on shortlistings.
The key features in our system are a shal-12Due to the space limitation, we include only the rent do-main result.
However, all three datasets follow a similar pattern.00.10.20.30.40.50.60.70.80 2000 4000 6000 8000 10000FullFull(P)ExtExt(P)number of listingsF1Figure 6: F1-score of our system over varying corpus sizefor the rent domainlow semantic parser and a listing-focused clusteringmodule.
Our evaluation shows the benefits of thetwo features across multiple domains.
To improveour system further, we plan the following works.First, we plan to compare our system with super-vised systems to identify the gap between the twosystems.
Second, as in (Poon and Domingos, 2010),we plan to explore a joint learning method to com-bine the tasks of tokenization, forming the main con-cept cluster and forming the attribute clusters; thesetasks depend on the outputs of one another.
Fi-nally, we plan to explore that external knowledgeresources such as DBPedia (Auer et al 2007) andFreeBase (Bollacker et al 2008) can be used to fur-ther improve performance.6 AcknowledgementsWe would like to thank Colin Puri and Rey Vasquezfor their contribution to this work.
We also thankthe anonymous reviewers for their helpful commentsand suggestions for improving the paper.ReferencesSo?ren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, Richard Cyganiak, and Zachary Ives.
2007.Dbpedia: a nucleus for a web of open data.
In Pro-ceedings of the 6th international The semantic weband 2nd Asian conference on Asian semantic web con-ference, ISWC?07/ASWC?07, pages 722?735, Berlin,Heidelberg.
Springer-Verlag.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceed-ings of the 36th Annual Meeting of the Associationfor Computational Linguistics and 17th InternationalConference on Computational Linguistics - Volume 1,1090ACL ?98, pages 86?90, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Richard Beaufort, Sophie Roekhaut, Louise-Ame?lieCougnon, and Ce?drick Fairon.
2010.
A hybridrule/model-based finite-state framework for normaliz-ing sms messages.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, ACL ?10, pages 770?779, Stroudsburg, PA, USA.Association for Computational Linguistics.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuring hu-man knowledge.
In Proceedings of the 2008 ACMSIGMOD international conference on Management ofdata, SIGMOD ?08, pages 1247?1250, New York, NY,USA.
ACM.Nathanael Chambers and Dan Jurafsky.
2011.
Template-based information extraction without the templates.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies - Volume 1, HLT ?11, pages 976?986, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Ming-Wei Chang, Lev Ratinov, and Dan Roth.2007.
Guiding semi-supervision with constraint-driven learning.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Linguis-tics, pages 280?287, Prague, Czech Republic, June.Association for Computational Linguistics.Harr Chen, Edward Benson, Tahira Naseem, and ReginaBarzilay.
2011.
In-domain relation discovery withmeta-constraints via posterior regularization.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies - Volume 1, HLT ?11, pages 530?540,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Rayid Ghani, Katharina Probst, Yan Liu, Marko Krema,and Andrew Fano.
2006.
Text mining for product at-tribute extraction.
SIGKDD Explor.
Newsl., 8(1):41?48, June.Trond Grenager, Dan Klein, and Christopher D. Man-ning.
2005.
Unsupervised learning of field segmen-tation models for information extraction.
In Proceed-ings of the 43rd Annual Meeting on Association forComputational Linguistics, ACL ?05, pages 371?378,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Daniel Gruhl, Meena Nagarajan, Jan Pieper, ChristineRobson, and Amit Sheth.
2009.
Context and domainknowledge enhanced entity spotting in informal text.In Proceedings of the 8th International Semantic WebConference, ISWC ?09, pages 260?276, Berlin, Hei-delberg.
Springer-Verlag.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofthe main conference on Human Language TechnologyConference of the North American Chapter of the As-sociation of Computational Linguistics, HLT-NAACL?06, pages 320?327, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Dekang Lin and Patrick Pantel.
2001.
Dirt@sbt@discovery of inference rules from text.
In Pro-ceedings of the seventh ACM SIGKDD internationalconference on Knowledge discovery and data min-ing, KDD ?01, pages 323?328, New York, NY, USA.ACM.Xiaohua Liu, Shaodian Zhang, Furu Wei, and MingZhou.
2011.
Recognizing named entities in tweets.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies - Volume 1, HLT ?11, pages 359?367, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Matthew Michelson and Craig A. Knoblock.
2005.
Se-mantic annotation of unstructured and ungrammaticaltext.
In Proceedings of the 19th international jointconference on Artificial intelligence, IJCAI?05, pages1091?1098, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Matthew Michelson and Craig A. Knoblock.
2009.
Ex-ploiting background knowledge to build reference setsfor information extraction.
In Proceedings of the21st international jont conference on Artifical intel-ligence, IJCAI?09, pages 2076?2082, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Hoifung Poon and Pedro Domingos.
2007.
Joint in-ference in information extraction.
In Proceedings ofthe 22nd national conference on Artificial intelligence- Volume 1, AAAI?07, pages 913?918.
AAAI Press.Hoifung Poon and Pedro Domingos.
2010.
Unsuper-vised ontology induction from text.
In Proceedingsof the 48th Annual Meeting of the Association forComputational Linguistics, ACL ?10, pages 296?305,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Duangmanee (Pew) Putthividhya and Junling Hu.
2011.Bootstrapped named entity recognition for product at-tribute extraction.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP ?11, pages 1557?1567, Stroudsburg, PA,USA.
Association for Computational Linguistics.Joseph Reisinger and Marius Pas?ca.
2011.
Fine-grainedclass label markup of search queries.
In Proceed-ings of the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies - Volume 1, HLT ?11, pages 1200?1209,1091Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Sunita Sarawagi and William W. Cohen.
2004.
Semi-markov conditional random fields for information ex-traction.
In In Advances in Neural Information Pro-cessing Systems 17, pages 1185?1192.Satoshi Sekine.
2006.
On-demand information ex-traction.
In Proceedings of the COLING/ACL onMain conference poster sessions, COLING-ACL ?06,pages 731?738, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005.
Bidi-rectional inference with the easiest-first strategy fortagging sequence data.
In Proceedings of the confer-ence on Human Language Technology and EmpiricalMethods in Natural Language Processing, HLT ?05,pages 467?474, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: vector space models of semantics.J.
Artif.
Int.
Res., 37(1):141?188, January.William E. Winkler.
1990.
String comparator metricsand enhanced decision rules in the fellegi-sunter modelof record linkage.
In Proceedings of the Section onSurvey Research, pages 354?359.1092
