Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 48?57, Dublin, Ireland, August 23-29 2014.Capturing Cultural Differences in Expressions of IntentionsMarc T. Tomlinson David B. BracewellLanguage ComputerRichardson TX 75080marc, david, wayne@languagecomputer.comWayne KrugAbstractThe intersection of psychology and computational linguistics is capable of providing novel au-tomated insight into the language of everyday cognition through analysis of micro-blogs.
WhileTwitter is often seen as banal or focused only on the who, what, when or where tweets can ac-tually serve as a source for learning about the language people use to express complex cogntivestates and their cultural identity.
In this contribution we introduce a novel model which cap-tures latent cultural dimensions through an individual?s expressions of intentionality.
We thenshow how these latent cultures can be used to create a culturally-sensitive model which providesenahnced detection of signals of intentionality in tweets.
Finally, we demonstrate how thesemodels reveal interesting cross-cultural differences in the goals and motivations of individualsfrom different cultures.1 IntroductionSocial media platforms have enabled new forms of discourse and have also provided enormous quantities of dataon these communications.
For instance, the popular microblogging service Twitter provides an exceptionally use-ful source of user-generated content which has attracted considerable interest from researchers in computationallinguistics (Ritter et al., 2009; Gimpel et al., 2011).
Most of the language processing on tweets has involvedthe identification of sentiment (Davidov et al., 2010), summarization (Sharifi et al., 2010), conversational mod-els of Dialogue acts (Ritter et al., 2009), or lexical and semantic processing.
In this effort we expand on theseprevious approaches and show how individuals express their cultural identity through expressions revealing theirintentionality towards events and provide a way of capturing this information.We define intentionality as the amount of effort an individual is willing to expend to achieve a goal(Ajzen, 1991).Goals represent future states or events which an individual wishes to happen.
Accordingly, intentions are goalsfor which an individuals is willing to expend at least some minimal amount of effort to bring about.
While peopleexpress goals throughout the day, intentions are the goals that they are willing to follow through with.
Identifyingwhen a goal is actually an intention requires the successful recognition of many distinct cognitive factors that canbe revealed through the individual?s use of language.There is a long history of studies that have worked towards identifying a set of factors that underly an individual?sintentions (Ajzen and Fishbein, 1977; Ajzen, 1991; Malle and Knobe, 1997; Sloman et al., 2012) of which, thesetting of goals is one important factor.
These studies have concentrated on identifying the factors that affect anindividual?s motivation.
The studies have also identified a set of factors that people use to gauge the intentionalityof other individuals.
However, these factors have always been manually identified by an expert from an individual?sspeech or writing.
It is not clear that these features can actually be detected automatically in language.Intentions have also been considered in computational linguistics.
In their seminal work entitled, ?Attention,Intentions, and the Structure of Discourse?
(Grosz and Sidner, 1986), Grosz and Sidner point out the fundamentalrole of intentions and their effect on the theory and processing of discourse structure.
They even define a set of in-tentions that can be held by individuals that are relevant to discourse theory.
In contrast, we focus on understandingintentions outside of the discourse.
In addition, we work with a more general definition of intentions taken frompsychology, defining intentionality as the amount of effort an individual is willing to expend to achieve a goal.Culture refers to the set of beliefs, norms, and customs shared by a group of people.
Beliefs and culture areinseparably tied to intentions and language (Ajzen and Fishbein, 1977; Tomasello et al., 2005).
Culture affects anauthor?s proclivity to have a particular intention, for example Hofstede?s dimension of power distance (Hofstede,1980) would suggest that individuals from high power-distance cultures have a lower likelihood of performingThis work is licenced under a Creative Commons Attribution 4.0 International license.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/48actions with the intention of overriding the actions of an individual of higher status.
Culture can also affect theway in which individuals reason about other agents?
intentions and the set of actions that are used to realize anindividual?s intentions.
While considerable work has looked at the link between cultures and intentions, here weshow how a latent representation of an individual?s culture derived from their intentions can be utilized to explorethe intersection between culture and intentions using the vast amount of written expressions present on Twitter.In this contribution, instead of focusing on the discourse meaning of intentions, we look at how personal in-tentions can be understood through Twitter posts by focusing on the language of those posts contain.
We brieflydiscuss previous work showing how it is possible to capture language that reveal cognitive factors of intentionalitywhich could be used to capture broader intentions.
Critically, we then augment the models of the cognitive factorsof intentionality by accounting for the culture of the authors on Twitter.
Twitter contains an immense number ofauthors covering a variety of different cultures definable at different levels, for example women, college-students,or fitness buffs.We have evaluated the models on a very large set of over 7.5 million tweets which cover a sampling of Twitterfrom early 2011 to the middle of 2013.
Our sample includes just over 900,000 authors.
We found very promisingresults for identifying the factors of intentionality, but by considering culture we were able to provide a significantimprovement of those results.
We have shown that cognitive factors of intentionality, including goals, control andskill, and rewards can be recognized through the use of simple language models.
Similarly, our cultural modelswere based on traditional techniques for latent variable modeling through principal component analysis enablingan understanding of the cultural distribution of intentions.The remainder of the paper is organized as follows.
We first present the cognitive factors of intentionality thatwe have used for this contribution.
We then present a new cultural model of authors on Twitter and compare it toexisting approaches in the literature.
We then present a series of models which capture the cultural variation of thecognitive factors of intentionality.
Finally, we present a look at some of the cultural differences identified throughour approach.2 Factors of IntentionalityWhile there are numerous factors that affect an individual?s intentionality (Ajzen, 1991; Malle and Knobe, 1997;Sloman et al., 2012), in this contribution we focus on investigating the most historically central factors: goals,perceptions of control, and rewards.
Below we provide brief examples of the three factors before detailing ourapproach for identification of latent cultures.2.1 Factor 1: GoalsThe first factor that we consider is evidence that an individual has a goal.
Goals are expressions of a desire fora change of state or rewards which could require an action on the part of the individual.
The setting of goals forboth action and inaction have been linked to many different motivational and long-term outcomes (Albarracin etal., 2011; Locke, 1968).
Examples of goals are(1) I want to finish my paper(2) I want to be famousThe first example of a goal expresses an intention to perform an action which could result in a positive rewardfor the individual, however it doees not mention the reward.
In contrast, the second example expresses a clearexpectation for a reward (fame), but does not describe the actions that will lead to that reward.
Does the individualwant to be President or the next Kardashian?
Additionally, in contrast to explicit goals stated by an individual,goals can also be inferred by other people based on an analysis of actions (perceived intended events) carried outby the individual.
For example, it is presumed that an individual has a goal to win the lottery when they buy alottery ticket, or that the occupants of a car full of beach toys is headed to or from a beach.
Goals represent thefactor that has seen the most recent attention in terms of the creation of automatic methods for their recognition(Chen et al., 2013; Banerjee et al., 2012).2.2 Factor 2: Perception of ControlIntentions are revealed not just through goals, but also through words expressing skill or a level of control.
Individ-uals that feel that they have more control over a situation will expend more effort on their actions (Ajzen, 1991).Individuals are also perceived by others as having greater intentionality for actions that they have control over orexhibit skill at.
We considered multiple ways in which an individual can express their perceived control over anevent, subdividing this factor into three sub-factors.
The first sub-factor captures expressions which indicate skill.
(3) Just helped some guy push his gas-less car to the garage #iamwoman #hearmeroar49Table 1: Example Hash Tags and Tweets.Cognitive Factor Sample Tags Sample TweetsF1: Goal #goalinlife, #mywish ?3 more days of studying?F2: Control #dowhatisay, #kissmyfeet ?I defy the law of gravity?F2: Skill #madskillz, #iamapro ?you are flat out amazing to watch?F2: Lack of Control #oops, #cantstop ?cannot believe I said that?F3: Negative Reward Self #fml, #crap ?I just locked the keys in my car?F3: Negative Reward Other #worstdriverever, #awkward ?It does make me cringe?F3: Positive Reward Self #whyismile, #victoryismine ?my cats make me smile?F3: Positive Reward Other #ff, #thatsbadass ?Solar panels on the white house?The second sub-factor captures expressions of control.
(4) I?m in control here!The third sub-factor captures expressions of lack-of-control.
(5) i?m a little nervous for tomorrowWhile several linguistic theories exist that could be utilized to create systems detecting control, such as agency(Dowty, 1991), there is no prominent work on automatically identifying control directly in an individual?s expres-sions.2.3 Factor 3: Reception of RewardsIntentions can also be inferred when an individual receives a reward.
(6) I?m so proud of what I did(7) Your work sucks!Rewards can be positive (increasing the likelihood of the action being repeated, Example 6) or negative (decreasingthe likelihood of the action in the future, Example 7).
In addition, rewards can come from the individual (self-directed rewards, Example 6) or from other individuals (other-directed rewards, Example 7).
This establishesfour sub-factors for rewards.
Knowing that an individual received a reward increases the likelihood that theyhad effortful participation in the event.
In addition, evidence of negative rewards are strongly inferential forintentionality (Knobe, 2003).
Interpretations of rewards are very culturally sensitive.
For example, a commentsuch as ?That is disgusting?
would have a good chance of being interpreted as a positive reward when it was madeas a comment to a user-generated contribution on the website DeviantArt.com.
Additionally, the effect of rewardson motivation is not always clear-cut.
Experts seek out and are actually motivated by criticism (Finkelstein andFishbach, 2012).2.4 Linking Hashtags and Factors of IntentionalityThe factors and sub-factors described above capture expressions which can be used to infer an individual?s inten-tionality towards a future action.
In Tomlinson et al.
(2014) we showed that it is possible to link particular hashtagsused by people on Twitter to these cognitive factors.
Our approach utilized two annotators.
The first annotator,through trial and error, identified a large number of potential candidate tags for each sub-factor.
The annotator thenrated each hashtag for how well tweets containing that hashtag exhibited each sub-factor (on a scale of 1-5).
Thesecond annotator then separately rated each tag which scored a 4 or 5.
The two annotators had an agreement rateof 87%.
178 tags in all were agreed to be a 4 or 5 by both annotators and considered representative of the particularsub-factor.
Examples of the hashtags utilized and tweets with those tags are shown in Table 1.
The tweets havebeen modified slightly to preserve anonymity.3 Identification of Latent CulturesIn the preceding section we discussed examples of goals, control, and rewards, and discussed how hashtags areused on Twitter to mark a tweet expressing one of these factors.
Some of these examples require cultural knowledgein order to correctly interpret.
In this section we present the latent model of culture that is used for learning thecultural specific expressions of the factors of intentionality.503.1 SVD-Model of CultureA considerable amount of work has demonstrated how particular social characteristics of individuals can be iden-tified on Twitter, such as gender, age, and political orientation (Zamal et al., 2012; Pennacchiotti and Popescu,2011).
While superb results can be obtained for identifying these characteristics of authors using a complex set offeatures, this approach does not neccesarily allow for generalization to other data sets.
Therefore we settled on anapproach utilizing a specially trained latent variable model.
Instead of utilizing Latent-Dirichlet Allocation (LDA,Blei, Ng, and Jordan, 2003 ) as Pennacchiotti and Popescu we utilized a spectral analysis based on singular-valuedecomposition (SVD).
This approach has been shown to be generally superior to LDA on the domain of topicmodeling (Chen et al., 2011), but has not been tested for cultural modeling.3.1.1 DataWe randomly sampled 1.6 million tweets from a Twitter dataset that had been generated by retrieving tweets thatcarried at least one of the hashtags linked to a cognitive factor of intentionality (and other posts by that author).
Inaddition, we restricted the set to authors for which we had at least 20 posts in our dataset.
For this dataset, all ofthe markup was left in the tweet (e.g.
hashtags, urls, etc.
).3.1.2 ModelFrom our dataset we created a set of documents, D = {a1, a2, .
.
.
, aA}.
Where each airepresents the entire col-lection of tweets for a single author that contain mentions of goals, skill/control, or rewards.
This set of documentscontains N words and hashtags.
We then create a matrix, X ?
<N,A, where each author represents a row in thematrix and the columns are the number of times that the corresponding word or hashtag was used by that author.Then we perform a singular value decomposition of the matrix to solveX = V SCT(1)Where S is a k x k matrix whose off-diagonal entries equal 0 and the on-diagonal entries are the k singular valuesfor the matrix X .
For our approach we set k equal to 100.
V represents a mapping of the words into our reducedspace <n,k, and C <i,kcontains a weighting for each author with respect to the kthlatent cultural dimension.
Thecultural model can be used to identify the culture of an unseen author through the creation of a projection matrix,P .P = V S?1(2)This matrix projects the tweets that make up the author into our latent cultural space C. This allows us to map eachauthor in our complete data set into our latent space which can then be used for training and testing.
The latentcultural space can be used to characterize the culture of an author as a distribution over the dimensions.
Below weevaluate our latent cultures on the shared dataset provided by Zamal et al.
2012.3.2 Evaluating the Latent CulturesCulture is a system of shared beliefs and actions.
Culture is often shared between individuals based on socialsimilarity, this can be within a language, nation, gender, age-group or other social distinction.
Thus, being able toidentify an individual?s culture should facilitate detection of socio-demographic information.
To test this we lookedat using the latent cultural dimensions to predict socio-demographics on Twitter.
We looked at the systems abilityto identify gender (male vs. female), age (young vs. old), and political orientation (Democrat vs. Republican) ofindividuals based on their exhibition of particular latent cultural dimensions.
In this model we first representedan individual?s tweets as a distribution over the latent dimensions.
We then utilized two different statistical ap-proaches to find associations between particular dimensions and the relevant socio-demographic information.
Fora comparison, we tested our SVD-culture model against a similarly trained LDA model and a model based onn-grams.3.2.1 DataWe utilized the publicly available dataset from Zamal, Liu, and Ruths (2012).
The dataset consisted of Twitteruser names and associated meta-data identifying their gender (Male or Female), age (two classes, young and old),and political orientation (Liberal or Conservative).
Unfortunately, many of the identified tweets were no longeravailable from the Twitter API, but we successfully retrieved 2.6 million tweets from authors identified in thedataset with 310 users identified for gender, 320 identified for their age, and 380 for their political affiliation.
Thetweets in our dataset are substantially different from the original dataset because of the time over which they werecollected.
Zamal, et al.
?s tweets were from 2012 and before, whereas our tweets covered much of 2013.
Thissuggests that comparisons of the raw numbers should be made with caution, particularly in the political area.51Table 2: Results for identifying user demographics based on latent cultural dimensions compared to linguistic styleand an ensemble method utilized by Zamal et al (2012).Zamal et al.
N-Grams LDA SVDN F F F FGender 310 .80 .57 .71 .70Age 320 .75 .63 .66 .67Political Orientation 380 .89 .73 .66 .683.2.2 Modeling & ResultsTo provide a comprehensive view of the strengths and weakness of our approach we compared several models fortheir ability to correctly predict the cultural demographics of individuals on Twitter.
We first established a base-linemodel which was an n-gram language model created from the language used by each individual in their tweets.This model learned to identify the cultural demographics based on the frequency with which individual?s in thatdemographic used sequences of words, called n-grams.
This approach is consistently ranked as one of the singlebest approaches to authorship identification and performs well on a large variety of datasets.We also tested the SVD-Culture model introduced above on this dataset.
For this experiment, we trained alogistic-regression based classifier to identify the demographic information of an author based on the vector createdby projecting that author into our latent space.Finally, to look for a difference in the performance between an SVD-based latent representation and one basedon LDA (Pennacchiotti and Popescu, 2011), we also trained and tested an LDA-based Culture model.
The modelwas trained on the same data as the SVD-based model and utilized the same number of dimensions.All of the models were tested and trained utilizing 10-fold cross validation.
It is very important to point outthat the data sets used to generate the underlying latent representational models did not include any of the tweetsfrom the data used for the 10-fold cross validation.
That data was only utilized for the supervision of the logisticregression.The results of the base model, the SVD model, the LDA model, and the original results presented by Zamalet al.
(2012) are shown in Table 2.
The latent models are clearly superior to the language model, on averageoutperforming it by a significant margin of 4%.
As expected, the SVD-based model does outperform the LDAmodel on average, though it is only by 1%, on average.The strength of this approach is in its simplicity.
The latent cultural dimensions have been learned on a whollydifferent dataset than that used for testing, this supports good generalization performance.
While the latent SVD-cultural model does not reach the performance of the system created by Zamal et al.
(2012).
Zamal et al.
?s resultswere obtained using a plethora of different feature types, which were specifically trained to solve each individualproblem.
As pointed out in Cohen and Ruths (2013) this causes some issues on transfer to a novel dataset, becausethe selected features were not representative of differences between liberals and conservatives in the second dataset.In contrast, we suggest that the latent cultural model learns a more general representation utilizing only the set offeatures provided by the underlying latent cultural models, which were not trained on any of the data in the testset.
Additionally, the latent SVD model is easy to implement and train.Importantly, these results indicate that the latent cultural dimensions capture similarities in the ways in whichindividuals of similar socio-demographics express themselves on Twitter.
The model is able to easily identify thegender, age, and political affiliation of individuals based on their tweets.
In the next section we show how we canutilize these latent cultural dimensions to facilitate learning of expressions conveying factors of intentionality.4 Cultural Sensitive Identification of Cognitive Factors of Intentionality in LanguageRecognizing language that expresses factors of intentionality is complicated because of the wide variety of waysin which they can be expressed as shown in the examples in the previous section.
While some work has exploredautomatic goal recognition, most recently by (Chen et al., 2013) and (Banerjee et al., 2012), little work has beendone automatically characterizing the other factors, though work in detecting social implicatures in language issimilar (Bracewell et al., 2012b).
We first present a general framework for learning to model the content of tweetsthat express a given factor from our cognitive model, we then show how this approach can be enhanced with theaddition of latent cultural dimensions.4.1 Culture Agnostic ModelHere we introduce the General Model that serves as the basis for the culture specific models.
It is so namedbecause it applies to all cultures.
We utilized an n-gram based language model to identify the factors in tweets.52We first constructed a vocabulary of all n-grams between 2 and 4 words in length.
Each tweet, j, which is labeledwith a hashtag linked to a sub-factor f , is represented as a vector, Xj.
Entries in Xjcorrespond to the numberof occurrences in the tweet of the ithn-gram from the vocabulary.
We examined two different mathematicalapproaches to modeling the cognitive factors to gain a better understanding of the problem.The first approach utilized a Naive-Bayes based classifier (NB) wherep(F = f |X) =p(F = f) ?
p(X|F = f)p(X)(3)The second approach utilized an L2-loss logistic regression model (L2):p(F = f |X,W ) =11 + exp(w0+?iwiXi)(4)In which the weights, W , are learned by maximizing Equation (4)mf?jlog p(yj|Xj;W )?
?||W ||22(5)where mfrepresents a balanced training set created by randomly sampling the training tweets that are tied tosub-factor f and an equal number of tweets that express one of the other factors.
For solving the maximizationproblem we utilized the LibLinear package (Fan et al., 2008).4.2 Culture Sensitive ModelsWe compared two different methods for integrating the culture information from the SVD-based culture modelinto the models for identifying the cognitive factors of intentionality.
Both models assume that the authors havebeen partitioned into a set of cultures, L, but differ in their modeling of the link between language and cognitivefactors.In order to identify the cultures of the authors we utilize a clustering of the latent dimensions produced by theSVD model, a spectral clustering (Kannan et al., 2004).
We utilized a simple hierarchical clustering that capitalizeson the y largest singular values.
We create a set of hierarchical clusters based on a median split of each of the firsty columns in our latent space.
When y = 1 we have two clusters where the authors have been split based on themedian value of the first latent dimension, with y = 2 each cluster is then independently split by the author?s valuealong the second latent dimension, giving four clusters, and so on.4.2.1 Culture-Specific ModelOur first method, which we call the culture specific model uses a separate model of each factor for each latentculture, l ?
L. We first identify a tweet x, as belonging to a given culture, l. We then determine whether or not thelanguage it contains expresses a particular cognitive factor based onp(F = f |xl, Ll) (6)To learn the function we utilize a linear classifier, Logistic-Regression with an L2 regularization term, and limitthe training data to authors that belong to the particular culture.4.2.2 Joint-Culture ModelOur second model, which we call the joint culture model utilizes an ensemble based approach.
For each tweet, xl,we calculate both a culture specific view of the language in the tween p(F = f |xl, Ll) and a culture agnostic viewp(F = f |xl), taking the classification is that is most confident.
This joint approach utilizes the culture-agnosticmodel to smooth deficiencies caused by insufficient culture-specific data.4.2.3 Number of CulturesWe explored settings of y = {2, 3, 4, 5} latent dimensions which equates to {2, 4, 8, 16, 32} latent cultures.
Au-thors are first split according into their cultural group and then tweets from each culture are broken into a trainingand testing set.
Because of the amount of data we utilized only a 5-fold cross validation procedure.
In addition,we also tested a random culture model that randomly assigned authors to cultures instead of utilizing the spectralclustering.
When creating these random cultures we balanced the number of authors in each random culture withthe corresponding spectral cultures.53Table 3: Accuracies for modeling each sub-factor of intentionality.
L2 represents results obtained using an L2-regularized linear regression, NB represents naive-Bayes, #Cultures signifies the number of latent cultural dimen-sions used for clustering.General L2 - Culture Specific L2 -Joint Culture#Cultures NB-0 L2-0 2 3 4 5 2 3 4 5 NB-5F1:Goals 79.8 80.9 81.1 80.9 79.8 78.8 82.1 82.2 82.1 82.0 79.1F2:Control 70.1 75.5 75.5 75.3 74.4 73.8 76.4 76.4 76.4 76.4 72.3F2:Lack of Control 69.1 73.7 75.2 74.9 74.1 72.9 75.9 75.7 75.8 75.6 71.6F2:Skill 73.2 76.2 77.6 77.0 76.4 75.5 78.2 78.2 78.1 78.1 75.3F3:Positive Other 78.3 82.9 84.3 84.1 83.7 83.4 84.5 84.4 84.4 84.5 81.9F3:Positive Self 66.0 69.1 70.6 70.3 69.4 68.7 71.3 71.3 71.3 71.4 68.4F3:Negative Other 68.7 72.3 73.6 73.4 72.5 71.6 74.1 74.0 74.1 74.0 70.8F3:Negative Self 69.3 72.4 73.6 73.3 71.9 71.3 74.4 74.3 73.9 73.7 71.14.3 DataTesting was done on a large number of tweets (7.5 million) that contained tweets from individuals that used anyof the representative hashtags.
In our collection hashtags exhibiting the sub-factor of control contained the largestnumber with approximately 575,000 tweets, while we only collected 110,000 tweets which were marked with ahashtags indicating positive rewards for the actions of other individuals.
For training and testing purposes weremoved all URLs, hashtags, and @users from the tweets.
We then discarded tweets that were less than two wordslong.
This approach is conservative, because we removed the classifier?s ability to directly learn co-occurringhashtags, however we wanted to ensure that we would minimize deficient solutions and maximize the ability ofthe models to transfer from Twitter to other genres of text.4.4 Results and DiscussionThe accuracy of the classifiers for identifying each sub-factor are shown in Table 3.
The accuracies reflect theclassifiers ability to separate tweets that have a hashtag representing the given sub-factor from those that do not.The results suggest that all of the models are adequately capturing the differences between the cognitive factors.On average, the logistic regression based classifier achieves a 3.5 percent advantage in accuracy over the Naive-Bayes model, showing a clear advantage for the improved feature selection of the L2-loss logistic regression.
Bothmodels required a similar amount of time to train and test.To conserve space Table 3 shows only the results for the 5-dimension Joint Culture Naive-Bayes model.
Theresults for the Naive-Bayes model match the pattern exhibited by the logistic-regression Joint Culture model,except that the Naive-Bayes Joint-Culture model increases steadily as more groups are added with a maximumperformance with 5 latent dimensions.
With 5 latent dimension the gap between the two ML approaches shrinksto 2.8 percent (73.2 to 76.0).On average the Joint Culture model shows a 1.8 percent improvement (74.3 to 76.1) over the culture neutralmodel for the L2-Logistic Regression, while it is a larger 2.2 percent for the Naive Bayes based approach (71.2 to73.4).
A comparison of the error reduction shows that the cultural integration is very promising.
While the L2-losslogistic regression provides an 11 percent error reduction over the Naive-Bayes, the joint culture model achieves acomparable 7-9 percent reduction in error over the L2-loss regression and the Naive-Bayes model.The improvements are strongest for positive self directed reward factor, skill factor, and lack of control factor.Interestingly, the models also exhibited considerable variation in accuracies across the different cultures, for exam-ple utilizing 3 dimensions positive rewards for others in one culture is recognized at 92 percent (this group contains43,556 tweets), while for another culture of approximately the same size it is only recognized at 77 percent.
Un-fortunately, when moving to 4 dimension our clustering algorithm splits the group at 92 percent into two groupswhere the factor can only be recognized with an average of 88 percent accuracy.
This suggests that more complexclusterings strategies within the latent space would be beneficial.While not shown in the table for space reasons, we also tested the joint culture model utilizing a random assign-ment of authors to cultures, instead of relying on the assignment produced by the SVD-model.
As expected therandom model performed, on average, at approximately the same level as the general model, 74.6% compared to74.5% respectively.
Though the random culture model exhibited considerable variation in relation to the real jointculture model across the different factors.
This evidence reinforces the idea that the latent cultures are coherentand that individuals within those cultures express the factors of intentionality in similar ways.54Table 4: Example cultures and the tags that are commonly associated with that factor.Cultural Label Cognitive Factor Common TagsAlterantive Medicine Health Positive Rewards Self #almond, #radish, #curdGeek Interest Positive Rewards Self #theobroma, #freefiction, #nanotechTeenagers Positive Rewards Self #bored, #me, #cuteUrban Hip/Hop Positive Rewards Self #bosslife, #teamfastfollow, #indiechartsMartial Fitness Goals #healthynews, #fitnessimages, #fitsoHip/Hop Goals #soundcloud, #support, #dlGeneral Religion Goals #singer, #jesus, #judasInspections of tweets where the cognitive factors have been discovered suggest that many times the hashtagsare used sarcastically.
Anecdotally, we also examined a list of the top hashtags associated with instances labeledby our approach and found good generalization to novel hashtags.
We looked at a list of the hashtags based onthe average confidence of the labels being applied to the tweets containing those tags, we found many reasonablecandidate tags.
For example, tweets containing the hashtags #day1 and #day2 were among the most likely to belabeled as exhibiting a goal even though neither were identified by our annotators initially.
These two tags are usedby individuals on the first and second day of pursuing a goal.The results presented in this section suggest that breaking down the authors by culture before learning modelslinking the hashtags marking expressions of the cognitive factors of intentionality to language provides a significantbenefit.
It also hints at some interesting differences between the groups.
In the next section we briefly explore someof those differences.5 Investigating Cultural Differences in the Language of IntentionalityWe investigated the cultural discriminations made by the model by looking at the hashtags that were the mostpopular for each culture.
Two annotators provided labels for each of the cultures based on the most frequenthashtags for that culture.
We found that some of the cultures could easily be labeled based on their differentialuse of topical hashtags.
Many of the latent cultures reflected notions of distinctions between cultural (or sub-cultural) groups, such as along political orientation or socio-demographics (urban, hipster, university students,single mothers, and political activist).
In addition to the latent cultures that weighed on group identity, some of theother clusters captured more topical information, such as being fitness oriented or discussions focused around sex.The cultural distinctions allowed us to quantify the differences in the event and intentionality associations acrossthe cultures and differences in expressions indicating cognitive factors of intentionality.
For instance, activists andurban individuals were most likely to produce tweets expressing control over situations.
There were also groups,such as the camaraderie group where individuals typically set goals that will benefit a group in some way as well asthe individual.
In most of these cases, the author is the member of a team or some other group that will be engagingin a cooperative or competitive activity.
Some authors from this cultural group express goals of providing director moral support to specific teams or groups of which they are not members.
Others have goals of attending groupevents or gatherings with no particular membership.
In most cases, goals in this culture are associated with positiverewards or defeating an opponent.Table 4 shows the most probable tags by cognitive factor for some of the more interesting groups.
These listswere generated by first eliminating all tags from the culture that were not predictive of the culture.
To do this, wegenerated an estimate of the mean and variance for each hashtag in our dataset across all of the different cultures.We then eliminated all tags where the probability of the tag given the culture was not significantly different thanits estimate given the general population.
This has the effect of removing that hashtags that signaled the cognitivefactors because they had a fairly general distribution across the cultures.6 ConclusionIn this paper we presented a novel approach for identifying factors of intentionality in tweets.
Further, we showedhow a latent cultural model could be used to enhance those identifications through an improved understanding ofhow these factors are expressed across the various cultures.
The latent cultural dimensions identified by the modelcorrespond well with real cultural demographic information.This work presents several exciting possibilities, while Twitter is notoriously difficult for traditional naturallanguage processing work because it doesn?t follow established syntactic and semantic conventions, models learnedover Twitter data are able to transfer to other types of social media, such as user-generated content sites (Tomlinsonet al., 2014a).
Hashtags provide a very interesting form of distant annotation that could reduce the amount of timeand effort required to create models which capture a nuanced understanding of social or psychological pragmatics,55such as social acts (Bender et al., 2011; Bracewell et al., 2012a), thus making the exploration of a richer languageunderstanding more tractable.Lastly, we have also shown that the models provide an ability to look at differences between cultures in the howand when of their expressions of factors relating to intentionality.
People express lots of goals, but what affectswhen they actually intent them.
These models should be able to provide a novel view on the pulse of a city (Riosand Lin, 2013) or citizens?
cognitive responses to events (Dodds et al., 2011).
We can use these techniques toidentify what events make people establish new goals or instill feelings of a loss of control?AcknowledgmentThis work was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department ofDefense US Army Research Laboratory contract number W911NF-12-C-0063.
The U.S. Government is authorizedto reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted asnecessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoD/ARL,or the U.S. Government.ReferencesIcek Ajzen and Martin Fishbein.
1977.
Attitude-behavior relations: A theoretical analysis and review of empiricalresearch.
Psychological Bulletin, 84(5):888?918.Icek Ajzen.
1991.
The Theory of Planned Behavior.
Organizational Behavior and Human Decision Processes,50:179?211.D.
Albarracin, J. Hepler, and M. Tannenbaum.
2011.
General Action and Inaction Goals: Their Behavioral,Cognitive, and Affective Origins and Influences.
Current Directions in Psychological Science, 20(2):119?123,April.Nilanjan Banerjee, Dipanjan Chakraborty, Anupam Joshi, Sumit Mittal, Angshu Rai, and B. Ravindran.
2012.Towards Analyzing Micro-Blogs for Detection and Classification of Real-Time Intentions.
ICWSM.E.M.
Bender, J.T.
Morgan, Meghan Oxley, Mark Zachry, Brian Hutchinson, Alex Marin, Bin Zhang, and MariOstendorf.
2011.
Annotating social acts: Authority claims and alignment moves in wikipedia talk pages.
ACLHLT 2011, (June):48.David M Blei, Andrew Y Ng, and Michael I Jordan.
2003.
Latent Dirichlet Allocation.
Journal of MachineLearning Research, 3:993?1022.David B Bracewell, Marc T Tomlinson, Mary Brunson, Jesse Plymale, Jiajun Bracewell, and Daniel Boerger.2012a.
Annotation of Adversarial and Collegial Social Actions in Discourse.
In 6th Linguistic AnnotationWorkshop, number July, pages 184?192.David B Bracewell, Marc T. Tomlinson, and Hui Wang.
2012b.
Identification of Social Acts in Dialogue.
InCOLING, number December 2012, pages 375?390.Xi Chen, Bing Bai, Qihang Lin, and Jaime G Carbonell.
2011.
Sparse Latent Semantic Analysis.
In SDM.Zhiyuan Chen, Bing Liu, Meichun Hsu, Malu Castellanos, and Riddhiman Ghosh.
2013.
Identifying IntentionPosts in Discussion Forums.
In NAACL-HLT, number June, pages 1041?1050.Raviv Cohen and Derek Ruths.
2013.
Classifying Political Orientation on Twitter : It s Not Easy !
In ICWSM-2013, pages 91?99.Dmitry Davidov, Oren Tsur, and Ari Rappaport.
2010.
Enhanced Sentiment Learning Using Twitter Hashtags andSmileys.
In Coling, number August, pages 241?249.PS Dodds, KD Harris, and IM Kloumann.
2011.
Temporal patterns of happiness and information in a global socialnetwork: Hedonometrics and Twitter.
PloS one, 6.David Dowty.
1991.
Thematic Proto-Roles and Argument Selection.
Linguistic Society of America, 67(3):547?619.RE Fan, KW Chang, CJ Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR: A library for large linearclassification.
Journal of Machine Learning Research, 9(2008):1871?1874.56Stacey R. Finkelstein and Ayelet Fishbach.
2012.
Tell Me What I Did Wrong: Experts Seek and Respond toNegative Feedback.
Journal of Consumer Research, 39(1):22?38, June.Kevin Gimpel, Nathan Schneider, Brendan O Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, and Noah A Smith.
2011.
Part-of-Speech Tagging for Twitter :Annotation , Features , and Experiments.
In Proceedings of the Association for Computational Linguistics,number 2.B Grosz and C Sidner.
1986.
Attention, intentions, and the structure of discourse.
Computational Linguistics,12(3).Geert Hofstede.
1980.
Culture?s consequences: International differences in work-related values.
Sage Publica-tions, Inc.R.
Kannan, S. Vempala, and A. Vetta.
2004.
On clusterings: Good, bad and spectral.
Journal of the ACM (JACM),51(3):497?515.J.
Knobe.
2003.
Intentional action and side effects in ordinary language.
Analysis, 63(3):190?194, July.E.
A. Locke.
1968.
Toward a theory of task motivation and incentives.
Organizational behavior and humanperformance, 3(2).Bertram Malle and J. Knobe.
1997.
The Folk Concept of Intentionality.
Journal of Experimental Psychology,33(2):101?121.Marco Pennacchiotti and Ana-maria Popescu.
2011. to Twitter User Classification.
In ICWSM?11, pages 281?288.Miguel Rios and Jimmy Lin.
2013.
Visualizing the?
Pulse?
of World Cities on Twitter.
Seventh InternationalAAAI Conference on Weblogs .
.
.
, pages 717?720.Alan Ritter, Colin Cherry, and Bill Dolan.
2009.
Unsupervised Modeling of Twitter Conversations.
In HTL-NAACL.Beaux Sharifi, Mark-anthony Hutton, and Jugal Kalita.
2010.
Summarizing Microblogs Automatically.
In ACL-HLT, number June, pages 685?688.Steven a. Sloman, Philip M. Fernbach, and Scott Ewing.
2012.
A Causal Model of Intentionality Judgment.
Mind& Language, 27(2):154?180, April.Michael Tomasello, Malinda Carpenter, Josep Call, Tanya Behne, and Henrike Moll.
2005.
Understanding andsharing intentions: the origins of cultural cognition.
The Behavioral and brain sciences, 28(5):675?91; discus-sion 691?735, October.Marc T Tomlinson, David Bracewell, Wayne Krug, and David Hinote.
2014a.
# impressme : The Language ofMotivation in User Generated Content.
In CICLING.Marc T Tomlinson, David Bracewell, Wayne Krug, David Hinote, and Mary Draper.
2014b.
# mygoal : FindingMotivations on Twitter.
In LREC - 2014.
ELRA.Faiyaz Al Zamal, Wendy Liu, and Derek Ruths.
2012.
Homophily and Latent Attribute Inference : InferringLatent Attributes of Twitter Users from Neighbors.
In ICWSM-2012.57
