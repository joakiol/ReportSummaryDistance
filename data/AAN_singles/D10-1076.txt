Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 778?788,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsTraining continuous space language models:some practical issuesLe Hai Son and Alexandre Allauzen and Guillaume Wisniewski and Franc?ois YvonUniv.
Paris-Sud, France and LIMSI/CNRSBP 133, 91403 Orsay CedexFirstname.Lastname@limsi.frAbstractUsing multi-layer neural networks to esti-mate the probabilities of word sequences isa promising research area in statistical lan-guage modeling, with applications in speechrecognition and statistical machine transla-tion.
However, training such models for largevocabulary tasks is computationally challeng-ing which does not scale easily to the hugecorpora that are nowadays available.
In thiswork, we study the performance and behav-ior of two neural statistical language modelsso as to highlight some important caveats ofthe classical training algorithms.
The inducedword embeddings for extreme cases are alsoanalysed, thus providing insight into the con-vergence issues.
A new initialization schemeand new training techniques are then intro-duced.
These methods are shown to greatly re-duce the training time and to significantly im-prove performance, both in terms of perplexityand on a large-scale translation task.1 IntroductionStatistical language models play an important role inmany practical applications, such as machine trans-lation and automatic speech recognition.
Let V bea finite vocabulary, statistical language models de-fine distributions over sequences of words wL1 in V?usually factorized as:P (wL1 ) = P (w1)L?l=1P (wl|wl?11 )Modeling the joint distribution of several discreterandom variables (such as words in a sentence) isdifficult, especially in real-world Natural LanguageProcessing applications where V typically containsdozens of thousands words.Many approaches to this problem have been pro-posed over the last decades, the most widely usedbeing back-off n-gram language models.
n-grammodels rely on a Markovian assumption, and de-spite this simplification, the maximum likelihood es-timate (MLE) remains unreliable and tends to under-estimate the probability of very rare n-grams, whichare hardly observed even in huge corpora.
Con-ventional smoothing techniques, such as Kneser-Ney and Witten-Bell back-off schemes (see (Chenand Goodman, 1996) for an empirical overview,and (Teh, 2006) for a Bayesian interpretation), per-form back-off on lower order distributions to pro-vide an estimate for the probability of these unseenevents.
n-gram language models rely on a discretespace representation of the vocabulary, where eachword is associated with a discrete index.
In thismodel, the morphological, syntactic and semanticrelationships which structure the lexicon are com-pletely ignored, which negatively impact the gen-eralization performance of the model.
Various ap-proaches have proposed to overcome this limita-tion, notably the use of word-classes (Brown et al,1992; Niesler, 1997), of generalized back-off strate-gies (Bilmes et al, 1997) or the explicit integrationof morphological information in the random-forestmodel (Xu and Jelinek, 2004; Oparin et al, 2008).One of the most successful alternative to date is touse distributed word representations (Bengio et al,2003), where distributionally similar words are rep-resented as neighbors in a continuous space.
This778turns n-grams distributions into smooth functionsof the word representations.
These representationsand the associated probability estimates are jointlycomputed in a multi-layer neural network architec-ture.
This approach has showed significant andconsistent improvements when applied to automaticspeech recognition (Schwenk, 2007; Emami andMangu, 2007; Kuo et al, 2010) and machine trans-lation tasks (Schwenk et al, 2006).
Hence, contin-uous space language models are becoming increas-ingly used.
These successes have revitalized the re-search on neuronal architectures for language mod-els, and given rise to several new proposals (see, forinstance, (Mnih and Hinton, 2007; Mnih and Hinton,2008; Collobert and Weston, 2008)).
A major diffi-culty with these approaches remains the complexityof training, which does not scale well to the mas-sive corpora that are nowadays available.
Practicalsolutions to this problem are discussed in (Schwenk,2007), which introduces a number of optimizationand tricks to make training doable.
Even then, train-ing a neuronal language model typically takes days.In this paper, we empirically study the conver-gence behavior of two multi-layer neural networksfor statistical language modeling, comparing thestandard model of (Bengio et al, 2003) with the log-bilinear (LBL) model of (Mnih and Hinton, 2007).Our contributions are the following: we first pro-pose a reformulation of Mnih and Hinton?s model,which reveals its similarity with extant models, andallows a direct and fair comparison with the stan-dard model.
For the standard model, these resultshighlight the impact of parameter initialization.
Wefirst investigate a re-initialization method which al-lows to escape from the local extremum the standardmodel converges to.
While this method yields a sig-nificative improvement, the underlying assumptionabout the structure of the model does not meet therequirement of very large-scale tasks.
We thereforeintroduce a different initialization strategy, calledone vector initialization.
Experimental results showthat these novel training strategies drastically reducethe total training time, while delivering significantimprovements both in terms of perplexity and in alarge-scale translation task.The rest of this paper is organized as follows.
Wefirst describe, in Section 2, the standard and the LBLlanguage models.
By reformulating the latter, weshow that both models are very similar and empha-size the remaining differences.
Section 2.4 discussescomplexity issues and possible solutions to reducethe training time.
We then report, in Section 3, pre-liminary experimental results that enlighten somecaveats of the standard approach.
Based on theseobservations, we introduce in Section 4 novel andmore efficient training schemes, yielding improvedperformance and a reduced training time both onsmall and large scale experiments.2 Continuous space language modelsLearning a language model amounts to estimate theparameters of the discrete conditional distributionover words given each possible history, where thehistory corresponds to some function of the preced-ing words.
For an n-gram model, the history con-tains the n ?
1 preceding words, and the modelparameters correspond to P (wl|wl?1l?n+1).
Continu-ous space language models aim at computing theseestimates based on a distributed representation ofwords (Bengio et al, 2003), thereby reducing thesparsity issues that plague conventional maximumlikelihood estimation.
In this approach, each wordin the vocabulary is mapped into a real-valued vec-tor and the conditional probability distributions arethen expressed as a (parameterized) smooth func-tion of these feature vectors.
The formalism of neu-ral networks allows to express these two steps in awell-known framework, where, crucially, the map-ping and the model parameters can be learned inconjunction.
In the next paragraphs, we describe thetwo continuous space language models consideredin our study and present the various issues associ-ated with the training of such models, as well as theirmost common remedies.2.1 The standard modelIn the following, we will consider words as indicesin a finite dictionary of size V ; depending on thecontext, w will either refer to the word or to its in-dex in the dictionary.
A word w can also be repre-sented by a 1-of-V coding vector v of RV in whichall elements are null except the wth.
In the standardapproach of (Bengio et al, 2003), the feed-forwardnetwork takes as input the n?1 word history and de-livers an estimate of the probability P (wl|wl?1l?n+1)779as its output.
It consists of three layers.The first layer builds a continuous representationof the history by mapping each word into its real-valued representation.
This mapping is defined byRTv, where R ?
RV?m is a projection matrixand m is the dimension of the continuous projectionword space.
The output of this layer is a vector i of(n ?
1)m real numbers obtained by concatenatingthe representations of the context words.
The pro-jection matrix R is shared along all positions in thehistory vector and is learned automatically.The second layer introduces a non-linear trans-form, where the output layer activation values aredefined by h = tanh (Wihi + bih) , where i is theinput vector, Wih ?
RH?
(n?1)m and bih ?
RH arethe parameters of this layer.
The vector h ?
RH canbe considered as an higher (more abstract) represen-tation of the context than i.The third layer is an output layer that estimates thedesired probability, thanks to the softmax function:P (wl = k|wl?1l?n+1) =exp(ok)?k?
exp(ok?
)(1)o = Whoh + bho, (2)where Who ?
RV?H and bho ?
RV are respec-tively the projection matrix and the bias term associ-ated with this layer.
The wth component in P corre-sponds to the estimated probability of the wth wordof the vocabulary given the input history vector.The standard model has two hyper-parameters(the dimension of projection space m and the size ofhidden layer, H) that define the architecture of theneural network and a set of free parameters ?
thatneed to be learned from data: the projection matrixR, the weight matrix Wih, the bias vector bih, theweight matrix Who and the bias vector bho.In this model, the projection matrices R and Whoplay similar roles as they define maps between thevocabulary and the hidden representation.
The factthat R assigns similar representations to historywords w1 and w2 implies that these words can beexchanged with little impact on the resulting prob-ability distribution.
Likewise, the similarity of twolines in Who is an indication that the correspondingwords tend to have a similar behavior, i.e.
tend tohave a similar probabilities of occurrence in all con-texts.
In the remainder, we will therefore refer to Ras the matrix representing the context space, and toWho as the matrix for the prediction space.2.2 The log-bilinear modelThe work reported (Mnih and Hinton, 2007) de-scribes another parameterization of the architectureintroduced in the previous section.
This parameter-ization is based on Factored Restricted BoltzmannMachine.
According to (Mnih and Hinton, 2007),this model, termed the log-bilinear language model(LBL), achieves, for large vocabulary tasks, bet-ter results in terms of perplexity than the standardmodel, even if the reasons beyond this improvementremain unclear.In this section, we will describe this model andshow how it relates to the standard model.
The LBLmodel estimates the n-gram parameters by:P (wl|wl?1l?n+1) =exp(?E(wl;wl?1l?n+1))?w exp(?E(w;wl?1l?n+1))(3)In this equation, E is an energy function defined as:E(wl;wl?11 ) = ?(l?1?k=l?n+1vkTRCTk)RTvl(4)?
brTRTvl ?
bvTvl= ?vTl R(l?1?k=l?n+1CkRTvk + br)?
vTl bv (5)where R is the projection matrix introduced above,(vk)l?n+1?k?l?1 are the 1-of-V coding vectors forthe history words and vl is the coding vector for wl;Ck ?
Rm?m is a combination matrix and br and bvdenote bias vectors.
All these parameters need to belearned during training.Equation (4) can be rewritten using the notationsintroduced for the standard model.
We then renamebr and bv respectively bih and bho.
We also denotei the concatenation of the (n ?
1) vectors RTvk;likewise Wih denotes the H ?
(n?
1)m matrix ob-tained by concatenating row-wise the (n ?
1) ma-trices Ck.
With these new notations, equations (4)780and (3) can be rewritten as:h = Wihi + biho = Rh + bhoP (wl = k|wl?1l?n+1) =exp(ok)?k?
exp(ok?
)This formulation allows to highlight the similarity ofthe LBL model and the standard model.
These twomodels differ only by the activation function of theirhidden layer (linear for the LBL model and tangenthyperbolic for the standard model) and by their def-inition of the prediction space: for the LBL model,the context space and the prediction space are thesame (R = Who, and thus H = m), while in thestandard model, the prediction space is defined in-dependently from the context space.
This restrictiondrastically reduces the number of free parameters ofthe LBL model.It is finally noteworthy to outline the similarityof this model with standard maximum entropy lan-guage models (Lau et al, 1993; Rosenfeld, 1996).Let x denote the binary vector formed by stackingthe (n-1) 1-of-V encodings of the history words;then the conditional probability distributions esti-mated in the model are proportional to expF (x),where F is an affine transform of x.
The main dif-ference with MaxEnt language models are thus therestricted form of the feature functions, which onlytest one history word, and the particular representa-tion of F , which is defined as:F (x) = RWihR?Tv + Rbih + bhowhere, as before, R?
is formed by concatenating(n?
1) copies of the projection matrix R.2.3 Training and inferenceTraining the two models introduced above can beachieved by maximizing the log-likelihood L of theparameters ?.
This optimization is usually per-formed by stochastic back-propagation as in (Ben-gio et al, 2003).
For all our experiments, the learn-ing rate is fixed at 5?10?3.
The learning weight de-cay and the the weight decay (respectively 1?
10?9and 0) seem to have a minor impact on the results.Learning starts with a random initialization of theparameters under the uniform distribution and con-verges to a local maximum of the log-likelihoodfunction.
Moreover, to prevent overfitting, an earlystopping strategy is adopted: after each epoch, train-ing is stopped when the likelihood of a validation setstops increasing.2.4 Complexity issuesThe main problem with neural language models istheir computational complexity.
For the two mod-els presented in this section, the number of floatingpoint operations needed to predict the label of a sin-gle example is1:((n?
1) ?m + 1)?H + (H + 1)?
V (6)where the first term of the sum corresponds to thecomputation of the hidden layer and the second oneto the computation of the output layer.
The projec-tion of the context words amounts to select one rowof the projection matrix R, as the words are repre-sented with a 1-of-V coding vector.
We can there-fore assume that the computation complexity of thefirst layer is negligible.
Most of the computationtime is thus spent in the output layer, which impliesthat the computing time grows linearly with the vo-cabulary size.
Training these models for large scaletasks is therefore challenging, and a number of trickshave been introduced to make training and inferencetractable (Schwenk and Gauvain, 2002; Schwenk,2007).Short list A simple method to reduce the com-plexity in inference and in learning is to reducethe size of the output vocabulary (Schwenk, 2007):rather than estimating the probability P (wl =w|wl?1l?n+1) for all words in the vocabulary, we onlyestimate it for the N most frequent words of thetraining set (the so-called short-list).
In this case,two vocabularies need to be considered, correspond-ing respectively to the context vocabulary Vc used todefine the history; and the prediction vocabulary Vp.However, this method fails to deliver any probabilityestimate for words outside of the prediction vocab-ulary, meaning that a fall-back strategy needs to bedefined for those words.
In practice, neural network1Recall that learning requires to repeatedly predict the labelfor all the examples in the training set.781language models are combined with a conventionaln-gram model as described in (Schwenk, 2007).Batch mode and resampling Additional speed-ups can be obtained by propagating several exam-ples at once through the network (Bilmes et al,1997).
This ?batch mode?
allows to factorize thematrix operations and cut down both inference andtraining time.
In all our experiments, we used abatch size of 64.
Moreover, the training time is lin-ear in the number of examples in the training data2.Training on very large corpora, which, nowadays,comprise billions of word tokens, cannot be per-formed exhaustively and requires to adopt resam-pling strategies, whereby, at each epoch, the systemis trained with only a small random subset of thetraining data.
This approach enables to effectivelyestimate neural language models on very large cor-pora; it has also been observed empirically that sam-pling the training data can increase the generaliza-tion performance (Schwenk, 2007).3 A head-to-head comparisonIn this section, we analyze a first experimentalstudy of the two neural network language modelsintroduced in Section 2 in order to better under-stand the differences between these models espe-cially in terms of the word representations they in-duce.
Based on this study, we will propose, in thenext section, improvements of both the speed andthe prediction capacity of the models.
In all our ex-periments, 4-gram language models are used.3.1 CorpusThe data we use for training is a large monolingualcorpus, containing all the English texts in the par-allel data of the Arabic to English NIST 2009 con-strained task3.
It consists of 176 millions word to-kens with 532, 557 different word types as the sizeof vocabulary.
The perplexity is computed with re-spect to the 2006 NIST test data, which is used hereas our development data.2Equation (6) gives the complexity of inference for a singleexample.3http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_ConstrainedResources.pdf3.2 Convergence studyIn a first experiment, we trained the two models inthe same setting: we choose to consider a smallvocabulary comprising the 10, 000 most frequentwords.
The same vocabulary is used to constrainthe words occurring in the history and the wordsto be predicted.
The size of hidden layer is set tom = H = 200, the history contains the 3 precedingwords, we use a batch size of 64, a resampling rateof 5% and no weight decay.Figure 1 displays the perplexity convergencecurve measured on the development data for thestandard and the LBL models4.
The convergenceperplexities after the combination with the standardback-off model are also provided for all the mod-els in table 2 (see section 4.3).
We can observethat the LBL model converges faster than the stan-dard model: the latter needs 13 epochs to reachthe stopping criteria, while the former only needs6 epochs.
However, upon convergence, the stan-dard model reaches a lower perplexity than the LBLmodel.0 2 4 6 8 10 12 14120130140150160170180epochsperplexityPerplexitystandardlog bilinearFigure 1: Convergence rate of the standard and the LBLmodels evaluated by the evolution of the perplexity on adevelopment setAs described in Section 2.2, the main differencebetween the standard and the LBL model is the waythe context and the prediction spaces are defined: inthe standard model, the two spaces are distinct; in4The use of a back-off 4-model estimated with the modifiedKnesser-Ney smoothing on the same training data achieves aperplexity of 141 on the development data.782the LBL model, they are bound to be the same.
Witha smaller number of parameters, the LBL model cannot capture as many characteristics of the data as thestandard model, but it converges faster5.
This differ-ence in convergence can be explained by the scarcityof the updates in the projection matrix R in thestandard model: during backpropagation, only thoseweights that are associated with words in the historyare updated.
By contrast, each training sample up-dates all the weights in the prediction matrix Who.3.3 An analysis of the continuous word spaceTo deepen our understanding, we propose to furtheranalyze the induced word embeddings by finding,for some randomly selected words, the five nearestneighbors (according to the Euclidian distance) inthe context space and in the prediction space of thetwo models.
Results are presented in Table 1.If we look first at the standard model, the globalpicture is that for frequent words (is, are, and, toa lesser extend, have), both spaces seem to definemeaningful neighborhood, corresponding to seman-tic and syntactic similarities; this is less true for rarerwords, where we see a greater discrepancy betweenthe context and prediction spaces.
For instance, thedate 1947 seems to be randomly associated in thecontext space, while the 5 nearest words in the pre-diction space form a consistent set of dates.
Thesame trend is also observed for the word Castro.
Ourinterpretation is that for less frequent words, the pro-jection vectors are hardly ever updated and remainclose to their original random initialization.By contrast, the similarities in the (unique) pro-jection space of the LBL remain consistent for allfrequency ranges, and are very similar to the predic-tion space of the standard model.
This seems to val-idate our hypothesis that in the standard model, theprediction space is learned much faster than the con-text space and corroborates our interpretation of theimpact of the scarce updates of rare words.
Anotherpossible explanation is that there is no clear relation5We could increase the number of parameters of the LBLmodel for a fairer comparison with the standard model.
How-ever, this would also increase the size of the vocabulary andcause two new issues: on one hand, the time complexity woulddrastically increase for the LBL model, and on the other hand,both models would not be comparable in terms of perplexity astheir vocabulary would be different.between the context space and the target function:the context space is learned only indirectly by back-propagation.
As a result, due to the random initial-ization of the parameters and to data sparsity, manyvectors of R might be blocked in some local max-ima, meaning that similar vectors cannot be groupedin a consistent way and that the induced similarity ismore ?loose?.4 Improving the standard modelIn Section 3.2, we observed that slightly better re-sults can be obtained with the standard rather thanwith the LBL model.
The latter is however muchfaster to train, and seems to induce better projectionmatrices.
Both effects can be attributed to the partic-ular parameterization of this model, which uses thesame projection matrix both for the context and forthe prediction spaces.
In this section, we proposeseveral new learning regimes that allowed us to im-prove the standard model in terms of both speed andprediction capacity.
All these improvements rely onthe idea of sharing word representations.
While thisidea is not new (see for instance (Collobert and We-ston, 2008)), our analysis enables to better under-stand its impact on the convergence rate.
Finally, theimprovements we propose are evaluated on a real-word machine translation task.4.1 Improving performances withre-initializationThe experiments reported in the previous sectionsuggest that it is possible to improve the perfor-mances of the standard model by building a bettercontext space.
Thus, we introduce a new learningregime, called re-initialization which aims to im-prove the context space by re-injecting the informa-tion on word neighborhoods that emerges in the pre-diction space.
One possible implementation of thisidea is as follows:1. train a standard model until convergence;2. use the prediction space of this model to ini-tialize the context space of a new model; theprediction space is chosen randomly;3. train this new model.783Table 1: The 5 closest words in the representation spaces of the standard and LBL language models.word (frequency) model space 5 most closest wordsis standard context was are were be been900, 350 standard prediction was has would had willLBL both was reveals proves are ONare standard context were is was be been478, 440 standard prediction were could will have canLBL both were is was FOR ONhave standard context had has of also the465, 417 standard prediction are were provide remain willLBL both had has Have were embracemeeting standard context meetings conference them 10 talks150, 317 standard prediction undertaking seminar meetings gathering projectLBL both meetings summit gathering festival hearingImam standard context PCN rebellion 116.
Cuba 49787 standard prediction Castro Sen Nacional Al- RossLBL both Salah Khaled Al- Muhammad Khalid1947 standard context 36 Mercosur definite 2002-2003 era774 standard prediction 1965 1945 1968 1964 1975LBL both 1965 1976 1964 1968 1975Castro standard context exclusively 12.
Boucher Zeng Kelly768 standard prediction Singh Clark da Obasanjo RossLBL both Clark Singh Sabri Rafsanjani SenFigure 2: Evolution of the perplexity on a developmentset for various initialization regimes.The evolution of the perplexity with respect to train-ing epochs for this new method is plotted on Fig-ure 2, where we only represent the evolution of theperplexity during the third training step.
As can beseen, at convergence, the perplexity the model esti-mated with this technique is about 10% smaller thanthe perplexity of the standard model.This result can be explained by considering the re-initialization as a form of annealing technique: re-initializing the context space allows to escape fromthe local extrema the standard model converges to.The fact that the prediction space provides a goodinitialization of the context space also confirms ouranalysis that one difficulty with the standard modelis the estimation of the context space parameters.4.2 Iterative re-initializationThe re-initialization policy introduced in the previ-ous section significantly reduces the perplexity, atthe expense of a longer training time, as it requiresto successively train two models.
As we now knowthat the parameters of the prediction space are fasterto converge, we introduce a second training regimecalled iterative re-initialization which aims to takeadvantage of this property.
We summarize this newtraining regime as follows:1.
Train the model for one epoch.2.
Use the prediction space parameters to reini-tialize the context space.3.
Iterate steps (1) and (2) until convergence.784Figure 3: Evolution of the perplexity on the training datafor various initialization regimes.This regimes yields a model that is somewhat in-between the standard and LBL models as it adds arelationship between the two representation spaces,which lacks in the former model.
This relationship ishowever not expressed through the tying of the cor-responding parameters; instead we let the predictionspace guide the convergence of the context space.As a consequence, we hope that it can achieve a con-vergence speed as fast as the one of the LBL modelwithout degrading its prediction capacity.The result plotted on Figure 2 shows that this in-deed the case: using this training regime, we ob-tained a perplexity similar to the one of the stan-dard model, while at the same time reducing thetotal training time by more than a half, which isof great practical interest (each epoch lasts approxi-mately 8 hours on a 3GHz Xeon processor).Figure 3 displays the perplexity convergencecurve measured on the training data for the standardlearning regime as well as for the re-initializationand iterative re-initialization.
These results showthe same trend as for the perplexity measured onthe development data, and suggest a regularizationeffect of the re-initialization schemes rather than al-lowing the models to escape local optima.4.3 One vector initializationPrinciple The new training regimes introducedabove outperform the standard training regime bothin terms of perplexity and of training time.
However,exchanging information between the context andprediction spaces is only possible when the samevocabulary is used in both spaces.
As discussedin Section 2.4, this configuration is not realistic forvery large-scale tasks.
This is because increasing thenumber of predicted word types is much more com-putationally demanding than increasing the numberof types in the context vocabulary.
Thus, the formervocabulary is typically order of magnitudes largerthan the latter, which means that the re-initializationstrategies can no longer be directly used.It is nonetheless possible to continue drawing in-spirations from the observations made in Section 3,and, crucially, to question the random initializationstrategy.
As discussed above, this strategy may ex-plain why the neighborhoods in the induced con-text space for the less frequent types were diffi-cult to interpret.
As a straightforward alternative,we consider a different initialization strategy whereall the words in the context vocabulary are initiallyprojected onto the same (random) point in the con-text space.
The intuition is that it will be easier tobuild meaningful neighborhoods, especially for raretypes, if all words are initially considered similarand only diverge if there is sufficient evidence in thetraining data to suggest that they should.
This modelis termed the one vector initialization model.Experimental evaluation To validate this ap-proach, we compare the convergence of a standardmodel trained (with the standard learning regime)with the one vector initialization regime.
The con-text vocabulary is defined by the 532, 557 words oc-curring in the training data and the prediction vo-cabulary by the 10, 000 most frequent words6.
Allother parameters are the same as in the previousexperiments.
Based on the curves displayed onFigure 4, we can observe that the model obtainedwith the one vector initialization regime outperformsthe model trained with a completely random ini-tialization.
Moreover, the latter reaches conver-gence in only 14 epochs, while the learning regimewe propose only needs 9 epochs.
Convergence iseven faster than when we used the standard trainingregime and a small context vocabulary.6In this case, the distinction between the context and the pre-diction vocabulary rules out the possibility of a relevant compar-ison based on perplexity between the continuous space languagemodel and a standard back-off language model.7850 5 10 15100110120130140150160170180epochsperplexityPerplexitystandardone vector initializationFigure 4: Perplexity with all-10, 000, 200?
200 modelsTable 2: Summary of the perplexity (PPX) results mea-sured on the same development set with the different con-tinuous space language models.
For all of them, the prob-abilities are combined with the back-off n-gram modelVc size Model # epochs PPX10000 log bilinear 6 239standard 13 227iterative reinit.
6 223reinit.
11 211all standard 14 276one vector init.
9 260To illustrate the impact of our initializationscheme, we also used a principal component anal-ysis to represent the induced word representationsin a two dimensional space.
Figure 5 represents thevectors associated with numbers7 in red, while allother words are represented in blue.
Two differentmodels are used: the standard model on the left, andthe one vector initialization model on the right.
Wecan observe that, for the standard model, most ofthe red points are scattered all over a large portionof the representation space.
On the opposite, forthe one vector initialization model, points associatedwith numbers are much more concentrated: this issimply because all the points are originally identi-cal, and the training aim to spread the point aroundthis starting point.
We also created the closest wordlist reported in Table 3, in a manner similar to Ta-ble 1.
Clearly, the new method seems to yield more7Number are all the words consisting only of digits, with anoptional sign, point or comma such as: 1947; 0,001; -8,2.
(a) with the standard model (b) with the one vector initial-ization modelFigure 5: Comparison of the word embedding in the con-text space for numbers (red points).meaningful neighborhoods in the context space.It is finally noteworthy to mention that when usedwith a small context vocabulary (as in the experi-mental setting of Section 4.1) this initialization strat-egy underperforms the standard initialization.
Thisis simply due to the much greater data sparsity inthe large context vocabulary experiments, where therarer word types are really rare (they typically occuronce or twice).
By contrast, the rarer words in thesmall vocabulary tasks occurred more than severalhundreds times in the training corpus, which wasmore than sufficient to guide the model towards sat-isfactory projection matrices.
This finally suggeststhat there still exists room for improvement if wecan find more efficient initialization strategies thanstarting from one or several random points.4.4 Statistical machine translation experimentsAs a last experiment, we compare the various mod-els on a large scale machine translation task.
Sta-tistical language models are key component of cur-rent statistical machine translation systems (Koehn,2010), where they both help disambiguate lexicalchoices in the target language and influence thechoice of the right word ordering.
The integration ofa neural network language model in such a system isfar from easy, given the computational cost of com-puting word probabilities, a task that is performedrepeatedly during the search of the best translation.We then had to resort to a two pass decoding ap-proach: the first pass uses a conventional back-offlanguage model to produce a n-best list (the n mostlikely translations and their associated scores); in thesecond pass, the probability of the neural languagemodel is computed for each hypothesis and the n-786Table 3: The 5 closest words in the context space of the standard and one vector initialization language modelsword (freq.)
model 5 closest wordsis standard was are were been remains900, 350 1 vector init.
was are be were beenconducted standard undertaken launched $270,900 Mufamadi 6.44-km-long18, 388 1 vector init.
pursued conducts commissioned initiated executedCambodian standard Shyorongi $3,192,700 Zairian depreciations teachers2, 381 1 vector init.
Danish Latvian Estonian Belarussian Bangladeshiautomatically standard MSSD Sarvodaya $676,603,059 Kissana 2,652,6271, 528 1 vector init.
routinely occasionally invariably inadvertently seldomTosevski standard $12.3 Action,3 Kassouma 3536 Applique34 1 vector init.
Shafei Garvalov Dostiev Bourloyannis-Vrailas GrandiOctober-12 standard 39,572 anti-Hutu $12,852,200 non-contracting Party?s8 1 vector init.
March-26 April-11 October-1 June-30 August43727th standard Raqu Tatsei Ayatallah Mesyats Langlois1 1 vector init.
4160th 3651st 3487th 3378th 3558thbest list is accordingly reordered to produce the finaltranslations.The different language models discussed in thisarticle are evaluated on the Arabic to EnglishNIST 2009 constrained task.
For the continuousspace language model, the training data consistsin the parallel corpus used to train the translationmodel (previously described in section 3.1).
The de-velopment data is again the 2006 NIST test set andthe test data is the official 2008 NIST test set.
Oursystem is built using the open-source Moses toolkit(Koehn et al, 2007) with default settings.
To setup our baseline results, we used an extensively op-timized standard back-off 4-grams language modelusing Kneser-Ney smoothing described in (Allauzenet al, 2009).
The weights used during the rerankingare tuned using the Minimum Error Rate Trainingalgorithm (Och, 2003).
Performance is measuredbased on the BLEU (Papineni et al, 2002) scores,which are reported in Table 4.Table 4: BLEU scores on the NIST MT08 test set withdifferent language models.Vc size Model # epochs BLEUall baseline - 37.810000 log bilinear 6 38.2standard 13 38.3iterative reinit.
6 38.4reinit.
11 38.4all standard 14 38.6one vector init.
9 38.7All the experimented neural language modelsyield to a significant BLEU increase.
The best re-sult is obtained by the one vector initialization stan-dard model which achieves a 0.9 BLEU improve-ment.
While this results is similar to the one ob-tained with the standard model, the training time isreduced here by a third.5 ConclusionIn this work, we proposed three new methodsfor training neural network language models andshowed their efficiency both in terms of computa-tional complexity and generalization performance ina real-word machine translation task.
These meth-ods rely on conclusions drawn from a careful studyof the convergence rate of two state-of-the-art mod-els and are based on the idea of sharing the dis-tributed word representations during training.Our work highlights the impact of the initializa-tion and the training scheme for neural network lan-guage models.
Both our experimental results andour new training methods can be closely related tothe pre-training techniques introduced by (Hintonand Salakhutdinov, 2006).
Our future work will thusaim at studying the connections between our empir-ical observations and the deep learning framework.AcknowledgmentsThis work was partly realized as part of the QuaeroProgram, funded by OSEO, the French agency forinnovation.787ReferencesAlexandre Allauzen, Josep Crego, Aure?lien Max, andFranc?ois Yvon.
2009.
LIMSI?s statistical transla-tion systems for WMT?09.
In Proceedings of theFourth Workshop on Statistical Machine Translation,pages 100?104, Athens, Greece, March.
Associationfor Computational Linguistics.Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
JMLR, 3:1137?1155.J.
Bilmes, K. Asanovic, C. Chin, and J. Demmel.
1997.Using phipac to speed error back-propagation learn-ing.
Acoustics, Speech, and Signal Processing, IEEEInternational Conference on, 5:4153.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Comput.Linguist., 18(4):467?479.Stanley F. Chen and Joshua Goodman.
1996.
An empiri-cal study of smoothing techniques for language model-ing.
In Proc.
ACL?96, pages 310?318, San Francisco.Ronan Collobert and Jason Weston.
2008.
A uni-fied architecture for natural language processing: deepneural networks with multitask learning.
In Proc.of ICML?08, pages 160?167, New York, NY, USA.ACM.Ahmed Emami and Lidia Mangu.
2007.
Empirical studyof neural network language models for Arabic speechrecognition.
In Proc.
ASRU?07, pages 147?152, Ky-oto.
IEEE.Geoffrey E. Hinton and Ruslan R. Salakhutdinov.
2006.Reducing the dimensionality of data with neural net-works.
Science, 313(5786):504?507, July.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proc.ACL?07, pages 177?180, Prague, Czech Republic.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press.Hong-Kwang Kuo, Lidia Mangu, Ahmad Emami, andImed Zitouni.
2010.
Morphological and syntactic fea-tures for arabic speech recognition.
In Proc.
ICASSP2010.Raymond Lau, Ronald Rosenfeld, and Salim Roukos.1993.
Adaptive language modeling using the maxi-mum entropy principle.
In Proc HLT?93, pages 108?113, Princeton, New Jersey.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.
InProc.
ICML ?07, pages 641?648, New York, NY, USA.Andriy Mnih and Geoffrey E Hinton.
2008.
A scalablehierarchical distributed language model.
In D. Koller,D.
Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-vances in Neural Information Processing Systems 21,volume 21, pages 1081?1088.Thomas R. Niesler.
1997.
Category-based statisticallanguage models.
Ph.D. thesis, University of Cam-bridge.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
ACL?03, pages160?167, Sapporo, Japan.Ilya Oparin, Ondr?ej Glembek, Luka?s?
Burget, and JanC?ernocky?.
2008.
Morphological random forests forlanguage modeling of inflectional languages.
In Proc.SLT?08, pages 189?192.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalu-ation of machine translation.
In Proc.
ACL?02, pages311?318, Philadelphia.Ronald Rosenfeld.
1996.
A maximum entropy approachto adaptive statistical language modeling.
Computer,Speech and Language, 10:187?228.Holger Schwenk and Jean-Luc Gauvain.
2002.
Connec-tionist language modeling for large vocabulary contin-uous speech recognition.
In Proc.
ICASSP, pages 765?768, Orlando, FL.Holger Schwenk, Daniel De?chelotte, and Jean-Luc Gau-vain.
2006.
Continuous space language modelsfor statistical machine translation.
In Proc.
COL-ING/ACL?06, pages 723?730.Holger Schwenk.
2007.
Continuous space languagemodels.
Comput.
Speech Lang., 21(3):492?518.Yeh W. Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proc.
ofACL?06, pages 985?992, Sidney, Australia.Peng Xu and Frederik Jelinek.
2004.
Random forests inlanguage modeling.
In Proceedings of EMNLP?2004,pages 325?332, Barcelona, Spain.788
