Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 477?485,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsPredicting Response to Political Blog Posts with Topic ModelsTae Yano William W. Cohen Noah A. SmithSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{taey,wcohen,nasmith}@cs.cmu.eduAbstractIn this paper we model discussions in onlinepolitical blogs.
To do this, we extend LatentDirichlet Allocation (Blei et al, 2003), in var-ious ways to capture different characteristicsof the data.
Our models jointly describe thegeneration of the primary documents (posts)as well as the authorship and, optionally, thecontents of the blog community?s verbal reac-tions to each post (comments).
We evaluateour model on a novel comment prediction taskwhere the models are used to predict whichblog users will leave comments on a givenpost.
We also provide a qualitative discussionabout what the models discover.1 IntroductionWeb logging (blogging) and its social impact haverecently attracted considerable public and scientificinterest.
One use of blogs is as a community dis-cussion forum, especially for political discussionand debate.
Blogging has arguably opened a newchannel for huge numbers of people to express theirviews with unprecedented speed and to unprece-dented audiences.
Their collective behavior in theblogosphere has already been noted in the Ameri-can political arena (Adamic and Glance, 2005).
Inthis paper we attempt to deliver a framework usefulfor analyzing text in blogs quantitatively as well asqualitatively.
Better blog text analysis could lead tobetter automated recommendation, organization, ex-traction, and retrieval systems, and might facilitatedata-driven research in the social sciences.Apart from the potential social utility of text pro-cessing for this domain, we believe blog data is wor-thy of scientific study in its own right.
The sponta-neous, reactive, and informal nature of the languagein this domain seems to defy conventional analyticalapproaches in NLP such as supervised text classifi-cation (Mullen and Malouf, 2006), yet the data arerich in argumentative, topical, and temporal struc-ture that can perhaps be modeled computationally.We are especially interested in the semi-causal struc-ture of blog discussions, in which a post ?spawns?comments (or fails to do so), which meander amongtopics and asides and show the personality of theparticipants and the community.Our approach is to develop probabilistic mod-els for the generation of blog posts and commentsjointly within a blog site.
The model is an extensionof Latent Dirichlet Allocation (Blei et al, 2003).Unsupervised topic models can be applied to collec-tions of unannotated documents, requiring very lit-tle corpus engineering.
They can be easily adaptedto new problems by altering the graphical model,then applying standard probabilistic inference algo-rithms.
Different models can be compared to ex-plore the ramifications of different hypotheses aboutthe data.
For example, we will explore whether thecontents of posts a user has commented on in thepast and the words she has used can help predictwhich posts she will respond to in the future.The paper is organized as follows.
In ?2 we re-view prior work on topic modeling for documentcollections and studies of social media like politicalblogs.
We then provide a qualitative characterizationof political blogs, highlighting some of the featureswe believe a computational model should captureand discuss our new corpus of political blogs (?3).We present several different candidate topic modelsthat aim to capture these ideas in ?4.
?5 shows ourempirical evaluation on a new comment predictiontask and a qualitative analysis of the models learned.2 Related WorkNetwork analysis, including citation analysis, hasbeen applied to document collections on the Web(Cohn and Hofmann, 2001).
Adamic and Glance(2005) applied network analysis to the political bl-477ogosphere.
The study modeled the large, complexstructure of the political blogosphere as a networkof hyperlinks among the blog sites, demonstrated theviability of link structure for information discovery,though their analysis of text content was less exten-sive.
In contrast, the text seems to be of interestto social scientists studying blogs as an artifact ofthe political process.
Although attempts to quanti-tatively analyze the contents of political texts havebeen made, results from classical, supervised textclassification experiments are mixed (Mullen andMalouf, 2006; Malouf and Mullen, 2007).
Also, aconsensus on useful, reliable annotation or catego-rization schemes for political texts, at any level ofgranularity, has yet to emerge.Meanwhile, latent topic modeling has become awidely used unsupervised text analysis tool.
The ba-sic aim of those models is to discover recurring pat-terns of ?topics?
within a text collection.
LDA wasintroduced by Blei et al (2003) and has been espe-cially popular because it can be understood as a gen-erative model and because it discovers understand-able topics in many scenarios (Steyvers and Grif-fiths, 2007).
Its declarative specification makes iteasy to extend for new kinds of text collections.
Thetechnique has been applied to Web document collec-tions, notably for community discovery in social net-works (Zhang et al, 2007), opinion mining in userreviews (Titov and McDonald, 2008), and sentimentdiscovery in free-text annotations (Branavan et al,2008).
Dredze et al (2008) applied LDA to a collec-tion of email for summary keyword extraction.
Theauthors evaluated the model with proxy tasks such asrecipient prediction.
More closely related to the dataconsidered in this work, Lin et al (2008) applied avariation of LDA to ideological discourse.A notable trend in the recent research is to aug-ment the models to describe non-textual evidencealongside the document collection.
Several suchstudies are especially relevant to our work.
Blei andJordan (2003) were one of the earliest results in thistrend.
The concept was developed into more generalframework by Blei and McAuliffe (2008).
Steyverset al (2004) and Rosen-Zvi et al (2004) first ex-tended LDA to explicitly model the influence of au-thorship, applying the model to a collection of aca-demic papers from CiteSeer.
The model combinedthe ideas from the mixture model proposed by Mc-Callum (1999) and LDA.
In this model, an abstractnotion ?author?
is associated with a distribution overtopics.
Another approach to the same document col-lection based on LDA was used for citation networkanalysis.
Erosheva et al (2004), following Cohn andHofmann (2001), defined a generative process notonly for each word in the text, but also its citationto other documents in the collection, thereby cap-turing the notion of relations between the documentinto one generative process.
Nallapati and Cohen(2008) introduced the Link-PLSA-LDA model, inwhich the contents of the citing document and the?influences?
on the document (its citations to exist-ing literature), as well as the contents of the citeddocuments, are modeled together.
They further ap-plied the Link-PLSA-LDA model to a blog corpusto analyze its cross citation structure via hyperlinks.In this work, we aim to model the data within blogconversations, focusing on comments left by a blogcommunity in response to a blogger?s post.3 Political Blog DataWe discuss next the dataset used in our experiments.3.1 CorpusWe have collected blog posts and comments from40 blog sites focusing on American politics duringthe period November 2007 to October 2008, con-temporaneous with the presidential elections.
Thediscussions on these blogs focus on American poli-tics, and many themes appear: the Democratic andRepublican candidates, speculation about the resultsof various state contests, and various aspects ofinternational and (more commonly) domestic poli-tics.
The sites were selected to have a variety ofpolitical leanings.
From this pool we chose fiveblogs which accumulated a large number of postsduring this period: Carpetbagger (CB),1 Daily Kos(DK),2 Matthew Yglesias (MY),3 Red State (RS),4and Right Wing News (RWN).5 CB and MY ceasedas independent bloggers in August 2008.6 Because1http://www.thecarpetbaggerreport.com2http://www.dailykos.com3http://matthewyglesias.theatlantic.com4http://www.redstate.com5http://www.rightwingnews.com6The authors of those blogs now write for larger on-line media, CB for Washingon Monthly at http://www.478MY RWN CB RS DKTime span (from 11/11/07) ?8/2/08 ?10/10/08 ?8/25/08 ?6/26/08 ?4/9/08# training posts 1607 1052 1080 2045 2146# words (total) 110,788 194,948 183,635 321,699 221,820(on average per post) (68) (185) (170) (157) (103)# comments 56,507 34,734 34,244 59,687 425,494(on average per post) (35) (33) (31) (29) (198)(unique commenters, on average) (24) (13) (24) (14) (93)# words in comments (total) 2,287,843 1,073,726 1,411,363 1,675,098 8,359,456(on average per post) (1423) (1020) (1306) (819) (3895)(on average per comment) (41) (31) (41) (27) (20)Post vocabulary size 6,659 9,707 7,579 12,282 10,179Comment vocabulary size 33,350 22,024 24,702 25,473 58,591Size of user pool 7,341 963 5,059 2,789 16,849# test posts 183 113 121 231 240Table 1: Details of the blog data used in this paper.our focus in this paper is on blog posts and theircomments, we discard posts on which no one com-mented within six days.
We also remove posts withtoo few words: specifically, we retain a post onlyif it has at least five words in the main entry, andat least five words in the comment section.
Allposts are represented as text only (images, hyper-links, and other non-text contents are ignored).
Tostandardize the texts, we remove from the text 670commonly used stop words, non-alphabet symbolsincluding punctuation marks, and strings consistingof only symbols and digits.
We also discard infre-quent words from our dataset: for each word in apost?s main entry, we kept it only if it appears atleast one more time in some main entry.
We ap-ply the same word pruning to the comment sectionas well.
The corpus size and the vocabulary size ofthe five datasets are listed in Table 1.
In addition,each user?s handle is replaced with a unique inte-ger.
The dataset is available for download at http://www.ark.cs.cmu.edu/blog-data.3.2 Qualitative Properties of BlogsWe believe that readers?
reactions to blog posts arean integral part of blogging activity.
Often com-ments are much more substantial and informativethan the post.
While circumspective articles limitthemselves to allusions or oblique references, read-ers?
comments may point to heart of the matter morewashingtonmonthly.com and MY for Think Progressathttp://yglesias.thinkprogress.org.boldly.
Opinions are expressed more blatantly incomments.
Comments may help a human (or au-tomated) reader to understand the post more clearlywhen the main text is too terse, stylized, or technical.Although the main entry and its comments arecertainly related and at least partially address similartopics, they are markedly different in several ways.First of all, their vocabulary is noticeably different.Comments are more casual, conversational, and fullof jargon.
They are less carefully edited and there-fore contain more misspellings and typographical er-rors.
There is more diversity among comments thanwithin the single-author post, both in style of writingand in what commenters like to talk about.
Depend-ing on the subjects covered in a blog post, differenttypes of people are inspired to respond.
We believethat analyzing a piece of text based on the reactionit causes among those who read it is a fascinatingproblem for NLP.Blog sites are also quite distinctive from eachother.
Their language, discussion topics, and col-lective political orientations vary greatly.
Their vol-umes also vary; multi-author sites (such as DK, RS)may consistently produce over twenty posts per day,while single-author sites (such asMY, CB) may havea day with only one post.
Single author sites alsotend to have a much smaller vocabulary and rangeof interests.
The sites are also culturally differentin commenting styles; some sites are full of shortinterjections, while others have longer, more analyt-ical comments.
On some sites, users appear to be479?
?z z?u?w?DN M1?
?z z?w?
u?
?
?w?DMN1Figure 1: Left:LinkLDA (Eroshevaet al, 2004), withvariables reassigned.Right:CommentLDA.
Intraining, w, u, and(in CommentLDA)w?
are observed.
D isthe number of blogposts, and N and Mare the word countsin the post and the allof its comments,respectively.
Here we?count by verbosity.
?close-knit, while others have high turnover.In the next section, we describe how we applytopic models to political blogs, and how these prob-abilistic models can put to use to make predictions.4 Generative ModelsThe first model we consider is LinkLDA, which isanalogous to the model of Erosheva et al (2004),though the variables are given different meaningshere.7 The graphical model is depicted in Fig.
1(left).
As in LDA and its many variants, this modelpostulates a set of latent ?topic?
variables, whereeach topic k corresponds to a multinomial distribu-tion ?k over the vocabulary.
In addition to gener-ating the words in the post from its topic mixture,this model also generates a bag of users who respondto the post, according to a distribution ?
over usersgiven topics.
In this model, the topic distribution ?is all that determines the text content of the post andwhich users will respond to the post.LinkLDA models which users are likely to re-spond to a post, but it does not model what theywill write.
Our new model, CommentLDA, gen-erates the contents of the comments (see Fig.
1,right).
In order to capture the differences in lan-guage style between posts and comments, however,we use a different conditional distribution over com-ment words given topics, ??.
The post text, commenttext, and commenter distributions are all interdepen-dent through the (latent) topic distribution ?, and atopic k is defined by:7Instead of blog commenters, they modeled citations.?
A multinomial distribution ?k over post words;?
A multinomial distribution ?
?k over commentwords; and?
A multinomial distribution ?k over blog com-menters who might react to posts on the topic.Formally, LinkLDA and CommentLDA generateblog data as follows: For each blog post (1 to D):1.
Choose a distribution ?
over topics accordingto Dirichlet distribution ?.2.
For i from 1 to Ni (the length of the post):(a) Choose a topic zi according to ?.
(b) Choose a word wi according to the topic?spost word distribution ?zi .3.
For j from 1 to Mi (the length of the commentson the post, in words):(a) Choose a topic z?j .
(b) Choose an author uj from the topic?s com-menter distribution ?z?j .
(c) (CommentLDA only) Choose a word w?jaccording to the topic?s comment worddistribution ?
?z?j .4.1 Variations on Counting UsersAs described, CommentLDA associates each com-ment word token with an independent author.
Inboth LinkLDA and CommentLDA, this ?countingby verbosity?
will force ?
to give higher probabil-ity to users who write longer comments with more480words.
We consider two alternative ways to countcomments, applicable to both LinkLDA and Com-mentLDA.
These both involve a change to step 3 inthe generative process.Counting by response (replaces step 3): For j from1 to Ui (the number of users who respond to thepost): (a) and (b) as before.
(c) (CommentLDA only)For ` from 1 to `i,j (the number of words in uj?scomments), choose w?` according to the topic?s com-ment word distribution ?
?z?j .
This model collapses allcomments by a user into a single bag of words on asingle topic.8Counting by comments (replaces step 3): For jfrom 1 to Ci (the number of comments on the post):(a) and (b) as before.
(c) (CommentLDA only) For `from 1 to `i,j (the number of words in comment j),choose w?` according to the topic?s comment worddistribution ?
?z?j .
Intuitively, each comment has atopic, a user, and a bag of words.The three variations?counting users by ver-bosity, response, or comments?correspond to dif-ferent ways of thinking about topics in political blogdiscourse.
Counting by verbosity will let garruloususers define the topics.
Counting by response ismore democratic, letting every user who respondsto a blog post get an equal vote in determining whatthe post is about, no matter how much that user says.Counting by comments gives more say to users whoengage in the conversation repeatedly.4.2 ImplementationWe train our model using empirical Bayesian esti-mation.
Specifically, we fix ?
= 0.1, and we learnthe values of word distributions ?
and ??
and userdistribution ?
by maximizing the likelihood of thetraining data:p(w,w?,u | ?, ?, ?
?, ?)
(1)(Obviously, ??
is not present in the LinkLDA mod-els.)
This requires an inference step that marginal-izes out the latent variables, ?, z, and z?, for whichwe use Gibbs sampling as implemented by the Hier-archical Bayes Compiler (Daume?, 2007).
The Gibbs8The counting-by-response models are deficient, since theyassume each user will only be chosen once per blog post, thoughthey permit the same user to be chosen repeatedly.sampling inference algorithm for LDA was first in-troduced by Griffiths and Steyvers (2004) and hassince been used widely.5 Empirical EvaluationWe adopt a typical NLP ?train-and-test?
strategy thatlearns the model parameters on a training datasetconsisting of a collection of blog posts and theircommenters and comments, then considers an un-seen test dataset from a later time period.
Manykinds of predictions might be made about the testset and then evaluated against the true comment re-sponse.
For example, the likelihood of a user tocomment on the post, given knowledge of ?
can beestimated as:9p(u | wN1 , ?, ?)
=K?z=1p(u | z, ?
)p(z | wN1 , ?
)=K?z=1?z,u ?
?z (2)The latter is in a sense a ?guessing game,?
a pre-diction on who is going to comment on a new blogpost.
A similar task was used by Nallapati and Co-hen (2008) for assessing the performance of Link-PLSA-LDA: they predicted the presence or absenceof citation links between documents.
We report theperformance on this prediction task using our sixblog topic models (LinkLDA and CommentLDA,with three counting variations each).Our aim is to explore and compare the effective-ness of the different models in discovering topicsthat are useful for a practical task.
We also give aqualitative analysis of topics learned.5.1 Comment PredictionFor each political blog, we trained the three varia-tions each of LinkLDA and CommentLDA.
Modelparameters ?, ?, and (in CommentLDA) ??
werelearned by maximizing likelihood, with Gibbs sam-pling for inference, as described in ?4.2.
The num-ber of topics K was fixed at 15.A simple baseline method makes a post-independent prediction that ranks users by theircomment frequency.
Since blogs often have a ?coreconstituency?
of users who post frequently, this is a9Another approach would attempt to integrate out ?.481n=5 n=10 n=20 n=30 oracleMYFreq.
23.93 18.68 14.20 11.65 13.18NB 25.13 19.28 14.20 11.63 13.54Link-v 20.10 14.04 11.17 9.23 11.32Link-r 26.77 18.63 14.64 12.47 14.03Link-c 25.13 18.85 14.61 11.91 13.84Com-v 22.84 17.15 12.75 10.69 12.77Com-r 27.54 20.54 14.61 12.45 14.35Com-c 22.40 18.50 14.83 12.56 14.20Max 94.75 89.89 73.63 58.76 92.60RWNFreq.
32.56 30.17 22.61 19.7 27.19NB 25.63 34.86 27.61 22.03 18.28Link-v 28.14 21.06 17.34 14.51 19.81Link-r 32.92 29.29 22.61 18.96 26.32Link-c 32.56 27.43 21.15 17.43 25.09Com-v 29.02 24.07 19.07 16.04 22.71Com-r 36.10 29.64 23.8 19.26 25.97Com-c 32.03 27.43 19.82 16.25 23.88Max 90.97 76.46 52.56 37.05 96.16CBFreq.
33.38 28.84 24.17 20.99 21.63NB 36.36 31.15 25.08 21.40 23.22Link-v 32.06 26.11 19.79 17.43 18.31Link-r 37.02 31.65 24.62 20.85 22.34Link-c 36.03 32.06 25.28 21.10 23.44Com-v 32.39 26.36 20.95 18.26 19.85Com-r 35.53 29.33 24.33 20.22 22.02Com-c 33.71 29.25 23.80 19.86 21.68Max 99.66 98.34 88.88 72.53 95.58RSFreq.
25.45 16.75 11.42 9.62 17.15NB 22.07 16.01 11.60 9.76 16.50Link-v 14.63 11.9 9.13 7.76 11.38Link-r 25.19 16.92 12.14 9.82 17.98Link-c 24.50 16.45 11.49 9.32 16.76Com-v 14.97 10.51 8.46 7.37 11.3 0Com-r 15.93 11.42 8.37 6.89 10.97Com-c 17.57 12.46 8.85 7.34 12.14Max 80.77 62.98 40.95 29.03 91.86DKFreq.
24.66 19.08 15.33 13.34 9.64NB 35.00 27.33 22.25 19.45 13.97Link-v 20.58 19.79 15.83 13.88 10.35Link-r 33.83 27.29 21.39 19.09 13.44Link-c 28.66 22.16 18.33 16.79 12.60Com-v 22.16 18.00 16.54 14.45 10.92Com-r 33.08 25.66 20.66 18.29 12.74Com-c 26.08 20.91 17.47 15.59 11.82Max 100.00 100.00 100.00 99.09 98.62Table 2: Comment prediction results on 5 blogs.
See text.strong baseline.
We also compared to a Na?
?ve Bayesclassifier (with word counts in the post?s main en-try as features).
To perform the prediction task withour models, we took the following steps.
First, weremoved the comment section (both the words andthe authorship information) from the test data set.Then, we ran a Gibbs sampler with the partial data,fixing the model parameters to their learned valuesand the blog post words to their observed values.This gives a posterior topic mixture for each post (?in the above equations).10 We then computed eachuser?s comment prediction score for each post as inEq.
2.
Users are ordered by their posterior probabil-ities.
Note that these posteriors have different mean-ings for different variations:?
When counting by verbosity, the value is the prob-ability that the next (or any) comment word willbe generated by the user, given the blog post.?
When counting by response, the value is the prob-ability that the user will respond at all, given theblog post.
(Intuitively, this approach best matchesthe task at hand.)?
When counting by comments, the value is theprobability that the next (or any) comment will begenerated by the user, given the blog post.We compare our commenter ranking-by-likelihood with the actual commenters in the testset.
We report in Tab.
2 the precision (macro-averaged across posts) of our predictions at variouscut-offs (n).
The oracle column is the precisionwhere it is equal to the recall, equivalent to thesituation when the true number of commentersis known.
(The performance of random guessingis well below 1% for all sites at cut-off pointsshown.)
?Freq.?
and ?NB?
refer to our baselinemethods.
?Link?
refers to LinkLDA and ?Com?
toCommentLDA.
The suffixes denote the countingmethods: verbosity (?-v?
), response (?-r?
), andcomments (?-c?).
Recall that we considered onlythe comments by the users seen at least once in thetraining set, so perfect precision, as well as recall, isimpossible when new users comment on a post; theMax row shows the maximum performance possiblegiven the set of commenters recognizable from thetraining data.10For a few cases we checked the stability of the sampler andfound results varied by less than 1% precision across ten runs.482Our results suggest that, if asked to guess 5 peo-ple who would comment on a new post given somesite history, we will get 25?37% of them right, de-pending on the site, given the content of a new post.We achieved some improvement over both thebaseline and Na?
?ve Bayes for some cut-offs on threeof the five sites, though the gains were very smallfor and RS and CB.
LinkLDA usually works slightlybetter than CommentLDA, except for MY, whereCommentLDA is stronger, and RS, where Com-mentLDA is extremely poor.
Differences in com-menting style are likely to blame: MY has relativelylong comments in comparison to RS, as well as DK.MY is the only site where CommentLDA variationsconsistently outperformed LinkLDA variations, aswell as Na?
?ve Bayes classifiers.
This suggests thatsites with more terse comments may be too sparseto support a rich model like CommentLDA.In general, counting by response works best,though counting by comments is a close rival insome cases.
We observe that counting by responsetends to help LinkLDA, which is ignorant of theword contents of the comment, more than it helpsCommentLDA.
Varying the counting method canbring as much as 10% performance gain.Each of the models we have tested makes differ-ent assumptions about the behavior of commenters.Our results suggest that commenters on differentsites behave differently, so that the same modelingassumptions cannot be made universally.
In futurework, we hope to permit blog-specific propertiesto be automatically discovered during learning, sothat, for example, the comment words can be ex-ploited when they are helpful but assumed indepen-dent when they are not.
Of course, improved per-formance might also be obtained with more topics,richer priors over topic distributions, or models thattake into account other cues, such as the time of thepost, pages it links to, etc.
It is also possible that bet-ter performance will come from more sophisticatedsupervised models that do not use topics.5.2 Qualitative EvaluationAside from prediction tasks such as above, themodel parameters by themselves can be informative.?
defines which words are likely to occur in the postbody for a given topic.
??
tells which words arelikely to appear in the collective response to a partic-ular topic.
Similarity or divergence of the two dis-tributions can tell us about differences in languageused by bloggers and their readers.
?
expressesusers?
topic preferences.
A pair or group of par-ticipants may be seen as ?like-minded?
if they havesimilar topic preferences (perhaps useful in collabo-rative filtering).Following previous work on LDA and its exten-sions, we show words most strongly associated witha few topics, arguing that some coherent clustershave been discovered.
Table 3 shows topics discov-ered in MY using CommentLDA (counting by com-ments).
This is the blog site where our models mostconsistently outperformed the Na?
?ve Bayes classi-fiers and LinkLDA, therefore we believe the modelwas a good fit for this dataset.Since the site is concentrated on American pol-itics, many of the topics look alike.
Table 3 showsthe most probable words in the posts, comments, andboth together for five hand-picked topics that wererelatively transparent.
The probabilistic scores ofthose words are computed with the scoring methodsuggested by Blei and Lafferty (in press).The model clustered words into topics pertain-ing to religion and domestic policy (first and lasttopics in Table 3) quite reasonably.
Some of thereligion-related words make sense in light of cur-rent affairs.11 Some words in the comment sec-tion are slightly off-topic from the issue of religion,such as dawkins12 or wright,13 but are relevant inthe context of real-world events.
Notice those wordsrank highly only in the comment section, showingdifferences between discussion in the post and thecomments.
This is also noticeable, for example, inthe ?primary?
topic (second in Table 3), where theRepublican primary receives more discussion in themain post, and in the ?Iraq war?
and ?energy?
top-ics, where bloggers discuss strategy and commenters11Mitt Romney was a candidate for the Republican nomi-nation in 2008 presidential election.
He is a member of TheChurch of Jesus Christ of Latter-Day Saints.
Another candi-date, Mike Huckabee, is an ordained Southern Baptist minister.Moktada al-Sadr is an Iraqi theologian and political activist, andJohn Hagee is an influential televangelist.12Richard Dawkins is a well known evolutionary biologistwho is a vocal critic of intelligent design.13We believe this is a reference to Rev.
Jeremiah Wright ofTrinity United Church of Christ, whose inflammatory rhetoricwas negatively associated with then-candidate Barack Obama.483religion; in both: people, just, american, church, believe, god, black, jesus, mormon, faith, jews, right, say,mormons, religious, pointin posts: romney, huckabee, muslim, political, hagee, cabinet, mitt, consider, true, anti, problem,course, views, life, real, speech, moral, answer, jobs, difference, muslims, hardly, going,christianityin comments: religion, think, know, really, christian, obama, white, wright, way, said, good, world, science,time, dawkins, human, man, things, fact, years, mean, atheists, blacks, christiansprimary; in both: obama, clinton, mccain, race, win, iowa, delegates, going, people, state, nomination, primary,hillary, election, polls, party, states, voters, campaign, michigan, justin posts: huckabee, wins, romney, got, percent, lead, barack, point, majority, ohio, big, victory, strong,pretty, winning, support, primaries, south, rulesin comments: vote, think, superdelegates, democratic, candidate, pledged, delegate, independents, votes,white, democrats, really, way, caucuses, edwards, florida, supporters, wisconsin, countIraq war; inboth:american, iran, just, iraq, people, support, point, country, nuclear, world, power, military,really, government, war, army, right, iraqi, thinkin posts: kind, united, forces, international, presence, political, states, foreign, countries, role, need,making, course, problem, shiite, john, understand, level, idea, security, mainin comments: israel, sadr, bush, state, way, oil, years, time, going, good, weapons, saddam, know, maliki,want, say, policy, fact, said, shia, troopsenergy; in both: people, just, tax, carbon, think, high, transit, need, live, going, want, problem, way, market,money, income, cost, densityin posts: idea, public, pretty, course, economic, plan, making, climate, spending, economy, reduce,change, increase, policy, things, stimulus, cuts, low, fi nancial, housing, bad, realin comments: taxes, fuel, years, time, rail, oil, cars, car, energy, good, really, lot, point, better, prices, pay,city, know, government, price, work, technologydomestic policy;in both:people, public, health, care, insurance, college, schools, education, higher, children, think,poor, really, just, kids, want, school, going, betterin posts: different, things, point, fact, social, work, large, article, getting, inequality, matt, simply,percent, tend, hard, increase, huge, costs, course, policy, happenin comments: students, universal, high, good, way, income, money, government, class, problem, pay, amer-icans, private, plan, american, country, immigrants, time, know, taxes, costTable 3: The most probable words for some CommentLDA topics (MY).focus on the tangible (oil, taxes, prices, weapons).While our topic-modeling approach achievesmixed results on the prediction task, we believe itholds promise as a way to understand and summa-rize the data.
Without CommentLDA, we would notbe able to easily see the differences noted above inblogger and commenter language.
In future work,we plan to explore models with weaker indepen-dence assumptions among users, among blog postsover time, and even across blogs.
This line of re-search will permit a more nuanced understandingof language in the blogosphere and in political dis-course more generally.6 ConclusionIn this paper we applied several probabilistic topicmodels to discourse within political blogs.
We in-troduced a novel comment prediction task to assessthese models in an objective evaluation with possi-ble practical applications.
The results show that pre-dicting political discourse behavior is challenging,in part because of considerable variation in user be-havior across different blog sites.
Our results showthat using topic modeling, we can begin to make rea-sonable predictions as well as qualitative discoveriesabout language in blogs.AcknowledgmentsThis research was supported by a gift from MicrosoftResearch and NSF IIS-0836431.
The authors appreciatehelpful comments from the anonymous reviewers, Ja-HuiChang, Hal Daume?, and Ramesh Nallapati.
We thankShay Cohen for his help with inference algorithms andthe members of the ARK group for reviewing this paper.484ReferencesL.
Adamic and N. Glance.
2005.
The political blogo-sphere and the 2004 U.S. election: Divided they blog.In Proceedings of the 2nd Annual Workshop on the We-blogging Ecosystem: Aggregation, Analysis and Dy-namics.D.
Blei and M. Jordan.
2003.
Modeling annotated data.In Proceedings of the 26th Annual International ACMSIGIR Conference on Research and Development inInformaion Retrieval.D.
Blei and J. Lafferty.
In press.
Topic models.
In A. Sri-vastava and M. Sahami, editors, Text Mining: Theoryand Applications.
Taylor and Franci.D.
Blei and J. McAuliffe.
2008.
Supervised topic mod-els.
In Advances in Neural Information ProcessingSystems 20.D.
Blei, A. Ng, and M. Jordan.
2003.
Latent Dirich-let alocation.
Journal of Machine Learning Research,3:993?1022.S.
R. K. Branavan, H. Chen, J. Eisenstein, and R. Barzi-lay.
2008.
Learning document-level semantic prop-erties from free-text annotations.
In Proceedings ofACL-08: HLT.D.
Cohn and T. Hofmann.
2001.
The missing link?aprobabilistic model of document content and hyper-text connectivity.
In Neural Information ProcessingSystems 13.H.
Daume?.
2007.
HBC: Hierarchical Bayes compiler.http://www.cs.utah.edu/?hal/HBC.M.
Dredze, H. M. Wallach, D. Puller, and F. Pereira.2008.
Generating summary keywords for emails us-ing topics.
In Proceedings of the 13th InternationalConference on Intelligent User Interfaces.E.
Erosheva, S. Fienberg, and J. Lafferty.
2004.
Mixedmembership models of scientific publications.
Pro-ceedings of the National Academy of Sciences, pages5220?5227, April.T.
L. Griffiths and M. Steyvers.
2004.
Finding scien-tific topics.
Proceedings of the National Academy ofSciences, 101 Suppl.
1:5228?5235, April.W.-H. Lin, E. Xing, and A. Hauptmann.
2008.
A jointtopic and perspective model for ideological discourse.In Proceedings of 2008 European Conference on Ma-chine Learning and Principles and Practice of Knowl-edge Discovery in Databases.R.
Malouf and T. Mullen.
2007.
Graph-based user clas-sification for informal online political discourse.
InProceedings of the 1st Workshop on Information Cred-ibility on the Web.A.
McCallum.
1999.
Multi-label text classification witha mixture model trained by EM.
In AAAI Workshop onText Learning.T.
Mullen and R. Malouf.
2006.
A preliminary investi-gation into sentiment analysis of informal political dis-course.
In Proceedings of AAAI-2006 Spring Sympo-sium on Computational Approaches to Analyzing We-blogs.R.
Nallapati and W. Cohen.
2008.
Link-PLSA-LDA: Anew unsupervised model for topics and influence ofblogs.
In Proceedings of the 2nd International Con-ference on Weblogs and Social Media.M.
Rosen-Zvi, T. Griffiths, M. Steyvers, and P Smyth.2004.
The author-topic model for authors and docu-ments.
In Proceedings of the 20th Conference on Un-certainty in Artificial Intelligence.M.
Steyvers and T. Griffiths.
2007.
Probabilistic topicmodels.
In T. Landauer, D. Mcnamara, S. Dennis,and W. Kintsch, editors, Handbook of Latent SemanticAnalysis.
Lawrence Erlbaum Associates.M.
Steyvers, P. Smyth, M. Rosen-Zvi, and T. L. Grif-fiths.
2004.
Probabilistic author-topic models for in-formation discovery.
In Proceedings of the tenth ACMSIGKDD international conference on Knowledge dis-covery and data mining.I.
Titov and R. McDonald.
2008.
A joint model of textand aspect ratings for sentiment summarization.
InProceedings of ACL-08: HLT.H.
Zhang, B. Qiu, C. L. Giles, H. C. Foley, and J. Yen.2007.
An LDA-based community structure discoveryapproach for large-scale social networks.
In Proceed-ings of the IEEE International Conference on Intelli-gence and Security Informatics.485
