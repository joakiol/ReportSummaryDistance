Web Text Corpus for Natural Language ProcessingVinci Liu and James R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australia{vinci,james}@it.usyd.edu.auAbstractWeb text has been successfully used astraining data for many NLP applications.While most previous work accesses webtext through search engine hit counts, wecreated a Web Corpus by downloadingweb pages to create a topic-diverse collec-tion of 10 billion words of English.
Weshow that for context-sensitive spellingcorrection the Web Corpus results are bet-ter than using a search engine.
For the-saurus extraction, it achieved similar over-all results to a corpus of newspaper text.With many more words available on theweb, better results can be obtained by col-lecting much larger web corpora.1 IntroductionTraditional written corpora for linguistics researchare created primarily from printed text, such asnewspaper articles and books.
With the growth ofthe World Wide Web as an information resource, itis increasingly being used as training data in Nat-ural Language Processing (NLP) tasks.There are many advantages to creating a corpusfrom web data rather than printed text.
All webdata is already in electronic form and thereforereadable by computers, whereas not all printeddata is available electronically.
The vast amountof text available on the web is a major advantage,with Keller and Lapata (2003) estimating that over98 billion words were indexed by Google in 2003.The performance of NLP systems tends to im-prove with increasing amount of training data.Banko and Brill (2001) showed that for context-sensitive spelling correction, increasing the train-ing data size increases the accuracy, for up to 1billion words in their experiments.To date, most NLP tasks that have utilised webdata have accessed it through search engines, us-ing only the hit counts or examining a limitednumber of results pages.
The tasks are reducedto determining n-gram probabilities which arethen estimated by hit counts from search enginequeries.
This method only gathers informationfrom the hit counts but does not require the com-putationally expensive downloading of actual textfor analysis.
Unfortunately search engines werenot designed for NLP research and the reported hitcounts are subject to uncontrolled variations andapproximations (Nakov and Hearst, 2005).
Volk(2002) proposed a linguistic search engine to ex-tract word relationships more accurately.We created a 10 billion word topic-diverse WebCorpus by spidering websites from a set of seedURLs.
The seed set is selected from the OpenDirectory to ensure that a diverse range of top-ics is included in the corpus.
A process of textcleaning transforms the HTML text into a formuseable by most NLP systems ?
tokenised words,one sentence per line.
Text filtering removes un-wanted text from the corpus, such as non-Englishsentences and most lines of text that are not gram-matical sentences.
We compare the vocabulary ofthe Web Corpus with newswire.Our Web Corpus is evaluated on two NLP tasks.Context-sensitive spelling correction is a disam-biguation problem, where the correction word in aconfusion set (e.g.
{their, they?re}) needs to be se-lected for a given context.
Thesaurus extraction isa similarity task, where synonyms of a target wordare extracted from a corpus of unlabelled text.
Ourevaluation demonstrates that web text can be usedfor the same tasks as search engine hit counts andnewspaper text.
However, there is a much largerquantity of freely available web text to exploit.2332 Existing Web CorporaThe web has become an indispensible resourcewith a vast amount of information available.
ManyNLP tasks have successfully utilised web data, in-cluding machine translation (Grefenstette, 1999),prepositional phrase attachment (Volk, 2001), andother-anaphora resolution (Modjeska et al, 2003).2.1 Search Engine Hit CountsMost NLP systems that have used the web accessit via search engines such as Altavista and Google.N-gram counts are approximated by literal queries?w1 ... wn?.
Relations between two words areapproximated in Altavista by the NEAR operator(which locates word pairs within 10 tokens of eachother).
The overall coverage of the queries canbe expanded by morphological expansion of thesearch terms.Keller and Lapata (2003) demonstrated a highdegree of correlation between n-gram estimatesfrom search engine hit counts and n-gram frequen-cies obtained from traditional corpora such as theBritish National Corpus (BNC).
The hit countsalso had a higher correlation to human plausibil-ity judgements than the BNC counts.The web count method contrasts with tradi-tional methods where the frequencies are obtainedfrom a corpus of locally available text.
While thecorpus is much smaller than the web, an accu-rate count and further text processing is possiblebecause all of the contexts are readily accessible.The web count method obtains only an approxi-mate number of matches on the web, with no con-trol over which pages are indexed by the searchengines and with no further analysis possible.There are a number of limitations in the searchengine approximations.
As many search enginesdiscard punctuation information (especially whenusing the NEAR operator), words considered ad-jacent to each other could actually lie in differ-ent sentences or paragraphs.
For example in Volk(2001), the system assumes that a preposition at-taches to a noun simply when the noun appearswithin a fixed context window of the preposition.The preposition and noun could in fact be relateddifferently or be in different sentences altogether.The speed of querying search engines is anotherconcern.
Keller and Lapata (2003) needed to ob-tain the frequency counts of 26,271 test adjectivepairs from the web and from the BNC for the taskof prenominal adjective ordering.
While extract-ing this information from the BNC presented nodifficulty, making so many queries to the Altavistawas too time-consuming.
They had to reduce thesize of the test set to obtain a result.Lapata and Keller (2005) performed a widerange of NLP tasks using web data by queryingAltavista and Google.
This included variety ofgeneration tasks (e.g.
machine translation candi-date selection) and analysis tasks (e.g.
preposi-tional phrase attachment, countability detection).They showed that while web counts usually out-performed BNC counts and consistently outper-formed the baseline, the best performing systemis usually a supervised method trained on anno-tated data.
Keller and Lapata concluded that hav-ing access linguistic information (accurate n-gramcounts, POS tags, and parses) outperforms using alarge amount of web data.2.2 Spidered Web CorporaA few projects have utilised data downloaded fromthe web.
Ravichandran et al (2005) used a col-lection of 31 million web pages to produce nounsimilarity lists.
They found that most NLP algo-rithms are unable to run on web scale data, espe-cially those with quadratic running time.
Halacsyet al (2004) created a Hungarian corpus from theweb by downloading text from the .hu domain.From a 18 million page crawl of the web a 1 bil-lion word corpus is created (removing duplicatesand non-Hungarian text).A terabyte-sized corpus of the web was col-lected at the University of Waterloo in 2001.
Abreadth first search from a seed set of universityhome pages yielded over 53 billion words, requir-ing 960GB of storage.
Clarke et al (2002) andTerra and Clarke (2003) used this corpus for theirquestion answering system.
They obtained in-creasing performance with increasing corpus sizebut began reaching asymptotic behaviour at the300-500GB range.3 Creating the Web CorpusThere are many challenges in creating a web cor-pus, as the World Wide Web is unstructured andwithout a definitive directory.
No simple methodexists to collect a large representative sample ofthe web.
Two main approaches exist for collect-ing representative web samples ?
IP address sam-pling and random walks.
The IP address sam-pling technique randomly generates IP addresses234and explores any websites found (Lawrence andGiles, 1999).
This method requires substantial re-sources as many attempts are made for each web-site found.
Lawrence and Giles reported that 1 in269 tries found a web server.Random walk techniques attempt to simulate aregular undirected web graph (Henzinger et al,2000).
In such a graph, a random walk would pro-duce a uniform sample of the nodes (i.e.
the webpages).
However, only an approximation of such agraph is possible, as the web is directed (i.e.
youcannot easily determine all web pages linking toa particular page).
Most implementations of ran-dom walks approximates the number of backwardlinks by using information from search engines.3.1 Web SpideringWe created a 10 billion word Web Corpus by spi-dering the web.
While the corpus is not designedto be a representative sample of the web, we at-tempt to sample a topic-diverse collection of websites.
Our web spider is seeded with links from theOpen Directory1.The Open Directory has a broad coverage ofmany topics on the web and allows us to createa topic-diverse collection of pages.
Before the di-rectory can be use, we had to address several cov-erage skews.
Some topics have many more linksin the Open Directory than others, simply dueto the availability of editors for different topics.For example, we found that the topic University ofConnecticut has roughly the same number of linksas Ontario Universities.
We would normally ex-pect universities in a whole province of Canada tohave more coverage than a single university in theUnited States.
The directory was also constructedwithout keeping more general topics higher in thetree.
For example, we found that Chicken Salad ishigher in the hierarchy than Catholicism.
The OpenDirectory is flattened by a rule-based algorithmwhich is designed to take into account the cover-age skews of some topics to produce a list of 358general topics.From the seed URLs, the spider performs abreadth-first search.
It randomly selects a topicnode from the list and next unvisited URL from thenode.
It visits the website associated from the linkand samples pages within the same section of thewebsite until a minimum number of words havebeen collected or all of the pages were visited.1The Open Directory Project, http://www.dmoz.orgExternal links encountered during this process areadded to the link collection of the topic node re-gardless of the actual topic of the link.
Althoughwebsites of one topic tends to link to other web-sites of the same topic, this process contributes toa topic drift.
As the spider traverses away fromthe original seed URLs, we are less certain of thetopic included in the collection.3.2 Text CleaningText cleaning is the term we used to describe theoverall process of converting raw HTML found onthe web into a form useable by NLP algorithms?
white space delimited words, separated into onesentence per line.
It consists of many low-levelprocesses which are often accomplished by sim-ple rule-based scripts.
Our text cleaning process isdivided into four major steps.First, different character encoding of HTMLpages are transform into ISO Latin-1 and HTMLnamed-entities (e.g.
&nbsp; and &amp;) translatedinto their single character equivalents.Second, sentence boundaries are marked.
Suchboundaries are difficult to identify on web text asit does not always consists of grammatical sen-tences.
A section of a web page may be math-ematical equations or lines of C++ code.
Gram-matical sentences need to be separated from eachother and from other non-sentence text.
Sentenceboundary detection for web text is a much harderproblem than newspaper text.We use a machine learning approach to identify-ing sentence boundaries.
We trained a MaximumEntropy classifier following Ratnaparkhi (1998)to disambiguate sentence boundary on web text,training on 153 manually marked web pages.
Sys-tems for newspaper text only use regular text fea-tures, such as words and punctuations.
Our systemfor web text uses HTML tag features in additionto regular text features.
HTML tag features areessential for marking sentence boundaries in webtext, as many boundaries in web text are only indi-cated by HTML tags and not by the text.
Our sys-tem using HTML tag features achieves 95.1% ac-curacy in disambiguating sentence boundaries inweb text compared to 88.9% without using suchfeatures.Third, tokenisation is accomplished using thesed script used for the Penn Treebank project(MacIntyre, 1995), modified to correctly tokeniseURLs, emails, and other web-specific text.235The final step is filtering, where unwanted textis removed from the corpus.
A rule-based com-ponent analyses each web page and each sentencewithin a page to identify sections that are unlikelyto be useful text.
Our rules are similar to thoseemployed by Halacsy et al (2004), where the per-centage of non-dictionary words in a sentence ordocument helps identify non-Hungarian text.
Weclassify tokens into dictionary words, word-liketokens, numbers, punctuation, and other tokens.Sentences or documents with too few dictionarywords or too many numbers, punctuation, or othertokens are discarded.4 Corpus StatisticsComparing the vocabulary of the Web Corpus andexisting corpora is revealing.
We compared withthe Gigaword Corpus, a 2 billion token collection(1.75 billion words before tokenisation) of news-paper text (Graff, 2003).
For example, what typesof tokens appears more frequently on the web thanin newspaper text?
From each corpus, we ran-domly select a 1 billion word sample and classifiedthe tokens into seven disjoint categories:Numeric ?
At least one digit and zero or morepunctuation characters, e.g.
2, 3.14, $5.50Uppercase ?
Only uppercase, e.g.
REUTERSTitle Case ?
An uppercase letter followed by oneor more lowercase letters, e.g.
DilbertLowercase ?
Only lowercase, e.g.
violinAlphanumeric ?
At least one alphabetic and onedigit (allowing for other characters), e.g.
B2B,mp3, RedHat-9Hyphenated Word ?
Alphabetic characters andhyphens, e.g.
serb-dominated, vis-a-visOther ?
Any other tokens4.1 Token Type AnalysisAn analysis by token type shows big differencesbetween the two corpora (see Table 1).
The samesize samples of the Gigaword and the Web Corpushave very different number of token types.
Titlecase tokens is a significant percentage of the tokentypes encountered in both corpora, possibly repre-senting named-entities in the text.
There are also asignificant number of tokens classified as others inthe Web Corpus, possibly representing URLs andemail addresses.
While 2.2 million token types arefound in the 1 billion word sample of the Giga-word, about twice as many (4.8 million) are foundin an equivalent sample of the Web Corpus.Gigaword Web CorpusTokens 1 billion 1 billionToken Types 2.2 million 4.8 millionNumeric 343k 15.6% 374k 7.7%Uppercase 95k 4.3% 241k 5.0%Title Case 645k 29.3% 946k 19.6%Lowercase 263k 12.0% 734k 15.2%Alpha- 165k 7.6% 417k 8.6%numericHyphenated 533k 24.3% 970k 20.1%Other 150k 6.8% 1,146k 23.7%Table 1: Classification of corpus token by typeGigaword Web Corpusrreceive reeceive receieverecceive recesive recivereceieve recieive recveiverecive receivce receivvereceiv receivee recevereceivea receivrceive reyceive1.7 misspellings per 3.7 misspellings perdictionary word dictionary word3.1m misspellings in 5.6m misspellings in699m dict.
words 669m dict.
wordsTable 2: Misspellings of receive4.2 MisspellingOne factor contributing to the larger number of to-ken types in theWeb Corpus, as compared with theGigaword, is the misspelling of words.
Web docu-ments are authored by people with a widely vary-ing command of English and their pages are notas carefully edited as newspaper articles.
Thus,we anticipate a significantly larger number of mis-spellings and typographical errors.We identify some of the misspellings by let-ter combinations that are one transformation awayfrom a correctly spelled word.
Consider a targetword, correctly spelled.
Misspellings can be gen-erated by inserting, deleting, or substituting oneletter, or by reordering any two adjacent letters (al-though we keep the first letter of the original word,as very few misspellings change the first letter).Table 2 shows some of the misspellings of theword receive found in the Gigaword and the WebCorpus.
While only 5 such misspellings werefound in the Gigaword, 16 were found in the Web236Algorithm Training Testing AA WAAUnpruned Brown Brown 94.1 96.4Winnow 80% 20%Unpruned Brown WSJ 89.5 94.5Winnow 80% 40%Winnow Brown WSJ 93.1 96.6Semi-Sup.
80%* 40%Search Altavista Brown 89.3 N/AEngine 100%Table 3: Context-sensitive spelling correction(* denotes also using 60% WSJ, 5% corrupted)Corpus.
For all words found in the Unix dictio-nary, an average of 1.7 misspellings are found perword in the Gigaword by type.
The proportion ofmistakes found in the Web Corpus is roughly dou-ble that of the Gigaword, at 3.7 misspellings perdictionary word.
However, misspellings only rep-resent a small portion of tokens (5.6 million out of699 million instances of dictionary word are mis-spellings in the Web Corpus).5 Context-Sensitive Spelling CorrectionA confusion set is a collection of words whichare commonly misused by even native speakersof a language because of their similarity.
Forexample, the words {it?s, its}, {affect, effect},and {weather, whether} are often mistakenly inter-changed.
Context-sensitive spelling correction isthe task of selecting the correct confusion wordin a given context.
Two different metrics havebeen used to evaluate the performance of context-sensitive spelling correction algorithms.
The Av-erage Accuracy (AA) is the performance by typewhereas the Weighted Average Accuracy (WAA)is the performance by token.5.1 Related WorkGolding and Roth (1999) used the Winnow mul-tiplicative weight-updating algorithm for context-sensitive spelling correction.
They found thatwhen a system is tested on text from a differentfrom the training set the performance drops sub-stantially (see Table 3).
Using the same algorithmand 80% of the Brown Corpus, the WAA droppedfrom 96.4% to 94.5% when tested on 40% WSJinstead of 20% Brown.For cross corpus experiments, Golding andRoth devised a semi-supervised algorithm that istrained on a fixed training set but also extracts in-formation from the same corpus as the testing set.Their experiments showed that even if up to 20%of the testing set is corrupted (using wrong con-fusion words), a system that trained on both thetraining and testing sets outperformed the systemthat only trained on the training set.
The WinnowSemi-Supervised method increases the WAA backup to 96.6%.Lapata and Keller (2005) utilised web countsfrom Altavista for confusion set disambiguation.Their unsupervised method uses collocation fea-tures (one word to the left and right) whereco-occurrence estimates are obtained from webcounts of bigrams.
This method achieves a statedaccuracy of 89.3% AA, similar to the cross corpusexperiment for Unpruned Winnow.5.2 ImplementationContext-sensitive spelling correction is an idealtask for unannotated web data as unmarked textis essentially labelled data for this particular task,as words in a reasonably well-written text are pos-itive examples of the correct usage of confusionwords.To demonstrate the utility of a large collectionof web data on a disambiguation problem, we im-plemented the simple memory-based learner fromBanko and Brill (2001).
The learner trains onsimple collocation features, keeping a count of(wi?1,wi+1), wi?1, and wi+1 for each confusionword wi.
The classifier first chooses the confusionword which appears with the context bigram mostfrequently, followed by the left unigram, right uni-gram, and then the most frequent confusion word.Three data sets were used in the experiments:the 2 billion word Gigaword Corpus, a 2 billionword sample of our 10 billion word Web Corpus,and the full 10 billion word Web Corpus.5.3 ResultsOur experiments compare the results when thethree corpora were trained using the same algo-rithm.
The memory-based learner was tested usingthe 18 confusion word sets from Golding (1995)on the WSJ section of the Penn Treebank and theBrown Corpus.For the WSJ testing set, the 2 billion word WebCorpus does not achieve the performance of theGigaword (see Table 4).
However, the 10 billionword Web Corpus results approach that of the Gi-gaword.
Training on the Gigaword and testing237Training Testing AA WAAGigaword WSJ 93.7 96.12 billion 100%Web Corpus WSJ 92.7 94.12 billion 100%Web Corpus WSJ 93.3 95.110 billion 100%Gigaword Brown 90.7 94.62 billion 100%Web Corpus Brown 90.8 94.82 billion 100%Web Corpus Brown 91.8 95.410 billion 100%Table 4: Memory-based learner resultson WSJ is not considered a true cross-corpus ex-periment, as the two corpora belong to the samegenre of newspaper text.
Compared to the Win-now method, the 10 billion word Web Corpus out-performs the cross corpus experiment but not thesemi-supervised method.For the Brown Corpus testing set, the 2 billionword Web Corpus and the 2 billion word Giga-word achieved similar results.
The 10 billion wordWeb Corpus achieved 95.4% WAA, higher thanthe 94.6% from the 2 billion Gigaword.
This andthe above result with the WSJ suggests that theWeb Corpus approach is comparable with trainingon a corpus of printed text such as the Gigaword.The 91.8% AA of the 10 billion word Web Cor-pus testing on the WSJ is better than the 89.3%AA achieved by Lapata and Keller (2005) us-ing the Altavista search engine.
This suggeststhat a web collected corpus may be a more accu-rate method of estimating n-gram frequencies thanthrough search engine hit counts.6 Thesaurus ExtractionThesaurus extraction is a word similarity task.
It isa natural candidate for using web corpora as mostsystems extract synonyms of a target word from anunlabelled corpus.
Automatic thesaurus extractionis a good alternative to manual construction meth-ods, as such thesauri can be updated more easilyand quickly.
They do not suffer from bias, lowcoverage, and inconsistency that human creatorsof thesauri introduce.Thesauri are useful in many NLP and Informa-tion Retrieval (IR) applications.
Synonyms helpexpand the coverage of system but providing al-ternatives to the inputed search terms.
For n-gramestimation using search engine queries, some NLPapplications can boost the hit count by offering al-ternative combination of terms.
This is especiallyhelpful if the initial hit counts are too low to bereliable.
In IR applications, synonyms of searchterms help identify more relevant documents.6.1 MethodWe use the thesaurus extraction system imple-mented in Curran (2004).
It operates on the dis-tributional hypothesis that similar words appearin similar contexts.
This system only extracts oneword synonyms of nouns (and not multi-word ex-pressions or synonyms of other parts of speech).The extraction process is divided into two parts.First, target nouns and their surrounding contextsare encoded in relation pairs.
Six different typesof relationships are considered:?
Between a noun and a modifying adjective?
Between a noun and a noun modifier?
Between a verb and its subject?
Between a verb and its direct object?
Between a verb and its indirect object?
Between a noun and the head of a modifyingprepositional phraseThe nouns (including subject and objects) are thetarget headwords and the relationships are repre-sented in context vectors.
In the second stage ofthe extraction process, a comparison is made be-tween context vectors of headwords in the corpusto determine the most similar terms.6.2 EvaluationThe evaluation of a list of synonyms of a targetword is subject to human judgement.
We use theevaluation method of Curran (2004), against goldstandard thesauri results.
The gold standard listis created by combining the terms found in fourthesauri: Macquarie, Moby, Oxford and Roget?s.The inverse rank (InvR) metric allows a com-parison to be made between the extracted rank listof synonyms and the unranked gold standard list.For example, if the extracted terms at ranks 3, 5,and 28 are found in the gold standard list, thenInvR = 13 +15 +128?= 0.569.238Corpus INVR INVR MAXGigaword 1.86 5.92Web Corpus 1.81 5.92Table 5: Average INVR for 300 headwordsWord INVR Scores Diff.1 picture 3.322 to 0.568 2.7542 star 2.380 to 0.119 2.2613 program 3.218 to 1.184 2.0344 aristocrat 2.056 to 0.031 2.0255 box 3.194 to 1.265 1.9296 cent 2.389 to 0.503 1.8867 home 2.306 to 0.523 1.783............296 game 1.097 to 2.799 -1.702297 bloke 0.425 to 2.445 -2.020298 point 1.477 to 3.540 -2.063299 walk 0.774 to 3.184 -2.410300 chain 0.224 to 3.139 -2.915Table 6: InvR scores ranked by difference, Giga-word to Web CorpusGigaword (24 matches out of 200)house apartment building run office resident residenceheadquarters victory native place mansion room trip milefamily night hometown town win neighborhood life sub-urb school restaurant hotel store city street season area roadhomer day car shop hospital friend game farm facility cen-ter north child land weekend community loss return hour.
.
.Web Corpus (18 matches out of 200)page loan contact house us owner search finance mortgageoffice map links building faq equity news center estate pri-vacy community info business car site web improvementextention heating rate directory room apartment familyservice rental credit shop life city school property placelocation job online vacation store facility library free .
.
.Table 7: Synonyms for homeGigaword (9 matches out of 200)store retailer supermarket restaurant outlet operator shopshelf owner grocery company hotel manufacturer retailfranchise clerk maker discount business sale superstorebrand clothing food giant shopping firm retailing industrydrugstore distributor supplier bar insurer inc. conglomer-ate network unit apparel boutique mall electronics carrierdivision brokerage toy producer pharmacy airline inc .
.
.Web Corpus (53 matches out of 200)necklace supply bracelet pendant rope belt ring ear-ring gold bead silver pin wire cord reaction clasp jewelrycharm frame bangle strap sterling loop timing plate metalcollar turn hook arm length string retailer repair strandplug diamond wheel industry tube surface neck broochstore molecule ribbon pump choker shaft body .
.
.Table 8: Synonyms for chain6.3 ResultsWe used the same 300 evaluation headwords asCurran (2004) and extracted the top 200 synonymsfor each headword.
The evaluation headwordswere extracted from two corpora for comparison ?a 2 billion word sample of our Web Corpus and the2 billion words in the Gigaword Corpus.
Table 5shows the average InvR scores over the 300 head-words for the two corpora ?
one of web text andthe other newspaper text.
The InvR values differby a negligible 0.05 (out of a maximum of 5.92).6.4 AnalysisHowever on a per word basis one corpus can sigif-icantly outperform the other.
Table 6 ranks the 300headwords by difference in the InvR score.
Whilemuch better results were extracted for words likehome from the Gigaword, much better results wereextracted for words like chain from the Web Cor-pus.Table 7 shows the top 50 synoyms extracted forthe headword home from the Gigaword and theWeb Corpus.
While similar number of correct syn-onyms were extracted from both corpora, the Gi-gaword matches were higher in the extracted listand received a much higher InvR score.
In the listextracted from theWeb Corpus, web-related collo-cations such as home page and search home appear.Table 8 shows the top 50 synoyms extractedfor the headword chain from both corpora.
Whilethere are only a total of 9 matches from the Giga-word Corpus, there are 53 matches from the WebCorpus.
A closer examination shows that the syn-onyms extracted from the Gigaword belong onlyto one sense of the word chain, as in chain stores.The gold standard list and the Web Corpus resultsboth contain the necklace sense of the word chain.The Gigaword results show a skew towards thebusiness sense of the word chain, while the WebCorpus covers both senses of the word.While individual words can achieve better re-sults in either the Gigaword or the Web Corpusthan the other, the aggregate results of synonymextraction for the 300 headwords are the same.
Forthis task, the Web Corpus can replace the Giga-word without affecting the overall result.
How-ever, as some words are perform better under dif-ferent corpora, an aggregate of the Web Corpusand the Gigaword may produce the best result.2397 ConclusionIn this paper, the accuracy of natural language ap-plication training on a 10 billion wordWeb Corpusis compared with other methods using search en-gine hit counts and corpora of printed text.In the context-sensitive spelling correction task,a simple memory-based learner trained on ourWeb Corpus achieved better results than methodbased on search engine queries.
It also rival someof the state-of-the-art systems, exceeding the ac-curacy of the Unpruned Winnow method (the onlyother true cross-corpus experiment).
In the task ofthesaurus extraction, the same overall results areobtained extracting from the Web Corpus as a tra-ditional corpus of printed texts.The Web Corpus contrasts with other NLP ap-proaches that access web data through search en-gine queries.
Although the 10 billion words WebCorpus is smaller than the number of words in-dexed by search engines, better results have beenachieved using the smaller collection.
This is dueto the more accurate n-gram counts in the down-loaded text.
Other NLP tasks that require furtheranalysis of the downloaded text, such a PP attach-ment, may benefit more from the Web Corpus.We have demonstrated that carefully collectedand filtered web corpora can be as useful asnewswire corpora of equivalent sizes.
Using thesame framework describe here, it is possible tocollect a much larger corpus of freely availableweb text than our 10 billion word corpus.
As NLPalgorithms tend to perform better when more datais available, we expect state-of-the-art results formany tasks will come from exploiting web text.AcknowledgementsWe like to thank our anonymous reviewers and theLanguage Technology Research Group at the Uni-versity of Sydney for their comments.
This workhas been supported by the Australian ResearchCouncil under Discovery Project DP0453131.ReferencesMichele Banko and Eric Brill.
2001.
Scaling to very verylarge corpora for natural language disambiguation.
InProceedings of the ACL, pages 26?33, Toulouse, France,9?11 July.Charles L.A. Clarke, Gordon V. Cormack, M. Laszlo,Thomas R. Lynam, and Egidio Terra.
2002.
The im-pact of corpus size on question answering performance.In Proceedings of the ACM SIGIR, pages 369?370, Tam-pere, Finland.James Curran.
2004.
From Distributional to Semantic Simi-larity.
PhD thesis, University of Edinburgh, UK.Andrew R. Golding and Dan Roth.
1999.
A winnow-basedapproach to context-sensitive spelling correction.
Ma-chine Learning, 34(1-3):107?130.Andrew R. Golding.
1995.
A bayesian hybrid method forcontext-sensitive spelling correction.
In Proceedings ofthe Third Workshop on Very Large Corpora, pages 39?53,Somerset, NJ USA.
ACL.David Graff.
2003.
English Gigaword.
Technical ReportLDC2003T05, Linguistic Data Consortium, Philadelphia,PA USA.Gregory Grefenstette.
1999.
The WWW as a resource forexample-based MT tasks.
In the ASLIB Translating andthe Computer Conference, London, UK, October.Peter Halacsy, Andras Kornai, Laszlo Nemeth, Andras Rung,Istvan Szakadat, and Vikto Tron.
2004.
Creating openlanguage resources for Hungarian.
In Proceedings of theLREC, Lisbon, Portugal.M.
R. Henzinger, A. Heydon, M. Mitzenmacher, and M. Na-jork.
2000.
On near-uniform URL sampling.
In Proceed-ings of the 9th International World Wide Web Conference.Frank Keller and Mirella Lapata.
2003.
Using the web to ob-tain frequencies for unseen bigrams.
Computational Lin-guistics, 29(3):459?484.Mirella Lapata and Frank Keller.
2005.
Web-based modelsfor natural language processing.
ACM Transactions onSpeech and Language Processing.Steve Lawrence and C. Lee Giles.
1999.
Accessibility ofinformation on the web.
Nature, 400:107?109, 8 July.Robert MacIntyre.
1995.
Sed script to produce PennTreebank tokenization on arbitrary raw text.
Fromhttp://www.cis.upenn.edu/ treebank/tokenizer.sed.Natalia N. Modjeska, Katja Markert, and Malvina Nissim.2003.
Using the web in machine learning for other-anaphora resolution.
In Proceedings of the EMNLP, pages176?183, Sapporo, Japan, 11?12 July.Preslav Nakov and Marti Hearst.
2005.
A study of usingsearch engine page hits as a proxy for n-gram frequencies.In Recent Advances in Natural Language Processing.Adwait Ratnaparkhi.
1998.
Maximum Entropy Models forNatural Language Ambiguity Resolution.
PhD thesis,University of Pennsylvania, Philadelphia, PA USA.Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.2005.
Randomized algorithms and NLP: Using localitysensitive hash functions for high speed noun clustering.In Proceedings of the ACL, pages 622?629.E.
L. Terra and Charles L.A. Clarke.
2003.
Frequency es-timates for statistical word similarity measures.
In Pro-ceedings of the HLT, Edmonton, Canada, May.Martin Volk.
2001.
Exploiting the WWW as a corpus toresolve PP attachment ambiguities.
In Proceedings of theCorpus Linguistics 2001, Lancaster, UK, March.Martin Volk.
2002.
Using the web as corpus for linguis-tic research.
Ta?hendusepu?u?ja.
Catcher of the Meaning.
AFestschrift for Professor Haldur ?Oim.240
