A FLEXIBLE EXAMPLE-BASED PARSER BASED ON THE SSTC"Mosleh Hmoud A1-Adhaileh & Tang Enya KongComputer Aided Translation UnitSchool of computer sciencesUniversity Sains Malaysia1 1800 PENANG, MALAYSIAmosleh @ cs.
usm.my, enyakong @cs.
usm.
myAbstractIn this paper we sketch an approach for Natural Language parsing.
Our approach is an example-basedapproach, which relies mainly on examples that already parsed to their representation structure, and on theknowledge that we can get from these examples the required information to parse a new input sentence .
In ourapproach, examples are annotated with the Structured String Tree Correspondence (SSTC) annotation schemawhere each SSTC describes a sentence, a representation tree as well as the correspondence b tween substrhzgs inthe sentence and subtrees in the representation tree.
In the process of parsing, we first try to build subtrees forphrases in the input sentence which have been successfully found in the example-base - a bottom up approach.These subtrees will then be combined together to form a single rooted representation tree based on an example withsimilar representation structure - a top down approach.Keywords: Example-based parsing, SSTC.1.
INTRODUCTIONIn natural anguage processing (NLP), one keyproblem is how to design an effective parsing system.Natural language parsing is the process of analyzingor parsing that takes sentences in a natural anguageand converts them to some representation formsuitable for further interpretation towards someapplications might be required, for example,translation, text abstraction, question-answering, etc.The generated representation tree structure can be aphrase structure tree, a dependency tree or a logicalstructure tree, as required by the application involved.Here we design an approach for parsing naturallanguage to its representation structure, whichdepends on related examples already parsed in theexample-base.
This approach is called example-basedparsing, as oppose to the traditional approaches ofnatural language parsing which normally are based onrewriting rules.
Here linguistic knowledge xtracteddirectly from the example-base will be used to parse anatural language sentence (i.e.
using past languageexperiences instead of rules).
For a new sentence, tobuild its analysis (i.e.
representation structure tree),ideally if the sentence is already in the example-base,its analysis is found there too, but in general, theinput sentence will not be found in the example-base.In such case, a method is used to retrieve close relatedexamples and use the knowledge from theseexamples to build the analysis for the input sentence.In general, this approach relies on the assumption thatif two strings (phrase or sentence) are "close", theiranalysis hould be "close" too.
If the analysis of thefirst one is known, the analysis of the other can beobtained by making some modifications in theanalysis of the first one.The example-based approach has become acommon technique for NLP applications, especiallyin MT as reported in \[1\], \[2\] or \[3\].
However, a mainproblem normally arises in the current approacheswhich indirectly limits their applications in thedevelopment of a large scale and practical example-based system.
Namely the lack of flexibility increating the representation tree due to the restrictionthat correspondences between odes (terminal or nonterminal) of the representation tree and words of thesentence must be one-to-one and some even restrict itto only in projective manner according to certaintraversai order.
This restriction ormally results tothe inefficient usage of the example-base.
In thispaper, we shall first discuss on certain cases whereprojective representation trees are inadequate forcharacterizing representation structures of somenatural linguistic phenomena, i.e.
featurisation,lexicalisation and crossed dependencies.
Next, we?
The work reported inthis paper issupported bythe IRPA research programs, under project number 04-02-05-6001 funded by the Ministry ofScience, Technology and Environment, Malaysia.687propose to overcome the problem by introducing aflexible annotation schema called Structured String-Tree Correspondence(SSTC) which describes asentencel a representation tree, and thecorrespondence between substrings in the sentenceand subtrees in the representation tree.
Finally, wepresent a algorithm to parse natural languagesentences based on the SSTC annotation schema.2.
NON-PROJECT IVE  CORRESPONDE-NCES IN  NATURAL LANGUAGESENTENCESIn this section, we shall present some caseswhere projective representation tree is found to beinadequate for characterizing representation tree ofsome natural language sentences.
The casesillustrated here are featurisation, lexicalisation andcrossed dependencies.
An example containingmixture of these non-projective correspondences al owill be presented.2.1 Featur i sa t ionFeaturisation occurs when a linguist decides that aparticular substring in the sentence, should not berepresented as a subtree in the representation tree butperhaps as a collection of features.
For example, asillustrated in figure 1, this would be the case forprepositions in arguments which can be interpreted aspart of the predicate and not the argument, and shouldbe featurised into the predicate (e.g.
"up" in "picks-up"), the particle "up" is featurised as a part of thefeature properties of the verb "pick".picks upHe picks up the ballFigure 1: Featurisation2.2 Lex ica l i sa t ionLexicalisation is the case when a particularsubtree in the representation tree presents themeaning of some part of the string, which is notorally realized in phonological form.
Lexicalisationmay result from the correspondence of a subtree inthe tree to an empty substring in the sentence, orsubstring in the sentence to more than one subtree inthe tree.
Figure 2 illustrates the sentence "John eatsthe apple and Mary the pear" where "eats" in thesentence corresponds to more than one node in thetree.andea_ .
/ "oO~~eatsJohn eats the apple and Mary tile pearFigure 2: Lexicalisation2.3 Crossed  dependenc iesThe most complicated case of string-treecorrespondence is when dependencies are intertwinedwith each other.
It is a very common phenomenon inatural anguage.
In crossed dependencies, subtree inthe tree corresponds to single substring in thesentence, but the words in a substring are distributedover the whole sentence in a discontinuous manner,in relation to the subtree they correspond to.
Anexample of crossed dependencies i  occurred in theb n c n sentences of the form (a n v I n>0), figure 3illustrates the representation tree for the string "aa vbb cc " (also written a.la.2 v b.lb.2 c.lc.2 to showthe positions), this akin to the 'respectively' problemin English sentence like "John and Mary give Pauland Ann trousers and dresses respectively" \[4\].va.1 b.1 \[ c.1 __v  1'4?Figure 3: Crossed dependenciesSometimes the sentence contains mixture of thesenon-projective correspondences, figure 4 illustratesthe sentence "He picks the ball up", which containsboth featurisation and crossed dependencies.
Here,the particle "up" is separated from its verb "picks" bya noun phrase "the ball" in the string.
And "up" isfeaturised into the verb "picks" (e.g.
"up" in "picks-up").picl/pick:s upFigure 4: Mixture of featurisationand crossed ependencies6883.
STRUCTURED STRING-TREECORRESPONDENCE (SSTC)The correspondence b tween the string on onehand, and its representation f meaning on the otherhand, is defined in terms of finer subcorrespondencesbetween substrings of the sentence and subtrees of thetree.
Such correspondence is made of two interrelatedcorrespondences, one between nodes and substrings,and the other between subtrees and substrings, (thesubstrings being possibly discontinuous in bothcases).The notation used in SSTC to denote acorrespondence onsists of a pair of intervals X/Yattached to each node in the tree, where X(SNODE)denotes the interval containing the substring thatcorresponds to the node, and Y(STREE) denotes theinterval containing the substring that corresponds tothe subtree having the node as root \[4\].Figure 5 illustrates the sentence "all cats eatmice" with its corresponding SSTC.
It is a simpleprojective correspondence.
An interval is assigned toeach word in the sentence, i.e.
(0-1) for "all", (1-2)for "cats", (2-3) for "eat" and (3-4) for "mice".
Asubstring in the sentence that corresponds toa node inthe representation tree is denoted by assigning theinterval of the substring to SNODE of the node, e.g.the node "cats" with SNODE interval (1-2)corresponds to the word "cats" in the string with thesimilar interval.
The correspondence betweensubtrees and substrings are denoted by the intervalassigned to the STREE of each node e.g.
the subtreerooted at node "eat" with STREE interval (0-4)corresponds to the whole sentence "all cats eat mice".Tree eat(2-3/0-4)3.4,3.4,all(0-1/0-1)~ tString all cats eat mice(0-1) (1-2) (2-3) (3-4)Figure 5: An SSTC recording the sentence "all catseat mice" and its Dependency tree together with thecorrespondences between substrings of the sentenceand subtrees of the tree.4.
USES OF  SSTC ANNOTATION INEXAMPLE-BASED PARSINGIn order to enhance the quality of example-based systems, sentences in the example-base arenormally annotated with theirs constituency ordependency structures which in turn allow example-based parsing to be established at the structurallevel.
To facilitate such structural annotation, herewe annotate the examples based on the StructuredString-Tree Correspondence (SSTC).
The SSTC is ageneral structure that can associate, to string in alanguage, arbitrary tree structure as desired by theannotator to be the interpretation structure of thestring, and more importantly is the facility to specifythe correspondence between the string and theassociated tree which can be interpreted for bothanalysis and synthesis in NLP.
These features arevery much desired in the design of an annotationscheme, in particular for the treatment of linguisticphenomena which are not-standard e.g.
crosseddependencies \[5\].Since the example in the example-base aredescribed in terms of SSTC, which consists of asentence (the text), a dependency tree' (the linguisticrepresentation) and the mapping between the two(correspondence); example-based parsing isperformed by giving a new input sentence, followedby getting the related examples(i.e, examples thatcontains ame words in the input sentence) from theexample-base, and used them to compute therepresentation tree for the input sentence guided bythe correspondence b tween the string and the treeas discussed in the following sections.
Figure 6illustrates the general schema for example-based NLparsing based on the SSTC schema.sentenceInputExample.
Iibased / \Parsing OutputFigure 6: Example-based natural language parsing based onthe SSTC schema.4.
1 The parsing algorithmThe example-based approach in MT \[1\], \[2\] or\[3\], relies on the assumption that if two sentencesare "close", their analysis should be "close" too.
Ifthe analysis of the first one is known, the analysis ofthe other can be obtained by making somemodifications in the analysis of the first one (i.e.i Each node is tagged with syntactic category to enablesubstitution at category level.689close: distance not too large, modification: editoperations (insert, delete, replace) \[6\].In most of the cases, similar sentence might notoccurred in the example-base, so the system utilizedsome close related examples to the given inputsentence (i.e.
similar structure to the input sentence orcontain some words in the input sentence).
For that itis necessary to construct several subSSTCs (calledsubstitutions hereafter) for phrases in the inputsentence according to their occurrence in theexamples from the example-base.
These substitutionsare then combined together to form a complete SSTCas the output.Suppose the system intends to parse the sentence" the old man picks the green lamp up", dependingon the following set of examples representing theexample-base.picks{v\] uplp\](1-2+4-5/0-5)He\[hi ball{n\](0-1/0-1) (3-4/2-4)Ithe\[detl(2-3/2-3)He picks the ball up0-1 1-2 2-3 3-4 4-5(1)tums\[v\](3-4/0-5)signal{n\] on\[adv\](2-3/0-3) (4-5/4-5)/ ~theldet\] green\[adj\](0-1/0-1) (1-2/1-2)The green signal turns on0-1 I-2 2-3 3-4 4-5(2)is{v\](2-3/0-4)lamp\[nl off\[adv\](1-2/0-2) (3-4/3-4)Itheldetl(0-1/0-1)The lamp is off0-1 I-2 2-3 3-4died{v\](3-4/0-4)mJn\[n\] (2-3/0-3)the\[det\] old\[adj\](0-1/0-1) (1-2/1-2)The old man died0-1 1-2 2-3 3-4(3) (4)The example-base is first processed to retrievesome knowledge related to each word in the example-base to form a knowledge index.
Figure 7 shows theknowledge index constructed based on the example-base given above.
The knowledge retrieved for eachword consists of:1.
Example number:  The example number of one ofthe examples which containing this word with thisknowledge.
Note that each example in the example-base is assigned with a number as its identifier.2.
Frequency: The frequency of occurrence in theexample-base for this word with the similarknowledge.3.
Category: Syntactic ategory of this word.4.
Type: Type of this word in the dependency tree (0:terminal, l: non-terminal).- Terminal word: The word which is at thebottom level of the tree structure, namely theword without any son/s under it (i.e.STREE=SNODE in SSTC annotation).- Non terminal word: The word which islinked to other word/s at the lower level,namely the word that has son/s (i.e.STREE~:SNODE in SSTC annotation).5.
Status: Status of this word in the dependency tree(0: root word, 1 : non-root word, 2: friend word)- Friend word: In case of featurisation, if aword is featurised into other word, thisword is called friend for that word, e.g.
theword "up" is a friend for the word "picks"in figure 1.6.
Parent category: Syntactic ategory of the parentnode of this word in the dependency tree.7.
Position: The position of the parent node in thesentence (0: after this word, 1 : before this word).8.
Next knowledge: A pointer pointing to the nextpossible knowledge of this word.
Note that a wordmight have more than one knowledge, e.g.
"man"could be a verb or a noun.Based on the constructed knowledge index in figure7, the system built the following table of knowledgefor the input sentence:The input sentence: the old man picks the green0-1 1-2 2-3 3-4 4-5 5-6the 0 1 1old 1 2 4man 2 3 4picks 3 4 1the 4 5 1green 5 6 2lamp 6 7 3up 7 8 14 det 0 1 nl ad j0  1 n1 n 1 1 v1 v 1 04 det 0 1 nl ad j0  1 v1 n 1 i v1 p l 2 vlamp up6-7 7-80 nil0 nil0 nilnil0 nil0 nil0 nil1 nilNote that to each word in the input sentence, thesystem built a record which contain the word,SNODE interval, and a linked list of possibleknowledge related to the word as recorded in theknowledge index.
The following figure describes anexample record for the word <the>:This mean:the word <the>, snode(0-1), one of the examplesthat contain the word with this knowledge isexample l, this knowledge r peated 4 time in theexample-base, the category of the word is <det>,it is a terminal node, non-root node, the parentcategory is <n>, and the parent appear after it inthe sentence.690=glExample No.
Ifrequeneylcategory It pe Is~tus IParent categorylPosition INextKn.
\ [the ~ I 4 det 0 I n 0 nil.old - ~ 4 1 adj 0 I n 0 nil.he - ~ I I n 0 I v 0 nil.turns - ~ 2 1 v I 0 nil.ball - ~ I I n 1 I v I nil.green - ~ 2 I adj 0 1 n 0 nil.signal - ~ 2 I n I I v 0 nil.on - ~ 2 1 adv 0 I v 1 nil.ticks - ~ I 1 v I 0 nil.off - ~ 3 1 adv 0 1 v 1 nil.man - ~ 4 I n 1 1 v 0 nil.died - ~ 4 I v I 0 nil.lamp - ~ 3 I n 1 I v 0 nil.up - ~ 1 1 p I 2 v 1 nil.Figure 7: The knowledge index for the words in the example-base.This knowledge will be used to build thesubstitutionsfor the input sentence, as we will discussin the next section.4.1.1 Substitutions generationIn order to build substitutions, the system firstclassifies the words in the input sentence intoterminal words and non-terminal words.
For eachterminal word, the system tries to identify the non-terminal word it may be connected to based on thesyntactic category and the position of the non-terminal word in the input sentence (i.e.
before orafter the terminal word) guided by SNODE interval.In the input sentence given above, the terminalwords are "the", "old" and "green" and based on theknowledge table for the words in the input sentence,they may be connected as son node to the first non-terminal with category \[n\] which appear after them inthe input sentence.For ( "the" 0-1, and "old" 1-2 ) they are connected assons to the word ("man" 2-3).nowledge I\] Non-terminal Iable II wordStn\] IFor ("the" 4-5, and "green" 5-6 ) they are connectedas sons to the word ("lamp" 6-7).~ n o w l e d g e  I I Non- termina l  II - ,~" - " ,pv - - - lamp\ [n \ ]I 'he' ~-~ SU~ebnStl_~ertaUttio??
I~  .
.
.
.I green ~"  ~ generatorThe remainder non-terminal words, which are notconnected to any terminal word, will be treated asseparate substitutions.From the input sentence the system builds thefollowing substitutions respectively :man\ [n \ ]  p icks \ [v \ ]  lamp\ [n \ ]  up\ [p \ ](2 -3/0-3)  (3-4/0-8)  (6-7/4-7)  (7 -8 / - )theldet\]  o ld\ [adj \ ]  the\[de(\]  g reen\ [ad j \ ](0-1/0-1)  (1-2/1-2)  (4-5/4-5~ (5-6/5-6)(1) (2) (3) (4)Note that this approach is quite similar to thegeneration of constituents in bottom-up chart parsingexcept that the problem of handling multipleoverlapping constituents is not addressed here.4.1.2 Subst i tu t ions  combinationIn order to combine the substitutions to form acomplete SSTC, the system first finds non-terminalwords of input sentence, which appear as root wordof some dependency trees in the example SSTCs.
Ifmore than one example are found (in most cases), thesystem will calculate the distance between the inputsentence and the examples, and the closest example691(namely one with minimum distance) will be chosento proceed further.In our example, the word "picks" is the onlyword in the sentence which can be the root word, soexample (1) which containing "pick" as root will beused as the base to construct he output SSTC.
Thesystem first generates the substitutions for example(1) based on the same assumptions mentioned earlierin substitutions generation, which are :heln\] Picks\[v\] ball\[n\] uplPl(0-1/0-1) (1-2/0-5) (3-4~2-4) (4-5/-)Ithe\[det\](2-3/2-3)(1) (2) (3) (4)Distance calculation:Here the system utilizes distance calculation todetermine the plausible example, which SSTCstructure will be used as a base to combine thesubstitutions at the input sentence.
We define aheuristic to calculate the distance, in terms of editingoperations.
Editing operations are insert (E --> p),deletion (p- - )E)  and replacing (a "-) s).
Editiondistances, which have been proposed in many works\[7\], \[8\] and \[9\], reflect a sensible notion, and it can berepresented as metrics under some hypotheses.
Theydefined the edition distances as number of editingoperations to transfer one word to another form, i.e.how many characters needed to be edited based oninsertion, deletion or replacement.
Since words arestrings of characters, entences are strings of words,editing distances hence are not confined to words,they may be used on sentences \[6\].With the similar idea, we define the editiondistance as: (i) The distance is calculated at level ofsubstitutions (i.e.
only the root nodes of thesubstitutions will be considered, not all the words inthe sentences).
(ii) The edit operations are done basedon the syntactic ategory of the root nodes, (i.e.
thecomparison between the input sentence and anexample is based on the syntactic ategory of the rootnodes of their substitutions, not based on the words).The distance is calculated based on the number ofediting operations (deletions and insertion) needed totransfer the input sentence substitutions to theexample substitutions, by assigning weight to each ofthese operations: 1to insertion and 1 to deletion.e.g.
:a) S 1: The old man eats an apple.$2: He eats a sweet cake.man \[n\] eats \[v\] f '  aplle in)the~\ [ad j \ ]  ea~~ ~an \[det\]He In\] Iv\] cake ln\]a ldet\] sweet \[adj\]In (a), the distance between S1 and $2 is 0.b)He (nlboy\[nlIThe \[detlS 1: He eats an apple in the garden.$2: The boy who drinks tea eats the cake.eats \[v\] ~ ~  garden \[n\]who~\[~l\] d r i ~ : : ~ ~ ~ l n \ ]Ithe \[det\]In (b), the distance between S1 and $2 is(3+2)=5.Note that when a substitution is decided to bedeleted from the example, all the words of the relatedsubstitutions (i.e.
the root of the substitutions and allother words that may link to it as brothers, or son/s),are deleted too.
This series is determined by referringto an example containing this substitution in theexample-base.
For example in (b) above, thesubstitution rooted with "who" must be deleted, hencesubstitutions "drinks" and "tea" must be deleted too,similarly "in" must be deleted hence "garden" must bedeleted too.Before making the replacement, the system mustfirst check that the root nodes categories forsubstitutions in both the example and the inputsentence are the same, and that these substitutions areoccurred in the same order (i.e.
the distance is 0).
Ifthere exist additional substitutions in the inputsentence (i.e.
the distance ~: 0), the system will eithercombine more than one substitution into a singlesubstitution based on the knowledge index beforereplacement is carried out or treat it as optionalsubstitution which will be added as additional subtreeunder the root.
On the other hand, additionalsubstitutions appear in the example will be treated asoptional substitutions and hence can be removed.Additional substitutions are determined duringdistance calculation.Replacement:Next the substitutions in example (1) will be replacedby the corresponding substitutions generated from theinput sentence to form a final SSTC.
The replacement692process is done by traversing the SSTC tree structurefor the example in preorder traversal, and eachsubstitution in the tree structure replaced with itscorresponding substitution i  the input sentence.
Thisapproach is analogous to top down parsing technique.Figure 8, illustrates the parsing schema for the inputsentence " The old malt picks the green lamp up".Input sentenceThe old man picks the green lamp upsubstitutions Ii m I(I) ~theldeq oldladj\]( 2 ) ~I the\[det\] greenladjl\[(4)k~ ~p.-pickslvl up \[Pl(1-2+4-5/0-5) / \He \[hi balllnl(0-1/0-1) (3-4/2-4)Itheldetl(2-3/2-3)He picks the ball up0-1 1-2 2-3 3-4 4-5SSTC base \[ i;istructure ~,,,~ ......?
I I .
JRep lacement  \]l ~-qISSTC examplesubstitutionsI,t l ,olnl I - ( 2 ) ~I uptp)I c4) I !Output SSTC ~,structurepicks\[v\] uplp\]man\[n\](2-3/0-3) lamp\[n\](6-7/4-7) / \  / \the\[det\] oldladj\] the\[det\] green\[adj\](O-I/0-l) (1-2/1-2) (4-5/4-5) (5-6/5-6)The old man picks the green lamp up0-1 I-2 2-3 3-4 4-5 5-6 6-7 7-8I .
.
.
.
.
.
.
.
.
.
.
.
.
.Figure 8: The parsing schema based on the SSTC for thesentence "the old man picks the green lamp up" usingexample ( 1 ).5.
CONCLUSIONIn this paper, we sketch an approach for parsingNL string, which is an example-based approachrelies on the examples that already parsed to theirrepresentation structures, and on the knowledge thatwe can get from these examples information eededto parse the input sentence.A flexible annotation schema called StructuredString-Tree Correspondence (SSTC) is introduced toexpress linguistic phenomena such as featurisation,lexicalisation and crossed dependencies.
We alsopresent an overview of the algorithm to parse naturallanguage sentences based on the SSTC annotationschema.
However, to obtain a full version of theparsing algorithm, there are several other problemswhich needed to be considered further, i.e.
thehandling of multiple substitutions, an efficientmethod to calculate the distance between the inputsentence and the examples, and lastly a detailedformula to compute the resultant SSTC obtained fromthe combination process especially when deletion ofoptional substitutions are involved.Re ferences :\[1\] M.Nagao, "A Framework of a mechanicaltranslation between Japanese and English by analogyprinciple", in; A. Elithorn, R. Benerji, (Eds.
),Artificial and Human Intelligence, Elsevier:Amsterdam.\[2\] V.Sadler & Vendelmans, "Pilot implementation fa bilingual knowledge bank", Proc.
of Coling-90,Helsinki, 3, 1990, 449-451.\[3\] S. Sato & M.Nagao, "Example-based Translationof technical Terms", Proc.
of TMI-93, Koyoto, 1993,58-68.\[4\] Y. Zaharin & C. Boitet, "Representation trees andstring-tree correspondences", Proc.
of Coling-88,Budapest, 1988, 59-64.\[5\] E. K. Tang & Y. Zaharin, "Handling CrossedDependencies with the STCG", Proc.
of NLPRS'95,Seoul, 1995,\[6\] Y.Lepage & A.Shin-ichi, "Saussurian analogy: atheoritical account and its application", Proc.
ofColing-96, Copenhagen, 2, 1996, 717-722.\[7\] V. I. Levenshtein, "Binary codes capable ofcorrecting deletions, insertions and reversals", Dokl.Akad.
Nauk SSSR, 163, No.
4, 1965, 845-848.English translation hz Soviet Physics-doklady, 10,No.
8, 1966, 707-710.\[8\] Robert A. Wagner & Michael J. Fischer, " TheString-to String Correction Problem", Journal for theAssociation of Computing Machinery, 21, No.
1,1974, 168-173.\[9\] Stanley M. Selkow, "The Tree-to-Tree EditingProblem", Information Processing Letters, 6, No.
6,1977, 184-186.693
