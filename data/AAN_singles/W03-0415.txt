Using LSA and Noun Coordination Information to Improve the Precisionand Recall of Automatic Hyponymy ExtractionScott Cederberg Dominic WiddowsCenter for the Study of Language and Information210 Panama StreetStanford UniversityStanford CA 94305{cederber,dwiddows}@csli.stanford.eduAbstractIn this paper we demonstrate methods of im-proving both the recall and the precision of au-tomatic methods for extraction of hyponymy(IS A) relations from free text.
By applying la-tent semantic analysis (LSA) to filter extractedhyponymy relations we reduce the rate of er-ror of our initial pattern-based hyponymy ex-traction by 30%, achieving precision of 58%.Applying a graph-based model of noun-nounsimilarity learned automatically from coordi-nation patterns to previously extracted correcthyponymy relations, we achieve roughly a five-fold increase in the number of correct hy-ponymy relations extracted.1 IntroductionThis paper demonstrates that mathematical models formeasuring semantic similarity between concepts can beused to improve the learning of hyponymy relationshipsbetween concepts from free text.
In particular, we showthat latent semantic analysis can be used to filter results,giving an increase in precision, and that neighbors in agraph built from coordination information can be used toimprove recall.The goal of extracting semantic information from textis well-established, and has encouraged work on lexicalacquisition (Roark and Charniak, 1998), information ex-traction (Cardie, 1997), and ontology engineering (Hahnand Schnattinger, 1998).
The purpose of this kind ofwork is to collect information about the meanings of lexi-cal items or phrases, and the relationships between them,so that the process of building semantic resources (suchas ontologies and dictionaries) by hand can be automatedor at least helped.One of the standard ways of arranging concepts is in aconcept hierarchy or taxonomy such as the WordNet nountaxonomy (Fellbaum, 1998).
The fundamental relation-ship between objects in a taxonomy is called hyponymy,where y is a hyponym of x if every y is also an x. Forexample, every trout is also a fish, so we say that troutis a hyponym (?below name?)
of fish and conversely, fishis a hypernym (?above name?)
of trout.
Other names ex-ist for variants of the hyponymy relationship, such as anIS A relationship, a parent-node / child-node relationship,and a broader term / narrower term relationship.
It is alsonoted that the genus of an object, in traditional lexico-graphic terms, is often a hypernym of that object (Guthrieet al, 1996).
Throughout this paper we will write y < xfor the relationship ?y is a hyponym of x?.
In this paper,we use the hyponymy relationship to describe subset re-lationships, so we regard y < x to be true if the set of y?scan reasonably be said to be a subset of the set of x?s.1Because hyponymy relationships are so central toknowledge engineering, there have been numerous at-tempts to learn them from text, beginning with thoseof Hearst (1992).
We review this work in Section 2,where we reproduce similar experiments as a baselinefrom which to expand.
The rest of the paper demon-strates ways in which other mathematical models builtfrom text corpora can be used to improve hyponymy ex-traction.
In Section 3, we show how latent semantic anal-ysis can be used to filter potential relationships accord-ing to their ?semantic plausibility?.
In Section 4, weshow how correctly extracted relationships can be usedas ?seed-cases?
to extract several more relationships, thusimproving recall; this work shares some similarities withthat of Caraballo (1999).
In Section 5 we show that com-bining the techniques of Section 3 and Section 4 improvesboth precision and recall.
Section 6 demonstrates that1Another possible view is that ?hyponymy?
should only re-fer to core relationships, not contingent ones (so pheasant <bird might be accepted but pheasant < food might not be, be-cause it depends on context and culture).
We use the broader?subset?
definition because contingent relationships are an im-portant part of world-knowledge (and are therefore worth learn-ing), and because in practice we found the distinction difficult toenforce.
Another definition is given by Caraballo (1999): ?.
.
.
aword A is said to be a hypernym of a word B if native speakersof English accept the sentence ?B is a (kind of) A.?
?linguistic tools such as lemmatization can be used to re-liably put the extracted relationships into a normalized or?canonical?
form for addition to a semantic resource.2 Pattern-Based Hyponymy ExtractionThe first major attempt to extract hyponyms from textwas that of Hearst (1992), described in more detail in(Hearst, 1998), who extracted relationships from the textof Grolier?s Encyclopedia.
The method is illustrated bythe following example.
The sentence excerptEven then, we would trail behind other Euro-pean Community members, such as Germany,France and Italy.
.
.
(BNC)2indicates that Germany, France, and Italy are all Euro-pean Community members.
More generally, phrases ofthe formx such as y1 (y2, .
.
.
, and/or yn)frequently indicate that the yi are all hyponyms ofthe hypernym x. Hearst identifies several other con-structions that have a tendency to indicate hyponymy,calling these constructions lexicosyntactic patterns, andanalyses the results.
She reports that 52% of the re-lations extracted by the ?or other?
pattern (see Ta-ble 1) were judged to be ?pretty good relations?.
Amore recent variant of this technique was implementedby Alfonseca and Manandhar (2001), who compare thecollocational patterns of words from The Lord of theRings with those of words in the WordNet taxonomy,adding new nouns to WordNet with an accuracy of28%.
Using a much more knowledge-intensive approach,Hahn and Schnattinger (1998) improve ?learning accu-racy?
from around 50% to over 80% by forming a numberof hypotheses and accepting only those which are mostconsistent with their current ontology.
Their methods arelike ours in that the ?concept learning?
combines infor-mation from several occurrences, but differ in that theyrely on a detailed existing ontology into which to fit thenew relationships between concepts.Our initial experiment was to construct a hyponymyextraction system based on the six lexicosyntactic pat-terns identified in (Hearst, 1998), which are listed in Ta-ble 1.
We first used a chunker to mark noun groups, andthen recognized and extracted noun groups occurring aspart of one of the extraction patterns.3We applied these extraction patterns to an approxi-mately 430,000-word extract from the beginning of the2This excerpt and others in this paper are from the BritishNational Corpus.3The chunker used was LT CHUNK, fromthe University of Edinburgh?s Language Tech-nology Group.
It can be downloaded fromhttp://www.ltg.ed.ac.uk/software/chunk/.x such as y1 (, y2, .
.
.
, and/or yn)such x as y1 (, y2, .
.
.
, and/or yn)y1 (, y2, .
.
.
, yn,) or other xy1 (, y2, .
.
.
, yn,) and other xx, including y1 (, y2, .
.
.
, and/or yn)x, especially y1 (, y2, .
.
.
, and/or yn)Table 1: The lexicosyntactic patterns described byHearst (1998), which we used in the work described inthis paper.
Each of these patterns is taken to indicate thehyponymy relation(s) yi < x.British National Corpus (BNC).
The patterns extracted513 relations.
We selected 100 of the extracted relationsat random and each author evaluated them by hand, scor-ing each relation on a scale from 4 (correct) to 0 (incor-rect), defined as follows:4.
Extracted hypernym and hyponym exactly correct asextracted.3.
Extracted hypernym and hyponym are correct aftera slight modification, such as depluralization or theremoval of an article (e.g.
a, the) or other precedingword.2.
Extracted hypernym and hyponym have somethingcorrect, e.g.
a correct noun without a necessaryprepositional phrase, a correct noun with a superflu-ous prepositional phrase, or a noun + prepositionalphrase where the object of the preposition is correctbut the preposition itself and the noun to which itattaches are superfluous.
Thus these hyponymy re-lations are potentially correct but will require poten-tially difficult processing to extract an exactly cor-rect relation.
Some of the errors which would needto be corrected were in preprocessing (e.g.
on thepart of the noun-group chunker) and others were er-rors caused by our hyponymy extractor (e.g.
tackingon too many or too few prepositional phrases).1.
The relation extracted is correct in some sense, butis too general or too context specific to be useful.This category includes relations that could be madeuseful by anaphora resolution (e.g.
replacing ?this?with its referent).0.
The relation extracted is incorrect.
This results whenthe constructions we recognize are used for a pur-pose other than indicating the hyponymy relation.The results of each of the authors?
evaluations of the100-relation random sample are show in Table 2.4 For4Table 2 suggests that although there is significant disagree-ment about how to assign scores of 1 and 0, inter-annotatorscore Author 1 Author 24 4 23 34 352 14 131 35 220 13 28Table 2: Number of the 100 randomly selected hyponymyrelations (of 513 extracted) to which each of the authorsassigned the five available scores.purposes of calculating precision, we consider those rela-tions with a score of 4 or 3 to be correct and those with alower score to be incorrect.
After discussion between theauthors on disputed annotations to create ?gold standard?annotations, we found that 40 of the 100 relations in ourrandom sample were correct according to this criterion.In other words, 40% of the relations extracted were ex-actly correct or would be correct with the use of minorpost-processing consisting of lemmatization and removalof common types of qualifying words.
(We describe ourapplication of such post-processing in Section 6.
)Thus our initial implementation of Hearst-style hy-ponymy extraction achieved 40% precision.
This is lessthan the 52% precision reported in (Hearst, 1998).
Webelieve this discrepancy to be mainly due to the dif-ference between working with the BNC and Grolier?sencyclopedia?as noted by Hearst, the encyclopedia isdesigned to be especially rich in conceptual relationshipspresented in an accessible format.Various problems with the pattern-based extractionmethod explain the 60% of extracted relations that wereincorrect and/or useless.
One problem is that the con-structions that we assume to indicate hyponymy are oftenused for other purposes.
For instance, the patternx including y1, y2, .
.
.
, and ynwhich indicates hyponymy in sentences such asIllnesses, including chronic muscle debility,herpes, tremors and eye infections, have comeand gone.
(BNC)and is a quite productive source of hyponymy relations,can be used instead to indicate group membership:agreement regarding the assignment of scores of 4, 3, and 2 isquite high.
Indeed, considering the rougher distinction we usefor reporting precision, in which scores of 4 and 3 are deemedcorrect and scores of 2, 1, and 0 are deemed incorrect, we foundthat inter-annotator agreement across all relations annotated (in-cluding those from this random sample and those from the sam-ple described in Section 3) was 86%.
We discussed each ofthe relations in the 14% of cases where we disagreed until wereached agreement; this produced the ?gold standard?
annota-tions to which we refer.Often entire families including young childrenneed practical home care .
.
.
(BNC)While all children are members of families, the hy-ponymy relationship child < family does not hold, sinceit is not true that all children are families.Another source of errors in lexicosyntactic hyponymyextraction is illustrated by the sentenceA kit such as Edme Best Bitter, Tom CaxtonBest Bitter, or John Bull Best Bitter will be agood starting kit.
(BNC)which indicates the (potentially useful) relations EdmeBest Bitter < beer-brewing kit, Tom Caxton Best Bitter< beer-brewing kit, and John Bull Best Bitter < beer-brewing kit, but only when we use the context to inferthat the type of ?kit?
referred to is a beer-brewing kit, aprocess that is difficult by automatic means.
Without thisinference, the extracted relations Edme Best Bitter < kit,etc., while correct in a certain sense, are not helpful.
Onefrequent source of such problems is anaphora that requireresolution.There are also problems related to prepositional phraseattachment.3 Improving Precision Using LatentSemantic AnalysisSolving all of the problems with pattern-based hyponymyextraction that we describe above would require near-human-level language understanding, but we have ap-plied a far simpler technique for filtering out many of theincorrect and spurious extracted relations with good re-sults, using a variant of latent semantic analysis (LSA)(Deerwester et al, 1990; Baeza-Yates and Ribiero-Neto,1999, p. 44).
LSA is a method for representing wordsas points in a vector space, whereby words which are re-lated in meaning should be represented by points whichare near to one another.
The LSA model we built is sim-ilar to that described in (Schu?tze, 1998).
First 1000 fre-quent content words (i.e.
not on the stoplist)5 were chosenas ?content-bearing words?.
Using these content-bearingwords as column labels, the other words in the corpuswere assigned row vectors by counting the number oftimes they occured within a 15-word context window ofa content-bearing word.
Singular-value decomposition(Deerwester et al, 1990) was then used to reduce thenumber of dimensions from 1000 to 100.
Similarity be-tween two vectors (points) was measured using the cosineof the angle between them, in the same way as the simi-larity between a query and a document is often measured5A ?stoplist?
is a list of frequent words which have littlesemantic content in themselves, such as prepositions and pro-nouns (Baeza-Yates and Ribiero-Neto, 1999, p. 167).score Author 1 Author 24 4 53 57 522 18 141 12 190 9 10Table 3: Number of the 100 top-ranked hyponymy re-lations (of 513 extracted) to which each of the authorsassigned the five available scores.in information retrieval (Baeza-Yates and Ribiero-Neto,1999, p. 28).
Effectively, we could use LSA to measurethe extent to which two words x and y usually occur insimilar contexts.
This LSA similarity score will be calledsim(x, y).Since we expect a hyponym and its hypernym to besemantically similar, we can use the LSA similarity be-tween two terms as a test of the plausibility of a putativehyponymy relation between those terms.
If their similar-ity is low, it is likely that they do not have a true and use-ful hyponymy relationship; the relation was probably ex-tracted erroneously for one or more of the reasons listedabove.
If the similarity between two terms is high, wehave increased confidence that a hyponymy relationshipexists between them, because we know that they are atleast in similar ?semantic regions?.We ranked the 513 putative hyponym/hypernym pairsthat we extracted from our trial excerpt of the BNC ac-cording to the similarity between the putative hypernymand the putative hyponym in each pair; i.e.
for each pairx and y where the relationship y < x had been suggested,we calculated the cosine similarity sim(x, y), then weranked the extracted relations from highest to lowest sim-ilarity.
We then manually evaluated the accuracy of thetop 100 extracted relations according to this ranking us-ing the 5-point scale described in Section 2.
We foundthat 58 of these 100 top-ranked relations received scoresof 4 or 3 according to our ?gold standard?
annotations.Comparing this 58% precision with the 40% precisionobtained on a random sample in Section 2, we determinethat LSA achieved a 30% reduction in error (see Table 3for a breakdown of annotation results by author).6Thus LSA proved quite an effective filter.
LSA pro-vides broad-based semantic information learned statis-tically over many occurences of words; lexicosyntactichyponymy extraction learns semantic information fromspecific phrases within a corpus.
Thus we have bene-fitted from combining local patterns with statistical in-6It should be noted that 24 of the top 100 hyponymy rela-tions evaluated in this section were also in the randomly-chosensample of 100 relations described in Section 2.
Thus there werea total of 176 distinct hyponymy relations across both test sets.formation.
Considered in analogy with the process bywhich humans learn from reading, we might think ofthe semantic information learned by LSA as backgroundknowledge that is applied by the reader when determiningwhat can accurately be gleaned from a particular sentencewhen it is read.4 Improving Recall Using CoordinationInformationOne of the main challenges facing hyponymy extractionis that comparatively few of the correct relations thatmight be found in text are expressed overtly by the simplelexicosyntactic patterns used in Section 2, as was appar-ent in the results presented in that section.This problem has been addressed by Caraballo (1999),who describes a system that first builds an unlabelled hi-erarchy of noun clusters using agglomerative bottom-upclustering of vectors of noun coordination information.The leaves of this hierarchy (corresponding to nouns)are assigned hypernyms using Hearst-style lexicosyntac-tic patterns.
Internal nodes in the hierarchy are then la-belled with hypernyms of the leaves they subsume ac-cording to a vote of these subsumed leaves.We proceed along similar lines, using noun coordi-nation information and an alternative graph-based clus-tering method.
We do not build a complete hierarchy,but our method nonetheless obtains additional hypernym-hyponym pairs not extracted by lexicosyntactic patterns.Our method is based on the following sort of inference.Consider the sentenceThis is not the case with sugar, honey, grapemust, cloves and other spices which increaseits merit.
(BNC)which provides evidence that clove is a kind of spice.Given this, the sentenceShips laden with nutmeg or cinnamon, clovesor coriander once battled the Seven Seas tobring home their precious cargo.
(BNC)might suggest that nutmeg, cinnamon, and coriander arealso spices, because they appear to be similar to cloves.Thus we can learn the hyponymy relations nutmeg <spice, cinnamon < spice, and coriander < spice thatare not directly attested by lexicosyntactic patterns in ourtraining corpus.This kind of information from coordination patternshas been used for work in automatic lexical acquisition(Riloff and Shepherd, 1997; Roark and Charniak, 1998;Widdows and Dorow, 2002).
The basic rationale behindthese methods is that words that occur together in listsare usually semantically similar in some way: for exam-ple, the phrasey1, y2, and y3suggests that there is some link between y1 and y2, etc.Performing this analysis on a whole corpus results in adata structure which holds a collection of nouns and ob-served noun-noun relationships.
If we think of the nounsas nodes and the noun-noun relationships as edges, thisdata structure is a graph (Bolloba?s, 1998), and combina-toric methods can be used to analyze its structure.Work using such techniques for lexical acquisition hasproceeded by building classes of related words from asingle ?seed-word?
with some desired property (such asbeing a representative of a paticular semantic class).
Forexample, in order to extract a class of words referring tokinds of disease from a corpus, you start with a singleseed-word such as typhoid, and then find other nouns thatoccur in lists with typhoid.
Using the graph model de-scribed above, Widdows and Dorow (2002) developed acombinatoric algorithm for growing clusters from a sin-gle seed-word, and used these methods to find correctnew members for chosen categories with an accuracy ofover 80%.The idea that certain patterns can be identified usingfinite-state techniques and used as evidence for seman-tic relationships is the same as Hearst?s (1992), but ap-pears to be more effective for finding just similar wordsrather than hypernyms because there are many more in-stances of simple coordination patterns than of hyper-nymy patterns?in the lists we used to extract these re-lationships, we see much more cooccurence of words onthe same ontological level than between words from dif-ferent ontological levels.
For example, in the BNC thereare 211 instances of the phrase ?fruit and vegetables?
and9 instances of ?carrots and potatoes?, but no instances of?fruit and potatoes?, only 1 instance of ?apples and veg-etables?, and so on.This sort of approach should be ideal for improvingthe recall of automatic hyponymy extraction, by using thehyponym from each of the correct hypernym/hyponympairs as a seed-word for the category represented by thehypernym?for example, from the relationship clove <spice, the word clove could be taken as a seed-word, withthe assumption that words which frequently occur in co-ordination with clove are also names of spices.We used the algorithm of (Widdows and Dorow, 2002)on the British National Corpus to see if many more hy-ponymy relations would be extracted in this way.
Foreach correct pair y < x where y was a single-word hy-ponym of x discovered by the lexicosyntactic patterns ofSection 2, we collected the 10 words most similar to y ac-cording to this algorithm and tested to see if these neigh-bors were also hyponyms of x.Of the 176 extracted hyponyms that we evaluated byhand in the overlapping test sets described in Section 2and Section 3, 95 were rated 4 or 3 on our 5-point scor-ing system (Section 2) by at least one of the authors.
Con-sidering these correct or nearly-correct relations in theirhand-corrected form, we found that 45 of these 95 rela-tions involved single-word hyponyms.
(We restricted ourattention to these 45 relations because the graph modelwas built using only single words as nodes in the graph.
)This set of 45 correct hypernym ?seed-pairs?
was ex-tended by another potential 459 pairs (slightly more than10 for each seed-pair because if there was a tie for 10thplace both neighbors were used).
Of these, 211 (46%)were judged to be correct hypernym pairs and 248 (54%)were not.7 This accuracy compares favorably with the ac-curacy of 40% obtained for the raw hyponymy extractionexperiments in Section 2, suggesting that inferring newrelations by using corpus-based similarities to previouslyknown relations is more reliable than trying to learn com-pletely new relations even if they are directly attested inthe corpus.
However, our accuracy falls way short of thefigure of 82% reported by Widdows and Dorow (2002).We believe this is because the classes in (Widdows andDorow, 2002) are built from carefully selected seed-examples: ours are built from an uncontrolled sampleof seed-examples extracted automatically from a corpus.We outline three cases where this causes a critical differ-ence.The ambiguity of ?mass?One of the correct hyponymy relations extracted in ourexperiments in Section 2 was mass < religious service.Using mass as a seed suggested the following candidatesas potential hyponyms of religious service:Seed Semantically Similar Wordsmass length weight angle shape depthheight range charge size momentumAll these neighbors are related to the ?measurement ofphysical property?
sense of the word mass rather than the?religious service?
sense.
The inferred hyponymy rela-tions are all incorrect because of this mismatch.The specific properties of ?nitrogen?Another true relation we extracted was nitrogen < nu-trient.
Using the same process as above gave the follow-ing neighbors of nitrogen:Seed Semantically Similar Wordsnitrogen methane dioxide carbon hydrogen methanolvapour ammonia oxide oxygen monoxide waterThese neighboring terms are not in general nutrients,and the attempt to infer new hyponymy relations is a fail-7As before, we consider scores of 4 and 3 on our 5-pointscale to be correct and lower scores to be incorrect.
The pre-cision of graph-model results (reported in this section and inSection 5), unlike those reported elsewhere, are based on theannotations of a single author.ure in this case.
While the relationship nitrogen < nu-trient is one of the many facts which go to make up thevast store of world-knowledge that an educated adult usesfor reasoning, it is not a necessary property of nitrogenitself, and one could arguably ?know?
the meaning ofnitrogen without being aware of this fact.
In traditionallexicographic terms, the fact that nitrogen is a nutrientmight be regarded as part of the differentiae rather thanthe genus of nitrogen.
Had our seed-pair instead beennitrogen < gas or nitrogen < chemical element, manycorrect hyponymy relations would have been inferred byour method, and both of these classifications are centralto the meaning of nitrogen.Accurate levels of abstraction for ?dill?Finally, even when the hyponymy relationship y < xused as a seed-case was central to the meaning of y andall of the neighbors of y were related to this meaning,they were still not always hyponyms of x but sometimesmembers of a more general category.
For example, usingthe correct seed-pair dill < herb we retrieved the follow-ing suggested hyponyms for herb:Seed Semantically Similar Wordsdill rind fennel seasoning juice saucepepper parsley vinegar oil purAll of these items are related to dill, but only some ofthem are herbs.
The other items should also be placedin the same general area of a taxonomy as dill, but ascooking ingredients rather than specifically herbs.In spite of these problems, the algorithm for improv-ing recall by adding neighbors of the correct hyponymsworked reasonably well, obtaining 211 correct relation-ships from 45 seeds, an almost fivefold increase in recall,with an accuracy of 46%, which is better than that of ourbaseline pattern-matching hyponymy extractor.It is possible that using coordination (such as co-occurence in lists) as a measure of noun-noun similarityis well-adapted for this sort of work, because it mainlyextracts ?horizontal?
relationships between items of sim-ilar specificity or similar generality.
Continuing the ge-ometric analogy, these mainly ?horizontal?
relationshipsmight be expected to combine particularly well with seedexamples of ?vertical?
relationships, i.e.
hyponymy rela-tionships.5 Combining LSA and Coordination toImprove Precision and RecallHaving used two separate techniques to improve preci-sion and recall in isolation, it made sense to combineour methods to improve performance overall.
This wasaccomplished by applying LSA filtering as described inSection 3 to the results obtained by extending our initialhypernym pairs with coordination patterns in Section 4.LSA filtering of extended results: phase IThe first application of filtering to the additional hy-ponymy relations obtained using noun-cooccurrence wasstraightforward.
We took the 459 potential hyponymyrelationships obtained in Section 4.
For each of theprospective hyponyms y of a given hypernym x, we com-puted the LSA similarity sim(x, y).
We then consideredonly those potential hyponyms whose LSA similarity tothe hypernym surpassed a certain threshhold.
Using thistechnique with an experimentally determined threshholdof 0.15, we obtained a set of 260 hyponymy relations ofwhich 166 were correct (64%, as opposed to the 46%correct in the unfiltered results).
The LSA filtering hadremoved 154 incorrect relationships and only 45 correctones, reducing the overall error rate by 33%.In particular, this technique removed all but one ofthe spurious religious service hyponyms which were ob-tained through inappropriate similarities with mass in theexample in Section 4, though it was much less effectivein filtering the neighbors of nitrogen and dill, as might beexpected.LSA filtering of extended results: phase IIFor some of the hyponymy relations to which we ap-plied our extension technique, the hypernym had multiplewords.8 In some of these cases, it was clear that one ofthe words in the hypernym had a meaning more closelyrelated to the original (correct) hyponym.
For instance, inthe mass < religious service relation, the word religioustells us more about the appropriate meaning of mass thandoes the word service.
It thus seemed that, at least in cer-tain cases, we might be able to get more traction in LSAfiltering of potential additional hyponyms by first select-ing a particular word from the hypernym as the ?mostimportant?
and using that word rather than the entire hy-pernym for filtering.9We thus applied a simple two-step algorithm to refinethe filtering technique presented above:1.
The LSA similarity between the original (correct)hyponym and each word in the hypernym is com-puted.
The words of the hypernym are ranked ac-cording to these similarities.2.
The word in the hypernym that has the highest LSAsimilarity to the original (correct) hyponym is usedinstead of the entire hypernym for phase-I-style fil-tering.8The graph model used to obtain new candidate hyponymswas built using single words, which is why our extended resultsinclude some multiword expressions among the hypernyms butonly single word hyponyms.9When using an entire multiword hypernym for filtering, aterm-vector was produced for the multiword hypernym by aver-aging the LSA vectors for the constituent words.This filtering technique, with an LSA-similarity thresh-hold of 0.15, resulted in the extraction of 35 correct and25 incorrect relationships.
In contrast, using LSA simi-larity with the whole expression rather than the most im-portant word resulted in the extraction of 32 correct and30 incorrect relationships for those hypernyms with mul-tiple words.
On the face of it, selecting only the mostimportant part of the hypernym for comparison enabledus to obtain more correct and fewer incorrect relations,but it is also clear that by this stage in our experimentsour sample of seed-relationships had become too smallfor these results to be statistically significant.However, the examples we considered did demonstrateanother point?that LSA could help to determine whichparts of a multiword expression were semantically rel-evant.
For example, one of the seed-relationships wasFrance < European Community member.
Finding thatsim(france, european) > sim(france, community),we could infer that the adjective European was central tothe meaning of the hyponym, whereas for the examplewallflowers < hardy biennials the opposite conclusion,that hardy is an adjectival modifier which isn?t central tothe relationship, could be drawn.
However, these conclu-sions could also be drawn by using established colloca-tion extraction techniques (Manning and Schu?tze, 1999,Ch.
5) to find semantically significant multiword expres-sions.6 Obtaining Canonical Forms forRelationsAn important part of extracting semantic relations likethose discussed in this paper is converting the terms inthe extracted relations to a canonical form.
In the caseof our extracted hyponymy relations, such normalizationconsists of two steps:1.
Removing extraneous articles and qualifiers.
Ourextracted hyponyms and hypernyms were often inthe form ?another x?, ?some x?, and so forth, wherex is the hypernym or hyponym that we actually wantto consider.2.
Converting nouns to their singular form.
This is el-ementary morphological analysis, or a limited formof lemmatization.We performed the second of these steps using themorph morphological analysis software (Minnen et al,2001).10 To perform the first step of removing modifiers,we implemented a Perl script to do the following:10This software is freely available fromhttp://www.cogs.susx.ac.uk/lab/nlp/carroll/morph.html.?
Remove leading determiners from the beginning ofthe hypernym and from the beginning of the hy-ponym.?
Remove leading prepositions from the beginning ofthe hypernym.
Doing this after removing leadingdeterminers eliminates the common ?those of?
con-struction.?
Remove cardinal numbers from the hypernym andthe hyponym.?
Remove possessive prefixes from the hypernym andthe hyponym.?
Remove ?set of?
and ?number of?
from the hy-pernym and the hyponym.
This ad hoc but rea-sonable procedure eliminates common troublesomeconstructions not covered by the above rules.?
Remove leading adjectives from hypernyms, but notfrom hyponyms.
In addition to removing ?other?,this amounts to playing it safe.
By removing leadingadjectives we make potential hypernyms more gen-eral, and thus more likely to be a superset of theirpotential hyponym.
While this removal sometimesmakes the learned relationship less useful, it sel-dom makes it incorrect.
We leave adjectives on hy-ponyms to make them more specific, and thus morelikely to be a subset of their purported hypernym.Using these simple rules, we were able to convert 73of the 78 relations orginally scored as 3 (see Section 2)to relations receiving a score of 4.
This demonstrates asa ?proof of concept?
that comparatively simple languageprocessing techniques can be used to map relationshipsfrom the surface forms in which they were observed intext to a canonical form which could be included in a se-mantic resource.7 Conclusion and Further WorkThe results presented in this paper demonstrate that theapplication of linguistic information from automatically-learned mathematical models can significantly enhanceboth the precision and the recall of pattern-based hy-ponymy extraction techniques.
Using a graph model ofnoun similarity we were able to obtain an almost five-fold improvement in recall, though the precision of thistechnique is clearly affected by the correctness of the?seed-relationships?
used.
Using LSA filtering we elimi-nated spurious relations extracted by the original patternmethod, reducing errors by 30%.
Such filtering also elim-inated spurious relations learned using the graph modelthat were the result of lexical ambiguity and of seed hy-ponymy relations inappropriate for the technique, reduc-ing errors by 33%.This paper suggests many possibilities for future work.First of all, it would be interesting to apply LSA to a sys-tem for building an entire hypernym-labelled ontology inroughly the way described in (Caraballo, 1999), perhapsby using an LSA-weighted voting method to determinewhich hypernym would be used to label each node.
Weare considering how to extend our techniques to such atask.Also, systematic comparison of the lexicosyntacticpatterns used for extraction to determine the relative pro-ductiveness and accuracy of each pattern might proveilluminating, as would comparison across different cor-pora to determine the impact of the topic area andmedium/format of documents on the effectiveness of hy-ponymy extraction.
Ultimately, the ability to predict apriori how well a knowledge-extraction system will workon a previously unseen corpus will be crucial to its use-fulness.Applying the techniques of this paper to a system thatused mutual bootstrapping (Riloff and Jones, 1999) tofind additional extraction patterns would also be interest-ing (such an approach is suggested in (Hearst, 1998)).And of course, further refinement of the mathematicalmodels we use and our methods of learning them, includ-ing more sophisticated use of available tools for linguisticpre-processing, such as the identification and indexing ofmultiword expressions, could further improve the preci-sion and recall of hyponymy extraction techniques.AcknowledgementsThis research was supported in part by the ResearchCollaboration between the NTT Communication ScienceLaboratories, Nippon Telegraph and Telephone Corpora-tion and CSLI, Stanford University, and by EC/NSF grantIST-1999-11438 for the MUCHMORE project.
Thanksalso to Stanley Peters for his helpful comments on an ear-lier draft.ReferencesEnrique Alfonseca and Suresh Manandhar.
2001.
Im-proving an ontology refinement method with hy-ponymy patterns.
In Third International Conferenceon Language Resources and Evaluation, pages 235?239, Las Palmas, Spain.Ricardo Baeza-Yates and Berthier Ribiero-Neto.
1999.Modern Information Retrieval.
Addison Wesley /ACM Press.Be?la Bolloba?s.
1998.
Modern Graph Theory.
Num-ber 184 in Graduate Texts in Mathematics.
Springer-Verlag.Sharon Caraballo.
1999.
Automatic construction of ahypernym-labeled noun hierarchy from text.
In 37thAnnual Meeting of the Association for ComputationalLinguistics: Proceedings of the Conference, pages120?126.Claire Cardie.
1997.
Empirical methods in informationextraction.
AI Magazine, 18:65?79.Scott Deerwester, Susan Dumais, George Furnas,Thomas Landauer, and Richard Harshman.
1990.
In-dexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41(6):391?407.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge MA.L Guthrie, J Pustejovsky, Y Wilks, and B Slator.
1996.The role of lexicons in natural language processing.Communications of the ACM, 39(1):63?72.Udo Hahn and Klemens Schnattinger.
1998.
Towardstext knowledge engineering.
In AAAI/IAAI, pages524?531.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In COLING, Nantes,France.Marti A. Hearst, 1998.
WordNet: An Electronic LexicalDatabase, chapter 5, Automated discovery of WordNetrelations, pages 131?152.
MIT Press, Cambridge MA.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Process-ing.
The MIT Press, Cambridge, Massachusetts.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of english.
NaturalLanguage Engineering, 7(3):207?223.Ellen Riloff and Rosie Jones.
1999.
Learning dictionar-ies for infomation extraction by multi-level bootstrap-ping.
In Proceedings of the Sixteenth National Confer-ence on Artificial Intelligence, pages 472?479.
AAAI.Ellen Riloff and Jessica Shepherd.
1997.
A corpus-basedapproach for building semantic lexicons.
In ClaireCardie and Ralph Weischedel, editors, Proceedings ofthe Second Conference on Empirical Methods in Natu-ral Language Processing, pages 117?124.
Associationfor Computational Linguistics, Somerset, New Jersey.Brian Roark and Eugene Charniak.
1998.
Noun-phraseco-occurence statistics for semi-automatic semanticlexicon construction.
In COLING-ACL, pages 1110?1116.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?124.Dominic Widdows and Beate Dorow.
2002.
A graphmodel for unsupervised lexical acquisition.
In 19th In-ternational Conference on Computational Linguistics,pages 1093?1099, Taipei, Taiwan, August.
