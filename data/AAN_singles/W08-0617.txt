BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 98?99,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsExtracting Protein-Protein Interaction based on Discriminative Training ofthe Hidden Vector State ModelDeyu Zhou and Yulan HeInformatics Research Centre, The University of Reading, Reading RG6 6BX, UKEmail:d.zhou@reading.ac.uk, y.he@reading.ac.uk1 IntroductionThe knowledge about gene clusters and protein in-teractions is important for biological researchersto unveil the mechanism of life.
However, largequantity of the knowledge often hides in the liter-ature, such as journal articles, reports, books andso on.
Many approaches focusing on extracting in-formation from unstructured text, such as patternmatching, shallow and deep parsing, have been pro-posed especially for extracting protein-protein inter-actions (Zhou and He, 2008).A semantic parser based on the Hidden VectorState (HVS) model for extracting protein-protein in-teractions is presented in (Zhou et al, 2008).
TheHVS model is an extension of the basic discreteMarkov model in which context is encoded as astack-oriented state vector.
Maximum Likelihoodestimation (MLE) is used to derive the parametersof the HVS model.
In this paper, we propose a dis-criminative approach based on parse error measureto train the HVS model.
To adjust the HVS model toachieve minimum parse error rate, the generalizedprobabilistic descent (GPD) algorithm (Kuo et al,2002) is used.
Experiments have been conducted onthe GENIA corpus.
The results demonstrate mod-est improvements when the discriminatively trainedHVS model outperforms its MLE trained counter-part by 2.5% in F-measure on the GENIA corpus.2 MethodologiesThe Hidden Vector State (HVS) model (He andYoung, 2005) is a discrete Hidden Markov Model(HMM) in which each HMM state represents thestate of a push-down automaton with a finite stacksize.Normally, MLE is used for generative probabil-ity model training in which only the correct modelneeds to be updated during training.
It is be-lieved that improvement can be achieved by train-ing the generative model based on a discriminativeoptimization criteria (Klein and Manning, 2002) inwhich the training procedure is designed to maxi-mize the conditional probability of the parses giventhe sentences in the training corpus.
That is, not onlythe likelihood for the correct model should be in-creased but also the likelihood for the incorrect mod-els should be decreased.Assuming the most likely semantic parse treeC?
= Cj and there are altogether M semantic parsehypotheses for a particular sentence W , a parse er-ror measure (Juang et al, 1993; Chou et al, 1993;Chen and Soong, 1994) can be defined asd(W ) = ?
logP (W,Cj) + log[1M ?
1?i,i6=jP (W,Ci)?
]1?
(1)where ?
is a positive number and is used to se-lect competing semantic parses.
When ?
= 1,the competing semantic parse term is the averageof all the competing semantic parse scores.
When?
?
?, the competing semantic parse term be-comes maxi.i6=jP (W,Ci) which is the score for the topcompeting semantic parse.
By varying the value of?, we can take all the competing semantic parses intoconsideration.
d(W ) > 0 implies classification er-ror and d(W ) ?
0 implies correct decision.The sigmoid function can be used to normalized(W ) in a smooth zero-one range and the loss func-tion is thus defined as (Juang et al, 1993):`(W ) = sigmoid(d(W )) (2)98wheresigmoid(x) = 11 + e?
?x(3)Here, ?
is a constant which controls the slope of thesigmoid function.The update formula is given by:?k+1 = ?k ?
?k?`(Wi, ?k) (4)where ?k is the step size.Using the definition of `(Wi, ?k) and after work-ing out the mathematics, we get the update formu-lae 5, 6, 7,(logP (n|c?))?
= logP (n|c?)?
??`(di)(1?
`(di))??
?I(Cj , n, c?)
+?i,i6=jI(Ci, n, c?
)P (Wi, Ci, ?)?
?i,i6=j P (Wi, Ci, ?)???
(5)(logP (c[1]|c[2..D]))?
= logP (c[1]|c[2..D])?
??`(di)(1?
`(di))??
?I(Cj , c[1], c[2..D]) +?i,i6=jI(Ci, c[1], c[2..D])P (Wi, Ci, ?)?
?i,i6=j P (Wi, Ci, ?)???
(6)(logP (w|c))?
= logP (w|c)?
??`(di)(1?
`(di))??
?I(Cj , w, c) +?i,i6=jI(Ci, w, c)P (Wi, Ci, ?)?
?i,i6=j P (Wi, Ci, ?)???
(7)where I(Ci, n, c?)
denotes the number of timesthe operation of popping up n semantic tags atthe current vector state c?
in the Ci parse tree,I(Ci, c[1], c[2..D]) denotes the number of times theoperation of pushing the semantic tag c[1] at the cur-rent vector state c[2..D] in the Ci parse tree andI(Ci, w, c) denotes the number of times of emittingthe word w at the state c in the parse tree Ci.3 Experimental Setup and ResultsGENIA (Kim et al, 2003) is a collection of 2000 re-search abstracts selected from the search results ofMEDLINE database using keywords (MESH terms)?human, blood cells and transcription factors?.
Allthese abstracts were then split into sentences andthose containing more than two protein names andat least one interaction keyword were kept.
Alto-gether 3533 sentences were left and 2500 sentenceswere sampled to build our data set.The results using MLE and discriminative train-ing are listed in Table 1.
Discriminative trainingimproves on the MLE by relatively 2.5% where NTable 1: Performance comparison of MLE versus Dis-criminative trainingMeasurement GENIAMLE DiscriminativeRecall 61.78% 64.59%Precision 61.16% 61.51%F-measure 61.47% 63.01%and I are set to 5 and 200 individually.
Here N de-notes the number of semantic parse hypotheses andI denotes the the number of sentences in the trainingdata.ReferencesJ.K.
Chen and F.K.
Soong.
1994.
An n-best candidates-based discriminative training for speech recognitionapplications.
IEEE Transactions on Speech and AudioProcessing, 2:206 ?
216.W.
Chou, C.H.
Lee, and B.H.
Juang.
1993.
Minimumerror rate training based on n-best string models.
InAcoustics, Speech, and Signal Processing, IEEE Inter-national Conference on ICASSP ?93, volume 2, pages652 ?
655.Y.
He and S. Young.
2005.
Semantic processing usingthe hidden vector state model.
Computer Speech andLanguage, 19(1):85?106.B.H.
Juang, W. Chou, and C.H.
Lee.
1993.
Statisticaland discriminative methods for speech recognition.
InRubio, editor, Speech Recognition and Understanding,NATO ASI Series, Berlin.
Springer-Verlag.JD.
Kim, T. Ohta, Y. Tateisi, and J Tsujii.
2003.
GE-NIA corpus?semantically annotated corpus for bio-textmining.
Bioinformatics, 19(Suppl 1):i180?2.D.
Klein and C. D. Manning.
2002.
Conditional struc-ture versus conditional estimation in nlp models.
InProc.
the ACL-02 conference on Empirical methods innatural language processing, pages 9?16, Universityof Pennsylvania, PA.H.-K.J.
Kuo, E. Fosle-Lussier, H. Jiang, and C.H.
Lee.2002.
Discriminative training of language modelsfor speech recognition.
In Acoustics, Speech, andSignal Processing, IEEE International Conference onICASSP ?02, volume 1, pages 325 ?
328.Deyu Zhou and Yulan He.
2008.
Extracting Interac-tions between Proteins from the Literature.
Journalof Biomedical Informatics, 41:393?407.Deyu Zhou, Yulan He, and Chee Keong Kwoh.
2008.Extracting Protein-Protein Interactions from the Liter-ature using the Hidden Vector State Model.
Interna-tional Journal of Bioinformatics Research and Appli-cations, 4(1):64?80.99
