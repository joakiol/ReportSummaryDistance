Learning Correlations between Linguistic Indicators and SemanticConstraints:Reuse of Context-Dependent Descriptions of EntitiesDragomir  R .
RadevDepar tment  of Computer  ScienceColumbia UniversityNew York, NY 10027radev@cs.columbia.eduAbst rac tThis paper presents the results of a study on thesemantic constraints imposed on lexical choiceby certain contextual indicators.
We show howsuch indicators are computed and how correla-tions between them and the choice of a nounphrase description of a named entity can be au-tomatically established using supervised learn-ing.
Based on this correlation, we have devel-oped a technique for automatic lexical choice ofdescriptions of entities in text generation.
Wediscuss the underlying relationship between thepragmatics of choosing an appropriate descrip-tion that serves a specific purpose in the auto-matically generated text and the semantics ofthe description itself.
We present our work inthe framework of the more general concept ofreuse of linguistic structures that are automati-cally extracted from large corpora.
We presenta formal evaluation of our approach and we con-clude with some thoughts on potential applica-tions of our method.1 In t roduct ionHuman writers constantly make deliberate deci-sions about picking a particular way of express-ing a certain concept.
These decisions are madebased on the topic of the text and the effect thatthe writer wants to achieve.
Such contextualand pragmatic constraints are obvious to ex-perienced writers who produce context-specifictext without much effort.
However, in order fora computer to produce text in a similar way,either these constraints have to be added man-ually by an expert or the system must be ableto acquire them in an automatic way.An example related to the lexical choice ofan appropriate nominal description of a personshould make the above clear.
Even though itseems intuitive that Bill Clinton should alwaysbe described with the NP "U. S. president" or avariation thereof, it turns out that many otherdescriptions appear in on-line news stories thatcharacterize him in light of the topic of the arti-cle.
For example, an article from 1996 on elec-tions uses "Bill Clinton, the democratic pres-idential candidate", while a 1997 article on afalse bomb alert in Little Rock, Ark.
uses "BillClinton, an Arkansas native".This paper presents the results of a study ofthe correlation between amed entities (people,places, or organizations) and noun phrases usedto describe them in a corpus.Intuitively, the use of a description is based ona deliberate decision on the part of the authorof a piece of text.
A writer is likely to select adescription that puts the entity in the contextof the rest of the article.It is known that the distribution of words ina document is related to its topic (Salton andMcGill, 1983).
We have developed related tech-niques for approximating pragmatic onstraintsusing words that appear in the immediate con-text of the entity.We will show that context influences thechoice of a description, as do several other lin-guistic indicators.
Each of the indicators by it-self doesn't provide enough empirical data thatdistinguishes among all descriptions that are re-lated to a an entity.
However, a carefully se-lected combination of such indicators providesenough information in order pick an appropriatedescription with more than 80% accuracy.Section 2 describes how we can automaticallyobtain enough constraints on the usage of de-scriptions.
In Section 3, we show how such con-structions are related to language reuse.In Section 4 we describe our experimentalsetup and the algorithms that we have designed.Section 5 includes a description of our results.1072In Section 6 we discuss some possible exten-sions to our study and we provide some thoughtsabout possible uses of our framework.2 P rob lem Descr ip t ionLet's define the relation DescriptionOf(E) tobe the one between a named entity E and anoun phrase, D, describing the named entity.In the example shown in Table 1, there are twoentity-description pairs.DescriptionOf ("Tareq Aziz") = "Iraq'sDeputy Prime Minister"DescriptionOf ("Richard Butler") = "ChiefU.N.
arms inspector"Chief U.N. arms inspector Richard Butlermet Iraq's Deputy Prime Minister Tareq AzizMonday after rejecting Iraqi attempts to setdeadlines for finishing his work.Figure 1: Sample sentence containing twoentity-description pairs.Each entity appearing in a text can have mul-tiple descriptions (up to several dozen) associ-ated with it.We call the set of all descriptions related tothe same entity in a corpus, a profile of thatentity.
Profiles for a large number of entitieswere compiled using our earlier system, PRO-FILE (Radev and McKeown, 1997).
It turnsout that there is a large variety in the size ofthe profile (number of distinct descriptions) fordifferent entities.
Table 1 shows a subset of theprofile for Ung Huot, the former foreign ministerof Cambodia, who was elected prime minister atsome point of time during the run of our exper-iment.
A few sample semantic features of thedescriptions in Table 1 are shown as separatecolumns.We used information extraction techniques tocollect entities and descriptions from a corpusand analyzed their lexical and semantic proper-ties.We have processed 178 MB 1 of newswireand analyzed the use of descriptions relatedto 11,504 entities.
Even though PROFILE ex-tracts other entities in addition to people (e.g.,1The corpus contains 19,473 news tories that coverthe period October 1, 1997 - January 9, 1998 that wereavailable through PROFILE.places and organizations), we have restrictedour analysis to names of people only.
We claim,however, that a large portion of our findings re-late to the other types of entities as well.We have investigated 35,206 tuples, consist-ing of an entity, a description, an article ID,and the position (sentence number) in the arti-cle in which the entity-description pair occurs.Since there are 11,504 distinct entities, we hadon average 3.06 distinct descriptions per entity(DDPE).
Table 2 shows the distribution ofDDPE values across the corpus.
Notice that alarge number of entities (9,053 out of the 11,504)have a single description.
These are not as in-teresting for our analysis as the remaining 2,451entities that have DDPE values between 2 and24.10' : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :!F !
i l  i!i i i!i!ii if!
ii !iiii!i'o' a!!!!!~!~iii!iiii!!~i!
!~: !iii!~i!~!!i~!~i!iiii~ii~..
: ~!!i~iiiiiiii~i!~!i!iiiii~i~!!
!~i!i!iiii~i\]i i i l  i,o ?
,o '  ,o 'X - Number of d~inct  ~l~crlpl~ne per ent Ily (DDPE)Figure 2: Number of distinct descriptions perentity (log-log scale)3 Language Reuse  in TextGenerat ionText generation usually involves lexical choice -that is, choosing one way of referring to an en-tity over another.
Lexical choice refers to a vari-ety of decisions that have to made in text gener-ation.
For example, picking one among severalequivalent (or nearly equivalent) constructionsis a form of lexical choice (e.g., "The Utah Jazzhanded the Boston Celtics a de fear' vs. "TheUtah Jazz defeated the Boston Celtics" (Robin,1994)).
We are interested in a different aspectof the problem: namely learning the rules thatcan be used for automatically selecting an ap-propriate description of an entity in a specific1073Descr ipt iona senior memberCambodia'sCambodian foreign ministerco-premierfirst prime ministerforeign ministerHis ExcellencyMr.new co-premiernew first prime ministernewly-appointed first prime ministerpremierprime ministeraddressingXSemantic categoriescountry male new political postXXXTable 1: Profile of Ung Huotcou at~612lO8238910111213149,0531,4814721821127431XXXseniorityXXXXXXXXXXDDPE ~ 41617181924Table 2: Number of distinct descriptions per entity (DDPE).context.To be feasible and scaleable, a technique forsolving a particular case of the problem of lex-icai choice must involve automated learning.
Itis also useful if the technique can specify enoughconstraints on the text to be generated so thatthe number of possible surface realizations thatmatch the semantic onstraints is reduced sig-nificantly.
The easiest case in which lexicalchoice can be made is when the full surfacestructure can be used, and when it has been au-tomatically extracted from a corpus.
Of course,the constraints on the use of the structure in thegenerated text have to be reasonably similar tothe ones in the source text.We have found that a natural application forthe analysis of entity-description pairs is lan-guage reuse, which includes techniques of ex-tracting shallow structure from a corpus andapplying that structure to computer-generatedtexts.Language reuse involves two components: asource text written by a human and a targettext, that is to be automatically generated bya computer, partially making use of structuresreused from the source text.
The source textis the one from which particular surface struc-tures are extracted automatically, along withthe appropriate syntactic, semantic, and prag-matic constraints under which they are used.Some examples of language reuse include col-location analysis (Smadja, 1993), the use ofentire factual sentences extracted from cor-pora (e.g., "'Toy Story' is the Academy Awardwinning animated film developed by Pixar~'),and summarization using sentence xtraction(Paice, 1990; Kupiec et al, 1995).
In the caseof summarization through sentence xtraction,the target text has the additional property ofbeing a subtext of the source text.
Other tech-niques that can be broadly categorized as lan-guage reuse are learning relations from on-linetexts (Mitchell, 1997) and answering naturallanguage questions using an on-line encyclope-ia (Kupiec, 1993).Stydying the concept of language reuse is re-warding because it allows generation systems toleverage on texts written by humans and theirdeliberate choice of words, facts, structure.We mentioned that for language reuse to take1074place, the generation system has to use the samesurface structure in the same syntactic, seman-tic, and pragmatic ontext as the source textfrom which it was extracted.
Obviously, all ofthis information is typically not available to ageneration system.
There are some special casesin which most of it can be automatically com-puted.Descriptions of entities are a particular in-stance of a surface structure that can be reusedrelatively easily.
Syntactic onstraints relatedto the use of descriptions are modest - since de-scriptions are always noun phrases that appearas either pre-modifiers or appositions 2, they arequite flexibly usable in any generated text inwhich an entity can be modified with an ap-propriate description.
We will show in the restof the paper how the requisite semantic (i.e.,"what is the meaning of the description to pick")and pragmatic onstraints (i.e., "what purposedoes using the description achieve ?')
can be ex-tracted automatically.Given a profile like the one shown in Table 1,and an appropriate set of semantic onstraints(columns 2-7 of the table), the generation com-ponent needs to perform a profile lookup andselect a row (description) that satisfies most orall semantic onstraints.
For example, if the se-mantic constraints specify that the descriptionhas to include the country and the political po-sition of Ung Huot, the most appropriate de-scription is "Cambodian foreign minister".4 Exper imenta l  SetupIn our experiments, we have used two widelyavailable tools - WordNet and Ripper.WordNet (Miller et al, 1990) is an on-linehierarchical lexical database which contains e-mantic information about English words (in-cluding hypernymy relations which we use inour system).
We use chains of hypernyms whenwe need to approximate the usage of a particu-lar word in a description using its ancestor andsibling nodes in WordNet.
Particularly usefulfor our application are the synset offsets of thewords in a description.
The synset offset is anumber that uniquely identifies a concept node(synset) in the WordNet hierarchy.
Figure 3shows that the synset offset for the concept "ad-ministrator, decision maker" is "(07063507}',2We haven't included relative clauses in our study.while its hypernym, "head, chief, top dog" hasa synset offset of "~07311393} ".Ripper (Cohen, 1995) is an algorithm thatlearns rules from example tuples in a relation.Attributes in the tuples can be integers (e.g.,length of an article, in words), sets (e.g., se-mantic features), or bags (e.g., words that ap-pear in a sentence or document).
We use Rip-per to learn rules that correlate context andother linguistic indicators with the semanticsof the description being extracted and subse-quently reused.
It is important o notice thatRipper is designed to learn rules that classifydata into atomic classes (e.g., "good", "aver-age", and "bad").
We had to modify its al-gorithm in order to classify data into sets ofatoms.
For example, a rule can have the form"if CONDITION then \[( 07063762} {02864326}{ 0001795~}\] "3 .
This rule states that if a certain"CONDITION" (which is a function of the in-dicators related to the description) is met, thenthe description is likely to contain words thatare semantically related to the three WordNetnodes \[{07063762} {02864326} {00017954}\].The stages of our experiments are describedin detail in the remainder of this section.4.1 Semant ic  tagg ing  of descr ip t ionsOur system, PROFILE, processes WWW-accessible newswire on a round-the-clock basisand extracts entities (people, places, and orga-nizations) along with related descriptions.
Theextraction grammar, developed in CREP (Du-ford, 1993), covers a variety of pre-modifier andappositional noun phrases.For each word wi in a description, we use aversion of WordNet to extract he synset offsetof the immediate parent of wi.4.2 F ind ing  l inguist ic cuesInitially, we were interested in discovering rulesmanually and then validating them using thelearning algorithm.
However, the task proved(nearly) impossible considering the sheer sizeof the corpus.
One possible rule that we hy-pothesized and wanted to verify empirically atthis stage was parallelism.
This linguistically-motivated rule states that in a sentence witha parallel structure (consider, for instance, the3These offsets correspond to the WordNet nodes"manager", internetn, and "group"1075DIRECTOR:  {07063762} director, manager, managing director=~ {07063507} administrator, decision maker=~ {07311393} head, chief, top dog=~ {06950891} leader=~ {00004123} person, individual, someone, somebody, mortal, human, soul=~ {00002086} life form, organism, being, living thing=~ {00001740} entity, somethingFigure 3: Hypernym chain of "director" in WordNet, showing synset offsets.sentence fragment "... Alija Izetbegovic, a Mus-lim, Kresimir Zubak, a Croat, and MomciloKrajisnik, a Serb... ") all entities involved havesimilar descriptions.
However, rules at such adetailed syntactic level take too long to processon a 180 MB corpus and, further, no more thana handful of such rules can be discovered manu-ally.
As a result, we made a decision to extractall indicators automatically.
We would also liketo note that using syntactic information on sucha large corpus doesn't appear particularly fea-sible.
We limited therefore our investigationto lexicai, semantic, and contextual indicatorsonly.
The following subsection describes the at-tributes used.4.3 Ext ract ing  l inguist ic  cuesautomat ica l lyThe list of indicators that we use in our systemare the following:?
Context :  (using a window of size 4, ex-cluding the actual description used, butnot the entity itself) - e.g., "\['clinton''clinton' 'counsel' counsel' decision' deci-sion' 'gore' 'gore' 'ind' 'ind' 'index' 'news''november' 'wednesday'\]" is a bag of wordsfound near the description of Bill Clintonin the training corpus.?
Length  of  the  art ic le:  - an integer.?
Name of  the entity: - e.g., "Bill Clin-ton".?
Prof i le:  The entire profile related to a per-son (all descriptions of that person that arefound in the training corpus).?
Synset Offsets: - the WordNet node num-bers of all words (and their parents)) thatappear in the profile associated with theentity that we want to describe.4.4 App ly ing  mach ine  learn ing  methodTo learn rules, we ran Ripper on 90% (10,353)of the entities in the entire corpus.
We kept theremaining 10% (or 1,151 entities) for evaluation.Sample rules discovered by the system areshown in Table 3.5 Results and EvaluationWe have performed a standard evaluation of theprecision and recall that our system achieves inselecting a description.
Table 4 shows our re-sults under two sets of parameters.Precision and recall are based on how well thesystem predicts a set of semantic onstraints.Precision (or P)  is defined to be the number ofmatches divided by the number of elements inthe predicted set.
Recall (or R) is the numberof matches divided by the number of elementsin the correct set.
If, for example, the systempredicts \[A\] \[B\] \[C\], but the set of constraintson the actual description is \[B\] \[D\], we wouldcompute that P = 33.3% and R --- 50.0%.
Ta-ble 4 reports the average values of P and R forall training examples 4.Selecting appropriate descriptions based onour algorithm is feasible even though the val-ues of precision and recall obtained may seemonly moderately high.
The reason for this isthat the problem that we axe trying to solve isunderspecified.
That is, in the same context,more than one description can be potentiallyused.
Mutually interchangeable d scriptions in-clude synonyms and near synonyms ("leader"vs. "chief) or pairs of descriptions of differentgenerality (U.S. president vs. president).
This4We run Ripper in a so-called "noise-free mode",which causes  the  condition parts of the rules it discoversto be mutually exclusive and therefore, the values of Pand R on the training data  are  both 100~.1076Rule DecisionIF CONTEXT - inflation {09613349} (politician)IF PROFILES " detective AND CONTEXT " agency {07485319} (policeman)IF CONTEXT - celine {07032298} (north_american)Table 3" Sample rules discovered by the system.Training set size5001,0002,0005,00010,00015,00020,00025,00030,00050,000word nodes onlyPrecision Recall64.29% 2.86%71.43% 2.86%42.86% 40.71%59.33% 48.40%69.72% 45.04%76.24% 44.02%76.25% 49.91%83.37% 52.26%80.14% 50.55%83.13% 58.54%word and parent nodesPrecision78.57%85.71%67.86%64.67%74.44%73.39%79.08%82.39%82.77%88.87%Recall2.86%2.86%62.14%53.73%59.32%53.17%58.70%57.49%57.66%63.39%Table 4: Values for precision and recall using word nodes only (left) and both word and parentnodes (right).type of evaluation requires the availability of hu-man judges.There are two parts to the evaluation: howwell does the system performs in selecting se-mantic features (WordNet nodes) and how wellit works in constraining the choice of a descrip-tion.
To select a description, our system does alookup in the profile for a possible descriptionthat satisfies most semantic onstraints (e.g., weselect a row in Table 1 based on constraints onthe columns).Our system depends crucially on the multiplecomponents hat we use.
For example, the shal-low CREP grammar that is used in extractingentities and descriptions often fails to extractgood descriptions, mostly due to incorrect PPattachment.
We have also had problems fromthe part-of-speech tagger and, as a result, weoccasionally incorrectly extract word sequencesthat do not represent descriptions.6 App l i ca t ions  and  Future  WorkWe should note that PROFILE is part of alarge system for information retrieval and sum-marization of news through information extrac-tion and symbolic text generation (McKeownand Radev, 1995).
We intend to use PROFILEto improve lexical choice in the summary gen-eration component, especially when producinguser-centered summaries or summary updates(Radev and McKeown, 1998 to appear).
Thereare two particularly appealing cases - (1) whenthe extraction component has failed to extract adescription and (2) when the user model (user'sinterests, knowledge of the entity and personalpreferences for sources of information and for ei-ther conciseness orverbosity) dictates that a de-scription should be used even when one doesn'tappear in the texts being summarized.A second potentially interesting applicationinvolves using the data and rules extracted byPROFILE for language r generation.
I  (Radevand McKeown, 1998 to appear) we show howthe conversion of extracted descriptions intocomponents of a generation grammar allows forflexible (re)generation of new descriptions thatdon't appear in the source text.
For example,a description can be replaced by a more generalone, two descriptions can be combined to forma single one, or one long description can be de-constructed into its components, ome of whichcan be reused as new descriptions.We are also interested in investigating an-other idea - that of predicting the use of a de-scription of an entity even when the correspond-ing profile doesn't contain any description at all,or when it contains only descriptions that con-tain words that are not directly related to thewords predicted by the rules of PROFILE.
Inthis case, if the system predicts a semantic at-1077egory that doesn't match any of the descriptionsin a specific profile, two things can be done: (1)if there is a single description in the profile, topick that one, and (2) if there is more than onedescription, pick the one whose semantic vectoris closest o the predicted semantic vector.Finally, the profile extractor will be used aspart of a large-scale, automatically generatedWho's who site which will be accessible bothby users through a Web interface and by NLPsystems through a client-server API.7 Conc lus ionIn this paper, we showed that context and otherlinguistic indicators correlate with the choiceof a particular noun phrase to describe an en-tity.
Using machine learning techniques from avery large corpus, we automatically extracteda large set of rules that predict the choice of adescription out of an entity profile.
We showedthat high-precision automatic prediction of anappropriate description in a specific context ispossible.8 AcknowledgmentsThis material is based upon work supported bythe National Science Foundation under GrantsNo.
IRI-96-19124, IRI-96-18797, and CDA-96-25374, as well as a grant from Columbia Uni-versity's Strategic Initiative Fund sponsored bythe Provost's Office.
Any opinions, findings,and conclusions or recommendations expressedin this material are those of the author(s) anddo not necessarily reflect the views of the Na-tional Science Foundation.The author is grateful to the followingpeople for their comments and suggestions:Kathy McKeown, Vasileios Hatzivassiloglou,and Hongyan Jing.Re ferencesWilliam W. Cohen.
1995.
Fast effective ruleinduction.
In Proc.
12th International Con-ference on Machine Learning, pages 115-123.Morgan Kaufmann.Darrin Duford.
1993.
CREP: a regularexpression-matching textual corpus tool.Technical Report CUCS-005-93, ColumbiaUniversity.Julian M. Kupiec, Jan Pedersen, and FrancineChen.
1995.
A trainable document summa-rizer.
In Proceedings, 18th Annual Interna-tional ACM SIGIR Conference on Researchand Development in Information Retrieval,pages 68-73, Seattle, Washington, July.Julian M. Kupiec.
1993.
MURAX: A robustlinguistic approach for question answering us-ing an on-line encyclopedia.
In Proceedings,16th Annual International ACM SIGIR Con-ference on Research and Development in In-formation Retrieval.Kathleen R. McKeown and Dragomir R. Radev.1995.
Generating summaries of multiple newsarticles.
In Proceedings, 18th Annual Interna-tional A CM SIGIR Conference on Researchand Development in Information Retrieval,pages 74-82, Seattle, Washington, July.George A. Miller, Richard Beckwith, ChristianeFellbaum, Derek Gross, and Katherine J.Miller.
1990.
Introduction to WordNet: Anon-line lexical database.
International Jour-hal of Lexicography (special issue), 3(4):235-312.Tom M. Mitchell.
1997.
Does machine learningreally work?
AI Magazine, 18(3).Chris Paice.
1990.
Constructing literatureabstracts by computer: Techniques andprospects.
Information Processing and Man-agement, 26:171-186.Dragomir R. Radev and Kathleen R. McKe-own.
1997.
Building a generation knowledgesource using internet-accessible newswire.
InProceedings of the 5th Conference on Ap-plied Natural Language Processing, Washing-ton, DC, April.Dragomir R. Radev and Kathleen R. McK-eown.
1998, to appear.
Generating natu-ral language summaries from multiple on-linesources.
Computational Linguistics.Jacques Robin.
1994.
Revision-Based Gener-ation of Natural Language Summaries Pro-viding Historical Background.
Ph.D. the-sis, Computer Science Department, ColumbiaUniversity.G.
Salton and M.J. McGill.
1983.
Introductionto Modern Information Retrieval.
ComputerSeries.
McGraw Hill, New York.Frank Smadja.
1993.
Retrieving collocationsfrom text: Xtract.
Computational Linguis-tics, 19(1):143-177, March.1078
