Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 5?13,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsContext-Dependent Regression Testing for Natural Language ProcessingElaine FarrowHuman Communication Research CentreSchool of InformaticsUniversity of EdinburghEdinburgh, UKElaine.Farrow@ed.ac.ukMyroslava O. DzikovskaHuman Communication Research CentreSchool of InformaticsUniversity of EdinburghEdinburgh, UKM.Dzikovska@ed.ac.ukAbstractRegression testing of natural language sys-tems is problematic for two main reasons:component input and output is complex, andsystem behaviour is context-dependent.
Wehave developed a generic approach whichsolves both of these issues.
We describe ourregression tool, CONTEST, which supportscontext-dependent testing of dialogue systemcomponents, and discuss the regression testsets we developed, designed to effectively iso-late components from changes and problemsearlier in the pipeline.
We believe that thesame approach can be used in regression test-ing for other dialogue systems, as well as intesting any complex NLP system containingmultiple components.1 IntroductionNatural language processing systems, and dialoguesystems in particular, often consist of large sets ofcomponents operating as a pipeline, including pars-ing, semantic interpretation, dialogue management,planning, and generation.
Testing such a system canbe a difficult task for several reasons.
First, the com-ponent output may be context-dependent.
This isparticularly true for a dialogue system ?
referenceresolution, ellipsis, and sometimes generation typi-cally have to query the system state to produce theiroutput, which depends both on the state of the world(propositions defined in a knowledge base) and onthe dialogue history (object salience).
Under theseconditions, unit testing using the input and output ofa single component in isolation is of limited value?
the entire system state needs to be preserved tocheck that context-dependent components are func-tioning as expected.Second, the inputs and outputs of most systemcomponents are usually very complex and oftenchange over time as the system develops.
Whentwo complex representations are compared it maybe difficult to determine what impact any change islikely to have on system performance (far-reachingor relatively trivial).
Further, if we test componentsin isolation by saving their inputs, and these inputsare reasonably complex, then it will become difficultto maintain the test sets for the components furtheralong the pipeline (such as diagnosis and generation)as the output of the earlier components changes dur-ing development.The simplest way to deal with both of these is-sues would be to save a set of test dialogues as agold standard, checking that the final system out-put is correct given the system input.
However, thispresents another problem.
If a single component(generation, for example) malfunctions, it becomesimpossible to verify that a component earlier in thepipeline (for example, reference resolution) is work-ing properly.
In principle we could also save themessages passing between components and comparetheir content, but then we are faced again with theproblems arising from the complexity of componentinput and output which we described above.To solve these problems, we developed a regres-sion tool called CONTEST (for CONtext-dependentTESTing).
CONTEST allows the authors of individ-ual system components to control what informationto record for regression testing.
Test dialogues are5saved and replayed through the system, and individ-ual components are tested by comparing only theirspecific regression output, ignoring the outputs gen-erated by other components.
The components areisolated by maintaining a minimal set of inputs thatare guaranteed to be processed correctly.To deal with issues of output complexity we ex-tend the approach of de Paiva and King (2008) fortesting a deep parser.
They created test sets at dif-ferent levels of granularity, some including detailedrepresentations, but some just saving very simpleoutput of a textual entailment component.
Theyshowed that, given a carefully selected test set, test-ing on the final system output can be a fast and effec-tive way to discover problems in the interpretationpipeline.We show how the same idea can be used to testother dialogue system components as well.
We de-scribe the design of three different test sets thateffectively isolate the interpretation, tutorial plan-ning and generation components of our system.
Us-ing CONTEST allows us to detect system errors andmaintain consistent test sets even as the underlyingrepresentations change, and gives us much greaterconfidence that the results of our testing are relevantto the performance of the system with real users.The rest of this paper is organised as follows.
InSection 2 we describe our system and its compo-nents in more detail.
The design of the CONTESTtool and the test sets are described in Sections 3 and4.
Finally, in Section 5 we discuss how the inter-active nature of the dialogue influences the designof the test sets and the process of verifying the an-swers; and we discuss features that we would like toimplement in the future.2 BackgroundThis work has been carried out to support the devel-opment of BEETLE (Callaway et al, 2007), a tuto-rial dialogue system for basic electricity and elec-tronics.
The goal of the BEETLE system is to teachconceptual knowledge using natural language dia-logue.
Students interact with the system through agraphical user interface (GUI) which includes a chatinterface,1 a window to browse through slides con-1The student input is currently typed to avoid issues withautomated speech recognition of complex utterances.taining reading material and diagrams, and an inter-face to a circuit simulator where students can buildand manipulate circuits.The system consists of twelve components alto-gether, including a knowledge base representing thestate of the world, a curriculum planner responsiblefor the lesson structure, and dialogue managementand NLP components.
We developed CONTEST sothat it could be used to test any system component,though our testing focuses on the natural languageunderstanding and generation components.2BEETLE uses a standard natural language pro-cessing pipeline, starting with a parser, lexical in-terpreter, and dialogue manager.
The dialogue man-ager handles all input from the GUI (text, buttonpresses and circuits) and also supports generic di-alogue processing, such as dealing with interpreta-tion failures and moving the lesson along.
Studentanswers are processed by the diagnosis and tuto-rial planning components (discussed below), whichfunction similarly to planning and execution com-ponents in task oriented dialogue systems.
Finally,a generation subsystem converts the semantic repre-sentations output by the tutorial planner into the finaltext to be presented to the student.The components communicate with each otherusing the Open Agent Architecture (Martin et al,1998).
CONTEST is implemented as an OAA agent,accepting requests to record messages.
However,OAA is not essential for the system design ?
anycommunication architecture which supports addingextra agents into a system would work equally well.BEETLE aims to get students to support their rea-soning using natural language, since explanationsand contentful talk are associated with learning gain(Purandare and Litman, 2008).
This requires de-tailed analyses of student answers in terms of cor-rect, incorrect and missing parts (Dzikovska et al,2008; Nielsen et al, 2008).
Thus, we use the TRIPSparser (Allen et al, 2007), a deep parser which pro-duces detailed analyses of student input.
The lexicalinterpreter extracts a list of objects and relationshipsmentioned, which are checked against the expectedanswer.
These lists are fairly long ?
many expectedanswers have ten or more relations in them.
The2All our components are rule-based, but we expect the sameapproach would work for components of a statistical nature.6diagnoser categorises each of the objects and rela-tionships as correct, contradictory or irrelevant.
Thetutorial planner makes decisions about the remedi-ation strategy, choosing one strategy from a set ofabout thirteen depending on the question type andtutorial context.
Finally, the generation system usesthe FUF/SURGE (Elhadad, 1991) deep generator togenerate feedback automatically.Obviously, the output from the deep parser andthe input to the tutorial planner and generator arequite complex, giving rise to the types of problemsthat we discussed in the introduction.
We alreadyhad a tool for unit-testing the parser output (Swift etal., 2004), plus some separate tools to test the diag-noser and the generation component, but the com-plexity of the representations made it impractical tomaintain large test sets that depended on such com-plex inputs and outputs.
We also wanted a unifiedway to test all the components in the context of theentire system.
This led to the creation of CONTEST,which we describe in the rest of the paper.3 The CONTEST ToolFigure 1: The regression testing process.In this section we describe the process for creat-ing and using test cases, illustrated in Figure 1.
Thefirst step in building a useful regression tool is tomake it possible to run the same dialogue throughthe system many times without retyping the studentanswers.
We added a wrapper around the GUI to in-tercept and record the user actions and system callsfor later playback, thus creating a complete recordof the session.
Every time our system runs, a newsaved session file is automatically created and savedin a standard location.
This file forms the basis forour test cases.
It uses an XML format, which ishuman-readable and hand-editable, easily extensibleand amenable to automatic processing.
A (slightlysimplified) example of a saved session file is shownin Figure 2.
Here we can see that a slide was dis-played, the tutor asked the question ?Which compo-nents (if any) are in a closed path in circuit 1??
andthe student answered ?the battery and the lightbulb?.Creating a new test case is then a simple matter ofstarting the system and performing the desired ac-tions, such as entering text and building circuits inthe circuit simulator.
If the system is behaving as itshould, the saved session file can be used directly asa test case.
If the system output is not as desired, thefile can be edited in any text editor.Of course, this only allows the final output of thesystem to be tested, and we have already discussedthe shortcomings of such an approach: if a com-ponent late in the pipeline has problems, there isno way to tell if earlier components behaved as ex-pected.
To remedy this, we added a mechanism forcomponents other than the GUI to record their owninformation in the saved session file.Components can be tested in effective isolation bycombining two mechanisms: carefully designed testsets which focus on a single component and (impor-tantly) are expected to succeed even if some othercomponent is having problems; and a regression toolwhich allows us to test the output of an individualcomponent.
Our test sets are discussed in detail inSection 4.
The remainder of this section describesthe design of the tool.CONTEST reads in a saved session file and re-produces the user actions (such as typing answersor building circuits), producing a new saved ses-sion file as its output.
If there have been changes tothe system since the test was created, replaying thesame actions may lead to new slides and tutor mes-sages being displayed, and different recorded outputfrom intermediate components.
For example, giventhe same student answers, the diagnosis may havechanged, leading to different tutor feedback.
Wecompare the newly generated output file against theinput file.
If there are no differences, the test is con-sidered to have passed.
As the input and output files7<test><action agent="tutor" method="showSlide">lesson1-oe/exercise/img1.html</action><action agent="tutor" method="showOutput">Which components (if any) are in a closed path in circuit 1?</action><action agent="student" method="submitText">the battery and the lightbulb</action></test>Figure 2: A saved session file showing a single interaction between tutor and student.are identical in format, the comparison can be doneusing a ?diff?
command.With each component recording its own output, itcan be the case that there are many differences be-tween old and new files.
It is therefore important tobe able to choose the level of detail we want whencomparing saved session files, so that the output ofa single component can be checked independentlyof other system behaviour.
We solved this problemby creating a set of standard XSLT filters.
One fil-ter picks out just the dialogue between student andtutor to produce a transcript of the session.
Otherfilters select the output from one particular compo-nent, for example the tutorial planner, with the tutorquestions included to provide context.
In general,we wrote one filter for each component.CONTEST creates a test report by comparing theexpected and actual outputs of the system on eachtest run.
We specify which filter to use (based onwhich component we are testing).
If the test fails,we can examine the relevant differences using the?ediff?
mode in the emacs text editor.
More sophis-ticated approaches are possible, such as using a fur-ther XSL transform to count all the errors of a partic-ular type, but we have found ediff to be good enoughfor our purposes.
With the filters in place we onlysee the differences for the component we are testing.Since component regression output is designed to besmall and human-readable, checking the differencesis a very quick process.Test cases can be run individually or in groups.33Test cases are usually grouped by directory, but symboliclinks allow us to use the same case in several groups.Using CONTEST, we can create a single report for agroup of test cases: the individual outputs are com-bined to create a new output file for the group andthis is compared to the (combined) input file, withfilters applied in the usual way.
This is a very use-ful feature, allowing us to create a report for all the?good answer?
cases (for example) in one step.Differences do not always indicate errors; some-times they are simply changes or additions to therecorded information.
After satisfying ourselves thatthe reported differences are intentional changes, wecan update the test cases to reflect the output of thelatest run.
Subsequent runs will test against the newbehaviour.
CONTEST includes an update tool whichcan update a group of cases with a single command.This is simpler and less error-prone than editing po-tentially hundreds of files by hand.4 Test CasesWe have built several test sets for each component,amounting to more than 400 individual test cases.We describe examples of the test sets for three of ourcomponents in more detail below, to demonstratehow we use CONTEST.4.1 Interpretation Test CasesWe have a test set consisting of ?good answers?
foreach of the questions in our system which we use totest the interpretation component.
The regression in-formation recorded by the interpretation componentincludes the internal ID code of the matched answerand a code indicating whether it is a ?best?, ?good?
or?minimal?
answer.
This is enough to allow us to de-8<test name="closed_path_discussion"><action agent="tutor" method="showOutput">What are the conditions that are required to make a bulb light up?</action><action agent="student" method="submitText">a bulb must be in a closed path with the battery</action><action agent="simpleDiagnosis" method="logForRegression">student-act: answer atype: diagnosis consistency: []code: complete subcode: bestanswer_id: conditions_for_bulb_to_light_ans1</action></test>Figure 3: A sample test case from our ?good answers?
set showing the diagnosis produced for the student?s answer.tect many possible errors in interpretation.
We canrun this test set after every change to the parsing orinterpretation components.A (slightly simplified) example of our XML testcase format is shown in Figure 3, with the tutor ques-tion ?What are the conditions that are required tomake a bulb light up??
and the student answer ?abulb must be in a closed path with the battery?.
Theanswer diagnosis shows that the system recognisedthat the student was attempting to answer the ques-tion (rather than asking for help), that the answermatch was complete, with no missing or incorrectparts, and the answer was consistent with the state ofthe world as perceived by the system.4 The matchedanswer is marked as the best one for that question.While the recorded information does not supplythe full interpretation, it can suggest the source ofvarious possible errors.
If interpretation fails, thestudent act will be set to uninterpretable,and the code will correspond to the reasonfor failed interpretation: unknown inputif the parse failed, unknown mapping orrestriction failure if lexical interpretationfailed, and unresolvable if reference resolutionfailed.
If interpretation worked, but took incorrectscoping or attachment decisions, the resultingproposition is likely to be inconsistent with the4Sometimes students are unable to interpret diagrams, orare lacking essential background knowledge, and therefore saythings that contradict the information in the domain model.
Thesystem detects and remediates such cases differently from gen-eral errors in explanations (Dzikovska et al, 2006).current knowledge base, and an inconsistency codewill be reported.
In addition, verifying the matchedanswer ID provides some information in case onlya partial interpretation was produced.
Sometimesdifferent answer IDs correspond to answers that arevery complete versus answers that are acceptablebecause they address the key point of the question,but miss some small details.
Thus if a differentanswer ID has matched, it indicates that someinformation was probably lost in interpretation.The codes we report were not devised specificallyfor the regression tests.
They are used internally toallow the system to produce accurate feedback aboutmisunderstandings.
However, because they indicatewhere the error is likely to originate (parsing, lexi-cal interpretation, scoping and disambiguation), theycan help us to track it down.We have another test set for ?special cases?, suchas the student requesting a hint or giving up.
An ex-ample is shown in Figure 4.
Here the student givesup completely on the first question, then asks forhelp with the second.
We use this test case to checkthat the set phrases ?I give up?
and ?help?
are un-derstood by the system.
The ?special cases?
test setincludes a variety of help request phrasings observedin the corpora we collected.
Note that this examplewas recorded while using a tutorial policy that re-sponds to help requests by simply providing the an-swer.
This does not matter for testing interpretation,since the information recorded in the test case willdistinguish help requests from give ups, regardless9T: Which components (if any) are in a closed pathin circuit 1?S: I give upT: The answer is the battery and the bulb in 1.T: Which components (if any) are in a closed pathin circuit 2?S: helpT: Here?s the answer.
The bulb in 2.Figure 4: The transcript of a test case for ?I give up?
and?help?.
T: is the tutor, S: is the student.of the specific tutorial policy used by the system.Finally, we have a test set for detection of un-interpretable utterances.
In a tutoring system, stu-dents often use incorrect terminology or unex-pected phrasings which the system cannot under-stand.
While we expect coverage and robustness ofparsing and interpretation components to improveas the system develops, some percentage of errorsis to be expected simply because even state-of-the-art parsers are not 100% accurate.
Thus the sys-tem needs to detect problems whenever possible,and produce useful help messages.
We have col-lected a set of student utterances that correspondto typical interpretation problems ?
failed parses,unknown words, sentences that are parsed but theparser makes an incorrect attachment decision, sen-tences where the student used terminology incor-rectly (violating sortal constraints encoded in theknowledge base), a pronominal referent that couldnot be resolved.
Identifying the problem type cor-rectly is important because it affects later processingin the dialogue manager and tutorial planner.
Thuswe have a set of test cases using the collected sen-tences, which we use to ensure that the problems areflagged correctly.4.2 Tutoring Test CasesThe BEETLE tutorial planner makes decisions onsystem behaviour and corresponds to planning andexecution components in other dialogue systems.Here, we are attempting to test that the tutorial plan-ner selected the right system action.
Therefore, sincewe are not testing the interpretation component, wedon?t vary the form of the language we use.
Instead,T: Predict the state (on, off) of each bulb in dia-gram 1.S: all bulbs are offT: OK, we?ll check your prediction in a moment.T: Predict the state (on, off) of each bulb in dia-gram 2.S: all bulbs are offT: OK, we?ll check your prediction in a moment.T: Test your prediction for diagram 1 by openingboth switches and observing the bulbs.T: Test your prediction for diagram 2 by closingboth switches and observing the bulbs.T: Did the bulbs behave the way you expected?S: noT: Right.
In your second prediction, you said thatBulb A, Bulb B and Bulb C would be off.
ButBulb A, Bulb B and Bulb C are lit.Figure 5: A tutorial planner test case transcript.
T: is thetutor, S: is the student.we vary the combination of good and bad student an-swers, and record the action chosen by the system.The tutorial planner chooses feedback in 2 stages.First, a general algorithm is chosen depending on theexercise type and student input type: there are sepa-rate algorithms for addressing, for example, what todo if the student input was not interpreted, and forcorrect and incorrect answers.
Choosing the algo-rithm requires some computation depending on thequestion context.
Once the main algorithm is cho-sen, different tutorial strategies can be selected, andthis is reflected in the regression output: the systemrecords a keyword corresponding to the chosen algo-rithm, and then the name of the strategy along withkey strategy parameters.For example, Figure 5 shows the transcript froma test case for a common exercise type from ourlessons: a so called predict-verify-evaluate se-quence.
In this example, the student is asked topredict the behaviour of three light bulbs in a cir-cuit, test it by manipulating the circuit in the simu-lation environment, and then evaluate whether theirpredictions matched the circuit behaviour.
The sys-tem reinforces the point of the exercise by producinga summary of discrepancies between the student?s10<action agent="tutor" method="showOutput">Did the bulbs behave the way you expected?</action><action agent="student" method="submitText">no</action><action agent="tc-bee" method="logForRegression">EVALUATE (INCORRECT-PREDICTION NO_NO)</action>Figure 6: An excerpt from a tutorial planner test case showing the recorded summary output.predictions and the observed outcomes.An excerpt from the corresponding test case isshown in Figure 6.
Here we can see the tutor askthe evaluation question ?Did the bulbs behave theway you expected??
and the student answer ?no?.The EVALUATE algorithm was chosen to handle thestudent answer, and from the set of available strate-gies the INCORRECT-PREDICTION strategy waschosen.
That strategy takes a parameter indicatingif there was a discrepancy when the student evalu-ated the results (here NO NO, corresponding to theexpected and actual evaluation result inputs).In contrast, in the first example in Figure 4, wherethe student gives up and doesn?t provide an an-swer, the tutorial planner output is REMEDIATE(BOTTOM-OUT Q IDENTIFY).
This shows thatthe system has chosen to use a REMEDIATE algo-rithm, and a ?bottom-out?
(giving away the answer)strategy for remediation.
The strategy parameterQ IDENTIFY (which depends on the question type)determines the phrasing to be used in the generatorto verbalise the tutor?s feedback.The saved output allows us to see that the cor-rect algorithm was chosen to handle the student in-put (for example, that the REMEDIATE algorithmis correctly chosen after an incorrect student answerto an explanation question), and that the algorithmchooses a strategy appropriate for the tutorial con-text.
Certain errors can still go undetected here, forexample, if the algorithm for verbalising the chosenstrategy in the generator is broken.
Developing sum-mary inputs to detect such errors is part of plannedfuture work.In order to isolate the tutorial planner from inter-pretation, we use standard fixed phrasings for stu-dent answers.
The answer phrasings in the ?goodanswers?
test set for interpretation (described in Sec-tion 4.1) are guaranteed to be understood correctly,so we use only these phrasings in our tutorial plannertest cases.
Thus, we are able to construct tests whichwill not be affected by problems in the interpretationpipeline.4.3 Generation Test CasesTo test generation, we have a set of test cases wherethe student immediately says ?I give up?
in responseto each question.
This phrase is used in our systemto prevent the students getting stuck ?
the tutorialpolicy is to immediately stop and give the answer tothe question.
The answers given are generated bya deep generator from internal semantic representa-tions, so this test set gives us the assurance that allrelevant domain content is being generated properly.This is not a complete test for the generation capabil-ities of our system, since each explanation questioncan have several possible answers of varying degreesof quality (suggested by experienced human tutors(Dzikovska et al, 2008)), and we always choosethe best possible answer when the student gives up.However, it gives us confidence that the student cangive up at any point and receive an answer which canbe used as a template for future answers.5 Discussion and Future WorkWe have created more than 400 individual test casesso far.
There are more than 50 for the interpretationcomponent, more than 150 for the tutorial plannerand more than 200 for the generation component.We are developing new test sets based on other sce-narios, such as responding to each question with a11help request.
We are also refining the summary in-formation recorded by each component.An important feature of our testing approach isthe use of short summaries rather than the inter-nal representations of component inputs and outputs.Well-designed summaries provide key informationin an easy-to-read format that can remain constant asinternal formats change and develop over time.
Webelieve that this approach would be useful for otherlanguage processing systems, since at present thereare few standardised formats in the community andrepresentations are typically developed and refinedtogether with the algorithms that use them.The decision about what information to include inthe summary is vital to the success and overall use-fulness of the regression tool.
If too much detail isrecorded, there will be many spurious changes andit will be burdensome to keep a large regression setupdated.
If too little detail is recorded, unwantedchanges in the system may go undetected.
The con-tent of the test cases we discussed in Section 4 rep-resents our approach to such decisions.Interpretation was perhaps the most difficult, be-cause it has a particularly complex output.
In deter-mining the information to record, we were followingthe solution of de Paiva and King (2008) who use thedecision result of the textual entailment system as away to efficiently test parser output.
For our sys-tem, the information output by the diagnoser aboutanswer correctness proved to have a similar function?
it effectively provides information about whetherthe output of the interpretation component was us-able, without the need to check details carefully.The main challenge for our tutorial planner andgeneration components (corresponding to planningand execution components in a task-oriented dia-logue system) was to ensure that they were suffi-ciently isolated so as to be unaffected by errors in in-terpretation.
We achieve this by maintaining a smallset of known phrasings which are guaranteed to beinterpreted correctly; this ensures that in practice,the downstream components are isolated from un-wanted changes in interpretation.Our overall methodology of recording and test-ing summary information for individual componentscan be used with any complex NLP system.
The spe-cific details of what information to record obviouslydepends on the domain, but our experience suggestssome general principles.
For testing the interpreta-tion pipeline, it is useful to record pre-existing errorcodes and a qualitative summary of the informationused to decide on the next system action.
Where werecord the code output by the diagnoser, an informa-tion seeking system could record, for example, thenumber of slots filled and the number of items re-trieved from a database.
It is also useful to recorddecisions taken by the system, or actions performedin response to user input; so, just as we record infor-mation about the chosen tutorial policy, other sys-tems can record the action taken ?
whether it is tosearch the database, query a new slot, or confirm aslot value.One major improvement that we have planned forthe future is adding another layer of test case man-agement to CONTEST, to enable us to produce sum-maries and statistics about the total number of testcases that have passed and failed, instead of check-ing reports individually.
Such statistics can be im-plemented easily using another XSL transform ontop of the existing filters to count the number oftest cases with no differences and produce summarycounts of each type of error detected.6 ConclusionThe regression tool we developed, CONTEST, solvestwo of the major issues faced when testing dia-logue systems: context-dependence of componentbehaviour and complexity of component output.
Wedeveloped a generic approach based on runningsaved dialogues through the system, and checkingsummary information recorded by different compo-nents against separate gold standards.
We demon-strated that test sets can be designed in such a way asto effectively isolate downstream components fromchanges and problems earlier in the pipeline.
We be-lieve that the same approach can be used in regres-sion testing for other dialogue systems, as well as intesting any complex NLP system containing multi-ple components.AcknowledgementsThis work has been supported in part by Office ofNaval Research grant N000140810043.
We thankCharles Callaway for help with generation and tu-toring tests.12ReferencesJames Allen, Myroslava Dzikovska, Mehdi Manshadi,and Mary Swift.
2007.
Deep linguistic processingfor spoken dialogue systems.
In Proceedings of theACL-07 Workshop on Deep Linguistic Processing.Charles B. Callaway, Myroslava Dzikovska, Elaine Far-row, Manuel Marques-Pita, Colin Matheson, and Jo-hanna D. Moore.
2007.
The Beetle and BeeDiff tutor-ing systems.
In Proceedings of the SLaTE-2007 Work-shop, Farmington, Pennsylvania, USA, September.Valeria de Paiva and Tracy Holloway King.
2008.
De-signing testsuites for grammar-based systems in appli-cations.
In Coling 2008: Proceedings of the workshopon Grammar Engineering Across Frameworks, pages49?56, Manchester, England, August.
Coling 2008 Or-ganizing Committee.Myroslava O. Dzikovska, Charles B. Callaway, andElaine Farrow.
2006.
Interpretation and generation ina knowledge-based tutorial system.
In Proceedings ofEACL-06 workshop on knowledge and reasoning forlanguage processing, Trento, Italy, April.Myroslava O. Dzikovska, Gwendolyn E. Campbell,Charles B. Callaway, Natalie B. Steinhauser, ElaineFarrow, Johanna D. Moore, Leslie A. Butler, andColin Matheson.
2008.
Diagnosing natural languageanswers to support adaptive tutoring.
In Proceed-ings 21st International FLAIRS Conference, CoconutGrove, Florida, May.Michael Elhadad.
1991.
FUF: The universal unifier usermanual version 5.0.
Technical Report CUCS-038-91,Dept.
of Computer Science, Columbia University.D.
Martin, A. Cheyer, and D. Moran.
1998.
Buildingdistributed software systems with the open agent ar-chitecture.
In Proceedings of the Third InternationalConference on the Practical Application of IntelligentAgents and Multi-Agent Technology, Blackpool, Lan-cashire, UK.Rodney D. Nielsen, Wayne Ward, and James H. Martin.2008.
Learning to assess low-level conceptual under-standing.
In Proceedings 21st International FLAIRSConference, Coconut Grove, Florida, May.Amruta Purandare and Diane Litman.
2008.
Content-learning correlations in spoken tutoring dialogs atword, turn and discourse levels.
In Proceedings 21stInternational FLAIRS Conference, Coconut Grove,Florida, May.Mary D. Swift, Joel Tetreault, and Myroslava O.Dzikovska.
2004.
Semi-automatic syntactic and se-mantic corpus annotation with a deep parser.
In Pro-ceedings of LREC-2004.13
