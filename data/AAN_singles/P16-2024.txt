Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 143?148,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsSimple PPDB: A Paraphrase Database for SimplificationEllie PavlickUniversity of Pennsylvaniaepavlick@seas.upenn.eduChris Callison-BurchUniversity of Pennsylvaniaccb@cis.upenn.eduAbstractWe release the Simple ParaphraseDatabase, a subset of of the ParaphraseDatabase (PPDB) adapted for the taskof text simplification.
We train a super-vised model to associate simplificationscores with each phrase pair, producingrankings competitive with state-of-the-art lexical simplification models.
Ournew simplification database contains4.5 million paraphrase rules, making itthe largest available resource for lexicalsimplification.1 MotivationLanguage is complex, and the process of readingand understanding language is difficult for manygroups of people.
The goal of text simplificationis to rewrite text in order to make it easier to un-derstand, for example, by children (De Belder andMoens, 2010), language learners (Petersen andOstendorf, 2007), people with disabilities (Relloet al, 2013; Evans et al, 2014), and even bymachines (Siddharthan et al, 2004).
Automatictext simplification (Napoles and Dredze, 2010;Wubben et al, 2012; Xu et al, 2016) has the po-tential to dramatically increase access to informa-tion by making written documents available at allreading levels.Full text simplification involves many steps,including grammatical restructuring and summa-rization (Feng, 2008).
One of the most basicsubtasks is lexical simplification (Specia et al,2012)?
replacing complicated words and phraseswith simpler paraphrases.
While there is active re-search in the area of lexical simplification (Costerand Kauchak, 2011a; Glava?s and?Stajner, 2015;Paetzold, 2015), existing models have been by-and-large limited to single words.
Often, how-medical practitioner ?
doctorlegislative texts ?
lawshypertension ?
high blood pressureprevalent ?
very commonsignificant quantity ?
a lotimpact negatively ?
be badTable 1: In lexical simplification, it is often necessary to re-place single words with phrases or phrases with single words.The above are examples of such lexical simplifications cap-tured by the Simple PPDB resource.ever, it is preferable, or even necessary to para-phrase a single complex word with multiple sim-pler words, or to paraphrase multiple words with asingle word.
For example, it is difficult to imaginea simple, single-word paraphrase of hypertension,but the three-word phrase high blood pressure is avery good simplification (Table 1).
Such phrasalsimplifications are overlooked by current lexicalsimplification models, and thus are often unavail-able to the end-to-end text simplification systemsthat require them.Recent research in data-driven paraphrasing hasproduced enormous resources containing millionsof meaning-equivalent phrases (Ganitkevitch etal., 2013).
Such resources capture a wide range oflanguage variation, including the types of lexicaland phrasal simplifications just described.
In thiswork, we apply state-of-the-art machine learnedmodels for lexical simplification in order to iden-tify phrase pairs from the Paraphrase Database(PPDB) applicable to the task of text simplifica-tion.
We introduce Simple PPDB,1a subset of theParaphrase Database containing 4.5 million sim-plifying paraphrase rules.
The large scale of Sim-ple PPDB will support research into increasinglyadvanced methods for text simplification.1http://www.seas.upenn.edu/?nlp/resources/simple-ppdb.tgz1432 Identifying Simplification Rules2.1 Paraphrase RulesThe Paraphrase Database (PPDB) is currentlythe largest available collection of paraphrases.Each paraphrase rule in the database has anautomatically-assigned quality score between 1and 5 (Pavlick et al, 2015).
In this work, we usethe PPDB-TLDR2dataset, which contains 14 mil-lion high-scoring lexical and phrasal paraphrases,and is intended to give a generally good tradeoffbetween precision and recall.
To preprocess thedata, we lemmatize all of the phrases, and removerules which differ only by morphology, punctu-ation, or stop words, or which involve phraseslonger than 3 words.
The resulting list contains7.5 million paraphrase rules covering 625K uniquelemmatized words and phrases.2.2 Lexical Simplification ModelOur goal is to build a model which can accuratelyidentify paraphrase rules that both 1) simplify theinput phrase and 2) preserve its meaning.
Thatis, we want to avoid a model which favors ?sim-ple?
words (e.g.
the, and) even when they capturenone of the meaning of the input phrase.
We there-fore train our model to make a three-way distinc-tion between rules which simplify the input, ruleswhich make the input less simple, and rules whichgenerate bad paraphrases.Data.
We collect our training data in two phases.First, we sample 1,000 phrases from the vocab-ulary of the PPDB.
We limit ourselves to wordswhich also appear at least once in the Newselacorpus for text simplifcation (Xu et al, 2015),in order to ensure that we focus our model onthe types of words for which the final resource ismost likely to be applied.
For each of these 1,000words/phrases, we sample up to 10 candidate para-phrases from PPDB, stratified evenly across para-phrase quality scores.
We ask workers on Ama-zon Mechanical Turk to rate each of the chosenparaphrase rules on a scale from 1 to 5 to indi-cate how well the paraphrase preserves the mean-ing of the original phrase.
We use the same an-notation design used in Pavlick et al (2015).
Wehave 5 workers judge each pair, omitting workerswho do not provide correct answers on the em-bedded gold-standard pairs which we draw fromWordNet.
For 62% of the paraphrase rules we had2http://paraphrase.org/#/downloadscored, the average human rating falls below 3, in-dicating that the meaning of the paraphrase differssubstantially from that of the input.
We assign allof these rules to the ?bad paraphrase?
class.We take the remaining 3,758 meaning-preserving paraphrase rules (scored ?3 in theabove annotation task) and feed them into asecond annotation task, in which we identifyrules that simplify the input.
We use the sameannotation interface as in Pavlick and Nenkova(2015), which asks workers to choose which ofthe two phrases is simpler, or to indicate thatthere is no difference in complexity.
We collect 7judgements per pair and take the majority label,discarding pairs for which the majority opinionwas that there was no difference.
We include eachrule in our training data twice, once as an instanceof a ?simplifying?
rule, and once in the reversedirection as an instance of a ?complicating?
rule.In the end, our training dataset contains 11,829pairs, with the majority class being ?bad para-phrase?
(47%), and the remaining split evenlybetween ?simplifying?
and ?complicating?
para-phrase rules (26% each).Features.
We use a variety of features that havebeen shown in prior work to give good signalabout phrases?
relative complexity.
The fea-tures we include are as follows: phrase lengthin words and in characters, frequency accordingto the Google NGram corpus (Brants and Franz,2006), number of syllables, the relative frequencyof usage in Simple Wikipedia compared to normalWikipedia (Pavlick and Nenkova, 2015), charac-ter unigrams and bigrams, POS tags, and the aver-aged Word2Vec word embeddings for the words inthe phrase (Mikolov et al, 2013).
For each phrasepair ?e1, e2?, for each feature f , we include f(e1),f(e2) and f(e1)?f(e2).3We also include the co-sine similarity of the averaged word embeddingsand the PPDB paraphrase quality score as features.We train a multi-class logistic regressionmodel4to predict if the application of a paraphraserule will result in 1) simpler output, 2) more com-plex output, or 3) non-sense output.Performance.
Table 2 shows the performance ofthe model on cross-validation, compared to severalbaselines.
The full model achieves 60% accuracy,3We do not compute the difference f(e1) ?
f(e2) forsparse features, i.e.
character ngrams and POS tags.4http://scikit-learn.org/144Acc Prec.Random 47.1% 0.0%Simple/Regular Wiki.
Ratio 49.1% 47.6%Length in Characters 51.4% 47.3%Google Ngram Frequency 51.4% 44.2%Number of Syllables 51.5% 45.3%Supervised Model, W2V 54.7% 46.3%Supervised Model, Full 60.4% 52.9%Table 2: Accuracy on 10-fold cross-validation, and precisionfor identifying simplifying rules.
Folds are constructed sothat train and test vocabularies are disjoint.5 points higher than the strongest baseline, a su-pervised model which uses only word embeddingsas features.2.3 Simple PPDBWe run the trained model described above overall 7.5 million paraphrase rules.
From the pre-dictions, we construct Simple PPDB: a list of 4.5million simplifying paraphrase rules.
A rule inSimple PPDB is represented as a triple, consist-ing of a syntactic category, and input phrase, anda simplified output phrase.
Each rule is associatedwith both a paraphrase quality score from 1 to 5(taken from PPDB 2.0), and simplification con-fidence score from 0 to 1.0 (our classifier?s con-fidence in the prediction that the rule belongs tothe ?simplifying?
class).
Note that ranking viathe confidence scores of a classification model hasnot, to our knowledge, been explored in previouswork on lexical simplification.
The remainder ofthis paper evaluates the quality of the simplifica-tion ranking.
For an evaluation of the paraphrasequality ranking, see Pavlick et al (2015).
Table 3shows examples of some of the top ranked para-phrases according to Simple PPDB?s simplifica-tion score for several input phrases.3 EvaluationTo evaluate Simple PPDB, we apply it in a set-ting intended to emulate the way it is likely to beused in practice.
We use the Newsela Simplifica-tion Dataset (Xu et al, 2015), a corpus of manu-ally simplified news articles.
This corpus is cur-rently the cleanest available simplification datasetand is likely to be used to train and/or evaluate thesimplification systems that we envision benefittingmost from Simple PPDB.We draw a sample of 100 unique word types(?targets?)
from the corpus for which SimplePPDB has at least one candidate simplification.For each target, we take Simple PPDB?s full listof simplification rules which are of high qualityaccording to the PPDB 2.0 paraphrase score5andwhich match the syntactic category of the target.On average, Simple PPDB proposes 8.8 such can-didate simplifications per target.Comparison to existing methods.
Our base-lines include three existing methods for gener-ating lists of candidates that were proposed inprior work.
The methods we test for generatinglists of candidate paraphrases for a given targetare: the WordNetGenerator, which pulls syn-onyms from WordNet (Devlin and Tait, 1998;Carroll et al, 1999), the KauchakGenerator,which generates candidates based on automaticalignments between Simple Wikipedia and normalWikipedia (Coster and Kauchak, 2011a), and theGlavasGenerator, which generates candidatesfrom nearby phrases in vector space (Glava?s and?Stajner, 2015) (we use the pre-trained Word2VecVSM (Mikolov et al, 2013)).For each generated list, we follow Horn et al(2014)?s supervised SVM Rank approach to rankthe candidates for simplicity.
We reimplement themain features of their model: namely, word fre-quencies according to the Google NGrams cor-pus (Brants and Franz, 2006) and the SimpleWikipedia corpus, and the alignment probabili-ties according to automatic word alignments be-tween Wikipedia and Simple Wikipedia sentences(Coster and Kauchak, 2011b).
We omit the lan-guage modeling features since our evaluation doesnot consider the context in which the substitutionis to be applied.All of these methods (the three generation meth-ods and the ranker) are implemented as part of theLEXenstein toolkit (Paetzold and Specia, 2015).We use the LEXenstein implementations for theresults reported here, using off-the-shelf configu-rations and treating each method as a black box.Setup.
We use each of the generate-and-rankmethods to produce a ranked list of simplificationcandidates for each of the 100 targets drawn fromthe Newsela corpus.
When a generation methodfails to produce any candidates for a given tar-get, we simply ignore that target for that partic-ular method.
This is to avoid giving Simple PPDB5Heuristically, we define ?high quality?
as?3.5 for wordsand ?4 for phrases.145keenly omit employment opportunity remediedmost strongly leave out a new job set rightdeeply delete it opportunity be fixedstrongly be removed business opportunity be correctedeagerly forget about it the job to be resolvedvery be ignored labour be solvedTable 3: Examples of top-ranked simplifications proposed by Simple PPDB for several input words.
Often, the best simplifi-cation for a single word is a multiword phrase, or vice-versa.
These many-to-one mappings are overlooked when systems useonly length or frequency as a proxy for simplicity.an unfair advantage, since, by construction, PPDBwill have full coverage of our list of 100 targets.
Inthe end, the GlavasGenerator is evaluated over 95,the WordNetGenerator over 82, and the Kauchak-Generator over 48.
The results in Table 4 do notchange significantly if we restrict all systems tothe 48 targets which the KauchakGenerator is ca-pable of handling.
Since the GlavasGenerator iscapable of producing an arbitrary number of can-didates for each target, we limit the length of eachof its candidate lists to be equal to the numberof candidates produced by Simple PPDB for thatsame target.Human judgments.
For each of the proposedrules from all four systems, we collect humanjudgements on Amazon Mechanical Turk, usingthe same annotation interface as before.
That is,we ask 7 workers to view each pair and indicatewhich of the two phrases is simpler, or to indicatethat there is no difference.
We take the majority la-bel to be the true label for each rule.
Workers showmoderate agreement on the 3-way task (?
= 0.4?
0.03), with 14% of pairs receiving unanimousagreement and 37% receiving the same label from6 out of 7 annotators.
We note that the ?
metricis likely a lower bound, as it punishes low agree-ment on pairs for which there is little difference incomplexity, and thus the ?correct?
answer is notclear (e.g.
for the pair ?matter, subject?, 3 annota-tors say that matter is simpler, 2 say that subject issimpler, and 2 say there is no difference).Results.
Table 4 compares the different meth-ods in terms of how well they rank simplifyingrules above non-simplifying rules.
Simple PPDB?sranking of the relative simplicity achieves an av-eraged precision of 0.72 (0.77 P@1), comparedto 0.70 (0.69 P@1) achieved by the Horn et al(2014) system?
i.e.
the KauchakGenerator+SVMRanker.
We hypothesize that the performancedifference between these two ranking systems isAvg.
Prec.
P@1Glavas+SVR 0.21 0.13Wordnet+SVR 0.53 0.50Kauchak+SVR 0.70 0.69Simple PPDB 0.72 0.77Table 4: Precision of relative simplification rankings of threeexisting lexical simplification methods compared to the Sim-ple PPDB resource in terms of Average Precision and P@1(both range from 0 to 1 and higher is better).
All of the ex-isting methods were evaluated using the implementations asprovided in the LEXenstein toolkit.likely due to a combination of the additional fea-tures applied in Simple PPDB?s model (e.g.
wordembeddings) and the difference in training data(Simple PPDB?s model was trained on 11K para-phrase pairs with trinary labels, while the Horn etal.
(2014) model was trained on 500 words, eachwith a ranked list of paraphrases).
Table 5 pro-vides examples of the top-ranked simplificationcandidates proposed by each of the methods de-scribed.alarmGlavas enrage, perturb, stunWordNet horrify, dismay, alert, appall, appalKauchak pure, worryPPDB worry, concern, alertgenuineGlavas credible, sort, feign, phoney, good na-turedness, sincere, sincerely, insincere,bonafideWordNet real, actual, unfeigned, literal, echt, trueKauchak thermalPPDB true, real, actual, honest, sincereTable 5: Examples of candidate simplifications proposed bySimple PPDB and by three other generate-and-rank methods.Bold words were rated by humans to be simpler than the tar-get word.
Note that these candidates are judged on simplicity,not on their goodness as paraphrases.In addition, Simple PPDB offers the largestcoverage (Table 6).
It has a total vocabulary of624K unique words and phrases, and providesthe largest number of potential simplifications for146Avg.
PPs Totalper Input Vocab.Glavas+SVR ?
?Kauchak+SVR 4.4 127KWordnet+SVR 6.7 155KSimple PPDB 8.8 624KTable 6: Overall coverage of three existing lexical simplifica-tion methods compared to the Simple PPDB resource.
Glavasis marked as?
since it generates candidates based on near-ness in vector space, and in theory could generate as manywords/phrases as are in the vocabulary of the vector space.each target?
for the 100 targets drawn from theNewsela corpus, PPDB provided an average of 8.8candidates per target.
The next best generator, theWordNet-based system, produces only 6.7 candi-dates per target on average, and has a total vocab-ulary of only 155K words.4 ConclusionWe have described Simple PPDB, a subset of theParaphrase Database adapted for the task of textsimplification.
Simple PPDB is built by apply-ing state-of-the-art machine learned models forlexical simplification to the largest available re-source of lexical and phrasal paraphrases, result-ing in a web-scale resource capable of supportingresearch in data-driven methods for text simplifi-cation.
We have shown that Simple PPDB offerssubstantially increased coverage of both wordsand multiword phrases, while maintaining highquality compared to existing methods for lexicalsimplification.
Simple PPDB, along with the hu-man judgements collected as part of its creation, isfreely available with the publication of this paper.6AcknowledgmentsThis research was supported by a Facebook Fel-lowship, and by gifts from the Alfred P. SloanFoundation, Google, and Facebook.
This mate-rial is based in part on research sponsored by theNSF grant under IIS-1249516 and DARPA undernumber FA8750-13-2-0017 (the DEFT program).The U.S. Government is authorized to reproduceand distribute reprints for Governmental purposes.The views and conclusions contained in this pub-lication are those of the authors and should not beinterpreted as representing official policies or en-dorsements of DARPA and the U.S. Government.6http://www.seas.upenn.edu/?nlp/resources/simple-ppdb.tgzWe would especially like to thank Ani Nenkovafor suggesting this line of research and for pro-viding the initial ideas on which this work builds.We would also like to thank Courtney Napoles andWei Xu for valuable discussions, the anonymousreviewers for thoughtful comments, and the Ama-zon Mechanical Turk annotators for their contri-butions.ReferencesThorsten Brants and Alex Franz.
2006.
Web 1T5-gram Version 1.
Linguistic Data Consortium,Philadelphia.John Carroll, Guido Minnen, Darren Pearce, YvonneCanning, Siobhan Devlin, and John Tait.
1999.Simplifying text for language-impaired readers.
InProceedings of EACL, volume 99, pages 269?270.Will Coster and David Kauchak.
2011a.
Learning tosimplify sentences using Wikipedia.
In Proceedingsof the Workshop on Monolingual Text-To-Text Gen-eration, pages 1?9, Portland, Oregon, June.
Associ-ation for Computational Linguistics.William Coster and David Kauchak.
2011b.
SimpleEnglish Wikipedia: A new text simplification task.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 665?669, Portland,Oregon, USA, June.
Association for ComputationalLinguistics.Jan De Belder and Marie-Francine Moens.
2010.
Textsimplification for children.
In Proceedings of the SI-GIR workshop on accessible search systems, pages19?26.
ACM.Siobhan Devlin and John Tait.
1998.
The use ofa psycholinguistic database in the simplification oftext for aphasic readers.
Linguistic databases, pages161?173.Richard Evans, Constantin Orasan, and Iustin Dor-nescu.
2014.
An evaluation of syntactic simplifica-tion rules for people with autism.
In Proceedings ofthe 3rd Workshop on Predicting and Improving TextReadability for Target Reader Populations (PITR),pages 131?140.Lijun Feng.
2008.
Text simplification: A survey.
TheCity University of New York, Tech.
Rep.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In Proceedings of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 758?764, Atlanta, Georgia, June.Association for Computational Linguistics.147Goran Glava?s and Sanja?Stajner.
2015.
Simplifyinglexical simplification: Do we need simplified cor-pora?
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguisticsand the 7th International Joint Conference on Natu-ral Language Processing (Volume 2: Short Papers),pages 63?68, Beijing, China, July.
Association forComputational Linguistics.Colby Horn, Cathryn Manduca, and David Kauchak.2014.
Learning a lexical simplifier using Wikipedia.In Proceedings of the 52nd Annual Meeting of theAssociation for Computational Linguistics (Volume2: Short Papers), pages 458?463, Baltimore, Mary-land, June.
Association for Computational Linguis-tics.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Courtney Napoles and Mark Dredze.
2010.
Learn-ing Simple Wikipedia: A cogitation in ascertainingabecedarian language.
In Proceedings of the NAACLHLT 2010 Workshop on Computational Linguisticsand Writing: Writing Processes and Authoring Aids,pages 42?50, Los Angeles, CA, USA, June.
Associ-ation for Computational Linguistics.Gustavo Paetzold and Lucia Specia.
2015.
LEXen-stein: A framework for lexical simplification.
InProceedings of ACL-IJCNLP 2015 System Demon-strations, pages 85?90, Beijing, China, July.
Associ-ation for Computational Linguistics and The AsianFederation of Natural Language Processing.Gustavo Paetzold.
2015.
Reliable lexical simplifica-tion for non-native speakers.
In Proceedings of the2015 Conference of the North American Chapter ofthe Association for Computational Linguistics: Stu-dent Research Workshop, pages 9?16, Denver, Col-orado, June.
Association for Computational Linguis-tics.Ellie Pavlick and Ani Nenkova.
2015.
Inducing lexicalstyle properties for paraphrase and genre differen-tiation.
In Proceedings of the 2015 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 218?224, Denver, Colorado, May?June.
Association for Computational Linguistics.Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch,Benjamin Van Durme, and Chris Callison-Burch.2015.
PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, andstyle classification.
In Proceedings of the 53rd An-nual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Confer-ence on Natural Language Processing (Volume 2:Short Papers), pages 425?430, Beijing, China, July.Association for Computational Linguistics.Sarah E. Petersen and Mari Ostendorf.
2007.
Text sim-plification for language learners: a corpus analysis.In SLaTE, pages 69?72.
Citeseer.Luz Rello, Ricardo A. Baeza-Yates, and Horacio Sag-gion.
2013.
The impact of lexical simplificationby verbal paraphrases for people with and withoutdyslexia.
In CICLing (2), pages 501?512.Advaith Siddharthan, Ani Nenkova, and KathleenMcKeown.
2004.
Syntactic simplification for im-proving content selection in multi-document sum-marization.
In Proceedings of the 20th internationalconference on Computational Linguistics, page 896.Association for Computational Linguistics.Lucia Specia, Sujay Kumar Jauhar, and Rada Mihal-cea.
2012.
SemEval-2012 task 1: English lexi-cal simplification.
In *SEM 2012: The First JointConference on Lexical and Computational Seman-tics ?
Volume 1: Proceedings of the main conferenceand the shared task, and Volume 2: Proceedings ofthe Sixth International Workshop on Semantic Eval-uation (SemEval 2012), pages 347?355, Montr?eal,Canada, 7-8 June.
Association for ComputationalLinguistics.Sander Wubben, Antal Van Den Bosch, and EmielKrahmer.
2012.
Sentence simplification by mono-lingual machine translation.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics: Long Papers-Volume 1, pages1015?1024.
Association for Computational Linguis-tics.Wei Xu, Chris Callison-Burch, and Courtney Napoles.2015.
Problems in current text simplification re-search: New data can help.
Transactions of the As-sociation for Computational Linguistics, 3:283?297.Wei Xu, Courtney Napoles, Ellie Pavlick, QuanzeChen, and Chris Callison-Burch.
2016.
Optimizingstatistical machine translation for text simplification.Transactions of the Association for ComputationalLinguistics, 4.148
