HUGHES TRAINABLE TEXT SKIMMER :MUC-3 TEST RESULTS AND ANALYSI SCharles P .
DolanSeth R. GoldmanThomas V. CudaAlan M. NakamuraHughes Research Laboratories3011 Malibu Canyon Road M/S RL96Malibu, CA 90265Test resultsFigure 1 gives the official results for the Hughes Trainable Text Skimmer used for MUC 3(TTS-MUC3) .
TTS is a largely statistical system, using a K-Nearest Neighbor classifie rwith the output of a shallow parser as features.
(See the System Summary section of thi svolume for a detailed description of TTS-MUC3).
The performance, on a slot by slot basi sis, therefore, what one might expect: the pure set fills such as "Incident Type" and"Category" have much better performance than the string fills such as "Human Target ."
Inaddition, we can see that "Incident Date" and "Incident Location," for which special codewas written, have performance above that of the string fills.SLOTREC PRE OVG FAL---------------------------------- -template-id 74 53 47incident-date 41 55 1incident-type 46 63 0 1category 55 55 21 27indiv-perps 17 39 23org-perps 30 35 33perp-confidence 20 28 32 5phys-target-ids 18 37 33phys-target-num 18 41 29phys-target-types 13 28 33 1human-target-ids 25 17 65human-target-num 15 20 2 0human-target-types 31 20 65 1 0target-nationality 0 * * 0instrument-types 0 0 50 0incident-location 37 54 0phys-effects 19 35 50 2human-effects 5 12 58 2-----------------------------------MATCHED ONLY 41 36 3 8MATCHED/MISSING 31 36 3 8ALL TEMPLATES 31 22 62SET FILLS ONLY 29 34 412Figure 1 : Official TST2 Score reportDistribution of laborOne calendar month and approximately three (3) person months were spent on MUC3 .Before MUC3, we had constructed a text database facility and the pattern matcher used fo r7 6shallow parsing .
Therefore much of the time for MUC3 was spend evaluating alternative sfor the statistical engine.
Approximately 45% of the time was spent developing code forideas that were not used in the final system .
Of the remaining time, 30% was spen tdeveloping code to extract and format information for MUC3 templates (including code toparse the templates of the DEV corpus), 15% was spent coding and tuning the K-Neares tNeighbor classifier, and 10% was spent creating phrasal patterns, either by hand orextracting them automatically from the templates for the DEV corpus .Test settingsThe test settings for TTS-MUC3 were tuned to maximize recall .
This resulted in roughlyequal recall and precision .
Some results in the companion paper in the System Summar ysection of this volume indicate that we might tune TTS-MUC3 for higher precision at th eexpense of recall .
However, we believe that there are enough different algorithms tha tmight substantially improve the performance of TTS that evaluating such trade-offs i spremature .
For the official test, we used K=12 in the pattern classifier .
The patternclassifier returns a set of hypotheses for various set and string fills .
The hypotheses arereturned with strengths between 0 .0 and 1 .0 which are then compared to a threshold ; al lthe thresholds on the feature extraction were extremely low (e .g., 0.1) .Limiting factorsThe limiting factor for the Hughes TTS-MUC3 system was time .
The K-Nearest Neighborclassifier is surprisingly effective, but there are many variations that we did not have time totry.
With a small amount of extra time we could make small improvements there .
Inaddition, we suspect that our algorithm for grouping sentences into topics was responsibl efor many of our errors .
However, improving this portion of the system will take muc hmore time and, we believe, will require the addition of domain knowledge into theprocessing .TrainingThe training regimen was extremely simple .
A word frequency analysis was performed onthe DEV corpus, and we selected those words that occurred between 10 and 105 times a sour content bearing words, resulting in about 1000 such words .
These words were the ngrouped by hand into approximately 400 conceptual classes .
In addition, words wereadded to the lexicon for numbers, ordinals, roman numerals, compass points, etc .
Thelexicon and the DEV templates were used to drive the construction of phrases .
Phraseswere created from string fills by substituting conceptual classes for words .
For example ,"SIX JESUITS" would drive the creation of the phrase, ( : N UMBER - W: RELEGIOUS?ORDER?W) .
The type of the string fill served as the semantic feature fo rthe phrase .For some phrases there where conflicts, for example, many phrases that might be mappe dto :ACTIVE -MILITARY as a human target, might also be mapped to :STATE -SPONSORED-VIOLENCE-INDIV as a perpetrating individual .
For these phrases, themost frequent usage was chosen .
After creating a large number of phrases automaticall y(approximately 1000), a set of hand constructed phrases was added to augment and repai rthat set (approximately 200) .All the stories in the DEV corpus were used to build the case memory, however, thenumber of cases per different "Type of Incident" was limited to 35 .
This means that once35 cases of a particular incident type (i .e ., Murder) had been seen, future cases of this typewere ignored .
This attempt to balance the training data was necessary because the numbe r77of stories for each type of incident varied greatly .
By restricting the maximum stories pertopic, we tended to ignore many of the later stories in the training set.Domain independent modulesAll the modules in TTS-MUC3 are domain independent.
However, all the modules exceptthe date extraction module, require some amount of training .
Besides the training describedabove, the location extraction module requires a location database, including what location scontain what other locations .
The overhead for constructing such training sets an ddatabases is quite large, but we feel that for applications of sufficient leverage, good use rinterface design will ease the burden of constructing the training set and reduce the time fordeploying TTS in new domains .
In addition, integration with on-line data sources such a smap databases will eliminate the burden of creating special data files for natural languag eprocessing.78
