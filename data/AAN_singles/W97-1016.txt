Resolving PP attachment Ambiguities with Memory-BasedLearningJakub Zavrel, Walter Daelemans, Jorn VeenstraComputational Linguistics, Tilburg UniversityPO Box 90153, 5000 LE Tilburg, The Netherlandszavrel, walter, veenstra?kub, nlAbstractIn this paper we describe the applicationof Memory-Based Learning to the problemof Prepositional Phrase attachment disam-biguation.
We compare Memory-BasedLearning, which stores examples in mem-ory and generalizes by using intelligent sim-ilarity metrics, with a number of recentlyproposed statistical methods that are wellsuited to large numbers of features.
Weevaluate our methods on a common bench-mark dataset and show that our methodcompares favorably to previous methods,and is well-suited to incorporating vari-ous unconventional representations of wordpatterns such as value difference metricsand Lexical Space.1 Introduct ionA central issue in natural language analysis isstructural ambiguity resolution.
A sentence isstructurally ambiguous when it can be assignedmore than one syntactic structure.
The drosophilaof structural ambiguity resolution is PrepositionalPhrase (PP) attachment.
Several sources of infor-mation can be used to resolve PP attachment am-biguity.
Psycholinguistic theories have resulted indisambiguation strategies which use syntactic infor-mation only, i.e.
structural properties of the parsetree are used to choose between different attachmentsites.
Two principles based on syntactic informa-tion are Minimal Attachment (MA) and Late Clo-sure (LC) (Frazier, 1979).
MA tries to construct theparse tree that has the fewest nodes, whereas LCtries to attach new constituents as low in the parsetree as possible.
These strategies always choose thesame attachment regardless of the lexical content ofthe sentence.
This results in a wrong attachment inone of the following sentences:1 She cats pizza with a fork.2 She cats pizza with anchovies.In sentence 1, the PP "with a fork" is attached tothe verb "eats" (high attachment).
Sentence 2 dif-fers only minimally from the first sentence; here, thePP "with anchovies" does not attach to the verb butto the NP "pizza" (low attachment).
In languageslike English and Dutch, in which there is very littleovert case marking, syntactic information alone doesnot suffice to explain the difference in attachmentsites between such sentences.
The use of syntac-tic principles makes it necessary to re-analyse thesentence, using semantic or even pragmatic infor-mation, to reach the correct decision.
In the exam-ple sentences 1 and 2, the meaning of the head ofthe object of 'with' determines low or high attach-ment.
Several semantic riteria have been workedout to resolve structural ambiguities.
However, pin-ning down the semantic properties of all the wordsis laborious and expensive, and is only feasible in avery restricted omain.
The modeling of pragmaticinference seems to be even more difficult in a com-putational system.Due to the difficulties with the modeling of se-mantic strategies for ambiguity resolution, an at-tractive alternative is to look at the statistics ofword patterns in annotated corpora.
In such a cor-pus, different kinds of information used to resolveattachment ambiguity are, implicitly, represented inco-occurrence r gularities.
Several statistical tech-niques can use this information in learning attach-ment ambiguity resolution.Hindle and Rooth (1993) were the first to showthat a corpus-based approach to PP attachment am-biguity resolution can lead to good results.
Forsentences with a verb~noun attachment ambigu-ity, they measured the lexical association betweenthe noun and the preposition, and the verb andthe preposition in unambiguous sentences.
Theirmethod bases attachment decisions on the ratio andZavrcl, Daelemans ~4 Veenstra 136 Memory-Based PP AttachmentJakub Zavrel, Walter Daelemans and Jorn Veenstra (1997) Resolving PP attachment Ambiguities withMemory-Based Learning.
In T.M.
EUison (ed.)
CoNLL97: Computational Natural Language Learning, ACL~ 136-144.1997 Association for Computational Linguisticsreliability of these association strengths.
Note thatHindle and Rooth did not include information aboutthe second noun and therefore could not distinguishbetween sentence 1'and 2.
Their method is also dif-ficult to extend to'more elaborate combinations ofinformation sources.More recently, a number of statistical meth-ods better suited to larger numbers of featureshave been proposed for PP-attachment.
Brill andResnik (1994) appl!ed Error-Driven Transformation-Based Learning, Ratnaparkhi, Reynar and Roukos(1994) applied a Maximum Entropy model, Franz(1996) used a Loglinear model, and Collins andBrooks (1995) obtained good results using a Back-Off model.In this paper, we examine whether Memory-BasedLearning (MBL), a family of statistical methodsfrom the field of Machine Learning, can improve onthe performance of previous approaches.
Memory-Based Learning is described in Section 2.
In orderto make a fair comparison, we evaluated our meth-ods on the common benchmark dataset first usedin Ratnaparkhi, Reynar, and Roukos (1994).
in sec-tion 3, the experiments with our method on this dataare described.
An important advantage of MBL isits use of similarity-based reasoning.
This makes itsuited to the use of various unconventional represen-tations of word patterns (Section 2).
In Section 3 acomparison is provided between two promising rep-resentational forms, Section 4 contains acomparisonof our method to previous work, and we concludewith section 5.2 Memory-Based  Learn ingClassification-based machine learning algorithmscan be applied in learning disambiguation problemsby providing them with a set of examples derivedfrom an annotated corpus.
Each example consistsof an input vector representing the context of anattachment ambiguity in terms of features (e.g.
syn-tactic features, words, or lexical features in the caseof PP-attachment); and an output class (one of afinite number of possible attachment positions rep-resenting the correct attachment position for the in-put context).
Machine learning algorithms extrap-olate from the examples to new input cases, eitherby extracting regularities from the examples in theform of rules, decision trees, connection weights, orprobabilities in greedy learning algorithms, or bya more direct use of analogy in lazy learning algo-rithms.
It is the latter approach which we investigatein this paper.
It is our experience that lazy learn-ing (such as the Memory-Based Learning approachadopted here) is more effective for several anguage-processing problems (see Daelemans (1995) for anoverview) than more eager learning approaches.
Be-cause language-processing tasks typically can onlybe described as a complex interaction of regularities,subregularities and (families of) exceptions, toringall empirical data as potentially useful in analogicalextrapolation works better than extracting the mainregularities and forgetting the individual examples(Daelemans, 1996).Analogy from Nearest NeighborsThe techniques used are variants and extensions ofthe classic k-nearest neighbor (k-NN) classifier al-gorithm.
The instances of a task are stored in atable, together with the associated "correct" out-put.
When a new pattern is processed, the k nearestneighbors of the pattern are retrieved from memoryusing some similarity metric.
The output is deter-mined by extrapolation from the k nearest neigh-bors.
The most common extrapolation method ismajority voting which simply chooses the most com-mon class among the k nearest neighbors as an out-put.Similarity metricsThe most basic metric for patterns with symbolicfeatures i the Overlap metr ic  given in Equations 1and 2; where A(X, Y) is the distance between pat-terns X and Y, represented by n features, wi is aweight for feature i, and 5 is the distance per fea-ture.
The k-NN algorithm with this metric, andequal weighting for all features is called IB1 (Aha,Kibler, and Albert, 1991).
Usually k is set to 1.A(X, Y) = f i  w~ 5(x~, Yi) (1)i= lwhere:5(x,, y~) = 0 i f  x, = Yi, else 1 (2)This metric simply counts the r/umber of(mis)matching feature values in both patterns.
Ifno information about the importance of featuresis available, this is a reasonable choice.
But ifwe have information about feature relevance, wecan add linguistic bias to weight or select differentfeatures (Cardie, 1996).
An alternative, moreempiricist, approach is to look at the behavior offeatures in the set of examples used for training.We can compute Statistics about the relevanceof features by looking at which features are goodpredictors of the class labels.
Information Theoryprovides a useful tool for measuring feature rele-vance in this way, see Quinlan (1993).Zavrel, Daelemans ~4 Veenstra 137 Memory-Based PP AttachmentI n fo rmat ion  Ga in  (IG) weighting looks at eachfeature in isolation, and measures how much infor-mation it contributes to our knowledge of the cor-rect class label.
The Information Gain of featuref is measured by computing the difference in un-certainty (i.e.
entropy) between the situations with-out and with knowledge of the value of that feature(Equation 3):w!
= H(C) - Eveyf P(v) ?
H(CIv)sift) (3)si(f) = - E P(v)log 2 P(v) (4)veVfWhere C is the set of class labels, V!is the set of values for feature f ,  andH(C) : -EeeC P(c) log 2P(c) is the entropyof the class labels.
The probabilities are estimatedfrom relative frequencies in the training set.
Thenormalizing factor si(f) (split info) is included toavoid a bias in favor of features with more values.It represents the amount of information eeded torepresent all values of the feature (Equation 4).
Theresulting IG values can then be used as weights inEquation 1.
The k-NN algorithm with this metricis called IBI-IG, see Daelemans and van den Bosch(1992).The possibility of automatically determining therelevance of features implies that many different andpossibly irrelevant features can be added to the fea-ture set.
This is a very convenient methodology iftheory does not constrain the choice sufficiently be-forehand, or if we wish to measure the importanceof various information sources experimentally.MVDM and LexSpaceAlthough IBI-IG solves the problem of feature rele-vance to a certain extent, it does not take into ac-count that the symbols used as values in the inputvector features (in this case words, syntactic ate-gories, etc.)
are not all equally similar to each other.According to the Overlap metric, the words Japanand China are as similar as Japan and pizza.
Wewould like Japan and China to be more similar toeach other than Japan and pizza.
This linguisticknowledge could be encoded into the word represen-tations by hand, e.g.
by replacing words with se-mantic labels, but again we prefer a more empiricistapproach in which distances between values of thesame feature are computed ifferentially on the ba-sis of properties of the training set.
To this end, weuse the Modified Value Difference Metric (MVDM),see Cost and Salzberg (1993); a variant of a met-ric first defined in Stanfill and Waltz (1986).
Thismetric (Equation 5) computes the frequency distri-bution of each value of a feature over the categories.Depending on the similarity of their distributions,pairs of values are assigned a distance.5(v1, v2) = ~ IP(CdV1) - P(CdV2)\[ (5)i=1In this equation, V1 and V2 are two possible val-ues for feature f; the distance is the sum over all ncategories; and P (C~ \]1~) is estimated by the relativefrequency of the value ~ being classified as categoryi.In our PP-attachment problem, the effect of thismetric is that words (as feature values) are groupedaccording to the category distribution of the pat-terns they belong to.
It is possible to cluster thedistributions of the values over the categories, andobtain classes of similar words in this fashion.
Foran example of this type of unsupervised learning asa side-effect of supervised learning, see Daelemans,Berck, and Gillis (1996).
In a sense, the MVDMcan be interpreted as implicitly implementing a sta-tistically induced, distributed, non-symbolic repre-sentation of the words.
In this case, the categorydistribution for a specific word is its lexical represen-tation.
Note that the representation for each wordis entirely dependent on its behavior with respect oa particular classification task.In many practical applications of MB-NLP, weare confronted with a very limited set of examples.This poses a serious problem for the MVD metric.Many values occur only once in the whole dataset.
This means that if two such values occurwith the same class, the MVDM will regard themas identical, and if they occur with two differentclasses their distance will be maximal.
In manycases, the latter condition reduces the MVDM tothe overlap metric, and additionaly some caseswill be counted as an exact match on the basis ofvery shaky evidence.
It is, therefore, worthwileto investigate whether the value difference matrix5 (~,~)  can be reused from one task to another.This would make it possible to reliably estimate allthe 5 parameters on a task for which we have a largeamount of training material, and to profit fromtheir availability for the MVDM of a smaller domain.Such a possibility of reuse of lexical similarity isfound in the application of Lexical Space represen-tation (Schiitze, 1994, Zavrel and Veenstra, 1995).In LexSpace, each word is represented by a vector ofZavrel, Daelemans 8J Veenstra 138 Memory-Based PP Attachmentreal numbers that stands for a "fingerprint" of thewords' distributional behavior across local contextsin a large corpus.
The distances between vectors canbe taken as a me~ure  of similarity.
In Table 1, anumber of examples of nearest neighbors are shown.For each focus-word f ,  a score is kept of the num-ber of co-occurrences of words from a fixed set ofC context-words wl (1 < i < C) in a large corpus.Previous work by Hughes (1994) indicates that thetwo neighbors on the left and on the right (i.e.
thewords in positions n - 2, n - 1, n + 1, n + 2, rela-tive to word n) are a good choice of context.
Theposition of a word in Lexical Space is thus given bya four component vector, of which each componenthas as many dimensions as there are context words.The dimensions represent the conditional probabili-ties P(w~-2\[f)... P(w~+ilf).We derived the distributional vectors of all 71479unique words present in the 3 million words of WallStreet Journal text, taken from the ACL/DCI  CD-ROM I (1991).
For the contexts, i.e.
the dimen-sions of Lexical Space, we took the 250 most frequentwords.To reduce the 1000 dimensional Lexical Space vec-tors to a manageable format we applied PrincipalComponent Analysis 1 (PCA) to reduce them to amuch lower number of dimensions.
PCA accom-plishes the dimension reduction that preserves asmuch of the structure of the original data as pos-sible.
Using a measure of the correctness of the clas-sification of a word in Lexical Space with respect oa linguistic categorization (see Zavrel and Veenstra(1995)) we found that PCA can reduce the dimen-sionality from 1000 to as few as 25 dimensions withvirtually no loss, and sometimes even an improve-ment of the quality of the organization.Note that the LexSpace representations are taskindependent in that they only reflect the structure ofneighborhood relations between words in text.
How-ever, if the task at  hand has some positive relationto context prediction, Lexical Space representationsare useful.3 MBL  fo r  PP  a t tachmentThis section describes experiments with a numberof Memory-Based models for PP attachment disam-biguation.
The first model is based on the lexical in-formation only, i.e.
the attachment decision is madeby looking only at the identity of the words in thepattern.
The second model considers the issue oflex-1 Using the simplesvd package, which waskindly provided by Hinrich Schfitze.
This softwarecan be obtained from f tp : / / cs l i .
s tan ford .edu/pub/pr os it/papers/s implesvd/.ical representation in the MBL framework, by takingas features either task dependent (MVDM) or taskindependent (LexSpace) syntactic vector represen-tations for words.
The introduction of vector repre-senations leads to a number of modifications to thedistance metrics and extrapolation rules in the MBLframework.
A final experiment examines a numberof weighted voting rules.The experiments in this section are conductedon a simplified version of the "full" PP-attachmentproblem, i.e.
the attachment of a PP in the se-quence: VP NP PP.
The data consist of four-tuplesof words, extracted from the Wall Street JournalTreebank (Marcus, Santorini, and Marcinkiewicz,1993) by a group at IBM (Ratnaparkhi, Reynar, andRoukos, 1994).
2 They took all sentences that con-tained the pattern VP NP PP and extracted the headwords from the constituents, yielding a V N1 P N2pattern.
For each pattern they recorded whether thePP was attached to the verb or to the noun in thetreebank parse.
Example sentences 1 and 2 wouldthen become:3 eats, pizza, with, fork, Y.4 eats, pizza, with, anchovies, N.The data set contains 20801 training patterns,3097 test patterns, and an independent validationset of 4039 patterns for parameter optimization.
Ithas been used in statistical disambiguation meth-ods by Ratnaparkhi, Reynar, and Roukos (1994) andCollins and Brooks (1995); this allows a comparisonof our models to the methods they tested.
All of themodels described below were trained on all of thetraining examples and the results are given for the3097 test patterns.
For the benchmark comparisonwith other methods from the literature, we use onlyresults for which all parameters have been optimizedon the validation set.In addition to the computational work, Ratna-parkhi, Reynar, and Roukos (1994) performed astudy with three human subjects, all experiencedtreebank annotators, who were given a small ran-dom sample of the test sentences (either as four-tuples or as full sentences), and who had to give thesame binary decision.
The humans, when given thefour-tuple, gave the same answer as the Treebankparse 88.2 % of the time, and when given the wholesentence, 93.2 % of the time.
As a baseline, we canconsider either the Late Closure principle, which al-ways attaches to the noun and yields a score of only2 The dataset is avaliable fromftp://ftp, cis.
upenn, edu/pub/adwait/PPatt achDat a/.We would like to thank Michael Collins for pointing thisbenchmark out to us.Zavrel, Daelemans 86 Veenstra 139 Memory-Based PP Attachment"iN for(in)0.05 ~n si ce(in)0.10 at(in)0.11 after(in)0.11 under(in)0.11on(in)0.12 until(in) 0.12 by(in)0.13 among(in)0.14 before(in)0.16' "  GROUP nnnetwork(nn)0.08 firm(nn)0.11 measure(nn.
)0 11 package(nn)0.11 chain(nn)0.11\] club(np)0.11 bin(nn)0.11 partnership(nn)0.12 panel(nn)0.12 fund(nn)0.12J JAPAN npchina(rip)0.16 france(np)0.16 britain(np)0.19 canada(np)0.19 mexico(rip)0.19J india(rip)0.19 australia(np)0.20 korea(np)0.22 ital~(np)0.23 detroit (np)0.23Table 1: Some examples of the direct neighbors of words in a Lexical Space (context:250 lexicon:5000 norm:l).The 10 nearest neighbors of the word in upper case are listed by ascending distance.59.0 % correct, or the most likely attachment associ-ated with the preposition, which reaches an accuracyof 72.2 %.The training data for this task are rather sparse.Of the 3097 test patterns, only 150 (4.8 %) occurredin the training set; 791 (25.5 %) patterns had at least1 mismatching word with any pattern in the trainingset; 1963 (63.4 %) patterns at least 2 mismatches;and 193 (6.2 %) patterns at least 3 mismatches.Moreover, the test set contains many words that arenot present in any of the patterns in the training set.Table 2 shows the counts of feature values and un-known values.
This table also gives the InformationGain estimates of feature relevance.Over lap -Based  Mode lsIn a first experiment, we used the IB1 algorithmand the IBi- IG algorithm.
The results of these al-gorithms and other methods from the literature aregiven in Table 3.
The addition of IG weights clearlyhelps, as the high weight of the P feature in effect pe-nalizes the retrieval of patterns which do not matchin the preposition.
As we have argued in Zavrel andDaelemans (1997), this corresponds exactly to thebehavior of the Back-Off algorithm of Collins andBrooks (1995), so that it comes as no surprise thatthe accuracy of both methods is the same.
Notethat the Back-Off model was constructed after per-forming a number of validation experiments on held-out data to determine which terms to include and,more importantly, which to exclude from the back-off sequence.
This process is much more laboriousthan the automatic omputation of IG-weights onthe training set.The other methods for which results have beenreported on this dataset include decision trees,Maximum Entropy (Ratnaparkhi, Reynar, andRoukos, 1994), and Error-Driven Transformation-Based Learning (Brill and Resnik, 1994), 3 whichwere clearly outperformed by both IB1 and IBI-IG,even though e.g.
Brill ~ Resnik used more elaboratefeature sets (words and WordNet classes).
AddingaThe results of Brill's method on the present bench-mark were reconstructed by Collins and Brooks (1995).more elaborate features is also possible in the MBLframework.
In this paper, however, we focus on moreeffective use of the existing features.
Because theOverlap metric neglects information about the de-gree of mismatch if feature-values are not identical,it is worthwhile to look at more finegrained repre-sentations and metrics.Continuous Vector Representations forWordsIn experiments with Lexical Space representations,every word in a pattern was replaced by its PCAcompressed LexSpace vector, yielding patterns with25x4 numerical features and a discrete target cate-gory.
The distance metric used was the sum of theLexSpace vector distance per feature, where the dis-tance between two vectors is computed as one minusthe cosine, normalized by the cumulative norm.
Be-cause no two patterns have the same distance in thiscase, to use only the nearest neighbor(s) means ex-trapolating from exactly one nearest neighbor.In preliminary experiments, this was found to givebad results, so we also experimented with varioussettings for k : the parameter that determines thenumber of neighbors considered for the analogy.
Thesame was done for the MVDM metric which hasa similar behavior.
We found that LexSpace per-formed best when k was set to 13 (83.3 % correct);MVDM obtained its best score when k was set to50 (80.5 % correct).
Although these parameterswere found by optimization on the test set, we cansee in Figure 1 that LexSpace actually outperformsMVDM for all settings of k. Thus, the represen-tations from LexSpace which represent he behav-ior of the values independent of the requirementsof this particular classification task outperform thetask specific representations u ed by MVDM.
Thereason is that the task specific representations arederived only from the small number of occurrencesof each value in the training set, whereas the amountof text available to refine the LexSpace vectors ispractically unlimited.
Lexical Space however, doesnot outperform the simple Overlap metric (83.7 %correct) in this form.
We suspected that the reasonfor this is the fact that when continuous represen-Zavrel, Daelemans ~ Veenstra 140 Memory-Based PP AttachmentFeatureVN1PN2Ctrain values total va lues  unknown IG weight3243 3475 232 0.034315 4613 298 0.0366 69 3 0.105451 5781 330 0.032 2 0 -Table 2: Statistics of the PP attachment data set.MethodOverlapOverlap IG ratioC4.5Maximum EntropyTransformationsBack-off modelLate ClosureMost Likely for each Ppercent83.7 %84.1%79.7 %77.7 %81.9 %84.1%59.0 %72.0 %correctTable 3: Scores on the Ratnaparkhi et al PP-attachment test set (see text); the scores of Maximum Entropyare taken from Ratnaparkhi et al (1994); the scores of Transformations and Back-off are taken from Collins& Brooks (1995).
The C4.5 decision tree results, and the baselines have been computed by the authors.tations are used, the number of neighbors is exactlyfixed to k, whereas the number of neighbors used inthe Overlap metric is, in effect, dependent on thespecificity of the match.Weighted VotingThis section examines possibilities for improving thebehavior of LexSpace vectors for MBL by consider-ing various weighted voting methods.The fixed number of neighbors in the continuousmetrics can result in an oversmoothing effect.
Thek-NN classifier tries to estimate the conditionalclass probabilities from samples in a local region ofthe data space.
The radius of the region is deter-mined by the distance of the k-furthest neighbor.If k is very small and i) the nearest neighborsare not nearby due to data sparseness, or ii) thenearest neighbor classes are unreliable due to noise,the "local" estimate tends to be very poor, asillustrated in Figure 1.
Increasing k and thus takinginto account a larger region around the query inthe dataspace makes it possible to overcome thiseffect by smoothing the estimate.
However, whenthe majority voting method is used, smoothing caneasily become oversmoothing, because the radiusof the neighborhood is as large as the distance ofthe k'th nearest neighbor, irrespective of the localproperties of the data.
Selected points from beyondthe "relevant neighborhood" will receive a weightequal to the close neighbors in the voting function,which can result in unnecessary classification errors.A solution to this problem is the use of a weightedvoting rule which weights the vote of each of thenearest neighbors by a function of their distance tothe test pattern (query).
This type of voting rulewas first proposed by Dudani (1976).
In his scheme,the nearest neighbor gets a weight of 1, the furthestneighbor a weight of 0, and the other weights arescaled linearly to the interval in between.dk-dj if dk ~ dl 4k-d, (6)Wj = 1 if dk = dlwhere dj is the distance to the query of the j ' thnearest neighbor, dl the distance of the nearestneighbor, and dk the distance of the furthest (k'th)neighbor.Dudani further proposed the inverse distanceweight (Equation 7), which has recently become pop-ular in the MBL literature (Wettschereck, 1994).
InEquation 7, a small constant is usually added to thedenominator to avoid division by zero.1 (7) wj= ~jAnother weighting function considered here isbased on the work of Shepard (1987), who argues fora universal perceptual law, in which the relevance ofa previous timulus for the generalization to a newstimulus is an exponentially decreasing function ofits distance in a psychological space.
This gives theweighed voting function of Equation 8, where o~ andfl are constants determining the slope and the powerof the exponential decay function.
In the experi-ments reported below, oe = 3.0 and fl = 1.0.Zavrel, Daelemans ~ Veenstra 141 Memory-Based PP Attachment8477PP"pp.l~.,space"pp.nwdm" --*-/42'o .
.
.
.
.
.
:o 1 30 40 50 60 70 80 100kFigure 1: Accuracy on the PP-attachment test set of of MVDM and LexSpace representations as a functionof k, the number of nearest neighbors.MethodLexSpace (Dudani, k=30)LexSpace (Dudani, k=5O, IG)% correct84.2 %84.4 %Table 4: Scores on the Ratnaparkhi et al PP-attachment test set with Lexical Space representa-tions.
The values of k, the voting function, and theIG weights were determined on the training and val-idation sets.= (8)Figure 2 shows the results on the test set for a widerange of k for these voting methods when applied tothe LexSpace represented PP-attachment dataset.With the inverse distance weighting function theresults are better than with majority voting, buthere, too, we see a steep drop for k's larger than 17.Using Dudani's weighting function, the results be-come optimal for larger values of k, and remain goodfor a wide range of k values.
Dudani's weightingfunction also gives us the best overall result, i.e.
if weuse the best possible setting for k for each method,as determined by performance on the validation set(see Table 4).The Dudani weighted k-nearest neighbor classi-fier (k=30) slightly outperforms Collins ~ Brooks'(1995) Back-Off model.
A luther small increasewas obtained by combining LexSpace representa-tions with IG weighting of the features, and Dudani'sweighted voting function.
Although the improve-ment over Back-Off is quite limited, these results arenonetheless interesting because they show that MBLcan gain from the introduction of extra informationsources, whereas this is very difficult in the Back-Off algorithm.
For comparison, consider that theperformance of the Maximum Entropy model withdistributional word-class features is still only 81.6%on this data.4 D iscuss ionIf we compare the accuracy of humans on theV,N,P,N patterns (88.2 % correct) with that of ourmost accurate method (84.4 %), we see that theparadigm of learning disambiguation methods fromcorpus statistics offers good prospects for an effec-tive solution to the problem.
After the initial effortby Hindle and Rooth (1993), it has become clearthat this area needs tatistical methods in which aneasy integration of many information sources is pos-sible.
A number of methods have been applied tothe task with this goal in mind.Brill and Resnik (1994) applied Error-DrivenTransformation-Based Learning to this task, usingthe verb, nounl, preposition, and noun2 features.Their method tries to maximize accuracy with aminimal amount of rules.
They found an increasein performance by using semantic information fromWordNet.
Ratnaparkhi, Reynar, and Roukos (1994)used a Maximum Entropy model and a decision treeon the dataset hey extracted from the Wall StreetJournal corpus.
They also report performance gainswith word features derived by an unsupervised clus-tering method.
Ratnaparkhi et al ignored lowfrequency events.
The accuracy of these two ap-proaches is not optimal.
This is most likely dueto the fact that they treat low frequency events asnoise, though these contain a lot of information i  asparse domain such as PP-attachment.
Franz (1996)used a Loglinear model for PP attachment.
The fea-tures he used were the preposition, the verb levelZavrel, Daelemans 8J Veenstra 142 Memory-Based PP Attachmentppg1.58171~.50 10 20 30 40 50 60 70 80 90 100kFigure 2: Accuracy on the PP-attachment test set of various voting methods as a function of k, the numberof nearest neighbors.
(the lexical association between the verb and thepreposition), the noun level (idem dito for nounl),the noun tag (POS-tag for nounl), noun definite-ness (of nounl), and the PP-object tag (POS-tagfor noun2).
A Loglinear model keeps track of theinteraction between all the features, though at afairly high computational cost.
The dataset hatwas used in Franz' work is no longer available, mak-ing a direct comparison of the performance impos-sible.
Collins and Brooks (1995) used a Back-Offmodel, which enables them to take low frequency ef-fects into account on the Ratnaparkhi dataset (withgood results).
In Zavrel and Daelemans (1997) it isshown that Memory-Based and Back-Off type meth-ods are closely related, which is mirrored in the per-formance l vels.
Collins and Brooks got slightly bet-ter results (84.5 %) after reducing the sparse dataproblem by preprocessing the dataset, e.g.
replacingall four-digit words with 'YEAR'.
The experimentswith Lexical Space representations have as yet notshown impressive performance gains over Back-Off,but they have demonstrated that the MBL frame-work is well-suited to experimentation with rich lex-ical representations.5 Conc lus ionWe have shown that our MBL approach isvery com-petent in solving attachment ambiguities; it achievesbetter generalization performance than many previ-ous statistical approaches.
Moreover, because wecan measure the r'elevance of the features using aninformation gain metric (IBI-IG), we are able to addfeatures without a high cost in model selection or anexplosion in the number of parameters.An additional advantage of the MBL approach isthat, in contrast to the other statistical approaches,it is founded in the use of similarity-based reasoning.Therefore, it makes it possible to experiment withdifferent ypes of distributed non-symbolic lexicalrepresentations extracted from corpora using unsu-pervised learning.
This promises to be a rich sourceof extra information.
We have also shown that taskspecific similarity metrics such as MVDM are sen-sitive to the sparse data problem.
LexSpace is lesssensitive to this problem because of the large amountof data which is available for its training.AcknowledgementsThis research was done in the context of the "Induc-tion of Linguistic Knowledge" research programme,partially supported by the Foundation for Lan-guage Speech and Logic (TSL), which is funded bythe Netherlands Organization for Scientific Research(NWO).ReferencesAha, D., D. Kibler, and M. Albert.
1991.
Instance-based learning algorithms.
Machine Learning,6:37-66.Brill, E. and P. Resnik.
1994.
A rule-based ap-proach to prepositional phrase attachment dis-ambiguation.
In Proc.
of 15th annual confer-ence on Computational Linguistics.Cardie, Claire.
1996.
Automatic feature set selectionfor case-based learning of linguistic knowledge.In Proc.
of Conference on Empirical Methodsin NLP.
University of Pennsylvania.Collins, M.J and J. Brooks.
1995.
Preposi-tional phrase attachment through a backed-offZavrel, Daelemans ~ Veenstra 143 Memory-Based PP Attachmentmodel.
In Proc.
of Third Workshop on VeryLarge Corpora, Cambridge.Cost, S. and S. Salzberg.
1993.
A weighted near-est neighbour algorithm for learning with sym-bolic features.
Machine Learning, 10:57-78.Daelemans, W. 1995.
Memory-based lexical acqui-sition and processing.
In P. Steffens, editor,Machine Translation and the Lexicon, volume898 of Lecture Notes in Artificial Intelligence.Springer-Verlag, Berlin, pages 85-98.Daelemans, W. 1996.
Abstraction considered harm-ful: Lazy learning of language processing.
InProc.
of 6th Belgian-Dutch Conference on Ma-chine Learning, pages 3-12.
Benelearn.Daelemans, Walter, Peter Berck, and Steven Gillis.1996.
Unsupervised iscovery of phonologicalcategories through supervised learning of mor-phological rules.
In Proc.
of 16th Int.
Conf.on Computational Linguistics, pages 95-100.Center for Sprogteknologi.Daelemans, Walter and Antal van den Bosch.
1992.Generalisation performance of backpropaga-tion learning on a syllabification task.
In Proc.of TWLT3: Connectionism and NLP, pages27-37.
Twente University.Dudani, S.A. 1976.
The distance-weighted k-nearestneighbor ule.
In IEEE Transactions on Sys-tems, Man, and Cybernetics, volume SMC-6,pages 325-327.Franz, A.
1996.
Learning PP attachment from cor-pus statistics.
In S. Wermter, E. Riloff, andG.
Scheler, editors, Connectionist, Statistical,and Symbolic Approaches to Learning for Nat-ural Language Processing, volume 1040 of Lec-ture Notes in Artificial Intelligence.
Springer-Verlag, New York, pages 188-202.Frazier, L. 1979.
On Comprehending Sentences:Syntactic Parsing Strategies.
Ph.d thesis, Uni-versity of Connecticut.Hindle, D. and M. Rooth.
1993.biguity and lexical relations.Linguistics, 19:103-120.Structural am-ComputationalHughes, J.
1994.
Automatically Acquiring a Classi-fication of Words.
Ph.d thesis, School of Com-puter Studies, The University of Leeds.Marcus, M., B. Santorini, and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of en-glish: The penn treebank.
Computational Lin-guistics, 19(2):313-330.Quinlan, J.R. 1993. c4.5: Programs for MachineLearning.
San Marco, CA: Morgan Kaufmann.Ratnaparkhi, A., J. Reynar, and S. Roukos.
1994.A maximum entropy model for prepositionalphrase attachment.
In Workshop on HumanLanguage Technology, Plainsboro, N J, March.ARPA.Schfitze, H. 1994.
Distributional part-of-speech tag-ging.
In Proc.
of 7th Conference of the Euro-pean Chapter of the Association for Computa-tional Linguistics, Dublin, Ireland.Shepard, R.N.
1987.
Toward a universal law of gen-eralization for psychological science.
Science,237:1317-1228.Stanfill, C. and D. Waltz.
1986.
Toward memory-based reasoning.
Communications of theACM, 29(12):1213-1228, December.Wettschereck, D. 1994.
A study of distance-basedmachine learning algorithms.
Ph.d thesis, Ore-gon State University.Zavrel, J. and W. Daelemans.
1997.
Memory-based learning: Using similarity for smooth-ing.
In Proc.
of 35th annual meeting of theA CL, Madrid.Zavrel, J. and J. Veenstra.
1995.
The language nvi-ronment and syntactic word class acquisition.In F. Wijnen and C. Koster, editors, Proc.
ofGroningen Assembly on Language Acquisition(GALA95), Groningen.Zavrel, Daelemans 8J Veenstra 144 Memory-Based pP Attachment
