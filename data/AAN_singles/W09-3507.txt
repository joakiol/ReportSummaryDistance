Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 40?43,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPA Language-Independent Transliteration Schema Using CharacterAligned Models At NEWS 2009Praneeth Shishtla, Surya Ganesh V, Sethuramalingam Subramaniam, Vasudeva VarmaLanguage Technologies Research Centre,IIIT-Hyderabad, Indiapraneethms@students.iiit.ac.in{suryag,sethu}@research.iiit.ac.in, vv@iiit.ac.inAbstractIn this paper we present a statisticaltransliteration technique that is languageindependent.
This technique uses statis-tical alignment models and ConditionalRandom Fields (CRF).
Statistical align-ment models maximizes the probability ofthe observed (source, target) word pairsusing the expectation maximization algo-rithm and then the character level align-ments are set to maximum posterior pre-dictions of the model.
CRF has efficienttraining and decoding processes which isconditioned on both source and target lan-guages and produces globally optimal so-lution.1 IntroductionA significant portion of out-of-vocabulary (OOV)words in machine translation systems, informationextraction and cross language retrieval models arenamed entities (NEs).
If the languages are writtenin different scripts, these named entities must betransliterated.
Transliteration is defined as the pro-cess of obtaining the phonetic translation of namesacross languages.
A source language word canhave more than one valid transliteration in the tar-get language.
In areas like Cross Language Infor-mation Retrieval (CLIR), it is important to gener-ate all possible transliterations of a Named Entity.Most current transliteration systems use a gen-erative model for transliteration such as freelyavailable GIZA++1 (Och and Ney , 2000), animplementation of the IBM alignment mod-els (Brown et al, 1993) and HMM alignmentmodel.
These systems use GIZA++ to get charac-ter level alignments from word aligned data.
The1http://www.fjoch.com/GIZA++.htmltransliteration system (Nasreen and Larkey , 2003)is built by counting up the alignments and convert-ing the counts to conditional probabilities.In this paper, we describe our participationin NEWS 2009 Machine Transliteration SharedTask (Li et al, 2009).
We present a simple statis-tical, language independent technique which usesstatistical alignment models and Conditional Ran-dom Fields (CRFs) (Hanna , 2004).
Using thistechnique a desired number of transliterations aregenerated for a given word.2 Previous workOne of the works on Transliteration is done byArababi et al (Arababi et.
al., 1994).
Theymodel forward transliteration through a combina-tion of neural net and expert systems.
Work inthe field of Indian Language CLIR was done byJaleel and Larkey (Larkey et al, 2003).
Theydid this based on their work in English-Arabictransliteration for CLIR (Nasreen and Larkey ,2003).
Their approach was based on HMM us-ing GIZA++ (Och and Ney , 2000).
Prior work inArabic-English transliteration for machine trans-lation purpose was done by Arababi (Arbabi et al,1994).
They developed a hybrid neural networkand knowledge-based system to generate multi-ple English spellings for Arabic person names.Knight and Graehl (Knight and Graehl , 1997) de-veloped a five stage statistical model to do backtransliteration, that is, recover the original En-glish name from its transliteration into JapaneseKatakana.
Stalls and Knight (Stalls and Knight ,1998) adapted this approach for back translitera-tion from Arabic to English of English names.
Al-Onaizan and Knight (Onaizan and Knight , 2002)have produced a simpler Arabic/English translit-erator and evaluates how well their system canmatch a source spelling.
Their work includes an40evaluation of the transliterations in terms of theirreasonableness according to human judges.
Noneof these studies measures their performance on aretrieval task or on other NLP tasks.
Fujii andIshikawa (Fujii and Ishikawa , 2001) describe atransliteration system for English-Japanese CLIRthat requires some linguistic knowledge.
Theyevaluate the effectiveness of their system on anEnglish-Japanese CLIR task.3 Problem DescriptionThe problem can be stated formally as a se-quence labeling problem from one language al-phabet to other.
Consider a source language wordx1x2..xi..xN where each xi is treated as a wordin the observation sequence.
Let the equivalenttarget language orthography of the same word bey1y2..yi..yN where each yi is treated as a label inthe label sequence.
The task here is to generate avalid target language word (label sequence) for thesource language word (observation sequence).x1 ??????
y1x2 ??????
y2.
?????
?- ..
?????
?- ..
?????
?- .xN ??????
yNHere the valid target language alphabet (yi) for asource language alphabet (xi) in the input sourcelanguage word may depend on various factors like1.
The source language alphabet in the inputword.2.
The context (alphabets) surrounding sourcelanguage alphabet (xi) in the input word.3.
The context (alphabets) surrounding targetlanguage alphabet (yi) in the desired outputword.4 Transliteration using alignment modelsand CRFOur approach for transliteration is dividedinto two phases.
The first phase inducescharacter alignments over a word-alignedbilingual corpus, and the second phase usessome statistics over the alignments to translit-erate the source language word and generatethe desired number of target language words.The selected statistical model for transliterationis based on a combination of statistical alignmentmodels and CRF.
The alignment models maximizethe probability of the observed (source, target)word pairs using the expectation maximizationalgorithm.
After the maximization process iscomplete, the character level alignments areset to maximum posterior predictions of themodel.
This alignment is used to get characterlevel alignment of source and target languagewords.
From the character level alignmentobtained we compare each source languagecharacter to a word and its corresponding tar-get language character to a label.
Conditionalrandom fields (CRFs) are a probabilistic frame-work for labeling and segmenting sequentialdata.
We use CRF to generate target languageword (similar to label sequence) from sourcelanguage word (similar to observation sequence).CRFs are undirected graphical models whichdefine a conditional distribution over a label se-quence given an observation sequence.
We defineCRFs as conditional probability distributionsP (Y |X) of target language words given sourcelanguage words.
The probability of a particulartarget language word Y given source languageword X is the normalized product of potentialfunctions each of the forme(?j?jtj(Yi?1,Yi,X,i))+(?k?ksk(Yi,X,i))where tj(Yi?1, Yi, X, i) is a transition featurefunction of the entire source language word andthe target language characters at positions i andi?
1 in the target language word; sk(Yi, X, i) is astate feature function of the target language wordat position i and the source language word; and ?jand ?k are parameters to be estimated from train-ing data.Fj(Y,X) =n?i=1fj(Yi?1, Yi, X, i)where each fj(Yi?1, Yi, X, i) is either a statefunction s(Yi?1, Yi, X, i) or a transition functiont(Yi?1, Yi, X, i).
This allows the probability of atarget language word Y given a source languageword X to be written asP (Y |X,?)
= (1Z(X))e(?
?jFj(Y,X))Z(X) is a normalization factor.415 Our Transliteration systemThe whole model has three important phases.
Twoof them are off-line processes and the other is a on-line process.
The two off-line phases are prepro-cessing the parallel corpora and training the modelusing CRF++2 (Lafferty et al, 2001).
CRF++ is asimple, customizable, and open source implemen-tation of Conditional Random Fields (CRFs) forsegmenting/labeling sequential data.
The on-linephase involves generating desired number of targetlanguage transliterations (UTF-8 encoded) for thegiven English input word.
In our case, the sourceis always an English word.
The same system isused for every language pair which makes it a lan-guage independent.
The target languages consistof Chinese, Hindi, Kannada Tamil and Russianwords.5.1 PreprocessingThe training file is converted into a format re-quired by CRF++.
The sequence of steps in pre-processing are1.
Both source and target language words wereprefixed with a begin symbol B and suffixedwith an end symbol E which correspond tostart and end states.
English words were con-verted to lower case.2.
The training words were segmented in tounigrams and the source-target word pairswere aligned using GIZA++ (IBM model1,HMM alignment model, IBM model3 andIBM model4).3.
The alignment consist of NULLs on sourcelanguage i.e., a target language unigram isaligned to NULL on the source language.These NULLs are problematic during on-line phase (as positions of NULLs are un-known).
So, these NULLs are removed byappending the target language unigram to theunigram of its previous alignment.
For exam-ple, the following alignment,k ?
KNULL ?
Atransforms to -k ?
KA2http://crfpp.sourceforge.net/So, in the final alignment, the source side al-ways contains unigrams and the target sidemight contain ngrams which depends on al-phabet size of the languages.
These threesteps are performed to get the character levelalignment for each source and target lan-guage training words.4.
This final alignment is transformed to train-ing format as required by CRF++ to work.In the training format, a source language un-igram aligned to a target language ngram iscalled a token.
Each token must be repre-sented in one line, with the columns sepa-rated by white space (spaces or tabular char-acters).
Each token should have equal num-ber of columns.5.2 Training PhaseThe preprocessing phase converts the corpus intoCRF++ input file format.
This file is used totrain the CRF model.
The training requires a tem-plate file which specifies the features to be selectedby the model.
The training is done using Lim-ited memory Broyden-Fletcher-Goldfarb-Shannonmethod (L-BFGS) (Liu and Nocedal, 1989) whichuses quasi-newton algorithm for large scale nu-merical optimization problem.
We used Englishcharacters as features for our model and a windowsize of 5.5.3 TransliterationFor a language pair, the list of English words thatneed to be transliterated is taken.
These words areconverted into CRF++ test file format and translit-erated using the trained model which gives the topn probable English words.
CRF++ uses forwardViterbi and backward A* search whose combina-tion produces the exact n-best results.
This processis repeated for all the five language pairs.6 ResultsIn this section, we present the results of our par-ticipation in the NEWS-2009 shared task.
Weconducted our experiments on five language pairsnamely English-Chinese (Li et al, 2004), English-{Hindi, Kannada, Tamil, Russian} (Kumaran andKellner , 2007).
As specified in NEWS 2009 Ma-chine Transliteration Shared Task (Li et al, 2009),we submitted our standard runs on all the five lan-guage pairs.
Table 1 shows the results of our sys-tem.42Language Pair Accuracy in top-1 Mean F-score MRR MAPref MAP10 MAPsysEnglish-Tamil 0.406 0.894 0.542 0.399 0.193 0.193English-Hindi 0.407 0.877 0.544 0.402 0.195 0.195English-Russian 0.548 0.916 0.640 0.548 0.210 0.210English-Chinese 0.493 0.804 0.600 0.493 0.192 0.192English-Kannada 0.350 0.864 0.482 0.344 0.175 0.175Table 1: Transliteration results for the language pairs7 ConclusionIn this paper, we have described our translitera-tion system build on a discriminative model usingCRF and statistical alignment models.
As men-tioned earlier, our system is language independentand works on any language pair provided parallelword lists are available for training in the particu-lar language pair.
The main advantage of our sys-tem is that we use no language-specific heuristicsin any of our modules and hence it is extensible toany language-pair with least effort.ReferencesA.
Kumaran, Tobias Kellner.
2007.
A generic frame-work for machine transliteration, Proc.
of the 30thSIGIR.A.
L. Berger.
1997.
The improved iterative scalingalgorithm: A gentle introduction.Arbabi, M. and Fischthal, S. M. and Cheng, V. C. andBart, E. 1994.
Algorithms for Arabic name translit-eration, IBM Journal of Research And Development.Al-Onaizan Y, Knight K. 2002.
Machine translation ofnames in Arabic text.
Proceedings of the ACL con-ference workshop on computational approaches toSemitic languages.Arababi Mansur, Scott M. Fischthal, Vincent C. Cheng,and Elizabeth Bar.
1994.
Algorithms for Arabicname transliteration.
IBM Journal of research andDevelopment.D.
C. Liu and J. Nocedal.
1989.
On the limited memoryBFGS method for large-scale optimization, Math.Programming 45 (1989), pp.
503?528.Fujii Atsushi and Tetsuya Ishikawa.
2001.Japanese/English Cross-Language InformationRetrieval: Exploration of Query Translation andTransliteration.
Computers and the Humanities,Vol.35, No.4, pp.389-420.H.
M. Wallach.
2002.
Efficient training of conditionalrandom fields.
Masters thesis, University of Edin-burgh.HannaM.Wallach.
2004.
Conditional Random Fields:An Introduction.Haizhou Li, A Kumaran, Min Zhang, Vladimir Pervou-chine.
2009.
Whitepaper of NEWS 2009 MachineTransliteration Shared Task.
Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS2009), Singapore.Haizhou Li, A Kumaran, Vladimir Pervouchine, MinZhang.
2009.
Report on NEWS 2009 MachineTransliteration Shared Task.
Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS2009), Singapore.Haizhou Li, Min Zhang, Jian Su.
2004.
A joint sourcechannel model for machine transliteration.
Proc.
ofthe 42nd ACL.J.
Darroch and D. Ratcliff.
1972.
Generalized iterativescaling for log-linear models.
The Annals of Mathe-matical Statistics, 43:14701480.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
ofICML, pp.282-289.Knight Kevin and Graehl Jonathan.
1997.
Machinetransliteration.
In Proceedings of the 35th AnnualMeeting of the Association for Computational Lin-guistics, pp.
128-135.
Morgan Kaufmann.Larkey, Connell,AbdulJaleel.
2003.
Hindi CLIR inThirty Days.Nasreen Abdul Jaleel and Leah S. Larkey.
2003.Statistical Transliteration for English-Arabic CrossLanguage Information Retrieval.Och Franz Josef and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
Proc.
of the 38th An-nual Meeting of the Association for ComputationalLinguistics, pp.
440-447, Hong Kong, China.P.
F. Brown, S. A. Della Pietra, and R. L. Mercer.1993.
The mathematics of statistical machine trans-lation: Parameter estimation.
Computational Lin-guistics, 19(2):263-311.Phil Blunsom and Trevor Cohn.
2006.
DiscriminativeWord Alignment with Conditional Random Fields.Stalls Bonnie Glover and Kevin Knight.
1998.
Trans-lating names and technical terms in Arabic text.43
