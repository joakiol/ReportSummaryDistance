Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 67?76,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsSyntactic and Semantic Structure for Opinion Expression DetectionRichard Johansson and Alessandro MoschittiDISI, University of TrentoVia Sommarive 14 Povo, 38123 Trento (TN), Italy{johansson, moschitti}@disi.unitn.itAbstractWe demonstrate that relational featuresderived from dependency-syntactic andsemantic role structures are useful for thetask of detecting opinionated expressionsin natural-language text, significantly im-proving over conventional models basedon sequence labeling with local features.These features allow us to model the wayopinionated expressions interact in a sen-tence over arbitrary distances.While the relational features make the pre-diction task more computationally expen-sive, we show that it can be tackled effec-tively by using a reranker.
We evaluatea number of machine learning approachesfor the reranker, and the best model re-sults in a 10-point absolute improvementin soft recall on the MPQA corpus, whiledecreasing precision only slightly.1 IntroductionThe automatic detection and analysis of opinion-ated text ?
subjectivity analysis ?
is potentiallyuseful for a number of natural language processingtasks.
Examples include retrieval systems answer-ing queries about how a particular person feelsabout a product or political question, and varioustypes of market analysis tools such as review min-ing systems.A primary task in subjectivity analysis is tomark up the opinionated expressions, i.e.
thetext snippets signaling the subjective content ofthe text.
This is necessary for further analysis,such as the determination of opinion holder andthe polarity of the opinion.
The MPQA corpus(Wiebe et al, 2005), a widely used corpus anno-tated with subjectivity information, defines twotypes of subjective expressions: direct subjectiveexpressions (DSEs), which are explicit mentionsof opinion, and expressive subjective elements(ESEs), which signal the attitude of the speakerby the choice of words.
DSEs are often verbs ofstatement and categorization, where the opinionand its holder tend to be direct semantic argumentsof the verb.
ESEs, on the other hand, are less easyto categorize syntactically; prototypical exampleswould include value-expressing adjectives suchas beautiful, biased, etc.
In addition to DSEs andESEs, the MPQA corpus also contains annotationfor non-subjective statements, which are referredto as objective speech events (OSEs).
Examples(1) and (2) show two sentences from the MPQAcorpus where DSEs and ESEs have been manuallyannotated.
(1) For instance, he [denounced]DSE as a [humanrights violation]ESE the banning and seizure ofsatellite dishes in Iran.
(2) This [is viewed]DSE as the [mainimpediment]ESE to the establishment of po-litical order in the country .The task of marking up these expressions hasusually been approached using straightforwardsequence labeling techniques using simple fea-tures in a small contextual window (Choi et al,2006; Breck et al, 2007).
However, due to thesimplicity of the feature sets, this approach failsto take into account the fact that the semanticand pragmatic interpretation of sentences is notonly determined by words but also by syntacticand shallow-semantic relations.
Crucially, takinggrammatical relations into account allows us tomodel how expressions interact in various waysthat influence their interpretation as subjectiveor not.
Consider, for instance, the word said inexamples (3) and (4) below, where the interpre-tation as a DSE or an OSE is influenced by thesubjective content of the enclosed statement.67(3) ?We will identify the [culprits]ESE of theseclashes and [punish]ESE them,?
he [said]DSE .
(4) On Monday, 80 Libyan soldiers disembarkedfrom an Antonov transport plane carrying militaryequipment, an African diplomat [said]OSE .In this paper, we demonstrate how syntacticand semantic structural information can be usedto improve opinion detection.
While this fea-ture model makes it impossible to use the stan-dard sequence labeling method, we show that witha simple strategy based on reranking, incorporat-ing structural features results in a significant im-provement.
We investigate two different rerankingstrategies: the Preference Kernel approach (Shenand Joshi, 2003) and an approach based on struc-ture learning (Collins, 2002).
In an evaluationon the MPQA corpus, the best system we evalu-ated, a structure learning-based reranker using thePassive?Aggressive learning algorithm, achieveda 10-point absolute improvement in soft recall,and a 5-point improvement in F-measure, over thebaseline sequence labeler .2 Motivation and Related WorkMost approaches to analysing the sentiment ofnatural-language text have relied fundamentallyon purely lexical information (see (Pang et al,2002; Yu and Hatzivassiloglou, 2003), inter alia)or low-level grammatical information such as part-of-speech tags and functional words (Wiebe et al,1999).
This is in line with the general consensusin the information retrieval community that verylittle can be gained by complex linguistic process-ing for tasks such as text categorization and search(Moschitti and Basili, 2004).However, it has been suggested that subjectiv-ity analysis is inherently more subtle than cate-gorization and that structural linguistic informa-tion should therefore be given more attention inthis context.
For instance, Karlgren et al (2010)argued from a Construction Grammar viewpoint(Croft, 2005) that grammatical constructions notonly connect words, but can also be viewed as lex-ical items in their own right.
Starting from thisintuition, they showed that incorporating construc-tion items into a bag-of-words feature representa-tion resulted in improved results on a number ofcoarse-grained opinion analysis tasks.
These con-structional features were domain-independent andwere manually extracted from dependency parsetrees.
They found that the most prominent con-structional feature for subjectivity analysis was theTense Shift construction.While the position by Karlgren et al (2010)?
that constructional features signal opinion ?originates from a particular theoretical frameworkand may be controversial, syntactic and shallow-semantic relations have repeatedly proven usefulfor subtasks of subjectivity analysis that are in-herently relational, above all for determining theholder or topic of a given opinion.
Works us-ing syntactic features to extract topics and holdersof opinions are numerous (Bethard et al, 2005;Kobayashi et al, 2007; Joshi and Penstein-Rose?,2009; Wu et al, 2009).
Semantic role analysis hasalso proven useful: Kim and Hovy (2006) useda FrameNet-based semantic role labeler to deter-mine holder and topic of opinions.
Similarly, Choiet al (2006) successfully used a PropBank-basedsemantic role labeler for opinion holder extrac-tion, and Wiegand and Klakow (2010) recently ap-plied tree kernel learning methods on a combina-tion of syntactic and semantic role trees for thesame task.
Ruppenhofer et al (2008) argued thatsemantic role techniques are useful but not com-pletely sufficient for holder and topic identifica-tion, and that other linguistic phenomena must bestudied as well.
One such linguistic pheonomenonis the discourse structure, which has recently at-tracted some attention in the opinion analysis com-munity (Somasundaran et al, 2009).3 Opinion Expression Detection UsingSyntactic and Semantic StructuresPrevious systems for opinionated expressionmarkup have typically used simple feature setswhich have allowed the use of efficient off-the-shelf sequence labeling methods based on Viterbisearch (Choi et al, 2006; Breck et al, 2007).
Thisis not possible in our case since we would like toextract structural, relational features that involvepairs of opinionated expressions and may applyover an arbitrarily long distance in the sentence.While it is possible that search algorithms forexact or approximate inference can be construc-tured for the arg max problem in this model, wesidestepped this issue by using a reranking decom-position of the problem: We first apply a standardViterbi-based sequence labeler using no structuralfeatures and generate a small candidate set of sizek.
Then, a second and more complex model picks68the top candidate from this set without having tosearch the whole candidate space.The advantages of a reranking approach com-pared to more complex approaches requiring ad-vanced search techniques are mainly simplicityand efficiency: this approach is conceptually sim-ple and fairly easy to implement provided that k-best output can be generated efficiently, and fea-tures can be arbitrarily complex ?
we don?t haveto think about how the features affect the algorith-mic complexity of the inference step.
A commonobjection to reranking is that the candidate set maynot be diverse enough to allow for much improve-ment unless it is very large; the candidates maybe trivial variations that are all very similar to thetop-scoring candidate (Huang, 2008).3.1 Syntactic and Semantic StructuresWe used the syntactic?semantic parser by Johans-son and Nugues (2008a) to annnotate the sen-tences with dependency syntax (Mel?c?uk, 1988)and shallow semantic structures in the PropBank(Palmer et al, 2005) and NomBank (Meyers etal., 2004) frameworks.
Figure 1 shows an exampleof the annotation: The sentence they called him aliar, where called is a DSE and liar is an ESE, hasbeen annotated with dependency syntax (above thetext) and PropBank-based semantic role structure(below the text).
The predicate called, which isan instance of the PropBank frame call.01, hasthree semantic arguments: the Agent (A0), theTheme (A1), and the Predicate (A2), which are re-alized on the surface-syntactic level as a subject,a direct object, and an object predicative comple-ment, respectively.
]ESEThey calledcall.01SBJOPRDliarhim[ [aA1A0 A2]DSENMODOBJFigure 1: Syntactic and shallow semantic struc-ture.3.2 Sequence LabelerWe implemented a standard sequence labeler fol-lowing the approach of Collins (2002), whiletraining the model using the Passive?Aggressivealgorithm (Crammer et al, 2006) instead of theperceptron.
We encoded the opinionated expres-sion brackets using the IOB2 encoding scheme(Tjong Kim Sang and Veenstra, 1999).
Figure 2shows an example of a sentence with a DSE andan ESE and how they are encoded in the IOB2 en-coding.This Ois Oviewed B-DSEas Othe Omain B-ESEimpediment I-ESEFigure 2: Sequence labeling example.The sequence labeler used word, POS tag, andlemma features in a window of size 3.
In addi-tion, we used prior polarity and intensity featuresderived from the lexicon created by Wilson et al(2005).
In the example, viewed is listed as hav-ing strong prior subjectivity but no polarity, andimpediment has strong prior subjectivity and neg-ative polarity.
Note that prior subjectivity does notalways imply subjectivity in a particular context;this is why contextual features are essential for thistask.This sequence labeler is used to generate thecandidate set for the reranker; the Viterbi algo-rithm is easily modified to give k-best output.
Togenerate training data for the reranker, we carriedout a 5-fold cross-validation procedure: We splitthe training set into 5 pieces, trained a sequencelabeler on pieces 1 to 4, applied it to piece 5 andso on.3.3 Reranker FeaturesThe rerankers use two types of structural fea-tures: syntactic features extracted from the depen-dency tree, and semantic features extracted fromthe predicate?argument (semantic role) graph.The syntactic features are based on pathsthrough the dependency tree.
This creates a smallcomplication for multiword opinionated expres-sions; we select the shortest possible path in suchcases.
For instance, in Example (1), the path willbe computed between denounced and violation,and in Example (2) between viewed and impedi-ment.We used the following syntactic features:69SYNTACTIC PATH.
Given a pair of opinion ex-pressions, we use a feature representing thelabels of the two expressions and the path be-tween them through the syntactic tree.
Forinstance, for the DSE called and the ESE liarin Figure 1, we represent the syntactic config-uration using the feature DSE:OPRD?
:ESE,meaning that the path from the DSE to theESE consists of a single link, where the de-pendency edge label is OPRD (object predica-tive complement).LEXICALIZED PATH.
Same as above,but with lexical information attached:DSE/called:OPRD?:ESE/liar.DOMINANCE.
In addition to the features basedon syntactic paths, we created a more genericfeature template describing dominance re-lations between expressions.
For instance,from the graph in Figure 1, we extract thefeature DSE/called?ESE/liar, mean-ing that a DSE with the word called domi-nates an ESE with the word liar.The semantic features were the following:PREDICATE SENSE LABEL.
For every predi-cate found inside an opinion expression, weadd a feature consisting of the expression la-bel and the predicate sense identifier.
For in-stance, the verb call which is also a DSE isrepresented with the feature DSE/call.01.PREDICATE AND ARGUMENT LABEL.
Forevery argument of a predicate inside anopinion expression, we create a featurerepresenting the predicate?argument pair:DSE/call.01:A0.CONNECTING ARGUMENT LABEL.
When apredicate inside some opinion expression isconnected to some argument inside anotheropinion expression, we use a feature con-sisting of the two expression labels and theargument label.
For instance, the ESE liaris connected to the DSE call via an A2 la-bel, and we represent this using a featureDSE:A2:ESE.Apart from the syntactic and semantic features,we also used the score output from the base se-quence labeler as a feature.
We normalized thescores over the k candidates so that their exponen-tials summed to 1.3.4 Preference Kernel ApproachThe first reranking strategy we investigated wasthe Preference Kernel approach (Shen and Joshi,2003).
In this method, the reranking problem ?learning to select the correct candidate h1 from acandidate set {h1, .
.
.
, hk} ?
is reduced to a bi-nary classification problem by creating pairs: pos-itive training instances ?h1, h2?, .
.
.
, ?h1, hk?
andnegative instances ?h2, h1?, .
.
.
, ?hk, h1?.
This ap-proach has the advantage that the abundant toolsfor binary machine learning can be exploited.It is also easy to show (Shen and Joshi, 2003)that if we have a kernel K over the candidate spaceT , we can construct a valid kernel PK over thespace of pairs T ?
T as follows:PK(h1, h2) = K(h11, h12) + K(h21, h22)?
K(h11, h22) ?
K(h21, h12),where hi are the pairs of hypotheses ?h1i , h2i ?
gen-erated by the base model.
This makes it possibleto use kernel methods to train the reranker.
Wetried two types of kernels: linear kernels and treekernels.3.4.1 Linear KernelWe created feature vectors extracted from the can-didate sequences using the features described inSection 3.3.
We then trained linear SVMs usingthe LIBLINEAR software (Fan et al, 2008), usingL1 loss and L2 regularization.3.4.2 Tree KernelTree kernels have been successful for a number ofstructure extraction tasks, such as relation extrac-tion (Zhang et al, 2006; Nguyen et al, 2009) andopinion holder extraction (Wiegand and Klakow,2010).
A tree kernel implicitly represents a largespace of fragments extracted from trees and couldthus reduce the need for manual feature design.Since the paths that we extract manually (Sec-tion 3.3) can be expressed as tree fragments, thismethod could be an interesting alternative to themanually extracted features used with the linearkernel.We therefore implemented a reranker usingthe Partial Tree Kernel (Moschitti, 2006), andwe trained it using the SVMLight-TK software1,which is a modification of SVMLight (Joachims,1Available at http://dit.unitn.it/?moschitt701999)2.
It is still an open question how depen-dency trees should be represented for use withtree kernels (Suzuki et al, 2003; Nguyen et al,2009); we used the representation shown in Fig-ure 3.
Note that we have concatenated the opinionexpression labels to the POS tag nodes.
We did notuse any of the features from Section 3.3 except forthe base sequence labeler score.TOPROOTOBJSBJPRPthey himOPRDPRPNMODDTNN?ESVBD?DScalledaliarFigure 3: Representation of a dependency treewith opinion expressions for tree kernels.3.5 Structure Learning ApproachThe Preference Kernel approach reduces thereranking problem to a binary classification taskon pairs, after which a standard SVM optimizer isused to train the reranker.
A problem with thismethod is that the optimization problem solvedby the SVM ?
maximizing the classification ac-curacy on a set of independent pairs ?
is not di-rectly related to the performance of the reranker.Instead, the method employed by many rerankersfollowing Collins and Duffy (2002) directly learna scoring function that is trained to maximize per-formance on the reranking task.
We will refer tothis approach as the structure learning method.While there are batch learning algorithms thatwork in this setting (Tsochantaridis et al, 2005),online learning methods have been more popularfor efficiency reasons.
We investigated two onlinelearning algorithms: the popular structured per-ceptron Collins and Duffy (2002) and the Passive?Aggressive (PA) algorithm (Crammer et al, 2006).To increase robustness, we averaged the weightvectors seen during training as in the Voted Per-ceptron (Freund and Schapire, 1999).The difference between the two algorithms isthe way the weight vector is incremented in eachstep.
In the perceptron, for a given input x, we up-date based on the difference between the correct2http://svmlight.joachims.orgoutput y and the predicted output y?, where ?
isthe feature representation function:y?
?
arg maxh w ?
?
(x, h)w ?
w + ?
(x, y) ?
?
(x, y?
)In the PA algorithm, which is based on the the-ory of large-margin learning, we instead find they?
that violates the margin constraints maximally.The update step length ?
is computed based on themargin; this update is bounded by a regularizationconstant C:y?
?
arg maxh w ?
?
(x, h) +??
(y, h)?
?
min(C,w(?(x,y?)??(x,y))+??(y,y?)??(x,y?)??
(x,y)?2)w ?
w + ?(?
(x, y) ?
?
(x, y?
))The algorithm uses a cost function ?.
We usedthe function ?
(y, y?)
= 1 ?
F (y, y?
), where F isthe soft F-measure described in Section 4.1.
Withthis approach, the learning algorithm thus directlyoptimizes the measure we are interested in, i.e.
theF-measure.4 ExperimentsWe carried out the experiments on version 2 of theMPQA corpus (Wiebe et al, 2005), which we splitinto a test set (150 documents, 3,743 sentences)and a training set (541 documents, 12,010 sen-tences).4.1 Evaluation MetricsSince expression boundaries are hard to define ex-actly in annotation guidelines (Wiebe et al, 2005),we used soft precision and recall measures to scorethe quality of the system output.
To derive the softprecision and recall, we first define the span cov-erage c of a span s with respect to another span s?,which measures how well s?
is covered by s:c(s, s?)
=|s ?
s?||s?|In this formula, the operator | ?
| counts tokens, andthe intersection ?
gives the set of tokens that twospans have in common.
Since our evaluation takesspan labels (DSE, ESE, OSE) into account, we setc(s, s?)
to zero if the labels associated with s ands?
are different.Using the span coverage, we define the span setcoverage C of a set of spans S with respect to aset S?:C(S,S?)
=?sj?S?s?k?S?c(sj, s?k)71We now define the soft precision P and recall Rof a proposed set of spans ?S with respect to a goldstandard set S as follows:P (S, ?S) = C(S,?S)|?S|R(S, ?S) = C(?S,S)|S|Note that the operator | ?
| counts spans in this for-mula.Conventionally, when measuring the quality ofa system for an information extraction task, a pre-dicted entity is counted as correct if it exactlymatches the boundaries of a corresponding en-tity in the gold standard; there is thus no rewardfor close matches.
However, since the boundariesof the spans annotated in the MPQA corpus arenot strictly defined in the annotation guidelines(Wiebe et al, 2005), measuring precision and re-call using exact boundary scoring will result in fig-ures that are too low to be indicative of the use-fulness of the system.
Therefore, most work us-ing this corpus instead use overlap-based preci-sion and recall measures, where a span is countedas correctly detected if it overlaps with a span inthe gold standard (Choi et al, 2006; Breck et al,2007).
As pointed out by Breck et al (2007), thisis problematic since it will tend to reward longspans ?
for instance, a span covering the wholesentence will always be counted as correct if thegold standard contains any span for that sentence.The precision and recall measures proposedhere correct the problem with overlap-based mea-sures: If the system proposes a span covering thewhole sentence, the span coverage will be low andresult in a low soft precision.
Note that our mea-sures are bounded below by the exact measuresand above by the overlap-based measures.4.2 Reranking ApproachesWe compared the reranking architectures and themachine learning methods described in Section 3.In these experiments, we used a candidate set sizek of 8.
Table 1 shows the results of the evaluationsusing the precision and recall measures describedabove.
The baseline is the result of taking the top-scoring output from the sequence labeler withoutapplying any reranking.The results show that the rerankers using man-ual feature extraction outperform the tree-kernel-based reranker, which obtains a score just abovethe baseline.
It should be noted that the mas-sive training time of kernel-based machine learn-ing precluded a detailed tuning of parameters andSystem P R FBaseline 63.36 46.77 53.82Pref-linear 64.60 50.17 56.48Pref-TK 63.97 46.94 54.15Struct-Perc 62.84 48.13 54.51Struct-PA 63.50 51.79 57.04Table 1: Evaluation of reranking architectures andlearning methods.representation ?
on the other hand, we did not needto spend much time on parameter tuning and fea-ture design for the other rerankers.In addition, we note that the best performancewas obtained using the PA algorithm and the struc-ture learning architecture.
The PA algorithm isa simple online learning method and still out-performs the SVM used in the preference-kernelreranker.
This suggests that the structure learningapproach is superior for this task.
It is possiblethat a batch learning method such as SVMstruct(Tsochantaridis et al, 2005) could improve the re-sults even further.4.3 Candidate Set SizeIn any method based on reranking, it is importantto study the influence of the candidate set size onthe quality of the reranked output.
In addition, aninteresting question is what the upper bound onreranker performance is ?
the oracle performance.Table 2 shows the result of an experiment that in-vestigates these questions.
We used the rerankerbased on the Passive?Aggressive method in thisexperiment since this reranker gave the best resultsin the previous experiment.Reranked Oraclek P R F P R F1 63.36 46.77 53.82 63.36 46.77 53.822 63.70 48.17 54.86 72.66 55.18 62.724 63.57 49.78 55.84 79.12 62.24 69.688 63.50 51.79 57.04 83.72 68.14 75.1316 63.00 52.94 57.54 86.92 72.79 79.2332 62.15 54.50 58.07 89.18 76.76 82.5164 61.02 55.67 58.22 91.08 80.19 85.28128 60.22 56.45 58.27 92.63 83.00 87.55256 59.87 57.22 58.51 94.01 85.27 89.43Table 2: Oracle and reranker performance as afunction of candidate set size.As is common in reranking tasks, the rerankercan exploit only a fraction of the potential im-provement ?
the reduction of the F-measure error72is between 10 and 15 percent of the oracle errorreduction for all candidate set sizes.The most visible effect of the reranker is thatthe recall is greatly improved.
However, this doesnot seem to have an adverse effect on the precisionuntil the candidate set size goes above 8 ?
in fact,the precision actually improves over the baselinefor small candidate set sizes.
After the size goesabove 8, the recall (and the F-measure) still rises,but at the cost of decreased precision.4.4 Impact of FeaturesWe studied the impact of syntactic and seman-tic structural features on the performance of thereranker.
Table 3 shows the result of the inves-tigation for syntactic features.
Using all the syn-tactic features (and no semantic features) gives anF-measure roughly 4 points above the baseline, us-ing the PA reranker with a k of 64.
We then mea-sured the F-measure obtained when each one ofthe three syntactic features had been removed.
Itis clear that the unlexicalized syntactic path is themost important syntactic feature; the effect of thetwo lexicalized features seems to be negligible.System P R FBaseline 63.36 46.77 53.82All syntactic 62.45 53.19 57.45No SYN PATH 64.40 48.69 55.46No LEX PATH 62.62 53.19 57.52No DOMINANCE 62.32 52.92 57.24Table 3: Effect of syntactic features.A similar result was obtained when studying thesemantic features (Table 4).
Removing the CON-NECTING ARGUMENT LABEL feature, which isunlexicalized, has a greater effect than removingthe other two semantic features, which are lexical-ized.System P R FBaseline 63.36 46.77 53.82All semantic 61.26 53.85 57.31No PREDICATE SL 61.28 53.81 57.30No PRED+ARGLBL 60.96 53.61 57.05No CONN ARGLBL 60.73 50.47 55.12Table 4: Effect of semantic features.Since our most effective structural featurescombine a pair of opinion expression labels witha tree fragment, it is interesting to study whetherthe expression labels alone would be enough.
Ifthis were the case, we could conclude that theimprovement is caused not by the structural fea-tures, but just by learning which combinationsof labels are common in the training set, suchas that DSE+ESE would be more common thanOSE+ESE.
We thus carried out an experimentcomparing a reranker using label pair featuresagainst rerankers based on syntactic features only,semantic features only, and the full feature set.
Ta-ble 5 shows the results.
We see that the rerankerusing label pairs indeed achieves a performancewell above the baseline.
However, its performanceis below that of any reranker using structural fea-tures.
In addition, we see no improvement whenadding label pair features to the structural featureset; this is to be expected since the label pair infor-mation is subsumed by the structural features.System P R FBaseline 63.36 46.77 53.82Label pairs 62.05 52.68 56.98All syntactic 62.45 53.19 57.45All semantic 61.26 53.85 57.31Syn + sem 61.02 55.67 58.22Syn + sem + pairs 61.61 54.78 57.99Table 5: Structural features compared to labelpairs.4.5 Comparison with Breck et al (2007)Comparison of systems in opinion expression de-tection is often nontrivial since evaluation settingshave differed widely.
Since our problem setting?
marking up and labeling opinion expressions inthe MPQA corpus ?
is most similar to that de-scribed by Breck et al (2007), we carried out anevaluation using the setting used in their experi-ment.For compatibility with their experimental setup,this experiment differed from the ones describedin the previous sections in the following ways:?
The system did not need to distinguish DSEsand ESEs and did not have to detect theOSEs.?
The results were measured using the overlap-based precision and recall, although this isproblematic as pointed out in Section 4.1.73?
Instead of the training/test split we used in theprevious evaluations, the systems were evalu-ated using a 10-fold cross-validation over thesame set of 400 documents as used in Breck?sexperiment.Again, our reranker uses the PA method with ak of 64.
Table 6 shows the results.System P R FBreck et al (2007) 71.64 74.70 73.05Baseline 80.85 64.38 71.68Reranked 76.40 78.23 77.30Table 6: Results using the Breck et al (2007) eval-uation setting.We see that the performance of our system isclearly higher ?
in both precision and recall ?
thanthat reported by Breck et al (2007).
This showsagain that the structural features are effective forthe task of finding opinionated expressions.We note that the performance of our base-line sequence labeler is lower than theirs; thisis to be expected since they used a more com-plex batch learning algorithm (conditional randomfields) while we used an online learner, and theyspent more effort on feature design.
This indicatesthat we should be able to achieve even higher per-formance using a stronger base model.5 ConclusionWe have shown that features derived from gram-matical and semantic role structure can be used toimprove the detection of opinionated expressionsin subjectivity analysis.
Most significantly, the re-call is drastically increased (10 points) while theprecision decreases only slightly (3 points).
Thisresult compares favorably with previously pub-lished results, which have been biased towardsprecision and scored low on recall.The long-distance structural features gives us amodel that has predictive power as well as being oftheoretical interest: this model takes into accountthe interactions between opinion expressions in asentence.
While these structural features give usa powerful model, they come at a computationalcost; prediction is more complex than in a stan-dard sequence labeler based on purely local fea-tures.
However, we have shown that a predictionstrategy based on reranking suffices for this task.We analyzed the impact of the syntactic and se-mantic features and saw that the best model in-cludes both types of features.
The most effectivefeatures we have found are purely structural, i.e.based on tree fragments in a syntactic or seman-tic tree.
Features involving words did not seem tohave the same impact.
We also showed that the im-provement is not explainable by mere correlationsbetween opinion expression labels.We investigated a number of implementationstrategies for the reranker and concluded that thestructural learning framework seemed to give thebest performance.
We were not able to achievethe same performance using tree kernels as withmanually extracted features.
It is possible that thiscould be improved with a better strategy for rep-resenting dependency structure for tree kernels, orif the tree kernels could be incorporated into thestructural learning framework.The flexible architecture we have presented en-ables interesting future research: (i) a straight-forward improvement is the use of lexical simi-larity to reduce data sparseness, e.g.
(Basili etal., 2005; Basili et al, 2006; Bloehdorn et al,2006).
However, the similarity between subjectivewords, which have multiple senses against otherwords may negatively impact the system accu-racy.
Therefore, the use of the syntactic/semantickernels, i.e.
(Bloehdorn and Moschitti, 2007a;Bloehdorn and Moschitti, 2007b), to syntacticallycontextualize word similarities may improve thereranker accuracy.
(ii) The latter can be fur-ther boosted by studying complex structural ker-nels, e.g.
(Moschitti, 2008; Nguyen et al, 2009;Dinarelli et al, 2009).
(iii) More specific pred-icate argument structures such those proposed inFrameNet, e.g.
(Baker et al, 1998; Giuglea andMoschitti, 2004; Giuglea and Moschitti, 2006; Jo-hansson and Nugues, 2008b) may be useful tocharacterize the opinion holder and the sentencesemantic context.Finally, while the strategy based on rerankingresulted in a significant performance boost, it re-mains to be seen whether a higher accuracy canbe achieved by developing a more sophisticatedinference algorithm based on dynamic program-ming.
However, while the development of suchan algorithm is an interesting problem, it will notnecessarily result in a more usable system ?
whenusing a reranker, it is easy to trade accuracy forefficiency.74AcknowledgementsThe research leading to these results has receivedfunding from the European Community?s Sev-enth Framework Programme (FP7/2007-2013) un-der grant agreement 231126: LivingKnowledge ?Facts, Opinions and Bias in Time, and from Trust-worthy Eternal Systems via Evolving Software,Data and Knowledge (EternalS, project numberFP7 247758).
In addition, we would like to thankEric Breck for clarifying his results and experi-mental setup.ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In Proceed-ings of COLING/ACL-1998.Roberto Basili, Marco Cammisa, and Alessandro Mos-chitti.
2005.
Effective use of WordNet seman-tics via kernel-based learning.
In Proceedings ofCoNLL-2005, pages 1?8, Ann Arbor, Michigan.Roberto Basili, Marco Cammisa, and Alessandro Mos-chitti.
2006.
A semantic kernel to classify texts withvery few training examples.
In in Informatica, an in-ternational journal of Computing and Informatics.Steven Bethard, Hong Yu, Ashley Thornton, VasileiosHatzivassiloglou, and Dan Jurafsky.
2005.
Extract-ing opinion propositions and opinion holders usingsyntactic and lexical cues.
In James G. Shanahan,Yan Qu, and Janyce Wiebe, editors, Computing Atti-tude and Affect in Text: Theory and Applications.Stephan Bloehdorn and Alessandro Moschitti.
2007a.Combined syntactic and semantic kernels for textclassification.
In Proceedings of ECIR 2007, Rome,Italy.Stephan Bloehdorn and Alessandro Moschitti.
2007b.Structure and semantics for expressive text kernels.In In Proceedings of CIKM ?07.Stephan Bloehdorn, Roberto Basili, Marco Cammisa,and Alessandro Moschitti.
2006.
Semantic kernelsfor text classification based on topological measuresof feature similarity.
In Proceedings of ICDM 06,Hong Kong, 2006.Eric Breck, Yejin Choi, and Claire Cardie.
2007.
Iden-tifying expressions of opinion in context.
In Pro-ceedings of IJCAI-2007, Hyderabad, India.Yejin Choi, Eric Breck, and Claire Cardie.
2006.
Jointextraction of entities and relations for opinion recog-nition.
In Proceedings of EMNLP 2006.Michael Collins and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: Kernels overdiscrete structures, and the voted perceptron.
InPro-ceedings of ACL?02.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and ex-periments with perceptron algorithms.
In Proceed-ings of the 2002 Conference on Empirical Methodsin Natural Language Processing (EMNLP 2002),pages 1?8.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Schwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Journal of MachineLearning Research, 2006(7):551?585.William Croft.
2005.
Radical and typological argu-ments for radical construction grammar.
In J.-O.O?stman and M. Fried, editors, Construction Gram-mars: Cognitive grounding and theoretical exten-sions.Marco Dinarelli, Alessandro Moschitti, and GiuseppeRiccardi.
2009.
Re-ranking models based-on smalltraining data for spoken language understanding.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages1076?1085, Singapore, August.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.Yoav Freund and Robert E. Schapire.
1999.
Largemargin classification using the perceptron algorithm.Machine Learning, 37(3):277?296.Ana-Maria Giuglea and Alessandro Moschitti.
2004.Knowledge Discovering using FrameNet, VerbNetand PropBank.
In In Proceedings of the Workshopon Ontology and Knowledge Discovering at ECML2004, Pisa, Italy.Ana-Maria Giuglea and Alessandro Moschitti.
2006.Semantic role labeling via FrameNet, VerbNet andPropBank.
In Proceedings of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the Association for Computa-tional Linguistics, pages 929?936, Sydney, Aus-tralia, July.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL-08: HLT, pages 586?594, Columbus, UnitedStates.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
Advances in Kernel Methods ?Support Vector Learning, 13.Richard Johansson and Pierre Nugues.
2008a.Dependency-based syntactic?semantic analysis withPropBank and NomBank.
In CoNLL 2008: Pro-ceedings of the Twelfth Conference on NaturalLanguage Learning, pages 183?187, Manchester,United Kingdom.75Richard Johansson and Pierre Nugues.
2008b.
Theeffect of syntactic representation on semantic rolelabeling.
In Proceedings of the 22nd InternationalConference on Computational Linguistics (Coling2008), pages 393?400, Manchester, UK.Mahesh Joshi and Carolyn Penstein-Rose?.
2009.
Gen-eralizing dependency features for opinion mining.In Proceedings of ACL/IJCNLP 2009, Short PapersTrack.Jussi Karlgren, Gunnar Eriksson, Magnus Sahlgren,and Oscar Ta?ckstro?m.
2010.
Between bagsand trees ?
constructional patterns in text used forattitude identification.
In Proceedings of ECIR2010, 32nd European Conference on InformationRetrieval, Milton Keynes, United Kingdom.Soo-Min Kim and Eduard Hovy.
2006.
Extract-ing opinions, opinion holders, and topics expressedin online news media text.
In Proceedings ofACL/COLING Workshop on Sentiment and Subjec-tivity in Text.Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.2007.
Extracting aspect-evaluation and aspect-of re-lations in opinion mining.
In Proceedings of Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-CoNLL-2007).Igor A. Mel?c?uk.
1988.
Dependency Syntax: Theoryand Practice.
State University Press of New York,Albany.Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004.
The NomBank project:An interim report.
In HLT-NAACL 2004 Work-shop: Frontiers in Corpus Annotation, pages 24?31,Boston, United States.Alessandro Moschitti and Roberto Basili.
2004.
Com-plex linguistic features for text classification: Acomprehensive study.
In Proceedings of ECIR.AlessandroMoschitti.
2006.
Making tree kernels prac-tical for natural language learning.
In Proccedingsof EACL?06.Alessandro Moschitti.
2008.
Kernel methods, syntaxand semantics for relational text categorization.
InProceeding of CIKM ?08, NY, USA.Truc-Vien T. Nguyen, Alessandro Moschitti, andGiuseppe Riccardi.
2009.
Convolution kernels onconstituent, dependency and sequential structuresfor relation extraction.
In Proceedings of EMNLP.Martha Palmer, DanGildea, and Paul Kingsbury.
2005.The proposition bank: An annotated corpus of se-mantic roles.
Computational Linguistics, 31(1):71?105.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification us-ing machine learning techniques.
In Proceedings ofEMNLP.Josef Ruppenhofer, Swapna Somasundaran, and JanyceWiebe.
2008.
Finding the sources and targets ofsubjective expressions.
In Proceedings of LREC.Libin Shen and Aravind Joshi.
2003.
An SVM basedvoting algorithmwith application to parse reranking.In Proceedings of the CoNLL.Swapna Somasundaran, Galileo Namata, JanyceWiebe, and Lise Getoor.
2009.
Supervised andunsupervised methods in employing discourse rela-tions for improving opinion polarity classification.In Proceedings of EMNLP 2009: conference on Em-pirical Methods in Natural Language Processing.Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and EisakuMaeda.
2003.
Hierarchical directed acyclic graphkernel: Methods for structured natural languagedata.
In Proceedings of the 41th Annual Meeting ofAssociation for Computational Linguistics (ACL).Erik F. Tjong Kim Sang and Jorn Veenstra.
1999.
Rep-resenting text chunks.
In Proceedings of EACL99,pages 173?179, Bergen, Norway.Iannis Tsochantaridis, Thorsten Joachims, ThomasHofmann, and Yasemin Altun.
2005.
Large marginmethods for structured and interdependent outputvariables.
Journal of Machine Learning Research,6(Sep):1453?1484.Janyce Wiebe, Rebecca Bruce, and Thomas O?Hara.1999.
Development and use of a gold standard dataset for subjectivity classifications.
In Proceedingsof the 37th Annual Meeting of the Association forComputational Linguistics.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Evalu-ation, 39(2-3):165?210.Michael Wiegand and Dietrich Klakow.
2010.
Con-volution kernels for opinion holder extraction.
InProceedings of HLT-NAACL 2010.
To appear.Theresa Wilson, Janyce Wiebe, and Paul Hoff-mann.
2005.
Recognizing contextual polarity inphrase-level sentiment analysis.
In Proceedings ofHLT/EMNLP 2005.YuanbinWu, Qi Zhang, XuanjingHuang, and LideWu.2009.
Phrase dependency parsing for opinion min-ing.
In Proceedings of EMNLP.Hong Yu and Vasileios Hatzivassiloglou.
2003.
To-wards answering opinion questions: Separating factsfrom opinions and identifying the polarity of opin-ion sentences.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP-2003), pages 129?136, Sapporo, Japan.Min Zhang, Jie Zhang, and Jian Su.
2006.
ExploringSyntactic Features for Relation Extraction using aConvolution tree kernel.
In Proceedings of NAACL.76
