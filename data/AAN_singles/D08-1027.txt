Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 254?263,Honolulu, October 2008. c?2008 Association for Computational LinguisticsCheap and Fast ?
But is it Good?Evaluating Non-Expert Annotations for Natural Language TasksRion Snow?
Brendan O?Connor?
Daniel Jurafsky?
Andrew Y.
Ng?
?Computer Science Dept.Stanford UniversityStanford, CA 94305{rion,ang}@cs.stanford.edu?Dolores Labs, Inc.832 Capp St.San Francisco, CA 94110brendano@doloreslabs.com?Linguistics Dept.Stanford UniversityStanford, CA 94305jurafsky@stanford.eduAbstractHuman linguistic annotation is crucial formany natural language processing tasks butcan be expensive and time-consuming.
We ex-plore the use of Amazon?s Mechanical Turksystem, a significantly cheaper and fastermethod for collecting annotations from abroad base of paid non-expert contributorsover the Web.
We investigate five tasks: af-fect recognition, word similarity, recognizingtextual entailment, event temporal ordering,and word sense disambiguation.
For all five,we show high agreement between Mechani-cal Turk non-expert annotations and existinggold standard labels provided by expert label-ers.
For the task of affect recognition, we alsoshow that using non-expert labels for trainingmachine learning algorithms can be as effec-tive as using gold standard annotations fromexperts.
We propose a technique for biascorrection that significantly improves annota-tion quality on two tasks.
We conclude thatmany large labeling tasks can be effectivelydesigned and carried out in this method at afraction of the usual expense.1 IntroductionLarge scale annotation projects such as TreeBank(Marcus et al, 1993), PropBank (Palmer etal., 2005), TimeBank (Pustejovsky et al, 2003),FrameNet (Baker et al, 1998), SemCor (Miller etal., 1993), and others play an important role innatural language processing research, encouragingthe development of novel ideas, tasks, and algo-rithms.
The construction of these datasets, how-ever, is extremely expensive in both annotator-hoursand financial cost.
Since the performance of manynatural language processing tasks is limited by theamount and quality of data available to them (Bankoand Brill, 2001), one promising alternative for sometasks is the collection of non-expert annotations.In this work we explore the use of Amazon Me-chanical Turk1 (AMT) to determine whether non-expert labelers can provide reliable natural languageannotations.
We chose five natural language under-standing tasks that we felt would be sufficiently nat-ural and learnable for non-experts, and for whichwe had gold standard labels from expert labelers,as well as (in some cases) expert labeler agree-ment information.
The tasks are: affect recogni-tion, word similarity, recognizing textual entailment,event temporal ordering, and word sense disam-biguation.
For each task, we used AMT to annotatedata and measured the quality of the annotations bycomparing them with the gold standard (expert) la-bels on the same data.
Further, we compare machinelearning classifiers trained on expert annotations vs.non-expert annotations.In the next sections of the paper we introducethe five tasks and the evaluation metrics, and offermethodological insights, including a technique forbias correction that improves annotation quality.21 http://mturk.com2 Please see http://blog.doloreslabs.com/?p=109for a condensed version of this paper, follow-ups, and on-going public discussion.
We encourage comments to be di-rected here in addition to email when appropriate.
DoloresLabs Blog, ?AMT is fast, cheap, and good for machine learningdata,?
Brendan O?Connor, Sept. 9, 2008.
More related work athttp://blog.doloreslabs.com/topics/wisdom/.2542 Related WorkThe idea of collecting annotations from volunteercontributors has been used for a variety of tasks.Luis von Ahn pioneered the collection of data viaonline annotation tasks in the form of games, includ-ing the ESPGame for labeling images (von Ahn andDabbish, 2004) and Verbosity for annotating wordrelations (von Ahn et al, 2006).
The Open MindInitiative (Stork, 1999) has taken a similar approach,attempting to make such tasks as annotating wordsense (Chklovski and Mihalcea, 2002) and common-sense word relations (Singh, 2002) sufficiently ?easyand fun?
to entice users into freely labeling data.There have been an increasing number of experi-ments using Mechanical Turk for annotation.
In (Suet al, 2007) workers provided annotations for thetasks of hotel name entity resolution and attributeextraction of age, product brand, and product model,and were found to have high accuracy comparedto gold-standard labels.
Kittur et al (2008) com-pared AMT evaluations of Wikipedia article qual-ity against experts, finding validation tests were im-portant to ensure good results.
Zaenen (Submitted)studied the agreement of annotators on the problemof recognizing textual entailment (a similar task anddataset is explained in more detail in Section 4).At least several studies have already used AMTwithout external gold standard comparisons.
In(Nakov, 2008) workers generated paraphrases of250 noun-noun compounds which were then usedas the gold standard dataset for evaluating an au-tomatic method of noun compound paraphrasing.Kaisser and Lowe (2008) use AMT to help build adataset for question answering, annotating the an-swers to 8107 questions with the sentence contain-ing the answer.
Kaisser et al (2008) examines thetask of customizing the summary length of QA out-put; non-experts from AMT chose a summary lengththat suited their information needs for varying querytypes.
Dakka and Ipeirotis (2008) evaluate a docu-ment facet generation system against AMT-suppliedfacets, and also use workers for user studies of thesystem.
Sorokin and Forsyth (2008) collect data formachine vision tasks and report speed and costs sim-ilar to our findings; their summaries of worker be-havior also corroborate with what we have found.In general, volunteer-supplied or AMT-supplieddata is more plentiful but noisier than expert data.It is powerful because independent annotations canbe aggregated to achieve high reliability.
Sheng etal.
(2008) explore several methods for using manynoisy labels to create labeled data, how to choosewhich examples should get more labels, and how toinclude labels?
uncertainty information when train-ing classifiers.
Since we focus on empirically val-idating AMT as a data source, we tend to stick tosimple aggregation methods.3 Task DesignIn this section we describe Amazon MechanicalTurk and the general design of our experiments.3.1 Amazon Mechanical TurkWe employ the Amazon Mechanical Turk systemin order to elicit annotations from non-expert label-ers.
AMT is an online labor market where workersare paid small amounts of money to complete smalltasks.
The design of the system is as follows: one isrequired to have an Amazon account to either sub-mit tasks for annotations or to annotate submittedtasks.
These Amazon accounts are anonymous, butare referenced by a unique Amazon ID.
A Requestercan create a group of Human Intelligence Tasks (orHITs), each of which is a form composed of an arbi-trary number of questions.
The user requesting an-notations for the group of HITs can specify the num-ber of unique annotations per HIT they are willingto pay for, as well as the reward payment for eachindividual HIT.
While this does not guarantee thatunique people will annotate the task (since a singleperson could conceivably annotate tasks using mul-tiple accounts, in violation of the user agreement),this does guarantee that annotations will be collectedfrom unique accounts.
AMT also allows a requesterto restrict which workers are allowed to annotate atask by requiring that all workers have a particularset of qualifications, such as sufficient accuracy ona small test set or a minimum percentage of previ-ously accepted submissions.
Annotators (variouslyreferred to as Workers or Turkers) may then annotatethe tasks of their choosing.
Finally, after each HIThas been annotated, the Requester has the option ofapproving the work and optionally giving a bonusto individual workers.
There is a two-way commu-255nication channel between the task designer and theworkers mediated by Amazon, and Amazon handlesall financial transactions.3.2 Task DesignIn general we follow a few simple design principles:we attempt to keep our task descriptions as succinctas possible, and we attempt to give demonstrativeexamples for each class wherever possible.
We havepublished the full experimental design and the datawe have collected for each task online3.
We haverestricted our study to tasks where we require onlya multiple-choice response or numeric input withina fixed range.
For every task we collect ten inde-pendent annotations for each unique item; this re-dundancy allows us to perform an in-depth study ofhow data quality improves with the number of inde-pendent annotations.4 Annotation TasksWe analyze the quality of non-expert annotations onfive tasks: affect recognition, word similarity, rec-ognizing textual entailment, temporal event recogni-tion, and word sense disambiguation.
In this sectionwe define each annotation task and the parametersof the annotations we request using AMT.
Addition-ally we give an initial analysis of the task results,and summarize the cost of the experiments.4.1 Affective Text AnalysisThis experiment is based on the affective text an-notation task proposed in Strapparava and Mihalcea(2007), wherein each annotator is presented with alist of short headlines, and is asked to give numericjudgments in the interval [0,100] rating the headlinefor six emotions: anger, disgust, fear, joy, sadness,and surprise, and a single numeric rating in the inter-val [-100,100] to denote the overall positive or nega-tive valence of the emotional content of the headline,as in this sample headline-annotation pair:Outcry at N Korea ?nuclear test?
(Anger, 30), (Disgust,30), (Fear,30), (Joy,0),(Sadness,20), (Surprise,40), (Valence,-50).3All tasks and collected data are available athttp://ai.stanford.edu/?rion/annotations/.For our experiment we select a 100-headline samplefrom the original SemEval test set, and collect 10affect annotations for each of the seven label types,for a total of 7000 affect labels.We then performed two comparisons to evaluatethe quality of the AMT annotations.
First, we askedhow well the non-experts agreed with the experts.We did this by comparing the interannotator agree-ment (ITA) of individual expert annotations to thatof single non-expert and averaged non-expert anno-tations.
In the original experiment ITA is measuredby calculating the Pearson correlation of one anno-tator?s labels with the average of the labels of theother five annotators.
For each expert labeler, wecomputed this ITA score of the expert against theother five; we then average these ITA scores acrossall expert annotators to compute the average expertITA (reported in Table 1 as ?E vs. E?.
We then do thesame for individual non-expert annotations, averag-ing Pearson correlation across all sets of the five ex-pert labelers (?NE vs.
E?).
We then calculate the ITAfor each expert vs. the averaged labels from all otherexperts and non-experts (marked as ?E vs.
All?)
andfor each non-expert vs. the pool of other non-expertsand all experts (?NE vs.
All?).
We compute theseITA scores for each emotion task separately, aver-aging the six emotion tasks as ?Avg.
Emo?
and theaverage of all tasks as ?Avg.
All?.Emotion E vs. E E vs. All NE vs. E NE vs. AllAnger 0.459 0.503 0.444 0.573Disgust 0.583 0.594 0.537 0.647Fear 0.711 0.683 0.418 0.498Joy 0.596 0.585 0.340 0.421Sadness 0.645 0.650 0.563 0.651Surprise 0.464 0.463 0.201 0.225Valence 0.759 0.767 0.530 0.554Avg.
Emo 0.576 0.603 0.417 0.503Avg.
All 0.580 0.607 0.433 0.510Table 1: Average expert and non-expert ITA on test-setThe results in Table 1 conform to the expectationthat experts are better labelers: experts agree withexperts more than non-experts agree with experts,although the ITAs are in many cases quite close.
Butwe also found that adding non-experts to the goldstandard (?E vs.
All?)
improves agreement, suggest-ing that non-expert annotations are good enough toincrease the overall quality of the gold labels.
Our256first comparison showed that individual experts werebetter than individual non-experts.
In our next com-parison we ask how many averaged non-experts itwould take to rival the performance of a single ex-pert.
We did this by averaging the labels of each pos-sible subset of n non-expert annotations, for valueof n in {1, 2, .
.
.
, 10}.
We then treat this average asthough it is the output of a single ?meta-labeler?, andcompute the ITA with respect to each subset of fiveof the six expert annotators.
We then average theresults of these studies across each subset size; theresults of this experiment are given in Table 2 and inFigure 1.
In addition to the single meta-labeler, weask: what is the minimum number of non-expert an-notations k from which we can create a meta-labelerthat has equal or better ITA than an expert annotator?In Table 2 we give the minimum k for each emotion,and the averaged ITA for that meta-labeler consist-ing of k non-experts (marked ?k-NE?).
In Figure 1we plot the expert ITA correlation as the horizontaldashed line.Emotion 1-Expert 10-NE k k-NEAnger 0.459 0.675 2 0.536Disgust 0.583 0.746 2 0.627Fear 0.711 0.689 ?
?Joy 0.596 0.632 7 0.600Sadness 0.645 0.776 2 0.656Surprise 0.464 0.496 9 0.481Valence 0.759 0.844 5 0.803Avg.
Emo.
0.576 0.669 4 0.589Avg.
All 0.603 0.694 4 0.613Table 2: Average expert and averaged correlation over10 non-experts on test-set.
k is the minimum number ofnon-experts needed to beat an average expert.These results show that for all tasks except ?Fear?we are able to achieve expert-level ITA with theheld-out set of experts within 9 labelers, and fre-quently within only 2 labelers.
Pooling judgmentsacross all 7 tasks we find that on average it re-quires only 4 non-expert annotations per example toachieve the equivalent ITA as a single expert anno-tator.
Given that we paid US$2.00 in order to collectthe 7000 non-expert annotations, we may interpretour rate of 3500 non-expert labels per USD as atleast 875 expert-equivalent labels per USD.4.2 Word SimilarityThis task replicates the word similarity task used in(Miller and Charles, 1991), following a previous2 4 6 8 100.450.550.65correlationanger2 4 6 8 100.550.650.75correlationdisgust2 4 6 8 100.400.500.600.70correlationfear2 4 6 8 100.350.450.550.65correlationjoy2 4 6 8 100.550.650.75annotatorscorrelationsadness2 4 6 8 100.200.300.400.50annotatorscorrelationsurpriseFigure 1: Non-expert correlation for affect recognitiontask initially proposed by (Rubenstein and Good-enough, 1965).
Specifically, we ask for numericjudgments of word similarity for 30 word pairs ona scale of [0,10], allowing fractional responses4 .These word pairs range from highly similar (e.g.,{boy, lad}), to unrelated (e.g., {noon, string}).
Nu-merous expert and non-expert studies have shownthat this task typically yields very high interannota-tor agreement as measured by Pearson correlation;(Miller and Charles, 1991) found a 0.97 correla-tion of the annotations of 38 subjects with the an-notations given by 51 subjects in (Rubenstein andGoodenough, 1965), and a following study (Resnik,1999) with 10 subjects found a 0.958 correlationwith (Miller and Charles, 1991).In our experiment we ask for 10 annotations eachof the full 30 word pairs, at an offered price of $0.02for each set of 30 annotations (or, equivalently, atthe rate of 1500 annotations per USD).
The mostsurprising aspect of this study was the speed withwhich it was completed; the task of 300 annotationswas completed by 10 annotators in less than 11 min-4(Miller and Charles, 1991) and others originally used anumerical score of [0,4].257utes from the time of submission of our task to AMT,at the rate of 1724 annotations / hour.As in the previous task we evaluate our non-expert annotations by averaging the numeric re-sponses from each possible subset of n annotatorsand computing the interannotator agreement withrespect to the gold scores reported in (Miller andCharles, 1991).
Our results are displayed in Figure2, with Resnik?s 0.958 correlation plotted as the hor-izontal line; we find that at 10 annotators we achievea correlation of 0.952, well within the range of otherstudies of expert and non-expert annotations.2 4 6 8 100.840.900.96annotationscorrelationWord Similarity ITAFigure 2: ITA for word similarity experiment4.3 Recognizing Textual EntailmentThis task replicates the recognizing textual entail-ment task originally proposed in the PASCAL Rec-ognizing Textual Entailment task (Dagan et al,2006); here for each question the annotator is pre-sented with two sentences and given a binary choiceof whether the second hypothesis sentence can beinferred from the first.
For example, the hypothesissentence ?Oil prices drop?
would constitute a trueentailment from the text ?Crude Oil Prices Slump?,but a false entailment from ?The government an-nounced last week that it plans to raise oil prices?.We gather 10 annotations each for all 800 sen-tence pairs in the PASCAL RTE-1 dataset.
For thisdataset expert interannotator agreement studies havebeen reported as achieving 91% and 96% agreementover various subsections of the corpus.
When con-sidering multiple non-expert annotations for a sen-tence pair we use simple majority voting, breakingties randomly and averaging performance over allpossible ways to break ties.
We collect 10 annota-tions for each of 100 RTE sentence pairs; as dis-played in Figure 3, we achieve a maximum accu-racy of 89.7%, averaging over the annotations of 10workers5.2 4 6 8 100.700.800.90annotationsaccuracyRTE ITAFigure 3: Inter-annotator agreement for RTE experiment4.4 Event AnnotationThis task is inspired by the TimeBank corpus (Puste-jovsky et al, 2003), which includes among its anno-tations a label for event-pairs that represents the tem-poral relation between them, from a set of fourteenrelations (before, after, during, includes, etc.).
Weimplement temporal ordering as a simplified versionof the TimeBank event temporal annotation task:rather than annotating all fourteen event types, werestrict our consideration to the two simplest labels:?strictly before?
and ?strictly after?.
Furthermore,rather than marking both nouns and verbs in the textas possible events, we only consider possible verbevents.
We extract the 462 verb event pairs labeledas ?strictly before?
or ?strictly after?
in the Time-Bank corpus, and we present these pairs to annota-tors with a forced binary choice on whether the eventdescribed by the first verb occurs before or after thesecond.
For example, in a dialogue about a planeexplosion, we have the utterance: ?It just blew up inthe air, and then we saw two fireballs go down to the,5It might seem pointless to consider an even number of an-notations in this circumstance, since the majority voting mech-anism and tie-breaking yields identical performance for 2n + 1and 2n + 2 annotators; however, in Section 5 we will considermethods that can make use of the even annotations.258to the water, and there was a big small, ah, smoke,from ah, coming up from that?.
Here for each anno-tation we highlight the specific verb pair of interest(e.g., go/coming, or blew/saw) and ask which eventoccurs first (here, go and blew, respectively).The results of this task are presented in Figure 4.We achieve high agreement for this task, at a rateof 0.94 with simple voting over 10 annotators (4620total annotations).
While an expert ITA of 0.77 wasreported for the more general task involving all four-teen labels on both noun and verb events, no expertITA numbers have been reported for this simplifiedtemporal ordering task.2 4 6 8 100.700.800.90annotatorsaccuracyTemp.
Ordering ITAFigure 4: ITA for temporal ordering experiment4.5 Word Sense DisambiguationIn this task we consider a simple problem on whichmachine learning algorithms have been shown toproduce extremely good results; here we annotatepart of the SemEval Word Sense DisambiguationLexical Sample task (Pradhan et al, 2007); specif-ically, we present the labeler with a paragraph oftext containing the word ?president?
(e.g., a para-graph containing ?Robert E. Lyons III...was ap-pointed president and chief operating officer...?)
andask the labeler which one of the following threesense labels is most appropriate:1) executive officer of a firm, corporation, or university2) head of a country (other than the U.S.)3) head of the U.S., President of the United StatesWe collect 10 annotations for each of 177 examplesof the noun ?president?
for the three senses given inSemEval.
As shown in Figure 5, performing simplemajority voting (with random tie-breaking) over an-notators results in a rapid accuracy plateau at a veryhigh rate of 0.994 accuracy.
In fact, further analy-sis reveals that there was only a single disagreementbetween the averaged non-expert vote and the goldstandard; on inspection it was observed that the an-notators voted strongly against the original gold la-bel (9-to-1 against), and that it was in fact found tobe an error in the original gold standard annotation.6After correcting this error, the non-expert accuracyrate is 100% on the 177 examples in this task.
Thisis a specific example where non-expert annotationscan be used to correct expert annotations.Since expert ITA was not reported per word onthis dataset, we compare instead to the performanceof the best automatic system performance for dis-ambiguating ?president?
in SemEval Task 17 (Cai etal., 2007), with an accuracy of 0.98.2 4 6 8 100.9800.9901.000annotatorsaccuracyWSD ITAFigure 5: Inter-annotator agreement for WSD experiment4.6 SummaryCost Time Labels LabelsTask Labels (USD) (hrs) per USD per hrAffect 7000 $2.00 5.93 3500 1180.4WSim 300 $0.20 0.174 1500 1724.1RTE 8000 $8.00 89.3 1000 89.59Event 4620 $13.86 39.9 333.3 115.85WSD 1770 $1.76 8.59 1005.7 206.1Total 21690 25.82 143.9 840.0 150.7Table 3: Summary of costs for non-expert labels6The example sentence began ?The Egyptian president saidhe would visit Libya today...?
and was mistakenly marked asthe ?head of a company?
sense in the gold annotation (exampleid 24:0@24@wsj/23/wsj 2381@wsj@en@on).2590 200 400 600 8000.40.60.81.0number of annotationsaccuracyFigure 6: Worker accuracies on the RTE task.
Each pointis one worker.
Vertical jitter has been added to points onthe left to show the large number of workers who did theminimum amount of work (20 examples).In Table 3 we give a summary of the costs asso-ciated with obtaining the non-expert annotations foreach of our 5 tasks.
Here Time is given as the to-tal amount of time in hours elapsed from submittingthe group of HITs to AMT until the last assignmentis submitted by the last worker.5 Bias correction for non-expertannotatorsThe reliability of individual workers varies.
Someare very accurate, while others are more careless andmake mistakes; and a small few give very noisy re-sponses.
Furthermore, for most AMT data collec-tion experiments, a relatively small number of work-ers do a large portion of the task, since workers maydo as much or as little as they please.
Figure 6 showsaccuracy rates for individual workers on one task.Both the overall variability, as well as the prospectof identifying high-volume but low-quality workers,suggest that controlling for individual worker qual-ity could yield higher quality overall judgments.In general, there are at least three ways to enhancequality in the face of worker error.
More work-ers can be used, as described in previous sections.Another method is to use Amazon?s compensationmechanisms to give monetary bonuses to highly-performing workers and deny payments to unreli-able ones; this is useful, but beyond the scope ofthis paper.
In this section we explore a third alterna-tive, to model the reliability and biases of individualworkers and correct for them.A wide number of methods have been explored tocorrect for the bias of annotators.
Dawid and Skene(1979) are the first to consider the case of havingmultiple annotators per example but unknown truelabels.
They introduce an EM algorithm to simul-taneously estimate annotator biases and latent labelclasses.
Wiebe et al (1999) analyze linguistic anno-tator agreement statistics to find bias, and use a sim-ilar model to correct labels.
A large literature in bio-statistics addresses this same problem for medicaldiagnosis.
Albert and Dodd (2004) review severalrelated models, but argue they have various short-comings and emphasize instead the importance ofhaving a gold standard.Here we take an approach based on gold standardlabels, using a small amount of expert-labeled train-ing data in order to correct for the individual biasesof different non-expert annotators.
The idea is to re-calibrate worker?s responses to more closely matchexpert behavior.
We focus on categorical examples,though a similar method can be used with numericdata.5.1 Bias correction in categorical dataFollowing Dawid and Skene, we model labels andworkers with a multinomial model similar to NaiveBayes.
Every example i has a true label xi.
For sim-plicity, assume two labels {Y,N}.
Several differ-ent workers give labels yi1, yi2, .
.
.
yiW .
A worker?sconditional probability of response is modeled asmultinomial, and we model each worker?s judgmentas conditionally independent of other workers giventhe true label xi, i.e.
:P (yi1, .
.
.
, yiW , xi) =(?wP (yiw|xi))p(xi)To infer the posterior probability of the true labelfor a new example, worker judgments are integratedvia Bayes rule, yielding the posterior log-odds:logP (xi = Y |yi1 .
.
.
yiW )P (xi = N |yi1 .
.
.
yiW )=?wlogP (yiw|xi = Y )P (yiw|xi = N)+ logP (xi = Y )P (xi = N)260The worker response likelihoods P (yw|x = Y )and P (yw|x = N) can be directly estimated fromfrequencies of worker performance on gold standardexamples.
(If we used maximum likelihood esti-mation with no Laplace smoothing, then each yw|xis just the worker?s empirical confusion matrix.
)For MAP label estimation, the above equation de-scribes a weighted voting rule: each worker?s vote isweighted by their log likelihood ratio for their givenresponse.
Intuitively, workers who are more than50% accurate have positive votes; workers whosejudgments are pure noise have zero votes; and an-ticorrelated workers have negative votes.
(A simplerform of the model only considers accuracy rates,thus weighting worker votes by log accw1?accw .
But weuse the full unconstrained multinomial model here.
)5.1.1 Example tasks: RTE-1 and eventannotationWe used this model to improve accuracy on theRTE-1 and event annotation tasks.
(The other cate-gorical task, word sense disambiguation, could notbe improved because it already had maximum accu-racy.)
First we took a sample of annotations givingk responses per example.
Within this sample, wetrained and tested via 20-fold cross-validation acrossexamples.
Worker models were fit using Laplacesmoothing of 1 pseudocount; label priors were uni-form, which was reasonably similar to the empiricaldistribution for both tasks.annotatorsaccuracy0.70.80.9RTEannotators0.70.80.9before/afterGold calibratedNaive votingFigure 7: Gold-calibrated labels versus raw labelsFigure 7 shows improved accuracy at differentnumbers of annotators.
The lowest line is for thenaive 50% majority voting rule.
(This is equivalentto the model under uniform priors and equal accu-racies across workers and labels.)
Each point is thedata set?s accuracy against the gold labels, averagedacross resamplings each of which obtains k annota-tions per example.
RTE has an average +4.0% ac-curacy increase, averaged across 2 through 10 anno-tators.
We find a +3.4% gain on event annotation.Finally, we experimented with a similar calibrationmethod for numeric data, using a Gaussian noisemodel for each worker: yw|x ?
N(x + ?w, ?w).On the affect task, this yielded a small but consis-tent increases in Pearson correlation at all numbersof annotators, averaging a +0.6% gain.6 Training a system with non-expertannotationsIn this section we train a supervised affect recogni-tion system with expert vs. non-expert annotations.6.1 Experimental DesignFor the purpose of this experiment we create a sim-ple bag-of-words unigram model for predicting af-fect and valence, similar to the SWAT system (Katzet al, 2007), one of the top-performing systems onthe SemEval Affective Text task.7 For each tokent in our training set, we assign t a weight for eachemotion e equal to the average emotion score ob-served in each headline H that t participates in.
i.e.,if Ht is the set of headlines containing the token t,then:Score(e, t) =?H?Ht Score(e,H)|Ht|With these weights of the individual tokens wemay then compute the score for an emotion e of anew headline H as the average score over the set oftokens t ?
H that we?ve observed in the training set(ignoring those tokens not in the training set), i.e.
:Score(e,H) =?t?HScore(e, t)|H|Where |H| is simply the number of tokens inheadline H , ignoring tokens not observed in thetraining set.7 Unlike the SWAT system we perform no lemmatization,synonym expansion, or any other preprocessing of the tokens;we simply use whitespace-separated tokens within each head-line.2616.2 ExperimentsWe use 100 headlines as a training set (examples500-599 from the test set of SemEval Task 14), andwe use the remaining 900 headlines as our test set.Since we are fortunate to have the six separate ex-pert annotations in this task, we can perform an ex-tended systematic comparison of the performance ofthe classifier trained with expert vs. non-expert data.Emotion 1-Expert 10-NE k k-NEAnger 0.084 0.233 1 0.172Disgust 0.130 0.231 1 0.185Fear 0.159 0.247 1 0.176Joy 0.130 0.125 ?
?Sadness 0.127 0.174 1 0.141Surprise 0.060 0.101 1 0.061Valence 0.159 0.229 2 0.146Avg.
Emo 0.116 0.185 1 0.135Avg.
All 0.122 0.191 1 0.137Table 4: Performance of expert-trained and non-expert-trained classifiers on test-set.
k is the minimum numberof non-experts needed to beat an average expert.For this evaluation we compare the performanceof systems trained on expert and non-expert annota-tions.
For each expert annotator we train a systemusing only the judgments provided by that annota-tor, and then create a gold standard test set using theaverage of the responses of the remaining five label-ers on that set.
In this way we create six indepen-dent expert-trained systems and compute the aver-age across their performance, calculated as Pearsoncorrelation to the gold standard; this is reported inthe ?1-Expert?
column of Table 4.Next we train systems using non-expert labels;for each possible subset of n annotators, for n ?
{1, 2, .
.
.
, 10} we train a system, and evaluate bycalculating Pearson correlation with the same set ofgold standard datasets used in the expert-trained sys-tem evaluation.
Averaging the results of these stud-ies yields the results in Table 4.As in Table 2 we calculate the minimum numberof non-expert annotations per example k required onaverage to achieve similar performance to the ex-pert annotations; surprisingly we find that for fiveof the seven tasks, the average system trained with asingle set of non-expert annotations outperforms theaverage system trained with the labels from a sin-gle expert.
One possible hypothesis for the causeof this non-intuitive result is that individual labelers(including experts) tend to have a strong bias, andsince multiple non-expert labelers may contribute toa single set of non-expert annotations, the annotatordiversity within the single set of labels may have theeffect of reducing annotator bias and thus increasingsystem performance.7 ConclusionWe demonstrate the effectiveness of using AmazonMechanical Turk for a variety of natural languageannotation tasks.
Our evaluation of non-expert la-beler data vs. expert annotations for five tasks foundthat for many tasks only a small number of non-expert annotations per item are necessary to equalthe performance of an expert annotator.
In a detailedstudy of expert and non-expert agreement for an af-fect recognition task we find that we require an av-erage of 4 non-expert labels per item in order to em-ulate expert-level label quality.
Finally, we demon-strate significant improvement by controlling for la-beler bias.AcknowledgmentsThanks to Nathanael Chambers, Annie Zaenen,Rada Mihalcea, Qi Su, Panos Ipeirotis, Bob Car-penter, David Vickrey, William Morgan, and LukasBiewald for useful discussions, and for the gener-ous support of Dolores Labs.
This work was sup-ported in part by the Disruptive Technology Office(DTO)?s Advanced Question Answering for Intelli-gence (AQUAINT) Phase III Program.ReferencesPaul S. Albert and Lori E. Dodd.
2004.
A CautionaryNote on the Robustness of Latent Class Models forEstimating Diagnostic Error without a Gold Standard.Biometrics, Vol.
60 (2004), pp.
427-435.Collin F. Baker, Charles J. Fillmore and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proc.
ofCOLING-ACL 1998.Michele Banko and Eric Brill.
2001.
Scaling to VeryVery Large Corpora for Natural Language Disam-biguation.
In Proc.
of ACL-2001.Junfu Cai, Wee Sun Lee and Yee Whye Teh.
2007.
Im-proving Word Sense Disambiguation Using Topic Fea-tures.
In Proc.
of EMNLP-2007 .262Timothy Chklovski and Rada Mihalcea.
2002.
Buildinga sense tagged corpus with Open Mind Word Expert.In Proc.
of the Workshop on ?Word Sense Disam-biguation: Recent Successes and Future Directions?,ACL 2002.Timothy Chklovski and Yolanda Gil.
2005.
TowardsManaging Knowledge Collection from Volunteer Con-tributors.
Proceedings of AAAI Spring Symposiumon Knowledge Collection from Volunteer Contributors(KCVC05).Ido Dagan, Oren Glickman and Bernardo Magnini.2006.
The PASCAL Recognising Textual EntailmentChallenge.
Machine Learning Challenges.
LectureNotes in Computer Science, Vol.
3944, pp.
177-190,Springer, 2006.Wisam Dakka and Panagiotis G. Ipeirotis.
2008.
Au-tomatic Extraction of Useful Facet Terms from TextDocuments.
In Proc.
of ICDE-2008.A.
P. Dawid and A. M. Skene.
1979.
Maximum Like-lihood Estimation of Observer Error-Rates Using theEM Algorithm.
Applied Statistics, Vol.
28, No.
1(1979), pp.
20-28.Michael Kaisser and John B. Lowe.
2008.
A Re-search Collection of QuestionAnswer Sentence Pairs.In Proc.
of LREC-2008.Michael Kaisser, Marti Hearst, and John B. Lowe.2008.
Evidence for Varying Search Results SummaryLengths.
In Proc.
of ACL-2008.Phil Katz, Matthew Singleton, Richard Wicentowski.2007.
SWAT-MP: The SemEval-2007 Systems forTask 5 and Task 14.
In Proc.
of SemEval-2007.Aniket Kittur, Ed H. Chi, and Bongwon Suh.
2008.Crowdsourcing user studies with Mechanical Turk.
InProc.
of CHI-2008.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics 19:2, June 1993.George A. Miller and William G. Charles.
1991.
Con-textual Correlates of Semantic Similarity.
Languageand Cognitive Processes, vol.
6, no.
1, pp.
1-28, 1991.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T. Bunke.
1993.
A semantic concordance.
InProc.
of HLT-1993.Preslav Nakov.
2008.
Paraphrasing Verbs for NounCompound Interpretation.
In Proc.
of the Workshopon Multiword Expressions, LREC-2008.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The Proposition Bank: A Corpus Annotated with Se-mantic Roles.
Computational Linguistics, 31:1.Sameer Pradhan, Edward Loper, Dmitriy Dligach andMartha Palmer.
2007.
SemEval-2007 Task-17: En-glish Lexical Sample, SRL and All Words.
In Proc.of SemEval-2007 .James Pustejovsky, Patrick Hanks, Roser Saur, AndrewSee, Robert Gaizauskas, Andrea Setzer, DragomirRadev, Beth Sundheim, David Day, Lisa Ferro andMarcia Lazo.
2003.
The TIMEBANK Corpus.
InProc.
of Corpus Linguistics 2003, 647-656.Philip Resnik.
1999.
Semantic Similarity in a Taxon-omy: An Information-Based Measure and its Applica-tion to Problems of Ambiguity in Natural Language.JAIR, Volume 11, pages 95-130.Herbert Rubenstein and John B. Goodenough.
1965.Contextual Correlates of Synonymy.
Communicationsof the ACM, 8(10):627?633.Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeiro-tis.
2008.
Get Another Label?
Improving Data Qual-ity and Data Mining Using Multiple, Noisy Labelers.In Proc.
of KDD-2008.Push Singh.
2002.
The public acquisition of common-sense knowledge.
In Proc.
of AAAI Spring Sympo-sium on Acquiring (and Using) Linguistic (and World)Knowledge for Information Access, 2002.Alexander Sorokin and David Forsyth.
2008.
Util-ity data annotation with Amazon Mechanical Turk.To appear in Proc.
of First IEEE Workshop onInternet Vision at CVPR, 2008.
See also:http://vision.cs.uiuc.edu/annotation/David G. Stork.
1999.
The Open Mind Initiative.IEEE Expert Systems and Their Applications pp.
16-20, May/June 1999.Carlo Strapparava and Rada Mihalcea.
2007.
SemEval-2007 Task 14: Affective Text In Proc.
of SemEval-2007.Qi Su, Dmitry Pavlov, Jyh-Herng Chow, and Wendell C.Baker.
2007.
Internet-Scale Collection of Human-Reviewed Data.
In Proc.
of WWW-2007.Luis von Ahn and Laura Dabbish.
2004.
Labeling Im-ages with a Computer Game.
In ACM Conference onHuman Factors in Computing Systems, CHI 2004.Luis von Ahn, Mihir Kedia and Manuel Blum.
2006.Verbosity: A Game for Collecting Common-SenseKnowledge.
In ACM Conference on Human Factorsin Computing Systems, CHI Notes 2006.Ellen Voorhees and Hoa Trang Dang.
2006.
Overview ofthe TREC 2005 question answering track.
In Proc.
ofTREC-2005.Janyce M. Wiebe, Rebecca F. Bruce and Thomas P.O?Hara.
1999.
Development and use of a gold-standard data set for subjectivity classifications.
InProc.
of ACL-1999.Annie Zaenen.
Submitted.
Do give a penny for theirthoughts.
International Journal of Natural LanguageEngineering (submitted).263
