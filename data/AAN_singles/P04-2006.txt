iSTART: Paraphrase RecognitionChutima BoonthumComputer Science DepartmentOld Dominion University, Norfolk, VA-23508  USAcboont@cs.odu.eduAbstractParaphrase recognition is used in a num-ber of applications such as tutoring sys-tems, question answering systems, andinformation retrieval systems.
The con-text of our research is the iSTART read-ing strategy trainer for science texts,which needs to understand and recognizethe trainee?s input and respond appropri-ately.
This paper describes the motivationfor paraphrase recognition and develops adefinition of the strategy as well as a rec-ognition model for paraphrasing.
Lastly,we discuss our preliminary implementa-tion and research plan.1 IntroductionA web-based automated reading strategy trainercalled iSTART (Interactive Strategy Trainer forActive Reading and Thinking) adaptively assignsindividual students to appropriate reading train-ing programs.
It follows the SERT (Self-Explanation Reading Training) methodology de-veloped by McNamara (in press) as a way to im-prove high school students?
reading ability byteaching them to use active reading strategies inself-explaining difficult texts.
Details of thestrategies can be found in McNamara (in press)and of iSTART in Levinstein et al (2003)During iSTART?s practice module, the studentself-explains a sentence.
Then the trainer ana-lyzes the student?s explanation and responds.
Thecurrent system uses simple word- matching algo-rithms to evaluate the student?s input that do notyield results that are sufficiently reliable or accu-rate.
We therefore propose a new system for han-dling the student?s explanation more effectively.Two major tasks of this semantically-based sys-tem are to (1) construct an internal representationof sentences and explanations and (2) recognizethe reading strategies the student uses beginningwith paraphrasing.Construct an Internal Representation: Wetransform the natural language explanation into arepresentation suitable for later analysis.
TheSentence Parser gives us a syntactically andmorphologically tagged representation.
We trans-form the output of the Link Grammar parser(CMU, 2000) that generates syntactical and mor-phological information into an appropriateknowledge representation using the Representa-tion Generator.Recognize Paraphrasing: In what follows,we list the paraphrase patterns that we plan tocover and define a recognition model for eachpattern.
This involves two steps: (1) recognizingparaphrasing patterns, and (2) reporting the re-sult.
The Paraphrase Recognizer compares twointernal representation (one is of a given sentenceand another is of the student?s explanation) andfinds paraphrase matches (?concept-relation-concept?
triplet matches) according to a para-phrasing pattern.
The Reporter provides the finalsummary of the total paraphrase matches, notingunmatched information in either the sentence orthe explanation.
Based on the similarity measure,the report will include whether the student hasfully or partially paraphrased a given sentenceand whether it contains any additional informa-tion.2 ParaphraseWhen two expressions describe the same situa-tion, each is considered to be a paraphrase of theother.
There is no precise paraphrase definition ingeneral; instead there are frequently-acceptedparaphrasing patterns to which various authori-ties refer.
Academic writing centers (ASU Writ-ing Center, 2000; BAC Writing Center; USCAWriting Room; and Hawes, 2003) provide anumber of characterizations, such as using syno-nyms, changing part-of-speech, reordering ideas,breaking a sentence into smaller ones, using defi-nitions, and using examples.
McNamara (inpress), on the other hand, does not consider usingdefinitions or examples to be part of paraphras-ing, but rather considers them elaboration.
Stede(1996) considers different aspects or intentions tobe paraphrases if they mention the same contentor situation.Instead of attempting to find a single para-phrase definition, we will start with six com-monly mentioned paraphrasing patterns:1.
Synonym: substitute a word with its syno-nym, e.g.
help, assist, aid;2.
Voice: change the voice of sentence from ac-tive to passive or vice versa;3.
Word-Form/Part-of-speech: change a wordinto a different form, e.g.
change a noun to averb, adverb, or adjective;4.
Break down Sentence: break a long sen-tence down into small sentences;5.
Definition/Meaning: substitute a word withits definition or meaning;6.
Sentence Structure: use different sentencestructures to express the same thing.If the explanation has any additional informationor misses some information that appeared in theoriginal sentence, we should be able to detect thisas well for use in discovering additional strate-gies employed.3 Recognition ModelTo recognize paraphrasing, we convert naturallanguage sentences into Conceptual Graphs (CG,Sowa, 1983; 1992) and then compare two CGsfor matching according to paraphrasing patterns.The matching process is to find as many ?con-cept-relation-concept triplet?
matches as possi-ble.
A triplet match means that a triplet from thestudent?s input matches with a triplet from thegiven sentence.
In particular, the left-concept,right-concept, and relation of both sub-graphshave to be exactly the same, or the same under atransformation based on a relationship of synon-ymy (or other relation defined in WordNet), orthe same because of idiomatic usage.
It is alsopossible that several triplets of one sentence to-gether match a single triplet of the other.
At theend of this pattern matching, a summary result isprovided: total paraphrasing matches, unpara-phrased information and additional information(not appearing in the given sentence).3.1 Conceptual Graph GenerationA natural language sentence is converted into aconceptual graph using the Link Grammar parser.This process mainly requires mapping one ormore Link connector types into a relation of theconceptual graph.A parse from the Link Grammar consists oftriplets: starting word, an ending word, and aconnector type between these two words.
Forexample, [1 2 (Sp)] means word-1 connects toword-2 with a subject connector or that word-1 isthe subject of word-2.
The sentence ?A walnut iseaten by a monkey?
is parsed as follows:[(0=LEFT-WALL)(1=a)(2=walnut.n)(3=is.v)(4=eaten.v)(5=by)(6=a)(7=monkey.n)(8=.
)][[0 8 (Xp)][0 2 (Wd)][1 2 (Dsu)][2 3 (Ss)][3 4 (Pv)][4 5 (MVp)][5 7 (Js)][6 7 (Ds)]]We then convert each Link triplet into a corre-sponding CG triplet.
Two words in the Link trip-let can be converted into two concepts of the CG.To decide whether to put a word on the left or theright side of the CG triplet, we define a mappingrule for each Link connector type.
For example, aLink triplet [1 2 (S*)] will be mapped to the?Agent?
relation, with word-2 as the left-conceptand word-1 as the right-concept: [Word-2] ?
(Agent) ?
[Word-1].
Sometimes it is necessaryto consider several Link triplets in generating asingle CG triplet.
A CG of previous example isshown below:0 [0 8 (Xp)]  -> #S#  -> - N/A -1 [0 2 (Wd)]  -> #S#  -> - N/A -2 [1 2 (Dsu)] -> #S#  ->[walnut.n]->(Article)->[a]3 [2 3 (Ss)] -> #M# S + Pv (4) # ->[eaten.v]->(Patient)->[walnut.n]4 [3 4 (Pv)] -> #M# Pv +MV(5)+O(6)# ->[eaten.v] -> (Agent) -> [monkey.n]5 [4 5 (MVp)] -> #S#  eaten.v by6 [5 7 (Js)]  -> #S#  monkey.n by7 [6 7 (Ds)]  -> #S#  ->[monkey.n] -> (Article) -> [a]Each line (numbered 0-7) shows a Link tripletand its corresponding CG triplet.
These will beused in the recognition process.
The ?#S#?
and?#M?
indicate single and multiple mapping rules.3.2 Paraphrase RecognitionWe illustrate our approach to paraphrase patternrecognition on single sentences: using synonyms(single or compound-word synonyms and idio-matic expressions), changing the voice, using adifferent word form, breaking a long sentenceinto smaller sentences, substituting a definitionfor a word, and changing the sentence structure.Preliminaries: Before we start the recognitionprocess, we need to assume that we have all theinformation about the text: each sentence hasvarious content words (excluding such ?stopwords?
as a, an, the, etc.
); each content word hasa definition together with a list of synonyms, an-tonyms, and other relations provided by WordNet(Fellbaum, 1998).
To prepare a given text and asentence, we plan to have an automated processthat generates necessary information as well asmanual intervention to verify and rectify theautomated result, if necessary.Single-Word Synonyms: First we discoverthat both CGs have the same pattern and then wecheck whether words in the same position aresynonyms.
Example:?Jenny helps Kay?
[Help]  ?
(Agent) ?
[Person: Jenny]+??
(Patient) ?
[Person: Kay]vs.?Jenny assists Kay?
[Assist]  ?
(Agent) ?
[Person: Jenny]+??
(Patient) ?
[Person: Kay]Compound-Word Synonyms: In this case,we need to be able to match a word and its com-pound-word synonym.
For example, ?install?
has?set up?
and ?put in?
as its compound-word syno-nyms.
The compound words are declared by theparser program.
During the preliminary process-ing CGs are pre-generated.
[Install] ?
(Object) ?
[Thing]?
[Set-Up] ?
(Object) ?
[Thing]?
[Put-In] ?
(Object) ?
[Thing]Then, this case will be treated like the single-word synonym.
?Jenny installs a computer?
[Install]  ?
(Agent) ?
[Person: Jenny]+??
(Object) ?
[Computer]vs.?Jenny sets up a computer?
[Set-Up]  ?
(Agent) ?
[Person: Jenny]+??
(Object) ?
[Computer]Idiomatic Clause/Phrase: For each idiom, aCG will be generated and used in the comparisonprocess.
For example, the phrase ?give someone ahand?
means ?help?.
The preliminary process willgenerate the following conceptual graph:[Help] ?
(Patient) ?
[Person: x]?
[Give] ?
(Patient) ?
[Person: x]+??
(Object) ?
[Hand]which gives us?Jenny gives Kay a hand?
[Give]  ?
(Agent) ?
[Person: Jenny]+??
(Patient) ?
[Person: Kay]+??
(Object) ?
[Hand]In this example, one might say that a ?hand?might be an actual (physical) hand rather than asynonym phrase for ?help?.
To reduce this par-ticular ambiguity, the analysis of the context maybe necessary.Voice: Even if the voice of a sentence ischanged, it will have the same CG.
For example,both ?Jenny helps Kay?
and ?Kay is helped byJenny?
have the same graphs as follows:[Help]  ?
(Agent) ?
[Person: Jenny]+??
(Patient) ?
[Person: Kay]At this time we are assuming that if two CGs areexactly the same, it means paraphrasing bychanging voice pattern.
However, we plan to in-troduce a modified conceptual graph that retainsthe original sentence structure so that we can ver-ify that it was paraphrasing by change of voiceand not simple copying.Part-of-speech: A paraphrase can be gener-ated by changing the part-of-speech of somekeywords.
In the following example, the studentuses ?a historical life story?
instead of ?life his-tory?, and ?similarity?
instead of ?similar?.Original sentence: ?All thunderstorms have a similarlife history.
?Student?s Explanation: ?All thunderstorms havesimilarity in their historical life story.
?To find this paraphrasing pattern, we look for thesame word, or a word that has the same base-form.
In this example, the sentences share thesame base-form for ?similar?
and ?similarity?
aswell as for ?history?
and ?historical?.Breaking long sentence: A sentence can beexplained by small sentences coupled up togetherin such a way that each covers a part of the origi-nal sentence.
We integrate CGs of all sentencesin the student?s input together before comparingit with the original sentence.Original sentence: ?All thunderstorms have a similarlife history.?
[Thunderstorm: ?]
?
(Feature) ?
[History] ?
(Attribute) ?
[Life](Attribute) ?
[Similar]Student?s Explanation: ?Thunderstorms have lifehistory.
It is similar among all thunderstorms?
[Thunderstorm] ?
(Feature) ?
[History] ?
(Attribute) ?
[Life][It] (pronoun)?
(Attribute) ?
[Similar](Mod) ?
[Thunderstorm: ?]
(among)We will provisionally assume that the studentuses only the words that appear in the sentence inthis breaking down process.
One solution is tocombine graphs from all sentences together.
Thiscan be done by merging graphs of the same con-cept.
This process involves pronoun resolution.In this example, ?it?
could refer to ?life?
or ?his-tory?.
Our plan is to exercise all possible pronounreferences and select one that gives the best para-phrasing recognition result.Definition/Meaning: A CG is pre-generatedfor a definition of each word and its associations(synonyms, idiomatic expressions, etc.).
To finda paraphrasing pattern of using the definition, forexample, a ?history?
means ?the continuum ofevents occurring in succession leading from thepast to the present and even into the future?, webuild a CG for this as shown below:[Continuum] ?
(Attribute) ?
[Event: ?
][Occur] ?
(Patient) ?
[Event: ?
](Mod) ?
[Succession] (in)[Lead] ?
(Initiator) ?
[Succession](Source) ?
[Time: Past] (from)(Path) ?
[Time: Present] (to)(Path) ?
[Time: Future] (into)We refine this CG by incorporating CGs of thedefinition into a single integrated CG, if possible.
(Patient) ?
[Event: ?
](Mod) ?
[Succession] (in)(Source) ?
[Time: Past] (from)(Path) ?
[Time: Present] (to)(Path) ?
[Time: Future] (into)From WordNet 2.0, the synonyms of ?past?, ?pre-sent?, and ?future?
found to be ?begin, start, be-ginning process?, ?middle, go though, middleprocess?, and ?end, last, ending process?, respec-tively.
The following example shows how theycan be used in recognizing paraphrases.Original sentence: ?All thunderstorms have a similarlife history.?
[Thunderstorm: ?]
?
(Feature) ?
[History] ?
(Attribute) ?
[Life](Attribute) ?
[Similar]Student?s Explanation: ?Thunderstorms go throughsimilar cycles.
They will begin the same, go throughthe same things, and end the same way.?
[Go] ?
(Agent) ?
[Thunderstorm: #](Path) ?
[Cycle] ?
(Attribute) ?
[Similar][Begin] ?
(Agent) ?
[Thunderstorm: #](Attribute) ?
[Same][Go-Through] ?
(Agent) ?
[Thunderstorm: #](Path) ?
[Thing: ? ]
?
(Attribute) ?
[Same][End] ?
(Agent) ?
[Thunderstorm: #](Path) ?
[Way: ? ]
?
(Attribute) ?
[Same]From this CG, we found the use of ?begin?, ?go-through?, and ?end?, which are parts of the CG ofhistory?s definition.
These together with the cor-respondence of words in the sentences show thatthe student has used paraphrasing by using adefinition of ?history?
in the self-explanation.Sentence Structure: The same thing can besaid in a number of different ways.
For example,to say ?There is someone happy?, we can say?Someone is happy?, ?A person is happy?, or?There is a person who is happy?, etc.
As can beeasily seen, all sentences have a similar CG trip-let of ?
[Person: ?]
?
(Char) ?
[Happy]?
in theirCGs.
But, we cannot simply say that they areparaphrases of each other; therefore, need tostudy more on possible solutions.3.3 Similarity MeasureThe similarity between the student?s input andthe given sentence can be categorized into one ofthese four cases:1.
Complete paraphrase without extra info.2.
Complete paraphrase with extra info.3.
Partial paraphrase without extra info.4.
Partial paraphrase with extra info.To distinguish between ?complete?
and ?partial?paraphrasing, we will use the triplet matchingresult.
What counts as complete depends on thecontext in which the paraphrasing occurs.
If weconsider the paraphrasing as a writing technique,the ?complete?
paraphrasing would mean that alltriplets of the given sentence are matched tothose in the student?s input.
Similarly, if any trip-lets in the given sentence do not have a match, itmeans that the student is ?partially?
paraphrasingat best.
On the other hand, if we consider theparaphrasing as a reading behavior or strategy,the ?complete?
paraphrasing may not need alltriplets of the given sentence to be matched.Hence, recognizing which part of the student?sinput is a paraphrase of which part of the givensentence is significant.
How can we tell that thisexplanation is an adequate paraphrase?
Can weuse information provided in the given sentence asa measurement?
If so, how can we use it?
Thesequestions still need to be answered.4 Related WorkA number of people have worked on paraphras-ing such as the multilingual-translation recogni-tion by Smith (2003), the multilingual sentencegeneration by Stede (1996), universal modelparaphrasing using transformation by Murata andIsahara (2001), DIRT ?
using inference rules inquestion answering and information retrieval byLin and Pantel (2001).
Due to the space limita-tion we will mention only a few related works.ExtrAns (Extracting answers from technicaltexts) by (Molla et al 2003) and (Rinaldi et al2003) uses minimal logical forms (MLF) to rep-resent both texts and questions.
They identifyterminological paraphrases by using a term-basedhierarchy with their synonyms and variations;and syntactic paraphrases by constructing acommon representation for different types of syn-tactic variation via meaning postulates.
Absent aparaphrase, they loosen the criteria by using hy-ponyms, finding highest overlap of predicates,and simple keyword matching.Barzilay & Lee (2003) also identify para-phrases in their paraphrased sentence generationsystem.
They first find different paraphrasingrules by clustering sentences in comparable cor-pora using n-gram word-overlap.
Then for eachcluster, they use multi-sequence alignment to findintra-cluster paraphrasing rules: either morpho-syntactic or lexical patterns.
To identify inter-cluster paraphrasing, they compare the slot val-ues without considering word ordering.In our system sentences are represented byconceptual graphs.
Paraphrases are recognizedthrough idiomatic expressions, definition, andsentence break up.
Morpho-syntatic variationsare also used but in more general way than theterm hierarchy-based approach of ExtrAns.5 Preliminary ImplementationWe have implemented two components to recog-nize paraphrasing with the CG for a single simplesentence: Automated Conceptual Graph Genera-tor and Automated Paraphrasing Recognizer.Automated Conceptual Graph Generator: is aC++ program that calls the Link Grammar API toget the parse result for the input sentence, andgenerates a CG.
We can generate a CG for a sim-ple sentence using the first linkage result.
Futureversions will deal with complex sentence struc-ture as well as multiple linkages, so that we cancover most paraphrases.Automated Paraphrasing Recognizer: The in-put to the Recognizer is a pair of CGs: one fromthe original sentence and another from the stu-dent?s explanation.
Our goal is to recognizewhether any paraphrasing was used and, if so,what was the paraphrasing pattern.
Our first im-plementation is able to recognize paraphrasing ona single sentence for exact match, direct synonymmatch, first level antonyms match, hyponyms andhypernyms match.
We plan to cover more rela-tionships available in WordNet as well as defini-tions, idioms, and logically equivalentexpressions.
Currently, voice difference is treatedas an exact match because both active voiceshave the same CGs and we have not yet modifiedthe conceptual graph as indicated above.6 Discussion and Remaining WorkOur preliminary implementation shows us thatparaphrase recognition is feasible and allows usto recognize different types of paraphrases.
Wecontinue to work on this and improve our recog-nizer so that it can handle more word relationsand more types of paraphrases.
During the test-ing, we will use data gathered during our previ-ous iSTART trainer experiments.
These are theactual explanations entered by students who weregiven the task of explaining sentences.
Fortu-nately, quite a bit of these data have been evalu-ated by human experts for quality of explanation.Therefore, we can validate our paraphrasing rec-ognition result against the human evaluation.Besides implementing the recognizer to coverall paraphrasing patterns addressed above, thereare many issues that need to be solved and im-plemented during this course of research.The Representation for a simple sentence isthe Conceptual Graph, which is not powerfulenough to represent complex, compound sen-tences, multiple sentences, paragraphs, or entiretexts.
We will use Rhetorical Structure Theory(RST) to represent the relations among the CGsof these components of these more complexstructures.
This will also involve Pronoun Reso-lution as well as Discourse Chunking.
Once arepresentation has been selected, we will imple-ment an automated generator for such representa-tion.The Recognizer and Paraphrase Reporter haveto be completed.
The similarity measures forwriting technique and reading behavior must stillbe defined.Once all processes have been implemented, weneed to verify that they are correct and validatethe results.
Finally, we can integrate this recog-nition process into the iSTART trainer in order toimprove the existing evaluation system.AcknowledgementsThis dissertation work is under the supervision ofDr.
Shunichi Toida and Dr. Irwin Levinstein.iSTART is supported by National Science Foun-dation grant REC-0089271.ReferencesASU Writing Center.
2000.
Paraphrasing: RestatingIdeas in Your Own Words.
Arizona State Univer-sity, Tempe: AZ.BAC Writing Center.
Paraphrasing.
Boston Architec-tural Center.
Boston: MA.Carnegie Mellon University.
2000.
Link Grammar.R.
Barzilay and L. Lee.
2003.
Learning to Paraphrase:An Unsupervised Approach Using Multiple-Sequence Alignment.
In HLT-NAACL, Edmonton:Canada, pp.
16-23.C.
Boonthum, S. Toida, and I. Levinstein.
2003.
Para-phrasing Recognition through Conceptual Graphs.Computer Science Department, Old DominionUniversity, Norfolk: VA. (TR# is not available)C. Boonthum.
2004. iSTART: Paraphrasing Recogni-tion.
Ph.D. Proposal: Computer Science Depart-ment, Old Dominion University, VA.C.
Fellbaum.
1998.
WordNet: an electronic lexicaldatabase.
The MIT Press: MA.K.
Hawes.
2003.
Mastering Academic Writing: Writea Paraphrase Sentence.
University of Memphis,Memphis: TN.I.
Levinstein, D. McNamara, C. Boonthum, S. Pillar-isetti, and K. Yadavalli.
2003.
Web-Based Inter-vention for Higher-Order Reading Skills.
In ED-MEDIA, Honolulu: HI, pp.
835-841.D.
Lin and P. Pantel.
2001.
Discovery of InferenceRules for Question Answering.
Natural LanguageEngineering 7(4):343-360.W.
Mann and S. Thompson, 1987.
Rhetorical Struc-ture Theory: A Theory of Text Organization.
TheStructure of Discourse, Ablex.D.
McNamara.
(in press).
SERT: Self-ExplanationReading Training.
Discourse Processes.D.
Molla, R. Schwitter, F. Rinaldi, J. Dowdall, and M.Hess.
2003.
ExtrAns: Extracting Answers fromTechnical Texts.
IEEE Intelligent System 18(4):12-17.M.
Murata and H. Isahara.
2001.
Universal Model forParaphrasing ?
Using Transformation Based on aDefined Criteria.
In NLPRS: Workshop on Auto-matic Paraphrasing: Theories and Application.F.
Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and D.Molla.
2003.
Exploiting Paraphrases in QuestionAnswering System.
In ACL: Workshop in Para-phrasing, Sapporo: Japan, pp.
25-32.N.
Smith.
2002.
From Words to Corpora: RecognizingTranslation.
In EMNLP, Philadelphia: PA.J.
Sowa.
1983.
Conceptual Structures: InformationProcessing in Mind and Machine.
Addison-Wesley,MA.J.
Sowa.
1992.
Conceptual Graphs as a UniversalKnowledge Representation.
Computers Math.
Ap-plication, 23(2-5): 75-93.M.
Stede.
1996.
Lexical semantics and knowledgerepresentation in multilingual sentence generation.Ph.D.
thesis: Department of Computer Science,University of Toronto, Canada.USCA Writing Room.
Paraphrasing.
The Universityof South Carolina: Aiken.
Aiken: SC.
