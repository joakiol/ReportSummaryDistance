TALP System for the English Lexical Sample TaskGerard Escudero?, Llu?
?s Ma`rquez?
and German Rigau?
?TALP Research Center.
EUETIB.
LSI.
UPC.
escudero@lsi.upc.es?TALP Research Center.
LSI.
UPC.
lluism@lsi.upc.es?
IXA Group.
UPV/EHU.
rigau@si.ehu.es1 IntroductionThis paper describes the TALP system on the En-glish Lexical Sample task of the Senseval-31 event.The system is fully supervised and relies on a par-ticular Machine Learning algorithm, namely Sup-port Vector Machines.
It does not use extra exam-ples than those provided by Senseval-3 organisers,though it uses external tools and ontologies to ex-tract part of the representation features.Three main characteristics have to be pointed outfrom the system architecture.
The first thing is theway in which the multiclass classification problemposed by WSD is addressed using the binary SVMclassifiers.
Two different approaches for binarizingmulticlass problems have been tested: one?vs?alland constraint classification.
In a cross-validationexperimental setting the best strategy has been se-lected at word level.
Section 2 is devoted to explainthis issue in detail.The second characteristic is the rich set of fea-tures used to represent training and test examples.Topical and local context features are used as usual,but also syntactic relations and semantic features in-dicating the predominant semantic classes in the ex-ample context are taken into account.
A detaileddescription of the features is presented in section 3.And finally, since each word represents a learningproblem with different characteristics, a per?wordfeature selection has been applied.
This tuning pro-cess is explained in detail in section 4.The last two sections discuss the experimental re-sults (section 5) and present the main conclusions ofthe work performed (section 6).2 Learning FrameworkThe TALP system belongs to the supervised Ma-chine Learning family.
Its core algorithm is theSupport Vector Machines (SVM) learning algorithm(Cristianini and Shawe-Taylor, 2000).
Given a setof binary training examples, SVMs find the hy-perplane that maximizes the margin in a high di-1http://www.senseval.orgmensional feature space (transformed from the in-put space through the use of a non-linear function,and implicitly managed by using the kernel trick),i.e., the hyperplane that separates with maximal dis-tance the positive examples from the negatives.
Thislearning bias has proven to be very effective for pre-venting overfitting and providing good generalisa-tion.
SVMs have been also widely used in NLPproblems and applications.One of the problems in using SVM for the WSDproblem is how to binarize the multiclass classifi-cation problem.
The two approximations tested inthe TALP system are the usual one?vs?all and therecently introduced constraint?classification frame-work (Har-Peled et al, 2002).In the one?vs?all approach, the problem is de-composed into as many binary problems as classeshas the original problem, and one classifier istrained for each class trying to separate the exam-ples of that class (positives) from the examples ofall other classes (negatives).
This method assumesthe existence of a separator between each class andthe set of all other classes.
When classifying a newexample, all binary classifiers predict a class andthe one with highest confidence is selected (winner?take?all strategy).2.1 Constraint ClassificationConstraint classification (Har-Peled et al, 2002) isa learning framework that generalises many multi-class classification and ranking schemes.
It consistsof labelling each example with a set of binary con-straints indicating the relative order between pairsof classes.
For the WSD setting of Senseval-3, wehave one constraint for each correct class (sense)with each incorrect class, indicating that the clas-sifier to learn should give highest confidence to thecorrect classes than to the negatives.
For instance, ifwe have 4 possible senses {1, 2, 3, 4} and a trainingexample with labels 2 and 3, the constraints corre-sponding to the example are {(2>1), (2>4), (3>1),and (3>4)}.
The aim of the methodology is to learna classifier consistent with the partial order definedAssociation for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemsby the constraints.
Note that here we are not as-suming that perfect separators can be constructedbetween each class and the set of all other classes.Instead, the binary decisions imposed are more con-servative.Using Kesler?s construction for multiclass classi-fication, each training example is expanded into aset of (longer) binary training examples.
Findinga vector?based separator in this new training set isequivalent to find a separator for each of the binaryconstraints imposed by the problem.
The construc-tion is general, so we can use SVMs directly on theexpanded training set to solve the multiclass prob-lem.
See (Har-Peled et al, 2002) for details.3 FeaturesWe have divided the features of the system in 4 cat-egories: local, topical, knowledge-based and syn-tactic features.
First section of table 1 shows thelocal features.
The basic aim of these featuresis to modelize the information of the surroundingwords of the target word.
All these features are ex-tracted from a ?3?word?window centred on the tar-get word.
The features also contain the position ofall its components.
To obtain Part?of?Speech andlemma for each word, we used FreeLing 2.
Mostof these features have been doubled for lemma andword form.Three types of Topical features are shown in thesecond section of table 1.
Topical features try toobtain non?local information from the words of thecontext.
For each type, two overlapping sets ofredundant topical features are considered: one ex-tracted from a ?10?word?window and another con-sidering all the example.The third section of table 1 presents theknowledge?based features.
These features havebeen obtained using the knowledge contained intothe Multilingual Central Repository (MCR) of theMEANING project3 (Atserias et al, 2004).
For eachexample, the feature extractor obtains, from eachcontext, all nouns, all their synsets and their associ-ated semantic information: Sumo labels, domain la-bels, WordNet Lexicographic Files, and EuroWord-Net Top Ontology.
We also assign to each label aweight which depends on the number of labels as-signed to each noun and their relative frequenciesin the whole WordNet.
For each kind of seman-tic knowledge, summing up all these weights, theprogram finally selects those semantic labels withhigher weights.2http://www.lsi.upc.es/?nlp/freeling3http://www.lsi.upc.es/?meaninglocal feats.Feat.
Descriptionform form of the target wordlocat all part?of?speech / forms / lemmas inthe local contextcoll all collocations of two part?of?speech /forms / lemmascoll2 all collocations of a form/lemma and apart?of?speech (and the reverse)first form/lemma of the first noun / verb /adjective / adverb to the left/right of thetarget wordtopical feats.Feat.
Descriptiontopic bag of forms/lemmassbig all form/lemma bigrams of the examplecomb forms/lemmas of consecutive (or not)pairs of the open?class?words in theexampleknowledge-based feats.Feat.
Descriptionf sumo first sumo labela sumo all sumo labelsf semf first wn semantic file labela semf all wn semantic file labelsf tonto first ewn top ontology labela tonto all ewn top ontology labelsf magn first domain labela magn all domain labelssyntactical feats.Feat.
Descriptiontgt mnp syntactical relations of the target wordfrom miniparrels mnp all syntactical relations from miniparyar noun NounModifier, ObjectTo, SubjectTofor nounsyar verb Object, ObjectToPreposition, Preposi-tion for verbsyar adjs DominatingNoun for adjectivesTable 1: Feature SetFinally, the last section of table 1 describesthe syntactic features which contains features ex-tracted using two different tools: Dekang Lin?sMinipar4 and Yarowsky?s dependency pattern ex-tractor.It is worth noting that the set of features presentedis highly redundant.
Due to this fact, a feature se-lection process has been applied, which is detailedin the next section.4 Experimental SettingFor each binarization approach, we performed a fea-ture selection process consisting of two consecutivesteps:4http://www.cs.ualberta.ca/?lindek/minipar.htm?
POS feature selection: Using the Senseval?2corpus, an exhaustive selection of the best setof features for each particular Part?of?Speechwas performed.
These feature sets were takenas the initial sets in the feature selection pro-cess of Senseval-3.?
Word feature selection: We applied aforward(selection)?backward(deletion) two?step procedure to obtain the best featureselection per word.
For each word, the processstarts with the best feature set obtained in theprevious step according to its Part?of?Speech.Now, during selection, we consider thosefeatures not selected during POS featureselection, adding all features which producesome improvement.
During deletion, we con-sider only those features selected during POSfeature selection, removing all features whichproduces some improvement.
Although thisaddition?deletion procedure could be iterateduntil no further improvement is achieved, weonly performed a unique iteration becauseof the computational overhead.
One briefexperiment (not reported here) for one?vs?allachieves an increase of 2.63% in accuracyfor the first iteration and 0.52% for a secondone.
First iteration improves the accuracy of53 words and the second improves only 15.Comparing the evolution of these 15 words,the increase in accuracy is of 2.06% for thefirst iteration and 1.68% for the second one.These results may suggest that accuracy couldbe increased by this iteration procedure.The result of this process is the selection of thebest binarization approach and the best feature setfor each individual word.Considering feature selection, we have inspectedthe selected attributes for all the words and we ob-served that among these attributes there are fea-tures of all four types.
The most selected featuresare the local ones, and among them those of ?firstnoun/adjective on the left/right?
; from topical fea-tures the most selected ones are the ?comb?
and in aless measure the ?topic?
; from the knowledge?basedthe most selected feature are those of ?sumo?
and?domains labels?
; and from syntactical ones, thoseof ?Yarowsky?s patterns?.
All the features previ-ously mentioned where selected at least for 50 ofthe 57 Senseval?3 words.
Even so, it is useful theuse of all features when a selection procedure is ap-plied.
These general features do not work fine forall words.
Some words make use of the less selectedfeatures; that is, every word is a different problem.Regarding the implementation details of the sys-tem, we used SVMlight (Joachims, 2002), a very ro-bust and complete implementation of Support Vec-tor Machines learning algorithms, which is freelyavailable for research purposes5 .
A simple linealkernel with a regularization C value of 0.1 wasapplied.
This parameter was empirically decidedon the basis of our previous experiments on theSenseval?2 corpus.
Additionally, previous tests us-ing non?linear kernels did not provide better results.The selection of the best feature set and the bi-narization scheme per word described above, havebeen performed using a 5-fold cross validation pro-cedure on the Senseval-3 training set.
The five parti-tions of the training set were obtained maintaining,as much as possible, the initial distribution of exam-ples per sense.After several experiments considering the ?U?
la-bel as an additional regular class, we found that weobtained better results by simply ignoring it.
Then,if a training example was tagged only with this la-bel, it was removed from the training set.
If the ex-ample was tagged with this label and others, the ?U?label was also removed from the learning example.In that way, the TALP system do not assigns ?U?labels to the test examples.Due to lack of time, the TALP system presentedat the competition time did not include a com-plete model selection for the constraint classifica-tion binarization setting.
More precisely, 14 wordswere processed within the complete model selectionframework, and 43 were adjusted with a fixed one?vs?all approach but a complete feature selection.After the competition was closed, we implementedthe constraint classification setting more efficientlyand we reprocessed again the data.
Section 5 showsthe results of both variants.A rough estimation of the complete model selec-tion time for both approaches is the following.
Thetraining spent about 12 hours (OVA setting) and 5days (CC setting) to complete6 , suggesting that themain drawback of these approaches is the computa-tional overhead.
Fortunately, the process time canbe easily reduced: the CC layer could be portedfrom Perl to C++ and the model selection could beeasily parallelized (since the treatment of each wordis independent).5 ResultsTable 2 shows the accuracy obtained on the train-ing set and table 3 the results of our system (SE3,5http://svmlight.joachims.org6These figures were calculated using a 800 MHz PentiumIII PC with 320 Mb of memory.TALP), together with the most frequent sense base-line (mfs), the recall result of the best system in thetask (best), and the recall median between all par-ticipant systems (avg).
These last three figures wereprovided provided by the organizers of the task.OVA(base) in table 2 stands for the results of theone?vs?all approach on the starting feature set (5?fold?cross validation on the training set).
CC(base)refers to the constrain?classification setting on thestarting feature set.
OVA(best) and CC(best) meanone?vs?all and constraint?classification with theirrespective feature selection.
Finally, SE3 stands forthe system officially presented at competition time7and TALP stands for the complete architecture.method accuracyOVA(base) 72 38%CC(base) 72.28%OVA(best) 75.27%CC(best) 75.70%SE3 75.62%TALP 76.02%Table 2: Overall results of all system variants on thetraining setIt can be observed that the feature selection pro-cess consistently improves the accuracy by around3 points, both in OVA and CC binarization set-tings.
Constraint?classification is slightly betterthan one?vs?all approach when feature selectionis performed, though this improvement is not con-sistent along all individual words (detailed resultsomitted) neither statistically significant (z?test with0.95 confidence level).
Finally, the combinedbinarization?feature selection further increases theaccuracy in half a point (again this difference is notstatistically significant).measure mfs avg best SE3 TALPfine 55.2 65.1 72.9 71.3 71.6coarse 64.5 73.7 79.5 78.2 78.2Table 3: Overall results on the Senseval-3 test setHowever, when testing the complete architectureon the official test set, we obtained an accuracy de-crease of more than 4 points.
It remains to be ana-lyzed if this difference is due to a possible overfit-ting to the training corpus during model selection,or simply is due to the differences between train-ing and test corpora.
Even so, the TALP systemachieves a very good performance, since there is a7Only 14 words were processed with the full architecture.difference of only 1.3 points in fine and coarse re-call respect to the best system of the English lexicalsample task of Senseval?3.6 ConclusionsRegarding supervised Word Sense Disambiguation,each word can be considered as a different classi-fication problem.
This implies that each word hasdifferent feature models to describe its senses.We have proposed and tested a supervised sys-tem in which the examples are represented througha very rich and redundant set of features (using theinformation content coherently integrated within theMultilingual Central Repository of the MEANINGproject), and which performs a specialized selectionof features and binarization process for each word.7 AcknowledgmentsThis research has been partially funded by the Eu-ropean Commission (Meaning Project, IST-2001-34460), and by the Spanish Research Department(Hermes Project: TIC2000-0335-C03-02).ReferencesJ.
Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-roll, B. Magnini, P. Vossen 2004.
The MEAN-ING Multilingual Central Repository.
In Pro-ceedings of the Second International WordNetConference.N.
Cristianini and J. Shawe-Taylor 2000.
An Intro-duction to Support Vector Machines.
CambridgeUniversity Press.T.
Joachims 2002.
Learning to Classify Text UsingSupport Vector Machines.
Dissertation, Kluwer.S.
Har-Peled and D. Roth and D. Zimak 2002.
Con-straint Classification for Multiclass Classificationand Ranking.
In Proceedings of the 15th Work-shop on Neural Information Processing Systems.
