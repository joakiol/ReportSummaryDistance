Proceedings of NAACL-HLT 2013, pages 1061?1071,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsTarget Language Adaptation of Discriminative Transfer ParsersOscar T?ckstr?m?SICS | Uppsala UniversitySwedenoscar@sics.seRyan McDonaldGoogleNew Yorkryanmcd@google.comJoakim Nivre?Uppsala UniversitySwedenjoakim.nivre@lingfil.uu.seAbstractWe study multi-source transfer parsing forresource-poor target languages; specificallymethods for target language adaptation ofdelexicalized discriminative graph-based de-pendency parsers.
We first show how recentinsights on selective parameter sharing, basedon typological and language-family features,can be applied to a discriminative parser bycarefully decomposing its model features.
Wethen show how the parser can be relexicalizedand adapted using unlabeled target languagedata and a learning method that can incorporatediverse knowledge sources through ambiguouslabelings.
In the latter scenario, we exploittwo sources of knowledge: arc marginals de-rived from the base parser in a self-trainingalgorithm, and arc predictions from multipletransfer parsers in an ensemble-training algo-rithm.
Our final model outperforms the state ofthe art in multi-source transfer parsing on 15out of 16 evaluated languages.1 IntroductionMany languages still lack access to core NLP tools,such as part-of-speech taggers and syntactic parsers.This is largely due to the reliance on fully supervisedlearning methods, which require large quantities ofmanually annotated training data.
Recently, meth-ods for cross-lingual transfer have appeared as apromising avenue for overcoming this hurdle for bothpart-of-speech tagging (Yarowsky et al 2001; Dasand Petrov, 2011) and syntactic dependency parsing(Hwa et al 2005; Zeman and Resnik, 2008; Ganchevet al 2009; McDonald et al 2011; Naseem et al?Work primarily carried out while at Google, NY.2012).
While these methods do not yet compete withfully supervised approaches, they can drastically out-perform both unsupervised methods (Klein and Man-ning, 2004) and weakly supervised methods (Naseemet al 2010; Berg-Kirkpatrick and Klein, 2010).A promising approach to cross-lingual transferof syntactic dependency parsers is to use multiplesource languages and to tie model parameters acrossrelated languages.
This idea was first explored forweakly supervised learning (Cohen and Smith, 2009;Snyder et al 2009; Berg-Kirkpatrick and Klein,2010) and recently by Naseem et al(2012) for multi-source cross-lingual transfer.
In particular, Naseemet alshowed that by selectively sharing parametersbased on typological features of each language, sub-stantial improvements can be achieved, comparedto using a single set of parameters for all languages.However, these methods all employ generative mod-els with strong independence assumptions and weakfeature representations, which upper bounds their ac-curacy far below that of feature-rich discriminativeparsers (McDonald et al 2005; Nivre, 2008).In this paper, we improve upon the state of the artin cross-lingual transfer of dependency parsers frommultiple source languages by adapting feature-richdiscriminatively trained parsers to a specific targetlanguage.
First, in ?4 we show how selective sharingof model parameters based on typological traits canbe incorporated into a delexicalized discriminativegraph-based parsing model.
This requires a carefuldecomposition of features into language-generic andlanguage-specific sets in order to tie specific targetlanguage parameters to their relevant source languagecounterparts.
The resulting parser outperforms themethod of Naseem et al(2012) on 12 out of 16 eval-uated languages.
Second, in ?5 we introduce a train-1061ing method that can incorporate diverse knowledgesources through ambiguously predicted labelings ofunlabeled target language data.
This permits effectiverelexicalization and target language adaptation of thetransfer parser.
Here, we experiment with two differ-ent knowledge sources: arc sets, which are filtered bymarginal probabilities from the cross-lingual transferparser, are used in an ambiguity-aware self-trainingalgorithm (?5.2); these arc sets are then combinedwith the predictions of a different transfer parser in anambiguity-aware ensemble-training algorithm (?5.3).The resulting parser provides significant improve-ments over a strong baseline parser and achieves a13% relative error reduction on average with respectto the best model of Naseem et al(2012), outper-forming it on 15 out of the 16 evaluated languages.2 Multi-Source Delexicalized TransferThe methods proposed in this paper fall into the delex-icalized transfer approach to multilingual syntacticparsing (Zeman and Resnik, 2008; McDonald et al2011; Cohen et al 2011; S?gaard, 2011).
In contrastto annotation projection approaches (Yarowsky et al2001; Hwa et al 2005; Ganchev et al 2009; Spreyerand Kuhn, 2009), delexicalized transfer methods donot rely on any bitext.
Instead, a parser is trainedon annotations in a source language, relying solelyon features that are available in both the sourceand the target language, such as ?universal?
part-of-speech tags (Zeman and Resnik, 2008; Naseem et al2010; Petrov et al 2012), cross-lingual word clusters(T?ckstr?m et al 2012) or type-level features derivedfrom bilingual dictionaries (Durrett et al 2012).1This parser is then directly used to parse the targetlanguage.
For languages with similar typology, thismethod can be quite accurate, especially when com-pared to purely unsupervised methods.
For instance,a parser trained on English with only part-of-speechfeatures can correctly parse the Greek sentence in Fig-ure 1, even without knowledge of the lexical itemssince the sequence of part-of-speech tags determinesthe syntactic structure almost unambiguously.Learning with multiple languages has been shownto benefit unsupervised learning (Cohen and Smith,1Note that T?ckstr?m et al(2012) and Durrett et al(2012)do require bitext or a bilingual dictionary.
The same holds formost cross-lingual representations, e.g., Klementiev et al(2012).?
????
?????
????
?????
??
??????
.
(The) (John) (gave) (to-the) (Maria) (the) (book) .DET NOUN VERB ADP NOUN DET NOUN PDET NSUBJROOTPREP POBJDOBJDETPUNCFigure 1: A Greek sentence which is correctly parsed by adelexicalized English parser, provided that part-of-speechtags are available in both the source and target language.2009; Snyder et al 2009; Berg-Kirkpatrick andKlein, 2010).
Annotations in multiple languagescan be combined in delexicalized transfer as well, aslong as the parser features are available across the in-volved languages.
This idea was explored by McDon-ald et al(2011), who showed that target languageaccuracy can be improved by simply concatenatingdelexicalized treebanks in multiple languages.
Insimilar work, Cohen et al(2011) proposed a mixturemodel in which the parameters of a generative targetlanguage parser is expressed as a linear interpola-tion of source language parameters, whereas S?gaard(2011) showed that target side language models canbe used to selectively subsample training sentencesto improve accuracy.
Recently, inspired by the phylo-genetic prior of Berg-Kirkpatrick and Klein (2010),S?gaard and Wulff (2012) proposed ?
among otherideas ?
a typologically informed weighting heuristicfor linearly interpolating source language parameters.However, this weighting did not provide significantimprovements over uniform weighting.The aforementioned approaches work well fortransfer between similar languages.
However, theirassumptions cease to hold for typologically divergentlanguages; a target language can rarely be describedas a linear combination of data or model parametersfrom a set of source languages, as languages tendto share varied typological traits; this critical insightis discussed further in ?4.
To account for this issue,Naseem et al(2012) recently introduced a novel gen-erative model of dependency parsing, in which thegenerative process is factored into separate steps forthe selection of dependents and their ordering.
Theparameters used in the selection step are all languageindependent, capturing only head-dependent attach-ment preferences.
In the ordering step, however, pa-rameters are selectively shared between subsets of1062Feature Description81A Order of Subject, Object and Verb85A Order of Adposition and Noun86A Order of Genitive and Noun87A Order of Adjective and Noun88A Order of Demonstrative and Noun89A Order of Numeral and NounTable 1: Typological features from WALS (Dryer andHaspelmath, 2011), proposed for selective sharing byNaseem et al(2012).
Feature 89A has the same value forall studied languages, while 88A differs only for Basque.These features are therefore subsequently excluded.source languages based on typological features ofthe languages extracted from WALS ?
the WorldAtlas of Language Structures (Dryer and Haspelmath,2011) ?
as shown in Table 1.
In the transfer scenario,where no supervision is available in the target lan-guage, this parser achieves the hitherto best publishedresults across a number of languages; in particularfor target languages with a word order divergent fromthe source languages.However, the generative model of Naseem et alisquite impoverished.
In the fully supervised setting,it obtains substantially lower accuracies comparedto a standard arc-factored graph-based parser (Mc-Donald et al 2005).
Averaged across 16 languages,2the generative model trained with full supervision onthe target language obtains an accuracy of 67.1%.
Acomparable lexicalized discriminative arc-factoredmodel (McDonald et al 2005) obtains 84.1%.
Evenwhen delexicalized, this model reaches 78.9%.
Thisgap in supervised accuracy holds for all 16 languages.Thus, while selective sharing is a powerful device fortransferring parsers across languages, the underly-ing generative model used by Naseem et al(2012)restricts its potential performance.3 Basic Models and Experimental SetupInspired by the superiority of discriminative graph-based parsing in the supervised scenario, we inves-tigate whether the insights of Naseem et al(2012)on selective parameter sharing can be incorporatedinto such models in the transfer scenario.
We first re-view the basic graph-based parser framework and the2Based on results in Naseem et al(2012), excluding English.experimental setup that we will use throughout.
Wethen delve into details on how to incorporate selec-tive sharing in this model in ?4.
In ?5, we show howlearning with ambiguous labelings in this parser canbe used for further target language adaptation, boththrough self-training and through ensemble-training.3.1 Discriminative Graph-Based ParserLet x denote an input sentence and let y ?
Y(x)denote a dependency tree, where Y(x) is the set ofwell-formed dependency trees spanning x. Hence-forth, we restrictY(x) to projective dependency trees,but all our methods are equally applicable in the non-projective case.
Provided a vector of model parame-ters ?, the probability of a dependency tree y ?
Y(x),conditioned on a sentence x, has the following form:p?
(y | x) =exp{?>?
(x, y)}?y?
?Y(x) exp {?>?
(x, y?
)}.Without loss of generality, we restrict ourselves tofirst-order models, where the feature function ?
(x, y)factors over individual arcs (h,m) in y, such that?
(x, y) =?(h,m)?y?
(x, h,m) ,where h ?
[0, |x|] and m ?
[1, |x|] are the indicesof the head word and the dependent word of thearc; h = 0 represents a dummy ROOT token.
Themodel parameters are estimated by maximizing thelog-likelihood of the training dataD = {(xi, yi)}ni=1,L(?
;D) =n?i=1log p?
(yi | xi) .We use the standard gradient-based L-BFGS algo-rithm (Liu and Nocedal, 1989) to maximize the log-likelihood.
Eisner?s algorithm (Eisner, 1996) is usedfor inference of the Viterbi parse and arc-marginals.3.2 Data Sets and Experimental SetupTo facilitate comparison with the state of the art, weuse the same treebanks and experimental setup asNaseem et al(2012).
Notably, we use the map-ping proposed by Naseem et al(2010) to map fromfine-grained treebank specific part-of-speech tags tocoarse-grained ?universal?
tags, rather than the morerecent mapping proposed by Petrov et al(2012).
For1063l[l]?
h.p[l]?
m.p[l]?
h.p?
m.pd?
w.81A?
1[h.p = VERB ?
m.p = NOUN]d?
w.81A?
1[h.p = VERB ?
m.p = PRON]d?
w.85A?
1[h.p = ADP ?
m.p = NOUN]d?
w.85A?
1[h.p = ADP ?
m.p = PRON]d?
w.86A?
1[h.p = NOUN ?
m.p = NOUN]d?
w.87A?
1[h.p = NOUN ?
m.p = ADJ]d?
l; [d?
l]?
h.p; [d?
l]?
m.p[d?
l]?
h.p?
m.p[d?
l]?
h.p?
h+1.p?
m?1.p?
m.p[d?
l]?
h?1.p?
h.p?
m?1.p?
m.p[d?
l]?
h.p?
h+1.p?
m.p?
m+1.p[d?
l]?
h?1.p?
h.p?
m.p?
m+1.ph.p?
between.p?
m.pDelexicalized MSTParser Selectively sharedBareFigure 2: Arc-factored feature templates for graph-based parsing.
Direction: d ?
{LEFT, RIGHT}; dependency length:l ?
{1, 2, 3, 4, 5+}; part of speech of head / dependent / words between head and dependent: h.p / m.p / between.p ?
{NOUN, VERB, ADJ, ADV, PRON, DET, ADP, NUM, CONJ, PRT, PUNC, X}; token to the left / right of z: z?1 / z+1; WALSfeatures: w.X for X = 81A, 85A, 86A, 87A (see Table 1).
[?]
denotes an optional template, e.g., [d?
l] ?
h.p?
m.pexpands to templates d?
l?
h.p?
m.p and h.p?
m.p, so that the template also falls back on its undirectional variant.each target language evaluated, the treebanks of theremaining languages are used as labeled training data,while the target language treebank is used for testingonly (in ?5 a different portion of the target languagetreebank is additionally used as unlabeled trainingdata).
We refer the reader to Naseem et al(2012) fordetailed information on the different treebanks.
Dueto divergent treebank annotation guidelines, whichmakes fine-grained evaluation difficult, all resultsare evaluated in terms of unlabeled attachment score(UAS).
In line with Naseem et al(2012), we use goldpart-of-speech tags and evaluate only on sentencesof length 50 or less excluding punctuation.3.3 Baseline ModelsWe compare our models to two multi-source base-line models.
The first baseline, NBG, is the gener-ative model with selective parameter sharing fromNaseem et al(2012).3 This model is trained withouttarget language data, but we investigate the use ofsuch data in ?5.4.
The second baseline, Delex, is adelexicalized projective version of the well-knowngraph-based MSTParser (McDonald et al 2005).The feature templates used by this model are shownto the left in Figure 2.
Note that there is no selectivesharing in this model.The second and third columns of Table 2 show theunlabeled attachment scores of the baseline modelsfor each target language.
We see that Delex performswell on target languages that are related to the major-ity of the source languages.
However, for languages3Model ?D-,To?
in Table 2 from Naseem et al(2012).that diverge from the Indo-European majority family,the selective sharing model, NBG, achieves substan-tially higher accuracies.4 Feature-Based Selective SharingThe results for the baseline models are not surpris-ing considering the feature templates used by Delex.There are two fundamental issues with these fea-tures when used for direct transfer.
First, all butone template include the arc direction.
Second,some features are sensitive to local word order; e.g.,[d?
l]?
h.p?
h+1.p?
m?1.p?
m.p, which modelsdirection as well as word order in the local contextsof the head and the dependent.
Such features do nottransfer well across typologically different languages.In order to verify that these issues are the cause ofthe poor performance of the Delex model, we removeall directional features and all features that modellocal word order from Delex.
The feature templatesof the resulting Bare model are shown in the centerof Figure 2.
These features only model selectionalpreferences and dependency length, analogously tothe selection component of NBG.
The performanceof Bare is shown in the fourth column of Table 2.The removal of most of the features results in a per-formance drop on average.
However, for languagesoutside of the Indo-European family, Bare is oftenmore accurate, especially for Basque, Hungarian andJapanese, which supports our hypothesis.4.1 Sharing Based on Typological FeaturesAfter removing all directional features, we now care-fully reintroduce them.
Inspired by Naseem et al1064Graph-Based ModelsLang.
NBG Delex Bare Share Similar Familyar 57.2 43.3 43.1 52.7 52.7 52.7bg 67.6 64.5 56.1 65.4 62.4 65.4ca 71.9 72.0 58.1 66.1 80.2 77.6cs 43.9 40.5 43.1 42.5 45.3 43.5de 54.0 57.0 49.3 55.2 58.1 59.2el 61.9 63.2 57.7 62.9 59.9 63.2es 62.3 66.9 52.6 59.3 69.0 67.1eu 39.7 29.5 43.3 46.8 46.8 46.8hu 56.9 56.2 60.5 64.5 64.5 64.5it 68.0 70.8 55.7 63.5 74.6 72.5ja 62.3 38.9 50.6 57.1 64.6 65.9nl 56.2 57.9 51.6 55.0 51.8 56.8pt 76.2 77.5 63.0 72.7 78.4 78.4sv 52.0 61.4 55.9 58.8 48.8 63.5tr 59.1 37.4 36.0 41.7 59.5 59.4zh 59.9 45.1 47.9 54.8 54.8 54.8avg 59.3 55.1 51.5 57.4 60.7 62.0Table 2: Unlabeled attachment scores of the multi-sourcetransfer models.
Boldface numbers indicate the best resultper language.
Underlined numbers indicate languageswhose group is not represented in the training data (thesedefault to Share under Similarity and Family).
NBG is the?D-,To?
model in Table 2 from Naseem et al(2012).
(2012), we make use of the typological features fromWALS (Dryer and Haspelmath, 2011), listed in Ta-ble 1, to selectively share directional parameters be-tween languages.
As a natural first attempt at sharingparameters, one might consider forming the cross-product of all features of Delex with all WALS prop-erties, similarly to a common domain adaptation tech-nique (Daum?
III, 2007; Finkel and Manning, 2009).However, this approach has two issues.
First, it re-sults in a huge number of features, making the modelprone to overfitting.
Second, and more critically, itties together languages via features for which theyare not typologically similar.
Consider English andFrench, which are both prepositional and thus havethe same value for WALS property 85A.
These lan-guages will end up sharing a parameter for the feature[d?
l]?h.p = NOUN?m.p = ADJ?w.85A; yet theyhave the exact opposite direction of attachment pref-erence when it comes to nouns and adjectives.
Thisproblem applies to any method for parameter mixingthat treats all the parameters as equal.Like Naseem et al(2012), we instead share pa-rameters more selectively.
Our strategy is to use therelevant part-of-speech tags of the head and depen-dent to select which parameters to share, based onvery basic linguistic knowledge.
The resulting fea-tures are shown to the right in Figure 2.
For example,there is a shared directional feature that models the or-der of Subject, Object and Verb by conjoining WALSfeature 81A with the arc direction and an indicatorfeature that fires only if the head is a verb and the de-pendent is a noun.
These features would not be veryuseful by themselves, so we combine them with theBare features.
The accuracy of the resulting Sharemodel is shown in column five of Table 2.
Althoughthis model still performs worse than NBG, it is animprovement over the Delex baseline and actuallyoutperforms the former on 5 out of the 16 languages.4.2 Sharing Based on Language GroupsWhile Share models selectional preferences and arcdirections for a subset of dependency relations, itdoes not capture the rich local word order informa-tion captured by Delex.
We now consider two ways ofselectively including such information based on lan-guage similarity.
While more complex sharing couldbe explored (Berg-Kirkpatrick and Klein, 2010), weuse a flat structure and consider two simple groupingsof the source and target languages.First, the Similar model consists of the featuresused by Share together with the features from Delexin Figure 2.
The latter are conjoined with an indicatorfeature that fires only when the source and targetlanguages share values for all the WALS features inTable 1.
This is accomplished by adding the templatef?
[w.81A?
w.85A?
w.86A?
w.87A?
w.88A]for each template f in Delex.
This groups: 1) Cata-lan, Italian, Portuguese and Spanish; 2) Bulgarian,Czech and English; 3) Dutch, German and Greek;and 4) Japanese and Turkish.
The remaining lan-guages do not share all WALS properties with atleast one source language and thus revert to Share,since they cannot exploit these grouped features.Second, instead of grouping languages accordingto WALS, the Family model is based on a simplesubdivision into Indo-European languages (Bulgar-ian, Catalan, Czech, Greek, English, Spanish, Italian,1065Dutch, Portuguese, Swedish) and Altaic languages(Japanese, Turkish).
This is accomplished with in-dicator features analogous to those used in Similar.The remaining languages are again treated as isolatesand revert to Similar.The results for these models are given in the lasttwo columns of Table 2.
We see that by adding theserich features back into the fold, but having them fireonly for languages in the same group, we can sig-nificantly increase the performance ?
from 57.4%to 62.0% on average when considering Family.
Ifwe consider our original Delex baseline, we see anabsolute improvement of 6.9% on average and a rela-tive error reduction of 15%.
Particular gains are seenfor non-Indo-European languages; e.g., Japanese in-creases from 38.9% to 65.9%.
Furthermore, Familyachieves a 7% relative error reduction over the NBGbaseline and outperforms it on 12 of the 16 languages.This shows that a discriminative graph-based parsercan achieve higher accuracies compared to generativemodels when the features are carefully constructed.5 Target Language AdaptationWhile some higher-level linguistic properties of thetarget language have been incorporated through se-lective sharing, so far no features specific to the targetlanguage have been employed.
Cohen et al(2011)and Naseem et al(2012) have shown that usingexpectation-maximization (EM) to this end can insome cases bring substantial accuracy gains.
For dis-criminative models, self-training has been shown tobe quite effective for adapting monolingual parsers tonew domains (McClosky et al 2006), as well as forrelexicalizing delexicalized parsers using unlabeledtarget language data (Zeman and Resnik, 2008).
Sim-ilarly T?ckstr?m (2012) used self-training to adapt amulti-source direct transfer named-entity recognizer(T?ckstr?m et al 2012) to different target languages,?relexicalizing?
the model with word cluster features.However, as discussed in ?5.2, standard self-trainingis not optimal for target language adaptation.5.1 Ambiguity-Aware TrainingIn this section, we propose a related training method:ambiguity-aware training.
In this setting a discrimi-native probabilistic model is induced from automat-ically inferred ambiguous labelings over unlabeledtarget language data, in place of gold-standard depen-dency trees.
The ambiguous labelings can combinemultiple sources of evidence to guide the estimationor simply encode the underlying uncertainty from thebase parser.
This uncertainty is marginalized out dur-ing training.
The structure of the output space, e.g.,projectivity and single-headedness constraints, alongwith regularities in the feature space, can togetherguide the estimation, similar to what occurs with theexpectation-maximization algorithm.Core to this method is the idea of an ambiguouslabeling y?
(x) ?
Y(x), which encodes a set of pos-sible dependency trees for an input sentence x. Insubsequent sections we describe how to define suchlabelings.
Critically, y?
(x) should be large enough tocapture the correct labeling, but on the other handsmall enough to provide concrete guidance for modelestimation.
Ideally, y?
(x) will capture heterogenousknowledge that can aid the parser in target languageadaptation.
In a first-order arc-factored model, wedefine y?
(x) in terms of a collection of ambiguousarc setsA(x) = {A(x,m)}|x|m=1, whereA(x,m) de-notes the set of ambiguously specified heads for themth token in x.
Then, y?
(x) is defined as the set ofall projective dependency trees spanning x that canbe assembled from the arcs in A(x).Methods for learning with ambiguous labelingshave previously been proposed in the context ofmulti-class classification (Jin and Ghahramani, 2002),sequence-labeling (Dredze et al 2009), log-linearLFG parsing (Riezler et al 2002), as well as fordiscriminative reranking of generative constituencyparsers (Charniak and Johnson, 2005).
In contrast toDredze et al who allow for weights to be assignedto partial labels, we assume that the ambiguous arcsare weighted uniformly.
For target language adapta-tion, these weights would typically be derived fromunreliable sources and we do not want to train themodel to simply mimic their beliefs.
Furthermore,with this assumption, learning is simply achievedby maximizing the marginal log-likelihood of theambiguous training set D?
= {(xi, y?(xi)}ni=1,L(?
; D?)
=n?i=1log????y?y?(xi)p?
(y | xi)????
?
??
?22 .In maximizing the marginal log-likelihood, the modelis free to distribute probability mass among the trees1066in the ambiguous labeling to its liking, as long as themarginal log-likelihood improves.
The same objec-tive function is used by Riezler et al(2002) and Char-niak and Johnson (2005).
A key difference is that inthese works, the ambiguity is constrained through asupervised signal, while we use ambiguity as a wayto achieve self-training, using the base-parser itself,or some other potentially noisy knowledge source asthe sole constraints.
Note that we have introducedan `2-regularizer, weighted by ?.
This is importantas we are now training lexicalized target languagemodels which can easily overfit.
In all experiments,we optimize parameters with L-BFGS.
Note also thatthe marginal likelihood is non-concave, so that weare only guaranteed to find a local maximum.5.2 Ambiguity-Aware Self-TrainingIn standard self-training ?
hereafter referred to asViterbi self-training ?
a base parser is used to la-bel each unlabeled sentence with its most probableparse tree to create a self-labeled data set, which issubsequently used to train a supervised parser.
Thereare two reasons why this simple approach may work.First, if the base parser?s errors are not too systematicand if the self-training model is not too expressive,self-training can reduce the variance on the new do-main.
Second, self-training allows for features in thenew domain with low support ?
or no support in thecase of lexicalized features ?
in the base parser tobe ?filled in?
by exploiting correlations in the featurerepresentation.
However, a potential pitfall of thisapproach is that the self-trained parser is encouragedto blindly mimic the base parser, which leads to errorreinforcement.
This may be particularly problematicwhen relexicalizing a transfer parser, since the lexicalfeatures provide the parser with increased power andthereby an increased risk of overfitting to the noise.To overcome this potential problem, we propose anambiguity-aware self-training (AAST) method that isable to take the noise of the base parser into account.We use the arc-marginals of the base parser toconstruct the ambiguous labeling y?
(x) for a sentencex.
For each token m ?
[1, |x|], we first sort the set ofarcs in which m is the dependent, {(h,m)}|x|h=0, bythe marginal probabilities of the arcs:p?
(h,m | x) =?
{y?Y(x) | (h,m)?y}p?
(y | x)We next construct the ambiguous arc set A(x,m) byadding arcs (h,m) in order of decreasing probability,until their cumulative probability exceeds ?, i.e.
until?(h,m)?A(x,m)p?
(h,m | x) ?
?
.Lower values of ?
result in more aggressive pruning,with ?
= 0 corresponding to including no arc and?
= 1 corresponding to including all arcs.
We alwaysadd the highest scoring tree y?
to y?
(x) to ensure thatit contains at least one complete projective tree.Figure 3 outlines an example of how (and why)AAST works.
In the Greek example, the genitivephrase ?
pi???????
??????
(the stay of vessels) isincorrectly analyzed as a flat noun phrase.
This is notsurprising given that the base parser simply observesthis phrase as DET NOUN NOUN.
However, lookingat the arc marginals we can see that the correct anal-ysis is available during AAST, although the actualmarginal probabilities are quite misleading.
Further-more, the genitive noun ??????
also appears in otherless ambiguous contexts, where the base parser cor-rectly predicts it to modify a noun and not a verb.This allows the training process to add weight to thecorresponding lexical feature pairing ??????
with anoun head and away from the feature pairing it witha verb.
The resulting parser correctly predicts thegenitive construction.5.3 Ambiguity-Aware Ensemble-TrainingWhile ambiguous labelings can be used as a meansto improve self-training, any information that canbe expressed as hard arc-factored constraints can beincorporated, including linguistic expert knowledgeand annotation projected via bitext.
Here we exploreanother natural source of information: the predic-tions of other transfer parsers.
It is well known thatcombining several diverse predictions in an ensem-ble often leads to improved predictions.
However, inmost ensemble methods there is typically no learninginvolved once the base learners have been trained(Sagae and Lavie, 2006).
An exception is the methodof Sagae and Tsujii (2007), who combine the outputsof many parsers on unlabeled data to train a parserfor a new domain.
However, in that work the learneris not exposed to the underlying ambiguity of thebase parsers; it is only given the Viterbi parse of thecombination system as the gold standard.
In contrast,1067?
pi???????
??????
?pi????pi????
????
??
???
?DET NOUN NOUN VERB ADV DET NOUN0.550.440.620.360.100.87?
pi???????
??????
?pi????pi????
????
??
???
?DET NOUN NOUN VERB ADV DET NOUN?
pi???????
??????
?pi????pi????
????
??
???
?DET NOUN NOUN VERB ADV DET NOUNFigure 3: An example of ambiguity-aware self-training(AAST) on a sentence from the Greek self-training data.The sentence roughly translates to The stay of vesselsis permitted only for the day.
Top: Arcs from the basemodel?s Viterbi parse are shown above the sentence.
Whenonly the part-of-speech tags are observed, the parser tendsto treat everything to the left of the verb as a head-finalnoun phrase.
The dashed arcs below the sentence arethe arcs for the true genitive construction stay of vessels.These arcs and the corresponding incorrect arcs in theViterbi parse are marked with their marginal probabilities.Middle: The ambiguous labeling y?
(x), which is usedas supervision in AAST.
Additional non-Viterbi arcs arepresent in y?
(x); for clarity, these are not shown.
Whenlearning with AAST, probability mass will be pushed to-wards any tree consistent with y?(x).
Marginal probabili-ties are ignored at this stage, so that all arcs in y?
(x) aretreated as equals.
Bottom: The Viterbi parse of the AASTmodel, which has selected the correct arcs from y?
(x).we propose an ambiguity-aware ensemble-training(AAET) method that treats the union of the ensemblepredictions for a sentence x as an ambiguous labelingy?(x).
An additional advantage of this approach isthat the ensemble is compiled into a single modeland therefore does not require multiple models to bestored and used at runtime.It is straightforward to construct y?
(x) from multi-ple parsers.
Let Ak(x,m) be the set of arcs for themth token in x according to the kth parser in the en-semble.
When arc-marginals are used to construct theambiguity set, |Ak(x,m)| ?
1, but when the Viterbi-parse is used, Ak(x,m) is a singleton.
We next formA(x,m) =?kAk(x,m) as the ensemble arc ambi-guity set from which y?
(x) is assembled.
In this study,we combine the arc sets of two base parsers: first, thearc-marginal ambiguity set of the base parser (?5.2);and second, the Viterbi arc set from the NBG parserof Naseem et al(2012) in Table 2.4 Thus, the lat-ter will have singleton arc ambiguity sets, but whencombined with the arc-marginal ambiguity sets ofour base parser, the result will encode uncertaintyderived from both parsers.5.4 Adaptation ExperimentsWe now study the different approaches to target lan-guage adaptation empirically.
As in Naseem et al(2012), we use the CoNLL training sets, stripped ofall dependency information, as the unlabeled targetlanguage data in our experiments.
We use the Familymodel as the base parser, which is used to label theunlabeled target data with the Viterbi parses as wellas with the ambiguous labelings.
The final modelis then trained on this data using standard lexical-ized features (McDonald et al 2005).
Since labeledtraining data is unavailable in the target language,we cannot tune any hyper-parameters and simply set?= 1 and ?= 0.95 throughout.
Although the lattermay suggest that y?
(x) contains a high degree of am-biguity, in reality, the marginal distributions of thebase model have low entropy and after filtering with?
= 0.95, the average number of potential heads perdependent ranges from 1.4 to 3.2, depending on thetarget language.The ambiguity-aware training methods, that isambiguity-aware self-training (AAST) and ambiguity-aware ensemble-training (AAET), are compared tothree baseline systems.
First, NBG+EM is the gen-erative model of Naseem et al(2012) trained withexpectation-maximization on additional unlabeledtarget language text.
Second, Family is the best dis-criminative model from the previous section.
Third,Viterbi is the basic Viterbi self-training model.
Theresults of each of these models are shown in Table 3.There are a number of things that can be observed.First, Viterbi self-training helps slightly on average,but the gains are not consistent and there are evendrops in accuracy for some languages.
Second, AASToutperforms the Viterbi variant on all languages and4We do not have access to the marginals of NBG.1068Target AdaptationLang.
NBG+EM Family Viterbi AAST AAETar 59.3 52.7 52.6 53.5 58.7bg 67.0 65.4 66.4 67.9 73.0ca 71.7 77.6 78.0 79.9 76.1cs 44.3 43.5 43.6 44.4 48.3de 54.1 59.2 59.7 62.5 61.5el 67.9 63.2 64.5 65.5 69.6es 62.0 67.1 68.2 68.5 66.9eu 47.8 46.8 47.5 48.6 49.4hu 58.6 64.5 64.6 65.6 67.5it 65.6 72.5 71.6 72.4 73.4ja 64.1 65.9 65.7 68.8 72.0nl 56.6 56.8 57.9 58.1 60.2pt 75.8 78.4 79.9 80.7 79.9sv 61.7 63.5 63.4 65.5 65.5tr 59.4 59.4 59.5 64.1 64.2zh 51.0 54.8 54.8 57.9 60.7avg 60.4 62.0 62.4 64.0 65.4Table 3: Target language adaptation using unlabeled tar-get data.
AAST: ambiguity-aware self-training.
AAET:ambiguity-aware ensemble-training.
Boldface numbersindicate the best result per language.
Underlined numbersindicate the best result, excluding AAET.
NBG+EM is the?D+,To?
model from Naseem et al(2012).nearly always improves on the base parser, althoughit sees a slight drop for Italian.
AAST improves theaccuracy over the base model by 2% absolute on av-erage and by as much as 5% absolute for Turkish.Comparing this model to the NBG+EM baseline, weobserve an improvement by 3.6% absolute, outper-forming it on 14 of the 16 languages.
Furthermore,ambiguity-aware self-training appears to help morethan expectation-maximization for generative (unlex-icalized) models.
Naseem et alobserved an increasefrom 59.3% to 60.4% on average by adding unlabeledtarget language data and the gains were not consistentacross languages.
AAST, on the other hand, achievesconsistent gains, rising from 62.0% to 64.0% on av-erage.
Third, as shown in the rightmost column ofTable 3, ambiguity-aware ensemble-training is indeeda successful strategy; AAET outperforms the previ-ous best self-trained model on 13 and NB&G+EMon 15 out of 16 languages.
The relative error reduc-tion with respect to the base Family model is 9% onaverage, while the average reduction with respect toNBG+EM is 13%.Before concluding, two additional points are worthmaking.
First, further gains may potentially beachievable with feature-rich discriminative models.While the best generative transfer model of Naseemet al(2012) approaches its upper-bounding super-vised accuracy (60.4% vs. 67.1%), our relaxed self-training model is still far below its supervised coun-terpart (64.0% vs. 84.1%).
One promising statisticalong these lines is that the oracle accuracy for theambiguous labelings of AAST is 75.7%, averagedacross languages, which suggests that other trainingalgorithms, priors or constraints could improve theaccuracy substantially.
Second, relexicalization is akey component of self-training.
If we use delexical-ized features during self-training, we only observe asmall average improvement from 62.0% to 62.1%.6 ConclusionsWe contributed to the understanding of multi-sourcesyntactic transfer in several complementary ways.First, we showed how selective parameter sharing,based on typological features and language familymembership, can be incorporated in a discriminativegraph-based model of dependency parsing.
We thenshowed how ambiguous labelings can be used to in-tegrate heterogenous knowledge sources in parsertraining.
Two instantiations of this framework wereexplored.
First, an ambiguity-aware self-trainingmethod that can be used to effectively relexicalizeand adapt a delexicalized transfer parser using unla-beled target language data.
Second, an ambiguity-aware ensemble-training method, in which predic-tions from different parsers can be incorporated andfurther adapted.
On average, our best model providesa relative error reduction of 13% over the state-of-the-art model of Naseem et al(2012), outperformingit on 15 out of 16 evaluated languages.Acknowledgments We thank Alexander Rush forhelp with the hypergraph framework used for inference.Tahira Naseem kindly provided us with her data sets andthe predictions of her systems.
This work benefited frommany discussions with Yoav Goldberg and members of theGoogle parsing team.
We finally thank the three anony-mous reviewers for their valuable feedback.
The work ofthe first author was partly funded by the Swedish NationalGraduate School of Language Technology (GSLT).1069ReferencesTaylor Berg-Kirkpatrick and Dan Klein.
2010.
Phyloge-netic grammar induction.
In Proceedings of ACL.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.In Proceedings of ACL.Shay B. Cohen and Noah A. Smith.
2009.
Shared lo-gistic normal distributions for soft parameter tying inunsupervised grammar induction.
In Proceedings ofNAACL.Shay B. Cohen, Dipanjan Das, and Noah A. Smith.
2011.Unsupervised structure prediction with non-parallelmultilingual guidance.
In Proceedings of EMNLP.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proceedings of ACL-HLT.Hal Daum?
III.
2007.
Frustratingly easy domain adapta-tion.
In Proceedings of ACL.Mark Dredze, Partha Pratim Talukdar, and Koby Cram-mer.
2009.
Sequence learning from data with multiplelabels.
In Proceedings of the ECML/PKDD Workshopon Learning from Multi-Label Data.Matthew S. Dryer and Martin Haspelmath, editors.
2011.The World Atlas of Language Structures Online.
Mu-nich: Max Planck Digital Library.
http://wals.info/.Greg Durrett, Adam Pauls, and Dan Klein.
2012.
Syntac-tic transfer using a bilingual lexicon.
In Proceedings ofEMNLP-CoNLL.Jason Eisner.
1996.
Three new probabilistic models fordependency parsing: an exploration.
In Proceedings ofCOLING.Jenny Rose Finkel and Christopher D. Manning.
2009.Hierarchical Bayesian domain adaptation.
In Proceed-ings of HLT-NAACL.Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.2009.
Dependency grammar induction via bitext pro-jection constraints.
In Proceedings of ACL-IJCNLP.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Natural Language Engineering, 11(03):311?325.Rong Jin and Zoubin Ghahramani.
2002.
Learning withmultiple labels.
In Proceedings of NIPS.Dan Klein and Chris D. Manning.
2004.
Corpus-basedinduction of syntactic structure: models of dependencyand constituency.
In Proceedings of ACL.Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.2012.
Inducing crosslingual distributed representationsof words.
In Proceedings of COLING.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming, 45.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Reranking and self-training for parser adaptation.In Proceedings of ACL.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of ACL.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.Multi-source transfer of delexicalized dependencyparsers.
In Proceedings of EMNLP.Tahira Naseem, Harr Chen, Regina Barzilay, and MarkJohnson.
2010.
Using universal linguistic knowl-edge to guide grammar induction.
In Proceedings ofEMNLP.Tahira Naseem, Regina Barzilay, and Amir Globerson.2012.
Selective sharing for multilingual dependencyparsing.
In Proceedings of ACL.Joakim Nivre.
2008.
Algorithms for deterministic incre-mental dependency parsing.
Computational Linguis-tics, 34(4):513?553.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In Proceedings ofLREC.Stefan Riezler, Tracy H. King, Ronald M. Kaplan, RichardCrouch, John T. Maxwell, III, and Mark Johnson.2002.
Parsing the Wall Street Journal using a lexical-functional grammar and discriminative estimation tech-niques.
In Proceedings of ACL.Kenji Sagae and Alon Lavie.
2006.
Parser combinationby reparsing.
In Proceedings of NAACL.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependency pars-ing and domain adaptation with LR models and parserensembles.
In Proceedings of EMNLP-CoNLL.Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, andRegina Barzilay.
2009.
Adding more languages im-proves unsupervised multilingual part-of-speech tag-ging: A Bayesian non-parametric approach.
In Pro-ceedings of NAACL.Anders S?gaard and Julie Wulff.
2012.
An empiricalstudy of non-lexical extensions to delexicalized transfer.In Proceedings of COLING.Anders S?gaard.
2011.
Data point selection for cross-language adaptation of dependency parsers.
In Pro-ceedings of ACL.Kathrin Spreyer and Jonas Kuhn.
2009.
Data-drivendependency parsing of new languages using incompleteand noisy training data.
In Proceedings of CONLL.Oscar T?ckstr?m, Ryan McDonald, and Jakob Uszkoreit.2012.
Cross-lingual word clusters for direct transfer oflinguistic structure.
In Proceedings of NAACL-HLT.Oscar T?ckstr?m.
2012.
Nudging the envelope of directtransfer methods for multilingual named entity recog-nition.
In Proceedings of the NAACL-HLT 2012 Work-shop on Inducing Linguistic Structure (WILS 2012).1070David Yarowsky, Grace Ngai, and Richard Wicentowski.2001.
Inducing multilingual text analysis tools via ro-bust projection across aligned corpora.
In Proceedingsof HLT.Daniel Zeman and Philip Resnik.
2008.
Cross-languageparser adaptation between related languages.
In IJC-NLP Workshop: NLP for Less Privileged Languages.1071
