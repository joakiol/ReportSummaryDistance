Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 202?206, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsDuluth : Word Sense Induction Applied to Web Page ClusteringTed PedersenDepartment of Computer ScienceUniversity of MinnesotaDuluth, MN 55812 USAtpederse@d.umn.eduhttp://senseclusters.sourceforge.netAbstractThe Duluth systems that participated in task11 of SemEval?2013 carried out word senseinduction (WSI) in order to cluster Web searchresults.
They relied on an approach that repre-sented Web snippets using second?order co?occurrences.
These systems were all imple-mented using SenseClusters, a freely availableopen source software package.1 IntroductionThe goal of task 11 of SemEval?2013 was to clus-ter Web search results (Navigli and Vannella, 2013).The test data consisted of the top 64 Google resultsfor each of 100 potentially ambiguous queries, for atotal of 6,400 test instances.
The Web snippets re-turned for each query were clustered and evaluatedseparately, with an overall evaluation score providedfor each system.The problem of Web page clustering is one ofthe use cases envisioned for SenseClusters (Peder-sen and Kulkarni, 2007; Pedersen, 2010a), a freelyavailable open source software package developedat the University of Minnesota, Duluth starting in2002.
It supports first and second?order clusteringof contexts using both co?occurrence matrices (Pu-randare and Pedersen, 2004; Kulkarni and Pedersen,2005) and Latent Semantic Analysis (Landauer andDumais, 1997).SenseClusters has participated in various forms atdifferent SenseEval and SemEval shared tasks, in-cluding SemEval-2007 (Pedersen, 2007), SemEval-2010 (Pedersen, 2010b) and also in an i2b2 clinicalmedicine task (Pedersen, 2006).2 Duluth SystemWhile we refer to three Duluth systems (sys1, sys7,and sys9), in reality these are all variations of thesame overall system.
All three are based on second?order context clustering as provided in SenseClus-ters.
The query terms are treated exactly like anyother word in the snippets, which is called headlessclustering in SenseClusters.2.1 Common aspects to all systemsThe input to sys1, sys7, and sys9 consists of 64Web search snippets, each approximately 25 wordsin length.
All text was converted to upper case priorto processing.
The goal was to group the 64 snip-pets for each query into k distinct clusters, where kwas automatically determined by the PK2 method ofSenseClusters (Pedersen and Kulkarni, 2006a; Ped-ersen and Kulkarni, 2006b).
Each discovered clus-ter represents a different underlying meaning of thegiven query term that resulted in those snippets be-ing returned.
Word sense induction was carried outseparately on the Web snippets associated with eachquery term, meaning that the algorithm was run 100times and clustered 64 Web page snippets each time.In second?order context clustering, the words ina context (i.e., Web snippet) to be clustered are re-placed by vectors that are derived from some cor-pus of text.
The corpora used are among the maindifferences in the Duluth systems.
Once the wordsin a context are replaced by vectors, those vectorsare averaged together to create a new representa-tion of the context.
That representation is said tobe second?order because each word is representedby its direct or first order co?occurrences, and simi-202larities between words in the same Web snippet arecaptured by the set of words that mutually co?occurwith them.If car is represented by the vector [motor, mag-azine, insurance], and if life is represented by thevector [sentence, force, insurance], then car and lifeare said to be second?order co-occurrences becausethey both occur with insurance.
A second?order co-occurrence can capture more indirect relationshipsbetween words, and so these second?order connec-tions tend to be more numerous and more subtlethan first?order co?occurrences (which would re-quire that car and life co?occur near or adjacent toeach other in a Web snippet to establish a relation-ship).The co-occurrence matrix is created by finding bi-grams that occur more than a given number of times(this varies per system) and have a log-likelihood ra-tio greater than 3.84.1 Then, the first word in a bi-gram is represented in the rows of the matrix, thesecond word is represented in the columns.
Thevalue in the corresponding cell is the log-likelihoodscore.
This matrix is therefore not symmetric, andhas different entries for old age and age old.
Also,any bigram that includes one or two stop words (e.g.,to fire, running to, for the) will be excluded and notincluded in the co-occurrence matrix and will not beincluded in the overall sample count used for com-puting the log?likelihood ratio.
To summarize then,words in a Web snippet are represented by the wordswith which they occur in bigrams, where the contextword is the first word in the bigram, and the vectoris the set of words that follow it in bigrams.Once the co?occurrence matrix is created, it maybe optionally reduced by Singular Value Decompo-sition.
The result of this will be a matrix with thesame number of rows prior to SVD, but a reducednumber of columns.
The goal of SVD is to com-press together columns of words with similar co?occurrence patterns, and thereby reduce the size andnoisiness of the data.
Whether the matrix is reducedor not, then each word in each snippet to be clusteredis replaced by a vector from that matrix.
A word is1This value corresponds with a p-value of 0.05 when testingfor significance, meaning that bigrams with log-likelihood atleast equal to 3.84 have at least a 95% chance of having beendrawn from a population where their co-occurrence is not bychance.replaced by the row in the co-occurrence matrix towhich it corresponds.
Any words that do not havean entry in the co-occurrence matrix will not be rep-resented.
Then, the contexts are clustered using themethod of repeated bisections (Zhao and Karypis,2004), where the number of clusters is automaticallydiscovered using the PK2 method.2.2 Differences among systemsThe main difference among the systems was the cor-pora used to create their co-occurrence matrices.The smallest corpus was used by sys7, which sim-ply treated the 64 snippets returned by each queryas the corpus for creating a co?occurrence matrix.Thus, each query term had a unique co-occurrencematrix that was created from the Web snippets re-turned by that query.
This results in a very smallamount of data per query (approx.
25 words/snip-pet * 64 snippets = 1600 words), and so bigramswere allowed to have up to three intervening wordsthat were skipped (in order to increase the numberof bigrams used to create the co?occurrence ma-trix).
Bigrams were excluded if they only occurred 1time, had a log?likelihood ratio of less than 3.84, orwere made up of one or two stop words.
Even withthis more flexible definition of bigram, the resultingco?occurrence matrices were still quite small.
Thelargest resulting co?occurrence matrix for any querywas 221 x 222, with 602 non?zero values (meaningthere were 602 different bigrams used as features).The smallest of the co-occurrence matrices was 102x 113 with 242 non?zero values.
Given these smallsizes, SVD was not employed in sys7.sys1 and sys9 used larger corpora, and thereforerequired bigrams to be made up of adjacent wordsthat occurred 5 or more times, had log?likelihoodratio scores of 3.84 or above, and contained no stopwords.
Rather than having a different co?occurrencematrix for each query, sys1 and sys9 created a singleco-occurrence matrix for all queries.In sys1, all the Web snippet results for all 100queries were combined into a single corpus.
Thus,the co?occurrence matrix was based on bigram fea-tures found in a corpus of 6,400 Web snippets thatconsisted of approximately 160,000 words.
Thisresulted in a co?occurrence matrix of size 771 x952 with 1,558 non?zero values prior to SVD.
AfterSVD the matrix was 771 x 90, and all cells had non-203zero values (as a result of SVD).
Note that if thereare less than 3,000 columns in a co-occurrence ma-trix, the columns are reduced down to 10% of theiroriginal size.
If there are more than 3,000 columnsthen it is reduced to 300 dimensions.
This followsrecommendations for SVD given for Latent Seman-tic Analysis (Landauer and Dumais, 1997).Rather than using task data, sys9 uses the first10,000 paragraphs of Associated Press newswire(APW) that appear in the English Gigaword corpus(1st edition) (Graff and Cieri, 2003).
This createda corpus of approximately 3.6 million words whichresulted in a co-occurrence matrix prior to SVD of9,853 x 10,995 with 43,199 non-zero values.
AfterSVD the co?occurrence matrix was 9,853 by 300.3 ResultsVarious measures were reported by the task organiz-ers, including F1 (F1-13), the Rand Index (RI), theAdjusted Rand Index (ARI), and the Jaccard Coeffi-cient.
More details can be found in (Di Marco andNavigli, 2013).In addition we computed the paired F-Score (F-10) (Artiles et al 2009) as used in the 2010 Se-mEval word sense induction task (Manandhar et al2010) and the F-Measure (F-SC), which is providedby SenseClusters.
This allows for the comparison ofresults from this task with the 2010 task and variousresults from SenseClusters.The organizers also provided scores for S-recalland S-precision (Zhai et al 2003), however forthese to be meaningful the results for each clustermust be output in ranked order.
The Duluth sys-tems did not make a ranking distinction among theinstances in each cluster, and so these scores are notparticularly meaningful for the Duluth systems.3.1 Comparisons to BaselinesTable 1 includes the results of the three submittedDuluth systems, plus numerous baselines.
RandXdesignates a random baseline where senses were as-signed by randomly assigning a value between 1 andX.
In word sense induction, the labels assigned todiscovered clusters are arbitrary, so a random base-line is a convenient sanity check.
MFS replicates themost frequent sense baseline from supervised learn-ing by simply assigning all instances for a word toa single cluster.
This is sometimes also known asthe ?all?in?one?
baseline.
Gold are the evaluationresults when the gold standard data is provided asinput (and compared to itself).The various baselines give us a sense of the char-acteristics of the different evaluation measures, anda few points emerge.
We have argued previously(Pedersen, 2010a) that any evaluation measure usedfor word sense induction needs to be able to ex-pose random baselines and distinguish them frommore systematic results.
By this standard a numberof measures are found to be lacking.
In SemEval?2010 we demonstrated that the V-Measure (Rosen-berg and Hirschberg, 2007) had an overwhelmingbias towards systems that produce larger numbers ofclusters ?
as a result it scored random baselines thatgenerated larger number of clusters (like Rand25and Rand50) very highly.The Rand Index (RI), which does not correctfor chance agreement, also scores random baselineshigher than both non?random systems and MFS.The Adjusted Rand Index (ARI) corrects for chanceand scores random systems near 0, but it also scoresMFS near 0.
According to ARI, random systemsand MFS perform at essentially the same level.
Thisis a troublesome tendency when evaluating wordsense induction systems, since MFS is often consid-ered a reasonable baseline that provides useful re-sults.
Many words have relatively skewed distribu-tions where they are mostly used in one sense, andthis is exactly what is approximated by MFS.Of the measures included in Table 1, the pairedFScore (F-10), the F-Measure (F-SC), and the Jac-card Coefficient provide results that seem most ap-propriate for word sense induction.
This is becausethese measures score random baselines lower thanMFS, and that RandX scores lower than RandY,when (X > Y).
The paired FScore (F-10) and theJaccard Coefficient arrived at similar results, whereRand50 received an extremely low score, and MFSscored the highest.
The F-measure (F-SC) had asimilar profile, except that the decline in evaluationscores as X grows in RandX is somewhat less.The paired F-Score (F-10), the F-Measure (F-SC),and F1 (F1-13) all score MFS at approximately 54%,which is intuitively appealing since that is the per-centage of instances correctly clustered if all in-stances are placed into a single cluster.
However, in204Table 1: Experimental ResultsSystem F-10 F-SC Jaccard F1-13 RI ARI clusters sizesys1 46.53 46.90 31.79 56.83 52.18 5.75 2.5 26.5sys7 45.89 44.03 31.03 58.78 52.04 6.78 3.0 25.2sys9 35.56 37.21 22.24 57.02 54.63 2.59 3.3 19.8Rand2 41.49 42.86 26.99 54.89 50.06 -0.04 2.0 32.0Rand5 25.17 31.28 14.52 56.73 56.13 0.12 5.0 12.8Rand10 15.05 28.71 8.18 59.67 58.10 0.02 10.0 6.4Rand25 7.01 26.78 3.63 66.89 59.24 -0.15 23.2 2.8Rand50 4.07 25.97 2.00 76.19 59.73 0.10 35.9 1.8MFS 54.06 54.42 39.90 54.42 39.90 0.0 1.0 64.0Gold 100.00 100.00 100.00 100.00 100.00 99.0 7.7 11.6other cases these measures begin to diverge.
F1 (F1-13) tends to score random baselines even higher thanMFS, and Rand50 gets a higher score than Rand2,which is somewhat counter intuitive.
In fact accord-ing to F1 (F1-13), Rand50 would have been the topranked system in task 11.
It appears that F1 (F1-13) is strongly influenced by cluster purity, but doesnot penalize a system for creating too many clusters.Thus, as the number of clusters increases, F1 (F1-13) will consistently improve since smaller clustersare nearly always more pure than larger ones.Interestingly enough, the Rand Index (RI) and theJaccard Coefficient both score MFS at 39%.
Thisnumber does not have an intuitively appealing inter-pretation, and thereafter RI and Jaccard diverge.
RIscores random baselines higher than MFS, whereasthe Jaccard Coefficient takes the more reasonablepath of scoring random baselines well below MFS.3.2 Duluth Systems EvaluationThe FScore (F-10), F-Measure (F-SC), and JaccardCoefficient result in a comparable and consistentview of the system results.
sys1 was found to be themost accurate, followed closely by sys7.
All threemeasures showed that sys9 lagged considerably.While all three systems relied on second?orderco-occurrences, sys7 used the least amount of data,while sys9 used the most.
This shows that better re-sults can be obtained using the Web snippets to beclustered as the source of the co?occurrence data (assys1 and sys7 did) rather than larger amounts of pos-sibly less relevant text (as sys9 did).Each of these systems created a roughly compara-ble number of clusters (on average, per query term,shown in the column labeled clusters).
sys7 created2.53, while sys9 created 3.01, and sys1 found 3.32.The average number of web snippets in the discov-ered clusters (shown in the column labeled size) arelikewise somewhat consistent: sys1 was the largestat 26.5, sys7 had 25.2, and sys9 was the smallestwith 19.8.
The gold standard found an average of7.7 queries per cluster and 11.6 snippets per cluster.After the competition sys1 and sys9 were runwithout SVD.
There was no significant difference inresults with or without SVD.
This is consistent withprevious work that found SVD had relatively littleimpact in name discrimination experiments (Peder-sen et al 2005).4 Conclusionssys7 achieved the best results by using very smallco-occurrence matrices of approximately one to twohundred rows and columns.
While small, this datawas most relevant to the task since it was made up ofthe Web snippets to be clustered.
sys1 increased thesize of the co?occurrence matrix to 771 x 96 by us-ing all of the test data, but saw no increase in perfor-mance.
sys9 used the largest corpus, which resultedin a co?occurrence matrix of 9,853 x 300, and hadthe poorest results of the Duluth systems.Sixty?four instances is a small amount of data forclustering.
In future we will augment each querywith additional unannotated web snippets that willbe discarded after clustering.
Hopefully the core 64instances that remain will be clustered more effec-tively given the cushion provided by the extra data.205ReferencesJ.
Artiles, E.
Amigo?, and J. Gonzalo.
2009.
The role ofnamed entities in Web People Search.
In Proceedingsof the 2009 Conference on Empirical Methods in Nat-ural Language Processing, pages 534?542, Singapore,August.A.
Di Marco and R. Navigli.
2013.
Clustering and di-versifying web search results with graph-based wordsense induction.
Computational Linguistics, 39(4):1?46.D.
Graff and C. Cieri.
2003.
English Gigaword.
Lin-guistic Data Consortium, Philadelphia.A.
Kulkarni and T Pedersen.
2005.
SenseClusters: Un-supervised discrimination and labeling of similar con-texts.
In Proceedings of the Demonstration and Inter-active Poster Session of the 43rd Annual Meeting ofthe Association for Computational Linguistics, pages105?108, Ann Arbor, MI, June.T.
Landauer and S. Dumais.
1997.
A solution to Plato?sproblem: The Latent Semantic Analysis theory of ac-quisition, induction and representation of knowledge.Psychological Review, 104:211?240.S.
Manandhar, I. Klapaftis, D. Dligach, and S. Pradhan.2010.
SemEval-2010 Task 14: Word sense inductionand disambiguation.
In Proceedings of the SemEval2010 Workshop : the 5th International Workshop onSemantic Evaluations, Uppsala, Sweden, July.R.
Navigli and D. Vannella.
2013.
Semeval-2013 task11: Evaluating word sense induction and disambigua-tion within an end-user application.
In Proceedingsof the 7th International Workshop on Semantic Eval-uation (SemEval 2013), in conjunction with the Sec-ond Joint Conference on Lexical and ComputationalSemantics (*SEM-2013), Atlanta, June.T.
Pedersen and A. Kulkarni.
2006a.
Automatic clus-ter stopping with criterion functions and the gap statis-tic.
In Proceedings of the Demonstration Session ofthe Human Language Technology Conference and theSixth Annual Meeting of the North American Chap-ter of the Association for Computational Linguistics,pages 276?279, New York City, June.T.
Pedersen and A. Kulkarni.
2006b.
Selecting theright number of senses based on clustering criterionfunctions.
In Proceedings of the Posters and DemoProgram of the Eleventh Conference of the EuropeanChapter of the Association for Computational Linguis-tics, pages 111?114, Trento, Italy, April.T.
Pedersen and A. Kulkarni.
2007.
Discovering identi-ties in web contexts with unsupervised clustering.
InProceedings of the IJCAI-2007 Workshop on Analyticsfor Noisy Unstructured Text Data, pages 23?30, Hy-derabad, India, January.T.
Pedersen, A. Purandare, and A. Kulkarni.
2005.
Namediscrimination by clustering similar contexts.
In Pro-ceedings of the Sixth International Conference on In-telligent Text Processing and Computational Linguis-tics, pages 220?231, Mexico City, February.T.
Pedersen.
2006.
Determining smoker status using su-pervised and unsupervised learning with lexical fea-tures.
In Working Notes of the i2b2 Workshop on Chal-lenges in Natural Language Processing for ClinicalData, Washington, DC, November.T.
Pedersen.
2007.
UMND2 : SenseClusters appliedto the sense induction task of Senseval-4.
In Proceed-ings of the Fourth International Workshop on SemanticEvaluations (SemEval-2007), pages 394?397, Prague,Czech Republic, June.T.
Pedersen.
2010a.
Computational approaches to mea-suring the similarity of short contexts.
Technical re-port, University of Minnesota Supercomputing Insti-tute Research Report UMSI 2010/118, October.T.
Pedersen.
2010b.
Duluth-WSI: SenseClusters appliedto the sense induction task of semEval-2.
In Proceed-ings of the SemEval 2010 Workshop : the 5th Interna-tional Workshop on Semantic Evaluations, pages 363?366, Uppsala, July.A.
Purandare and T. Pedersen.
2004.
Word sensediscrimination by clustering contexts in vector andsimilarity spaces.
In Proceedings of the Conferenceon Computational Natural Language Learning, pages41?48, Boston, MA.A.
Rosenberg and J. Hirschberg.
2007.
V-measure: Aconditional entropy-based external cluster evaluationmeasure.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 410?420, Prague, Czech Republic, June.C.
X. Zhai, W. Cohen, and J. Lafferty.
2003.
Be-yond independent relevance: methods and evaluationmetrics for subtopic retrieval.
In Proceedings of the26th Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 10?17.
ACM.Y.
Zhao and G. Karypis.
2004.
Empirical and theoreticalcomparisons of selected criterion functions for docu-ment clustering.
Machine Learning, 55:311?331.206
