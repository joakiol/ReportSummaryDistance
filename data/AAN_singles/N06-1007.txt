Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 49?56,New York, June 2006. c?2006 Association for Computational LinguisticsAcquisition of Verb Entailment from TextViktor PekarComputational Linguistics GroupUniversity of WolverhamptonMB109 Stafford StreetWolverhampton WV1 1SB, UKv.pekar@wlv.ac.ukAbstractThe study addresses the problem of auto-matic acquisition of entailment relationsbetween verbs.
While this task has muchin common with paraphrases acquisitionwhich aims to discover semantic equiva-lence between verbs, the main challengeof entailment acquisition is to captureasymmetric, or directional, relations.
Mo-tivated by the intuition that it often under-lies the local structure of coherent text, wedevelop a method that discovers verb en-tailment using evidence about discourserelations between clauses available in aparsed corpus.
In comparison with earlierwork, the proposed method covers a muchwider range of verb entailment types andlearns the mapping between verbs withhighly varied argument structures.1 IntroductionThe entailment relations between verbs are a naturallanguage counterpart of the commonsense knowl-edge that certain events and states give rise to otherevents and states.
For example, there is an entail-ment relation between the verbs buy and belong,which reflects the commonsense notion that if some-one has bought an object, this object belongs to thatperson.A lexical resource encoding entailment can serveas a useful tool in many tasks where automatic in-ferencing over natural language text is required.
InQuestion Answering, it has been used to establishthat a certain sentence found in the corpus can serveas a suitable, albeit implicit answer to a query (Cur-tis et al, 2005), (Girju, 2003), (Moldovan and Rus,2001).
In Information Extraction, it can similarlyhelp to recognize relations between named entitiesin cases when the entities in the text are linked bya linguistic construction that entails a known extrac-tion pattern, but not by the pattern itself.
A lexicalentailment resource can contribute to information re-trieval tasks via integration into a textual entailmentsystem that aims to recognize entailment betweentwo larger text fragments (Dagan et al, 2005).Since entailment is known to systematically inter-act with the discourse organization of text (Hobbs,1985), an entailment resource can be of interest totasks that deal with structuring a set of individualfacts into coherent text.
In Natural Language Gener-ation (Reiter and Dale, 2000) and Multi-DocumentSummarization (Barzilay et al, 2002) it can be usedto order sentences coming from multiple, possiblyunrelated sources to produce a coherent document.The knowledge is essential for compiling answersfor procedural questions in a QA system, when sen-tences containing relevant information are spreadacross the corpus (Curtis et al, 2005).The present paper is concerned with the prob-lem of automatic acquisition of verb entailment fromtext.
In the next section we set the backgroundfor the study by describing previous work.
Wethen define the goal of the study and describe ourmethod for verb entailment acquisition.
After thatwe present results of its experimental evaluation.
Fi-nally, we draw conclusions and outline future work.2 Previous WorkThe task of verb entailment acquisition appears tohave much in common with that of paraphrase ac-quisition (Lin and Pantel, 2001), (Pang et al, 2003),(Szpektor et al, 2004).
In both tasks the goal isto discover pairs of related verbs and identify map-49pings between their argument structures.
The impor-tant distinction is that while in a paraphrase the twoverbs are semantically equivalent, entailment is a di-rectional, or asymmetric, relation: one verb entailsthe other, but the converse does not hold.
For ex-ample, the verbs buy and purchase paraphrase eachother: either of them can substitute its counterpart inmost contexts without altering their meaning.
Theverb buy entails own so that buy can be replaced withown without introducing any contradicting contentinto the original sentence.
Replacing own with buy,however, does convey new meaning.To account for the asymmetric character of entail-ment, a popular approach has been to use lexico-syntactic patterns indicative of entailment.
In(Chklovski and Pantel, 2004) different types of se-mantic relations between verbs are discovered us-ing surface patterns (like ?X-ed by Y-ing?
for en-ablement1, which would match ?obtained by bor-rowing?, for example) and assessing the strengthof asymmetric relations as mutual information be-tween the two verbs.
(Torisawa, 2003) collectedpairs of coordinated verbs, i.e.
matching patternslike ?X-ed and Y-ed?, and then estimated the prob-ability of entailment using corpus counts.
(Inuiet al, 2003) used a similar approach exploitingcausative expressions such as because, though, andso.
(Girju, 2003) extracted causal relations betweennouns like ?Earthquakes generate tsunami?
by firstusing lexico-syntactic patterns to collect relevantdata and then using a decision tree classifier to learnthe relations.
Although these techniques have beenshown to achieve high precision, their reliance onsurface patterns limits their coverage in that they ad-dress only those relations that are regularly madeexplicit through concrete natural language expres-sions, and only within sentences.The method for noun entailment acquisition by(Geffet and Dagan, 2005) is based on the idea of dis-tributional inclusion, according to which one nounis entailed by the other if the set of occurrence con-texts of the former subsumes that of the latter.
How-ever, this approach is likely to pick only a particularkind of verb entailment, that of troponymy (such as1In (Chklovski and Pantel, 2004) enablement is defined tobe a relation where one event often, but not necessarily always,gives rise to the other event, which coincides with our definitionof entailment (see Section 3).march-walk) and overlook pairs where there is littleoverlap in the occurrence patterns between the twoverbs.In tasks involving recognition of relations be-tween entities such as Question Answering and In-formation Extraction, it is crucial to encode themapping between the argument structures of twoverbs.
Pattern-matching often imposes restrictionson the syntactic configurations in which the verbscan appear in the corpus: the patterns employed by(Chklovski and Pantel, 2004) and (Torisawa, 2003)derive pairs of only those verbs that have identicalargument structures, and often only those that in-volve a subject and a direct object.
The methodfor discovery of inference rules by (Lin and Pantel,2001) obtains pairs of verbs with highly varied argu-ment structures, which also do not have to be iden-tical for the two verbs.
While the inference rulesthe method acquires seem to encompass pairs re-lated by entailment, these pairs are not distinguishedfrom paraphrases and the direction of relation insuch pairs is not recognized.To sum up, a major challenge in entailment ac-quisition is the need for more generic methods thatwould cover an unrestricted range of entailmenttypes and learn the mapping between verbs withvaried argument structures, eventually yielding re-sources suitable for robust large-scale applications.3 Verb EntailmentVerb entailment relations have been traditionally at-tracting a lot of interest from lexical semantics re-search and their various typologies have been pro-posed (see, e.g., (Fellbaum, 1998)).
In this study,with the view of potential practical applications, weadopt an operational definition of entailment.
Wedefine it to be a semantic relation between verbswhere one verb, termed premise P , refers to eventEp and at the same time implies event Eq, typicallydenoted by the other verb, termed consequence Q.The goal of verb entailment acquisition is thento find two linguistic templates each consisting ofa verb and slots for its syntactic arguments.
In thepair, (1) the verbs are related in accordance withour definition of entailment above, (2) there is amapping between the slots of the two templates and(3) the direction of entailment is indicated explic-50itly.
For example, in the template pair ?buy(obj:X)?
belong(subj:X)?
the operator ?
specifies that thepremise buy entails the consequence belong, and Xindicates a mapping between the object of buy andthe subject of belong, as in The company boughtshares.
- The shares belong to the company.As opposed to logical entailment, we do not re-quire that verb entailment holds in all conceivablecontexts and view it as a relation that may be moreplausible in some contexts than others.
For eachverb pair, we therefore wish to assign a score quan-tifying the likelihood of its satisfying entailment insome random context.4 ApproachThe key assumption behind our approach is that theability of a verb to imply an event typically denotedby a different verb manifests itself in the regular co-occurrence of the two verbs inside locally coherenttext.
This assumption is not arbitrary: as discourseinvestigations show (Asher and Lascarides, 2003),(Hobbs, 1985), lexical entailment plays an impor-tant role in determining the local structure of dis-course.
We expect this co-occurrence regularity tobe equally characteristic of any pair of verbs relatedby entailment, regardless of is type and the syntacticbehavior of verbs.The method consists of three major steps.
First,it identifies pairs of clauses that are related in thelocal discourse.
From related clauses, it then cre-ates templates by extracting pairs of verbs alongwith relevant information as to their syntactic be-havior.
Third, the method scores each verb pairin terms of plausibility of entailment by measuringhow strongly the premise signals the appearance ofthe consequence inside the text segment at hand.
Inthe following sections, we describe these steps inmore detail.4.1 Identifying discourse-related clausesWe attempt to capture local discourse relatednessbetween clauses by a combination of several surfacecues.
In doing so, we do not build a full discourserepresentation of text, nor do we try to identify thetype of particular rhetorical relations between sen-tences, but rather identify pairs of clauses that arelikely to be discourse-related.Textual proximity.
We start by parsing the cor-pus with a dependency parser (we use Connexor?sFDG (Tapanainen and Ja?rvinen, 1997)), treatingevery verb with its dependent constituents as aclause.
For two clauses to be discourse-related, werequire that they appear close to each other in thetext.
Adjacency of sentences has been previouslyused to model local coherence (Lapata, 2003).
Tocapture related clauses within larger text fragments,we experiment with windows of text of various sizesaround a clause.Paragraph boundaries.
Since locally relatedsentences tend to be grouped into paragraphs, wefurther require that the two clauses appear within thesame paragraph.Common event participant.
Entity-based theo-ries of discourse (e.g., (Grosz et al, 1995)) claimthat a coherent text segment tends to focus on aspecific entity.
This intuition has been formalizedby (Barzilay and Lapata, 2005), who developed anentity-based statistical representation of local dis-course and showed its usefulness for estimating co-herence between sentences.
We also impose this asa criterion for two clauses to be discourse-related:their arguments need to refer to the same participant,henceforth, anchor.
We identify the anchor as thesame noun lemma appearing as an argument to theverbs in both clauses, considering only subject, ob-ject, and prepositional object arguments.
The anchormust not be a pronoun, since identical pronouns mayrefer to different entities and making use of such cor-respondences is likely to introduce noise.4.2 Creating templatesOnce relevant clauses have been identified, we cre-ate pairs of syntactic templates, each consisting of averb and the label specifying the syntactic role theanchor occupies near the verb.
For example, givena pair of clauses Mary bought a house.
and Thehouse belongs to Mary., the method will extract twopairs of templates: {buy(obj:X), belong(subj:X)}and {buy(subj:X), belong(to:X).
}Before templates are constructed, we automati-cally convert complex sentence parses to simpler,but semantically equivalent ones so as to increasethe amount of usable data and reduce noise:?
Passive constructions are turned into active51ones: X was bought by Y ?
Y bought X;?
Phrases with coordinated nouns and verbs aredecomposed: X bought A and B ?
X bought A,X bought B; X bought and sold A ?
X bought A,X sold A.?
Phrases with past and present participles areturned into predicate structures: the group ledby A ?
A leads the group; the group leading themarket ?
the group leads the market.The output of this step is V ?
P ?Q, a set of pairsof templates {p, q}, where p ?
P is the premise,consisting of the verb vp and rp ?
the syntactic re-lation between vp and the anchor, and q ?
Q is theconsequence, consisting of the verb vq and rq ?
itssyntactic relation to the anchor.4.3 Measuring asymmetric associationTo score the pairs for asymmetric association, weuse a procedure similar to the method by (Resnik,1993) for learning selectional preferences of verbs.Each template in a pair is tried as both a premiseand a consequence.
We quantify the ?preference?of the premise p for the consequence q as the con-tribution of q to the amount of information p con-tains about its consequences seen in the data.
First,we calculate Kullback-Leibler Divergence (Cover.and Thomas, 1991) between two probability distrib-utions, u ?
the prior distribution of all consequencesin the data and w ?
their posterior distribution givenp, thus measuring the information p contains aboutits consequences:Dp(u||w) =?nu(x) log u(x)w(x) (1)where u(x) = P (qx|p), w(x) = P (qx), and x rangesover all consequences in the data.
Then, the score fortemplate {p, q} expressing the association of q withp is calculated as the proportion of q?s contributionto Dp(u||w):Score(p, q) = P (q|p) log P (q|p)P (p) Dp(u||w)?1 (2)In each pair we compare the scores in both di-rections, taking the direction with the greater scoreto indicate the most likely premise and consequenceand thus the direction of entailment.5 Evaluation Design5.1 TaskTo evaluate the algorithm, we designed a recognitiontask similar to that of pseudo-word disambiguation(Schu?tze, 1992), (Dagan et al, 1999).
The task was,given a certain premise, to select its correct conse-quence out of a pool with several artificially createdincorrect alternatives.The advantages of this evaluation technique aretwofold.
On the one hand, the task mimics manypossible practical applications of the entailment re-source, such as sentence ordering, where, given asentence, it is necessary to identify among severalalternatives another sentence that either entails or isentailed by the given sentence.
On the other hand,in comparison with manual evaluation of the directoutput of the system, it requires minimal human in-volvement and makes it possible to conduct large-scale experiments.5.2 DataThe experimental material was created from theBLLIP corpus, a collection of texts from the WallStreet Journal (years 1987-89).
We chose 15 tran-sitive verbs with the greatest corpus frequency andused a pilot run of our method to extract 1000highest-scoring template pairs involving these verbsas a premise.
From them, we manually selected 129template pairs that satisfied entailment.For each of the 129 template pairs, four false con-sequences were created.
This was done by randomlypicking verbs with frequency comparable to that ofthe verb of the correct consequence.
A list of parsedclauses from the BLLIP corpus was consulted to se-lect the most typical syntactic configuration of eachof the four false verbs.
The resulting five templatepairs, presented in a random order, constituted a testitem.
Figure 1 illustrates such a test item.The entailment acquisition method was evaluatedon entailment templates acquired from the BritishNational Corpus.
Even though the two corpora arequite different in style, we assume that the evalua-tion allows conclusions to be drawn as to the relativequality of performance of the methods under consid-eration.521* buy(subj:X,obj:Y)?own(subj:X,obj:Y)2 buy(subj:X,obj:Y)?approve(subj:X,obj:Y)3 buy(subj:X,obj:Y)?reach(subj:X,obj:Y)4 buy(subj:X,obj:Y)?decline(subj:X,obj:Y)5 buy(subj:X,obj:Y)?compare(obj:X,with:Y)Figure 1: An item from the test dataset.
The tem-plate pair with the correct consequence is markedby an asterisk.5.3 Recognition algorithmDuring evaluation, we tested the ability of themethod to select the correct consequence among thefive alternatives.
Our entailment acquisition methodgenerates association scores for one-slot templates.In order to score the double-slot templates in theevaluation material, we used the following proce-dure.Given a double-slot template, we divide it intotwo single-slot ones such that matching argumentsof the two verbs along with the verbs themselvesconstitute a separate template.
For example, ?buy(subj:X, obj:Y) ?
own (subj:X, obj:Y)?
will be de-composed into ?buy (subj:X) ?
own (subj:X)?
and?buy (obj:Y) ?
own (obj:Y)?.
The scores of thesetwo templates are then looked up in the generateddatabase and averaged.
In each test item, the fivealternatives are scored in this manner and the onewith the highest score was chosen as containing thecorrect consequence.The performance was measured in terms of accu-racy, i.e.
as the ratio of correct choices to the totalnumber of test items.
Ties, i.e.
cases when the cor-rect consequence was assigned the same score as oneor more incorrect ones, contributed to the final accu-racy measure proportionate to the number of tyingalternatives.This experimental design corresponds to a ran-dom baseline of 0.2, i.e.
the expected accuracy whenselecting a consequence template randomly out of 5alternatives.6 Results and DiscussionWe now present the results of the evaluation of themethod.
In Section 6.1, we study its parameters anddetermine the best configuration.
In Section 6.2, wecompare its performance against that of human sub-jects as well as that of two state-of-the-art lexical re-sources: the verb entailment knowledge contained inWordNet2.0 and the inference rules from the DIRTdatabase (Lin and Pantel, 2001).6.1 Model parametersWe first examined the following parameters of themodel: the window size, the use of paragraphboundaries, and the effect of the shared anchor onthe quality of the model.6.1.1 Window size and paragraph boundariesAs was mentioned in Section 4.1, a free parame-ter in our model is a threshold on the distance be-tween two clauses, that we take as an indicator thatthe clauses are discourse-related.
To find an opti-mal threshold, we experimented with windows of1, 2 ... 25 clauses around a given clause, takingclauses appearing within the window as potentiallyrelated to the given one.
We also looked at the ef-fect paragraph boundaries have on the identificationof related clauses.
Figure 2 shows two curves de-picting the accuracy of the method as a function ofthe window size: the first one describes performancewhen paragraph boundaries are taken into account(PAR) and the second one when they are ignored(NO PAR).Figure 2: Accuracy of the algorithm as a functionof window size, with and without paragraph bound-aries used for delineating coherent text.One can see that both curves rise fairly steeply upto window size of around 7, indicating that many en-tailment pairs are discovered when the two clausesappear close to each other.
The rise is the steepest53between windows of 1 and 3, suggesting that entail-ment relations are most often explicated in clausesappearing very close to each other.PAR reaches its maximum at the window of 15,where it levels off.
Considering that 88% of para-graphs in BNC contain 15 clauses or less, we takethis as an indication that a segment of text whereboth a premise and its consequence are likely to befound indeed roughly corresponds to a paragraph.NO PAR?s maximum is at 10, then the accuracystarts to decrease, suggesting that evidence founddeeper inside other paragraphs is misleading to ourmodel.NO PAR performs consistently better than PARuntil it reaches its peak, i.e.
when the window size isless than 10.
This seems to suggest that several ini-tial and final clauses of adjacent paragraphs are alsolikely to contain information useful to the model.We tested the difference between the maximaof PAR and NO PAR using the sign test, the non-parametric equivalent of the paired t-test.
The testdid not reveal any significance in the difference be-tween their accuracies (6-, 7+, 116 ties: p = 1.000).6.1.2 Common anchorWe further examined how the criterion of thecommon anchor influenced the quality of the model.We compared this model (ANCHOR) against the onethat did not require that two clauses share an anchor(NO ANCHOR), i.e.
considering only co-occurrenceof verbs concatenated with specific syntactic role la-bels.
Additionally, we included into the experimenta model that looked at plain verbs co-occurring in-side a context window (PLAIN).
Figure 3 comparesthe performance of these three models (paragraphboundaries were taken into account in all of them).Compared with ANCHOR, the other two modelsachieve considerably worse accuracy scores.
Thedifferences between the maximum of ANCHOR andthose of the other models are significant accordingto the sign test (ANCHOR vs NO ANCHOR: 44+, 8-,77 ties: p < 0.001; ANCHOR vs PLAIN: 44+, 10-,75 ties: p < 0.001).
Their maxima are also reachedsooner (at the window of 7) and thereafter their per-formance quickly degrades.
This indicates that thecommon anchor criterion is very useful, especiallyfor locating related clauses at larger distances in thetext.Figure 3: The effect of the common anchor on theaccuracy of the method.The accuracy scores for NO ANCHOR and PLAINare very similar across all the window size settings.It appears that the consistent co-occurrence of spe-cific syntactic labels on two verbs gives no addi-tional evidence about the verbs being related.6.2 Human evaluationOnce the best parameter settings for the methodwere found, we compared its performance againsthuman judges as well as the DIRT inference rulesand the verb entailment encoded in the WordNet 2.0database.Human judges.
To elicit human judgments onthe evaluation data, we automatically converted thetemplates into a natural language form using a num-ber of simple rules to arrange words in the correctgrammatical order.
In cases where an obligatorysyntactic position near a verb was missing, we sup-plied the pronouns someone or something in that po-sition.
In each template pair, the premise was turnedinto a statement, and the consequence into a ques-tion.
Figure 4 illustrates the result of converting thetest item from the previous example (Figure 1) intothe natural language form.During the experiment, two judges were askedto mark those statement-question pairs in each testitem, where, considering the statement, they couldanswer the question affirmatively.
The judges?
deci-sions coincided in 95 of 129 test items.
The Kappastatistic is ?=0.725, which provides some indicationabout the upper bound of performance on this task.54X bought Y.
After that:1* Did X own Y?2 Did X approve Y?3 Did X reach Y?4 Did X decline Y?5 Did someone compare X with Y?Figure 4: A test item from the test dataset.
The cor-rect consequence is marked by an asterisk.DIRT.
We also experimented with the inferencerules contained in the DIRT database (Lin and Pan-tel, 2001).
According to (Lin and Pantel, 2001), aninference rule is a relation between two verbs whichare more loosely related than typical paraphrases,but nonetheless can be useful for performing infer-ences over natural language texts.
We were inter-ested to see how these inference rules perform onthe entailment recognition task.For each dependency tree path (a graph linking averb with two slots for its arguments), DIRT con-tains a list of the most similar tree paths along withthe similarity scores.
To decide which is the mostlikely consequence in each test item, we looked upthe DIRT database for the corresponding two depen-dency tree paths.
The template pair with the greatestsimilarity was output as the correct answer.WordNet.
WordNet 2.0 contains manually en-coded entailment relations between verb synsets,which are labeled as ?cause?, ?troponymy?, or ?en-tailment?.
To identify the template pair satisfyingentailment in a test item, we checked whether thetwo verbs in each pair are linked in WordNet interms of one of these three labels.
Because Word-Net does not encode the information as to the rela-tive plausibility of relations, all template pairs whereverbs were linked in WordNet, were output as cor-rect answers.Figure 5 describes the accuracy scores achievedby our entailment acquisition algorithm, the two hu-man judges, DIRT and WordNet.
For comparisonpurposes, the random baseline is also shown.Our algorithm outperformed WordNet by 0.38and DIRT by 0.15.
The improvement is significantvs.
WordNet (73+, 27-, 29 ties: p<0.001) as well asvs.
DIRT (37+, 20-, 72 ties: p=0.034).We examined whether the improvement on DIRTwas due to the fact that DIRT had less extensiveFigure 5: A comparison of performance of theproposed algorithm, WordNet, DIRT, two humanjudges, and a random baseline.coverage, encoding only verb pairs with similarityabove a certain threshold.
We re-computed the ac-curacy scores for the two methods, ignoring caseswhere DIRT did not make any decision, i.e.
wherethe database contained none of the five verb pairsof the test item.
On the resulting 102 items, ourmethod was again at an advantage, 0.735 vs. 0.647,but the significance of the difference could not beestablished (21+, 12-, 69 ties: p=0.164).The difference in the performance between our al-gorithm and the human judges is quite large (0.103vs.
Judge 1 and 0.088 vs Judge 2), but significanceto the 0.05 level could not be found (vs. Judge 1:17-, 29+, 83 ties: p=0.105; vs. Judge 2: 15-, 27+,ties 87: p=0.09).7 ConclusionIn this paper we proposed a novel method for au-tomatic discovery of verb entailment relations fromtext, a problem that is of potential benefit for manyNLP applications.
The central assumption behindthe method is that verb entailment relations mani-fest themselves in the regular co-occurrence of twoverbs inside locally coherent text.
Our evaluationhas shown that this assumption provides a promis-ing approach for discovery of verb entailment.
Themethod achieves good performance, demonstratinga closer approximation to the human performancethan inference rules, constructed on the basis of dis-tributional similarity between paths in parse trees.A promising direction along which this work55can be extended is the augmentation of the currentalgorithm with techniques for coreference reso-lution.
Coreference, nominal and pronominal, isan important aspect of the linguistic realization oflocal discourse structure, which our model did nottake into account.
As the experimental evaluationsuggests, many verbs related by entailment occurclose to one another in the text.
It is very likely thatmany common event participants appearing in suchproximity are referred to by coreferential expres-sions, and therefore noticeable improvement canbe expected from applying coreference resolutionto the corpus prior to learning entailment patternsfrom it.AcknowledgementsWe are grateful to Nikiforos Karamanis and Mirella Lapataas well as three anonymous reviewers for valuable commentsand suggestions.
We thank Patrick Pantel and Dekang Lin formaking the DIRT database available for this study.ReferencesN.
Asher and A. Lascarides.
2003.
Logics of Conversation.Cambridge University Press.R.
Barzilay and M. Lapata.
2005.
Modeling local coherence:an entity-based approach.
In Proceedings of the 43rd An-nual Meeting of the Association for Computational Linguis-tics (ACL?05), pages 141?148.R.
Barzilay, N. Elhadad, and K. McKeown.
2002.
Inferringstrategies for sentence ordering in multidocument summa-rization.
JAIR.T.
Chklovski and P. Pantel.
2004.
VERBOCEAN: Mining theweb for fine-grained semantic verb relations.
In In Proceed-ings of Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP?04).T.M.
Cover.
and J.A.
Thomas.
1991.
Elements of InformationTheory.
Wiley-Interscience.J.
Curtis, G. Matthews, and D. Baxter.
2005.
On the effectiveuse of cyc in a question answering system.
In Proceedingsthe IJCAI?05 Workshop on Knowledge and Reasoning forAnswering Questions.I.
Dagan, L. Lee, and F. Pereira.
1999.
Similarity-based mod-els of cooccurrence probabilities.
Machine Learning, 34(1-3):43?69.I.
Dagan, O. Glickman, and B. Magnini.
2005.
The pascalrecognising textual entailment challenge.
In PASCAL Chal-lenges Workshop on Recognising Textual Entailment.C.
Fellbaum, 1998.
WordNet: An Electronic Lexical Database,chapter Semantic network of English verbs.
MIT Press.M.
Geffet and I. Dagan.
2005.
The distributional inclusion hy-potheses and lexical entailment.
In Proceedings of the 43rdAnnual Meeting of the Association for Computational Lin-guistics (ACL?05), pages 107?114.R.
Girju.
2003.
Automatic detection of causal relations forquestion answering.
In Proceedings of the ACL?03 Work-shop on ?Multilingual Summarization and Question Answer-ing - Machine Learning and Beyond?.B.
Grosz, A. Joshi, and S.Weinstein.
1995.
Centering : a frame-work for modeling the local coherence of discourse.
Com-putational Linguistics, 21(2):203?225.J.R.
Hobbs.
1985.
On the coherence and structure of discourse.Technical Report CSLI-85-37, Center for the Study of Lan-guage and Information.T.
Inui, K.Inui, and Y.Matsumoto.
2003.
What kinds andamounts of causal knowledge can be acquired from text byusing connective markers as clues?
In Proceedings of the6th International Conference on Discovery Science, pages180?193.M.
Lapata.
2003.
Probabilistic text structuring: experimentswith sentence ordering.
In Proceedings of the 41rd AnnualMeeting of the Association for Computational Linguistics(ACL?03), pages 545?552.D.
Lin and P. Pantel.
2001.
Discovery of inference rulesfor question answering.
Natural Language Engineering,7(4):343?360.D.
Moldovan and V. Rus.
2001.
Logic form transformationof WordNet and its applicability to question answering.
InProceedings of the 39th Annual Meeting of the Associationfor Computational Linguistics (ACL?01).B.
Pang, K. Knight, and D. Marcu.
2003.
Syntax-basedalignment of multiple translations: extracting paraphrasesand generating new sentences.
In Proceedings of HLT-NAACL?2003.E.
Reiter and R. Dale.
2000.
Building Natural Language Gen-eration Systems.
Cambidge University Press.P.
Resnik.
1993.
Selection and Information: A Class-BasedApproach to Lexical Relationships.
Ph.D. thesis, Universityof Pennsylvania.H.
Schu?tze.
1992.
Context space.
In Fall Symposium on Prob-abilistic Approaches to Natural Language, pages 113?120.I.
Szpektor, H. Tanev, I. Dagan, and B. Coppola.
2004.
Scalingweb-based acquisition of entailment relations.
In Proceed-ings of Empirical Methods in Natural Language Processing(EMNLP?04).P.
Tapanainen and T. Ja?rvinen.
1997.
A non-projective depen-dency parser.
In Proceedings of the 5th Conference on Ap-plied Natural Language Processing, pages 64?71.K.
Torisawa, 2003.
Questions and Answers: Theoreticaland Applied Perspectives, chapter An unsupervised learningmethod for commonsensical inference rules on events.
Uni-versity of Utrecht.56
