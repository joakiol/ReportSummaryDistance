Automatic Recognition of Chinese Unknown Words1 Based on Roles Tagging2Kevin Zhang (Hua-Ping ZHANG)  Qun LIU   Hao ZHANG    Xue-Qi CHENGSoftware Division, Institute of Computing Technology, Chinese Academy of SciencesNO.
6, South Road, Kexueyuan, Zhongguancun, Haidian Dist.
P.O.
BOX 2704, Beijing, P.R.
China, 100080Email: {zhanghp,liuqun, zhanghao,cxq}@software.ict.ac.cnAbstractThis paper presents a unified solution, whichis based on the idea of ?roles tagging?, to thecomplicated problems of Chinese unknown wordsrecognition.
In our approach, an unknown word isidentified according to its component tokens andcontext tokens.
In order to capture the functions oftokens, we use the concept of roles.
Roles aretagged through applying the Viterbi algorithm inthe fashion of a POS tagger.
In the resulted mostprobable roles sequence, all the eligible unknownwords are recognized through a maximum patternsmatching.
We have got excellent precision andrecalling rates, especially for person names andtransliterations.
The result and experiments in oursystem ICTCLAS shows that our approach basedon roles tagging is simple yet effective.Keywords: Chinese unknown words recognition,roles tagging, word segmentation, Viterbialgorithm.IntroductionIt is well known that word segmentation is aprerequisite to Chinese information processing.Previous research and work in word segmentationhave made great progresses.
However, cases withunknown words are not satisfactory.
In general,any lexicon is limited and unable to cover all thewords in real texts or speeches.
According to ourstatistics on a 2,305,896-character news corpusfrom the People's Daily, there are about 1.19%unknown words.
But they are difficult to berecalled and often greatly reduce the recognitionrate of known words close to them.
For example,the sentence ?
?
?
?
?
?
?
?
?
?
?
(Pronunciation: ?Bu Zhang Sun Jia Zheng ZaiGong Zuo.?)
has two valid segmentations:  ???/???/?/???
(The minister Sun Jiazheng isat work) and ???
/??
/??
/??
?
(Theminister Sun Jia now is at work).
?????
is aperson name in the first, while ????
is anothername in the latter.
Meanwhile, the string ??????
will lead to overlapping ambiguity and bring acollision between the unknown word ?????
(Sun Jiazheng) and ????
(zheng zai; now).What?s more, the recognizing precision rates ofperson names, place names, and transliterations are91.26%, 69.12%, and 82.83%, respectively, whilethe recalling rates of them are just 68.77%, 60.47%,and 78.29%, respectively.
(Data from officialtesting in 1999) [Liu (1999)] In a word, unknownwords recognition has become one of the biggeststumbling blocks on the way of Chinese lexicalanalysis.
A proper solution is important andurgent.Various approaches are taken in Chineseunknown words recognition.
They can be broadlycategorized into ?one-for-one?, ?one-for-several?and ?one-for-all?
based on the number ofcategories of unknown words, which can berecognized.
One-for-one solutions solve aparticular problem, such as person namerecognition [Song (1993); Ji (2001)], place namerecognition [Tan (1999)] and transliterationrecognition [Sun (1993)].
Similarly,one-for-several approaches provide one solutionfor several specific categories of unknown words[Lv (2001); Luo (2001)].
One-for-all solutions, asfar as we know, have not been applicable yet[Chen (1999); He (2001)].Although currently practicable methods couldachieve great precision or recalling rates in somespecial cases, they have their inherent deficiencies.First of all, rules applied are mostly summarizedby linguists through painful study of all kinds ofhuge ?special name libraries?
[Luo (2001)].
It?stime-consuming, expensive and inflexible.
Thecategories of unknown words are diverse and theamount of such words is huge.
With the rapiddevelopment of the Internet, this situation isbecoming more and more serious.
Therefore, it?svery difficult to summarize simple yet thoroughrules about their compositions and contexts.Secondly, the recognition process cannot beactivated until some ?indicator?
tokens arescanned in.
For instance, possible surnames ortitles often trigger person name recognition on thefollowing 2 or more characters.
In the case ofplace name recognition, the postfixes such as?
?
?
(county), ?
?
?
(city) will activate therecognition on the previous characters.
What?smore, these methods tend to work only on themonosyllabic tokens, which are obvious fragmentsafter tokenization [Luo (2001); Lv (2001)].
It takesthe risk of losing lots of unknown words withoutany explicit features.
Furthermore, this triggermechanism cannot resolve the ambiguity.
Forexample, unknown word ?????
(Fang Lin Shan)maybe a person name ??/???
(Fang Linshan) ora place name ???/??
(Fanglin Mountain).This paper presents a one-for-all approachbased on roles tagging to avoid such problems.The process is: tagging tokens after wordsegmentation with the most probable roles andmaking unknown words recognition based on rolessequence.
The mechanism of roles tagging is justlike that of a small and simple Part-Of-Speechtagger.The paper is organized as follows: In section2, we will describe the approach in general.Following that, we will present the solution inpractice.
In the final part, we provide recognitionexperiments using roles-tagging methods.
Theresult and possible problems are discussed as well.1 Unknown words recognition based on rolestagging1.1 Lexical roles of unknown wordsUnknown words are often made up ofdistinctive components, most of which aremonosyllabic characters or short words; inaddition, there are some regular relations betweenunknown words and their locality, especially withtheir left and right context.
As we often write orspeak, a Chinese person name is usually comprisedof a one-or-two-character surname and a followinggiven name of one or two characters, like ?????
(Xiao Jianqun) and ?????
(Zhu-Ge Liang).The  previous words are mostly titles,occupations or some conjunctive words, such as????
(Manager), ????
(Driver) and ???
(To).The following words tend to be verbs such as ???
(to say) , ????
(to express).
Similar components,contexts and relations can be discovered in placename, transliteration, organization name, or othertypes of unknown words.We define unknown word roles with respectto varied internal components, previous andsucceeding contexts and other tokens in aparticular sentence.
Various roles are extractedaccording to their functions in the forming ofdifferent unknown words.
Person names roles andtransliterations roles set are shown in table 1a and1b respectively.
Using the roles set of person name,the tokens sequence ??/?/??/?/?/?/?/?/?/??/?/??/?/?/??/?
(What Zhou Enlaiand Deng Yunchao used before death arepresented in the museum) will be tagged as ?
?/A?/A ?
?/K ?/B ?/C ?/D ?/M ?/B ?/C?
?/V ?/A ?
?/A ?/A ?/A?
?/A?.Role Significance ExamplesB Surname or familyname.?/?/?/????
?/?C First Chinese char inthe 2-char given name?/?/?/?
?D Last Chinese char inthe 2-char given name.?/?/?/?
?E Given name with asingle Chinese char.
?/?F Prefix in the name.
?/??
?/?G Postfix in the name.
?/???/??
?/?K Previous context beforeperson name.?/?
?/?/?/?/?/?L Succeeding contextfollowing person name.???/???
?/?M Parts between twoperson names.?
?/?/?/?/?/?/?/?/?U Known wordsgenerated by previouscontext and the firstcomponent of name.??
/??
/?
/?/?/??/???
/??
/??
/?/?/?/V Known wordsgenerated by the lastcomponent and nextcontext.?
/?
/??
/??
/, ?
/?
/?
?/?.....A Others tokens notmentioned above.??/??/??/?
?/?/?/?/Table 1a: Roles set of Chinese person namesRole Significance ExamplesB The firstcomponent oftransliteration?/?/?C Middle component ?/?/?/?/?/?/?/?/?D Last component  ?/?/?.....Table 1b: Roles set of transliterations1.2 Roles tagging and unknown words recognitionOn the one hand, the sentence include wordswith different roles for a particular category ofunknown words, on the other hand, such wordscan be recognized after identifying their rolessequence.
That is: tagging tokens after wordsegmentation with the most probable rolessequence, then recognizing unknown words bymaximum patterns matching on the final rolessequence.Roles tagging is similar to Part-Of-Speechtagging.
Our tagging process is based on ViterbiAlgorithm [Rabiner and Juang (1989)], which is toselect the optimum with maximum probabilityfrom all possible tag sequences.
The methodologyand its deduction is given as below:Suppose that T is the tokens sequence afterword segmentation and R is the roles sequence forT.
We take the role sequence R# with themaximum probability as the best choice.
That is:T=(t1, t 2, ?
, t m),R=(r1, r2, ?
, rm), m>0,R#= arg P(R|T)...................?........E1RmaxAccording to the Bayes equation, we can get:P(R|T)= P(R)P(T|R)/P(T) ...................E2For a particular token sequence, P(T) is aconstant.
So, We can get E3 based on E1 and E2:R#= arg P(R)P(T|R) ......................E3RmaxWe may consider T as the observation valuesequence while R as the state sequence hiddenbehind the observation.
Now we introduce HiddenMarkov Model [Rabiner and Juang (1986)] toresolve such a typical problem:P(R) P(T|R)?
?=?miiiii rrprtp01 )|()|(?R#?
.......E4Rmaxarg ?=?miiiii rrprtp01 )|()|(?R#??
......E5Rminarg ?=?+miiiii rrprtp01)}|(ln)|({lnE5 is simpler for computation than E4.Now, we can find the most possible tokensequence with equation E5.
It?s a simpleapplication of Viterbi Algorithm.The final recognition through maximum patternmatching is not performed on the original texts butperformed on roles sequence.
The person patternsare {BBCD, BBE, BBZ, BCD, BE, BG, BXD, BZ, CD, FB,Y, XD}.
Before matching, we should split thetokens whose roles are like ?U?
or ?V?
(whichindicate that the related token is generated byinternal components and the outside contexts ofunknown words) into two proper parts.
Such aprocessing can recall more unknown words andreduce the overlapping collision.
As for the abovesample sentence, the final roles sequence aftersplitting is ?AAKBCDMBCDLAAAAAA?.
Therefore,we can identify the possible person names ?????
and ?????
according to the recognitionpattern ?BCD?.1.3  Automatic acquisition of roles knowledgeAs described in E5, the tag sequence R#  isdecided by two kinds of factors: and.
is the probability of atoken t)|( ii rtp)|( 1?ii rrp )|( ii rtp( irpi given the condition of being tagged withrole ri, while  is the transitiveprobability from role r)| 1?iri-1 to role ri.
Both factors areuseful lexical knowledge for tagging and finalrecognition.
According to laws of large numbers, ifthe training corpus is large enough, we can acquirethe roles knowledge as following:)|( ii rtp ?C(ti,ri)/C(ri) ................??.........
E6Where C(ti, ri) is the count of token ti  being role ri;and C(ri) is the count of role ri.
)|( 1?ii rrp ?C(ri-1,ri)/C(ri-1) ........?.?.....
?E7Where C(ri-1,ri) is the count of role ri-1  followedby role ri.C(ti,ri), C(ri) and C(ri-1,ri) are extracted fromcorpus through a training process.
The trainingcorpus came from one-month news from thePeople?s Daily with 2,305,896 Chinese characters,which are manually checked after wordsegmentation and POS tagging (It can bedownloaded at icl.pku.edu.cn, the homepage of theInstitute of Computational Linguistics, PekingUniversity).However, the corpus is tagged with thePart-Of-Speech set.
Before training, the originalPOS tags should be converted to the proper rolesby analysing every token in the sentence.2 Algorithm and implementationThe unknown words recognition based onroles tagging has three main steps: automaticacquisition of roles knowledge from the corpus;roles tagging with Viterbi algorithm and unknownwords recognition through maximum patternmatching.Viterbi algorithm is a classic approach instatistics.
It aims to select the optimum rolessequence with maximum possibility from allpossible results.
Our evaluation function fordecision-making is E5 given in sub-section 1.2.Considering the length limitation of this paper, weskip the details.Therefore, we only provide algorithms forroles knowledge learning.
In the last part, theentire process of unknown words recognition willbe listed.2.1 Roles knowledge learningInput: Corpus which is segmented and POStaggedT: the type of unknown words;R: Roles set of TOutput: C(ti,ri), C(ri) and C(ri-1,ri)Algorithm:(1) Get one sentence S from corpus C;(2) Extract all tokens and POS tags from S;(3) Convert all POS tags to roles in T after roleanalysis.
(4) Store the tokens whose role is not ?A?
into therecognition lexicons of unknown words T, where?A?
is not internal components nor context role.
(5) Calculate the total number C(ti,ri) of token tibeing role ri.
At the same time, count C(ri), which isthe number of role ri appearances.
(6) Sum C(ri-1,ri) which is the times of role ri-1followed by role ri.
(7) If no more sentences in the corpus C, exit; elsego to (1)First of all, we must explain step (3).
Ourcorpus is tagged with POS and person, place ororganization name are tagged with ?nr?, ?ns?
or ?nt?respectively; Such POS are unique and differentfrom noun.
Transliterations can be extracted fromwords tagged with ?nr?
or ?ns?
and throughanalysing its component chars.
So we can easilylocate such kinds of words.
Meanwhile, we canjudge whether a word is unknown by looking it upin the core lexicon.
Then we can identify roles ofwords according to their locality, which are beforeor following a particular unknown word.Here we provide a sample sentence from ourcorpus like ??
?/r  ?
?/ns  ?
?/t  ?
?/t?/n  ?
?/n  ?/nr  ?
?/nr  ?/w  ?/nr  ?
?/nr  ??/v?.
In step (2), we can extract tokensand tags like ???
?/ ?r?
; ???
?/ ?ns?
and so on.When we train person recognition roles, firstly, welocate person name ?
?/nr  ??/nr?
and ??/nr??/nr?
just by searching POS ?nr?
; Secondly,judge whether they are unknown after lookingthem up in the core lexicon; At last we can tagunknown words component and their context neartheir locality.
So the final roles after conversionare ??
?/A  ?
?/A ?
?/A 1?/A ?/A ?
?/K ?/B  ?/C?/D ?/M ?/B ?/C?/D ??/L?.
Then we can train the parameters based onnew segmentation and person recognition rolessequence.In addition, we train every different kind ofunknown word on the same corpus individually.That is: person roles, place roles and other rolesare acquired respectively.
Therefore, the unknownplace recognition roles sequence of the abovesentence may like ??
?/K  ?/B?/D  ??/L?
?/A  ?/A  ?
?/K  ?/A ?
?/A  ?/A?/A ?
?/A  ??/A?.
Such a mechanism cangreatly reduce the problem of sparse data.2.2 The entire process of Unknown wordsrecognitionInput: Original sentence S;R: the roles set of unknown words;P: pattern sets for recognition.Output: Possible unknown words of type T.Algorithm:(1) Word segmentation (we segment words onsentence S with N-shortest paths method[Hua-Ping ZHANG, Qun LIU (2002)]);(2) Tag tokens sequence with roles in set R usingViterbi algorithm.
Get the roles sequence R#with maximum possibility.
(3) Split tokens whose role is like ?U?
or ?V?
in theperson roles.
These roles indicate that theinternal components glue together with theircontext.
(4) Maximum match final roles sequence to therecognition patterns P and record theirposition.
(5) Generate the candidate unknown wordsaccording to the result of pattern matching.
(6) Exclude those candidates which are fit for theexclusive rules.
(For example, Chinese personname can not include non-Chinese chars.
)(7) Output the possible unknown words.Now, we take person recognition on thesentence ??????????????????????
as exemplification.
In the first place,we can get the sequence ???/??/??/??/?/??/?/?/?/?/?/?/?/??/?
after roughword segmentation; Then we tag it with Viterbialgorithm using person recognition roles lexiconand transitive array.
So, the most probable rolessequence is ?AAAAAKBCDMBCDL?.Therefore,candidate perosn names ?????
and ????
?can be recognized after maximum string matching.3 Experiments and DiscussionsBoth close and open recognition test wereconducted.
In the close test, we tested our systemwithin the training corpus, which is the knowledgebase for recognition.
Open test, however, is morerealistic, because it is performed on arbitrary realtexts outside the training corpus.
The corpus in ourexperiments is from 2-months news in 1998 fromthe People?s Daily.In this paper, we only provide the recognitionresults of Chinese person and transliterations.
Therecognition of place names and other kind ofunknown words can get similar performance.3.1 Recognition experiment of Chinese person nameTest Type Close OpenCorpus (news date) 1.1-2.20 2.20-2.28Corpus Size  14,446K 2,605KNum of Chineseperson names21,256 3,149Num of recognizedperson names27,813 4,130Num of correctly 20,865 2,886recognized namesPrecision rate 75.02% 69.88%Recalling rate 98.17% 91.65%F-measurement  85.05% 79.30%Table 2 Experiment results of Chinese personnames recognitionIn Tables 2, precision rate and recalling rate aredefined as equations E6 and E7 respectively.
Inaddition, F-measurement is a uniformly weightedharmonic mean of precision rate and recalling rateas shown in E8.Precision rate=wordsrecognized of numwordsrecognizedcorrectly  of num?
?..E6Recalling rate=wordsunknown   totalofnumwordsrecognizedcorrectly  of num?
?..E7F-measurement =ratePrecision rate Recalling2ratePrecision rate Recalling+??...
?.E83.2 Recognition Experiments of transliterationsTest Type Close OpenCorpus (news date) 1.1-2.20 2.20-2.28Corpus Size  14,446K 2,605KNum oftransliterations9,059 1,592Num of recognizedtransliterations10,013 1,930Num of correctlyrecognizedtransliterations8,946 1,496Precision rate 89.35% 77.52%Recalling rate 98.75% 93.97%F-measurement  93.85% 84.96%Table 3 Results of transliterations recognition3.3 DiscussionsThe traditional ways to test unknown wordsrecognition is to collect sentences includingunknown words and to make recognitionexperiments.
Those sentences that haven?t the typeof unknown words will be excluded fromexperiments in the pre-processing.
In ourexperiments, we just take the realistic corpus andmake no filtering.
Therefore, the precision ratesmay be lower but closer to the realistic linguisticenvironment than previous tests.
We have madeexperiments in the traditional way and theprecision rate can be improved by less than 15%.In a word, there is no comparable with precisionrates of previous unknown words recognitionexperiment.In addition, our experiments show that theunknown words recognition based on role taggingcan achieve very high recalling rates.
For such aproblem, recalling is more essential than precision.Low recalling rate means that we have no chanceto recognize many unknown words through anyefforts in the following steps, although wordsrecognized are mostly valid; However, precisionrate can be greatly improved in other processes,such as POS tagging or sentence simple parsing.
Inour system ICTCLAS (Institute of ComputingTechnology, Chinese Lexical System), we canexclude most invalid unknown words during POStagging.
The precision rate of Chinese personnames recognition can achieve over 95% afterPOS tagging while the recalling rate is notreduced.Our approach is purely corpus-based.
We allknow that, in any usual corpus, unknown wordsare sparsely distributed.
If we depend totally on thecorpus, the problem of sparse data is inevitable.But in the fine-tuning of our system, we foundsome countermeasures and successfully solved theproblem.Lexical knowledge from linguists can beincorporated into the system.
This does not meanthat we fall back to the old ways.
We just demandfor those general rules about name formation toavoid apparent mistakes.
As to person namerecognition, there are several strict restrictions,such as the length of name, the order betweensurname and given name.Except for enlarging the training corpus, weprovide two more counteractions:Firstly, a ?best n?
approach [Hua-Ping ZHANG,Qun LIU (2002)], which provides n (n>1) possibletag sequences with leading probabilities, is feasible.Usually the desired tag sequence could bere-targeted or constructed from the best nsequences.
In this way, we improved the recallingrate at the cost of precision rate.
But given a betterrecalling, we could remedy in latter stages oflanguage processing.
When 3 most probablesequences are employed, the recalling andprecision of unknown words in ICTCLAS can beenhanced obviously.The second resolution is training on a namelibrary in addition to training on a corpus.
As weall know, it?s easier and cheaper to get a personname library or other special name libraries than tosegment and tag a corpus.
We could extract theinner components relations from the unknownwords libraries, and then merge these data into theroles information from the original corpus.
Whenthe special name libraries were introduced, bothprecision and recalling rates can be improved.ConclusionThe paper presents a one-for-all approach forChinese unknown words recognition based onroles tagging.
At first, we define roles set for everycategory of unknown words according to thefunction of tokens, such as internal component orcontexts.
Unknown words are recognized on rolessequence, tagged with the roles set using Viterbialgorithm.
The knowledge about roles is extractedfrom the learning on corpus.
Experiments on largesize corpus verify that the approach based on roletagging is simple and applicable.AcknowledgementsFirst of all, our thanks go to the Institute ofComputational Linguistics, Peking University forproviding the corpus.
And we owe lots of thanks toour colleagues: Zougang, Li Jifeng, Li Sujian,LiShengtao, Zhu Hailong, Zhao Hongchao, WangShuxi and Dr. Zhou Lixin.
We would especiallyexpress gratitude to the chief scientist Bai Shuo.ReferencesK.Y.
Liu (1999)  The Assessment to Automatic WordSegmentation and POS Tagging Software.Proceedings of the 4th Conference on ChineseComputer Intelligent Interface and Application,Beijing.Z.
Luo and R. Song (2001)  Integrated and FastRecognition of Proper Noun in Modern Chinese WordSegmentation.
Proceedings of InternationalConference on Chinese Computing 2001, Singapore,pp.
323-328.H.
Luo and Z. Ji (2001)  Inverse Name FrequencyModel and Rules Based on Chinese Name Identifying.In "Natural Language Understanding and MachineTranslation", C. N. Huang & P. Zhang, ed., TsinghuaUniv.
Press, Beijing, China, pp.
123-128.R.
Song (1993)  Person Name Recognition MethodBased on Corpus and Rule.
In ?ComputationalLanguage Research and Development", L. W. Chen& Q. Yuan, ed., Beijing Institute of Linguistic Press.H.
Y. Tan (1999)  Chinese Place AutomaticRecognition Research.
In "Proceedings ofComputational Language ", C. N. Huang & Z.D.Dong, ed., Tsinghua Univ.
Press, Beijing, China.M.S.
Sun (1993)  English Transliteration AutomaticRecognition.
In "Computational LanguageResearch and Development", L. W. Chen & Q.
Yuan,ed., Beijing Institute of Linguistic Press.Y.J.
Lv, T. J. Zhao (2001)  Levelled Unknown ChineseWords Resolution by Dynamic Programming.Journal of Chinese Information Processing.
15, 1, pp.28-33.X.
H. Chen (1999)  One-for-all Solution for UnknownWord in Chinese Segmentation.
Application ofLanguage and Character, 3.Y.
He (2001)  Identification of Unlisted Words onTransitive Probability of Monosyllabic Words.
In"Natural Language Understanding and MachineTranslation", C. N. Huang & P. Zhang, ed., TsinghuaUniv.
Press, Beijing, China, pp.
123-128.Hua-Ping  ZHANG, Qun LIU (2002)  Model ofChinese Words Rough Segmentation Based onN-Shortest-Paths Method.
Journal of ChineseInformation Processing.
16, 5, pp.
77-83.L.
R.Rabiner (1989)  A Tutorial on Hidden MarkovModels and Selected Applications in SpeechRecognition.
Proceedings of IEEE 77(2):pp.257-286.L.R.
Rabiner and B.H.
Juang, (Jun.
1986) AnIntroduction to Hidden Markov Models.
IEEEASSP Mag., Pp.4-166.???????????????????3???
?
?
?
?
??
?Email : zhanghp@software.ict.ac.cn?????????????????
2704?????????????
6?, ????
: ??????????????????????????????????????????????????????????????????????????????????
Viterbi???????
Token????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
ICTCLAS?????????????????????????????????????????????????:?????????????????Viterbi?
?1 We define unknown words to be those not included inthe core lexicon and unable to be generated by FSA,such as person name, place name.
But numeric orcommon time word is not unknown because they cangenerate by a simple FSA.2 Related research in this paper is supported byFoundation of National Key Basic Research Project (ID:G1998030507-4 and G1998030510).3 ??????????????????(???G1998030507-4?G1998030510)??
?
