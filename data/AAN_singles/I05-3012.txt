Integrating Collocation Features in Chinese Word SenseDisambiguationWanyin LiDepartment of ComputingThe Hong Kong PolytechnicUniversityHong Hom, Kowloon, HKcswyli@comp.polyu.edu.hkQin LuDepartment of ComputingThe Hong Kong PolytechnicUniversityHong Hom, Kowloon, HKcsqinlu@comp.polyu.edu.hkWenjie LiDepartment of ComputingThe Hong Kong PolytechnicUniversityHong Hom, Kowloon, HKcswjli@comp.polyu.edu.hkAbstractThe selection of features is critical in pro-viding discriminative information for clas-sifiers in Word Sense Disambiguation(WSD).
Uninformative features will de-grade the performance of classifiers.
Basedon the strong evidence that an ambiguousword expresses a unique sense in a givencollocation, this paper reports our experi-ments on automatic WSD using collocationas local features based on the corpus ex-tracted from People?s Daily News (PDN)as well as the standard SENSEVAL-3 dataset.
Using the Na?ve Bayes classifier as ourcore algorithm, we have implemented aclassifier using a feature set combiningboth local collocation features and topicalfeatures.
The average precision on thePDN corpus has 3.2% improvement com-pared to 81.5% of the baseline systemwhere collocation features are not consid-ered.
For the SENSEVAL-3 data, we havereached the precision rate of 37.6% by in-tegrating collocation features intocontextual features, to achieve 37% im-provement  over  26.7% of precision in thebaseline system.
Our experiments haveshown that collocation features can be usedto reduce the size of human tagged corpus.1 IntroductionWSD tries to resolve lexical ambiguity whichrefers to the fact that a word may have multiplemeanings such as the word ?walk?
in  ?Walk orBike to school?
and ?BBC Education WalkThrough Time?, or the Chinese word  ????
in??????
(?local government?)
and ?????????
(?He is also partly right?).
WSD tries toautomatically assign an appropriate sense to anoccurrence of a word in a given context.Various approaches have been proposed to dealwith the word sense disambiguation problemincluding rule-based approaches, knowledge ordictionary based approaches, corpus-based ap-proaches, and hybrid approaches.
Among theseapproaches, the supervised corpus-based ap-proach had been applied and discussed by manyresearches ([2-8]).
According to [1], the corpus-based supervised machine learning methods arethe most successful approaches to WSD wherecontextual features have been used mainly todistinguish ambiguous words in these methods.However, word occurrences in the context aretoo diverse to capture the right pattern, whichmeans that the dimension of contextual wordswill be very large when all words in the trainingsamples are used for WSD [14].
Certainuninformative features will weaken the dis-criminative power of a classifier resulting in alower precision rate.
To narrow down the con-text, we propose to use collocations as contex-tual information as defined in Section 3.1.2.
It isgenerally understood that the sense of an am-biguous word is unique in a given collocation[19].
For example, ????
means ?burden?
butnot ?baggage?
when it appears in the collocation??????
(?
burden of thought?
).In this paper, we apply a classifier to combinethe local features of collocations which containthe target word with other contextual features todiscriminate the ambiguous words.
The intuitionis that when the target context captures a collo-cation, the influence of other dimensions of87contextual words can be reduced or even ig-nored.
For example, in the expression ???????????
?
(?terrorists burned down thegene laboratory?
), the influence of contextualword ????
(?gene?)
should be reduced to workon the target word ????
because ??????
isa collocation whereas ????
and ????
are notcollocations even though they do co-occur.
Ourintention is not to generally replace contextualinformation by collocation only.
Rather, wewould like to use collocation as an additionalfeature in WSD.
We still make use of other  con-textual features because of the following reasons.Firstly, contextual information is proven to beeffective for WSD in the previous researchworks.
Secondly, collocations may be independ-ent on the training corpus and a sentence in con-sideration may not contain any collocation.Thirdly, to fix the tie case such as ????????????
(?terrorists?
gene checking?),????
means ?human?
when presented inthe collocation ?????
?, but ?particle?in the collocation ??????.
The primarypurpose of using collocation in WSD is to im-prove precision rate without any sacrifices inrecall rate.
We also want to investigate whetherthe use of collocation as an additional featurecan reduce the size of hand tagged sense corpus.The rest of this paper is organized as follows.Section 2 summarizes the existing Word SenseDisambiguation techniques based on annotatedcorpora.
Section 3 describes the classifier andthe features in our proposed WSD approach.Section 4 describes the experiments and theanalysis of our results.
Section 5 is the conclu-sion.2 Related WorkAutomating word sense disambiguation tasksbased on annotated corpora have been proposed.Examples of supervised learning methods forWSD appear in [2-4], [7-8].
The learning algo-rithms applied including: decision tree, decision-list [15], neural networks [7], na?ve Bayesianlearning ([5],[11]) and maximum entropy [10].Among these leaning methods, the most impor-tant issue is what features will be used to con-struct the classifier.
It is common in WSD to usecontextual information that can be found in theneighborhood of the ambiguous word in trainingdata ([6], [16-18]).
It is generally true that whenwords are used in the same sense, they havesimilar context and co-occurrence information[13].
It is also generally true that the nearby con-text words of an ambiguous word give more ef-fective patterns and features values than thosefar from it [12].
The existing methods considerfeatures selection for context representation in-cluding both local and topic features where localfeatures refer to the information pertained onlyto the given context and topical features are sta-tistically obtained from a training corpus.
Mostof the recent works for English corpus including[7] and [8], which combine both local and topi-cal information in order to improve their per-formance.
An interesting study on featureselection for Chinese [10] has considered topicalfeatures as well as local collocational, syntactic,and semantic features using the maximum en-tropy model.
In Dang?s [10] work, collocationalfeatures refer to the local PoS information andbi-gram co-occurrences of words within 2 posi-tions of the ambiguous word.
A useful resultfrom this work based on (about one millionwords) the tagged People?s Daily News showsthat adding more features from richer levels oflinguistic information such as PoS taggingyielded no significant improvement (less than1%) over using only the bi-gram co-occurrencesinformation.
Another similar study for Chinese[11] is based on the Naive Bayes classifiermodel which has taken into consideration PoSwith position information and bi-gram templatesin the local context.
The system has a reported60.40% in both precision and recall based on theSENSEVAL-3 Chinese training data.
Eventhough in both approaches, statistically signifi-cant bi-gram co-occurrence information is used,they are not necessarily true collocations.
Forexample, in the express ????????????????????
?, the bi-grams intheir system are (???
,???
, ????
, ????
, ?????
, ????
??,?
????
Some bi-grams such as????
may have higher frequency butmay introduce noise when considering it as fea-tures in disambiguating the sense ?human|?
?and ?symbol|???
like in the example case of?????????.
In our system, we do not relyon co-occurrence information.
Instead, we util-ize true collocation information (??
?, ??
)which fall in the window size of (-5, +5) as fea-88tures and the sense of ?human|??
can be de-cided clearly using this features.
The collocationinformation is a pre-prepared collocation listobtained from a collocation extraction systemand verified with syntactic and semantic meth-ods ([21], [24]).Yarowsky [9] used the one sense per collocationproperty as an essential ingredient for an unsu-pervised Word-Sense Disambiguation algorithmto perform bootstrapping algorithm on a moregeneral high-recall disambiguation.
A few re-cent research works have begun to pay attentionto collocation features on WSD.
Domminic [19]used three different methods called bilingualmethod, collocation method and UMLS (UnifiedMedical Language System) relation basedmethod to disambiguate unsupervised Englishand German medical documents.
As expected,the collocation method achieved a good preci-sion around 79% in English and 82% in Germanbut a very low recall which is 3% in English and1% in German.
The low recall is due to the na-ture of UMLS where many collocations wouldalmost never occur in natural text.
To avoid thisproblem, we combine the contextual features inthe target context with the pre-prepared colloca-tions list to build our classifier.3 The Classifier With Topical Contex-tual and Local Collocation Features3.1 The Feature SetAs stated early, an important issue is what fea-tures will be used to construct the classifier inWSD.
Early researches have proven that usinglexical statistical information, such as bi-gramco-occurrences was sufficient to produce closeto the best results [10] for Chinese WSD.
In-stead of including bi-gram features as part ofdiscrimination features, in our system, we con-sider both topical contextual features as well aslocal collocation features.
These features areextracted form the 60MB human sense-taggedPeople?s Daily News with segmentation infor-mation.3.1.1 Topical Contextual FeaturesNiu [11] proved in his experiments that Na?veBayes classifier achieved best disambiguationaccuracy with small topical context window size(< 10 words).
We follow their method and setthe contextual window size as 10 in our system.Each of the Chinese words except the stopwords inside the window range will be consid-ered as one topical feature.
Their frequencies arecalculated over the entire corpus with respect toeach sense of an ambiguous word w.  The sensedefinitions are obtained from HowNet.3.1.2 Local Collocation FeaturesWe chose collocations as the local features.
Acollocation is a recurrent and conventional fixedexpression of words which holds syntactic andsemantic relations [21].
Collocations can beclassified as fully fixed collocations, fixed col-locations, strong collocations and loose colloca-tions.
Fixed collocations means the appearanceof one word implies the co-occurrence of an-other one such as ??????
(?burden of his-tory?
), while strong collocations allows verylimited substitution of the components, for ex-ample, ??????
(?local college?
), or ?
?????
(?local university?).
The sense of ambiguouswords can be uniquely determined in these twotypes of collocations, therefore are the colloca-tions applied in our system.
The sources of thecollocations will be explained in Section 4.1.In both Niu [11] and Dang?s [10] work, topicalfeatures as well as the so called collocationalfeatures were used.
However, as discussed inSection 2, they both used bi-gram co-occurrences as the additional local features.However, bi-gram co-occurrences only indicatestatistical significance which may not actuallysatisfy the conceptual definition of collocations.Thus instead of using co-occurrences of bi-grams, we take the true bi-gram collocationsextracted from our system and use this data tocompare with bi-gram co-occurrences to test theusefulness of collocation for WSD.
The localfeatures in our system make use of the colloca-tions using the template (wi, w) within a windowsize of ten (where i = ?
5).
For example, ?????????????
(?Governmentdepartments and local government commandedthat?)
fits the bi-gram collocation template (w,w1) with the value of (????).
During thetraining and the testing processes, the countingof frequency value of the collocation feature willbe increased by 1 if a collocation containing theambiguous word occurs in a sentence.
To have agood analysis on collocation features, we havealso developed an algorithm using lonelyadjacent bi-gram as locals features(named Sys-89adjacent bi-gram as locals features(named Sys-tem A)  and another using collocation as localfeatures(named System B).3.2 The Collocation ClassifierWe consider all the features in the features set F= Ft ?Fl = {f1, f2,  ?
, fm } as independent, whereFt stands for the topical contextual features set,and Fl stands for the local collocation featuresset.
For an ambiguous word w with n senses, letSw = {ws1, ws2,  ?
, wsn } be the sense set.
Forthe contextual features, we directly apply theNa?ve Bayes algorithm using Add-LambdaSmoothing to handle unknown words:)|(log)(log)(1 sijFfsisi wfpwpwscoretj?
?+=(1)For each sense siw of an ambiguous word w:)()()(wfreqwfreqwp sisi =                       (2)For each contextual feature fj respects to eachsense siw of w :),(),()|(siFftsijsij wffreqwffreqwfptt?
?=   (3)To integrate the local collocation feature fj ?
Flwith respect to each sense siw  of w, we use thefollows formula:)()()( 21 sisisi wscorewscorewscore ?+= ?
(4)where ?
is tuned from experiments (Section 4.5),score1( siw ) refers the score of the topical con-textual features based on formula (1) andscore2( siw ) refers the score of collocation fea-tures with respect to the sense sjw  of w definedbelow.?
?=lj Ffsjjsi wfwscore )|()(2 ?
(5)where ?
(fj| sjw ) = 1 for fj ?
Fl if the collocationoccurs in the local context.
Otherwise this termis set as 0.Finally, we choose the right skw so that)(maxarg sks wscores k=        (6)4 Experimental ResultsWe have designed a set of experiments to com-pare the classifier with and without the colloca-tion features.
In system A, the classifier is builtwith local bi-gram features and topical contex-tual features.
The classifier in system B is con-structed from combining the local collocationfeatures with topical features.4.1 Preparation the Data SetWe have selected 20 ambiguous words fromnouns and verbs with the sense number as 4 inaverage.
The sense definition is taken fromHowNet [22].
To show the effect of the algo-rithm, we try to choose words with high degreeof ambiguity, high frequency of use [23], andhigh frequency of constructing collocations.
Theselection of these 20 words is not completelyrandom although within each criterion class wedo try to pick word randomly.Based on the 20 words, we extracted 28,000sentences from the 60 MB People?s Daily Newswith segmentation information as our train-ing/test set which is then manually sense-tagged.The collocation list is constructed from acombination of a digital collocation dictionary, areturn result from a collocation automatic ex-traction system [21], and a hand collection fromthe People?s Daily News.
As we stated early, thesense of ambiguous words in the fixed colloca-tions and strong collocations can be decideduniquely although they are not unique in loosecollocations.
For example, the ambiguous word????
in the collocation ???????
mayhave both the sense of ?appearance|???
or?reputation|???.
Therefore, when labeling thesense of collocations, we filter out the oneswhich cannot uniquely determine the sense ofambiguous words inside.
However, this does notmean that loose collocations have no contribu-tion in WSD classification.
We simply reduce itsweight when combining it with the contextualfeatures compared with the fixed and strong col-locations.
The sense and collocation distributionover the 20 words on the training examples canbe found in Table 1.Table 1.
Sense and Collocation Distribution of the 20 tar-get words in the training corpusAm.WT# S1co#S2co#S3co#S4co#S5co#S6co#90??
31 1  13010 NA?
? 499 479  324180 0 0NA?
? 944 908  1291117101800 NA?
? 409 3  2389171170 NA?
? 110 3  01013669 NA?
? 41 3  037610 NA?
? 4885 26  0340720449213562611 NA?
? 3508 7  048431941448259194NA?
? 348 312  1172211144 NA?
? 4438 3983 72133101233715312310223445?
? 1987 1712 72327410 NA?
? 83 36  14474 00 NA?
? 995 168  108827513 NA?
? 31 11  32011 NA?
? 2725 227 1772498491024241898201NA?
? 592 1  020863367124 16 1NA?
? 1155 756  571399135 NA?
? 2792 691  98176511333629 0NA?
? 2460 82  6336111231474877103NA?
? 125 11  064015332430 NAT#: total number of sentences contain the ambiguous words1- s6: sense no; co#: number of collocations in each sense4.2 The Effect of Collocation FeaturesWe recorded 6 trials with average precision oversix-fold validation for each word.
Their averageprecision for the six trials in the system A, and Bcan be found in Table 2 and Table 3.
From Ta-ble 3, regarding to precision, there are 16 wordshave improved and 4 words remained the samein the system B.
The results from the both sys-tem confirmed that collocation features do im-prove the precision.
Note that 4 words have thesame precision in the two systems, which fallinto two cases.
In the first case, it can be seenthat these words already have very high preci-sion in the system A (over 93%) which meansthat one sense dominates all other senses.
In thiscase, the additional collation information is notnecessary.
In fact, when we checked the inter-mediate outputs, the score of the candidatesenses of the ambiguous words contained in thecollocations get improved.
Even though, itwould not change the result.
Secondly, no collo-cation appeared in the sentences which aretagged incorrectly in the system A.
This is con-firmed when we check the error files.
For exam-ple, the word ????
with the sense as ????(?closeness?)
appeared in 4492 examples overthe total 4885 examples (91.9%).
In the meantime, 99% of collocation in its collocation listhas the same sense of ???
?
(?closeness?
).Only one collocation ?????
has the sense of???
?
(?power?).
Therefore, the collocationfeatures improved the score of sense ???
?which is already the highest one based on thecontextual features.As can be seen from Table 3, the collocationfeatures work well for the sparse data.
For ex-ample, the word ????
in the training corpushas only one example with the sense ???
(?hu-man?
), the other 30 examples all have the sense????
(?management?).
Under this situation,the topical contextual features failed to identifythe right sense for the only appearance of thesense ???
(?human?)
in the training instance??????????????????.
How-ever, it can be correctly identified in the systemB because the appearance of the collocation ???????
?.To well show the effect of collocations onthe accuracy of classifier for the task of WSD,we also tested both systems on SENSEVAL-3data set, and the result is recorded in the Table 4.From the difference in the relative improvementof both data sets, we can see that collocationfeatures work well when the statistical model isnot sufficiently built up such as from a smallcorpus like SENSEVAL-3.
Actually, in this case,the training examples appear in the corpus onlyonce or twice so that the parameters for suchsparse training examples may not be accurate toforecast the test examples, which convinces usthat collocation features are effective on han-dling sparse training data even for unknownwords.
Fig.
1 shows the precision comparison inthe system A, and B on SENVESAL-3.Table 2.
Average Precision (5/6 training, 1/6 test) ofsystem A on People?s Daily NewsAmb.W T1 T2 T3 T4 T5 T6Ave.Prec.?
? 1.00 1.00 1.00 1.00 1.00 .83 .972?
? .90 .97 1.00 1.00 .97 .98 .972?
? .97 .96 .96 .92 .98 .96 .95891?
? .94 .94 .97 .92 .97 .97 .951?
? 1.00 1.00 .77 .94 .88 1.00 .932?
? .83 1.00 1.00 1.00 .83 .90 .927?
? .93 .95 .91 .92 .92 .92 .925?
? .93 .94 .89 .91 .89 .90 .91?
? .94 .93 .86 .93 .89 .87 .903?
? .83 .94 .89 .90 .88 .94 .897?
? .86 .88 .92 .84 .82 .87 .865?
? .92 .84 .92 .76 .84 .72 .833?
? .84 .83 .88 .82 .88 .71 .827?
? .80 .60 .80 .20 1.00 1.00 .733?
? .68 .72 .67 .77 .70 .68 .703?
? .51 .67 .47 .60 .68 .59 .586?
? .70 .63 .66 .64 .64 .64 .652??
.57 .74 .55 .64 .72 .67 .648?
? .65 .58 .66 .64 .54 .47 .58?
? .55 .50 .45 .45 .45 .64 .507Total Average Precision 0.815Table 3.
Average Precision (5/6 training, 1/6 test) ofsystem B on People?s Daily NewsAmb.W T1 T2 T3 T4 T5 T6Ave.Prec.??
1.00 1.00 1.00 1.00 1.00 1.00 1.00?
? .90 .97 1.00 1.00 .97 .98 .970?
? .96 .98 .97 .96 .98 .96 .968?
? .94 .94 .97 .94 .97 .98 .957?
? 1.00 1.00 .77 .94 .88 1.00 .931?
? .83 1.00 1.00 1.00 .83 .90 .927?
? .93 .95 .91 .92 .92 .92 .925?
? .92 .95 .92 .92 .91 .91 .922?
? .94 .94 .86 .93 .91 .87 .908?
? .80 .95 .89 .93 .89 .94 .902?
? .87 .88 .92 .84 .83 .91 .875?
? .84 1.00 .92 .76 .84 .77 .855?
? .88 .86 .89 .84 .90 .74 .852?
? 1.00 .80 .80 .20 1.00 1.00 .800?
? .69 .72 .68 .79 .75 .72 .725?
? .69 .76 .73 .74 .82 .79 .755?
? .58 .59 .70 .67 .64 .59 .628?
? .68 .67 .66 .63 .65 .63 .653?
? .65 .68 .71 .61 .70 .69 .673?
? .60 .55 .54 .54 .54 .64 .568Total Average Precision 0.840Table 4.
Average Precision of System A & B onSENSEVAL-3 Data SetAmb.WordTotalSAve.
Prec.
inSys AAve.
Prec.in Sys B?
? 48 .207 .290?
? 20 .742 .742? 49 .165 .325? 25 .325 .325?
? 36 .260 .373?
? 30 .167 .267?
? 30 .192 .392?
? 36 .635 .635? 57 .238 .275?
? 36 .327 .385?
? 31 .100 .322? 40 .358 .442?
? 40 .308 .308? 76 .110 .123? 28 .308 .475?667.
500.
30 ?
? 42 .165 .260? 57 .037 .422?
? 28 .833 .103Total Ave.Precision .276 .376Fig.
1.
The precision comparison in system A, and B basedon SENSEVAL-34.3 The Effect of Collocations on the Sizeof Training Corpus NeededHwee [21] stated that a large-scale, humansense-tagged corpus is critical for a supervisedlearning approach to achieve broad coverageand high accuracy WSD.
He conducted a thor-ough study on the effect of training examples onthe accuracy of supervised corpus based WSD.As the result showed, WSD accuracy continuesto climb as the number of training examples in-creases.
Similarly, we have tested the system A,and B with the different size of training corpusbased on the PDN corpus we prepared.
Our ex-periment results shown in Fig 2 follow the samefact.
The purpose we did the testing is that wehope to disclose the effect of collocations on thesize of training corpus needed.
From Fig 2, wecan see by using the collocation features, theprecision of the system B has increased sloweralong with the growth of training examples thanthe precision of the system A.
The result is rea-sonable because with collocation feature, thestatistical contextual information over the entirecorpus becomes side effect.
Actually, as can beseen from Fig.
2, after using collocation features92in the system B, even we use 1/6 corpus as train-ing, the precision is still higher than we use 5/6train corpus in the system A.Fig.
2.
The precision variation respect to the size of   train-ing corpus in system A, and B based on PDN corpus4.4 Investigation of Sense Distribution onthe Effect of Collocation FeaturesTo investigate the sense distribution on the ef-fect of collocation features, we selected the am-biguous words with the number of sense variedfrom 2 to 6.
In each level of the sense number,the words are selected randomly.
Table 5 showsthe effect of sense distribution on the effect ofcollocation features.
From the table, we can seethat the collocation features work well when thesense distribution is even for a particular am-biguous word under which case the classifiermay get confused.Table 5.
The Effect of Sense Distribution on the Effect ofcollocation FeaturesAmb.wordPrec.WihtoutcollPrec.WithcollSense#SenseDistri.?
? .972 1 2 97% *?
? .97 .97 4 96% *?
? .957 .968 5 96% *?
? .951 .957 3 95% *?
? .931 .931 3 92% *?
? .927 .927 3 90% *?
? .925 .925 5 92% *?
? .915 .922 4 91% *?
? .903 .908 3 90% *?
? .902 .902 6 90% *?
? .865 .875 2 86% o?
? .833 .855 3 ^?
? .823 .852 2 83% o?
? .733 .8 2 ^?
? .706 .725 4 ^??
.65 .653 4 ^?
? .618 .755 4 ^?
? .582 .628 2 ^?
? .563 .673 4 ^?
? .507 .568 5 ^*: over 90% samples fall in one dominate sense^: Even distribution over all senseso: 83% to 86% samples fall in one dominate sense4.5 The Test of ?We have conducted a set of experiments basedon both the PDN corpus and SENSEVLA-3 datato set the best value of ?
for the formula (4) de-scribed in Section 3.2.
The best start value of ?is tested based on the precision rate which isshown in Fig.
3.
It is shown from the experimentthat ?
takes the start value of 0.5 for both cor-puses.Fig.
3.
The best value of ?
vs the precision rate5 Conclusion and the Future WorkThis paper reports a corpus-based Word SenseDisambiguation approach for Chinese word us-ing local collocation features and topical contex-tual features.
Compared with the base-linesystems in which a Na?ve Bayes classifier isconstructed by combining the contextual fea-tures with the bi-gram features, the new systemachieves 3% precision improvement in averagein Peoples?
Daily News corpus and 10% im-provement in SENSEVAL-3 data set.
Actually,it works very well when disambiguating thesense with sparse distribution over the entirecorpus under which the statistic calculationprone to identify it incorrectly.
In the same time,because disambiguating using collocation fea-93tures does not need statistical calculation, itmakes contribution to reduce the size of humantagged corpus needed which is critical and timeconsuming in corpus based approach.Because different types of collocations mayplay different roles in classifying the sense of anambiguous word, we hope to extend this workby integrating collocations with different weightbased on their types in the future, which mayneed a pre-processing job to categorize the col-locations automatically.6 AcknowledgementsWe would like to present our thanks to the IRLaboratory in HIT University of China for shar-ing their sense number definition automaticallyextracted from HowNet with us.References1.
Hwee Tou Ng, Bin Wang, Yee Seng Chan.
ExploitingParallel Texts for Word Sense Disambiguation.
ACL-03(2003)2.
Black E.: An experiment in computational discrimina-tion of English word senses.
IBM Journal of Researchand Development, v.32, n.2, (1988) 185-1943.
Gale, W. A., Church, K. W. and Yarowsky, D.: Amethod for disambiguating word senses in a large cor-pus.
Computers and the Humanities, v.26, (1993) 415-4394.
Leacock, C., Towell, G. and Voorhees, E. M.: Corpus-based statistical sense resolution.
In Proceedings of theARPA Human Languages Technology Workshop (1993)5.
Leacock, C., Chodorow, M., & Miller G.
A..Using Cor-pus Statistics and WordNet Relations for Sense Identifi-cation.
Computational Linguistics, 24:1, (1998) 147?1656.
Sch?tze, H.: Automatic word sense discrimination.Computational Linguistics, v.24, n.1, (1998) 97-1247.
Towell, G. and Voorhees, E. M.: Disambiguating highlyambiguous words.
Computational Linguistics, v.24, n.1,(1998) 125-1468.
Yarowsky, D.: Decision lists for lexical ambiguity reso-lution: Application to accent restoration in Spanish andFrench.
In Proceedings of the Annual Meeting of theAssociation for Computational Linguistics, (1994) 88-959.
Yarowsky, D.: Unsupervised word sense disambiguationrivaling supervised methods.
In Proceedings of the An-nual Meeting of the Association for Computational Lin-guistics, (1995)189-19610.
Dang, H. T., Chia, C. Y., Palmer M., & Chiou, F.D.,Simple Features for Chinese Word Sense Disambigua-tion.
In Proc.
of COLING (2002)11.
Zheng-Yu Niu, Dong-Hong Ji, Chew Lim Tan, Opti-mizing Feature Set for Chinese Word Sense Disam-biguation.
To appear in Proceedings of the 3rdInternational Workshop on the Evaluation of Systemsfor the Semantic Analysis of Text (SENSEVAL-3).Barcelona, Spain (2004)12.
Chen, Jen Nan and Jason S. Chang, A Concept-basedAdaptive Approach to Word SenseDisambiguation,Proceedings of 36th Annual Meeting of the Associationfor Computational Linguistics and 17th InternationalConference on Computational linguistics.COLING/ACL-98 (1998) 237-24313.
Rigau, G., J. Atserias and E. Agirre, Combining Unsu-pervised Lexical Knowledge Methods for Word SenseDisambiguation, Proceedings of joint 35th AnnualMeeting of the Association for Computational Linguis-tics and 8th Conference of the European Chapter of theAssociation for Computational Linguistics(ACL/EACL?97), Madrid, Spain (1997)14.
Jong-Hoon Oh, and Key-Sun Choi, C02-1098.
: WordSense Disambiguation using Static and Dynamic SenseVectors.
COLING (2002)15.
Yarowsky, D., Hierarchical Decision Lists for WordSense Disambiguation.
Computers and the Humanities,34(1-2), (2000) 179?18616.
Agirre, E. and G. Rigau (1996) Word Sense Disam-biguation using Conceptual Density, Proceedings of16th International Conference on Computational Lin-guistics.
Copenhagen, Denmark, COLING (1996)17.
Escudero, G., L. M?rquez and G. Rigau, Boosting Ap-plied to Word Sense Disambiguation.
Proceedings ofthe 11th European Conference on Machine Learning(ECML 2000) Barcelona, Spain.
2000.
Lecture Notes inArtificial Intelligence 1810.
R. L. de M?ntaras and E.Plaza (Eds.).
Springer Verlag (2000)18.
Gruber, T. R., Subject-Dependent Co-occurrence andWord Sense Disambiguation.
Proceedings of 29th An-nual Meeting of the Association for Computational Lin-guistics (1991)19.
Dominic Widdows, Stanley Peters, Scott Cederberg,Chiu-Ki Chan, Diana Steffen, Paul Buitelaar, Unsuper-vised Monolingual and Bilingual Word-Sense Disam-biguation of Medical Documents using UMLS.Appeared in Natural Language Processing in Biomedi-cine,.
ACL 2003 Workshop, Sapporo, Japan (2003) 9?1620.
Hwee Tou Ng., Getting serious about word sense dis-ambiguation.
In Proceedings of the ACL SIGLEXWorkshop on Tagging Text with Lexical Seman-tics:Why, What, and How?
(1997) 1?721.
Ruifeng Xu , Qin Lu, and Yin Li, An automatic Chi-nese Collocation Extraction Algorithm Based On Lexi-cal Statistics.
In Proceedings of the NLPKE Workshop(2003)22.
D. Dong and Q. Dong, HowNet.http://www.keenage.com, (1991)23.
Chih-Hao Tsai,http://technology.chtsai.org/wordlist/, (1995-2004)24.
Q. Lu, Y. Li, and R. F. Xu, Improving Xtract for Chi-nese Collocation Extraction.
Proceedings of IEEE In-ternational Conference on Natural Language Processingand Knowledge Engineering, Beijing (2003)94
