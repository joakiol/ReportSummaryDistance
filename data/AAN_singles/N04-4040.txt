A Lexically-Driven Algorithm for Disfluency DetectionMatthew Snover, Bonnie DorrInstitute for Advanced Computer Studies1University of MarylandCollege Park, MD 20740snover,bonnie@umiacs.umd.eduRichard SchwartzBBN9861 Broken Land ParkwayColumbia, MD 21046schwartz@bbn.comAbstractThis paper describes a transformation-based learn-ing approach to disfluency detection in speech tran-scripts using primarily lexical features.
Our methodproduces comparable results to two other systemsthat make heavy use of prosodic features, thusdemonstrating that reasonable performance can beachieved without extensive prosodic cues.
In addi-tion, we show that it is possible to facilitate the iden-tification of less frequently disfluent discourse mark-ers by taking speaker style into account.1 IntroductionDisfluencies in human speech are widespread and causeproblems for both downstream processing and humanreadability of speech transcripts.
Recent human studies(Jones et al, 2003) have examined the effect of disfluen-cies on the readability of speech transcripts.
These resultssuggest that the ?cleaning?
of text by removing disfluentwords can increase the speed at which readers can processtext.
Recent work on detecting edits for use in parsingof speech transcripts (Core and Schubert, 1999), (Char-niak and Johnson, 2001) has shown an improvement inthe parser error rate by modeling disfluencies.Many researchers investigating disfluency detectionhave focused on the use of prosodic cues, as opposed tolexical features (Nakatani and Hirschberg, 1994).
Thereare different approaches to detecting disfluencies.
In oneapproach, one can first try to locate evidence of a gen-eral disfluency, e.g., using prosodic features or languagemodel discontinuations.
These locations are called inter-ruption points (IPs).
Following this, it is generally suffi-cient to look in the nearby vicinity of the IP to find the dis-1This work is supported in part by BBNT Con-tract 9500006806 and an NSF CISE Infrastructure AwardEIA0130422.fluent words.
The most successful approaches so far com-bine the detection of IPs using prosodic features and lan-guage modeling techniques (Liu et al, 2003), (Shriberget al, 2001), (Stolcke et al, 1998).Our work is based on the premise that the vast ma-jority of disfluencies can be detected using primarilylexical features?specifically the words themselves andpart-of-speech (POS) labels?without the use of exten-sive prosodic cues.
Lexical modeling of disfluencies withonly minimal acoustic cues has been shown to be suc-cessful in the past using strongly statistical techniques(Heeman and Allen, 1999).
We shall discuss our algo-rithm and compare it to two other algorithms that makeextensive use of acoustic features.
Our algorithm per-forms comparably on most of the tasks assigned and insome cases outperforms systems that used both prosodicand lexical features.We discuss the task definition in Section 2.
In Section3 we describe our Transformation-Based Learning (TBL)algorithm and its associated features.
Section 4 presentsresults for our system and two other systems that makeheavy use of prosodic features to detect disfluencies.
Wethen discuss the errors made by our system, in Section 5,and discuss our conclusions and future work in Section 6.2 EARS Disfluency AnnotationOne of the major goals of the DARPA program forEffective, Affordable, Reusable Speech-to-Text (EARS)(Wayne, 2003) is to provide a rich transcription of speechrecognition output, including speaker identification, sen-tence boundary detection and the annotation of disfluen-cies in the transcript (This collection of additional fea-tures is also known as Metadata).
One result of this pro-gram has been production of an annotation specificationfor disfluencies in speech transcripts and the transcriptionof sizable amounts of speech data, both from conversa-tional telephone speech and broadcast news, according tothis specification (Strassel, 2003).The task of disfluency detection is to distinguish flu-ent from disfluent words.
The EARS MDE (MetaDataExtraction) program addresses two types of disfluencies:(i) edits?words that were not intended to be said andthat are normally replaced with the intended words, suchas repeats, restarts, and revisions; and (ii) fillers?wordswith no meaning that are used as discourse markers andpauses, such as ?you know?
and ?um?.3 The AlgorithmWe set out to solve the task of disfluency detection usingprimarily lexical features in a system we call System A.This section describes our algorithm, including the set offeatures we use to identify disfluencies.The training data for the system are time aligned refer-ence speech transcripts, with speaker identification, sen-tence boundaries, edits, fillers and interruption points an-notated.
The input for evaluation is a transcript, eithera reference transcript or a speech recognizer output tran-script.
Some of the evaluation data may be marked withsentence boundaries and speaker identification.
The taskis to identify which words in the transcript are fillers, ed-its, or fluent.
The evaluation data was held out, and notavailable for tuning system parameters.The input to System A is a transcript of either con-versational telephone speech (CTS) or broadcast newsspeech (BNEWS).
In all experiments, the system wastrained on reference transcripts, but was tested on bothreference and speech output transcripts.We use a Transformation-Based Learning (TBL)(Brill, 1995) algorithm to induce rules from the trainingdata.
TBL is a technique for learning a set of rules thattransform an initial hypothesis for the purpose of reduc-ing the error rate of the hypothesis.
The set of possi-ble rules is found by expanding rule templates, which aregiven as an input.
The algorithm greedily selects the rulethat reduces the error rate the most, applies it to the data,and then searches for the next rule.
The algorithm haltswhen there are no more rules that can reduce the errorrate by more than the threshold.
The output of the systemis an ordered set of rules, which can then be applied tothe test data to annotate it for disfluencies.We allow one of three tags to be assigned to each word:edit, filler or fluent.
Since only 15% of the words in con-versational speech are disfluent, we begin with the initialhypothesis that all the words in the corpus are fluent.
Thesystem then learns rules to relabel words as edits or fillersin order to reduce the number of errors.
The rules are it-eratively applied to the data from left to right.3.1 Feature SetThe rules learned by the system are conditioned on sev-eral features of each of the words including the lexeme(the word itself), a POS tag for the word, whether theword is followed by a silence and whether the word is ahigh frequency word.
That is, whether the word is morefrequent for this speaker than in the rest of the corpus.The last feature (high frequency of the word) is useful foridentifying when words that are usually fluent?but aresometimes disfluent (such as ?like?
)?are more likely tobe disfluencies, with the intuition being that if a speakeris using the word ?like?
very frequently, then it is likelythat the word is being used as a filler.
The word ?like?for example was only a disfluency 22% of the time it oc-curred.
So a rule that always tags ?like?
as a disfluencywould hurt rather than help the system.23.2 Rule TemplatesThe system was given a set of 33 rule templates, whichwere used to generate the set of possible rules.
Not allrule templates generated rules that were chosen by thesystem.
Below is a representative subset of rule templateschosen by the system.
Change the label of:1. word X from L1 to L2.2.
word sequence X Y to L1.3.
left side of simple repeat to L1.4.
word with POS X from L1 to L2 if followed by word withPOS Y.5.
word from L1 to L2 if followed by words X Y.6.
word X with POS Y from L1 to L2.7.
A to L1 in the pattern A POS X B A, where A and B canbe any words.8.
left side of repeat with POS X in the middle to L1.9.
word with POS X from L1 to L2 if followed by silenceand followed by word with POS Y.10.
word X that is a high frequency word for the speaker fromL1 to L2.4 ResultsAll of the results in this section are from training and eval-uation on data produced by the Linguistic Data Consor-tium (LDC) for the EARS Metadata community.
Therewere 491,543 tokens in the CTS training set and 189,766tokens in the BNEWS training set.
The CTS evaluationset contained 33,670 tokens and the BNEWS evaluationset contained 14,544 tokens.We compare our System A to two other systems thatwere designed for the same task, System B and SystemC.
System C was only applied to conversational speech,so there are no results for it on broadcast news transcripts.Our system was also given the same speech recognitionoutput as System C for the conversational speech condi-tion, whereas System B used transcripts produced by adifferent speech recognition system.2We use a POS tagger (Ratnaparkhi, 1996) trained onswitchboard data with the additional tags of FP (filled pause)and FRAG (word fragment).System B used both prosodic cues and lexical informa-tion to detect disfluencies.
The prosodic cues were mod-eled by a decision tree classifier, whereas the lexical in-formation was modeled using a 4-gram language model,separately trained for both CTS and BNEWS.System C first inserts IPs into the text using a decision-tree classifier based on both prosodic and lexical featuresand then uses TBL.
In addition to POS, System C?s fea-ture set alo includes whether the word is commonly usedas a filler, edit, back-channel word, or is part of a short re-peat.
Turn and segment boundary flags were also used bythe system.
Whereas System A only attempted to learnthree labels (filler, edit and fluent), System C attemptedto learn many subtypes of disfluencies, which were notdistinguished in the evaluation.4.1 Lexeme Error RateWe use Lexeme Error Rate (LER) as a measure of recog-nition effectiveness.
This measure is the same as the tra-ditional word-error rate used in speech recognition, ex-cept that filled pauses and fragments are not optionallydeletable.
The LERs of the speech transcripts used by thethree systems were all fairly similar (about 25% for CTSand 12% for BNEWS).4.2 Top Rules LearnedA total of 106 rules were learned by the system for CTS?the top 10 rules learned are:1.
Label all fluent filled pauses as fillers.2.
Label the left side of a simple repeat as an edit.3.
Label ?you know?
as a filler.4.
Label fluent ?well?s with a UH part-of-speech as a filler.5.
Label fluent fragments as edits.6.
Label ?I mean?
as a filler.7.
Label the left side of a simple repeat separated by a filledpause as an edit.8.
Label the left side of a simple repeat separated by a frag-ment as an edit.9.
Label edit filled pauses as fillers.10.
Label edit fragments at end of sentence as fluent.Of the errors that system was able to fix in the CTS train-ing data, the top 5 rules were responsible for correcting86%, the top ten rules, for 94% and the top twenty, for96%.All systems were evaluated using rteval (Rich Tran-scription Evaluation) version 2.3 (Kubala and Srivastava,2003).
Rteval aligns the system output to the annotatedreference transcripts in such a way as to minimize the lex-eme error rate.
The error rate is the number of disfluencyerrors (insertions and deletions) divided by the number ofdisfluent tokens in the reference transcript.
Edit and fillererrors are calculated separately.
The results of the evalu-ation are shown in Table 1.
Most of the small differencesin the CTS results were not found to be significantly dif-ferent.Data System Edit Err Filler ErrCTS Reference A 68.0% 18.1%B 59.0% 18.2%C 75.1% 23.2%CTS Speech A 87.9% 48.8%B 87.5% 46.9%C 88.5% 51.0%BNews Reference A 45.3% 6.5%B 44.2% 7.9%BNews Speech A 93.9% 57.2%B 96.1% 50.4%Table 1: Disfluency Detection Results5 Error AnalysisIt is clear from the discrepancies between the referenceand speech condition that a large portion of the errors (amajority except in the case of edit detection for CTS) aredue to errors in the STT (Speech-To-Text).
This is mostnotable for fillers in broadcast news where the error ratefor our system increases from 6.5% to 57.2%.
Such atrend can be seen for the other systems, indicating that?even with prosodic models?the other systems were notmore robust to the lexical errors.All three systems produced comparable results on allof the conditions, with the only large exception being editdetection for CTS Reference, where System B had an er-ror rate of 59% compared to our system?s error rate of68%.3The speech output condition suffers from several typesof errors due to errors in the transcript produced by thespeech transcription system.
First, the system can outputthe wrong word causing it to be misannotated.
27% of ouredit errors in CTS and 19% of our filler errors occurredwhen the STT system misrecognized the word.
If a filledpause is hallucinated, the disfluency detection system willalways annotate it as a filler.
Errors also occur (19% ofour edit and 12% of our filler error) when the recognizerdeletes a word that was an edit or a filler.
Finally, errorsin the context words surrounding disfluencies can affectdisfluency detection as well.One possible method to correct for the STT errorswould be to train our system on speech output from therecognizer rather than on reference transcripts.
Anotheroption would be to use a word recognition confidencescore from the recognizer as a feature in the TBL sys-tem; these were not used.
A more systematic analysisof the errors caused by the recognizer and their effect ondisfluencies also needs to be performed.System A has a much higher error for edits than fillers,due, in large part, to the presence of long, difficult to3This is possibly due to the prosodic model employed bySystem B, though no significant gain was shown for the otherconditions.detect edits.
Consider the following word sequence: ?
[and whenever they come out with a warning ] you knowthey were coming out with a warning about trains ?.
Theportion within square brackets is the edit to be detected.The difficulty in finding such regions is that the edit itselfappears very fluent.
One can identify these regions byexamining what comes after the edit and finding that ishighly similar in content to the edit region.
Prosodic fea-tures can be useful in identifying the interruption pointat which the edit ends, but the degree to which the editextends backwards from this point still needs to be iden-tified.
Long distance dependencies should reveal the editregion, and it is possible that parsing or semantic analysisof the text would be a useful technique to employ.
In ad-dition there are other cues such as the filler ?you know?after the edit which can be used to locate these edit re-gions.
Long edit regions (of length four or more) are re-sponsible for 48% of the edit errors in the CTS referencecondition for our system.6 Conclusions and Future WorkWe have presented a TBL approach to detecting disfluen-cies that uses primarily lexical features.
Our system per-formed comparably with other systems that relied on bothprosodic and lexical features.
Our speaker style (high fre-quency word) feature enabled us to detect rarer disfluen-cies, although this was not a large factor in our perfor-mance.
It does appear to be a promising technique forfuture research however.The technique described here shows promise for ex-tension to disfluency detection in other languages.
SinceTBL is a weakly statistical technique, it does not requirea large training corpus and could be more rapidly appliedto new languages.
Assuming the basic forms of disflu-encies in other languages are similar to those in English,very few modifications would be required.The longer edits that the system currently misses maybe detectable using parsing, with the intuition that aparser trained on fluent speech may perform poorly in thepresence of longer edits.
Techniques using parse trees toidentify disfluencies have shown success in the past (Hin-dle, 1983).
The system could use portions of the parsestructure as features and could relabel entire subtrees ofthe parse tree.
Repeated words are another feature of thelonger edits, which we might leverage off of by perform-ing a weighted alignment of the edit and the repair.
Even-tually it may prove that more elaborate acoustic cues willbe needed to identify these edits, at which point a modelof interruption points could be included as a feature in therules learned by the system.ReferencesEric Brill.
1995.
Transformation-based error-driven learningand natural language processing: A case study in part-of-speech tagging.
Computational Linguistics, 21(4):543?565.Eugene Charniak and Mark Johnson.
2001.
Edit detectionand parsing for transcribed speech.
In Proceedings of theNAACL.Mark G. Core and Lenhart K. Schubert.
1999.
A model ofspeech repairs and other disruptions.
In Susan E. Brennan,Alain Giboin, and David Traum, editors, Working Papers ofthe AAAI Fall Symposium on Psychological Models of Com-munication in Collaborative Systems, pages 48?53, MenloPark, California.
AAAI.Peter Heeman and James Allen.
1999.
Speech repairs, into-national phrases, and discourse marker: Modeling speakers?utterances in spoken dialogue.
Computational Linguistics,25(4).Donald Hindle.
1983.
Deterministic parsing of syntactic non-fluencies.
In Proceedings of ACL, pages 123?128.Douglas Jones, Florian Wolf, Edward Gibson, Elliott Williams,Evelina Fedorenko, Douglas Reynolds, and Marc Zissman.2003.
Measuring the readability of automatic speech-to-texttranscripts.
In Proceedings of Eurospeech, Geneva.Francis Kubala and Amit Srivastava.
2003.
A Framework forEvaluating Rich Transcription Technology.
BBN Ears Web-site.
http://www.speech.bbn.com/ears.Yang Liu, Elizabeth Shriberg, and Andreas Stolcke.
2003.Automatic disfluency identification in coversational speechusing multiple knowledge sources.
In Proceedings of Eu-rospeech, Geneva.Christine Nakatani and Julia Hirschberg.
1994.
A corpus-basedstudy of repair cue in spontaneous speech.
Journal of theAcoustical Society of America, 95(3):160?1616.Adwait Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In ACL-SIGDAT Proceedings of theConference on Empirical Methods in Natural Language Pro-cessing, pages 133?142, Philadelphia, PA.Elizabeth Shriberg, Andreas Stolcke, and Dan Baron.
2001.Can prosody aid the automatic processing of multi-partymeetings?
evidence from predicting punctuation, disfluen-cies, and overlapping speech.
In Proceedings of ISCA Tuto-rial and Research Workshop on Prosody in Speech Recogni-tion and Understanding, pages 139?146, Red Bank, NJ.Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates, Mari Os-tendorf, Dilek Hakkani, Madelaine Plauche, Gokhan Tur,and Yu Lu.
1998.
Automatic detection of sentence bound-aries and disfluencies based on recognized words.
In Pro-ceedings of the ICSLP, volume 5, pages 2247?2250, Sydney,Australia.Stephanie Strassel.
2003.
Guidelines for RT-03 Transcription?
Version 2.2.
Linguistic Data Consortium, Universitry ofPennsylvannia.Charles Wayne.
2003.
Effective, Affordable, Reusable Speech-to-Text (EARS).
Official web site for DARPA/EARS Pro-gram.
http://www.darpa.muk/iao/EARS.htm.
