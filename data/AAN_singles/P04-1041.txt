Long-Distance Dependency Resolution in Automatically AcquiredWide-Coverage PCFG-Based LFG ApproximationsAoife Cahill, Michael Burke, Ruth O?Donovan, Josef van Genabith, Andy WayNational Centre for Language Technology and School of Computing,Dublin City University, Dublin, Ireland{acahill,mburke,rodonovan,josef,away}@computing.dcu.ieAbstractThis paper shows how finite approximations oflong distance dependency (LDD) resolution can beobtained automatically for wide-coverage, robust,probabilistic Lexical-Functional Grammar (LFG)resources acquired from treebanks.
We extract LFGsubcategorisation frames and paths linking LDDreentrancies from f-structures generated automati-cally for the Penn-II treebank trees and use themin an LDD resolution algorithm to parse new text.Unlike (Collins, 1999; Johnson, 2002), in our ap-proach resolution of LDDs is done at f-structure(attribute-value structure representations of basicpredicate-argument or dependency structure) with-out empty productions, traces and coindexation inCFG parse trees.
Currently our best automaticallyinduced grammars achieve 80.97% f-score for f-structures parsing section 23 of the WSJ part of thePenn-II treebank and evaluating against the DCU1051 and 80.24% against the PARC 700 Depen-dency Bank (King et al, 2003), performing at thesame or a slightly better level than state-of-the-arthand-crafted grammars (Kaplan et al, 2004).1 IntroductionThe determination of syntactic structure is an im-portant step in natural language processing as syn-tactic structure strongly determines semantic inter-pretation in the form of predicate-argument struc-ture, dependency relations or logical form.
For asubstantial number of linguistic phenomena suchas topicalisation, wh-movement in relative clausesand interrogative sentences, however, there is an im-portant difference between the location of the (sur-face) realisation of linguistic material and the loca-tion where this material should be interpreted se-mantically.
Resolution of such long-distance de-pendencies (LDDs) is therefore crucial in the de-termination of accurate predicate-argument struc-1Manually constructed f-structures for 105 randomly se-lected trees from Section 23 of the WSJ section of the Penn-IITreebankture, deep dependency relations and the construc-tion of proper meaning representations such as log-ical forms (Johnson, 2002).Modern unification/constraint-based grammarssuch as LFG or HPSG capture deep linguistic infor-mation including LDDs, predicate-argument struc-ture, or logical form.
Manually scaling rich uni-fication grammars to naturally occurring free text,however, is extremely time-consuming, expensiveand requires considerable linguistic and computa-tional expertise.
Few hand-crafted, deep unificationgrammars have in fact achieved the coverage androbustness required to parse a corpus of say the sizeand complexity of the Penn treebank: (Riezler etal., 2002) show how a deep, carefully hand-craftedLFG is successfully scaled to parse the Penn-II tree-bank (Marcus et al, 1994) with discriminative (log-linear) parameter estimation techniques.The last 20 years have seen continuously increas-ing efforts in the construction of parse-annotatedcorpora.
Substantial treebanks2 are now availablefor many languages (including English, Japanese,Chinese, German, French, Czech, Turkish), othersare currently under construction (Arabic, Bulgarian)or near completion (Spanish, Catalan).
Treebankshave been enormously influential in the develop-ment of robust, state-of-the-art parsing technology:grammars (or grammatical information) automat-ically extracted from treebank resources providethe backbone of many state-of-the-art probabilis-tic parsing approaches (Charniak, 1996; Collins,1999; Charniak, 1999; Hockenmaier, 2003; Kleinand Manning, 2003).
Such approaches are attrac-tive as they achieve robustness, coverage and per-formance while incurring very low grammar devel-opment cost.
However, with few notable exceptions(e.g.
Collins?
Model 3, (Johnson, 2002), (Hocken-maier, 2003) ), treebank-based probabilistic parsersreturn fairly simple ?surfacey?
CFG trees, with-out deep syntactic or semantic information.
Thegrammars used by such systems are sometimes re-2Or dependency banks.ferred to as ?half?
(or ?shallow?)
grammars (John-son, 2002), i.e.
they do not resolve LDDs but inter-pret linguistic material purely locally where it oc-curs in the tree.Recently (Cahill et al, 2002) showed howwide-coverage, probabilistic unification grammarresources can be acquired automatically from f-structure-annotated treebanks.
Many second gen-eration treebanks provide a certain amount ofdeep syntactic or dependency information (e.g.
inthe form of Penn-II functional tags and traces)supporting the computation of representations ofdeep linguistic information.
Exploiting this in-formation (Cahill et al, 2002) implement anautomatic LFG f-structure annotation algorithmthat associates nodes in treebank trees with f-structure annotations in the form of attribute-valuestructure equations representing abstract predicate-argument structure/dependency relations.
From thef-structure annotated treebank they automaticallyextract wide-coverage, robust, PCFG-based LFGapproximations that parse new text into trees andf-structure representations.The LFG approximations of (Cahill et al, 2002),however, are only ?half?
grammars, i.e.
like mostof their probabilistic CFG cousins (Charniak, 1996;Johnson, 1999; Klein and Manning, 2003) they donot resolve LDDs but interpret linguistic materialpurely locally where it occurs in the tree.In this paper we show how finite approxima-tions of long distance dependency resolution can beobtained automatically for wide-coverage, robust,probabilistic LFG resources automatically acquiredfrom treebanks.
We extract LFG subcategorisationframes and paths linking LDD reentrancies fromf-structures generated automatically for the Penn-II treebank trees and use them in an LDD resolu-tion algorithm to parse new text.
Unlike (Collins,1999; Johnson, 2002), in our approach LDDs areresolved on the level of f-structure representation,rather than in terms of empty productions and co-indexation on parse trees.
Currently we achieve f-structure/dependency f-scores of 80.24 and 80.97for parsing section 23 of the WSJ part of the Penn-II treebank, evaluating against the PARC 700 andDCU 105 respectively.The paper is structured as follows: we give abrief introduction to LFG.
We outline the automaticf-structure annotation algorithm, PCFG-based LFGgrammar approximations and parsing architecturesof (Cahill et al, 2002).
We present our subcategori-sation frame extraction and introduce the treebank-based acquisition of finite approximations of LFGfunctional uncertainty equations in terms of LDDpaths.
We present the f-structure LDD resolutionalgorithm, provide results and extensive evaluation.We compare our method with previous work.
Fi-nally, we conclude.2 Lexical Functional Grammar (LFG)Lexical-Functional Grammar (Kaplan and Bres-nan, 1982; Dalrymple, 2001) minimally involvestwo levels of syntactic representation:3 c-structureand f-structure.
C(onstituent)-structure representsthe grouping of words and phrases into largerconstituents and is realised in terms of a CF-PSG grammar.
F(unctional)-structure representsabstract syntactic functions such as SUBJ(ect),OBJ(ect), OBL(ique), closed and open clausalCOMP/XCOMP(lement), ADJ(unct), APP(osition)etc.
and is implemented in terms of recursive featurestructures (attribute-value matrices).
C-structurecaptures surface grammatical configurations, f-structure encodes abstract syntactic informationapproximating to predicate-argument/dependencystructure or simple logical form (van Genabithand Crouch, 1996).
C- and f-structures are re-lated in terms of functional annotations (constraints,attribute-value equations) on c-structure rules (cf.Figure 1).SNP VPU.N.
V NPsigns treaty[SUBJ[PRED U.N.]PRED signOBJ[PRED treaty]]S ?
NP VP?SUBJ=?
?=?VP ?
V NP?=?
?OBJ=?NP ?
U.N V ?
signs?PRED=U.N.
?PRED=signFigure 1: Simple LFG C- and F-StructureUparrows point to the f-structure associated with themother node, downarrows to that of the local node.The equations are collected with arrows instanti-ated to unique tree node identifiers, and a constraintsolver generates an f-structure.3 Automatic F-Structure AnnotationThe Penn-II treebank employs CFG trees with addi-tional ?functional?
node annotations (such as -LOC,-TMP, -SBJ, -LGS, .
.
. )
as well as traces and coin-dexation (to indicate LDDs) as basic data structures.The f-structure annotation algorithm of (Cahill et3LFGs may also involve morphological and semantic levelsof representation.al., 2002) exploits configurational, categorial, Penn-II ?functional?, local head and trace informationto annotate nodes with LFG feature-structure equa-tions.
A slightly adapted version of (Magerman,1994)?s scheme automatically head-lexicalises thePenn-II trees.
This partitions local subtrees of depthone (corresponding to CFG rules) into left and rightcontexts (relative to head).
The annotation algo-rithm is modular with four components (Figure 2):left-right (L-R) annotation principles (e.g.
leftmostNP to right of V head of VP type rule is likely to bean object etc.
); coordination annotation principles(separating these out simplifies other componentsof the algorithm); traces (translates traces and coin-dexation in trees into corresponding reentrancies inf-structure ( 1 in Figure 3)); catch all and clean-up.Lexical information is provided via macros for POStag classes.L/R Context ?
Coordination ?
Traces ?
Catch-AllFigure 2: Annotation AlgorithmThe f-structure annotations are passed to a con-straint solver to produce f-structures.
Annotationis evaluated in terms of coverage and quality, sum-marised in Table 1.
Coverage is near complete with99.82% of the 48K Penn-II sentences receiving asingle, connected f-structure.
Annotation quality ismeasured in terms of precision and recall (P&R)against the DCU 105.
The algorithm achieves anF-score of 96.57% for full f-structures and 94.3%for preds-only f-structures.4SS-TPC- 1NPU.N.VPVsignsNPtreatyNPDettheNheadlineVPVsaidST- 1??????
?TOPIC[SUBJ[PRED U.N.]PRED signOBJ[PRED treaty]]1SUBJ[SPEC thePRED headline]PRED sayCOMP 1??????
?Figure 3: Penn-II style tree with LDD trace and cor-responding reentrancy in f-structure4Full f-structures measure all attribute-value pairs includ-ing?minor?
features such as person, number etc.
The stricterpreds-only captures only paths ending in PRED:VALUE.# frags # sent percent0 85 0.1761 48337 99.8202 2 0.004all predsP 96.52 94.45R 96.63 94.16Table 1: F-structure annotation results for DCU 1054 PCFG-Based LFG ApproximationsBased on these resources (Cahill et al, 2002) de-veloped two parsing architectures.
Both generatePCFG-based approximations of LFG grammars.In the pipeline architecture a standard PCFG isextracted from the ?raw?
treebank to parse unseentext.
The resulting parse-trees are then annotated bythe automatic f-structure annotation algorithm andresolved into f-structures.In the integrated architecture the treebankis first annotated with f-structure equations.An annotated PCFG is then extracted whereeach non-terminal symbol in the grammarhas been augmented with LFG f-equations:NP[?OBJ=?]
?
DT[?SPEC=?]
NN[?=?]
.
Nodesfollowed by annotations are treated as a monadiccategory for grammar extraction and parsing.Post-parsing, equations are collected from parsetrees and resolved into f-structures.Both architectures parse raw text into ?proto?
f-structures with LDDs unresolved resulting in in-complete argument structures as in Figure 4.SSNPU.N.VPVsignsNPtreatyNPDettheNheadlineVPVsaid????
?TOPIC[SUBJ[PRED U.N.]PRED signOBJ[PRED treaty]]SUBJ[SPEC thePRED headline]PRED say????
?Figure 4: Shallow-Parser Output with UnresolvedLDD and Incomplete Argument Structure (cf.
Fig-ure 3)5 LDDs and LFG FU-EquationsTheoretically, LDDs can span unbounded amountsof intervening linguistic material as in[U.N. signs treaty]1 the paper claimed .
.
.
a source said []1.In LFG, LDDs are resolved at the f-structure level,obviating the need for empty productions and tracesin trees (Dalrymple, 2001), using functional uncer-tainty (FU) equations.
FUs are regular expressionsspecifying paths in f-structure between a source(where linguistic material is encountered) and a tar-get (where linguistic material is interpreted seman-tically).
To account for the fronted sentential con-stituents in Figures 3 and 4, an FU equation of theform ?
TOPIC = ?
COMP* COMP would be required.The equation states that the value of the TOPIC at-tribute is token identical with the value of the finalCOMP argument along a path through the immedi-ately enclosing f-structure along zero or more COMPattributes.
This FU equation is annotated to the top-icalised sentential constituent in the relevant CFGrules as followsS ?
S NP VP?TOPIC=?
?SUBJ=?
?=?
?TOPIC=?COMP*COMPand generates the LDD-resolved proper f-structurein Figure 3 for the traceless tree in Figure 4, as re-quired.In addition to FU equations, subcategorisation in-formation is a crucial ingredient in LFG?s accountof LDDs.
As an example, for a topicalised con-stituent to be resolved as the argument of a localpredicate as specified by the FU equation, the localpredicate must (i) subcategorise for the argument inquestion and (ii) the argument in question must notbe already filled.
Subcategorisation requirementsare provided lexically in terms of semantic forms(subcat lists) and coherence and completeness con-ditions (all GFs specified must be present, and noothers may be present) on f-structure representa-tions.
Semantic forms specify which grammaticalfunctions (GFs) a predicate requires locally.
For ourexample in Figures 3 and 4, the relevant lexical en-tries are:V ?
said ?PRED=say??
SUBJ, ?
COMP?V ?
signs ?PRED=sign??
SUBJ, ?
OBJ?FU equations and subcategorisation requirementstogether ensure that LDDs can only be resolved atsuitable f-structure locations.6 Acquiring Lexical and LDD ResourcesIn order to model the LFG account of LDD resolu-tion we require subcat frames (i.e.
semantic forms)and LDD resolution paths through f-structure.
Tra-ditionally, such resources were handcoded.
Here weshow how they can be acquired from f-structure an-notated treebank resources.LFG distinguishes between governable (argu-ments) and nongovernable (adjuncts) grammati-cal functions (GFs).
If the automatic f-structureannotation algorithm outlined in Section 3 gen-erates high quality f-structures, reliable seman-tic forms can be extracted (reverse-engineered):for each f-structure generated, for each level ofembedding we determine the local PRED valueand collect the governable, i.e.
subcategoris-able grammatical functions present at that levelof embedding.
For the proper f-structure inFigure 3 we obtain sign([subj,obj]) andsay([subj,comp]).
We extract frames fromthe full WSJ section of the Penn-II Treebank with48K trees.
Unlike many other approaches, our ex-traction process does not predefine frames, fullyreflects LDDs in the source data-structures (cf.Figure 3), discriminates between active and pas-sive frames, computes GF, GF:CFG category pair-as well as CFG category-based subcategorisationframes and associates conditional probabilities withframes.
Given a lemma l and an argument list s, theprobability of s given l is estimated as:P(s|l) := count(l, s)?ni=1 count(l, si)Table 2 summarises the results.
We extract 3586verb lemmas and 10969 unique verbal semanticform types (lemma followed by non-empty argu-ment list).
Including prepositions associated withthe subcategorised OBLs and particles, this numbergoes up to 14348.
The number of unique frametypes (without lemma) is 38 without specific prepo-sitions and particles, 577 with.
F-structure anno-tations allow us to distinguish passive and activeframes.
Table 3 shows the most frequent seman-tic forms for accept.
Passive frames are markedp.
We carried out a comprehensive evaluation ofthe automatically acquired verbal semantic formsagainst the COMLEX Resource (Macleod et al,1994) for the 2992 active verb lemmas that both re-sources have in common.
We report on the evalu-ation of GF-based frames for the full frames withcomplete prepositional and particle infomation.
Weuse relative conditional probability thresholds (1%and 5%) to filter the selection of semantic forms(Table 4).
(O?Donovan et al, 2004) provide a moredetailed description of the extraction and evaluationof semantic forms.Without Prep/Part With Prep/PartLemmas 3586 3586Sem.
Forms 10969 14348Frame Types 38 577Active Frame Types 38 548Passive Frame Types 21 177Table 2: Verb ResultsSemantic Form Occurrences Prob.accept([obj,subj]) 122 0.813accept([subj],p) 9 0.060accept([comp,subj]) 5 0.033accept([subj,obl:as],p) 3 0.020accept([obj,subj,obl:as]) 3 0.020accept([obj,subj,obl:from]) 3 0.020accept([subj]) 2 0.013accept([obj,subj,obl:at]) 1 0.007accept([obj,subj,obl:for]) 1 0.007accept([obj,subj,xcomp]) 1 0.007Table 3: Semantic forms for the verb accept.Threshold 1% Threshold 5%P R F-Score P R F-ScoreExp.
73.7% 22.1% 34.0% 78.0% 18.3% 29.6%Table 4: COMLEX ComparisonWe further acquire finite approximations of FU-equations.
by extracting paths between co-indexedmaterial occurring in the automatically generated f-structures from sections 02-21 of the Penn-II tree-bank.
We extract 26 unique TOPIC, 60 TOPIC-RELand 13 FOCUS path types (with a total of 14,911 to-ken occurrences), each with an associated probabil-ity.
We distinguish between two types of TOPIC-REL paths, those that occur in wh-less constructions,and all other types (c.f Table 5).
Given a path p andan LDD type t (either TOPIC, TOPIC-REL or FO-CUS), the probability of p given t is estimated as:P(p|t) := count(t, p)?ni=1 count(t, pi)In order to get a first measure of how well the ap-proximation models the data, we compute the pathtypes in section 23 not covered by those extractedfrom 02-21: 23/(02-21).
There are 3 such path types(Table 6), each occuring exactly once.
Given thatthe total number of path tokens in section 23 is 949,the finite approximation extracted from 02-23 cov-ers 99.69% of all LDD paths in section 23.7 Resolving LDDs in F-StructureGiven a set of semantic forms s with probabilitiesP(s|l) (where l is a lemma), a set of paths p withP(p|t) (where t is either TOPIC, TOPIC-REL or FO-CUS) and an f-structure f , the core of the algorithmto resolve LDDs recursively traverses f to:find TOPIC|TOPIC-REL|FOCUS:g pair; retrieveTOPIC|TOPIC-REL|FOCUS paths; for each path pwith GF1 : .
.
.
: GFn : GF, traverse f along GF1 : .
.
.
:GFn to sub-f-structure h; retrieve local PRED:l;add GF:g to h iff?
GF is not present at hwh-less TOPIC-REL # wh-less TOPIC-REL #subj 5692 adjunct 1314xcomp:adjunct 610 obj 364xcomp:obj 291 xcomp:xcomp:adjunct 96comp:subj 76 xcomp:subj 67Table 5: Most frequent wh-less TOPIC-REL paths02?21 23 23 /(02?21)TOPIC 26 7 2FOCUS 13 4 0TOPIC-REL 60 22 1Table 6: Number of path types extracted?
h together with GF is locally complete and co-herent with respect to a semantic form s for lrank resolution by P(s|l) ?
P(p|t)The algorithm supports multiple, interacting TOPIC,TOPIC-REL and FOCUS LDDs.
We use P(s|l) ?P(p|t) to rank a solution, depending on how likelythe PRED takes semantic frame s, and how likelythe TOPIC, FOCUS or TOPIC-REL is resolved usingpath p. The algorithm also supports resolution ofLDDs where no overt linguistic material introducesa source TOPIC-REL function (e.g.
in reduced rela-tive clause constructions).
We distinguish betweenpassive and active constructions, using the relevantsemantic frame type when resolving LDDs.8 Experiments and EvaluationWe ran experiments with grammars in both thepipeline and the integrated parsing architectures.The first grammar is a basic PCFG, while A-PCFGincludes the f-structure annotations.
We apply aparent transformation to each grammar (Johnson,1999) to give P-PCFG and PA-PCFG.
We trainon sections 02-21 (grammar, lexical extraction andLDD paths) of the Penn-II Treebank and test on sec-tion 23.
The only pre-processing of the trees that wedo is to remove empty nodes, and remove all Penn-II functional tags in the integrated model.
We evalu-ate the parse trees using evalb.
Following (Riezler etal., 2002), we convert f-structures into dependencytriple format.
Using their software we evaluate thef-structure parser output against:1.
The DCU 105 (Cahill et al, 2002)2.
The full 2,416 f-structures automatically gen-erated by the f-structure annotation algorithmfor the original Penn-II trees, in a CCG-style(Hockenmaier, 2003) evaluation experimentPipeline IntegratedPCFG P-PCFG A-PCFG PA-PCFG2416 Section 23 trees# Parses 2416 2416 2416 2414Lab.
F-Score 75.83 80.80 79.17 81.32Unlab.
F-Score 78.28 82.70 81.49 83.28DCU 105 F-StrsAll GFs F-Score (before LDD resolution) 79.82 79.24 81.12 81.20All GFs F-Score (after LDD resolution) 83.79 84.59 86.30 87.04Preds only F-Score (before LDD resolution) 70.00 71.57 73.45 74.61Preds only F-Score (after LDD resolution) 73.78 77.43 78.76 80.972416 F-StrsAll GFs F-Score (before LDD resolution) 81.98 81.49 83.32 82.78All GFs F-Score (after LDD resolution) 84.16 84.37 86.45 86.00Preds only F-Score (before LDD resolution) 72.00 73.23 75.22 75.10Preds only F-Score (after LDD resolution) 74.07 76.12 78.36 78.40PARC 700 Dependency BankSubset of GFs following (Kaplan et al, 2004) 77.86 80.24 77.68 78.60Table 7: Parser Evaluation3.
A subset of 560 dependency structures of thePARC 700 Dependency Bank following (Ka-plan et al, 2004)The results are given in Table 7.
The parent-transformed grammars perform best in both archi-tectures.
In all cases, there is a marked improve-ment (2.07-6.36%) in the f-structures after LDD res-olution.
We achieve between 73.78% and 80.97%preds-only and 83.79% to 87.04% all GFs f-score,depending on gold-standard.
We achieve between77.68% and 80.24% against the PARC 700 follow-ing the experiments in (Kaplan et al, 2004).
Fordetails on how we map the f-structures producedby our parsers to a format similar to that of thePARC 700 Dependency Bank, see (Burke et al,2004).
Table 8 shows the evaluation result brokendown by individual GF (preds-only) for the inte-grated model PA-PCFG against the DCU 105.
Inorder to measure how many of the LDD reentran-cies in the gold-standard f-structures are capturedcorrectly by our parsers, we developed evaluationsoftware for f-structure LDD reentrancies (similarto Johnson?s (2002) evaluation to capture traces andtheir antecedents in trees).
Table 9 shows the resultswith the integrated model achieving more than 76%correct LDD reentrancies.9 Related Work(Collins, 1999)?s Model 3 is limited to wh-tracesin relative clauses (it doesn?t treat topicalisation,focus etc.).
Johnson?s (2002) work is closest toours in spirit.
Like our approach he provides a fi-nite approximation of LDDs.
Unlike our approach,however, he works with tree fragments in a post-processing approach to add empty nodes and theirDEP.
PRECISION RECALL F-SCOREadjunct 717/903 = 79 717/947 = 76 78app 14/15 = 93 14/19 = 74 82comp 35/43 = 81 35/65 = 54 65coord 109/143 = 76 109/161 = 68 72det 253/264 = 96 253/269 = 94 95focus 1/1 = 100 1/1 = 100 100obj 387/445 = 87 387/461 = 84 85obj2 0/1 = 0 0/2 = 0 0obl 27/56 = 48 27/61 = 44 46obl2 1/3 = 33 1/2 = 50 40obl ag 5/11 = 45 5/12 = 42 43poss 69/73 = 95 69/81 = 85 90quant 40/55 = 73 40/52 = 77 75relmod 26/38 = 68 26/50 = 52 59subj 330/361 = 91 330/414 = 80 85topic 12/12 = 100 12/13 = 92 96topicrel 35/42 = 83 35/52 = 67 74xcomp 139/160 = 87 139/146 = 95 91OVERALL 83.78 78.35 80.97Table 8: Preds-only results of PA-PCFG against theDCU 105antecedents to parse trees, while we present an ap-proach to LDD resolution on the level of f-structure.It seems that the f-structure-based approach is moreabstract (99 LDD path types against approximately9,000 tree-fragment types in (Johnson, 2002)) andfine-grained in its use of lexical information (sub-cat frames).
In contrast to Johnson?s approach, ourLDD resolution algorithm is not biased.
It com-putes all possible complete resolutions and order-ranks them using LDD path and subcat frame prob-abilities.
It is difficult to provide a satisfactory com-parison between the two methods, but we have car-ried out an experiment that compares them at thef-structure level.
We take the output of Charniak?sPipeline IntegratedPCFG P-PCFG A-PCFG PA-PCFGTOPICPrecision (11/14) (12/13) (12/13) (12/12)Recall (11/13) (12/13) (12/13) (12/13)F-Score 0.81 0.92 0.92 0.96FOCUSPrecision (0/1) (0/1) (0/1) (0/1)Recall (0/1) (0/1) (0/1) (0/1)F-Score 0 0 0 0TOPIC-RELPrecision (20/34) (27/36) (34/42) (34/42)Recall (20/52) (27/52) (34/52) (34/52)F-Score 0.47 0.613 0.72 0.72OVERALL 0.54 0.67 0.75 0.76Table 9: LDD Evaluation on the DCU 105Charniak -LDD res.
+LDD res.
(Johnson, 2002)All GFs 80.86 86.65 85.16Preds Only 74.63 80.97 79.75Table 10: Comparison at f-structure level of LDDresolution to (Johnson, 2002) on the DCU 105parser (Charniak, 1999) and, using the pipelinef-structure annotation model, evaluate against theDCU 105, both before and after LDD resolution.Using the software described in (Johnson, 2002) weadd empty nodes to the output of Charniak?s parser,pass these trees to our automatic annotation algo-rithm and evaluate against the DCU 105.
The re-sults are given in Table 10.
Our method of resolv-ing LDDs at f-structure level results in a preds-onlyf-score of 80.97%.
Using (Johnson, 2002)?s methodof adding empty nodes to the parse-trees results inan f-score of 79.75%.
(Hockenmaier, 2003) provides CCG-based mod-els of LDDs.
Some of these involve extensive clean-up of the underlying Penn-II treebank resource priorto grammar extraction.
In contrast, in our approachwe leave the treebank as is and only add (but nevercorrect) annotations.
Earlier HPSG work (Tateisiet al, 1998) is based on independently constructedhand-crafted XTAG resources.
In contrast, we ac-quire our resources from treebanks and achieve sub-stantially wider coverage.Our approach provides wide-coverage, robust,and ?
with the addition of LDD resolution ?
?deep?or ?full?, PCFG-based LFG approximations.
Cru-cially, we do not claim to provide fully adequate sta-tistical models.
It is well known (Abney, 1997) thatPCFG-type approximations to unification grammarscan yield inconsistent probability models due toloss of probability mass: the parser successfully re-turns the highest ranked parse tree but the constraintsolver cannot resolve the f-equations (generated inthe pipeline or ?hidden?
in the integrated model)and the probability mass associated with that tree islost.
This case, however, is surprisingly rare for ourgrammars: only 0.0018% (85 out of 48424) of theoriginal Penn-II trees (without FRAGs) fail to pro-duce an f-structure due to inconsistent annotations(Table 1), and for parsing section 23 with the in-tegrated model (A-PCFG), only 9 sentences do notreceive a parse because no f-structure can be gen-erated for the highest ranked tree (0.4%).
Parsingwith the pipeline model, all sentences receive onecomplete f-structure.
Research on adequate prob-ability models for unification grammars is impor-tant.
(Miyao et al, 2003) present a Penn-II tree-bank based HPSG with log-linear probability mod-els.
They achieve coverage of 50.2% on section23, as against 99% in our approach.
(Riezler etal., 2002; Kaplan et al, 2004) describe how a care-fully hand-crafted LFG is scaled to the full Penn-IItreebank with log-linear based probability models.They achieve 79% coverage (full parse) and 21%fragement/skimmed parses.
By the same measure,full parse coverage is around 99% for our automat-ically acquired PCFG-based LFG approximations.Against the PARC 700, the hand-crafted LFG gram-mar reported in (Kaplan et al, 2004) achieves an f-score of 79.6%.
For the same experiment, our bestautomatically-induced grammar achieves an f-scoreof 80.24%.10 ConclusionsWe presented and extensively evaluated a finiteapproximation of LDD resolution in automati-cally constructed, wide-coverage, robust, PCFG-based LFG approximations, effectively turning the?half?
(or ?shallow?
)-grammars presented in (Cahillet al, 2002) into ?full?
or ?deep?
grammars.
Inour approach, LDDs are resolved in f-structure, nottrees.
The method achieves a preds-only f-scoreof 80.97% for f-structures with the PA-PCFG inthe integrated architecture against the DCU 105and 78.4% against the 2,416 automatically gener-ated f-structures for the original Penn-II treebanktrees.
Evaluating against the PARC 700 Depen-dency Bank, the P-PCFG achieves an f-score of80.24%, an overall improvement of approximately0.6% on the result reported for the best hand-craftedgrammars in (Kaplan et al, 2004).AcknowledgementsThis research was funded by Enterprise Ireland Ba-sic Research Grant SC/2001/186 and IRCSET.ReferencesS.
Abney.
1997.
Stochastic attribute-value gram-mars.
Computational Linguistics, 23(4):597?618.M.
Burke, A. Cahill, R. O?Donovan, J. van Gen-abith, and A.
Way 2004.
The Evaluation ofan Automatic Annotation Algorithm against thePARC 700 Dependency Bank.
In Proceedingsof the Ninth International Conference on LFG,Christchurch, New Zealand (to appear).A.
Cahill, M. McCarthy, J. van Genabith, and A.Way.
2002.
Parsing with PCFGs and Auto-matic F-Structure Annotation.
In Miriam Buttand Tracy Holloway King, editors, Proceedingsof the Seventh International Conference on LFG,pages 76?95.
CSLI Publications, Stanford, CA.E.
Charniak.
1996.
Tree-Bank Grammars.
InAAAI/IAAI, Vol.
2, pages 1031?1036.E.
Charniak.
1999.
A Maximum-Entropy-InspiredParser.
Technical Report CS-99-12, Brown Uni-versity, Providence, RI.M.
Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Uni-versity of Pennsylvania, Philadelphia, PA.M.
Dalrymple.
2001.
Lexical-Functional Gram-mar.
San Diego, CA; London Academic Press.J.
Hockenmaier.
2003.
Parsing with Generativemodels of Predicate-Argument Structure.
In Pro-ceedings of the 41st Annual Conference of theAssociation for Computational Linguistics, pages359?366, Sapporo, Japan.M.
Johnson.
1999.
PCFG models of linguistictree representations.
Computational Linguistics,24(4):613?632.M.
Johnson.
2002.
A simple pattern-matching al-gorithm for recovering empty nodes and theirantecedents.
In Proceedings of the 40th AnnualMeeting of the Association for ComputationalLinguistics, pages 136?143, Philadelphia, PA.R.
Kaplan and J. Bresnan.
1982.
Lexical Func-tional Grammar, a Formal System for Grammat-ical Representation.
In The Mental Representa-tion of Grammatical Relations, pages 173?281.MIT Press, Cambridge, MA.R.
Kaplan, S. Riezler, T. H. King, J. T. Maxwell,A.
Vasserman, and R. Crouch.
2004.
Speed andaccuracy in shallow and deep stochastic parsing.In Proceedings of the Human Language Tech-nology Conference and the 4th Annual Meetingof the North American Chapter of the Associ-ation for Computational Linguistics, pages 97?104, Boston, MA.T.H.
King, R. Crouch, S. Riezler, M. Dalrymple,and R. Kaplan.
2003.
The PARC700 dependencybank.
In Proceedings of the EACL03: 4th Inter-national Workshop on Linguistically InterpretedCorpora (LINC-03), pages 1?8, Budapest.D.
Klein and C. Manning.
2003.
Accurate Unlexi-calized Parsing.
In Proceedings of the 41st An-nual Meeting of the Association for Computa-tional Linguistics (ACL?02), pages 423?430, Sap-poro, Japan.C.
Macleod, A. Meyers, and R. Grishman.
1994.The COMLEX Syntax Project: The First Year.In Proceedings of the ARPA Workshop on HumanLanguage Technology, pages 669-703, Princeton,NJ.D.
Magerman.
1994.
Natural Language Parsing asStatistical Pattern Recognition.
PhD thesis, Stan-ford University, CA.M.
Marcus, G. Kim, M.A.
Marcinkiewicz, R. Mac-Intyre, A. Bies, M. Ferguson, K. Katz, and B.Schasberger.
1994.
The Penn Treebank: Anno-tating Predicate Argument Structure.
In Proceed-ings of the ARPA Workshop on Human LanguageTechnology, pages 110?115, Princeton, NJ.Y.
Miyao, T. Ninomiya, and J. Tsujii.
2003.
Proba-bilistic modeling of argument structures includ-ing non-local dependencies.
In Proceedings ofthe Conference on Recent Advances in NaturalLanguage Processing (RANLP), pages 285?291,Borovets, Bulgaria.R.
O?Donovan, M. Burke, A. Cahill, J. van Gen-abith, and A.
Way.
2004.
Large-Scale Induc-tion and Evaluation of Lexical Resources fromthe Penn-II Treebank.
In Proceedings of the 42ndAnnual Conference of the Association for Com-putational Linguistics (ACL-04), Barcelona.S.
Riezler, T.H.
King, R. Kaplan, R. Crouch,J.
T. Maxwell III, and M. Johnson.
2002.
Pars-ing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estima-tion Techniques.
In Proceedings of the 40th An-nual Conference of the Association for Compu-tational Linguistics (ACL-02), pages 271?278,Philadelphia, PA.Y.
Tateisi, K. Torisawa, Y. Miyao, and J. Tsujii.1998.
Translating the XTAG English Grammarto HPSG.
In 4th International Workshop on TreeAdjoining Grammars and Related Frameworks,Philadelphia, PA, pages 172?175.J.
van Genabith and R. Crouch.
1996.
Directand Underspecified Interpretations of LFG f-Structures.
In Proceedings of the 16th Interna-tional Conference on Computational Linguistics(COLING), pages 262?267, Copenhagen.
