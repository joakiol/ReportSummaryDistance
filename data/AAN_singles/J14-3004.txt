Feature-Frequency?Adaptive On-lineTraining for Fast and Accurate NaturalLanguage ProcessingXu Sun?Peking UniversityWenjie Li?
?Hong Kong Polytechnic UniversityHoufeng Wang?Peking UniversityQin Lu?Hong Kong Polytechnic UniversityTraining speed and accuracy are two major concerns of large-scale natural language processingsystems.
Typically, we need to make a tradeoff between speed and accuracy.
It is trivial to improvethe training speed via sacrificing accuracy or to improve the accuracy via sacrificing speed.Nevertheless, it is nontrivial to improve the training speed and the accuracy at the same time,which is the target of this work.
To reach this target, we present a new training method, feature-frequency?adaptive on-line training, for fast and accurate training of natural language process-ing systems.
It is based on the core idea that higher frequency features should have a learning ratethat decays faster.
Theoretical analysis shows that the proposed method is convergent with a fastconvergence rate.
Experiments are conducted based on well-known benchmark tasks, includingnamed entity recognition, word segmentation, phrase chunking, and sentiment analysis.
Thesetasks consist of three structured classification tasks and one non-structured classification task,with binary features and real-valued features, respectively.
Experimental results demonstratethat the proposed method is faster and at the same time more accurate than existing methods,achieving state-of-the-art scores on the tasks with different characteristics.?
Key Laboratory of Computational Linguistics (Peking University), Ministry of Education, Beijing, China,and School of EECS, Peking University, Beijing, China.
E-mail: xusun@pku.edu.cn.??
Department of Computing, Hong Kong Polytechnic University, Hung Hom, Kowloon 999077, HongKong.
E-mail: cswjli@comp.polyu.edu.hk.?
Key Laboratory of Computational Linguistics (Peking University), Ministry of Education, Beijing, China,and School of EECS, Peking University, Beijing, China.
E-mail: wanghf@pku.edu.cn.?
Department of Computing, Hong Kong Polytechnic University, Hung Hom, Kowloon 999077, HongKong.
E-mail: csluqin@comp.polyu.edu.hk.Submission received: 27 December 2012; revised version received: 30 May 2013; accepted for publication:16 September 2013.doi:10.1162/COLI a 00193?
2014 Association for Computational LinguisticsComputational Linguistics Volume 40, Number 31.
IntroductionTraining speed is an important concern of natural language processing (NLP) systems.Large-scale NLP systems are computationally expensive.
In many real-world applica-tions, we further need to optimize high-dimensional model parameters.
For example,the state-of-the-art word segmentation system uses more than 40 million features (Sun,Wang, and Li 2012).
The heavyNLPmodels together with high-dimensional parameterslead to a challenging problem onmodel training, whichmay require week-level trainingtime even with fast computing machines.Accuracy is another very important concern of NLP systems.
Nevertheless, usuallyit is quite difficult to build a system that has fast training speed and at the same timehas high accuracy.
Typically we need to make a tradeoff between speed and accuracy,to trade training speed for higher accuracy or vice versa.
In this work, we have triedto overcome this problem: to improve the training speed and the model accuracy at thesame time.There are twomajor approaches for parameter training: batch and on-line.
Standardgradient descent methods are normally batch training methods, in which the gradientcomputed by using all training instances is used to update the parameters of the model.The batch training methods include, for example, steepest gradient descent, conjugategradient descent (CG), and quasi-Newtonmethods like limited-memory BFGS (Nocedaland Wright 1999).
The true gradient is usually the sum of the gradients from eachindividual training instance.
Therefore, batch gradient descent requires the trainingmethod to go through the entire training set before updating parameters.
This is whybatch training methods are typically slow.On-line learning methods can significantly accelerate the training speed comparedwith batch training methods.
A representative on-line training method is the stochasticgradient descent method (SGD) and its extensions (e.g., stochastic meta descent) (Bottou1998; Vishwanathan et al.
2006).
The model parameters are updated more frequentlycompared with batch training, and fewer passes are needed before convergence.
Forlarge-scale data sets, on-line training methods can be much faster than batch trainingmethods.However, we find that the existing on-line training methods are still not goodenough for training large-scale NLP systems?probably because those methods arenot well-tailored for NLP systems that have massive features.
First, the convergencespeed of the existing on-line training methods is not fast enough.
Our studies show thatthe existing on-line training methods typically require more than 50 training passesbefore empirical convergence, which is still slow.
For large-scale NLP systems, thetraining time per pass is typically long and fast convergence speed is crucial.
Second,the accuracy of the existing on-line training methods is not good enough.
We want tofurther improve the training accuracy.
We try to deal with the two challenges at thesame time.
Our goal is to develop a new training method for faster and at the same timemore accurate natural language processing.In this article, we present a new on-line training method, adaptive on-line gradientdescent based on feature frequency information (ADF),1 for very accurate and faston-line training of NLP systems.
Other than the high training accuracy and fast train-ing speed, we further expect that the proposed training method has good theoretical1 ADF source code and tools can be obtained from http://klcl.pku.edu.cn/member/sunxu/index.htm.564Sun et al.
Feature-Frequency?Adaptive On-line Training for Natural Language Processingproperties.
We want to prove that the proposed method is convergent and has a fastconvergence rate.In the proposed ADF training method, we use a learning rate vector in the on-lineupdating.
This learning rate vector is automatically adapted based on feature frequencyinformation in the training data set.
Each model parameter has its own learning rateadapted on feature frequency information.
This proposal is based on the simple intu-ition that a feature with higher frequency in the training process should have a learningrate that decays faster.
This is because a higher frequency feature is expected to bewell optimized with higher confidence.
Thus, a higher frequency feature is expected tohave a lower learning rate.
We systematically formalize this intuition into a theoreticallysound training algorithm, ADF.The main contributions of this work are as follows:r On the methodology side, we propose a general purpose on-line trainingmethod, ADF.
The ADF method is significantly more accurate thanexisting on-line and batch training methods, and has faster training speed.Moreover, theoretical analysis demonstrates that the ADF method isconvergent with a fast convergence rate.r On the application side, for the three well-known tasks, including namedentity recognition, word segmentation, and phrase chunking, the proposedsimple method achieves equal or even better accuracy than the existinggold-standard systems, which are complicated and use extra resources.2.
Related WorkOur main focus is on structured classification models with high dimensional features.For structured classification, the conditional random fields model is widely used.
Toillustrate that the proposed method is a general-purpose training method not limited toa specific classification task or model, we also evaluate the proposal for non-structuredclassification tasks like binary classification.
For non-structured classification, the max-imum entropy model (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996)is widely used.
Here, we review the conditional random fields model and the relatedwork of on-line training methods.2.1 Conditional Random FieldsThe conditional random field (CRF) model is a representative structured classificationmodel and it is well known for its high accuracy in real-world applications.
The CRFmodel is proposed for structured classification by solving ?the label bias problem?
(Lafferty, McCallum, and Pereira 2001).
Assuming a feature function that maps a pair ofobservation sequence x and label sequence y to a global feature vector f, the probabilityof a label sequence y conditioned on the observation sequence x is modeled as follows(Lafferty, McCallum, and Pereira 2001):P(y|x,w) =exp {w>f (y,x)}??y?
exp {w>f (y?
,x)}(1)wherew is a parameter vector.565Computational Linguistics Volume 40, Number 3Given a training set consisting of n labeled sequences, zi = (xi,yi), for i = 1 .
.
.n,parameter estimation is performed by maximizing the objective function,L(w) =n?i=1logP(yi|xi,w)?
R(w) (2)The first term of this equation represents a conditional log-likelihood of trainingdata.
The second term is a regularizer for reducing overfitting.
We use an L2 prior,R(w) = ||w||22?2 .
In what follows, we denote the conditional log-likelihood of each sampleas logP(yi|xi,w) as `(zi,w).
The final objective function is as follows:L(w) =n?i=1`(zi,w)?||w||22?2(3)2.2 On-line TrainingThe most representative on-line training method is the SGD method (Bottou 1998;Tsuruoka, Tsujii, and Ananiadou 2009; Sun et al.
2013).
The SGD method uses arandomly selected small subset of the training sample to approximate the gradient ofan objective function.
The number of training samples used for this approximation iscalled the batch size.
By using a smaller batch size, one can update the parametersmore frequently and speed up the convergence.
The extreme case is a batch size of 1,and it gives the maximum frequency of updates, which we adopt in this work.
In thiscase, the model parameters are updated as follows:wt+1 = wt + ?t?wtLstoch(zi,wt) (4)where t is the update counter, ?t is the learning rate or so-called decaying rate, andLstoch(zi,wt) is the stochastic loss function based on a training sample zi.
(More detailsof SGD are described in Bottou [1998], Tsuruoka, Tsujii, and Ananiadou [2009], andSun et al.
[2013].)
Following the most recent work of SGD, the exponential decayingrate works the best for natural language processing tasks, and it is adopted in ourimplementation of the SGD (Tsuruoka, Tsujii, and Ananiadou 2009; Sun et al.
2013).Other well-known on-line training methods include perceptron training (Freundand Schapire 1999), averaged perceptron training (Collins 2002), more recent devel-opment/extensions of stochastic gradient descent (e.g., the second-order stochasticgradient descent training methods like stochastic meta descent) (Vishwanathan et al.2006; Hsu et al.
2009), and so on.
However, the second-order stochastic gradient descentmethod requires the computation or approximation of the inverse of the Hessian matrixof the objective function, which is typically slow, especially for heavily structured classi-fication models.
Usually the convergence speed based on number of training iterationsis moderately faster, but the time cost per iteration is slower.
Thus the overall time costis still large.Compared with the related work on batch and on-line training (Jacobs 1988;Sperduti and Starita 1993; Dredze, Crammer, and Pereira 2008; Duchi, Hazan, andSinger 2010; McMahan and Streeter 2010), our work is fundamentally different.
TheproposedADF trainingmethod is based on feature frequency adaptation, and to the bestof our knowledge there is no prior work on direct feature-frequency?adaptive on-line566Sun et al.
Feature-Frequency?Adaptive On-line Training for Natural Language Processingtraining.
Compared with the confidence-weighted (CW) classification method and itsvariation AROW (Dredze, Crammer, and Pereira 2008; Crammer, Kulesza, and Dredze2009), the proposed method is substantially different.
While the feature frequencyinformation is implicitly modeled via a complicated Gaussian distribution frameworkin Dredze, Crammer, and Pereira (2008) and Crammer, Kulesza, and Dredze (2009),the frequency information is explicitly modeled in our proposal via simple learningrate adaptation.
Our proposal is more straightforward in capturing feature frequencyinformation, and it has no need to use Gaussian distributions and KL divergence,which are important in the CW and AROW methods.
In addition, our proposal is aprobabilistic learning method for training probabilistic models such as CRFs, whereasthe CW and AROW methods (Dredze, Crammer, and Pereira 2008; Crammer, Kulesza,and Dredze 2009) are non-probabilistic learning methods extended from perceptron-style approaches.
Thus, the framework is different.
This work is a substantial extensionof the conference version (Sun, Wang, and Li 2012).
Sun, Wang, and Li (2012) focus onthe specific task of word segmentation, whereas this article focuses on the proposedtraining algorithm.3.
Feature-Frequency?Adaptive On-line LearningIn traditional on-line optimization methods such as SGD, no distinction is made fordifferent parameters in terms of the learning rate, and this may result in slow conver-gence of the model training.
For example, in the on-line training process, suppose thehigh frequency feature f1 and the low frequency feature f2 are observed in a trainingsample and their corresponding parameters w1 and w2 are to be updated via the samelearning rate ?t.
Suppose the high frequency feature f1 has been updated 100 timesand the low frequency feature f2 has only been updated once.
Then, it is possible thatthe weight w1 is already well optimized and the learning rate ?t is too aggressive forupdating w1.
Updating the weight w1 with the learning rate ?t may make w1 be farfrom the well-optimized value, and it will require corrections in the future updates.
Thiscauses fluctuations in the on-line training and results in slow convergence speed.
Onthe other hand, it is possible that the weight w2 is poorly optimized and the same learn-ing rate ?t is too conservative for updating w2.
This also results in slow convergencespeed.To solve this problem, we propose ADF.
In spite of the high accuracy and fastconvergence speed, the proposed method is easy to implement.
The proposed methodwith feature-frequency?adaptive learning rates can be seen as a learning method withspecific diagonal approximation of the Hessian information based on assumptions offeature frequency information.
In this approximation, the diagonal elements of thediagonal matrix correspond to the feature-frequency?adaptive learning rates.
Accord-ing to the aforementioned example and analysis, it assumes that a feature with higherfrequency in the training process should have a learning rate that decays faster.3.1 AlgorithmIn the proposed ADF method, we try to use more refined learning rates than traditionalSGD training.
Instead of using a single learning rate (a scalar) for all weights, we extendthe learning rate scalar to a learning rate vector, which has the same dimension as theweight vector w. The learning rate vector is automatically adapted based on feature567Computational Linguistics Volume 40, Number 3frequency information.
By doing so, each weight has its own learning rate, and we willshow that this can significantly improve the convergence speed of on-line learning.In the ADF learning method, the update formula is:wt+1 = wt +?t ?
gt (5)The update term gt is the gradient term of a randomly sampled instance:gt = ?wtLstoch(zi,wt) = ?wt{`(zi,wt)?||wt||22n?2}In addition, ?t ?
Rf+ is a positive vector-valued learning rate and ?
denotes thecomponent-wise (Hadamard) product of two vectors.The learning rate vector ?t is automatically adapted based on feature frequencyinformation in the updating process.
Intuitively, a feature with higher frequency in thetraining process has a learning rate that decays faster.
This is because a weight withhigher frequency is expected to be more adequately trained, hence a lower learningrate is preferable for fast convergence.
We assume that a high frequency feature shouldhave a lower learning rate, and a low frequency feature should have a relatively higherlearning rate in the training process.We systematically formalize this idea into a theoret-ically sound training algorithm.
The proposedmethodwith feature-frequency?adaptivelearning rates can be seen as a learning method with specific diagonal approximationof the inverse of the Hessian matrix based on feature frequency information.Given awindow size q (number of samples in awindow), we use a vector v to recordthe feature frequency.
The kth entry vk corresponds to the frequency of the feature k inthis window.
Given a feature k, we use u to record the normalized frequency:u = vk/qFor each feature, an adaptation factor ?
is calculated based on the normalized frequencyinformation, as follows:?
= ??
u(??
?
)where ?
and ?
are the upper and lower bounds of a scalar, with 0 < ?
< ?
< 1.
Intu-itively, the upper bound ?
corresponds to the adaptation factor of the lowest frequencyfeatures, and the lower bound ?
corresponds to the adaptation factor of the highestfrequency features.
The optimal values of ?
and ?
can be tuned based on specific real-world tasks, for example, via cross-validation on the training data or using held-outdata.
In practice, via cross-validation on the training data of different tasks, we foundthat the following setting is sufficient to produce adequate performance for most of thereal-world natural language processing tasks: ?
around 0.995, and ?
around 0.6.
Thisindicates that the feature frequency information has similar characteristics across manydifferent natural language processing tasks.As we can see, a feature with higher frequency corresponds to a smaller scalar vialinear approximation.
Finally, the learning rate is updated as follows:?k ?
?
?k568Sun et al.
Feature-Frequency?Adaptive On-line Training for Natural Language ProcessingADF learning algorithm1: procedure ADF(Z,w, q, c, ?, ?
)2: w ?
0, t?
0, v ?
0, ?
?
c3: repeat until convergence4: .
Draw a sample zi at random from the data set Z5: .
v ?
UPDATEFEATUREFREQ(v, zi)6: .
if t > 0 and t mod q = 07: .
.
?
?
UPDATELEARNRATE(?, v)8: .
.
v ?
09: .
g ?
?wLstoch(zi,w)10: .
w ?
w +?
?
g11: .
t?
t+ 112: returnw13:14: procedure UPDATEFEATUREFREQ(v, zi)15: for k ?
features used in sample zi16: .
vk ?
vk + 117: return v18:19: procedure UPDATELEARNRATE(?, v)20: for k ?
all features21: .
u?
vk/q22: .
??
??
u(??
?
)23: .
?k ?
?
?k24: return ?Figure 1The proposed ADF on-line learning algorithm.
In the algorithm, Z is the training data set; q, c, ?,and ?
are hyper-parameters; q is an integer representing window size; c is for initializing thelearning rates; and ?
and ?
are the upper and lower bounds of a scalar, with 0 < ?
< ?
< 1.With this setting, different features correspond to different adaptation factors basedon feature frequency information.
Our ADF algorithm is summarized in Figure 1.The ADF training method is efficient because the only additional computation(compared with traditional SGD) is the derivation of the learning rates, which is simpleand efficient.
As we know, the regularization of SGD can perform efficiently via the opti-mization based on sparse features (Shalev-Shwartz, Singer, and Srebro 2007).
Similarly,the derivation of ?t can also perform efficiently via the optimization based on sparsefeatures.
Note that although binary features are common in natural language processingtasks, the ADF algorithm is not limited to binary features and it can be applied to real-valued features.3.2 Convergence AnalysisWe want to show that the proposed ADF learning algorithm has good convergenceproperties.
There are two steps in the convergence analysis.
First, we show that theADF update rule is a contraction mapping.
Then, we show that the ADF training isasymptotically convergent, and with a fast convergence rate.To simplify the discussion, our convergence analysis is based on the convex lossfunction of traditional classification or regression problems:L(w) =n?i=1`(xi, yi,w ?
f i)?||w||22?2569Computational Linguistics Volume 40, Number 3where f i is the feature vector generated from the training sample (xi, yi).
L(w) is a func-tion in w ?
f i, such as 12 (yi ?w ?
f i)2 for regression or log[1+ exp(?yiw ?
f i)] for binaryclassification.To make convergence analysis of the proposed ADF training algorithm, we need tointroduce several mathematical definitions.
First, we introduce Lipschitz continuity:Definition 1 (Lipschitz continuity)A function F : X ?
R is Lipschitz continuous with the degree of D if |F(x)?
F(y)| ?D|x?
y| for ?x, y ?
X .
X can be multi-dimensional space, and |x?
y| is the distancebetween the points x and y.Based on the definition of Lipschitz continuity, we give the definition of theLipschitz constant ||F||Lip as follows:Definition 2 (Lipschitz constant)||F||Lip := inf{D where |F(x)?
F(y)| ?
D|x?
y| for ?x, y}In other words, the Lipschitz constant ||F||Lip is the lower bound of the continuity degreethat makes the function F Lipschitz continuous.Further, based on the definition of Lipschitz constant, we give the definition ofcontraction mapping as follows:Definition 3 (Contraction mapping)A function F : X ?
X is a contraction mapping if its Lipschitz constant is smaller than1: ||F||Lip < 1.Then, we can show that the traditional SGD update is a contraction mapping.Lemma 1 (SGD update rule is contraction mapping)Let ?
be a fixed low learning rate in SGD updating.
If ?
?
(||x2i || ?
||?y?
`(xi, yi, y?
)||Lip)?1,the SGD update rule is a contraction mapping in Euclidean space with Lipschitz con-tinuity degree 1?
?/?2.The proof can be extended from the relatedwork on convergence analysis of parallelSGD training (Zinkevich et al.
2010).
The stochastic training process is a one-following-one dynamic update process.
In this dynamic process, if we use the same update rule F,we havewt+1 = F(wt) andwt+2 = F(wt+1).
It is only necessary to prove that the dynamicupdate is a contraction mapping restricted by this one-following-one dynamic process.That is, for the proposed ADF update rule, it is only necessary to prove it is a dynamiccontraction mapping.
We formally define dynamic contraction mapping as follows.Definition 4 (Dynamic contraction mapping)Given a function F : X ?
X , suppose the function is used in a dynamic one-following-one process: xt+1 = F(xt) and xt+2 = F(xt+1) for ?xt ?
X .
Then, the function F is adynamic contraction mapping if ?D < 1, |xt+2 ?
xt+1| ?
D|xt+1 ?
xt| for ?xt ?
X .We can see that a contraction mapping is also a dynamic contraction mapping, buta dynamic contraction mapping is not necessarily a contraction mapping.
We first show570Sun et al.
Feature-Frequency?Adaptive On-line Training for Natural Language Processingthat the ADF update rule with a fixed learning rate vector of different learning rates isa dynamic contraction mapping.Theorem 1 (ADF update rule with fixed learning rates)Let ?
be a fixed learning rate vector with different learning rates.
Let ?max be the max-imum learning rate in the learning rate vector ?
: ?max := sup{?i where ?i ?
?}.
Thenif ?max ?
(||x2i || ?
||?y?
`(xi, yi, y?
)||Lip)?1, the ADF update rule is a dynamic contractionmapping in Euclidean space with Lipschitz continuity degree 1?
?max/?2.The proof is sketched in Section 5.Further, we need to prove that the ADF update rule with a decaying learningrate vector is a dynamic contraction mapping, because the real ADF algorithm has adecaying learning rate vector.
In the decaying case, the condition that ?max ?
(||x2i || ?||?y?
`(xi, yi, y?
)||Lip)?1 can be easily achieved, because ?
continues to decay with anexponential decaying rate.
Even if the ?
is initialized with high values of learning rates,after a number of training passes (denoted as T) ?T is guaranteed to be small enough sothat ?max := sup{?i where ?i ?
?T} and ?max ?
(||x2i || ?
||?y?
`(xi, yi, y?)||Lip)?1.
Withoutlosing generality, our convergence analysis starts from the pass T and we take ?T as ?0in the following analysis.
Thus, we can show that the ADF update rule with a decayinglearning rate vector is a dynamic contraction mapping:Theorem 2 (ADF update rule with decaying learning rates)Let ?t be a learning rate vector in the ADF learning algorithm, which is decayingover the time t and with different decaying rates based on feature frequency infor-mation.
Let ?t start from a low enough learning rate vector ?0 such that ?max ?
(||x2i || ?
||?y?
`(xi, yi, y?
)||Lip)?1, where ?max is the maximum element in?0.
Then, the ADFupdate rule with decaying learning rate vector is a dynamic contraction mapping inEuclidean space with Lipschitz continuity degree 1?
?max/?2.The proof is sketched in Section 5.Based on the connections between ADF training and contraction mapping, wedemonstrate the convergence properties of the ADF training method.
First, we provethe convergence of the ADF training.Theorem 3 (ADF convergence)ADF training is asymptotically convergent.The proof is sketched in Section 5.Further, we analyze the convergence rate of the ADF training.
When we have thelowest learning rate?t+1 = ?
?t, the expectation of the obtainedwt is as follows (Murata1998; Hsu et al.
2009):E(wt) = w?
+t?m=1(I ??0?mH(w?
))(w0 ?w?
)where w?
is the optimal weight vector, and H is the Hessian matrix of the objectivefunction.
The rate of convergence is governed by the largest eigenvalue of the functionCt =?tm=1(I ??0?mH(w?)).
Following Murata (1998) and Hsu et al.
(2009), we canderive a bound of rate of convergence, as follows.571Computational Linguistics Volume 40, Number 3Theorem 4 (ADF convergence rate)Assume ?
is the largest eigenvalue of the function Ct =?tm=1(I ??0?mH(w?)).
For theproposed ADF training, its convergence rate is bounded by ?, and we have?
?
exp {?0????
1}where ?
is the minimum eigenvalue ofH(w?
).The proof is sketched in Section 5.The convergence analysis demonstrates that the proposed method with feature-frequency-adaptive learning rates is convergent and the bound of convergence rateis analyzed.
It demonstrates that increasing the values of ?0 and ?
leads to a lowerbound of the convergence rate.
Because the bound of the convergence rate is just anup-bound rather than the actual convergence rate, we still need to conduct automatictuning of the hyper-parameters, including ?0 and ?, for optimal convergence rate inpractice.
The ADF training method has a fast convergence rate because the feature-frequency-adaptive schema can avoid the fluctuations on updating the weights of highfrequency features, and it can avoid the insufficient training on updating the weights oflow frequency features.
In the following sections, we perform experiments to confirmthe fast convergence rate of the proposed method.4.
EvaluationOur main focus is on training heavily structured classification models.
We evaluate theproposal on three NLP structured classification tasks: biomedical named entity recogni-tion (Bio-NER), Chinese word segmentation, and noun phrase (NP) chunking.
For thestructured classification tasks, the ADF training is based on the CRF model (Lafferty,McCallum, and Pereira 2001).
Further, to demonstrate that the proposed method isnot limited to structured classification tasks, we also perform experiments on a non-structured binary classification task: sentiment-based text classification.
For the non-structured classification task, the ADF training is based on themaximum entropymodel(Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996).4.1 Biomedical Named Entity Recognition (Structured Classification)The biomedical named entity recognition (Bio-NER) task is from the BIONLP-2004shared task.
The task is to recognize five kinds of biomedical named entities, includingDNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text miningcorpus (Kim et al.
2004).
A typical approach to this problem is to cast it as a sequentiallabeling task with the BIO encoding.This data set consists of 20,546 training samples (from 2,000 MEDLINE articleabstracts, with 472,006 word tokens) and 4,260 test samples.
The properties of the dataare summarized in Table 1.
State-of-the-art systems for this task include Settles (2004),Finkel et al.
(2004), Okanohara et al.
(2006), Hsu et al.
(2009), Sun, Matsuzaki, et al.
(2009), and Tsuruoka, Tsujii, and Ananiadou (2009).Following previous studies for this task (Okanohara et al.
2006; Sun, Matsuzaki,et al.
2009), we use word token?based features, part-of-speech (POS) based features,and orthography pattern?based features (prefix, uppercase/lowercase, etc.
), as listed inTable 2.
With the traditional implementation of CRF systems (e.g., the HCRF package),the edges features usually contain only the information of yi?1 and yi, and ignore the572Sun et al.
Feature-Frequency?Adaptive On-line Training for Natural Language ProcessingTable 1Summary of the Bio-NER data set.#Abstracts #Sentences #WordsTrain 2,000 20,546 (10/abs) 472,006 (23/sen)Test 404 4,260 (11/abs) 96,780 (23/sen)Table 2Feature templates used for the Bio-NER task.
wi is the current word token on position i. ti is thePOS tag on position i. oi is the orthography mode on position i. yi is the classification label onposition i. yi?1yi represents label transition.
A?
B represents a Cartesian product betweentwo sets.Word Token?based Features:{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi, wiwi+1}?
{yi, yi?1yi}Part-of-Speech (POS)?based Features:{ti?2, ti?1, ti, ti+1, ti+2, ti?2ti?1, ti?1ti, titi+1, ti+1ti+2, ti?2ti?1ti, ti?1titi+1, titi+1ti+2}?
{yi, yi?1yi}Orthography Pattern?based Features:{oi?2, oi?1, oi, oi+1, oi+2, oi?2oi?1, oi?1oi, oioi+1, oi+1oi+2}?
{yi, yi?1yi}information of the observation sequence (i.e., x).
The major reason for this simple real-ization of edge features in traditional CRF implementation is to reduce the dimensionof features.
To improve the model accuracy, we utilize rich edge features following Sun,Wang, and Li (2012), in which local observation information of x is combined in edgefeatures just like the implementation of node features.
A detailed introduction to richedge features can be found in Sun, Wang, and Li (2012).
Using the feature templates,we extract a high dimensional feature set, which contains 5.3?
107 features in total.Following prior studies, the evaluation metric for this task is the balanced F-scoredefined as 2PR/(P+ R), where P is precision and R is recall.4.2 Chinese Word Segmentation (Structured Classification)Chinese word segmentation aims to automatically segment character sequences intoword sequences.
Chinese word segmentation is important because it is the first stepfor most Chinese language information processing systems.
Our experiments are basedon the Microsoft Research data provided by The Second International Chinese WordSegmentation Bakeoff.
In this data set, there are 8.8?
104 word-types, 2.4?
106 word-tokens, 5?
103 character-types, and 4.1?
106 character-tokens.
State-of-the-art systemsfor this task include Tseng et al.
(2005), Zhang, Kikui, and Sumita (2006), Zhang andClark (2007), Gao et al.
(2007), Sun, Zhang, et al.
(2009), Sun (2010), Zhao et al.
(2010),and Zhao and Kit (2011).The feature engineering follows previous work on word segmentation (Sun, Wang,and Li 2012).
Rich edge features are used.
For the classification label yi and the labeltransition yi?1yi on position i, we use the feature templates as follows (Sun, Wang, andLi 2012):r Character unigrams located at positions i?
2, i?
1, i, i+ 1, and i+ 2.573Computational Linguistics Volume 40, Number 3r Character bigrams located at positions i?
2, i?
1, i and i+ 1.r Whether xj and xj+1 are identical, for j = i?
2, .
.
.
, i+ 1.r Whether xj and xj+2 are identical, for j = i?
3, .
.
.
, i+ 1.r The character sequence xj,i if it matches a word w ?
U, with the constrainti?
6 < j < i.
The item xj,i represents the character sequence xj .
.
.
xi.U represents the unigram-dictionary collected from the training data.r The character sequence xi,k if it matches a word w ?
U, with the constrainti < k < i+ 6.r The word bigram candidate [xj,i?1, xi,k] if it hits a word bigram[wi, wj] ?
B, and satisfies the aforementioned constraints on j and k.B represents the word bigram dictionary collected from the training data.r The word bigram candidate [xj,i, xi+1,k] if it hits a word bigram[wi, wj] ?
B, and satisfies the aforementioned constraints on j and k.All feature templates are instantiated with values that occurred in training samples.The extracted feature set is large, and there are 2.4?
107 features in total.
Our evaluationis based on a closed test, and we do not use extra resources.
Following prior studies, theevaluation metric for this task is the balanced F-score.4.3 Phrase Chunking (Structured Classification)In the phrase chunking task, the non-recursive cores of noun phrases, called base NPs,are identified.
The phrase chunking data is extracted from the data of the CoNLL-2000shallow-parsing shared task (Sang and Buchholz 2000).
The training set consists of 8,936sentences, and the test set consists of 2,012 sentences.
We use the feature templatesbased on word n-grams and part-of-speech n-grams, and feature templates are shownin Table 3.
Rich edge features are used.
Using the feature templates, we extract 4.8?
105features in total.
State-of-the-art systems for this task include Kudo and Matsumoto(2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al.
(2006), Sun et al.
(2008), and Tsuruoka, Tsujii, and Ananiadou (2009).
Following priorstudies, the evaluation metric for this task is the balanced F-score.4.4 Sentiment Classification (Non-Structured Classification)To demonstrate that the proposed method is not limited to structured classification, weselect a well-known sentiment classification task for evaluating the proposed methodon non-structured classification.Table 3Feature templates used for the phrase chunking task.
wi, ti, and yi are defined as before.Word-Token?based Features:{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi, wiwi+1}?
{yi, yi?1yi}Part-of-Speech (POS)?based Features:{ti?1, ti, ti+1, ti?2ti?1, ti?1ti, titi+1, ti+1ti+2, ti?2ti?1ti, ti?1titi+1, titi+1ti+2}?
{yi, yi?1yi}574Sun et al.
Feature-Frequency?Adaptive On-line Training for Natural Language ProcessingGenerally, sentiment classification classifies user review text as a positive or neg-ative opinion.
This task (Blitzer, Dredze, and Pereira 2007) consists of four subtasksbased on user reviews from Amazon.com.
Each subtask is a binary sentiment clas-sification task based on a specific topic.
We use the maximum entropy model forclassification.
We use the same lexical features as those used in Blitzer, Dredze, andPereira (2007), and the total number of features is 9.4?
105.
Following prior work, theevaluation metric is binary classification accuracy.4.5 Experimental SettingAs for training, we perform gradient descent with the proposed ADF training method.To compare with existing literature, we choose four popular training methods, a rep-resentative batch training method, and three representative on-line training methods.The batch training method is the limited-memory BFGS (LBFGS) method (Nocedal andWright 1999), which is considered to be one of the best optimizers for log-linear modelslike CRFs.
The on-line training methods include the SGD training method, which weintroduced in Section 2.2, the structured perceptron (Perc) training method (Freundand Schapire 1999; Collins 2002), and the averaged perceptron (Avg-Perc) trainingmethod (Collins 2002).
The structured perceptron method and averaged perceptronmethod are non-probabilistic training methods that have very fast training speed dueto the avoidance of the computation on gradients (Sun, Matsuzaki, and Li 2013).
Alltraining methods, including ADF, SGD, Perc, Avg-Perc, and LBFGS, use the same setof features.We also compared the ADF method with the CW method (Dredze, Crammer, andPereira 2008) and the AROW method (Crammer, Kulesza, and Dredze 2009).
The CWand AROW methods are implemented based on the Confidence Weighted LearningLibrary.2 Because the current implementation of the CW and AROW methods do notutilize rich edge features, we removed the rich edge features in our systems to makemore fair comparisons.
That is, we removed rich edge features in the CRF-ADF setting,and this simplified method is denoted as ADF-noRich.
The second-order stochasticgradient descent training methods, including the SMD method (Vishwanathan et al.2006) and the PSA method (Hsu et al.
2009), are not considered in our experimentsbecause we find those methods are quite slow when running on our data sets with highdimensional features.We find that the settings of q, ?, and ?
in the ADF training method are not sensitiveamong specific tasks and can be generally set.
We simply set q = n/10 (n is the numberof training samples).
It means that feature frequency information is updated 10 timesper iteration.
Via cross-validation only on the training data of different tasks, we findthat the following setting is sufficient to produce adequate performance for most ofthe real-world natural language processing tasks: ?
around 0.995 and ?
around 0.6.This indicates that the feature frequency information has similar characteristics acrossmany different natural language processing tasks.Thus, we simply use the following setting for all tasks: q = n/10, ?
= 0.995, and?
= 0.6.
This leaves c (the initial value of the learning rates) as the only hyper-parameterthat requires careful tuning.
We perform automatic tuning for c based on the trainingdata via 4-fold cross-validation, testing with c = 0.005, 0.01, 0.05, 0.1, respectively, andthe optimal c is chosen based on the best accuracy of cross-validation.
Via this automatic2 http://webee.technion.ac.il/people/koby/code-index.html.575Computational Linguistics Volume 40, Number 3tuning, we find it is proper to set c = 0.005, 0.1, 0.05, 0.005, for the Bio-NER, wordsegmentation, phrase chunking, and sentiment classification tasks, respectively.To reduce overfitting, we use an L2 Gaussian weight prior (Chen and Rosenfeld1999) for the ADF, LBFGS, and SGD training methods.
We vary the ?
with differentvalues (e.g., 1.0, 2.0, and 5.0) for 4-fold cross validation on the training data of differenttasks, and finally set ?
= 5.0 for all training methods in the Bio-NER task; ?
= 5.0 forall training methods in the word segmentation task; ?
= 5.0, 1.0, 1.0 for ADF, SGD,and LBFGS in the phrase chunking task; and ?
= 1.0 for all training methods in thesentiment classification task.
Experiments are performed on a computer with an Intel(R)Xeon(R) 2.0-GHz CPU.4.6 Structured Classification Results4.6.1 Comparisons Based on Empirical Convergence.
First, we check the experimental re-sults of different methods on their empirical convergence state.
Because the perceptrontraining method (Perc) does not achieve empirical convergence even with a very largenumber of training passes, we simply report its results based on a large enough numberof training passes (e.g., 200 passes).
Experimental results are shown in Table 4.As we can see, the proposed ADF method is more accurate than other trainingmethods, either the on-line ones or the batch one.
It is a bit surprising that the ADFmethod performs even more accurately than the batch training method (LBFGS).
Wenotice that some previous work also found that on-line training methods could haveTable 4Results for the Bio-NER, word segmentation, and phrase chunking tasks.
The results and thenumber of passes are decided based on empirical convergence (with score deviation of adjacentfive passes less than 0.01).
For the non-convergent case, we simply report the results based on alarge enough number of training passes.
As we can see, the ADF method achieves the bestaccuracy with the fastest convergence speed.Bio-NER Prec Rec F-score Passes Train-Time (sec)LBFGS (batch) 67.69 70.20 68.92 400 152,811.34SGD (on-line) 70.91 72.69 71.79 91 76,549.21Perc (on-line) 65.37 66.95 66.15 200 20,436.69Avg-Perc (on-line) 68.76 72.56 70.61 37 3,928.01ADF (proposal) 71.71 72.80 72.25 35 27,490.24Segmentation Prec Rec F-score Passes Train-Time (sec)LBFGS (batch) 97.46 96.86 97.16 102 13,550.68SGD (on-line) 97.58 97.11 97.34 27 6,811.15Perc (on-line) 96.99 96.03 96.50 200 8,382.606Avg-Perc (on-line) 97.56 97.05 97.30 16 716.87ADF (proposal) 97.67 97.31 97.49 15 4,260.08Chunking Prec Rec F-score Passes Train-Time (sec)LBFGS (batch) 94.57 94.09 94.33 105 797.04SGD (on-line) 94.48 94.04 94.26 56 903.88Perc (on-line) 93.66 93.31 93.48 200 543.51Avg-Perc (on-line) 94.34 94.04 94.19 12 33.45ADF (proposal) 94.66 94.38 94.52 17 282.17576Sun et al.
Feature-Frequency?Adaptive On-line Training for Natural Language Processingbetter performance than batch training methods such as LBFGS (Tsuruoka, Tsujii, andAnaniadou 2009; Schaul, Zhang, and LeCun 2012).
The ADF training method canachieve better results probably because the feature-frequency?adaptive training schemacan produce more balanced training of features with diversified frequencies.
TraditionalSGD training may over-train high frequency features and at the same time may haveinsufficient training of low frequency features.
The ADF training method can avoidsuch problems.
It will be interesting to perform further analysis in future work.We also performed significance tests based on t-tests with a significance level of0.05.
Significance tests demonstrate that the ADF method is significantly more accuratethan the existing training methods in most of the comparisons, whether on-line orbatch.
For the Bio-NER task, the differences between ADF and LBFGS, SGD, Perc,and Avg-Perc are significant.
For the word segmentation task, the differences betweenADF and LBFGS, SGD, Perc, and Avg-Perc are significant.
For the phrase chunkingtask, the differences between ADF and Perc and Avg-Perc are significant; the differencesbetween ADF and LBFGS and SGD are non-significant.Moreover, as we can see, the proposed method achieves a convergence state withthe least number of training passes, and with the least wall-clock time.
In general, theADF method is about one order of magnitude faster than the LBFGS batch trainingmethod and several times faster than the existing on-line training methods.4.6.2 Comparisons with State-of-the-Art Systems.
The three tasks are well-known bench-mark tasks with standard data sets.
There is a large amount of published research onthose three tasks.
We compare the proposed method with the state-of-the-art systems.The comparisons are shown in Table 5.As we can see, our system is competitive with the best systems for the Bio-NER,word segmentation, and NP-chunking tasks.
Many of the state-of-the-art systems useextra resources (e.g., linguistic knowledge) or complicated systems (e.g., voting overTable 5Comparing our results with some representative state-of-the-art systems.Bio-NER Method F-score(Okanohara et al.
2006) Semi-Markov CRF + global features 71.5(Hsu et al.
2009) CRF + PSA(1) training 69.4(Tsuruoka, Tsujii, and Ananiadou 2009) CRF + SGD-L1 training 71.6Our Method CRF + ADF training 72.3Segmentation Method F-score(Gao et al.
2007) Semi-Markov CRF 97.2(Sun, Zhang, et al.
2009) Latent-variable CRF 97.3(Sun 2010) Multiple segmenters + voting 96.9Our Method CRF + ADF training 97.5Chunking Method F-score(Kudo and Matsumoto 2001) Combination of multiple SVM 94.2(Vishwanathan et al.
2006) CRF + SMD training 93.6(Sun et al.
2008) Latent-variable CRF 94.3Our Method CRF + ADF training 94.5577Computational Linguistics Volume 40, Number 3multiple models).
Thus, it is impressive that our single model?based system withoutextra resources achieves good performance.
This indicates that the proposed ADFtraining method can train model parameters with good generality on the test data.4.6.3 Training Curves.
To study the detailed training process and convergence speed, weshow the training curves in Figures 2?4.
Figure 2 focuses on the comparisons betweenthe ADF method and the existing on-line training methods.
As we can see, the ADFmethod converges faster than other on-line training methods in terms of both trainingpasses and wall-clock time.
The ADF method has roughly the same training speed perpass compared with traditional SGD training.Figure 3 (Top Row) focuses on comparing the ADF method with the CW method(Dredze, Crammer, and Pereira 2008) and the AROW method (Crammer, Kulesza, andDredze 2009).
Comparisons are based on similar features.
As discussed before, the ADF-noRich method is a simplified system, with rich edge features removed from the CRF-ADF system.
As we can see, the proposed ADF method, whether with or without richedge features, outperforms the CWandAROWmethods.
Figure 3 (BottomRow) focuseson the comparisons with different mini-batch (the training samples in each stochasticupdate) sizes.
Representative results with a mini-batch size of 10 are shown.
In general,we find larger mini-batch sizes will slow down the convergence speed.
Results demon-strate that, compared with the SGD training method, the ADF training method is lesssensitive to mini-batch sizes.Figure 4 focuses on the comparisons between the ADF method and the batchtraining method LBFGS.
As we can see, the ADF method converges at least one order0 20 40 60 80 1006667686970717273Number of Passes)ADFSGDPerc0 20 40 60 80 1009696.296.496.696.89797.297.497.6Segmentation (ADF vs. on-line)Number of Passes)ADFSGDPerc0 20 40 60 80 10092.59393.59494.5 Chunking (ADF vs. on-line)Number of Passes)ADFSGDPerc0 1 2 3 4 5 6x 1046667686970717273Training Time (sec))ADFSGDPerc0 2,000 4,000 6,000 8,000 10,0009696.296.496.696.89797.297.497.6Segmentation (ADF vs. on-line)Training Time (sec))ADFSGDPerc0 200 400 600 80092.59393.59494.5 Chunking (ADF vs. on-line)Training Time (sec))ADFSGDPercFigure 2Comparisons among the ADF method and other on-line training methods.
(Top Row)Comparisons based on training passes.
As we can see, the ADF method has the best accuracyand with the fastest convergence speed based on training passes.
(Bottom Row) Comparisonsbased on wall-clock time.578Sun et al.
Feature-Frequency?Adaptive On-line Training for Natural Language Processing0 20 40 60 80 10060626466687072Bio?NER (ADF vs. CW/AROW)Number of PassesF?score (%)ADFADF?noRichCWAROW0 20 40 60 80 10092939495969798Segmentation (ADF vs. CW/AROW)Number of PassesF?score  (%)ADFADF?noRichCWAROW0 20 40 60 80 10092.59393.59494.595 Chunking (ADF vs. CW/AROW)Number of PassesF?score (%)ADFADF?noRichCWAROW0 20 40 60 80 100686970717273 Bio?NER (MiniBatch=10)Number of PassesF?score (%)ADFSGD0 20 40 60 80 10096.496.696.89797.297.497.6Segmentation (MiniBatch=10)Number of PassesF?score  (%)ADFSGD0 20 40 60 80 10093.293.493.693.89494.294.494.6Chunking (MiniBatch=10)Number of PassesF?score (%)ADFSGDFigure 3(Top Row) Comparing ADF and ADF-noRich with CW and AROW methods.
As we can see,both the ADF and ADF-noRich methods work better than the CW and AROW methods.
(Bottom Row) Comparing different methods with mini-batch = 10 in the stochastic learningsetting.magnitude faster than the LBFGS training in terms of both training passes and wall-clock time.
For the LBFGS training, we need to determine the LBFGSmemory parameterm, which controls the number of prior gradients used to approximate the Hessianinformation.
A larger value of m will potentially lead to more accurate estimationof the Hessian information, but at the same time will consume significantly morememory.
Roughly, the LBFGS training consumes m times more memory than the ADFon-line training method.
For most tasks, the default setting of m = 10 is reasonable.
Weset m = 10 for the word segmentation and phrase chunking tasks, and m = 6 for theBio-NER task due to the shortage of memory for m > 6 cases in this task.4.6.4 One-Pass Learning Results.
Many real-world data sets can only observe the trainingdata in one pass.
For example, some Web-based on-line data streams can only appearonce so that the model parameter learning should be finished in one-pass learning (seeZinkevich et al.
2010).
Hence, it is important to test the performance in the one-passlearning scenario.In the one-pass learning scenario, the feature frequency information is computed?on the fly?
during on-line training.
As shown in Section 3.1, we only need to havea real-valued vector v to record the cumulative feature frequency information, whichis updated when observing training instances one by one.
Then, the learning ratevector ?
is updated based on the v only and there is no need to observe the traininginstances again.
This is the same algorithm introduced in Section 3.1 and no change isrequired for the one-pass learning scenario.
Figure 5 shows the comparisons betweenthe ADF method and baselines on one-pass learning.
As we can see, the ADF method579Computational Linguistics Volume 40, Number 30 100 200 300 4006667686970717273 Bio?NER (ADF vs. batch)Number of PassesF?score (%)ADFLBFGS0 100 200 300 4009696.296.496.696.89797.297.497.6Segmentation (ADF vs. batch)Number of PassesF?score  (%)ADFLBFGS0 100 200 300 40093.693.89494.294.4Chunking (ADF vs. batch)Number of PassesF?score (%)ADFLBFGS0 5 10 15x 1046667686970717273 Bio?NER (ADF vs. batch)Training Time (sec)F?score (%)ADFLBFGS0 1 2 3 4 5x 1049696.296.496.696.89797.297.497.6Segmentation (ADF vs. batch)Training Time (sec)F?score  (%)ADFLBFGS0 1,000 2,000 3,00092.59393.59494.5 Chunking (ADF vs. batch)Training Time (sec))ADFLBFGSFigure 4Comparisons between the ADF method and the batch training method LBFGS.
(Top Row)Comparisons based on training passes.
As we can see, the ADF method converges much fasterthan the LBFGS method, and with better accuracy on the convergence state.
(Bottom Row)Comparisons based on wall-clock time.consistently outperforms the baselines.
This also reflects the fast convergence speed ofthe ADF training method.4.7 Non-Structured Classification ResultsIn previous experiments, we showed that the proposed method outperforms existingbaselines on structured classification.
Nevertheless, we want to show that the ADFmethod also has good performance on non-structured classification.
In addition, thistask is based on real-valued features instead of binary features.Figure 5Comparisons among different methods based on one-pass learning.
As we can see, the ADFmethod has the best accuracy on one-pass learning.580Sun et al.
Feature-Frequency?Adaptive On-line Training for Natural Language ProcessingTable 6Results on sentiment classification (non-structured binary classification).Accuracy Passes Train-Time (sec)LBFGS (batch) 87.00 86 72.20SGD (on-line) 87.13 44 55.88Perc (on-line) 84.55 25 5.82Avg-Perc (on-line) 85.04 46 12.22ADF (proposal) 87.89 30 57.12Experimental results of different training methods on the convergence state areshown in Table 6.
As we can see, the proposed method outperforms all of the on-lineand batch baselines in terms of binary classification accuracy.
Here again we observethat the ADF and SGD methods outperform the LBFGS baseline.The training curves are shown in Figure 6.
As we can see, the ADF method con-verges quickly.
Because this data set is relatively small and the feature dimension ismuch smaller than previous tasks, we find the baseline training methods also havefast convergence speed.
The comparisons on one-pass learning are shown in Fig-ure 7.
Just as for the experiments for structured classification tasks, the ADF method0 20 40 60 80 1008283848586878889 Sentiment (ADF vs. on-line)Number of PassesAccuracy (%)ADFSGDPerc0 20 40 60 808283848586878889 Sentiment (ADF vs. on-line)Training Time (sec)Accuracy (%)ADFSGDPerc0 50 100 1508283848586878889 Sentiment (ADF vs. batch)Number of PassesAccuracy (%)ADFLBFGS0 50 100 1508283848586878889 Sentiment (ADF vs. batch)Training Time (sec)Accuracy (%)ADFLBFGSFigure 6F-score curves on sentiment classification.
(Top Row) Comparisons among the ADF method andon-line training baselines, based on training passes and wall-clock time, respectively.
(BottomRow) Comparisons between the ADF method and the batch training method LBFGS, based ontraining passes and wall-clock time, respectively.
As we can see, the ADF method outperformsboth the on-line training baselines and the batch training baseline, with better accuracy andfaster convergence speed.581Computational Linguistics Volume 40, Number 3Figure 7One-pass learning results on sentiment classification.outperforms the baseline methods on one-pass learning, with more than 12.7% errorrate reduction.5.
ProofsThis section gives proofs of Theorems 1?4.Proof of Theorem 1 Following Equation (5), the ADF update rule is F(wt) := wt+1 =wt +?
?
gt.
For ?wt ?
X ,|F(wt+1)?
F(wt)|= |F(wt+1)?wt+1|= |wt+1 +?
?
gt+1 ?wt+1|= |?
?
gt+1|= [(a1b1)2 + (a2b2)2 + ?
?
?+ (af bf )2]1/2?
[(?maxb1)2 + (?maxb2)2 + ?
?
?+ (?maxbf )2]1/2= |?maxgt+1|= |FSGD(wt+1)?
FSGD(wt)|(6)where ai and bi are the ith elements of the vector ?
and gt+1, respectively.
FSGD is theSGD update rule with the fixed learning rate ?max such that ?max := sup{?i where ?i ??}.
In other words, for the SGD update rule FSGD, the fixed learning rate ?max is derivedfrom the ADF update rule.
According to Lemma 1, the SGD update rule FSGD is acontraction mapping in Euclidean space with Lipschitz continuity degree 1?
?max/?2,given the condition that ?max ?
(||x2i || ?
||?y?
`(xi, yi, y?)||Lip)?1.
Hence, it goes to|FSGD(wt+1)?
FSGD(wt)| ?
(1?
?max/?2)|wt+1 ?wt| (7)Combining Equations (6) and (7), it goes to|F(wt+1)?
F(wt)| ?
(1?
?max/?2)|wt+1 ?wt|Thus, according to the definition of dynamic contraction mapping, the ADF update ruleis a dynamic contraction mapping in Euclidean space with Lipschitz continuity degree1?
?max/?2.
ut582Sun et al.
Feature-Frequency?Adaptive On-line Training for Natural Language ProcessingProof of Theorem 2 As presented in Equation (5), the ADF update rule is F(wt) :=wt+1 = wt +?t ?
gt.
For ?wt ?
X ,|F(wt+1)?
F(wt)|= |?t+1 ?
gt+1|= [(a1b1)2 + (a2b2)2 + ?
?
?+ (af bf )2]1/2?
[(?maxb1)2 + (?maxb2)2 + ?
?
?+ (?maxbf )2]1/2= |FSGD(wt+1)?
FSGD(wt)|(8)where ai is the ith element of the vector ?t+1.
bi and FSGD are the same as before.Similar to the analysis of Theorem 1, the third step of Equation (8) is valid because ?maxis the maximum learning rate at the beginning and all learning rates are decreasingwhen t is increasing.
The proof can be easily derived following the same steps in theproof of Theorem 1.
To avoid redundancy, we do not repeat the derivation.
utProof of Theorem 3 Let M be the accumulative change of the ADF weight vectorwt:Mt :=?t?=1,2,...,t|wt?+1 ?wt?
|To prove the convergence of the ADF, we need to prove the sequence Mt converges ast??.
Following Theorem 2, we have the following formula for the ADF training:|F(wt+1)?
F(wt)| ?
(1?
?max/?2)|wt+1 ?wt|where ?max is the maximum learning rate at the beginning.
Let d0 := |w2 ?w1| andq := 1?
?max/?2, then we have:Mt =?t?=1,2,...,t|wt?+1 ?wt?
|?
d0 + d0q+ d0q2 + ?
?
?+ d0qt?1= d0(1?
qt)/(1?
q)(9)When t?
?, d0(1?
qt)/(1?
q) goes to d0/(1?
q) because q < 1.
Hence, we have:Mt ?
d0/(1?
q)Thus, Mt is upper-bounded.
Because we know that Mt is a monotonically increasingfunction when t?
?, it follows that Mt converges when t??.
This completes theproof.
ut583Computational Linguistics Volume 40, Number 3Proof of Theorem 4 First, we haveeigen(Ct) =t?m=1(1??0?m?)?
exp{?
?0?t?m=1?m}Then, we have0 ?n?j=1(1?
aj) ?n?j=1e?aj = e?
?nj=1 ajThis is because 1?
aj ?
e?aj given 0 ?
aj < 1.
Finally, because?tm=1 ?m ??1??
whent?
?, we haveeigen(Ct) ?
exp{??0?t?m=1?m}?
exp{??0??1?
?
}This completes the proof.
ut6.
ConclusionsIn this work we tried to simultaneously improve the training speed andmodel accuracyof natural language processing systems.
We proposed the ADF on-line training method,based on the core idea that high frequency features should result in a learning rate thatdecays faster.
We demonstrated that the ADF on-line training method is convergentand has good theoretical properties.
Based on empirical experiments, we can state thefollowing conclusions.
First, the ADF method achieved the major target of this work:faster training speed and higher accuracy at the same time.
Second, the ADF methodwas robust: It had good performance on several structured and non-structured classifi-cation tasks with very different characteristics.
Third, the ADF method worked well onboth binary features and real-valued features.
Fourth, the ADF method outperformedexisting methods in a one-pass learning setting.
Finally, our method achieved state-of-the-art performance on several well-known benchmark tasks.
To the best of ourknowledge, our simple method achieved a much better F-score than the existing bestreports on the biomedical named entity recognition task.AcknowledgmentsThis work was supported by the NationalNatural Science Foundation of China(no.
61300063, no.
61370117), the DoctoralFund of Ministry of Education of China(no.
20130001120004), a Hong KongPolytechnic University internal grant(4-ZZD5), a Hong Kong RGC Project(no.
PolyU 5230/08E), the National HighTechnology Research and DevelopmentProgram of China (863 Program,no.
2012AA011101), and the Major NationalSocial Science Fund of China (no.
12&ZD227).This work is a substantial extension of theconference version presented at ACL 2012(Sun, Wang, and Li 2012).584Sun et al.
Feature-Frequency?Adaptive On-line Training for Natural Language ProcessingReferencesBerger, Adam L., Vincent J. Della Pietra, andStephen A. Della Pietra.
1996.
A maximumentropy approach to natural languageprocessing.
Computational Linguistics,22(1):39?71.Blitzer, John, Mark Dredze, and FernandoPereira.
2007.
Biographies, Bollywood,boom-boxes and blenders: Domainadaptation for sentiment classification.In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics,pages 440?447, Prague.Bottou, Le?on.
1998.
Online algorithmsand stochastic approximations.
InD.
Saad, editor.
Online Learning and NeuralNetworks.
Cambridge University Press,pages 9?42.Chen, Stanley F. and Ronald Rosenfeld.1999.
A Gaussian prior for smoothingmaximum entropy models.
TechnicalReport CMU-CS-99-108, CarnegieMellon University.Collins, Michael.
2002.
Discriminativetraining methods for hidden Markovmodels: Theory and experiments withperceptron algorithms.
In Proceedings ofEMNLP?02, pages 1?8, Philadelphia, PA.Crammer, Koby, Alex Kulesza, and MarkDredze.
2009.
Adaptive regularization ofweight vectors.
In NIPS?09, pages 414?422,Vancouver.Dredze, Mark, Koby Crammer, andFernando Pereira.
2008.
Confidence-weighted linear classification.
InProceedings of ICML?08, pages 264?271,Helsinki.Duchi, John, Elad Hazan, and Yoram Singer.2010.
Adaptive subgradient methodsfor online learning and stochasticoptimization.
Journal of MachineLearning Research, 12:2,121?2,159.Finkel, Jenny, Shipra Dingare, Huy Nguyen,Malvina Nissim, Christopher Manning,and Gail Sinclair.
2004.
Exploiting contextfor biomedical entity recognition: Fromsyntax to the Web.
In Proceedings ofBioNLP?04, pages 91?94, Geneva.Freund, Yoav and Robert Schapire.
1999.Large margin classification using theperceptron algorithm.
Machine Learning,37(3):277?296.Gao, Jianfeng, Galen Andrew, Mark Johnson,and Kristina Toutanova.
2007.
A comparativestudy of parameter estimation methods forstatistical natural language processing.
InProceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics(ACL?07), pages 824?831, Prague.Hsu, Chun-Nan, Han-Shen Huang, Yu-MingChang, and Yuh-Jye Lee.
2009.
Periodicstep-size adaptation in second-ordergradient descent for single-pass on-linestructured learning.
Machine Learning,77(2-3):195?224.Jacobs, Robert A.
1988.
Increased rates ofconvergence through learning rateadaptation.
Neural Networks, 1(4):295?307.Kim, Jin-Dong, Tomoko Ohta, YoshimasaTsuruoka, and Yuka Tateisi.
2004.Introduction to the bio-entity recognitiontask at JNLPBA.
In Proceedings ofBioNLP?04, pages 70?75, Geneva.Kudo, Taku and Yuji Matsumoto.
2001.Chunking with support vector machines.In Proceedings of NAACL?01, pages 1?8,Pittsburgh, PA.Lafferty, John, Andrew McCallum, andFernando Pereira.
2001.
Conditionalrandom fields: Probabilistic models forsegmenting and labeling sequencedata.
In ICML?01, pages 282?289,Williamstown, MA.McDonald, Ryan, Koby Crammer, andFernando Pereira.
2005.
Flexible textsegmentation with structured multilabelclassification.
In Proceedings of HLT/EMNLP?05, pages 987?994, Vancouver.McMahan, H. Brendan and Matthew J.Streeter.
2010.
Adaptive boundoptimization for online convexoptimization.
In Proceedings of COLT?10,pages 244?256, Haifa.Murata, Noboru.
1998.
A statistical studyof on-line learning.
In D. Saad, editor.Online Learning in Neural Networks.Cambridge University Press, pages 63?92.Nocedal, Jorge and Stephen J. Wright.1999.
Numerical optimization.
Springer.Okanohara, Daisuke, Yusuke Miyao,Yoshimasa Tsuruoka, and Jun?ichi Tsujii.2006.
Improving the scalability ofsemi-Markov conditional randomfields for named entity recognition.In Proceedings of COLING-ACL?06,pages 465?472, Sydney.Ratnaparkhi, Adwait.
1996.
A maximumentropy model for part-of-speech tagging.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing1996, pages 133?142, Pennsylvania.Sang, Erik Tjong Kim and Sabine Buchholz.2000.
Introduction to the CoNLL-2000shared task: Chunking.
In Proceedingsof CoNLL?00, pages 127?132, Lisbon.Schaul, Tom, Sixin Zhang, and Yann LeCun.2012.
No more pesky learning rates.
CoRR,abs/1206.1106.585Computational Linguistics Volume 40, Number 3Settles, Burr.
2004.
Biomedical named entityrecognition using conditional randomfields and rich feature sets.
In Proceedingsof BioNLP?04, pages 104?107, Geneva.Shalev-Shwartz, Shai, Yoram Singer, andNathan Srebro.
2007.
Pegasos: Primalestimated sub-gradient solver for SVM.In Proceedings of ICML?07, pages 807?814,Corvallis, OR.Sperduti, Alessandro and Antonina Starita.1993.
Speed up learning and networkoptimization with extended backpropagation.
Neural Networks, 6(3):365?383.Sun, Weiwei.
2010.
Word-based andcharacter-based word segmentationmodels: Comparison and combination.In COLING?10 (Posters), pages 1,211?1,219,Beijing.Sun, Xu, Takuya Matsuzaki, and Wenjie Li.2013.
Latent structured perceptronsfor large-scale learning with hiddeninformation.
IEEE Transactions onKnowledge and Data Engineering,25(9):2,063?2,075.Sun, Xu, Takuya Matsuzaki, DaisukeOkanohara, and Jun?ichi Tsujii.
2009.Latent variable perceptron algorithm forstructured classification.
In Proceedingsof the 21st International Joint Conferenceon Artificial Intelligence (IJCAI 2009),pages 1,236?1,242, Pasadena, CA.Sun, Xu, Louis-Philippe Morency, DaisukeOkanohara, and Jun?ichi Tsujii.
2008.Modeling latent-dynamic in shallowparsing: A latent conditional model withimproved inference.
In Proceedings ofCOLING?08, pages 841?848, Manchester.Sun, Xu, Houfeng Wang, and Wenjie Li.
2012.Fast online training with frequency-adaptive learning rates for Chinese wordsegmentation and new word detection.In Proceedings of ACL?12, pages 253?262,Jeju Island.Sun, Xu, Yaozhong Zhang, TakuyaMatsuzaki, Yoshimasa Tsuruoka, andJun?ichi Tsujii.
2009.
A discriminativelatent variable Chinese segmenter withhybrid word/character information.In Proceedings of NAACL-HLT?09,pages 56?64, Boulder, CO.Sun, Xu, Yao Zhong Zhang, TakuyaMatsuzaki, Yoshimasa Tsuruoka, andJun?ichi Tsujii.
2013.
Probabilistic Chineseword segmentation with non-localinformation and stochastic training.Information Processing & Management,49(3):626?636.Tseng, Huihsin, Pichuan Chang, GalenAndrew, Daniel Jurafsky, and ChristopherManning.
2005.
A conditional randomfield word segmenter for SIGHAN bakeoff2005.
In Proceedings of the Fourth SIGHANWorkshop, pages 168?171, Jeju Island.Tsuruoka, Yoshimasa, Jun?ichi Tsujii, andSophia Ananiadou.
2009.
Stochasticgradient descent training forl1-regularized log-linear models withcumulative penalty.
In Proceedings ofACL?09, pages 477?485, Suntec.Vishwanathan, S. V. N., Nicol N.Schraudolph, Mark W. Schmidt, andKevin P. Murphy.
2006.
Acceleratedtraining of conditional randomfields with stochastic meta-descent.In Proceedings of ICML?06, pages 969?976,Pittsburgh, PA.Zhang, Ruiqiang, Genichiro Kikui, andEiichiro Sumita.
2006.
Subword-basedtagging by conditional random fields forChinese word segmentation.
In Proceedingsof the Human Language TechnologyConference of the NAACL, CompanionVolume: Short Papers, pages 193?196,New York City.Zhang, Yue and Stephen Clark.
2007.Chinese segmentation with a word-basedperceptron algorithm.
In Proceedings of the45th Annual Meeting of the Association ofComputational Linguistics, pages 840?847,Prague.Zhao, Hai, Changning Huang, Mu Li,and Bao-Liang Lu.
2010.
A unifiedcharacter-based tagging framework forChinese word segmentation.
ACMTransactions on Asian Language InformationProcessing, 9(2): Article 5.Zhao, Hai and Chunyu Kit.
2011.
Integratingunsupervised and supervised wordsegmentation: The role of goodnessmeasures.
Information Sciences,181(1):163?183.Zinkevich, Martin, Markus Weimer,Alexander J. Smola, and Lihong Li.2010.
Parallelized stochastic gradientdescent.
In Proceedings of NIPS?10,pages 2,595?2,603, Vancouver.586
