Structural Ambiguity and LexicalRelationsDona ld  H ind le*AT&T Bell LaboratoriesMats  Rooth  tAT&T Bell LaboratoriesWe propose that many ambiguous prepositional phrase attachments can be resolved on the basisof the relative strength of association of the preposition with verbal and nominal heads, estimatedon the basis of distribution in an automatically parsed corpus.
This suggests that a distributionalapproach can provide an approximate solution to parsing problems that, in the worst case, callfor complex reasoning.1.
IntroductionPrepositional phrase attachment is the canonical case of structural ambiguity, as in thetimeworn example:Example 1I saw the man with the telescope.An analysis where the prepositional phrase \[pp with the telescope\] is part of the objectnoun phrase has the semantics "the man who had the telescope"; an analysis where thePP has a higher attachment (perhaps as daughter of VP) is associated with a semanticswhere the seeing is achieved by means of a telescope.
The existence of such ambiguityraises problems for language models.
It looks like it might require extremely complexcomputation todetermine what attaches to what.
Indeed, one recent proposal suggeststhat resolving attachment ambiguity requires the construction of a discourse model inwhich the entities referred to in a text are represented and reasoned about (Altmannand Steedman 1988).
We take this argument to show that reasoning essentially involv-ing reference in a discourse model is implicated in resolving attachment ambiguitiesin a certain class of cases.
If this phenomenon is typical, there is little hope in thenear term for building computational models capable of resolving such ambiguities inunrestricted text.1.1 Structure-Based Ambiguity ResolutionThere have been several structure-based proposals about ambiguity resolution in theliterature; they are particularly attractive because they are simple and don't demandcalculations in the semantic or discourse domains.
The two main ones are as follows.?
Right Association--a constituent tends to attach to another constituentimmediately to its right (Kimball 1973).
* AT&T Bell Laboratories, 600 Mountain Ave., Murray Hill, NJ 07974, USA.f The new affiliation of the second author is: Institut ffir maschinelle Sprachverarbeitung, U iversit/itStuttgart.
(~) 1993 Association for Computational LinguisticsComputational Linguistics Volume 19, Number 1?
Minimal Attachment--a constituent tends to attach to an existingnonterminal using the fewest additional syntactic nodes (Frazier 1978).For the particular case we are concerned with, attachment of a prepositional phrasein a verb + object context as in Example 1, these two principles--at least given theversion of syntax that Frazier assumes--make opposite predictions: Right Associationpredicts noun attachment, while Minimal Attachment predicts verb attachment.Psycholinguistic work on structure-based strategies i primarily concerned withmodeling the time course of parsing and disambiguation, and acknowledges that otherinformation enters into determining a final parse.
Still, one can ask what informationis relevant o determining a final parse, and it seems that in this domain structure-based disambiguation is not a very good predictor.
A recent study of attachmentof prepositional phrases in a sample of written responses to a "Wizard of Oz" travelinformation experiment shows that neither Right Association or Minimal Attachmentaccounts for more than 55% of the cases (Whittemore, Ferrara, and Brunner 1990).
Andexperiments by Taraban and McClelland (1988) show that the structural models arenot in fact good predictors of people's behavior in resolving ambiguity.1.2 Resolving Ambiguity through Lexical AssociationsWhittemore, Ferrara, and Brunner (1990) found lexical preferences to be the key toresolving attachment ambiguity.
Similarly, Taraban and McClelland found that lexicalcontent was key in explaining people's behavior.
Various previous proposals for guid-ing attachment disambiguation by the lexical content of specific words have appeared(e.g.
Ford, Bresnan, and Kaplan 1982; Marcus 1980).
Unfortunately, it is not clear wherethe necessary information about lexical preferences i  to be found.
Jenson and Binot(1987) describe the use of dictionary definitions for disambiguation, but dictionariesare typically rather uneven in their coverage.
In the Whittemore, Ferrara, and Brunnerstudy (1990), the judgment of attachment preferences had to be made by hand for thecases that their study covered; no precompiled list of lexical preferences was avail-able.
Thus, we are posed with the problem of how we can get a good list of lexicalpreferences.Our proposal is to use co-occurrence of verbs and nouns with prepositions in alarge body of text as an indicator of lexical preference.
Thus, for example, the prepo-sition to occurs frequently in the context send NP_, that is, after the object of the verbsend.
This is evidence of a lexical association of the verb send with to.
Similarly, fromoccurs frequently in the context withdrawal_, and this is evidence of a lexical asso-ciation of the noun withdrawal with the preposition from.
This kind of association isa symmetric notion: it provides no indication of whether the preposition is selectingthe verbal or nominal head, or vice versa.
We will treat the association as a propertyof the pair of words.
It is a separate issue, which we will not be concerned with inthe initial part of this paper, to assign the association to a particular linguistic licens-ing relation.
The suggestion that we want to explore is that the association revealedby textual distribution--whether its source is a complementation relation, a modifica-tion relation, or something else--gives us information eeded to resolve prepositionalattachment in the majority of cases.2.
Discovering Lexical Association in TextA 13 million-word sample of Associated Press news stories from 1989 were auto-matically parsed by the Fidditch parser (Hindle 1983 and in press), using Church's104Donald Hindle and Mats Rooth Structural Ambiguity and Lexical RelationsTable 1A sample of NP heads, preceding verbs, andfollowing prepositions derived from the parsedcorpus.Verb Noun Prep Syntaxa.
change in -Vb.
regulationc.
aim PRO-+ atd.
remedy shortage ofe.
good inf.
DART-PNPg.
assuage citizenh.
scarcity ofi.
item asj.
wiperk.
VING1.
VINGpart-of-speech analyzer as a preprocessor (Church 1988), a combination that we willcall simply "the parser."
The parser produces a single partial syntactic descriptionof a sentence.
Consider Example 2, and its parsed representation in Example 3.
Theinformation in the tree representation is partial in the sense that some attachmentinformation is missing: the nodes dominated by "?"
have not been integrated intothe syntactic representation.
Note in particular that many PPs have not been attached.This is a symptom of the fact that the parser does not (in many cases) have the kind oflexical information that we have just claimed is required in resolving PP attachment.Example 2The radical changes in export and customs regulations evidently are aimed at rem-edying an extreme shortage of consumer goods in the Soviet Union and assuagingcitizens angry over the scarcity of such basic items as soap and windshield wipers.From the syntactic analysis provided by the parser, we extracted a table containingthe heads of all noun phrases.
For each noun phrase head, we recorded the follow-ing preposition if any occurred (ignoring whether or not the parser had attached thepreposition to the noun phrase), and the preceding verb if the noun phrase was theobject of that verb.
The entries in Table 1 are those generated from the text above.Each noun phrase in Example 3 is associated with an entry in the Noun column ofthe table.
Usually this is simply the root of the head of the noun phrase: good is theroot of the head of consumer goods.
Noun phrases with no head, or where the head isnot a common oun, are coded in a special way: DARTopNP represents a noun phrasebeginning with a definite article and headed by a proper noun, and VING represents agerundive noun phrase.
PRO-+ represents he empty category which, in the syntactictheory underlying the parser, is assumed to be the object of the passive verb aimed.In cases where a prepositional phrase follows the noun phrase, the head prepositionappears in the Prep column; attached and unattached prepositional phrases generatethe same kinds of entries.
If the noun phrase is an object, the root of the governingverb appears in the Verb column: aim is the root of aimed, the verb governing the empty105Computat iona l  L ingu is t i cs  Vo lume 19, Number  1Example  3I INP AUX VPDART NBAR PP ADV TNS VPRES VPPRT NP,v_ 7 , , , ,The evidently are aimed pro+ADJ NPL PREP NPI 1 I Iradical changes in NBARII IN NPLII \ ]regulationsN CONJ NPLI I Iexport and customs?
?
?I I IPP PP CONJPPREP NP PKEP NP CONJ NPI I I I I Iat VP in I I and VP\] DART NBARI I I I I IVING NP the PNP VING NPremedying I I assuaging NBARIART NBAR PP PNP PNP \]an Soviet Union \]ADJ N PR,EP NP citizensI I I iextreme shortage of NBARN NPLI Iconsumer goods?
?
?
?I I I IADJP PP PP FINI I I I ADJI I I I .\] PREP NP PREP NPangry I ii I 1?ver I I as NBARDART NBAR PP \[I I _A_  I Ithe N I \ ]  N NPL\] PREPNP I Iscarcity \[ \[ \] I wipersof NBAR N CONJ NI I I\[ \[ soap and windshieldADJ N NPLI I isuch basic items106Donald  H ind le  and  Mats Rooth Structural  Ambigu i ty  and  Lexical Relat ionscategory \[fRo +\].
The last column in the table, labeled Syntax, marks with the symbol-V all cases where there is no preceding verb that might license the preposition: theinitial subject of Example 2 is such a case.In the 13 million-word sample, 2,661,872 noun phrases were identified.
Of these,467,920 were recognized as the object of a verb, and 753,843 were followed by apreposition.
Of the object noun phrases identified, 223,666 were ambiguous verb-noun-preposition triples.3.
Estimating AssociationsThe table of verbs, nouns, and prepositions i in several respects an imperfect sourceof information about lexical associations.
First, the parser gives us incorrect analysesin some cases.
For instance, in the analysis partially described in Example 4a, theparser incorrectly classified probes as a verb, resulting in a table entry probe lightningin.
Similarly, in Example 4b, the infinitival marker to has been misidentified as apreposition.Example 4a.
\[NpThe space\] \[w~s probes\] [Npdetected lightning\] \[pp in Jupiter's upperatmosphere\] and observed auroral emissions like Earth's northern lightsin the Jovian polar regions.b.
The Bush administration told Congress on Tuesday it wants to\[v preserve\] \[Npthe right\] \[~ \[~ to\] control entry\] to the United States ofanyone who was ever a Communist.Second, a preposition in an entry might be structurally related to neither the nounof the entry nor the verb (if there is one), even if the entry is derived from a correctparse.'
For instance, the phrase headed by the preposition might have a higher locusof attachment:Example 5a.
The Supreme Court today agreed to consider einstating the murderconviction of a New York City man who confessed to \[v,NG killing\] \[Nrhisformer girlfriend\] \[p after\] police illegally arrested him at his home.b.
NBC was so afraid of hostile advocacy groups and unnerving advertisersthat it shot its dramatization of the landmark court case that\[VPAST legalized\] [Nrabortion\] \[Pr under two phony script titles\]The temporal phrase headed by after modifies confess, but given the procedure de-scribed above, Example 5a results in a tuple kill girlfriend after.
In the second example,a tuple legalize abortion under is extracted, although the PP headed by under modifiesthe higher verb shot.Finally, entries of the form verb noun preposition do not tell us whether to inducea lexical association between verb and preposition or between oun and preposition.
Wewill view the first two problems as noise that we do not have the means to eliminate,1 For present purposes, we can consider a parse correct if it contains no incorrect information in therelevant area.
Provided the PPs in Example 5 are unattached, the parses would be correct in this sense.The incorrect information is added by our table construction step, which (given our interpretation f thetable) assumes that a preposition following an object NP modifies either the NP or its governing verb.107Computational Linguistics Volume 19, Number 1and partially address the third problem in a procedure we will now describe.
We wantto use the verb-noun-preposition table to derive a table of bigrams counts, where abigram is a pair consisting of a noun or verb and an associated preposition (or nopreposition).
To do this we need to try to assign each preposition that occurs either tothe noun or to the verb that it occurs with.
In some cases it is fairly certain whether thepreposition attaches to the noun or the verb; in other cases, this is far less certain.
Ourapproach is to assign the clear cases first, then to use these to decide the unclear casesthat can be decided, and finally to divide the data in the remaining unresolved casesbetween the two hypotheses (verb and noun attachment).
The procedure for assigningprepositions i as follows:.......No Preposition--if  there is no preposition, the noun or verb is simplyentered with a special symbol NULL, conceived of as the nullpreposition.
(Items b, f, g, and j-1 in Table 1 are assigned).Sure Verb Attach 1--the preposition is attached to the verb if the nounphrase head is a pronoun.Sure Verb Attach 2---the preposition is attached to the verb if the verb ispassivized, unless the preposition is by.
The instances of by following apassive verb were left unassigned.
(Item c in Table 1 is assigned).Sure Noun Attach--the preposition is attached to the noun, if the nounphrase occurs in a context where no verb could license the prepositionalphrase, specifically if the noun phrase is in a subject or other pre-verbalposition.
The required syntactic information is present in the last columnof the table derived from the parse.
(Item a in Table 1 is assigned.
)Ambiguous Attach 1--Using the table of attachments computed so far, ifthe LA-score for the ambiguity (a score that compares the probability ofnoun versus verb attachment, as described below) is greater than 2.0 orless than -2.0, then assign the preposition according to the LA-score.Iterate until this step produces no new attachments.
(Item d in Table 1may be assigned.
)Ambiguous Attach 2-- for  the remaining ambiguous triples, split thedatum between the noun and the verb, assigning a count of .5 to thenoun-preposition pair and .5 to the verb-preposition pair.
(Item d inTable 1 is assigned, if not assigned in the previous step.
)Unsure Attach--assign remaining pairs to the noun.
(Items e, h, and i inTable 1 are assigned.
)This procedure gives us bigram counts representing the frequency with which a givennoun occurs associated with an immediately following preposition (or no preposition),or a given verb occurs in a transitive use and is associated with a preposition imme-diately following the object of the verb.
We use the following notation: f(w, p) is thefrequency count for the pair consisting of the verb or noun w and the preposition p.The unigram frequency count for the word w (either a verb, noun, or preposition) canbe viewed as a sum of bigram frequencies, and is written f(w).
For instance, if p is apreposition, f(p) = ~wf~W, p).108Donald Hindle and Mats Rooth Structural Ambiguity and Lexical Relations3.1 The Procedure for Guessing AttachmentOur object is to develop a procedure to guess whether a preposition is attached tothe verb or its object when a verb and its object are followed by a preposition.
Weassume that in each case of attachment ambiguity, there is a forced choice betweentwo outcomes: the preposition attaches either to the verb or to the noun.
2 For example,in Example 6, we want to choose between two possibilities: either into is attached tothe verb send or it is attached to the noun soldier.Example 6Moscow sent more than 100,000 soldiers into Afghanistan ...In particular, we want to choose between two structures:Example 7a.
verb_attach structure: \[vpsend \[NP" soldier NULL\] \[pp into ..\] ..\]b. noun_attach structure: \[vpsend \[NP" soldier \[Fpinto ..\]\] .. \]For the verb_attach case, we require not only that the preposition attach to the verbsend but also that the noun soldier have no following prepositional phrase attached:since into directly follows the head of the object noun phrase, there is no room forany post-modifier of the noun soldier.
We use the notation NULL to emphasize thatin order for a preposition licensed by the verb to be in the immediately postnominalposition, the noun must have no following complements (or adjuncts).
For the case ofnoun attachment, the verb may or may not have additional prepositional complementsfollowing the prepositional phrase associated with the noun.Since we have a forced choice between two outcomes, it is appropriate to usea likelihood ratio to compare the attachment probabilities (cf.
Mosteller and Wallace1964).
3 In particular, we look at the log of the ratio of the probability of verb_attachto the probability of noun_attach.
We will call this log likelihood ratio the LA (lexicalassociation) score.P(verb~ttach p \[ v, n)LA(v, n,p) = log 2 P(noun_attach p \[ v, n)For the current example,P(verb_attach into \[ sendv, soldierN) ,~ P( into\[sendv ) ?
P(NULL\[soldierN)andP(noun_attach into \[ sendv, soldierN) ~, P(into\[soldierN).Again, the probability of noun attachment does not involve a term indicating that theverb sponsors no (additional) complement; when we observe a prepositional phrasethat is in fact attached to the object NP, the verb might or might not have a complementor adjunct following the object phrase.2 Thus we are ignoring the fact that the preposition may in fact be licensed by neither the verb nor thenoun, as in Example 5.3 In earlier versions of this paper we used a t-test for deciding attachment and a different procedure forestimating the probabilities.
The current procedure has several advantages.
Unlike the t-test usedpreviously, it is sensitive to the magnitude of the difference between the two probabilities, not to ourconfidence in our ability to estimate those probabilities accurately.
And our estimation procedure hasthe property that it defaults (in case of novel words) to the average behavior for nouns or verbs, forinstance, reflecting a default preference with of for noun attachment.109Computational Linguistics Volume 19, Number 1We can estimate these probabilities from the table of co-occurrence counts as: 4P(into\[sendv)P (NULL Jsoldier N )P( into\[soldierN ) ,-~f(sendv, into) 86- - .049 f(sendv) 1742.5f(soldierN, NULL) 1182f(soldierN) 1478 - -  - .800f(soldierN, into) 1- - -  - .0007 f (soldierN) 1478Thus, the LA score for this example is:LA ( sendv , soldierN , into) = log 2 .049 *.800.00075.81The LA score has several useful properties.
The sign indicates which possibility,verb attachment or noun attachment, is more likely; an LA score of zero means theyare equally likely.
The magnitude of the score indicates how much more probable oneoutcome is than the other.
For example, if the LA score is 2.0, then the probabil ity ofverb attachment is four times greater than noun attachment.
Depending on the task,we can require a certain threshold of LA score magnitude before making a decision)As usual, in dealing with counts from corpora we must  confront the problem ofhow to estimate probabilities when counts are small.
The max imum likelihood estimatedescribed above is not very good when frequencies are small, and when frequenciesare zero, the formula will not work at all.
We use a crude adjustment to observedfrequencies that has the right general properties, though it is not likely to be a verygood estimate when frequencies are small.
For our purposes, however exploring ingeneral the relation of distribution in a corpus to attachment d isambiguat ion- -webelieve it is sufficient.
Other approaches to adjusting small frequencies are discussedin Church et al (1991) and Gale, Church, Yarowsky (in press).The idea is to use the typical association rates of nouns and verbs to interpolateour probabilities.
Where f(N, p) = En f(n, p), f(V, p) = ~vf(V,  p), f(N) -- End(n) and4 The nonintegral count for send is a consequence of the data-splitting step Ambiguous Attach 2, and thedefinition of unigram frequencies as a sum of bigram frequencies.5 An advantage of the likelihood ratio approach is that we can use it in a Bayesian discriminationframework to take into account other factors that might influence our decision about attachment (seeGale, Church, and Yarowsky \[in press\] for a discussion of this approach).
We know of course that otherinformation has a bearing on the attachment decision.
For example, we have observed that if the nounphrase object includes a superlative adjective as a premodifier, then noun attachment is certain (for asmall sample of 16 cases).
We could easily take this into account by setting the prior odds ratio toheavily favor noun attachment: let's suppose that if there is a superlative in the object noun phrase,then noun attachment is say 1000 times more probable than verb attachment; otherwise, they areequally probable.
Then following Mosteller and Wallace (1964), we assume thatFinal attachment odds = log 2(initial odds) + LA.In case there is no superlative in the object, the initial log odds will be zero (verb and noun attachmentare equally probable), and the final odds will equal our LA score.
If there is a superlative,1Final attachment odds = log 21-- ~ -}- LA(v, n, p).110Donald Hindle and Mats Rooth Structural Ambiguity and Lexical Relationsf(V) = ~vf(V),  we redefine our probability estimates in the following way:f(n,p) + f(N,p) f(N)P(p \[ n) = f(n) + 1f(v, p) + \[(V,p) f(v)P(P l v) = f(v) + 1When f(n) is zero, the estimate for P(p I n) is the average (~)  across all nouns, f(N)and similarly for verbs.
When f(n, p) is zero, the estimate used is proportional to thisaverage.
If we have seen only one case of a noun and it occurred with a prepositionp (that is f(n, p) = 1 and f(n) = 1 ), then our estimate is nearly cut in half.
This is thekind of effect we want, since under these circumstances we are not very confident in1 as an estimate of P(p I n).
When f(n, p) is large, the adjustment factor does not makemuch difference.
In general; this interpolation procedure adjusts small counts in theright direction and has little effect when counts are large.For our current example, this estimation procedure changes the LA score little:?
\ f (V , in to )  f(soldierN,NULL)+~LL) f (sendv,mto) + f~fl-ffy-LA(sendv, soldier~, into) = log 2 f(sendv)+l f(s?ldierN)+l.
.
f (N  in to )  f(s?l&erN~mt?
)+"f (soldier N) + 186a.
2292 1182q 2047311385435 26~594l ~510~21742.5+124341478+11-} 26565941478+1= 5.87.The LA score of 5.87 for this example is positive and therefore indicates verb attach-ment; the magnitude is large enough to suggest a strong preference for verb attach-ment.
This method of calculating the LA score was used both to decide unsure casesin building the bigram tables as described in Ambiguous Attach 1, and to make theattachment decisions in novel ambiguous cases, as discussed in the sections following.4.
Testing AttachmentTo evaluate the performance of the procedure, 1000 test sentences in which the parseridentified an ambiguous verb-noun-preposit ion triple were randomly selected fromAP news stories.
These sentences were selected from stories included in the 13 mill ion-word sample, but the particular sentences were excluded from the calculation of lexicalassociations.
The two authors first guessed attachments on the verb-noun-preposit iontriples, making a judgment on the basis of the three headwords alone.
The judges wererequired to make a choice in each instance.
This task is in essence the one that we willgive the computer- - to judge the attachment without any more information than thepreposition and the heads of the two possible attachment sites.This initial step provides a rough indication of what we might expect o be achiev-able based on the information our procedure is using.
We also wanted a standard ofcorrectness for the test sentences.
We again judged the attachment for the 1000 triples,111Computational Linguistics Volume 19, Number 1this time using the full-sentence context, first grading the test sentences separately,and then discussing examples on which there was disagreement.
Disambiguating thetest sample turned out to be a surprisingly difficult task.
While many decisions werestraightforward, more than 10% of the sentences seemed problematic to at least oneauthor.
There are several kinds of constructions where the attachment decision is notclear theoretically.
These include idioms as in Examples 8 and 9, light verb construc-tions (Example 10), and small clauses (Example 11).Example 8But over time, misery has given way to mending.Example 9The meeting will take place in Quantico.Example 10Bush has said he would not make cuts in Social Security.Example 11Sides said Francke kept a .38-caliber revolver in his car's glove compartment.In the case of idioms, we made the assignment on the basis of a guess about thesyntactic structure of the idiom, though this was sometimes difficult to judge.
Wechose always to assign light verb constructions to noun attachment, based on the factthat the noun supplies the lexical information about what prepositions are possible,and small clauses to verb attachment, based on the fact that this is a predicativeconstruction lexically licensed by the verb.Another difficulty arose with cases where there seemed to be a systematic se-mantically based indeterminacy about the attachment.
In the situation described byExample 12a, the bar and the described event or events are presumably in the samelocation, and so there is no semantic reason to decide on one attachment.
Example 12bshows a systematic benefactive indeterminacy: if you arrange something for someone,then the thing arranged is also for them.
The problem in Example 12c is that signingan agreement usually involves two participants who are also parties to the agreement.Example 13 gives some further examples drawn from another test sample.Example 12a .
.
.
.
known to frequent he same bars in one neighborhood.b.
Inaugural officials reportedly were trying to arrange a reunion for Bushand his old submarine buddies ...c. We have not signed a settlement agreement with them.Example 13a.
It said the rebels issued a challenge to soldiers in the area and fought withthem for 30 minutes.b.
The worst such attack came Nov. 11 when a death squad firingsubmachine guns killed 43 people in the northwest town of Segovia.c.
Another charge raised at the Contra news conference was that theSandinistas have mined roads along the Honduran and Costa Ricanborders.112Donald Hindle and Mats Rooth Structural Ambiguity and Lexical Relationsd.
Buckner said Control Data is in the process of negotiating a new lendingagreement with its banks.e .
.
.
.
which would require ailing banks and S&Ls to obtain advancepermission from regulators before raising high-cost deposits throughmoney brokers.f.
She said the organization has opened a cleaning center in Seward and shewas going to Kodiak to open another.In general, we can say that an attachment is semantically indeterminate if situationsthat verify the meaning associated with one attachment also make the meaning as-sociated with the other attachment true.
Even a substantial overlap (as opposed toidentity) between the classes of situations verifying the two meanings makes an at-tachment choice difficult.The problems in determining attachments are heterogeneous.
The idiom, lightverb, and small clause constructions represent cases where the simple distinction be-tween noun attachment and verb attachment perhaps does not make sense, or is verytheory-dependent.
It seems to us that the phenomenon of semantically based inde-terminacy deserves further exploration.
If it is often difficult to decide what licensesa prepositional phrase, we need to develop language models that appropriately cap-ture this.
For our present purpose, we decided to make an attachment choice in allcases, in some cases relying on controversial theoretical considerations, or relativelyunanalyzed intuitions.In addition to the problematic cases, 120 of the 1000 triples identified automati-cally as instances of the verb-object-preposition c figuration turned out in fact to beother constructions, often as the result of parsing errors.
Examples of this kind weregiven above, in the context of our description of the construction of the verb-noun-preposition table.
Some further misidentifications that showed up in the test sampleare: identifying the subject of the complement clause of say as its object, as in Exam-ple 10, which was identified as (say ministers from), and misparsing two constituents aa single-object noun phrase, as in Example 11, which was identified as (make subject o).Example 14a.
Ortega also said deputy foreign ministers from the five governmentswould meet Tuesday in Managua, ...b.
Congress made a deliberate choice to make this commission subject o theopen meeting requirements ...After agreeing on the 'correct' attachment for the test sample, we were left with 880 dis-ambiguated verb-noun-preposition triples, having discarded the examples that werenot instances of the relevant construction.
Of these, 586 are noun attachments and 294are verb attachments.4.1 Evaluating PerformanceFirst, consider how the simple structural attachment preference schemas perform atpredicting the outcome in our test set.
Right Association predicts noun attachmentand does better, since in our sample there are more noun attachments, but it still hasan error rate of 33%.
Minimal Attachment, interpreted as entailing verb attachment,has the complementary error rate of 67%.
Obviously, neither of these procedures iparticularly impressive.113Computational Linguistics Volume 19, Number 1Table 2Performance on the test sentences for two human judges and thelexical association procedure (LA).LA actual N actual V precision recallN guess 496 89 N .848 .846V guess 90 205 V .695 .697neither 0 0 combined .797 .797Judge 1 actual N actual V precision recallN guess 527 48 N .917 .899V guess 59 246 V .807 .837neither 0 0 combined .878 .878Judge 2 actual N actual V precision recallN guess 482 29 N .943 .823V guess 104 265 V .718 .901neither 0 0 combined .849 .849Now consider the performance of our lexical association (LA) procedure for the880 standard test sentences.
Table 2 shows the performance for the two human judgesand for the lexical association attachment procedure.
First, we note that ~he task ofjudging attachment on the basis of verb, noun, and preposition alone is not easy.
Thefigures in the entry labeled "combined precision" indicate that the human judges hadoverall error rates of 12-15%.
6 The lexical association procedure is somewhat worsethan the human judges, with an error rate of 20%, but this is an improvement overthe structural strategies.The table also gives results broken down according to N vs. V attachment.
Theprecision figures indicate the proportion of test items assigned to a given categorythat actually belong to the category.
For instance, N precision is the fraction of casesthat the procedure identified as N attachments hat actually were N attachments.
Therecall figures indicate the proportion of test items actually belonging to a given categorythat were assigned to that category: N precision is the fraction of actual N attachmentsthat were identified as N attachments.
The LA procedure recognized about 85% of the586 actual noun attachment examples as noun attachments, and about 70% of theactual verb attachments as verb attachments.If we restrict he lexical association procedure to choose attachment only in caseswhere the absolute value of the LA score is greater than 2.0 (an arbitrary thresholdindicating that the probability of one attachment is four times greater than the other),we get attachment judgments on 621 of the 880 test sentences, with overall precision ofabout 89%.
On these same examples, the judges also showed improvement, as evidentin Table 3.
76 Combined precision is the proportion of assigned items that were correctly assigned.
Combined recallis the proportion of all items that were correctly assigned.
These differ if some items are leftunassigned, asin Table 3.7 The recall figures for the judges do not have the usual interpretation, since the judges did not have theoption of passing on test items.
The ratios given in parentheses indicate the proportion of items in therelevant category that were both correctly labeled by the judge and had a likelihood ratio score withabsolute value greater than 2.0.
Thus these figures are of interest only in relation to the recall figuresfor LA.114Donald Hindle and Mats Rooth Structural Ambiguity and Lexical RelationsTable 3Performance for the LA procedure with a cutoff of 2 (\] LA I> 2.0), and forthe human judges on the sentences classified by LA.LA actual N actual V precision recallN guess 416 31 N .931 .710V guess 39 135 V .776 .459neither 131 128 combined .887 .626Judge 1 actual N actual V precision recallN guess 424 20 N .955 (.724)V guess 31 146 V .825 (.497)neither 131 128 combined .918 (.648)Judge 2 actual N actual V precision recallN guess 402 12 N .971 (.686)V guess 53 154 V .744 (.524)neither 131 128 combined .895 (.632)Table 4Precision recall tradeoff for the LA procedure.LA score cases covered precision recall16.0 103 (11.7%) 0.990 0.1168.0 298 (33.9) 0.966 0.3274.0 478 (54.3) 0.923 0.5013.0 530 (60.2) 0.917 0.5522.0 621 (70.6) 0.887 0.6261.5 666 (75.7) 0.871 0.6591.0 718 (81.6) 0.852 0.6950.5 796 (90.5) 0.823 0.7440.25 840 (95.5) 0.807 0.7700 880 (100.0) 0.797 0.797The fact that an LA score threshold improves precision indicates that the LA scoregives information about how confident we can be about an attachment choice.
Insome applications, this information is useful.
For instance, suppose that we wantedto incorporate the PP attachment procedure in a parser such as Fidditch.
It mightbe preferable to achieve increased precision in PP attachment, in return for leavingsome PPs unattached.
For this purpose, a threshold could be used.
Table 4 shows thecombined precision and recall levels at various LA thresholds.
It is clear that the LAscore can be used effectively to trade off precision and recall, with a floor for the forcedchoice at about 80%.A comparison of Table 3 with Table 2 indicates, however, that the decline in recall issevere for V attachment.
And in general, the performance of the LA procedure is worseon V attachment examples than on N attachments, according to both precision andrecall criteria.
The next section is concerned with a classification of the test examples,which gives insight into why performance on V attachments i  worse.115Computational Linguistics Volume 19, Number 1Table 5Performance ofthe lexical attachment procedure by underlyingrelationship.relation count %correctargument oun 378 92.1argument verb 104 84.6adjunct noun 91 74.7adjunct verb 101 64.4light verb 19 57.9small clause 13 76.9idiom 19 63.2locative indeterminacy 42 69.0systematic ndeterminacy 35 62.9other 78 61.54.2 Underlying RelationsOur model takes frequency of co-occurrence asevidence of an underlying relationshipbut makes no attempt to determine what sort of relationship is involved.
It is interest-ing to see what kinds of relationships are responsible for the associations the model isidentifying.
To investigate this we categorized the 880 triples according to the natureof the relationship underlying the attachment.
In many cases, the decision was diffi-cult.
The argument/adjunct distinction showed many gray cases between clear partici-pants in an action and clear adjuncts, such as temporal modifiers.
We made rough bestguesses to partition the cases into the following categories: argument, adjunct, idiom,small clause, systematic locative indeterminacy, other systematic ndeterminacy, andlight verb.
With this set of categories, 78 of the 880 cases remained so problematic thatwe assigned them to the category other.Table 5 shows the proportion of items in a given category that were assigned thecorrect attachment by the lexical association procedure.
Even granting the roughnessof the categorization, some clear patterns emerge.
Our approach is most successful atattaching arguments correctly.
Notice that the 378 noun arguments constitute 65% ofthe total 586 noun attachments, while the 104 verb arguments amount o only 35%of the 294 verb attachments.
Furthermore, performance with verb adjuncts is worsethan with noun adjuncts.
Thus much of the problem with V attachments noted in theprevious ection appears to be attributable to a problem with adjuncts, particularlyverbal ones.
Performance on verbal arguments remains worse than performance onnominal ones, however.The remaining cases are all complex in some way, and the performance is poor onthese classes, showing clearly the need for a more elaborated model of the syntacticstructure that is being identified.5.
Comparison with a DictionaryThe idea that lexical preference is a key factor in resolving structural ambiguity leadsus naturally to ask whether existing dictionaries can provide information relevant todisambiguation.
The Collins COBUILD English Language Dictionary (Sinclair et al1987) is useful for a comparison with the AP sample for several reasons: it was com-piled on the basis of a large text corpus, and thus may be less subject o idiosyncrasy116Donald Hindle and Mats Rooth Structural Ambiguity and Lexical RelationsTable 6Count of noun and verb associations for COBUILD and the AP sample.Source Total NOUN VERBCOBUILD 4,233 1,942 2,291AP sample 76,597 56,368 20,229AP sample (f > 3) 20,005 14,968 5,037AP sample 7,822 5,286 2,536(f > 3 and I > 2.0)COBUILD O AP 2,703 1,415 1,288COBUILD n AP 1,235 671 564(f > 3 and I > 2.0)than other works, and it provides, in a separate field, a direct indication of prepositionstypically associated with many nouns and verbs.
From a machine-readable v rsion ofthe dictionary, we extracted a list of 1,942 nouns associated with a particular preposi-tion, and of 2,291 verbs associated with a particular preposition after an object nounphrase.
8These 4,233 pairs are many fewer than the number of associations in the AP sample(see Table 6), even if we ignore the most infrequent pairs.
Of the total 76,597 pairs,20,005 have a frequency greater than 3, and 7,822 have a frequency that is greaterthan 3 and more than 4 times what one would predict on the basis of the unigramfrequencies of the noun or verb and the preposition.
9We can use the fixed lexicon of noun-preposit ion and verb-preposition associ-ations derived from COBUILD to choose attachment in our test set.
The COBUILDdictionary has information on 257 of the 880 test verb-noun-preposit ion triples.
In241 of those cases, there is information only on noun or only on verb association.
Inthese cases, we can use the dictionary to choose the attachment according to the asso-ciation indicated.
In the remaining 16 cases, associations between the preposition andboth the noun and the verb are recorded in the dictionary.
For these, we select nounattachment, since it is the more probable outcome in general.
For the remaining cases,we assume that the dictionary makes no decision.
Table 7 gives the results obtained8 We included particles in these associations, since these were also included in our corpus of bigrams.9 The latter condition is spelled out asf(w,p) > 4f(w) f(p)N N N ~where U is ~w, J (w,  p), the total number of token bigrams.
It is equivalent tow and p having amutual information (defined asI(w, p) = log 2 f@ f_~u )greater than 2.
This threshold of 2, of course, is an arbitrary cutoff.117Computational Linguistics Volume 19, Number 1Table 7Performance on the test sentences for a fixed lexicon extractedfrom the COBUILD dictionary and a fixed lexicon extracted fromthe AP sample.COBUILD actual N actual V precision recallN guess 132 17 n .886 .225V guess 41 67 v .620 .228neither 413 210 combined .774 .226AP Fixed actual N actual V precision recaHN guess 283 31 n .901 .483V guess 27 119 v .815 .405neither 276 144 combined .874 .457by this attachment procedure.
The precision figure is similar to that obtained by thelexical association procedure with a threshold of zero, but the recall is far lower: thedictionary provides insufficient information in most cases.Like the lexicon derived from the COBUILD dictionary, the fixed lexicon of 7,822corpus-derived associations derived from our bigram table as described above (thatis, all bigrams where f(w~ p) > 3 and I(w, p) > 2) contains categorical informationabout associations.
Using it for disambiguation i the way the COBUILD dictionarywas used gives the results indicated in Table 7.
The precision is similar to that whichwas achieved with the LA procedure with a threshold of 2, although the recall islower.
This suggests that while overall coverage of association pairs is important, theinformation about the relative strengths of associations contributing to the LA scoreis also significant.It must be noted that the dictionary information we derived from COBUILD wascomposed for people to use in printed form.
It seems likely that associations wereleft out because they did not serve this purpose in one way or another.
For instance,listing many infrequent or semantically predictable associations might be confusing.Furthermore, our procedure undoubtedly gained advantage from the fact that the testitems are drawn from the same body of text as the training corpus.
Nevertheless, theresults of this comparison suggest hat for the purpose of this paper, a partially parsedcorpus is a better source of information than a dictionary.
This conclusion should notbe overstated, however.
Table 6 showed that most of the associations in each lexiconare not found in the others.
Table 8 is a sample of a verb-preposit ion associationdictionary obtained by merging information from the AP sample and from COBUILD,illustrating both the common ground and the differences between the two lexicons.Each source of information provides intuitively important associations that are missingfrom the other.6.
D iscuss ionIn our judgment, the results of the lexical association procedure are good enoughto make it useful for some purposes, in particular for inclusion in a parser such asFidditch.
The fact that the LA score provides a measure of confidence increases thisusefulness, since in some applications (such as exploratory linguistic analysis of text118Donald Hindle and Mats Rooth Structural Ambiguity and Lexical RelationsTable 8Verb--(NP)-Preposition associations in the COBUILD dictionaryand in the AP sample (with f(v~ p) > 3 and I(v~ p) > 2.0).AP sample COBUILDapproach about as at withappropriate for forapprove because by for underapproximate toarbitrate betweenargue out witharm witharousearraign onarrange forarray inarrest forbeforewithinas in onfor so through withafter along_with atduring near onoutside whilearrogateascribeask about forassassinate inassault atassemble atassert overassign to 36assist inassociate withtotoabouttoin withwithcorpora) it is advantageous to be able to achieve increased precision in exchange fordiscarding a proportion of the data.From another perspective, our results are less good than what might be demanded.The performance of the human judges with access just to the verb-noun-preposit iontriple is a standard of what is possible based on this information, and the lexicalassociation procedure falls somewhat short of this standard.
The analysis of underlyingrelations indicated some particular areas in which the procedure did not do well, andwhere there is therefore room for improvemenf.
In particular, performance on adjunctswas poor.
A number of classes of adjuncts, such as temporal ones, are fairly easy toidentify once information about the object of the preposition is taken into account.Beginning with such an identification step (which could be conceived of as adding afeature such as \[+temporal\] to individual prepositions, or replacing individual tokenprepositions with an abstract emporal preposition) might yield a lexical associationprocedure that would do better with adjuncts.
But it is also possible that a procedurethat evaluates associations with individual nouns and verbs is simply inappropriatefor adjuncts.
This is an area for further investigation.This experiment was deliberately limited to one kind of attachment ambiguity.However, we expect that the method will be extendable to other instances of PPattachment ambiguity, such as the ambiguity that arises when several prepositionalphrases follow a subject NP, and to ambiguities involving other phrases, especiallyphrases uch as infinitives that have syntactic markers analogous to a preposition.119Computational Linguistics Volume 19, Number 1We began this paper by alluding to several approaches to PP attachment, specifi-cally work assuming the construction of discourse models, approaches based on struc-tural attachment preferences, and work indicating a dominant role for lexical prefer-ence.
Our results tend to confirm the importance of lexical preference.
However, wecan draw no firm conclusions about the other approaches.
Since our method yieldedincorrect results on roughly 20% of the cases, its coverage is far from complete.
Thisleaves a lot of work to be done, within both psycholinguistic and computational p-proaches.
Furthermore, as we noted above, contemporary psycholinguistic work isconcerned with modeling the time course of parsing.
Our experiment gives no infor-mation about how lexical preference information is exploited at this level of detail,or the importance of such information compared with other factors such as struc-tural .preferences at a given temporal stage of the human parsing process.
However,the numerical estimates of lexical association we have obtained may be relevant o apsycholinguistic nvestigation of this issue.AcknowledgmentsWe thank Bill Gale, Ken Church, and DavidYarowsky for many helpful discussions ofthis work and are grateful to four reviewersand Christian Rohrer for their comments onan earlier version.ReferencesAltmann, Gerry, and Steedman, Mark(1988).
"Interaction with context duringhuman sentence processing."
Cognition,30(3), 191-238.Church, Kenneth W. (1988).
"A stochasticparts program and noun phrase parser forunrestricted text."
In Proceedings, SecondConference on Applied Natural LanguageProcessing.
Austin, Texas, 136-143.Church, Kenneth; Gale, William; Hanks,Patrick; and Hindle, Donald (1991).
"Using statistics in lexical analysis."
InLexical Acquisition: Exploiting On-LineResources to Build a Lexicon, edited by UriZernik, 115-164.
Lawrence ErlbaumAssociates.Ford, Marilyn; Bresnan, Joan; and Kaplan,Ronald M. (1982).
"A competence-basedtheory of syntactic losure."
In The MentalRepresentation f Grammatical Relations,edited by Joan Bresnan, 727-796.
MITPress.Frazier, Lyn (1978).
"On comprehendingsentences: Syntactic parsing strategies.
"Doctoral dissertation, University ofConnecticut.Gale, William A.; Church, Kenneth W.; andYarowsky, David (In press).
"A methodfor disambiguating word senses in a largecorpus."
Computers and Humanities.Hindle, Donald (1983).
"User manual forfidditch, a deterministic parser."
NavalResearch Laboratory TechnicalMemorandum 7590-142, Naval ResearchLaboratory, Washington, D.C.Hindle, Donald (in press).
"A parser for textcorpora."
In Computational Approaches tothe Lexicon, edited by B. T. S. Atkins andA.
Zampolli.
Oxford University Press.Jenson, Karen, and Binot, Jean-Louis (1987).
"Disambiguating prepositional phraseattachments by using on-line dictionarydefinitions."
Computational Linguistics,13(3--4), 251-260.Kimball, J.
(1973).
"Seven principles ofsurface structure parsing in naturallanguage."
Cognition, 2, 15-47.Marcus, Mitchell P. (1980).
A Theory ofSyntactic Recognition for Natural Language.MIT Press.Mosteller, Frederick, and Wallace, David L.(1964).
Inference and Disputed Authorship:The Federalist.
Addison-Wesley.Sinclair, J.; Hanks, P.; Fox, G.; Moon, R.;Stock, P. et al (1987).
Collins COBUILDEnglish Language Dictionary.
Collins.Taraban, Roman, and McClelland, James L.(1988).
"Constituent attachment andthematic role assignment in sentenceprocessing: Influences of content-basedexpectations."
Journal of Memory andLanguage, 27, 597-632.Whittemore, Greg; Ferrara, Kathleen; andBrunner, Hans (1990).
"Empirical study ofpredictive powers of simple attachmentschemes for post-modifier prepositionalphrases."
Proceedings, 28th Annual Meetingof the Association for ComputationalLinguistics, 23-30.120
