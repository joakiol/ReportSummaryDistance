GENERALITY  AND OBJECTIV ITYCentral Issues in Putting aDialogue Evaluation Tool into Practical UseLaila Dybkj~er, Niels Ole Bemsen and Hans Dybkj~erThe Maersk Me-Kinney Moiler Institute for Production TechnologyOdense University, Campusvej 55, 5230 Odense M, Denmarkemai ls:  lai la@mip.ou.dk, nob@mip.ou.dk,  dybkjaer@mip.ou.dkphone: (+45) 65 57 35 44 fax: (+45) 66 15 76 97Abst ractThis paper presents a first set of test results onthe generality and objectivity of the DialogueEvaluation Tool DET.
Building on the assump-tion that most, if not all, dialogue design errorscan be viewed as problems of non-cooperativesystem behaviour, DET has two closely relatedaspects to its use.
Firstly, it may be used for thediagnostic evaluation of spoken human-machinedialogue.
Following the detection of miscommu-nication, DET enables in-depth classification ofmiscornmunication problems that are caused byflawed dialogue design and supports the repair ofthose problems, preventing their future occur-rence.
Secondly, DET can be used to guide earlydialogue design in order to prevent dialogue de-sign errors from occurring in the implementedsystem.
We describe the development and in-house testing of the tool, and present he resultsof ongoing work on testing its generality and ob-jectivity on an external corpus, i.e.
an early cor-pus from the Sundial project in spoken languagedialogue systems development.1.
IntroductionSpoken language technologies are being viewed as oneof the most important next steps towards truly naturalinteractive systems which are able to communicate withhumans the same way that humans communicate witheach other.
After more than a decade of promises thatversatile spoken language dialogue systems (SLDSs)using speaker-independent co tinuous speech recognitionwere just around the comer, the first such systems arenow in the market place.
These developments highlightthe needs for novel tools and methods that can supportefficient development and evaluation of SLDSs.There is currently no best practice methodologyavailable which specialises oftware engineering bestpractice to the particular purposes of dialogue ngineer-ing, that is, to the development and evaluation ofSLDSs.
In June 1997, a European Concerted Action,DISC (Spoken Language Dialogue Systems and Com-ponents - Best Practice in Development and Evaluation),will be launched with the goal of systematically address-ing this problem.
DISC aims to develop a first detailedand integrated set of development and evaluation meth-ods and procedures (guidelines, checklists, heuristics) forbest practice in the field of dialogue ngineering as wellas a range of much needed ialogue ngineering supportconcepts and software tools.
The goals of dialogue ngi-neering include optimisation of the user-friendliness ofSLDSs which will ultimately determine their rankamong emerging input/output technologies.
The presentpaper will present ongoing work on one of the tools thatare planned to result from DISC.It is a well-recognised fact that the production of anew software ngineering tool or method is difficult andtime consuming.
The difficulties lie not only in theinitial conception of, for instance, a new tool, or in tooldrafting and early in-house testing.
Even if these stagesyield encouraging results, there is a long way to go be-fore the tool can stand on its own and be used as an in-tegral part of best practice in the field.
One central rea-son why this is the case is the problem of generalisa-tion.
A tool which only works, or is only known towork, on a single system, in a highly restricted omainof application, or in special circumstances, i  of littleinterest to other developers.
In-house testing will inevi-tably be made on a limited number of systems and ap-plication domains and often is subject to other limita-tions of scope as well.
To achieve and demonstrate anacceptable degree of generality, the tool must be itera-tively developed and tested on systems and applicationdomains, and in circumstances that are significantlydifferent from those available in-house.
Achievement ofgenerality therefore requires access to other systems,corpora and/or development processes.
Such access isnotoriously difficult to obtain for several reasons, in-cluding commercial confidentiality, protection of in-house know-how and protection of developers' time.
Asecond reason why software ngineering tool or methoddevelopment is difficult and time consuming is the prob-lem of objectivity.
It is not sufficient hat some methodor tool has been trialled on many different cases and inwidely different conditions.
It must also have beenshown that different developers are able to use the newmethod or tool with approximately the same result onthe same corpus, system or development process.
Thebenefits from using a new tool or method should attachto that tool or method rather than to its originators.17,/Prior to the start of DISC, we have developed andtested a tool for dialogue design evaluation on an in-house SLDSs project (Bemsen et al 1996, Bemsen etal.
1997a).
The paper will present first test results onthe generality and objectivity of this tool called DET(Dialogue Evaluation Tool).
Building on the assumptionthat most, if not all, dialogue design errors can beviewed as problems of non-cooperative system behav-iour, DET has two closely related aspects to its use.Firstly, it may be used as part of a methodology fordiagnostic evaluation of spoken human-machine dia-logue.
Following the detection of human-machine mis-communication, DET enables in-depth classification ofmiscommunication problems that are caused by flaweddialogue design.
In addition, the tool supports the repair"of those problems, preventing their occurrence in futureuser interactions with the system.
Secondly, DET can beused to guide early dialogue design in order to preventdialogue design errors from occurring in the imple-mented system.
The distinction between use of DET fordiagnostic evaluation and as design guide mainly dependson the stage of systems development a which it is be-ing used.
When used prior to implementation, DET actsas a design guide; when applied to an implemented sys-tem, DET acts as a diagnostic evaluation tool.
In whatfollows, we describe the development and in-house test-ing of the tool (Section 2), present ongoing work ontesting its generality and objectivity (Section 3), andconclude the paper taking a look at the work ahead(Section 4).2.
Tool DevelopmentDET was developed in the course of designing, imple-menting and testing the dialogue model for the Danishdialogue system (Bernsen et al 1997b).
The system is awalk-up-and-use prototype SLDS for over-the-phoneticket reservation for Danish domestic flights.
The sys-tem's dialogue model was developed using the Wizard ofOz (WOZ) simulation method.
Based on the problems ofdialogue interaction observed in the WOZ corpus weestablished a set of guidelines for the design of coopera-tive spoken dialogue.
Each observed problem was con-sidered a case in which the system, in addressing theuser, had violated a guideline of cooperative dialogue.The WOZ corpus analysis led to the identification of 14guidelines of cooperative spoken human-machine dia-logue based on analysis of 120 examples of user-systeminteraction problems.
If those guidelines were observedin the design of the system's dialogue behaviour, weassumed, this would increase the smoothness of user-system interaction and reduce the amount of user-initiated meta-communication needed for clarification andrepair.The guidelines were refined and consolidated throughcomparison with a well-established body of maxims ofcooperative human-human dialogue which turned out to18form a subset of our guidelines (Grice 1975, Bemsen etal.
1996).
The resulting 22 guidelines were grouped un-der seven different aspects of dialogue, such as informa-tiveness and partner asymmetry, and split into genericguidelines and specific guidelines.
A generic guidelinemay subsume one or more specific guidelines whichspecialise the generic guideline to a certain class of phe-nomena.
The guidelines are presented in Figure 1.The consolidated guidelines were then tested as a toolfor the diagnostic evaluation of a corpus of 57 dialoguescollected uring a scenario-based, controlled user test ofthe implemented system.
The fact that we had the sce-narios meant hat problems of dialogue interaction couldbe objectively detected through comparison betweenexpected (according to the scenario) and actual user-system exchanges.
Each detected problem was (a) charac-terised with respect o its symptom, (b) a diagnosis wasmade, sometimes through inspection of the log of sys-tem module communication, and (c) one or several cureswere proposed.
The 'cure' part of diagnostic analysissuggests ways of repairing system dialogue behaviour.The diagnostic analysis may demonstrate that new guide-lines of cooperative dialogue design must be added, thusenabling continuous assessment of the scope of DET.We found that nearly all dialogue design errors in theuser test could be classified as violations of our guide-lines.
Two specific guidelines on meta-communication,SGI0 and SGI 1, had to be added, however.
This was nosurprise as meta-comrnunication had not been simulatedand thus was mostly absent in the WOZ corpus.3.
Generalising the ToolAs pointed out in Section 2, success in early tool devel-opment is not enough if the aim is to be able to rec-ommend the tool to other SLDS developers on a solidbasis.
The early development phase focused on oneSLDS with one particular dialogue structure, in oneparticular domain of application, designed for a particulartype of task, i.e.
reservation, in one particular develop-ment phase, i.e.
evaluation of an implemented system,and in circumstances of controlled user testing where wehad available the scenarios used by the subjects as wellas the full design specification of the system.
To testand increase the generality of the tool, we are currentlyapplying DET as a dialogue design guide to a WOZ cor-pus from the Sundial project (Peckham 1993).Ideally, testing DET on the Sundial corpus will in-crease the generality that can be claimed for the tool infour different ways: (1) the system dialogue is differentfrom that of the Danish dialogue system; (2) the tasktype is different, i.e.
information vs. reservation; (3) thetest type~tool purpose pairs are different: whereas in thecase of the Danish dialogue system, DET was used fordiagnostic evaluation in a controlled user test, the tool isbeing used as an early dialogue design guide in the caseof Sundial; and (4) circumstances are different becauseDialogue GGAspect No.Group 1: GG IInformativenessGG2Group 2: GG3Truth and evidence GG4Group 3: GG5RelevanceGroup 4: GG6Manner GG7iGroup 5:Partner asymmetryGG8GG9GGI0Group 6:BackgroundknowledgeGG11GG12Group 7:Repair andclarificationGGI3S G Generic or Specific GuidelineNo.SG6ii SG7*Make your contribution as informative as is required (for the current pur-poses of the exchange).SG 1 Be fully explicit in communicating to users the commitments hey havemade.SG2 Provide feedback on each piece of information provided by the user.
*Do not make your contribution more informative than is required.
*Do not say what you believe to be false.
*Do not say that for which you lack adequate evidence.
*Be relevant, i.e.
be appropriate othe immediate needs at each stage of thetransaction.
*Avoid obscurity of expression.
*Avoid ambiguity.SG3 Provide same formulation of the same question (or address) to users every-where in the system's dialogue turns.
*Be brief (avoid unnecessary prolixity).
*Be orderly.Inform the dialogue partners of important non-normal characteristics whichthey should take into account in order to behave cooperatively in dialogue.Ensure the feasibility of what is required of them.SG4 Provide clear and comprehensible communication fwhat the system canand cannot do.SG5 Provide clear and sufficient instructions to users on how to interact with thesystem.Take partners' relevan!
background knowledge into account.Take into account possible (and possibly erroneous) user inferences byanalogy from related task domains.Separate whenever possible between the needs of novice and expert users(user-adaptive dialogue).Take into account legitimate partner expectations a to your own back-ground knowledge.SG8 Provide sufficient task domain knowledge and inference.Enable repair or clarification meta-communication in case of comrnunica-tion failure.SG9 Initiate repair meta-communication if system understanding has failed.SG 10 Initiate clarification meta-communication in case of inconsistent user input.SG 11 Initiate clarification meta-communication in case of ambiguous user input.Figure 1.
Guidelines for cooperative system dialogue.
GG means generic guideline.
SG means specific guideline.The generic guidelines are expressed at the same level of generality as are the Gricean maxims (marked with an *).Each specific guideline is subsumed by a generic guideline.
The left-hand column characterises the aspect of dialogueaddressed by each guideline.we do not have the scenarios used in Sundial and do nothave access to the early design specification of the Sun-dial system.
I fDET works well under circumstances (4),we shall know more on how to use it for the analysis ofcorpora produced without scenarios, such as in fieldtests, or without he scenarios being available.The important generalisation (4) poses a particularproblem of objectivity.
When, as in controlled user test-ing, the scenarios used by subjects are available, it isrelatively straightforward to detect he dialogue designerrors that are present in the transcribed corpus usingobjective methods.
The objectivity problem then reducesto that of whether different analysers arrive at the same19classifications of the identified problems.
When, as inmany realistic cases in which DET might be used, noscenarios exist or are available, an additional problemarises of whether the corpus analysers are actually ableto detect he same problems in a dialogue prior to classi-fying them.
If not, DET will not necessarily be uselessbut will be less useful in circumstances in which theobjective number of dialogue design errors matters.
Inthe test case, objectivity of detection will have to bebased on the empirical fact, if it is a fact, that developerswho are well-versed in using the tool actually do detectthe same problems.4.
The  S imulated  SystemThe Sundial dialogues are early WOZ dialogues in whichsubjects seek time and route information on BritishAirways flights and sometimes on other airline flightsas well.
The emerging system seems to understand thefollowing types of domain information:1.
Departure airport including terminal.2.
Arrival airport including terminal.3.
Time-tabled departure date.4.
Time-tabled departure time.5.
Time-tabled arrival date.6.
Time-tabled arrival time.7.
Flight number.8.
Actual departure date (not verified).9.
Actual departure time.10.
Actual arrival date (not verified).11.
Actual arrival time.12.Distinction between BA flights which it knowsabout, and other flights which it does not knowabout but for which users are referred to airporthelp desks, sometimes by being given the phonenumbers of those desks.By contrast with the Danish dialogue system, the Sun-dial system being developed through the use of the ana-lysed corpus uses a delayed feedback strategy.
Instead ofproviding immediate f edback on each piece of informa-tion provided by the user, the system waits until theuser has provided the information ecessary for execut-ing a query to its database.
It then provides implicitfeedback through answering the query.
Until the user hasbuilt up a flail query, which of course may be done in asingle utterance but sometimes takes several utterancesto do-  the system would only respond by asking formore information or by correcting errors in the informa-tion provided by the user.
The delayed feedback strategyis natural in human-human communication but mightbe considered somewhat dangerous in SLDSs because ofthe risk of accumulating system misunderstandingswhich the user will only discover ather late in the dia-logue.
We would not argue, however, that the delayedfeedback strategy is impossible to implement and sue-cessflally use for flight information systems of the com-plexity of the intended Sundial system.
Still, this com-plexity is considerable, in particular, perhaps, due to the20intended ability of the system of distinguishing betweentimetabled and actual points in time.
It is not an easydesign task to get the system's dialogue contributionsright at all times when this distinction has to be trans-parently present throughout.Another point about he corpus worth mentioning isthat the simulated system understands the user amaz-ingly well and in many respects behaves just like a hu-man travel agent.
The implication is that several of theguidelines in Figure 1, such as GGll/SG6/SG7 onbackground knowledge, and GG13, SG9/SG10/SGI 1 onmeta-communication are not likely to be violated in thetranscribed dialogues.
It should be added that it is notaccidental that exactly these guidelines are not likely tobe violated in the transcribed ialogues.
The reason isthat it is difficult to realistically simulate the limitedmeta-communication and background-understandingabilities of implemented systems.
As to the nov-ice/expert distinction (SG7), this is hardly relevant osophisticated flight information systems such as thepresent one.
A final guideline which is not likely to beviolated in the transcriptions, is SG I on user commit-ments.
The reason simply is that users seeking flightinformation do not make any commitments: they merelyask for information.5.
Methodology and ResultsThe Sundial WOZ corpus comprises approx.
100 flighttravel information dialogues concerning British Airwaysflights.
The corpus was produced by 10 subjects whoeach performed 9 or 10 dialogues based on scenariosselected from a set of 24 scenarios.
We do not have thesescenarios.
The transcriptions came with a header whichidentifies each dialogue, markup of user and system ut-terances, consecutive numbering of the lines in eachdialogue transcription, and markup of pauses, ahs,hmms and coughs.
For the first generality test of DET,we have selected 33 dialogues.
Three dialogues were usedfor initial discussions among the two analysers.
Theremaining 30 dialogues were split into two sub-corporaof 15 dialogues each.
Each sub-corpus was analysed bythe two analysers.
Methodologically, we analysed eachsystem utterance in isolation as well as in its dialoguecontext o identify violations of the guidelines.
Utter-ances which reflected one or more dialogue design prob-lems were annotated with indication of the guideline(s)violated and a brief explanation of the problem(s).
UsingTEI, we have changed the existing markup of utterancesto make each utterance unique across the entire corpus.In addition, we have a&led markup for guideline viola-tion.
An example is shown in Figure 2.Having independently analysed the two sub-corporaof 15 dialogues each, the analysers discussed each of the384 claimed guideline violations and sought to reachconsensus on as many classifications-by-guideline aspossible.
This lead to the following 10 status descriptorsfor the claimed guideline violations:<u id="U 1:7-1">(0.4) #h yes I'm enquiring about flight number beeay two eight six flying in later today from san fran-cisco (0.4) could you tell me %coughs% 'souse mewhich airport and terminal it's arriving at and whattime (9) %coughs% (2) %coughs%, .
.<u id="S 1:7-6">(I 0) flight two eight six from san francisco arrives atlondon heathrow terminal four at thirteen ten<violation ref="Sl:7-6" guideline="SG2"> Date notmentioned.
The tabled arrival time is probably al-ways the same for a given flight number but theremay be days on which there is no flight with a givennumber.<violation ref="Sl:7-6" guideline="GG7"> It is notclear if the time provided is that of the timetable orthe actual (expected) arrival time of the flight.Figure 2, Markup of part of a dialogue from the Sun-dial corpus.
The excerpt contains auser question and thesystem's answer to that question.
The user's query wasfirst misunderstood but this part of the dialogue has beenleft out in the figure (indicated as: .... ).
The system'sanswer violates two guidelines, SG2 and GG7, as indi-cated in the markup.
(id) Identity = The same design error case identified byboth annotators.
(c) Complementarity = A design error case identified byone annotator..(cv) Consequence violations = Design error cases thatwould not have arisen had a more fundamental designerror been avoided.
(us) User symptoms = Symptoms of design errors asevidenced from user dialogue behaviour.
(a) Alternatives =Alternative classifications of the samedesign error case by the two annotators.
(re) Reclassification = Agreed reclassification of a de-sign error case.
(roe) Reclassification toalready identified case = Agreedreclassification of a design error case as being identicalto one that had already been identified.
(ud) Undecidable =Agreed undecidable design error clas-sification.
(deb) Debatable =The annotators disagreed on a higher-level issue involved in whether to classify a system ut-terance as a design error case.
(rej) Rejects = Agreed rejections of attributed esigner ror  cases .Based on the consensus discussion, the analysers cre-ated two tables, one for each sub-corpus.
The tables werestructured by guideline and showed the violations of aparticular guideline that had been identified by one of theGuidel ineGG1NOB-NOB$8:1-6i$8:6-3$8:9-4$9:I-3$9:6-2$9:9-3$9:10-2S10:1-3;10:1-4S10:6-3Comments  NOB-LDud: 2 different interpretations $8:1-5possible of  $8:1-5id+deb: offer/give phone no.
$8:1-6c+deb: offer/give phone no.
$8:3-3c: scheduled not statede+deb: offer/give phone no.
$8:9-2c: scheduled not statedc: actual not stateda: actual not stated + GG7c: S should specify desired $9:9-2informationc: actual not stateda: scheduled not stated +GG7Iid+deb: offer/give phone no.
!
S 10:I-3i+ re: from SG8id+deb: offer/give phone no.
S10:1-4+ re: from SG8c+deb: offer/give phone no.
S 10:1-5id+deb: offer/give phone no.
S10:1-9+ re: from SG8rej: no need to mention arri- S10:6-2val airporta: failed S clarification +GG5l id+deb: offer/give phone no.
SI0:9-2+ re: from SG8Figure 3.
Table of claimed violations of GGI.
NOB-NOB is NOB's annotation of the NOB sub-corpus.
LD-NOB is LD's annotation of  that sub-corpus.
The tablecontains 18 cases of which 16 are agreed violations ofGG1 (id, c and a), one is undecidable (ud) and one wasrejected (rej).
The table shows that 4 cases were reclassi-fied (re), that the two cases of alternative classificationsinvolved GG1 and GG7, and that an agreed classificationinvolved adebate on a component issue (id/c+deb).two analysers, each violation being characterised, inaddition, by its unique utterance identifier, its statusdescriptor and a brief description (Figure 3).Of the 384 claimed guideline violations, 344 wereagreed upon as constituting actual guideline violations,comprising the status descriptors identity, complemen-tarity, consequence violations, user symptoms, altema-tives, reclassification (re) and reclassification (rce).
40claimed guideline violations were undecidable, not agreedupon or jointly rejected by the analysers.
These figuresare not very meaningful in themselves, however, be-cause many identified esign guideline violations wereidentical.
This is illustrated in Figure 3 in which thecase of offer/give phone no.
recurs no less than 8 times.The analysers agreed that the system should always offer21the phone number of an alternative information servicewhen it was not itself able to provide the desired infor-mation, instead of merely telling users to ring that alter-native service.
The analysers disagreed, however, onwhether the system should start by offering the phonenumber or provide the phone number ight away (of.
debin Figure 3), What we need as SLDS developers i  not atool which tells us many times of the same dialoguedesign error but a tool which helps us find as many dif-ferent dialogue design errors as quickly as possible.
Wetake this to mean that when annotat ing spoken dialoguetranscriptions, it can be waste o f  t ime and effort to anno-tate the same design error twice.
A single annotation,once accepted, will lead to a different and improved de,-Guide- No.
of agreed No.
ofl ine v io la t ions  typesiGGI  16+11 6' :~"~" G ~, S ,1 Not relevant in information Svs2!
:,, /~;: :,?,~,~SG2GG26+10 32+1 3GG3 8+7 IGG4 1+0 1GG5 8+5 6GG6 I+2 2GG7 I 7+9 7SG3 30+39!'
1. .
.
.
.
.
.
.
.
.
.
respect  ......... .
.
.
.
.
.
.
.
.
,,:: ~ ..... ~: : , .
:  ii~:",i.~5!
:Y '<::~:/The:~sys,.eFm Is Suceessful~in thiSg <~;.,,a,**:,~G G 10 Massively violated in SG4 andSG5ISG4 21+18 1SG5 15+20 1. .
.
.
r< ;.
;=.,~C>~:G G 12 Violated in SG8 ISG8 6+3 1The system~ understands >:.
:: : ; ~ :' :~ :~ :~:: "~Figure 4.
Cases and types of dialogue design errorssorted by guideline violated.
Note that Figure 4 does notinclude the cases and types that were either undecidable,disagreed, or rejected (see Figure 5).22sign.
However, if resource limitations enforce restric-tions on the number of dialogue design errors which canbe repaired, the number and severity of the different dia-logue design errors will have to be taken into account.Following the reasoning of the preceding paragraph,the analysers proceeded to distil the different ypes ofguideline violations or dialogue design errors identifiedin the corpus.
This led to a much simpler picture, asshown in Figure 4.Figure 5 shows the nature of the types of guidelineviolation referred to in Figure 4 as well as the types thatwere undecidable, disagreed upon or rejected.
It should benoted that the term "type" is in this context rathervague.
Some of the types of guideline violation in Fig-ure 5 are very important o the design of a habitablehuman-system spoken dialogue, such as the demand fora more informative opening presentation of what thesystem can and cannot do, others are of less importancebecause they appear to be rather special cases, such aswhen the system offers a phone number to a user whoalready told the system that s/he had this phone number;some types cover a wealth of different individual cases,such as the many differences in phrasing the same mes-sage to the user, others cover just a single case or anumber of identical cases; and, of course, some types aremore difficult to repair than others.
However, commonto all these guideline violations is that they should beremedied in the implemented system if at all possible.Jointly, Figures 4 and 5 show that 15 guideline vio-lation types were found by both analysers, 9 types werefound by one analyser only, one type, in fact, a singlecase, was undecidable on the evidence provided by thetranscription, 3 types were disagreed upon, and 6 typeswere rejected uring the consensus discussion, No typeswere found that demanded revision or extension of theguidelines.
The Sundial corpus was analysed by two ofthe DET tool developers.
It cannot be excluded, there-fore, that others might in the corpus have found typesthat demanded revision or extension of the guidelines.This will have to be tested in a future exercise.
How-ever, on the evidence provided, the guidelines generalisewell to a different dialogue and task type (el.
Section 3).We also found that the guidelines generalise well to thedifferent test type~tool purpose  pair of the Sundial cor-pus.
In fact, it is not much different o use the guide-lines for early evaluation during WOZ and using theguidelines for diagnostic evaluation of an implementedsystem.
In both cases, one works with transcribed atato which the guidelines are then applied.Turning now to the objectivity or intersubjectivityof the performed analysis, we mentioned earlier that thisraises two issues wrt.
the Sundial corpus: (a) to whichextent do the analysers identify the same cases/types ofguideline violation?
and (b) to which extent do the ana-lysers classify the identified cases/types in the sameway?
During DET development, we never tested for ob-jectivity of annotation.Agreed StatusFound by AI + A2:identity +alternatives(including consequenceviolations)Found by A 1:complementarity(including reclassifica-tions)Found by A2:complementarity +user symptoms(including reclassifica-tions)UndecidableDisagreed?
Types of Caseactual arrival/departure not statedscheduled arrival/departure not statedfailed S clarificationS should offer phone no.no feedback on arrival/departure day, on BA and/or on routemissing/ambiguous feedback on timeU: has phone no.
S: offers phone no.departure time instead of arrival time providedphone number provided although user has it alreadyS: handles all flights - "BA does not handle Airline X.
"S: "no flights are leaving Crete today"scheduled vs. actual arrival/dep, time not distinguishedAM and PM not distinguishedmany variations in S's phrasestoo little said on what system can and cannot do: BA often missing,time-table nquiries always missingS should specify the information it needsS provides insufficient information for the user to determine if it is thewanted answerS repeats more than the 4 phone no.
digits asked for"flight info."
known to be false: S knows only BAS: encourages inquiry on airline unknown to itS: "flights between London and Aberdeen are not part of the BA shuttleservice, there is a service from London Heathrow terminal one" (rc fromGG5 to GG6)U: arriving flights?, S: leaving flights: imprecise feedbacksystem says it is not sure of the information it providedopen S intro requires interaction i structions on waiting, verbosity etc.2 different interpretations possible of $8:1-5whether to just offer or actually give phone no.delayed feedback strategyBA to Zurich: when open meta-cornrnunicationRejected no need to mention arrival airportno S-goodbye: U hung up!delayed feedback strategy could defend this caseresponse package OKno BA flights from Warsawthe system needs not have recent events infoFigure 5.
Cases and types of dialogue design errors sorted by guideline violated.Guidel ineGGI,GG7GG1,GG7GGI,GG5GG1SG2SG2,GG7GG2, GG5GG5GG5,GG2GG5GG6,GG7GG7GG7SG3SG4, SG8GG1GGIGG2GG3GG5GG6SG2GG4SG5GG1SG2GG7GGISG2SG2GG2IGG7SG4, SG8As to (a), the comparatively high number of guidelineviolation types found by one analyser but not by theother, i.e.
9 types compared to the 15 types found byboth analysers, either shows that we are not yet expertsin applying the guidelines to novel corpora, or that thetool is inherently only "62.5 % objective".
This needsfurther investigation.
However, a different considerationis pertinent here.
Consider, for instance, analyser A1.A I found the 15 guideline violation types which werealso found by A2 plus another 6 guideline violationtypes.
Compared to these 21 types, analyser A2 onlymanaged to add 3 new guideline violation types.
Sup-pose that, on average, either of two expert analysers findequally many guideline violation types not found by the23other.
In the present case, this number would be 4.5guideline violation types.
A single expert in using thetool would then find 19.5/24 or 81% of the guidelineviolation types found by two analysers together.
Still,we don't know how many new guideline violations athird expert might find and whether we would see rapidconvergence towards zero new guideline violations.
Itwould of course be encouraging if this proved to be thecase.
The 3 types disagreed upon and the 6 rejected typesillustrate, we suggest, that dialogue design is not anexact science!
Taken together, however, the 4.5 guide-line violation types added by the second analyser and the9 disagreed or rejected types suggest he usefulness ofhaving two different developers applying the tool to atranscribed corpus.
Finally, the single undecidable casewas one in which the (non-transcribed) prosody of whatthe user said might have made the difference.
Followingthe system's statement that " I 'm sorry there are noflights leaving Crete today", the user asked "did you saythere aren't any flights leaving Crete today?"
One ana-lyser took the user's question to be a simple request ohave the system's tatement repeated, in which case noguideline violation would have been committed by thesystem.
The other analyser took the user's question tobe an incredulous request for more information ("did yousay there AREN'T ANY flights leaving Crete today?
"),in which case the system's subsequent reply "Yes"would have been a violation of GG 1.As to (b), Figure 5 shows that the two analysersproduced several alternative classifications.
It should benoted, however, that the number of these disagreementshas been exaggerated by the data abstraction that wentinto the creation of a small number of types as shown inFigures 4 and 5.
In fact, alternative classifications wereonly made in 7 cases.
It appears to be a simple fact thatthere will always be data on guideline violation whichlegitimately may be classified in different ways.
Depend-ing on the context, the fact that the system says toolittle about what it can and cannot do can be a violationof either SG4 or SG8.
If  it says so up front, this is anSG4 but if it later demonstrates that it has said too lit-tle, this should be an SG8 but it is comparatively in-nocuous if an analyser happens to classify the violationas an SG4.
GG1 (say enough) and GG7 (don't be am-biguous) are sometimes two faces of the same coin: ifyou don't say enough, what you say may be ambiguous.Similarly GGI (say enough) and GG5 (be relevant), mayon occasion be two faces of the same coin: if you don'tsay enough, what you actually do say may be irrelevant.The same applies to GG2 (superfluous information) andGG5 (relevance): superfluous information may be irrele-vant information.
SG2 (provide feedback) and GG7(don't be ambiguous) may also overlap in particularcases: missing feedback on, e.g., time may imply thatthe utterance becomes ambiguous.
Finally, GG6 (avoidobscurity) and GG7 (don't be ambiguous) may on occa-sion be difficult to distinguish: obscure utterances some-times lend themselves toa variety of interpretations.246.
Conclusion and Future WorkWe find the results reported in this paper encouraging.The tool has generalised well to the Sundial corpus andsome amount of objectivity has been demonstrated withrespect o type identification and classification.
As thiswas out first attempt at using the tool independently ofone another, we intend to repeat he exercise using theinsights gained.
Two times 15 Sundial dialogues will beused for the purpose.
Following that, we plan to repeatthe experiment with a small sub-corpus of the Philipscorpus which comprises 13.500 field test dialogues con-ceming train timetable information (Aust et al 1995).This will add a new dialogue and task type, as well asthe new circumstances of a field trial to the generalitytest of the tool.
If and when convincing enerality and asatisfactory degree of objectivity in using DET havebeen achieved, a final transfer problem must be ad-dressed.
This problem concerns how to transfer DET toother developers in some "packaged" form which doesnot assume person-to-person tuition.
This should enableother SLDS developers to quickly and efficiently learn touse DET at the same level of objectivity as has beenachieved uring the tests of the tool.
Only then willDET be ready for inclusion among the growing numberof dialogue engineering best practice development andevaluation tools.
As a first step in addressing the transferproblem, we have recently included a DET novice in theteam.
He is an experienced computational linguist butwith little experience in SLDS development.
We areinvestigating what it takes to make him an expert inusing DET by having him analyse the same Sundialsub-corpus as was reported on above and we hope thathe will participate in the planned second Sundial sub-corpus exercise.
Following these steps, the final taskwill be to define an explicit and simple training schemefor how to become an expert in using the tool.Re ferencesAust, H., Oerder, M., Seide, F. and Steinbiss, V.: The Phil-ips Automatic Train Timetable Information System.Speech Communication 17, 1995, 249-262.Bernsen, N.O., Dybkj~er, H. and Dybkj~r, L.: Cooperativ-ity in human-machine and human-human spoken dia-logue.
Discourse Processes, Vol.
21, No.
2, 1996, 213-236.Bernsen, N.O., Dybkjaer, H. and Dybkj~er, L.: What shouldyour speech system say to its users, and how?
Guide-lines for the design of spoken language dialogue sys-tems.
To appear in IEEE Computer, 1997a.Bemsen, N.O., Dybkj~er, H. and Dybkja~r, L.: DesigningInteractive Speech Systems.
From First Ideas to UserTesting.
Springer Verlag, to appear, 1997b.Grice, P.: Logic and conversation.
In Cole, P. and Morgan,J.L., Eds.
Syntax and Semantics, Vol.
3, Speech Acts,New York, Academic Press, 1975, 41-58.Peckham, J.: A new generation of spoken dialogue systems:Results and lessons from the SUNDIAL project.
Proceed-ings of Eurospeech "93, Berlin, 1993, 33-40.
