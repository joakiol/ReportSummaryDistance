MOVEMENT IN ACTIVE PRODUCTION NETWORKSMark A. JonesAlan S. DriacollAT&T Bell LaboratoriesMurray Hill, New Jersey 07974ABSTRACTWe describe how movement is handled in a class ofcomputational devices called active production networks(APNs).
The APN model is a parallel, activation-basodframework that ha= been applied to other aspects ofnatural language processing.
The model is briefly defined,the notation and mechanism for movement is explained,and then several examples are given which illustrate howvarious conditions on movement can naturally be explainedin terms of limitations of the APN device.I.
INTRODUCTIONMovement is an important phenomenon in naturallanguages.
Recently, proposals uch as Gazdar's dcrivodrules (Gazdar, 1982) and Pereira's extraposition grammars(Pereirao 1983) have attemptod to find minimal extensionsto the context-free framework that would allow the descrip-tion of movement.
In this paper, we describe a class ofcomputational devices for natural language processing.called active production networks (APNs), and explorehow certain kinds of movement are handled.
In particular.we are concerned with left extraposition, such as Subject-auxiliary Inversion.
Wh-movement, and NP holes in rela-tive clauses, in these cos?s, the extraposod constituentleaves a trace which is insertod at a later point in the pro-cessing.
This paper builds on the research reported inJones (1983) and Jones (forthcoming).7,.
ACTIVE PRODUCTION NgrwoP J~7..1 Tim i~vk~Our contention is that only a class of parallel deviceswill prove to be powerful enough to allow broad contextualpriming, to pursue alternative hypotheses, and to explainthe paradox that the performance of a sequential systemoften degrades with new knowledge, whereas human per-formance usually improves with learning and experience.
=There are a number of new parallel processing (connection-?
st) models which are sympathetic to this view--Anderson(1983).
Feldman and Ballard (1982), Waltz and Pollack(1985).
McClelland and Rumelhart (1981, 1982), andFahlman.
Hinton and Sejnowski (1983).Many of the connection?st models use iterative relaxa-tion techniques with networks containing excitatory andinhibitory links.
They have primarily been used as best-fitcategorizers in large recognition spaces, and it is not yetclear how they will implement the rule-governed behaviorof parsers or problem solvers.
Rule-based systems need astrong notion of an operating state, and they dependheavily on appropriate variable binding schemes for opera-tions such as matching (e.g.. unification) and recurs?on.The APN model directly supports a rule-based interpreta-tion, while retaining much of the general flavor ofI.
1"be htmmm li~ity to L:mrfofm mlpatztmmtlly e?patm,m opmltmm =alia s~ y  ~,  imt'alkd loud,mum remforou this b?fid.connection?sin.
An active production network is a rule-oriented, distributed processing system based on the follow-ing principles:1.
Each node in the network executes a uniform activa-tion algorithm and assumes states in response to mes-sage (,such as expectation, inhibition, and activation)that arrive locally; the node can, in turn, relay mes-sages, initiate messages, and spawn new instances toprocess message activity.
Although the patterns thatdefine a node's behavior may be quite idiosyncratic orspocializod, the algorithm that interprets the patternis the same for each node in the network.2.
Messages are relatively simple.
They have an associ-ated time, strength, and purpose (e.g., to post anexpectation).
They do not encode complex structuressuch as entire binding lists, parse trees, feature lists,or meaning representations, z Consequently, no struc-ture is explicitly built; the "result" of a computationconsists entirely of the activation trace and the newstate of the network.Figure I gives an artificial', but comprehensive exampleof an APN grammar in graphical form.
The grammargenerates the strings--a, b. acd.
ace.
bed.
bee.
fg and g l -and illustrates mapy of the pattern language features andgrammar writing paradigms.
The network responds to$ourcex which activate the network at its leaves.
Activa-tion messages spread '*upward" through the network.
Atconjunctive nodes (seq and and), expectation messages areposted for the legal continuations of the pattern; inhibitionmessages are sent down previous links when new activa-tions are recorded.P J~Figure i.
A Sample APNIn parsing applications, partially instantiatcd nodes areviewed as phrase structure rules whose next constituent isexpected.
The sources primarily arise from exogenous2.
For ?
sit'tatar ?oaaectioaett vnew, ~ F?ldman sad B#llard (1982) orWaltz ted Pollack (198S).
A compemoa or markor patuns, valueImaan I ?ad uoreltricted melmzle pinball =yttm=t= i  ipvea ia Fahlmnm,Hlalal lad Scjnowl~ (IgS)).161strobings of the network by external inputs.
In generationor problem solving applications, partially instantiated nodesare viewed as partially satisfied goals which have out.stand-ing subgoaLs whine solutions are de=ired.
The source= inthis case are endogenously generated.
The compatibility ofthe=e two views not only allows the same network to beused for both parsing and generation, but also permitsprocesu~ to share in the interaction of internal and exter-nal sources of information.
This compatibility, somewhatsurprisingly, turned out to be crucial to our treatment ofmovement, but it is aLso clearly desirable for other aspectsof natural anguage processing in which parsing and prob-lem solving interact (e.8., referenco resolution and infer-en(~P.
).Each node in an APN is defined by a pattern, writtenin the pattern language of Figure 2.
A pattern describesthe me=age= to which a node rmponds, and the new mes-sage= and internal state= that are produced.
Each subpat-tern of the form ($ v binding-put) in the pattern for nodeN is a variable binding site; a variable binding takes placewhen an instance of a node in binding-gat activates areference to variable v of node N. Implicitly, a patterndefines the set of state= and.
state transitions for a node.The ?
(optiouality), + (repetition) and ?
(optional repeti-tion) operators do not extend the expressiveness of thelanguage, but have been added for convenience.
They canbe replaced in preprocessin8 by equivalent expre&sions, jFormal semantic definitions of the m_~_~$e passingbehavior for each primitive operator have been specified.pattern ::-- binding-site(seq pattern ...)(and pattern ...)(or pattern ...)(?
pattern)(+ binding.site)(.
binding-site)binding-site ::-- ($ vat binding-pattern)binding.pauern ::-- nodeI (and binding-pattern ...)I (or binding-pattern ...)Figure 7..
The APN Pattern LanguageAn important distinction that the pattern languagemakes is in the synchronicity* of activation signals.
Thepattern (and ($ vl X) ($ v2 \]'3) require= that the activa-tion from X and F emanate from distinct network sources,while the pattern ($ v (and X I"3) insists that instances ofX and Y are activated from the same source.
In the\].
The enact chore= o( cq~s'acors in the pattern tan |up  it  t matewhat~at= mine from the =!~=m~attma of the APN maciaa~.4, - r~  ?nulreat APN model allocate= ~ telueatmUy.
The ten=$yllgiteomlclly reflC~lt th l  fact th l l  t\[~ ~ kicl~Uly o4 r t~ i ~m~se= can be Ioc~y COmlm '.,,I f~m tlm=r tiuw ~f ~ T I~u k in |  u the ,ctJvuua= pmau= rims \[=== a~ugb to coacli~aa the networkbmmi ,  mai my, m0 scuvatmm.
Alua'aaUvety, a,:Uvalma mela l~ covidemV t l~ mmr?~ ideatiW =t as a4di,t*...-t l~ram, et~n.
ia t l~  csm.
m=Jmeaeunt iom cam a*su.hq~ ~ at t t ' '  prom t'~h, ~ e( tit iaaemmltalcxp~?ume ~mvtm,,_,~._ F.= re~l~ iy  illlequndeut i , i.. o,m'lapmay nm po~ a p~Vlem.graphical representation f an APN, synchrony is indicatedby a short tail above the subpattern expression; thedefinition of U in Figure I illustrates both conventions:(and ($ vl (and TI)) ($ v2 S)).2.3 Am F..~m~Figure 3 shows the stages in parsing the string acd.
Anexogenous source Exog-srcO first activates a, which is notcurrently supported by a source and, hence, is in an inac-tive state.
The activation of an inactive or inhibited nodegive= rise to a new instance (nO) to record the binding.The instance is effectively a new node in the network, andderives its pattern from the spawning node.
The activationspreads upward to the other instances hown in Figure3(a).
The labels on each node indicate the current activa-tion level, repreu:nted as an integer between 0 and 9,inclusive.PO(9)qo(9) cI IaO(9)  IIExog-~rc0(9) \[Exog-srcJ(a) trace structure after apo(4)Q0 c0(4) saO I TExog-src0 Exog-srcl(9) d e fExog-src(b) trace structure after acpO(9)Q0 cO S0(9)I Exog-src0 Exog-srcl d0(9)JExog-src2(9)(c) trace structzure after acd\ [~p le  3, Stalp=l in Parsing acd162The activation of a node causes its pattern to be4re)instantiated and a variable to be (re)bound.
For exam-pie.
in the activation of RO, the pattern (seq ($ vi Q) (5v2 c'9) is replaced by (seq ($ vi (or Q QO)) ($ v2 c)).
andthe variable vl is bound to (20.
For simplicity, only theactive links are shown in Figure 3.
RO posts an expecta-tion message for node C which can further its pattern.The source Exog-secO is said to be supporting the activa-tion of nodea nO.
QO.
RO and PO above it, and the expecta-tions or inhibitions that are generated by these nodes.
Forthe current paper we will assume that exogenous sourcesremain fully on for the duration of the sentenco, sIn Figure 3(b), another exogenous ource Exog-srclactivates c, which furthers the pattern for RO.
RO sends aninhibition message to QO, posts expectations for S, andrelays an activation message to P0, which rebind~ its vari-able to RO and a~umes a new activation value.
Figure3(c) shows the final situation after d has been activated.The synchronous conjunction of SO is satisfied hy TO anddO.
RO is fully satisfied (activation value of 9), and PO isre-satisfied.1,4 Gramm~ Writbql P~UlpmThe APN in Figure I illustrates everal grammar writ-ing paradigms.
The situation in which an initial prefixstring (a or b) satisfies a constituent (P), but can be fol-lowed by optional suffix strings (cd or ce) occurs frequentlyin natural language grammars.
For example, noun phraseheads in English have optional prenominal and postnominalmodifiers.
The synchronous disjunction at P allows thelocal role of a or b to change, while preserving its interpre-tation as part of a P. It is also simple to encode optionalprefixes.Another common situation in natural language gram-mars is specialization of a constituent based on some inter-hal feature.
Noun phrases in English, for exampl?, can bespecialized hy case; verb phrases can be specialized as par-ticipial, tensed or infinitive.
In Figure l, node S is a spe.cialization which represents "Ts with d-ness or e-ness, butnot f-heSS.'"
The specialization is constructed by a synchro-nous conjunction of features that arise from subtrees some-where below the node to be specialized.The APN model also provides for node outputs to hepartitioned into independent classes for the purl~s?~ ,~)f theactivation algorithm.
The nodes in the classes form levelsin the network and represent orthogonal systems ofclassification.
The cascading of expectations from dilfcrentI~els can implement context-sensitive b haviors uch asfeature agreement and s':mantic sclectionai restrictiops.This is described in Jones (forthcoming).
In the next sec-tion, we will introduce a grammar writing paradigm torepresent movement, another type of non..context-fre?behavior.$.
It is interertins to sp~'ulatc: on the oOm~lUamC~ o( vsr~w relauua~q of~hiu ?al~m~l~Oe.
Fundam,mt~l limitatmm in the allocatm of ~ maybe reJalod to limiuUmna in sluart term memory (~r buff're space indc'tl~iatMi?
zzleJ?l~ I??
Matctul, 19BO).
Lin|uilti?
?emmzinUl ~ onOoQM~tlt~l?
IcqtStb oou~ be col=ted tO ~r l~ daca),.
~ |yntlcli?Mlzdca path bebav~?
mJlbl be rclltad to accc.h=Itad iowr~ decay r.atmmdby inbibitioo from ?
~up~ml l  bypmbmia.
Anythin$ mum than ?
f ,~m~iJ ~t t t re  at ,hi=3.
MOVI~W..NTFrom the APN perspective, movement (limited here toleft-extrapnsition) ecessitates the endogenous reactivationof a trace that was created earlier in the process.
To cap..ture the trace so that expectations for its reactivation canbe posted, we use the following type of rule: (seq (5 vl ...X... ) ($ v2 ... (and X X-see Y) ...).
When an instance,XO, first activatea this rule, vl is bound to XO; the secondoccurrence X in the rule is constrained to match instancesof XO, and expectations for XO, X-see and Y are created.No new exogenous source can satisfy the synchronous con-junction; only an endogenous X.src can.
The rule is simi-lar to the notion of an X followed by a Y with an X hole init (cf.
Gazdar, 1982).NP- t  rae l l  CNP V V \]I ?
.~ .<>.~ 7 p .
.
.
/ I ~ .~e t~ N ~ ran  cnasecl /I a the  ?
.~mOU.
.
~ /Figure 4.
A Grammar for Relative ClausesFigure 4 defines a grammar with an NP hoic in a rela-tive clause; other type, s of \[eft-extraposition are handledanalogously.
Our treatment of relatives is adapted fromC'homsky and Lasnik (1977).
The movement rule for S is:(seq ($ vl (and Cutup Re/ (or Exog.src PRO-src)) ($ v2(and Rel Rel.src S))).
The rule restricts the first instanceof Re/ to arise either from an exogenous relative pronounsuch as which or from an endogenously generated (phono-logically null) pronoun PRO.
The second variable issatisfied when Rei,src simultaneously reactivates a trace ofthe Rel instance and inserts an NP-tracc into an S.It is instructive to consider how phonologically null pro-nouns are inserted before we discuss how movement occursby trace insertion.
The phrase, \[NP the mouse \ [~ PRO="that ...\]\], illustrates how a relative pronoun PRO isinserted.
Figure 5(a) shows the network after parsing thecat.
When the complementizer that appears next in theinput, PRO-src receives inhibition (marked by downwardarrows in Figure 5(b)) from Rel.CompO.
Non-exogenous163sources uch as PRO-src and Rel.src are activated in con-texts in which they are expected and then receive inhibi-tion.
Figure 5(c) shows the resulting network after PRO-src has been activated, The inserted pronoun behaves pre-cisely as an input pronoun with respect to subsequentmovement.The trace generation ecessary for movement uses thesame insertion mechanism described above.
Figures 6(a)-(d) illustrate various stages in parsing the phraso, \[/vp thecat \[~" whichi \[$ tl ranll\], in Figure 6(a), after parsingthe cat which, synchronous expectations are posted for anS which contains a reactivation of the RelO trace by Rel.see.
The signal sent to S by Rei.src will be in the form ofan NP (through NP-trace).Figure 6(b) shows how the input of ran produces inhi-bition on Rei-src from SI.
The inhibition on Rei-srccaus~ it to activate (just as in the null pronoun insertion)to try to satisfy the current contextual expectations.
Fig-ure 6(c) shows the network after Rel-src has activated tosupply the trace.
The only remaining problem is thatRel-src is actively inhibiting itself through .~0.
6 WhenRel-src activates again, new instances are created for theinhibited nodes as they are re-activated; the uninhibitednodes are simply rebound.
The final structure is shown inFigure 6(d).it is interesting that the network automatically enforcesthe restriction that the relative pronoun, complementizerand subject of the embedded sentence cannot all be miss-ing.
PRO must be generated before its trace can beinserted as the subject.
Furthermore.
since expectationsare strongest for the first link of a sequence, expectationswill be much weaker for the VP in the relative clause(under S under S") than for the top-level VP under SO.The fact that the device blocks certai'n structures,without explicit weli-formedness constraints, is quitesignificant.
Wherever possible, we would like to accountfor the complexity of the data through the compositebehavior of a universal device and a simple, general gram-mar.
We consider the description of a device which embo-dies the appropriate principles more parsimonious than alist of complex conditions and filters, and, to the extentthat its architecture is independently motivated by proc,'ss-ink (i.e.. performance) considerations, of greater thcorcticalinterestfAs we have seen, certain interpretations can besuppressed by expectations from elsewhere in the network.Furthermore, the occurrence of traces and empty consti-tuents is severely constrained because they must be sup-plied by endogenous ources, which can only suppurt a sin-tie constituent at any given time.
For NP movement,these two properties of the device, taken together.elfectively enforce Ross's Complex NP Constraint (Ross.1967), which states that, "No element contained in a6.
Another ,~sy o4" rut?inS thi,J iJ that the noa~ynchroetM:ity of the twovanaMea in the I~ttern hat ~ viohtted.
The wdt-inhibittoa f ?
murcgocgtwt in othcnr conteat~ in the APN ft'tnM:lmek eve?
for egolgno~ttoMt~eL Is net,aerita hat contai?
leJ't.rm;urtiv?
cyr.t~ or ,endmSl~tmtttaghn~nta (e.S.. PP lUaghfl~'ltt), tett-iahibltioa C ll Ifiu naturally Uthe t~ult at nemum~ me-de~rmiaim~ ae.tctivatioe f ?
~\[-inhil~t~mum d'egUvety Ixorgtva the aea-tyarJumigity ~ pmuwnt.?.
1"I~ work 4 Margin (1980) iain tJ~tm~&l~t.sentence dominated by an NP  with a lexLcal head nounmay be moved out of that NP  by a transformation.
"To see why this constraint is enforced, consider the twokinds of sentences that an NP  with a lexical head nounmight dominate.
If the embedded sentence is a relativeclause, as in.
\[pip the rat \[~" whichl \ [$ the cat \[~" whichj\[S fj chased/I\] \]  likes fish\]J\], then Rel.src cannot supportboth traces.
If the embedded sentence is a noun comple-ment (not shown in Figure 4).
as in.
\[NP the rat \[~"whichi \[S he read a report \[~" that \ [$ the cat chasedfl\]\]\]\]\], then there is only one trace in the intendedinterpretation, but there is nondeterminlsm during parsingbetween the noun complement and the relative clauseinterpretation.
The interference aus?,, the trace to bebound to the innermost relative pronoun in the relativeclause interpretation.'
Thus, the combined properties ofthe device and grammar consistently block those structureswhich violate the Complex NP Constraint.
Our prelim-inary findings for other types of movement (e.g., Subject-auxiliary Inversion, Wh-movement, and Raising) indicatethat they also have natural APN explanations.4.
IMPLF.aMENTATION 8ml Fu'ruRg DIMF.CrlONSAlthough the re.torch described in this summary is pri-marily of a theoretic nature, the basic ideas involved inusing APNs for recognition and generation are beingimplemented and tested in Zetalisp on a Symbolics LispMachine.
We have also hand-simulated data on movementfrom the literature to design the theory and algorithmspresented in this paper.
We are currently designing net-works for a broad coverage syntactic grammar of Englishand for additional, cascaded levels for NP role mappingand case frames.
The model has aLso been adapted as ageneral, context-driven problem solver, although more workremains to be done.We are considering ways of integrating iterative relaxa-tion techniques with the rule-based framework of APNs.This is particularly necessary in helping the network toidentify expectation coalitions.
In Figure 5(a), for exam-pie.
there should be virtually no expectations for Rel-src,since it cannot satisfy any of the dominating synchronousconjunctions.
Some type of non-activating feedback fromthe sources eems to be necessary.S.
SUI~ARYRecent linguistic theories have attempted to inducegeneral principles (e.g., CNPC.
Subjacency, and the Struc-ture Preserving Hypothesis) from the detailed structuraldescriptions of earlier transformational theories (Chomsky,1981), Our research can be viewed as an attempt tuinduce the machine that embodies theae principles.
In thispaper, we have described a class of candidate machine~,called active production networks, and outlined how theyhandle movement as a natural way in which machine andgrammar interact.The APN framework was initially developed as a plau-sible cognitive model for language processing, which wouldhave real-time processing behavior, and extensive8.
Uhle tO r~-.~,-~.--i ?oeskJs~ttmsJ wb~t rg~lto tO e.lp~t~om q~t~nfftb~ t J r ?~tm heud ia s ~.tr tlmt ~ nemmtg.164so(4)NPO(9) VPCNPO (9 )  "'"o stheOIExog-s rcOcat0(9)  ~ " "| L T ComnExog-sr?l (9) I r~/ .h4Ch PRO% \ that  for(a) trace structure after the catSo(4)NPO(9)ICNPO(9)NO - -~VPOetO?
I \  ' /..=,~ Rel -Com?O (4 )Re I ~ - - - ~ ~  4=er0(9)I(b) trace structure after the cat ... thatNPO(4) vP/CNPO(4)Oat0  NO/ ItneO catOI l ExOg-SrCO ~x Og-S?
'C !SO(4)/ _ ~ ~, -Ico~,oo ~ 9 ) ~ ~"  --.
~ NP- t  r 'ace CNPRe lO(9)  I \[ Comglement tzerO I / .I..\1 ,.
'..o J/ \[ p~o-src( )\[ g-(c) trace structure after the cat PRO that.Figure 5.
Relative Pronoun Insertioncontextual processing and learning capabilities based on aformal notion of expectations.
That movement also seemsnaturally expressible in a way that is consistent withcurrent linguistic theories i  quite intriguing.REFERENCESAnderson, J. R. (1983).
The Architecture of Cognition,Harvard University Press, Cambridge.Chomsky.
N. (1981).
Lectures on Government and Bind-ing.
Foris Publications, Dordrecht.Chomsky, N. and Lasnik, H. (1977).
"Filters and Con-trol," Linguistic Inquiry g, 425-504.Fahlman, S. E. (1979).
NETL" A System for Represent-ing and Using Real-World Knowledge.
MIT Press, Cam-bridge.Fahlman, S. E., Hinton, G. E. and Sejnowski, T.
J.(1983).
"Massively Parallel Architectures for Ah NFTL,Thistle, and Boltzmann Machines," AAAI.83 ConferenceProceedings.Feldman.
J, A. and Ballard, D. It.
(1982).
"Connection-ist Models and Their Properties," Cognitive Science 6,205-254.Gazdar, G. (1982).
"Phrase Structure Grammar," TheNature of Syntactic Representation, Jacubson and Pullum,eds., Reidel, Boston, 131 -186.Jones.
M. A.. (1983).
"Activation-Based Parsi.g."
8thIJCAI, Karlsruhe, W. Germany, 678-682.Jones, M.A.
(forthcoming).
submitted for publication.Marcus.
M. P. (1980).
A Theory of S),ntactic Recogni.lion for Natural L,znguage, M IT Press, Cambridge.Pereira.
F. (1983).
"Logic for Natural LanguageAnalysis," technical report 275, SRI International.
MenloPark.Ross, J. R. (1967).
Constraints on Variables.in Syntax,unpublished Ph.D. thesis, MIT, Cambridge.Waltz.
D. L. and Pollack, J.
B.
(1985).
"MassivelyParallel Parsing: A Strongly Interactive Model of NaturalLanguage Interpretation," Cognitive Science, 9, 51-74.165SO(2)VP NPO(4) /CNPO(4)OetO NOtfleOi I I'~c?
.
'P?
(9.~ 1 sI i~tchO( t )  ?
l l - -~  r iC l  CNISO(2)NPO(4 ~*~='~''`mr~~ " VP /CNPO(4)OetO NO $0(4).
I r~=o~ ~ i ~ , (4 )Sxog l i fe0  Exoo-src l  I "~4#" / ~ I ~=lo / ~ .
~ v=o(9)\[ li" / / t 1I wntchO /N I - - IP IC i  VO(9),.o.-..<~.,/.
; ~!
?o<,,(a) trace structure after ihe cat which (b) trace structure after the cat which ... ranNP0(9) VP /CNP0(9)0et0 NO SO(9}~..o~,,oo ~,o:.,<, '4~ I .o,<.~oo/ I I i l ;~o  / .
.
-< .
.
/< .o ( , ,  ~ ~,o ,~y ,:?oS0(9)NP0{9) VP /CNPO(9)..---'70et0 NO S0(9)1/ cato tneOI wn~c~O =n4cn00(9~NP- t race0(9)  v0~:  \ / /  , Exog ranOIExog-Sr?3 pe l - s rc (9 ) |  Exog-src3(c) trace structure just after the cat which t ran (d) final trace structurel;igwe 6.
Parsin8 Rclativc Clauses166
