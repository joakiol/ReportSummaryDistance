LAW VIII - The 8th Linguistic Annotation Workshop, pages 82?86,Dublin, Ireland, August 23-24 2014.Multiple views as aid to linguistic annotation error analysisMarilena Di BariUniversity of Leedsmlmdb@leeds.ac.ukSerge SharoffUniversity of Leedss.sharoff@leeds.ac.ukMartin ThomasUniversity of Leedsm.thomas@leeds.ac.ukAbstractThis paper describes a methodology for supporting the task of annotating sentiment in naturallanguage by detecting borderline cases and inconsistencies.
Inspired by the co-training strategy,a number of machine learning models are trained on different views of the same data.
The predic-tions obtained by these models are then automatically compared in order to bring to light highlyuncertain annotations and systematic mistakes.
We tested the methodology against an Englishcorpus annotated according to a fine-grained sentiment analysis annotation schema (SentiML).We detected that 153 instances (35%) classified differently from the gold standard were accept-able and further 69 instances (16%) suggested that the gold standard should have been improved.1 IntroductionThis work pertains to the phase of testing the reliability of human annotation.
The strength of ourapproach relies on the fact that we use multiple supervised machine learning classifiers and analyse theirpredictions in parallel to automatically identify disagreements.
Those, in fact, ultimately lead to thediscovery of borderline cases in the annotation, an expensive task in terms of time when carried outmanually.Predictions with a number of different labels are manually analysed, since they may indicate inconsis-tencies in the annotation and cases difficult to annotate.
Conversely, cases with high agreement suggestthat the annotation schema is reliable.
On the one hand, the analysis of those disagreements, in conjunc-tion with the gold annotations, provides fresh insights about the efficacy of the features provided to theclassifiers for the learning phase.
On the other hand, when all the classifiers agree on a wrong annotation,it is a strong signal of ambiguity in the annotation schema and/or guidelines.In Section 2 we briefly introduce the data to which we apply the methodology described in Section 3.In Section 4 we report results.
In Section 5 we mention studies related to ours and in Section 6 we drawconclusions and identify steps for future work.2 DataWe tested our methodology on the SentiML corpus (Di Bari et al., 2013) for which the annotationguidelines, as well as the original and annotated texts, are publicly available1.
The corpus consists of307 English sentences (6987 tokens), taken from political speeches, TED talks (Cettolo et al., 2012), andnews items from the MPQA opinion corpus (Wilson, 2008).The aim of its annotation is to encapsulate opinions in pairs, by marking the role that each word takes(modifier or target).
For example, in?More of you have lost your homes and even more are watching your home values plummet?there would be two pairs: modifier ?lost?
and target ?homes?, and modifier ?values?
and target ?plum-met?.
Such two pairs are called appraisal groups.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1http://corpus.leeds.ac.uk/marilena/SentiML82Figure 1: Example of dependency tree.
Dependency trees provide features for the machine learning step.For each of these elements several features are annotated that are believed to improve the task ofsentiment analysis.
The study presented here relates to the automatic identification of modifiers andtargets.3 MethodologyTo test our methodology we selected a corpus for which various types of linguistic information relatedto appraisal groups were annotated.
We started with the identification of modifiers and targets, since thisrepresents the base of all the other levels of annotation.To test the reliability of annotation we set 10% of our annotated corpus aside, and performed themachine learning part of the study on the remaining 90% of our corpus.The first step consists of preparing the features for the machine learning phase.
The optimal set tomodel the annotation task varies from problem to problem.
We used the following:?
Word features, representing the ordinal identifier, word form, lemma and POS tag of each word.?
Contextual features, representing the lemma and POS tags of the preceding and succeeding words.?
Dependency-based features, representing the reference to the word on which the current token de-pends in the dependency tree (head) along with its lemma, POS tag and relation type (see Fig-ure 1) (Nivre, 2005).?
Number of linked modifiers, representing the number of adjectives and adverbs linked to the currentword in the dependency tree.?
Role, representing the predicted role (modifier or target) of the current token in conveying sentiment.The predictions are computed using fixed syntactic rules.?
Gazetteer-based sentiment.
We used the NRC Word-Emotion Association Lexicon (Mohammad,2011) to represent the a-priori sentiment of each word, i.e.
regardless of its context.Once the features are ready, two or more feature partitions (called views in the co-training strategy)have to be defined in order to be as orthogonal as possible (Abney, 2007).
We opted for a linguistically-grounded dichotomy: lexical features (word features, role and gazetteer-based sentiment) versus syntac-tic features (contextual and dependency-based features, number of linked modifiers).
The training andtest sets are split accordingly.At this point, machine learning classifiers are chosen.
These need to be confidence-rated, i.e.
able toprovide a confidence rate for each prediction.
In our experiments we selected Na?
?ve Bayes, Radial BasisFunction Network and Logistic Regression2.
These models rely on very different strategies, which makesthe analysis more reliable.
We discarded Support Vector Machines since in our preliminary experimentsthey achieved high precision (a range between 0.60 and 0.77 across modifiers and targets), but verylow recall (a range between 0.05 and 0.06 across modifiers and targets), which resulted in a very lowF-measure (a range between 0.09 and 0.11 across modifiers and targets).A model for each combination of view and classifier is then produced and tested on the test set.
Weperformed a 10-fold cross-validation.
In the test phase, we opted for a numerical threshold of 0.67 toconsider the predictions reliable.
A prediction with a confidence lower than the threshold is considereduncertain.For each instance we obtained six predictions, which potentially differ from one another.
The agree-ment score is calculated for each class in order to identify the most frequent prediction.2In each case we used the implementation provided by WEKA (http://www.cs.waikato.ac.nz/?ml/weka/).83Feature set ClassifierModifier TargetPrecision Recall F?=1Precision Recall F?=1LexicalNa?
?ve Bayes 0.71 0.10 0.48 0.82 0.12 0.43RBF Network 0.52 0.56 0.54 0.51 0.59 0.55Logistic regression 0.59 0.42 0.49 0.61 0.48 0.54SyntacticNa?
?ve Bayes 0.46 0.48 0.47 0.82 0.12 0.43RBF Network 0.49 0.35 0.40 0.55 0.50 0.53Logistic regression 0.58 0.22 0.32 0.60 0.41 0.49Table 1: Performance of the classifiers trained on two views, lexical and syntactic.
Experiments havebeen performed using 10-fold cross-validation.At this point, only the predictions different from the gold annotations are considered: the higher theagreement score, the more the instance is interesting in the context of our analysis.The final step consists of manually investigating such cases to shed light on the errors.
In this experi-ment we opted for the use of a simple protocol based on the following classification schema:?
W (wrong), where the classifiers disagree with the gold annotation, which we judge to be correct.?
A (ambiguous), where the classifiers disagree with the gold annotation and we judge both to bevalid.
In such cases, the guidelines need to be clearer or the annotation method could have beensimpler.?
M (to modify), where we judge that the gold annotation is incorrect.This approach has the advantage of yielding a much reduced subset of instances to be examined man-ually, with respect to the full set.4 ResultsTable 1 shows the performances of the six models obtained from the training of each combination ofview and classifier, mentioned in Section 3.
F-measures for modifiers range between 0.32 and 0.54for modifiers, and 0.43 and 0.55 for targets.
Overall, the RBF Network trained on the lexical viewperforms best.
However, there is no huge difference in general in performances between the lexical andthe syntactic feature sets, which is good in the light of data sparseness.Performance on the the empty class (no category assigned) was exceptionally good, as 76% was pre-dicted out of the gold 77%, whereas the performance on the modifiers was 4% out of the gold 12% andthe performance on the targets was 5% out of the gold 11%.
Although the annotation allows each tokento be simultaneously annotated as modifier and target, we have not reported the performances for the MTclass as the cases were not significant.
Finally, there was a 15% of cases in which the classifiers were notconfident.In relation to the manual classification of errors (see final paragraph of Section 3) we found that, outof the total test instances (2066), in 436 cases the most predicted class differed from the gold standard:the label W was assigned 214 times (49%), the label A was assigned 153 times (35%), the label Mwas assigned 69 times (16%).
W was mostly assigned when the modifier or the target was correctlyidentified, but not its counterpart in the pair (e.g., ?way forward?, ?blame society?, ?wrong side?).
It wasalso assigned when a word was correctly identified as evoking sentiment (e.g., ?destroy?, ?flourish?
),but only the first of two or more targets was identified (e.g., ?women and children?, ?the city and thecountry?
).A was assigned when an adverb was annotated as modifier (e.g., ?through corruption?, ?seize gladly?,?tragically reminded?
): these are cases in which human annotators decide to include the adverb if it isregarded as important for the sentiment.
Other cases in which the label has been used is with compoundmodifiers (e.g., ?face to face?, ?in the face of?
), phrasal verbs (e.g., ?turn back?, ?carried forth?, ?cameforth?)
and difficult couples to link (e.g., ?instruments with which we meet them?
[challenges]).
Finally,this label was also used in cases in which the prediction was sensible, but considered less accurate thanthe gold one (e.g., in ?enjoy relative plenty?, the gold standard was ?enjoy plenty?
and the classifiers84predicted ?relative plenty?
).M was assigned when another modifier had been wrongly annotated by the annotator, instead of mod-ifying the value of the force of the current one (e.g., in ?much more?, only ?more??
should have beenannotated with high force), in the case of couples with no sentiment (e.g., ?future generations?, ?differentform?
), of couples not previously identified (e.g., ?stairway filled with smoke?, ?icy river?)
or couplesthat could have been annotated in an easier way (e.g., ?provoke us to step up and do something?, ?imageresonates with us?
).5 Related workEvaluating the reliability of human annotation is a challenging and widely studied task (Pustejovskyand Stubbs, 2012).
The standard solution is the measurement of an inter-annotator agreement (IAA)coefficient according to a variety of formulae that depend on the characteristics of the annotation set-ting (Artstein and Poesio, 2008).For example, in the case of Wilson (2008) and Read and Carroll (2007), it was useful to understandinconsistencies in the selection of the span for attitudes and targets.
Since this represents only one ofthe commonly recognized challenges, some studies have focused on practically testing a methodologicalframework for schema development for fine-grained and quality semantic annotations.
(Bayerl et al.,2003).Our approach varies from the standard procedure in ways similar to that of Snow et al.
(2008).
Foreach expert annotator (six in total) they trained a system using only the judgements provided by theseannotators, and then created a test set using the average of the responses of the remaining five labellers onthat set.
This resulted in six independent expert-trained systems.
The difference with our methodologyis that we trained six independent classifiers, but based on judgements of only one human annotator, andcompared the average of the responses of six classifiers with the gold standard.Jin et al.
(2009) also used the strategy of selecting the labelled sentences agreed upon by their classi-fiers and achieved good performances in the task of identifying opinion sentences.Finally, our methodology is also similar to one of those mentioned by Yu (2014).
The author used thetraditional co-training strategy, i.e.
providing a small pool of unlabelled data to two classifiers with con-fidence rates, in order to obtain automatically labelled examples that would be added to an initial set oflabelled ones.
Subsequently, this final large set is used to train the the two classifiers and a combinationof them (constructed by multiplying their predictions) is eventually the one used to label new docu-ments.
Five strategies were applied to obtain the views: (a) using unigrams and bigrams as features, (b)randomly splitting the feature set in two, (c) using two different supervised learning algorithms becausethey would provide useful examples to each other since based on different learning assumptions; (d)randomly splitting the training set, and (e) applying a character-based language model (CLM) and a bag-of-words model (BOW).
We extended the third strategy by using three classifiers and two different viewsfor each of them, and by applying this to the task of annotation validation rather than semi-supervisedlearning.6 ConclusionsIn this paper we have presented a methodology that makes use of multiple classifiers (based on differentviews) in order to detect inconsistent annotations and borderline cases.
In our test set, we found thatin 35% of the wrongly classified cases the predictions were different but acceptable, and in the 16% ofthem the predictions suggested that the gold standard was wrong.
On the other hand, the data resultingfrom such procedure related to non-disagreeing predictions can be regarded as expression of either theefficacy of the annotation schema and guidelines or the features used for the machine learning step.Our next goal is to improve the performances of the classifiers over the instances that were incorrectlyhandled, currently accounting for the 26% in our test set.
We will also test the same methodology overthe extraction of the link between targets and modifiers (appraisal groups).
The machine learning models,the datasets and the error analysis are publicly available in order to ensure reproducibility3.3http://corpus.leeds.ac.uk/marilena/SentiML/LAW2014_error_analysis.zip85ReferencesSteven Abney.
2007.
Semisupervised Learning for Computational Linguistics.
Chapman & Hall/CRC, 1st edition.Ron Artstein and Massimo Poesio.
2008.
Inter-coder agreement for computational linguistics.
Comput.
Linguist.,34(4):555?596, December.Petra S. Bayerl, Harald L?ungen, Ulrike Gut, and Karsten I. Paul.
2003.
Methodology for reliable schema de-velopment and evaluation of manual annotations.
In Proceedings of the Workshop on Knowledge Markup andSemantic Annotation at the Second International Conference on Knowledge Capture (K-CAP 2003, pages 17?23.Mauro Cettolo, Christian Girardi, and Marcello Federico.
2012.
Wit3: Web inventory of transcribed and translatedtalks.
In Proceedings of the 16thConference of the European Association for Machine Translation (EAMT),pages 261?268, Trento, Italy, May.Marilena Di Bari, Serge Sharoff, and Martin Thomas.
2013.
SentiML: Functional annotation for multilingual sen-timent analysis.
In DH-case 2013: Collaborative Annotations in Shared Environments: metadata, vocabulariesand techniques in the Digital Humanities, ACM International Conference Proceedings.Wei Jin, Hung H. Ho, and Rohini K. Srihari.
2009.
Opinionminer: a novel machine learning system for web opin-ion mining and extraction.
In Proceedings of the 15th ACM SIGKDD international conference on Knowledgediscovery and data mining, KDD ?09, pages 1195?1204, New York, NY, USA.
ACM.Saif Mohammad.
2011.
From once upon a time to happily ever after: Tracking emotions in novels and fairy tales.In Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences,and Humanities, pages 105?114, Portland, OR, USA, June.Joakim Nivre.
2005.
Dependency grammar and dependency parsing.
Technical report, V?axj?o University.James Pustejovsky and Amber Stubbs.
2012.
Natural Language Annotation for Machine Learning.
Oreilly andAssociate Series.
O?Reilly Media, Incorporated.Jonathon Read, David Hope, and John Carroll.
2007.
Annotating expressions of appraisal in English.
In Proceed-ings of the Linguistic Annotation Workshop, LAW ?07, pages 93?100, Stroudsburg, PA, USA.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and Andrew Y. Ng.
2008.
Cheap and fast?but is it good?
:Evaluating non-expert annotations for natural language tasks.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP ?08, pages 254?263, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Theresa Ann Wilson.
2008.
Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity,and Attitudes of Private States.
Ph.D. thesis, University of Pittsburgh.Ning Yu.
2014.
Exploring co-training strategies for opinion detection.
Journal of the Asssociation for InformationScience and Technology.86
