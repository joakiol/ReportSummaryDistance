Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 146?149,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsDERIUNLP: A Context Based Approach to Automatic KeyphraseExtractionGeorgeta BordeaUnit for Natural Language ProcessingDigital Enterprise Research InstituteNational University of Ireland, Galwaygeorgeta.bordea@deri.orgPaul BuitelaarUnit for Natural Language ProcessingDigital Enterprise Research InstituteNational University of Ireland, Galwaypaul.buitelaar@deri.orgAbstractThe DERI UNLP team participated in theSemEval 2010 Task #5 with an unsuper-vised system that automatically extractskeyphrases from scientific articles.
Ourapproach does not only consider a generaldescription of a term to select keyphrasecandidates but also context information inthe form of ?skill types?.
Even thoughour system analyses only a limited set ofcandidates, it is still able to outperformbaseline unsupervised and supervised ap-proaches.1 IntroductionKeyphrases provide users overwhelmed by therichness of information currently available withuseful insight into document content but at thesame time they are a valuable input for a variety ofNLP applications such as summarization, cluster-ing and searching.
The SemEval 2010 competitionincluded a task targeting the Automatic KeyphraseExtraction from Scientific Articles (Kim et al,2010).
Given a set of scientific articles partic-ipants are required to assign to each documentkeyphrases extracted from text.We participated in this task with an unsuper-vised approach for keyphrase extraction that doesnot only consider a general description of a termto select candidates but also takes into consider-ation context information.
The larger context ofour work is the extraction of expertise topics forExpertise Mining (Bordea, 2010).Expertise Mining is the task of automaticallyextracting expertise topics and expertise profilesfrom a collection of documents.
Even though theExpertise Mining task and the Keyphrase Extrac-tion task are essentially different, it is importantto assess the keyphraseness of extracted expertisetopics, i.e., their ability to represent the contentof a document.
Here we will report only relevantfindings for the Keyphrase Extraction task, focus-ing on the overlapping aspects of the two afore-mentioned tasks.After giving an overview of related work in sec-tion 2 we introduce skill types and present our can-didate selection method in section 3.
Section 4 de-scribes the features used for ranking and filteringthe candidate keyphrases and Section 5 presentsour results before we conclude in Section 6.2 Related WorkThe current methods for keyphrase extraction canbe categorized in supervised and unsupervised ap-proaches.
Typically any keyphrase extraction sys-tem works in two stages.
In the first stage a gen-eral set of candidates is selected by extracting thetokens of a text.
In the second stage unsupervisedapproaches combine a set of features in a rank toselect the most important keyphrases and super-vised approaches use a training corpus to learn akeyphrase extraction model.Mihalcea and Tarau (2004) propose an unsuper-vised approach that considers single tokens as ver-tices of a graph and co-occurrence relations be-tween tokens as edges.
Candidates are ranked us-ing PageRank and adjacent keywords are mergedinto keyphrases in a post-processing step.
Thefrequency of noun phrase heads is exploited byBarker and Cornacchia (2000), using noun phrasesas candidates and ranking them based on term fre-quency and term length.Kea is a supervised system that uses all n-gramsof a certain length, a Naive Bayes classifier andtf-idf and position features (Frank et al, 1999).Turney (2000) introduces Extractor, a supervisedsystem that selects stems and stemmed n-gramsas candidates and tunes its parameters (mainly re-lated to frequency, position, length) with a ge-netic algorithm.
Hulth (2004) experiments withthree types of candidate terms (i.e., n-grams, nounphrase chunks and part-of-speech tagged words146that match a set of patterns) and constructs classi-fiers by rule induction using features such as termfrequency, collection frequency, relative positionand PoS tags.The candidate selection method is the main dif-ference between our approach and previous work.We did not use only a general description of a termto select candidates, but we also took into consid-eration context information.3 The Skill Types Candidate SelectionMethodSkill types are important domain words that aregeneral enough to be used in different subfieldsand that reflect theoretical or practical expertise.Consider for instance the following extracts fromscientific articles:...analysis of historical trends......duplicate photo detection algorithm ......approach for data assimilation......methodology for reservoir characterization...In all four examples the expertise topic (e.g.,?historical trends?, ?duplicate photo detection al-gorithm?, ?data assimilation?, ?reservoir charac-terization?)
is introduced by a skill type (e.g.,?analysis?, ?algorithm?, ?approach?, ?methodol-ogy?).
Some of these skill types are valid forany scientific area (e.g.
?approach?, ?method?,?analysis?, ?solution?)
but we can also identifydomain specific skill types, e.g., for computerscience ?implementation?, ?algorithm?, ?develop-ment?, ?framework?, for physics ?proof?, ?prin-ciples?, ?explanation?
and for chemistry ?law?,?composition?, ?mechanism?, ?reaction?, ?struc-ture?.Our system is based on the GATE natural lan-guage processing framework (Cunningham et al,2002) and it uses the ANNIE IE system includedin the standard GATE distribution for text tok-enization, sentence splitting and part-of-speechtagging.
The GATE processing pipeline is de-picted in Figure 1, where the light grey boxes em-body components available as part of the GATEframework whereas the dark grey boxes representcomponents implemented as part of our system.We manually extract a set of 81 single word skilltypes for the Computer Science field by analysingword frequencies for topics from the ACM classi-fication system1.
The skill types that appear most1ACM classification system: http://www.acm.org/about/class/Figure 1: GATE Processing Pipelinefrequently in keyphrases given in the training setare ?system?, ?model?
and ?information?.
TheSkill Types Gazetteer adds annotations for skilltypes and then the JAPE Transducer uses regularexpressions to annotate candidates.We rely on a syntactic description of a term todiscover candidate keyphrases that appear in theright context of a skill type or that include a skilltype.
The syntactic pattern for a term is definedby a sequence of part-of-speech tags, mainly anoun phrase.
We consider that a noun phrase is ahead noun accompanied by a set of modifiers (i.enouns, adjectives) that includes proper nouns, car-dinal numbers (e.g., ?P2P systems?)
and gerunds(e.g., ?ontology mapping?, ?data mining?).
Termsthat contain the preposition ?of?
(e.g., ?quality ofservice?)
or the conjunction ?and?
(e.g., ?searchand rescue?)
were also allowed.4 Ranking and FilteringFor the ranking stage we use several features al-ready proposed in the literature such as length ofa keyphrase, tf-idf and position.
We also take intoconsideration the collection frequency in the con-text of a skill type.Ranking.
Longer candidates in terms ofnumber of words are ranked higher, because theyare more descriptive.
Keyphrases that appearmore frequently with a skill type in the collectionof documents are also ranked higher.
Thereforewe define the rank for a topic as:147Method 5P 5R 5F 10P 10R 10F 15P 15R 15FTF-IDF 22 7.5 11.19 17.7 12.07 14.35 14.93 15.28 15.1NB 21.4 7.3 10.89 17.3 11.8 14.03 14.53 14.87 14.7ME 21.4 7.3 10.89 17.3 11.8 14.03 14.53 14.87 14.7DERIUNLP 27.4 9.35 13.94 23 15.69 18.65 22 22.51 22.25DUB 15.83 5.13 7.75 13.40 8.68 10.54 13.33 12.96 13.14Table 1: Baseline and DERIUNLP Performance aver Combined KeywordsSystem 5P 5R 5F 10P 10R 10F 15P 15R 15FBest 39.0 13.3 19.8 32.0 21.8 26.0 27.2 27.8 27.5Average 29.6 10.1 15 26.1 17.8 21.2 21.9 22.4 22.2Worst 9.4 3.2 4.8 5.9 4.0 4.8 5.3 5.4 5.3DERIUNLP 27.4 9.4 13.9 23.0 15.7 18.7 22.0 22.5 22.3Table 2: Performance over Combined KeywordsRi,j= Tni?
Fni?
tfidfi,jWhere Riis the rank for the candidate i and thedocument j, Tniis the normalized number of to-kens (number of tokens divided by the maximumnumber of tokens for a keyphrase), Fniis the nor-malized collection frequency of the candidate inthe context of a skill type (collection frequency di-vided by the maximum collection frequency), andtfidfiis the TF-IDF for candidate i and topic j(computed based on extracted topics not based onall words).Filtering.
Several approaches (Paukkeri et al,2008; Tomokiyo and Hurst, 2003) use a referencecorpus for keyphrase extraction.
We decided touse the documents available on the Web as a ref-erence corpus, therefore we use an external websearch engine to filter out the candidates that aretoo general from the final result set.
If a candi-date has more than 109hits on the web it is toogeneral to be included in the final result set.
A lotof noise is introduced by general combination ofwords that could appear in any document.
We re-move candidates longer than eight words and weignore keyphrases that have one letter words orthat include non-alphanumerical characters.Acronyms.
Acronyms usually replace longor frequently referenced terms.
Results are im-proved by analysing acronyms (Krulwich andBurkey, 1996) because most of the times the ex-panded acronym is reported as a keyphrase, not theacronym and because our rank is sensitive to thenumber of words in a keyphrase.
We consider thelength of an acronym to be the same as the lengthof its expansion and we report only the expansionas a keyphrase.Position.
The candidates that appear in the titleor the introduction of a document are more likelyto be relevant for the document.
We divide eachdocument in 10 sections relative to document sizeand we increase the ranks for keyphrases first men-tioned in one of these sections (200% increase forthe first section, 100% increase for the second sec-tion and 25% for the third section).
Candidateswith a first appearance in the last section of a doc-ument are penalised by 25%.5 EvaluationThe SemEval task organizers provided two setsof scientific articles, a set of 144 documents fortraining and a set of 100 documents for test-ing.
No information was provided about the sci-entific domain of the articles but at least someof them are from Computer Science.
The av-erage length of the articles is between 6 and8 pages including tables and pictures.
Threesets of answers were provided: author-assignedkeyphrases, reader-assigned keyphrases and com-bined keyphrases (combination of the first twosets).
The participants were asked to assign a num-ber of exactly 15 keyphrases per document.All reader-assigned keyphrases are extractedfrom the papers, whereas some of the author-assigned keyphrases do not occur explicitly in thetext.
Two alternations of keyphrase are accepted:A of B / B A and A?s B.
In case that the seman-tics changes due to the alternation, the alternationis not included in the answer set.
The traditionalevaluation metric was followed, matching the ex-tracted keyphrases with the keyphrases in the an-swer sets and calculating precision, recall and F-score.
In both tables the column labels start with anumber which stands for the top 5, 10 or 15 candi-dates.
The characters P, R, F mean micro-averagedprecision, recall and F-scores.
For baselines, 1, 2,3 grams were used as candidates and TF-IDF asfeatures.In Table 1 the keyphrases extracted by our sys-tem are compared with keyphrases extracted by148an unsupervised method that ranks the candidatesbased on TF-IDF scores and two supervised meth-ods using Naive Bayes (NB) and maximum en-tropy(ME) in WEKA2.
Our performance is wellabove the baseline in all cases.To show the contribution of skill types we in-cluded the results for a baseline version of oursystem (DUB) that does not rank the candidatesusing the normalized collection frequency in thecontext of a skill type Fnibut the overall collec-tion frequency (i.e., the number of occurrences ofa keyphrase in the corpus).
The significantly in-creased results compared to our baseline versionshow the effectiveness of skill types for keyphrasecandidate ranking.Table 2 presents our results in comparison withresults of other participants.
Even though our sys-tem considers in the first stage a significantly lim-ited set of candidates the results are very close tothe average results of other participants.
Our sys-tem performed 8th best out of 19 participants fortop 15 keyphrases, 10th best for top 10 keyphrasesand 13th best for top 5 keyphrases, which indicatesthat our approach could be improved by using amore sophisticated ranking method.6 ConclusionsIn this paper we have reported the performanceof an unsupervised approach for keyphrase extrac-tion that does not only consider a general descrip-tion of a term to select keyphrase candidates butalso takes into consideration context information.The method proposed here uses term extractiontechniques (the syntactic description of a term),classical keyword extraction techniques(TF-IDF,length, position) and contextual evidence (skilltypes).We argued that so called ?skill types?
(e.g.,?methods?, ?approach?, ?analysis?)
are a usefulinstrument for selecting keyphrases from a doc-ument.
Another novel aspect of this approach isusing the collection of documents available on theWeb (i.e., number of hits for a keyphrase) insteadof a reference corpus.
It would be interesting toevaluate the individual contributions of skill typesfor Keyphrase Extraction by adding them as a fea-ture in a classical system like KEA.Future work will include an algorithm for auto-matic extraction of skill types for a domain and ananalysis of the performance of each skill type.2WEKA:http://www.cs.waikato.ac.nz/ml/weka/7 AknowledgementsThis work is supported by Science Foundation Ire-land under Grant No.
SFI/08/CE/I1380 (Lion-2).ReferencesKen Barker and Nadia Cornacchia.
2000.
Using NounPhrase Heads to Extract Document Keyphrases.
InCanadian Conference on AI, pages 40?52.
Springer.Georgeta Bordea.
2010.
Concept Extraction Appliedto the Task of Expert Finding.
In Extended SemanticWeb Conference 2010, PhD Symposium.
Springer.H.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2002.
GATE: A Framework and Graph-ical Development Environment for Robust NLPTools and Applications.
In Proceedings of the 40thAnniversary Meeting of the Association for Compu-tational Linguistics.Eibe Frank, Gordon W Paynter, Ian H Witten, CarlGutwin, and Craig G Nevill-Manning.
1999.Domain-Specific Keyphrase Extraction.
In Pro-ceedings of the 16th International Joint Conferenceon Aritfiicial Intelligence, pages 668?673.Anette Hulth.
2004.
Enhancing Linguistically Ori-ented Automatic Keyword Extraction.
In Proceed-ings of HLT/NAACL: Short Papers, pages 17?20.Su Nam Kim, Alyona Medelyan, Min-Yen Kan, andTimothy Baldwin.
2010.
SemEval-2010 Task 5:Automatic Keyphrase Extraction from Scientific Ar-ticles.
In Proceedings of the ACL 2010 Workshop onEvaluation Exercises on Semantic Evaluation (Se-mEval 2010).Bruce Krulwich and Chad Burkey.
1996.
Learn-ing user information interests through extraction ofsemantically significant phrases.
In Proc.
AAAISpring Symp.
Machine Learning in Information Ac-cess, Menlo Park, Calif. Amer.
Assoc.
for ArtificialIntelligence.Rada Mihalcea and Paul Tarau.
2004.
Textrank:Bringing order into texts.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 404?411.Mari-Sanna Paukkeri, Ilari T. Nieminen, Polla Matti,and Timo Honkela.
2008.
A Language-IndependentApproach to Keyphrase Extraction and Evaluation.In Coling 2008 Posters, number August, pages 83?86.Takashi Tomokiyo and Matthew Hurst.
2003.
A Lan-guage Model Approach to Keyphrase Extraction.
InProceedings of the ACL 2003 work- shop on Multi-word expressions, pages 33?40.Peter D Turney.
2000.
Learning algorithms forkeyphrase extraction.
Information Retrieval, 2:303?336.149
