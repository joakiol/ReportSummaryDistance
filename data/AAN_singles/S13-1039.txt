Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 266?275, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsPredicting the Compositionality of Multiword ExpressionsUsing Translations in Multiple LanguagesBahar Salehi??
and Paul Cook??
NICTA Victoria Research Laboratory?
Department of Computing and Information SystemsThe University of MelbourneVictoria 3010, Australiabsalehi@student.unimelb.edu.au, paulcook@unimelb.edu.auAbstractIn this paper, we propose a simple, language-independent and highly effective method forpredicting the degree of compositionality ofmultiword expressions (MWEs).
We comparethe translations of an MWE with the trans-lations of its components, using a range ofdifferent languages and string similarity mea-sures.
We demonstrate the effectiveness ofthe method on two types of English MWEs:noun compounds and verb particle construc-tions.
The results show that our approach iscompetitive with or superior to state-of-the-artmethods over standard datasets.1 Compositionality of MWEsA multiword expression (MWE) is any combina-tion of words with lexical, syntactic or semanticidiosyncrasy (Sag et al 2002; Baldwin and Kim,2009), in that the properties of the MWE are notpredictable from the component words.
For exam-ple, with ad hoc, the fact that neither ad nor hoc arestandalone English words, makes ad hoc a lexically-idiosyncratic MWE; with shoot the breeze, on theother hand, we have semantic idiosyncrasy, as themeaning of ?to chat?
in usages such as It was goodto shoot the breeze with you1 cannot be predictedfrom the meanings of the component words shootand breeze.Semantic idiosyncrasy has been of particular in-terest to NLP researchers, with research on bi-nary compositional/non-compositional MWE clas-1The example is taken from http://www.thefreedictionary.comsification (Lin, 1999; Baldwin et al 2003), ora three-way compositional/semi-compositional/non-compositional distinction (Fazly and Stevenson,2007).
There has also been research to suggest thatMWEs span the entire continuum from full compo-sitionality to full non-compositionality (McCarthy etal., 2003; Reddy et al 2011).Investigating the degree of MWE compositional-ity has been shown to have applications in informa-tion retrieval and machine translation (Acosta et al2011; Venkatapathy and Joshi, 2006).
As an exam-ple of an information retrieval system, if we werelooking for documents relating to rat race (mean-ing ?an exhausting routine that leaves no time forrelaxation?2), we would not be interested in docu-ments on rodents.
These results underline the needfor methods for broad-coverage MWE composition-ality prediction.In this research, we investigate the possibility ofusing an MWE?s translations in multiple languagesto measure the degree of the MWE?s compositional-ity, and investigate how literal the semantics of eachcomponent is within the MWE.
We use Panlex totranslate the MWE and its components, and comparethe translations of the MWE with the translationsof its components using string similarity measures.The greater the string similarity, the more composi-tional the MWE is.Whereas past research on MWE compositionalityhas tended to be tailored to a specific MWE type(McCarthy et al 2007; Kim and Baldwin, 2007;Fazly et al 2009), our method is applicable toany MWE type in any language.
Our experiments2This definition is from WordNet 3.1.266over two English MWE types demonstrate that ourmethod is competitive with state-of-the-art methodsover standard datasets.2 Related WorkMost previous work on measuring MWE composi-tionality makes use of lexical, syntactic or semanticproperties of the MWE.
One early study on MWEcompositionality was Lin (1999), who claimed thatthe distribution of non-compositional MWEs (e.g.shoot the breeze) differs significantly from the dis-tribution of expressions formed by substituting oneof the components with a semantically similar word(e.g.
shoot the wind).
Unfortunately, the methodtends to fall down in cases of high statistical id-iosyncrasy (or ?institutionalization?
): consider fry-ing pan which is compositional but distributionallyvery different to phrases produced through word-substitution such as sauteing pan or frying plate.Some research has investigated the syntacticproperties of MWEs, to detect their composition-ality (Fazly et al 2009; McCarthy et al 2007).The assumption behind these methods is that non-compositional MWEs are more syntactically fixedthan compositional MWEs.
For example, make a de-cision can be passivised, but shoot the breeze cannot.One serious problem with syntax-based methods istheir lack of generalization: each type of MWE hasits own characteristics, and these characteristics dif-fer from one language to another.
Moreover, someMWEs (such as noun compounds) are not flexiblesyntactically, no matter whether they are composi-tional or non-compositional (Reddy et al 2011).Much of the recent work on MWEs focuses ontheir semantic properties, measuring the semanticsimilarity between the MWE and its components us-ing different resources, such as WordNet (Kim andBaldwin, 2007) or distributional similarity relativeto a corpus (e.g.
based on Latent Semantic Analysis:Schone and Jurafsky (2001), Bannard et al(2003),Reddy et al(2011)).
The size of the corpus is im-portant in methods based on distributional similarity.Unfortunately, however, large corpora are not avail-able for all languages.Reddy et al(2011) hypothesize that the num-ber of common co-occurrences between a givenMWE and its component words indicates the de-gree of compositionality of that MWE.
First, the co-occurrences of a given MWE/word are consideredas the values of a vector.
They then measure theCosine similarity between the vectors of the MWEand its components.
Bannard et al(2003) presentedfour methods to measure the compositionality of En-glish verb particle constructions.
Their best resultis based on the previously-discussed method of Lin(1999) for measuring compositionality, but uses amore-general distributional similarity model to iden-tify synonyms.Recently, a few studies have investigated usingparallel corpora to detect the degree of composi-tionality (Melamed, 1997; Moiro?n and Tiedemann,2006; de Caseli et al 2010; Salehi et al 2012).The general approach is to word-align the sourceand target language sentences and analyse align-ment patterns for MWEs (e.g.
if the MWE is al-ways aligned as a single ?phrase?, then it is a strongindicator of non-compositionality).
de Caseli etal.
(2010) consider non-compositional MWEs to bethose candidates that align to the same target lan-guage unit, without decomposition into word align-ments.
Melamed (1997) suggests using mutual in-formation to investigate how well the translationmodel predicts the distribution of words in the tar-get text given the distribution of words in the sourcetext.
Moiro?n and Tiedemann (2006) show that en-tropy is a good indicator of compositionality, be-cause word alignment models are often confused bynon-compositional MWEs.
However, this assump-tion does not always hold, especially when deal-ing with high-frequency non-compositional MWEs.Salehi et al(2012) tried to solve this problem withhigh frequency MWEs by using word alignment inboth directions.3 They computed backward and for-ward entropy to try to remedy the problem with es-pecially high-frequency phrases.
However, their as-sumptions were not easily generalisable across lan-guages, e.g., they assume that the relative frequencyof a specific type of MWE (light verb constructions)in Persian is much greater than in English.Although methods using bilingual corpora are in-tuitively appealing, they have a number of draw-backs.
The first and the most important problem3The IBM models (Brown et al 1993), e.g., are not bidi-rectional, which means that the alignments are affected by thealignment direction.267is data: they need large-scale parallel bilingual cor-pora, which are available for relatively few languagepairs.
Second, since they use statistical measures,they are not suitable for measuring the composition-ality of MWEs with low frequency.
And finally,most experiments have been carried out on Englishpaired with other European languages, and it is notclear whether the results translate across to otherlanguage pairs.3 ResourcesIn this research, we use the translations of MWEsand their components to estimate the relative de-gree of compositionality of a MWE.
There areseveral resources available to translate words intovarious languages such as Babelnet (Navigli andPonzetto, 2010),4 Wiktionary,5 Panlex (Baldwin etal., 2010) and Google Translate.6 As we are ide-ally after broad coverage over multiple languagesand MWEs/component words in a given language,we exclude Babelnet and Wiktionary from our cur-rent research.
Babelnet covers only six languagesat the time of writing this paper, and in Wiktionary,because it is constantly being updated, words andMWEs do not have translations into the same lan-guages.
This leaves translation resources such asPanlex and Google Translate.
However, after man-ually analysing the two resources for a range ofMWEs, we decided not to use Google Translate fortwo reasons: (1) we consider the MWE out of con-text (i.e., we are working at the type level and do notconsider the usage of the MWE in a particular sen-tence), and Google Translate tends to generate com-positional translations of MWEs out of context; and(2) Google Translate provides only one translationfor each component word/MWE.
This left Panlex.Panlex is an online translation database that isfreely available.
It contains lemmatized words andMWEs in a large variety of languages, with lemma-based (and less frequently sense-based) links be-tween them.
The database covers more than 1353languages, and is made up of 12M lemmas and ex-pressions.
The translations are sourced from hand-made electronic dictionaries, making it more accu-4http://lcl.uniroma1.it/babelnet/5http://www.wiktionary.org/6http://translate.google.com/rate than translation dictionaries generated automat-ically, e.g.
through word alignment.
Usually thereare several direct translations for a word/MWEfrom one language to another, as in translationswhich were extracted from electronic dictionaries.
Ifthere is no direct translation for a word/MWE in thedatabase, we can translate indirectly via one or morepivot languages (indirect translation: Soderland etal.
(2010)).
For example, English ivory tower hasdirect translations in only 13 languages in Panlex,including French (tour d?ivoire) but not Esperanto.There is, however, a translation of tour d?ivoire intoEsperanto (ebura turo), allowing us to infer an indi-rect translation between ivory tower and ebura turo.4 DatasetWe evaluate our method over two datasets, as de-scribed below.REDDY (Reddy et al 2011): 90 English (binary)noun compounds (NCs), where the overall NC andeach component word has been annotated for com-positionality on a scale from 0 (non-compositional)to 5 (compositional).
In order to avoid issueswith polysemy, the annotators were presented witheach NC in a sentential context.
The authors triedto achieve a balance of compositional and non-compositional NCs: based on a threshold of 2.5, thedataset consists of 43 (48%) compositional NCs, 46(51%) NCs with a compositional usage of the firstcomponent, and 54 (60%) NCs with a compositionalusage of the second component.BANNARD (Bannard, 2006): 160 English verbparticle constructions (VPCs) were annotated forcompositionality relative to each of the two compo-nent words (the verb and the particle).
Each annota-tor was asked to annotate each of the verb and parti-cle as yes, no or don?t know.
Based on the ma-jority annotation, among the 160 VPCs, 122 (76%)are verb-compositional and 76 (48%) are particle-compositional.We compute the proportion of yes tags to get thecompositionality score.
This dataset, unlike REDDY,does not include annotations for the compositional-ity of the whole VPC, and is also less balanced, con-taining more VPCs which are verb-compositionalthan verb-non-compositional.268ScorePanlexComponentsMWETranslateCompare......TranslationsFigure 1: Schematic of our proposed method5 MethodTo predict the degree of compositionality of anMWE, we require a way to measure the semanticsimilarity of the MWE with its components.
Ourhypothesis is that compositional MWEs are morelikely to be word-for-word translations in a givenlanguage than non-compositional MWEs.
Hence, ifwe can locate the translations of the components inthe translation of the MWE, we can deduce that itis compositional.
Our second hypothesis is that themore languages we use as the basis for determin-ing translation similarity between the MWE and itscomponent words, the more accurately we will beable to estimate compositionality.
Thus, rather thanusing just one translation language, we experimentwith as many languages as possible.Figure 1 provides a schematic outline of ourmethod.
The MWE and its components are trans-lated using Panlex.
Then, we compare the transla-tion of the MWE with the translations of its compo-nents.
In order to locate the translation of each com-ponent in the MWE translation, we use string simi-English Persian Translationkick the bucket mordkick zadthe ?bucket satlmake a decision tasmim gereftmake sakhta yekdecision tasmimpublic service khadamaat omumipublic omumiservice khedmatTable 1: English MWEs and their components with theirtranslation in Persian.
Direct matches between the trans-lation of a MWE and its components are shown in bold;partial matches are underlined.larity measures.
The score shown in Figure 1 is de-rived from a given language.
In Section 6, we showhow to combine scores across multiple languages.As an example of our method, consider theEnglish-to-Persian translation of kick the bucket asa non-compositional MWE and make a decision asa semi-compositional MWE (Table 1).7 By locatingthe translation of decision (tasmim) in the translationofmake a decision (tasmim gereftan), we can deducethat it is semi-compositional.
However, we cannotlocate any of the component translations in the trans-lation of kick the bucket.
Therefore, we concludethat it is non-compositional.
Note that in this simpleexample, the match is word-level, but that due to theeffects of morphophonology, the more likely situa-tion is that the components don?t match exactly (aswe observe in the case of khadamaat and khedmatfor the public service example), which motivates ouruse of string similarity measures which can capturepartial matches.We consider the following string similarity mea-sures to compare the translations.
In each case,we normalize the output value to the range [0, 1],where 1 indicates identical strings and 0 indicatescompletely different strings.
We will indicate thetranslation of the MWE in a particular language t asMWE t, and the translation of a given component in7Note that the Persian words are transliterated into Englishfor ease of understanding.269language t as component t.Longest common substring (LCS): The LCSmeasure finds the longest common substring be-tween two strings.
For example, the LCS betweenABABC and BABCAB is BABC.
We calculate a nor-malized similarity value based on the length of theLCS as follows:LongestCommonString (MWE t, component t)min(len(MWE t), len(component t))Levenshtein (LEV1): The Levenshtein distancecalculates for the number of basic edit operations re-quired to transpose one word into the other.
Editsconsist of single-letter insertions, deletions or sub-stitutions.
We normalize LEV1 as follows:1?
LEV1 (MWEt, component t)max(len(MWE t), len(component t))Levenshtein with substitution penalty (LEV2):One well-documented feature of Levenshtein dis-tance (Baldwin, 2009) is that substitutions are in factthe combination of an addition and a deletion, and assuch can be considered to be two edits.
Based on thisobservation, we experiment with a variant of LEV1with this penalty applied for substitutions.
Similarlyto LEV1, we normalize as follows:1?
LEV2 (MWEt, component t)len(MWE t) + len(component t)Smith Waterman (SW) This method is based onthe Needleman-Wunsch algorithm,8 and was devel-oped to locally-align two protein sequences (Smithand Waterman, 1981).
It finds the optimal simi-lar regions by maximizing the number of matchesand minimizing the number of gaps necessary toalign the two sequences.
For example, the opti-mal local sequence for the two sequences below isAT?
?ATCC, in which ?-?
indicates a gap:8The Needleman-Wunsch (NW) algorithm, was designed toalign two sequences of amino-acids (Needleman and Wunsch,1970).
The algorithm looks for the sequence alignment whichmaximizes the similarity.
As with the LEV score, NW min-imizes edit distance, but also takes into account character-to-character similarity based on the relative distance between char-acters on the keyboard.
We exclude this score, because it ishighly similar to the LEV scores, and we did not obtain encour-aging results using NW in our preliminary experiments.Seq1: ATGCATCCCATGACSeq2: TCTATATCCGTAs the example shows, it looks for the longest com-mon string but has an in-built mechanism for includ-ing gaps in the alignment (with penalty).
This char-acteristic of SW might be helpful in our task, be-cause there may be morphophonological variationsbetween the MWE and component translations (asseen above in the public service example).
We nor-malize SW similarly to LCS:len(alignedSequence)min(len(MWE t), len(component t))6 Computational ModelGiven the scores calculated by the aforementionedstring similarity measures between the translationsfor a given component word and the MWE, we needsome way of combining scores across componentwords.9 First, we measure the compositionality ofeach component within the MWE (s1 and s2):s1 = f1(sim1(w1,MWE), ..., simi(w1,MWE ))s2 = f1(sim1(w2,MWE), ..., simi(w2,MWE ))where sim is a string similarity measure, simi indi-cates that the calculation is based on translations inlanguage i, and f1 is a score combination function.Then, we compute the overall compositionality ofthe MWE (s3) from s1 and s2 using f2:s3 = f2(s1, s2)Since we often have multiple translations for a givencomponent word/MWE in Panlex, we exhaustivelycompute the similarity between each MWE transla-tion and component translation, and use the highestsimilarity as the result of simi.
If an instance doesnot have a direct/indirect translation in Panlex, weassign a default value, which is the mean of the high-est and lowest annotation score (2.5 for REDDY and0.5 for BANNARD).
Note that word order is not anissue in our method, as we calculate the similarityindependently for each MWE component.In this research, we consider simple functions forf1 such as mean, median, product, min and max.
f29Note that in all experiments we only combine scores givenby the same string similarity measure.270NCLanguage Frequency FamilyCzech 100 SlavicNorwegian 100 GermanicPortuguese 100 RomanceThai 99 Kam-thaiFrench 95 RomanceChinese 94 ChineseDutch 93 GermanicRomanian 91 RomanceHindi 67 IndicRussian 43 SlavicTable 2: The 10 best languages for REDDY using LCS.was selected to be the same as f1 in all situations,except when we use mean for f1.
Here, followingReddy et al(2011), we experimented with weightedmean:f2(s1, s2) = ?s1 + (1?
?
)s2Based on 3-fold cross validation, we chose ?
= 0.7for REDDY.10Since we do not have judgements for the com-positionality of the full VPC in BANNARD (we in-stead have separate judgements for the verb andparticle), we cannot use f2 for this dataset.
Ban-nard et al(2003) observed that nearly all of theverb-compositional instances were also annotated asparticle-compositional by the annotators.
In linewith this observation, we use s1 (based on the verb)as the compositionality score for the full VPC.7 Language SelectionOur method is based on the translation of an MWEinto many languages.
In the first stage, we chose 54languages for which relatively large corpora wereavailable.11 The coverage, or the number of in-stances which have direct/indirect translations inPanlex, varies from one language to another.
Inpreliminary experiments, we noticed that there isa high correlation (about 0.50 for BANNARD and10We considered values of ?
from 0 to 1, incremented by 0.1.11In future work, we intend to look at the distribution of trans-lations of the given MWE and its components in corpora formany languages.
The present method does not rely on the avail-ability of large corpora.VPC:verbLanguage Frequency FamilyBasque 100 BasqueLithuanian 100 BalticSlovenian 100 SlavicHebrew 99 SemiticArabic 98 SemiticCzech 95 SlavicSlovak 92 SlavicLatin 79 ItalicTagalog 74 AustronesianPolish 44 SlavicTable 3: The 10 best languages for the verb componentof BANNARD using LCS.VPC:particleLanguage Frequency FamilyFrench 100 RomanceIcelandic 100 GermanicThai 100 Kam-thaiIndonesian 92 IndonesianSpanish 90 RomanceTamil 87 DravidianTurkish 83 TurkicCatalan 79 RomanceOccitan 76 RomanceRomanian 69 RomanceTable 4: The 10 best languages for the particle compo-nent of BANNARD using LCS.about 0.80 for REDDY) between the usefulness ofa language and its translation coverage on MWEs.Therefore, we excluded languages with MWE trans-lation coverage of less than 50%.
Based on nested10-fold cross validation in our experiments, we se-lect the 10 most useful languages for each cross-validation training partition, based on the Pearsoncorrelation between the given scores in that languageand human judgements.12 The 10 best languagesare selected based only on the training set for eachfold.
(The languages selected for each fold will laterbe used to predict the compositionality of the itemsin the testing portion for that fold.)
In Tables 2, 312Note that for VPCs, we calculate the compositionality ofonly the verb part, because we don?t have the human judge-ments for the whole VPC.271f1 sim() N1 N2 NCMeanSW 0.541 0.396 0.637LCS 0.525 0.431 0.649LEV1 0.405 0.200 0.523LEV2 0.481 0.263 0.577ProdSW 0.451 0.287 0.410LCS 0.430 0.233 0.434LEV1 0.299 0.128 0.311LEV2 0.294 0.188 0.364MedianSW 0.443 0.334 0.544LCS 0.408 0.365 0.553LEV1 0.315 0.054 0.376LEV2 0.404 0.134 0.523MinSW 0.420 0.176 0.312LCS 0.347 0.225 0.307LEV1 0.362 0.310 0.248LEV2 0.386 0.345 0.338MaxSW 0.371 0.408 0.345LCS 0.406 0.430 0.335LEV1 0.279 0.362 0.403LEV2 0.380 0.349 0.406Table 5: Correlation on REDDY (NCs).
N1, N2 and NC,are the first component of the noun compound, its secondcomponent, and the noun compound itself, respectively.and 4, we show how often each language was se-lected in the top-10 languages over the combined100 (10?10) folds of nested 10-fold cross valida-tion, based on LCS.13 The tables show that the se-lected languages were mostly consistent over thefolds.
The languages are a mixture of Romance,Germanic and languages from other families (basedon Voegelin and Voegelin (1977)), with no standoutlanguage which performs well in all cases (indeed,no language occurs in all three tables).
Additionally,there is nothing in common between the verb and theparticle top-10 languages.8 ResultsAs mentioned before, we perform nested 10-foldcross-validation to select the 10 best languages onthe training data for each fold.
The selected lan-guages for a given fold are then used to compute s113Since our later results show that LCS and SW have higherresults, we only show the best languages using LCS.
Theselargely coincide with those for SW.f1 sim() Verb ParticleMeanSW 0.369 0.510LCS 0.406 0.509LEV1 0.335 0.454LEV2 0.340 0.460ProdSW 0.315 0.316LCS 0.339 0.299LEV1 0.322 0.280LEV2 0.342 0.284MedianSW 0.316 0.409LCS 0.352 0.423LEV1 0.295 0.387LEV2 0.309 0.368MinSW 0.262 0.210LCS 0.329 0.251LEV1 0.307 0.278LEV2 0.310 0.281MaxSW 0.141 0.288LCS 0.268 0.299LEV1 0.145 0.450LEV2 0.170 0.398Table 6: Correlation on BANNARD (VPC), based on thebest-10 languages for the verb and particle individuallyand s2 (and s3 for NCs) for each instance in the testset for that fold.
The scores are compared with hu-man judgements using Pearson?s correlation.
Theresults are shown in Tables 5 and 6.
Among the fivefunctions we experimented with for f1, Mean per-forms much more consistently than the others.
Me-dian is less prone to noise, and therefore performsbetter than Prod, Max and Min, but it is still worsethan Mean.For the most part, LCS and SW perform betterthan the other measures.
There is little to separatethese two methods, partly because they both look fora sequence of similar characters, unlike LEV1 andLEV2 which do not consider contiguity of match.The results support our hypothesis that using mul-tiple target languages rather than one, results in amore accurate prediction of MWE compositionality.Our best result using the 10 selected languages onREDDY is 0.649, as compared to the best single-language correlation of 0.497 for Portuguese.
OnBANNARD, the best LCS result for the verb com-ponent is 0.406, as compared to the best single-272language correlation of 0.350 for Lithuanian.Reddy et al(2011) reported a correlation of 0.714on REDDY.
Our best correlation is 0.649.
Note thatReddy et al(2011) base their method on identifi-cation of MWEs in a corpus, thus requiring MWE-specific identification.
Given that this has beenshown to be difficult for MWE types including En-glish VPCs (McCarthy et al 2003; Baldwin, 2005),the fact that our method is as competitive as this ishighly encouraging, especially when you considerthat it can equally be applied to different types ofMWEs in other languages.
Moreover, the computa-tional processing required by methods based on dis-tributional similarity is greater than our method, asit does not require processing a large corpus.Finally, we experimented with combining ourmethod (STRINGSIMMEAN) with a reimplementationof the method of Reddy et al(2011), based on sim-ple averaging, as detailed in Table 7.
The results arehigher than both component methods and the state-of-the-art for REDDY, demonstrating the comple-mentarity between our proposed method and meth-ods based on distributional similarity.In Table 8, we compare our results(STRINGSIMMEAN) with those of Bannard etal.
(2003), who interpreted the dataset as a binaryclassification task.
The dataset used in their studyis a subset of BANNARD, containing 40 VPCs, ofwhich 29 (72%) were verb compositional and 23(57%) were particle compositional.
By applying athreshold of 0.5 over the output of our regressionmodel, we binarize the VPCs into the compositionaland non-compositional classes.
According to theresults shown in Table 6, LCS is a better similaritymeasure for this task.
Our proposed method hashigher results than the best results of Bannard etal.
(2003), in part due to their reliance on VPCidentification, and the low recall on the task, asreported in the paper.
Our proposed method doesnot rely on a corpus or MWE identification.9 Error AnalysisWe analyse items in REDDY which have a high dif-ference (more than 2.5) between the human anno-tation and our scores (using LCS and Mean).
Thewords are cutting edge, melting pot, gold mine andivory tower, which are non-compositional accord-ing to REDDY.
After investigating their translations,we came to the conclusion that the first three MWEshave word-for-word translations in most languages.Hence, they disagree with our hypothesis that word-for-word translation is a strong indicator of compo-sitionality.
The word-for-word translations might bebecause of the fact that they have both compositionaland non-compositional senses, or because they arecalques (loan translations).
However, we have triedto avoid such problems with calques by using trans-lations into several languages.For ivory tower (?a state of mind that is discussedas if it were a place?
)14 we noticed that we have a di-rect translation into 13 languages.
Other languageshave indirect translations.
By checking the directtranslations, we noticed that, in French, the MWE istranslated to tour and tour d?ivoire.
A noisy (wrong)translation of tour ?tower?
resulted in wrong indirecttranslations for ivory tower and an inflated estimateof compositionality.10 Conclusion and Future WorkIn this study, we proposed a method to predict MWEcompositionality based on the translation of theMWE and its component words into multiple lan-guages.
We used string similarity measures betweenthe translations of the MWE and each of its compo-nents to predict the relative degree of composition-ality.
Among the four similarity measures that weexperimented with, LCS and SW were found to besuperior to edit distance-based methods.
Our best re-sults were found to be competitive with state-of-the-art results using vector-based approaches, and werealso shown to complement state-of-the-art methods.In future work, we are interested in investigatingwhether alternative ways of combining our proposedmethod with vector-based models can lead to fur-ther enhancements in results.
These models couldbe especially effective when comparing translationswhich are roughly synonymous but not string-wisesimilar.AcknowledgmentsWe would like to thank Timothy Baldwin, Su NamKim, and the anonymous reviewers for their valu-able comments and suggestions.14This definition is from Wordnet 3.1.273sim() STRINGSIMMEAN STRINGSIMMEAN + Reddy et alSW 0.637 0.735LCS 0.649 0.742LEV1 0.523 0.724LEV2 0.577 0.726Table 7: Correlation after combining Reddy et als method and our method with Mean for f1 (STRINGSIMMEAN ).
Thecorrelation using Reddy et als method is 0.714.Method Precision Recall F-score (?
= 1) AccuracyBannard et al(2003) 0.608 0.666 0.636 0.600STRINGSIMMEAN 0.862 0.718 0.774 0.693Table 8: Results for the classification task.
STRINGSIMMEAN is our method using Mean for f1NICTA is funded by the Australian Governmentas represented by the Department of Broadband,Communications and the Digital Economy and theAustralian Research Council through the ICT Cen-tre of Excellence program.ReferencesOtavio Costa Acosta, Aline Villavicencio, and Viviane PMoreira.
2011.
Identification and treatment of multi-word expressions applied to information retrieval.
InProceedings of the ALC Workshop on MWEs: fromParsing and Generation to the Real World (MWE2011), pages 101?109.Timothy Baldwin and Su Nam Kim.
2009.
Multiwordexpressions.
In Nitin Indurkhya and Fred J. Damerau,editors, Handbook of Natural Language Processing.CRC Press, Boca Raton, USA, 2nd edition.Timothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical model ofmultiword expression decomposability.
In Proceed-ings of the ACL-2003 Workshop on Multiword Expres-sions: Analysis, Acquisition and Treatment, pages 89?96, Sapporo, Japan.Timothy Baldwin, Jonathan Pool, and Susan M Colow-ick.
2010.
Panlex and lextract: Translating all wordsof all languages of the world.
In Proceedings of the23rd International Conference on Computational Lin-guistics: Demonstrations, pages 37?40.Timothy Baldwin.
2005.
The deep lexical acquisition ofEnglish verb-particle constructions.
Computer Speechand Language, Special Issue on Multiword Expres-sions, 19(4):398?414.Timothy Baldwin.
2009.
The hare and the tortoise:Speed and reliability in translation retrieval.
MachineTranslation, 23(4):195?240.Colin Bannard, Timothy Baldwin, and Alex Lascarides.2003.
A statistical approach to the semantics of verb-particles.
In Proceedings of the ACL 2003 workshopon Multiword expressions: analysis, acquisition andtreatment-Volume 18, pages 65?72.Colin James Bannard.
2006.
Acquiring Phrasal Lexiconsfrom Corpora.
Ph.D. thesis, University of Edinburgh.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Helena Medeiros de Caseli, Carlos Ramisch, Maria dasGrac?as Volpe Nunes, and Aline Villavicencio.
2010.Alignment-based extraction of multiword expressions.Language Resources and Evaluation, 44(1):59?77.Afsaneh Fazly and Suzanne Stevenson.
2007.
Dis-tinguishing subtypes of multiword expressions usinglinguistically-motivated statistical measures.
In Pro-ceedings of the ACL 2007Workshop on A Broader Per-spective on Multiword Expressions, pages 9?16.Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.2009.
Unsupervised type and token identificationof idiomatic expressions.
Computational Linguistics,35(1):61?103.Su Nam Kim and Timothy Baldwin.
2007.
Detectingcompositionality of english verb-particle constructionsusing semantic similarity.
In Proceedings of the 7thMeeting of the Pacific Association for ComputationalLinguistics (PACLING 2007), pages 40?48.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proceedings of the 37thannual meeting of the Association for ComputationalLinguistics on Computational Linguistics, pages 317?324.Diana McCarthy, Bill Keller, and John Carroll.
2003.Detecting a continuum of compositionality in phrasalverbs.
In Proceedings of the ACL 2003 workshop274on Multiword expressions: analysis, acquisition andtreatment-Volume 18, pages 73?80.Diana McCarthy, Sriram Venkatapathy, and Aravind KJoshi.
2007.
Detecting compositionality of verb-object combinations using selectional preferences.
InProceedings of the 2007 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL), pages 369?379.I.
Dan Melamed.
1997.
Automatic discovery of non-compositional compounds in parallel data.
In Pro-ceedings of the Fifth Workshop on Very Large Cor-pora.
EMNLP.Begona Villada Moiro?n and Jo?rg Tiedemann.
2006.Identifying idiomatic expressions using automaticword-alignment.
In Proceedings of the EACL 2006Workshop on Multi-wordexpressions in a multilingualcontext, pages 33?40.Roberto Navigli and Simone Paolo Ponzetto.
2010.
Ba-belnet: Building a very large multilingual semanticnetwork.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 216?225, Uppsala, Sweden.Saul B Needleman and Christian D Wunsch.
1970.
Ageneral method applicable to the search for similaritiesin the amino acid sequence of two proteins.
Journal ofmolecular biology, 48(3):443?453.Siva Reddy, Diana McCarthy, and Suresh Manandhar.2011.
An empirical study on compositionality in com-pound nouns.
In Proceedings of IJCNLP, pages 210?218.Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-take, and Dan Flickinger.
2002.
Multiword ex-pressions: A pain in the neck for nlp.
In Proceed-ings of the 3rd International Conference on IntelligentText Processing Computational Linguistics (CICLing-2002), pages 189?206.
Springer.Bahar Salehi, Narjes Askarian, and Afsaneh Fazly.
2012.Automatic identification of Persian light verb con-structions.
In Proceedings of the 13th InternationalConference on Intelligent Text Processing Computa-tional Linguistics (CICLing-2012), pages 201?210.Patrick Schone and Dan Jurafsky.
2001.
Is knowledge-free induction of multiword unit dictionary headwordsa solved problem.
In Proceedings of the 6th Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP 2001), pages 100?108.TF Smith and MS Waterman.
1981.
Identification ofcommonmolecular subsequences.
Molecular Biology,147:195?197.Stephen Soderland, Oren Etzioni, Daniel S Weld, KobiReiter, Michael Skinner, Marcus Sammer, Jeff Bilmes,et al2010.
Panlingual lexical translation via proba-bilistic inference.
Artificial Intelligence, 174(9):619?637.Sriram Venkatapathy and Aravind K Joshi.
2006.
Us-ing information about multi-word expressions for theword-alignment task.
In Proceedings of the Workshopon Multiword Expressions: Identifying and ExploitingUnderlying Properties, pages 20?27.Charles Frederick Voegelin and FlorenceMarie Voegelin.1977.
Classification and index of the world?s lan-guages, volume 4.
Elsevier Science Ltd.275
