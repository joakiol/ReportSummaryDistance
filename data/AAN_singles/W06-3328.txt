Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 138?145,New York City, June 2006. c?2006 Association for Computational LinguisticsBootstrapping and Evaluating Named Entity Recognition in the BiomedicalDomainAndreas VlachosComputer LaboratoryUniversity of CambridgeCambridge, CB3 0FD, UKav308@cl.cam.ac.ukCaroline GasperinComputer LaboratoryUniversity of CambridgeCambridge, CB3 0FD, UKcvg20@cl.cam.ac.ukAbstractWe demonstrate that bootstrapping a genename recognizer for FlyBase curationfrom automatically annotated noisy text ismore effective than fully supervised train-ing of the recognizer on more generalmanually annotated biomedical text.
Wepresent a new test set for this task based onan annotation scheme which distinguishesgene names from gene mentions, enablinga more consistent annotation.
Evaluatingour recognizer using this test set indicatesthat performance on unseen genes is itsmain weakness.
We evaluate extensionsto the technique used to generate trainingdata designed to ameliorate this problem.1 IntroductionThe biomedical domain is of great interest to in-formation extraction, due to the explosion in theamount of available information.
In order to dealwith this phenomenon, curated databases have beencreated in order to assist researchers to keep up withthe knowledge published in their field (Hirschman etal., 2002; Liu and Friedman, 2003).
The existenceof such resources in combination with the need toperform information extraction efficiently in orderto promote research in this domain, make it a veryinteresting field to develop and evaluate informationextraction approaches.Named entity recognition (NER) is one of themost important tasks in information extraction.
Ithas been studied extensively in various domains,including the newswire (Tjong Kim Sang andDe Meulder, 2003) domain and more recently thebiomedical domain (Blaschke et al, 2004; Kim etal., 2004).
These shared tasks aimed at evaluat-ing fully supervised trainable systems.
However,the limited availability of annotated material in mostdomains, including the biomedical, restricts the ap-plication of such methods.
In order to circum-vent this obstacle several approaches have been pre-sented, among them active learning (Shen et al,2004) and rule-based systems encoding domain spe-cific knowledge (Gaizauskas et al, 2003).In this work we build on the idea of bootstrapping,which has been applied by Collins & Singer (1999)in the newsire domain and by Morgan et al (2004)in the biomedical domain.
This approach is based oncreating training material automatically using exist-ing domain resources, which in turn is used to traina supervised named entity recognizer.The structure of this paper is the following.
Sec-tion 2 describes the construction of a new test setto evaluate named entity recognition for Drosophilafly genes.
Section 3 compares bootstrapping to theuse of manually annotated material for training a su-pervised method.
An extension to the evaluation ofNER appear in Section 4.
Based on this evaluation,section 5 discusses ways of improving the perfor-mance of a gene name recognizer bootstrapped onFlyBase resources.
Section 6 concludes the paperand suggests some future work.2 Building a test setIn this section we present a new test set created toevaluate named entity recognition for Drosophila flygenes.
To our knowledge, there is only one othertest set built for this purpose, presented in Morgan et138al.
(2004), which was annotated by two annotators.The inter-annotator agreement achieved was 87% F-score between the two annotators, which accordingto the authors reflects the difficulty of the task.Vlachos et al(2006) evaluated their system onboth versions of this test set and obtained signifi-cantly different results.
The disagreements betweenthe two versions were attributed to difficulties in ap-plying the guidelines used for the annotation.
There-fore, they produced a version of this dataset resolv-ing the differences between these two versions usingrevised guidelines, partially based on those devel-oped for ACE (2004).
In this work, we applied theseguidelines to construct a new test set, which resultedin their refinement and clarification.The basic idea is that gene names (<gn>) are an-notated in any position they are encountered in thetext, including cases where they are not referring tothe actual gene but they are used to refer to a differ-ent entity.
Names of gene families, reporter genesand genes not belonging to Drosophila are tagged asgene names too:?
the <gn>faf</gn> gene?
the <gn>Toll</gn> protein?
the <gn>string</gn>-<gn>LacZ</gn>reporter genesIn addition, following the ACE guidelines, foreach gene name we annotate the shortest surround-ing noun phrase.
These noun phrases are classifiedfurther into gene mentions (<gm>) and other men-tions (<om>), depending on whether the mentionsrefer to an actual gene or not respectively.
Most ofthe times, this distinction can be performed by look-ing at the head noun of the noun phrase:?
<gm>the <gn>faf</gn> gene</gm>?
<om>the <gn>Reaper</gn> protein</om>However, in many cases the noun phrase itselfis not sufficient to classify the mention, especiallywhen the mention consists of just the gene name, be-cause it is quite common in the biomedical literatureto use a gene name to refer to a protein or to othergene products.
In order to classify such cases, theannotators need to take into account the context inwhich the mention appears.
In the following exam-ples, the word of the context that enables us to makeMorgan et al new datasetabstracts 86 82tokens 16779 15703gene-names 1032 629unique 347 326gene-namesTable 1: Statistics of the datasetsthe distinction between gene mentions (<gm>) andother mentions is underlined:?
... ectopic expression of<gm><gn>hth</gn></gm> ...?
... transcription of<gm><gn>string</gn></gm> ...?
... <om><gn>Rols7</gn></om> localizes ...It is worth noticing as well that sometimes morethan one gene name may appear within the samenoun phrase.
As the examples that follow demon-strate, this enables us to annotate consistently casesof coordination, which is another source of disagree-ment (Dingare et al, 2004):?
<gm><gn>male-specific lethal-1</gn>,<gn>-2</gn> and <gn>-3</gn> genes</gm>The test set produced consists of the abstractsfrom 82 articles curated by FlyBase1.
We used thetokenizer of RASP2 (Briscoe and Carroll, 2002) toprocess the text, resulting in 15703 tokens.
The sizeand the characteristics of the dataset is comparablewith that of Morgan et al(2004) as it can be observedfrom the statistics of Table 1, except for the num-ber of non-unique gene-names.
Apart from the dif-ferent guidelines, another difference is that we usedthe original text of the abstracts, without any post-processing apart from the tokenization.
The datasetfrom Morgan et al (2004) had been stripped fromall punctuation characters, e.g.
periods and commas.Keeping the text intact renders this new dataset morerealistic and most importantly it allows the use oftools that rely on this information, such as syntacticparsers.The annotation of gene names was performedby a computational linguist and a FlyBase curator.1www.flybase.net2http://www.cogs.susx.ac.uk/lab/nlp/rasp/139We estimated the inter-annotator agreement in twoways.
First, we calculated the F-score achieved be-tween them, which was 91%.
Secondly, we used theKappa coefficient (Carletta, 1996), which has be-come the standard evaluation metric and the scoreobtained was 0.905.
This high agreement scorecan be attributed to the clarification of what genename should capture through the introduction ofgene mention and other mention.
It must be men-tioned that in the experiments that follow in the restof the paper, only the gene names were used to eval-uate the performance of bootstrapping.
The identifi-cation and the classification of mentions is the sub-ject of ongoing research.The annotation of mentions presented greater dif-ficulty, because computational linguists do not havesufficient knowledge of biology in order to use thecontext of the mentions whilst biologists are nottrained to identify noun phrases in text.
In this ef-fort, the boundaries of the mentions where definedby the computational linguist and the classificationwas performed by the curator.
A more detailed de-scription of the guidelines, as well as the corpus it-self in IOB format are available for download3.3 Bootstrapping NERFor the bootstrapping experiments presented in thispaper we employed the system developed by Vla-chos et al (2006), which was an improvement of thesystem of Morgan et al (2004).
In brief, the ab-stracts of all the articles curated by FlyBase wereretrieved and tokenized by RASP (Briscoe and Car-roll, 2002).
For each article, the gene names andtheir synonyms that were recorded by the curatorswere annotated automatically on its abstract usinglongest-extent pattern matching.
The pattern match-ing is flexible in order to accommodate capitaliza-tion and punctuation variations.
This process re-sulted in a large but noisy training set, consistingof 2,923,199 tokens and containing 117,279 genenames, 16,944 of which are unique.
The abstractsused in the test set presented in the previous sectionwere excluded.
We used them though to evaluate theperformance of the training data generation processand the results were 73.5% recall, 93% precision and82.1% F-score.3www.cl.cam.ac.uk/users/av308/Project Index/node5.htmlTraining Recall Precision F-scorestd 75% 88.2% 81.1%std-enhanced 76.2% 87.7% 81.5%BioCreative 35.9% 37.4% 36.7%Table 2: Results using Vlachos et al (2006) systemThis material was used to train the HMM-basedNER module of the open-source toolkit LingPipe4.The performance achieved on the corpus presentedin the previous section appears in Table 2 in the row?std?.
Following the improvements suggested byVlachos et al (2006), we also re-annotated as gene-names the tokens that were annotated as such by thedata generation process more than 80% of the time(row ?std-enhanced?
), which slightly increased theperformance.In order to assess the usefulness of this bootstrap-ping method, we evaluated the performance of theHMM-based tagger if we trained it on manually an-notated data.
For this purpose we used the anno-tated data from BioCreative-2004 (Blaschke et al,2004) task 1A.
In that task, the participants were re-quested to identify which terms in a biomedical re-search article are gene and/or protein names, whichis roughly the same task as the one we are deal-ing with in this paper.
Therefore we would expectthat, even though the material used for the anno-tation is not drawn from the exact domain of ourtest data (FlyBase curated abstracts), it would stillbe useful to train a system to identify gene names.The results in Table 2 show that this is not the case.Apart from the domain shift, the deterioration of theperformance could also be attributed to the differ-ent guidelines used.
However, given that the tasksare roughly the same, it is a very important resultthat manually annotated training material leads toso poor performance, compared to the performanceachieved using automatically created training data.This evidence suggests that manually created re-sources, which are expensive, might not be usefuleven in slightly different tasks than those they wereinitially designed for.
Moreover, it suggests thatthe use of semi-supervised or unsupervised methodsfor creating training material are alternatives worth-exploring.4http://www.alias-i.com/lingpipe/1404 Evaluating NERThe standard evaluation metric used for NER is theF-score (Van Rijsbergen, 1979), which is the har-monic average of Recall and Precision.
It is verysuccessful and popular, because it penalizes systemsthat underperform in any of these two aspects.
Also,it takes into consideration the existence multi-tokenentities by rewarding systems able to identify theentity boundaries correctly and penalizing them forpartial matches.
In this section we suggest an exten-sion to this evaluation, which we believe is mean-ingful and informative for trainable NER systems.Two are the main expectations from trainable sys-tems.
The first one is that they will be able to iden-tify entities that they have encountered during theirtraining.
This is not as easy as it might seem, be-cause in many domains token(s) representing en-tity names of a certain type can appear as commonwords or representing an entity name of a differenttype.
Using examples from the biomedical domain,?to?
can be a gene name but it is also used as a prepo-sition.
Also gene names are commonly used as pro-tein names, rendering the task of distinguishing be-tween the two types non-trivial, even if examples ofthose names exist in the training data.
The secondexpectation is that trainable systems should be ableto learn from the training data patterns that will al-low it to generalize to unseen named entities.
Im-portant role in this aspect of the performance playthe features that are dependent on the context andon observations on the tokens.
The ability to gener-alize to unseen named entities is very significant be-cause it is unlikely that training material can coverall possible names and moreover, in most domains,new names appear regularly.A common way to assess these two aspects is tomeasure the performance on seen and unseen dataseparately.
It is straightforward to apply this in taskswith token-based evaluation, such as part-of-speechtagging (Curran and Clark, 2003).
However, in thecase of NER, this is not entirely appropriate dueto the existence of multi-token entities.
For exam-ple, consider the case of the gene-name ?head inhi-bition defective?, which consists of three commonwords that are very likely to occur independently ofeach other in a training set.
If this gene name ap-pears in the test set but not in the training set, witha token-based evaluation its identification (or not)would count towards the performance on seen to-kens if the tokens appeared independently.
More-over, a system would be rewarded or penalized foreach of the tokens.
One approach to circumventthese problems and evaluate the performance of asystem on unseen named entities, is to replace allthe named entities of the test set with strings thatdo not appear in the training data, as in Morgan etal.
(2004).
There are two problems with this eval-uation.
Firstly, it alters the morphology of the un-seen named entities, which is usually a source ofgood features to recognize them.
Secondly, it affectsthe contexts in which the unseen named entities oc-cur, which don?t have to be the same as that of seennamed entities.In order to overcome these problems, we used thefollowing method.
We partitioned the correct an-swers and the recall errors according to whether thenamed entity at question have been encountered inthe training data as a named entity at least once.
Theprecision errors are partitioned in seen and unseendepending on whether the string that was incorrectlyannotated as a named entity by the system has beenencountered in the training data as a named entityat least once.
Following the standard F-score defi-nition, partially recognized named entities count asboth precision and recall errors.In examples from the biomedical domain, if ?to?has been encountered at least once as a gene name inthe data but an occurrence of in the test dataset is er-roneously tagged as a gene name, this will count as aprecision error on seen named entities.
Similarly, if?to?
has never been encountered in the training dataas a gene name but an occurrence of it in the testdataset is erroneously tagged as a common word,this will count as a recall error on unseen named en-tities.
In a multi-token example, if ?head inhibitiondefective?
is a gene name in the test dataset and ithas been seen as such in the training data but theNER system tagged (erroneously) ?head inhibition?as a gene name (which is not the training data), thenthis would result in a recall error on seen named en-tities and a precision error on unseen named entities.5 Improving performanceUsing this extended evaluation we re-evaluated thenamed entity recognition system of Vlachos et141Recall Precision F-score # entitiesseen 95.9% 93.3% 94.5% 495unseen 32.3% 63% 42.7% 134overall 76.2% 87.7% 81.5% 629Table 3: Extended evaluational.
(2006) and Table 3 presents the results.
The biggap in the performance on seen and unseen namedentities can be attributed to the highly lexicalizednature of the algorithm used.
Tokens that have notbeen seen in the training data are passed on to a mod-ule that classifies them according to their morphol-ogy, which given the variety of gene names and theiroverlap with common words is unlikely to be suffi-cient.
Also, the limited window used by the tagger(previous label and two previous tokens) does notallow the capture of long-range contexts that couldimprove the recognition of unseen gene names.We believe that this evaluation allows fair com-parison between the data generation process thatcreating the training data and the HMM-based tag-ger.
This comparison should take into account theperformance of the latter only on seen named enti-ties, since the former is applied only on those ab-stracts for which lists of the genes mentioned havebeen compiled manually by the curators.
The re-sult of this comparison is in favor of the HMM,which achieves 94.5% F-score compared to 82.1%of the data generation process, mainly due to the im-proved recall (95.9% versus 73.5%).
This is a veryencouraging result for bootstrapping techniques us-ing noisy training material, because it demonstratesthat the trained classifier can deal efficiently with thenoise inserted.From the analysis performed in this section, itbecomes obvious that the system is rather weak inidentifying unseen gene names.
The latter contribute31% of all the gene names in our test dataset, withrespect to the training data produced automaticallyto train the HMM.
Each of the following subsec-tions describes different ideas employed to improvethe performance of our system.
As our baseline,we kept the version that uses the training data pro-duced by re-annotating as gene names tokens thatappear as part of gene names more than 80% oftimes.
This version has resulted in the best perfor-mance obtained so far.Training Recall Precision F-score coverbsl 76.2% 87.7% 81.5% 69%sub 73.6% 83.6% 78.3% 69.6%bsl+sub 82.2% 83.4% 82.8% 79%Table 4: Results using substitution5.1 SubstitutionA first approach to improve the overall performanceis to increase the coverage of gene names in thetraining data.
We noticed that the training setproduced by the process described earlier contains16944 unique gene names, while the dictionary ofall gene names from FlyBase contains 97227 entries.This observation suggests that the dictionary is notfully exploited.
This is expected, since the dictio-nary entries are obtained from the full papers whilethe training data generation process is applied onlyto their abstracts which are unlikely to contain all ofthem.In order to include all the dictionary entries inthe training material, we substituted in the trainingdataset produced earlier each of the existing genenames with entries from the dictionary.
The pro-cess was repeated until each of the dictionary entrieswas included once in the training data.
The assump-tion that we take advantage of is that gene namesshould appear in similar lexical contexts, even if theresulting text is nonsensical from a biomedical per-spective.
For example, in a sentence containing thephrase ?the sws mutant?, the immediate lexical con-text could justify the presence of any gene name inthe place ?sws?, even though the whole sentencewould become untruthful and even incomprehensi-ble.
Although through this process we are boundto repeat errors of the training data, we expect thegains from the increased coverage to alleviate theireffect.
The resulting corpus consisted of 4,062,439tokens containing each of the 97227 gene names ofthe dictionary once.
Training the HMM-based tag-ger with this data yielded 78.3% F-score (Table 4,row ?sub?).
438 out of the 629 genes of the test setwere seen in the training data.The drop in precision exemplifies the importanceof using naturally occurring training material.
Also,59 gene names that were annotated in the trainingdata due to the flexible pattern matching are not in-142Training Recall Precision F unseenscore F scorebsl 76.2% 87.7% 81.5% 42.7%bsl-excl 80.8% 81.1% 81% 51.3%Table 5: Results excluding sentences without enti-tiescluded anymore since they are not in the dictionary,which explains the drop in recall.
Given these ob-servations, we trained HMM-based tagger on bothversions of the training data, which consisted of5,527,024 tokens, 218,711 gene names, 106,235 ofwhich are unique.
The resulting classifier had seenin its training data 79% of the gene names in thetest set (497 out of 629) and it achieved 82.8% F-score (row ?bsl+sub?
in Table 4).
It is worth point-ing out that this improvement is not due to amelio-rating the performance on unseen named entities butdue to including more of them in the training data,therefore taking advantage of the high performanceon seen named entities (93.7%).
Direct comparisonsbetween these three versions of the system on seenand unseen gene names are not meaningful becausethe separation in seen and seen gene names changeswith the the genes covered in the training set andtherefore we would be evaluating on different data.5.2 Excluding sentences not containing entitiesFrom the evaluation of the dictionary based tagger inSection 3 we confirmed our initial expectation thatit achieves high precision and relatively low recall.Therefore, we anticipate most mistakes in the train-ing data to be unrecognized gene names (false neg-atives).
In an attempt to reduce them, we removedfrom the training data sentences that did not containany annotated gene names.
This process resultedin keeping 63,872 from the original 111,810 sen-tences.
Apparently, such processing would removemany correctly identified common words (true neg-atives), but given that the latter are more frequent inour data we expect it not to have significant impact.The results appear in Table 5.In this experiment, we can compare the perfor-mances on unseen data because the gene names thatwere included in the training data did not change.As we expected, the F-score on unseen gene namesrose substantially, mainly due to the improvement inrecall (from 32.3% to 46.2%).
The overall F-scoredeteriorated, which is due to the drop in precision.An error analysis showed that most of the precisionerrors introduced were on tokens that can be partof gene names as well as common words, whichsuggests that removing from the training data sen-tences without annotated entities, deprives the clas-sifier from contexts that would help the resolutionof such cases.
Still though, such an approach couldbe of interest in cases where we expect a significantamount of novel gene names.5.3 Filtering contextsThe results of the previous two subsections sug-gested that improvements can be achieved throughsubstitution and exclusion of sentences without en-tities, attempting to include more gene names in thetraining data and exclude false negatives from them.However, the benefits from them were hampered be-cause of the crude way these methods were applied,resulting in repetition of mistakes as well as exclu-sion of true negatives.
Therefore, we tried to fil-ter the contexts used for substitution and the sen-tences that were excluded using the confidence ofthe HMM based tagger.In order to accomplish this, we used the ?std-enhanced?
version of the HMM based tagger to re-annotate the training data that had been generatedautomatically.
From this process, we obtained a sec-ond version of the training data which we expectedto be different from the original one by the data gen-eration process, since the HMM based tagger shouldbehave differently.
Indeed, the agreement betweenthe training data and its re-annotation by the HMMbased tagger was 96% F-score.
We estimated theentropy of the tagger for each token and for eachsentence we calculated the average entropy over allits tokens.
We expected that sentences less likelyto contain errors would be sentences on which thetwo versions of the training data would agree andin addition the HMM based tagger would annotatewith low entropy, an intuition similar to that of co-training (Blum and Mitchell, 1998).
Following this,we removed from the dataset the sentences on whichthe HMM-based tagger disagree with the annota-tion of the data generation process, or it agreed withbut the average entropy of their tokens was abovea certain threshold.
By setting this threshold at143Training Recall Precision F-score coverfilter 75.6% 85.8% 80.4% 65.5%filter-sub 80.1% 81% 80.6% 69.6%filter-sub 83.3% 82.8% 83% 79%+bslTable 6: Results using filtering0.01, we kept 72,534 from the original 111,810 sen-tences, which contained 61798 gene names, 11,574of which are unique.
Using this dataset as trainingdata we achieved 80.4% F-score (row ?filter?
in Ta-ble 6).
Even though this score is lower than ourbaseline (81.5% F-score), this filtered dataset shouldbe more appropriate to apply substitution because itwould contain fewer errors.Indeed, applying substitution to this dataset re-sulted in better results, compared to applying it tothe original data.
The performance of the HMM-based tagger trained on it was 80.6% F-score (row?filter-sub?
in Table 6) compared to 78.3% (row?sub?
in Table 4).
Since both training datasetscontain the same gene names (the ones containedin the FlyBase dictionary), we can also comparethe performance on unseen data, which improvedfrom 46.7% to 48.6%.
This improvement can beattributed to the exclusion of some false negativesfrom the training data, which improved the recall onunseen data from 42.9% to 47.1%.
Finally, we com-bined the dataset produced with filtering and substi-tution with the original dataset.
Training the HMM-based tagger on this dataset resulted in 83% F-score,which is the best performance we obtained.6 Conclusions - Future workIn this paper we demonstrated empirically the effi-ciency of using automatically created training mate-rial for the task of Drosophila gene name recogni-tion by comparing it with the use of manually an-notated material from the broader biomedical do-main.
For this purpose, a test dataset was createdusing novel guidelines that allow more consistentmanual annotation.
We also presented an informa-tive evaluation of the bootstrapped NER system thatrevealed that indicated its weakness in identifyingunseen gene names.
Based on this result we ex-plored ways to improve its performance.
These in-cluded taking fuller advantage of the dictionary ofgene names from FlyBase, as well as filtering outlikely mistakes from the training data using confi-dence estimations from the HMM-based tagger.Our results point out some interesting directionsfor research.
First of all, the efficiency of bootstrap-ping calls for its application in other tasks for whichuseful domain resources exist.
As a complementtask to NER, the identification and classification ofthe mentions surrounding the gene names shouldbe tackled, because it is of interest to the users ofbiomedical IE systems to know not only the genenames but also whether the text refers to the actualgene or not.
This could also be useful to anaphoraresolution systems.
Future work for bootstrappingNER in the biomedical domain should include ef-forts to incorporate more sophisticated features thatwould be able to capture more abstract contexts.
Inorder to evaluate such approaches though, we be-lieve it is important to test them on full papers whichpresent greater variety of contexts in which genenames appear.AcknowledgmentsThe authors would like to thank Nikiforos Karama-nis and the FlyBase curators Ruth Seal and Chi-hiro Yamada for annotating the dataset and their ad-vice in the guidelines.
We would like also to thankMITRE organization for making their data availableto us and in particular Alex Yeh for the BioCre-ative data and Alex Morgan for providing us withthe dataset used in Morgan et al (2004).
The authorswere funded by BBSRC grant 38688 and CAPESaward from the Brazilian Government.ReferencesACE.
2004.
Annotation guidelines for entity detectionand tracking (EDT).Christian Blaschke, Lynette Hirschman, and AlexanderYeh, editors.
2004.
Proceedings of the BioCreativeWorkshop, Granada, March.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of COLT 1998.E.
J. Briscoe and J. Carroll.
2002.
Robust accurate statis-tical annotation of general text.
In Proceedings of the1443rd International Conference on Language Resourcesand Evaluation, pages 1499?1504.Jean Carletta.
1996.
Assessing agreement on classifi-cation tasks: The kappa statistic.
Computational Lin-guistics, 22(2):249?254.M.
Collins and Y.
Singer.
1999.
Unsupervised modelsfor named entity classification.
In Proceedings of theJoint SIGDAT Conference on EMNLP and VLC.J.
Curran and S. Clark.
2003.
Investigating gis andsmoothing for maximum entropy taggers.
In Pro-ceedings of the 11th Annual Meeting of the EuropeanChapter of the Association for Computational Linguis-tics.S.
Dingare, J. Finkel, M. Nissim, C. Manning, andC.
Grover.
2004.
A system for identifying named en-tities in biomedical text: How results from two evalua-tions reflect on both the system and the evaluations.
InThe 2004 BioLink meeting at ISMB.R.
Gaizauskas, G. Demetriou, P. J. Artymiuk, and P. Wil-let.
2003.
Protein structures and information ex-traction from biological texts: The ?PASTA?
system.BioInformatics, 19(1):135?143.L.
Hirschman, J. C. Park, J. Tsujii, L. Wong, and C. H.Wu.
2002.
Accomplishments and challenges inliterature data mining for biology.
Bioinformatics,18(12):1553?1561.J.
Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier,editors.
2004.
Proceedings of JNLPBA, Geneva.H.
Liu and C. Friedman.
2003.
Mining terminologi-cal knowledge in large biomedical corpora.
In PacificSymposium on Biocomputing, pages 415?426.A.
A. Morgan, L. Hirschman, M. Colosimo, A. S. Yeh,and J.
B. Colombe.
2004.
Gene name identificationand normalization using a model organism database.J.
of Biomedical Informatics, 37(6):396?410.D.
Shen, J. Zhang, J. Su, G. Zhou, and C. L. Tan.
2004.Multi-criteria-based active learning for named entityrecongition.
In Proceedings of ACL 2004, Barcelona.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the conll-2003 shared task: Language-independent named entity recognition.
In WalterDaelemans and Miles Osborne, editors, Proceedingsof CoNLL-2003, pages 142?147.
Edmonton, Canada.C.
J.
Van Rijsbergen.
1979.
Information Retrieval, 2ndedition.
Dept.
of Computer Science, University ofGlasgow.A.
Vlachos, C. Gasperin, I. Lewin, and T. Briscoe.
2006.Bootstrapping the recognition and anaphoric linking ofnamed entities in drosophila articles.
In Proceedingsof PSB 2006.145
