Proceedings of the Workshop on Linguistic Distances, pages 51?62,Sydney, July 2006. c?2006 Association for Computational LinguisticsEvaluation of String Distance Algorithms for DialectologyWilbert Heeringa, Peter Kleiweg, Charlotte Gooskens & John NerbonneHumanities Computing, University of Groningen{W.J.Heeringa, P.C.J.Kleiweg, C.S.Gooskens, J.Nerbonne}@rug.nlAbstractWe examine various string distance mea-sures for suitability in modeling dialectdistance, especially its perception.
We findmeasures superior which do not normalizefor word length, but which are are sensi-tive to order.
We likewise find evidence forthe superiority of measures which incor-porate a sensitivity to phonological con-text, realized in the form of n-grams?although we cannot identify which formof context (bigram, trigram, etc.)
is best.However, we find no clear benefit in us-ing gradual as opposed to binary segmen-tal difference when calculating sequencedistances.1 IntroductionWe compare string distance measures for theirvalue in modeling dialect distances.
Traditionaldialectology relies on identifying language fea-tures which are common to one dialect area whiledistinguishing it from others.
It has difficultyin dealing with partial matches of linguistic fea-tures and with non-overlapping language patterns.Therefore Seguy (1973) and Goebl (1982; 1984)advocate using aggregates of linguistic features toanalyze dialectal patterns, effectively introducingthe perspective of DIALECTOMETRY.Kessler (1995) introduced the use of string editdistance measure as a means of calculating the dis-tance between the pronunciations of correspond-ing words in different dialects.
Following Seguy?sand Goebl?s lead, he calculated this distance forpairs of pronunciations of many words in manyIrish-speaking towns.
String edit distance is sen-sitive to the degrees of overlap of strings and al-lows one to process large amounts of pronunci-ation data, including that which does not followother isoglosses neatly.
Heeringa (2004) exam-ines several variants of edit distance applied toNorwegian and Dutch data, focusing on measureswhich involve a length normalization, and whichignore phonological context, and demonstratingthat measures using binary segment differencesare no worse than those using feature-based mea-sures of segment difference.This paper inspects a range of further refine-ments in measuring pronunciation differences.First, we inspect the role of normalization bylength, showing that it actually worsens non-normalized measures.
Second, we compare editdistance measures to simpler measures which ig-nore linear order, and show that order-sensitivityis important.
Third, we inspect measures whichare sensitive to phonetic context, and show thatthese, too, tend to be superior.
Fourth, we com-pare versions of string edit distance which areconstrained to respect syllable structure (alwaysmatching vowels with vowels, etc.
), and concludethat this is mildly advantageous.
Finally we com-pare binary (i.e., same/different) treatments of thesegments in edit distance to gradual treatments ofsegment differentiation, and find no indication ofthe superiority of the gradual measures.The quality of the measures is assayed primarilythrough their agreement with the judgments of di-alect speakers about which varieties are perceivedas more similar (or dissimilar) to their own.
Inaddition we inspect a validation technique whichpurports to show how successfully a dialect mea-sure uncovers the geographic structure in the data(Nerbonne and Kleiweg, 2006), but this techniqueyields unstable results when applied to our data.We have perception data only for Norwegian, so51that data figures prominently in our argument, andwe evaluate both Norwegian and German data ge-ographically.The results differ, and the perceptual results(concerning Norwegian) are most easily inter-pretable.
There we find, as noted above, thatnon-normalized measures are superior to normal-ized ones, that both order and context sensitiv-ity are worthwhile, as is the vowel/consonant dis-tinction.
The (geographic) results for German aremore complicated, but also less stable.
We includethem for the sake of completeness.In addition we note two minor contributions.First, although some literature ends up evaluat-ing both distance and similarity measures, becausethese are not consistently each others?
inverses un-der some normalizations (Kondrak, 2005; Inkpenet al, 2005), we suggest a normalization based onalignment length which guarantees that similarityis exactly the inverse of distance, allowing us toconcentrate on distance.Second, we note that there is no great problemin applying edit distance to bigrams and trigrams,even though recent literature has been scepticalabout the feasibility of this step.
For exampleKessler (2005) writes:[...] one major shortcoming [in applyingedit distance to linguistic data, WH et althat is rarely discussed is that the pho-netic environment of the sounds in ques-tion cannot be taken into account, whilestill making use of the efficient dynamicprogramming algorithm (p. 253).Somewhat further Kessler writes: ?Currently, thepredominant solution to this problem is to ignorecontext entirely.?
In fact Kondrak (2005) appliesedit distance straightforwardly using n-gram asbasic elements.
Our findings accord with Kon-drak?s, who also found no problem in applying editdistance using n-grams, but we evaluate the tech-nique in its application to dialectology.1.1 BackgroundHeeringa (2004) demonstrates that edit distanceapplied to comparable words (see below for ex-amples) is a superior measure of dialect distancewhen compared to unigram corpus frequency andalso that it is superior to both the frequency of pho-netic features in corpora (a technique which Hop-penbrouwers & Hoppenbrouwers (2001) had ad-vocated) and to the frequency of phonetic featurestaken one word at a time.
Heeringa compares thesetechniques using the results of a perception ex-periment we also employ below.
Heeringa showsthat word-based techniques are superior to corpus-based techniques, and moreover, that most word-based techniques perform about the same.
Wetherefore ignore measures which view corpora asundifferentiated collections below and study onlyword-based techniques.A further question was whether to comparewords based on a binary difference between seg-ments or whether to use instead phonetic fea-tures to derive a more sensitive measure of seg-ment distance.
It turned out that measures us-ing binary segment distinctions outperform thefeature-based methods (see Heeringa, pp.
184?186), even though a number of feature systems andcomparisons of feature vectors were experimentedwith.
We likewise accept these results (at least forpresent purposes) and focus exclusively on mea-sures using the binary segment distinctions below.Kondrak (2005) and Inkpen et al (2005) presentseveral methods for measuring string similarityand distance which complement Heeringa?s resultsnicely.
We should note, however, that these pa-pers focus on other areas of application, viz., theproblems of identifying (i) technical names whichmight be confused, (ii) linguistic cognates (wordsfrom the same root), and (iii) translational cog-nates (words which may be used as translationalequivalences).
Inkpen et al consider 12 differentorthographic similarity measures, including somein which the order of segments does not play a role(e.g., DICE), and others which use order in align-ment (e.g.
edit distance).
They further considercomparison on the basis of unigrams, bigrams, tri-grams and ?xbigrams,?
which are trigrams withoutthe middle element.
Some methods are similaritymeasures, others are distance measures.
We returnto this in Section 2.1.2 This paperIn this paper we apply string distance measuresto Norwegian and German dialect data.
Asnoted above, we focus on word-based methodsin which segments are compared at a binary(same/different) level.
The methods we considerwill be explained in Section 2.
Section 3 de-scribes the Norwegian and German data to whichthe methods are applied.
In Section 4 we describehow we evaluate the methods, namely by com-52paring the algorithmic results to the distances asperceived by the dialect speakers themselves.
Welikewise aimed to evaluate by calculating the de-gree to which a measure uncovers geographic co-hesion in dialect data, but as we shall see, thismeans of validation yields rather unstable results.In Section 5 we present results for the differentmethods and finally, in Section 6, we draw someconclusions.2 String Comparison AlgorithmsIn this section we describe a number of stringcomparison algorithms largely following Inkpenet al (2005).
The methods can be classified ac-cording to different factors: representation (un-igram, bigram, trigram, xbigram), comparisonof n-grams (binary or gradual), status of order(with or without alignment), and type of align-ment (free or forced alignment with respect tothe vowel/consonant distinction).
We illustratethe methods with examples, in which we compareGerman and Dutch dialect pronunciations of theword milk.12.1 Contextual sensitivityIn the German dialect of Reelkirchen milk is pro-nounced as [mElk@].
The bigram notation is [?mmE El lk k@ @?]
and the trigram notation is [?
?m?mE mEl Elk lk@ k@?
@??].
The same word is pro-nounced as [mEl@c?]
in the German dialect of Tann.The bigram and trigram representations are [?mmE El l@ @c?
c??]
and [?
?m ?mE mEl El@ l@c?
@c??
c???
]respectively.In the simplest method we present in this paper,the distance is found by calculating 1 minus twicethe number of shared segment n-grams divided bythe total number of n-grams in both words.
Inkpenet al mention a bigram-based, a trigram-basedand a xbigram-based procedure, which they callDICE, TRIGRAM and XDICE respectively.
Wealso consider an unigram-based procedure whichwe call UNIGRAM.
The two pronunciations sharefour unigrams: [m, E, l] and [@].
There are 5 + 5 =10 unigram tokens in total in the two words, so theunigram similarity is (2 ?
4)/10 = 0.8, and thedistance 1 ?
0.8 = 0.2.
The two pronunciationsshare three bigrams: [?m, mE] and [El].
There are6 + 6 = 12 bigram tokens in the two strings, sobigram similarity is (2?3)/12 = 0.5, and the dis-tance 1 ?
0.5 = 0.5.
Finally, the two pronuncia-1Our transcriptions omit diacritics for simplicity?s sake.tions have three trigrams in common: [?
?m, ?mE]and [mEl] among 7+7 = 14 in total, yielding a tri-gram similarity of (2 ?
3)/14 = 0.4 and distance1?
0.4 = 0.6.Our interest in this issue is linguistic: longern-grams allow comparison on the basis of phoniccontext, and unigram comparisons have correctlybeen criticized for ignoring this (Kessler, 2005).2.2 Order of segmentsWhen comparing the German dialect pronuncia-tion of Reelkirchen [mElk@] with the Dutch dialectpronunciation of Haarlem [mEl@k], the unigramprocedure presented above will detect no differ-ence.
One might argue that we are dealing witha swap, but this is effectively an appeal to order.The example is not convincing for n-gram mea-sures, n ?
2, but we should prefer to separateissues of order from issues of context sensitivity.We use edit distance (aka Levenshtein distance)for this purpose, and we assume familiarity withthis (Kruskal, 1999).
In our use of edit distance alloperations have a cost of 1.2.3 Normalization by lengthWhen the edit distance is divided by the lengthof the longer string, Inkpen et al call it normal-ized edit distance (NED).
In our approach we di-vide ?raw edit distance?
by alignment length.
Thesame minimum distance found by the edit distancealgorithm may be obtained on the basis of sev-eral alignments which may have different lengths.We found that the longest alignment has the great-est number of matches.
Therefore we normalizeby dividing the edit distance by the length of thelongest alignment.We have normally employed a length normal-ization in earlier work (Heeringa, 2004), reason-ing that words are such fundamental linguisticunits that dialect perception was likely to be word-based.
We shall test this premise in this paper.Marzal & Vidal (1993) show that the normal-ized edit distance between two strings cannot beobtained via ?post-normalization?, i.e., by firstcomputing the (unnormalized) edit distance andthen normalizing this by the length of the cor-responding editing path.
Unnormalized edit dis-tance satisfies the triangle inequality, which is ax-iomatic for distances, but the quantities obtainedvia post-normalization need not satisfy this ax-iom.
Marzdal & Vidal provide an alternative pro-cedure which is guaranteed to produce genuine53distances, satisfying all of the relevant axioms.
Intheir modified algorithm, one computes one min-imum weight for each of the possible lengths ofediting paths at each point in the computationallattice.
Once all these weights are calculated, theyare divided by their corresponding path lengths,and the minimum quotient represents the normal-ized edit distance.The basic idea behind edit distance is to find theminimum cost of changing one string into another.Length normalization represents a deviation fromthis basic idea.
If a higher cost corresponds with alonger path length so that quotient of the edit costsdivided by the path length is minimal, then Marzal& Vidal?s procedure opts for the minimal normal-ized length, while post-normalization seeks whatone might call ?the normalized minimal length?
(see Marzal & Vidal?s example 3.1 and Figure 2,p.
928).Marzal & Vidal?s examples of normalized mini-mal distances which are not also minimal normal-ized distances all involve operation costs we nor-mally do not employ.
In particular they allow IN-DELS (insertions and deletions) to be associatedwith much lower costs than substitutions, so thatthe longer paths associated with derivations in-volving indels is more than compensated by thelength normalization.
Our costs are never struc-tured in this way, so we conjecture that our post-normalizations do not genuinely run the risk of vi-olating the distance axioms.
We use 0 for the costof mapping a symbol to itself, 1 to map it to a dif-ferent symbol, including the empty symbol (cov-ering the costs of indels), and?
for non-allowedmappings2 We maintain therefore that (unnormal-ized) costs higher than the minimum will nevercorrespond to longer alignment lengths.
If this isso, then the minimal edit cost divided by align-ment length will also be the minimal normalizedcost.
If the unnormalized edit distance is mini-mal, we claim that the post-normalized edit dis-tance must therefore be minimal as well.We inspect an example to illustrate these issues.We compare the Frisian (Grouw), [mOlk@], withthe Haarlem pronunciation [mEl@k].
The Leven-shtein algorithm may align the pronunciations asfollows:2For example, in some versions of edit distance, the value?
is assigned to the replacement of a vowel by a consonantin order to avoid alignments which violate syllabic structure.1 2 3 4 5 6m O l k @m E l @ k1 1 1The one pronunciation is transformed into theother by substituting [E] for [O], by deleting [@]after [l], and by inserting [@] after [k].
Sinceeach operation has a cost of 1, and the align-ment is 6 elements long, the normalized distanceis (1 + 1 + 1)/6 = 0.5.
The Levenshtein dis-tance will also find an alignment in which the[@]?s are matched, while the [k]?s are inserted anddeleted.
That alignment gives the same (normal-ized) distance.
Levenshtein distance will not findan alignment any longer than the one shown here,since longer alignments will not yield the mini-mum cost.
This also holds for the examples shownbelow.2.4 n-gram weightsIn the dialect of the German dialect of Frohn-hausen milk is pronounced as [mIlj@], and in theGerman of Gro?wechsungen as [mElIc?].
If wecompare these using the techniques of Section 2.2,using bigrams, we obtain the following:1 2 3 4 5 6-m mI Il lj j@ @--m mE El lI Ik k-1 1 1 1 1Since n-grams are compared in a binary way, thenormalized distance is equal to (1 + 1 + 1 + 1 +1)/6 = 0.83.
But [mI] and [mE] (second posi-tion) are clearly more similar to each other than[j@] and [Ik] (fifth position).
Inkpen et al suggestweighting n-gram differences using segment over-lap.
They provide a formula for measuring grad-ual similarity of n-grams to be used in BI-DISTand TRI-DIST.
Since we measure distances ratherthan similarity, we calculate n-gram distance asfollows:s(x1...xn, y1...yn) = 1n?ni=1 d(xi, yi)where d(a, b) returns 1 if a and b are different, and0 otherwise.
We apply this to our example:1 2 3 4 5 6-m mI Il lj j@ @--m mE El lI Ik k-0.5 0.5 0.5 1 0.5obtaining (0.5+0.5+0.5+1+0.5)/6 = 3.0/6 =0.5 distance after normalization.542.5 Linguistic AlignmentWhen comparing the Frisian (Grouw) dialectpronunciation, [mOlk@], with that of GermanGro?wechsungen, [mElIc?
], using unigrams, we ob-tain:1 2 3 4 5m O l k @m E l I c?1 1 1The normalized distance is then (1 + 1 + 1)/5 =0.6.
But this is linguistically an implausible align-ment: syllables do not align when e.g.
[k] alignswith [I], etc.
We may remedy this by requir-ing the Levenshtein algorithm to respect the dis-tinction between vowels and consonants, requir-ing that the alignments respect this distinction withonly three exceptions, in particular that semivow-els [j, w] may match vowels (or consonants), thatthe maximally high vowels [i, u] match conso-nants (or vowels), and that [@] match sonorant con-sonants (nasals and liquids) in addition to vow-els.
Disallowed matches are weighted so heav-ily (via the cost of the substitution operation) thatthe algorithm always will use alternative align-ments, effectively preferring insertions and dele-tions (indels) instead.
Applying these restrictions,we obtain the following, with normalized distance(1 + 1 + 1 + 1)/6 = 0.67:1 2 3 4 5 6m O l k @m E l I c?1 1 1 1In comparisons based on bigrams, we allowtwo bigrams to match when at least one seg-ment pair matches, the first, the second, or both.Two trigrams match when at least the middle pairmatches.
Comparing the same pronunciations asabove using bigrams without linguistic conditions,we obtain the following alignment:1 2 3 4 5 6-m mO Ol lk k@ @--m mE El lI Ic?
c?-1 1 1 1 10.5 0.5 0.5 1 0.5The normalized distance is (1 + 1 + 1 + 1 +1)/6 = 0.83 using binary bigram weights (costs),and (0.5 + 0.5 + 0.5 + 1 + 0.5)/6 = 0.5 usinggradual weights.
But the above alignment does notrespect the vowel/consonant distinction at the fifthposition, where neither [k] vs. [I] nor [@] vs.
[c?]
isallowed.
We correct this at once:1 2 3 4 5 6 7-m mO Ol lk k@ @--m mE El lI Ic?
c?-1 1 1 1 1 10.33 0.33 0.67 1 1 1Using binary bigram weights, the normalized dis-tance is (1 + 1 + 1 + 1 + 1 + 1)/7 = 0.86.The calculation based on gradual weights is abit more complex.
Two bigrams may match evenwhen a non-allowed pair occurs in one of the twopositions, e.g., [k] vs. [I] at the fourth position inthe alignment immediately above.
The cost of thismatch should be higher (via weights) than that ofan allowed pair with different elements?e.g., thepair [O] versus [E] at the second or third position?but not so high that the match cannot occur.We settle on the following scheme.
Two n-grams [x1...xn] and [y1...yn] can only match if atleast one pair (xi, yi) matches linguistically.
Weweight linguistically mismatching pairs (xj , yj)twice as high as matching (but non-identical)pairs.
Since we have at most n ?
1 matchingpairs, and at least 1 mismatching pair, we set themost expensive match of two n-grams to 1, and weassign the weight of 2/(2n ?
1) to a mismatch-ing pair, and 1/(n ?
1) to a matching (but non-identical) one.
Indels cost the same as the mostcostly (matching) n-grams, in this case 1.In our bigram-based example, we obtain aweight of 2/(2 ?
2 ?
1) = 0.67 at position4, since the pair [k] vs. [I] is a linguistic mis-match.
At positions 2 and 3 we obtain weightsof 1/(2?2?1) = 0.33 since [O] and [E] are (non-identical) matches.
Note that a segment (vowel orconsonant) versus ?-?
(boundary) is processed asa mismatch.
Therefore the weight at position 6 isequal to 0.33 ([k] vs.
[c?])
+0.67 ([@] versus [-]),summing to 1.2.6 Similarity vs. distanceTheoretically, similarity and distance should beeach others?
inverses.
Thus in Section 2.1 wesuggested that similarity should always be (1 ?distance).
This is not always straightforward whenwe normalize.Inkpen et al use both similarity and dis-tance measures.
Similarity measures are LCSR(Longest Common Subsequence Ratio), BI-SIMand TRI-SIM (LCSR generalized to bigrams andtrigrams), and the corresponding distance mea-sures are NED, BI-DIST and TRI-DIST.
The mea-sures are further distinguished in the way n-gram55weights are compared: as binary weights in thesimilarity measures, and as gradual weights in thedistance measures.
When comparing the pronun-ciations of Frisian Hindelopen [mO@lk@] with Ger-man Gro?wechsungen, [mElIc?
], and respecting thelinguistic alignment conditions (Section 2.5) weobtain:m O @ l k @m E l I c?0 1 1 0 1 1 1The non-normalized similarity is equal to 2, andthe non-normalized distance is equal to 5.
Inkpenet al normalize ?by dividing the total edit cost bythe length of the longer string?
which is 6 in ourexample.
Other possibilities are dividing by thelength of the shorter string (5), the average lengthof the two strings (5.5) or the length of the align-ment (7).
Summarizing:shorter longer average align-string string string mentsim.
0.4 0.33 0.36 0.29dist.
1.0 0.83 0.91 0.71total 1.4 1.17 1.27 1.00Only the normalization via alignment length re-spects the wish that we regard similarity and dis-tance as each others?
inverses.
3 We can enforcethis requirement in other approaches by first nor-malizing and then taking the inverse, but we takethe result above to indicate that normalization viaalignment length is the most natural procedure.3 Data SourcesThe methods presented in Section 2 are appliedto Norwegian and German dialect data describedin this section.
We emphasize that we measureddistances only at the level of the segmental base,ignoring stress and tone marks, suprasegmentalsand diacritics.
We in fact examined measurementswhich included the effects of segmental diacritics,which, however resulted in decreased consistencyand no apparent increase in quality.3.1 NorwegianThe Norwegian data comes from a database com-prising more than 50 dialect sites, compiled byJ?rn Almberg and Kristian Skarb?
of the Depart-ment of Linguistics of the University of Trond-3We have no proof that normalization by alignment lengthalways allows this simple relation to similarity, but we haveexamined a large number of calculations in which this alwaysseems to hold.heim.4 The database includes recordings and tran-scriptions of the fable ?The North Wind and theSun?
in various Norwegian dialects.
The Norwe-gian text consists of 58 different words, some ofwhich occur more than once, in which case weseek a least expensive pairing of the different el-ements (Nerbonne and Kleiweg, 2003, p. 349).On the basis of the recordings, Gooskens car-ried out a perception experiment which we de-scribe in Section 4.1.
The experiment is basedon 15 dialects, the total number of dialects avail-able at that time (spring, 2000).
Since we want touse the results of the experiment for validating ourmethods, we used the same set of 15 Norwegiandialects.
It is important to note that Gooskens pre-sented the recordings holistically, including differ-ences in syntax, intonation and morphology.
Ourmethods are restricted to words.3.2 GermanThe German data comes from the PhonetischerAtlas Deutschlands and includes 186 dialect lo-cations.
For each location 201 words wererecorded and transcribed.
The data are availableat the Forschungsinstitut fu?r deutsche Sprache -Deutscher Sprachatlas in Marburg.
The materialis from translations of Wenker-Sa?tze, taken fromthe famous survey by Georg Wenker in the 1879?1887 among teachers from ?
40.000 locations inGermany.
The transcriptions are made on the basisof recordings made under the direction of JoachimGo?schel in the 1960?s and 1970?s in West Ger-many (Go?schel 1992, pp.
64?70).
After the Ger-man reunification similar surveys were conductedin former East Germany.The data were transcribed by four transcribers,and each item was transcribed independently byat least two phoneticians who subsequently con-sulted to come to an agreement.
In 2002 the datawas digitized at the University of Groningen.4 Validation MethodsWhen we apply a measurement technique to a spe-cific problem we are interested both in the con-sistency of the measure and in its validity.
Theconsistency of the measurement reflects the degreeto which the independent elements in the samplesample tend to provide the same signal.
Nun-nally (1978, p.211) recommends the generalized4The database is available at http://www.ling.hf.ntnu.no/nos/.56form of the Spearman-Brown formula for this pur-pose, which has come to be known as the CRON-BACH?S ?
value.
It is determined by the inter-itemcorrelation, i.e.
the average correlation coefficientfor all of the pairs of items in the test, and thetest size.
The Cronbach?s ?
measure rises withthe sample size, and it is therefore normally usedto determine whether samples are large enough toprovide reliable signals.The validity of a measure, or more precisely,the application of a measure to a particular prob-lem is much more difficult and controversial issue(Nunnally, 1978, Chap.
3), but the basic issue iswhether the procedures in fact measure what theypurport to measure, in our case the sort of pro-nunciation similarity which is important in distin-guishing similar language varieties.
In examiningour measures for their validity in identifying thesort of pronunciation similarity which plays a rolein dialectology we compare the measures to otherindications we have that pronunciations are dialec-tally similar.
We discuss these below in more de-tail.
We consider the correlation with distances asperceived by the dialect speakers themselves (seeSection 4.1) and the local (geographic) incoher-ence of dialect distances (see Section 4.2).4.1 PerceptionThe best opportunity for examining the quality ofthe measurements presents itself in the case ofNorwegian, for which we were able to obtain theresults of a perception experiment (Gooskens andHeeringa, 2004).
For each of 15 varieties a record-ing of the fable ?The North Wind and the Sun?
waspresented to 15 groups of Norwegian high schoolpupils, one group from each of the 15 dialects sitesrepresented in the material.
All pupils were famil-iar with their own dialect and had lived most oftheir lives in the place in question (on average 16.7years).
Each group consisted of 16 to 27 listeners.The mean age of the listeners was 17.8 years, 52percent were female and 48 percent male.The 15 dialects were presented in a randomizedorder, and each session was preceded by a (short)practice run.
While listening to the dialects thelisteners were asked to judge each of the 15 di-alects on a scale from 1 (similar to native dialect)to 10 (not similar to native dialect).
This meansthat each group of listeners judged the linguisticdistances between their own dialect and the 15 di-alects, including their own dialect.
In this waywe get a matrix with 15 ?
15 perceived linguis-tic distances.
This matrix is not completely sym-metric.
For example, the distance which the lis-teners from Bergen perceived between their owndialect and the dialect of Trondheim (8.55) is dif-ferent from the distance as perceived by the listen-ers from Trondheim to Bergen (7.84).In order to use this material to calibrate the dif-ferent computational measurements, we examinethe correlations between the 15?15 computationalmatrices with the 15?15 perceptual matrix.
In cal-culating correlations we excluded the distances ofdialects with respect to themselves, i.e.
the dis-tance of Bergen to Bergen, of Bjugn to Bjugn,etc.
In computational matrices these values are al-ways zero, in the perceptual matrix they vary, butare normally greater than zero.
This may be dueto non-geographic (social or individual) variation,but it distorts results in a non-random way (diago-nal distances can only be too high, never too low),we exclude them when calculating the correlationcoefficient.We calculated the standard Pearson product-moment correlation coefficient, but we interpretits significance cautiously, using the Mantel test(Bonnet and Van de Peer, 2002).
In classical teststhe assumption is made that the observations areindependent, which observations in distance ma-trices emphatically are not.
This is certainly truefor calculations of geographic distances, which areminimally constrained to satisfy the standard dis-tance axioms (non-negativity, symmetry, and thetriangle inequality).
We have argued above (?
2.2)that the edit distances we employ are likewise gen-uine distances, which means that sums of editdistances are likewise constrained, and thereforeshould not be regarded as independent observa-tions (in the sense need for hypothesis testing).The Mantel test raises the standards of signif-icance a good deal?
so much that it will turnout that our small (15 ?
15) matrices would needto differ by more than 0.1 in correlation coeffi-cient in order to demonstrate significance.
We willnonetheless urge that the results should be takenseriously as the data needed is difficult to obtain,and the indications are fairly clear (see below).4.2 Local IncoherenceIt is fundamental to dialectology that geographi-cally closer varieties are, in general, linguisticallymore similar.
Nerbonne and Kleiweg (2006) use57this fact to select more probative measurements,namely those measurements which maximize thedegree to which geographically close elements arelikewise seen to be linguistically similar.
Givenour emphasis on distance it is slightly more con-venient to formulate a measure of LOCAL INCO-HERENCE and then to examine the degree to whichvarious string distance measures minimize it.
Thebasic idea is that we begin with each measurementsite s, and inspect the n linguistically most similarsites in order of decreasing linguistic similarity tos.
We then measure how far away these linguisti-cally most similar sites are geographically, for ex-ample, in kilometers.
Good measurements showthat linguistically similar sites are geographicallyclose better than poor measurements do.The details of the formulation reflect the re-sults of dialectometry that dialect distances cer-tainly increase with geographic distance, levelingoff, however, so that geographically more remotevariety-pairs tend to have more nearly the samelinguistic distances to each other.
We sort varietypairs in order of decreasing linguistic similarityand weight more similar ones exponentially morethan less similar ones.
Given this disproportion-ate weighting of the most similar varieties, it alsoquickly becomes uninteresting to incorporate theeffects of more than a small number of geographi-cally closest varieties.
We restrict our attention tothe eight most similar linguistic varieties in calcu-lating local incoherence.Il =1nn?i=1DLi ?DGiDGiDLi =k?j=1dLi,j ?
2?0.5jDGi =k?j=1dGi,j ?
2?0.5jdLi,j , dGi,j : geo.
dist.
between i en jdLi,1??
?n?1 : geo.
dist.
sorted by increasing ling.
diff.dGi,1??
?n?1 : geo.
dist, sorted by increasing geo.
dist.Several remarks may be helpful in understand-ing the proposed measurement.
First, all of the di,jconcern geographic distances.
dLi,1??
?n?1 (summedin DLi ) range over the geographic distances, ar-ranged, however, in increasing order of linguisticdistance, while dGi,1??
?n?1 (summed in DGi ) rangesover the geographic distances among the sites inthe sample, arranged in increasing order of geo-graphic distance.
We examine the latter as an idealcase.
If a given measurement technique alwaysdemonstrated that the neighbors of a given siteused the most similar varieties, then DLi would bethe same DGi , and Il would be 0.
Second, we haveargued above that it is appropriate to count mostsimilar varieties much more heavily in Il, and thisis reflected in the exponential decay in the weight-ing, i.e., 2?0.5j where j ranges over the increas-ingly less similar sites.
Given this weighting ofmost similar varieties, we are also justified in re-stricting the sum inDLi =?kj=1[.
.
.]
to k = 8, andall of the results below use this limitation, whichlikewise improves efficiency.We suppress further discussion of the calcu-lation in the interest of saving space here, not-ing, however, that we used two different notionsof geographic distance.
When examining mea-surements of the German data, we measured geo-graphic distance ?as the crow flies?, but since Nor-way is very mountainous, we used (19th century)travel distances (Gooskens, ).5 Experiments and ResultsIn this section we present results based on the Nor-wegian and German data sources in 5.1 and Sec-tions 5.3.For each data source we consider 40 string com-parison algorithms.
We distinguish between meth-ods with a binary comparison of n-grams andthose with a gradual comparison of n-grams (seeSection 2.4).
Within the category of binary meth-ods, we distinguish between three groups.
In thefirst group, strings are compared just by countingthe number of common n-grams, ignoring the or-der of elements, see Section 2.1).
In the secondgroup the n-grams are aligned (see Section 2.2).We call this ?free alignment?.
In the third groupwe insist on the linguistically informed alignmentof n-grams (see Section 2.5), dubbing this ?forcedalignment?.
Within the category of gradual meth-ods, we distinguish between ?free alignment?
(seeSection 2.6) and ?forced alignment?.
Finally, foreach of these methods, we consider both an un-normalized version of the measure as well as onenormalized by length (see Section 2.3).A measure can only be valid when it is con-sistent, but it may be consistent without beingvalid.
Since consistency is a necessary condi-58binary gradualno free forc.
free forc.align- align- align- align- align-ment ment ment ment mentuni 0.69 0.66 0.66 0.66 0.66bi 0.70 0.69 0.69 0.66 0.68tri 0.71 0.70 0.72 0.66 0.73xbi 0.70 0.69 0.72 0.67 0.73Table 1: Correlations between perceptual dis-tances and unnormalized string edit distance mea-surements among 15 Norwegian dialects.
Highercoefficients indicate better results.tion for validity, we check the consistency of pho-netic distance methods.
For each of the meth-ods we calculated Cronbach?s ?
values, which isbased on the average inter-correlation among thewords (Heeringa, 2004, pp.
170?173).
A widely-accepted threshold in social science for an accept-able ?
is 0.70 (Nunnally, 1978).
After the consis-tency check, we discuss validation results.5.1 Norwegian PerceptionIn this section we first discuss results of unnormal-ized string edit distance measures, and will com-pare them with their normalized counterparts far-ther onwards in this section.The Cronbach?s ?
values of the unnormalizedmeasurements vary from 0.84 to 0.87.
The Cron-bach?s ?
values of the methods with ?forced align-ment?
are a bit lower than the ?
values of the othermethods.
An outlier arises when using the ?forcedalignment?
and gradual bigram distances: ?=0.78,but these all indicate that the measurements arequite consistent.We calculated correlations to the perceptual dis-tances which are described in Section 4.1.
Re-sults are given in Table 1.
Let?s note that theeffect size, i.e., the r value itself, is quite high,0.66 < r < 0.73, meaning that the various dis-tance measure are accounting for 43.6?53.3% ofthe variance in the perception measurements.
Allof the correlation coefficients are massively signif-icant (p < 0.001), but given the stringency of theMantel test, they do not differ significantly fromone another.The correlations are quite similar.
The maxi-mal difference we found was 0.07, so that we con-clude that none of the methods is strikingly betteror worse in operationalizing the level of pronunci-ation difference that dialect speakers are sensitivebinary gradualno free forc.
free forc.align- align- align- align- align-ment ment ment ment mentuni 0.66 0.66 0.66 0.66 0.66bi 0.67 0.67 0.67 0.66 0.66tri 0.68 0.68 0.70 0.66 0.70xbi 0.68 0.68 0.70 0.69 0.70Table 2: Correlations between perceptual dis-tances and different normalized string edit dis-tance measurements among 15 Norwegian di-alects.
Higher coefficients indicate better results.to.The small flood of numbers in Table 1 mayseem confusing.
Therefore we calculated averagesper factor which are presented in Table 4.
We in-vite the reader to refer to both Table 1 and Tablee 4in following the discussion below.
Table 4 showssystematic differences.
For example, contextuallysensitive measures (bigrams, trigrams, and xbi-grams) are usually better (and never worse) thanunigram measures.
The differences among thedifferent means of operationalizing context (bi-grams, trigrams and xbigrams) seem unremark-able, however.
Third, measures which are sensi-tive to linear order are slightly worse than thosewhich are not (variants of DICE) on average5.But when comparing the first column in Table 1with the others, we see that the highest correla-tions (0.73) are found among the order sensitivemethods.
Fourth, forcing alignment to respectvowel/consonant differences yields a modest im-provement in scores.
Fifth, we see no clear ad-vantage in measurements which weight n-gramsmore sensitively to those binary comparison meth-ods which distinguish only same and different.Sixth, and most surprisingly, we can compareTable 1 which provides the correlation of edit dis-tances which were not normalized for length, withTable 2, which provides the results of the mea-surements which were normalized.
For some nor-malized measurements the Cronbach?s ?
value areminimally higher (0.01).
But comparison of thecorrelation coefficients shows that normalizationnever improves measurements, and often leads to adeterioration.
In Table 4 averages for the normal-ized measurements are given.
Normalized mea-5When using the unnormalized versions of the ?DICE?family, the distance is just equal to the number of non-sharedn-grams.59binary gradualno free forc.
free forc.align- align- align- align- align-ment ment ment ment mentuni 0.41 0.37 0.37 0.37 0.37bi 0.37 0.35 0.37 0.36 0.35tri 0.37 0.33 0.35 0.36 0.35xbi 0.36 0.35 0.35 0.37 0.35Table 3: Local incoherence values based on traveldistances for the unnormalized string edit dis-tance measurements between 15 Norwegian di-alects.
The lower the local incoherence value, thebetter the measurement technique.surements display the same systematic differencesthat unnormalized measurements show, except forthe differences between methods which considerthe order of segments and methods which do not.Measures which are sensitive to linear order areslightly better than those which are not (variantsof DICE).5.2 Norwegian Geographic SensitivityAs we mentioned in Section 4.2, Norway is veryrugged.
Therefore we based our local incoher-ence values on travel distances rather than on ge-ographic distances ?as the crow flies?.
We com-puted local incoherence values for both unnormal-ized and normalized string edit distance measure-ments.
The comparison confirms the findings ofSection 5.1: unnormalized methods always per-form better than normalized ones.
The unnormal-ized results are presented in Table 3.Recall that lower local incoherence valuesshould reflect better measurement techniques.When we examine the table as a whole, we noteagain that the various techniques are not hugelydifferent?they perform with similar degrees ofsuccess.In Table 4, we find average local incoherencevalues for the factors under investigation.
We findfirst that contextually sensitive measures (bigrams,trigrams, and xbigrams) are again superior to un-igram methods, and second, measures which aresensitive to linear order are superior to the DICE-like methods (unnormalized versions).
Third, lin-guistically informed alignments, which respect thevowel/consonant distinction, perform better thanuninformed (?free?)
alignment (for the normalizedversions).
Fourth, the average values do not sug-gest any benefit to the gradual weighting of n-grams in comparison with the binary weighting.Most surprisingly, normalization again appears tohave a deleterious effect on the probity of the mea-surements.We must stress again that these finer interpreta-tions results require confirmation with a larger setof sites.5.3 German Geographic SensitivityWhen checking the consistency of the Germanmeasurements we find Cronbach?s ?
values of0.95 and 0.96 for all methods without alignmentor with ?free alignment?
and for all unigram basedmethods.
The higher Cronbach?s ?
levels for thisdata set reflect the fact that it is larger.
We findlower ?
values of 0.83?0.85 for the methods with?forced alignment?.
This accords with the consis-tency results for the Norwegian measurements.When using bigrams, ?
is equal to 0.80 (binary,normalized), 0.51 (gradual, normalized), 0.74 (bi-nary, unnormalized) and 0.45 (gradual, unnormal-ized).
These low values are striking, and we foundno explanation for them, but they suggest that weshould not attach much significance to this combi-nation of measurement properties.
On average, theunnormalized ?
?s are the same as the normalized?
?s.Since consistency values are higher than 0.70(with one exception), we validated the methods bycalculating the geographic local incoherence val-ues.
We would have preferred to use perceptions,but we have no such data in the German case.Since we found unnormalized string edit dis-tance measurements superior to normalized onesin the Sections 5.1 and 5.2, we focus in this sec-tion on the unnormalized methods.
Unnormalizedresults are shown in Table 5.Recall that the lower the local incoherencevalue, the better the measurement technique.
Weinclude this table for the sake of completeness, butit is clear that the results do not jibe with the re-sults obtained from the Norwegian data.
Unigram-based processing appears to be superior, and con-text inferior; order-sensitive processing is inferiorto order-insensitive processing, and linguisticallyinformed (?forced?)
alignment appears to offer noadvantage.We leave the contrast between the Norwegianand German results as a puzzle to be addressed infuture work, but it should be clear that we have60Factor Correlation with Local Number ofperception incoherence measurementsraw normalized raw normalizedno order 0.70 0.67 0.38 0.45 4order 0.69 0.68 0.36 0.46 16unnormalized 0.69 0.36 20normalized 0.68 0.43 20binary 0.69 0.68 0.36 0.43 8gradual 0.68 0.67 0.36 0.43 8free 0.67 0.67 0.36 0.43 8forced 0.70 0.68 0.36 0.42 8unigram 0.67 0.66 0.38 0.45 5bigram 0.68 0.67 0.36 0.45 5trigram 0.70 0.68 0.35 0.42 5xbigram 0.70 0.69 0.36 0.41 5Table 4: Average correlations between perceptual distances and raw, i.e., unnormalized string edit dis-tance measurements among 15 Norwegian dialects.
Higher coefficients and lower local incoherencevalues indicate better results.binary gradualno free forc.
free forc.align- align- align- align- align-ment ment ment ment mentuni 0.94 0.88 0.87 0.88 0.87bi 1.00 0.98 2.09 0.92 5.71tri 1.09 1.05 2.45 0.93 2.09xbi 0.96 0.95 2.45 0.98 2.45Table 5: Local incoherence values based on geo-graphic distances for for the unnormalized stringedit distance measurements 186 German dialects.The lower the local incoherence value, the betterthe measurement technique.rather more confidence in the Norwegian than inthe German results.
This is due on the one had tothe availability of independently behavioral datawe can use to independently validate our compu-tations, but also to the more stable set of values wesee in the case of the Norwegian data.
Exactly whythe German data is so much more variable is alsoa question we must postpone to future work.6 Conclusions and ProspectsIn this paper we examined a range of string com-parison algorithms by applying them to Norwe-gian and German dialect comparison.
The Nor-wegian results suggest that sensitivity to linguis-tic context in the form of n-grams, and to linguis-tic structure in alignment improves measurementtechniques, but they do not confirm the value ofdifferential weighting for n-grams.
The resultsmostly suggest that sensitivity to order of seg-ments improves the measurements.The larger German data likewise is unfortu-nately more recalcitrant (as are other data sets wehave examined, but in which we have less confi-dence).
A disadvantage of the German data maybe that several transcribers were involved, work-ing over a period of twenty years, and that twotypes of surveys were used, having different or-ders of sentences.
There may be subtle differencesin pronunciation as a result of subjects?
becomingmore relaxed or more impatient in the course of asurvey interview.On the other hand, the Norwegian data set issmall (15 dialect sites).
Our conclusions rely onassumptions of its quality and transcriber consis-tency, but this warrants further examination.
Wealso cannot exclude the possibility that optimalmeasurements depend on features of the languageand/or data set.It is tempting to wish to redo this study using alarge, antiseptically clean data set, transcribed reli-ably by a minimal number of phoneticians, but themore important practical direction may be to tryto understand which properties of data sets are im-portant in selecting variants of pronunciation dis-tance measures.
Atlases of material on languagevarieties simply are not always clean and reliable,and if we wish to contribute to their analysis, we61must keep this in mind.AcknowledgmentsWe are grateful to Therese Leinonen, Jens Mobergand Jelena Prokic?
for comments on this work, andin particular for their suggestion that one shouldalso examine the length normalization.
We alsothank the workshop reviewers, in particularly onewho was productively harsh about the treatmentof normalization in an earlier version, and alsopointed out literature we had insufficiently takennote of.
Finally, we are indebted to the Nether-lands Organization for Scientific Research, NWO,for support (project ?Determinants of Dialect Vari-ation, 360-70-120, P.I.
J. Nerbonne)ReferencesEric Bonnet and Yves Van de Peer.
2002. zt: A soft-ware tool for simple and partial Mantel tests.
Jour-nal of Statistical Software, 7(10):1?12.
Availablevia: http://www.jstatsoft.org/.Hans Goebl.
1982.
Dialektometrie: Prinzipienund Methoden des Einsatzes der NumerischenTaxonomie im Bereich der Dialektgeographie.
?Osterreichische Akademie der Wissenschaften,Wien.Hans Goebl.
1984.
Dialektometrische Studien: An-hand italoromanischer, ra?toromanischer und gal-loromanischer Sprachmaterialien aus AIS und ALF.3 Vol.
Max Niemeyer, Tu?bingen.Charlotte Gooskens.
Traveling time as a predictor oflinguistic distance.
Dialectologia et Geolinguistica.submitted, 3/2004.Charlotte Gooskens and Wilbert Heeringa.
2004.
Per-ceptual evaluation of Levenshtein dialect distancemeasurements using Norwegian dialect data.
Lan-guage Variation and Change, 16(3):189?207.Joachim Go?schel.
1992.
Das Forschungsinstitut fu?rDeutsche Sprache ?Deutscher Sprachatlas.
Wis-senschaftlicher Bericht, Das Forschungsinstitut fu?rDeutsche Sprache, Marburg.Wilbert Heeringa.
2004.
Measuring Dialect Pronunci-ation Differences using Levenshtein Distance.
Ph.D.thesis, Rijksuniversiteit Groningen.Cor Hoppenbrouwers and Geer Hoppenbrouwers.2001.
De indeling van de Nederlandse streektalen:Dialecten van 156 steden en dorpen geklasseerd vol-gens de FFM (feature frequentie methode).
Konin-klijke Van Gorcum, Assen.Diana Inkpen, O. Frunza, and Grzegorz Kondrak.2005.
Automatic Identification of Cognates andFalse Friends in French and English.
In Galia An-gelova, Kalina Bontcheva, Ruslan Mitkov, NicolasNicolov, and Nicolai Nicolov, editors, InternationalConference Recent Advances in Natural LanguageProcessing, pages 251?257, Borovets.Brett Kessler.
1995.
Computational dialectology inIrish Gaelic.
In Proc.
of the European ACL, pages60?67, Dublin.Brett Kessler.
2005.
Phonetic comparision algo-rithms.
Transactions of the Philological Society,103(2):243?260.Grzegorz Kondrak.
2005.
N-gram similarity and dis-tance.
In Proceedings of the Twelfth InternationalConference on String Processing and InformationRetrieval (SPIRE 2005), pages 115?126, BuenosAires, Argentina.Joseph Kruskal.
1999.
An overview of sequence com-parison.
In David Sankoff and Joseph Kruskal, edi-tors, Time Warps, String Edits and Macromolecules:The Theory and Practice of Sequence Comparison,pages 1?44.
CSLI, Stanford.
11983.Andre?s Marzal and Enrique Vidal.
1993.
Computationof normalized edit distance and applications.
IEEETransactions on Pattern Analysis and Machine In-telligence, 15(9):926?932.John Nerbonne and Peter Kleiweg.
2003.
Lexical vari-ation in LAMSAS.
Computers and the Humani-ties, 37(3):339?357.
Special Iss.
on ComputationalMethods in Dialectometry ed.
by John Nerbonne andWilliam Kretzschmar, Jr.John Nerbonne and Peter Kleiweg.
2006.
Toward adialectological yardstick.
Quantitative Linguistics,13.
accepted.Jum C. Nunnally.
1978.
Psychometric Theory.McGraw-Hill, New York.Jean Se?guy.
1973.
La dialectometrie dans l?atlas lin-guistique de gascogne.
Revue de Linguistique Ro-mane, 37:1?24.62
