Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 904?912,Beijing, August 20102D Trie for Fast ParsingXian Qian, Qi Zhang, Xuanjing Huang, Lide WuInstitute of Media ComputingSchool of Computer Science, Fudan University{xianqian, qz, xjhuang, ldwu}@fudan.edu.cnAbstractIn practical applications, decoding speedis very important.
Modern structuredlearning technique adopts template basedmethod to extract millions of features.Complicated templates bring about abun-dant features which lead to higher accu-racy but more feature extraction time.
Wepropose Two Dimensional Trie (2D Trie),a novel efficient feature indexing structurewhich takes advantage of relationship be-tween templates: feature strings generatedby a template are prefixes of the featuresfrom its extended templates.
We applyour technique to Maximum Spanning Treedependency parsing.
Experimental resultson Chinese Tree Bank corpus show thatour 2D Trie is about 5 times faster thantraditional Trie structure, making parsingspeed 4.3 times faster.1 IntroductionIn practical applications, decoding speed is veryimportant.
Modern structured learning techniqueadopts template based method to generate mil-lions of features.
Such as shallow parsing (Shaand Pereira, 2003), named entity recognition(Kazama and Torisawa, ), dependency parsing(McDonald et al, 2005), etc.The problem arises when the number of tem-plates increases, more features generated, mak-ing the extraction step time consuming.
Espe-cially for maximum spanning tree (MST) depen-dency parsing, since feature extraction requiresquadratic time even using a first order model.
Ac-cording to Bohnet?s report (Bohnet, 2009), a fastFeatureGenerationTemplate:p .word+p .pos0 0 Feature:lucky/ADJIndex:3228~3233FeatureRetrievalParse TreeBuild lattice, inference etc.Figure 1: Flow chart of dependency parsing.p0.word, p0.pos denotes the word and POS tagof parent node respectively.
Indexes correspondto the features conjoined with dependency types,e.g., lucky/ADJ/OBJ, lucky/ADJ/NMOD, etc.feature extraction beside of a fast parsing algo-rithm is important for the parsing and trainingspeed.
He takes 3 measures for a 40X speedup,despite the same inference algorithm.
One impor-tant measure is to store the feature vectors in fileto skip feature extraction, otherwise it will be thebottleneck.Now we quickly review the feature extractionstage of structured learning.
Typically, it consistsof 2 steps.
First, features represented by stringsare generated using templates.
Then a feature in-dexing structure searches feature indexes to getcorresponding feature weights.
Figure 1 showsthe flow chart of MST parsing, where p0.word,p0.pos denote the word and POS tag of parentnode respectively.We conduct a simple experiment to investi-gate decoding time of MSTParser, a state-of-the-art java implementation of dependency parsing 1.Chinese Tree Bank 6 (CTB6) corpus (Palmer and1http://sourceforge.net/projects/mstparser904Step Feature Index Other TotalGeneration RetrievalTime 300.27 61.66 59.48 421.41Table 1: Time spent of each step (seconds) ofMSTParser on CTB6 standard test data (2660 sen-tences).
Details of the hardware and corpus aredescribed in section 5Xue, 2009) with standard train/development/testsplit is used for evaluation.
Experimental resultsare shown in Table 1.
The observation is that timespent of inference is trivial compared with featureextraction.
Thus, speeding up feature extraction iscritical especially when large template set is usedfor high accuracy.General indexing structure such as Hash andTrie does not consider the relationships betweentemplates, therefore they could not speed up fea-ture generation, and are not completely efficientfor searching feature indexes.
For example, fea-ture string s1 generated by template ?p0.word?is prefix of feature s2 from template ?p0.word +c0.word?
(word pair of parent and child), henceindex of s1 could be used for searching s2.
Fur-ther more, if s1 is not in the feature set, then s2must be absent, its generation can be skipped.We propose Two Dimensional Trie (2D Trie),a novel efficient feature indexing structure whichtakes advantage of relationship between tem-plates.
We apply our technique to MaximumSpanning Tree dependency parsing.
Experimentalresults on CTB6 corpus show that our 2D Trie isabout 5 times faster than traditional Trie structure,making parsing speed 4.3 times faster.The paper is structured as follows: in section 2,we describe template tree which represents rela-tionship between templates; in section 3, we de-scribe our new 2D Trie structure; in section 4, weanalyze the complexity of the proposed methodand general string indexing structures for parsing;experimental results are shown in section 5; weconclude the work in section 6.2 Template tree2.1 Formulation of templateA template is a set of template units which aremanually designed: T = {t1, .
.
.
, tm}.
For con-Unit Meaningp?i/pi the ith node left/right to parent nodec?i/ci the ith node left/right to child noder?i/ri the ith node left/right to root noden.word word of node nn.pos POS tag of node nn.length word length of node n|l conjoin current feature with linear distancebetween child node and parent node|d conjoin current feature with direction of de-pendency (left/right)Table 2: Template units appearing in this papervenience, we use another formulation: T = t1 +.
.
.+tm.
All template units appearing in this paperare described in Table 2, most of them are widelyused.
For example, ?T = p0.word + c0.word|l ?denotes the word pair of parent and child nodes,conjoined with their distance.2.2 Template treeIn the rest of the paper, for simplicity, let si be afeature string generated by template Ti.We define the relationship between templates:T1 is the ancestor of T2 if and only T1 ?
T2, andT2 is called the descendant of T1.
Recall that,feature string s1 is prefix of feature s2.
SupposeT3 ?
T1 ?
T2, obviously, the most efficient wayto look up indexes of s1, s2, s3 is to search s3 first,then use its index id3 to search s1, and finally useid1 to search s2.
Hence the relationship betweenT2 and T3 can be neglected.Therefore we define direct ancestor of T1: T2is a direct ancestor of T1 if T2 ?
T1, and there isno template T ?
such that T2 ?
T ?
?
T1.
Corre-spondingly, T1 is called the direct descendant ofT2.Template graph G = (V,E) is a directed graphthat represents the relationship between templates,where V = {T1, .
.
.
, Tn} is the template set, E ={e1, .
.
.
, eN} is the edge set.
Edge from Ti to Tjexists, if and only if Ti is the direct ancestor ofTj .
For templates having no ancestor, we add anempty template as their common direct ancestor,which is also the root of the graph.The left part of Figure 2 shows a templategraph for templates T1 =p0.word, T2 =p0.pos ,T3 =p0.word + p0.pos.
In this example, T3 has 2direct ancestors, but in fact s3 has only one prefix905p .word0p .word +p pos0 0.rootp .word0rootp .pos0p .pos0 p .pos0Figure 2: Left graph shows template graph forT1 =p0.word, T2 =p0.pos , T3 =p0.word +p0.pos.
Right graph shows the corresponding tem-plate tree, where each vertex saves the subset oftemplate units that do not belong to its fatherwhich depends on the order of template units ingeneration step.
If s3 = s1 + s2, then its prefix iss1, otherwise its prefix is s2.
In this paper, we sim-ply use the breadth-first tree of the graph for dis-ambiguation, which is called template tree.
Theonly direct ancestor T1 of T2 in the tree is calledfather of T2, and T2 is a child of T1.
The rightpart of Figure 2 shows the corresponding templatetree, where each vertex saves the subset of tem-plate units that do not belong to its father.2.3 Virtual vertexConsider the template tree in the left part of Figure3, red vertex and blue vertex are partially over-lapped, their intersection is p0.word, if string sfrom template T =p0.word is absent in feature set,then both nodes can be neglected.
For efficientlypruning candidate templates, each vertex in tem-plate tree is restricted to have exactly one templateunit (except root).
Another important reason forsuch restriction will be given in the next section.To this end, virtual vertexes are created formulti-unit vertexes.
For efficient pruning, the newvirtual vertex should extract the most commontemplate unit.
A natural goal is to minimize thecreation number.
Here we use a simple greedystrategy, for the vertexes sharing a common fa-ther, the most frequent common unit is extractedas new vertex.
Virtual vertexes are iteratively cre-ated in this way until all vertexes have one unit.The final template tree is shown in the right part ofFigure 3, newly created virtual vertexes are shownin dashed circle.rootp .word+p .word+p .word-1 01p .word+p pos0 0. c .word+c pos0 0.rootp .word0p .pos0 p .word-1p .word1c .word0c .pos0Figure 3: Templates that are partially overlapped:Tred ?
Tblue =p0.word, virtual vertexes shown indashed circle are created to extract the commonunitrootp .word0p .pos0parse tagVV NN... ... ... ............Level 0Level 1Level 2 VV ...Figure 4: 2D Trie for single template, alphabets atlevel 1 and level 2 are the word set, POS tag setrespectively3 2D Trie3.1 Single template caseTrie stores strings over a fixed alphabet, in ourcase, feature strings are stored over several alpha-bets, such as word list, POS tag list, etc.
which areextracted from training corpus.To illustrate 2D Trie clearly, we first consider asimple case, where only one template used.
Thetemplate tree degenerates to a sequence, we coulduse a Trie like structure for feature indexing, theonly difference from traditional Trie is that nodesat different levels could have different alphabets.One example is shown in Figure 4.
There are 3feature strings from template ?p0.word + p0.pos?
:{parse/VV, tag/VV, tag/VV}.
Alphabets at level1 and level 2 are the word set, POS tag set re-spectively, which are determined by correspond-ing template vertexes.As mentioned before, each vertex in templatetree has exactly one template unit, therefore, ateach level, we look up an index of a word or POS906HehadbeenasalesandmarketingexecutivewithChryslerfor20yearsPRPVBDVBNDTNNSCCNNNNINNNPINCDNNS264827311121041150640631337419236023156022030056677821272804130112120613060214Figure 5: Look up indexes of words and POS tagsbeforehand.tag in sentence, not their combinations.
Hence thenumber of alphabets is limited, and all the indexescould be searched beforehand for reuse, as shownin Figure 5, the token table is converted to a in-dex table.
For example, when generating featuresat position i of a sentence, template ?r0.word +r1.word?
requires index of i+1th word in the sen-tence, which could be reused for generation at po-sition i+ 1.3.2 General caseGenerally, for vertex in template tree with K chil-dren, children of corresponding Trie node are ar-ranged in a matrix of K rows and L columns, Lis the size of corresponding alphabet.
If the vertexis not virtual, i.e., it generates features, one morerow is added at the bottom to store feature indexes.Figure 6 shows the 2D Trie for a general templatetree.3.3 Feature extractionWhen extracting features for a pair of nodes in asentence, template tree and 2D Trie are visited inbreath first traversal order.
Each time, an alpha-bet and a token index j from index table are se-lected according to current vertex.
For example,POS tag set and the index of the POS tag of par-ent node are selected as alphabet and token indexrespectively for vertex ?p0.pos?.
Then children inthe jth column of the Trie node are visited, validchildren and corresponding template vertexes aresaved for further retrieval or generate feature in-dexes if the child is at the bottom and current Trienode is not virtual.
Two queues are maintained tobeen......... .........VBNp .word+p .pos?been/VBN0 0...... ......p .word?been0... ...rootrootp .word0p .pos0 c .word0had .........p .word?had0 ...VBDp .word+p .pos?had/VBD0 0...... ......Hep .word+w .wordhad/He0 0?......nmod vmodobj subFeatureindex array-1 -13327 2510nmod vmodobj sub-1 7821-1 -1............ ............ beenp .word+w .word?had/been0 0 ......invalidFigure 6: 2D trie for a general template tree.Dashed boxes are keys of columns, which are notstored in the structuresave the valid children and Trie nodes.
Details offeature extraction algorithm are described in Al-gorithm 1.3.4 ImplementationWhen feature set is very large, space complexityof 2D Trie is expensive.
Therefore, we use DoubleArray Trie structure (Aoe, 1989) for implementa-tion.
Since children of 2D Trie node are arrangedin a matrix, not an array, so each element of thebase array has a list of bases, not one base in stan-dard structure.
For children that store features,corresponding bases are feature indexes.
One ex-ample is shown in Figure 7.
The root node has3 bases that point to three rows of the child ma-trix of vertex ?p0.word?
respectively.
Number ofbases in each element need not to be stored, sinceit can be obtained from template vertex in extrac-tion procedure.Building algorithm is similarly to Double ArrayTrie, when inserting a Trie node, each row of thechild matrix is independently insert into base andcheck arrays using brute force strategy.
The inser-907been... been.........had...had had...... .........been ... had had... ... ... ...... ... been hadroot base1 base2base3root base 2base1base3...VBD... ...VBN.........VBDVBN...base1base1 base1base1-1 -1... ... 3327 2510 ... ......... ......... -1 7821-1 -1... ...-1 -1... ... 3327 2510 ... ......... ......... -1 7821-1 -1... ...BasearrayFeature index arrayFeature index arrayFigure 7: Build base array for 2D Trie in Figure 6.
String in the box represents the key of the child.Blank boxes are the invalid children.
The root node has 3 bases that point to three rows of the childmatrix of vertex ?p0.word?
respectivelyAlgorithm 1 Feature extraction using 2D TrieInput: 2D Trie that stores features, templatetree, template graph, a table storing token in-dexes, parent and child positionsOutput: Feature index set S of dependencyfrom parent to child.Create template vertex queue Q1 and Trienode queue Q2.
Push roots of template treeand Trie into Q1, Q2 respectively.
S = ?while Q1 is not empty, doPop a template vertex T from Q1 and a Trienode N from Q2.
Get token index j fromindex table according to T .for i = 1 to child number of Tif child of N at row i column j is valid,push it into Q2 and push the ith childof T into Q1.elseremove decedents of ith child of Tfrom template treeend ifend forif T is not virtual and the last child of N incolumn j is validEnumerate dependency types, addvalid feature indexes to Send ifend whileReturn S.tion repeats recursively until all features stored.4 Complexity analysisLet?
|T | = number of templates?
|t| = number of template units?
|V | = number of vertexes in template tree,i.e, |t|+ number of virtual vertexes?
|F | = number of features?
l = length of sentence?
|f | = average length of feature stringsThe procedure of 2D Trie for feature extractionconsists of 2 steps: tokens in string table aremapped to their indexes, then Algorithm 1 is car-ried out for all node pairs of sentence.
In the firststep, we use double array Trie for efficient map-ping.
In fact, time spent is trivial compared withstep 2 even by binary search.
The main time spentof Algorithm 1 is the traversal of the whole tem-plate tree, in the worst case, no vertexes removed,so the time complexity of a sentence is l2|V |,which is proportional to |V |.
In other words, mini-mizing the number of virtual vertexes is importantfor efficiency.For other indexing structures, feature genera-tion is a primary step of retrieval.
For each node908Structure Generation Retrieval2D Trie l2|V |Hash / Trie l2|t| l2|f ||T |Binary Search l2|t| l2|T | log |F |Table 3: Time complexity of different indexingstructures.pair of sentence, |t| template units are processed,including concatenations of tokens and split sym-bols (split tokens in feature strings), boundarycheck ( e.g, p?1.word is out of boundary for be-ginning node of sentence).
Thus the generationrequires l2|t| processes.
Notice that, time spent ofeach process varies on the length of tokens.For feature string s with length |s|, if perfecthashing technique is adopted for index retrieval, ittakes |s| calculations to get hash value and a stringcomparison to check the string at the calculatedposition.
So the time complexity is proportional to|s|, which is the same as Trie.
Hence the total timefor a sentence is l2|f ||T |.
If binary search is usedinstead, log |F | string comparisons are required,complexity for a sentence is l2|T | log |F |.Time complexity of these structures is summa-rized in Table 3.5 Experiments5.1 Experimental settingsWe use Chinese Tree Bank 6.0 corpus for evalua-tion.
The constituency structures are converted todependency trees by Penn2Malt 2 toolkit and thestandard training/development/test split is used.257 sentences that failed in the conversion wereremoved, yielding 23316 sentences for training,2060 sentences for development and 2660 sen-tences for testing respectively.Since all the dependency trees are projective,a first order projective MST parser is naturallyadopted.
Online Passive Aggressive algorithm(Crammer et al, 2006) is used for fast training, 2parameters, i.e, iteration number and C, are tunedon development data.
The quality of the parser ismeasured by the labeled attachment score (LAS),i.e., the percentage of tokens with correct head anddependency type.2http://w3.msi.vxu.se/ nivre/research/Penn2Malt.htmlGroup IDs #Temp.
#Vert.
#Feat.
LAS1 1-2 72 91 3.23M 79.55%2 1-3 128 155 10.4M 81.38%3 1-4 240 275 25.0M 81.97%4 1-5 332 367 34.8M 82.44%Table 5: Parsing accuracy and number of tem-plates, vertexes in template tree, features in decod-ing stage (zero weighted features are excluded) ofeach group.We compare the proposed structure with Trieand binary search.
We do not compare with per-fect hashing, because it has the same complex-ity as Trie, and is often used for large data baseretrieval, since it requires only one IO opera-tion.
For easy comparison, all feature indexingstructures and the parser are implemented withC++.
All experiments are carried out on a 64bitlinux platform (CPU: Intel(R) Xeon(R) E5405,2.00GHz, Memory: 16G Bytes).
For each tem-plate set, we run the parser five times on test dataand the averaged parsing time is reported.5.2 Parsing speed comparisonTo investigate the scalability of our method, richtemplates are designed to generate large featuresets, as shown in Table 4.
All templates are orga-nized into 4 groups.
Each row of Table 5 showsthe details of a group, including parsing accu-racy and number of templates, vertexes in tem-plate tree, and features in decoding stage (zeroweighted features are excluded).There is a rough trend that parsing accuracyincreases as more templates used.
Though suchtrend is not completely correct, the clear conclu-sion is that, abundant templates are necessary foraccurate parsing.Though algorithm described in section 2.3 forminimizing the number of virtual vertexes isheuristic, empirical results are satisfactory, num-ber of newly created vertexes is only 10% as orig-inal templates.
The reason is that complex tem-plates are often extended from simple ones, theirdifferences are often one or two template units.Results of parsing time comparison are shownin Table 6.
We can see that though time com-plexity of dynamic programming is cubic, pars-ing time of all systems is consistently dominated909ID Templates1 pi.word pi.pos pi.word+pi.posci.word ci.pos ci.word+ci.pos (|i| ?
2)pi.length pi.length+pi.posci.length ci.length+ci.pos (|i| ?
1)p0.length+c0.length|ld p0.length+c0.length+c0.pos|ld p0.length+p0.pos+c0.length|ldp0.length+p0.pos+c0.pos|ld p0.pos+c0.length+c0.pos|ld p0.length+p0.pos+c0.length+c0.pos|ldpi.length+pj .length+ck.length+cm.length|ld (|i|+ |j|+ |k|+ |m| ?
2)r0.word r?1.word+r0.word r0.word+r1.wordr0.pos r?1.pos+r0.pos r0.pos+r1.pos2 pi.pos+cj .pos|d pi.word+cj .word|d pi.pos+cj .word+cj .pos|dpi.word+pi.pos+cj .pos|d pi.word+pi.pos+cj .word|d pi.word+cj .word+cj .pos|dpi.word+pi.pos+cj .word+cj .pos|d (|i|+ |j| = 0)Conjoin templates in the row above with |l3 Similar with 2 |i|+ |j| = 14 Similar with 2 |i|+ |j| = 25 pi.word + pj .word + ck.word|d pi.word + cj .word + ck.word|dpi.pos + pj .pos + ck.pos|d pi.pos + cj .pos + ck.pos|d (|i|+ |j|+ |k| ?
2)Conjoin templates in the row above with |lpi.word + pj .word + pk.word + cm.word|d pi.word + pj .word + ck.word + cm.word|dpi.word + cj .word + ck.word + cm.word|dpi.pos + pj .pos + pk.pos + cm.pos|d pi.pos + pj .pos + ck.pos + cm.pos|dpi.pos + cj .pos + ck.pos + cm.pos|d (|i|+ |j|+ |k|+ |m| ?
2)Conjoin templates in the row above with |lTable 4: Templates used in Chinese dependency parsing.by feature extraction.
When efficient indexingstructure adopted, i.e, Trie or Hash, time index re-trieval is greatly reduced, about 4-5 times fasterthan binary search.
However, general structuressearch features independently, their results couldnot guide feature generation.
Hence, feature gen-eration is still time consuming.
The reason is thatprocessing each template unit includes a series ofsteps, much slower than one integer comparisonin Trie search.On the other hand, 2D Trie greatly reduces thenumber of feature generations by pruning the tem-plate graph.
In fact, no string concatenation oc-curs when using 2D Trie, since all tokens are con-verted to indexes beforehand.
The improvementis significant, 2D Trie is about 5 times faster thanTrie on the largest feature set, yielding 13.4 sen-tences per second parsing speed, about 4.3 timesfaster.Space requirement of 2D Trie is about 2.1 timesas binary search, and 1.7 times as Trie.
One possi-ble reason is that column number of 2D Trie (e.g.size of words) is much larger than standard doublearray Trie, which has only 256 children, i.e, rangeof a byte.
Therefore, inserting a 2D Trie node ismore strict, yielding sparser double arrays.5.3 Comparison against state-of-the-artRecent works on dependency parsing speedupmainly focus on inference, such as expectedlinear time non-projective dependency parsing(Nivre, 2009), integer linear programming (ILP)for higher order non-projective parsing (Martinset al, 2009).
They achieve 0.632 seconds per sen-tence over several languages.
On the other hand,Goldberg and Elhadad proposed splitSVM (Gold-berg and Elhadad, 2008) for fast low-degree poly-nomial kernel classifiers, and applied it to transi-tion based parsing (Nivre, 2003).
They achieve53 sentences per second parsing speed on En-glish corpus, which is faster than our results, sincetransition based parsing is linear time, while forgraph based method, complexity of feature ex-traction is quadratic.
Xavier Llu?
?s et al (Llu?
?set al, 2009) achieve 8.07 seconds per sentencespeed on CoNLL09 (Hajic?
et al, 2009) ChineseTree Bank test data with a second order graphicmodel.
Bernd Bohnet (Bohnet, 2009) also usessecond order model, and achieves 610 minutes onCoNLL09 English data (2399 sentences, 15.3 sec-ond per sentence).
Although direct comparisonof parsing time is difficult due to the differencesin data, models, hardware and implementations,910Group Structure Total Generation Retrieval Other Memory sent/secTrie 87.39 63.67 10.33 13.39 402M 30.441 Binary Search 127.84 62.68 51.52 13.64 340M 20.812D Trie 39.74 26.29 13.45 700M 66.94Trie 264.21 205.19 39.74 19.28 1.3G 10.072 Binary Search 430.23 212.50 198.72 19.01 1.2G 6.182D Trie 72.81 53.95 18.86 2.5G 36.53Trie 620.29 486.40 105.96 27.93 3.2G 4.293 Binary Search 982.41 484.62 469.44 28.35 2.9G 2.712D Trie 146.83 119.56 27.27 5.9G 18.12Trie 854.04 677.32 139.70 37.02 4.9G 3.114 Binary Search 1328.49 680.36 609.70 38.43 4.1G 2.002D Trie 198.31 160.38 37.93 8.6G 13.41Table 6: Parsing time of 2660 sentences (seconds) on a 64bit linux platform (CPU: Intel(R) Xeon(R)E5405, 2.00GHz, Memory: 16G Bytes).
Title ?Generation?
and ?Retrieval?
are short for feature gen-eration and feature index retrieval steps respectively.System sec/sent(Martins et al, 2009) 0.63(Goldberg and Elhadad, 2008) 0.019(Llu?
?s et al, 2009) 8.07(Bohnet, 2009) 15.3(Galley and Manning, 2009) 15.6ours group1 0.015ours group2 0.027ours group3 0.055ours group4 0.075Table 7: Comparison against state of the art, di-rect comparison of parsing time is difficult due tothe differences in data, models, hardware and im-plementations.these results demonstrate that our structure canactually result in a very fast implementation of aparser.
Moreover, our work is orthogonal to oth-ers, and could be used for other learning tasks.6 ConclusionWe proposed 2D Trie, a novel feature indexingstructure for fast template based feature extrac-tion.
The key insight is that feature strings gener-ated by a template are prefixes of the features fromits extended templates, hence indexes of searchedfeatures can be reused for further extraction.
Weapplied 2D Trie to dependency parsing task, ex-perimental results on CTB corpus demonstrate theadvantages of our technique, about 5 times fasterthan traditional Trie structure, yielding parsingspeed 4.3 times faster, while using only 1.7 timesas much memory.7 AcknowledgmentsThe author wishes to thank the anonymousreviewers for their helpful comments.
Thiswork was partially funded by 973 Program(2010CB327906), The National High Technol-ogy Research and Development Program of China(2009AA01A346), Shanghai Leading AcademicDiscipline Project (B114), Doctoral Fund of Min-istry of Education of China (200802460066), andShanghai Science and Technology DevelopmentFunds (08511500302).ReferencesAoe, Jun?ichi.
1989.
An efficient digitalsearch algorithm by using a double-array struc-ture.
IEEE Transactions on software andengineer-ing, 15(9):1066?1077.Bohnet, Bernd.
2009.
Efficient parsing of syntacticand semantic dependency structures.
In Proceed-ings of the Thirteenth Conference on ComputationalNatural Language Learning (CoNLL 2009): SharedTask, pages 67?72, Boulder, Colorado, June.
Asso-ciation for Computational Linguistics.Crammer, Koby, Joseph Keshet, Shai Shalev-Shwartz,and Yoram Singer.
2006.
Online passive-aggressivealgorithms.
In JMLR 2006.911Galley, Michel and Christopher D. Manning.
2009.Quadratic-time dependency parsing for machinetranslation.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 773?781,Suntec, Singapore, August.
Association for Compu-tational Linguistics.Goldberg, Yoav and Michael Elhadad.
2008. splitsvm:Fast, space-efficient, non-heuristic, polynomial ker-nel computation for nlp applications.
In Proceed-ings of ACL-08: HLT, Short Papers, pages 237?240,Columbus, Ohio, June.
Association for Computa-tional Linguistics.Hajic?, Jan, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The conll-2009 shared task: Syntactic and semantic dependen-cies in multiple languages.
In Proceedings of theThirteenth Conference on Computational NaturalLanguage Learning (CoNLL 2009): Shared Task,pages 1?18, Boulder, Colorado, June.
Associationfor Computational Linguistics.Kazama, Jun?ichi and Kentaro Torisawa.
A new per-ceptron algorithm for sequence labeling with non-local features.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 315?324.Llu?
?s, Xavier, Stefan Bott, and Llu?
?s Ma`rquez.
2009.A second-order joint eisner model for syntactic andsemantic dependency parsing.
In Proceedings of theThirteenth Conference on Computational NaturalLanguage Learning (CoNLL 2009): Shared Task,pages 79?84, Boulder, Colorado, June.
Associationfor Computational Linguistics.Martins, Andre, Noah Smith, and Eric Xing.
2009.Concise integer linear programming formulationsfor dependency parsing.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP, pages 342?350, Suntec, Singapore, August.
Association forComputational Linguistics.McDonald, Ryan, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics, pages 91?97.
Association for Computa-tional Linguistics.Nivre, Joakim.
2003.
An efficient algorithm forprojective dependency parsing.
In Proceedings ofthe 11th International Conference on Parsing Tech-niques, pages 149?160.Nivre, Joakim.
2009.
Non-projective dependencyparsing in expected linear time.
In Proceedings ofthe Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP,pages 351?359, Suntec, Singapore, August.
Asso-ciation for Computational Linguistics.Palmer, Martha and Nianwen Xue.
2009.
Adding se-mantic roles to the Chinese Treebank.
Natural Lan-guage Engineering, 15(1):143?172.Sha, Fei and Fernando Pereira.
2003.
Shallow pars-ing with conditional random fields.
In Proceedingsof the 2003 Human Language Technology Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 134?141,May.912
