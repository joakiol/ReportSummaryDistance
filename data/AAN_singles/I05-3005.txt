Morphological features help POS tagging of unknown words acrosslanguage varietiesHuihsin TsengDept.
of LinguisticsUniversity of ColoradoBoulder, CO 80302tseng@colorado.eduDaniel JurafskyDept.
of LinguisticsStanford UniversityStanford, CA 94305jurafsky@stanford.eduChristopher ManningDept.
of Computer ScienceStanford UniversityStanford, CA 94305manning@stanford.eduAbstractPart-of-speech tagging, like any supervised statisticalNLP task, is more difficult when test sets are verydifferent from training sets, for example when tag-ging across genres or language varieties.
We exam-ined the problem of POS tagging of differentvarieties of Mandarin Chinese (PRC-Mainland, PRC-Hong Kong, and Taiwan).
An analytic study firstshowed that unknown words were a major source ofdifficulty in cross-variety tagging.
Unknown wordsin English tend to be proper nouns.
By contrast, wefound that Mandarin unknown words were mostlycommon nouns and verbs.
We showed these resultsare caused by the high frequency of morphologicalcompounding in Mandarin; in this sense Mandarin ismore like German than English.
Based on this analy-sis, we propose a variety of new morphological un-known-word features for POS tagging, extendingearlier work by others on unknown-word tagging inEnglish and German.
Our features were implementedin a maximum entropy Markov model.
Our systemachieves state-of-the-art performance in Mandarintagging, including improving unknown-word taggingperformance on unseen varieties in Chinese Treebank5.0 from 61% to 80% correct.1 IntroductionPart-of-speech tagging is an important enabling taskfor natural language processing, and state-of-the-arttaggers perform quite well, when training and testdata are drawn from the same corpus.
Part-of-speechtagging is more difficult, however, when a test set isdrawn from a corpus that includes significantly dif-ferent varieties of the language.
One factor that mayplay a role in this cross-variety difficulty is the pres-ence of test-set words that were unseen in cross-variety training sets.We chose Mandarin Chinese to study this question ofcross-variety and unknown-word POS tagging.
Man-darin is both a spoken and a written language; as awritten language, it is the official written language ofthe PRC (Mainland and Hong Kong), and Taiwan.Thus regardless of which dialect people speak athome, they write in Mandarin.
But the varieties ofMandarin written in the PRC (Mainland and HongKong) and Taiwan differ in orthography, lexicon,and even grammar about as much as the British,American, and Australian varieties of English (ormore in some cases).
The corpus we use, ChineseTreebank 5.0 (Palmer et al, 2005), contains datafrom the three language varieties as well as differentgenres within the varieties.
It thus provides a gooddata set for the impact of language variation on tag-ging performance.Previous work on POS tagging of unknown wordshas proposed a number of features based on prefixesand suffixes and spelling cues like capitalization(Toutanova et al 2003, Brants 2000, Ratnaparkhi1996).
For example, these systems followedSamuelsson (1993) in using n-grams of letter se-quences ending and starting each word as unknownword features.
But these features have mainly beentested on inflectional languages like English andGerman, whose derivational and inflectional affixestend to be a strong indicator of word classes; Brants(2000), for example, showed that an English wordending in the suffix -able was very likely to be anadjective.
Chinese, by contrast, has more than 4000frequent affix characters.
The amount of training datafor each affix is thus quite sparse and (as we willshow later) Chinese affixes are quite ambiguous intheir part-of-speech identity.
Furthermore, it is possi-ble that n-gram features may not be suited to Chineseat all, since Chinese words are much shorter thanEnglish (averaging 2.4 characters per word comparedwith 7.7 for English, for unknown words in CTB 5.0and Wall Street Journal (Marcus et el., 1993)).In order to deal with these difficulties, we first per-formed an analytic study with the goal of understand-ing the morphological properties of unknown wordsin Chinese.
Based on this analysis, we then proposenew morphological features for addressing the un-known word problem.
We also showed how thesefeatures could make use of a non-CTB corpus thathad been labeled with very different POS tags, byconverting those tags into features.32The remainder of the paper is organized as follows.The next section is concerned with a corpus analysisof cross language variety differences and introducesChinese morphology.
In Section 3, we evaluate anumber of lexical, sequence, and linguistic features.Section 4 reviews related work and summarizes ourcontribution.2 DataChinese Treebank 5.0 (CTB) contains 500K words ofnewspaper and magazine articles annotated with seg-mentation, part-of-speech, and syntactic constituencyinformation.
It includes data from three major mediasources, XH1 from PRC, HKSAR2 from Hong Kong,and SM3 from Taiwan.
In terms of genre, both XHand HKSAR focus on politics and economic issues,and SM more on topics such as culture, health, edu-cation and travel.
All of the files in CTB are encodedusing Guo Biao (GB) and use simplified characters.We did some cleanup of character encoding errors inCTB before running our experiments.
Taiwan andHong Kong still use the traditional forms of charac-ters, while PRC-Mainland has adopted simplifiedforms of many characters, which also collapse somedistinctions between characters.
Additionally a dif-ferent character set encoding is standardly used.
Thearticles in HKSAR and SM originally used tradi-tional characters and Big 5 encoding, but prior toinclusion in the CTB corpus they had been convertedinto simplified characters and GB.
Some errors seemto have crept into this conversion process, acciden-tally leaving traditional characters such as ?
insteadof simplified ?
(after),  ?
for ?
(for),  ??
and??
and ??
(what), all of which we fixed.
Wealso normalized half width numbers, alphabets, andpunctuation to full width.
Finally we removed the -NONE- traces left over from CTB parse trees.3 Corpus analysisWe begin with an analytic study of potential prob-lems for POS tagging on cross language variety data.3.1 More unknown words across varieties?We first test our hypothesis that a test set from a dif-ferent language variety will contain more unknownwords.
Table 1 has the number of words in ourdevset that were unseen in the XH-only training set(we describe our training/dev/test split more fully inthe next section).
The devset contains equal amountsof data from all three varieties (XH, HKSAR, andSM).
As table 1 shows, in data taken from the same1 Xinhua Agency2 Information Services Department of Hong Kong Special Admin-istrative Region3 Sinorama magazinesource as the training data (XH), 4.63% of the wordswere unseen in training, compared to the much largernumbers of unknown words in the cross-variety data-sets (14.3% and 16.7%).
Some of this difference isprobably due to genre as well, especially for the out-lier-genre SM set.Table 1 Percent of words in devset that were unseen in anXH-only  training set.
See Table 4 for more details.Data Set Lang Variety Source Genre % unkXH  MainlandMandarinXinhua News 4.6HKSAR Hong KongMandarinHKSAR News 14.2SM TaiwanMandarinSino-ramaMagazine 16.7Devset Mix Mix Mix 12.03.2 What are the unknown words?In this section, we analyze the part-of-speech charac-teristics of the unknown words in our devset.Table 2 Word class distribution of unknown words indevset, XH, HKSAR, SM.
Devset represents the conjunc-tion of the three varieties.
CC, DT, LC, P, PN, PU, and SPare considered as closed classes by CTB.Word class Devset XH HKSAR SMAD (adverb) 74 2 23 49CC (coordinating conj.)
7 - - 7CD (cardinal number) 151 108 23 20DT (determiner) 10 - 6 4FW (foreign words) 2 2 - -JJ (other noun modifier) 79 14 38 27LC (localizer/postposit) 1 - 1 -M (measure word) 12 2 4 6NN (common noun) 1128 131 520 477NR (proper noun) 400 92 156 152NT (temporal noun) 53 3 38 12OD (ordinal number) 4 - 4 -P (preposition) 16 1 8 7PN (pronoun) 10 - 3 7PU (punctuation) 361 - 110 251SP(sentence final particle) 1 - - 1VA(predicative adjective) 43 1 19 23VV (other verbs) 497 25 215 257Total 2849 381 1168 1300Table 2 shows that the majority of Chinese unknownwords are common nouns (NN) and verbs (VV).
Thisholds both within and across different varieties.
Be-yond the content words, we find that 10.96% and21.31% of unknown words are function words inHKSAR and SM data.
Such unknown function wordsinclude the determiner gewei (?everybody?
), the con-junction huoshi (?or?
), the preposition liantong(?with?
), the pronoun nali (?where?
), and symbolsused as quotes  ???
and ???
(punctuation).
XHdoes contain words with similar function (huozhe33?or?, yu ?with?, dajia ?everybody?, quotation marks??
?and ???).
Our result thus suggests that eachMandarin variety may have characteristic functionwords.3.3 Cross language comparisonA key goal of our work is to understand the way thatunknown words differ across languages.
We thuscompare Chinese, German, and English.
FollowingBrants (2000), we extracted 10% of the data from thePenn Treebank Wall Street Journal (WSJ 4 ) andNEGRA5 (Brants et al, 1999) as observation samplesto compare to the rest of the corpora.In these observation samples, we found that Chinesewords are more ambiguous in POS than English andGerman; 29.9% of tokens in CTB have more thanone POS tag, while only 19.8% and 22.9% of tokensare ambiguous in English and German, respectively.Table 3 shows that 40.6% of unknown words areproper nouns6  in English, while both Chinese andGerman have less than 15% of unknown words asproper nouns.
Unlike English, 60% of the unknownwords in Chinese and German are verbs and commonnouns.
In the next section we investigate the cause ofthis similarity between Chinese and German un-known word distribution.Table 3 Comparison of unknown words in English, Ger-man and Mandarin.
The English and German data are ex-tracted from WSJ and NEGRA.
Chinese data is our CTBdevset.Language English% German% Chinese%Proper nouns 40.6 12.2 14.0Other nouns 24.0 53.0 41.5Verbs 6.8 11.4 19.0ALL 100.0 100.0 100.04 Morphological analysisIn order to understand the causes of the similarity ofChinese and German, and to help suggest possiblefeatures, we turn here to an introduction to Chinesemorphology and its implications for part-of-speechtagging.4 WSJ unknown words are those in WSJ 19-21 but unseen in WSJ0-18; these are the devset and training set from Toutanova et al(2003).5 The unknown words of NEGRA are words in a 10% randomlyextracted set that were unseen in the rest of the corpus.6 We treat NNP (proper noun) and NNPS(proper noun plural) asproper nouns, NN(noun) and NNS(noun plural) as other nouns,and V* as verbs in WSJ.
We treat NE (Eigennamen) as propernouns, NN (Normales Nomen) as other nouns, and V* as verbs inNEGRA.
We treat NR as proper nouns, NN and NT as other nouns,and V* as verbs in CTB.4.1 Chinese morphologyChinese words are typically formed by four morpho-logical processes: affixation, compounding, idiomi-zation, and reduplication, as shown in Table 4.In affixation, a bound morpheme is added to othermorphemes, forming a larger unit.
Chinese has asmall number of prefixes and infixes7 and numeroussuffixes (Chao 1968, Li and Thompson 1981).
Chi-nese prefixes include items such as gui (?noble?)
inguixing (?your name?
), bu (?not?)
in budaode (?im-moral?
), and  lao (?senior?)
in laohu (?tiger?)
andlaoshu (?mouse?).
There are a number of Chinesesuffixes, including zhe (?marks a person who is anagent of an action?)
in zuozhe (?author?
), shi (?mas-ter?)
in laoshi (?teacher?
), ran (-ly) in huran (?sud-denly?
), and xin (-ity or ?ness) in kenengxin(?possibility?
).Compound words are composed of multiple stemmorphemes.
Chao (1968) describes a few of the dif-ferent compounding rules in Mandarin, such as coor-dinate compound, subject predicate compound, nounnoun compound, adj noun compound and so on.
Twoexamples of coordinate compounds are anpaiARRANGE-ARRANGE (?to arrange, arrangement?
)and xuexi STUDY-STUDY (?to study?
).Table 4 Chinese morphological rules and examplesExamplesPrefix lao (?senior?)
in laohu ( ?tiger?
)Suffix shi (?master?)
in laoshi (?teacher?
)Compounding xuexi  (?to study?, ?study?
)Idiomization wanshiruyi (?everything is fine?
)Reduplication changchang (?taste a bit?
)Compounding is extremely common in both Chineseand German.
The phrase ?income tax?
is treated asan NP in English, but it is a word in German, Ein-kommensteuer, and in Chinese, suodesui.
We suggestthat it is this rich use of compounding that causes thewide variety of unknown common nouns and verbsin Chinese and German.
However, there are still dif-ferences in their compound rules.
German com-pounds can compose with a large number of elements,but Chinese compounds normally consist of twobases.
Most German compounds are nouns, but Chi-nese has both noun and verb compounds.Two final types of Chinese morphological processesthat we will not focus on are idiomization (in which awhole phrase such as wanshiruyi (?everything isfine?)
functions as a word, and reduplication, inwhich a morpheme or word is repeated to form a newword such as the formation of changchang (?taste a7 Chinese only has two infixes, which are de and bu (not).
We donot discuss infixes in the paper, because they are handled phrasallyrather than lexically in CTB.34bit?
), from chang ?taste?.
(Chao 1968, Li andThompson 1981).4.2 DifficultyThe morphological characteristics of Chinese createvarious problems for part-of-speech tagging.
First,Chinese suffixes are short and sparse.
Because of theprevalence of compounding and the fact that the mor-phemes are short (1 character long), there are morethan 4000 affixes.
This means that the identity of anaffix is often a sparsely-seen feature for predictingPOS.
Second, Chinese affixes are poor cues to POSbecause they are ambiguous; for example 63% ofChinese suffix tokens in CTB have more than onepossible tag, while only 31% of English suffix tokensin WSJ have more than one tag.
Most English suf-fixes are derivational and inflectional suffixes like   -able, -s and -ed.
Such functional suffixes are used toindicate word classes or syntactic function.
Chinese,however, has no inflectional suffixes and only a fewderivational suffixes and so suffixes may not be asgood a cue for word classes.
Finally, since Chinesehas no derivational morpheme for nominalization, itis difficult to distinguish a nominalization and a verb.These points suggest that morpheme identity, whichis the major feature used in previous research on un-known words in English and German, will be insuffi-cient in Chinese.
This suggests the need for moresophisticated features, which we will introduce be-low.5 ExperimentsWe evaluate our tagger under several experimentalconditions: after showing the effects of data cleanupwe show basic results based on features found to beuseful by previous research.
Next, we introduce addi-tional morphology-based unknown word features,and finally, we experiment with training data of vari-able sizes and different language varieties.5.1 Data setsTo study the significance of training on differentvarieties of data, we created three training sets: train-ing set I contains data only from one variety, trainingset II contains data from 3 varieties, and is similar intotal size to training set I.
Training set III also con-tains data from 3 varieties and has twice much dataas training set I.
To facilitate comparison of perform-ance both between and within Mandarin varieties,both the devset and the test set we created are com-posed of three varieties of data.
The XH test data weselected was identical to the test set used in previousparsing research by Bikel and Chiang (2000).
For theremaining data, we included HKSAR and SM datathat is similar in size to the XH test set.
Table 5 de-tails characteristics of the data sets.Table 5 Data set splits used.
The unknown word tokens arewith respect to Training I.Data set Sect'ns Token Un-knownTraining I 26-270, 600-931 213986 -Training II 600-931, 500-527,1001-1039204701 -Training III 001-270, 301-527,590-593, 600-1039,1043-1151485321 -Devset  23839 2849XH 001-025 7844 381HKSAR 500-527 8202 1168SM 590-593, 1001-1002 7793 1300Test set  23522 2957XH 271-300 8008 358HKSAR 528-554 7153 1020SM 594-596, 1040-1042 8361 15795.2 The modelOur model builds on research into loglinear modelsby Ng and Low (2004), Toutanova et al, (2003) andRatnaparkhi (1996).
The first research uses inde-pendent maximum entropy classifiers, with a se-quence model imposing categorical valid tagsequence constraints.
The latter two use maximumentropy Markov models (MEMM) (McCallum et al,2000), that use log-linear  models to obtain the prob-abilities of a state transition given an observation andthe previous state, as illustrated in Figure 1 (a).Figure 1 Graphical representation of transition probabilitycalculation used in maximum entropy Markov models.
(a)The previous state and the current word are used to calcu-late the transition probabilities for the next state transition.
(b) Same as (a), but when model is run right to left.Using left-to-right transition probabilities, as in Fig-ure 1 (a), the equation for the MEMM can be for-mally stated as the following, where by di representsthe set of features the transition probabilities are con-ditioned on:( ) ( )iii d|tPwt,P ?=Maximum entropy is used to calculate the probabilityP(ti| di) using the equation below.
Here, fj(ti,di) repre-sents a feature derived from the available contextualinformation (e.g.
current word, previous tag, nextword, etc.
)TiTi-1WiTiWi(a) (b)Ti+135( )( )( )?
??
?=Tt'ijiiji)d,t'exp()d,t(expd|tP i ffjjjj?
?We also used Gaussian prior to prevent overfitting.This technique allows us to utilize a large number oflexical and MEMM state sequence based featuresand also provides an intuitive framework for the useof morphological features generated from unknownword models.5.3 Data cleanupBefore investigating the effect of our new features,we show the effects of data cleanup.
Table 6 illus-trates the .46 (absolute) performance gain obtainedby cleaning character encoding errors and normaliz-ing half width to full width.We also clustered punctuation symbols, since train-ing set I has too many (36) variety of punctuations,compared to 9 in WSJ.
We clustered punctuations,for example grouping ???
and ???
together.
Thismapping renders an overall improvement of .08%.All models in the following sections are then trainedon font-normalized and punctuation clustered data.Table 6 Improvement of tagging accuracy after datacleanup.
The features used by all of the models are theidentity of the two previous words, the current word andthe two following word.
No features based on the sequenceof tags were used.Models Token A8 % ?
Token A% Unk A %2Rw+2Lw 87.11 - 47.03+Cleanup 87.57 0.46 48.54+PU 87.65 0.08 49.265.4 Sequence featuresWe examined several tag sequence features fromboth left and right side of the current word.
We usethe term lexical features to refer to features derivedfrom the identity of a word, and tag sequence fea-tures refer to features derived from the tags of sur-rounding words.These features have been shown to be useful in pre-vious research on English (Toutanova et al 2003,Brants 2000, Thede and Harper 1999)The models9 in Table 7 list the different tag sequencefeatures used; they also use the same lexical featuresfrom the model 2Rw+2Lw shown in Table 6.
The ta-ble shows that Model Lt+LLt conditioning on theprevious tag and the conjunction of the two previous8 We abbreviate accuracy as ?A?.9 Except where otherwise stated, during training, a count cutoff of3 is applied to all features found in the training set.
If a featureoccurs fewer than 3 times, it is simply removed from the trainingdata.
All models are trained on training set I and evaluated on thedevset.tags yields 88.27%.
As such, using the sequence fea-tures<ti-1, ti-1ti-2> achieves the current best result.So far, there are no features specifically tailored to-ward unknown words in the model.Table 7 Tagging accuracy of different sequence feature sets.Models Feature sets Token A%Unk A %Rt+RRt+2Rw+2Lw<ti,ti+1>,<ti,ti+1,ti+2>+ lexical features88.10 50.11Lt+LLt+2Rw+2Lw<ti,ti-1>,<ti,ti-1,ti-2>+lexical features88.27 51.165.5 Unknown word modelStarting with Model Lt+LLt from the last section, weintroduce 8 features to improve the performance ofthe tagger on unknown words.
In the sections thatfollow, the model using affixation in conjunctionwith the basic lexical features described above isconsidered to be our baseline.We considered words that occur less than 7 times inthe training set I as rare; if Wi is rare, an unknownword feature is used in place of a feature based onthe actual word?s identity.
During evaluation, un-known word features are used for all words that oc-curred zero to 7 times in the training data.
In addition,when tagging such rare and unknown words, we re-strict the set of possible tags to just those tags thatwere associated with one or more rare words in thetraining data.5.5.1 AffixationOur affixation feature is motivated by similar fea-tures seen in inflectional language models.
(Ng andLow 2004, Toutanova et al 2003, Brants 2000, Rat-naparkhi 1996, Samuelsson 1993).
Since Chinesealso has affixation, it is reasonable to incorporate thisfeature into our model.
For this feature, we use char-acter n-gram prefixes and suffixes for n up to 4.10 Anexample is:???
INFORMATION-BAG "folder"Wi=???
?a folder?FAFFIX={(prefix1,?
), (prefix2,??
), (prefix3,???
), (suffix1,?
), (suffix2,??
), (suffix3,???
)}5.5.2 CTBMorph (CTBM)While affix information can be very informative, weshowed earlier that affixes in Chinese are sparse,short, and ambiguous.
Thus as our first new featurewe used a POS-vector of the set of tags a given affixcould have.
We used the training set to build a mor-pheme/POS dictionary with the possible tags for each10 Despite the short average word length, we found that affixes upto size 4 worked better than affixes only up to size 2, perhapsmainly because they help with long proper nouns and temporalexpressions.36morpheme.
Thus for each prefix and suffix that oc-curs with each CTB tag in the training set I, we asso-ciate a set of binary features corresponding to eachCTB tag.
In the example below the prefix ?
oc-curred in both NN and VV words, but not AD or AS.Prefix1=?, suffix1=?FCTBM-pre= {(AD,0),(AS,0),?(NN,1),?
(VV,1)}FCTBM-suf= {(AD,0),(AS,0),?(NN,1),?
(VV,0)}This model smoothes affix identity and the quantityof active CTBMorph features for a given affix ex-presses the degree of ambiguity associated with thataffix.Figure 2 Pseudo-code for CTBMorphGenCTBMorphFeatureSet (Word W)FeatureSet f;for each t in CTB tag set:for each single-character prefix or suffix k of Wif t.affixList contain k f.appendPair(t, 1)else f.appendPair(t, 0)5.5.3 ASBCOne way to deal with robustness is to add more var-ied training data.
For example the Academic SinicaBalanced Corpus11 contains POS-tagged data from adifferent variety (Taiwanese Mandarin).
But the tagsin this corpus are not easily converted to the CTBtags.
This problem of labeled data from very differ-ent tagsets can happen more generally.
We introducetwo alternative methods for making use of such acorpus.5.5.3.1 ASBCMorph (ASBCM)The ASBCMorph feature set is generated in an iden-tical manner to the CTBMorph feature set, exceptthat rather than generating the morpheme table usingCTB, another corpus is used.
The morpheme table isgenerated from the Academic Sinica Balanced Cor-pus, ASBC (Huang and Chen 1995), a 5 M wordbalanced corpus written in Taiwanese Mandarin.
Asthe CTB annotation guide12 states, the mapping be-tween the tag sets used in the two corpora is non-trivial.
As such, the ASBC data can not be directlyused to augment the training set.
However, using ourASBCMorph feature, we are still able to derive somebenefit out of such an alternative corpus.5.5.3.2 ASBCWord (ASBCW)The ASBCWord feature is identical to theASBCMorph feature, except that instead of using atable of tags that occur with each affix, we use a tableof tags that a word occurs with in the ASBC data.11 The ASBC was originally encoded in traditional Big5 character,and we converted it to simplified GB.12 http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdfThus, a rare word in the CTB training/test set isaugmented with features that correspond to all of thetags that the given word occurred with in the ASBCcorpus, i.e.
in this case, the pos tag of the identicalword in ASBC, ???.Wi=???FASBCWord={(A,0),(Caa,0),(Cab,0)?
(V_2,0)}5.5.4 Verb affixThis feature set contains only two feature values,based on whether a list of verb affixes contains theprefix or suffix of an unknown word.
We use theverb affix list created by the Chinese KnowledgeInformation Processing Group13 at Academia Sinica.It contains 735 frequent verb prefixes and 282 fre-quent verb suffixes.
For  example,Prefix1=?,  suffix1=?Fverb={(verb prefix, 1), (verb suffix, 0)}5.5.5 RadicalsRadicals are the basic building blocks of Chinesecharacters.
There are over 214 radicals, and all Chi-nese characters contain one or more of them.
Some-times radicals reflect the meaning of a character.
Forexample, the characters ?
(monkey), ?
(pig) ?
(kitty cat) all contain the radical ?
that roughlymeans ?something that is an animal?.
For our radicalbased feature, we use the radical map from the Uni-han database.14 The radicals associated with the char-acters in the prefix and suffix of unknown wordswere incorporated into the model as features, for ex-ample:Prefix1=?, suffix1=?FRADICAL={(radical prefix, ?
), (radical suffix,?
)}5.5.6 Named Entity Morpheme (NEM)There is a convention that the suffix of a named en-tity points out the essential meaning of the namedentity.
For example, the suffix bao (newspaper) ap-pears in Chinese translation of ?WSJ?, huaerjierebao.The suffix he (river) is used to identify rivers, forexample in huanghe (yellow river).To take advantage of this fact, we made 3 tables ofnamed entity characters from the Chinese EnglishNamed Entity Lists (CENEL) (Huang 2002).
Theselists consist of a table of Chinese first name charac-ters, a table of Chinese last name characters, and a13 http://turing.iis.sinica.edu.tw/affix/14 Unihan database is downloadable from their website:http://www.unicode.org/charts/unihan.html.37Table 8 Devset performance of the cumulatively rare word models, starting with the baseline.
The second and third columns show thechange in token accuracies and unknown word accuracies from the baseline for each feature introduced cumulatively.
The fourth columnshows the improvement from each feature set.
The six columns on the right side of the table shows the error rate for the 5 most frequenttagsets of unknown words and the rest of unknown words.Error analysis: error rate % of unknown words in each POSFeature (add one in) Token Unk A% ?
Unk A% NN VV NR PU CD OthersLt+LLt 88.27 51.16 - 16.67 57.14 68.25 100.00 16.56 60.86+Suffix 89.70 60.74 9.58 12.50 41.65 44.75 100.00 5.30 37.25+Prefix ?
baseline 90.03 63.66 2.92 10.55 36.62 40.00 100.00 3.97 34.76+CTBM 91.48 76.13 12.47 13.74 31.99 36.00 1.99 0.00 20.58+ASBCM 91.69 77.36 1.23 14.01 28.37 33.75 1.99 0.66 19.57+ASBCW 91.85 78.84 1.48 13.30 23.54 33.50 1.42 0.00 17.93+Verb affix 91.82 79.05 0.21 12.59 24.14 32.75 0.85 0.00 17.76+Radical 91.85 79.09 0.04 11.88 24.75 33.50 0.85 0.00 18.78+NEM 91.91 79.61 0.53 12.23 23.54 31.00 0.85 0.00 18.39+Length?best 91.97 79.86 0.25 12.15 22.94 30.25 0.85 0.00 18.21table of named entity suffixes such as organization,place, and company names in CENEL.
Our namedentity feature set contains 3 features, each corre-sponding to one of the three tables just described.
Togenerate these features, first, we check if the prefixof an unknown is in the Chinese last name table.
Sec-ond, we check if the suffix is in the Chinese firstname table.
Third, we check if the suffix of an un-known word is in the table of named entity suffixes.In Chinese last names are written in front of a firstname, and the whole name is considered as a word,for example:Prefix1=?,  suffix1=?FNEM={(last name, 0), (first name, 0), (NE suffix,1)}5.5.7 Length of a wordThe length of a word can be a useful feature, becausethe majority of words in CTB have less than 3 char-acters.
Words that have more than 3 characters arenormally proper nouns, numbers, and idioms.
There-fore, we incorporate this feature into the system.
Forexample:Wi=??
?, Flength={(length , 3)}5.5.8 EvaluationTable 8 shows our results using the standard maxi-mum entropy forward feature selection algorithm; ateach iteration we add the feature family that mostsignificantly improves the log likelihood of the train-ing data given the model.
We seed the feature spacesearch with the features in Model Lt+LLt.
From thismodel, adding suffix information gives a 9.58% (ab-solute) gain on unknown word tagging.
Subsequentlyadding in prefix makes unknown word accuracy goup to 63.66%.
Our first result is that Chinese affixesare indeed informative for unknown words.
On theright side of Table 8, we can see that this perform-ance gain is derived from better tagging of commonnouns, verbs, proper nouns, numbers and others.
Be-cause earlier work in many languages including Chi-nese uses these simple prefix and suffix features(Brants 2000, Ng and Low 2004) we consider thisperformance (63.66% on unknown words) as ourbaseline.Adding in the feature set CTBM gives another12.47% (absolute) improvement on unknown words.With this feature, punctuation shows the largest tag-ging improvement.
The CTBM feature helps to iden-tify punctuation since all other characters have beenseen in different morpheme table made from thetraining set.
That is, for a given word the lack ofCTBM features cues that the word is a punctuationmark.
Also, while this feature set generally helps alltagsets, it hurts a bit on nouns.Adding in the ASBC feature sets yields another1.23% and 1.48% (absolute) gains on unknownwords.
These two feature sets generally improve per-formance on all tagsets.
Including the verb affix fea-ture helps with common nouns and proper nouns, buthurts the performance on verbs.
Overall, it yields0.21% gain on unknown words.
Finally, adding theradical feature helps the most on nouns, while subse-quently adding in the name entity morphemes help toreduce the error on proper nouns by 2.50%.
Finally,adding in feature length renders a 0.25% gain onunknown words.
Commutatively, applying the fea-ture sets results in an overall accuracy of 91.97% andan unknown word accuracy of 79.86%.5.6   Experiments with the training sets ofvariable sizes and varietiesIn this section, we compare our best model with thebaseline model using different corpora size and lan-guage varieties in the training set.
All the evaluationsare reported on the test set, which has roughly equalamounts of data from XH, HKSAR, and SM.The left column of Table 9 shows that when we traina model only on a single language variety and test ona mixed variety data, our unknown word accuracy is79.50%, which is 18.48% (absolute) better than thebaseline.
The middle column shows when the train-ing set is composed of different varieties and hencelooks like the test set, performance of both the base-line and our best model improves.38Table 9 Comparison of the baseline and our best model.Using different training sets to evaluate on the test set.
(McNemar?s Test  p <.001)Training  I Training  II Training IIIToken Unk Token Unk  Token UnkBase-line89.17 61.02 92.54 74.78 93.51 81.11Best 91.34 79.50 93.00 81.62 93.74 86.33The right column shows the effect of doubling thetraining set size, using mixed varieties.
As expected,using more data benefits both models.These results show that having training data fromdifferent varieties is better than having data from onesource.
But crucially, our morphological-based fea-tures improve the tagging performance on unknownwords even when the training set includes some datathat resembles the test set.How good are our best numbers, i.e.
93.7% on POStagging in CTB 5.0?
Unfortunately, there are noclean direct comparisons in the literature.
The closestresult in the literature is Xue et al (2002), who re-train the Ratnaparkhi (1996) tagger and reach accu-racies of 93% using CTB-I.
However CTB-I containsonly XH data and furthermore the data split is nolonger known for this experiment (Xue p.c.)
so acomparison is not informative.
However, our per-formance on tagging when trained on Training I andtested on just the XH part of the test set is 94.44%,which might be a more relevant comparison to Xueet al (2002).6 ConclusionPrevious research in part-of-speech tagging has re-sulted in taggers that perform well when the trainingset and test set are both drawn from the same corpus.Unfortunately, for many potential real world applica-tions, such an arrangement is just not possible.Our results show that using sophisticated morpho-logical features can help solve this robustness prob-lem.
These features would presumably also beapplicable to other languages and NLP tasks thatcould benefit from the use of morphological informa-tionBesides these tagging results, our research providesvaluable analytic results on understanding the natureof unknown words cross-linguistically.
Our resultsthat unknown words in Chinese are not proper nounslike in English, but rather common nouns and verbs,suggest a similarity to German.
We suggest this isbecause both German and Chinese, despite their hugedifferences in genetic, area, and other typologicalcharacteristics, tend to form unknown words througha similar word formation rule, compounding.7 AcknowledgementThanks to Kristina Toutanova and Galen Andrew fortheir generous help and to the anonymous reviewers.This work was partially funded by ARDAAQUAINT and by NSF award IIS-0325646.8 ReferencesBikel, Daniel and David Chiang.
2000.
Two statisti-cal parsing models applied to the Chinese Tree-bank.
In CLP 2.Brants, Thorsten.
2000.
TnT: a statistical part-of-speech tagger.
In ANLP 6.Brants, Thorsten Wojciech Skut, Hans Uszkoreit.1999.
Syntactic Annotation of a German Newspa-per Corpus In: Anne Abeill?
: ATALA sur le CorpusAnnot?s pour la Syntaxe Treebanks.Chao, Yuen Ren.
1968.
A Grammar of Spoken Chi-nese.
Berkeley: University of California Press.Huang, Chu-ren.
and Keh-Jiann Chen.
1995.
Aca-demic Sinica Balanced Corpus.
Technical Report95-02/98-04.
Academic Sinica.Huang, Shudong.
2002.
Chinese <-> English NameEntity Lists Version 1.0 beta.
Catalog number:LDC2003E01.Li, Charles and Sandra A Thompson.
1981.
Manda-rin Chinese: A Functional Reference Grammar.Berkeley: University of California Press.McCallum, Andrew, Dayne Freitag, FernandoPereira.
2000.
Maximum Entropy Markov Modelsfor Information Extraction and Segmentation.
InICML 17.Marcus, Mitchel, Beatrice Santorini and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
In Compu-tational Linguistics, 19.Ng, Hwee Tou and Jin Kiat Low.
2004.
Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-Once?Word-Based or Character-Based?
In EMNLP 9.Martha Palmer, Fu-Dong Chiou, Nianwen Xue,Tsan-Kuang Lee.
2005.
Chinese Treebank 5.0.Catalog number: LDC2005T01.Ratnaparkhi, Adwait.
1996.
A maximum entropymodel forpart-of-speech tagging.
In EMNLP 1.Thede, Scott and Mary P. Harper.
1999.
Second-order hidden Markov model for part-of-speechtagging.
In ACL 37.Toutanova, Kristina, Dan Klein, Christopher Man-ning, and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Net-work.
In HLT-NAACL 2003.Samuelsson, Christer.
1993.
Morphological taggingbased entirely on bayesian inference.
In NCCL 9.Xue, Nianwen, Fu-dong Chiou and Martha Palmer.2002.
Building a large-scale annotated Chinesecorpus.
In COLING.39
