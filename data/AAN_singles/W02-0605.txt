Using eigenvectors of the bigram graphto infer morpheme identityMikhail Belkin     John GoldsmithDepartment of Mathematics Department of LinguisticsUniversity of ChicagoChicago IL 60637misha@math.uchicago.edu  ja-goldsmith@uchicago.eduAbstractThis paper describes the results ofsome experiments exploring statisticalmethods to infer syntactic categoriesfrom a raw corpus in an unsupervisedfashion.
It shares certain points incommon with Brown et at (1992) andwork that has grown out of that: itemploys statistical techniques toderive categories based on whatwords occur adjacent to a given word.However, we use an eigenvectordecomposition of a nearest-neighborgraph to produce a two-dimensionalrendering of the words of a corpus inwhich words of the same syntacticcategory tend to form clusters andneighborhoods.
We exploit thistechnique for extending the value ofautomatic learning of morphology.
Inparticular, we look at the suffixesderived from a corpus byunsupervised learning of morphology,and we ask which of these suffixeshave a consistent syntactic function(e.g., in English, -ed is primarily amark of verbal past tense, does but ?smarks both noun plurals and 3rdperson present on verbs).1 IntroductionThis paper describes some results of our effortsto develop statistical techniques forunsupervised learning of syntactic word-behavior, with two specific goals: (1) thedevelopment of visualization tools displayingsyntactic behavior of words, and (2) thedevelopment of quantitative techniques to testwhether a given candidate set of words acts in asyntactically uniform way, in a given corpus.1In practical terms, this means the developmentof computational techniques which accept acorpus in an unknown language as input, andproduce as output a two-dimensional image,with each word identified as a point on theimage, in such a fashion that words with similarsyntactic behavior will be placed near to eachother on the image.We approach the problem in two stages: first,a nearest-neighbor analysis, in which a graph isconstructed which links words whosedistribution is similar, and second, what wemight call a planar projection of this graph ontoR2, that is to say, a two-dimensional region,which is maximally faithful to the relationsexpressed by the nearest-neighbor graph.2 MethodThe construction of the nearest-neighbor graphis a process which allows for many linguisticand practical choices.
Some of these we haveexperimented with, and others we have not,simply using parameter values that seemed to usto be reasonable.
Our goal is to develop a graphin which vertices represent words, and edgesrepresent pairs of words whose distribution in acorpus is similar.
We then develop arepresentation of the graph by a symmetricmatrix, and compute a small number of theeigenvectors of the normalized laplacian for1We are grateful to Yali Amit for drawing ourattention to Shi and Malik 1997, to Partha Niyogi forhelpful comments throughout the development of thismaterial, and to Jessie Pinkham for suggestions on anearlier draft of this paper.July 2002, pp.
41-47.
Association for Computational Linguistics.ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia,Morphological and Phonological Learning: Proceedings of the 6th Workshop of thewhich the eigenvalues are smallest.
Theseeigenvectors provide us with the coordinatesnecessary for our desired planar representation,as explained in section 2.2.2.1 Nearest-neighbor graphconstruction.We begin with the reasonable workingassumption that to determine the syntacticcategory of a given word w, it is the set of wordswhich appears immediate before w, and the setof words that appears immediately after w, thatgives the best immediate evidence of a word?ssyntactic behavior.
In a natural sense, under thatassumption, an explicit description of thebehavior of a word w in a corpus is a sparsevector L = [l1, l2, ?, lV], of length V (where ?V?is the number of words in the vocabulary of thecorpus), indicating by li how often each word vioccurs immediately to the left of w, and also ansimilar vector R, also of length V, indicatinghow often each word occurs immediately to theright of w. Paraphrasing this, we may view thesyntactic behavior of a word in a corpus as beingexpressed by its location in a space of 2Vdimensions, or a vector from the origin to thislocation; this space has a natural decompositioninto two spaces, called Left and Right, each ofdimension V.Needless to say, such a representation is notdirectly illuminating -- nor does it provide a wayto cogently present similarities or clusteringsamong words.
We now construct a symmetricalgraph (?LeftGraph?
), whose vertices are the Kmost frequent words in the corpus.
(We haveexperimented with K = 500 and K = 1000).
Foreach word w, we compute the cosine of theangle between the vector w and the K-1 otherwords wi: |||| iiwwww ?, and use this figure to selectthe N words closest to w. We have experimentedwith N = 5,10,20 and 50.
We insert an edge (vi,vj) in LeftGraph if vi is one of the N wordsclosest to vj or vj is one of the N words closest tovi.. We follow the same construction forRightGraph in the parallel fashion.
In much ofthe discussion that follows, the reader may takewhatever we say about LeftGraph to holdequally true of RightGraph when not otherwisestated.2.2 Projection of nearest-neighborgraph by spectral decompositionIn the canonical matrix representation of a(unweighted) graph, an entry M(i,j), with idistinct from j, is 1 if the graph includes an edge(i,j) and 0 otherwise.
All diagonal elements arezero.
The degree of a vertex of a graph is thenumber of edges adjacent to it; the degree of themthvertex, d(vm)is thus the sum of the values inthe mth row of M. If we define D as the diagonalmatrix whose entry D(m,m) is d(vm), the degreeof vm, then the laplacian of the graph is definedas D ?
M. The normalized laplacian L is definedas D?
( D ?
M ) D?.
The effect of normalizationon the laplacian is to divide the weight of anentry M(i,j) that represents the edge between viand vj by )()(1ji vdvd, and to set the values ofthe diagonal elements to 1.2The laplacian is a symmetric matrix which isknown to be positive semi-definite (Chung1997).
Therefore all the eigenvalues of thelaplacian are non-negative.
We return to thespace of our observations by premultiplying theeigenvectors by D?.
We will refer to theseeigenvectors derived from LeftGraph (pre-multiplied by D?)
as {L0, L1, ?}
and thosederived from RightGraph as {R0, R1, ?
}.Now, L0 (and R0) are trivial (they merelyexpress the frequency of the words in thecorpus), but L1 and L2 provide us with veryuseful information.
They each consist of avector with one coordinate for each word amongthe K most frequent words in the corpus, andthus can be conceived of as a 1-dimensionalrepresentation of the vocabulary.
In particular,L1 is the 1-dimensional representation thatoptimally preserves the notion of localitydescribed by the graph we have just constructed,and the choice of the top N eigenvectorsprovides a representation which optimallypreserves the graph-locality in N-space.
Byvirtue of being eigenvectors in the sameeigenvector decomposition, L1 and L2 areorthogonal, but subject to that limitation, theprojection to R2 using the coordinates of L1 andL2 is the 2-dimensional representation that best2Our attention was drawn to the relevance of thenormalized laplacian by Shi and Malik 1997, whoexplore a problem in the domain of vision.
We areindebted to Chung 1997 on spectral graph theory.preserves the locality described by the graph inquestion (Chung 1997, Belkin and Niyogi2002).Thus, to the extent that the syntactic behaviorof a word can be characterized by the set of itsimmediate right- and left-hand neighbors (whichis, to be sure, a great simplification of syntacticreality), using the lowest-valued eigenvectorsprovides a good graphical representation ofwords, in the sense that words with similar left-hand neighbors will be close together in therepresentation derived from the LeftGraph (andsimilarly for RightGraph).2.3 Choice of graphsWe explore below two types of projection to 2dimensions: plotting the 1st and 2nd eigenvectorsof LeftGraph (and RightGraph), and plotting the1st eigenvectors of LeftGraph and RightGraphagainst each other.
In all of these cases, we havebuilt a graph using the 20 nearest neighbors.
Infuture work, we would like to look at varyingthe number of nearest neighbors that are linkedto a given word.
From manual inspection, onecan see that in all cases, the nearest two or threewords are very similar; but the depth of thenearest neighbor list that reflects words of trulysimilar behavior is, roughly, inverselyproportional to the frequency of the word.
Thisis not surprising, in the sense that higherfrequency words tend to be grammatical words,and for such words there are fewer members ofthe same category.2.4  EnglishFigure 1 illustrates the results of plotting the 1stand 2nd eigenvectors of LeftGraph based on thefirst 1,000,000 words of the Brown corpus, andusing the 1,000 most frequent words andconstructing a graph based on the 20 nearestneighbors.
Figure 2 illustrates the results derivedfrom the first two eigenvectors of RightGraph.Figures 1 and 2 suggest natural clusterings,based both on density and on the extreme valuesof the coordinates.
In Figure 1 (LeftGraph), thebottom corner consists primarily of non-finiteverbs (be, do, make); the left corner of finiteverbs (was, had, has); the right corner primarilyof nouns (world, way, system); while the topshows little homogeneity, though it includes theprepositions.
See Appendix 1 for details; thewords given in the appendix are a complete listof the words in a neighborhood that includes theextreme tip of the representation.
As we moveaway from the extremes, in some cases we finda less homogeneous distribution of categories,while in others we find local pockets oflinguistic homogeneity: for example, regionscontaining names of cities, others containingnames of countries or languages.Figure 1 English based on left-neighborsFigure 2 English based on right-neighborsIn Figure 2, the bottom corner consists ofadjectives (social, national, white), the leftcorner of words that often are followed by of(most, number, kind, secretary), the right cornerprimarily by prepositions (of, in for, on by) andthe top corner of words that often are followedby to (going, wants, according), (See Appendix2 for details).2.5 FrenchFigure 3 illustrates the results of plotting the 1stand 2nd eigenvectors of LeftGraph based on thefirst 1,000,000 words of a French encyclopedia,using the 1,000 most frequent words andconstructing a graph based on the 20 nearestneighbors.The bottom left tip of the figure consistsentirely of feminine nouns (guerre, population,fin), the right tip of plural nouns (ann?es, ?tats-unis, r?gions), the top tip of finite verbs (est, fut,a, avait) plus se and y.
A bit under the top tipone finds two sharp-tipped clusters; the one onthe left consists of masculine nouns (pays, sud,monde).
Other internal clusters, not surprisingly,are composed of words which, with highfrequency, are preceded by a specific pre-position (e.g., preceded by ?
: peu, l?est, Paris;by en: particulier, effet, and feminine names ofgeographical areas such as France).Figure 4 illustrates plotting the 1steigenvector of LeftGraph against the 1steigenvector of RightGraph.
We find a striking?striped?
effect which is due to themasculine/feminine gender system of French.There are three stripes that stand out at the topof the figure.
The one on the extreme leftconsists of singular feminine nouns, the one toits right, but left of center, consists of singularmasculine nouns, and the one on the extremeright consists of plural nouns of both genders.The lowest region of the graph, somewhat left ofcenter, contains grammatical morphemes.
At thevery bottom are found relative andsubordinating conjunctions (o?, car, lequel,laquel, lesquelles, lesquels, quand, si), and justabove them are the prepositions: selon, durant,malgr?, pendant, apr?s, entre, jusqu?
?, contre,sur, etc.
)We find it striking that the gender system ofFrench has such a pervasive impact upon theglobal form of the 1st eigenvector map as inFigure 4, and we plan to run further experimentswith other language with gender systems to seethe extent to which this result obtainsconsistently.Figure 3 French based on left-neighborsFigure 4 French 1st eigenvector of Left and Right3 Identifying syntactic behavior ofautomatically identified suffixesInteresting as they are, the representations wehave seen are not capable of specifyingmembership in grammatical categories in anabsolute sense.
In this section, we explore theapplication of this representation to a text whichhas been morphologically analyzed by alanguage-neutral morphological analyzer.
Forthis purpose, we employ the algorithm describedin Goldsmith (2001), which takes an unanalyzedcorpus and provides an analysis of the wordsinto stems and suffixes.
What is useful aboutthat algorithm for our purposes is that it sharesthe same commitment to analysis based only ona raw (untreated) natural text, and neither hand-coding nor prior linguistic knowledge.The algorithm in Goldsmith (2001) linkseach stem in the corpus to the set of suffixes(called its signature) with which it appears inthe corpus.
Thus the stem jump might appearwith the three suffixes ed-ing-s in a givencorpus.But a morphological analyzer alone is notcapable of determining whether the ?ed thatappears in the signature ed-ing-s is the same ?edsuffix that appears in the signature ed-ing (forexample), or whether the suffix ?s in ed-ing-s isthe same suffix that appears in the signatureNULL-s-?s (this last signature is the oneassociated with the stem boy in a corpuscontaining the words boy-boys-boy?s).
Amoment?s reflection shows that the suffix ?ed isindeed the same verbal past tense suffix in bothcases, but the suffix ?s is different: in the firstcase, it is a verbal suffix, while in the second itis a noun suffix.In general, morphological information alonewill not be able to settle these questions, andthus automatic morphology alone will not beable to determine which signatures should be?collapsed?
(that is, ed-ing-s should be viewedas a special sub-case of the signature NULL-ed-ing-s, but NULL-s is not to be treated as aspecial case of NULL-ed-ing-s).We therefore have asked whether therudimentary syntactic analysis described in thepresent paper could provide the informationneeded for the automatic morphologicalanalyzer.The answer appears to be that if a suffix hasan unambiguous syntactic function, then thatsuffix?s identity can be detected automaticallyeven when it appears in several differentsignatures.
As we will see momentarily, theclear example of this is English -ed, which is(almost entirely) a verbal suffix.
When a suffixis not syntactically homogeneous, then thewords in which that suffix appears are scatteredover a much larger region, and this differenceappears to be quite sharply measurable.3.1 The case of  the verbal suffix ?edIn the automatic morphological analysis of thefirst 1,000,000 words of the Brown corpus thatwe produced, there are 26 signatures thatcontain the suffix ?ed: NULL.ed.s, e.ed.ing,NULL.ed.er.es.ing, and 23 others of similar sort.We calculated a nearest neighbor graph asdescribed above, with a slight variation.
Weconsidered the 1000 most frequent words to beatomic and unanalyzed morphologically, andthen of the remaining words, we automaticallyreplaced each stem with its correspondingsignature.
Thus as jumped is analyzed asjump+ed, and jump is assigned the signatureNULL.ed.er.s.ing (based on the actual forms ofthe stem found in the corpus), the word jumpedis replaced in the bigram calculations by thepseudo-word NULL.ed.er.s.ing_ed: the stemjump is replaced by its signature, and the actualsuffix -ed remains unchanged, but is separatedfrom its stem by an underscore _.
Thus all wordsending in ?ed whose stems show the samemorphological variations are treated as a singleelement, from the point of view of our presentsyntactic analysis.We hoped, therefore, that these 26 signatureswith ?ed appended to them would appear veryclose to each other in our 2-dimensionalrepresentation, and this was exactly what wefound.To quantify this result, we calculated thecoordinates of these 26 signatures in thefollowing way.
We normalize coordinates sothat the lowest x-coordinate value is 0.0 and thehighest is 1.0; likewise for the y-coordinates.Using these natural units, then, on the LeftGraphdata, the average distance from each of thesignatures to the center of these 26 points is0.050.
While we do not have at present acriterion to evaluate the closeness of thisclustering, this appears to us at this point to bewell within the range that an eventual criterionwill establish.
(A distance of 0.05 by thismeasure is a distance equal to 5% along eitherone of the axes, a fairly small distance.)
On theRightGraph data, the average distance is 0.054.3.2 The cases of ?s and ?ingBy contrast, when we look at the range of the 19signatures that contain the suffix ?s, the averagedistance to mean in the LeftGraph is 0.265, andin the RightGraph, 0.145; these points are muchmore widely scattered.
We interpret this asbeing due to the fact that ?s serves at least twofunctions: it marks the 3rd person present formof the verb, as well as the nominal plural.Similarly, the suffix ?ing marks both theverbal progressive form as well as thegerundive, used both as an adjective and as anoun, and we expect a scattering of these formsas a result.
We find an average to mean of 0.096in the LeftGraph, and of 0.143 in theRightGraph.By way of even greater contrast, we cancalculate the scatter of the NULL suffix, whichis identified in all stems that appear without asuffix (e.g., the verb play, the noun boy).
This?suffix?
has an average distance to mean of0.312 in the LeftGraph, and 0.192 in theRightGraph.
This is the scatter we would expectof a group of words that have no linguisticcoherence.3.3 Additional suffixes testedSuffix ?ly occurs with five signatures, and anaverage distance to mean of 0.032 in LeftGraph,and 0.100 in RightGraph.3 The suffix ?s occursin only two signatures, but their averagedistance to mean is 0.000 [sic] in LeftGraph, and0.012 in RightGraph.
Similarly, the suffix ?alappears in two signatures (NULL.al.s andNULL.al), and their average distance to mean is0.002 in LeftGraph, and also 0.002 inRightGraph.
The suffix ?ate appears in threesignatures, with an average distance to mean of0.069 in LeftGraph, and 0.080 in RightGraph.The suffix ?ment appears in two signatures, withan average distance to mean of 0.012 inLeftGraph, and 0.009 in RightGraph.3.4 French suffixes ?ait, -er, -a, -ant, -eWe performed the same calculation for theFrench suffix ?ait as for the English suffixesdiscussed above.
?ait is the highest frequency3rd person singular imperfect verbal suffix, andas such is one of the most common verbalsuffixes, and it has no other syntactic functions.It appears in seven signatures composed ofverbal suffixes, and they cluster well in thespaces of both LeftGraph and RightGraph, withan average distance to mean of 0.068 in theLeftGraph, and 0.034 in the RightGraph.The French suffix ?er is by far the mostfrequent infinitival marker, and it appears in 14signatures, with an average distance to mean of0.055 in LeftGraph, and 0.071 in RightGraph.The 3rd singular simple past suffix ?a appearsin 11 signatures, and has an average distance tomean of 0.023 in LeftGraph, and 0.029 inRightGraph.The present participle verbal suffix ?antappears in 10 suffixes, and has an average3This latter figure deserves a bit more scrutiny; oneof the five is an outlier: if we restricted our attentionto four of them, the average distance to mean is0.014.distance to mean of 0.063 in LeftGraph, and of0.088 in RightGraph.On the other hand, the suffix ?e appears asthe last suffix in a syntactically heterogeneousset of words: nouns, verbs, and adjectives.
It hasan average distance to mean of 0.290 inLeftGraph and of 0.130 in RightGraph.
This isas we expect: it is syntactically heterogeneous,and therefore shows a large average distance tomean.3.5 SummaryHere are the average distances to mean for thecases where we expect syntactic coherence andthe cases where we do not expect syntacticcoherence.
Our hypothesis is that the numberswill be small for the suffixes where we expectcoherence, and large for those where we do notexpect coherence, and this hypothesis is stronglyborne out.
We note empirically that we maytake an average value of the two columns of .10as a reasonable cut-off point.LeftGraph RightGraphExpect coherence:ed 0.050 0.054-ly 0.032 0.100?s  0.000 0.012-al 0.002 0.002-ate 0.069 0.080-ment 0.012 0.009-ait 0.068 0.034-er 0.055 0.071-a 0.023 0.029-ant 0.063 0.088LeftGraph RightGraphExpect little/no coherence:-s 0.265 0.145-ing  0.096 0.143NULL 0.312 0.192-e 0.290 0.130Figure 5 Average distance to mean of suffixes4 ConclusionsWe have presented a simple yet mathematicallysound method for representing the similarity oflocal syntactic behavior of words in a largecorpus, and suggested one practical application.We have by no means exhausted the possibilitiesof this treatment.
For example, it seems veryreasonable to adjust the number of nearestneighbors permitted in the graph based on word-frequency: the higher the frequency, the fewerthe number of nearest neighbors would bepermitted in the graph.
We leave this and otherquestions for future research.This method does not appear strong enoughat present to establish syntactic categories withsharp boundaries, but it is strong enough todetermine with some reliability whether  sets ofwords proposed by other, independent heuristics(such as presence of suffixes determined byunsupervised learning of morphology) aresyntactically homogenous.The reader can download the files discussed inthis paper and a graphical viewer fromhttp://humanities.uchicago.edu/faculty/goldsmith/eigenvectors/.Appendix 1Typical examples from corners of Figure 1.Bottom:be do me make seeget take go say putfind give provide keep runtell leave pay hold liveLeft:was had has would saidcould did might went thoughttold took asked knew feltbegan saw gave looked becameRight:world way same united rightsystem city case church problemcompany past field cost departmentuniversity rate center door surfaceTop:and to in that forhe as with on byat or from but Ithey we there you whoAppendix 2Typical examples from corners of Figure 2.Bottom:social national white local  politicalpersonal private strong medical finalblack French technical nuclear britishhealth husband blueLeft:most number kind full typesecretary amount front instead membersort series rest types  pieceimage lackRight:of in for on byat from into after throughunder since  during against amongwithin along across including nearTop:going want seems seemed ablewanted likely difficult according duetried decided trying related tryReferencesMikhail Belkin and Partha Niyogi.
2002.Laplacian Eigenmaps for DimensionalityReduction and Data Representation.
TR-2002-01.
Available at http://www.cs.uchicago.edu/research/publications/techreports/TR-2002-02Peter F. Brown, Vincent J. Della Pietra, PeterV.
deSouza, Jenifer C. Lai, Robert L. Mercer.1992.
Class-based n-gram models of naturallanguage.
Computational Linguistics 18(4): 467?479.Chung, Fan R.K. 1997.
Spectral GraphTheory.
Regional Conf.
Series in Math., Vol.
92,Amer.
Math.
Soc., Providence, RI.Goldsmith, J.
2001.
Unsupervised learning ofthe morphology of a natural language.Computational Linguistics 27(2): 153-198.Shi, Jianbo and Jitendra Malik.
2000.Normalized Cuts and Image Segmentation.IEEE Transactions on Pattern Analysis andMachine Intelligence 22(8): 888-905.
