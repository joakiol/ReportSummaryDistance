Proceedings of the Workshop on BioNLP, pages 80?88,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsTX Task:Automatic Detection of Focus Organisms in Biomedical PublicationsThomas Kappeler, Kaarel Kaljurand, Fabio Rinaldi?Institute of Computational Linguistics, University of Zurichkappeler@bluewin.ch, kalju@cl.uzh.ch, rinaldi@cl.uzh.chAbstractIn biomedical information extraction (IE), acentral problem is the disambiguation of am-biguous names for domain specific entities,such as proteins, genes, etc.
One importantdimension of ambiguity is the organism towhich the entities belong: in order to disam-biguate an ambiguous entity name (e.g.
a pro-tein), it is often necessary to identify the spe-cific organism to which it refers.In this paper we present an approach to thedetection and disambiguation of the focus or-ganism(s), i.e.
the organism(s) which are thesubject of the research described in scientificpapers, which can then be used for the disam-biguation of other entities.The results are evaluated against a gold stan-dard derived from IntAct annotations.
Theevaluation suggests that the results may al-ready be useful within a curation environmentand are certainly a baseline for more complexapproaches.1 IntroductionThe task of identifying the organisms which are in-volved in research described in biomedical articlesis extremely important for the field of biomedical in-formation extraction (IE), both in itself and in con-nection with other tasks.
In itself, because the con-cept of biological taxonomy is basic for every re-searcher: organisms and their taxonomic classifica-tion can be used very effectively in various contexts,for example to restrict searches, a classical infor-mation retrieval (IR) task.
At the same time, anybiomedical text mining system would be incompletewithout the possibility to use organisms as concepts,e.g.
in finding (statistical) associations, which can?Corresponding authorthen be used to form hypotheses about causal rela-tions.The necessity of identifying organisms is evenmore evident as part of other important entity recog-nition tasks in biomedical information extraction(IE), e.g.
identification and disambiguation of pro-teins mentioned in the literature.
For example,within the PPI task (identification of protein-proteininteractions) of Biocreative II (Krallinger et al,2008), the identification of the focus organism wasseen by many participants as an essential subtask inorder to properly disambiguate protein names.
Pro-tein interactions are fundamental for most biologicalprocesses, therefore they are at the focus of a hugeand fast growing number of biomedical papers.
Asthese cannot all be read or even inspected by the re-searchers, databases such as IntAct (Kerrien et al,2006) or MINT (Zanzoni et al, 2002) try to create areliable catalogue of experimentally detected inter-actions by extracting them ?manually?
from the lit-erature through the usage of human experts.
This isknown as ?curation?, a costly and time-consumingprocess, which could be speeded up much by effi-cient, robust and precise extraction tools.One of the most important obstacles for efficientautomatic identification of proteins is the extremeambiguity of the commonly used protein names inthe literature.
The fragmentation of the biomedicalscientific community into lots of extremely special-ized sub-communities seems to be the main reasonfor this ambiguity.
In most cases, the ambiguity isbetween homologous proteins of different species.Any human reader belonging to the sub-communityconcerned can, in general, disambiguate an ambigu-ous protein name like ?goat?
(which can refer toproteins found in four different organisms: human,rat, mouse and zebrafish), as the species is obviousto them from the context.
However, this ambiguity80remains problematic for IE systems (and even forcurators in some cases) and needs to be solved be-fore more complex tasks, such as protein interactiondetection, can be effectively tackled (Rinaldi et al,2008).Our goal is to be able to identify automaticallythe focus organisms, i.e.
the organisms that arementioned in the paper as the hosts of the exper-iments described, or as the sources of the entitiesinvolved.
This information can then be used for tag-ging papers for more efficient organism-based infor-mation retrieval, or, more commonly, for the dis-ambiguation of other entities mentioned in the samepaper.
Since organism recognition is normally per-formed with reference to a taxonomical organization(of Linnean origin) of all known organisms (in ourcase, the NCBI taxonomy) this task is often referredto as ?TX task?.In the rest of this paper we describe in section 2the resources used and the approach followed in or-der to extract and rank candidate organisms.
In sec-tion 3 we present our results and propose a more finegrained interpretation of the task, which we againevaluate.
Finally in section 4 we compare our ap-proach to previous work and discuss its limitations.2 MethodsOur approach can be described briefly as (1) find allexplicit mentions of organisms either by their scien-tific or ?common?
names; (2) count these mentionsand combine the resulting numbers with a simpleuse of statistics to arrive at a ranked list or a sim-ple set of organisms which can be used, among otherthings, to disambiguate ambiguous protein names inthe article under investigation.2.1 Resources UsedThe first step for this approach was to choose awidely accepted taxonomy which not just includesunambiguous identifiers for all known organisms,but also provides a sufficiently large list of namesfor them.
The taxonomy selected for this was theNCBI Taxonomy1.1Available as archive taxdmp.zip fromftp://ftp.ncbi.nih.gov/pub/taxonomy/.
We worked with aversion downloaded on July 10th 2008.
The file nodes.dmpcontains the taxonomy as a set of 443,299 nodes for the taxaand immediate-dominance-relations between them.
The fileAs most of these organism are unlikely to ever oc-cur in biomedical literature, we decided to restrictour interest to the organisms for which a UniProtorganism mnemonic identifier exists.
UniProt(UniProt Consortium, 2007) is a database containingdetailed information about known proteins, obtainedby a process of curation of the biomedical literature.For every protein, a ?mnemonic?
identifier is de-fined (e.g.
HBA HUMAN for ?Human HemoglobinA?)
which is composed by a shorthand for the pro-tein name and a simple unique identifier for the or-ganism.
Within the UniProt entry for the protein,the organism is also referred to by its NCBI iden-tifier, allowing the construction of a mapping fromthe mnemonic identifiers for the organisms used byUniProt to their equivalent NCBI identifiers.The set of organism that have a UniProtmnemonic identifier (11,444 organisms) probablycovers the near totality of organisms that have beensubject to research in molecular biology.
In theNCBI taxonomy 31,733 names are defined for thatsubset of organisms.
Although several classes ofnames are defined by NCBI, for the purpose ofthis work we distinguish only between ?scientificnames?
and the other classes (pooled together as?common names?
).2As an additional source of information, we usedthe IntAct database of protein interactions3 for twodifferent purposes:?
to derive statistical measures used later by theprogram, most importantly the frequency ofeach focus organism in papers curated by Int-Act (using the IntAct annotations as the sourcesof the ?focus?).?
to derive a gold standard against which our pro-grams could be testedIntAct provides an annotated set of protein in-teractions.
Each interaction is enriched with de-tailed information about the two proteins involvednames.dmp connects one or several names (619,325) of differ-ent nameclasses (such as ?scientific?
or ?common?)
to eachnode.
The nodes (taxa) are referred to by numeric identifiers.2While there are no ambiguous ?scientific names?
in thistaxonomy, there are several ambiguous ?common names?, butonly very few of these occurred in our sample, e.g.
?mink?,?barley?, ?green monkey?, and they are very rare.3Version of May 2008, downloaded fromhttp://www.ebi.ac.uk/intact/site/contents/downloads.jsf81(from which the reference organisms can be recov-ered), and with the identifier of the paper from whichthe interaction was originally derived in the curationprocess.
This allows to build a gold standard by as-sociating each paper to its focus organisms.The sample used in our experiments is a set of 621PubMed-indexed full text articles, dating from 1995to 2007, for which IntAct annotations are available.42.2 First Experiments and NormalizationAs an initial experiment, we performed a simplelexical lookup of the names of the 11,444 organ-isms under consideration.
In previous applicationsof IE techniques for biomedical literature (Kappeleret al, 2008; Rinaldi et al, 2008) we found thatsimple techniques for the generation of variants ofthe known names significantly benefited the recallof the application.
For example, multiword proteinnames can be subject to a number of minor variants,such as the introduction of hyphens or the separationof compound words, which make automatic recog-nition more challenging.
In the case of organismnames, although our initial expectations were sim-ilar, we found the benefit (in terms of additional re-call) of such variants to be extremely limited, possi-bly because names of species are used more consis-tently than the names of proteins or genes.Therefore it was possible to implement a simplerapproach to recognition of organism names, basedon lexical lookup against a database containing allnames of interest, coupled with a simple normaliza-tion step which removes trivial orthographic differ-ences (such as hyphens) between the key word inthe database and the lookup word from the docu-ment (for details see (Kaljurand et al, 2009)).
Theinclusion of other biomedical NE?s (such as pro-tein names, method names, cell line names) in thedatabase together with a strict implementation ofthe ?longest match?
principle leads to better preci-sion by eliminating false positives caused by match-ing organism names with a fragment of a multiwordterm for another entity (such as the method ?yeasttwo-hybrid?
).As mentioned, the names provided by the NCBI4The reason of this particular choice is that the same subsetwas used for experiments related to the automatic detection ofexperimental methods, also using IntAct annotations as a goldstandard, described in (Kappeler et al, 2008).taxonomy have been classified into ?scientificnames?
or ?common names?.
Using only ?scientificnames?
appeared as an effective way to obtain betterprecision, but we soon discovered that precision ofthe common names suffered most by a few very badnames, such as ?Li?, which is a ?common name?
forLIV (Louping ill virus) in the taxonomy, but appearsonly (and very frequently) as Chinese surname in thetexts.
By eliminating about 25 of similar misleading?common names?
the results of this class rose to thesame level as the ?scientific names?, so there wasno reason to exclude the whole class (as that wouldhave harmed recall).Since the bibliography might contain spuriousmentions of other organisms, we automatically re-moved it from the main text.
However, contrary toexpectations, this did not lead to better results forthis task (at least after the elimination of the mislead-ing ?common names?
mentioned above), but wasnot reversed because of its effects on other tasks.
Anintuition from other tasks was to use the abstractsinstead of the full text of the articles, because thatwould tend to exclude accidental mentions of organ-isms leading to false positives.
But a main problemof this approach is that many abstracts do not yieldany organism mentions.
Whenever they do though,their precision is high.
So there is a strong case forgiving the mentions there a higher weight, but obvi-ously the rest of the article plays an important roleas well.
We experimentally found that counting an?abstract mention?
as equivalent to 25 ?fulltext men-tions?
worked best.2.3 Measures Improving RecallAn experiment using all names provided by NCBIand considering all mentions of those names in thefulltext version of each article led to a recall of 83%,leading us to conclude that either the taxonomy doesnot contain all names used, or some organisms aresuggested to the human reader by the context and/orhis anticipations.
The first of these problems wasadressed by adding some generated names to thetermbase, the second by the use of a default.Several possible ways of generating new namesautomatically from the names in the database wereconsidered, but only two were applied successfully,as described below.
One of them was the automaticgeneration of additional names from the nameclass82?scientific name?
(for organisms of species or sub-species level) by the process of replacing the firstword (which would be the genus name in the classi-cal Linnean binomial nomenclature) by its first letterand a dot.
The resulting names, such as ?E.
coli?, arewidely used, but not included in the taxonomy.
Aseemingly large disadvantage of this approach is itspotential for ambiguity: 338 of the resulting namesrefer to more than one organism.
But the test onour sample showed that of these only 4 occurred atall, only 1 more than once: ?C.
elegans?
(potentiallyreferring to the organisms identified in UniProt asCAEEL, CENEL, CESEL and CUNEL) which al-ways stood for CAEEL, i.e.
?Caenorhabditis ele-gans?.
So excluding the other options for ?C.
ele-gans?
eliminated the ambiguity (at least in our sam-ple).
We observed that this type of name is in fre-quent use only for few species and in this case theunabbreviated name is often used first, so the addi-tion of this generated nameclass added little to re-call.The other type of name missing from the taxon-omy is the use of the (Linnean) genus name for avery frequent species, e.g.
?Arabidopsis?
used for?Arabidopsis thaliana?.
Experiments showed thatthis type could not be reliably generated automat-ically from the ?scientific names?, as this name-class includes many names which do not followthe rules of Linnean binomial nomenclature, mostlyvirus names such as ?Human papillomavirus typeme180?
where the first word is generally not agenus name, but a host name.
So the problem of(potentially huge) ambiguity in this type of nameswas not even researched, instead the names of thistype for the most frequent organisms were gener-ated manually and those which improved the resultswere included into the termbase (Saccharomyces,Arabidopsis, Drosophila, Escherichia, Xenopus andSynechocystis).
The addition of this generatednameclass did not add much to recall for the samereason as for the first group: in most cases the un-abbreviated name appears in the paper as well.
To-gether both groups improved recall by about 3.4%.As HUMAN is the most frequent organism inthis context, it was obvious that a default HUMANwould take care of many cases where human readersdisambiguate ambiguous protein names even with-out any explicit mentions of this species.
As thereTable 1: Most frequent organisms in IntAct (derived frominteractor proteins and host organisms)ORG freqHUMAN 0.281YEAST 0.272MOUSE 0.091ARATH 0.056CERAE 0.037RAT 0.033DROME 0.028SCHPO 0.023ECOLX 0.020ECOLI 0.013are no cases (with the current termbase and sample)of articles with no organism mentions in the full text,we chose to have a default triggered by no findingsin the abstract.
Experiments showed that ?
contraryto intuition ?
a weight of the default proportional tothe total number of mentions (just adding a percent-age to HUMAN) would lead to worse results than anabsolute value for the default.52.4 Measures Improving PrecisionThe simple approach of considering every mentionof each organism (after excluding the misleadingcommon names, as described above), leads to a pre-cision of only 27.6%, therefore the list of organismidentifiers obtained in this way has to be consideredas a ?candidates list?
from which a selection has tobe made.Candidates can be of course ranked according tonumber of mentions in each article.
A ranking basedon the mention counts, taking into account the cor-rection factor of 25 for mentions in the abstract (asdescribed in section 2.2), was still far from opti-mal, so we multiplied the mentions with the relativefrequencies of the organisms in a micro-averagedfrequency table (table 1) computed over all of Int-Act (not just our sample, to avoid overfitting) andsmoothed roughly by attributing 1% of the probabil-ity mass to all unseen organisms (over 11,000).
Thisranking did far better than expected and after nor-5 A tentative explanation: In a small paper, the effect of ac-cidental mentions of ?wrong?
organisms is much larger than inbig papers (where the important organisms are mentioned againand again).
This detrimental effect may be counterbalanced bya relatively stronger default.83malizing the whole list to 1, a minimal threshold forthe score could be set up to maximize the f-score byimproving precision at the cost of recall.
The actualvalue of the threshold (currently 0.04) is of coursearbitrary, depending on what measure one wants tomaximize.Another problem to be tackled is that different pa-pers will have different numbers of focus organisms,ranging from one (in about 70% of the cases), to sev-eral hundreds (in a few very infrequent cases).
Itcould be assumed that being able to correctly guessthe number of focus organisms would lead to im-provement in the TX task, as we could pick only asmany candidate organisms (in their ranking order)as the expected number for the paper.
However, anexperiment using the gold standard as an oracle topredict the number of organisms to be returned asa result, instead of using a threshold in the ranking,did not perform much better (recall was about 1.7%higher), so we decided not to spend any energy onexploring ways to predict the number of organismsas the effect would be minimal, even with perfectprediction.Further experiments, such as giving differentweights to mentions of names of different name-classes, did not lead to better results.
Including in-formation about the precision or recall of the namesencountered in our test set (or the organisms pre-dicted by them) in the formula for the weights6 didnot lead to better results either.3 Evaluation and analysis of resultsSo finally the program in its current form considersall organism mentions, as delivered by the termbasesearch, eliminates the problematic common names,counts the mentions for each organism in fulltextand abstracts, multiplies the latter by 25 and addsthem to the fulltext mentions.
In case of no abstractmentions, a default of 28 fulltext mentions is addedto HUMAN (equivalent to about one abstract men-tion).The result for each organism is multiplied by therelative frequency of the organism in IntAct and di-vided by the sum of the results over all organisms to6An idea suggested by its successful use in the detection ofexperimental methods in (Kappeler et al, 2008) and (Rinaldi etal., 2008).Table 2: Most frequent false positives for the best resultswith our sampleORG freqHUMAN 121YEAST 104MOUSE 68ECOLX 18DROME 13ARATH 11RAT 9Table 3: Most frequent false negatives for the best resultswith our sampleORG freqCERAE 73MOUSE 59RAT 40YEAST 21BOVIN 14ECOLI 13ECOLX 13normalize the sum of the values to 1 (100%).
All or-ganisms under the threshold of 0.04 (or 4%) are theneliminated from the list.Our best results (max.
f-score) for the task of find-ing all organisms in the gold standard combining or-ganisms of interacting proteins and host organismsare: precision: 0.742; recall: 0.738; f-score: 0.740.An analysis of the most frequent false positivesis reported in table 2.
The ranking is more or lessidentical with the frequency table (table 1), which iswhat we would expect.
Manual inspection of someof the papers causing these false positives gave thefollowing results:?
Some names of experimental methods contain-ing organism names (which could avoid falsepositives if recognized as methods) were notyet included in the termbase.?
Some organisms (or their proteins respectively)are discussed in the paper, but not as results ofthe authors own experiments, so they do not ap-pear in the gold standard.
Obviously the cura-tors consider only the novel findings reportedin the paper, and all background information isignored.84Table 4: Most frequent organisms in IntAct (derived frominteractor proteins only)ORG freqHUMAN 0.380MOUSE 0.123YEAST 0.108ARATH 0.080RAT 0.047DROME 0.040SCHPO 0.032ECOLI 0.019BOVIN 0.016CAEEL 0.014?
While in some cases the annotators seem to de-cide that an organism is just used as part ofthe method and does not merit an inclusion, inother cases the annotators do not seem to treatthe problem the same way.An analysis of the most frequent false negativesis reported in table 3.
The ranking is certainly notidentical with the frequency table (table 1), whichwas unexpected.
Manual inspection of some of thepapers causing these false negatives gave the follow-ing results:?
Some common names such as ?mice?, and ad-jectives such as ?murine?, were absent from thetaxonomy (while ?transgenic mice?
e.g.
waspresent).?
There are probably more hints to recognizeECOLI (Escherichia coli K12) than just thepresence of the string ?K12?
(or ?K-12?).
Ourprogram tends to attribute all mentions of ?Es-cherichia coli?
without this string to ECOLX,generating false negatives for ECOLI and falsepositives for ECOLX.?
The extremely high false negative rate forCERAE (Chlorocebus aethiops, also known asCercopithecus aethiops) is a consequence of itsvery different frequencies as source of interac-tor proteins and as a host organism.The problem with CERAE suggests that it mightbe necessary to consider separately organisms intheir roles as sources of the interactor proteins and ashosts for the experiments.
CERAE is only frequentas a host organism, but in this role it does not appearin the papers by any of the organism names givenby the taxonomy (such as ?Chlorocebus aethiops?,?Cercopithecus aethiops?, ?African green monkey?,?grivet?, ?savanah monkey?
or ?vervet monkey?
).The reason is that often only the names of cell lines(e.g.
?Vero?)
derived from the organism appear inthe paper.7 To a lesser degree, this is true as well forpapers where YEAST appears in this role.A first step to deal with this problem consistedin creating different frequency tables for organismsas source of interactor proteins and as hosts of theexperiment (tables 4 and 5).
As these frequency ta-bles are very different from each other and from thecombined one (table 1) and as the combined taskof identifying ?protein organisms?
and ?host organ-isms?
seems to be artificial in any case, we decidedto split the problem accordingly: (a) identify organ-isms from which interacting proteins are derived; (b)identify host organisms.
The results for each of thesenew tasks are not yet as good as the result for thecombined task we described above, but as the infor-mation we are looking for now is more specific, thiswas to be expected.3.1 Identification of ?Interactor Organisms?In order to obtain a solution for this more specifictask, we just kept the formula as for the original task,but replaced the frequency table for ?interactor andhost organisms?
(table 1) by a new one for ?interac-tors only?
(table 4).
At the same time we raised thethreshold to 18%: as the new freqency tables tendedto nearly eliminate several typical host organisms,the remaining candidates for ?interactor organisms?profited by this, so the threshold had to be raisedto maximize f-score.
The rest of the parameters re-mained identical.Obviously, a new gold standard for ?interactorsonly?
had to be derived from IntAct.
Our best resultsfor this new task are: precision: 0.697; recall: 0.693;f-score: 0.695.3.2 Identification of ?Host Organisms?For this alternative task we also had to improve theinput, not just the formula, as we noticed that of-7 The Vero lineage is a very popular cell line isolated fromkidney epithelial cells extracted from an African green monkey(?Cercopithecus aethiops?
).85Table 5: Most frequent organisms in IntAct (host organ-isms only), freq* is computed excluding ?in vitro?ORG freq freq*?in vitro?
0.363 -YEAST 0.262 0.412HUMAN 0.167 0.264CERAE 0.036 0.057MOUSE 0.035 0.055ARATH 0.021 0.034DROME 0.021 0.034SCHPO 0.020 0.031ECOLX 0.017 0.027RAT 0.010 0.015ten species which were given as hosts by IntActwere not mentioned by any of their names (mostimportantly CERAE).
So we decided to include an-other category of biological named entities in ourtermbase, namely cell line names.
These were de-rived from one of the largest collections of celllines information: the Cell Lines Knowledge Base(CLKB, (Sarntivijai et al, 2008)).
However, a fewcell line names which are type-ambiguous with othertypes of NE?s in our termbase (normally proteins)had to be ignored to avoid conflicts.
Another newinput to the formula was the mention of ?in vitro?,contained in our termbase as a method, but used bythe IntAct annotators as annotation for the ?host or-ganism?.The following adaptations to the ranking formulawere necessary.
The frequency table for ?interactorand host organisms?
(table 1) was replaced by a newone for ?hosts only?, including ?in vitro?
(table 5).At the same time the default had to be changed to?in vitro?
and was given a nearly identical weightof 30 fulltext mentions (instead of 28), the thresh-old remained at 4% and the abstract mentions weregiven a weight of 35 fulltext mentions.
The new cellline mentions were given a weight of 3 fulltext men-tions for their respective organisms.
Of course, anew gold standard for ?interactors only?
was derivedfrom IntAct also in this case.
Our best results yet forthis new task are: precision: 0.689; recall: 0.737;f-score: 0.712.4 Related Work and DiscussionThe task of organism recognition is only recentlystarting to emerge as an independent subtask inbiomedical IE.
For example, the latest BioCreativecompetitive evaluation of text mining system for bi-ology8 included a task of protein-protein interactiondetection (Krallinger et al, 2008).
Although organ-ism recognition was not officially evaluated, manyparticipants found that it was an indispensable stepin order to perform accurate protein recognition anddisambiguation.
As a consequence, the BioCreativemeta-server (Leitner et al, 2008), offers organismrecognition as one of its services (called ?TX task?).
(Wang and Matthews, 2008) is perhaps the mostcomprehensive study to date dealing with speciesdisambiguation for term disambiguation.
They com-bine a rule-based species disambiguation approachwith a maximum entropy classifier based on con-textual features of the term to be disambiguated.They evaluate in detail the contribution of both ap-proaches over two separate corpora.
While previouswork has shown the benefits of using species infor-mation for term disambiguation (Alex et al, 2008;Rinaldi et al, 2008), this is perhaps the first studywhich also provides a separate evaluation of speciesdisambiguation in itself.
Since their purpose is touse the organism mentions to disambiguate entities,they evaluate how far their system can identify theorganisms associated with each entity mention inthe document.
They report a level of accuracy thatreaches 74.24% on one of their test corpora.Since our results are for whole articles, not singleentity mentions, they are not directly comparable.The advantage of our approach resides in its simplic-ity, since it does not require a specifically designedtraining set, being based only on publicly availablestandard databases.
This reduces not only the costcompared to building own resources, but also en-sures that their quality is monitored.In this paper we have not discussed how our re-sults can be used in the disambiguation of entities.As long as only one organism is selected as the fo-cus of a given research publication, this is a rathertrivial task.
However, as mentioned already in sec-tion 2.4, it is often the case that multiple organismsare considered within the same publication.
In that8http://www.biocreative.org/86case, organism mentions would need to be ?local-ized?
within the article in order to serve for disam-biguation purposes, as done in (Wang and Matthews,2008).
Our own approach to this problem is pre-sented and discussed in (Kaljurand et al, 2009).One important limitation of our approach is itsreliance on explicit mentions of organisms by theirnames as stored in the termbase (or minor variantsthereof).
Using all the names available to us (in-cluding cell lines) and their variants we could so farachieve only a maximal value of 88% recall, whichmeans that 12% of the organisms are not referred toby any name in our resources.
This may be due toeither missing names in the termbase (the organismsare mentioned, but by different names) or becausethey are identified by human readers through othercontextual hints which may consist of any sort of in-formation,9 and may presuppose massive amountsof background knowledge.
The first problem mightbe adressed by adding other sources of names to ourtermbase.
The second problem might be adressedby using a machine learning approach, which how-ever brings with it a whole set of new problems, suchas selection and representation of the features rele-vant for training, as well as the fact that a sufficientlylarge training corpus needs to be available.Another limitation of our approach is the fact thatits development and testing rests on its applicationto the identification of either organisms of proteininteractors or host organisms.
The original formu-lation of the goal that motivated this work was ?toidentify automatically the organisms forming part ofthe subject matter of scientific papers?.
This leavesopen the question of the application of the results,and is deliberately vague in the wording ?part of thesubject matter?, which includes but is not confinedto the cases mentioned above.
This formulation wasmotivated by a desire to keep the task as generic aspossible, so that the resulting application could notonly be used as a module for the protein disambigua-tion task, but also for other tasks of NE disambigua-tion with respect to organisms, as well as for organ-ism identification as an independent task.
Addition-ally, the ranked list of candidate organisms deliveredby our program could also be presented to human9A trivial example would be a publication in a journal whichspecializes in research on a single organism.users, who might want to use them in novel ways,for example in an assisted curation environment.However, the gold standard by which we test ourresults is tailored to its application as a protein dis-ambiguation module, just as the frequency tables weuse.
Even apart from this, the appropriateness of thegold standard is partly questionable, as it does notonly prefer organisms involved in protein interac-tions to those that are not, but also ?new?
knowledgeto ?old?
knowledge, etc.
Our approach, based on?correcting?
simple counts of organism mentions us-ing frequency tables, can only be successful as longas there is a gold standard for the specific applica-tion that is being pursued.
We can derive from Int-Act useful gold standards for organisms from whichprotein interactors are derived or host organisms, butwe have no gold standard for ?organism identifica-tion?
as an independent task.5 ConclusionIn this paper we discussed an approach to the prob-lem of ?organism identification?
as an independenttask, based only on standard resources.
Whilethe initial results were interesting, the experimentalsetup led us to identify more specific aspects of theproblem, and in particular to distinguish organismsmentioned in their roles as sources of the interact-ing proteins and as hosts of the experiments.
Wehave shown that a clear identification of the differentfunctional roles played by organism mentions canlead to more accurate results.Although a fully automated disambiguation pro-cess based on organism mentions is not within im-mediate reach, the results described in this paperappear already potentially useful for protein namedisambiguation in a curation environment.
An-other possible application would be in biomedi-cal curation-based databases, for the semi-automatictagging of publications with their focus organisms.AcknowledgementsThis research is partially funded by the Swiss NationalScience Foundation (grant 100014-118396/1).
Addi-tional support is provided by Novartis Pharma AG, NI-TAS, Text Mining Services, CH-4002, Basel, Switzer-land.
We thank the anonymous reviewers for their in-sightful comments.87References[Alex et al2008] Beatrice Alex, Claire Grover, BarryHaddow, Mijail Kabadjov, Ewan Klein, MichaelMatthews, Richard Tobin, and Xinglong Wang.
2008.Automating curation using a natural language process-ing pipeline.
Genome Biology, 9(Suppl 2):S10.
[Kaljurand et al2009] Kaarel Kaljurand, Fabio Rinaldi,Thomas Kappeler, and Gerold Schneider.
2009.
Us-ing existing biomedical resources to detect and groundterms in biomedical literature.
In 12th Conference onArtificial Intelligence in Medicine (AIME?09), Verona,Italy, 18?22 July.
[Kappeler et al2008] Thomas Kappeler, SimonClematide, Kaarel Kaljurand, Gerold Schneider,and Fabio Rinaldi.
2008.
Towards automatic de-tection of experimental methods from biomedicalliterature.
In Third International Symposium onSemantic Mining in Biomedicine (SMBM).
[Kerrien et al2006] S. Kerrien, Y. Alam-Faruque,B.
Aranda, I. Bancarz, A. Bridge, C. Derow, E. Dim-mer, M. Feuermann, A. Friedrichsen, R. Huntley,C.
Kohler, J. Khadake, C. Leroy, A. Liban, C. Lieftink,L.
Montecchi-Palazzi, S. Orchard, J. Risse, K. Robbe,B.
Roechert, D. Thorneycroft, Y. Zhang, R. Apweiler,and H. Hermjakob.
2006.
IntAct ?
Open SourceResource for Molecular Interaction Data.
NucleicAcids Research.
[Krallinger et al2008] Martin Krallinger, Florian Leit-ner, Carlos Rodriguez-Penagos, and Alfonso Valencia.2008.
Overview of the protein-protein interaction an-notation extraction task of BioCreative II.
Genome Bi-ology, 9(Suppl 2):S4.
[Leitner et al2008] Florian Leitner, Martin Krallinger,Carlos Rodriguez-Penagos, Jo?rg Hakenberg, Con-rad Plake, Cheng-Ju Kuo, Chun-Nan Hsu, RichardTzong-Han Tsai, Hsi-Chuan Hung, William W. Lau,Calvin A. Johnson, Rune Saetre, Kazuhiro Yoshida,Yan Hua Chen, Sun Kim, Soo-Yong Shin, Byoung-TakZhang, William A. Baumgartner, Lawrence Hunter,Barry Haddow, Michael Matthews, Xinglong Wang,Patrick Ruch, Fre?de?ric Ehrler, Arzucan O?zgu?r, Gu?nesErkan, Dragomir R. Radev, Michael Krauthammer,ThaiBinh Luong, Robert Hoffmann, Chris Sander, andAlfonso Valencia.
2008.
Introducing meta-servicesfor biomedical information extraction.
Genome Biol-ogy, 9(Suppl 2):S6.
[Rinaldi et al2008] Fabio Rinaldi, Thomas Kappeler,Kaarel Kaljurand, Gerold Schneider, Manfred Klen-ner, Simon Clematide, Michael Hess, Jean-Marc vonAllmen, Pierre Parisot, Martin Romacker, and ThereseVachon.
2008.
OntoGene in BioCreative II.
GenomeBiology, 9(Suppl 2):S13.
[Sarntivijai et al2008] Sirarat Sarntivijai, Alexander S.Ade, Brian D. Athey, and David J.
States.
2008.
Abioinformatics analysis of the cell line nomenclature.Bioinformatics, 24(23):2760?2766.
[UniProt Consortium2007] UniProt Consortium.
2007.The universal protein resource (UniProt).
NucleicAcids Research, 35:D193?7.
[Wang and Matthews2008] Xinglong Wang and MichaelMatthews.
2008.
Distinguishing the species ofbiomedical named entities for term identification.BMC Bioinformatics, 9(Suppl 11):S6.
[Zanzoni et al2002] A. Zanzoni, L. Montecchi-Palazzi,M.
Quondam, G. Ausiello, M. Helmer-Citterich, andG.
Cesareni.
2002.
MINT: a Molecular INTeractiondatabase.
FEBS Letters, 513(1):135?140.88
