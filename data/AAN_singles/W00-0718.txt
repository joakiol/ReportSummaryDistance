In: Proceedings of CoNLL-2000 and LLL-2000, pages 95-98, Lisbon, Portugal, 2000.ALLiS: a Symbolic Learning System forNatural Language LearningHerv@ D@jeanSeminar ffir SprachwissenschaftUniversit?t Tfibingendej  eanOsf s. nph i l ,  un i - tueb ingen,  de1 In t roduct ionWe present ALLiS, a learning system for iden-tifying syntactic structures which uses theoryrefinement.
When other learning techniques(symbolic or statistical) are widely used in Nat-ural Language Learning, few applications usetheory refinement (Abecker and Schmid, 1996),(Mooney, 1993).
We would like to show thateven a basic implementation of notions usedin TR is enough to build an efficient machinelearning system concerning the task of learninglinguistic structures.ALLiS relies on the use of background knowl-edge and default values in order to build up aninitial grammar and on the use of theory refine-ment in order to improve this grammar.
Thiscombination provides a good machine learningframework (efficient and fast) for Natural Lan-guage Learning.
After presenting theory refine-ment (Section 2) and a general description ofALLiS (Section 3), we will show how each stepof TR is applying in the specific ase of learninglinguistic structures (non-recursive phrases).2 About  Theory  Ref inementTheory refinement (hereafter TR) consists ofimproving an existing knowledge base so thatit fits more with data.
No work using the-ory refinement applied to the grammar learningparadigm seems to have been developed.
Wewould like to point out in this article the ad-equacy between theory refinement and NaturalLanguage Learning.
For a more detailed presen-tation of TR, we refer the reader to (Abeckerand Schmid, 1996), (Brunk, 1996).
(Mooney,1993) defines it as:Theory refinement systems developedin Machine Learning automaticallymodify a Knowledge Base to render itconsistent with a set of classified train-ing examples.This technique thus consists of improving agiven Knowledge Base (here a grammar) onthe basis of examples (here a treebank).
Somemethods try to modify the initial knowledgebase as little as possible.
(Abecker and Schmid,1996) presents the general algorithm as:1.
Build a more or less correct grammar onthe basis of background knowledge.2.
Refine this grammar using training exam-ples:(a) Identify the revision points(b) Correct themThe first step consists of acquiring an initialgrammar (or more generally a knowledge base).In this work, the initial grammar is automati-cally induced from a tagged and bracketed cor-pus.
The second step (refinement) compares theprediction of the initial grammar with the train-ing corpus in order to, first, identify the revi-sion points, i.e.
points that are not correctlydescribed by the grammar, and second, to cor-rect these revision points.3 ALL iSALLiS (Architecture for Learning LinguisticStructures) (D~jean, 2000a) is a symbolic ma-chine learning system which generates categori-sation rules from a tagged and bracketed cor-pus.
These categorisation rules allow (partial)parsing.
Unless (Brill, 1993), these rules can-not be directly used in order to parse a text.ALLiS uses an internal formalism in order torepresent he grammar rules it has learned.This internal representation (Table 1) allows95the use of different systems in order to  parsethe structures.
Each system requires a conver-sion of theses rules into its formalism.
Thisuse of "intermediary" formalism allows the sep-aration of two different problems: the genera-tion of (linguistic) rules and the use of them.Unless Transformation-Based Learning (Brill,1993) which modifies training data each time arule is learned, ALLiS always uses the originaltraining data.
By this way you try to separatethe problem of learning "linguistic" rules to theproblem of parsing (the adequate use of theserules).
The rules generated contains enoughinformation (elements which compose the con-texts, structures of these elements) so that wecan correctly generate rules for a specific parser.We can note that, although rules have to be or-dered during the parse, this order does not de-pend on the order used during the learning step,but depends on the category of the element.Context in( l )  or out(2)Tag Left Right W L RVBG PRP$ 1 1VBG POS 1 1VBG JJ 1 1VBG DT 1 1VBG TO 1 2VBG IN 1 2VBG VBG 1 2Table 1: Contexts generated for the categorisa-tion of the category AL (NP).Table 1 shows a part of the file generated con-cerning the categorisation f the tag VBG.
Thefirst line has to be read: when the tag VBG oc-curs after the tag PRP$ (left context) and whenthe tag PRP$ occurs in the structure (L=l(in)),the tag VBG is categorised as AL (left adjunct:see next section).
In order to parse a text, amodule automatically converts this formalisminto appropriate formalisms which can be usedby existing symbolic parsers.
Several tools havebeen tried: the CASS parser (Abney, 1996),XFST (Karttunen et al, 1997)) and LT TTT(Glover et al, 1999).
The TTT formalism seemsto be the most appropriate (rules are easy togenerate and the resulting parser is fast).
TheTTT rule corresponding to the first line of thetable 1 is given table 2<RULE name="AL" targ_sg="?
\[CAT=' AL' \] "><REL match="W \[C= ' PRP$ 'm_mod= ' TEST 'S='NP'\] "></REL><REL match="W\[C='VBG'\] "></REL></RULE>Table 2: TTT formalism.4 The  Generat ion  o f  the  In i t ia lGrammarThe first step is to assign to each tag of thecorpus a default category corresponding to itsmost frequent behaviour regarding the structurewe want to learn.
The result of its operation isa set of rules which assign a default category toeach tag.In general, the baseline is computed by givingan element its most frequent ag.
ALLiS usesan initial grammar which is a little more so-phisticated: it uses the same principle with theexception that the default ag depends on con-texts.
Generally the chunk tagset is composedof three tags: B,I, and O. ALLiS uses a subcat-egorisation of the I category.
It considers thata structure is composed of a Nucleus (tag N)with optional eft and right adjuncts (AL andAR).
These three classes (AL, N, AR) possessan attribute B 1 with the value +/ - .
Further-more, an element is considered as AL/AR iff itoccurs before/after a nucleus.
For this reason,a tag such as j j2 can be categorised as AL orO(outside) according to its context.
Precisionand recall of this initial grammar axe around86%.
An example of NP analyse provided bythe initial grammar is:(i) \[ I t _PRP_N \] ' s_VBZ_O \[ t raders_NNS_O \]squar ing_VBG_O \[ posit ions_NNS_N \] .
(2) The_DT_0  operat ing_VBG_O \[chief_NN_N's_POS_ALB+\] \[post_NN_N\] is_VBZ_Onew_JJ_O .The initial grammar categorises the tag VBGas occurring by default outside an NP, whichis mainly the case (as in example (1)).
But in1Introduction f a break with the preceding adjacentstructure (this property simulates the B tag).2j j: adjective (Penn treebank tagset).96some cases this default categorisation is wrong(example 2).
Since the default structure is de-fined as: S ~ \[AL* N AR*\]+, the phrase theoperating chief can not be correctly parsed bythe initial grammar.
Such an error can be fixedduring the refinement step as explained in thenext section.5 The  Ref inementOnce this initial grammar is built, we confrontit to the bracketed corpus, and apply the re-finement step.
The general theory refinementalgorithm given by (Abecker and Schmid, 1996)is:- Find revision points in theory- Create possible revisions- Choose best revision- Revise theoryuntil no revision improves theoryThe next sections now show how these oper-ations are performed by ALLiS.5.1 Revis ion PointsRevision points correspond to errors generatedby the initial grammar.
In the example (2), theword operciting does not belong to the NP sincethe tag VBG is categorised as O(outside NP).This is thus a revision point.
During the re-finement, ALLiS finds out all the occurrencesof a tag whose categorisation i the trainingcorpus does not correspond to the categorisa-tion provided by the initial grammar.
Once re-vision points are identified, ALLiS disposes oftwo kinds of operators in order to fix errors:the specialisation and the generalisation.
Wejust use basic implementation f these opera-tors, but it is nevertheless enough to get efficientresults comparable to other systems (Table 5).5.2 The Special isat ionThe specialisation relies on two operations: thecontextualisation and the lexicalisation.
Thecontextualisation consists of specifying contextsin which a rule categorises with a high accuracyan element.
The table 1 provides examples ofcontexts for the tag VBG in which this tag oc-curs in an NP, and thus which fix the revisionpoint of example (2).
The lexicalisation consistsContextTag Word Left Right W L RVBG operating NN 1 1VBG recurring NNS 1 1VBG continuing NNS 1 1Table 3: Lexicalisation of the tag VBG.of replacing 3 a tag by a specific word (Table 3).Some words in some contexts can have a be-haviour which can not be detected at the taglevel.
If contextualisation is rather corpus in-dependent, lexicalisation generates rules whichdepend of the type of the training corpus.
Moredetails about these two operations can be foundin (D@jean, 2000b).5.3 The General isat ionAfter specialisation, some structures are stillnot recognised.
If some revisions points can notbe fixed using only local contexts, a generalisa-tion (by relaxing constraints) in the definition ofthe structure can improve parsing.
A structureis composed of a nucleus and optional adjuncts(Section 3).
Such a structure can not recog-nise all the sequences categorised as NP in thetraining corpus.
These unrecognised sequencesare composed of elements without nucleus.
Inexample (3), the sequence the reawakening com-poses a NP although it is tagged as AL AL byALLiS.
(3) \[the_DT reawakening_VBG\] of_IN\[the_DT abortion-rights_NNS move-ment_NN\]Generalisation consists of accepting some se-quences of elements which do no correspond toa whole structure (S --+ AL* N AR* \] AL+ 1AR+).
The technique we use for this general-isation is just the deletion of the element N inthe rule describing a structure.
More generally,this step allows the.
correct parse of sequenceswhere ellipsises occur.
The most frequent par-tial structures correspond to the sequences: DTJ J, DT VBG and DT.5.4 The Select ion of  RulesDuring the operations of specialisation and gen-eralisation, rules are generated in order to im-prove the initial grammar.
But combinationaThe lexicalisation can be considered as a replace-ment of a variable by a constant.97of both lexicalisation and contextualisation canyield rules which are redundant.
In the table 4,the two last rules are learned whereas the firstis enough.Tag Word left rightVBG operating NNVBG operating IN NNVBG operating VBD NNTable 4: Superfluous rules.P/R  FNP  TKS00 93.63/92.89 93.26MPRZ99 92.4/93.1 92.8ALLiS 92.38/92.71 92.54XTAG99 91.8/93.0 92.37RM95 92.27/91.80 92.03VP ALLiS 92.48/92.92 92.70Table 5: Results for NP and VP structures (pre-cision/recall).The purpose of its step is to reduce the num-ber of rules ALLiS generated.
In fact the num-ber of rules can be reduced uring the speciali-sation step.
But a simplest way is to select somerules after specialisation and generalisation ac-cording to heuristics.The heuristic we used consists first of select-ing the most frequent rules and then amongthem, those having the richest (longest) context(several rules can be obtained using only the cri-terion of frequency).
In our case (learning lin-guistic structures), this heuristic provides goodresult, but a more efficient algorithm might mayconsist of parsing the corpus with the candidaterules and to select he most frequent rules pro-viding the best parse.We can note that these superfluous rules donot generally produce wrong analyses, even ifsome are not linguistically motivated.
The factthat we try to get the minimal revised theory iscomputationally interesting since the reductionof rules eases parsing.6 Resu l tsALLiS was used in order to learn several struc-tures (D6jean, 2000b).
The table 5 shows re-sults for VP and NP and results obtained byother systems 4.
The best result is obtained by(Tjong Kim Sang, 2000) using a combination ofNP representation.
ALLiS offers the best scorefor the symbolic systems.7 Conc lus ionWe showed that even a simple implementationof TR provides good results (comparable toother systems) for learning non-recursive struc-tures from bracketed corpora.
The next steps4More complete r sults are shown in (D6jean, 2000a)and (D6jean, 2000b).concern two directions.
First the improvementof algorithms used by ALLiS (especially the se-lection of rules).
The second step consists ofapplying ALLiS on other structures, especiallythe clause so that ALLiS can provide a morecomplete parsing.Re ferencesAndreas Abecker and Klaus Schmid.
1996.
Fromtheory refinement to kb maintenance: a positionstatement.
In ECAI'96, Budapest, Hungary.Steven Abney.
1996.
Partial parsing via finite-statecascades.
In Proceedings of the ESSLLI '96 Ro-bust Parsing Workshop.Eric Brill.
1993.
A Corpus-Based Approach to Lan-guage Learning.
Ph.D: thesis, Department ofComputer and Information Science, University ofPennsylvania.Clifford Alan Brunk.
1996.
An investigationo/ Knowledge Intensive Approaches to ConceptLearning and Theory Refinement.
Ph.D. thesis,University of California, Irvine.Herv6 D6jean.
2000a.
Theory refinement andnatural language learning.
In COLING'2000,Saarbrficken.Herv6 D6jean.
2000b.
A use of xml for ma-chine learning.
In Proceeding o\] the workshopon Computational Natural Language Learning,CONLL '2000.Claire Glover, Andrei Mikheev, andColin Matheson, 1999.
LT TTT ver-sion 1.
O: Text Tokenisation Soytware.http://www.ltg.ed.ac.uk/software/ttt/.Lauri Karttunen, Tamgts GaA1, and Andr6 Kempe.1997.
Xerox finite-state tool.
Technical report,Xerox Research Centre Europe, Grenoble.Raymond J. Mooney.
1993.
Induction over the un-explained: Using overly-general domain theoriesto aid concept learning.
Machine Learning, 10:79.Erik F. Tjong Kim Sang.
2000.
Noun phraserepresentation by system combination.
In Mor-gan Kaufman Publishers, editor, Proceedings o/ANLP-NAACL 2000, Seattle.98
