Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1723?1732,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsMulti-Task Learning for Multiple Language TranslationDaxiang Dong, Hua Wu, Wei He, Dianhai Yu and Haifeng WangBaidu Inc, Beijing, China{dongdaxiang, wu hua, hewei06, yudianhai, wanghaifeng}@baidu.comAbstractIn this paper, we investigate the problem oflearning a machine translation model thatcan simultaneously translate sentencesfrom one source language to multipletarget languages.
Our solution is inspiredby the recently proposed neural machinetranslation model which generalizesmachine translation as a sequencelearning problem.
We extend the neuralmachine translation to a multi-tasklearning framework which shares sourcelanguage representation and separatesthe modeling of different target languagetranslation.
Our framework can be appliedto situations where either large amountsof parallel data or limited parallel datais available.
Experiments show thatour multi-task learning model is able toachieve significantly higher translationquality over individually learned model inboth situations on the data sets publiclyavailable.1 IntroductionTranslation from one source language to multipletarget languages at the same time is a difficult taskfor humans.
A person often needs to be familiarwith specific translation rules for differentlanguage pairs.
Machine translation systemssuffer from the same problems too.
Under thecurrent classic statistical machine translationframework, it is hard to share information acrossdifferent phrase tables among different languagepairs.
Translation quality decreases rapidly whenthe size of training corpus for some minoritylanguage pairs becomes smaller.
To conquer theproblems described above, we propose amulti-task learning framework based on asequence learning model to conduct machinetranslation from one source language to multipletarget languages, inspired by the recentlyproposed neural machine translation(NMT)framework proposed by Bahdanau et al (2014).Specifically, we extend the recurrent neuralnetwork based encoder-decoder framework to amulti-task learning model that shares an encoderacross all language pairs and utilize a differentdecoder for each target language.The neural machine translation approach hasrecently achieved promising results in improvingtranslation quality.
Different from conventionalstatistical machine translation approaches, neuralmachine translation approaches aim at learninga radically end-to-end neural network model tooptimize translation performance by generalizingmachine translation as a sequence learningproblem.
Based on the neural translationframework, the lexical sparsity problem and thelong-range dependency problem in traditionalstatistical machine translation can be alleviatedthrough neural networks such as long short-term memory networks which provide greatlexical generalization and long-term sequencememorization abilities.The basic assumption of our proposedframework is that many languages differ lexicallybut are closely related on the semantic and/or thesyntactic levels.
We explore such correlationacross different target languages and realize itunder a multi-task learning framework.
We treat aseparate translation direction as a sub RNNencode-decoder task in this framework whichshares the same encoder (i.e.
the same sourcelanguage representation) across differenttranslation directions, and use a different decoderfor each specific target language.
In this way, thisproposed multi-task learning model can make fulluse of the source language corpora acrossdifferent language pairs.
Since the encoder partshares the same source language representation1723across all the translation tasks, it may learnsemantic and structured predictive representationsthat can not be learned with only a small amountof data.
Moreover, during training we jointlymodel the alignment and the translation processsimultaneously for different language pairs underthe same framework.
For example, when wesimultaneously translate from English intoKorean and Japanese, we can jointly learn latentsimilar semantic and structure information acrossKorea and Japanese because these two languagesshare some common language structures.The contribution of this work is three folds.First, we propose a unified machine learningframework to explore the problem of translatingone source language into multiple targetlanguages.
To the best of our knowledge, thisproblem has not been studied carefully in thestatistical machine translation field before.Second, given large-scale training corpora fordifferent language pairs, we show that ourframework can improve translation quality oneach target language as compared with the neuraltranslation model trained on a single languagepair.
Finally, our framework is able to alleviatethe data scarcity problem, using language pairswith large-scale parallel training corpora toimprove the translation quality of those with fewparallel training corpus.The following sections will be organized asfollows: in section 2, related work will bedescribed, and in section 3, we will describe ourmulti-task learning method.
Experiments thatdemonstrate the effectiveness of our frameworkwill be described in section 4.
Lastly, we willconclude our work in section 5.2 Related WorkStatistical machine translation systems often relyon large-scale parallel and monolingual trainingcorpora to generate translations of high quality.Unfortunately, statistical machine translationsystem often suffers from data sparsity problemdue to the fact that phrase tables are extracted fromthe limited bilingual corpus.
Much work has beendone to address the data sparsity problem suchas the pivot language approach (Wu and Wang,2007; Cohn and Lapata, 2007) and deep learningtechniques (Devlin et al, 2014; Gao et al, 2014;Sundermeyer et al, 2014; Liu et al, 2014).On the problem of how to translate one sourcelanguage to many target languages within onemodel, few work has been done in statisticalmachine translation.
A related work in SMT isthe pivot language approach for statistical machinetranslation which uses a commonly used languageas a ?bridge?
to generate source-target translationfor language pair with few training corpus.
Pivotbased statistical machine translation is crucial inmachine translation for resource-poor languagepairs, such as Spanish to Chinese.
Consideringthe problem of translating one source languageto many target languages, pivot based SMTapproaches does work well given a large-scalesource language to pivot language bilingual corpusand large-scale pivot language to target languagescorpus.
However, in reality, language pairsbetween English and many other target languagesmay not be large enough, and pivot-based SMTsometimes fails to handle this problem.
Ourapproach handles one to many target languagetranslation in a different way that we directly learnan end to multi-end translation system that doesnot need a pivot language based on the idea ofneural machine translation.Neural Machine translation is a emergingnew field in machine translation, proposedby several work recently (Kalchbrenner andBlunsom, 2013; Sutskever et al, 2014; Bahdanauet al, 2014), aiming at end-to-end machinetranslation without phrase table extraction andlanguage model training.
Different fromtraditional statistical machine translation, neuralmachine translation encodes a variable-lengthsource sentence with a recurrent neural networkinto a fixed-length vector representation anddecodes it with another recurrent neural networkfrom a fixed-length vector into variable-lengthtarget sentence.
A typical model is the RNNencoder-decoder approach proposed by Bahdanauet al (2014), which utilizes a bidirectionalrecurrent neural network to compress the sourcesentence information and fits the conditionalprobability of words in target languages witha recurrent manner.
Moreover, soft alignmentparameters are considered in this model.
As aspecific example model in this paper, we adopt aRNN encoder-decoder neural machine translationmodel for multi-task learning, though all neuralnetwork based model can be adapted in ourframework.In the natural language processing field, a1724notable work related with multi-task learningwas proposed by Collobert et al (2011) whichshared common representation for input wordsand solve different traditional NLP tasks such aspart-of-Speech tagging, name entity recognitionand semantic role labeling within one framework,where the convolutional neural network modelwas used.
Hatori et al (2012) proposed tojointly train word segmentation, POS tagging anddependency parsing, which can also be seen asa multi-task learning approach.
Similar idea hasalso been proposed by Li et al (2014) in Chinesedependency parsing.
Most of multi-task learningor joint training frameworks can be summarizedas parameter sharing approaches proposed byAndo and Zhang (2005) where they jointly trainedmodels and shared center parameters in NLPtasks.
Researchers have also explored similarapproaches (Sennrich et al, 2013; Cui et al, 2013)in statistical machine translation which are oftenrefered as domain adaption.
Our work explores thepossibility of machine translation under the multi-task framework by using the recurrent neuralnetworks.
To the best of our knowledge, this is thefirst trial of end to end machine translation undermulti-task learning framework.3 Multi-task Model for MultipleLanguage TranslationOur model is a general framework for translatingfrom one source language to many targets.
Themodel we build in this section is a recurrentneural network based encoder-decoder model withmultiple target tasks, and each task is a specifictranslation direction.
Different tasks share thesame translation encoder across different languagepairs.
We will describe model details in thissection.3.1 Objective FunctionGiven a pair of training sentence {x,y}, astandard recurrent neural network basedencoder-decoder machine translation model fits aparameterized model to maximize the conditionalprobability of a target sentence y given a sourcesentence x , i.e., argmax p(y|x).
We extend thisinto multiple languages setting.
In particular,suppose we want to translate from English tomany different languages, for instance,French(Fr), Dutch(Nl), Spanish(Es).
Paralleltraining data will be collected before training, i.e.En-Fr, En-Nl, En-Es parallel sentences.
Since theEnglish representation of the three language pairsis shared in one encoder, the objective functionwe optimize is the summation of severalconditional probability terms conditioned onrepresentation generated from the same encoder.L(?)
= argmax?
(?Tp(1NpNp?ilog p(yiTp|xiTp; ?
))(1)where ?
= {?src,?trgTp, Tp= 1, 2, ?
?
?
, Tm},?srcis a collection of parameters for sourceencoder.
And ?trgTpis the parameter setof the Tpth target language.
Npis the sizeof parallel training corpus of the pth languagepair.
For different target languages, the targetencoder parameters are seperated so we have Tmdecoders to optimize.
This parameter sharingstrategy makes different language pairs maintainthe same semantic and structure information of thesource language and learn to translate into targetlanguages in different decoders.3.2 Model DetailsSuppose we have several language pairs(xTp,yTp) where Tpdenotes the index of the Tpthlanguage pair.
For a specific language pair, givena sequence of source sentence input(xTp1, xTp2, ?
?
?
, xTpn), the goal is to jointlymaximize the conditional probability for eachgenerated target word.
The probability ofgenerating the tth target word is estimated as:p(yTpt|yTp1, ?
?
?
, yTpt?1, xTp) = g(yTpt?1, sTpt, cTpt)(2)where the function g is parameterized by afeedforward neural network with a softmax outputlayer.
And g can be viewed as a probabilitypredictor with neural networks.
sTptis a recurrentneural network hidden state at time t, which canbe estimated as:sTpt= f(sTpt?1, yTpt?1, cTpt) (3)the context vector cTptdepends on a sequence ofannotations (h1, ?
?
?
, hLx) to which an encodermaps the input sentence, where Lxis thenumber of tokens in x.
Each annotation hiis a bidirectional recurrent representation withforward and backward sequence information1725around the ith word.ctTp=Lx?j=1aTpijhj(4)where the weight aTptjis a scalar computed byaTptj=exp(eTptj)?LTpxk=1exp(eTptk)(5)eTptj= ?
(st?1Tp,hj) (6)aTptjis a normalized score of etjwhich is a softalignment model measuring how well the inputcontext around the jth word and the output wordin the tth position match.
etjis modeled through aperceptron-like function:?
(x,y) = vTtanh(Wx + Uy) (7)To compute hj, a bidirectional recurrent neuralnetwork is used.
In the bidirectional recurrentneural network, the representation of a forwardsequence and a backward sequence of the inputsentence is estimated and concatenated to be asingle vector.
This concatenated vector can beused to translate multiple languages during the testtime.hj= [??hj;?
?hj]T(8)From a probabilistic perspective, our model isable to learn the conditional distribution of severaltarget languages given the same source corpus.Thus, the recurrent encoder-decoders are jointlytrained with several conditional probabilitiesadded together.
As for the bidirectional recurrentneural network module, we adopt the recentlyproposed gated recurrent neural network (Choet al, 2014).
The gated recurrent neuralnetwork is shown to have promising results inseveral sequence learning problem such as speechrecognition and machine translation where inputand output sequences are of variable length.
Itis also shown that the gated recurrent neuralnetwork has the ability to address the gradientvanishing problem compared with the traditionalrecurrent neural network, and thus the long-rangedependency problem in machine translation canbe handled well.
In our multi-task learningframework, the parameters of the gated recurrentneural network in the encoder are shared, which isformulated as follows.ht= (I?
zt) ht?1+ zt?ht(9)zt= ?
(Wzxt+ Uzht?1) (10)?ht= tanh(Wxt+ U(rtht?1)) (11)rt= ?
(Wrxt+ Urht?1) (12)Where I is identity vector and  denotes elementwise product between vectors.
tanh(x) and ?
(x)are nonlinear transformation functions that can beapplied element-wise on vectors.
The recurrentcomputation procedure is illustrated in 1, wherextdenotes one-hot vector for the tth word in asequence.Figure 1: Gated recurrent neural networkcomputation, where rtis a reset gate responsiblefor memory unit elimination, and ztcan be viewedas a soft weight between current state informationand history information.tanh(x) =ex?
e?xex+ e?x(13)?
(x) =11 + e?x(14)The overall model is illustrated in Figure 2where the multi-task learning framework withfour target languages is demonstrated.
Thesoft alignment parameters Aifor each encoder-decoder are different and only the bidirectionalrecurrent neural network representation is shared.3.3 OptimizationThe optimization approach we use is themini-batch stochastic gradient descent approach(Bottou, 1991).
The only difference between ouroptimization and the commonly used stochasticgradient descent is that we learn several mini-batches within a fixed language pair for severalmini-batch iterations and then move onto the nextlanguage pair.
Our optimization procedure isshown in Figure 3.1726Figure 2: Multi-task learning framework for multiple-target language translationFigure 3: Optimization for end to multi-end model3.4 Translation with Beam SearchAlthough parallel corpora are available for theencoder and the decoder modeling in the trainingphrase, the ground truth is not available during testtime.
During test time, translation is produced byfinding the most likely sequence via beam search.
?Y = argmaxYp(YTp|STp) (15)Given the target direction we want to translate to,beam search is performed with the shared encoderand a specific target decoder where search spacebelongs to the decoder Tp.
We adopt beam searchalgorithm similar as it is used in SMT system(Koehn, 2004) except that we only utilize scoresproduced by each decoder as features.
The sizeof beam is 10 in our experiments for speedupconsideration.
Beam search is ended until the end-of-sentence eos symbol is generated.4 ExperimentsWe conducted two groups of experiments toshow the effectiveness of our framework.
Thegoal of the first experiment is to show thatmulti-task learning helps to improve translationperformance given enough training corpora for alllanguage pairs.
In the second experiment, weshow that for some resource-poor language pairswith a few parallel training data, their translationperformance could be improved as well.4.1 DatasetThe Europarl corpus is a multi-lingual corpusincluding 21 European languages.
Here we onlychoose four language pairs for our experiments.The source language is English for all languagepairs.
And the target languages are Spanish(Es), French (Fr), Portuguese (Pt) and Dutch(Nl).
To demonstrate the validity of ourlearning framework, we do some preprocessingon the training set.
For the source language,we use 30k of the most frequent words forsource language vocabulary which is sharedacross different language pairs and 30k mostfrequent words for each target language.
Out-of-vocabulary words are denoted as unknownwords, and we maintain different unknown wordlabels for different languages.
For test sets,we also restrict all words in the test set tobe from our training vocabulary and mark theOOV words as the corresponding labels as inthe training data.
The size of training corpus inexperiment 1 and 2 is listed in Table 1 where1727Training Data InformationLang En-Es En-Fr En-Nl En-Pt En-Nl-sub En-Pt-subSent size 1,965,734 2,007,723 1,997,775 1,960,407 300,000 300,000Src tokens 49,158,635 50,263,003 49,533,217 49,283,373 8,362,323 8,260,690Trg tokens 51,622,215 52,525,000 50,661,711 54,996,139 8,590,245 8,334,454Table 1: Size of training corpus for different language pairsEn-Nl-sub and En-Pt-sub are sub-sampled dataset of the full corpus.
The full parallel trainingcorpus is available from the EuroParl corpus,downloaded from EuroParl public websites1.
Wemimic the situation that there are only a small-scale parallel corpus available for some languagepairs by randomly sub-sampling the training data.The parallel corpus of English-Portuguese andEnglish-Dutch are sub-sampled to approximately15% of the full corpus size.
We select two dataLanguage pair En-Es En-Fr En-Nl En-PtCommon test 1755 1755 1755 1755WMT2013 3000 3000 - -Table 2: Size of test set in EuroParl Commontestset and WMT2013sets as our test data.
One is the EuroParl Commontest set2in European Parliament Corpus, the otheris WMT 2013 data set3.
For WMT 2013, onlyEn-Fr, En-Es are available and we evaluate thetranslation performance only on these two testsets.
Information of test sets is shown in Table 2.4.2 Training DetailsOur model is trained on Graphic Processing UnitK40.
Our implementation is based on the opensource deep learning package Theano (Bastien etal., 2012) so that we do not need to take careabout gradient computations.
During training, werandomly shuffle our parallel training corpus foreach language pair at each epoch of our learningprocess.
The optimization algorithm and modelhyper parameters are listed below.?
Initialization of all parameters are fromuniform distribution between -0.01 and 0.01.?
We use stochastic gradient descent withrecently proposed learning rate decaystrategy Ada-Delta (Zeiler, 2012).1http:www.statmt.orgeuroparl2http://www.statmt.org/wmt14/test.tgz3http://matrix.statmt.org/test sets?
Mini batch size in our model is set to 50 sothat the convergence speed is fast.?
We train 1000 mini batches of data in onelanguage pair before we switch to the nextlanguage pair.?
For word representation dimensionality, weuse 1000 for both source language and targetlanguage.?
The size of hidden layer is set to 1000.We trained our multi-task model with a multi-GPU implementation due to the limitation ofGraphic memory.
And each target decoder istrained within one GPU card, and we synchronizeour source encoder every 1000 batches among allGPU card.
Our model costs about 72 hours on fulllarge parallel corpora training until convergenceand about 24 hours on partial parallel corporatraining.
During decoding, our implementation onGPU costs about 0.5 second per sentence.4.3 EvaluationWe evaluate the effectiveness of our method withEuroParl Common testset and WMT 2013 dataset.BLEU-4 (Papineni et al, 2002) is used as theevaluation metric.
We evaluate BLEU scores onEuroParl Common test set with multi-task NMTmodels and single NMT models to demonstratethe validity of our multi-task learning framework.On the WMT 2013 data sets, we compareperformance of separately trained NMT models,multi-task NMT models and Moses.
We use theEuroParl Common test set as a development set inboth neural machine translation experiments andMoses experiments.
For single NMT models andmulti-task NMT models, we select the best modelwith the highest BLEU score in the EuroParlCommon testset and apply it to the WMT 2013dataset.
Note that our experiment settings in NMTis equivalent with Moses, considering the sametraining corpus, development sets and test sets.17284.4 Experimental ResultsWe report our results of three experiments toshow the validity of our methods.
In the firstexperiment, we train multi-task learning modeljointly on all four parallel corpora and compareBLEU scores with models trained separately oneach parallel corpora.
In the second experiment,we utilize the same training procedures asExperiment 1, except that we mimic the situationwhere some parallel corpora are resource-poor andmaintain only 15% data on two parallel trainingcorpora.
In experiment 3, we test our learnedmodel from experiment 1 and experiment 2 onWMT 2013 dataset.
Table 3 and 4 show thecase-insensitive BLEU scores on the Europarlcommon test data.
Models learned from the multi-task learning framework significantly outperformthe models trained separately.
Table 4 showsthat given only 15% of parallel training corpusof English-Dutch and English-Portuguese, it ispossible to improve translation performance on allthe target languages as well.
This result makessense because the correlated languages benefitfrom each other by sharing the same predictivestructure, e.g.
French, Spanish and Portuguese, allof which are from Latin.
We also notice that eventhough Dutch is from Germanic languages, it isalso possible to increase translation performanceunder our multi-task learning framework whichdemonstrates the generalization of our model tomultiple target languages.Lang-Pair En-Es En-Fr En-Nl En-PtSingle NMT 26.65 21.22 28.75 20.27Multi Task 28.03 22.47 29.88 20.75Delta +1.38 +1.25 +1.13 +0.48Table 3: Multi-task neural translation v.s.
singlemodel given large-scale corpus in all languagepairsWe tested our selected model on the WMT 2013dataset.
Our results are shown in Table 5 whereMulti-Full is the model with Experiment 1 settingand the model of Multi-Partial uses the samesetting in Experiment 2.
The English-Frenchand English-Spanish translation performances areimproved significantly compared with modelstrained separately on each language pair.
NoteLang-Pair En-Es En-Fr En-Nl* En-Pt*Single NMT 26.65 21.22 26.59 18.26Multi Task 28.29 21.89 27.85 19.32Delta +1.64 +0.67 +1.26 +1.06Table 4: Multi-task neural translation v.s.
singlemodel with a small-scale training corpus on somelanguage pairs.
* means that the language pair issub-sampled.that this result is not comparable with the resultreported in (Bahdanau et al, 2014) as we usemuch less training corpus.
We also compare ourtrained models with Moses.
On the WMT 2013data set, we utilize parallel corpora for Mosestraining without any extra resource such as large-scale monolingual corpus.
From Table 5, it isshown that neural machine translation modelshave comparable BLEU scores with Moses.
Onthe WMT 2013 test set, multi-task learning modeloutperforms both single model and Moses resultssignificantly.4.5 Model Analysis and DiscussionWe try to make empirical analysis throughlearning curves and qualitative results to explainwhy multi-task learning framework works well inmultiple-target machine translation problem.From the learning process, we observed that thespeed of model convergence under multi-tasklearning is faster than models trained separatelyespecially when a model is trained for resource-poor language pairs.
The detailed learning curvesare shown in Figure 4.
Here we study thelearning curve for resource-poor language pairs,i.e.
English-Dutch and En-Portuguese, for whichonly 15% of the bilingual data is sampled fortraining.
The BLEU scores are evaluated on theEuroparl common test set.
From Figure 4, itcan be seen that in the early stage of training,given the same amount of training data for eachlanguage pair, the translation performance ofthe multi-task learning model is improved morerapidly.
And the multi-task models achieve bettertranslation quality than separately trained modelswithin three iterations of training.
The reasonof faster and better convergence in performanceis that the encoder parameters are shared acrossdifferent language pairs, which can make full useof all the source language training data across thelanguage pairs and improve the source language1729Nmt Baseline Nmt Multi-Full Nmt Multi-Partial MosesEn-Fr 23.89 26.02(+2.13) 25.01(+1.12) 23.83En-Es 23.28 25.31(+2.03) 25.83(+2.55) 23.58Table 5: Multi-task NMT v.s.
single model v.s.
moses on the WMT 2013 test setFigure 4: Faster and Better convergence in Multi-task Learning in multiple language translationrepresentation.The sharing of encoder parameters is usefulespecially for the resource-poor language pairs.In the multi-task learning framework, the amountof the source language is not limited by theresource-poor language pairs and we are able tolearn better representation for the source language.Thus the representation of the source languagelearned from the multi-task model is more stable,and can be viewed as a constraint that leveragestranslation performance of all language pairs.Therefore, the overfitting problem and the datascarcity problem can be alleviated for languagepairs with only a few training data.
In Table 6,we list the three nearest neighbors of some sourcewords whose similarity is computed by usingthe cosine score of the embeddings both in themulti-task learning framework (from Experimenttwo ) and in the single model (the resource-poor English-Portuguese model).
Although thenearest neighbors of the high-frequent words suchas numbers can be learned both in the multi-taskmodel and the single model, the overall quality ofthe nearest neighbors learned by the resource-poorsingle model is much poorer compared with themulti-task model.The multi-task learning framework also generatestranslations of higher quality.
Some examples areshown in Table 7.
The examples are from theMultiTask Nearest neighborsprovide deliver 0.78, providing 0.74,give 0.72crime terrorism 0.66, criminal 0.65,homelessness 0.65regress condense 0.74, mutate 0.71,evolve 0.70six eight 0.98,seven 0.96, 12 0.94Single-Resource-Poor Nearest Neighborsprovide though 0.67,extending 0.56,parliamentarians 0.44crime care 0.75, remember 0.56, three0.53regress committing 0.33, accuracy0.30, longed-for 0.28six eight 0.87, three 0.69, thirteen0.65Table 6: Source language nearest-neighbor comparisonbetween the multi-task model and the single modelWMT 2013 test set.
The French and Spanishtranslations generated by the multi-task learningmodel and the single model are shown in the table.5 ConclusionIn this paper, we investigate the problem of how totranslate one source language into several differenttarget languages within a unified translationmodel.
Our proposed solution is based on the1730English Students, meanwhile, say the course isone of the most interesting around.Reference-Fr Les ?etudiants, pour leur part, assurentque le cours est l?
un des plusint?eressants.Single-Fr Les ?etudiants, entre-temps, disententendu l?
une des plus int?eressantes.Multi-Fr Les ?etudiants, en attendant, disent qu?
ilest l?
un des sujets les plus int?eressants.English In addition, they limited the rightof individuals and groups to provideassistance to voters wishing to register.Reference-Fr De plus, ils ont limit?e le droit depersonnes et de groupes de fournirune assistance aux ?electeurs d?esirant s?inscrire.Single-Fr En outre, ils limitent le droit desparticuliers et des groupes pour fournirl?
assistance aux ?electeurs.Multi-Fr De plus, ils restreignent le droit desindividus et des groupes `a fournir uneassistance aux ?electeurs qui souhaitentenregistrer.Table 7: Translation of different target languagesgiven the same input in our multi-task model.recently proposed recurrent neural network basedencoder-decoder framework.
We train a unifiedneural machine translation model under the multi-task learning framework where the encoder isshared across different language pairs and eachtarget language has a separate decoder.
To thebest of our knowledge, the problem of learningto translate from one source to multiple targetshas seldom been studied.
Experiments show thatgiven large-scale parallel training data, the multi-task neural machine translation model is ableto learn good predictive structures in translatingmultiple targets.
Significant improvement can beobserved from our experiments on the data setspublicly available.
Moreover, our framework isable to address the data scarcity problem of someresource-poor language pairs by utilizing large-scale parallel training corpora of other languagepairs to improve the translation quality.
Our modelis efficient and gets faster and better convergencefor both resource-rich and resource-poor languagepair under the multi-task learning.In the future, we would like to extend ourlearning framework to more practical setting.
Forexample, train a multi-task learning model withthe same target language from different domainsto improve multiple domain translation withinone model.
The correlation of different targetlanguages will also be considered in the futurework.AcknowledgementThis paper is supported by the 973 programNo.
2014CB340505.
We would like tothank anonymous reviewers for their insightfulcomments.ReferencesRie Kubota Ando and Tong Zhang.
2005.
Aframework for learning predictive structures frommultiple tasks and unlabeled data.
Journal ofMachine Learning Research, 6:1817?1853.Dzmitry Bahdanau, Kyunghyun Cho, and YoshuaBengio.
2014.
Neural machine translation byjointly learning to align and translate.
CoRR,abs/1409.0473.Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu,James Bergstra, Ian J. Goodfellow, ArnaudBergeron, Nicolas Bouchard, David Warde-Farley,and Yoshua Bengio.
2012.
Theano: new featuresand speed improvements.
CoRR, abs/1211.5590.L?eon Bottou.
1991.
Stochastic gradient learning inneural networks.
In Proceedings of Neuro-N?
?mes91, Nimes, France.
EC2.KyungHyun Cho, Bart van Merrienboer, DzmitryBahdanau, and Yoshua Bengio.
2014.
On theproperties of neural machine translation: Encoder-decoder approaches.
CoRR, abs/1409.1259.Trevor Cohn and Mirella Lapata.
2007.
Machinetranslation by triangulation: Making effective use ofmulti-parallel corpora.
In Proc.
ACL, pages 728?735.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel P. Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Lei Cui, Xilun Chen, Dongdong Zhang, Shujie Liu,Mu Li, and Ming Zhou.
2013.
Multi-domainadaptation for SMT using multi-task learning.
InProc.
EMNLP, pages 1055?1065.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard M. Schwartz, and John Makhoul.2014.
Fast and robust neural network joint modelsfor statistical machine translation.
In Proc.
ACL,pages 1370?1380.Jianfeng Gao, Xiaodong He, Wen-tau Yih, andLi Deng.
2014.
Learning continuous phraserepresentations for translation modeling.
In Proc.ACL, pages 699?709.1731Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, andJun?ichi Tsujii.
2012.
Incremental joint approach toword segmentation, POS tagging, and dependencyparsing in chinese.
In Proc.
ACL, pages 1045?1053.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentcontinuous translation models.
In Proc.
EMNLP,pages 1700?1709.Philipp Koehn.
2004.
Pharaoh: A beamsearch decoder for phrase-based statistical machinetranslation models.
In Machine Translation:From Real Users to Research, 6th Conference ofthe Association for Machine Translation in theAmericas, AMTA 2004, Washington, DC, USA,September 28-October 2, 2004, Proceedings, pages115?124.Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu,and Wenliang Chen.
2014.
Joint optimizationfor chinese POS tagging and dependency parsing.IEEE/ACM Transactions on Audio, Speech &Language Processing, 22(1):274?286.Shujie Liu, Nan Yang, Mu Li, and Ming Zhou.
2014.A recursive recurrent neural network for statisticalmachine translation.
In Proc.
ACL, pages 1491?1500.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automaticevaluation of machine translation.
In Proc.
ACL,ACL 2002, pages 311?318, Stroudsburg, PA, USA.Association for Computational Linguistics.Rico Sennrich, Holger Schwenk, and Walid Aransa.2013.
A multi-domain translation model frameworkfor statistical machine translation.
In Proc.
ACL,pages 832?840.Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,and Hermann Ney.
2014.
Translation modelingwith bidirectional recurrent neural networks.
InProc.
EMNLP, pages 14?25.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.2014.
Sequence to sequence learning with neuralnetworks.
In Advances in Neural InformationProcessing Systems 27: Annual Conference onNeural Information Processing Systems 2014,December 8-13 2014, Montreal, Quebec, Canada,pages 3104?3112.Hua Wu and Haifeng Wang.
2007.
Pivotlanguage approach for phrase-based statisticalmachine translation.
In Proc.
ACL, pages 165?181.Matthew D. Zeiler.
2012.
ADADELTA: an adaptivelearning rate method.
CoRR, abs/1212.5701.1732
