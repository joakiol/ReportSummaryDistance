Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 25?32Manchester, August 2008MultiSumQuery-Based Multi-Document SummarisationMichael RosnerDept.
Artificial IntelligenceUniversity of Maltamike.rosner@um.edu.mtCarl CamilleriDept.
Artificial IntelligenceUniversity of Maltaccam0002@um.edu.mtAbstractThis paper describes a generic, open-domain multi-document summarisationsystem which combines new and exist-ing techniques in a novel way.
The sys-tem is capable of automatically identify-ing query-related online documents andcompiling a report from the most use-ful sources, whilst presenting the result insuch a way as to make it easy for the re-searcher to look up the information in itsoriginal context.1 IntroductionAlthough electronic resources have several in-herent advantages over traditional research me-dia, they also introduce several drawbacks, suchas Information Overload (Edmunds and Morris,2000),which has become synonymous with the in-formation retrieval phase of any research-relatedtask.
Another problem which is directly relatedto the one just described is that of Source Iden-tification (Eppler and Mengis, 2004).
This refersto the problem of having relevant results intermin-gled with results that are less relevant, or actuallyirrelevant.Lastly, the researcher usually has to also manu-ally traverse the relevant sources of information inorder to form an answer to the research query.These problems have led to the study of vari-ous areas in computing, all of which aim to try andminimise the manual effort of information retrievaland extraction, one of which is Multi-DocumentSummarisation (MDS).The core aim of any MDS system is that of pro-cessing multiple sources of information and out-putting a relatively brief but broad report or sum-c?
2008.
Licensed under the Creative CommonsAttribution-Non ommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.mary.
Uses of MDS systems vary widely, fromsummarisation of closed-domain documents, suchas news documents (Evans et al, 2004), to aggre-gation of information from several sources in anopen domain.2 Aims and ObjectivesMDS techniques can be used in various tools thatmay help addressing the problems described inSection 1.
On the other hand, a brief study of therelevant literature indicates that the majority of thework done in this area concerns closed-domainssuch as news summarisation, which is perhaps thereason why such tools have not yet become morepopular.
The objectives of this study are thustwofold.?
The primary objective is that of design-ing, implementing and evaluating an open-domain, query-based MDS system which iscapable of compiling an acceptably-coherentreport from the most relevant online sourcesof information, whilst making it easy for thereader to access the full source of informationin its original context.?
A secondary objective of this study is SearchEngine Optimisation (SEO): We require thesystem to produce summaries which, if pub-lished on the Internet, would be deemed rel-evant to the original query by search en-gine ranking algorithms.
This is measuredby keyoword density in the summary.
Suc-cess on this objective addresses the problemof Source Identification since the summarywould at the very least serve as a gateway tothe other relevant sources from which it wasformed.Unsurprisingly, one of the problems that has tobe overcome in the field of summarisation and par-ticularly in an open-domain system such as ours is25the quality of output, as measured by a number ofdifferent linguistic and non-linguistic criteria (seeSection 5).
We have adopted a number of noveltechniques to address this such as?
Multi-Layered Architecture?
Sentence Ordering Model?
Heuristic Sentence Filtering?
Paragraph Clustering3 Background3.1 Search Engine Ranking CriteriaSearch engine ranking algorithms vary, and arecontinuously being optimised in order to providebetter and more accurate results.
However, someguidelines that outline factors which web mastersneed to take into account have been established (cf.Google (2007), Vaughn (2007)).When ranking documents for a particular searchquery, ranking algorithms take into account bothon-page and off-page factors.
Off-page factorscomprise mainly the number and quality of in-bound links to a particular page, whilst on-pagefactors comprise various criteria, most importantof which is the relevance of the content to thesearch query.3.2 Multi-Document SummarisationSeveral different approaches and processes havebeen developed in automatic MDS systems.
Thesevary according to the problem domain, which usu-ally defines particular formats for both input andoutput.
However, five basic sub-systems of anyMDS system can be identified (Mani, 2001).1.
Unit Identification During this first phase,input documents are parsed and tokenisedinto ?units?, which can vary from singlewords to whole documents, according to theapplication problem.2.
Unit Matching (Clustering) The secondstage involves grouping similar units together.In the context of MDS, similar units usu-ally mean either identical or informationally-equivalent strings (Clarke, 2004), with thepurpose of discovering the main themes in thedifferent units and identify the most salientones.3.
Unit Filtering The filtering stage eliminatesunits residing in clusters which are deemed tobe non-salient.4.
Compacting During this phase, it is often as-sumed that different clusters contain similarunits.
Thus, a sample of units from differentclusters is chosen.5.
Presentation/Summary Generation Thelast phase of the MDS process involves usingthe output from the Compacting stage, andgenerating a summary.
Usually, na?
?ve stringconcatenation does not produce coherentsummaries and thus, techniques such asnamed entity normalisation and sentenceordering criteria are used at this stage.3.3 Clustering TechniquesAs outlined in Section 3.2, MDS often makes useof clustering techniques in order to group togethersimilar units.
Clustering can be defined as a pro-cess which performs ?unsupervised classificationof patterns into groups based on their similarity?
(Clarke, 2004).A particular clustering technique typically con-sists of three main components:1.
Pattern Representation2.
Similarity Measure3.
Clustering AlgorithmThe very generic nature of our problem domainrequires a clustering technique which is both suit-able and without scenario-dependant parameters.Fung?s algorithm (Fung et al, 2003), comprising apre-processing stage and a further three-phase coreprocess, uses the following concepts, and is brieflydescribed in Figure 1.ItemSet A set of words occurring togetherwithin a document.
An ItemSet composed of kwords is called a k-ItemSet.Global Support The Global Support of a worditem is the number of documents from the docu-ment collection it appears in (cf.
document fre-quency).Cluster Support The Cluster Support of a worditem is the number of documents within a cluster itappears in.261.
Pre-Processing - stem, remove stopwords and convert to TFxIDF represen-tation2.
Discover Global Frequent ItemSets3.
For each Global Frequent ItemSet (GFI)create a corresponding cluster, contain-ing all documents that contain all itemsfound within the GFI associated witheach cluster.
This GFI will act as a ?la-bel?
to the cluster.4.
Make Clusters DisjointFigure 1: Hierarchical Document Clustering UsingFrequent ItemsetsFrequent ItemSet An ItemSet occurring in apre-determined minimum portion of a documentcollection.
The pre-defined minimum is referredto as the Minimum Support, and is usually deter-mined empirically according to the application.Global Frequent ItemSet An ItemSet which isfrequent within the whole document collection.The words within a Global Frequent ItemSet arereferred to as Global Frequent Items, whilst theminimum support is referred to as the MinimumGlobal Support.Cluster Frequent ItemSet An ItemSet whichis frequent within a cluster.
In this context, theminimum support is referred to as the MinimumCluster Support.With these definitions, it is now possible to de-scribe into more detail the core non-trivial phasesof the algorithm.3.3.1 Discovering Global Frequent ItemSetsFrom the definition of an ItemSet, it can be con-cluded that the set of ItemSets is the power set ofall features1within the document collection.
Giveneven a small document collection, enumerating allthe possible ItemSets and checking which of themare Global Frequent would be intractable.
In orderto discover Global Frequent ItemSets, the authorsrecommend the use of the Apriori Candidate Gen-eration algorithm, a data mining algorithm pro-posed by Agrawal and Srikant (1994).
This algo-1Features here constitute distinct, single words found inthe whole document collection.
In practice, stemming is ap-plied before feature extraction.rithm defines a way to reduce the number of candi-date frequent ItemSets generated.
The generationalgorithm basically operates on the principle that,given a set of frequent k-1-ItemSets, a set of can-didate frequent k-ItemSets can be generated suchthat each candidate is composed of frequent k-1-ItemSets.Agrawal and Srikant (1994) also mention a sim-ilar algorithm proposed by Mannila et al (1994).As illustrated in Figure 2, this algorithm consistsof first generating candidates, and then pruning theresult based on a principle similar to that men-tioned.3.3.2 Making Clusters DisjointThe purpose of the last phase of the algorithm isconverting a fuzzy cluster result to its crisp equiva-lent.
In order to identify the best cluster for a doc-ument contained in multiple clusters, the authorsdefine the scoring function illustrated in the equa-tion of Figure 3, where x is a global and cluster-frequent item in docj, x?a global frequent but notcluster frequent item in docj, and n(x) a weightedfrequency (TF.IDF) of feature x in docj.Using this function, the best cluster for a partic-ular document is that which maximises the score.In case of a tie, the most specific cluster (havingthe largest number of labels) is chosen.4 ProcedureThe system was designed in two parts, namely asimple web-based user interface and a server pro-cess responsible for iterating sequentially over userqueries and performing the content retrieval andsummarisation tasks.
The following sections de-scribe the various sub-systems that compose theserver process.4.1 Content RetrievalThe Content Retrieval sub-system is responsiblefor retrieving web documents related to a user?squery.
This is done simply by querying a searchengine and retrieving the top ranked documents2.Although throughout the course of this study thesystem was configured to use only Google as itsdocument source, the number of search enginesthat can be queried is arbitrary, and the system can2It was empirically determined that retrieving the top 30ranked documents achieved the best results.
Consideringless documents meant that, in most scenarios, main relevantsources were missed, whilst considering more documentscaused the infiltration of irrelevant information271.
JoinCk= {X ?X?|X,X??
Lk?1, |X ?X?| = k ?
2}2.
PruneCk= {X ?
C?k|X contains k members of Lk?1}Figure 2: Candidate Generation Algorithm by Mannila et al (1994)Score(Ci?
docj) =?xn(x)?
cluster support(x)??x?n(x?)?
global support(x?
)Figure 3: Definition of Scoring Functionbe given a set of parameters to query a particularsearch engine.4.2 Content ExtractionThe Content Extraction module is responsible fortransforming the retrieved HTML documents intoraw text.
However, a simple de-tagging processis not sufficient.
This module was designed so asto be able to identify the main content of a webdocument, and leave out other clutter such as nav-igation menus and headings.
Finn et al (2001) in-troduce a generic method to achieve this, by trans-lating the content extraction problem to an optimi-sation problem.
The authors observe that, essen-tially, an HTML document consists of two types ofelements, that is, actual text and HTML tags.
Thus,such a document can easily be encoded as a binarystring B, where 0 represents a natural word, whilst1 represents and HTML tag.
Figure 4 shows a typ-ical graphical representation obtained when cumu-lative HTML tag tokens are graphed against thecumulative number of tokens in a typical HTMLdocument.Finn et al (2001) suggest that, typically, theplateau that can be discerned in such a graph con-tains the actual document content.
Therefore, inorder to extract the content, the start and end pointof the plateau (marked with black boxes in Figure, and referred to hereafter as i and j respectively)must be identified.The optimisation problem now becomes max-imisation of the number of HTML tags below iand above j, in parallel with maximisation of thenumber of natural language words between i andj.
The maximisation formula proposed by the au-thors is given by Equation 1.Figure 4: Total HTML Tokens VS Total Tokens(Finn et al, 2001)Ti,j=i?1?n=0Bn+j?n=i(1?Bn)+N?1?n=j+1(1?Bn) (1)Our Content Extraction module is further de-composed into three sub-modules.
The first is apre-processing module, which parses out the bodyof the HTML document, and removes superfluouscontent such as scripts and styling sections.
Thesecond and core sub-module consists namely of animplementation of the content extraction methodintroduced by Finn et al (2001), which is pri-marily responsible for identifying the main con-tent section of the input document.
The last post-processing module then ensures that the outputfrom the previous sub-modules is converted to rawtext, by performing an HTML detagging process-ing and also inserting paragraph marks in placeswhere they are explicit cf.
HTML <p> tag) orwhere an element from a predefined set of HTMLtext break delimiters occurs.284.3 SummarisationThe overall design of the core summarisation mod-ule is loosely based upon the two-tiered MDSarchitecture introduced by Torralbo et al (2005)The following sections map our system to a sim-ilar two-tiered architecture, and explain how eachmodule operates.Document IdentificationDocument Identification is trivial, since docu-ments are explicitly defined by the content retrievalmodule, the output of which is basically a set ofquery-related text documents.Document FilteringThe job of Document Filtering is partially doneat the very beginning by the search engine.
How-ever, our system further refines the document col-lection by pre-processing each document, apply-ing a noise3removal procedure, stemming and stopword and rare word removal.
Each document isthen converted to a bag of words, or the VectorSpace Model, where each word is associated withits corresponding TF?IDF measure.
Any docu-ment which, after pre-processing, ends up with anempty bag of words, is filtered out from the doc-ument collection.
Furthermore, in order to ensurethe robustness of the system especially in subse-quent intensive processing, documents which arelonger than 5 times the average document lengthare truncated.Paragraph IdentificationAs outlined in Section 4.2, the Content Extrac-tion sub-system inserts paragraph indicators in thetext wherever appropriate.
Thus, the paragraphidentification phase is trivial, and entails only split-ting the content of a document at the indicated po-sitions.Paragraph Clustering and FilteringIn contrast to the technique of Torralbo et al(2005), a paragraph filtering module was intro-duced in order to select only the most informa-tive, query-related paragraphs.
To achieve this,we implemented the clustering technique out-lined in Section 3.3 in order to obtain clusters ofthematically-similar paragraphs, using the GlobalFrequent ItemSet generation technique from Man-nila et al (1994) and setting the Minimum Global3?Noise?
refers to any character which is not in the En-glish alphabet.1.
For each paragraph pk(a) Initialise the target summary Sumkas an empty text(b) Let p = pk(c) Remove the first sentence s from p,and add it at the end of Sumk.
(d) Calculate the similarity between sand the first sentence of all the para-graphs, using the size of the inter-section of the two vectors of wordsas a similarity metric.
(e) Let p be the paragraph whose firstsentence maximises the similarity,and go back to step (c) with thatparagraph.
If the best similarity is0, stop.2.
Choose the longest one of the k differentsummaries.Figure 5: Summary Generation Algorithm (Tor-ralbo et al, 2005)Support and Minimum Cluster Support parametersto 35 and 50 respectively.The filtering technique then consists of simplychoosing the largest cluster.
This is based on theintuition that most of the paragraphs having thecentral theme as their main theme will get clus-tered together.
Therefore, choosing the largestcluster of paragraphs would filter out irrelevantparagraphs.
This paragraph filtering method mayfilter out paragraphs which are actually relevant,however, we rely on the redundancy of informa-tion usually found in information obtained fromthe web.
Thus, the paragraph filtering gives moreimportance to filtering out all the irrelevant para-graphs.Summary GenerationThe role of the summary generation module isto generate a report from a cluster of paragraphs.We based our summary generation method on thatused by Torralbo et al (2005), which is illustratedin Figure 5.
However, in order to make it moreapplicable to our problem domain and increase theoutput quality, we introduced some improvements.Sentence Ordering Model We introduced aprobabilistic sentence ordering model which en-ables the algorithm to choose the sentence that29maximises the probability given the previous sen-tence.
The sentence ordering model, based on amethod of probabilistic text structuring introducedby Lapata (2003), is trained upon the whole doc-ument collection.
We used Minipar (Lin, 1993),a dependency-based parser, in order to identifyverbs, nouns, verb dependencies and noun depen-dencies.
Using counts of these features and Sim-ple Good-Turing smoothing (Gale and Sampson,1995), we were able to construct a probabilisticsentence ordering model such that, during sum-mary generation, given the previous sentence, weare able to identify the sentence which is the mostlikely to occur from the pool of sentences appear-ing at the beginning of the remaining paragraphs.Sentence Filtering We also introduced at thisstage a method to filter out sentences that decreasethe coherency and fluency of the resultant sum-mary.
This is based on two criteria:1.
Very low probability of occurrenceIf the most likely next-occurring sentence thatis chosen and removed from a paragraph stillhas a very low probabilistic score, it is notadded to the output summary.2.
HeuristicsWe also introduce a set of heuristics to filterout sentences having a wrong construction orsentences which would not make sense in agiven context.
These heuristics include:(a) Sentences with no verbs(b) Sentences starting with an adverb andoccurring at a paragraph transitionwithin the summary(c) Sentences occurring at a context switch4within the summary and starting with aword matched with a select list of wordsthat usually occur as anaphora(d) Malformed sentences (including sen-tences not starting with a capital letterand very short sentences)5 Evaluation5.1 Automatic Evaluation5.1.1 Coherence EvaluationIn order to evaluate the local coherence of the re-ports generated by the system, we employed an au-4Context Switch refers to scenarios where a candidate sen-tence comes from a different document than that of the lastsentence in the summary.tomatic coherence evaluation method introducedby Barzilay and Lapata (2005)5.
The main objec-tive of this part of the evaluation phase was to de-termine the effect on output quality when param-eters are varied, namely the minimum cluster sup-port parameter for the clustering algorithm, and thekey phrase popularity.From this evaluation, we empirically deter-mined that the optimum minimum cluster supportthreshold for this application is 50, whilst the qual-ity of the output is directly proportional to the key-word popularity.5.1.2 Keyword Density EvaluationHere we focused on determining whether thesecondary objective was achieved (cf.
section 2).We measured the frequency of occurrence of thekeyword phrase within the output, or more specifi-cally, the keyword density.
The average key phrasedensity achieved by the system was 1.32%, whentaking into account (i) the original keyword phraseand its constituent keywords, and (ii) secondarykeyword phrases and their constituents.5.2 Manual Quality EvaluationIn order to measure the quality of the output anddetermine whether the objectives of the study wasachieved, three users were introduced to the sys-tem and asked to grade the system, on a scale of1-5, on several criteria.
Table 1 illustrates the re-sults obtained from this evaluation.6 Conclusions6.1 Interpretation of ResultsIn this section we will identify some conclusionselicited from the results obtained from the evalua-tion phase and illustrated in Section 5.Automatic Coherence Evaluation The auto-matic coherence evaluation tests, although, in thisapplication, the level of ?coherence?
indicated didnot match that of manual evaluation, providednonetheless a standard by which different outputsfrom the system using different parameters and ap-plication scenarios could be compared.
From theresults, we could empirically determine that theoptimal value for the cluster support parameter wasaround 50%.
Furthermore, unsurprisingly, the sys-tem tends to produce output of a higher quality5Data required to set up the automatic coherence eval-uation model was available from the author?s websitehttp://people.csail.mit.edu/regina/coherence/.30Grammaticality Non-Redundancy Referential Clarity Focus Structure Naturalness UsefulnessAverage 3.62 2.21 4.03 4.28 3.27 2.76 4.78Table 1: Results of Manual Evaluationin scenarios where the keyword phrase is popular,and thus more data is available.SEO Evaluation From an SEO perspective, itwas predictable that the system would producequery-related text, since its data source is ob-tained from query-related search engine results.However, the resulting average keyword densityachieved is significant, and is at a level which istotally acceptable by most search engine rankingalgorithms6.Manual Quality Evaluation Due to limited re-sources, the results of the manual evaluation pro-cedure were not statistically significant since onlythree users were involved in evaluating six sum-maries.
However, allowing for a factor of sub-jectivity, some conclusions could still be elicited,namely:1.
The system did not perform well enough tohave its output rated as high as a manual sum-marisation procedure.
This can be concludedfrom the low rating on the output Naturalnesscriterion, as well as from the presence of re-peated and irrelevant content in some of theoutput summaries.2.
The system performed acceptably well ingenerating reports that were adequately co-herent and high-level enough to give anoverview of concepts represented by users?queries.
This can be concluded from the av-erage scores achieved in the Focus and Refer-ential Clarity criteria.3.
The evaluators were also asked to give a gradeindicating whether this system and similartools would actually be useful.
A positivegrade was obtained on this criterion, indicat-ing that the system achieved the MDS objec-tive, enabling users to get a brief overviewof the topic as well as facilitating documentidentification.When comparing these results to those achievedby Torralbo et al (2005), we can elicit two mainconclusions:6Very high keyword density (more than a threshold of 2%- 5%) is usually considered as a spammy technique known askeyword stuffing.1.
Although our system achieved lower rank-ings on the Non-Redundancy, Structure andGrammaticality criteria, these rankings werenot unacceptable.
We could attributed this tothe more generic domain in which our sys-tem operates, where it is not possible to in-troduce fixed heuristics such as those used byTorralbo et al (2005) for avoiding repeatedinformation by replacing a term definition byits corresponding acronym.
Such heuristicstend to be relevant in the context of such aterm definition system.2.
Our system achieved higher grades on theReferential Clarity and Focus criteria.
Giventhe fact that the system of Torralbo et al(2005) retrieves results from search enginesin a similar way used by our system, the im-provement Focus might be attributed to thefact that our paragraph filtering methodol-ogy tends to perform well in selecting onlythe most relevant parts of the document base.Furthermore, the improved grade achieved inthe Referential Clarity criterion might arisefrom the more advanced sentence orderingmethodology used, as well as to the differentheuristic-based sentence filtering techniquesemployed by our summary generation mod-ule.6.2 LimitationsThe main limitation is that the quality of the outputis very susceptible to the quality and amount of re-sources available.
However, we also noticed a se-vere fall in quality where results were largely com-posed of business-oriented portals, which tend tolack textual information.
Furthermore, the outputsummary is largely dictated by the results of searchengines.
Therefore, the queries submitted to thesystem must be formulated similarly to those sub-mitted to search engines, since the system wouldfail to generate a focused summary for querieswhich, when submitted to traditional search en-gines, return irrelevant results.The system performance is also limited by thequality and number of external components beingreferenced, which are not state of the art and which31introduce performance bottlenecks by imposing abatch-processing regime.6.3 Final ConclusionsOur system combines several existing techniquesin a novel way.
New techniques, such as ourHeuristic-Based Sentence Filtering algorithm, arealso introduced.The primary objective of creating an MDS wasachieved albeit with limited ?coherency?.
How-ever, our system was considered a useful researchtool - supporting the hypothesis that a partially co-herent but understandable report with minimumeffort is arguably better than a perfectly coherentone, if the latter is unrealistically laborious to pro-duce.The secondary SEO objective was alsoachieved, to the extent that the system generatedquery-related content that has a natural level ofkey phrase density.
Such content has the potentialof being considered query-related also by searchengine ranking algorithms, if published within theright context.7 Future WorkThere remains much is to be done.
We propose:?
To increase the output quality and natural-ness by focusing on an a sub-system foranaphora identification and resolution whichwould complement our probabilistic sentenceordering model.?
To widen the scope by applying the system tosources of information other than web docu-ments.?
To convert our batch-processing system to aninteractive one by incorporating all the re-quired tools within the same environment.ReferencesAgrawal, R. and Ramakrishnan Srikant.
1994.
Fastalgorithms for mining association rules in largedatabases.
In VLDB?94, Proc.
of 20th InternationalConference on Very Large Data Bases, Sept. 1994,Santiago de Chile, Chile, pages 487?499.
MorganKaufmann.Barzilay, R. and M. Lapata.
2005.
Modeling localcoherence: an entity-based approach.
In ACL ?05:Proc.
43rd Annual Meeting of the ACL, pages 141?148, Morristown, NJ, USA.
ACL.Clarke, J.
2004.
Clustering techniques for multi-document summarisation.
Master?s thesis, Univer-sity of Sheffield.Edmunds, A. and A. Morris.
2000.
The problem ofinformation overload in business organisations: a re-view of the literature.
Int.
Journal of InformationManagement, 20(1):17?28.Eppler, M.J. and J. Mengis.
2004.
The Concept ofInformation Overload: A Review of Literature fromOrganization Science, Accounting, Marketing, MIS,and Related Disciplines.
The Information Society,20(5):325?344.Evans, D.K., J.L.
Klavans, and K.R.
McKeown.
2004.Columbia Newsblaster: Multilingual News Summa-rization on the Web.
Proc.
HLT Conference and theNAACL Annual Meeting.Finn, A., N. Kushmerick, and B. Smyth.
2001.
Factor fiction: Content classification for digital libraries.In DELOS Workshop: Personalisation and Recom-mender Systems in Digital Libraries.Fung, B.C.M., K. Wang, and M. Ester.
2003.
Hier-archical Document Clustering Using Frequent Item-sets.
Proc.
of the SIAM International Conference onData Mining, 30.Gale, W.A.
and G. Sampson.
1995.
Good-Turing Fre-quency Estimation Without Tears.
Journal of Quan-titative Linguistics, 2(3):217?237.Google.
2007.
Google Webmaster Guidelines.http://www.google.com/support/webmasters/bin/answer.py?answer=35769.Lapata, M. 2003.
Probabilistic text structuring: Ex-periments with sentence ordering.
Proc.
of the 41stMeeting of the ACL, pages 545?552.Lin, Dekang.
1993.
Principle-based parsing withoutovergeneration.
In Meeting of the ACL, pages 112?120.Mani, I.
2001.
Automatic Summarization.
Computa-tional Linguistics, 28(2).Mannila, Heikki, Hannu Toivonen, and A. InkeriVerkamo.
1994.
Efficient algorithms for discov-ering association rules.
In Fayyad, Usama M. andRamasamy Uthurusamy, editors, AAAI Workshopon Knowledge Discovery in Databases (KDD-94),pages 181?192, Seattle, Washington.
AAAI Press.Torralbo, R., E. Alfonseca, A. Moreno-Sandoval, andJ.M.
Guirao.
2005.
Automatic generation ofterm definitions using multidocument summarisa-tion from the Web.
In Proc.
Workshop on CrossingBarriers in Text Summarisation Research, RANLPBorovets.Vaughn.
2007.
Google Ranking Factors- SEO Checklist.
http://www.vaughns-1-pagers.com/internet/google-ranking-factors.htm.32
