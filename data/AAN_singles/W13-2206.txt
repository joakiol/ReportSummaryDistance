Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 78?84,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsFeature Decay Algorithms for Fast Deployment of Accurate StatisticalMachine Translation SystemsErgun Bic?iciCentre for Next Generation Localisation,Dublin City University, Dublin, Ireland.ergun.bicici@computing.dcu.ieAbstractWe use feature decay algorithms (FDA)for fast deployment of accurate statisticalmachine translation systems taking onlyabout half a day for each translation direc-tion.
We develop parallel FDA for solvingcomputational scalability problems causedby the abundance of training data for SMTmodels and LM models and still achieveSMT performance that is on par with us-ing all of the training data or better.
Par-allel FDA runs separate FDA models onrandomized subsets of the training dataand combines the instance selections later.Parallel FDA can also be used for selectingthe LM corpus based on the training setselected by parallel FDA.
The high qual-ity of the selected training data allows usto obtain very accurate translation outputsclose to the top performing SMT systems.The relevancy of the selected LM corpuscan reach up to 86% reduction in the num-ber of OOV tokens and up to 74% reduc-tion in the perplexity.
We perform SMTexperiments in all language pairs in theWMT13 translation task and obtain SMTperformance close to the top systems us-ing significantly less resources for trainingand development.1 IntroductionStatistical machine translation (SMT) is a data in-tensive problem.
If you have the translations forthe source sentences you are translating in yourtraining set or even portions of it, then the trans-lation task becomes easier.
If some tokens are notfound in your training data then you cannot trans-late them and if some translated word do not ap-pear in your language model (LM) corpus, then itbecomes harder for the SMT engine to find theircorrect position in the translation.Current SMT systems also face problemscaused by the proliferation of various parallel cor-pora available for building SMT systems.
Thetraining data for many of the language pairs inthe translation task, part of the Workshop on Ma-chine translation (WMT13) (Callison-Burch et al2013), have increased the size of the available par-allel corpora for instance by web crawled corporaover the years.
The increased size of the trainingmaterial creates computational scalability prob-lems when training SMT models and can increasethe amount of noisy parallel sentences found.
Asthe training set sizes increase, proper training setselection becomes more important.At the same time, when we are going to trans-late just a couple of thousand sentences, possiblybelonging to the same target domain, it does notmake sense to invest resources for training SMTmodels over tens of millions of sentences or evenmore.
SMT models like Moses already have filter-ing mechanisms to create smaller parts of the builtmodels that are relevant to the test set.In this paper, we develop parallel feature decayalgorithms (FDA) for solving computational scal-ability problems caused by the abundance of train-ing data for SMT models and LM models and stillachieve SMT performance that is on par with us-ing all of the training data or better.
Parallel FDAruns separate FDA models on randomized subsetsof the training data and combines the instance se-lections later.
We perform SMT experiments inall language pairs of the WMT13 (Callison-Burchet al 2013) and obtain SMT performance close tothe baseline Moses (Koehn et al 2007) system us-ing less resources for training.
With parallel FDA,we can solve not only the instance selection prob-lem for training data but also instance selection forthe LM training corpus, which allows us to trainhigher order n-gram language models and modelthe dependencies better.Parallel FDA improves the scalability of FDA78and allows rapid prototyping of SMT systems fora given target domain or task.
Parallel FDA can bevery useful for MT in target domains with limitedresources or in disaster and crisis situations (Lewiset al 2011) where parallel corpora can be gath-ered by crawling and selected by parallel FDA.Parallel FDA also improves the computational re-quirements of FDA by selecting from smaller cor-pora and distributing the work load.
The highquality of the selected training data allows us toobtain very accurate translation outputs close tothe top performing SMT systems.
The relevancyof the LM corpus selected can reach up to 86% re-duction in the number of OOV tokens and up to74% reduction in the perplexity.We organize our work as follows.
We describeFDA and parallel FDA models in the next section.We also describe how we extend the FDA modelfor LM corpus selection.
In section 3, we presentour experimental results and in the last section, wesummarize our contributions.2 Feature Decay Algorithms for InstanceSelectionIn this section, we describe the FDA algorithm,the parallel FDA model, and how FDA traininginstance selection algorithms can be used also forinstance selection for language model corpora.2.1 Feature Decay Algorithm (FDA)Feature decay algorithms (Bic?ici and Yuret,2011a) increase the diversity of the training set bydecaying the weights of n-gram features that havealready been included.
FDAs try to maximize thecoverage of the target language features for the testset.
Translation performance can improve as weinclude multiple possible translations for a givenword, which increases the diversity of the trainingset.
A target language feature that does not appearin the selected training instances will be difficult toproduce regardless of the decoding algorithm (im-possible for unigram features).
FDA tries to findas many training instances as possible to increasethe chances of covering the correct target languagefeature by reducing the weight of the included fea-tures after selecting each training instance.Algorithm 1 gives the pseudo-code for FDA.We improve FDA with improved scaling, wherethe score for each sentence is scaled proportionalto the length of the sentence, which reduces theaverage length of the training instances.Algorithm 1: The Feature Decay AlgorithmInput: Parallel training sentences U , test setfeatures F , and desired number oftraining instances N .Data: A priority queue Q, sentence scoresscore, feature values fval.Output: Subset of the parallel sentences to beused as the training data L ?
U .1 foreach f ?
F do2 fval(f)?
init(f,U)3 foreach S ?
U do4 score(S)?
1|S|s?f?features(S)fval(f)5 enqueue(Q, S,score(S))6 while |L| < N do7 S ?
dequeue(Q)8 score(S)?
1|S|s?f?features(S)fval(f)9 if score(S) ?
topval(Q) then10 L ?
L ?
{S}11 foreach f ?
features(S) do12 fval(f)?
decay(f,U ,L)13 else14 enqueue(Q, S,score(S))The input to the algorithm consists of paralleltraining sentences, the number of desired traininginstances, and the source language features of thetest set.
The feature decay function (decay) isthe most important part of the algorithm wherefeature weights are multiplied by 1/n where nis the count of the feature in the current train-ing set.
The initialization function (init) calcu-lates the log of inverse document frequency (idf):init(f,U) = log(|U|/(1 + C(f,U))), where|U| is the sum of the number of features appear-ing in the training corpus and C(f,U) is the num-ber of times feature f appear in U .
Further ex-periments with the algorithm are given in (Bic?iciand Yuret, 2011a).
We improve FDA with a scal-ing factor that prefers shorter sentences defined as:|S|s, where s is the power of the source sentencelength and we set it to 0.9 after optimizing it overthe perplexity of the LM built over the selectedcorpus (further discussed in Section 2.3).2.2 Parallel FDA ModelFDA model obtains a sorting over all of the avail-able training corpus based on the weights of thefeatures found on the test set.
Each selected train-79Algorithm 2: Parallel FDAInput: U , F , and N .Output: L ?
U .1 U ?
shuffle(U)2 U ,M ?
split(U , N)3 L ?
{}4 S ?
{}5 foreach Ui ?
U do6 Li,Si ?
FDA(Ui,F ,M)7 add(L,Li)8 add(S ,Si)9 L ?
merge(L,S )ing instance effects which feature weights will bedecayed and therefore can result in a different or-dering of the instances if previous instance selec-tions are altered.
This makes it difficult to par-allelize the FDA algorithm fully.
Parallel FDAmodel first shuffles the parallel training sentences,U , and distributes them to multiple splits for run-ning individual FDA models on them.The input to parallel FDA also consists of paral-lel training sentences, the number of desired train-ing instances, and the source language features ofthe test set.
The first step shuffles the parallel train-ing sentences and the next step splits into equalparts and outputs the split files and the adjustednumber of instances to select from each, M .
Sincewe split into equal parts, we select equal numberof sentences, M , from each split.
Then we runFDA on each file to obtain sorted files,L, togetherwith their scores, S .
merge combines k sortedlists into one sorted list in O(Mk log k) whereMk is the total number of elements in all of theinput lists.
1 The obtained L is the new training setto be used for SMT experiments.
We compared thetarget 2-gram feature coverage of the training setsobtained with FDA and parallel FDA and foundthat parallel FDA achieves close performance.Parallel FDA improves the scalability of FDAand allows rapid prototyping of SMT systems fora given target domain or task.
Parallel FDA alsoimproves the computational requirements of FDAby selecting from smaller corpora and distributingthe work load, which can be very useful for MT indisaster scenarios.1 (Cormen et al 2009), question 6.5-9.
Merging k sortedlists into one sorted list using a min-heap for k-way merging.2.3 Instance Selection for the LanguageModel CorpusThe language model corpus is very important forimproving the SMT performance since it helpsfinding the correct ordering among the translatedtokens or phrases.
Increased LM corpus size canincrease the SMT performance where doubling theLM corpus can improve the BLEU (Papineni etal., 2002) by 0.5 (Koehn, 2006).
However, al-though LM corpora resources are more abundant,training on large LM corpora also poses compu-tational scalability problems and until 2012, LMcorpora such as LDC Gigaword corpora were notfully utilized due to memory limitations of com-puters and even with large memory machines, theLM corpora is split into pieces, interpolated, andmerged (Koehn and Haddow, 2012) or the LMorder is decreased to use up to 4-grams (Markuset al 2012) or low frequency n-gram counts areomitted and better smoothing techniques are de-veloped (Yuret, 2008).
Using only the given train-ing data for building the LM is another optionused for limiting the size of the corpus, whichcan also obtain the second best performance inSpanish-English translation task and in the toptier for German-English (Guzman et al 2012;Callison-Burch et al 2012).
This can also indi-cate that prior knowledge of the test set domainand its similarity to the available parallel trainingdata may be diminishing the gains in SMT perfor-mance through better language modeling or betterdomain adaptation.For solving the computational scalability prob-lems, there is a need for properly selecting LMtraining data as well.
We select LM corpus withparallel FDA based on this observation:No word not appearing in the trainingset can appear in the translation.It is impossible for an SMT system to translatea word unseen in the training corpus nor can ittranslate it with a word not found in the targetside of the training set 2.
Thus we are only in-terested in correctly ordering the words appear-ing in the training corpus and collecting the sen-tences that contain them for building the LM.
Atthe same time, we want to be able to model longerrange dependencies more efficiently especially formorphologically rich languages (Yuret and Bic?ici,2Unless the translation is a verbatim copy of the source.802009).
Therefore, a compact and more relevantLM corpus can be useful.Selecting the LM corpus is harder.
First of all,we know which words should appear in the LMcorpus but we do not know which phrases shouldbe there since the translation model may reorderthe translated words, find different translations,and generate different phrases.
Thus, we use 1-gram features for LM corpus selection.
At thesame time, in contrast with selecting instances forthe training set, we are less motivated to increasethe diversity since we want predictive power onthe most commonly observed patterns.
Thus, wedo not initialize feature weights with the idf scoreand instead, we use the inverse of the idf scorefor initialization, which is giving more importanceto frequently occurring words in the training set.This way of LM corpus selection also allows usto obtain a more controlled language and helps uscreate translation outputs within the scope of thetraining corpus and the closely related LM corpus.We shuffle the LM corpus available before split-ting and select from individual splits, to preventextreme cases.
We add the training set directlyinto the LM and also add the training set not se-lected into the pool of sentences that can be se-lected for the LM.
The scaling parameter s is opti-mized over the perplexity of the training data withthe LM built over the selected LM corpus.3 ExperimentsWe experiment with all language pairs inboth directions in the WMT13 translationtask (Callison-Burch et al 2013), which includeEnglish-German (en-de), English-Spanish (en-es),English-French (en-fr), English-Czech (en-cs),and English-Russian (en-ru).
We develop transla-tion models using the phrase-based Moses (Koehnet al 2007) SMT system.
We true-case all of thecorpora, use 150-best lists during tuning, set themax-fertility of GIZA++ (Och and Ney, 2003) toa value between 8-10, use 70 word classes learnedover 3 iterations with mkcls tool during GIZA++training, and vary the language model orderbetween 5 to 9 for all language pairs.
The de-velopment set contains 3000 sentences randomlysampled from among all of the developmentsentences provided.Since we do not know the best training setsize that will maximize the performance, we relyon previous SMT experiments (Bic?ici and Yuret,2011a; Bic?ici and Yuret, 2011b) to select theproper training set size.
We choose close to 15million words and its corresponding number ofsentences for each training corpus and 10 millionsentences for each LM corpus not including theselected training set, which is added later.
Thiscorresponds to selecting roughly 15% of the train-ing corpus for en-de and 35% for ru-en, and due totheir larger size, 5% for en-es, 6% for cs-en, 2%for en-fr language pairs.
The size of the LM cor-pus allows us to build higher order models.
Thestatistics of the training data selected by the paral-lel FDA is given in Table 1.
Note that the trainingset size for different translation directions differslightly since we run a parallel FDA for each.cs / en de / en es / en fr / en ru / enwords (#M) 186 / 215 92 / 99 409 / 359 1010 / 886 41 / 44sents (#K) 867 631 841 998 709words (#M) 13 / 15 16 / 17 23 / 21 26 / 22 16 / 18Table 1: Comparison of the training data availableand the selected training set by parallel FDA foreach language pair.
The size of the parallel cor-pora is given in millions (M) of words or thou-sands (K) of sentences.After selecting the training set, we select theLM corpora using the words in the target side ofthe training set as the features.
For en, es, andfr, we have access to the LDC Gigaword corpora,from which we extract only the story type newsand for en, we exclude the corpora from XinhuaNews Agency (xin eng).
The size of the LM cor-pora from LDC and the monolingual LM corporaprovided by WMT13 are given in Table 2.
Forall target languages, we select 10M sentences withparallel FDA from the LM corpora and the remain-ing training sentences and add the selected trainingdata to obtain the LM corpus.
Thus the size of theLM corpora is 10M plus the number of sentencesin the training set as given in Table 1.#M cs de en es fr ruLDC - - 3402 949 773 -Mono 388 842 1389 341 434 289Table 2: The size of the LM corpora from LDCand the monolingual language model corpora pro-vided in millions (M) of words.With FDA, we can solve not only the instanceselection problem for the training data but alsothe instance selection problem for the LM train-ing corpus and achieve close target 2-gram cover-81S ?
en en?
Tcs-en de-en es-en fr-en ru-en en-cs en-de en-es en-fr en-ruWMT13 .2620 .2680 .3060 .3150 .2430 .1860 .2030 .3040 .3060 .1880BLEUc .2430 .2414 .2909 .2539 .2226 .1708 .1792 .2799 .2379 .1732BLEUc diff .0190 .0266 .0151 .0611 .0204 .0152 .0238 .0241 .0681 .0148LM order 7 9 7 9 6 5 5 5 7 5BLEUc, n .2407, 5 .2396, 5 .2886, 8 .2532, 6 .2215, 9 .1698, 9 .1784, 9 .2794, 9 .2374, 9 .1719, 9Table 3: Best BLEUc results obtained on the translation task together with the LM order used whenobtaining the result compared with the best constrained Moses results in WMT12 and WMT13.
The lastrow compares the BLEUc result with respect to using a different LM order.age using about 5% of the available training dataand 5% of the available LM corpus for instance foren.
A smaller LM training corpus also allows usto train higher order n-gram language models andmodel the dependencies better and achieve lowerperplexity as given in Table 5.3.1 WMT13 Translation Task ResultsWe run a number of SMT experiments for eachlanguage pair varying the LM order used and ob-tain different results and sorted these based on thetokenized BLEU performance, BLEUc.
The bestBLEUc results obtained on the translation task to-gether with the LM order used when obtaining theresults are given in Table 3.
We also list the top re-sults from WMT13 (Callison-Burch et al 2013) 3,which use phrase-based Moses for comparison 4and the BLEUc difference we obtain.
For trans-lation tasks with en as the target, higher order n-gram LM perform better whereas for translationtasks with en as the source, mostly 5-gram LMperform the best.
We can obtain significant gainsin BLEU (+0.0023) using higher order LMs.For all translation tasks except fr-en and en-fr,we are able to obtain very close results to the topMoses system output (0.0148 to 0.0266 BLEUcdifference).
This shows that we can obtain veryaccurate translation outputs yet use only a smallportion of the training corpus available, signifi-cantly reducing the time required for training, de-velopment, and deployment of an SMT system fora given translation task.We are surprised by the lower performance inen-fr or fr-en translation tasks and the reason is,we believe, due to the inherent noise in the Gi-gaFrEn training corpus 5.
FDA is an instance se-3We use the results from matrix.statmt.org.4Phrase-based Moses systems usually rank in the top 3.5We even found control characters in the corpora.lection tool and it does not filter out target sen-tences that are noisy since FDA only looks at thesource sentences when selecting training instancepairs.
Noisy instances may be caused by a sen-tence alignment problem and one way to fix themis to measure the sentence alignment accuracy byusing a similarity score over word distributionssuch as the Zipfian Word Vectors (Bic?ici, 2008).Since noisy parallel corpora can decrease the per-formance, we also experimented with discardingthe GigaFrEn corpus in the experiments.
However,this decreased the results by 0.0003 BLEU in con-trast to 0.004-0.01 BLEU gains reported in (Koehnand Haddow, 2012).
Also, note that the BLEU re-sults we obtained are lower than in (Koehn andHaddow, 2012), which may be an indication thatour training set size was small for this task.3.2 Training Corpus QualityWe measure the quality of the training corpus bythe coverage of the target 2-gram features of thetest set, which is found to correlate well with theBLEU performance achievable (Bic?ici and Yuret,2011a).
Table 4 presents the source (scov) and tar-get (tcov) 2-gram feature coverage of both the par-allel training corpora (train) that we select fromand the training sets obtained with parallel FDA.We show that we can obtain coverages close to us-ing all of the available training corpora.3.3 LM Corpus QualityWe compare the perplexity of the LM trained onall of the available training corpora for the de-enlanguage pair versus the LM trained on the paral-lel FDA training corpus and the parallel FDA LMcorpus.
The number of OOV tokens become 2098,2255, and 291 respectively for English and 2143,2555, and 666 for German.
To be able to com-pare the perplexities, we take the OOV tokens intoconsideration during calculations.
Tokenized LM82cs-en de-en es-en fr-en ru-en en-cs en-de en-es en-fr en-rutrain scov .70 .74 .85 .83 .66 .82 .82 .84 .87 .78tcov .82 .82 .84 .87 .78 .70 .74 .85 .83 .66FDA scov .70 .74 .85 .82 .66 .82 .82 .84 .84 .78tcov .74 .75 .77 .78 .75 .59 .67 .78 .76 .61Table 4: Source (scov) and target (tcov) 2-gram feature coverage comparison of the training corpora(train) with the training sets obtained with parallel FDA (FDA).corpus has 247M tokens for en and 218M tokensfor de.
We assume that each OOV word in en orde contributes log(1/218M) to the log probabil-ity, which we round to ?19.
We also present re-sults for the case when we handle OOV words bet-ter with a cost of ?11 each in Table 5.Table 5 shows that we reduce the perplexitywith a LM built on the training set selected withparallel FDA, which uses only 15% of the trainingdata for de-en.
More significantly, the LM build onthe LM corpus selected by the parallel FDA is ableto decrease both the number of OOV tokens andthe perplexity and allows us to efficiently modelhigher order relationships as well.
We reach up to86% reduction in the number of OOV tokens andup to 74% reduction in the perplexity.log OOV = ?19 log OOV = ?11ppl train FDA FDA LM train FDA FDA LMen3 763 774 203 431 419 1874 728 754 192 412 409 1785 725 753 191 410 408 1766 724 753 190 409 408 1767 724 753 190 409 408 176de3 1255 1449 412 693 713 3434 1216 1428 398 671 703 3315 1211 1427 394 668 702 3276 1210 1427 393 668 702 3267 1210 1427 392 668 702 326Table 5: Perplexity comparison of the LM builtfrom the training corpus (train), parallel FDA se-lected training corpus (FDA), and the parallel FDAselected LM corpus (FDA LM).3.4 Computational CostsIn this section, we quantify how fast the overallsystem runs for a given language pair.
The in-stance selection times are dependent on the num-ber of training sentences available for the languagepair for training set selection and for the target lan-guage for LM corpus selection.
We give the av-erage number of minutes it takes for the parallelFDA to finish selection for each direction and foreach target language in Table 6.time (minutes) en-fr en-ruParallel FDA train 50 18Parallel FDA LM 66 50Table 6: The average time in the number of min-utes for parallel FDA to select instances for thetraining set or for the LM corpus for languagepairs en-fr and en-ru.Once the training set and the LM corpus areready, the training of the phrase-based SMT modelMoses takes about 12 hours.
Therefore, we areable to deploy an SMT system for the target trans-lation task in about half a day and still obtain veryaccurate translation results.4 ContributionsWe develop parallel FDA for solving computa-tional scalability problems caused by the abun-dance of training data for SMT models and LMmodels and still achieve SMT performance that ison par with the top performing SMT systems.
Thehigh quality of the selected training data and theLM corpus allows us to obtain very accurate trans-lation outputs while the selected the LM corpus re-sults in up to 86% reduction in the number of OOVtokens and up to 74% reduction in the perplexityand allows us to model higher order dependencies.FDA and parallel FDA raise the bar of expec-tations from SMT translation outputs with highlyaccurate translations and lowering the bar to entryfor SMT into new domains and tasks by allowingfast deployment of SMT systems in about half aday.
Parallel FDA provides a new step towardsrapid SMT system development in budgeted train-ing scenarios and can be useful in developing ma-chine translation systems in target domains withlimited resources or in disaster and crisis situationswhere parallel corpora can be gathered by crawl-ing and selected by parallel FDA.
Parallel FDA isalso allowing a shift from general purpose SMTsystems towards task adaptive SMT solutions.83AcknowledgmentsThis work is supported in part by SFI(07/CE/I1142) as part of the Centre for NextGeneration Localisation (www.cngl.ie) at DublinCity University and in part by the European Com-mission through the QTLaunchPad FP7 project(No: 296347).
We also thank the SFI/HEA IrishCentre for High-End Computing (ICHEC), Koc?University, and Deniz Yuret for the provision ofcomputational facilities and support.ReferencesErgun Bic?ici and Deniz Yuret.
2011a.
Instance se-lection for machine translation using feature decayalgorithms.
In Proceedings of the Sixth Workshopon Statistical Machine Translation, pages 272?283,Edinburgh, Scotland, July.
Association for Compu-tational Linguistics.Ergun Bic?ici and Deniz Yuret.
2011b.
RegMT systemfor machine translation, system combination, andevaluation.
In Proceedings of the Sixth Workshopon Statistical Machine Translation, pages 323?329,Edinburgh, Scotland, July.
Association for Compu-tational Linguistics.Ergun Bic?ici.
2008.
Context-based sentence alignmentin parallel corpora.
In Proceedings of the 9th In-ternational Conference on Intelligent Text Process-ing and Computational Linguistics (CICLing 2008),LNCS, Haifa, Israel, February.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montre?al, Canada, June.
Association forComputational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2013.Findings of the 2013 workshop on statistical ma-chine translation.
In Proceedings of the EighthWorkshop on Statistical Machine Translation, pages10?51.
Association for Computational Linguistics,August.Thomas H. Cormen, Charles E. Leiserson, Ronald L.Rivest, and Clifford Stein.
2009.
Introduction toAlgorithms (3.
ed.).
MIT Press.Francisco Guzman, Preslav Nakov, Ahmed Thabet, andStephan Vogel.
2012.
Qcri at wmt12: Experi-ments in spanish-english and german-english ma-chine translation of news text.
In Proceedings of theSeventh Workshop on Statistical Machine Transla-tion, pages 298?303, Montre?al, Canada, June.
Asso-ciation for Computational Linguistics.Philipp Koehn and Barry Haddow.
2012.
Towardseffective use of training data in statistical machinetranslation.
In Proceedings of the Seventh Workshopon Statistical Machine Translation, pages 317?321,Montre?al, Canada, June.
Association for Computa-tional Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Annual Meeting of the Assoc.
for Compu-tational Linguistics, pages 177?180, Prague, CzechRepublic, June.Philipp Koehn.
2006.
Statistical machine translation:the basic, the novel, and the speculative.
Tutorial atEACL 2006.William Lewis, Robert Munro, and Stephan Vogel.2011.
Crisis mt: Developing a cookbook for mtin crisis situations.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages501?511, Edinburgh, Scotland, July.
Association forComputational Linguistics.Freitag Markus, Peitz Stephan, Huck Matthias, NeyHermann, Niehues Jan, Herrmann Teresa, WaibelAlex, Hai-son Le, Lavergne Thomas, AllauzenAlexandre, Buschbeck Bianka, Crego Joseph Maria,and Senellart Jean.
2012.
Joint wmt 2012 submis-sion of the quaero project.
In Proceedings of theSeventh Workshop on Statistical Machine Transla-tion, pages 322?329, Montre?al, Canada, June.
Asso-ciation for Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA, July.
Association for Computa-tional Linguistics.Deniz Yuret and Ergun Bic?ici.
2009.
Modeling mor-phologically rich languages using split words andunstructured dependencies.
In Proceedings of theACL-IJCNLP 2009 Conference Short Papers, pages345?348, Suntec, Singapore, August.
Associationfor Computational Linguistics.Deniz Yuret.
2008.
Smoothing a tera-word languagemodel.
In Proceedings of ACL-08: HLT, Short Pa-pers, pages 141?144, Columbus, Ohio, June.
Asso-ciation for Computational Linguistics.84
