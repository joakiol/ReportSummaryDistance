Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 31?34,New York City, New York, June 2006. c?2006 Association for Computational LinguisticsShallow Discourse Structure for Action Item DetectionMatthew Purver, Patrick Ehlen, and John NiekraszCenter for the Study of Language and InformationStanford UniversityStanford, CA 94305{mpurver,ehlen,niekrasz}@stanford.eduAbstractWe investigated automatic action itemdetection from transcripts of multi-partymeetings.
Unlike previous work (Gruen-stein et al, 2005), we use a new hierarchi-cal annotation scheme based on the rolesutterances play in the action item assign-ment process, and propose an approachto automatic detection that promises im-proved classification accuracy while en-abling the extraction of useful informationfor summarization and reporting.1 IntroductionAction items are specific kinds of decisions commonin multi-party meetings, characterized by the con-crete assignment of tasks together with certain prop-erties such as an associated timeframe and reponsi-ble party.
Our aims are firstly to automatically de-tect the regions of discourse which establish actionitems, so their surface form can be used for a tar-geted report or summary; and secondly, to identifythe important properties of the action items (such asthe associated tasks and deadlines) that would fos-ter concise and informative semantically-based re-porting (for example, adding task specifications to auser?s to-do list).
We believe both of these aims arefacilitated by taking into account the roles differentutterances play in the decision-making process ?
inshort, a shallow notion of discourse structure.2 BackgroundRelated Work Corston-Oliver et al (2004) at-tempted to identify action items in e-mails, usingclassifiers trained on annotations of individual sen-tences within each e-mail.
Sentences were anno-tated with one of a set of ?dialogue?
act classes; oneclass Task included any sentence containing itemsthat seemed appropriate to add to an ongoing to-do list.
They report good inter-annotator agreementover their general tagging exercise (?
> 0.8), al-though individual figures for the Task class are notgiven.
They then concentrated on Task sentences,establishing a set of predictive features (in whichword n-grams emerged as ?highly predictive?)
andachieved reasonable per-sentence classification per-formance (with f-scores around 0.6).While there are related tags for dialogue act tag-ging schema ?
like DAMSL (Core and Allen, 1997),which includes tags such as Action-Directiveand Commit, and the ICSI MRDA schema(Shriberg et al, 2004) which includes a committag ?
these classes are too general to allow iden-tification of action items specifically.
One compa-rable attempt in spoken discourse took a flat ap-proach, annotating utterances as action-item-relatedor not (Gruenstein et al, 2005) over the ICSI andISL meeting corpora (Janin et al, 2003; Burger etal., 2002).
Their inter-annotator agreement was low(?
= .36).
While this may have been partly dueto their methods, it is notable that (Core and Allen,1997) reported even lower agreement (?
= .15) ontheir Commit dialogue acts.
Morgan et al (forth-coming) then used these annotations to attempt auto-31matic classification, but achieved poor performance(with f-scores around 0.3 at best).Action Items Action items typically embody thetransfer of group responsibility to an individual.This need not be the person who actually performsthe action (they might delegate the task to a subor-dinate), but publicly commits to seeing that the ac-tion is carried out; we call this person the owner ofthe action item.
Because this action is a social ac-tion that is coordinated by more than one person,its initiation is reinforced by agreement and uptakeamong the owner and other participants that the ac-tion should and will be done.
And to distinguishthis action from immediate actions that occur duringthe meeting and from more vague future actions thatare still in the planning stage, an action item will bespecified as expected to be carried out within a time-frame that begins at some point after the meeting andextends no further than the not-too-distant future.
Soan action item, as a type of social action, often com-prises four components: a task description, a time-frame, an owner, and a round of agreement amongthe owner and others.
The related discourse tends toreflect this, and we attempt to exploit this fact here.3 Baseline ExperimentsWe applied Gruenstein et al (2005)?s flat annotationschema to transcripts from a sequence of 5 short re-lated meetings with 3 participants recorded as partof the CALO project.
Each meeting was simulatedin that its participants were given a scenario, butwas not scripted.
In order to avoid entirely data-or scenario-specific results (and also to provide anacceptable amount of training data), we then addeda random selection of 6 ICSI and 1 ISL meetingsfrom Gruenstein et al (2005)?s annotations.
Like(Corston-Oliver et al, 2004) we used support vec-tor machines (Vapnik, 1995) via the classifier SVM-light (Joachims, 1999).
Their full set of features arenot available to us, but we experimented with com-binations of words and n-grams and assessed classi-fication performance via a 5-fold validation on eachof the CALO meetings.
In each case, we trainedclassifiers on the other 4 meetings in the CALO se-quence, plus the fixed ICSI/ISL training selection.Performance (per utterance, on the binary classifica-tion problem) is shown in Table 1; overall f-scorefigures are poor even on these short meetings.
Thesefigures were obtained using words (unigrams, aftertext normalization and stemming) as features ?
weinvestigated other discriminative classifier methods,and the use of 2- and 3-grams as features, but noimprovements were gained.Mtg.
Utts AI Utts.
Precision Recall F-Score1 191 22 0.31 0.50 0.382 156 27 0.36 0.33 0.353 196 18 0.28 0.55 0.374 212 15 0.20 0.60 0.305 198 9 0.19 0.67 0.30Table 1: Baseline Classification Performance4 Hierarchical AnnotationsTwo problems are apparent: firstly, accuracy islower than desired; secondly, identifying utterancesrelated to action items does not allow us to ac-tually identify those action items and extract theirproperties (deadline, owner etc.).
But if the ut-terances related to these properties form distinctsub-classes which have their own distinct features,treating them separately and combining the results(along the lines of (Klein et al, 2002)) might al-low better performance, while also identifying theutterances where each property?s value is extracted.Thus, we produced an annotation schema whichdistinguishes among these four classes.
The firstthree correspond to the discussion and assignmentof the individual properties of the action item (taskdescription, timeframe and owner); the fi-nal agreement class covers utterances which ex-plicitly show that the action item is agreed upon.Since the task description subclass ex-tracts a description of the task, it must include anyutterances that specify the action to be performed,including those that provide required antecedents foranaphoric references.
The owner subclass includesany utterances that explicitly specify the responsibleparty (e.g.
?I?ll take care of that?, or ?John, we?llleave that to you?
), but not those whose functionmight be taken to do so implicitly (such as agree-ments by the responsible party).
The timeframesubclass includes any utterances that explicitly referto when a task may start or when it is expected tobe finished; note that this is often not specified with32a date or temporal expression, but rather e.g.
?bythe end of next week,?
or ?before the trip to Aruba?.Finally, the agreement subclass includes any ut-terances in which people agree that the action shouldand will be done; not only acknowledgements by theowner themselves, but also when other people ex-press their agreement.A single utterance may be assigned to more thanone class: ?John, you need to do that by nextMonday?
might count as owner and timeframe.Likewise, there may be more than one utterance ofeach class for a single action item: John?s response?OK, I?ll do that?
would also be classed as owner(as well as agreement).
While we do not requireall of these subclasses to be present for a set of ut-terances to qualify as denoting an action item, weexpect any action item to include most of them.We applied this annotation schema to the same12 meetings.
Initial reliability between two anno-tators on the single ISL meeting (chosen as it pre-sented a significantly more complex set of actionitems than others in this set) was encouraging.
Thebest agreement was achieved on timeframe utter-ances (?
= .86), with owner utterances slightlyless good (between ?
= .77), and agreement anddescription utterances worse but still accept-able (?
= .73).
Further annotation is in progress.5 ExperimentsWe trained individual classifiers for each of the utter-ance sub-classes, and cross-validated as before.
Foragreement utterances, we used a naive n-gramclassifier similar to that of (Webb et al, 2005) for di-alogue act detection, scoring utterances via a set ofmost predictive n-grams of length 1?3 and making aclassification decision by comparing the maximumscore to a threshold (where the n-grams, their scoresand the threshold are automatically extracted fromthe training data).
For owner, timeframe andtask description utterances, we used SVMsas before, using word unigrams as features (2- and3-grams gave no improvement ?
probably due to thesmall amount of training data).
Performance var-ied greatly by sub-class (see Table 2), with some(e.g.
agreement) achieving higher accuracy than thebaseline flat classifications, but others being worse.As there is now significantly less training data avail-able to each sub-class than there was for all utter-ances grouped together in the baseline experiment,worse performance might be expected; yet somesub-classes perform better.
The worst performingclass is owner.
Examination of the data showsthat owner utterances are more likely than otherclasses to be assigned to more than one category;they may therefore have more feature overlap withother classes, leading to less accurate classification.Use of relevant sub-strings for training (rather thanfull utterances) may help; as may part-of-speech in-formation ?
while proper names may be useful fea-tures, the name tokens themselves are sparse andmay be better substituted with a generic tag.Class Precision Recall F-Scoredescription 0.23 0.41 0.29owner 0.12 0.28 0.17timeframe 0.19 0.38 0.26agreement 0.48 0.44 0.40Table 2: Sub-class Classification PerformanceEven with poor performance for some of the sub-classifiers, we should still be able to combine themto get a benefit as long as their true positives cor-relate better than their false positives (intuitively, ifthey make mistakes in different places).
So far wehave only conducted an initial naive experiment, inwhich we combine the individual classifier decisionsin a weighted sum over a window (currently set to5 utterances).
If the sum over the window reachesa given threshold, we hypothesize an action item,and take the highest-confidence utterance given byeach sub-classifier in that window to provide thecorresponding property.
As shown in Table 3, thisgives reasonable performance on most meetings, al-though it does badly on meeting 5 (apparently be-cause no explicit agreement takes place, while ourmanual weights emphasized agreement).1 Most en-couragingly, the correct examples provide some use-ful ?best?
sub-class utterances, from which the rele-vant properties could be extracted.These results can probably be significantly im-proved: rather than sum over the binary classifica-tion outputs of each classifier, we can use their con-fidence scores or posterior probabilities, and learn1Accuracy here is currently assessed only over correct de-tection of an action item in a window, not correct assignment ofall sub-classes.33Mtg.
AIs Correct False+ False- F-Score1 3 2 1 1 0.672 4 1 0 3 0.403 5 2 1 3 0.504 4 4 0 0 1.005 3 0 1 3 0.00Table 3: Combined Classification Performancethe combination weights to give a more robust ap-proach.
There is still a long way to go to evaluatethis approach over more data, including the accu-racy and utility of the resulting sub-class utterancehypotheses.6 Discussion and Future WorkSo accounting for the structure of action items ap-pears essential to detecting them in spoken dis-course.
Otherwise, classification accuracy is lim-ited.
We believe that accuracy can be improved, andthe detected utterances can be used to provide theproperties of the action item itself.
An interestingquestion is how and whether the structure we usehere relates to discourse structure in more generaluse.
If a relation exists, this would shed light on thedecision-making process we are attempting to (be-gin to) model, and might allow us to use other (moreplentiful) annotated data.Our future efforts focus on annotating more meet-ings to obtain large training and testing sets.
We alsowish to examine performance when working fromspeech recognition hypotheses (as opposed to thehuman transcripts used here), and the best way to in-corporate multiple hypotheses (either as n-best listsor word confusion networks).
We are actively inves-tigating alternative approaches to sub-classifier com-bination: better performance (and a more robust andtrainable overall system) might be obtained by usinga Bayesian network, or a maximum entropy classi-fier as used by (Klein et al, 2002).
Finally, we aredeveloping an interface to a new large-vocabularyversion of the Gemini parser (Dowding et al, 1993)which will allow us to use semantic parse informa-tion as features in the individual sub-class classifiers,and also to extract entity and event representationsfrom the classified utterances for automatic additionof entries to calendars and to-do lists.ReferencesS.
Burger, V. MacLaren, and H. Yu.
2002.
The ISLMeet-ing Corpus: The impact of meeting type on speechstyle.
In Proceedings of the 6th International Confer-ence on Spoken Language Processing (ICSLP 2002).M.
Core and J. Allen.
1997.
Coding dialogues withthe DAMSL annotation scheme.
In D. Traum, edi-tor, AAAI Fall Symposium on Communicative Actionin Humans and Machines.S.
Corston-Oliver, E. Ringger, M. Gamon, and R. Camp-bell.
2004.
Task-focused summarization of email.
InProceedings of the Text Summarization Branches OutACL Workshop.J.
Dowding, J. M. Gawron, D. Appelt, J.
Bear, L. Cherny,R.
Moore, and D. Moran.
1993.
Gemini: A naturallanguage system for spoken language understanding.In Proc.
31st Annual Meeting of the Association forComputational Linguistics.A.
Gruenstein, J. Niekrasz, and M. Purver.
2005.
Meet-ing structure annotation: Data and tools.
In Proceed-ings of the 6th SIGdial Workshop on Discourse andDialogue.A.
Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,N.
Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-cke, and C.Wooters.
2003.
The ICSI Meeting Corpus.In Proc.
IEEE International Conference on Acoustics,Speech, and Signal Processing (ICASSP 2003).T.
Joachims.
1999.
Making large-scale SVM learningpractical.
In B. Scho?lkopf, C. Burges, and A. Smola,editors, Advances in Kernel Methods ?
Support VectorLearning.
MIT Press.D.
Klein, K. Toutanova, H. T. Ilhan, S. D. Kamvar, andC.
D.Manning.
2002.
Combining heterogeneous clas-sifiers for word-sense disambiguation.
In Proceedingsof the ACL Workshop on Word Sense Disambiguation:Recent Successes and Future Directions.W.
Morgan, S. Gupta, and P.-C. Chang.
forthcoming.Automatically detecting action items in audio meetingrecordings.
Ms., under review.E.
Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Car-vey.
2004.
The ICSI Meeting Recorder Dialog ActCorpus.
In Proceedings of the 5th SIGdial Workshopon Discourse and Dialogue.S.
Siegel and J. N. J. Castellan.
1988.
NonparametricStatistics for the Behavioral Sciences.
McGraw-Hill.V.
N. Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer.N.
Webb, M. Hepple, and Y. Wilks.
2005.
Dialogue actclassification using intra-utterance features.
In Proc.AAAI Workshop on Spoken Language Understanding.34
