Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 793?800,Sydney, July 2006. c?2006 Association for Computational LinguisticsOntologizing Semantic RelationsMarco PennacchiottiART Group - DISPUniversity of Rome ?Tor Vergata?Viale del Politecnico 1Rome, Italypennacchiotti@info.uniroma2.itPatrick PantelInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA90292pantel@isi.eduAbstractMany algorithms have been developedto harvest lexical semantic resources,however few have linked the minedknowledge into formal knowledge re-positories.
In this paper, we propose twoalgorithms for automatically ontologiz-ing (attaching) semantic relations intoWordNet.
We present an empiricalevaluation on the task of attaching part-of and causation relations, showing animprovement on F-score over a baselinemodel.1 IntroductionNLP researchers have developed many algo-rithms for mining knowledge from text and theWeb, including facts (Etzioni et al 2005), se-mantic lexicons (Riloff and Shepherd 1997),concept lists (Lin and Pantel 2002), and wordsimilarity lists (Hindle 1990).
Many recent ef-forts have also focused on extracting binary se-mantic relations between entities, such asentailments (Szpektor et al 2004), is-a (Ravi-chandran and Hovy 2002), part-of (Girju et al2003), and other relations.The output of most of these systems is flat listsof lexical semantic knowledge such as ?Italy is-acountry?
and ?orange similar-to blue?.
However,using this knowledge beyond simple keywordmatching, for example in inferences, requires itto be linked into formal semantic repositoriessuch as ontologies or term banks like WordNet(Fellbaum 1998).Pantel (2005) defined the task of ontologizinga lexical semantic resource as linking its terms tothe concepts in a WordNet-like hierarchy.
Forexample, ?orange similar-to blue?
ontologizes inWordNet to ?orange#2 similar-to blue#1?
and?orange#2 similar-to blue#2?.
In his framework,Pantel proposed a method of inducing ontologi-cal co-occurrence vectors 1  which are subse-quently used to ontologize unknown terms intoWordNet with 74% accuracy.In this paper, we take the next step and exploretwo algorithms for ontologizing binary semanticrelations into WordNet and we present empiricalresults on the task of attaching part-of and causa-tion relations.
Formally, given an instance(x, r, y) of a binary relation r between terms xand y, the ontologizing task is to identify theWordNet senses of x and y where r holds.
Forexample, the instance (proton, PART-OF, element)ontologizes into WordNet as (proton#1, PART-OF,element#2).The first algorithm that we explore, called theanchoring approach, was suggested as a promis-ing avenue of future work in (Pantel 2005).
Thisbottom up algorithm is based on the intuition thatx can be disambiguated by retrieving the set ofterms that occur in the same relation r with y andthen finding the senses of x that are most similarto this set.
The assumption is that terms occur-ring in the same relation will tend to have similarmeaning.
In this paper, we propose a measure ofsimilarity to capture this intuition.In contrast to anchoring, our second algorithm,called the clustering approach, takes a top-downview.
Given a relation r, suppose that we aregiven every conceptual instance of r, i.e., in-stances of r in the upper ontology like (parti-cles#1, PART-OF, substances#1).
An instance(x, r, y) can then be ontologized easily by findingthe senses of x and y that are subsumed by ances-tors linked by a conceptual instance of r. For ex-ample, the instance (proton, PART-OF, element)ontologizes to (proton#1, PART-OF, element#2)since proton#1 is subsumed by particles andelement#2 is subsumed by substances.
The prob-lem then is to automatically infer the set of con-1 The ontological co-occurrence vector of a concept con-sists of all lexical co-occurrences with the concept in acorpus.793ceptual instances.
In this paper, we develop aclustering algorithm for generalizing a set of re-lation instances to conceptual instances by look-ing up the WordNet hypernymy hierarchy forcommon ancestors, as specific as possible, thatsubsume as many instances as possible.
An in-stance is then attached to its senses that are sub-sumed by the highest scoring conceptualinstances.2 Relevant WorkSeveral researchers have worked on ontologizingsemantic resources.
Most recently, Pantel (2005)developed a method to propagate lexical co-occurrence vectors to WordNet synsets, formingontological co-occurrence vectors.
Adopting anextension of the distributional hypothesis (Harris1985), the co-occurrence vectors are used tocompute the similarity between synset/synset andbetween lexical term/synset.
An unknown term isthen attached to the WordNet synset whose co-occurrence vector is most similar to the term?sco-occurrence vector.
Though the author sug-gests a method for attaching more complex lexi-cal structures like binary semantic relations, thepaper focused only on attaching terms.Basili (2000) proposed an unsupervisedmethod to infer semantic classes (WordNet syn-sets) for terms in domain-specific verb relations.These relations, such as (x, EXPAND, y) are firstautomatically learnt from a corpus.
The semanticclasses of x and y are then inferred using concep-tual density (Agirre and Rigau 1996), a Word-Net-based measure applied to all instantiation ofx and y in the corpus.
Semantic classes representpossible common generalizations of the verb ar-guments.
At the end of the process, a set of syn-tactic-semantic patterns are available for eachverb, such as:(social_group#1, expand, act#2)(instrumentality#2, expand, act#2)The method is successful on specific relationswith few instances (such as domain verb rela-tions) while its value on generic and frequentrelations, such as part-of, was untested.Girju et al (2003) presented a highly super-vised machine learning algorithm to infer seman-tic constraints on part-of relations, such as(object#1, PART-OF, social_event#1).
These con-straints are then used as selectional restrictions inharvesting part-of instances from ambiguouslexical patterns, like ?X of Y?.
The approachshows high performance in terms of precisionand recall, but, as the authors acknowledge, itrequires large human effort during the trainingphase.Others have also made significant additions toWordNet.
For example, in eXtended WordNet(Harabagiu et al 1999), the glosses in WordNetare enriched by disambiguating the nouns, verbs,adverbs, and adjectives with synsets.
Anotherwork has enriched WordNet synsets with topi-cally related words extracted from the Web(Agirre et al 2001).
Finally, the general task ofword sense disambiguation (Gale et al 1991) isrelevant since there the task is to ontologize eachterm in a passage into a WordNet-like sense in-ventory.
If we had a large collection of sense-tagged text, then our mining algorithms coulddirectly discover WordNet attachment points atharvest time.
However, since there is little highprecision sense-tagged corpora, methods are re-quired to ontologize semantic resources withoutfully disambiguating text.3 Ontologizing Semantic RelationsGiven an instance (x, r, y) of a binary relation rbetween terms x and y, the ontologizing task is toidentify the senses of x and y where r holds.
Inthis paper, we focus on WordNet 2.0 senses,though any similar term bank would apply.Let Sx and Sy be the sets of all WordNet sensesof x and y.
A sense pair, sxy, is defined as anypair of senses of x and y: sxy={sx, sy} where sx?Sxand sy?Sy.
The set of all sense pairs Sxy consistsof all permutations between senses in Sx and Sy.In order to attach a relation instance (x, r, y)into WordNet, one must:?
Disambiguate x and y, that is, find the subsetsS'x?Sx and S'y?Sy for which the relation r holds;and?
Instantiate the relation in WordNet, using thesynsets corresponding to all correct permuta-tions between the senses in S'x and S'y.
We de-note this set of attachment points as S'xy.If Sx or Sy is empty, no attachments are produced.For example, the instance (study, PART-OF, re-port) is ontologized into WordNet through thesenses S'x={survey#1, study#2} andS?y={report#1}.
The final attachment points S'xyare:(survey#1, PART-OF, report#1)(study#1, PART-OF, report#1)Unlike common algorithms for word sensedisambiguation, here it is important to take intoconsideration the semantic dependency betweenthe two terms x and y.
For example, an entity thatis part-of a study has to be some kind of informa-794tion.
This knowledge about mutual selectionalpreference (the preferred semantic class that fillsa certain relation role, as x or y) can be exploitedto ontologize the instance.In the following sections, we propose two al-gorithms for ontologizing binary semantic rela-tions.3.1 Method 1: Anchor ApproachGiven an instance (x, r, y), this approach fixes theterm y, called the anchor, and then disambiguatesx by looking at all other terms that occur in therelation r with y.
Based on the principle of distri-butional similarity (Harris 1985), the algorithmassumes that the words that occur in the samerelation r with y will be more similar to the cor-rect sense(s) of x than the incorrect ones.
Afterdisambiguating x, the process is then invertedwith x as the anchor to disambiguate y.In the first step, y is fixed and the algorithmretrieves the set of all other terms X' that occur inan instance (x', r, y), x' ?
X'2.
For example, giventhe instance (reflections, PART-OF, book), and aresource containing the following relations:(false allegations, PART-OF, book)(stories, PART-OF, book)(expert analysis, PART-OF, book)(conclusions, PART-OF, book)the resulting set X' would be: {allegations, sto-ries, analysis, conclusions}.All possible permutations, Sxx', between thesenses of x and the senses of each term in X',called Sx', are computed.
For each sense pair{sx, sx'} ?
Sxx', a similarity score r(sx, sx') is calcu-lated using WordNet:)(1),(1),( ''' xxxxx sfssdssr ?+=where the distance d(sx, sx') is the length of theshortest path connecting the two synsets in thehypernymy hierarchy of WordNet, and f(sx') isthe number of times sense sx' occurs in any of theinstances of X'.
Note that if no connection be-tween two synsets exists, then r(sx, sx') = 0.The overall sense score for each sense sx of xis calculated as:?
?=''),()( 'xx Ssxxx ssrsrFinally, the algorithm inverts the process bysetting x as the anchor and computes r(sy) for2 For semantic relations between complex terms, like (ex-pert analysis, PART-OF, book), only the head noun of termsare recorded, like ?analysis?.
As a future work, we plan touse the whole term if it is present in WordNet.each sense of y.
All possible permutations ofsenses are computed and scored by averagingr(sx) and r(sy).
Permutations scoring higher than athreshold ?1 are selected as the attachment pointsin WordNet.
We experimentally set ?1 = 0.02.3.2 Method 2: Clustering ApproachThe main idea of the clustering approach is toleverage the lexical behaviors of the two terms inan instance as a whole.
The assumption is thatthe general meaning of the relation is derivedfrom the combination of the two terms.The algorithm is divided in two main phases.In the first phase, semantic clusters are built us-ing the WordNet senses of all instances.
A se-mantic cluster is defined by the set of instancesthat have a common semantic generalization.
Wedenote the conceptual instance of the semanticcluster as the pair of WordNet synsets that repre-sents this generalization.
For example the follow-ing two part-of instances:(second section, PART-OF, Los Angeles-area news)(Sandag study, PART-OF, report)are in a common cluster represented by the fol-lowing conceptual instance:[writing#2, PART-OF, message#2]since writing#2 is a hypernym of both sectionand study, and message#2 is a hypernym of newsand report3.In the second phase, the algorithm attaches aninstance into WordNet by using WordNet dis-tance metrics and frequency scores to select thebest cluster for each instance.
A good cluster isone that:?
achieves a good trade-off between generalityand specificity; and?
disambiguates among the senses of x and y us-ing the other instances?
senses as support.For example, given the instance (second section,PART-OF, Los Angeles-area news) and the follow-ing conceptual instances:[writing#2, PART-OF, message#2][object#1, PART-OF, message#2][writing#2, PART-OF, communication#2][social_group#1, PART-OF, broadcast#2][organization#, PART-OF, message#2]the first conceptual instance should be scoredhighest since it is both not too generic nor toospecific and is supported by the instance (Sandagstudy, PART-OF, report), i.e., the conceptual in-stance subsumes both instances.
The second and3 Again, here, we use the syntactic head of each term forgeneralization since we assume that it drives the meaningof the term itself.795the third conceptual instances should be scoredlower since they are too generic, while the lasttwo should be scored lower since the sense forsection and news are not supported by other in-stances.
The system then outputs, for each in-stance, the set of sense pairs that are subsumedby the highest scoring conceptual instance.
In theprevious example:(section#1, PART-OF, news#1)(section#1, PART-OF, news#2)(section#1, PART-OF, news#3)are selected, as they are subsumed by [writing#2,PART-OF, message#2].
These sense pairs are thenretained as attachment points into WordNet.Below, we describe each phase in more detail.Phase 1: Cluster BuildingGiven an instance (x, r, y), all sense pair permu-tations sxy={sx, sy} are retrieved from WordNet.A set of candidate conceptual instances, Cxy, isformed for each instance from the permutation ofeach WordNet ancestor of sx and sy, following thehypernymy link, up to degree ?2.Each candidate conceptual instance,c={cx, cy}, is scored by its degree of generaliza-tion as follows:)1()1(1)( +?+= yx nncrwhere ni is the number of hypernymy linksneeded to go from si to ci, for i ?
{x, y}.
r(c)ranges from [0, 1] and is highest when little gen-eralization is needed.For example, the instance (Sandag study,PART-OF, report) produces 70 sense pairs sincestudy has 10 senses and report has 7 senses.
As-suming ?2=1, the instance sense (survey#1, PART-OF, report#1) has the following set of candidateconceptual instances:Cxy nx ny r(c)(survey#1, PART-OF,report#1) 0 0 1(survey#1, PART-OF,document#1) 0 1 0.5(examination#1, PART-OF,report#1) 1 0 0.5(examination#1, PART-OF,document#1) 1 1 0.25Finally, each candidate conceptual instance cforms a cluster of all instances (x, r, y) that havesome sense pair sx and sy as hyponyms of c. Notealso that candidate conceptual instances may besubsumed by other candidate conceptual in-stances.
Let Gc refer to the set of all candidateconceptual instances subsumed by candidateconceptual instance c.Intuitively, better candidate conceptual in-stances are those that subsume both many in-stances and other candidate conceptual instances,but at the same time that have the least distancefrom subsumed instances.
We capture this intui-tion with the following score of c:cccGg GIGgrcscore c loglog)()( ??=?
?where Ic is the set of instances subsumed by c.We experimented with different variations of thisscore and found that it is important to put moreweight on the distance between subsumed con-ceptual instances than the actual number of sub-sumed instances.
Without the log terms, thehighest scoring conceptual instances are too ge-neric (i.e., they are too high up in the ontology).Phase 2: Attachment Points SelectionIn this phase, we utilize the conceptual instancesof the previous phase to attach each instance(x, r, y) into WordNet.At the end of Phase 1, an instance can be clus-tered in different conceptual instances.
In orderto select an attachment, the algorithm selects thesense pair of x and y that is subsumed by thehighest scoring candidate conceptual instance.
Itand all other sense pairs that are subsumed bythis conceptual instance are then retained as thefinal attachment points.As a side effect, a final set of conceptual in-stances is obtained by deleting from each candi-date those instances that are subsumed by ahigher scoring conceptual instance.
Remainingconceptual instances are then re-scored usingscore(c).
The final set of conceptual instancesthus contains unambiguous sense pairs.4 Experimental ResultsIn this section we provide an empirical evalua-tion of our two algorithms.4.1 Experimental SetupResearchers have developed many algorithms forharvesting semantic relations from corpora andthe Web.
For the purposes of this paper, we maychoose any one of them and manually validate itsmined relations.
We choose Espresso4, a general-purpose, broad, and accurate corpus harvestingalgorithm requiring minimal supervision.
Adopt-4 Reference suppressed ?
the paper introducing Espressohas also been submitted to COLING/ACL 2006.796ing a bootstrapping approach, Espresso takes asinput a few seed instances of a particular relationand iteratively learns surface patterns to extractmore instances.Test SetsWe experiment with two relations: part-of andcausation.
The causation relation occurs when anentity produces an effect or is responsible forevents or results, for example (virus, CAUSE, in-fluenza) and (burning fuel, CAUSE, pollution).
Wemanually built five seed relation instances forboth relations and apply Espresso to a datasetconsisting of a sample of articles from theAquaint (TREC-9) newswire text collection.
Thesample consists of 55.7 million words extractedfrom the Los Angeles Times data files.
Espressoextracted 1,468 part-of instances and 1,129 cau-sation instances.
We manually validated the out-put and randomly selected 200 correct relationinstances of each relation for ontologizing intoWordNet 2.0.Gold StandardWe manually built a gold standard of all correctattachments of the test sets in WordNet.
For eachrelation instance (x, r, y), two human annotatorsselected from all sense permutations of x and ythe correct attachment points in WordNet.
Forexample, for (synthetic material, PART-OF, filter),the judges selected the following attachmentpoints: (synthetic material#1, PART-OF, filter#1)and (synthetic material#1, PART-OF, filter#2).
Thekappa statistic (Siegel and Castellan Jr. 1988) onthe two relations together was ?
= 0.73.SystemsThe following three systems are evaluated:?
BL: the baseline system that attaches each rela-tion instance to the first (most common)WordNet sense of both terms;?
AN: the anchor approach described in Section3.1.?
CL: the clustering approach described in Sec-tion 3.2.4.2 Precision, Recall and F-scoreFor both the part-of and causation relations, weapply the three systems described above andcompare their attachment performance using pre-cision, recall, and F-score.
Using the manuallybuilt gold standard, the precision of a system on agiven relation instance is measured as the per-centage of correct attachments and recall ismeasured as the percentage of correct attach-ments retrieved by the system.
Overall systemprecision and recall are then computed by aver-aging the precision and recall of each relationinstance.Table 1 and Table 2 report the results on thepart-of and causation relations.
We experimen-tally set the CL generalization parameter ?2 to 5and the ?1 parameter for AN to 0.02.4.3 DiscussionFor both relations, CL and AN outperform thebaseline in overall F-score.
For part-of, Table 1shows that CL outperforms BL by 13.6% in F-score and AN by 9.4%.
For causation, Table 2shows that AN outperforms BL by 4.4% on F-score and CL by 0.6%.The good results of the CL method on thepart-of relation suggest that instances of this rela-tion are particularly amenable to be clustered.The generality of the part-of relation in fact al-lows the creation of fairly natural clusters, corre-sponding to different sub-types of part-of, asthose proposed in (Winston 1983).
The causationrelation, however, being more difficult to defineat a semantic level (Girju 2003), is less easy tocluster and thus to disambiguate.Both CL and AN have better recall than BL,but precision results vary with CL beating BLonly on the part-of relation.
Overall, the systemperformances suggest that ontologizing semanticrelations into WordNet is in general not easy.The better results of CL and AN with respectto BL suggest that the use of comparative seman-tic analysis among corpus instances is a goodway to carry out disambiguation.
Yet, the BLSYSTEM PRECISION RECALL F-SCOREBL 45.0% 25.0% 32.1%AN 41.7% 32.4% 36.5%CL 40.0% 32.6% 35.9%Table 2.
System precision, recall and F-score onthe causation relation.SYSTEM PRECISION RECALL F-SCOREBL 54.0% 31.3% 39.6%AN 40.7% 47.3% 43.8%CL 57.4% 49.6% 53.2%Table 1.
System precision, recall and F-score onthe part-of relation.797method shows surprisingly good results.
Thisindicates that also a simple method based onword sense usage in language can be valuable.An interesting avenue of future work is to bettercombine these two different views in a singlesystem.The low recall results for CL are mostly at-tributed to the fact that in Phase 2 only the bestscoring cluster is retained for each instance.
Thismeans that instances with multiple senses that donot have a common generalization are not cap-tured.
For example the part-of instance (wings,PART-OF, chicken) should cluster both in[body_part#1, PART-OF, animal#1] and[body_part#1, PART-OF, food#2], but only thebest scoring one is retained.5 Conceptual Instances: Other UsesOur clustering approach from Section 3.2 is en-abled by learning conceptual instances ?
relationsbetween mid-level ontological concepts.
Beyondthe ontologizing task, conceptual instances maybe useful for several other tasks.
In this section,we discuss some of these opportunities and pre-sent small qualitative evaluations.Conceptual instances represent common se-mantic generalizations of a particular relation.For example, below are two possible conceptualinstances for the part-of relation:[person#1, PART-OF, organization#1][act#1, PART-OF, plan#1]The first conceptual instance in the example sub-sumes all the part-of instances in which one ormore persons are part of an organization, such as:(president Brown, PART-OF, executive council)(representatives, PART-OF, organization)(students, PART-OF, orchestra)(players, PART-OF, Metro League)Below, we present three possible ways of ex-ploiting these conceptual instances.Support to Relation Extraction ToolsConceptual instances may be used to support re-lation extraction algorithms such as Espresso.Most minimally supervised harvesting algo-rithm do not exploit generic patterns, i.e.
thosepatterns with high recall but low precision, sincethey cannot separate correct and incorrect rela-tion instances.
For example, the pattern ?X of Y?extracts many correct relation instances like?wheel of the car?
but also many incorrect oneslike ?house of representatives?.Girju et al (2003) described a highly super-vised algorithm for learning semantic constraintson generic patterns, leading to a very significantincrease in system recall without deterioratingprecision.
Conceptual instances can be used toautomatically learn such semantic constraints byacting as a filter for generic patterns, retainingonly those instances that are subsumed by highscoring conceptual instances.
Effectively, con-ceptual instances are used as selectional restric-tions for the relation.
For example, our systemdiscards the following incorrect instances:(week, CAUSE, coalition)(demeanor, CAUSE, vacuum)as they are both part of the very low scoring con-ceptual instance [abstraction#6, CAUSE, state#1].Ontology Learning from TextEach conceptual instance can be viewed as aformal specification of the relation at hand.
Forexample, Winston (1983) manually identified sixsub-types of the part-of relation: member-collection, component-integral object, portion-mass, stuff-object, feature-activity and place-area.
Such classifications are useful in applica-tions and tasks where a semantically rich organi-zation of knowledge is required.
Conceptualinstances can be viewed as an automatic deriva-tion of such a classification based on corpus us-age.
Moreover, conceptual instances can be usedto improve the ontology learning process itself.For example, our clustering approach can beseen as an inductive step producing conceptualinstances that are then used in a deductive step tolearn new instances.
An algorithm could iteratebetween the induction/deduction cycle until nonew relation instances and conceptual instancescan be inferred.Word Sense DisambiguationWord Sense Disambiguation (WSD) systems canexploit the selectional restrictions identified byconceptual instances to disambiguate ambiguousterms occurring in particular contexts.
For exam-ple, given the sentence:?the board is composed by members of different countries?and a harvesting algorithm that extracts the part-of relation (members, PART-OF, board), the sys-tem could infer the correct senses for board andmembers by looking at their closest conceptualinstance.
In our system, we would infer the at-tachment (member#1, PART-OF, board#1) since itis part of the highest scoring conceptual instance[person#1, PART-OF, organization#1].7985.1 Qualitative EvaluationTable 3 and Table 4 list samples of the highestranking conceptual instances obtained by oursystem for the part-of and causation relations.Below we provide a small evaluation to verify:?
the correctness of the conceptual instances.Incorrect conceptual instances such as [attrib-ute#2, CAUSE, state#4], discovered by our sys-tem, can impede WSD and extraction toolswhere precise selectional restrictions areneeded; and?
the accuracy of the conceptual instances.Sometimes, an instance is incorrectly attachedto a correct conceptual instance.
For example,the instance (air mass, PART-OF, cold front) isincorrectly clustered in [group#1, PART-OF,multitude#3] since mass and front both have asense that is descendant of group#1 and multi-tude#3.
However, these are not the correctsenses of mass and front for which the part-ofrelation holds.For evaluating correctness, we manually ver-ify how many correct conceptual instances areproduced by Phase 2 of the clustering approachdescribed in Section 3.2.
The claim is that a cor-rect conceptual instance is one for which the re-lation holds for all possible subsumed senses.
Forexample, the conceptual instance [group#1,PART-OF, multitude#3] is correct, as the relationholds for every semantic subsumption of the twosenses.
An example of an incorrect conceptualinstance is [state#4, CAUSE, abstraction#6] sinceit subsumes the incorrect instance (audience,CAUSE, new context).
A manual evaluation of thehighest scoring 200 conceptual instances, gener-ated on our test sets described in Section 4.1,showed 82% correctness for the part-of relationand 86% for causation.For estimating the overall clustering accuracy,we evaluated the number of correctly clusteredinstances in each conceptual instance.
For exam-ple, the instance (business people, PART-OF,committee) is correctly clustered in [multitude#3,PART-OF, group#1] and the instance (law, PART-OF, constitutional pitfalls) is incorrectly clusteredin [group#1, PART-OF, artifact#1].
We estimatedthe overall accuracy by manually judging theinstances attached to 10 randomly sampled con-ceptual instances.
The accuracy for part-of is84% and for causation it is 76.6%.6 ConclusionsIn this paper, we proposed two algorithms forautomatically ontologizing binary semantic rela-tions into WordNet: an anchoring approach anda clustering approach.
Experiments on the part-of and causation relations showed promising re-sults.
Both algorithms outperformed the baselineon F-score.
Our best results were on the part-ofrelation where the clustering approach achieved13.6% higher F-score than the baseline.The induction of conceptual instances hasopened the way for many avenues of futurework.
We intend to pursue the ideas presented inSection 5 for using conceptual instances to:i) support knowledge acquisition tools by learn-ing semantic constraints on extracting patterns;ii) support ontology learning from text; and iii)improve word sense disambiguation through se-lectional restrictions.
Also, we will try differentsimilarity score functions for both the clusteringand the anchor approaches, as those surveyed inCorley and Mihalcea (2005).CONCEPTUAL INSTANCE SCORE # INSTANCES INSTANCES[multitude#3, PART-OF, group#1] 2.04 10(ordinary people, PART-OF, Democratic Revolutionary Party)(unlicensed people, PART-OF, underground economy)(young people, PART-OF, commission)(air mass, PART-OF, cold front)[person#1, PART-OF, organization#1] 1.71 43(foreign ministers, PART-OF, council)(students, PART-OF, orchestra)(socialists, PART-OF, Iraqi National Joint Action Committee)(players, PART-OF, Metro League)[act#2, PART-OF, plan#1] 1.60 16(major concessions, PART-OF, new plan)(attacks, PART-OF, coordinated terrorist plan)(visit, PART-OF, exchange program)(survey, PART-OF, project)[communication#2, PART-OF, book#1] 1.14 10(hints, PART-OF, booklet)(soup recipes, PART-OF, book)(information, PART-OF, instruction manual)(extensive expert analysis, PART-OF, book)[compound#2, PART-OF, waste#1] 0.57 3(salts, PART-OF, powdery white waste)(lime, PART-OF, powdery white waste)(resin, PART-OF, waste)Table 3.
Sample of the highest scoring conceptual instances learned for the part-of relation.
For eachconceptual instance, we report the score(c), the number of instances, and some example instances.799The algorithms described in this paper may beapplied to ontologize many lexical resources ofsemantic relations, no matter the harvesting algo-rithm used to mine them.
In doing so, we havethe potential to quickly enrich our ontologies,like WordNet, thus reducing the knowledge ac-quisition bottleneck.
It is our hope that we will beable to leverage these enriched resources, albeitwith some noisy additions, to improve perform-ance on knowledge-rich problems such as ques-tion answering and textual entailment.ReferencesAgirre, E. and Rigau, G. 1996.
Word sensedisambiguation using conceptual density.
InProceedings of COLING-96.
pp.
16-22.
Copenhagen,Danmark.Agirre, E.; Ansa, O.; Martinez, D.; and Hovy, E. 2001.Enriching WordNet concepts with topic signatures.
InProceedings of NAACL Workshop on WordNet andOther Lexical Resources: Applications, Extensionsand Customizations.
Pittsburgh, PA.Basili, R.; Pazienza, M.T.
; and Vindigni, M. 2000.Corpus-driven learning of event recognition rules.
InProceedings of Workshop on Machine Learning andInformation Extraction (ECAI-00).Corley, C. and Mihalcea, R. 2005.
Measuring theSemantic Similarity of Texts.
In Proceedings of theACL Workshop on Empirical Modelling of SemanticEquivalence and Entailment.
Ann Arbor, MI.Etzioni, O.; Cafarella, M.J.; Downey, D.; Popescu, A.-M.; Shaked, T.; Soderland, S.; Weld, D.S.
; and Yates,A.
2005.
Unsupervised named-entity extraction fromthe Web: An experimental study.
ArtificialIntelligence, 165(1): 91-134.Fellbaum, C. 1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.Gale, W.; Church, K.; and Yarowsky, D. 1992.
Amethod for disambiguating word senses in a largecorpus.
Computers and Humanities, 26:415-439.Girju, R.; Badulescu, A.; and Moldovan, D. 2003.Learning semantic constraints for the automaticdiscovery of part-whole relations.
In Proceedings ofHLT/NAACL-03.
pp.
80-87.
Edmonton, Canada.Girju, R. 2003.
Automatic Detection of Causal Relationsfor Question Answering.
In Proceedings of ACLWorkshop on Multilingual Summarization andQuestion Answering.
Sapporo, Japan.Harabagiu, S.; Miller, G.; and Moldovan, D. 1999.WordNet 2 - A Morphologically and SemanticallyEnhanced Resource.
In Proceedings of SIGLEX-99.pp.1-8.
University of Maryland.Harris, Z.
1985.
Distributional structure.
In: Katz, J.
J.(ed.)
The Philosophy of Linguistics.
New York:Oxford University Press.
pp.
26?47.Hindle, D. 1990.
Noun classification from predicate-argument structures.
In Proceedings of ACL-90.
pp.268?275.
Pittsburgh, PA.Lin, D. and Pantel, P. 2002.
Concept discovery from text.In Proceedings of COLING-02.
pp.
577-583.
Taipei,Taiwan.Pantel, P. 2005.
Inducing Ontological Co-occurrenceVectors.
In Proceedings of ACL-05.
pp.
125-132.
AnnArbor, MI.Ravichandran, D. and Hovy, E.H. 2002.
Learning surfacetext patterns for a question answering system.
InProceedings of ACL-2002.
pp.
41-47.
Philadelphia,PA.Riloff, E. and Shepherd, J.
1997.
A corpus-basedapproach for building semantic lexicons.
InProceedings of EMNLP-97.Siegel, S. and Castellan Jr., N. J.
1988.
NonparametricStatistics for the Behavioral Sciences.
McGraw-Hill.Szpektor, I.; Tanev, H.; Dagan, I.; and Coppola, B.
2004.Scaling web-based acquisition of entailment relations.In Proceedings of EMNLP-04.
Barcelona, Spain.Winston, M.; Chaffin, R.; and Hermann, D. 1987.
Ataxonomy of part-whole relations.
Cognitive Science,11:417?444.CONCEPTUAL INSTANCE SCORE # INSTANCES INSTANCES[change#3, CAUSE, state#4] 1.49 17(separation, CAUSE, anxiety)(demotion, CAUSE, roster vacancy)(budget cuts, CAUSE, enrollment declines)(reduced flow, CAUSE, vacuum)[act#2, CAUSE, state#3] 0.81 20(oil drilling, CAUSE, air pollution)(workplace exposure, CAUSE, genetic injury)(industrial emissions, CAUSE, air pollution)(long recovery, CAUSE, great stress)[person#1, CAUSE, act#2] 0.64 12(homeowners, CAUSE, water waste)(needlelike puncture, CAUSE, physician)(group member, CAUSE, controversy)(children, CAUSE, property damage)[organism#1, CAUSE, disease#1] 0.03 4(parasites, CAUSE, pneumonia)(virus, CAUSE, influenza)(chemical agents, CAUSE, pneumonia)(genetic mutation, CAUSE, Dwarfism)Table 4.
Sample of the highest scoring conceptual instances learned for the causation relation.
Foreach conceptual instance, we report score(c) , the number of instances, and some example instances.800
