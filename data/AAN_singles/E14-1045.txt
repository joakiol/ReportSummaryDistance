Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 424?433,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsDependency Tree Abstraction for Long-Distance Reordering inStatistical Machine TranslationChenchen DingDepartment of Computer ScienceUniversity of Tsukuba1-1-1 Tennodai, Tsukuba, Ibaraki , Japantei@mibel.cs.tsukuba.ac.jpYuki AraseMicrosoft ResearchNo.
5 Danling St., Haidian Dist.Beijing, P.R.
Chinayukiar@microsoft.comAbstractWord reordering is a crucial techniquein statistical machine translation in whichsyntactic information plays an importantrole.
Synchronous context-free gram-mar has typically been used for this pur-pose with various modifications for addingflexibilities to its synchronized tree gen-eration.
We permit further flexibilitiesin the synchronous context-free grammarin order to translate between languageswith drastically different word order.
Ourmethod pre-processes a parallel corpus byabstracting source-side dependency trees,and performs long-distance reordering ontop of an off-the-shelf phrase-based sys-tem.
Experimental results show that ourmethod significantly outperforms previousphrase-based and syntax-based models fortranslation between English and Japanese.1 IntroductionSince the inception of statistical machine trans-lation (SMT), long-distance word reordering hasbeen a notable challenge, particularly when trans-lating between languages with drastically differentword orders, such as subject-verb-object (SVO)and subject-object-verb (SOV) languages like En-glish and Japanese, respectively.
Phrase-basedmodels (Koehn et al., 2003; Och and Ney, 2004;Xiong et al., 2006) have been strong in localtranslation and reordering.
However, phrase-basedmodels cannot effectively conduct long-distancereordering because they are based purely on statis-tics of syntax-independent phrases.
As a comple-mentary approach to phrase-based models, someresearchers have incorporated syntactic informa-tion into an SMT framework (Wu, 1997; Yamadaand Knight, 2001; Liu et al., 2006) using syn-chronous context-free grammar (SCFG) (Aho andwhen the fluid pressure cylinder 31 is used, fluid is gradually applied.,isgraduallyapplied.
fluidwhenisused[N]thefluidpressurecylinder31[N][X][X](root)Figure 1: English abstraction tree exampleUllman, 1972).
The original SCFG assumes thatthe syntactic trees of the source and target lan-guages can be derived synchronously.
However,this assumption is too strict for handling paral-lel sentences that are often comparable rather thanparallel.
For alleviating this assumption, some re-searchers have added flexibilities in synchronizedtree generation (Wu, 1997; Burkett et al., 2010).In addition, in the SMT framework, there is anapproach that alleviates the assumption by onlygenerating the source-side syntactic tree and pro-jecting it to the target-side sentence (Yamada andKnight, 2001; Liu et al., 2006).In practice, these existing methods are not flex-ible enough to handle parallel sentence pairs, es-pecially those of SVO and SOV languages.
There-fore, we permit further flexibility in SCFG aimingto effectively conduct long-distance reordering.We design our method as a pre-processing proce-dure so that we can use a well-developed phrase-based system without adding heavy computationalcomplexity to the system.
Specifically, we proposean abstraction tree that is a shallow and nestedrepresentation, i.e., abstraction of the dependencytree as Fig.
1 depicts.
Our method pre-processes aparallel corpus by generating source-side abstrac-tion trees and projecting the trees onto the target-side sentences.
It then decomposes the corpusby collecting corresponding node pairs as a newcorpus, and finally trains the phrase-based model.In this manner, the source-side grammar is deter-mined on the fly for each sentence based on a de-424pendency parse of the source sentence.
The targetside of each production in the grammar is deter-mined by running the phrase-based decoder.We empirically show effectiveness of ourmethod for English-to-Japanese and Japanese-to-English translations by comparing it to phrase-based and syntax-based models.
Experimental re-sults show that our method significantly outper-forms the previous methods with respect to theBLEU (Papineni et al., 2002) metric.2 Related WorkFor adding flexibilities to SCFG under an SMTscenario, previous studies generate only a source-side syntactic tree and project it to the target-sidesentence regardless of the true target-side syntacticstructure.
Liu et al.
(2006) propose a tree-to-stringmodel using a source-side constituency tree to ex-tract correspondences between the source-side treeand the target-side sentence.
Quirk et al.
(2005)and Xie et al.
(2011) use a dependency tree for thesame purpose.
Since these methods project a fine-grained source-side syntax tree, an accurate pro-jection is possible only when the target-side sen-tence has a syntactic structure that is similar tothe source-side.
Zhu and Xiao (2011) and Huangand Pendus (2013) generalize rules obtained bythe tree-to-string model to increase the chance ofrule matching at decoding.
Despite their merits,none of these methods resolves the problem of treeprojection to the target-side.The hierarchical phrase-based model (HIERO)proposed by Chiang (2007) is independent of anysyntactic information and generates SCFG rulesonly from parallel sentence pairs.
Li et al.
(2012)and Feng et al.
(2012) incorporate syntactic infor-mation into HIERO as soft constraints.
Since thesemethods are bound by the original HIERO rulesthat are independent of syntactic information, theirrules cannot represent the global syntactic struc-ture of a sentence.There are also pre-reordering methods for long-distance reordering in SVO-to-SOV translationsusing heuristics designed based on source-sidesyntactic structures (Xu et al., 2009; Isozaki et al.,2010; Isozaki et al., 2012).
They are fine-tuned tohandle only specific reordering problems in a pre-determined language pair.
Another approach is tostatistically learn pre-reordering rules from a cor-pus; however, this requires a highly parallel train-ing corpus consisting of literal translations to learnAlgorithm 1 CKY-style decodingInput: Input sentence u and its dependency tree ru, transla-tion model TM , block-LM bLM , sentence-LM sLM ,size of m-best m1: ?u?
generate abstraction tree of u using ru2: NodeTrans[][]?
?3: for all node in ?udo4: m-best?
Decode(node, TM, bLM,m)5: (start, end)?
start and end indices of node in u6: NodeTrans[start][end]?
m-best7: end for8: for start := 1 to |u| do9: for span := 0 to |u| ?
1 do10: end?
start+ span11: ChildTrans[]?
?12: for all (i, j) such that start ?
i ?
j ?
end do13: if NodeTrans[i][j] 6= ?
then14: add NodeTrans[i][j] to ChildTrans15: end if16: end for17: CubePruning(NodeTrans[start][end],ChildTrans, sLM,m)18: end for19: end foreffective rules (Neubig et al., 2012; Navratil et al.,2012).
Such a training dataset is not widely avail-able in many languages.3 Overview of the Proposed MethodOur method pre-processes sentences in a paral-lel corpus based on source-side abstraction trees.It first generates an abstraction tree ?sof a source-side sentence s by abstracting its dependency treers: (s, rs) ?
?s.
It then projects the tree to thetarget-side sentence t for generating a target-sideabstraction tree ?tthat has exactly the same struc-ture to ?s, i.e., (?s, t) ?
?t.
The abstraction-tree generation process can be adapted to translat-ing languages by specifying source-side part-of-speeches (POSs) as input.
Abstraction tree struc-tures also depend on the dependency grammar thata parser uses.
In this study, we assume commonlyused Stanford typed dependency (de Marneffe etal., 2006) for English and the chunk-based depen-dency with ipadic (Asahara and Matsumoto, 2003)for Japanese.
Investigation of effects of differentdependency grammars is our future work.We decompose the sentence pair into node pairsaccording to correspondences between the sourceand target abstraction trees, and generate a newcorpus referred to as a block-corpus (Fig.
6).
Us-ing the block-corpus and the original corpus, wetrain a phrase-based model.
Its translation modelis trained with the block-corpus, and two target-side language models (LMs) are trained with theblock-corpus (referred to as block-LM) and the425a pressure cylinder usedhigh ishot waterFigure 2: [N] node detection exampleoriginal corpus (referred to as sentence-LM), re-spectively.
Thus the sentence-LM can be trainedusing a larger-scale monolingual corpus.
Com-pared to previous methods that also decomposesentence pairs (Xu et al., 2005; Sudoh et al., 2010),our method is more syntax-oriented.In decoding, we adopt the parsing algorithmwith cube-pruning (Huang and Chiang, 2005) intoa phrase-based decoder to translate the abstrac-tion tree of an input sentence efficiently.
AsAlgorithm 1 shows, our decoder first generatesthe abstraction tree of the input sentence (line1), and independently translates each node us-ing the block-LM that models ordering amongnon-terminals and lexical words (line 3?7).
Itthen combines the m-best translation hypothesesof each node to construct a sentence-level trans-lation (line 8?19).
Specifically, we insert sets ofthe m-best translation hypotheses of child nodesinto the m-best hypotheses of their parent nodeby replacing the corresponding non-terminals us-ing the cube-pruning (line 17).
The ordering ofthese child nodes has been determined in theirparent node by the phrase-based model that re-gards non-terminals only as single words.
Bydoing so, long-distance reordering is solved con-sidering the global syntactic structure and con-texts (lexical strings) preserved in the node.
Incube-pruning, we use the sentence-LM to com-pose fluent sentence-level translation.
The block-LM and sentence-LM scores are treated as inde-pendent features.The computational complexity of our decoder(line 3?19) is O(|N |C), where |N | is the numberof nodes in the abstraction-tree and C is a con-stant representing the complexity of phrase-baseddecoder and cube-pruning.
Since combinations ofhypotheses in cube-pruning are determined by theabstraction-tree in our method, the computationalcost is significantly smaller than HIERO?s case.4 Abstraction Tree GenerationIn this section, we provide the formal definition ofan abstraction tree and the generation method.4.1 Definition of Abstraction TreeWe define an abstraction tree as ?
= {N , E},where N is a set of nodes and E is a set ofwhen is gradually applied .used ,is[N] [N]Figure 3: [X] and [P] node detection.
Since the word?used?
is a head, it and its governing span are detached fromthe root ?applied?
as a child node.edges.
For conducting abstraction based on syn-tactic structures, we merge a span governed by adependency head as a node and represent it by anon-terminal in a parent node.
As a result, the i-thnodeNiconsists of a sequence of lexical words wand non-terminals L that replace spans governedby heads in the corresponding child nodes:Ni= {?|?1, .
.
.
, ?|Ni|}, ?k?
{w,L}.The edge Eijbetween a parent node Niandits child node Njcorresponds to a governor-dependent relationship from the head in Nito itsdependent wxin Nj.
wxis another head and gov-erns other words in Nj.
The span covered by Njis replaced by a non-terminal in Ni.We use three kinds of labels to represent L forexplicitly using syntactic information that is use-ful for long-distance reordering; [N], [P], and [X]according to the head in the corresponding node.We label a child node [N] when its head word isa noun and forms a base noun phrase, [P] whenits head word is an adposition1, and [X] for otherslike verb phrases, conjunctive phrases, and rela-tive phrases.
These nodes play different roles in asentence.
An [N] node, i.e., a base noun phrase,adds context to a sentence.
A [P] node dependson other phrases and generally appears relativelyfreely in a sentence.
Thus, we assume that the [P]node requires special reordering.The abstraction tree depicted in Fig.
1 has a par-ent [X] node ?when [N] is used?
and its child [N]node ?the fluid pressure cylinder 31.?
The word?used?
governs ?cylinder?
in the [N] node, and the[N] node folds the context in the [X] node.4.2 Tree ConstructionWe begin with detecting [N] nodes, then pro-ceed to [P] and [X] nodes.
These processes requireto specify source-side POSs as input for adaptingto translating languages.
We finally flatten frag-mented nodes.For detecting [N] nodes, we take a POS of nounas input and identify each noun and its governingspan, i.e., a string of all governed words, using thesource-side dependency tree as Fig.
2 shows.
We1A preposition in English and a postposition in Japanese.426Algorithm 2 [P] and [X] node detectionInput: Source-side sentence s and its dependency tree rs,POS list of adpositions PPos, [N] node list N -NodesOutput: [P] and [X] nodes P -Nodes, X-Nodes1: P -Nodes[]?
?, X-Nodes[]?
?2: HeadList[]?
root of rs3: repeat4: head?
pop node from HeadList5: ChildList[]?
all dependents of head6: for all child in ChildList do7: if child /?
N -Nodes and child has dependentsthen8: add child to HeadList9: remove child from ChildList10: end if11: end for12: start?
smallest start index of nodes in ChildList13: end?
largest end index of nodes in ChildList14: if POS of head ?
PPos then15: add span [start, end] of s to P -Nodes16: else17: add span [start, end] of s to X-Nodes18: end if19: remove head from HeadList20: until HeadList = ?regard descendant dependents as being governedby the noun for detecting a noun phrase of a com-plete form.
We extract the span as a node and re-place it by an [N] label in the sentence.Next, we identify [P] and [X] nodes given a listof source-side POSs of adpositions as input.
AsAlgorithm 2 shows, after [N] node detection, wetrace the dependency tree from its root to leaves(line 3?20).
We find all the dependents to the root,then check if each dependent is a head.
If a de-pendent of the root is a head and governs otherwords, we detach the dependent to process later(line 6?11).
We then find the smallest start indexand largest end index of dependent words and setthe corresponding span as a node (if a dependentis in an [N] node, we use the start and end indicesof the [N] node).
Each node is labeled accord-ing to the POS of its head as [P] or [X] (line 14?18).
We then take the detached dependent as a newroot and repeat the process until no more detach-ment is possible.
The computational complexityis O(|s|2).
Through this process, a span with di-rect dependencies is extracted as a node, and otherspans with descendant dependencies become de-scendant nodes, replaced by non-terminals in theirparent node as shown in Fig.
3.4.3 Handling Complex Noun PhraseAs described in Sec.
4.2, we detect a noun phraseas an [N] node.
However, an [N] node be-comes more complex than a base noun phrasewhen the head governs a clause, such as a relativethe cyli n der that isusesmachine ??
[N] [X]original [N]Figure 4: Handling a complex noun phraseclause.
Such a complex node may require long-distance reordering of an inside clause when trans-lating.
Therefore, we separate the noun phrase andclause.
We take a POS list whose word can be ahead of [P] and [X] nodes (preposition, verb, to,Wh-determiner/pronoun/adverb, and coordinatingconjunction for English) as input.
If the POS of anoun?s dependent is in the list, we detach the de-pendency arc, and then re-attach the dependencyarc to the head of the noun.
As a result, the basenoun phrase becomes an [N] node and its clausebecomes a [P] or [X] node that is transformed to asibling of the [N] node.In Fig.
4, the word ?cylinder?
in the original [N]node has a relative clause and governs ?is.?
We de-tach the dependency arc and re-attach it to ?uses?
(the head of ?cylinder?
), so that the noun phraseand the clause become sibling [N] and [X] nodes.4.4 Flattening Fragmented NodesThe above processes are independent of the sizeof each node, meaning they produce fragmentednodes of only a few words.
Such fragmentednodes make the tree projection to the target-sidedifficult.
To solve this problem, we flatten the ab-straction tree as shown in Algorithm 3.
We pro-cess an internal node in ?sfrom bottom to top.
Ifthe covering span of an internal node is less than athreshold ?
?
N, its child nodes are merged (line 3and 4, Algorithm 3).
Specifically, we reinsert thechild nodes by replacing the corresponding non-terminals with lexical strings that the child nodeshave been covered by.
The computational cost isO(|N |).
We investigate the effect of ?
in the fol-lowing evaluation section (Table 2).5 Abstraction Tree ProjectionIn this section, we describe a method for project-ing the obtained source-side abstraction tree ontothe target-side sentence.5.1 Tree Structure ProjectionWe use word alignment results for tree structureprojection.
However, accurate word alignmentis challenging when handling language pairs inwhich long-distance reordering is needed, and thealignment noise propagates to the tree projection.427Algorithm 3 Tree flatteningInput: Abstraction tree ?s, threshold ?Output: Flattened tree ?
?s1: for all internal node in ?s, from bottom to top do2: (start, end)?
start and end indices of node3: if end?
start+ 1 < ?
then4: ?
?s?MergeChildNodes(node, ?s)5: end if6: end forTo avoid this problem, we first omit alignmentlinks of function words whose alignment qualitytends to be lower than that of the content words.We then complement the quality of word align-ment by adapting the syntactic cohesion assump-tion (Yamada and Knight, 2001) that assumes aword string covered by a sub-tree of the source-side syntactic tree corresponds to a string of con-tiguous words in the target-side sentence.
Follow-ing the assumption, we project the k-th node ofthe source-side abstraction tree N(s)kto a string ofcontiguous words in the target-side:N(s)k7?
ti, .
.
.
, tj, s.t.
1 ?
i ?
j ?
|t|,where tiis the i-th word in the target-side sentenceand |t| is the number of words in the sentence.For each node of the source-side abstractiontree, we first obtain its covering span.
We thendefine a vector c ?
{0, 1}nwhose elements repre-sent word alignment links in a binary manner.
Ifand only if the i-th target word is aligned to a wordin the span, the i-th element of c becomes 1, oth-erwise it is 0.
Since the original word alignmentrepresented by the vector c may be noisy, we finda vector c??
{0, 1}nthat maximizes the syntac-tic cohesion assumption.
In c?, only consecutiveelements between two indices i and j are 1, andothers are 0.
We derive such c?as follows:Cmin(c) = {c?| argminc??c??
c?
}, (1)s.t.
?
i, j, 1 ?
i ?
j ?
n andc?k={1 i ?
k ?
j,0 otherwise,c?= argmaxc??Cmin(c)?c??.
(2)The operator ?
?
?
computes the Euclidean normof a vector and c?kis the k-th element of a vectorc?.
Finally, c?represents the best possible wordlinks that maximize the syntactic cohesion as-sumption, i.e., the longest contiguous word stringin the target-side, and that are closest to the orig-inal word alignment.
Specifically, Eq.
(1) deter-mines vectors that have the smallest distance toAlgorithm 4 Tree projectionInput: Source-side abstraction tree ?s, target-side sentencet, word alignmentAw between s and tOutput: Target-side abstraction tree ?t1: ?t[]?
?2: remove links of function words inAw3: for span := |s| ?
1 to 0 do4: for start := 1 to |s| do5: end?
start+ span6: if span [start, end] ?
?sthen7: c?
GenerateV ector([start, end],Aw)8: c?
?
Solve(c)  Eq.
(1) and Eq.
(2)9: (i, j)?
start and end indices of c?10: add span [i, j] of t as a node into ?t11: Aw ?
UpdateWordAlignment(c, c?
)12: end if13: end for14: end forthe original vector c while satisfying the hard con-straint, and Eq.
(2) selects the one whose norm islargest, i.e., a vector that has longest contiguousword links to the target-side.
For computationalefficiency, we use the greedy-search so that thecomputational cost is O(|t|).
When Eq.
(2) hasmultiple solutions, word links in these solutionsare equally likely, and thus we merge them into aunique solution.
Specifically, we take the unionof the solutions and find the smallest index ilandlargest index irwhose elements are 1.
We then setall elements between iland irto 1.As Algorithm 4 shows, we conduct this pro-cess in a top-down manner throughout the abstrac-tion tree (line 3?14).
When processing each node,word alignment links are updated by overwritinglinks in c with the ones in c?
(line 11).
The com-putational cost isO(|N(s)||t|), where |N(s)| is thenumber of nodes in ?s.
Figure 5 shows a projectionexample of a node.
A node of ?when [N] is used?covers a span of ?when the fluid pressure cylin-der 31 is used.?
The words in the span are alignedto the 1st, 2nd, and 5th target words (chunks)2;however, the link to the 5th target word (chunk)is a mis-alignment.
With the alignment vector cof [1, 1, 0, 0,1, 0, 0], we can remove this misalign-ment and derive c?of [1, 1, 0, 0,0, 0, 0].5.2 Fixed-Expression RecoveryThe abstraction tree generation and projection arebased on a dependency tree, and thus may over-segment a fixed-expression, such as idioms andmulti-word expressions.
Since a fixed-expressioncomposes a complete meaning using a contiguousword string, splitting it into different nodes results2In Japanese, a unit of dependency is a chunk in general,and thus we conduct chunking before projection.428Node:Targetsentence:Span: when the pressure cylinder 31 usedfluid is??
?
????
31 ?
??
?
??
?
??
?
??
?
????
?
??
?usediswhen [N]?
= 1, 1, 0, 0, 1, 0, 0Figure 5: Abstraction tree projectionin poor translation.
To avoid this issue, we gener-ate a list of fixed-expressions using conventionalmethods (Evert, 2008) and force them to remain inone node.
On both the source and target abstrac-tion trees, we recursively reinsert nodes to theirparent node when such a fixed-expression is over-segmented and spread over multiple nodes.5.3 Block-Corpus ConstructionAfter tree structure projection, we extract cor-responding node pairs as a block-corpus.
Eachnode pair has a form ?
?s,?t,AL?, where ?s ?
{?s, L}nrepresents the source-side node oflength n. It consists of a sequence of lexicalwords in the source-side vocabulary ?sand non-terminals L. ?t?
{?t, L}msimilarly representsthe target-side node of length m. AL preservescorrespondences between the non-terminals in thesource and target nodes.Specifically, we extract a pair of leaf nodes asa pair of lexical strings.
As for internal nodes,we use the same non-terminal labels appearingin the source-side node at the target-side node.Namely, the span covered by child nodes are re-placed by corresponding non-terminal labels in thesource-side node.
At the same time, we record thecorrespondence between the non-terminals.
Fig-ure 6 shows an example of the block-corpus, inwhich the boxed indices indicate correspondencesof non-terminals in the source and target nodes.6 EvaluationWe evaluate our method in English-to-Japanese(EJ) and Japanese-to-English (JE) translationtasks, since long-distance reordering is a seriousproblem in this language pair.6.1 Experiment CorpusWe use NTCIR-7 PATMT (Fujii et al., 2008), apublicly available standard evaluation dataset, forEJ and JE machine translation.
The dataset is con-structed using English and Japanese patents andconsists of 1.8 million parallel sentence pairs fortraining, 915 sentence pairs for development, and1, 381 sentence pairs for testing.
The developmentInput parallel corpusWhen the fluid pressure cylinder 31is used , fluid is gradually applied .??
?
????
31 ?
??
???
?
??
?
??
?
????
?
??
?Block-Corpus[X] , [N]   is gradually applied .
[X] [N]    ?
??
?
??
???
??
?
??
?when [N] is used [N]    ?
??
?the fluid pressure cylinder 31 ??
?
????
31fluid ?
?00 0 01 1Figure 6: Block-corpus example.
Boxed indices link non-terminals in the source and target exemplars.and test sets have one reference per sentence.
Thisdataset is bidirectional and can be used for both EJand JE translation evaluation.6.2 Implementation of Proposed MethodWe implement our method for EJ and JEtranslation tasks.
In both cases, we use anin-house implementation of English POS tag-ger (Collins, 2002) and a Japanese morpholog-ical analyzer (Kudo et al., 2004) for tokeniza-tion and POS tagging.
As for EJ translation,we use the Stanford parser (de Marneffe et al.,2006) to obtain English abstraction trees.
Wealso use an in-house implementation of a Japanesechunker (Kudo and Matsumoto, 2002) to obtainchunks in Japanese sentences.
We apply the chun-ker just before tree projection for using a chunkas a projection unit, since a chunk is the basicunit in Japanese.
As for JE translation, we usea popular Japanese dependency parser (Kudo andMatsumoto, 2002) to obtain Japanese abstractiontrees.
We convert Japanese chunk-level depen-dency tree to a word-level using a simple heuris-tic.
We use GIZA++ (Och and Ney, 2003) with thegrow-diag-final-and heuristic for word alignment.We use an in-house implementation ofthe bracketing transduction grammar (BTG)model (Xiong et al., 2006) as the phrase-basedmodel that our method relies on for translation.Non-terminals in our block-corpus are regardedas a single word, and their alignments AL deter-mined in the block-corpus are exclusively used toalign them.
We set the maximum phrase length to5 when training the translation model, since wefind that the performance is stable even settinglarger values as in (Koehn et al., 2003).
We thentrain the sentence-LM and block-LM using theoriginal corpus and the obtained block-corpus,respectively.
We ignore a sentence-end tag (</s>)in the block-LM.
With each corpus, we train a5-gram LM using the SRI toolkit (Stolcke, 2002).4296.3 Comparison MethodSince our method pre-processes the parallel cor-pus based on SCFG with increased flexibility andtrains a BTG model using the processed corpus,we compare our method to another BTG modeltrained only with the original corpus (simply re-ferred to as the BTG model).
We also com-pare to the tree-to-string model and HIERO usingstate-of-the-art implementations available in theMoses system (Koehn et al., 2007), since they arebased on SCFG.
The tree-to-string model requiressource-side constituency trees.
For EJ transla-tion, we use a state-of-the-art English constituencyparser (Miyao and Tsujii, 2005; Miyao and Tsujii,2008).
For JE translation, we transform a Japanesedependency tree into a constituency tree using asimple heuristic because there is no publicly avail-able constituency parser.
During the translationmodel training, we use the same setting as ourmethod.
In addition, we set the maximum spanof rule extraction to infinity for the tree-to-stringmodel and 10 for HIERO following Moses?
de-fault.
We use the sentence-LM in these models asthey assume.In addition, we compare our method to Head-Finalization (Isozaki et al., 2010; Isozaki et al.,2012) because it has achieved the best BLEU scorein EJ translation by handling long-distance re-ordering.
It is a specialized method to EJ trans-lation, where a syntactic head in an English sen-tence is reordered behind its constituents for com-plying with the head-final nature of the Japaneselanguage.
We pre-process the parallel corpus us-ing the Head-Finalization and train a BTG modelusing the same setting with our method to observethe effect of different pre-processing methods.During decoding, we set the translation tablesize to 10 for each source string, and the stackand beam sizes in the cube pruning to 100 for ourmethod (i.e., m-best = 100) and all other mod-els.
The maximum reordering span in the tree-to-string model and HIERO is the same as the ruleextraction setting (infinity and 10, respectively).We set the word reordering limit to infinity for ourmethod and the BTG model, while we set it to 3for Head-Finalization as their papers report.We tune feature weights by the minimum errorrate training (Och, 2003) to maximize the BLEUscore using the development set.
As an evaluationmetric, we compute the BLEU score using the testset, and all the scores discussed in Sec.
6.4 are theMethod EJ JEProposed method (?
= 10) 31.78 28.55BTG 28.82??26.98?
?HIERO 29.27?
?27.96?Tree-to-string 30.97??26.28?
?Head-Finalization 29.52?
?NATable 1: Test-set BLEU scores.
The symbol?
?representsa significant difference at the p < .01 level and?indicates asignificant difference at the p < .05 level against our method.test-set BLEU scores.
Significance tests are con-ducted using bootstrap sampling (Koehn, 2004).6.4 Result and DiscussionIn this section, we present experimental resultsand discuss them in detail.Overall Performance Table 1 shows the BLEUscores, in which our method significantly outper-forms all other models for both EJ and JE transla-tion tasks.
These results indicate that our methodeffectively incorporates syntactic information intothe phrase-based model and improves the transla-tion quality.For EJ translation, our method outperformsthe BTG model by 2.96, the HIERO by 2.51,the tree-to-string model by 0.81, and the Head-Finalization3by 2.26 in terms of BLEU score.When we compare our method to the Head-Finalization, both of them improve the BTG modelby pre-processing the parallel corpus.
Moreover,our method outperforms the Head-Finalization us-ing richer syntactic information.For JE translation, our method outperforms theBTG model by 1.57, the HIERO by 0.59, and thetree-to-string model by 2.27 in terms of BLEUscore.
Our method and the tree-to-string model,which depend on syntactic information, largelyoutperform the BTG model and HIERO in EJtranslation.
While the BTG model and HIERO,which are independent of syntactic information,outperform the tree-to-string model in JE trans-lation.
One reason for this phenomenon is thatEnglish is a strongly configurational language thathas rigid word order while Japanese is an agglu-tinative language that has relatively free word or-der.
A rigid syntactic structure provides solid cluesfor word reordering when translated into a flexiblelanguage, while a flexible structure provides weakclues for fitting it to a rigid structure.3The BLEU score reported in this experiment differs fromtheir papers.
This may be because they use a phrase-basedmodel in the Moses system, while we use the BTG model.430?
EJ JEBLEU height BLEU height0 31.15 4.1 (1.5) 28.41 4.2 (1.4)3 30.88 3.8 (1.7) 28.34 3.9 (1.6)5 31.21 3.7 (1.5) 28.39 3.8 (1.5)8 31.61 3.4 (1.4) 28.52 3.4 (1.4)10 31.78 3.1 (1.3) 28.55 3.2 (1.3)12 31.76 2.9 (1.3) 28.54 3.0 (1.3)15 31.25 2.6 (1.2) 28.21 2.7 (1.2)?
28.82 1.0 (?)
26.98 1.0 (?
)Table 2: Effect of threshold ?Effect of Flattening Threshold Table 2 showsBLEU scores when changing the flattening thresh-old ?
in our method, and averages and standarddeviations of the abstraction tree heights (?
= ?is equal to the BTG model).
The performance im-proves as we increase the threshold, i.e., increas-ing the level of abstraction.
Our method achievesthe best BLEU score when ?
= 10 for both EJ andJE translation, with the performance degrading aswe further increase the threshold.This trend shows the trade-off between phrase-based and syntax-based approaches.
When thethreshold is too small, an abstraction tree be-comes closer to the dependency tree and the tree-projection becomes difficult.
In addition, con-text information becomes unavailable when con-ducting long-distance reordering with a deep tree.On the other hand, when setting the threshold toolarge, the abstraction tree becomes too abstractedand syntactic structures useful for long-distanceword reordering are lost.
We need to balance theseeffects by setting an appropriate threshold.Effect of Non-Terminals and Fixed-ExpressionsWe change the kinds of non-terminal labels in anabstraction tree to investigate their effect on thetranslation quality.
When we merge the [P] labelto the [X] label, i.e., use only [N] and [X] labels,the BLEU score drops 0.40 in EJ translation whilethe score is unaffected in JE translation.
This isbecause flexible Japanese syntax does not differ-entiate postpositional phrases with others, whileEnglish syntax prohibits such a flexibility.When we merge all labels and only use the [X]label, the BLEU score drops 0.57 in EJ transla-tion and 0.43 in JE translation.
This result sup-ports our design of the abstraction tree that distin-guishes non-terminals according to their differentfunctionalities in a sentence.We also evaluate the effect of fixed-expressionsas described in Sec.
5.2.
Results show a significantchange when over-splitting fixed-expressions; theBLEU score drops 1.13 for EJ and 0.36 for JEtranslation without reinserting fixed-expressions.Method acceptable ?
global ?
local ?Proposed 52 30 4BTG 34 38 7Tree-to-string 47 32 7Table 3: Error distribution in 100 samples of EJ translationError Analysis We randomly sample 100 trans-lation outputs per our method (?
= 10), BTG, andtree-to-string models for each EJ and JE transla-tion tasks, and manually categorize errors basedon (Vilar et al., 2006).
We focus primarily onreordering errors and exclusively categorize thesamples into acceptable translations, translationswith only global or local reordering errors, as wellas others that are complicated combinations of var-ious errors.
An acceptable translation correctlyconveys the information in a source sentence evenif it contains minor grammatical errors.Table 3 shows the distribution of acceptabletranslations and those with global/local reorderingerrors in the EJ task (results of JE task are omitteddue to the severe space limitation, but their trendis similar).
It confirms that our method reduces re-ordering errors, not only for long-distance but forlocal reordering, and increases the ratio of accept-able translations compared to the BTG and tree-to-string models.
We also find that long-distancereordering was attempted in 85, 66, and 70 sen-tences by our method, BTG, and tree-to-string, re-spectively, among these translations.
The resultsshow that our method performs long-distance re-ordering more frequently than others.When we compare translations performed byour method to those performed by the tree-to-string model, we observe that their effectivenessdepends on a range of reordering.
Our method iseffective in long-distance reordering like those ofclauses, while the tree-to-string model performsmiddle-range reordering well.
This is due to thetrade-off regarding the level of abstraction as dis-cussed in the flattening threshold experiment.7 Conclusion and Future WorkWe have proposed an abstraction tree for effec-tively conducting long-distance reordering usingan off-the-shelf phrase-based model.
Evaluationresults show that our method outperforms conven-tional phrase-based and syntax-based models.We plan to investigate the effect of translatinglanguage pairs and dependency grammars in ab-straction tree generation.
In addition, we will ap-ply a structure-aware word aligner (Neubig et al.,2011) to improve the tree projection.431ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
The the-ory of parsing, translation, and compiling.
Prentice-Hall Inc.Masayuki Asahara and Yuji Matsumoto.
2003.ipadic version 2.7.0 user?s manual.
http://sourceforge.jp/projects/ipadic/docs/ipadic-2.7.0-manual-en.pdf.David Burkett, John Blitzer, and Dan Klein.
2010.Joint parsing and alignment with weakly synchro-nized grammars.
In Proceedings of Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology (NAACL-HLT 2010), pages 127?135.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and ex-periments with perceptron algorithms.
In Proceed-ings of Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2002), pages 1?8.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of International Conference on Lan-guage Resources and Evaluation (LREC 2006),pages 449?454.Stefan Evert.
2008.
Corpora and collocations.
In AnkeL?udeling and Merja Kyt?o, editors, Corpus Linguis-tics.
An International Handbook, volume 2, chap-ter 58.
Mouton de Gruyter.Yang Feng, Dongdong Zhang, Mu Li, Ming Zhou,and Qun Liu.
2012.
Hierarchical chunk-to-stringtranslation.
In Proceedings of Annual Meeting ofthe Association for Computational Linguistics (ACL2012), pages 950?958.Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, andTakehito Utsuro.
2008.
Overview of the patenttranslation task at the NTCIR-7 workshop.
In Pro-ceedings of NTCIR-7 Workshop Meeting (NTCIR),pages 389?400.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of International Workshopon Parsing Technology (IWPT 2005), pages 53?64.Fei Huang and Cezar Pendus.
2013.
Generalized re-ordering rules for improved SMT.
In Proceedingsof Annual Meeting of the Association for Computa-tional Linguistics (ACL 2013), pages 387?392.Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, andKevin Duh.
2010.
Head Finalization: A simple re-ordering rule for SOV languages.
In Proceedingsof Joint Workshop on Statistical Machine Transla-tion and Metrics MATR (WMT-MetricsMATR 2010),pages 244?251.Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, andKevin Duh.
2012.
HPSG-based preprocessingfor English-to-Japanese translation.
ACM Trans-actions on Asian Language Information Processing(TALIP), 11(3):8:1?8:16.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedingsof Conference of the North American Chapter ofthe Association for Computational Linguistics onHuman Language Technology (NAACL-HLT 2003),pages 48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2007),pages 177?180.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofConference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2004), pages 388?395.Taku Kudo and Yuji Matsumoto.
2002.
Japanese de-pendency analysis using cascaded chunking.
In Pro-ceedings of Conference on Natural Language Learn-ing (CoNLL 2002), pages 1?7.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying conditional random fields toJapanese morphological analysis.
In Proceedings ofConference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2004), pages 230?237.Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef vanGenabith.
2012.
Head-driven hierarchical phrase-based translation.
In Proceedings of Annual Meet-ing of the Association for Computational Linguistics(ACL 2012), pages 33?37.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of International Con-ference on Computational Linguistics and AnnualMeeting of the Association for Computational Lin-guistics (COLING-ACL 2006), pages 609?616.Yusuke Miyao and Jun?ichi Tsujii.
2005.
Probabilis-tic disambiguation models for wide-coverage HPSGparsing.
In Proceedings of Annual Meeting on Asso-ciation for Computational Linguistics (ACL 2005),pages 83?90.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature for-est models for probabilistic HPSG parsing.
Compu-tational Linguistics, 34(1):35?80.Jiri Navratil, Karthik Visweswariah, and Ananthakrish-nan Ramanathan.
2012.
A comparison of syntac-tic reordering methods for English-German machine432translation.
In Proceedings of International Confer-ence on Computational Linguistics (COLING 2012),pages 2043?2058.Graham Neubig, Taro Watanabe, Eiichiro Sumita,Shinsuke Mori, and Tatsuya Kawahara.
2011.An unsupervised model for joint phrase alignmentand extraction.
In Proceedings of Annual Meetingof the Association for Computational Linguistics:Human Language Technologies (ACL-HLT 2011),pages 632?641.Graham Neubig, Taro Watanabe, and Shinsuke Mori.2012.
Inducing a discriminative parser to optimizemachine translation reordering.
In Proceedings ofJoint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL 2012), pages843?853.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Franz J. Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30(4):417?449.Franz J. Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings ofAnnual Meeting on Association for ComputationalLinguistics (ACL 2003), pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofAnnual Meeting on Association for ComputationalLinguistics (ACL 2002), pages 311?318.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal SMT.
In Proceedings of AnnualMeeting on Association for Computational Linguis-tics (ACL 2005), pages 271?279.Andreas Stolcke.
2002.
SRILM - An extensible lan-guage modeling toolkit.
In Proceedings of Interna-tional Conference on Spoken Language Processing(ICSLP 2002), pages 901?904.Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Tsu-tomu Hirao, and Masaaki Nagata.
2010.
Divideand translate: Improving long distance reorderingin statistical machine translation.
In Proceedingsof Joint Workshop on Statistical Machine Transla-tion and Metrics MATR (WMT-MetricsMATR 2010),pages 418?427.David Vilar, Jia Xu, Luis Fernando d?Haro, and Her-mann Ney.
2006.
Error analysis of statistical ma-chine translation output.
In Proceedings of Interna-tional Conference on Language Resources and Eval-uation (LREC 2006), pages 697?702.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Jun Xie, Haitao Mi, and Qun Liu.
2011.
A noveldependency-to-string model for statistical machinetranslation.
In Proceedings of Conference on Em-pirical Methods in Natural Language Processing(EMNLP 2011), pages 216?226.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model for sta-tistical machine translation.
In Proceedings of In-ternational Conference on Computational Linguis-tics and Annual Meeting on Association for Com-putational Linguistics (COLING-ACL 2006), pages521?528.Jia Xu, Richard Zens, and Hermann Ney.
2005.Sentence segmentation using IBM word alignmentmodel 1.
In Proceedings of Annual Conference ofthe European Association for Machine Translation(EAMT 2005), pages 280?287.Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz J.Och.
2009.
Using a dependency parser to improveSMT for subject-object-verb languages.
In Proceed-ings of Conference of the North American Chapterof the Association for Computational Linguistics onHuman Language Technology (NAACL-HLT 2009),pages 245?253.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceedingsof Annual Meeting on Association for Computa-tional Linguistics (ACL 2001), pages 523?530.Jingbo Zhu and Tong Xiao.
2011.
Improving decodinggeneralization for tree-to-string translation.
In Pro-ceedings of Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies (ACL-HLT 2001), pages 418?423.433
