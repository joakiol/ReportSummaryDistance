Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 572?577,Dublin, Ireland, August 23-24, 2014.SINAI: Voting System for Twitter Sentiment AnalysisEugenio Mart?
?nez-C?amara, Salud Mar?
?a Jim?enez-Zafra,M.
Teresa Mart?
?n-Valdivia, L. Alfonso Ure?na-L?opezSINAI Research GroupUniversity of Jae?nE-23071, Jae?n (Spain){emcamara, sjzafra, maite, laurena}@ujaen.esAbstractThis article presents the participation ofthe SINAI research group in the task Sen-timent Analysis in Twitter of the SemEvalWorkshop.
Our proposal consists of avoting system of three polarity classifierswhich follow a lexicon-based approach.1 IntroductionOpinion Mining (OM) or Sentiment Analysis (SA)is the task focuses on the computational treatmentof opinion, sentiment and subjectivity in texts(Pang and Lee, 2008).
Currently, OM is a trendytask in the field of Natural Language Processingdue mainly to the fact of the growing interest inthe knowledge of the opinion of people from dif-ferent sectors of the society.The interest in the research community for theextraction of the sentiment in Twitter posts is re-flected in the organization of several workshopswith the aim of promoting the research in this task.Two are the most relevant, the first is the taskSentiment Analysis in Twitter celebrated withinthe SemEval workshop whose first edition was in2013 (Nakov et al., 2013).
The second is the work-shop TASS1, which is a workshop for promot-ing the research in sentiment analysis in Spanishin Twitter.
The first edition of the workshop tookplace in 2012 (Villena-Roma?n et al., 2013).The 2014 edition of the task Sentiment Analy-sis in Twitter proposes a first subtask, which hasas challenge the sentiment classification at entitylevel, and a second subtask that consists of thepolarity classification at document or tweet level.The training corpus is the same than the formerThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/1http://www.daedalus.es/TASSedition, but this year the test corpus is consider-ably bigger than the prior one.
A wider descriptionof the task and the corpus can be read in (Rosen-thal et al., 2014).We present an unsupervised polarity classifica-tion system for the subtask B of the task Senti-ment Analysis in Twitter.
The system is basedon a voting strategy of three lexicon-based senti-ment classifiers.
The sentiment analysis researchcommunity broadly knows the lexicons selected.They are, SentiWordNet (Baccianella et al., 2010),the lexicon developed by Hu and Liu (Hu andLiu, 2004) and the MPQA lexicon (Wilson et al.,2005).The rest of the paper is organized as follows.The following section focuses on the descriptionof the different sentiment resources used for de-veloping the sentiment classifiers.
The subsequentsection outlines the system proposed for the 2014edition of the task.
The last section exposes theanalysis of the results reached this year.2 Sentiment lexical resourcesSentiment lexicons are lexical resources com-posed of opinion-bearing words and some of themalso of sentiment phrases of idioms.
Most of thesentiment lexicons are formed by a list of wordswithout any additional information.A sentiment classifier based on list of opinion-bearing words usually consists of finding out thewords of the list in a given document.
This methodcan be considered very simple for the complexityof OM, but it has reached acceptable results in dif-ferent domains and also is applied in real systemslike Trayt.com2.Our experience in the field of SA allows us toassert that sentiment lexicons can be divided de-pending on the information linked to each word,2Trayt.com is a search engine of reviews of restaurants.The polarity classifier of Tryt.com is a lexicon-based systemwhich uses the opinion list compiled by Bing Liu.572so three groups can be found:?
List of opinion-bearing words: These lexi-cons are usually two lists of polar words, oneof them of positive words and another oneof negative terms.
Some examples of thiskind of sentiment lexicons are for English theone compiled by (Hu and Liu, 2004), and forSpanish, the iSOL lexicon (Molina-Gonza?lezet al., 2013).?
List of opinion-bearing words with syntacticinformation: As it is wider known, OM is adomain-dependent task and can be also saidthat a context-dependent task.
Thus, somelexicons add syntactic information with theaim of offering some information for disam-biguating the term, and also provide a differ-ent orientation of the word depending on itsPOS-tags.
One example of this kind of lexi-con is MPQA subjectivity lexicon (Wilson etal., 2005).?
Knowledge base sentiment lexicons: Theselexicons usually indicate the semantic orien-tation of the different senses of each word,whereas the previous lexicons only indicatethe polarity of each word.
Also, it is verycommon that in the knowledge base senti-ment lexicons each sense is linked to the like-lihood of being positive, negative and neutral.One example of this kind of polar lexicon isSentiWordNet (Baccianella et al., 2010).In the polarity classifier developed for the work-shop a lexicon of each type has been utilised.
Thesentiment linguistic resources used has been:?
Sentiment lexicon compiled by Bing Liu:The lexicon was used the first time in (Huand Liu, 2004).
Since then, the authors havebeen updating the list, and currently the list isformed by 2006 positive words and 4783 neg-ative words.
Also, the lexicon includes somemisspellings with the aim of better represent-ing the language used in the Internet.?
MPQA Subjectivity lexicon (Wilson et al.,2005): The lexicon is formed by over 8000subjectivity clues.
Subjectivity clues arewords and phrases that have subjective us-ages.
The lexicon was developed joiningwords compiled by the authors and withwords taken from General Inquirer.
EachFigure 1: Architecture of the system.word is linked with its grade of subjectivity,with its part of speech tag and with its seman-tic orientation.
Due to the fact that each wordhas its POS-tag there are some words that de-pending on its POS have a different semanticorientation.?
SentiWordNet 3.0 (Baccianella et al., 2010):is a lexical resource which assigns three sen-timent scores to each synset of WordNet:positivity, negativity and objectivity.3 Polarity classificationWe wanted to take advantage from our experi-ence in meta-classification in OM for the 2014edition of the task, Sentiment Analysis in Twit-ter.
We have reached good results in OM us-ing meta-classifiers in different domains (Perea-Ortega et al., 2013) and (Mart?
?n-Valdivia et al.,2013).
Therefore, we propose a voting system thatcombines three polarity classifiers.
The general ar-chitecture of the system is shown in Figure 1.Tokenization is a common step of the three clas-sifiers.
Due to the specific characteristics of thelanguage used in Twitter, a specific tokenizer forTwitter was preferred to use.
The tokenizer pub-lished by Christopher Potts3was selected and up-dated, with the aim of recognizing a wider rangeof tokens.When the tweet is tokenized, the following stepis discover its polarity.
Each of the three polarityclassifiers follows the same strategy for the clas-sification, but they perform different operationson each tweet.
The classifier based on the lexi-con compiled by Bing Liu (C BingL) consists ofseeking each token in the opinion-bearing words3http://sentiment.christopherpotts.net/tokenizing.html573list.
Therefore, after the tokenization, any linguis-tic operation has to be performed on the tweet.This classifier classifies a tweet as positive if thenumber of positive tokens is greater or equal thanthe number of negative tokens.
If there are not po-lar tokens, the polarity of the tweet is neutral.The second polarity classifier is the based onMPQA lexicon (C MPQA).
Some of the wordsthat are in the MPQA lexicon are lemmatized,and also the sentiment depends on their POS-tag.Thus, to take advantage of all the information of-fered by MPQA is needed to perform a morpho-logical analysis to each tweet.
The morphologicalanalysis firstly identifies the POS-tag of each to-ken of the tweet, and then the lemmatizer extractsthe lemma of the token.Recently, some linguistic tools have been pub-lished to carry out linguistic analysis in tweets.Currently, two POS-taggers for Twitter are avail-able.
One of them, is the described in (Gimpel etal., 2011) and the second one in (Derczynski et al.,2013).
Although the authors of the two systemsare competing for which of the two taggers are bet-ter, our selection was based on the usability of thetwo systems.
To use the tagger developed by Gim-pel et al.
is needed to download their software,meanwhile the one developed by Derczynski et al.can be integrated in other taggers.
On our point ofview, the tagger of Derczynski et al.
has the ad-vantage of offering the training model of the tag-ger4, which allows us to integrate it in other POS-tagging tools.
The training model of the taggerwas integrated in the Stanford Part-of-Speech Tag-ger5.
When each token of the tweet is associatedwith its corresponding POS-tag, the lemmatizer isrun over the tweet.
The lemmatizer used is the of-fered by the toolkit for Natural Language Process-ing, NLTK (Bird et al., 2009).
When each tokenis accompanied by its corresponding POS-tag andlemma, the polarity classifier can seek each tokenin the MPQA subjective lexicon.Besides the label of the polar class (positive ornegative), each entry in the MPQA corpus has afield called type, which indicates whether the termis considered strongly subjective or the term isconsidered weakly subjective.
Thus, in the calcu-lation of the polarity score these two levels of sub-jectivity are considered, so when the term is strongsubjective it is considered to have a score of 1, and4https://gate.ac.uk/wiki/twitter-postagger.html5http://nlp.stanford.edu/software/tagger.shtmlwhen the term is weak subjective the system con-siders the term as less important and its score is0.75.The polarity classifier based on the use of Sen-tiWordNet (C SWN) needs that each word of thetweet is linked with its POS-tag and its lemma,so the same pipeline that the classifier based onMPQA follows is also followed by the classifierbased on SentiWordNet.In the bibliography about OM can be found dif-ferent ways to calculate the polarity class whenSentiWordNet is used as a sentiment knowledgebase.
Some works perform a disambiguationmethod with the aim of selecting only the synsetthat corresponds with the sense of the word inthe context of the given document.
But there areother works that do not perform any disambigua-tion method, and also reach good results.
De-necke in (Denecke, 2008) describes a very sim-ple method to calculate the polarity of each of thewords of a document without the need of a dis-ambiguation process.
The method consists of cal-culating per each word in the document, which isin SentiWordNet, the arithmetic mean of the pos-itive, negative and neutral score of each of thesynsets that the word has in SentiWordNet.
Whenthe scores of each word are calculated, the scoreof the document is determined as the arithmeticmean of each score of the words.
The class of thedocument is corresponded with the greatest polarscore (positive, negative, neutral).
Due to the ac-ceptable results that the Denecke formula reaches,we have introduced a soft disambiguation processwith the aim of improving the classification ac-curacy.
This soft disambiguation process consistof only taking those synsets corresponding withPOS-tag of the word whose polarity are being cal-culated.
For example, the word ?good?
can do thefunction of an adverb, a noun or an adjective.
InSentiWordNet, there are two synsets of ?good?
asan adverb, four synset of ?good?
as a noun, andtwenty-one synsets as an adjective.
If the polar-ity score is calculated with the Denecke formula,the twenty-seven synsets are used.
Meanwhile, ifit used our proposal, and the word ?good?
in thegiven sentence is acting as an adverb, then onlythe two synsets of the word ?good?
when it is ad-verb are considered to calculate the polarity score.During the development of the system, we no-ticed that synsets have a lower probability to bepositive or negative, and most of them in Senti-574WordNet are neutral.
With the aim of boosting thelikelihood to be positive or negative, the polarityclassifier does not consider the neutral score of thesynset.
If the positive score is greater than the neg-ative score and greater than 0.15 then the term ispositive.
If the negative score is greater than thepositive score and greater than 0.15 then the wordis negative, in other case the word is neutral.Each of the polarity classifiers take into consid-eration the presence of emoticons, the expressionsof laughing and negation.
The emoticons are pro-cessed as words, so for determining their polaritya sentiment lexicon of emoticons was built.
Thepolar lexicon of emoticons consists of fifty-eightpositive emoticons and forty-four negative ones.Laughing expressions usually express a positivesentiment, so when a laughing expression is de-tected the counter of positive words is increasedby one.
The strategy for negation identificationis a bit straightforward but effective.
Due to thespecific linguistic characteristics of tweets, a strat-egy based on windows of words has been imple-mented.
When a polar word is identified, it issought in the previous three words whether thereis a negative particle.
In those cases that a nega-tive particle is found, the polarity of the sentimentword is reversed, that it to say if a positive (neg-ative) word is negated the system considers it asnegative (positive).The last step of the polarity classifier is the run-ning of a voting system among the three polarityclassifiers.
Three are the possible output values ofthe three base classifiers {negative, neutral, pos-itive}.
When the majority class is positive, thetweet is classified as positive, when the majorityclass is negative then negative is the class assignedto the tweet and when majority class is neutral orthere is not a majority class then the tweet is clas-sified as neutral.4 Analysis of the resultsBefore showing the results reached in the evalu-ation of the task, the results accomplished in thedevelopment phase of the system will be shown.Three main systems were assessed during the de-velopment phase:?
Baseline (BL): The three base classifierscompose the baseline system, but the threepolarity scores of SentiWordNet are consid-ered and negation is not taken into account.?
Neutral scores are not considered (NN): It isthe same than the Baseline system but theneutral scores of SentiWordNet are not con-sidered.?
Negation identification (NI): The neutralscores of SentiWordNet are not taken into ac-count and the negation is identified.The results are show in Table 1.Precision Recall F1 Accuracy Improve (Acc.
)BL 55.85% 52.02% 53.87% 60.32% -NN 56.03% 52.27% 54.09% 60.46% 0.23%NI 57.22% 53.41% 55.25% 61.12% 1.33%Table 1: Results achieved during the developingphase.As can be seen in Table 1 the systems (NN) and(NI) reach better results than the baseline, so allthe modifications to the baseline are good for thepolarity classification process.
The results confirmour hypothesis that the neutral score of the synsetsin SentiWordNet are not contributing positivelyto the sentiment classification.
Also, a straight-forward strategy for identifying the scope of thenegation improves the accuracy of the classifica-tion.
The results help us to choose the final con-figuration of the system.
As is described in theformer section the final polarity classification sys-tem follows a voting scheme of three base lexicon-based polarity classifiers.
The three base classi-fiers take into consideration the presence of emoti-cons, laughing expressions, identifies the scope ofnegation, and the classifier based on SentiWord-Net does not take into consideration the neutralscore of the synsets.The edition 2014 of the task Sentiment Analysisin Twitter has assessed the systems with five dif-ferent corpus tests: LiveJournal2014, SMS2013,Twitter2013, Twitter2014, Twitter2014Sarcasm.The results reached with each of the test corpusare shown in Table 2.Some of the results shown in Table 2 are muchclosed to the results reached during the develop-ment phase, because all of the F1 scores are closedto 55%.
The lower results have been reached withthe corpus Twitter2014 and Twitter2014Sarcasm.The poor results in Twitter2014Sarcasm are due tothe lack of a module in the system for the detectionof sarcasm.
A sarcastic sentence is usually a sen-tence with a sentiment that expresses the opposite575Precision Recall F1LiveJournal2014Positive 60.19% 76.95% 67.54%Negative 36,51% 75,00% 49.12%Neutral 82.48% 51.36% 63.31%Overall ?
?
58.33%SMS2013Positive 63.01% 60.19% 61.57%Negative 42.13% 71.86% 53.12%Neutral 82.27% 73.72% 77,76%Overall ?
?
57.34%Twitter2013Positive 60.56% 70.15% 65.01%Negative 28.29% 50.15% 36.17%Neutral 73.66% 57.06% 64.31%Overall ?
?
50.59%Twitter2014Positive 57.13% 77.49% 65.77%Negative 27.23% 42.64% 33.23%Neutral 73.54% 49.20% 58.96%Overall ?
?
49.50%Twitter2014SarcasmPositive 57.58% 48.72% 52.78%Negative 5.00% 100.00% 9.52%Neutral 84.62% 24.44% 37.93%Overall ?
?
31.15%Table 2: Results reached with the test corpus.sentiment, so a polarity classifier without a spe-cific module to treat this linguistic phenomenonwill be probably misclassified the sarcastic sen-tences.
The results for Twitter2014Sarcasm forthe negative class indicate this problem.
The lowvalue of the precision and the high value of the re-call in the negative class mean that a high numberof negative sentences have been classified as posi-tive.The analysis of the results is completed withthe assessment of our method.
We proceed fromthe hypothesis that a combination of several clas-sifiers will improve the final classification.
Ourhypothesis is based on own previous publications,(Perea-Ortega et al., 2013) and (Mart?
?n-Valdivia etal., 2013).
We have classified the test corpus witheach of the three base classifiers, with the aim ofknowing the performance of each one.
The resultsare shown in Table 3.Table 3 shows that the classifier C BingLreaches better results than the combination ofthe three classifiers.
The first conclusion wedraw from this fact is that the good perfor-mance of meta-classifiers with large opinions isnot achieved with the short texts of Twitter.
But,this conclusion is preliminary, because the lowerresults of the voting system may be due to a notgood combination of the three classifiers.
So wehave to continue working in the analysis on howto build a meta-classifier for OM in Twitter.
Therest of the classifiers reached lower results than thevoting system.
Another reason that the voting sys-tem achieved lower results than C BingL may be-cause the three classifiers are not heterogeneous,F1C BingL C SWN C MPQALiveJournal2014Positive 68.11% 42.62% 65.20%Negative 55.43% 39.81% 49.60%Neutral 64.03% 58.07% 58.43%Overall 61.77% 41.21% 57.40%SMS2013Positive 61.67% 43.53% 53.56%Negative 54.19% 28.79% 52.678%Neutral 76.00% 75.85% 68.38%Overall 57.93% 36.16% 53.12%Twitter2013Positive 68.30% 23.40% 62.37%Negative 46.20% 11.60% 37.75%Neutral 61.17% 62.11% 57.39%Overall 57.25% 17.50% 50.06%Twitter2014Positive 69.33% 22.17% 66.74%Negative 41.55% 9.79% 33.00%Neutral 53.25% 55.63% 52.76%Overall 55.44% 15.98% 49.87%Twitter2014SarcasmPositive 56.10% 27.27% 52.06%Negative 17.78% 9.52% 8.51%Neutral 44.44% 30.24% 30.77%Overall 36.94% 18.40% 30.28%Table 3: Results reached by each base classifierwith the test corpus.in other words, when one of the systems misclas-sified a document the other ones classify it cor-rectly, so the base classifiers help each other, andthe combination of systems reaches better resultsthan the individual systems.
But, in our case maybe that the systems are not heterogeneous, so ourongoing work is the study of the heterogeneity be-tween the three classifiers.If we focus only in the results achieved byC BingL, it is remarkable that the higher differ-ence is in the negative class.
C BingL reachesgreater results than the voting system in negativeclass, and it has the same negation treatment mod-ule that the voting system.
This fact allow us to saythat the low results in the negative class reached bythe voting system is not due to the negation treat-ment module, and may because by the own com-bination method.To sum up, after analysing the results, we havenoticed that the same meta-classifier methodologythat we usually apply to large reviews cannot bedirectly apply to tweets.
Therefore, our ongoingwork is focused firstly on conducting a deep anal-ysis of the results presented in this work, and sec-ondly in the study on how to improve of polarityclassification in Twitter following a unsupervisedmethodology, and thirdly on how to build a goodmeta-classifier for OM in Twitter.AcknowledgementsThis work has been partially supported by a grantfrom the Fondo Europeo de Desarrollo Regional576(FEDER), ATTOS project (TIN2012-38536-C03-0) from the Spanish Government, AORESCUproject (P11-TIC-7684 MO) from the regionalgovernment of Junta de Andaluc?
?a and CEATIC-2013-01 project from the University of Jae?n.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Se-bastiani.
2010.
SentiWordnet 3.0: An enhancedlexical resource for sentiment analysis and opinionmining.
In Proceedings of the Seventh InternationalConference on Language Resources and Evaluation(LREC?10), Valletta, Malta, may.
European Lan-guage Resources Association (ELRA).Steven Bird, Ewan Klein, and Edward Loper.
2009.Natural language processing with Python.
O?ReillyMedia, Inc.Kerstin Denecke.
2008.
Using SentiWordnet for mul-tilingual sentiment analysis.
In Data EngineeringWorkshop, 2008.
ICDEW 2008.
IEEE 24th Interna-tional Conference on, pages 507?512, April.Leon Derczynski, Alan Ritter, Sam Clark, and KalinaBontcheva.
2013.
Twitter part-of-speech taggingfor all: Overcoming sparse and noisy data.
InProceedings of the International Conference RecentAdvances in Natural Language Processing RANLP2013, pages 198?206, Hissar, Bulgaria, September.INCOMA Ltd. Shoumen, BULGARIA.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor Twitter: Annotation, features, and experiments.In Proceedings of the 49th Annual Meeting of theACL: Human Language Technologies: Short Papers- Volume 2, HLT ?11, pages 42?47, Stroudsburg, PA,USA.
ACL.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the TenthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?04, pages168?177, New York, NY, USA.
ACM.Mar?
?a-Teresa Mart?
?n-Valdivia, Eugenio Mart?
?nez-Ca?mara, Jose-M. Perea-Ortega, and L. AlfonsoUren?a Lo?pez.
2013.
Sentiment polarity detection inSpanish reviews combining supervised and unsuper-vised approaches.
Expert Syst.
Appl., 40(10):3934?3942, August.M.
Dolores Molina-Gonza?lez, Eugenio Mart?
?nez-Ca?mara, Maria Teresa Mart?
?n-Valdivia, and Jose?
M.Perea-Ortega.
2013.
Semantic orientation for po-larity classification in spanish reviews.
Expert Syst.Appl., 40(18):7250?7257.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
Semeval-2013 task 2: Sentiment analysis inTwitter.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshop onSemantic Evaluation (SemEval 2013), pages 312?320, Atlanta, Georgia, USA, June.
ACL.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1-2):1?135, January.Jose?
M. Perea-Ortega, M. Teresa Mart??n-Valdivia,L.
Alfonso Uren?a Lo?pez, and Eugenio Mart??nez-Ca?mara.
2013.
Improving polarity classification ofbilingual parallel corpora combining machine learn-ing and semantic orientation approaches.
Journal ofthe American Society for Information Science andTechnology, 64(9):1864?1877.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanov.
2014.
SemEval-2014 Task 9:Sentiment Analysis in Twitter.
In Preslav Nakov andTorsten Zesch, editors, Proceedings of the 8th In-ternational Workshop on Semantic Evaluation, Se-mEval ?14, Dublin, Ireland.Julio Villena-Roma?n, Sara Lana-Serrano, Euge-nio Mart?
?nez-Ca?mara, and Jose?
Carlos Gonza?lez-Cristo?bal.
2013.
TASS - Workshop on sentimentanalysis at SEPLN.
Procesamiento del LenguajeNatural, 50.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the Con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,HLT ?05, pages 347?354, Stroudsburg, PA, USA.ACL.577
