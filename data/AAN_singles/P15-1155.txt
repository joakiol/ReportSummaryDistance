Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1608?1617,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsPredicting Salient Updates for Disaster SummarizationChris Kedzie and Kathleen McKeownColumbia UniversityDepartment of Computer Science{kedzie, kathy}@cs.columbia.eduFernando DiazMicrosoft Researchfdiaz@microsoft.comAbstractDuring crises such as natural disasters orother human tragedies, information needsof both civilians and responders often re-quire urgent, specialized treatment.
Moni-toring and summarizing a text stream dur-ing such an event remains a difficult prob-lem.
We present a system for update sum-marization which predicts the salience ofsentences with respect to an event andthen uses these predictions to directly biasa clustering algorithm for sentence se-lection, increasing the quality of the up-dates.
We use novel, disaster-specificfeatures for salience prediction, includinggeo-locations and language models repre-senting the language of disaster.
Our eval-uation on a standard set of retrospectiveevents using ROUGE shows that salienceprediction provides a significant improve-ment over other approaches.1 IntroductionDuring crises, information is critical for first re-sponders, crisis management organizations, andthose caught in the event.
When the event is sig-nificant, as in the case of Hurricane Sandy, theamount of content produced by traditional newsoutlets, government agencies, relief organizations,and social media can vastly overwhelm those try-ing to monitor the situation.
Crisis informatics(Palen et al, 2010) is dedicated to finding methodsfor sharing the right information in a timely fash-ion during such an event.
Research in this field hasfocused on human-in-the-loop approaches rang-ing from on the ground information gathering tocrowdsourced reporting and disaster management(Starbird and Palen, 2013).Multi-document summarization has the poten-tial to assist the crisis informatics community.
Au-tomatic summarization could deliver relevant andsalient information at regular intervals, even whenhuman volunteers are unable to.
Perhaps more im-portantly it could help filter out unnecessary andirrelevant detail when the volume of incoming in-formation is large.
While methods for identifying,tracking, and summarizing events from text basedinput have been explored extensively (Allan et al,1998; Filatova and Hatzivassiloglou, 2004; Wanget al, 2011), these experiments were not devel-oped to handle streaming data from a heteroge-neous environment at web scale.
These methodsalso rely heavily on redundancy which is subop-timal for time sensitive domains where there is ahigh cost in delaying information.In this paper, we present an update summariza-tion system to track events across time.
Our sys-tem predicts sentence salience in the context of alarge-scale event, such as a disaster, and integratesthese predictions into a clustering based multi-document summarization system.
We demonstratethat combining salience with clustering producesmore relevant summaries compared to baselinesusing clustering or relevance alone.
Our experi-ments suggest that this is because our system isbetter able to adapt to dynamic changes in inputvolume that adversely affect methods that use re-dundancy as a proxy for salience.In addition to the tight integration between clus-tering and salience prediction, our approach alsoexploits knowledge about the event to determinesalience.
Thus, salience represents both how typi-cal a sentence is of the event type (e.g., industrialaccident, hurricane, riot) and whether it specifiesinformation about this particular event.
Our fea-ture representation includes a set of language mod-els, one for each event type, to measure the typi-cality of the sentence with regard to the currentevent, the distance of mentioned locations fromthe center of the event, and the change in wordfrequencies over the time of the event.
While weevaluate these features in the domain of disasters,1608this approach is generally applicable to many up-date summarization tasks.Our approach achieves a statistically significantimprovement in ROUGE scores compared to mul-tiple baselines.
Additionally, we introduce novelmethods for estimating the average informationgain each update provides and how completely theupdate summary covers the event it is tracking; oursystem?s updates contain more relevant informa-tion on average than the competing baselines.The remainder of the paper is organized as fol-lows.
We begin with a review of related workin the information retrieval and multi-documentsummarization literature.
Section 3 outlines thedetails of our salience and summarization models.Next we describe our data (Section 4) and experi-ments (Section 5).
Finally, we discuss our results(Section 6) and conclude the paper.2 Related WorkA principal concern in extractive multi-documentsummarization is the selection of salient sentencesfor inclusion in summary output (Nenkova andMcKeown, 2012).
Existing approaches generallyfall into one of three categories, each with specifictrade-offs with respect to update summarization.First, centrality-focused approaches (includinggraph (Erkan and Radev, 2004), cluster (Hatzivas-siloglou et al, 2001), and centroid (Radev et al,2004) methods) are very natural for retrospectiveanalysis in the sense that they let the data ?speakfor itself.?
These methods equate salience withcentrality, either to the input or some other ag-gregate object (i.e.
a cluster center or input cen-troid).
However, they rely chiefly on redundancy.When applied to an unfolding event, there may notexist enough redundant content at the event onsetfor these methods to exploit.
Once the event onsethas passed, however, the redundancy reduction ofthese methods is quite beneficial.The second category, predictive approaches, in-cludes ranking and classification based methods.Sentences have been ranked by the average wordprobability, average TF*IDF score, and the num-ber of topically related words (topic-signatures inthe summarization literature) (Nenkova and Van-derwende, 2005; Hovy and Lin, 1998; Lin andHovy, 2000).
The first two statistics are easilycomputable from the input sentences, while thethird only requires an additional, generic back-ground corpus.
In classification based methods,model features are usually derived from humangenerated summaries, and are non-lexical in na-ture (e.g., sentence starting position, number oftopic-signatures, number of unique words).
Sem-inal work in this area has employed na?
?ve Bayesand logistic regression classifiers to identify sen-tences for summary inclusion (Kupiec et al, 1995;Conroy et al, 2001).
While these methods areless dependent on redundancy, the expressivenessof their features is limited.
Our model expandson these basic features to account for geographic,temporal, and language model features.The last category includes probabilistic(Haghighi and Vanderwende, 2009), informationtheoretic, and set cover (Lin and Bilmes, 2011)approaches.
While these methods are focused onproducing diverse summaries, they are difficult toadapt to the streaming setting, where we do notnecessarily have a fixed summary length and thecorpus to be summarized contains many irrelevantsentences, i.e.
there are large portions of thecorpora that we specifically want to avoid.Several researchers have recognized the impor-tance of summarization during natural disasters.
(Guo et al, 2013) developed a system for detect-ing novel, relevant, and comprehensive sentencesimmediately after a natural disaster.
(Wang andLi, 2010) present a clustering-based approach toefficiently detect important updates during naturaldisasters.
The algorithm works by hierarchicallyclustering sentences online, allowing the system tooutput a more expressive narrative structure than(Guo et al, 2013).
Our system attempts to unifythese system?s approaches (predictive ranking andclustering respectively).3 MethodOur update summarization system takes as inputa) a short query defining the event to be tracked(e.g.
?Hurricane Sandy?
), b) an event categorydefining the type of event to be tracked (e.g.
?hur-ricane?
), c) a stream of time-stamped documentspresented in temporal order, and d) an evaluationtime period of interest.
While processing docu-ments throughout the time period of interest, thesystem outputs sentences from these documentslikely to be useful to the query issuer.
We referto these selected sentences as updates.In order to measure the usefulness of a sys-tem?s updates, we consider the degree to whichthe system output reflects the different aspects of1609?
hurricane force wind warnings are in effectfrom Rhode Island Sound to ChincoteagueBay?
over 5000 commercial airline flights sched-uled for October 28 and October 29 werecancelledFigure 1: Example nuggets from Hurricane Sandy.an event.
Events are often composed of a vari-ety of sub-events.
For example, the HurricaneSandy event includes sub-events related to thestorm making landfall, the ensuing flooding, themany transportation issues, among many others.An ideal system would update the user about eachof these sub-events as they occur.
We refer tothese sub-events as the nuggets associated with anevent.
A nugget is defined as a fine-grained atomicsub-event associated with an event.
We present2 example nuggets associated with the HurricaneSandy event in Figure 1.
Each event has anywherefrom 50 to several hundred nuggets in total in ourgold dataset.
We describe how these nuggets arefound in Section 4.Throughout our treatment of our algorithm, thesalience of an update captures the degree to whichit reflects an event?s unobserved nuggets.
Assum-ing that we have a text representation for each ofour nuggets, the salience of an update u with re-spect to a set of nuggets N is defined as,salience(u) = maxn?Nsim(u, n) (1)where sim(?)
is the semantic similarity such as thecosine similarity of latent vectors associated withthe update and nugget text (Guo and Diab, 2012).3.1 Update SummarizationOur system architecture follows a simple pipelinedesign where each stage provides an additionallevel of processing or filtering of the input sen-tences.
We begin with an empty update summaryU .
At each hour we receive a new batch of sen-tences btfrom the stream of event relevant docu-ments and perform the following actions:1. predict the salience of sentences in bt(Sec-tion 3.2),2. select a set of exemplar sentences in btbycombining clustering with salience predic-tions (Section 3.3),3. add the most novel and salient exemplars toU (Section 3.4).The resultant list of updates U is our summary ofthe event.3.2 Salience Prediction3.2.1 FeaturesWe want our model to be predictive of sen-tence salience across different event instancesso we avoid event-specific lexical features.
In-stead, we extract features such as language modelscores, geographic relevance, and temporal rele-vance from each sentence.Basic Features We employ several basic fea-tures that have been used previously in supervisedmodels to rank sentence salience (Kupiec et al,1995; Conroy et al, 2001).
These include sen-tence length, the number of capitalized words nor-malized by sentence length, document position,and number of named entities.
The data streamcomprises text extracted from raw html docu-ments; these features help to downweight sen-tences that are not content (e.g.
web page titles,links to other content) or more heavily weight im-portant sentences (e.g., that appear in prominentpositions such as paragraph initial or article ini-tial).Query Features Query features measure therelationship between the sentence and the eventquery and type.
These include the number ofquery words present in the sentence in addition tothe number of event type synonyms, hypernyms,and hyponyms using WordNet (Miller, 1995).
Forexample, for event type earthquake, we match sen-tence terms ?quake?, ?temblor?, ?seism?, and ?af-tershock?.Language Model Features Language modelsallow us to measure the likelihood of producinga sentence from a particular source.
We considertwo types of language model features.
The firstmodel is estimated from a corpus of generic newsarticles (we used the 1995-2010 Associated Presssection of the Gigaword corpus (Graff and Cieri,2003)).
This model is intended to assess the gen-eral writing quality (grammaticality, word usage)of an input sentence and helps our model to selectsentences written in the newswire style.The second model is estimated from text spe-cific to our event types.
For each event typewe create a corpus of related documents usingpages and subcategories listed under a related1610Storm Earthquake Meteor ImpactAccident Riot ProtestHostages Shooting BombingFigure 2: TREC TS event types.Wikipedia category.
For example, the languagemodel for event type ?earthquake?
is estimatedfrom Wikipedia pages under the category Cate-gory:Earthquakes.
Fig.
2 lists the event typesfound in our dataset.
These models are intendedto detect sentences similar to those appearing insummaries of other events in the same category(e.g.
most earthquake summaries are likely to in-clude higher probability for ngrams including thetoken ?magnitude?).
While we focus our system onthe language of news and disaster, we emphasizethat the use of language modeling can be an effec-tive feature for multi-document summarization forother domains that have related text corpora.We use the SRILM toolkit (Stolcke and others,2002) to implement a 5-gram Kneser-Ney modelfor both the background language model and theevent specific language models.
For each sentencewe use the average token log probability undereach model as a feature.Geographic Relevance Features The disastersin our corpus are all phenomena that affect somepart of the world.
Where possible, we would liketo capture a sentence?s proximity to the event, i.e.when a sentence references a location, it should beclose to the area of the disaster.There are two challenges to using geographicfeatures.
First, we do not know where the event is,and second, most sentences do not contain refer-ences to a location.
We address the first issue byextracting all locations from documents relevant tothe event at the current hour and looking up theirlatitude and longitude using a publicly availablegeo-location service.
Since the documents that areat least somewhat relevant to the event, we assumein aggregate the locations should give us a rougharea of interest.
The locations are clustered andwe treat the resulting cluster centers as the eventlocations for the current time.The second issue arises from the fact that themajority of sentences in our data do not containexplicit references to locations, i.e.
a sequence oftokens tagged as location named entities.
Our in-tuition is that geographic relevance is important inthe disaster domain, and we would like to take ad-vantage of the sentences that do have location in-formation present.
To make up for this imbalance,we instead compute an overall location for thedocument and derive geographic features based onthe document?s proximity to the event in question.These features are assigned to all sentences in thedocument.Our method of computing document-level ge-ographic relevance features is as follows.
Usingthe locations in each document, we compute themedian distance to the nearest event location.
Be-cause document position is a good indicator of im-portance we also compute the distance of the firstmentioned location to the nearest event location.All sentences in the document take as featuresthese two distance calculations.
Because someevents can move, we also compute these distancesto event locations from the previous hour.Temporal Relevance Features As we trackevents over time, it is likely that the coverage ofthe event may die down, only to spike back upwhen there is a breaking development.
Identify-ing terms that are ?bursty,?
i.e.
suddenly peakingin usage, can help to locate novel sentences thatare part of the most recent reportage and have yetto fall into the background.We compute the IDF for each hour in our datastream.
For each sentence, the average TF*IDFfor the current hour t is taken as a feature.
Addi-tionally, we use the difference in average TF*IDFfrom time t to t ?
i for i = {1, .
.
.
, 24} to mea-sure how the TF*IDF scores for the sentence havechanged over the last 24 hours, i.e.
we keep thesentence term frequencies fixed and compute thedifference in IDF.
Large changes in IDF value in-dicate the sentence contains bursty terms.
We alsouse the time (in hours) since the event started as afeature.3.2.2 ModelGiven our feature representation of the input sen-tences, we need only target salience values formodel learning.
For each event in our trainingdata, we sample a set of sentences and each sen-tence?s salience is computed according to Equa-tion 1.
This results in a training set of sen-tences, their feature representations, and their tar-get salience values to predict.We opt to use a Gaussian process (GP) re-gression model (Rasmussen and Williams, 2006)with a Radial Basis Function (RBF) kernel for thesalience prediction task.
Our features fall naturally1611into five groups and we use a separate RBF kernelfor each, using the sum of each feature group RBFkernel as the final input to the GP model.3.3 Exemplar SelectionOnce we have predicted the salience for a batchof sentences, we must now select a set of updatecandidates, i.e.
sentences that are both salient andrepresentative of the current batch.
To accomplishthis, we combine the output of our salience pre-diction model with the affinity propagation algo-rithm.
Affinity propagation (AP) is a clusteringalgorithm that identifies a subset of data points asexemplars and forms clusters by assigning the re-maining points to one of the exemplars (Frey andDueck, 2007).
AP attempts to maximize the netsimilarity objectiveS =n?i:i 6=eisim(i, ei) +n?i:i=eisalience(ei)where eiis the exemplar of the i-th data point, andfunctions sim and salience express the pairwisesimilarity of data points and our predicted apri-ori preference of a data point to be an exemplarrespectively.
AP differs from other k-centers algo-rithms in that it simultaneously considers all datapoints as exemplars, making it less prone to find-ing local optima as a result of poor initialization.Furthermore, the second term in S incorporatesthe individual importance of data points as candi-date exemplars; most other clustering algorithmsonly make use of the first term, i.e.
the pairwisesimilarities between data points.AP has several useful properties and interpre-tations.
Chiefly, the number of clusters k is nota model hyper-parameter.
Given that our task re-quires clustering many batches of streaming data,searching for an optimal k would be computation-ally prohibitive.
With AP, k is determined by thesimilarities and preferences of the data.
Generallylower preferences will result in fewer clusters.Recall that salience(s) is a prediction of thesemantic similarity of s to information about theevent be summarized, i.e.
the set of event nuggets.Intuitively, when maximizing objective functionS, AP must balance between best representing theinput data and representing the most salient in-put.
Additionally, when the level of input is highbut the salience predictions are low, the preferenceterm will guide AP toward a solution with fewerclusters; vice-versa when input is very salient onaverage but the volume of input is low.
The adap-tive nature of our model differentiates our methodfrom most other update summarization systems.3.4 Update SelectionThe exemplar sentences from the exemplar selec-tion stage are the most salient and representative ofthe input for the current hour.
However, we needto reconcile these sentences with updates from theprevious hour to ensure that the most salient andleast redundant updates are selected.
To ensurethat only the most salient updates are selected weapply a minimum salience threshold; after exem-plar sentences have been identified, any exemplarswhose salience is less than ?salare removed fromconsideration.Next, to prevent adding updates that are redun-dant with previous output updates, we filter out ex-emplars that are too similar to previous updates.The exemplars are examined sequentially in orderof decreasing salience and a similarity threshold isapplied, where the exemplar is ignored if its max-imum semantic similarity to any previous updatesin the summary is greater than ?sim.Exemplars that pass these thresholds are se-lected as updates and added to the summary.4 DataFor the document stream, we use the news portionof the 2014 TREC KBA Stream Corpus (Frank etal., 2012).
The documents from this corpus comefrom hourly crawls of the web covering October2011 through February 2013.Our experiments also make use of the TRECTemporal Summarization (TS) Track data from2013 and 2014 (Aslam et al, 2013).
This data in-cludes 25 events and event metadata (e.g., a usersearch query for the event, the event type, andevent evaluation time frame).
All events occurredduring the time span of the TREC KBA StreamCorpus.
For each event we create a stream of rel-evant documents by selecting only documents thatcontain the complete set of query words.Along with the metadata, NIST assessors con-structed a set of ground truth nuggets for eachevent.
Nuggets are brief and important text snip-pets that represent sub-events that should be con-veyed by an ideal update summary.
In order to ac-complish this, for each event, assessors were pro-vided with the revision history of the Wikipediapage associated with the event.
For example, the1612revision history for the Wikipedia page for ?Hurri-cane Sandy?
will contain text additions includingthose related to individual nuggets.
The assess-ment task involves reviewing the Wikipedia revi-sions in the evaluation time frame and markingthe text additions capturing a new, unique nugget.More detail on this process can be found in thetrack description (Aslam et al, 2013).5 ExperimentsWe evaluate our system on two metrics: ROUGE(Lin, 2004), an automatic summarization methodand an evaluation of system expected gain andcomprehensiveness?metrics adapted from theTREC TS track (Aslam et al, 2013).5.1 Training and TestingOf the 25 events in the TREC TS data, 24 arecovered by the news portion of the TREC KBAStream Corpus.
From these 24, we set asidethree events to use as a development set.
Allsystem salience and similarity threshold parame-ters are tuned on the development set to maximizeROUGE-2 F1 scores.We train a salience model for each event us-ing 1000 sentences randomly sampled from theevent?s document stream.We perform a leave-one-out evaluation of eachevent.
At test time, we predict a sentence?ssalience using the average predictions of the 23other models.5.2 ROUGE EvaluationROUGE measures the ngram overlap betweena model summary and an automatically gener-ated system summary.
Model summaries foreach event were constructed by concatenating theevent?s nuggets.
Generally, ROUGE evaluationassumes both model and system summaries are ofa bounded length.
Since our systems are summa-rizing events over a span of two weeks time, thetotal length of our system output is much longerthan the model.
To address this, for each sys-tem/event pair, we sample with replacement 1000random summaries of length less than or equal tothe model summary (truncating the last sentencewhen neccessary).
The final ROUGE scores forthe system are the average scores from these 1000samples.Because we are interested in system perfor-mance over time, we also evaluate systems at 12hour intervals using the same regime as above.The model summaries in this case are retrospec-tive, and this evaluation reveals how quickly sys-tems can cover information in the model.5.3 Expected Gain and ComprehensivenessNIST developed metrics for evaluating updatesummarization systems as part of the TREC TStrack.
We present results on two of these metrics,the expected gain and comprehensiveness.Expected Gain We treat the event?s nuggets asunique units of information.
When a system addsan update to its summary, it is potentially addingsome of this nugget information.
It would be in-structive to know how much unique and novel in-formation each update is adding on average to thesummary.
To that end, we defineE[Gain] =|Sn||S|where S is the set of system updates, Snis the setof nuggets contained in S, and | ?
| is the numberof elements in the set.
To compute the set Snwematch each system update to 0 or more nuggets,where an update matches a nugget if their seman-tic similarity is above a threshold.
Snresults fromthe unique set of nuggets matched.
Because anupdate can map to more than one nugget, it is pos-sible to receive an expected gain greater than 1.An expected gain of 1 would indicate that everysentence was both relevant and contained a uniquepiece of information.Comprehensiveness Additionally, we can usethe nuggets to measure the completeness of an up-date summary.
We defineComprehensiveness =|Sn||N |where N is the set of event nuggets.
A compre-hensiveness of 1 indicates that the summary hascovered all nugget information for the event; themaximum attainable comprehensiveness is 1.Update-nugget matches are computed automat-ically; a match exists if the semantic similarity ofthe update/nugget pair is above a threshold.
De-termining an optimal threshold to count matchesis difficult so we evaluate at threshold values rang-ing from .5 to 1, where values closer to 1 aremore conservative estimates of performance.
Amanual inspection of matches suggests that se-mantic similarity values around .7 produce reason-able matches.
The average semantic similarity of1613manual matches performed by NIST assessors wasmuch lower at approximately .25, increasing ourconfidence in the automatic matches in the .5?1range.5.4 Model ComparisonsWe refer to our complete model asAP+SALIENCE.
We compare this modelagainst several variants and baselines intended tomeasure the contribution of different components.All thresholds for all runs are tuned on thedevelopment set.Affinity Propagation only (AP) The purposeof this model is to directly measure the effect ofintegrating salience and clustering by providing abaseline that uses the identical clustering compo-nent, but without the salience information.
In thismodel, input sentences are apriori equally likelyto be exemplars; the salience values are uniformlyset as the median value of the input similarityscores, as is commonly used in the AP literature(Frey and Dueck, 2007).
After clustering a sen-tence batch, the exemplars are examined in orderof increasing time since event start and selectedas updates if their maximum similarity to the pre-vious updates is less than ?sim, as in the noveltyfiltering stage of AP+SALIENCE.Hierarchichal Agglomerative Clustering(HAC) We provide another clustering baseline,single-linkage hierarchichal agglomerative clus-tering.
We include this baseline to show thatAP+SALIENCE is not just an improvement overAP but centrality driven methods in general.HAC was chosen over other clustering ap-proaches because the number of clusters is not anexplicit hyper-parameter.
To produce flat clustersfrom the hierarchical clustering, we flatten theHAC dendrogram using the cophenetic distancecriteria, i.e.
observations in each flat cluster haveno greater a cophenetic distance than a threshold.Cluster centers are determined to be the sentencewith highest cosine similariy to the flat clustermean.
Cluster centers are examined in time orderand are added to the summary if their similarity toprevious updates is below a similarity threshold?simas is done in the AP model.Rank by Salience (RS) We also isolate the im-pact of our salience model in order to demonstratethat the fusion of clustering and salience predic-tion improves over predicting salience alone.
Inthis model we predict the salience of sentences asin step 1 for AP+SALIENCE.
We omit the cluster-ing phase (step 2).
Updates are selected identicallyto step 3 of AP+SALIENCE, proceeding in orderof decreasing salience, selecting updates that areabove a salience threshold ?saland below a simi-larity threshold ?simwith respect to the previouslyselected updates.6 Results6.1 ROUGEROUGE-1System Recall Prec.
F1AP+SALIENCE 0.282 0.344 0.306AP 0.245 0.285 0.263RS 0.230 0.271 0.247HAC 0.169 0.230 0.186ROUGE-2System Recall Prec.
F1AP+SALIENCE 0.045 0.056 0.049AP 0.033 0.038 0.035RS 0.031 0.037 0.034HAC 0.017 0.024 0.019Table 1: System ROUGE performance.Table 1 shows our results for system outputsamples against the full summary of nuggets us-ing ROUGE.
This improvement is statistically sig-nificant for all ngram precision, recall, and F-measures at the ?
= .01 level using the Wilcoxonsigned-rank test.AP+SALIENCE maintains its performanceabove the baselines over time as well.
Fig-ure 3 shows the ROUGE-1 scores over time.We show the difference in unigram precision(bigram precision is not shown but it followssimilar curve).
Within the initial days of theevent, AP+SALIENCE is able to take the leadover the over systems in ngram precision.
TheAP+SALIENCE model is better able to find salientupdates earlier on; for the disaster domain, this isan especially important quality of the model.Moreover, the AP+SALIENCE?s recall is not di-minished by the high precision and remains com-petitive with AP.
Over time AP+SALIENCE?s re-call also begins to pull away, while the other mod-els start to suffer from topic drift.6.2 Expected Gain and ComprehensivenessFigure 4 shows the expected gain across a rangeof similarity thresholds, where thresholds closer16142 4 6 8 100.00.10.20.30.4Prec.AP+SalienceAPHACRS2 4 6 8 10days since event start0.00.10.20.30.4RecallFigure 3: System ROUGE-1 performance overtime.to 1 are more conservative estimates.
The rankingof the systems remains constant across the sweepwith AP+SALIENCE beating all baseline systems.Predicting salience in general is helpful for keep-ing a summary on topic as the RS approach outperforms the clustering only approaches on ex-pected gain.When looking at the comprehensiveness of thesummaries AP outperforms AP+SALIENCE.
Thecompromise encoded in the AP+SALIENCE ob-jective function, between being representative andbeing salient, is seen clearly here where the per-formance of the AP+SALIENCE methods is lowerbounded by the salience focused RS system andupper bounded by the clustering only AP system.Overall, AP+SALIENCE achieves the best balanceof these two metrics.6.3 Feature AblationTable 2 shows the results of our feature ablationtests.
Removing the language models yields astatistically significant drop in both ngram recalland F-measure.
Interestingly, removing the ba-sic features leads to an increase in both unigramand bigram precision; in the bigram case this isenough to cause a statistically significant increasein F-measure over the full model.
In other words,the generic features actually lead to an inferiormodel when we can incorporate more appropri-ate domain specific features.
The result mirrorsSparck Jones?
claim that generic approaches to0.5 0.6 0.7 0.8 0.9 1.00.000.020.040.060.080.100.12F-measure0.5 0.6 0.7 0.8 0.9 1.00.000.020.040.060.080.100.12ExpectedGain0.5 0.6 0.7 0.8 0.9 1.0Similarity Threshold0.00.10.20.30.40.50.6Comp.AP+SalienceAPHACRSFigure 4: Expected Gain and Comprehensivenessperformance.summarization cannot produce a useful summary(Sparck-Jones, 1998).Removing the language model and geographicrelevance features leads to a statistically signifi-cant drop in ROUGE-1 F1 scores.
Unfortunately,this is not the case for the temporal relevancefeatures.
We surmise that these features are toostrongly correlated with each other, i.e.
the differ-ences in TF*IDF between hours are definitely noti.i.d.
variables.7 ConclusionIn this paper, we have presented an update sum-marization system for the disaster domain, anddemonstrated improved system performance byintegrating sentence salience with clustering.We also have shown that features specificallytargeted to the domain of disaster yield better sum-maries.
We developed novel features that capturethe language typical of different event types andthat identify sentences specific to the particulardisaster based on location.In the future we would like to explore the appli-16152012 Pakistan Garment Factory Fires?
The fire broke out when people in the building were trying to start their generator after the electricity went out.
?Pakistani television showed pictures of what appeared to be a three-story building with flames leaping from the top-floorwindows and smoke billowing into the night sky.
?The people went to the back side of the building but there was no access, so we had to made forceful entries and rescuethe people, said Numan Noor, a firefighter on the scene.?
?We have recovered 63 bodies, including three found when we reached the basement of the building,?
Karachi fire chiefEhtesham Salim told AFP on Wednesday.2012 Romanian Protests?Clashes between riot police and demonstrators have also erupted in the Romanian capital Bucharest for a third day in arow.?
BOC urged Romanians to understand that tough austerity measures are needed to avoid a default.?
More than 1,000 protesters rallied in Bucharest?s main university square, blocking traffic.
?Bucharest : a Romanian medical official says 59 people suffered injuries as days of protests against the government andausterity measures turned violent.Figure 5: AP+SALIENCE summary excerpts.ROUGE-1System Recall Prec.
F1Full System 0.282 0.344 0.306No Basic 0.263 0.380?0.294No LM 0.223?0.361 0.254?No Time 0.297?0.367?
?0.322?No Geo 0.232?
?0.381 0.265?No Query 0.251 0.377 0.280ROUGE-2System Recall Prec.
F1Full System 0.045 0.056 0.049No Basic 0.046 0.068?
?0.051?No LM 0.033?0.056 0.038?No Time 0.052??0.064??0.056?
?No Geo 0.037?0.065 0.042No Query 0.043 0.068?0.048Table 2: Feature ablation ROUGE performance.?
indicates statistically significant difference fromfull model at the ?
= .05 level.
??
indicates sta-tistically significant difference from full model atthe ?
= .01 level.cation of the AP+SALIENCE model and featuresto a wider class of events.8 AcknowledgmentsThe research described here was supported in partby the National Science Foundation (NSF) underIIS-1422863.
Any opinions, findings and conclu-sions or recommendations expressed in this paperare those of the authors and do not necessarily re-flect the views of the NSF.ReferencesJames Allan, Jaime G Carbonell, George Doddington,Jonathan Yamron, and Yiming Yang.
1998.
Topicdetection and tracking pilot study final report.Javed Aslam, Matthew Ekstrand-Abueg, Virgil Pavlu,Fernado Diaz, and Tetsuya Sakai.
2013.
Trec2013 temporal summarization.
In Proceedings ofthe 22nd Text Retrieval Conference (TREC), Novem-ber.James M Conroy, Judith D Schlesinger, PO Dianne,Mary E Okurowski, et al 2001.
Using hmm andlogistic regression to generate extract summaries forduc.G?unes Erkan and Dragomir R Radev.
2004.Lexrank: Graph-based lexical centrality as saliencein text summarization.
J. Artif.
Intell.
Res.
(JAIR),22(1):457?479.Elena Filatova and Vasileios Hatzivassiloglou.
2004.Event-based extractive summarization.
In ACLWorkshop on Summarization, Barcelona, Spain.John R Frank, Max Kleiman-Weiner, Daniel ARoberts, Feng Niu, Ce Zhang, Christopher R?e, andIan Soboroff.
2012.
Building an entity-centricstream filtering test collection for trec 2012.
Tech-nical report, DTIC Document.Brendan J Frey and Delbert Dueck.
2007.
Clusteringby passing messages between data points.
science,315(5814):972?976.David Graff and C Cieri.
2003.
English gigaword cor-pus.
Linguistic Data Consortium.1616Weiwei Guo and Mona Diab.
2012.
A simple unsuper-vised latent semantics based approach for sentencesimilarity.
In Proceedings of the First Joint Con-ference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference andthe shared task, and Volume 2: Proceedings of theSixth International Workshop on Semantic Evalua-tion, pages 586?590.
Association for ComputationalLinguistics.Qi Guo, Fernando Diaz, and Elad Yom-Tov.
2013.Updating users about time critical events.
In PavelSerdyukov, Pavel Braslavski, SergeiO.
Kuznetsov,Jaap Kamps, Stefan R?uger, Eugene Agichtein, IlyaSegalovich, and Emine Yilmaz, editors, Advancesin Information Retrieval, volume 7814 of Lec-ture Notes in Computer Science, pages 483?494.Springer Berlin Heidelberg.Aria Haghighi and Lucy Vanderwende.
2009.
Ex-ploring content models for multi-document summa-rization.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 362?370.
Association forComputational Linguistics.Vasileios Hatzivassiloglou, Judith L Klavans,Melissa L Holcombe, Regina Barzilay, Min-YenKan, and Kathleen McKeown.
2001.
Simfinder:A flexible clustering tool for summarization.
Pro-ceedings of the NAACL Workshop on AutomaticSummarizatio.Eduard Hovy and Chin-Yew Lin.
1998.
Automatedtext summarization and the summarist system.
InProceedings of a workshop on held at Baltimore,Maryland: October 13-15, 1998, pages 197?214.Association for Computational Linguistics.Julian Kupiec, Jan Pedersen, and Francine Chen.
1995.A trainable document summarizer.
In Proceedingsof the 18th annual international ACM SIGIR confer-ence on Research and development in informationretrieval, pages 68?73.
ACM.Hui Lin and Jeff Bilmes.
2011.
A class of submodu-lar functions for document summarization.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies-Volume 1, pages 510?520.
As-sociation for Computational Linguistics.Chin-Yew Lin and Eduard Hovy.
2000.
The automatedacquisition of topic signatures for text summariza-tion.
In Proceedings of the 18th conference on Com-putational linguistics-Volume 1, pages 495?501.
As-sociation for Computational Linguistics.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Text SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pages 74?81.George A Miller.
1995.
Wordnet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.Ani Nenkova and Kathleen McKeown.
2012.
A surveyof text summarization techniques.
In Mining TextData, pages 43?76.
Springer.Ani Nenkova and Lucy Vanderwende.
2005.
The im-pact of frequency on summarization.
Microsoft Re-search, Redmond, Washington, Tech.
Rep. MSR-TR-2005-101.Leysia Palen, Kenneth M Anderson, Gloria Mark,James Martin, Douglas Sicker, Martha Palmer, andDirk Grunwald.
2010.
A vision for technology-mediated support for public participation & assis-tance in mass emergencies & disasters.
In Proceed-ings of the 2010 ACM-BCS visions of computer sci-ence conference, page 8.
British Computer Society.Dragomir R Radev, Hongyan Jing, Ma?gorzata Sty?s,and Daniel Tam.
2004.
Centroid-based summariza-tion of multiple documents.
Information Processing& Management, 40(6):919?938.Carl Edward Rasmussen and Christopher K. I.Williams.
2006.
Gaussian Processes for MachineLearning.
The MIT Press.Karen Sparck-Jones.
1998.
Automatic summarizing:factors and directions.
In Advances in automatic textsummarization, eds.
Mani and Maybury.Kate Starbird and Leysia Palen.
2013.
Working andsustaining the virtual disaster desk.
In Proceedingsof the 2013 conference on Computer supported co-operative work, pages 491?502.
ACM.Andreas Stolcke et al 2002.
Srilm-an extensible lan-guage modeling toolkit.
In INTERSPEECH.Dingding Wang and Tao Li.
2010.
Document up-date summarization using incremental hierarchicalclustering.
In Proceedings of the 19th ACM inter-national conference on Information and knowledgemanagement, CIKM ?10, pages 279?288, New York,NY, USA.
ACM.William Yang Wang, Kapil Thadani, and KathleenMcKeown.
2011.
Identifying event descriptions us-ing co-training with online news summaries.
In pro-ceedings of IJCNLP, Chiang-Mai, Thailand, Nov.1617
