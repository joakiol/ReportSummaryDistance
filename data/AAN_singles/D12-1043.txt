Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 466?477, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsAn ?AI readability?
formula for French as a foreign languageThomas Franc?oisIRCS, University of Pennsylvania3401 Walnut Street Suite 400A Room 423Philadelphia, PA 19104, USAfrthomas@sas.upenn.eduCe?drick FaironCENTAL, UCLouvainPlace Blaise Pascal, 11348 Louvain-la-Neuve, BelgiumCedrick.Fairon@uclouvain.beAbstractThis paper present a new readability formulafor French as a foreign language (FFL), whichrelies on 46 textual features representative ofthe lexical, syntactic, and semantic levels aswell as some of the specificities of the FFLcontext.
We report comparisons between sev-eral techniques for feature selection and var-ious learning algorithms.
Our best model,based on support vector machines (SVM), sig-nificantly outperforms previous FFL formulas.We also found that semantic features behavepoorly in our case, in contrast with some pre-vious readability studies on English as a firstlanguage.1 IntroductionWhether in a first language (L1) or a second and for-eign language (L2), learning to read has been and re-mains one of the major concerns of education.
Whena teacher wants to improve his/her students?
readingskills, he/she uses reading exercises, whether thereare guided or independent.
For this practice to beefficient, it is necessary that the texts suit the levelof students (O?Connor et al2002).
This conditionis sometimes difficult to meet for teachers wishingto get off the beaten tracks by not using texts fromlevelled textbooks or readers.In this context, readability formulas have longbeen used to help teachers faster select texts for theirstudents.
These formulas are reproducible meth-ods that aim at matching readers and texts relativeto their reading difficulty level.
The Flesch (1948)and Dale and Chall (1948) formulas are probablythe best-known examples of those.
They are typicalof classic formulas, the first major methodologicalparadigm developed in the field during the 40?s and50?s.
They were kept as parsimonious as possible,using linear regression to combined two, or some-times, three surface features, such as word meanlength, sentence mean length, or proportion of out-of-simple-vocabulary words.Later, some scholars (Kintsch and Vipond, 1979;Redish and Selzer, 1985) argued that the classic for-mulas suffer from several shortcomings.
These for-mulas only take into account superficial features, ig-noring other important aspects contributing to textdifficulty, such as coherence, content density, infer-ence load, etc.
They also omit the interactive as-pect of the reading process.
In the 80?s, a secondparadigm, inspired by structuro-cognitivist theories,intended to overcome these issues.
It focused onhigher textual dimensions, such as inference load(Kintsch and Vipond, 1979; Kemper, 1983), den-sity of concepts (Kintsch and Vipond, 1979), ormacrostructure (Meyer, 1982).
However, these at-tempts did not achieve better results than the clas-sic approach, even though they used more principledand more complex features.Recently, a third paradigm, referred to as the ?AIreadability?
by Franc?ois (2011a), has emerged in thefield.
Studies that are part of this current share threekey features: the use of a large number of texts as-sessed by experts (coming from textbooks, simpli-fied newspapers or web resources) as training data ;the use of NPL-enable features able to capture awider range of readability factors, and the combi-nation of those features through a machine learning466algorithm.
Since the work of Si and Callan (2001),this paradigm have spawn several studies for English(Collins-Thompson and Callan, 2005; Heilman etal., 2008; Schwarm and Ostendorf, 2005; Feng etal., 2010).However, for French, the field is far from being sothriving.
To our knowledge, only two ?AI readabil-ity?
have been designed so far for French L1 andonly one for French as a foreign language (FFL)(see Section 2).
This paper reports some experi-ments aimed at designing a more efficient readabil-ity model for FFL.
In Section 2, it is further arguewhy a new formula was necessary for FFL.
Section3 covers the various methodological steps requiredto devise the model, whose results are reported inSection 4.
Finally, Section 5 discusses some inter-esting insights gained by this work.2 Readability models for FrenchReadability of French never enjoyed a large suc-cess: while readability studies on English dates backto the 20?s, it is only in 1957 that the French-speaking world discovered it through the work ofConquet (1957).
Since then, only a few studies fo-cused on the topic.The two first French L1 formulas were adap-tations of the Flesch formula (Kandel and Moles,1958; de Landsheere, 1963).
It is only withHenry (1975) that French got a model fitting theparticularities of the language.
Henry used clozetests to assess the level of 60 texts from primary andsecondary school textbooks and trained three for-mulas on this corpus.
It is worth mentioning thatHenry?s formulas have been applied to FFL by Cor-naire (1988).
During the same time, Richaudeauexplored a different path, as a representative of thestructuro-cognitivist paradigm.
He used the num-ber of words recalled by a subject after he/she hasjust read a sentence as a device to measure under-standing and provided an ?efficiency formula?
oftexts (Richaudeau, 1979).
Although more modernin its conception, Richaudeau?s hard-to-implementformula did not achieve the same recognition in theFrench speaking world as Henry?s.After those two major efforts, few works fol-lowed.
It is worth mentioning two more authors:Mesnager (1989), who designed a classic formulafor children that draw inspiration from the Dale andChall (1948) formula and Daoust et al1996), whodeveloped SATO-CALIBRAGE, a program assessingtext difficulty from the first to the eleventh grade.It can be considered as the first ?AI formula?
forFrench L1, since it made use of NLP-enabled fea-tures.
It is also the last formula published for FrenchL1, if we except the adaptation of the model byCollins-Thompson and Callan (2004) to French.As regards to French L2, the literature is evensparser.
Tharp (1939) published a first formula tak-ing into account one particularity of the L2 context:the cognates.
Those are words sharing a similarform and meaning across two languages and hav-ing a facilitating effect in reading.
This idea was re-cently replicated by Uitdenbogerd (2005), who com-bined a syntactic feature, the mean number of wordsper sentence, with the number of cognates per 100words in her formula.
Although taking into accountthis effect of the L1 on L2 reading is very interest-ing, these two studies are confined to a limited audi-ence: English speakers learning French.
As regardsa more generic approach, Franc?ois (2009) recentlypublished an ?AI formula?
for FFL, based on lo-gistic regression and ten features.
Among those, hestressed the use of verbal tense information as a wayto improve performance.
However, the set of fea-tures he experimented remains limited (about 20).From all this, it seems clear that FFL readabilityneeds to be addressed more thoroughly, especially ifwe are willing to get a generic model, able to makepredictions for L2 readers with any L1 background.The rest of this paper describes one such attempt.3 Design of the formulaThe design of an ?AI readability?
formula involvesthe same three steps as a classification problem.First, one need to gather a gold-standard corpuslarge enough to reliably train the parameters of alearning algorithm, as described in Section 3.1.
Thenext step, covered in Section 3.2, consists in defin-ing a set of predictors, that is to say, linguistic char-acteristics of the texts that will be used to predict thedifficulty level of new texts.
Finally, the best sub-set of these predictors is combined within a learningalgorithm to obtain the best model possible.
Experi-ments at this level are reported in Section 3.3.4673.1 The corpusA gold-standard for readability consists in texts la-belled according to their difficulty.
For this, it is firstnecessary to choose a difficulty scale used for the la-bels (for English L1, it is usually the 12 grade levelsscale), that also constrains the output of the formula.Then, each text have to be assessed with a methodable to measure the reading comprehension level ofthe target population.Regarding the scale, an obvious choice forthe foreign language context was the begin-ner/intermediate/advanced continuum, recently re-defined in the Common European Framework ofReference for Languages (CEFR) (Council of Eu-rope, 2001) as the six following levels: A1 (Break-through); A2 (Waystage); B1 (Threshold); B2 (Van-tage); C1 (Effective Operational Proficiency) and C2(Mastery).
This scale has now become the referencefor foreign language education, at least in Europe.Assessing the reading difficulty of texts with re-spect to a target population of readers was a morechallenging issue.
Several techniques have beenused in the literature, the most important of whichare comprehension tests, cloze tests and expertjudgements.
They all postulate a given population ofreaders, although relying on expert judgements savethe need for a sample of subjects to take a test.
Inthis case, texts comes from textbooks whose contentdifficulty have been assessed by the publishers.This last criterion is now mainstream in ?AI read-ability?, since it is very practical and facilitates thecreation of a large corpus, but it has its own short-comings.
Studies such as van Oosten et al2011)found that expert agreement on a same corpus oftexts might be insufficient for a classification task.For this study, we nevertheless relied on expertjudgements, since we needed a large amount of la-belled texts to ensure a robust statistical learning.We selected 28 FFL textbooks, published after 2001and designed for adults or adolescents learning FFLfor general purposes.
From those, we extracted2,160 texts related to a reading comprehension taskand assigned to each of them the same level as thetextbook it came from.As it was expected from van Oosten et al2011)?sstudy, differences in the publishers?
conception ofdifficulty led to an heterogeneous labelling betweentextbooks.
This heterogeneity was detected in threeof the six levels (A1, A2, and B1) using ANOVAbased on two classic readability features as inde-pendent variables: the mean number of words persentence and the mean number of letters per word.A subsequent qualitative analysis revealed that mostof the heterogeneity was coming from textbooks fol-lowing the new didactic approach recommended bythe CEFR: the task-oriented approach, which fo-cuses more on the task than the text when labellingthe overall reading activity.
Therefore, we decidedto remove those type of textbooks from our corpus,which amounted to 5 books and 249 texts.
The re-maining 1,852 excerpts were kept for our experi-ments.
Their distribution is displayed in Table 1 asregard to the number of texts and tokens.3.2 The predictorsIn a second step, every text of the corpus was rep-resented as a numeric vector of 406 features, eachof them representing a linguistic dimension of thetext as a single number.
Their implementation drewon two different sources of inspiration: the existingpredictors in the English and French literature andthe psycholinguistic literature on the reading pro-cess.
The complete set was classified in four fam-ilies, depending on the kind of information each oneis supposed to represent.
These families were: ?lex-ical?, ?syntactic?, ?semantic?, and ?specific to FFLcontext?.
Each of them was further divided in sub-families, described in the rest of the section 1.3.2.1 Lexical FeaturesLexical features have been shown to be the mostimportant level of information in many readabilitystudies (Chall and Dale, 1995; Lorge, 1944).
It isthen not surprising that a wide range of lexical pre-dictors have been developed in the literature.
Ourown set comprised the following subfamilies:Statistics of lexical frequencies: frequencies ofwords in a text are a good indicator of the text?s over-all difficulty (Stenner, 1996).
They are usually sum-marized via the mean, but we also tested the median,the interquartile range, as well as the 75th and 90thpercentiles.1Space restrictions did not enable us to formally definedeach variable used in this study.
The reader may consultFranc?ois (2011b) for a more comprehensive description.468A1 A2 B1 B2 C1 C2 Total430(58.561) 380(75.779) 552(176.973) 198(71.701) 184(92.327) 108(35.202) 1, 852(510; 543)Table 1: Distribution of the number of texts and tokens per level in our corpus.We used Lexique3 (New et al2007) as our fre-quency database.
It is a lexicon including about50,000 lemmas and 125,000 inflected forms whosefrequencies were obtained from movie subtitles.Since French has a rich morphology, we consideredthe probabilities of both lemma and inflected forms.Moreover, following an idea from Elley (1969), wealso computed the above mentioned statistics forgiven POS words, such as content word, nouns,verbs, etc.Percentage of words not in a reference list: partof the Dale and Chall (1948)?s formula, this featureis one of the most famous in readability.
For ourexperiments, two word lists for FFL were used: thewell-known ?
but already dated ?
Gougenheim etal.
(1964)?s list and a second one that was found atthe end of one FFL textbook: Alter Ego (Berthet etal., 2006).
Different sizes were also experimentedfor both lists.Word length: mean word length is another classicfeature in readability (Flesch, 1948; Smith, 1961).We used various statistics based on the number ofletters per word (mean, median, percentiles, etc.
).N-grams models: Si and Callan (2001) shownthat n-grams models can successfully be applied toreadability.
We then used both a simple unigram ap-proach based on the frequencies from Lexique3, anda more complex bigram model trained on two dif-ferent corpora: the Google n-grams (Michel et al2011) and a corpus of newspaper articles from LeSoir amounting to 5, 000, 000 words 2.
Both werenormalized according the length n of the text as fol-lows:P (text) =1nn?i=1logP (wi|h) (1)where wi is the ith word and h a limited history oflength 0 (unigram) or 1 (bigram).2Smoothing algorithms used were respectively the simpleGood-Turing algorithm (Gale and Sampson, 1995) for unigramsand linear interpolation (Chen and Goodman, 1999) for the bi-grams.Lexical diversity: the repetition effect is anotherfactor known to affect the reading process (Bowers,2000).
It has been mainly implemented through theclassic type-token ratio (TTR) that suffers from be-ing dependent on the text length.
This is why wedefined a normalized TTR, which is the mean scoreof several TTRs, computed on text?s fragments ofequal length.
This way, long texts were made com-parable with short ones.Orthographic neighborhood: we finally sug-gested a new lexical variable, based on the fact thatsome characteristics of the orthographic neighbors 3of a word are known to impact the reading of thisword (Andrews, 1997).
Thirteen predictors wereimplemented to account for the number or the fre-quency of the orthographic neighbors of all words ina text.3.2.2 Syntactic featuresThe syntactic level of information is another tradi-tional area of investigation in readability.
Althoughmost of the scholars in the field agree that it does notlead to such efficient predictors as the lexical level,they have noticed it can be combined with the latterto improve performance of readability formulas.
Wetherefore investigated the following subfamilies:Sentence length: the traditional approach to syn-tactic difficulty relied on the number of words persentence.
We have approached it through variousstatistics such as the mean, the median, or severalpercentiles.Part of speech ratios: Bormuth (1966) demon-strated the good predictive power of some POS ra-tios in a text.
We computed 156 ratios based onTreeTagger?s POS (Schmid, 1994).
They operatedas proxies for the syntactic complexity of sentences,since we did not use features based on a parser 4.3The orthographic neighbors of a word X have been definedby Coltheart (1978) as all the words of the same length asX andvarying from it only by one letter (eg.
FIST and GIST).4This choice was motivated as follows.
Bormuth (1966),who performed a manual annotation of the syntactic structures469Verbs: although the tense and moods found in atext have been hardly considered in the field, Car-reiras et al1997) suggested that verbal aspects areimportant while building a mental representation ofa text and therefore impact its understanding.
Theyhelp the reader to distinguish between major andminor elements associated with events described bythese verbs.
We therefore replicated and enhancedthe feature set proposed by Franc?ois (2009), consid-ering either binary indicators or proportions of theuse of tenses or moods in a text.3.2.3 Semantic featuresThe importance of semantic and cognitivefactors have been particularly stressed by thestructuro-cognitivist paradigm, although Miller andKintsch (1980), as well as Kemper (1983), eventu-ally admitted not being able to demonstrate the supe-riority of those new predictors over traditional ones.More recent work also reported limited evidence ofthis alleged superiority (Pitler and Nenkova, 2008;Feng et al2010).
In order to clarify as much aspossible the situation for FFL, we implemented thefollowing features:Personnalization level: Dale and Tyler (1934)suggested that informal texts should be easier to readand that informality might be assessed through thetype of personal pronouns found in a text.
On this as-sumption, 13 variables were defined to take into ac-count various personal pronouns proportions in thetext.Conceptual density: Kintsch et al1975) showedthat the number of propositions as well as the num-ber of different arguments in a sentence influenceits reading time.
Following Kintsch?s propositionalmodel, we used Denside?es (Lee et al2010) to cap-ture conceptual complexity.
It is a program able toestimate the mean number of propositions per wordin a text using 35 rules relying on lexical and POSclues.in its corpus, noticed that features based on parse trees wereless efficient than classic ones, such as sentence length or partof speech ratios.
Therefore, it seemed unlikely that the infor-mation collected by means of syntactic parsers, which are stillcommitting a significant number of errors, at least for French,would belie these findings.Lexical cohesion : the level of cohesion in a textwas measured as the average cosine of all pair ofadjacent sentences in the text.
Each sentence wasrepresented by a numeric weighted vector (based onwords) and projected in a vector space.
As sug-gested by Foltz and al.
(1998), two methods wereused to define the vector space and weight everyword: the tf-idf (term frequency-inverse documentfrequency) and the latent semantic analysis (LSA).The first approach, called ?word overlap?, corre-sponds to the ?noun overlap?
defined by Graesser etal.
(2004, 199), except that all type of POS are takeninto account.
For LSA, we applied a singular valuedecomposition (SVD), and after comparing varioussizes with a cross-validation procedure, we retaineda small 15-dimensional space.3.2.4 Features specific to FFLApart from the effect of cognates (Uitdenbogerd,2005; Tharp, 1939), few features specific to the L2context were previously investigated.
It is probablybecause such an approach requires to train a modelfor each pair of language of interest and gather suit-able data for evaluation.
Since our study intended todesign a generic model, we focused on specific pre-dictors affecting L2 reading, whatever the learner?smother tongue is:Multi-word expressions (MWE): MWEs are ac-knowledged to cause problems to L2 learners forproduction (Bahns and Eldaw, 1993).
However, theeffect of MWE on the reception side remains un-clear, especially for beginners.
Ozasa et al2007)tested the mean of the absolute frequency of allMWEs in a text as an indication of its difficulty,but it appeared non significant.
In a latter experi-ment involving a larger set of MWE-based predic-tors, Franc?ois and Watrin (2011) detected a signifi-cant, but limited effect.
We therefore replicated thisset, which includes variables based on the frequen-cies of MWE, their syntactic structure, their numberor their length.
Frequencies were estimated on thesame corpora as the bigram model described above(Google and Le Soir).Type of text: Finally, we defined five simple vari-ables aiming at identifying dialogues, such as pres-ence of commas, ratio of punctuation, etc.
as sug-gested by Henry (1975).
This focus on dialogue was470Level of information Tag Description of the variable ?LexicalPA-Alterego Proportion of absent words from a list of easy words 0.653X90FFFC 90th percentile of inflected forms for content words only ?0.643ML3 Unigram model based on lemmas ?0.553NLM Mean number of letters per word 0.483TTR Type-token ratio based on lemma 0.283MedNeigh+Freq Median number of more frequent neighbor for words ?0.233SyntacticNMP Mean number of words per sentence 0.623NWS90 Length (in words) of the 90th percentile sentence 0.613LSDaoust Percentage of sentences longer than 30 words (Daoust et al1996) 0.563PPres Presence of at least one present participle in the text 0.443PRO.PRE Ratio of pronouns on prepositions ?0.353PPres-C Proportion of present participle among verbs 0.413PPasse Presence of at least one past participle 0.393Impf Presence of at least one imperfect 0.273Subp Presence of at least one subjunctive present 0.273Cond Presence of at least one conditional 0.233Imperatif Presence of at least one imperative 0.02Subi Presence of at least one subjunctive imperfect 0.05SemanticavLocalLsa-Lem Average intersentential cohesion measured via LSA 0.633PP1P2 Percentage of P1 and P2 personal pronouns ?0.333SpecificNAColl Proportion of MWE having the structure NOUN ADJ 0.293BINGUI Presence of commas 0.463Table 2: Spearman correlation for some predictors in our set with difficulty.
A positive correlation means that thedifficulty of texts increases with the value of the predictor.
Signification levels are the following 1 :< 0.05; 2 :< 0.01;and 3 : < 0.001.explained by their extensive use in foreign languageteaching, especially in the first levels.
Furthermore,even for L1, various scholars stressed the fact thatdialogues are often written in a simpler style andhave a more mundane content (Dolch, 1948; Flesch,1948).3.3 The algorithmsThe last step in the development of our formula wasto select the most informative subset of features andcombine them in a state-of-the-art machine learn-ing algorithm.
The algorithms originally consid-ered were six: multinomial and ordinal logistic re-gression (respectively MLR and OLR), classifica-tion trees, bagging, boosting (both based on decisiontrees) and support vector machine (SVM).
However,since the logistic models and the SVM clearly out-performed the others three, we will reported onlyabout those in the next section.4 ResultsThe experiments based on this methodology weretwofold.
First, we assessed the predictive powerof each of the 406 features, considered in a bivari-ate relationship with difficulty.
Second, we selectedvarious subsets of features for training models andcompared their performance.
The two next sectionssummarize the main findings obtained during thesetwo steps.4.1 The efficiency of predictorsSpearman correlation was used to assess the effi-ciency of each predictor, to better account for non-linear relationships with the criterion.
Values forsome variables among the four families are reportedin Table 2.
In accordance with the literature, it ap-peared that the best family of predictors were thelexical one, followed by the syntactic one.
On thecontrary, semantic and specific to FFL features didnot perform so well, with the exception of the LSA-based feature (avLocalLsa-Lem).Of all predictors, the best was surprisingly PA-Alterego, a list-based variable inspired by Dale andChall (1948), but adapted to the FFL context, sincethe list of easy words used came from a FFL text-book (Alter Ego 1).
This suggests that, although thepredictive power of ?specific to FFL?
features waslow, specialization to the FFL context was beneficialat other levels.4714.2 The modelsOnce the best single predictors were identified, itwas possible to combine several of them in a read-ability model for comparison.
This required somecorpus preparation.
Since preliminary experimentsshowed that the equal prior probabilities are requiredto ensure a unbiased training, the whole corpus wasresampled to get the same number of texts per level(108), which amounted to a total of 648 texts.
Wethen split this smaller corpus into two sets.
240 textswere kept for development purposes, mainly featureselection and estimation of the meta-parameters ?and C for the SVM.
The remaining 408 texts wereused for evaluating performance of our readabilitymodels.4.2.1 Selection of the featuresSeveral ways of selecting the smallest ?best?
sub-set of features were compared, given that somevariables are partly redundant when combined to-gether.
The first method was based on thestructuro-cognitivist assumption that readability for-mulas should include other features than just lexico-syntactical ones, in order to maximize variety of in-formation.
Therefore, we tried an ?expert?
selec-tion, keeping either the best feature among each ofthe four families (set Exp1), or the two best features(set Exp2) 5.These ?expert?
approaches were compared to anautomatic selection, using either a stepwise proce-dure 6 for logistic regression (OLR and MLR) ora built-in regularization (Bishop, 2006, 10) for theSVM, based on the 46 best predictors inside eachsubfamily.For the sake of comparison, we also defined twoother sets: one that corresponds to a random clas-sification (the empty subset), and a baseline, basedon two classics predictors (number of letters perword and number of words per sentence), whichaimed to mimic classic formulas such as those of5For the syntactic level, since the two best variables be-longed to the same subfamily (see Section 3.2) and were toohighly intercorrelated, the 90th percentile of the sentence length(NWS90) was replaced by the best feature from another subfam-ily: the presence of at least one present participle (PPres).6In order to suppress as much random effects as possible, theselection process was repeated 100 times via a bootstrapping.632 procedure (Tuffe?ry, 2007, 396-371) and only the featuresselected at least 50 times out of 100 were kept.Flesch (1948) or Dale and Chall (1948).
A summaryof the features included in each subset is available inTable 3.4.2.2 Evaluation of the modelsThe next step then consisted in training logisticand SVM models for each of the above subsets.Their performances, reported in Table 4, were as-sessed using five measures: the multiple correlationratio (R), the accuracy (acc), the adjacent accuracy 7(adjacc), the root mean square error (rmse) and themean absolute error (mae).
It should be noted thateach of these measures was estimated through a ten-fold cross-validation procedure, which allowed us tocompare performances of different models with a T-test.The comparison between the models was per-formed in two steps.
First, we computed T-testsbased on adjacc to compare the models based ona same set of features (either Exp1, Exp2, or Auto).This allowed us to pick up the best classifier for eachset.
In a second step, these three best models werecompared the same way, which resulted in the se-lection of the very best classifier.
The decision ofadopting the adjacent accuracy as a criterion insteadof the accuracy was motivated by our conviction thatour system should rather avoid serious errors (i.e.larger than one level) than be more accurate, whilesometimes generating terrible mistakes.
However, itappeared that both metrics were mostly consistent.The performance of the different models are dis-played in Table 4.
It is first interesting to note thatthe baseline (based on SVM) already gives interest-ing results.
It reaches a classification accuracy of34%, which is about twice the random.
As regardsthe first model (Exp1), based on RLM and includingfour predictors, it outperforms the baseline by 5%, adifference close to significance (t(9) = 1.77; p =0.055).
Therefore, combining variables from sev-eral families seems to improve performance over the?classic?
baseline, limited to lexico-syntactic fea-tures.This finding is reinforced by the SVM modelfrom Exp2, which includes eight features.
It per-forms significantly better than the baseline (t(9) =7Heilman et al2008) defined it as ?the proportion of pre-dictions that were within one level of the human assigned labelfor the given text?.472Model name Classifieur Set of featuresExp1 OLR, MLR and SVM PA-Alterego + NMP + avLocalLsa-Lem + BINGUIExp2 OLR, MLR and SVM PA-Alterego + X90FFFC + NMP + PPres + avLocalLsa-Lem + PP1P2 + BINGUI + NACollAuto-OLR OLR PA-Alterego + NMP + PPres + ML3Auto-MLR MLRPA-Alterego + Cond + Imperatif + Impf + PPasse + PPres + Subi + Subp+ BINGUI + TTR + NWS90 + LSDaoust + MedNeigh+FreqAuto-SVM SVM all the 46 variablesTable 3: Results from the two selection process: expert and automatic.
Description of the features can be found inTable 2.Model Classifier Parameters R acc adjacc rmse maeRandom / / / 16.6 44.4 / /Baseline SVM ?
= 0.05;C = 25 0.62 34.0 68.2 1.51 1.06Exp1 RLM / 0.70 39.4 74.2 1.34 0.97Exp2 SVM ?
= 0.002;C = 75 0.73 40.8 77.9 1.28 0.94Auto-OLR OLR / 0.71 39.6 76.1 1.33 0.96Auto SVM ?
= 0.004;C = 5 0.73 49.1 79.6 1.27 0.90Table 4: Evaluation measures for the best difficulty model from each feature set (Exp1, Exp2 and Auto), along withvalues for a random classification, and the ?classic?
baseline.2.36; p = 0.02), with an accuracy gain of 7%.
How-ever, to that point, it was not clear whether this supe-riority was indeed a consequence of maximizing thekind of information brought to the model or merelythe result of the increased number of predictor.We thus performed another experiment to addressthis issue.
The model Exp1 was compared withAuto-OLR, the best ordinal logistic model obtainedthrough the stepwise selection (see Tables 4 and3), and previously discarded as a result of the T-test comparisons.
Like Exp1, it also contains fourpredictors, but they are all lexical or syntactic fea-tures.
Therefore, this model does not maximize thetype of information.
Surprisingly, we observed thatAuto-OLR obtained similar and even slightly bet-ter performance than Exp1 (+2% for both acc andadjacc).
Thus, the claim that maximizing the sourceof information should yield better models did notstand on our data.Finally, our best performing model was based onthe Auto feature set and SVM.
Its accuracy was in-creased by 8% in comparison with the Exp2 model,which is clearly a significant improvement (t(9) =2.61; p = 0.01), and outperformed the baseline by15%.
As mentioned previously, this model includes46 features coming from our four families.
It isworth mentioning that the quality of the predictionsis not the same across the levels, as shown in Ta-ble 5.
They are more accurate for classes situatedat both ends of the difficulty scale, namely A1, C1and C2.
For A1, this is explained because texts forbeginners are more typical, having very short sen-tences and simple words.
However, the case of C1and C2 classes is more surprising and might be dueto some specificities of the learning algorithm.A1 A2 B1 B2 C1 C2Adj.
acc.
100% 71% 67% 71% 86% 83%Table 5: Adjacent accuracy per level, computed on oneof the 10 folds.
Its adjacent accuracy was 79%, which isvery similar to the average value of the model.We also assessed the specific contribution of eachfamily of features in two ways: on one hand, wetrained a model including only the features from thisfamily; on the other hand, we trained a model in-cluding all features except those from this family.Results for the four families are displayed at Table 6.It appeared that the lexical family was the mostaccurate set of predictors (40.5%) and yielded thehighest loss in performance when set aside, espe-cially for adjacent accuracy.
In fact, this was theonly set whose absence significantly impacted ad-jacent accuracy, suggesting that the other type ofpredictors can only improve the accuracy of predic-tions, but are not able to reduce the amount of crit-ical mistakes.
The second best family was, expect-edly, the syntactic one.
Its accuracy closely matchthat of the lexical set, although more severe mistakeswere made, as shown by the drop in adjacent accu-473racy.
Finally, our two other families was clearly in-ferior, but they still improved slightly the accuracyof our model, although not the adjacent accuracy.Family only All except familyAcc.
Adj.
acc.
Acc.
Adj.
acc.Lexical 40.5 75.6 41.1 73.5Syntactic 39.3 69.5 43.2 78.4Semantic 28.8 61.5 47.8 79.2FFL 24.9 58.5 47.8 79.6Table 6: Accuracy and adjacent accuracy (in percentage)for models either using only one family of predictors, orincluding all 46 features except those of one family.4.2.3 Comparaison with previous workComparisons with other FFL models are difficultto provide: not only there are few formulas availablefor FFL, but some of these focus on a different au-dience, making comparability low.
This is why wewere able to compare our results with only two pre-vious models.The first of them is a classic readability formulaby Kandel and Moles (1958), which is an adaptationof the Flesch (1948) formula for French:Y = 207?
1.015lp?
0.736lm (2)where Y is a readability score ranging from 100(easiest) to 0 (harder); lp is the average number ofwords per sentence and lm is the average numberof syllables per 100 words.
Although it was not de-signed for FFL, we considered it, since it is one ofthe most well-known formula for French and the twofeatures combined are very general.
Their predic-tive power should not vary much in both contexts, asshown by Greenfield (2004) for English.
We evalu-ated it on the same test corpus as our SVM modeland obtained really lower values : a R of 0.55 andan accuracy of 33%.The second model was that of Franc?ois (2009),which is based on a multinomial logistic regressionincluding ten features: a unigram model similar toML3, the number of letters per word, the number ofwords per sentence, and binary variables indicatingthe presence of a past participle, present participle,imperfect, infinitive, conditional, future and presentsubjunctive tenses in the text.
To our knowledge,this model is the best current generic model avail-able for FFL.
On our data, it yielded an accuracy of41% and an adjacent accuracy of 72.7%, both esti-mated through a 10-fold cross-validation procedure.Therefore, our new approach achieved an accuracygain of 8% over this state-of-the-art model, whichwas considered as a significant difference (t(9) =3.72; p = 0.002).Apart of those two studies, Uitdenbogerd (2005)also developed recently a FFL formula.
However, asexplained previously, this work focused on a spe-cific category of L2 readers, the English-speakerslearning FFL, which resulted in a different problem.She reported a higher R than us (0.87 against 0.73).However, this value might be the training one andwas estimated on a small amount of novel begin-nings.
It is therefore likely that our model generalizebetter, especially across genres and L2 readers withdifferent L1 backgrounds.5 Discussion and conclusionIn this paper, we introduced a new ?AI readability?formula for FFL, able to predict the level of textsaccording to the largely-spread CEFR scale.
Ourmodel is based on a SVM classifier and combines 46features corresponding to several levels of linguis-tic information.
Among those, we suggested somenew features: the normalized TTR and the set ofvariables based on several characteristics of words?neighbors.
Comparing our approach with two pre-viously published formulas, our model significantlyoutperformed both these works.
Therefore, it repre-sent a robust generic solution for FFL readers will-ing to find various kind of texts that suit their lin-guistic abilities.Besides the creation of a new FFL readabilityformula, this study produced two valuable insights.First, we showed that maximizing the type of lin-guistic information might not be the best path to go,since a model based on four lexico-syntactic fea-tures yielded predictions as accurate as those of amodel relying on our Exp1 set of variables.
How-ever, this finding might be partly accounted by thelower predictive power of the features from the se-mantic and specific-to-FFL family, with the notableexception of the LSA-based predictor (avLocalLsa-Lem), which is the third best predictor when consid-ered alone.This leads us to our second finding, relative to the474set of semantic features.
Yet their importance waslargely praised in the structuro-cognitivist paradigmand in most of the recent works, our experimentscast serious doubts about their efficiency, at least ina L2 context.
Not only the expert models, to whichwe imposed the presence of one or two semantic pre-dictors, did not perform the best, but none of thefeatures from our semantic set was retained duringthe automatic selection of the variables for the lo-gistic models.
On the contrary, in some subsets,the LSA-based feature was sometimes considered ascollinear with the other variables.
Finally and fore-most, we showed that dropping the semantic featuresdid not impact significantly the performance of ourbest model.With reservations one may have because of thelimited number of semantic predictors in our set,these results however raise some concerns aboutwhether the information coming from semantic vari-ables is really different from that carried by lexico-syntactic features.
Our results clearly show thatthis may not be the case.
This conclusion con-tradicts the assumptions of the structuro-cognitivistparadigm, but corroborates Chall and Dale (1995)?sview that the information carried by semantic pre-dictors is largely correlated with that of lexico-syntactical ones.Further investigation on this issue would defi-nitely be worthwhile, since several facts could ex-plain these contradictory findings.
First, it might bethat semantic and lexical predictors are correlatedbecause the methods used for the parameterizationof the semantic factors heavily relie on lexical infor-mation.
This is the case for the LSA, as well as forthe propositional approach of the content density.Alternatively, this difference with other work inL1 could be due to the L2 context.
Chall andDale (1995) explained that the lexicon and the syn-tax are more important for children learning to readthan for more advanced readers, who then becomemore sensitive to organisationnal aspects.
From thethreshold hypothesis (Alderson, 1984), we knowthat before reaching a sufficient level of proficiency,L2 learners struggle mostly with the lexicon andthe syntactic structures.
This might explain whylexico-syntactic predictors were so predominant inour experiments.
Some further experiments are thusneeded to investigate which of these facts better ac-count for our findings on the semantic features.A last avenue of research worth mentioning wouldbe to develop the family of specific-to-FFL predic-tors, to determine whether taking into account theimpact of a given L1 language on the readability ofL2 texts would increase performance over a genericmodel enough so that tuning efforts are worthwhile.AcknowledgmentsThomas Franc?ois was an Aspirant F.N.R.S.
whenthis work was performed.
The writing of this paperwas done while being a recipient of a Fellowship ofthe Belgian American Educational Foundation.
Wethank both for their support.
We would also like toacknowledge the invaluable help of Bernadette De-hottay for the collection of the corpus used in thatstudy.ReferencesJ.C.
Alderson.
1984.
Reading in a foreign language :a reading problem or a language problem ?
In J.C.Alderson and A.H Urquhart, editors, Reading in a For-eign Language, pages 1?24.
Longman, New York.S.
Andrews.
1997.
The effect of orthographic similarityon lexical retrieval: Resolving neighborhood conflicts.Psychonomic Bulletin & Review, 4(4):439?461.J.
Bahns and M. Eldaw.
1993.
Should We Teach EFLStudents Collocations?
System, 21(1):101?14.A.
Berthet, C. Hugot, V. Kizirian, B. Sampsonis, andM.
Waendendries.
2006.
Alter Ego 1.
Hachette, Paris.C.M.
Bishop.
2006.
Pattern recognition and machinelearning.
Springer, New York.J.R.
Bormuth.
1966.
Readability: A new approach.Reading research quarterly, 1(3):79?132.J.S.
Bowers.
2000.
In defense of abstractionist theoriesof repetition priming and word identification.
Psycho-nomic bulletin and review, 7(1):83?99.M.
Carreiras, N. Carriedo, M.A.
Alonso, andA.
Ferna?ndez.
1997.
The role of verb tense andverb aspect in the foregrounding of informationduring reading.
Memory & Cognition, 25(4):438?446.J.S.
Chall and E. Dale.
1995.
Readability Revisited:The New Dale-Chall Readability Formula.
BrooklineBooks, Cambridge.S.
Chen and J. Goodman.
1999.
An empirical study ofsmoothing techniques for language modeling.
Com-puter Speech and Language, 13(4):359?393.K.
Collins-Thompson and J. Callan.
2004.
A languagemodeling approach to predicting reading difficulty.
In475Proceedings of HLT/NAACL 2004, pages 193?200,Boston, USA.K.
Collins-Thompson and J. Callan.
2005.
Predict-ing reading difficulty with statistical language models.Journal of the American Society for Information Sci-ence and Technology, 56(13):1448?1462.M.
Coltheart.
1978.
Lexical access in simple readingtasks.
In G. Underwood, editor, Strategies of infor-mation processing, pages 151?216.
Academic Press,London.A.
Conquet.
1957.
La lisibilite?.
Assemble?e Permanentedes CCI de Paris, Paris.C.M.
Cornaire.
1988.
La lisibilite?
: essai d?applicationde la formule courte d?Henry au franc?ais languee?trange`re.
Canadian Modern Language Review,44(2):261?273.Council of Europe.
2001.
Common European Frame-work of Reference for Languages: Learning, Teach-ing, Assessment.
Press Syndicate of the University ofCambridge.E.
Dale and J.S.
Chall.
1948.
A formula for predictingreadability.
Educational research bulletin, 27(1):11?28.E.
Dale and R.W.
Tyler.
1934.
A study of the fac-tors influencing the difficulty of reading materials foradults of limited reading ability.
The Library Quar-terly, 4:384?412.F.
Daoust, L. Laroche, and L. Ouellet.
1996.
SATO-CALIBRAGE: Pre?sentation d?un outil d?assistance auchoix et a` la re?daction de textes pour l?enseignement.Revue que?be?coise de linguistique, 25(1):205?234.G.
de Landsheere.
1963.
Pour une application des testsde lisibilite?
de Flesch a` la langue franc?aise.
Le TravailHumain, 26:141?154.E.W.
Dolch.
1948.
Problems in reading.
The GarrardPress, Champaign : Illinois.W.B.
Elley.
1969.
The assessment of readability bynoun frequency counts.
Reading Research Quarterly,4(3):411?427.L.
Feng, M. Jansche, M. Huenerfauth, and N. Elhadad.2010.
A Comparison of Features for Automatic Read-ability Assessment.
In COLING 2010: Poster Volume,pages 276?284.R.
Flesch.
1948.
A new readability yardstick.
Journal ofApplied Psychology, 32(3):221?233.P.W.
Foltz, W. Kintsch, and T.K.
Landauer.
1998.
Themeasurement of textual coherence with latent semanticanalysis.
Discourse processes, 25(2):285?307.T.
Franc?ois and P. Watrin.
2011.
On the contributionof MWE-based features to a readability formula forFrench as a foreign language.
In Proceedings of theInternational Conference RANLP 2011.T.
Franc?ois.
2009.
Combining a statistical languagemodel with logistic regression to predict the lexicaland syntactic difficulty of texts for FFL.
In Proceed-ings of the 12th Conference of the EACL : Student Re-search Workshop, pages 19?27.T.
Franc?ois.
2011a.
La lisibilite?
computationnelle: un renouveau pour la lisibilite?
du franc?ais languepremie`re et seconde ?
International Journal of Ap-plied Linguistics (ITL), 160:75?99.T.
Franc?ois.
2011b.
Les apports du traitement au-tomatique du langage a` la lisibilite?
du franc?ais languee?trange`re.
Ph.D. thesis, Universite?
Catholique de Lou-vain.
Thesis Supervisors : Ce?drick Fairon and AnneCatherine Simon.W.A.
Gale and G. Sampson.
1995.
Good-Turing fre-quency estimation without tears.
Journal of Quantita-tive Linguistics, 2(3):217?237.G.
Gougenheim, R. Miche?a, P. Rivenc, and A. Sauvageot.1964.
L?e?laboration du franc?ais fondamental (1erdegre?).
Didier, Paris.A.C.
Graesser, D.S.
McNamara, M.M.
Louwerse, andZ.
Cai.
2004.
Coh-Metrix: Analysis of text on co-hesion and language.
Behavior Research Methods, In-struments, & Computers, 36(2):193?202.J.
Greenfield.
2004.
Readability formulas for EFL.Japan Association for Language Teaching, 26(1):5?24.M.
Heilman, K. Collins-Thompson, and M. Eskenazi.2008.
An analysis of statistical models and featuresfor reading difficulty prediction.
In Proceedings of theThird Workshop on Innovative Use of NLP for Build-ing Educational Applications, pages 1?8.G.
Henry.
1975.
Comment mesurer la lisibilite?.
Labor,Bruxelles.L.
Kandel and A. Moles.
1958.
Application de l?indicede Flesch a` la langue franc?aise.
Cahiers E?tudes deRadio-Te?le?vision, 19:253?274.S.
Kemper.
1983.
Measuring the inference load of a text.Journal of Educational Psychology, 75(3):391?401.W.
Kintsch and D. Vipond.
1979.
Reading comprehen-sion and readability in educational practice and psy-chological theory.
In L.G.
Nilsson, editor, Perspec-tives on Memory Research, pages 329?365.
LawrenceErlbaum, Hillsdale, NJ.W.
Kintsch, E. Kozminsky, W.J.
Streby, G. McKoon, andJ.M.
Keenan.
1975.
Comprehension and recall of textas a function of content variables1.
Journal of VerbalLearning and Verbal Behavior, 14(2):196?214.H.
Lee, P. Gambette, E.
Maille?, and C. Thuillier.
2010.Denside?es: calcul automatique de la densite?
des ide?esdans un corpus oral.
In Actes de la douxime Rencon-tre des tudiants Chercheurs en Informatique pour leTraitement Automatique des langues (RECITAL).476I.
Lorge.
1944.
Predicting readability.
the Teachers Col-lege Record, 45(6):404?419.J.
Mesnager.
1989.
Lisibilite?
des textes pour enfants: unnouvel outil?
Communication et Langages, 79:18?38.B.J.F.
Meyer.
1982.
Reading research and the composi-tion teacher: The importance of plans.
College com-position and communication, 33(1):37?49.J.B.
Michel, Y.K.
Shen, A.P.
Aiden, A. Veres, M.K.
Gray,The Google Books Team, J.P. Pickett, D. Hoiberg,D.
Clancy, P. Norvig, J. Orwant, S. Pinker, M.A.Nowak, and E.L. Aiden.
2011.
Quantitative analysisof culture using millions of digitized books.
Science,331(6014):176?182.J.R.
Miller and W. Kintsch.
1980.
Readability and re-call of short prose passages: A theoretical analysis.Journal of Experimental Psychology: Human Learn-ing and Memory, 6(4):335?354.B.
New, M. Brysbaert, J. Veronis, and C. Pallier.
2007.The use of film subtitles to estimate word frequencies.Applied Psycholinguistics, 28(04):661?677.R.E.
O?Connor, K.M.
Bell, K.R.
Harty, L.K.
Larkin, S.M.Sackor, and N. Zigmond.
2002.
Teaching reading topoor readers in the intermediate grades: A comparisonof text difficulty.
Journal of Educational Psychology,94(3):474?485.T.
Ozasa, G. Weir, and M. Fukui.
2007.
Measuring read-ability for Japanese learners of English.
In Proceed-ings of the 12th Conference of Pan-Pacific Associationof Applied Linguistics.E.
Pitler and A. Nenkova.
2008.
Revisiting readabil-ity: A unified framework for predicting text quality.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 186?195.J.C.
Redish and J. Selzer.
1985.
The place of readabilityformulas in technical communication.
Technical com-munication, 32(4):46?52.F.
Richaudeau.
1979.
Une nouvelle formule de lisibilite?.Communication et Langages, 44:5?26.H.
Schmid.
1994.
Probabilistic part-of-speech taggingusing decision trees.
In Proceedings of InternationalConference on New Methods in Language Processing,volume 12.
Manchester, UK.S.E.
Schwarm and M. Ostendorf.
2005.
Reading levelassessment using support vector machines and statis-tical language models.
Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics, pages 523?530.L.
Si and J. Callan.
2001.
A statistical model for sci-entific readability.
In Proceedings of the Tenth Inter-national Conference on Information and KnowledgeManagement, pages 574?576.
ACM New York, NY,USA.E.A.
Smith.
1961.
Devereaux readability index.
TheJournal of Educational Research, 54(8):289?303.A.J.
Stenner.
1996.
Measuring reading comprehensionwith the lexile framework.
In Fourth North AmericanConference on Adolescent/Adult Literacy.J.B.
Tharp.
1939.
The Measurement of Vocabulary Dif-ficulty.
Modern Language Journal, pages 169?178.S.
Tuffe?ry.
2007.
Data mining et statistiquede?cisionnelle l?intelligence des donne?es.
E?d.
Technip,Paris.S.
Uitdenbogerd.
2005.
Readability of French as a for-eign language and its uses.
In Proceedings of the Aus-tralian Document Computing Symposium, pages 19?25.P.
van Oosten, V. Hoste, and D. Tanghe.
2011.
A pos-teriori agreement as a quality measure for readabil-ity prediction systems.
In A. Gelbukh, editor, Com-putational Linguistics and Intelligent Text Processing,volume 6609 of Lecture Notes in Computer Science,pages 424?435.
Springer, Berlin / Heidelberg.477
