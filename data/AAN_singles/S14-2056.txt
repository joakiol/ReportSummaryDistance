Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 335?340,Dublin, Ireland, August 23-24, 2014.In-House: An Ensemble of Pre-Existing Off-the-Shelf ParsersYusuke Miyao?, Stephan Oepen?
?, and Daniel Zeman?
?National Institute of Informatics, Tokyo?University of Oslo, Department of Informatics?Potsdam University, Department of Linguistics?Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguisticsyusuke@nii.ac.jp, oe@ifi.uio.no, zeman@ufal.mff.cuni.czAbstractThis submission to the open track ofTask 8 at SemEval 2014 seeks to connectthe Task to pre-existing, ?in-house?
pars-ing systems for the same types of targetsemantic dependency graphs.1 Background and MotivationThe three target representations for Task 8 atSemEval 2014, Broad-Coverage Semantic Depen-dency Parsing (SDP; Oepen et al., 2014), arerooted in language engineering efforts that havebeen under continuous development for at leastthe past decade.
The gold-standard semantic de-pendency graphs used for training and testing inthe Task result from largely manual annotation, inpart re-purposing and adapting resources like thePenn Treebank (PTB; Marcus et al., 1993), Prop-Bank (Palmer et al., 2005), and others.
But thegroups who prepared the SDP target data have alsoworked in parallel on automated parsing systemsfor these representations.Thus, for each of the target representations,there is a pre-existing parser, often developed inparallel to the creation of the target dependencygraphs, viz.
(a) for the DM representation, theparser of the hand-engineered LinGO English Re-source Grammar (ERG; Flickinger, 2000); (b) forPAS, the Enju parsing system (Miyao, 2006), withits probabilistic HPSG acquired through linguis-tic projection of the PTB; and (c) for PCEDT,the scenario for English analysis within the Treexframework (Popel and ?abokrtsk?, 2010), com-bining data-driven dependency parsing with hand-engineered tectogrammatical conversion.
At leastThis work is licenced under a Creative Commons At-tribution 4.0 International License; page numbers and theproceedings footer are added by the organizers.
http://creativecommons.org/licenses/by/4.0/for DM and PAS, these parsers have been exten-sively engineered and applied successfully in avariety of applications, hence represent relevantpoints of comparison.
Through this ?in-house?submission (of our ?own?
parsers to our ?own?task), we hope to facilitate the comparison of dif-ferent approaches submitted to the Task with thispre-existing line of parser engineering.2 DM: The English Resource GrammarSemantic dependency graphs in the DM target rep-resentation, DELPH-IN MRS-Derived Bi-LexicalDependencies, stem from a two-step ?reduc-tion?
(simplification) of the underspecified logical-form meaning representations output natively bythe ERG parser, which implements the linguis-tic framework of Head-Driven Phrase StructureGrammar (HPSG; Pollard and Sag, 1994).
Gold-standard DM training and test data for the Taskwere derived from the manually annotated Deep-Bank Treebank (Flickinger et al., 2012), whichpairs Sections 00?21 of the venerable PTB WallStreet Journal (WSJ) Corpus with complete ERG-compatible HPSG syntactico-semantic analyses.DeepBank as well as the ERG rely on Minimal Re-cursion Semantics (MRS; Copestake et al., 2005)for meaning representation, such that the exactsame post-processing steps could be applied to theparser outputs as were used in originally reducingthe gold-standard MRSs from DeepBank into theSDP bi-lexical semantic dependency graphs.Parsing Setup The ERG parsing system is a hy-brid, combining (a) the hand-built, broad-coverageERG with (b) an efficient chart parser for uni-fication grammars and (c) a conditional proba-bility distribution over candidate analyses.
Theparser most commonly used with the ERG, calledPET (Callmeier, 2002),1constructs a complete,1The SDP test data was parsed using the 1212 releaseof the ERG, using PET and converter versions from what335subsumption-based parse forest of partial HPSGderivations (Oepen and Carroll, 2000), and thenextracts from the forest n-best lists (in globallycorrect rank order) of complete analyses accordingto a discriminative parse ranking model (Zhang etal., 2007).
For our experiments, we trained theparse ranker on Sections 00?20 of DeepBank andotherwise used the default, non-pruning develop-ment configuration, which is optimized for accu-racy.
In this setup, ERG parsing on average takesclose to ten seconds per sentence.Post-Parsing Conversion After parsing, MRSsare reduced to DM bi-lexical semantic dependen-cies in two steps.
First, Oepen and L?nning(2006) define a conversion to variable-free Ele-mentary Dependency Structures (EDS), which (a)maps each predication in the MRS logical-formmeaning representation to a node in a dependencygraph and (b) transforms argument relations rep-resented by shared logical variables into directeddependency links between graph nodes.
This firststep of the conversion is ?mildly?
lossy, in thatsome scope-related information is discarded; theEDS graph, however, will contain the same num-ber of nodes and the same set of argument de-pendencies as there are predications and semanticrole assignments in the original MRS.
In particu-lar, the EDS may still reflect non-lexical semanticpredications introduced by grammatical construc-tions like covert quantifiers, nominalization, com-pounding, or implicit conjunction.2Second, in another conversion step that is notinformation-preserving, the EDS graphs are fur-ther reduced into strictly bi-lexical form, i.e.
a setof directed, binary dependency relations holdingexclusively between lexical units.
This conversionis defined by Ivanova et al.
(2012) and seeks to(a) project some aspects of construction seman-tics onto word-to-word dependencies (for exampleintroducing specific dependency types for com-pounding or implicit conjunction) and (b) relatethe linguistically informed ERG-internal tokeniza-tion to the conventions of the PTB.3Seeing as bothis called the LOGON SVN trunk as of January 2014; seehttp://moin.delph-in.net/LogonTop for detail.2Conversely, semantically vacuous parts of the originalinput (e.g.
infinitival particles, complementizers, relative pro-nouns, argument-marking prepositions, auxiliaries, and mostpunctuation marks) were not represented in the MRS in thefirst place, hence have no bearing on the conversion.3Adaptations of tokenization encompass splitting ?multi-word?
ERG tokens (like such as or ad hoc), as well as ?hiding?ERG token boundaries at hyphens or slashes (e.g.
77-year-conversion steps are by design lossy, DM seman-tic dependency graphs present a true subset of theinformation encoded in the full, original MRS.3 PAS: The Enju Parsing SystemEnju Predicate?Argument Structures (PAS) arederived from the automatic HPSG-style annota-tion of the PTB, which was primarily used for thedevelopment of the Enju parsing system4(Miyao,2006).
A notable feature of this parser is that thegrammar is not developed by hand; instead, theEnju HPSG-style treebank is first developed, andthe grammar (or, more precisely, the vast major-ity of lexical entries) is automatically extractedfrom the treebank (Miyao et al., 2004).
In this?projection?
step, PTB annotations such as emptycategories and coindexation are used for deriv-ing the semantic representations that correspondto HPSG derivations.
Its probabilistic model fordisambiguation is also trained using this treebank(Miyao and Tsujii, 2008).5The PAS data set is an extraction of predicate?argument structures from the Enju HPSG tree-bank.
The Enju parser outputs results in ?ready-to-use?
formats like phrase structure trees andpredicate?argument structures, as full HPSG anal-yses are not friendly to users who are not famil-iar with the HPSG theory.
The gold-standard PAStarget data in the Task was developed using thisfunction; the conversion program from full HPSGanalyses to predicate?argument structures was ap-plied to the Enju Treebank.Predicate?argument structures (PAS) representword-to-word semantic dependencies, such as se-mantic subject and object.
Each dependency typeis represented with two elements: the type of thepredicate, such as verb and adjective, and the ar-gument label, such as ARG1 and ARG2.6old), which the PTB does not split.4See http://kmcs.nii.ac.jp/enju/.5Abstractly similar to the ERG, the annotations of theEnju treebank instantiate the linguistic theory of HPSG.However, the two resources have been developed indepen-dently and implementation details are quite different.
Themost significant difference is that the Enju HPSG treebank isdeveloped by linguistic projection of PTB annotations, andthe Enju parser derived from the treebank; conversely, theERG was predominantly manually crafted, and it was laterapplied in the DeepBank re-annotation of the WSJ Corpus.6Full details of the predicate?argument structures in theEnju HPSG Treebank, are available in two documents linkedfrom the Enju web site (see above), viz.
the Enju OutputSpecification Manual and the XML Format Documentation.336Parsing Setup Basically we used the publiclyavailable package of the Enju parser ?as is?
(see theabove web site).
We did not change default pars-ing parameters (beam width, etc.)
and features.However, the release version of the Enju parser istrained with the HPSG treebank corresponding tothe Penn Treebank WSJ Sections 2?21, which in-cludes the test set of the Task (Section 21).
There-fore, we re-trained the Enju parser using Sections0?20, and used this re-trained parser in preparingthe PAS semantic dependency graphs in this en-semble submission.Post-Parsing Conversion The dependency for-mat of the Enju parser is almost equivalent to whatis provided as the PAS data set in this shared task.Therefore, the post-parsing conversion for the PASdata involves only formatting, viz.
(a) format con-version into the tabular file format of the Task; and(b) insertion of dummy relations for punctuationtokens ignored in the output of Enju.74 PCEDT: The Treex Parsing ScenarioThe Prague Czech-English Dependency Treebank(PCEDT; Haji?c et al., 2012)8is a set of parallel de-pendency trees over the same WSJ texts from thePenn Treebank, and their Czech translations.
Sim-ilarly to other treebanks in the Prague family, thereare two layers of syntactic annotation: analytical(a-trees) and tectogrammatical (t-trees).
Unlikefor the other two representations used in the Task,for PCEDT there is no pre-existing parsing systemdesigned to deliver the full scale of annotationsof the SDP gold-standard data.
The closest avail-able match is a parsing scenario implemented inthe Treex natural language processing framework.Parsing Setup Treex9(Popel and ?abokrtsk?,2010) is a modular, open-source framework origi-nally developed for transfer-based machine trans-lation.
It can accomplish any NLP-related taskby sequentially applying to the same piece of datavarious blocks of code.
Blocks operate on a com-mon data structure and are chained in scenarios.Some early experiments with scenarios for tec-togrammatical analysis of English were describedby Klime?
(2007).
It is of interest that they report7The Enju parser ignores tokens tagged as ?.
?, whilethe PAS representation includes them with dummy relations;thus, missing periods are inserted in post-processing by com-parison to the original PTB token sequence.8See http://ufal.mff.cuni.cz/pcedt2.0/.9See http://ufal.mff.cuni.cz/treex/.U.S.
should regulate X more stringently than  YCPRPATPREDACTPATMANNCPRPATPREDACTPATMANN CPRFigure 1: PCEDT asserts two copies of the tokenregulate (shown here as ?regulate?
and ??, under-lined).
Projecting t-nodes onto the original tokens,required by the SDP data format, means that the node will be merged with regulate.
The edgesgoing to and from  will now lead to and from reg-ulate (see the dotted arcs), which results in a cycle.To get rid of the cycle, we skip  and connect di-rectly its children, as shown in the final SDP graphbelow the sentence.an F1score of assigning functors (dependency la-bels in PCEDT terminology) of 70.3%; however,their results are not directly comparable to ours.Due to the modular nature of Treex, there arevarious conceivable scenarios to get the t-tree ofa sentence.
We use the default scenario that con-sists of 48 blocks: two initial blocks (reading theinput), one final block (writing the output), twoA2N blocks (named entity recognition), twelveW2A blocks (dependency parsing at the analyticallayer) and 31 A2T and T2T blocks (creating thet-tree based on the a-tree).Most blocks are highly specialized in one par-ticular subtask (e.g.
there is a block just to makesure that quotation marks are attached to the rootof the quoted subtree).
A few blocks are respon-sible for the bulk of the work.
The a-tree is con-structed by a block that contains the MST Parser(McDonald et al., 2005), trained on the CoNLL2007 English data (Nivre et al., 2007), i.e.
Sec-tions 2?11 of the PTB, converted to dependencies.The annotation style of CoNLL 2007 differs fromPCEDT 2.0, and thus the unlabeled attachmentscore of the analytical parser is only 66%.Obviously one could expect better results if weretrained the MST Parser directly on the PCEDTa-trees, and on the whole training data.
The onlyreason why we did not do so was lack of time.Our results thus really demonstrate what is avail-able ?off-the-shelf?
; on the other hand, the PCEDTcomponent of our ensemble fails to set any ?upperbound?
of output quality, as it definitely is not bet-337John brought and ate ripe apples and pearsACTCONJCONJPRED.m PRED.mRSTRPAT.m PAT.mPATTOP TOPPATPATACTACTCONJ.m CONJ.mRSTRRSTRPATCONJ.m CONJ.mFigure 2: Coordination in PCEDT t-tree (above)and in the corresponding SDP graph (below).ter informed than the other systems participatingin the Task.Functor assignment is done heuristically, basedon POS tags and function words.
The primaryfocus of the scenario was on functors that couldhelp machine translation, thus it only generated25 different labels (of the total set of 65 labels inthe SDP gold-standard data)10and left about 12%of all nodes without functors.
Precision peaks at78% for ACT(or) relations, while the most fre-quent error type (besides labelless dependencies)is a falsely proposed RSTR(iction) relation.
BothACT and RSTR are among the most frequent de-pendency types in PCEDT.Post-Parsing Conversion Once the t-tree hasbeen constructed, it is converted to the PCEDTtarget representation of the Task, using the sameconversion code that was used to prepare the gold-standard SDP data.11SDP graphs are defined over surface tokens butthe set of nodes of a t-tree need not correspondone-to-one to the set of tokens.
For example, thereare no t-nodes for punctuation and function words(except in coordination); these tokens are renderedas semantically vacuous in SDP, i.e.
they do notparticipate in edges.
On the other hand, t-trees cancontain generated nodes, which represent elidedwords and do not correspond to any surface to-10The system was able to output the following functors (or-dered in the descending order of their frequency in the sys-tem output): RSTR, PAT, ACT, CONJ.member, APP, MANN,LOC, TWHEN, DISJ.member, BEN, RHEM, PREC, ACMP,MEANS, ADVS.member, CPR, EXT, DIR3, CAUS, COND,TSIN, REG, DIR2, CNCS, and TTILL.11In the SDP context, the target representation derivedfrom the PCEDT is called by the same name as the origi-nal treebank; but note that the PCEDT semantic dependencygraphs only encode a subset of the information annotated atthe tectogrammatical layer of the full treebank.DM PAS PCEDTLF LM LF LM LF LMPriberam .8916 .2685 .9176 .3783 .7790 .1068In-House .9246 .4807 .9206 .4384 .4315 .0030UF UM UF UM UF UMPriberam .9032 .2990 .9281 .3924 .8903 .3071In-House .9349 .5230 .9317 .4429 .6919 .0148Table 1: End-to-end ?in-house?
parsing results.ken.
Most generated nodes are leaves and, thus,can simply be omitted from the SDP graphs.
Othergenerated nodes are copies of normal nodes andthey are linked to the same token to which thesource node is mapped.
As a result, one token canappear at several different positions in the tree; ifwe project these occurrences into one node, thegraph will contain cycles.
We decided to removeall generated nodes causing cycles.
Their chil-dren are attached to their parents and inherit thefunctor of the generated node (Figure 1).
The con-version procedure also removes cycles caused bymore fine-grained tokenization of the t-layer.Furthermore, t-trees use technical edges to cap-ture paratactic constructions where the relationsare not ?true?
dependencies.
The conversion pro-cedure extracts true dependency relations: Eachconjunct is linked to the parent or to a shared childof the coordination.
In addition, there are alsolinks from the conjunction to the conjuncts andthey are labeled CONJ.m(ember).
These links pre-serve the paratactic structure (which can even benested) and the type of coordination.
See Figure 2for an example.5 Results and ReflectionsSeeing as our ?in-house?
parsers are not directlytrained on the semantic dependency graphs pro-vided for the Task, but rather are built from ad-ditional linguistic resources, we submitted resultsfrom the parsing pipelines sketched in Sections 2to 4 above to the open SDP track.
Table 1summarizes parser performance in terms of la-beled and unlabeled F1(LF and UF)12and full-sentence exact match (LM and UM), comparingto the best-performing submission (dubbed Prib-eram; Martins and Almeida, 2014) to this track.Judging by the official SDP evaluation metric, av-erage labeled F1over the three representations,our ensemble ranked last among six participating12Our ensemble members exhibit comparatively small dif-ferences in recall vs. precision.338teams; in terms of unlabeled average F1, the ?in-house?
submission achieved the fourth rank.As explained in the task description (Oepen etal., 2014), parts of the WSJ Corpus were excludedfrom the SDP training and testing data becauseof gaps in the DeepBank and Enju treebanks, andto exclude cyclic dependency graphs, which cansometimes arise in the DM and PCEDT conver-sions.
For these reasons, one has to allow for thepossibility that the testing data is positively bi-ased towards our ensemble members.13But evenwith this caveat, it seems fair to observe that theERG and Enju parsers both are very competitivefor the DM and PAS target representations, respec-tively, specifically so when judged in exact matchscores.
A possible explanation for these resultslies in the depth of grammatical information avail-able to these parsers, where DM or PAS seman-tic dependency graphs are merely a simpliefiedview on the complete underlying HPSG analyses.These parsers have performed well in earlier con-trastive evaluation too (Miyao et al., 2007; Benderet al., 2011; Ivanova et al., 2013; inter alios).Results for the Treex English parsing scenario,on the other hand, show that this ensemble mem-ber is not fine-tuned for the PCEDT target rep-resentation; due to the reasons mentioned above,its performance even falls behind the shared taskbaseline.
As is evident from the comparison oflabeled vs. unlabeled F1scores, (a) the PCEDTparser is comparatively stronger at recovering se-mantic dependency structure than at assigning la-bels, and (b) about the same appears to be the casefor the best-performing Priberam system (on thistarget representation).AcknowledgementsData preparation and large-scale parsing in theDM target representation was supported throughaccess to the ABEL high-performance computingfacilities at the University of Oslo, and we ac-knowledge the Scientific Computing staff at UiO,the Norwegian Metacenter for Computational Sci-ence, and the Norwegian tax payers.
This projecthas been supported by the infrastructural funding13There is no specific evidence that the WSJ sentences ex-cluded in the Task for technical issues in either of the under-lying treebanks or conversion procedures would be compara-tively much easier to parse for other submissions than for themembers of our ?in-house?
ensemble, but unlike other sys-tems these parsers ?had a vote?
in the selection of the data,particularly so for the DM and PAS target representations.by the Ministry of Education, Youth and Sports ofthe Czech Republic (CEP ID LM2010013).ReferencesBender, E. M., Flickinger, D., Oepen, S., andZhang, Y.
(2011).
Parser evaluation over localand non-local deep dependencies in a large cor-pus.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Pro-cessing (p. 397 ?
408).
Edinburgh, Scotland,UK.Callmeier, U.
(2002).
Preprocessing and encodingtechniques in PET.
In S. Oepen, D. Flickinger,J.
Tsujii, and H. Uszkoreit (Eds.
), Collabora-tive language engineering.
A case study in effi-cient grammar-based processing (p. 127 ?
140).Stanford, CA: CSLI Publications.Copestake, A., Flickinger, D., Pollard, C., andSag, I.
A.
(2005).
Minimal Recursion Seman-tics.
An introduction.
Research on Languageand Computation, 3(4), 281 ?
332.Flickinger, D. (2000).
On building a more ef-ficient grammar by exploiting types.
NaturalLanguage Engineering, 6 (1), 15 ?
28.Flickinger, D., Zhang, Y., and Kordoni, V. (2012).DeepBank.
A dynamically annotated treebankof the Wall Street Journal.
In Proceedings of the11th International Workshop on Treebanks andLinguistic Theories (p. 85 ?
96).
Lisbon, Portu-gal: Edi?
?es Colibri.Haji?c, J., Haji?cov?, E., Panevov?, J., Sgall, P.,Bojar, O., Cinkov?, S., .
.
.
?abokrtsk?, Z.(2012).
Announcing Prague Czech-English De-pendency Treebank 2.0.
In Proceedings of the8th International Conference on Language Re-sources and Evaluation (p. 3153 ?
3160).
Istan-bul, Turkey.Ivanova, A., Oepen, S., Dridan, R., Flickinger, D.,and ?vrelid, L. (2013).
On different approachesto syntactic analysis into bi-lexical dependen-cies.
An empirical comparison of direct, PCFG-based, and HPSG-based parsers.
In Proceedingsof the 13th International Conference on ParsingTechnologies (p. 63 ?
72).
Nara, Japan.Ivanova, A., Oepen, S., ?vrelid, L., andFlickinger, D. (2012).
Who did what to whom?339A contrastive study of syntacto-semantic depen-dencies.
In Proceedings of the Sixth LinguisticAnnotation Workshop (p. 2 ?
11).
Jeju, Republicof Korea.Klime?, V. (2007).
Transformation-based tec-togrammatical dependency analysis of English.In V. Matou?ek and P. Mautner (Eds.
), Text,speech and dialogue 2007, LNAI 4629 (p. 15 ?22).
Berlin / Heidelberg, Germany: Springer.Marcus, M., Santorini, B., and Marcinkiewicz,M.
A.
(1993).
Building a large annotated cor-pora of English: The Penn Treebank.
Computa-tional Linguistics, 19, 313 ?
330.Martins, A. F. T., and Almeida, M. S. C. (2014).Priberam.
A turbo semantic parser with secondorder features.
In Proceedings of the 8th In-ternational Workshop on Semantic Evaluation.Dublin, Ireland.McDonald, R., Pereira, F., Ribarov, K., and Haji?c,J.
(2005).
Non-projective dependency parsingusing spanning tree algorithms.
In Proceedingsof the Human Language Technology Conferenceand Conference on Empirical Methods in Nat-ural Language Processing (p. 523 ?
530).
Van-couver, British Columbia, Canada.Miyao, Y.
(2006).
From linguistic theory tosyntactic analysis.
Corpus-oriented grammardevelopment and feature forest model.
Doc-toral Dissertation, University of Tokyo, Tokyo,Japan.Miyao, Y., Ninomiya, T., and Tsujii, J.
(2004).Corpus-oriented grammar development for ac-quiring a Head-Driven Phrase Structure Gram-mar from the Penn Treebank.
In Proceedings ofthe 1st International Joint Conference on Natu-ral Language Processing (p. 684 ?
693).Miyao, Y., Sagae, K., and Tsujii, J.
(2007).Towards framework-independent evaluation ofdeep linguistic parsers.
In Proceedings ofthe 2007 Workshop on Grammar Engineeringacross Frameworks (p. 238 ?
258).
Palo Alto,California.Miyao, Y., and Tsujii, J.
(2008).
Feature forestmodels for probabilistic HPSG parsing.
Com-putational Linguistics, 34(1), 35 ?
80.Nivre, J., Hall, J., K?bler, S., McDonald, R., Nils-son, J., Riedel, S., and Yuret, D. (2007).
TheCoNLL 2007 shared task on dependency pars-ing.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural Lan-guage Processing and Conference on NaturalLanguage Learning (p. 915 ?
932).
Prague,Czech Republic.Oepen, S., and Carroll, J.
(2000).
Ambiguitypacking in constraint-based parsing.
Practicalresults.
In Proceedings of the 1st Meeting of theNorth American Chapter of the Association forComputational Linguistics (p. 162 ?
169).
Seat-tle, WA, USA.Oepen, S., Kuhlmann, M., Miyao, Y., Zeman, D.,Flickinger, D., Haji?c, J., .
.
.
Zhang, Y.
(2014).SemEval 2014 Task 8.
Broad-coverage seman-tic dependency parsing.
In Proceedings of the8th International Workshop on Semantic Evalu-ation.
Dublin, Ireland.Oepen, S., and L?nning, J. T. (2006).Discriminant-based MRS banking.
In Proceed-ings of the 5th International Conference onLanguage Resources and Evaluation (p. 1250 ?1255).
Genoa, Italy.Palmer, M., Gildea, D., and Kingsbury, P. (2005).The Proposition Bank.
A corpus annotated withsemantic roles.
Computational Linguistics,31(1), 71 ?
106.Pollard, C., and Sag, I.
A.
(1994).
Head-DrivenPhrase Structure Grammar.
Chicago, USA:The University of Chicago Press.Popel, M., and ?abokrtsk?, Z.
(2010).
TectoMT.Modular NLP framework.
Advances in NaturalLanguage Processing, 293 ?
304.Zhang, Y., Oepen, S., and Carroll, J.
(2007).Efficiency in unification-based n-best parsing.In Proceedings of the 10th International Con-ference on Parsing Technologies (p. 48 ?
59).Prague, Czech Republic.340
