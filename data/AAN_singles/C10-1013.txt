Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 107?115,Beijing, August 2010Towards an optimal weighting of context words based on distanceBernard Brosseau-Villeneuve*#, Jian-Yun Nie*, Noriko Kando#* Universit?
de Montr?al, Email: {brosseab, nie}@iro.umontreal.ca# National Institute of Informatics, Email: {bbrosseau, kando}@nii.ac.jpAbstractWord Sense Disambiguation (WSD) of-ten relies on a context model or vectorconstructed from the words that co-occurwith the target word within the same textwindows.
In most cases, a fixed-sizedwindow is used, which is determined bytrial and error.
In addition, words withinthe same window are weighted uniformlyregardless to their distance to the targetword.
Intuitively, it seems more reason-able to assign a stronger weight to con-text words closer to the target word.
How-ever, it is difficult to manually define theoptimal weighting function based on dis-tance.
In this paper, we propose a unsu-pervised method for determining the op-timal weights for context words accord-ing to their distance.
The general idea isthat the optimal weights should maximizethe similarity of two context models of thetarget word generated from two randomsamples.
This principle is applied to bothEnglish and Japanese.
The context mod-els using the resulting weights are usedin WSD tasks on Semeval data.
Our ex-perimental results showed that substantialimprovements in WSD accuracy can beobtained using the automatically definedweighting schema.1 IntroductionThe meaning of a word can be defined by thewords that accompany it in the text.
This is theprinciple often used in previous studies on WordSense Disambiguation (WSD) (Ide and V?ronis,1998; Navigli, 2009).
In general, the accompa-nying words form a context vector of the targetword, or a probability distribution of the contextwords.
For example, under the unigram bag-of-words assumption, this means building p(x|t) =count(x,t)?x?
count(x?,t), where count(x, t) is the count ofco-occurrences of word x with the target word tunder a certain criterion.
In most studies, x andt should co-occur within a window of up to kwords or sentences.
The bounds are usually se-lected in an ad-hoc fashion to maximize systemperformance.
Occurrences inside the window of-ten weight the same without regard to their po-sition.
This is counterintuitive.
Indeed, a wordcloser to the target word generally has a greatersemantic constraint on the target word than a moredistant word.
It is however difficult to definethe optimal weighting function manually.
To getaround this, some systems add positional featuresfor very close words.
In information retrieval, tomodel the strength of word relations, some studieshave proposed non-uniform weighting methods ofcontext words, which decrease the importance ofmore distant words in the context vector.
How-ever, the weighting functions are defined manu-ally.
It is unclear that these functions can best cap-ture the impact of the context words on the targetword.In this paper, we propose an unsupervisedmethod to automatically learn the optimal weightof a word according to its distance to the targetword.
The general principle used to determinesuch weight is that, if we randomly determinetwo sets of windows containing the target wordfrom the same corpus, the meaning ?
or mixtureof meanings for polysemic words ?
of the targetword in the two sets should be similar.
As the con-text model ?
a probability distribution for the con-text words ?
determines the meaning of the targetword, the context models generated from the twosets should also be similar.
The weights of con-text words at different distance are therefore de-107termined so as to maximize the similarity of con-text models generated from the two sets of sam-ples.
In this paper, we propose a gradient descentmethod to find the optimal weights.
We will seethat the optimal weighting functions are differentfrom those used in previous studies.
Experimenta-tion on Semeval-2007 English and Semeval-2010Japanese lexical sample task data shows that im-provements can be attained using the resultingweighting functions on simple Na?ve Bayes (NB)systems in comparison to manually selected func-tions.
This result validates the general principlewe propose in this paper.The remainder of this paper is organized as fol-lows: typical uses of text windows and relatedwork are presented in Section 2.
Our methodis presented in Section 3.
In Section 4 to 6,we show experimental results on English andJapanese WSD.
We conclude in Section 7 withdiscussion and further possible extensions.2 Uses of text windowsModeling the distribution of words around onetarget word, which we call context model, hasmany uses.
For instance, one can use it to definea co-occurrence-based stemmer (Xu and Croft,1998), which uses window co-occurrence statis-tics to calculate the best equivalence classes for agroup of word forms.
In the study of Xu and Croft,they suggest using windows of up to 100 words.Context models are also widely used in WSD.For example, top performing systems on EnglishWSD tasks in Semeval-2007, such as NUS-ML(Cai et al, 2007), all made use of bag-of-wordsfeatures around the target word.
In this case, theyfound that the best results can be achieved using awindow size of 3.Both systems limit the size of their windows fordifferent purposes.
The former uses a large size inorder to model the topic of the documents contain-ing the word rather than the word?s meaning.
Thelatter would limit the size because bag-of-wordsfeatures further from the target word would not besufficiently related to its meaning (Ide and V?ro-nis, 1998).
We see that there is a compromise be-tween taking fewer, highly related words, or tak-ing more, lower quality words.
However, there isno principled way to determine the optimal sizeof windows.
The size is determined by trial anderror.A more questionable aspect in the above sys-tems is that for bag-of-words features, all wordsin a window are given equal weights.
This iscounterintuitive.
One can easily understand thata context word closer to the target word gener-ally imposes a stronger constraint on the meaningof the latter, than a more distant context word.
Itis then reasonable to define a weighting functionthat decreases along with distance.
Several studiesin information retrieval (IR) have proposed suchfunctions to model the strength of dependency be-tween words.
For instance, Gao et al (2002)proposed an exponential decay function to capturethe strength of dependency between words.
Thisfunction turns out to work better than the uniformweighting in the IR experiments.Song and Bruza (2003) used a fixed-size slid-ing window to determine word co-occurrences.This is equivalent to define a linear decay func-tion for context words.
The context vectors de-fined this way are used to estimate similarity be-tween words.
A use of the resulting similarity inquery expansion in IR turned out to be successful(Bai et al, 2005).In a more recent study, Lv and Zhai (2009) eval-uated several kernel functions to determine theweights of context words according to distance,including Gaussian kernel, cosine kernel, and soon.
As for the exponential and linear decayingfunctions, all these kernel functions have fixedshapes, which are determined manually.Notice that the above functions have only beentested in IR experiments.
It is not clear howthese functions perform in WSD.
More impor-tantly, all the previous studies have investigatedonly a limited number of weighting functions forcontext words.
Although some improvements us-ing these functions have been observed in IR, itis not clear whether the functions can best capturethe true impact of the context words on the mean-ing of the target word.
Although the proposedfunctions comply with the general principle thatcloser words are more important than more dis-tant words, no principled way has been proposedto determine the particular shape of the functionfor different languages and collections.108In this paper, we argue that there is indeed a hid-den weighting function that best capture the im-pact of context words, but the function cannot bedefined manually.
Rather, the best function shouldbe the one that emerges naturally from the data.Therefore, we propose an unsupervised method todiscover such a function based on the followingprinciple: the context models for a target wordgenerated from two random samples should besimilar.
In the next section, we will define in detailhow this principle is used.3 Computing weights for distancesIn this section, we present our method for choos-ing how much a word occurrence should count inthe context model according to its distance to thetarget word.
In this study, for simplicity, we as-sume that all word occurrences at a given distancecount equally in the context model.
That is, weignore other features such as POS-tags, which areused in other studies on WSD.Let C be a corpus, W a set of text windows forthe target word w, cW,i,x the count of occurrencesof word x at distance i in W , cW,i the sum of thesecounts, and ?i the weight put on one word occur-rence at distance i. Then,PML,W (x) =?i ?icW,i,x?i ?icW,i(1)is the maximum likelihood estimator for x in thecontext model of w. To counter the zero probabil-ity problem, we apply Dirichlet smoothing withthe collection language model as a prior:PDir,W (x) =?i ?icW,i,x + ?WP (x|C)?i ?icW,i + ?W(2)The pseudo-count ?W can be a constant, or can befound by using Newton?s method, maximizing thelog likelihood via leave-one-out estimation:L?1(?|W, C) =?i?x?V ?icW,i,x log?icW,i,x?
?i+?P (x|C)?j ?jcW,j?
?i+?The general process, which we call automaticDirichlet smoothing, is similar to that describedin (Zhai and Lafferty, 2002).To find the best weights for our model we pro-pose the following process:?
Let T be the set of all windows containingthe target word.
We randomly split this setinto two sets A and B.?
We want to find ??
that maximizes the sim-ilarity of the models obtained from the twosets, by minimizing their mutual cross en-tropy:l(?)
= H(PML,A, PDir,B) + (3)H(PML,B , PDir,A)In other words, we want ?i to represent how muchan occurrence at distance i models the contextbetter than the collection language model, whosecounts are weighted by the Dirichlet parameter.We hypothesize that target words occur in limitedcontexts, and as we get farther from them, the pos-sibilities become greater, resulting in sparse andless related counts.
Since two different sets of thesame word are essentially noisy samples of thesame distribution, the weights maximizing theirmutual generation probabilities should model thisphenomenon.One may wonder why we do not use a distri-bution similarity metric such as Kullback?Leibler(KL) divergence or Information Radius (IRad).The reason is that with enough word occurrences(big windows or enough samples), the most sim-ilar distributions are found with uniform weights,when all word counts are used.
KL divergenceis especially problematic as, since it requiressmoothing, the weights will converge to the de-generate weights ?
= 0, where only the identicalsmoothing counts remain.
Entropy minimizationis therefore needed in the objective function.To determine the optimal weight of ?i, we pro-pose a simple gradient descent minimizing (3)over ?.
The following are the necessary deriva-tives:?l?
?i= ?H(PML,A, PDir,B)?
?i+?H(PML,B , PDir,A)?
?i?H(PML,W , PDir,(T?W ))??i=109?
?x?V[?PML,W (x)?
?ilog PDir,(T?W )(x)+?PDir,(T?W )(x)??i?
PML,W (x)PDir,(T?W )(x)]?PML,W (x)?
?i= cW,i,x ?
PML,W (x)cW,i?j ?jcW,j?PDir,W (x)?
?i= cW,i,x ?
PDir,W (x)cW,i?j ?jcW,j + ?WWe use stochastic gradient descent: one word isselected randomly, it?s gradient is computed, asmall gradient step is done and the process is re-peated.
A pseudo-code of the process can befound in Algorithm 1.Algorithm 1 LearnWeight(C, ?, )?
?
1krepeatT ?
{Get windows for next word}(A,B) ?RandomPartition(T )for W in A,B doPML,W ?MakeML(W ,?
)?W ?ComputePseudoCount(W ,C)PDir,W ?MakeDir( PML,W , ?W , C)end forgrad ?
?H(PML,A, PDir,B) +?H(PML,B, PDir,A)?
?
??
?
grad?grad?until ?
?i < return ?/max{?i}Now, as the objective function would eventu-ally go towards putting nearly all weight on ?1,we hypothesize that the farthest distances shouldhave a near-zero contribution, and determine thestop criterion as having one weight go under asmall threshold.
Alternatively, a control set ofheld out words can be used to observe the progressof the objective function or the gradient length.When more and more weight is put on the fewclosest positions, the objective function and gra-dient depends on less counts and will become lessstable.
This can be used as a stop criterion.The above weight learning process is appliedon an English collection and a Japanese collectionwith ?
=  = 0.001, and ?
= 1000.
In the nextsections, we will describe both resulting weight-ing functions in the context of WSD experiments.4 Classifiers for supervised WSD tasksSince we use the same systems for both Englishand Japanese experiments, we will briefly discussthe used classifiers in this section.
In both tasks,the objective is to maximize WSD accuracy onheld-out data, given that we have a set of trainingtext passages containing a sense-annotated targetword.The first of our baselines, the Most FrequentSense (MFS) system always selects the most fre-quent sense in the training set.
It gives us a lowerbound on system accuracies.Na?ve Bayes (NB) classifiers score classes us-ing the Bayes formula under a feature indepen-dence assumption.
Let w be the target word in agiven window sample to be classified, the scoringformula for sense class S is:Score(w,S) = P (S)PTar(w|S)?Tar?
?xi?context(w) PCon(xi|S)?Con?dist(xi)where dist(xi) is the distance between the contextword xi and the target word w. The target wordbeing an informative feature present in all sam-ples, we use it in a target word language modelPTar .
The surrounding words are summed in thecontext model PCon as shown in equation (1).
Aswe can see with the presence of ?
in the equation,the scoring follows the same weighting scheme aswe do when accumulating counts, since the sam-ples to classify follow the same distribution as thetraining ones.
Also, when a language model usesautomatic Dirichlet smoothing, the impact of thefeatures against the prior is controlled with themanual parameters ?Tar or ?Con.
When a man-ual smoothing parameter is used, it also handlesimpact control.
Our systems use the followingweight functions:Uniform: ?i = 11?i?
?, where ?
is a window sizeand 1 the indicator function.Linear: ?i = max{0, 1 ?
(i ?
1)?
}, where ?
isthe decay rate.110Exponential: ?i = e?(i?1)?
, where ?
is the ex-ponential parameter.Learned: ?i is the weight learned as shown pre-viously.The parameters for NB systems are identical forall words of a task and were selected by exhaustivesearch, maximizing leave-one-out accuracy on thetraining set.
For each language model, we triedLaplace, manual Dirichlet and automatic Dirichletsmoothing.For the sake of comparison, also we provide aSupport Vector Machine (SVM) classifier, whichproduces the best results in Semeval 2007.
Weused libSVM with a linear kernel, and regular-ization parameters were selected via grid searchmaximizing leave-one-out accuracy on the train-ing set.
We tested the following windows limits:all words in sample, current sentence, and variousfixed window sizes.
We used the same featuresas the NB systems, testing Boolean, raw count,log-of-counts and counts from weight functionsrepresentations.
Although non-Boolean featureshad good leave-one-out precision on the trainingdata, since SVM does not employ smoothing, onlyBoolean features kept good results on test data, soour SVM baseline uses Boolean features.5 WSD experiments on Semeval-2007English Lexical SampleThe Semeval workshop holds WSD tasks such asthe English Lexical Sample (ELS) (Pradhan et al,2007).
The task is to maximize WSD accuracy ona selected set of polysemous words, 65 verbs and35 nouns, for which passages were taken from theWSJ Tree corpus.
Passages contain a couple ofsentences around the target word, which is manu-ally annotated with a sense taken from OntoNotes(Hovy et al, 2006).
The sense inventory is quitecoarse, with an average of 3.6 senses per word.Instances count are listed in Table 1.Train Test TotalVerb 8988 2292 11280Noun 13293 2559 15852Total 22281 4851Table 1: Number of instances in the ELS dataFigure 1: Weight curve for AP88-90Since there are only 100 target words and in-stances are limited in the Semeval collection, wedo not have sufficient samples to estimate the op-timal weights for context words.
Therefore, weused the AP88-90 corpus of the TREC collection(CD 1 & 2) in our training process.
The AP col-lection contains 242,918 documents.
Since ourclassifiers use word stems, the collection was alsostemmed with the Porter stemmer and sets of win-dows were built for all word stems.
To get near-uniform counts in all distances, only full win-dows with a size of 100, which was consideredbig enough without any doubt, were kept.
In orderto get more samples, windows to the right and tothe left were separated.
For each target word, weused 1000 windows.
A stoplist of the top 10 fre-quent words was used, but place holders were leftin the windows to preserve the distances.
Mul-tiple consecutive stop words (ex: ?of the?)
weremerged, and the target word stem, being the samefor all samples of a set, was ignored in the con-struction of context models.
The AP collection re-sults in 32,650 target words containing 5,870,604windows.
The training process described in Sec-tion 3 is used to determine the best weights of con-text words.
Figure 1 shows the first 40 elementsof the resulting weighting function curve.As we can see, the curve is neither exponen-tial, linear, or any of the forms used by Lv andZhai.
Its form is rather similar to x?
?, or ratherlog?1(?
+ x) minus some constant.
The decrease111System Cross-Val (%) Test set (%)MFS 78.66 77.76Uniform NB 86.04 84.52SVM 85.53 85.03Linear NB 86.89 85.71Exp.
NB 87.80 86.23Learned NB 88.46 86.70Table 2: WSD accuracy on Semeval-2007 ELCrate is initially very high and then reduces as itbecomes closer to zero.
This long tail is notpresent in any of the previously suggested func-tions.
The large difference between the above op-timal weighting function and the functions usedin previous studies would indicate that the latterare suboptimal.
Also, as we can see, the rela-tion between context words and the target wordis mostly gone after a few words.
This wouldmotivate the commonly used very small windowswhen using a uniform weights, since using a big-ger window would further widen the gap betweenthe used weight and the optimal ones.Now for the system settings, the context wordswere processed the same way as the external cor-pus.
The target word was used without stemmingbut had the case stripped.
The NB systems usedthe concatenation of the AP collection and theSemeval data for the collection language model.This is motivated by the fact that the Semeval datais not balanced: it contains only a small number ofpassages containing the target words.
This makeswords related to them unusually frequent.
Theclass priors used an absolute discounting of 0.5 onclass counts.
Uniform NB uses a window of size 4,a Laplace smoothing of 0.65 on PTar and an au-tomatic Dirichlet with ?Con = 0.7 on PCon.
Lin-ear NB has ?
= 0.135, uses a Laplace smoothingof 0.85 on PTar and an automatic Dirichlet with?Con = 0.985 on PCon.
Exp NB has ?
= 0.27,uses a Laplace smoothing of 2.8 on PTar and anautomatic Dirichlet with ?Con = 1.01 on PCon.The SVM system uses a window of size 3.
Oursystem, Learned NB uses a Laplace smoothing of1.075 on PTar , and an automatic Dirichlet with?Con = 1.025 on PCon.
The results on WSD arelisted in Table 2.
WSD accuracy is measured bythe proportion of correctly disambiguated wordsamong all the word samples.
The cross-validationis performed on the training data with leave-one-out and is shown as a hint of the capacity of themodels.
A randomization test comparing Expo-nential NB and Learned NB gives a p-value of0.0508, which is quite good considering the exten-sive trials used to select the exponential parameterin comparison to a single curve computed from adifferent corpus.
This performance is comparableto the current state of the art.
It outperforms mostof the systems participating in the task (Pradhan etal., 2007).
Out of 14 systems, the best results hadaccuracies of 89.1*, 89.1*, 88.7, 86.9 and 86.4 (*indicates post-competition submissions).
Noticethat most previous systems used SVM with ad-ditional features such as local collocations, posi-tional word features and POS tags.
Our approachonly uses bag-of-words in a Na?ve Bayes classi-fier.
Therefore, the performance of our method issub-optimal.
With additional features and betterclassification methods, we can expect that betterperformance can be obtained.
In future work, wewill investigate the applications of SVM with ournew term weighting scheme, together with addi-tional types of features.6 WSD experiments on Semeval-2010Japanese Lexical SampleThe Semeval-2010 Japanese WSD task (Okumuraet al, 2010) consists of 50 polysemous wordsfor which examples were taken from the BCCWJcorpus (Maekawa, 2008).
It was manually seg-mented, POS-tagged, and annotated with sensestaken from the Iwanami Kokugo dictionary.
Theselected words have 50 samples for both the train-ing and test set.
The task is identical to the ELSof the previous experiment.Since the data was again insufficient to com-pute the optimal weighting curve, we used theMainichi-2005 corpus of NTCIR-8.
We tried toreproduce the same kind of segmentation as thetraining data by using the Chasen parser with Uni-Dic, which nevertheless results in different wordsegments as the training data.
For the corpus andSemeval data, conjugations (setsuzoku-to, jod?-shi, etc.
), particles (all jo-shi), symbols (blanks,kig?, etc.
), and numbers were stripped.
When a112Figure 2: Weight curve for Mainichi 2005base-form reading was present (for verbs and ad-jectives), the token was replaced by the Kanjis(Chinese characters) in the word writing concate-nated with the base-form reading.
This treatmentis somewhat equivalent to the stemming+stop listof the ELS tasks.
The resulting curve can be seenin Figure 2.As we can see, the general form of the curveis similar to that of the English collection, butis steeper.
This suggests that the meaning ofJapanese words can be determined using onlythe closest context words.
Words further than afew positions away have very small impact onthe target word.
This can be explained by thegrammatical structure of the Japanese language.While English can be considered a Subject-Verb-Complement language, Japanese is consideredSubject-Complement-Verb.
Verbs, mostly foundat the end of a sentence, can be far apart from theirsubject, and vice versa.
The window distance istherefore less useful to capture the relatedness inJapanese than in English since Japanese has morenon-local dependencies.The Semeval Japanese test data being part of abalanced corpus, untagged occurrences of the tar-get words are plenty, so we can benefit from usingthe collection-level counts for smoothing.
Uni-form NB uses a window of size 1, manual Dirich-let smoothing of 4 for PTar and 90 for the PCon.Linear NB has ?
= 0.955, uses a manual Dirichletsmoothing of 6.25 on PTar and manual DirichletSystem Cross-Val (%) Test set (%)MFS 75.23 68.96SVM 82.55 74.92Uniform NB 82.47 76.16Linear NB 82.63 76.48Exp.
NB 82.68 76.44Learned NB 82.67 76.52Table 3: WSD accuracy on Semeval-2010 JWSDsmoothing with ?Con = 65 on PCon.
Exp NBhas ?
= 2.675, uses a manual Dirichlet smooth-ing of 6.5 on PTar and a manual Dirichlet of 70on PCon.
The SVM system uses a window size of1 and Boolean features.
Learned NB used a man-ual Dirichlet smoothing of 4 for PTar and auto-matic Dirichlet smoothing with ?Con = 0.6 forPCon.
We believe this smoothing is beneficialonly on this system because it uses more words(the long tail), that makes the estimation of thepseudo-count more accurate.
Results on WSD arelisted in Table 3.
As we can see, the difference be-tween the NB models is less substantial than forEnglish.
This may be due to differences in thesegmentation parameters of our external corpus:we used the human-checked segmentation foundin the Semeval data for classification, but used aparser to segment our external corpus for weightlearning.
We are positive that the Chasen parserwith the UniDic dictionary was used to create theinitial segmentation in the Semeval data, but theremay be differences in versions and the initial seg-mentation results were further modified manually.Another reason for the results could be that thesystems use almost the same weights: UniformNB and SVM both used windows of size 1, andthe Japanese curve is steeper than the English one,making the context model account to almost onlyimmediately adjacent words.
So, even if our con-text model contains more context words at largerdistances, their weights are very low.
This makesall context model quite similar.
Nevertheless, westill observe some gain in WSD accuracy.
Theseresults show that the curves work as expected evenin different languages.
However, the weightingcurve is strongly language-dependent.
It couldalso be collection-dependent ?
we will investigate113this aspect in the future, using different collec-tions.7 ConclusionsThe definition of context vector and context modelis critical in WSD.
In previous studies in IR, de-caying weight along with distance within a textwindow have been proposed.
However, the de-caying functions are defined manually.
Althoughsome of the functions produced better results thanthe uniform weighting, there is no evidence show-ing that these functions best capture the impactof the context words on the meaning of the tar-get word.
This paper proposed an unsupervisedmethod for finding optimal weights for contextwords according to their distance to the targetword.
The general idea was to find the weightsthat best fit the data, in such a way that the contextmodels for the same target word generated fromtwo random windows samples become similar.
Itis the first time that this general principle is usedfor this purpose.
Our experiments on WSD in En-glish and Japanese suggest the validity of the prin-ciple.In this paper, we limited context models to bag-of-words features, excluding additional featuressuch as POS-tags.
Despite this simple type of fea-ture and the use of a simple Na?ve Bayes classifier,the WSD accuracy we obtained can rival the otherstate-of-the-art systems with more sophisticatedfeatures and classification algorithms.
This resultindicates that a crucial aspect in WSD is the def-inition of an appropriate context model, and ourweighting method can generate more reasonableweights of context words than using a predefineddecaying function.Our experiments also showed that the optimalweighting function is language-dependent.
Weobtained two different functions for English andJapanese, although their general shapes are simi-lar.
In fact, the optimal weighting function reflectsthe linguistic properties: as dependent words inJapanese can be further away from the target worddue to its linguistic structure, the optimal weight-ing quickly decays, meaning that we can rely lesson distant context words.
This also shows a lim-itation of this study: distance is not the sole cri-terion to determine the impact of a context word.Other factors, such as POS-tag and syntactic de-pendency, can play an important role in the con-text model.
These additional factors are comple-mentary to the distance criterion and our approachcan be extended to include such additional fea-tures.
This extension is part of our future work.Another limitation of straight window distanceis that all words introduce the same distance, re-gardless of their nature.
In our experiments, tomake the distance a more sensible metric, wemerged consecutive stop words in one placeholdertoken.
The idea behind this it that some words,such as stop words, should introduce less distancethan others.
On the opposite, we can easily un-derstand that tokens such as commas, full stops,parentheses and paragraph should introduce a big-ger distance than regular words.
We could there-fore use a congruence score for a word, an indi-cator showing on average how much what comesbefore is similar to what comes after the word.Also, we have combined our weighting schemawith NB classifier.
Other classifiers such as SVMcould lead to better results.
The utilization of ournew weighting schema with SVM is another fu-ture work.Finally, the weights computed with our methodhas been used in WSD tasks.
The weights couldbe seen as the expected strength of relation be-tween two words in a document according to theirdistance.
The consideration of word relationshipsin documents and queries is one of the endeav-ors in current research in IR.
The new weightingschema could be easily integrated with a depen-dency model in IR.
We plan to perform such inte-gration in the future.AcknowledgmentsThe authors would like to thank Florian Boudinand Satoko Fujisawa for helpful comments onthis work.
This work is partially supportedby Japanese MEXT Grant-in-Aid for ScientificResearch on Info-plosion (#21013046) and theJapanese MEXT Research Student Scholarshipprogram.114ReferencesBai, Jing, Dawei Song, Peter Bruza, Jian-Yun Nie, andGuihong Cao.
2005.
Query expansion using termrelationships in language models for information re-trieval.
In CIKM ?05 Proceedings, pages 688?695,New York, NY, USA.
ACM.Cai, Jun Fu, Wee Sun Lee, and Yee Whye Teh.
2007.Nus-ml: improving word sense disambiguation us-ing topic features.
In SemEval ?07 Proceedings,pages 249?252, Morristown, NJ, USA.
Associationfor Computational Linguistics.Cheung, Percy and Pascale Fung.
2004.
Translationdisambiguation in mixed language queries.
Ma-chine Translation, 18(4):251?273.Gao, Jianfeng, Ming Zhou, Jian-Yun Nie, HongzhaoHe, and Weijun Chen.
2002.
Resolving query trans-lation ambiguity using a decaying co-occurrencemodel and syntactic dependence relations.
In SI-GIR ?02 Proceedings, pages 183?190, New York,NY, USA.
ACM.Ide, Nancy and Jean V?ronis.
1998.
Introduction tothe special issue on word sense disambiguation: thestate of the art.
Comput.
Linguist., 24(1):2?40.Lv, Yuanhua and ChengXiang Zhai.
2009.
Positionallanguage models for information retrieval.
In SIGIR?09 Proceedings, pages 299?306, New York, NY,USA.
ACM.Maekawa, Kikuo.
2008.
Compilation of the bal-anced corpus of contemporary written japanese inthe kotonoha initiative (invited paper).
In ISUC?08 Proceedings, pages 169?172, Washington, DC,USA.
IEEE Computer Society.Navigli, Roberto.
2009.
Word sense disambiguation:A survey.
ACM Comput.
Surv., 41(2):1?69.Okumura, Manabu, Kiyoaki Shirai, Kanako Komiya,and Hikaru Yokono.
2010.
Semeval-2010 task:Japanese wsd.
In SemEval ?10 Proceedings.
Asso-ciation for Computational Linguistics.Pradhan, Sameer S., Edward Loper, Dmitriy Dligach,and Martha Palmer.
2007.
Semeval-2007 task 17:English lexical sample, srl and all words.
In Se-mEval ?07 Proceedings, pages 87?92, Morristown,NJ, USA.
Association for Computational Linguis-tics.Song, D. and P. D. Bruza.
2003.
Towards context sen-sitive information inference.
Journal of the Amer-ican Society for Information Science and Technol-ogy, 54(4):321?334.Xu, Jinxi and W. Bruce Croft.
1998.
Corpus-based stemming using cooccurrence of word vari-ants.
ACM Trans.
Inf.
Syst., 16(1):61?81.Zhai, ChengXiang and John Lafferty.
2002.
Two-stage language models for information retrieval.
InSIGIR ?02 Proceedings, pages 49?56, New York,NY, USA.
ACM.115
