Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1058?1066,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsEfficient Inference Through Cascades of Weighted Tree TransducersJonathan May and Kevin KnightInformation Sciences InstituteUniversity of Southern CaliforniaMarina del Rey, CA 90292{jonmay,knight}@isi.eduHeiko VoglerTechnische Universita?t DresdenInstitut fu?r Theoretische Informatik01062 Dresden, Germanyheiko.vogler@tu-dresden.deAbstractWeighted tree transducers have been pro-posed as useful formal models for rep-resenting syntactic natural language pro-cessing applications, but there has beenlittle description of inference algorithmsfor these automata beyond formal founda-tions.
We give a detailed description ofalgorithms for application of cascades ofweighted tree transducers to weighted treeacceptors, connecting formal theory withactual practice.
Additionally, we presentnovel on-the-fly variants of these algo-rithms, and compare their performanceon a syntax machine translation cascadebased on (Yamada and Knight, 2001).1 MotivationWeighted finite-state transducers have found re-cent favor as models of natural language (Mohri,1997).
In order to make actual use of systems builtwith these formalisms we must first calculate theset of possible weighted outputs allowed by thetransducer given some input, which we call for-ward application, or the set of possible weightedinputs given some output, which we call backwardapplication.
After application we can do some in-ference on this result, such as determining its khighest weighted elements.We may also want to divide up our problemsinto manageable chunks, each represented by atransducer.
As noted by Woods (1980), it is eas-ier for designers to write several small transduc-ers where each performs a simple transformation,rather than painstakingly construct a single com-plicated device.
We would like to know, then,the result of transformation of input or output bya cascade of transducers, one operating after theother.
As we will see, there are various strate-gies for approaching this problem.
We will con-sider offline composition, bucket brigade applica-tion, and on-the-fly application.Application of cascades of weighted stringtransducers (WSTs) has been well-studied (Mohri,1997).
Less well-studied but of more recent in-terest is application of cascades of weighted treetransducers (WTTs).
We tackle application of WTTcascades in this work, presenting:?
explicit algorithms for application of WTT cas-cades?
novel algorithms for on-the-fly application ofWTT cascades, and?
experiments comparing the performance ofthese algorithms.2 Strategies for the string caseBefore we discuss application of WTTs, it is help-ful to recall the solution to this problem in the WSTdomain.
We recall previous formal presentationsof WSTs (Mohri, 1997) and note informally thatthey may be represented as directed graphs withdesignated start and end states and edges labeledwith input symbols, output symbols, and weights.1Fortunately, the solution for WSTs is practicallytrivial?we achieve application through a seriesof embedding, composition, and projection oper-ations.
Embedding is simply the act of represent-ing a string or regular string language as an iden-tity WST.
Composition of WSTs, that is, generat-ing a single WST that captures the transformationsof two input WSTs used in sequence, is not at alltrivial, but has been well covered in, e.g., (Mohri,2009), where directly implementable algorithmscan be found.
Finally, projection is another triv-ial operation?the domain or range language canbe obtained from a WST by ignoring the output orinput symbols, respectively, on its arcs, and sum-ming weights on otherwise identical arcs.
By em-bedding an input, composing the result with thegiven WST, and projecting the result, forward ap-plication is accomplished.2 We are then left witha weighted string acceptor (WSA), essentially aweighted, labeled graph, which can be traversed1We assume throughout this paper that weights are inR+ ?
{+?
}, that the weight of a path is calculated as theproduct of the weights of its edges, and that the weight of a(not necessarily finite) set T of paths is calculated as the sumof the weights of the paths of T .2For backward applications, the roles of input and outputare simply exchanged.1058A Ba : a / 1 a : a / 1C(a) Input string ?a a?
embedded in anidentity WSTEa : b / .
1 a : a / .
9b : a / .
5 Da : b / .
4a : a / .
6b : a / .
5b : b / .
5b : b / .
5(b) first WST in cascadea : c / .
6b: c / .
7Fa : d / .
4b: d / .
3(c) second WST in cascadeE Fa : c / .
0 7a : c / .
5 4b: c / .
6 5b: d / .
3 5D Fa : c / .
2 8a : c / .
3 6b: c / .
6 5b: d / .
3 5a : d / .
3 6a : d / .
0 3 a : d / .
2 4a : d / .
1 2(d) Offline composition approach:Compose the transducersA DB D CDa : b / .
1B Ea : a / .
9CE(e) Bucket brigade approach:Apply WST (b) to WST (a)A D FB D F CD Fd / .
0 3c/ .
0 7B E Fc / .
5 4CE Fc/ .
5 4c / .
3 6c/ .
2 8c/ .
0 7d / .
3 6d / .
0 3d / .
3 6d / .
1 2d / .
2 4(f) Result of offline or bucket applicationafter projectionA D FB D F CD Fd / .
0 3B E Fc/ .
5 4CE Fc / .
3 6c/ .
2 8c/ .
0 7d / .
3 6d / .
1 2d / .
2 4(g) Initial on-the-flystand-in for (f)A D FB D F CD Fd / .
0 3B E Fc / .
5 4C E Fc / .
3 6c/ .
2 8c/ .
0 7d / .
3 6d / .
1 2d / .
2 4(h) On-the-fly stand-in after exploringoutgoing edges of state ADFA D FB D F CD Fd / .
0 3B E Fc / .
5 4CE Fc / .
3 6c/ .
2 8c/ .
0 7d / .
3 6d / .
1 2d / .
2 4(i) On-the-fly stand-in after best path has been foundFigure 1: Three different approaches to application through cascades of WSTs.by well-known algorithms to efficiently find the k-best paths.Because WSTs can be freely composed, extend-ing application to operate on a cascade of WSTsis fairly trivial.
The only question is one of com-position order: whether to initially compose thecascade into a single transducer (an approach wecall offline composition) or to compose the initialembedding with the first transducer, trim uselessstates, compose the result with the second, and soon (an approach we call bucket brigade).
The ap-propriate strategy generally depends on the struc-ture of the individual transducers.A third approach builds the result incrementally,as dictated by some algorithm that requests in-formation about it.
Such an approach, which wecall on-the-fly, was described in (Pereira and Ri-ley, 1997; Mohri, 2009; Mohri et al, 2000).
Ifwe can efficiently calculate the outgoing edges ofa state of the result WSA on demand, without cal-culating all edges in the entire machine, we canmaintain a stand-in for the result structure, a ma-chine consisting at first of only the start state ofthe true result.
As a calling algorithm (e.g., an im-plementation of Dijkstra?s algorithm) requests in-formation about the result graph, such as the set ofoutgoing edges from a state, we replace the currentstand-in with a richer version by adding the resultof the request.
The on-the-fly approach has a dis-tinct advantage over the other two methods in thatthe entire result graph need not be built.
A graphi-cal representation of all three methods is presentedin Figure 1.3 Application of tree transducersNow let us revisit these strategies in the settingof trees and tree transducers.
Imagine we have atree or set of trees as input that can be representedas a weighted regular tree grammar3 (WRTG) anda WTT that can transform that input with someweight.
We would like to know the k-best trees theWTT can produce as output for that input, alongwith their weights.
We already know of severalmethods for acquiring k-best trees from a WRTG(Huang and Chiang, 2005; Pauls and Klein, 2009),so we then must ask if, analogously to the stringcase, WTTs preserve recognizability4 and we canform an application WRTG.
Before we begin, how-ever, we must define WTTs and WRTGs.3.1 Preliminaries5A ranked alphabet is a finite set ?
such that ev-ery member ?
?
?
has a rank rk(?)
?
N. Wecall ?
(k) ?
?, k ?
N the set of those ?
?
?such that rk(?)
= k. The set of variables is de-notedX = {x1, x2, .
.
.}
and is assumed to be dis-joint from any ranked alphabet used in this paper.We use ?
to denote a symbol of rank 0 that is notin any ranked alphabet used in this paper.
A treet ?
T?
is denoted ?
(t1, .
.
.
, tk) where k ?
0,?
?
?
(k), and t1, .
.
.
, tk ?
T?.
For ?
?
?
(0) we3This generates the same class of weighted tree languagesas weighted tree automata, the direct analogue of WSAs, andis more useful for our purposes.4A weighted tree language is recognizable iff it can berepresented by a wrtg.5The following formal definitions and notations areneeded for understanding and reimplementation of the pre-sented algorithms, but can be safely skipped on first readingand consulted when encountering an unfamiliar term.1059write ?
?
T?
as shorthand for ?().
For every setS disjoint from ?, let T?
(S) = T?
?S , where, forall s ?
S, rk(s) = 0.We define the positions of a treet = ?
(t1, .
.
.
, tk), for k ?
0, ?
?
?
(k),t1, .
.
.
, tk ?
T?, as a set pos(t) ?
N?
such thatpos(t) = {?}
?
{iv | 1 ?
i ?
k, v ?
pos(ti)}.The set of leaf positions lv(t) ?
pos(t) are thosepositions v ?
pos(t) such that for no i ?
N,vi ?
pos(t).
We presume standard lexicographicorderings < and ?
on pos.Let t, s ?
T?
and v ?
pos(t).
The label of tat position v, denoted by t(v), the subtree of t atv, denoted by t|v, and the replacement at v by s,denoted by t[s]v, are defined as follows:1.
For every ?
?
?
(0), ?(?)
= ?, ?|?
= ?, and?[s]?
= s.2.
For every t = ?
(t1, .
.
.
, tk) such thatk = rk(?)
and k ?
1, t(?)
= ?, t|?
= t,and t[s]?
= s. For every 1 ?
i ?
k andv ?
pos(ti), t(iv) = ti(v), t|iv = ti|v, andt[s]iv = ?
(t1, .
.
.
, ti?1, ti[s]v, ti+1, .
.
.
, tk).The size of a tree t, size (t) is |pos(t)|, the car-dinality of its position set.
The yield set of a treeis the set of labels of its leaves: for a tree t, yd (t)= {t(v) | v ?
lv(t)}.Let A and B be sets.
Let ?
: A ?
T?
(B)be a mapping.
We extend ?
to the mapping ?
:T?(A)?
T?
(B) such that for a ?A, ?
(a) = ?
(a)and for k ?
0, ?
?
?
(k), and t1, .
.
.
, tk ?
T?(A),?(?
(t1, .
.
.
, tk)) = ?(?
(t1), .
.
.
, ?(tk)).
We indi-cate such extensions by describing ?
as a substi-tution mapping and then using ?
without furthercomment.We use R+ to denote the set {w ?
R | w ?
0}and R?+ to denote R+ ?
{+?
}.Definition 3.1 (cf.
(Alexandrakis and Bozapa-lidis, 1987)) A weighted regular tree grammar(WRTG) is a 4-tuple G = (N,?, P, n0) where:1.
N is a finite set of nonterminals, with n0 ?
Nthe start nonterminal.2.
?
is a ranked alphabet of input symbols, where?
?N = ?.3.
P is a tuple (P ?, pi), where P ?
is a finite setof productions, each production p of the formn ??
u, n ?
N , u ?
T?
(N), and pi : P ?
?
R+is a weight function of the productions.
We willrefer to P as a finite set of weighted produc-tions, each production p of the form npi(p)???
u.A production p is a chain production if it isof the form niw??
nj , where ni, nj ?
N .66In (Alexandrakis and Bozapalidis, 1987), chain produc-tions are forbidden in order to avoid infinite summations.
Weexplicitly allow such summations.A WRTG G is in normal form if each produc-tion is either a chain production or is of theform nw??
?
(n1, .
.
.
, nk) where ?
?
?
(k) andn1, .
.
.
, nk ?
N .For WRTG G = (N,?, P, n0), s, t, u ?
T?
(N),n ?
N , and p ?
P of the form nw??
u, weobtain a derivation step from s to t by replacingsome leaf nonterminal in s labeled n with u. For-mally, s ?pG t if there exists some v ?
lv(s)such that s(v) = n and s[u]v = t. We say thisderivation step is leftmost if, for all v?
?
lv(s)where v?
< v, s(v?)
?
?.
We henceforth as-sume all derivation steps are leftmost.
If, forsome m ?
N, pi ?
P , and ti ?
T?
(N) for all1 ?
i ?
m, n0 ?p1 t1 .
.
.
?pm tm, we saythe sequence d = (p1, .
.
.
, pm) is a derivationof tm in G and that n0 ??
tm; the weight of dis wt(d) = pi(p1) ?
.
.
.
?
pi(pm).
The weightedtree language recognized by G is the mappingLG : T?
?
R?+ such that for every t ?
T?, LG(t)is the sum of the weights of all (possibly infinitelymany) derivations of t in G. A weighted tree lan-guage f : T?
?
R?+ is recognizable if there is aWRTG G such that f = LG.We define a partial ordering  on WRTGssuch that for WRTGs G1 = (N1,?, P1, n0) andG2 = (N2,?, P2, n0), we say G1  G2 iffN1 ?
N2 and P1 ?
P2, where the weights arepreserved.Definition 3.2 (cf.
Def.
1 of (Maletti, 2008))A weighted extended top-down tree transducer(WXTT) is a 5-tupleM = (Q,?,?, R, q0) where:1.
Q is a finite set of states.2.
?
and ?
are the ranked alphabets of in-put and output symbols, respectively, where(?
??)
?Q = ?.3.
R is a tuple (R?, pi), where R?
is a finite setof rules, each rule r of the form q.y ??
u forq ?
Q, y ?
T?
(X), and u ?
T?
(Q ?
X).We further require that no variable x ?
X ap-pears more than once in y, and that each vari-able appearing in u is also in y. Moreover,pi : R?
?
R?+ is a weight function of therules.
As for WRTGs, we refer to R as a finiteset of weighted rules, each rule r of the formq.ypi(r)???
u.A WXTT is linear (respectively, nondeleting)if, for each rule r of the form q.yw??
u, eachx ?
yd (y) ?
X appears at most once (respec-tively, at least once) in u.
We denote the classof all WXTTs as wxT and add the letters L and Nto signify the subclasses of linear and nondeletingWTT, respectively.
Additionally, if y is of the form?
(x1, .
.
.
, xk), we remove the letter ?x?
to signify1060the transducer is not extended (i.e., it is a ?tradi-tional?
WTT (Fu?lo?p and Vogler, 2009)).For WXTT M = (Q,?,?, R, q0), s, t ?
T?(Q?
T?
), and r ?
R of the form q.yw??
u, we obtaina derivation step from s to t by replacing someleaf of s labeled with q and a tree matching y by atransformation of u, where each instance of a vari-able has been replaced by a corresponding subtreeof the y-matching tree.
Formally, s?rM t if thereis a position v ?
pos(s), a substitution mapping?
: X ?
T?, and a rule q.yw??
u ?
R such thats(v) = (q, ?
(y)) and t = s[??
(u)]v, where ??
isa substitution mapping Q ?
X ?
T?
(Q ?
T?
)defined such that ??
(q?, x) = (q?, ?
(x)) for allq?
?
Q and x ?
X .
We say this derivation stepis leftmost if, for all v?
?
lv(s) where v?
< v,s(v?)
?
?.
We henceforth assume all derivationsteps are leftmost.
If, for some s ?
T?, m ?
N,ri ?
R, and ti ?
T?
(Q ?
T?)
for all 1 ?
i ?
m,(q0, s) ?r1 t1 .
.
.
?rm tm, we say the sequenced = (r1, .
.
.
, rm) is a derivation of (s, tm) in M ;the weight of d is wt(d) = pi(r1) ?
.
.
.
?
pi(rm).The weighted tree transformation recognized byM is the mapping ?M : T?
?
T?
?
R?+ , suchthat for every s ?
T?
and t ?
T?, ?M (s, t) is thesum of the weights of all (possibly infinitely many)derivations of (s, t) inM .
The composition of twoweighted tree transformations ?
: T??T?
?
R?+and ?
: T??T?
?
R?+ is the weighted tree trans-formation (?
;?)
: T?
?
T?
?R?+ where for everys ?
T?
and u ?
T?, (?
;?
)(s, u) =?t?T??
(s, t)?
?
(t, u).3.2 Applicable classesWe now consider transducer classes where recog-nizability is preserved under application.
Table 1presents known results for the top-down tree trans-ducer classes described in Section 3.1.
Unlikethe string case, preservation of recognizability isnot universal or symmetric.
This is important forus, because we can only construct an applicationWRTG, i.e., a WRTG representing the result of ap-plication, if we can ensure that the language gen-erated by application is in fact recognizable.
Ofthe types under consideration, only wxLNT andwLNT preserve forward recognizability.
The twoclasses marked as open questions and the otherclasses, which are superclasses of wNT, do not orare presumed not to.
All subclasses of wxLT pre-serve backward recognizability.7 We do not con-sider cases where recognizability is not preservedin the remainder of this paper.
If a transducer Mof a class that preserves forward recognizability isapplied to a WRTG G, we can call the forward ap-7Note that the introduction of weights limits recognizabil-ity preservation considerably.
For example, (unweighted) xTpreserves backward recognizability.plication WRTG M(G).
and ifM preserves back-ward recognizability, we can call the backward ap-plication WRTG M(G)/.Now that we have explained the applicationproblem in the context of weighted tree transduc-ers and determined the classes for which applica-tion is possible, let us consider how to build for-ward and backward application WRTGs.
Our ba-sic approach mimics that taken for WSTs by us-ing an embed-compose-project strategy.
As instring world, if we can embed the input in a trans-ducer, compose with the given transducer, andproject the result, we can obtain the applicationWRTG.
Embedding a WRTG in a wLNT is a triv-ial operation?if the WRTG is in normal form andchain production-free,8 for every production of theform nw??
?
(n1, .
.
.
, nk), create a rule of the formn.?
(x1, .
.
.
, xk)w??
?
(n1.x1, .
.
.
, nk.xk).
Rangeprojection of a wxLNT is also trivial?for everyq ?
Q and u ?
T?
(Q ?
X) create a productionof the form qw??
u?
where u?
is formed from uby replacing all leaves of the form q.x with theleaf q, i.e., removing references to variables, andw is the sum of the weights of all rules of the formq.y ??
u in R.9 Domain projection for wxLT isbest explained by way of example.
The left side ofa rule is preserved, with variables leaves replacedby their associated states from the right side.
So,the rule q1.?(?
(x1), x2)w??
?
(q2.x2, ?
(?, q3.x1))would yield the production q1w??
?(?
(q3), q2) inthe domain projection.
However, a deleting rulesuch as q1.?
(x1, x2)w??
?
(q2.x2) necessitates theintroduction of a new nonterminal ?
that can gen-erate all of T?
with weight 1.The only missing piece in our embed-compose-project strategy is composition.
Algorithm 1,which is based on the declarative construction ofMaletti (2006), generates the syntactic composi-tion of a wxLT and a wLNT, a generalizationof the basic composition construction of Baker(1979).
It calls Algorithm 2, which determinesthe sequences of rules in the second transducerthat match the right side of a single rule in thefirst transducer.
Since the embedded WRTG is oftype wLNT, it may be either the first or secondargument provided to Algorithm 1, depending onwhether the application is forward or backward.We can thus use the embed-compose-project strat-egy for forward application of wLNT and back-ward application of wxLT and wxLNT.
Note thatwe cannot use this strategy for forward applica-8Without loss of generality we assume this is so, sincestandard algorithms exist to remove chain productions(Kuich, 1998; E?sik and Kuich, 2003; Mohri, 2009) and con-vert into normal form (Alexandrakis and Bozapalidis, 1987).9Finitely many such productions may be formed.1061tion of wxLNT, even though that class preservesrecognizability.Algorithm 1 COMPOSE1: inputs2: wxLTM1 = (Q1,?,?, R1, q10)3: wLNTM2 = (Q2,?,?, R2, q20)4: outputs5: wxLTM3 = ((Q1?Q2),?,?, R3, (q10 , q20)) suchthatM3 = (?M1 ; ?M2).6: complexity7: O(|R1|max(|R2|size(u?
), |Q2|)), where u?
is thelargest right side tree in any rule in R18: Let R3 be of the form (R?3, pi)9: R3 ?
(?, ?
)10: ??
{(q10 , q20)} {seen states}11: ??
{(q10 , q20)} {pending states}12: while ?
6= ?
do13: (q1, q2)?any element of ?14: ??
?
\ {(q1, q2)}15: for all (q1.yw1???
u) ?
R1 do16: for all (z, w2) ?
COVER(u,M2, q2) do17: for all (q, x) ?
yd (z)?
((Q1?Q2)?X) do18: if q 6?
?
then19: ??
?
?
{q}20: ??
?
?
{q}21: r ?
((q1, q2).y ??
z)22: R?3 ?
R?3 ?
{r}23: pi(r)?
pi(r) + (w1 ?
w2)24: return M34 Application of tree transducer cascadesWhat about the case of an input WRTG and a cas-cade of tree transducers?
We will revisit the threestrategies for accomplishing application discussedabove for the string case.In order for offline composition to be a viablestrategy, the transducers in the cascade must beclosed under composition.
Unfortunately, of theclasses that preserve recognizability, only wLNTis closed under composition (Ge?cseg and Steinby,1984; Baker, 1979; Maletti et al, 2009; Fu?lo?p andVogler, 2009).However, the general lack of composability oftree transducers does not preclude us from con-ducting forward application of a cascade.
We re-visit the bucket brigade approach, which in Sec-tion 2 appeared to be little more than a choice ofcomposition order.
As discussed previously, ap-plication of a single transducer involves an embed-ding, a composition, and a projection.
The embed-ded WRTG is in the class wLNT, and the projectionforms another WRTG.
As long as every transducerin the cascade can be composed with a wLNTto its left or right, depending on the applicationtype, application of a cascade is possible.
Notethat this embed-compose-project process is some-what more burdensome than in the string case.
Forstrings, application is obtained by a single embed-ding, a series of compositions, and a single projec-Algorithm 2 COVER1: inputs2: u ?
T?
(Q1 ?X)3: wTM2 = (Q2,?,?, R2, q20)4: state q2 ?
Q25: outputs6: set of pairs (z, w) with z ?
T?
((Q1 ?
Q2) ?
X)formed by one or more successful runs on u by rulesin R2, starting from q2, and w ?
R?+ the sum of theweights of all such runs.7: complexity8: O(|R2|size(u))9: if u(?)
is of the form (q1, x) ?
Q1 ?X then10: zinit ?
((q1, q2), x)11: else12: zinit ?
?13: ?last ?
{(zinit, {((?, ?
), q2)}, 1)}14: for all v ?
pos(u) such that u(v) ?
?
(k) for somek ?
0 in prefix order do15: ?v ?
?16: for all (z, ?, w) ?
?last do17: for all v?
?
lv(z) such that z(v?)
= ?
do18: for all (?
(v, v?
).u(v)(x1, .
.
.
, xk)w??
?h)?R2do19: ??
?
?20: Form substitution mapping ?
: (Q2 ?
X)?
T?
((Q1 ?
Q2 ?X) ?
{?
}).21: for i = 1 to k do22: for all v??
?
pos(h) such thath(v??)
= (q?2, xi) for some q?2 ?
Q2 do23: ??
(vi, v?v??)?
q?224: if u(vi) is of the form(q1, x) ?
Q1 ?X then25: ?
(q?2, xi)?
((q1, q?2), x)26: else27: ?
(q?2, xi)?
?28: ?v ?
?v ?
{(z[?(h)]v?
, ?
?, w ?
w?
)}29: ?last ?
?v30: Z ?
{z | (z, ?, w) ?
?last}31: return {(z,X(z,?,w)?
?lastw) | z ?
Z}tion, whereas application for trees is obtained by aseries of (embed, compose, project) operations.4.1 On-the-fly algorithmsWe next consider on-the-fly algorithms for ap-plication.
Similar to the string case, an on-the-fly approach is driven by a calling algorithm thatperiodically needs to know the productions in aWRTG with a common left side nonterminal.
Theembed-compose-project approach produces an en-tire application WRTG before any inference al-gorithm is run.
In order to admit an on-the-flyapproach we describe algorithms that only gen-erate those productions in a WRTG that have agiven left nonterminal.
In this section we ex-tend Definition 3.1 as follows: a WRTG is a 6-tuple G = (N,?, P, n0,M,G) where N,?, P,and n0 are defined as in Definition 3.1, and eitherM = G = ?,10 orM is a wxLNT and G is a nor-mal form, chain production-free WRTG such that10In which case the definition is functionally unchangedfrom before.1062type preserved?
sourcew[x]T No See w[x]NTw[x]LT OQ (Maletti, 2009)w[x]NT No (Ge?cseg and Steinby, 1984)wxLNT Yes (Fu?lo?p et al, 2010)wLNT Yes (Kuich, 1999)(a) Preservation of forward recognizabilitytype preserved?
sourcew[x]T No See w[x]NTw[x]LT Yes (Fu?lo?p et al, 2010)w[x]NT No (Maletti, 2009)w[x]LNT Yes See w[x]LT(b) Preservation of backward recognizabilityTable 1: Preservation of forward and backward recognizability for various classes of top-down treetransducers.
Here and elsewhere, the following abbreviations apply: w = weighted, x = extended LHS, L= linear, N = nondeleting, OQ = open question.
Square brackets include a superposition of classes.
Forexample, w[x]T signifies both wxT and wT.Algorithm 3 PRODUCE1: inputs2: WRTG Gin = (Nin,?, Pin, n0,M,G) suchthat M = (Q,?,?, R, q0) is a wxLNT andG = (N,?, P, n?0,M?, G?)
is a WRTG in normalform with no chain productions3: nin ?
Nin4: outputs5: WRTG Gout = (Nout, ?, Pout, n0,M,G), such thatGin  Gout and(ninw??
u) ?
Pout?
(ninw??
u) ?M(G).6: complexity7: O(|R||P |size(y?
)), where y?
is the largest left side treein any rule in R8: if Pin contains productions of the form ninw??
u then9: return Gin10: Nout ?
Nin11: Pout ?
Pin12: Let nin be of the form (n, q), where n ?
N and q ?
Q.13: for all (q.yw1???
u) ?
R do14: for all (?, w2) ?
REPLACE(y,G, n) do15: Form substitution mapping ?
: Q ?
X ?T?
(N ?Q) such that, for all v ?
yd (y) and q?
?Q, if there exist n?
?N and x ?X such that ?
(v)= n?
and y(v) = x, then ?
(q?, x) = (n?, q?
).16: p?
?
((n, q)w1?w2?????
?
(u))17: for all p ?
NORM(p?, Nout) do18: Let p be of the form n0w??
?
(n1, .
.
.
, nk) for?
?
?
(k).19: Nout ?
Nout ?
{n0, .
.
.
, nk}20: Pout ?
Pout ?
{p}21: return CHAIN-REM(Gout)G M(G)..
In the latter case, G is a stand-in forM(G)., analogous to the stand-ins for WSAs andWSTs described in Section 2.Algorithm 3, PRODUCE, takes as input aWRTG Gin = (Nin,?, Pin, n0,M,G) and a de-sired nonterminal nin and returns another WRTG,Gout that is different from Gin in that it has moreproductions, specifically those beginning with ninthat are in M(G).. Algorithms using stand-insshould call PRODUCE to ensure the stand-in theyare using has the desired productions beginningwith the specific nonterminal.
Note, then, thatPRODUCE obtains the effect of forward applica-Algorithm 4 REPLACE1: inputs2: y ?
T?
(X)3: WRTG G = (N,?, P, n0,M,G) in normal form,with no chain productions4: n ?
N5: outputs6: set ?
of pairs (?, w) where ?
is a mappingpos(y) ?
N and w ?
R?+ , each pair indicatinga successful run on y by productions in G, startingfrom n, and w is the weight of the run.7: complexity8: O(|P |size(y))9: ?last ?
{({(?, n)}, 1)}10: for all v ?
pos(y) such that y(v) 6?
X in prefix orderdo11: ?v ?
?12: for all (?, w) ?
?last do13: ifM 6= ?
and G 6= ?
then14: G?
PRODUCE(G, ?
(v))15: for all (?
(v) w???
y(v)(n1, .
.
.
, nk)) ?
P do16: ?v ?
?v?{(??
{(vi, ni), 1 ?
i ?
k}, w?w?
)}17: ?last ?
?v18: return ?lastAlgorithm 5 MAKE-EXPLICIT1: inputs2: WRTG G = (N,?, P, n0,M,G) in normal form3: outputs4: WRTG G?
= (N ?,?, P ?, n0,M,G), in normal form,such that ifM 6= ?
andG 6= ?, LG?
= LM(G).
, andotherwise G?
= G.5: complexity6: O(|P ?|)7: G?
?
G8: ??
{n0} {seen nonterminals}9: ??
{n0} {pending nonterminals}10: while ?
6= ?
do11: n?any element of ?12: ??
?
\ {n}13: ifM 6= ?
and G 6= ?
then14: G?
?
PRODUCE(G?, n)15: for all (n w??
?
(n1, .
.
.
, nk)) ?
P ?
do16: for i = 1 to k do17: if ni 6?
?
then18: ??
?
?
{ni}19: ??
?
?
{ni}20: return G?1063g0g0w1???
?
(g0, g1)g0w2???
?
g1w3???
?
(a) Input WRTG Ga0a0.?
(x1, x2)w4???
?
(a0.x1, a1.x2)a0.?
(x1, x2)w5???
?
(a2.x1, a1.x2)a0.?w6???
?
a1.?w7???
?
a2.?w8???
?
(b) First transducerMA in the cascadeb0b0.?
(x1, x2)w9???
?
(b0.x1, b0.x2)b0.?w10???
?
(c) Second transducerMB in the cascadeg0a0w1?w4?????
?
(g0a0, g1a1)g0a0w1?w5?????
?
(g0a2, g1a1)g0a0w2?w6?????
?
g1a1w3?w7?????
?
(d) Productions ofMA(G).
built as a consequenceof building the completeMB(MA(G).).g0a0b0g0a0b0w1?w4?w9???????
?
(g0a0b0, g1a1b0)g0a0b0w2?w6?w10????????
?
g1a1b0w3?w7?w10????????
?
(e) CompleteMB(MA(G).
).Figure 2: Forward application through a cascadeof tree transducers using an on-the-fly method.tion in an on-the-fly manner.11 It makes calls toREPLACE, which is presented in Algorithm 4, aswell as to a NORM algorithm that ensures normalform by replacing a single production not in nor-mal form with several normal-form productionsthat can be combined together (Alexandrakis andBozapalidis, 1987) and a CHAIN-REM algorithmthat replaces a WRTG containing chain productionswith an equivalent WRTG that does not (Mohri,2009).As an example of stand-in construction, con-sider the invocation PRODUCE(G1, g0a0), whereG1 = ({g0a0}, {?, ?, ?, ?
}, ?, g0a0, MA, G), Gis in Figure 2a,12 and MA is in 2b.
The stand-inWRTG that is output contains the first three of thefour productions in Figure 2d.To demonstrate the use of on-the-fly applicationin a cascade, we next show the effect of PRO-DUCE when used with the cascadeG?MA ?MB ,where MB is in Figure 2c.
Our driving al-gorithm in this case is Algorithm 5, MAKE-11Note further that it allows forward application of classwxLNT, something the embed-compose-project approach didnot allow.12By convention the initial nonterminal and state are listedfirst in graphical depictions of WRTGs and WXTTs.rJJ.JJ(x1, x2, x3) ??
JJ(rDT.x1, rJJ.x2, rVB.x3)rVB.VB(x1, x2, x3) ??
VB(rNNPS.x1, rNN.x3, rVB.x2)t.?gentle?
??
?gentle?
(a) Rotation rulesiVB.NN(x1, x2) ??
NN(INS iNN.x1, iNN.x2)iVB.NN(x1, x2) ??
NN(iNN.x1, iNN.x2)iVB.NN(x1, x2) ??
NN(iNN.x1, iNN.x2, INS)(b) Insertion rulest.VB(x1, x2, x3) ??
X(t.x1, t.x2, t.x3)t.?gentleman?
??
j1t.?gentleman?
??
EPSt.INS ??
j1t.INS ??
j2(c) Translation rulesFigure 3: Example rules from transducers usedin decoding experiment.
j1 and j2 are Japanesewords.EXPLICIT, which simply generates the full ap-plication WRTG using calls to PRODUCE.
Theinput to MAKE-EXPLICIT is G2 = ({g0a0b0},{?, ?
}, ?, g0a0b0,MB ,G1).13 MAKE-EXPLICITcalls PRODUCE(G2, g0a0b0).
PRODUCE thenseeks to cover b0.?
(x1, x2)w9??
?
(b0.x1, b0.x2)with productions from G1, which is a stand-in forMA(G).. At line 14 of REPLACE, G1 is im-proved so that it has the appropriate productions.The productions of MA(G).
that must be builtto form the complete MB(MA(G).).
are shownin Figure 2d.
The complete MB(MA(G).).
isshown in Figure 2e.
Note that because we usedthis on-the-fly approach, we were able to avoidbuilding all the productions in MA(G).
; in par-ticular we did not build g0a2w2?w8?????
?, while abucket brigade approach would have built this pro-duction.
We have also designed an analogous on-the-fly PRODUCE algorithm for backward appli-cation on linear WTT.We have now defined several on-the-fly andbucket brigade algorithms, and also discussed thepossibility of embed-compose-project and offlinecomposition strategies to application of cascadesof tree transducers.
Tables 2a and 2b summa-rize the available methods of forward and back-ward application of cascades for recognizability-preserving tree transducer classes.5 Decoding ExperimentsThe main purpose of this paper has been topresent novel algorithms for performing applica-tion.
However, it is important to demonstrate thesealgorithms on real data.
We thus demonstratebucket-brigade and on-the-fly backward applica-tion on a typical NLP task cast as a cascade ofwLNT.
We adapt the Japanese-to-English transla-13Note that G2 is the initial stand-in for MB(MA(G).
).,since G1 is the initial stand-in forMA(G)..1064method WST wxLNT wLNToc???bb???otf?
?
?
(a) Forward applicationmethod WST wxLT wLT wxLNT wLNToc??
?
??bb?
?
?
?
?otf?
?
?
?
?
(b) Backward applicationTable 2: Transducer types and available methods of forward and backward application of a cascade.oc = offline composition, bb = bucket brigade, otf = on the fly.tion model of Yamada and Knight (2001) by trans-forming it from an English-tree-to-Japanese-stringmodel to an English-tree-to-Japanese-tree model.The Japanese trees are unlabeled, meaning theyhave syntactic structure but all nodes are labeled?X?.
We then cast this modified model as a cas-cade of LNT tree transducers.
Space does not per-mit a detailed description, but some example rulesare in Figure 3.
The rotation transducer R, a sam-ple of which is in Figure 3a, has 6,453 rules, theinsertion transducer I, Figure 3b, has 8,122 rules,and the translation transducer, T , Figure 3c, has37,311 rules.We add an English syntax language model L tothe cascade of transducers just described to bet-ter simulate an actual machine translation decod-ing task.
The language model is cast as an iden-tity WTT and thus fits naturally into the experimen-tal framework.
In our experiments we try severaldifferent language models to demonstrate varyingperformance of the application algorithms.
Themost realistic language model is a PCFG.
Eachrule captures the probability of a particular se-quence of child labels given a parent label.
Thismodel has 7,765 rules.To demonstrate more extreme cases of the use-fulness of the on-the-fly approach, we build a lan-guage model that recognizes exactly the 2,087trees in the training corpus, each with equalweight.
It has 39,455 rules.
Finally, to be ultra-specific, we include a form of the ?specific?
lan-guage model just described, but only allow theEnglish counterpart of the particular Japanese sen-tence being decoded in the language.The goal in our experiments is to apply a singletree t backward through the cascadeL?R?I?T ?tand find the 1-best path in the application WRTG.We evaluate the speed of each approach: bucketbrigade and on-the-fly.
The algorithm we use toobtain the 1-best path is a modification of the k-best algorithm of Pauls and Klein (2009).
Our al-gorithm finds the 1-best path in a WRTG and ad-mits an on-the-fly approach.The results of the experiments are shown inTable 3.
As can be seen, on-the-fly applicationis generally faster than the bucket brigade, aboutdouble the speed per sentence in the traditionalLM type method time/sentencepcfg bucket 28spcfg otf 17sexact bucket >1mexact otf 24s1-sent bucket 2.5s1-sent otf .06sTable 3: Timing results to obtain 1-best from ap-plication through a weighted tree transducer cas-cade, using on-the-fly vs. bucket brigade back-ward application techniques.
pcfg = model rec-ognizes any tree licensed by a pcfg built fromobserved data, exact = model recognizes each of2,000+ trees with equal weight, 1-sent = modelrecognizes exactly one tree.experiment that uses an English PCFG languagemodel.
The results for the other two languagemodels demonstrate more keenly the potential ad-vantage that an on-the-fly approach provides?thesimultaneous incorporation of information fromall models allows application to be done more ef-fectively than if each information source is consid-ered in sequence.
In the ?exact?
case, where a verylarge language model that simply recognizes eachof the 2,087 trees in the training corpus is used,the final application is so large that it overwhelmsthe resources of a 4gb MacBook Pro, while theon-the-fly approach does not suffer from this prob-lem.
The ?1-sent?
case is presented to demonstratethe ripple effect caused by using on-the fly.
In theother two cases, a very large language model gen-erally overwhelms the timing statistics, regardlessof the method being used.
But a language modelthat represents exactly one sentence is very small,and thus the effects of simultaneous inference arereadily apparent?the time to retrieve the 1-bestsentence is reduced by two orders of magnitude inthis experiment.6 ConclusionWe have presented algorithms for forward andbackward application of weighted tree trans-ducer cascades, including on-the-fly variants, anddemonstrated the benefit of an on-the-fly approachto application.
We note that a more formal ap-proach to application of WTTs is being developed,1065independent from these efforts, by Fu?lo?p et al(2010).AcknowledgmentsWe are grateful for extensive discussions withAndreas Maletti.
We also appreciate the in-sights and advice of David Chiang, Steve De-Neefe, and others at ISI in the preparation ofthis work.
Jonathan May and Kevin Knight weresupported by NSF grants IIS-0428020 and IIS-0904684.
Heiko Vogler was supported by DFGVO 1011/5-1.ReferencesAthanasios Alexandrakis and Symeon Bozapalidis.1987.
Weighted grammars and Kleene?s theorem.Information Processing Letters, 24(1):1?4.Brenda S. Baker.
1979.
Composition of top-down andbottom-up tree transductions.
Information and Con-trol, 41(2):186?213.Zolta?n E?sik and Werner Kuich.
2003.
Formal tree se-ries.
Journal of Automata, Languages and Combi-natorics, 8(2):219?285.Zolta?n Fu?lo?p and Heiko Vogler.
2009.
Weighted treeautomata and tree transducers.
In Manfred Droste,Werner Kuich, and Heiko Vogler, editors, Handbookof Weighted Automata, chapter 9, pages 313?404.Springer-Verlag.Zolta?n Fu?lo?p, Andreas Maletti, and Heiko Vogler.2010.
Backward and forward application ofweighted extended tree transducers.
Unpublishedmanuscript.Ferenc Ge?cseg and Magnus Steinby.
1984.
Tree Au-tomata.
Akade?miai Kiado?, Budapest.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Harry Bunt, Robert Malouf, and AlonLavie, editors, Proceedings of the Ninth Interna-tional Workshop on Parsing Technologies (IWPT),pages 53?64, Vancouver, October.
Association forComputational Linguistics.Werner Kuich.
1998.
Formal power series over trees.In Symeon Bozapalidis, editor, Proceedings of the3rd International Conference on Developments inLanguage Theory (DLT), pages 61?101, Thessa-loniki, Greece.
Aristotle University of Thessaloniki.Werner Kuich.
1999.
Tree transducers and formal treeseries.
Acta Cybernetica, 14:135?149.Andreas Maletti, Jonathan Graehl, Mark Hopkins, andKevin Knight.
2009.
The power of extended top-down tree transducers.
SIAM Journal on Comput-ing, 39(2):410?430.Andreas Maletti.
2006.
Compositions of tree se-ries transformations.
Theoretical Computer Science,366:248?271.Andreas Maletti.
2008.
Compositions of extended top-down tree transducers.
Information and Computa-tion, 206(9?10):1187?1196.Andreas Maletti.
2009.
Personal Communication.Mehryar Mohri, Fernando C. N. Pereira, and MichaelRiley.
2000.
The design principles of a weightedfinite-state transducer library.
Theoretical ComputerScience, 231:17?32.Mehryar Mohri.
1997.
Finite-state transducers in lan-guage and speech processing.
Computational Lin-guistics, 23(2):269?312.Mehryar Mohri.
2009.
Weighted automata algo-rithms.
In Manfred Droste, Werner Kuich, andHeiko Vogler, editors, Handbook of Weighted Au-tomata, chapter 6, pages 213?254.
Springer-Verlag.Adam Pauls and Dan Klein.
2009.
K-best A* parsing.In Keh-Yih Su, Jian Su, Janyce Wiebe, and HaizhouLi, editors, Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP, pages 958?966, Suntec,Singapore, August.
Association for ComputationalLinguistics.Fernando Pereira and Michael Riley.
1997.
Speechrecognition by composition of weighted finite au-tomata.
In Emmanuel Roche and Yves Schabes, ed-itors, Finite-State Language Processing, chapter 15,pages 431?453.
MIT Press, Cambridge, MA.William A.
Woods.
1980.
Cascaded ATN gram-mars.
American Journal of Computational Linguis-tics, 6(1):1?12.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceedingsof 39th Annual Meeting of the Association for Com-putational Linguistics, pages 523?530, Toulouse,France, July.
Association for Computational Lin-guistics.1066
