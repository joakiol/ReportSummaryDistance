Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1081?1091,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsQueries as a Source of Lexicalized Commonsense KnowledgeMarius Pas?caGoogle Inc.1600 Amphitheatre ParkwayMountain View, California 94043mars@google.comAbstractThe role of Web search queries has beendemonstrated in the extraction of attributesof instances and classes, or of sets of re-lated instances and their class labels.
Thispaper explores the acquisition of open-domain commonsense knowledge, usu-ally available as factual knowledge, fromWeb search queries.
Similarly to previ-ous work in open-domain information ex-traction, knowledge extracted from text- in this case, from queries - takes theform of lexicalized assertions associatedwith open-domain classes.
Experimentalresults indicate that facts extracted fromqueries complement, and have competitiveaccuracy levels relative to, facts extractedfrom Web documents by previous meth-ods.1 IntroductionMotivation: Open-domain information extrac-tion methods (Etzioni et al., 2005; Pennac-chiotti and Pantel, 2009; Wang and Cohen, 2009;Kozareva and Hovy, 2010; Wu et al., 2012) aimat distilling text into knowledge assertions aboutclasses, instances and relations among them (Et-zioni et al., 2011).
Ideally, the assertions wouldcomplement or expand upon knowledge avail-able in popular, human-created resources such asWikipedia (Remy, 2002) and Freebase (Bollackeret al., 2008), reducing costs and scalability is-sues associated with manual editing, curation andmaintenance of knowledge.Candidate knowledge assertions extracted fromtext for various instances and classes (Banko et al.,2007; Cafarella et al., 2008; Wu and Weld, 2010)must satisfy several constraints in order to be use-ful.
First, their boundaries must be correctly iden-tified within the larger context (e.g., a documentsentence) from which they are extracted.
In prac-tice, this is a challenge with arbitrary Web docu-ments, where even instances and class labels thatare complex nouns, and thus still shorter than can-didate assertions, are difficult to precisely detectand pick out from surrounding text (Downey etal., 2007).
This causes the extraction of assertionslike companies may ?be in the process?, hurri-canes may ?run from june?, or video games may?make people?
(Fader et al., 2011).
Second, theassertions must be correctly associated with theircorresponding instance or class.
In practice, tag-ging and parsing errors over documents of arbi-trary quality may cause the extracted assertions tobe associated with the wrong instances or classes.Examples are video games may ?watch movies?,or video games may ?read a book?.
Third, the as-sertions, even if true, must refer to relevant prop-erties or facts, rather than to statements of littleor no practical interest to anyone.
In practice,relevant properties may be difficult to distinguishfrom uninteresting statements in Web documents.Consequently, assertions extracted from Web doc-uments include the facts that companies may ?sayin a statement?, or that hurricanes may ?be justaround the corner?
or may ?be in effect?.Contributions: This paper explores the use ofWeb search queries, as opposed to Web docu-ments, as a textual source from which knowl-edge pertaining to open-domain classes can beextracted.
Previous explorations of the role ofqueries in information extraction include the ac-quisition of attributes of instances (Alfonsecaet al., 2010) and of classes (Van Durme andPas?ca, 2008); the acquisition of sets of related1081instances (Sekine and Suzuki, 2007; Jain andPennacchiotti, 2010) and their class labels (VanDurme and Pas?ca, 2008; Pantel et al., 2012); thedisambiguation of instances mentioned in queriesrelative to entries in external knowledge reposito-ries (Pantel and Fuxman, 2011) and its applica-tion in query expansion (Dalton et al., 2014); andthe extraction of the most salient of the instancesmentioned in a given Web document (Gamon etal., 2013).
In comparison, this paper shows thatqueries also lend themselves to the acquisition offactual knowledge beyond attributes, like the factsthat companies may ?buy back stock?, hurricanesmay ?need warm water?, and video games may?come out on tuesdays?.To extract knowledge assertions for diverseclasses of interest to Web users, the method ap-plies simple extraction patterns to queries.
Thepresence of the source queries, from which the as-sertions are extracted, is in itself deemed evidencethat the Web users who submitted the queriesfind the assertions to be relevant and not just ran-dom statements.
Experimental results indicate thatknowledge assertions extracted from queries com-plement, and have competitive accuracy levels rel-ative to, knowledge extracted from Web docu-ments by previous methods.2 Extraction from QueriesQueries as Knowledge: Users tend to formu-late their Web search queries based on knowl-edge that they already possess at the time of thesearch (Pas?ca, 2007).
Therefore, search queriesplay two roles simultaneously: in addition to re-questing new information, they indirectly conveyknowledge in the process.A fact corresponds to a property that, togetherwith other properties, help define the semantics ofthe class and its interaction with other classes.
Theextraction of factual knowledge from queries startsfrom the intuition that, if a fact F is relevant for aclass C, then users are likely to ask for variousaspects of the fact F , in the context of the classC.
If companies may ?pay dividends?
or ?get au-dited?, and such properties are relatively promi-nent for companies, then users eventually submitqueries to inquire about the facts.Often, queries will be simple concatenations ofkeywords: ?companies pay dividends?
or perhaps?company dividends?, ?audit companies?.
Sincethere are no restrictions on the linguistic structureQuery logsTarget classesDisease: {diseases, illnesses, medical conditions, ...}how does an actor prepare for a role   how do actors get an agenthow do actors get paid   why do actors need to warm upwhy are actors left handed   how do actors memorize their linesHurricane: {hurricanes, ...}how is a disease transmitted   how are diseases inherited from parentshow is a disease treated   how is a disease diagnosedhow do diseases enter the body   how does a disease mutatewhy does a hurricane weaken over land   how are hurricanes predictedwhy does a hurricane lose strength over land   how is a hurricane forecastedwhy does a hurricane have an eye   how does a hurricane dissipateExtracted factsActor: {prepare for a role, get an agent, get paid, be left handed,Disease: {be transmitted, be inherited from parents, be treated,Hurricane: {weaken over land, be predicted, lose strength over land,need to warm up, memorize their lines, ...}be diagnosed, enter the body, mutate, ...}be forecasted, have an eye, dissipate, ...}Actor: {actors, ...}Figure 1: Overview of extraction of knowledgefrom Web search queriesof keyword-based queries, extracting facts fromsuch queries would be difficult.
But if queries arerestricted to fact-seeking questions, the expectedformat of the questions makes it easier to iden-tify the likely boundaries of the class and the factmentioned in the queries.
Queries such as ?whydoes a (company)C(pay dividends)F?
and ?howdo (companies)C(get audited)F?, follow the lin-guistic structure, even if minimal, imposed by for-mulating the query as a question.
This allows oneto approximate the location of the class C, possi-bly towards the beginning of the query; the start ofthe fact F , possibly as the verb immediately fol-lowing the class; and the end of the fact, whichpossibly coincides with the end of the query.Acquisition from Queries: The extractionmethod proposed in this paper takes as input a setof target classes, each of which is available as aset of class descriptors, i.e., phrases that describethe class.
It also has access to a set of anonymizedqueries.
As illustrated in Figure 1, the method se-lects queries that contain a class descriptor andwhat is deemed to be likely a fact.
It outputsranked lists of facts for each class.
The extrac-tion consists in several stages: 1) the selection ofa subset of queries that refer to a class in a formthat suggests the queries inquire about a fact of theclass; 2) the extraction of facts, from query frag-ments that describe the property of interest to userssubmitting the queries; and 3) the aggregation and1082ranking of facts of a class.Extraction Patterns: In order to determinewhether a query contains a fact for a class, thequery is matched against the extraction patternsfrom Table 1.The use of targeted patterns in relation extrac-tion has been suggested before (Hearst, 1992;Fader et al., 2011; Mesquita et al., 2013).
Specifi-cally, in (Tokunaga et al., 2005), the patterns ?A ofD?
or ?what is the A of D?
extract noun-phrase Aattributes from queries and documents, for phrasedescriptors D of the class.
In our case, the pat-terns are constructed such that they match ques-tions that likely inquire about the reason why, ormanner in which, a relevant fact F may hold fora class C. For example, the first pattern from Ta-ble 1 matches the queries ?why does a companypay dividends?
and ?why do video games comeout on tuesdays?.
These queries seek explanationsfor why certain properties may hold for companiesand video games respectively.A class C can be mentioned in queries throughlexicalized, phrase descriptors D that capture itsmeaning.
The descriptors D of the class Cmay be available as non-disambiguated items, i.e.,as strings (companies, firms, businesses, videogames); or as disambiguated items, that is, aspointers to knowledge base entries with a disam-biguated meaning (Company, Video Game).
In thefirst case, the matching of a query fragment, onone hand, to the portion of an extraction patterncorresponding to the class C, on the other hand,consists in simple string matching with one of thedescriptors D specified for C. In the second case,the matching requires that the disambiguation ofthe query fragment, in the context of the query,matches the desired disambiguated meaning of Cfrom the pattern.
The subset of queries matchingany of the extraction patterns, for any descriptorD of a class C, are the queries that contribute toextracting facts of the class C.If a pattern from Table 1 employs a form of theauxiliary verb ?be?, the extracted facts are modi-fied by having the verb ?be?
inserted at their be-ginning.
For example, the fact ?be stored side-ways?
is extracted from the query ?why is winestored sideways?.
In all patterns, the candidatefact is required to start with a verb that acts as thepredicate of the query.Ranking of Facts: Facts of a class C are aggre-gated from facts of individual class descriptors D.Extraction Pattern?
Examples of Matched Querieswhy [does|did|do] [a|an|the|<nothing>] D F?
why does a (company)D(pay dividends)F?
why do (planes)D(take longer to fly west than east)F?
why do (video games)D(come out on tuesdays)Fwhy [is|was|were] [a|an|the|<nothing>] D F?
why are (cars)D(made of steel)F?
why is a (newspaper)D(written in columns)F?
why is (wine)D(stored sideways)Fhow [does|did|do] [a|an|the|<nothing>] D F?
how does a (company)D(use financial statements)F?
how does (food)D(get absorbed)F?
how do (stadiums)D(get cleaned)Fhow [is|was|were] [a|an|the|<nothing>] D F?
how are (hurricanes)D(predicted)F?
how is a (treaty)D(ratified)F?
how is a (cell phone)D(unlocked)FTable 1: The extraction patterns match querieslikely to inquire about facts of a class (D=a phraseacting as a class descriptor; F=a sequence of to-kens whose first token is the head verb of thequery)A fact F is deemed more relevant for C if the factis extracted for more of the descriptors D of theclass C, and for fewer descriptors D that do notbelong to the class C. Concretely, the score of afact for a class is the lower bound of the Wilsonscore interval (Brown et al., 2001):Score(F,C) = LowBound(Wilson(N+, N?))where:?
the number of positive observations N+isthe number of queries for which the fact A isextracted for some descriptor D of the class C,|{Query(D,A)}D?C|; and?
the number of negative observations N?isthe number of queries for which the fact F is ex-tracted for some descriptors D outside of the classC, |{Query(D,A)}D/?C|.The scores are internally computed at 95% con-fidence.
Facts of each class are ranked in decreas-ing order of their scores.
In case of ties, facts areranked in decreasing order of the frequency sumof the source queries from which the facts are ex-tracted.3 Experimental SettingTextual Data Sources: The experiments relyon a random sample of around 1 billion fully-anonymized Web search queries in English.
Thesample is drawn from queries submitted to ageneral-purpose Web search engine.
Each queryis available independently from other queries, andis accompanied by its frequency of occurrence in1083Target Class (class descriptors to be looked up in queries)Actor (actors) Mountain (mountains)Aircraft (planes) Movie (movies)Award (awards) NationalPark (national parks)Battle (battles) NbaTeam (nba teams)Car (cars) Newspaper (newspapers)CartoonChar Painter(cartoon characters) (painters)CellPhone ProgLanguage(cell phones) (programming languages)ChemicalElem (elements) Religion (religions)City (cities) River (rivers)Company (companies) SearchEngine (search engines)Country (countries) SkyBody (celestial bodies)Currency (currencies) Skyscraper (skyscrapers)DigitalCamera SoccerClub(digital cameras) (soccer teams)Disease (diseases) SportEvent (sport events)Drug (drugs) Stadium (stadiums)Empire TerroristGroup(empires) (terrorist groups)Flower (flowers) Treaty (treaties)Food (foods) University (universities)Holiday (holidays) VideoGame (video games)Hurricane (hurricanes) Wine (wines)Table 2: Set of 40 target classes used in the evalu-ation of extracted factsthe query logs.Target Classes: Table 2 shows the set of 40 tar-get classes for evaluating the extracted facts.
Sim-ilar evaluation strategies were followed in previ-ous work (Pas?ca, 2007).
As illustrated earlier inFigure 1, a target class consists in a small set ofphrase descriptors.
The phrase descriptors are se-lected such that they best approximate the mean-ing of the class.
In general, the descriptors can beselected and expanded with any strategy from anysource.
One such possible source might be syn-onym sets from WordNet (Fellbaum, 1998).
Fol-lowing a stricter strategy, the sets of descriptorsin our experiments contain only one phrase each,manually selected to match the target class.
Ex-amples are the sets of phrase descriptors {actors}for the class Actor and {nba teams} for NbaTeam.The occurrence of a descriptor (nba teams) ina query (?how do nba teams make money?)
isdeemed equivalent to a mention of the correspond-ing class (NbaTeam) in that query.
Each set of de-scriptors of a class is then expanded (not shown inTable 2), to also include the singular forms of thedescriptors (e.g., nba team for nba teams).
Furtherinclusion of additional descriptors would increasethe coverage of the extracted facts.Experimental Runs: The baseline run RDisthe extraction method introduced in (Fader et al.,2011).
The method produces triples of an instanceor a class, a text fragment capturing a fact, and an-other instance or class.
In these experiments, thesecond and third elements of each triple are con-catenated together, giving pairs of an instance ora class, and a fact applying to it.
The baselinerun is applied to around 500 million Web docu-ments in English.
1 In addition to the baseline run,the method introduced in this paper constitutes thesecond experimental run RQ.
Facts extracted bythe two experimental runs are directly compara-ble: both are text snippets extracted from the re-spective sources of text - documents in the case ofRD, or queries in the case of RQ.Parameter Settings: Queries that match any ofthe extraction patterns from Table 1 are syntacti-cally parsed (Petrov et al., 2010), in order to verifythat the first token of an extracted fact is the headverb of the query.
Extracted facts that do not sat-isfy the constraint are discarded.
A positive sideeffect of doing so is to avoid extraction from someof the particularly subjective queries.
For exam-ple, facts extracted from the queries ?why is (A)evil?
or ?why is (B) ugly?, where (A) and (B) arethe name of a company and actress respectively,are discarded.4 Evaluation ResultsAccuracy: The measurement of recall requiresknowledge of the complete set of items (in ourcase, facts) to be extracted.
Unfortunately, thisnumber is often unavailable in information extrac-tion tasks in general (Hasegawa et al., 2004), andfact extraction in particular.
Indeed, the manualenumeration of all facts of each target class, tomeasure recall, is unfeasible.
Therefore, the eval-uation focuses on the assessment of accuracy.Following evaluation methodology from priorwork (Pas?ca, 2007), the top 50 facts, from a rankedlists extracted for each target class, are manuallyassigned correctness labels.
A fact is marked asvital, if it must be present among representativefacts of the class; okay, if it provides useful butnon-essential information; and wrong, if it is in-correct (Pas?ca, 2007).
For example, the facts ?runon kerosene?, ?be delayed?
and ?fly wiki?
are an-notated as vital, okay and wrong respectively forthe class Aircraft.
To compute the precision score1At the time when the experiments were conducted, thefacts were extracted by the baseline run from English doc-uments in the ClueWeb collection, and were accessible athttp://reverb.cs.washington.edu.1084Target Class: Sample of Extracted Facts (with SourceQueries)Target Class: Sample of Extracted Facts (with SourceQueries)Actor (may): prepare for a role (how does an actor preparefor a role), get an agent (how do actors get an agent), do lovescenes (how do actors do love scenes), get paid (how do actorsget paid), be left handed (why are actors left handed), need towarm up (why do actors need to warm up)Car (may): backfire (why does a car backfire), burn oil (whydo cars burn oil), pull to the right (why do cars pull to theright), pull to the left (why does a car pull to the left), catchon fire (how does a car catch on fire), run hot (why do carsrun hot), get repossessed (why do cars get repossessed)Company (may): buy back stock (how does a company buyback stock), go public (why does a company go public), buyback shares (why do companies buy back shares), incorporatein delaware (why do companies incorporate in delaware), paydividends (why does a company pay dividends), merge (howdo companies merge)Disease (may): be transmitted (how is a disease transmitted),be inherited from parents (how are diseases inherited fromparents), affect natural selection (how do diseases affect nat-ural selection), be treated (how is a disease treated), affect theconquest of the americas (how did diseases affect the conquestof the americas), be diagnosed (how is a disease diagnosed)Hurricane (may): weaken over land (why does a hurricaneweaken over land), be predicted (how are hurricanes pre-dicted), lose strength over land (why does a hurricane losestrength over land), have an eye (why does a hurricane havean eye), be forecasted (how is a hurricane forecasted), dissi-pate (how does a hurricane dissipate), lose strength (how dohurricanes lose strength)NbaTeam (may): make money (how does an nba team makemoney), communicate to win (how does an nba team commu-nicate to win), want expiring contracts (why do nba teamswant expiring contracts), make the playoffs (how do nbateams make the playoffs), get their names (how do nba teamsget their names), do sign and trades (why do nba teams dosign and trades), lose money (how do nba teams lose money)Table 3: Examples of facts extracted for various classes by run RQClass Precision Class Precision@10 @20 @50 @10 @20 @50RDRQRDRQRDRQRDRQRDRQRDRQActor 0.60 0.85 0.57 0.85 0.60 0.83 Mountain 0.20 0.75 0.10 0.72 0.05 0.55Aircraft 0.50 0.95 0.42 0.87 0.47 0.81 Movie 0.40 0.20 0.37 0.20 0.40 0.32Award 0.50 0.25 0.45 0.25 0.52 0.23 NationalPark 0.40 0.70 0.32 0.72 0.30 0.69Battle 0.25 0.45 0.42 0.46 0.38 0.44 NbaTeam 0.60 0.75 0.42 0.80 0.20 0.77Car 0.55 0.80 0.62 0.82 0.52 0.75 Newspaper 0.25 0.80 0.32 0.55 0.44 0.59CartoonChar 0.25 0.60 0.22 0.57 0.18 0.55 Painter 0.30 0.75 0.40 0.65 0.42 0.61CellPhone 0.75 0.90 0.75 0.82 0.55 0.82 ProgLanguage 0.20 0.75 0.25 0.72 0.25 0.70ChemicalElem 0.45 0.90 0.45 0.72 0.54 0.72 Religion 0.10 0.80 0.30 0.70 0.13 0.69City 0.30 0.80 0.27 0.67 0.27 0.63 River 0.65 0.95 0.70 0.87 0.54 0.57Company 0.60 0.95 0.57 0.95 0.53 0.91 SearchEngine 0.40 0.70 0.37 0.65 0.38 0.64Country 0.30 0.85 0.25 0.90 0.20 0.83 SkyBody 0.55 0.00 0.32 0.00 0.28 0.00Currency 0.40 0.90 0.25 0.85 0.22 0.73 Skyscraper 0.45 0.85 0.37 0.77 0.24 0.78DigitalCamera 0.30 0.90 0.35 0.85 0.42 0.77 SoccerClub 0.35 0.15 0.37 0.33 0.41 0.31Disease 0.55 0.90 0.60 0.70 0.64 0.60 SportEvent 0.30 0.00 0.27 0.00 0.32 0.00Drug 0.20 0.95 0.30 0.87 0.40 0.78 Stadium 0.50 0.85 0.50 0.77 0.47 0.75Empire 0.15 0.45 0.12 0.52 0.23 0.49 TerroristGroup 0.90 0.55 0.70 0.55 0.55 0.53Flower 0.60 0.90 0.50 0.80 0.48 0.78 Treaty 1.00 0.75 0.90 0.75 0.77 0.59Food 0.65 0.80 0.55 0.85 0.43 0.85 University 0.10 0.95 0.05 0.92 0.10 0.70Holiday 0.30 0.25 0.17 0.22 0.19 0.14 VideoGame 0.20 0.90 0.25 0.85 0.28 0.77Hurricane 0.40 0.80 0.37 0.77 0.32 0.73 Wine 0.70 1.00 0.60 0.87 0.56 0.70Average-Class 0.43 0.71 0.40 0.67 0.38 0.63Table 4: Relative accuracy of facts extracted from documents in run RD, vs. facts extracted from queriesin run RQover a set of facts, the correctness labels are con-verted to numeric values: vital to 1.0, okay to 0.5,and wrong to 0.0.
Precision is the sum of the cor-rectness values of the facts, divided by the numberof facts.
Table 3 shows a sample of facts extractedfrom queries by run RQ, which are judged to bevital or okay.Table 4 provides a comparison of precision atranks 10, 20 and 50, for each of the 40 targetclasses and as an average over all target classes.The scores vary from one class to another and be-tween the two runs, for example 0.22 (RD) and0.73 (RQ) for the class Currency at rank 50, but0.77 (RD) and 0.59 (RQ) for Treaty.
Run RQfailsto extract any facts for two of the target classes,SkyBody and SportEvent.
Therefore, it receives nocredit for those classes during the computation ofprecision.Over all target classes, run RQis superior to runRD, with relative precision boosts of 65% (0.71vs.
0.43) at rank 10, 67% at rank 20, and 65% atrank 50.
The results show that facts extracted from1085Run: [Ranked Facts Extracted from Text for a Sample of Classes]Class: Actor (may):RD: [do a great job, get the part, play their roles, play their parts, play their characters, be on a theatre, die aged 81, be allgreat, deliver their lines, portray their characters, take on a role, be best known for his role, play the role of god, be people,give great performances, bring the characters to life, wear a mask, be the one, have chemistry, turn director, read the script, ..]RQ: [prepare for a role, get an agent, do love scenes, get paid, be left handed, need to warm up, get started, get paid so much,memorize their lines, get ripped so fast, remember their lines, make themselves cry, learn their lines, jump out of a windowin times square, lose weight so fast, play dead, be paid, kiss, remember lines, memorize lines, get discovered, get paid formovies, go uncredited, say break a leg, get their start, have perfect skin, become actors, ..]Class: Car (may):RD: [get a tax write-off, can be more competitive than airline rates, be in good condition, be first for second hand cars, be inthe shop, relocate to a usa firm, be in motion, come to a stop, hire companies, be in great shape, be for sale, hire service fromspain, ride home, be on fire, use the autos.com, come to a halt, catch fire, be on road, be on display, go on sale, hit a tree, beavailable for delivery, stop in front, be a necessity, go off the road, pull out in front, hire services, run out of gas, ..]RQ: [backfire, burn oil, save ostriches from extinction, pull to the right, pull to the left, catch on fire, run hot, sputter, getrepossessed, have a top speed, be called a car, have gears, get impounded, be called cars, go to auction, called whip, made ofsteel, get hot in the sun, shake at high speed, changed america, totaled, cut out, cut off while driving, fail emissions, protectfrom lightning, run rich, lose oil, become electrically charged, cut off, flip over, know tire pressure, have a maximum speed,require premium gas, shake at high speeds, stall out, cause acid rain, fog up, get stuck in park, need an oil change, ..]Class: Company (may):RD: [say in a statement, specialize in local moves, be in the process, go out of business, have been in business, be in business,do business, file for bankruptcy, make money, be on track, say in a press release, be a place, have cut back on health insurance,state in a press release, be on the verge, save money, be in talks, have helped thousands of consumers, reduce costs, go bust,be in the midst, say in a release, be founded in 1999, be in trouble, be founded in 2000, be losing money, ..]RQ: [buy back stock, go public, buy back shares, incorporate in delaware, pay dividends, merge, go global, go international,use financial statements, verify education, expand internationally, go green, verify employment, need a website, choose toform as a corporation, do market research, go private, diversify, go into administration, get on angies list, pay dividend, struckoff, buy back their shares, get audited, need a mission statement, repurchase common stock, spin off, get listed on the nyse,create value, distribute dividends, need a strategic plan, ..]Class: Mountain (may):RD: [spot fever, meet the sea, be covered with snow, be covered in snow, be the place, come into view, be on fire, be fun,fly fishing, be volcano, be moved out of their places, enjoy the exhilaration, meet the ocean, be available for hire, keep theirsecrets, win the mwc in 2010, ..]RQ: [affect rainfall, affect the climate of an area, affect climate, be measured, be formed, be created, be made, grow, affectweather, have snow on top, affect solar radiation, affect temperature, be formed ks2, affect the weather, be built, affect people,look blue, tops cold, affect neighboring climates, be formed video, help shape the development of greek civilization, be madefor kids, occur, affect the climate, be formed, be formed wikipedia, have roots, affect precipitation, exist, affect life on earth,be formed kids, float in avatar, erode, have snow on the top, affect the political character of greece, help rain form, ..]Table 5: Comparative top facts extracted for a sample of classes from documents (RD) or queries (RQ)queries have higher levels of accuracy.Facts from Documents vs. Queries: Table 5compares the top facts extracted by the two exper-imental runs for a sample of target classes.
Mostcommonly, erroneous facts are extracted by runRDdue to the extraction of relatively uninterest-ing properties (a Company may ?say in a state-ment?
or ?be in the process?).
Other errors inRDare caused by wrong boundary detection offacts within documents (a Company may ?be inthe midst?
), or by the association of a fact with thewrong instance or class (a Car may ?hire compa-nies?
or ?hire services?
).As for facts extracted by run RQ, they are some-times too informal, due to the more conversa-tional nature of queries when compared to docu-ments.
Queries may suggest that a Car may ?knowtire pressure?.
Occasionally, similarly to factsfrom documents, they have wrong boundaries (aMountain may ?be made for kids?
or ?be formedwikipedia?
); and they may correspond to less in-teresting, or too specific, properties (a Companymay ?incorporate in delaware?).
Lastly, queriesmay appear to be questions, but occasionally theyreally are not.
An example is the query ?why didthe actor jump out of the window in times square?,which may refer to a joke.
When such queriesmatch one of the extraction patterns, they producewrong facts.
Overall, Table 5 corroborates thescores from Table 4.
It suggests that a) facts ex-tracted by either RDor RQstill need refinement,before they can capture essential characteristicsof the respective classes and nothing else; and b)facts extracted in run RQhave higher quality thanfacts extracted in run RD.
Indeed, because fact-seeking queries inquire about the value (or rea-son, or manner) of some relations of an instance,the facts themselves tend to be more relevant thanfacts extracted from arbitrary document sentences.An issue related to facts extracted from text1086is their ability to capture the kind of ?obvious?commonsense knowledge (Zang et al., 2013) thatwould be essential for machine-driven reasoning.If it is obvious that ?teachers give lectures?, howlikely is it for such information to be explic-itly stated in documents or, even more interest-ingly, inquired about in queries?
Anecdotal ev-idence gathered during experimentation suggeststhat queries do produce many commonsense facts,perhaps even surprisingly so given that a) queriestend to be shorter and grammatically simpler thandocument sentences; and b) the patterns in Ta-ble 1 are relatively more restrictive than the pat-terns used in (Fader et al., 2011).
Indeed, the pat-terns in Table 1, when applied to queries like ?whydo teachers give homework?, ?why do teach-ers give grades?, actually produce commonsenseknowledge that teachers give homework, grades(to their students).
In fact, the quality of equivalentfacts extracted from documents in (Fader et al.,2011) may be lower.
Concretely, facts extractedin (Fader et al., 2011) state that what teachers giveis students, class, homework and feedback, in thisorder.
The first two of these extractions are errors,likely caused by the incorrect detection of com-plex entities and their inter-dependencies in docu-ment sentences (Downey et al., 2007).A necessary condition for the usefulness of ex-tracted facts is that the source text contain consis-tent, true information.
But both documents andqueries may contain contradictory or false infor-mation, whether due to unsupported conjectures,unintended errors or systematic campaigns thatfall under the scope of adversarial information re-trieval (Castillo and Davison, 2011).
The phenom-ena potentially affect prior work on Web-basedopen-domain extraction, and potentially affect thequality of facts extracted from queries in this pa-per.
For example, facts extracted from queries like?why do companies like obamacare?
and ?why docompanies hate obamacare?
would be inconsis-tent, if not incorrect.Occasionally, facts extracted from the two textsources refer to the same properties.
For exam-ple, a VideoGame may ?be good for the hand-eyecoordination?, according to documents; and may?improve hand eye coordination?, according toqueries.
Nevertheless, facts derived from querieslikely serve as a complement, rather than replace-ment, of facts from documents.
In particular, factsextracted from queries make no attempt to iso-late the value of the respective properties, whereasfacts extracted from documents usually do.Stricter Comparison of Data Sources: In theexperiments described so far, distinct sets of pat-terns are applied in the experimental runs to doc-uments vs. queries.
More precisely, run RDap-plies the patterns introduced in (Fader et al., 2011)to document sentences, whereas run RQthe pat-terns shown in Table 1 to queries.
To more ac-curately gauge the role of queries vs. documentsin extracting facts from unstructured text, addi-tional experiments isolate the effect of extractingfacts from different types of data sources.
Forthis purpose, the same set of patterns from Ta-ble 1 is matched against the sentences from around500 million Web documents.
The patterns are ap-plied to document sentences converted to lower-case, similarly to how they are applied to queries.This corresponds to a new experimental run RDS,which employs the same patterns as the earlier runRQbut runs over document sentences instead ofqueries.As an average over the target classes, the pre-cision of facts extracted by run RDSis 0.50, 0.47and 0.44 at ranks 10, 20 and 50 respectively.
Twoconclusions can be drawn from comparing thesescores with the average scores from the earlier Ta-ble 4.
First, the average precision of run RDSishigher than for run RD.
In other words, whenextracting from document sentences in RDSandRD, the patterns proposed in our method givefewer and more accurate facts than the patternsfrom (Fader et al., 2011).
Second, although RDSismore accurate than RD, it is less accurate than runRQ.
Note that, among the top 50 facts extractedfor each target class by runs RDSand RQ, an aver-age of 13% of the facts are extracted by both runs.There are several phenomena contributing to thedifference in precision.
While inherently noisy,queries tend to be more compact, and thereforemore focused.
In comparison, document sentencesmatching the patterns are often more convoluted(e.g., ?who do cities keep building stadiums de-spite study after study showing they do not makemoney?, or ?how does a company go from lowassociate satisfaction to #15 on the fortune 100best list in the midst of a crippling recession?
).Furthermore, both queries and sentences may notbe useful questions from which relevant facts canbe extracted, even when they match the extractionpatterns.
However, anecdotal evidence suggests1087that this happens more frequently with documentsentences than with queries.
Examples includedocument sentences extracted from sites aggregat-ing jokes (?why did the cell phone ask to see thepsychologist?).
The results confirm that queriesrepresent an intriguing resource for fact extraction,providing a useful complement to document sen-tences for the purpose of extracting facts.Quantitative Results: From the set of queriesused as input in run RQ, 3.8% of all queries startwith why or how.
In turn, 13.6% of them matchone of the extraction patterns from Table 1, andtherefore produce a candidate fact in RQ.
In thecase of run RDS, 18.7% of the document sentencesthat start with why or how match one of the pat-terns from Table 1.Choice of Extraction Patterns: The sets of pat-terns sometimes employed in relation extractionfrom documents (Hearst, 1992) occasionally ben-efit from the addition of new patterns, or the re-finement into more specific patterns (Kozareva etal., 2008).
Similarly, the set of patterns proposedin Table 1, which targets the extraction of factsfrom queries, is neither exhaustive nor final.
Otherpatterns beyond why and how may prove useful,whether they rely on relatively less frequent whenand where queries, or extract relations contain-ing underspecified arguments from who or whatqueries.When applied to queries in run RQ, the how pat-terns from Table 1 match 3.3 times more queriesthan the why patterns.In separate experiments, why vs. how patternsfrom Table 1 are temporarily disabled.
The ra-tio of facts extracted on average per target class inrun RQdiminishes from 100% (with both patterns)to 30% (with why only) or 70% (with how only).Overall, no difference in accuracy is observed overfacts extracted by why vs. how patterns.Choice of Phrase Descriptors: A separate experi-ment investigates the impact of expanding the setsof phrase descriptors associated with each targetclass.
Among many possible strategies, each set ofphrase descriptors associated with a target class isexpanded automatically, using WordNet and dis-tributional similarities.
For this purpose, for eachtarget class, the set of synonyms and hyponyms ofall senses, if any, available in WordNet for eachphrase descriptor is intersected with the set of the50 most distributionally similar phrases, if any,available for each phrase descriptor.
The origi-nal set of phrase descriptors of each target classis then expanded, to include the phrases from theintersected set, if any.A repository of distributionally similar phrasesis collected in advance following (Lin and Wu,2009; Pantel et al., 2009), from a sample of around200 million Web documents.
Their intersectionwith phrases collected from WordNet aims at re-ducing the noise associated with expansion solelyfrom either source.
For example, for the classActor, the set of phrases {player, worker, heavy,plant, actress, comedian, film star, ..} is collectedfrom WordNet for the descriptor actors.
The setis intersected with the set of phrases {film stars,performers, comedians, actresses, ..} most dis-tributionally similar to actors.
Examples of setsof phrase descriptors after expansion are {actors,actresses, comedians, players, film stars, ..}, forthe class Actor; and {battles, naval battles, fights,skirmishes, struggles, ..}, for Battle.On average, the sets of phrase descriptors as-sociated with each target class contains 2 vs. 11phrases, before vs. after expansion.
Some of thesets of phrase descriptors, such as for the targetclasses CartoonChar and DigitalCamera, remainunchanged after expansion.
As expected, expan-sion may introduce noisy phrase descriptors, suchas players for Actor, or diets for Food.
The pres-ence of noisy phrase descriptors lowers the preci-sion of the extracted facts.
After expansion, theprecision scores of RQ, as an average over all tar-get classes, become smaller by 6% (0.71 vs. 0.67),at rank 10; 6% (0.67 vs. 0.63), at rank 20; and 7%(0.63 vs. 0.59), at rank 50.
Expansion also affectsrelative coverage, increasing the average numberof facts extracted by RQper target class by morethan twice (i.e., by a factor of 2.6).Redundant Facts: Due to lexical variation inthe source text fragments, some of the extractedfacts may be near-duplicates of one another.
Ingeneral, the phenomenon affects facts extractedfrom text by previous methods (Van Durme andPas?ca, 2008; Etzioni et al., 2011; Fader et al.,2011).
In particular, it affects facts extracted fromboth documents or queries in our experiments.For example, the facts extracted from documentsfor Actor include ?play their roles?, ?play theirparts?, ?play their characters?
and ?portrayedtheir characters?.
Separately, the facts ?memorizetheir lines?, ?remember their lines?
and ?learntheir lines?
are extracted from queries for the class1088Actor.
The automatic detection of equivalent factswould increase the usefulness of facts extractedfrom text in general, and of facts extracted by themethod presented here in particular.5 Related WorkA variety of methods address the more generaltask of acquisition of open-domain relations fromtext, e.g., (Banko et al., 2007; Carlson et al., 2010;Wu and Weld, 2010; Fader et al., 2011; Lao etal., 2011; Mausam et al., 2012; Lopez de La-calle and Lapata, 2013).
In general, relations ex-tracted from document sentences (e.g., ?ClaudeMonet was born in Paris?)
are tuples of an argu-ment (claude monet), a text fragment acting as thelexicalized relation (was born in), and another ar-gument (paris) (cf.
(Banko et al., 2007; Fader etal., 2011; Mausam et al., 2012)).
For convenience,the relation and second argument may be concate-nated into a fact applying to the first argument, asin ?was born in paris?
for claude monet.
Rel-atively shallow tools like part of speech taggers,or more complex tools like semantic taggers (VanDurme et al., 2008; Van Durme et al., 2009) can beemployed in order to extract relations from docu-ment sentences.
The former choice scales betterto Web documents of arbitrary quality, whereasthe latter could be more accurate over high-qualitydocuments such as news articles (Mesquita etal., 2013).
In both cases, document sentencesmentioning an instance or a class may refer toproperties of the instance that people other thanthe author of the document are less likely to in-quire about.
Consequently, even top-ranked ex-tracted relations occasionally include less infor-mative ones, such as ?come into view?
for mountrainier, ?be on the table?
for madeira wine, or?allow for features?
for javascript (Fader et al.,2011).Data available within Web documents, fromwhich relations are extracted in previous work,includes unstructured (Banko et al., 2007; Faderet al., 2011), structured (Raju et al., 2008) andsemi-structured text (Yoshinaga and Torisawa,2007; Pasupat and Liang, 2014), layout format-ting tags (Wong et al., 2008), itemized lists or ta-bles (Cafarella et al., 2008).
Another source ishuman-compiled resources (Wu and Weld, 2010)including infoboxes and category labels (Nastaseand Strube, 2008; Hoffart et al., 2013; Wang etal., 2013; Flati et al., 2014) in Wikipedia, or topicsand relations in Freebase (Weston et al., 2013; Yaoand Van Durme, 2014).Whether Web search queries are a useful tex-tual data source for open-domain information ex-traction has been investigated in several tasks.
Ex-amples are collecting unlabeled sets of similar in-stances (Jain and Pennacchiotti, 2010), extract-ing attributes of instances (Alfonseca et al., 2010;Pas?ca, 2014), identifying mentions in queriesof instances defined in a manually-created re-source (Pantel et al., 2012), and extracting themost salient of the instances mentioned withinWeb documents (Gamon et al., 2013).Other previous work shares the intuition that thesubmission of Web search queries is influencedby, and indicative of, various relations.
Relationsare loosely defined, either by approximating themvia distributional similarities (Alfonseca et al.,2009), or by exploring the acquisition of untyped,similarity-based relations from query logs (Baeza-Yates and Tiberi, 2007).
In both cases, the com-puted relations hold among full-length queries.Untyped relations can also be identified amongquery terms for the purpose of query reformula-tion (Wang and Zhai, 2008).
More generally, thechoice of query substitutions may reveal variousrelations among full queries or query terms (Joneset al., 2006), but requires individual queries to beconnected to one another via query sessions or viasearch-result click-through data.6 ConclusionAnonymized search queries submitted by Webusers represent requests for knowledge.
Collec-tively, they can also be seen as informal, lexi-calized knowledge assertions.
By asking about aproperty of some class, fact-seeking queries im-plicitly assert the relevance of the property for theclass.Since Web search queries refer to propertiesthat Web users are collectively interested in, fac-tual knowledge extracted from queries tends to bemore relevant than facts extracted from arbitrarydocuments using previous methods.
Current workexplores the extraction of facts from implicit ratherthan explicit fact-seeking questions, that is, fromqueries that do not start with a question prefix; andthe combination of queries as a source of more ac-curate facts, and documents as a source of morenumerous facts.1089ReferencesE.
Alfonseca, K. Hall, and S. Hartmann.
2009.
Large-scalecomputation of distributional similarities for queries.
InProceedings of the 2009 Conference of the North Amer-ican Association for Computational Linguistics (NAACL-HLT-09), Short Papers, pages 29?32, Boulder, Colorado.E.
Alfonseca, M. Pas?ca, and E. Robledo-Arnuncio.
2010.Acquisition of instance attributes via labeled and relatedinstances.
In Proceedings of the 33rd International Con-ference on Research and Development in Information Re-trieval (SIGIR-10), pages 58?65, Geneva, Switzerland.R.
Baeza-Yates and A. Tiberi.
2007.
Extracting semanticrelations from query logs.
In Proceedings of the 13thACM Conference on Knowledge Discovery and Data Min-ing (KDD-07), pages 76?85, San Jose, California.M.
Banko, Michael J Cafarella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open information extraction fromthe Web.
In Proceedings of the 20th International JointConference on Artificial Intelligence (IJCAI-07), pages2670?2676, Hyderabad, India.K.
Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor.2008.
Freebase: A collaboratively created graph databasefor structuring human knowledge.
In Proceedings of the2008 International Conference on Management of Data(SIGMOD-08), pages 1247?1250, Vancouver, Canada.L.
Brown, T. Cai, and A. DasGupta.
2001.
Interval es-timation for a binomial proportion.
Statistical Science,16(2):101?117.M.
Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.2008.
WebTables: Exploring the power of tables on theWeb.
In Proceedings of the 34th Conference on VeryLarge Data Bases (VLDB-08), pages 538?549, Auckland,New Zealand.A.
Carlson, J. Betteridge, R. Wang, E. Hruschka, andT.
Mitchell.
2010.
Coupled semi-supervised learning forinformation extraction.
In Proceedings of the 3rd ACMConference on Web Search and Data Mining (WSDM-10),pages 101?110, New York.C.
Castillo and B. Davison.
2011.
Adversarial web search.Journal of Foundations and Trends in Information Re-trieval, 4(5):377?486.J.
Dalton, L. Dietz, and J. Allan.
2014.
Entity query featureexpansion using knowledge base links.
In Proceedings ofthe 37th International Conference on Research and Devel-opment in Information Retrieval (SIGIR-14), pages 365?374, Gold Coast, Queensland, Australia.D.
Downey, M. Broadhead, and O. Etzioni.
2007.
Locatingcomplex named entities in Web text.
In Proceedings ofthe 20th International Joint Conference on Artificial Intel-ligence (IJCAI-07), pages 2733?2739, Hyderabad, India.O.
Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked,S.
Soderland, D. Weld, and A. Yates.
2005.
Unsupervisednamed-entity extraction from the Web: an experimentalstudy.
Artificial Intelligence, 165(1):91?134.O.
Etzioni, A. Fader, J. Christensen, S. Soderland, andMausam.
2011.
Open information extraction: The secondgeneration.
In Proceedings of the 22nd International JointConference on Artificial Intelligence (IJCAI-11), pages 3?10, Barcelona, Spain.A.
Fader, S. Soderland, and O. Etzioni.
2011.
Identifyingrelations for open information extraction.
In Proceedingsof the 2011 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP-11), pages 1535?1545,Edinburgh, Scotland.C.
Fellbaum, editor.
1998.
WordNet: An Electronic LexicalDatabase and Some of its Applications.
MIT Press.T.
Flati, D. Vannella, T. Pasini, and R. Navigli.
2014.
Two isbigger (and better) than one: the Wikipedia Bitaxonomyproject.
In Proceedings of the 52nd Annual Meeting ofthe Association for Computational Linguistics (ACL-14),pages 945?955, Baltimore, Maryland.M.
Gamon, T. Yano, X.
Song, J. Apacible, and P. Pantel.2013.
Identifying salient entities in web pages.
In Pro-ceedings of the 22nd International Conference on Infor-mation and Knowledge Management (CIKM-13), pages2375?2380, Burlingame, California.T.
Hasegawa, S. Sekine, and R. Grishman.
2004.
Discover-ing relations among named entities from large corpora.
InProceedings of the 42nd Annual Meeting of the Associa-tion for Computational Linguistics (ACL-04), pages 415?422, Barcelona, Spain.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of the 14thInternational Conference on Computational Linguistics(COLING-92), pages 539?545, Nantes, France.J.
Hoffart, F. Suchanek, K. Berberich, and G. Weikum.
2013.YAGO2: a spatially and temporally enhanced knowledgebase from Wikipedia.
Artificial Intelligence Journal.
Spe-cial Issue on Artificial Intelligence, Wikipedia and Semi-Structured Resources, 194:28?61.A.
Jain and M. Pennacchiotti.
2010.
Open entity extrac-tion from Web search query logs.
In Proceedings of the23rd International Conference on Computational Linguis-tics (COLING-10), pages 510?518, Beijing, China.R.
Jones, B. Rey, O. Madani, and W. Greiner.
2006.
Gener-ating query substitutions.
In Proceedings of the 15h WorldWide Web Conference (WWW-06), pages 387?396, Edin-burgh, Scotland.Z.
Kozareva and E. Hovy.
2010.
A semi-supervised methodto learn and construct taxonomies using the web.
In Pro-ceedings of the 2010 Conference on Empirical Methods inNatural Language Processing (EMNLP-10), pages 1110?1118, Cambridge, Massachusetts.Z.
Kozareva, E. Riloff, and E. Hovy.
2008.
Semanticclass learning from the Web with hyponym pattern link-age graphs.
In Proceedings of the 46th Annual Meeting ofthe Association for Computational Linguistics (ACL-08),pages 1048?1056, Columbus, Ohio.N.
Lao, T. Mitchell, and W. Cohen.
2011.
Random walk in-ference and learning in a large scale knowledge base.
InProceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP-11), pages529?539, Edinburgh, Scotland.D.
Lin and X. Wu.
2009.
Phrase clustering for discrimina-tive learning.
In Proceedings of the 47th Annual Meetingof the Association for Computational Linguistics (ACL-IJCNLP-09), pages 1030?1038, Singapore.O.
Lopez de Lacalle and M. Lapata.
2013.
Unsupervised re-lation extraction with general domain knowledge.
In Pro-ceedings of the 2013 Conference on Empirical Methods inNatural Language Processing (EMNLP-13), pages 415?425, Seattle, Washington.Mausam, M. Schmitz, S. Soderland, R. Bart, and O. Etzioni.2012.
Open language learning for information extraction.In Proceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Computa-tional Natural Language Learning (EMNLP-CoNLL-12),pages 523?534, Jeju Island, Korea.1090F.
Mesquita, J. Schmidek, and D. Barbosa.
2013.
Effective-ness and efficiency of open relation extraction.
In Pro-ceedings of the 2013 Conference on Empirical Methods inNatural Language Processing (EMNLP-13), pages 447?457, Seattle, Washington.V.
Nastase and M. Strube.
2008.
Decoding Wikipedia cat-egories for knowledge acquisition.
In Proceedings ofthe 23rd National Conference on Artificial Intelligence(AAAI-08), pages 1219?1224, Chicago, Illinois.M.
Pas?ca.
2007.
Organizing and searching the World WideWeb of facts - step two: Harnessing the wisdom of thecrowds.
In Proceedings of the 16th World Wide Web Con-ference (WWW-07), pages 101?110, Banff, Canada.M.
Pas?ca.
2014.
Acquisition of noncontiguous class at-tributes from Web search queries.
In Proceedings of the14th Conference of the European Chapter of the Asso-ciation for Computational Linguistics (EACL-14), pages386?394, Gothenburg, Sweden.P.
Pantel and A. Fuxman.
2011.
Jigs and lures: Associatingweb queries with structured entities.
In Proceedings of the49th Annual Meeting of the Association for ComputationalLinguistics (ACL-11), pages 83?92, Portland, Oregon.P.
Pantel, E. Crestan, A. Borkovsky, A. Popescu, and V. Vyas.2009.
Web-scale distributional similarity and entityset expansion.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Processing(EMNLP-09), pages 938?947, Singapore.P.
Pantel, T. Lin, and M. Gamon.
2012.
Mining entity typesfrom query logs via user intent modeling.
In Proceedingsof the 50th Annual Meeting of the Association for Compu-tational Linguistics (ACL-12), pages 563?571, Jeju Island,Korea.P.
Pasupat and P. Liang.
2014.
Zero-shot entity extractionfrom Web pages.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Linguistics(ACL-14), pages 391?401, Baltimore, Maryland.M.
Pennacchiotti and P. Pantel.
2009.
Entity extraction viaensemble semantics.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural Language Process-ing (EMNLP-09), pages 238?247, Singapore.S.
Petrov, P. Chang, M. Ringgaard, and H. Alshawi.
2010.Uptraining for accurate deterministic question parsing.
InProceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP-10), pages705?713, Cambridge, Massachusetts.S.
Raju, P. Pingali, and V. Varma.
2008.
An unsupervised ap-proach to product attribute extraction.
In Proceedings ofthe 31st International Conference on Research and Devel-opment in Information Retrieval (SIGIR-08), pages 35?42,Singapore.M.
Remy.
2002.
Wikipedia: The free encyclopedia.
OnlineInformation Review, 26(6):434.S.
Sekine and H. Suzuki.
2007.
Acquiring ontologicalknowledge from query logs.
In Proceedings of the 16thWorld Wide Web Conference (WWW-07), Posters, pages1223?1224, Banff, Canada.K.
Tokunaga, J. Kazama, and K. Torisawa.
2005.
Automaticdiscovery of attribute words from Web documents.
InProceedings of the 2nd International Joint Conference onNatural Language Processing (IJCNLP-05), pages 106?118, Jeju Island, Korea.B.
Van Durme and M. Pas?ca.
2008.
Finding cars, goddessesand enzymes: Parametrizable acquisition of labeled in-stances for open-domain information extraction.
In Pro-ceedings of the 23rd National Conference on Artificial In-telligence (AAAI-08), pages 1243?1248, Chicago, Illinois.B.
Van Durme, T. Qian, and L. Schubert.
2008.
Class-driven attribute extraction.
In Proceedings of the 22ndInternational Conference on Computational Linguistics(COLING-08), pages 921?928, Manchester, United King-dom.B.
Van Durme, P. Michalak, and L. Schubert.
2009.
Deriv-ing generalized knowledge from corpora using Wordnetabstraction.
In Proceedings of the 12th Conference of theEuropean Chapter of the Association for ComputationalLinguistics (EACL-09), pages 808?816, Athens, Greece.R.
Wang and W. Cohen.
2009.
Automatic set instance ex-traction using the Web.
In Proceedings of the 47th AnnualMeeting of the Association for Computational Linguistics(ACL-IJCNLP-09), pages 441?449, Singapore.X.
Wang and C. Zhai.
2008.
Mining term association pat-terns from search logs for effective query reformulation.In Proceedings of the 17th International Conference on In-formation and Knowledge Management (CIKM-08), pages479?488, Napa Valley, California.Z.
Wang, Z. Li, J. Li, J. Tang, and J. Pan.
2013.
Trans-fer learning based cross-lingual knowledge extraction forWikipedia.
In Proceedings of the 51st Annual Meeting ofthe Association for Computational Linguistics (ACL-13),pages 641?650, Sofia, Bulgaria.J.
Weston, A. Bordes, O. Yakhnenko, and N. Usunier.
2013.Connecting language and knowledge bases with embed-ding models for relation extraction.
In Proceedings of the2013 Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP-13), pages 1366?1371, Seat-tle, Washington.T.
Wong, W. Lam, and T. Wong.
2008.
An unsuper-vised framework for extracting and normalizing productattributes from multiple Web sites.
In Proceedings of the31st International Conference on Research and Develop-ment in Information Retrieval (SIGIR-08), pages 35?42,Singapore.F.
Wu and D. Weld.
2010.
Open information extraction usingWikipedia.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics (ACL-10),pages 118?127, Uppsala, Sweden.W.
Wu, , H. Li, H. Wang, and K. Zhu.
2012.
Probase: a prob-abilistic taxonomy for text understanding.
In Proceedingsof the 2012 International Conference on Management ofData (SIGMOD-12), pages 481?492, Scottsdale, Arizona.X.
Yao and B.
Van Durme.
2014.
Information extractionover structured data: Question Answering with Freebase.In Proceedings of the 52nd Annual Meeting of the Associ-ation for Computational Linguistics (ACL-14), pages 956?966, Baltimore, Maryland.N.
Yoshinaga and K. Torisawa.
2007.
Open-domainattribute-value acquisition from semi-structured texts.
InProceedings of the 6th International Semantic Web Con-ference (ISWC-07), Workshop on Text to Knowledge: TheLexicon/Ontology Interface (OntoLex-2007), pages 55?66, Busan, South Korea.L.
Zang, C. Cao, Y. Cao, Y. Wu, and C. Cao.
2013.
A sur-vey of commonsense knowledge acquisition.
Journal ofComputer Science and Technology, 28(4):689?719.1091
