Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 414?417,Prague, June 2007. c?2007 Association for Computational LinguisticsUOY: A Hypergraph Model For Word Sense Induction & DisambiguationIoannis P. KlapaftisUniversity of YorkDepartment of Computer Sciencegiannis@cs.york.ac.ukSuresh ManandharUniversity of YorkDepartment of Computer Sciencesuresh@cs.york.ac.ukAbstractThis paper is an outcome of ongoing re-search and presents an unsupervised methodfor automatic word sense induction (WSI)and disambiguation (WSD).
The inductionalgorithm is based on modeling the co-occurrences of two or more words usinghypergraphs.
WSI takes place by detect-ing high-density components in the co-occurrence hypergraphs.
WSD assigns toeach induced cluster a score equal to the sumof weights of its hyperedges found in the lo-cal context of the target word.
Our systemparticipates in SemEval-2007 word sense in-duction and discrimination task.1 IntroductionThe majority of both supervised and unsupervisedapproaches to WSD is based on the ?fixed-list?
ofsenses paradigm where the senses of a target wordis a closed list of definitions coming from a stan-dard dictionary (Agirre et al, 2006).
Lexicographershave long warned about the problems of such an ap-proach, since dictionaries are not suited to this task;they often contain general definitions, they sufferfrom the lack of explicit semantic and topical rela-tions or interconnections, and they often do not re-flect the exact content of the context, in which thetarget word appears (Veronis, 2004).To overcome this limitation, unsupervised WSDhas moved towards inducing the senses of a targetword directly from a corpus, and then disambiguat-ing each instance of it.
Most of the work in WSIis based on the vector space model, where the con-text of each instance of a target word is representedas a vector of features (e.g second-order word co-occurrences) (Schutze, 1998; Purandare and Peder-sen, 2004).
These vectors are clustered and the re-sulting clusters represent the induced senses.
How-ever, as shown experimentally in (Veronis, 2004),vector-based techniques are unable to detect low-frequency senses of a target word.Recently, graph-based methods were employed inWSI to isolate highly infrequent senses of a targetword.
HyperLex (Veronis, 2004) and the adaptationof PageRank (Brin and Page, 1998) in (Agirre et al,2006) have been shown to outperform the most fre-quent sense (MFS) baseline in terms of supervisedrecall, but they still fall short of supervised WSDsystems.Graph-based approaches operate on a 2-dimensional space, assuming a one-to-one relation-ship between co-occurring words.
However, thisassumption is insufficient, taking into account thefact that two or more words are usually combinedto form a relationship of concepts in the context.Additionally, graph-based approaches fail to modeland exploit the existence of collocations or termsconsisting of more than two words.This paper proposes a method for WSI, whichis based on a hypergraph model operating ona n-dimensional space.
In such a model, co-occurrences of two or more words are representedusing weighted hyperedges.
A hyperedge is a moreexpressive representation than a simple edge, be-cause it is able to capture the information sharedby two or more words.
Our system participates in414SemEval-2007 word sense induction and discrimi-nation task (SWSID) (Agirre and Soroa, 2007).2 Sense Induction & DisambiguationThis section presents the induction and disambigua-tion algorithms.2.1 Sense Induction2.1.1 The Hypergraph ModelA hypergraph H = (V, F ) is a generalization ofa graph, which consists of a set of vertices V and aset of hyperedges F ; each hyperedge is a subset ofvertices.
While an edge relates 2 vertices, a hyper-edge relates n vertices (where n ?
1).
In our prob-lem, we represent each word by a vertex and anyset of co-occurring related words by a hyperedge.In our approach, we restrict hyperedges to 2, 3 or4 words.
Figure 1 shows an example of an abstracthypergraph model 1.Figure 1: An example of a HypergraphThe degree of a vertex is the number of hyper-edges it belongs to, and the degree of a hyperedge isthe number of vertices it contains.
A path in the hy-pergraph model is a sequence of vertices and hyper-edges such as v1, f1, ..., vi?1, fi?1, vi, where vk arevertices, fk are hyperedges, each hyperedge fk con-tains vertices to its left and right in the path and nohyperedge or vertex is repeated.
The length of a pathis the number of hyperedges it contains, the distancebetween two vertices is the shortest path betweenthem and the distance between two hyperedges is theminimum distance of all the pairs of their vertices.2.1.2 Building The HypergraphLet bp be the base corpus from which we inducethe senses of a target word tw.
Our bp consists ofBNC and all the SWSID paragraphs containing the1Image was taken from Wikipedia (Rocchini, 2006)target word.
The total size of bp is 2000 paragraphs.Note that if SWSID paragraphs of tw are more than2000, BNC is not used.In order to build the hypergraph, tw is removedfrom bp and each paragraph pi is POS-tagged.
Fol-lowing the example in (Agirre et al, 2006), onlynouns are kept and lemmatised.
We apply two fil-tering heuristics.
The first one is the minimum fre-quency of nouns (parameter p1), and the second oneis the minimum size of a paragraph (parameter p2).A key problem at this stage is the determination ofrelated vertices (nouns), which can be grouped intohyperedges and the weighting of each such hyper-edge.
We deal with this problem by using associa-tion rules (Agrawal and Srikant, 1994).
Frequent hy-peredges are detected by calculating support, whichshould exceed a user-defined threshold (parameterp3).Let f be a candidate hyperedge and a, b, c its ver-tices.
Then freq(a, b, c) is the number of para-graphs in bp, which contain all the vertices of f , andn is the total size of bp.
Support of f is shown inEquation 1.support(f) =freq(a, b, c)n(1)The weight assigned to each collected hyperedge,f , is the average of m calculated confidences, wherem is the size of f .
Let f be a hyperedge containingthe vertices a, b, c. The confidence for the rule r0 ={a, b} => {c} is defined in Equation 2.confidence(r0) =freq(a, b, c)freq(a, b)(2)Since there is a three-way relationship among a, band c, we have two more rules r1 = {a, c} => {b}and r2 = {b, c} => {a}.
Hence, the weighting off is the average of the 3 calculated confidences.
Weapply a filtering heuristic (parameter p4) to removehyperedges with low weights from the hypergraph.At the end of this stage, the constructed hypergraphis reduced, so that our hypergraph model agrees withthe one described in subsection 2.1.1.2.1.3 Extracting SensesPreliminary experiments on 10 nouns ofSensEval-3 English lexical-sample task (Mihalceaet al, 2004) (S3LS), suggested that our hypergraphs415are small-world networks, since they exhibiteda high clustering coefficient and a small averagepath length.
Furthermore, the frequency of verticeswith a given degree plotted against the degreeshowed that our hypergraphs satisfy a power-lawdistribution P (d) = c ?
d?
?, where d is the vertexdegree, P (d) is the frequency of vertices withdegree d. Figure 2 shows the log-log plot for thenoun difference of S3LS.Figure 2: Log-log plot for the noun difference.In order to extract the senses of the target word,we modify the HyperLex algorithm (Veronis, 2004)for selecting the root hubs of the hypergraph as fol-lows.
At each step, the algorithm finds the vertex viwith the highest degree, which is selected as a roothub, according to two criteria.The first one is the minimum number of hyper-edges it belongs to (parameter p5), and the second isthe average weight of the first p5 hyperedges (para-meter p6) 2.
If these criteria are satisfied, then hyper-edges containing vi are grouped to a single cluster cj(new sense) with a 0 distance from vi, and removedfrom the hypergraph.
The process stops, when thereis no vertex eligible to be a root hub.Each remaining hyperedge, fk, is assigned to thecluster, cj , closest to it, by calculating the minimumdistance between fk and each hyperedge of cj as de-fined in subsection 2.1.1.
The weight assigned to fkis inversely proportional to its distance from cj .2.2 Word Sense DisambiguationGiven an instance of the target word, tw, paragraphpi containing tw is POS-tagged, nouns are kept and2Hyperedges are sorted in decreasing order of weightlemmatised.
Next, each induced cluster cj is as-signed a score equal to the sum of weights of itshyperedges found in pi.3 Evaluation3.1 Preliminary ExperimentsThis method is an outcome of ongoing research.Due to time restrictions we were able to test andtune (Table 1), but not optimize, our system only ona very small set of nouns of S3LS targeting at a highsupervised recall.
Our supervised recall on the 10first nouns of S3LS was 66.8%, 9.8% points abovethe MFS baseline.Parameter Valuep1:Minimum frequency of a noun 8p2:Minimum size of a paragraph 4p3:Support threshold 0.002p4:Average confidence threshold 0.2p5:Minimum number of hyperedges 6p6:Minimum average weight of hyperedges 0.25Table 1: Chosen parameters for our system3.2 SemEval-2007 ResultsTables 2 and 3 show the average supervised recall,FScore, entropy and purity of our system on nounsand verbs of the test data respectively.
The submit-ted answer consisted only of the winning cluster perinstance of a target word, in effect assigning it withweight 1 (default).Entropy measures how well the various gold stan-dard senses are distributed within each cluster, whilepurity measures how pure a cluster is, containing ob-jects from primarily one class.
In general, the lowerthe entropy and the larger the purity values, the bet-ter the clustering algorithm performs.Measure Proposed methodology MFSEntropy 25.5 46.3Purity 89.8 82.4FScore 65.8 80.7Sup.
Recall 81.6 80.9Table 2: System performance for nouns.For nouns our system achieves a low entropy anda high purity outperforming the MFS baseline, but alower FScore.
This can be explained by the fact thatthe average number of clusters we produce for nounsis 11, while the gold standard average of senses isaround 2.8.
For verbs the performance of our system416is worse than for nouns, although entropy and puritystill outperform the MFS baseline.
FScore is verylow, despite that the average number of clusters weproduce for verbs (around 8) is less than the numberof clusters we produce for nouns.
This means thatfor verbs the senses of gold standard are much morespread among induced clusters than for nouns, caus-ing a low unsupervised recall.
Overall, FScore re-sults are in accordance with the idea of microsensesmentioned in (Agirre et al, 2006).
FScore is biasedtowards clusters similar to the gold standard sensesand cannot capture that theory.Measure Proposed methodology MFSEntropy 28.9 44.4Purity 82.0 77F-score 45.1 76.8Sup.
Recall 73.3 76.2Table 3: System performance for verbs.Our supervised recall for verbs is 73.3%, and be-low the MFS baseline (76.2%), which no systemmanaged to outperform.
For nouns our supervisedrecall is 81.6%, which is around 0.7% above theMFS baseline.
In order to fully examine the perfor-mance of our system we applied a second evaluationof our methodology using the SWSID official soft-ware.The solution per target word instance included theentire set of clusters with their associated weights(Table 4).
Results show that the submitted answer(instance - winning cluster), was degrading seri-ously our performance both for verbs and nouns dueto the loss of information in the mapping step.POS Proposed Methodology MFSNouns 84.3 80.9Verbs 75.6 76.2Total 80.2 78.7Table 4: Supervised recall in second evaluation.Our supervised recall for nouns has outperformedthe MFS baseline by 3.4% with the best systemachieving 86.8%.
Performance for verbs is 75.6%,0.6% below the best system and MFS.4 ConclusionWe have presented a hypergraph model for wordsense induction and disambiguation.
Preliminaryexperiments suggested that our reduced hypergraphsare small-world networks.
WSI identifies the highlyconnected components (hubs) in the hypergraph,while WSD assigns to each cluster a score equal tothe sum of weights of its hyperedges found in thelocal context of a target word.Results show that our system achieves high en-tropy and purity performance outperforming theMFS baseline.
Our methodology achieves a lowFScore producing clusters that are dissimilar to thegold standard senses.
Our supervised recall fornouns is 3.4% above the MFS baseline.
For verbs,our supervised recall is below the MFS baseline,which no system managed to outperform.ReferencesEneko Agirre and Aitor Soroa.
2007.
Semeval-2007 task2: Evaluating word sense induction and discriminationsystems.
In Proceedings of SemEval-2007.
ACL.Eneko Agirre, David Mart?
?nez, Oier Lo?pez de Lacalle,and Aitor Soroa.
2006.
Two graph-based algorithmsfor state-of-the-art wsd.
In Proceedings of the EMNLPConference, pages 585?593.
ACL.Rakesh Agrawal and Ramakrishnan Srikant.
1994.
Fastalgorithms for mining association rules in large data-bases.
In VLDB ?94: Proceedings of the 20th Inter-national Conference on Very Large DataBases, pages487?499, USA.
Morgan Kaufmann Publishers Inc.Sergey Brin and Lawrence Page.
1998.
The anatomy ofa large-scale hypertextual Web search engine.
Com-puter Networks and ISDN Systems, 30(1?7):107?117.Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-riff.
2004.
The senseval-3 english lexical sample task.In R. Mihaleca and P. Edmonds, editors, SensEval-3Proceedings, pages 25?28, Spain, July.
ACL.Amruta Purandare and Ted Pedersen.
2004.
Word sensediscrimination by clustering contexts in vector andsimilarity spaces.
In Proceedings of CoNLL-2004,pages 41?48.
ACL.Claudio Rocchini.
2006.
Hypergraph sample image.Wikipedia.Hinrich Schutze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?123.Jean Veronis.
2004.
Hyperlex:lexical cartography forinformation retrieval.
Computer Speech & Language,18(3).417
