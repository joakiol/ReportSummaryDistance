Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 5?12,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Semi-Automatic Method forAnnotating a Biomedical Proposition BankWen-Chi Chou1, Richard Tzong-Han Tsai1,2, Ying-Shan Su1,Wei Ku1,3, Ting-Yi Sung1 and Wen-Lian Hsu11Institute of Information Science, Academia Sinica, Taiwan, ROC.2Dept.
of Computer Science and Information Engineering, National Taiwan University, Taiwan, ROC.3Institute of Molecular Medicine, National Taiwan University, Taiwan, ROC.
{jacky957,thtsai,qnn,wilmaku,tsung,hsu}@iis.sinica.edu.twAbstractIn this paper, we present a semi-automatic approach for annotating se-mantic information in biomedical texts.The information is used to constructa biomedical proposition bank calledBioProp.
Like PropBank in the newswiredomain, BioProp contains annotations ofpredicate argument structures and seman-tic roles in a treebank schema.
To con-struct BioProp, a semantic role labeling(SRL) system trained on PropBank isused to annotate BioProp.
Incorrect tag-ging results are then corrected by humanannotators.
To suit the needs in the bio-medical domain, we modify the Prop-Bank annotation guidelines and charac-terize semantic roles as components ofbiological events.
The method can sub-stantially reduce annotation efforts, andwe introduce a measure of an upperbound for the saving of annotation efforts.Thus far, the method has been appliedexperimentally to a 4,389-sentence tree-bank corpus for the construction of Bio-Prop.
Inter-annotator agreement meas-ured by kappa statistic reaches .95 forcombined decision of role identificationand classification when all argument la-bels are considered.
In addition, we showthat, when trained on BioProp, our bio-medical SRL system called BIOSMILEachieves an F-score of 87%.1 IntroductionThe volume of biomedical literature available onthe Web has grown enormously in recent years, atrend that will probably continue indefinitely.Thus, the ability to process literature automati-cally would be invaluable for both the design andinterpretation of large-scale experiments.
To thisend, several information extraction (IE) systemsusing natural language processing techniqueshave been developed for use in the biomedicalfield.
Currently, the focus of IE is shifting fromthe extraction of nominal information, such asnamed entities (NEs) to verbal information thatrepresents the relations between NEs, e.g., eventsand function (Tateisi et al, 2004; Wattarujeekritet al, 2004).
In the IE of relations, the roles ofNEs participating in a relation must be identifiedalong with a verb of interest.
This task involvesidentifying main roles, such as agents and objects,and adjunct roles (ArgM), such as location, man-ner, timing, condition, and extent.
This identifi-cation task is called semantic role labeling (SRL).The corresponding roles of the verb (predicate)are called predicate arguments, and the wholeproposition is known as a predicate argumentstructure (PAS).To develop an automatic SRL system for thebiomedical domain, it is necessary to train thesystem with an annotated corpus, called proposi-tion bank (Palmer et al, 2005).
This corpus con-tains annotations of semantic PAS?s superim-posed on the Penn Treebank (PTB) (Marcus etal., 1993; Marcus et al, 1994).
However, theprocess of manually annotating the PAS?s toconstruct a proposition bank is quite time-consuming.
In addition, due to the complexity ofproposition bank annotation, inconsistent annota-tion may occur frequently and further complicate5the annotation task.
In spite of the above difficul-ties, there are proposition banks in the newswiredomain that are adequate for training SRL sys-tems (Xue and Palmer, 2004; Palmer et al, 2005).In addition, according to the CoNLL-2005shared task (Carreras and M?rquez, 2005), theperformance of SRL systems in general does notdecline significantly when tagging out-of-domaincorpora.
For example, when SRL systems trainedon the Wall Street Journal (WSJ) corpus wereused to tag the Brown corpus, the performanceonly dropped by 15%, on average.
In comparisonto annotating from scratch, annotation effortsbased on the results of an available SRL systemare much reduced.
Thus, we plan to use a news-wire SRL system to tag a biomedical corpus andthen manually revise the tagging results.
Thissemi-automatic procedure could expedite theconstruction of a biomedical proposition bank foruse in training a biomedical SRL system in thefuture.2 The Biomedical Proposition Bank -BioPropAs proposition banks are semantically annotatedversions of a Penn-style treebank, they provideconsistent semantic role labels across differentsyntactic realizations of the same verb.
The an-notation captures predicate-argument structuresbased on the sense tags of polysemous verbs(called framesets) and semantic role labels foreach argument of the verb.
Figure 1 shows theannotation of semantic roles, exemplified by thefollowing sentence: ?IL4 and IL13 receptors ac-tivate STAT6, STAT3 and STAT5 proteins innormal human B cells.?
The chosen predicate isthe word ?activate?
; its arguments and their as-sociated word groups are illustrated in the figure.IL4 and IL 13receptorsactivate STAT6, STAT3andSTAT5 proteinsthe humanB cellsinNPArg0 predicate AM-LOC Arg1NPNP-SBJ VPVP PPFigure 1.
A treebank annotated with semanticrole labelsSince proposition banks are annotated on topof a Penn-style treebank, we selected a biomedi-cal corpus that has a Penn-style treebank as ourcorpus.
We chose the GENIA corpus (Kim et al,2003), a collection of MEDLINE abstracts se-lected from the search results with the followingkeywords: human, blood cells, and transcriptionfactors.
In the GENIA corpus, the abstracts areencoded in XML format, where each abstractalso contains a MEDLINE UID, and the title andcontent of the abstract.
The text of the title andcontent is segmented into sentences, in whichbiological terms are annotated with their seman-tic classes.
The GENIA corpus is also annotatedwith part-of-speech (POS) tags (Tateisi and Tsu-jii, 2004), and co-references are added to part ofthe GENIA corpus by the MedCo project at theInstitute for Infocomm Research, Singapore(Yang et al, 2004).The Penn-style treebank for GENIA, createdby Tateisi et al (2005), currently contains 500abstracts.
The annotation scheme of the GENIATreebank (GTB), which basically follows thePenn Treebank II (PTB) scheme (Bies et al,1995), is encoded in XML.
However, in contrastto the WSJ corpus, GENIA lacks a propositionbank.
We therefore use its 500 abstracts withGTB as our corpus.
To develop our biomedicalproposition bank, BioProp, we add the proposi-tion bank annotation on top of the GTB annota-tion.In the following, we report on the selection ofbiomedical verbs, and explain the difference be-tween their meaning in PropBank (Palmer et al,2005), developed by the University of Pennsyl-vania, and their meaning in BioProp (a biomedi-cal proposition bank).
We then introduce Bio-Prop?s annotation scheme, including how wemodify a verb?s framesets and how we defineframesets for biomedical verbs not defined inVerbNet (Kipper et al, 2000; Kipper et al, 2002).2.1 Selection of Biomedical VerbsWe selected 30 verbs according to their fre-quency of use or importance in biomedical texts.Since our targets in IE are the relations of NEs,only sentences containing protein or gene namesare used to count each verb?s frequency.
Verbsthat have general usage are filtered out in orderto ensure the focus is on biomedical verbs.
Someverbs that do not have a high frequency, but playimportant roles in describing biomedical rela-tions, such as ?phosphorylate?
and ?transacti-vate?, are also selected.
The selected verbs arelisted in Table 1.6Predicate Frameset Exampleexpress(VerbNet)Arg0: agentArg1: themeArg2: recipient or destina-tion[Some legislatorsArg0][expressedpredicate] [concern that a gas-taxincrease would take too long and possibly damage chances of amajor gas-tax-increasing ballot initiative that voters will considernext JuneArg1 ].translate(VerbNet)Arg0: causer of transfor-mationArg1: thing changingArg2: end stateArg3: start stateBut some cosmetics-industry executives wonder whether [tech-niques honed in packaged goodsArg1] [willAM-MOD] [translatepredicate][to the cosmetics businessArg2].express(BioProp)Arg0: causer of expressionArg1: thing expressing[B lymphocytes and macrophagesArg0] [expresspredicate] [closelyrelated immunoglobulin G ( IgG ) Fc receptors ( Fc gamma RII )that differ only in the structures of their cytoplasmic domainsArg1].Table 2.
Framesets and examples of ?express?
and ?translate?Type Verb list1 encode, interact, phosphorylate,  transactivate2 express, modulate3 bind4activate, affect, alter, associate, block,decrease differentiate, encode, enhance,increase, induce, inhibit, mediate, mu-tate, prevent, promote, reduce, regulate,repress, signal, stimulate, suppress,transform, triggerTable 1.
Selected biomedical verbs and theirtypes2.2 Framesets of Biomedical VerbsAnnotation of BioProp is mainly based onLevin?s verb classes, as defined in the VerbNetlexicon (Kipper et al, 2000).
In VerbNet, thearguments of each verb are represented at thesemantic level, and thus have associated seman-tic roles.
However, since some verbs may havedifferent usages in biomedical and newswiretexts, it is necessary to customize the framesetsof biomedical verbs.
The 30 verbs in Table 1 arecategorized into four types according to the de-gree of difference in usage: (1) verbs that do notappear in VerbNet due to their low frequency inthe newswire domain; (2) verbs that do appear inVerbNet, but whose biomedical meanings andframesets are undefined; (3) verbs that do appearin VerbNet, but whose primary newswire andbiomedical usage differ; (4) verbs that have thesame usage in both domains.Verbs of the first type play important roles inbiomedical texts, but rarely appear in newswiretexts and thus are not defined in VerbNet.
Forexample, ?phosphorylate?
increasingly appearsin the fast-growing PubMed abstracts that reportexperimental results on phosphorylated events;therefore, it is included in our verb list.
However,since VerbNet does not define the frameset for?phosphorylate?, we must define it after analyz-ing all the sentences in our corpus that containthe verb.
Other type 1 verbs may correspond toverbs in VerbNet; in such cases, we can borrowthe VerbNet definitions and framesets.
For ex-ample, ?transactivate?
is not found in VerbNet,but we can adopt the frameset of ?activate?
forthis verb.Verbs of the second type appear in VerbNet,but have unique biomedical meanings that areundefined.
Therefore, the framesets correspond-ing to their biomedical meanings must be added.In most cases, we can adopt framesets fromVerbNet synonyms.
For example, ?express?
isdefined as ?say?
and ?send very quickly?
inVerbNet.
However, in the biomedical domain, itsusage is very similar to ?translate?.
Thus, we canuse the frameset of ?translate?
for ?express?.
Ta-ble 2 shows the framesets and corresponding ex-amples of ?express?
in the newswire domain andbiomedical domain, as well as that of ?translate?in VerbNet.Verbs of the third type also appear in VerbNet.Although the newswire and biological senses aredefined therein, their primary newswire sense isnot the same as their primary biomedical sense.?Bind,?
for example, is common in the newswiredomain, and it usually means ?to tie?
or ?restrainwith bonds.?
However, in the biomedical domain,its intransitive use- ?attach or stick to?- is farmore common.
For example, a Google search forthe phrase ?glue binds to?
only returned 21 re-sults, while the same search replacing ?glue?with ?protein?
yields 197,000 hits.
For suchverbs, we only need select the appropriate alter-native meanings and corresponding framesets.Lastly, for verbs of the fourth type, we can di-7rectly adopt the newswire definitions and frame-sets, since they are identical.2.3 Distribution of Selected VerbsThere is a significant difference between the oc-currence of the 30 selected verbs in biomedicaltexts and their occurrence in newswire texts.
Theverbs appearing in verb phrases constitute only1,297 PAS?s, i.e., 1% of all PAS?s, in PropBank(shown in Figure 2), compared to 2,382 PAS?s,i.e., 16% of all PAS?s, in BioProp (shown inFigure 3).
Furthermore, some biomedical verbshave very few PAS?s in PropBank, as shown inTable 3.
The above observations indicate that itis necessary to annotate a biomedical propositionbank for training a biomedical SRL system.Figure 2.
The percentage of the 30 biomedicalverbs and other verbs in PropBankFigure 3.
The percentage of the 30 biomedicalverbs and other verbs in BioProp3  Annotation of BioProp3.1 Annotation ProcessAfter choosing 30 verbs as predicates, weadopted a semi-automatic method to annotateBioProp.
The annotation process consists of thefollowing steps: (1) identifying predicate candi-dates; (2) automatically annotating the biomedi-cal semantic roles with our WSJ SRL system; (3)transforming the automatic tagging results intoWordFreak (Morton and LaCivita, 2003) format;and (4) manually correcting the annotation re-sults with the WordFreak annotation tool.
Wenow describe these steps in detail:Verbs # in BioProp Ratio(%)# inPropBank Ratio(%)induce 290 1.89 16 0.01bind 252 1.64 0 0activate 235 1.53 2 0express 194 1.26 53 0.03inhibit 184 1.20 6 0increase 166 1.08 396 0.24regulate 122 0.79 23 0.01mediate 104 0.68 1 0stimulate 93 0.61 11 0.01associate 82 0.53 51 0.03encode 79 0.51 0 0affect 60 0.39 119 0.07enhance 60 0.39 28 0.02block 58 0.38 71 0.04reduce 55 0.36 241 0.14decrease 54 0.35 16 0.01suppress 38 0.25 4 0interact 36 0.23 0 0alter 27 0.18 17 0.01transactivate 24 0.16 0 0modulate 22 0.14 1 0phosphorylate 21 0.14 0 0transform 21 0.14 22 0.01differentiate 21 0.14 2 0repress 17 0.11 1 0prevent 15 0.10 92 0.05promote 14 0.09 52 0.03trigger 14 0.09 40 0.02mutate 14 0.09 1 0signal 10 0.07 31 0.02Table 3.
The number and percentage of PAS?sfor each verb in BioProp and PropBank1.
Each word with a VB POS tag in a verbphrase that matches any lexical variant ofthe 30 verbs is treated as a predicate candi-date.
The automatically selected targets arethen double-checked by human annotators.As a result, 2,382 predicates were identifiedin BioProp.2.
Sentences containing the above 2,382predicates were extracted and labeledautomatically by our WSJ SRL system.
Intotal, 7,764 arguments were identified.3.
In this step, sentences with PAS annota-tions are transformed into WordFreak for-mat (an XML format), which allows anno-tators to view a sentence in a tree-like fash-ion.
In addition, users can customize the tagset of arguments.
Other linguistic informa-tion can also be integrated and displayed in8WordFreak, which is a convenient annota-tion tool.4.
In the last step, annotators check the pre-dicted semantic roles using WordFreak andthen correct or add semantic roles if thepredicted arguments are incorrect or miss-ing, respectively.
Three biologists with suf-ficient biological knowledge in our labora-tory performed the annotation task after re-ceiving computational linguistic trainingfor approximately three months.Figure 4 illustrates an example of BioProp an-notation displayed in WordFreak format, usingthe frameset of ?phophorylate?
listed in Table 4.This annotation process can be used to con-struct a domain-specific corpus when a general-purpose tagging system is available.
In our ex-perience, this semi-automatic annotation schemesaves annotation efforts and improves the anno-tation consistency.Predicate FramesetphosphorylateArg0: causer of phosphorylationArg1: thing being phosphorylatedArg2: end stateArg3: start stateTable 4.
The frameset of ?phosphorylate?3.2 Inter-annotation AgreementWe conducted preliminary consistency tests on2,382 instances of biomedical propositions.
Theinter-annotation agreement was measured by thekappa statistic (Siegel and Castellan, 1988), thedefinition of which is based on the probability ofinter-annotation agreement, denoted by P(A), andthe agreement expected by chance, denoted byP(E).
The kappa statistics for inter-annotationagreement were .94 for semantic role identifica-tion and .95 for semantic role classification whenArgM labels were included for evaluation.
WhenArgM labels were omitted, kappa statisticswere .94 and .98 for identification and classifica-tion, respectively.
We also calculated the resultsof combined decisions, i.e., identification andclassification.
(See Table 5.
)3.3 Annotation EffortsSince we employ a WSJ SRL system that labelssemantic roles automatically, human annotatorscan quickly browse and determine correct tag-ging results; thus, they do not have to examineFigure 4.
An example of BioProp displayed withWordFreakP(A) P(E) Kappascorerole identification .97 .52 .94role classification .96 .18 .95 including ArgMcombined decision .96 .18 .95role identification .97 .26 .94role classification .99 .28 .98 excluding ArgMcombined decision .99 .28 .98Table 5.
Inter-annotator agreementall tags during the annotation process, as in thefull manual annotation approach.
Only incor-rectly predicted tags need to be modified, andmissed tags need to be added.
Therefore, annota-tion efforts can be substantially reduced.
Toquantify the reduction in annotation efforts, wedefine the saving of annotation effort, ?, as:)1(nodes missed of# incorrect  of # correct  of #nodes  labeled correctly  of #nodes all of#nodes  labeled correctly  of #++<=?In Equation (1), since the number of nodesthat need to be examined is usually unknown, we9use an easy approximation to obtain an upperbound for ?.
This is based on the extremely op-timistic assumption that annotators should beable to recover a missed or incorrect label byonly checking one node.
However, in reality, thiswould be impossible.
In our annotation process,the upper bound of ?
for BioProp is given by:%4640975189321531666821893218932==++<?
,which means that, at most, the annotation effortcould be reduced by 46%.A more accurate tagging system is preferredbecause the more accurate the tagging system,the higher the upper bound ?
will be.4 Disambiguation of Argument Annota-tionDuring the annotation process, we encountered anumber of problems resulting from different us-age of vocabulary and writing styles in generalEnglish and the biomedical domain.
In this sec-tion, we describe three major problems and pro-pose our solutions.4.1 Cue Words for Role ClassificationPropBank annotation guidelines provide a list ofwords that can help annotators decide an argu-ment?s type.
Similarly, we add some rules to ourBioProp annotation guideline.
For example, ?invivo?
and ?in vitro?
are used frequently in bio-medical literature; however, they seldom appearin general English articles.
According to theirmeanings, we classify them as location argument(AM-LOC).In addition, some words occur frequently inboth general English and in biomedical domainsbut have different meanings/usages.
For instance,?development?
is often tagged as Arg0 or Arg1in general English, as shown by the followingsentence:Despite the strong case for stocks, however, mostpros warn that [individualsArg0] shouldn't try to[profitpredicate] [from short-term developmentsArg1].However, in the biomedical domain, ?devel-opment?
always means the stage of a disease,cell, etc.
Therefore, we tag it as temporal argu-ment (AM-TMP), as shown in the following sen-tence:[Rhom-2 mRNAArg1] is [expressedpredicate] [inearly mouse developmentAM-TMP] [in centralnervous system, lung, kidney, liver, and spleenbut only very low levels occur in thymusAM-LOC].4.2 Additional Argument TypesIn PropBank, the negative argument (AM-NEG)usually contains explicit negative words such as?not?.
However, in the biomedical domain, re-searchers usually express negative meaning im-plicitly by using ?fail?, ?unable?, ?inability?,?neither?, ?nor?, ?failure?, etc.
Take ?fail?
as anexample.
It is tagged as a verb in general English,as shown in the following sentence:But [the new pactArg1] will force huge debt on thenew firm and [couldAM-MOD] [stillAM-TMP] [failpredi-cate] [to thwart rival suitor McCaw CellularArg2].Negative results are important in the biomedi-cal domain.
Thus, for annotation purposes, wecreate additional negation tag (AM-NEG1) thatdoes not exist in PropBank.
The following sen-tence is an example showing the use of AM-NEG1:[TheyArg0] [failAM-NEG1] to [inducepredicate] [mRNAof TNF-alphaArg1] [after 3 h of culture AM-TMP].In this example, if we do not introduce theAM-NEG1, ?fail?
is considered as a verb like inPropBank, not as a negative argument, and it willnot be included in the proposition for the predi-cate ?induce?.
Thus, BioProp requires the ?AM-NEG1?
tag to precisely express the correspond-ing proposition.4.3 Essentiality of Biomedical KnowledgeSince PAS?s contain more semantic information,proposition bank annotators require more domainknowledge than annotators of other corpora.
InBioProp, many ambiguous expressions requirebiomedical knowledge to correctly annotate them,as exemplified by the following sentence in Bio-Prop:In the cell types tested, the LS mutations indi-cated an apparent requirement not only for theintact NF-kappa B and SP1-binding sites but alsofor [several regions between -201 and -130Arg1][notAM-NEG] [previouslyAM-MNR] [associatedpredi-cate][with viral infectivityArg2].Annotators without biomedical knowledgemay consider [between -201 and -130] as extentargument (AM-EXT), because the PropBankguidelines define numerical adjuncts as AM-10EXT.
However, it means a segment of DNA.
It isan appositive of [several regions]; therefore, itshould be annotated as part of Arg1 in this case.5 Effect of Training Corpora on SRLSystemsTo examine the possibility that BioProp can im-prove the training of SRL systems used forautomatic tagging of biomedical texts, we com-pare the performance of systems trained on Bio-Prop and PropBank in different domains.
Weconstruct a new SRL system (called a BIOmedi-cal SeMantIc roLe labEler, BIOSMILE) that istrained on BioProp and employs all the featuresused in our WSJ SRL system (Tsai et al, 2006).As with POS tagging, chunking, and namedentity recognition, SRL can also be formulated asa sentence tagging problem.
A sentence can berepresented by a sequence of words, a sequenceof phrases, or a parsing tree; the basic units of asentence in these representations are words,phrases, and constituents, respectively.
Haciogluet al (2004) showed that tagging phrase-by-phrase (P-by-P) is better than word-by-word (W-by-W).
However, Punyakanok et al (2004)showed that constituent-by-constituent (C-by-C)tagging is better than P-by-P.
Therefore, we useC-by-C tagging for SRL in our BIOSMILE.SRL can be divided into two steps.
First, weidentify all the predicates.
This can be easily ac-complished by finding all instances of verbs ofinterest and checking their part-of-speech (POS)tags.
Second, we label all arguments correspond-ing to each predicate.
This is a difficult problem,since the number of arguments and their posi-tions vary according to a verb?s voice (ac-tive/passive) and sense, along with many otherfactors.In BIOSMILE, we employ the maximum en-tropy (ME) model for argument classification.We use Zhang?s MaxEnt toolkit(http://www.nlplab.cn/zhangle/maxent_toolkit.html) and the L-BFGS (Nocedal and Wright, 1999)method of parameter estimation for our MEmodel.
Table 6 shows the features we employ inBIOSMILE and our WSJ SRL system.To compare the effects of using biomedicaltraining data versus using general English data,we train BIOSMILE on 30 randomly selectedtraining sets from BioProp (g1,.., g30), and WSJSRL system on 30 from PropBank (w1,.., w30),each of which has 1,200 training PAS?s.BASIC FEATURESz Predicate ?
The predicate lemmaz Path ?
The syntactic path through the parsing treefrom the parse constituent being classified to thepredicatez Constituent typez Position ?
Whether the phrase is located before or af-ter the predicatez Voice ?
passive: If the predicate has a POS tag VBN,and its chunk is not a VP, or it is preceded by a formof ?to be?
or ?to get?
within its chunk; otherwise, it isactivez Head word ?
Calculated using the head word tabledescribed by Collins (1999)z Head POS ?
The POS of the Head Wordz Sub-categorization ?
The phrase structure rule thatexpands the predicate?s parent node in the parsingtreez First and last Word and their POS tagsz Level ?
The level in the parsing treePREDICATE FEATURESz Predicate?s verb classz Predicate POS tagz Predicate frequencyz Predicate?s context POSz Number of predicatesFULL PARSING FEATURESz Parent?s, left sibling?s, and right sibling?s paths,constituent types, positions, head words and headPOS tagsz Head of PP parent ?
If the parent is a PP, then thehead of this PP is also used as a featureCOMBINATION FEATURESz Predicate distance combinationz Predicate phrase type combinationz Head word and predicate combinationz Voice position combinationOTHERSz Syntactic frame of predicate/NPz Headword suffixes of lengths 2, 3, and 4z Number of words in the phrasez Context words & POS tagsTable 6.
The features used in our argument clas-sification modelWe then test both systems on 30 400-PAS testsets from BioProp, with g1 and w1 being tested ontest set 1, g2 and w2 on set 2, and so on.
Then wegenerate the scores for g1-g30 and w1-w30, andcompare their averages.Table 7 shows the experimental results.
Whentested on the biomedical corpus, BIOSMILE out-performs the WSJ SRL system by 22.9%.
Thisresult is statistically significant as expected.Training Test Precision Recall F-scorePropBank BioProp 74.78 56.25 64.20BioProp BioProp 88.65 85.61 87.10Table 7.
Performance comparison of SRL sys-tems trained on BioProp and PropBank116 Conclusion & Future WorkThe primary contribution of this study is the an-notation of a biomedical proposition bank thatincorporates the following features.
First, thechoice of 30 representative biomedical verbs ismade according to their frequency and impor-tance in the biomedical domain.
Second, sincesome of the verbs have different usages and oth-ers do not appear in the WSJ proposition bank,we redefine their framesets and add some newargument types.
Third, the annotation guidelinesin PropBank are slightly modified to suit theneeds of the biomedical domain.
Fourth, usingappropriate argument types, framesets and anno-tation guidelines, we construct a biomedicalproposition bank, BioProp, on top of the popularbiomedical GENIA Treebank.
Finally, we em-ploy a semi-automatic annotation approach thatuses an SRL system trained on the WSJ Prop-Bank.
Incorrect tagging results are then correctedby human annotators.
This approach reduces an-notation efforts significantly.
For example, inBioProp, the annotation efforts can be reducedby, at most, 46%.
In addition, trained on BioProp,BIOSMILE?s F-score increases by 22.9% com-pared to the SRL system trained on the PropBank.In our future work, we will investigate morebiomedical verbs.
Besides, since there are fewbiomedical treebanks, we plan to integrate fullparsers in order to annotate syntactic and seman-tic information simultaneously.
It will then bepossible to apply the SRL techniques more ex-tensively to biomedical relation extraction.ReferencesAnn Bies, Mark Ferguson, Karen Katz, and RobertMacIntyre.
1995.
Bracketing Guidelines for Tree-bank II Style Penn Treebank Project.
Technical re-port, University of Pennsylvania.Xavier Carreras and Llu?s M?rquez.
2005.
Introduc-tion to the CoNLL-2005 Shared Task: SemanticRole Labeling.
In Proceedings of CoNLL-2005.Michael Collins.
1999.
Head-driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Kadri Hacioglu, Sameer Pradhan, Wayne Ward,James H. Martin, and Daniel Jurafsky.
2004.
Se-mantic Role Labeling by Tagging SyntacticChunks.
In Proceedings of CoNLL-2004.Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and Jun'-ichi Tsujii.
2003.
GENIA corpus?a semanticallyannotated corpus for bio-textmining.
Bioinformat-ics, 19(Suppl.
1): i180-i182.Karin Kipper, Hoa Trang Dang, and Martha Palmer.2000.
Class-based construction of a verb lexicon.In Proceedings of AAAI-2000.Karin Kipper, Martha Palmer, and Owen Rambow.2002.
Extending PropBank with VerbNet semanticpredicates.
In Proceedings of AMTA-2002.Mitchell Marcus, Grace Kim, Mary AnnMarcinkiewicz, Robert MacIntyre, Ann Bies, MarkFerguson, Karen Katz, and Britta Schasberger.1994.
The Penn Treebank: Annotating predicateargument structure.
In Proceedings of ARPA Hu-man Language Technology Workshop.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional Linguistics, 19(2): 313-330.Thomas Morton and Jeremy LaCivita.
2003.
Word-Freak: an open tool for linguistic annotation.
InProceedings of HLT/NAACL-2003.Jorge Nocedal and Stephen J Wright.
1999.
Numeri-cal Optimization, Springer.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Corpusof Semantic Roles.
Computational Linguistics,31(1).Vasin Punyakanok, Dan Roth, Wen-tau Yih, and DavZimak.
2004.
Semantic Role Labeling via IntegerLinear Programming Inference.
In Proceedings ofCOLING-2004.Sidney Siegel and N. John Castellan.
1988.
Non-parametric Statistics for the Behavioral Sciences.New York, McGraw-Hill.Richard Tzong-Han Tsai, Wen-Chi Chou, Yu-ChunLin, Cheng-Lung Sung, Wei Ku, Ying-Shan Su,Ting-Yi Sung, and Wen-Lian Hsu.
2006.
BIOS-MILE: Adapting Semantic Role Labeling for Bio-medical Verbs: An Exponential Model Coupledwith Automatically Generated Template Features.In Proceedings of BioNLP'06.Yuka Tateisi, Tomoko Ohta, and Jun-ichi Tsujii.
2004.Annotation of Predicate-argument Structure of Mo-lecular Biology Text.
In Proceedings of theIJCNLP-04 workshop on Beyond Shallow Analyses.Yuka Tateisi and Jun-ichi Tsujii.
2004.
Part-of-Speech Annotation of Biology Research Abstracts.In Proceedings of the 4th International Conferenceon Language Resource and Evaluation.Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, andJun-ichi Tsujii.
2005.
Syntax Annotation for theGENIA corpus.
In Proceedings of IJCNLP-2005.Tuangthong Wattarujeekrit, Parantu K Shah, andNigel Collier1.
2004.
PASBio: predicate-argumentstructures for event extraction in molecular biology.BMC Bioinformatics, 5(155).Nianwen Xue and Martha Palmer.
2004.
CalibratingFeatures for Semantic Role Labeling.
In Proceed-ings of the EMNLP-2004.Xiaofeng Yang, Guodong Zhou, Jian Su, and ChewLim Tan.
2004.
Improving Noun Phrase Corefer-ence Resolution by Matching Strings.
In Proceed-ings of 1st International Joint Conference on Natu-ral Language Processing: 226-233.12
