Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 430?437, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsNTNU: Domain Semi-Independent Short Message Sentiment Classification?yvind Selmer Mikael Brevik Bjo?rn Gamba?ck Lars BungumDepartment of Computer and Information ScienceNorwegian University of Science and Technology (NTNU)Sem S?lands vei 7?9, NO?7491 Trondheim, Norway{oyvinsel,mikaelbr}@stud.ntnu.no {gamback,larsbun}@idi.ntnu.noAbstractThe paper describes experiments using gridsearches over various combinations of ma-chine learning algorithms, features and pre-processing strategies in order to produce theoptimal systems for sentiment classification ofmicroblog messages.
The approach is fairlydomain independent, as demonstrated by thesystems achieving quite competitive resultswhen applied to short text message data, i.e.,input they were not originally trained on.1 IntroductionThe informal texts in microblogs such as Twitterand on other social media represent challenges fortraditional language processing systems.
The posts(?tweets?)
are limited to 140 characters and oftencontain misspellings, slang and abbreviations.
Onthe other hand, the posts are often opinionated innature as a very result of their informal character,which has led Twitter to being a gold mine for sen-timent analysis (SA).
SA for longer texts, such asmovie reviews, has been explored since the 1990s;1however, the limited amount of attributes in tweetsmakes the feature vectors shorter than in documentsand the task of analysing them closely related tophrase- and sentence-level SA (Wilson et al 2005;Yu and Hatzivassiloglou, 2003).
Hence there areno guarantees that algorithms that perform well ondocument-level SA will do as well on tweets.
Onthe other hand, it is possible to exploit some of thespecial features of the web language, e.g., emoticons1See Pang and Lee (2008); Feldman (2013) for overviews.and emotionally loaded abbreviations.
Thus the datawill normally go through some preprocessing beforeany classification is attempted, e.g., by filtering outTwitter specific symbols and functions, in particularretweets (reposting another user?s tweet), mentions(?
@?, tags used to mention another user), hashtags(?#?, used to tag a tweet to a certain topic), emoti-cons, and URLs (linking to an external resource,e.g., a news article or a photo).
The first system to re-ally use Twitter as a corpus was created as a studentcourse project at Stanford (Go et al 2009).
Pak andParoubek (2010) experimented with sentiment clas-sification of tweets using Support Vector Machinesand Conditional Random Fields, benchmarked witha Na?
?ve Bayes Classifier baseline, but were unableto beat the baseline.
Later, and as Twitter has grownin popularity, many other systems for Twitter Senti-ment Analysis (TSA) have been developed (see, e.g.,Maynard and Funk, 2011; Mukherjee et al 2012;Saif et al 2012; Chamlertwat et al 2012).Clearly, it is possible to classify the sentiment oftweets in a single step; however, the approach toTSA most used so far is a two-step strategy wherethe first step is subjectivity classification and thesecond step is polarity classification.
The goal ofsubjectivity classification is to separate subjectiveand objective statements.
Pak and Paroubek (2010)counted word frequencies in a subjective vs an ob-jective set of tweets; the results showed that in-terjections and personal pronouns are the strongestindicators of subjectivity.
In general, these wordclasses, adverbs and (in particular) adjectives (Hatzi-vassiloglou and Wiebe, 2000) have shown to begood subjectivity indicators, which has made part-430of-speech (POS) tagging a reasonable technique forfiltering out objective tweets.
Early research onTSA showed that the challenging vocabulary madeit harder to accurately tag tweets; however, Gimpelet al(2011) report on using a POS tagger for mark-ing tweets, performing with almost 90% accuracy.Polarity classification is the task of separating thesubjective statements into positives and negatives.Kouloumpis et al(2011) tried different solutions fortweet polarity classification, and found that the bestperformance came from using n-grams together withlexicon and microblog features.
Interestingly, per-formance dropped when a POS tagger was included.They speculate that this can be due to the accuracyof the POS tagger itself, or that POS tagging just isless effective for analysing tweet polarity.In this paper we will explore the application ofa set of machine learning algorithms to the task ofTwitter sentiment classification, comparing one-stepand two-step approaches, and investigate a range ofdifferent preprocessing methods.
What we explic-itly will not do, is to utilise a sentiment lexicon, eventhough many methods in TSA rely on lexica with asentiment score for each word.
Nielsen (2011) man-ually built a sentiment lexicon specialized for Twit-ter, while others have tried to induce such lexicaautomatically with good results (Velikovich et al2010; Mohammad et al 2013).
However, sentimentlexica ?
and in particular specialized Twitter senti-ment lexica ?
make the classification more domaindependent.
Here we will instead aim to exploit do-main independent approaches as far as possible, andthus abstain from using sentiment lexica.
The rest ofthe paper is laid out as follows: Section 2 introducesthe twitter data sets used in the study.
Then Section 3describes the system built for carrying out the twittersentiment classification experiments, which in turnare reported and discussed in Sections 4 and 5.2 DataManually collecting information from Twitter wouldbe a tedious task, but Twitter offers a well doc-umented Representational State Transfer Applica-tion Programming Interface (REST API) which al-lows users to collect a corpus from the micro-blogosphere.
Most of the data used in TSA re-search is collected through the Twitter API, either byTraining Dev 1 Dev 2 NTNUClass Num % Num % Num % Num %Negative 1288 15 176 21 340 26 86 19Neutral 4151 48 144 45 739 21 232 50Positive 3270 37 368 35 575 54 142 31Total 8709 688 1654 461Table 1: The data sets used in the experimentssearching for a certain topic/keyword or by stream-ing realtime data.
Four different data sets were usedin the experiments described below.
three were sup-plied by the organisers of the SemEval?13 sharedtask on Twitter sentiment analysis (Wilson et al2013), in the form of a training set, a smaller initialdevelopment set, and a larger development set.
Allsets consist of manually annotated tweets on a rangeof topics, including different products and events.Tweet-level classification (Task 2B) was split intotwo subtasks in SemEval?13, one allowing trainingonly on the data sets supplied by the organisers (con-strained) and one allowing training also on externaldata (unconstrained).
To this end, a web applica-tion2 for manual annotation of tweets was built andused to annotate a small fourth data set (?NTNU?
).Each of the 461 tweets in the ?NTNU?
data set wasannotated by one person only.The distribution of target classes in the data sets isshown in Table 1.
The data was neither preprocessednor filtered, and thus contain hashtags, URLs, emoti-cons, etc.
However, all the data sets provided bySemEval?13 had more than three target classes (e.g.,?objective?, ?objective-OR-neutral?
), so tweets thatwere not annotated as ?positive?
or ?negative?
weremerged into the ?neutral?
target class.Due to Twitter?s privacy policy, the given data setsdo not contain the tweet text, but only the tweet IDwhich in turn can be used to download the text.
TheTwitter API has a limit on the number of downloadsper hour, so SemEval?13 provided a Python scriptto scrape texts from https://twitter.com.
Thisscript was slow and did not download the texts for alltweet IDs in the data sets, so a faster and more pre-cise download script3 for node.js was implementedand submitted to the shared task organisers.2http://tinyurl.com/tweetannotator3http://tinyurl.com/twitscraper4313 Experimental SetupIn order to run sentiment classification experiments,a general system was built.
It has a Sentiment Anal-ysis API Layer which works as a thin extension ofthe Twitter API, sending all tweets received in par-allel to a Sentiment Analysis Classifier server.
Afterclassification, the SA API returns the same JSONstructure as the Twitter API sends out, only with anadditional attribute denoting the tweet?s sentiment.The Sentiment Analysis Classifier system consistsof preprocessing and classification, described below.3.1 PreprocessingAs mentioned in the introduction, most approachesto Twitter sentiment analysis start with a pre-processing step, filtering out some Twitter specificsymbols and functions.
Go et al(2009) used ?
:)?and ?:(?
as labels for the polarity, so did not removethese emoticons, but replaced URLs and user nameswith placeholders.
Kouloumpis et al(2011) usedboth an emoticon set and a hashtagged set.
The lat-ter is a subset of the Edinburgh Twitter corpus whichconsists of 97 million tweets (Petrovic?
et al 2010).Some approaches have also experimented with nor-malizing the tweets, and removing redundant letters,e.g., ?loooove?
and ?crazyyy?, that are used to ex-press a stronger sentiment in tweets.
Redundant let-ters are therefore often not deleted, but words rathertrimmed down to one additional redundant letter, sothat the stronger sentiment can be taken into consid-eration by a score/weight adjustment for that feature.To find the best features to use, a set of eight dif-ferent combinations of preprocessing methods wasdesigned, as detailed in Table 2.
These include nopreprocessing (P0, not shown in the table), whereall characters are included as features; full remove(P4), where all special Twitter features like usernames, URLs, hashtags, retweet (RT ) tags, andemoticons are stripped; and replacing Twitter fea-tures with placeholder texts to reduce vocabulary.The ?hashtag as word?
method transforms a hashtagto a regular word and uses the hashtag as a feature.
?Reduce letter duplicate?
removes redundant char-acters more than three (?happyyyyyyyy!!!!!!?
??happyyy!!!?).
Some methods, like P1, P2, P4, P5and P7 remove user names from the text, as theymost likely are just noise for the sentiment.
Still,Method P1 P2 P3 P4 P5 P6 P7Remove Usernames X X X X XUsername placeholder XRemove URLs X X X XURL placeholder XRemove hashtags X XHashtag as word XHashtag placeholder XRemove RT -tags X X XRemove emoticons X XReduce letter duplicate X X X XNegation attachment X X XTable 2: Overview of the preprocessing methodsthe fact that there are references to URLs and usernames might be relevant for the sentiment.
To makethese features more informative for the machinelearning algorithms, a preprocessing method (P3)was implemented for replacing them with place-holders.
In addition, a very rudimentary treatmentof negation was added, in which the negation is at-tached to the preceding and following words, so thatthey will also reflect the change in sentence polarity.Even though this preprocessing obviously isTwitter-specific, the results after it will still be do-main semi-independent, in as far as the strings pro-duced after the removal of URLs, user names, etc.,will be general, and can be used for system training.3.2 ClassificationThe classification step currently supports threemachine learning algorithms from the Pythonscikit-learn4 package: Na?
?ve Bayes (NB),Maximum Entropy (MaxEnt), and Support VectorMachines (SVM).
These are all among the super-vised learners that previously have been shown toperform well on TSA, e.g., by Bermingham andSmeaton (2010) who compared SVM and NB formicroblogs.
Interestingly, while the SVM techniquenormally beats NB and MaxEnt on longer texts, thatcomparison indicated that it has some trouble withoutperforming NB when feature vectors are shorter.Three different models were implemented:1.
One-step model: a single algorithm classifiestweets as negative, neutral or positive.2.
Two-step model: the tweets are first classifiedas either subjective or neutral.
Those that are4http://scikit-learn.org432Figure 1: Performance across all models (red=precision, blue=recall, green=F1-score, brown=accuracy)classified as subjective are then sent to polarityclassification (i.e., negative or positive).3.
Boosting (Freund and Schapire, 1997): a wayto combine classifiers by generating a set ofsub-models, each of which predicts a sentimenton its own and then sends it to a voting processthat selects the sentiment with highest score.In all cases, the final classification is returned to theAPI Layer sentiment provider.4 Experimental ResultsExperiments were carried out using the platform in-troduced in the previous section, with models builton the training set of Table 1.
The testing systemgenerates and trains different models based on a setof parameters, such as classification algorithm, pre-processing methods, whether or not to use inversedocument frequency (IDF) or stop words.
A gridsearch option can be activated, so that a model isgenerated with the best possible parameter set forthe given algorithm, using 10-fold cross validation.4.1 Selection of Learners and FeaturesAn extensive grid search was conducted.
This searchcycled through different algorithms, parameters andpreprocessing techniques.
The following param-eters were included in the search.
Three binary(Yes/No) parameters: Use IDF, Use SmoothIDF, and Use Sublinear IDF, together withngram (unigram/bigram/trigram).
SVMand MaxEnt models in addition included C andNB models alpha parameters, all with the valueranges [0.1/0.3/0.5/0.7/0.8/1.0].
SVMand MaxEnt models also had penalty (L1/L2).Figure 1 displays the precision, recall, F1-score,and accuracy for each of the thirteen classifiers withthe Dev 2 data set (see Table 1) used for evaluation.Note that most classifiers involving the NB algo-rithm perform badly, both in terms of accuracy andF-score.
This was observed for the other data sets aswell.
Further, we can see that one-step classifiers didbetter than two-step models, with MaxEnt obtainingthe best accuracy, but SVM a slightly better F-score.433Data set Dev 2 Dev 1Learner SVM MaxEnt SVM MaxEntPrecision 0.627 0.647 0.700 0.561Recall 0.592 0.578 0.726 0.589F1-score 0.598 0.583 0.707 0.556Accuracy 0.638 0.645 0.728 0.581Table 3: Best classifier performance (bold=best score;all classifiers were trained on the training set of Table 1)A second grid search with the two best classifiersfrom the first search was performed instead using thesmaller Dev 1 data set for evaluation.
The resultsfor both the SVM and MaxEnt classifiers are shownin Table 3.
With the Dev 1 data set, SVM performsmuch better than MaxEnt.
The larger Dev 2 develop-ment set contains more neutral tweets than the Dev 1set, which gives us reasons to believe that evaluatingon the Dev 2 set favours the MaxEnt classifier.A detailed error analysis was conducted by in-specting the confusion matrices of all classifiers.
Ingeneral, classifiers involving SVM tend to give bet-ter confusion matrices than the others.
Using SVMonly in a one-step model works well for positive andneutral tweets, but a bit poorer for negative.
Two-step models with SVM-based subjectivity classifica-tion exhibit the same basic behaviour.
The one-stepMaxEnt model classifies more tweets as neutral thanthe other classifiers.
Using MaxEnt for subjectivityclassification and either MaxEnt or SVM for polarityclassification performs well, but is too heavy on thepositive class.
Boosting does not improve and be-haves in a fashion similar to two-step MaxEnt mod-els.
All combinations involving NB tend to heavilyfavour positive predictions; only the two-step mod-els involving another algorithm for polarity classifi-cation gave some improvement for negative tweets.The confusion matrices of the two best learnersare shown in Figures 2a-2d, where a learner is shownto perform better if it has redish colours on the maindiagonal and blueish in the other fields, as is the casefor SVM on the Dev 1 data set (Figure 2c).As a part of the grid search, all preprocessingmethods were tested for each classifier.
Figure 3shows that P2 (removing user names, URLs, hash-tags prefixes, retweet tokens, and redundant letters)is the preprocessing method which performs best(a) SVM Dev 2 (b) MaxEnt Dev 2(c) SVM Dev 1 (d) MaxEnt Dev 1Figure 2: SVM and MaxEnt confusion matrices (out-put is shown from left-to-right: negative-neutral-positive;the correct classes are in the same order, top-to-bottom.?Hotter?
colours (red) indicate that more instances wereassigned; ?colder?
colours (blue) mean fewer instances.
)P2 P7 P6 P1 P302468101043 31Figure 3: Statistics of preprocessing usage(gives the best accuracy) and thus used most of-ten (10 times).
Figure 3 also indicates that URLsare noisy and do not contain much sentiment, whilehashtags and emoticons tend to be more valuablefeatures (P2 and P7 ?
removing URLs ?
performbest, while P4 and P5 ?
removing hashtags andemoticons in addition to URLs ?
perform badly).434Data set Twitter SMSSystem NTNUC NTNUU NTNUC NTNUUPrecision 0.652 0.633 0.659 0.623Recall 0.579 0.564 0.646 0.623F1-score 0.590 0.572 0.652 0.623F1 + /?
0.532 0.507 0.580 0.546Table 4: The NTNU systems in SemEval?134.2 SemEval?13 NTNU Systems and ResultsBased on the information from the grid search, twosystems were built for SemEval?13.
Since one-stepSVM-based classification showed the best perfor-mance on the training data, it was chosen for thesystem participating in the constrained subtask, NT-NUC.
The preprocessing also was the one with thebest performance on the provided data, P2 whichinvolves lower-casing all letters; reducing letter du-plicates; using hashtags as words (removing #); andremoving all URLs, user names and RT -tags.Given the small size of the in-house (?NTNU?
)data set, no major improvement was expected fromadding it in the unconstrained task.
Instead, a rad-ically different set-up was chosen to create a newsystem, and train it on both the in-house and pro-vided data.
NTNUU utilizes a two-step approach,with SVM for subjectivity and MaxEnt for polarityclassification, a combination intended to capture thestrengths of both algorithms.
No preprocessing wasused for the subjectivity step, but user names wereremoved before attempting polarity classification.As further described by Wilson et al(2013), theSemEval?13 shared task involved testing on a set of3813 tweets (1572 positive, 601 negative, and 1640neutral).
In order to evaluate classification perfor-mance on data of roughly the same length and type,but from a different domain, the evaluation data alsoincluded 2094 Short Message Service texts (SMS;492 positive, 394 negative, and 1208 neutral).Table 4 shows the results obtained by the NTNUsystems on the SemEval?13 evaluation data, in termsof average precision, recall and F-score for all threeclasses, as well as average F-score for positive andnegative tweets only (F1+/?
; i.e., the measure usedto rank the systems participating in the shared task).5 Discussion and ConclusionAs can be seen in Table 4, the extra data availableto train the NTNUU system did not really help it:it gets outperformed by NTNUC on all measures.Notably, both systems perform well on the out-of-domain data represented by the SMS messages,which is encouraging and indicates that the approachtaken really is domain semi-independent.
This wasalso reflected in the rankings of the two systems inthe shared task: both were on the lower half amongthe participating systems on Twitter data (24th/36resp.
10th/15), but near the top on SMS data, withNTNUC being ranked 5th of 28 constrained systemsand NTNUU 6th of 15 unconstrained systems.Comparing the results to those shown in Table 3and Figure 1, NTNUC?s (SVM) performance is inline with that on Dev 2, but substantially worsethan on Dev 1; NTNUU (SVM?MaxEnt) performsslightly worse too.
Looking at system output withand without the ?NTNU?
data, both one-step SVMand MaxEnt models and SVM?MaxEnt classifiedmore tweets as negative when trained on the ex-tra data; however, while NTNUC benefited slightlyfrom this, NTNUU even performed better without it.An obvious extension to the present work wouldbe to try other classification algorithms (e.g., Condi-tional Random Fields or more elaborate ensembles)or other features (e.g., character n-grams).
Ratherthan the very simple treatment of negation usedhere, an approach to automatic induction of scopethrough a negation detector (Councill et al 2010)could be used.
It would also be possible to relaxthe domain-independence further, in particular toutilize sentiment lexica (including twitter specific),e.g., by automatic phrase-polarity lexicon extraction(Velikovich et al 2010).
Since many users tweetfrom their smartphones, and a large number of themuse iPhones, several tweets contain iPhone-specificsmilies (?Emoji?).
Emoji are implemented as theirown character set (rather than consisting of charac-ters such as ?:)?
and ?
:(?, etc.
), so a potentially majorimprovement could be to convert them to character-based smilies or to emotion-specific placeholders.AcknowledgementsThanks to Amitava Das for initial discussions and tothe human annotators of the ?NTNU?
data set.435ReferencesBermingham, A. and Smeaton, A. F. (2010).
Clas-sifying sentiment in microblogs: Is brevity anadvantage?
In Proceedings of the 19th Inter-national Conference on Information and Knowl-edge Management, pages 1833?1836, Toronto,Canada.
ACM.Chamlertwat, W., Bhattarakosol, P., Rungkasiri, T.,and Haruechaiyasak, C. (2012).
Discovering con-sumer insight from Twitter via sentiment anal-ysis.
Journal of Universal Computer Science,18(8):973?992.Councill, I. G., McDonald, R., and Velikovich, L.(2010).
What?s great and what?s not: learningto classify the scope of negation for improvedsentiment analysis.
In Proceedings of the 48thAnnual Meeting of the Association for Compu-tational Linguistics, pages 51?59, Uppsala, Swe-den.
ACL.
Workshop on Negation and Specula-tion in Natural Language Processing.Feldman, R. (2013).
Techniques and applications forsentiment analysis.
Communications of the ACM,56(4):82?89.Freund, Y. and Schapire, R. E. (1997).
A decision-theoretic generalization of on-line learning andapplication to boosting.
Journal of Computer andSystem Sciences, 55(1):119?139.Gimpel, K., Schneider, N., O?Connor, B., Das, D.,Mills, D., Eisenstein, J., Heilman, M., Yogatama,D., Flanigan, J., and Smith, N. A.
(2011).
Part-of-speech tagging for Twitter: Annotation, fea-tures, and experiments.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technolo-gies, volume 2: short papers, pages 42?47, Port-land, Oregon.
ACL.Go, A., Huang, L., and Bhayani, R. (2009).
Twit-ter sentiment analysis.
Technical Report CS224NProject Report, Department of Computer Science,Stanford University, Stanford, California.Hatzivassiloglou, V. and Wiebe, J. M. (2000).
Ef-fects of adjective orientation and gradability onsentence subjectivity.
In Proceedings of the 18thInternational Conference on Computational Lin-guistics, pages 299?305, Saarbru?cken, Germany.ACL.HLT10 (2010).
Proceedings of the 2010 HumanLanguage Technology Conference of the NorthAmerican Chapter of the Association for Com-putational Linguistics, Los Angeles, California.ACL.Kouloumpis, E., Wilson, T., and Moore, J.
(2011).Twitter sentiment analysis: The good the bad andthe OMG!
In Proceedings of the 5th InternationalConference on Weblogs and Social Media, pages538?541, Barcelona, Spain.
AAAI.Maynard, D. and Funk, A.
(2011).
Automatic detec-tion of political opinions in tweets.
In #MSM2011(2011), pages 81?92.Mohammad, S., Kiritchenko, S., and Zhu, X.(2013).
NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets.
In SemEval?13(2013).#MSM2011 (2011).
Proceedings of the 1stWorkshop on Making Sense of Microposts(#MSM2011), Heraklion, Greece.Mukherjee, S., Malu, A., Balamurali, A., and Bhat-tacharyya, P. (2012).
TwiSent: A multistage sys-tem for analyzing sentiment in Twitter.
In Pro-ceedings of the 21st International Conference onInformation and Knowledge Management, pages2531?2534, Maui, Hawaii.
ACM.Nielsen, F. A?.
(2011).
A new ANEW: Evaluation ofa word list for sentiment analysis in microblogs.In #MSM2011 (2011), pages 93?98.Pak, A. and Paroubek, P. (2010).
Twitter as a cor-pus for sentiment analysis and opinion mining.
InProceedings of the 7th International Conferenceon Language Resources and Evaluation, Valetta,Malta.
ELRA.Pang, B. and Lee, L. (2008).
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1-2):1?135.Petrovic?, S., Osborne, M., and Lavrenko, V. (2010).The Edinburgh Twitter corpus.
In HLT10 (2010),pages 25?26.
Workshop on Computational Lin-guistics in a World of Social Media.Saif, H., He, Y., and Alani, H. (2012).
Semanticsentiment analysis of Twitter.
In Proceedings ofthe 11th International Semantic Web Conference,pages 508?524, Boston, Massachusetts.
Springer.436SemEval?13 (2013).
Proceedings of the Interna-tional Workshop on Semantic Evaluation, Sem-Eval ?13, Atlanta, Georgia.
ACL.Velikovich, L., Blair-Goldensohn, S., Hannan, K.,and McDonald, R. (2010).
The viability of web-derived polarity lexicons.
In HLT10 (2010), pages777?785.Wilson, T., Kozareva, Z., Nakov, P., Ritter, A.,Rosenthal, S., and Stoyanov, V. (2013).
SemEval-2013 Task 2: Sentiment analysis in Twitter.
InSemEval?13 (2013).Wilson, T., Wiebe, J., and Hoffmann, P. (2005).
Rec-ognizing contextual polarity in phrase-level senti-ment analysis.
In Proceedings of the 2005 Hu-man Language Technology Conference and Con-ference on Empirical Methods in Natural Lan-guage Processing, pages 347?354, Vancouver,British Columbia, Canada.
ACL.Yu, H. and Hatzivassiloglou, V. (2003).
Towards an-swering opinion questions: Separating facts fromopinions and identifying the polarity of opinionsentences.
In Collins, M. and Steedman, M., edi-tors, Proceedings of the 2003 Conference on Em-pirical Methods in Natural Language Processing,pages 129?136, Sapporo, Japan.
ACL.437
