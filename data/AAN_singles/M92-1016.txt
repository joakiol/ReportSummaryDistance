PARAMAX SYSTEMS CORPORATION :MUC-4 TEST RESULTS AND ANALYSI SCarl Weir and Barry SilkParamax Systems CorporationValley Forge Lab sPaoli, Pennsylvaniaweir@prc .unisys.com(215) 648-236 9INTRODUCTIONThe data extraction system submitted by Paramax for evaluation in MUC-4 is a new implementatio nwritten in CLIPS, a forward-chaining system developed and maintained by NAS A 's Johnson Space Center[1] .
Using CLIPS as a forward-chaining engine is desirable because it runs on a wide range of machine s(including Sun Sparc stations, Apple Mac IIs, and PCs), it is available at little or no cost from th egovernment, it is fast, and it comes with good documentation and support services.The Paramax MUC-4 development team consisted of one Paramax staff member and one governmen temployee on sabbatical at Paramax .
The data extraction module was designed and implemented in les sthan two months, using less than four person-months of labor .
Developing inference rules for the systemdid not require any linguistic expertise or any detailed knowledge of CLIPS?neither of the developer shad any prior experience using CLIPS .
All that was required was knowledge of the domain and the dat aextraction task to be performed .TEST RESULT SThe Paramax MUC-4 system's ALL TEMPLATES score summaries for the TST3 and TST4 test set sare listed below .
The Paramax system generated more spurious responses in each of the two tests thanany other system : the average number of TST3 spurious responses for all systems participating in MUC- 4was 883 and the average number of TST4 spurious responses was 867 ; the Paramax system generated2207 and 2240 spurious responses, respectively.TEST POS ACT COR PAR INC ICR IPA SPU MIS NON REC PRE OV GTST3 1693 3264TST4 1253 314860722522 552918419514 154 2207 636 22242 117 2240 345 205042226 8502071Since the Paramax MUC-4 implementation is substantially different from the Paramax MUC-3 submis-sion, the two systems are difficult to compare .
2 The rules developed for the MUC-4 system were initiallybased on rules developed for the MUC-3 system, but the MUC-3 and MUC-4 rule formalisms are signif-icantly different in structure and functionality .
In Figure 1, the TST2 scores for the Paramax MUC-3system and the TST3 progress scores for the MUC-4 system are listed .3An examination of the scores in Figure 1 indicates that improvements in recall between MUC-3 an dMUC-4 have generally resulted in degraded precision scores .
However, the P&R F scores for the MUC- 31 Barry Silk is a U .S .
government employee on sabbatical at Paramax .2 The Paramax MUC-3 system was submitted by the Unisys Center for Advanced Informaton Technology (CAIT), whic hhas since been renamed Paramax Valley Forge Labs R&D .3 NRaD (formerly NOSC) rescored the MUC-3 TST2 scores of veteran sites in order to calculate F measures .
The TST2scores listed in Figure 1 are these rescored results, not the ones that appear in the MUC-3 proceedings [21 .
The MUC-4TST3 progress scores differ slightly from the official MUC-4 TST3 scores ; the differences result from minor adjustmentswhich make the comparision with MUC-3 TST2 scores more meaningful .128TOTAL SCORES FROM MUC-3 EVALUATIO NSLOT POS ACT COR PARINC ICRIPA SPUMIS NON REC PREOVG FA Ltemplate-id 85 47 35 0 0 00 1250 36 41 74 2 6inc-date 81 32 21 7 4 07 049 4 30 76 0inc-loc 85 31 7 18 6 01 054 0 19 52 0inc-type 85 35 34 1 0 00 050 0 40 98 0 0inc-stage 85 35 33 0 2 00 050 0 39 94 0 1perp-inc-cat 59 25 15 0 7 00 337 23 25 60 12 9perp-ind-id 80 14 5 1 4 01 470 32 7 39 2 8perp-org-id 49 43 12 1 6 21 2430 28 26 29 5 6perp-org-conf 49 38 7 2 10 02 1930 28 16 21 50 6phys-tgt-id 53 17 10 2 0 02 541 47 21 65 2 9phys-tgt-type 53 17 9 1 2 01 541 47 18 56 29 0phys-tgt-nation 4 0 0 0 0 00 04 82 0 * * 0phys-tgt-effect 36 17 5 3 1 00 827 58 18 38 47 2hum-tgt-name 34 14 6 0 1 00 727 55 18 43 5 0hum-tgt-desc 102 32 14 0 10 10 878 23 14 44 2 5hum-tgt-type 106 38 15 4 7 04 1280 20 16 45 32 2hum-tgt-nation 10 2 0 0 0 00 210 75 0 0 100 0hum-tgt-effect 82 21 0 12 5 08 465 27 7 28 19 1MATCHED/MISSING 1053 411 193 52 65 327 101743 549 21 53 24MATCHED/SPURIOUS 428 528 193 52 65 327 218118 299 51 41 4 1MATCHED ONLY 428 411 193 52 65 327 101118 200 51 53 24ALL TEMPLATES 1053 528 193 52 65 327 218743 648 21 41 4 1SET FILLS ONLY 569 228 118 23 34 015 53394 360 23 57 23 0STRING FILLS ONLY 318 120 47 4 21 34 48246 185 15 41 40TEXT FILTERING 60 40 36 * * ** 424 36 60 90 10 1 0P&R 2P&R P&2 RF-MEASURES 27 .77 34 .44 23 .27TOTAL SCORES FROM MUC-4 TST3 PROGRESS TES TSLOT POS ACT COR PARINC ICRIPA SPUMIS NON REC PREOVG FALtemplate-id 115 189 86 0 0 00 103 29 14 75 46 5 4inc-date 111 84 40 19 25 219 0 27 4 44 59 0inc-loc 115 82 22 46 14 09 0 33 0 39 55 0inc-type 115 86 79 7 0 00 0 29 0 72 96 0 0inc-stage 115 86 81 0 5 00 0 29 0 70 94 0 2perp-inc-cat 75 80 42 0 13 00 25 20 15 56 52 31 2 5perp-ind-id 87 64 19 0 21 40 24 47 35 22 30 38perp-org-id 59 90 31 0 10 80 49 18 27 52 34 54perp-org-conf 59 82 8 2 31 02 41 18 27 15 11 50 1 1phys-tgt-id 66 60 17 2 11 12 30 36 49 27 30 5 0phys-tgt-type 66 60 16 4 10 03 30 36 49 27 30 50 2phys-tgt-nation 2 0 0 0 0 00 0 2 114 0 * 0phys-tgt-effect 40 57 9 8 3 05 37 20 57 32 23 65 6hum-tgt-name 56 43 24 2 3 22 14 27 62 45 58 32hum-tgt-desc 135 186 45 9 35 68 97 46 13 37 27 5 2hum-tgt-type 146 184 47 26 20 023 91 53 13 41 33 49 7hum-tgt-nation 17 0 0 0 0 00 0 17 102 0 * * 0hum-tgt-effect 124 145 39 21 9 019 76 55 26 40 34 52 7MATCHED/MISSING 1388 1389 519 146 210 2392 514 513 593 43 43 37MATCHED/SPURIOUS 1043 2627 519 146 210 2392 1752 168 1133 57 22 67MATCHED ONLY 1043 1389 519 146 210 2392 514 168 394 57 43 37ALL TEMPLATES 1388 2627 519 146 210 2392 1752 513 1332 43 22 67SET FILLS ONLY 759 780 321 68 91 052 300 279 403 47 46 38 2STRING FILLS ONLY 403 443 136 13 80 2112 214 174 186 35 32 48P&R 2P&R P&2RF-MEASURES 29 .11 24.38 36 .1 1Figure 1: MUC-3 and MUC-4 Performance Compariso n129TST2 and MUC-4 TST3 evaluations indicate an improvement of 1 .34 in overall performance .
F measuresare determined using the following formula :F ?
(p';1 .0)xPxR where P is precision, R is recall, and Q is the relative importance given to recal lp xP}Rover precision .
4No analyses of statistical significance were performed among MUC-3 TST2 and MUC-4 TST3 per-formances .
However, analyses of statistical significance were performed among F scores across system sparticipating in MUC-4 .
The results of these analyses indicate that for the P&R F measure (in whichprecision and recall are given equal weight), there was no significant difference in performance between th eParamax system and the system submitted by SRA .
Similarly, on the 2P&R F measure (in which precisio nis given more weight), there was no significant difference in performance between the Paramax system an dthe systems submitted for evaluation by McDonnell-Douglas (MDC) and New Mexico-Brandeis (NM-BR) .Finally, on the P&2R F measure (in which recall is given more weight), there was no significant differenc ein performance between the Paramax system and the system submitted by BBN .
Appendix G provide sadditional information on F scores and how the analyses of statistical significance were performed .ANALYSISThe Paramax MUC-4 implementation satisfied the key goal of its developers : a fast rule developmentcycle .
The Paramax MUC-3 system was implemented using a forward-chaining engine called Pfc, whic his written in Prolog .
Although the Pfc rule formalism has a number of interesting properties, includin gin particular a mechanism for easily escaping to Prolog in order to use Prolog's built-in factbase and t oreason in a backward-chaining fashion, the system as a whole was inefficient .
Processing a standard testset of 100 messages using the MUC-3 implementation required 40 hours of processing time running onthree separate Sun workstations.
In contrast, the Paramax MUC-4 system implemented in CLIPS ca nprocess 100 messages in just 31. hours on one Sun workstation .
This dramatic improvement in the ruledevelopment cycle made it possible to achieve a respectable level of performance in a small amount oftime .The mid-range performance of the Paramax MUC-4 system could have been significantly improved i fadditional staffing had been available to better engineer the implementation .'
After the MUC-4 test, i twas determined that a bug existed in the preprocessing code for recognizing sentence boundaries?sentenc eendings terminated by double quotes were not recognized .
Since sentence boundaries play a very importantrole in determining the relative likelihood of possible slot values, this problem had a significant impacton the accuracy of the system's slot value preferencing heuristics .
The problem could have been easilyresolved if enough staffing had been available to more carefully examine system output during trainin gruns.
Bugs in the forward-chaining rule base were also discovered after the MUC-4 test that would hav ebeen easy to correct and that had a dramatic cumulative impact on performance .
Examples of such bug sare given in the Paramax MUC-4 system summary.The Paramax system 's high rate of spurious responses was caused by a poor performance in establishin gcoreference among event descriptions .
This poor performance was caused in large part by a lack o ftime/staffing to develop routine heuristics for merging similar templates .
For example, in some cases th eParamax system would generate two identical templates for the same message .
In other cases, the sametarget would arise in two different templates of the same type for the same message (ie, the same buildingwould be bombed, the same individual would be killed, and so forth) .
Improving the set of heuristics use dto establish object coreference will be a top priority for the Paramax team in MUC-5 .
These improvementsshould result in a lower rate of spurious responses .4 1n the case of P&R F scores, for which recall and precision are given equal weight, ,0 = 1 .0.
'No formal mechanism exists for determining the level of effort dedicated to the development of MUC-4 systems, andthe informal estimates offered by the participating research groups are surely inaccurate.
We estimate that implementationswhich performed better than the Paramax system generally involved double the staffing level?most such systems wer edeveloped with government support, which is not the case for the Paramax system .130CONCLUDING REMARK SThe Paramax MUC-4 system takes about 32 hours to process 100 messages on a Sparc2 with 32MB o fmemory and a normal CPU load (ie, with a text editor or two in use) .
The CLIPS-based data extractioncomponent's average elapsed processing time per text in the MUC-4 TST3 data set is 1 minute, 47 seconds .This processing speed permits a fast rule development cycle, which is critical in building knowledge-base dsystems .A failure to insure a rapid rule development cycle is a common mistake among research groups that ar enot accustomed to building large-scale text processing systems .
This mistake was made by a number o fresearch groups in MUC-3, and the Paramax team and other research groups, most notably SRI, rectifie dthis mistake in MUC-4.
The MUC-4 development strategies of Paramax and SRI were roughly similar : arapid rule development cycle was insured by stripping away inefficient linguistic analysis techniques .
TheSRI MUC-4 system performed significantly better than the Paramax submission, but this is very likely aresult of greater staffing resources than the consequence of some fundamental difference in approach .For both the Paramax and SRI research teams, the decision to eliminate linguistic analysis technique swas more a recognition of the primary importance of satisfying the requirements of knowledge-based sys-tems than it was a rejection of linguistic analysis as a useful methodology in text processing .
Linguisti canalysis is still clearly necessary for achieving finer-grained data extraction capabilities, but additional re -search must be performed to improve the efficiency and robustness of the techniques .
Meanwhile, the dat aextraction capabilities of systems with only rudimentary linguistic analysis techniques are capable of gen-erating data bases with sufficient detail to cause researchers to begin worrying about system developmen tissues beyond the data extraction process itself.
Paramount among these issues is the need to performobject coreference on the database level?in other words, to recognize that multiple database records ar edescribing the same object .
Until object cofererence on the data base level becomes a manageable problem ,it will be difficult to use the data bases that are now being extracted .The decision on the part of the Paramax team to build a completely new text processing implementatio nfor MUC-4 was a difficult one to make .
Although it was clearly necessary to achieve a fast rule developmen tcycle, it was also clear that building a new implementation in only a couple of months with limited staffin gwas a high risk venture .
But in retrospect, the Paramax team is confident that the right decision wa smade; system development requirements were prioritized and the need for a rapid rule development cycl ecame out on top .What is truly surprising is that the Paramax MUC-4 system did as well as it did, given the level o feffort that went into developing it .
CLIPS has proven to be an excellent choice for building rule-base dtext analysis systems : it is an extremely fast forward-chaining engine, and it is easily integrated wit hother analysis components .
Several CLIPS rule modules developed for the MUC-4 system can be reused ,particularly the rules used to recognize proper names .
Since the MUC-4 test, the Paramax team ha simplemented a specialized proper name database containing over 9,000 entries in C in order to reduce thesize of the CLIPS fact base .
This strategy should further improve the modularity and reasoning efficienc yof the text processing system .REFERENCES[1] Software Technology Branch .
CLIPS Reference Manual.
Lyndon B .
Johnson Space Center, September1991 .Jsc-25012 .
[2] DARPA, Software and Intelligent Systems Technology Office .
Third Message Understanding Confer-ence (MUC-3) .
Morgan Kaufmann, May 1991 .131
