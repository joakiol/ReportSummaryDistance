First Joint Conference on Lexical and Computational Semantics (*SEM), pages 667?672,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsSAGAN: An approach to Semantic Textual Similaritybased on Textual EntailmentJulio Castillo??
Paula Estrella?
?FaMAF, UNC, Argentina?UTN-FRC, Argentinajotacastillo@gmail.compestrella@famaf.unc.edu.arAbstractIn this paper we report the results obtainedin the Semantic Textual Similarity (STS)task, with a system primarily developed fortextual entailment.
Our results are quitepromising, getting a run ranked 39 in theofficial results with overall Pearson, andranking 29 with the Mean metric.1 IntroductionFor the last couple of years the research com-munity has focused on a deeper analysis of naturallanguages, seeking to capture the meaning of thetext in different contexts: in machine translationpreserving the meaning of the translations is cru-cial to determine whether a translation is useful ornot, in question-answering understanding the ques-tion leads to the desired answers (while the oppo-site case makes a system rather frustrating to theuser) and the examples could continue.
In thisnewly defined task, Semantic Textual Similarity,there is hope that efforts in different areas will beshared and united towards the goal of identifyingmeaning and recognizing equivalent, similar orunrelated texts.
Our contribution to the task, isfrom a textual entailment point of view, as will bedescribed below.The paper is organized as follows: Section 2 de-scribes the relevant tasks, Section 3 describes thearchitecture of the system, then Section 4 showsthe experiments carried out and the results ob-tained, and Section 5 presents some conclusionsand future work.2 Related workIn this section we briefly describe two differenttasks that are closely related and in which our sys-tem has participated with very promising results.2.1 Textual EntailmentTextual Entailment (TE) is defined as a genericframework for applied semantic inference, wherethe core task is to determine whether the meaningof a target textual assertion (hypothesis, H) can beinferred from a given text (T).
For example, giventhe pair (T,H):T: Fire bombs were thrown at the Tunisian embas-sy in BernH: The Tunisian embassy in Switzerland was at-tackedwe can conclude that T entails H.The recently created challenge ?RecognisingTextual Entailment?
(RTE) started in 2005 withthe goal of providing a binary answer for each pair(H,T), namely whether there is entailment or not(Dagan et al, 2006).
The RTE challenge has mu-tated over the years, aiming at accomplishing more667accurate and specific solutions; for example, in2008 a three-way decision was proposed (insteadof the original binary decision) consisting of ?en-tailment?, ?contradiction?
and ?unknown?
; in 2009the organizers proposed a pilot task, the TextualEntailment Search (Bentivogli et al 2009), consist-ing in finding all the sentences in a set of docu-ments that entail a given Hypothesis and since2010 there is a Novelty Detection Task, whichmeans that RTE systems are required to judgewhether the information contained in each H isnovel with respect to (i.e., not entailed by) the in-formation contained in the corpus.2.2 Semantic Textual SimilarityThe pilot task STS was recently defined inSemeval 2012 (Aguirre et al, 2012) and has asmain objective measuring the degree of semanticequivalence between two text fragments.
STS isrelated to both Recognizing Textual Entailment(RTE) and Paraphrase Recognition, but has theadvantage of being a more suitable model for mul-tiple NLP applications.As mentioned before, the goal of the RTE task(Bentivogli et al 2009) is determining whether themeaning of a hypothesis H can be inferred from atext T. Thus, TE is a directional task and we saythat T entails H, if a person reading T would inferthat H is most likely true.
The difference with STSis that STS consists in determining how similartwo text fragments are, in a range from 5 (totalsemantic equivalence) to 0 (no relation).
Thus,STS mainly differs from TE in that the classifica-tion is graded instead of binary.
In this manner,STS is filling the gap between several tasks.3 System architectureSagan is a RTE system (Castillo and Cardenas,2010) which has taken part of several challenges,including the Textual Analysis Conference 2009and TAC 2010, and the Semantic Textual Similari-ty and Cross Lingual Textual Entailment for con-tent synchronization as part of the Semeval 2012.The system is based on a machine learning ap-proach and it utilizes eight WordNet-based(Fellbaum, 1998) similarity measures, as explainedin (Castillo, 2011), with the purpose of obtainingthe maximum similarity between two WordNetconcepts.
A concept is a cluster of synonymousterms that is called a synset in WordNet.
Thesetext-to-text similarity measures are based on thefollowing word-to-word similarity metrics:(Resnik, 1995), (Lin, 1997), (Jiang and Conrath,1997), (Pirr?
and Seco, 2008), (Wu & Palmer,1994), Path Metric, (Leacock & Chodorow, 1998),and a semantic similarity to sentence level namedSemSim (Castillo and Cardenas,2010).Pre-ProcessingSimilarity ScoreMSRWord Level Semantic MetricsExtraction FeaturesSVM withRegressionTest Set:  MSR,MSRvid,Europarl,SMT-news, WNRUN 1Normalizer Stemmer ParserResnik SemSimW&PLin ...Sentence Level Semantic MetricMSR+MSRvidRUN 2RUN 3MSR+MSRvid+EuroparlTraining sets:...Fig.1.
System architectureThe system construct a model of the semanticsimilarity of two texts (T,H) as a function of thesemantic similarity of the constituent words ofboth phrases.
In order to reach this objective, weused a text to text similarity measure which isbased on word to word similarity.
Thus, we expectthat combining word to word similarity metrics totext level would be a good indicator of text to textsimilarity.Additional information about how to producefeature vectors as well as each word- and sentence-level metric can be found in (Castillo, 2011).
Thearchitecture of the system is shown in Figure 1.The training set used for the submitted runs arethose provided by the organizers of the STS.
How-ever we also experimented with RTE datasets asdescribed in the next Section.6684 Experiments and ResultsFor preliminary experiments before the STS Chal-lenge, we used the training set provided by theorganizers, denoted with "_train", and consisting of750 pairs of sentences from the MSR ParaphraseCorpus (MSRpar), 750 pairs of sentences from theMSRvid Corpus (MSRvid), 459 pairs of sentencesof the Europarl WMT2008 development set (SMT-eur).
We also used the RTE datasets from PascalRTE Challenge (Dagan et al, 2006) as part of ourtraining sets.
Additionally, at the testing stage, weused the 399 pairs of  news conversation (SMT-news) and 750 pairs of sentences where the firstone comes from Ontonotes and the second onefrom a WordNet definition (On-WN).In STS Challenge it was required that participat-ing systems do not use the test set of MSR-Paraphrase, the text of the videos in MSR-Video,and the data from the evaluation tasks at any WMTto develop or train their systems.
Additionally, wealso assumed that the dataset to be processed wasunknown in the testing phase, in order to avoid anykind of tuning of the system.4.1 Preliminary ExperimentsIn a preliminary study performed before the finalsubmission, we experimented with three machinelearning algorithms Support Vector Machine(SVM) with regression and polynomial kernel,Multilayer perceptron (MLP), and Linear Regres-sion (LR).
Table 1 shows the results obtained with10-fold cross validation technique and Table 2shows the results of testing them with two datasetsand 3 classifiers over MSR_train.Classifier Pearson c.cSVM with regression 0.54MLP 0.51LinearRegression 0.54Table 1.
Results obtained using MSR training set(MSRpar + MSRvid) with 10 fold-cross validation.Training set & ML algorithm Pearson c.cEuroparl + SVM w/ regression 0.61Europarl + MLP 0.44Europarl + linear regression 0.61MSRvid + SVM w/ regression 0.70MSRvid + MLP 0.52MSRvid + linear regression 0.69Table 2.
Results obtained using MSR training setResults reported in Table 1 show that weachieved the best performance with SVM withregression and Linear Regression classifiers andusing MLP we obtained the worst results to predicteach dataset.
To our surprise, a linear regressionclassifier reports better accuracy that MLP, it maybe mainly due to the correlation coefficient used,namely Pearson, which is a measure of a lineardependence between two variables and linear re-gression builds a model assuming linear influenceof independent features.
We believe that usingSpearman correlation should be better than usingthe Pearson coefficient given that Spearman as-sumes non-linear correlation among variables.However, it is not clear how it behaves when sev-eral dataset are combined to obtain a global score.Indeed, further discussion is needed in order tofind the best metric to the STS pilot task.
Giventhese results, in our submission for the STS pilottask we used a combination of STS datasets astraining set and the SVM with regression classifier.Because our approach is mainly based on ma-chine learning the quality and quantity of dataset isa key factor to determine the performance of thesystem, thus we decided to experiment with RTEdatasets too (Bentivogli et el., 2009) with the aimof increasing the size of the training set.To achieve this goal, first we chose the RTE3dataset because it is simpler than subsequent da-tasets and it was proved to provide a high accuracypredicting other datasets (Castillo, 2011).
Second,taking into account that RTE datasets are binaryclassified as YES or NO entailment, we assumedthat a non entailment can be treated as a value of2.0 in the STS pilot task and an entailment can bethought of as a value of 3.0 in STS.
Of course,many pairs classified as 3.0 could be mostly equiv-alent (4.0) or completely equivalent (5.0) but weignored this fact in the following experiment.Training set Test set Pearsonc.c.RTE3 MSR_train 0.4817RTE3 MSRvid_train 0.5738RTE3 Europarl_train 0.4746MSR_train+RTE3 MSRvid_train 0.5652MSR_train+RTE3 Europarl_train 0.5498MSRvid_train+RTE3 MSR_train 0.4559MSRvid_train+RTE3 Europarl_train 0.4964Table 3.
Results obtained using RTE in the training setsand SVM w/regression as classifier669From these experiments we conclude that RTE3alone is not enough to adequately predict neither ofthe STS datasets, and it is understandable if wenote that only one pair with 2.0 and 3.0 scores arepresent in this dataset.On the other hand, by combining RTE3 with aSTS corpus we always obtain a slight decrease inperformance in comparison to using STS alone.
Itis likely due to an unbalanced set and possiblecontradictory pairs (e.g: a par in RTE3 classified as3.0 when it should be classified 4.3).
Thus, weconclude that in order to use the RTE datasets oursystem needs a manual annotation of the degree ofsemantic similarity of every pair <T,H> of RTEdataset.Having into account that in our training phasewe obtained a decrease in performance using RTEdatasets we decided not to submit any run usingthe RTE datasets.4.2 Submission to the STS shared taskOur participation in the shared task consisted ofthree different runs using a SVM classifier withregression; the runs were set up as follows:- Run 1: system trained on a subset of the Mi-crosoft Research Paraphrase Corpus (Dolan andBrockett, 2005), named MSR and consisting of750 pairs of sentences marked with a degree ofsimilarity from 5 to 0.- Run 2: in addition to the MSR corpus we incor-porated another 750 sentences extracted from theMicrosoft Research Video Description Corpus(MSRvid), annotated in the same way as MSR.- Run 3: to the 1500 sentences from the MSR andMSRvid corpus we incorporated 734 pairs of sen-tences from the Europarl corpus used as develop-ment set in the WMT 2008; all sentences areannotated with the degree of similarity from 5 to 0.It is very interesting to note that we used thesame system configurations for every dataset ofeach RUN.
In this manner, we did not perform anykind of tuning to a particular dataset before oursubmission.
We decided to ignore the "name" ofeach dataset and apply our system regardless of theparticular dataset.
Surely, if we take into accountwhere each dataset came from we can develop aparticular strategy for every one of them, but weassumed that this kind of information is unknownto our system.The official scores of the STS pilot task is thePearson correlation coefficient, and other varia-tions of Pearson which were proposed by the or-ganizers with the aim of better understanding thebehavior of the competing systems among the dif-ferent scenarios.These metric are named ALL (overall Pearson),ALLnrm (normalized Pearson) and Mean(weighted mean), briefly described below:- ALL: To compute this metric, first a new datasetwith the union of the five gold datasets is createdand then the Pearson correlation is calculated overthis new dataset.- ALLnrm: In this metric, the Pearson correlationis computed after the system outputs for each da-taset are fitted to the gold standard using leastsquares.- Mean: This metric is a weighted mean across thefive datasets, where the weight is given by thequantity of pairs in each dataset.Table 5 report the results achieved with thesemetrics followed by an individual Pearson correla-tion for each dataset.Interestingly, if we analyze the size of data sets,we see that the larger the training set used, thegreater the efficiency gains with ALL metric.
Ineffect, RUN3 used 2234 pairs, RUN2 used 1500pairs and RUN1 was composed by 750 pairs.
Thishighlights the need for larger datasets for the pur-pose of building more accurate models.With ALLnrm our system achieved better re-sults but since this metric is based on normalizedPearson correlation which assumes a linear correla-tion, we believe that this metric is not representa-tive of the underlying phenomenon.
For example,conducting manual observation we can see thatpairs from SMT-news are much harder to classifythan MSRvid pairs.
This results can also be evi-denced from others participating teams who almostalways achieved better results with MSRvid thanSMT-news dataset.The last metric proposed is the Mean and we areranked 29 among participating teams.
It is proba-bly due to the weight of SMT-news (399 pairs) issmaller than MSR or MSRvid.Mean metrics seems to be more suitable for thistask but lack an important issue, do not have intoaccount the different "complexity" of the datasets.It is also a issue for all metrics proposed.
We be-lieve that incorporating to Mean metric a complex-ity factor weighting for each dataset based on a670human judge assignment could be more suitablefor the STS evaluation.
We think in complexity asan underlying concept referring to the difficulty ofdetermine how semantically related two sentencesare to one another.
Thus, two sentences with highlexical overlap should have a low complexity andinstead two sentences that requires deep inferenceto determine similarity should have a high com-plexity.
This should be heighted by human annota-tors and could be a method for a more preciseevaluation of STS systems.Finally, we suggested measuring this new chal-lenging task using a weighted Mean of theSpearman's rho correlation coefficient by incorpo-rating a factor to weigh the difficulty of each da-taset.Run ALL Rank ALLnrmRankNrmMeanRankMeanMSRparMSRvidSMT-eurOn-WNSMT-newsBest Run ,8239 1 ,8579 2 ,6773 1 ,6830 ,8739 ,5280 ,6641 ,4937Worst Run -,0260 89 ,5933 89 ,1016 89 ,1109 ,0057 ,0348 ,1788 ,1964Sagan-RUN1 ,5522 57 ,7904 47 ,5906 29 ,5659 ,7113 ,4739 ,6542 ,4253Sagan-RUN2 ,6272 42 ,8032 37 ,5838 34 ,5538 ,7706 ,4480 ,6135 ,3894Sagan-RUN3 ,6311 39 ,7943 45 ,5649 46 ,5394 ,7560 ,4181 ,5904 ,3746Table 5.
Official results of the STS challenge5 Conclusions and future workIn this paper we present Sagan, an RTE systemapplied to the task of Semantic Textual Similarity.After a preliminary study of the classifiers perfor-mance for the task, we decided to use a combina-tion of STS datasets for training and the classifierSVM with regression.
With this setup the systemwas ranked 39 in the best run with overall Pearson,and ranked 29 with Mean metric.
However, bothrankings are based on the Pearson correlation coef-ficient and we believe that this coefficient is notthe best suited for this task, thus we proposed aMean Spearman's rho correlation coefficientweighted by complexity, instead.
Therefore, fur-ther application of other metrics should be one inorder to find the most representative and fair eval-uation metric for this task.
Finally, while promis-ing results were obtained with our system, it stillneeds to be tested on a diversity of settings.
This iswork in progress, as the system is being tested as ametric for the evaluation of machine translation, asreported in (Castillo and Estrella, 2012).ReferencesChristoph Tillmann, Stephan Vogel, Hermann Ney,Arkaitz Zubiaga, and Hassan Sawaf.
1997.
Acceler-ated DP Based Search For Statistical Translation.
InProceedings of the 5th European Conference onSpeech Communication and Technology(EUROSPEECH-97).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th AnnualMeeting of the Association forComputational Linguistics(ACL-02), pages 311?318.Sonja Nie?en, Franz Josef Och, Gregor Leusch, andHermann Ney.
2000.
A Evaluation Tool for MachineTranslation:Fast Evaluation for MT Research.
InProceedings of the 2nd International Conference onLanguage Resources and Evaluation (LREC-2000).G.
Doddington.
2002.
Automatic Evaluation of MachineTranslation Quality using N-gram Co-occurrenceStatistics.
In Proceedings of the 2nd InternationalConference on Human Language Technology Re-search (HLT-02), pages 138?145, San Francisco,CA, USA.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Pro-ceedings of the 43th Annual Meeting of theAssociation of Computational Linguistics (ACL-05),pages 65?72.Michael Denkowski and Alon Lavie.
2011.
METEOR-NEXT and the METEOR Paraphrase Tables: Im-proved Evaluation Support For Five Target Lan-guages.
Proceedings of the ACL 2010 JointWorkshop on Statistical Machine Translation andMetrics MATR.671He Yifan, Du Jinhua, Way Andy, and Van Josef .
2010.The DCU dependency-based metric in WMT-MetricsMATR 2010.
In: WMT 2010 - Joint FifthWorkshop on Statistical Machine Translation andMetrics MATR, ACL, Uppsala, Sweden.Chi-kiu Lo and Dekai Wu.
2011.
MEANT: inexpensive,high-accuracy, semi-automatic metric for evaluatingtranslation utility based on semantic roles.
49th An-nual Meeting of the Association for ComputationalLinguistic (ACL-2011).
Portland, Oregon, US.Matthew Snover, Bonnie J. Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human Anno-tation.
In Proceedings of the 7th Conference of theAssociation for Machine Translation in the Americas(AMTA-06), pages 223?231.Ido Dagan, Oren Glickman and Bernardo Magnini.2006.
The PASCAL Recognising Textual EntailmentChallenge.
In Qui?onero-Candela, J.; Dagan, I.;Magnini, B.; d'Alch?-Buc, F.
(Eds.)
Machine Learn-ing Challenges.
Lecture Notes in Computer Science ,Vol.
3944, pp.
177-190, Springer.Shachar Mirkin, Lucia Specia, Nicola Cancedda, IdoDagan, Marc Dymetman, and Idan Szpektor.
2009.Source-language entailment modeling for translatingunknown terms.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL.Stroudsburg, PA, USA, 791-799.Wilker Aziz and Marc Dymetmany and Shachar Mirkinand Lucia Specia and Nicola Cancedda and Ido Da-gan.
2010.
Learning an Expert from Human Annota-tions in Statistical Machine Translation: the Case ofOut-of-VocabularyWords.
In: Proceedings of the14th annual meeting of the European Association forMachine Translation (EAMT), Saint-Rapha, France.Dahlmeier, Daniel  and  Liu, Chang  and  Ng, HweeTou.
2011.TESLA at WMT 2011: Translation Evalu-ation and Tunable Metric.In: Proceedings of theSixth Workshop on Statistical Machine Translation.ACL,  pages 78-84, Edinburgh, Scotland.S.
Pado, D. Cer, M. Galley, D. Jurafsky and C. Man-ning.
2009.
Measuring Machine Translation Qualityas Semantic Equivalence: A Metric Based on Entail-ment Features.
Journal of MT 23(2-3), 181-193.S.
Pado, M. Galley, D. Jurafsky and C. Manning.
2009a.Robust Machine Translation Evaluation with Entail-ment Features.
Proceedings of ACL 2009.Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonza-lez-Agirre.
2012.
SemEval-2012 Task 6: A Pilot onSemantic Textual Similarity.
In Proceedings of the6th International Workshop on Semantic    Evalua-tion (SemEval 2012), in conjunction with the FirstJoint    Conference on Lexical and ComputationalSemantics (*SEM 2012).Bentivogli, Luisa, Dagan Ido, Dang Hoa, Giampiccolo,Danilo, Magnini Bernardo.2009.The Fifth PASCALRTE Challenge.
In: Proceedings of the Text AnalysisConference.Fellbaum C. 1998.
WordNet: An Electronic LexicalDatabase, volume 1.
MIT Press.Castillo Julio.
2011.
A WordNet-based semantic ap-proach to textual entailment and cross-lingual textu-al entailment.
International Journal of MachineLearning and Cybernetics - Springer, Volume 2,Number 3.Castillo Julio and Cardenas Marina.
2010.
Using sen-tence semantic similarity based onWordNet in recog-nizing textual entailment.
Iberamia 2010.
In LNCS,vol 6433.
Springer, Heidelberg, pp 366?375.Castillo Julio.
2010.
A semantic oriented approach totextual entailment using WordNet-based measures.MICAI 2010.
LNCS, vol 6437.
Springer, Heidelberg,pp 44?55.Castillo Julio.
2010.
Using machine translation systemsto expand a corpus in textual entailment.
In: Proceed-ings of the Icetal 2010.
LNCS, vol 6233, pp 97?102.Resnik P. 1995.
Information content to evaluate seman-tic similarity in a taxonomy.
In: Proceedings of IJCAI1995, pp 448?453 907.Castillo Julio, Cardenas Marina.
2011.
An Approach toCross-Lingual Textual Entailment using Online Ma-chine Translation Systems.
Polibits Journal.
Vol 44.Castillo Julio and Estrella Paula.
2012.
Semantic Textu-al Similarity for MT evaluation.
NAACL 2012Seventh Workshop on Statistical Machine Transla-tion.
WMT 2012, Montreal, Canada.Lin D. 1997.
An information-theoretic definition ofsimilarity.
In: Proceedings of Conference on MachineLearning, pp 296?304 909.Jiang J, Conrath D.1997.
Semantic similarity based oncorpus statistics and lexical taxonomy.
In: Proceed-ings of theROCLINGX 911Pirro G., Seco N. 2008.
Design, implementation andevaluation of a new similarity metric combining fea-ture and intrinsic information content.
In: ODBASE2008, Springer LNCS.Wu Z, Palmer M. 1994.
Verb semantics and lexicalselection.
In: Proceedings of the 32nd ACL 916.Leacock C, Chodorow M. 1998.
Combining local con-text and WordNet similarity for word sense identifi-cation.
MIT Press, pp 265?283 919Hirst G, St-Onge D .
1998.
Lexical chains as represen-tations of context for the detection and correction ofmalapropisms.
MIT Press, pp 305?332 922Banerjee S, Pedersen T. 2002.
An adapted lesk algo-rithm for word sense disambiguation using WordNet.In: Proceeding of CICLING-02William B. Dolan and Chris Brockett.2005.
Automati-cally Constructing a Corpus of Sentential Para-phrases.
Third International Workshop onParaphrasing (IWP2005).
Asia Federation of NaturalLanguage Processing.672
