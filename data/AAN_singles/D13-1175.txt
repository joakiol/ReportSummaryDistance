Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1688?1699,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsBoosting Cross-Language Retrieval by LearningBilingual Phrase Associations from Relevance RankingsArtem Sokolov and Laura Jehl and Felix Hieber and Stefan RiezlerDepartment of Computational LinguisticsHeidelberg University, 69120 Heidelberg, Germany{sokolov,jehl,hieber,riezler}@cl.uni-heidelberg.deAbstractWe present an approach to learning bilin-gual n-gram correspondences from relevancerankings of English documents for Japanesequeries.
We show that directly optimizingcross-lingual rankings rivals and complementsmachine translation-based cross-language in-formation retrieval (CLIR).
We propose an ef-ficient boosting algorithm that deals with verylarge cross-product spaces of word correspon-dences.
We show in an experimental evalu-ation on patent prior art search that our ap-proach, and in particular a consensus-basedcombination of boosting and translation-basedapproaches, yields substantial improvementsin CLIR performance.
Our training and testdata are made publicly available.1 IntroductionThe central problem addressed in Cross-LanguageInformation Retrieval (CLIR) is that of translatingor projecting a query into the language of the docu-ment repository across which retrieval is performed.There are two main approaches to tackle this prob-lem: The first approach leverages the standard Sta-tistical Machine Translation (SMT) machinery toproduce a single best translation that is used assearch query in the target language.
We will hence-forth call this the direct translation approach.
Thistechnique is particularly useful if large amounts ofdata are available in domain-specific form.Alternative approaches avoid to solve the hardproblem of word reordering, and instead rely ontoken-to-token translations that are used to projectthe query terms into the target language with aprobabilistic weighting of the standard term tf-idfscheme.
Darwish and Oard (2003) termed thismethod the probabilistic structured query approach.The advantage of this technique is an implicit queryexpansion effect due to the use of probability distri-butions over term translations (Xu et al 2001).
Re-cent research has shown that leveraging query con-text by extracting term translation probabilities fromn-best direct translations of queries instead of usingcontext-free translation tables outperforms both di-rect translation and context-free projection (Ture etal., 2012b; Ture et al 2012a).While direct translation as well as probabilisticstructured query approaches use machine learning tooptimize the SMT module, retrieval is done by stan-dard search algorithms in both approaches.
For ex-ample, Google?s CLIR approach uses their standardproprietary search engine (Chin et al 2008).
Ture etal.
(2012b; 2012a) use standard retrieval algorithmssuch as BM25 (Robertson et al 1998).
That means,machine learning in SMT-based approaches concen-trates on the cross-language aspect of CLIR and isagnostic of the ultimate ranking task.In this paper, we present a method to projectsearch queries into the target language that is com-plementary to SMT-based CLIR approaches.
Ourmethod learns a table of n-gram correspondences bydirect optimization of a ranking objective on rele-vance rankings of English documents for Japanesequeries.
Our model is similar to the approach ofBai et al(2010) who characterize their technique as?Learning to rank with (a Lot of) Word Features?.Given a set of search queries q ?
IRQ and docu-1688ments d ?
IRD, where the jth dimension of a vectorindicates the occurrence of the jth word for dictio-naries of size Q and D, we want to learn a scoref(q,d) between a query and a given document us-ing the model1f(q,d) = q>Wd =Q?i=1D?j=1qiWijdj .We take a pairwise ranking approach to optimiza-tion.
That is, given labeled data in the form of aset R of tuples (q,d+,d?
), where d+ is a relevant(or higher ranked) document and d?
an irrelevant(or lower ranked) document for query q, the goalis to find a weight matrix W ?
IRQ?D such thatf(q,d+) > f(q,d?)
for all data tuples from R.The scoring model learns weights for all possiblecorrespondences of query terms and document termsby directly optimizing the ranking objective at hand.Such a phrase table contains domain-specific wordassociations that are useful to discern relevant fromirrelevant documents, something that is orthogonaland complementary to standard SMT models.The challenge of our approach can be explainedby constructing a joint feature map ?
from the outerproduct of the vectors q and d where?
((i?1)D+j)(q,d) = (q?
d)ij = (qd>)ij .
(1)Using this feature map, we see that the score func-tion f can be written in the standard form of a lin-ear model that computes the inner product betweena weight vector w and a feature vector ?
wherew, ?
?
IRQ?D andf(q,d) = ?w, ?(q,d)?.
(2)While various standard algorithms exist to optimizelinear models, the difficulty lies in the memory foot-print and capacity of the word-based model.
A full-sized model includes Q ?
D parameters which iseasily in the billions even for moderately sized dic-tionaries.
Clearly, an efficient implementation andremedies against overfitting are essential.The main contribution of our paper is the pre-sentation of algorithms that make learning a phrase1With bold letters we denote vectors for query q and docu-ment d. Vector components are denoted with normal font lettersand indices (e.g., qi).table by direct rank optimization feasible, and anexperimental verification of the benefits of this ap-proach, especially with regard to a combinationof the orthogonal information sources of ranking-based and SMT-based CLIR approaches.
Our ap-proach builds upon a boosting framework for pair-wise ranking (Freund et al 2003) that allows themodel to grow incrementally, thus avoiding havingto deal with the full matrix W .
Furthermore, wepresent an implementation of boosting that utilizesparallel estimation on bootstrap samples from thetraining set for increased efficiency and reduced er-ror (Breiman, 1996).
Our ?bagged boosting?
ap-proach allows to combine incremental feature selec-tion, parallel training, and efficient management oflarge data structures.We show in an experimental evaluation on large-scale retrieval on patent abstracts that our boostingapproach is comparable in MAP and improves sig-nificantly by 13-15 PRES points over very competi-tive translation-based CLIR systems that are trainedon 1.8 million parallel sentence pairs from Japanese-English patent documents.
Moreover, a combinationof the orthogonal information learned in ranking-based and translation-based approaches improvesover 7 MAP points and over 15 PRES points over therespective translation-based system in a consensus-based voting approach following the Borda Counttechnique (Aslam and Montague, 2001).2 Related WorkRecent research in CLIR follows the two mainparadigms of direct translation and probabilisticstructured query approaches.
An example for thefirst approach is the work of Magdy and Jones(2011) who presented an efficient technique to adaptoff-the-shelf SMT systems for CLIR by trainingthem on data pre-processed for retrieval (case fold-ing, stopword removal, stemming).
Nikoulina et al(2012) presented an approach to direct translation-based CLIR where the n-best list of an SMT systemis re-ranked according to the MAP performance ofthe translated queries.
The probabilistic structuredquery approach has seen a lot of work on context-aware query expansion across languages, based onvarious similarity statistics (Ballesteros and Croft,1998; Gao et al 2001; Lavrenko et al 2002; Gao1689et al 2007).
At the time of writing this paper, themost recent extension to this paradigm is Ture etal.
(2012a).
In addition to projecting terms fromn-best translations, they propose a projection ex-tracted from the hierarchical phrase- based grammarmodels, and a scoring method based on multi-tokenterms.
Since the latter techniques achieved onlymarginal improvements over the context-sensitivequery translation from n-best lists, we did not pur-sue them in our work.CLIR in the context of patent prior art searchwas done as extrinsic evaluation at the NTCIRPatentMT2 workshops until 2010, and has been on-going in the CLEF-IP3 benchmarking workshopssince 2009.
However, most workshop participantsdid either not make use of automatic translation atall, or they used an off-the-shelf translation tool.This is due to the CLEF-IP data collection whereparts of patent documents are provided as man-ual translations into three languages.
In order toevaluate CLIR in a truly cross-lingual scenario, wecreated a large patent CLIR dataset where queriesand documents are Japanese and English patent ab-stracts, respectively.Ranking approaches to CLIR have been presentedby Guo and Gomes (2009) who use pairwise rank-ing for patent retrieval.
Their method is a classicallearning-to-rank setup where retrieval scores such astf-idf or BM25 are combined with domain knowl-edge on patent class, inventor, date, location, etc.into a dense feature vector of a few hundred fea-tures.
Methods to learn word-based translation cor-respondences from supervised ranking signals havebeen presented by Bai et al(2010) and Chen etal.
(2010).
These approaches tackle the problem ofcomplexity and capacity of the cross product matrixof word correspondences from different directions.The first proposes to learn a low rank representa-tion of the matrix; the second deploys sparse onlinelearning under `1 regularization to keep the matrixsmall.
Both approaches are mainly evaluated in amonolingual setting.
The cross-lingual evaluationpresented in Bai et al(2010) uses weak translation-based baselines and non-public data such that a di-rect comparison is not possible.2http://research.nii.ac.jp/ntcir/ntcir/3http://www.ifs.tuwien.ac.at/?clef-ip/A combination of bagging and boosting in thecontext of retrieval has been presented by Pavlov etal.
(2010) and Ganjisaffar et al(2011).
This workis done in a standard learning-to-rank setup using afew hundred dense features trained on hundreds ofthousands of pairs.
Our setup deals with billions ofsparse features (from the cross-product of the un-restricted dictionaries) trained on millions of pairs(sampled from a much larger space).
Parallel boost-ing where all feature weights are updated simultane-ously has been presented by Collins et al(2002) andCanini et al(2010).
The first method distributes thegradient calculation for different features among dif-ferent compute nodes.
This is not possible in our ap-proach because we construct the cross-product ma-trix on-the-fly.
The second approach requires sub-stantial efforts in changing the data representationto use the MapReduce framework.
Overall, one ofthe goals of our work is sequential updating for im-plicit feature selection, something that runs contraryto parallel boosting.3 CLIR Approaches3.1 Direct translation approachFor direct translation, we use the SCFG decodercdec (Dyer et al 2010)4 and build grammars us-ing its implementation of the suffix array extractionmethod described in Lopez (2007).
Word align-ments are built from all parallel data using mgiza5and the Moses scripts6.
SCFG models use the samesettings as described in Chiang (2007).
Trainingand querying of a modified Kneser-Ney smoothed 5-gram language model are done on the English sideof the training data using KenLM (Heafield, 2011)7.Model parameters were optimized using cdec?s im-plementation of MERT (Och (2003)).At retrieval time, all queries are translatedsentence-wise and subsequently re-joined to formone query per patent.
Our baseline retrieval systemuses the Okapi BM25 scores for document ranking.4https://github.com/redpony/cdec5http://www.kyloo.net/software/doku.php/mgiza:overview6http://www.statmt.org/moses/?n=Moses.SupportTools7http://kheafield.com/code/kenlm/estimation/16903.2 Probabilistic structured query approachEarly Probabilistic Structured Query approaches(Xu et al 2001; Darwish and Oard, 2003) representtranslation options by lexical, i.e., token-to-tokentranslation tables that are estimated using standardword alignment techniques (Och and Ney, 2000).Later approaches (Ture et al 2012b; Ture et al2012a) extract translation options from the decoder?sn-best list for translating a particular query.
Thecentral idea is to let the language model choose flu-ent, context-aware translations for each query termduring decoding.
This retains the desired query-expansion effect of probabilistic structured models,but it reduces query drift by filtering translationswith respect to the context of the full query.A projection of source language query terms f ?F into the target language is achieved by repre-senting each source token f by its probabilisticallyweighted translations.
The score of target documentE, given source language query F , is computed bycalculating the BM25 rank over projected term fre-quency and document frequency weights as follows:score(E|F ) =?f?FBM25(tf(f,E), df(f)) (3)tf(f,E) =?e?Eftf(e, E)p(e|f)df(f) =?e?Efdf(e)p(e|f)where Ef = {e ?
E|p(e|f) > pL} is the set oftranslation options for query term f with probabilitygreater than pL.
We also use a cumulative thresholdpC so that only the most probable options are addeduntil pC is reached.Ture et al(2012b; 2012a) achieved best retrievalperformance by interpolating between (context-free)lexical translation probabilities plex estimated onsymmetrized word alignments, and (context-aware)translation probabilities pnbest estimated on the n-best list of an SMT decoder:p(e|f) = ?pnbest(e|f) + (1?
?
)plex(e|f) (4)pnbest(e|f) is estimated by calculating expectationsof term translations from k-best translations:pnbest(e|f) =?nk=1 ak(e, f)D(k, F )?nk=1?e?
ak(e?, f)D(k, F )where ak(e, f) is a function indicating an alignmentof target term e to source term f in the kth derivationof queryF , andD(k, F ) is the model score of the kthderivation in the n-best list for query F .We use the same hierarchical phrase-based sys-tem that was used for direct translation to calcu-late n-best translations for the probabilistic struc-tured query approach.
This allows us to extractword alignments between source and target text forF from the SCFG rules used in the derivation.
Theconcept of self-translation is covered by the de-coder?s ability to use pass-through rules if words orphrases cannot be translated.Probabilistic structured queries that includecontext-aware estimates of translation probabilitiesrequire a preservation of sentence-wise context-sensitivity also in retrieval.
Thus, unlike the directtranslation approach, we compute weighted termand document frequencies for each sentence s inquery F separately.
The scoring (3) of a target doc-ument for a multiple sentence query then becomes:score(E|F ) =?s in F?f?sBM25(tf(f,E), df(f))3.3 Direct Phrase Table Learning fromRelevance RankingsPairwise Ranking using Boosting The generalform of the RankBoost algorithm (Freund et al2003; Collins and Koo, 2005) defines a scoringfunction f(q,d) on query q and document d as aweighted linear combination of T weak learners htsuch that f(q,d) =?Tt=1wtht(q,d).
Weak learn-ers can belong to an arbitrary family of functions,but in our case they are restricted to the simplestcase of unparameterized indicator functions select-ing components of the feature vector ?
(q,d) in (1)such that f is of the standard linear form (2).
In ourexperiments, these features indicate the presence ofpairs of uni- and bi-grams from the source-side vo-cabulary of query terms and the target-side vocabu-lary of document-terms, respectively.
Furthermore,in order to simulate the pass-through behavior ofSMT, we introduce additional features to the modelthat indicate the identity of terms in source and tar-get.
All identity features have the same fixed weight?, which is found on the development set.For training, we are given labeled data in the form1691of a set R of tuples (q,d+,d?
), where d+ is a rel-evant (or higher ranked) document and d?
an ir-relevant (or lower ranked) document for query q.RankBoost?s objective is to correctly rank query-document pairs such that f(q,d+) > f(q,d?)
forall data tuples from R. RankBoost achieves this byoptimizing the following convex exponential loss:Lexp =?(q,d+,d?)?RD(q,d+,d?)ef(q,d?
)?f(q,d+),where D(q,d+,d?)
is a non-negative importancefunction on pairs of documents for a given q.We optimize Lexp in a greedy iterative fash-ion, which closely follows an efficient algorithm ofCollins and Koo (2005) for the case of binary-valuedh.
In each step, the single feature h is selected thatprovides the largest decrease of Lexp, i.e., that hasthe largest projection on the direction of the gradi-ent ?hLexp.
Because of the sequential nature ofthe algorithm, RankBoost implicitly performs auto-matic feature selection and regularization (Rosset etal., 2004), which is crucial to reduce complexity andcapacity for our application.Parallelization and Bagging To achieve paral-lelization we use a variant of bagging (Breiman,1996) on top of boosting, which has been observedto improve performance, reduce variance and istrivial to parallelize.
The procedure is describedas part of Algorithm 1: From the set of prefer-ence pairs R, draw S equal-sized samples withreplacement and distribute to nodes.
Then, us-ing each of the samples as a training set, sep-arate boosting models {wst , hst}, s = 1 .
.
.
S aretrained that contain the same number of featurest = 1 .
.
.
T .
Finally the models are averaged:f(q,d) = 1S?t?swsthst (q,d).Algorithm The entire training procedure is out-lined in Algorithm 1.
For each possible feature hwe maintain auxiliary variables W+h and W?h :W?h =?(q,d+,d?):h(q,d+)?h(q,d?)=?1D(q,d+,d?
),which are the cumulative weights of correctly andincorrectly ranked instances by a candidate featureh.
The absolute value of ?Lexp/?h can be ex-pressed as??
?W+h ??W?h??
which is used as fea-ture selection criterion (Collins and Koo, 2005).The optimum of minimizing Lexp over w (withfixed h) can be shown to be w = 12 lnW+h +ZW?h +Z,where  is a smoothing parameter to avoid prob-lems with small W?h (Schapire and Singer, 1999),and Z =?(q,d+,d?)?RD(q,d+,d?).
Further-more, for each step t of the learning process, valuesof D are updated to concentrate on pairs that havenot been correctly ranked so far:Dt+1 = Dt ?
ewt(ht(q,d?)?ht(q,d+)).
(5)Finally, to speed up learning, on iteration t werecalculate W?h only for those h that cooccurwith previously selected ht and keep the rest un-changed (Collins and Koo, 2005).Algorithm 1: Bagged BoostingInput: training tuplesR, max number of featuresT , initial D0, smoothing param.
 ' 10?5Initialize:fromR draw S samples with replacement anddistribute to nodesLearn:for all samples s = 1 .
.
.
S in parallel docalculate W+h ,W?h , Z on sample?s datafor all t = 1 .
.
.
T dochoose ht = argmaxh??
?W+h ??W?h?
?and wt = 12 lnW+h +ZW?h +Zupdate Dt according to (5)update W?h for all h that cooccur with htendreturn to master {hst , wst }, t = 1 .
.
.
TendBagging:return scoring functionf(q,d) = 1S?t?swsthst (q,d)Implementation Because of the total number offeatures (billions) there are several obstacles for thestraight-forward implementation of Algorithm 1.First, we cannot directly access all pairs (q,d)containing a particular feature h needed for calcu-lating W?h .
Building an inverted index is compli-cated as it needs to fit into memory for fast fre-1692quent access8.
We resort to the on-the-fly creation ofthe cross-product space of features, following priorwork by Grangier and Bengio (2008) and Goel et al(2008).
That is, while processing a pair (q,d), weupdate W?h for all h found for the pair.Second, even if the explicit representation of allfeatures is avoided by on-the-fly feature construc-tion, we still need to keep all W?h in addressableRAM.
To achieve that we use hash kernels (Shi etal., 2009) and map original features into b-bit integerhashes.
The values W?h?
for new, ?hashed?, featuresh?
become W?h?
=?h:HASH(h)=h?W?h .
We usedthe MurmurHash3 function on the UTF-8 represen-tations of features and b = 30 (resulting in morethan 1 billion distinct hashes).4 Model Combination by Borda CountsSMT-based approaches to CLIR and our boostingapproach have different strengths.
The SMT-basedapproaches produce fluent translations that are use-ful for matching general passages written in natu-ral language.
Both baseline SMT-based approachespresented above are agnostic of the ultimate retrievaltask and are not specifically adapted for it.
Theboosting method, on the other hand, learns domain-specific word associations that are useful to discernrelevant from irrelevant documents.
In order to com-bine these orthogonal sources of information in away that democratically respects each approach weuse Borda Counts, i.e., a consensus-based votingprocedure that has been successfully employed toaggregate ranked lists of documents for metasearch(Aslam and Montague, 2001).We implemented a weighted version of the BordaCount method where each voter has a fixed amountof voting points which she is free to distribute amongthe candidates to indicate the amount of preferenceshe is giving to each of them.
In the case of retrieval,for each q, the candidates are the scored documentsin the retrieved subset of the whole document set.The aggregate score fagg for two rankings f1(q,d)8It is possible to construct separate query and document in-verted indices and intersect them on the fly to determine theset of documents that contains some pair of words.
In practice,however, we found the overhead of set intersection during eachfeature access prohibitive.and f2(q,d) for all (q,d) in the test set is then:fagg(q,d) = ?f1(q,d)?d f1(q,d)+(1??
)f2(q,d)?d f2(q,d).In practice, the normalizations sum over the top Kretrieved documents.
If a document is present onlyin the top-K list of one system, its score is con-sidered zero for the other system.
The aggregatedscores fagg(q,d) are sorted in descending order andtop K scores are kept for evaluation.Using the terminology proposed by Belkin etal.
(1995), combining several systems?
scores withBorda Counts can be viewed as the ?data fusion?approach to IR, that merges outputs of the systems,while the PSQ baseline is an example of the ?querycombination?
approach that extends the query at theinput.
Both techniques were earlier found to havesimilar performance in CLIR tasks based on directtranslation, with a preference for the data fusion ap-proach (Jones and Lam-Adesina, 2002).5 Translation and Ranking Data5.1 Parallel Translation DataFor Japanese-to-English patent translation we useddata provided by the organizers of the NTCIR9workshop for the JP-EN PatentMT subtask.
In par-ticular, we used the data provided for NTCIR-7 (Fu-jii et al 2008), consisting of 1.8 million parallelsentence pairs from the years 1993-2002 for train-ing.
For parameter tuning we used the develop-ment set of the NTCIR-8 test collection, consistingof 2,000 sentence pairs.
The data were extractedfrom the description section of patents publishedby the Japanese Patent Office (JPO) and the UnitedStates Patent and Trademark Office (USPTO) by themethod described in Utiyama and Isahara (2007).Japanese text was segmented using the MeCab10toolkit.
Following Feng et al(2011), we applieda modified version of the compound splitter de-scribed in Koehn and Knight (2003) to katakanaterms, which are often transliterations of Englishcompound words.
As these are usually not split byMeCab, they can cause a large number of out-of-vocabulary terms.9http://research.nii.ac.jp/ntcir/ntcir/10https://code.google.com/p/mecab/1693#queries #relevant #unique docstrain 107,061 1,422,253 888,127dev 2,000 26,478 25,669test 2,000 25,173 24,668Table 1: Statistics of ranking data.For the English side of the training data, we ap-plied a modified version of the tokenizer included inthe Moses scripts.
This tokenizer relies on a list ofnon-breaking prefixes which mark expressions thatare usually followed by a ?.?
(period).
We cus-tomized the list of prefixes by adding some abbrevi-ations like ?Chem?, ?FIG?
or ?Pat?, which are spe-cific to patent documents.5.2 Ranking Data from Patent CitationsGraf and Azzopardi (2008) describe a method to ex-tract relevance judgements for patent retrieval frompatent citations.
The key idea is to regard patent doc-uments that are cited in a query patent, either by thepatent applicant, or by the patent examiner or in apatent office?s search report, as relevant for the querypatent.
Furthermore, patent documents that are re-lated to the query patent via a patent family relation-ship, i.e., patents granted by different patent author-ities but related to the same invention, are regardedas relevant.
We assign three integer relevance levelsto these three categories of relationships, with high-est relevance (3) for family patents, lower relevancefor patents cited in search reports by patent examin-ers (2), and lowest relevance level (1) for applicants?citations.
We also include all patents which are inthe same patent family as an applicant or examinercitation to avoid false negatives.
This methodol-ogy has been used to create patent retrieval data atCLEF-IP11 and proved very useful to automaticallycreate a patent retrieval dataset for our experiments.For the creation of our dataset, we used theMAREC12 citation graph to extract patents in cita-tion or family relation.
Since the Japanese portionof the MAREC corpus only contains English ab-stracts, but not the Japanese full texts, we mergedthe patent documents in the NTCIR-10 test collec-tion described above with the Japanese (JP) section11http://www.ifs.tuwien.ac.at/?clef-ip/12http://www.ifs.tuwien.ac.at/imp/marec.shtmlof MAREC.
Title, abstract, description and claimswere added to the MAREC-JP data if the docu-ment was available in NTCIR.
In order to keep par-allel data for SMT training separate from rankingdata, we used only data from the years 2003-2005to extract training data for ranking, and two smalldatasets of 2,000 queries each from the years 2006-2007 for development and testing.
Table 1 gives anoverview over the data used for ranking.
For de-velopment and test data, we randomly added irrele-vant documents from the NTCIR-10 collection untilwe obtained two pools of 100,000 documents.
Thenecessary information to reproduce the exact train,development and test data samples is downloadablefrom authors?
webpage13.The experiments reported here use only the ab-stract of the Japanese and English patents in ourtraining, development and test collection.6 Experiments6.1 System DevelopmentSystem development and evaluation in our exper-iments was done on the ranking data describedin the previous section (see Table 1).
We reportMean Average Precision (MAP) scores, using thetrec eval (ver.
8.1) script from the TREC evalu-ation campaign14, with a limit of top K = 1, 000 re-trieved documents for each query.
Furthermore, weuse the Patent Retrieval Evaluation Score (PRES)15introduced by Magdy and Jones (2010).
This met-ric accounts for both precision and recall.
In thestudy by Magdy and Jones (2010), PRES agreedwith MAP in almost 80% of cases, and both agreedon the ranks of the best and the worst IR system.Both MAP and PRES scores are reported in the samerange [0, 1], and 0.01 stands for 1 MAP (PRES)point.
Statistical significance of pairwise systemcomparisons was assessed using the paired random-ization test (Noreen, 1989; Smucker et al 2007).For each system, optimal meta-parameter settingswere found by choosing the configuration with high-est MAP score on the development set.
These results13http://www.cl.uni-heidelberg.de/statnlpgroup/boostclir14http://trec.nist.gov/trec_eval15http://www.computing.dcu.ie/?wmagdy/Scripts/PRESeval.htm1694method MAP PRESdev test dev test1 DT 0.2636 0.2555 0.5669 0.56812 PSQ lexical table 0.2520 0.2444 0.5445 0.54983 PSQ n-best table 0.2698 0.2659 0.5789 0.5851Boost-1g 0.2064 1230.1982 0.5850 120.6122Boost-2g 0.2526 30.2474 0.6900 1230.7196Table 2: MAP and PRES scores for CLIR methods (bestconfigurations) on the development and test sets.
Prefixednumbers denote statistical significance of a pairwise com-parison with the baseline indicated by the superscript.
Forexample, the bottom right result shows that Boost-2g issignificantly better than DT (method 1), PSQ lexical ta-ble (method 2) and PSQ n-best table (method 3).
(together with PRES results) are shown in the sec-ond and fourth column of Table 2.The direct translation approach (DT) was devel-oped in three configurations: no stopword filtering,small stopword list (52 words) and a large stopwordlist (543 words).
The last configuration achieved thehighest score (MAP 0.2636).The probabilistic structured query (PSQ) ap-proach was developed using the lexical translationtable and the translation table estimated on the de-coder?s n-best list, both optionally pruned with avariable lower pL and cumulative pC threshold onthe word pair probability in the table (Section 3.2).A further meta-parameter of PSQ was whether to usestandard or unique n-best lists.
Finally, all variantswere coupled with the same stopword filters as inthe DT approach.
The configurations that achievedthe highest scores were: MAP 0.2520 for PSQ witha lexical table (pL = 0.01, pC = 0.95, no stop-word filtering), and MAP 0.2698 for PSQ with atranslation table estimated on the n-best list (pL =0.005, pC = 0.95, large stopword list).
Interpolat-ing between lexical and n-best tables did not im-prove results in our experiments, thus we set ?
= 1in equation (4).Each SMT-based system was run with 4 differentMERT optimizations, leading to variations of lessthan 1 MAP point for each system.
The best con-figurations for DT and PSQ on the development setwere fixed and used for evaluation on the test set.Training of the boosting approach (Boost) wasdone in parallel on bootstrap samples from the train-ing data.
First, a query q (i.e., a Japanese abstract)was sampled uniformly from all training queries.method MAP PRESdev test dev testDT + PSQ n-best 0.2778 ?0.2726 0.5884 ?0.5942DT + Boost-1g 0.2778 ?0.2728 0.6157 ?0.6225DT + Boost-2g 0.3309 ?0.3300 0.7132 ?0.7279PSQ lexical + Boost-1g 0.2695 ?0.2653 0.6068 ?0.6131PSQ lexical + Boost-2g 0.3215 ?0.3187 0.7071 ?0.7240PSQ n-best + Boost-1g 0.2863 ?0.2850 0.6309 ?0.6402PSQ n-best + Boost-2g 0.3439 ?0.3416 0.7212 ?0.7376Table 3: MAP and PRES scores of the aggregated mod-els on the development and test sets.
Development scorescorrespond to peaks in Figures 1 and 3, respectively, forMAP and PRES; test scores are given for the ?
?s deliv-ering these peaks on the development set.
Prefixed ?
in-dicates statistical significance of the result difference be-tween aggregated system and the respective translation-based system used in the aggregation.Then we sampled independently and uniformly arelevant document d+ (i.e., an English abstract)from the English patents marked relevant for theJapanese patent, and a random document d?
fromthe whole pool of English patent abstracts.
If d?had a relevance score greater or equal to the rele-vance score of d+, it was resampled.
The initial im-portance weight D0 for a triplet (q,d+,d?)
was setto the positive difference in relevance scores for d+and d?.
Each bootstrap sample consisted of 10 pairsof documents for each of 10, 000 queries, resultingin 100, 000 training instances per sample.The Boost approach was developed for uni-gramand combined uni- and bi-gram versions.
We ob-served that the performance of the Boost methodcontinuously improved with the number of iterationsT and with the number of samples S, but saturatedat about 15-20 samples without visible over-fittingin the tested range of T .
Therefore we arbitrarilystopped training after obtaining 5, 000 features persample, and used 35 samples for uni-gram versionand 65 samples for the combined bi-gram version,resulting in models with 104K and 172K unique fea-tures, respectively.
The optimal values for the pass-through weight ?
were found to be 0.3 and 0.2 forthe uni-gram and bi-gram models on the develop-ment set.
The best configuration of uni-gram andbi-gram model achieved MAP scores of 0.2064 and0.2526 the development set.
Using stopword filtersduring training did not improve the results here.16950.240.250.260.270.280.290.300.310.320.330.340.350  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1MAP?DT + Boost-2gPSQ lexical + Boost-2gPSQ n-best + Boost-2gDT, 0.2636PSQ lexical, 0.2520PSQ n-best, 0.2698Boost-2g, 0.2526PSQ n-best + DTFigure 1: MAP rank aggregation for combinations of thebi-gram boosting and the baselines on the dev set.0.240.250.260.270.280.290.300.310.320.330.340.350  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1MAP?dev, PSQ n-best + Boost-2gtest, PSQ n-best + Boost-2gdev, PSQ n-best, 0.2698test, PSQ n-best, 0.2659dev, Boost-2g, 0.2526test, Boost-2g, 0.2474Figure 2: MAP rank aggregation for the bi-gram boostingand the ?PSQ n-best table?
approach on dev and test sets.0.540.560.580.600.620.640.660.680.700.720  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1PRES?DT + Boost-2gPSQ lexical + Boost-2gPSQ n-best + Boost-2gDT, 0.5669PSQ lexical, 0.5445PSQ n-best, 0.5789Boost-2g, 0.6900PSQ n-best + DTFigure 3: PRES rank aggregation for combinations of thebi-gram boosting and the baselines on the dev set.6.2 Testing and Model CombinationThe third and the fifth columns of Table 2 give acomparison of the MAP scores of the baseline ap-proaches and the Boost model evaluated individu-ally on the test set.
Each score corresponds to thebest configuration found on the development set.
Wesee that the PSQ approach using n-best lists for pro-jection outperforms all other methods in terms ofMAP, but loses to both Boost approaches when eval-uated with PRES.
Direct translation is about 1 MAPpoint lower than PSQ n-best; Boost with combineduni- and bi-grams is another 0.8 MAP points worse,but is better in terms of PRES, especially for the bi-gram version.
Given the fact that the complex SMTsystem behind the direct translation and PSQ ap-proach is trained and tuned on very large in-domaindatasets, the performance of the bare phrase tableinduced by the Boost method is respectable.Our best results are obtained by a combinationof the orthogonal information sources of the SMTand the Boost approaches.
We evaluated the BordaCount aggregation scheme on the development datain order to find the optimal value for ?
?
[0, 1].
Theinterpolation was done for the best combined uni-and bi-gram boosting model with the best variants ofthe DT and PSQ approaches.
As can be seen fromFigures 1 and 3, rank aggregation by Borda Countoutperforms both individual approaches by a largemargin.
Figure 2 verifies that the results are trans-ferable from the development set to the test set.
Thebest performing system combination on the develop-ment data is also optimal on the test data.Table 3 shows the retrieval performance of thebest baseline model (PSQ n-best) combined withthe best Boost model (bi-gram), with an impres-sive gain of over 7 MAP points (15 PRES points)over the best individual baseline result from Table 2.Even when, according to the PRES measure (Fig-ure 3), the Boost-2g system is better on its own, in-jecting complementary information from the PSQ orDT approach still contributes several points.
Simi-lar gains are obtained by model combination of theDT approach with the best Boost model.
However,a combination of the SMT-based CLIR approachesDT and PSQ barely improved results over the bestinput model.
In summary, aggregating rankings ishelpful for orthogonal systems, but not for systemsincluding similar information.16966.3 AnalysisTable 4 lists some of the top-200 selected featuresfor the boosting approach (the most common trans-lation of the Japanese term is put in subscript).We see that the direct ranking approach is ableto penalize uni- and bi-gram cooccurrences that areharmful for retrieval by assigning them a negativeweight, e.g., the pairing of?
?resolution with image.Pairs of uni- and bi-grams that are useful for re-trieval are boosted by positive weights, e.g., the pair?
?compression,?machine and compressor captures animportant compound.
Further examples, not shownin the table, are matches of the same source (tar-get) n-gram with several different target (source) n-grams, e.g., the Japanese term ?
?image is pairednot only with its main translation, but also withdozens of related notions: video, picture, scanning,printing, photosensitive, pixel, background etc.
Thishas a query expansion effect that is not possible insystems that use one translation or a small list of n-best translations.
In addition, associations of sourcen-grams with overlapping target n-grams help boostthe final score: e.g., the same term?
?image is pos-itively paired with target bi-grams as {an,original},{original,image} and {image,for}.
This has the ef-fect of compensating for the lack of handling phraseoverlaps in an SMT decoder.7 ConclusionWe presented a boosting approach to induce a tableof bilingual n-gram correspondences by direct pref-erence learning on relevance rankings.
This tablecan be seen as a phrase table that encodes word-based information that is orthogonal and comple-mentary to the information in standard translation-based CLIR approaches.
We compared our boostingapproach to very competitive CLIR baselines thatuse a complex SMT system trained and tuned onlarge in-domain datasets.
Furthermore, our patentretrieval setup gives SMT-based approaches an ad-vantage in that queries consist of several normal-length sentences, as opposed to the short queriescommon to web search.
Despite this and despite thetiny size (about 170K parameters) of the boostingphrase table, compared to standard SMT phrase ta-bles, this approach reached performance similar todirect translation using a full SMT model in termst ht (uni- & bi-grams) wt1 ?layer - layer 1.292 ??
?data - data 1.133 ?
?circuit - circuit 1.1376 ?in - voltage -0.3977 ?guide,?power - conductive 1.2581 ?
?resolution - image -0.2599 ?
?speed - transmission 1.68100 ?
?LCD - liquid,crystal 1.73123 ?power - force 0.91124 ?
?compression,?machine - compressor 2.83132 ???
?cable - cable 1.81133 ?hyper,?
?sound wave - ultrasonic 3.34169 ?
?particle - particles 1.57170 ?
?calculation - for,each 1.14184 ??
?rotor - rotor 2.01185 ?
?detection,?vessel - detector 1.43Table 4: Examples of the features found by boosting.of MAP, and was significantly better in terms ofPRES.
Overall, we obtained the best results by amodel combination using consensus- based votingwhere the best SMT-based approach was combinedwith the boosting phrase table (gaining more than 7MAP or 15 PRES points).
We attribute this to thefact that the boosting approach augments SMT ap-proaches with valuable information that is hard toget in approaches that are agnostic about the rank-ing data and the ranking task at hand.The experimental setup presented in this paperuses relevance links between patent abstracts asranking data.
While this technique is useful to de-velop patent retrieval systems, it would be interest-ing to see if our results transfer to patent retrievalscenarios where full patent documents are used in-stead of only abstracts, or to standard CLIR scenar-ios that use short search queries in retrieval.AcknowledgementsThe research presented in this paper was supportedin part by DFG grant ?Cross-language Learning-to-Rank for Patent Retrieval?.
We would like to thankEugen Ruppert for his contribution to the rankingdata construction.1697ReferencesJaved A. Aslam and Mark Montague.
2001.
Models formetasearch.
In Proceedings of the ACM SIGIR Con-ference on Research and Development in InformationRetrieval (SIGIR?01), New Orleans, LA.Bing Bai, Jason Weston, David Grangier, RonanCollobert, Kunihiko Sadamasa, Yanjun Qi, OlivierChapelle, and Kilian Weinberger.
2010.
Learning torank with (a lot of) word features.
Information Re-trieval Journal, 13(3):291?314.Lisa Ballesteros and W. Bruce Croft.
1998.
Resolvingambiguity for cross-language retrieval.
In Proceedingsof the ACM SIGIR Conference on Research and De-velopment in Information Retrieval (SIGIR?98), Mel-bourne, Australia.Nicholas J. Belkin, Paul Kantor, Edward A.
Fox, andJoseph A. Shaw.
1995.
Combining the evidenceof multiple query representations for information re-trieval.
Inf.
Process.
Manage., 31(3):431?448.Leo Breiman.
1996.
Bagging predictors.
Journal of Ma-chine Learning Research, 24:123?140.Kevin Canini, Tushar Chandra, Eugene Ie, Jim McFad-den, Ken Goldman, Mike Gunter, Jeremiah Harm-sen, Kristen LeFevre, Dmitry Lepikhin, Tomas LloretLlinares, Indraneel Mukherjee, Fernando Pereira, JoshRedstone, Tal Shaked, and Yoram Singer.
2010.Sibyl: A system for large scale machine learning.In LADIS: The 4th ACM SIGOPS/SIGACT Workshopon Large Scale Distributed Systems and Middleware,Zurich, Switzerland.Xi Chen, Bing Bai, Yanjun Qi, Qihang Ling, and JaimeCarbonell.
2010.
Learning preferences with millionsof parameters by enforcing sparsity.
In Proceedingsof the IEEE International Conference on Data Mining(ICDM?10), Sydney, Australia.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Jeffrey Chin, Maureen Heymans, Alexandre Kojoukhov,Jocelyn Lin, and Hui Tan.
2008.
Cross-languageinformation retrieval.
Patent Application.
US2008/0288474 A1.Michael Collins and Terry Koo.
2005.
Discrimina-tive reranking for natural language parsing.
Compu-tational Linguistics, 31(1):25?69.Michael Collins, Robert E. Schapire, and Yoram Singer.2002.
Logistic regression, AdaBoost and Bregmandistances.
Journal of Machine Learning Research,48(1-3):253?285.Kareem Darwish and Douglas W. Oard.
2003.
Proba-bilistic structured query methods.
In Proceedings.
ofthe ACM SIGIR Conference on Research and Devel-opment in Information Retrieval (SIGIR?03), Toronto,Canada.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010. cdec: Adecoder, alignment, and learning framework for finite-state and context-free translation models.
In Proceed-ings of the ACL 2010 System Demonstrations, Upp-sala, Sweden.Minwei Feng, Christoph Schmidt, Joern Wuebker,Stephan Peitz, Markus Freitag, and Hermann Ney.2011.
The RWTH Aachen system for NTCIR-9PatentMT.
In Proceedings of the NTCIR-9 Workshop,Tokyo, Japan.Yoav Freund, Ray Iyer, Robert E. Schapire, and YoramSinger.
2003.
An efficient boosting algorithm forcombining preferences.
Journal of Machine LearningResearch, 4:933?969.Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, andTakehito Utsuro.
2008.
Overview of the patent trans-lation task at the NTCIR-7 workshop.
In Proceedingsof NTCIR-7 Workshop Meeting, Tokyo, Japan.Yasser Ganjisaffar, Rich Caruana, and Cristina VideiraLopes.
2011.
Bagging gradient-boosted trees forhigh precision, low variance ranking models.
In Pro-ceedings of the ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR?11),Beijing, China.Jianfeng Gao, Jian-Yun Nie, Endong Xun, Jian Zhang,Ming Zhou, and Changning Huang.
2001.
Improv-ing query translation for cross-language informationretrieval using statistical models.
In Proceedings ofthe ACM SIGIR Conference on Research and Devel-opment in Information Retrieval (SIGIR?01), New Or-leans, LA.Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian Hu,Kam-Fai Wong, and Hsiao-Wuen Hon.
2007.
Cross-lingual query suggestion using query logs of differentlanguages.
In Proceedings of the ACM SIGIR Con-ference on Research and Development in InformationRetrieval (SIGIR?07), Amsterdam, The Netherlands.Sharad Goel, John Langford, and Alexander L. Strehl.2008.
Predictive indexing for fast search.
In Advancesin Neural Information Processing Systems, Vancouver,Canada.Erik Graf and Leif Azzopardi.
2008.
A methodologyfor building a patent test collection for prior art search.In Proceedings of the 2nd International Workshop onEvaluating Information Access (EVIA), Tokyo, Japan.David Grangier and Samy Bengio.
2008.
A discrimi-native kernel-based approach to rank images from textqueries.
IEEE Transactions on Pattern Analysis andMachine Intelligence (PAMI), 30(8):1371?1384.Yunsong Guo and Carla Gomes.
2009.
Ranking struc-tured documents: A large margin based approach for1698patent prior art search.
In Proceedings of the Interna-tional Joint Conference on Artificial Intelligence (IJ-CAI?09), Pasadena, CA.Kenneth Heafield.
2011.
KenLM: faster and smaller lan-guage model queries.
In Proceedings of the EMNLP2011 Sixth Workshop on Statistical Machine Transla-tion (WMT?11), Edinburgh, UK.Gareth J.F.
Jones and Adenike M. Lam-Adesina.
2002.Combination methods for improving the reliability ofmachine translation based cross-language informationretrieval.
In Proceedings of the 13th Irish Interna-tional Conference on Artificial Intelligence and Cog-nitive Science (AICS?02), Limerick, Ireland.Philipp Koehn and Kevin Knight.
2003.
Empirical meth-ods for compound splitting.
In Proceedings of theConference on European Chapter of the Associationfor Computational Linguistics (EACL?03), Budapest,Hungary.Victor Lavrenko, Martin Choquette, and W. Bruce Croft.2002.
Cross-lingual relevance models.
In Proceed-ings of the ACM Conference on Research and Devel-opment in Information Retrieval (SIGIR?02), Tampere,Finland.Adam Lopez.
2007.
Hierarchical phrase-based trans-lation with suffix arrays.
In Proceedings of theJoint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL 2007), Prague,Czech Republic.Walid Magdy and Gareth J.F.
Jones.
2010.
PRES: ascore metric for evaluating recall-oriented informationretrieval applications.
In Proceedings of the ACM SI-GIR conference on Research and development in in-formation retrieval (SIGIR?10), New York, NY.Walid Magdy and Gareth J. F. Jones.
2011.
An efficientmethod for using machine translation technologies incross-language patent search.
In Proceedings of the20th ACM Conference on Informationand KnowledgeManagement (CIKM?11), Glasgow, Scotland, UK.Vassilina Nikoulina, Bogomil Kovachev, Nikolaos Lagos,and Christof Monz.
2012.
Adaptation of statisticalmachine translation model for cross-lingual informa-tion retrieval in a service context.
In Proceedings ofthe 13th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL?12),Avignon, France.Eric W. Noreen.
1989.
Computer Intensive Methodsfor Testing Hypotheses.
An Introduction.
Wiley, NewYork.Franz Josef Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of the 38thMeeting of the Association for Computational Linguis-tics (ACL?00), Hongkong, China.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Meeting on Association for Computational Lin-guistics (ACL?03), Sapporo, Japan.Dmitry Pavlov, Alexey Gorodilov, and Cliff A. Brunk.2010.
Bagboo: a scalable hybrid bagging-the-boosting model.
In Proceedings of the 19th ACMInternational Conference on Information and Knowl-edge Management (CIKM?10), Toronto, Canada.Stephen E. Robertson, Steve Walker, and MichelineHancock-Beaulieu.
1998.
Okapi at TREC-7.
InProceedings of the Seventh Text REtrieval Conference(TREC-7), Gaithersburg, MD.Saharon Rosset, Ji Zhu, and Trevor Hastie.
2004.
Boost-ing as a regularized path to a maximum margin clas-sifier.
Journal of Machine Learning Research, 5:941?973.Robert E. Schapire and Yoram Singer.
1999.
Im-proved boosting algorithms using confidence-ratedpredictions.
Journal of Machine Learning Research,37(3):297?336.Qinfeng Shi, James Petterson, Gideon Dror, John Lang-ford, Alexander J. Smola, Alexander L. Strehl, andVishy Vishwanathan.
2009.
Hash Kernels.
In Pro-ceedings of the 12th Int.
Conference on Artificial In-telligence and Statistics (AISTATS?09), Irvine, CA.Mark D. Smucker, James Allan, and Ben Carterette.2007.
A comparison of statistical significance tests forinformation retrieval evaluation.
In Proceedings of the16th ACM conference on Conference on Informationand Knowledge Management (CIKM ?07), New York,NY.Ferhan Ture, Jimmy Lin, and Douglas W. Oard.
2012a.Combining statistical translation techniques for cross-language information retrieval.
In Proceedings of theInternational Conference on Computational Linguis-tics (COLING 2012), Bombay, India.Ferhan Ture, Jimmy Lin, and Douglas W. Oard.
2012b.Looking inside the box: Context-sensitive translationfor cross-language information retrieval.
In Proceed-ings of the ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR 2012),Portland, OR.Masao Utiyama and Hitoshi Isahara.
2007.
A Japanese-English patent parallel corpus.
In Proceedings of MTSummit XI, Copenhagen, Denmark.Jinxi Xu, Ralph Weischedel, and Chanh Nguyen.
2001.Evaluating a probabilistic model for cross-lingual in-formation retrieval.
In Proceedings of the ACM SIGIRConference on Research and Development in Informa-tion Retrieval (SIGIR?01), New York, NY.1699
