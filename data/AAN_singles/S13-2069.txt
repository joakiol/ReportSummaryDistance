Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 418?424, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics[LVIC-LIMSI]: Using Syntactic Features and Multi-polarity Words forSentiment Analysis in TwitterMorgane Marchand1,2, Alexandru Lucian Ginsca1, Romaric Besanc?on1, Olivier Mesnard1(1) CEA-LIST, DIASI, LVICCEA SACLAY - Nano-INNOV - Bt.
861 - Point courrier 17391191 Gif-sur-Yvette Cedex, France(2) LIMSI-CNRSBat 508, BP133,91403 Orsay Cedexmorgane.marchand@cea.fr; alexandru.ginsca@cea.frromaric.besancon@cea.fr; olivier.mesnard@cea.frAbstractThis paper presents the contribution of ourteam at task 2 of SemEval 2013: SentimentAnalysis in Twitter.
We submitted a con-strained run for each of the two subtasks.
In theContextual Polarity Disambiguation subtask,we use a sentiment lexicon approach combinedwith polarity shift detection and tree kernelbased classifiers.
In the Message Polarity Clas-sification subtask, we focus on the influenceof domain information on sentiment classifica-tion.1 IntroductionIn the past decade, new forms of communication,such as microblogging and text messaging haveemerged and became ubiquitous.
These short mes-sages are often used to share opinions and sentiments.The Sentiment Analysis in Twitter task promotes re-search that will lead to a better understanding of howsentiment is conveyed in tweets and texts.
In thispaper, we describe our contribution at task 2 of Se-mEval 2013 (Wilson et al 2013).
For the ContextualPolarity Disambiguation subtask, covered in section2, we use a system that combines a lexicon basedapproach to sentiment detection with two types ofsupervised learning methods, one used for polarityshift identification and one for tweet segment classi-fication in the absence of lexicon words.
The thirdsection presents the Message Polarity Classificationsubtask.
We focus here on the influence of domaininformation on sentiment classification by detectingwords that change their polarity across domains.2 Task A: Contextual PolarityDisambiguationIn this section we present our approach for the con-textual polarity disambiguation task in which, givena message containing a marked instance of a word ora phrase, the system has to determine whether thatinstance is positive, negative or neutral in that con-text.
For this task, we submitted a single run usingonly the tweets provided by the organizers.2.1 System descriptionBased on the predominant strategy, sentiment anal-ysis systems can be divided into those that focus onsentiment lexicons together with a set of rules andthose that rely on machine learning techniques.
Forthis task, we use a mixed approach in which we firstfilter the tweets based on the occurrences of wordsfrom a sentiment lexicon and then apply differentsupervised learning methods on the grounds of thisinitial classification.
In Figure 1 we detail the work-flow of our system.
We use the + , ?
and ?
symbolsto denote a positive, negative and neutral tweet seg-ment, respectively.
Also, we use the a?
b notationwhen referring to a polarity shift from a to b.2.1.1 Data preprocessingThe language used in Twitter presents some partic-ularities, such as the use of hashtags or user mentions.In order to maximize the efficiency of language pro-cessing methods, such as lemmatization and syntacticparsing, we perform several normalization steps.
Weremove the # symbol, all @ mentions and links andperform lower case conversion.
Also, if a vowel isrepeated more than 3 times in a word, we reduce it to418Figure 1: Contextual polarity disambiguation task system descriptiona single occurrence and we reduce multiple consecu-tive punctuation marks to a single one.
Finally, welemmatize the normalized text.Emoticons have been successfully used as senti-ment indicators in tweets (Davidov et al 2010).
Inour approach, we map a set of positive emoticons tothe word good and a set of negative emoticons to theword bad.
We use the following sets of emoticons:?
Positive emoticons: :) , :-) , :D , =) , :?)
, :o) , :P, >:) , :?>, >:|, <3 , ;>, ;) , ;-) , ;>, (: , (;?
Negative emoticons: :( , : ( , :-( , :?
( , :/ , :<, ;(Traits of informal language have been used as fea-tures in Twitter sentiment classification tasks (Goet al 2009).
In order to avoid the loss of possi-ble useful information, we keep record of the per-formed normalizations as binary features associatedto a tweet segment.
We retain the following set of fea-tures: hasPositiveEmoticon, hasNegativeEmoticon,hasHashtag, hasAtSign, hasConsecutivePunctuation,hasConsecutiveVowels, hasUpperCaseWords.2.1.2 Classification methodsIn a first step, we select tweet segments that con-tain at least one word from a lexicon and assign toit the polarity of that word.
If there are more thanone sentiment words with different polarities in thesegment, we keep the most frequent polarity and inthe few cases where there is an equal number of posi-tive and negative words, we take the polarity of thelast one.
Next, we look for negation indicators (e.g.not, ?t) using a set of words and rules and replacethem with the NEG token.
We then identify instanceswhere there is a shift between the polarity predictedfrom the lexicon and the one from the ground truth.In order to account for the unbalanced datasets(e.g.192 instances where there is a +?
?
shift and 3188where the positive instance was correctly identifiedfrom the lexicon) we use cost sensitive classifiers.
Wedefine a cost matrix in which the cost of the classifiermaking a false positive error is three times higherthan a false negative error.
Using this approach weguide the classifier to provide less but more confidentpredictions for the existence of a polarity shift whileallowing it to make more errors when predicting theabsence of a shift.
For these classifiers, we use a Bagof Words representation of the lemmatized segments.When a word from the sentiment lexicon does notappear in the tweet segment, we use a one vs. allclassification approach with a SVM classifier andtree kernels.
The tree kernel is a function betweentwo trees that computes a normalized similarity scorein the range [0,1] (Culotta and Sorensen, 2004).
Forour task, we use an implementation of tree kernelsfor syntactic parse trees (Moschitti, 2006) that is builton top of the SVM-Light library (Joachims, 1999) ina similar manner to that presented in (Ginsca, 2012).We build the syntactic parse trees with the StanfordCoreNLP library (Klein and Manning, 2003).2.2 Evaluation and ResultsFor the experiments presented in this section, wemerge the training and development datasets and forthe polarity shift and sentiment classification experi-ments we report the results using a 5-fold cross vali-dation technique over the resulting dataset.4192.2.1 Lexicon choice influenceConsidering that the selection of a lexicon playsan important role on the performance of our system,we tested 3 widely used sentiment lexicons: Sen-tiWordNet 3 (Baccianella et al 2010), Bing Liu?sOpinion Lexicon (Hu and Liu, 2004) and MPQASubjectivity Lexicon (Wilson et al 2005).
Differentcombinations of these lexicons were tried and in Ta-ble 1 we present the top performing ones.
Besidesthe F-Measure for positive (Fp) and negative (Fn)instances, we also list the percentage of instances inwhich appears at least one word from the lexicon.SentiWordnet appoints polarity weights to words,ranging from 0 to 1.
An important parameter is thethreshold over which a word is considered to have acertain polarity.
We tested several values (from 0.5 to0.9 with a step of 0.05) and the best results in termsof F-Measure were obtained for a threshold of 0.75.Our finding is consistent with the value suggested in(Chen et al 2012).Lexicon Found(%) Fp FnLiu 55.7 0.93 0.85MPQA 61.4 0.89 0.76SentWN 79.4 0.86 0.78Liu+MPQA 67.1 0.89 0.78Liu+SentWN 79.4 0.87 0.81Liu+MPQA+SentWN 79.4 0.86 0.81Table 1: Influence of lexicon on the F-Measure for positiveand negative segments2.2.2 Polarity shift experimentsWe tested several classifiers using the Weka toolkit(Hall et al 2009) and found that the best resultswere obtained with the Sequential Minimal Optimiza-tion (SMO) classifier.
For instance, when classifying+ ?
?
shifts, SMO correctly identified 91 out of192 polarity shifts in contrast with 68 and 41 detectedby a Random Forests and a Naive Bayes classifier,respectively.
For the +?
?
classification, the SMOclassifier finds 2 out of 34 shifts, for ?
?
+, 15 outof 238 and for ?
?
?, 2 out of 32 shifts are found.After changing the polarity of sentiment segments asfound by the 4 classifiers, we obtain an increase inF-Measure from 0.930 to 0.947 for positive segmentsand from 0.851 to 0.913 for negative segments.
Ourchoice of the Bag of Words model instead of a parsetree representation for these classifiers is justified bythe poor performance of tree kernels when dealingwith unbalanced data.2.2.3 Sentiment classification experimentsModel Class Avg.
F-scoreBasic Treepositive 0.780negative 0.645neutral 0.227Tree + Numericpositive 0.768negative 0.590neutral 0.132Tree + Context 2positive 0.801negative 0.676neutral 0.231Table 2: Comparison between different models used forsegment polarity classificationIn a series of preliminary experiments, we testedseveral classifiers trained on a Bag of Words modeland an SVM classifier with a tree kernel.
We foundthat the parse tree representation of a tweet segmentprovided a higher accuracy.
This shows that althoughsmall, when a segment contains more than one word,its syntactic structure becomes a relevant feature.In Table 2 we compare the results of 3 tree basedmodels.
In the Basic Tree model, we use only thesyntactic parse tree representation of a tweet seg-ment.
For the Tree + Numeric model, we use theinitial tree kernel together with a polynomial kernelon the binary structure features presented in section2.1.1.
In the Tree + Context model, we include in theparse tree, besides the given section, k tokens (words,punctuation) from the whole tweet that surround theselected segment.
We performed tests with k from 1to 5 and obtained the best results with a k value of 2.2.2.4 Competition resultsFor the Twitter dataset, we ranked 4th out of23 groups that submitted constrained runs.
Whencombining the results of the constrained and uncon-strained submissions, our run was ranked 5th out ofa total of 29 submissions.
For the SMS dataset, weranked 5th out of a total of 18 groups for the con-strained setting and our submission was ranked 5thout of 24 combined runs.
In Table 3, we detail theresults we obtained on the competition test datasets.420Class P R F-scoreTwitter positive 0.8623 0.9140 0.8874Twitter negative 0.8453 0.8086 0.8265Twitter neutral 0.4127 0.1625 0.2332SMS positive 0.7107 0.8945 0.7921SMS negative 0.8687 0.7609 0.8112SMS neutral 0.3684 0.0440 0.0787Table 3: Competition results overview on the Twitter andSMS datasets2.3 DiscussionThe robustness of our approach is proved by thelow standard deviation of the F-Measure scores ob-tained over each of the the 5 folds used for evaluation(0.026) but also by the small difference between theresults we obtained during the development phaseand those reported on the competition test dataset.The choice of lexicons results in a trade-off betweenthe percentage of instances classified with either thelexicon and polarity shift or the supervised learningmethod.
Although the first one yields better resultsand it is apparently desirable to have a better cover-age of lexicon terms, this would reduce the number ofinstances for training a classifier leading to a poorerperformance of this approach.3 Task B: Message Polarity ClassificationIn this section, we present our approach for the mes-sage polarity classification task in which, given amessage, the system has to determine whether it ex-presses a positive, negative, or neutral sentiment.
Asfor Task A, we submitted a single constrained run.3.1 Preprocessing of the corporaWe use as training corpora the training data, mergedwith the development data.
After the deletion oftweets no longer available, our final training set con-tains 10402 tweets: 3855 positive, 1633 negative and4914 objective or neutral.
In the preprocessing step,we first remove the web addresses from the tweets toreduce the noise.
Then, we extract the emoticons andcreate new features with the number of occurrencesof each type of emoticon.
The different emoticonstypes are presented in Table 4.
Then, we lemmatizethe text using LIMA, a linguistic analyzer of CEALIST (Besanc?on et al 2010).
:-) :) =) X) x) Smile:-( :( =( Sadness:-D :D =D X-D XD x-D xD :?)
Laugh;-) ;) Wink< 3 Heart:?-( :?
( =?
( TearTable 4: Common emoticon types3.2 Boostexter baselineTo classify the tweets, we used the BoosTexter1 clas-sifier (Schapire and Singer, 2000) in its discrete Ad-aBoost.MH version, setting the number of iterationsto 1000.
We used two types of features: a Bag ofWords of lemmatized uni-, bi- and tri-grams and thenumber of occurrences of each emoticon type.Bog of words features Emoticon type featurewow lady gaga be great Smile 1Table 5: Example of tweet representationBoostexter is designed to maximize the accuracy,not the F-score, which is the chosen evaluation metricfor this task.
As the training data contain few negativeexamples, the classifier tends to under-detect thisclass.
In order to favour the negative class detection,we balance the training corpora.
So our final systemis trained on 4899 tweets (1633 of each class, chosenrandomly).
The accuracy results are not presentedhere.
However, the gain between our baseline andour final system has the same order of magnitude.3.3 Integration of domain informationSome words can change their polarity between twodifferent domains (Navigli, 2012; Yoshida et al2011).
For example, the word ?return?
is positivein ?I can?t wait to return to my book?.
However, it isoften very negative when we are talking about someelectronics device, as in ?I had to return my phoneto the store?.
This phenomenon happens even inmore closely related domains: ?I was laughing all thetime?
is a good point for a comedy film but a bad onefor a horror film.
We call such words or expressions?multi-polarity words?.
This phenomenon is different1BoosTexter is a general purpose machine-learning programbased on boosting for building a classifier from text.421from polysemy, as a word can keep the same meaningacross domains while changing its polarity and it canlead to classification error (Wilson et al 2009).
In(Marchand, 2013), we have shown, on a corpus of re-views, that a sensible amount of multi-polarity wordsinfluences the results of common opinion classifiers.Their deletion or their differentiation leads to betterclassification results.
Here, we test this approach ona corpus of tweets.3.3.1 Domain generation with LDAIn order to apply our method, we need to assigndomains to tweets.
For that purpose, we use LatentDirichlet Allocation (LDA) (Blei et al 2003).
Weused the Mallet LDA implementation (McCallum,2002).
The framework uses Gibbs sampling to con-stitute the sample distributions that are exploited forthe creation of the topic models.
The models are builtusing the lemmatized tweets from the training anddevelopment data.
We performed tests with a numberof domains ranging from 5 to 25, with a step of 5.Each LDA representation of a tweet is encoded byinferring a domain distribution.
For example, if amodel with 5 domains is used, we generate a vectorof length 5, where each the i-th value is the propor-tion of terms belonging to the i-th domain.Domain 1 tonight, watch, time, todayDomain 2 win, vote, obama, blackDomain 3 game, play, win, teamDomain 4 apple, international, sun, andersonDomain 5 ticket, show, open, liveTable 6: Most representative words of each domain (5domains version)In first experiments with crossvalidation on train-ing data, the 5 domains version, presented in Table 6,appears to be the most efficient.
Therefore, in the restof the paper, results are shown only for this version.3.3.2 Detection of multi-polarity wordsFor detecting the multi-polarity words, we use thepositive and negative labels of the training data.
Wemake the assumption that positive words will mostlyappear in positive tweets and negative words in neg-ative tweets.
Between two different corpora, we de-termine words with different polarity across corporaby using a ?2 test on their profile of occurrence inpositive and negative tweets in both corpora.
The riskof false positive is set to 0.05.
The words are alsoselected only if they occur more often than a giventhreshold.
For the SemEval task B, we apply thisdetection for each domain.
Each time, we detect thewords that change their polarity between a specificdomain and all the others.
For example, the word?black?
is detected as positive in the second domain,related to the election of Barack Obama, and neutralin the rest of the tweets.
At the end of this procedure,we have 5 collections of words which change theirpolarity (one different collection for each domain).These collections are rather small: from 21 to 61multi-polarity words are detected depending on thedomain and the parameters.3.3.3 Differentiation of multi-polarity wordsWe tested different strategies in order to integratethe domain information in the Sentiment Classifica-tion in Twitter task.?
Domain-specific: 5 different classifiers aretrained on the domain specific subpart of thetweets, without change on the data.?
Diff-topic: 5 different classifiers are trained onthe whole corpus, where the detected multi-polarity words are differentiated into ?word-domainX?
and ?word-other?.?
Change-all: only 1 classifier is trained.
Similarto the previous one, except all the differentia-tions are made at the same time.?
Keep-topic: 5 different classifiers are trained.The detected multi-polarity words are kept in-side their domain and deleted in the others.?
Remove-all: 5 different classifiers are trained.The detected multi-polarity words are deletedinside and outside their domain.For the change-all version, we use only one classifier:all test tweets are classified using the same classifier.In the other versions, we obtain 5 classifiers.
Foreach test tweet, we determine its domain profile us-ing topic models of LDA.
Then we use a mix of allthe classifiers with weighting according to the LDAmixture2.
The domain-specific version gives worse2The weight is the exponential of the LDA score.422results than the baseline trained on the whole originalcorpus and is not represented on the figures.Figure 2: Average F-measure results for the best set ofparameters for each method.We tested all these versions with two training sets:first, using all the training tweets to train the clas-sifiers (Figure 2) and secondly, only the tweets forwhich a domain can be confidently attributed (at leasta 75% score from the LDA model) (Figure 3).
In thiscase, the training set contains 2889 tweets.
The runsubmitted to SemEval corresponds to the change-allversion, trained with all the training tweets.Figure 3: Average F-measure results for the best set ofparameters for each method.Empirically, we set the threshold for the numberof occurrences to 10 in the first experiment and onlyto 5 in the domain confident experiment, due to thesmallest size of the training corpora.3.4 Analysis of the result and discussionUsing a boosting method with lemma trigrams andemoticons features is a good fully automatic baseline.We are in the mid range of results of all the partici-pants (19th out of 48 submissions for the tweets and26th out of 42 submissions for the SMS).
We try toinclude domain information to improve the opinionclassification.
As we don?t have a reference domaindifferentiation for the tweets, we separate them us-ing the LDA method.
The domain-specific version,which does not take into account the multi-polaritywords, degrades the performances(-1.85% in the firstexperiment, -2.8% in the second).
On the contrary,all our versions which use multi-polarity words, es-pecially remove-all version, improve the F-measure.The final improvement is small but it has to be re-lated to the small number of multi-polarity words wehave detected (in average, 36 words per domain).
Wethink that the tweet collection is too small for the ?2test to detect a lot of words with enough confidence.For comparison, in our experiment on reviews, wedetected about 400 multi-polarity words per domain.It is also worth noticing that for the domain confi-dent experiment, the improvement is more sensible(+1.46% versus +0.70%) even if the absolute value ofthe score is not better, due to a much smaller trainingdata.
It?s a good argument for our method.
Anotherquestion is about the method used to separate thetweets into different domains.
We plan to have morecontrol on the domains by using a more supervisedmethod based on the categories of Wikipedia.4 ConclusionIn this paper, we presented our contribution to Se-mEval 2013 task 2: Sentiment Analysis in Twitter.For the Contextual Polarity Disambiguation subtask,we described a very efficient and robust method basedon a sentiment lexicon associated with a polarity shiftdetector and a tree based classification.
As for theMessage Polarity Classification, we focused on theimpact of domain information.
With only 4899 train-ing tweets, we achieve good performances and wedemonstrate that words with changing polarity caninfluence the classification performance.One of the challenges of this SemEval task was tosee how well sentiment analysis models trained us-ing Twitter data would generalize to a SMS dataset.Looking at our result but also at the submissions ofother participants, a drop of performance can be ob-served between the results on the Twitter and SMStest datasets.
In (Hu et al 2013), the authors per-form a thorough study on the differences between thelanguage used on Twitter and that of SMS messagesand chat.
They find that Twitter language is moreconservative and less informal than SMS and onlinechat and that the language of Twitter can be seen asa projection of a formal register in a restricted space.This is a good indicator to the difficulty of using aTwitter centered system on a SMS dataset.423AcknowledgmentsThis work was partly supported by the MUCKEproject (http://ifs.tuwien.ac.at/?mucke/) through agrant from the French National Research Agency(ANR), FP7 CHIST-ERA Programme (ANR-12-CHRI-0007-04).ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.
InProceedings of the 7th conference on International Lan-guage Resources and Evaluation (LREC10), Valletta,Malta, May.Romaric Besanc?on, Gae?l de Chalendar, Olivier Ferret,Faiza Gara, Olivier Mesnard, Meriama Lab, and Nasre-dine Semmar.
2010.
Lima : A multilingual frameworkfor linguistic analysis and linguistic resources develop-ment and evaluation.
In Nicoletta Calzolari (Confer-ence Chair), Khalid Choukri, Bente Maegaard, JosephMariani, Jan Odijk, Stelios Piperidis, Mike Rosner,and Daniel Tapias, editors, Proceedings of LREC?10,Valletta, Malta, may.
European Language ResourcesAssociation (ELRA).David M Blei, Andrew Y Ng, and Michael I Jordan.
2003.Latent dirichlet alcation.
the Journal of machineLearning research, 3:993?1022.Lu Chen, Wenbo Wang, Meenakshi Nagarajan, ShaojunWang, and Amit P Sheth.
2012.
Extracting diversesentiment expressions with target-dependent polarityfrom twitter.
Proceedings of ICWSM.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedings ofthe 42nd Annual Meeting on Association for Computa-tional Linguistics, page 423.
Association for Computa-tional Linguistics.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using twitter hashtags andsmileys.
In Proceedings of the 23rd International Con-ference on Computational Linguistics: Posters, pages241?249.
Association for Computational Linguistics.Alexandru Lucian Ginsca.
2012.
Fine-grained opin-ion mining as a relation classification problem.
In2012 Imperial College Computing Student Workshop,volume 28, pages 56?61.
Schloss Dagstuhl?Leibniz-Zentrum fuer Informatik.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.CS224N Project Report, Stanford, pages 1?12.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.
2009.The weka data mining software: an update.
ACMSIGKDD Explorations Newsletter, 11(1):10?18.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowledgediscovery and data mining, pages 168?177.
ACM.Yuheng Hu, Kartik Talamadupula, and Subbarao Kamb-hampati.
2013.
Dude, srsly?
: The surprisingly formalnature of twitters language.
Proceedings of ICWSM.Thorsten Joachims.
1999.
Making large scale svm learn-ing practical.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for ComputationalLinguistics-Volume 1, pages 423?430.
Association forComputational Linguistics.Morgane Marchand.
2013.
Fouille dopinion: ces mots quichangent de polarite?
selon le domaine.
In Proceedingsof the 8e Rencontres Jeunes Chercheurs en RecherchedInformation (RJCRI).Andrew Kachites McCallum.
2002.
Mallet: A machinelearning for language toolkit.Alessandro Moschitti.
2006.
Making tree kernels prac-tical for natural language learning.
In Proceedings ofEACL, volume 6, pages 113?120.R.
Navigli.
2012.
A quick tour of word sense disam-biguation, induction and related approaches.
SOFSEM2012: Theory and Practice of Computer Science, pages115?129.Robert E Schapire and Yoram Singer.
2000.
Boostex-ter: A boosting-based system for text categorization.Machine learning, 39(2-3):135?168.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005.Recognizing contextual polarity in phrase-level senti-ment analysis.
In Proceedings of the conference onHuman Language Technology and Empirical Methodsin Natural Language Processing, pages 347?354.
As-sociation for Computational Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009.Recognizing contextual polarity: An exploration offeatures for phrase-level sentiment analysis.
Computa-tional Linguistics, 35.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, AlanRitter, Sara Rosenthal, and Veselin Stoyanov.
2013.Semeval-2013 task 2: Sentiment analysis in twitter.In Proceedings of the 7th International Workshop onSemantic Evaluation.
Association for ComputationalLinguistics.Yasuhisa Yoshida, Tsutomu Hirao, Tomoharu Iwata,Masaaki Nagata, and Yuji Matsumoto.
2011.
Transferlearning for multiple-domain sentiment analysis - iden-tifying domain dependent/independent word polaritys.In AAAI.424
