Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 91?95,Baltimore, Maryland, 26-27 July 2014.c?2014 Association for Computational LinguisticsNTHU at the CoNLL-2014 Shared TaskJian-Cheng Wu*, Tzu-Hsi Yen*, Jim Chang*, Guan-Cheng Huang*,Jimmy Chang*, Hsiang-Ling Hsu+, Yu-Wei Chang+, Jason S. Chang** Department of Computer Science+ Institute of Information Systems and ApplicationsNational Tsing Hua UniversityHsinChu, Taiwan, R.O.C.
30013{wujc86, joseph.yen, cwebb.tw, a2961353,rocmewtwo, ilovecat6717, teer1990, jason.jschang}@gmail.comAbstractIn this paper, we describe a system for cor-recting grammatical errors in texts writtenby non-native learners.
In our approach, agiven sentence with syntactic features aresent to a number of modules, each focuseson a specific error type.
A main programintegrates corrections from these modulesand outputs the corrected sentence.
Weevaluated our system on the official testdata of the CoNLL-2014 shared task andobtained 0.30 in F-measure.1 IntroductionMillions of non-native learners are using Englishas their second language (ESL) or foreign lan-guage (EFL).
These learners often make differentkinds of grammatical errors and are not aware ofit.
With a grammatical error corrector applies rulesor statistical learning methods, learners can use thesystem to improve the quality of writing, and be-come more aware of the common errors.
It mayalso help learners improve their writing skills.The CoNLL-2014 shared task is aimed at pro-moting research on correcting grammatical errors.Types of errors handled in the shared task are ex-tended from the five types in the previous sharedtask to include all common errors present in an es-say.In this paper, we focus on the following errorsmade by ESL writers:?
Spelling and comma?
Article and determiner?
Preposition?
Preposition + verb (interactive)?
Noun number?
Word form?
Subject-verb-agreementFor each error type, we developed and tuned amodule based on the official development data.
Amain program combines the correction hypothesesfrom these modules and produces the final correc-tion.
If multiple modules propose different cor-rections to the same word/phrase, the correctionproposed by the module with the highest precisionwill be chosen.2 Method2.1 Spelling and Comma moduleIn this section, we correct comma errors andspelling errors, including missing/extraneous hy-phens.
For simplicity, we adopt Aspell1andGingerIt2to detect spelling errors and generatepossible replacements, considered as confusablewords, which might contain the word with cor-rect spelling.
Then, we replace the word beingchecked with confusable words to generate sen-tences.
Language models trained on well-formedtexts are used to measure the probability of these1http://aspell.net/2https://pypi.python.org/pypi/gingeritWe use GingerIt only for correcting missing/extraneoushyphens91candidates.
Candidate with the highest probabilityis chosen as correction.Omitted commas form a large proportion ofpunctuation errors.
We apply the CRF model pro-posed by Israel, et.
al.
(2012) with some mod-ification.
We replace distance features with syn-tactic features.
More specifically, we do not usefeatures such as distances to the start of sentenceor last comma.
And we add two features, one in-dicates whether a word is at the end of a clause,and the other indicates whether the current clausestarts with a prepositional phrase.2.2 Subject-verb-agreement moduleThis module corrects subject-verb-agreement er-rors in a given sentence.
Consider the sentence?The boy in blue pants are my brother?.
The cor-rect sentence should be ?The boy in blue pants ismy brother?.
Since a verb could be far from it?ssubject, using ngram counts may fail to detect andcorrect such an error.We use a rule-based method in this module.In the first stage, we identify the subject of eachclause by using information from the parser.
Boththe dependency relation and syntactic structure areused in this stage.
Dependency relations such asnsubj and rcmod indicate subjects of subject-verb relation.
If there is a verb that has not beenassigned a subject via dependency relations, headof noun phrase in the same clause will be used in-stead.
And in the second stage, we check whethersubject and verbs agree, for each clause in the sen-tence.Here we explain our correction process in moredetail.
For each clause, the singular and pluralforms of verbs in the clause must be consistentwith the subject of the clause unless the subjectis a quantifier.
Consider the following sentences:The number of cats is ten.A number of cats are playing.Since our judgment only depends on the subjectnumber, it?s hard to tell whether should we usea plural verb or not in this case.
The quantifierswe do not handle are listed as follow: number, lot,quantity, variety, type, amount, neither.2.3 Number module and Forms moduleWe correct noun number error in two stages.
Inthe first stage, we generate a confusion set for eachword.
While constructing confusion set for nounnumber, both of the singular form and plural formare included in the set.
While constructing con-fusion set for word forms, we use the word fam-ilies in Academic Word List (AWL)3and BritishNational Corpus (BNC4000)4.
Given a contentword, all the words in the same family exceptantonyms are entered into the confusion set.
How-ever, comparative form and superlative form of anadjective are eliminated from the confusion set,since replacing an adjective with these forms is asemantic rather than syntactic decision.
The fol-lowing examples illustrate what kinds of alterna-tives are eliminated:antonyms: misleading for the word leadsemantics: higher for the word highAdditionally, in the forms module, a correc-tion is ignored, if it is actually correcting a verbtense, subject-verb-agreement, or noun number er-ror.
We use part-of-speech (POS) tag to check this.More specifically, any corrections that changes aword with a VBZ tag to a word with a VBP orVBD tag is ignored, and vice versa.
And any cor-rections that switches a noun between it?s singularform and plural form is also ignored.With the confusion sets, we proceed to thesecond stage.
In this stage, we use words inthe confusion set to attempt to replace potentialerrors.
Language models trained on well-formedtext are used to validate the replacement decisions.Given a word w, If there is an alternative w?
thatfits in the context better, w is flagged as an errorand w?
is returned as a correction.Here is our formula for correcting errorsP (O) =Pngram(O) + Prnn(O)2P (R) =Pngram(R) + Prnn(R)2Promotion =P (R)?
P (O)|O|While checking a content word w, we replacew in the original sentence O with alternatives andgenerate candidates C. We then generate the can-didate R with the highest probability among all3http://www.victoria.ac.nz/lals/resources/academicwordlist/sublists4http://simple.wiktionary.org/wiki/Wiktionary:BNC_spoken_freq92candidates.
We use an interpolated probability5ofngram language model Pngramand recurrent neu-ral network language model Prnn.
Promotion in-dicates the increase in probability per word afterwe replace sentence O with the candidate R. Weuse word number to normalize Promotion follow-ing Dahlmeier, et al.
(2012).
Corrections are madeonly if Promotion is higher than a empirically de-termined threshold.62.4 Article and Determiner moduleIn this subsection, we describe how we correct er-rors of omitting a determiner or adding a spuri-ous determiner.
The language models mentionedin the last subsection are also used in this module.We tune our thresholds for making corrections ondevelopment data7, and found that deleting a de-terminer should have a lower threshold while in-serting one should have a higher one, so we setdifferent thresholds accordingly.8To cope with the situation where a deter-miner/article is far ahead of the head of the nounphrase, we apply another constraint while makingcorrection decision.First, we calculate statistics on the head of nounphrases.
We extract the most frequent 100,000terms in Web-1T 5-gram corpus.
These terms areused to search their definitions in Wikipedia (usu-ally at the first paragraph).
The characteristic of adefinition is that it has no prior context and mostof the noun phrases with a determiner are uniqueor known to the general public.
Heads of thesenouns phrases are likely to always appear with adeterminer.Heads that tend to appear with a determinerthe help us to decide whether a determiner shouldbe added to a noun phrase.
We add a determinerusing two constraints.
We only insert a determineror an article, if the statistics indicate that head ofa noun phrase tends to have a determiner, or thepromotion of log probability is much higher thanthe threshold.
A similar constraint is also applied,for deleting a determiner or an article.5the probabilities present in the formula are log probabil-ities6the threshold for noun number module is 0.035 and 0.050for word form module.
These threshold were set empiricallyafter testing on development data7test data of the CoNLL-2013 shared task8the threshold for insertion is 0.035 and 0.040 for deletion2.5 Preposition moduleFor preposition errors, we focus on handling twotypes of errors: REPLACE and DELETE.
Apreposition, which should be deleted from thegiven sentence, is regarded as a DELETE error,whereas for a preposition, which should be re-placed with a more appropriate alternative, is re-garded as a REPLACE error.
In this work, wecorrect the two types of errors based on the as-sumption that the usage of preposition often de-pends on the collocation relations with a verb ornoun.
Therefore, we use the dependency relationssuch as dobj and pobj, and prep to identifythe related words, and then we validate the usageof prepositions, and correct the preposition errors.A dependency-based model is proposed in thispaper to handle the preposition errors.
The modelconsists of two stages: detecting the possiblepreposition errors and correcting the errors.In the first stage, we use the Stanford depen-dency parser (Klein and Manning, 2003) to extractthe dependency relations for each preposition.
Therelation tuples, which contain the preposition, verbor noun, and prepositional object.
For example,the tuple of verb-prep-object (listen, to, music),or the tuple of noun-prep-object (point, of, view)are extracted for validation.
We identify a preposi-tion containing as an error, if the tuple containingthe preposition does not occur in a reference listbuilt using a reference corpus.
In order to resolvethe data sparseness and false alarm problems, weneed a sufficiently large list of validated tuples.In this study, the reference tuple lists are gener-ated from the Google Books Syntactic N-grams(Goldberg and Orwant, 2013)9.
For example, wecan find (come, to, end, 236864) and (lead, to, re-sult, 57632) in the verb-preposition-object refer-ence list.
We have generated 21,773,752 differentdependency tuples for our purpose.In the second stage, we attempt to correct allpotential preposition errors.
At first, a list of can-didate tuples is generated by substituting the orig-inal preposition in the error tuple with alterna-tive prepositions.
For example, the generated can-didate tuples for the error tuple (join, at, party)will include (join, in, party), (join, on, party), etc.On the other hand, the tuple, (join, party), which9Data sets available from http://commondatastorage.googleapis.com/books/syntactic-ngrams/index.html93deletes the preposition, is also taken into consid-eration.
All candidates are ranked according tothe frequency provided by the reference lists.
Thepreposition in the tuple with the highest frequencyis returned as the correction suggestion.Figure 1: Sample annotated trigramsFigure 2: Sample trigram groupFigure 3: Sample phrase translation for a trigramgroup2.6 Interactive errors moduleThis module uses a new method for correctingserial grammatical errors in a given sentence inlearners writing.
A statistical translation model isgenerated to attempt to translate the input with se-rial and interactive errors into a grammatical sen-tence.
The method involves automatically learn-ing translation models based on Web-scale n-gram.
The model corrects trigrams containing se-rial preposition-verb errors via translation.
Eval-uation on a set of sentences in a learner corpusshows that the method corrects serial errors rea-sonably well.Consider an error sentence ?I have difficulty tounderstand English.?
The correct sentence for thisshould be ?I have difficulty in understanding En-glish.?
It is hard to correct these two errors one byone, since the errors are dependent on each other.Intuitively, by identifying difficulty to understandas containing serial errors and correct it to diffi-culty in understanding, we can handle this kind ofproblem more effectively.First, we generate translation phrase table asfollows.
We begin by selecting trigrams related toserial errors and correction from Web 1T 5-gram.Figure 1 shows some sample annotation trigrams.Then, we group the selected trigrams by the firstand last word in the trigrams.
See Figure 2 for asample VPV group of trigrams.
Finally, we gener-ate translation phrase table for each group.
Figure3 shows a sample translation phrase table.At run time, we tag the input sentence with partof speech information in order to find trigramsthat fit the type of serial errors.
Then, we searchphrase table and generate translations for theinput phrases in a machine translation decoder toproduce a corrected sentence.3 ExperimentTwo types of trigram language models, ngrammodel and recurrent neural network (RNN) model,are used in correcting spelling, noun number, wordform, and determiner errors.
We trained the ngramlanguage model on English Gigaword and BNCcorpus, using the SRILM tool (Stolcke, 2002).We train the RNN model with RNNLM toolkit(Mikolov et al., 2011).
Complexity of training theRNN language model is much higher, so we trainit on a smaller corpus, the British National Corpus(BNC).We used the Stanford Parser (Klein and Christo-pher D. Manning, 2003) to obtain dependency re-lations in the preposition module, and to obtainPOS tags for the word form module.
The subject-verb-agreement module also uses dependency re-lations contained in test data.
Dependency rela-tions in Google Books Syntactic N-grams (Gold-94berg and Orwant, 2013) were also used to developour dendepency-based model in the prepositionmodule.To assess the effectiveness of the proposedmethod, we used the official training, develop-ment, and test data of the CoNLL-2014 sharedtask.
On the test data, our system obtained the pre-cision, recall and F0.5score of 0.351, 0.189, and0.299.
The following table shows the performancebreakdown by module.Figure 4: The performance breakdown by module.
(Displayed in %)In the spelling and hyphen module, candidatesfrom Aspell include words that only differ fromthe original word in one character, s. Languagemodels are then used to choose the candidate withhighest probability as our correction.
The moduletherefore gives some corrections about noun num-bers or subject-verb-agreement.
As a result, somecorrections made by this module overlap with cor-rections made by the noun numbers module andthe subject-verb-agreement module, which makesthe recall of correcting spelling and hyphen errors,4.11%, overestimated.4 ConclusionIn this work, we have built several modules for er-ror detection and correction.
For different typesof errors, we developed modules independentlyusing different features and thresholds.
If mul-tiple modules propose different corrections to aword/phrase, the one proposed by the module withhigher precision will be chosen.
Many avenuesfor future work present themselves.
We plan tointegrate modules in a more flexible way.
Whenfaced with different corrections made by differentmodules, the decision would better be based onthe confidence of each correction with a uniformstandard, but not on the confidence of modules.AcknowledgmentsWe would like to acknowledge the funding sup-ports from Delta Electronic Corp and NationalScience Council (contract no: NSC 100-2511-S-007-005-MY3), Taiwan.
We are also thankful toanonymous reviewers and the organizers of theshared task.ReferencesDaniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng Ng2012.
NUS at the HOO 2012 Shared Task.
In Pro-ceedings of the Seventh Workshop on Building Ed-ucational Applications Using NLP, Association forComputational Linguistics, June 7.Daniel Dahlmeier and Hwee Tou Ng, 2012.
BetterEvaluation for Grammatical Error Correction.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL 2012).,568-672Daniel Dahlmeier, Hwee Tou Ng, Siew Mei Wu.
2013.Building a Large Annotated Corpus of Learner En-glish: The NUS Corpus of Learner English.
InProceedings of the 8th Workshop on Innovative Useof NLP for Building Educational Applications(BEA2013)Yoav Goldberg and Jon Orwant 2013.
A dataset ofsyntactic-ngrams over time from a very large cor-pus of English books.
In Proceedings of the SecondJoint Conference on Lexical and Computational Se-mantics, Atlanta, GA, 2013.Ross Israel, Joel Tetreault, and Martin Chodorow2012.
Correcting Comma Errors in Learner Essays,and Restoring Commas in Newswire Text.
In Pro-ceeding of the 2012 Conference of the North Amer-ica Chapter of the Association for ComputationalLinguistics: Human Language Technologies,284-294, Montreal Canada, June.
Association for Com-putational LinguisticsDan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Meeting of the Association for ComputationalLinguistics, 423-430.Tomas Mikolov, Anoop Deoras, Dan Povey, LukarBurget, and Jan Honza Cernocky 2011.
Strategiesfor Training Large Scale Neural Network LanguageModels Proceedings of ASRU 2011Andreas Stolcke 2002.
SRILM-An Extensible Lan-guage Modeling Toolkit In Proceedings of the Inter-national Conference on Spoken Language Process-ing, vol 2, 901-90495
