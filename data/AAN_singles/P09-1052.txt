Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 459?467,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPEmploying Topic Models for Pattern-based Semantic Class DiscoveryHuibin Zhang1*     Mingjie Zhu2*     Shuming Shi3     Ji-Rong Wen31Nankai University2University of Science and Technology of China3Microsoft Research Asia{v-huibzh, v-mingjz, shumings, jrwen}@microsoft.comAbstract?A semantic class is a collection of items(words or phrases) which have semanticallypeer or sibling relationship.
This paper studiesthe employment of topic models to automati-cally construct semantic classes, taking as thesource data a collection of raw semanticclasses (RASCs), which were extracted by ap-plying predefined patterns to web pages.
Theprimary requirement (and challenge) here isdealing with multi-membership: An item maybelong to multiple semantic classes; and weneed to discover as many as possible the dif-ferent semantic classes the item belongs to.
Toadopt topic models, we treat RASCs as ?doc-uments?, items as ?words?, and the final se-mantic classes as ?topics?.
Appropriatepreprocessing and postprocessing are per-formed to improve results quality, to reducecomputation cost, and to tackle the fixed-kconstraint of a typical topic model.
Experi-ments conducted on 40 million web pagesshow that our approach could yield better re-sults than alternative approaches.1 IntroductionSemantic class construction (Lin and Pantel,2001; Pantel and Lin, 2002; Pasca, 2004; Shinza-to and Torisawa, 2005; Ohshima et al, 2006)tries to discover the peer or sibling relationshipamong terms or phrases by organizing them intosemantic classes.
For example, {red, white,black?}
is a semantic class consisting of colorinstances.
A popular way for semantic class dis-covery is pattern-based approach, where prede-fined patterns (Table 1) are applied to a?
This work was performed when the authors were interns atMicrosoft Research Asiacollection of web pages or an online web searchengine to produce some raw semantic classes(abbreviated as RASCs, Table 2).
RASCs cannotbe treated as the ultimate semantic classes, be-cause they are typically noisy and incomplete, asshown in Table 2.
In addition, the information ofone real semantic class may be distributed in lotsof RASCs (R2 and R3 in Table 2).Type PatternSENT NP {, NP}*{,} (and|or) {other} NPTAG <UL>  <LI>item</LI>  ?
<LI>item</LI>  </UL>TAG <SELECT> <OPTION>item?<OPTION>item </SELECT>* SENT: Sentence structure patterns; TAG: HTML Tag patternsTable 1.
Sample patternsR1: {gold, silver, copper, coal, iron, uranium}R2: {red, yellow, color, gold, silver, copper}R3: {red, green, blue, yellow}R4: {HTML, Text, PDF, MS Word, Any file type}R5: {Today, Tomorrow, Wednesday, Thursday, Friday,Saturday, Sunday}R6: {Bush, Iraq, Photos, USA, War}Table 2.
Sample raw semantic classes (RASCs)This paper aims to discover high-quality se-mantic classes from a large collection of noisyRASCs.
The primary requirement (and chal-lenge) here is to deal with multi-membership, i.e.,one item may belong to multiple different seman-tic classes.
For example, the term ?Lincoln?
cansimultaneously represent a person, a place, or acar brand name.
Multi-membership is more pop-ular than at a first glance, because quite a lot ofEnglish common words have also been borrowedas company names, places, or product names.For a given item (as a query) which belongs tomultiple semantic classes, we intend to return thesemantic classes separately, rather than mixingall their items together.Existing pattern-based approaches only pro-vide very limited support to multi-membership.For example, RASCs with the same labels (orhypernyms) are merged in (Pasca, 2004) to gen-459erate the ultimate semantic classes.
This is prob-lematic, because RASCs may not have (accurate)hypernyms with them.In this paper, we propose to use topic modelsto address the problem.
In some topic models, adocument is modeled as a mixture of hidden top-ics.
The words of a document are generated ac-cording to the word distribution over the topicscorresponding to the document (see Section 2 fordetails).
Given a corpus, the latent topics can beobtained by a parameter estimation procedure.Topic modeling provides a formal and conve-nient way of dealing with multi-membership,which is our primary motivation of adopting top-ic models here.
To employ topic models, we treatRASCs as ?documents?, items as ?words?, andthe final semantic classes as ?topics?.There are, however, several challenges in ap-plying topic models to our problem.
To beginwith, the computation is intractable forprocessing a large collection of RASCs (our da-taset for experiments contains 2.7 million uniqueRASCs extracted from 40 million web pages).Second, typical topic models require the numberof topics (k) to be given.
But it lacks an easy wayof acquiring the ideal number of semantic classesfrom the source RASC collection.
For the firstchallenge, we choose to apply topic models tothe RASCs containing an item q, rather than thewhole RASC collection.
In addition, we also per-form some preprocessing operations in whichsome items are discarded to further improve effi-ciency.
For the second challenge, consideringthat most items only belong to a small number ofsemantic classes, we fix (for all items q) a topicnumber which is slightly larger than the numberof classes an item could belong to.
And then apostprocessing operation is performed to mergethe results of topic models to generate the ulti-mate semantic classes.Experimental results show that, our topicmodel approach is able to generate higher-qualitysemantic classes than popular clustering algo-rithms (e.g., K-Medoids and DBSCAN).We make two contributions in the paper: Onone hand, we find an effective way of construct-ing high-quality semantic classes in the pattern-based category which deals with multi-membership.
On the other hand, we demonstrate,for the first time, that topic modeling can be uti-lized to help mining the peer relationship amongwords.
In contrast, the general related relation-ship between words is extracted in existing topicmodeling applications.
Thus we expand the ap-plication scope of topic modeling.2 Topic ModelsIn this section we briefly introduce the two wide-ly used topic models which are adopted in ourpaper.
Both of them model a document as a mix-ture of hidden topics.
The words of every docu-ment are assumed to be generated via agenerative probability process.
The parameters ofthe model are estimated from a training processover a given corpus, by maximizing the likelih-ood of generating the corpus.
Then the model canbe utilized to inference a new document.pLSI: The probabilistic Latent Semantic In-dexing Model (pLSI) was introduced in Hof-mann (1999), arose from Latent SemanticIndexing (Deerwester et al, 1990).
The follow-ing process illustrates how to generate a docu-ment d in pLSI:1.
Pick a topic mixture distribution ?(?
|?).2.
For each word wi in da.
Pick a latent topic z with the probabil-ity ?(?|?)
for wib.
Generate wi with probability ?(??
|?
)So with k latent topics, the likelihood of gene-rating a document d is?(?)
=  ?
??
?
?(?|?)??
(2.1)LDA (Blei et al, 2003): In LDA, the topicmixture is drawn from a conjugate Dirichlet priorthat remains the same for all documents (Figure1).
The generative process for each document inthe corpus is,1.
Choose document length N from a Pois-son distribution Poisson(?).2.
Choose ?
from a Dirichlet distributionwith parameter ?.3.
For each of the N words wi.a.
Choose a topic z from a Multinomialdistribution with parameter ?.b.
Pick a word wi from ?
??
?,?
.So the likelihood of generating a document is?(?)
=  ?(?|?)??(?|?)?
??
?,?
????
(2.2)Figure 1.
Graphical model representation of LDA,from Blei et al (2003)w?
z?
?NM4603 Our ApproachThe source data of our approach is a collection(denoted as CR) of RASCs extracted via applyingpatterns to a large collection of web pages.
Givenan item as an input query, the output of our ap-proach is one or multiple semantic classes for theitem.
To be applicable in real-world dataset, ourapproach needs to be able to process at least mil-lions of RASCs.3.1 Main IdeaAs reviewed in Section 2, topic modeling pro-vides a formal and convenient way of groupingdocuments and words to topics.
In order to applytopic models to our problem, we map RASCs todocuments, items to words, and treat the outputtopics yielded from topic modeling as our seman-tic classes (Table 3).
The motivation of utilizingtopic modeling to solve our problem and buildingthe above mapping comes from the followingobservations.1) In our problem, one item may belong tomultiple semantic classes; similarly in topicmodeling, a word can appear in multiple top-ics.2) We observe from our source data thatsome RASCs are comprised of items in mul-tiple semantic classes.
And at the same time,one document could be related to multipletopics in some topic models (e.g., pLSI andLDA).Topic modeling Semantic class constructionword item (word or phrase)document RASCtopic semantic classTable 3.
The mapping from the concepts in topicmodeling to those in semantic class constructionDue to the above observations, we hope topicmodeling can be employed to construct semanticclasses from RASCs, just as it has been used inassigning documents and words to topics.There are some critical challenges and issueswhich should be properly addressed when topicmodels are adopted here.Efficiency: Our RASC collection CR containsabout 2.7 million unique RASCs and 26 million(1 million unique) items.
Building topic modelsdirectly for such a large dataset may be computa-tionally intractable.
To overcome this challenge,we choose to apply topic models to the RASCscontaining a specific item rather than the wholeRASC collection.
Please keep in mind that ourgoal in this paper is to construct the semanticclasses for an item when the item is given as aquery.
For one item q, we denote CR(q) to be allthe RASCs in CR containing the item.
We believebuilding a topic model over CR(q) is much moreeffective because it contains significantly fewer?documents?, ?words?, and ?topics?.
To furtherimprove efficiency, we also perform preprocess-ing (refer to Section 3.4 for details) before build-ing topic models for CR(q), where some low-frequency items are removed.Determine the number of topics: Most topicmodels require the number of topics to be knownbeforehand1.
However, it is not an easy task toautomatically determine the exact number of se-mantic classes an item q should belong to.
Ac-tually the number may vary for different q. Oursolution is to set (for all items q) the topic num-ber to be a fixed value (k=5 in our experiments)which is slightly larger than the number of se-mantic classes most items could belong to.
Thenwe perform postprocessing for the k topics toproduce the final properly semantic classes.In summary, our approach contains threephases (Figure 2).
We build topic models forevery CR(q), rather than the whole collection CR.A preprocessing phase and a postprocessingphase are added before and after the topic model-ing phase to improve efficiency and to overcomethe fixed-k problem.
The details of each phaseare presented in the following subsections.Figure 2.
Main phases of our approach3.2 Adopting Topic ModelsFor an item q, topic modeling is adopted toprocess the RASCs in CR(q) to generate k seman-tic classes.
Here we use LDA as an example to1 Although there is study of non-parametric Bayesian mod-els (Li et al, 2007) which need no prior knowledge of topicnumber, the computational complexity seems to exceed ourefficiency requirement and we shall leave this to futurework.R580R1R2CRItem qPreprocessing?400??1?
?2?T5T1T2C3C1C2TopicmodelingPostprocessingT3T4CR(q)461illustrate the process.
The case of other genera-tive topic models (e.g., pLSI) is very similar.According to the assumption of LDA and ourconcept mapping in Table 3, a RASC (?docu-ment?)
is viewed as a mixture of hidden semanticclasses (?topics?).
The generative process for aRASC R in the ?corpus?
CR(q) is as follows,1) Choose a RASC size (i.e., the number ofitems in R): NR ~ Poisson(?
).2) Choose a k-dimensional vector ??
from aDirichlet distribution with parameter ?.3) For each of the NR items an:a) Pick a semantic class ??
from a mul-tinomial distribution with parameter??
.b) Pick an item an from ?(??
|??
,?)
,where the item probabilities are pa-rameterized by the matrix ?.There are three parameters in the model: ?
(ascalar), ?
(a k-dimensional vector), and ?
(a?
?
?
matrix where V is the number of distinctitems in CR(q)).
The parameter values can be ob-tained from a training (or called parameter esti-mation) process over CR(q), by maximizing thelikelihood of generating the corpus.
Once ?
isdetermined, we are able to compute ?(?|?,?
),the probability of item a belonging to semanticclass z.
Therefore we can determine the membersof a semantic class z by selecting those itemswith high ?
?
?,?
values.The number of topics k is assumed known andfixed in LDA.
As has been discussed in Section3.1, we set a constant k value for all differentCR(q).
And we rely on the postprocessing phaseto merge the semantic classes produced by thetopic model to generate the ultimate semanticclasses.When topic modeling is used in documentclassification, an inference procedure is requiredto determine the topics for a new document.Please note that inference is not needed in ourproblem.One natural question here is: Considering thatin most topic modeling applications, the wordswithin a resultant topic are typically semanticallyrelated but may not be in peer relationship, thenwhat is the intuition that the resultant topics hereare semantic classes rather than lists of generallyrelated words?
The magic lies in the ?docu-ments?
we used in employing topic models.Words co-occurred in real documents tend to besemantically related; while items co-occurred inRASCs tend to be peers.
Experimental resultsshow that most items in the same output seman-tic class have peer relationship.It might be noteworthy to mention the exchan-geability or ?bag-of-words?
assumption in mosttopic models.
Although the order of words in adocument may be important, standard topic mod-els neglect the order for simplicity and other rea-sons2.
The order of items in a RASC is clearlymuch weaker than the order of words in an ordi-nary document.
In some sense, topic models aremore suitable to be used here than in processingan ordinary document corpus.3.3 Preprocessing and PostprocessingPreprocessing is applied to CR(q) before we buildtopic models for it.
In this phase, we discardfrom all RASCs the items with frequency (i.e.,the number of RASCs containing the item) lessthan a threshold h. A RASC itself is discardedfrom CR(q) if it contains less than two items afterthe item-removal operations.
We choose to re-move low-frequency items, because we foundthat low-frequency items are seldom importantmembers of any semantic class for q.
So the goalis to reduce the topic model training time (byreducing the training data) without sacrificingresults quality too much.
In the experiments sec-tion, we compare the approaches with and with-out preprocessing in terms of results quality andefficiency.
Interestingly, experimental resultsshow that, for some small threshold values, theresults quality becomes higher after preprocess-ing is performed.
We will give more discussionsin Section 4.In the postprocessing phase, the output seman-tic classes (?topics?)
of topic modeling aremerged to generate the ultimate semantic classes.As indicated in Sections 3.1 and 3.2, we fix thenumber of topics (k=5) for different corpus CR(q)in employing topic models.
For most items q,this is a larger value than the real number of se-mantic classes the item belongs to.
As a result,one real semantic class may be divided into mul-tiple topics.
Therefore one core operation in thisphase is to merge those topics into one semanticclass.
In addition, the items in each semanticclass need to be properly ordered.
Thus mainoperations include,1) Merge semantic classes2) Sort the items in each semantic classNow we illustrate how to perform the opera-tions.Merge semantic classes: The merge processis performed by repeatedly calculating the simi-2 There are topic model extensions considering word orderin documents, such as Griffiths et al (2005).462larity between two semantic classes and mergingthe two ones with the highest similarity until thesimilarity is under a threshold.
One simple andstraightforward similarity measure is the Jaccardcoefficient,???
?1 ,?2 =?1 ?
?2?1 ?
?2(3.1)where ?1 ?
?2  and ?1 ?
?2  are respectively theintersection and union of semantic classes C1 andC2.
This formula might be over-simple, becausethe similarity between two different items is notexploited.
So we propose the following measure,???
?1 ,?2 =???
(?, ?)???2??
?1?1 ?
?2(3.2)where |C| is the number of items in semanticclass C, and sim(a,b) is the similarity betweenitems a and b, which will be discussed shortly.
InSection 4, we compare the performance of theabove two formulas by experiments.Sort items: We assign an importance score toevery item in a semantic class and sort them ac-cording to the importance scores.
Intuitively, anitem should get a high rank if the average simi-larity between the item and the other items in thesemantic class is high, and if it has high similari-ty to the query item q.
Thus we calculate the im-portance of item a in a semantic class C asfollows,?
?|?
= ?
?sim(a,C)+(1-?)
?sim(a,q) (3.3)where ?
is a parameter in [0,1], sim(a,q) is thesimilarity between a and the query item q, andsim(a,C) is the similarity between a and C, calcu-lated as,???
?,?
=???
(?, ?)????
(3.4)Item similarity calculation: Formulas 3.2,3.3, and 3.4 rely on the calculation of the similar-ity between two items.One simple way of estimating item similarityis to count the number of RASCs containing bothof them.
We extend such an idea by distinguish-ing the reliability of different patterns and pu-nishing term similarity contributions from thesame site.
The resultant similarity formula is,???(?,?)
= log(1 + ?(?(??
,?
))???=1)?
?=1(3.5)where Ci,j is a RASC containing both a and b,P(Ci,j) is the pattern via which the RASC is ex-tracted, and w(P) is the weight of pattern P. As-sume all these RASCs belong to m sites with Ci,jextracted from a page in site i, and ki being thenumber of RASCs corresponding to site i. Todetermine the weight of every type of pattern, werandomly selected 50 RASCs for each patternand labeled their quality.
The weight of eachkind of pattern is then determined by the averagequality of all labeled RASCs corresponding to it.The efficiency of postprocessing is not a prob-lem, because the time cost of postprocessing ismuch less than that of the topic modeling phase.3.4 Discussion3.4.1 Efficiency of processing popular itemsOur approach receives a query item q from usersand returns the semantic classes containing thequery.
The maximal query processing timeshould not be larger than several seconds, be-cause users would not like to wait more time.Although the average query processing time ofour approach is much shorter than 1 second (seeTable 4 in Section 4), it takes several minutes toprocess a popular item such as ?Washington?,because it is contained in a lot of RASCs.
In or-der to reduce the maximal online processingtime, our solution is offline processing popularitems and storing the resultant semantic classeson disk.
The time cost of offline processing isfeasible, because we spent about 15 hours on a 4-core machine to complete the offline processingfor all the items in our RASC collection.3.4.2 Alternative approachesOne may be able to easily think of other ap-proaches to address our problem.
Here we dis-cuss some alternative approaches which aretreated as our baseline in experiments.RASC clustering: Given a query item q, run aclustering algorithm over CR(q) and merge allRASCs in the same cluster as one semantic class.Formula 3.1 or 3.2 can be used to compute thesimilarity between RASCs in performing cluster-ing.
We try two clustering algorithms in experi-ments: K-Medoids and DBSCAN.
Please note k-means cannot be utilized here because coordi-nates are not available for RASCs.
One draw-back of RASC clustering is that it cannot dealwith the case of one RASC containing the itemsfrom multiple semantic classes.Item clustering: By Formula 3.5, we are ableto construct an item graph GI to record theneighbors (in terms of similarity) of each item.Given a query item q, we first retrieve its neigh-bors from GI, and then run a clustering algorithmover the neighbors.
As in the case of RASC clus-tering, we try two clustering algorithms in expe-riments: K-Medoids and DBSCAN.
The primarydisadvantage of item clustering is that it cannotassign an item (except for the query item q) to463multiple semantic classes.
As a result, when weinput ?gold?
as the query, the item ?silver?
canonly be assigned to one semantic class, althoughthe term can simultaneously represents a colorand a chemical element.4 Experiments4.1 Experimental SetupDatasets: By using the Open Directory Project(ODP3) URLs as seeds, we crawled about 40 mil-lion English web pages in a breadth-first way.RASCs are extracted via applying a list of sen-tence structure patterns and HTML tag patterns(see Table 1 for some examples).
Our RASC col-lection CR contains about 2.7 million uniqueRASCs and 1 million distinct items.Query set and labeling: We have volunteersto try Google Sets4, record their queries beingused, and select overall 55 queries to form ourquery set.
For each query, the results of all ap-proaches are mixed together and labeled by fol-lowing two steps.
In the first step, the standard(or ideal) semantic classes (SSCs) for the queryare manually determined.
For example, the idealsemantic classes for item ?Georgia?
may includeCountries, and U.S. states.
In the second step,each item is assigned a label of ?Good?, ?Fair?,or ?Bad?
with respect to each SSC.
For example,?silver?
is labeled ?Good?
with respect to ?col-ors?
and ?chemical elements?.
We adopt metricMnDCG (Section 4.2) as our evaluation metric.Approaches for comparison: We compareour approach with the alternative approaches dis-cussed in Section 3.4.2.LDA: Our approach with LDA as the topicmodel.
The implementation of LDA is basedon Blei?s code of variational EM for LDA5.pLSI: Our approach with pLSI as the topicmodel.
The implementation of pLSI is basedon Schein, et al (2002).KMedoids-RASC: The RASC clustering ap-proach illustrated in Section 3.4.2, with theK-Medoids clustering algorithm utilized.DBSCAN-RASC: The RASC clustering ap-proach with DBSCAN utilized.KMedoids-Item: The item clustering ap-proach with the K-Medoids utilized.DBSCAN-Item: The item clustering ap-proach with the DBSCAN clustering algo-rithm utilized.3 http://www.dmoz.org4 http://labs.google.com/sets5 http://www.cs.princeton.edu/~blei/lda-c/K-Medoids clustering needs to predefine thecluster number k. We fix the k value for all dif-ferent query item q, as has been done for the top-ic model approach.
For fair comparison, the samepostprocessing is made for all the approaches.And the same preprocessing is made for all theapproaches except for the item clustering ones(to which the preprocessing is not applicable).4.2 Evaluation MethodologyEach produced semantic class is an ordered listof items.
A couple of metrics in the informationretrieval (IR) community like Precision@10,MAP (mean average precision), and nDCG(normalized discounted cumulative gain) areavailable for evaluating a single ranked list ofitems per query (Croft et al, 2009).
Among themetrics, nDCG (Jarvelin and Kekalainen, 2000)can handle our three-level judgments (?Good?,?Fair?, and ?Bad?, refer to Section 4.1),????@?
=?
?
/log(?
+ 1)??=1??
?
/log(?
+ 1)?
?=1(4.1)where G(i) is the gain value assigned to the i?thitem, and G*(i) is the gain value assigned to thei?th item of an ideal (or perfect) ranking list.Here we extend the IR metrics to the evalua-tion of multiple ordered lists per query.
We usenDCG as the basic metric and extend it toMnDCG.Assume labelers have determined m SSCs(SSC1~SSCm, refer to Section 4.1) for query qand the weight (or importance) of SSCi is wi.
As-sume n semantic classes are generated by an ap-proach and n1 of them have corresponding SSCs(i.e., no appropriate SSC can be found for theremaining n-n1 semantic classes).
We define theMnDCG score of an approach (with respect toquery q) as,?????
?
=?1????
?
?????(SSC?)?i=1??mi=1(4.2)where?????
????
=0                                         ??
??
= 01??max?
?
[1, ??](????
??
,?  )
??
??
?
0(4.3)In the above formula, nDCG(Gi,j) is the nDCGscore of semantic class Gi,j; and ki denotes thenumber of semantic classes assigned to SSCi.
Fora list of queries, the MnDCG score of an algo-rithm is the average of all scores for the queries.The metric is designed to properly deal withthe following cases,464i).
One semantic class is wrongly split intomultiple ones: Punished by dividing ??
inFormula 4.3;ii).
A semantic class is too noisy to be as-signed to any SSC: Processed by the?n1/n?
in Formula 4.2;iii).
Fewer semantic classes (than the numberof SSCs) are produced: Punished in For-mula 4.3 by assigning a zero value.iv).
Wrongly merge multiple semanticclasses into one: The nDCG score of themerged one will be small because it iscomputed with respect to only one singleSSC.The gain values of nDCG for the three relev-ance levels (?Bad?, ?Fair?, and ?Good?)
are re-spectively -1, 1, and 2 in experiments.4.3 Experimental  Results4.3.1 Overall performance comparisonFigure 3 shows the performance comparison be-tween the approaches listed in Section 4.1, usingmetrics MnDCG@n (n=1?10).
Postprocessingis performed for all the approaches, where For-mula 3.2 is adopted to compute the similaritybetween semantic classes.
The results show thatthat the topic modeling approaches producehigher-quality semantic classes than the otherapproaches.
It indicates that the topic mixtureassumption of topic modeling can handle themulti-membership problem very well here.Among the alternative approaches, RASC clus-tering behaves better than item clustering.
Thereason might be that an item cannot belong tomultiple clusters in the two item clustering ap-proaches, while RASC clustering allows this.
Forthe RASC clustering approaches, although oneitem has the chance to belong to different seman-tic classes, one RASC can only belong to onesemantic class.Figure 3.
Quality comparison (MnDCG@n) amongapproaches (frequency threshold h = 4 in preprocess-ing; k = 5 in topic models)4.3.2 Preprocessing experimentsTable 4 shows the average query processing timeand results quality of the LDA approach, by va-rying frequency threshold h. Similar results areobserved for the pLSI approach.
In the table, h=1means no preprocessing is performed.
The aver-age query processing time is calculated over allitems in our dataset.
As the threshold h increases,the processing time decreases as expected, be-cause the input of topic modeling gets smaller.The second column lists the results quality(measured by MnDCG@10).
Interestingly, weget the best results quality when h=4 (i.e., theitems with frequency less than 4 are discarded).The reason may be that most low-frequencyitems are noisy ones.
As a result, preprocessingcan improve both results quality and processingefficiency; and h=4 seems a good choice in pre-processing for our dataset.hAvg.
Query Proc.Time (seconds)Quality(MnDCG@10)1 0.414 0.2812 0.375 0.2943 0.320 0.3224 0.268 0.3315 0.232 0.3286 0.210 0.3157 0.197 0.3158 0.184 0.3139 0.173 0.288Table 4.
Time complexity and quality comparisonamong LDA approaches of different thresholds4.3.3 Postprocessing experimentsFigure 4.
Results quality comparison among topicmodeling approaches with and without postprocessing(metric: MnDCG@10)The effect of postprocessing is shown in Figure4.
In the figure, NP means no postprocessing isperformed.
Sim1 and Sim2 respectively meanFormula 3.1 and Formula 3.2 are used in post-processing as the similarity measure between00.050.10.150.20.250.30.350.40.451 2 3 4 5 6 7 8 9 10pLSI LDA KMedoids-RASCDBSCAN-RASC KMedoids-Item DBSCAN-Itemn0.270.280.290.30.310.320.330.34LDA pLSINPSim1Sim2465semantic classes.
The same preprocessing (h=4)is performed in generating the data.
It can beseen that postprocessing improves results quality.Sim2 achieves more performance improvementthan Sim1, which demonstrates the effectivenessof the similarity measure in Formula 3.2.4.3.4 Sample resultsTable 5 shows the semantic classes generated byour LDA approach for some sample queries inwhich the bad classes or bad members are hig-hlighted (to save space, 10 items are listed here,and the query itself is omitted in the resultantsemantic classes).Query Semantic ClassesappleC1: ibm, microsoft, sony, dell, toshiba,  sam-sung, panasonic, canon, nec, sharp ?C2: peach, strawberry, cherry, orange, bana-na, lemon, pineapple, raspberry, pear, grape?goldC1: silver, copper, platinum, zinc, lead, iron,nickel, tin, aluminum, manganese ?C2: silver, red, black, white, blue, purple,orange, pink, brown, navy ?C3: silver, platinum, earrings, diamonds,rings, bracelets, necklaces, pendants, jewelry,watches ?C4: silver, home, money, business, metal,furniture, shoes, gypsum, hematite, fluorite?lincolnC1: ford, mazda, toyota, dodge, nissan, hon-da, bmw, chrysler, mitsubishi, audi ?C2: bristol, manchester, birmingham, leeds,london, cardiff, nottingham, newcastle, shef-field, southampton ?C3: jefferson, jackson, washington, madison,franklin, sacramento, new york city, monroe,Louisville, marion ?computerscienceC1: chemistry, mathematics, physics, biolo-gy, psychology, education, history, music,business, economics ?Table 5.
Semantic classes generated by our approachfor some sample queries (topic model = LDA)5 Related WorkSeveral categories of work are related to ours.The first category is about set expansion (i.e.,retrieving one semantic class given one term or acouple of terms).
Syntactic context information isused (Hindle, 1990; Ruge, 1992; Lin, 1998) tocompute term similarities, based on which simi-lar words to a particular word can directly bereturned.
Google sets is an online service which,given one to five items, predicts other items inthe set.
Ghahramani and Heller (2005) introducea Bayesian Sets algorithm for set expansion.
Setexpansion is performed by feeding queries toweb search engines in Wang and Cohen (2007)and Kozareva (2008).
All of the above work onlyyields one semantic class for a given query.Second, there are pattern-based approaches in theliterature which only do limited integration ofRASCs (Shinzato and Torisawa, 2004; Shinzatoand Torisawa, 2005; Pasca, 2004), as discussedin the introduction section.
In Shi et al (2008),an ad-hoc approach was proposed to discover themultiple semantic classes for one item.
The thirdcategory is distributional similarity approacheswhich provide multi-membership support (Har-ris, 1985; Lin  and Pantel, 2001; Pantel and Lin,2002).
Among them, the CBC algorithm (Panteland Lin, 2002) addresses the multi-membershipproblem.
But it relies on term vectors and centro-ids which are not available in pattern-based ap-proaches.
It is therefore not clear whether it canbe borrowed to deal with multi-membership here.Among the various applications of topicmodeling, maybe the efforts of using topic modelfor Word Sense Disambiguation (WSD) are mostrelevant to our work.
In Cai et al(2007), LDA isutilized to capture the global context informationas the topic features for better performing theWSD task.
In Boyd-Graber et al (2007), LatentDirichlet with WordNet (LDAWN) is developedfor simultaneously disambiguating a corpus andlearning the domains in which to consider eachword.
They do not generate semantic classes.6 ConclusionsWe presented an approach that employs topicmodeling for semantic class construction.
Givenan item q, we first retrieve all RASCs containingthe item to form a collection CR(q).
Then we per-form some preprocessing to CR(q) and build atopic model for it.
Finally, the output semanticclasses of topic modeling are post-processed togenerate the final semantic classes.
For the CR(q)which contains a lot of RASCs, we perform of-fline processing according to the above processand store the results on disk, in order to reducethe online query processing time.We also proposed an evaluation methodologyfor measuring the quality of semantic classes.We show by experiments that our topic modelingapproach outperforms the item clustering andRASC clustering approaches.AcknowledgmentsWe wish to acknowledge help from XiaokangLiu for mining RASCs from web pages, Chan-gliang Wang and Zhongkai Fu for data process.466ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022.Bruce Croft, Donald Metzler, and Trevor Strohman.2009.
Search Engines: Information Retrieval inPractice.
Addison Wesley.Jordan Boyd-Graber, David Blei, and XiaojinZhu.2007.
A topic model for word sense disambig-uation.
In Proceedings EMNLP-CoNLL 2007, pag-es 1024?1033, Prague, Czech Republic, June.Association for Computational Linguistics.Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh.
2007.NUS-ML: Improving word sense disambiguationusing topic features.
In Proceedings of the Interna-tional Workshop on Semantic Evaluations, volume4.Scott Deerwester, Susan T. Dumais, GeorgeW.
Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Journalof the American Society for Information Science,41:391?407.Zoubin Ghahramani and Katherine A. Heller.
2005.Bayesian Sets.
In Advances in Neural InformationProcessing Systems (NIPS05).Thomas L. Griffiths, Mark Steyvers, David M.Blei,and Joshua B. Tenenbaum.
2005.
Integratingtopics and syntax.
In Advances in Neural Informa-tion Processing Systems 17, pages 537?544.
MITPressZellig Harris.
Distributional Structure.
The Philoso-phy of Linguistics.
New York: Oxford UniversityPress.
1985.Donald Hindle.
1990.
Noun Classification from Pre-dicate-Argument Structures.
In Proceedings ofACL90, pages 268?275.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proceedings of the 22nd annual inter-national ACM SIGIR99, pages 50?57, New York,NY, USA.
ACM.Kalervo Jarvelin, and Jaana Kekalainen.
2000.
IREvaluation Methods for Retrieving Highly Rele-vant Documents.
In Proceedings of the 23rd An-nual International ACM SIGIR Conference onResearch and Development in Information Retriev-al (SIGIR2000).Zornitsa Kozareva, Ellen Riloff and Eduard Hovy.2008.
Semantic Class Learning from the Web withHyponym Pattern Linkage Graphs, In Proceedingsof ACL-08.Wei Li, David M. Blei, and Andrew McCallum.
Non-parametric Bayes Pachinko Allocation.
In Proceed-ings of Conference on Uncertainty in Artificial In-telligence (UAI), 2007.Dekang Lin.
1998.
Automatic Retrieval and Cluster-ing of Similar Words.
In Proceedings of COLING-ACL98, pages 768-774.Dekang Lin and Patrick Pantel.
2001.
Induction ofSemantic Classes from Natural Language Text.
InProceedings of SIGKDD01, pages 317-322.Hiroaki Ohshima, Satoshi Oyama, and Katsumi Tana-ka.
2006.
Searching coordinate terms with theircontext from the web.
In WISE06, pages 40?47.Patrick Pantel and Dekang Lin.
2002.
DiscoveringWord Senses from Text.
In Proceedings ofSIGKDD02.Marius Pasca.
2004.
Acquisition of CategorizedNamed Entities for Web Search.
In Proc.
of 2004CIKM.Gerda Ruge.
1992.
Experiments on Linguistically-Based Term Associations.
In InformationProcessing & Management, 28(3), pages 317-32.Andrew I. Schein,  Alexandrin Popescul,  Lyle H.Ungar and David M. Pennock.
2002.
Methods andmetrics for cold-start recommendations.
In Pro-ceedings of SIGIR02, pages  253-260.Shuming Shi, Xiaokang Liu and Ji-Rong Wen.
2008.Pattern-based Semantic Class Discovery with Mul-ti-Membership Support.
In CIKM2008, pages1453-1454.Keiji Shinzato and Kentaro Torisawa.
2004.
Acquir-ing Hyponymy Relations from Web Documents.
InHLT/NAACL04, pages 73?80.Keiji Shinzato and Kentaro Torisawa.
2005.
A SimpleWWW-based Method for Semantic Word ClassAcquisition.
In RANLP05.Richard C. Wang and William W. Cohen.
2007.
Lan-gusage-Independent Set Expansion of Named Enti-ties Using the Web.
In ICDM2007.467
