Coling 2010: Poster Volume, pages 463?471,Beijing, August 2010Learning to Annotate Scientific PublicationsMinlie HuangState Key Laboratory of IntelligentTechnology and Systems,Dept.
Computer Science and Tech-nology, Tsinghua Universityaihuang@tsinghua.edu.cnZhiyong LuNational Center for Bio-technology Information (NCBI),U.S. National Library of Medi-cine, National Institutes of Healthluzh@ncbi.nlm.nih.govAbstractAnnotating scientific publications withkeywords and phrases is of greatimportance to searching, indexing, andcataloging such documents.
Unlikeprevious studies that focused on user-centric annotation, this paper presents ourinvestigation of various annotationcharacteristics on service-centric anno-tation.
Using a large number of publiclyavailable annotated scientific publica-tions, we characterized and compared thetwo different types of annotationprocesses.
Furthermore, we developed anautomatic approach of annotatingscientific publications based on amachine learning algorithm and a set ofnovel features.
When compared to othermethods, our approach shows significant-ly improved performance.
Experimentaldata sets and evaluation results are pub-licly available at the supplementary web-site1.1 IntroductionWith the rapid development of the Internet, theonline document archive is increasing quicklywith a growing speed.
Such a large volume andthe rapid growth pose great challenges for docu-ment searching, indexing, and cataloging.
Tofacilitate these processes, many concepts havebeen proposed, such as Semantic Web (Berners-Lee et al, 2001), Ontologies (Gruber, 1993),Open Directory Projects like Dmoz2, folksono-1 http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/indexing2 http://www.dmoz.org/mies (Hotho et al, 2006), and social tagging sys-tems like Flickr and CiteULike.
Annotating doc-uments or web-pages using Ontologies and OpenDirectories are often limited to a manually con-trolled vocabulary (developed by service provid-ers) and a small number of expert annotators,which we call service-centric annotation.
Bycontrast, social tagging systems in which regis-tered users can freely use arbitrary words to tagimages, documents or web-pages, belong to us-er-centric annotation.
Although many advantag-es have been reported in user-centric annotation,low-quality and undesired annotations are alwaysobserved due to uncontrolled user behaviors (Xuet al, 2006; Sigurbj?rnsson and Zwol, 2008).Moreover, the vocabulary involved in user-centric annotation is arbitrary, unlimited, andrapid-growing in nature, causing more difficul-ties in tag-based searching and browsing (Bao etal., 2007; Li et al, 2007).Service-centric annotation is of importance formanaging online documents, particularly in serv-ing high-quality repositories of scientific litera-ture.
For example, in biomedicine, Gene Ontolo-gy (Ashburner et al, 2000) annotation has beenfor a decade an influential research topic of un-ifying reliable biological knowledge from thevast amount of biomedical literature.
Documentannotation can also greatly help service providerssuch as ACM/IEEE portals to provide better userexperience of search.
Much work has been de-voted to digital document annotation, such asontology-based (Corcho, 2006) and semantic-oriented (Eriksson, 2007).This paper focuses on service-centric annota-tion.
Our task is to assign an input document alist of entries.
The entries are pre-defined by acontrolled vocabulary.
Due to the data availabili-ty, we study the documents and vocabulary in the463biomedical domain.
We first analyze human an-notation behaviors in two millions previouslyannotated documents.
When compared to user-centric annotation, we found that the two annota-tion processes have major differences and thatthey also share some common grounds.
Next, wepropose to annotate new articles with a learningmethod based on the assumption that documentssimilar in content share similar annotations.
Tothis end, we utilize a logistic regression algo-rithm with a set of novel features.
We evaluateour approach with extensive experiments andcompare it to the state of the art.
The contribu-tions of this work are two-fold: First, we presentan in-depth analysis on annotation behaviors be-tween service-centric and user-centric annotation.Second, we develop an automatic method forannotating scientific publications with significantimprovements over other systems.The remainder of the paper is organized as fol-lows: We present several definitions in Section 2and the analysis of annotation behaviors in Sec-tion 3.
In Section 4, we presented the logisticregression algorithm for annotation.
Benchmark-ing results are shown in Section 5.
We surveyedrelated work in Section 6 and summarized ourwork in Section 7.2 DefinitionsA controlled vocabulary: V, a set of pre-specified entries for describing certain topics.Entries in the vocabulary are organized in a hie-rarchical structure.
This vocabulary can be mod-ified under human supervision.Vocabulary Entry: an entry in a controlledvocabulary is defined as a triplet: VE = (MT,synonyms, NodeLabels).
MT is a major term de-scribing the entry, and NodeLabels are a list ofnode labels in the hierarchical tree.
An entry isidentified by its MT, and a MT may have mul-tiple node labels as a MT may be mapped to sev-eral nodes of a hierarchical tree.Entry Binary Relation: ISA(VEi, VEj) meansentry VEj is a child of entry VEi, and SIB(VEi,VEj) meaning that VEj is a sibling of entry VEi.
Aset of relations determine the structure of a hie-rarchy.Entry Depth: the depth of an entry relative tothe root node in the hierarchy.
The root node hasa depth of 1 and the immediate children of a rootnode has a depth of 2, and so on.
A major termmay be mapped to several locations in the hie-rarchy, thus we have minimal, maximal, and av-erage depths for each MT.Given the above definitions, a controlled vo-cabulary is defined as {<VEi, ISA(VEi,VEj),SIB(VEi,VEj)>|any i, j }.
The annotation task isstated as follows: given a document D, predictinga list of entries VEs that are appropriate for anno-tating the document.
In our framework, we ap-proach the task as a ranking problem, as detailedin Section 4.3 Analyzing Service-centric AnnotationBehaviorAnalyzing annotation behaviors can greatly faci-litate assessing annotation quality, reliability, andconsistency.
There has been some work on ana-lyzing social tagging behaviors in user-centricannotation systems (Sigurbj?rnsson and Zwol,2008; Suchanek et al, 2008).
However, to thebest of our knowledge, there is no such analysison service-centric annotation.
In social taggingsystems, no specific skills are required for partic-ipating; thus users can tag the resources with ar-bitrary words (the words may even be totally ir-relevant to the content, such as ?todo?).
By con-trast, in service-centric annotation, the annotatorsmust be trained, and they must comply with a setof strict guidelines to assure the consistent anno-tation quality.
Therefore, it is valuable to studythe differences between the two annotationprocesses.3.1 PubMed Document CollectionTo investigate annotation behaviors, we down-loaded 2 million documents from PubMed3, oneof the largest search portals for biomedical ar-ticles.
These articles were published from Jan. 1,2000 to Dec. 31, 2008.
All these documents havebeen manually annotated by National LibraryMedicine (NLM) human curators.
The controlledvocabulary used in this system is the MedicalSubject Headings (MeSH?
)4, a thesaurus describ-ing various biomedical topics such as diseases,chemicals and drugs, and organisms.
There are25,588 entries in the vocabulary in 2010, andthere are updates annually.
By comparison, thevocabulary used in user-centric annotation is re-3 http://www.ncbi.nlm.nih.gov/pubmed/4 http://www.nlm.nih.gov/mesh/464markably larger (usually more than 1 million tags)and more dynamic (may be updated every day).3.2 Annotation CharacteristicsFirst, we examine the distribution of the numberof annotated entries in the document collection.For each number of annotated entries, wecounted the number of documents with respect todifferent numbers of annotations.
The number ofannotations per document among these 2 milliondocuments varies from 1 (with 176,383 docu-ments) to 97 (with one document only).
The av-erage number of annotations per document is10.10, and the standard deviation is 5.95.Figure 1.
The original distribution and simulatednormal distribution.
Each data point denotes thenumber of documents (y-axis) that has the cor-responding number of entries (x-axis).As illustrated in Figure 1, when there are morethan 4 annotations, the distribution fits a normaldistribution.
Comparing with user-centric annota-tion, there are three notable observations: a), themaximal number of annotations per document(97) is much smaller (in social tagging systemsthe number amounts to over 104) due to muchless annotators involved in service-centric anno-tation than users in user-centric annotation; b),the number of annotations assigned to documentsconforms to a normal distribution, which has notyet been reported in user-centric annotation; c),similar to user-centric annotation, the number ofdocuments that have only one annotation ac-counts for a large proportion.Second, we investigate whether the Zipf law(Zipf, 1949) holds in service-centric annotation.To this end, we ranked all the entries accordingto the frequency of being annotated to docu-ments.
We plotted the curve in logarithm scale,as illustrated in Figure 2.
The curve can be simu-lated by a linear function in logarithm scale ifignoring the tail which corresponds to very infre-quently used entries.
To further justify this find-ing, we ranked all the documents according tothe number of assigned annotations and plottedthe curve in logarithm scale, as shown in Figure3.
Similar phenomenon is observed.
In conclu-sion, the Zipf law also holds in service-centricannotation, just as reported in user-centric anno-tation (Sigurbj?rnsson and Zwol, 2008).Figure 2.
The distribution of annotated entryfrequency.
X-axis is the rank of entries (rankingby the annotation frequency), and y-axis is thefrequency of an entry being used in annotation.Figure 3.
The distribution of the number of an-notated entries.
X-axis is the rank of a document(in log10 scale), and y-axis is the number of anno-tations assigned to documents (in log2 scale).Furthermore, as mentioned in Section 2, thevocabulary corresponds to a hierarchy tree once aset of binary relations were defined.
Thus wecan easily obtain the minimal, maximal, and av-erage depth of an entry.
The larger depth an entryhas, the more specific meaning it has.Therefore, we investigate whether service-centric annotation is performed at very specificlevel (with larger depth) or general level (withsmaller depth).
We define prior depth and anno-tation depth for this study, as follows:( )PriorDepth                    (1)| |VE VDep VEV?=?0200004000060000800001000001200001400001600001800002000001 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52originalnormalNumber of annotated entriesNumber of documents11001000010000001 10 100 1000 10000 10000012481632641281 10 100 1000 10000 100000Rank of the documentNumber of annotations465AnnoDepth Pr( )* ( )     (2)VE VVE Dep VE?=?
( )Pr( )                               (3)( )VE Vf VEVEf VE?=?where Dep(VE) is the minimal, maximal, or av-erage depth of an entry, f(VE) is the usage fre-quency of VE in annotation, and |V| is the num-ber of entries in the vocabulary.
The two formu-las are actually the mathematical expectations ofthe hierarchy?s depth under two distributions re-spectively: a uniform distribution (1/|V|) and theannotation distribution (formula (3)).
As shownin Table 1, the two expectations are close.
Thismeans the annotation has not been biased to ei-ther general or specific level, which suggests thatthe annotation quality is sound.Dep(VE) PriorDepth AnnoDepthMAX 4.88 4.56MIN 4.25 4.02AVG 4.56 4.29Table 1.
Annotation depth comparison.Figure 4.
The imbalance frequency (y-axis) ofannotated categories (x-axis).3.3 Annotation Categorization ImbalanceWe investigate here whether service-centric an-notation is biased to particular categories in thehierarchy.
We define a category as the label ofroot nodes in the hierarchy.
In our vocabulary,there are 11 categories that have at least one an-notation.
The complete list of these categories isavailable at the website5 .
Three newly createdcategories have no annotations in the documentcollection.
The total number of annotations with-in a category was divided by the number of en-5 http://www.nlm.nih.gov/mesh/2010/mesh_browser/MeSHtree.Z.htmltries in that category, as different categories mayhave quite different numbers of entries.
If an en-try is mapped to multiple locations, its annota-tions will be counted to corresponding categoriesrepeatedly.From Figure 4, we can see that there is imbal-ance with respect to the annotations in differentcategories.
Category ?diseases?
has 473.5 anno-tations per entry (totally 4408 entries in this cat-egory).
Category ?chemicals and drugs?
has423.0 annotations per entry (with 8815 entries intotal).
Due to the fact that diseases and chemicalsand drugs are hot scientific topics, these catego-ries are largely under-annotated.
The most fre-quently annotated category is: ?named groups?
(7144.4 annotations per entry), with 199 entriesin total.
The issue of imbalanced categorizationmay be due to that the topics of the documentcollection are of imbalance; and that the vocabu-lary was updated annually, so that the latest en-tries were used less frequently.
As shown in (Si-gurbj?rnsson and Zwol, 2008), this imbalanceissue was also observed in user-centric annota-tion, such as in Flickr Tagging.4 Learning to AnnotateAs shown in Section 3, there are much fewer an-notations per document in service-centric annota-tion than in user-centric annotations.
Service-centric annotation is of high quality, and is li-mited to a controlled vocabulary.
However, ma-nual annotation is time-consuming and labor in-tensive, particularly when seeking high quality.Indeed, our analysis shows that on average ittakes over 90 days for a PubMed citation to bemanually annotated with MeSH terms.
Thus wepropose to annotate articles automatically.
Spe-cifically, we approach this task as a rankingproblem: First, we retrieve k-nearest neighboring(KNN) documents for an input document using aretrieval model (Lin and Wilbur, 2007).
Second,we obtain an initial list of annotated entries fromthose retrieved neighboring documents.
Third,we rank those entries using a logistic regressionmodel.
Finally, the top N ranked entries are sug-gested as the annotations for the target document.4.1 Logistic RegressionWe propose a probabilistic framework of directlyestimating the probability that an entry can beused to annotate a document.
Given a document010002000300040005000600070008000Anatomy[A]Organisms [B]Diseases[C]ChemicalsandDrugs [D]Analytical,Diagnosticand ?PsychiatryandPsychology[F]PhenomenaandProcesses ?Disciplinesand?Anthropology, Education,?Technology, Industry, ?InformationScience[L]NamedGroups [M]HealthCare[N]466D and an entry VE, we compute the probabilityPr(R(VE)|D) directly using a logistic regressionalgorithm.
R(VE) is a binary random variableindicating whether VE should be assigned as anannotation of the document.
According to thisprobability, we can rank the entries obtainedfrom neighboring documents.
Much work usedLogistic Regression as classification: Pr(R=1|D)>?
where ?
is a threshold, but it is difficult tospecify an appropriate value for the threshold inthis work, as detailed in Section 5.5.We applied the logistic regression model tothis task.
Logistic regression has been successful-ly employed in many applications including mul-tiple ranking list merging (Si and Callan, 2005)and answer validation for question answering(Ko et al, 2007).
The model gives the followingprobability:1 1Pr( ( ) | ) exp( * ) 1 exp( * )       (4)m mi i i ii iR VE D b w x b w x= =?
?= + + +?
??
??
?where x= (x1, x2, ?, xm) is the feature vector forVE and m is the number of features.For an input document D, we can obtain an in-itial list of entries {VE1,VE2,?,VEn} from itsneighboring documents.
Each entry is thenrepresented as a feature vector as x= (x1, x2, ?,xm).
Given a collection of N documents that havebeen annotated manually, each document willhave a corresponding entry list, {VE1,VE2,?,VEn}, and each VEi has gold-standard la-bel yi=1 if VEi was used to annotate D, or yi=0otherwise.
Note that the number of entries of la-bel 0 is much larger than that of label 1 for eachdocument.
This may bias the learning algorithm.We will discuss this in Section 5.5.
Given suchdata, the parameters can be estimated using thefollowing formula:( )* *, 1 1, argmax log Pr( ( ) | )          (5)jLNi jw b j iw b R VE D= == ?
?where Lj is the number of entries to be ranked forDj, and N is the total number of training docu-ments.
We can use the Quasi-Newton algorithmfor parameter estimation (Minka, 2003).
In thispaper, we used the WEKA6 package to imple-ment this model.4.2 FeaturesWe developed various novel features to buildconnections between an entry and the document6http://www.cs.waikato.ac.nz/ml/weka/ .text.
When computing these features, both theentry?s text (major terms, synonyms) and thedocument text (title and abstract) are tokenizedand stemmed.
To compute these features, wecollected a set of 13,999 documents (each hastitle, abstract, and annotations) from PubMed.Prior probability feature.
We compute theappearance probability of a major term (MT),estimated on the 2 million documents.
This priorprobability reflects the prior quality of an entry.Unigram overlap with the title.
We count thenumber of unigrams overlapping between the MTof an entry and the title, dividing by the totalnumber of unigrams in the MT.Bigram overlap with the document.
We firstconcatenate the title and abstract, then count thenumber of bigram overlaps between the MT andthe concatenated string, dividing by the totalnumber of bigrams in the MT.Multinomial distribution feature.
This fea-ture assumes that the words in a major term ap-pear in the document text with a multinomialdistribution, as follows:#( , )Pr( | )Pr( | ) | | !
*     (6)#( , )!w MTw MTw TextMT Text MTw MT?= ?#( , )Pr( | ) (1 ) Pr ( )   (7)#( , )iciww Textw Text ww Text?
?= ?
+?where:#(w,MT) - The number of times that w appears inMT; Similarly for #(w,Text);|MT| - The number of single words in MT;Text - Either the title or abstract, thus we havetwo features of this type: Pr(MT|Title) andPr(MT|Abstract);Prc(w) - The probability of word w occurring in abackground corpus.
This is obtained from a uni-gram language model that was estimated on the13,999 articles;?
?
A smoothing parameter that was empiricallyset to be 0.2.Query-likelihood features.
The major term ofan entry is viewed as a query, and this class offeatures computes likelihood scores between thequery (as Q) and the article D (either the title orthe abstract).
We used the very classic okapimodel (Robertson et al 1994), as follows:( ) 0.5( , )*log( ) 0.5( , )   (8)| |0.5 1.5* ( , )(| |)q QN df qtf q Ddf qOkapi Q DDtf q Davg D??
??
+?
?+?
?= ?
?+ +?
??
?
?467where:tf(q,D) - The count of q occurring in document D;|D| - The total word counts in document D;df(q) - The number of documents containingword q;avg(|D|) - The average length of documents inthe collection;N - The total number of documents (13,999).We have two features: okapi(MT, Title) andokapi(MT, Abstract).
In other words, the title andabstract are processed separately.
The advantageof using such query-likelihood scores is that theygive a probability other than a binary judgmentof whether a major term should be annotated tothe article, as only indirect evidence exists forannotating a vocabulary entry to an article inmost cases.Neighborhood features.
The first featurerepresents the number of neighboring documentsthat include the entry to be annotated for a doc-ument.
The second feature, instead of countingdocuments, sums document similarity scores.The two features are formulated as follows, re-spectively:{ }( | ) | ,     (9)i i i kfreq MT D D MT D D= ?
??
;( | ) ( , )          (10)i i kiMT D Dsim MT D sim D D?
?
?= ?where ?k is the k-nearest neighbors for an inputdocument D and sim(Di,Dj) is the similarity scorebetween a target document and its neighboringdocument, given by the retrieval model.Synonym Features.
Each vocabulary entryhas synonyms.
We designed two binary features:one judges whether there exists a synonym thatcan be exactly matched to the article text (titleand abstract); and the other measures whetherthere exists a synonym whose unigram wordshave all been observed in the article text.5 Experiment5.1 DatasetsTo justify the effectiveness of our method, wecollected two datasets.
We randomly selected aset of 200 documents from PubMed to train thelogistic regression model (named Small200).
Fortesting, we used a benchmark dataset, NLM2007,which has been previously used in benchmarkingbiomedical document annotation7 (Aronson et al,7http://ii.nlm.nih.gov/.2004; Vasuki and Cohen, 2009; Trieschnigg etal., 2009).
The two datasets have no overlap withthe aforementioned 13,999 documents.
Eachdocument in these two sets has only title and ab-stract (i.e., no full text).
The statistics listed inTable 2 show that the two datasets are alike interms of annotations.
Note that we also evaluateour method on a larger dataset of 1000 docu-ments, but due to the length limit, the results arenot presented in this paper.Dataset DocumentsTotalannotationsAverageannotationsSmall200 200 2,736 13.7NLM2007 200 2,737 13.7Table 2.
Statistics of the two datasets.5.2 Evaluation MetricsWe use precision, recall, F-score, and mean av-erage precision (MAP) to evaluate the rankingresults.
As can be seen from Section 3.2, thenumber of annotations per document is about 10.Thus we evaluated the performance with top 10and top 15 items.5.3 Comparison to Other ApproachesWe compare our approach to three methods onthe benchmark dataset - NLM2007.
The first sys-tem is NLM?s MTI system (Aronson et al, 2004).This is a knowledge-rich method that employsNLP techniques, biomedical thesauruses, and aKNN module.
It also utilizes handcrafted filteringrules for refinement.
The second and third me-thods rank entries according to Formula (9) and(10), respectively (Trieschnigg et al, 2009).We trained our model on Small200.
All fea-ture values were normalized to [0,1] using themaximum values of each feature.
The number ofneighbors was set to be 20.
Neighboring docu-ments were retrieved from PubMed using theretrieval model described in (Lin and Wilbur,2007).
Existing document annotations were notused in retrieving similar documents as theyshould be treated as unavailable for new docu-ments.
As the average number of annotations perdocument is around 13 (see Table 2), we com-puted precision, recall, F-score, and MAP withtop 10 and 15 entries, respectively.Results in Table 3 demonstrate that our me-thod outperforms all other methods.
It has sub-stantial improvements over MTI.
To justifywhether the improvement over using neighbor-468hood similarity is significant, we conducted thePaired t-test (Goulden, 1956).
When comparingresults of using learning vs. neighborhood simi-larity in Table 3, the p-value is 0.028 for top 10and 0.001 for top 15 items.
This shows that theimprovement achieved by our approach is statis-tically significant (at significance level of 0.05).Methods Pre.
Rec.
F. MAPTop10MTI .468 .355 .404 .400Frequency .635 .464 .536 .598Similarity .643 .469 .542 .604Learning .657 .480 .555 .622Top15MTI .404 .442 .422 .400Frequency .512 .562 .536 .598Similarity .524 .574 .548 .604Learning .539 .591 .563 .622Table 3.
Comparative results on NLM2007.5.4 Choosing Parameter kWe demonstrate here our search for the optimalnumber of neighboring documents in this task.As shown in Table 4, the more neighbors, thelarger number of gold-standard annotationswould be present in neighboring documents.With 20 neighbors a fairly high upper-bound re-call (UBR) is observed (about 85% of gold-standard annotations of a target document werepresent in its 20 neighbors?
annotations), and theaverage number of entries (Avg_VE) to be rankedis about 100.Figure 5.
The performance (y-axis) varies withthe number of neighbors (x-axis).MeasureThe number of neighboring documents5  10   15  20  25 30UBR .704 .793 .832 .856 .871 .882Avg_VE 38.8 64.1 83.6 102.2 119.7 136.4Table 4.
The upper-bound recall (UBR) and av-erage number of entries (Avg_VE) with differentnumber of neighboring documents.To investigate whether the number of neigh-boring documents affects performance, we expe-rimented with different numbers of neighboringdocuments.
We trained a model on Small200,and tested it on NLM2007.
The curves in Figure5 show that the performance becomes very closewhen choosing no less than 10 neighbors.
Thisinfers that reliable performance can be obtained.The best performance (F-score of 0.563) is ob-tained with 20 neighbors.
Thus, the parameter kis set to be 20.5.5 Data Imbalance IssueAs mentioned in Section 4.1, there is a data im-balance issue in our task.
For each document, weobtained an initial list of entries from 20 neigh-boring documents.
The average number of gold-standard annotations is about 13, while the aver-age number of entries to be ranked is around 100(see Table 4).
Thus the number of entries of label0 (negative examples) is much larger than that oflabel 1 (positive examples).
We did not applyany filtering strategy because the gold-standardannotations are not proportional to their occur-ring frequency in the neighboring documents.
Infact, as shown in Figure 6, the majority of gold-standard annotations appear in only few docu-ments among 20 neighbors.
For example, thereare about 250 gold-standard annotations appear-ing in only one of 20 neighboring documents and964 appearing in less than 6 neighboring docu-ments.
Therefore, applying any filtering strategybased on their occurrence in neighboring docu-ments may be harmful to the performance.Figure 6.
The distribution of annotations.
X-axisis the number of neighboring documents inwhich gold-standard annotations are found.5.6 Feature AnalysisTo investigate the impact of different groups offeatures, we performed a feature ablation study.The features were divided into four groups.
Foreach round of this study, we remove one groupof features from the entire feature set, re-train themodel on Small200, and then test the perfor-mance on NLM2007 with top 15 entries.
We di-vided the features into four independent groups:0.450.50.550.60.655 10 15 20 25 30MAPRecallF-scorePrecision0501001502002503001 2 3 4 5 6 7 8 9 1011121314151617181920Goldstandard annotations469prior probability features, neighborhood features,synonym features, and other features (includingunigram/bigram feature, query likelihood feature,etc., see Section 4.2).
Results in Table 5 showthat neighborhood features are dominant: remov-ing such features leads to a remarkable decreasein performance.
On the other hand, using onlyneighborhood features (the last row) yields sig-nificant worse results than using all features.This means that combining all features togetherindeed contributes to the optimal performance.Feature Set Pre.
Rec.
F. MAPAll features .539 .591 .563 .622- Prior probability .538 .590 .563  .622- Neighborhood features .419* .459* .438*  .467*- Synonym features .532 .583 .556  .611- Other features .529 .580 .553  .621Only neighborhood features .523* .573* .547* .603*Table 5.
Feature analysis.
Those marked by starsare significantly worse than the best results.5.7 DiscussionsAll methods that rely on neighboring documentshave performance ceilings.
Specifically, for theNLM2007 dataset, the upper bound recall isaround 85.6% with 20 neighboring documents,as shown in Table 5.
Due to the same reason, thisgenre of methods is also limited to recommendentries that are recently added to the controlledvocabulary as such entries may have not beenannotated to any document yet.
This phenome-non has been demonstrated in the annotation be-havior analysis: those latest entries have substan-tially fewer annotations than older ones.6 Related WorkOur work is closely related to ontology-based orsemantic-oriented document annotation (Corcho,2006; Eriksson, 2007).
This work is also relatedto KNN-based tag suggestion or recommendationsystems (Mishne, 2006).The task here is similar to keyword extraction(Nguyen and Kan, 2007; Jiang et al, 2009), butthere is a major difference: keywords are alwaysoccurring in the document, while when an entryof a controlled vocabulary was annotated to adocument, it may not appear in text at all.As for the task tackled in this paper, i.e., anno-tating biomedical publications, three genres ofapproaches have been proposed: (1) k-NearestNeighbor model: selecting annotations fromneighboring documents, ranking and filteringthose annotations (Vasuki and Cohen, 2009; Tri-eschnigg et al, 2009).
(2) Classification model:learning the association between the documenttext and an entry (Ruch, 2006).
(3) Based onknowledge resources: using domain thesaurusesand NLP techniques to match an entry with con-cepts in the document text (Aronson, 2001;Aronson et al, 2004).
(4) LDA-based topic mod-el: (M?rchen et al, 2008).7 ConclusionThis paper presents a novel study on service-centric annotation.
Based on the analysis resultsof 2 million annotated scientific publications, weconclude that service-centric annotation exhibitsthe following unique characteristics: a) the num-ber of annotation per document is significantsmaller, but it conforms to a normal distribution;and b) entries of different granularity (general vs.specific) are used appropriately by the trainedannotators.
Service-centric and user-centric an-notations have in common that the Zipf lawholds and categorization imbalance exists.Based on these observations, we introduced alogistic regression approach to annotate publica-tions, with novel features.
Significant improve-ments over other systems were obtained on abenchmark dataset.
Although our features aretailored for this task in biomedicine, this ap-proach may be generalized for similar tasks inother domains.AcknowledgementsThis work was supported by the Intramural Re-search Program of the NIH, National Library ofMedicine.
The first author was also supported bythe Chinese Natural Science Foundation undergrant No.
60803075 and the grant from the Inter-national Development Research Center, Ottawa,Canada IRCI.ReferencesAlan R. Aronson.
Effective mapping of biomedicaltext to the UMLS Metathesaurus: the metamapprogram.
In Proc AMIA Symp 2001. p. 17-21.Alan Aronson, Alan R. Aronson, James Mork, JamesG.
Mork, Clifford Gay, Clifford W. Gay, SusanneHumphrey, Susanne M. Humphrey, Willie Rogers,Willie J. Rogers.
The NLM Indexing Initiative's470Medical Text Indexer.
Stud Health Technol In-form.
2004;107(Pt 1):268-72.Michael Ashburner, Catherine A.
Ball, Judith A.Blake, David Botstein, Heather Butler, et al GeneOntology: tool for the unification of biology.
NatGenet.
2000 May; 25(1):25-9.Shenghua Bao, Xiaoyuan Wu, Ben Fei, Guirong Xue,Zhong Su, and Yong Yu.
Optimizing Web SearchUsing Social Annotations.
WWW 2007, May 8?12,2007, Banff, Alberta, Canada.
Pp 501-510.Tim Berners-Lee, James Hendler and Ora Lassila.The Semantic Web.
Scientific American Magazine.
(May 17,  2001).Oscar Corcho.
Ontology based document annotation:trends and open research problems.
InternationalJournal of Metadata, Semantics and Ontologies,Volume 1,  Issue 1, Pages: 47-57, 2006.Henrik Eriksson.
An Annotation Tool for SemanticDocuments.
In Proceedings of the 4th Europeanconference on The Semantic Web: Research andApplications, pages 759-768, 2007.
Innsbruck,Austria.Cyril Harold Goulden.
Methods of Statistical Analy-sis, 2nd ed.
New York: Wiley, pp.
50-55, 1956.Thomas R. Gruber (1993).
A Translation Approach toPortable Ontology Specifications.
Knowledge Ac-quisition, 5(2), 1993, pp.
199-220.Andreas Hotho, Robert Jaschke, Christoph Schmitz,Gerd Stumme.
Information Retrieval in Folksono-mies: Search and Ranking.
In ?The Semantic Web:Research and Applications?, Vol.
4011 (2006), pp.411-426.Xin Jiang, Yunhua Hu, Hang Li.
A Ranking Ap-proach to Keyphrase Extraction.
SIGIR?09, July19?23, 2009, Boston, Massachusetts, USA.Jeongwoo Ko, Luo Si, Eric Nyberg.
A ProbabilisticFramework for Answer Selection in QuestionAnswering.
Proceedings of NAACL HLT 2007,pages 524?531, Rochester, NY, April 2007.Rui Li, Shenghua Bao, Ben Fei, Zhong Su, and YongYu.
Towards Effective Browsing of Large ScaleSocial Annotations.
In WWW ?07: Proceedings ofthe 16th international conference on World WideWeb, 2007.Jimmy Lin and W. John Wilbur.
PubMed related ar-ticles: a probabilistic topic-based model for contentsimilarity.
BMC Bioinformatics 8: (2007).Thomas P. Minka.
A Comparison of Numerical Op-timizers for Logistic Regression.
2003.
Unpub-lished draft.Gilad Mishne.
AutoTag: A Collaborative Approach toAutomated Tag Assignment for Weblog Posts.WWW 2006, May 22?26, 2006, Edinburgh, Scot-land.
pages 953?954.Fabian M?rchen, Math?us Dejori, Dmitriy Fradkin,Julien Etienne, Bernd Wachmann, Markus Bund-schus.
Anticipating annotations and emergingtrends in biomedical literature.
In KDD '08: pp.954-962.Thuy Dung Nguyen and Min-Yen Kan. KeyphraseExtraction in Scientific Publications.
In Proc.
of In-ternational Conference on Asian Digital Libraries(ICADL ?07), pages 317-326.Stephen E. Robertson, Steve Walker, Susan Jones,Micheline Hancock-Beaulieu, and Mike Gatford.Okapi at TREC-3.
In Proceedings of the Third TextREtrieval Conference (TREC 1994).
Gaithersburg,USA, November 1994.Patrick Ruch.
Automatic assignment of biomedicalcategories: toward a generic approach.
Bioinfor-matics.
2006 Mar 15;22(6):658-64.Luo Si and Jamie Callan.
2005 CLEF2005: Multilin-gual retrieval by combining multiple multilingualranked lists.
In Proceedings of Cross-LanguageEvaluation Forum.B?rkur Sigurbj?rnsson and Roelof van Zwol.
FlickrTag Recommendation based on Collective Know-ledge.
WWW 2008, April 21?25, 2008, Beijing,China.
Pp.
327-336.Fabian M. Suchanek, Milan Vojnovi?c, Dinan Guna-wardena.
Social Tags: Meaning and Suggestions.CIKM?08, October 26?30, 2008, Napa Valley, Cal-ifornia, USA.Dolf Trieschnigg, Piotr Pezik, Vivian Lee, Franciskade Jong, Wessel Kraaij, Dietrich Rebholz-Schuhmann.
MeSH Up: effective MeSH text clas-sification for improved document retrieval.
Bioin-formatics, Vol.
25 no.
11 2009, pages 1412?1418.Vidya Vasuki and Trevor Cohen.
Reflective RandomIndexing for Semiautomatic Indexing of the Bio-medical Literature.
AMIA 2009, San Francisco,Nov.
14-18, 2009.Zhichen Xu, Yun Fu, Jianchang Mao, and Difu Su.Towards the Semantic Web: Collaborative TagSuggestions.
In WWW2006: Proceedings of theCollaborative Web Tagging Workshop (2006).George K. Zipf.
(1949) Human Behavior and thePrinciple of Least-Effort.
Addison-Wesley.471
