Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 367?377,Berlin, Germany, August 7-12, 2016. c?2016 Association for Computational LinguisticsQuery Expansion with Locally-Trained Word EmbeddingsFernando DiazMicrosoftfdiaz@microsoft.comBhaskar MitraMicrosoftbmitra@microsoft.comNick CraswellMicrosoftnickr@microsoft.comAbstractContinuous space word embeddingshave received a great deal of atten-tion in the natural language processingand machine learning communities fortheir ability to model term similarityand other relationships.
We study theuse of term relatedness in the contextof query expansion for ad hoc informa-tion retrieval.
We demonstrate thatword embeddings such as word2vec andGloVe, when trained globally, under-perform corpus and query specific em-beddings for retrieval tasks.
These re-sults suggest that other tasks benefit-ing from global embeddings may alsobenefit from local embeddings.1 IntroductionContinuous space embeddings such asword2vec (Mikolov et al, 2013b) or GloVe(Pennington et al, 2014a) project terms ina vocabulary to a dense, lower dimensionalspace.
Recent results in the natural lan-guage processing community demonstrate theeffectiveness of these methods for analogyand word similarity tasks.
In general, theseapproaches provide global representations ofwords; each word has a fixed representation,regardless of any discourse context.
While aglobal representation provides some advan-tages, language use can vary dramatically bytopic.
For example, ambiguous terms can eas-ily be disambiguated given local informationin immediately surrounding words (Harris,1954; Yarowsky, 1993).
The window-basedtraining of word2vec style algorithms exploitsthis distributional property.A global word embedding, even whentrained using local windows, risks captur-ing only coarse representations of those top-ics dominant in the corpus.
While a par-ticular embedding may be appropriate for aspecific word within a sentence-length con-text globally, it may be entirely inappropri-ate within a specific topic.
Gale et al re-fer to this as the ?one sense per discourse?property (Gale et al, 1992).
Previous workby Yarowsky demonstrates that this propertycan be successfully combined with informa-tion from nearby terms for word sense dis-ambiguation (Yarowsky, 1995).
Our work ex-tends this approach to word2vec-style trainingin the context word similarity.For many tasks that require topic-specificlinguistic analysis, we argue that topic-specificrepresentations should outperform global rep-resentations.
Indeed, it is difficult to imaginea natural language processing task that wouldnot benefit from an understanding of the localtopical structure.
Our work focuses on a queryexpansion, an information retrieval task wherewe can study different lexical similarity meth-ods with an extrinsic evaluation metric (i.e.retrieval metrics).
Recent work has demon-strated that similarity based on global wordembeddings can be used to outperform clas-sic pseudo-relevance feedback techniques (Sor-doni et al, 2014; al Masri et al, 2016).We propose that embeddings be learned ontopically-constrained corpora, instead of largetopically-unconstrained corpora.
In a retrievalscenario, this amounts to retraining an em-bedding on documents related to the topic ofthe query.
We present local embeddings whichcapture the nuances of topic-specific languagebetter than global embeddings.
There issubstantial evidence that global methods un-derperform local methods for information re-367trieval tasks such as query expansion (Xu andCroft, 1996), latent semantic analysis (Hull,1994; Schu?tze et al, 1995; Singhal et al,1997), cluster-based retrieval (Tombros andvan Rijsbergen, 2001; Tombros et al, 2002;Willett, 1985), and term clustering (Attar andFraenkel, 1977).
We demonstrate that thesame holds true when using word embeddingsfor text retrieval.2 MotivationFor the purpose of motivating our approach,we will restrict ourselves to word2vec althoughother methods behave similarly (Levy andGoldberg, 2014).
These algorithms involvediscriminatively training a neural network topredict a word given small set of contextwords.
More formally, given a target word wand observed context c, the instance loss is de-fined as,`(w, c) = log ?(?
(w) ?
?
(c))+ ?
?
Ew?
?C[log ?(??
(w) ?
?
(w))]where ?
: V ?
<kprojects a term into a k-dimensional embedding space, ?
: Vm?
<kprojects a set of m terms into a k-dimensionalembedding space, and w is a randomly sam-pled ?negative?
context.
The parameter ?
con-trols the sampling of random negative terms.These matrices are estimated over a set of con-texts sampled from a large corpus and mini-mize the expected loss,Lc= Ew,c?pc[`(w, c)] (1)where pcis the distribution of word-contextpairs in the training corpus and can be esti-mated from corpus statistics.While using corpus statistics may makesense absent any other information, oftentimeswe know that our analysis will be topicallyconstrained.
For example, we might be analyz-ing the ?sports?
documents in a collection.
Thelanguage in this domain is more specializedand the distribution over word-context pairsis unlikely to be similar to pc(w, c).
In fact,prior work in information retrieval suggeststhat documents on subtopics in a collectionhave very different unigram distributions com-pared to the whole corpus (Cronen-Townsendet al, 2002).
Let pt(w, c) be the probabilitylog(weight)-1 0 1 2 3 4 5050100150Figure 1: Importance weights for terms occur-ring in documents related to ?argentina peg-ging dollar?
relative to frequency in gigaword.of observing a word-context pair conditionedon the topic t. The expected loss under thisdistribution is (Shimodaira, 2000),Lt= Ew,c?pc[pt(w, c)pc(w, c)`(w, c)](2)In general, if our corpus consists of sufficientlydiverse data (e.g.
Wikipedia), the support ofpt(w, c) is much smaller than and containedin that of pc(w, c).
The loss, `, of a con-text that occurs more frequently in the topic,will be amplified by the importance weight?
=pt(w,c)pc(w,c).
Because topics require special-ized language, this is likely to occur; at thesame time, these contexts are likely to be un-deremphasized in training a model accordingto Equation 1.In order to quantify this, we took a topicfrom a TREC ad hoc retrieval collection (seeSection 5 for details) and computed the im-portance weight for each term occurring inthe set of on-topic documents.
The histogramof weights ?
is presented in Figure 1.
Whilelarger probabilities are expected since the sizeof a topic-constrained vocabulary is smaller,there are a non-trivial number of terms withmuch larger importance weights.
If the loss,`(w), of a word2vec embedding is worse forthese words with low pc(w), then we expectthese errors to be exacerbated for the topic.Of course, these highly weighted terms mayhave a low value for pt(w) but a very highvalue relative to the corpus.
We can adjust the368KL0.000.050.100.15rankFigure 2: Pointwise Kullback-Leibler diver-gence for terms occurring in documents re-lated to ?argentina pegging dollar?
relative tofrequency in gigaword.weights by considering the pointwise Kullback-Leibler divergence for each word w,Dw(pt?pc) = pt(w) logpt(w)pc(w)(3)Words which have a much higher value ofpt(w) than pc(w) and have a high absolutevalue of pt(w) will have high pointwise KLdivergence.
Figure 2 shows the divergencesfor the top 100 most frequent terms in pt(w).The higher ranked terms (i.e.
good query ex-pansion candidates) tend to have much higherprobabilities than found in pc(w).
If the losson those words is large, this may result in poorembeddings for the most important words forthe topic.A dramatic change in distribution betweenthe corpus and the topic has implications forperformance precisely because of the objectiveused by word2vec (i.e.
Equation 1).
The train-ing emphasizes word-context pairs occurringwith high frequency in the corpus.
We willdemonstrate that, even with heuristic down-sampling of frequent terms in word2vec, thesetechniques result in inferior performance forspecific topics.Thus far, we have sketched out why usingthe corpus distribution for a specific topic mayresult in undesirable outcomes.
However, it iseven unclear that pt(w|c) = pc(w|c).
In fact,we suspect that pt(w|c) 6= pc(w|c) because ofthe ?one sense per discourse?
claim (Gale etal., 1992).
We can qualitatively observe thedifference in pc(w|c) and pt(w|c) by trainingglobal localcutting taxsqueeze deficitreduce voteslash budgetreduction reductionspend houselower billhalve plansoften spendfreeze billionFigure 3: Terms similar to ?cut?
for a word2vecmodel trained on a general news corpus andanother trained only on documents related to?gasoline tax?.two word2vec models: the first on the large,generic Gigaword corpus and the second on atopically-constrained subset of the gigaword.We present the most similar terms to ?cut?using both a global embedding and a topic-specific embedding in Figure 3.
In this case,the topic is ?gasoline tax?.
As we can see, the?tax cut?
sense of ?cut?
is emphasized in thetopic-specific embedding.3 Local Word EmbeddingsThe previous section described several reasonswhy a global embedding may result in over-general word embeddings.
In order to performtopic-specific training, we need a set of topic-specific documents.
In information retrievalscenarios users rarely provide the system withexamples of topic-specific documents, insteadproviding a small set of keywords.Fortunately, we can use information re-trieval techniques to generate a query-specificset of topical documents.
Specifically, weadopt a language modeling approach to do so(Croft and Lafferty, 2003).
In this retrievalmodel, each document is represented as a max-imum likelihood language model estimatedfrom document term frequencies.
Query lan-guage models are estimated similarly, usingterm frequency in the query.
A documentscore then, is the Kullback-Leibler divergencebetween the query and document language369models,D(pq?pd) =?w?Vpq(w) logpq(w)pd(w)(4)Documents whose language models are moresimilar to the query language model will havea lower KL divergence score.
For consistencywith prior work, we will refer to this as thequery likelihood score of a document.The scores in Equation 4 can be passedthrough a softmax function to derive a multi-nomial over the entire corpus (Lavrenko andCroft, 2001),p(d) =exp(?D(pq?pd))?d?exp(?D(pq?pd?
))(5)Recall in Section 2 that training a word2vecmodel weights word-context pairs accordingto the corpus frequency.
Our query-basedmultinomial, p(d), provides a weighting func-tion capturing the documents relevant to thistopic.
Although an estimation of the topic-specific documents from a query will be im-precise (i.e.
some nonrelevant documents willbe scored highly), the language use tends tobe consistent with that found in the knownrelevant documents.We can train a local word embedding us-ing an arbitrary optimization method by sam-pling documents from p(d) instead of uni-formly from the corpus.
In this work, we useword2vec, although any method that operateson a sample of documents can be used.4 Query Expansion with WordEmbeddingsWhen using language models for retrieval,query expansion involves estimating an alter-native to pq.
Specifically, when each expansionterm is associated with a weight, we normalizethese weights to derive the expansion languagemodel, pq+ .
This language model is then in-terpolated with the original query model,p1q(w) = ?pq(w) + (1?
?
)pq+(w) (6)This interpolated language model can thenbe used with Equation 4 to rank documents(Abdul-Jaleel et al, 2004).
We will refer tothis as the expanded query score of a docu-ment.Now we turn to using word embeddings forquery expansion.
Let U be an |V| ?
k termembedding matrix.
If q is a |V| ?
1 columnterm vector for a query, then the expansionterm weights are UUTq.
We then take the topk terms, normalize their weights, and computepq+(w).We consider the following alternatives forU.
The first approach is to use a globalmodel trained by sampling documents uni-formly.
The second approach, which we pro-pose in this paper, is to use a local modeltrained by sampling documents from p(d).5 Methods5.1 DataTo evaluate the different retrieval strategiesdescribed in Section 3, we use the followingdatasets.
Two newswire datasets, trec12 androbust, consist of the newswire documents andassociated queries from TREC ad hoc retrievalevaluations.
The trec12 corpus consists of Tip-ster disks 1 and 2; and the robust corpusconsists of Tipster disks 4 and 5.
Our thirddataset, web, consists of the ClueWeb 2009Category B Web corpus.
For the Web cor-pus, we only retain documents with a Water-loo spam rank above 70.1We present corpusstatistics in Table 1.We consider several publicly available globalembeddings.
We use four GloVe embed-dings of different dimensionality trained on theunion of Wikipedia and Gigaword documents.2We use one publicly available word2vec em-bedding trained on Google News documents.3We also trained a global embedding for trec12and robust using the entire corpus.
Instead oftraining a global embedding on the large webcollection, we use a GloVe embedding trainedon Common Crawl data.4We train local embeddings with word2vecusing one of three retrieval sources.
First, weconsider documents retrieved from the targetcorpus of the query (i.e.
trec12, robust, orweb).
We also consider training a local embed-1https://plg.uwaterloo.ca/~gvcormac/clueweb09spam/2http://nlp.stanford.edu/data/glove.6B.zip3https://code.google.com/archive/p/word2vec/4http://nlp.stanford.edu/data/glove.840B.300d.zip370docs words queriestrec12 469,949 438,338 150robust 528,155 665,128 250web 50,220,423 90,411,624 200news 9,875,524 2,645,367 -wiki 3,225,743 4,726,862 -Table 1: Corpora used for retrieval and localembedding training.ding by performing a retrieval on large auxil-iary corpora.
We use the Gigaword corpus asa large auxiliary news corpus.
We hypothe-size that retrieving from a larger news corpuswill provide substantially more local trainingdata than a target retrieval.
We also use aWikipedia snapshot from December 2014.
Wehypothesize that retrieving from a large, highfidelity corpus will provide cleaner languagethan that found in lower fidelity target do-mains such as the web.
Table 1 shows therelative magnitude of these auxiliary corporacompared to the target corpora.All corpora in Table 1 were stopped usingthe SMART stopword list5and stemmed us-ing the Krovetz algorithm (Krovetz, 1993).We used the Indri implementation for indexingand retrieval.65.2 EvaluationWe consider several standard retrieval eval-uation metrics, including NDCG@10 and in-terpolated precision at standard recall points(Ja?rvelin and Keka?la?inen, 2002; van Rijsber-gen, 1979).
NDCG@10 provides insight intoperformance specifically at higher ranks.
Aninterpolated precision recall graph describessystem performance throughout the entireranked list.5.3 TrainingAll retrieval experiments were conductedby performing 10-fold cross-validation acrossqueries.
Specifically, we cross-validatethe number of expansion terms, k ?
{5, 10, 25, 50, 100, 250, 500}, and interpolationweight, ?
?
[0, 1].
For local word2vec train-ing, we cross-validate the learning rate ?
?
{10?1, 10?2, 10?3}.5http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop6http://www.lemurproject.org/indri/All word2vec training used the publiclyavailable word2vec cbow implementation.7When training the local models, we sampled1000 documents from p(d) with replacement.To compensate for the much smaller corpussize, we ran word2vec training for 80 iter-ations.
Local word2vec models use a fixedembedding dimension of 400 although otherchoices did not significantly affect our results.Unless otherwise noted, default parameter set-tings were used.In our experiments, expanded queriesrescore the top 1000 documents from an ini-tial query likelihood retrieval.
Previous resultshave demonstrated that this approach resultsin performance nearly identical with an ex-panded retrieval at a much lower cost (Diaz,2015).
Because publicly available embeddingsmay have tokenization inconsistent with ourtarget corpora, we restricted the vocabularyof candidate expansion terms to those occur-ring in the initial retrieval.
If a candidate termwas not found in the vocabulary of the embed-ding matrix, we searched for the candidate ina stemmed version of the embedding vocabu-lary.
In the event that the candidate term wasstill not found after this process, we removedit from consideration.6 ResultsWe present results for retrieval experimentsin Table 2.
We find that embedding-basedquery expansion outperforms our query like-lihood baseline across all conditions.
Whenusing the global embedding, the news corporabenefit from the various embeddings in differ-ent situations.
Interestingly, for trec12, usingan embedding trained on the target corpus sig-nificantly outperforms all other global embed-dings, despite using substantially less data toestimate the model.
While this performancemay be due to the embedding having a tok-enization consistent with the target corpus, itmay also come from the fact that the corpusis more representative of the target documentsthan other embeddings which rely on onlinenews or are mixed with non-news content.
Tosome extent this supports our desire to movetraining closer to the target distribution.Across all conditions, local embeddings sig-7https://code.google.com/p/word2vec/371Table 2: Retrieval results comparing query expansion based on various global and local embed-dings.
Bolded numbers indicate the best expansion in that class of embeddings.
Wilcoxon signedrank test between bolded numbers indicates statistically significant improvements (p < 0.05) forall collections.global localwiki+giga gnews target target giga wikiQL 50 100 200 300 300 400 400 400 400trec12 0.514 0.518 0.518 0.530 0.531 0.530 0.545 0.535 0.563*0.523robust 0.467 0.470 0.463 0.469 0.468 0.472 0.465 0.475 0.517*0.476web 0.216 0.227 0.229 0.230 0.232 0.218 0.216 0.234 0.236 0.258*0.0 0.2 0.4 0.6 0.8 1.00.00.10.20.30.40.50.60.7trec12recallprecisionQLgloballocal0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.8robustrecallprecisionQLgloballocal0.0 0.2 0.4 0.6 0.8 1.00.00.10.20.30.40.50.6webrecallprecisionQLgloballocalFigure 4: Interpolated precision-recall curves for query likelihood, the best global embedding,and the best local embedding from Table 2.nificantly outperform global embeddings forquery expansion.
For our two news collec-tions, estimating the local model using a re-trieval from the larger Gigaword corpus led tosubstantial improvements.
This effect is al-most certainly due to the Gigaword corpus be-ing similar in writing style to the target cor-pus but, at the same time, providing signifi-cantly more relevant content (Diaz and Met-zler, 2006).
As a result, the local embeddingis trained using a larger variety of topical ma-terial than if it were to use a retrieval from thesmaller target corpus.
An embedding trainedwith a retrieval from Wikipedia tended to per-form worse most likely because the language isdissimilar from news content.
Our web col-lection, on the other hand, benefitted morefrom embeddings trained using retrievals fromthe general Wikipedia corpus.
The Gigawordcorpus was less useful here because news-stylelanguage is almost certainly not representativeof general web documents.Figure 4 presents interpolated precision-recall curves comparing the baseline, the bestglobal query expansion method, and the bestlocal query expansion method.
Interestingly,although global methods achieve strong per-formance for NDCG@10, these improvementsover the baseline are not reflected in ourprecision-recall curves.
Local methods, on theother hand, almost always strictly dominateboth the baseline and global expansion acrossall recall levels.The results support the hypothesis that lo-cal embeddings provide better similarity mea-sures than global embeddings for query expan-sion.
In order to understand why, we first com-pare the performance differences between localand global embeddings.
Figure 2 suggests thatwe should adopt a local embedding when thelocal unigram language model deviates fromthe corpus language model.
To test this, wecomputed the KL divergence between the lo-cal unigram distribution,?dp(w|d)p(d), andthe corpus unigram language model (Cronen-Townsend et al, 2002).
We hypothesize that,when this value is high, the topic languageis different from the corpus language and the372Table 3: Kendall?s ?
and Spearman?s ?
be-tween improvement in NDCG@10 and lo-cal KL divergence with the corpus languagemodel.
The improvement is measured for thebest local embedding over the best global em-bedding.?
?trec12 0.0585 0.0798robust 0.0545 0.0792web 0.0204 0.0283global embedding will be inferior to the localembedding.
We tested the rank correlation be-tween this KL divergence and the relative per-formance of the local embedding with respectto the global embedding.
These correlationsare presented in Table 3.
Unfortunately, wefind that the correlation is low, although it ispositive across collections.We can also qualitatively analyze the differ-ences in the behavior of the embeddings.
If wehave access to the set of documents labeled rel-evant to a query, then we can compute the fre-quency of terms in this set and consider thoseterms with high frequency (after stopping andstemming) to be good query expansion can-didates.
We can then visualize where theseterms lie in the global and local embeddings.In Figure 5, we present a two-dimensional pro-jection (van der Maaten and Hinton, 2008)of terms for the query ?ocean remote sens-ing?, with those good candidates highlighted.Our projection includes the top 50 candidatesby frequency and a sample of terms occurringin the query likelihood retrieval.
We noticethat, in the global embedding, the good can-didates are spread out amongst poorer candi-dates.
By contrast, the local embedding clus-ters the candidates in general but also situatesthem closely around the query.
As a result, wesuspect that the similar terms extracted fromthe local embedding are more likely to includethese good candidates.7 DiscussionThe success of local embeddings on this taskshould alarm natural language processing re-searchers using global embeddings as a rep-resentational tool.
For one, the approach oflearning from vast amounts of data is only ef-globallocalFigure 5: Global versus local embedding ofhighly relevant terms.
Each point represents acandidate expansion term.
Red points havehigh frequency in the relevant set of docu-ments.
White points have low or no frequencyin the relevant set of documents.
The bluepoint represents the query.
Contours indicatedistance from the query.fective if the data is appropriate for the taskat hand.
And, when provided, much smallerhigh-quality data can provide much better per-formance.
Beyond this, our results suggestthat the approach of estimating global repre-sentations, while computationally convenient,may overlook insights possible at query time,or evaluation time in general.
A similar localembedding approach can be adopted for anynatural language processing task where topi-cal locality is expected and can be estimated.Although we used a query to re-weight the cor-pus in our experiments, we could just as eas-ily use alternative contextual information (e.g.a sentence, paragraph, or document) in othertasks.Despite these strong results, we believe that373there are still some open questions in thiswork.
First, although local embeddings pro-vide effectiveness gains, they can be quite in-efficient compared to global embeddings.
Webelieve that there is opportunity to improvethe efficiency by considering offline computa-tion of local embeddings at a coarser level thanqueries but more specialized than the corpus.If the retrieval algorithm is able to select theappropriate embedding at query time, we canavoid training the local embedding.
Second,although our supporting experiments (Table 3,Figure 5) add some insight into our intuition,the results are not strong enough to providea solid explanation.
Further theoretical andempirical analysis is necessary.8 Related WorkTopical adaptation of models The short-comings of learning a single global vector rep-resentation, especially for polysemic words,have been pointed out before (Reisinger andMooney, 2010b).
The problem can be ad-dressed by training a global model with multi-ple vector embeddings per word (Reisinger andMooney, 2010a; Huang et al, 2012) or topic-specific embeddings (Liu et al, 2015).
Thenumber of senses for each word may be fixed(Neelakantan et al, 2015), or determined us-ing class labels (Trask et al, 2015).
However,to the best of our knowledge, this is the firsttime that training topic-specific word embed-dings has been explored.Several methods exist in the language mod-eling community for topic-dependent adapta-tion of language models (Bellegarda, 2004).These can lead to performance improvementsin tasks such as machine translation (Zhao etal., 2004) and speech recognition (Nanjo andKawahara, 2004).
Topic-specific data may begathered in advance, by identifying corpus oftopic-specific documents.
It may also be gath-ered during the discourse, using multiple hy-potheses from N-best lists as a source of topic-specific language.
Then a topic-specific lan-guage model is trained (or the global model isadapted) online using the topic-specific train-ing data.
A topic-dependent model may becombined with the global model using lin-ear interpolation (Iyer and Ostendorf, 1999)or other more sophisticated approaches (Fed-erico, 1996; Kuhn and De Mori, 1990).
Sim-ilarly to the adaptation work, we use topic-specific documents to train a topic-specificmodel.
In our case the documents come froma first round of retrieval for the user?s cur-rent query, and the word embedding modelis trained based on sentences from the topic-specific document set.
Unlike the past work,we do not focus on interpolating the local andglobal models, although this is a promisingarea for future work.
In the current studywe focus on a direct comparison between thelocal-only and global-only approach, for im-proving retrieval performance.Word embeddings for IR InformationRetrieval has a long history of learning repre-sentations of words that are low-dimensionaldense vectors.
These approaches can bebroadly classified into two families based onwhether they are learnt based on a term-document matrix or term co-occurence data.Using the term-document matrix for embed-ding leads to several well-studied approachessuch as LSA (Deerwester et al, 1990), PLSA(Hofmann, 1999), and LDA (Blei et al,2003; Wei and Croft, 2006).
The perfor-mance of these models varies depending on thetask, for example they are known to performpoorly for retrieval tasks unless combined withlexical features (Atreya and Elkan, 2011a).Term-cooccurence based embeddings, such asword2vec (Mikolov et al, 2013b; Mikolov etal., 2013a) and (Pennington et al, 2014b),have recently been remarkably popular formany natural language processing and logi-cal reasoning tasks.
However, there are rel-atively less known successful applications ofthese models in IR.
Ganguly et.
al.
(Gan-guly et al, 2015) used the word similarity inthe word2vec embedding space as a way to es-timate term transformation probabilities in alanguage modelling setting for retrieval.
Morerecently, Nalisnick et.
al.
(Nalisnick et al,2016) proposed to model document about-nessby computing the similarity between all pairsof query and document terms using dual em-bedding spaces.
Both these approaches es-timate the semantic relatedness between twoterms as the cosine distance between them inthe embedding space(s).
We adopt a similarnotion of term relatedness but focus on demon-374strating improved retrieval performance usinglocally trained embeddings.Local latent semantic analysis Despitethe mathematical appeal of latent seman-tic analysis, several experiments suggest thatits empirical performance may be no betterthan that of ranking using standard term vec-tors (Deerwester et al, 1990; Dumais, 1995;Atreya and Elkan, 2011b).
In order to addressthe coarseness of corpus-level latent seman-tic analysis, Hull proposed restricting analysisto the documents relevant to a query (Hull,1994).
This approach significantly improvedover corpus-level analysis for routing tasks, aresult that has been reproduced in consequentresearch (Schu?tze et al, 1995; Singhal et al,1997).
Our work can be seen as an extensionof these results to more recent techniques suchas word2vec.9 ConclusionWe have demonstrated a simple and effectivemethod for performing query expansion withword embeddings.
Importantly, our resultshighlight the value of locally-training wordembeddings in a query-specific manner.
Thestrength of these results suggests that otherresearch adopting global embedding vectorsshould consider local embeddings as a poten-tially superior representation.
Instead of usinga ?Sriracha sauce of deep learning,?
as em-bedding techniques like word2vec have beencalled, we contend that the situation some-times requires, say, that we make a be?chamelor a mole verde or a sambal?or otherwiselearn to cook.ReferencesNasreen Abdul-Jaleel, James Allan, W. BruceCroft, Fernando Diaz, Leah Larkey, XiaoyanLi, Donald Metzler, Mark D. Smucker, TrevorStrohman, Howard Turtle, and Courtney Wade.2004.
Umass at trec 2004: Novelty and hard.
InOnline Proceedings of 2004 Text REtrieval Con-ference.Mohannad al Masri, Catherine Berrut, and Jean-Pierre Chevallet.
2016.
A comparison ofdeep learning based query expansion withpseudo-relevance feedback and mutual informa-tion.
In Nicola Ferro, Fabio Crestani, Marie-Francine Moens, Josiane Mothe, Fabrizio Sil-vestri, Maria Giorgio Di Nunzio, Claudia Hauff,and Gianmaria Silvello, editors, Proceedings ofthe 38th European Conference on IR Research(ECIR 2016), pages 709?715, Cham.
SpringerInternational Publishing.Avinash Atreya and Charles Elkan.
2011a.
La-tent semantic indexing (lsi) fails for trec collec-tions.
ACM SIGKDD Explorations Newsletter,12(2):5?10.Avinash Atreya and Charles Elkan.
2011b.
Latentsemantic indexing (lsi) fails for trec collections.SIGKDD Explor.
Newsl., 12(2):5?10, March.R.
Attar and A. S. Fraenkel.
1977.
Local feed-back in full-text retrieval systems.
J. ACM,24(3):397?417, July.Jerome R Bellegarda.
2004.
Statistical lan-guage model adaptation: review and perspec-tives.
Speech communication, 42(1):93?108.David M. Blei, Andrew Y. Ng, and Michael I. Jor-dan.
2003.
Latent dirichlet alocation.
J. Mach.Learn.
Res., 3:993?1022.W.
Bruce Croft and John Lafferty.
2003.
LanguageModeling for Information Retrieval.
KluwerAcademic Publishing.Steve Cronen-Townsend, Yun Zhou, and W. BruceCroft.
2002.
Predicting query performance.
InSIGIR ?02: Proceedings of the 25th annual in-ternational ACM SIGIR conference on Researchand development in information retrieval, pages299?306, New York, NY, USA.
ACM Press.Scott C. Deerwester, Susan T. Dumais, Thomas K.Landauer, George W. Furnas, and Richard A.Harshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society of In-formation Science, 41(6):391?407.Fernando Diaz and Donald Metzler.
2006.
Im-proving the estimation of relevance models usinglarge external corpora.
In SIGIR ?06: Proceed-ings of the 29th annual international ACM SI-GIR conference on Research and development ininformation retrieval, pages 154?161, New York,NY, USA.
ACM Press.Fernando Diaz.
2015.
Condensed list relevancemodels.
In Proceedings of the 2015 InternationalConference on The Theory of Information Re-trieval, ICTIR ?15, pages 313?316, New York,NY, USA, May.
ACM.Susan T. Dumais.
1995.
Latent semantic in-dexing (LSI): TREC-3 report.
In Overview ofthe Third Text REtrieval Conference (TREC-3),pages 219?230.Marcello Federico.
1996.
Bayesian estimationmethods for n-gram language model adaptation.In Spoken Language, 1996.
ICSLP 96.
Proceed-ings., Fourth International Conference on, vol-ume 1, pages 240?243.
IEEE.375William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
One sense per discourse.In Proceedings of the Workshop on Speech andNatural Language, HLT ?91, pages 233?237,Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Debasis Ganguly, Dwaipayan Roy, Mandar Mitra,and Gareth J.F.
Jones.
2015.
Word embeddingbased generalized language model for informa-tion retrieval.
In Proceedings of the 38th Inter-national ACM SIGIR Conference on Researchand Development in Information Retrieval, SI-GIR ?15, pages 795?798, New York, NY, USA.ACM.Zellig S. Harris.
1954.
Distributional structure.WORD, 10(2-3):146?162.Thomas Hofmann.
1999.
Probabilistic latent se-mantic indexing.
In SIGIR ?99: Proceedings ofthe 22nd annual international ACM SIGIR con-ference on Research and development in infor-mation retrieval, pages 50?57, New York, NY,USA.
ACM Press.Eric H Huang, Richard Socher, Christopher DManning, and Andrew Y Ng.
2012.
Improvingword representations via global context and mul-tiple word prototypes.
In Proceedings of the 50thAnnual Meeting of the Association for Com-putational Linguistics: Long Papers-Volume 1,pages 873?882.
Association for ComputationalLinguistics.David Hull.
1994.
Improving text retrieval for therouting problem using latent semantic indexing.In Proceedings of the 17th Annual InternationalACM SIGIR Conference on Research and De-velopment in Information Retrieval, SIGIR ?94,pages 282?291, New York, NY, USA.
Springer-Verlag New York, Inc.R.M.
Iyer and M. Ostendorf.
1999.
Modeling longdistance dependence in language: topic mixturesversus dynamic cache models.
Speech and AudioProcessing, IEEE Transactions on, 7(1):30?39,Jan.Kalervo Ja?rvelin and Jaana Keka?la?inen.
2002.
Cu-mulated gain-based evaluation of ir techniques.TOIS, 20(4):422?446.Robert Krovetz.
1993.
Viewing morphology as aninference process.
In SIGIR ?93: Proceedings ofthe 16th annual international ACM SIGIR con-ference on Research and development in infor-mation retrieval, pages 191?202, New York, NY,USA.
ACM Press.Roland Kuhn and Renato De Mori.
1990.
A cache-based natural language model for speech recog-nition.
Pattern Analysis and Machine Intelli-gence, IEEE Transactions on, 12(6):570?583.Victor Lavrenko and W. Bruce Croft.
2001.
Rele-vance based language models.
In Proceedings ofthe 24th annual international ACM SIGIR con-ference on Research and development in infor-mation retrieval, pages 120?127.
ACM Press.Omer Levy and Yoav Goldberg.
2014.
Neuralword embedding as implicit matrix factoriza-tion.
In Z. Ghahramani, M. Welling, C. Cortes,N.D.
Lawrence, and K.Q.
Weinberger, editors,Advances in Neural Information Processing Sys-tems 27, pages 2177?2185.
Curran Associates,Inc.Yang Liu, Zhiyuan Liu, Tat-Seng Chua, andMaosong Sun.
2015.
Topical word embed-dings.
In Proceedings of the Twenty-NinthAAAI Conference on Artificial Intelligence,AAAI?15, pages 2418?2424.
AAAI Press.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013a.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg SCorrado, and Jeff Dean.
2013b.
Distributedrepresentations of words and phrases and theircompositionality.
In C.J.C.
Burges, L. Bottou,M.
Welling, Z. Ghahramani, and K.Q.
Wein-berger, editors, Advances in Neural InformationProcessing Systems 26, pages 3111?3119.
CurranAssociates, Inc.Eric Nalisnick, Bhaskar Mitra, Nick Craswell, andRich Caruana.
2016.
Improving documentranking with dual word embeddings.
In Proc.WWW.
International World Wide Web Confer-ences Steering Committee.Hiroaki Nanjo and Tatsuya Kawahara.
2004.Language model and speaking rate adaptationfor spontaneous presentation speech recognition.Speech and Audio Processing, IEEE Transac-tions on, 12(4):391?400.Arvind Neelakantan, Jeevan Shankar, AlexandrePassos, and Andrew McCallum.
2015.
Efficientnon-parametric estimation of multiple embed-dings per word in vector space.
arXiv preprintarXiv:1504.06654.Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning.
2014a.
Glove: Global vec-tors for word representation.
In Empirical Meth-ods in Natural Language Processing (EMNLP),pages 1532?1543.Jeffrey Pennington, Richard Socher, and Christo-pher D Manning.
2014b.
Glove: Global vec-tors for word representation.
Proc.
EMNLP,12:1532?1543.Joseph Reisinger and Raymond Mooney.
2010a.A mixture model with sharing for lexical se-mantics.
In Proceedings of the 2010 Conference376on Empirical Methods in Natural Language Pro-cessing, pages 1173?1182.
Association for Com-putational Linguistics.Joseph Reisinger and Raymond J Mooney.
2010b.Multi-prototype vector-space models of wordmeaning.
In Human Language Technologies:The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Com-putational Linguistics, pages 109?117.
Associa-tion for Computational Linguistics.Hinrich Schu?tze, David A.
Hull, and Jan O. Peder-sen. 1995.
A comparison of classifiers and doc-ument representations for the routing problem.In Proceedings of the 18th Annual InternationalACM SIGIR Conference on Research and De-velopment in Information Retrieval, SIGIR ?95,pages 229?237, New York, NY, USA.
ACM.Hidetoshi Shimodaira.
2000.
Improving predic-tive inference under covariate shift by weightingthe log-likelihood function.
Journal of Statisti-cal Planning and Inference, 90(2):227 ?
244.Amit Singhal, Mandar Mitra, and Chris Buckley.1997.
Learning routing queries in a query zone.SIGIR Forum, 31(SI):25?32, July.Alessandro Sordoni, Yoshua Bengio, and Jian-YunNie.
2014.
Learning concept embeddings forquery expansion by quantum entropy minimiza-tion.
In Proceedings of the Twenty-Eighth AAAIConference on Artificial Intelligence, AAAI?14,pages 1586?1592.
AAAI Press.Anastasios Tombros and C. J. van Rijsbergen.2001.
Query-sensitive similarity measures forthe calculation of interdocument relationships.In CIKM ?01: Proceedings of the tenth interna-tional conference on Information and knowledgemanagement, pages 17?24, New York, NY, USA.ACM Press.Anastasios Tombros, Robert Villa, and C. J.Van Rijsbergen.
2002.
The effectiveness ofquery-specific hierarchic clustering in informa-tion retrieval.
Inf.
Process.
Manage., 38(4):559?582, July.Andrew Trask, Phil Michalak, and John Liu.
2015.sense2vec-a fast and accurate method for wordsense disambiguation in neural word embed-dings.
arXiv preprint arXiv:1511.06388.Laurens van der Maaten and Geoffrey E. Hinton.2008.
Visualizing high-dimensional data usingt-sne.
Journal of Machine Learning Research,9:2579?2605.C.
J. van Rijsbergen.
1979.
Information Retrieval.Butterworths.Xing Wei and W. Bruce Croft.
2006.
LDA-baseddocument models for ad-hoc retrieval.
In SI-GIR ?06: Proceedings of the 29th annual inter-national ACM SIGIR conference on Researchand development in information retrieval, pages178?185, New York, NY, USA.
ACM Press.Peter Willett.
1985.
Query-specific automatic doc-ument classification.
In International Forumon Information and Documentation, volume 10,pages 28?32.Jinxi Xu and W. Bruce Croft.
1996.
Query expan-sion using local and global document analysis.In Proceedings of the 19th Annual InternationalACM SIGIR Conference on Research and De-velopment in Information Retrieval, SIGIR ?96,pages 4?11, New York, NY, USA.
ACM.David Yarowsky.
1993.
One sense per colloca-tion.
In Proceedings of the Workshop on HumanLanguage Technology, HLT ?93, pages 266?271,Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.David Yarowsky.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.
InProceedings of the 33rd Annual Meeting on As-sociation for Computational Linguistics, ACL?95, pages 189?196, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Bing Zhao, Matthias Eck, and Stephan Vogel.2004.
Language model adaptation for statisti-cal machine translation with structured querymodels.
In Proceedings of the 20th InternationalConference on Computational Linguistics, COL-ING ?04, Stroudsburg, PA, USA.
Association forComputational Linguistics.377
