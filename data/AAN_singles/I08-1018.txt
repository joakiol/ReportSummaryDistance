Generic Text Summarization UsingProbabilistic Latent Semantic IndexingHarendra BhandariGraduate School of Information ScienceNara Institute of Science and TechnologyNara 630-0192, Japanharendra-b@is.naist.jpTakahiko ItoGraduate School of Information ScienceNara Institute of Science and TechnologyNara 630-0192, Japantakahahi-i@is.naist.jpMasashi ShimboGraduate School of Information ScienceNara Institute of Science and TechnologyNara  630-0192, Japanshimbo@is.naist.jpYuji MatsumotoGraduate School of Information ScienceNara Institute of Science and TechnologyNara 630-0192, Japanmatsu@is.naist.jpAbstractThis paper presents a strategy to generate ge-neric summary of documents using ProbabilisticLatent Semantic Indexing.
Generally a docu-ment contains several topics rather than a singleone.
Summaries created by human beings tendto cover several topics to give the readers anoverall idea about the original document.
Hencewe can expect that a summary containing sen-tences from better part of the topic spectrumshould make a better summary.
PLSI hasproven to be an effective method in topic detec-tion.
In this paper we present a method for cre-ating extractive summary of the document byusing PLSI to analyze the features of documentsuch as term frequency and graph structure.
Wealso show our results, which was evaluated us-ing ROUGE, and compare the results with othertechniques, proposed in the past.1 IntroductionThe advent of the Internet has made a wealth oftextual data available to everyone.
Finding a spe-cific piece of information in this mass of data canbe compared with "finding a small needle in a largeheap of straw."
Search engines do a remarkable jobin providing a subset of the original data set whichis generally a lot smaller than the original pile ofdata.
However the subset provided by the searchengines is still substantial in size.
Users need tomanually scan through all the information con-tained in the list of results provided by the searchengines until the desired information is found.
Thismakes automatic summarization the task of greatimportance as the users can then just read thesummaries and obtain an overview of the document,hence saving a lot of time during the process.Several methods have been proposed in the fieldof automatic text summarization.
In general twoapproaches have been taken, extract-based summa-rization and abstract-based summarization.
Whileextract-based summarization focuses in findingrelevant sentences from the original document andusing the exact sentences as a summary, abstract-based summaries may contain the words or phrasesnot present in the original document (Mani, 1999).The summarization task can also be classified asquery-oriented or generic.
The query-orientedsummary presents text that contains informationrelevant to the given query, and the generic sum-marization method presents the summary that givesoverall sense of the document (Goldstein et al1998).
In this paper, we will focus on extract-basedgeneric single-document summarization.In the recent years graph based techinques havebecome very popular in automatic text summariza-133tion (Erkan and Radev, 2004), (Mihalcea, 2005).These techniques view each sentence as a node of agraph and the similarities between each sentencesas the links between those sentences.
Generally thelinks are retained only if the similarity values be-tween the sentences exceed a pre-determinedthreshold value; the links are discarded otherwise.The sentences are then ranked using some graphranking algorithms such as HITS (Kleinberg, 1998)or PageRank (Brin and Page, 1998) etc.
Howeverthe graph ranking algorithms tend to give the high-est ranking to the sentences related to one centraltopic in the document.
So if a document containsseveral topics, these algorithms will only chooseone central topic and rank the sentences related tothose topic higher than any other topics, ignoringthe importance of other topics present.
This willcreate summaries that may not cover the overalltopics of the document and hence cannot be con-sidered generic enough.
We will focus on thatproblem and present a way to create better genericsummary of the document using PLSI (Hofmann1999) which covers several topics in the documentand is closer to the summaries created by humanbeings.
The benchmarking done using DUC2 2002data set showed that our technique improves overother proposed methods in terms of ROUGE1evaluation score.2 Related Work2.1 Maximal Marginal Relevance(MMR)MMR is a summarization procedure based on vec-tor-space model and is suited to generic summari-zation (Goldstein et al 1999).
In MMR the sen-tence are chosen according to the weighed combi-nation of their general relevance in the documentand their redundancy with the sentences alreadychosen.
Both the relevance and redundancy aremeasured using cosine similarity.
Relevance is thecosine similarity of a sentence with rest of the sen-tence in the document whereas  redundancy ismeasured using cosine similarity between the sen-tence and the sentences already chosen for thesummary.2.2 Graph Based SummarizationThe graph-based summarization procedure are be_________________________________________1 ROUGE:http://openrouge.com/default.aspx2 http://duc.nist.govcoming increasingly popular in recent years.
LexPageRank (Erkan and Radev, 2004) is one of suchmethods.
LexPageRank constructs a graph whereeach sentence is a node and links are the similari-ties between the sentences.
Similarity is measuredusing cosine similarity of the word vectors, and ifthe similarity value is more than certain thresholdvalue the link is kept otherwise the links are re-moved.
PageRank is an algorithm which has beensuccessfully applied by Google search engine torank the search results.
Similarly PageRank is ap-plied in LexPageRank to rank the nodes (or, sen-tences) of the resultant graph.
A similar summari-zation method has been proposed by Mihalcea(2005).Algorithms like HITS and PageRank calculatethe principal eigenvector (hence find the principalcommunity) of the matrix representing the graph.But as illustrated in Figure 1, another eigenvectorwhich is slightly smaller than the principal eigen-vector may exist.
In documents, each communityrepresented by the eigenvectors can be consideredas a topic present in the document.
As these algo-rithms tend to ignore the influence of eigenvectorsother than largest one, the sentences related to top-ics other than a central one can be ignored, andcreating the possibility for the inclusion of redun-dant sentences as well.
This kind of summary can-not be considered as a generic one.Figure 1.
In algorithms like HITS and PageRankonly the principal eigenvectors are considered.
Inthe figure the vector EV1 is slightly larger thanvector EV2, but the score commanded by membersof EV2 communities are ignored.As we mentioned in section 1, we take into consid-eration the sentences from all the topics generatedby PLSI in the summary, hence getting a more ge-neric summary.2.3 Latent Semantic AnalysisLatent Semantic Analysis (LSA) (Deerwester et al,EV1EV21341990) takes the high dimensional vector space rep-resentation of the document based on term fre-quency and projects it to lesser dimension space.
Itis thought that the similarities between the docu-ments can be more reliably estimated in the re-duced latent space representation than original rep-resentation.
LSA has been applied in areas of textretrieval (Deerwester et al, 1990) and automatictext summarization (Gong and Liu, 2001).
LSA isbased on Singular Value Decomposition (SVD) ofm?n term-document matrix A.
Each entry in A, Aij,represents the frequency of term i in document j.Using SVD, the matrix A is decomposed intoU,S,V as,A=USVTU=Matrix of n left singular vectorsS=diag(?i)=Diagonal matrix of singular valueswhere with ?i?
?i+1 for all i.VT=Matrix of right singular vectors.
Eachrow represents a topic and the values in eachrow represent the score of  documents,represented by each columns, for the topicrepresented by the row.Gong and Liu (2001) have proposed a scheme forautomatic text summarization using LSA.
Theiralgorithm can be stated below.a.
Choose the highest ranked sentence fromkth right singular vector in matrix VT anduse the sentence in summary.b.
If k reaches the predefined number, termi-nate the process; otherwise, go to step aagain.LSA categorizes sentences on the basis of the top-ics they belong to.
Gong and Liu?s method pickssentences from various topics hence producing thesummaries that are generic in nature.In section 3 we explain how PLSI is more ad-vanced form of LSA.
In section 5, we compare oursummarization results with that of LSA.3 Probabilistic Latent Semantic IndexingProbabilistic Latent Semantic Indexing (PLSI)(Hofmann, 1999) is a new approach to automateddocument indexing, and is based on a statisticallatent class model for factor analysis of count data.PLSI is considered to be a probabilistic analogue ofLatent Semantic Indexing (LSI), which is a docu-ment indexing technique based on LSA.
Despitethe success of LSI, it is not devoid of deficits.
Themain argument against LSI is pointed to its unsatis-factory statistical foundations.
In contrast, PLSI hassolid statistical foundations, as it is based on themaximum likelihood principle and defines a propergenerative model of data.
Hofmann (1999) hasshown that PLSI indeed performs betterthan LSI in several text retrieval experiments.
Thefactor representation obtained in PLSI allows us toclassify sentences according to the topics they be-long to.
We will use this ability of PLSI to generatesummary of document that are more generic in na-ture by picking sentences from different topics.4 Summarization with PLSI4.1 The Latent Variable Model for DocumentOur document model is similar to Aspect Model(Hofmann et al 1999, Saul and Pereira, 1997) usedby Hoffman (1999).
The model attempts to associ-ate an unobserved class variable z?Z={z1, ..., zk}(in our case the topics contained in the document),with two sets of observables, documents (d?D={d1,?..dm}, sentences in our case) and words (w?W={w1,?,wn}) contained in documents.
In termsof generative model it can be defined as follows:-A document d is selected with probability P(d)-A latent class z is selected with probabilityP(z|d)-A word w is selected with probability P(w|z)For each document-word pair (d,w), the likelihoodfor each pair can be represented asP(d,w)=P(d)P(w|d)=P(d) ?zP(w|z)P(z|d).Following the maximum likelihood principle P(d),P(z|d), P(w|z) are determined by the maximizationof of log-likelihood function,L= ?d?wn(d,w)logP(d,w)where n(d,w) denotes the term frequency, i.e., thenumber of time w occurred in d.4.2 Maximizing Model LikelihoodExpectation Maximization (EM) is the standardprocedure for maximizing  likelihood estimation inthe presence of latent variables.
EM is an iterativeprocedure and each of the iteration contains twosteps.
(a) An Expectation (E) step, where the poste-rior probabilities for latent variable z are computedand (b) Maximization (M) step, where parametersfor given posterior probabilities are computed.The aspect model can be re-parameterized usingthe Bayes?
rule as follows:135P(d,w)= ?zP(z) P(d|z) P(w|z) .Then using the re-parameterized equation the E-step calculates the posterior for z by'( ) ( | ) ( | )( ') ( | ') ( | ')( | , )zP z P d z P w zP z P d z P w zP z d w = ?This step calculates the probability that word wpresent in document d can be described by the fac-tor corresponding to z.
Subsequently, the M-stepre-evaluates the parameters using following equa-tions., '( , ) ( | , )( , ') ( | , ')( | ) ,dd wn d w P z d wn d w P z d wP w z =??
(1)',( , ) ( | , )( ', ) ( | ', )( | ) ,wd wn d w P z d wn d w P z d wP d z =??
(2),,( , ) ( | , )( , )( ) d wd wn d w P z d wn d wP z =??
(3)Alternating the E- and M- steps one approaches aconverging point which describes local maximumof the log-likelihood.We used the tempered EM (TEM) as describedby Hofmann (1999).
TEM basically introduces acontrol parameter B, upon which the E-step ismodified as,'( )[ ( | ) ( | )]( ')[ ( | ') ( | ')]( | , )BBzP z P d z P w zP z P d z P w zP z d w = ?
(4)The TEM reduces to original EM if B=1.4.3 Summarization procedureWe applied PLSI in 4 different ways during thesummarization process.
We will denote each of the4 ways as PROC1, PROC2, PROC3, PROC4.Each of the four summarization procedure is dis-cussed below.PROC1 (Dominant topic only): PROC1 consists ofthe following steps:a.
Each document is represented as term-frequency matrix.b.
P(w|z), P(d|z), and P(z) (as in (1), (2), (3)) arecalculated until the convergence criteria forEM-algorithm is met.
P(d|z) represents the im-portance of document d in given topic repre-sented by z and P(z) represents the importanceof the topic z itself in the document d.c. z with highest probability P(z) is picked as thecentral topic of the document and then the sen-tences with highest P(d|z) score contained inselected topic are picked.d.
The top scoring sentences are used in thesummary.PROC2 (Dominant topic only): PROC2 is the graphbased method.
PROC2 is similar to PROC1 exceptfor the fact that instead of using term-frequencymatrix we use sentence-similarity matrix.
Sen-tence-similarity matrix A is n?n matrix where n isthe number of sentences present in the document.Cosine similarity of each sentence present in thedocument with respect to all the sentences is calcu-lated.
The cosine-similarity values calculated areused instead of term-frequency values as in PROC1.Each entry Aij in matrix A is 0 if the cosine similar-ity value between sentence i and sentence j is lessthan threshold value and 1 if greater.
We used 0.2as the threshold value in our experiments afternormalizing cosine similarity value.
Steps b, c, dfrom PROC1 are followed after the initial proce-dure is complete.This method is analogous to PHITS (Cohn andChang (2001)) method where the authors utilizedPLSI to find communities in hyperlinked environ-ment.PROC3 (Multiple topics): In both PROC1 andPROC2 we did not take the advantage of the factthat PLSI divides a document into several topics.We only used the sentences from highest rankedtopic.
In PROC3 we attempt to combine the sen-tences from different topics while forming thesummary.
PROC3 can be explained in the follow-ing steps.a.
Steps a and b from PROC1 are taken as normal.b.
We mentioned that P(d|z) represents the scoreof the sentence d in topic z.
In this procedurewe will create new score R for each sentenceusing following relation.R=?zP(d|z)P(z)=P(d)136This will essentially score the sentences with ge-neric values or the sentences which have good in-fluence ranging over several topics better.c.
We pick the sentences that score highest scoreR as the summary.PROC3 will pick sentences from several topicsresulting in better generic summary of the docu-ment.PROC4 (Multiple Topics): PROC4 is essentiallyPROC3 except for the first few steps.
PROC4 doesnot use the matrix created in PROC1 instead it usesthe similarity-matrix produced in PROC2.
Once thesimilarity matrix is created P(z) and P(d|z) are cal-culated as in step b of PROC1.
Then steps b and cof PROC3 are taken to produce the summary of thedocument.5 Experiments and ResultsWe produced summaries for all the proceduresmentioned in section 4.3.
We used DUC 2002 dataset for summarization.
DUC 2002 contains test datafor both multiple document and single documentsummarization.
It also contains summaries createdby human beings for both single document andmultiple document summarization.
Our focus inthis paper is single document summarization.After creating summaries we evaluated summa-ries using ROUGE.
ROUGE has been the standardbenchmarking technique for summarization tasksadopted by Document Understanding Conference(DUC).
We also compared our results with othersummarization methods such as LexPageRank (Er-kan and Radev, 2004) and Gong and Liu?s (2001)LSA-based method.
We also compared the resultswith HITS based method which is similar to Lex-PageRank but instead of PageRank, HITS is usedas ranking algorithm (Klienberg 1998).The resultsare listed in Table 1.We used five measures for evaluation, Rouge-LRouge1, Rouge2, Rouge-SU4 and F1.
These meth-ods are standard methods used in DUC evaluationTable 1: Evaluation of summariesThe table shows the score of summaries generated using methods described in section 4.3.
On the tablen means number of topics into which the document has been divided into.
Control parameter B from (4)was fixed to 0.75 in this case.Method Used nROUGE-L(recall) Rouge1 Rouge-2 Rouge-SU4PROC1 2 0.499 0.557 0.242 0.272PROC2 2 0.465 0.515 0.227 0.2532 0.571 0.634 0.291 0.3213 0.571 0.628 0.288 0.3184 0.571 0.62 0.28 0.315 0.571 0.613 0.274 0.305PROC3 6 0.5 0.612 0.27 0.3022 0.473 0.508 0.225 0.253 0.472 0.504 0.22 0.2454 0.472 0.5 0.219 0.2445 0.472 0.492 0.213 0.238PROC4 6 0.471 0.483 0.207 0.231Compared Methods*LexPageRank   0.522 0.577 0.265 0.291*LSA   0.414 0.463 0.186 0.215*HITS   0.504            0.562                0.251                    0.282137tests and these schemes are known to be very effecttive to calculate the correlation between the sum-maries.
All of the scores can be calculated usingRouge package.
Rouge is based on N-gram statis-tics (Lin and Hovy, 2003).
Rouge has been knownto highly correlate with human evaluations.
Ac-cording to (Lin and Hovy, 2003), among the meth-ods implemented in ROUGE, ROUGE-N (N=1,2),ROUGE-L, ROUGE-S are relatively simple andwork very well even when the length of summaryis quite short, which is mostly the case in singledocument summarization.
ROUGE-N, ROUGE-Land ROUGE-S are all basically the recall scores.As DUC keeps the length of the summaries con-stant recall is the main evaluation criterion.
F-measure is also shown in the table as a referenceparameter, but since we kept the length of oursummaries constant, too, the ROUGE-L, ROUGE-N and ROUGE-S scores carry the highest weight.As seen on Table 1, the scores gained by PROC1and PROC2 are less than others.
This is mainlybecause the sentences chosen by these methodswere simply chosen from one topic.
As PROC3and PROC4 use sentences from several topics thescore of PROC3 and PROC4 were better thanPROC1 and PROC2.
For methods PROC3 andPROC4 we took the summaries for topics 2through 6 and found that the method performedwell when the number of topics was kept between2 to 4.
But the difference was very small, and ingeneral the performance was quite stable.We also compared our results to other methodssuch as LexPageRank and LSA and found thatPROC3 performed quite well when compared tothose methods.
LexPageRank was marginally bet-ter in F-measure (F1) but PROC3 got best recallscores.
PROC3 also outperformed LSA by 0.16 inrecall (ROUGE-L) scores.
Comparison to HITSalso shows PROC3 more advantageous.6 DiscussionIn this paper we have argued that choosing sen-tences from multiple topics makes a better genericsummary.
It is especially true if we compare ourmethod to graph based ranking methods like HITSand PageRank.
Richardson and Domingos (2002)have mentioned that both HITS and PageRank suf-fer from the topic drift.
This not only makes thesealgorithms susceptible for exclusion of importantsentences outside the main topic but miss the sen-tences from main topic as well.
Cohn and Chang(2001) also have shown similar results for HITS.They (Cohn and Chang) have shown that the cen-tral topic identified by HITS (principal eigenvec-tor) may not always correspond to the most au-thoritative topic.
The main topic in fact may berepresented by smaller eigenvectors rather than theprincipal one.
They also show that the topic segre-Effect of parameter B on scores0.5670.56750.5680.56850.5690.56950.570.57050.5710.57150.5720.57250.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Temperature(B)Rouge-LScoren=2n=3n=4Figure 2: Effect of tempering factor B in the ROUGE-L score for PROC3.138gation in HITS is quite extreme so if we just useprincipal eigenvector, first there is a chance of be-ing drifted away from the main topic hence produc-ing low quality summary and there is also a chanceof missing out other important topics due to theextreme segregation of communities.
In PLSI thesegregation of topics is not as extreme.
If a sen-tence is related to several topics the sentence canattain high rank in many topics.We can see from the scores that the performanceof graph based algorithms like LexPageRank andHITS are not as good as our method.
This can beattributed to the fact that the graph based summar-izers take only a central topic under consideration.The method that proved most successful in oursummarization was the one where we extracted thesentences that had the most influence in the docu-ment.We used the tempered version of EM-algorithm(4) in our summarization task.
We evaluated theeffect of tempering factor B in performance ofsummarization for PROC3.
We found that that thetempering factor did not influence the results by abig margin.
We conducted our experiment usingvalues of B from 0.1 through 1.0 incrementing eachstep by 0.1.
The results are shown in Figure 2.
Inthe results shown in Table 1 the value for temper-ing factor was set to 0.75.7 Conclusion and Future WorkIn this paper we presented a method for creatinggeneric summaries of the documents using PLSI.PLSI allowed us classify the sentences present inthe document into several topics.
Our summaryincluded sentences from all the topics, which madethe generation of generic summary possible.
Ourexperiments showed that the results we obtained insummarization tasks were better than some othermethods we compared with.
LSA can also be usedto summarize documents in similar manner by ex-tracting sentences from several topics, but our ex-periments showed that PLSI performs better thanLSA.
In the future we plan to investigate how morerecent methods such as LDA (Blei et al perform indocument summarization tasks.
We also plan toapply our methods to multiple document summari-zation.8 AcknowledgementWe pay our special gratitude to the reviewers whohave taken their time to give very useful commentson our work.
The comments were very useful forus to as we were able to provide wider perspectiveon our work with the help of those comments.References:Blei D, Ng A, and Jordan M.2003.
Journal of Ma-chine Learning Research 3 993-1022.Brin S and Page L.1998.
The Anatomy of a Large-Scale Hypertextual Web Search Engine.
ComputerNetworks 30(1-7): 107-117.Carbonell J and Goldstein J.1998.
The Use ofMMR, Diversity-Based Reranking for ReorderingDocuments and Producing Summaries.
Proc.
ACMSIGIRCohn D, Chang H.2001.
Learning to probabilisti-vally identify authoritative documents.
Proceedingsof 18th International Conference of Machine Learn-ing.Deerwester S, Dumais ST, Furnas GT, LandauerTK, and Harshman R.1990.
Indexing by LatentSemantic Analysis.
Journal of the American Soci-ety of Information Science.Erkan G and Radev DR.2004.
LexPageRank: Pres-tige in Multi-Document Text Summariza-tion.EMNLP.Gong Y and Liu X.2001.Generic text Summariza-tion using relevance measure and latent semanticanalysis.Proc ACM SIGIR.Hofmann, T.1999.Probabilistic Latent SemanticIndexing.
Twenty Second International ACM-SIGIR Conference on Informatioln Retrieval.Hofmann, et al1998.
Unsupervised Learning fromDyadic Data.
Technical Report TR-98-042, Inter-national Computer Science Insitute, Berkeley, CA.Kleinberg J.1998.
Authoritative sources in a hyper-linked environment.
Proc.
9th ACM-SIAM Sym-posium on Discrete Algorithms.139Mani I.
1999.
Advances in Automatic Text Sum-marization.
MIT Press, Cambridge, MA, USA.Mihalcea R.2005.Language Independent ExtractiveSummarization.
AAAIRichardson M, Domingos P.2002.
The IntelligentSurfer: Probabilistic Combination of Link and Con-tent Information in PageRank.
Advances in NeuralInformation Processing Systems 14Saul L and Pereria F.1997.Aggregate and mixed-order Markov models for statistical language proc-essing.Proc 21st ACM-SIGIR International Confer-ence on Research and Development in InformationRetrieval.140
