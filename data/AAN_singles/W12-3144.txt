Proceedings of the 7th Workshop on Statistical Machine Translation, pages 349?355,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsThe Karlsruhe Institute of Technology Translation Systemsfor the WMT 2012Jan Niehues, Yuqi Zhang, Mohammed Mediani, Teresa Herrmann, Eunah Cho and Alex WaibelKarlsruhe Institute of TechnologyKarlsruhe, Germanyfirstname.lastname@kit.eduAbstractThis paper describes the phrase-based SMTsystems developed for our participationin the WMT12 Shared Translation Task.Translations for English?German andEnglish?French were generated using aphrase-based translation system which isextended by additional models such asbilingual, fine-grained part-of-speech (POS)and automatic cluster language models anddiscriminative word lexica.
In addition, weexplicitly handle out-of-vocabulary (OOV)words in German, if we have translations forother morphological forms of the same stem.Furthermore, we extended the POS-basedreordering approach to also use informationfrom syntactic trees.1 IntroductionIn this paper, we describe our systems for theNAACL 2012 Seventh Workshop on Statistical Ma-chine Translation.
We participated in the SharedTranslation Task and submitted translations forEnglish?German and English?French.
We use aphrase-based decoder that can use lattices as inputand developed several models that extend the stan-dard log-linear model combination of phrase-basedMT.
In addition to the POS-based reordering modelused in past years, for German-English we extendedit to also use rules learned using syntax trees.The translation model was extended by the bilin-gual language model and a discriminative word lex-icon using a maximum entropy classifier.
For theFrench-English and English-French translation sys-tems, we also used phrase table adaptation to avoidoverestimation of the probabilities of the huge, butnoisy Giga corpus.
In the German-English system,we tried to learn translations for OOV words by ex-ploring different morphological forms of the OOVswith the same lemma.Furthermore, we combined different languagemodels in the log-linear model.
We used word-based language models trained on different parts ofthe training corpus as well as POS-based languagemodels using fine-grained POS information and lan-guage models trained on automatic word clusters.The paper is organized as follows: The next sec-tion gives a detailed description of our systems in-cluding all the models.
The translation results forall directions are presented afterwards and we closewith a conclusion.2 System DescriptionFor the French?English systems the phrase tableis based on a GIZA++ word alignment, while thesystems for German?English use a discriminativeword alignment as described in Niehues and Vogel(2008).
The language models are 4-gram SRI lan-guage models using Kneser-Ney smoothing trainedby the SRILM Toolkit (Stolcke, 2002).The problem of word reordering is addressed withPOS-based and tree-based reordering models as de-scribed in Section 2.3.
The POS tags used in thereordering model are obtained using the TreeTagger(Schmid, 1994).
The syntactic parse trees are gen-erated using the Stanford Parser (Rafferty and Man-ning, 2008).An in-house phrase-based decoder (Vogel, 2003)is used to perform translation.
Optimization with349regard to the BLEU score is done using MinimumError Rate Training as described in Venugopal et al(2005).
During decoding only the top 10 translationoptions for every source phrase are considered.2.1 DataOur translation models were trained on the EPPSand News Commentary (NC) corpora.
Furthermore,the additional available data for French and English(i.e.
UN and Giga corpora) were exploited in thecorresponding systems.The systems were tuned with the news-test2011data, while news-test2011 was used for testing in allour systems.
We trained language models for eachlanguage on the monolingual part of the training cor-pora as well as the News Shuffle and the Gigaword(version 4) corpora.
The discriminative word align-ment model was trained on 500 hand-aligned sen-tences selected from the EPPS corpus.2.2 PreprocessingThe training data is preprocessed prior to trainingthe system.
This includes normalizing special sym-bols, smart-casing the first word of each sentenceand removing long sentences and sentences withlength mismatch.For the German parts of the training corpus, inorder to obtain a homogenous spelling, we use thehunspell1 lexicon to map words written according toold German spelling rules to new German spellingrules.In order to reduce the OOV problem of Germancompound words, Compound splitting as describedin Koehn and Knight (2003) is applied to the Ger-man part of the corpus for the German-to-Englishsystem.The Giga corpus received a special preprocessingby removing noisy pairs using an SVM classifier asdescribed in Mediani et al (2011).
The SVM clas-sifier training and test sets consist of randomly se-lected sentence pairs from the corpora of EPPS, NC,tuning, and test sets.
Giving at the end around 16million sentence pairs.2.3 Word ReorderingIn contrast to modeling the reordering by a distance-based reordering model and/or a lexicalized distor-1http://hunspell.sourceforge.net/tion model, we use a different approach that relies onPOS sequences.
By abstracting from surface wordsto POS, we expect to model the reordering more ac-curately.
For German-to-English, we additionallyapply reordering rules learned from syntactic parsetrees.2.3.1 POS-based Reordering ModelIn order to build the POS-based reordering model,we first learn probabilistic rules from the POS tagsof the training corpus and the alignment.
Contin-uous reordering rules are extracted as described inRottmann and Vogel (2007) to model short-range re-orderings.
When translating between German andEnglish, we apply a modified reordering model withnon-continuous rules to cover also long-range re-orderings (Niehues and Kolss, 2009).2.3.2 Tree-based Reordering ModelWord order is quite different between German andEnglish.
And during translation especially verbs orverb particles need to be shifted over a long dis-tance in a sentence.
Using discontinuous POS rulesalready improves the translation tremendously.
Inaddition, we apply a tree-based reordering modelfor the German-English translation.
Syntactic parsetrees provide information about the words in a sen-tence that form constituents and should therefore betreated as inseparable units by the reordering model.For the tree-based reordering model, syntactic parsetrees are generated for the whole training corpus.Then the word alignment between the source andtarget language part of the corpus is used to learnrules on how to reorder the constituents in a Ger-man source sentence to make it matches the Englishtarget sentence word order better.
In order to applythe rules to the source text, POS tags and a parsetree are generated for each sentence.
Then the POS-based and tree-based reordering rules are applied.The original order of words as well as the reorderedsentence variants generated by the rules are encodedin a word lattice.
The lattice is then used as input tothe decoder.For the test sentences, the reordering based onPOS and trees allows us to change the word orderin the source sentence so that the sentence can betranslated more easily.
In addition, we build reorder-ing lattices for all training sentences and then extract350phrase pairs from the monotone source path as wellas from the reordered paths.2.4 Translation ModelsIn addition to the models used in the baseline systemdescribed above, we conducted experiments includ-ing additional models that enhance translation qual-ity by introducing alternative or additional informa-tion into the translation modeling process.2.4.1 Phrase table adaptationSince the Giga corpus is huge, but noisy, it isadvantageous to also use the translation probabil-ities of the phrase pair extracted only from themore reliable EPPS and News commentary cor-pus.
Therefore, we build two phrase tables for theFrench?English system.
One trained on all dataand the other only trained on the EPPS and Newscommentary corpus.
The two models are then com-bined using a log-linear combination to achieve theadaptation towards the cleaner corpora as describedin (Niehues et al, 2010).
The newly created trans-lation model uses the four scores from the generalmodel as well as the two smoothed relative frequen-cies of both directions from the smaller, but cleanermodel.
If a phrase pair does not occur in the in-domain part, a default score is used instead of a rela-tive frequency.
In our case, we used the lowest prob-ability.2.4.2 Bilingual Language ModelIn phrase-based systems the source sentence issegmented by the decoder according to the best com-bination of phrases that maximize the translationand language model scores.
This segmentation intophrases leads to the loss of context information atthe phrase boundaries.
Although more target sidecontext is available to the language model, sourceside context would also be valuable for the decoderwhen searching for the best translation hypothesis.To make also source language context available weuse a bilingual language model, in which each tokenconsists of a target word and all source words it isaligned to.
The bilingual tokens enter the translationprocess as an additional target factor and the bilin-gual language model is applied to the additional fac-tor like a normal language model.
For more detailssee Niehues et al (2011).2.4.3 Discriminative Word LexicaMauser et al (2009) have shown that the useof discriminative word lexica (DWL) can improvethe translation quality.
For every target word, theytrained a maximum entropy model to determinewhether this target word should be in the translatedsentence or not using one feature per one sourceword.When applying DWL in our experiments, wewould like to have the same conditions for the train-ing and test case.
For this we would need to changethe score of the feature only if a new word is addedto the hypothesis.
If a word is added the second time,we do not want to change the feature value.
In orderto keep track of this, additional bookkeeping wouldbe required.
Also the other models in our translationsystem will prevent us from using a word too often.Therefore, we ignore this problem and can calcu-late the score for every phrase pair before startingwith the translation.
This leads to the following def-inition of the model:p(e|f) =J?j=1p(ej |f) (1)In this definition, p(ej |f) is calculated using a max-imum likelihood classifier.Each classifier is trained independently on theparallel training data.
All sentences pairs where thetarget word e occurs in the target sentence are usedas positive examples.
We could now use all othersentences as negative examples.
But in many ofthese sentences, we would anyway not generate thetarget word, since there is no phrase pair that trans-lates any of the source words into the target word.Therefore, we build a target vocabulary for everytraining sentence.
This vocabulary consists of alltarget side words of phrase pairs matching a sourcephrase in the source part of the training sentence.Then we use all sentence pairs where e is in the tar-get vocabulary but not in the target sentences as neg-ative examples.
This has shown to have a postiveinfluence on the translation quality (Mediani et al,2011) and also reduces training time.2.4.4 Quasi-Morphological Operations forOOV wordsSince German is a highly inflected language, therewill be always some word forms of a given Ger-351Figure 1: Quasi-morphological operationsman lemma that did not occur in the training data.In order to be able to also translate unseen wordforms, we try to learn quasi-morphological opera-tions that change the lexical entry of a known wordform to the unknown word form.
These have shownto be beneficial in Niehues and Waibel (2011) usingWikipedia2 titles.
The idea is illustrated in Figure 1.If we look at the data, our system is able to trans-late a German word Kamin (engl.
chimney), but notthe dative plural form Kaminen.
To address thisproblem, we try to automatically learn rules howwords can be modified.
If we look at the example,we would like the system to learn the following rule.If an ?en?
is appended to a German word, as it isdone when creating the dative plural form of Kami-nen, we need to add an ?s?
to the end of the Englishword in order to perform the same morphologicalword transformation.
We use only rules where theending of the word has at most 3 letters.Depending on the POS, number, gender or case ofthe involved words, the same operation on the sourceside does not necessarily correspond to the same op-eration on the target side.To account for this ambiguity, we rank the differ-ent target operation using the following four featuresand use the best ranked one.
Firstly, we should notgenerate target words that do not exist.
Here, wehave an advantage that we can use monolingual datato determine whether the word exists.
In addition,a target operation that often coincides with a givensource operation should be better than one that israrely used together with the source operation.
Wetherefore look at pairs of entries in the lexicon andcount in how many of them the source operation canbe applied to the source side and the target operationcan be applied to the target side.
We then use onlyoperations that occur at least ten times.
Furthermore,2http://www.wikipedia.org/we use the ending of the source and target word todetermine which pair of operations should be used.Integration We only use the proposed method forOOVs and do not try to improve translations ofwords that the baseline system already covers.
Welook for phrase pairs, for which a source operationops exists that changes one of the source words f1into the OOV word f2.
Since we need to apply atarget operation to one word on the target side of thephrase pair, we only consider phrase pairs where f1is aligned to one of the target words of the phrasecontaining e1.
If a target operation exists given f1and ops, we select the one with the highest rank.Then we generate a new phrase pair by applyingops to f1 and opt to e1 keeping the original scoresfrom the phrase pairs, since the original and syn-thesized phrase pair are not directly competing any-way.
We do not add several phrase pairs generatedby different operations, since we would then need toadd the features used for ranking the operations intothe MERT.
This is problematic, since the operationswere only used for very few words and therefore agood estimation of the weights is not possible.2.5 Language ModelsThe 4-gram language models generated by theSRILM toolkit are used as the main language mod-els for all of our systems.
For English-French andFrench-English systems, we use a good quality cor-pus as in-domain data to train in-domain languagemodels.
Additionally, we apply the POS and clus-ter language models in different systems.
All lan-guage models are integrated into the translation sys-tem by a log-linear combination and received opti-mal weights during tuning by the MERT.2.5.1 POS Language ModelsThe POS language model is trained on the POSsequences of the target language.
In this evalua-tion, the POS language model is applied for theEnglish-German system.
We expect that having ad-ditional information in form of probabilities of POSsequences should help especially in case of the richmorphology of German.
The POS tags are gener-ated with the RFTagger (Schmid and Laws, 2008)for German, which produces fine-grained tags thatinclude person, gender and case information.
We352use a 9-gram language model on the News Shuf-fle corpus and the German side of all parallel cor-pora.
More details and discussions about the POSlanguage model can be found in Herrmann et al(2011).2.5.2 Cluster Language ModelsThe cluster language model follows a similar ideaas the POS language model.
Since there is a datasparsity problem when we substitute words with theword classes, it is possible to make use of largercontext information.
In the POS language model,POS tags are the word classes.
Here, we generatedword classes in a different way.
First, we clusterthe words in the corpus using the MKCLS algorithm(Och, 1999) given a number of classes.
Second, wereplace the words in the corpus by their cluster IDs.Finally, we train an n-gram language model on thiscorpus consisting of cluster IDs.
Generally, all clus-ter language models used in our systems are 5-gram.3 ResultsUsing the models described above we performedseveral experiments leading finally to the systemsused for generating the translations submitted to theworkshop.
The following sections describe the ex-periments for the individual language pairs and showthe translation results.
The results are reported ascase-sensitive BLEU scores (Papineni et al, 2002)on one reference translation.3.1 German-EnglishThe experiments for the German-English translationsystem are summarized in Table 1.
The Baselinesystem uses POS-based reordering, discriminativeword alignment and a language model trained on theNews Shuffle corpus.
By adding lattice phrase ex-traction small improvements of the translation qual-ity could be gained.Further improvements could be gained by addinga language model trained on the Gigaword corpusand adding a bilingual and cluster-based languagemodel.
We used 50 word classes and trained a 5-gram language model.
Afterwards, the translationquality was improved by also using a discriminativeword lexicon.
Finally, the best system was achievedby using Tree-based reordering and using specialtreatment for the OOVs.
This system generates aBLEU score of 22.31 on the test data.
For the lasttwo systems, we did not perform new optimizationruns.System Dev TestBaseline 23.64 21.32+ Lattice Phrase Extraction 23.76 21.36+ Gigaward Language Model 24.01 21.73+ Bilingual LM 24.19 21.91+ Cluster LM 24.16 22.09+ DWL 24.19 22.19+ Tree-based Reordering - 22.26+ OOV - 22.31Table 1: Translation results for German-English3.2 English-GermanThe English-German baseline system uses alsoPOS-based reordering, discriminative word align-ment and a language model based on EPPS, NC andNews Shuffle.
A small gain could be achieved by thePOS-based language model and the bilingual lan-guage model.
Further gain was achieved by usingalso a cluster-based language model.
For this lan-guage model, we use 100 word classes and traineda 5-gram language model.
Finally, the best systemuses the discriminative word lexicon.System Dev TestBaseline 17.06 15.57+ POSLM 17.27 15.63+ Bilingual LM 17.40 15.78+ Cluster LM 17.77 16.06+ DWL 17.75 16.28Table 2: Translation results for English-German3.3 English-FrenchTable 3 summarizes how our English-French sys-tem evolved.
The baseline system here was trainedon the EPPS, NC, and UN corpora, while the lan-guage model was trained on all the French part ofthe parallel corpora (including the Giga corpus).
Italso uses short-range reordering trained on EPPSand NC.
This system had a BLEU score of around26.7.
The Giga parallel data turned out to be quite353beneficial for this task.
It improves the scores bymore than 1 BLEU point.
More importantly, addi-tional language models boosted the system quality:around 1.8 points.
In fact, three language modelswere log-linearly combined: In addition to the afore-mentioned, two additional language models weretrained on the monolingual sets (one for News andone for Gigaword).
We could get an improvementof around 0.2 by retraining the reordering rules onEPPS and NC only, but using Giza alignment fromthe whole data.
Adapting the translation model byusing EPPS and NC as in-domain data improves theBLEU score by only 0.1.
This small improvementmight be due to the fact that the news domain isvery broad and that the Giga corpus has already beencarefully cleaned and filtered.
Furthermore, using abilingual language model enhances the BLEU scoreby almost 0.3.
Finally, incorporating a cluster lan-guage model adds an additional 0.1 to the score.This leads to a system with 30.58.System Dev TestBaseline 24.96 26.67+ GigParData 26.12 28.16+ Big LMs 29.22 29.92+ All Reo 29.14 30.10+ PT Adaptation 29.15 30.22+ Bilingual LM 29.17 30.49+ Cluster LM 29.08 30.58Table 3: Translation results for English-French3.4 French-EnglishThe development of our system for the French-English direction is summarized in Table 4.
Thebaseline system for this direction was trained on theEPPS, NC, UN and Giga parallel corpora, while thelanguage model was trained on the French part of theparallel training corpora.
The baseline system in-cludes the POS-based reordering model with short-range rules.
The largest improvement of 1.7 BLEUscore was achieved by the integration of the biggerlanguage models which are trained on the Englishversion of News Shuffle and the Gigaword corpus(v4).
We did not add the language models from themonolingual English version of EPPS and NC data,since the experiments have shown that they did notprovide improvement in our system.
The secondlargest improvement came from the domain adap-tation that includes an in-domain language modeland adaptations to the phrase extraction.
The BLEUscore has improved about 1 BLEU in total.
The in-domain data we used here are parallel EPPS and NCcorpus.
Further gains were obtained by augmentingthe system with a bilingual language model addingaround 0.2 BLEU to the previous score.
The sub-mitted system was obtained by adding the cluster5-gram language model trained on the News Shuf-fle corpus with 100 clusters and thus giving 30.25 asthe final score.System Dev TestBaseline 25.81 27.15+ Indomain LM 26.17 27.91+ PT Adaptation 26.33 28.11+ Big LMs 28.90 29.82+ Bilingual LM 29.14 30.09+ Cluster LM 29.31 30.25Table 4: Translation results for French-English4 ConclusionsWe have presented the systems for our participationin the WMT 2012 Evaluation for English?Germanand English?French.
In all systems we could im-prove by using a class-based language model.
Fur-thermore, the translation quality could be improvedby using a discriminative word lexicon.
Therefore,we trained a maximum entropy classifier for ev-ery target word.
For English?French, adapting thephrase table helps to avoid using wrong parts of thenoisy Giga corpus.
For the German-to-English sys-tem, we could improve the translation quality addi-tionally by using a tree-based reordering model andby special handling of OOV words.
For the inversedirection we could improve the translation qualityby using a 9-gram language model trained on thefine-grained POS tags.AcknowledgmentsThis work was realized as part of the Quaero Pro-gramme, funded by OSEO, French State agency forinnovation.354ReferencesTeresa Herrmann, Mohammed Mediani, Jan Niehues,and Alex Waibel.
2011.
The karlsruhe institute oftechnology translation systems for the wmt 2011.
InProceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 379?385, Edinburgh, Scot-land, July.
Association for Computational Linguistics.Philipp Koehn and Kevin Knight.
2003.
Empirical Meth-ods for Compound Splitting.
In EACL, Budapest,Hungary.Arne Mauser, Sas?a Hasan, and Hermann Ney.
2009.
Ex-tending Statistical Machine Translation with Discrim-inative and Trigger-based Lexicon Models.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing: Volume 1 - Vol-ume 1, EMNLP ?09, Singapore.Mohammed Mediani, Eunah Cho, Jan Niehues, TeresaHerrmann, and Alex Waibel.
2011.
The kit english-french translation systems for iwslt 2011.
In Proceed-ings of the eight International Workshop on SpokenLanguage Translation (IWSLT).Jan Niehues and Muntsin Kolss.
2009.
A POS-BasedModel for Long-Range Reorderings in SMT.
InFourth Workshop on Statistical Machine Translation(WMT 2009), Athens, Greece.Jan Niehues and Stephan Vogel.
2008.
DiscriminativeWord Alignment via Alignment Matrix Modeling.
InProc.
of Third ACL Workshop on Statistical MachineTranslation, Columbus, USA.Jan Niehues and Alex Waibel.
2011.
Using wikipedia totranslate domain-specific terms in smt.
In Proceedingsof the eight International Workshop on Spoken Lan-guage Translation (IWSLT).Jan Niehues, Mohammed Mediani, Teresa Herrmann,Michael Heck, Christian Herff, and Alex Waibel.2010.
The KIT Translation system for IWSLT 2010.In Marcello Federico, Ian Lane, Michael Paul, andFranc?ois Yvon, editors, Proceedings of the seventh In-ternational Workshop on Spoken Language Transla-tion (IWSLT), pages 93?98.Jan Niehues, Teresa Herrmann, Stephan Vogel, and AlexWaibel.
2011.
Wider Context by Using Bilingual Lan-guage Models in Machine Translation.
In Sixth Work-shop on Statistical Machine Translation (WMT 2011),Edinburgh, UK.Franz Josef Och.
1999.
An efficient method for deter-mining bilingual word classes.
In Proceedings of theninth conference on European chapter of the Associa-tion for Computational Linguistics, EACL ?99, pages71?76, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
Technical ReportRC22176 (W0109-022), IBM Research Division, T. J.Watson Research Center.Anna N. Rafferty and Christopher D. Manning.
2008.Parsing three german treebanks: lexicalized and un-lexicalized baselines.
In Proceedings of the Workshopon Parsing German.Kay Rottmann and Stephan Vogel.
2007.
Word Reorder-ing in Statistical Machine Translation with a POS-Based Distortion Model.
In TMI, Sko?vde, Sweden.Helmut Schmid and Florian Laws.
2008.
Estimation ofConditional Probabilities with Decision Trees and anApplication to Fine-Grained POS Tagging.
In COL-ING 2008, Manchester, Great Britain.Helmut Schmid.
1994.
Probabilistic Part-of-Speech Tag-ging Using Decision Trees.
In International Con-ference on New Methods in Language Processing,Manchester, UK.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
of ICSLP, Denver,Colorado, USA.Ashish Venugopal, Andreas Zollman, and Alex Waibel.2005.
Training and Evaluation Error MinimizationRules for Statistical Machine Translation.
In Work-shop on Data-drive Machine Translation and Beyond(WPT-05), Ann Arbor, MI.Stephan Vogel.
2003.
SMT Decoder Dissected: WordReordering.
In Int.
Conf.
on Natural Language Pro-cessing and Knowledge Engineering, Beijing, China.355
