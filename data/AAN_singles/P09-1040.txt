Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 351?359,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPNon-Projective Dependency Parsing in Expected Linear TimeJoakim NivreUppsala University, Department of Linguistics and Philology, SE-75126 UppsalaVa?xjo?
University, School of Mathematics and Systems Engineering, SE-35195 Va?xjo?E-mail: joakim.nivre@lingfil.uu.seAbstractWe present a novel transition system fordependency parsing, which constructs arcsonly between adjacent words but can parsearbitrary non-projective trees by swappingthe order of words in the input.
Addingthe swapping operation changes the timecomplexity for deterministic parsing fromlinear to quadratic in the worst case, butempirical estimates based on treebank datashow that the expected running time is infact linear for the range of data attested inthe corpora.
Evaluation on data from fivelanguages shows state-of-the-art accuracy,with especially good results for the labeledexact match score.1 IntroductionSyntactic parsing using dependency structures hasbecome a standard technique in natural languageprocessing with many different parsing models, inparticular data-driven models that can be trainedon syntactically annotated corpora (Yamada andMatsumoto, 2003; Nivre et al, 2004; McDonaldet al, 2005a; Attardi, 2006; Titov and Henderson,2007).
A hallmark of many of these models is thatthey can be implemented very efficiently.
Thus,transition-based parsers normally run in linear orquadratic time, using greedy deterministic searchor fixed-width beam search (Nivre et al, 2004; At-tardi, 2006; Johansson and Nugues, 2007; Titovand Henderson, 2007), and graph-based modelssupport exact inference in at most cubic time,which is efficient enough to make global discrim-inative training practically feasible (McDonald etal., 2005a; McDonald et al, 2005b).However, one problem that still has not founda satisfactory solution in data-driven dependencyparsing is the treatment of discontinuous syntacticconstructions, usually modeled by non-projectivedependency trees, as illustrated in Figure 1.
In aprojective dependency tree, the yield of every sub-tree is a contiguous substring of the sentence.
Thisis not the case for the tree in Figure 1, where thesubtrees rooted at node 2 (hearing) and node 4(scheduled) both have discontinuous yields.Allowing non-projective trees generally makesparsing computationally harder.
Exact inferencefor parsing models that allow non-projective treesis NP hard, except under very restricted indepen-dence assumptions (Neuhaus and Bro?ker, 1997;McDonald and Pereira, 2006; McDonald andSatta, 2007).
There is recent work on algorithmsthat can cope with important subsets of all non-projective trees in polynomial time (Kuhlmannand Satta, 2009; Go?mez-Rodr?
?guez et al, 2009),but the time complexity is at best O(n6), whichcan be problematic in practical applications.
Eventhe best algorithms for deterministic parsing run inquadratic time, rather than linear (Nivre, 2008a),unless restricted to a subset of non-projectivestructures as in Attardi (2006) and Nivre (2007).But allowing non-projective dependency treesalso makes parsing empirically harder, becauseit requires that we model relations between non-adjacent structures over potentially unboundeddistances, which often has a negative impact onparsing accuracy.
On the other hand, it is hardlypossible to ignore non-projective structures com-pletely, given that 25% or more of the sentencesin some languages cannot be given a linguisticallyadequate analysis without invoking non-projectivestructures (Nivre, 2006; Kuhlmann and Nivre,2006; Havelka, 2007).Current approaches to data-driven dependencyparsing typically use one of two strategies to dealwith non-projective trees (unless they ignore themcompletely).
Either they employ a non-standardparsing algorithm that can combine non-adjacentsubstructures (McDonald et al, 2005b; Attardi,2006; Nivre, 2007), or they try to recover non-351ROOT0 A1 ?DEThearing2 ?SBJis3 ?ROOTscheduled4 ?VGon5 ?NMODthe6 ?DETissue7 ?PCtoday8 ?ADV.9? PFigure 1: Dependency tree for an English sentence (non-projective).projective dependencies by post-processing theoutput of a strictly projective parser (Nivre andNilsson, 2005; Hall and Nova?k, 2005; McDonaldand Pereira, 2006).
In this paper, we will adopta different strategy, suggested in recent work byNivre (2008b) and Titov et al (2009), and pro-pose an algorithm that only combines adjacentsubstructures but derives non-projective trees byreordering the input words.The rest of the paper is structured as follows.In Section 2, we define the formal representationsneeded and introduce the framework of transition-based dependency parsing.
In Section 3, we firstdefine a minimal transition system and explainhow it can be used to perform projective depen-dency parsing in linear time; we then extend thesystem with a single transition for swapping theorder of words in the input and demonstrate thatthe extended system can be used to parse unre-stricted dependency trees with a time complexitythat is quadratic in the worst case but still linearin the best case.
In Section 4, we present experi-ments indicating that the expected running time ofthe new system on naturally occurring data is infact linear and that the system achieves state-of-the-art parsing accuracy.
We discuss related workin Section 5 and conclude in Section 6.2 Background Notions2.1 Dependency Graphs and TreesGiven a set L of dependency labels, a dependencygraph for a sentence x = w1, .
.
.
, wn is a directedgraph G = (Vx, A), where1.
Vx = {0, 1, .
.
.
, n} is a set of nodes,2.
A ?
Vx ?
L?
Vx is a set of labeled arcs.The set Vx of nodes is the set of positive integersup to and including n, each corresponding to thelinear position of a word in the sentence, plus anextra artificial root node 0.
The set A of arcs is aset of triples (i, l, j), where i and j are nodes and lis a label.
For a dependency graph G = (Vx, A) tobe well-formed, we in addition require that it is atree rooted at the node 0, as illustrated in Figure 1.2.2 Transition SystemsFollowing Nivre (2008a), we define a transitionsystem for dependency parsing as a quadruple S =(C, T, cs, Ct), where1.
C is a set of configurations,2.
T is a set of transitions, each of which is a(partial) function t : C ?
C,3.
cs is an initialization function, mapping asentence x = w1, .
.
.
, wn to a configurationc ?
C,4.
Ct ?
C is a set of terminal configurations.In this paper, we take the set C of configurationsto be the set of all triples c = (?, B,A) such that?
and B are disjoint sublists of the nodes Vx ofsome sentence x, andA is a set of dependency arcsover Vx (and some label set L); we take the initialconfiguration for a sentence x = w1, .
.
.
, wn tobe cs(x) = ([0], [1, .
.
.
, n], { }); and we take theset Ct of terminal configurations to be the set ofall configurations of the form c = ([0], [ ], A) (forany arc set A).
The set T of transitions will bediscussed in detail in Sections 3.1?3.2.We will refer to the list?
as the stack and the listB as the buffer, and we will use the variables ?
and?
for arbitrary sublists of ?
and B, respectively.For reasons of perspicuity, we will write ?
with itshead (top) to the right and B with its head to theleft.
Thus, c = ([?|i], [j|?
], A) is a configurationwith the node i on top of the stack ?
and the nodej as the first node in the buffer B.Given a transition system S = (C, T, cs, Ct), atransition sequence for a sentence x is a sequenceC0,m = (c0, c1, .
.
.
, cm) of configurations, suchthat1.
c0 = cs(x),2. cm ?
Ct,3.
for every i (1 ?
i ?
m), ci = t(ci?1) forsome t ?
T .352Transition ConditionLEFT-ARCl ([?|i, j], B,A)?
([?|j], B,A?
{(j, l, i)}) i 6= 0RIGHT-ARCl ([?|i, j], B,A)?
([?|i], B,A?
{(i, l, j)})SHIFT (?, [i|?
], A)?
([?|i], ?, A)SWAP ([?|i, j], ?, A)?
([?|j], [i|?
], A) 0 < i < jFigure 2: Transitions for dependency parsing; Tp = {LEFT-ARCl, RIGHT-ARCl, SHIFT}; Tu = Tp ?
{SWAP}.The parse assigned to S by C0,m is the depen-dency graph Gcm = (Vx, Acm), where Acm is theset of arcs in cm.A transition system S is sound for a class G ofdependency graphs iff, for every sentence x andtransition sequence C0,m for x in S, Gcm ?
G. Sis complete for G iff, for every sentence x and de-pendency graph G for x in G, there is a transitionsequence C0,m for x in S such that Gcm = G.2.3 Deterministic Transition-Based ParsingAn oracle for a transition system S is a functiono : C ?
T .
Ideally, o should always return theoptimal transition t for a given configuration c, butall we require formally is that it respects the pre-conditions of transitions in T .
That is, if o(c) = tthen t is permissible in c. Given an oracle o, deter-ministic transition-based parsing can be achievedby the following simple algorithm:PARSE(o, x)1 c?
cs(x)2 while c 6?
Ct3 do t?
o(c); c?
t(c)4 return GcStarting in the initial configuration cs(x), theparser repeatedly calls the oracle function o for thecurrent configuration c and updates c according tothe oracle transition t. The iteration stops when aterminal configuration is reached.
It is easy to seethat, provided that there is at least one transitionsequence in S for every sentence, the parser con-structs exactly one transition sequence C0,m for asentence x and returns the parse defined by the ter-minal configuration cm, i.e., Gcm = (Vx, Acm).Assuming that the calls o(c) and t(c) can both beperformed in constant time, the worst-case timecomplexity of a deterministic parser based on atransition system S is given by an upper bound onthe length of transition sequences in S.When building practical parsing systems, theoracle can be approximated by a classifier trainedon treebank data, a technique that has been usedsuccessfully in a number of systems (Yamada andMatsumoto, 2003; Nivre et al, 2004; Attardi,2006).
This is also the approach we will take inthe experimental evaluation in Section 4.3 Transitions for Dependency ParsingHaving defined the set of configurations, includinginitial and terminal configurations, we will nowfocus on the transition set T required for depen-dency parsing.
The total set of transitions that willbe considered is given in Figure 2, but we will startin Section 3.1 with the subset Tp (p for projective)consisting of the first three.
In Section 3.2, wewill add the fourth transition (SWAP) to get the fulltransition set Tu (u for unrestricted).3.1 Projective Dependency ParsingThe minimal transition set Tp for projective depen-dency parsing contains three transitions:1.
LEFT-ARCl updates a configuration with i, jon top of the stack by adding (j, l, i) to A andreplacing i, j on the stack by j alone.
It ispermissible as long as i is distinct from 0.2.
RIGHT-ARCl updates a configuration withi, j on top of the stack by adding (i, l, j) toA and replacing i, j on the stack by i alone.3.
SHIFT updates a configuration with i as thefirst node of the buffer by removing i fromthe buffer and pushing it onto the stack.The system Sp = (C, Tp, cs, Ct) is sound andcomplete for the set of projective dependencytrees (over some label set L) and has been used,in slightly different variants, by a number oftransition-based dependency parsers (Yamada andMatsumoto, 2003; Nivre, 2004; Attardi, 2006;353Transition Stack (?)
Buffer (B) Added Arc[ROOT0] [A1, .
.
.
, .9]SHIFT [ROOT0,A1] [hearing2, .
.
.
, .9]SHIFT [ROOT0,A1, hearing2] [is3, .
.
.
, .9]LADET [ROOT0, hearing2] [is3, .
.
.
, .9] (2, DET, 1)SHIFT [ROOT0, hearing2, is3] [scheduled4, .
.
.
, .9]SHIFT [ROOT0, .
.
.
, is3, scheduled4] [on5, .
.
.
, .9]SHIFT [ROOT0, .
.
.
, scheduled4, on5] [the6, .
.
.
, .9]SWAP [ROOT0, .
.
.
, is3, on5] [scheduled4, .
.
.
, .9]SWAP [ROOT0, hearing2, on5] [is3, .
.
.
, .9]SHIFT [ROOT0, .
.
.
, on5, is3] [scheduled4, .
.
.
, .9]SHIFT [ROOT0, .
.
.
, is3, scheduled4] [the6, .
.
.
, .9]SHIFT [ROOT0, .
.
.
, scheduled4, the6] [issue7, .
.
.
, .9]SWAP [ROOT0, .
.
.
, is3, the6] [scheduled4, .
.
.
, .9]SWAP [ROOT0, .
.
.
, on5, the6] [is3, .
.
.
, .9]SHIFT [ROOT0, .
.
.
, the6, is3] [scheduled4, .
.
.
, .9]SHIFT [ROOT0, .
.
.
, is3, scheduled4] [issue7, .
.
.
, .9]SHIFT [ROOT0, .
.
.
, scheduled4, issue7] [today8, .9]SWAP [ROOT0, .
.
.
, is3, issue7] [scheduled4, .
.
.
, .9]SWAP [ROOT0, .
.
.
, the6, issue7] [is3, .
.
.
, .9]LADET [ROOT0, .
.
.
, on5, issue7] [is3, .
.
.
, .9] (7, DET, 6)RAPC [ROOT0, hearing2, on5] [is3, .
.
.
, .9] (5, PC, 7)RANMOD [ROOT0, hearing2] [is3, .
.
.
, .9] (2, NMOD, 5)SHIFT [ROOT0, .
.
.
, hearing2, is3] [scheduled4, .
.
.
, .9]LASBJ [ROOT0, is3] [scheduled4, .
.
.
, .9] (3, SBJ, 2)SHIFT [ROOT0, is3, scheduled4] [today8, .9]SHIFT [ROOT0, .
.
.
, scheduled4, today8] [.9]RAADV [ROOT0, is3, scheduled4] [.9] (4, ADV, 8)RAVG [ROOT0, is3] [.9] (3, VG, 4)SHIFT [ROOT0, is3, .9] [ ]RAP [ROOT0, is3] [ ] (3, P, 9)RAROOT [ROOT0] [ ] (0, ROOT, 3)Figure 3: Transition sequence for parsing the sentence in Figure 1 (LA = LEFT-ARC, RA = REFT-ARC).Nivre, 2008a).
For proofs of soundness and com-pleteness, see Nivre (2008a).As noted in section 2, the worst-case time com-plexity of a deterministic transition-based parser isgiven by an upper bound on the length of transitionsequences.
In Sp, the number of transitions for asentence x = w1, .
.
.
, wn is always exactly 2n,since a terminal configuration can only be reachedafter n SHIFT transitions (moving nodes 1, .
.
.
, nfrom B to ?)
and n applications of LEFT-ARCl orRIGHT-ARCl (removing the same nodes from ?
).Hence, the complexity of deterministic parsing isO(n) in the worst case (as well as in the best case).3.2 Unrestricted Dependency ParsingWe now consider what happens when we add thefourth transition from Figure 2 to get the extendedtransition set Tu.
The SWAP transition updatesa configuration with stack [?|i, j] by moving thenode i back to the buffer.
This has the effect thatthe order of the nodes i and j in the appended list?+B is reversed compared to the original wordorder in the sentence.
It is important to note thatSWAP is only permissible when the two nodes ontop of the stack are in the original word order,which prevents the same two nodes from beingswapped more than once, and when the leftmostnode i is distinct from the root node 0.
Note alsothat SWAP moves the node i back to the buffer, sothat LEFT-ARCl, RIGHT-ARCl or SWAP can sub-sequently apply with the node j on top of the stack.The fact that we can swap the order of nodes,implicitly representing subtrees, means that wecan construct non-projective trees by applying354o(c) =????????
?LEFT-ARCl if c = ([?|i, j], B,Ac), (j, l, i)?A and Ai ?
AcRIGHT-ARCl if c = ([?|i, j], B,Ac), (i, l, j)?A and Aj ?
AcSWAP if c = ([?|i, j], B,Ac) and j <G iSHIFT otherwiseFigure 4: Oracle function for Su = (C, Tu, cs, Ct) with target tree G = (Vx, A).
We use the notation Aito denote the subset of A that only contains the outgoing arcs of the node i.LEFT-ARCl or RIGHT-ARCl to subtrees whoseyields are not adjacent according to the originalword order.
This is illustrated in Figure 3, whichshows the transition sequence needed to parse theexample in Figure 1.
For readability, we representboth the stack ?
and the bufferB as lists of tokens,indexed by position, rather than abstract nodes.The last column records the arc that is added tothe arc set A in a given transition (if any).Given the simplicity of the extension, it is ratherremarkable that the system Su = (C, Tu, cs, Ct)is sound and complete for the set of all depen-dency trees (over some label set L), including allnon-projective trees.
The soundness part is triv-ial, since any terminating transition sequence willhave to move all the nodes 1, .
.
.
, n from B to ?
(using SHIFT) and then remove them from ?
(us-ing LEFT-ARCl or RIGHT-ARCl), which will pro-duce a tree with root 0.For completeness, we note first that projectiv-ity is not a property of a dependency tree in itself,but of the tree in combination with a word order,and that a tree can always be made projective byreordering the nodes.
For instance, let x be a sen-tence with dependency tree G = (Vx, A), and let<G be the total order on Vx defined by an inordertraversal of G that respects the local ordering of anode and its children given by the original wordorder.
Regardless of whether G is projective withrespect to x, it must by necessity be projective withrespect to <G.
We call <G the projective ordercorresponding to x and G and use it as our canoni-cal way of finding a node order that makes the treeprojective.
By way of illustration, the projectiveorder for the sentence and tree in Figure 1 is: A1<G hearing2 <G on5 <G the6 <G issue7 <G is3<G scheduled4 <G today8 <G .9.If the words of a sentence x with dependencytree G are already in projective order, this meansthat G is projective with respect to x and that wecan parse the sentence using only transitions in Tp,because nodes can be pushed onto the stack in pro-jective order using only the SHIFT transition.
Ifthe words are not in projective order, we can usea combination of SHIFT and SWAP transitions toensure that nodes are still pushed onto the stack inprojective order.
More precisely, if the next nodein the projective order is the kth node in the buffer,we perform k SHIFT transitions, to get this nodeonto the stack, followed by k?1 SWAP transitions,to move the preceding k ?
1 nodes back to thebuffer.1 In this way, the parser can effectively sortthe input nodes into projective order on the stack,repeatedly extracting the minimal element of <Gfrom the buffer, and build a tree that is projectivewith respect to the sorted order.
Since any inputcan be sorted using SHIFT and SWAP, and any pro-jective tree can be built using SHIFT, LEFT-ARCland RIGHT-ARCl, the system Su is complete forthe set of all dependency trees.In Figure 4, we define an oracle function o forthe system Su, which implements this ?sort andparse?
strategy and predicts the optimal transitiont out of the current configuration c, given the tar-get dependency tree G = (Vx, A) and the pro-jective order <G.
The oracle predicts LEFT-ARClor RIGHT-ARCl if the two top nodes on the stackshould be connected by an arc and if the depen-dent node of this arc is already connected to all itsdependents; it predicts SWAP if the two top nodesare not in projective order; and it predicts SHIFTotherwise.
This is the oracle that has been used togenerate training data for classifiers in the experi-mental evaluation in Section 4.Let us now consider the time complexity of theextended system Su = (C, Tu, cs, Ct) and let usbegin by observing that 2n is still a lower boundon the number of transitions required to reach aterminal configuration.
A sequence of 2n transi-1This can be seen in Figure 3, where transitions 4?8, 9?13, and 14?18 are the transitions needed to make sure thaton5, the6 and issue7 are processed on the stack before is3 andscheduled4.355Figure 5: Abstract running time during training (black) and parsing (white) for Arabic (1460/146 sen-tences) and Danish (5190/322 sentences).tions occurs when no SWAP transitions are per-formed, in which case the behavior of the systemis identical to the simpler system Sp.
This is im-portant, because it means that the best-case com-plexity of the deterministic parser is still O(n) andthat the we can expect to observe the best case forall sentences with projective dependency trees.The exact number of additional transitionsneeded to reach a terminal configuration is deter-mined by the number of SWAP transitions.
SinceSWAP moves one node from ?
to B, there willbe one additional SHIFT for every SWAP, whichmeans that the total number of transitions is 2n +2k, where k is the number of SWAP transitions.Given the condition that SWAP can only apply in aconfiguration c = ([?|i, j], B,A) if 0 < i < j, thenumber of SWAP transitions is bounded by n(n?1)2 ,which means that 2n + n(n ?
1) = n + n2 is anupper bound on the number of transitions in a ter-minating sequence.
Hence, the worst-case com-plexity of the deterministic parser is O(n2).The running time of a deterministic transition-based parser using the system Su is O(n) in thebest case and O(n2) in the worst case.
But whatabout the average case?
Empirical studies, basedon data from a wide range of languages, haveshown that dependency trees tend to be projectiveand that most non-projective trees only containa small number of discontinuities (Nivre, 2006;Kuhlmann and Nivre, 2006; Havelka, 2007).
Thisshould mean that the expected number of swapsper sentence is small, and that the running time islinear on average for the range of inputs that occurin natural languages.
This is a hypothesis that willbe tested experimentally in the next section.4 ExperimentsOur experiments are based on five data sets fromthe CoNLL-X shared task: Arabic, Czech, Danish,Slovene, and Turkish (Buchholz and Marsi, 2006).These languages have been selected because thedata come from genuine dependency treebanks,whereas all the other data sets are based on somekind of conversion from another type of represen-tation, which could potentially distort the distribu-tion of different types of structures in the data.4.1 Running TimeIn section 3.2, we hypothesized that the expectedrunning time of a deterministic parser using thetransition system Su would be linear, rather thanquadratic.
To test this hypothesis, we examinehow the number of transitions varies as a func-tion of sentence length.
We call this the abstractrunning time, since it abstracts over the actualtime needed to compute each oracle prediction andtransition, which is normally constant but depen-dent on the type of classifier used.We first measured the abstract running time onthe training sets, using the oracle to derive thetransition sequence for every sentence, to see howmany transitions are required in the ideal case.
Wethen performed the same measurement on the testsets, using classifiers trained on the oracle transi-tion sequences from the training sets (as describedbelow in Section 4.2), to see whether the trainedparsers deviate from the ideal case.The result for Arabic and Danish can be seen356Arabic Czech Danish Slovene TurkishSystem AS EM AS EM AS EM AS EM AS EMSu 67.1 (9.1) 11.6 82.4 (73.8) 35.3 84.2 (22.5) 26.7 75.2 (23.0) 29.9 64.9 (11.8) 21.5Sp 67.3 (18.2) 11.6 80.9 (3.7) 31.2 84.6 (0.0) 27.0 74.2 (3.4) 29.9 65.3 (6.6) 21.0Spp 67.2 (18.2) 11.6 82.1 (60.7) 34.0 84.7 (22.5) 28.9 74.8 (20.7) 26.9 65.5 (11.8) 20.7Malt-06 66.7 (18.2) 11.0 78.4 (57.9) 27.4 84.8 (27.5) 26.7 70.3 (20.7) 19.7 65.7 (9.2) 19.3MST-06 66.9 (0.0) 10.3 80.2 (61.7) 29.9 84.8 (62.5) 25.5 73.4 (26.4) 20.9 63.2 (11.8) 20.2MSTMalt 68.6 (9.4) 11.0 82.3 (69.2) 31.2 86.7 (60.0) 29.8 75.9 (27.6) 26.6 66.3 (9.2) 18.6Table 1: Labeled accuracy; AS = attachment score (non-projective arcs in brackets); EM = exact match.in Figure 5, where black dots represent trainingsentences (parsed with the oracle) and white dotsrepresent test sentences (parsed with a classifier).For Arabic there is a very clear linear relationshipin both cases with very few outliers.
Fitting thedata with a linear function using the least squaresmethod gives us m = 2.06n (R2 = 0.97) for thetraining data and m = 2.02n (R2 = 0.98) for thetest data, where m is the number of transitions inparsing a sentence of length n. For Danish, thereis clearly more variation, especially for the train-ing data, but the least-squares approximation stillexplains most of the variance, with m = 2.22n(R2 = 0.85) for the training data and m = 2.07n(R2 = 0.96) for the test data.
For both languages,we thus see that the classifier-based parsers havea lower mean number of transitions and less vari-ance than the oracle parsers.
And in both cases, theexpected number of transitions is only marginallygreater than the 2n of the strictly projective transi-tion system Sp.We have chosen to display results for Arabicand Danish because they are the two extremes inour sample.
Arabic has the smallest variance andthe smallest linear coefficients, and Danish has thelargest variance and the largest coefficients.
Theremaining three languages all lie somewhere inthe middle, with Czech being closer to Arabic andSlovene closer to Danish.
Together, the evidencefrom all five languages strongly corroborates thehypothesis that the expected running time for thesystem Su is linear in sentence length for naturallyoccurring data.4.2 Parsing AccuracyIn order to assess the parsing accuracy that canbe achieved with the new transition system, wetrained a deterministic parser using the new tran-sition system Su for each of the five languages.For comparison, we also trained two parsers usingSp, one that is strictly projective and one that usesthe pseudo-projective parsing technique to recovernon-projective dependencies in a post-processingstep (Nivre and Nilsson, 2005).
We will refer tothe latter system as Spp.
All systems use SVMclassifiers with a polynomial kernel to approxi-mate the oracle function, with features and para-meters taken from Nivre et al (2006), which wasthe best performing transition-based system in theCoNLL-X shared task.2Table 1 shows the labeled parsing accuracy ofthe parsers measured in two ways: attachmentscore (AS) is the percentage of tokens with thecorrect head and dependency label; exact match(EM) is the percentage of sentences with a com-pletely correct labeled dependency tree.
The scorein brackets is the attachment score for the (small)subset of tokens that are connected to their headby a non-projective arc in the gold standard parse.For comparison, the table also includes resultsfor the two best performing systems in the origi-nal CoNLL-X shared task, Malt-06 (Nivre et al,2006) and MST-06 (McDonald et al, 2006), aswell as the integrated system MSTMalt, which isa graph-based parser guided by the predictions ofa transition-based parser and currently has the bestreported results on the CoNLL-X data sets (Nivreand McDonald, 2008).Looking first at the overall attachment score, wesee that Su gives a substantial improvement overSp (and outperforms Spp) for Czech and Slovene,where the scores achieved are rivaled only by thecombo system MSTMalt.
For these languages,there is no statistical difference between Su andMSTMalt, which are both significantly better thanall the other parsers, except Spp for Czech (Mc-Nemar?s test, ?
= .05).
This is accompaniedby an improvement on non-projective arcs, where2Complete information about experimental settings canbe found at http://stp.lingfil.uu.se/?nivre/exp/.357Su outperforms all other systems for Czech andis second only to the two MST parsers (MST-06and MSTMalt) for Slovene.
It is worth noting thatthe percentage of non-projective arcs is higher forCzech (1.9%) and Slovene (1.9%) than for any ofthe other languages.For the other three languages, Su has a dropin overall attachment score compared to Sp, butnone of these differences is statistically signifi-cant.
In fact, the only significant differences inattachment score here are the positive differencesbetweenMSTMalt and all other systems for Arabicand Danish, and the negative difference betweenMST-06 and all other systems for Turkish.
Theattachment scores for non-projective arcs are gen-erally very low for these languages, except for thetwo MST parsers on Danish, but Su performs atleast as well as Spp on Danish and Turkish.
(Theresults for Arabic are not very meaningful, giventhat there are only eleven non-projective arcs inthe entire test set, of which the (pseudo-)projectiveparsers found two and Su one, while MSTMalt andMST-06 found none at all.
)Considering the exact match scores, finally, it isvery interesting to see that Su almost consistentlyoutperforms all other parsers, including the combosystem MSTMalt, and sometimes by a fairly widemargin (Czech, Slovene).
The difference is statis-tically significant with respect to all other systemsexcept MSTMalt for Slovene, all except MSTMaltand Spp for Czech, and with respect to MSTMaltfor Turkish.
For Arabic and Danish, there are nosignificant differences in the exact match scores.We conclude that Su may increase the probabil-ity of finding a completely correct analysis, whichis sometimes reflected also in the overall attach-ment score, and we conjecture that the strength ofthe positive effect is dependent on the frequencyof non-projective arcs in the language.5 Related WorkProcessing non-projective trees by swapping theorder of words has recently been proposed by bothNivre (2008b) and Titov et al (2009), but thesesystems cannot handle unrestricted non-projectivetrees.
It is worth pointing out that, although thesystem described in Nivre (2008b) uses four tran-sitions bearing the same names as the transitionsof Su, the two systems are not equivalent.
In par-ticular, the system of Nivre (2008b) is sound butnot complete for the class of all dependency trees.There are also affinities to the system of Attardi(2006), which combines non-adjacent nodes onthe stack instead of swapping nodes and is equiva-lent to a restricted version of our system, where nomore than two consecutive SWAP transitions arepermitted.
This restriction preserves linear worst-case complexity at the expense of completeness.Finally, the algorithm first described by Covington(2001) and used for data-driven parsing by Nivre(2007), is complete but has quadratic complexityeven in the best case.6 ConclusionWe have presented a novel transition system fordependency parsing that can handle unrestrictednon-projective trees.
The system reuses standardtechniques for building projective trees by com-bining adjacent nodes (representing subtrees withadjacent yields), but adds a simple mechanism forswapping the order of nodes on the stack, whichgives a system that is sound and complete for theset of all dependency trees over a given label setbut behaves exactly like the standard system forthe subset of projective trees.
As a result, the timecomplexity of deterministic parsing is O(n2) inthe worst case, which is rare, but O(n) in the bestcase, which is common, and experimental resultson data from five languages support the conclusionthat expected running time is linear in the lengthof the sentence.
Experimental results also showthat parsing accuracy is competitive, especiallyfor languages like Czech and Slovene where non-projective dependency structures are common, andespecially with respect to the exact match score,where it has the best reported results for four outof five languages.
Finally, the simplicity of thesystem makes it very easy to implement.Future research will include an in-depth erroranalysis to find out why the system works betterfor some languages than others and why the exactmatch score improves even when the attachmentscore goes down.
In addition, we want to explorealternative oracle functions, which try to minimizethe number of swaps by allowing the stack to betemporarily ?unsorted?.AcknowledgmentsThanks to Johan Hall and Jens Nilsson for helpwith implementation and evaluation, and to MarcoKuhlmann and three anonymous reviewers foruseful comments.358ReferencesGiuseppe Attardi.
2006.
Experiments with a multi-language non-projective dependency parser.
In Pro-ceedings of CoNLL, pages 166?170.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of CoNLL, pages 149?164.Michael A. Covington.
2001.
A fundamental algo-rithm for dependency parsing.
In Proceedings of the39th Annual ACM Southeast Conference, pages 95?102.Carlos Go?mez-Rodr?
?guez, David Weir, and John Car-roll.
2009.
Parsing mildly non-projective depen-dency structures.
In Proceedings of EACL, pages291?299.Keith Hall and Vaclav Nova?k.
2005.
Corrective mod-eling for non-projective dependency parsing.
InProceedings of IWPT, pages 42?52.Jiri Havelka.
2007.
Beyond projectivity: Multilin-gual evaluation of constraints and measures on non-projective structures.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics, pages 608?615.Richard Johansson and Pierre Nugues.
2007.
Incre-mental dependency parsing using online learning.
InProceedings of the Shared Task of EMNLP-CoNLL,pages 1134?1138.Marco Kuhlmann and Joakim Nivre.
2006.
Mildlynon-projective dependency structures.
In Proceed-ings of the COLING/ACL Main Conference PosterSessions, pages 507?514.Marco Kuhlmann and Giorgio Satta.
2009.
Treebankgrammar techniques for non-projective dependencyparsing.
In Proceedings of EACL, pages 478?486.Ryan McDonald and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proceedings of EACL, pages 81?88.Ryan McDonald and Giorgio Satta.
2007.
On the com-plexity of non-projective data-driven dependencyparsing.
In Proceedings of IWPT, pages 122?131.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005a.
Online large-margin training of de-pendency parsers.
In Proceedings of ACL, pages 91?98.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005b.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof HLT/EMNLP, pages 523?530.RyanMcDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with atwo-stage discriminative parser.
In Proceedings ofCoNLL, pages 216?220.Peter Neuhaus and Norbert Bro?ker.
1997.
The com-plexity of recognition of linguistically adequate de-pendency grammars.
In Proceedings of ACL/EACL,pages 337?343.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In Proceedings of ACL, pages 950?958.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projective dependency parsing.
In Proceedings ofACL, pages 99?106.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-based dependency parsing.
In Proceedingsof CoNLL, pages 49?56.Joakim Nivre, Johan Hall, Jens Nilsson, Gu?lsenEryig?it, and Svetoslav Marinov.
2006.
Labeledpseudo-projective dependency parsing with supportvector machines.
In Proceedings of CoNLL, pages221?225.Joakim Nivre.
2004.
Incrementality in deterministicdependency parsing.
In Proceedings of the Work-shop on Incremental Parsing: Bringing Engineeringand Cognition Together (ACL), pages 50?57.Joakim Nivre.
2006.
Constraints on non-projective de-pendency graphs.
In Proceedings of EACL, pages73?80.Joakim Nivre.
2007.
Incremental non-projective de-pendency parsing.
In Proceedings of NAACL HLT,pages 396?403.Joakim Nivre.
2008a.
Algorithms for deterministic in-cremental dependency parsing.
Computational Lin-guistics, 34:513?553.Joakim Nivre.
2008b.
Sorting out dependency pars-ing.
In Proceedings of the 6th International Con-ference on Natural Language Processing (GoTAL),pages 16?27.Ivan Titov and James Henderson.
2007.
A latent vari-able model for generative dependency parsing.
InProceedings of IWPT, pages 144?155.Ivan Titov, James Henderson, Paola Merlo, andGabriele Musillo.
2009.
Online graph planarizationfor synchronous parsing of semantic and syntacticdependencies.
In Proceedings of IJCAI.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proceedings of IWPT, pages 195?206.359
