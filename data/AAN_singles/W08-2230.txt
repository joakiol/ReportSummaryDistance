Addressing the ResourceBottleneck to CreateLarge-Scale Annotated TextsJon ChamberlainUniversity of Essex (UK)email: jchamb@essex.ac.ukMassimo PoesioUniversity of Essex (UK) & Universit?
di Trento (Italy)email: poesio@essex.ac.ukUdo KruschwitzUniversity of Essex (UK)email: udo@essex.ac.ukAbstractLarge-scale linguistically annotated resources have become available inrecent years.
This is partly due to sophisticated automatic and semi-automatic approaches that work well on specific tasks such as part-of-speech tagging.
For more complex linguistic phenomena like anaphoraresolution there are no tools that result in high-quality annotations with-out massive user intervention.
Annotated corpora of the size needed formodern computational linguistics research cannot however be created bysmall groups of hand annotators.
The ANAWIKI project strikes a balancebetween collecting high-quality annotations from experts and applying agame-like approach to collecting linguistic annotation from the generalWeb population.
More generally, ANAWIKI is a project that explores towhat extend expert annotations can be substituted by a critical mass ofnon-expert judgements.375376 Chamberlain, Poesio, and Kruschwitz1 IntroductionSyntactically annotated language resources have long been around, but the greatestobstacle to progress towards systems able to extract semantic information from textis the lack of semantically annotated corpora large enough to be used to train andevaluate semantic interpretation methods.
Recent efforts to create resources to sup-port large evaluation initiatives in the USA such as Automatic Context Extraction(ACE), Translingual Information Detection, Extraction and Summarization (TIDES),and GALE are beginning to change this, but just at a point when the community isbeginning to realize that even the 1M word annotated corpora created in substantialefforts such as Prop-Bank (Palmer et al, 2005) and the OntoNotes initiative (Hovyet al, 2006) are likely to be too small.Unfortunately, the creation of 100M-plus corpora via hand annotation is likely tobe prohibitively expensive.
Such a large hand-annotation effort would be even lesssensible in the case of semantic annotation tasks such as coreference or wordsensedisambiguation, given on the one side the greater difficulty of agreeing on a ?neutral?theoretical framework, on the other the difficulty of achieving more than moderateagreement on semantic judgments (Poesio and Artstein, 2005).The ANAWIKI project1 presents an effort to create high-quality, large-scale anaphor-ically annotated resources (Poesio et al, 2008) by taking advantage of the collabora-tion of the Web community, both through co-operative annotation efforts using tra-ditional annotation tools and through the use of game-like interfaces.
This makesANAWIKI a very ambitious project.
It is not clear to what extend expert annotationscan in fact be substituted by those judgements submitted by the general public as partof a game.
If successful, ANAWIKI will actually be more than just an anaphora anno-tation tool.
We see it as a framework aimed at creating large-scale annotated corporain general.2 Creating Resources through Web CollaborationLarge-scale annotation of low-level linguistic information (part-of-speech tags) be-gan with the Brown Corpus, in which very low-tech and time consuming methodswere used; but already for the creation of the British National Corpus (BNC), the first100M-word linguistically annotated corpus, a faster methodology was developed con-sisting of preliminary annotation with automatic methods followed by partial hand-correction (Burnard, 2000).
Medium and large-scale semantic annotation projects(coreference, wordsense) are a fairly recent innovation in Computational Linguistics(CL).
The semi-automatic annotation methodology cannot yet be used for this type ofannotation, as the quality of, for instance, coreference resolvers is not yet high enoughon general text.Collective resource creation on the Web offers a different way to the solution ofthis problem.
Wikipedia is perhaps the best example of collective resource creation,but it is not an isolated case.
The willingness of Web users to volunteer on the Webextends to projects to create resources for Artificial Intelligence.
One example is theOpenMind Commonsense project, a project to mine commonsense knowledge (Singh,2002) to which 14,500 participants contributed nearly 700,000 sentences.
A more1http://www.anawiki.orgAddressing the Resource Bottleneck to Create Annotated Texts 377recent, and perhaps more intriguing, development is the use of interactive game-styleinterfaces to collect knowledge such as von Ahn et al (2006).
Perhaps the best knownexample of this approach is the ESP game, a project to label images with tags througha competitive game (von Ahn, 2006); 13,500 users played the game, creating 1.3Mlabels in 3 months.
If we managed to attract 15,000 volunteers, and each of them wereto annotate 10 texts of 700 words, we would get a corpus of the size of the BNC.ANAWIKI builds on the proposals for marking anaphoric information allowing forambiguity developed in ARRAU (Poesio and Artstein, 2005) and previous projects.The ARRAU project found that (i) using numerous annotators (up to 20 in some ex-periments) leads to a much more robust identification of the major interpretation al-ternatives (although outliers are also frequent); and (ii) the identification of alternativeinterpretations is much more frequently a case of implicit ambiguity (each annotatoridentifies only one interpretation, but these are different) than of explicit ambiguity(annotators identifying multiple interpretations).
The ARRAU project also developedmethods to analyze collections of such alternative interpretations and to identify out-liers via clustering that will be exploited in this project.Figure 1: A screenshot of the Serengeti expert annotation tool.3 Annotation ToolsAttempts to create hand annotated corpora face the dilemma of either going for thetraditional CL approach of high-quality annotation (of limited size) by experts or toinvolve a large population of non-experts which could result in large-scale corporaof inferior quality.
The ANAWIKI project bridges this gap by combining both ap-proaches to annotate the data: an expert annotation tool and a game interface.
Both378 Chamberlain, Poesio, and KruschwitzFigure 2: A screenshot of the Game Interface (Annotation Mode).tools are essential parts of ANAWIKI.We briefly describe both, with a particular focuson the game interface.3.1 Expert Annotation ToolAn expert annotation tool is used to obtain Gold Standard annotations from computa-tional linguists.
In the case of anaphora annotationwe use the Serengeti tool developedat the University of Bielefeld (St?hrenberg et al, 2007).
The anaphoric annotation ofmarkables within this environment will be very detailed and will serve as a trainingcorpus as well as quality check for the second tool (see below).
Figure 1 is a screen-shot of this interface.3.2 Game InterfaceA game interface is used to collect annotations from the general Web population.
Thegame interface integrates with the database of the expert annotation tool but aims tocollect large-scale (rather than detailed) anaphoric relations.
Users are simply askedto assign an anaphoric link but are not asked to specify what type (or what features)are present.Phrase Detectives2 is a game offering a simple user interface for non-expert usersto learn how to annotate text and to make annotation decisions.
The goal of the gameis to identify relationships between words and phrases in a short text.
Markables are2http://www.phrasedetectives.orgAddressing the Resource Bottleneck to Create Annotated Texts 379identified in the text by automatic pre-processing.
There are 2 ways to annotate withinthe game: by selecting the markable that is the antecedent of the anaphor (AnnotationMode ?
see Figure 2); or by validating a decision previously submitted by anotheruser (Validation Mode).
One motivation for Validation Mode is that we anticipate itto be twice as fast as Annotation Mode (Chklovski and Gil, 2005).Users begin the game at the training level and are given a set of annotation taskscreated from the Gold Standard.
They are given feedback and guidance when theyselect an incorrect answer and points when they select the correct answer.
Whenthe user gives enough correct answers they graduate to annotating texts that will beincluded in the corpus.
Occasionally, a graduated user will be covertly given a GoldStandard text to annotate.
This is the foundation of the user rating system used tojudge the quality of the user?s annotations.The game is designed to motivate users to annotate the text correctly by using com-parative scoring (awarding points for agreeing with the Gold Standard), and retroac-tive scoring (awarding points to the previous user if they are agreed with by the currentuser).
Using leader boards and assigning levels for points has been proven to be aneffective motivator, with users often using these as targets (von Ahn, 2006).
The gameinterface is described in more detail elsewhere (Chamberlain et al, 2008).4 ChallengesWe are aiming at a balanced corpus, similar to the BNC, that includes texts fromProject Gutenberg, the Open American National Corpus, the Enron corpus and otherfreely available sources.
The chosen texts are stripped of all presentation formatting,HTML and links to create the raw text.
This is automatically parsed to extract mark-ables consisting of noun phrases.
The resulting XML format is stored in a relationaldatabase that can be used in both the expert annotation tool and the game.There are a number of challenges remaining in the project.
First of all, the fullyautomated processing of a substantial (i.e.
multi-million) word corpus comprisingmore than just news articles turned out to be non-trivial both in terms of robustness ofthe processing tools as well as in terms of linguistic quality.A second challenge is to recruit enough volunteers to annotate a 100 million wordcorpus within the timescale of the project.
It is our intention to use social networkingsites (including Facebook, Bebo, and MySpace) to attract volunteers to the game andmotivate participation by providing widgets (code segments that display the user?sscore and links to the game) to add to their profile pages.Finally, the project?s aim is to generate a sufficiently large collection of annotationsfrom which semantically annotated corpora can be constructed.
The usefulness of thecreated resources can only be proven, for example, by training anaphora resolutionalgorithms on the resulting annotations.
This will be future work.5 Next StepsWe are currently in the process of building up a critical mass of source texts.
Our aimis to have a corpus size of 1M words by September 2008.
By this time we also intendhaving a multilingual user interface (initially English, Italian and German) with thecapacity to annotate texts in different languages although this is not the main focus.380 Chamberlain, Poesio, and KruschwitzIn the future we will be considering extending the interface to include different anno-tation tasks, for example marking coreference chains or Semantic Web mark-up.
Wewould like to present the game interface to gain feedback from the linguistic commu-nity.AcknowledgementsANAWIKI is funded by EPSRC (EP/F00575X/1).
Thanks to Daniela Goecke, MaikSt?hrenberg, Nils Diewald and Dieter Metzing.
We also want to thank all volunteerswho have already contributed to the project and the reviewers for valuable feedback.ReferencesBurnard, L. (2000).
The British National Corpus Reference guide.
Technical report,Oxford University Computing Services, Oxford.Chamberlain, J., M. Poesio, and U. Kruschwitz (2008).
Phrase Detectives: A Web-based Collaborative Annotation Game.
In Proceedings of the International Con-ference on Semantic Systems (I-Semantics?08), Graz.
Forthcoming.Chklovski, T. and Y. Gil (2005).
Improving the design of intelligent acquisition in-terfaces for collecting world knowledge from web contributors.
In Proceedings ofK-CAP ?05, pp.
35?42.Hovy, E., M.Marcus, M. Palmer, L. Ramshaw, and R. Weischedel (2006).
OntoNotes:The 90% Solution.
In Proceedings of HLT-NAACL06.Palmer, M., D. Gildea, and P. Kingsbury (2005).
The proposition bank: An annotatedcorpus of semantic roles.
Computational Linguistics 31(1), 71?106.Poesio, M. and R. Artstein (2005).
The reliability of anaphoric annotation, recon-sidered: Taking ambiguity into account.
In Proceedings of the ACL Workshop onFrontiers in Corpus Annotation, pp.
76?83.Poesio, M., U. Kruschwitz, and J. Chamberlain (2008).
ANAWIKI: Creating anaphor-ically annotated resources through Web cooperation.
In Proceedings of LREC?08,Marrakech.Singh, P. (2002).
The public acquisition of commonsense knowledge.
In Proceedingsof the AAAI Spring Symposium on Acquiring (and Using) Linguistic (and World)Knowledge for Information Access, Palo Alto, CA.St?hrenberg, M., D. Goecke, N. Diewald, A. Mehler, and I. Cramer (2007).
Web-based annotation of anaphoric relations and lexical chains.
In Proceedings of theACL Linguistic Annotation Workshop, pp.
140?147.von Ahn, L. (2006).
Games with a purpose.
Computer 39(6), 92?94.von Ahn, L., R. Liu, and M. Blum (2006).
Peekaboom: a game for locating objects inimages.
In Proceedings of CHI ?06, pp.
55?64.
