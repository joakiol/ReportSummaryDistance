Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 28?36,Beijing, August 2010Expanding textual entailment corpora from Wikipedia using co-trainingFabio Massimo ZanzottoUniversity of Rome ?Tor Vergata?zanzotto@info.uniroma2.itMarco PennacchiottiYahoo!
Labpennac@yahoo-inc.comAbstractIn this paper we propose a novel methodto automatically extract large textual en-tailment datasets homogeneous to existingones.
The key idea is the combination oftwo intuitions: (1) the use of Wikipediato extract a large set of textual entail-ment pairs; (2) the application of semi-supervised machine learning methods tomake the extracted dataset homogeneousto the existing ones.
We report empiricalevidence that our method successfully ex-pands existing textual entailment corpora.1 IntroductionDespite the growing success of the RecognizingTextual Entailment (RTE) challenges (Dagan etal., 2006; Bar-Haim et al, 2006; Giampiccolo etal., 2007), the accuracy of most textual entailmentrecognition systems are still below 60%.
An in-tuitive way to improve performance is to providesystems with larger annotated datasets.
This is es-pecially true for machine learning systems, wherethe size of the training corpus is an important fac-tor.
As a consequence, several attempts have beenmade to train systems using larger datasets ob-tained by merging RTE corpora of different chal-lenges.
Unfortunately, experimental results showa significant decrease in accuracy (de Marneffe etal., 2006).
There are two major reasons for thiscounter-intuitive result:Homogeneity.
As indicated by many studies (e.g.
(Siefkes, 2008)), homogeneity of the training cor-pus is an important factor for the applicability ofsupervised machine learning models, since exam-ples with similar properties often imply more ef-fective models.
Unfortunately, the corpora of thefour RTE challenges are not homogenous.
Indeed,they model different properties of the textual en-tailment phenomenon, as they have been createdusing slightly (but significantly) different method-ologies.
For example, part of the RTE-1 dataset(Dagan et al, 2006) was created using compara-ble documents, where positive entailments have alexical overlap higher than negative ones (Nichol-son et al, 2006; Dagan et al, 2006).
Comparabledocuments have not been used as a source of laterRTE corpora, making RTE-1 odd with respect toother datasets.Corpus size.
RTE corpora are relatively smallin size (typically 800 pairs).
The increase insize obtained by merging corpora from differentchallenges is not a viable solution.
Much largerdatasets, of one or more order of magnitude, areneeded to capture the complex properties charac-terizing entailment.A key issue for the future development of RTEis then the creation of datasets fulfilling two prop-erties: (1) large size; (2) homogeneity wrt.
ex-isting RTE corpora.
The task of creating largedatasets is unfeasible for human annotators.
Col-laborative annotation environments such as theAmazon Mechanical Turk1 can help to annotatepairs of sentences in positive or negative entail-ment (Zaenen, submitted; Snow et al, 2008).
Yet,these environments can hardly solve the problemof finding relevant pairs of sentences.
Completelyautomatic processes of dataset creation have beenproposed (Burger and Ferro, 2005; Hickl et al,2006).
Unfortunately, these datasets are not ho-mogeneous wrt.
to the RTE datasets, as they are1http://mturk.com28created using different methodologies.
In this pa-per we propose a novel method to automaticallyextract entailment datasets which are guaranteedto be large and homogeneous to RTE ones.
Thekey idea is the combination of two factors: (1) theuse of Wikipedia as source of a large set of tex-tual entailment pairs; (2) the application of semi-supervised machine learning methods, namely co-training, to make corpora homogeneous to RTE.The paper is organized as follows.
In Section 2we report on previous attempts in automaticallycreating RTE corpora.
In Section 3 we outline im-portant properties that these corpora should have,and introduce our methodology to extract an RTEcorpus from Wikipedia (the WIKI corpus), con-forming to these properties.
In Section 4 we de-scribe how co-training techniques can be lever-aged to make the WIKI corpus homogeneous toexisting RTE corpora.
In Section 5 we report em-pirical evidence that the combination of the WIKIcorpus and co-training is successful.
Finally, inSection 6 we draw final conclusions and outlinefuture work.2 Related WorkThe first attempt to automatically create largeRTE corpora was proposed by Burger andFerro (Burger and Ferro, 2005), with the MITREcorpus, a corpus of positive entailment examplesextracted from the XIE section of the Gigawordnews collection (Graff, 2003).
The idea of the ap-proach is that the headline and the first paragraphof a news article should be (near-)paraphrase.
Au-thors then collect paragraph-headline pairs as Text(T ) - Hypothesis (H) examples, where the head-lines plays the role of H .
The final corpus con-sists of 100,000 pairs, with an estimated accuracyof 70% ?
i.e.
two annotators checked a sampleof about 500 pairs, and verified that 30% of thesewere either false entailments or noisy pairs.
Themajor limitation of the Burger and Ferro (Burgerand Ferro, 2005)?s approach is that the final cor-pus consist only of positive examples.
Becauseof this imbalance, the corpus cannot be positivelyused by RTE learning systems.Hickl et al (2006) propose a solution to theproblem, providing a methodology to extract bothpositive and negative pairs (the LCC corpus).
Apositive corpus consisting of 101,000 pairs is ex-tracted similarly to (Burger and Ferro, 2005).
Cor-pus accuracy is estimated on a sample of 2,500 ex-amples, achieving 92% (i.e.
almost all examplesare positives), 22 points higher than Burger andFerro.
A negative corpus of 119,000 is extractedeither: (1) selecting sequential sentences includ-ing mentions of a same named entity (98.000pairs); (2) selecting pairs of sentences connectedby words such as even though, although, other-wise, but (21,000 pairs).
Estimated accuracy forthe two techniques is respectively 97% and 94%.Hickl and colleagues show that expanding theRTE-2 training set with the LCC corpus (the ex-pansion factor is 125), their RTE system im-proves 10% accuracy.
This suggests that by ex-panding with a large and balanced corpus, en-tailment recognition performance drastically im-proves.
This intuition is later contradicted in asecond experiment by Hickl and Bensley (2007).Authors use the LCC corpus with the RTE-3 train-ing set to train a new RTE system, showing an im-provement in accuracy of less than 1% wrt.
theRTE-3 training alone.Overall, evidence suggests that automatic ex-pansion of the RTE corpora do not always lead toperformance improvement.
This highly dependson how balanced the corpus is, on the RTE systemadopted, and on the specific RTE dataset that isexpanded.3 Extracting the WIKI corpusIn this section we outline some of the propertiesthat a reliable corpus for RTE should have (Sec-tion 3.1), and show that a corpus extracted fromWikipedia conforms to these properties (Sec-tion 3.2).3.1 Good practices in building RTE corporaPrevious work in Section 2 and the vast literatureon RTE suggest that a ?reliable?
corpus for RTEshould have, among others, the following proper-ties:(1) Not artificial.
Textual entailment is a complexphenomenon which encompasses different lin-guistic levels.
Entailment types range from verysimple polarity mismatches and syntactic alterna-tions, to very complex semantic and knowledge-29S?1 In this regard, some have charged the New World Translation Committee with being inconsistent.S?2 In this regard, some have charged the New World Translation Committee with not be consistent.S?
?1 The ?Stockholm Network?
is Europe?s only dedicated service organisation for market-oriented think tanks andthinkers.S?
?2 The ?Stockholm Network?
is, according to its own site, Europe?s only dedicated service organisation for market-oriented think tanks and thinkers.Figure 1: Sentence pairs from the Wikipedia revision corpusbased inferences.
These different types of en-tailments are naturally distributed in texts, suchas news and every day conversations.
A reliableRTE corpus should preserve this important prop-erty, i.e.
it should be rich in entailment typeswhose distribution in the corpus is similar to thatin real texts; and should not include unrepresenta-tive hand-crafted prototypical examples.
(2) Balanced and consistent.
A reliable corpusshould be balanced, i.e.
composed by an equal orcomparable number of positive and negative ex-amples.
This is particularly critical for RTE sys-tems based on machine learning: highly imbal-anced class distributions often result in poor learn-ing performance (Japkowicz and Stephen, 2002;Kubat and Matwin, 1997).
Also, the positive andnegative subsets of the corpus should be consis-tent, i.e.
created using the same methodology.
Ifthis property is not preserved, the risk is a learningsystem building a model which separates positiveand negatives according to the properties charac-terizing the two methodologies, instead of thoseof the entailment phenomenon.
(3) Not biased on lexical overlap.
A major criti-cism on the RTE-1 dataset was that it containedtoo many positive examples with high lexicaloverlap wrt.
negative examples (Nicholson et al,2006).
Glickman et al (2005) show that an RTEsystem using word overlap to decide entailment,surprisingly achieves an accuracy of 0.57 on RTE-1 test set.
These performances are comparable tothose obtained on the same dataset by more so-phisticated and principled systems.
Learning fromthis experience, a good corpus for RTE shouldavoid imbalances on lexical overlap.
(4) Homogeneous to existing RTE corpora.Corpus homogeneity is a key property for any ma-chine learning approach (Siefkes, 2008).
A newcorpus for RTE should then model the same orsimilar entailments types of the reliable existingones (e.g., those of the RTE challenges).
If this isnot the case, RTE system will be unable to learna coherent model, thus resulting in a decrease inperformance.The MITRE corpus satisfies property (1), butdoes not (2) and (3), as it is highly imbalanced(it contains mostly positive examples), and isfairly biased on lexical overlap, as most examplesof headline-paragraph pairs have many words incommon.
The LCC corpus suffers the problem ofinconsistency, as positive and negative examplesare derived with radically different methodolo-gies.
Both the MITRE and the LCC corpora aredifficult to merge with the RTE challenge datasets,as they are not homogeneous ?
i.e.
they have beenbuilt using very different methodologies.3.2 Extracting the corpus from WikipediarevisionsOur main intuition in using Wikipedia to buildan entailment corpus is that the wiki frameworkshould provide a natural source of non-artificialexamples of true and false entailments, throughits revision system.
Wikipedia is an open ency-clopedia, where every person can behave as anauthor, inserting new entries or modifying exist-ing ones.
We call original entry S1 a piece oftext in Wikipedia before it is modified by an au-thor, and revision S2 the modified text.
The pri-mary concern of Wikipedia authors is to reshapea document according to their intent, by addingor replacing pieces of text.
Excluding vandalism,there are several reasons for making a revision:missing information, misspelling, syntactic errors,and, more importantly, disagreement on the con-tent.
For example, in Fig.
1, S?
?1 is revised to S?
?2 ,as the author disagrees on the content of S?
?1 .Our hypothesis is that (S1, S2) pairs representgood candidates of both true and false entailmentpairs (T,H), as they represent semantically close30pieces of texts.
Also, Wikipedia pairs conform tothe properties listed in the previous section, as de-scribed in the following.
(S1, S2) pairs are not artificial, as we extractthem from pieces of original texts, without anymodification or post-processing.
Also, pairs arerich of different entailment types, whose distribu-tion is a reliable sample of language in use2.
Asshown later in the paper, a collection of (S1, S2)pairs is likely balanced on positive and negativeexamples, as authors either contradict the contentof the original entry (false entailment) or add newinformation to the existing content (true entail-ment).
Positive and negative pairs are guaranteedto be consistent, as they are drawn from the sameWikipedia source.
Finally, the Wikipedia is notbiased in lexical overlap: A sentence S2 replac-ing S1, usually changes only a few words.
Yet,the meaning of S2 may or may not change wrt.the meaning of S1 ?
i.e.
the lexical overlap ofthe two sentences is very high, but the entailmentrelation between S1 and S2 may be either posi-tive or negative.
For example, in Fig.
1 both pairshave high overlap, but the first is a positive en-tailment (S?1 ?
S?2), while the second is negative(S?
?1 ?
S?
?2 ).An additional interesting property of Wikipediarevisions is that the transition from S1 to S2 iscommented by the author.
The comment is apiece of text where authors explain and motivatethe change (e.g.
?general cleanup of spelling andgrammar?, ?revision: Eysenck died in 1997!!?
).Even if very small, the comment can be used todetermine if S1 and S2 are in entailment or not.In the following section we show how we lever-age comments to make the WIKI corpus homoge-neous to those of the RTE challenges.4 Expanding the RTE corpus with WIKIusing co-trainingUnlike the LCC corpus where negative and posi-tive examples are clearly separated, the WIKI cor-pus mixes the two sets ?
i.e.
it is unlabelled.
Inorder to exploit the WIKI corpus in the RTE task,one should either manually annotate the corpus,2It has been shown that web documents (as Wikipedia)are reliable samples of language (Keller and Lapata, 2003).CO-TRAINING ALGORITHM(L,U ,k)returns h1,h2,L1,L2set L1 = L2 = Lwhile stopping condition is not met?
learn h1 on F1 from L1, and learn h2 on F1 fromL2,?
classify U with h1 obtaining U1, and classify Uwith h2 obtaining U2?
select and remove k-best classified examples u1and u2 from respectively U1 and U2?
add u1 to L2 and u2 to L1Figure 2: General co-training algorithmor find an alternative strategy to leverage the cor-pus even if unlabelled.
As manual annotation isunfeasible, we choose the second solution.
Thegoal is then to expand a labelled RTE challengetraining set with the unlabelled WIKI, so that theperformance of an RTE system can increase overan RTE test set.In the literature, several techniques have beenproposed to use unlabelled data to expand atraining labelled corpus, e.g.
Expectation-Maximization (Dempster et al, 1977).
We hereapply the co-training technique, first proposed by(Blum and Mitchell, 1998) and then successfullyleveraged and analyzed in different settings (Ab-ney, 2002).
Co-training can be applied when theunlabelled dataset alows two independent viewson its instances (applicability condition).In this section, we first provide a short descrip-tion of the co-training algorithm (Section 4.1).
Wethen investigate if different RTE corpora conformto the applicability condition (Section 4.2).
Fi-nally, we show that our WIKI corpus conforms tothe condition, and then apply co-training by creat-ing two independent views (Section 4.3).4.1 Co-trainingThe co-training algorithm uses unlabelled data toincrease classification performance, and to indi-rectly increasing the size of labelled corpora.
Thealgorithm can be applied only under a specific ap-plicability condition: corpus?
instances must havetwo independent views, i.e.
they can be modeledby two independent feature sets.We here adopt a slightly modified version of the31cotraining algorithm, as described in Fig.2.
Underthe applicability condition, instances are modeledon a feature space F = F1 ?
F2 ?
C , where F1and F2 are the two independent views and C isthe set of the target classes (in our case, true andfalse entailment).
The algorithm starts with an ini-tial set of training labelled examples L and a setof unlabelled examples U .
The set L is copiedin two sets L1 and L2, used to train two differ-ent classifiers h1 and h2, respectively using viewsF1 and F2.
The two classifiers are used to clas-sify the unlabelled set U , obtaining two differentclassifications, U1 and U2.
Then comes the co-training step: the k-best classified instances in U1are added to L2 and feed the learning of a newclassifier h2 on the feature space F2.
Similarly, thek-best instances in U2 are added to L1 and train anew classifier h1 on F1.The procedure repeats until a stopping condi-tion is met.
This can be either a fixed number ofadded unlabelled examples (Blum and Mitchell,1998), the performance drop on a control set oflabelled instances, or a filter on the disagreementof h1 and h2 in classifying U (Collins and Singer,1999).
The final outcome of co-training is the newset of labelled examples L1?L2 and the two clas-sifier h1 and h2, obtained from the last iteration.4.2 Applicability condition on RTE corporaIn order to leverage co-training for homoge-neously expanding an RTE corpus, it is neces-sary to have a large unlabelled corpus which sat-isfies the applicability condition.
Unfortunately,existing methodologies cannot guarantee the con-dition.For example, the corpora from which thedatasets of the RTE challenges were derived, werecreated from the output of applications perform-ing specific tasks (e.g., Question&Answering, In-formation Extraction, Machine Translation, etc.
).These corpora do not offer the possibility to cre-ate two completely independent views.
Indeed,each extracted pair is composed only by the tex-tual fragments of T and H , i.e.
the only infor-mation available are the two pieces of texts, fromwhich it is difficult to extract completely indepen-dent sets of features, as linguistic features tend tobe dependent.The MITRE corpus is extracted using two sub-sequent sentences, the title and the first paragraph.The LCC negative corpus is extracted using twocorrelated sentences or subsentences.
Also inthese two cases, it is very hard to find a view that isindependent from the space of the sentence pairs.None of the existing RTE corpora can then beused for co-training.
In the next section we showthat this is not the case for the WIKI corpus.4.3 Creating independent views on the WIKIcorpusThe WIKI corpus is naturally suited for co-training, as for each (S1, S2) pair, it is possibleto clearly define two independent views:?
content-pair view: a set of features modelingthe actual textual content of S1 and S2.
Thisview is typically available also in any otherRTE corpus.?
comment view: a set of features regarding therevision comment inserted by an author.
Thisview represents ?external?
information (wrt.to the text fragments) which are peculiar ofthe WIKI corpus.These two views are most likely independent.Indeed, the content-pair view deals with the con-tent of the Wikipedia revision, while the com-ment view describes the reason why a revisionhas been made.
This setting is very similar tothe original one proposed for co-training by Blumand Mitchell (Blum and Mitchell, 1998), wherethe target problem was the classification of webpages, and the two independent views on a pagewere (1) its content and (2) its hyperlinks.In the rest of this section we describe the featurespaces we adopt for the two independent views.4.3.1 Content-pair viewThe content-pair view is the classical view usedin RTE.
The original entry S1 represents the TextT , while the revision S2 is the Hypothesis H .Any feature space of those reported in the textualentailment literature could be applied.
We hereadopt the space that represents first-order syntac-tic rewrite rules (FOSR), as described in (Zan-zotto and Moschitti, 2006).
In this feature space,each feature represents a syntactic first-order or32grounded rewrite rule.
For example, the rule:?
= l ?
r=SNP X VPVBPboughtNP Y?SNP Y VPVBPownsNP Xis represented by the feature < l, r >.
A (T,H)pair activates a feature if it unifies with the relatedrule.
A detailed discussion of the FOSR featurespace is given in (Zanzotto et al, 2009) and ef-ficient algorithms for the computation of the re-lated kernel functions can be found in (Moschittiand Zanzotto, 2007; Zanzotto and Dell?Arciprete,2009).4.4 Comment viewA review comment is typically a textual fragmentdescribing the reason why an author has decidedto make a revision.
In most cases the comment isnot a well-formed sentence, as authors tend to useinformal slang expressions and abbreviations (e.g.
?details: Trelew Massacre; cat: Dirty War, copy-edit?, ?removed a POV vandalism by Spylab?,?dab ba:clean up using Project:AWB?).
In thesecases, where syntactic analysis would mostly fail,it is advisable to use simpler surface approachesto build the feature space.
We then use a stan-dard bag-of-word space, combined with a bag-of-2-grams space.
For the first space we keep onlymeaningful content words, by using a standardstop-list including articles, prepositions, and veryfrequent words such as be and have.
The sec-ond space should help in capturing small text frag-ments containing functional words: we then keepall words without using any stop-list.5 ExperimentsThe goals of our experiments are the following:(1) check the quality of the WIKI corpus, i.e.
ifpositive and negative examples well represent theentailment phenomenon; (2) check if WIKI con-tains examples similar to those of the RTE chal-lenges, i.e.
if the corpus is homogeneous to RTE;(3) check if the WIKI corpus improves classifica-tion performance when used to expand the RTEdatasets using the co-training technique describedin Section 4.5.1 Experimental SetupIn order to check the above claims, we needto experiment with both manually labelled andunlabelled corpora.
As unlabelled corpora weadopt:wiki unlabelled: An unlabelled WIKI corpus ofabout 3,000 examples.
The corpus has been builtby downloading 40,000 Wikipedia pages dealingwith 800 entries about politics, scientific theories,and religion issues.
We extracted original entriesand revisions from the XML and wiki code,collecting an overall corpus of 20,000 (S1, S2)pairs.
We then randomly selected the final 3,000pairs.news: A corpus of 1,600 examples obtained usingthe methods adopted for the LCC corpus, bothfor negative and positive examples (Hickl et al,2006).3 We randomly divided the corpus in twoparts: 800 training and 800 testing examples.Each set contains an equal number of 400 positiveand negative pairs.As labelled corpora we use:RTE-1,RTE-2, and RTE-3: The corpora fromthe first three RTE challenges (Dagan et al, 2006;Bar-Haim et al, 2006; Giampiccolo et al, 2007).We use the standard split between training andtesting.wiki: A manually annotated corpus of 2,000examples from the WIKI corpus.
Pairs have beenannotated considering the original entry as theH and the revision as T .
Noisy pairs containingvandalism or grammatical errors were removed(these accounts for about 19% of the examples).In all, the annotation produced 945 positiveexamples (strict entailments and paraphrases) and669 negative examples (reverse strict entailmentsand contradictions).
The annotation was carriedout by two experienced researchers, each oneannotating half of the corpus.
Annotation guide-lines follow those used for the RTE challenges.43For negative examples, we adopt the headline - first para-graph extraction methodology.4Annotators were initially trained on a small developmentcorpus of 200 pairs.
The inter-annotator agreement on thisset, computed using the Kappa-statistics (Siegel and Castel-lan, 1988), was 0.60 corresponding to substantial agreement,33The corpus has been randomly split in threeequally numerous parts: development, training,and testing.
We kept aside the development todesign the features, while we used training andtesting for the experiments.We use the Charniak Parser (Charniak, 2000)for parsing sentences, and SVM-light (Joachims,1999) extended with the syntactic first-order rulekernels described in (Zanzotto and Moschitti,2006; Moschitti and Zanzotto, 2007) for creatingthe FOSR feature space.5.2 Experimental ResultsThe first experiment aims at checking the qual-ity of the WIKI corpus, by comparing the perfor-mance obtained by a standard RTE system overthe corpus in exam with those obtained over anyRTE challenge corpus.
The hypothesis is that ifperformance is comparable, then the corpus inexam has the same complexity (and quality) asthe RTE challenge corpora.
We then indepen-dently experiment with the wiki and the newscorpora with the training-test splits reported inSection 5.1.
As RTE system we adopt an SVMmodel learnt on the FOSR feature space describedin Section 4.3.1.The accuracies of the system on the wikiand news corpora are respectively 70.73% and94.87%.
The performance of the system on thewiki corpus are in line with those obtained overthe RTE-2 dataset (60.62%).
This suggests thatthe WIKI corpus is at least as complex as the RTEcorpora (i.e.
positive and negatives are not triv-ially separable).
On the contrary, the news cor-pus is much easier to separate.
Pilot experimentsshow that increasing the size of the news corpus,accuracy reaches nearly 100%.
This indicates thatpositive and negative examples in the news cor-pus are extremely different.
Indeed, as mentionedin Section 3.1, news is not consistent ?
i.e.
theextraction methods for the positives and the neg-atives are so different that the examples can beeasily recognized using evidence not representa-tive of the entailment phenomenon (e.g.
for nega-tive examples, the lexical overlap is extremely lowwrt.
positives).in line with the RTE challenge annotation efforts.Training Corpus AccuracyRTE-2 60.62RTE-1 51.25RTE-3 57.25wiki 56.00news 53.25RTE-2+RTE-1 58.5RTE-2+RTE-3 59.62RTE-2+news 56.75RTE-2+wiki 59.25RTE-1+wiki 53.37RTE-3+wiki 59.00Table 1: Accuracy of different training corpora over RTE-2test.In a second experiment we aim at checking ifWIKI is homogeneous to the RTE challenge cor-pora ?
i.e.
if it contains (T,H) pairs similar tothose of the RTE corpora.
If this holds, we wouldexpect the performance of the RTE system to im-prove (or at least not decrease) when expanding agiven RTE challenge corpus with WIKI.
de Marn-effe et al (2006) already showed in their experi-ment that it is extremely difficult to obtain betterperformance by simply expanding an RTE chal-lenge training corpus with corpora of other chal-lenges, since different corpora are usually not ho-mogeneous.We here repeat a similar experiment: we ex-periment with different combinations of trainingsets, over the same test set (namely, RTE-2 test).Results are reported in Table 1.
The higher per-formance is the one of the system when trained onRTE-2 training set (second row) ?
i.e.
a corpuscompletely homogeneous to RTE-2 would pro-duce the same performance as RTE-2 training.As expected, the models learnt on RTE-1 andRTE-3 perform worse (third and fourth rows): inparticular, RTE-1 seems extremely different fromRTE-2, as results show.
The wiki corpus is moresimilar to RTE-2 than the news corpus, i.e.
per-formance are higher.
Yet, it is quite surprising thatthe news corpus yields to a performance drop asin (Hickl et al, 2006) it shows a high performanceincrease.The expansion of RTE-2 with the above cor-pora (seventh-tenth rows) lead to a drop in per-formance, suggesting that none of the corporais completely homogeneous to RTE-2.
Yet, theperformance drop of the wiki corpus (RTE-2 +346060.56161.5620 20 40 60 80 100accuracyunlabelled examplesRTE2RTE3Figure 3: Co-training accuracy curve on the two corpora.wiki) is comparable to the performance drop ob-tained using the other two RTE corpora (RTE-2 +RTE-1 and RTE-2 + RTE-3).
This indicates thatwiki is more homogeneous to RTE than news?
i.e.
it contains (T,H) pairs that are similar tothe RTE examples.
Interestingly, wiki combinedwith other RTE corpora (RTE-1 + wiki and RTE-3 + wiki) increases performance wrt.
the modelsobtained with RTE-1 and RTE-3 alone (last tworows).In a final experiment, we check if the WIKIcorpus improves the performance when combinedwith the RTE-2 training in a co-training setting, asdescribed in Section 4.
This would confirm thatWIKI is homogeneous to the RTE-2 corpus, andcould then be successfully adopted in future RTEcompetitions.
As test sets, we experiment bothwith RTE-2 and RTE-3 test.
In the co-training,we use the RTE-2 training set as initial set L, andwiki unlabelled as the unlabelled set U .5Figure 3 reports the accuracy curves obtainedby the classifier h1 learnt on the content view, ateach co-training iteration, both on the RTE-2 andRTE-3 test sets.
As the comment view is not avail-able in the RTE sets, the comment-view classi-fier become active only after the first 10 examplesare fed as training from the content view classi-5Note that only wiki unlabelled allows both views de-scribed in Section 4.3.fier.
As expected, performance increase for somesteps and then become stable for RTE-3 and de-crease for RTE-2.
This is the only case in whichwe verified an increase in performance using cor-pora other than the official ones from RTE chal-lenges.
This result suggests that the WIKI corpuscan successfully contribute to learn better textualentailment models for RTE.6 ConclusionsIn this paper we proposed a method for expandingexisting textual entailment corpora that leveragesWikipedia.
The method is extremely promisingas it allows building corpora homogeneous to ex-isting ones.
The model we have presented is notstrictly related to the RTE corpora.
This methodcan then be used to expand corpora such as theFracas test-suite (Cooper et al, 1996) which ismore oriented to specific semantic phenomena.Even if the performance increase of the com-pletely unsupervised cotraining method is not ex-tremely high, this model can be used to semi-automatically expanding corpora by using activelearning techniques (Cohn et al, 1996).
Theinitial increase of performances is an interestingstarting point.In the future, we aim at releasing the annotatedportion of the WIKI corpus to the community; wewill also carry out further experiments and refinethe feature spaces.
Finally, as Wikipedia is a mul-tilingual resource, we will use the WIKI method-ology to semi-automatically build RTE corporafor other languages.ReferencesSteven Abney.
2002.
Bootstrapping.
In Proceedings of40th Annual Meeting of the Association for Computa-tional Linguistics, pages 360?367, Philadelphia, Pennsyl-vania, USA, July.
Association for Computational Linguis-tics.Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, DaniloGiampiccolo, and Idan Magnini, Bernardo Szpektor.2006.
The second pascal recognising textual entail-ment challenge.
In Proceedings of the Second PASCALChallenges Workshop on Recognising Textual Entailment,Venice, Italy.Avrim Blum and Tom Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In COLT: Proceed-ings of the Conference on Computational Learning The-ory.
Morgan Kaufmann.35John Burger and Lisa Ferro.
2005.
Generating an entailmentcorpus from news headlines.
In Proceedings of the ACLWorkshop on Empirical Modeling of Semantic Equiva-lence and Entailment, pages 49?54, Ann Arbor, Michi-gan, June.
Association for Computational Linguistics.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proc.
of the 1st NAACL, pages 132?139, Seat-tle, Washington.David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan.1996.
Active learning with statistical models.
Journal ofArtificial Intelligence Research, 4:129?145.Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
In In Proceedingsof the Joint SIGDAT Conference on Empirical Methods inNatural Language Processing and Very Large Corpora,pages 100?110.Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox, Jo-han Van Genabith, Jan Jaspars, Hans Kamp, David Mil-ward, Manfred Pinkal, Massimo Poesio, and Steve Pul-man.
1996.
Using the framework.
Technical Report LRE62-051 D-16, The FraCaS Consortium.
Technical report.Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006.The pascal recognising textual entailment challenge.
InQuionero-Candela et al, editor, LNAI 3944: MLCW 2005,pages 177?190, Milan, Italy.
Springer-Verlag.Marie-Catherine de Marneffe, Bill MacCartney, TrondGrenager, Daniel Cer, Anna Rafferty, and ChristopherD.
Manning.
2006.
Learning to distinguish valid tex-tual entailments.
In Proceedings of the Second PASCALChallenges Workshop on Recognising Textual Entailment,Venice, Italy.A.P.
Dempster, N.M. Laird, and D.B.
Rubin.
1977.
Max-imum likelihood from incomplete data via the em algo-rithm.
Journal of the Royal Statistical Society, Series B,39(1):1?38.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and BillDolan.
2007.
The third pascal recognizing textual en-tailment challenge.
In Proceedings of the ACL-PASCALWorkshop on Textual Entailment and Paraphrasing, pages1?9, Prague, June.
Association for Computational Lin-guistics.Oren Glickman, Ido Dagan, and Moshe Koppel.
2005.
Webbased probabilistic textual entailment.
In Proceedings ofthe 1st Pascal Challenge Workshop, Southampton, UK.David Graff.
2003.
English gigaword.Andrew Hickl and Jeremy Bensley.
2007.
A discoursecommitment-based framework for recognizing textual en-tailment.
In Proceedings of the ACL-PASCAL Workshopon Textual Entailment and Paraphrasing, pages 171?176,Prague, June.
ACL.Andrew Hickl, John Williams, Jeremy Bensley, KirkRoberts, Bryan Rink, and Ying Shi.
2006.
Recognizingtextual entailment with LCCs GROUNDHOG system.
InProceedings of the 2nd PASCAL Challenge Workshop onRTE, Venice, Italy.N.
Japkowicz and S. Stephen.
2002.
The class imbalanceproblem: A systematic study.
Intelligent Data Analysis,6(5).Thorsten Joachims.
1999.
Making large-scale svm learningpractical.
In B. Schlkopf, C. Burges, and A. Smola, edi-tors, Advances in Kernel Methods-Support Vector Learn-ing.
MIT Press.Frank Keller and Mirella Lapata.
2003.
Using the web toobtain frequencies for unseen bigrams.
ComputationalLinguistics, 29(3), September.M.
Kubat and S. Matwin.
1997.
Addressing the curse of in-balanced data sets: One-side sampleing.
In Proceedingsof the 14th International Conference on Machine Learn-ing, pages 179?186.
Morgan Kaufmann.Alessandro Moschitti and Fabio Massimo Zanzotto.
2007.Fast and effective kernels for relational learning fromtexts.
In Proceedings of the International Conference ofMachine Learning (ICML), Corvallis, Oregon.Jeremy Nicholson, Nicola Stokes, and Timothy Baldwin.2006.
Detecting entailment using an extended imple-mentation of the basic elements overlap metric.
In Pro-ceedings of the Second PASCAL Challenges Workshop onRecognising Textual Entailment, Venice, Italy.Christian Siefkes.
2008.
An Incrementally Trainable Statis-tical Approach to Information Extraction.
VDM Verlag,Saarbrucken, Germany.S.
Siegel and Jr. N. J. Castellan.
1988.
NonparametricStatistics for the Behavioral Sciences.
McGraw-Hill.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Ng.
2008.
Cheap and fast ?
but is it good?
eval-uating non-expert annotations for natural language tasks.In Proceedings of the 2008 Conference on EmNLP, pages254?263, Honolulu, Hawaii.
ACL.Annie Zaenen.
submitted.
Do give a penny for theirthoughts.
Journal of Natural Language Engineering.Fabio Massimo Zanzotto and Lorenzo Dell?Arciprete.
2009.Efficient kernels for sentence pair classification.
In Con-ference on Empirical Methods on Natural Language Pro-cessing, pages 91?100, 6-7 August.Fabio Massimo Zanzotto and Alessandro Moschitti.
2006.Automatic learning of textual entailments with cross-pairsimilarities.
In Proceedings of the 21st Coling and 44thACL, pages 401?408, Sydney, Australia, July.Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessan-dro Moschitti.
2009.
A machine learning approach totextual entailment recognition.
NATURAL LANGUAGEENGINEERING, 15-04:551?582.
Accepted for pubblica-tion.36
