The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 73?79,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsUtilizing Cumulative Logit Models and Human Computation on AutomatedSpeech AssessmentLei ChenEducational Testing Service (ETS)Princeton, NJ, 08541lchen@ets.orgAbstractWe report two new approaches for buildingscoring models used by automated speechscoring systems.
First, we introduce the Cu-mulative Logit Model (CLM), which has beenwidely used in modeling categorical outcomesin statistics.
On a large set of responsesto an English proficiency test, we systemati-cally compare the CLM with two other scor-ing models that have been widely used, i.e.,linear regression and decision trees.
Our ex-periments suggest that the CLM has advan-tages in its scoring performance and its robust-ness to limited-sized training data.
Second, wepropose a novel way to utilize human ratingprocesses in automated speech scoring.
Ap-plying accurate human ratings on a small setof responses can improve the whole scoringsystem?s performance while meeting cost andscore-reporting time requirements.
We findthat the scoring difficulty of each speech re-sponse, which could be modeled by the degreeto which it challenged human raters, couldprovide a way to select an optimal set of re-sponses for the application of human scor-ing.
In a simulation, we show that focusingon challenging responses can achieve a largerscoring performance improvement than sim-ply applying human scoring on the same num-ber of randomly selected responses.1 IntroductionAutomated assessment is a process by which com-puter algorithms are used to score test-taker inputs,which could be essays, short-text descriptions, read-aloud sentences, or spontaneous speech responsesto open-end questions.
Until recently, human scor-ing has been predominantly used for scoring thesetypes of inputs.
Several limitations of the humanscoring process have been identified in previous re-search (Bennett, 2006).
First, the human scoringprocess is influenced by many hidden factors, suchas human raters?
mood and fatigue conditions.
Inaddition, human raters may not strictly follow therubrics designed to guide the scoring process in theirpractical scoring sessions.
Furthermore, human rat-ing is also an expensive and slow process, especiallyfor large-scale tests.There has been an increasing number of studiesconcerning the use of speech processing and natu-ral language processing (NLP) technologies to auto-matically score spoken responses (Eskenazi, 2009).In these machine scoring systems, a set of featuresrelated to multiple aspects of human speaking capa-bilities, e.g., fluency, pronunciation, intonation, vo-cabulary usage, grammatical accuracy, and content,is extracted automatically.
Then, statistical mod-els, such as the widely used linear regression mod-els, classification and regression trees (CART), aretrained based on human ratings and these features.For new responses, the trained statistical models areapplied to predict machine scores.The performance of current automated speechscoring systems, especially for spontaneous speechresponses, still lags markedly behind the perfor-mance of human scoring.
To improve the perfor-mance of automated speech scoring, an increas-ing number of research studies have been under-taken (Jang, 2009; Chen and Zechner, 2011; Chenand Yoon, 2011).
However, these studies havemostly focused on exploring additional speech fea-tures, not on building alternative scoring models.Hence, in this paper, we will report on two new linesof research focusing on the scoring model part of au-73tomated speech scoring systems.
In particular, wewill introduce the Cumulative Logit Model (CLM),which is not widely used in NLP, and compare it sys-tematically with other widely-used modeling meth-ods.
In addition, we will propose a hybrid scoringsystem inspired by the recent trend of involving hu-man computation in machine learning tasks (Quinnet al, 2010), which consists of both human scoringand machine scoring to achieve a balance of scoringaccuracy, speed, and cost.The remainder of the paper is organized as fol-lows: Section 2 reviews the previous research ef-forts; Section 3 describes both the test from whichour experimental data were collected and the auto-mated speech scoring system; Section 4 introducesthe Cumulative Logit Model (CLM) and reports asystematic comparison with two other widely usedmodeling approaches; Section 5 proposes using bothhuman scoring and machine scoring to achieve atrade-off between scoring accuracy, speed, and cost,and shows a simulation.
Finally, Section 6 con-cludes the paper and describes our plans for futureresearch.2 Related WorkIn the language testing field, it is critical how easily ascore can be interpreted by test takers and stakehold-ers.
Therefore, ?white-box?
machine learning meth-ods (mostly from the field of statistics) are favoredover black-box systems (e.g., neural networks) andwidely used in automated scoring systems.
For ex-ample, SRI?s EduSpeak system (Franco et al, 2010)used a decision-tree model to automatically producea speaking score from a set of discrete score la-bels.
Linear Discrimination Analysis (LDA) hasbeen used in pronunciation evaluation (Hacker etal., 2005).
In a speech scoring system described byZechner et al (2009), a linear regression (LR) modelwas used to predict human scores.Applying linear regression, which is designed forcontinuous outcomes, on ordinal outcomes, such asdiscrete human rated scores, is questioned by somestatisticians.A linear regression model does not ex-ploit the fact that the scores can assumeonly a limited number of values and hencemay provide inefficient approximations toessay scores obtained by raters.
Conse-quently, estimation based on a model thatassumes that the response is categoricalwill be more accurate than linear regres-sion.
A cumulative logit model, some-times called a proportional odds model, isone such model (Haberman and Sinharay,2010).The CLM was compared systematically with anordinary linear regression model in terms of au-tomated essay scoring (Haberman and Sinharay,2010).
Based on their experiment on a large varietyof TOEFL prompts, they suggested that the CLMshould be considered a very attractive alternative toregression analysis.In recent years, a new trend of research in the ma-chine learning field is to use human computation toprovide additional help, especially on difficult tasks.For example, after the ESP game (Von Ahn, 2006),an increasing number of human computation basedgames emerged to use a large number of human par-ticipants to solve many machine learning problems,such as human identification for image processingand sentiment annotation in natural language pro-cessing (NLP).
Quinn and Bederson (2011) reviewresearch in this area.
Furthermore, Quinn et al(2010) proposed a hybrid mechanism to integrateboth human computation and machine learning toachieve a balance between speed, cost, and quality.In this paper, we will follow the advances in thetwo directions mentioned above, including usingCML as a modeling method and obtaining comple-mentary computing by integrating machine scoringwith human scoring to further improve the scoringmodels in automated speech scoring systems.3 Data and Automated Scoring System3.1 DataAEST is a large-scale English test for assessing test-takers?
English proficiency in reading, writing, lis-tening, and speaking.
The data used in our exper-iments was collected from operational AEST tests.In each test session, test takers were required to re-spond to six speaking test questions to provide in-formation or express their opinions.Each spoken response was assigned a score in therange of 1 to 4, or 0 if the candidate either made no74attempt to answer the item or produced a few wordstotally unrelated to the topic.
Each spoken responsecould also receive a ?technical difficulty?
(TD) labelwhen technical issues may have degraded the audioquality to such degree that a fair evaluation was notpossible.
Note that in the experiments reported inthis paper, we excluded both 0 and TD responsesfrom our analyses.
The human scoring process usedthe scoring rules designed for the AEST test.
Froma large pool of certified human raters, two humanraters were randomly selected to score each responsein parallel.
If two raters?
scores had a discrepancylarger than one point, a third rater with more expe-rience in human scoring was asked to give a finalscore.
Otherwise, the final scores used were takenfrom the first human rater in each rater pair.The Pearson correlation r among human raterswas calculated as 0.64.
The second human scoreshad a correlation of 0.63 to the final scores while thefirst human scores had a correlation of 0.99.
Thisis due to the fact that only in about 2% of the cases,two human scores have a discrepancy larger than onepoint.
Table 1 describes the data size and final scoredistribution of the four score levels.N 1(%) 2(%) 3(%) 4 (%)49813 4.56 37.96 47.74 9.74Table 1: Human score distribution of the AEST datasets3.2 Automated scoring systemTo automatically score spontaneous speech, we usedthe method proposed in Chen et al (2009).
In thismethod, a speech recognizer is used to recognizenon-native speech and a forced alignment is con-ducted based on the obtained recognition hypothe-ses.
From the recognition and alignment outputs,a number of features were extracted from multi-ple aspects, such as the timing profiles, recogni-tion confidence scores, alignment likelihoods, etc.For speech recognition and forced alignment, weused a gender-independent, fully continuous Hid-den Markov Model (HMM) speech recognizer.
OurASR system was trained from about 800 hours ofnon-native speech data and its corresponding wordtranscriptions.
We extracted the following two typesof features, including (1) fluency and intonationfeatures based on the speech recognition output asdescribed in Xi et al (2008) and (2) pronuncia-tion features that indicated the quality of phonemesand phoneme durations as described in Chen et al(2009).4 A comparison of three machine learningmethods in automated speech scoringWe will briefly introduce CLM and then compareit with two other widely used scoring methods, i.e.,linear regression and CART.
In most of the relatedprevious investigations, several machine learning al-gorithms were compared using a fixed number of in-stances.
However, as shown in recent studies, suchas Rozovskaya and Roth (2011), judging an algo-rithm requires consideration of the impact of the sizeof the training data set.
Therefore, in our exper-iment, we compared three algorithms on differentsizes of training samples.Let the response?s holistic score be Y = 1, 2, ...J(J is 4 in our study on the AEST data) and let theassociated probabilities be pi1, pi2, ...piJ .
Thereforethe probability of a predicted score is not larger thanjP (Y ?
j) = pi1 + pi2 + ...+ pij (1)The logit of this probability can be estimated aslogP (Y ?
j)1?
P (Y ?
j)= ?j +K?k=1?kXk (2)where K is the number of speech features.
We cansee that a CLM contains K ?s where each ?
is asso-ciated with one feature.
In addition, for each score j,there is an intercept ?j .
The CLM is a special caseof multinomial logistic regression, which is namedMaximum Entropy (MaxEnt) model (Berger et al,1996) and is well known by NLP researchers.
InCLM, the ranking order of the labels being predictedis emphasized.
However, in MaxEnt models, thereis no assumption about the relationship of the labelsbeing predicted.For CLM, we used the Ye?s VGAM R pack-age (Yee, 2010) as our implementation.
For or-dinary linear regression and CART methods, weused corresponding implementations in the WEKAtoolkit (Hall et al, 2009), i.e., lm and J48 tree,through the RWeka package (Hornik et al, 2009)so that we could run these three algorithms inside R.75From the available speech features, we firstrun an inter-correlation analysis among these fea-tures.
Then, two feature selection approaches imple-mented in the caret R package (Kuhn, 2008) wereused to select useful features from about 80 fea-tures.
First, all feature-pairs whose inter-correlationwas higher than 0.80 were analyzed and one featurefor each pair was removed.
Next, a recursive fea-ture elimination (RFE) based on a linear regressionmodel was utilized to reduce the feature size to just20.Using a stratified sampling based on the finalscores, the whole data set was split into a training set(with 44, 830 instances) and a test set (with 4, 980instances).
Then, on a log10 scale, we tried usingincreasing number of training samples from 100 to104.5.
For each training data set size, we randomlyselected the size of training samples from the train-ing set, built the three models, and evaluated themodels on the entire test data.
For each data set size,such process was repeated 10 times.
The evaluationresult is the averaged values from these 10 iterations.We repeated the same experiment on the top 5, 10,15, and 20 features.
The evaluation metrics includewidely used measures in the field of automated scor-ing, including Pearson correlation r and quadraticweighted Kappa ?
(hereafter weighted ?)
betweenthe machine predicted scores and human final scoresin this data set.Figure 1 shows the Pearson r and weighted ?
val-ues of the three methods vs. an increasing numbersof training samples.
We find that the CLM alwayshas the highest weighted ?
value among these threemethods for each data size level.
The CART per-forms poorly, especially facing a limited number oftraining samples.
However, when the training datasize is large enough, the performance gap betweenthe CART and other regression models becomessmaller.
For two regression models, when work-ing on 20 features, both Pearson r and weighted ?values plateaued after reaching 1000 training sam-ples.
More importantly, we find that the CLM stillcan provide a quite high value of weighted ?
evenjust using 100 training samples.
This is very impor-tant for automated assessments in cases where thereare not enough pre-test responses to fully train thescoring model.
When using other feature selections(5, 10, and 15), we also observed the same trend asshown in the Figure 1.log10(dSize)corr0.380.400.420.440.460.480.50lll l l l2.5 3.0 3.5 4.0 4.5MLl CLMJ48MRlog10(dSize)kappa0.350.400.45lll l l l2.5 3.0 3.5 4.0 4.5MLl CLMJ48MRFigure 1: Weighted ?
and Pearson correlation r of LR,CART, and CLM vs. an increasing number of trainingsamples when using 20 features.5 Utilizing human computation to supportautomated speech scoringOn spontaneous speech responses, the performanceof automated scoring still lags behind human rat-ings.
For example, on the test set (4, 098 samples),among human raters both the Pearson r and theweighted ?
values are about 0.6, much higher thanthe best automated scoring results we saw in the pre-vious section (around 0.5).
There are many possi-ble reasons for such a big performance gap betweenautomated speech scoring and human scoring.
Forexample, the automated features?
lack of a measure-ment of content accuracy and relevance might pro-vide an explanation for part of the performance gap.As a result, to our knowledge, there has not been anycommercial application of automated speech scoringon high-stakes speaking tests to open-ended ques-tions.To further improve the speech scoring system?sperformance, inspired by Quinn et al (2010), we76propose to include human computation ?
humanrating of speech responses ?
in the automatedspeech scoring system.
Previously, there have beensome efforts to use human computation in auto-mated speech scoring systems.
For example, it iswell known that human scores were used to train au-tomated scoring models.
For essay scoring, an auto-mated scoring system, e-rater, has been used to val-idate the human rating process (Enright and Quin-lan, 2010).
One advantage of using both human ande-rater to score is that about 10% of human ratingrequests for double-scoring required in operationalessay scoring could be saved.
However, there hasbeen no previous work investigating the joint use ofhuman scoring and machine scoring.
By using thesetwo scoring methods together, we hope to achieve abalance among scoring accuracy, speed, and cost.From a total of N test responses, we need askhumans to score m, where m << N .
Therefore,an important question concerning the joint use ofhuman scoring and machine scoring is how to findthese m responses so that the expensive and slowhuman scoring process can provide a large perfor-mance gain.
In this paper, we will report our prelim-inary research results of focusing on the responseschallenging to machine scoring process.Since the responses used in this paper were se-lected to be double-scored responses from a verylarge pool of AEST responses, we use the ratingcondition of each doubly-scored response to pre-dict how challenging any given response is.
Forspeech responses for which two human raters gavedifferent holistic scores, we assumed that these re-sponses were not only difficult to score for humanbeings, but also for the machine learning method,which has been trained from human scores in a su-pervised learning way.
We call the responses onwhich two human raters agreed easy-case responsesand the responses on which two human raters dis-agreed hard-case ones.
Table 2 reports on the appli-cation of trained automated speech assessment sys-tems to these two types of responses.
From the en-tire testing set, human raters agreed on 3, 128 re-sponses, but disagreed on 1, 852 responses.
Fromthe training set described in the previous section,we randomly sampled 1, 000 responses to train aCLM model using those 20 features used in Sec-tion 4.
Then, the trained CLM model was evalu-ated on these two types of responses, respectively.Table 2 reports the evaluation metrics averaged on20 trials of using different training set portions.
Wecan clearly see that the machine scoring has a sig-nificantly better performance on the easy-case re-sponses than the hard-case responses.
Therefore, itis natural to focus expensive/slow human computa-tion efforts on these hard-case responses.metric easy-case hard-caseagreement(%) 68.16 48.08r 0.594 0.377weighted ?
0.582 0.355Table 2: Evaluation of automated speech assessment sys-tems on two types of speech responses.
For the responseson which two human raters agreed, the machine has a sta-tistically significantly better performance.Suppose that we can obtain the type of each re-sponse, hard-case vs. easy-case, in some way, wethen can focus our human scoring efforts on hard-case responses only since machine scoring performsmuch worse on them.
Figure 2 depicts the re-sults of one trial of using human scoring to replacean increasing number of machine scores.
Among4, 980 responses in the test set, the blue curve showsthe weighted ?
values after replacing an increasingnumber of machine scores with human scores.
Here,we used the scores provided by the second rater fromeach rater pair.
This set of human scores had a Pear-son r of 0.626 with the final scores.
We also re-placed the same number of responses, but withoutdistinguishing easy- and hard-case responses by thecorresponding human scores.
The results are shownin the red curve.
We can observe that the weighted?
values increased from about 0.50, which was ob-tained by using only machine scoring, to about 0.58by asking humans to score all hard-case responses,about 33% of all responses.
Among the two meth-ods to select the responses for using human scoring,we can clearly see that the strategy of focusing onhard-case responses can achieve higher weighted ?when spending the same amount of human efforts asthe strategy of randomly selecting responses.6 DiscussionsIn this paper, we reported on two experiments forimproving the scoring model in automated sponta-77# items scored by humankappa0.520.530.540.550.560.57lll llll lll500 1000 1500methodl hardrandomFigure 2: Weighted ?
values when using human ratingresults to replace machine-predicted scores on hard-caseresponses or a similar number of responses that are ran-domly selected.neous speech assessment.
In the first experiment, wesystematically compared a new modeling method,Cumulative Logit Model (CLM), which has beenwidely used in statistics, with other two widely usedmodeling methods, linear regression and CART.We compared these three modeling methods ona large test data set (containing 4, 980 responses)and evaluated these methods on a series of train-ing data sizes.
The experimental results suggestthat the CLM model consistently achieves the bestperformance (measured in Pearson r and quadraticweighted ?
between the predicted scores and humanrated scores).
More importantly, we find that theCLM can work quite well even when just using hun-dreds of responses in the training stage.
This findingis especially important for building scoring modelswhen pre-test data is limited.Although automated scoring has been designed toovercome several disadvantages of the human ratingprocess, our experiments are meant to initiate sci-entific debate on how best to combine the strengthsof human and automated scoringto achieve an opti-mal compromise of scoring accuracy, cost, and time.At least for current automated scoring systems forspontaneous speech, the machine performance lagsbehind the reliability of the human rating process.We also found that the automated system performedworse on hard-case responses on which even two hu-man raters did not agree.
In a simulation study, weshowed that jointly using human scoring and ma-chine scoring can further improve the scoring per-formance obtained by just using automated speechscoring.
By focusing human scoring, which is ex-pensive, slow, but more accurate, on a set of re-sponses specially selected from the entire set of re-sponses, we can achieve larger gains of scoring per-formance than randomly assigning the same amountof responses for human scoring.
Therefore, from anengineering point of view of building more accuratescoring systems, it is promising to design a hybridsystem consisting of both human scoring and ma-chine scoring.For future research, given the automated speechscoring system?s large performance variation on twotypes of responses, it is worthwhile finding a reli-able way to automatically predict a responses?
con-dition, i.e., whether it is hard or easy to score forhumans or for machines.
We need to consider bothproficiency features we used in this paper and otherfeatures measuring audio quality.
Finding such in-formation can help us decide when to use machinescoring and when to rely on human raters.
In addi-tion, other applications of human computation, suchas asking humans to adjust machine predicted scoresor using human rated scores accumulated in scoringoperations to routinely update the machine scoringsystem will be explored.ReferencesR.E.
Bennett.
2006.
Moving the field forward: Somethoughts on validity and automated scoring.
Auto-mated scoring of complex tasks in computer-basedtesting, pages 403?412.A.
Berger, S. Pietra, and V. Pietra.
1996.
A maximum en-tropy approach to natural language processing.
Com-putational Linguistics, 22:39?72.L.
Chen and S. Yoon.
2011.
Detecting structural eventfor assessing non-native speech.
In 6th Workshop onInnovative Use of NLP for Building Educational Ap-plications, page 74.78Miao Chen and Klaus Zechner.
2011.
Computing andevaluating syntactic complexity features for automatedscoring of spontaneous non-native speech.
In ACL?11,pages 722?731.L.
Chen, K. Zechner, and X Xi.
2009.
Improved pro-nunciation features for construct-driven assessment ofnon-native spontaneous speech.
In NAACL-HLT.M.K.
Enright and T. Quinlan.
2010.
Complement-ing human judgment of essays written by english lan-guage learners with e-rater scoring.
Language Testing,27(3):317?334.M.
Eskenazi.
2009.
An overview of spoken languagetechnology for education.
Speech Communication,51(10):832?844.H.
Franco, H. Bratt, R. Rossier, V. Rao Gadde,E.
Shriberg, V. Abrash, and K. Precoda.
2010.
EduS-peak: a speech recognition and pronunciation scoringtoolkit for computer-aided language learning applica-tions.
Language Testing, 27(3):401.S.J.
Haberman and S. Sinharay.
2010.
The application ofthe cumulative logistic regression model to automatedessay scoring.
Journal of Educational and BehavioralStatistics, 35(5):586.C.
Hacker, T. Cincarek, R. Grubn, S. Steidl, E. Noth, andH.
Niemann.
2005.
Pronunciation Feature Extraction.In Proceedings of DAGM 2005.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I. H Witten.
2009.
The WEKA data min-ing software: An update.
ACM SIGKDD ExplorationsNewsletter, 11(1):10?18.K.
Hornik, C. Buchta, and A. Zeileis.
2009.
Open-source machine learning: R meets weka.
Computa-tional Statistics, 24(2):225?232.T.
Y Jang.
2009.
Automatic assessment of non-nativeprosody using rhythm metrics: Focusing on koreanspeakers?
english pronunciation.
In Proc.
of the 2ndInternational Conference on East Asian Linguistics.M.
Kuhn.
2008.
Building predictive models in r us-ing the caret package.
Journal of Statistical Software,28(5):1?26.A.J.
Quinn and B.B.
Bederson.
2011.
Human compu-tation: a survey and taxonomy of a growing field.
InProceedings of the 2011 annual conference on Humanfactors in computing systems, page 14031412.A.J.
Quinn, B.B.
Bederson, T. Yeh, and J. Lin.
2010.CrowdFlow: integrating machine learning with me-chanical turk for speed-cost-quality flexibility.
Betterperformance over iterations.A.
Rozovskaya and D. Roth.
2011.
Algorithm selectionand model adaptation for ESL correction tasks.
Ur-bana, 51:61801.L.
Von Ahn.
2006.
Games with a purpose.
Computer,39(6):92?94.X.
Xi, D. Higgins, K. Zechner, and D. Williamson.2008.
Automated Scoring of Spontaneous Speech Us-ing SpeechRater v1.0.
Technical report, EducationalTesting Service.Thomas W. Yee.
2010.
The VGAM package for categor-ical data analysis.
J. Statist.
Soft., 32(10):1?34.Klaus Zechner, Derrick Higgins, Xiaoming Xi, andDavid M. Williamson.
2009.
Automatic scoring ofnon-native spontaneous speech in tests of spoken en-glish.
Speech Communication, 51:883?895, October.79
