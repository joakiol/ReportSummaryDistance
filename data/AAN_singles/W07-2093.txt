Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 418?421,Prague, June 2007. c?2007 Association for Computational LinguisticsUP13: Knowledge-poor Methods (Sometimes) Perform PoorlyThierry PoibeauLaboratoire d?Informatique de Paris-NordCNRS UMR 7030 et universit?
Paris 1399, avenue J.-B.
Cl?ment F-93430 Villetaneusethierry.poibeau@lipn.univ-paris13.frAbstractThis short paper presents a system developed atthe Universit?
Paris 13 for the Semeval 2007Metonymy Resolution Task (task #08, locationname track; see Markert and Nissim, 2007).The system makes use of plain word formsonly.
In this paper, we evaluate the accuracy ofthis minimalist approach, compare it to a morecomplex one which uses both syntactic andsemantic features, and discuss its usefulness formetonymy resolution in general.1 IntroductionThis short paper presents the system developed atthe Universit?
Paris 13 for the Metonymyresolution task, during Semeval 2007 (Markert andNissim, 2007).
Two sub-tasks were proposed,concerning 1) country names and 2) companynames.
We only participated in the first task(country names).
We developed a simple approachwhich we present and thoroughly evaluate in thispaper.
We discuss the relevance of this approachand compare it to more complex ones.2 MotivationWe participated in the metonymy task with a verybasic system.
The idea was to investigate theefficiency of a minimalist (though, not Chomskian)system.
This system tags entities on the basis ofdiscriminative (plain) word forms occurring in agiven window only.
Our aim was to find out whichword forms are discriminative enough to beconsidered as parameters.In the past, we developed a system formetonymy resolution for French, evaluated in theframework of the ESTER evaluation (Gravier,2004).
This system, described in Poibeau (2006),uses various kinds of information, among others:plain word forms, part-of-speech tags, andsyntactic and semantic tags (conceptual wordclasses).The usefulness of complex linguistic features(especially syntactic and semantic tags) isquestionable: they may be hard to compute, error-prone and their contribution is not clear.
Wetherefore developed a new version of the systemmainly based on 1) a distributional analysis (onsurface word forms) along with 2) a filteringprocess.
The latter restricted metonymic readingsto country and capital names (as opposed to otherlocation names), since they include a vast majorityof the metonymic readings (this proved to beefficient but is of course a harsh pragmatic over-simplification without real linguistic basis).
Wenevertheless obtained a highly versatile system,performing reasonably well, compared to ourprevious, much more complex implementation(F-score was .58 instead of .63; we computedF-score with ?=1).In the framework of the Semeval evaluation, thefiltering process is irrelevant since only countrynames are considered as entities.
However, wethought that it would be interesting to develop avery basic system, to evaluate the performance onecan obtain using plain word forms only.3 A (too) Lazy ApproachWe chose not to use any part-of-speech tagger orsyntactic or semantic analyzer; we did not use anyexternal knowledge or any other annotated corpusthan the one provided for the training phase.
Sinceno NLP tool was used, we had to duplicate most ofthe words in order to get the singular and the pluralform.
Our system is thus very simple compared to418the state-of-art in this domain (e.g.
Nissim andMarkert, 2003).We used discriminative plain words only.
Theseare gathered as follows: all the words in a givenwindow (here we use a 7 word window, before andafter the target entity since it gave the best resultson the training data) are extracted and associatedwith two classes (literal vs. non literal).
We thusconsider the most discriminative words, i.e.
wordsthat appear frequently in some contexts but not inothers (literal vs. non-literal readings).Discriminative words are elements that areabnormally frequent or rare in one corpuscompared to another one.Characteristic features are selected based ontheir probabilities.
Probability levels measure thesignificance of the differences between the relativefrequency of an expression or a feature within agroup (or a category) with its global relativefrequency calculated over the entire corpus (Lafon,1980).
They are calculated under the hypothesis ofa random distribution of the forms.
The smaller theprobability levels, the more characteristic thecorresponding forms (Lebart and Salem, 1997).We thus obtained 4 lists of discriminative words(literal vs. non-literal ?
before vs. after the targetentity).
As the result, some semantic familiesemerged, especially for words appearing beforeliteral readings: lists of prepositions (in, at,within?)
and geographical items (east, west,western?).
Some lists were manually completed,when a ?natural?
series appeared to be incomplete(for example, if we got east, west, north, wecompleted the word series with south).3.1 Reducing the Size of the Search SpaceThe approach described so far may seems a bitsimplistic (and, indeed, it is!
), but nevertheless ityielded highly discriminative features.
Forexample, if we only tag country namesimmediately preceded by the preposition in as?literal?, we obtain the results presented in table 1(in the following tables, precision is the mostrelevant issue; coverage gives an idea of thepercentage of tagged entities by the consideredfeature, compared to the total number of entities tobe tagged).
Figure 1 shows that detecting thepreposition in in front of a location namediscriminates almost perfectly 23% of the literalreadings.Training TestPrecision 1 .98Coverage .23 .23Table 1.
Results for the pattern in + LOC(result tag = literal)A simple discriminative analysis of the trainingcorpus produces the following list of prepositionsand geographical discriminative features: "at","within", "in", "into", "from", "coast","land", "area", "southern", "south", "east","north", "west", "western", "eastern", etc 1 .Table 2 presents the results obtained from this listof words (occurring in a 7 word window, on theleft of the target word):Training TestPrecision .91 .88Coverage .60 .55Table 2.
Results for the pattern <at+within+?>+ LOC (note that table 1 is contained in table 2)Another typical feature was the use of the entity ina genitive construction (e.g.
in Iran's officialcommitment, Iran is considered as a literalreading).
The presence of 's on the right side of thetarget entity is highly discriminative (table 3):Training TestPrecision .87 .89Coverage .15 .17Table 3.
Results for the pattern LOC?s(result tag = literal)This strategy may seem strange, since the task is tofind metonymic readings rather than literal ones(the baseline is to tag all the target entities asliteral).
However, it is useful in reducing the sizeof the search space by approximately 50%.
Thismeans that more than 70% of the entities with aliteral meaning can be tagged with a confidencearound 90% using this technique, thus reducing thenumber of problematic cases.
The resulting file isrelatively balanced: it contains about 50-60% ofliteral meaning and 40-50% of metaphoricalmeaning (instead of a classical ratio 80% vs. 20%).1 The list also contains nouns and verbs like: "enter","entered", "fly", "flown", "went", "go", "come","land", "country", "mountain"?4193.2 Looking for Metonymy, Desperately ?We used the same strategy for metonymicreadings.
We have observed in the past that wordforms are much more efficient for literal readingsthan for metonymic readings.
However, the factthat the location name is followed by a verb like"has", "should", "was", "would", "will"seemed to be discriminative on the training corpus.Unfortunately, this feature did not work well onthe test corpus (table 4).Training TestPrecision .6 .3Coverage .1 .04Table 4.
Results for the pattern LOC + <was,should?> (result tag = metonymic)This simply means that a syntactic analysis wouldbe useful to discriminate between the sentenceswhere the target entity is the subject of thefollowing verb (in this context, the entity is most ofthe time used with a metaphoric reading; to gofurther, one needs to filter the verb according tosemantic classes).Another point that was clear from the taskguidelines was that sport?s teams correspond tometonymic readings.
The list of characteristicwords for this class, obtained from the trainingcorpus was the following: "player", "team","defender", "plays", "role", "score","scores", "scored", "win", "won", "cup", "v"2,"against", "penalty", "goal", "goals","champion", "champions", etc.
But, bad luck!This list did not work well on the test corpus either:Training TestPrecision .64 .32Coverage .13 .05Table 5.
Results for the pattern LOC +<player, team?>  (result tag = metonymic)Table 5 shows that coverage as well as precisionare very low.Yet another category included words related tothe political role of countries, which entails ametonymic reading: "role", "institution","preoccupation", "attitude", "ally","allies", "institutions", "initiative",2 v for versus, especially in sports: Arsenal-MU  3 v 2.
"according", "authority"?
All these categorieshad low coverage on the test corpus.
This is not sosurprising and is related to our knowledge-poorstrategy: the training corpus is relatively small andit was foreseeable that we would miss most of therelevant contexts.
However, we wanted to maintainprecision above .5 (i.e.
relevant contexts shouldremain relevant), but failed in this, as one can seefrom the overall results.4 Overall EvaluationWe mainly discuss here the results of the coarseevaluation, where only literal vs non-literalmeanings were targeted.
We did not develop anyspecific strategy for the other tracks (medium andfine) since there were too few examples in thetraining data.
We just transferred non-literalreadings to the most probable class according tothe training corpus (metonymic for medium,place-for-people for fine).
However, theperformance of our system (i.e.
accuracy) isrelatively stable between these three tracks, sincethe distribution of examples between the differentclasses is very unequally distributed.Before giving the results, recall that our purposewas to investigate a knowledge-poor strategy, inorder to establish how far one can get using onlysurface indicators.
Thus, unsurprisingly, our resultsfor the training corpus were already lower thanthose obtained using a more sophisticated system(Nissim and Markert, 2003).
They are however agood indicator of performance when one uses onlysurface features.The accuracy on the training corpus was .815.Precision and recall are presented in the table 6.Literal Non-lit.Precision .88 .54Recall .88 .57P&R .88 .55Table 6.
Overall results on the training corpusAccuracy on the test corpus is .754 only.
Table 7shows the results obtained for the different kinds oflocation names.
The result is obvious: there is asignificant drop in both recall and precision,compared to the results on the training corpus.420Literal Non lit.Precision .83 .38Recall .86 .31P&R .84 .34Table 7.
Overall results on the test corpus5 DiscussionMetonymy is a complex linguistic phenomenonand it is thus no surprise that such a basic systemperformed badly, even if the drop in precisionbetween training and test set was disappointing.The main conclusion of this approach is thatsurface forms can be used to reduce the size of thesearch space with a relatively good accuracy.
Alarge part of the literal readings can be taggedusing surface forms only.
For the remaining cases,the use of more sophisticated linguistic information(both syntactic and semantic) is necessary.During this work, we discovered someproblematic target entities whose annotation ischallenging.
For instance, we tagged the followingexample as metonymic (because of the keywords?role?
and ?above?
), whereas it is tagged asliteral in the gold standard:This two-track approach was seen (?)
asreflecting continued manoeuvring overthe role of the <annot> <locationreading="literal"> United States</location> </annot> in the alliance, ?See also the following example (tagged by oursystem as metonymic because of the keyword?relations?, but assumed to be literal in the goldstandard):Relations with China and <annot><location reading="literal"> Singapore</location></annot> ?On the other hand, the following example wastagged as literal by our system (due to thepreposition in) instead of metonymic.After their European Championshipvictory (?
), Holland will be expectedto do well in <annot> <locationreading="metonymic" metotype="place-for-event"> Italy </location></annot>.If Italy is assumed to refer to the World Cupoccurring in Italy, we think that the literal readingis not completely irrelevant (a paraphrase could be:?
?to do well during their stay in Italy?
which isclearly literal).Metonymy is a form of figurative speech ?inwhich one expression is used to refer to thereferent of a related one?
(Markert and Nissim,2007).
The phenomenon corresponds to a semanticshift in interpretation (?a profile shift?)
thatappears to be a function of salience (Cruse andCroft, 2004).
We assume that this semantic shiftdoes not completely erase the original referent: itrather puts the focus on a specific feature of thecontent (?the profile?)
of the standard referent.
Ifwe adopt this theory, we can explain why it may bedifficult to tag some examples, since both readingsmay co-exist.6 ConclusionIn this paper, we presented a (minimalist) systemfor metonymy resolution and evaluated itsusefulness for the task.
The system worked wellfor reducing the size of the search space butperformed badly for the recognition of metonymicreadings themselves.
It should be used incombination with more complex features,especially syntactic and semantic information.ReferencesA.
Cruse and W. Croft.
2004.
Meaning in language, anintroduction to semantics and pragmatics.
OxfordUniversity Press, Oxford.G.
Gravier, J.-F. Bonastre, E. Geoffrois, S. Galliano, K.Mc Tait and K. Choukri.
2004.
The ESTERevaluation campaign for the rich transcription ofFrench broadcast news?.
Proceedings of LREC?04.Lisbon, Portugal.
pp.
885?888.P.
Lafon.
1980.
Sur la variabilit?
de la fr?quence desformes dans un corpus.
Mots.
1. pp.
127?165.L.
Lebart and A. Salem.
1997.
Exploring Textual Data.Springer.
Berlin.K.
Markert and M. Nissim.
2007.
Task08: MetonymyResolution at Semeval 2007.
Proceedings of Semeval2007.
Prague, Czech Rep.M.
Nissim  and K. Markert.
2003.
Syntactic Featuresand Word Similarity for supervised MetonymyResolution.
Proceedings of ACL?03.
Sapporo, Japan.pp.
56?63.T.
Poibeau.
2006.
Dealing with Metonymic Readings ofNamed Entities.
Proceedings of COGSCI?06.Vancouver, Canada.
pp.
1962?1968.421
