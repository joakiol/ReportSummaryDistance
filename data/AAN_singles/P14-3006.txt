Proceedings of the ACL 2014 Student Research Workshop, pages 41?47,Baltimore, Maryland USA, June 22-27 2014.c?2014 Association for Computational LinguisticsAn Exploration of Embeddings for Generalized PhrasesWenpeng Yin and Hinrich Sch?utzeCenter for Information and Language ProcessingUniversity of Munich, Germanywenpeng@cis.lmu.deAbstractDeep learning embeddings have been suc-cessfully used for many natural languageprocessing problems.
Embeddings aremostly computed for word forms althoughlots of recent papers have extended this toother linguistic units like morphemes andword sequences.
In this paper, we definethe concept of generalized phrase that in-cludes conventional linguistic phrases aswell as skip-bigrams.
We compute em-beddings for generalized phrases and showin experimental evaluations on corefer-ence resolution and paraphrase identifica-tion that such embeddings perform betterthan word form embeddings.1 MotivationOne advantage of recent work in deep learning onnatural language processing (NLP) is that linguis-tic units are represented by rich and informativeembeddings.
These embeddings support betterperformance on a variety of NLP tasks (Collobertet al, 2011) than symbolic linguistic representa-tions that do not directly represent informationabout similarity and other linguistic properties.Embeddings are mostly derived for word forms al-though a number of recent papers have extendedthis to other linguistic units like morphemes (Lu-ong et al, 2013), phrases and word sequences(Socher et al, 2010; Mikolov et al, 2013).1Thus,an important question is: what are the basic lin-guistic units that should be represented by embed-dings in a deep learning NLP system?
Buildingon the prior work in (Socher et al, 2010; Mikolovet al, 2013), we generalize the notion of phrase toinclude skip-bigrams (SkipBs) and lexicon entries,1Socher et al use the term ?word sequence?.
Mikolov etal.
use the term ?phrase?
for word sequences that are mostlyfrequent continuous collocations.where lexicon entries can be both ?continuous?and ?noncontinuous?
linguistic phrases.
Exam-ples of skip-bigrams at distance 2 in the sentence?this tea helped me to relax?
are: ?this*helped?,?tea*me?, ?helped*to?
.
.
.
Examples of linguisticphrases listed in a typical lexicon are continuousphrases like ?cold cuts?
and ?White House?
thatonly occur without intervening words and discon-tinous phrases like ?take over?
and ?turn off?
thatcan occur with intervening words.
We considerit promising to compute embeddings for thesephrases because many phrases, including the fourexamples we just gave, are noncompositional orweakly compositional, i.e., it is difficult to com-pute the meaning of the phrase from the meaningof its parts.
We write gaps as ?*?
for SkipBs and?
?
for phrases.We can approach the question of what basiclinguistic units should have representations froma practical as well as from a cognitive point ofview.
In practical terms, we want representationsto be optimized for good generalization.
Thereare many situations where a particular task involv-ing a word cannot be solved based on the worditself, but it can be solved by analyzing the con-text of the word.
For example, if a coreferenceresolution system needs to determine whether theunknown word ?Xiulan?
(a Chinese first name)in ?he helped Xiulan to find a flat?
refers to ananimate or an inanimate entity, then the SkipB?helped*to?
is a good indicator for the animacy ofthe unknown word ?
whereas the unknown worditself provides no clue.From a cognitive point of view, it can be arguedthat many basic units that the human cognitive sys-tem uses have multiple words.
Particularly con-vincing examples for such units are phrasal verbsin English, which often have a non-compositionalmeaning.
It is implausible to suppose that weretrieve atomic representations for, say, ?keep?,?up?, ?on?
and ?from?
and then combine them to41form the meanings of the expressions ?keep yourhead up,?
?keep the pressure on,?
?keep him fromlaughing?.
Rather, it is more plausible that we rec-ognize ?keep up?, ?keep on?
and ?keep from?
asrelevant basic linguistic units in these contexts andthat the human cognitive systems represents themas units.We can view SkipBs and discontinuous phrasesas extreme cases of treating two words that do notoccur next to each other as a unit.
SkipBs are de-fined purely statistically and we will consider anypair of words as a potential SkipB in our exper-iments below.
In contrast, discontinuous phrasesare well motivated.
It is clear that the words?picked?
and ?up?
in the sentences ?I picked itup?
belong together and form a unit very similar tothe word ?collected?
in ?I collected it?.
The mostuseful definition of discontinuous units probablylies in between SkipBs and phrases: we definitelywant to include all phrases, but also some (but notall) statistical SkipBs.
The initial work presentedin this paper may help in finding a good ?compro-mise?
definition.This paper contributes to a preliminary inves-tigation of generalized phrase embeddings andshows that they are better suited than word em-bedding for a coreference resolution classificationtask and for paraphrase identification.
Anothercontribution lies in that the phrase embeddings werelease2could be a valuable resource for others.The remainder of this paper is organized as fol-lows.
Section 2 and Section 3 introduce how tolearn embeddings for SkipBs and phrases, respec-tively.
Experiments are provided in Section 4.Subsequently, we analyze related work in Section5, and conclude our work in Section 6.2 Embedding learning for SkipBsWith English Gigaword Corpus (Parker et al,2009), we use the skip-gram model as imple-mented in word2vec3(Mikolov et al, 2013) to in-duce embeddings.
Word2vec skip-gram scheme isa neural network language model, using a givenword to predict its context words within a windowsize.
To be able to use word2vec directly with-out code changes, we represent the corpus as asequence of sentences, each consisting of two to-kens: a SkipB and a word that occurs between the2http://www.cis.lmu.de/pub/phraseEmbedding.txt.bz23https://code.google.com/p/word2vec/two enclosing words of the SkipB.
The distancek between the two enclosing words can be var-ied.
In our experiments, we use either distancek = 2 or distance 2 ?
k ?
3.
For example, fork = 2, the trigramwi?1wiwi+1generates the sin-gle sentence ?wi?1*wi+1wi?
; and for 2 ?
k ?
3,the fourgram wi?2wi?1wiwi+1generates thefour sentences ?wi?2*wiwi?1?, ?wi?1*wi+1wi?,?wi?2*wi+1wi?1?
and ?wi?2*wi+1wi?.In this setup, the middle context of SkipBs arekept (i.e., the second token in the new sentences),and the surrounding context of words of originalsentences are also kept (i.e., the SkipB in the newsentences).
We can run word2vec without anychanges on the reformatted corpus to learn embed-dings for SkipBs.
As a baseline, we run word2vecon the original corpus to compute embeddings forwords.
Embedding size is set to 200.3 Embedding learning for phrases3.1 Phrase collectionPhrases defined by a lexicon have not been deeplyinvestigated before in deep learning.
To collectcanonical phrase set, we extract two-word phrasesdefined in Wiktionary4, and two-word phrases de-fined in Wordnet (Miller and Fellbaum, 1998) toform a collection of size 95218.
This collectioncontains phrases whose parts always occur next toeach other (e.g., ?cold cuts?)
and phrases whoseparts more often occur separated from each other(e.g., ?take (something) apart?
).3.2 Identification of phrase continuityWiktionary and WordNet do not categorizephrases as continuous or discontinous.
So we needa heuristic for determining this automatically.For each phrase ?A B?, we compute[c1, c2, c3, c4, c5] where ci, 1 ?
i ?
5, indi-cates there are cioccurrences of A and B in thatorder with a distance of i.
We compute thesestatistics for a corpus consisting of Gigawordand Wikipedia.
We set the maximal distanceto 5 because discontinuous phrases are rarelyseparated by more than 5 tokens.If c1is 10 times higher than (c2+c3+c4+c5)/4,we classify ?A B?
as continuous, otherwise as dis-continuous.
Taking phrase ?pick off?
as an ex-ample, it gets vector [1121, 632, 337, 348, 4052],c1(1121) is smaller than the average 1342.25, so4http://en.wiktionary.org/wiki/Wiktionary:Main_Page42?pick off?
is set as ?discontinuous?.
Further con-sider ?Cornell University?
which gets [14831, 16,177, 331, 3471], satisfying above condition, henceit is treated as a continuous phrase.3.3 Sentence reformattingGiven the continuity information of phrases,sentence ??
?
?A ?
?
?B ?
?
?
?
is reformated into??
?
?A B ?
?
?A B ?
?
?
?
if ?A B?
is a discontinu-ous phrase and is separated by maximal 4 words,and sentence ??
?
?AB ?
?
?
?
into ??
?
?A B ?
?
?
?
if?A B?
is a continuous phrase.In the first case, we use phrase ?A B?
to replaceeach of its component words for the purpose ofmaking the context of both constituents availableto the phrase in learning.
For the second situation,it is natural to combine the two words directly toform an independent semantic unit.Word2vec is run on the reformatted corpus tolearn embeddings for both words and phrases.Embedding size is also set to 200.3.4 Examples of phrase neighborsUsually, compositional methods for learning rep-resentations of multi-word text suffer from the dif-ficulty in integrating word form representations,like word embeddings.
To our knowledge, there isno released embeddings which can directly facil-itate measuring the semantic affinity between lin-guistic units of arbitrary lengths.
Table 1 attemptsto provide some nearest neighbors for given typ-ical phrases to show the promising perspectiveof our work.
Note that discontinuous phraseslike ?turn off?
have plausible single word nearestneighbors like ?unplug?.4 ExperimentsOur motivation for generalized phrases in Sec-tion 1 was that they can be used to infer the at-tributes of the context they enclose and that theycan capture non-compositional semantics.
Our hy-pothesis was that they are more suitable for thisthan word embeddings.
In this section we carryout two experiments to test this hypothesis.4.1 Animacy classification for markablesA markable in coreference resolution is a linguis-tic expression that refers to an entity in the realworld or another linguistic expression.
Examplesof markables include noun phrases (?the man?
),named entities (?Peter?)
and nested nominal ex-pressions (?their?).
We address the task of ani-macy classification of markables: classifying themas animate/inanimate.
This feature is useful forcoreference resolution systems because only ani-mate markables can be referred to using masculineand feminine pronouns in English like ?him?
and?she?.
Thus, this is an important clue for automat-ically clustering the markables of a document intocorrect coreference chains.To create training and test sets, we extract all39,689 coreference chains from the CoNLL2012OntoNotes corpus.5We label chains that con-tain an animate pronoun markable (?she?, ?her?,?he?, ?him?
or ?his?)
and no inanimate pronounmarkable (?it?
or ?its?)
as animate; and chainsthat contain an inanimate pronoun markable andno animate pronoun markable as inanimate.
Otherchains are discarded.We extract 39,942 markables and their contextsfrom the 10,361 animate and inanimate chains.The context of a markable is represented as aSkipB: it is simply the pair of the two words occur-ring to the left and right of the markable.
The goldlabel of a markable and its SkipB is the animacystatus of its chain: either animate or inanimate.
Wedivide all SkipBs having received an embedding inthe embedding learning phase into a training set of11,301 (8097 animate, 3204 inanimate) and a bal-anced test set of 4036.We use LIBLINEAR (Fan et al, 2008) for clas-sification, with penalty factors 3 and 1 for inan-imate and animate classes, respectively, becausethe training data are unbalanced.4.1.1 Experimental resultsWe compare the following representations for an-imacy classification of markables.
(i) Phrase em-bedding: Skip-bigram embeddings with skip dis-tance k = 2 and 2 ?
k ?
3; (ii) Word em-bedding: concatenation of the embeddings of thetwo enclosing words where the embeddings areeither standard word2vec embeddings (see Sec-tion 2) or the embeddings published by (Collobertet al, 2011);6(iii) the one-hot vector representa-tion of a SkipB: the concatentation of two one-hotvectors of dimensionality V where V is the sizeof the vocabulary.
The first (resp.
second) vector5http://conll.cemantix.org/2012/data.html6http://metaoptimize.com/projects/wordreprs/43turn off caught up take over macular degeneration telephone interviewswitch off mixed up take charge eye disease statementunplug entangled replace diabetic retinopathy interviewturning off involved take control cataracts conference callshut off enmeshed stay on periodontal disease teleconferenceblock out tangled retire epilepsy telephone callturned off mired succeed glaucoma toldfiddle with engaged step down skin cancer saidTable 1: Phrases and their nearest neighborsis the one-hot vector for the left (resp.
right) wordof the SkipB.
Experimental results are shown inTable 2.representation accuracyphrase embeddingk = 2 0.7032 ?
k ?
3 0.700word embeddingword2vec 0.668*?Collobert et al 0.662*?one-hot vectors 0.638*?Table 2: Classification accuracy.
Mark ?*?
meanssignificantly lower than ?phrase embedding?, k =2; ???
means significantly lower than ?phrase em-bedding?, 2 ?
k ?
3.
As significance test, we usethe test of equal proportion, p < .05, throughout.The results show that phrase embeddings havean obvious advantage in this classification task,both for k = 2 and 2 ?
k ?
3.
This validatesour hypothesis that learning embeddings for dis-continuous linguistic units is promising.In our error analysis, we found two types offrequent errors.
(i) Unspecific SkipBs.
ManySkipBs are equally appropriate for animate andinanimate markables.
Examples of such SkipBsinclude ?take*in?
and ?then*goes?.
(ii) Untypicaluse of specific SkipBs.
Even SkipBs that are spe-cific with respect to what type of markable theyenclose sometimes occur with the ?wrong?
typeof markable.
For example, most markables oc-curring in the SkipB ?of*whose?
are animate be-cause ?whose?
usually refers to an animate mark-able.
However, in the context ?.
.
.
the southeast-ern area of Fujian whose economy is the most ac-tive?
the enclosed markable is Fujian, a provinceof China.
This example shows that ?whose?
occa-sionally refers to an inanimate entity even thoughthese cases are infrequent.4.1.2 Nearest neighbors of SkipBsTable 3 shows some SkipBs and their nearestneighbors in descending order, where similarity iscomputed with cosine measure.A general phenomenon is that phrase embed-dings capture high degree of consistency in infer-ring the attributes of enclosed words.
Consideringthe neighbor list in the first column, we can esti-mate that a verb probably appears as the middletoken.
Furthermore, noun, pronoun, adjective andadverb can roughly be inferred for the remainingcolumns, respectively.74.2 Paraphrase identification taskParaphrase identification depends on semanticanalysis.
Standard approaches are unlikely to as-sign a high similarity score to the two sentences?he started the machine?
and ?he turned the ma-chine on?.
In our approach, embedding of thephrase ?turned on?
can greatly help us to infer cor-rectly that the sentences are paraphrases.
Hence,phrase embeddings and in particular embeddingsof discontinuous phrases seem promising in para-phrase detection task.We use theMicrosoft Paraphrase Corpus (Dolanet al, 2004) for evaluation.
It consists of a trainingset with 2753 true paraphrase pairs and 1323 falseparaphrase pairs, along with a test set with 1147true and 578 false pairs.
After discarding pairsin which neither sentence contains phrases, 3027training pairs (2123 true vs. 904 false) and 1273test pairs (871 true vs. 402 false) remain.7A reviewer points out that this is only a suggestive anal-ysis and that corpus statistics about these contexts would berequired to establish that phrase embeddings can predict part-of-speech with high accuracy.44who*afghanistan, some*told women*have with*responsibility he*worriedhad*afghanistan other*told men*have of*responsibility she*worriedhe*afghanistan two*told children*have and*responsibility was*worriedwho*iraq ?
*told girls*have ?
*responsibility is*worriedhave*afghanistan but*told parents*have that*responsibility said*worriedfighters*afghanistan one*told students*have ?s*responsibility that*worriedwho*kosovo because*told young*have the* responsibility they*worriedwas*afghanistan and*told people*have for*responsibility ?s*worriedTable 3: SkipBs and their nearest neighborsWe tackle the paraphrase identification task viasupervised binary classification.
Sentence repre-sentation equals to the addition over all the to-ken embeddings (words as well as phrases).
Aslight difference is that when dealing with a sen-tence like ??
?
?A B ?
?
?A B ?
?
?
?
we only consider?A B?
embedding once.
The system ?word em-bedding?
is based on the embeddings of singlewords only.
Subsequently, pair representation isderived by concatenating the two sentence vectors.This concatentation is then classified by LIBLIN-EAR as ?paraphrase?
or ?no paraphrase?.4.2.1 Experimental results and analysisTable 4 shows the performance of two methods.Phrase embeddings are apparently better.
Mostwork on paraphrase detection has devised intri-cate features and achieves performance numbershigher than what we report here (Ji and Eisenstein,2013; Madnani et al, 2012; Blacoe and Lapata,2012).
Our objective is only to demonstrate thesuperiority of considering phrase embedding overmerely word embedding in this standard task.We are interested in how phrase embeddingsmake an impact on this task.
To that end, we per-form an analysis on test examples where word em-beddings are better than phrase embeddings andvice versa.Table 5 shows four pairs, of which ?phrase em-bedding?
outperforms ?word embedding?
in theMethods Accuracy F1baseline 0.684 0.803word embedding 0.695 0.805phrase embedding 0.713 0.812Table 4: Paraphrase task results.first two examples, ?word embedding?
defeats?phrase embedding?
in the last two examples.
Inthe first pair, successful phrase detection enablesto split sentences into better units, thus the gener-ated representation can convey the sentence mean-ing more exactly.The meaning difference in the second pair orig-inates from the synonym substitution between?take over as chief financial officer?
and ?fillthe position?.
The embedding of the phrase?take over?
matches the embedding of the singleword ?fill?
in this context.
?Phrase embedding?
in the third pair suffersfrom wrong phrase detection.
Actually, ?in?
and?on?
can not be treated as a sound phrase in thatsituation even though ?in on?
is defined by Wik-tionary.
Indeed, this failure, to some extent, re-sults from the shortcomings of our method in dis-covering true phrases.
Furthermore, figuring outwhether two words are a phrase might need toanalyse syntactic structure in depth.
This work isdirectly based on naive intuitive knowledge, actingas an initial exploration.
Profound investigation isleft as future work.Our implementation discovers the containedphrases in the fourth pair perfectly.
Yet, ?word em-bedding?
defeats ?phrase embedding?
still.
Thepair is not a paraphrase partly because the numbersare different; e.g., there is a big difference between?5.8 basis points?
and ?50 basis points?.
Only amethod that can correctly treat numerical informa-tion can succeed here.
However, the appearance ofphrases ?central bank?, ?interest rates?
and ?ba-sis points?
makes the non-numerical parts moreexpressive and informative, leading to less dom-inant for digital quantifications.
On the contrary,though ?word embedding?
fails to split the sen-45GWP sentence 1 sentence 21 0 1 Common side effects includenasal congestion, runny nose, sore throatand cough, the FDA said .The most common side effects after get-ting the nasal spray were nasal congestion,runny nose, sore throat and cough .1 0 1 Douglas Robinson, a senior vice presidentof finance, will take over as chief financialofficer on an interim basis .Douglas Robinson, CA seniorvice president, finance, will fill theposition in the interim .1 1 0 They were being held Sunday in the CamdenCounty Jail on $ 100,000 bail each .The Jacksons remained in on CamdenCounty jail $ 100,000 bail .0 0 1 The interest rate sensitive two year Schatzyield was down 5.8 basis points at 1.99 per-cent .The Swedish central bank cut inter-est rates by 50 basis points to 3.0 percent.Table 5: Four typical sentence pairs in which the predictions of word embedding system and phraseembedding system differ.
G = gold annotation, W = prediction of word embedding system, P = predictionof phrase embedding system.
The formatting used by the system is shown.
The original word order ofsentence 2 of the third pair is ??
?
?
in Camden County jail on $ 100,000 bail?.tences into better units, it weakens unexpectedlythe expressiveness of subordinate context.
Thisexample demonstrates the difficulty of paraphraseidentification.
Differing from simple similaritytasks, two sentences are often not paraphraseseven though they may contain very similar words.5 Related workTo date, approaches to extend embedding (ormore generally ?representation?)
beyond individ-ual words are either compositional or holistic(Turney, 2012).The best known work along the first line is by(Socher et al, 2010; Socher et al, 2011; Socheret al, 2012; Blacoe and Lapata, 2012), in whichdistributed representations of phrases or even sen-tences are calculated from the distributed repre-sentations of their parts.
This approach is onlyplausible for units that are compositional, i.e.,whose properties are systematically predictablefrom their parts.
As well, how to develop a ro-bust composition function still faces big hurdles;cf.
Table 5.1 in (Mitchell and Lapata, 2010).
Ourapproach (as well as similar work on continuousphrases) makes more sense for noncompositionalunits.Phrase representations can also be derived bymethods other than deep learning of embed-dings, e.g., as vector space representations (Tur-ney, 2012; Turney, 2013; Dinu et al, 2013).
Themain point of this paper ?
generalizing phrases todiscontinuous phrases and computing representa-tions for them ?
is orthogonal to this issue.
Itwould be interesting to evaluate other types of rep-resentations for generalized phrases.6 Conclusion and Future WorkWe have argued that generalized phrases are partof the inventory of linguistic units that we shouldcompute embeddings for and we have shown thatsuch embeddings are superior to word form em-beddings in a coreference resolution task and stan-dard paraphrase identification task.In this paper we have presented initial work onseveral problems that we plan to continue in thefuture: (i) How should the inventory of continu-ous and discontinous phrases be determined?
Weused a purely statistical definition on the one handand dictionaries on the other.
A combination ofthe two methods would be desirable.
(ii) How canwe distinguish between phrases that only occur incontinuous form and phrases that must or can oc-cur discontinuously?
(iii) Given a sentence thatcontains the parts of a discontinuous phrase in cor-rect order, how do we determine that the cooccur-rence of the two parts constitutes an instance ofthe discontinuous phrase?
(iv) Which tasks benefitmost significantly from the introduction of gener-alized phrases?AcknowledgmentsThis work was funded by DFG (grant SCHU2246/4).
We thank Google for a travel grant tosupport the presentation of this paper.46ReferencesWilliam Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for semanticcomposition.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 546?556.
Association for Compu-tational Linguistics.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Georgiana Dinu, Nghia The Pham, and Marco Baroni.2013.
General estimation and evaluation of compo-sitional distributional semantic models.
In Proceed-ings of the Workshop on Continuous Vector SpaceModels and their Compositionality, pages 50?58.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.In Proceedings of the 20th international conferenceon Computational Linguistics, page 350.
Associa-tion for Computational Linguistics.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
The Journal ofMachine Learning Research, 9:1871?1874.Yangfeng Ji and Jacob Eisenstein.
2013.
Discrimi-native improvements to distributional sentence sim-ilarity.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, pages 891?896.Minh-Thang Luong, Richard Socher, and Christo-pher D Manning.
2013.
Better word representa-tions with recursive neural networks for morphol-ogy.
In Proceedings of the Conference on Computa-tional Natural Language Learning, pages 104?113.Nitin Madnani, Joel Tetreault, and Martin Chodorow.2012.
Re-examining machine translation metricsfor paraphrase identification.
In Proceedings of the2012 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 182?190.
Asso-ciation for Computational Linguistics.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013.
Distributed represen-tations of words and phrases and their composition-ality.
arXiv preprint arXiv:1310.4546.George Miller and Christiane Fellbaum.
1998.
Word-net: An electronic lexical database.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive sci-ence, 34(8):1388?1429.Robert Parker, Linguistic Data Consortium, et al2009.
English gigaword fourth edition.
LinguisticData Consortium.Richard Socher, Christopher D Manning, and An-drew Y Ng.
2010.
Learning continuous phraserepresentations and syntactic parsing with recursiveneural networks.
In Proceedings of the NIPS-2010Deep Learning and Unsupervised Feature LearningWorkshop, pages 1?9.Richard Socher, Jeffrey Pennington, Eric H Huang,Andrew Y Ng, and Christopher D Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 151?161.Richard Socher, Brody Huval, Christopher D Manning,and Andrew Y Ng.
2012.
Semantic compositional-ity through recursive matrix-vector spaces.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1201?1211.Peter D Turney.
2012.
Domain and function: A dual-space model of semantic relations and compositions.Journal of Artificial Intelligence Research, 44:533?585.Peter D Turney.
2013.
Distributional semantics be-yond words: Supervised learning of analogy andparaphrase.
Transactions of the Association forComputational Linguistics, 1:353?366.47
