Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 193?203,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsInducing Sentence Structure from Parallel Corpora for ReorderingJohn DeNeroGoogle Researchdenero@google.comJakob UszkoreitGoogle Researchuszkoreit@google.comAbstractWhen translating among languages that differsubstantially in word order, machine transla-tion (MT) systems benefit from syntactic pre-ordering?an approach that uses features froma syntactic parse to permute source wordsinto a target-language-like order.
This paperpresents a method for inducing parse trees au-tomatically from a parallel corpus, instead ofusing a supervised parser trained on a tree-bank.
These induced parses are used to pre-order source sentences.
We demonstrate thatour induced parser is effective: it not onlyimproves a state-of-the-art phrase-based sys-tem with integrated reordering, but also ap-proaches the performance of a recent pre-ordering method based on a supervised parser.These results show that the syntactic structurewhich is relevant to MT pre-ordering can belearned automatically from parallel text, thusestablishing a new application for unsuper-vised grammar induction.1 IntroductionRecent work in statistical machine translation (MT)has demonstrated the effectiveness of syntactic pre-ordering: an approach that permutes source sen-tences into a target-like order as a pre-processingstep, using features of a source-side syntactic parse(Collins et al, 2005; Xu et al, 2009).
Syntac-tic pre-ordering is particularly effective at apply-ing structural transformations, such as the order-ing change from a subject-verb-object (SVO) lan-guage like English to a subject-object-verb (SOV)language like Japanese.
However, state-of-the-artpre-ordering methods require a supervised syntac-tic parser to provide structural information abouteach sentence.
We propose a method that learnsboth a parsing model and a reordering model di-rectly from a word-aligned parallel corpus.
Our ap-proach, which we call Structure Induction for Re-ordering (STIR), requires no syntactic annotationsto train, but approaches the performance of a re-cent syntactic pre-ordering method in a large-scaleEnglish-Japanese MT system.STIR predicts a pre-ordering via two pipelinedmodels: (1) parsing and (2) tree reordering.
Thefirst model induces a binary parse, which definesthe space of possible reorderings.
In particular, onlytrees that properly separate verbs from their objectnoun phrases will license an SVO to SOV trans-formation.
The second model locally permutes thistree.
Our approach resembles work with binary syn-chronous grammars (Wu, 1997), but is distinct in itsemphasis on monolingual parsing as a first phase,and in selecting reorderings without the aid of atarget-side language model.The parsing model is trained to maximize theconditional likelihood of trees that license the re-orderings implied by observed word alignments ina parallel corpus.
This objective differs from thoseof previous grammar induction models, which typ-ically focus on succinctly explaining the observedsource language corpus via latent hierarchical struc-ture (Pereira and Schabes, 1992; Klein and Man-ning, 2002).
Our convex objective allows us to traina feature-rich log-linear parsing model, even withoutsupervised treebank data.Focusing on pre-ordering for MT leads to a new193perspective on the canonical NLP task of grammarinduction?one which marries the wide-spread sci-entific interest in unsupervised parsing models witha clear application and extrinsic evaluation method-ology.
To support this perspective, we highlight sev-eral avenues of future research throughout the paper.We evaluate STIR in a large-scale English-Japanese machine translation system.
We measurehow closely our predicted reorderings match thoseimplied by hand-annotated word alignments.
STIRapproaches the performance of the state-of-the-artpre-ordering method described in Genzel (2010),which learns reordering rules for supervised tree-bank parses.
STIR gives a translation improvementof 3.84 BLEU over a standard phrase-based systemwith an integrated reordering model.2 Parsing and Reordering ModelsSTIR consists of two pipelined log-linear models forparsing and reordering, as well as a third model forinducing trees from parallel corpora, trees that serveto train the first two models.
This section describesthe domain and structure of each model, while Sec-tion 3 describes features and learning objectives.Figure 1 depicts the relationship between the threemodels.
For each aligned sentence pair in a paral-lel corpus, the parallel parsing model selects a bi-nary tree t over the source sentence, such that t li-censes the reordering pattern implied by the wordalignment (Section 2.2).
The monolingual parsingmodel is trained to generate t without inspecting thealignments or target sentences (Section 2.3).
Thetree reordering model is trained to locally permute tto produce the target order (Section 2.4).
In the con-text of an MT system, the monolingual parser andtree reorderer are applied in sequence to pre-ordersource sentences.2.1 Unlabeled Binary TreesUnlabeled binary trees are central to the STIRpipeline.
We represent trees via their constituentspans.
Let [k, `) denote a span of indices of a 0-indexed word sequence e, where i ?
[k, `) if k ?i < `.
[0, n) denotes the root span covering thewhole sequence, where n = |e|.A tree t = (T ,N ) consists of a set of termi-nal spans T and non-terminal spans N .
Each non-??
?
??
?
??
????
?pair [subj] list to add to was0 1 2 3 4 5pair added to the lexicon[0,2) [4,6) [3,4) [2,3)?TargetAlignmentProjectionsParallelParseSourceInducedParseGlossReferenceOrderInducedOrderpair addedtothe lexiconpair addedtothe lexicon[ ] ][ ][ [ ]PositionsS V Os o vParallelcorpusParallelparsingmodelS V OTrees &rotationsMonolingualparsingmodelTreereorderingmodelS V OS V OS O VFigure 1: The training and reordering pipeline for STIRcontains three models.
The inputs and outputs of eachmodel are indicated by solid arrows, while dashed arrowsindicate the source of training examples.
The parallelparsing model provides tree and reordering examples thatare used to train the other models.
In an MT system, thetrained reordering pipeline (shaded) pre-orders a sourcesentence without target-side or alignment information.terminal span [k, `) ?
N has a split point m, wherek < m < ` splits the span into child spans [k,m)and [m, `).
Formally, a pair (T ,N ) is a well-formedtree over [0, n) if:?
The root span [0, n) ?
T ?
N .?
For each [k, `) ?
N , there exists exactly one msuch that {[k,m), [m, `)} ?
T ?
N .?
Terminal spans T are disjoint, but cover [0, n).These trees include multi-word terminal spans.
Itis often convenient to refer to a split non-terminaltriple (k,m, `) that include a non-terminal span[k, `) and its split point m. We denote the set ofthese triples asN+= {(k,m, `) : {[k, `), [k,m), [m, `)}?
T ?
N} .2.2 Parallel Parsing ModelThe first step in the STIR pipeline is to select a bi-nary parse of each source sentence in a parallel cor-pus, one which licenses the reordering implied bya word alignment.
Let the triple (e, f ,A) be analigned sentence pair, where e and f are word se-quences and A is a set of links (i, j) indicating thatei aligns to fj .The set A provides ordering information over e.To simplify definitions below, we first adjust A to194ignore all unaligned words in f .A?
= {(i, c(j)) : (i, j) ?
A}c(j) = |{j?
: j?
< j ?
?i such that (i, j?)
?
A}| .c(j) is the number of aligned words in f prior toposition j.
Next, we define a projection function:?
(i) =[minj?Jij,maxj?Jij + 1)Ji = {j : (i, j) ?
A?}
,and let ?
(i) = ?
if ei is unaligned.
We can extendthis projection function to spans [k, `) of e via union:?
(k, `) =?k?i<`?
(i) .We say that a span [k, `) aligns contiguously if?
(i, j) ?
A?, j ?
?
(k, `) implies i ?
[k, `) ,which corresponds to the familiar definition that[k, `) is one side of an extractable phrase pair.
Un-aligned spans do not align contiguously.Given this notion of projection, we can relatetrees to alignments.
A tree (T ,N ) over e respectsan alignment A?
if all [k, `) ?
T ?
N align con-tiguously, and for every (k,m, `), the projections?
(k,m) and ?
(m, `) are adjacent.
Projections areadjacent if the left bound of one is the right boundof the other, or if either is empty.The parallel parsing model is a linear model overtrees that respect A?, which factors over spans.s(t) =?
[k,`)?TwT?T (k, `) +?
(k,m,`)?N+wN?N (k,m, `)where the weight vector w = (wT wN ) scores fea-tures ?T on terminal spans and ?N on non-terminalspans and their split points.Exact inference under this model can be per-formed via a dynamic program that exploits the fol-lowing recurrence.
Let s(k, `) be the score of thehighest scoring binary tree over the span [k, `) thatrespects A?.
Then,sT (k, `) =????
?wT?T (k, `) if [k, `) alignscontiguously??
otherwisef(k,m, `) = s(k,m) + s(m, `) + wN?N (k,m, `)sN (k, `) = maxm:k<m<`??????
?f(k,m, `) if ?
(k,m) isadjacentto ?
(m, `)??
otherwises(k, `) = max [sT (k, `), sN (k, `)]Above, sT scores terminal spans while filtering outthose which are not contiguous.
The function fscores non-terminal spans by the sum of their childscores and additional features ?N of the parentspan.
The recursive function sN maximizes oversplit points while filtering out non-adjacent children.The recurrence will assign a score of ??
to anytree that does not respect A?.
Section 3 describesthe features of this model.
s(k, `) can be computedefficiently using the CKY algorithm.2.3 Monolingual Parsing ModelThe monolingual parsing model is trained to selectthe same trees as the parallel model, but withoutany features or constraints that reference word align-ments.
Hence, it can be applied to a source sentencebefore its translation is known.This model also scores untyped binary trees ac-cording to a linear model parameterized by somew = (wT wN ) that weights features on terminal andnon-terminal spans, respectively.
We impose a max-imum terminal length of L, but otherwise allow anybinary tree.
The score s(k, `) of the maximal treeover a span [k, `) satisfies the familiar recurrence:sM (k, `) ={wT?T (k, `) if `?
k ?
L??
otherwises(k, `) = max[sL(k, `), maxm:k<m<` f(k,m, `)]Inference under this recurrence can also be per-formed using the CKY algorithm.
Section 3 de-scribes the feature functions and training method.1952.4 Tree Reordering ModelGiven a binary tree (T ,N ) over a sentence e, wecan reorder e by (a) permuting the children of non-terminals and (b) permuting the words of terminalspans.
Formally, a reordering r assigns each termi-nal [k, `) ?
T a permutation ?
(k, `) of its wordsand each split non-terminal (k,m, `) a permutationb(k,m, `) of its subspans, which can be either mono-tone or inverted, in the case of a binary tree.
The per-mutation ?
(k, `) of a non-terminal span [k, `) /?
Tis defined recursively as:{?
(k,m) ?
(m, `) if b(k,m, `) is monotone?
(m, `) ?
(k,m) if b(k,m, `) is invertedIn this paper, we use a reordering model thatselects each terminal ?
(k, `) and each split non-terminal b(k,m, `) independently, conditioned onthe sentence e. While the sub-problems of choos-ing ?
(k, `) and b(k,m, `) are formally similar, weconsider and evaluate them separately because theformer deals only with local reordering, while thelatter involves long-distance structural reordering.Because our trees are binary, selecting b(k,m, `)is a binary classification problem.
Selecting ?
(k, `)for a terminal is a multiclass prediction problem thatchooses among the (` ?
k)!
permutations of ter-minal [k, `).
Development experiments in English-Japanese yielded the best results with a maximumterminal span length L = 2.
Hence, in experiments,terminal reordering is also binary classification.Because each permutation is independent of allthe others, reordering inference via a single passthrough the tree is optimal.
However, a more com-plex search procedure would be necessary to main-tain optimality if the decision of b(k,m, `) ref-erenced other permutations, such as ?
([k,m)) or?
([m, `)).
Coupling together inference in this wayrepresents a possible area of future study.3 Features and Training ObjectivesEach of these linear models factors over featureson either terminal spans [k, `) or split non-terminals(k,m, `).
Features vary in concert with the learningobjectives and search spaces of each model.Figure 2 shows an example sentence from our de-velopment corpus, including the target (Japanese)??
?
??
?
??
????
?pair [subj] list to add to was0 1 2 3 4 5pair added to the lexicon[0,2) [4,6) [3,4) [2,3)?TargetAlignmentProjectionsParallelParseSourceInducedParseGlossReferenceOrderInducedOrderpair addedtothe lexiconpair addedtothe lexicon[ ] ][ ][ [ ]PositionsFigure 2: An example from our development corpus, an-notated with the information flow (left) and annotationsand predictions (right).
Alignments inform projections,which are spans of the target associated with each sourceword.
The parallel parse may only include contiguousspans.
On the other hand, the induced parse may onlycondition on the source sentence.
The induced orderis restricted by the induced parse.
In this example, theinduced order is incorrect because the subject and verbform a constituent in the induced parse that cannot be sep-arated correctly by the reordering model.
This exampledemonstrates the important role of the induced parser inthe STIR pipeline.sentence, alignment, projections, parallel parser pre-diction, monolingual parser prediction, and pre-dicted permutation.
The feature descriptions belowreference this example.3.1 Tree Reordering FeaturesThe tree reordering model consists of two local clas-sifiers: the first can invert the two children of anon-terminal span, while the second can permute thewords of a terminal span.
The non-terminal classi-fier is trained on the trees that are selected by theparallel parsing model; the weights are chosen tominimize log loss of the correct permutation of eachspan (i.e., a maximum entropy model).The terminal model is a multi-class maximum en-tropy model over the n!
possible permutations of thewords in a terminal span.
To make reordering morerobust to monolingual parsing errors, the terminal196model is trained on all contiguous spans of each sen-tence up to length L, not just the terminal spans in-cluded in the parallel parsing tree.The feature templates we apply to each span canbe divided into the following five categories.
Mostfeatures are shared across the two models.Statistics.
From a large aligned parallel corpus, wecompute two statistics.?
PC(e) = count(e aligns contiguously)count(e) is the frac-tion of the time that a phrase e aligns con-tiguously to some target phrase, for allphrases up to length 4.?
PD(ei, ej) is the fraction of the time thattwo co-occuring source words ei and ejalign to adjacent positions in the target.The first statistic indicates whether a contigu-ous phrase in the source should stay contiguousafter reordering.
Features based on this statisticapply to both terminal and short non-terminalspans.
The second statistic indicates when apossibly discontiguous pair of words should beadjacent after reordering.
This statistic is ap-plied to pairs of words that would end up ad-jacent after an inversion: ek and e`?1 for span[k, `).
For instance, PC(added to) = 0.68 andPD(lexicon, to) = 0.19.Cluster.
All source word types are clustered intoword classes, which together maximize likeli-hood of the source side of a large parallel cor-pus under a hidden Markov model, as in Uszko-reit and Brants (2008).
Indicator features basedon clusterings over c classes are defined overwords ek, em?1, em and e`?1, as well as wordsequences for spans up to length 4.
Features areincluded for a variety of clusterings with sizesc ?
{23, 24, .
.
.
, 211}.POS.
A supervised part-of-speech (POS) taggerprovides coarse tags drawn from a 12 tag setT = {Verb, Noun, Pronoun, Conjunction,Adjective, Adverb, Adposition, Determiner,Number, Particle/Function word, Punctuation,Other} (Petrov et al, 2011).
Features based onthese tags are computed identically to the fea-tures based on word classes.Lexical.
For a list of very common words in thesource language, we include lexical indicatorfeatures for the boundary words ek and e`?1.For instance, the word ?to?
triggers a reorder-ing, as do prepositions in general.Length.
Length computed as `?k, length as a frac-tion of sentence length, and quantized lengthfeatures all contribute structural information.All features except POS are computed directlyfrom aligned parallel corpora.
The Cluster and POSfeatures play a similar role of expressing reorderingpatterns over collections of similar words.
The ab-lation study in Section 5 compares these two featuresets directly.3.2 Monolingual Parsing FeaturesThe monolingual parsing model is also trained dis-criminatively, but involves structured prediction, asin a conditional random field (Lafferty et al, 2001).Conditional likelihood objectives have proven ef-fective for supervised parsers (Finkel et al, 2008;Petrov and Klein, 2008).
Recall that the score of atree t = (T ,N ) factors over spans.s(t) =?
[k,`)?TwT?T (k, `) +?
[k,`)?NwN?N (k,m, `)P(t|e) = exp [s(t)]?(t?
)?B(e) exp [s(t?
)]where B(e) is the set of well-formed trees over e.The parallel parsing model (Section 2.2) producesa tree over the source sentence of each aligned sen-tence pair; these trees serve as our training exam-ples.
We can maximize their conditional likelihoodaccording to this model via gradient methods.
Eachtree t over sentence e has a cumulative feature vectorof dimension |w| = |wT |+|wN |, formed by stackingthe terminal and non-terminal vectors:?
(t, e) =??
?
[k,`)?T?T (k, `)?
[k,`)?N?N (k,m, `)?
?The contribution to the gradient objective from a treet for a sentence e is the difference between observed197and expected feature vectors.L(w) =?
(t,e)log P(t|e)?L(w) =?(t,e)???
(t, e)??t?
?B(e)P(t?|e) ?
?
(t?, e)?
?The second term in the gradient?the expectedfeature vector?can be computed efficiently becausethe feature vector ?(t?)
decomposes over the spansof t?.
In particular, the inside-outside algorithm pro-vides the quantities needed to compute the poste-rior probability of each terminal span [k, `) and eachsplit non-terminal (k,m, `).
Let, ?
(k, `) and ?
(k, `)be the outside and inside scores of a span, respec-tively, computed using a log-sum semiring.
Then,the log probablility that a terminal span [k, `) ap-pears in the tree for e under the posterior distribu-tion P(t|e) is ?
(k, `) + wT?T (k, `) .
Note that thisterminal posterior does not include the inside scoreof the span.The log probability that a non-terminal span [k, `)appears with split point m is?
(k, `) + ?
(k,m) + ?
(m, `) + wN?N (k,m, `)By the linearity of expectations, the expected featurevector for e can be computed by averaging the fea-ture vectors of each terminal and split non-terminalspan, weighted by their posterior probabilities.In future work, one may consider training thismodel to maximize the likelihood of an entire forestof trees, in order to maintain uncertainty over whichtree licensed a particular alignment.We are currently using l-BFGS to optimize thisobjective over a relatively small training corpus, for35 iterations.
For this reason, we only include lexi-cal features for very common words.
Distributed oronline training algorithms would perhaps allow formore training data (and therefore more lexicalizedfeatures) to be used in the future.The features of this parsing model share the sametypes as the tree reordering models, but vary in theirdefinition.
The differences stem primarily from thedifferent purpose of the model: here, features arenot meant to decide how to reorder the sentence, butinstead how to bracket the sentence hierarchically sothat it can be reordered.In particular, terminal spans have features on thesequence of POS tags and word clusters they con-tain, while a split non-terminal (k,m, `) is scoredbased on the tags/clusters of the following words andword pairs: ek, em?1, em, e`?1, (ek, em), (ek, e`?1),and (em?1, em).
The head word of a constituent of-ten appears at one of its boundary positions, and sothese features provide a proxy for explicitly trackingconstituent heads in a parser.Context features also appear, inspired by theconstituent-context model of Klein and Manning(2001).
For a span [k, `), we add indicator fea-tures on the POS tags and word clusters of the words(ek?1, e`) which directly surround the constituent.Features based on the statistic PC(e) are alsoscored in the parsing model on all spans of lengthup to 4.Length features score various structural aspects ofeach non-terminal (k,m, `), such as m?k`?k , m?kk?m , etc.One particularly interesting direction for futurework is to train a single parsing model that licensesthe reordering for several different languages.
Wemight expect that a reasonable syntactic bracket-ing of English would simultaneously license thehead-final transformations necessary to produce aJapanese or Korean ordering, and also the verb-subject-object ordering of formal Arabic.13.3 Parallel Parsing FeaturesThe parallel parsing model does not run at transla-tion time, but instead provides training examples tothe other two models.
Hence, defining an appropri-ate learning objective for this model is more chal-lenging.In the end, we are interested in selecting trees thatwe can learn to reproduce without an alignment (viathe monolingual parsing model) and which can bereordered reliably (via the tree reordering model).Note that by construction, any tree selected by theparallel parsing model can be reordered perfectly.However, some of those trees will be easier to re-produce and reorder than others.1An astute reviewer pointed out that no binary tree over anS-V-O sentence can license both S-O-V and V-S-O orderings.Hence, parse trees that are induced for multilingual reorderingwill need n-ary branches.1983.3.1 Reordering Loss FunctionIn order to measure the effectiveness of a reorder-ing pipeline, we would like a metric over permu-tations.
Fortunately, permutation loss for machinetranslation is already an established component ofthe METEOR metric, called a fragmentation penalty(Lavie and Agarwal, 2007).
We define a slight vari-ant of METEOR?s fragmentation penalty that rangesfrom 0 to 1.Given a sentence e, a reference permutation ?
?of (0, ?
?
?
, |e| ?
1), and a hypothesized permuta-tion ?
?, let chunks(?
?, ??)
be the minimum numberof ?chunks?
in ??
: the number of elements in a par-tition of ??
such that each contiguous subsequence isalso contiguous in ?
?.We can define the reordering score between twopermutations in terms of chunks.R(?
?, ??)
= |?
?| ?
chunks(?
?, ??)|?
?| ?
1 (1)If ??
= ?
?, then chunks(?
?, ??)
= 1.
If notwo adjacent elements of ??
are adjacent in ?
?, thenchunks(?
?, ??)
= |?|.
Hence, the metric defined byEquation 1 ranges from 0 to 1.The reference permutation ??
of a source sen-tence e can be defined from an aligned sentence pair(e, f ,A) by sorting the words ei of e by the leftbound of their projection ?(i).
Null-aligned wordsare placed to the left of the next aligned word to theirright in the original order.The reordering-specific loss functions defined inEquation 1 has been shown to correlate with humanjudgements of translation quality, especially for lan-guage pairs with substantial reordering like English-Japanese (Talbot et al, 2011).
Other reordering-specific loss functions also correlate with humanjudgements (Birch et al, 2010).
Future researchcould experiment with alternative reordering-basedloss functions, such as Kendall?s Tau, as suggestedby Birch and Osborne (2011).3.3.2 Parallel Parsing ObjectiveWe can train our reordering pipeline by dividingan aligned parallel corpus into two halves, A and B,where the monolingual parsing and tree reorderingmodels are trained on A, and their effectiveness isevaluated on held-out set B.
Then, the effectivenessof the parallel parsing model is best measured on B,given fully trained parsing and reordering models.?(e,??)?BR(?
(arg maxt?B(e)[w ?
?
(t)]), ??
)(2)Evaluating this objective involves training theother two models.
Therefore, we can only hope tooptimize this objective directly over a small dimen-sional space, for instance using a grid search.
Forthis reason, we currently only include 4 features inthe parallel parsing model for a tree t:1.
The sum of log PC(e) for all terminals e in twith length greater than 1.2.
The count of length-1 terminal spans in t.3.
The count of terminals of length greater than k.4.
An indicator feature of whether parenthesesand brackets are balanced in each span.The model weights of features 3 and 4 above arefixed to large negative constants to prefer terminalspans of length up to k and spans with balancedpunctuation.
The weight of feature 1 is fixed to 1,and weight 2 was set via line search to 0.3.
Tiesamong trees were broken randomly.Of course, the problem of selecting training treesneed not be directly tied to the end task of reorder-ing, as in Equation 2.
Instead, we might consider se-lecting trees according to a likelihood objective onthe source side of a parallel corpus, similar to howmonolingual grammar induction models often opti-mize corpus likelihood.
In such a case, we couldimagine training models with far more parameters,but we leave this research direction to future work.4 Related WorkOur approach to inducing hierarchical structure forpre-ordering relates to several areas of previouswork, including other pre-ordering methods, re-ordering models more generally, and models for theunsupervised induction of syntactic structure.4.1 Pre-Ordering ModelsOur reordering pipeline is intentionally similar toapproaches that use a treebank-trained supervised199parser to reorder source sentences at training andtranslation time (Xia and McCord, 2004; Collinset al, 2005; Lee et al, 2010).
Given a supervisedparser, a rule-based pre-ordering procedure can ei-ther be specified by hand (Xu et al, 2009) or learnedautomatically (Genzel, 2010).
We consider our ap-proach to be a direct extension of these approaches,but one which induces structure from parallel cor-pora rather than relying on a treebank.Tromble (2009) show that some pre-ordering ben-efits can be realized without a parsing step at all, byinstead casting pre-ordering as a permutation mod-eling problem.
While not splitting the task of pre-ordering into parsing and tree rordering, that workshows that pre-ordering models can be learned di-rectly from parallel corpora.4.2 Integrated Reordering ModelsDistortion models have been primary componentsin machine translation models since the advent ofstatistical MT (Brown et al, 1993).
In modernsystems, reordering models are integrated into de-coders as additional features in a discriminative log-linear model, which also includes a language model,translation features, etc.
In these cases, reorderingmodels interact with the strong signal of a target-side language model.
Because ordering predicitonis conflated with target-side generation, evaluationsare conducted on the entire generated output, whichcannot isolate reordering errors from other sorts oferrors, like lexical selection.Despite these differences, certain integrated re-ordering models are similar in character to syntacticpre-ordering models.
In particular, the tree rotationmodel of Yamada and Knight (2001) posited that re-ordering decisions involve rotations of a source-sidesyntax tree.
The parameters of such a model can betrained by treating tree rotations as latent variablesin a factored translation model, which parameterizesreordering and transfer separately but performs jointinference (Dyer and Resnik, 2010).
Syntactic re-ordering and transfer can also be modeled jointly,for instance in a tree-to-string translation system pa-rameterized by a transducer grammar.While the success of integrated reordering modelscertainly highlights the importance of reordering inmachine translation systems, we see several advan-tages to a pipelined, pre-ordering approach.
First,the pre-ordering model can be trained and evaluateddirectly.
Second, pre-ordering models need not fac-tor according to the same dynamic program as thetranslation model.
Third, the same reordering can beapplied during training (for word alignment and ruleextraction) and translation time without adding com-plexity to the extraction and decoding algorithms.Of course, integrating our model into translation in-ference represents a potentially fruitful avenue of fu-ture research.4.3 Grammar InductionThe language processing community actively workson the problem of automatically inducing grammat-ical structure from a corpus of text (Pereira andSchabes, 1992).
Some success in this area hasbeen demonstrated via generative models (Klein andManning, 2002), which often benefit from well-chosen priors (Cohen and Smith, 2009) or poste-rior constraints (Ganchev et al, 2009).
In princi-ple, these models must discover the syntactic pat-terns that govern a language from the sequences ofword tokens alone.
These models are often evalu-ated relative to reference treebank annotations.Grammar induction in the context of machinetranslation reordering offers different properties.The alignment patterns in a parallel corpus pro-vide an additional signal to models that is stronglytied to syntactic properties of the aligned languages.Also, the evaluation is straightforward?any syntac-tic structure that supports the prediction of reorder-ing is rewarded.Kuhn (2004) applied alignment-based constraintsto the problem of inducing probabilistic context-freegrammars, and showed an improvement with respectto Penn Treebank annotations over monolingual in-duction.
Their work is distinct from ours because itfocused on projecting distituents across languages,but mirrors ours in demonstrating that there is a rolefor aligned parallel corpora in grammar induction.Snyder et al (2009) also demonstrated that paral-lel corpora can play a role in improving the qualityof grammar induction models.
Their work differsfrom ours in that it focuses on multilingual lexicalstatistics and dependency relationships, rather thanreordering patterns.200Parsing Tree Reordering PipelinePrec Rec F1 accN accT RO RAll features 82.0 87.8 84.8 97.3 93.6 87.7 80.5Annotated word All but POS 81.3 87.7 84.4 97.0 92.6 86.6 79.4alignments All but Cluster 81.2 87.9 84.4 95.9 93.2 83.8 77.8All but POS & Cluster 75.4 82.0 78.5 89.2 89.7 66.8 49.7Learned alignments All features 72.5 61.0 66.3 91.6 83.3 72.0 49.5Monotone order 34.9Inverted order 30.8Syntactic pre-ordering (Genzel, 2010) 66.0Table 1: Accuracy of individual monolingual parsing and reordering models, as well as complete pipelines trained onannotated and learned word alignments.4.4 Bilingual Grammar InductionAlso related to STIR is previous work on bilingualgrammar induction from parallel corpora using ITG(Blunsom et al, 2009).
These models have focusedon learning phrasal translations ?
which are the ter-minal productions of a synchronous ITG ?
ratherthan reordering patterns that occur higher in the tree.Hence, while this paper shares formal machineryand data sources with that line of work, the modelsthemselves target orthogonal aspects of the transla-tion problem.5 Experimental ResultsAs training data for our models we used 14,000 En-glish sentences that were sampled from the web,translated into Japanese, and manually annotatedwith word alignments.
The annotation was carriedout by the original translators to promote consis-tency of analysis.
Talbot et al (2011) describes thiscorpus in further detail.
A held-out test set of 396manually aligned sentence pairs was used to evalu-ate reordering accuracy.
Statistics used for featureswere computed from the full, unreordered, automat-ically word aligned, parallel training corpus used forthe translation experiments described below.5.1 Individual Model AccuracyWe evaluate the accuracy of the monolingual parsingmodels by their span F1, relative to the trees inducedby the parallel parsing model on the held-out set.The first row of Table 1 shows that the model wasable to reliably replicate the parses induced fromalignments, at 84.8% F1.
The following three linesshow that removing either POS or cluster featuresdegrades performance by only 0.4% F1, indicatingthat POS features are largely redundant in the pres-ence of automatically induced word class features.Hence, no syntactic annotations are necessary at allto train the model.We report two accuracy measures for the tree re-ordering model, one for non-terminal spans (accN )and one for terminal spans (accT ).
The followingcolumn, labeled RO, is the reordering score of thetree reordering model applied to the oracle parallelparser tree.
This score is independent of the mono-lingual parsing model.The fifth line, labeled learned alignments, showsthe impact of replacing manual alignment anno-tations with learned Model 1 alignments, trainedin both directions and combined with the refinedheuristic (Brown et al, 1993; Och et al, 1999).The pipeline column shows the reordering scoreof the full STIR pipeline compared to two simplebaselines: Monotone applies no reordering, whileinverted simply inverts the word order.
STIR out-performs all three other systems.In the final line, we compare to the syntax-basedpre-ordering system described in Genzel (2010).This approach first parses source sentences with asupervised parser, then learns reordering rules thatpermute those trees.5.2 Translation QualityWe apply STIR as a pre-ordering step in a state-of-the-art phrase-based translation system from En-glish to Japanese (Koehn et al, 2003).
At training201time, pre-ordering is applied to the source side of ev-ery sentence pair in the training corpus before wordalignment and phrase extraction.
Likewise, every in-put sentence is pre-ordered at translation time.Our baseline is the same system, but without pre-ordering.
Our implementation?s integrated distor-tion model is expressed as a negative exponentialfunction of the distance between the current and pre-vious source phrase, with a maximum jump widthof four words.
Our in-house decoder is based on thealignment template approach to translation and usesa small set of standard feature functions during de-coding (Och and Ney, 2004).We compare to using an integrated lexicalized re-ordering model (Koehn and Monz, 2005), a forest-to-string translation model (Zhang et al, 2011) andfinally the syntactic pre-ordering technique of Gen-zel (2010) applied to the phrase-based baseline.
Weevaluate the impact of the proposed approach ontranslation quality as measured by the BLEU scoreon the token level (Papineni et al, 2002).The translation model is trained on 700 milliontokens of parallel text, primarily extracted from theweb using automated parallel document identifica-tion (Uszkoreit et al, 2010).
Alignments werelearned using two iterations of Model 1 and two it-erations of the HMM alignment model (Vogel et al,1996).
Our dev and test data sets consist of 3100and 1000 English sentences, respectively, that wererandomly sampled from the web and translated intoJapanese.
The eval set is a larger, heterogenousset containing 12,784 sentences.
In all cases, thefinal log-linear models were optimized on the devset using lattice-based Minimum Error Rate Train-ing (Macherey et al, 2008).Table 2 shows that STIR improves over the base-line system by a large margin of 3.84% BLEU (test).These gains are comparable in magnitude to thosereported in Genzel (2010).
Our induced parses arecompetitive with both systems that use syntacticparsers and substantially outperform lexicalized re-ordering.6 ConclusionWe have demonstrated that induced parses sufficefor pre-ordering.
We hope that future work in gram-mar induction will also consider pre-ordering as anBLEU %dev test evalBaseline 18.65 19.02 13.60Lexicalized Reordering 19.45 18.92 13.99Forest-to-String 23.08 22.85 16.60Syntactic Pre-ordering 22.59 23.28 16.31STIR: annotated 22.46 22.86 16.39STIR: learned 20.28 20.66 14.64Table 2: Translation quality, measured by BLEU, for En-glish to Japanese.
STIR results use both manually anno-tated and learned alignments.extrinsic evaluation.ReferencesAlexandra Birch and Miles Osborne.
2011.
Reorderingmetrics for MT.
In Proceedings of the Association forComputational Linguistics.Alexandra Birch, Phil Blunsom, and Miles Osborne.2010.
Metrics for MT evaluation: Evaluating reorder-ing.
Machine Translation.Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A Gibbs sampler for phrasal syn-chronous grammar induction.
In Proceedings of theAssociation for Computational Linguistics.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics.Shay Cohen and Noah Smith.
2009.
Shared logisticnormal distributions for soft parameter tying in unsu-pervised grammar induction.
In Proceedings of theNorth American Chapter of the Association for Com-putational Linguistics.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the Association forComputational Linguistics.Chris Dyer and Philip Resnik.
2010.
Context-free re-ordering, finite-state translation.
In Proceedings of theNorth American Chapter of the Association for Com-putational Linguistics.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, conditionalrandom field parsing.
In Proceedings of the Associa-tion for Computational Linguistics.Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.2009.
Dependency grammar induction via bitext pro-jection constraints.
In Proceedings of the Associationfor Computational Linguistics.202Dmitriy Genzel.
2010.
Automatically learning source-side reordering rules for large scale machine transla-tion.
In Proceedings of the Conference on Computa-tional Linguistics.Dan Klein and Christopher D. Manning.
2001.
Natu-ral language grammar induction using a constituent-context model.
In Proceedings of Neural InformationProcessing Systems.Dan Klein and Christopher D. Manning.
2002.
A gener-ative constituent-context model for improved grammarinduction.
In Proceedings of the Association for Com-putational Linguistics.Philipp Koehn and Christof Monz.
2005.
Shared task:Statistical machine translation between european lan-guages.
In Proceedings of the International Workshopon Spoken Language Translation.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the North American Chapter of the Associationfor Computational Linguistics.Jonas Kuhn.
2004.
Experiments in parallel-text basedgrammar induction.
In Proceedings of the Associationfor Computational Linguistics.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of the International Conference on MachineLearning.Alon Lavie and Abhaya Agarwal.
2007.
METEOR: Anautomatic metric for mt evaluation with high levels ofcorrelation with human judgments.
In Proceedings ofACL Workshop on Statistical Machine Translation.Young-Suk Lee, Bing Zhao, and Xiaoqiang Luo.
2010.Constituent reordering and syntax models for English-to-Japanese statistical machine translation.
In Pro-ceedings of the Conference on Computational Linguis-tics.Wolfgang Macherey, Franz Och, Ignacio Thayer, andJakob Uszkoreit.
2008.
Lattice-based minimum er-ror rate training for statistical machine translation.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics.Franz Josef Och, Christopher Tillman, and Hermann Ney.1999.
Improved alignment models for statistical ma-chine translation.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automatic eval-uation of machine translation.
In Proceedings of theAssociation for Computational Linguistics.Fernando Pereira and Yves Schabes.
1992.
Inside-outside reestimation from partially bracketed corpora.In Proceedings of the Association for ComputationalLinguistics.Slav Petrov and Dan Klein.
2008.
Sparse multi-scalegrammars for discriminative latent variable parsing.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011.A universal part-of-speech tagset.
Technical report.Benjamin Snyder, Tahira Naseem, and Regina Barzilay.2009.
Unsupervised multilingual grammar induction.In Proceedings of the Association for ComputationalLinguistics.David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Ja-son Katz-Brown, Masakazu Seno, and Franz J. Och.2011.
A lightweight evaluation framework for ma-chine translation reordering.
In Proceedings of theSixth Workshop on Statistical Machine Translation.Roy Tromble.
2009.
Learning linear ordering problemsfor better translation.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing.Jakob Uszkoreit and Thorsten Brants.
2008.
Distributedword clustering for large scale class-based languagemodeling in machine translation.
In Proceedings ofthe Association for Computational Linguistics.Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Du-biner.
2010.
Large scale parallel document mining formachine translation.
In Proceedings of the Conferenceon Computational Linguistics.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of the Conference on Computa-tional linguistics.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics.Fei Xia and Michael McCord.
2004.
Improving a sta-tistical mt system with automatically learned rewritepatterns.
In Proceedings of the Conference on Com-putational Linguistics.Peng Xu, Jaeho Kang, Michael Ringgard, and Franz Och.2009.
Using a dependency parser to improve smt forsubject-object-verb languages.
In Proceedings of theNorth American Chapter of the Association for Com-putational Linguistics.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proceedings of the As-sociation for Computational Linguistics.Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu.2011.
Binarized forest to string translation.
In Pro-ceedings of the Association for Computational Lin-guistics.203
