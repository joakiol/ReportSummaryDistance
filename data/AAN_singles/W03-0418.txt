Identifying Events using Similarity and ContextDominic R. Jones and Cynthia A. ThompsonSchool of ComputingUniversity of UtahSalt Lake City, UT 84103dominicj@cs.utah.edu, cindi@cs.utah.eduAbstractAs part of our work on automatically build-ing knowledge structures from text, we applymachine learning to determine which clausesfrom multiple narratives describing similar sit-uations should be grouped together as descrip-tions of the same type of occurrence.
Ourapproach to the problem uses textual similar-ity and context from other clauses.
Besidestraining data, our system uses only a partialparser as outside knowledge.
We present re-sults evaluating the cohesiveness of the aggre-gated clauses and a brief overview of how thiswork fits into our overall system.1 IntroductionEarly work in natural language processing included am-bitious research on the representation and use of informa-tion about commonly experienced situations (Schank andRiesbeck, 1981).
The concept of a script was introducedin this research, to explain how people understand thesesituations and make inferences about them.
A script isa stereotypical sequence of events that occur as part ofa larger situation and can be used to infer missing de-tails from a partial description of the larger occurrence,in essence providing a means for extracting informationthat is not actually present in a text.Research on scripts includes demonstrations of handbuilt scripts (Cullingford, 1978) and sketchy scripts (De-Jong, 1982) and the adjustment of hand-built scripts usinga genetic algorithm (Mauldin, 1989).
Work on learningschemata under constrained circumstances (Mooney andDeJong, 1985) pursues similar goals.Our research has indicated that scripts may not explic-itly occur in common types of text, such as newspaperstories or incident reports.
Other research also appears tosupport this conclusion (Clark and Porter, 1995).
There-fore, we are investigating event correlations as a moreappropriate and extractable knowledge structure.
In gen-eral, it appears that long event sequences do not reliablyrecur in our data.
We instead look for reliable correlationsbetween a small number of events.Our goal is to automatically extract correlated eventsfrom text, using only a partial parser as outside infor-mation.
To support this goal, we need to group clausesfrom distinct texts into coherent events, handling severalsources of variety in descriptions of the same type of oc-currence.
Synonymy and abbreviations are two commoncontributors.
A more important phenomenon is the ex-istence of semantic categories keyed to the events them-selves.
A number of different objects may participate inan event, and yet have dissimilarities that place them indifferent conventional semantic categories.
For example,a tree and a parked vehicle may both be collided with indifferent aircraft crashes, yet it is difficult to conceive of areasonably specific semantic category that contains both.Each is a physical object, yet there are a large number ofother physical objects that would not reasonably partici-pate in a crash in the same way (books, hamburgers, andmoons are a few examples).As a result of these phenomena, conventional semanticlexicons, whether hand built or automatically generated,differ from our work in two regards.
First, they groupwords, not clauses.
Second, they use pre-defined seman-tic categories instead of contextual relevance.Our answer to this problem is a technique that usestextual similarity and context from neighboring events todecide when to group clauses.
The only outside resourcewe use is a partial parser.
Our technique takes parsedtext and partially built event sequences and uses them togroup clauses that represent the same event.In the remainder of this paper, we present a briefoverview of the sequence learning system before describ-ing how we create events.
We also evaluate our eventformation technique using human judges?
ratings of thecohesiveness of the resulting events.
Finally, we discussareas of related work before concluding the paper.2 An Event Correlation Learning SystemTo understand the event formation process, some back-ground on our larger system is useful.
That sys-tem, SPANIEL (Sequence Processing and ANalysis forInference Enhancement and Learning), attempts to learnordered correlations between events from a corpus ofparsed narratives.1 We assume that the narratives all de-scribe related, but not necessarily identical, situations.An event is a particular atomic occurrence described by asingle clause.
SPANIEL uses a statistical model to identifycorrelations.
Our overall goal is to capture significant cor-relations between events.
We provide an overview here,and will discuss the details in a future paper.Briefly, SPANIEL uses a modified Markov model tocapture correlations.
The model attempts to capture a se-quence identifying the significant events in a text.
Ourmodification to Markov chaining captures the fact thatevents can be conditioned on a prior event that occursseveral clauses distant, instead of on direct sequential de-pendence between events.After the training data is parsed, SPANIEL creates agraph from individual clauses.
This graph is meant tocapture the semantically important information from eachclause in the training texts.
The nodes in the graph arecreated from the actual clauses, with the arcs in the graphindicating which clauses occur together in a text, and inwhat order.
Events are built from the nodes in the graph,with the arcs providing the starting point for correlationsbetween events.We convert the parsed text to graph nodes using aframe representation.
Each frame contains four slots.Three primary slots represent the agent, the action, andthe theme of the clause.
The agent and action slot fillersare straightforward: the agent slot takes a noun phrasethat was tagged as the clause?s subject by the parser, andthe action slot takes the clause?s verb phrase.
The themeslot is more flexible, and can be filled by a noun phrase,prepositional phrase, or adjective phrase that follows theverb.
Noun phrases tagged as the direct object have pri-ority, but the first prepositional or adjective phrase canserve as the filler in the absence of a direct object.2 Afourth slot is only filled if the system identifies a depen-dent clause through the presence of a clause marker orthe absence of a theme (e.g.
?She reported she was hav-ing control problems.?).
Words are not stemmed, except1Following previous authors, we use narrative instead ofstory to refer to a text, indicating that we do not expect to find afull plot structure.2Passive verbs result in rearrangement of the fillers.in two special cases described later.The three primary slots are filled by a head and a mod-ifier.
The head is the head word of the appropriate phrase.Prepositional phrases also concatenate the preposition tothe head.
Verb phrases include all verbs from the phrase,along with particles.
The modifier is a concatenation ofall adjectives, adverbs, and other nouns.
Two examples ofsentences and the resulting frames are given in Figure 1.We define three functions over the graph.
These func-tions allow us to both detect correlations between events,and identify when nodes might represent the same event.First, the cooccurrence of two nodes, C(Ni, Nj), is thenumber of narratives in which Ni occurs before Nj , andthus quantifies each arc.
Ni and Nj do not have to beneighbors: any number of other clauses could have sepa-rated them.
Cooccurrence is directional: C(Ni, Nj) isnot the same as C(Nj , Ni).
Next, two nodes Ni andNj match, denoted M(Ni, Nj), if their frames are iden-tical.3 Finally, two nodes Ni and Nj are similar, denotedS(Ni, Nj), if they demonstrate a certain degree of textualsimilarity.
This function plays an important role in eventcreation, and we define it in detail in the next section.Given this graph, SPANIEL uses a beam search to de-tect correlations of multiple events by incrementally ex-panding a sequence of events that occur frequently to-gether.
As of this writing, most resulting correlations in-clude only a pair of events, with rare sequences of threeevents.
Our goal is to find such correlations with min-imal domain-specific knowledge.
Thus, SPANIEL mustcreate events from individual nodes as it expands eventsequences.
We describe this event formation process, theheart of this paper, in the next section.3 Event Construction and RevisionThe goal of event formation is to identify nodes that canplay the same role within a domain.
Event formationstarts with a single node, identified as a potential part ofa correlation, and attempts to expand this simple event byadding additional nodes.
This process is unsupervised, sowe restrict the search by defining two criteria that nodesmust meet to be included in the same event.
First, theSimilarity criterion states that each node in an event mustbe textually similar to at least one other node in the event.Therefore, a node must be similar to at least one nodethat is already part of an event in order to be added to theevent.
Second, the InSeq criterion states that each nodein the event must occur, in the appropriate order, with atleast one node in each of the event?s immediate neighborsin the sequence.Similarity is determined by a function S(Ni, Nj) thatindicates whether two nodes share sufficient textual fea-3Identical frames do not guarantee identical text, as theframes are simplified versions of the original clauses.The pilot reported that he was running out of fuel.DC: nullAGENT: AIRCRAFTACTION: CRASHEDTHEME: INTO_GROUNDDC:AGENT: PILOTACTION: REPORTEDTHEME: nullDC: nullAGENT: HEACTION: WAS_RUNNING_OUTTHEME: OF_FUELThe aircraft crashed into the ground.Figure 1: Example FramesACTION: IMPACTEDTHEME: TREESDC: nullAGENT: AIRPLANEACTION: IMPACTEDTHEME: HILLDC: nullAGENT: AIRPLANEFigure 2: Textually Similar Nodestures.
Two nodes satisfy this function if two of theirthree primary slots share heads, with some exceptionsdescribed below.
Empty fillers are not considered simi-lar.
In addition, the nodes must have similar dependentclauses (identified by recursively applying the same cri-teria).
Null fillers for this slot are considered similar.
Anexample of two textually similar nodes is given in Fig-ure 2.Two exceptions to this general definition handle nom-inalizations and ?to be.?
First, if two nodes?
actions donot match, but the stemmed head of one node?s actionmatches the stemmed head of the other node?s theme,the nodes are considered similar.
Second, if the stemmedform of a node?s action is ?to be?, the action is ignored indetermining similarity.
Therefore, the node?s agent andtheme must match the other node for the nodes to be sim-ilar.The second criteria for adding nodes to events, In-Seq, ensures that all nodes in an event occur in simi-lar contexts.
Within a correlation, we call events im-mediately prior to or following a given event its neigh-bors.
Neighbors provide contextual information indicat-ing which nodes occur in similar places in narratives.The InSeq constraint states that for a node to be addedto an event, it must cooccur at least once with each ofthat event?s neighbors in the proper order.
To test for thiscooccurrence, we extend the definition of cooccurrence toapply to mixed node and event arguments.
Thus C(N, E)or C(E, N) is the number of times node N cooccurred inthe training data with any node from event E. The orderin which the arguments are given is the order in whichC(N3,E3) = 1C(N1,N3) = 1S(N2,N3) = trueC(N2,N4) = 1C(N1,N4) = 2N1 N4N3N2C(N1,N2) = 1E1 E3E2C(N3,N4) = 1C(E1,N2) = 1C(E1,N3) = 1C(N2,E3) = 1Figure 3: Example of Acceptable Node Pairthey must appear to count.An example of two nodes (N2 and N3) that meet ourtwo criteria is given in Figure 3.
The two nodes areboth textually similar (S(N2, N3) = true) and have thesame context (C(E1, N2), C(E1, N3), C(N2, E3), andC(N3, E3) are all greater than zero), so they can be com-bined into one event E2.Event formation is divided into two algorithms.
Thefirst, AUGMENT, adds nodes to an event based on thecontext provided by one neighbor.
The second, REVISE,removes nodes that do not meet the InSeq criterion whenan event acquires a second neighbor through further cor-relation expansion.3.1 The AUGMENT AlgorithmThe job of the AUGMENT algorithm is to form a newevent and add it to the front or back of a correlation.Therefore the new event initially has one neighboringevent.
Each new event is initialized from a single node,and AUGMENT adds all nodes that meet both the Similar-ity and InSeq criteria to this node.A phenomenon we refer to as self-cooccurrence com-plicates the augmentation process.
This is when twonodes in the same event both occur in the same narrative.Since assigning two nodes to an event proposes that thenodes represent the same occurrence, self-cooccurrenceis undesirable: it represents examples where the twoInput: seed, a seed node, B, a neighbor eventOutput: the augmented event, EFunction AUGMENT(seed, B)E = {seed}?Ns.t.
?M ?
E, S(N, M)if C(N, B) > 0 thenif C(N, B) > (C(N, E) + C(E,N)) thenE = E ?
Nreturn EFigure 4: AUGMENT Algorithm Pseudocodenodes are likely to actually represent different events.4 Toavoid adding to an event a node that actually representsa different type of occurrence, we bias the cooccurrencefunction to penalize events that contain multiple nodesfrom the same narrative.SPANIEL employs the AUGMENT algorithm, illus-trated in Figure 4, each time it evaluates the addition of anew event to a sequence.
The new event is seeded with aninitial node, and AUGMENT then checks each node that issimilar to the event.
If the node occurs at least once withthe neighboring event and does not occur more frequentlywith the event being augmented, the algorithm adds thenode to the event.
We show the pseudocode for augmen-tation at the beginning of a sequence; augmentation atthe conclusion of a sequence is analogous, with C(N, B)replaced by C(B, N) in all cases.Two special cases bootstrap event formation when acorrelation of two events is first created.
First, AUGMENTallows the InSeq criterion to be fulfilled by cooccurrencewith a node similar to the existing neighbor, since thatneighbor initially consists of a single node.
Second, sincethat neighbor also previously had no context to constrainit, AUGMENT is also called on the neighbor.3.2 The REVISE AlgorithmAs previously mentioned, AUGMENT only has one neigh-bor against which to enforce the InSeq criterion.
Fulfill-ment of the criterion using one neighbor does not ensurethat the node will be acceptable given a second neighbor.Therefore, if a second neighbor to this event is added tothe sequence, the REVISE algorithm removes nodes thatfail to remain acceptable.
Figure 5 illustrates the situa-tion with a sequence containing three events, the new onehaving just been added at the front of the sequence.
Eachnode in the new event meets the InSeq criterion, but nowa node in its neighbor does not.
REVISE removes suchnodes from events.
Any removals from the first event re-vised may also warrant removals from the next neighborin the sequence and so on.
Therefore, SPANIEL appliesREVISE to each existing node in the sequence in order,4This does not mean that identical events cannot occur in asequence.N6N5N4N3 N7N1N2E1E2N8E3Figure 5: Example of an InSeq ViolationInput: An event sequence, S =< E1, E2, .
.
.
, En >Output: An event sequence respecting InSeqFunction REVISE(S)modified = truei = 2while (modified && i ?
n)modified = false?X ?
Eiif C(Ei?1, X) = 0 thenEi = Ei \ Xmodified = trueif |Ei| = 0 thenreturn failurei++return S =< E1, .
.
.
, En >Figure 6: REVISE Algorithm Pseudocodestarting with the event neighboring the new event.REVISE (see Figure 6 for the pseudocode) checks thatall nodes in an event are still valid members, given thecontext provided by a new neighbor.
If the algorithm re-moves any nodes from the event, it proceeds to check thenext event in order.
The pseudocode presents this order-ing from left to right, as when an event is added to thefront end of the correlation.
The reverse traversal is anal-ogous, with the loop proceeding from n ?
1 down to 1instead of from 2 up to n, where n is the length of the se-quence.
Note that the first (respectively last) event doesnot need to be checked, as AUGMENT ensures the InSeqcriterion for this event with its single neighbor.REVISE can have one additional effect.
If revision re-sults in any event having no nodes, the proposed eventaddition fails.
In other words, the new event is inconsis-tent with the rest of the correlation.In summary, the AUGMENT algorithm creates an eventthat contains all nodes that are similar to the original nodeand that cooccur with the neighboring event in the se-quence.
Since AUGMENT works on an event that has onlyone neighbor, REVISE checks these constraints when asecond neighbor is added next to an event.
Together,the algorithms enforce the two criteria we use to defineevents.4 EvaluationAutomatic evaluation of individual events would be dif-ficult, since they primarily have meaning in the contextof a specific correlation.
Therefore, we primarily employmanual evaluation.
In addition, we describe a series ofablation tests that illustrate why events and our two eventformation criteria are needed.To conduct the evaluation, we applied SPANIEL toa corpus of narratives from the National TransportationSafety Board?s online database of summaries of aviationaccidents.
This database, as of our experiments, containsmore than 43,000 texts.
We sampled five sets of approx-imately 4,300 texts each, and subdivided these sets witheighty percent of the sample used for training and twentypercent used for testing.4.1 Event CohesionTo evaluate event generation, we presented human judgeswith events acquired by our system and asked them toscore each event by assessing the nodes?
concurrence indescribing an identifiable type of occurrence.
The judgeswere asked to apply the following procedure:1.
Read all nodes in the event, and determine what oc-currence the nodes describe.2.
Read all the nodes again, with the dominant conceptof the occurrence in mind.
For each node, assign ascore of one if the node matches the concept or zerootherwise.3.
For each event, total the scores of the nodes.
Thisgives a score for the event.The first step is asks the judge to choose the dominantconcept of each event based on their interpretation of thenodes.
The second and third steps assign a score to eachevent based on the nodes it contains.
We then divide thisscore by the number of nodes in the event to give a per-centage of nodes that match the dominant concept of theoccurrence represented by the event.
We call these theconforming nodes.We used two human judges, both of whom have back-grounds in computer science and aviation.
Each judgewas given the same set of results from the learning sys-tem, with each two event correlation separated into in-dividual events.
An example of an event, printed as the** Event 32 **-> PILOT MADE FORCED LANDING-> PILOT PERFORMED FORCED LANDING-> PLT MADE LEFT TURN-> PILOT MADE PRECAUTIONARY LANDING-> PLT MADE FORCED LANDING-> PILOT LANDED ON GRAVEL BAR-> PLT APPROACHING_LAND IN OPEN FIELD-> PLT ATTEMPTED LANDING-> PILOT ATTEMPTED_LAND AT NEARBY AIRPORT-> PILOT ELECTED_LAND IN FIELD-> PILOT LAND IN OPEN FIELD-> PILOT HAD_LAND_OFF AIRPORT-> PLT MADE EMERG LANDINGFigure 7: Example Learned EventSystem Judge 1 Judge 2Baseline 0.623 0.696Learned 0.925 0.930Table 1: Evaluation Resultsjudges received it, is given in Figure 7.
We generatedcorrelations from the five different data sets, and blindlychose one set to give the judges.
Figure 10 gives the dis-tribution of the number of nodes in each event.To provide a baseline, ten percent of the events pre-sented to the judges were generated by probabilisti-cally choosing a set of random nodes from the graph,with more frequent nodes chosen with higher probabil-ity.
Each random event contains between two and tennodes.
The judges were not informed of this procedure,and thus we can obtain a measure of their scoring appliedto a randomly generated event.In Table 1, we give the mean fraction of conformingnodes for our event generation algorithm and the randombaseline.5We present more detailed results in two graphs, one forthe results from each judge, in Figure 8 and Figure 9.
Weplot both the baseline and the learned results, giving thefraction of nodes that were judged conforming versus thesize of the event in nodes.
Where more than one event hadthe same number of nodes, we plot the mean conformingnode fraction for the events.
These graphs give an indi-cation of the cohesiveness of the events as the size of theevents increases.
The cohesiveness of the learned eventsdoes not appear to drop dramatically as the size of theevents increases, whereas the the baseline?s performancedoes appear to decrease.5Our second judge, instead of giving a numerical score anddescription of the event?s dominant concept, simply put a ques-tion mark for each of the two baseline events of size 10.
Weinterpret this result as no dominant concept, which equates tothe existence of a pathological dominant concept of size one,and therefore one conforming node.00.20.40.60.8110  20  30  40  50ConformingNodesEvent SizeLearnedBaselineFigure 8: First Judge?s Results00.20.40.60.8110  20  30  40  50ConformingNodesEvent SizeLearnedBaselineFigure 9: Second Judge?s Results4.2 Ablation TestingWe also evaluated the utility of events by eliminatingSPANIEL?s ability to acquire events.
The ablated ver-sion of SPANIEL can only learn correlations of nodes,not events.
We used the same five sets of training textsas input to the ablated system.
The full system learnedbetween 22 and 28 correlations meeting a probabilitythreshold corresponding to approximately 12 occurrencesin the training set, while the ablated system learned be-tween zero and two correlations with the same probabil-ity.We also ablated SPANIEL by removing each of the twocriteria, Similarity and InSeq, individually.
Removing ei-ther criteria rendered the system computationally infea-sible to run.
Neither version could expand even a sin-gle sequence in a reasonable amount of time.
This phe-nomenon appears to be the result of two similar causes.If the InSeq criterion is disabled, the system encounters achain reaction caused by similar nodes.
Each time a nodeis added to an event, more nodes become similar to theevent, causing uncontrolled expansion.
If the Similaritycriterion is disabled, any node that occurs in the properorder with the neighboring event can be added to theevent.
Since cooccurrence ignores intervening clauses,a potentially huge number of nodes can cooccur with anevent, again causing uncontrolled expansion.
These re-01234510 20 30 40 50#ofEventsEvent SizeLearnedRandomFigure 10: Event Sizessults support our hypothesis that, since event formation isan unsupervised process, it must be somewhat conserva-tive to avoid allowing excessive noise into events.5 Related WorkTwo lines of research are relevant to this work.
First, ourresearch is based on research into scripts.
Second, re-cent work in semantic lexicon learning is similar to ourwork, although it focuses on learning related words, notrelated clauses.
In addition, extraction patterns and caseframes bear some resemblance to our events.
Riloff andSchmelzenbach?s work (1998) is an example of this lineof research.
However, events use contextual informationabout other events, unlike extraction patterns and caseframes.The idea of a script originated with Schank and Abel-son (1977) through research on human knowledge struc-tures, and was demonstrated in the SAM system (Culling-ford, 1978).
Later work includes manual creation of a va-riety of knowledge structures including scripts to under-stand stories (Lehnert et al, 1983), application of man-ually generated scripts to the processing of newswirestories (DeJong, 1982), and a combination of apply-ing manually generated scripts to information retrievaland applying genetic algorithms to adjusting existingscripts (Mauldin, 1989).Semantic lexicons have been the focus of much re-search.
WordNet (Fellbaum, 1998) is a prominent exam-ple of a manually generated lexicon.
Two recent projectsin learning semantic lexicons apply automated techniquesto a small set of human provided seed words to createlists of words that the systems assign to the same se-mantic category.
Each project uses a different techniqueto evaluate word similarity.
Thelen (2002) uses similarcontext within a sentence.
Phillips (2002) mines syn-tactic structures.
Other researchers have also clusteredwords to create semantic lexicons.
Lin (1998) created athesaurus using syntactic relationships with other words.Rooth et al (1999) used clustering to create clusters sim-ilar to Levin verb classes (Levin, 1993).
Pereira, Tishbyand Lee (1993) clustered words according to context.6 ConclusionBoth of our criteria play essential roles in the event gen-eration process.
Using similarity alone would combineall clauses that, while similar on the surface, actually arereferring to different types of occurrences.
Using contextalone would combine all clauses encountered in the sameposition relative to some other clause.
Either approach al-lows excessive noise to contaminate the resulting events.Our event generation technique is an essential part ofSPANIEL, our event correlation learning system.
With-out a means for combining nodes, the system would beunable to generalize between different authors?
descrip-tions of occurrences, unless they used exactly the sameterminology.
While standardized lexicons have been in-vestigated (Kamprath et al, 1998), their use has not be-come common.
Therefore, competent event generation isrequired for success in detecting event correlations.In addition to their role in event correlations, eventscould be used as information extraction tools.
Using mul-tiple fillers from different nodes for two primary slotsmakes the event potentially useful as a pattern for extract-ing fillers for the third slot from specific documents.Our results show that this technique for generatingevents produces encouragingly coherent events and out-performs randomly grouping nodes.
Our technique forexploiting the training texts during event generation isrelatively simple.
Possible future work includes testingother algorithms for combining nodes, such as standardclustering techniques.
In summary, we have defined aninteresting problem and provided useful insight into itssolution, which furthers our research into learning eventcorrelations.7 AcknowledgementsWe would like to thank Robert Cornell and Donald Jonesfor evaluating our system.ReferencesPeter Clark and Bruce Porter.
1995.
Constructing scriptsfrom components: Working note 6.
Unpublishedmanuscript, University of Texas at Austin.Richard Cullingford.
1978.
Script Application: Com-puter Understanding of Newspaper Stories.
Ph.D. the-sis, Yale University, New Haven, Connecticut.Gerald DeJong.
1982.
An overview of the FRUMP sys-tem.
In Wendy Lehnert and Martin Ringle, editors,Strategies for Natural Language Processing.
LawrenceErlbaum Associates, Hillsdale, New Jersey.Christiane Fellbaum, editor.
1998.
WordNet, An Elec-tronic Lexical Database.
MIT Press.Christine Kamprath, Eric Adolphson, Teruko Mitamura,and Eric Nyberg.
1998.
Controlled language for mul-tilingual document production: Experience with cater-pillar technical english.
In Proceedings of the SecondInternational Workshop on Controlled Language Ap-plications (CLAW ?98).Wendy Lehnert, Michael Dyer, Peter Johnson, C J Yang,and Steve Harley.
1983.
BORIS ?
an experiment inin-depth understanding of narratives.
Artificial Intelli-gence, 20:15?62.Beth Levin.
1993.
English Verb Classes and Alterna-tions: A Preliminary Investigation.
The University ofChicago Press, Chicago, IL.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of COLINGACL ?98,Montreal, Canada, August.Michael L. Mauldin.
1989.
Information Retrieval by TextSkimming.
Ph.D. thesis, Carnegie Mellon University,Pittsburgh, PA.Raymond Mooney and Gerald DeJong.
1985.
Learningschemata for natural language processing.
In Proceed-ings of IJCAI-85.Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993.Distributional clustering of english words.
In Proceed-ings of ACL-93.William Phillips and Ellen Riloff.
2002.
Exploitingstrong syntactic heuristics and co-training to learn se-mantic lexicons.
In Proceedings of EMNLP-2002.Ellen Riloff and Mark Schmelzenbach.
1998.
An em-pirical approach to conceptual case frame acquisition.In Proceedings of the Sixth Workshop on Very LargeCorpora.Mats Rooth, Stefan Riezler, Detlaf Prescher, Glenn Car-roll, and Franz Beil.
1999.
Inducing a semantically an-notated lexicon via em-based clustering.
In Proceed-ings of ACL-99.Roger Schank and Robert Abelson.
1977.
Scripts, Plans,Goals and Understanding: An Inquiry into HumanKnowledge Structures.
Lawrence Erlbaum Associates,Hillsdale, New Jersey.Roger Schank and Christopher Riesbeck, editors.
1981.Inside Computer Understanding: Five Programs PlusMiniatures.
Lawrence Erlbaum Associates, Hillsdale,New Jersey.Michael Thelen and Ellen Riloff.
2002.
A bootstrappingmethod for learning semantic lexicons using extractionpattern contexts.
In Proceedings of EMNLP-2002.
