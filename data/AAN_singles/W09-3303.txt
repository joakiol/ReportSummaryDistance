Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 19?27,Suntec, Singapore, 7 August 2009.c?2009 ACL and AFNLPWiktionary and NLP: Improving synonymy networksEmmanuel NavarroIRIT, CNRS &Universit?
de Toulousenavarro@irit.frFranck SajousCLLE-ERSS, CNRS &Universit?
de Toulousesajous@univ-tlse2.frBruno GaumeCLLE-ERSS & IRIT, CNRS &Universit?
de Toulousegaume@univ-tlse2.frLaurent Pr?votLPL, CNRS &Universit?
de Provencelaurent.prevot@lpl-aix.frHsieh ShuKaiEnglish DepartmentNTNU, Taiwanshukai@gmail.comKuo Tzu-YiGraduate Institute of LinguisticsNTU, Taiwantzuyikuo@ntu.edu.twPierre MagistryTIGP, CLCLP, Academia Sinica,GIL, NTU, Taiwanpmagistry@gmail.comHuang Chu-RenDept.
of Chinese and Bilingual StudiesHong Kong Poly U. , Hong Kong.churenhuang@gmail.comAbstractWiktionary, a satellite of the Wikipediainitiative, can be seen as a potential re-source for Natural Language Processing.It requires however to be processed be-fore being used efficiently as an NLP re-source.
After describing the relevant as-pects of Wiktionary for our purposes, wefocus on its structural properties.
Then,we describe how we extracted synonymynetworks from this resource.
We pro-vide an in-depth study of these synonymynetworks and compare them to those ex-tracted from traditional resources.
Fi-nally, we describe two methods for semi-automatically improving this network byadding missing relations: (i) using a kindof semantic proximity measure; (ii) usingtranslation relations of Wiktionary itself.Note: The experiments of this paper are based on Wik-tionary?s dumps downloaded in year 2008.
Differences maybe observed with the current versions available online.1 IntroductionReliable and comprehensive lexical resources con-stitute a crucial prerequisite for various NLP tasks.However their building cost keeps them rare.
Inthis context, the success of the Princeton Word-Net (PWN) (Fellbaum, 1998) can be explained bythe quality of the resource but also by the lack ofserious competitors.
Widening this observation tomore languages only makes this observation moreacute.
In spite of various initiatives, costs makeresource development extremely slow or/and re-sult in non freely accessible resources.
Collabo-rative resources might bring an attractive solutionto this difficult situation.
Among them Wiktionaryseems to be the perfect resource for building com-putational mono-lingual and multi-lingual lexica.This paper focuses therefore on Wiktionary, howto improve it, and on its exploitation for creatingresources.In next section, we present some relevant infor-mation about Wiktionary.
Section 3 presents thelexical graphs we are using and the way we buildthem.
Then we pay some attention to evaluation(?4) before exploring some tracks of improvementsuggested by Wiktionary structure itself.2 WiktionaryAs previously said, NLP suffers from a lack oflexical resources, be it due to the low-quality ornon-existence of such resources, or to copyrights-related problems.
As an example, we considerFrench language resources.
Jacquin et al (2002)highlighted the limitations and inconsistenciesfrom the French EuroWordnet.
Later, Sagot andFi?er (2008) explained how they needed to re-course to PWN, BalkaNet (Tufis, 2000) and otherresources (notably Wikipedia) to build WOLF, afree French WordNet that is promising but still avery preliminary resource.
Some languages arestraight-off purely under-resourced.The Web as Corpus initiative arose (Kilgarriffand Grefenstette, 2003) as an attempt to designtools and methodologies to use the web for over-coming data sparseness (Keller and Lapata, 2002).Nevertheless, this initiative raised non-trivial tech-nical problems described in Baroni et al (2008).Moreover, the web is not structured enough to eas-ily and massively extract semantic relations.In this context, Wiktionary could appear to bea paradisiac playground for creating various lexi-19cal resources.
We describe below the Wiktionaryresource and we explain the restrictions and prob-lems we are facing when trying to exploit it.
Thisdescription may complete few earlier ones, for ex-ample Zesch et al (2008a).2.1 Collaborative editingWiktionary, the lexical companion to Wikipedia,is a collaborative project to produce a free-contentmultilingual dictionary.1As the other Wikipedia?ssatellite projects, the resource is not experts-led,rather filled by any kind of users.
The might-beinaccuracy of the resulting resource has lengthilybeen discussed and we will not debate it: see Giles(2005) and Britannica (2006) for an illustrationof the controversy.
Nevertheless, we think thatWiktionary should be less subject (so far) thanWikipedia to voluntary misleading content (be itfor ideological, commercial reasons, or alike).2.2 Articles contentAs one may expect, a Wiktionary article2may (notsystematically) give information on a word?s partof speech, etymology, definitions, examples, pro-nunciation, translations, synonyms/antonyms, hy-pernyms/hyponyms, etc.2.2.1 Multilingual aspectsWiktionary?s multilingual organisation may besurprising and not always meet one?s expectationsor intuitions.
Wiktionaries exist in 172 languages,but we can read on the English language mainpage, ?1,248,097 entries with English definitionsfrom over 295 languages?.
Indeed, a given wik-tionary describes the words in its own languagebut also foreign words.
For example, the Englisharticle moral includes the word in English (adjec-tive and noun) and Spanish (adjective and noun)but not in French.
Another example, boucher,which does not exist in English, is an article of theEnglish wiktionary, dedicated to the French noun(a butcher) and French verb (to cork up).A given wiktionary?s ?in other languages?
leftmenu?s links, point to articles in other wiktionar-ies describing the word in the current language.For example, the Fran?ais link in the dictionaryarticle of the English wiktionary points to an arti-cle in the French one, describing the English worddictionary.1http://en.wiktionary.org/2What article refers to is more fuzzy than classical entryor acceptance means.2.2.2 LayoutsIn the following paragraph, we outline wik-tionary?s general structure.
We only considerwords in the wiktionary?s own language.An entry consists of a graphical form and a cor-responding article that is divided into the follow-ing, possibly embedded, sections:?
etymology sections separate homonyms whenrelevant;?
among an etymology section, different partsof speech may occur;?
definitions and examples belong to a part ofspeech section and may be subdivided into sub-senses;?
translations, synonyms/antonyms and hy-pernyms/hyponyms are linked to a given part ofspeech, with or without subsenses distinctions.In figure 1 is depicted an article?s layout example.Figure 1: Layout of boot article (shortened)About subsenses, they are identified with an in-dex when first introduced but they may appear asa plain text semantic feature (without index) whenused in relations (translations, synonyms, etc.).
Itis therefore impossible to associate the relationsarguments to subsenses.
Secondly, subsense indexappears only in the current word (the source of therelation) and not in the target word?s article it islinked to (see orange French N. and Adj., Jan. 10,20083).A more serious issue appears when relations areshared by several parts of speech sections.
In Ital-3http://fr.wiktionary.org/w/index.php?title=orange&oldid=298131320ian, both synonyms and translations parts are com-mon to all words categories (see for example car-dinale N. and Adj., Apr.
26, 20094).2.3 Technical issuesAs Wikipedia and the other Wikimedia Founda-tion?s projects, the Wiktionary?s content manage-ment system relies on the MediaWiki softwareand on the wikitext.
As stated in Wikipedia?sMetaWiki article, ?no formal syntax has been de-fined?
for the MediaWiki and consequently it isnot possible to write a 100% reliable parser.Unlike Wikipedia, no HTML dump is availableand one has to parse the Wikicode.
Wikicodeis difficult to handle since wiki templates requirehandwritten rules that need to be regularly up-dated.
Another difficulty is the language-specificencoding of the information.
Just to mention one,the target language of a translation link is iden-tified by a 2 or 3 letters ISO-639 code for mostlanguages.
However in the Polish wiktionary thecomplete name of the language name (angielski,francuski, .
.
. )
is used.2.4 Parsing and modelingThe (non-exhaustive) aforementioned list of diffi-culties (see ?2.2.2 and ?2.3) leads to the followingconsequences:?
Writing a parser for a given wiktionary ispossible only after an in-depth observation of itssource.
Even an intensive work will not preventall errors as long as (i) no syntax-checking is madewhen editing an article and (ii) flexibility with the?tacitly agreed?
layout conventions is preserved.Better, flexibility is presented as a characteristic ofthe framework:?[.
.
. ]
it is not a set of rigid rules.
You mayexperiment with deviations, but other editorsmay find those deviations unacceptable, andrevert those changes.
They have just as muchright to do that as you have to make them.5?Moreover, a parser has to be updated every newdump, as templates, layout conventions (and soon) may change.
?Writing parsers for different languages is not asimple adjustment, rather a complete overhaul.?
When extracting a network of semantic rela-tions from a given wiktionary, some choices aremore driven by the wiktionary inner format thanscientific modelling choices.
An illustration fol-4http://it.wiktionary.org/w/index.php?title=cardinale&oldid=7582055http://en.wiktionary.org/wiki/WT:ELElows in ?3.2.
When merging information extractedfrom several languages, the homogenisation of thedata structure often leads to the choice of the poor-est one, resulting in a loss of information.2.5 The bigger the better?Taking advantage of colleagues mastering variouslanguages, we studied the wiktionary of the fol-lowing languages: French, English, German, Pol-ish and Mandarin Chinese.
A first remark con-cerns the size of the resource.
The official num-ber of declared articles in a given wiktionary in-cludes a great number of meta-articles which arenot word entries As of April 2009, the French wik-tionary reaches the first rank6, before the Englishone.
This can be explained by the automated im-port of public-domain dictionaries articles (Littr?1863 and Dictionnaire de l?Acad?mie Fran?aise1932-1935).
Table 1 shows the ratio between thetotal number of articles and the ?relevant?
ones(numbers based on year 2008 snapshots).Total Meta?Other?
?Relevantfr 728,266 25,244 369,948 337,074 46%en 905,963 46,202 667,430 192,331 21%de 88,912 7,235 49,672 32,005 36%pl 110,369 4,975 95,241 10,153 9%zh 131,752 8,195 112,520 1,037 0.7%?templates definitions, help pages, user talks, etc.?
?other languages, redirection links, etc.Table 1: Ratio of ?relevant?
articles in wiktionariesBy ?relevant?, we mean an article about a wordin the wiktionary?s own language (e.g.
not anarticle about a French word in the English Wik-tionary).
Among the ?relevant?
articles, someare empty and some do not contain any transla-tion nor synonym link.
Therefore, before decidingto use Wiktionary, it is necessary to compare theamount of extracted information contribution andthe amount of work required to obtain it .3 Study of synonymy networksIn this section, we study synonymy networks builtfrom different resources.
First, we introducesome general properties of lexical networks (?3.1).Then we explain how we build Wiktionary?s syn-onymy network and how we analyse its proper-ties.
In ?3.3, we show how we build similar graphsfrom traditional resources for evaluation purposes.3.1 Structure of lexical networksIn the following sections, a graph G = (V,E)is defined by a set V of n vertices and a setE ?
V2of m edges.
In this paper, V is6http://meta.wikimedia.org/wiki/List_of_Wiktionaries21a set of words and E is defined by a relationER7??
E : (w1, w2) ?
E if and only if w1R?
w2.Most of lexical networks, as networks extractedfrom real world, are small worlds (SW) net-works.
Comparing structural characteristics ofwiktionary-based lexical networks to some stan-dard resource should be done according to well-known properties of SW networks (Watts andStrogatz, 1998; Barabasi et al, 2000; Newman,2003; Gaume et al, 2008).
These properties are:?
Edge sparsity: SW are sparse in edgesm = O(n) or m = O(n log(n))?
Short paths: in SW, the average path length(L)7is short.
Generally there is at least one shortpath between any two nodes.?
High clustering: in SW, the clustering coef-ficient (C) that expresses the probability that twodistinct nodes adjacent to a given third one are ad-jacent, is an order of magnitude higher than forErdos-Renyi (random) graphs: CSWCrandom;this indicates that the graph is locally dense, al-though it is globally sparse.
?Heavy-tailed degree distribution: the distri-bution of the vertices incidence degrees follows apower law in a SW graph.
The probability P (k)that a given node has k neighbours decreases as apower law, P (k) ?
ka(a being a constant charac-teristic of the graph).
Random graphs conforms toa Poisson Law.3.2 Wiktionary?s networkGraph extraction Considering what said in?2.2.2 and ?2.4, we made the following choices:8?
Vertices: a vertex is built for each entry?s partof speech.?
Parts of speech: when modeling the linksfrom X (X having for part of speech PosX) toone of its synonyms Y , we assume that PosY=PosX, thus building vertex PosY.Y.?
Subsenses: subsenses are flattened.
First, thesubsenses are not always mentioned in the syn-onyms section.
Second, if we take into accountthe subsenses, they only appear in the source of therelation.
For example, considering in figure 1 therelation bootsyn???
kick (both nouns), and given the10 subsenses for boot and the 5 ones for kick, weshould build 15 vertices.
And we should then add7Average length of the shortest path between any twonodes.8These choices can clearly be discussed from a linguis-tic point of view and judged to be biased.
Nevertheless, weadopted them as a first approximation to make the modellingpossible.all the links between the mentioned boot?s sub-senses and the 5 kick?s existing subsenses.
Thiswould lead to a high number of edges, but thegraph would not be closer to the reality.
The waysubsenses appear in Wiktionary are unpredictable.
"Subsenses" correspond sometimes to homonymsor clear-cut senses of polysemous words, but canalso correspond to facets, word usage or regu-lar polysemy.
Moreover, some entries have nosubsenses distinction whereas it would be wor-thy.
More globally, the relevance of discrete wordsenses has been seriously questioned, see (Victorriand Fuchs, 1996) or (Kilgarriff, 1997) for veryconvincing discussions.
Two more practical rea-sons led us to this choice.
We want our method tobe reproducible for other languages and some wik-tionaries do not include subsenses.
At last, somegold standard resources (eg.
Dicosyn) have theirsubsenses flattened too and we want to comparethe resources against each other.?
Edges: wiktionary?s synonymy links are ori-ented but we made the graph symmetric.
For ex-ample, boot does not appear in kick?s synonyms.Some words even appear as synonyms without be-ing an entry of Wiktionary.From the boot example (figure 1), we extract ver-tices {N.boot, V.boot}, build {N.buskin,N.kick, V.kick} and we add the follow-ing (symmetrized) edges: N.boot?N.buskin,N.boot?N.kick and V.boot?V.kick.Graph properties By observing the table 2, wecan see that the graphs of synonyms extractedfrom Wiktionary are all typical small worlds.
In-deed their llccremains short, their Clccis alwaysgreater or equal than 0.2 and their distributioncurves of the vertices incidence degree is veryclose to a power law (a least-square method givesalways exponent alcc?
?2.35 with a confidencer2lccalways greater than 0.89).
It can also be seenthat the average incidence klccranges from 2.32to 3.32.9It means that no matter which language9It is noteworthy that the mean incidence of vertices is al-most always the same (close to 2.8) no matter the graph sizeis.
If we assume that all wiktionary?s graphs grow in a similarway but at different speed rates (after all it is the same frame-work), graphs (at least their statistical properties) from differ-ent languages can be seen as snapshots of the same graph atdifferent times.
This would mean that the number of graphsedges tends to grow proportionally with the number of ver-tices.
This fits with the dynamic properties of small worlds(Steyvers and Tenenbaum, 2005).
It means that for a wik-tionary system, even with many contributions, graph densityis likely to remain constant and we will see that in compar-ison to traditional lexical resources this density is quite low.22graph n m nlccmlccklccllccClccalccr2lccfr-N 18017 9650 3945 4690 2.38 10.18 0.2 -2.03 0.89fr-A 5411 2516 1160 1499 2.58 8.86 0.23 -2.04 0.95fr-V 3897 1792 886 1104 2.49 9.84 0.21 -1.65 0.91en-N 22075 11545 3863 4817 2.49 9.7 0.24 -2.31 0.95en-A 8437 4178 2486 3276 2.64 8.26 0.2 -2.35 0.95en-V 6368 3274 2093 2665 2.55 8.33 0.2 -2.01 0.93de-N 32824 26622 12955 18521 2.86 7.99 0.28 -2.16 0.93de-A 5856 6591 3690 5911 3.2 6.78 0.24 -1.93 0.9de-V 5469 7838 4574 7594 3.32 5.75 0.23 -1.92 0.9pl-N 8941 4333 2575 3143 2.44 9.85 0.24 -2.31 0.95pl-A 1449 731 449 523 2.33 7.79 0.21 -1.71 0.94pl-V 1315 848 601 698 2.32 5.34 0.2 -1.61 0.92n: number of vertices m: number of edgesk: avg.
number of neighbours per vertex l: avg.
path length between verticesC: clustering rate a: power law exponent with r2confidence_lcc: denotes on largest connected componentTable 2: Wiktionary synonymy graphs propertiesor part of speech, m = O(n) as for most of SWgraphs (Newman, 2003; Gaume et al, 2008).3.3 Building synonymy networks fromknown standardsWordNet There are many possible ways forbuilding lexical networks from PWN.
We triedseveral methods but only two of them are worthto be mentioned here.
The graphs we built havewords as vertices, not synsets or senses.
A firststraightforward method (method A) consists inadding an edge between two vertices only if thecorresponding words appear as elements of thesame synset.
This method produced many discon-nected graphs of various sizes.
Both the compu-tational method we planned to use and our intu-itions about such graphs were pointing towards abigger graph that would cover most of the lexicalnetwork.We therefore decided to exploit the hypernymyrelation.
Traditional dictionaries indeed proposehypernyms when one look for synonyms of veryspecific terms, making hypernymy the closest re-lation to synonymy at least from a lexicographicviewpoint.
However, adding all the hypernymy re-lations resulted in a network extremely dense inedges with some vertices having a high number ofneighbours.
This was due to the tree-like organi-sation of WordNet that gives a very special impor-tance to higher nodes of the tree.In the end we retained method B that consists inadding edges in following cases:?
if two words belong to the same synset;?
if a word only appears in a synset that is a leafof the tree and contains only this word, then cre-ate edges linking to words included in the hyper-nym(s) synset.We would like to study the evolution through time of wik-tionaries, however this is outside the scope of this paper.Therefore when a vertice w do not get any neigh-bour according to method A, method B adds edgeslinking w to words included in the hypernym(s)synset of the synset {w}.
We only added hyper-nyms for the leaves of the tree in order to keep ourrelations close to the synonymy idea.
This idea hasalready been exploited for some WordNet-basedsemantic distances calculation taking into accountthe depth of the relation in the tree (Leacock andChodorow, 1998).Dicosyn graphs Dicosyn is a compilation ofsynonym relations extracted from seven dictionar-ies (Bailly, Benac, Du Chazaud, Guizot, Lafaye,Larousse and Robert):10there is an edge r ?
s ifand only if r and s have the same syntactic cate-gory and at least one dictionary proposes s beinga synonym in the dictionary entry r. Then, eachof the three graphs (Nouns, Verbs, Adjectives) ob-tained is made symmetric (dicosyn-fr-N, dicosyn-fr-V and dicosyn-fr-A).Properties of the graphs extracted Table 3sums-up the structural properties of the synonymsnetworks built from standard resources.We can see that all the synonymy graphs ex-tracted from PWN or Dicosyn are SW graphs.Indeed their llccremains short, their Clccis al-ways greater or equal than 0.35 and their distri-bution curves of the vertices incidence degree isvery close to a power law (a least-square methodgives always exponent alccnear of ?2.30 with aconfidence r2lccalways greater than 0.85).
It canalso be observed that no matter the part of speech,the average incidence of Dicosyn-based graphs isalways lower than WordNet ones.10Dicosyn has been first produced at ATILF, before beingcorrected at CRISCO laboratory.
(http://elsap1.unicaen.fr/dicosyn.html)23graph n m nlccmlccklccllccClccalccr2lccpwn-en-N-A 117798 104929 12617 28608 4.53 9.89 0.76 -2.62 0.89pwn-en-N-B 117798 168704 40359 95439 4.73 7.79 0.72 -2.41 0.91pwn-en-A-A 21479 22164 4406 11276 5.12 9.08 0.75 -2.32 0.85pwn-en-A-B 21479 46614 15945 43925 5.51 6.23 0.78 -2.09 0.9pwn-en-V-A 11529 23019 6534 20806 6.37 5.93 0.7 -2.34 0.87pwn-en-V-B 11529 40919 9674 39459 8.16 4.66 0.64 -2.06 0.91dicosyn-fr-N 29372 100759 26143 98627 7.55 5.37 0.35 -2.17 0.92dicosyn-fr-A 9452 42403 8451 41753 9.88 4.7 0.37 -1.92 0.92dicosyn-fr-V 9147 51423 8993 51333 11.42 4.2 0.41 -1.88 0.91Table 3: Gold standard?s synonymy graphs properties4 Wiktionary graphs evaluationCoverage and global SW analysis By compar-ing tables 2 and 3, one can observe that:?
The lexical coverage of Wiktionary-based syn-onyms graphs is always quantitatively lower thanthose of standard resources although this maychange.
For example, to horn (in PWN), absentfrom Wiktionary in 2008, appeared in 2009.
Atlast, Wiktionary is more inclined to include someclass of words such as to poo (childish) or toprefetch, to google (technical neologisms).?
The average number of synonyms for an en-try of a Wiktionary-based resource is smaller thanthose of standard resources.
For example, com-mon synonyms such as to act/to play appear inPWN and not in Wiktionary.
Nevertheless, someother appear (rightly) in Wiktionary: to reduce/todecrease, to cook/to microwave.?
The clustering rate of Wiktionary-basedgraphs is always smaller than those of standard re-sources.
This is particularly the case for English.However, this specificity might be due to differ-ences between the resources themselves (Dicosynvs.
PWN) rather than structural differences at thelinguistic level.Evaluation of synonymy In order to evaluatethe quality of extracted synonymy graphs fromWiktionary, we use recall and precision measure.The objects we compare are not simple sets butgraphs (G = (V ;E)), thus we should compareseparately set of vertices (V ) and set of edges (E).Vertices are words and edges are synonymy links.Vertices evaluation leads to measure the resource(a) English Wiktionary vs. WordnetPrecision RecallNouns 14120/22075 = 0.64 14120/117798 = 0.12Adj.
5874/8437 = 0.70 5874/21479 = 0.27Verbs 5157/6368 = 0.81 5157/11529 = 0.45(b) French Wiktionary vs. DicosynPrecision RecallNouns 10393/18017 = 0.58 10393/29372 = 0.35Adj.
3076/5411 = 0.57 3076/9452 = 0.33Verbs 2966/3897 = 0.76 2966/9147 = 0.32Table 4: Wiktionary coveragecoverage whereas edges evaluation leads to mea-sure the quality of the synonymy links in Wik-tionary resource.First of all, the global picture (table 4) showsclearly that the lexical coverage is rather poor.
Alot of words included in standard resources are notincluded yet in the corresponding wiktionary re-sources.
Overall the lexical coverage is alwayslower than 50%.
This has to be kept in mind whilelooking at the evaluation of relations shown in ta-ble 5.
To compute the relations evaluation, eachresource has been first restricted to the links be-tween words being present in each resource.About PWN, since every link added withmethod A will also be added with method B, theprecision of Wiktionary-based graphs synonymslinks will be always lower for "method A graphs"than for "method B graphs".
Precision is rathergood while recall is very low.
That means that alot of synonymy links of the standard resourcesare missing within Wiktionary.
As for Dicosyn,the picture is similar with even better precision butvery low recall.5 Exploiting Wiktionary for improvingWiktionaryAs seen in section 4, Wiktionary-based resourcesare very incomplete with regard to synonymy.
Wepropose two tasks for adding some of these links:Task 1: Adding synonyms to Wiktionary bytaking into account its Small World characteristicsfor proposing new synonyms.
(a) English wiktionary vs. WordnetPrecision RecallNouns (A) 2503/6453 = 0.39 2503/11021 = 0.23Nouns (B) 2763/6453 = 0.43 2763/18440 = 0.15Adj.
(A) 786/3139 = 0.25 786/5712 = 0.14Adj.
(B) 1314/3139 = 0.42 1314/12792 = 0.10Verbs (A) 866/2667 = 0.32 866/10332 = 0.08Verbs (B) 993/2667 = 0.37 993/18725 = 0.05(b) French wiktionary vs. DicosynPrecision RecallNouns 3510/5075 = 0.69 3510/44501 = 0.08Adj.
1300/1677 = 0.78 1300/17404 = 0.07Verbs 899/1267 = 0.71 899/23968 = 0.04Table 5: Wiktionary synonymy links precision & recall24Task 2: Adding synonyms to Wiktionary bytaking into account the translation relations.We evaluate these two tasks against the bench-marks presented in section 3.2.5.1 Improving synonymy in Wiktionary byexploiting its small world structureWe propose here to enrich synonymy links of Wik-tionary by taking into account that lexical net-works have a high clustering coefficient.
Our hy-pothesis is that missing links in Wiktionary shouldbe within clusters.A high clustering coefficient means that twowords which are connected to a third one are likelyto be connected together.
In other words neigh-bours of my neighbours should also be in myneighbourhood.
We propose to reverse this prop-erty to the following hypothesis: "neighbour of myneighbours which are not in my neighbourhoodshould be a good neighbour candidate".
Thus thefirst method we test consist simply in connectingevery vertex to neighbours of its neighbours.
Onecan repeat this operation until the expected num-ber of edges is obtained.11Secondly we used the PROX approach pro-posed by (Gaume et al, 2009).
It is a stochasticmethod designed for studying ?Hierarchical SmallWorlds?.
Briefly put, for a given vertex u, onecomputes for all other vertices v the probabilitythat a randomly wandering particle starting fromu stands in v after a fixed number of steps.
LetP (u, v) be this value.
We propose to connect uto the k first vertices ranked in descending orderwith respect of P (u, v).
We always choose k pro-portionally to the original degree of u (number ofneighbours of u).For a small number of steps (3 in our case) ran-dom wanderings tend to be trapped into local clus-ter structures.
So a vertex v with a high P (u, v) islikely to belong to the same cluster as u, whichmeans that a link u?v might be relevant.Figure 2 shows precision, recall and f-scoreevolution for French verbs graph when edges areadded using ?neighourhood?
method (neigh), andusing ?Prox?
method.
Dashed line correspond tothe value theoretically obtained by choosing edgesat random.
First, both methods are clearly moreefficient than a random addition, which is not sur-prising but it seems to confirm our hypothesis thatmissing edges are within clusters.
Adding sharply11We repeat it only two times, otherwise the number ofadded edges is too large.0 2000 4000 6000 8000 10000 12000 140000.00.10.20.30.40.50.60.70.8Pprox3neighrandom0 2000 4000 6000 8000 10000 12000 140000.030.040.050.060.070.080.09R0 2000 4000 6000 8000 10000 12000 140000.050.060.070.080.090.100.110.120.13Ffr.VFigure 2: Precision, recall and F-score of French verbsgraph enlarged using only existing synonymy linksneighbours of neighbours seems to be as good asadding edges ranked by Prox, anyway the rankprovided by Prox permits to add a given numberof edges.
This ranking can also be useful to orderpotential links if one think about a user validationsystem.
Synonyms added by Prox and absent fromgold standards are not necessarily false.For example Prox proposes a relevant link ab-solve/forgive, not included in PWN.
Moreover,many false positive are still interesting to considerfor improving the resource.
For example, Proxadds relations such as hypernyms (to uncover/topeel) or inter-domain ?synonyms?
(to skin/to peel).This is due to high clustering (see ?3.1) and tothe fact that clusters in synonymy networks corre-lates with language concepts (Gaume et al, 2008;Duvignau and Gaume, 2008; Gaume et al, 2009;Fellbaum, 1999).Finally note that results are similar for otherparts of speech and other languages.5.2 Using Wiktionary?s translation links toimprove its synonymy networkAssuming that two words sharing many transla-tions in different languages are likely to be syn-onymous, we propose to use Wiktionary?s transla-tion links to enhance the synonymy network of agiven language.In order to rank links to be potentially added,we use a simple Jaccard measure: let Twbe the setof a word w?s translations, then for every coupleof words (w,w?)
we have:Jaccard(w,w?)
=|Tw?
Tw?||Tw?
Tw?|We compute this measure for every possible pairof words and then, starting from Wiktionary?s syn-onymy graph, we incrementally add links accord-ing to their Jaccard rank.25We notice first that most of synonymy linksadded by this method were not initially includedin Wiktionary?s synonymy network.
For exam-ple, regarding English verbs, 95% of 2000 bestranked proposed links are new.
Hence this methodmay be efficient to improve graph density.
How-ever one can wonder about the quality of the newadded links, so we discuss precision in the nextparagraph.In figure 3 is depicted the evolution of precision,recall and F-score for French verbs in the enlargedgraph in regard of the total number of edges.
Weuse Dicosyn graph as a gold standard.
The dashedline corresponds to theoretical scores one can ex-pect by adding randomly chosen links.First we notice that both precision and recallare significantly higher than we can expect fromrandom addition.
This confirms that words shar-ing the same translations are good synonym candi-dates.
Added links seem to be particularly relevantat the beginning for higher Jaccard scores.
Fromthe first dot to the second one we add about 1000edges (whereas the original graph contains 1792edges) and the precision only decreases from 0.71to 0.69.The methods we proposed in this section arequite simple and there is room for improvement.First, both methods can be combined in orderto improve the resource using translation linksand then using clusters structure.
One can alsothink to the corollary task that would consists inadding translation links between two languagesusing synonymy links of others languages.0 2000 4000 6000 8000 10000 120000.00.10.20.30.40.50.60.70.8Prandom0 2000 4000 6000 8000 10000 120000.020.040.060.080.100.120.140.16R0 2000 4000 6000 8000 10000 120000.040.060.080.100.120.140.160.180.200.22Ffr.VFigure 3: Precision, recall and F-score of French verbsgraph enlarged using translation links6 Conclusion and future workThis paper gave us the opportunity to share someWiktionary experience related lexical resourcesbuilding.
We presented in addition two approachesfor improving these resources and their evaluation.The first approach relies on the small world struc-ture of synonymy networks.
We postulated thatmany missing links in Wiktionary should be addedamong members of the same cluster.
The secondapproach assumes that two words sharing manytranslations in different languages are likely to besynonymous.
The comparison with traditional re-sources shows that our hypotheses are confirmed.We now plan to combine both approaches.The work presented in this paper combines aNLP contribution involving data extraction andrough processing of the data and a mathematicalcontribution concerning graph-like resource.
Inour viewpoint the second aspect of our work istherefore complementary of other NLP contribu-tions, like (Zesch et al, 2008b), involving moresophisticated NLP processing of the resource.Support for collaborative editing Our resultsshould be useful for setting up a more efficientframework for Wiktionary collaborative editing.We should be able to always propose a set of syn-onymy relations that are likely to be.
For exam-ple, when a contributor creates or edits an arti-cle, he may think about adding very few links butmight not bother providing an exhaustive list ofsynonyms.
Our tool can propose a list of potentialsynonyms, ordered by relevancy.
Each item of thislist would only need to be validated (or not).Diachronic study An interesting topic for futurework is a "diachronic" study of the resource.
Itis possible to access Wiktionary at several stages,this can be used for studying how such resourcesevolve.
Grounded on this kind of study, one maypredict the evolution of newer wiktionaries andforesee contributors?
NLP needs.
We would liketo set up a framework for everyone to test out newmethodologies for enriching and using Wiktionaryresources.
Such observatory, would allow to fol-low not only the evolution of Wiktionary but alsoof Wiktionary-grounded resources, that will onlyimprove thanks to steady collaborative develop-ment.Invariants and variabality Wiktionary as amassively mutiligual synonymy networks is anextremely promising resource for studying the(in)variability of semantic pairings such ashouse/family, child/fruit, feel/know... (Sweetser,1991; Gaume et al, 2009).
A systematic studywithin the semantic approximation frameworkpresented in the paper on Wiktionary data will becarried on in the future.26ReferencesA-L. Barabasi, R. Albert, H. Jeong, and G. Bianconi.2000.
Power-Law Distribution of the World WideWeb.
Science, 287.
(in Technical Comments).M.
Baroni, F. Chantree, A. Kilgarriff, and S. Sharoff.2008.
Cleaneval: a Competition for CleaningWeb Pages.
In Proceedings of the Conference onLanguage Resources and Evaluation (LREC), Mar-rakech.Encyclopaedia Britannica.
2006.
Fatally flawed: re-futing the recent study on encyclopedic accuracy bythe journal Nature.K.
Duvignau and B. Gaume.
2008.
Between wordsand world: Verbal "metaphor" as semantic or prag-matic approximation?
In Proceedings of Interna-tional Conference "Language, Communication andCognition", Brighton.C.
Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.C.
Fellbaum.
1999.
La repr?sentation des verbesdans le r?seau s?mantique Wordnet.
Langages,33(136):27?40.B.
Gaume, K. Duvignau, L. Pr?vot, and Y. Desalle.2008.
Toward a cognitive organization for electronicdictionaries, the case for semantic proxemy.
In Col-ing 2008: Proceedings of the Workshop on Cogni-tive Aspects of the Lexicon (COGALEX 2008), pages86?93, Manchester.B.
Gaume, K. Duvignau, and M. Vanhove.
2009.
Se-mantic associations and confluences in paradigmaticnetworks.
In M. Vanhove, editor, From Polysemy toSemantic Change: Towards a Typology of LexicalSemantic Associations, pages 233?264.
John Ben-jamins Publishing.J.
Giles.
2005.
Internet encyclopaedias go head tohead.
Nature, 438:900?901.C.
Jacquin, E. Desmontils, and L. Monceaux.
2002.French EuroWordNet Lexical Database Improve-ments.
In Proceedings of the Third InternationalConference on Intelligent Text Processing and Com-putational Linguistics (CICLING 2002), MexicoCity.F.
Keller and M. Lapata.
2002.
Using the web to over-come data sparseness.
In Proceedings of EMNLP-02, pages 230?237.A.
Kilgarriff and G. Grefenstette.
2003.
Introductionto the special issue on the web as corpus.
Computa-tional Linguistics, 29:333?347.A.
Kilgarriff.
1997.
I don?t believe in word senses.Computers and the humanities, 31(2):91?113.C.
Leacock and M. Chodorow.
1998.
Combining localcontext and wordnet similarity for word sense iden-tification.
In C. Fellbaum, editor, WordNet: An elec-tronic lexical database, pages 265?283.
MIT Press.M.
Newman.
2003.
The structure and function of com-plex networks.B.
Sagot and D. Fi?er.
2008.
Building a Free FrenchWordnet from Multilingual Resources.
In Proceed-ings of OntoLex 2008, Marrackech.M.
Steyvers and J.
B. Tenenbaum.
2005.
The large-scale structure of semantic networks: Statisticalanalyses and a model of semantic growth.
Cogni-tive Science, 29:41?78.E.
Sweetser.
1991.
From etymology to pragmatics.Cambridge University Press.D.
Tufis.
2000.
Balkanet design and development of amultilingual balkan wordnet.
Romanian Journal ofInformation Science and Technology, 7(1-2).B.
Victorri and C. Fuchs.
1996.
La polys?mie, con-struction dynamique du sens.
Herm?s.D.J.
Watts and S.H.
Strogatz.
1998.
Collective dynam-ics of small-world networks.
Nature, 393:440?442.T.
Zesch, C. M?ller, and I. Gurevych.
2008a.
Extract-ing Lexical Semantic Knowledge from Wikipediaand Wiktionary.
In Proceedings of the Conferenceon Language Resources and Evaluation (LREC),Marrakech.T.
Zesch, C. Muller, and I. Gurevych.
2008b.
Usingwiktionary for computing semantic relatedness.
InProceedings of 23rd AAAI Conference on ArtificialIntelligence.27
