Decoding Algorithm in Statistical Machine TranslationYe-Y i  Wang and  A lex  Waibe lLanguage Techno logy  Ins t i tu teSchool  of Computer  ScienceCarnegie  Mel lon Un ivers i ty5000 Forbes AvenueP i t t sburgh ,  PA 15213, USA{yyw, waibel}@cs, cmu.
eduAbst ractDecoding algorithm is a crucial part in sta-tistical machine translation.
We describea stack decoding algorithm in this paper.We present he hypothesis scoring methodand the heuristics used in our algorithm.We report several techniques deployed toimprove the performance of the decoder.We also introduce a simplified model tomoderate the sparse data problem and tospeed up the decoding process.
We evalu-ate and compare these techniques/modelsin our statistical machine translation sys-tem.1 In t roduct ion1.1 Stat is t ica l  Machine Trans lat ionStatistical machine translation is based on a channelmodel.
Given a sentence T in one language (Ger-man) to be translated into another language (En-glish), it considers T as the target of a communi-cation channel, and its translation S as the sourceof the channel.
Hence the machine translation taskbecomes to recover the source from the target.
Ba-sically every English sentence is a possible source fora German target sentence.
If we assign a probabilityP(S I T) to each pair of sentences (S, T), then theproblem of translation is to find the source S for agiven target T, such that P(S \[ T) is the maximum.According to Bayes rule,P(S IT)  = P (S)P (T  I S)P(T)  (1)Since the denominator is independent of S, we have-- arg maxP(S)P(T  I S) (2)STherefore a statistical machine translation systemmust deal with the following three problems:?
Modeling Problem: How to depict the processof generating a sentence in a source language,and the process used by a channel to generatea target sentence upon receiving a source sen-tence?
The former is the problem of languagemodeling, and the later is the problem of trans-lation modeling.
They provide a framework forcalculating P(S) and P(W I S) in (2).?
Learning Problem: Given a statistical languagemodel P(S) and a statistical translation modelP (T  I S), how to estimate the parameters inthese models from a bilingual corpus of sen-tences??
Decoding Problem: With a fully specified(framework and parameters) language andtranslation model, given a target sentence T,how to efficiently search for the source sentencethat satisfies (2).The modeling and learning issues have been dis-cussed in (Brown et ah, 1993), where ngram modelwas used for language modeling, and five differenttranslation models were introduced for the transla-tion process.
We briefly introduce the model 2 here,for which we built our decoder.In model 2, upon receiving a source English sen-tence e = el,.
?
-, el, the channel generates a Germansentence g = gl, ?
?
", g,n at the target end in the fol-lowing way:1.
With a distribution P(m I e), randomly choosethe length m of the German translation g. Inmodel 2, the distribution is independent of mand e:P(m \[ e) = ewhere e is a small, fixed number.2.
For each position i (0 < i < m) in g, find thecorresponding position ai in e according to anal ignment distribution P(a i  I i, a~ -1, m, e).
Inmodel 2, the distribution only depends on i, aiand the length of the English and German sen-tences:P(ai  l i, a~- l ,m,e)  = a(ai l i, m, l )3.
Generate the word gl at the position i of theGerman sentence from the English word ea~ at366the aligned position ai of gi, according to atranslation distribution P(gi t ~t~'~, st~i-t, e) =t(gl I ea~).
The distribution here only dependson gi and eai.Therefore, P(g l e) is the sum of the probabilitiesof generating  from e over all possible alignmentsA, in which the position i in the target sentence g isaligned to the position ai in the source sentence :P(gle) =I l me ~,  ... ~" IT t(g# le=jla(a~ Ij, l ,m)=a l=0 amm0j=lm !e 1"I ~ t(g# le,)a(ilj, t, m) (3)j= l  i=0(Brown et al, 1993) also described how to usethe EM algorithm to estimate the parameters a(i Ij, l, m) and $(g I e) in the aforementioned model.1.2 Decod ing  in Statistical MachineTrans lat ion(Brown et al, 1993) and (Vogel, Ney, and Tillman,1996) have discussed the first two of the three prob-lems in statistical machine translation.
Althoughthe authors of (Brown et al, 1993) stated that theywould discuss the search problem in a follow-up arti-?
cle, so far there have no publications devoted to thedecoding issue for statistical machine translation.On the other side, decoding algorithm is a crucialpart in statistical machine translation.
Its perfor-mance directly affects the quality and efficiency oftranslation.
Without a good and efficient decodingalgorithm, a statistical machine translation systemmay miss the best translation of an input sentenceeven if it is perfectly predicted by the model.2 S tack  Decod ing  A lgor i thmStack decoders are widely used in speech recognitionsystems.
The basic algorithm can be described asfollowing:1.
Initialize the stack with a null hypothesis.2.
Pop the hypothesis with the highest score offthe stack, name it as current -hypothes is .3. if cur rent -hypothes is  is a complete sentence,output it and terminate.4.
extend cur rent -hypothes is  by appending aword in the lexicon to its end.
Compute thescore of the new hypothesis and insert it intothe stack.
Do this for all the words in the lexi-con.5.
Go  to 2.2.1 Scoring the hypothesesIn stack search for statistical machine translation,a hypothesis H includes (a) the length l of thesource sentence, and (b) the prefix words in thesentence.
Thus a hypothesis can be written asH = l : ere2.. "ek, which postulates a source sen-tence of length l and its first k words.
The scoreof H, fit, consists of two parts: the prefix score gHfor ele2"" ek and the heuristic score hH for the partek+lek+2"-et that is yet to be appended to H tocomplete the sentence.2.1.1 Prefix score gH(3) can be used to assess a hypothesis.
Althoughit was obtained from the alignment model, it wouldbe easier for us to describe the scoring method ifwe interpret the last expression in the equation inthe following way: each word el in the hypothesiscontributes the amount e t(gj \[ ei)a(i l J, l, m) to theprobability of the target sentence word gj.
For eachhypothesis H = l : el,e2,-",ek, we use SH(j) todenote the probability mass for the target word glcontributed by the words in the hypothesis:kSH(j) = e~'~t(g~ lei)a(ilj, t,m) (4)i=0Extending H with a new word will increaseSn( j ) , l  < j < m.To make the score additive, the logarithm of theprobability in (3) was used.
So the prefix score con-tributed by the translation model is :~'\]~=0 log St/(j).Because our objective is to maximize P(e, g), wehave to include as well the logarithm of the languagemodel probability of the hypothesis in the score,therefore we havemg.
= ~IogS.
( j)  +j=0kE log P(el l ei-N+t'" el-l).i=0here N is the order of the ngram language model.The above g-score gH of a hypothesis H = l :ele?...ek can be calculated from the g-score of itsparent hypothesis P = l : ele2.. "ek-t:gH = gp+logP(ek lek -N+t ' ' ' ek - t )m+ ~-'~ log\[1 + et(gj l ek)a(k Ij, l, m)~=0 se( j )  \]SH(j) = Sp( j )+et(g j lek)a(k l j ,  l,m) (5)A practical problem arises here.
For a many earlystage hypothesis P, Sp(j) is close to 0.
This causesproblems because it appears as a denominator in (5)and the argument of the log function when calculat-ing gp.
We dealt with this by either limiting thetranslation probability from the null word (Brown367et al, 1993) at the hypothetical 0-position(Brown etal., 1993) over a threshold during the EM training,or setting SHo (j) to a small probability 7r instead of0 for the initial null hypothesis H0.
Our experimentsshow that lr = 10 -4 gives the best result.2.1.2 Heur i s t i csTo guarantee an optimal search result, the heuris-tic function must be an upper-bound of the scorefor all possible extensions ek+le/c+2...et(Nilsson,1971) of a hypothesis.
In other words, the benefitof extending a hypothesis hould never be under-estimated.
Otherwise the search algorithm will con-clude prematurely with a non-optimal hypothesis.On the other hand, if the heuristic function over-estimates the merit of extending a hypothesis toomuch, the search algorithm will waste a huge amountof time after it hits a correct result to safeguard theoptimality.To estimate the language model score h LM of theunrealized part of a hypothesis, we used the nega-tive of the language model perplexity PPtrain on thetraining data as the logarithm of the average proba-bility of predicting a new word in the extension froma history.
So we haveh LM = -(1 - k)PPtrai, + C. (6)Here is the motivation behind this.
We assume thatthe perplexity on training data overestimates thelikelihood of the forthcoming word string on av-erage.
However, when there are only a few wordsto be extended (k is close to 1), the language modelprobability for the words to be extended may bemuch higher than the average.
This is why the con-stant term C was introduced in (6).
When k << l,- ( l -k )PPtra in  is the dominating term in (6), so theheuristic language model score is close to the aver-age.
This can avoid overestimating the score toomuch.
As k is getting closer to l, the constant ermC plays a more important role in (6) to avoid un-derestimating the language model score.
In our ex-periments, we used C = PPtrain +log(Pmax), wherePm== is the maximum ngram probability in the lan-guage model.To estimate the translation model score, we intro-duce a variable va(j), the maximum contribution tothe probability of the target sentence word gj fromany possible source language words at any positionbetween i and l:vit(j) = max t(g~ \[e)a(klj, l ,m ).
(7)i<_/c<_l ,eEL~ " "here LE is the English lexicon.Since vit (j) is independent of hypotheses, it onlyneeds to be calculated once for a given target sen-tence.When k < 1, the heuristic function for the hypoth-esis H = 1 : ele2 -..e/c, is171hH = ~max{0,1og(v(/c+Dl(j)) - -  logSH(j)}j= l- ( t  - k)PP,~=., + c (8)where log(v(k+l)t( j ))-  logSg(j))  is the maximumincreasement that a new word can bring to the like-lihood of the j - th target word.When k = l, since no words can be appended tothe hypothesis, it is obvious that h~ = O.This heuristic function over-estimates the scoreof the upcoming words.
Because of the constraintsfrom language model and from the fact that a posi-tion in a source sentence cannot be occupied by twodifferent words, normally the placement of words inthose unfilled positions cannot maximize the likeli-hood of all the target words simultaneously.2.2 P run ing  and  abor t ing  searchDue to physical space limitation, we cannot keep allhypotheses alive.
We set a constant M, and when-ever the number of hypotheses exceeds M, the al-gorithm will prune the hypotheses with the lowestscores.
In our experiments, M was set to 20,000.There is time limitation too.
It is of little practicalinterest o keep a seemingly endless earch alive toolong.
So we set a constant T, whenever the decoderextends more than T hypotheses, it will abort thesearch and register a failure.
In our experiments, Twas set to 6000, which roughly corresponded to 2and half hours of search effort.2.3 Mu l t i -S tack  SearchThe above decoder has one problem: since theheuristic function overestimates the merit of ex-tending a hypothesis, the decoder always prefershypotheses of a long sentence, which have a bet-ter chance to maximize the likelihood of the targetwords.
The decoder will extend the hypothesis withlarge I first, and their children will soon occupy thestack and push the hypotheses of a shorter sourcesentence out of the stack.
If the source sentence isa short one, the decoder will never be able to findit, for the hypotheses leading to it have been prunedpermanently.This "incomparable" problem was solved withmulti-stack search(Magerman, 1994).
A separatestack was used for each hypothesized source sentencelength 1.
We do compare hypotheses in differentstacks in the following cases.
First, we compare acomplete sentence in a stack with the hypotheses inother stacks to safeguard the optimality of searchresult; Second, the top hypothesis in a stack is com-pared with that of another stack.
If the differenceis greater than a constant ~, then the less probableone will not be extended.
This is called soft-pruning,since whenever the scores of the hypotheses in otherstacks go down, this hypothesis may revive.368Z25000400O30002000100000 5 l0 15 20 25Sentence LengthEngfish - -30 35 405OOO4OOO3OOO1110110 5 I0 15 20 25Sentence LengthGerman - -30 35 40Figure 1: Sentence Length Distribution3 S tack  Search  w i th  a S impl i f iedMode lIn the IBM translation model 2, the alignment pa-rameters depend on the source and target sentencelength I and m. While this is an accurate model, itcauses the following difficulties:1. there are too many parameters and thereforetoo few trainingdata per parameter.
This maynot be a problem when massive training dataare available.
However, in our application, thisis a severe problem.
Figure 1 plots the lengthdistribution for the English and German sen-tences.
When sentences get longer, there arefewer training data available.2.
the search algorithm has to make multiple hy-potheses of different source sentence l ngth.
Foreach source sentence l ngth, it searches throughalmost the same prefix words and finally set-tles on a sentence length.
This is a very timeconsuming process and makes the decoder veryinefficient.To solve the first problem, we adjusted the countfor the parameter a(i \[ j, l, m) in the EM parameterestimation by adding to it the counts for the pa-rameters a(i l j, l', m'), assuming (l, m) and (1', m')are close enough.
The closeness were measured inmm'.
.
.
.
, .
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.- : , "  .
.
.
.
.
.
.
.
.
.
.
.
.
.
: , ' '.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
....# ....... ~ .
.
.~ .
.#  ....... #..~.
: .
.
.
.
.
.
.
.
.
?
.
.~ .
.
.~ .
.~ .
.
.~  .
.
.
.
.
.
.
.
.
.~1' 1Figure 2: Each x/y position represents a differentsource/target sentence length.
The dark dot at theintersection (l, m) corresponds to the set of countsfor the alignment parameters a(.
\[ o,l, m) in theEM estimation.
The adjusted counts are the sumof the counts in the neighboring sets residing insidethe circle centered at (1, m) with radius r. We tookr = 3 in our experiment.Euclidean distance (Figure 2).
So we havee(i I J, t, m) =e(i l j ,  l ' ,m';e,g ) (9)(I-l')~+(m-m')~<r~;e,gwhere ~(i I J, l, m) is the adjusted count for the pa-rameter a(i I J, 1, m), c(i I J, l, m; e, g) is the expectedcount for a(i I J, l, m) from a paired sentence (e g),and c( i l j ,  l ,m;e,g) = 0 when lel  l, or Igl ?
m,or i > l, or j > m.Although (9) can moderate the severity of the firstdata sparse problem, it does not ease the secondinefficiency problem at all.
We thus made a radi-cal change to (9) by removing the precondition that(l, m) and (l', m') must be close enough.
This re-sults in a simplified translation model, in which thealignment parameters are independent of the sen-tence length 1 and m:P( i l j ,  m,e) = P( i l j ,  l,m)-- a(i l J)here i , j  < Lm, and L,n is the maximum sentencelength allowed in the translation system.
A slightchange to the EM algorithm was made to estimatethe parameters.There is a problem with this model: given a sen-tence pair g and e, when the length of e is smallerthan  Lm, then the alignment parameters do not sumto 1:lela(ilj) < 1.
(10)i--0We deal with this problem by padding e to lengthLm with dummy words that never gives rise to anyword in the target of the channel.Since the parameters are independent of thesource sentence length, we do not have to make an369assumption about the length in a hypothesis.
When-ever a hypothesis ends with the sentence nd sym-bol </s> and its score is the highest, the decoderreports it as the search result.
In this case, a hypoth-esis can be expressed as H = e l ,e2, .
.
.
,ek ,  and IHIis used to denote the length of the sentence prefix ofthe hypothesis H, in this case, k.3.1 Heur i s t i csSince we do not make assumption of the source sen-tence length, the heuristics described above can nolonger be applied.
Instead, we used the followingheuristic function:h~./ = ~ max{0,1og( v IHI+I)(IHI+n)(j))} S.(j)-n  * PPt~ain + C (11)L. - IHIh.
= Pp(IHl+nlm)*h  (12)n----Ihere h~ is the heuristics for the hypothesis that ex-tend H with n more words to complete the sourcesentence (thus the final source sentence length is\[H\[ + n.) Pp(x \[ y) is the eoisson distribution of thesource sentence length conditioned on the target sen-tence length.
It is used to calculate the mean of theheuristics over all possible source sentence length, mis the target sentence length.
The parameters of thePoisson distributions can be estimated from trainingdata.4 Imp lementat ionDue to historical reasons, stack search got its currentname.
Unfortunately, the requirement for searchstates organization is far beyond what a stack andits push pop operations can handle.
What we reallyneed is a dynamic set which supports the followingoperations:1.
INSERT: to insert a new hypothesis into theset.2.
DELETE: to delete a state in hard pruning.3.
MAXIMUM: to find the state with the bestscore to extend.4.
MINIMUM: to find the state to be pruned.We used the Red-Black tree data structure (Cor-men, Leiserson, and Rivest, 1990) to implement thedynamic set, which guarantees that the above oper-ations take O(log n) time in the worst case, where nis the number of search states in the set.5 Per fo rmanceWe tested the performance of the decoders withthe scheduling corpus(Suhm et al, 1995).
Around30,000 parallel sentences (400,000 words altogetherfor both languages) were used to train the IBMmodel 2 and the simplified model with the EM algo-rithm.
A larger English monolingual corpus witharound 0.5 million words was used to train a bi-gram for language modelling.
The lexicon contains2,800 English and 4,800 German words in morpho-logically inflected form.
We did not do any prepro-cessing/analysis of the data as reported in (Brownet al, 1992).5.1 Decoder  Success RateTable 1 shows the success rate of three mod-els/decoders.
As we mentioned before, the compari-son between hypotheses of different sentence lengthmade the single stack search for the IBM model 2fail (return without a result) on a majority of thetest sentences.
While the multi-stack decoder im-proved this, the simplified model/decoder p oducedan output for all the 120 test sentences.5.2 T rans la t ion  AccuracyUnlike the case in speech recognition, it is quitearguable what "accurate translations" means.
Inspeech recognition an output can be compared withthe sample transcript of the test data.
In machinetranslation, a sentence may have several egitimatetranslations.
It is difficult to compare an outputfrom a decoder with a designated translation.
In-stead, we used human subjects to judge the machine-made translations.
The translations are classifiedinto three categories 1.1.
Correct translations: translations that aregrammatical nd convey the same meaning asthe inputs.2.
Okay translations: translations that convey thesame meaning but with small grammatical mis-takes or translations that convey most but notthe entire meaning of the input.3.
Incorrect translations: Translations that areungrammatical or convey little meaningful in-formation or the information is different fromthe input.Examples of correct, okay, and incorrect ransla-tions are shown in Table 2.Table 3 shows the statistics of the translation re-sults.
The accuracy was calculate by crediting a cor-rect translation 1 point and an okay translation 1/2point.There are two different kinds of errors in statis-tical machine translation.
A modeling erivr occurswhen the model assigns a higher score to an incor-rect translation than a correct one.
We cannot doanything about this with the decoder.
A decoding1 This is roughly the same as the classification i IBMstatistical translation, except we do not have "legitimatetranslation that conveys different meaning from the in-put" - -  we did not observed this case in our outputs.370Model 2, Single StackModel 2, Multi-StackSimplified ModelTotal Test Sentences Decoded Sentenced Failed sentences120 32 88120 83 37120 120 0Table 1: Decoder Success RateCorrectOkayIncorrectGermanEnglish (target)English (output)GermanEnglish/target)English (output)GermanEnglish (target)English/output/GermanEnglish/target)English (output)GermanEnglish (target)English (output)GermanEnglish (target)English (output)ich habe ein Meeting yon halb zehn bis um zwSlfI have a meeting from nine thirty to twelveI have a meeting from nine thirty to twelveversuchen wir sollten es vielleicht mit einem anderen Terminwe might want to try for some other timewe should try another timeich glaube nicht diis ich noch irgend etwas im Januar frei habeI do not think I have got anything open m JanuaryI think I will not free in Januaryich glaube wit sollten em weiteres Meeting vereinbarenI think we have to have another meetingI think we should fix a meetingschlagen Sie doch einen Termin vorwhy don't you suggest a timewhy you an appointmentich habe Zeit fiir den Rest des TagesI am free the rest of itI have time for the rest of JulyTable 2: Examples of Correct, Okay, and Incorrect Translations: for each translation, the first line is aninput German sentence, the second line is the human made (target) translation for that input sentence, andthe third line is the output from the decoder.error or search error happens when the search al-gorithm misses a correct translation with a higherscore.When evaluating a decoding algorithm, it wouldbe attractive if we can tell how many errors arecaused by the decoder.
Unfortunately, this is notattainable.
Suppose that we are going to translate aGerman sentence g, and we know from the samplethat e is one of its possible English translations.
Thedecoder outputs an incorrect e~ as the translation ofg.
If the score of e' is lower than that of e, we knowthat a search error has occurred.
On the other hand,if the score of e' is higher, we cannot decide if it is amodeling error or not, since there may still be otherlegitimate translations with a score higher than e ~- -  we just do not know what they are.Although we cannot distinguish a modeling errorfrom a search error, the comparison between the de-coder output's core and that of a sample transla-tion can still reveal some information about the per-formance of the decoder.
If we know that the de-coder can find a sentence with a better score thana "correct" translation, we will be more confidentthat the decoder is less prone to cause errors.
Ta-ble 4 shows the comparison between the score of theoutputs from the decoder and the score of the sam-ple translations when the outputs are incorrect.
Inmost cases, the incorrect outputs have a higher scorethan the sample translations.
Again, we consider a"okay" translation a half error here.This result hints that model deficiencies may be amajor source of errors.
The models we used here arevery simple.
With a more sophisticated model, moretraining data, and possibly some preprocessing, thetotal error rate is expected to decrease.5.3 Decod ing  SpeedAnother important issue is the efficiency of the de-coder.
Figure 3 plots the average number of statesbeing extended by the decoders.
It is grouped ac-cording to the input sentence length, and evaluatedon those sentences on which the decoder succeeded.The average number of states being extended inthe model 2 single stack search is not available forlong sentences, ince the decoder failed on most ofthe long sentences.The figure shows that the simplified model/decoderworks much more efficiently than the other mod-371TotalModel 2, Multi-Stack 83Simplified Model 120Correct Okay Incorrect Accuracy39 12 32 54.2%64 15 41 59.6%Table 3: Translation AccuracyModel 2, Multi-StackSimplified ModelTotal Errors Scoree > Scoree, Scoree < Seoree,38 3.5 (7.9%) 34.5 (92.1%)48.5 4.5 (9.3%) 44 (90.7%)Table 4: Sample Translations versus Machine-Made Translations60005000~d40003000=~ 2ooo Z10oo<0 j..Zh1-4"Model2-Single-S tack" , ,"Model2-Mult i-Stack" - -~"Simpli f ied-Moder'  , .........ii5-8 9-12 13-16 17-20Target Sentence LengthFigure 3: Extended States versus Target SentenceLengthels/decoders.6 Conc lus ionsWe have reported astack decoding algorithm for theIBM statistical translation model 2 and a simpli-fied model.
Because the simplified model has feweruarameters and does not have to posit hypotheseswith the same prefixes but different length, it out-performed the IBM model 2 with regard to bothaccuracy and efficiency, especially in our applicationthat lacks a massive amount of training data.
Inmost cases, the erroneous outputs from the decoderhave a higher score than the human made transla-tions.
Therefore it is less likely that the decoder isa major contributor of translation errors.7 AcknowledgementsWe would like to thank John Lafferty for enlight-ening discussions on this work.
We would also liketo thank the anonymous ACL reviewers for valuablecomments.
This research was partly supported byATR and the Verbmobil Project.
The vmws andconclusions in this document are those of the au-thors.Re ferencesBrown, P. F., S. A. Dellaopietra, V. J Della-Pietra,and R. L. Mercer.
1993.
The Mathematics ofSta-tistical Machine Translation: Parameter Estima-tion.
Computational Linguistics, 19(2):263-311.Brown, P. F., S. A. Della Pietra, V. J. Della Pietra,J.
D. Lafferty, and R. L. Mercer.
1992.
Analy-sis, Statistical Transfer, and Synthesis in MachineTranslation.
In Proceedings of the fourth Interna-tional Conference on Theoretical and Methodolog-ical Issues in Machine Translation, pages 83-100.Cormen, Thomas H., Charles E. Leiserson, andRonald L. Rivest.
1990.
Introduction to Al-gorithms.
The MIT Press, Cambridge, Mas-sachusetts.Magerman, D. 1994.
Natural Language Parsingas Statistical Pattern Recognition.
Ph.D. thesis,Stanford University.Nilsson, N. 1971.
Problem-Solving Methods in Arti-ficial Intelligence.
McGraw Hill, New York, NewYork.Suhm, B., P.Geutner, T. Kemp, A. Lavie, L. May-field, A. McNair, I. Rogina, T. Schultz, T. Slo-boda, W. Ward, M. Woszczyna, and A. Waibel.1995.
JANUS: Towards multilingual spoken lan-guage translation.
In Proceedings of the ARPASpeech Spoken Language Technology Workshop,Austin, TX, 1995.Vogel, S., H. Ney, and C. Tillman.
1996.
HMM-Based Word Alignment in Statistical Transla-tion.
In Proceedings of the Seventeenth Interna-tional Conference on Computational Linguistics:COLING-96, pages 836-841, Copenhagen, Den-mark.372
