Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 281?290,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsStop-probability estimates computed on a large corpusimprove Unsupervised Dependency ParsingDavid Marec?ek and Milan StrakaCharles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal and Applied LinguisticsMalostranske?
na?me?st??
25, 11800 Prague, Czech Republic{marecek,straka}@ufal.mff.cuni.czAbstractEven though the quality of unsuperviseddependency parsers grows, they often failin recognition of very basic dependencies.In this paper, we exploit a prior knowledgeof STOP-probabilities (whether a givenword has any children in a given direc-tion), which is obtained from a large rawcorpus using the reducibility principle.
Byincorporating this knowledge into Depen-dency Model with Valence, we managed toconsiderably outperform the state-of-the-art results in terms of average attachmentscore over 20 treebanks from CoNLL 2006and 2007 shared tasks.1 IntroductionThe task of unsupervised dependency parsing(which strongly relates to the grammar inductiontask) has become popular in the last decade, andits quality has been greatly increasing during thisperiod.The first implementation of Dependency Modelwith Valence (DMV) (Klein and Manning, 2004)with a simple inside-outside inference algo-rithm (Baker, 1979) achieved 36% attachmentscore on English and was the first system outper-forming the adjacent-word baseline.1Current attachment scores of state-of-the-art un-supervised parsers are higher than 50% for manylanguages (Spitkovsky et al, 2012; Blunsom andCohn, 2010).
This is still far below the super-vised approaches, but their indisputable advan-tage is the fact that no annotated treebanks areneeded and the induced structures are not bur-dened by any linguistic conventions.
Moreover,1The adjacent-word baseline is a dependency tree inwhich each word is attached to the previous (or the follow-ing) word.
The attachment score of 35.9% on all the WSJtest sentences was taken from (Blunsom and Cohn, 2010).supervised parsers always only simulate the tree-banks they were trained on, whereas unsupervisedparsers have an ability to be fitted to different par-ticular applications.Some of the current approaches are based onthe DMV, a generative model where the gram-mar is expressed by two probability distributions:Pchoose(cd|ch, dir), which generates a new childcd attached to the head ch in the direction dir (leftor right), and Pstop(STOP |ch, dir , ?
?
?
), whichmakes a decision whether to generate anotherchild of ch in the direction dir or not.2 Such agrammar is then inferred using sampling or varia-tional methods.Unfortunately, there are still cases where the in-ferred grammar is very different from the gram-mar we would expect, e.g.
verbs become leavesinstead of governing the sentences.
Rasooli andFaili (2012) and Bisk and Hockenmaier (2012)made some efforts to boost the verbocentricity ofthe inferred structures; however, both of the ap-proaches require manual identification of the POStags marking the verbs, which renders them use-less when unsupervised POS tags are employed.The main contribution of this paper is a consid-erable improvement of unsupervised parsing qual-ity by estimating the Pstop probabilities externallyusing a very large corpus, and employing this priorknowledge in the standard inference of DMV.
Theestimation is done using the reducibility principleintroduced in (Marec?ek and Z?abokrtsky?, 2012).The reducibility principle postulates that if a word(or a sequence of words) can be removed froma sentence without violating its grammatical cor-rectness, it is a leaf (or a subtree) in its dependencystructure.
For the purposes of this paper, we as-sume the following hypothesis:If a sequence of words can be removed from2The Pstop probability may be conditioned by additionalparameters, such as adjacency adj or fringe word cf , whichwill be described in Section 4.281Figure 1: Example of a dependency tree.
Se-quences of words that can be reduced are under-lined.a sentence without violating its grammatical cor-rectness, no word outside the sequence depends onany word in the sequence.Our hypothesis is a generalization of the origi-nal hypothesis since it allows a reducible sequenceto form several adjacent subtrees.Let?s outline the connection between the Pstopprobabilities and the property of reducibility.
Fig-ure 1 shows an example of a dependency tree.
Se-quences of reducible words are marked by thicklines below the sentence.
Consider for examplethe word ?further?.
It can be removed and thus,according to our hypothesis, no other word de-pends on it.
Therefore, we can deduce that thePstop probability for such word is high both forthe left and for the right direction.
The phrase?for further discussions?
is reducible as well andwe can deduce that the Pstop of its first word(?for?)
in the left direction is high since it cannothave any left children.
We do not know anythingabout its right children, because they can be lo-cated within the sequence (and there is really onein Figure 1).
Similarly, the word ?discussions?,which is the last word in this sequence, cannothave any right children and we can estimate that itsright Pstop probability is high.
On the other hand,non-reducible words such, as the verb ?asked?
inour example, can have children, and therefore theirPstop can be estimated as low for both directions.The most difficult task in this approach is to au-tomatically recognize reducible sequences.
Thisproblem, together with the estimation of the stop-probabilities, is described in Section 3.
Ourmodel, not much different from the classic DMV,is introduced in Section 4.
Section 5 describes theinference algorithm based on Gibbs sampling.
Ex-periments and results are discussed in Section 6.Section 7 concludes the paper.2 Related WorkReducibility: The notion of reducibility belongsto the traditional linguistic criteria for recogniz-ing dependency relations.
As mentioned e.g.
byKu?bler et al (2009), the head h of a construction cdetermines the syntactic category of c and can of-ten replace c. In other words, the descendants of hcan be often removed without making the sentenceincorrect.
Similarly, in the Dependency Analysisby Reduction (Lopatkova?
et al, 2005), the authorsassume that stepwise deletions of dependent ele-ments within a sentence preserve its syntactic cor-rectness.
A similar idea of dependency analysisby splitting a sentence into all possible acceptablefragments is used by Gerdes and Kahane (2011).We have directly utilized the aforementionedcriteria for dependency relations in unsuper-vised dependency parsing in our previous pa-per (Marec?ek and Z?abokrtsky?, 2012).
Our depen-dency model contained a submodel which directlyprioritized subtrees that form reducible sequencesof POS tags.
Reducibility scores of given POS tagsequences were estimated using a large corpus ofWikipedia articles.
The weakness of this approachwas the fact that longer sequences of POS tagsare very sparse and no reducibility scores couldbe estimated for them.
In this paper, we avoid thisshortcoming by estimating the STOP probabilitiesfor individual POS tags only.Another task related to reducibility is sentencecompression (Knight and Marcu, 2002; Cohn andLapata, 2008), which was used for text summa-rization.
The task is to shorten the sentences whileretaining the most important pieces of informa-tion, using the knowledge of the grammar.
Con-versely, our task is to induce the grammar usingthe sentences and their shortened versions.Dependency Model with Valence (DMV) hasbeen the most popular approach to unsuperviseddependency parsing in the recent years.
It was in-troduced by Klein and Manning (2004) and fur-ther improved by Smith (2007) and Cohen et al(2008).
Headden III et al (2009) introduce theExtended Valence Grammar and add lexicaliza-tion and smoothing.
Blunsom and Cohn (2010)use tree substitution grammars, which allow learn-ing of larger dependency fragments by employ-ing the Pitman-Yor process.
Spitkovsky et al(2010) improve the inference using iterated learn-ing of increasingly longer sentences.
Further im-provements were achieved by better dealing withpunctuation (Spitkovsky et al, 2011b) and new?boundary?
models (Spitkovsky et al, 2012).282Other approaches to unsupervised dependencyparsing were described e.g.
in (S?gaard, 2011),(Cohen et al, 2011), and (Bisk and Hockenmaier,2012).
There also exist ?less unsupervised?
ap-proaches that utilize an external knowledge of thePOS tagset.
For example, Rasooli and Faili (2012)identify the last verb in the sentence, minimizeits probability of reduction and thus push it tothe root position.
Naseem et al (2010) make useof manually-specified universal dependency rulessuch as Verb?Noun, Noun?Adjective.
McDon-ald et al (2011) identify the POS tags by a cross-lingual transfer.
Such approaches achieve betterresults; however, they are useless for grammar in-duction from plain text.3 STOP-probability estimation3.1 Recognition of reducible sequencesWe introduced a simple procedure for recog-nition of reducible sequences in (Marec?ek andZ?abokrtsky?, 2012): The particular sequence ofwords is removed from the sentence and if theremainder of the sentence exists elsewhere in thecorpus, the sequence is considered reducible.
Weprovide an example in Figure 2.
The bigram ?thisweekend?
in the sentence ?The next competitionis this weekend at Lillehammer in Norway.?
is re-ducible since the same sentence without this bi-gram, i.e., ?The next competition is at Lilleham-mer in Norway.
?, is in the corpus as well.
Simi-larly, the prepositional phrase ?of Switzerland?
isalso reducible.It is apparent that only very few reducible se-quences can be found by this procedure.
If weuse a corpus containing about 10,000 sentences, itis possible that we found no reducible sequencesat all.
However, we managed to find a sufficientamount of reducible sequences in corpora contain-ing millions of sentences, see Section 6.1 and Ta-ble 1.3.2 Computing the STOP-probabilityestimationsRecall our hypothesis from Section 1: If a se-quence of words is reducible, no word outside thesequence can depend on any word in the sequence.Or, in terms of dependency structure: A reduciblesequence consists of one or more adjacent sub-trees.
This means that the first word of a reduciblesequence does not have any left children and, sim-ilarly, the last word in a reducible sequence doesMartin Fourcade was sixth , maintaining his lead at the top ofthe overall World Cup standings , although Svendsen is nowonly 59 points away from the Frenchman in second .
The nextcompetition is this weekend at Lillehammer in Norway .Larinto saw off allcomers at Kuopio with jumps of 129.5 and124m for a total 240.9 points , just 0.1 points ahead ofcompatriot Matti Hautamaeki , who landed efforts of 127 and129.5m .
Third place went to Simon Ammann .
AndreasKofler , who won at the weekend at Kuusamo , was fourth butstays top of the season standings with 150 points .Third place went to Simon Ammann of Switzerland .
Ammannis currently just fifth , overall with 120 points .
The nextcompetition is at Lillehammer in Norway .Figure 2: Example of reducible sequences ofwords found in a large corpus.not have any right children.
We make use of thisproperty directly for estimating Pstop probabili-ties.Hereinafter, P eststop(ch, dir) denotes the STOP-probability we want to estimate from a large cor-pus; ch is the head?s POS tag and dir is the direc-tion in which the STOP probability is estimated.If ch is very often in the first position of reduciblesequences, P eststop(ch, left) will be high.
Similarly,if ch is often in the last position of reducible se-quences, P eststop(ch, right) will be high.For each POS tag ch in the given corpus,we first compute its left and right ?raw?
scoreSstop(ch, left) and Sstop(ch, right) as the relativenumber of times a word with POS tag ch was inthe first (or last) position in a reducible sequencefound in the corpus.
We do not deal with se-quences longer than a trigram since they are highlybiased.Sstop(ch, left) =# red.seq.
[ch, .
.
. ]
+ ?# ch in the corpusSstop(ch, right) =# red.seq.
[.
.
.
, ch] + ?# ch in the corpusNote that the Sstop scores are not probabilities.Their main purpose is to sort the POS tags accord-ing to their ?reducibility?.It may happen that for many POS tags thereare no reducible sequences found.
To avoid zeroscores, we use a simple smoothing by adding ?
toeach count:?
= # all reducible sequencesW ,283where W denotes the number of words in thegiven corpus.
Such smoothing ensures that morefrequent irreducible POS tags get a lower Sstopscore than the less frequent ones.Since reducible sequences found are verysparse, the values of Sstop(ch, dir) scores are verysmall.
To convert them to estimated probabilitiesP eststop(ch, dir), we need a smoothing that fulfillsthe following properties:(1) P eststop is a probability and therefore its valuemust be between 0 and 1.
(2) The number of no-stop decisions (no matterin which direction) equals to W (number ofwords) since such decision is made beforeeach word is generated.
The number of stopdecisions is 2W since they come after gener-ating the last children in both the directions.Therefore, the average P eststop(h, dir) over allwords in the treebank should be 2/3.After some experimenting, we chose the follow-ing normalization formulaP eststop(ch, dir) =Sstop(ch, dir)Sstop(ch, dir) + ?with a normalization constant ?.
The condition(1) is fulfilled for any positive value of ?.
Its exactvalue is set in accordance with the requirement (2)so that the average value of P eststop is 2/3.?dir?
{l,r}?c?Ccount(c)P eststop(c, dir) =23 ?
2W,where count(c) is the number of words with POStag c in the corpus.
We find the unique value of ?that fulfills the previous equation numerically us-ing a binary search algorithm.4 ModelWe use the standard generative DependencyModel with Valence (Klein and Manning, 2004).The generative story is the following: First, thehead of the sentence is generated.
Then, for eachhead, all its left children are generated, then theleft STOP, then all its right children, and then theright STOP.
When a child is generated, the al-gorithm immediately recurses to generate its sub-tree.
When deciding whether to generate anotherchild in the direction dir or the STOP symbol,we use the P dmvstop (STOP |ch, dir , adj , cf ) model.The new child cd in the direction dir is generatedaccording to the Pchoose(cd|ch, dir) model.
Theprobability of the whole dependency tree T is thefollowing:Ptree(T ) = Pchoose(head(T )|ROOT , right)?
Ptree(D(head(T )))Ptree(D(ch)) =?dir?
{l,r}?cd?deps(dir,h)P dmvstop (?STOP |ch, dir , adj , cf )Pchoose(cd|ch, dir)Ptree(D(cd))P dmvstop (STOP |ch, dir , adj , cf ),where Ptree(D(ch)) is probability of the subtreegoverned by h in the tree T .The set of features on which the P dmvstop andPchoose probabilities are conditioned varies amongthe previous works.
Our P dmvstop depends on thehead POS tag ch, direction dir , adjacency adj ,and fringe POS tag cf (described below).
Theuse of adjacency is standard in DMV and enablesus to have different P dmvstop for situations when nochild was generated so far (adj = 1).
That is,P dmvstop (ch, dir , adj = 1, cf ) decides whether theword ch has any children in the direction dir atall, whereas P dmvstop (h, dir , adj = 0, cf ) decideswhether another child will be generated next tothe already generated one.
This distinction is ofcrucial importance for us: although we know howto estimate the STOP probabilities for adj = 1from large data, we do not know anything aboutthe STOP probabilities for adj = 0.The last factor cf , called fringe, is the POS tagof the previously generated sibling in the currentdirection dir .
If there is no such sibling (in caseadj = 1), the head ch is used as the fringe cf .This is a relatively novel idea in DMV, introducedby Spitkovsky et al (2012).
We decided to usethe fringe word in our model since it gives slightlybetter results.We assume that the distributions of Pchoose andP dmvstop are good if the majority of the probabil-ity mass is concentrated on few factors; therefore,we apply a Chinese Restaurant process (CRP) onthem.The probability of generating a new child nodecd attached to ch in the direction dir given the his-tory (all the nodes we have generated so far) is284computed using the following formula:Pchoose(cd|ch, dir) ==?c 1|C| + count?
(cd, ch, dir)?c + count?
(ch, dir),where count?
(cd, ch, dir) denotes the number oftimes a child node cd has been attached to chin the direction dir in the history.
Similarly,count?
(ch, dir) is the number of times somethinghas been attached to ch in the direction dir .
The?c is a hyperparameter and |C| is the number ofdistinct POS tags in the corpus.3The STOP probability is computed in a similarway:P dmvstop (STOP |ch, dir , adj , cf ) ==?s23 + count?
(STOP , ch, dir , adj , cf )?s + count?
(ch, dir , adj , cf )where count?
(STOP , ch, dir , adj , cf ) is thenumber of times a head ch had the last child cfin the direction dir in the history.The contribution of this paper is the inclusionof the stop-probability estimates into the DMV.Therefore, we introduce a new model P dmv+eststop ,in which the probability based on the previouslygenerated data is linearly combined with the prob-ability estimates based on large corpora (Sec-tion 3).P dmv+eststop (STOP |ch, dir , 1, cf ) == (1?
?)
?
?s23 + count?
(STOP , ch, dir , 1, cf )?s + count?
(ch, dir , 1, cf )+?
?
P eststop(ch, dir)P dmv+eststop (STOP |ch, dir , 0, cf ) == P dmvstop (STOP |ch, dir , 0, cf )The hyperparameter ?
defines the ratio betweenthe CRP-based and estimation-based probability.The definition of the P dmv+eststop for adj = 0 equalsthe basic P dmvstop since we are able to estimate onlythe probability whether a particular head POS tagch can or cannot have children in a particular di-rection, i.e if adj = 1.3The number of classes |C| is often used in the denomi-nator.
We decided to put its reverse value into the numeratorsince we observed such model to perform better for a constantvalue of ?c over different languages and tagsets.Finally, we obtain the probability of the wholegenerated treebank as a product over the trees:Ptreebank =?T?treebankPtree(T ).An important property of the CRP is the fact thatthe factors are exchangeable ?
i.e.
no matter howthe trees are ordered in the treebank, the Ptreebankis always the same.5 InferenceWe employ the Gibbs sampling algorithm (Gilkset al, 1996).
Unlike in (Marec?ek and Z?abokrtsky?,2012), where edges were sampled individually,we sample whole trees from all possibilities on agiven sentence using dynamic programming.
Thealgorithm works as follows:1.
A random projective dependency tree is as-signed to each sentence in the corpus.2.
Sampling: We go through the sentences in arandom order.
For each sentence, we sam-ple a new dependency tree based on all othertrees that are currently in the corpus.3.
Step 2 is repeated in many iterations.
Inthis work, the number of iterations was setto 1000.4.
After the burn-in period (which was set to thefirst 500 iterations), we start collecting countsof edges between particular words that ap-peared during the sampling.5.
Parsing: Based on the collected counts, wecompute the final dependency trees usingthe Chu-Liu/Edmonds?
algorithm (1965) forfinding maximum directed spanning trees.5.1 SamplingOur goal is to sample a new projective dependencytree T with probability proportional to Ptree(T ).Since the factors are exchangeable, we can dealwith any tree as if it was the last one in the corpus.We use dynamic programming to sample atree with N nodes in O(N4) time.
Neverthe-less, we sample trees using a modified probabil-ity P ?tree(T ).
In Ptree(T ), the probability of anedge depends on counts of all other edges, includ-ing the edges in the same tree.
We instead useP ?tree(T ), where the counts are computed usingonly the other trees in the corpus, i.e., probabilities285of edges of T are independent.
There is a stan-dard way to sample using the real Ptree(T ) ?
wecan use P ?tree(T ) as a proposal distribution in theMetropolis-Hastings algorithm (Hastings, 1970),which then produces trees with probabilities pro-portional to Ptree(T ) using acceptance-rejectionscheme.
We do not take this approach and wesample proportionally to P ?tree(T ) only, becausewe believe that for large enough corpora, the twodistributions are nearly identical.To sample a tree containing words w1, .
.
.
, wNwith probability proportional to P ?tree(T ), we firstcompute three tables:?
ti(g, i, j) for g < i or g > j is the sum ofprobabilities of any tree on words wi, .
.
.
, wjwhose root is a child of wg, but not an outer-most child in its direction;?
to(g, i, j) is the same, but the tree is the out-ermost child of wg;?
fo(g, i, j) for g < i or g > j is thesum of probabilities of any forest on wordswi, .
.
.
, wj , such that all the trees are childrenof wg and are the outermost children of wg intheir direction.All the probabilities are computed using the P ?tree .If we compute the tables inductively from thesmallest trees to the largest trees, we can precom-pute all the O(N3) values in O(N4) time.Using these tables, we sample the tree recur-sively, starting from the root.
At first, we sam-ple the root r proportionally to the probability ofa tree with the root r, which is a product of theprobability of left children of r and right chil-dren of r. The probability of left children of ris either P ?stop(STOP |r, left) if r has no children,or P ?stop(?STOP |r, left)fo(r, 1, r?
1) otherwise;the probability of right children is analogous.After sampling the root, we sample the rangesof its left children, if any.
We sample the first leftchild range l1 proportionally either to to(r, 1, r?1)if l1 = 1, or to ti(r, l1, r ?
1)fo(r, 1, l1 ?
1)if l1 > 1.
Then we sample the second left childrange l2 proportionally either to to(r, 1, l1 ?
1)if l2 = 1, or to ti(r, l2, l1 ?
1)fo(r, 1, l2 ?
1)if l2 > 1, and so on, while there are any leftchildren.
The right children ranges are sampledsimilarly.
Finally, we recursively sample the chil-dren, i.e., their roots, their children and so on.
Itis simple to verify using the definition of Ptree thatthe described method indeed samples trees propor-tionally to P ?tree .5.2 ParsingBeginning the 500th iteration, we start collectingcounts of individual dependency edges during theremaining iterations.
After each iteration is fin-ished (all the trees in the corpus are re-sampled),we increment the counter of all directed pairs ofnodes which are connected by a dependency edgein the current trees.After the last iteration, we use these collectedcounts as weights and compute maximum directedspanning trees using the Chu-Liu/Edmonds?
algo-rithm (Chu and Liu, 1965).
Therefore, the result-ing trees consist of edges maximizing the sum ofindividual counts:TMST = argmaxT?e?Tcount(e)It is important to note that the MST algorithmmay produce non-projective trees.
Even if weaverage the strictly projective dependency trees,some non-projective edges may appear in the re-sult.
This might be an advantage since correctnon-projective edges can be predicted; however,this relaxation may introduce mistakes as well.6 Experiments6.1 DataWe use two types of resources in our experiments.The first type are CoNLL treebanks from the year2006 (Buchholz and Marsi, 2006) and 2007 (Nivreet al, 2007), which we use for inference and forevaluation.
As is the standard practice in unsuper-vised parsing evaluation, we removed all punctu-ation marks from the trees.
In case a punctuationnode was not a leaf, its children are attached to theparent of the removed node.For estimating the STOP probabilities (Sec-tion 3), we use the Wikipedia articles from W2Ccorpus (Majlis?
and Z?abokrtsky?, 2012), which pro-vide sufficient amount of data for our purposes.Statistics across languages are shown in Table 1.The Wikipedia texts were automatically tok-enized and segmented to sentences so that theirtokenization was similar to the one in the CoNLLevaluation treebanks.
Unfortunately, we were notable to find any segmenter for Chinese that wouldproduce a desired segmentation; therefore, we re-moved Chinese from evaluation.The next step was to provide the Wikipediatexts with POS tags.
We employed the TnT tag-ger (Brants, 2000) which was trained on the re-286language tokens red.
language tokens red.(mil.)
seq.
(mil.)
seq.Arabic 19.7 546 Greek 20.9 1037Basque 14.1 645 Hungarian 26.3 2237Bulgarian 18.8 1808 Italian 39.7 723Catalan 27.0 712 Japanese 2.6 31Czech 20.3 930 Portuguese 31.7 4765Danish 15.9 576 Slovenian 13.7 513Dutch 27.1 880 Spanish 53.4 1156English 85.0 7603 Swedish 19.2 481German 56.9 1488 Turkish 16.5 5706Table 1: Wikipedia texts statistics: total number oftokens and number of reducible sequences foundin them.spective CoNLL training data.
The quality of suchtagging is not very high since we do not use anylexicons or pretrained models.
However, it is suf-ficient for obtaining usable stop probability esti-mates.6.2 Estimated STOP probabilitiesWe applied the algorithm described in Section 3 onthe prepared Wikipedia corpora and obtained thestop-probabilities P eststop in both directions for allthe languages and their POS tags.
To evaluate thequality of our estimations, we compare them withP tbstop , the stop probabilities computed directly onthe evaluation treebanks.
The comparisons on fiveselected languages are shown in Figure 3.
The in-dividual points represent the individual POS tags,their size (area) shows their frequency in the par-ticular treebank.
The y-axis shows the stop prob-abilities estimated on Wikipedia by our algorithm,while the x-axis shows the stop probabilities com-puted on the evaluation CoNLL data.
Ideally, thecomputed and estimated stop probabilities shouldbe the same, i.e.
all the points should be on thediagonal.Let?s focus on the graphs for English.
Ourmethod correctly recognizes that adverbs RB andadjectives JJ are often leaves (their stop proba-bilities in both directions are very high).
More-over, the estimates for RB are even higher thanJJ, which will contribute to attaching adverbs toadjectives and not reversely.
Nouns (NN, NNS)are somewhere in the middle, the stop probabili-ties for proper nouns (NNP) are estimated higher,which is correct since they have much less modi-fiers then the common nouns NN.
The determin-ers are more problematic.
Their estimated stopprobability is not very high (about 0.65), while inthe real treebank they are almost always leaves.00.20.40.60.810 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1estimation from Wikiestimation computed on the treebankEnglish left-stop English right-stopNNNNPINDTNNSJJVBDRBNNNNPINDTNNSJJVBDRB00.20.40.60.810 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 100.20.40.60.810 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 100.20.40.60.810 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1German left-stop German right-stopSpanish left-stop Spanish right-stopCzech left-stop Czech right-stop00.20.40.60.810 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Hungarian left-stop Hungarian right-stopestimation from Wikiestimation from Wikiestimation from Wikiestimation from WikiNNDbZ:J^RRVBAAVfVpC=NNVVFINAPPRARTADVVAFINADJANNDbZ:J^RRVBAAVfVpC=NNARTAPPRADJAADVVAFINVVFINNEncdanp spaqvmFcccnc danpspaqvmFcccNcAfTfNpVmCc WpuncNcAfWpuncTfNpVmCcestimation computed on the treebankestimation computed on the treebankestimation computed on the treebankestimation computed on the treebankFigure 3: Comparison of P eststop probabilities esti-mated from raw Wikipedia corpora (y-axis) andof P tbstop probabilities computed from CoNLL tree-banks (x-axis).
The area of each point shows therelative frequency of an individual tag.287This is caused by the fact that determiners are of-ten obligatory in English and cannot be simplyremoved as, e.g., adjectives.
The stop probabil-ities of prepositions (IN) are also very well rec-ognized.
While their left-stop is very probable(prepositions always start prepositional phrases),their right-stop probability is very low.
The verbs(the most frequent verbal tag is VBD) have verylow both right and left-stop probabilities.
Our es-timation assigns them the stop probability about0.3 in both directions.
This is quite high, but still,it is one of the lowest among other more frequenttags, and thus verbs tend to be the roots of the de-pendency trees.
We could make similar analysesfor other languages, but due to space reasons weonly provide graphs for Czech, German, Spanish,and Hungarian in Figure 3.6.3 SettingsAfter a manual tuning, we have set our hyperpa-rameters to the following values:?c = 50, ?s = 1, ?
= 1/3We have also found that the Gibbs sampler doesnot always converge to a similar grammar.
For acouple of languages, the individual runs end upwith very different trees.
To prevent such differ-ences, we run each inference 50 times and take therun with the highest final Ptreebank (see Section 4)for the evaluation.6.4 ResultsTable 2 shows the results of our unsupervisedparser and compares them with results previouslyreported in other works.
In order to see the im-pact of using the estimated stop probabilities (us-ing model P dmv+eststop ), we provide results for clas-sical DMV (using model P dmvstop ) as well.
We donot provide results for Chinese since we do nothave any appropriate tokenizer at our disposal (seeSection 3), and also for Turkish from CoNLL 2006since the data is not available to us.We now focus on the third and fourth column ofTable 2.
The addition of estimated stop probabil-ities based on large corpora improves the parsingaccuracy on 15 out of 20 treebanks.
In many cases,the improvement is substantial, which means thatthe estimated stop probabilities forced the modelto completely rebuild the structures.
For exam-ple, in Bulgarian, if the P dmvstop model is used,all the prepositions are leaves and the verbs sel-dom govern sentences.
If the P dmv+eststop modelis used, prepositions correctly govern nouns andverbs move to roots.
We observe similar changeson Swedish as well.
Unfortunately, there are alsonegative examples, such as Hungarian, where theaddition of the estimated stop probabilities de-creases the attachment score from 60.1% to 34%.This is probably caused by not very good estimatesof the right-stop probability (see the last graph inFigure 3).
Nevertheless, the estimated stop proba-bilities increase the average score over all the tree-banks by more than 12% and therefore prove itsusefulness.In the last two columns of Table 2, we provideresults of two other works reported in the last year.The first one (spi12) is the DMV-based grammarinducer by Spitkovsky et al (2012),4 the secondone (mar12) is our previous work (Marec?ek andZ?abokrtsky?, 2012).
Comparing with (Spitkovskyet al, 2012), our parser reached better accuracy on12 out of 20 treebanks.
Although this might notseem as a big improvement, if we compare the av-erage scores over the treebanks, our system signif-icantly wins by more than 6%.
The second system(mar12) outperforms our parser only on one tree-bank (on Italian by less than 3%) and its averagescore over all the treebanks is only 40%, i.e., morethan 8% lower than the average score of our parser.To see the theoretical upper bound of our modelperformance, we replaced the P eststop estimates bythe P tbstop estimates computed from the evaluationtreebanks and run the same inference algorithmwith the same setting.
The average attachmentscore of such reference DMV is almost 65%.
Thisshows a huge space in which the estimation ofSTOP probabilities could be further improved.7 Conclusions and Future WorkIn this work, we studied the possibility of esti-mating the DMV stop-probabilities from a largeraw corpus.
We proved that such prior knowledgeabout stop-probabilities incorporated into the stan-dard DMV model significantly improves the unsu-pervised dependency parsing and, since we are notaware of any other fully unsupervised dependencyparser with higher average attachment score overCoNLL data, we state that we reached a new state-of-the-art result.54Possibly the current state-of-the-art results.
They werecompared with many previous works.5A possible competitive work may be the work by Blun-som and Cohn (2010), who reached 55% accuracy on Englishas well.
However, they do not provide scores measured onother CoNLL treebanks.288CoNLL this work other systemslanguage year P dmvstop P dmv+eststop reference P dmv+tbstop spi12 mar12Arabic 06 10.6 (?8.7) 38.2 (?0.5) 61.2 10.9 26.5Arabic 07 22.0 (?0.1) 35.3 (?0.2) 65.3 44.9 27.9Basque 07 41.1 (?0.2) 35.5 (?0.2) 52.3 33.3 26.8Bulgarian 06 25.9 (?1.4) 54.9 (?0.2) 73.2 65.2 46.0Catalan 07 34.9 (?3.4) 67.0 (?1.7) 72.0 62.1 47.0Czech 06 32.3 (?3.8) 52.4 (?5.2) 64.0 55.1 49.5Czech 07 32.9 (?0.8) 51.9 (?5.2) 62.1 54.2 48.0Danish 06 30.8 (?4.3) 41.6 (?1.1) 60.0 22.2 38.6Dutch 06 25.7 (?5.7) 47.5 (?0.4) 58.9 46.6 44.2English 07 36.5 (?5.9) 55.4 (?0.2) 63.7 29.6 49.2German 06 29.9 (?4.6) 52.4 (?0.7) 65.5 39.1 44.8Greek 07 42.5 (?6.0) 26.3 (?0.1) 64.7 26.9 20.2Hungarian 07 60.8 (?0.2) 34.0 (?0.3) 68.3 58.2 51.8Italian 07 34.5 (?0.3) 39.4 (?0.5) 64.5 40.7 43.3Japanese 06 64.8 (?3.4) 61.2 (?1.7) 76.4 22.7 50.8Portuguese 06 35.7 (?4.3) 69.6 (?0.1) 77.3 72.4 50.6Slovenian 06 50.1 (?0.2) 35.7 (?0.2) 50.2 35.2 18.1Spanish 06 38.1 (?5.9) 61.1 (?0.1) 65.6 28.2 51.9Swedish 06 28.0 (?2.3) 54.5 (?0.4) 61.6 50.7 48.2Turkish 07 51.6 (?5.5) 56.9 (?0.2) 67.0 44.8 15.7Average: 36.4 48.7 64.7 42.2 40.0Table 2: Attachment scores on CoNLL 2006 and 2007 data.
Standard deviations are provided in brack-ets.
DMV model using standard P dmvstop probability is compared with DMV with P dmv+eststop , which in-corporates STOP estimations based on reducibility principle.
The reference DMV uses P tbstop , which arecomputed directly on the treebanks.
The results reported in previous works by Spitkovsky et al (2012),and Marec?ek and Z?abokrtsky?
(2012) follows.In future work, we would like to focuson unsupervised parsing without gold POStags (see e.g.
Spitkovsky et al (2011a) andChristodoulopoulos et al (2012)).
We supposethat many of the current works on unsuperviseddependency parsers use gold POS tags only as asimplification of this task, and that the ultimatepurpose of this effort is to develop a fully unsu-pervised induction of linguistic structure from rawtexts that would be useful across many languages,domains, and applications.The software which implements the algorithmsdescribed in this paper, together with P eststop estima-tions computed on Wikipedia texts, can be down-loaded athttp://ufal.mff.cuni.cz/?marecek/udp/.AcknowledgmentsThis work has been supported by the AMALACH grant(DF12P01OVV02) of the Ministry of Culture of the CzechRepublic.Data and some tools used as a prerequisite forthe research described herein have been provided bythe LINDAT/CLARIN Large Infrastructural project, No.LM2010013 of the Ministry of Education, Youth and Sportsof the Czech Republic.We would like to thank Martin Popel, Zdene?k Z?abokrtsky?,Rudolf Rosa, and three anonymous reviewers for many usefulcomments on the manuscript of this paper.ReferencesJames K. Baker.
1979.
Trainable grammars for speechrecognition.
In Speech communication papers presentedat the 97th Meeting of the Acoustical Society, pages 547?550.Yonatan Bisk and Julia Hockenmaier.
2012.
Induction of lin-guistic structure with combinatory categorial grammars.The NAACL-HLT Workshop on the Induction of LinguisticStructure, page 90.Phil Blunsom and Trevor Cohn.
2010.
Unsupervised induc-tion of tree substitution grammars for dependency pars-ing.
In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP ?10,pages 1204?1213, Stroudsburg, PA, USA.
Association forComputational Linguistics.Thorsten Brants.
2000.
TnT - A Statistical Part-of-SpeechTagger.
Proceedings of the sixth conference on Appliednatural language processing, page 8.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proceedingsof the Tenth Conference on Computational Natural Lan-guage Learning, CoNLL-X ?06, pages 149?164, Strouds-burg, PA, USA.
Association for Computational Linguis-tics.Christos Christodoulopoulos, Sharon Goldwater, and MarkSteedman.
2012.
Turning the pipeline into a loop: Iter-ated unsupervised dependency parsing and PoS induction.In Proceedings of the NAACL-HLT Workshop on the In-duction of Linguistic Structure, pages 96?99, June.Y.
J. Chu and T. H. Liu.
1965.
On the Shortest Arborescenceof a Directed Graph.
Science Sinica, 14:1396?1400.289Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.
2008.Logistic normal priors for unsupervised probabilisticgrammar induction.
In Neural Information ProcessingSystems, pages 321?328.Shay B. Cohen, Dipanjan Das, and Noah A. Smith.
2011.Unsupervised structure prediction with non-parallel mul-tilingual guidance.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Processing,EMNLP ?11, pages 50?61, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Trevor Cohn and Mirella Lapata.
2008.
Sentence compres-sion beyond word deletion.
In Proceedings of the 22ndInternational Conference on Computational Linguistics -Volume 1, COLING ?08, pages 137?144, Stroudsburg, PA,USA.
Association for Computational Linguistics.Kim Gerdes and Sylvain Kahane.
2011.
Defining depen-dencies (and constituents).
In Proceedings of DependencyLinguistics 2011, Barcelona.Walter R. Gilks, S. Richardson, and David J. Spiegelhalter.1996.
Markov chain Monte Carlo in practice.
Interdisci-plinary statistics.
Chapman & Hall.W.
Keith Hastings.
1970.
Monte carlo sampling methodsusing markov chains and their applications.
Biometrika,57(1):pp.
97?109.William P. Headden III, Mark Johnson, and David McClosky.2009.
Improving unsupervised dependency parsing withricher contexts and smoothing.
In Proceedings of Hu-man Language Technologies: The 2009 Annual Confer-ence of the North American Chapter of the Associationfor Computational Linguistics, NAACL ?09, pages 101?109, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: models of depen-dency and constituency.
In Proceedings of the 42nd An-nual Meeting on Association for Computational Linguis-tics, ACL ?04, Stroudsburg, PA, USA.
Association forComputational Linguistics.Kevin Knight and Daniel Marcu.
2002.
Summarization be-yond sentence extraction: a probabilistic approach to sen-tence compression.
Artif.
Intell., 139(1):91?107, July.Sandra Ku?bler, Ryan T. McDonald, and Joakim Nivre.
2009.Dependency Parsing.
Synthesis Lectures on Human Lan-guage Technologies.
Morgan & Claypool Publishers.Marke?ta Lopatkova?, Martin Pla?tek, and Vladislav Kubon?.2005.
Modeling syntax of free word-order languages:Dependency analysis by reduction.
In Va?clav Matous?ek,Pavel Mautner, and Toma?s?
Pavelka, editors, Lecture Notesin Artificial Intelligence, Proceedings of the 8th Interna-tional Conference, TSD 2005, volume 3658 of LectureNotes in Computer Science, pages 140?147, Berlin / Hei-delberg.
Springer.Martin Majlis?
and Zdene?k Z?abokrtsky?.
2012.
Languagerichness of the web.
In Proceedings of the Eight Interna-tional Conference on Language Resources and Evaluation(LREC 2012), Istanbul, Turkey, May.
European LanguageResources Association (ELRA).David Marec?ek and Zdene?k Z?abokrtsky?.
2012.
Exploitingreducibility in unsupervised dependency parsing.
In Pro-ceedings of the 2012 Joint Conference on Empirical Meth-ods in Natural Language Processing and ComputationalNatural Language Learning, EMNLP-CoNLL ?12, pages297?307, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.
Multi-source transfer of delexicalized dependency parsers.
InProceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, pages 62?72, Edin-burgh, Scotland, UK., July.
Association for ComputationalLinguistics.Tahira Naseem, Harr Chen, Regina Barzilay, and Mark John-son.
2010.
Using universal linguistic knowledge to guidegrammar induction.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural Language Pro-cessing, EMNLP ?10, pages 1234?1244, Stroudsburg, PA,USA.
Association for Computational Linguistics.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald,Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007.The CoNLL 2007 Shared Task on Dependency Parsing.In Proceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL 2007, pages 915?932, Prague, Czech Re-public, June.
Association for Computational Linguistics.Mohammad Sadegh Rasooli and Heshaam Faili.
2012.
Fastunsupervised dependency parsing with arc-standard tran-sitions.
In Proceedings of ROBUS-UNSUP, pages 1?9.Noah Ashton Smith.
2007.
Novel estimation methodsfor unsupervised discovery of latent structure in natu-ral language text.
Ph.D. thesis, Baltimore, MD, USA.AAI3240799.Anders S?gaard.
2011.
From ranked words to dependencytrees: two-stage unsupervised non-projective dependencyparsing.
In Proceedings of TextGraphs-6: Graph-basedMethods for Natural Language Processing, TextGraphs-6, pages 60?68, Stroudsburg, PA, USA.
Association forComputational Linguistics.Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky.2010.
From baby steps to leapfrog: how ?less is more?
inunsupervised dependency parsing.
In Human LanguageTechnologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for ComputationalLinguistics, HLT ?10, pages 751?759, Stroudsburg, PA,USA.
Association for Computational Linguistics.Valentin I. Spitkovsky, Hiyan Alshawi, Angel X. Chang, andDaniel Jurafsky.
2011a.
Unsupervised dependency pars-ing without gold part-of-speech tags.
In Proceedings ofthe 2011 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2011).Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-sky.
2011b.
Punctuation: Making a point in unsuper-vised dependency parsing.
In Proceedings of the FifteenthConference on Computational Natural Language Learn-ing (CoNLL-2011).Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-sky.
2012.
Three Dependency-and-Boundary Models forGrammar Induction.
In Proceedings of the 2012 Con-ference on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learning(EMNLP-CoNLL 2012).290
