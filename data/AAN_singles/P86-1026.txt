FORUM ON CONNECTIONISMQuest ions  about  Connect ion is t  Models  of  Natura l  LanguageMark  LibermanATS~T Bell Laboratories600 Mountain AvenueMurray Hill, N J  07974MODERATOR STATEMENTMy role as interlocutor for this ACL  Forum on Connec-tionism is to promote discussion by asking questions andmaking provocative comments.
I will begin by asking somequestions that I will attempt to answer myself, in order todefine some terms.
I will then pose some questions for thepanel and the audience to discuss, if they are interested, andI will make a few critical comments on the abstracts sub-mitted by Waltz and Sejnowski, intended to provokeresponses from them.I.
What  is a "connectionist" modeffThe basic metaphor involves a finite set of nodes inter-connected by a finite set of directed arcs.
Each node trans-mits on its output arcs some function of what it receives onits input arcs; these transfer functions are usually describedparametrically, for instance in terms of a linear combinationof the inputs composed with some nonlinear threshold-likefunction; the transfer function may involve a random vari-able.A subset of the nodes (or arcs) are designated as inputsand/or outputs, whose values are supplied or used by the"environment.
""Time" is generally quantized and treated in an idealizedway, as if all connections involved a transmission delay ex-actly equal to the time quantum; this is presumably done forconvenience and tractability, since neural systems are notlike this.
The nodes' transfer function may contain somesort of memory, e.g.
an "activation level."
The state of thenetwork at time step t determines its state at time step t+l(at least probabilistically, if random variables are involved);the network calculates its response to a change in its inputby executing a sequence of time-steps sufficient to permit in-formation to propagate through the required number ofnodes, and to permit the system to attain (at leastapproximately) a fixed point, that maps back into itself orinto a state sufficiently close.Thus the system as a whole is usually defined so that itwill settle into a static configuration for a static input pat-tern;~(models whose dynamics exhibit limit cycles or chaoticsequences are easy to devise, but I am not aware that theyhave been used).Connectionist models fat least those with static fixedpoints) define a relation on their set of input/output nodevalues.
Without further constraints on the number of hiddennodes, the nodes' transfer function, etc., the defined relationcan obviously be anything at allIn fact, the circuits of a conventional digital computercan obviously be described in terms that make them"connectionist" in the very general sense given above.
Themost interesting connectionist models, such as the so-called"neural nets" of Hopfield and Tank, or the "Boltzmannmachine," are defined in much more specific ways.II.
How can we categor i ze  and compare  themany d i f ferent  types of  such mode ls  that  havebeen proposed?The situation is reminiscent of automata theory, wherethe basic metaphor of finite control, read/write head(s), in-put and output tape(s) has many  different variations.
Thegeneral theory of connectionist machines seems to be at arelatively early stage, however.
Some particular classes ofmachines have been investigated in detail, but at the level ofgenerality that seems appropriate for this panel, a generalmathematical characterization does not exist.Some crude distinctions seem worth making:Some models "learn" while others have to heprogrammed in every detail.
This is a gradient distinction,however, since the "learning" models require an appropriatenetwork architecture combined with an appropriate descrip-tion and presentation of the training material.Some models represent category-like information dif-fusely, through ensembles of cooperating nodes and arcs,while others follow the principle of "one concept, one node."III.
Why  are (some) connect ion is t  mode lsin terest ing .
~The term "interesting" is obviously a subjective one.
Thelist that follows expresses my own point of view.1.
Connectionist models are vaguely reminiscent ofneurological systems.
The analogy is extremelyloose, at best; neuronal circuits are themselvesapparently quite diverse, but they all shareproperties that are quite different from the con-nectionist models that are generally discussed.Still, it may  be that there are some deep connec-tions in terms of abstract information-processingmethods.2.
Connectionist information processing is generallyparallel and cooperative, with all calculationscompleted in a small humbler Of time steps.
Forcertain kinds of algorithms, network size scalesgracefully with problem size, with at worst smalltime penalties.3.
In some cases, learning algorithms exist: trainingof the network over appropriate input/outputpatterns causes the network to remember thepatterns and/or to "summarize" them accordingto statistical measures that depend on the net-work structure and the training method.
Thetrained network "generalizes" to new cases; itgeneralizes appropriately if the new cases fit thedesign implicit in the network structure, thetraining method, and the training data.
The samemechanisms also give the system some capacityto complete or correct patterns that are incom-plete or partly errorful.1814.
Some models (especially those that learn and thatrepresent patterns diffusely) blur distinctionsamong rule, memory, analogy.
There need be noformal or qualitative distinction between ageneralization and an exception, or between anexception and a subregularity, or between aliteral memory and the output of a calculation.For some cognitive systems (including a numberrelevant o natural language) this permits us totrade the possibly harmful consequences of givingup on finding deeper generalizations for the im-mense relief of not looking for perfectly regularrules that aren't there.5.
Some aspects of human psychology can be nicelymodeled in connectionist terms -- e.g., semanticpriming, the role of spaced practice, frequencyand recency effects, non-localized memory, res-toration effects, etc.6.
Since connectionist-like networks can be used tobuild arbitrary filters and other signal-processingsystems, it is possible in principle to build connec-tionist systems that treat signals and symbols inan integrated way.
This is a tricky point -- an or-dinary general-purpose computer educes a digitalfilter and a theorem-prover to calculations insame underlying instruction set, so the putativeintegration must be at a higher level of themodel.IV.
What  do connect lonlst  models have to tell usabout  the  s t ruc ture  of infinite sets of str ings?So far, well-defined connectionist models all deal withrelations over a finite set of elements; at least, no one seemsto have shown how to apply such models systematically tothe infinite sets of arbitrarily-long symbol-sequences thatform the subject matter of classical automata theory.Connectionist models can deal with sequences of symbolsin at least two ways: the first is to connect the symbol se-quence to an ordered set of nodes, and the second is to havethe network change state in an appropriate way as successivesymbols are presented.In the first mode, can we do anything that adds to ourunderstanding of the algorithms involved?
For instance, itseems straightforward to implement a parallel version ofstandard context-free parsing algorithms, by laying out a 2Dmatrix of cells (corresponding to the set of substrings) foreach of the nonterminal symbols, imposing connectivityalong the rows and up the columns for calculating immediatedomination relations, and so on.
Can such an architecture bepersuaded to learn a grammar from examples?
It is limited tosentences of fixed maximum length -- is this enough to makelearning possible?
Under what circumstances can the result-ing "trained" network be extended to longer inputs withoutretraining?
Are there more interesting spatial-layout parsingmodels?Many connectionist models are "finite impulse response"machines; that is, the consequences of an input pattern "dieout" after the pattern is removed, and the network's propen-sity to respond to further patterns is left unchanged.
If thischaracteristic is removed, and the network is made to cal-culate by changing state in response to a sequence of inputs,we can of course imitate classical automata in a connec-tioniat framework.
For instance, a push down store can bebuilt out of connectionist piece parts.
Can a connectionist ap-proach to processing of sequentially presented information dosomething mote interesting than this?
For instance, can thepotentially very complex dynamics of of such networks beexploited in a useful way?V.
Comments  on  SejnowskiIn evaluating Sejnowski's very interesting demonstrationof letter-to-sound learning, it is worth keeping a few facts inmind.First, the success percentages reported are by letter, notby word (according to a personal communication fromSejnowski).
Since the average word length was presumablyabout 7.4 (the average length of the 20000 commonest wordsin the Brown corpus), the success rate by word of thegeneralization from the 1000-word set to the 20000-word setmust have been approximately .8A7.4, or about 19~.
Withthe "additional training" (presumably training on the sameset it was then tested on), the figure of 92% translates to.92A7.4, or about 54~o correct by word.Second, the training did not just present words and theirpronunciations, but rather presented words and pronuncia-tions with the correspondences between letters and phonemesindicated in advance.
Thus the network does not have toparse and/or interrelate the two symbol sequences, but onlykeep track of the conditional probability of various possibletranslations of a given letter, given the surrounding letter se-quences.
My guess is that a probabilistic n-gram-basedtransducer, trained in exactly the same way (except that itwould only need to see each example once), would outper-form Sejnowski's network.
Thus the interesting thing aboutSejnowski's work is not, I think, the level of performance(which is not competitive with conventional approaches) butsome perhaps lifelike aspects of its mode of learning, types ofmistakes, etc.The best conventional letter-to-sound systems rely on alarge morph lexicon (Hunnicutt's "DECOMP" from MITalk)or systematic back-formation and other analogical processesoperating on a large lexicon of full words (Coker's "nounce"in the current Bell Labs text-to-speech system).
Coker's sys-tem gives 100?~ coverage of the dictionary, in principle; moreinterestingly, it gives better than g9~ (by word) coverage ofrandom text, despite the fact that only about 80?7oo f thewords are direct hits.
In other words, it is quite successful atguessing the pronunciation of words that it doesn't "know"by analogy to those that it does.
To take an especiallytrivial, but very useful, example, it is quite good at decom-posing unknown compound words into pairs of known words,with possible regular prefixes and suffixes.Thus I have a question for Sejnowski: what would be in-volved in training a connectionist network to perform at thelevel of Coker's system?
This is a case that should be welladapted to the connectionist approach -- after all, we aredealing with a relation over a finite set, training material iseasily available, and Coker's success proves that the methodof generalizing by analogy to a large knowledge base workswell.
Given this situation, is the poor  performance ofSejnowski's network due only to its small size?
Or was it setup in a way that prevents it from learning some relevantmorphographemic generalizations?VI .
Comments  on  Wal tzWaltz is very enthusiastic about the connectionist future.I agree that the possibilities are exciting.
However, I thinkthat it is important not to depreciate the future by oversell-ing the present.In particular, Waltz's statement that Sejnowski's NET-talk "learned the pronunciation rules of English fromexamples" is a bit of a stretcher -- \[ would prefer somethinglike "summarized lists of contextual letter-to-phoneme cor-respondences, and generalized from them to pronounce about20% of new words correctly, with many of its mistakes beingpsychologically plausible ones.
"182Waltz comments that connectionist models "promise tomake the integration of syntactic, semantic, pragmatic andmemory models simpler and more transparent."
The four-way categorization of syntax, semantics, pragmatics, andmemory strikes me as an odd way of dividing the world up;but I agree with what I take to be Waltz's main point.
Alittle later he observes that "connectionist learning models...have demonstrated surprising power in learning conceptsfrom example.." I'm not sure how surprising the accomplish-ments to date have been, but I agree that the possibilities arevery exciting.
What are the prospects for putting the"integrated processing" opportunities together with the"learning" opportunities?If we restrict our attention to text input rather thanspeech input, then the most interesting issues in natural an-guage processing, in my opinion, have to do with systemsthat could infer at least the lexical aspects of linguistic formand meaning from examples, not just for a toy example ortwo, but in a way that would converge on a plausible resultfor a major fraction of a language.
Here, few of the basicquestions eem to have answers.
In fact, from what I haveseen of the'literature in this area, many of the questionsremain unposed.Here are a few of the questions that come to mind in rela-tion to such a project.
What would such a system have tolearn?
What kind of inputs would it need to learn it, givenwhat sort of initial expectations, represented how?
Howmuch can be learned without knowledge of non-linguisticaspects of meaning?
How much of such knowledge can belearned from essentially linguistic experience?
Are currentconnectionist learning algorithms adequate in principle?
Howbig would the network have to be?
Is a non-toy version ofsuch a system computationally tractable today, assuming itwould work in principle?
If only toy versions are tractable,can anything be proved about how the system would scale?183
