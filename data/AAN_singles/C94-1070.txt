THE "WHITEBOARD" ARCHITECTURE:A WAY TO INTEGRATE HETEROGENEOUS COMPONENTS OF NLP SYSTEMSChristian Boitet Mark ScligmanGETA, IMAG (UJF & CNRS),150 rue de la Chimie, BP 5338041 Grenoble Cedex 9, Fr,'mceChristian .Boitet@imag.
frNI'R Interpreting Teleconununicafions Research Labs2-2 ltikari-dai, Seika-cho, Somku-gunKyoto 619-02, Japanseligman@it i. atr.
co. jpABSTRACTWe present a new softw,'u'e architecture for NLP systemsmade of heterogeneous components, and demonstrate anarchitectural prototype we have built at ATR iu thecontext of Speech Translation.KEYWORDS: Distributed NLP systems, Softwarearchitectures, Whiteboard.INTRODUCTIONSpeech translation systems must integrate componentshandling speech recognition, machine translation andspeech synthesis.
Speech recognition often uses specialhardware.
More components may be added in the future, fortask understanding, multimodal interaction, etc.
In moretraditional NLP systems, such ,'cq MT systems for writtentexts, there is also a trend towards distributing varioustasks on various machines.Sequential ,architectures \[10, 11\] offer ,an easy solntion,but lead to loss of information and lack of robustness.
Onthe other hand, reports on experimenls with blackboardarchitectures \[16, 13, 20\] show they also have problems.We ,are exploring an intermediate architecture, in whichcomponents are integrated under a coordinator, may bewritten in various programming languages, may use theirown data structures and algorithms, and may run in parallelon different machines.
The coordinator maintains in awhiteboard an image of the input and output data structuresof each component, at a suitable level of detail.
Thewhitehoard fosters reuse of partial results and avoidswasteful recomputation.
Each component process isencapsulated in a manager, which transforms it inlo aserver, commuuicating with external clients (including thecoordinator) via a system of mailboxes.
Managers handlethe conversions between internal (server) and external(client) data formats.
This protocol enhances modularityand clarity, because one needs to to explicitly andcompletely declare fl~e appearance of the partial results ofthe components on the whileboard.Managers may also make batch components appear :isincremental components by delivering outputs in apiecewise fashion, thus taking a first step towards ystemssimulating simultaneous translation.We have prc~luced a rudimentary architectural prototype,KASUGA, to demonstrate he above ideas.In fl~e first section, our four main guidelines ,are detailed:(1) record overall progress of components in a whiteboard;(2) let a coordinator schedule the work of components;(3) encapsnlate components in managers; and (4) use themanagers to simulate Incremental Processing.
In thesecond section, some high-level aspects of the KASUGAprototype ,are first described, and a simple demonstration isdiscnssed, in which incremental speech translation issimulated.
Lower-level details are then giveu on someinternal aspects.I.
T I lE  WI I1TEBOARD ARCHITECTURE1.
Record overall progress in a whitelmardThe whiteboard ,architecture is inspired by the chartarchitecture of the MIND system \[8\] and later systems orformalisms for NLP \[1, 5\], as well as by the blackbo~u'darchitecture, first introduced in HEARSAY-II \[6, 13\] forspeech recognition, l lowever, there is a significantdifference: tile components do not access the whiteboard,and need not even know of its existence.There are 2 main problems with the sequential pproach.?
Pl: loss of informationIf components ,are simply concatenated, as in Asnra\[10, 11\], it is difficult for them to share partial results.Information is lost at subsystem interfaces and workhas to be duplicated.
For example, the cited systemuses an LR parser to drive speech recognition; butsyntactic structures found are discarded whenrecognition candidates are passed to MT.
Completereparsing is thus needed.?
P2: lack of robustnessCommunication difficulties between subsystems mayalso dmnage robusmess.
During reparsing for MT inASURA, if no well-formed sentences are found, partiedsyntactic structures are discarded before semanticanalysis; thus there is no chauce to tr,'mslate partially,or to use semantic inlonnation to complete the parse.The pure blackboard approach solves P1, but not P2, andintroduces four other problems.?
P3: control of concurrent accessIn principle, all components are allowed to access theblackboard: complex protection and synchronizationmechanisms must be included, and fast componentsmay be considerably slowed down by having to waitfor permission to read or write.?
P4: commnnication overloadsThe amount of information exchanged may I~ large.
I1"components rnn on different machines, such :is isoften the case for speech-related componeuts, and maybe the case for Example-Based MT con~ponents in thefuture, commmfication overloads may annihilate thebcuciit of using spcckdized or distributed hardware.?
P5: efficiency problemsAs components compute directly on the blackbo,'u'd, itis a compromise by necessity, and can not offer theoptimal kind of data structure h~r each component.?
P6: debugging problemsThese ,are due to the complexity of writing eachcomponent with the complete blaekbo,'u'd in mind, andto the parallel nature of the whole computation.In the "whiteboard" approach, the global data structure ishidden from the components, and accessed only by a"coordinator".
(The whiteboard rawing is expanded later.
)426"1he "Whiteboard" Architecture: a way to integrate... Boitet & Seligman, COLING-94(.~Olllporlell\[I(~OlllpOIl(3ll\[2COlllpollent3, \]o lFigure l: the "whitelmard" arc'h#ect,~reThis simple change makes it possible to avo id  problemsP3-P6.
It has also at least two good points:- It encourages developers to clearly define andpublishwhat their inputs and outputs are, at least to the level ofdetail necessary to represent them in the whiteboard.- The whiteboard can be the central place where graphicalinterfaces are developed to allow for e~Lsy inspection, atv,'u'ious leve ls  o f  det~fil.As long as an NLP system uses a central record accessedouly by a "coordinator' and hidden fi'om the "comlxmeuts",it cau be said to use a whiteboard architecture.
It remainsopen what dala structures the whiteboard itself should use.As in \[21, we suggest the use of a time-aligned lattice, inwhich several types of nodes can be distinguished.
Instating our preference for lattices, we must first distinguishthem from grids, and then distinguish true lattices from 2types of quasi-lattice, charts and Q-graphs (fig.
2 & 3).Sent4NP8NPI NP2\]e~.~W ~ ..=..__.41~_ ~ O  _.._._._ii~ tornove and ~lrO und fur4 t,~kI I NPS I I I I./pa Senti = ' ~ = 1 \[ NP4 _ ,..-I Senl2Sent3r~oruove tiller cap and ground fuel tankFig.
2: chart built on a syntactically ambiguous a'et/tenceN(l~ht,N(S),N(can,N(P},.
V(spackle,P(3pI)..,'k.=.A(Iklht,N(S ,~  N(can, NtS}.
,.}
V{spar kle, P(3sffi ...~G{M).f} ~, - -.
V(spaflde,lnf,..)G(M}.. /I 4(light,N(S), N(ean, N(S), IV(sparkle,N(PauI,PN, k(light,N(S,Pl I V(can,T(P), I lN(soa,~, ~ ' AV(slight..,L~n ~ ~ iP"-' ~ ~ ONb(sfl),Poss} I GIM,F}) ~ N(S),G(M}) ,I v(light''') !
I N~,,.,l~.
I IN!
spark~'I G(M),..) N(P),G{M))N(light,N(S), V (c in,T( P~ ,=~....e V(spathle,lnf,.,)G(M)..,\] ~V(mod))Paul's ligh!
can(s) sparldo(s) slighttyFig.
3: A Q-graph for a phonetically ambiguous sentenceGrids have no arcs, but nodes Co,Tesponding to timespans.
A ncxle N spanning It132\] is implicitly connectedto another node N' spanning \[t'l,t'2\] iff its time spanbegins earlier 01 gt'l ), ends strictly earlier (t2<t'2), and therespective sp,'ms (a) are not too far apart anti (b) don'toverlap tc~) much (t2-max-gap_<t'l ~t2+max-ovorlap).
max-gap  and max-over lap are gapping and overlappingthreshokts \[12\].
Because t2<t'2, there can be no cycles.ht a lattice, by contrast, nodes and arcs are explicit.Cycles are also forhiddcn, and there must be a unique firstnode and a unique last node.
(;rids have often been used in NLP.
l"or example, Iheoutput of the phonetic omponent of Kt~AL \[121 was aword grid, and certain speech recognition programs at NI'Rl~r(?luce phoneme grids 1.
In gener~d, each uc~le bears a timespan, a label, and a score.
Grids can also be used torepresent an input text obtained by scanning abad original,or a stenotypy tape \[9\], and to implement some workingstructures (like flint of the Cocke algorithm).llowever, we will require explicit arcs in order toexplicitly model possible sequences, sometimes withassociated information concerning sequence probability.Thus mw grkls am insufficient for our whiteboards.Two kinds of quasi-lattices have been used extensively, intwo wtrietics.
First, chart structures have origi,mlly beenintr(~luccd by M. Kay in the MIND system around 1965\[8\], In a ch:ut, as understood tt~lay (Kay's charts were moregeneral), the nodes ,are arranged in a row, so that there isalways a path between any two given nodes.
The arcs bearthe information (label, score), not the nodes.
Ch\[u'ls arealso used by many unification-based natural hmguageanalyzers \[141.Chart structures are unsuitable for represcnting restflts ona whiteboard, however, because they are tmable torepresent alternate sequences.
Consider the alternate wordsequences of Figure 4.
It is not possible to arr.
'mge thewords in a single mw so that all and only the propersequences can be read out,1 1 it if you came I would like you to come emly tomorrow earlierFigure 4: A sentence with alternate formtdationsA second type of quasi-lattice is the Q-graphs of \[15\] andtheir exteasiou \[17\], the basic data structure for textrepresentation in tile METI~,O \[14\] aud TAUM-Aviation \[71systems.
A Q-graph is a loop-free graph wilh a tmiqueentry node and a uni(lue exit node.
As iu charts, theinlonnalion is carried on the arcs.
It cousisls in labeled oratmotaled trees.
As there may be no l)ath between twonixies, Q-graphs can indeed faithfully represent alternatesequences like those of Figure 4.
But in this case it isnecess;uy to use, on more thau one arc, identical abelsreferring to the same span of the input.
For representationon a whitcl?mrd, such duplication is a drawback.To simplify bookkeeping and visual presentation, weprefer a representation in which a given label referring to agiven span appeaJw in only one place.
A true lattice, likeflint of Figure 5, makes this possible.
"lhe decomposition of the laltice in htyers seems natural,aud leads to more clarity.
Fach layer contains results of1115, 16\].
By contrast, tile IIWIM \[20\] system used a"phonetic lattice" on which an extended ATN operated.427The "Whiteboard" Architecture: a way to integrate... Boitet & Seligmcm, COLING-94one component, selected to the "appropriate level ofdetail".
Its time-aligned character makes it possible toorganize it in such a way that everything which has beencomputed on a certain time interval at a certain layer maybe found in the same region.
Each layer has threedimensions, time, depth and label (or "class").
A node atposition (i,j,k) corresponds to the input segment of lengthj ending at time i and is of label k. All realizations of labelk corresponding to this segment are to be packed in thisnode, and all nodes corresponding toapproximately equ,'dinput segments am thus geometrically clustered.In other words, ambiguities are packed so that dynamicprogramming techniques may be applied on direct imagesof the whiteboard.
Figure 6 gives an ex,'unple, Where themain NP has been obtained in two ways.G Q III II%Figure 5: A word lattice (representing a sentence with alternate fornudationsArcs may optionally be augmented with activation or realistic hoice of layers, however.inhibition weights, so that ideas from the fast-developinglield of neural networks may be applied.language u~q~layers layersFigure 6: The whiteboard as a factorizing data structureThe true lattice, then, is our preferred structure for thewhiteboard.We said that the whiteboard could be a central place fortransp,'u'ent inspection, at suitable levels of detail.
We usethe notion of "shaded nodes" for this.- "White" nodes are the real nodes of the lattice.
Theycontain results of the computation of the componentassociated with their layer: a white node contains atleast a label, legal in its layer, such as NP, AP,CARDP,  VP... in the example above, and possiblymore complex information, as allowed by thedeclaration of the layer in the whitelx~ard.- "Grey" nodes may be added to show how the whitenodes have been constructed.
They don't belong to thelattice structure proper.
In the example above, theystand for rule instances, with the possibility of m-->nrules.
In other cases, they may be used to show thecorrespondences between odes ot two layers.
?-@ .
?.--@Figure 7: White and grey nodes corresponding to rule Rn:X1 X2...Xp -> Y1 Y2...Yq- "Black" nodes may be used to represent finer steps inthe computation of the component, e.g.
to reflect heactive edges of a chart parser.Whiteboard la rers are organized in a lc, op-li'ee dependencygraph.
Non-linguistic as well aslinguistic information can be recorded inappropriate layers.
For example, in amultimodal context, the syntacticanalyzer might use selected informationfrom a map layer, where pointing, etc.could be recorded.
Interlayerdependencies should be decl~u'ed, withassociated constraints, stating forinstance that only nodes with certainlabels can be related to other layers.Ilere is an illustration of that idea,wilhout any pretense to propose a~Ot\] ll|ellUlayer layerFigure 8: A hierarchy of layers in an hypotheticalwhiteboard for a nndtimodal NLP ,~'ystem2.
Let a coordin'ator schedule tile componentsIn its simplest form, a coordinator only transmits theresults of a component to Ihe next component(s).l lowever, it is in a position to carry out global strategiesby filtering low-ranking hypotheses and transmitting onlythe most promising part of a whitcboard layer to itsprocessiug component.
Further, if certain componentsmake uselhl predictions, the coordinator can pass these toother components as constraints, ,along with input.3.
Encapsulate components in managersDevelopers of components should be free to choose andvary their algorithms, data structures, programminglanguages, and possibly hardware (especially so lor speech-related components).
Our approach is to encapsulateexisting components in managers, which hide them andtransform them into servers.
This strategy has the furlheradv,'mtage of avoiding any direct call between coordinatorand components.
To plug in a new component, one justwrites a new manager, a good part of which is generic.428The "Whiteboard" Architecture: a way to integrate... Boitet & Seligman, COLING-94A m,'mager has a request box where clients send requeststo open or close connections.
A connection consists of apair of in and out mailboxes, with associated locks, mid isopened with certain paraneters, uch as its sleep time andcodes indicating pre-agreed import and export formats.
Thecoordinator puts work to do into in-boxes aid gets resultsin corresponding out-boxes.As illustrated in Figure 1 above, a client can open morethan one connection with the sane manager.
For exanple,au on-line dictionary might be called for displaying"progressive" word for word translation, as well as for,'mswering ternfinological requests by a human interpretcrsupervising several dialogues and l~ddng over if needed.And a malager can in principle have several clients.llowever, this potential is not used in KASUGA.4.
Simulate incrementa l  processingIn real life, simullanexms interpretation is often preferredover consecutive interpretation: although it may be lessexact, one is not forced to wait, and one can react evenbefore the end of tile speaker's utterance.
Incrementalprocessing will thus be an iinportant aspect of futuremachine interpretation systems.
For instance, a sem.
'mlicprocessor might begin working on the syntactic structureshypothesized for early parts of an utterance while laterparts ,are still being syntactically an,'dyzed \[19\].Even if a component (e.g., a W cun'ently existing speechrecognizer) has to get to file end of the utterance beforeproducing any result, its nmnager may still m;tke itsprocessing appear incremental, by delivering its resultpiecewise and iu the desired order.
I lence, this organiz'~tionmakes it possible to siintfiate future incrementalcomponents.11.
T I lE  KASUGA PROTOTYI 'E1.
External levelThe coordinator (KAS.COORD) is writtcn in KEK TM, auobject-oriented xpert system shell with excellent interface-building tools.
The whiteboard is declared ill KEF\]s objectlanguage.
KEE itself is written ill Common lisp.Three components are inw/lved:- speech recognition (SP.REC) providing :t 3-level grid,progrmnmcd in C \[15\];- ish'md-driven syntactic hart-parsing (SYNT.AN)deriving words and higher-level syntactic units,programned in C;- word-for-word translation (WW.TRANS) at file wordlevel, written in C aid running on another machine.The tanagers are written in Lisp, ,'rod run independently,in three Unix processes.
Each manager ,and the c(gmlinatorcan rat  in different Unix shells.
Although WW.TRANS isalready accessible as a server on a distant machine, we hadto create a manager lbr it to get the intended behavior.With only these components, it is possible to produce asimple demonstration in which incremental speechtranslation is simulated and the transparency gained byusing a whiteboard is illustrated.
The phonemes producedby SP.REC are assembled into words and phrases bySYNT.AN.
As this goes on, WW.TRANS producespossible word-for-word translations, which are presented onscreen ,'u,~ a word lattice.KASUGA's whiteboard has only three layers: phonemes;source words and phrases; and equivalent target words.
Atthe first layer, the phoneme lattice is represented withphonemes in nodes.
At the second layer, we retain only thecomplete substructures produced by SYNT.AN, that is, theinactive exlges.
Phonemes used in these slructures appearagain at that layer.In KEE, we define a class of NODES, with subclassesWHITE.NODES, GREY.NODES, PIlON.LAYI~P,.NOI)ES, audSYNT.
I,AYER.
NODES in tile syntactic htycr.
NODES have ageneric display method, and subclasses have specializedvariants (e.g., the placing of white nodes depends on theirtime interval, while that of grey nodes depends on that ofthe white nodes they cermet0.2.
Internal levelWhen a manager eceives a Make.Conuection requestfrola a client, it creates an in box and an out box (andassociated locks, used to prevent interference betweencomponents), through which information is p.'~ssed to andfrom the client.
The Make.Connection request includescodes showing in which format(s) the client is expectingto deposit data in the iu box and read data from tile outbox, lbr that connection.Mlhough data transfer could be programmed moreefficiently, e.g.
nsing lhfix sockets, our method is moregeneral, as it uses only the file system, and we believe itsoverhead will be negligible in comparison with tileprocessing times required by the compouents.Ikn each out box, the client (KASUGA) actbatcs a readerprocess and tile relewmt mauagcr actiwttes a writer process.Conversely, for each in box, tile client activates at writerprocess and the manager activates a reader process.
A zeaderprocess wakes up regul:uly and checks whether its mailboxis both non-empty and nnlocked.
If so, it locks themailbox; reads ils contents; empties tile mailbox; unlocksit; and goes to sleep again.
A writer process, bycomparison, wakes up regul:uly and checks whether itsmailbox is both empty and unlocked.
If so, it locks thebox, fills it with appropriate data, unlocks it, and goesback to sleep.
For example, the writer associated withSYNT.AN will deposit in the appropriate out box theimage of all tile inactive arcs created since the lm;t deposit.SItI?,EC provides, lor each of 40 prerecorded bunsetsu(elementary phrase), a set of about 25 phoneme malrices,one for each phoneme.
A malrix cell contains the score fora given phoneme with a given begimfiug/ending speechfrane pair.
These nmtrices are then compared, and 3 otherinatrices are computed.
The tnp-scoring ln:llrix contains ineach cell the tnl~-scnring phone and its score for Ihecorresponding begimliug/cnd.
The 2nd-scoring a~d 3rd-scoring matrices are computed sinfilarly.
These threemauices are used to build the first layer of the whiteboard.To build the whilcboard's second layer, an ishmd-drivenclmrt parser is used, where the matrices are cousklered asinitialized charts.
The over:dl best-scoring cell in the topmatrix is established as the only anchor, and hi-directionalsearching is carried out wilhin the (handset) limits set bymax-gap and max-overlap.
A CFG written by J. llosakafor tile ASURA demos is now used as is.
Parsing resultsare convertcd to syntact  Jc:.
\] a t< 5 ce.
N (by Olt\[ char t -to -  la t t i ce  filter) and brought into KEF~.Then an image lattice, ww.
la  t t \] c e. N, is comptlted asthe whiteboard's third layer, using a C-based ou-tine J-l{dictionary.
Each lexieal syntactic node gives rise to oueFmglish word for each meafing.
For example, ~ givesyes, yes-sir, the-lungs, ashes, etc.Layers of the whiteboard are represented by KEF,"planes".
We can move planes rehtlive to e\[ich olher; ztx~m429The "Whiteboard" Architecture: a way to integrate... Boitet & Seligman, COLING-94in various ways; put various information in the nodes(label, rule responsible, id, time span, score); exp,'md thenodes; open & close the nodes selectively.
And we cancolor the nodes according to their score.
It is possible toshow or hide various parts of the whiteboard.
In Figure 9,the first layer, the time grid, the lattice lines, and theinitial/final lattice nodes have been hidden.
Alternatively,we could hide constnlction (dotted) lines, rule boxes, labelboxes, etc.
The view of any part of the whiteboard can hechanged for emphasis: one can for instance interactivelyselect only the nodes above a certain confidence threshold.Overall processing can be inten'upted for examination.WW.lattice,N| ,~.
~ I start III , I I ?
I',,,,ose,s.,lII II I " - - "o  I ~ .
Io~ # %%O'Figure 9: a view of KASUGA ' s whiteboardIf this architecture is to be further developed in the future,one could use instead of KEE a general-purpose, portableinterface building toolkit in order to avoid the oved~ead ,'rodoverspecialization ,associated with using a complete xpertsystem shell.KAS.COORD writes and reads data to and from themanagers in a LISP-like format, and handles thetransformation i to KEE's internal fornmt.
Each managertranslates back ,and forth between that format and wbateverformat its associated component happens to be using.Ilence, formats must be precisely defined.
For inst,'mce, theedges produced by the speech recognizer are of the form(begin end phoneme score).
The nodes and edges of theconesponding phoneme layer in the whiteboard are of Iheform (node-id begin end phoneme score (in-arcs) (out-arcs)),with ares being of the form (are-id origin extremity weight).CONCLUSIONAlthough the concept of the whiteboard architecture Iresemerged in the context of rese,-u'ch in Speech Translation,it can be useful in other areas of NLP.
It has already beenused, in a prelimin,'u'y form, in dialogue-b~sed MT \[3\]: thetasks are distributed between the authoring stations and anMT server, m~d the coordinator maintains in a unique datastructure all intermediate stages of processing of all unitsof translation.The whiteboard ,architecture might be used with profit inall situations where it is important o integrate new orexisting components, e.g.
to build generic environmentsfor developing heterogeneous NLP systems.
Researcherswould thereby gain twice: by getting a clearer view ofwhat they (and others) ,are doing; and by being able to usegeneric interlace tools provided by the coordinator fordebugging and illustrating purposes.ACKNOWI ,EDGMENTSWe are grateful to M. Fiorenthm from Intellicorp, Inc.and K. Kurokawa from CSK, Inc., for providing a democopy of KEE TM and valuable technical support; to Dr.Y.
Yamazaki, President of ATP,-IT, and T. Morimoto,l lead of Dept.
4, for their support and encouragement; toII.
Singer, T. ltayashi, Y. Kitagawa, I\[.
Kashioka, andJ.
Hosaka, for their help in developing the components;and to K. II.
Loken-Kim, for sfimtflating discussions andproposing the term "whitelx~ard".REFERENCES\[1\] Barnett J., Knight K., Mani I.
& Rich E.119901 Knowledge and Natural Language Processing.Comm.
ACM, 33/8, 50-71.\[2\] Boltet C. (19881 Representation and Computationof Units of Translation for Machine Interpretation of SpokenTexts.
Comp.
& AI, 6, 505--546.\[3\] Baiter C. & Blanchon 11.
(1993) Dialogue-BasedMT for Monolingual Authors and the LIDIA project.
Proc.NLPRS'93, Fukuoka, 208--222.\[4\] Chandloux J.
& Gu~rard M.q*'.
(1981) METEO:un sysltSme ?t l'tCpreuve du temps.
META, 1, 17--22.\[5\] Colmeraner A.
(1970) Les systdmes-Q, unformalisme pour analyser et synthdtiser des phrases surordinateur.
TAIJM, Univ.
de MontrSal, dec. 1970.\[61 Erman L. D. & Lesser V. R. (19801 771e Hearsay-H Speech Understanding System : A Tutorial.
ht "Trends inSpeech Recognition", W.A.
Lea, ed., Prcntice-lhdl, 361-381.\[7i lsabelle P. & Bourbeau L. (1984) TAUM-A VIA770N: its technical features and some experbnentalresults.
Comp.
Ling., I1/I, 18 27.\[8\] Kay M. (1973) 77~e MIND system.
In "CourantComputer Science Symposium 8: Natural LanguageProcessing", R. Rustin, ed., Algorithntics Press, 155-188.\[9\] M6rlaldo 1|.
(1988) Multilevel decoding for Vety-Large-Size-Dictionaly speech recognition.
IBM Journal ofR&D, 32/2, March 1988, 227-237.\[101 Morlmoto T., St, zuki M., Takezawa T., KlknlG.-(., Nagata M. & Tnmokiyo M. (19921 A SpokenLanguage Translation System: SL-TRANS2.
Proc.
COLING-92, Nantes, vol.
3/4, 1048--1052.\[11\] Morlmoto T., Takezawa T., Yato F.,Sagayama S., Tashlro T., Nagata M. & al.
(1993)ATR's Speech Translation System: ASURA.
EuroSpeecb'93.\[12\] Qnlnton P. (19811) Contribution h lareconnaissance d la parole.
Utilisation tie mdthodesheuristiques pour la reconnaissance ti phrases.
TbSse d'Etat,Univ.
de Rennes, 239 p.\[13\] Reddy R. (19811) Machine Models of Speechl'erception.
In "Perception and Production of Fluent Speech",Cole, ed., Erlbaum, N.J., 215-242.\[141 Schleber S. M. (19861 An it, troduction tounification-based approaches togrammar.
CSLI Lect.
Notes 4.\[15\] Singer II.
& Sagayama S. (19921 Matrix Parserand its Application to 11MM-based Speech Recognition.IEICE, 111/SP92-76, I)SP92-61, 21-26.\[16\] Singer I1.
& Sagayama S. (1992) MatrixParsing applied to TDNN-based Speech Recognition.
JapaneseJournal of Speech Processing, 1992/3, 89-90.\[17\] Stewart G. (1975) Manuel du langage REZO.TAUM, Univ.
de Montrfal, 120 p.\[18\] Tomlta M. (19911) 77~e Generalized LRParser/Compiler V8-4 : a Software Package for Praclical NLl'rojects.
Proc.
COLIN(;-90, vol.
1/3, 59-63.\[19\] Waldster W. (1993) Planning MultimodalDiscourse.
Proc.
ACL-93, Columbus, Ohio, 95-96.\[20\] Wolf J. J.
& Woods W. A.
(19811) 771e IIWIMSpeech Understanding System.
In "Trends in SpeechRecognition", W. A. Lea, ed., Prentice-llall, 316-339.-0 -0 -0 -0 -0 -0 -0 -0 -0 -0 -430
