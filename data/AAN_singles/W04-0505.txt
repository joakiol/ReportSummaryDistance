BioGrapher: Biography Questions as aRestricted Domain Question Answering TaskOren TsurText and Data Mining GroupBar Ilan Universitytsuror@cs.biu.ac.ilMaarten de RijkeInformatics InstituteUniversity of Amsterdammdr@science.uva.nlKhalil Sima?anInstitute for Logic, Languageand ComputationUniversity of Amsterdamsimaan@science.uva.nlAbstractWe address Question Answering (QA) for biograph-ical questions, i.e., questions asking for biographi-cal facts about persons.
The domain of biographicaldocuments differs from other restricted domains inthat the available collections of biographies are in-herently incomplete: a major challenge is to answerquestions about persons for whom biographical in-formation is not present in biography collections.We present BioGrapher, a biographical QA systemthat addresses this problem by machine learning al-gorithms for biography classification.
BioGrapherfirst attempts to answer a question by searching ina given collection of biographies, using techniquestailored for the restricted nature of the domain.
Ifa biography is not found, BioGrapher attempts tofind an answer on the web: it retrieves documentsusing a web search engine, filters these using the bi-ography classifier, and then extracts answers fromdocuments classified as biographies.
Our empiricalresults show that biographical classification, prior toanswer extraction, improves the results.1 IntroductionAlthough most current research in question answer-ing (QA) is oriented towards open domains, aswitnessed by evaluation exercises such as TREC,CLEF, and NTCIR, various significant applicationsconcern restricted domains, e.g., software manuals.In restricted domains, a QA system faces questionsand documents that exhibit less variation in lan-guage use (e.g., words and fixed phrases, more spe-cific terminology) than in an open domain, and itcould access high-quality knowledge sources thatcover the entire domain.
Open domain QA as itis assessed at TREC, CLEF, and NTCIR concernsa broad variety of fairly restricted question types,such as location questions, monetary questions, bi-ography questions, questions that ask for conceptdefinitions, etc.
How useful or effective is it toadopt a restricted domain approach to some of thesequestion types?
In this paper we explore so-calledbiographical questions, e.g., Who was Algar Hiss?or Who is Sir John Hale?, i.e., questions that de-mand answers consisting of biograpical key factsthat are typically found in biographies and that typ-ically involve fixed phrases about, e.g., birthdates,education, societal roles.
This type of questions wasfound to be quite frequent in search engine logs.
Webelieve that biographical questions can be usefullyviewed as defining a restricted domain for QA: thedomain of biographical information as representedby biographies.Ideally, biographical questions are answered byretrieving a biography from some existing collec-tion of biographies (such as biography.com)and extracting snippets from it.
Such resources,however, have a limited coverage.
There will al-ways be people whose biographical information isnot contained in any of the existing collections.This necessitates retrieval of ?biography-like?
doc-uments, i.e., documents with biographical informa-tion.
The problem of identifying biography-likedocuments by machine learning algorithms turnsout to be a challenging but rewarding task as we willsee below.In this paper we address the problem of questionanswering within the biographical domain.
We de-scribe BioGrapher, a restricted domain QA systemfor answering bibliographical questions in which abaseline approach, that exploits biography collec-tions, is extended with a trainable biography classi-fier operating on the web in order to enhance cov-erage.
The baseline system helps us understand theusefulness of existing high quality biography col-lections within a QA system.
The extension of ourbaseline approach concerns the problem of identi-fying biography-like documents, and the extractionfrom such documents of answers for questions thatcould not be answered using biography collections.A main challenge lies in constructing an algorithmfor identifying documents containing usefull bio-graphical information that may provide an answerfor a given question.
To addresss this challenge, weexplore two machine learning algorithms: Ripper(Cohen and Singer, 1996) and Support-Vector Ma-chines (Joachims, 1998).Section 2 provides some background, and in Sec-tion 3 we briefly describe our baseline QA sys-tem, based on external knowledge sources comple-mented with a naive approach to retrieving biogra-phy snippets using a web search engine.
In Sec-tion 4 we prepare the ground for our text classifica-tion experiments.
In Section 5 we present two clas-sifiers: one loosely based on the Ripper algorithmand the other based on an SVM classifier.
In Sec-tion 6 we compare the performanc of the baselineagainst versions of the system integrated with thetwo classifiers.
Section 7 discusses the results andconsiders the possibility of applying our approachto other restricted domains.
We conclude in Sec-tion 8.2 Related WorkTwo kinds of related work are relevant to this pa-per: question answering against external knowledgesources, and genre detection (using classifiers).
Webriefly discuss both.Many QA research groups employed ExternalKnowledge Sources in order to improve perfor-mance.
For instance, (Chu-Carroll and Prager,2002) used WordNet to answer what is questions,using the isa hierarchy supported by WordNet.
(Hovy et al, 2002; Lin, 2002) used dictionariessuch as WordNet and web search results to re-rankanswers.
(Yang et al, 2003) preformed structureanalysis of the knowledge obtained from WordNetand the Web in order to further improve perfor-mance.We refer to (Sebastiani, 2002) for extensive re-view about machine learning in automated text clas-sification.
(Lewis, 1992) were among the first touse machine learning for genre detection trying tocategorize Reuters articles to predefined categories.Probabilistic classifiers were used by many groups(Lewis, 1998).
Much current text classification re-search is focused on Support Vector Machines, firstused for genre detection by (Joachims, 1998).3 A Na?
?ve BaselineIn this section we describe our baseline QA system.This system was used at TREC 2003, to produce an-swers to so-called person definition questions (Jij-koun et al, 2004; Voorhees, 2004).
We present theresults and give a short analysis of the system?s per-formance; as we will see, this provides further mo-tivation for the use of text classification for identi-fying biography-like documents.Definition questions at TREC 2003The QA track at TREC 2003 featured a subtaskdevoted to definition questions.
The latter camein three flavors: person definitions (e.g., Who isColin Powell?
), organization definitions (e.g., Whatis the U.N.?
), and concept definitions (e.g., What isgoth?).
Here, we are only interested person defini-tions.In response to a definition question, systems hadto return an unordered set of snippets; each snippetwas supposed to be a facet in the definition of thetarget.
There were no limits placed on either thelength of an individual answer string or on the num-ber of snippets in the list, although systems werepenalized for retrieving extraneous information.As our primary strategy for handling person def-inition questions, we consulted external resources.The main resource used is biography.com.While such resources contain biographies of manyhistorical and well-known people, they often lackbiographies of contemporary people that are not toowell-known.
To be able to deal with such cases webacked-off to using a web search engine (Google),and applied a na?
?ve heuristic approach.
We hand-crafted a set of features (such as ?born?, ?gradu-ated?, ?suffered?, etc.)
that we felt would triggerfor biography-like snippets.
Various subsets of thelarge feature set, together with the target of the def-inition question, were combined to form queries forthe web search engine.Given a set of candidate answer snippets, we per-formed two filtering steps before presenting the finalanswer: we separated non-relevant snippets fromvaluable snippets and we identified semantically-close snippets.
We addressed the first step by ana-lyzing the distances between query terms submittedto the search engine and the sets of features, and bymeans of shallow syntactic aspects of the differentfeatures such as sentence boundaries.
To addressthe second step we developed a snippet similaritymetric based on stemming, stopword removal andkeyword overlap by sorting and calculating the Lev-enshtein distance measure of similarity.1.
An ex-ample of the snippets filtering can be found in Ta-ble 1.
The table presents 3 of the returned snippetsfor the question Who is Sir John Hale?.
The firstand third snippet are filtered out, the first one fornon-relevancy and the third for its semantic similar-ity with the second, shorter, snippet.1The Levenshtein measure is a measure of the similarity be-tween two strings, which are refered to as the source string sand the target string t. The distance is the number of deletions,insertions, or substitutions required to transform s into t1 Sir Malcolm Bradbury (writer/teacher) Dead.Heart trouble.
.
.
.
Heywood Hale Broun (com-mentator, writer ) ?
Dead.
John Brunner (au-thor) Dead.
Stroke.
.
.
.
Description: Debunksthe rumor of his death.
.
.2 .
.
.
Professor Sir John Hale woke up, had .
.
.
Forher birthday in 1996, he wrote on the .
.
.
JohnHale died in his sleep - possibly following an-other stroke .
.
.3 Observer On 29 July 1992, Professor Sir JohnHale woke up .
.
.
her birthday in 1996, he wroteon the .
.
.
John Hale died in his sleep - possiblyfollowing another stroke.Table 1: Snippets filtering for Who is Sir John Hale?EvaluationEvaluation of individual person definition questionswas done using the F-measure: F = (?2 + 1)P ?R)/(?2P + R), where P is precision (to be de-fined shortly), R is recall (to be defined shortly),and ?
was set to 5, indicating that precision wasmore important than recall.
Length was used as acrude approximation to precision; it gives a systeman allowance of 100 (non-white-space) charactersfor each correct snippet it retrieved.
The precisionscore is set to one if the response is no longer thanthis allowance, otherwise it is downgraded using thefunction P = 1?
((length ?
allowance)/length).As to recall, for each question, the TREC asses-sors marked some snippets as vital and the remain-der as non-vital.
The non-vital snippets act as ?don?tcare?
condition.
That is, systems should be penal-ized for not retrieving vital nuggets, and for retriev-ing snippets that are not in the assessors?
snippetlists at all, but should be neither penalized nor re-warded for returning a non-vital snippet.
To im-plement the ?don?t care?
condition, snippet recall iscomputed only over vital snippets (Voorhees, 2004).In total, 30 person definition questions were eval-uated at the TREC 2003 QA track.
The overall Fscore of a run was obtained by averaging over allthe individual questions.Results and AnalysisThe F score obtained by the naive system describedin this section, on the TREC 2003 person definitionquestions, was 0.392.
An analysis of the resultsshows that, for questions that could be answeredfrom external biography resources, the baseline sys-tem obtains an F score of 0.586.In post-submission experiments we changed thesubsets of features we use in the queries sent toGoogle as well as the number of queries/subsets weuse.
The snippet similarity threshold was also tunedin order to filter out more snippets.
This resulted infewer unanswered questions, while the average an-swer length was decreased as well, by close to 50%.All in all, an informal evaluation showed increase inrecall, precision and in the overall F score.From our experience with our baseline systemwe learned the following important lesson: havinga (relatively) small number of high quality biogra-phy sources as a basis for each question?s answeris far better than using a broad and large variety ofsnippets returned by a web search engine.
Whileextending available biography resources so as to se-riously boost their coverage is not a feasible option,we want to do the next best thing: make sure weidentify good biography-like documents online, sothat we can use these to mine snippets from; to thisend we will use text classification.4 Preparing for Text ClassificationIn the previous section we suggested that using atext classifier might improve the performance of bi-ography QA.
Using text classifiers, we aim to iden-tify biography-like documents from which we canextract answers.
In this section we detail the docu-ment representations on which we will operate.Document and Text RepresentationText classifiers represent a document as a set of fea-tures d = {f1, f2,.
.
.
, fn} where n is the number ofactive features, that is, features that occur in the doc-ument.
A feature f can be a word, a set of words, astem or any phrasal structure, depending on its textrepresentation.
Each feature has a weight, usuallyrepresenting the number of occurrences of this fea-ture in the document.What is a suitable abstract representation of doc-uments for our biography domain?
We have defined7 clusters, groups of words (terms/tokens) with ahigh degree of pair-wise semantic relatedness.
Eachcluster has a meta-tag symbol (as can be seen in Ta-ble 2) and all occurrences of members of a clusterwere substituted by the cluster?s meta-tag.
An ex-ample of a document abstraction can be found in Ta-ble 3.
This abstraction captures typical similaritiesbetween biographical strings; e.g., for the two sen-tences John Kennedy was born in 1917 and WilliamShakespeare was born in 1564 we get the same ab-straction <NAME> <NAME> was born in <YEAR>.It is worth noting that some of the clusters,such as <CAP> and <PLACE>, <CAP> and <PN>and others may overlap.
Looking at the exam-ple in Table 3, we see that Abbey was born inChicago, Illinois, but the automatic abstractor mis-interpreted the token ?Ill.,?
marking it is <CAP> forcapitalized (possibly meaningful) word, but not as<NAME> the name of the subject of the biography<YEAR> four digits surrounded by white space,probably a year<D> sequence of number of digits other thanfour digits, can be part of a date, age etc.<CAP> a capitalized word in the middle of asentence that wasn?t substituted by anyother tag<PN> a proper name that is not the subject ofthe biography It substitutes any nameout of a list of thousand names<PLACE> denotes a name of a place, city or coun-try out of a list of more than thousandplaces<MONTH> denotes one of the twelve months<NOM> denotes a nominativeTable 2: Seven meta-tags used for document ab-straction<PLACE>.
A similar thing happens with the name?Wooldridge?
that is not very common; it shouldhave been <PN> instead of <CAP>.All procedures described below are preformed onabstract-meta-tagged documents.5 Identifying Biography DocumentsGiven a document, the task of a biography classifieris to decide whether a given document is a biogra-phy or not.
In this section we address the problemof acquiring biography classifiers by training ma-chine learning algorithms on data.
We present twobiography classification algorithms: a naive classi-fier based on Ripper (Cohen and Singer, 1996), andanother based on SVM (Joachims, 1998).
The twomethods differ radically both in the way they rep-resent the training data (i.e., document representa-tion), and in their learning approaches.
The naiveclassifier is obtained by a repetitive rule learning al-Original Lincoln, Abbey (b. Anna MarieWooldridge) 1930 ?
Jazz singer, com-poser/arranger, movie actress; born inChicago, Ill.
While a teenager shesang at school and church functionsand then toured locally with a danceband.Abstraction <NAME>, <NAME> ( b .
<PN> <CAP><CAP> ) <YEAR> - <CAP> singer ,composer/arranger , movie actress ;born in <PLACE> <CAP> .
Whilea teenager <NOM> sang at school andchurch functions and then toured lo-cally with a dance band .Table 3: Abstraction of jazz singer Abbey Wool-ridge?s biographygorithm.
We modified this algorithm to specificallyfit the task of identifying biographies.
The SVMlearns ?linear decision boundaries?
between the dif-ferent classes.
We employ here the implementationof SVMs by (Joachims, 1998).
Next we discuss thedetails of how each algorithm was used for learninga biography classifier.Naive ClassifierWe employ this algorithm for its simplicity and scal-ability.
This algorithm learns user-friendly rules,i.e., human-readable conjunctions of propositions,which can be converted to queries for a Booleansearch engine.
Furthermore, it is known to exhibitrelatively good results across a wide variety of class-fication problems, including tasks that involve largecollections of noisy data, similar to the large doc-ument collections that we face in definitional QA.The naive classifier consists of two main stages:(1) Rules building.
This is similar to Ripper?s firststage of building an initial rule set.
Our algorithmdeviates from standard implementations of Ripperin that the terms that serve as the literals in therules are n-grams of various lengths.
We feel thatn-grams, as opposed to individual literals (as in (Co-hen and Singer, 1996), better capture contextual ef-fects, which could be crucial in text classification.Our learner learns the rules as follows.
The set ofk-most frequent n-grams representing the trainingdocuments is split into two frequency-ordered lists:TLP (term-list-positive) containing the positive ex-ample set and TLN (term-list-negative) containingthe negative examples set.
The vector ~w is initial-ized to be TLP/(TLP ?
TLN), i.e., the most fre-quent n-grams extracted from the positive set thatare not top frequent in the negative set.
(2) Rule optimization.
Instead of Ripper?s rulepruning stage, our algorithm assigns a weight toeach rule/n-gram r in the rules vector accordingto the formula g(n)?f(r)C , where g(n) is an increas-ing function in the length of the n-gram (longer n-grams receive higher weights), f(r) is the ratio ofthe frequency of r in the positive examples to itsfrequency in the negative examples, and C is thesize of the training set.
The normalization by Cis merely for the purpose of tracking variations ofthe weights in different sizes of training sets.
Thepreference for longer n-grams can be justified bythe intuition that longer n-grams are more informa-tive as they stand for stricter contexts.
For example,the string ?
(<NAME> , <NAME> born in <YEAR>?seems more informative than the shorter string in<YEAR>).Training material.
The corpus we used as ourtraining set is a collection of 350 biographies.
Mostof the biographies were randomly sampled frombiography.com, while the rest were collectedfrom the web.
About 130 documents from the NewYork Times (NYT) 2000 collection were randomlyselected as negative example set.
The volumes ofthe positive and negative sets are equal.Various considerations played a role in buildingthis corpus.
The biographies from biography.com are ?clean?
in the sense that all of them werewritten as biographies.
To enable the learning offeatures of informal biographies, some other ?noisy?biographies such as biography-like newspaper re-views were added.
Furthermore, a small number ofdifferent biographies of the same person were man-ually added in order to enforce variation in style.We also added a small number of biographies fromother different sources to avoid any bias towards thebiography.com domain.Validation and tuning.
We tuned the naive algo-rithm on a separate validation set of documents.
Thevalidation set was collected in the same way as thetraining set.
It contained 60 biographies, of which40 were randomly sampled from biography.com, 10 ?clean?
biographies were collected fromvarious online sources, 10 other documents werenoisy biographies such as newspaper reviews.
Inaddition, another 40 non-biographical documentswere randomly retrieved from the web.The vector ~w is now used to rank the documentsof the validation set V in order to set a threshold?
that minimizes the false-positive and the false-negative errors.
Each document dj ?
V in the val-idation set is represented by a vector ~x, where xicounts the occurrences of wi in dj .
The score of thedocument is the normalized inner product of ~x and~w given by the function score(dj) = ~x?~wlength(dj) .In the validation stage some heuristic modifica-tions were applied by the algorithm.
For example,when the person name tag is absent, the documentgets the score of zero even though other parametersof the vector may be present.
We also normalizeddocument scores by document length.Support Vector Machines (SVMs)Now we describe the learning of a biography clas-sifier using SVMs.
Unlike many other classifiers,SVMs are capable of learning classification even ofnon-linearly-separable classes.
It is assumed thatclasses that are non-linearly separable in one di-mension may be linearly separable in higher di-mension.
SVMs offer two important advantagesfor text classification (Joachims, 1998; Sebastiani,2002): (1) Term selection is often not needed, asSVMs tend to be fairly robust to overfitting andcan scale up to considerable dimensionalities, and(2) No human and machine effort in parameter tun-ing on a validation set is needed, as there is a the-oretically motivated ?default?
choice of parametersettings which have been shown to provide best re-sults.The key idea behind SVMs is to boost the dimen-sion of the representation vectors and then to findthe best line or hyper-plane from the widest set ofparallel hyper-planes.
This hyper-plane maximizesthe distance between two elements in the set.
Theelements in the set are the support vectors.
Theo-retically, the classifier is determined by a very smallnumber of examples defining the category frontier,the support vectors.
Practically, finding the supportvectors is not a trivial task.Training SVMs.
The implementation used isSVM-light v.5 (Joachims, 1999).
The classifier wasrun with its default setting, with linear kernel func-tion and no kernel optimization tricks.
The SVM-light was trained on the very same (meta-tagged)training corpus the naive classifier was trained on.Since SVM is supposed to be robust and to fit bigand noisy collections, no feature selection methodwas applied.
The special feature underlying SVMsis the high dimensional representation of a docu-ment, allowing categorization by a hyper-plane ofhigh dimension; therefore each document was rep-resented by the vector of its stems.
The dimen-sion was boosted to include all the stems from thepositive set.
The boosted vector dimension was7935, the number of different stems in the collec-tion.
The number of support vectors discovered was17, which turned out to be too small for this task.Testing this model on the test set (the same test setused to test the naive classifier from previous sec-tion) yielded very poor results.
It seemed that theclassification was totally random.
Testing the classi-fier on smaller subsets of the training set (200, 300,400 documents) exposed signs of convergence, sug-gesting the training set is too sparse for the SVM.To overcome sparse data, more documents wereneeded.
The size of the training set was more thandoubled.
A total of 9968 documents was used asthe training set.
Just like the original training set,most of the biographies were randomly extractedfrom biography.com, while a few dozen bi-ographies were manually extracted from variousonline sources to correct for a possible bias inbiography.com.
Training SVM-light on thenew training set yielded 232 support vectors, whichseems enough to perform this classification tasks.6 Experimental ResultsIn order to test the effectiveness of the biographyclassifiers in improving question answering, we in-tegrated each one of them with the naive baselinebiographical QA system and tested the integratedsystem, called BioGrapher (Figure 1).
Before dis-cussing the results of this experiment, we brieflymention how the two classifiers performed on thepure classification task.
For this purpose, we cre-ated a test set including 47 documents that were re-trieved from the web.
The evaluation measure wasthe accuracy of the classifiers in recognizing biogra-phies.
The Ripper-based algorithm achieved 89%success, outranking the SVM which achieved 83%.A discussion of this difference is beyond the scopeof this paper (see (Tsur, 2003) for details).We tested BioGrapher on 11 out of the 30 bio-graphical questions in the TREC 2003 QA track.Those 11 questions were chosen as a test set be-cause the baseline system (Section 3) scored poorlyon them, suggesting that our baseline heuristics areincapable of effectively dealing with this type ofquestions.QuestionQuestionanalyzerSnippets filterAnswersnippetsBiographycollectionWebRetrievalengineDocumentclassifierFigure 1: BioGrapher system overviewTwo experiments were carried out, one for theRipper-based classifier and another for the SVM-based one.
For each definitional question Biogra-pher submits two simple queries to a web searchengine (e.g., Sir John Hale and Sir John Hale biog-raphy).
It retrieves the top 20 documents returnedby the search engine, thus obtaining, for each ques-tion, 40 documents amongst which it should finda biography.
BioGrapher then classifies the doc-uments into biographies and non-biographic doc-uments.
The distribution of documents that wereclassified as biographies can be found in Table 4.To simplify the experiments, and especially theQuestion Naive Classifier SVMWho is Alberto Tomba?
2 4Who is Albert Ghiorso?
8 2Who is Alexander Pope?
13 11Who is Alice Rivlin?
3 2Who is Absalom?
2 3Who is Nostradamus?
1 1Who is Machiavelli?
3 6Who is Andrea Bocceli?
1 1Who is Al Sharpton?
2 6Who is Aga Khan?
4 1Who is Ben Hur?
2 1Table 4: Distribution of documents retrieved (in-cluding false-positive)error analysis, we set up BioGrapher to return an-swer snippets from a single biography or biography-like document only.
Recall, the test questionswere such that there were no biographies for thequestion targets in the biography collection weused (biography.com): the biographies usedwere ones that BioGrapher identified on the web.We evaluated BioGrapher in the following manner.The assessor first determines whether the documentfrom which BioGrapher extracts answer snippets isa proper biography or not.
In case the document isnot a pure biography the F-score given to this ques-tion is zero.
Otherwise, the F-score was determinedin the manner described in Section 3.2BioGrapher with the Ripper-based ClassifierThe total number of documents that were classi-fied as biographies is 41 (out of 440 retrieved docu-ments).
However, analysis of the results reveals thatthe false positive ratio is high; only 20 of the 41 cho-sen documents were proper biography-like pages,the other ?biographies?
were very noisy.For 4 out of the 11 test questions, a properbiography was returned as the top ranking docu-ment.
While all 4 questions scored 0 at the origi-nal TREC evaluation, now their average F-score is0.732, improving the average F-score over all biog-raphy questions by 9.6% to 0.4659.BioGrapher with the SVM ClassifierThe total number of documents that were classi-fied as biographies is 38 (out of 440 retrieved docu-2Obviously, the F-score for snippets extracted from doc-uments incorrectly classified as biographies could be higherthan zero because these documents could still contain valuablepieces of biographical information that would contribute to theanswer?s F-score.
However, we decided to compute precisionand recall only for snippets extracted from documents correctlyclassified as biographies as we think of the biography classi-fier as a means to identify (?on-the-fly?)
quality documents thatcould in principle be added to a biography collection.ments).
However, just like in the case of the Ripper-based classifier, an analysis of the results revealsthat the false positive ratio is high; only 18 of the38 chosen documents were biography-like.The SVM classifier managed to return proper bi-ographies (as top ranking documents) for 5 out of11 questions.
The average F-score for those ques-tions is 0.674 instead of the original zero, improvingthe average F-score over all biography questions by9.7% to 0.4665.No biographies at all were retrieved for 4 of the11 definition targets in the test set, the same fourdefinition targets for which the Ripper-based classi-fier did not find biographies.
A closer look revealsthe same problems as with the Ripper-based classi-fier: a relatively high false-positive error ratio andweak ranking of the classified biographies.7 DiscussionThe results of the experiments using both classifiersare quite similar.
The system integrated with theSVM-based classifier achieved a slightly higher F-score but it still falls within the standard deviation.Our experiment serves as a proof of concept for thehypothesis that using text classification methods im-proves the baseline QA system in a principled way.In spite of the major improvement in the system?sperformance, we have found two main problemswith the classifiers.
First, although the classifiersmanaged to identify biography-like documents, theyhave a high false-positive ratio and too many er-rors in filtering out some of the non-pure-biographydocuments.
This happens when the documents re-trieved by the web search engine simply cannot beregarded as clean biographies by human assessors,although they do contain many biographic details.Second, most of the definition targets had biogra-phies retrieved and even classified as biographies,but the biographies were ranked below other noisybiographical documents, therefore the best biogra-phy was not presented as a source from which toextract answer snippets.
There are various obvi-ous paths to improve over the current system: (1)Improve the classifiers by better training and otherclassification algorithms; (2) Enable the extractionof answers from ?noisy?
biography-like documentsin such a way that the gain in recall is not reversedby a loss of precision; and (3) Allow for the extrac-tion of answer snippets from multiple biography-like documents, while avoiding to return overlap-ping snippets.8 ConclusionIn this paper we have addressed the problem of bi-ographical question answering.
The main challengein this restricted domain is the fact that the availablecollections of biography documents are (unavoid-ably) too small to admit answering all biographi-cal questions.
We use the web as a backoff sourcefor finding biography-like documents from which toextract answers in case a given biography collec-tion does not contain information about the ques-tion target.
We demonstrated the benefits of inte-grating a text classifier into a restricted domain QAsystem as a filter to web retrieval.
Finally, we be-lieve that the use of text classifiers can be benefi-cial for definitional QA, especially for identifyingdocuments from which the final answer should beextracted.
Future work will address the weaknessof the current implementation: improved biographyclassification and improved answer extraction frombiography-like documents.AcknowledgmentsMaarten de Rijke was supported by the Nether-lands Organization for Scientific Research (NWO)under project numbers 612-13-001, 365-20-005, 612.069.006, 612.000.106, 220-80-001,612.000.207, and 612.066.302.ReferencesJ.
Chu-Carroll and J. Prager.
2002.
Use of Word-Net hypernyms for answering what-is questions.In Proceedings of the Tenth Text REtrieval Con-ference (TREC 2001).
NIST Special Publication500-250.W.
Cohen and Y.
Singer.
1996.
Context sensitivelearning methods.
In Proceedings of the 19thACM International Conference on Research andDevelopment in Information Retrieval (SIGIR-96), pages 307?315.
ACM Press.E.
Hovy, U. Hermjakob, and C.Y.
Lin.
2002.
Theuse of external knowledge in factoid QA.
In Pro-ceedings of the Tenth Text REtrieval Conference(TREC 2001).
NIST Special Publication 500-250.V.
Jijkoun, G. Mishne, C. Monz, M. de Rijke,S.
Schlobach, and O. Tsur.
2004.
The Univer-sity of Amsterdam at the TREC 2003 QuestionAnswering Track.
In E.M. Voorhees, editor, Pro-ceedings TREC 2003.
NIST Special PublicationSP 500-255.T.
Joachims.
1998.
Text categorization with sup-port vector machines: Learning with many rele-vant features.
In Proceedings of ECML-98, 10thEuropean Conference on Machine Learning.T.
Joachims.
1999.
Svm-light v.5, making large-scale svm learning practical.
advances in kernelmethods - support vector learning.
B. Scholkopfand C. Burges and A. Smola (ed.)
MIT-Press.D.D.
Lewis.
1992.
Representation and learningin information retrieval.
Ph.D. thesis, GraduateSchool of the University of Maassachusetts.D.D.
Lewis.
1998.
Naive (bayes) at forty: The in-dependence assumption in information retrieval.In Proceedings of the 10th European Conferenceon Machine Learning, pages 137?142.
Springer-Verlag.C.Y.
Lin.
2002.
The effectiveness of dictionary andweb based answer reranking.
In The 19th Inter-national Conference on Computational Linguis-tics (COLING 2002).F.
Sebastiani.
2002.
Machine learning in auto-mated text categorization.
ACM Computing Sur-veys, 34(1):1?47.O.
Tsur.
2003.
Definitional question answeringusing trainable text classifiers.
Master?s thesis,ILLC, University of Amsterdam.E.M.
Voorhees.
2004.
Overview of the TREC 2003question answering track.
In Text REtrieval Con-ference (TREC 2004).
NIST Special Publication:SP 500-255.H.
Yang, T.-S. Chua, S. Wang, and C.-K. Koh.2003.
Structured use of external knowledge forevent-based open domain question answering.In Proceedings of the 26th annual internationalACM SIGIR conference on Research and de-velopment in informaion retrieval, pages 33?40.ACM Press.
