Fast, Deep-Linguistic Statistical Dependency ParsingGerold Schneider, Fabio Rinaldi, James DowdallInstitute of Computational Linguistics, University of Zurichfgschneid,rinaldig@ifi.unizh.ch, j.m.dowdall@sussex.ac.ukAbstractWe present and evaluate an implemented sta-tistical minimal parsing strategy exploiting DGcharateristics to permit fast, robust, deep-linguistic analysis of unrestricted text, and com-pare its probability model to (Collins, 1999) andan adaptation, (Dubey and Keller, 2003).
Weshow that DG allows for the expression of themajority of English LDDs in a context-free wayand oers simple yet powerful statistical mod-els.1 IntroductionWe present a fast, deep-linguistic statisticalparser that prots from DG characteristics andthat uses am minimal parsing strategy.
First,we rely on nite-state based approaches as longas possible, secondly where parsing is neces-sary we keep it context-free as long as possible1.For low-level syntactic tasks, tagging and base-NP chunking is used, parsing only takes placebetween heads of chunks.
Robust, successfulparsers (Abney, 1995; Collins, 1999) have shownthat this division of labour is particularly at-tractive for DG.Deep-linguistic, Formal Grammar parsershave carefully crafted grammars written by pro-fessional linguists.
But unrestricted real-worldtexts still pose a problem to NLP systems thatare based on Formal Grammars.
Few hand-crafted, deep linguistic grammars achieve thecoverage and robustness needed to parse largecorpora (see (Riezler et al, 2002), (Burke et al,2004) and (Hockenmaier and Steedman, 2002)for exceptions), and speed remains a seriouschallenge.
The typical problems can be groupedas follows.Grammar complexity Fully comprehensivegrammars are di?cult to maintain and consid-1Non-subject WH-question pronouns and supportverbs cannot be treated context-free with our approach.We use a simple pre-parsing step to analyze themerably increase parsing complexity.Parsing complexity Typical formal gram-mar parser complexity is much higher thanthe O(n3) for CFG.
The complexity of someformal grammars is still unknown.2Pars-ing algorithms able to treat completely un-restricted long-distance dependencies are NP-complete (Neuhaus and Broker, 1997).Ranking Returning all syntactically possibleanalyses for a sentence is not what is expectedof a syntactic analyzer.
A clear indication ofpreference is needed.Pruning In order to keep search spaces man-ageable it is necessary to discard unconvincingalternatives already during the parsing process.A number of robust statistical parsers thatoer solutions to these problems have becomeavailable (Charniak, 2000; Collins, 1999; Hen-derson, 2003).
In a statistical parser, the rank-ing of intermediate structures occurs naturallyand based on empirical grounds, while mostrule-based systems rely on ad hoc heuristics.With an aggressive beam for parse-time prun-ing (so in our parser), real-world parsing timecan be reduced to near-linear.
If one were toassume a constantly full xed beam, or uses anoracle (Nivre, 2004) it is linear in practice3.Also worst-case complexity for exhaustiveparsing is low, as these parsers are CFG-based (Eisner, 2000)4.
But they typically pro-duce CFG constituency data as output, treesthat do not express long-distance dependen-cies.
Although grammatical function and empty2For Tree-Adjoining Grammars (TAG) it is O(n7) orO(n8) depending on the implementation (Eisner, 2000).
(Sarkar et al, 2000) state that the theoretical bound ofworst time complexity for Head-Driven Phrase StructureGrammar (HPSG) parsing is exponential.3In practical terms, beam or oracle approach havevery similar eects4Parsing complexity of the original Collins Models isO(n5), but theoretically O(n3) would be possibleAntecedent POS Label Count Description Example1 NP NP * 22,734 NP trace Sam was seen *2 NP * 12,172 NP PRO * to sleep is nice3 WHNP NP *T* 10,659 WH trace the woman who you saw *T*(4) *U* 9,202 Empty units $ 25 *U*(5) 0 7,057 Empty complementizers Sam said 0 Sasha snores(6) S S *T* 5,035 Moved clauses Sam had to go, Sasha said *T*7 WHADVP ADVP *T* 3,181 WH-trace Sam explained how to leave *T*(8) SBAR 2,513 Empty clauses Sam had to go, said Sasha (SBAR)(9) WHNP 0 2,139 Empty relative pronouns the woman 0 we saw(10) WHADVP 0 726 Empty relative pronouns the reason 0 to leaveTable 1: The distribution of the 10 most frequent types of empty nodes and their antecedents inthe Penn Treebank (adapted from (Johnson, 2002)).
Bracketed line numbers only involve LDDs asgrammar artifactnodes annotation expressing long-distance de-pendencies are provided in Treebanks such asthe Penn Treebank (Marcus et al, 1993), moststatistical Treebank trained parsers fully orlargely ignore them5, which entails two prob-lems: rst, the training cannot prot from valu-able annotation data.
Second, the extractionof long-distance dependencies (LDD) and themapping to shallow semantic representations isnot always possible from the output of theseparsers.
This limitation is aggravated by a lackof co-indexation information and parsing errorsacross an LDD.
In fact, some syntactic relationscannot be recovered on congurational groundsonly.
For these reasons, (Johnson, 2002) refersto them as \half-grammars".An approach that relies heavily on DG char-acteristics is explored in this paper.
It usesa hand-written DG grammar and a lexicalizedprobability model.
It combines the low com-plexity of a CFG parser, the pruning and rank-ing advantages of statistical parsers and theability to express the majority of LDDs of For-mal Grammars.
After presenting the DG bene-ts, we dene our DG and introduce our statis-tical model.
Then, we give an evaluation.2 The Benet of DG CharacteristicsIn addition to some obvious benets, such asthe integration of chunking and parsing (Abney,1995), where a chunk largely corresponds to anucleus (Tesniere, 1959), or that in an endocen-tric theory projection can never fail, we presenteight characteristics in more detail, which intheir combination allow us to treat the majorityof English long-distance dependencies (LDD) inour DG parser Pro3Gres in a context-fee way.5(Collins, 1999) Model 2 uses some of the functionallabels, and Model 3 some long-distance dependenciesThe ten most frequent types of empty nodescover more than 60,000 of the approximately64,000 empty nodes of sections 2-21 of the PennTreebank.
Table 1, reproduced from (Johnson,2002) [line numbers and counts from the wholeTreebank added], gives an overview.2.1 No Empty NodesThe fact that traditional DG does not knowempty nodes allows a DG parser to use the e?-cient 0(n3) CYK algorithm.2.2 Only Content Words are NucleiOnly content words can be nuclei in a tradi-tional DG.
This means that empty units, emptycomplementizers and empty relative pronouns[lines 4,5,9,10] pose no problem for DG as theyare optional, non-head material.
For example, acomplementizer is an optional dependent of thesubordinated verb.2.3 No External Argument, ID/LPMoved clauses [line 6] are mostly PPs or clausalcomplements of verbs of utterance.
Only verbsof utterance allow subject-verb inversion in af-rmative clauses [line 8].
Our hand-writtengrammar provides rules with appropriate re-strictions for them, allowing an inversion of the\canonical" dependency direction under well-dened conditions, distinguishing between or-dre lineaire (linear precedence(LP)) and ordrestructural (immediate dominance(ID)).
Frontedpositions are available locally to the verb in atheory that does not posit a distinction betweeninternal and external arguments.2.4 Exploiting Functional DG LabelsThe fact that dependencies are often labeled isa main dierence between DG and constituency.We exploit this by using dedicated labels tomodel a range of constituency LDDs, relationsRelation Label Exampleverb{subject subj he sleepsverb{rst object obj sees itverb{second object obj2 gave (her) kissesverb{adjunct adj ate yesterdayverb{subord.
clause sentobj saw (they) cameverb{prep.
phrase pobj slept in bednoun{prep.
phrase modpp draft of papernoun{participle modpart report writtenverb{complementizer compl to eat applesnoun{preposition prep to the houseTable 2: Important Pro3Gres Dependencytypesspanning several constituency levels, includingempty nodes and functional Penn Treebank la-bels, by a purely local DG relation6.
The selec-tive mapping patterns for MLE counts of pas-sive subjects and control subjects from the PennTreebank, the most frequent NP traces [line 1],are e.g.
(@ stands for arbitrary nestedness):?hhhh((((NP-SBJ-X@nounVP@hhh(((Vpassive verbNP-NONE-*-X?hhhh((((NP-SBJ-X@nounVP@hhh(((Vcontrol-verbSNP-SBJ-NONE-*-XOur approach employs nite-state approxima-tions of long-distance dependencies, describedin (Schneider, 2003) for DG and (Cahill et al,2004) for Lexical Functional Grammar (LFG)Itleaves empty nodes underspecied but largelyrecoverable.
Table 2 gives an overview of im-portant dependencies.2.5 Monostratalism and FunctionalismWhile multistratal DGs exist and several de-pendency levels can be distinguished (Mel'cuk,1988) we follow a conservative view close to theoriginal (Tesniere, 1959), which basically parsesdirectly for a simple LFG f-structure withoutneeding a c-structure detour.6In addition to taking less decisions due to the gainedhigh-level shallowness, it is ensured that the lexical in-formation that matters is available in one central place,allowing the parser to take one well-informed decision in-stead of several brittle decisions plagued by sparseness.Collapsing deeply nested structures into a single depen-dency relation is less complex but has a similar eect asselecting what goes in to the parse history in history-based approaches.2.6 GraphsDG theory often conceives of DG structuresas graphs instead of trees (Hudson, 1984).
Astatistical lexicalized post-processing modulein Pro3Gres transforms selected subtrees intographs, e.g.
in order to express control.2.7 Transformation to Semantic LayerPro3Gres is currently being applied in a Ques-tion Answering system specically targeted attechnical domains (Rinaldi et al, 2004b).
Oneof the main advantages of a DG parser such asPro3Gres over other parsing approaches is thata mapping from the syntactic layer to a seman-tic layer (meaning representation) is partly sim-plied (Molla et al, 2000).2.8 Tesniere's TranslationsThe possible functional changes of a word calledtranslations (Tesniere, 1959) are an exceptionto endocentricity.
They are an important con-tribution to a traceless theory.
Gerunds (af-ter winning/VBG the race) or innitives [line2] may function as nouns, obviating the needfor an empty subject.
In nounless NPs such asthe poor, adjectives function as nouns, obviatingthe need for an empty noun head.
Participlesmay function as adjectives (Western industrial-ized/VBN countries), again obviating the needfor an empty subject.3 The Statistical Dependency ModelMost successful deep-linguistic DependencyParsers (Lin, 1998; Tapanainen and Jarvinen,1997) do not have a statistical base.
But oneDG advantage is precisely that it oers simplebut powerful statistical Maximum LikelihoodEstimation (MLE) models.
We now dene ourDG and the probability model.The rules of a context-free, unlabeled DGare equivalent to binary-branching CFG rewriterules in which the head and the mother node areisomorphic.
When converting DG structures toCFG, the order of application of these rules isnot necessarily known, but in a labeled DG, theset of rules can specify the order (Covington,1994).
Fig.
1 shows such two structures, equiv-alent except for the absence of functional la-bels in CFG.
Subj (but not PP ) has been usedin this example conversion to specify the appli-cation order, hence we get a repetition of theeat/V node, mirroring a traditional CFG S andVP distinction.In a binary CFG, any two constituents A andB which are adjacent during parsing are candi-ROOT the man eats apples with a forkWSENTSubjDetWObjWPPWPObjDeteat/Vhhhhh(((((man/NXXthe/Dtheman/Nmaneat/Vhhhhh(((((eat/Veatsapple/Nappleswith/Phh((with/Pwithfork/NXXa/Dafork/NforkFigure 1: DG and CFG representationdates for the RHS of a rewrite rule.
As terminaltypes we use word tags.X !
AB; e:g:NP !
DT NN (1)In DG, one of these is isomorphic to the LHS,i.e.
the head.
This grammar is also a BarePhrase Structure grammar known from Mini-malism (Chomsky, 1995).B !
AB; e:g: NN !
DT NN (2)A !
AB; e:g: V B !
V B PP (3)Labeled DG rules additionally use a syntacticrelation label R. A non-lexicalized model wouldbe:p(RjA !
AB)=#(R;A !
AB)#(A !
AB)(4)Research on PCFG and PP-attachment hasshown the importance of probabilizing on lexicalheads (a and b).p(RjA !
AB;a; b)=#(R;A !
AB; a; b)#(A !
AB; a; b)(5)All that A !
AB expresses is that the depen-dency relation is towards the right.p(Rjright; a; b)=#(R; right; a; b)#(right; a; b)(6)e.g.
for the Verb-PP attachment relation pobj(following (Collins and Brooks, 1995) includingthe description noun7)p(pobjjright; verb; prep; desc:noun)=#(pobj; right; verb; prep; desc:noun)#(right; verb; prep; desc:noun)The distance (measured in chunks) between ahead and a dependent is a limiting factor for theprobability of a dependency between them.p(R; distjright; a; b)=#(R; dist; right; a; b)#(right; a; b)(7)7PP is considered to be an exocentric category, sinceboth the preposition and the description noun can beseen as head; in LFG they appear as double-headMany relations are only allowed towards one di-rection, the left/right factor is absent for them.Typical distances mainly depend on the rela-tion.
Objects usually immediately follow theverb, while a PP attached to the verb may easilyfollow only at the second or third position, afterthe object and other PPs etc.
By application ofthe chain rule and assuming that distance is in-dependent of the lexical heads we get:p(R; distja; b)=#(R; a; b)#(a; b)#(R; dist)#R(8)We now explore Pro3Gres' main probabilitymodel by comparing it to (Collins, 1999), andan adaptation of it, (Dubey and Keller, 2003).3.1 Relation of Pro3Gres to CollinsModel 1We will rst consider the non-generative Model1 (Collins, 1999).
Both (Collins, 1999) Model1 and Pro3Gres are mainly dependency-basedstatistical parsers over heads of chunks, aclose relation can thus be expected.
The(Collins, 1999) Model 1 MLE estimation is:P (Rjha; atagi; hb; btagi; dist)=#(R; ha; atagi; hb; btagi; dist)#(ha; atagi; hb; btagi; dist)(9)Dierences in comparison to (8) are: Pro3Gres does not use tag information.This is because, rst, the licensing hand-written grammar is based on Penn tags. The second reason for not using tag infor-mation is because Pro3Gres backs o to se-mantic WordNet classes (Fellbaum, 1998)for nouns and to Levin classes (Levin, 1993)for verbs instead of to tags, which has theadvantage of being more ne-grained. Pro3Gres uses real distances, measured inchunks, instead of a feature vector.
Dis-tance is assumed to be dependent only onR, which reduces the sparse data problem.
(Chung and Rim, 2003) made similar ob-servations for Korean. The co-occurrence count in the MLE de-nominator is not the sentence-context, butthe sum of counts of competing relations.E.g.
the object and adjunct relation arein competition, as they are licensed by thesame tag sequence V B NN.
Pro3Gresmodels attachment (thus decision) proba-bilities, viewing parsing as a decision pro-cess. Relations (R) have a Functional DG de-nition, including LDDs.3.2 Relation to Collins Model 2(Collins, 1999) Model 2 extends the parser to in-clude a complement/adjunct distinction for NPsand subordinated clauses, and it includes a sub-categorisation frame model.For the subcategorisation-dependent genera-tion of dependencies in Model 2, rst the prob-abilities of the possible subcat frames are calcu-lated and the selected subcat frame is added asa condition.
Once a subcategorized constituenthas been found, it is removed from the subcatframe, ensuring that non-subcategorized con-stituents cannot be attached as complement,which is one of the two major function of asubcat frame.
The other major function of asubcat frame is to nd all the subcategorizedconstituents.
In order to ensure this, the prob-ability when a rewrite rule can stop expandingis calculated.
Importantly, the probability ofa rewrite rule with a non-empty subcat frameto stop expanding is low, the probability of arewrite rule with an empty subcat frame to stopexpanding is high.Pro3Gres includes a complement/adjunct dis-tinction for NPs.
The examples given in sup-port of the subcategorisation frame model in(Collins, 1999) Model 2 are dealt with by thehand-written grammar in Pro3Gres.Every complement relation type, namelysubj, obj, obj2, sentobj, can only occur once perverb, which ensures one of the two major func-tions of a subcat frame, that non-subcategorizedconstituents cannot be attached as comple-ments.
This amounts to keeping separate sub-cat frames for each relation type, where the se-lection of the appropriate frame and removingthe found constituent coincide, which has theadvantage of a reduced search space: no hy-pothesized, but unfound subcat frame elementsneed to be managed.
As for the second majorfunction of subcat frames { to ensure that if pos-sible all subcategorized constituents are found {the same principle applies: selection of subcatframe and removing of found constituents coin-cide; lexical information on the verb argumentcandidate is available at frame selection time al-ready.
This implies that Collins Model 2 takesan unnecessary detour.As for the probability of stopping the expan-sion of a rule { since DG rules are always binary{ it is always 0 before and 1 after the attach-ment.
But what is needed in place of interrela-tions of constituents of the same rewrite rule isproper cooperation of the dierent subcat types.For example, the grammar rules only allow anoun to be obj2 once obj has been found, or averb is required to have a subject unless it isnon-nite or a participle, or all objects need tobe closer to the verb than a subordinate clause.3.3 Relation to Dubey & Keller 03(Dubey and Keller, 2003) address the ques-tion whether models such as Collins also im-prove performance on freer word order lan-guages, in their case German.
German is con-siderably more inectional which means thatdiscarding functional information is more harm-ful, and which explains why the NEGRA an-notation has been conceived to be quiteat(Skut et al, 1997).
(Dubey and Keller, 2003)observe that models such as Collins when ap-plied directly perform worse than an unlexical-ized PCFG baseline.
The fact that learningcurves converge early indicates that this is notmainly a sparse data eect.
They suggest a lin-guistically motivated change, which is shown tooutperform the baseline.The (Collins, 1999) Model 2 rule generationmodel for P !
Lm:::L1HR1:::Rn, isP (RHSjLHS) = Ph(HjP; t(P ); l(P ))mYi=0Pl(Li; t(Li); l(Li)jP;H; t(H); l(H); d(i))nYi=0Pr(Ri; t(Ri); l(Ri)jP;H; t(H); l(H); d(i))PhP of head t(H) tag of H head wordLHS left-hand side RHS right-hand sidePl:1::mP(words left of head) Pr:1::nP(words right of head)H LHS Head Category P RHS Mother CategoryL left Constit.
Cat.
R right Constit.
Cat.l(H) head word of H d distance measureDubey & Keller suggest the following changein order to respect the NEGRAatness: Phisleft unchanged, but Pland Prare conditionedon the preceding sister instead of on the head:P (RHSjLHS) = Ph(HjP; t(P ); l(P ))mYi=0Pl(Li; t(Li); l(Li)jP;Li 1; t(Li 1); l(Li 1); d(i))nYi=0Pr(Ri; t(Ri); l(Ri)jP;Ri 1; t(Ri 1); l(Ri 1); d(i))Their new model performs considerably betterand also outperforms the unlexicalized baseline.The authors state that \[u]sing sister-head re-lationships is a way of counteracting theat-ness of the grammar productions; it implicitlyadds binary branching to the grammar."
(ibid.
).DG is binary branching by denition; addingbinary branching implicitly converts the CFGrules into an ad-hoc DG.Whether the combination ((Chomsky, 1995)merge) of two binary constituents directlyprojects to a \real" CFG rule LHS or an im-plicit intermediate constituent does not matter.Observations What counts is each individual FunctionalDG dependency, no matter whether it is ex-pressed as a sister-head or a head-head de-pendency, or stretches across several CFGlevels (control, modpart etc.
) Not adjacency (i,i-1) but headednesscounts.
Instead of conditioning on the pre-ceding (i-1) sister, conditioning on the realDG head is linguistically more motivated8. Not adjacency (i,i-1) but the type of GRcounts: the question why Dubey & Kellerdid not use the NEGRA GR labels has toarise when discussing a strongly inectionallanguage such as German. The use of a generative model, calculatingthe probability of a rule and ultimately theprobability of producing a sentence giventhe grammar only has theoretical advan-tages.
For practical purposes, modelingparsetime decision probabilities is as valid.With these observations in mind, we can com-pare Pro3Gres to (Dubey and Keller, 2003).As for the Base-NP Model, Pro3Gres only re-spects the best tagging & chunking result re-ported to it { a major source of errors (see sec-tion 4).
In DG, projection (although not ex-pansion) is deterministic.
H and P are usuallyisomorphic, if not Tesniere-translations are rule-based.
Since in DG, only lexical nodes are cat-egories, P=t(P).
Phis thus l(h), the prior, weignore it for maximizing.
In analogy, also cat-egory (L/R) and their tags are identical.
Therevised formula isP (RHSjLHS)=l(h)mYi=0Pl(t(Li); l(Li)jP; t(Li 1); l(Li 1); d(i))nYi=0Pr(t(Ri); l(Ri)jP; t(Ri 1); l(Ri 1); d(i))If a DG rule is head-right, P is Lior Ri, ifit is head-left, P is Li 1or Ri 1, respectively.8In primarily right-branching languages such as En-glish or German (i-1) actually amounts to being the headin the majority of, but not all cases.
In a more functionalDG perspective such as the one taken in Pro3Gres, theselanguages turn out to be less right-branching, however,with prepositions or determiners analyzed as markers tothe nominal head or complementizers or relative pro-nouns as markers to the verbal head of the subclause.Headedness and not direction matters.
Li/Riis replaced by Hiand L/Ri 1=i+1by H'.
H' isunderstood to be the DG dependent, although,as mentioned, H' could also be the DG head inthis implicit ad-hoc DG.P (RHSjLHS)=l(h)n+mYi=0Pl;r(t(Hi); l(Hi)jt(Hi); t(H0i); l(H0i); d(i))P (t(Hi)jt(Hi); t(H0i)) is a projection orattachment grammar model modeling theunlexicalized probability of t(H) and t(H')participating in a binary rule with t(H) ashead { the merge probability in Bare PhraseStructure (Chomsky, 1995); an unlabeled ver-sion of (4).
P (t(Hi); l(Hi)jt(Hi); t(H0i); l(H0i))is a lexicalized version of the same pro-jection or attachment grammar model;P (t(Hi); l(Hi)jt(Hi); t(H0i); l(H0i; d(i))) inaddition conditions on the distance9.
Pro3Gresexpresses the unlexicalized rules by licensinggrammar rules for relation R. Tags are not usedin Pro3Gres' model, because semantic backosand tag-based licensing rules are used.P (d(i)jl(Hi); l(H0i)) (10)The Pro3Gres main MLE estimation (8)(l(H) = a; l(H0) = b) diers from (10) by usinglabeled DG, and thus from the Dubey & KellerModel by using a consistent functional DG.4 Evaluation(Lin, 1995; Carroll et al, 1999) suggest eval-uating on the linguistically meaningful level ofdependency relations.
Two such evaluations arereported now.First, a general-purpose evaluation using ahand-compiled gold standard corpus (Carroll etal., 1999), which contains the grammatical re-lation data of 500 random sentences from theSusanne corpus.
The performance (table 3), ac-cording to (Preiss, 2003), is similar to a largeselection of statistical parsers and a grammat-ical relation nder.
Relations involving LDDsform part of these relations.
A selection of themis also given: WH-Subject (WHS), WH-Object(WHO), passive Subject (PSubj), control Sub-ject (CSubj), and the anaphor of the relativeclause pronoun (RclSubjA).9Since normalized probabilities are usedP (t(Hi); l(Hi)jt(Hi); t(H0i); l(H0i; d(i))) =P (t(Hi); d(i)jt(Hi); t(H0i); l(Hi); l(H0i))CARROLL Percentages for some relations, general, on Carroll testset only LDD-involvingSubject Object noun-PP verb-PP subord.
clause WHS WHO PSubj CSubj RclSubjAPrecision 91 89 73 74 68 92 60 n/a 80 89Recall 81 83 67 83 n/a 90 86 83 n/a 63GENIA Percentages for some relations, general, on GENIA corpusSubject Object noun-PP verb-PP subord.
clausePrecision 90 94 83 82 71Recall 86 95 82 84 75Table 3: Evaluation on Carroll's test suite on subj, obj, PP-attachment and clause subord.
relationsand a selection of 5 LDD relations, and on the terminology-annotated GENIA corpusSecondly, to answer how the parser performsover domains markedly dierent to the train-ing corpus, to test whether terminology is thekey to a successful parsing system, and to assessthe impact of chunking errors, the parser hasbeen applied to the GENIA corpus (Kim et al,2003), 2000 MEDLINE abstracts of more than400,000 words describing the results of Biomed-ical research, which is annotated for multi-wordterms and thus contains near-perfect chunking.100 random sentences from the GENIA corpushave been manually annotated and compared tothe parser output (Rinaldi et al, 2004a).5 ConclusionsWe have discussed how DG allows the expres-sion of the majority of LDDs in a context-free way and shown that DG allows for simplebut powerful statistical models.
An evaluationshows that the performance of its implementa-tion is state-of-the-art10.
Its parsing speed ofabout 300,000 words per hour is very good for adeep-linguistic parser and makes it fast enoughfor unlimited application.ReferencesSteven Abney.
1995.
Chunks and dependen-cies: Bringing processing evidence to bearon syntax.
In Jennifer Cole, Georgia Green,and Jerry Morgan, editors, ComputationalLinguistics and the Foundations of Linguis-tic Theory, pages 145{164.
CSLI.M.
Burke, A. Cahill, R. O'Donovan, J. vanGenabith, and A.
Way.
2004.
Treebank-based acquisistion of wide-coverage, proba-bilistic LFG resources: Project overview, re-sults and evaluation.
In The First Interna-tional Joint Conference on Natural LanguageProcessing (IJCNLP-04), Workshop "Beyondshallow analyses - Formalisms and statisti-cal modeling for deep analyses", Sanya City,China.10We are currently starting evaluation on the PARC700 corpusAoife Cahill, Michael Burke, Ruth O'Donovan,Josef van Genabith, and Andy Way.
2004.Long-distance dependency resolution in au-tomatically acquired wide-coverage PCFG-based LFG approximations.
In Proceedings ofACL-2004, Barcelona, Spain.John Carroll, Guido Minnen, and Ted Briscoe.1999.
Corpus annotation for parser evalua-tion.
In Proceedings of the EACL-99 Post-Conference Workshop on Linguistically Inter-preted Corpora, Bergen, Norway.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the NorthAmerican Chapter of the ACL, pages 132{139.Noam Chomsky.
1995.
The Minimalist Pro-gram.
The MIT Press, Cambridge, Mas-sachusetts.Hoojung Chung and Hae-Chang Rim.
2003.
Anew probabilistic dependency parsing modelfor head-nal, free word order languages.
IE-ICE Transaction on Information & System,E86-D, No.
11:2490{2493.Michael Collins and James Brooks.
1995.Prepositional attachment through a backed-o model.
In Proceedings of the Third Work-shop on Very Large Corpora, Cambridge,MA.Michael Collins.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D.thesis, University of Pennsylvania, Philadel-phia, PA.Michael A. Covington.
1994.
An empiricallymotivated reinterpretation of DependencyGrammar.
Technical Report AI1994-01, Uni-versity of Georgia, Athens, Georgia.Amit Dubey and Frank Keller.
2003.
Proba-bilistic parsing for German using sister-headdependencies.
In Proceedings of the 41st An-nual Meeting of the Association for Compu-tational Linguistics, Sapporo.Jason Eisner.
2000.
Bilexical grammars andtheir cubic-time parsing algorithms.
In HarryBunt and Anton Nijholt, editors, Advances inProbabilistic and Other Parsing Technologies.Kluwer.Christiane Fellbaum, editor.
1998.
WordNet:An Electronic Lexical Database.
MIT Press,Cambridge, MA.James Henderson.
2003.
Inducing historyrepresentations for broad coverage statisti-cal parsing.
In Proceedings of HLT-NAACL2003, Edmonton, Canada.Julia Hockenmaier and Mark Steedman.
2002.Generative models for statistical parsing withcombinatory categorial grammar.
In Proceed-ings of 40th Annual Meeting of the Associa-tion for Computational Linguistics, Philadel-phia.Richard Hudson.
1984.
Word Grammar.
BasilBlackwell, Oxford.Mark Johnson.
2002.
A simple pattern-matching algorithm for recovering emptynodes and their antecedents.
In Proceedingsof the 40th Meeting of the ACL, University ofPennsylvania, Philadelphia.J.D.
Kim, T. Ohta, Y. Tateisi, and J. Tsu-jii.
2003.
Genia corpus - a semantically an-notated corpus for bio-textmining.
Bioinfor-matics, 19(1):i180{i182.Beth C. Levin.
1993.
English Verb Classesand Alternations: a Preliminary Investiga-tion.
University of Chicago Press, Chicago,IL.Dekang Lin.
1995.
A dependency-basedmethod for evaluating broad-coverageparsers.
In Proceedings of IJCAI-95, Mon-treal.Dekang Lin.
1998.
Dependency-based evalua-tion of MINIPAR.
In Workshop on the Eval-uation of Parsing Systems, Granada, Spain.Mitch Marcus, Beatrice Santorini, and M.A.Marcinkiewicz.
1993.
Building a large anno-tated corpus of English: the Penn Treebank.Computational Linguistics, 19:313{330.Igor Mel'cuk.
1988.
Dependency Syntax: theoryand practice.
State University of New YorkPress, New York.Diego Molla, Gerold Schneider, Rolf Schwit-ter, and Michael Hess.
2000.
AnswerExtraction using a Dependency Grammarin ExtrAns.
Traitement Automatique deLangues (T.A.L.
), Special Issue on Depen-dency Grammar, 41(1):127{156.Peter Neuhaus and Norbert Broker.
1997.
Thecomplexity of recognition of linguistically ad-equate dependency grammars.
In Proceedingsof the 35th ACL and 8th EACL, pages 337{343, Madrid, Spain.Joakim Nivre.
2004.
Inductive dependencyparsing.
In Proceedings of Promote IT, Karl-stad University.Judita Preiss.
2003.
Using grammatical rela-tions to compare parsers.
In Proc.
of EACL03, Budapest, Hungary.Stefan Riezler, Tracy H. King, Ronald M. Ka-plan, Richard Crouch, John T. Maxwell,and Mark Johnson.
2002.
Parsing the WallStreet Journal using a Lexical-FunctionalGrammar and discriminative estimation tech-niques.
In Proc.
of the 40th Annual Meet-ing of the Association for Computational Lin-guistics (ACL'02), Philadephia, PA.Fabio Rinaldi, James Dowdall, Gerold Schnei-der, and Andreas Persidis.
2004a.
Answer-ing Questions in the Genomics Domain.
InACL 2004 Workshop on Question Answeringin restricted domains, Barcelona, Spain, 21{26 July.Fabio Rinaldi, Michael Hess, James Dowdall,Diego Molla, and Rolf Schwitter.
2004b.Question answering in terminology-rich tech-nical domains.
In Mark Maybury, edi-tor, New Directions in Question Answering.MIT/AAAI Press.Anoop Sarkar, Fei Xia, and Aravind Joshi.2000.
Some experiments on indicators ofparsing complexity for lexicalized grammars.In Proc.
of COLING.Gerold Schneider.
2003.
Extracting and usingtrace-free Functional Dependencies from thePenn Treebank to reduce parsing complex-ity.
In Proceedings of Treebanks and Linguis-tic Theories (TLT) 2003, Vaxjo, Sweden.Wojciech Skut, Brigitte Krenn, ThorstenBrants, and Hans Uszkoreit.
1997.
An anno-tation scheme for free word order languages.In Proceedings of the Fifth Conference on Ap-plied Natural Language Processing (ANLP-97), Washington, DC.Pasi Tapanainen and Timo Jarvinen.
1997.
Anon-projective dependency parser.
In Pro-ceedings of the 5th Conference on AppliedNatural Language Processing, pages 64{71.Association for Computational Linguistics.Lucien Tesniere.
1959.
Elements de SyntaxeStructurale.
Librairie Klincksieck, Paris.
