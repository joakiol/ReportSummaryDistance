Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1180?1190,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsFeature Selection in Kernel Space: A Case Study on Dependency ParsingXian Qian and Yang LiuThe University of Texas at Dallas800 W. Campbell Rd., Richardson, TX, USA{qx,yangl}@hlt.utdallas.eduAbstractGiven a set of basic binary features, wepropose a new L1norm SVM basedfeature selection method that explicitlyselects the features in their polynomialor tree kernel spaces.
The efficiencycomes from the anti-monotone propertyof the subgradients: the subgradient withrespect to a combined feature can bebounded by the subgradient with respectto each of its component features, anda feature can be pruned safely withoutfurther consideration if its correspondingsubgradient is not steep enough.
Weconduct experiments on the Englishdependency parsing task with a thirdorder graph-based parser.
Benefitingfrom the rich features selected in thetree kernel space, our model achieved thebest reported unlabeled attachment scoreof 93.72 without using any additionalresource.1 IntroductionIn Natural Language Processing (NLP) domain,existing linear models typically adopt exhaustivesearch to generate tons of features such thatthe important features are included.
However,the brute-force approach will guickly run outof memory when the feature space is extremelylarge.
Unlike linear models, kernel methodsprovide a powerful and unified framework forlearning a large or even infinite number of featuresimplicitly using limited memory.
However, manykernel methods scale quadratically in the numberof training samples, and can hardly reap thebenefits of learning a large dataset.
For example,the popular Penn Tree Bank (PTB) corpus fortraining an English part of speech (POS) taggerhas approximately 1M words, thus it takes 1M2time to compute the kernel matrix, which isunacceptable using current hardwares.In this paper, we propose a new feature selectionmethod that can efficiently select representativefeatures in the kernel space to improve thequality of linear models.
Specifically, givena limited number of basic features such asthe commonly used unigrams and bigrams, ourmethod performs feature selection in the spaceof their combinations, e.g, the concatenation ofthese n-grams.
A sparse discriminative modelis produced by training L1norm SVMs usingsubgradient methods.
Different from traditionaltraining procedures, we divide the feature vectorinto a number of segments, and sort them in acoarse-to-fine order: the first segment includesthe basic features, the second segment includesthe combined features composed of two basicfeatures, and so on.
In each iteration, we calculatethe subgradient segment by segment.
A combinedfeature and all its further combinations in thefollowing segments can be safely pruned if theabsolute value of its corresponding subgradient isnot sufficiently large.
The algorithm stops untilall features are pruned.
Besides, two simple yeteffective pruning strategies are proposed to filterthe combinations.We conduct experiments on Englishdependency parsing task.
Millions of deep,high order features derived by concatenatingcontextual words, POS tags, directions anddistances of dependencies are selected in thepolynomial kernel and tree kernel spaces.
Theresult is promising: these features significantlyimproved a state-of-the-art third order dependencyparser, yielding the best reported unlabeledattachment score of 93.72 without using anyadditional resource.11802 Related WorksThere are two solutions for learning in ultra highdimensional feature space: kernel method andfeature selection.Fast kernel methods have been intensivelystudied in the past few years.
Recently,randomized methods have attracted more attentiondue to its theoretical and empirical success, suchas the Nystr?om method (Williams and Seeger,2001) and random projection (Lu et al, 2014).In NLP domain, previous studies mainly focusedon polynomial kernels, such as the splitSVM andapproximate polynomial kernel (Wu et al, 2007).In feature selection domain, there has beenplenty of work focusing on fast computation,while feature selection in extremely highdimensional feature space is relatively lessstudied.
Zhang et al (2006) proposed aprogressive feature selection framework that splitsthe feature space into tractable disjoint sub-spacessuch that a feature selection algorithm can beperformed on each one of them, and then mergesthe selected features from different sub-spaces.The search space they studied contained morethan 20 million features.
Tan et al (2012)proposed adaptive feature scaling (AFS) schemefor ultra-high dimensional feature selection.
Thedimensionality of the features in their experimentsis up to 30 millions.Previous studies on feature selection in kernelspace typically used mining based approachesto prune feature candidates.
The key idea forefficient pruning is to estimate the upper bound ofstatistics of features without explicit calculation.The simplest example is frequent mining wherefor any n-gram feature, its frequency is boundedby any of its substrings.Suzuki et al (Suzuki et al, 2004) proposed toselect features in convolution kernel space basedon their chi-squared values.
They derived aconcise form to estimate the upper bound of chi-square values, and used PrefixScan algorithm toenumerates all the significant sub-sequences offeatures efficiently.Okanohara and Tsujii (Okanohara and Tsujii,2009) further combined the pruning techniquewith L1regularization.
They showed theconnection between L1regularization andfrequent mining: the L1regularizer provides aminimum support threshold to prune the gradientsof parameters.
They selected the combinationfeatures in a coarse-to-fine order, the gradientvalue for a combination feature can be boundedby each of its component feature, hence may bepruned without explicit calculation.
They alsosorted the features to tighten the bound.
Our ideais similar with theirs, the difference is that oursearch space is much larger: we did not restrict thenumber of component features.
We recursivelypruned the feature set and in each recursion weselected feature in a batch manner.
We furtheradopted an efficient data structure, spectral bloomfilter, to estimate the gradients for the candidatefeatures without generating them.3 The Proposed Method3.1 Basic IdeaGiven n training samples x1.
.
.
xnwith labelsy1.
.
.
yn?
Y , we extend the kernel over the inputspace to the joint input and output space by simplydefining fT(xi, y)f(xi, y?)
= K(xi, xj)I(y ==y?
), which is the same as Taskar?s (see (Taskar,2004), Page 68), where f is the explicit featuremap for the kernel, and I(?, ?)
is the indicatorfunction.Our task is to select a subset of representativeelements in the feature vector f .
Unlike previouslystudied feature selection problems, the dimensionof f could be extremely high.
It is impossible tostore the feature vector in the memory or even onthe disk.For easy illustration, we describe our methodfor the polynomial kernel, and it can be easilyextended to the tree kernel space.The R degree polynomial kernel space isestablished by a set of basic features B = {b0=1, b1, .
.
.
, b|B|} and their combinations.
In otherwords, each feature is the product of at most Rbasic features fj= bj1?
bj2?
?
?
?
?
bjr, r ?R.
As we assume that all features are binary1, fjcan be rewritten as the minimum of thesebasic features: fj= min{bj1, bj2, .
.
.
, bjr}.
Weuse Bj= {bj1, bj2, .
.
.
, bjr} to denote the set ofcomponent basic features for fj.
r is called theorder of feature fj.
For two features fj, fk, wesay fkis an extension of fjif Bj?
Bk.Take the document classification task as anexample, the basic features could be word n-grams, and the quadratic kernel (degree=2) spaceincludes the combinated features composed of two1Binary features are often used in NLP.1181n-grams, a second order feature is true if both n-grams appear in the document, it is an extension ofany of its component n-grams (first order features).We use L1norm SVMs for feature selection.Traditionally, the L1norm SVMs can be trainedusing subgradient descent and generate a sparseweight vector w for feature f .
Due to the highdimensionality in our case, we divide f into anumber of segments according to the order ofthe feature, the k-th segment includes the k-orderfeatures.
In each iteration, we update the weightsof features segment by segment.
When updatingthe weight of feature fjin the k-th segment, weestimate the subgradients with respective to fj?sextensions in the rest k + 1, k + 2, .
.
.
segmentsand keep their weights at zero if the subgradientsare not sufficiently steep.
In this way, we couldignore these features without explicit calculation.3.2 L1Norm SVMsSpecifically, the objective function for learning L1norm SVMs is:minwO(w) = C?w?1+?iloss(i)whereloss(i) = maxy?Y{wT?f(xi, y) + ?
(yi, y)}is the hinge loss function for the i-th sample.
?f(xi, y) = f(xi, yi) ?
f(xi, y) is the residualfeature vector, ?
(a, b) = 0 if a = b, otherwise?
(a, b) = 1.
Regularization parameter C controlsthe sparsity of w. With higher C, more zeroelements are generated.
We call a feature is firedif its value is 1.The objective function is a sum of piecewiselinear functions, hence is convex.
Subgradientdescent algorithm is one poplar approach forminimizing non-differentiable convex functions, itupdates w usingwnew= w ?
g?twhere g is the subgradient of w, ?tis the stepsize in the t-th iteration.
Subgradient algorithmconverges if the step size sequence is properlyselected (Boyd and Mutapcic, 2006).We are interested in the non-differentiable pointwj= 0.
Let y?i= maxy{wT?f(xi, y) +?
(yi, y)}, the prediction of the current model.According to the definition of subgradient, wehave, for each sample xi, ?f(xi, y?i) is asubgradient of loss(i), thus,?i?f(xi, y?i) is asubgradient of?iloss(i).Adding the penalty term C?w?1, we get thesubset of subgradients at wj= 0 for the objectivefunction?i?fj(xi, y?i)?
C ?
gj?
?i?fj(xi, y?i) + CWe can pick any gjto update wj.
Remind thatour purpose is to keep the model sparse, and wewould like to pick gj= 0 if possible.
That is, wecan keep wj= 0 if |?i?fj(xi, y?i)| ?
C.Obviously, for any j, we have|?i?fj(xi, y?i)| ?
?i?yfj(xi, y) = #fj,i.e., the frequency of feature fj.
Thus, we haveProposition 1 Let C be the threshold of thefrequency, the model generated by the subgradientmethod is sparser than frequent mining.3.3 Feature Selection Using Gradient MiningNow the problem is how to estimate|?i?fj(xi, y?i)| without explicit calculationfor each fj.In the following, we mix the terminologygradient and subgradient without loss of clarity.We define the positive gradient and negativegradient for wj#f+j=?i,yi?=y?ifj(xi, yi)#f?j=?i,yi?=y?ifj(xi, y?i)We have?i?fj(xi, y?i) =?i,y?i?=yi?fj(xi, y?i)= #f+j?#f?jThe estimation problem turns out to be a countingproblem: we collect all the incorrectly predictedsamples, and count #f+j, the frequency of fjfiredby the gold labels, and #f?jthe frequency of fjfired by the predictions.As mentioned above, each feature inpolynomial kernel space is defined asfj= min{b ?
Bj} = min{bj1, .
.
.
, bjr}.Equivalently, we can define fjin a recursiveway, which is more frequently used inthe rest of the paper.
That is, fj=1182min{min{bj2, .
.
.
, bjr},min{bj1, bj3, .
.
.
, bjr}, .
.
.
},which is the mimum of r features of order r ?
1.Formally, denote B?ijas the subset of Bjbyremoving its i-th element, then the r-orderfeature, we have fj= min{h1, .
.
.
, hr}, wherehk= min{b ?
B?kj}, 1 ?
k ?
r.We have the following anti-monotone property,which is the basis of our method#f+j?
#h+k?k#f?j?
#h?k?kIf there exists a k, such that#h+k?
C and#h?k?C, we have|?i?fj(xi, y?i)|= |#f+j?#f?j|?
max{#f+j,#f?j}?
max{mink{#h+k},mink{#h?k}}?
mink{max{#h+k,#h?k}}?
CThe third inequality comes from the wellknown min-max inequality: maximinj{aij} ?minjmaxi{aij}.
Thus, we could prune fjwithoutcalculating its corresponding gradient.This is a chain rule, which means that anyfeature that has fjas its component can alsobe pruned safely.
To see this, suppose ?
=min{.
.
.
, fj, .
.
. }
is such a combined feature, wehave|#?+?#?
?| ?
max{#?+,#??}?
max{#f+j,#f?j}?
CBased on this, we present the gradient miningbased feature selection framework in Algorithm 1.4 Prune the Candidate SetIn practice, Algorithm 1 is far from efficientbecause Line 17 may generate large amountsof candidate features that quickly consume thememory.
In this section, we introduce two pruningstrategies that could greatly reduce the size ofcandidates.Algorithm 1 Feature Selection Using GradientMiningRequire: Samples X = {x1, .
.
.
, xn} with labels{y1, .
.
.
, yn}, basic features B = {b1, .
.
.
, b|B|},threshold C > 0, max iteration number M , degree ofpolynomial kernel R, sequence of learning step {?t}.Ensure: Set of selected features S = {fj}, where fj=min{b ?
Bj},Bj?
B, |Bj| ?
R.1: Sr= ?, r = 1, .
.
.
, R {Srdenotes the selected r-orderfeatures}2: for t = 1 ?
M do3: Set S =?Rr=1Sr, f = the vector of features in S.4: Calculate y?i= maxy{wTf(xi, y) + ?
(yi, y)}, ?i.5: Initialize candidate set A = B6: for r = 1 ?
R do7: for all fj?
A do8: Calculate #f+j=?i,yi?=y?ifj(xi, yi) and#f?j=?i,yi?=y?ifj(xi, y?i)9: if #f+j,#f?j?
C and wj= 0 then10: Remove fjfrom A11: else12: wj= wj+(#f+j?#f?j+Csign(wj))?t13: end if14: end for15: Sr= A16: if r < R then17: Generate order-r + 1 candidates: A =Sr+1?
{h|h = min{f1, .
.
.
fr?
Sr}, orderof h is r + 1}18: end if19: end for20: end for4.1 Pre-TrainingUsually, the weights of features are initializedwith 0 in the training procedure.
However,this will select too many features in the firstiteration, because all samples are mis-classifiedin Line 4, the gradients #f+jand #f?jequalto the frequencies of the features, and many ofthem could be larger than C. Luckily, due tothe convexity of piecewise linear function, theoptimality of subgradient method is irrelevant withthe initial point.
So we can start with a well trainedmodel using a small subset of features such as theset of lower order features so that the predictionis more accurate and the gradients #f+and #f?are much lower.4.2 Bloom FilterThe second strategy is to use bloom filter to reducecandidates before putting them into the candidateset A.A bloom filter (Bloom, 1970) is a space efficientprobabilistic data structure designed to rapidlycheck whether an element is present in a set.
Inthis paper, we use one of its extension, spectral1183bloom filter (Cohen and Matias, 2003), whichcan efficiently calculate the upper bound of thefrequencies of elements.The base data structure of a spectral bloomfilter is a vector of L counters, where all countersare initialized with 0.
The spectral bloom filteruses m hash functions, h1, .
.
.
, hm, that map theelements to the range {1, .
.
.
L}.
When adding anelement f to the bloom filter, we hash it usingthe m hash functions, and get the hash codesh1(f), .
.
.
, hm(f), then we check the counters atpositions h1(f), .
.
.
, hm(f), and get the counts{c1, .
.
.
, cm}.
Let c?be the minimal count amongthese counts: c?= min{c1, .
.
.
, cm}, we increaseonly the counters whose counts are c?, whilekeeping other counters unchanged.To check the frequency of an element, we hashthe element and check the counters in the sameway.
The minimum count c?provides the upperbound of the frequency.
In other words, whenpruning elements with frequencies no greater thana predefined threshold ?, we could safely prune theelement if c??
?.In our case, we use the spectral bloom filter toeliminate the low-frequency candidates.To estimate the gradients of newly generatedr + 1-order candidates, we run Line 17 twice.
Inthe first round, we estimate the upper bound of#h+for each candidate and add the candidateto A if its upper bound is greater than apredefined threshold ?.
The second round issimilar, we add the candidates using the upperbound of h?.
We did not estimate #h+and#h?simultaneously, because this needs twobloom filters for positive and negative gradientsrespectively, which consumes too much memory.Specifically, in the first round, we initializethe spectral bloom filter so that all counters areset to zero.
Then for each incorrectly predictedsample xi, we generate r + 1-order candidatesby combining r-order candidates that are fired bythe gold label i.e., f(xi, yi) = 1.
Once a newcandidate is generated, we hash it and check itscorresponding m counters in the spectral bloomfilter.
If the minimal count c?= ?, we knowthat its positive gradient #f+may be greater than?.
So we keep all counts unchanged, and addthe candidate to A.
Otherwise, we increase thecounts by 1 using the method described above.The second round is similar.HewongamethetodayPRP/VBDthe/gameVBD/NNVBD/NNwongamethewongame today HewontodayFigure 1: A dependency parse tree (top), one ofits feature trees (middle) and some of its subtrees(bottom).
He ?
won ?
today is not a subtreebecause He and today are not adjacent siblings.5 Efficient Candidate Generation5.1 Polynomial KernelAs mentioned above, we generate the r + 1-order candidates by combining the candidates oforder r. An efficient feature generation algorithmshould be carefully designed to avoid duplicates,otherwise #f+and #f?may be over counted.The candidate generation algorithm is kerneldependent.
For polynomial kernel, we justcombine any two r-order candidates and removethe combined feature if its order is not r + 1.This method requires square running time for eachexample.5.2 Dependency Tree Kernel5.2.1 DefinitionCollins and Duffy (2002) proposed tree kernels forconstituent parsing which includes the all-subtreefeatures.
Similarly, we define dependency treekernel for dependency parsing.
For compatibilitywith the previously studied subtree featuresfor dependency parsing, we propose a newdependency tree kernel that is different fromCulotta and Sorensen?s (Culotta and Sorensen,2004).
Given a dependency parse tree Tcomposed of L words, L ?
1 arcs, each arc hasseveral basic features, such as the concatenationof the head word and the modifier word, theconcatenation of the word left to the head and thelower case of the word right to the modifier, thedistance of the arc, the direction of the arc, the1184concatenation of the POS tags of the head and themodifier, etc.A feature tree of T is a tree that has the samestructure as T , while each arc is replaced by anyof its basic features.
For a parse tree that hasL ?
1 arcs, and each arc has d basic features, thenumber of the feature trees is dL?1.
For example,the dependency parse tree for sentence He won thegame today is shown in Figure 1.
Suppose eacharc has two basic features: word pair and POS tagpair.
Then there are 24feature trees, because eacharc can be replaced by either word pair or POS tagpair.A subtree of a tree is a connected fragment inthe tree.
In this paper, to reduce computationalcost, we restrict that adjacent siblings in thesubtrees must be adjacent in the original tree.
Forexample He?
won?
game is a subtree, but He?
won?
today is not a subtree.
The motivationof the restriction is to reduce the number ofsubtrees, for a node having k children, there arek(k?1)/2 subtrees, but without the restriction thenumber of subtrees is exponential: 2k.A sub feature tree of a dependency tree T is afeature tree of any of its subtrees.
For example,the dependency tree in Figure 1 has 12 subtreesincluding four arcs, four arc pairs, the three arctriples and the full feature tree, and each subtreehaving s arcs has 2ssub feature trees.
Thus thedependency tree has 2?4+4?22+3?23+24= 64sub feature trees.Given two dependency trees T1and T2, thedependency tree kernel is defined as the number ofcommon sub feature trees of T1and T2.
Formally,the kernel function is defined asK(T1, T2) =?n1?T1,n2?T2?
(n1, n2)where ?
(n1, n2) denotes the number of commonsub feature trees rooted in n1and n2nodes.Like tree kernel, we can calculate ?
(n1, n2)recursively.
Let ciand c?jdenote the i-thchild of n1and j-th child of n2respectively,let STp,l(n1) denote the set of the sub featuretrees rooted in node n1and the children of theroot are cp, cp+1, .
.
.
, cp+l?1, we denote STq,l(n2)similarly.
Then we define?p,q,l(n1, n2) =?p,q|STp,l(n1)?STq,l(n2)|the number of common sub feature trees inSTp,l(n1) and STq,l(n2).abFigure 2: For any subtree rooted in a with therightmost leaf b, we could extend the subtree byany arc below or right to the path from a to b(shown in black)To calculate?p,q,l(n1, n2), we first consider thesub feature trees with only two levels, i.e., subfeature trees that are composed of n1, n2and someof their children.
We initialize ?p,q,1(n1, n2) withnumber of the common features of arcs n1?
cpand n2?
c?q.
Then we calculate ?p,q,l(n1, n2)recursively using?p,q,l(n1, n2)=?p,q,l?1(n1, n2) ?
?p+l,q+l,1(n1, n2)And ?
(n1, n2) =?p,q,l?p,q,l(n1, n2)Next we consider all the sub feature trees, wehave?p,q,l(n1, n2)=?p,q,l?1(n1, n2) ?
(1 + ?
(cp+l?1, c?q+l?1))Computing the dependency tree kernel for twoparse trees requires |T1|2?
|T2|2?min{|T1|, |T2|}running time in the worst case, as we need toenumerate p, q, l and n1, n2.One way to incorporate the dependency treekernel for parsing is to rerank theK best candidateparse trees generated by a simple linear model.Suppose there are n training samples, the sizeof the kernel matrix is (K ?
n)2, which isunacceptable for large datasets.5.2.2 Candidate GenerationFor constituent parsing, Kudo et al showedsuch an all-subtrees representation is extremelyredundant and a comparable accuracy can beachieved using just a small set of subtrees (Kudoet al, 2005).
Suzuki et al even showed that theover-fitting problem often arises when convolutionkernels are used in NLP tasks (Suzuki et al, 2004).Now we attempt to select representative sub1185feature trees in the kernel space using Algorithm1.
The r-order features in dependency tree kernelspace are the sub feature trees with r arcs.
Thecandidate feature generation in Line 17 has twosteps: first we generate the subtrees with r arcs,then we generate the sub feature trees for eachsubtree.The simplest way for subtree generation is toenumerate the combinations of r + 2 words in thesentence, and check if these words form a subtree.We can speed up the generation by using theresults of the subtrees with r + 1 words (r arcs).For each subtree Srwith r arcs, we can add anextra word to Srand generate Sr+1if the wordsform a subtree.This method has three issues: first, the timecomplexity is exponential in the length of thesentence, as there are 2Lcombinations of words,L is the sentence length; second, it may generateduplicated subtrees, and over counts the gradients.For example, there are two ways to generate thesubtree He won the game in Figure 1: we caneither add word He to the subtree won the game,or add word the to the subtree He won game; third,checking a fragment requires O(L) time.These issues can be solved using the wellknown rightmost-extension method (Zaki, 2002;Asai et al, 2002; Kudo et al, 2005) whichenumerates all subtrees from a given treeefficiently.
This method starts with a set of treesconsisting of single nodes, and then expands eachsubtree attaching a new node.Specifically, it first indexes the words in the pre-order of the parse tree.
When generating Sr+1,only the words whose indices are larger than thegreatest index of the words in Srare considered.In this way, each subtree is generated only once.Thus, we only need to consider two types ofwords: (i) the children of the rightmost leaf of Sr,(ii) the adjacent right sibling of the any node in Sr,as shown in Figure 2.The total number of subtrees is no greater thanL3, because the level of a subtree is less than L,and for the children of each node, there are at mostL2subsequences of siblings.
Therefore the timecomplexity for subtree extraction is O(L3).6 Experiments6.1 Experimental Results on English Dataset6.1.1 SettingsFirst we used the English Penn Tree Bank (PTB)with standard train/develop/test for evaluation.Sections 2-21 (around 40K sentences) were usedas training data, section 22 was used as thedevelopment set and section 23 was used as thefinal test set.We extracted dependencies using JoakimNivre?s Penn2Malt tool with Yamada andMatsumoto?s rules (Yamada and Matsumoto,2003).
Unlabeled attachment score (UAS)ignoring punctuation is used to evaluate parsingquality.We apply our technique to rerank the parse treesgenerated by a third order parser (Koo and Collins,2010) trained using 10 best MIRA algorithmwith 10 iterations.
We generate the top 10 bestcandidate parse trees using 10 fold cross validationfor each sentence in the training data.
The goldparse tree is added if it is not in the candidatelist.
Then we learn a reranking model using thesecandidate trees.
During testing, the score for aparse tree T is a linear combination of the twomodels:score(T ) = ?scoreO3(T ) + scorererank(T )where the meta-parameter ?
= 5 is tunedby grid search using the development dataset.scoreO3(T ) and scorererank(T ) are the outputs ofthe third order parser and the reranking classifierrespectively.For comparison, we implement the followingreranking models:?
Perceptron with Polynomial kernelsK(a,b) = (aTb+ 1)d, d = 2, 4, 8?
Perceptron with Dependency tree kernel.?
Perceptron with features generated bytemplates, including all siblings and fourthorder features.?
Perceptron with the features selected inpolynomial and tree kernel spaces, wherethreshold C = 3.The basic features to establish the kernel spacesinclude the combinations of contextual words orPOS tags of head and modifier, the length and1186whwm, phpm, whpm, phwmph?1pm, ph?1wm, phpm?1, whpm?1ph+1pm, ph+1wm, phpm+1, whpm+1ph?1phpm, phph+1pm, phpm?1pm, phpmpm+1Concatenate features above with length and directionphpbpmTable 1: Basic features in polynomial anddependency tree kernel spaces, wh: the word ofhead node, wmdenotes the word of modifier node,ph: the POS of head node, pmdenotes the POSof modifier node, ph+1: POS to the right of headnode, ph?1: POS to the left of modifier node,pm+1: POS to the right of head node, pm?1: POSto the left of modifier node, pb: POS of a word inbetween head and modifier nodes.direction of the arcs, and the POS tags of the wordslying between the head and modifier, as shown inTable 1.
The POS tags are automatically generatedby 10 fold cross validation during training, anda POS tagger trained using the full training dataduring testing which has an accuracy of 96.9% onthe development data and 97.3% on the test data.As kernel methods are not scalable for largedatasets, we applied the strategy proposed byCollins and Duffy (2002), to break the training setinto 10 chunks of roughly equal size, and trained10 separate kernel perceptrons on these data sets.The outputs from the 10 runs on test exampleswere combined through the voting procedure.For feature selection, we set the maximumiteration number M = 100.
We use the first orderand second order features for pre-training.
Wechoose the constant step size ?t= 1 because wefind this could quickly reduce the prediction errorin very few iterations.We use the SHA-1 hash function to generatethe hash codes for the spectral bloom filter.
TheSHA-1 hash function produces a 160-bit hash codefor each candidate feature.
The hash code is thensegmented into 5 segments, in this way we getfive hash codes h1, .
.
.
, h5.
Each code has 32 bits.Then we create 232(4G) counters.
The threshold?
is set to 3, thus each counter requires 2 bits tostore the counts.
The spectral bloom filter costs1G memory in total.Furthermore, to reduce memory cost, we savethe local data structure such as the selectedfeatures in Step 15 of Algorithm 1 wheneverpossible, and load them into the memory whenneeded.After feature selection, we did not use the L1System UAS TrainingTimeThird Order Parser 93.07 20 hrsQuadratic Kernel(QK) 93.41 6 hrsBiquadratic Kernel(BK) 93.45 6 hrs8-th Degree Polynomial Kernel(8K) 93.27 6 hrsDependency Tree Kernel (DTK) 93.65 10 daysLM with Template Features 93.39 4 minsLM with Features in QK 93.39 9 minsLM with Features in BK 93.44 0.5 hrsLM with Features in 8K 93.30 6 hrsLM with Features in DTK 93.72 36 hrs(Zhang and McDonald, 2014) 93.57 N/A(Zhang et al, 2013) 93.50 N/A(Ma and Zhao, 2012) 93.40 N/A(Bohnet and Kuhn, 2012) 93.39 N/A(Rush and Petrov, 2012) 93.30 N/A(Qian and Liu, 2013) 93.17 N/A(Hayashi et al, 2013) 93.12 1 hr(Martins et al, 2013) 93.07 N/A(Zhang and McDonald, 2012) 93.06 N/A(Koo and Collins, 2010) 93.04 N/A(Zhang and Nivre, 2011) 92.90 N/ATable 2: Comparison between our system and thestate-of-art systems on English dataset.
LM isshort for Linear Model, hrs, mins are short forhours and minutes respectivelySVM for testing, instead, we trained an averagedperceptron with the selected features.
Becausewe find that the averaged perceptron significantlyoutperforms L1SVM.6.1.2 ResultsExperimental results are listed in Table 2, allsystems run on a 64 bit Fedora operation systemwith a single Intel core i7 3.40GHz and 32Gmemory.
We also include results of representativestate-of-the art systems.It is clear that the use of kernels or the deepfeatures in kernel spaces significantly improvesthe baseline third order parser and outperformsthe reranking model with shallow, template-generated features.
Besides, our feature selectionoutperforms kernel methods in both efficiency andaccuracy.It is unsurprising that the dependency treekernel outperforms polynomial kernels, becauseit captures the structured information.
Forexample, polynomial kernels can not distinguishthe grand-child feature or sibling feature from thecombination of two separated arc features.When no additional resource is available, ourparser achieved the best reported performance93.72% UAS on English PTB dataset.
It is1187C #Feat #Template Hours Mem(G) UAS1 0.34G N/A stalled OOM N/A2 0.34G N/A stalled OOM N/A3 33.1M 11.4K 36 4.0 93.725 6.32M 2.1K 20 2.2 93.5510 2.10M 1.6K 5 1.4 93.40Table 3: Feature selection in dependency kernelspace with different threshold C.worth pointing that our method is orthogonal toother reported systems that benefit from advancedinference algorthms, such as cube pruning (Zhangand McDonald, 2014), AD3(Martins et al, 2013),etc.
We believe that combining our techniqueswith others?
will achieve further improvement.Reranking the candidate parse trees of 2416testing sentences takes 67 seconds, about 36sentences per second.To further understand the complexity of ouralgorithm, we perform feature selection independency tree kernel space with differentthresholds C and record the number of selectedfeatures and feature templates, the speed andmemory cost.
Table 3 shows the results.
Wecan see that our algorithm works efficiently whenC ?
3, but for C < 3, the number of selectedfeatures grows drastically, and the program runsout of memory (OOM).6.2 Experimental Results on CoNLL 2009DatasetNow we looked at the impact of our system onnon-English treebanks.
We evaluate our system onsix other languages from the CoNLL 2009 shared-task.
We used the best setting in the previousexperiment: reranking model is trained using thefeatures selected in the dependency tree kernelspace.
For POS tag features we used the predictedtags.As the third order parser can not handlenon-projective parse trees, we used the graphtransformation techniques to produce non-projective structures (Nivre and Nilsson,2005).
First, the training data for the parseris projectivized by applying a number of liftingoperations (Kahane et al, 1998) and encodinginformation about these lifts in arc labels.
Weused the path encoding scheme where the label ofeach arc is concatenated with two binary tags, oneindicates if the arc is lifted, the other indicates ifthe arc is along the lifting path from the syntacticto the linear head.
Then we train a projectiveLanguage Ours Official BestChinese 76.77 79.17Japanese 92.68 92.57German 87.40 87.48Spanish 87.82 87.64Czech 80.51 80.38Catalan 86.98 87.86Table 4: Experimental Results on CoNLL 2009non-English datasets.parser on the transformed data without arc labelinformation and a classifier to predict the arclabels based on the projectivized gold parse treestructure.
During testing, we run the parser andthe classifier in a pipeline to generate a labeledparse tree.
Labeled syntactic accuracy is reportedfor comparison.Comparison results are listed in Table 4.We achieved the best reported results on threelanguages, Japanese, Spanish and Czech.
Notethat CoNLL 2009 also provide the semanticlabeling annotation which we did not used in oursystem.
While some official systems benefit fromjointly learning parsing and semantic role labelingmodels.7 ConclusionIn this paper we proposed a new feature selectionalgorithm that selects features in kernel spacesin a coarse to fine order.
Like frequent mining,the efficiency of our approach comes fromthe anti-monotone property of the subgradients.Experimental results on the English dependencyparsing task show that our approach outperformsstandard kernel methods.
In the future, we wouldlike to extend our technique to other real valuedkernels such as the string kernels and taggingkernels.AcknowledgmentsWe thank three anonymous reviewers for theirvaluable comments.
This work is partly supportedby NSF award IIS-0845484 and DARPA underContract No.
FA8750-13-2-0041.
Any opinionsexpressed in this material are those of the authorsand do not necessarily reflect the views of thefunding agencies.1188ReferencesTatsuya Asai, Kenji Abe, Shinji Kawasoe, HirokiArimura, Hiroshi Sakamoto, and Setsuo Arikawa.2002.
Efficient substructure discovery from largesemi-structured data.
In Proceedings of the SecondSIAM International Conference on Data Mining,Arlington, VA, USA, April 11-13, 2002, pages 158?174.Burton H. Bloom.
1970.
Space/time trade-offs inhash coding with allowable errors.
Commun.
ACM,13(7):422?426, July.Bernd Bohnet and Jonas Kuhn.
2012.
The best ofbothworlds ?
a graph-based completion model fortransition-based parsers.
In Proc.
of EACL.S.
Boyd and A. Mutapcic.
2006.
Subgradient methods.notes for EE364.Saar Cohen and Yossi Matias.
2003.
Spectral bloomfilters.
In Proc.
of SIGMOD, SIGMOD ?03.Michael Collins and Nigel Duffy.
2002.
New rankingalgorithms for parsing and tagging: Kernels overdiscrete structures, and the voted perceptron.
InProc.
of ACL, ACL ?02.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proc.
of ACL,ACL ?04.Katsuhiko Hayashi, Shuhei Kondo, and YujiMatsumoto.
2013.
Efficient stacked dependencyparsing by forest reranking.
TACL, 1.Sylvain Kahane, Alexis Nasr, and Owen Rambow.1998.
Pseudo-projectivity, a polynomiallyparsable non-projective dependency grammar.In Proceedings of the 36th Annual Meeting of theAssociation for Computational Linguistics and17th International Conference on ComputationalLinguistics, Volume 1, pages 646?652, Montreal,Quebec, Canada, August.
Association forComputational Linguistics.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proc.
of ACL.Taku Kudo, Jun Suzuki, and Hideki Isozaki.
2005.Boosting-based parse reranking with subtreefeatures.
In Proceedings of the 43rd AnnualMeeting of the Association for ComputationalLinguistics (ACL?05), pages 189?196, Ann Arbor,Michigan, June.
Association for ComputationalLinguistics.Zhiyun Lu, Avner May, Kuan Liu, Alireza BagheriGarakani, Dong Guo, Aur?elien Bellet, Linxi Fan,Michael Collins, Brian Kingsbury, Michael Picheny,and Fei Sha.
2014.
How to scale up kernelmethods to be as good as deep neural nets.
CoRR,abs/1411.4000.Xuezhe Ma and Hai Zhao.
2012.
Fourth-order dependency parsing.
In Proceedings ofCOLING 2012: Posters, pages 785?796, Mumbai,India, December.
The COLING 2012 OrganizingCommittee.Andre Martins, Miguel Almeida, and Noah A. Smith.2013.
Turning on the turbo: Fast third-order non-projective turbo parsers.
In Proc.
of ACL.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projective dependency parsing.
In Proceedingsof the 43rd Annual Meeting on Association forComputational Linguistics, ACL ?05, pages 99?106, Stroudsburg, PA, USA.
Association forComputational Linguistics.Daisuke Okanohara and Jun?ichi Tsujii.
2009.Learning combination features with l1regularization.
In Proceedings of Human LanguageTechnologies: The 2009 Annual Conference of theNorth American Chapter of the Association forComputational Linguistics, Companion Volume:Short Papers, pages 97?100, Boulder, Colorado,June.
Association for Computational Linguistics.Xian Qian and Yang Liu.
2013.
Branch and boundalgorithm for dependency parsing with non-localfeatures.
TACL, 1.Alexander Rush and Slav Petrov.
2012.
Vine pruningfor efficient multi-pass dependency parsing.
InProc.
of NAACL.
Association for ComputationalLinguistics.Jun Suzuki, Hideki Isozaki, and Eisaku Maeda.2004.
Convolution kernels with feature selection fornatural language processing tasks.
In Proceedingsof the 42nd Meeting of the Association forComputational Linguistics (ACL?04), Main Volume,pages 119?126, Barcelona, Spain, July.Mingkui Tan, Ivor W. Tsang, and Li Wang.
2012.Towards large-scale and ultrahigh dimensionalfeature selection via feature generation.
CoRR,abs/1209.5260.Ben Taskar.
2004.
Learning Structured PredictionModels: A Large Margin Approach.
Ph.D. thesis,Stanford University.Christopher K. I. Williams and Matthias Seeger.
2001.Using the nystr?om method to speed up kernelmachines.
In NIPS.Yu-Chieh Wu, Jie-Chi Yang, and Yue-Shi Lee.
2007.An approximate approach for training polynomialkernel svms in linear time.
In Proc.
of ACL, ACL?07.Hiroyasu Yamada and Yuji Matsumoto.
2003.Statistical dependency analysis with support vectormachines.
In Proc.
of IWPT.1189Mohammed J. Zaki.
2002.
Efficiently mining frequenttrees in a forest.
In Proceedings of the Eighth ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, KDD ?02, pages 71?80, New York, NY, USA.
ACM.Hao Zhang and Ryan McDonald.
2012.
Generalizedhigher-order dependency parsing with cube pruning.In Proc.
of EMNLP.Hao Zhang and Ryan McDonald.
2014.
Enforcingstructural diversity in cube-pruned dependencyparsing.
In Proc.
of ACL.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProc.
of ACL-HLT.Qi Zhang, Fuliang Weng, and Zhe Feng.
2006.A progressive feature selection algorithm for ultralarge feature spaces.
In Proc.
of ACL.Hao Zhang, Liang Huang, Kai Zhao, and RyanMcDonald.
2013.
Online learning for inexacthypergraph search.
In Proc.
of EMNLP, pages 908?913.
Association for Computational Linguistics.1190
