Thesauruses for Prepositional Phrase AttachmentMark McLauchlanDepartment of InformaticsUniversity of SussexBrighton, BN1 9RHmrm21@sussex.ac.ukAbstractProbabilistic models have been effective in re-solving prepositional phrase attachment am-biguity, but sparse data remains a significantproblem.
We propose a solution based onsimilarity-based smoothing, where the proba-bility of new PPs is estimated with informa-tion from similar examples generated using athesaurus.
Three thesauruses are compared onthis task: two existing generic thesauruses anda new specialist PP thesaurus tailored for thisproblem.
We also compare three smoothingtechniques for prepositional phrases.
We findthat the similarity scores provided by the the-saurus tend to weight distant neighbours toohighly, and describe a better score based on therank of a word in the list of similar words.
Oursmoothing methods are applied to an existingPP attachment model and we obtain significantimprovements over the baseline.1 IntroductionPrepositional phrases are an interesting example of syn-tactic ambiguity and a challenge for automatic parsers.The ambiguity arises whenever a prepositional phrasecan modify a preceding verb or noun, as in the canoni-cal example I saw the man with the telescope.
In syn-tactic terms, the prepositional phrase attaches either tothe noun phrase or the verb phrase.
Many kinds of syn-tactic ambiguity can be resolved using structural infor-mation alone (Briscoe and Carroll, 1995; Lin, 1998a;Klein and Manning, 2003), but in this case both candidatestructures are perfectly grammatical and roughly equallylikely.
Therefore ambiguous prepositional phrases re-quire some kind of additional context to disambiguatecorrectly.
In some cases a small amount of lexical knowl-edge is sufficient: for example of almost always modifiesthe noun.
Other cases, such as the telescope example, arepotentially much harder since discourse or world knowl-edge might be required.Fortunately it is possible to do well at this task justby considering the lexical preferences of the words mak-ing up the PP.
Lexical preferences describe the tendencyfor certain words to occur together or only in specificconstructions.
For example, saw and telescope are morelikely to occur together than man and telescope, so wecan infer that the correct attachment is likely to be ver-bal.
The most useful lexical preferences are captured bythe quadruple (v, n1, p, n2) where v is the verb, n1 is thehead of the direct object, p is the preposition and n2 is thehead of the prepositional phrase.
A benchmark datasetof 27,937 such quadruples was extracted from the WallStreet Journal corpus by Ratnaparkhi et al (1994) andhas been the basis of many subsequent studies comparingmachine learning algorithms and lexical resources.
Thispaper examines the effect of particular smoothing algo-rithms on the performance of an existing statistical PPmodel.A major problem faced by any statistical attachment al-gorithm is sparse data, which occurs when plausible PPsare not well-represented in the training data.
For exam-ple, if the observed frequency of a PP in the training iszero then the maximum likelihood estimate is also zero.Since the training corpus only represents a fraction of allpossible PPs, this is probably an underestimate of the trueprobability.
An appealing course of action when facedwith an unknown PP is to consider similar known exam-ples instead.
For example, we may not have any data foreat pizza with fork, but if we have seen eat pasta with forkor even drink beer with straw then it seems reasonable tobase our decision on these instead.Similarity is a rather nebulous concept but for our pur-poses we can define it to be distributional similarity,where two words are considered similar if they occur insimilar contexts.
For example, pizza and pasta are sim-ilar since they both often occur as the direct object ofeat.
A thesaurus collects together lists of such similarwords.
The first step in constructing a thesaurus is tocollect co-occurrence statistics from some large corpusof text.
Each word is assigned a probability distributiondescribing the probability of it occurring with all otherwords, and by comparing distributions we can arrive at asimilarity score.
The corpus, co-occurrence relationshipsand distributional similarity metric all affect the nature ofthe final thesaurus.There has been a considerable amount of researchcomparing corpora, co-occurrence relations and similar-ity measures for general-purpose thesauruses, and thesethesauruses are often compared against wide-coverageand general purpose semantic resources such as Word-Net.
In this paper we examine whether it is useful to tai-lor the thesaurus to the task.
General purpose thesauruseslist words that tend to occur together in free text; wewant to find words that behave in similar ways specifi-cally within prepositional phrases.
To this end we createa PP thesaurus using existing similarity metrics but usinga corpus consisting of automatically extracted preposi-tional phrases.A thesaurus alone is not sufficient to solve the PP at-tachment problem; we also need a model of the lexi-cal preferences of prepositional phrases.
Here we usethe back-off model described in (Collins and Brooks,1995) but with maximum likelihood estimates smoothedusing similar PPs discovered using a thesaurus.
Suchsimilarity-based smoothing methods have been success-fully used in other NLP applications but our use of themhere is novel.
A key difference is that smoothing is notdone over individual words but over entire prepositionalphrases.
Similar PPs are generated by replacing eachcomponent word with a distributionally similar word, andwe define a similarity functions for comparing PPs.
Wefind that using a score based on the rank of a word in thesimilarity list is more accurate than the actual similarityscores provided by the thesaurus, which tend to weightless similar words too highly.In Section 2 we cover related work in PP attachmentand smoothing techniques, with a brief comparison be-tween similarity-based smoothing and the more common(for PP attachment) class-based smoothing.
Section 3 de-scribes Collins?
PP attachment model and our thesaurus-based smoothing extensions.
Section 4 discusses the the-sauruses used in our experiment and describes how thespecialist thesaurus is constructed.
Experimental resultsare given in Section 5 and we show statistically signifi-cant improvements over the baseline model using genericthesauruses.
Contrary to our hypothesis the specialistthesaurus does not lead to significant improvements andwe discuss possible reasons why it underperforms on thistask.2 Previous work2.1 PP attachmentEarly work on PP attachment disambiguation usedstrictly syntactic or high-level pragmatic rules to decideon an attachment (Frazier, 1979; Altman and Steedman,1988).
However, work by Whittemore et al (1990) andHindle and Rooth (1993) showed that simple lexical pref-erences alone can deliver reasonable accuracy.
Hindleand Rooth?s approach was to use mostly unambiguous(v, n1, p) triples extracted from automatically parsed textto train a maximum likelihood classifier.
This achievedaround 80% accuracy on ambiguous samples.This marked a flowering in the field of PP attachment,with a succession of papers bringing the whole armouryof machine learning techniques to bear on the problem.Ratnaparkhi et al (1994) trained a maximum entropymodel on (v, n1, p, n2) quadruples extracted from theWall Street Journal corpus and achieved 81.6% accuracy.The Collins and Brooks (1995) model scores 84.5% accu-racy on this task, and is one of the most accurate modelsthat do not use additional supervision.
The current stateof the art is 88% reported by Stetina and Nagao (1997)using the WSJ text in conjunction with WordNet.
Thenext section discusses other specific approaches that in-corporate smoothing techniques.2.2 Similarity-based smoothingSmoothing for statistical models involves adjusting prob-ability estimates away from the maximum likelihood es-timates to avoid the low probabilities caused by sparsedata.
Typically this involves mixing in probability distri-butions that have less context and are less likely to sufferfrom sparse data problems.
For example, if the probabil-ity of an attachment given a PP p(a|v, n1, p, n2) is unde-fined because that quadruple was not seen in the trainingdata, then a less specific distribution such as p(a|v, n1, p)can be used instead.
A wide range of different techniqueshave been proposed (Chen and Goodman, 1996) includ-ing the backing-off technique used by Collins?
model (seeSection 3).An alternative but complementary approach is to mixin probabilities from distributions over ?similar?
con-texts.
This is the idea behind both similarity-based andclass-based smoothing.
Class-based methods cluster sim-ilar words into classes which are then used in place of ac-tual words.
For example the class-based language modelof (Brown et al, 1992) is defined as:p(w2|w1) = p(w2|c2)p(c2|c1) (1)This helps solve the sparse data problem since thenumber of classes is usually much smaller than the num-ber of words.Class-based methods have been applied to the PP at-tachment task in several guises, using both automaticclustering and hand-crafted classes such as WordNet.
Liand Abe (1998) use both WordNet and an automatic clus-tering algorithm to achieve 85.2% accuracy on the WSJdataset.
The maximum entropy approach of Ratnaparkhiet al (1994) uses the mutual information clustering algo-rithm described in (Brown et al, 1992).
Although class-based smoothing is shown to improve the model in bothcases, some researchers have suggested that clusteringwords is counterproductive since the information lost byconflating words into broader classes outweighs the ben-efits derived from reducing data sparseness.
This remainsto be proven conclusively (Dagan et al, 1999).In contrast, similarity-based techniques do not discardany data.
Instead the smoothed probability of a word isdefined as the total probability of all similar words S(w)as drawn from a thesaurus, weighted by their similarity?(w,w?).
For example, the similarity-based languagemodel of (Dagan et al, 1999) is defined as:p(w2|w1) =?w?1?S(w1)?
(w1, w?1)p(w2|w?1) (2)where?w?1?S(w1)?
(w1, w?1) = 1.
The similarity func-tion reflects how often the two words appear in thesame context.
For example, Lin?s similarity metric (Lin,1998b) used in this paper is based on an information-theoretic comparison between a pair of co-occurrenceprobability distributions.This language model was incorporated into a speechrecognition system with some success (Dagan et al,1999).
Similarity-based methods have also been suc-cessfully applied word sense disambiguation (Dagan etal., 1997) and extraction of grammatical relations (Gr-ishman and Sterling, 1994).
Similarity-based smooth-ing techniques of the kind described here have not yetbeen applied to probabilistic PP attachment models.
Thememory-based learning approach of (Zavrel et al, 1997)is the closest point of contact and shares many of the sameideas, although the details are quite different.
Memory-based learning consults similar previously-seen examplesto make a decision, but the similarity judgements are usu-ally based on a strict feature matching measure ratherthan on co-occurrence statistics.
Under this scheme pizzaand pasta are as different as pizza and Paris.
To overcomethis Zavrel et al also experiment with features based on areduced-dimensionality vector of co-occurrence statisticsand note a small (0.2%) increase in performance, leadingto a final accuracy of 84.4%.Our use of specialist thesauruses for this task is alsonovel, although in they have been used in the some-what related field of selectional preference acquisition byp(a|v, n1, p, n2) =1.
f(a,v,n1,p,n2)f(v,n1,p,n2)2. f(a,v,n1,p)+f(a,v,p,n2)+f(a,n1,p,n2)f(v,n1,p)+f(v,p,n2)+f(n1,p,n2)3. f(a,v,p)+f(a,n1,p)+f(a,p,n2)f(v,p)+f(n1,p)+f(p,n2)4. f(a,p)f(p)5.
Default: noun attachmentFigure 1: Collins and Brooks (1995) backing off algo-rithm.
A less specific context is used when the denomi-nator is zero or p(a|v, n1, p, n2) = 0.5.Takenobu et.
al.
(1995).
Different thesauruses were cre-ated for different grammatical roles such as subject andobject, and used to build a set of word clusters.
Clus-ters based on specialist thesauruses were found to predictfillers for these roles more accurately than generic clus-ters.3 SmoothingOur baseline model is Collins and Brooks (1995) model,which implements the popular and effective backing-off smoothing technique.
The idea is to initially usep(a|v, n1, p, n2), but if there isn?t enough data to supporta maximum likelihood estimate of this distribution, orp(a|v, n1, p, n2) = 0.5, then the algorithm backs off anduses a distribution with less conditioning context.
Thebacking off steps are shown in Figure 1.If we use the similarity-based language model shownin (2) as a guide, then we can create a smoothed versionof Collins?
model using the weighted probability of allsimilar PPs (for brevity we use c in to indicate the context,in this case an entire PP quadruple):p(a|c) =?c??S(c)?
(c, c?)p(a|c?)
(3)In contrast to the language model shown in (2), the setof similar contexts S(c) and similarity function ?
(c, c?
)must be defined for multiple words (we abuse our no-tation slightly by using the same ?
and S for bothPPs and words, but the meaning should be clear fromthe context).
Thesauruses only supply neighbours andsimilarity scores for single words, but we can gener-ate distributionally similar PPs by replacing each wordin the phrase independently with a similar one providedby the thesaurus.
For example, if eat has two neigh-bours: S(eat) = {drink, enjoy}, and pizza has just one:S(pizza) = {pasta}, then the following examples willbe generated for eat pizza with fork:eat pasta with forkdrink pizza with forkdrink pasta with forkenjoy pizza with forkenjoy pasta with forkClearly this strategy of generates some nonsensical orat least unhelpful examples.
This is not necessarily a se-rious problem since such instances should occur at bestinfrequently in the training data.
Unfortunately our base-line model will back off and attempt to provide a rea-sonable probability for them all, for example by usingp(a|with) in place of p(a|enjoy, pasta, with, fork).This introduces unwanted noise into the smoothed prob-ability estimate.Our solution is to apply smoothing to the counts usedby the probability model.
The smoothed frequency ofa prepositional phrase fs(a, c) is the weighted averagefrequency of the set of similar PPs S(c):fs(a, c) =?c??S(c)?
(c, c?
)f(a, c?)
(4)These smoothed frequencies are used to calculate theconditional probabilities for the model.
For example, theprobability distribution in step one is defined as:p(a|v, n1, p, n2) =fs(a, v, n1, p, n2)fs(v, n1, p, n2)Distributionally similar triples are generated for steptwo using the same word replacement strategy andsmoothed frequency estimates for triples are calculatedin the same way as quadruples.
We back off to a smalleramount of context if the smoothed denominator is lessthan 1.
This is done for empirical reasons, since de-cisions based on very low frequency counts are unreli-able.
The distributions used in steps three and four arenot smoothed.
Attempting to disambiguate a PP basedon just two words is risky enough; introducing similarPPs found by replacing these two words with synonymsintroduces too much noise.Quadruples and triples are more reliable since the con-text rules out those unhelpful PPs.
For example, ourmodel automatically deals with polysemous words with-out the need for explicit word sense disambiguation.
Al-though thesauruses do conflate multiple senses in theirneighbour lists, implausible senses result in infrequentPPs.
The similarity set for the PP open plant in Ko-rea might contain open tree in Korea but the latter?s fre-quency is likely to be zero.
Generating triples is riskiersince there is less context to rule out unlikely PPs: thetriple tree in Korea is more plausible and possibly mis-leading.
But our model does have a natural preferencefor the most frequent sense in the thesaurus training cor-pus, which is a useful heuristic for word sense disam-biguation (Pedersen and Bruce, 1997).
For example, ifthe thesaurus is trained on business text then factory willbe ranked higher than tree when the thesaurus trained ona business corpus (this issue is discussed further in Sec-tion 5.2).Finally, to complete our PP attachment scheme weneed to define a similarity function between PPs, ex-pressed fully as ?
((v, n1, p, n2), (v?, n?1, p?, n?2)).
Theraw materials we have to work with are the similarityscores for matching pairs of verbs and nouns as given bythe thesaurus.
We do not smooth preposition counts.
Inthis paper we compare three similarity measures:?
average: The average similarity score of all wordpairs in the PP using the similarity measure pro-vided by the thesaurus.
For example, ?
(c, c?
)when c = (eat, pizza, with, fork) and c?
=(enjoy, pasta, with, fork) is defined as:13?
(eat, enjoy)+?
(pizza, pasta)+?
(fork, fork)The similarity score of identical words is assumedto be 1.?
rank: The rank score of the nth neighbour w?
of aword w is defined as:rs(w,w?)
= ?nwhere 0 ?
?
?
1.
The rank similarity scoresfor the pizza example above when ?
= 0.1 arers(eat, enjoy) = 0.2 and rs(pizza, pasta) = 0.1.The combined score for a PP is found by summingthe rank score for each word pair and subtracting thistotal from one:?
(c, c?)
= 1 ??w?v,n1,n2rs(w,w?
)We impose a floor of zero on this score.
Con-tinuing with the pizza example, the rank simi-larity score between (eat, pizza, with, fork) and(enjoy, pasta, with, fork) is ?
(c, c?)
= 1 ?
0.2 ?0.1 = 0.7.
Note that the similarity score provided bythe thesaurus is used to determine the ranking but itotherwise not used.?
single best: Instead of smoothing using several sim-ilar contexts, we can set ?
(c, c?)
= 1 for the clos-est context for which f(c?)
> 0 and ignore all oth-ers, thereby just replacing an unknown feature with asimilar known one.
This simplified form of smooth-ing may be appropriate for non-statistical modelsor situations where relative frequency estimates arehard to incorporate.4 ThesaurusesAs noted above, a thesaurus is a resource that groups to-gether words that are distributionally similar.
Althoughwe refer to such resources using the singular, a thesaurushas several parts for different word categories such asnouns, verbs and adjectives.We compare three thesauruses on this task.
The firsttwo are large-scale generic thesauruses, both constructedusing the similarity metric described in (Lin, 1998b), butbased on different corpora.
The first, which we call Lin,is derived from 300 million words of newswire text andis available on the Internet1.
The second, which we callWASPS, forms part of the WASPS lexicographical work-bench developed at Brighton University 2 and is derivedfrom the 100 million word BNC.
The co-occurrence re-lations for both are a variety of grammatical relationssuch as direct object, subject and modifier.
WASPS alsoincludes prepositional phrase relations but without at-tempting to disambiguate them.
All possible attachmentsare included under the assumption that correct attach-ments will tend to have higher frequency (Adam Kilgar-riff, p.c.
).These thesauruses are designed to find words that aresimilar in a very general sense, and are often comparedagainst hand-crafted semantic resources such as Word-Net.
However for the PP attachment task semantic sim-ilarity may be less important.
We are more interested inhow words behave in particular syntactic roles.
For exam-ple, eat and bake are rather loosely related semanticallybut will be close neighbours in PP terms if they both of-ten occur with prepositional phrase contexts such as pizzawith anchovies.The third thesaurus is designed to supply such spe-cialised, task-specific neighbours.
It consists of threesub-thesauruses, one for the each of the v, n1 and n2words in the PP (a preposition thesaurus was also con-structed with plausible-looking neighbours but was foundnot to be useful in practice).
The co-occurrence relationsused in each case consist of all possible subsets of thethree remaining words together with the attachment deci-sion.
For example, given eat pizza with fork the followingco-occurrences will be included in the thesaurus trainingcorpus:eat ?
n1-pizza,p-with,n2-fork,Neat ?
n1-pizza,p-with,Neat ?
n1-pizza,n2-fork,Neat ?
p-with,n2-fork,Neat ?
n1-pizza,Neat ?
p-with,Neat ?
n2-fork,N1http://www.cs.ualberta.ca/ lindek/downloads.htm2http://wasps.itri.brighton.ac.ukThe training corpus is created from 3.3 million prepo-sitional phrases extracted from the British National Cor-pus.
These PPs are identified semi-automatically using aversion of the weighted GR extraction scheme describedin (Carroll and Briscoe, 2001).
The raw text is parsedand any PPs that occur in a large percentage of the highlyranked candidate parses are considered reliable and addedto the thesaurus training corpus.
Mostly these are unam-biguous (v, p, n1) or (n1, p, n2) triples from phrases suchas we met in January.
The dataset is rather noisy dueto tagging and parsing errors, so we discarded any co-occurrence relations occurring fewer than 100 times.We use the similarity metric described in Weeds(2003).
This is a parameterised measure that can be ad-justed to suit different tasks, but to ensure compatibilitywith the two generic thesauruses we chose parameter set-tings that mimic Lin?s measure.5 ExperimentsFor our experiments we use the Wall Street Journaldataset created by Ratnaparkhi et al (1994).
This isdivided into a training set of 20,801 words, a develop-ment set of 4,039 words and a test set of 3,097 words.Each word was reduced to its morphological root usingthe morphological analyser described in (Minnen et al,2000).
Strings of four digits beginning with a 1 or 2are replaced with YEAR and all other digit strings in-cluding those including commas and full stops were re-placed with NUM.
Our implementation of Collins?
algo-rithm only achieves 84.3% on the test data, with the short-fall of 0.2% primarily due to the different morphologicalanalysers used35.1 SmoothingFirstly we compare the different PP similarity functions.Figure 2 shows the accuracy of each as a function ofk, the number of examples in S(c) .
The WASPS the-saurus was used in all cases.
The best smoothed model isrank with 85.1% accuracy when ?
= 0.05 and k = 15.The accuracy of rank with the smallest ?
value drops offrapidly when k > 10, showing that neighbours beyondthis point are providing unreliable evidence and shouldbe discounted more aggressively.
More interestingly, thisproblem also affects average, suggesting that the similar-ity scores provided by the thesaurus are also misleadinglyhigh for less similar words.
The same effect was also ob-served when we used the harmonic mean of all similarityscores, so it is unlikely that the problem is an artifact ofthe averaging operation.On the other hand, if ?
is set quite low (for example3This result is interesting since this analyser is more accuratethan the one used by Collins.
We chose to use this analyserbecause it matches the word forms in the thesauruses better.83.483.683.88484.284.484.684.88585.20  5  10  15  20  25Accuracy num.
neighbours (k)rank 0.01rank 0.05rank 0.1averagesingleFigure 2: Accuracy for different smoothing functions onthe development set plotted against k, the number of sim-ilar words used for smoothing?
= 0.01) then accuracy levels off very quickly as lesssimilar neighbours are assigned zero frequency.
The mid-dle value of ?
= 0.05 appears to offer a good trade-off.Regardless of the similarity function we can see that rel-atively small values for k are sufficient, which is goodnews for efficiency reasons (each attachment decision isan O(k) operation).Figure 3 shows the combined coverage of the tripleand quadruple features in Collins?
model, which are theonly smoothed features in our model.
For example, al-most 75% of attachment decisions are resolved by 3- or4-tuples using the average function and setting k = 25.Again, average is comparable to rank with ?
= 0.01.Table 1 compares the accuracy of the smoothed and un-smoothed models at each backing off stage.
Smoothinghas a negative effect on accuracy, but this is made for byan increase in accuracy.The reduction in the error rate with the single best pol-icy on the development set is somewhat less than with thesmoothed frequency models, and the results more error-prone and sensitive to the choice of k. These modelsare more likely to be unlucky with a choice of featurethan with the smoothed frequencies.
As noted above, thistechnique may still be useful for algorithms which cannotSmoothed Unsm.Stage Acc.
Cov.
Acc.
Cov.1 90.9 12.4 91.2 8.52 87.3 49.7 87.5 33.53 80.8 34.2 82.1 54.24 73.4 3.6 73.9 3.7Table 1: Accuracy and coverage of the first two backingoff stages on the development data.
The smoothed modeluses WASPS with ?
= 0.5 and k = 5.303540455055606570750  5  10  15  20  25Coverage num.
neighbours (k)rank 0.01rank 0.05rank 0.1averagesingleFigure 3: Coverage for different smoothing functionsagainst the number of neighbours used for smoothingeasily incorporate smoothed frequency estimates.5.2 ThesaurusesA thesaurus providing better neighbours should do betteron this task.
Figure 4 shows the accuracy of the three the-sauruses using rank smoothing and ?
= 0.05 on the de-velopment data.
Final results using k = 5 and ?
= 0.05on the data is shown in Table 2, together with the size ofthe noun sections of each thesaurus (the direct object the-saurus in the case of specialist) and coverage of 3- and4-tuples.Clearly both generic thesauruses consistently outper-form the specialist thesaurus.
The latter tends to pro-duce neighbours with have less obvious semantic simi-larity, for example providing pour as the first neighbourof fetch.
We hypothesised that using syntactic rather thansemantic neighbours could be desirable, but in this case itoften generates contexts that are unlikely to occur: pourprice of profit as a neighbour of fetch price of profit, forexample.
Although this may be a flaw in the approach,we may simply be using too few contexts to create a re-liable thesaurus.
Previous research has found that usingmore data leads to better quality thesauruses (Curran andMoens, 2002).
We are also conflating attachment prefer-ences, since a word must appear with similar contexts inboth noun and verb modifying PPs to achieve a high sim-Thesaurus Acc.
Size (N) Cov.None 84.30 - 30.5Lin 85.02 13,850 72.1WASPS 85.05 17,843 60.1Specialist 84.50 5,669 61.0Table 2: Accuracy on the test data using ?
= 0.05 andk = 5; the size of the noun section of each thesaurus, andcoverage of smoothed 4- and 3-tuples8484.284.484.684.88585.20  5  10  15  20  25Accuracy num.
neighbours (k)WASPSLinSpecialistFigure 4: Accuracy of the three different thesauruses onthe development set using rank smoothing with ?
= 0.05Method Accuracy WN?Zavrel et.
al.
(1997) 84.1 NoWASPS 85.1 NoLi & Abe (1998) 85.2 YesStetina & Nagao (1997) 88.1 YesTable 3: Accuracy of various attachment models usingWordNet or automatic clustering algorithmsilarity score.
There may be merit in creating separate the-sauruses for noun-attachment and verb-attachment, sincethere may be words that are strongly similar in only oneof these cases.Interestingly, although Lin is smaller than WASPS ithas better coverage.
This is most likely due to the differ-ent corpora used to construct each thesaurus.
Lin is builtusing newswire text which is closer in genre to the WallStreet Journal.
For example, the first neighbour for fetchin WASPS is grab, but none of the top 25 neighbours ofthis word in Lin have this sporting sense.
Both WASPSand specialist are derived from the BNC and have similarcoverage, although the quality of specialist neighbours isnot as good.The WASPS and Lin models produce statisticallysignificant (P < 0.05) improvements over the vanillaCollins model using a paired t-test with 10-fold cross-validation on the entire dataset4.
The specialist model isnot significantly better.
Table 3 compares our results withother comparable PP attachment models.On the face of it, these are not resounding improve-ments over the baseline, but this is a very hard task.Ratnaparkhi (1994) established a human upper bound of88.2% but subsequent research has put this as low as78.3% (Mitchell, 2003).
At least two thirds of the re-4The Collins model achieves 84.50?1.0% accuracy and thesmoothed model 84.90?1.0% accuracy by this measure.maining errors are therefore likely to be very difficult.An inspection of the data shows that many of the re-maining errors are due to poor neighbouring PPs be-ing used for smoothing.
For example, the PP in entrustcompany with cash modifies the verb, but no matchingquadruples are present in the training data.
The onlymatching (n1, p, n2) triple using WASPS is (industry, for,income), which appears twice in the training data modi-fying the noun.
The model therefore guesses incorrectlyeven though the thesaurus is providing what appear to besemantically appropriate neighbours.
Another example isattend meeting with representative, where the (v, p, n2)triple (talk, with, official) convinces the model to incor-rectly guess verb attachment.Part of the problem is that words in the PP are replacedindependently and without consideration to the remainingcontext.
However we had hoped the specialist thesaurusmight alleviate this problem by providing neighbours thatare more appropriate for this specific task.
Finding goodneighbours for verbs is clearly more difficult than fornouns since subcategorisation and selectional preferencesalso play a role.6 ConclusionOur results show that the similarity-based smoothing offrequency estimates significantly improves an already re-spectable probabilistic PP attachment model.
Howeverour hypothesis that a task-specific thesaurus would out-perform a generic thesaurus was not borne out by ourexperiments.
The neighbours provided by the specialistthesaurus are not as informative as those supplied by thegeneric thesauruses.
Of course, this negative result is nat-urally good news for developers of generic thesauruses.We described ways of finding and scoring distribution-ally similar PPs.
A significant number of errors in thefinal model can be traced to the way individual words inthe PP are replaced without regard to the wider context,producing neighbouring PPs that have conflicting attach-ment preferences.
The specialist thesaurus was not ableto overcome this problem.
A second finding is that dis-tributional similarity scores provided by all thesaurusesweight dissimilar neighbours too highly, and more ag-gressive weighting schemes are better for smoothing.Our aim is to apply similarity-based smoothing withboth generic and specialist thesauruses to other areas inlexicalised parse selection, particularly other overtly lex-ical problems such as noun-noun modifiers and conjunc-tion scope.
Lexical information has a lot of promise forparse selection in theory, but there are practical problemssuch as sparse data and genre effects (Gildea, 2001).
Ap-propriately trained thesauruses and similarity-based tech-niques should help to alleviate both problems.AcknowledgementsMany thanks to Julie Weeds and Adam Kilgarriff for pro-viding the specialist and WASPS thesauruses, and foruseful discussions.
Thanks also to the anonymous re-viewers for many helpful comments.ReferencesGerry Altman and Mark Steedman.
1988.
Interactionwith context during human sentence processing.
Cog-nition, 30:191?238.Edward Briscoe and John Carroll.
1995.
Developing andevaluating a probabilistic lr parser of part-of-speechand punctuation labels.
In Proceedings of the IWPT?95, pages 48?58.Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza,Jenifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):468?479.John Carroll and Ted Briscoe.
2001.
High precision ex-traction of grammatical relations.
In Proceedings ofthe IWPT ?01.Stanley Chen and Joshua Goodman.
1996.
An empiricalstudy of smoothing techniques for language modelling.In Proceedings of ACL ?96, pages 310?318.Michael Collins and James Brooks.
1995.
Prepositionalphrase attachment through a backed-off model.
In Pro-ceedings of EMNLP ?95, pages 27?38.James Curran and Mark Moens.
2002.
Scaling contextspace.
In Proceedings of the ACL ?02, pages 222?229.Ido Dagan, Lillian Lee, and Fernando Pereira.
1997.Similarity-based methods for word sense disambigua-tion.
In Proceedings of ACL ?97, pages 56?63.Ido Dagan, Lillian Lee, and Fernando Pereira.
1999.Similarity-based models of word cooccurrence prob-abilities.
Machine Learning, 34(1-3):43?69.L.
Frazier.
1979.
On comprehending sentences: Syn-actic parsing strategies.
Ph.D. thesis, University ofConnecticut.Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Proceedings of EMNLP ?01, Pittsburgh,PA.Ralph Grishman and John Sterling.
1994.
Generalizingautomatically generated selectional patterns.
In Pro-ceedings of COLING ?94, pages 742?747.Don Hindle and Mats Rooth.
1993.
Structural ambigu-ity and lexical relations.
Computational Linguistics,19:103?120.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalised parsing.
In Proceedings of ACL ?03.Hang Li and Naoki Abe.
1998.
Word clustering anddisambiguation based on co-occurrence data.
In Pro-ceedings of COLING ?98, pages 749?755.Dekang Lin.
1998a.
Dependency-based evaluation ofMINIPAR.
In Workshop on the Evaluation of ParsingSystems.Dekang Lin.
1998b.
An information-theoretic measureof similarity.
In Proceedings of ICML ?98, pages 296?304.Guido Minnen, John Carroll, and Darren Pearce.
2000.Robust, applied morphological generation.
In Pro-ceedings of INLG 2000, pages 201?208.Brian Mitchell.
2003.
Prepositional phrase attachmentusing machine learning algorithms.
Ph.D. thesis, Uni-versity of Sheffield.Ted Pedersen and Rebecca Bruce.
1997.
Distinguish-ing word senses in untagged text.
In Proceedings ofEMNLP ?97.Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.1994.
A maximum entropy model for prepositionalphrase attachment.
In Proceedings of the ARPA Work-shop on Human Language Technology, pages 250?255.Jiri Stetina and Makoto Nagao.
1997.
Corpus based PPattachment ambiguity resolution with a semantic dic-tionary.
In Proceedings of WVLC ?97, pages 66?80.Tokunaga Takenobu, Iwayama Makoto, and TanakaHozumi.
1995.
Automatic thesaurus constructionbased on grammatical relations.
In Proceedings of IJ-CAI ?95, pages 1308?1313.Julie Weeds.
2003.
A general framework for distribu-tional similarity.
In Proceedings of the EMNLP ?03.G.
Whittemore, K. Ferrara, and H. Brunner.
1990.
Em-pirical study of predictive powers of simple attachmentschemes for post-modifier prepositional phrases.
InProceedings of ACL ?90, pages 23?30.Jakub Zavrel, Walter Daelemans, and Jorn Veenstra.1997.
Resolving PP attachment ambiguities withmemory-based learning.
In Proceedings of CoNLL?97, pages 136?144.
