Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 259?268,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsWord Ordering with Phrase-Based GrammarsAdri`a de Gispert, Marcus Tomalin, William ByrneDepartment of Engineering, University of Cambridge, UKad465@cam.ac.uk, mt126@cam.ac.uk, wjb31@cam.ac.ukAbstractWe describe an approach to word orderingusing modelling techniques from statisti-cal machine translation.
The system in-corporates a phrase-based model of stringgeneration that aims to take unorderedbags of words and produce fluent, gram-matical sentences.
We describe the gen-eration grammars and introduce parsingprocedures that address the computationalcomplexity of generation under permuta-tion of phrases.
Against the best previousresults reported on this task, obtained us-ing syntax driven models, we report hugequality improvements, with BLEU scoregains of 20+ which we confirm with hu-man fluency judgements.
Our system in-corporates dependency language models,large n-gram language models, and mini-mum Bayes risk decoding.1 IntroductionWord ordering is a fundamental problem in NLPand has been shown to be NP-complete in dis-course ordering (Althaus et al., 2004) and in SMTwith arbitrary word reordering (Knight, 1999).Typical solutions involve constraints on the spaceof permutations, as in multi-document summari-sation (Barzilay and Elhadad, 2011) and preorder-ing in SMT (Tromble and Eisner, 2009; Genzel,2010).Some recent work attempts to address the fun-damental word ordering task directly, using syn-tactic models and heuristic search.
Wan et al.
(2009) use a dependency grammar to address wordordering, while Zhang and Clark (2011; 2012)use CCG and large-scale n-gram language models.These techniques are applied to the unconstrainedproblem of generating a sentence from a multi-setof input words.We describe GYRO (Get Your Order Right), aphrase-based approach to word ordering.
Given abag of words, the system first scans a large, trustedtext collection and extracts phrases consisting ofwords from the bag.
Strings are then generatedby concatenating these phrases in any order, sub-ject to the constraint that every string is a validreordering of the words in the bag, and the re-sults are scored under an n-gram language model(LM).
The motivation is that it is easier to makefluent sentences from phrases (snippets of fluenttext) than from words in isolation.GYRO builds on approaches developed for syn-tactic SMT (Chiang, 2007; de Gispert et al., 2010;Iglesias et al., 2011).
The system generates stringsin the form of weighted automata which can berescored using higher-order n-gram LMs, depen-dency LMs (Shen et al., 2010), and MinimumBayes Risk decoding, either using posterior prob-abilities obtained from GYRO or SMT systems.We report extensive experiments using BLEUand conclude with human assessments.
Weshow that despite its relatively simple formulation,GYRO gives BLEU scores over 20 points higherthan the best previously reported results, gener-ated by a syntax-based ordering system.
Humanfluency assessments confirm these substantial im-provements.2 Phrase-based Word OrderingWe take as input a bag of N words ?
={w1, .
.
.
, wN}.
The words are sorted, e.g.
alpha-betically, so that it is possible to refer to the ithword in the bag, and repeated words are distincttokens.
We also take a set of phrases, L(?)
that259are extracted from large text collections, and con-tain only words from ?.
We refer to phrases as u,i.e.
u ?
L(?).
The goal is to generate all permu-tations of ?
that can be formed by concatenationof phrases from L(?
).2.1 Word Order Generation GrammarConsider a subset A ?
?.
We can represent A byan N-bit binary string I(A) = I1(A) .
.
.
IN(A),where Ii(A) = 1 if wi?
A, and Ii(A) = 0 other-wise.
A Context-Free Grammar (CFG) for gener-ation can then be defined by the following rules:Phrase-based Rules: ?A ?
?
and ?u ?
L(A)I(A)?
uConcatenation Rules: ?A ?
?, B ?
A,C ?
Asuch that I(A) = I(B)+I(C) and I(B)?I(C) =0I(A)?
I(B) I(C)where ?
is the bit-wise logical ANDRoot: S ?
I(?
)We use this grammar to ?parse?
the list of thewords in the bag ?.
The grammar has one non-terminal per possible binary string, so potentially2Ndistinct nonterminals might be needed to gen-erate the language.
Each nonterminal can produceeither a phrase u ?
L(A), or the concatenation oftwo binary strings that share no bits in common.
Aderivation is sequence of rules that starts from thebit string I(?).
Rules are unweighted in this basicformulation.For example, assume the following bag?
= {a, b, c, d, e}, which we sort alphabet-ically.
Assume the phrases are L(?)
={?a b?, ?b a?, ?d e c?}.
The generation grammarcontains the following 6 rules:R1: 11000?
abR2: 11000?
baR3: 00111?
decR4: 11111?
11000 00111R5: 11111?
00111 11000R6: S?
11111Figure 1 represents all the possible derivationsin a hypergraph, which generate four alternativestrings.
For example, string ?d e c b a?
is ob-tained with derivation R6R5R3R2, whereas string?a b d e c?
is obtained via R6R4R1R3.2.2 Parsing a Bag of WordsWe now describe a general algorithm for parsing abag of words with phrase constraints.
The searcha b c d e11000 001113121221111111221{"a b d e c","b a d e c","d e c a b","d e c b a"}{"d e c"}{"a b", "b a"}Figure 1: Hypergraph representing gen-eration from {a, b, c, d, e} with phrases{?a b?, ?b a?, ?d e c?
}.is organized along a two-dimensional gridM [x, y]of 2N?1 cells, where each cell is associated witha unique nonterminal in the grammar (a bit stringI with at least one bit set to 1).
Each row x inthe grid has(Nx)cells, representing all the possibleways of covering exactly x words from the bag.There are N rows in total.For a bit string I , X(I) is the length of I , i.e.the number of 1?s in I .
In this way X(I(A))points to the row associated with set A. Thereis no natural ordering of cells within a row, sowe introduce a second function Y (I) which indi-cates which cell in row X(I) is associated with I .Hence M [X(I), Y (I)] is the cell associated withbit string I .
In the inverse direction, we using thenotation Ix,yto indicate a bit string associated withthe cell M [x, y].The basic parsing algorithm is given in Figure 2.We first initialize the grid by filling the cells linkedto phrase-based rules (lines 1-4 of Figure 2).
Thenparsing proceeds as follows.
For each row in in-creasing order (line 5), and for each of the non-empty cells in the row (line 6), try to combine itsbit string with any other bit strings (lines 7-8).
Ifcombination is admitted, then form the resultantbit string and add the concatenation rule to the as-sociated cell in the grid (lines 9-10).
The combi-nation will always yield a bit string that resides ina higher row of the grid, so search is exhaustive.If a rule is found in cell M [N, 1], there is a parse(line 11); otherwise none exists.
The complexityof the algorithm isO(2N?K).
If back-pointers arekept, traversing these from cell M [N, 1] yields allthe generated word sequences.The number of cells will grow exponentially asthe bag grows in size.
In practice, the number of260PARSE-BAG-OF-WORDSInput: bag of words ?
of size NInput: list of phrases L(?
)Initialize - Add phrase-based rules:1 M [x, y]?
?2 for each subset A ?
?3 for each phrase u ?
L(A)4 add rule I(A)?
u to cell M [X(I(A)), Y (I(A))]Parse:5 for each row x = 1, .
.
.
, N6 for each y = 1, .
.
.
,(Nx)7 for each valid A ?
?8 if Ix,y?
I(A) = 0, then9 I??
Ix,y+ I(A)10 add rule I??
Ix,yI(A) to cell M [X(I?
), Y (I?
)]11 if |M [N, 1]| > 0, success.Figure 2: Parsing algorithm for a bag of words.cells actually used in parsing can be smaller than2N?
1.
This depends strongly on the number ofdistinct phrase-based rules and the distinct subsetsof ?
they cover.
For example, if we consider 1-word subsets of ?, then all cells are needed andGYRO attempts all word permutation.
However,if only 10 distinct 5-word phrases and 20 distinct4-word phrases are considered for a bag of N=9words, then fewer than 431 cells will be used (20+ 10 for the initial cells at rows 4 and 5; plus allcombinations of 4-word subsets into row 8, whichis less than 400; plus 1 for the last cell at row 9).2.3 Generation from Exact ParsingWe are interested in producing the space of wordsequences generated by the grammar, and in scor-ing each of the sequences according to a word-based n-gram LM.
Assuming that parsing the bagof words suceeded, this is a very similar scenarioto that of syntax-based approaches to SMT: theoutput is a large collection of word sequences,which are built by putting together smaller unitsand which can be found by a process of expansion,i.e.
by traversing the back-pointers from an initialcell in a grid structure.
A significant difference isthat in syntax-based approaches the parsing stagetends to be computationally easier than the pars-ing stage has only a quadratic dependency on thelength of the input sentence.We borrow techniques from SMT to representand manipulate the space of generation hypothe-ses.
Here we follow the approach of expand-ing this space onto a Finite-State Automata (FSA)described in (de Gispert et al., 2010; Iglesias etal., 2011).
This means that in parsing, each cellM [x, y] is associated with an FSA Fx,y, which en-codes all the sequences generated by the grammar0111000200111 3001111100001a2b 3ba0 1d 2e 3c01a2b7d3ba8e4d 5e6c9c10a11bba110000011111111Expansion of RTN 11111Figure 3: RTN representing generation from{a, b, c, d, e} with phrases {?a b?, ?b a?, ?d e c?
}(top) and its expansion as an FSA (bottom).when covering the words marked by the bit stringof that cell.
When a rule is added to a cell, a newpath from the initial to the final state of Fx,yiscreated so that each FSA is the union of all pathsarising from the rules added to the cell.
Impor-tantly, when an instance of the concatenation ruleis added to a cell, the new path is built with onlytwo arcs.
These point to other FSAs at lower rowsin the grid so that the result has the form of aRecursive Transition Network with a finite depthof recursion.
Following the example from Sec-tion 2.1, the top three FSAs in Figure 3 representthe RTN for example from Figure 1.The parsing algorithm is modified as follows:4 add rule I(A)?
uas path to FSA FX(I(A)),Y (I(A))...10 add rule I??
Ix,yI(A)as path to FSA FX(I?
),Y (I?
)11 if NumStates(FN,1) > 1, success.At this point we specify two strategies:Algorithm 1: Full expansion is described by thepseudocode in Figure 4, excluding lines 2-3.
Arecursive FSA replacement operation (Allauzen etal., 2007) can be used to expand the FSA in thetop-most cell.
In our running example, the result261is the FSA at the bottom of Figure 3.
We thenapply a word-based LM to the resulting FSA viastandard FSA composition.
This outputs the com-plete (unpruned) language of interest, where eachword sequence generated from the bag accordingto the phrasal constraints is scored by the LM.Algorithm 2: Pruned expansion is described bythe pseudocode in Figure 4, now including lines2-3.
We introduce pruning because full, unprunedexpansion may not be feasible for large bags withmany phrasal rules.
Once parsing is done, we in-troduce the following bottom-up pruning strategy.For each row starting at row r, we union all FSAsof the row and expand the unioned FSA throughthe recursive replacement operation.
This yieldsthe space of all generation hypotheses of lengthr.
We then apply the language model to this lat-tice and reduce it under likelihood-based pruningat weight ?.
We then update each cell in the rowwith a new FSA obtained as the intersection of itsoriginal FSA and the pruned FSA.1This intersec-tion may yield an empty FSA for a particular cell(meaning that all its hypotheses were pruned outof the row), but it will always leave at least onesurviving FSA per row, guaranteeing that if pars-ing succeeds, the top-most cell will expand intoa non-empty FSA.
As we process higher rows,the replacement operation will yield smaller FSAsbecause some back-pointers will point to emptyFSAs.
In this way memory usage can be con-trolled through parameters r and ?.
Of course,when pruning in this way, the final output latticeL will not contain the complete space of hypothe-ses that could be generated by the grammar.2.4 Algorithm 3: Pruned Parsing andGenerationThe two generation algorithms presented aboverely on a completed initial parsing step.
However,given that the complexity of the parsing stage isO(2N?
K), this may not be achievable in prac-tice.
Leaving aside time considerations, the mem-ory required to store 2NFSAs will grow exponen-tially in N , even if the FSAs contain only pointersto other FSAs.
Therefore we also describe an al-gorithm to perform bottom-up pruning guided by1This step can be performed much more efficiently witha single forward pass of the resultant lattice.
This is possiblebecause the replace operation can yield a transducer wherethe input symbols encode a pointer to the original FSA, soin traversing the arcs of the pruned lattice, we know whicharcs will belong to which cell FSAs.
However, for ease ofexplanation we avoid this detail.FULL-PARSE-EXPANSIONInput: bag of words ?
of size NInput: list phrases L(?
)Input: word-based LM GOutput: word lattice L of generated sequencesGenerate:1 PARSE-BAG-OF-WORDS(?
)2 for each row x = r, .
.
.
, N ?
13 PRUNE-ROW(x)4 F ?
FSA-REPLACE(FN,1)5 return L?
F ?G6 function PRUNE-ROW(x) :7 F ?
?yFx,y8 F ?
FSA-REPLACE(F )9 F ?
F ?G10 F ?
FSA-PRUNE(F, ?
)11 for each cell y = 1, .
.
.
,(Nx)12 Fx,y?
Fx,y?
F13 returnFigure 4: Pseudocode for Algorithm 1 (excludinglines 2-3) and Algorithm 2 (including all lines).the LM during parsing.
The pseudocode is identi-cal to that of Algorithm 1 except for the followingchanges: in parsing (Figure 2) we pass G as inputand we call the row pruning function of Figure 4after line 5 if x ?
r.We note that there is a strong connection be-tween GYRO and the IDL approach of Soricutand Marcu (2005; 2006).
Our bag of words parsercould be cast in the IDL-formalism, and the FSA?Replace?
operation would be expressed by anIDL ?Unfold?
operation.
However, whereas theirwork applies pruning in the creation of the IDL-expression prior to LM application, GYRO usesunweighted phrase constraints so the LM must beconsidered for pruning while parsing.3 Experimental ResultsWe now report various experiments evaluating theperformance of the generation approach describedabove.
The system is evaluated using the MT08-nw, and MT09-nw testsets.
These correspond tothe first English reference of the newswire por-tion of the Arabic-to-English NIST MT evalua-tion sets2.
They contain 813 and 586 sentencesrespectively (53,325 tokens in total; average sen-tence length = 38.1 tokens after tokenization).
Inorder to reduce the computational complexity, allsentences with more than 20 tokens were dividedinto sub-sentences, with 20 tokens being the up-per limit.
Between 70-80% of the sentences in the2http://www.itl.nist.gov/iad/mig/tests/mt2626 8 10 12 14 16 18 20110100100010000 2grams3grams4grams5gramsSize of the bag of wordsNumber of n-gramsFigure 5: Average number of extracted phrases asa function of the bag of word size.testsets were divided in this way.
For each of thesesentences we create a bag.The GYRO system uses a n-gram LM estimatedover 1.3 billion words of English text, includingthe AFP and Xinhua portions of the GigaWordcorpus version 4 (1.1 billion words) and the En-glish side of various Arabic-English parallel cor-pora typically used in MT evaluations (0.2 billionwords).Phrases of up to length 5 are extracted for eachbag from a text collection containing 10.6 bil-lion words of English news text.
We use efficientHadoop-based look-up techniques to carry out thisextraction step and to retrieve rules for genera-tion (Pino et al., 2012).
The average number ofphrases extracted as a function of the size of thebag is shown in Figure 5.
These are the phrase-based rules of our generation grammar.3.1 Computational AnalysisWe analyze here the computational requirementsof the three alternative GYRO algorithms pre-sented in Sections 2.3 and 2.4.
We carry out thisanalysis on a subset of 200 random subsentencesfrom MT08-nw and MT09-nw chosen to have thesame sentence length distribution as the wholedata set.
For a fixed generation grammar com-prised of 3-gram, 4-gram and 5-gram rules only,we run each algorithm with a memory limitationof 20GB.
If the process reaches this limit, then itis killed.
Figure 6 reports the worst-case memorymemory required by each algorithm as a functionof the size of the bag.As shown, Full Expansion (Algorithm 1) is onlyfeasible for bags that contain at most 12 words.By contrast, Pruned Expansion (Algorithm 2) with?
= 10 is feasible for bags of up to 18 words.
For4 6 8 10 12 14 16 18 2002468101214161820bag of words sizememory consumption(in GB)Algorithm 1Algorithm 2 ?=10Algorithm 3 ?=10Algorithm 3 ?=5Figure 6: Worst-case memory required (GB) byeach GYRO algorithm relative to the size of thebags.bigger bags, the requirements of unpruned pars-ing make generation intractable under the mem-ory limit.
Finally, Pruned Parsing and Generation(Algorithm 3) is feasible at all bag sizes (up to 20words), and its memory requirements can be con-trolled via the beam-width pruning parameter ?.Harsher pruning (i.e.
lower ?)
will incur morecoverage problems, so it is desirable to use thehighest feasible value of ?.We emphasise that Algorithm 3, with suitablepruning strategies, can scale up to larger problemsquite readily and generate output from much largerinput sets than reported here.
We focus here ongeneration quality for moderate sized problems.3.2 Generation PerformanceWe now compare the GYRO system with theCombinatory Categorial Grammar (CCG)-basedsystem described in (Zhang et al., 2012).
Bymeans of extracted CCG rules, the CCG sys-tem searches for an optimal parse guided bylarge-margin training.
Each partial hypothesis (or?edge?)
is scored using the syntax model and a 4-gram LM trained similarly on one billion words ofEnglish Gigaword data.
Both systems are evalu-ated using BLEU (Papineni et al., 2002; Espinosaet al., 2010).For GYRO, we use the pruned parsing algo-rithm of Section 2.4 with r = 6 and ?
= 10and a memory usage limit of 20G.
The phrase-based rules of the grammar contain only 3-grams,263LM System MT08-nw MT09-nw4g CCG 48.0 48.83g GYRO 59.0 58.4GYRO +3g 63.0 64.14g GYRO +4g 65.5 65.9100-best oracle 76.1 76.1lattice oracle 80.4 80.2Table 1: CCG and GYRO BLEU scores.4-grams and 5-grams.3Under these conditions,GYRO finds an output for 91.4% of the bags.
Forthe remainder, we obtain an output either by prun-ing less or by adding bigram rules (in 7.2% of thebags), or simply by adding all words as unigramrules (1.4% of the bags).Table 1 gives the results obtained by CCG andGYRO under a 3-gram or a 4-gram LM.
BecauseGYRO outputs word lattices as opposed to a 1-best hypothesis, we can reapply the same LM tothe concatenated lattices of any sentences longerthan 20 to take into account context in subsentenceboundaries.
This is the result in the third row inthe Table, labeled ?GYRO +3g?.
We can see thatGYRO benefits significantly from this rescoring,beating the CCG system across both sets.
This ispossibly explained by the CCG system?s depen-dence upon in-domain data that have been explic-itly marked-up using the CCG formalism.
The fi-nal row reports the positive impact of increasingthe LM order to 4.Impact of generation grammar.
To measurethe benefits of using high-order n-grams as con-straints for generation, we also ran GYRO withunigram rules only.
This effectively does permu-tation under the LM with the pruning mechanismsdescribed.
The BLEU scores are 54.0 and 54.5 forMT08-nw and MT09 respectively.
This indicatesthat a strong GYRO grammar is very much neededfor this type of parsing and generation.Quality of generated lattices.
We assess thequality of the lattices output by GYRO under the4-gram LM by computing the oracle BLEU scoreof either the 100-best lists or the whole lattices4in the last two rows of Table 1.
In order to com-pute the latter, we use the linear approximationto BLEU that allows an efficient FST-based im-plementation of an Oracle search (Sokolov et al.,2012).
We draw two conclusions from these re-sults: (a) that there is a significant potential for im-3Any word in the bag that does not occur in the large col-lection of English material is added as a 1-gram rule.4Obtained by pruning at ?
= 10 in generation.provement from rescoring, in that even for small100-best lists the improvement found by the Ora-cle can exceed 10 BLEU points; and (b) that theoutput lattices are not perfect in that the Oraclescore is not 100.3.2.1 Rescoring GYRO outputWe now report on rescoring procedures intendedto improve the first-pass lattices generated byGYRO.Higher-order language models.
The first rowin Table 2 reports the result obtained when apply-ing a 5-gram LM to the GYRO lattices generatedunder a 4-gram.
The 5-gram is estimated over thecomplete 10.6 billion word collection using theuniform backoff strategy of (Brants et al., 2007).We find improvements of 3.0 and 1.9 BLEU withrespect to the 4-gram baseline.Dependency language models.
We now in-vestigate the benefits of applying a dependencyLM (Shen et al., 2010) in a rescoring mode.
Werun the MALT dependency parser5on the gener-ation hypotheses and rescore them according tolog(pLM) + ?dlog(pdepLM), i.e.
a weighted com-bination of the word-based LM and the depen-dency LM scores.
Since it is not possible to run theparser on the entire lattice, we carry out this exper-iment using the 100-best lists generated from theprevious experiment (?+5g?).
The dependency LMis a 3-gram estimated on the entire GigaWord ver-sion 5 collection (?5 billion words).
Results areshown in rows 2 and 3 in Table 2, where in eachrow the performance over the set used to tune theparameter ?dis marked with ?.
In either case, weobserve modest but consistent gains across bothsets.
We find this very promising considering thatthe parser has been applied to noisy input sen-tences.Minimum Bayes Risk Decoding.
We also useLattice-based Minimum Bayes Risk (LMBR) de-coding (Tromble et al., 2008; Blackwood et al.,2010a).
Here, the posteriors over n-grams arecomputed over the output lattices generated by theGYRO system.
The result is shown in row labeled?+5g +LMBR?, where again we find modest butconsistent gains across the two sets with respect tothe 5-gram rescored lattices.LMBR with MT posteriors.
We investigateLMBR decoding when applying to the generationlattice a linear combination of the n-gram pos-5Available at www.maltparser.org2644g GYRO rescoring: MT08-nw MT09-nw+5g 68.5 67.8+5g +depLM ?d= 0.4 68.7?68.1+5g +depLM ?d= 0.33 68.7 68.2?+5g +LMBR 68.6 68.3+5g +LMBR-mt ?
= 0.25 70.8?72.2+5g +LMBR-mt ?
= 0.25 70.8 72.2?Table 2: Results in BLEU when rescoring the lat-tices generated by GYRO using various strategies.Tuning conditions are marked by?.terior probabilities extracted from (a) the samegeneration lattice, and (b) from lattices producedby an Arabic-to-English hierarchical-phrase basedMT system developed for the NIST 2012 OpenMTEvaluation.
As noted, LMBR relies on a posteriordistribution over n-grams as part of its computa-tion or risk.
Here, we use LMBR with a posteriorof the form ?pGYRO+ (1??)
pMT.
This is effec-tively performing a system combination betweenthe GYRO generation system and the MT system(de Gispert et al., 2009; DeNero et al., 2010) butrestricting the hypothesis space to be that of theGYRO lattice (Blackwood et al., 2010b).
Resultsare reported in the last two rows of Table 2.
Rel-ative to 5-gram LM rescoring alone, we see gainsin BLEU of 2.3 and 4.4 in MT08-nw and MT09-nw, suggesting that posterior distributions over n-grams provided by SMT systems can give goodguidance in generation.
These results also suggestthat if we knew what words to use, we could gen-erate very good quality translation output.3.3 Analysis and examplesFigure 7 gives GYRO generation examples.
Theseare often fairly fluent, and it is striking how theoutput can be improved with guidance from theSMT system.
The examples also show the harsh-ness of BLEU, e.g.
?german and turkish officials?is penalised with respect to ?
turkish and germanofficials.?
Metrics based on richer meaning rep-resentations, such as HyTER, could be valuablehere (Dreyer and Marcu, 2012).Figure 8 shows BLEU and Sentence Preci-sion Rate (SPR), the percentage of exactly recon-structed sentences.
As expected, performance issensitive to length.
For bags of up to 10, GYROreconstructs the reference perfectly in over 65%of the cases.
This is a harsh performance metric,and performance falls to less than 10% for bagsof size 16-20.
For bags of 6-10 words, we findBLEU scores of greater than 85.
Performance is681 0862 66861 60842 46841 40g raams35?s12110201?2?1?2?1?226242?2?21202?2?2?2Size ofthbagwardbasNuagwamgn-?Sizea??gnbb?rb??baonb?t?tg?afNrba?
of?Figure 8: GYRO BLEU score and Sentence Pre-cision Rate as a function of the bag of words size.Computed on the concatenation of MT08-nw andMT09-nw.not as good for shorter segments, since these areoften headlines and bylines that can be ambiguousin their ordering.
The BLEU scores for bags ofsize 21 and higher are an artefact of our sentencesplitting procedure.
However, even for bag sizesof 16-to-20 GYRO has BLEU scores above 55.3.4 Human AssessmentsFinally, the CCG and 4g-GYRO+5g systems werecompared using crowd-sourced fluency judge-ments gathered on CrowdFlower.
Judges wereasked ?Please read the reference sentence andcompare the fluency of items 1 & 2.?
The test wasa selection of 75 fluent sentences of 20 words orless taken from the MT dev sets.
Each comparisonwas made by at least 3 judges.
With an average se-lection confidence of 0.754, GYRO was preferredin 45 cases, CCG was preferred in 14 cases, andsystems were tied 16 times.
This is consistent withthe significant difference in BLEU between thesesystems.4 Related Work and ConclusionOur work is related to surface realisation withinnatural language generation (NLG).
NLG typi-cally assumes a relatively rich input representationintended to provide syntactic, semantic, and otherrelationships to guide generation.
Example inputrepresentations are Abstract Meaning Represen-tations (Langkilde and Knight, 1998), attribute-value pairs (Ratnaparkhi, 2000), lexical predicate-argument structures (Bangalore and Rambow,2000), Interleave-Disjunction-Lock (IDL) expres-sions (Nederhof and Satta, 2004; Soricut andMarcu, 2005; Soricut and Marcu, 2006), CCG-bank derived grammars (White et al., 2007),265Hypothesis SBLEUREF a third republican senator joins the list of critics of bush ?s policy in iraq .
(a) critics of bush ?s iraq policy in a third of republican senator joins the list .
47.2(b) critics of bush ?s policy in iraq joins the list of a third republican senator .
69.8(c) critics of bush ?s iraq policy in a list of republican senator joins the third .
39.1(d) the list of critics of bush ?s policy in iraq a third republican senator joins .
82.9REF it added that these messages were sent to president bashar al-asad through turkish and german officials .
(a-c) it added that president bashar al-asad through these messages were sent to german and turkish officials .
61.5(d) it added that these messages were sent to president bashar al-asad through german and turkish officials .
80.8REF a prominent republican senator has joined the ranks of critics of george bush ?s policy in iraq , callingfor a new strategy just days before a new confrontation in congress(a) a prominent republican senator george has joined the ranks of critics of bush ?s policy in iraq , just daysbefore a new strategy in congress calling for a new confrontation66.7(b) a prominent republican senator has joined the ranks of critics of george bush ?s policy in iraq , just daysbefore congress calling for a new strategy in a new confrontation77.8(c) a prominent republican senator has joined the ranks of critics of george bush ?s policy in iraq , just daysbefore a new strategy in congress calling for a new confrontation82.3(d) a prominent republican senator has joined the ranks of critics of george bush ?s policy in iraq , callingfor a new strategy just days before a new confrontation in congress100Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b)GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt.
(a-c) indicates systems with identicalhypotheses.meaning representation languages (Wong andMooney, 2007) and unordered syntactic depen-dency trees (Guo et al., 2011; Bohnet et al., 2011;Belz et al., 2011; Belz et al., 2012)6.These input representations are suitable for ap-plications such as dialog systems, where the sys-tem maintains the information needed to gener-ate the input representation for NLG (Lemon,2011), or summarisation, where representationscan be automatically extracted from coherent,well-formed text (Barzilay and Elhadad, 2011; Al-thaus et al., 2004).
However, there are other appli-cations, such as automatic speech recognition andSMT that could possibly benefit from NLG, butwhich do not generate reliable linguistic annota-tion in their output.
For these problems it wouldbe useful to have systems, as described in this pa-per, which do not require rich input representa-tions.
We plan to investigate these applications infuture work.There is much opportunity for future develop-ment.
To improve coverage, the grammars of Sec-tion 2.1 could perform generation with overlap-ping, rather than concatenated, n-grams; and fea-tures could be included to define tuneable log-linear rule probabilities (Och and Ney, 2002; Chi-ang, 2007).
The GYRO grammar could be ex-tended using techniques from string-to-tree SMT,in particular by modifying the grammar so thatoutput derivations respect dependencies (Shen et6Surface Realisation Task, Generation Challenges 2011,www.nltg.brighton.ac.uk/research/genchal11al., 2010); this will make it easier to integrate de-pendency LMs into GYRO.
Finally, it would beinteresting to couple the GYRO architecture withautomata-based models of poetry and rhythmictext (Greene et al., 2010).AcknowledgementThe research leading to these results has receivedfunding from the European Union SeventhFramework Programme (FP7-ICT-2009-4)under grant agreement number 247762, theFAUST project faust-fp7.eu/faust/,and the EPSRC (UK) Programme GrantEP/I031022/1 (Natural Speech Technology)natural-speech-technology.org .ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
OpenFst:A general and efficient weighted finite-state trans-ducer library.
In Proceedings of CIAA, pages 11?23,Prague, Czech Republic.Ernst Althaus, Nikiforos Karamanis, and AlexanderKoller.
2004.
Computing locally coherent dis-courses.
In Proceedings of the 42nd Annual Meetingon Association for Computational Linguistics, page399.
Association for Computational Linguistics.Srinivas Bangalore and Owen Rambow.
2000.
Ex-ploiting a probabilistic hierarchical model for gen-eration.
In Proceedings of the 18th conference onComputational linguistics - Volume 1, COLING ?00,pages 42?48, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.266Regina Barzilay and Noemie Elhadad.
2011.
In-ferring strategies for sentence ordering in multi-document news summarization.
arXiv preprintarXiv:1106.1820.Anja Belz, Mike White, Dominic Espinosa, Eric Kow,Deirdre Hogan, and Amanda Stent.
2011.
The firstsurface realisation shared task: Overview and eval-uation results.
In Proceedings of the GenerationChallenges Session at the 13th European Workshopon Natural Language Generation, pages 217?226,Nancy, France.Anja Belz, Bernd Bohnet, Simon Mille, Leo Wanner,and Michael White.
2012.
The surface realisationtask: Recent developments and future plans.
In Pro-ceedings of the 7th International Natural LanguageGeneration Conference, pages 136?140, Utica, IL,USA.Graeme Blackwood, Adri`a de Gispert, and WilliamByrne.
2010a.
Efficient path counting transducersfor minimum Bayes-risk decoding of statistical ma-chine translation lattices.
In Proceedings of ACL:Short Papers, pages 27?32, Uppsala, Sweden.Graeme Blackwood, Adri`a de Gispert, and WilliamByrne.
2010b.
Fluency constraints for minimumBayes-risk decoding of statistical machine transla-tion lattices.
In Proceedings of COLING, pages 71?79, Beijing, China.Bernd Bohnet, Simon Mille, Beno?
?t Favre, and LeoWanner.
2011.
<StuMaBa>: From deep represen-tation to surface.
In Proceedings of the GenerationChallenges Session at the 13th European Workshopon Natural Language Generation, pages 232?235,Nancy, France.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large languagemodels in machine translation.
In Proceedings ofEMNLP-CoNLL, pages 858?867, Prague, Czech Re-public.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Adri`a de Gispert, Sami Virpioja, Mikko Kurimo, andWilliam Byrne.
2009.
Minimum Bayes risk com-bination of translation hypotheses from alternativemorphological decompositions.
In Proceedings ofHLT-NAACL: Short Papers, pages 73?76, Boulder,CO, USA.Adri`a de Gispert, Gonzalo Iglesias, Graeme Black-wood, Eduardo R. Banga, and William Byrne.
2010.Hierarchical phrase-based translation with weightedfinite-state transducers and shallow-n grammars.Computational Linguistics, 36(3):505?533.John DeNero, Shankar Kumar, Ciprian Chelba, andFranz Och.
2010.
Model combination for machinetranslation.
In Proceedings of HTL-NAACL, pages975?983, Los Angeles, CA, USA.Markus Dreyer and Daniel Marcu.
2012.
Hyter:Meaning-equivalent semantics for translation eval-uation.
In Proceedings of NAACL-HLT, pages 162?171, Montr?eal, Canada.Dominic Espinosa, Rajakrishnan Rajkumar, MichaelWhite, and Shoshana Berleant.
2010.
Furthermeta-evaluation of broad-coverage surface realiza-tion.
In Proceedings of EMNLP, pages 564?574,Cambridge, MA, USA.Dmitriy Genzel.
2010.
Automatically learning source-side reordering rules for large scale machine trans-lation.
In Proceedings of COLING, pages 376?384,Beijing, China.Erica Greene, Tugba Bodrumlu, and Kevin Knight.2010.
Automatic analysis of rhythmic poetry withapplications to generation and translation.
In Pro-ceedings of EMNLP, pages 524?533, Cambridge,MA, USA.Yuqing Guo, Josef Van Genabith, and Haifeng Wang.2011.
Dependency-based n-gram models for gen-eral purpose sentence realisation.
Natural LanguageEngineering, 17(04):455?483.Gonzalo Iglesias, Cyril Allauzen, William Byrne,Adri`a de Gispert, and Michael Riley.
2011.
Hi-erarchical phrase-based translation representations.In Proceedings of EMNLP, pages 1373?1383, Edin-burgh, Scotland, UK.Kevin Knight.
1999.
Decoding complexity in word-replacement translation models.
ComputationalLinguistics, 25(4):607?615.Irene Langkilde and Kevin Knight.
1998.
Gener-ation that exploits corpus-based statistical knowl-edge.
In Proceedings of ACL/COLING, pages 704?710, Montreal, Quebec, Canada.Oliver Lemon.
2011.
Learning what to say and how tosay it: Joint optimisation of spoken dialogue man-agement and natural language generation.
Com-puter Speech & Language, 25(2):210?221.Mark-Jan Nederhof and Giorgio Satta.
2004.
IDL-expressions: A formalism for representing and pars-ing finite languages in natural language processing.Journal of Artificial Intelligence Research, 21:287?317.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In Proceedings of ACL,pages 295?302, Philadelphia, PA, USA.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof ACL, pages 311?318, Philadelphia, PA, USA.Juan Pino, Aurelien Waite, and William Byrne.
2012.Simple and efficient model filtering in statistical ma-chine translation.
The Prague Bulletin of Mathemat-ical Linguistics, 98:5?24.267Adwait Ratnaparkhi.
2000.
Trainable methods for sur-face natural language generation.
In Proceedings ofNAACL, pages 194?201, Seattle, WA, USA.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2010.String-to-dependency statistical machine transla-tion.
Computational Linguistics, 36(4):649?671.Artem Sokolov, Guillaume Wisniewski, and FrancoisYvon.
2012.
Computing lattice bleu oracle scoresfor machine translation.
In Proceedings of EACL,pages 120?129, Avignon, France.Radu Soricut and Daniel Marcu.
2005.
Towards devel-oping generation algorithms for text-to-text applica-tions.
In Proceedings of ACL, pages 66?74, AnnArbor, MI, USA.Radu Soricut and Daniel Marcu.
2006.
StochasticLanguage Generation Using WIDL-Expressions andits Application in Machine Translation and Summa-rization.
In Proceedings of ACL, pages 1105?1112,Sydney, Australia.Roy Tromble and Jason Eisner.
2009.
Learning linearordering problems for better translation.
In Proceed-ings of EMNLP, pages 1007?1016, Singapore.Roy Tromble, Shankar Kumar, Franz Och, and Wolf-gang Macherey.
2008.
Lattice Minimum Bayes-Risk decoding for statistical machine translation.
InProceedings of EMNLP, pages 620?629, Honolulu,Hawaii, USA.Stephen Wan, Mark Dras, Robert Dale, and C?ecileParis.
2009.
Improving grammaticality in statisti-cal sentence generation: Introducing a dependencyspanning tree algorithm with an argument satisfac-tion model.
In Proceedings of EACL, pages 852?860, Athens, Greece.Michael White, Rajakrishnan Rajkumar, and ScottMartin.
2007.
Towards broad coverage surface real-ization with ccg.
In Proc.
of the Workshop on UsingCorpora for NLG: Language Generation and Ma-chine Translation (UCNLG+ MT).Yuk Wah Wong and Raymond J Mooney.
2007.
Gen-eration by inverting a semantic parser that uses sta-tistical machine translation.
Proceedings of Hu-man Language Technologies: The Conference ofthe North American Chapter of the Association forComputational Linguistics (NAACL-HLT-07), pages172?179.Yue Zhang and Stephen Clark.
2011.
Syntax-based Grammaticality Improvement using CCG andGuided Search.
In Proceedings of EMNLP, pages1147?1157, Edinburgh, Scotland, U.K.Yue Zhang, Graeme Blackwood, and Stephen Clark.2012.
Syntax-based word ordering incorporatinga large-scale language model.
In Proceedings ofEACL, pages 736?746, Avignon, France.268
