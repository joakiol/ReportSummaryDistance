INFORMATION RETRIEVAL USINGROBUST NATURAL LANGUAGE PROCESSINGTomek StrzalkowskiCourant Institute of  Mathematical SciencesNew York University715 Broadway, rm.
704New York, NY 10003tomek@cs.nyu.eduABSTRACTWe developed a fully automated Information Retrieval System which usesadvanced natural language processing techniques to enhance the effective-ness of traditional key-word based ocument retrieval.
In early experimentswith the standard CACM-3204 collection of abstracts, the augmented sys-tem has displayed capabilities that made it clearly superior to the purelystatistical base system.1.
OVERALL  DES IGNOur information retrieval system consists of a traditionalstatistical backbone (Harman and Candela, 1989) aug-mented with various natural language processing com-ponents that assist he system in database processing (stem-ming, indexing, word and phrase clustering, selectional res-trictions), and translate a user's information request into aneffective query.
This design is a careful compromisebetween purely statistical non-linguistic approaches andthose requiring rather accomplished (and expensive)semantic analysis of data, often referred to as 'conceptualretrieval'.
The conceptual retrieval systems, though quiteeffective, are not yet mature nough to be considered inserious information retrieval applications, the major prob-lems being their extreme inefficiency and the need formanual encoding of domain knowledge (Mauldin, 1991).In our system the database text is first processed with a fastsyntactic parser.
Subsequently certain types of phrases areextracted from the parse lxees and used as compound index-ing terms in addition to single-word terms.
The extractedphrases are statistically analyzed as syntactic ontexts inorder to discover a variety of similarity links betweensmaller subphrases and words occurring in them.
A furtherfiltering process maps these similarity links onto semanticrelations (generalization, specialization, synonymy, etc.
)after which they are used to transform user's request into asearch query.The user's natural anguage request is also parsed, and allindexing terms occurring in them are identified.
Next, cer-tain highly ambiguous (usually single-word) terms aredropped, provided that they also occur as elements in somecompound terms.
For example, "natural" is deleted from aquery already containing "natural language" because206"natural" occurs in many unrelated contexts: "naturalnumber", "natural ogarithm", natural approach", etc.
Atthe same time, other terms may be added, namely thosewhich are linked to some query term through admissiblesimilarity relations.
For example, "fortran" is added to aquery containing the compound term "program language"via a specification link.
After the final query is constructed,the database search follows, and a ranked list of documentsis returned.It should be noted that all the processing steps, those per-formed by the backbone system, and these performed bythe natural language processing components, are fullyautomated, and no human intervention or manual encodingis required.2.
FAST  PARSING WITH TTPTIP (Tagged Text Parser) is based on the Linguistic StringGrammar developed by Sager (1981).
Written in QuintusProlog, the parser currently encompasses more than 400grammar productions.
It produces regularized parse treerepresentations for each sentence that reflect he sentence'slogical structure.
The parser is equipped with a powerfulskip-and-fit recovery mechanism that allows it to operateeffectively in the face of ill-formed input or under a severetime pressure.
In the recent experiments with approximately6 million words of English texts, 1the parser's peed aver-aged between 0.45 and 0.5 seconds per sentence, or up to2600 words per minute, on a 21 MIPS SparcStation ELC.Some details of the parser are discussed below.
2TIP is a full grammar parser, and initially, it attempts togenerate a complete analysis for each sentence.
However,unlike an ordinary parser, it has a built-in timer which regu-lates the amount of time allowed for parsing any one sen-tence.
If a parse is not returned before the allotted timeI These include CACM-3204, MUC-3, and a selection of nearly6,000 technical articles extracted from Computer Library database (aZfffCommunications I c. CD-ROM).2 A complete description can be found in (Strzalkowski, 1991).elapses, the parser enters the skip-and-fit mode in which itwill try to "fit" the parse.
While in the skip-and-fit mode,the parser will attempt to forcibly reduce incomplete consti-tuents, possibly skipping portions of input in order to restartprocessing at a next unattempted constituent.
In otherwords, the parser will favor reduction to backtracking whilein the skip-and-fit mode.
The result of this strategy is anapproximate parse, partially fitted using top-down predic-tions.
The fragments skipped in the first pass are not thrownout, instead they are analyzed by a simple phrasal parserthat looks for noun phrases and relative clauses and thenattaches the recovered material to the main parse structure.As an illustration, consider the following sentence takenfrom the CACM-3204 corpus:The method is illustrated by the automatic onstructionof  both recursive and iterative programs operating onnatural numbers, lists, and trees, in order to construct aprogram satisfying certain specifications a theorem in-duced by those specifications is proved, and the desiredprogram is extracted from the proof.The italicized fragment is likely to cause additional compli-cations in parsing this lengthy string, and the parser may bebetter off ignoring this fragment altogether.
To do so suc-cessfully, the parser must close the currently open consti-tuent (i.e., reduce a program satisfying certainspecifications to NP), and possibly a few of its parent con-stituents, removing corresponding productions from furtherconsideration, until an appropriate production is reac-tivated.
In this case, T IP may force the following reduc-tions: SI --> to V NP; SA ----> SI; S ---> NP V NP SA, until theproduction S --> S and S is reached.
Next, the parser skipsinput to find and, and resumes normal processing.As may be expected, the skip-and-fit strategy will only beeffective if the input skipping can be performed with adegree of determinism.
This means that most of the lexicallevel ambiguity must be removed from the input text, priorto parsing.
We achieve this using a stochastic parts ofspeech tagger 3to preprocess the text.3.
WORD SUFFIX TRIMMERWord stemming has been an effective way of improvingdocument recall since it reduces words to their commonmorphological root, thus allowing more successful matches.On the other hand, stemming tends to decrease retrievalprecision, if care is not taken to prevent situations whereotherwise unrelated words are reduced to the same stem.
Inour system we replaced a traditional morphological stem-mer with a conservative dictionary-assisted suffix trimmer.
4The suffix trimmer performs essentially two tasks: (1) itreduces inflected word forms to their root forms as specifiedin the dictionary, and (2) it converts nominalized verb3 Courtesy of Bolt Beranek and Newman.4 We use Oxford Advanced Leamer's Dictionary (OALD) MRD.forms (eg.
"implementation", "storage") to the root forms ofcorresponding verbs (i.e., "implement", store").
This isaccomplished by removing a standard suffix, eg.
"stor+age", replacing it with a standard root ending ("+e"),and checking the newly created word against the dictionary,i.e., we check whether the original root ("storage") isdefined using the new root ("store").
This allows reducing"diversion" to "diverse" while preventing "version" to bereplaced by "verse".
Experiments with CACM-3204 collec-tion show an improvement in retrieval precision by 6% to8% over the base system equipped with a standard morpho-logical stemmer (the SMART stemmer).4.
HEAD-MODIFIER STRUCTURESSyntactic phrases extracted from TTP parse trees are head-modifier pairs: from simple word pairs to complex nestedstructures.
The head in such a pair is a central element of aphrase (verb, main noun, etc.)
while the modifier is one ofthe adjunct arguments of the head.
5For example, the phrasefast algorithm for parsing context-free languages yields thefollowing pairs: algorithm+fast, algorithm+parse,parse+language, language+context_free.
The followingtypes of pairs were considered: (1) a head noun and its leftadjective or noun adjunct, (2) a head noun and the head ofits right adjunct, (3) the main verb of a clause and the headof its object phrase, and (4) the head of the subject phraseand the main verb, These types of pairs account for most ofthe syntactic variants for relating two words (or simplephrases) into pairs carrying compatible semantic ontent.For example, the pair \[retrieve,information\] is extractedfrom any of the following fragments: information retrievalsystem; retrieval of information from databases; and infor-mation that can be retrieved by a user-controlled interac-tive search process.
6 An example is shown in the appen-dix .75.
TERM CORRELATIONS FROM TEXTHead-modifier pairs form compound terms used in databaseindexing.
They also serve as occurrence contexts forsmaller terms, including single-word terms.
In order todetermine whether such pairs signify any important associa-tion between terms, we calculate the value of the5 In the experiments reported here we extracted head-modifier wordpairs only.
CACM collection is too small to warrant generation of largercompounds, because of their low frequencies.To deal with nominal compounds we use frequency informationabout he pairs generated from the entire corpus to form preferences in am-biguous ituations, uch as natural language processing vs. dynamic infor-mation processing.7 Note that working with the parsed text ensures a high degree ofprecision in capturing the meaningful phrases, which is especially evidentwhen compared with the results usually obtained from either unprocessedor only partially processed text (Lewis and Croft, 1990).207Informational Contribution (IC) function for each elementin a pair.
Higher values indicate stronger association, andthe element having the largest value is considered semanti-cally dominant.
IC function is a derivative of Fano's mutualinformation formula recently used by Church and Hanks(1990) to compute word co-occurrence patterns in a 44 mil-lion word corpus of Associated Press news stories.
Theynoted that while generally satisfactory, the mutual informa-tion formula often produces counterintuitive r sults forlow-frequency data.
This is particularly worrisome for rela-tively smaller IR collections ince many important indexingterms would be eliminated from consideration.
Therefore,following suggestions in Wilks et al (1990), we adopted arevised formula that displays a more stable behavior evenon very low counts.
This new formula IC (x ,\[x,y \]) is'basedon (an estimate o0 the conditional probability of seeing aword y to the right of the word x, modified with a disper-sion parameter for x.fx~rlC (x,\[x,y \]) -n,, + d,, -1where fx~, is the frequency of \[x ,y \] in the corpus, n x is thenumber of pairs in which x occurs at the same position as inIx,y\], and d(x) is the dispersion parameter understood asthe number of distinct words with which x is paired.
WhenIC(x,\[x,y\])=O, x and y never occur together (i.e.,fx,y = 0); when IC(x,\[x,y\]) = 1, x occurs only with y (i.e.,fx,y =n,  and d~ = 1).
Selected examples generated fromCACM-3204 corpus are given in Table 2 at the end of thepaper.
IC values for terms become the basis for calculatingterm-to-term similarity coefficients.
If two terms tend to bemodified with a number of common modifiers and other-wise appear in few distinct contexts, we assign them a simi-larity coefficient, a real number between 0 and 1.
The simi-larity is determined by comparing distribution characteris-tics for both terms within the corpus: how much informa-tion contents do they carry, do their information contribu-tion over contexts vary greatly, are the common contexts inwhich these terms occur specific enough?
In general wewill credit high-contents erms appearing in identical con-texts, especially if these contexts are not too common-place.
8The relative similarity between two words xl and x zis obtained using the following formula (a is a large con-stant):SIM (x 1 ,x 2) = log (a ~ sim~ (x 1,x 9)wheresimy (x l ,x z) = MIN (I C (x 1,\[x l ,y \]) j C (x 2,\[x 2,y \]))* MIN(IC(y,\[xl,y\])JC(y,\[x2,y\]))The similarity function is further normalized with respect to8 It would not be appropriate to predict similarity between languageand logarithm on the basis of their co-occurrence with natural.SIM(xl,xl).
It may be worth pointing out that the similari-ties are calculated using term co-occurrences in syntacticrather than in document-size contexts, the latter being theusual practice in non-linguistic clustering (eg.
Sparck Jonesand Barber, 1971; Crouch, 1988; Lewis and Croft, 1990).Although the two methods of term clustering may be con-sidered mutually complementary in certain situations, webefieve that more and slxonger associations can be obtainedthrough syntactic-context clustering, given sufficientamount of data and a reasonably accurate syntactic parser.
96.
QUERY EXPANSIONSimilarity relations are used to expand user queries withnew terms, in an attempt o make the final search querymore comprehensive (adding synonyms) and/or morepointed (adding specializations).
1?
It follows that not allsimilarity relations will be equally useful in query expan-sion, for instance, complementary elations like the onebetween algol and fortran may actually harm system's per-formance, since we may end up retrieving many irrelevantdocuments.
Similarly, the effectiveness of a query contain-ing fortran is likely to diminish if we add a similar but farmore general term such as language.
On the other hand,database search is likely to miss relevant documents if weoverlook the fact that fortran is a programming language,or that interpolate is a specification of approximate.
Wenoted that an average set of similarities generated from atext corpus contains about as many "good" relations(synonymy, speciafization) as "bad" relations (antonymy,complementation, generalization), as seen from the queryexpansion viewpoint.
Therefore any attempt o separatethese two classes and to increase the proportion of "good"relations should result in improved retrieval.
This hasindeed been confirmed in our experiments where a rela-tively crude filter has visibly increased retrieval precision.In order to create an appropriate filter, we expanded the ICfunction into a global specificity measure called the cumu-lative informational contribution function (ICW).
ICW iscalculated for each term across all contexts in which itoccurs.
The general philosophy here is that a more specificword/phrase would have a more limited use, i.e., wouldappear in fewer distinct contexts.
ICW is similar to the stan-dard inverted ocument frequency (idj) measure xcept hatterm frequency is measured over syntactic units rather than9 Non-syntactic contexts cross sentence boundaries with no fuss,which is helpful with short, succinct documents (such as  CACM abstracts),but less o with longer texts.to Query expansion (i  the sense considered here, though not quite inthe same way) has been used in information retfeval research before (eg.Sparek Jones and Tait, 1984; Harman, 1988), usually with mixed results.An alternative is to use term clusters to create new terms, "metaterms", anduse them to index the database instead (eg.
Crouch, 1988; Lewis and Croft,1990).
We found that he query expansion approach gives the system oreflexibiUty, for instance, by making room for hypertext-style opic explora-tion via user feedback.208document size units.
11 Terms with higher ICW values aregenerally considered more specific, but the specificity com-parison is only meaningful for terms which are alreadyknown to be similar.
The new function is calculated accord-ing to the following formula: 12ICW(w) =ICL(w) * ICR (w)where (with nw, dw > 0):ICL (W) = Ic ( \ [w,_  \]) =n~aw(nw+aw-1)and analogously for IC R (w ).For any two terms w 1 and w 2, and a constant ~i > 1, ifICW(w2)>_~* ICW(wl) then w 2 is considered morespecific than w 1.
In addition, if SIM,~,~(Wl,Wz)=~> O,where 0 is an empirically established threshold, then w 2 canbe added to the query containing term w 1 with weight o.
13In the CACM-3204 collection:ICW (algol) = 0.0020923ICW (language) = 0.0000145ICW (approximate) = 0.0000218ICW (interpolate) = 0.0042410Therefore interpolate can be used to specialize approxi-mate, while language cannot be used to expand algol.
Notethat if 8 is well chosen (we used 5=10), then the above filterwill also help to reject antonymous and complementaryrelations, such as SIM~orm (pl_i,cobol)=0.685 withICW (pl_i)=O.O 175 and ICW (cobol)=0.0289.
We continueworking to develop more effective filters.
Examples offiltered similarity relations obtained from CACM-3204corpus are given in Table 3.7.
SUMMARY OF RESULTSThe preliminary series of experiments with the CACM-3204 collection of computer science abstracts howed aconsistent improvement in performance: the average preci-sion increased from 32.8% to 37.1% (a 13% increase),while the normalized recall went from 74.3% to 84.5% (a14% increase), in comparison with the statistics of the basesystem.
This improvement is a combined effect of the newstemmer, compound terms, term selection in queries, andquery expansion using filtered similarity relations.
Thechoice of similarity relation filter has beeen found critical inimproving retrieval precision through query expansion.
Itshould also be pointed out that only about 1.5% of all" We believe that measuring term specificity over document-sizecontexts (eg.
Sparck Jones, 1972) may not be appropriate in this case.
Inparticular, syntax-based contexts al low for processing texts without anyintemal document structure.m Slightly simplified here.13 The filter was most effective at cr = 0.57.similarity relations originally generated from CACM-3204were found admissible after filtering, contributing only 1.2expansion on average per query.
It is quite evidentsignificantly larger corpora are required to produce moredramatic results.
14 15 A detailed summary is given in Table1 below.These results, while modest by IR standards, are significantfor another reason as well.
They were obtained without anymanual intervention into the database or queries, andwithout using any other information about the databaseexcept for the text of the documents (i.e., not even the handgenerated keyword fields enclosed with most documentswere used).
Lewis and Croft (1990), and Croft et al (1991)report results similar to ours but they take advantage ofComputer Reviews categories manually assigned to somedocuments.
The purpose of this research is to explore thepotential of automated NLP in dealing with large scale IRproblems, and not necessarily to obtain the best possibleresults on any particular data collection.
One of our goals isto point a feasible direction for integrating NLP into thetraditional IR (Strzalkowski and Vauthey, 1991; GrishmanTests org.system suf~trimmer query exp.Recall Precision0.000.100.200.300.400.500.600.700.800.901.00Avg.
Prec.% changeNorm Rec.Queries0.764 0.7750.6740.5470.4490.3870.3290.2730.1980.1460.0930.0790.3280.743500.7930.688 0.7000.547 0.5730.479 0.4860.421 0.4210.356 0.3720.280 0.3040.222 0.2260.170 0.1740.112 0.1140.087 0.0900.356 0.3718.3 13.10.841 0.84250 50Table 1.
Recall/precision statistics for CACM-320414 KL Kwok (private communication) has suggested that he lowpercentage of admissible r lations might be similar to the phenomenon of'tight dusters' which while meaningful are so few that heir impact issmall.15 A sufficiently large text corpus i 20 million words or more.
Thishas been partially confirmed by experiments performed at the University ofMassachussetts (B. Croft, private communication).209and Strzalkowski, 1991).ACKNOWLEDGEMENTSWe would like to thank Donna Harman of NIST for makingher IR system available to us.
We would also like to thankRalph Weischedel and Marie Meteer of BBN for providingand assisting in the use of the part of speech tagger.
KLKwok has offered many helpful comments on an earlierdraft of this paper.
In addition, ACM has generously pro-vided us with text data from the Computer Library databasedistributed by Ziff Communications Inc.
This paper isbased upon work supported by the Defense AdvancedResearch Project Agency under Contract N00014-90-J-1851 from the Office of Naval Research, and the NationalScience Foundation under Grant IRI-89-02304.REFERENCES1.
Harman, Donna and Gerald Candela.
1989.
"RetrievingRecords from a Gigabyte of text on a.Minicomputer UsingStatistical Ranking."
Journal of the American Society forInformation Science, 41 (8), pp.
581-589.2.
Mauldin, Michael.
1991.
"Retrieval Performance inFerret: AConceptual Information Retrieval System."
Proceedings ofACM SIGIR-91, pp.
347-355.3.
Sager, Naomi.
1981.
Natural Language Information Pro-cessing.
Addison-Wesley.4.
Strzalkowski, Tomek.
1991.
"TI'P: A Fast and Robust Parserfor Natural Language."
Proteus Project Memo #43, CourantInstitute of Mathematical Science, New York University.5.
Lewis, David D. and W. Bruce Croft.
1990.
"Term Cluster-ing of Syntactic Phrases".
Proceedings of ACM SIGIR-90,pp.
385-405.6.
Church, Kenneth Ward and Hanks, Patrick.
1990.
"Wordassociation orms, mutual information, and lexicography.
"ComputationalLinguistics, 16(1), MIT Press, pp.
22-29.7.
Wilks, Yorick A., Dan Fass, Cheng-Ming Guo, James E.McDonald, Tony Plate, and Brian M. Slator.
1990.
"Provid-ing machine tractable dictionary tools."
Machine Transla-tion, 5, pp.
99-154.8.
Sparck Jones, K. and E. O. Barber.
1971.
"What makesautomatic keyword classification effective?"
Journal of theAmerican Society for Information Science, May-June, pp.166-175.9.
Crouch, Carolyn J.
1988.
"A cluster-based approach tothesaurus construction."
Proceedings of ACM SIGIR-88, pp.309-320.10.
Sparck Jones, K. and J. I. Tait.
1984.
"Automatic searchterm variant generation."
Journal of Documentation, 40(1),pp.
50-66.11.
Harrnan, Donna.
1988.
"Towards interactive query expan-sion."
Proceedings ofACM SIGIR-88, pp.
321-331.12.
Sparck Jones, Karen.
1972.
"Statistical interpretation ofterm specificity and its application in retrieval."
Journal ofDocumentation, 28(1 ), pp.
11-20.13.
Croft, W. Bruce, Howard R. Turtle, and David D. Lewis.1991.
"The Use of Phrases and Structured Queries in Infor-mation Retrieval."
Proceedings of ACM SIGIR-91, pp.
32-45.14.
Strzalkowski, Tomek and Barbara Vauthey.
1991.
"Fast TextProcessing for Information Retrieval."
Proceedings of the 4t.hDARPA Speech and Natural Language Workshop, Morgan-Kauffman, pp.
346-351.15.
Strzalkowski, Tomek and Barbara Vauthey.
1991.
"NaturalLanguage Processing in Automated Information Retrieval.
"Proteus Project Memo #42, Courant Institute of MathematicalScience, New York University.16.
Grishman, Ralph and Tomek Strzalkowski.
1991.
"Informa-tion Retrieval and Natural Language Processing."
Positionpaper at the workshop on Future Dkections in NaturalLanguage Processing in Information Retrieval, Chicago.APPENDIX:  SAMPLE DATADOCUMENT TEXT:*RECORD**F* NO2366*F* TITLEComplex gamma funcdon with error control*F* TEXTAn algorithm to compute the gamma function andlog gamma function of a complex variable is presented.The standard algorithm is modified in several respectsto insure the continuity of the function valueand to reduce accumulation of round-off errors.
Inaddition to computation of function values, thisalgorithm includes an object-time estimation of round-offerrors.
Experimental data with regard to theeffectiveness of this error control are presented.a fortran program for the algorithm appears in thealgorithms ection of this issue.HEAD+MODIFIER PAIRS EXTRACTED:function+gammapresent+algorithmcompute+functionfunction+loggamma+logvariable+complexalgorithm+standardreduce+accumulateerror+round offinclude+estimateestimate+errorpresent+dataeffective+controlprogramme+fortranalgorithm+issuecontrol+erroralgorithm+computefunction+functiongamma+functiongamma+variablemodify+algorithm,insure+continue,accumulate+error,algorithm+include,estimate+object_time,error+round_off,data+experimentalcontrol+errorsection+algorithm210word-- -theorymathematicaldistributenormalminimumrelativeretrieveinformsizemediumeditortextsystemparallelreadcharacterdiscusspanelimplicatelegalsystemdistributemakerecommendinferdeductivemakearrangeshareresourcecomprehendlanguagesyntaxlanguagesciencecomputemaintaincosthead+modifier pairtheory+mathematicaltheory+mathematicaldistribute+normaldistribute+normalminimum+relativeminimum+relativeretrieve+informretrieve+informsize+mediumsize +mediumeditor + texteditor+textsystem+parallelsystem+parallelread+ characterread+characterdiscuss+paneldiscuss+panelimplicate+legalimplicate+legalsystem+distributesystem +distributemake+recommendmake+recommendinfer+deductiveinfer+deductivemake+arrangemake+arrangeshare+resourceshare +resourcecomprehend+languagecomprehend+languagesyntax+ languagesyntax+languagescience +computescience+computeconcept+maintaincost+maintainIC coeff.Table 2.
IC coefficients obtained from CACM-3204word1*aimalgorithmalgorithmacquire*adjacency*algebraic*americanassertback-up*buddycommitteecorrectbabyloniancriticalbest-jitbound-context*duplexdeletionearlierencasegiveimaginaryincompleteinputlead*marriagemeanmethodmemorymatchlowerminorprogresspurduerangeround-offremotepulsepurposetechniquemethodtrainpairsymbolstandardinfermini-maxtime-share*symposiumtheoremoldfinalfirst-fitlrreliableinsertpreviousminimum-areapresentrealmissoutput*trailstable*standardtechniquestoragerecognizeupper*woman*trendstanfordvarietytrunca tetelerypewaveTable 3.
Filtered word similarities (* indicates themore specific term).
