Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
267?276, Prague, June 2007. c?2007 Association for Computational LinguisticsExploiting Multi-Word Units in History-Based Probabilistic GenerationDeirdre Hogan, Conor Cafferkey, Aoife Cahill?
and Josef van GenabithNational Centre for Language TechnologySchool of Computing, Dublin City UniversityDublin 9, Irelanddhogan,ccafferkey,josef@computing.dcu.ieAbstractWe present a simple history-based model forsentence generation from LFG f-structures,which improves on the accuracy of previousmodels by breaking down PCFG indepen-dence assumptions so that more f-structureconditioning context is used in the predic-tion of grammar rule expansions.
In addi-tion, we present work on experiments withnamed entities and other multi-word units,showing a statistically significant improve-ment of generation accuracy.
Tested on sec-tion 23 of the Penn Wall Street Journal Tree-bank, the techniques described in this paperimprove BLEU scores from 66.52 to 68.82,and coverage from 98.18% to 99.96%.1 IntroductionSentence generation, or surface realisation, is thetask of generating meaningful, grammatically cor-rect and fluent text from some abstract semantic orsyntactic representation of the sentence.
It is an im-portant and growing field of natural language pro-cessing with applications in areas such as transfer-based machine translation (Riezler and Maxwell,2006) and sentence condensation (Riezler et al,2003).
While recent work on generation in restricteddomains, such as (Belz, 2007), has shown promisingresults there remains much room for improvementparticularly for broad coverage and robust genera-tors, like those of Nakanishi et al (2005) and Cahill?
Now at the Institut fu?r Maschinelle Sprachverarbeitung,Universita?t Stuttgart, Azenbergstrae 12, D-70174 Stuttgart,Germany.
aoife.cahill@ims.uni-stuttgart.deand van Genabith (2006), which do not rely on hand-crafted grammars and thus can easily be ported tonew languages.This paper is concerned with sentence genera-tion from Lexical-Functional Grammar (LFG) f-structures (Kaplan, 1995).
We present improve-ments in previous LFG-based generation modelsfirstly by breaking down PCFG independence as-sumptions so that more f-structure conditioning con-text is included when predicting grammar rule ex-pansions.
This history-based approach has workedwell in parsing (Collins, 1999; Charniak, 2000) andwe show that it also improves PCFG-based genera-tion.We also present work on utilising named entitiesand other multi-word units to improve generationresults for both accuracy and coverage.
There hasbeen a limited amount of exploration into the useof multi-word units in probabilistic parsing, for ex-ample in (Kaplan and King, 2003) (LFG parsing)and (Nivre and Nilsson, 2004) (dependency pars-ing).
We are not aware of any similar work on gen-eration.
In the LFG-based generation algorithm pre-sented by Cahill and van Genabith (2006) complexnamed entities (i.e.
those consisting of more thanone word token) and other multi-word units can befragmented in the surface realization.
We show thatthe identification of such units may be used as a sim-ple measure to constrain the generation model?s out-put.We take the generator of (Cahill and van Gen-abith, 2006) as our baseline generator.
When testedon f-structures for all sentences from Section 23 ofthe Penn Wall Street Journal (WSJ) treebank (Mar-267cus et al, 1993), the techniques described in this pa-per improve BLEU score from 66.52 to 68.82.
Inaddition, coverage is increased from 98.18% to al-most 100% (99.96%).The remainder of the paper is structured as fol-lows: in Section 2 we review related work on sta-tistical sentence generation.
Section 3 describes thebaseline generation model and in Section 4 we showhow the new history-based model improves over thebaseline.
In Section 5 we describe the source of themulti-word units (MWU) used in our experimentsand the various techniques we employ to make useof these MWUs in the generation process.
Section 6gives experimental details and results.2 Related Work on Statistical GenerationIn (statistical) generators, sentences are generatedfrom an abstract linguistic encoding via the appli-cation of grammar rules.
These rules can be hand-crafted grammar rules, such as those of (Langkilde-Geary, 2002; Carroll and Oepen, 2005), createdsemi-automatically (Belz, 2007) or, alternatively,extracted fully automatically from treebanks (Ban-galore and Rambow, 2000; Nakanishi et al, 2005;Cahill and van Genabith, 2006).Insofar as it is a broad coverage generator, whichhas been trained and tested on sections of the WSJcorpus, our generator is closer to the generatorsof (Bangalore and Rambow, 2000; Langkilde-Geary,2002; Nakanishi et al, 2005) than to those designedfor more restricted domains such as weather fore-cast (Belz, 2007) and air travel domains (Ratna-parkhi, 2000).Another feature which characterises statisticalgenerators is the probability model used to select themost probable sentence from among the space of allpossible sentences licensed by the grammar.
Onegeneration technique is to first generate all possiblesentences, storing them in a word lattice (Langkildeand Knight, 1998) or, alternatively, a generation for-est, a packed represention of alternate trees proposedby the generator (Langkilde, 2000), and then selectthe most probable sequence of words via an n-gramlanguage model.Increasingly syntax-based information is beingincorporated directly into the generation model.
Forexample, Carroll and Oepen (2005) describe a sen-tence realisation process which uses a hand-craftedHPSG grammar to generate a generation forest.
Aselective unpacking algorithm allows the extractionof an n-best list of realisations where realisationranking is based on a maximum entropy model.
Thisunpacking algorithm is used in (Velldal and Oepen,2005) to rank realisations with features defined overHPSG derivation trees.
They achieved the best re-sults when combining the tree-based model with ann-gram language model.Nakanishi et al (2005) describe a treebank-extracted HPSG-based chart generator.
Importingtechniques developed for HPSG parsing, they applya log linear model to a packed representation of allalternative derivation trees for a given input.
Theyfound that a model which included syntactic infor-mation outperformed a bigram model as well as acombination of bigram and syntax model.The probability model described in this paper alsoincorporates syntactic information, however, unlikethe discriminative HPSG models just described, itis a generative history- and PCFG-based model.While Belz (2007) and Humphreys et al (2001)mention the use of contextual features for the rulesin their generation models, they do not provide de-tails nor do they provide a formal probability model.To the best of our knowledge this is the first paperproviding a probabilistic generative, history-basedgeneration model.3 Surface Realisation from f-StructuresCahill and van Genabith (2006) present a prob-abilistic surface generation model for LFG (Ka-plan, 1995).
LFG is a constraint-based theoryof grammar, which analyses strings in terms ofc(onstituency)-structure and f(unctional)-structure(Figure 1).
C-structure is defined in terms of CFGs,and f-structures are recursive attribute-value ma-trices which represent abstract syntactic functions(such as SUBJect, OBJect, OBLique, COMPlement(sentential), ADJ(N)unct), agreement, control, long-distance dependencies and some semantic informa-tion (e.g.
tense, aspect).C-structures and f-structures are related in a pro-jection architecture in terms of a piecewise corre-spondence ?.1 The correspondence is indicated in1Our formalisation follows (Kaplan, 1995).268S?=?NP VP(?
SUBJ)= ?
?=?NNP V NP?=?
?=?
(?
OBJ)= ?Susan contacted PRP(?
PRED) = ?Susan?
(?
PRED) = ?contact?
?=?(?
NUM) = SG (?
TENSE) = past(?
PERS) = 3 her(?
PRED) = ?pro?(?
NUM) = SG(?
PERS) = 3f1:?????
?PRED ?CONTACT?(?SUBJ)(?OBJ)?
?SUBJ f2:[PRED ?SUSAN?NUM SGPERS 3]OBJ f2:[PRED ?PRO?NUM SGPERS 3]TENSE PAST?????
?Figure 1: C- and f-structures with ?
links for the sentence Susan contacted her.terms of the curvy arrows pointing from c-structurenodes to f-structure components in Figure 1.
Givena c-structure node ni, the corresponding f-structurecomponent fj is ?(ni).
F-structures and the c-structure/f-structure correspondence are describedin terms of functional annotations on c-structurenodes (CFG grammar rules).
An equation of theform (?F) = ?
states that the f-structure associatedwith the mother of the current c-structure node (?
)has an attribute (grammatical function) (F), whosevalue is the f-structure of the current node (?
).The up-arrows and down-arrows are shorthand for?
(M(ni)) = ?
(ni) where ni is the c-structure nodeannotated with the equation.2Treebest := argmaxTreeP (Tree|F-Str) (1)P (Tree|F-Str) :=?X ?
Y in TreeFeats = {ai|?vj(?
(X))ai = vj}P (X ?
Y |X, Feats) (2)The generation model of (Cahill and van Gen-abith, 2006) maximises the probability of a treegiven an f-structure (Eqn.
1), and the string gener-ated is the yield of the highest probability tree.
Thegeneration process is guided by purely local infor-mation in the input f-structure: f-structure annotatedCFG rules (LHS ?
RHS) are conditioned on theirLHSs and on the set of features/attributes Feats ={ai|?vj?
(LHS)ai = vj}3 ?-linked to the LHS (Eqn.2M is the mother function on CFG tree nodes.3In words, Feats is the set of top level features/attributes(those attributes ai for which there is a value vi) of the f-structure ?
linked to the LHS.2).
Table 1 shows a generation grammar rule andconditioning features extracted from the example inFigure 1.
The probability of a tree is decomposedinto the product of the probabilities of the f-structureannotated rules (conditioned on the LHS and localFeats) contributing to the tree.
Conditional proba-bilities are estimated using maximum likelihood es-timation.grammar rule local conditioning featuresS(?=?)?
NP(?SUBJ=?)
VP(?=?)
S(?=?
), {SUBJ,OBJ,PRED,TENSE}Table 1: Example grammar rule (from Figure 1).Cahill and van Genabith (2006) note that condi-tioning f-structure annotated generation rules on lo-cal features (Eqn.
2) can sometimes cause the modelto make inappropriate choices.
Consider the follow-ing scenario where in addition to the c-/f-structure inFigure 1, the training set contains the c-/f-structuredisplayed in Figure 2.From Figures 1 and 2, the model learns (amongothers) the generation rules and conditional proba-bilities displayed in Tables 2 and 3.F-Struct Feats Grammar Rules Prob{SUBJ, OBJ, PRED} S(?=?)
?
NP(?SUBJ=?)
VP(?=?)
1{SUBJ, OBJ, PRED} VP(?=?)
?
V(?=?)
NP(?OBJ=?)
1{NUM, PER, GEN} NP(?SUBJ=?)
?
NNP(?=?)
0.5{NUM, PER, GEN} NP(?SUBJ=?)
?
PRP(?=?)
0.5{NUM, PER, GEN} NP(?OBJ=?)
?
PRP(?=?)
1Table 2: A sample of internal grammar rules ex-tracted from Figures 1 and 2.Given the input f-structure (for Sheaccepted) in Figure 3, (and assuming suit-able generation rules for intransitive VPs andaccepted) the model would produce the inappro-priate highest probability tree of Figure 4 with anincorrect case for the pronoun in subject position.269S?=?NP VP(?
SUBJ)= ?
?=?PRP V NP?=?
?=?
(?
OBJ)= ?She hired PRP(?
PRED) = ?pro?
(?
PRED) = ?hire?
?=?(?
NUM) = SG (?
TENSE) = past(?
PERS) = 3 her(?
PRED) = ?pro?(?
NUM) = SG(?
PERS) = 3f1 :?????
?PRED ?HIRE?(?SUBJ)(?OBJ)?
?SUBJ f2 :[PRED ?PRO?NUM SGPERS 3]OBJ f2 :[PRED ?PRO?NUM SGPERS 3]TENSE PAST?????
?Figure 2: C- and f-structures with ?
links for the sentence She hired her.F-Struct Feats Grammar Rules Prob{PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP(?=?)
?
she 0.33{PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP(?=?)
?
her 0.66Table 3: A sample of lexical item rules extractedfrom Figures 1 and 2.??????SUBJ?
?PRED proNUM sgPERS 3GEND fem?
?PRED acceptTENSE past?????
?Figure 3: Input f-structure for She accepted.To solve the problem, Cahill and van Gen-abith (2006) apply an automatic generation gram-mar transformation to their training data: they au-tomatically label CFG nodes with additional caseinformation and the model now learns the new im-proved generation rules of Tables 4 and 5.
Notehow the additional case labelling subverts the prob-lematic independence assumptions of the probabil-ity model and communicates the fact that a subjectNP has to be realised as nominative case from theS ?
NP-nom VP production, via the intermediateNP-nom ?
PRP-nom, down to the lexical produc-tion PRP-nom ?
she.
The labelling guarantees that,given the example f-structure in Figure 3, the modelgenerates the correct string she accepted.F-Struct Feats Grammar Rules{SUBJ, OBJ, PRED} S(?=?)
?
NP-nom(?SUBJ=?)
VP(?=?
){SUBJ, OBJ, PRED} VP(?=?)
?
V(?=?)
NP-acc(?OBJ=?
){NUM, PER, GEN} NP-nom(?SUBJ=?)
?
PRP-nom(?=?
){NUM, PER, GEN} NP-nom(?SUBJ=?)
?
NNP-nom(?=?
){NUM, PER, GEN} NP-acc(?OBJ=?)
?
PRP-acc(?=?
)Table 4: Internal grammar rules with case markings.S?=?NP VP(?
SUBJ)= ?
?=?PRP V?=?
?=?her accepted(?
PRED) = ?pro?
(?
PRED) = ?hire?(?
NUM) = SG (?
TENSE) = past(?
PERS) = 3Figure 4: Inappropriate output: her accepted.F-Struct Feats Grammar Rules{PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP-nom(?=?)
?
she{PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP-acc(?=?)
?
herTable 5: Lexical item rules with case markings4 A History-Based Generation ModelThe automatic generation grammar transform pre-sented in (Cahill and van Genabith, 2006) providesa solution to coarse-grained and (in fact) inappropri-ate independence assumptions in the basic genera-tion model.
However, there is a sense in which theproposed cure improves on the symptoms, but notthe cause of the problem: it weakens independenceassumptions by multiplying and hence increasingthe specificity of conditioning CFG category labels.There is another option available to us, and that isthe option we will explore in this paper: instead ofapplying a generation grammar transform, we willimprove the f-structure-based conditioning of thegeneration rule probabilities.
In the original model,rules are conditioned on purely local f-structure con-text: the set of features/attributes ?-linked to theLHS of a grammar rule.
As a direct consequenceof this, the conditioning (and hence the model) can-not not distinguish between NP, PRP and NNP rules270appropriate to e.g.
subject (SUBJ) or object con-texts (OBJ) in a given input f-structure.
However,the required information can easily be incorporatedinto the generation model by uniformly conditioninggeneration rules on their parent (mother) grammati-cal function, in addition to the local ?-linked featureset.
This additional conditioning has the effect ofmaking the choice of generation rules sensitive tothe history of the generation process, and, we argue,provides a simpler, more uniform, general, intuitiveand natural probabilistic generation model obviatingthe need for CFG-grammar transforms in the origi-nal proposal of (Cahill and van Genabith, 2006).In the new model, each generation rule is nowconditioned on the LHS rule CFG category, the setof features ?-linked to LHS and the parent grammat-ical function of the f-structure ?-linked to LHS.
In agiven c-/f-structure pair, for a CFG node n, the par-ent grammatical function of the f-structure ?-linkedto n is that grammatical function GF, which, if wetake the f-structure ?-linked to the mother M(n), andapply it to GF, returns the f-structure ?-linked to n:(?
(M(n))GF) = ?
(n).The basic idea is best explained by way of anexample.
Consider again Figure 1.
The mothergrammatical function of the f-structure f2 asso-ciated with node NP(?SUBJ=?)
and its daughterNNP(?=?)
(via the ?=?
functional annotation) isSUBJ, as (?
(M(n2))SUBJ) = ?
(n2), or equivalently(f1SUBJ) = f2.Given Figures 1 and 2 as training set, the im-proved model learns the generation rules (the mothergrammatical function of the outermost f-structure isassumed to be a dummy TOP grammatical function)of Tables 6 and 7.F-Struct Feats Grammar Rules{SUBJ, OBJ, PRED, TOP} S(?=?)
?
NP(?SUBJ=?)
VP(?=?
){SUBJ, OBJ, PRED, TOP} VP(?=?)
?
V(?=?)
NP(?OBJ=?
){NUM, PER, GEN, SUBJ} NP(?SUBJ=?)
?
PRP(?=?
){NUM, PER, GEN, OBJ} NP(?OBJ=?)
?
PRP(?=?
){NUM, PER, GEN, SUBJ} NP(?SUBJ=?)
?
NNP(?=?
)Table 6: Grammar rules with extra feature extractedfrom F-Structures.Note, that for our example the effect of the uni-form additional conditioning on mother grammat-ical function has the same effect as the genera-tion grammar transform of (Cahill and van Gen-abith, 2006), but without the need for the gram-F-Struct Feats Grammar Rules{PRED=PRO,NUM=SG PER=3, GEN=FEM, SUBJ} PRP(?=?)
?
she{PRED=PRO,NUM=SG PER=3, GEN=FEM, OBJ} PRP(?=?)
?
herTable 7: Lexical item rules.mar transform.
Given the input f-structure in Fig-ure 3, the model will generate the correct stringshe accepted.
In addition, uniform condition-ing on mother grammatical function is more generalthan the case-phenomena specific generation gram-mar transform of (Cahill and van Genabith, 2006),in that it applies to each and every sub-part of arecursive input f-structure driving generation, mak-ing available relevant generation history (context) toguide local generation decisions.The new history-based probabilistic generationmodel is defined as:P (Tree|F-Str) :=?X ?
Y in TreeFeats = {ai|?vj(?
(X))ai = vj}(?
(M(X)))GF = ?
(X)P (X ?
Y |X, Feats,GF) (3)Note that the new conditioning feature, the f-structure mother grammatical function, GF, is avail-able from structure previously generated in the c-structure tree.
As such, it is part of the history ofthe tree, i.e.
it has already been generated in the top-down derivation of the tree.
In this way, the gen-eration model resembles history-based models forparsing (Black et al, 1992; Collins, 1999; Charniak,2000).
Unlike, say, the parent annotation for parsingof (Johnson, 1998) the parent GF feature for a par-ticular node expansion is not merely extracted fromthe parent node in the c-structure tree, but is some-times extracted from an ancestor node further up thec-structure tree via intervening ?=?
functional an-notations.Section 6 provides evaluation results for the newmodel on section 23 of the Penn treebank.5 Multi-Word UnitsIn another effort to improve generator accuracy overthe baseline model we explored the use of multi-word units in generation.
We expect that the identi-fication of MWUs may be useful in imposing word-order constraints and reducing the complexity of thegeneration task.
Take, for example, the following271????????APP?????
?ADJUNCT[PRED ?New?NUM sgPERS 3]PRED ?York?NUM sgPERS 3??????????????
[APP[PRED ?New York?NUM sgPERS 3]]????????APP?????
?ADJUNCT[PRED ?New?/NE1 1NUM sgPERS 3]PRED ?York?/NE1 2NUM sgPERS 3?????????????
?Figure 5: Three different f-structure formats.
From left to right: the original f-structure format; the MWUchunk format; the MWU mark-up format.two sentences which show the gold version of a sen-tence followed by the version of the sentence pro-duced by the generator:Gold By this time , it was 4:30 a.m. in New York ,and Mr. Smith fielded a call from a New Yorkcustomer wanting an opinion on the Britishstock market , which had been having trou-bles of its own even before Friday ?s New Yorkmarket break .Test By this time , in New York , it was 4:30 a.m., and Mr. Smith fielded a call from New acustomer York , wanting an opinion on themarket British stock which had been havingtroubles of its own even before Friday ?s NewYork market break .The gold version of the sentence contains a multi-word unit, New York, which appears fragmented inthe generator output.
If multi-word units were eithertreated as one token throughout the generation pro-cess, or, alternatively, if a constraint were imposedon the generator such that multi-word units were al-ways generated in the correct order, then this shouldhelp improve generation accuracy.
In Section 5.1we describe the various techniques that were usedto incorporate multi-word units into the generationprocess and in 5.2 we detail the different types andsources of multi-word unit used in the experiments.Section 6 provides evaluation results on test and de-velopment sets from the WSJ treebank.5.1 Incorporating MWUs into the GenerationProcessWe carried out three types of experiment which, indifferent ways, enabled the generation process torespect the restrictions on word-order provided bymulti-word units.
For the first experiments (type1), the WSJ treebank training and test data werealtered so that multi-word units are concatenatedinto single words (for example, New York becomesNew York).
As in (Cahill and van Genabith, 2006) f-structures are generated from the (now altered) tree-bank and from this data, along with the treebanktrees, the PCFG-based grammar, which is used fortraining the generation model, is extracted.
Simi-larly, the f-structures for the test and developmentsets are created from Penn Treebank trees whichhave been modified so that multi-word units formsingle units.
The leftmost and middle f-structures inFigure 5 show an example of an original f-structureformat and a named-entity chunked format, respec-tively.
Strings output by the generator are then post-processed so that the concatenated word sequencesare converted back into single words.In the second experiment (type 2) only the testdata was altered with no concatenation of MWUscarried out on the training data.In the final experiments (type 3), instead of con-catenating named entities, a constraint is introducedto the generation algorithm which penalises the gen-eration of sequences of words which violate the in-ternal word order of named entities.
The input ismarked-up in such a way that, although named en-tities are no longer chunked together to form singlewords, the algorithm can read which items are partof named entities.
See the rightmost f-structure inFigure 5 for an example of an f-structure marked-up in this way.
The tag NE1 1, for example, indi-cates that the sub-f-structure is part of a named iden-tity with id number 1 and that the item correspondsto the first word of the named entity.
The baselinegeneration algorithm, following Kay (1996)?s workon chart generation, already contains the hard con-straint that when combining two chart edges theymust cover disjoint sets of words.
We added an ad-ditional constraint which prevents edges from beingcombined if this would result in the generation ofa string which contained a named entity which was272either incomplete or where the words in the namedentity were generated in the wrong order.5.2 Types of MWUs used in ExperimentsWe carry out experiments with multi-word unitsfrom three different sources.
First, we use the outputof the maximum entropy-based named entity recog-nition system of (Chieu and Ng, 2003).
This sys-tem identifies four types of named entity: person,organisation, location, and miscellaneous.
Addition-ally we use a dictionary of candidate multi-word ex-pressions based on a list from the Stanford Multi-word Expression Project4.
Finally, we also carry outexperiments with multi-word units extracted fromthe BBN Pronoun Coreference and Entity Type Cor-pus (Weischedel and Brunstein, 2005).
This supple-ments the Penn WSJ treebank?s one million words ofsyntax-annotated Wall Street Journal text with addi-tional annotations of 23 named entity types, includ-ing nominal-type named entities such as person, or-ganisation, location, etc.
as well as numeric typessuch as date, time, quantity and money.
Since theBBN corpus data is very comprehensive and is hand-annotated we take this be be a gold standard, repre-senting an upper bound for any gains that might bemade by identifying complex named entities in ourexperiments.5 Table 8 gives examples of the varioustypes of MWUs identified by the three sources.For our purposes we are not concerned with thedistinctions between different types of named enti-ties; we are merely exploiting the fact that they maybe treated as atomic units in the generation model.
Inall cases we disregard multi-word units that cross theoriginal syntactic bracketing of the WSJ treebank.An overview of the various types of multi-word unitsused in our experiments is presented in Table 9.6 Experimental EvaluationAll experiments were carried out on the WSJ tree-bank with sections 02-21 for training, section 24 fordevelopment and section 23 for final test results.
TheLFG annotation algorithm of (Cahill et al, 2004)was used to produce the f-structures for develop-ment, test and training sets.4mwe.stanford.edu5Although it is possible there are other types of MWUs thatmay be more suitable to the task than the named entities identi-fied by BBN, so further gains might be possible.MWU type ExamplesNames Martha MatthewsYoshio HatakeyamaOrganisations Rolls-Royce Motor Cars Inc.Washington State UniversityLocations New York CityNew ZealandTime expressions October 19thtwo years agothe 21st centuryQuantities $2.7 million to $3 millionabout 25 %60 mphPrepositional expressions in factat the timeon averageTable 8: Examples of some of the types of MWUfrom the three different sources.average number average length(Chieu and Ng, 2003) 0.61 2.40Stanford MWE Project 0.10 2.48BBN Corpus 1.15 2.66Table 9: Average number of MWUs per sentenceand average MWU length in the WSJ treebankgrouped by MWU source.Table 10 shows the final results for section 23.
Foreach test we present BLEU score results as well asString Edit Distance and coverage.
We measure sta-tistical significance using two different tests.
Firstwe use a bootstrap resampling method, popular formachine translation evaluations, to measure the sig-nificance of improvements in BLEU scores, with aresampling rate of 1000.6 We also calculated thesignificance of an increase in String Edit Distanceby carrying out a paired t-test on the mean differ-ence of the String Edit Distance scores.
In Table 10,?
means significant at level 0.005.
> means signif-icant at level 0.05.In Table 10, Baseline gives the results of thegeneration algorithm of (Cahill and van Genabith,2006).
HB Model refers to the improved modelwith the increased history context, as described inSection 4.
The results, where for example theBLEU score rises from 66.52 to 67.24, show thateven increasing the conditioning context by a limited6Scripts for running the bootstrapping method carriedout in our evaluation are available for download at projec-tile.is.cs.cmu.edu/research/public/tools/bootStrap/tutorial.htm273Section 23 (2416 sentences)Model BLEU StringEd Coverage BLEU Bootstrap Signif StringEd Paired T-Test1.
Baseline 66.52 68.69 98.182.
HB Model 67.24 69.89 99.88 ?
1 ?
13.
+MWU Best Automatic 67.81 70.36 99.92 ?
2 ?
24.
MWU BBN 68.82 70.92 99.96 ?
3 > 3Table 10: Results on Section 23 for all sentence lengths.amount increases the accuracy of the system signif-icantly for both BLEU and String Edit Distance.
Inaddition, coverage goes up from 98.18% to 99.88%.+MWU Best Automatic displays our best resultsusing automatically identified named entities.
Thesewere achieved using experiment type 2, describedin Section 5, with the MWUs produced by (Chieuand Ng, 2003).
Results displayed in Table 10 upto this point are cumulative.
The final row in Ta-ble 10, MWU BBN, shows the best results with BBNMWUs: the history-based model with BBN multi-word units incorporated in a type 1 experiment.We now discuss the various MWU experimentsin more detail.
See Table 11 for a breakdown ofthe MWU experiment results on the developmentset, WSJ section 24.
Our baseline for these exper-iments is the history-based generator presented inSection 4.
For each experiment type described inSection 5.1 we ran three experiments, varying thesource of MWUs.
First, MWUs came from the auto-matic NE recogniser of (Chieu and Ng, 2003), thenwe added the MWUs from the Stanford list and fi-nally we ran tests with MWUs extracted from theBBN corpus.Our first set of experiments (type 1), where bothtraining data and development set data were MWU-chunked, produced the worst results for the automat-ically chunked MWUs.
BLEU score accuracy actu-ally decreased for the automatically chunked MWUexperiments.
In an error analysis of type 1 ex-periments with (Chieu and Ng, 2003) concatenatedMWUs, we inspected those sentences where accu-racy had decreased from the baseline.
We foundthat for over half (51.5%) of these sentences, the in-put f-structures contained no multi-word units at all.The problem for these sentences therefore lay withthe probabilistic grammar extracted from the MWU-chunked training data.
When the source of MWUfor the type 1 experiments was the BBN, however,accuracy improved significantly over the baselineand the result is the highest accuracy achieved overall experiment types.
One possible reason for thelow accuracy scores in the type 1 experiments withthe (Chieu and Ng, 2003) MWU chunked data couldbe noisy MWUs which negatively affect the gram-mar.
For example, the named entity recogniserof (Chieu and Ng, 2003) achieves an accuracy of88.3% on section 23 of the Penn Treebank.In order to avoid changing the grammar throughconcatenation of MWU components (as in exper-iment type 1) and thus risking side-effects whichcause some heretofore likely constructions becomeless likely and vice versa, we ran the next set of ex-periments (type 2) which leave the original grammarintact and alter the input f-structures only.
Theseexperiments were more successful overall and weachieved an improvement over the baseline for bothBLEU and String Edit Distance scores with allMWU types.
As can be seen from Table 11 thebest score for automatically chunked MWUs arewith the (Chieu and Ng, 2003) MWUs.
Accuracydecreases marginally when we added the StanfordMWUs.
In our final set of experiments (type 3) al-though the accuracy for all three types of MWUsimproves over the baseline, accuracy is a little be-low the type 2 experiments.It is difficult to compare sentence generators sincethe information contained in the input varies greatlybetween systems, systems are evaluated on differenttest sets and coverage also varies considerably.
Inorder to compare our system with those of (Nakan-ishi et al, 2005) and (Langkilde-Geary, 2002) wereport our best results with automatically acquiredMWUs for sentences of ?
20 words in length onsection 23: our system gets coverage of 100% and aBLEU score of 71.39.
For the same test set Nakan-ishi et al (2005) achieved coverage of 90.75 and aBLEU score of 77.33.
Langkilde-Geary (2002) re-274Section 24 (1346 sentences)Model MWUs BLEU StringEd CoverageHB Model 65.85 69.93 99.93type 1 (Chieu and Ng, 2003) 65.81 70.34 99.93(training and test data chunked) +Stanford MWEs 64.81 69.67 99.93BBN 67.24 71.46 99.93type 2 (Chieu and Ng, 2003) 66.37 70.26 99.93(test data chunked) +Stanford MWEs 66.28 70.21 99.93BBN 66.84 70.74 99.93type 3 (Chieu and Ng, 2003) 66.30 70.12 100(internal generation constraint) +Stanford MWEs 66.07 70.02 99.93BBN 66.45 70.14 99.93Table 11: Results on Section 24, all sentence lengths.ports 82.7% coverage and a BLEU score of 75.7%on the same test set with the ?permute,no dir?
typeinput.
Langkilde-Geary (2002) report results for ex-periments with varying levels of linguistic detail inthe input given to the generator.
As with Nakanishiet al (2005) we find the ?permute,no dir?
type of in-put is most comparable to the level of informationcontained in our input f-structures.
Finally, the sym-bolic generator of Callaway (2003) reports a Sim-ple String Accuracy score of 88.84 and coverage of98.7% on section 23 for all sentence lengths.7 Conclusion and Future WorkWe have presented techniques which improve the ac-curacy of an already state-of-art surface generationmodel.
We found that a history-based model thatincreases conditioning context in PCFG style rulesby simply including the grammatical function of thef-structure parent, improves generator accuracy.
Inthe future we will experiment with increasing condi-tioning context further and using more sophisticatedsmoothing techniques to avoid sparse data problemswhen conditioning is increased.We have also demonstrated that automatically ac-quired multi-word units can bring about moderate,but significant, improvements in generator accuracy.For automatically acquired MWUs, we found thatthis could best be achieved by concatenating inputitems when generating the f-structure input to thegenerator, while training the input generation gram-mar on the original (i.e.
non-MWU concatenated)sections of the treebank.
Relying on the BBN cor-pus as a source of multi-word units, we gave an up-per bound to the potential usefulness of multi-wordunits in generation and showed that automaticallyacquired multi-word units, encouragingly, give re-sults not far below the upper bound.ReferencesSrinivas Bangalore and Owen Rambow.
2000.
Exploit-ing a probabilistic hierarchical model for generation.In Proceedings of the 18th COLING.Anja Belz.
2007.
Probabilistic generation of wether fore-cast texts.
In Proceedings of NAACL-HLT.Ezra Black, Fred Jelinek, John Lafferty, David M. Mager-man, Robert Mercer, and Salim Roukos.
1992.
To-wards history-based grammars: Using richer modelsfor probabilistic parsing.
In Proceeding of the 5thDARPA Speech and Language Workshop.Aoife Cahill and Josef van Genabith.
2006.
RobustPCFG-based generation using automatically acquiredLFG approximations.
In Proceedings of the 44th ACL.Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef vanGenabith, and Andy Way.
2004.
Long-distance de-pendency resolution in automatically acquired wide-coverage PCFG-based LFG approximations.
In Pro-ceedings of the 42nd ACL.Charles B. Callaway.
2003.
Evaluating coverage forlarge symbolic NLG grammars.
In In Proceedings ofthe 18th IJCAI.John A. Carroll and Stephan Oepen.
2005.
High ef-ficiency realization for a wide-coverage unificationgrammar.
In Proceedings of IJCNLP.Eugene Charniak.
2000.
A maximum entropy-inspiredparser.
In Proceedings of the 1st NAACL.Hai Leong Chieu and Hwee Tou Ng.
2003.
Named en-tity recognition with a maximum entropy approach.
InProceedings of the CoNLL.275Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Kevin Humphreys, Mike Calcagno, and David Weise.2001.
Reusing a statistical language model for gen-eration.
In Proceedings of the 8th European Workshopon Natural Language Generation (EWNLG).Mark Johnson.
1998.
PCFG models of linguistic treerepresentations.
Computational Linguistics, 24.Ronald M. Kaplan and Tracy Holloway King.
2003.Low-level mark-up and large-scale LFG grammar pro-cessing.
In Proceedings of the Lexical FunctionalGrammar Conference.Ron Kaplan.
1995.
The formal architecture oflexical-functional grammar.
In Dalrymple, Kaplan,Maxwell, and Zaenen, editors, Formal Issues inLexical-Functional Grammar, pages 7?27.
CSLI Pub-lications.Martin Kay.
1996.
Chart generation.
In Proceedings ofthe 34th ACL.Irene Langkilde and Kevin Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
InProceedings of the 36th Annual Meeting of the Asso-ciation for Computational Linguistics and the 17th In-ternational Conference on Computational Linguistics(ACL-COLING).Irene Langkilde-Geary.
2002.
An empirical verificationof coverage and correctness for a general-purpose sen-tence generator.
In Proceedings of the 2nd INLG.Irene Langkilde.
2000.
Forest-based statistical sentencegeneration.
In Proceedings of the 1st NAACL.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic models for disambiguation of anHPSG-based chart generator.
In Proceedings of the9th IWPT.Joakim Nivre and Jens Nilsson.
2004.
Multiword unitsin syntactic parsing.
In Workshop on Methodologiesand Evaluation of Multiword Units in Real-World Ap-plications.Adwait Ratnaparkhi.
2000.
Trainable methods for sur-face natural language generation.
In Proceedings ofthe 1st NAACL.Stefan Riezler and John T. Maxwell.
2006.
Grammat-ical machine translation.
In Proceedings of the 6thNAACL.Stefan Riezler, Tracy H. King, Richard Crouch, and An-nie Zaenen.
2003.
Statistical sentence condensationusing ambiguity packing and stochastic disambigua-tion methods for lexical-functional grammar.
In Pro-ceedings of the 3rd NAACL.Erik Velldal and Stephan Oepen.
2005.
Maximum en-tropy models for realization ranking.
In Proceedingsof the MT-Summit.Ralph Weischedel and Ada Brunstein, 2005.
BBN pro-noun coreference and entity type corpus.
TechnicalReport.276
