Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 409?417,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsImproved Translation with Source Syntax LabelsHieu HoangSchool of InformaticsUniversity of Edinburghh.hoang@sms.ed.ac.ukPhilipp KoehnSchool of InformaticsUniversity of Edinburghpkoehn@inf.ed.ac.ukAbstractWe present a new translation modelthat include undecorated hierarchical-stylephrase rules, decorated source-syntaxrules, and partially decorated rules.Results show an increase in translationperformance of up to 0.8% BLEU forGerman?English translation when trainedon the news-commentary corpus, usingsyntactic annotation from a source lan-guage parser.
We also experimented withannotation from shallow taggers and foundthis increased performance by 0.5% BLEU.1 IntroductionHierarchical decoding is usually described as aformally syntactic model without linguistic com-mitments, in contrast with syntactic decodingwhich constrains rules and production with lin-guistically motivated labels.
However, the decod-ing mechanism for both hierarchical and syntacticsystems are identical and the rule extraction aresimilar.Hierarchical and syntax statistical machinetranslation have made great progress in the lastfew years and can claim to represent the state ofthe art in the field.
Both use synchronous con-text free grammar (SCFG) formalism, consistingof rewrite rules which simultaneously parse the in-put sentence and generate the output sentence.
Themost common algorithm for decoding with SCFGis currently CKY+ with cube pruning works forboth hierarchical and syntactic systems, as imple-mented in Hiero (Chiang, 2005), Joshua (Li et al,2009), and Moses (Hoang et al, 2009)Rewrite rules in hierarchical systems have gen-eral applicability as their non-terminals are undec-orated, giving hierarchical system broad coverage.However, rules may be used in inappropriate sit-uations without the labeled constraints.
The gen-eral applicability of undecorated rules create spu-rious ambiguity which decreases translation per-formance by causing the decoder to spend moretime sifting through duplicate hypotheses.
Syntac-tic systems makes use of linguistically motivatedinformation to bias the search space at the expenseof limiting model coverage.This paper presents work on combining hier-archical and syntax translation, utilizing the highcoverage of hierarchical decoding and the in-sights that syntactic information can bring.
Weseek to balance the generality of using undeco-rated non-terminals with the specificity of labelednon-terminals.
Specifically, we will use syntac-tic labels from a source language parser to labelnon-terminal in production rules.
However, othersource span information, such as chunk tags, canalso be used.We investigate two methods for combining thehierarchical and syntactic approach.
In the firstmethod, syntactic translation rules are used con-currently with a hierarchical phrase rules.
Eachruleset is trained independently and used concur-rently to decode sentences.
However, results forthis method do not improve.The second method uses one translation modelcontaining both hierarchical and syntactic rules.Moreover, an individual rule can contain bothdecorated syntactic non-terminals, and undeco-rated hierarchical-style non-terminals (also, theleft-hand-side non-terminal may, or may not bedecorated).
This results in a 0.8% improvementover the hierarchical baseline and analysis suggestthat long-range ordering has been improved.We then applied the same methods but usinglinguistic annotation from a chunk tagger (Abney,1991) instead of a parser and obtained an improve-ment of 0.5% BLEU over the hierarchical base-line, showing that gains with additional source-side annotation can be obtained with simpler tools.2 Past WorkHierarchical machine translation (Chiang, 2005)extends the phrase-based model by allowing theuse of non-contiguous phrase pairs (?productionrules?).
It promises better re-ordering of transla-tion as the reordering rules are an implicit part ofthe translation model.
Also, hierarchical rules fol-low the recursive structure of the sentence, reflect-ing the linguistic notion of language.However, the hierarchical model has severallimitations.
The model makes no use of linguis-tic information, thus creating a simple model withbroad coverage.
However, (Chiang, 2005) alsodescribe heuristic constraints that are used during409rule extraction to reduce spurious ambiguity.
Theresulting translation model does reduces spuriousambiguity but also reduces the search space in anarbitrary manner which adversely affects transla-tion quality.Syntactic labels from parse trees can be usedto annotate non-terminals in the translation model.This reduces incorrect rule application by restrict-ing rule extraction and application.
However,as noted in (Ambati and Lavie, 2008) and else-where,the na?
?ve approach of constraining everynon-terminal to a syntactic constituent severelylimits the coverage of the resulting grammar,therefore, several approaches have been used toimprove coverage when using syntactic informa-tion.Zollmann and Venugopal (2006) allow rules tobe extracted where non-terminals do not exactlyspan a target constituent.
The non-terminals arethen labeled with complex labels which amalga-mates multiple labels in the span.
This increasecoverage at the expense of increasing data sparsityas the non-terminal symbol set increases dramati-cally.
Huang and Chiang (2008) use parse infor-mation of the source language, production rulesconsists of source tree fragments and target lan-guages strings.
During decoding, a packed for-est of the source sentence is used as input, theproduction rule tree fragments are applied to thepacked forest.
Liu et al (2009) uses joint decod-ing with a hierarchical and tree-to-string modeland find that translation performance increase for aChinese-English task.
Galley et al (2004) createsminimal translation rules which can explain a par-allel sentence pair but the rules generated are notoptimized to produce good translations or cover-age in any SMT system.
This work was extendedand described in (Galley et al, 2006) which cre-ates rules composed of smaller, minimal rules, aswell as dealing with unaligned words.
These mea-sures are essential for creating good SMT systems,but again, the rules syntax are strictly constrainedby a parser.Others have sought to add soft linguistic con-straints to hierarchical models using addition fea-ture functions.
Marton and Resnik (2008) add fea-ture functions to penalize or reward non-terminalswhich cross constituent boundaries of the sourcesentence.
This follows on from earlier work in(Chiang, 2005) but they see gains when finer grainfeature functions which different constituencytypes.
The weights for feature function is tunedin batches due to the deficiency of MERT whenpresented with many features.
Chiang et al (2008)rectified this deficiency by using the MIRA to tuneall feature function weights in combination.
How-ever, the translation model continues to be hierar-chical.Chiang et al (2009) added thousands oflinguistically-motivated features to hierarchicaland syntax systems, however, the source syntaxfeatures are derived from the research above.
Thetranslation model remain constant but the parame-terization changes.Shen et al (2009) discusses soft syntax con-straints and context features in a dependency treetranslation model.
The POS tag of the target headword is used as a soft constraint when applyingrules.
Also, a source context language model anda dependency language model are also used as fea-tures.Most SMT systems uses the Viterbi approxi-mation whereby the derivations in the log-linearmodel is not marginalized, but the maximumderivation is returned.
String-to-tree models buildon this so that the most probable derivation, in-cluding syntactic labels, is assumed to the mostprobable translation.
This fragments the deriva-tion probability and the further partition the searchspace, leading to pruning errors.
Venugopal et al(2009) attempts to address this by efficiently es-timating the score over an equivalent unlabeledderivation from a target syntax model.Ambati and Lavie (2008); Ambati et al (2009)notes that tree-to-tree often underperform modelswith parse tree only on one side due to the non-isomorphic structure of languages.
This motivatesthe creation of an isomorphic backbone into thetarget parse tree, while leaving the source parseunchanged.3 ModelIn extending the phrase-based model to the hier-archical model, non-terminals are used in transla-tion rules to denote subphrases.
Hierarchical non-terminals are undecorated so are unrestricted to thespan they cover.
In contrast, SCFG-based syntac-tic models restrict the extraction and applicationof non-terminals, typically to constituency spansof a parse tree or forest.
Our soft syntax modelcombine the hierarchical and source-syntactic ap-proaches, allowing translation rules with undeco-rated and decorated non-terminals with informa-tion from a source language tool.We give an example of the rules extracted froman aligned sentence in Figure 1, with a parse treeon the source side.Lexicalized rules with decorated non-terminalsare extracted, we list five (non-exhaustive) exam-ples below.410Figure 1: Aligned parsed sentenceNP ?
Musharrafs letzter Akt# Musharraf ?s Last ActNP ?
NE1 letzter Akt # X1 Last ActNP ?
NE1 ADJA2 Akt # X1 X2 ActNP ?
NE1 letzter NN2 # X1 Last X2TOP ?
NE1 ADJA2 Akt ?
# X1 X2 Act ?Hierarchical style rules are also extracted wherethe span doesn?t exactly match a parse constituent.We list 2 below.X ?
letzter Akt # Last ActX ?
letzter X1 # Last X1Unlexicalized rules with decorated non-terminals are also extracted:TOP ?
NP1 PUNC2 # X1 X2NP ?
NE1 ADJA2 NN3 # X1 X2 X3Rules are also extracted which contains a mix-ture of decorated and undecorated non-terminals.These rules can also be lexicalized or unlexical-ized.
A non-exhaustive sample is given below:X ?
ADJA1 Akt # X1 ActNP ?
NE1 X2 # X1 X2TOP ?
NE1 letzter X2 # X1 Last X2At decoding time, the parse tree of the inputsentence is available to the decoder.
Decoratednon-terminals in rules must match the constituentspan in the input sentence but the undecorated Xsymbol can match any span.Formally, we model translation as a string-to-string translation using a synchronous CFGthat constrain the application of non-terminals tomatching source span labels.
The source wordsand span labels are represented as an unweightedword lattice, < V,E >, where each edge in thelattice correspond to a word or non-terminal labelover the corresponding source span.
In the softsyntax experiments, edges with the default sourcelabel, X , are also created for all spans.
Nodesin the lattice represent word positions in the sen-tence.We encode the lattice in a chart, as describedin (Dyer et al, 2008).
A chart is is a tuple of 2-dimensional matrices < F,R >.
Fi,j is the wordor non-terminal label of the jth transition startingword position i. Ri,j is the end word position ofthe node on the right of the jth transition leavingword position i.The input sentence is decoded with a set oftranslation rules of the formX ?< ?Ls, ?,?>where ?
and ?
and strings of terminals and non-terminals.
Ls and the string ?
are drawn from thesame source alphabet, ?s.
?
is the target string,also consisting of terminals and non-terminals.
?is the one-to-one correspondence between non-terminals in ?
and ?.
Ls is the left-hand-side ofthe source.
As a string-to-string model, the left-hand-side of the target is always the default targetnon-terminal label, X .Decoding follows the CKY+ algorithms whichprocess contiguous spans of the source sentencebottom up.
We describe the algorithm as inferencerules, below, omitting the target side for brevity.Initialization[X ?
?
?Ls, i, i](X ?
?Ls) ?
GTerminal Symbol[X ?
?
?
Fj,k?Ls, i, j][X ?
?Fj,k ?
?Ls, i, j + 1]Non-Terminal Symbol[X ?
?
?
Fj,k?Ls, i, j] [X, j,Rj,k][X ?
?Fj,k ?
?Ls, i, Rj,k]411Left Hand Side[X ?
?
?
Ls, i, Ri,j ] [Fi,j = Ls][X ?
?Ls?, i, Ri,j ]Goal[X ?
?Ls?, 0, |V | ?
1]This model allows translation rules to take ad-vantage of both syntactic label and word context.The presence of default label edges between everynode allows undecorated non-terminals to be ap-plied to any span, allowing flexibility in the trans-lation model.This contrasts with the approach by (Zollmannand Venugopal, 2006) in attempting to improve thecoverage of syntactic translation.
Rather than cre-ating ad-hoc schemes to categories non-terminalswith syntactic labels when they do not span syn-tactic constituencies, we only use labels that arepresented by the parser or shallow tagger.
Nor dowe try to expand the space where rules can ap-ply by propagating uncertainty from the parser inbuilding input forests, as in (Mi et al, 2008), butwe build ambiguity into the translation rule.The model also differs from (Marton andResnik, 2008; Chiang et al, 2008, 2009) by addinginformative labels to rule non-terminals and re-quiring them to match the source span label.
Thesoft constraint in our model pertain not to a ad-ditional feature functions based on syntactic infor-mation, but to the availability of syntactic and non-syntactic informed rules.4 ParameterizationIn common with most current SMT systems, thedecoding goal of finding the most probable targetlanguage sentence t?, given a source language sen-tence st?
= argmaxt p(t|s) (1)The argmax function defines the search objec-tive of the decoder.
We estimate p(t|s) by decom-posing it into component modelsp(t|s) =1Z?mh?m(t, s)?m (2)where h?m(t, s) is the feature function for compo-nent m and ?m is the weight given to componentm.
Z is a normalization factor which is ignored inpractice.
Components are translation model scor-ing functions, language model, and other features.The problem is typically presented in log-space,which simplifies computations, but otherwise doesnot change the problem due to the monotonicity ofthe log function (hm = log h?m)log p(t|s) =?m?m hm(t, s) (3)An advantage of our model over (Marton andResnik, 2008; Chiang et al, 2008, 2009) is thenumber of feature functions remains the same,therefore, the tuning algorithm does not need to bereplaced; we continue to use MERT (Och, 2003).5 Rule ExtractionRule extraction follows the algorithm described in(Chiang, 2005).
We note the heuristics used for hi-erarchical phrases extraction include the followingconstraints:1. all rules must be at least partially lexicalized,2.
non-terminals cannot be consecutive,3.
a maximum of two non-terminals per rule,4.
maximum source and target span width of 10word5.
maximum of 5 source symbolsIn the source syntax model, non-terminals are re-stricted to source spans that are syntactic phraseswhich severely limits the rules that can be ex-tracted or applied during decoding.
Therefore, wecan adapt the heuristics, dropping some of the con-straints, without introducing too much complexity.1.
consecutive non-terminals are allowed2.
a maximum of three non-terminals,3.
all non-terminals and LHS must span a parseconstituentIn the soft syntax model, we relax the constraintof requiring all non-terminals to span parse con-stituents.
Where there is no constituency spans,the default symbol X is used to denote an undeco-rated non-terminal.
This gives rise to rules whichmixes decorated and undecorated non-terminals.To maintain decoding speed and minimize spu-rious ambiguity, item (1) in the syntactic extrac-tion heuristics is adapted to prohibit consecutiveundecorated non-terminals.
This combines thestrength of syntactic rules but also gives the trans-lation model more flexibility and higher coveragefrom having undecorated non-terminals.
There-fore, the heuristics become:1. consecutive non-terminals are allowed, butconsecutive undecorated non-terminals areprohibited2.
a maximum of three non-terminals,3.
all non-terminals and LHS must span a parseconstituent4125.1 Rule probabilitiesMaximum likelihood phrase probabilities, p(?t|?s),are calculated for phrase pairs, using fractionalcounts as described in (Chiang, 2005).
The max-imum likelihood estimates are smoothed usingGood-Turing discounting (Foster et al, 2006).
Aphrase count feature function is also create foreach translation model, however, the lexical andbackward probabilities are not used.6 DecodingWe use the Moses implementation of the SCFG-based approach (Hoang et al, 2009) which sup-port hierarchical and syntactic training and decod-ing used in this paper.
The decoder implementsthe CKY+ algorithm with cube pruning, as well ashistogram and beam pruning, all pruning param-eters were identical for all experiments for fairercomparison.All non-terminals can cover a maximum of 7source words, similar to the maximum rule spanfeature other hierarchical decoders to speed up de-coding time.7 ExperimentsWe trained on the New Commentary 2009 cor-pus1, tuning on a hold-out set.
Table 1 gives moredetails on the corpus.
nc test2007 was used fortesting.German EnglishTrain Sentences 82,306Words 2,034,373 1,965,325Tune Sentences 2000Test Sentences 1026Table 1: Training, tuning, and test conditionsThe training corpus was cleaned and filtered us-ing standard methods found in the Moses toolkit(Koehn et al, 2007) and aligned using GIZA++(Och and Ney, 2003).
Standard MERT weight tun-ing was used throughout.
The English half of thetraining data was also used to create a trigram lan-guage model which was used for each experiment.All experiments use truecase data and results arereported in case-sensitive BLEU scores (Papineniet al, 2001).The German side was parsed with the Bitparparser2.
2042 sentences in the training corpusfailed to parse and were discarded from the train-ing for both hierarchical and syntactic models to1http://www.statmt.org/wmt09/2http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/BitPar.html# Model % BLEUUsing parse tree1 Hierarchical 15.92 Syntax rules 14.93 Joint hier.
+ syntax rules 16.14 Soft syntax rules 16.7Using chunk tags5 Hierarchical 16.36 Soft syntax 16.8Table 2: German?English results for hierarchicaland syntactic models, in %BLEUensure that train on identical amounts of data.Similarly, 991 out of 1026 sentences were parsablein the test set.
To compare like-for-like, the base-line translates the same 991 sentences, but evalu-ated over 1026 sentences.
(In the experiments withchunk tags below, all 1026 sentences are used).We use as a baseline the vanilla hierarchicalmodel which obtained a BLEU score of 15.9%(see Table 2, line 1).7.1 Syntactic translationUsing the na?
?ve translation model constrainedwith syntactic non-terminals significantly de-creases translation quality, Table 2, line 2.
Wethen ran hierarchical concurrently with the syntac-tic models, line 3, but see little improvement overthe hierarchical baseline.
However, we see a gainof 0.8% BLEU when using the soft syntax model.7.2 ReachabilityThe increased performance using the soft syn-tax model can be partially explained by studyingthe effect of changes to the extraction and decod-ing algorithms has to the capacity of the transla-tion pipeline.
We run some analysis in which wetrained the phrase models with a corpus of onesentence and attempt to decode the same sentence.Pruning and recombination were disabled duringdecoding to negate the effect of language modelcontext and model scores.The first thousand sentences of the training cor-pus was analyzed, Table 3.
The hierarchical modelsuccessfully decode over half of the sentenceswhile a translation model constrained by a sourcesyntax parse tree manages only 113 sentences, il-lustrating the severe degradation in coverage whena naive syntax model is used.Decoding with a hierarchical and syntax modeljointly (line 3) only decode one extra sentenceover the hierarchical model, suggesting that theexpressive power of the hierarchical model almost413# Model Reachable sentences1 Hierarchical 57.8%2 Syntax rules 11.3%3 Joint hier.
+ syntax rules 57.9%4 Soft syntax rules 58.5%Table 3: Reachability of 1000 training sentences:can they be translated with the model?Figure 2: Source span lengthscompletely subsumes that of the syntactic model.The MERT tuning adjust the weights so that thesyntactic model is very rarely applied during jointdecoding, suggesting that the tuning stage prefersthe broader coverage of the hierarchical modelover the precision of the syntactic model.However, the soft syntax model slightly in-creases the reachability of the target sentences,lines 4.7.3 Rule Span WidthThe soft syntactic model contains rules with threenon-terminals, as opposed to 2 in the hierarchicalmodel, and consecutive non-terminals in the hopethat the rules will have the context and linguisticinformation to apply over longer spans.
There-fore, it is surprising that when decoding with asoft syntactic grammar, significantly more wordsare translated singularly and the use of long span-ning rules is reduced, Figure 2.However, looking at the usage of the glue rulespaints a different picture.
There is significantlyless usage of the glue rules when decoding withthe soft syntax model, Figure 3.
The use ofthe glue rule indicates a failure of the translationmodel to explain the translation so the decreasein its usage is evidence of the better explanatorypower of the soft syntactic model.An example of an input sentence, and the besttranslation found by the hierarchical and soft syn-tax model can be seen in Table 4.
Figure 4 is theFigure 3: Length and count of glue rules used de-coding test setFigure 4: Example input parse treeparse tree given to the soft syntax model.Inputlaut Ja?nos Veres wa?re dies im ersten Quartal 2008mo?glich .Hierarchical outputaccording to Ja?nos Veres this in the first quarter of 2008would be possible .Soft Syntaxaccording to Ja?nos Veres this would be possible in thefirst quarter of 2008 .Table 4: Example input and best output foundBoth output are lexically identical but the outputof the hierarchical model needs to be reordered tobe grammatically correct.
Contrast the derivationsproduced by the hierarchical grammar, Figure 5,with that produced with the soft syntax model,Figure 6.
The soft syntax derivation makes useof several non-lexicalized to dictate word order,shown below.X ?
NE1 NE2 # X1 X2X ?
V AFIN1 PDS2 # X1 X2X ?
ADJA1 NN2 # X1 X2X ?
APPRART1 X2 CARD3 # X1 X2 X3X ?
PP1 X2 PUNC3 # X2 X1 X3414Figure 5: Derivation with Hierarchical modelFigure 6: Derivation with soft syntax modelThe soft syntax derivation include several ruleswhich are partially decorated.
Crucially, the lastrule in the list above reorders the PP phraseand the non-syntactic phrase X to generate thegrammatically correct output.
The other non-lexicalized rules monotonically concatenate theoutput.
This can be performed by the glue rule, butnevertheless, the use of empirically backed rulesallows the decoder to better compare hypotheses.The derivation also rely less on the glue rules thanthe hierarchical model (shown in solid rectangles).Reducing the maximum number of non-terminals per rule reduces translation quality butincreasing it has little effect on the soft syntaxmodel, Table 5.
This seems to indicate that non-terminals are useful as context when applyingrules up to a certain extent.7.4 English to GermanWe experimented with the reverse language direc-tion to see if the soft syntax model still increased# non-terms % BLEU2 16.53 16.85 16.8Table 5: Effect on %BLEU of varying number ofnon-terminals# Model % BLEU1 Hierarchical 10.22 Soft syntax 10.6Table 6: English?German results in %BLEUtranslation quality.
The results were positive butless pronounced, Table 6.7.5 Using Chunk TagsParse trees of the source language provide use-ful information that we have exploited to create abetter translation model.
However, parsers are anexpensive resource as they frequently need manu-ally annotated training treebanks.
Parse accuracyis also problematic and particularly brittle whengiven sentences not in the same domain as thetraining corpus.
This also causes some sentencesto be unparseable.
For example, our original testcorpus of 1026 sentences contained 35 unparsablesentences.
Thus, high quality parsers are unavail-able for many source languages of interest.Parse forests can be used to mitigate the accu-racy problem, allowing the decoder to choose frommany alternative parses, (Mi et al, 2008).The soft syntax translation model is not depen-dent on the linguistic information being in a treestructure, only that the labels identify contiguousspans.
Chunk taggers (Abney, 1991) does justthat.
They offer higher accuracy than syntacticparser, are not so brittle to out-of-domain data andidentify chunk phrases similar to parser-based syn-tactic phrases that may be useful in guiding re-ordering.We apply the soft syntax approach as in the pre-vious sections but replacing the use of parse con-stituents with chunk phrases.Figure 7: Chunked sentence4157.6 Experiments with Chunk TagsWe use the same data as described earlier inthis chapter to train, tune and test our approach.The Treetagger chunker (Schmidt and Schulte imWalde, 2000) was used to tag the source (German)side of the corpus.
The chunker successfully pro-cessed all sentences in the training and test datasetso no sentences were excluded.
The increase train-ing data, as well as the ability to translate all sen-tences in the test set, explains the higher hierar-chical baseline than the previous experiments withparser data.
We use the noun, verb and preposi-tional chunks, as well as part-of-speech tags, emit-ted by the chunker.Results are shown in Table 2, line 5 & 6.
Usingchunk tags, we see a modest gain of 0.5% BLEU.The same example sentence in Table 4 is shownwith chunk tags in Figure 7.
The soft syntaxmodel with chunk tags produced the derivationtree shown in Figure 8.
The derivation make useof an unlexicalized rule local reordering.
In thisexample, it uses the same number of glue rule asthe hierarchical derivation but the output is gram-matically correct.Figure 8: Translated chunked sentenceHowever, overall, the number of glue rules usedshows the same reduction that we saw using softsyntax in the earlier section, as can be seen in Fig-ure 9.
Again, the soft syntax model, this time us-ing chunk tags, is able to reduce the use of the gluerule with empirically informed rules.8 ConclusionWe show in this paper that combining the gener-ality of the hierarchical approach with the speci-ficity of syntactic approach can improve transla-Figure 9: Chunk - Length and count of glue rulesused decoding test settion.
A reason for the improvement is the bet-ter long-range reordering made possible by the in-crease capacity of the translation model.Future work in this direction includes us-ing tree-to-tree approaches, automatically createdconstituency labels, and back-off methods be-tween decorated and undecorated rules.9 AcknowledgementThis work was supported in part by the EuroMa-trixPlus project funded by the European Commis-sion (7th Framework Programme) and in part un-der the GALE program of the Defense AdvancedResearch Projects Agency, Contract No.
HR0011-06-C-0022.ReferencesAbney, S. (1991).
Parsing by chunks.
In Robert Berwick,Steven Abney, and Carol Tenny: Principle-Based Parsing.Kluwer Academic Publishers.Ambati, V. and Lavie, A.
(2008).
Improving syntax driventranslation models by re-structuring divergent and non-isomorphic parse tree structures.
In AMTA.Ambati, V., Lavie, A., and Carbonell, J.
(2009).
Extraction ofsyntactic translation models from parallel data using syn-tax from source and target languages.
In MT Summit.Chiang, D. (2005).
A hierarchical phrase-based model forstatistical machine translation.
In Proceedings of the 43rdAnnual Meeting of the Association for Computational Lin-guistics (ACL?05), pages 263?270, Ann Arbor, Michigan.Association for Computational Linguistics.Chiang, D., Knight, K., and Wang, W. (2009).
11,001 newfeatures for statistical machine translation.
In Proceedingsof Human Language Technologies: The 2009 Annual Con-ference of the North American Chapter of the Associationfor Computational Linguistics, pages 218?226, Boulder,Colorado.
Association for Computational Linguistics.Chiang, D., Marton, Y., and Resnik, P. (2008).
Online large-margin training of syntactic and structural translation fea-tures.
In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages 224?233, Honolulu, Hawaii.
Association for ComputationalLinguistics.Dyer, C., Muresan, S., and Resnik, P. (2008).
Generalizingword lattice translation.
In Proceedings of ACL-08: HLT,pages 1012?1020, Columbus, Ohio.
Association for Com-putational Linguistics.416Foster, G., Kuhn, R., and Johnson, H. (2006).
Phrasetablesmoothing for statistical machine translation.
In Proceed-ings of the 2006 Conference on Empirical Methods in Nat-ural Language Processing, pages 53?61, Sydney, Aus-tralia.
Association for Computational Linguistics.Galley, M., Graehl, J., Knight, K., Marcu, D., DeNeefe, S.,Wang, W., and Thayer, I.
(2006).
Scalable inference andtraining of context-rich syntactic translation models.
InProceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting of the As-sociation for Computational Linguistics, pages 961?968,Sydney, Australia.
Association for Computational Lin-guistics.Galley, M., Hopkins, M., Knight, K., and Marcu, D. (2004).What?s in a translation rule?
In Proceedings of the JointConference on Human Language Technologies and theAnnual Meeting of the North American Chapter of the As-sociation of Computational Linguistics (HLT-NAACL).Hoang, H., Koehn, P., and Lopez, A.
(2009).
A UnifiedFramework for Phrase-Based, Hierarchical, and Syntax-Based Statistical Machine Translation.
In Proc.
of theInternational Workshop on Spoken Language Translation,pages 152?159, Tokyo, Japan.Huang, L. and Chiang, D. (2008).
Forest-based translationrule extraction.
In EMNLP, Honolulu, Hawaii.Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Fed-erico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C.,Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst,E.
(2007).
Moses: Open source toolkit for statistical ma-chine translation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and Poster Ses-sions, pages 177?180, Prague, Czech Republic.
Associa-tion for Computational Linguistics.Li, Z., Callison-Burch, C., Dyer, C., Khudanpur, S.,Schwartz, L., Thornton, W., Weese, J., and Zaidan, O.(2009).
Joshua: An open source toolkit for parsing-basedmachine translation.
In Proceedings of the Fourth Work-shop on Statistical Machine Translation, pages 135?139,Athens, Greece.
Association for Computational Linguis-tics.Liu, Y., Mi, H., Feng, Y., and Liu, Q.
(2009).
Joint decod-ing with multiple translation models.
In In Proceedings ofACL/IJCNLP 2009, pages 576?584, Singapore.Marton, Y. and Resnik, P. (2008).
Soft syntactic constraintsfor hierarchical phrased-based translation.
In Proceedingsof ACL-08: HLT, pages 1003?1011, Columbus, Ohio.
As-sociation for Computational Linguistics.Mi, H., Huang, L., and Liu, Q.
(2008).
Forest-based trans-lation.
In Proceedings of ACL-08: HLT, pages 192?199,Columbus, Ohio.
Association for Computational Linguis-tics.Och, F. J.
(2003).
Minimum error rate training for statisticalmachine translation.
In Proceedings of the 41st AnnualMeeting of the Association of Computational Linguistics(ACL).Och, F. J. and Ney, H. (2003).
A systematic comparison ofvarious statistical alignment models.
Computational Lin-guistics, 29(1):19?52.Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2001).BLEU: a method for automatic evaluation of machinetranslation.
Technical Report RC22176(W0109-022),IBM Research Report.Schmidt, H. and Schulte im Walde, S. (2000).
RobustGerman noun chunking with a probabilistic context-freegrammar.
In Proceedings of the International Conferenceon Computational Linguistics (COLING).Shen, L., Xu, J., Zhang, B., Matsoukas, S., and Weischedel,R.
(2009).
Effective use of linguistic and contextual infor-mation for statistical machine translation.
In Proceedingsof the 2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 72?80, Singapore.
Associa-tion for Computational Linguistics.Venugopal, A., Zollmann, A., Smith, N. A., and Vogel, S.(2009).
Preference grammars: Softening syntactic con-straints to improve statistical machine translation.
In Pro-ceedings of Human Language Technologies: The 2009 An-nual Conference of the North American Chapter of the As-sociation for Computational Linguistics, pages 236?244,Boulder, Colorado.
Association for Computational Lin-guistics.Zollmann, A. and Venugopal, A.
(2006).
Syntax augmentedmachine translation via chart parsing.
In Proceedings onthe Workshop on Statistical Machine Translation, pages138?141, New York City.
Association for ComputationalLinguistics.417
