Proceedings of the BioNLP Shared Task 2013 Workshop, pages 188?196,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsIRISA participation to BioNLP-ST 2013: lazy-learning and informationretrieval for information extraction tasksVincent ClaveauIRISA ?
CNRSCampus de Beaulieu, 35042 Rennes, Francevincent.claveau@irisa.frAbstractThis paper describes the informationextraction techniques developed in theframework of the participation of IRISA-TexMex to the following BioNLP-ST13tasks: Bacterial Biotope subtasks 1 and2, and Graph Regulation Network.
Theapproaches developed are general-purposeones and do not rely on specialized pre-processing, nor specialized external data,and they are expected to work indepen-dently of the domain of the texts pro-cessed.
They are classically based on ma-chine learning techniques, but we put theemphasis on the use of similarity mea-sures inherited from the information re-trieval domain (Okapi-BM25 (Robertsonet al 1998), language modeling (Hiem-stra, 1998)).
Through the good results ob-tained for these tasks, we show that thesesimple settings are competitive providedthat the representation and similarity cho-sen are well suited for the task.1 IntroductionThis paper describes the information extractiontechniques developed in the framework of theparticipation of IRISA-TexMex to BioNLP-ST13.For this first participation, we submitted runs forthree tasks, concerning entity detection and cat-egorization (Bacterial Biotope subtask 1, BB1),and relation detection and categorization (Bacte-rial Biotope subtask 2, BB2, and Graph Regula-tion Network, GRN).Our participation to the BioNLP shared taskstakes place in the broader context of our workin the Quaero research program1 in which weaim at developing fine grained indexing tools for1See www.quaero.org for a complete overview of thislarge research project.multimedia content.
Text-mining and informationextraction problems are thus important issues toreach this goal.
In this context, the approaches thatwe develop are general-purpose ones, that is, theyare not designed for a specific domain such as Bi-ology, Medecine, Genetics or Proteomics.
There-fore, the approaches presented in this paper do notrely on specialized pre-processing, nor specializedexternal data, and they are expected to work inde-pendently of the domain of the texts processed.The remaining of this paper is structured as fol-lows: the next section presents general insightson the methodology used throughout our partici-pation, whatever the task.
Sections 3, 4 and 5 re-spectively describe the techniques developed andtheir results for BB1, BB2 and GRN.
Last, someconclusive remarks and perspectives are given inSection 6.2 Methodological corpusFrom a methodological point of view, our ap-proaches used for these tasks are machine learn-ing ones.
Indeed, since the first approaches of in-formation extraction based on the definition of ex-traction patterns (Riloff, 1996; Soderland, 1999),using surface clues or syntactic and semantic in-formation (Miller et al 2000), machine learningtechniques have shown high performance and ver-satility.
Generally, the task is seen as a super-vised classification one: the training data are usedto infer a classifier able to handle new, unlabeleddata.
Most of the state-of-the-art techniques adoptthis framework, but differ in the kind of infor-mation used and on the way to use it.
For in-stance, concerning the syntactic information, dif-ferent representations were studied: sequences orsub-sequences (Culotta et al 2006; Bunescu andMooney, 2006), shallow parsing (Pustejovsky etal., 2002; Zelenko et al 2003), dependencies(Manine et al 2009), trees (Zhang et al 2006;Liu et al 2007), graphs (Culotta and Sorensen,1882004; Fundel et al 2007), etc.
Also, exploitingsemantic information, for instance through dis-tributional analysis seems promising (Sun et al2011).The approaches also differ in the inference tech-niques used.
Many were explored, like neural net-works (Barnickel et al 2009) or logistic regres-sion (Mintz et al 2009), but those relying on ametric space search, such as Support Vector Ma-chines (SVM) or k-Nearest Neighbours (kNN) areknown to achieve state-of-the-art results (Zelenkoet al 2003; Culotta and Sorensen, 2004).
Thecrux of the matter for these methods is to devisea good metric between the objects, that is, a goodkernel.
For instance, string kernels (Lodhi et al2002) or graph kernels (Tikk et al 2012) haveshown interesting performance.Our approaches in these shared tasks also adoptthis general framework.
In particular, they arechiefly based on simple machine learning tech-niques, such as kNN.
In this classification tech-nique, new instances whose classes are unknownare compared with training ones (instances withknown classes).
Among the latter, the closest onesin the feature space are used to decide the class ofthe new instance, usually by a majority vote.
Be-yond the apparent simplicity of this machine learn-ing technique, the heart of the problem relies in thetwo following points:?
using a relevant distance or similarity mea-sure in the feature space to compare the in-stances;?
finding the best voting process (number ofnearest neighbors, voting modalities...)There is no real training step per se, but kNN istruly a machine learning approach since the in-ductive step is made when computing the simi-larity and the vote for the classification of a newinstance, hence the qualification of ?lazy-learning?method.In our work, we explore the use of similaritymeasures inherited from the information retrieval(IR) domain.
Indeed, IR has a long history whenit comes to comparing textual elements (Rao etal., 2011) which may offer new similarity mea-sures for information extraction either for kernel-based methods or, in our case, for kNN.
There-fore, in the remaining of the article, we mainly de-scribe the choice of this similarity measure, andadopt the standard notation used in IR to denote asimilarity function: RSV (Retrieval Status Value,higher score denotes higher similarity).
In prac-tice, all the algorithms and tools were developedin Python, using NLTK (Loper and Bird, 2002) forbasic pre-processing.3 Term extraction and categorization:Bacteria Biotope sub-task 1This section describes our participation to sub-task 1 of the Bacteria Biotope track.
The firstsub-section presents the task as we interpretedit, which explains some conceptual choices ofour approach.
The latter is then detailed (sub-section 3.2) and its results are reported (sub-section 3.3).3.1 Task interpretationThis tasks aims at detecting and categorizing en-tities based on an ontology.
This task has someimportant characteristics:?
it has an important number of categories;?
categories are hierarchically organized;?
few examples for each categories are giventhrough the ontology and the examples.Moreover, some facts are observed in the trainingdata:?
entities are mostly noun phrase;?
most of the entities appear in a form veryclose to their corresponding ontology entry.Based on all these considerations and to ourpoint of view explained in the previous section,this task is interpreted as an automatic categoriza-tion one: a candidate (portion of the analyzed text)is assigned an ontological category or a negativeclass (stating) that the candidate does not belongto any spotted category.In the state-of-the-art, such problems are oftenconsidered as labeling ones for which stochastictechniques like HMM, MaxEnt models, or morerecently CRF (Lafferty et al 2001), have shownvery good results in a large variety of tasks (Wanget al 2006; Pranjal et al 2006, inter alia).
Yet,for this specific case, these techniques do not seemto be fully suited for different reasons:?
a very high number of possible classes isto be handled, which may cause complexityproblems;?
the size of the training set is relatively smallcompared to the size of the label set;189?
embedding external knowledge (i.e.
the on-tology), for instance as features, cannot bedone easily.On the contrary of these stochastic methods, ourapproach does not rely on the sequential aspect ofthe problem.
It is based on lazy machine learn-ing (kNN), detailed hereafter, with a descriptionallowing us to make the most of the ontology andthe annotated texts as training data.3.2 ApproachIn the approach developed for this task, the contextof a candidate is not taken into account.
We onlyrely on the internal components (word-forms) ofthe candidate to decide whether it is an entity andwhat is its category.
That is why both the ontologyand the given annotated texts are equally consid-ered as training data.More precisely, this approach is implemented intwo steps.
In the first step, the texts are searchedfor almost exact occurrences of an ontology entry.Slight variations are allowed, such as case, wordinsertion and singular/plural forms.
In practice,this approach is implemented with simple regu-lar expressions automatically constructed from theontology and the annotated texts.In the second step, a more complex processingis undergone in order to retrieve more entities (toimprove the recall rate).
It relies on a 1-nearestneighbor classification of the noun phrase (NP) ex-tracted from the text.
A NP chunker is built bytraining a MaxEnt model from the CONLL 2000shared task dataset (articles from the Wall StreetJournal corpus).
This NP chunker is first appliedon the training data.
All NP collected that donot belong to any wanted ontological categoriesare kept as examples of a negative class.
The NPchunker is then applied to the test data.
Each ex-tracted NP is considered as a candidate which iscompared with the ontological entries and the col-lected negative noun phrases.
This candidate fi-nally receives the same class than the closest NP(i.e.
the ontological category identifier or the neg-ative class).As explained in the previous section, the key-stone of such an approach is to devise an effi-cient similarity measure.
In order to retrieve theclosest known NP, we examine the word-formscomposing the candidate, considered as a bag-of-words.
An analogy is thus made with informationretrieval: ontological categories are considered asdocuments, and the candidate is considered as aquery.
A similarity measure inherited from infor-mation retrieval, called Okapi-BM25 (Robertsonet al 1998), is used.
It can be seen as a modernvariant of TF-IDF/cosine similarity, as detailed inEqn.
1 where t is a term occurring qtf times in thecandidate q, c a category (in which the term t oc-curs tf times), k1 = 2, k3 = 1000 and b = 0.75are constants, df is the document frquency (num-ber of categories in which t appears), dl is the doc-ument length, that is, in our case the number ofwords of the terms in that category, dlavg is the av-erage length (number of words) of a category.RSV (q, c) =?t?qqTF (t) ?
TF (t, c) ?
IDF (t)(1)with:qTF (t) =(k3 + 1) ?
qtfk3 + qtfTF (t, c) =tf ?
(k1 + 1)tf + k1 ?
(1?
b+ b ?
dl(c)/dlavg)IDF (t) = logN ?
df(t) + 0.5df(t) + 0.5Finally, the category c?
for the candidate q ischosen among the set C of all the possible ones(including the negative category), such that:c?
= argmaxc?CRSV (q, c)The whole approach is illustrated in Fig.
1.Still in order to improve recall, unknown words(words that do not appear in any category) undergoan additional process.
The definition of the wordin WordNet, if present, is used to extend the candi-date, in a very similar way to what would be queryexpansion (Voorhees, 1998).
In case of polyse-mous words, the first definition is used.3.3 ResultsFigure 2 presents the official results of the partic-ipating teams on the test dataset.
Our approachobtains good overall performance compared withother team?s results and ranks first in terms of SlotError Rate (SER, combining the number of substi-tution S, insertion I, deletion D and Matches M).As it appears, this is mainly due to a better recallrate.
Of course, this improved recall has its draw-back: the precision of our approach is a bit lowerthan some of the other teams.
This is confirmed190Figure 1: k-NN based approach based on IR similarity measuresFigure 2: BB1 official results: global performance rates (left); error analysis (right)by the general shape of our technique comparedwith others?
(Figure 2, right) with more matches,but also more insertions.In order to analyze the performance of eachcomponent, we also report results of step 1 (quasi-exact matches with regular expression) alone, step2 alone, and a study of the influence of usingWordNet to extend the candidate.
The resultsof these different settings, on the developmentdataset, are given in Figure 3 From these results,the first point worth noting is the difference ofoverall performance between the development setand the test set (SER on the latter is almost twotimes higher than on the former).
Yet, without ac-cess to the test set, a thorough analysis of this phe-nomenon cannot be undergone.
Another strikingpoint is the very good performance of step 1, thatFigure 3: Influence of each extraction component191is, the simple search for quasi identical ontologyphrases in the text.
Compared to this, step 2 per-forms worse, with many false negatives (deletions)and misclassifications (substitutions).
A close ex-amination of the causes of these errors reveals thatthe IR-based classification process is not at fault,but it is misled by wrong candidates proposed bythe NP chunker.
Besides the problem of perfor-mance of our chunker, it also underlines the limitof our hypothesis of using only noun phrases aspossible candidates.
In spite of these problems,step 2 provides complementary predictions to step1, as their combination obtains better results thaneach one.
This is also the case with the WordNet-based expansion, which brings slightly better re-sults.4 Extracting relation: Bacteria Biotopesub-task 2This section is dedicated to the presentation of ourparticipation to Bacteria Biotope sub-task 2.
Asfor the sub-task 1, we first present the task as weinterpreted it, then the approach, and last some re-sults.4.1 Task interpretationThis task aims at extracting and categorizing local-ization and part-of relations that may be reportedin scientific abstracts between Bacteria, Habitatand Geographical spots.
For this particular sub-task, the entities (boundaries in the text and type)were provided.As explained in Section 2, expert approachesbased on hand-coded patterns are outperformed bystate-of-the-art studies which consider this kind oftasks as a classification one.
Training data helpto infer a classifier able to decide, based on fea-tures extracted from the text, whether two entitiesshare a relation, and able to label this relation ifneeded.
We also adopt this framework and ex-ploit a system developed in-house (Ebadat, 2011)which has shown very good performance on theprotein-protein-interaction task of the LLL dataset(N?dellec, 2005).
From a computational point ofview, two directed relations are to be consideredfor this task, plus the ?negative?
relation statingthat no localization or part-of relation exists be-tween the entities.
Therefore, the classifier has tohandle five labels.4.2 ApproachThe extraction method used for this task onlyexploits shallow linguistic information, which iseasy to obtain and ensures the necessary robust-ness, while providing good results on previoustasks (Ebadat, 2011).
One of its main interests is totake into account the sequential aspect of the taskwith the help of n-gram language models.
Thus, arelation is represented by the sequence of lemmasoccurring between the agent and the target, if theagent occurs before the target, or between the tar-get and the agent otherwise.
A language model isbuilt for each example Ex, that is, the probabili-ties based on the occurrences of n-grams in Ex arecomputed; this language model is written MEx.The class (including the ?negative?
class) and di-rection (left-to-right, LTR or right-to-left, RTL) ofeach example is also memorized.Given a relation candidate (that is, two proteinsor genes in a sentence), it is possible to evaluateits proximity with any example, or more preciselythe probability that this example has generated thecandidate.
Let us note C =< w1, w2, ..., wm >the sequence of lemmas between the proteins.
Forn-grams of n lemmas, this probability is classi-cally computed as:P (C|MEx) =m?i=1P (wi|wi?n..wi?1,MEx)As for any language model in practice, probabil-ities are smoothed in order to prevent unseen n-grams to yield 0 for the whole sequence.
In theexperiments reported below, we consider bigramsof lemmas.
Different strategies for smoothing areused: as it is done in language modeling for IR(Hiemstra, 1998), probabilities estimated from theexample are linearly combined with those com-puted on the whole set of example for this class.In case of unknown n-grams, an interpolation withlower order n-grams (unigram in this case) com-bined with an absolute discounting (Ney et al1994) is performed.In order to prevent examples with long se-quences to be favored, the probability of generat-ing the example from the candidate (P (Ex|MC))is also taken into account.
Finally, the similaritybetween an example and a candidate is:RSV (Ex,C) = min (P (Ex|MC), P (C|MEx))The class is finally attributed to the candidate bya k-nearest neighbor algorithm: the k most sim-192Figure 4: BB2 official results in terms of recall,precision and F-scoreilar examples (highest RSV ) are calculated anda majority vote is performed.
For this task, kwas set to 10 according to cross-validation experi-ments.
This lazy-learning technique is expected tobe more suited to this kind of tasks than the model-based ones (such as SVM) proposed in the litera-ture since it better takes into account the variety ofways to express a relation.4.3 ResultsThe official results are presented in Figure 4.
Interms of F-score, our team ranks close second, butwith a different recall/precision compromise thanTEES-2.1.
The detailed results provided by theorganizers show that no Part-of relations are re-trieved.
From the analysis of errors on the devel-opment set, it appears that the simplicity of ourrepresentation is at fault in most cases of misclas-sifications.
Indeed, important keywords frequentlyoccur outside of the sub-sequence delimited by thetwo entities.
The use of syntactic information, asproposed for the GRN task in the next section, isexpected to help overcome this problem.5 Extracting relation: regulationnetwork5.1 Task interpretation and approachDespite the different application context and thedifferent evaluation framework, we consider thisrelation extraction task in a similar way than in theprevious section.
Therefore, we use the same ap-proach already described in Section 4.2.
Yet, in-stead of using the sequence of lemmas betweenthe entities, we rely on the sequence built from theFigure 5: Example of syntactic representationused for the GRN taskFigure 6: GRN official results in terms of strictSlot Error Rate (SER), recall, precision and F-scoreshortest syntactic path between the entities as it isdone in many studies (Manine et al 2009, interalia).
The text is thus parsed with MALT parser(Nivre, 2008) and its pre-trained Penn Treebankmodel (Marcus et al 1993).
The lemmas occur-ring along the syntactic path between the entities,from the source to the target, are collected as illus-trated in Figure 5.5.2 ResultsThe official results reported in Fig.
6 shows that al-though our approach only ranks fourth in terms ofSlot Error Rate (SER), its general performance iscompetitive in terms of Recall and F-score, but itsrelatively lower precision impacts the global SERscore.
It is also interesting to consider a relaxedversion of these evaluation measures in which sub-193Figure 7: GRN official results in terms of re-laxed Slot Error Rate (SER), recall, precision andF-scoreFigure 8: Analysis of errors of the GRN taskstitutions are not penalized.
It therefore evalu-ates the ability of the methods to build the regu-lation network whatever the real relation betweenentities.
As it appears in Figure 7, in that case,our approach brings the best results in terms of F-score and SER.
As for the BB2 task, it means thatthe pro-eminent errors are between labels of validrelations, but not on the validity of the relation.This is also noticeable in Figure 8 in which theglobal profile of our approach underlines its ca-pacity to retrieve more relations, but also to gener-ate more substitution and insertion errors than theother approaches.
The complete causes of thesemisclassifications are still to be investigated, but aclose examination of the results shows two possi-ble causes:?
the parser makes many mistakes on con-junction and prepositional attachment, whichis especially harmful for the long sentencesused in the dataset;?
our representation omits to include negationor important adverbs, which by definition arenot part of the shortest path, but are essentialto correctly characterize the relation.The first cause is not specific to these data and is awell-known problem of parsing, but hard to over-come at our level.
The second cause is specific toour approach, and militate, to some extents, to de-vise a more complex representation than the short-est path one.6 Conclusion and future workFor this first participation of IRISA to BioNLPshared tasks, simple models were implemented,using no domain-specific knowledge.
Accordingto the task, these models obtained more or lessgood rankings, but all have been shown to be com-petitive with other teams?
results.
Our approachesput the emphasis on the similarity computing be-tween known instances instead of complex ma-chine learning techniques.
By making analogieswith information retrieval, this similarity aims atbeing the most relevant for the considered task andat finding the closest known examples of any newinstance to be classified.For instance, we made the most of the vector-space measure Okapi-BM25 combined with a bag-of-word representation for the first sub-task ofBacterial Biotope, and of the language modelingadapted from (Hiemstra, 1998) for the sequentialrepresentation used in the second sub-task of Bac-terial Biotope and for Gene Regulation Network.Many parameters as well as other similaritychoices have not been explored due to the short de-lay imposed by the challenge schedule.
As a futurework, it would be interesting to automatically setthese parameters according to the data.
In partic-ular, a complex version of the BM-25 RSV func-tion permits to include relevance feedback, which,in our machine learning framework, correspondsto using training data to adapt the BM-25 for-mula.
Another research avenue concerns the syn-onymy/paraphrasing problem, which is not cor-rectly handled by our word-based methods.
Thus,semantic analysis techniques used in IR (and otherNLP domains) such as Latent Semantic Indexing(Deerwester et al 1990) or Latent Dirichlet Allo-cation (Blei et al 2003) may also lead to interest-ing results.AcknowledgmentThis work was partly funded by OSEO, the Frenchagency for innovation, in the framework of the194Quaero project.References[Barnickel et al009] T. Barnickel, J. Weston, R. Col-lobert, H.W.
Mewes, and V. St?mpflen.
2009.
Largescale application of neural network based semanticrole labeling for automated relation extraction frombiomedical texts.
PloS One, 4(7).
[Blei et al003] David M. Blei, Andrew Y. Ng, andMichael I. Jordan.
2003.
Latent dirichlet al-cation.
Journal of Machine Learning Research,3:993?1022, March.
[Bunescu and Mooney2006] R. Bunescu andR.
Mooney.
2006.
Subsequence kernels forrelation extraction.
Advances in Neural InformationProcessing Systems, 18.
[Culotta and Sorensen2004] A. Culotta and J. Sorensen.2004.
Dependency tree kernels for relation extrac-tion.
In Proceedings of the 42nd Annual Meeting onAssociation for Computational Linguistics.
[Culotta et al006] A. Culotta, A. McCallum, andJ.
Betz.
2006.
Integrating probabilistic extractionmodels and data mining to discover relations andpatterns in text.
In Proceedings of the main confer-ence on Human Language Technology Conferenceof the North American Chapter of the Association ofComputational Linguistics, pages 296?303.
[Deerwester et al990] Scott Deerwester, Susan T. Du-mais, George W. Furnas, Thomas K. Landauer, andRichard Harshman.
1990.
Indexing by latent se-mantic analysis.
Journal of the American Societyfor Information Science, 41(6):391?407.
[Ebadat2011] Ali Reza Ebadat.
2011.
Extract-ing protein-protein interactions with language mod-elling.
In Proceeding of the RANLP Student Re-search Workshop, pages 60?66, Hissar, Bulgaria.
[Fundel et al007] K. Fundel, R. K?ffner, and R. Zim-mer.
2007.
Relex ?
relation extraction using depen-dency parse trees.
Bioinformatics, 23(3):365?371.
[Hiemstra1998] D. Hiemstra.
1998.
A linguisticallymotivated probabilistic model of information re-trieval.
In Proc.
of European Conference on DigitalLibraries, ECDL, Heraklion, Greece.
[Lafferty et al001] J. Lafferty, A. McCallum, andF.
Pereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In International Conference on Ma-chine Learning (ICML).
[Liu et al007] Y. Liu, Z. Shi, and A. Sarkar.
2007.Exploiting rich syntactic information for relationextraction from biomedical articles.
In HumanLanguage Technologies 2007: Conf.
North Ameri-can Chapter of the Association for ComputationalLinguistics; Companion Volume (NAACL-Short?07),pages 97?100.
[Lodhi et al002] H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins.
2002.
Textclassification using string kernels.
Journal of Ma-chine Learning Research, 2:419?444.
[Loper and Bird2002] Edward Loper and Steven Bird.2002.
Nltk: the natural language toolkit.
In Pro-ceedings of the ACL-02 Workshop on Effective toolsand methodologies for teaching natural languageprocessing and computational linguistics - Volume1, ETMTNLP ?02, pages 63?70, Stroudsburg, PA,USA.
Association for Computational Linguistics.
[Manine et al009] A.-P. Manine, E. Alphonse, andP.
Bessi?res.
2009.
Learning ontological rulesto extract multiple relations of genic interactionsfrom text.
Int.
Journal of Medical Informatics,78(12):31?38.
[Marcus et al993] Mitchell P. Marcus, Mary AnnMarcinkiewicz, and Beatrice Santorini.
1993.Building a large annotated corpus of english:the penn treebank.
Computational Linguistics,19(2):313?330, June.
[Miller et al000] S. Miller, H. Fox, L. Ramswhaw,and R. Weischedel.
2000.
A novel use of statisticalparsing to extract information from text.
In Proc.1st North American Chapter of the Association forComputational Linguistics Conf., pages 226?233.
[Mintz et al009] M. Mintz, S. Bills, R. Snow, andD.
Jurafsky.
2009.
Distant supervision for rela-tion extraction without labeled data.
In Proc.
JointConf.
47th Annual Meeting of the ACL and the 4thInt.
Joint Conf.
on Natural Language Processing ofthe AFNLP.
[Ney et al994] Hermann Ney, Ute Essen, and Rein-hard Kneser.
1994.
On structuring probabilistic de-pendencies in stochastic language modelling.
Com-puter Speech and Language, 8:1?38.
[Nivre2008] Joakim Nivre.
2008.
Algorithms for de-terministic incremental dependency parsing.
Com-putational Linguistics, 34(4):513?553.
[N?dellec2005] Claire N?dellec, editor.
2005.
Learn-ing language in logic ?
Genic interaction extractionchallenge, in Proc.
of the 4th Learning Language inLogic Workshop (LLL?05), Bonn, Germany.
[Pranjal et al006] Awasthi Pranjal, Rao Delip, andRavindran Balaraman.
2006.
Part of speech taggingand chunking with hmm and crf.
In Proceedings ofNLP Association of India (NLPAI) Machine Learn-ing Contest.
[Pustejovsky et al002] J. Pustejovsky, J. Castano,J.
Zhang, M. Kotecki, and B. Cochran.
2002.
Ro-bust relational parsing over biomedical literature:Extracting inhibit relations.
In Proceedings of thePacific Symposium in Biocomputing, pages 362?373.195[Rao et al011] Delip Rao, Paul McNamee, and MarkDredze.
2011.
Entity linking: Finding extracted en-tities in a knowledge base.
In Multi-source, Multi-lingual Information Extraction and Summarization.
[Riloff1996] E. Riloff.
1996.
Automatically generat-ing extraction patterns form untagged text.
In Proc.13th Natl.
Conf.
on Artificial Intelligence (AAAI-96),pages 1044?1049.
[Robertson et al998] Stephen E. Robertson, SteveWalker, and Micheline Hancock-Beaulieu.
1998.Okapi at TREC-7: Automatic Ad Hoc, Filtering,VLC and Interactive.
In Proceedings of the 7th TextRetrieval Conference, TREC-7, pages 199?210.
[Soderland1999] S. Soderland.
1999.
Learning infor-mation extraction rules for semi-structured and freetext.
Machine Learning Journal, 34(1-3):233?272.
[Sun et al011] A.
Sun, R. Grishman, and S. Sekine.2011.
Semi-supervised relation extraction withlarge-scale word clustering.
In Proc.
49th AnnualMeeting of the Association for Computational Lin-guistics, pages 521?529.
[Tikk et al012] D. Tikk, P. Thomas, P. Palaga, J. Hak-enberg, and U. Leser.
2012.
A comprehensivebenchmark of kernel methods to extract protein-protein interactions from literature.
PLoS Comput-ing Biology, 6(7).
[Voorhees1998] E. Voorhees, 1998.
C. Fellbaum (ed.
),WORDNET: An Electronic Lexical Database, chap-ter Using WORDNET for Text Retrieval, pages285?303.
The MIT Press.
[Wang et al006] Tao Wang, Jianguo Li, Qian Diao,Yimin Zhang Wei Hu, and Carole Dulong.
2006.Semantic event detection using conditional randomfields.
In IEEE Conference on Computer Vision andPattern Recognition Workshop (CVPRW ?06).
[Zelenko et al003] D. Zelenko, C. Aone, andA.
Richardella.
2003.
Kernel methods for relationextraction.
Journal of Machine Learning Research,3:1083?1106.
[Zhang et al006] M. Zhang, J. Zhang, and J. Su.2006.
Exploring syntactic features for relation ex-traction using a convolution tree kernel.
In Pro-ceedings of the main conference on Human Lan-guage Technology Conference of the North Amer-ican Chapter of the Association of ComputationalLinguistics, pages 288?295.196
