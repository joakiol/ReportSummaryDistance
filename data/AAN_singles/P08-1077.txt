Proceedings of ACL-08: HLT, pages 674?682,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsLarge Scale Acquisition of Paraphrases for Learning Surface PatternsRahul Bhagat?Information Sciences InstituteUniversity of Southern CaliforniaMarina del Rey, CArahul@isi.eduDeepak RavichandranGoogle Inc.1600 Amphitheatre ParkwayMountain View, CAdeepakr@google.comAbstractParaphrases have proved to be useful in manyapplications, including Machine Translation,Question Answering, Summarization, and In-formation Retrieval.
Paraphrase acquisitionmethods that use a single monolingual corpusoften produce only syntactic paraphrases.
Wepresent a method for obtaining surface para-phrases, using a 150GB (25 billion words)monolingual corpus.
Our method achieves anaccuracy of around 70% on the paraphrase ac-quisition task.
We further show that we canuse these paraphrases to generate surface pat-terns for relation extraction.
Our patterns aremuch more precise than those obtained by us-ing a state of the art baseline and can extractrelations with more than 80% precision foreach of the test relations.1 IntroductionParaphrases are textual expressions that convey thesame meaning using different surface words.
For ex-ample consider the following sentences:Google acquired YouTube.
(1)Google completed the acquisition of YouTube.
(2)Since they convey the same meaning, sentences(1) and (2) are sentence level paraphrases, and thephrases ?acquired?
and ?completed the acquisitionof ?
in (1) and (2) respectively are phrasal para-phrases.Paraphrases provide a way to capture the vari-ability of language and hence play an important?Work done during an internship at Google Inc.role in many natural language processing (NLP) ap-plications.
For example, in question answering,paraphrases have been used to find multiple pat-terns that pinpoint the same answer (Ravichandranand Hovy, 2002); in statistical machine transla-tion, they have been used to find translations forunseen source language phrases (Callison-Burch etal., 2006); in multi-document summarization, theyhave been used to identify phrases from differentsentences that express the same information (Barzi-lay et al, 1999); in information retrieval they havebeen used for query expansion (Anick and Tipirneni,1999).Learning paraphrases requires one to ensure iden-tity of meaning.
Since there are no adequate se-mantic interpretation systems available today, para-phrase acquisition techniques use some other mech-anism as a kind of ?pivot?
to (help) ensure semanticidentity.
Each pivot mechanism selects phrases withsimilar meaning in a different characteristic way.
Apopular method, the so-called distributional simi-larity, is based on the dictum of Zelig Harris ?youshall know the words by the company they keep?
:given highly discriminating left and right contexts,only words with very similar meaning will be foundto fit in between them.
For paraphrasing, this hasbeen often used to find syntactic transformations inparse trees that preserve (semantic) meaning.
An-other method is to use a bilingual dictionary or trans-lation table as pivot mechanism: all source languagewords or phrases that translate to a given foreignword/phrase are deemed to be paraphrases of oneanother.
In this paper we call the paraphrases thatcontain only words as surface paraphrases and those674that contain paths in a syntax tree as syntactic para-phrases.We here, present a method to acquire surfaceparaphrases from a single monolingual corpus.
Weuse a large corpus (about 150GB) to overcome thedata sparseness problem.
To overcome the scalabil-ity problem, we pre-process the text with a simpleparts-of-speech (POS) tagger and then apply localitysensitive hashing (LSH) (Charikar, 2002; Ravichan-dran et al, 2005) to speed up the remaining compu-tation for paraphrase acquisition.
Our experimentsshow results to verify the following main claim:Claim 1: Highly precise surface paraphrases can beobtained from a very large monolingual corpus.With this result, we further show that these para-phrases can be used to obtain high precision surfacepatterns that enable the discovery of relations in aminimally supervised way.
Surface patterns are tem-plates for extracting information from text.
For ex-ample, if one wanted to extract a list of company ac-quisitions, ??ACQUIRER?
acquired ?ACQUIREE?
?would be one surface pattern with ??ACQUIRER?
?and ??ACQUIREE??
as the slots to be extracted.Thus we can claim:Claim 2: These paraphrases can then be used forgenerating high precision surface patterns for rela-tion extraction.2 Related WorkMost recent work in paraphrase acquisition is basedon automatic acquisition.
Barzilay and McKeown(2001) used a monolingual parallel corpus to obtainparaphrases.
Bannard and Callison-Burch (2005)and Zhou et al (2006) both employed a bilingualparallel corpus in which each foreign language wordor phrase was a pivot to obtain source language para-phrases.
Dolan et al (2004) and Barzilay and Lee(2003) used comparable news articles to obtain sen-tence level paraphrases.
All these approaches relyon the presence of parallel or comparable corporaand are thus limited by their availability and size.Lin and Pantel (2001) and Szpektor et al (2004)proposed methods to obtain entailment templates byusing a single monolingual resource.
While both dif-fer in their approaches, they both end up finding syn-tactic paraphrases.
Their methods cannot be used ifwe cannot parse the data (either because of scale ordata quality).
Our approach on the other hand, findssurface paraphrases; it is more scalable and robustdue to the use of simple POS tagging.
Also, our useof locality sensitive hashing makes finding similarphrases in a large corpus feasible.Another task related to our work is relation extrac-tion.
Its aim is to extract instances of a given rela-tion.
Hearst (1992) the pioneering paper in the fieldused a small number of hand selected patterns to ex-tract instances of hyponymy relation.
Berland andCharniak (1999) used a similar method for extract-ing instances of meronymy relation.
Ravichandranand Hovy (2002) used seed instances of a relationto automatically obtain surface patterns by queryingthe web.
But their method often finds patterns thatare too general (e.g., X and Y), resulting in low pre-cision extractions.
Rosenfeld and Feldman (2006)present a somewhat similar web based method thatuses a combination of seed instances and seed pat-terns to learn good quality surface patterns.
Boththese methods differ from ours in that they learnrelation patterns on the fly (from the web).
Ourmethod however, pre-computes paraphrases for alarge set of surface patterns using distributional sim-ilarity over a large corpus and then obtains patternsfor a relation by simply finding paraphrases (offline)for a few seed patterns.
Using distributional simi-larity avoids the problem of obtaining overly gen-eral patterns and the pre-computation of paraphrasesmeans that we can obtain the set of patterns for anyrelation instantaneously.Romano et al (2006) and Sekine (2006) used syn-tactic paraphrases to obtain patterns for extractingrelations.
While procedurally different, both meth-ods depend heavily on the performance of the syntaxparser and require complex syntax tree matching toextract the relation instances.
Our method on theother hand acquires surface patterns and thus avoidsthe dependence on a parser and syntactic matching.This also makes the extraction process scalable.3 Acquiring ParaphrasesThis section describes our model for acquiring para-phrases from text.6753.1 Distributional SimilarityHarris?s distributional hypothesis (Harris, 1954) hasplayed an important role in lexical semantics.
Itstates that words that appear in similar contexts tendto have similar meanings.
In this paper, we applythe distributional hypothesis to phrases i.e.
word n-grams.For example, consider the phrase ?acquired?
ofthe form ?X acquired Y ?.
Considering the con-text of this phrase, we might find {Google, eBay,Yahoo,...} in position X and {YouTube, Skype,Overture,...} in position Y .
Now consider anotherphrase ?completed the acquisition of ?, again of theform ?X completed the acquisition of Y ?.
For thisphrase, we might find {Google, eBay, Hilton Hotelcorp.,...}
in position X and {YouTube, Skype, BallyEntertainment Corp.,...} in position Y .
Since thecontexts of the two phrases are similar, our exten-sion of the distributional hypothesis would assumethat ?acquired?
and ?completed the acquisition of ?have similar meanings.3.2 Paraphrase Learning ModelLet p be a phrase (n-gram) of the form X p Y ,where X and Y are the placeholders for words oc-curring on either side of p. Our first task is tofind the set of phrases that are similar in meaningto p. Let P = {p1, p2, p3, ..., pl} be the set of allphrases of the form X pi Y where pi ?
P .
LetSi,X be the set of words that occur in position X ofpi and Si,Y be the set of words that occur in posi-tion Y of pi.
Let Vi be the vector representing pisuch that Vi = Si,X ?
Si,Y .
Each word f ?
Vihas an associated score that measures the strengthof the association of the word f with phrase pi; asdo many others, we employ pointwise mutual infor-mation (Cover and Thomas, 1991) to measure thisstrength of association.pmi(pi; f) = log P (pi,f)P (pi)P (f) (1)The probabilities in equation (1) are calculated byusing the maximum likelihood estimate over ourcorpus.Once we have the vectors for each phrase pi ?
P ,we can find the paraphrases for each pi by finding itsnearest neighbors.
We use cosine similarity, whichis a commonly used measure for finding similaritybetween two vectors.If we have two phrases pi ?
P and pj ?
P withthe corresponding vectors Vi and Vj constructedas described above, the similarity between the twophrases is calculated as:sim(pi; pj) = Vi!Vj|Vi|?|Vj | (2)Each word in Vi (and Vj) has with it an associatedflag which indicates weather the word came fromSi,X or Si,Y .
Hence for each phrase pi of the formX pi Y , we have a corresponding phrase ?pi thathas the form Y pi X.
This is important to find cer-tain kinds of paraphrases.
The following examplewill illustrate.
Consider the sentences:Google acquired YouTube.
(3)YouTube was bought by Google.
(4)From sentence (3), we obtain two phrases:1. pi = acquired which has the form ?X acquired Y ?where ?X = Google?
and ?Y = YouTube?2.
?pi = ?acquired which has the form ?Y acquiredX?
where ?X = YouTube?
and ?Y = Google?Similarly, from sentence (4) we obtain two phrases:1. pj = was bought by which has the form ?X wasbought by Y ?
where ?X = YouTube?
and ?Y =Google?2.
?pj = ?was bought by which has the form ?Ywas bought by X?
where ?X = Google?
and ?Y= YouTube?The switching of X and Y positions in (3) and (4)ensures that ?acquired?
and ?
?was bought by?
arefound to be paraphrases by the algorithm.3.3 Locality Sensitive HashingAs described in Section 3.2, we find paraphrases ofa phrase pi by finding its nearest neighbors basedon cosine similarity between the feature vector ofpi and other phrases.
To do this for all the phrasesin the corpus, we?ll have to compute the similaritybetween all vector pairs.
If n is the number of vec-tors and d is the dimensionality of the vector space,finding cosine similarity between each pair of vec-tors has time complexity O(n2d).
This computationis infeasible for our corpus, since both n and d arelarge.676To solve this problem, we make use of Local-ity Sensitive Hashing (LSH).
The basic idea behindLSH is that a LSH function creates a fingerprintfor each vector such that if two vectors are simi-lar, they are likely to have similar fingerprints.
TheLSH function we use here was proposed by Charikar(2002).
It represents a d dimensional vector by astream of b bits (b & d) and has the property of pre-serving the cosine similarity between vectors, whichis exactly what we want.
Ravichandran et al (2005)have shown that by using the LSH nearest neighborscalculation can be done in O(nd) time.1.4 Learning Surface PatternsLet r be a target relation.
Our task is to find a set ofsurface patterns S = {s1, s2, ..., sn} that express thetarget relation.
For example, consider the relation r= ?acquisition?.
We want to find the set of patternsS that express this relation:S = {?ACQUIRER?
acquired ?ACQUIREE?,?ACQUIRER?
bought ?ACQUIREE?, ?ACQUIREE?was bought by ?ACQUIRER?,...
}.The remainder of the section describes our modelfor learning surface patterns for target relations.4.1 Model AssumptionParaphrases express the same meaning using differ-ent surface forms.
So if one knew a pattern that ex-presses a target relation, one could build more pat-terns for that relation by finding paraphrases for thesurface phrase(s) in that pattern.
This is the basicassumption of our model.For example, consider the seed pattern??ACQUIRER?
acquired ?ACQUIREE??
forthe target relation ?acquisition?.
The surface phrasein the seed pattern is ?acquired?.
Our model thenassumes that we can obtain more surface patternsfor ?acquisition?
by replacing ?acquired?
in theseed pattern with its paraphrases i.e.
{bought, ?wasbought by2,...}.
The resulting surface patterns are:1The details of the algorithm are omitted, but interestedreaders are encouraged to read Charikar (2002) and Ravichan-dran et al (2005)2The ???
in ?
?was bought by?
indicates that the?ACQUIRER?
and ?ACQUIREE?
arguments of the inputphrase ?acquired?
need to be switched for the phrase ?wasbought by?.{?ACQUIRER?
bought ?ACQUIREE?, ?ACQUIREE?was bought by ?ACQUIRER?,...
}4.2 Surface Pattern ModelLet r be a target relation.
Let SEED = {seed1,seed2,..., seedn} be the set of seed patterns that ex-press the target relation.
For each seedi ?
SEED,we obtain the corresponding set of new patternsPATi in two steps:1.
We find the surface phrase, pi, using a seedand find the corresponding set of paraphrases,Pi = {pi,1, pi,2, ..., pi,m}.
Each paraphrase,pi,j ?
Pi, has with it an associated score whichis similarity between pi and pi,j .2.
In seed pattern, seedi, we replace the sur-face phrase, pi, with its paraphrases andobtain the set of new patterns PATi ={pati,1, pati,2, ..., pati,m}.
Each pattern haswith it an associated score, which is the same asthe score of the paraphrase from which it wasobtained3 .
The patterns are ranked in the de-creasing order of their scores.After we obtain PATi for each seedi ?
SEED,we obtain the complete set of patterns, PAT , forthe target relation r as the union of all the individualpattern sets, i.e., PAT = PAT1 ?
PAT2 ?
... ?PATn.5 Experimental MethodologyIn this section, we describe experiments to validatethe main claims of the paper.
We first describe para-phrase acquisition, we then summarize our methodfor learning surface patterns, and finally describe theuse of patterns for extracting relation instances.5.1 ParaphrasesFinding surface variations in text requires a largecorpus.
The corpus needs to be orders of magnitudelarger than that required for learning syntactic varia-tions, since surface phrases are sparser than syntac-tic phrases.For our experiments, we used a corpus of about150GB (25 billion words) obtained from GoogleNews4 .
It consists of few years worth of news data.3If a pattern is generated from more than one seed, we assignit its average score.4The corpus was cleaned to remove duplicate articles.677We POS tagged the corpus using Tnt tagger (Brants,2000) and collected all phrases (n-grams) in the cor-pus that contained at least one verb, and had a nounor a noun-noun compound on either side.
We re-stricted the phrase length to at most five words.We build a vector for each phrase as described inSection 3.
Tomitigate the problem of sparseness andco-reference to a certain extent, whenever we have anoun-noun compound in the X or Y positions, wetreat it as bag of words.
For example, in the sen-tence ?Google Inc. acquired YouTube?, ?Google?and ?Inc.?
will be treated as separate features in thevector5.Once we have constructed all the vectors, we findthe paraphrases for every phrase by finding its near-est neighbors as described in Section 3.
For our ex-periments, we set the number of random bits in theLSH function to 3000, and the similarity cut-off be-tween vectors to 0.15.
We eventually end up witha resource containing over 2.5 million phrases suchthat each phrase is connected to its paraphrases.5.2 Surface PatternsOne claim of this paper is that we can find good sur-face patterns for a target relation by starting with aseed pattern.
To verify this, we study two target re-lations6:1.
Acquisition: We define this as the relation be-tween two companies such that one companyacquired the other.2.
Birthplace: We define this as the relation be-tween a person and his/her birthplace.For ?acquisition?
relation, we start with the sur-face patterns containing only the words buy and ac-quire:1.
??ACQUIRER?
bought ?ACQUIREE??
(and itsvariants, i.e.
buy, buys and buying)2.
??ACQUIRER?
acquired ?ACQUIREE??
(and itsvariants, i.e.
acquire, acquires and acquiring)5This adds some noise in the vectors, but we found that thisresults in better paraphrases.6Since we have to do all the annotations for evaluations onour own, we restricted our experiments to only two commonlyused relations.This results in a total of eight seed patterns.For ?birthplace?
relation, we start with two seedpatterns:1.
??PERSON?
was born in ?LOCATION??2.
??PERSON?
was born at ?LOCATION?
?.We find other surface patterns for each of theserelations by replacing the surface words in the seedpatterns by their paraphrases, as described in Sec-tion 4.5.3 Relation ExtractionThe purpose of learning surface patterns for a rela-tion is to extract instances of that relation.
We usethe surface patterns obtained for the relations ?ac-quisition?
and ?birthplace?
to extract instances ofthese relations from the LDC North American NewsCorpus.
This helps us to extrinsically evaluate thequality of the surface patterns.6 Experimental ResultsIn this section, we present the results of the experi-ments and analyze them.6.1 BaselinesIt is hard to construct a baseline for comparing thequality of paraphrases, as there isn?t much work inextracting surface level paraphrases using a mono-lingual corpus.
To overcome this, we show the effectof reduction in corpus size on the quality of para-phrases, and compare the results informally to theother methods that produce syntactic paraphrases.To compare the quality of the extraction patterns,and relation instances, we use the method presentedby Ravichandran and Hovy (2002) as the baseline.For each of the given relations, ?acquisition?
and?birthplace?, we use 10 seed instances, downloadthe top 1000 results from the Google search enginefor each instance, extract the sentences that containthe instances, and learn the set of baseline patternsfor each relation.
We then apply these patterns tothe test corpus and extract the corresponding base-line instances.6.2 Evaluation CriteriaHere we present the evaluation criteria we used toevaluate the performance on the different tasks.678ParaphrasesWe estimate the quality of paraphrases by annotatinga random sample as correct/incorrect and calculatingthe accuracy.
However, estimating the recall is diffi-cult given that we do not have a complete set of para-phrases for the input phrases.
Following Szpektor etal.
(2004), instead of measuring recall, we calculatethe average number of correct paraphrases per inputphrase.Surface PatternsWe can calculate the precision (P ) of learned pat-terns for each relation by annotating the extractedpatterns as correct/incorrect.
However calculatingthe recall is a problem for the same reason as above.But we can calculate the relative recall (RR) of thesystem against the baseline and vice versa.
The rela-tive recallRRS|B of system S with respect to systemB can be calculated as:RRS|B = CS?CBCBwhere CS is the number of correct patterns found byour system and CB is the number of correct patternsfound by the baseline.
RRB|S can be found in a sim-ilar way.Relation ExtractionWe estimate the precision (P ) of the extracted in-stances by annotating a random sample of instancesas correct/incorrect.
While calculating the true re-call here is not possible, even calculating the truerelative recall of the system against the baseline isnot possible as we can annotate only a small sam-ple.
However, following Pantel et al (2004), we as-sume that the recall of the baseline is 1 and estimatethe relative recall RRS|B of the system S with re-spect to the baseline B using their respective pre-cision scores PS and PB and number of instancesextracted by them |S| and |B| as:RRS|B = PS?|S|PB?|B|6.3 Gold StandardIn this section, we describe the creation of gold stan-dard for the different tasks.ParaphrasesWe created the gold standard paraphrase test set byrandomly selecting 50 phrases and their correspond-ing paraphrases from our collection of 2.5 millionphrases.
For each test phrase, we asked two annota-tors to annotate its paraphrases as correct/incorrect.The annotators were instructed to look for strictparaphrases i.e.
equivalent phrases that can be sub-stituted for each other.To obtain the inter-annotator agreement, the twoannotators annotated the test set separately.
Thekappa statistic (Siegal and Castellan Jr., 1988) was?
= 0.63.
The interesting thing is that the anno-tators got this respectable kappa score without anyprior training, which is hard to achieve when oneannotates for a similar task like textual entailment.Surface PatternsFor the target relations, we asked two annotators toannotate the patterns for each relation as either ?pre-cise?
or ?vague?.
The annotators annotated the sys-tem as well as the baseline outputs.
We consider the?precise?
patterns as correct and the ?vague?
as in-correct.
The intuition is that applying the vague pat-terns for extracting target relation instances mightfind some good instances, but will also find manybad ones.
For example, consider the following twopatterns for the ?acquisition?
relation:?ACQUIRER?
acquired ?ACQUIREE?
(5)?ACQUIRER?
and ?ACQUIREE?
(6)Example (5) is a precise pattern as it clearly identi-fies the ?acquisition?
relation while example (6) isa vague pattern because it is too general and saysnothing about the ?acquisition?
relation.
The kappastatistic between the two annotators for this task was?
= 0.72.Relation ExtractionWe randomly sampled 50 instances of the ?acquisi-tion?
and ?birthplace?
relations from the system andthe baseline outputs.
We asked two annotators to an-notate the instances as correct/incorrect.
The anno-tators marked an instance as correct only if both theentities and the relation between them were correct.To make their task easier, the annotators were pro-vided the context for each instance, and were freeto use any resources at their disposal (including aweb search engine), to verify the correctness of theinstances.
The annotators found that the annotationfor this task was much easier than the previous two;the few disagreements they had were due to ambigu-ity of some of the instances.
The kappa statistic forthis task was ?
= 0.91.679Annotator AccuracyAverage # correctparaphrasesAnnotator 1 67.31% 4.2Annotator 2 74.27% 4.28Table 1: Quality of paraphrasesare being distributed to approved a revision to thehave been distributed to unanimously approved a neware being handed out to approved an annualwere distributed to will consider adopting a?are handing out approved a revisedwill be distributed to all approved a newTable 2: Example paraphrases6.4 Result SummaryTable 1 shows the results of annotating the para-phrases test set.
We do not have a baselineto compare against but we can analyze them inlight of numbers reported previously for syntac-tic paraphrases.
DIRT (Lin and Pantel, 2001) andTEASE (Szpektor et al, 2004) report accuracies of50.1% and 44.3% respectively compared to our av-erage accuracy across two annotators of 70.79%.The average number of paraphrases per phrase ishowever 10.1 and 5.5 for DIRT and TEASE respec-tively compared to our 4.2.
One reason why thisnumber is lower is that our test set contains com-pletely random phrases from our set (2.5 millionphrases): some of these phrases are rare and havevery few paraphrases.
Table 2 shows some para-phrases generated by our system for the phrases ?arebeing distributed to?
and ?approved a revision tothe?.Table 3 shows the results on the quality of surfacepatterns for the two relations.
It can be observedthat our method outperforms the baseline by a widemargin in both precision and relative recall.
Table 4shows some example patterns learned by our system.Table 5 shows the results of the quality of ex-tracted instances.
Our system obtains very high pre-cision scores but suffers in relative recall given thatthe baseline with its very general patterns is likelyto find a huge number of instances (though a verysmall portion of them are correct).
Table 6 showssome example instances we extracted.acquisition birthplaceX agreed to buy Y X , who was born in YX , which acquired Y X , was born in YX completed its acquisitionof YX was raised in YX has acquired Y X was born in NNNNa in YX purchased Y X , born in YaEach ?N?
here is a placeholder for a number from 0 to 9.Table 4: Example extraction templatesacquisition birthplace1.
Huntington BancsharesInc.
agreed to acquire Re-liance Bank1.
Cyril Andrew Ponnam-peruma was born in Galle2.
Sony bought ColumbiaPictures2.
Cook was born in NNNNin Devonshire3.
Hanson Industries buysKidde Inc.3.
Tansey was born inCincinnati4.
Casino America inc.agreed to buy Grand Palais4.
Tsoi was born in NNNN inUzbekistan5.
Tidewater inc. acquiredHornbeck Offshore ServicesInc.5.
Mrs. Totenberg was bornin San FranciscoTable 6: Example instances6.5 Discussion and Error AnalysisWe studied the effect of the decrease in size of theavailable raw corpus on the quality of the acquiredparaphrases.
We used about 10% of our original cor-pus to learn the surface paraphrases and evaluatedthem.
The precision, and the average number ofcorrect paraphrases are calculated on the same testset, as described in Section 6.2.
The performancedrop on using 10% of the original corpus is signif-icant (11.41% precision and on an average 1 cor-rect paraphrase per phrase), which shows that we in-deed need a large amount of data to learn good qual-ity surface paraphrases.
One reason for this dropis also that when we use only 10% of the originaldata, for some of the phrases from the test set, we donot find any paraphrases (thus resulting in 0% accu-racy for them).
This is not unexpected, as the largerresource would have a much larger recall, whichagain points at the advantage of using a large dataset.
Another reason for this performance drop couldbe the parameter settings: We found that the qual-ity of learned paraphrases depended greatly on thevarious cut-offs used.
While we adjusted our model680Relation Method # PatternsAnnotator 1 Annotator 2P RR P RRAcquisitionBaseline 160 55% 13.02% 60% 11.16%Paraphrase Method 231 83.11% 28.40% 93.07% 25%BirthplaceBaseline 16 31.35% 15.38% 31.25% 15.38%Paraphrase Method 16 81.25% 40% 81.25% 40%Table 3: Quality of Extraction PatternsRelation Method # PatternsAnnotator 1 Annotator 2P RR P RRAcquisitionBaseline 1, 261, 986 6% 100% 2% 100%Paraphrase Method 3875 88% 4.5% 82% 12.59%BirthplaceBaseline 979, 607 4% 100% 2% 100%Paraphrase Method 1811 98% 4.53% 98% 9.06%Table 5: Quality of instancesparameters for working with smaller sized data, it isconceivable that we did not find the ideal setting forthem.
So we consider these numbers to be a lowerbound.
But even then, these numbers clearly indi-cate the advantage of using more data.We also manually inspected our paraphrases.
Wefound that the problem of ?antonyms?
was some-what less pronounced due to our use of a large cor-pus, but they still were the major source of error.For example, our system finds the phrase ?sell?
asa paraphrase for ?buy?.
We need to deal with thisproblem separately in the future (may be as a post-processing step using a list of antonyms).Moving to the task of relation extraction, we seefrom table 5 that our system has a much lower rel-ative recall compared to the baseline.
This was ex-pected as the baseline method learns some very gen-eral patterns, which are likely to extract some goodinstances, even though they result in a huge hit toits precision.
However, our system was able to ob-tain this performance using very few seeds.
So anincrease in the number of input seeds, is likely to in-crease the relative recall of the resource.
The ques-tion however remains as to what good seeds mightbe.
It is clear that it is much harder to come up withgood seed patterns (that our system needs), than seedinstances (that the baseline needs).
But there aresome obvious ways to overcome this problem.
Oneway is to bootstrap.
We can look at the paraphrasesof the seed patterns and use them to obtain more pat-terns.
Our initial experiments with this method usinghandpicked seeds showed good promise.
However,we need to investigate automating this approach.Another method is to use the good patterns from thebaseline system and use them as seeds for our sys-tem.
We plan to investigate this approach as well.One reason, why we have seen good preliminary re-sults using these approaches (for improving recall),we believe, is that the precision of the paraphrases isgood.
So either a seed doesn?t produce any new pat-terns or it produces good patterns, thus keeping theprecision of the system high while increasing rela-tive recall.7 ConclusionParaphrases are an important technique to handlevariations in language.
Given their utility in manyNLP tasks, it is desirable that we come up withmethods that produce good quality paraphrases.
Webelieve that the paraphrase acquisition method pre-sented here is a step towards this very goal.
We haveshown that high precision surface paraphrases can beobtained by using distributional similarity on a largecorpus.
We made use of some recent advances intheoretical computer science to make this task scal-able.
We have also shown that these paraphrasescan be used to obtain high precision extraction pat-terns for information extraction.
While we believethat more work needs to be done to improve the sys-tem recall (some of which we are investigating), thisseems to be a good first step towards developing aminimally supervised, easy to implement, and scal-able relation extraction system.681ReferencesP.
G. Anick and S. Tipirneni.
1999.
The paraphrasesearch assistant: terminological feedback for iterativeinformation seeking.
In ACM SIGIR, pages 153?159.C.
Bannard and C. Callison-Burch.
2005.
Paraphras-ing with bilingual parallel corpora.
In Association forComputational Linguistics, pages 597?604.R.
Barzilay and L. Lee.
2003.
Learning to paraphrase: anunsupervised approach using multiple-sequence align-ment.
In In Proceedings North American Chapter ofthe Association for Computational Linguistics on Hu-man Language Technology, pages 16?23.R.
Barzilay and K. R. McKeown.
2001.
Extracting para-phrases from a parallel corpus.
In In Proceedings ofAssociation for Computational Linguistics, pages 50?57.R.
Barzilay, K. R. McKeown, and M. Elhadad.
1999.Information fusion in the context of multi-documentsummarization.
InAssociation for Computational Lin-guistics, pages 550?557.M.
Berland and E. Charniak.
1999.
Finding parts in verylarge corpora.
In In Proceedings of Association forComputational Linguistics, pages 57?64.T.
Brants.
2000.
Tnt ?
a statistical part-of-speech tag-ger.
In In Proceedings of the Applied NLP Conference(ANLP).C.
Callison-Burch, P. Koehn, and M. Osborne.
2006.Improved statistical machine translation using para-phrases.
In Human Language Technology Conferenceof the North American Chapter of the Association ofComputational Linguistics, pages 17?24.M.
S. Charikar.
2002.
Similarity estimation techniquesfrom rounding algorithms.
In In Proceedings of thethiry-fourth annual ACM symposium on Theory ofcomputing, pages 380?388.T.M.
Cover and J.A.
Thomas.
1991.
Elements of Infor-mation Theory.
John Wiley & Sons.B.
Dolan, C. Quirk, and C. Brockett.
2004.
Unsuper-vised construction of large paraphrase corpora: ex-ploiting massively parallel news sources.
In In Pro-ceedings of the conference on Computational Linguis-tics (COLING), pages 350?357.Z.
Harris.
1954.
Distributional structure.
Word, pages10(23):146?162.M.
A. Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of the confer-ence on Computational linguistics, pages 539?545.D.
Lin and P. Pantel.
2001.
Dirt: Discovery of infer-ence rules from text.
In ACM SIGKDD internationalconference on Knowledge discovery and data mining,pages 323?328.P.
Pantel, D. Ravichandran, and E.H. Hovy.
2004.
To-wards terascale knowledge acquisition.
In In Proceed-ings of the conference on Computational Linguistics(COLING), pages 771?778.D.
Ravichandran and E.H. Hovy.
2002.
Learning sur-face text for a question answering system.
In Associ-ation for Computational Linguistics (ACL), Philadel-phia, PA.D.
Ravichandran, P. Pantel, and E.H. Hovy.
2005.
Ran-domized algorithms and nlp: using locality sensitivehash function for high speed noun clustering.
In InProceedings of Association for Computational Lin-guistics, pages 622?629.L.
Romano, M. Kouylekov, I. Szpektor, I. Dagan, andA.
Lavelli.
2006.
Investigating a generic paraphrase-based approach for relation extraction.
In In Proceed-ings of the European Chapter of the Association forComputational Linguistics (EACL).B.
Rosenfeld and R. Feldman.
2006.
Ures: an unsuper-vised web relation extraction system.
In Proceedingsof the COLING/ACL on Main conference poster ses-sions, pages 667?674.S.
Sekine.
2006.
On-demand information extraction.
InIn Proceedings of COLING/ACL, pages 731?738.S.
Siegal and N.J. Castellan Jr. 1988.
NonparametricStatistics for the Behavioral Sciences.
McGraw-Hill.I.
Szpektor, H. Tanev, I. Dagan, and B. Coppola.
2004.Scaling web-based acquisition of entailment relations.In In Proceedings of Empirical Methods in NaturalLanguage Processing, pages 41?48.L.
Zhou, C.Y.
Lin, D. Munteanu, and E.H. Hovy.
2006.Paraeval: using paraphrases to evaluate summaries au-tomatically.
In In Proceedings of the Human Lan-guage Technology Conference of the North AmericanChapter of the Association of Computational Linguis-tics, pages 447?454.682
