Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2355?2365,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsImproving Information Extraction by Acquiring External Evidence withReinforcement LearningKarthik NarasimhanCSAIL, MITkarthikn@mit.eduAdam YalaCSAIL, MITadamyala@mit.eduRegina BarzilayCSAIL, MITregina@csail.mit.eduAbstractMost successful information extraction sys-tems operate with access to a large collec-tion of documents.
In this work, we explorethe task of acquiring and incorporating exter-nal evidence to improve extraction accuracyin domains where the amount of training datais scarce.
This process entails issuing searchqueries, extraction from new sources and rec-onciliation of extracted values, which are re-peated until sufficient evidence is collected.We approach the problem using a reinforce-ment learning framework where our modellearns to select optimal actions based on con-textual information.
We employ a deep Q-network, trained to optimize a reward func-tion that reflects extraction accuracy while pe-nalizing extra effort.
Our experiments ontwo databases ?
of shooting incidents, andfood adulteration cases ?
demonstrate that oursystem significantly outperforms traditionalextractors and a competitive meta-classifierbaseline.11 IntroductionIn many realistic domains, information extraction(IE) systems require exceedingly large amounts ofannotated data to deliver high performance.
In-creases in training data size enable models to han-dle robustly the multitude of linguistic expressionsthat convey the same semantic relation.
Consider,for instance, an IE system that aims to identify en-tities such as the perpetrator and the number of vic-1Code is available at http://people.csail.mit.edu/karthikn/rl-ie/ShooterName: Scott WesterhuisNumKilled: 6A couple and four children found dead in theirburning South Dakota home had been shot in anapparent murder-suicide, officials said Monday....Scott Westerhuis?s cause of death was "shotgunwound with manner of death as suspected sui-cide," it added in a statement.Figure 1: Sample news article on a shooting case.
Notehow the article contains both the name of the shooter andthe number of people killed but both pieces of informa-tion require complex extraction schemes.tims in a shooting incident (Figure 1).
The docu-ment does not explicitly mention the shooter (ScottWesterhuis), but instead refers to him as a suicidevictim.
Extraction of the number of fatally shot vic-tims is similarly difficult, as the system needs to in-fer that "A couple and four children" means six peo-ple.
Even a large annotated training set may not pro-vide sufficient coverage to capture such challengingcases.In this paper, we explore an alternative approachfor boosting extraction accuracy, when a large train-ing corpus is not available.
Instead, the proposedmethod utilizes external information sources to re-solve ambiguities inherent in text interpretation.Specifically, our strategy is to find other documentsthat contain the information sought, expressed in aform that a basic extractor can "understand".
Forinstance, Figure 2 shows two other articles describ-ing the same event, wherein the entities of interest2355The six members of a South Dakota family founddead in the ruins of their burned home were fa-tally shot, with one death believed to be a suicide,authorities said Monday.AG Jackley says all evidence supports the storyhe told based on preliminary findings back inSeptember: Scott Westerhuis shot his wife andchildren with a shotgun, lit his house on fire withan accelerant, then shot himself with his shotgun.Figure 2: Two other articles on the same shooting case.The first article clearly mentions that six people werekilled.
The second one portrays the shooter in an easilyextractable form.?
the number of people killed and the name of theshooter ?
are expressed explicitly.
Processing suchstereotypical phrasing is easier for most extractionsystems, compared to analyzing the original sourcedocument.
This approach is particularly suitable forextracting information from news where a typicalevent is covered by multiple news outlets.The challenges, however, lie in (1) performingevent coreference (i.e.
retrieving suitable articles de-scribing the same incident) and (2) reconciling theentities extracted from these different documents.Querying the web (using the source article?s title forinstance) often retrieves documents about other inci-dents with a tangential relation to the original story.For example, the query ?4 adults, 1 teenager shot inwest Baltimore 3 april 2015?
yields only two rele-vant articles among the top twenty results on Bingsearch, while returning other shooting events at thesame location.
Moreover, the values extracted fromthese different sources require resolution since someof them might be inaccurate.One solution to this problem would be to performa single search to retrieve articles on the same eventand then reconcile values extracted from them (say,using a meta-classifier).
However, if the confidenceof the new set of values is still low, we might wishto perform further queries.
Thus, the problem is in-herently sequential, requiring alternating phases ofquerying to retrieve articles and integrating the ex-tracted values.We address these challenges using a Reinforce-ment Learning (RL) approach that combines queryformulation, extraction from new sources, and valuereconciliation.
To effectively select among possibleactions, our state representation encodes informa-tion about the current and new entity values alongwith the similarity between the source article andthe newly retrieved document.
The model learnsto select good actions for both article retrieval andvalue reconciliation in order to optimize the rewardfunction, which reflects extraction accuracy and in-cludes penalties for extra moves.
We train the RLagent using a Deep Q-Network (DQN) (Mnih et al,2015) that is used to predict both querying and rec-onciliation choices simultaneously.
While we use amaximum entropy model as the base extractor, thisframework can be inherently applied to other extrac-tion algorithms.We evaluate our system on two datasets whereavailable training data is inherently limited.
Thefirst dataset is constructed from a publicly availabledatabase of mass shootings in the United States.
Thedatabase is populated by volunteers and includesthe source articles.
The second dataset is derivedfrom a FoodShield database of illegal food adulter-ations.
Our experiments demonstrate that the finalRL model outperforms basic extractors as well asa meta-classifier baseline in both domains.
For in-stance, in the Shootings domain, the average accu-racy improvement over the meta-classifier is 7%.2 Related WorkOpen Information Extraction Existing work inopen IE has used external sources from theweb to improve extraction accuracy and cover-age (Agichtein and Gravano, 2000; Etzioni et al,2011; Fader et al, 2011; Wu and Weld, 2010).Such research has focused on identifying multipleinstances of the same relation, independent of thecontext in which this information appears.
In con-trast, our goal is to extract information from addi-tional sources about a specific event described in asource article.
Therefore, the novel challenge of ourtask resides in performing event coreference (Lee etal., 2012; Bejan and Harabagiu, 2014) (i.e identify-ing other sources describing the same event) whilesimultaneously reconciling extracted information.Moreover, relations typically considered by open IEsystems have significantly higher coverage in on-line documents than a specific incident described in2356a few news sources.
Hence, we require a differentmechanism for finding and reconciling online infor-mation.Entity linking, multi-document extraction andevent coreference Our work also relates to thetask of multi-document information extraction,where the goal is to connect different mentions ofthe same entity across input documents (Mann andYarowsky, 2005; Han et al, 2011; Durrett and Klein,2014).
Since this setup already includes multiple in-put documents, the model is not required to lookfor additional sources or decide on their relevance.Also, while the set of input documents overlap interms of entities mentioned, they do not necessarilydescribe the same event.
Given these differences insetup, the challenges and opportunities of the twotasks are distinct.Knowledge Base Completion and Online SearchRecent work has explored several techniques to per-form Knowledge Base Completion (KBC) such asvector space models and graph traversal (Socher etal., 2013; Yang et al, 2014; Gardner et al, 2014;Neelakantan et al, 2015; Guu et al, 2015).
Thoughour work also aims at increasing extraction recallfor a database, traditional KBC approaches do notrequire searching for additional sources of informa-tion.
West et al (2014) explore query reformula-tion in the context of KBC.
Using existing searchlogs, they learn how to formulate effective queriesfor different types of database entries.
Once querylearning is completed, the model employs several se-lected queries, and then aggregates the results basedon retrieval ranking.
This approach is complemen-tary to the proposed method, and can be combinedwith our approach if search logs are available.Kanani and McCallum (2012) also combinesearch and information extraction.
In their task offaculty directory completion, the system has to finddocuments from which to extract desired informa-tion.
They employ reinforcement learning to addresscomputational bottlenecks, by minimizing the num-ber of queries, document downloads and extractionaction.
The extraction accuracy is not part of thisoptimization, since the baseline IE system achieveshigh performance on the relations of interest.
Hence,given different design goals, the two RL formula-tions are very different.
Our approach is also closein spirit to the AskMSR system (Banko et al, 2002)which aims at using information redundancy on theweb to better answer questions.
Though our goal issimilar, we learn to query and consolidate the dif-ferent sources of information instead of using pre-defined rules.
Several slot-filling methods have ex-perimented with query formulation over web-basedcorpora to populate knowledge bases (Surdeanu etal., 2010; Ji and Grishman, 2011).3 FrameworkWe model the information extraction task as amarkov decision process (MDP), where the modellearns to utilize external sources to improve uponextractions from a source article (see Figure 3).
TheMDP framework allows us to dynamically incorpo-rate entity predictions while also providing flexibil-ity to choose the type of articles to extract from.
Ateach step, the system has to reconcile extracted val-ues from a related article (enew) with the current setof values (ecur), and decide on the next query forretrieving more articles.We represent the MDP as a tuple ?S,A, T,R?,where S = {s} is the space of all possible states,A = {a = (d, q)} is the set of all actions, R(s, a) isthe reward function, and T (s?|s, a) is the transitionfunction.
We describe these in detail below.States The state s in our MDP consists of the ex-tractor?s confidence in predicted entity values, thecontext from which the values are extracted and thesimilarity between the new document and the origi-nal one.
We represent the state as a continuous real-valued vector (Figure 3) incorporating these piecesof information:1.
Confidence scores of current and newly extractedentity values.2.
One-hot encoding of matches between currentand new values.3.
Unigram/tf-idf counts2 of context words.
Theseare words that occur in the neighborhood of theentity values in a document (e.g.
the wordswhich, left, people and wounded in the phrase?which left 5 people wounded?).4.
tf-idf similarity between the original article andthe new article.2Counts are computed on the documents used to train thebasic extraction system.2357selectReconcileQ extractsearchShooterName Scott WesterhuisNumKilled 4NumWounded 2City PlatteShooterName Scott WesterhuisNumKilled 6NumWounded 0City PlattequeryShooterName Scott WesterhuisNumKilled 6NumWounded 2City PlatteShooterName Scott WesterhuisNumKilled 5NumWounded 0City S.D.State	1 State	2Current Values:ShooterName?
Scott WesterhuisNumKilled?
4NumWounded?
2City?
PlatteNew Values:ShooterName?
Scott WesterhuisNumKilled?
6NumWounded?
0City?
PlatteState:?0.3, 0.2, 0.5, 0.1, ?
currentConf0.4, 0.6, 0.2, 0.4, ?
newConf1, 0, 0, 1, 0, 1, 1, 0, ?
matches0.2, 0.3, ..., 0.1, 0.5, ?
contextWords0.65?
?
document tf-idf similarityFigure 3: Left: Illustration of a transition in the MDP ?
the top box in each state shows the current entities and thebottom one consists of the new entities extracted from a downloaded article on the same event.
Right: Sample staterepresentation (bottom) in the MDP based on current and new values of entities (top).
currentConf : confidence scoresof current entities, newConf : confidence scores of new entities, contextWords: tf-idf counts of context words.Actions At each step, the agent is required to taketwo actions - a reconciliation decision d and a querychoice q.
The decision d on the newly extracted val-ues can be one of the following types: (1) accept aspecific entity?s value (one action per entity)3, (2)accept all entity values, (3) reject all values or (4)stop.
In cases 1-3, the agent continues to inspectmore articles, while the episode ends if a stop ac-tion (4) is chosen.
The current values and confidencescores are simply updated with the accepted valuesand the corresponding confidences.4 The choice q isused to choose the next query from a set of automat-ically generated alternatives (details below) in orderto retrieve the next article.Rewards The reward function is chosen to maxi-mize the final extraction accuracy while minimizingthe number of queries.
The accuracy component iscalculated using the difference between the accuracyof the current and the previous set of entity values:R(s, a) =?entity jAcc(ejcur)?
Acc(ejprev)There is a negative reward per step to penalize theagent for longer episodes.3No entity-specific features are used for action selection.4We also experiment with other forms of value reconcilia-tion.
See Section 5 for details.Queries The queries are based on automaticallygenerated templates, created using the title of an ar-ticle along with words5 most likely to co-occur witheach entity type in the training data.
Table 1 pro-vides some examples ?
for instance, the second tem-plate contains words such as arrested and identifiedwhich often appear around the name of the shooter.?title??title?
+ (police | identified | arrested | charged)?title?
+ (killed | shooting | injured | dead | people)?title?
+ (injured | wounded | victim)?title?
+ (city | county | area)Table 1: Examples of different query templates for websearch for articles on mass shootings.
The | symbol repre-sents logical OR.
The last 4 queries contain context wordsaround values for entity types ShooterName, NumKilled,NumWounded and City, respectively.
At query time,?title?
is replaced by the source article?s title.We use a search engine to query the web for arti-cles on the same event as the source article and re-trieve the top k links per query.6 Documents that aremore than a month older than the original article arefiltered out of the search results.Transitions Each episode starts off with a singlesource article xi from which an initial set of entity5Stop words, numeric terms and proper nouns are filtered.6We use k=20 in our experiments.2358values are extracted.
The subsequent steps in theepisode involve the extra articles, downloaded usingdifferent types of query formulations based on thesource article.
A single transition in the episode con-sists of the agent being given the state s containinginformation about the current and new set of values(extracted from a single article) using which the nextaction a = (d, q) is chosen.
The transition functionT (s?|s, a) incorporates the reconciliation decision dfrom the agent in state s along with the values fromthe next article retrieved using query q and producesthe next state s?.
The episode stops whenever d is astop decision.Algorithm 1 details the entire MDP frameworkfor the training phase.
During the test phase, eachsource article is handled only once in a singleepisode (lines 8-23).Algorithm 1 MDP framework for Information Extrac-tion (Training Phase)1: Initialize set of original articles X2: for xi ?
X do3: for each query template T q do4: Download articles with query T q(xi)5: Queue retrieved articles in Y qi6: for epoch = 1,M do7: for i = 1, |X| do //episode8: Extract entities e0 from xi9: ecur ?
e010: q?
0, r?
0 //query type, reward11: while Y qi not empty do12: Pop next article y from Y qi13: Extract entities enew from y14: Compute tf-idf similarity Z(xi, y)15: Compute context vector C(y)16: Form state s using ecur, enew, Z(xi, y)and C(y)17: Send (s, r) to agent18: Get decision d, query q from agent19: if q == ?end_episode?
then break20: eprev ?
ecur21: ecur ?
Reconcile(ecur, enew, d)22: r ?
?entity j Acc(ejcur)?
Acc(ejprev)23: Send (send, r) to agent4 Reinforcement Learning for InformationExtractionIn order to learn a good policy for an agent, we uti-lize the paradigm of reinforcement learning (RL).The MDP described in the previous section canbe viewed in terms of a sequence of transitions(s, a, r, s?).
The agent typically utilizes a state-action value function Q(s, a) to determine whichaction a to perform in state s. A commonly usedtechnique for learning an optimal value function isQ-learning (Watkins and Dayan, 1992), in whichthe agent iteratively updates Q(s, a) using the re-wards obtained from episodes.
The updates are de-rived from the recursive Bellman equation (Suttonand Barto, 1998) for the optimal Q:Qi+1(s, a) = E[r + ?maxa?
Qi(s?, a?)
| s, a]Here, r = R(s, a) is the reward and ?
is a factordiscounting the value of future rewards and the ex-pectation is taken over all transitions involving states and action a.Since our problem involves a continuousstate space S, we use a deep Q-network(DQN) (Mnih et al, 2015) as a function ap-proximator Q(s, a) ?
Q(s, a; ?).
The DQN, inwhich the Q-function is approximated using a deepneural network, has been shown to learn better valuefunctions than linear approximators (Narasimhanet al, 2015; He et al, 2015) and can capturenon-linear interactions between the different piecesof information in our state.We use a DQN consisting of two linear layers (20hidden units each) followed by rectified linear units(ReLU), along with two separate output layers.7 Thenetwork takes the continuous state vector s as inputand predicts Q(s, d) and Q(s, q) for reconciliationdecisions d and query choices q simultaneously us-ing the different output layers (see Supplementarymaterial for the model architecture).Parameter Learning The parameters ?
of theDQN are learnt using stochastic gradient de-scent with RMSprop (Tieleman and Hinton, 2012).Each parameter update aims to close the gap be-tween the Q(st, at; ?)
predicted by the DQN andthe expected Q-value from the Bellman equation,rt + ?maxaQ(st+1, a; ?).
Following Mnih et al(2015), we make use of a (separate) target Q-network to calculate the expected Q-value, in order7We did not observe significant differences with additionallinear layers or the choice of non-linearity (Sigmoid/ReLU).2359Algorithm 2 Training Procedure for DQN agent with-greedy exploration1: Initialize experience memory D2: Initialize parameters ?
randomly3: for episode = 1,M do4: Initialize environment and get start state s15: for t = 1, N do6: if random() <  then7: Select a random action at8: else9: Compute Q(st, a) for all actions a10: Select at = argmax Q(st, a)11: Execute action at and observe reward rt andnew state st+112: Store transition (st, at, rt, st+1) in D13: Sample random mini batch of transitions(sj , aj , rj , sj+1) from D14: yj ={rj , if sj+1 is terminalrj + ?
maxa?
Q(sj+1, a?
; ?t), else15: Perform gradient descent step on the lossL(?)
= (yj ?Q(sj , aj ; ?
))216: if st+1 == send then breakto have ?stable updates?.
The target Q-network isperiodically updated with the current parameters ?.We also make use of an experience replay memoryD to store transitions.
To perform updates, we sam-ple a batch of transitions (s?, a?, s?
?, r) at random fromD and minimize the loss function8:L(?)
= Es?,a?
[(y ?Q(s?, a?
; ?
))2]where y = r + ?maxa?
Q(s?
?, a?
; ?t) is the target Q-value.
The learning updates are made every trainingstep using the following gradients:??L(?)
= Es?,a?
[2(y ?Q(s?, a?
; ?))?
?Q(s?, a?
; ?
)]Algorithm 2 details the DQN training procedure.5 Experimental SetupData We perform experiments on two differentdatasets.
For the first set, we collected data from theGun Violence archive,9 a website tracking shootingsin the United States.
The data contains a news articleon each shooting and annotations for (1) the name ofthe shooter, (2) the number of people killed, (3) thenumber of people wounded, and (4) the city where8The expectation is over the transitions sampled uniformlyat random from D.9www.shootingtracker.com/Main_PageNumber Shootings AdulterationTrain Test Dev Train Test DevSource articles 306 292 66 292 148 42Downloaded articles 8201 7904 1628 7686 5333 1537Table 2: Stats for Shootings and Adulteration datasetsthe incident took place.
We consider these as theentities of interest, to be extracted from the articles.The second dataset we use is the Foodshield EMAdatabase10 documenting adulteration incidents since1980.
This data contains annotations for (1) the af-fected food product, (2) the adulterant and (3) thelocation of the incident.
Both datasets are classicexamples where the number of recorded incidents isinsufficient for large-scale IE systems to leverage.For each source article in the above databases, wedownload extra articles (top 20 links) using the BingSearch API11 with different automatically generatedqueries.
We use only the source articles from thetrain portion to learn the parameters of the base ex-tractor.
The entire train set with downloaded arti-cles is used to train the DQN agent and the meta-classifier baseline (described below).
All parame-ters are tuned on the dev set.
For the final results,we train the models on the combined train and devsets and use the entire test set (source + downloadedarticles) to evaluate.
Table 2 provides data statistics.Extraction model We use a maximum entropyclassifier as the base extraction system, since it pro-vides flexibility to capture various local context fea-tures and has been shown to perform well for in-formation extraction (Chieu and Ng, 2002).
Theclassifier is used to tag each word in a documentas one of the entity types or not (e.g.
{Shooter-Name, NumKilled, NumWounded, City, Other} inthe Shootings domain).
Then, for each tag exceptOther, we choose the mode of the values to obtainthe set of entity extractions from the article.12 Fea-tures used in the classifier are provided in the Sup-plementary material.The features and context window c = 4 of neigh-boring words are tuned to maximize performance ona dev set.
We also experimented with a conditionalrandom field (CRF) (with the same features) forthe sequence tagging (Culotta and McCallum, 2004)10www.foodshield.org/member/login/11www.bing.com/toolbox/bingsearchapi12We normalize numerical words (e.g.
"one" to "1") beforetaking the mode.2360but obtained worse empirical performance (see Sec-tion 6).
The parameters of the base extraction modelare not changed during training of the RL model.Evaluation We evaluate the extracted entity val-ues against the gold annotations and report thecorpus-level average accuracy on each entity type.For entities like ShooterName, the annotations (andthe news articles) often contain multiple names (firstand last) in various combinations, so we consider re-trieving either name as a successful extraction.
Forall other entities, we look for exact matches.Baselines We explore 4 types of baselines:Basic extractors: We use the CRF and the Maxentclassifier mentioned previously.Aggregation systems: We examine two systemsthat perform different types of value reconciliation.The first model (Confidence) chooses entity valueswith the highest confidence score assigned by thebase extractor.
The second system (Majority) takesa majority vote over all values extracted from thesearticles.
Both methods filter new entity values usinga threshold ?
on the cosine similarity over the tf-idfrepresentations of the source and new articles.Meta-classifer: To demonstrate the importance ofmodeling the problem in the RL framework, we con-sider a meta-classifier baseline.
The classifier oper-ates over the same input state space and producesthe same set of reconciliation decisions {d} as theDQN.
For training, we use the original source arti-cle for each event along with a related downloadedarticle to compute the state.
If the downloaded ar-ticle has the correct value and the original one doesnot, we label it as a positive example for that entityclass.
If multiple such entity classes exist, we cre-ate several training instances with appropriate labels,and if none exist, we use the label corresponding tothe reject all action.
For each test event, the clas-sifier is used to provide decisions for all the down-loaded articles and the final extraction is performedby aggregating the value predictions using the Con-fidence-based scheme described above.Oracle: Finally, we also have an ORACLE scorewhich is computed assuming perfect reconciliationand querying decisions on top of the Maxent baseextractor.
This helps us analyze the contribution ofthe RL system in isolation of the inherent limitationsof the base extractor.RL models We perform experiments using threevariants of RL agents ?
(1) RL-Basic, which per-forms only reconciliation decisions13, (2) RL-Query,which takes only query decisions with the reconcil-iation strategy fixed (similar to Kanani and McCal-lum (2012)), and (3) RL-Extract, our full system in-corporating both reconciliation and query decisions.We train the models for 10000 steps every epochusing the Maxent classifier as the base extractor, andevaluate on the entire test set every epoch.
The finalaccuracies reported are averaged over 3 independentruns; each run?s score is averaged over 20 epochs af-ter 100 epochs of training.
The penalty per step is setto -0.001.
For the DQN, we use the dev set to tuneall parameters.
We used a replay memory D of size500k, and a discount (?)
of 0.8.
We set the learn-ing rate to 2.5E?5.
The  in -greedy exploration isannealed from 1 to 0.1 over 500k transitions.
Thetarget-Q network is updated every 5k steps.6 ResultsPerformance Table 3 demonstrates that our sys-tem (RL-Extract) obtains a substantial gain in ac-curacy over the basic extractors on all entity typesover both domains.
For instance, RL-Extract is11.4% more accurate than the basic Maxent extrac-tor on City and 7.1% better on NumKilled, whilealso achieving gains of more than 5% on the otherentities on the Shootings domain.
The gains onthe Adulteration dataset are also significant, up toa 11.5% increase on the Location entity.We can also observe that simple aggregationschemes like the Confidence and Majority base-lines don?t handle the complexity of the task well.RL-Extract outperforms these by 7.2% on Shoot-ings and 5% on Adulteration averaged over all enti-ties.
Further, the importance of sequential decision-making is established by RL-Extract performing sig-nificantly better than the meta-classifier (7.0% onShootings over all entities).
This is also due to thefact that the meta-classifier aggregates over the en-tire set of extra documents, including the long tail ofnoisy, irrelevant documents.
Finally, we see the ad-vantage of enabling the RL system to select queriesas our full model RL-Extract obtains significant im-13Articles are presented to the agent in a round-robin fashionfrom the different query lists.2361System Shootings AdulterationShooterName NumKilled NumWounded City Food Adulterant LocationCRF extractor 9.5 65.4 64.5 47.9 41.2 28.3 51.7Maxent extractor 45.2 69.7 68.6 53.7 56.0 52.7 67.8Confidence Agg.
(? )
45.2 (0.6) 70.3 (0.6) 72.3 (0.6) 55.8 (0.6) 56.0 (0.8) 54.0 (0.8) 69.2 (0.6)Majority Agg.
(? )
47.6 (0.6) 69.1 (0.9) 68.6 (0.9) 54.7 (0.7) 56.7 (0.5) 50.6 (0.95) 72.0 (0.4)Meta-classifier 45.2 70.7 68.4 55.3 55.4 52.7 72.0RL-Basic 45.2 71.2 70.1 54.0 57.0 55.1 76.1RL-Query (conf) 39.6 66.6 69.4 44.4 39.4 35.9 66.4RL-Extract 50.0 77.6?
74.6?
65.6?
59.6?
58.9?
79.3?ORACLE 57.1 86.4 83.3 71.8 64.8 60.8 83.9Table 3: Accuracy of various baselines (italics), our system (DQN) and the Oracle on Shootings and Adulterationdatasets.
Agg.
refers to aggregation baselines.
Bold indicates best system scores.
?statistical significance of p <0.0005 vs basic Maxent extractor using the Student-t test.
Numbers in parentheses indicate the optimal threshold (?
)for the aggregation baselines.
Confidence-based reconciliation was used for RL-Query.Entity System: Value ExampleShooterName Basic: StewartA source tells Channel 2 Action News that Thomas Lee has been arrested inMississippi ... Sgt .
Stewart Smith, with the Troup County Sheriff?s office, said.RL-Extract: Lee Lee is accused of killing his wife, Christie; ...NumKilled Basic: 0 Shooting leaves 25 year old Pittsfield man dead , 4 injuredRL-Extract: 1 One man is dead after a shooting Saturday night at the intersection of DeweyAvenue and Linden Street.NumWounded Basic: 0 Three people are dead and a fourth is in the hospital after a murder suicideRL-Extract: 1 3 dead, 1 injured in possible Fla. murder-suicideCity Basic: EnglewoodA 2 year old girl and four other people were wounded in a shooting in WestEnglewood Thursday night, police saidRL-Extract: Chicago At least 14 people were shot across Chicago between noon and 10:30 p.m.Thursday.
The last shooting left five people wounded.Table 4: Sample outputs (along with corresponding article snippets) on the Shootings domain showing correct predic-tions from RL-Extract where the basic extractor (Maxent) fails.0 20 40 60 80 100Epoch0.000.050.100.150.200.25Reward35404550556065707580Accuracy (%)Figure 4: Evolution of average reward (solidblack) and accuracy on various entities (dashedlines; red=ShooterName, magenta=NumKilled,blue=NumWounded, green=City) on the test set ofthe Shootings domain.provements over RL-Basic on both domains.
Thefull model also outperforms RL-Query, demonstrat-ing the importance of performing both query selec-tion and reconciliation in a joint fashion.Figure 4 shows the learning curve of the agent bymeasuring reward on the test set after each trainingepoch.
The reward improves gradually and the ac-curacy on each entity increases simultaneously.
Ta-ble 4 provides some examples where our model isable to extract the right values when the baselinefails.
One can see that in most cases this is due tothe model making use of articles with prototypicallanguage or articles containing the entities in readilyextractable form.Analysis We also analyze the importance of dif-ferent reconciliation schemes, rewards and context-vectors in RL-Extract on the Shootings domain (Ta-ble 5).
In addition to simple replacement (Re-2362Reconciliation Context Reward Accuracy Steps(RL-Extract) S K W CConfidence tf-idf Step 47.5 71.5 70.4 60.1 8.4Majority tf-idf Step 43.6 71.8 69.0 59.2 9.9Replace No context Step 44.4 77.1 72.5 63.4 8.0Replace Unigram Step 48.9 76.8 74.0 63.2 10.0Replace tf-idf Episode 42.6 62.3 68.9 52.7 6.8Replace tf-idf Step 50.0 77.6 74.6 65.6 9.4Table 5: Effect of using different reconciliation schemes, context-vectors, and rewards in our RL framework (Shoot-ings domain).
The last row is the overall best scheme (deviations from this are in italics).
Context refers to thetype of word counts used in the state vector to represent entity context.
Rewards are either per step or per episode.
(S: ShooterName, K: NumKilled, W: NumWounded, C: City, Steps: Average number of steps per episode)place), we also experiment with using Confidenceand Majority-based reconciliation schemes for RL-Extract.
We observe that the Replace scheme per-forms much better than the others (2-6% on all enti-ties) and believe this is because it provides the agentwith more flexibility in choosing the final values.From the same table, we see that using the tf-idf counts of context words as part of the state pro-vides better performance than using no context orusing simple unigram counts.
In terms of rewardstructure, providing rewards after each step is em-pirically found to be significantly better (>10% onaverage) compared to a single delayed reward perepisode.
The last column shows the average numberof steps per episode ?
the values range from 6.8 to10.0 steps for the different schemes.
The best sys-tem (RL-Extract with Replace, tf-idf and step-basedrewards) uses 9.4 steps per episode.7 ConclusionsIn this paper, we explore the task of acquiring andincorporating external evidence to improve informa-tion extraction accuracy for domains with limitedaccess to training data.
This process comprises is-suing search queries, extraction from new sourcesand reconciliation of extracted values, repeated untilsufficient evidence is obtained.
We use a reinforce-ment learning framework and learn optimal actionsequences to maximize extraction accuracy whilepenalizing extra effort.
We show that our model,trained as a deep Q-network, outperforms traditionalextractors by 7.2% and 5% on average on two differ-ent domains, respectively.
We also demonstrate theimportance of sequential decision-making by com-paring our model to a meta-classifier operating onthe same space, obtaining up to a 7% gain.AcknowledgementsWe thank David Alvarez, Tao Lei and Ramya Ra-makrishnan for helpful discussions and feedback,and the members of the MIT NLP group and theanonymous reviewers for their insightful comments.We also gratefully acknowledge support from aGoogle faculty award.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting relations from large plain-text collections.In Proceedings of the fifth ACM conference on Digitallibraries, pages 85?94.
ACM.Michele Banko, Eric Brill, Susan Dumais, and JimmyLin.
2002.
Askmsr: Question answering using theworldwide web.
In Proceedings of 2002 AAAI SpringSymposium on Mining Answers from Texts and Knowl-edge Bases, pages 7?9.Cosmin Adrian Bejan and Sanda Harabagiu.
2014.
Un-supervised event coreference resolution.
Computa-tional Linguistics, 40(2):311?347.Hai Leong Chieu and Hwee Tou Ng.
2002.
A maximumentropy approach to information extraction from semi-structured and free text.
In Proceedings of AAAI.Aron Culotta and Andrew McCallum.
2004.
Confidenceestimation for information extraction.
In Proceedingsof HLT-NAACL 2004: Short Papers, pages 109?112.Association for Computational Linguistics.Greg Durrett and Dan Klein.
2014.
A joint model for en-tity analysis: Coreference, typing, and linking.
Trans-2363actions of the Association for Computational Linguis-tics, 2:477?490.Oren Etzioni, Anthony Fader, Janara Christensen,Stephen Soderland, and Mausam Mausam.
2011.Open information extraction: The second generation.In IJCAI, volume 11, pages 3?10.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1535?1545.
Association for Computational Linguis-tics.Matt Gardner, Partha Talukdar, Jayant Krishnamurthy,and Tom Mitchell.
2014.
Incorporating vector spacesimilarity in random walk inference over knowledgebases.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 397?406, Doha, Qatar, October.
As-sociation for Computational Linguistics.Kelvin Guu, John Miller, and Percy Liang.
2015.Traversing knowledge graphs in vector space.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, pages 318?327,Lisbon, Portugal, September.
Association for Compu-tational Linguistics.Xianpei Han, Le Sun, and Jun Zhao.
2011.
Collectiveentity linking in web text: a graph-based method.
InProceedings of the 34th international ACM SIGIR con-ference on Research and development in InformationRetrieval, pages 765?774.
ACM.Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, LihongLi, Li Deng, and Mari Ostendorf.
2015.
Deep re-inforcement learning with an action space defined bynatural language.
arXiv preprint arXiv:1511.04636.Heng Ji and Ralph Grishman.
2011.
Knowledge basepopulation: Successful approaches and challenges.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies - Volume 1, HLT ?11, pages 1148?1158, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Pallika H Kanani and Andrew K McCallum.
2012.
Se-lecting actions for resource-bounded information ex-traction using reinforcement learning.
In Proceed-ings of the fifth ACM international conference on Websearch and data mining, pages 253?262.
ACM.Heeyoung Lee, Marta Recasens, Angel Chang, MihaiSurdeanu, and Dan Jurafsky.
2012.
Joint entity andevent coreference resolution across documents.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, EMNLP-CoNLL ?12, pages 489?500, Stroudsburg, PA, USA.Association for Computational Linguistics.Gideon S Mann and David Yarowsky.
2005.
Multi-fieldinformation extraction and cross-document fusion.
InProceedings of the 43rd annual meeting on associationfor computational linguistics, pages 483?490.
Associ-ation for Computational Linguistics.Volodymyr Mnih, Koray Kavukcuoglu, David Silver,Andrei A. Rusu, Joel Veness, Marc G. Bellemare,Alex Graves, Martin Riedmiller, Andreas K. Fidje-land, Georg Ostrovski, Stig Petersen, Charles Beat-tie, Amir Sadik, Ioannis Antonoglou, Helen King,Dharshan Kumaran, Daan Wierstra, Shane Legg, andDemis Hassabis.
2015.
Human-level control throughdeep reinforcement learning.
Nature, 518(7540):529?533, 02.Karthik Narasimhan, Tejas Kulkarni, and Regina Barzi-lay.
2015.
Language understanding for text-basedgames using deep reinforcement learning.
In Proceed-ings of the Conference on Empirical Methods in Natu-ral Language Processing.Arvind Neelakantan, Benjamin Roth, and Andrew Mc-Callum.
2015.
Compositional vector space modelsfor knowledge base completion.
In Proceedings of the53rd Annual Meeting of the Association for Computa-tional Linguistics and the 7th International Joint Con-ference on Natural Language Processing (Volume 1:Long Papers), pages 156?166, Beijing, China, July.Association for Computational Linguistics.Richard Socher, Danqi Chen, Christopher D Manning,and Andrew Ng.
2013.
Reasoning with neural ten-sor networks for knowledge base completion.
InAdvances in Neural Information Processing Systems,pages 926?934.Mihai Surdeanu, David McClosky, Julie Tibshirani, JohnBauer, Angel X Chang, Valentin I Spitkovsky, andChristopher D Manning.
2010.
A simple distant su-pervision approach for the tac-kbp slot filling task.
InProceedings of Text Analysis Conference 2010 Work-shop.Richard S Sutton and Andrew G Barto.
1998.
Introduc-tion to reinforcement learning.
MIT Press.Tijmen Tieleman and Geoffrey Hinton.
2012.
Lecture6.5-rmsprop: Divide the gradient by a running aver-age of its recent magnitude.
COURSERA: Neural Net-works for Machine Learning, 4.Christopher JCH Watkins and Peter Dayan.
1992.
Q-learning.
Machine learning, 8(3-4):279?292.Robert West, Evgeniy Gabrilovich, Kevin Murphy, Shao-hua Sun, Rahul Gupta, and Dekang Lin.
2014.Knowledge base completion via search-based questionanswering.
In Proceedings of the 23rd internationalconference on World wide web, pages 515?526.
ACM.Fei Wu and Daniel S Weld.
2010.
Open information ex-traction using wikipedia.
In Proceedings of the 48th2364Annual Meeting of the Association for ComputationalLinguistics, pages 118?127.
Association for Computa-tional Linguistics.Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao,and Li Deng.
2014.
Embedding entities and relationsfor learning and inference in knowledge bases.
arXivpreprint arXiv:1412.6575.2365
