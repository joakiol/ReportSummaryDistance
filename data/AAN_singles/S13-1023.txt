Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 162?168, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsLIPN-CORE: Semantic Text Similarity using n-grams, WordNet, SyntacticAnalysis, ESA and Information Retrieval based FeaturesDavide Buscaldi, Joseph Le Roux,Jorge J.
Garc?
?a FloresLaboratoire d?Informatique de Paris Nord,CNRS, (UMR 7030)Universite?
Paris 13, Sorbonne Paris Cite?,F-93430, Villetaneuse, France{buscaldi,joseph.le-roux,jgflores}@lipn.univ-paris13.frAdrian PopescuCEA, LIST,Vision & ContentEngineering LaboratoryF-91190 Gif-sur-Yvette, Franceadrian.popescu@cea.frAbstractThis paper describes the system used by theLIPN team in the Semantic Textual Similaritytask at *SEM 2013.
It uses a support vector re-gression model, combining different text sim-ilarity measures that constitute the features.These measures include simple distances likeLevenshtein edit distance, cosine, Named En-tities overlap and more complex distances likeExplicit Semantic Analysis, WordNet-basedsimilarity, IR-based similarity, and a similar-ity measure based on syntactic dependencies.1 IntroductionThe Semantic Textual Similarity task (STS) at*SEM 2013 requires systems to grade the degree ofsimilarity between pairs of sentences.
It is closelyrelated to other well known tasks in NLP such as tex-tual entailment, question answering or paraphrasedetection.
However, as noticed in (Ba?r et al 2012),the major difference is that STS systems must give agraded, as opposed to binary, answer.One of the most successful systems in *SEM2012 STS, (Ba?r et al 2012), managed to grade pairsof sentences accurately by combining focused mea-sures, either simple ones based on surface features(ie n-grams), more elaborate ones based on lexicalsemantics, or measures requiring external corporasuch as Explicit Semantic Analysis, into a robustmeasure by using a log-linear regression model.The LIPN-CORE system is built upon this idea ofcombining simple measures with a regression modelto obtain a robust and accurate measure of tex-tual similarity, using the individual measures as fea-tures for the global system.
These measures includesimple distances like Levenshtein edit distance, co-sine, Named Entities overlap and more complex dis-tances like Explicit Semantic Analysis, WordNet-based similarity, IR-based similarity, and a similar-ity measure based on syntactic dependencies.The paper is organized as follows.
Measures arepresented in Section 2.
Then the regression model,based on Support Vector Machines, is described inSection 3.
Finally we discuss the results of the sys-tem in Section 4.2 Text Similarity Measures2.1 WordNet-based Conceptual Similarity(Proxigenea)First of all, sentences p and q are analysed in or-der to extract all the included WordNet synsets.
Foreach WordNet synset, we keep noun synsets and putinto the set of synsets associated to the sentence, Cpand Cq, respectively.
If the synsets are in one of theother POS categories (verb, adjective, adverb) welook for their derivationally related forms in orderto find a related noun synset: if there is one, we putthis synsets in Cp (or Cq).
For instance, the word?playing?
can be associated in WordNet to synset(v)play#2, which has two derivationally relatedforms corresponding to synsets (n)play#5 and(n)play#6: these are the synsets that are addedto the synset set of the sentence.
No disambiguationprocess is carried out, so we take all possible mean-ings into account.GivenCp andCq as the sets of concepts containedin sentences p and q, respectively, with |Cp| ?
|Cq|,162the conceptual similarity between p and q is calcu-lated as:ss(p, q) =?c1?Cpmaxc2?Cqs(c1, c2)|Cp|(1)where s(c1, c2) is a conceptual similarity measure.Concept similarity can be calculated by differentways.
For the participation in the 2013 Seman-tic Textual Similarity task, we used a variation ofthe Wu-Palmer formula (Wu and Palmer, 1994)named ?ProxiGenea?
(from the french Proximite?Ge?ne?alogique, genealogical proximity), introducedby (Dudognon et al 2010), which is inspired by theanalogy between a family tree and the concept hi-erarchy in WordNet.
Among the different formula-tions proposed by (Dudognon et al 2010), we chosethe ProxiGenea3 variant, already used in the STS2012 task by the IRIT team (Buscaldi et al 2012).The ProxiGenea3 measure is defined as:s(c1, c2) =11 + d(c1) + d(c2)?
2 ?
d(c0)(2)where c0 is the most specific concept that is presentboth in the synset path of c1 and c2 (that is, the LeastCommon Subsumer or LCS).
The function returningthe depth of a concept is noted with d.2.2 IC-based SimilarityThis measure has been proposed by (Mihalcea etal., 2006) as a corpus-based measure which usesResnik?s Information Content (IC) and the Jiang-Conrath (Jiang and Conrath, 1997) similarity metric:sjc(c1, c2) =1IC(c1) + IC(c2)?
2 ?
IC(c0)(3)where IC is the information content introduced by(Resnik, 1995) as IC(c) = ?
logP (c).The similarity between two text segments T1 andT2 is therefore determined as:sim(T1, T2) =12????w?{T1}maxw2?
{T2}ws(w,w2) ?
idf(w)?w?{T1}idf(w)+?w?{T2}maxw1?
{T1}ws(w,w1) ?
idf(w)?w?{T2}idf(w)???
(4)where idf(w) is calculated as the inverse documentfrequency of word w, taking into account GoogleWeb 1T (Brants and Franz, 2006) frequency counts.The semantic similarity between words is calculatedas:ws(wi, wj) = maxci?Wi,cjinWjsjc(ci, cj).
(5)where Wi and Wj are the sets containing all synsetsin WordNet corresponding to word wi and wj , re-spectively.
The IC values used are those calcu-lated by Ted Pedersen (Pedersen et al 2004) on theBritish National Corpus1.2.3 Syntactic DependenciesWe also wanted for our systems to take syntac-tic similarity into account.
As our measures arelexically grounded, we chose to use dependen-cies rather than constituents.
Previous experimentsshowed that converting constituents to dependen-cies still achieved best results on out-of-domaintexts (Le Roux et al 2012), so we decided to usea 2-step architecture to obtain syntactic dependen-cies.
First we parsed pairs of sentences with theLORG parser2.
Second we converted the resultingparse trees to Stanford dependencies3.Given the sets of parsed dependenciesDp andDq,for sentence p and q, a dependency d ?
Dx is atriple (l, h, t) where l is the dependency label (for in-stance, dobj or prep), h the governor and t the depen-dant.
We define the following similarity measure be-tween two syntactic dependencies d1 = (l1, h1, t1)and d2 = (l2, h2, t2):dsim(d1, d2) = Lev(l1, l2)?idfh ?
sWN (h1, h2) + idft ?
sWN (t1, t2)2(6)where idfh = max(idf(h1), idf(h2)) and idft =max(idf(t1), idf(t2)) are the inverse document fre-quencies calculated on Google Web 1T for the gov-ernors and the dependants (we retain the maximumfor each pair), and sWN is calculated using formula2, with two differences:?
if the two words to be compared are antonyms,then the returned score is 0;1http://www.d.umn.edu/?tpederse/similarity.html2https://github.com/CNGLdlab/LORG-Release3We used the default built-in converter provided with theStanford Parser (2012-11-12 revision).163?
if one of the words to be compared is not inWordNet, their similarity is calculated usingthe Levenshtein distance.The similarity score between p and q, is then cal-culated as:sSD(p, q) = max???
?di?DpmaxdjinDqdsim(di, dj)|Dp|,?di?DqmaxdjinDpdsim(di, dj)|Dq|???
(7)2.4 Information Retrieval-based SimilarityLet us consider two texts p and q, an Information Re-trieval (IR) system S and a document collection Dindexed by S. This measure is based on the assump-tion that p and q are similar if the documents re-trieved by S for the two texts, used as input queries,are ranked similarly.Let be Lp = {dp1 , .
.
.
, dpK} and Lq ={dq1 , .
.
.
, dqK}, dxi ?
D the sets of the top K docu-ments retrieved by S for texts p and q, respectively.Let us define sp(d) and sq(d) the scores assigned byS to a document d for the query p and q, respectively.Then, the similarity score is calculated as:simIR(p, q) = 1??d?Lp?Lq?
(sp(d)?sq(d))2max(sp(d),sq(d))|Lp ?
Lq|(8)if |Lp ?
Lq| 6= ?, 0 otherwise.For the participation in this task we indexed acollection composed by the AQUAINT-24 and theEnglish NTCIR-85 document collections, using theLucene6 4.2 search engine with BM25 similarity.The K value was empirically set to 20 after sometests on the STS 2012 data.2.5 ESAExplicit Semantic Analysis (Gabrilovich andMarkovitch, 2007) represents meaning as a4http://www.nist.gov/tac/data/data_desc.html#AQUAINT-25http://metadata.berkeley.edu/NTCIR-GeoTime/ntcir-8-databases.php6http://lucene.apache.org/coreweighted vector of Wikipedia concepts.
Weightsare supposed to quantify the strength of the relationbetween a word and each Wikipedia concept usingthe tf-idf measure.
A text is then represented as ahigh-dimensional real valued vector space spanningall along the Wikipedia database.
For this particulartask we adapt the research-esa implementation(Sorg and Cimiano, 2008)7 to our own home-madeweighted vectors corresponding to a Wikipediasnapshot of February 4th, 2013.2.6 N-gram based SimilarityThis feature is based on the Clustered Keywords Po-sitional Distance (CKPD) model proposed in (Bus-caldi et al 2009) for the passage retrieval task.The similarity between a text fragment p and an-other text fragment q is calculated as:simngrams(p, q) =?
?x?Qh(x, P )1d(x, xmax)?ni=1wi(9)Where P is the set of n-grams with the highestweight in p, where all terms are also contained in q;Q is the set of all the possible n-grams in q and nis the total number of terms in the longest passage.The weights for each term and each n-gram are cal-culated as:?
wi calculates the weight of the term tI as:wi = 1?log(ni)1 + log(N)(10)Where ni is the frequency of term ti in theGoogle Web 1T collection, and N is the fre-quency of the most frequent term in the GoogleWeb 1T collection.?
the function h(x, P ) measures the weight ofeach n-gram and is defined as:h(x, Pj) ={ ?jk=1wk if x ?
Pj0 otherwise(11)7http://code.google.com/p/research-esa/164Where wk is the weight of the k-th term (seeEquation 10) and j is the number of terms thatcompose the n-gram x;?
1d(x,xmax) is a distance factor which reduces theweight of the n-grams that are far from theheaviest n-gram.
The function d(x, xmax) de-termines numerically the value of the separa-tion according to the number of words betweena n-gram and the heaviest one:d(x, xmax) = 1 + k?
ln(1 + L) (12)where k is a factor that determines the impor-tance of the distance in the similarity calcula-tion and L is the number of words between an-gram and the heaviest one (see Equation 11).In our experiments, k was set to 0.1, the defaultvalue in the original model.2.7 Other measuresIn addition to the above text similarity measures, weused also the following common measures:2.7.1 CosineGiven p = (wp1 , .
.
.
, wpn) and q =(wq1 , .
.
.
, wqn) the vectors of tf.idf weights asso-ciated to sentences p and q, the cosine distance iscalculated as:simcos(p,q) =n?i=1wpi ?
wqi?n?i=1wpi2 ?
?n?i=1wqi2(13)The idf value was calculated on Google Web 1T.2.7.2 Edit DistanceThis similarity measure is calculated using theLevenshtein distance as:simED(p, q) = 1?Lev(p, q)max(|p|, |q|)(14)where Lev(p, q) is the Levenshtein distance be-tween the two sentences, taking into account thecharacters.2.7.3 Named Entity OverlapWe used the Stanford Named Entity Recognizerby (Finkel et al 2005), with the 7 class modeltrained for MUC: Time, Location, Organization,Person, Money, Percent, Date.
Then we calculated aper-class overlap measure (in this way, ?France?
asan Organization does not match ?France?
as a Loca-tion):ONER(p, q) =2 ?
|Np ?Nq||Np|+ |Nq|(15)where Np and Nq are the sets of NEs found, respec-tively, in sentences p and q.3 Integration of Similarity MeasuresThe integration has been carried out using the?-Support Vector Regression model (?-SVR)(Scho?lkopf et al 1999) implementation providedby LIBSVM (Chang and Lin, 2011), with a radialbasis function kernel with the standard parameters(?
= 0.5).4 ResultsIn order to evaluate the impact of the different fea-tures, we carried out an ablation test, removing onefeature at a time and training a new model with thereduced set of features.
In Table 2 we show the re-sults of the ablation test for each subset of the *SEM2013 test set; in Table 1 we show the same test on thewhole test set.
Note: the results have been calculatedas the Pearson correlation test on the whole test setand not as an average of the correlation scores cal-culated over the composing test sets.Feature Removed Pearson LossNone 0.597 0N-grams 0.596 0.10%WordNet 0.563 3.39%SyntDeps 0.602 ?0.43%Edit 0.584 1.31%Cosine 0.596 0.10%NE Overlap 0.603 ?0.53%IC-based 0.598 ?0.10%IR-Similarity 0.510 8.78%ESA 0.601 ?0.38%Table 1: Ablation test for the different features on thewhole 2013 test set.165FNWN Headlines OnWN SMTFeature Pearson Loss Pearson Loss Pearson Loss Pearson LossNone 0.404 0 0.706 0 0.694 0 0.301 0N-grams 0.379 2.49% 0.705 0.12% 0.698 ?0.44% 0.289 1.16%WordNet 0.376 2.80% 0.695 1.09% 0.682 1.17% 0.278 2.28%SyntDeps 0.403 0.08% 0.699 0.70% 0.679 1.49% 0.284 1.62%Edit 0.402 0.19% 0.689 1.70% 0.667 2.72% 0.286 1.50%Cosine 0.393 1.03% 0.683 2.38% 0.676 1.80% 0.303 ?0.24%NE Overlap 0.410 ?0.61% 0.700 0.67% 0.680 1.37% 0.285 1.58%IC-based 0.391 1.26% 0.699 0.75% 0.669 2.50% 0.283 1.76%IR-Similarity 0.426 ?2.21% 0.633 7.33% 0.589 10.46% 0.249 5.19%ESA 0.391 1.22% 0.691 1.57% 0.702 ?0.81% 0.275 2.54%Table 2: Ablation test for the different features on the different parts of the 2013 test set.FNWN Headlines OnWN SMT ALLN-grams 0.285 0.532 0.459 0.280 0.336WordNet 0.395 0.606 0.552 0.282 0.477SyntDeps 0.233 0.409 0.345 0.323 0.295Edit 0.220 0.536 0.089 0.355 0.230Cosine 0.306 0.573 0.541 0.244 0.382NE Overlap 0.000 0.216 0.000 0.013 0.020IC-based 0.413 0.540 0.642 0.285 0.421IR-based 0.067 0.598 0.628 0.241 0.541ESA 0.328 0.546 0.322 0.289 0.390Table 3: Pearson correlation calculated on individual features.The ablation test show that the IR-based featureshowed up to be the most effective one, especiallyfor the headlines subset (as expected), and, quite sur-prisingly, on the OnWN data.
In Table 3 we showthe correlation between each feature and the result(feature values normalised between 0 and 5): fromthis table we can also observe that, on average, IR-based similarity was better able to capture the se-mantic similarity between texts.
The only exceptionwas the FNWN test set: the IR-based similarity re-turned a 0 score 178 times out of 189 (94.1%), indi-cating that the indexed corpus did not fit the contentof the FNWN sentences.
This result shows also thelimits of the IR-based similarity score which needsa large corpus to achieve enough coverage.4.1 Shared submission with INAOE-UPVOne of the files submitted by INAOE-UPV,INAOE-UPV-run3 has been produced using sevenfeatures produced by different teams: INAOE, LIPNand UMCC-DLSI.
We contributed to this joint sub-mission with the IR-based, WordNet and cosine fea-tures.5 Conclusions and Further WorkIn this paper we introduced the LIPN-CORE sys-tem, which combines semantic, syntactic an lexi-cal measures of text similarity in a linear regressionmodel.
Our system was among the best 15 runs forthe STS task.
According to the ablation test, the bestperforming feature was the IR-based one, where asentence is considered as a query and its meaningrepresented as a set of documents indexed by an IRsystem.
The second and third best-performing mea-sures were WordNet similarity and Levenshtein?sedit distance.
On the other hand, worst perform-ing similarity measures were Named Entity Over-lap, Syntactic Dependencies and ESA.
However, acorrelation analysis calculated on the features takenone-by-one shows that the contribution of a feature166on the overall regression result does not correspondto the actual capability of the measure to representthe semantic similarity between the two texts.
Theseresults raise the methodological question of how tocombine semantic, syntactic and lexical similaritymeasures in order to estimate the impact of the dif-ferent strategies used on each dataset.Further work will include richer similarity mea-sures, like quasi-synchronous grammars (Smith andEisner, 2006) and random walks (Ramage et al2009).
Quasi-synchronous grammars have beenused successfully for paraphrase detection (Das andSmith, 2009), as they provide a fine-grained model-ing of the alignment of syntactic structures, in a veryflexible way, enabling partial alignments and the in-clusion of external features, like Wordnet lexical re-lations for example.
Random walks have been usedeffectively for paraphrase recognition and as a fea-ture for recognizing textual entailment.
Finally, wewill continue analyzing the question of how to com-bine a wide variety of similarity measures in such away that they tackle the semantic variations of eachdataset.AcknowledgmentsWe would like to thank the Quaero project and theLabEx EFL8 for their support to this work.References[Ba?r et al012] Daniel Ba?r, Chris Biemann, IrynaGurevych, and Torsten Zesch.
2012.
Ukp: Computingsemantic textual similarity by combining multiplecontent similarity measures.
In Proceedings of the6th International Workshop on Semantic Evaluation,held in conjunction with the 1st Joint Conferenceon Lexical and Computational Semantics, pages435?440, Montreal, Canada, June.
[Brants and Franz2006] Thorsten Brants and Alex Franz.2006.
Web 1t 5-gram corpus version 1.1.
[Buscaldi et al009] Davide Buscaldi, Paolo Rosso,Jose?
Manuel Go?mez, and Emilio Sanchis.
2009.
An-swering questions with an n-gram based passage re-trieval engine.
Journal of Intelligent Information Sys-tems (JIIS), 34(2):113?134.
[Buscaldi et al012] Davide Buscaldi, Ronan Tournier,Nathalie Aussenac-Gilles, and Josiane Mothe.
2012.8http://www.labex-efl.orgIrit: Textual similarity combining conceptual simi-larity with an n-gram comparison method.
In Pro-ceedings of the 6th International Workshop on Se-mantic Evaluation (SemEval 2012), Montreal, Que-bec, Canada.
[Chang and Lin2011] Chih-Chung Chang and Chih-JenLin.
2011.
LIBSVM: A library for support vectormachines.
ACM Transactions on Intelligent Systemsand Technology, 2:27:1?27:27.
Software availableat http://www.csie.ntu.edu.tw/?cjlin/libsvm.
[Das and Smith2009] Dipanjan Das and Noah A. Smith.2009.
Paraphrase identification as probabilistic quasi-synchronous recognition.
In Proc.
of ACL-IJCNLP.
[Dudognon et al010] Damien Dudognon, Gilles Hubert,and Bachelin Jhonn Victorino Ralalason.
2010.Proxige?ne?a : Une mesure de similarite?
conceptuelle.In Proceedings of the Colloque Veille Strate?gique Sci-entifique et Technologique (VSST 2010).
[Finkel et al005] Jenny Rose Finkel, Trond Grenager,and Christopher Manning.
2005.
Incorporating non-local information into information extraction systemsby gibbs sampling.
In Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics, ACL ?05, pages 363?370, Stroudsburg, PA,USA.
Association for Computational Linguistics.
[Gabrilovich and Markovitch2007] Evgeniy Gabrilovichand Shaul Markovitch.
2007.
Computing seman-tic relatedness using wikipedia-based explicit semanticanalysis.
In Proceedings of the 20th international jointconference on Artifical intelligence, IJCAI?07, pages1606?1611, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.[Jiang and Conrath1997] J.J. Jiang and D.W. Conrath.1997.
Semantic similarity based on corpus statisticsand lexical taxonomy.
In Proc.
of the Int?l.
Conf.
onResearch in Computational Linguistics, pages 19?33.
[Le Roux et al012] Joseph Le Roux, Jennifer Foster,Joachim Wagner, Rasul Samad Zadeh Kaljahi, andAnton Bryl.
2012.
DCU-Paris13 Systems for theSANCL 2012 Shared Task.
In The NAACL 2012 FirstWorkshop on Syntactic Analysis of Non-CanonicalLanguage (SANCL), pages 1?4, Montre?al, Canada,June.
[Mihalcea et al006] Rada Mihalcea, Courtney Corley,and Carlo Strapparava.
2006.
Corpus-based andknowledge-based measures of text semantic similarity.In Proceedings of the 21st national conference on Ar-tificial intelligence - Volume 1, AAAI?06, pages 775?780.
AAAI Press.
[Pedersen et al004] Ted Pedersen, Siddharth Patward-han, and Jason Michelizzi.
2004.
Wordnet::similarity:measuring the relatedness of concepts.
In Demon-stration Papers at HLT-NAACL 2004, HLT-NAACL?167Demonstrations ?04, pages 38?41, Stroudsburg, PA,USA.
Association for Computational Linguistics.
[Ramage et al009] Daniel Ramage, Anna N. Rafferty,and Christopher D. Manning.
2009.
Random walksfor text semantic similarity.
In Proceedings of the2009 Workshop on Graph-based Methods for NaturalLanguage Processing, pages 23?31.
The Associationfor Computer Linguistics.
[Resnik1995] Philip Resnik.
1995.
Using informationcontent to evaluate semantic similarity in a taxonomy.In Proceedings of the 14th international joint confer-ence on Artificial intelligence - Volume 1, IJCAI?95,pages 448?453, San Francisco, CA, USA.
MorganKaufmann Publishers Inc.[Scho?lkopf et al999] Bernhard Scho?lkopf, PeterBartlett, Alex Smola, and Robert Williamson.
1999.Shrinking the tube: a new support vector regressionalgorithm.
In Proceedings of the 1998 conference onAdvances in neural information processing systems II,pages 330?336, Cambridge, MA, USA.
MIT Press.
[Smith and Eisner2006] David A. Smith and Jason Eisner.2006.
Quasi-synchronous grammars: Alignment bysoft projection of syntactic dependencies.
In Proceed-ings of the HLT-NAACL Workshop on Statistical Ma-chine Translation, pages 23?30, New York, June.
[Sorg and Cimiano2008] Philipp Sorg and Philipp Cimi-ano.
2008.
Cross-lingual Information Retrieval withExplicit Semantic Analysis.
In Working Notes for theCLEF 2008 Workshop.
[Wu and Palmer1994] Zhibiao Wu and Martha Palmer.1994.
Verbs semantics and lexical selection.
In Pro-ceedings of the 32nd annual meeting on Associationfor Computational Linguistics, ACL ?94, pages 133?138, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.168
