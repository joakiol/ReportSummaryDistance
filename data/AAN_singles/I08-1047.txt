Minimally Supervised Learning of Semantic Knowledgefrom Query LogsMamoru Komachi                                        Hisami SuzukiNara Institute of Science and Technology                      Microsoft Research8916-5 Takayama                                      One Microsoft WayIkoma, Nara 630-0192, Japan                          Redmond, WA 98052 USAmamoru-k@is.naist.jp            hisamis@microsoft.comAbstractWe propose a method for learning semanticcategories of words with minimal supervi-sion from web search query logs.
Our me-thod is based on the Espresso algorithm(Pantel and Pennacchiotti, 2006) for ex-tracting binary lexical relations, but makesimportant modifications to handle querylog data for the task of acquiring semanticcategories.
We present experimental resultscomparing our method with two state-of-the-art minimally supervised lexical know-ledge extraction systems using Japanesequery log data, and show that our methodachieves higher precision than the pre-viously proposed methods.
We also showthat the proposed method offers an addi-tional advantage for knowledge acquisitionin an Asian language for which word seg-mentation is an issue, as the method utiliz-es no prior knowledge of word segmenta-tion, and is able to harvest new terms withcorrect word segmentation.1 IntroductionExtraction of lexical knowledge from a large col-lection of text data with minimal supervision hasbecome an active area of research in recent years.Automatic extraction of relations by exploitingrecurring patterns in text was pioneered by Hearst(1992), who describes a bootstrapping procedurefor extracting words in the hyponym (is-a) relation,starting with three manually given lexico-syntacticpatterns.
This idea of learning with a minimallysupervised bootstrapping method using surface textpatterns was subsequently adopted for many tasks,including relation extraction (e.g., Brin, 1998; Ri-loff and Jones, 1999; Pantel and Pennacchiotti,2006) and named entity recognition (e.g., Collinsand Singer, 1999; Etzioni et al, 2005).In this paper, we describe a method of learningsemantic categories of words using a large collec-tion of Japanese search query logs.
Our method isbased on the Espresso algorithm (Pantel and Pen-nacchiotti, 2006) for extracting binary lexical rela-tions, adapting it to work well on learning unaryrelations from query logs.
The use of query data asa source of knowledge extraction offers someunique advantages over using regular text.?
Web search queries capture the interest of searchusers directly, while the distribution of the Webdocuments do not necessarily reflect the distri-bution of  what people search (Silverstein et al,1998).
The word categories acquired from querylogs are thus expected to be more useful for thetasks related to search.?
Though user-generated queries are often veryshort, the words that appear in queries are gen-erally highly relevant for the purpose of wordclassification.?
Many search queries consist of keywords, whichmeans that the queries include word segmenta-tion specified by users.
This is a great source ofknowledge for learning word boundaries forthose languages whose regularly written textdoes not indicate word boundaries, such as Chi-nese and Japanese.Although our work naturally fits into the largergoal of building knowledge bases automaticallyfrom text, to our knowledge we are the first to ex-plore the use of Japanese query logs for the pur-pose of minimally supervised semantic categoryacquisition.
Our work is similar to Sekine and Su-zuki (2007), whose goal is to augment a manuallycreated dictionary of named entities by finding358contextual patterns from English query logs.
Ourwork is different in that it does not require a full-scale list of categorized named entities but a smallnumber of seed words, and iterates over the data toextract more patterns and instances.
Recent workby Pa?ca (2007) and Pa?ca and Van Durme (2007)also uses English query logs to extract lexicalknowledge, but their focus is on learning attributesfor named entities, a different focus from ours.2 Related WorkIn this section, we describe three state-of-the-artalgorithms of relation extraction, which serve asthe baseline for our work.
They are briefly summa-rized in Table 1.
The goal of these algorithms is tolearn target instances, which are the words belong-ing to certain categories (e.g., cat for the Animalclass), or in the case of relation extraction, thepairs of words standing in a particular relationship(e.g., pasta::food for is-a relationship), given thecontext patterns for the categories or relation typesfound in source data.2.1 Pattern InductionThe first step toward the acquisition of instances isto extract context patterns.
In previous work, theseare surface text patterns, e.g., X such as Y, for ex-tracting words in an is-a relation, with some heu-ristics for finding the pattern boundaries in text.
Aswe use query logs as the source of knowledge, wesimply used everything but the instance string in aquery as the pattern for the instance, in a mannersimilar to Pa?ca et al (2006).
For example, theseed word JAL in the query ?JAL+flight_schedule?yields the pattern "#+flight_schedule".1 Note thatwe perform no word segmentation or boundarydetection heuristics in identifying these patterns,which makes our approach fast and robust, as the1 # indicates where the instance occurs in the querystring, and + indicates a white space in the original Jap-anese query.
The underscore symbol (_) means therewas originally no white space; it is used merely to makethe translation in English more readable.2 The manual classification assigns only one categorysegmentation errors introduce noise in extractedpatterns, especially when the source data containsmany out of vocabulary items.The extracted context patterns must then be as-signed a score reflecting their usefulness in extract-ing the instances of a desired type.
Frequency is apoor metric here, because frequent patterns may beextremely generic, appearing across multiple cate-gories.
Previously proposed methods differ in howto assign the desirability scores to the patterns theyfind and in using the score to extract instances, aswell as in the treatment of generic patterns, whoseprecision is low but whose recall is high.2.2 Sekine and Suzuki (2007)?s AlgorithmFor the purpose of choosing the set of context pat-terns that best characterizes the categories, Sekineand Suzuki (2007) report that none of the conven-tional co-occurrence metrics such as tf.idf, mutualinformation and chi-squared tests achieved goodresults on their task, and propose a new measure,which is based on the number of different instancesof the category a context c co-occurs with,lized by its token frequency for all categories:CcgfcScore type )(log)( ?)1000()1000()()()(ctopFctopfCcFcfcginsttypeinsttype?
?where ftype is the type frequency of instance termsthat c co-occurs with in the category, Finst is thetoken frequency of context c in the entire data andctop1000 is the 1000 most frequent contexts.
Sincethey start with a large and reliable named entitydictionary, and can therefore use several hundredseed terms, they simply used the top-k highest-scoring contexts and extracted new named entitiesonce and for all, without iteration.
Generic patternsreceive low scores, and are therefore ignored bythis algorithm.2.3 The Basilisk AlgorithmThelen and Riloff (2002) present a frameworkcalled Basilisk, which extracts semantic lexicons# of seed Target # of iteration Corpus LanguageSekine & Suzuki ~600 Categorized NEs 1 Query log EnglishBasilisk 10 Semantic lexicon ?
MUC-4 EnglishEspresso ~10 Semantic relations ?
TREC EnglishTchai 5 Categorized words ?
Query log JapaneseTable 1: Summary of algorithms359for multiple categories.
It starts with a small set ofseed words and finds all patterns that match theseseed words in the corpus.
The bootstrappingprocess begins by selecting a subset of the patternsby the RlogF metric (Riloff, 1996):)log()(log iiii FNFpatternFR ?
?where Fi is the number of category members ex-tracted by patterni and Ni is the total number ofinstances extracted by patterni.
It then identifiesinstances by these patterns and scores each in-stance by the following formula:iPjji PFwordAvgLogi????
1)1log()(where Pi is the number of patterns that extractwordi.
They use the average logarithm to selectinstances to balance the recall and precision of ge-neric patterns.
They add five best instances to thelexicon according to this formula, and the boot-strapping process starts again.
Instances are cumu-latively collected across iterations, while patternsare discarded at the end of each iteration.2.4 The Espresso AlgorithmWe will discuss the Espresso framework (Panteland Pennacchiotti, 2006) in some detail becauseour method is based on it.
It is a general-purpose,minimally supervised bootstrapping algorithm thattakes as input a few seed instances and iterativelylearns surface patterns to extract more instances.The key to Espresso lies in its use of generic pat-terns: Pantel and Pennacchiotti (2006) assume thatcorrect instances captured by a generic pattern willalso be instantiated by some reliable patterns,which denote high precision and low recall pat-terns.Espresso starts from a small set of seed in-stances of a binary relation, finds a set of surfacepatterns P, selects the top-k patterns, extracts thehighest scoring m instances, and repeats theprocess.
Espresso ranks all patterns in P accordingto reliability r?, and retains the top-k patterns forinstance extraction.
The value of k is incrementedby one after each iteration.The reliability of a pattern p is based on the in-tuition that a reliable pattern co-occurs with manyreliable instances.
They use pointwise mutual in-formation (PMI) and define the reliability of a pat-tern p as its average strength of association acrosseach input instance i in the set of instances I,weighted by the reliability of each instance i:Iirpipmipr Ii pm i??
??????????)(max),()(?
?where r?
(i) is the reliability of the instance i  andmaxpmi is the maximum PMI between all patternsand all instances.
The PMI between instance i ={x,y} and pattern p  is estimated by:,**,,*,,,log),( pyxypxpipmi ?where ypx ,, is the frequency of pattern p instan-tiated with terms x and y (recall that Espresso istargeted at extracting binary relations) and wherethe asterisk represents a wildcard.
They multipliedpmi(i,p) with the discounting factor suggested inPantel and Ravichandran (2004) to alleviate a biastowards infrequent events.The reliability of an instance is defined similar-ly: a reliable instance is one that associates with asmany reliable patterns as possible.Pprpipmiir Pp pm i??
??????????)(max),()(?
?where r?
(p) is the reliability of pattern p, and P isthe set of surface patterns.
Note that r?
(i) and r?
(p)are recursively defined: the computation of the pat-tern and instance reliability alternates between per-forming pattern reranking and instance extraction.Similarly to Basilisk, instances are cumulativelylearned, but patterns are discarded at the end ofeach iteration.3 The Tchai AlgorithmIn this section, we describe the modifications wemade to Espresso to derive our algorithm calledTchai.3.1 Filtering Ambiguous Instances and Pat-ternsAs mentioned above, the treatment of high-recall,low-precision generic patterns (e.g., #+map,#+animation) present a challenge to minimallysupervised learning algorithms due to their am-guity.
In the case of semantic category acquisition,the problem of ambiguity is exacerbated, becausenot only the acquired patterns, but also the in-stances can be highly ambiguous.
For example,360once we learn an ambiguous instance such as Po-kemon, it will start collecting patterns for multiplecategories (e.g., Game, Animation and Movie),which is not desirable.In order to control the negative effect of the ge-neric patterns, Espresso introduces a confidencemetric, which is similar but separate from the re-liability measure, and uses it to filter out the gener-ic patterns falling below a confidence threshold.
Inour experiments, however, this metric did not pro-duce a score that was substantially different fromthe reliability score.
Therefore, we did not use aconfidence metric, and instead opted for noting ambiguous instances and patterns, where wedefine ambiguous instance as one that inducesmore than 1.5 times the number of patterns ofviously accepted reliable instances, and ambiguous(or generic) pattern as one that extracts more thantwice the number of instances of previously ac-cepted reliable patterns.
As we will see in Section4, this modification improves the precision of theextracted instances, especially in the early stages ofiteration.3.2 Scaling Factor in Reliability ScoresAnother modification to the Espresso algorithm toreduce the power of generic patterns is to use localmaxpmi instead of global maxpmi.
Since PMI ranges[?
?, +?
], the point of dividing pmi(i,p) by maxpmiin Espresso is to normalize the reliability to [0, 1].However, using PMI directly to estimate the relia-bility of a pattern when calculating the reliabilityof an instance may lead to unexpected results be-cause the absolute value of PMI is highly variableacross instances and patterns.
We define the localmaxpmi of the reliability of an instance to be theabsolute value of the maximum PMI for a giveninstance, as opposed to taking the maximum for allinstances in a given iteration.
Local maxpmi of thereliability of a pattern is defined in the same way.As we show in the next section, this modificationhas a large impact on the effectiveness of our algo-rithm.3.3 Performance ImprovementsTchai, unlike Espresso, does not perform thepattern induction step between iterations; rather, itsimply recomputes the reliability of the patternsinduced at the beginning.
Our assumption is thatfairly reliable patterns will occur with at least oneof the seed instances if they occur frequentlyenough in query logs.
Since pattern induction iscomputationally expensive, this modificationreduces the computation time by a factor of 400.4 ExperimentIn this section, we present an empirical comparisonof Tchai with the systems described in Section 2.4.1 Experimental SetupQuery logs: The data source for instance extrac-tion is an anonymized collection of query logssubmitted to Live Search from January to February2007, taking the top 1 million unique queries.
Que-ries with garbage characters are removed.
Almostall queries are in Japanese, and are accompaniedby their frequency within the logs.Target categories: Our task is to learn word cate-gories that closely reflect the interest of web searchusers.
We believe that a useful categorization ofwords is task-specific, therefore we did not startwith any externally available ontology, but choseto start with a small number of seed words.
For ourtask, we were given a list of 23 categories relevantfor web search, with a manual classification of the10,000 most frequent search words in the log ofDecember 2006 (which we henceforth refer to asthe 10K list) into one of these categories.
2  Forevaluation, we chose two of the categories, Traveland Financial Services: Travel is the largest cate-gory containing 712 words of the 10K list (as allthe location names are classified into this category),while Financial Services was the smallest, contain-ing 240 words.Systems: We compared three different systemsdescribed in Section 2 that implement an iterativealgorithm for lexical learning:2 The manual classification assigns only one categoryper word, which is not optimal given how ambiguousthe category memberships are.
However, it is also verydifficult to reliably perform a multi-class categorizationby hand.Category Seeds (with English translation)Travel jal, ana, jr, ????
(jalan), hisFinance ?????
(Mizuho Bank), ??????
(SMBC), jcb, ?
?
?
?
(ShinseiBank), ????
(Nomura Securities)Table 2: Seed instances for Travel and Financial Ser-vices categories361?
Basilisk: The algorithm by (Thelen and Riloff,2002) described in Section 2.?
Espresso: The algorithm by (Pantel and Pennac-chiotti, 2006) described in Sections 2 and 3.?
Tchai: The Tchai algorithm described in thispaper.For each system, we gave the same seed instances.The seed instances are the 5 most frequent wordsbelonging to these categories in the 10K list; theyare given in Table 2.
For the Travel category, ?jal?and ?ana?
are airline companies, ?jr?
stand for Ja-pan Railways, ?jalan?
is an online travel informa-tion site, and ?his?
is a travel agency.
In theFinance category, three of them are banks, and theother two are a securities company and a creditcard firm.
Basilisk starts by extracting 20 patterns,and adds 100 instances per iteration.
Espresso andTchai start by extracting 5 patterns and add 200instances per iteration.
Basilisk and Tchai iterated20 times, while Espresso iterated only 5 times dueto computation time.4.2 Results4.2.1 Results of the Tchai algorithmTables 3 and 4 are the results of the Tchai algo-rithm compared to the manual classification.
Table3 shows the results for the Travel category.
Theprecision of Tchai is very high: out of the 297words classified into the Travel domain that werealso in the 10K list, 280 (92.1%) were learnedrectly.
3  It turned out that the 17 instances that3 As the 10K list contained 712 words in the Travel cat-egory, the recall against that list is fairly low (~40%).The primary reason for this is that all location names areclassified as Travel in the 10K list, and 20 iterations arerepresent the precision error were due to the ambi-guity of hand labeling, as in??????????
?Tokyo Disneyland?, which is a popular travel des-tination, but is classified as Entertainment in themanual annotation.
We were also able to correctlylearn 251 words that were not in the 10K list ac-cording to manual verification; we also harvested125 new words ?incorrectly?
into the Travel do-main, but these words include common nouns re-lated to Travel, such as ??
?fishing?
and ?????
?rental car?.
Results for the Finance domainshow a similar trend, but fewer instances are ex-tracted.Sample instances harvested by our algorithmare given in Table 5.
It includes subclasses of tra-vel-related terms, for some of which no seed wordswere given (such as Hotels and Attractions).
Wealso note that segmentation errors are entirely ab-sent from the collected terms, demonstrating thatquery logs are in fact excellently suited for acquir-ing new words for languages with no explicit wordsegmentation in text.4.2.2 Comparison with Basilisk and EspressoFigures 1 and 2 show the precision results compar-ing Tchai with Basilisk and Espresso for the Traveland Finance categories.
Tchai outperforms Basiliskand Espresso for both categories: its precision isconstantly higher for the Travel category, and itachieves excellent precision for the Finance cate-gory, especially in early iterations.
The differencesin behavior between these two categories are dueto the inherent size of these domains.
For thenot enough to enumerate all frequent location names.Another reason is that the 10K list consists of queriesbut our algorithm extracts instances ?
this sometimescauses a mismatch, e.g.,Tchai extracts???
?Ritz?
butthe 10K list contains ??????
?Ritz Hotel?.10K list Not in10K list Travel Not TravelTravel 280 17 251Not Travel 0 7 125Table 3: Comparison with manual annotation:Travel category10K list  Not in10K list Finance Not FinanceFinance 41 30 30Not Finance 0 5 99Table 4: Comparison with manual annotation:Financial Services categoryType Examples (with translation)Place ???
(Turkey), ?????
(LasVegas), ???
(Bali Island)Travel agency Jtb, ???
(www.tocoo.jp), ya-hoo (Yahoo !
Travel), net cruiserAttraction ????????
(Disneyland),usj (Universal Studio Japan)Hotel ?????
(Imperial Hotel), ???
(Ritz Hotel)Transportation ????
(Keihin Express), ????
(Nara Kotsu Bus Lines)Table 5: Extracted Instances362smaller Finance category, Basilisk and Espressoboth suffered from the effect of generic patternssuch as #??????
?homepage?
and #????card?
in early iterations, whereas Tchai did notselect these patterns.Figure 1: Basilisk, Espresso vs. Tchai: TravelFigure 2: Basilisk, Espresso vs. Tchai: FinanceComparing these algorithms in terms of recallis more difficult, as the complete set of words foreach category is not known.
However, we can es-timate the relative recall given the recall of anothersystem.
Pantel and Ravichandran (2004) definedrelative recall as:||||| BPAPCCCCCCRRRBABABABABA ?????
?where RA|B is the relative recall of system A givensystem B, CA and CB are the number of correct in-stances of each system, and C is the number of truecorrect instances.
CA and CB can be calculated byusing the precision, PA and PB, and the number ofinstances from each system.
Using this formula,we estimated the relative recall of each system rel-ative to Espresso.
Tables 6 and 7 show that Tchaiachieved the best results in both precision and rela-tive recall in the Travel domain.
In the Financedomain, Espresso received the highest relativecall but the lowest precision.
This is because Tchaiuses a filtering method so as not to select genericpatterns and instances.Table 8 shows the context patterns acquired bydifferent systems after 4 iterations for the Traveldomain.4 The patterns extracted by Basilisk are notentirely characteristic of the Travel category.
Forexample, ?p#sonic?
and ?google+#lytics?
onlymatch the seed word ?ana?, and are clearly irrele-vant to the domain.
Basilisk uses token count toestimate the score of a pattern, which may explainthe extraction of these patterns.
Both Basilisk andEspresso identify location names as context pat-terns (e.g., #??
?Tokyo?, #??
?Kyushu?
), whichmay be too generic to be characteristic of the do-main.
In contrast, Tchai finds context patterns thatare highly characteristic, including terms related totransportation (#+?????
?discount plane tick-et?, #?????
?mileage?)
and accommodation(#+???
?hotel?
).4.2.3 Contributions of Tchai componentsIn this subsection, we examine the contribution ofeach modification to the Espresso algorithm wemade in Tchai.Figure 3 illustrates the effect of eachmodification proposed for the Tchai algorithm inSection 3 on the Travel category.
Each line in thegraph corresponds to the Tchai algorithm with andwithout the modification described in Sections 3.1and 3.2.
It shows that the modification to themaxpmi function (purple) contributes most signifi-cantly to the improved accuracy of our system.
Thefiltering of generic patterns (green) does not show4 Note that Basilisk and Espresso use context patternsonly for the sake of collecting instances, and are notinterested in the patterns per se.
However, they can bequite useful in characterizing the semantic categoriesthey are acquired for, so we chose to compare them here.# of inst.
Precision Rel.recallBasilisk 651 63.4 1.26Espresso 500 65.6 1.00Tchai 680 80.6 1.67Table 6: Precision (%) and relative recall: Tra-vel domain# of inst.
Precision Rel.recallBasilisk 278 27.3 0.70Espresso 704 15.2 1.00Tchai 223 35.0 0.73Table 7: Precision (%) and relative recall: Finan-cial Services domain363a large effect in the precision of the acquired in-stances for this category, but produces steadily bet-ter results than the system without it.Figure 4 compares the original Espresso algo-rithm and the modified Espresso algorithm whichperforms the pattern induction step only at the be-ginning of the bootstrapping process, as describedin Section 3.3.
Although there is no significant dif-ference in precision between the two systems, thismodification greatly improves the computationtime and enables efficient extraction of instances.We believe that our choice of the seed instances tobe the most frequent words in the category produc-es sufficient patterns for extracting new instances.Figure 3: System precision w/o each modificationFigure 4: Modification to the pattern induction step5 ConclusionWe proposed a minimally supervised bootstrap-ping algorithm called Tchai.
The main contributionof the paper is to adapt the general-purpose Es-presso algorithm to work well on the task of learn-ing semantic categories of words from query logs.The proposed method not only has a superior per-formance in the precision of the acquired wordsinto semantic categories, but is faster and collectsmore meaningful context patterns for characteriz-ing the categories than the unmodified Espressoalgorithm.
We have also shown that the proposedmethod requires no pre-segmentation of the sourcetext for the purpose of knowledge acquisition.AcknowledgementsThis research was conducted during the first au-thor?s internship at Microsoft Research.
We wouldlike to thank the colleagues at Microsoft Research,especially Dmitriy Belenko and Christian K?nig,for their help in conducting this research.ReferencesSergey Brin.
1998.
Extracting Patterns and Relationsfrom the World Wide Web.
WebDB Workshop at 6thInternational Conference on Extending DatabaseTechnology, EDBT '98.
pp.
172-183.Michael Collins and Yoram Singer.
1999.
UnsupervisedModels for Named Entity Classification.
Proceedingsof the Joint SIGDAT Conference on Empirical Me-thods in Natural Language Processing and VeryLarge Corpora.
pp.
100-110.Oren Etzioni, Michael Cafarella, Dong Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Da-niel S. Weld, and Alexander Yates.
2005.
Unsuper-vised Named-Entity Extraction from the Web: AnExperimental Study.
Artificial Intelligence.
165(1).pp.
91-134.Marti Hearst.
1992.
Automatic Acquisition of Hypo-nyms from Large Text Corpora.
Proceedings of theSystem Sample Patterns (with English translation)Basilisk #???
(east_japan), #???
(west_japan), p#sonic, #???
(timetable), #??
(Kyushu),  #+?????
(mileage), #??
(bus),  google+#lytics, #+??
(fare),  #+??
(domestic), #???
(hotel)Espresso #??
(bus), ?
?#(Japan), #???
(hotel), #??
(road), #??
(inn), ?
?#(Fuji), #??
(Tokyo), #??
(fare), #??
(Kyushu), #???
(timetable), #+??
(travel), #+???
(Nagoya)Tchai #+???
(hotel), #+???
(tour), #+??
(travel), #??
(reserve), #+???
(flight_ticket), #+?????
(discount_flight_titcket), #?????
(mileage), ???
?+#(Haneda Airport)Table 8: Sample patterns acquired by three algorithms364Fourteenth International Conference on Computa-tional Linguistics.
pp 539-545.Patrick Pantel and Marco Pennacchiotti.
2006.
Espresso:Leveraging Generic Patterns for Automatically Har-vesting Semantic Relations.
Proceedings of the 21stInternational Conference on Computational Linguis-tics and the 44th annual meeting of the ACL.
pp.
113-120.Patrick Pantel and Deepak Ravichandran.
2004.
Auto-matically Labeling Semantic Classes.
Proceedings ofHuman Language Technology Conference of theNorth American Chapter of the Association for Com-putational Linguistics (HLT/NAACL-04).
pp.
321-328.Marius Pa?ca.
2004.
Acquisition of Categorized NamedEntities for Web Search.
Proceedings of the 13thACM Conference on Information and KnowledgeManagement (CIKM-04).
pp.
137-145.Marius Pa?ca.
2007.
Organizing and Searching theWorld Wide Web of Fact ?
Step Two: Harnessing theWisdom of the Crowds.
Proceedings of the 16th In-ternational World Wide Web Conference (WWW-07).pp.
101-110.Marius Pa?ca and Benjamin Van Durme.
2007.
WhatYou Seek is What You Get: Extraction of ClassAttributes from Query Logs.
Proceedings of the 20thInternational Joint Conference on Artificial Intelli-gence (IJCAI-07).
pp.
2832-2837.Marius Pa?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-chits and Alpa Jain.
2006.
Organizing and Searchingthe World Wide Web of Facts ?
Step One: the One-Million Fact Extraction Challenge.
Proceedings ofthe 21st National Conference on Artificial Intelli-gence (AAAI-06).
pp.
1400-1405.Ellen Riloff.
1996.
Automatically Generating ExtractionPatterns from Untagged Text.
Proceedings of theThirteenth National Conference on Artificial Intelli-gence.
pp.
1044-1049.Ellen Riloff and Rosie Jones.
1999.
Learning Dictiona-ries for Information Extraction by Multi-Level Boot-strapping.
Proceedings of the Sixteenth NationalConference on Artificial Intellligence (AAAI-99).
pp.474-479.Satoshi Sekine and Hisami Suzuki.
2007.
AcquiringOntological Knowledge from Query Logs.
Proceed-ings of the 16th international conference on WorldWide Web.
pp.
1223-1224.Craig Silverstein, Monika Henzinger, Hannes Marais,and Michael Moricz.
1998.
Analysis of a Very LargeAltaVista Query Log.
Digital SRC Technical Note#1998-014.Michael Thelen and Ellen Riloff.
2002.
A BootstrappingMethod for Learning Semantic Lexicons using Ex-traction Pattern Contexts.
Proceedings of Conferenceon Empirical Methods in Natural LanguageProcessing.
pp.
214-221.365
