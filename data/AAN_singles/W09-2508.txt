Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 52?60,Suntec, Singapore, 6 August 2009.c?2009 ACL and AFNLPUsing Hypernymy Acquisition to Tackle (Part of) Textual EntailmentElena AkhmatovaCentre for Language TechnologyMacquarie UniversitySydney, Australiaelena@ics.mq.edu.auMark DrasCentre for Language TechnologyMacquarie UniversitySydney, Australiamadras@ics.mq.edu.auAbstractWithin the task of Recognizing TextualEntailment, various existing work has pro-posed the idea that tackling specific sub-types of entailment could be more produc-tive than taking a generic approach to en-tailment.
In this paper we look at onesuch subtype, where the entailment in-volves hypernymy relations, often foundin Question Answering tasks.
We investi-gate current work on hypernymy acquisi-tion, and show that adapting one such ap-proach leads to a marked improvement inentailment classification accuracy.1 IntroductionThe goal of the Recognizing Textual Entailment(RTE) task (Dagan et al, 2006) is, given a pair ofsentences, to determine whether a Hypothesis sen-tence can be inferred from a Text sentence.
Themajority of work in RTE is focused on finding ageneric solution to the task.
That is, creating a sys-tem that uses the same algorithm to return a yesor no answer for all textual entailment pairs.
Ageneric approach never works well for every sin-gle entailment pair: there are entailment pairs thatare recognized poorly by all the generic systems.Some approaches consequently propose acomponent-based model.
In this framework,a generic system would have additional specialcomponents that take care of special subclasses ofentailment pairs.
Such a component is involvedwhen a pair of its subclass is recognized.
Vander-wende and Dolan (2005), and subsequently Van-derwende et al (2006), divide all the entailmentpairs according to whether categorization couldbe accurately predicted based solely on syntacticcues.
Related to this, Akhmatova and Dras (2007)present an entailment type where the relationshipexpressed in the Hypothesis is encoded in a syn-tactic construction in the Text.Vanderwende et al (2006) note that what theyterm is-a relationships are a particular problem intheir approach.
Observing that this encompasseshypernymy relations, and that there has been afair amount of recent work on hypernymy acquisi-tion, where ontologies containing hypernymy rela-tions are extended with corpus-derived additions,we propose a HYPERNYMY ENTAILMENT TYPE tolook at in this paper.
In this type, the Hypothesisstates a hypernymy relationship between elementsof the Text: for example, This was seen as a be-trayal by the EZLN and other political groups im-plies that EZLN is a political group.
This subtypeis of particular relevance to Question Answering(QA): in the RTE-2 dataset,1for example, all is-aHypotheses were drawn from QA data.In this paper we take the hypernymy acquisitionwork of Snow et al (2005) as a starting point, andthen investigate how to adapt it to an entailmentcontext.
We see this as an investigation of a moregeneral approach, where work in a separate area ofNLP can be adapted to define a related entailmentsubclass.Section 2 of the paper discusses the relevantwork from the areas of component-based RTE andhypernymy extraction.
Section 3 defines the hy-pernymy entailment type and expands on the mainidea of the paper.
Section 4 describes the experi-mental set-up and the results; and Section 5 con-cludes the work.2 Related Work2.1 Component-based RTEVanderwende et al (2006) use an approach basedon logical forms, which they generate by the NLP-win parser.
Nodes in the resulting syntactic de-pendency graphs for Text and Hypothesis are thenheuristically aligned; then syntax-based heuristics1http://pascallin.ecs.soton.ac.uk/Challenges/RTE2, (Bar-Haim et al, 2006)52are applied to detect false entailments.
As notedabove, is-a relations fared particularly badly.
Inour approach, we do not use such a heavy dutyrepresentation for the task, using instead the tech-niques of hypernym acquisition described in Sec-tion 2.2.
Cabrio et al (2008) proposed what theycall a combined specialized entailment engine.They have created a general framework, based ondistance between T and H (they measure the costof the editing operations such as insertion, dele-tion and substitution, which are required to trans-form the text T into the hypothesis H) and sev-eral modular entailment engines, each of which isable to deal with an aspect of language variabil-ity such as negation or modal verbs.
Akhmatovaand Dras (2007) built a specific component froma subset of entailment pairs that are poorly recog-nized by generic systems participating in an RTEChallenge.
These are the entailment pairs where aspecific syntactic construction in the Text encodesa semantic relationship between its elements thatis explicitly shown in the Hypothesis, as in exam-ple (1):(1) Text: Japan?s Kyodo news agency said theUS could be ready to set up a liaisonoffice?the lowest level of diplomaticrepresentation?in Pyongyang if it abandonsits nuclear program.Hypothesis: Kyodo news agency is based inJapan.The entailment pairs share a set of similar fea-tures: they have a very high word overlap regard-less of being a true or false entailments, for ex-ample.
High word overlap is one of the featuresfor an RTE system for the majority of the entail-ment pair types, which presumably hints at true,but this is not useful in our case.
Akhmatova andDras (2007) described a two-fold probabilistic ap-proach to recognizing entailment, that in its turnwas based on the well-known noisy channel modelfrom Statistical Machine Translation (Brown etal., 1990).
In the work of this paper, by contrast,we look at only identifying a hypernymy-relatedText, so the problem reduces to one of classifica-tion over the Text.2.2 Hypernymy ExtractionThe aim of work on hypernymy extraction is usu-ally the enrichment of a lexical resource such asWordNet, or creation of specific hierarchical lex-ical data directly for the purpose of some appli-cation, such as information extraction or ques-tion answering.
There can be found several ap-proaches to the task of hypernymy extraction: co-occurrence approaches, asymmetric associationmeasures, and pattern-based methods.Cooccurence Approaches Co-occurrence ap-proaches first cluster words into similarity classesand consider the elements of a class to be sib-lings of one parent.
Therefore the search for aparent for some members from the class gives aparent for the other members of the class.
Thefirst work that introduced co-occurrence methodsto the field is that of Caraballo (1999).
First sheclusters nouns into groups based on conjunctiveand appositive data collected from the Wall StreetJournal.
Nouns are grouped according to the sim-ilarity of being seen with other nouns in conjunc-tive and appositive relationships.
In the secondstage, using some knowledge about which con-juncts connect hypernyms reliably, a parent for agroup of nouns is searched for in the same text cor-pora.
Other co-occurrence methods can be foundin works by Pantel et al (2004) and Pantel andRavichandran (2004).Asymmetric Association Measures In Asym-metric Association (see Dias et al (2008)) hy-pernymy is derived through the measure of howmuch one word ?attracts?
another one.
When hear-ing ?fruit?, more common fruits will be likely tocome into mind such as ?apple?
or ?banana?.
Inthis case, there exists an oriented association be-tween ?fruit?
and ?mango?
(mango?
fruit) whichindicates that ?mango?
attracts ?fruit?
moresothan ?fruit?
attracts ?mango?.
As a consequence,?fruit?
is more likely to be a more general termthan ?mango?.Pattern-based Methods Pattern-based methodsare based on the observation that hypernyms tendto be connected in the sentences by specific wordsor patterns, and that some patterns can predicthypernymy with very high probability, like theX and other Y pattern.
Generally, some amountof manual work on finding the seed patterns isdone first.
Automated algorithms use these pat-terns for discovering more patterns and for thesubsequent hypernymy extraction.
The fundamen-tal work for the pattern-based approaches is that ofHearst (1992).
More recently, Snow et al (2005)and Snow et al (2006) have described a method ofhypernymy extraction using machine learning of53patterns.
Pattern-based methods are known to besuccessfully used for the creation of hierarchicaldata for other languages as well, such as Dutch;for example, see Tjong Kim Sang and Hofmann(2007).
For our purposes, pattern-based methodsare particularly suitable, as we have as context twowords and a single pattern connecting them; wethus describe these approaches in more detail.In her early work on pattern-based hypernymyextraction Hearst (1992) noticed that a particularsemantic relationship between two nouns in thesentence can be indicated by the presence of cer-tain lexico-syntactic patterns linking those nouns.Hypernymy (is-a, is a kind of relation) is one suchrelationship.Linking two noun phrases via the patternssuch NPyas NPxoften implies that NPxis ahyponym of NPy, that is NPxis a kind of NPy.She gives the following example to illustrate thepatterns(2) The bow lute, such as the Bambara ndang, isplucked and has an individual curved neckfor each string.Hearst comments that most fluent readers of En-glish who have never before encountered the termBambara ndang will nevertheless from this sen-tence infer that a Bambara ndang is a kind of bowlute.
This is true even if the reader has only a fuzzyconception of what a bow lute is.
The completeset of patterns semi-automatically found by Hearstare:1.
NPyand other NPx2.
NPyor other NPx3.
NPysuch as NPx4.
such NPyas NPx5.
NPyincluding NPx6.
NPy, especially NPxSnow et al (2005) had the aim of building uponHearst?s work in order to extend the WordNetsemantic taxonomy by adding to it hypernym-hyponym pairs of nouns that are connected by awider set of lexico-syntactic pairs.
They devel-oped an automatic approach for finding hypernym-hyponym pairs of nouns in the text corpus withouta set of predefined patterns.The work was carried out on a corpus of 6 mil-lion newswire sentences.
Every pair of nouns(ni, nj) in the sentence was extracted.
The pairswere labelled as Known Hypernym pair if njisan ancestor of the first sense of niin the WordNethypernym taxonomy (Fellbaum, 1998).
A nounpair might have been assigned to the second setof Known Non-Hypernym pairs if both nouns arecontained within WordNet, but neither noun is anancestor of the other in the WordNet hypernymtaxonomy for any senses of either noun.
Eachsentence was parsed using MINIPAR.
The depen-dency relations between niand njconstituted thelexico-syntactic patterns connecting Known Hy-pernyms or Known Non-Hypernyms.
The mainidea of their work was then to collect all the lexico-syntactic patterns that may indicate the hypernymyrelation and use them as the features for a decisiontree to classify NP pairs as hypernym-hyponym ornot-hypernym-hyponym pairs.Snow et al (2005) state in their work that the de-pendency paths acquired automatically containedall the patterns mentioned in Hearst (1992).
Thecomparison of the results of a classifier whose vec-tors were created from all the patterns seen withthe Known Hypernyms in their corpus, and a clas-sifier whose vectors contained only the patterns ofHearst (1992), showed that the results of the for-mer classifier are considerably better than that ofthe latter one.
In an RTE context where the en-tailment recognition relies on recognising hyper-nymy, an approach like this, where patterns ac-quired from a corpus are used, could be useful; buthow it should best be adapted is not clear.
That isthen the goal of this paper.3 Hypernymy Entailment Type3.1 DefinitionWe define Hypernymy Entailment to be an en-tailment relationship where the is-a relationshipbetween two nouns in the hypothesis is ?hid-den behind?
the lexico-syntactic pattern connect-ing them in the text.
Being more precise, theText-Hypothesis pairs of interest have the follow-ing characteristics:1.
The Hypothesis is a simple sentence.
That isa sentence that consists of a subject, a 3rd per-son form the verb to be, and a direct object,and that contains no subordinate clauses.2.
Both subject and object of the Hypothesis (orin some cases their morphological variants)are found in the text.Thus, the hypernymy relationship is not stated inthe Text, but is hidden in the way the subject and54object of the Hypothesis are connected to eachother in the Text.
Examples of the true hypernymyentailment pairs are as follows:2(3) Text: Soon after the EZLN had returned toChiapas, Congress approved a differentversion of the COCOPA Law, which did notinclude the autonomy clauses, claiming theywere in contradiction with someconstitutional rights (private property andsecret voting); this was seen as a betrayal bythe EZLN and other political groups.Hypothesis: EZLN is a political group.Both EZLN and political groups are present in thetext sentence, and are connected by an is-a relationin the hypothesis.
The pattern and other and thesyntactical connection between the noun phrasesgive a good indication that the noun phrases are inthe hypernym-hyponym relationship.
An exampleof a false hypernymy entailment pair is as follows:(4) Text: Laboring side by side on the outer hullof the station?s crew quarters, VladimirDezhurov and Mikhail Turin mountedscience packages and two Eastman KodakCo.
placards while U.S. astronaut FrankCulbertson looked on from inside thecomplex.Hypothesis: Vladimir Dezhurov is a U.S.astronaut.3.2 IdeaIn the case of Snow et al (2005) the main accentis on automatic extraction of all the patterns thatmight, even if not reliably on their own, predictthe hypernymy relation between two nouns.
Theirtask is, given a previously unseen pair of nouns,to determine whether they are in a hypernymy re-lationship, using a classifier whose feature valuesare derived from many occurrences of acquiredpatterns in a corpus.In our own work we are put in the situationwhere there is only one pattern that is availableto judge if two words are in a hypernym/hyponymrelation, not the whole text corpus as in the caseof Snow et al (2005).
Thus, we are mostly inter-ested in the prediction of the hypernymy using thispattern that is available for us.
The fact that thenamed entities we are working with, such as per-son, organization, location, are not that frequently2Examples (3) - (4) are taken from the RTE2 test corpus.seen in any text corpora also shifts the accent ontothe pattern rather than on the word pair itself.
Aswell as the fact that even in the case when twowords are hypernym-hyponym, that may not fol-low at all from the sentence that they are seen in;and non hypernym-hyponym pair can be used assuch in a metaphoric expression or just in a par-ticular sentence we are dealing with.
To illustrate,consider example (5):(5) Text: Note that the auxiliary verb functionderives from the copular function; and,depending on one?s point of view, one canstill interpret the verb as a copula and thefollowing verbal form as being adjectival.Hypothesis: A copular is a verb.Snow et al (2005) aim to determine whethercopular and verb are in a hypernymy relation; tothis end they use the as a pattern as in this exam-ple, along with all others throughout the corpus.The reliability of the as a pattern (which as it turnsout is quite high) adds weight to the accumulatedevidence, but is not the sole evidence.
In the in-dividual case, however, it can be incorrect, as inexample (6):(6) Text: In the 1980s, Minneapolis took itsplace as a center of the arts, with the WalkerArts Center leading the nation inappreciation of pop and postmodern art , anda diverse range of musicians, from Prince toH?usker D?u to the Replacements to theSuburbs to Soul Asylum keeping up with thenation in musical innovation.Hypothesis: A centre is a place.Example (6) has a similar structure to exam-ple (5), but center governs a preposition of afterit, that seem to make the hypernymy more doubt-ful in this context.
Taking into account all of theabove, the major focus of the work has shifted forus from the word pair to the environment it has oc-curred in.
Thus, we use the major ideas from thework of Snow et al (2005), but as we show be-low, it is necessary to develop a more complex setof counts in order to apply this to our entailmentstype.
In particular, we expect that the division ofpatterns into lexical and syntactic parts, in order toscore them separately, is beneficial for entailment.Again, it is a result of scarcity of information: wehave only one text sentence, not the whole text cor-pus to make the entailment decision.554 Experimental Setup4.1 DataOur goal is to build a classifier that will detectwhether a given potential hypernymy entailmentpair is true or false; we first need to construct setsof such pairs for training and testing.
As our ba-sic data source, we use 500 000 sentences fromthe Wikipedia XML corpus (Denoyer and Galli-nari, 2006); this is the corpus used by Akhmatovaand Dras (2007), and related to one used in oneset of experiments by Snow et al (2005).
Thesesentences were parsed with the MINIPAR parser.We identified Known Hypernym pairs as didSnow et al (2005) (see Section 2.2); of our ba-sic corpus, 13310 sentences contained Known Hy-pernyms.
From these sentences we extracted thedependency relations between the Known Hyper-nyms, of which there were 166 different types; werefer to these as syntactic patterns hereafter.We reserved 259 of these sentences to constructa test set for our approach, as described below.These sentences were selected randomly in pro-portion to the syntactic patterns occurring in theoverall set.
The remaining sentences constitutedour SYNTACTIC PATTERN TRAINING SET.
For thetest set, these sentences constituted the Texts; toderive the Hypotheses, we extracted the KnownHypernyms and connected them by is a. Thesesentences were annotated with yes if they entailhypernymy, and no otherwise; the resulting anno-tated data has 2:1 ratio of no to yes.
The mainannotation was carried out by the first author, withthe second author carrying out a separate annota-tion to evaluate agreement.
The number of itemswhere there was agreement was 206, giving a ?of 0.54.
This is broadly in line with the ?
found inconstruction of the RTE datasets (?
= 0.6) (Glick-man, 2006) where it is characterized as ?moder-ate agreement?, based on Landis and Koch (1977).Results later are presented for both the overall setof 259 (based on the first author?s original annota-tions) and for the subset with agreement of 206.As our additional, much larger data source forderiving purely lexical patterns and associatedscores, we use the Web1T n-gram corpus (Brantsand Franz, 2006), which provides n-grams andtheir counts for up to 5-grams inclusive.
We usethese n-grams to get the lexical patterns of length1, 2 and 3 that connect Known Hypernyms andKnown Non-Hypernyms correspondingly.
Thelength is up to 3 as we need 2 slots for the nounsfrom the pair itself.
The counts are extracted withthe help of the software get1t written by Hawkeret al (2007).
We refer to this as our LEXICAL PAT-TERN TRAINING SET.4.2 BaselinesWe use two baselines.
The first is a simple most-frequent one, choosing always false (noting fromSection 4.1 that this is more common by a ratioof approximately 2:1).
For the second one, we at-tempt to use the idea of Snow et al (2005) in astraightforward way.
We note again that the fixedcontext for a given Known Hypernym pair that wehave, unlike Snow et al (2005), is the single Text;we therefore cannot apply the classifier from thatwork directly.
Our second baseline based on theirapproach is as follows.
For each sentence we lookat all nouns it contains.
If a pair of nouns from thesentence is a Known-Hypernym pair we save thelexical pattern connecting the nouns and the syn-tactic pattern between the nouns in a pattern list.We take into account only those syntactic patternsthat have been seen in the corpus at least threetimes.
We then consider that a test entailment pairis a true entailment if both the lexical pattern be-tween the nouns in question and the syntactic con-nection between them is found in the list.4.3 Two-Part ModelWe now propose a two-component model to com-pensate for the fixed context.
The first component,scorelex, involves the use of the lexical pattern topredict hypernymy.
Unless we know somethingelse about the structure of the text sentence, thepattern (a sequence of words) that connects twoentities in question is the only evidence of the pos-sible hypernym-hyponym relation between them.It does not guarantee the relation itself, but themore probable it is that the pattern predicts hyper-nymy, the more probable it is that the entailmentrelation between the Text and Hypothesis holds.To motivate the second component, we take as anexample the patternNPyand other NPx, the firstof the Hearst (1992) patterns and a good predictorof hypernymy, and consider the following exam-ples:(7) Text: Mr. Smith and other employees stayedin the office.Hypothesis: Mr. Smith is an employee.
(8) Text: I talked to Mr. Smith and other56employees stayed in the office.Hypothesis: Mr. Smith is an employee.Mr.
Smith and an employee are connected inboth cases by and other.
We know that the pat-tern and other is a good indicator of the hyper-nymy relation.
The probability of the pattern andother to predict the hypernymy relation is the priorprobability of the entailment relation in a text-hypothesis pair.
As can be seen in examples (7)and (8), there is an entailment relationship only inexample (7); in example (8) entailment does nothold.The second component scoresyntis an indica-tor of the syntactic possibility of the entailmentrelationship.
Hypernym-hyponyms tend to be incertain syntactic relations in the sentence, suchas being subjects of the same verb, for example,in the cases where we can decide on the relationof the hypernymy between them.
Other syntac-tic relationships, even though they may connecthypernym and hyponym, do not allow us to con-clude that there is a hypernymy relation betweenthe words.
As it can be seen from examples (7)and (8), every syntactical relation has its own levelof certainty about the hypernym relation betweenMr.
Smith and an employee, and therefore aboutthe fact that the Text entails the Hypothesis.4.3.1 Lexical PatternsFrom our lexical pattern training corpus, we de-rived for both Known Hypernym and Known Non-Hypernym pairs, the counts of both tokens (to-tal number of pairs connected) and types (num-ber of different pairs connected).
To illustrate, wetake two example pairs, w1= rock and w2=material, and w1= rice and w2= grain.
Wefind rock , and other material occurs 47 times, andrice , and other grain 166 times.
Totalling these,that would give us the following statistics for thepattern , and other: seen with the Known Hyper-nyms 213 times (total of tokens), connecting 2 dif-ferent pairs (total of types).
We hypothesize thatknowing the number of different types of patternswill be important as a way of compensating for themore limited context relative to Snow et al (2005)which used only the number of pattern tokens.The above can be illustrated by the counts ob-tained for patterns of Hearst (1992); see the firstfive rows of Table 1.
One can see from thefirst three examples that in all cases the numberof times the pattern has been seen with KnownHypernyms is overwhelmingly higher than withthat of Known Non-Hypernyms.
Even more ex-tremely, in the next two examples in Table 1,Known Non-Hypernyms were not seen with thesepatterns at all.
We contrast these with the non-Hearst patterns (extracted from our lexical patterncorpus) in the last two rows.
As one can see,the patterns and detailed travel and online gamecaribbean have been seen only with the KnownHypernyms, and the frequency counts are veryclose to that of the pattern , especially.
Both pat-terns however have connected the constituents ofonly one Known Hypernyms pair.
That puts somedoubt on the general reliability of the pattern tomake hypernymy judgements.We then define our scoring metric, based onthe following quantities: C(h-tok), the number oftimes the pattern has been seen with Known Hy-pernyms; C(nh-tok), the number of times the pat-tern has been seen with Known Non-Hypernyms;C(h-type), the number of times the pattern hasbeen seen with different Known Hypernym pat-terns; C(nh-type), the number of times the pat-tern has been seen with different Known Non-Hypernym patterns.
We then define our lexicalscoring function as follows:scorelex=C(h-tok)C(h-tok) + C(nh-tok)?C(h-type)C(h-type) + C(nh-type)We use it to score patterns where the numberof times the pattern has been seen with differentKnown Hypernyms (C(h-type)) is greater than athreshold, here 5; for patterns below this thresh-old, the score is 0.
We determined on this scoringfunction in comparison to others (notably usingonly token proportions, the first term in the scor-ing function above) by using them to rank patternsand then assess the relative ranking of the Hearstpatterns among all others.
Under the scoring func-tion above, the Hearst patterns were ranked high-est, with patterns or other, such as and and othertaking the first, second and third positions respec-tively.4.3.2 Syntactic PatternsTo estimate the probability of various syntacticpatterns from our syntactic pattern training cor-pus, ideally we would annotate every sentence as57Table 1: Counts for the patterns of Hearst (1992) obtained from the Web1T corpusseen withPatternHypernyms Non- Different DifferentHypernyms Hypernyms Non-HypernymsNPyand other NPx172036 1716 486 3NPyor other NPx421083 1016 965 11NPysuch as NPx86158 384 355 4NPyincluding NPx68098 0 251 0NPy, especially NPy10236 0 80 0NPyand detailed travel NPx9870 0 1 0NPyonline game caribbean NPx9874 0 1 0true or false according to whether the hypernymyis entailed from the sentence or not.
The annota-tion would allow the calculation of the likelihoodfor every syntactical relation to indicate the entail-ment relationship.It is quite a time-consuming task to annotateenough data to get reliable counts for all the syn-tactical patterns.
Therefore, as an approximatefirst step we have divided all the sentences intothree groups according to the type of a lexical pat-terns that connects a pair of Known Hypernyms:Hearst patterns; the patterns that were found fromour lexical pattern training corpus; and all otherpatterns.
We have assumed that Hearst patterns,as being a good indication of hypernymy, may inmost cases predict entailment as well; the auto-matically derived lexical patterns may still some-time predict entailment, but less well than theHearst patterns; and the unknown patterns are notconsidered to be good predictors of the entailmentat all.
Thus, for the initial estimate of the syntac-tical probabilities of the entailment we have em-ployed a very coarse approximation of the max-imum likelihood estimate of the probability of asyntactic pattern implying an entailment, weight-ing these three groups with the values 1, 0.5 and 0respectively.
This leads to a score as follows:scoresynt-basic= 0.5?C(automatic lexical pattern)C(all patterns)+ 1.0?C(Hearst pattern)C(all patterns)where C(X) represents the count of occurrencesof the pattern type X .As a more refined scoring metric, we identi-fied the set of the most frequent syntactic patternsTable 2: Syntactic Pattern ProbabilitiesPattern Basic P Improved Pobj 0.34 0.0pcomp-n mod 0.40 0.038appo 0.73 0.90conj 0.76 0.10mod pcomp-n 0.64 0.38mod pcomp-n mod 0.45 0.023mod conj 0.97 0.10Table 3: Model Evaluation (full set of 259 / agreedsubset of 206)Model AccuracyBaseline (most frequent) 69% / 70%Baseline (Snow) 71% / 72%Lexical component only 60% / 60%Improved syntactic component only 67% / 69%Lexical and Basic Syntactic Component 76% / 73%Lexical and Improved Syntactic Component 82% / 83%and annotated data for them, in order to improvetheir probability estimates.
Taking the seven mostfrequent, we annotated 100 randomly chosen sen-tences for each of the syntactical patterns contain-ing them from the syntactic pattern training cor-pus.
As a result of the annotation the probabilitiesof the syntactical patterns to indicate entailmenthas changed.
The basic probabilities and the re-vised probabilities for these seven syntactic pat-terns can be found in Table 2.4.4 Results and DiscussionWe combine the lexical and syntactic scores asfeatures to the J48 decision tree of WEKA (Wit-58ten and Frank, 1999).
Our evaluation is a 10-foldcross-validation on the test set.
Results are as inTable 3, presented for both the full test set of 259and for the subset with agreement of 206.We note first of all that the simple approach de-rived from Snow et al (2005), as described in Sec-tion 4.2, does perform marginally better than thebaseline of choosing always false.
The lexical orsyntactic components alone do not perform betterthan the most-frequent baseline approach.
Thisis expected, as that approach includes both lexi-cal and syntactic components.
The lexical com-bined with the basic syntactic component does im-prove over the baselines.
However, the lexicalcombined with the improved syntactic componentexperiences a much higher improvement.
Overall,the results for the full set and for the subset arebroadly the same, showing the same relative be-haviour.The lexical only component falsely recognizesexamples such as example (9) as true, as it has nosupport of syntax.
Just a comma by itself suffi-ciently frequently indicates entailment in case ofapposition, so the lexical component is misled.
(9) Text: There were occasional outbreaks ofviolence, but most observers considered itremarkable that such an obvious breakdownof the capitalist system had not led to a rapidgrowth of socialism, communism, or fascism(as happened for example in Germany).Hypothesis: Communism is a socialism.Syntax only, even though it prevents the mis-takes of the lexical-only component for the exam-ples above, introduces its own mistakes.
Knowingthat the subject and object in the Hypothesis arelinked by direct dependency relations to a prepo-sition in the Text is useful, but without a lexicalpattern can be too permissive, as in example (10):(10) Text: However, Griffin attracted criticism forwriting in the aftermath of the bombing ofthe Admiral Duncan pub bombing (whichkilled three people, including a pregnantwoman) that the gay people protestingagainst the murders were ?flaunting theirperversion in front of the world?s journalists,and showed just why so many ordinarypeople find these creatures disgusting?.Hypothesis: Criticism is a writing.Both baseline and the final hypernymy entailmentengine work well in the cases where the counts foror against entailment are very high, as in examples(11) and (12), which are correctly recognized as atrue and a false entailment by both systems.
(11) Text: Carbon compounds form the basis ofall life on Earth and the carbon-nitrogencycle provides some of the energy producedby the sun and other stars.Hypothesis: Sun is a star.
(12) Text: In 1792 British explorer GeorgeVancouver set up a small settlement near thevillage of Yerba Buena (later downtown SanFrancisco) which became a small base forEnglish, Russian, and other European furtraders, explorers, and settlers.Hypothesis: Village is a settlement.The final hypernymy system works better for moremarginal cases, such as example (13).
(13) Text: The trials were held in the German cityof Nuremberg from 1945 to 1949 at theNuremberg Palace of Justice.Hypothesis: Nuremberg is a city.The pattern of can not be called a good hint for hy-pernymy, but in some special cases, like that of thecity and its name, the hypernymy is obvious.
Divi-sion into lexical and syntactic parts helped in dis-covering the pattern and adjusting better its prob-ability of entailing hypernymy.
All this supportsour idea that to compensate for the lack of infor-mation in the case of RTE the lexico-syntactic pat-terns should be divided into their lexical and syn-tactic components.5 ConclusionIn this paper we have shown how work in hyper-nymy acquisition can be adapted to tackle a spe-cific subtype of related entailment problem.
Fol-lowing work by Snow et al (2005), we have de-fined an obvious first adaptation which nonethe-less marginally improves over the baseline.
Wehave then shown that by separating lexical andsyntactic patterns we can obtain a significant im-provement on the entailment classification accu-racy.
In our future work we aim to construct abaseline generic RTE engine and test its perfor-mance with and without this and other componentsin order to analyse the work of a component-basedmodel as a whole.
The approach also suggests thatadapting work from other areas of NLP for entail-ment subclasses is promising.59ReferencesElena Akhmatova and Mark Dras.
2007.
Entailmentdue to syntactically encoded semantic relationships.In Proceedings of ALTA-2007, pages 4?12.Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,Danilo Giampiccolo, Bernardo Magnini, and IdanSzpektor.
2006.
The second pascal recognising tex-tual entailment challenge.
In The second PASCALRecognising Textual Entailment Challenge, pages 3?11, Venice, Italy.Thorsten Brants and Alex Franz.
2006.
Web 1t 5-gram corpus version 1.
Technical report, GoogleResearch.Peter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Fredrick Jelinek, John D.Lafferty, Robert L. Mercer, and Paul S. Roossin.1990.
A statistical approach to machine translation.In Computational Linguistics, volume 16, pages 79?
85.Elena Cabrio, Milen Kouylekov, and BernardoMagnini.
2008.
Combining specialized entailmentengines for RTE-4.
In Proceedings of TAC-2008.Sharon Caraballo.
1999.
Automatic acquisition of ahypernym-labeled noun hierarchy from text.
In Pro-ceedings of ACL-99, pages 120?126.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The pascal recognising textual entailmentchallenge.
In Quionero-Candela, J.; Dagan, I.;Magnini, B.; d?Alch-Buc, F.
(Eds.)
Machine Learn-ing Challenges.
Lecture Notes in ComputerScience,volume 3944, pages 177 ?
190.
Springer.Ludovic Denoyer and Patrick Gallinari.
2006.
TheWikipedia XML Corpus.
In SIGIR Forum, 40(1),pages 64?69.Ga?el Dias, Raycho Mukelov, and Guillaume Cleuziou.2008.
Unsupervised learning of general-specificnoun relations from the web.
In Proceedings ofFLAIRS Conference, pages 147?152.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
The MIT Press.Oren Glickman.
2006.
Applied Textual Entailment.Ph.D.
thesis, Bar Ilan University.Tobias Hawker, Mary Gardiner, and Andrew Ben-netts.
2007.
Practical queries of a massive n-gramdatabase.
In Proceedings of ALTA-2007, pages 40?48.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In COLING, pages539?545.Richard J. Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, 33(1):159?174.Patrick Pantel and Deepak Ravichandran.
2004.
Auto-matically labeling semantic classes.
In Proceedingsof HLT/NAACL-04, pages 321?328.Patrick Pantel, Deepak Ravichandran, and EduardHovy.
2004.
Towards terascale semantic acquisi-tion.
In Proceedings of Coling 2004, pages 771?777, Geneva, Switzerland, Aug 23?Aug 27.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2005.Learning syntactic patterns for automatic hypernymdiscovery.
In Lawrence K. Saul, Yair Weiss, andL?eon Bottou, editors, Advances in Neural Informa-tion Processing Systems 17, pages 1297?1304, Cam-bridge, MA.
MIT Press.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2006.Semantic taxonomy induction from heterogenousevidence.
In Proceedings of ACL-2006, pages 801?808.E.F.
Tjong Kim Sang and K. Hofmann.
2007.
Auto-matic extraction of dutch hypernym-hyponym pairs.In Proceedings of CLIN-2006, Leuven, Belgium.LOT, Netherlands Graduate School of Linguistics.Lucy Vanderwende andWilliam B. Dolan.
2005.
Whatsyntax can contribute in the entailment task.
In Pro-ceedings of MLCW, pages 205?216.Lucy Vanderwende, Arul Menezes, and Rion Snow.2006.
Microsoft research at RTE-2: Syntactic con-tributions in the entailment task: an implementation.In Proceedings of 2nd PASCAL Challenges Work-shop on Recognizing Textual Entailment.Ian H. Witten and Eibe Frank.
1999.
Data Mining:Practical Machine Learning Tools and Techniqueswith Java Implementations.
Morgan Kaufmann.60
