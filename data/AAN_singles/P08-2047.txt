Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 185?188,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSemantic Types of Some Generic Relation Arguments:Detection and EvaluationSophia KatrenkoInstitute of InformaticsUniversity of Amsterdamthe Netherlandskatrenko@science.uva.nlPieter AdriaansInstitute of InformaticsUniversity of Amsterdamthe Netherlandspietera@science.uva.nlAbstractThis paper presents an approach to detec-tion of the semantic types of relation argu-ments employing the WordNet hierarchy.
Us-ing the SemEval-2007 data, we show thatthe method allows to generalize relation ar-guments with high precision for such genericrelations as Origin-Entity, Content-Container,Instrument-Agency and some other.1 Introduction and MotivationA common approach to learning relations is com-posed from two steps, identification of argumentsand relation validation.
This methodology is widelyused in different domains, such as biomedical.
Forinstance, in order to extract instances of a relation ofprotein interactions, one has to first identify all pro-tein names in text and, second, verify if a relationbetween them holds.Clearly, if arguments are already given, accuracyof relation validation is higher compared to the sit-uation when the arguments have to be identified au-tomatically.
In either case, this methodology is ef-fective for the domain-dependent relations but is notconsidered for more generic relation types.
If a rela-tion is more generic, such as Part-Whole, it is moredifficult to identify its arguments because they canbe of many different semantic types.
An exam-ple below contains a causality relation (virus causesflu).
Note that syntactic information is not sufficientto be able to detect such relation mention and thebackground knowledge is needed.A person infected with a particular flu virusstrain develops antibody against that virus.In this paper we propose a method to detect se-mantic types of the generic relation arguments.
Forthe Part-Whole relation, it is known that it embracessuch subtypes as Member-Collection or Place-Areawhile there is not much information on the other re-lation types.
We do not claim semantic typing tobe sufficient to recognize relation mentions in text,however, it would be interesting to examine the ac-curacy of relation extraction when the backgroundknowledge only is used.
Our aim is therefore to dis-cover precise generalizations per relation type ratherthan to cover all possible relation mentions.2 A Method: Making Semantic Types ofArguments ExplicitWe propose a method for generalizing relation argu-ment types based on the positive and negative exam-ples of a given relation type.
It is also necessary thatthe arguments of a relation are annotated using somesemantic taxonomy, such as WordNet (Fellbaum,1998).
Our hypothesis is as follows: because ofthe positive and negative examples, it should be pos-sible to restrict semantic types of arguments usingnegative examples.
If negative examples are nearlypositive, the results of such generalization should beprecise.
Or, in machine learning terms, such neg-ative examples are close to the decision boundaryand if used during generalization, precision will beboosted.
If negative examples are far from the de-cision boundary, their use will most likely not helpto identify semantic types and will result in over-generalization.To test this hypothesis, we use an idea borrowedfrom induction of the deterministic finite automata.185Gx1 Gy1Gy2Gy3Gx4 Gy4Gx1 LCSGy1 ,Gy2 ,Gy3Gx4Gy2Gy4Figure 1: Generalization process.More precisely, to infer deterministic finite automata(DFA) from positive and negative examples, one firstbuilds the maximal canonical automaton (MCA)(Pernot et al, 2005) with one starting state and aseparate sequence of states for each positive exam-ple and then uses a merging strategy such that nonegative examples are accepted.Similarly, for a positive example < xi, yi > wecollect all f hyperonyms Hxi = h1xi , h2xi , .
.
.
, hfxifor xi where h1xi is an immediate hyperonym and hfxiis the most general hyperonym.
The same is done foryi.
Next, we use all negative examples to find Gxiand Gyi which are generalization types of the argu-ments of a given positive example < xi, yi >.
Inother words, we perform generalization per relationargument in a form of one positive example vs. allnegative examples.
Because of the multi-inheritancepresent in WordNet, it is possible to find more hy-peronymy paths than one.
To take it into account,the most general hyperonym hfxi equals to a splittingpoint/node.It is reasonable to assume that the presence of ageneral semantic category of one argument will re-quire a more specific semantic category for the other.Generalization per argument is, on the one hand,useful because none of the arguments share a seman-tic category with the corresponding arguments of allnegative examples.
On the other hand, it is too re-strictive if one aims at identification of the relationtype.
To avoid this, we propose to generalize seman-tic category of one argument by taking into accounta semantic category of the other.
In particular, onecan represent a binary relation as a bipartite graphwhere the corresponding nodes (relation arguments)are connected.
A natural way of generalizing wouldbe to combine the nodes which differ on the basis oftheir similarity.
In case of WordNet, we can use aleast common subsumer (LCS) of the nodes.
Giventhe bipartite graph in Figure 1, it can be done as fol-lows.
For every vertex Gxi in one part which is con-nected to several vertices Gy1 , .
.
.
, Gyk in the other,we compute LCS of Gy1 , .
.
.
, Gyk .
Note that we re-quire the semantic contrains on both arguments to besatisfied in order to validate a given relation.
Gener-alization via LCS is carried out in both directions.This step is described in more detail in Algorithm 1.Algorithm 1 Generalization via LCS1: MemoryM = ?2: Direction: ?3: for all < Gxi , Gyi >?
G do4: Collect all < Gxj , Gyj >, j = 0, .
.
.
, l s. t.Gxi = Gxj5: if exists < Gxk , Gyj > s. t. Gxi 6= Gxk then6: G = G ?
{< Gxj , Gyj >}7: end if8: Compute L = LCSGy0 ,...,Gyl9: Replace < Gxj , Gyj >,j = 0, .
.
.
, l with <Gxj ,L > in G10: M =M?
{< Gxj ,L >}11: end for12: Direction: ?13: for all < Gxi , Gyi >?
G do14: Collect all < Gxj , Gyj >, j = 0, .
.
.
, l s. t. Gyi =Gyj and< Gxj , Gyj >/?M15: Compute L = LCSGx0 ,...,Gxl16: Replace < Gxj , Gyj >, j = 0, .
.
.
, l with <L, Gyj > in G17: end for18: return GExample Consider, for instance, two sentencesfrom the SemEval data (Instrument-Agency rela-tion).013 ?The test is made by inserting theend of a <e1>jimmy</e1> or other<e2>burglar</e2>?s tool and endeavouringto produce impressions similar to those whichhave been found on doors or windows.
?WordNet(e1) = ?jimmy%1:06:00::?, Word-Net(e2) = ?burglar%1:18:00::?, Instrument-Agency(e1, e2) = ?true?040 ?<e1>Thieves</e1> used a<e2>blowtorch</e2> and bolt cuttersto force their way through a fenced area186topped with razor wire.?
WordNet(e1) =?thief%1:18:00::?, WordNet(e2) = ?blow-torch%1:06:00::?, Instrument-Agency(e2, e1)= ?true?First, we find the sense keys correspondingto the relation arguments, (?jimmy%1:06:00::?,?burglar%1:18:00::?)
= (jimmy#1, burglar#1)and (?blowtorch%1:06:00::?, ?thief%1:18:00::?)
=(blowtorch#1, thief#1).By using negative exam-ples, we obtain the following pairs: (apparatus#1,bad person#1) and (bar#3, bad person#1).
Thesepairs share the second argument and it makesit possible to apply generalization in the direc-tion ?.
LCS of apparatus#1 and bar#3 isinstrumentality#3 and hence the generalized pairbecomes (instrumentality#3, bad person#1).Note that an order in which the directions are cho-sen in Algorithm 1 does not affect the resulting gen-eralizations.
Keeping all generalized pairs in thememory M ensures that whatever direction (?
or?)
a user chooses first, the output of the algorithmwill be the same.Until now, we have considered generalization inone step only.
It would be natural to extend this ap-proach to the iterative generalization such that it isperformed until no further generalization steps canbe made (it corresponds either to the two specific ar-gument types or to the situation when the top of thehierarchy is reached).
However, such method wouldmost likely result in overgeneralization by boost-ing recall but drastically decreasing precision.
Asan alternative we propose to use memory MI de-fined over the iterations.
After each iteration stepevery generalized pair < Gxi , Gyi > is applied tothe training set and if it accepts at least one negativeexample, it is either removed from the set G (firstiteration) or this generalization pair is decomposedback into the pairs it was formed from (all other it-erations).
By employing backtracking we guaranteethat empirical error on the training set Eemp = 0.3 EvaluationData For semantic type detection, we use 7 binaryrelations from the training set of the SemEval-2007competition, all definitions of which share the re-quirement of the syntactic closeness of the argu-ments.
Further, their definitions have various restric-tions on the nature of the arguments.
Short descrip-tion of the relation types we study is given below.Cause-Effect(X,Y) This relation takes place if, givena sentence S, it is possible to entail that X is the causeof Y .
Y is usually not an entity but a nominal denotingoccurrence (activity or event).Instrument-Agency(X,Y) This relation is true if S en-tails the fact that X is the instrument of Y (Y uses X).Further, X is an entity and Y is an actor or an activity.Product-Producer(X,Y) X is a product of Y , or Yproduces X , where X is any abstract or concrete object.Origin-Entity(X,Y) X is the origin of Y where X canbe spatial or material and Y is the entity derived from theorigin.Theme-Tool(X,Y) The tool Y is intended for X is ei-ther its result or something that is acted upon.Part-Whole(X,Y) X is part of Y and this rela-tion can be one of the following five types: Place-Area, Stuff-Object, Portion-Mass, Member-Collectionand Component-Integral object.Content-Container(X,Y) A sentence S entails thefact that X is stored inside Y .
Moreover, X is not a com-ponent of Y and can be removed from it.We hypothesize that Cause-Effect and Part-Wholeare the relation types which may require sententialinformation to be detected.
These two relations al-low a greater variety of arguments and the seman-tic information alone might be not sufficient.
Suchrelation types as Product-Producer or Instrument-Agency are likely to benefit more from the externalknowledge.
Our method depends on the positive andnegative examples in the training set and on the se-mantic hierarchy we use.
If some parts of the hierar-chy are more flat, the resulting patterns may be toogeneral.As not all examples have been annotated withthe information from WordNet, we removed themform the test data while conducting this experiment.Content-Container turned out to be the only rela-tion type whose examples are fully annotated.
Incontrast, Product-Producer is a relation type withthe most information missing (9 examples removed).There is no reason to treat relation mentions as mu-tually exclusive, therefore, only negative exampleprovided for a particular relation type are used todetermine semantic types of its arguments.Discussion The entire generalization process re-sults in a zero-error on the training set.
It doesnot, however, guarantee to hold given a new dataset.
The loss in precision on the unseen exam-187Relation type P, % R, % A, % B-A, %Origin-Entity 100 26.5 67.5 55.6Content-Container 81.8 47.4 67.6 51.4Cause-Effect 100 2.8 52.7 51.2Instrument-Agency 78.3 48.7 67.6 51.3Product-Producer 77.8 38.2 52.4 66.7Theme-Tool 66.7 8.3 65.2 59.2Part-Whole 66.7 15.4 66.2 63.9avg.
81.6 26.8 62.7 57.0Table 1: Performance on the test dataples can be caused by the generalization pairs whereboth arguments are generalized to the higher levelin the hierarchy than it ought to be.
To checkhow the algorithm behaves, we first evaluate thespecialization step on the test data from the Se-mEval challenge.
Among all the relation types,only Instrument-Agency, Part-Whole and Content-Container fail to obtain 100% precision after thespecialization step.
It means that, already at thisstage, there are some false positives and the contex-tual classification is required to achieve better per-formance.The results of the method introduced here are pre-sented in Table 1.
Systems which participated inSemEval were categorized depending on the inputinformation they have used.
The category Word-Net implies that WordNet was employed but it doesnot exclude a possibility of using other resources.Therefore, to estimate how well our method per-forms, we calculated accuracy and compared itagainst a baseline that always returns the most fre-quent class label (B-A).
Given the results of theteams participating in the challenge, the organizersmention Product-Producer as one of the easiest rela-tions, while Origin-Entity and Theme-Tool are con-sidered to be ones of the hardest to detect (Girjuet al, 2007).
Interestingly, Origin-Entity obtainsthe highest precision compared to the other relationtypes while using our approach.Table 2 contains some examples of the semantictypes we found for each relation.
Some of themare quite specific (e.g., Origin-Entity), while theother arguments may be very general (e.g., Cause-Effect).
The examples of the patterns for Part-Whole can be divided in several subtypes, such asMember-Collection (person#1, social group#1),Place-Area (top side#1, whole#2) or Stuff-Object(germanium#1, mineral#1).Relation (GX , GY )Content-Container(physical entity#1, vessel#3)Instrument- (instrumentality#3, bad person#1)Agency (printing machine#1, employee#1)Cause- (cognitive operation#1, joy#1)Effect (entity#1, harm#2)(cognitive content#1,communication#2)Product- (knowledge#1, social unit#1)Producer (content#2, individual#1)(instrumentality#3,business organisation#1)Origin- (article#1, section#1)Entity (vegetation#1, plant part#1)(physical entity#1, fat#1)Theme- (abstract entity#1, implementation#2)Tool (animal#1, water#6)(nonaccomplishment#1,human action#1)Part- (top side#1, whole#2)Whole (germanium#1, mineral#1)(person#1, social group#1)Table 2: Some examples per relation type.4 ConclusionsAs expected, the semantic types derived for suchrelations as Origin-Entity, Content-Container andInstrument-Agency provide high precision on thetest data.
In contrast, precision for Theme-Tool isthe lowest which has been noted by the participantsof the SemEval-2007.
In terms of accuracy, Cause-Effect seems to obtain 100% precision but low recalland accuracy.
An explanation for that might be afact that causation can be characterized by a greatvariety of argument types many of which have beenabsent in the training data.
Origin-Entity obtains themaximal precision with accuracy much higher thanbaseline.ReferencesChristiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Nicholas Pernot, Antoine Cornue?jols, and Michele Se-bag.
2005.
Phase transition within grammatical infer-ence.
In Proceedings of IJCAI 2005.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney and Deniz Yuret.
2007.SemEval-2007 Task 04: Classification of SemanticRelations between Nominals.
In ACL 2007.188
