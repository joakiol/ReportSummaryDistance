Proceedings of ACL-08: HLT, pages 923?931,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsCredibility Improves Topical Blog Post RetrievalWouter WeerkampISLA, University of Amsterdamweerkamp@science.uva.nlMaarten de RijkeISLA, University of Amsterdammdr@science.uva.nlAbstractTopical blog post retrieval is the task of rank-ing blog posts with respect to their relevancefor a given topic.
To improve topical blog postretrieval we incorporate textual credibility in-dicators in the retrieval process.
We considertwo groups of indicators: post level (deter-mined using information about individual blogposts only) and blog level (determined usinginformation from the underlying blogs).
Wedescribe how to estimate these indicators andhow to integrate them into a retrieval approachbased on language models.
Experiments onthe TREC Blog track test set show that bothgroups of credibility indicators significantlyimprove retrieval effectiveness; the best per-formance is achieved when combining them.1 IntroductionThe growing amount of user generated content avail-able online creates new challenges for the informa-tion retrieval (IR) community, in terms of search andanalysis tasks for this type of content.
The introduc-tion of a blog retrieval track at TREC (Ounis et al,2007) has created a platform where we can begin toaddress these challenges.
During the 2006 editionof the track, two types of blog post retrieval wereconsidered: topical (retrieve posts about a topic)and opinionated (retrieve opinionated posts about atopic).
Here, we consider the former task.Blogs and blog posts offer unique features thatmay be exploited for retrieval purposes.
E.g.,Mishne (2007b) incorporates time in a blog postretrieval model to account for the fact that manyblog queries and posts are a response to a newsevent (Mishne and de Rijke, 2006).
Data qualityis an issue with blogs?the quality of posts rangesfrom low to edited news article-like.
Some ap-proaches to post retrieval use indirect quality mea-sures (e.g., elaborate spam filtering (Java et al,2007) or counting inlinks (Mishne, 2007a)).Few systems turn the credibility (Metzger, 2007)of blog posts into an aspect that can benefit the re-trieval process.
Our hypothesis is that more credibleblog posts are preferred by searchers.
The idea of us-ing credibility in the blogosphere is not new: Rubinand Liddy (2006) define a framework for assessingblog credibility, consisting of four main categories:blogger?s expertise and offline identity disclosure;blogger?s trustworthiness and value system; infor-mation quality; and appeals and triggers of a per-sonal nature.
Under these four categories the authorslist a large number of indicators, some of which canbe determined from textual sources (e.g., literary ap-peal), and some of which typically need non-textualevidence (e.g., curiosity trigger); see Section 2.We give concrete form to Rubin and Liddy(2006)?s indicators and test their impact on blog postretrieval effectiveness.
We do not consider all indi-cators: we only consider indicators that are textual innature, and to ensure reproducibility of our results,we only consider indicators that can be derived fromthe TRECBlog06 corpus (and that do not need addi-tional resources such as bloggers?
profiles that maybe hard to obtain for technical or legal reasons).We detail and implement two groups of credibilityindicators: post level (these use information aboutindividual posts) and blog level (these use informa-tion from the underlying blogs).
Within the postlevel group, we distinguish between topic depen-dent and independent indicators.
To make mattersconcrete, consider Figure 1: both posts are relevantto the query ?tennis,?
but based on obvious surfacelevel features of the posts we quickly determine Post2 to be more credible than Post 1.
The most obviousfeatures are spelling errors, the lack of leading capi-tals, and the large number of exclamation marks and923Post 1as for today (monday) we had no school!
yaaylabor day.
but we had tennis from 9-11 at thehighschool.
after that me suzi melis & ashleyhad a picnic at cecil park and then played ten-nis.
i just got home right now.
it was a veryvery very fun afternoon.
(...) we will have ashort week.
mine will be even shorter b/c iwont be there all day on friday cuz we havethe Big 7 Tournament at like keystone oaks orsumthin.
so i will miss school the whole day.Post 2Wimbledon champion Venus Williams haspulled out of next week?s Kremlin Cup witha knee injury, tournament organisers said onFriday.
The American has not played sincepulling out injured of last month?s ChinaOpen.
The former world number one has beentroubled by various injuries (...) Williams?swithdrawal is the latest blow for organisers af-ter Australian Open champion and home fa-vorite Marat Safin withdrew (...).Figure 1: Two blog posts relevant to the query ?tennis.
?personal pronouns?i.e., topic independent ones?and the fact that the language usage in the secondpost is more easily associated with credible infor-mation about tennis than the language usage in thefirst post?i.e., a topic dependent feature.Our main finding is that topical blog post retrievalcan benefit from using credibility indicators in theretrieval process.
Both post and blog level indi-cator groups each show a significant improvementover the baseline.
When we combine all featureswe obtain the best retrieval performance, and thisperformance is comparable to the best performingTREC 2006 and 2007 Blog track participants.
Theimprovement over the baseline is stable across mosttopics, although topic shift occurs in a few cases.The rest of the paper is organized as follows.
InSection 2 we provide information on determiningcredibility; we also relate previous work to the cred-ibility indicators that we consider.
Section 3 speci-fies our retrieval model, a method for incorporatingcredibility indicators in our retrieval model, and es-timations of credibility indicators.
Section 4 givesthe results of our experiments aimed at assessingthe contribution of credibility towards blog post re-trieval effectiveness.
We conclude in Section 5.2 Credibility IndicatorsIn our choice of credibility indicators we use (Ru-bin and Liddy, 2006)?s work as a reference point.We recall the main points of their framework andrelate our indicators to it.
We briefly discuss othercredibility-related indicators found in the literature.2.1 Rubin and Liddy (2006)?s workRubin and Liddy (2006) proposed a four factor an-alytical framework for blog-readers?
credibility as-sessment of blog sites, based in part on evidential-ity theory (Chafe, 1986), website credibility assess-ment surveys (Stanford et al, 2002), and Van House(2004)?s observations on blog credibility.
The fourfactors?plus indicators for each of them?are:1. blogger?s expertise and offline identity disclo-sure (a: name and geographic location; b: cre-dentials; c: affiliations; d: hyperlinks to others;e: stated competencies; f : mode of knowing);2. blogger?s trustworthiness and value system (a:biases; b: beliefs; c: opinions; d: honesty; e:preferences; f : habits; g: slogans)3. information quality (a: completeness; b: ac-curacy; c: appropriateness; d: timeliness; e:organization (by categories or chronology); f :match to prior expectations; g: match to infor-mation need); and4.
appeals and triggers of a personal nature (a:aesthetic appeal; b: literary appeal (i.e., writingstyle); c: curiosity trigger; d: memory trigger;e: personal connection).2.2 Our credibility indicatorsWe only consider credibility indicators that avoidmaking use of the searcher?s or blogger?s identity(i.e., excluding 1a, 1c, 1e, 1f, 2e from Rubin andLiddy?s list), that can be estimated automaticallyfrom available test collections only so as to facilitaterepeatability of our experiments (ruling out 3e, 4a,4c, 4d, 4e), that are textual in nature (ruling out 2d),and that can be reliably estimated with state-of-the-art language technology (ruling out 2a, 2b, 2c, 2g).For reasons that we explain below, we also ignorethe ?hyperlinks to others?
indicator (1d).The indicators that we do consider?1b, 2f, 3a,3b, 3c, 3d, 3f, 3g, 4b?are organized in two groups,924depending on the information source that we useto estimate them, post level and blog level, and theformer is further subdivided into topic independentand topic dependent.
Table 1 lists the indicators weconsider, together with the corresponding Rubin andLiddy indicator(s).Let us quickly explain our indicators.
First, weconsider the use of capitalization to be an indicatorof good writing style, which in turn contributes toa sense of credibility.
Second, we identify West-ern style emoticons (e.g., :-) and :-D) in blogposts, and assume that excessive use indicates a lesscredible blog post.
Third, words written in all capsare considered shouting in a web environment; weconsider shouting to be indicative for non-credibleposts.
Fourth, a credible author should be able towrite without (a lot of) spelling errors; the morespelling errors occur in a blog post, the less credi-ble we consider it to be.
Fifth, we assume that cred-ible texts have a reasonable length; the text shouldsupply enough information to convince the reader ofthe author?s credibility.
Sixth, assuming that muchof what goes on in the blogosphere is inspired byevents in the news (Mishne and de Rijke, 2006), webelieve that, for news related topics, a blog post ismore credible if it is published around the time ofthe triggering news event (timeliness).
Seventh, oursemantic indicator also exploits the news-related na-ture of many blog posts, and ?prefers?
posts whoselanguage usage is similar to news stories on thetopic.
Eighth, blogs are a popular place for spam-mers; spam blogs are not considered credible and wewant to demote them in the search results.
Ninth,comments are a notable blog feature: readers of ablog post often have the possibility of leaving a com-ment for other readers or the author.
When peo-ple comment on a blog post they apparently find thepost worth putting effort in, which can be seen as anindicator of credibility (Mishne and Glance, 2006).Tenth, blogs consist of multiple posts in (reverse)chronological order.
The temporal aspect of blogsmay indicate credibility: we assume that bloggerswith an irregular posting behavior are less crediblethan bloggers who post regularly.
And, finally, weconsider the topical fluctuation of a blogger?s posts.When looking for credible information we wouldlike to retrieve posts from bloggers that have a cer-tain level of (topical) consistency: not the fluctuatingindicator topic de- post level/ related Rubin &pendent?
blog level Liddy indicatorcapitalization no post 4bemoticons no post 4bshouting no post 4bspelling no post 4bpost length no post 3atimeliness yes post 3dsemantic yes post 3b, 3cspam no blog 3b, 3c, 3f, 3gcomments no blog 1bregularity no blog 2fconsistency no blog 2fTable 1: Credibility indicatorsbehavior of a (personal) blogger, but a solid interest.2.3 Other workIn a web setting, credibility is often couched interms of authoritativeness and estimated by exploit-ing the hyperlink structure.
Two well-known exam-ples are the PageRank and HITS algorithms (Liu,2007), that use the link structure in a topic indepen-dent or topic dependent way, respectively.
Zhou andCroft (2005) propose collection-document distanceand signal-to-noise ratio as priors for the indicationof quality in web ad hoc retrieval.
The idea of usinglink structure for improving blog post retrieval hasbeen researched, but results do not show improve-ments.
E.g., Mishne (2007a) finds that retrieval per-formance decreased.
This confirms lessons fromthe TREC web tracks, where participants found noconclusive benefit from the use of link informationfor ad hoc retrieval tasks (Hawking and Craswell,2002).
Hence, we restrict ourselves to the use ofcontent-based features for blog post retrieval, thusignoring indicator 1d (hyperlinks to others).Related to credibility in blogs is the automatic as-sessment of forum post quality discussed by Weimeret al (2007).
The authors use surface, lexical, syn-tactic and forum-specific features to classify forumposts as bad posts or good posts.
The use of forum-specific features (such as whether or not the postcontains HTML, and the fraction of characters thatare inside quotes of other posts), gives the highestbenefits to the classification.
Working in the com-munity question/answering domain, Agichtein et al(2008) use a content features, as well non-content in-formation available, such as links between items and925explicit quality ratings from members of the com-munity to identify high-quality content.As we argued above, spam identification may bepart of estimating a blog (or blog post?s) credibility.Spam identification has been successfully applied inthe blogosphere to improve retrieval effectiveness;see, e.g., (Mishne, 2007b; Java et al, 2007).3 ModelingIn this section we detail the retrieval model that weuse, incorporating ranking by relevance and by cred-ibility.
We also describe how we estimate the credi-bility indicators listed in Section 2.3.1 Baseline retrieval modelWe address the baseline retrieval task using alanguage modeling approach (Croft and Lafferty,2003), where we rank documents given a query:p(d|q) = p(d)p(q|d)p(q)?1.
Using Bayes?
Theo-rem we rewrite this, ignoring expressions that do notinfluence the ranking, obtainingp(d|q) ?
p(d)p(q|d), (1)and, assuming that query terms are independent,p(d|q) ?
p(d)?t?q p(t|?d)n(t,q), (2)where ?d is the blog post model, and n(t, q) denotesthe number of times term t occurs in query q. Toprevent numerical underflows, we perform this com-putation in the log domain:log p(d|q) ?
log p(d) +?t?qn(t, q) log p(t|?d) (3)In our final formula for ranking posts based on rel-evance only we substitute n(t, q) by the probabilityof the term given the query.
This allows us to assigndifferent weights to query terms and yields:log p(d|q) ?
log p(d) +?t?qp(t|q) log p(t|?d).
(4)For our baseline experiments we assume that allquery terms are equally important and set p(t|q) setto be n(t, q) ?
|q|?1.
The component p(d) is the topicindependent (?prior?)
probability that the documentis relevant; in the baseline model, priors are ignored.3.2 Incorporating credibilityNext, we extend Eq.
4 by incorporating estimationsof the credibility indicators listed in Table 1.
Recallthat our credibility indicators come in two kinds?post level and blog level?and that the post levelindicators can be topic indepedent or topic depen-dent, while all blog level indicators are topic inde-pendent.
Now, modeling topic independent indi-cators is easy?they can simply be incorporated inEq.
4 as a weighted sum of two priors:p(d) = ?
?
ppl(d) + (1?
?)
?
pbl(d), (5)where ppl(d) and pbl(d) are the post level and bloglevel prior probability of d, respectively.
The priorsppl and pbl are defined as equally weighted sums:ppl(d) =?i15 ?
pi(d)pbl(d) =?j14 ?
pj(d),where i ?
{capitalization, emoticons, shouting,spelling, post length} and j ?
{spam, comments,regularity, consistency}.
Estimations of the priorspi and pj are given below; the weighting parameter?
is determined experimentally.Modeling topic dependent indicators is slightymore involved.
Given a query q, we create a querymodel ?q that is a mixture of a temporal query model?temporal and a semantic query model ?semantic:p(t|?q) = (6)?
?
p(t|?temporal) + (1?
?)
?
p(t|?semantic).The component models ?temporal and ?semantic willbe estimated below; the parameter ?
will be esti-mated experimentally.Our final ranking formula, then, is obtained byplugging in Eq.
5 and 6 in Eq.
4:log p(d|q) ?
log p(d)+ ?
(?t p(t|q) ?
log p(t|?d)) (7)+ (1?
?)
(?t p(t|?q) ?
log p(t|?d)) .3.3 Estimating credibility indicatorsNext, we specify how each of the credibility indica-tors is estimated; we do so in two groups: post leveland blog level.9263.3.1 Post level credibility indicatorsCapitalization We estimate the capitalizationprior as follows:pcapitalization(d) = n(c, s) ?
|s|?1, (8)where n(c, s) is the number of sentences startingwith a capital and |s| is the number of sentences;we only consider sentences with five or more words.Emoticons The emoticons prior is estimated aspemoticons(d) = 1?
n(e, d) ?
|d|?1, (9)where n(e, d) is the number of emoticons in the postand |d| is the length of the post in words.Shouting We use the following equation to esti-mate the shouting prior:pshouting(d) = 1?
n(a, d) ?
|d|?1, (10)where n(a, d) is the number of all caps words in blogpost d and |d| is the post length in words.Spelling The spelling prior is estimated aspspelling(d) = 1?
n(m, d) ?
|d|?1, (11)where n(m, d) is the number of misspelled (or un-known) words and |d| is the post length in words.Post length The post length prior is estimated us-ing |d|, the post length in words:plength(d) = log(|d|).
(12)Timeliness We estimate timeliness using the time-based language models ?temporal proposed in (Liand Croft, 2003; Mishne, 2007b).
I.e., we use a newscorpus from the same period as the blog corpus thatwe use for evaluation purposes (see Section 4.2).
Weassign a timeliness score per post based on:p(d|?temporal) = k?1 ?
(n(date(d), k) + 1) , (13)where k is the number of top results from the initialresult list, date(d) is the date associated with doc-ument d, and n(date(d), k) is the number of docu-ments in k with the same date as d. For our initialresult list we perform retrieval on both the blog andthe news corpus and take k = 50 for both corpora.Semantic A semantic query model ?semantic isobtained using ideas due to Diaz and Metzler (2006).Again, we use a news corpus from the same periodas the evaluation blog corpus and estimate ?semantic.We issue the query to the external news corpus, re-trieve the top 10 documents and extract the top 10distinctive terms from these documents.
These termsare added to the original query terms to capture thelanguage usage around the topic.3.3.2 Blog level credibility indicatorsSpam filtering To estimate the spaminess of ablog, we take a simple approach.
We train an SVMclassifier on a labeled splog blog dataset (Kolariet al, 2006) using the top 1500 words for both spamand non-spam blogs as features.
For each classifiedblog d we have a confidence value s(d).
If the clas-sifier cannot make a decision (s(d) = 0) we setpspam(d) to 0, otherwise we use the following totransform s(d) into a spam prior pspam(d):pspam(d) =s(d)2|s(d)|+?1 ?
s(d)2s(d)2 + 2|s(d)|+12.
(14)Comments We estimate the comment prior aspcomment(d) = log(n(r, d)), (15)where n(r, d) is the number of comments on post d.Regularity To estimate the regularity prior we usepregularity(d) = log(?interval), (16)where ?interval expresses the standard deviation ofthe temporal intervals between two successive posts.Topical consistency Here we use an approachsimilar to query clarity (Cronen-Townsend andCroft, 2002): based on the list of posts from thesame blog we compare the topic distribution of blogB to the topic distribution in the collection C andassign a ?clarity?
value to B; a score further awayfrom zero indicates a higher topical consistency.
Weestimate the topical consistency prior asptopic(d) = log(clarity(d)), (17)where clarity(d) is estimated byclarity(d) =?w p(w|B) ?
log(p(w|B)p(w))?w p(w|B)(18)with p(w) = count(w,C)|C| and p(w|B) =count(w,B)|B| .9273.3.3 EfficiencyAll estimators discussed above can be imple-mented efficiently: most are document priors andcan therefore be calculated offline.
The only topicdependent estimators are timeliness and languageusage; both can be implemented efficiently as spe-cific forms of query expansion.4 EvaluationIn this section we describe the experiments we con-ducted to answer our research questions about theimpact of credibility on blog post retrieval.4.1 Research questionsOur research revolves around the contribution ofcredibility to the effectiveness of topical blog postretrieval: what is the contribution of individual indi-cators, of the post level indicators (topic dependentor independent), of the blog level indicators, and ofall indicators combined?
And do different topicsbenefit from different indicators?
To answer our re-search question we compared the performance of thebaseline retrieval system (as detailed in Section 3.1)with extensions of the baseline system with a singleindicator, a set of indicators, or all indicators.4.2 SetupWe apply our models to the TREC Blog06 cor-pus (Macdonald and Ounis, 2006).
This corpushas been constructed by monitoring around 100,000blog feeds for a period of 11 weeks in early 2006,downloading all posts created in this period.
Foreach permalink (HTML page containing one blogpost) the feed id is registered.
We can use this idto aggregate post level features to the blog level.
Inour experiments we use only the HTML documents,3.2M permalinks, which add up to around 88 GB.The TREC 2006 and 2007 Blog tracks each offer50 topics and assessments (Ounis et al, 2007; Mac-donald et al, 2007).
For topical relevancy, assess-ment was done using a standard two-level scale: thecontent of the post was judged to be topically rele-vant or not.
The evaluation metrics that we use arestandard ones: mean average precision (MAP) andprecision@10 (p@10) (Baeza-Yates and Ribeiro-Neto, 1999).
For all our retrieval tasks we use thetitle field (T) of the topic statement as query.To estimate the timeliness and semantic cred-ibility indicators, we use AQUAINT-2, a set ofnewswire articles (2.5 GB, about 907K documents)that are roughly contemporaneous with the TRECBlog06 collection (AQUAINT-2, 2007).
Articles arein English and come from a variety of sources.Statistical significance is tested using a two-tailedpaired t-test.
Significant improvements over thebaseline are marked with M (?
= 0.05) or N (?
=0.01).
We use O and H for a drop in performance(for ?
= 0.05 and ?
= 0.01, respectively).4.3 Parameter estimationThe models proposed in Section 3.2 contain param-eters ?, ?
and ?.
These parameters need to be esti-mated and, hence, require a training and test set.
Weuse a two-fold parameter estimation process: in thefirst cycle we estimate the parameters on the TREC2006 Blog topic set and test these settings on the top-ics of the TREC 2007 Blog track.
The second cyclegoes the other way around and trains on the 2007set, while testing on the 2006 set.Figure 2 shows the optimum values for ?, ?, and?
on the 2006 and the 2007 topic sets for both MAP(bottom lines) and p@10 (top lines).
When look-ing at the MAP scores, the optimal setting for ?
isalmost identical for the two topic sets: 0.4 for the2006 set and 0.3 for the 2007 set, and also the op-timal setting for ?
is very similar for both sets: 0.4for the 2006 set and 0.5 for the 2007 set.
As to ?,it is clear that timeliness does not improve the per-formance over using the semantic feature alone andthe optimal setting for ?
is therefore 0.0.
Both ?and ?
show similar behavior on p@10 as on MAP,but for ?
we see a different trend.
If early precisionis required, the value of ?
should be increased, giv-ing more weight to the topic-independent post levelfeatures compared to the blog level features.4.4 Retrieval performanceTable 2 lists the retrieval results for the baseline, foreach of the credibility indicators (on top of the base-line), for four subsets of indicators, and for all in-dicators combined.
The baseline performs similarto the median scores at the TREC 2006 Blog track(MAP: 0.2203; p@10: 0.564) and somewhat belowthe median MAP score at 2007 Blog track (MAP:0.3340) but above the median p@10 score: 0.3805.92800.10.20.30.40.50.60.7MAP/ P10lambda2006200700.10.20.30.40.50.60.7MAP/ P10beta2006200700.10.20.30.40.50.60.7MAP/ P10mu20062007Figure 2: Parameter estimation on the TREC 2006 and 2007 Blog topics.
(Left): ?.
(Center): ?.
(Right): ?.2006 2007map p@10 map p@10baseline 0.2156 0.4360 0.2820 0.5160capitalization 0.2155 0.4500 0.2824 0.5160emoticons 0.2156 0.4360 0.2820 0.5200shouting 0.2159 0.4320 0.2833 0.5100spelling 0.2179M 0.4480M 0.2839N 0.5220post length 0.2502N 0.4960N 0.3112N 0.5700Ntimeliness 0.1865H 0.4520 0.2660 0.4860semantic 0.2840N 0.6240N 0.3379N 0.6640Nspam filtering 0.2093 0.4700 0.2814 0.5760Ncomments 0.2497N 0.5000N 0.3099N 0.5600Nregularity 0.1658H 0.4940M 0.2353H 0.5640Mconsistency 0.2141H 0.4220 0.2785O 0.5040post level 0.2374N 0.4920N 0.2990N 0.5660N(topic indep.
)post level 0.2840N 0.6240N 0.3379N 0.6640N(topic dep.
)post level 0.2911N 0.6380N 0.3369N 0.6620N(all)blog level 0.2391N 0.4500 0.3023N 0.5580Nall 0.3051N 0.6880N 0.3530N 0.6900NTable 2: Retrieval performance on 2006 and 2007 topics,using ?
= 0.3, ?
= 0.4, and ?
= 0.0.Some (topic independent) post level indicatorshurt the MAP score, while others help (for bothyears, and both measures).
Combined, the topicindependent post level indicators perform less wellthan the use of one of them (post length).
As tothe topic dependent post level indicators, timelinesshurts performance on MAP for both years, whilethe semantic indicator provides significant improve-ments across the board (resulting in a top 2 score interms of MAP and a top 5 score in terms of p@10,when compared to the TREC 2006 Blog track par-ticipants that only used the T field).Some of the blog level features hurt more thanthey help (regularity, consistency), while the com-ments feature helps, on all measures, and for bothyears.
Combined, the blog level features help lessthan the use of one of them (comments).As a group, the combined post level features helpmore than either of the two post level sub groupsalone.
The blog level features show similar results tothe topic-independent post level features, obtaininga significant increase on both MAP and p@10, butlower than the topic-dependent post level features.The grand combination of all credibility indica-tors leads to a significant improvement over any ofthe single indicators and over any of the four subsetsconsidered in Table 2.
The MAP score of this runis higher than the best performing run in the TREC2006 Blog track and has a top 3 performance onp@10; its 2007 performance is just within the tophalf on both MAP and p@10.4.5 AnalysisNext we examine the differences in average preci-sion (per topic) between the baseline and subsets ofindicators (post and blog level) and the grand com-bination.
We limit ourselves to an analysis of theMAP scores.
Figure 3 displays the per topic averageprecision scores, where topics are sorted by absolutegain of the grand combination over the baseline.In 2006, 7 (out of 50) topics were negatively af-fected by the use of credibility indicators; in 2007,15 (out of 50) were negatively affected.
Table 3 liststhe topics that displayed extreme behavior (in termsof relative performance gain or drop in AP score).While the extreme drops for both years are in thesame range, the gains for 2006 are more extremethan for 2007.The topic that is hurt most (in absolute terms)by the credibility indicators is the 2007 topic 910:aperto network (AP -0.2781).
The semantic indi-cator is to blame for this decrease is: the terms in-cluded in the expanded query shift the topic from awireless broadband provider to television networks.929-0.4-0.3-0.2-0.100.10.20.30.4APdifferencetopicsallpostblog-0.4-0.3-0.2-0.100.10.20.30.4APdifferencetopicsallpostblogFigure 3: Per-topic AP differences between baseline run and runs with blog level features (triangles), post level features(circles) and all feature (squares) on the 2006 (left) en 2007 (right) topics.Table 3: Extreme performance gains/drops of the grandcombination over the baseline (MAP).2006id topic % gain/loss900 mcdonalds +525.9%866 foods +446.2%865 basque +308.6%862 blackberry -21.5%870 barry bonds -35.2%898 business intelligence resources -78.8%2007id topic % gain/loss923 challenger +162.1%926 hawthorne heights +160.7%945 bolivia +125.5%943 censure -49.4%928 big love -80.0%904 alterman -84.2%Topics that gain most (in absolute terms) are 947(sasha cohen; AP +0.3809) and 923 (challenger; AP+0.3622) from the 2007 topic set.Finally, the combination of all credibility indica-tors hurts 7 (2006) plus 15 (2007) equals 22 topics;for the post level indicators get a performance dropin AP for 28 topics (10 plus 18, respectively) and forthe blog level indicators we get a drop for 15 topics(4 plus 11, respectively).
Hence, the combination ofall indicators strikes a good balance between overallperformance gain and per topic risk.5 ConclusionsWe provided efficient estimations for 11 credibilityindicators and assessed their impact on topical blogpost retrieval, on top of a content-based retrievalbaseline.
We compared the contribution of these in-dicators, both individually and in groups, and foundthat (combined) they have a significant positive im-pact on topical blog post retrieval effectiveness.
Cer-tain single indicators, like post length and comments,make good credibility indicators on their own; thebest performing credibility indicator group consistsof topic dependent post level ones.
Other futurework concerns indicator selection: instead of takingall indicators on board, consider selected indicatorsonly, in a topic dependent fashion.Our choice of credibility indicators was based ona framework proposed by Rubin and Liddy (2006):the estimators we used are natural implementationsof the selected indicators, but by no means the onlypossible ones.
In future work we intend to extendthe set of indicators considered so as to include, e.g.,stated competencies (1e), by harvesting and analyz-ing bloggers?
profiles, and to extend the set of esti-mators for indicators that we already consider suchas reading level measures (e.g., Flesch-Kincaid) forthe literary appeal indicator (4b).AcknowledgmentsWe would like to thank our reviewers for their feed-back.
Both authors were supported by the E.U.
ISTprogramme of the 6th FP for RTD under projectMultiMATCH contract IST-033104.
De Rijke wasalso supported by NWO under project numbers017.001.190, 220-80-001, 264-70-050, 354-20-005,600.065.120, 612-13-001, 612.000.106, 612.066.-302, 612.069.006, 640.001.501, and 640.002.501.930ReferencesAgichtein, E., Castillo, C., Donato, D., Gionis, A., andMishne, G. (2008).
Finding high-quality content insocial media.
In WSDM ?08.AQUAINT-2 (2007).
URL: http://trec.nist.gov/data/qa/2007_qadata/qa.07.guidelines.html#documents.Baeza-Yates, R. and Ribeiro-Neto, B.
(1999).
ModernInformation Retrieval.
Addison Wesley.Chafe, W. (1986).
Evidentiality in English conversionand academic writing.
In Chaf, W. and Nichols, J., ed-itors, Evidentiality: The Linguistic Coding of Episte-mology, volume 20, pages 261?273.
Ablex PublishingCorporation.Croft, W. B. and Lafferty, J., editors (2003).
LanguageModeling for Information Retrieval.
Kluwer.Cronen-Townsend, S. and Croft, W. (2002).
Quantifyingquery ambiguity.
In Proceedings of Human LanguageTechnology 2002, pages 94?98.Diaz, F. and Metzler, D. (2006).
Improving the estima-tion of relevance models using large external corpora.In SIGIR ?06: Proceedings of the 29th annual interna-tional ACM SIGIR conference on Research and devel-opment in information retrieval, pages 154?161, NewYork.
ACM Press.Hawking, D. and Craswell, N. (2002).
Overview of theTREC-2001 web track.
In The Tenth Text RetrievalConferences (TREC-2001), pages 25?31.Java, A., Kolari, P., Finin, T., Joshi, A., and Martineau, J.(2007).
The blogvox opinion retrieval system.
In TheFifteenth Text REtrieval Conference (TREC 2006).Kolari, P., Finin, T., Java, A., and Joshi, A.(2006).
Splog blog dataset.
URL: http://ebiquity.umbc.edu/resource/html/id/212/Splog-Blog-Dataset.Li, X. and Croft, W. (2003).
Time-based language mod-els.
In Proceedings of the 12th International Con-ference on Information and Knowledge Managment(CIKM), pages 469?475.Liu, B.
(2007).
Web Data Mining.
Springer-Verlag, Hei-delberg.Macdonald, C. and Ounis, I.
(2006).
The trec blogs06collection: Creating and analyzing a blog test collec-tion.
Technical Report TR-2006-224, Department ofComputer Science, University of Glasgow.Macdonald, C., Ounis, I., and Soboroff, I.
(2007).Overview of the trec 2007 blog track.
In TREC 2007Working Notes, pages 31?43.Metzger, M. (2007).
Making sense of credibility on theweb: Models for evaluating online information andrecommendations for future research.
Journl of theAmerican Society for Information Science and Tech-nology, 58(13):2078?2091.Mishne, G. (2007a).
Applied Text Analytics for Blogs.PhD thesis, University of Amsterdam, Amsterdam.Mishne, G. (2007b).
Using blog properties to improveretrieval.
In Proceedings of ICWSM 2007.Mishne, G. and de Rijke, M. (2006).
A study of blogsearch.
In Lalmas, M., MacFarlane, A., Ru?ger, S.,Tombros, A., Tsikrika, T., and Yavlinsky, A., editors,Advances in Information Retrieval: Proceedings 28thEuropean Conference on IR Research (ECIR 2006),volume 3936 of LNCS, pages 289?301.
Springer.Mishne, G. and Glance, N. (2006).
Leave a reply: Ananalysis of weblog comments.
In Proceedings ofWWW 2006.Ounis, I., de Rijke, M., Macdonald, C., Mishne, G.,and Soboroff, I.
(2007).
Overview of the trec-2006blog track.
In The Fifteenth Text REtrieval Conference(TREC 2006) Proceedings.Rubin, V. and Liddy, E. (2006).
Assessing credibilityof weblogs.
In Proceedings of the AAAI Spring Sym-posium: Computational Approaches to Analyzing We-blogs (CAAW).Stanford, J., Tauber, E., Fogg, B., and Marable, L. (2002).Experts vs online consumers: A comparative cred-ibility study of health and finance web sites.
URL:http://www.consumerwebwatch.org/news/report3_credibilityresearch/slicedbread.pdf.Van House, N. (2004).
Weblogs: Credibility andcollaboration in an online world.
URL: people.ischool.berkeley.edu/?vanhouse/Van\%20House\%20trust\%20workshop.pdf.Weimer, M., Gurevych, I., and Mehlhauser, M. (2007).Automatically assessing the post quality in online dis-cussions on software.
In Proceedings of the ACL 2007Demo and Poster Sessions, pages 125?128.Zhou, Y. and Croft, W. B.
(2005).
Document qualitymodels for web ad hoc retrieval.
In CIKM ?05: Pro-ceedings of the 14th ACM international conference onInformation and knowledge management, pages 331?332.931
