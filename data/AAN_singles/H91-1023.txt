SESSION 3: MACHINE TRANSLATIONJaime CarbonellCarnegie Me l lon  Univers i tyCenter for Machine Translat ionP i t tsburgh,  PA  15213Machine Translation (MT) technology has progressedsignificantly since the days of the ALPAC* report.
Inparticular, multiple paradigms are being investigated rangingfrom statistical methods to full knowledge-based interlingualMT systems.
Much of the recent work is based on advances innatural language processing since ALPAC in the 1960s,including:?
Semantic  analys is  to resolve lexical and syntacticambiguities during parsing, and thus reduce translationerrors very significantly.?
Un i f i ca t ion  grammars  allowing syntactic andsemantic onstraints to be checked in a unified mannerwhile parsing, and permitting reversible grammars--i.e.,the same grammars to be used for generation as well as foranalysis.?
Advanced pars ing  methodo log ies ,  includingaugmented-LR compilation where knowledge sources(syntactic grammars, lexicons, and semantic ontologies)can be defined and maintained separately but are jointlycompiled to apply simultaneously at run time, both inparsing and in generation.?
Natura l  language generat ion ,  focusing on how tostructure fluent target-language output, an activity nottruly investigated in the pre-ALPAC days.?
Automated  corpus  ana lys is  tools,  statistical andother means of extracting useful information from largebi- or multi-lingual corpora, including collocations,transfers, and contextual cues for disambiguation.?
MRDs => MTDs, use of electronic machine-readabledictionaries (MRDs) to partially automate the creation ofmachine-tractable dictionaries (MTDs) in processableinternal form for parsers and generators, permittingprincipled sealing up in MT configurations.APPROACHES TO MODERN MTIn light of these advances, several major MT paradigms haveevolved to supplant the early hand-coded direct-transfermethods.
One approach is purely statistical, as practiced atIn 1965 the United States Academy of Science cemmissoned a study of hestate of the art in Machine Translation, whose findings were published thefollowing year and become popularly known as the ALPAC report.
Inessence, ALPAC argued that there was insufficient scientific basis in naturallangauge processing to perform reliable machine t~anslatinn, and the largeexpensive computers of the time would make NIT eeonornically infeasible.Both situations have since changed rastically, invaLidating the ALPACconciusions.
In fact, DARPA has played a major role in fostering thedevelopment of the NLP scientific infrsstmcture in the post-ALPAC years.IBM, in which the direct-transfer paradigm is still king andtranslation is viewed as transduction between two character (orword) streams--essential ly two encodings of the samemessage.
However, the direct transfer ules are totally learnedby statistical analysis of large bi-lingual corpora, rather thanlaboriously and incompletely hand-coded.
A drawback of thestatistical approach, of course, is that it carmot guarantee theaccuracy of any textual passage being translated, but ratherstrives to minimize the total number of errors over time.Another MT approach is to provide a measure of analysis ofthe source language prior to transfer.
At minimum,morphological and syntactic analysis is performed, then thetransfer component transforms the parse trees intocorresponding parse trees in the target language withappropriate l xical substitution.
These transformed parse treesare then used to generate the desired target exts.
Performinganalysis and generation reduces the size of the transfercomponent, which is a major benefit, considering thattranslation across N languages requires O(N 2) transfergrammars.
Transfer at the syntactic level represents theclassical approach on which most commercial attempts at MTare based.
The problem with classical transfer is that it toomakes a significant number of errors in the translation,primarily through its inability to reduce much of the lexical andsyntactic ambiguity of the source language texts.A deeper analysis, including semantic restrictions to producecase frames (rather than parse trees), reduces both the number oferrors (as some ambiguity has been resolved) and the size of thetransfer component (case frame representations in differentlanguages will be much more similar to each other thansyntactic parse trees).
Some Japanese firms working inmachine translation, for instance, have adopted this approach,which they call semantic transfer.
Since Japanese and Englishare more different han two Indo-European languages, there ismore justification for the deeper level of analysis and the desireto minimize the size of the transfer component.The deepest level of analysis produces an underlying non-ambiguous emantic representation, i dependent of both sourceand target language.
This is called the interlingua orpivotapproach, and it trades off much more work at analysis andgeneration for no work at all in the transfer phase.
Benefitsinclude much lower errors (as ambiguities must be resolved toproduce interlingua), and no N 2 problem, as there is no transfercomponent.
Thus the interlingua pproach is particularly well-suited for multi-lingual translation.
The most serious problemwith the interlingual approach is its requirements for vastknowledge bases if one desires general-purpose, highlyaccurate translation in any domain.
Specialized-domaininterlingual systems are far more practical.139EVALUATION METHODSYorick Wilks, one of America's foremost MT researchers,states that "Machine translation evaluations methods are betterdeveloped than machine translation itself."
In a sense, he isright.
Since MT is a complete throughput process from sourceto target ext that corresponds precisely to the task of humantranslations, it is not difficult to compare the two and providerelative assessments.
MT has been evaluated with respect osemantic accuracy of the translation and intelligibility of thefinal output.
But, other factors uch as degree of automation areequally relevant.
The more human intervention (e.g., pre andpost editing) required to produce a good translation, the lessuseful the MT system.MT evaluations must always be made task-relative.
On theone hand, MT for the sole purpose of scanning translated textsin order to establish their relevance to a given topic must befast, with little if any human assistance, but can be rough,partially inaccurate and of low-legibility output.
On the otherhand, technical or legal texts translated for publication must beaccurate and legible, although slower processing and additionalhuman assistance may be tolerated.
Therefore, one challengefor the DARPA MT research community is to develop moreappropriate, task-sensitive and comprehensive evaluationcriteria.PREVIEW OF MT PAPERSThe three papers on machine translation i this section coveronly part of the DARPA MT effort--and this should beinterpreted as a sign of the breadth of coverage and DARPAinterest in the field, pursuing different technologies atdifferentsites.
In particular, the knowledge-based interlinguaapproaches of CMU, CRL and ISI are not represented, but form amajor component of the overall program.
Those areasrepresented in this volume include translation as abduetivereasoning at SRI and two papers on the role of statistics inmachine translation.
More precisely, statistical word-sensedisambiguation at IBM and establishing lexical-transfercorrespondences for MT at AT&T are discussed in some detail.Jerry Hobbs and Megurni Kameyama t SRI extend theirexisting TACITUS architecture for abductive natural languageinterpretation and apply it to the task of translating a fewexamples into Japanese.
The work is exciting in the sense ofshowing how an existing system and underlying theory aresufficiently general to address the new task: machinetranslation across radically different languages.
Other issues,such as sealing up, are not yet addressed in this work.Peter Brown, Stephen Delia Pietra, Vincent Della Pietra andRobert Mercer at IBM developed a statistically-based word-sense disambiguation method as an integral component of astatistical machine translation system.
In fact, statistical helpin word disambiguation may prove to be of major help in moretraditional MT approaches (transfer and interlingua) whendefinitive semantic and syntactic knowledge do not narrowdown word senses to a single candidate.
Therefore this workshould be followed closely by those researchers of the non-statistical-MT persuasion as well.William Gale and Ken Church develop a mathematicalinfrastructure for determining lexical correspondences acrosswords in parallel ~,-~s.
Parallel text means that the same text isavailable in two (or more) languages.
Establishing lexicalcorrespondences is crucial for building knowledge bases forsymbolic translation methods (whether transfer or interlingual)and for automated training of statistical translation methodssuch as that advocated at IBM by Brown, Mercer, et al Gale andChurch argue that their methods are statistically more reliablethan the earlier IBM methods for establishing wordcorrespondences.140
