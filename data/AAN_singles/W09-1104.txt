Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 12?20,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsData-Driven Dependency Parsing of New LanguagesUsing Incomplete and Noisy Training DataKathrin Spreyer and Jonas KuhnDepartment of LinguisticsUniversity of Potsdam, Germany{spreyer,kuhn}@ling.uni-potsdam.deAbstractWe present a simple but very effective ap-proach to identifying high-quality data innoisy data sets for structured problems likeparsing, by greedily exploiting partial struc-tures.
We analyze our approach in an anno-tation projection framework for dependencytrees, and show how dependency parsers fromtwo different paradigms (graph-based andtransition-based) can be trained on the result-ing tree fragments.
We train parsers for Dutchto evaluate our method and to investigateto which degree graph-based and transition-based parsers can benefit from incompletetraining data.
We find that partial correspon-dence projection gives rise to parsers that out-perform parsers trained on aggressively fil-tered data sets, and achieve unlabeled attach-ment scores that are only 5% behind the aver-age UAS for Dutch in the CoNLL-X SharedTask on supervised parsing (Buchholz andMarsi, 2006).1 IntroductionMany weakly supervised approaches to NLP rely onheuristics or filtering techniques to deal with noisein unlabeled or automatically labeled training data,e.g., in the exploitation of parallel corpora for cross-lingual projection of morphological, syntactic or se-mantic information.
While heuristic approaches canimplement (linguistic) knowledge that helps to de-tect noisy data (e.g., Hwa et al (2005)), they are typ-ically task- and language-specific and thus introducea component of indirect supervision.
Non-heuristicfiltering techniques, on the other hand, employ re-liability measures (often unrelated to the task) topredict high-precision data points (e.g., Yarowskyet al (2001)).
In order to reach a sufficient levelof precision, filtering typically has to be aggressive,especially for highly structured tasks like parsing.Such aggressive filtering techniques incur massivedata loss and enforce trade-offs between the qualityand the amount of usable data.Ideally, a general filtering strategy for weakly su-pervised training of structured analysis tools shouldeliminate noisy subparts in the automatic annota-tion without discarding its high-precision aspects;thereby data loss would be kept to a minimum.In this paper, we propose an extremely simple ap-proach to noise reduction which greedily exploitspartial correspondences in a parallel corpus, i.e.,correspondences potentially covering only substruc-tures of translated sentences.
We implemented thismethod in an annotation projection framework tocreate training data for two dependency parsers rep-resenting different parsing paradigms: The MST-Parser (McDonald et al, 2005) as an instance ofgraph-based dependency parsing, and the Malt-Parser (Nivre et al, 2006) to represent transition-based dependency parsing.
In an empirical evalu-ation, we investigate how they react differently toincomplete and noisy training data.Despite its simplicity, the partial correspondenceapproach proves very effective and leads to parsersthat achieve unlabeled attachment scores that areonly 5% behind the average UAS for Dutch in theCoNLL-X Shared Task (Buchholz and Marsi, 2006).After a summary of related work in Sec.
2, wediscuss dependency tree projection (Sec.
3) and par-tial correspondence (Sec.
4).
In Sec.
5, we give anoverview of graph- and transition-based dependencyparsing and describe how each can be adapted fortraining on partial training data in Sec.
6.
Experi-mental results are presented in Sec.
7, followed byan analysis in Sec.
8.
Sec.
9 concludes.12a.
b. c.English (L1): I have two questions You are absolutely right You are absolutely rightDutch (L2): Ik heb twee vragen U heeft volkomen gelijk U heeft volkomen gelijk12 3Figure 1: Dependency tree projection from English to Dutch.
(a) Ideal scenario with bidirectional alignments.
(b)Projection fails due to weak alignments.
(c) Constrained fallback projection.2 Related WorkAnnotation projection has been applied to many dif-ferent NLP tasks.
On the word or phrase level, theseinclude morphological analysis, part-of-speech tag-ging and NP-bracketing (Yarowsky et al, 2001),temporal analysis (Spreyer and Frank, 2008), or se-mantic role labeling (Pado?
and Lapata, 2006).
Inthese tasks, word labels can technically be intro-duced in isolation, without reference to the rest ofthe annotation.
This means that an aggressive filtercan be used to discard unreliable data points (wordsin a sentence) without necessarily affecting high-precision data points in the same sentence.
By us-ing only the bidirectional word alignment links, onecan implement a very robust such filter, as the bidi-rectional links are generally reliable, even thoughthey have low recall for overall translational cor-respondences (Koehn et al, 2003).
The bidirec-tional alignment filter is common practice (Pado?
andLapata, 2006); a similar strategy is to discard en-tire sentences with low aggregated alignment scores(Yarowsky et al, 2001).On the sentence level, Hwa et al (2005) werethe first to project dependency trees from Englishto Spanish and Chinese.
They identify unreliabletarget parses (as a whole) on the basis of the num-ber of unaligned or over-aligned words.
In addition,they manipulate the trees to accommodate for non-isomorphic sentences.
Systematic non-parallelismsbetween source and target language are then ad-dressed by hand-crafted rules in a post-projectionstep.
These rules account for an enormous increasein the unlabeled f-score of the direct projections,from 33.9 to 65.7 for Spanish and from 26.3 to 52.4for Chinese.
But they need to be designed anew forevery target language, which is time-consuming andrequires knowledge of that language.Research in the field of unsupervised and weaklysupervised parsing ranges from various forms of EMtraining (Pereira and Schabes, 1992; Klein and Man-ning, 2004; Smith and Eisner, 2004; Smith and Eis-ner, 2005) over bootstrapping approaches like self-training (McClosky et al, 2006) to feature-basedenhancements of discriminative reranking models(Koo et al, 2008) and the application of semi-supervised SVMs (Wang et al, 2008).
The partialcorrespondence method we present in this paper iscompatible with such approaches and can be com-bined with other weakly supervised machine learn-ing schemes.
Our approach is similar to that ofClark and Curran (2006) who use partial trainingdata (CCG lexical categories) for domain adaptation;however, they assume an existing CCG resource forthe language in question to provide this data.3 Projection of Dependency TreesMost state-of-the-art parsers for natural languagesare data-driven and depend on the availability of suf-ficient amounts of labeled training data.
However,manual creation of treebanks is time-consuming andlabour-intensive.
One way to avoid the expensiveannotation process is to automatically label the train-ing data using annotation projection (Yarowsky etal., 2001): Given a suitable resource (such as aparser) in language L1, and a word-aligned paral-lel corpus with languages L1 and L2, label the L1-portion of the parallel text (with the parser) and copythe annotations to the corresponding (i.e., aligned)elements in language L2.
This is illustrated in Fig.1a.
The arrows between English and Dutch wordsindicate the word alignment.
Assuming we have aparser to produce the dependency tree for the En-glish sentence, we build the tree for the Dutch sen-tence by establishing arcs between words wD (e.g.,Ik) and hD (heb) if there are aligned pairs (wD, wE)13#sents w/ avg.
sent vocabprojected parse length (lemma)unfiltered (100,000) 24.92 19,066bidirectional 2,112 6.39 1,905fallback 6,426 9.72 4,801bi+frags?3 7,208 9.44 4,631Table 1: Data reduction effect of noise filters.
(Ik and I) and (hD, hE) (heb and have) such that hEis the head of wE in the English tree.Annotation projection assumes direct correspon-dence (Hwa et al, 2005) between languages (orannotations), which?although it is valid in manycases?does not hold in general: non-parallelismbetween corresponding expressions in L1 and L2causes errors in the target annotations.
The wordalignment constitutes a further source for errors if itis established automatically?which is typically thecase in large parallel corpora.We have implemented a language-independentframework for dependency projection and use theEuroparl corpus (Koehn, 2005) as the parallel text.Europarl consists of the proceedings of the Euro-pean Parliament, professionally translated in 11 lan-guages (approx.
30mln words per language).
Thedata was aligned on the word level with GIZA++(Och and Ney, 2003).1 In the experiments reportedhere, we use the language pair English-Dutch, withEnglish as the source for projection (L1) and Dutchas L2.
The English portion of the Europarl cor-pus was lemmatized and POS tagged with the Tree-Tagger (Schmid, 1994) and then parsed with Malt-Parser (which is described in Sec.
6), trained on adependency-converted version of the WSJ part fromthe Penn Treebank (Marcus et al, 1994), but withthe automatic POS tags.
The Dutch sentences wereonly POS tagged (with TreeTagger).23.1 Data Loss Through FilteringWe quantitatively assess the impact of various fil-tering techniques on a random sample of 100,000English-Dutch sentence pairs from Europarl (avg.1Following standard practice, we computed word align-ments in both directions (L1 ?
L2 and L2 ?
L1); this givesrise to two unidirectional alignments.
The bidirectional align-ment is the intersection of the two unidirectional ones.2The Dutch POS tags are used to train the monolingualparsers from the projected dependency trees (Sec.
7).24.9 words/sentence).
The English dependencytrees are projected to their Dutch counterparts as ex-plained above for Fig.
1a.The first filter we examine is the one that consid-ers exclusively bidirectional alignments.
It admitsdependency arcs to be projected only if the head hEand the dependent wE are each aligned bidirection-ally with some word in the Dutch sentence.
This isindicated in Fig.
1b, where the English verb are isaligned with the Dutch translation heeft only in onedirection.
This means that none of the dependenciesinvolving are are projected, and the projected struc-ture is not connected.
We will discuss in subsequentsections how less restricted projection methods canstill incorporate such data.Table 1 shows the quantitative effect of the bidi-rectional filter in the row labeled ?bidirectional?.
Theproportion of usable sentences is reduced to 2.11%.Consequently, the vocabulary size diminishes by afactor of 10, and the average sentence length dropsconsiderably from almost 25 to less than 7 words,suggesting that most non-trivial examples are lost.3.2 Constrained Fallback ProjectionAs an instance of a more relaxed projection of com-plete structures, we also implemented a fallback tounidirectional links which projects further depen-dencies after a partial structure has been built basedon the more reliable bidirectional links.
That is, thedependencies established via unidirectional align-ments are constrained by the existing subtrees, andare subject to the wellformedness conditions for de-pendency trees.3 Fig.
1c shows how the fallbackmechanism, initialized with the unconnected struc-ture built with the bidirectional filter, recovers aparse tree for the weakly aligned sentence pair inFig.
1b.
Starting with the leftmost word in the Dutchsentence and its English translation (U and You),there is a unidirectional alignment for the head ofYou: are is aligned to heeft, so U is established asa dependent of heeft via fallback.
Likewise, heeftcan now be identified as the root node.
Note that the(incorrect) alignment between heeft and You will notbe pursued because it would lead to heeft being a de-pendent of itself and thus violating the wellformed-3I.e., single headedness and acyclicity; we do not require thetrees to be projective, but instead train pseudo-projective models(Nivre and Nilsson, 2005) on the projected data (cf.
fn.
5).14#frags 1 2 3 4?15 >15#words<4 425 80 12 ?
?4?9 1,331 1,375 1,567 4,793 ?10?19 339 859 1,503 27,910 52220?30 17 45 143 20,756 10,087>30 0 5 5 4,813 23,362Table 2: Fragmented parses projected with the alignmentfilter.
The sentences included in the data set ?bi+frags?3?are in boldface.ness conditions.
Finally, the subtree rooted in gelijkis incorporated as the second dependent of heeft.As expected, the proportion of examples that passthis filter rises, to 6.42% (Table 1, ?fallback?).
How-ever, we will see in Sec.
7 that parsers trained onthis data do not improve over parsers trained on thebidirectionally aligned sentences alone.
This is pre-sumably due to the noise that inevitably enters thetraining data through fallback.4 Partial Correspondence ProjectionSo far, we have only considered complete trees,i.e., projected structures with exactly one root node.This is a rather strict requirement, given that evenstate-of-the-art parsers sometimes fail to produceplausible complete analyses for long sentences, andthat non-sentential phrases such as complex nounphrases still contain valuable, non-trivial informa-tion.
We therefore propose partial correspondenceprojection which, in addition to the complete anno-tations produced by tree-oriented projection, yieldspartial structures: It admits fragmented analyses incase the tree-oriented projection cannot construct acomplete tree.
Of course, the nature of those frag-ments needs to be restricted so as to exclude datawith no (interesting) dependencies.
E.g., a sentenceof five words with a parse consisting of five frag-ments provides virtually no information about de-pendency structure.
Hence, we impose a limit (fixedat 3 after quick preliminary tests on automaticallylabeled development data) on the number of frag-ments that can make up an analysis.
Alternatively,one could require a minimum fragment size.As an example, consider again Fig.
1b.
This ex-ample would be discarded in strict tree projection,but under partial correspondence it is included as apartial analysis consisting of three fragments:U heeft volkomen gelijkAlthough the amount of information provided inthis analysis is limited, the arc between gelijk andvolkomen, which is strongly supported by the align-ment, can be established without including poten-tially noisy data points that are only weakly aligned.We use partial correspondence in combinationwith bidirectional projection.4 As can be seen inTable 1 (?bi+frags?3?
), this combination boosts theamount of usable data to a range similar to that ofthe fallback technique for trees; but unlike the latter,partial correspondence continues to impose a high-precision filter (bidirectionality) while improving re-call through relaxed structural requirements (partialcorrespondence).
Table 2 shows how fragment sizevaries with sentence length.5 Data-driven Dependency ParsingModels for data-driven dependency parsing can beroughly divided into two paradigms: Graph-basedand transition-based models (McDonald and Nivre,2007).5.1 Graph-based ModelsIn the graph-based approach, global optimizationconsiders all possible arcs to find the tree T?
s.t.T?
= argmaxT?Ds(T ) = argmaxT?D?
(i,j,l)?ATs(i, j, l)where D is the set of all well-formed dependencytrees for the sentence, AT is the set of arcs in T , ands(i, j, l) is the score of an arc between words wi andwj with label l. The specific graph-based parser weuse in this paper is the MSTParser of McDonald etal.
(2005).
The MSTParser learns the scoring func-tion s using an online learning algorithm (Crammerand Singer, 2003) which maximizes the margin be-tween T?
and D \ {T?
}, based on a loss function thatcounts the number of words with incorrect parentsrelative to the correct tree.5.2 Transition-based ModelsIn contrast to the global optimization employed ingraph-based models, transition-based models con-struct a parse tree in a stepwise way: At each point,4Fragments from fallback projection turned out not to behelpful as training data for dependency parsers.15the locally optimal parser action (transition) t?
is de-termined greedily on the basis of the current config-uration c (previous actions plus local features):t?
= argmaxt?Ts(c, t)where T is the set of possible transitions.
As a rep-resentative of the transition-based paradigm, we usethe MaltParser (Nivre et al, 2006).
It implements in-cremental, deterministic parsing algorithms and em-ploys SVMs to learn the transition scores s.6 Parsing with Fragmented TreesTo make effective use of the fragmented trees pro-duced by partial correspondence projection, bothparsing approaches need to be adapted for trainingon sentences with unconnected substructures.
Herewe briefly discuss how we represent these structures,and then describe how we modified the parsers.We use the CoNLL-X data format for dependencytrees (Buchholz and Marsi, 2006) to encode partialstructures.
Specifically, every fragment root spec-ifies as its head an artificial root token w0 (distin-guished from a true root dependency by a specialrelation FRAG).
Thus, sentences with a fragmentedparse are still represented as a single sentence, in-cluding all words; the difference from a fully parsedsentence is that unconnected substructures are at-tached directly under w0.
For instance, the partialparse in Fig.
1b would be represented as follows (de-tails omitted):(1) 1 U pron 0 FRAG2 heeft verb 0 ROOT3 volkomen adj 4 mod4 gelijk noun 0 FRAG6.1 Graph-based Model: fMSTIn the training phase, the MSTParser tries to max-imize the scoring margin between the correct parseand all other valid dependency trees for the sentence.However, in the case of fragmented trees, the train-ing example is not strictly speaking correct, in thesense that it does not coincide with the desired parsetree.
In fact, this desired tree is among the otherpossible trees that MST assumes to be incorrect, orat least suboptimal.
In order to relax this assump-tion, we have to ensure that the loss of the desiredtree is zero.
While it is impossible to single out thisone tree (since we do not know which one it is), wecan steer the margin in the right direction with a lossfunction that assigns zero loss to all trees that areconsistent with the training example, i.e., trees thatdiffer from the training example at most on thosewords that are fragment roots (e.g., gelijk in Fig.
1).To reflect this notion of loss during optimization, wealso adjust the definition of the score of a tree:s(T ) = ?
(i,j,l)?AT : l 6=FRAGs(i, j, l)We refer to this modified model as f(iltering)MST.6.2 Transition-based Model: fMaltIn the transition-based paradigm, it is particularlyimportant to preserve the original context (includ-ing unattached words) of a partial analysis, becausethe parser partly bases its decisions on neighboringwords in the sentence.Emphasis of the role of isolated FRAG dependentsas context rather than proper nodes in the tree canbe achieved, as with the MSTParser, by eliminat-ing their effect on the margin learned by the SVMs.Since MaltParser scores local decisions, this simplyamounts to suppressing the creation of SVM train-ing instances for such nodes (U and gelijk in (1)).That is, where the feature model refers to contextinformation, unattached words provide this infor-mation (e.g., the feature vector for volkomen in (1)contains the form and POS of gelijk), but there areno instances indicating how they should be attachedthemselves.
This technique of excluding fragmentroots during training will be referred to as fMalt.7 Experiments7.1 SetupWe train instances of the graph- and the transition-based parser on projected dependencies, and occa-sionally refer to these as ?projected parsers?.5All results were obtained on the held-outCoNLL-X test set of 386 sentences (avg.
12.95The MaltParsers use the projective Nivre arc-standard pars-ing algorithm.
For SVM training, data are split on the coarsePOS tag, with a threshold of 5,000 instances.
MSTParser in-stances use the projective Eisner parsing algorithm, and first-order features.
The input for both systems is projectivized usingthe head+path schema (Nivre and Nilsson, 2005).16Malt MSTAlpino 80.05 82.43EP 75.33 73.09Alpino + EP 77.47 81.63baseline 1 (previous) 23.65baseline 2 (next) 27.63Table 3: Upper and lower bounds (UAS).words/sentence) from the Alpino treebank (van derBeek et al, 2002).
The Alpino treebank consistsmostly of newspaper text, which means that we areevaluating the projected parsers, which are trainedon Europarl, in an out-of-domain setting, in the ab-sence of manually annotated Europarl test data.Parsing performance is measured in terms of un-labeled attachment score (UAS), i.e., the proportionof tokens that are assigned the correct head, irrespec-tive of the label.6To establish upper and lower bounds for our taskof weakly supervised dependency parsing, we pro-ceed as follows.
We train MaltParsers and MST-Parsers on (i) the CoNLL-X training portion of theAlpino treebank (195,000 words), (ii) 100,000 Eu-roparl sentences parsed with the parser obtainedfrom (i), and (iii) the concatenation of the datasets (i) and (ii).
The first is a supervised upperbound (80.05/82.43% UAS)7 trained on manuallylabeled in-domain data, while the second constitutesa weaker bound (75.33/73.09%) subject to the sameout-of-domain evaluation as the projected parsers,and the third (77.47%) is a self-trained version of (i).We note in passing that the supervised model doesnot benefit from self-training.
Two simple baselinesprovide approximations to a lower bound: Baseline1 attaches every word to the preceding word, achiev-ing 23.65%.
Analogously, baseline 2 attaches everyword to the following word (27.63%).
These sys-tems are summarized in Table 3.6The labeled accuracy of our parsers lags behind the UAS,because the Dutch dependency relations in the projected anno-tations arise from a coarse heuristic mapping from the originalEnglish labels.
We therefore report only UAS.7The upper bound models are trained with the same param-eter settings as the projected parsers (see fn.
5), which were ad-justed for noisy training data.
Thus improvements are likelywith other settings: Nivre et al (2006) report 81.35% for aDutch MaltParser with optimized parameter settings.
McDon-ald et al (2006) report 83.57% with MST.words Malt MSTa.
trees (bidirectional) 13,500 65.94 67.76trees (fallback) 62,500 59.28 65.08bi+frags?3 68,000 55.09 57.14bi+frags?3 (fMalt/fMST) 68,000 69.15 70.02b.
trees (bidirectional) 100,000 61.86 69.91trees (fallback) 100,000 60.05 64.84bi+frags?3 100,000 54.50 55.87bi+frags?3 (fMalt/fMST) 100,000 68.65 69.86c.
trees (bidirectional) 102,300 63.32 69.85trees (fallback) 465,500 53.45 64.88bi+frags?3 523,000 51.48 57.20bi+frags?3 (fMalt/fMST) 523,000 69.52 70.33Table 4: UAS of parsers trained on projected dependencystructures for (a) a sample of 100,000 sentences, subjectto filtering, (b) 10 random samples, each with 100,000words after filtering (average scores given), and (c) theentire Europarl corpus, subject to filtering.7.2 ResultsTable 4a summarizes the results of training parserson the 100,000-sentence sample analyzed above.Both the graph-based (MST) and the transition-based (Malt) parsers react similarly to the more orless aggressive filtering methods, but to different de-grees.
The first two rows of the table show theparsers trained on complete trees (?trees (bidirec-tional)?
and ?trees (fallback)?).
In spite of the ad-ditional training data gained by the fallback method,the resulting parsers do not achieve higher accuracy;on the contrary, there is a drop in UAS, especiallyin the transition-based model (?6.66%).
The in-creased level of noise in the fallback data has less(but significant)8 impact on the graph-based coun-terpart (?2.68%).Turning to the parsers trained on partial cor-respondence data (?bi+frags?3?
), we observe evengreater deterioration in both parsing paradigms if thedata is used as is.
However, in combination with thefMalt/fMST systems (?bi+frags?3 (fMalt/fMST)?
),both parsers significantly outperform the tree-8Significance testing (p<.01) was performed by means ofthe t-test on the results of 10 training cycles (Table 4c ?trees(fb.)?
only 2 cycles due to time constraints).
For the experimentsin Table 4a and 4c, the cycles differed in terms of the order inwhich sentences where passed to the parser.
In Table 4b we basesignificance on 10 true random samples for training.17Recall Precisiondep.
length 1 2 3?6 ?7 root 1 2 3?6 ?7 roota.
trees (bi.)
83.41 66.44 52.94 40.64 52.45 82.46 66.06 61.38 34.95 50.97trees (fb.)
82.20 64.21 54.59 37.95 55.72 82.64 61.41 54.39 31.96 68.55bi+frags?3 70.18 59.50 46.61 32.14 61.87 83.75 67.22 58.25 32.81 27.01bi+frags?3 (fMalt) 89.23 75.34 59.18 41.65 59.06 83.46 69.05 65.85 48.21 75.79Alpino-Malt 92.81 84.94 75.11 65.44 66.15 89.71 81.08 77.56 62.57 84.58b.
trees (bi.)
87.53 73.79 59.57 46.79 71.01 86.43 74.08 64.78 45.17 66.79trees (fb.)
82.53 69.37 55.77 37.46 70.24 85.31 69.29 59.85 40.14 53.99bi+frags?3 68.11 57.48 34.30 13.00 90.68 90.28 78.54 66.36 43.70 23.41bi+frags?3 (fMST) 87.73 72.84 62.55 50.15 67.78 86.94 71.60 66.05 48.48 68.20Alpino-MST 94.13 86.60 76.91 65.14 71.60 91.76 82.49 76.23 71.96 85.38Table 5: Performance relative to dependency length.
(a) Projected MaltParsers and (b) projected MSTParsers.oriented models (?trees (bidirectional)?)
by 3.21%(Malt) and 2.26% (MST).It would be natural to presume that the superior-ity of the partial correspondence filter is merely dueto the amount of training data, which is larger bya factor of 5.04.
We address this issue by isolat-ing the effect on the quality of the data, and hencethe success at noise reduction: In Table 4b, we con-trol for the amount of data that is effectively usedin training, so that each filtered training set consistsof 100,000 words.
Considering the Malt models, wefind that the trends suggested in Table 4a are con-firmed: The pattern of relative performance emergeseven though any quantitative (dis-)advantages havebeen eliminated.9 10 Interestingly, the MSTParserdoes not appear to gain from the increased variety(cf.
Table 1) in the partial data: it does not differsignificantly from the ?trees (bi.)?
model.Finally, Table 4c provides the results of trainingon the entire Europarl, or what remains of the corpusafter the respective filters have applied.
The resultscorroborate those obtained for the smaller samples.In summary, the results support our initial hy-pothesis that partial correspondence for sentencescontaining a highly reliable part is preferable to9The degree of skewedness in the filtered data is not con-trolled, as it is an important characteristic of the filters.10Some of the parsers trained on the larger data sets (Table4b+c) achieve worse results than their smaller counterparts inTable 4a.
We conjecture that it is due to the thresholded POS-based data split, performed prior to SVM training: Larger train-ing sets induce decision models with more specialized SVMs,which are more susceptible to tagging errors.
This could beavoided by increasing the threshold for splitting.relaxing the reliability citerion, and?in the caseof the transition-based MaltParser?also to aggres-sively filtering out all but the reliable complete trees.With UASs around 70%, both systems are only 5%behind the average 75.07% UAS achieved for Dutchin the CoNLL-X Shared Task.8 AnalysisWe have seen that the graph- and the transition-based parser react similarly to the various filteringmethods.
However, there are interesting differencesin the magnitude of the performance changes.
Ifwe compare the two tree-oriented filters ?trees (bi.
)?and ?trees (fb.
)?, we observe that, although both Maltand MST suffer from the additional noise that is in-troduced via the unidirectional alignments, the dropin accuracy is much less pronounced in the latter,graph-based model.
Recall that in this paradigm,optimization is performed over the entire tree byscoring edges independenly; this might explain whynoisy arcs in the training data have only a negligi-ble impact.
Conversely, the transition-based Malt-Parser, which constructs parse trees in steps of lo-cally optimal decisions, has an advantage when con-fronted with partial structures: The individual frag-ments provide exactly the local context, plus lexicalinformation about the (unconnected) wider context.To give a more detailed picture of the differencesbetween predicted and actual annotations, we showthe performance (of the parsers from Table 4b) sep-arately for binned arc length (Table 5) and sen-tence length (Table 6).
As expected, the perfor-mance of both the supervised upper bounds (Alpino-18sent.
length <4 4?9 10?19 20?30 > 30a.
trees (bi.)
73.87 62.13 65.67 60.81 55.18trees (fb.)
69.91 57.84 62.29 60.04 55.47bi+frags?3 74.14 54.40 56.62 54.07 48.95bi+fr?3 (fMalt) 73.51 65.69 71.70 68.49 63.71Alpino-Malt 81.98 69.81 81.11 82.82 76.02b.
trees (bi.)
76.67 70.16 73.09 69.56 63.57trees (fb.)
73.24 64.93 67.79 64.98 57.70bi+frags?3 77.48 59.65 55.96 55.27 52.74bi+fr?3 (fMST) 73.24 67.84 73.46 70.04 62.92Alpino-MST 81.98 72.24 85.10 83.86 78.51Table 6: UAS relative to sentence length.
(a) ProjectedMaltParsers and (b) projected MSTParsers.Malt/MST) and the projected parsers degrades as de-pendencies get longer, and the difference betweenthe two grows.
Performance across sentence lengthremains relatively stable.
But note that both tablesagain reflect the pattern we saw in Table 4.
Impor-tantly, the relative ranking (in terms of f-score, notshown, resp.
UAS) is still in place even in long dis-tance dependencies and long sentences.
This indi-cates that the effects we have described are not arti-facts of a bias towards short dependencies.In addition, Table 5 sheds some light on the im-pact of fMalt/fMST in terms of the trade-off betweenprecision and recall.
Without the specific adjust-ments to handle fragments, partial structures in thetraining data lead to an immense drop in recall.
Bycontrast, when the adapted parsers fMalt/fMST areapplied, they boosts recall back to a level compara-ble to or even above that of the tree-oriented pro-jection parsers, while maintaining precision.
Again,this effect can be observed across all arc lengths, ex-cept arcs to root, which naturally the ?bi+frags?
mod-els are overly eager to predict.Finally, the learning curves in Fig.
2 illus-trate how much labeled data would be required toachieve comparable performance in a supervisedsetting.
The graph-based upper bound (Alpino-MST) reaches the performance of fMST (trainedon the entire Europarl) with approx.
25,000 wordsof manually labeled treebank data; Alpino-Maltachieves the performance of fMalt with approx.35,000 words.
The manual annotation of even thesemoderate amounts of data involves considerable ef-forts, including the creation of annotation guidelinesFigure 2: Learning curves for the supervised upperbounds.
They reach the performance of the projectedparsers with ?25,000 (MST) resp.
35,000 (Malt) words.and tools, the training of annotators etc.9 ConclusionIn the context of dependency parsing, we have pro-posed partial correspondence projection as a greedymethod for noise reduction, and illustrated how itcan be integrated with data-driven parsing.
Our ex-perimental results show that partial tree structuresare well suited to train transition-based dependencyparsers.
Graph-based models do not benefit as muchfrom additional partial structures, but instead aremore robust to noisy training data, even when thetraining set is very small.In future work, we will explore how well the tech-niques presented here for English and Dutch workfor languages that are typologically further apart,e.g., English-Greek or English-Finnish.
Moreover,we are going to investigate how our approach, whichessentially ignores unknown parts of the annotation,compares to approaches that marginalize over hid-den variables.
We will also explore ways of combin-ing graph-based and transition-based parsers alongthe lines of Nivre and McDonald (2008).AcknowledgmentsThe research reported in this paper has been sup-ported by the German Research Foundation DFG aspart of SFB 632 ?Information structure?
(project D4;PI: Kuhn).19ReferencesSabine Buchholz and Erwin Marsi.
2006.
Conll-x sharedtask on multilingual dependency parsing.
In Proceed-ings of CoNLL-X, pages 149?164, New York City,June.Stephen Clark and James R. Curran.
2006.
Partial train-ing for a lexicalized-grammar parser.
In Proceed-ings of HLT-NAACL 2006, pages 144?151, New York,June.Koby Crammer and Yoram Singer.
2003.
Ultraconserva-tive online algorithms for multiclass problems.
Jour-nal of Machine Learning Reseach, 3:951?991, Jan-uary.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Natural Language Engineering, 11(3):311?325.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of ACL2004, pages 478?485, Barcelona, Spain.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT-NAACL 2003, pages 127?133.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings of theMT Summit 2005.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Pro-ceedings of ACL-HLT 2008), pages 595?603, Colum-bus, Ohio, June.Mitchell Marcus, Grace Kim, Mary Marcinkiewicz,Robert MacIntyre, Ann Bies, Mark Ferguson, KarenKatz, and Britta Schasberger.
1994.
The Penn tree-bank: Annotating predicate argument structure.
InARPA Human Language Technology Workshop.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of HLT-NAACL 2006, pages 152?159, New York,June.Ryan McDonald and Joakim Nivre.
2007.
Characteriz-ing the errors of data-driven dependency parsing mod-els.
In Proceedings of EMNLP-CoNLL 2007, pages122?131.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof HLT-EMNLP 2005).Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with a two-stage discriminative parser.
In Proceedings of CoNLL-X.Joakim Nivre and Ryan McDonald.
2008.
Integratinggraph-based and transition-based dependency parsers.In Proceedings of ACL-HLT 2008, pages 950?958,Columbus, Ohio, June.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projectivedependency parsing.
In Proceedings of ACL 2005,pages 99?106.Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en Eryig?it,and Svetoslav Marinov.
2006.
Labeled pseudo-projective dependency parsing with support vector ma-chines.
In Proceedings of CoNLL-X, pages 221?225.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Sebastian Pado?
and Mirella Lapata.
2006.
Optimal con-stituent alignment with edge covers for semantic pro-jection.
In Proceedings of COLING/ACL 2006, Syd-ney, Australia.Fernando Pereira and Yves Schabes.
1992.
Inside-outside reestimation from partially bracketed corpora.In Proceedings of ACL 1992, pages 128?135.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In International Conferenceon New Methods in Language Processing, pages 44?49, Manchester, England.Noah A. Smith and Jason Eisner.
2004.
Annealingtechniques for unsupervised statistical language learn-ing.
In Proceedings of ACL 2004, pages 487?494,Barcelona, July.Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: Training log-linear models on unlabeled data.In Proceedings of ACL 2005, pages 354?362, Ann Ar-bor, MI, June.Kathrin Spreyer and Anette Frank.
2008.
Projection-based acquisition of a temporal labeller.
In Proceed-ings of IJCNLP 2008, Hyderabad, India, January.Leonoor van der Beek, Gosse Bouma, Robert Malouf,and Gertjan van Noord.
2002.
The Alpino depen-dency treebank.
In Computational Linguistics in theNetherlands (CLIN).Qin Iris Wang, Dale Schuurmans, and Dekang Lin.
2008.Semi-supervised convex training for dependency pars-ing.
In Proceedings of ACL-HLT 2008, pages 532?540, Columbus, Ohio, June.David Yarowsky, Grace Ngai, and Richard Wicentowski.2001.
Inducing multilingual text analysis tools via ro-bust projection across aligned corpora.
In Proceedingsof HLT 2001.20
