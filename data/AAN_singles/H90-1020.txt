Evaluation of Spoken Language Systems:the ATIS DomainP.
J. PriceSRI International333 Ravenswood Ave.Menlo Park, CA 94025AbstractProgress can be measured and encouraged via standardsfor comparison and evaluation.
Though qualitative as-sessments can be useful in initial stages, quantifiablemeasures of systems under the same conditions are es-sential for comparing results and assessing claims.
Thispaper will address the emerging standards for evaluationof spoken language systems.Introduct ion and BackgroundNumbers are meaningless unless it is clear where theycome from.
The evaluation of any technology is greatlyenhanced in usefulness if accompanied by documentedstandards for assessment.
There has been a growing ap-preciation in the speech recognition community of theimportance of standards for reporting performance.
Theavailability of standard atabases and protocols for eval-uation has been an important component in progress inthe field and in the sharing of new ideas.
Progress towardevaluating spoken language systems, like the technologyitself, is beginning to emerge.
This paper presents omebackground on the problem and outlines the issues andinitial experiments in evaluating spoken language sys-tems in the "common" task domain, known as ATIS (AirTravel Information Service).The speech recognition community has reached agree-ment on some standards for evaluating speech recogni-tion systems, and is beginning to evolve a mechanism forrevising these standards as the needs of the communitychange (e.g., as new systems require new kinds of data, asnew system capabilities emerge, or as refinements in ex-isting methods develop).
A protocol for testing speaker-dependent and speaker-independent speech recognitionsystems on read speech with a 1000-word vocabulary,(e.g., \[6\]), coordinated through the National Institute ofStandards and Technology (NIST), has been operatingfor several years.
This mechanism has inspired a healthyenvironment of competitive cooperation, and has led todocumented major performance improvements and hasincreased the sharing of methodologies and of data.Evaluation of natural language (NL) understandingis more difficult than recognition because (1) the phe-nomena of interest occur less frequently (a given corpuscontains more phones and words than syntactic or se-mantic phenomena), (2) semantics is far more domaindependent than phonetics or phonology, hence changingdomains is more labor intensive, and (3) there is lessagreement on what constitutes the "correct" analysis.However, MUCK, Message Understanding Conference,is planning the third in a series of message understand-ing evaluations for later this year (August 1990).
Theobjective is to carry out evaluations of text interpreta-tion systems.
The previous evaluation, carried out inMarch-June 1989, yielded quantitative measures of per-formance for eight natural language processing systems\[4, 5\].
The systems are evaluated on performance ona template-filling task and scored on measures of com-pleteness and precision \[7\].So far, we have discussed the evaluation of automaticspeech recognition (i.e., the algorithmic translation fromhuman speech to machine readable text), and of someaspects of natural anguage understanding (i.e., the au-tomatic computation of a meaning and the generation,if needed, of an appropriate response).
The evalua-tion of Spoken language systems represents a big stepbeyond the  previous evaluation mechanisms described.The input is spontaneous, rather than read, speech.
Thespeech is recorded in an office environment, rather thanin a sound-isolated booth.
The subjects are involvedin problem-solving scenarios.
The systems to be testedwill be evaluated on the answers returned from a com-mon database.
The rest of this paper focuses on thesteps taken by the DARPA speech and natural anguagecommunity to develop a common evaluation databaseand scoring software and protocols.
The first use of thismechanism took place June 1990.
However, given thegreatly increased challenge, the first use of the mecha-nism is more a test of the mechanism than of the systemsevaluated.It has become clear in carrying out the evaluationmechanism that the needs of common evaluation aresometimes at odds with the needs of well-designed sys-tems.
In particular, the common evaluation ignores di-alogue beyond a single query-response pair, and all in-teractive aspects of systems.
A proposal for dialogueevaluation is included in \[3\], this volume.Though the initial evaluation mechanism, describedbelow, represents a major effort, and an enormous ad-91vance over past evaluations, we still fall short of a com-pletely adequate valuation mechanism for spoken lan-guage systems.
Some forms of evaluation may have to bepostponed to the system level and measured in terms oftime to complete a task, or units sold.
We need to con-tinue to elaborate methods of evaluation that are mean-ingful.
Numbers alone are insufficient.
We need to findways of gaining insight into differences that distinguishvarious systems or system configurations.I ssuesIn this section we will outline the major evaluation is-sues that have taken up a good deal of our time andenergy over the past several months, including: the sep-aration of training and testing materials, black box vs.glass box evaluations, quantitative vs. qualitative val-uation, the selection of a domain, the collection of thedata, transcribing and processing the data, documentingand classifying the data, obtaining canonical answers,and scoring of answers.Independent  T ra in ing  and  Test  SetsThe importance of independent training/developmentdata and testing data has been acknowledged in speechrecognition evaluation for some time.
The idea is lessprominent in natural language understanding.
The fo-cus in linguistics on competence rather than performancehas meant hat many developers of syntactic and seman-tic models have not traditionally evaluated their systemson a corpus of observed ata.
Those who have lookedat data, have typically referred to a few token exam-ples and have not evaluated systematically on an entirecorpus.
Still more rare is evaluation on an independentcorpus, a corpus not used to derive or modify the theoryor model.
There is no doubt that a system can eventu-ally be made to handle any finite number of evaluationsentences.
Having a test suite of phenomena is essentialfor evaluating and comparing competing theories.
Moreimportant for an application, however, is a test on an in-dependent set of sentences that represent phenomena thesystem is likely to encounter.
This ensures that develop-ers have handled the phenomena observed in the trainingset in a manner that will generalize, and it properly (forsystems rather than theories) focuses the evaluation ofvarious phenomena in proportion to their likelihood ofoccurrence.
That is, though from a theoretical perspec-tive it may be important o cover certain phenomena, inan application, the coverage of those phenomena mustbe weighed against he costs (how much larger or sloweris the resulting system) and benefits (how frequently dothe phenomena occur).B lack  Box  versus  G lass  Box  Eva luat ionEvaluating components of a system is important in sys-tem development, though not necessarily useful for com-paring various systems, unless the systems evaluated arevery similar, which is not often the case.
Since the moti-vation for evaluating components of a system is for inter-nal testing, there is less need to reach wide-spread agree-ment in the community on the measurement methodol-ogy.
System-internal measures can be used to evalu-ate component technologies as a function of their designparameters; for example, recognition accuracy can betested as a function of syntactic and phonological per-plexity, and parser performance can be measured as afunction of the accuracy of the word input.
In addi-tion, these measures are useful in assessing the amountof progress being made, and how changes in various com-ponents affect each other.A useful means of evaluating system performance isthe time to complete a task successfully.
This measurecannot be used to compare systems unless they are aimedat completing the same task.
It is, however, useful inassessing the system in comparison to problem solvingwithout the spoken language system in question.
Forexample, if the alternative to a database query spokenlanguage system is the analysis of huge stacks of paper-work, the simple measure of time-to-complete-task canbe important in showing the efficiency gains of such asystem.Time-to-complete-task, however, is a difficult measureto use in evaluating a decision-support system because(1) individual differences in cognitive skill in the po-tential user population will be large in relation to thesystem-related differences under test, and (2) the puzzle-solving nature of the task may complicate proceduresthat reuse subjects as their own controls.
Therefore,care should be taken in the design of such measures.For example, it is clear that when variability across sub-jects is large, it is important o evaluate on a large poolof users, or to use a within-subject design.
The lat-ter is possible if equivalent forms of certain tasks canbe developed.
In this case, each subject could performone form of the task using the spoken language systemand another form using an alternative (such as examin-ing stacks of papers, or using typed rather than spokeninput, or using a database query language rather thannatural anguage).Quant i ta t ive  versus  Qua l i ta t iveEva luat ionQualitative evaluation (for example, do users seem tolike the system) can be encouraging, rewarding and caneven sell systems.
But more convincing to those whocannot observe the system themselves are quantitativeautomated measures.
Automation of the measures isimportant because we want to avoid any possibility ofnudging the data wittingly or unwittingly, and of er-rors arising from fatigue and inattention.
Further, ifthe process is automated, we can observe far more datathan otherwise possible, which is important in language,where the units occur infrequently and where the vari-ation across subjects is large.
For these measures to bemeaningful, they should be standardized insofar as pos-92sible, and they should be reproducible.
These are thegoals of the DARPA-NIST protocols for evaluation ofspoken language systems.
These constraints form a realchallenge to the community in defining meaningful per-formance measures.L imi t ing  the  DomainSpoken language systems for the near future will nothandle all of English, but, rather, will be limited to adomain-specific sub-language.
Accurate modeling of thesub-language will depend on analysis of domain-specificdata.
Since no spoken language systems currently havea wide range of users, and since variability across usersis expected to be large, we are simulating applicationsin which a large population of potential users can besampled.The domain used for the standard evaluation is ATISusing the on-line Official Airline Guide (OAG), whichwe have put into a relational format.
This applicationhas many advantages for an initial system, including thefollowing:?
It takes advantage of an existing public domain realdatabase, the Official Airline Guide, used by hun-dreds of thousands of people.?
It is a rich and interesting domain, including data onschedules and fares, hotels and car rentals, groundtransportation, local information, airport statistics,trip and travel packages, and on-time rates.?
A wide pool of users are familiar with the domainand can understand and appreciate problem solv-ing in the domain (this is crucial both for initialdata collection for development and for demonstrat-ing the advantages of a new technology to potentialfuture users in a wide variety of domains).?
The domain can be easily scaled with the technol-ogy, which is important for rapid prototyping andfor taking advantage of advances in capabilities.?
The domain includes a good deal that can be portedto other domains, such as generic database queryand interactive problem solving.Related to the issue of limiting the domain is the is-sue of limiting the vocabulary.
In the past, for speechrecognition, we have used a fixed vocabulary.
For spon-taneous peech, however, as opposed to read speech, howdoes one specify the vocabulary?
Initially, we have notfixed the vocabulary, and merely observed the lexicalitems that occur.
However, it is an impossible task tofully account for every possible word that might occur,and it is a very large task to derive methods to detectnew words.
It is also a very large task to properly han-dle these new words, and one that probably will involveinteractive systems that do not meet the requirementsof our current common evaluation methods.
However,there is evidence that people can accomplish tasks usinga quite restricted vocabulary.
Therefore, it may be possi-ble to provide some training of subjects, and some toolsin the data collection methods so that a fixed vocab-ulary can be specified and feedback can automaticallybe given to subjects when extra-lexical material occurs.This would meet the needs of spontaneous speech, ofcommon evaluation and of a fixed vocabulary (where onecould choose to include or exclude the occurring extra-lexical items in the evaluation).Co l lec t ing  Data  fo r  Eva luat ionIn order to collect he data we need for evaluating spokenlanguage systems, we have developed a pnambic system(named after the line in the Wizard of Of: "pay no at-tention to the man behind the curtain").
In this systema subject is led to believe that the interaction is takingplace with a computer, when in fact the queries are han-dled by a transcriber wizard (who transcribes the speechand sends it to the subject's creen) and a database wiz-ard who is supplied with a tool for rapid access to theonline database in order to respond to the queries.
Thewizard is not allowed to perform complex tasks.
Thewizard may only retrieve data from the database or sendone of a small number of other responses, uch as "yourquery requires reasoning beyond the capabilities of thesystem."
In general, the guidelines for the wizard areto handle requests that the wizard understands and thedatabase can answer.
The data must be analyzed after-wards to assess whether the wizard did the right thing.The subjects in the data collection are asked to solveone of several air travel planning scenarios.
The goalof the scenarios is to inspire the subjects with realisticproblems and to help them focus on problem solving.
Asample scenario is:Plan a business trip to 4 different cities (ofyour choice), using public ground transporta-tion to and from the airports.
Save time andmoney where you can.
The client is an airplanebuff and enjoys flying on different kinds of air-craft.Further details on the data collection mechanism isprovided in \[2\] in this volume.T ranscr ip t ion  Convent ionsThe session transcriptions, i.e., the sentences displayedto the subject, represent the subject's peech in a nat-ural English text style.
Errors or dysfluencies (such asfalse starts) that the subject corrects will not appear inthe transcription.
Grammatical errors that the subjectdoes not correct (such as number disagreement) will ap-pear in the transcription as spoken by the subject.
Thetranscription wizard will follow general English princi-ples, such as those described in The Chicago Manual ofStyle (13th Edition, 1982).
The tremendous interactivepressure on the transcription wizard will inevitably lead9 \ ]to transcription errors, so these conventions erve as aguide.This initial transcription will then be verified andcleaned up as required.
The result can be used as conven-tional input to text-based natural anguage understand-ing systems.
It will represent what the subject "meantto say", in that it will not include dysfluencies correctedby the subject.
However, it may contain ungrammaticalinput.In order to evaluate the differences between previ-ously collected read-speech corpera nd the spontaneous-speech corpus, subjects will read the transcriptions oftheir sessions.
The text used to prompt this reading willbe derived from the natural anguage transcription whilelistening to the spoken input.
It will obey standard tex-tual transcriptions to look natural to the user, exceptwhere this might affect the utterance.
For example, forthe fare restriction code "VU/ i "  the prompt may appearas "V U slash one" or as "V U one", depending on whatthe subject said.Finally, the above transcription eeds to be furthermodified to take into account various speech phenom-ena, according to conventions for their representation.For example, obviously mispronounced words that arenevertheless intelligible will be marked with asterisks,words verbally deleted by the subject will be enclosed inangle brackets, words interrupted will end in a hyphen,some non-speech acoustic events will be noted in squarebrackets, pauses will be be marked with a period approx-imately corresponding to each elapsed second, commaswill be used for less salient boundaries, an exclamationmark before a word or syllable indicates emphatic stress,and unusual vowel lengthening will be indicated by acolon immediately after the lengthened sound.
Some ofthe indications will be useful for speech recognition sys-tems, but not all of them will be included in the referencestrings for evaluating the speech recognition output.The various transcriptions are illustrated in the ex-amples below, with the agreed upon file extensions inparentheses, where applicable:* SESSION TRANSCRIPT ION:Show me a generic description of a 757.
* NL TEXT INPUT (.nli):Show me a general description of a 757.?
PROMPTING TEXT (.ptx):Show me a general description of a seven fifty seven.?
SPEECH DETAIL (.sro):<l ist> show me: a general description, of a sevenfifty seven?
SPEECH REFERENCE (.snr):SHOW ME A GENERAL DESCRIPTION OF ASEVEN F IFTY  SEVENData ClassificationOnce collected and processed, the data will have to beclassified.
Ambiguous queries will be excluded from theevaluation set only if it is impossible for a person to tellwithout context what the preferred reading is.
Anotherissue is minor syntactic or semantic ill-formedness.
Ourguideline here is that if the query is interpretable, it willbe accepted, unless it is so ill-formed that it is clear thatit is not intended to be normal conversational English.All presuppositions about the number of answers (eitherexistence or uniqueness) will be ignored, and these arethe only types of presupposition failures noted to date.Any other types of presupposition failure that make thequery truly unanswerable will no doubt also have madeit impossible for t:he wizard to generate a database query,and will be ruled out on those grounds.
Queries that areformed of more than one sentence will not automaticallybe ruled out.
The examples observed so far are clearlyinterpretable as expressing multiple constraints that canbe combined into a single query.Evaluatable queries will be identified by exception,i.e., those that are none of the following:1. context dependent,.
vague, ambiguous, disambiguated only by context,or otherwise failing to yield a single canonicaldatabase answer,3.
grossly ill-formed,4.
other unanswerable queries (i.e., those not given adatabase by the wizard),5. queries from a noncooperative subject.Canon ica l  Answers  and  Scor ingCanonical answers will, in general, be the corrected ver-sion of the answer returned under the wizard's control.These will have to be cleaned up in the case that thewizard makes an error, or if the answer given by thewizard was the (cooperative) context-dependent answer,which may differ from a context-independent answer, ifit exists.
In the context of a database query system,the wizard is instructed to interpret queries broadly asdatabase requests.
Thus, we believe that "yes/no" ques-tions will be in general interpreted as a request for a list,rather than the word "yes" or "no", as in "Are there anymorning flights to Denver?"
Other conventions involvetreatment of strings for comparison purposes and case-sensitivity, the appearance of extra columns in tabularanswers, and the inclusion of identifying fields (see \[1\]for details).Scoring is accomplished using standardized software,and conventions for inputs and outputs.
Comparingscalar answers simply means comparing values.
Tableanswers are more interesting, since in general the orderof the columns is irrelevant o correctness.
For single-element answers, a scalar answer and a table containinga single element are judged equivalent, for both specifi-cations and answers.
For our first experiment with thenew protocols, sites were only required to report resultson the natural language component.
The transcriptions94were released a few days before the results were to bereported.
One site, CMU, reported results on speechinputs.
See [1] for further details on scoring.ConclusionsThe process of coming to agreement on conventions forevaluation of spoken language systems, and implement-ing such procedures has been a larger task than most ofus anticipated.
We are still learning, and sometimes ithas been painful.
However, the rewards of an automatic,common mechanism for system evaluation is worth theeffort, and we believe the spoken language program willbenefit enormously from this effort.
There still is a gooddeal more work to do as we find ways to meet the con-straints of evaluation i  a way that makes ense for thedevelopment ofspoken language systems.AcknowledgementsThis article is based on a perusing of the voluminousemail and phone discussions involving numerous peoplefrom various sites, including BBN, CMU, MIT, MIT-LL, NIST, SRI, TI, and Unisys.
The author gratefullyacknowledges the important roles played by individu-als from each of these sites.
The program described isfunded by DARPA, the particular contract hat fundedthe writing of this paper is through DARPA under Officeof Naval Research contract N00014-90-C-0085.References[1] L. Bates and S. Boisen, "Developing an EvaluationMethodology for Spoken Language Systems," thisvolume.
[2] C. Hemphill, J. Godfrey, and G. Doddington, "TheATIS Spoken Language Systems Pilot Corpus," thisvolume.
[3] L. Hirschman, D. Dahl, D. McKay, L. Norton, andM.
Linebarger, "Beyond Class A: A proposal forAutomatic Evaluation of Discourse," this volume.
[4] D. Pallett and W. Fisher, "Performance Results Re-ported to NIST," this volume.
[5] D. Pallett, chair, "ATIS Site Reports and GeneralDiscussion," Session 5, this volume.
[6] P. J.
Price, W. M. Fisher, J. Bernstein, and D. S.Pallett, "The DARPA 1000-Word Resource Man-agement Database for Continuous Speech Recogni-tion," Proc.
ICASSP, 1988.
Database available onCD-ROM.
[7] B. Sondheim, "Plans for a Task-Oriented Evalua-tion of Natural Language Understanding Systems,"Proc.
of ~he DARPA Speech and Natural LanguageWorkshop, Feb. 1989.95
