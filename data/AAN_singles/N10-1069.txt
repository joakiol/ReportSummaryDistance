Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 456?464,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsDistributed Training Strategies for the Structured PerceptronRyan McDonald Keith Hall Gideon MannGoogle, Inc., New York / Zurich{ryanmcd|kbhall|gmann}@google.comAbstractPerceptron training is widely applied in thenatural language processing community forlearning complex structured models.
Like allstructured prediction learning frameworks, thestructured perceptron can be costly to trainas training complexity is proportional to in-ference, which is frequently non-linear in ex-ample sequence length.
In this paper weinvestigate distributed training strategies forthe structured perceptron as a means to re-duce training times when computing clustersare available.
We look at two strategies andprovide convergence bounds for a particu-lar mode of distributed structured perceptrontraining based on iterative parameter mixing(or averaging).
We present experiments ontwo structured prediction problems ?
named-entity recognition and dependency parsing ?to highlight the efficiency of this method.1 IntroductionOne of the most popular training algorithms forstructured prediction problems in natural languageprocessing is the perceptron (Rosenblatt, 1958;Collins, 2002).
The structured perceptron has manydesirable properties, most notably that there is noneed to calculate a partition function, which isnecessary for other structured prediction paradigmssuch as CRFs (Lafferty et al, 2001).
Furthermore,it is robust to approximate inference, which is of-ten required for problems where the search spaceis too large and where strong structural indepen-dence assumptions are insufficient, such as parsing(Collins and Roark, 2004; McDonald and Pereira,2006; Zhang and Clark, 2008) and machine trans-lation (Liang et al, 2006).
However, like all struc-tured prediction learning frameworks, the structureperceptron can still be cumbersome to train.
Thisis both due to the increasing size of available train-ing sets as well as the fact that training complexityis proportional to inference, which is frequently non-linear in sequence length, even with strong structuralindependence assumptions.In this paper we investigate distributed trainingstrategies for the structured perceptron as a meansof reducing training times when large computingclusters are available.
Traditional machine learningalgorithms are typically designed for a single ma-chine, and designing an efficient training mechanismfor analogous algorithms on a computing cluster ?often via a map-reduce framework (Dean and Ghe-mawat, 2004) ?
is an active area of research (Chuet al, 2007).
However, unlike many batch learningalgorithms that can easily be distributed through thegradient calculation, a distributed training analog forthe perceptron is less clear cut.
It employs online up-dates and its loss function is technically non-convex.A recent study by Mann et al (2009) has shownthat distributed training through parameter mixing(or averaging) for maximum entropy models canbe empirically powerful and has strong theoreticalguarantees.
A parameter mixing strategy, which canbe applied to any parameterized learning algorithm,trains separate models in parallel, each on a disjointsubset of the training data, and then takes an averageof all the parameters as the final model.
In this paper,we provide results which suggest that the percep-tron is ill-suited for straight-forward parameter mix-ing, even though it is commonly used for large-scalestructured learning, e.g., Whitelaw et al (2008) fornamed-entity recognition.
However, a slight mod-456ification we call iterative parameter mixing can beshown to: 1) have similar convergence properties tothe standard perceptron algorithm, 2) find a sepa-rating hyperplane if the training set is separable, 3)reduce training times significantly, and 4) producemodels with comparable (or superior) accuracies tothose trained serially on all the data.2 Related WorkDistributed cluster computation for many batchtraining algorithms has previously been examinedby Chu et al (2007), among others.
Much of therelevant prior work on online (or sub-gradient) dis-tributed training has been focused on asynchronousoptimization via gradient descent.
In this sce-nario, multiple machines run stochastic gradient de-scent simultaneously as they update and read froma shared parameter vector asynchronously.
Earlywork by Tsitsiklis et al (1986) demonstrated thatif the delay between model updates and reads isbounded, then asynchronous optimization is guaran-teed to converge.
Recently, Zinkevich et al (2009)performed a similar type of analysis for online learn-ers with asynchronous updates via stochastic gra-dient descent.
The asynchronous algorithms inthese studies require shared memory between thedistributed computations and are less suitable tothe more common cluster computing environment,which is what we study here.While we focus on the perceptron algorithm, thereis a large body of work on training structured pre-diction classifiers.
For batch training the most com-mon is conditional random fields (CRFs) (Laffertyet al, 2001), which is the structured analog of maxi-mum entropy.
As such, its training can easily be dis-tributed through the gradient or sub-gradient com-putations (Finkel et al, 2008).
However, unlike per-ceptron, CRFs require the computation of a partitionfunction, which is often expensive and sometimesintractable.
Other batch learning algorithms includeM3Ns (Taskar et al, 2004) and Structured SVMs(Tsochantaridis et al, 2004).
Due to their efficiency,online learning algorithms have gained attention, es-pecially for structured prediction tasks in NLP.
Inaddition to the perceptron (Collins, 2002), othershave looked at stochastic gradient descent (Zhang,2004), passive aggressive algorithms (McDonald etPerceptron(T = {(xt,yt)}|T |t=1)1. w(0) = 0; k = 02. for n : 1..N3.
for t : 1..T4.
Let y?
= argmaxy?
w(k) ?
f(xt,y?)5.
if y?
6= yt6.
w(k+1) = w(k) + f(xt,yt)?
f(xt,y?)7.
k = k + 18. return w(k)Figure 1: The perceptron algorithm.al., 2005; Crammer et al, 2006), the recently intro-duced confidence weighted learning (Dredze et al,2008) and coordinate descent algorithms (Duchi andSinger, 2009).3 Structured PerceptronThe structured perceptron was introduced by Collins(2002) and we adopt much of the notation and pre-sentation of that study.
The structured percetron al-gorithm ?
which is identical to the multi-class per-ceptron ?
is shown in Figure 1.
The perceptron is anonline learning algorithm and processes training in-stances one at a time during each epoch of training.Lines 4-6 are the core of the algorithm.
For a input-output training instance pair (xt,yt) ?
T , the algo-rithm predicts a structured output y?
?
Yt, where Ytis the space of permissible structured outputs for in-put xt, e.g., parse trees for an input sentence.
Thisprediction is determined by a linear classifier basedon the dot product between a high-dimensional fea-ture representation of a candidate input-output pairf(x,y) ?
RM and a corresponding weight vectorw ?
RM , which are the parameters of the model1.If this prediction is incorrect, then the parametersare updated to add weight to features for the cor-responding correct output yt and take weight awayfrom features for the incorrect output y?.
For struc-tured prediction, the inference step in line 4 is prob-lem dependent, e.g., CKY for context-free parsing.A training set T is separable with margin ?
>0 if there exists a vector u ?
RM with ?u?
= 1such that u ?
f(xt,yt) ?
u ?
f(xt,y?)
?
?, for all(xt,yt) ?
T , and for all y?
?
Yt such that y?
6= yt.Furthermore, letR ?
||f(xt,yt)?f(xt,y?
)||, for all(xt,yt) ?
T and y?
?
Yt.
A fundamental theorem1The perceptron can be kernalized for non-linearity.457of the perceptron is as follows:Theorem 1 (Novikoff (1962)).
Assume training setT is separable by margin ?.
Let k be the number ofmistakes made training the perceptron (Figure 1) onT .
If training is run indefinitely, then k ?
R2?2 .Proof.
See Collins (2002) Theorem 1.Theorem 1 implies that if T is separable then 1) theperceptron will converge in a finite amount of time,and 2) will produce a w that separates T .
Collinsalso proposed a variant of the structured perceptronwhere the final weight vector is a weighted averageof all parameters that occur during training, whichhe called the averaged perceptron and can be viewedas an approximation to the voted perceptron algo-rithm (Freund and Schapire, 1999).4 Distributed Structured PerceptronIn this section we examine two distributed trainingstrategies for the perceptron algorithm based on pa-rameter mixing.4.1 Parameter MixingDistributed training through parameter mixing is astraight-forward way of training classifiers in paral-lel.
The algorithm is given in Figure 2.
The idea issimple: divide the training data T into S disjointshards such that T = {T1, .
.
.
, TS}.
Next, trainperceptron models (or any learning algorithm) oneach shard in parallel.
After training, set the finalparameters to a weighted mixture of the parametersof each model using mixture coefficients ?.
Notethat we call this strategy parameter mixing as op-posed to parameter averaging to distinguish it fromthe averaged perceptron (see previous section).
It iseasy to see how this can be implemented on a clusterthrough a map-reduce framework, i.e., the map steptrains the individual models in parallel and the re-duce step mixes their parameters.
The advantages ofparameter mixing are: 1) that it is parallel, makingit possibly to scale to extremely large data sets, and2) it is resource efficient, in particular with respectto network usage as parameters are not repeatedlypassed across the network as is often the case forexact distributed training strategies.For maximum entropy models, Mann et al (2009)show it is possible to bound the norm of the dif-PerceptronParamMix(T = {(xt,yt)}|T |t=1)1.
Shard T into S pieces T = {T1, .
.
.
, TS}2. w(i) = Perceptron(Ti) ?3.
w =?i ?iw(i) ?4.
return wFigure 2: Distributed perceptron using a parameter mix-ing strategy.
?
Each w(i) is computed in parallel.
?
?
={?1, .
.
.
, ?S}, ?
?i ?
?
: ?i ?
0 and?i ?i = 1.ference between parameters trained on all the dataserially versus parameters trained with parametermixing.
However, their analysis requires a stabil-ity bound on the parameters of a regularized max-imum entropy model, which is not known to holdfor the perceptron.
In Section 5, we present empir-ical results showing that parameter mixing for dis-tributed perceptron can be sub-optimal.
Addition-ally, Dredze et al (2008) present negative parame-ter mixing results for confidence weighted learning,which is another online learning algorithm.
The fol-lowing theorem may help explain this behavior.Theorem 2.
For a any training set T separable bymargin ?, the perceptron algorithm trained througha parameter mixing strategy (Figure 2) does not nec-essarily return a separating weight vector w.Proof.
Consider a binary classification settingwhere Y = {0, 1} and T has 4 instances.We distribute the training set into two shards,T1 = {(x1,1,y1,1), (x1,2,y1,2)} and T2 ={(x2,1,y2,1), (x2,2,y2,2)}.
Let y1,1 = y2,1 = 0 andy1,2 = y2,2 = 1.
Now, let w, f ?
R6 and usingblock features, define the feature space as,f(x1,1, 0) = [1 1 0 0 0 0] f(x1,1, 1) = [0 0 0 1 1 0]f(x1,2, 0) = [0 0 1 0 0 0] f(x1,2, 1) = [0 0 0 0 0 1]f(x2,1, 0) = [0 1 1 0 0 0] f(x2,1, 1) = [0 0 0 0 1 1]f(x2,2, 0) = [1 0 0 0 0 0] f(x2,2, 1) = [0 0 0 1 0 0]Assuming label 1 tie-breaking, parameter mixing re-turns w1=[1 1 0 -1 -1 0] and w2=[0 1 1 0 -1 -1].
Forany ?, the mixed weight vector w will not separateall the points.
If both ?1/?2 are non-zero, then allexamples will be classified 0.
If ?1=1 and ?2=0,then (x2,2,y2,2) will be incorrectly classified as 0and (x1,2,y1,2) when ?1=0 and ?2=1.
But there is aseparating weight vector w = [-1 2 -1 1 -2 1].This counter example does not say that a parametermixing strategy will not converge.
On the contrary,458if T is separable, then each of its subsets is separa-ble and converge via Theorem 1.
What it does sayis that, independent of ?, the mixed weight vectorproduced after convergence will not necessarily sep-arate the entire data, even when T is separable.4.2 Iterative Parameter MixingConsider a slight augmentation to the parametermixing strategy.
Previously, each parallel percep-tron was trained to convergence before the parame-ter mixing step.
Instead, shard the data as before, buttrain a single epoch of the perceptron algorithm foreach shard (in parallel) and mix the model weights.This mixed weight vector is then re-sent to eachshard and the perceptrons on those shards reset theirweights to the new mixed weights.
Another singleepoch of training is then run (again in parallel overthe shards) and the process repeats.
This iterativeparameter mixing algorithm is given in Figure 3.Again, it is easy to see how this can be imple-mented as map-reduce, where the map computes theparameters for each shard for one epoch and the re-duce mixes and re-sends them.
This is analogousto batch distributed gradient descent methods wherethe gradient for each shard is computed in parallel inthe map step and the reduce step sums the gradientsand updates the weight vector.
The disadvantage ofiterative parameter mixing, relative to simple param-eter mixing, is that the amount of information sentacross the network will increase.
Thus, if networklatency is a bottleneck, this can become problematic.However, for many parallel computing frameworks,including both multi-core computing as well as clus-ter computing with high rates of connectivity, this isless of an issue.Theorem 3.
Assume a training set T is separableby margin ?.
Let ki,n be the number of mistakes thatoccurred on shard i during the nth epoch of train-ing.
For any N , when training the perceptron withiterative parameter mixing (Figure 3),N?n=1S?i=1?i,nki,n ?R2?2Proof.
Let w(i,n) to be the weight vector for theith shard after the nth epoch of the main loop andlet w([i,n]?k) be the weight vector that existed onshard i in the nth epoch k errors before w(i,n).
LetPerceptronIterParamMix(T = {(xt,yt)}|T |t=1)1.
Shard T into S pieces T = {T1, .
.
.
, TS}2. w = 03. for n : 1..N4.
w(i,n) = OneEpochPerceptron(Ti,w) ?5.
w =?i ?i,nw(i,n) ?6.
return wOneEpochPerceptron(T , w?)1.
w(0) = w?
; k = 02. for t : 1..T3.
Let y?
= argmaxy?
w(k) ?
f(xt,y?)4.
if y?
6= yt5.
w(k+1) = w(k) + f(xt,yt)?
f(xt,y?)6.
k = k + 17. return w(k)Figure 3: Distributed perceptron using an iterative param-eter mixing strategy.
?
Each w(i,n) is computed in paral-lel.
?
?n = {?1,n, .
.
.
, ?S,n}, ?
?i,n ?
?n: ?i,n ?
0 and?n:?i ?i,n = 1.w(avg,n) be the mixed vector from the weight vec-tors returned after the nth epoch, i.e.,w(avg,n) =S?i=1?i,nw(i,n)Following the analysis from Collins (2002) Theorem1, by examining line 5 of OneEpochPerceptron inFigure 3 and the fact that u separates the data by ?
:u ?w(i,n) = u ?w([i,n]?1)+ u ?
(f(xt,yt)?
f(xt,y?))?
u ?w([i,n]?1) + ??
u ?w([i,n]?2) + 2?.
.
.
?
u ?w(avg,n?1) + ki,n?
(A1)That is, u ?
w(i,n) is bounded below by the averageweight vector for the n-1st epoch plus the numberof mistakes made on shard i during the nth epochtimes the margin ?.
Next, by OneEpochPerceptronline 5, the definition ofR, and w([i,n]?1)(f(xt,yt)?f(xt,y?))
?
0 when line 5 is called:?w(i,n)?2 = ?w([i,n]?1)?2+?f(xt,yt)?
f(xt,y?
)?2+ 2w([i,n]?1)(f(xt,yt)?
f(xt,y?))?
?w([i,n]?1)?2 +R2?
?w([i,n]?2)?2 + 2R2.
.
.
?
?w(avg,n?1)?2 + ki,nR2 (A2)459That is, the squared L2-norm of a shards weight vec-tor is bounded above by the same value for the aver-age weight vector of the n-1st epoch and the numberof mistakes made on that shard during the nth epochtimes R2.Using A1/A2 we prove two inductive hypotheses:u ?w(avg,N) ?N?n=1S?i=1?i,nki,n?
(IH1)?w(avg,N)?2 ?N?n=1S?i=1?i,nki,nR2 (IH2)IH1 implies ?w(avg,N)?
?
?Nn=1?Si=1 ?i,nki,n?since u ?w ?
?u??w?
and ?u?
= 1.The base case is w(avg,1), where we can observe:u ?wavg,1 =S?i=1?i,1u ?w(i,1) ?S?i=1?i,1ki,1?using A1 and the fact that w(avg,0) = 0 for the sec-ond step.
For the IH2 base case we can write:?w(avg,1)?2 =?????S?i=1?i,1w(i,1)????
?2?S?i=1?i,1?w(i,1)?2 ?S?i=1?i,1ki,1R2The first inequality is Jensen?s inequality, and thesecond is true by A2 and ?w(avg,0)?2 = 0.Proceeding to the general case, w(avg,N):u ?w(avg,N) =S?i=1?i,N (u ?w(i,N))?S?i=1?i,N (u ?w(avg,N?1) + ki,N?
)= u ?w(avg,N?1) +S?i=1?i,Nki,N??[N?1?n=1S?i=1?i,nki,n?
]+S?i=1?i,Nki,N=N?n=1S?i=1?i,nki,n?The first inequality uses A1, the second step?i ?i,N = 1 and the second inequality the induc-tive hypothesis IH1.
For IH2, in the general case,we can write:?w(avg,N)?2 ?S?i=1?i,N?w(i,N)?2?S?i=1?i,N (?w(avg,N?1)?2 + ki,NR2)= ?w(avg,N?1)?2 +S?i=1?i,Nki,NR2?
[N?1?n=1S?i=1?i,nki,nR2]+S?i=1?i,Nki,NR2=N?n=1S?i=1?i,nki,nR2The first inequality is Jensen?s, the second A2, andthe third the inductive hypothesis IH2.
Putting to-gether IH1, IH2 and ?w(avg,N)?
?
u ?w(avg,N):[N?n=1S?i=1?i,nki,n]2?2 ?
[N?n=1S?i=1?i,nki,n]R2which yields:?Nn=1?Si=1 ?i,nki,n ?R2?24.3 AnalysisIf we set each ?n to be the uniform mixture, ?i,n =1/S, then Theorem 3 guarantees convergence toa separating hyperplane.
If?Si=1 ?i,nki,n = 0,then the previous weight vector already separatedthe data.
Otherwise,?Nn=1?Si=1 ?i,nki,n is still in-creasing, but is bounded and cannot increase indefi-nitely.
Also note that if S = 1, then ?1,n must equal1 for all n and this bound is identical to Theorem 1.However, we are mainly concerned with how fastconvergence occurs, which is directly related to thenumber of training epochs each algorithm must run,i.e., N in Figure 1 and Figure 3.
For the non-distributed variant of the perceptron we can say thatNnon dist ?
R2/?2 since in the worst case a singlemistake happens on each epoch.2 For the distributedcase, consider setting ?i,n = ki,n/kn, where kn =?i ki,n.
That is, we mix parameters proportional tothe number of errors each made during the previousepoch.
Theorem 3 still implies convergence to a sep-arating hyperplane with this choice.
Further, we can2It is not hard to derive such degenerate cases.460bound the required number of epochs Ndist:Ndist ?Ndist?n=1S?i=1[ki,n]ki,nkn ?Ndist?n=1S?i=1ki,nknki,n ?R2?2Ignoring when all ki,n are zero (since the algorithmwill have converged), the first inequality is true sinceeither ki,n ?
1, implying that [ki,n]ki,n/kn ?
1, orki,n = 0 and [ki,n]ki,n/kn = 1.
The second inequal-ity is true by the generalized arithmetic-geometricmean inequality and the final inequality is Theo-rem 3.
Thus, the worst-case number of epochs isidentical for both the regular and distributed percep-tron ?
but the distributed perceptron can theoreti-cally process each epoch S times faster.
This ob-servation holds only for cases where ?i,n > 0 whenki,n ?
1 and ?i,n = 0 when ki,n = 0, which doesnot include uniform mixing.5 ExperimentsTo investigate the distributed perceptron strategiesdiscussed in Section 4 we look at two structured pre-diction tasks ?
named entity recognition and depen-dency parsing.
We compare up to four systems:1.
Serial (All Data): This is the classifier returnedif trained serially on all the available data.2.
Serial (Sub Sampling): Shard the data, selectone shard randomly and train serially.3.
Parallel (Parameter Mix): Parallel strategydiscussed in Section 4.1 with uniform mixing.4.
Parallel (Iterative Parameter Mix): Parallelstrategy discussed in Section 4.2 with uniformmixing (Section 5.1 looks at mixing strategies).For all four systems we compare results for both thestandard perceptron algorithm as well as the aver-aged perceptron algorithm (Collins, 2002).We report the final test set metrics of the con-verged classifiers to determine whether any loss inaccuracy is observed as a consequence of distributedtraining strategies.
We define convergence as ei-ther: 1) the training set is separated, or 2) the train-ing set performance measure (accuracy, f-measure,etc.)
does not change by more than some pre-definedthreshold on three consecutive epochs.
As with mostreal world data sets, convergence by training set sep-aration was rarely observed, though in both casestraining set accuracies approached 100%.
For bothtasks we also plot test set metrics relative to the userwall-clock taken to obtain the classifier.
The resultswere computed by collecting the metrics at the endof each epoch for every classifier.
All experimentsused 10 shards (Section 5.1 looks at convergence rel-ative to different shard size).Our first experiment is a named-entity recogni-tion task using the English data from the CoNLL2003 shared-task (Tjong Kim Sang and De Meul-der, 2003).
The task is to detect entities in sentencesand label them as one of four types: people, organi-zations, locations or miscellaneous.
For our exper-iments we used the entire training set (14041 sen-tences) and evaluated on the official developmentset (3250 sentences).
We used a straight-forwardIOB label encoding with a 1st order Markov fac-torization.
Our feature set consisted of predicatesextracted over word identities, word affixes, orthog-raphy, part-of-speech tags and corresponding con-catenations.
The evaluation metric used was microf-measure over the four entity class types.Results are given in Figure 4.
There are a num-ber of things to observe here: 1) training on a singleshard clearly provides inferior performance to train-ing on all data, 2) the simple parameter mixing strat-egy improves upon a single shard, but does not meetthe performance of training on all data, 3) iterativeparameter mixing achieves performance as good asor better than training serially on all the data, and4) the distributed algorithms return better classifiersmuch quicker than training serially on all the data.This is true regardless of whether the underlying al-gorithm is the regular or the averaged perceptron.Point 3 deserves more discussion.
In particular, theiterative parameter mixing strategy has a higher finalf-measure than training on all the data serially thanthe standard perceptron (f-measure of 87.9 vs. 85.8).We suspect this happens for two reasons.
First, theparameter mixing has a bagging like effect whichhelps to reduce the variance of the per-shard classi-fiers (Breiman, 1996).
Second, the fact that parame-ter mixing is just a form of parameter averaging per-haps has the same effect as the averaged perceptron.Our second set of experiments looked at the muchmore computationally intensive task of dependencyparsing.
We used the Prague Dependency Tree-bank (PDT) (Hajic?
et al, 2001), which is a Czech461Wall Clock0.650.70.750.80.85TestDataF-measurePerceptron -- Serial (All Data)Perceptron -- Serial (Sub Sampling)Perceptron -- Parallel (Parameter Mix)Perceptron -- Parallel (Iterative Parameter Mix)Wall Clock0.70.750.80.85TestDataF-measureAveraged Perceptron -- Serial (All Data)Averaged Perceptron -- Serial (Sub Sampling)Averaged Perceptron -- Parallel (Parameter Mix)Averaged Perceptron -- Parallel (Iterative Parameter Mix)Reg.
Perceptron Avg.
PerceptronF-measure F-measureSerial (All Data) 85.8 88.2Serial (Sub Sampling) 75.3 76.6Parallel (Parameter Mix) 81.5 81.6Parallel (Iterative Parameter Mix) 87.9 88.1Figure 4: NER experiments.
Upper figures plot test data f-measure versus wall clock for both regular perceptron (left)and averaged perceptron (right).
Lower table is f-measure for converged models.language treebank and currently one of the largestdependency treebanks in existence.
We used theCoNLL-X training (72703 sentences) and testingsplits (365 sentences) of this data (Buchholz andMarsi, 2006) and dependency parsing models basedon McDonald and Pereira (2006) which factors fea-tures over pairs of dependency arcs in a tree.
Toparse all the sentences in the PDT, one must use anon-projective parsing algorithm, which is a knownNP-complete inference problem when not assumingstrong independence assumptions.
Thus, the use ofapproximate inference techniques is common in or-der to find the highest weighted tree for a sentence.We use the approximate parsing algorithm given inMcDonald and Pereira (2006), which runs in timeroughly cubic in sentence length.
To train such amodel is computationally expensive and can take onthe order of days to train on a single machine.Unlabeled attachment scores (Buchholz andMarsi, 2006) are given in Figure 5.
The same trendsare seen for dependency parsing that are seen fornamed-entity recognition.
That is, iterative param-eter mixing learns classifiers faster and has a finalaccuracy as good as or better than training seriallyon all data.
Again we see that the iterative parame-ter mixing model returns a more accurate classifierthan the regular perceptron, but at about the samelevel as the averaged perceptron.5.1 Convergence PropertiesSection 4.3 suggests that different weighting strate-gies can lead to different convergence properties,in particular with respect to the number of epochs.For the named-entity recognition task we ran fourexperiments comparing two different mixing strate-gies ?
uniform mixing (?i,n=1/S) and error mix-ing (?i,n=ki,n/kn) ?
each with two shard sizes ?S = 10 and S = 100.
Figure 6 plots the numberof training errors per epoch for each strategy.We can make a couple observations.
First, themixing strategy makes little difference.
The rea-son being that the number of observed errors perepoch is roughly uniform across shards, makingboth strategies ultimately equivalent.
The other ob-servation is that increasing the number of shardscan slow down convergence when viewed relative toepochs3.
Again, this appears in contradiction to theanalysis in Section 4.3, which, at least for the caseof error weighted mixtures, implied that the num-ber of epochs to convergence was independent ofthe number of shards.
But that analysis was basedon worst-case scenarios where a single error occurson a single shard at each epoch, which is unlikely tooccur in real world data.
Instead, consider the uni-3As opposed to raw wall-clock/CPU time, which benefitsfrom faster epochs the more shards there are.462Wall Clock0.740.760.780.80.820.84UnlabeledAttachment ScorePerceptron -- Serial (All Data)Perceptron -- Serial (Sub Sampling)Perceptron -- Parallel (Iterative Parameter Mix)Wall Clock0.780.790.80.810.820.830.840.85UnlabeledAttachment ScoreAveraged Perceptron -- Serial (All Data)Averaged Perceptron -- Serial (Sub Sampling)Averaged Perceptron -- (Iterative Parameter Mix)Reg.
Perceptron Avg.
PerceptronUnlabeled Attachment Score Unlabeled Attachment ScoreSerial (All Data) 81.3 84.7Serial (Sub Sampling) 77.2 80.1Parallel (Iterative Parameter Mix) 83.5 84.5Figure 5: Dependency Parsing experiments.
Upper figures plot test data unlabeled attachment score versus wall clockfor both regular perceptron (left) and averaged perceptron (right).
Lower table is unlabeled attachment score forconverged models.0 10 20 30 40 50Training Epochs0200040006000800010000# TrainingMistakesError mixing (10 shards)Uniform mixing (10 shards)Error mixing (100 shards)Uniform mixing (100 shards)Figure 6: Training errors per epoch for different shardsize and parameter mixing strategies.form mixture case.
Theorem 3 implies:N?n=1S?i=1ki,nS?R2?2=?N?n=1S?i=1ki,n ?
S ?R2?2Thus, for cases where training errors are uniformlydistributed across shards, it is possible that, in theworst-case, convergence may slow proportional thethe number of shards.
This implies a trade-off be-tween slower convergence and quicker epochs whenselecting a large number of shards.
In fact, we ob-served a tipping point for our experiments in whichincreasing the number of shards began to have an ad-verse effect on training times, which for the named-entity experiments occurred around 25-50 shards.This is both due to reasons described in this sectionas well as the added overhead of maintaining andsumming multiple high-dimensional weight vectorsafter each distributed epoch.It is worth pointing out that a linear term S inthe convergence bound above is similar to conver-gence/regret bounds for asynchronous distributedonline learning, which typically have bounds lin-ear in the asynchronous delay (Mesterharm, 2005;Zinkevich et al, 2009).
This delay will be on aver-age roughly equal to the number of shards S.6 ConclusionsIn this paper we have investigated distributing thestructured perceptron via simple parameter mixingstrategies.
Our analysis shows that an iterative pa-rameter mixing strategy is both guaranteed to sepa-rate the data (if possible) and significantly reducesthe time required to train high accuracy classifiers.However, there is a trade-off between increasingtraining times through distributed computation andslower convergence relative to the number of shards.Finally, we note that using similar proofs to thosegiven in this paper, it is possible to provide theoreti-cal guarantees for distributed online passive aggres-sive learning (Crammer et al, 2006), which is a formof large-margin perceptron learning.
Unfortunatelyspace limitations prevent exploration here.Acknowledgements: We thank Mehryar Mohri, Fer-nando Periera, Mark Dredze and the three anonymous re-views for their helpful comments on this work.463ReferencesL.
Breiman.
1996.
Bagging predictors.
Machine Learn-ing, 24(2):123?140.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proceed-ings of the Conference on Computational Natural Lan-guage Learning.C.T.
Chu, S.K.
Kim, Y.A.
Lin, Y.Y.
Yu, G. Bradski, A.Y.Ng, and K. Olukotun.
2007.
Map-Reduce for ma-chine learning on multicore.
In Advances in NeuralInformation Processing Systems.M.
Collins and B. Roark.
2004.
Incremental parsing withthe perceptron algorithm.
In Proceedings of the Con-ference of the Association for Computational Linguis-tics.M.
Collins.
2002.
Discriminative training methods forhidden Markov models: Theory and experiments withperceptron algorithm.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing.K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,and Y.
Singer.
2006.
Online passive-aggressive algo-rithms.
The Journal of Machine Learning Research,7:551?585.J.
Dean and S. Ghemawat.
2004.
MapReduce: Simpli-fied data processing on large clusters.
In Sixth Sym-posium on Operating System Design and Implementa-tion.M.
Dredze, K. Crammer, and F. Pereira.
2008.Confidence-weighted linear classification.
In Pro-ceedings of the International Conference on Machinelearning.J.
Duchi and Y.
Singer.
2009.
Efficient learning usingforward-backward splitting.
In Advances in Neural In-formation Processing Systems.J.R.
Finkel, A. Kleeman, and C.D.
Manning.
2008.
Effi-cient, feature-based, conditional random field parsing.In Proceedings of the Conference of the Associationfor Computational Linguistics.Y.
Freund and R.E.
Schapire.
1999.
Large margin clas-sification using the perceptron algorithm.
MachineLearning, 37(3):277?296.J.
Hajic?, B. Vidova Hladka, J.
Panevova?, E. Hajic?ova?,P.
Sgall, and P. Pajas.
2001.
Prague Dependency Tree-bank 1.0.
LDC, 2001T10.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof the International Conference on Machine Learning.P.
Liang, A.
Bouchard-Co?te?, D. Klein, and B. Taskar.2006.
An end-to-end discriminative approach to ma-chine translation.
In Proceedings of the Conference ofthe Association for Computational Linguistics.G.
Mann, R. McDonald, M. Mohri, N. Silberman, andD.
Walker.
2009.
Efficient large-scale distributedtraining of conditional maximum entropy models.
InAdvances in Neural Information Processing Systems.R.
McDonald and F. Pereira.
2006.
Online learning ofapproximate dependency parsing algorithms.
In Pro-ceedings of the Conference of the European Chapterof the Association for Computational Linguistics.R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProceedings of the Conference of the Association forComputational Linguistics.C.
Mesterharm.
2005.
Online learning with delayed la-bel feedback.
In Proceedings of Algorithmic LearningTheory.A.B.
Novikoff.
1962.
On convergence proofs on percep-trons.
In Symposium on the Mathematical Theory ofAutomata.F.
Rosenblatt.
1958.
The perceptron: A probabilisticmodel for information storage and organization in thebrain.
Psychological Review, 65(6):386?408.B.
Taskar, C. Guestrin, and D. Koller.
2004.
Max-marginMarkov networks.
In Advances in Neural InformationProcessing Systems.E.
F. Tjong Kim Sang and F. De Meulder.
2003.
Intro-duction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.
In Proceed-ings of the Conference on Computational Natural Lan-guage Learning.J.
N. Tsitsiklis, D. P. Bertsekas, and M. Athans.
1986.Distributed asynchronous deterministic and stochasticgradient optimization algorithms.
IEEE Transactionson Automatic Control, 31(9):803?812.I.
Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.2004.
Support vector machine learning for interdepen-dent and structured output spaces.
In Proceedings ofthe International Conference on Machine learning.C.
Whitelaw, A. Kehlenbeck, N. Petrovic, and L. Ungar.2008.
Web-scale named entity recognition.
In Pro-ceedings of the International Conference on Informa-tion and Knowledge Management.Y.
Zhang and S. Clark.
2008.
A tale of two parsers: In-vestigating and combining graph-based and transition-based dependency parsing using beam-search.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing.T.
Zhang.
2004.
Solving large scale linear predictionproblems using stochastic gradient descent algorithms.In Proceedings of the International Conference on Ma-chine Learning.M.
Zinkevich, A. Smola, and J. Langford.
2009.
Slowlearners are fast.
In Advances in Neural InformationProcessing Systems.464
