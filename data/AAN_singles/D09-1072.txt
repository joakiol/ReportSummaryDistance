Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 688?697,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPA Simple Unsupervised Learner for POS Disambiguation RulesGiven Only a Minimal LexiconQiuye Zhao Mitch MarcusDept.
of Computer & Information ScienceUniversity of Pennsylvaniaqiuye, mitch@cis.upenn.eduAbstractWe propose a new model for unsupervisedPOS tagging based on linguistic distinc-tions between open and closed-class items.Exploiting notions from current linguis-tic theory, the system uses far less infor-mation than previous systems, far simplercomputational methods, and far sparserdescriptions in learning contexts.
By ap-plying simple language acquisition tech-niques based on counting, the system isgiven the closed-class lexicon, acquires alarge open-class lexicon and then acquiresdisambiguation rules for both.
This sys-tem achieves a 20% error reduction forPOS tagging over state-of-the-art unsuper-vised systems tested under the same con-ditions, and achieves comparable accuracywhen trained with much less prior infor-mation.1 IntroductionAll recent research on unsupervised tagging, aswell as the majority of work on supervised tag-gers, views POS tagging as a sequential labelingproblem and treats all POS tags, both closed- andopen-class, as roughly equivalent.
In this work weexplore a different understanding of the taggingproblem, viewing it as a process of first identifyingfunctional syntactic contexts, which are flaggedby closed-class items, and then using these func-tional contexts to determine the POS labels.
Thisdisambiguation model differs from most previouswork in three ways: 1) it uses different encod-ings over two distinct domains (roughly open- andclosed-class words) with complementary distribu-tion (and so decodes separately); 2) it is determin-istic and 3) it is non-lexicalized.
By learning dis-ambiguation models for open- and closed- classesseparately, we found that the deterministic, rule-based model can be learned from unannotated databy a simple strategy of selecting a rule in each ap-propriate context with the highest count.In contrast to this, most previous work on un-supervised tagging (especially for English) con-centrates on improving the parameter estima-tion techniques for training statistical disambigua-tion models from unannotated data.
For exam-ple, (Smith&Eisner, 2005) proposes contrastiveestimation (CE) for log-linear models (CRF),achieving the current state-of-the-art performanceof 90.4%; (Goldwater&Griffiths, 2007) appliesa Bayesian approach to improve maximum-likelihood estimation (MLE) for training genera-tive models (HMM).
In the main experiments ofboth of these papers, the disambiguation modelis learned, but the algorithms assume a completeknowledge of the lexicon with all possible tags foreach word.
In this work, we propose making sucha large lexicon unnecessary by learning the bulkof the lexicon along with learning a disambigua-tion model.Little previous work has been done on this nat-ural and simple idea because the clusters found byprevious induction schemes are not in line with thelexical categories that we care about.
(Chan, 2008)is perhaps the first with the intention of generat-ing ?a discrete set of clusters.?
By applying simi-lar techniques to (Chan, 2008), which we discusslater, we can generate clusters that closely approx-imate the central open-class lexical categories, amajor advance, but we still require a closed-classlexicon specifying possible tags for these words.This asymmetry in our lexicon acquisition modelconforms with our understanding of natural lan-guage as structured data over two distinct domainswith complementary distribution: open-class (lex-ical) and closed-class (functional).Provided with only a closed-class lexicon of288 words, about 0.6% of the full lexicon, the sys-tem acquires a large open-class lexicon and thenacquires disambiguation rules for both closed- and688open-class words, achieving a tagging accuracy of90.6% for a 24k dataset, as high as the currentstate-of-the-art (90.4%) achieved with a completedictionary.
In the test condition where both algo-rithms are provided with a full lexicon, and aretrained and evaluated over the same 96k dataset,we reduce the tagging error by up to 20%.In Section 2 we explain our understanding of thePOS tagging problem in detail and define the no-tions of functional context and open- and closed-class elements.
Then we will introduce our meth-ods for acquiring the lexicon (Section 3) and learn-ing disambiguation models (Section 4, 5 and 6)step by step.
Results are reported in Section 7 fol-lowed by Section 8 which discusses the linguisticmotivation behind this work and the simplicity andefficiency of our model.2 The Tagging ProblemIn most work on both unsupervised and supervisedproblem, tagging is viewed as a sequential label-ing problem.
In this work, however, we would liketo explore another view on tagging especially con-sidering language as structured data.The engineering concept of POS tags derivesfrom the linguistic notion of syntactic categorywhich specifies the combinatorial properties of aword in an underlying (syntactic) structure.
Giventhe parse structure for a given word sequencewhich breaks the input into recursive functionaldomains such as IP, VP and NP, the POS tag ofeach word can be directly inferred.
Of course, as-suming a pre-parsed structure as input to POS tag-ging is somewhat ridiculous, but it strongly mo-tivates us to highlight the features of structuralinformation for POS tagging.
Without resortingto any intermediate representations richer than theinput string, we propose for engineering purposesto capture the features of interest for POS taggingby the functional items in language themselves.Then tagging is considered to be a process of iden-tifying the functional contexts (functional items incontext) in which the categorical property of thetarget item can be inferred.Following ideas in current linguistic theory dis-cussed in Section 8, we observe that the functionalcategories and some morphological endings serveas markers of the functional domains themselves(discussed above) and sit abstractly at the edge ofthose domains; the open-class (lexical) items mustsit within appropriate functional domains.
Morespecifically, although long distance dependenciesare not at all rare, for a token in sequence, weonly consider adjacent closed-class words and theverbal categorical feature (but not morphology) asfunctional contexts, the core concept in our disam-biguation model.Our system uses five open-class categories:three basic lexical categories verb, noun and ad-verb, and two derived Nominal categories (the twokinds of participles in English); and consider allother words not included in those categories to beclosed-class items.Overall, for the task of unsupervised tagging,we use a rule-based disambiguation model con-taining disambiguation rules conditioned on func-tional contexts, and the model is learned fromunannotated data constrained by much less lexi-cal knowledge than most previous work, namelythe closed-class lexicon as introduced below.2.1 Closed-class LexiconA dictionary containing all possible tags for eachword is very useful to constrain the unsupervisedlearning of a POS disambiguation model, and inmost previous work, a full lexicon computed fromthe WSJ corpus (the source of both training andtest datasets) is used for both learning and tagging.Since a full lexicon is not a reasonable resource,we aim to limit the required knowledge to func-tional (closed-class) words only.It is hard to define functional words in a lin-guistically strict sense, but this category is closeto the notion within the engineering field of NLPof closed-class words, classes of words that arenot open for new members.
From the engineer-ing point of view, this implies that a closed classhas a finite and static number of members, so itsmembers can be listed once and for all.For English, lists of closed-class categories suchas preposition, pronoun or even degree adverb, areobtainable resources, but this is not necessarily thecase for other languages.
In this paper, we leavethe automatic acquisition of a closed-class lexiconfor future work.
For experiments in this work, weautomatically compute a closed-class lexicon fromthe WSJ treebank 00-24 sections by picking outthose words that are labeled predominantly withclosed-class tags1.
For each word selected as aclosed-class word, all possible tags encountered1For each word, if the number of instances labeled byclosed-class tags is greater than by open-class tags, we selectit as a closed-class word.689more than twice in the WSJ corpus are reservedin the closed-class lexicon, so closed-class wordsmay also have open-class tags in our data set, asource of noise in our results.
As a core part oflanguage, this closed-class lexicon containing 288entries, about 0.6% of the full lexicon by types,should be invariant over various genres, which isconfirmed in experiments on both WSJ and Browncorpus2.2.2 TagsetThe 45 tags in the Penn Tagset (Marcus et al,2003) contain more information than just basiclexical categories.
In recent work on unsupervisedlearning of POS taggers following (Smith&Eisner,2005), the Penn tagset is reduced to 17 tags whichnicely improves the tagging performance.Based on our view of POS tags as local mark-ers of underlying syntactic structure, we derive 27tags from a feature-based analysis of the originalPenn tagset.
The main principle for reduction isthat we collapse any two tags which are not distin-guishable by structural features; such features in-clude +/-N, +/-V for predication and +/-wh, +/-enfor movement3.
For example, under our analysis,the tag ?VBG?
has the features [+V, +N, -tense, -en], tag ?VBD?
[+V, +tense(past), -en], and ?VB?
[+V, -tense(finite), -en].
However, since we do notconsider the tense feature to be a structural feature,we do not distinguish ?VBD?
from ?VB?
; sinceN(ominal) is a structural feature, ?VBG?
remainsdistinct from both ?VBD?
and ?VB?.
The 27 tagsdo not cover all cases of ambiguities of closed-class words in the original Penn tagset.
Most no-tably, adjectives are not separated from nouns.This reduction naturally follows the crucialproperties of our disambiguation model.
First ofall, our model is not lexicalized, so it can onlycapture basic interactive relations between cate-gories but cannot capture lexical dependencies,which are heavily required to disambiguate ?RP?2There are two special classes of words worthy of dis-cussion with respect to being closed or open.
1.
While themorphological ending ?-ly?
freely introduces adverbs, thiscategory is otherwise essentially closed class; and 2.
Thereare obviously unboundedly many numbers(CD), but all thesematch some regular pattern.
So we include adverbs withoutexplicit morphological marking in the closed-class lexicon(we frankly doubt adverbs can be acquired by distributionalclustering); and as for numbers, we embed exactly such aregular pattern in our model.3Not all features of tags are listed here, and further dis-cussion of the feature-based analysis of the tagset is to bereported in other work.
This analysis of tags is motivated byChomsky.Tagset #tags #closed #open amb./tokenSmith&Eisner 17 7 6 2.07ThisWork 27 12 6 1.83Penn 45 15 15 2.74Table 1: Comparison of tagsetsCategory Open tags Closed tagsVerbal VB ...Nominal NN, VBN, VBG DT, CD, PRP($), WDT, WP($)None RB CC, EX, IN, MD, POS, TOTable 2: N/V categories of 27 POS tagswith ?IN?
or ?PDT?
with ?DT?
(so these two pairsare collapsed).
More importantly, the structuralinformation carried by the closed-class items isthe key feature of our disambiguation model, butnouns and adjectives are not distinguishable bytheir structural positions (in NP), so they are notto be distinguished in our tagset4.We use this new reduced tagset with 27 tags inour experiments5.
For the purposes of comparison,we map the results using our 27 tag tagset to thecommonly-used 17 tag tagset6, and evaluate ouralgorithms for both tagsets.
Table 1 compare thethree tagsets, and the ambiguity column shows theaverage number of ambiguous tags per token inWSJ corpus section 00-24.2.3 NV categoryBy using the reduced 27 tags, we found in thiswork that the heart of the disambiguation taskfor open-class words is to distinguish them in theNominal vs. Verbal domains; and for the closed-class words, the Nominal vs. Verbal property ofthe adjacent context words is also very helpful for4Due to the indistinguishable roles of adjectives andnouns in Noun Phrase, it is also hard to extract the adjectivesfrom nouns for lexicon acquisition.5For open-class categories, we keep VB (for VB*), NNfor (NN*), VBG, VBN and RB (for RB*), and we reduce theJJ* tags to the tag NN and for closed-class tags, we keep al-most all the original distinctions, except for two pairs: ?PDT?and ?DT?
; ?RP?
and ?IN?.
Also ?WRB?
is reduced to ?RB?.6In our tagset, there are two coarser tags which stand formore than one tag in the 17 tags: ?NN?
stands for both ?N?
and?ADJ?
and ?IN?
for both ?RP?
and ?IN?.
So to map the outputof coarser tags to the finer ones, we need to look up the full-lexicon, since adjectives are not extracted from nouns in thelexicon acquisition process.
For a word tagged as ?NN?
witha possible tag of ?JJ?, if the following word is also tagged as?NN?, then the current ?NN?
is mapped to ?JJ?.
On the otherhand, no action is done for mapping ?IN?, so gold ?RP?
isalways mis-tagged as ?IN?
after mapping.
If our tagging sys-tem outputs a finer tag (e.g.
WDT) then it is reduced to thecorresponding coarser one (e.g.
?W?)
in mapping to 17 tags.690disambiguation.
The Nominal vs. Verbal propertyis defined through N/V categories of POS tags, andwe list each category containing both closed-classand open-class tags in table 2.3 Acquiring the open-class lexiconNot being equipped with a full lexicon, our systemtakes the closed-class lexicon as given, and auto-matically computes possible tags, which must beopen class, for all other words in the acquisitionprocess as described below.
There are five open-class tags in our reduced tagset, as we describeabove: ?VBG?
and ?VBN?
represent two kindsof derived Nominal elements, with correspond-ing morphological endings attached to the verbalroots; and ?RB?
represents the adverbial class intowhich new words can only be introduced if affixedwith the special ending ?-ly?.
Taking into accountthis special morphology, we divide our construc-tion of the open-class lexicon into two steps: N/V-Clustering and Morphing.
At the N/V-clusteringstep, we classify the base-forms (roots) of open-class words into two clusters in a sparse featurespace.
At the Morphing step, we count on the em-bedded functional elements (i.e.
morphology) toderive specific tags for words in each cluster.3.1 ClusteringInducing syntactic categories is a language ac-quisition task on which there has been ex-tensive research, e.g.
(Clark, 2003) and(Sch?utze, 1993), based largely on variantsof distributional clustering.
In a standardsetup of POS clustering, each target word tobe clustered, wi, is represented as a vector,<count(wi,C1), count(wi,C2),...,count(wi,Cm)>,collecting counts of occurrences ofwiin each con-text, Cj.
Then the chosen algorithm clusters thefeature vectors according to similarity.In previous work, the contextual features arelexical, so the length of a feature vector variesfrom hundreds to thousands of features.
Theclustering algorithm then runs over this high-dimensional space, which is computationally quiteintensive.
Unlike previous work, our system onlyemploys seven features, all functional, to representtarget words, and we are paid back by a substantialimprovement in efficiency.
Each open-class wordis represented in the feature space by the followingseven component vector: <left:DT, left:MD, mid:-?, mid:-ed, mid:-ing, right:DT, right:MD>.
Thefirst two values in this vector represent the countsof modal verbs (MD) and determiners (DT) occur-ring to the left of all forms of a base form; the threevalues in the middle represent the counts of threepossible morphological forms of a word; and thelast two values represent the counts of an immedi-ately followingMD and DT.
This radical reductionof the feature space eliminates any need for so-phisticated clustering techniques.
For the purposeof convenience, we use a basic k-means clusteringalgorithm which allows us to specify the numberof output clusters (Maffi, 2007).As is well known, clustering all words in a cor-pus using distributional clustering results in a highnumber of clusters.
For example, (Sch?utze, 1993)induces 200 clusters and (Clark, 2003) choosesbetween 16-128; and most of these induced cate-gories are difficult to associate with a specific POStag.
Chan?s recent thesis work (Chan, 2008) pro-vides us with a solution to this problem.
In the firstpass of Chan?s model for unsupervised lexical cat-egory induction, verbs are separated from all othercategories with a high level of purity; the secondpass separates adjectives from nouns by using thecategorical results from the first pass as an addi-tional feature7.
His experiments for a wide rangeof languages show that the ?restriction to clus-ter base forms only8?
is crucial to induce clustersmore in line with the definition of the open-classsyntactic categories we care about here.Here, we follow a variant of Chan?s approach,grouping words with their base-forms for cluster-ing.
For example, we group all occurrences of thetransformed (morphological) forms, (start, starts,starting and started), in a particular context, Cj,together with the base form start to form a singlecount for (start, Cj), in forming the correspond-ing feature vectors.
Given this, since all inflectionsof one base form share the same feature vector, allinflections enter into the same class of their base-form.
In (Chan, 2008), morphological base formsare the output of a new morphology induction al-gorithm he develops.
Here, we simply extract thebase form of a word by stripping three possibleforms of endings: -s, -ing and -ed9.7For simplicity, we don?t run a second pass but reduceadjectives to noun.8See p.139 in (Chan, 2008)9This simple strategy, as well as more complex morpho-logical analyzers, cannot deal with irregular verbs, so we listin memory the corresponding ?regular?
ending of each irreg-ular verb.
For example, we know that the ending of ran is?-ed?, but we DO NOT know that ran is only the past tense6913.2 MorphingAfter the clustering step, which we intend to sep-arate the Nominal and Verbal classes, two clustersas desired are induced, but we still need a methodto automatically decide which one is which.
Atrick that works well in practice is simply to pickthe smaller class as the Verbal class.
These twoclasses reflect the basic categories of the roots;by a generative mechanism observed in most lan-guages, roots (base-forms) are transformed intoderived categories by fusing with functional el-ements, which surface as the few morphologicalendings in English.For all words in the Nominal class, except forthose with the ending -ly, the only possible tag foreach is ?NN?, since no finer categories of ?NN?
ex-ist in our reduced tagset.
On the other hand, for aword with ending -ly falling into the N class, wesimply assume that its tag must be ?RB?, althoughthis assumption may have a few exceptions.The Verbal class contains all words with ver-bal roots.
There are two specific endings in En-glish serving as morphological markers of derivedNominal categories, -ed and -ing, correspond-ing to derived categories ?VBN?
and ?VBG?
re-spectively.
So for each word ending with -ed,we assign two possible tags to it, ?VB?
(our re-duced form of ?VBD?)
and ?VBN?
; and for eachword ending with -ing we assume only one pos-sible tag, ?VBG?, although this assumption maysystematically introduce tagging error confusing?VBG?
and ?NN?.
For example, if the feature vec-tor representing the base-form group start, starts,started,starting is classified into the verbal class,then both starts and start will receive one possi-ble tag ?VB?
; starting will receive one possible tag?VBG?
; but started will receive two possible tags?VBN?
and ?VB?.As one may notice, start and starts should havetwo senses, noun and verb, but the Nominal senseis lost in the Morphing step.
For such cases, weintroduce a simple supplemental process to com-pensate for the missing Nominal sense.
For a wordwith the possible tag ?VB?
(not ?VBG?
or ?VBN?
)as determined in the Morphing step, if it is everseen following a determiner in context, anotherpossible tag ?NN?
will be assigned to it.Remember that, as introduced in Sect 2.2,form of run, because the ending ?-ed?
is ambiguous for bothpast tense and past participle.
The list of irregular verbs isobtained from http://www.englishpage.com.
?VBN?, ?VBG?
and ?NN?
are of category N and?VB?
is of category V. Then for each word in theresulting lexicon, there is maximally one possibletag of it falling in either category N or V, so thecategory information (N or V) is enough for thedisambiguation task, as specified in Section 6.4 Unsupervised TaggingTaking a dictionary as input, the task of unsuper-vised tagging is to learn a disambiguation modelfrom unannotated data and apply this model fordisambiguating the occurrences of words in con-text.
In this section, we are going to introduce therepresentation of our disambiguation model first,and then discuss how it affects the system design.In the following two sections, we will describe thealgorithms for learning and decoding the languagemodel respectively.4.1 Disambiguation ModelAgain, we view tagging as a process of identifyingfunctional context, from which the proper taggingsimply follows.
Given this, we represent the lan-guage model as a set of disambiguation rules con-ditioned on functional contexts that predict cate-gorical information, with each rule of the form ofr = (con : cat) with con and cat the functionalcontext and categorical information respectively.In both open- and closed-class domains, givena pair of words (Wl,Wr), the disambiguationrules check the functional property of Wland pre-dicts the N/V category of Wr.
However, in theopen-class disambiguation model, con representsclosed-class items as well as verbal feature, but inthe closed-class disambiguation model, con rep-resents closed-class categories (closed-class POStags).
In disambiguating an open-class word, conis checked against the preceding closed-class wordor verbal feature (if any), and cat of the follow-ing open-class word is predicted.
In disambiguat-ing a closed-class word cw, each possible tag ofcw may invoke a rule and each rule will predict aN/V category of the following item; if some rulemakes the right prediction, the corresponding tagis assigned to cw.
For example, he:V, a disam-biguation rule for open-class words, says that if anopen-class token follows the closed-class item he,then the Verbal tag should be assigned to this to-ken.
On the other hand, IN:N, a disambiguationrule for closed-class words, says that if a closed-class token precedes a Nominal word (open- or692closed- class) in context and has a possible tag of?IN?, then tag it with ?IN?.This rule-based disambiguation model is deter-ministic in the sense that for each token in contextthere is maximally one tag that can be predicted.Not being statistically parameterized, this greedyprediction requires that 1) each rule is determin-istic and 2) in each context, only one rule is in-voked (which is guaranteed by the selection stepintroduced in Section 5.2).
Moreover, this disam-biguation model is non-lexicalized in that it is onlyconditioned on the functional items in context butnot the target word itself.4.2 System DesignIdeally, we should use closed-class tags in con-text for disambiguating open-class words becauseclosed-class words are potentially ambiguous; butthis would cause a chicken-egg problem.
If wedid this, then the learning of disambiguation rulesfor closed-class words requires category informa-tion for open-class items and vice versa, but noneof the required category information is availablefrom the unannotated data10.
Thanks to how lan-guage works (including principally the low de-gree of ambiguity of closed-class words), it isgood enough practically, as shown by our exper-iments, to encode the disambiguation model foropen-class words using closed-class items withoutcategorical information.In this way, we can learn the disambiguationmodel of open-class items from raw data; how-ever, closed-class disambiguation model is betterlearned after open-class words are disambiguated.Then there are four models in the system for learn-ing and tagging over two distinct domains: Model-LC and Model-LO for learning the disambigua-tion model of closed- and open-class words re-spectively; Model-DC and Model-DO for disam-biguating closed- and open-class words respec-tively; and they must be executed in a strict orderas follows: Model-LO ?
Model-DO ?
Model-LC ?
Model-DC, as illustrated in Figure 1.5 Learning Disambiguation RulesIn this section, we describe the learning algorithmused in both Model-LO and Model-LC.
Althoughthere is no annotated data available for learning,10Our disambiguation model is not statistically parameter-ized, so this problem can not be resolved by any kind of pa-rameter estimation technique as in previous work on unsuper-vised tagging.Disamb.LOLearningLCDODCOpenClosedFigure 1: The order of the four models in system.we can use the unambiguous events in data toestablish the disambiguation rules and apply therules to ambiguous events.
The only difference inimplementation of the two models lies in the ?rule-extraction?, corresponding to different interpreta-tions of unambiguous events for learning open-and closed-class disambiguation models.
Afterbeing extracted from pairs of adjacent words in theinput sequence, the rules are counted and selectedusing the same algorithm in both models.5.1 Rule-extractionFor open-class words, disambiguation rules areextracted from raw data.
A pair of adjacent words(Wl,Wr) is considered unambiguous if it satis-fies the following two conditions: 1.
Wlis inthe closed class or an unambiguous type with onlypossible tag of ?VB?
; and 2. all possible tags ofWrfall in the same N/V category (Nominal or Verbalbut not mixed).
If (Wl,Wr) is unambiguous in thissense, then extract rule r = (con : cat), wherecon is Wl(for closed-class words) or ?V?
(for un-ambiguous verbal words), and cat is the N/V cat-egory of Wr.
For example, in the sequence (...hehas claimed..), the pair (he, has) is unambiguousin that he is a closed-class item and has has onlyone possible tag, ?VB?, so a rule ((he : V ) isextracted; but (has, claimed) is not usable sinceclaimed has two possible tags: ?VB?
of categoryV and ?VBN?
of category N.Disambiguation rules for closed-class words areextracted after open-class disambiguation.
A pairof adjacent words (Wl,Wr) is considered unam-biguous if it satisfies the following two conditions:1.
Wlis in the closed class and has only onepossible tag in the closed-class lexicon; 2.
Wris either disambiguated or all possible tags of Wrfall in the same N/V category.
If (Wl,Wr) is un-ambiguous in the above sense, then extract ruler = (con : cat), where con is the single tag ofWl, and cat is the N/V category of Wr.
For ex-ample, in the sequence ?...for his stepping...?, thepair (for his) is unambiguous in that for has only693one possible tag ?IN?
and both possible tags of his,?PRP?
and ?PRP$?, fall into the Nominal category,then a rule (IN : N) is extracted; but (his about)is not usable since his has more than one possi-ble tag and about has two possible tags, ?RB?
and?IN?, which are neither both ?N?
nor both ?V?.5.2 Counting and SelectingIn the counting step, a set of rules R is first initial-ized to be empty, and then, as each disambigua-tion rule r is generated while passing through thedata, if not already in R, it is added with an ini-tial count of one; otherwise, Nr, the count of r,is incremented by one.
Note that we know thatfor a rule, (con : cat), the prediction cat canonly be either N or V; then for each context con,there are two forms of rules counted, (con : N)or (con : V ).
By selecting the rule with a greatercount for each context, we guarantee that the re-sulting disambiguation model is deterministic.6 TaggingGiven our rule-based, deterministic languagemodel, tagging is a straightforward process ofdecoding the disambiguation rules.
Recall thatthere are two separate tagging models in the sys-tem, Model-DO and Model-DC for disambiguat-ing open- and closed-class respectively.The inputs to Model-DO are the open-class lex-icon, the disambiguation rules learned in Model-LO and raw data in sequence.
For each ambiguousopen-class word w in sequence if the precedingclosed-class word (if any) invokes a disambigua-tion rule, r = (con : cat), then pick the possibletag of w that falls in the category of cat (N or V),as discussed in Section 3.2.
If no rule is triggeredour default choice is ?NN?
; but if ?NN?
is not a pos-sible tag, we assume the default domain is Verbal(so the ?VB?
tag is favored).The application of disambiguation rules inModel-DC is a little more complex.
For eachambiguous closed-class word cw in sequence fol-lowed by a token of category cat, N or V, pick apossible tag of cw, con, such that (con : cat) isa rule learned in Model-LC.
If no tag is picked,a random choice is made.
While there are resid-ual cases that no functional context can help withtagging, the disambiguation model proposed herecombined with random choice results in a goodoverall performance, as shown in section 7.3.dict.
with words of count > dd 1 2 3 ?
#tag(percent lex.)
(100%) (55%) (41%) (0.6%) -BHMM2 87.3 79.6 65.0 - 17CRF/CE 90.4 77.0 71.7 - 17model-17 91.8 ... ... 90.6 17model-27 93.2 ... ... 92.1 27LDA+AC 93.4 91.2 89.7 - 17Table 3: Tagging accuracy with partial dictionaries over24k dataset; our closed-class lexicon is the closest approxi-mation to the?
column .7 ResultsOur unsupervised tagging system is com-pared to the following models As reported in(Banko&Moore, 2004), ?the quality of the lexiconmade available to unsupervised learner made thegreatest difference to tagging accuracy?.
So weonly compare our experiments to recent workbuilt over the same dataset and a full lexiconautomatically extracted from the Penn Treebank.As described in section 2.1, the closed-classlexicon, special in our experiments, is also auto-matically constructed from the WSJ corpus, andwill be used in experiments on both WSJ andBrown corpora below11.
CRF/CE (Smith&Eisner,2005) and BHMM2 (Goldwater&Griffiths, 2007)have been discussed briefly in the introduction.LDA+AC (Toutanova&Johnson, 2007) is actuallya semi-unsupervised model given the prior onp(t|w); despite this additional information, ourmodel outperforms it in experiments with partialdictionaries.
For the purpose of comparison,our experiments use the same dataset as in theseprevious work, varying in sizes from 12K to 96K.In addition to reporting on our own tagset with 27tags, we also map the results onto the 17 tags usedin other models as explained above.7.1 Unsupervised Tagging over PartialDictionariesAs shown in Table 3, reducing the dictionary byfiltering rare words (with count<= d) has not beena promising track to follow for accomplishing thetask with as little information as possible.
How-ever, by introducing a lexicon acquisition step, weachieve a tagging accuracy of 90.6% for the 24Ktest data with no prior open-class lexicon, pro-vided with only a minimal lexicon of closed-classitems (about 0.6% of the full lexicon), as high as11If we control the quality of the closed-class lexicon (butstill leave the full-lexicon untouched) by filtering out errorsin the Treebank, the performance is considerably higher.694size 12K 24k 48k 96K #tag lex.BHMM2 85.8 84.4 85.7 85.8 17 fullCRF/CE 86.2 88.6 88.4 89.4 17 fullModel-17 91.0 91.6 91.6 91.5 17 fullModel-27 93.1 93.6 93.5 93.4 27 fullmodel-17 88.9 89.3 90.2 90.4 17 closedmodel-27 90.9 91.2 92.0 92.2 27 closedTable 4: Tagging Accuracy of models trained over datasetvarying in sizes with full/closed-class lexiconthe best previous performance of 90.4 given a fulllexicon (CRF/CE with d = 1)12.One other work that investigates the use of alimited lexicon is (Haghighi&Klein, 2006), whichdevelops a prototype-drive approach to propagatethe categorical property using distributional simi-larity features; using only three exemplars of eachtag, they achieve a tagging accuracy of 80.5% us-ing a somewhat larger dataset but also the fullPenn tagset, which is much larger.7.2 Varying in sizesAs shown in Table 4, our new algorithm reducestagging error by up to 20% over the state-of-the-art given a full lexicon, from 89.4% to 91.5% overthe 96k dataset13.To better understand the learning property ofour system and to get an estimate of the vari-ance of our results above, we repeated the exper-iments above, starting with either the full lexiconor just the closed-class lexicon, with datasets vary-ing from 0.5K to 96K in size, and repeated eachexperiment 60 times on different sequences, withfour samples randomly selected from the Browncorpus, one from the training data reported aboveand the others from the WSJ corpus.
As shown inFigure 2, for the closed-class lexicon experiments,the standard deviation of tagging accuracy over thedataset of each size sharply decreases as the size ofthe data increases, as expected.
It is also clear that12Since we are facing an unsupervised task, the training setis unannotated, and hence there is no reason not to use it as thetest set as well.
For the sake of comparison, we use the samesplit of the dataset for training as previous work.
In Table 3the tagging model is trained over 96k and evaluated on 24k,but in Table 4, the tagging model is trained and evaluated overtest and training sets of the same size.13With a full lexicon, we need to disambiguate betweenopen-class tags which fall into the same N/V category, whichis beyond the ability of our disambiguation rules which pre-dict N or V only.
When more than one possible tag in thesame category predicted by the disambiguation rule, we sim-ply make a random choice.
Although not as constrained asthe acquired lexicon, a full lexicon does improve the taggingperformance, since the automatic lexicon acquisition is farfrom perfect.8889909192939495960.5k1k3k6k12k24k48k96k0.20.40.60.811.21.41.61.82Accuracy (%) (tagging all words)standard deviation of accuracy over same sizeSizeof data (k)60 samples per sizestandarddeviationtraining databrown corpusFigure 2: Standard Deviation of Tagging Accuracy withclosed-class lexicon; 60 samples for each size, randomly se-lected from both Brown and WSJ corpus.system with system withclosed-class lexicon full lexiconsub-model #errors accuracy #errors accuracyModel-DO 1089 87.3% 3546 78.9%Model-DC 1694 89.6% 1709 89.7%random 1148 44.2% 981 44.9%recall 3650 - 75 -total 7581 75.2% 6311 82.1%#ambiguous 30563 35229Table 5: The number of errors and percent ambiguous to-kens tagged correctly in the 96k dataset with 27 tags.
For ei-ther system built upon closed-class lexicon or full lexicon, thetable shows the disambiguation accuracy and number of er-rors for each sub-model in the system: Model-DO for disam-biguating open-class, Model-DC for disambiguating Closed-class and random choice.
The numbers of recall errors (goldtag not in dictionary) and total errors for each system are alsoshown.the performance of our algorithm on the Browncorpus is as strong as on the WSJ corpus.
Resultsfor the full-lexicon are similar.7.3 Error AnalysisThere are certainly cases that no functional contextcan help with tagging, since our disambiguationmodels are encoded by functional context only.Thus it is worth a closer look to how often thesystem resorts to random choice, as well as to thedisambiguation accuracy of either disambiguationmodel for open- and closed- class learned fromunannotated data.
We show the disambiguationaccuracy of ambiguous words only for each modelin Table 5, and also the number of errors due toimperfect lexicons or random choice.8 Discussion and Future WorkIn this work on unsupervised tagging, we com-bine lexicon acquisition with the learning of a695POS disambiguation model.
Moreover, the dis-ambiguation model we used is deterministic, non-lexicalized and defined over two distinct do-mains with complementary distribution (open- andclosed-class).Building a lexicon based on induced clustersrequires our morphological knowledge of threespecial endings in English: -ing, -ed and -s; onthe other hand, to reduce the feature space usedfor category induction, we utilize vectors of func-tional features only, exploiting our knowledge ofthe role of determiners and modal verbs.
How-ever, the above information is restricted to the lex-icon acquisition model.
Taking a lexicon as in-put, which either consists of a known closed-classlexicon together with an acquired open-class lexi-con or is composed by automatic extraction fromthe Penn Treebank, we need NO language-specificknowledge for learning the disambiguation model.We would like to point the reader to (Chan,2008) for more discussion on Category induc-tion14; and discussions below will concentrate onthe proposed disambiguation model.Current Chomskian theory, developed in theMinimalist Program (MP) (Chomsky, 2006), ar-gues (very roughly speaking) that the syntacticstructure of a sentence is built around a scaffold-ing provided by a set of functional elements15.Each of these provides a large tree fragment(roughly corresponding to what Chomsky calls aphase) that provide the piece parts for full utter-ances.
Chomsky observes that when these frag-ments combine, only the very edge of the frag-ments can change and that the internal structure ofthese fragments is rigid (he labels this observationthe Phase Impenetrability Condition, PIC).
Withthe belief in PIC, we propose the concept of func-tional context, in which category property can bedetermined; also we notice the distinct distributionof the elements (functional) on the edge of phaseand those (lexical) assembled within the phase.Instead of chasing the highest possible perfor-mance by using the strongest method possible, wewanted to explore how well a deterministic, non-lexicalized model, following certain linguistic in-tuitions, can approach the NLP problem.
For the14In our experiment, using the base-forms and adding acompensation process improves the coverage rate of the ac-quired lexicon from 79% to 93%.15Such as determiners (for NPs), complementizers like that(for clauses), and case assigning elements associated withtransitive verbs (for propositions).unsupervised tagging task, this simple model, withless than two hundred rules learned, even outper-forms non-deterministic generative models withten of thousands of parameters.Another motivation for our pursuit of this deter-ministic, non-lexicalized model is computationalefficiency16.
It takes less than 3 minutes total forour model to acquire the lexicon, learn the disam-biguation model, tag raw data and evaluate the out-put for a 96k dataset on a small laptop17.
And amodel using only counting and selecting is com-mon in the research field of language acquisitionand perhaps more compatible to the way humansprocess language.We are certainly aware that our work does notyet address two problems: 1).
How the systemcan be adapted to work for other languages and2) How to automatically obtain the knowledge offunctional elements.
We believe that, given theproper understanding of functional elements, oursystem will be easily adapted to other languages,but we clearly need to test this hypothesis.
Also,we are highly interested in completing our systemby incorporating the acquisition of functional el-ements.
(Chan, 2008) presents an extensive dis-cussion of his work on morphological inductionand (Mintz et al, 2002) presents interesting psy-chological experiments we can build on to acquireclosed-class words.9 AcknowledgmentsWe thank the National Science Foundation for itssupport of this work under grant IIS-0415138.
Wegreatly appreciate the comments of the anony-mous reviewers; section 7.3 is newly added andtwo more paragraphs are added to section 2.2 inresponse to their comments.
Also, we would liketo thank an anonymous reviewer of a earlier ver-sion of this paper, whose thoughtful suggestion ledto a restructuring of the current version.
We bene-fited greatly from our discussions with Dr. CharlesYang.
Noah Smith provided the data sets and de-tails of the 17 tag tagset used in previous work.Finally, we thank Constantine Lignos for his care-ful editing of earlier versions.16In some sense, the Minimalist Program was proposed toexplore the idea that the existence of Syntax is especially mo-tivated by efficient language processing.17On a Intel Core 2 Duo P8600 2.40 GHz CPU.696ReferencesMichele Banko and Robert C. Moore.
2004.
Part ofspeech tagging in context.
In COLING, 2004.Erwin Chan.
2008.
Structures and distributions inmorphological learning.
Ph.D. dissertation, Dept.of Computer and Information Science, UPenn.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proceedings of the 10th Meeting of theEACL.Chomsky, N. 2006.
Approaching UG from below.MIT.Frank, Robert.
2006.
Phase theory and Tree AdjoiningGrammar.
Lingua.Sharon Goldwater and Thomas L. Griffiths.
2007.A fully Bayesian approach to unsupervised Part-of-Speech tagging.
In Proceedings of ACL.Haghighi and D. Klein.
2006.
Prototype-driven learn-ing for sequence models.
In Proceedings of HLT-NAACL.Kroch, A. and Joshi, A. K. 1985.
Linguistic Relevanceof Tree Adjoining Grammars.
Technical Report MS-CIS-85-18, Department of Computer and Informa-tion Science, University of Pennsylvania.Charles N. Li, Sandra A. Thompson.
Mandarin Chi-nese: A Functional Reference Grammar Universityof California Press, 1989Hrafn Loftsson.
Tagging Icelandic text: A linguisticrule-based approach Nordic Journal of Linguistics(2008), 31:47-72 Cambridge University PressLeonardo Maffi.
Implementation of K-means cluster-ing in Python.http://www.fantascienza.net/leonardo/so/kmeans/kmeans.htmlMitchell P. Marcus , Mary Ann Marcinkiewicz , Beat-rice Santorini, 1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, v.19 n.2, June 1993.T.H.
Mintz, E.L. Newport and T.G.
Bever.
2002.
Thedistributional structure of grammatical categories inspeech to young children.
Cognitive Science 26(2002), pp.
393C424.Hinrich Sch?utze.
1993.
Part-of-speech induction fromscratch.
In Proceedings of the 31st Meeting of theACL.Noah A. Smith.
Novel Estimation Methods for Un-supervised Discovery of Latent Structure in NaturalLanguage Text.
Ph.D. thesis, Johns Hopkins Uni-versity Department of Computer Science, Baltimore,MD, October 2006.L Shen, G Satta and A Joshi.
2007.
Guided Learningfor Bidirectional Sequence Classification In Pro-ceedings of ACL.Noah Smith and Jason Eisner.
2005.
Contrastive es-timation: Training log-linear models on unlabeleddata.
In Proceedings of the 43rdMeeting of the ACL.Kristina Toutanova and Mark Johnson.
2007.
ABayesian LDA-based model for semi-supervisedpart-of-speech tagging.
In NIPS2007.Charles Yang.
2002.
Knowledge and learning in natu-ral language.
Oxford University Press.
(Chapter 3).697
