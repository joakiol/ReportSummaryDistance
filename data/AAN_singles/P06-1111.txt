Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 881?888,Sydney, July 2006. c?2006 Association for Computational LinguisticsPrototype-Driven Grammar InductionAria HaghighiComputer Science DivisionUniversity of California Berkeleyaria42@cs.berkeley.eduDan KleinComputer Science DivisionUniversity of California Berkeleyklein@cs.berkeley.eduAbstractWe investigate prototype-driven learning for pri-marily unsupervised grammar induction.
Priorknowledge is specified declaratively, by providing afew canonical examples of each target phrase type.This sparse prototype information is then propa-gated across a corpus using distributional similar-ity features, which augment an otherwise standardPCFG model.
We show that distributional featuresare effective at distinguishing bracket labels, but notdetermining bracket locations.
To improve the qual-ity of the induced trees, we combine our PCFG in-duction with the CCM model of Klein and Manning(2002), which has complementary stengths: it iden-tifies brackets but does not label them.
Using onlya handful of prototypes, we show substantial im-provements over naive PCFG induction for Englishand Chinese grammar induction.1 IntroductionThere has been a great deal of work on unsuper-vised grammar induction, with motivations rang-ing from scientific interest in language acquisi-tion to engineering interest in parser construc-tion (Carroll and Charniak, 1992; Clark, 2001).Recent work has successfully induced unlabeledgrammatical structure, but has not successfullylearned labeled tree structure (Klein and Manning,2002; Klein and Manning, 2004; Smith and Eis-ner, 2004) .In this paper, our goal is to build a system capa-ble of producing labeled parses in a target gram-mar with as little total effort as possible.
We in-vestigate a prototype-driven approach to grammarinduction, in which one supplies canonical ex-amples of each target concept.
For example, wemight specify that we are interested in trees whichuse the symbol NP and then list several examplesof prototypical NPs (determiner noun, pronouns,etc., see figure 1 for a sample prototype list).
Thisprototype information is similar to specifying anannotation scheme, which even human annotatorsmust be provided before they can begin the con-struction of a treebank.
In principle, prototype-driven learning is just a kind of semi-supervisedlearning.
However, in practice, the information weprovide is on the order of dozens of total seed in-stances, instead of a handful of fully parsed trees,and is of a different nature.The prototype-driven approach has threestrengths.
First, since we provide a set of targetsymbols, we can evaluate induced trees usingstandard labeled parsing metrics, rather than thefar more forgiving unlabeled metrics described in,for example, Klein and Manning (2004).
Second,knowledge is declaratively specified in an inter-pretable way (see figure 1).
If a user of the systemis unhappy with its systematic behavior, they canalter it by altering the prototype information (seesection 7.1 for examples).
Third, and related tothe first two, one does not confuse the ability ofthe system to learn a consistent grammar with itsability to learn the grammar a user has in mind.In this paper, we present a series of experimentsin the induction of labeled context-free trees us-ing a combination of unlabeled data and sparseprototypes.
We first affirm the well-known re-sult that simple, unconstrained PCFG inductionproduces grammars of poor quality as measuredagainst treebank structures.
We then augment aPCFGwith prototype features, and show that thesefeatures, when propagated to non-prototype se-quences using distributional similarity, are effec-tive at learning bracket labels on fixed unlabeledtrees, but are still not enough to learn good treestructures without bracketing information.
Finally,we intersect the feature-augmented PCFGwith theCCM model of Klein and Manning (2002), a high-quality bracketing model.
The intersected modelis able to learn trees with higher unlabeled F1 thanthose in Klein and Manning (2004).
More impor-881tantly, its trees are labeled and can be evaluatedaccording to labeled metrics.
Against the EnglishPenn Treebank, our final trees achieve a labeled F1of 65.1 on short sentences, a 51.7% error reductionover naive PCFG induction.2 Experimental SetupThe majority of our experiments induced treestructures from the WSJ section of the EnglishPenn treebank (Marcus et al, 1994), though seesection 7.4 for an experiment on Chinese.
To fa-cilitate comparison with previous work, we ex-tracted WSJ-10, the 7,422 sentences which con-tain 10 or fewer words after the removal of punc-tuation and null elements according to the schemedetailed in Klein (2005).
We learned models on allor part of this data and compared their predictionsto the manually annotated treebank trees for thesentences on which the model was trained.
As inprevious work, we begin with the part-of-speech(POS) tag sequences for each sentence rather thanlexical sequences (Carroll and Charniak, 1992;Klein and Manning, 2002).Following Klein and Manning (2004), we reportunlabeled bracket precision, recall, and F1.
Notethat according to their metric, brackets of size 1are omitted from the evaluation.
Unlike that work,all of our induction methods produce trees labeledwith symbols which are identified with treebankcategories.
Therefore, we also report labeled pre-cision, recall, and F1, still ignoring brackets ofsize 1.13 Experiments in PCFG inductionAs an initial experiment, we used the inside-outside algorithm to induce a PCFG in thestraightforward way (Lari and Young, 1990; Man-ning and Schu?tze, 1999).
For all the experimentsin this paper, we considered binary PCFGs overthe nonterminals and terminals occuring in WSJ-10.
The PCFG rules were of the following forms:?
X ?
Y Z, for nonterminal types X,Y, andZ, with Y 6= X or Z 6= X?
X ?
t Y , X ?
Y t, for each terminal t?
X ?
t t?, for terminals t and t?For a given sentence S, our CFG generates la-beled trees T over S.2 Each tree consists of binary1In cases where multiple gold labels exist in the gold trees,precision and recall were calculated as in Collins (1999).2Restricting our CFG to a binary branching grammar re-sults in an upper bound of 88.1% on unlabeled F1.productions X(i, j) ?
?
over constituent spans(i, j), where ?
is a pair of non-terminal and/orterminal symbols in the grammar.
The generativeprobability of a tree T for S is:PCFG(T, S) =?X(i,j)??
?TP (?|X)In the inside-outside algorithm, we iterativelycompute posterior expectations over productionoccurences at each training span, then use thoseexpectations to re-estimate production probabili-ties.
This process is guaranteed to converge to alocal extremum of the data likelihood, but initialproduction probability estimates greatly influencethe final grammar (Carroll and Charniak, 1992).
Inparticular, uniform initial estimates are an (unsta-ble) fixed point.
The classic approach is to add asmall amount of random noise to the initial prob-abilities in order to break the symmetry betweengrammar symbols.We randomly initialized 5 grammars using tree-bank non-terminals and trained each to conver-gence on the first 2000 sentences of WSJ-10.Viterbi parses were extracted for each of these2000 sentences according to each grammar.
Ofcourse, the parses?
symbols have nothing to anchorthem to our intended treebank symbols.
That is, anNP in one of these grammars may correspond tothe target symbol VP, or may not correspond wellto any target symbol.
To evaluate these learnedgrammars, we must map the models?
phrase typesto target phrase types.
For each grammar, we fol-lowed the common approach of greedily mappingmodel symbols to target symbols in the way whichmaximizes the labeled F1.
Note that this can, anddoes, result in mapping multiple model symbolsto the most frequent target symbols.
This experi-ment, labeled PCFG?
NONE in figure 4, resulted inan average labeled F1 of 26.3 and an unlabeled F1of 45.7.
The unlabeled F1 is better than randomlychoosing a tree (34.7), but not better than alwayschoosing a right branching structure (61.7).Klein and Manning (2002) suggest that the taskof labeling constituents is significantly easier thanidentifying them.
Perhaps it is too much to aska PCFG induction algorithm to perform both ofthese tasks simultaneously.
Along the lines ofPereira and Schabes (1992), we reran the inside-outside algorithm, but this time placed zero masson all trees which did not respect the bracketingof the gold trees.
This constraint does not fully882Phrase Prototypes Phrase PrototypesNP DT NN VP VBN IN NNJJ NNS VBD DT NNNNP NNP MD VB CDS PRP VBD DT NN QP CD CDDT NN VBD IN DT NN RB CDDT VBZ DT JJ NN DT CD CDPP IN NN ADJP RB JJTO CD CD JJIN PRP JJ CC JJADVP RB RBRB CDRB CC RBVP-INF VB NN NP-INF NN POSFigure 1: English phrase type prototype list man-ually specified (The entire supervision for our sys-tem).
The second part of the table is additionalprototypes discussed in section 7.1.eliminate the structural uncertainty since we areinducing binary trees and the gold trees are flat-ter than binary in many cases.
This approach ofcourse achieved the upper bound on unlabeled F1,because of the gold bracket constraints.
However,it only resulted in an average labeled F1 of 52.6(experiment PCFG ?
GOLD in figure 4).
While thislabeled score is an improvement over the PCFG ?NONE experiment, it is still relatively disappoint-ing.3.1 Encoding Prior Knowledge withPrototypesClearly, we need to do something more thanadding structural bias (e.g.
bracketing informa-tion) if we are to learn a PCFG in which the sym-bols have the meaning and behaviour we intend.How might we encode information about our priorknowledge or intentions?Providing labeled trees is clearly an option.
Thisapproach tells the learner how symbols should re-cursively relate to each other.
Another option is toprovide fully linearized yields as prototypes.
Wetake this approach here, manually creating a listof POS sequences typical of the 7 most frequentcategories in the Penn Treebank (see figure 1).3Our grammar is limited to these 7 phrase typesplus an additional type which has no prototypesand is unconstrained.4 This list grounds each sym-3A possible objection to this approach is the introductionof improper reasearcher bias via specifying prototypes.
Seesection 7.3 for an experiment utilizing an automatically gen-erated prototype list with comparable results.4In our experiments we found that adding prototypes formore categories did not improve performance and took morebol in terms of an observable portion of the data,rather than attempting to relate unknown symbolsto other unknown symbols.Broadly, we would like to learn a grammarwhich explains the observed data (EM?s objec-tive) but also meets our prior expectations or re-quirements of the target grammar.
How mightwe use such a list to constrain the learning ofa PCFG with the inside-outside algorithm?
Wemight require that all occurences of a prototypesequence, say DT NN, be constituents of the cor-responding type (NP).
However, human-elicitedprototypes are not likely to have the property that,when they occur, they are (nearly) always con-stituents.
For example, DT NN is a perfectly rea-sonable example of a noun phrase, but is not a con-stituent when it is part of a longer DT NN NN con-stituent.
Therefore, when summing over trees withthe inside-outside algorithm, we could require aweaker property: whenever a prototype sequenceis a constituent it must be given the label specifiedin the prototype file.5 This constraint is enough tobreak the symmetry between the model labels, andtherefore requires neither random initialization fortraining, nor post-hoc mapping of labels for eval-uation.
Adding prototypes in this way and keep-ing the gold bracket constraint gave 59.9 labeledF1.
The labeled F1 measure is again an improve-ment over naive PCFG induction, but is perhapsless than we might expect given that the model hasbeen given bracketing information and has proto-types as a form of supervision to direct it.In response to a prototype, however, we maywish to conclude something stronger than a con-straint on that particular POS sequence.
We mighthope that sequences which are similar to a proto-type in some sense are generally given the samelabel as that prototype.
For example, DT NN is anoun phrase prototype, the sequence DT JJ NN isanother good candidate for being a noun phrase.This kind of propagation of constraints requiresthat we have a good way of defining and detect-ing similarity between POS sequences.3.2 Phrasal Distributional SimilarityA central linguistic argument for constituent typesis substitutability: phrases of the same type appeartime.
We note that we still evaluate against all phrase typesregardless of whether or not they are modeled by our gram-mar.5Even this property is likely too strong: prototypes mayhave multiple possible labels, for example DT NN may alsobe a QP in the English treebank.883Yield Prototype Skew KL Phrase Type Skew KLDT JJ NN DT NN 0.10 NP 0.39IN DT VBG NN IN NN 0.24 PP 0.45DT NN MD VB DT NNS PRP VBD DT NN 0.54 S 0.58CC NN IN NN 0.43 PP 0.71MD NNS PRP VBD DT NN 1.43 NONE -Figure 2: Yields along with most similar proto-types and phrase types, guessed according to (3).in similar contexts and are mutually substitutable(Harris, 1954; Radford, 1988).
For instance, DTJJ NN and DT NN occur in similar contexts, andare indeed both common NPs.
This idea has beenrepeatedly and successfully operationalized usingvarious kinds of distributional clustering, wherewe define a similarity measure between two itemson the basis of their immediate left and right con-texts (Schu?tze, 1995; Clark, 2000; Klein and Man-ning, 2002).As in Clark (2001), we characterize the distribu-tion of a sequence by the distribution of POS tagsoccurring to the left and right of that sequence ina corpus.
Each occurence of a POS sequence ?falls in a context x ?
y, where x and y are the ad-jacent tags.
The distribution over contexts x ?
yfor a given ?
is called its signature, and is denotedby ?(?).
Note that ?(?)
is composed of contextcounts from all occurences, constitiuent and dis-tituent, of ?.
Let ?c(?)
denote the context dis-tribution for ?
where the context counts are takenonly from constituent occurences of ?.
For eachphrase type in our grammar,X , define ?c(X) to bethe context distribution obtained from the countsof all constituent occurences of type X:?c(X) = Ep(?|X) ?c(?)
(1)where p(?|X) is the distribution of yield types forphrase type X .
We compare context distributionsusing the skewed KL divergence:DSKL(p, q) = DKL(p?
?p + (1?
?
)q)where ?
controls how much of the source distribu-tions is mixed in with the target distribution.A reasonable baseline rule for classifying thephrase type of a POS yield is to assign it to thephrase from which it has minimal divergence:type(?)
= argminXDSKL(?c(?
), ?c(X)) (2)However, this rule is not always accurate, and,moreover, we do not have access to ?c(?)
or?c(X).
We chose to approximate ?c(X) us-ing the prototype yields for X as samples fromp(?|X).
Letting proto(X) denote the (few) pro-totype yields for phrase type X , we define ??(X):??
(X) =1|proto(X)|???proto(X)?(?
)Note ??
(X) is an approximation to (1) in sev-eral ways.
We have replaced an expectation overp(?|X) with a uniform weighting of proto(X),and we have replaced ?c(?)
with ?(?)
for eachterm in that expectation.
Because of this, we willrely only on high confidence guesses, and allowyields to be given a NONE type if their divergencefrom each ??
(X) exceeds a fixed threshold t. Thisgives the following alternative to (2):type(?)
= (3){NONE, if minX DSKL(?(?
), ??
(X)) < targminX DSKL(?(?
), ??
(X)), otherwiseWe built a distributional model implementingthe rule in (3) by constructing ?(?)
from contextcounts in the WSJ portion of the Penn Treebankas well as the BLIPP corpus.
Each ??
(X) was ap-proximated by a uniform mixture of ?(?)
for eachof X?s prototypes ?
listed in figure 1.This method of classifying constituents is veryprecise if the threshold is chosen conservativelyenough.
For instance, using a threshold of t =0.75 and ?
= 0.1, this rule correctly classifies themajority label of a constituent-type with 83% pre-cision, and has a recall of 23% over constituenttypes.
Figure 2 illustrates some sample yields, theprototype sequence to which it is least divergent,and the output of rule (3).We incorporated this distributional informationinto our PCFG induction scheme by adding a pro-totype feature over each span (i, j) indicating theoutput of (3) for the yield ?
in that span.
Asso-ciated with each sentence S is a feature map Fspecifying, for each (i, j), a prototype feature pij .These features are generated using an augmentedCFG model, CFG+, given by:6PCFG+(T, F ) =?X(i,j)??
?TP (pij |X)P (?|X)=?X(i,j)??
?T?CFG+(X ?
?, pij)6Technically, all features in F must be generated for eachassignment to T , which means that there should be terms inthis equation for the prototype features on distituent spans.However, we fixed the prototype distribution to be uniformfor distituent spans so that the equation is correct up to a con-stant depending on F .884P (S|ROOT) ?
ROOTS?
P (NP VP|S)P (P = NONE|S)XXXXXP (NN NNS|NP)P (P = NP|NP)ffNP HHHNNNpayrollsNNFactoryVP?
P (VBD PP|VP)P (P = VP|VP)aaa!!!VBDfellPP?
P (IN NN|PP)P (P = PP|PP)!!!
aaaNNNovemberINinFigure 3: Illustration of PCFG augmented withprototype similarity features.where ?CFG+(X ?
?, pij) is the local factor forplacing X ?
?
on a span with prototype featurepij .
An example is given in figure 3.For our experiments, we fixed P (pij |X) to be:P (pij |X) ={0.60, if pij = Xuniform, otherwiseModifying the model in this way, and keeping thegold bracketing information, gave 71.1 labeled F1(see experiment PROTO ?
GOLD in figure 4), a40.3% error reduction over naive PCFG inductionin the presence of gold bracketing information.We note that the our labeled F1 is upper-boundedby 86.0 due to unary chains and more-than-binaryconfigurations in the treebank that cannot be ob-tained from our binary grammar.We conclude that in the presence of gold bracketinformation, we can achieve high labeled accu-racy by using a CFG augmented with distribu-tional prototype features.4 Constituent Context ModelSo far, we have shown that, given perfect per-fect bracketing information, distributional proto-type features allow us to learn tree structures withfairly accurate labels.
However, such bracketinginformation is not available in the unsupervisedcase.Perhaps we don?t actually need bracketing con-straints in the presence of prototypes and distri-butional similarity features.
However this exper-iment, labeled PROTO ?
NONE in figure 4, gaveonly 53.1 labeled F1 (61.1 unlabeled), suggestingthat some amount of bracketing constraint is nec-essary to achieve high performance.Fortunately, there are unsupervised systemswhich can induce unlabeled bracketings with rea-sonably high accuracy.
One such model isthe constituent-context model (CCM) of Kleinand Manning (2002), a generative distributionalmodel.
For a given sentence S, the CCM generatesa bracket matrix, B, which for each span (i, j), in-dicates whether or not it is a constituent (Bij = c)or a distituent (Bij = d).
In addition, it generatesa feature map F ?, which for each span (i, j) in Sspecifies a pair of features, F ?ij = (yij , cij), whereyij is the POS yield of the span, and cij is the con-text of the span, i.e identity of the conjoined leftand right POS tags:PCCM (B,F?)
= P (B)?
(i,j)P (yij |Bij)P (cij |Bij)The distribution P (B) only places mass on brack-etings which correspond to binary trees.
Wecan efficiently compute PCCM (B,F ?)
(up toa constant) depending on F ?
using local fac-tors ?CCM (yij , cij) which decomposes over con-stituent spans:7PCCM (B,F?)
??
(i,j):Bij=cP (yij |c)P (cij |c)P (yij |d)P (cij |d)=?
(i,j):Bij=c?CCM (yij , cij)The CCM by itself yields an unlabeled F1 of 71.9on WSJ-10, which is reasonably high, but does notproduce labeled trees.5 Intersecting CCM and PCFGThe CCM and PCFG models provide complemen-tary views of syntactic structure.
The CCM explic-itly learns the non-recursive contextual and yieldproperties of constituents and distituents.
ThePCFG model, on the other hand, does not explic-itly model properties of distituents but instead fo-cuses on modeling the hierarchical and recursiveproperties of natural language syntax.
One wouldhope that modeling both of these aspects simulta-neously would improve the overall quality of ourinduced grammar.We therefore combine the CCM with our feature-augmented PCFG, denoted by PROTO in exper-iment names.
When we run EM on either ofthe models alone, at each iteration and for eachtraining example, we calculate posteriors over that7Klein (2005) gives a full presentation.885model?s latent variables.
For CCM, the latent vari-able is a bracketing matrix B (equivalent to an un-labeled binary tree), while for the CFG+ the latentvariable is a labeled tree T .
While these latentvariables aren?t exactly the same, there is a closerelationship between them.
A bracketing matrixconstrains possible labeled trees, and a given la-beled tree determines a bracketing matrix.
Oneway to combine these models is to encourage bothmodels to prefer latent variables which are com-patible with each other.Similar to the approach of Klein and Manning(2004) on a different model pair, we intersect CCMand CFG+ by multiplying their scores for any la-beled tree.
For each possible labeled tree over asentence S, our generative model for a labeled treeT is given as follows:P (T, F, F ?)
= (4)PCFG+(T, F )PCCM (B(T ), F?
)where B(T ) corresponds to the bracketing ma-trix determined by T .
The EM algorithm for theproduct model will maximize:P (S,F, F ?)
=?T?T (S)PCCM (B,F?
)PCFG+(T, F )=?BPCCM (B,F?
)?T?T (B,S)PCFG+(T, F )where T (S) is the set of labeled trees consistentwith the sentence S and T (B,S) is the set of la-beled trees consistent with the bracketing matrixB and the sentence S. Notice that this quantity in-creases as the CCM and CFG+ models place proba-bility mass on compatible latent structures, givingan intuitive justification for the success of this ap-proach.We can compute posterior expectations over(B, T ) in the combined model (4) using a variantof the inside-outside algorithm.
The local factorfor a binary rule r = X ?
Y Z, over span (i, j),with CCM features F ?ij = (yij , cij) and prototypefeature pij , is given by the product of local factorsfor the CCM and CFG+ models:?
(r, (i, j)) = ?CCM (yij , cij)?CFG+(r, pij)From these local factors, the inside-outside al-gorithm produces expected counts for each binaryrule, r, over each span (i, j) and split point k, de-noted by P (r, (i, j), k|S, F, F ?).
These posteriorsare sufficient to re-estimate all of our model pa-rameters.Labeled UnlabeledSetting Prec.
Rec.
F1 Prec.
Rec.
F1No BracketsPCFG ?
NONE 23.9 29.1 26.3 40.7 52.1 45.7PROTO ?
NONE 51.8 62.9 56.8 59.6 76.2 66.9Gold BracketsPCFG ?
GOLD 47.0 57.2 51.6 78.8 100.0 88.1PROTO ?
GOLD 64.8 78.7 71.1 78.8 100.0 88.1CCM BracketsCCM - - - 64.2 81.6 71.9PCFG ?
CCM 32.3 38.9 35.3 64.1 81.4 71.8PROTO ?
CCM 56.9 68.5 62.2 68.4 86.9 76.5BEST 59.4 72.1 65.1 69.7 89.1 78.2UBOUND 78.8 94.7 86.0 78.8 100.0 88.1Figure 4: English grammar induction results.
Theupper bound on labeled recall is due to unarychains.6 CCM as a BracketerWe tested the product model described in sec-tion 5 on WSJ-10 under the same conditions asin section 3.
Our initial experiment utilizes noprotoype information, random initialization, andgreedy remapping of its labels.
This experiment,PCFG ?
CCM in figure 4, gave 35.3 labeled F1,compared to the 51.6 labeled F1 with gold brack-eting information (PCFG ?
GOLD in figure 4).Next we added the manually specified proto-types in figure 1, and constrained the model to givethese yields their labels if chosen as constituents.This experiment gave 48.9 labeled F1 (73.3 unla-beled).
The error reduction is 21.0% labeled (5.3%unlabeled) over PCFG ?
CCM.We then experimented with adding distributionalprototype features as discussed in section 3.2 us-ing a threshold of 0.75 and ?
= 0.1.
This experi-ment, PROTO ?
CCM in figure 4, gave 62.2 labeledF1 (76.5 unlabeled).
The error reduction is 26.0%labeled (12.0% unlabeled) over the experiment us-ing prototypes without the similarity features.
Theoverall error reduction from PCFG?
CCM is 41.6%(16.7%) in labeled (unlabeled) F1.7 Error AnalysisThe most common type of error by our PROTO ?CCM system was due to the binary grammar re-striction.
For instance common NPs, such as DT JJNN, analyzed as [NP DT [NP JJ NN] ], which pro-poses additional N constituents compared to theflatter treebank analysis.
This discrepancy greatly,and perhaps unfairly, damages NP precision (seefigure 6).
However, this is error is unavoidable886SXXXXXNPNNPFranceVPXXXXXMDcanVPhhhhhhh(((((((VBboastNPXXXXXNPaaa!!!NPaaa!!!DTtheNNlionPOS?sNNsharePPPPPPINofNPHHHJJhigh-pricedNNSbottlesShhhhhhhhh(((((((((NNPFranceVPhhhhhhhh((((((((VPXXXXXVPZZMDcanVBboastNPaaa!!
!NPll,,DTtheNNlionPPZZPOS?sNNsharePPPPPPINofNPHHHJJhigh-pricedNNSbottlesShhhhhhhh((((((((NNPFranceVPhhhhhhhhh(((((((((VPPPPPMDcanVPXXXXXVBboastNPPPPPNPbb""DTtheNPcc##NNlionPOS?sNNsharePPPPPPINofNPHHHJJhigh-pricedNNSbottlesa) b) c)Figure 5: Examples of corrections from adding VP-INF and NP-POS prototype categories.
The tree in (a)is the Treebank parse, (b) is the parse with PROTO ?
CCM model, and c) is the parse with the BEST model(added prototype categories), which fixes the possesive NP and infinitival VP problems, but not the PPattachment.given our grammar restriction.Figure 5(b) demonstrates three other errors.
Pos-sessive NPs are analyzed as [NP NN [PP POS NN ]], with the POS element treated as a prepositionand the possessed NP as its complement.
Whilelabeling the POS NN as a PP is clearly incorrect,placing a constituent over these elements is notunreasonable and in fact has been proposed bysome linguists (Abney, 1987).
Another type oferror also reported by Klein and Manning (2002)is MD VB groupings in infinitival VPs also some-times argued by linguists (Halliday, 2004).
Moreseriously, prepositional phrases are almost alwaysattached ?high?
to the verb for longer NPs.7.1 Augmenting PrototypesOne of the advantages of the prototype driven ap-proach, over a fully unsupervised approach, is theability to refine or add to the annotation specifica-tion if we are not happy with the output of our sys-tem.
We demonstrate this flexibility by augment-ing the prototypes in figure 1 with two new cate-gories NP-POS and VP-INF, meant to model pos-sessive noun phrases and infinitival verb phrases,which tend to have slightly different distributionalproperties from normal NPs and VPs.
These newsub-categories are used during training and thenstripped in post-processing.
This prototype listgave 65.1 labeled F1 (78.2 unlabeled).
This exper-iment is labeled BEST in figure 4.
Looking at theCFG-learned rules in figure 7, we see that the basicstructure of the treebank grammar is captured.7.2 Parsing with only the PCFGIn order to judge how well the PCFG componentof our model did in isolation, we experimentedwith training our BEST model with the CCM com-ponent, but dropping it at test time.
This experi-Label Prec.
Rec.
F1S 79.3 80.0 79.7NP 49.0 74.4 59.1VP 80.4 73.3 76.7PP 45.6 78.6 57.8QP 36.2 78.8 49.6ADJP 29.4 33.3 31.2ADVP 25.0 12.2 16.4Figure 6: Precision, recall, and F1 for individualphrase types in the BEST modelRule Probability Rule ProbabilityS ?
NP VP 0.51 VP ?
VBZ NP 0.20S ?
PRP VP 0.13 VP ?
VBD NP 0.15S ?
NNP VP 0.06 VP ?
VBP NP 0.09S ?
NNS VP 0.05 VP ?
VB NP 0.08NP ?
DT NN 0.12 ROOT ?
S 0.95NP ?
NP PP 0.09 ROOT ?
NP 0.05NP ?
NNP NNP 0.09NP ?
JJ NN 0.07PP ?
IN NP 0.37 QP ?
CD CD 0.35PP ?
CC NP 0.06 QP ?
CD NN 0.30PP ?
TO VP 0.05 QP ?
QP PP 0.10PP ?
TO QP 0.04 QP ?
QP NNS 0.05ADJP ?
RB VBN 0.37 ADVP ?
RB RB 0.25ADJP ?
RB JJ 0.31 ADVP ?
ADJP PRP 0.15ADJP ?
RBR JJ 0.09 ADVP ?
RB CD 0.10Figure 7: Top PCFG Rules learned by BEST modelment gave 65.1 labeled F1 (76.8 unlabeled).
Thisdemonstrates that while our PCFG performancedegrades without the CCM, it can be used on itsown with reasonable accuracy.7.3 Automatically Generated PrototypesThere are two types of bias which enter into thecreation of prototypes lists.
One of them is thebias to choose examples which reflect the annota-tion semantics we wish our model to have.
Thesecond is the iterative change of prototypes in or-der to maximize F1.
Whereas the first is appro-887priate, indeed the point, the latter is not.
In or-der to guard against the second type of bias, weexperimented with automatically extracted gener-ated prototype lists which would not be possiblewithout labeled data.
For each phrase type cat-egory, we extracted the three most common yieldassociated with that category that differed in eitherfirst or last POS tag.
Repeating our PROTO ?
CCMexperiment with this list yielded 60.9 labeled F1(76.5 unlabeled), comparable to the performanceof our manual prototype list.7.4 Chinese Grammar InductionIn order to demonstrate that our system is some-what language independent, we tested our modelon CTB-10, the 2,437 sentences of the ChineseTreebank (Ircs, 2002) of length at most 10 af-ter punctuation is stripped.
Since the authorshave no expertise in Chinese, we automatically ex-tracted prototypes in the same way described insection 7.3.
Since we did not have access to a largeauxiliary POS tagged Chinese corpus, our distri-butional model was built only from the treebanktext, and the distributional similarities are presum-ably degraded relative to the English.
Our PCFG?
CCM experiment gave 18.0 labeled F1 (43.4 un-labeled).
The PROTO ?
CCM model gave 39.0 la-beled F1 (53.2 unlabeled).
Presumably with ac-cess to more POS tagged data, and the expertise ofa Chinese speaker, our system would see increasedperformance.
It is worth noting that our unlabeledF1 of 53.2 is the best reported from a primarilyunsupervised system, with the next highest figurebeing 46.7 reported by Klein and Manning (2004).8 ConclusionWe have shown that distributional prototype fea-tures can allow one to specify a target labelingscheme in a compact and declarative way.
Thesefeatures give substantial error reduction in labeledF1 measure for English and Chinese grammar in-duction.
They also achieve the best reported un-labeled F1 measure.8 Another positive propertyof this approach is that it tries to reconcile thesuccess of distributional clustering approaches togrammar induction (Clark, 2001; Klein and Man-ning, 2002), with the CFG tree models in the su-pervised literature (Collins, 1999).
Most impor-tantly, this is the first work, to the authors?
knowl-8The next highest results being 77.1 and 46.7 for Englishand Chinese respectively from Klein and Manning (2004).edge, which has learned CFGs in an unsupervisedor semi-supervised setting and can parse naturallanguage language text with any reasonable accu-racy.Acknowledgments We would like to thank theanonymous reviewers for their comments.
Thiswork is supported by a Microsoft / CITRIS grantand by an equipment donation from Intel.ReferencesStephen P. Abney.
1987.
The English Noun Phrase in itsSentential Aspect.
Ph.D. thesis, MIT.Glenn Carroll and Eugene Charniak.
1992.
Two experimentson learning probabilistic dependency grammars from cor-pora.
Technical Report CS-92-16.Alexander Clark.
2000.
Inducing syntactic categories by con-text distribution clustering.
In CoNLL, pages 91?94, Lis-bon, Portugal.Alexander Clark.
2001.
The unsupervised induction ofstochastic context-free grammars using distributional clus-tering.
In CoNLL.Michael Collins.
1999.
The Unsupervised learning of NaturalLanguage Structure.
Ph.D. thesis, University of Rochester.M.A.K Halliday.
2004.
An introduction to functional gram-mar.
Edward Arnold, 2nd edition.Zellig Harris.
1954.
Distributional Structure.
University ofChicago Press, Chicago.Nianwen Xue Ircs.
2002.
Building a large-scale annotatedchinese corpus.Dan Klein and Christopher Manning.
2002.
A generativeconstituent-context model for improved grammar induc-tion.
In ACL.Dan Klein and Christopher Manning.
2004.
Corpus-basedinduction of syntactic structure: Models of dependency andconstituency.
In ACL.Dan Klein.
2005.
The unsupervised learning of Natural Lan-guage Structure.
Ph.D. thesis, Stanford University.Karim Lari and Steve Young.
1990.
The estimation ofstochastic context-free grammars using the insideoutsidealgorithm.
Computer Speech and Language, 2(4):35?56.Christopher D. Manning and Hinrich Schu?tze.
1999.
Foun-dations of Statistical Natural Language Processing.
TheMIT Press.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated corpusof english: The penn treebank.
Computational Linguistics,19(2):313?330.Fernando C. N. Pereira and Yves Schabes.
1992.
Inside-outside reestimation from partially bracketed corpora.
InMeeting of the Association for Computational Linguistics,pages 128?135.Andrew Radford.
1988.
Transformational Grammar.
Cam-bridge University Press, Cambridge.Hinrich Schu?tze.
1995.
Distributional part-of-speech tagging.In EACL.Noah A. Smith and Jason Eisner.
2004.
Guiding unsuper-vised grammar induction using contrastive estimation.
InWorking notes of the IJCAI workshop on Grammatical In-ference Applications.888
