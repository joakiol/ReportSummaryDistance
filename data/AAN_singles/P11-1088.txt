Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 875?884,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsUsing Deep Morphology to Improve Automatic Error Detectionin Arabic Handwriting RecognitionNizar Habash and Ryan M. RothCenter for Computational Learning SystemsColumbia University{habash,ryanr}@ccls.columbia.eduAbstractArabic handwriting recognition (HR) is achallenging problem due to Arabic?s con-nected letter forms, consonantal diacritics andrich morphology.
In this paper we isolatethe task of identification of erroneous wordsin HR from the task of producing correctionsfor these words.
We consider a variety oflinguistic (morphological and syntactic) andnon-linguistic features to automatically iden-tify these errors.
Our best approach achievesa roughly ?15% absolute increase in F-scoreover a simple but reasonable baseline.
A de-tailed error analysis shows that linguistic fea-tures, such as lemma (i.e., citation form) mod-els, help improve HR-error detection preciselywhere we expect them to: semantically inco-herent error words.1 IntroductionAfter years of development, optical character recog-nition (OCR) for Latin-character languages, such asEnglish, has been refined greatly.
Arabic, however,possesses a complex orthography and morphologythat makes OCR more difficult (M?rgner and Abed,2009; Halima and Alimi, 2009; Magdy and Dar-wish, 2006).
Because of this, only a few systemsfor Arabic OCR of printed text have been devel-oped, and these have not been thoroughly evalu-ated (M?rgner and Abed, 2009).
OCR of Arabichandwritten text (handwriting recognition, or HR),whether online or offline, is even more challengingcompared to printed Arabic OCR, where the unifor-mity of letter shapes and other factors allow for eas-ier recognition (Biadsy et al, 2006; Natarajan et al,2008; Saleem et al, 2009).OCR and HR systems are often improved by per-forming post-processing; these are attempts to eval-uate whether each word, phrase or sentence in theOCR/HR output is legal and/or probable.
When anillegal word or phrase is discovered (error detec-tion), these systems usually attempt to generate a le-gal alternative (error correction).
In this paper, wepresent a HR error detection system that uses deeplexical and morphological feature models to locatepossible "problem zones" ?
words or phrases thatare likely incorrect ?
in Arabic HR output.
We usean off-the-shelf HR system (Natarajan et al, 2008;Saleem et al, 2009) to generate an N-best list of hy-potheses for each of several scanned segments ofArabic handwriting.
Our problem zone detection(PZD) system then tags the potentially erroneous(problem) words.
A subsequent HR post-processingsystem can then focus its effort on these words whengenerating additional alternative hypotheses.
Weonly discuss the PZD system and not the task ofnew hypothesis generation; the evaluation is on er-ror/problem identification.
PZD can also be useful inhighlighting erroneous text for human post-editors.This paper is structured as follows: Section 2 pro-vides background on the difficulties of the ArabicHR task.
Section 3 presents an analysis of HR er-rors and defines what is considered a problem zoneto be tagged.
The experimental features, data andother variables are outlined in Section 4.
The exper-iments are presented and discussed in Section 5.
Wediscuss and compare to some related work in detailin Section 6.
Conclusions and suggested avenues offor future progress are presented in Section 7.2 Arabic Handwriting RecognitionChallengesArabic has several orthographic and morphologicalproperties that make HR challenging (Darwish andOard, 2002; Magdy and Darwish, 2006; M?rgnerand Abed, 2009).8752.1 Arabic Orthography ChallengesThe use of cursive, connected script creates prob-lems in that it becomes more difficult for a machineto distinguish between individual characters.
This iscertainly not a property unique to Arabic; methodsdeveloped for other cursive script languages (suchas Hidden Markov Models) can be applied success-fully to Arabic (Natarajan et al, 2008; Saleem et al,2009; M?rgner and Abed, 2009; Lu et al, 1999).Arabic writers often make use of elongation(tatweel/kashida) to beautify the script.
Arabic alsocontains certain ligature constructions that requireconsideration during OCR/HR (Darwish and Oard,2002).
Sets of dots and optional diacritic markersare used to create character distinctions in Arabic.However, trace amounts of dust or dirt on the origi-nal document scan can be easily mistaken for thesemarkers (Darwish and Oard, 2002).
Alternatively,these markers in handwritten text may be too small,light or closely-spaced to readily distinguish, caus-ing the system to drop them entirely.
While Arabicdisconnective letters may make it hard to determineword boundaries, they could plausibly contribute toreduced ambiguity of otherwise similar shapes.2.2 Arabic Morphology ChallengesArabic words can be described in terms of theirmorphemes.
In addition to concatenative prefixesand suffixes, Arabic has templatic morphemes calledroots and patterns.
For example, the word ??D.KA???
?wkmkAtbhm1 (w+k+mkAtb+hm) ?and like their of-fices?
has two prefixes and one suffix, in additionto a stem composed of the root H.H?
k-t-b ?writ-ing related?
and the pattern m1A23.2 Arabic wordscan also be described in terms of lexemes and inflec-tional features.
The set of word forms that only varyinflectionally among each other is called the lexeme.A lemma is a particular word form used to representthe lexeme word set ?
a citation form that standsin for the class (Habash, 2010).
For instance, thelemma I.J??
mktb ?office?
represents the class ofall forms sharing the core meaning ?office?
: I.KA?
?mkAtb ?offices?
(irregular plural), I.J???
@ Almktb ?the1All Arabic transliterations are presented in the HSBtransliteration scheme (Habash et al, 2007).2The digits in the pattern correspond to positions where rootradicals are inserted.office?, A?D.J???
lmktbhA ?for her office?, and so on.Just as the lemma abstracts over inflectional mor-phology, the root abstracts over both inflectionaland derivational morphology and thus provides avery high level of lexical abstraction, indicating the?core?
meaning of the word.
The Arabic root H.H?k-t-b ?writing related?, e.g., relates words like I.J?
?mktb ?office?, H.
?J??
mktwb ?letter?, and?J.J?
ktybh?
?military unit (of conscripts)?.Arabic morphology allows for tens of billions ofpotential, legal words (Magdy and Darwish, 2006;Moftah et al, 2009).
The large potential vocabularysize by itself complicates HR methods that rely onconventional, word-based dictionary lookup strate-gies.
In this paper we consider the value of morpho-lexical and morpho-syntactic features such as lem-mas and part-of-speech tags, respectively, that mayallow machine learning algorithms to learn general-izations.
We do not consider the root since it hasbeen shown to be too general for NLP purposes(Larkey et al, 2007).
Other researchers have usedstems for OCR correction (Magdy and Darwish,2006); we discuss their work and compare to it inSection 6, but we do not present a direct experimen-tal comparison.3 Problem Zones in HandwritingRecognition3.1 HR Error ClassificationsWe can classify three types of HR errors: substi-tutions, insertions and deletions.
Substitutions in-volve replacing the correct word by another incor-rect form.
Insertions are words that are incorrectlyadded into the HR hypothesis.
An insertion erroris typically paired with a substitution error, wherethe two errors reflect a mis-identification of a singleword as two words.
Deletions are simply missingwords.
Examples of these different types of errorsappear in Table 1.
In the dev set that we study here(see Section 4.1), 25.8% of the words are marked asproblematic.
Of these, 87.2% are letter-based words(henceforth words), as opposed to 9.3% punctuationand 3.5% digits.Orthogonally, 81.4% of all problem words aresubstitution errors, 10.6% are insertion errors and7.9% are deletion errors.
Whereas punctuation sym-bols are 9.3% of all errors, they represent over 38%876REF !
?X?m.?'@?J?A??YK.P?J.
??
@??J??
@ ??YKB@?
?PA???JJ?J????
@ ?m'A?AK???????
@ A?E@ ?
?' 	Qj.
?@!
Aljwdh?
?Alyh?
zbdh?
?lbh?
tSn?wA ?n wAl?ndls wfArs AlqsTnTynyh?
yAfAtHy Almslmwn ?yhA ?
?jztmHYP ?X?m.?'@?J?A?
??K?
?J??
@?Q????
@ 	???YgB@?
?PA???JJ?J????
@ ?GAKAK.???????
@ A?E@ ?
?' Q?@Aljwdh?
?Alyh?
ywh n ?lyh tSrfwA An lmn wAlAx?
wfArs AlqsTnTynyh?
bAtAn?
Almslmwn AyhA ?m ?
?yrPZD PROB OK PROB PROB PROB PROB PROB PROB PROB OK OK PROB OK PROB PROB PROBDELX INS SUB DOTS SUB ORTH INS SUB SUB ORTH INS SUBTable 1: An example highlighting the different types of Arabic HR errors.
The first row shows the reference sentence(right-to-left).
The second row shows an automatically generated hypothesis of the same sentence.
The last row showswhich words in the hypothesis are marked as problematic (PROB) by the system and the specific category of theproblem (illustrative, not used by system): SUB (substituted), ORTH (substituted by an orthographic variant), DOTS(substituted by a word with different dotting), INS (inserted), and DELX (adjacent to a deleted word).
The remainingwords are tagged as OK.
The reference translates as ?Are you unable O?Moslems, you who conquered Constantinopleand Persia and Andalusia, to manufacture a tub of high quality butter!?.
The hypothesis roughly translates as ?I loanthen O?Moslems Pattani Constantinople, and Persia and taking from whom that you spend on him N Yeoh high quality?.of all deletion errors, almost 22% of all insertion er-rors and less than 5% of substitution errors.
Simi-larly digits, which are 3.5% of all errors, are almost14% of deletions, 7% of insertions and just over 2%of all substitutions.
Punctuation and digits bring dif-ferent challenges: whereas punctuation marks are asmall class, their shape is often confusable with Ara-bic letters or letter components, e.g., @A?
and !
orP r and ,.
Digits on the other hand are a hard classto language model since the vocabulary (of multi-digit numbers) is infinite.
Potentially this can beaddressed using a pattern-based model that capturesforms of digit sequences (such as date and currencyformats); we leave this as future work.Words (non-digit, non-punctuation) still consti-tute the majority in every category of error: 47.7%of deletions, 71.3% of insertions and over 93%of substitutions.
Among substitutions, 26.5% aresimple orthographic variants that are often normal-ized in Arabic NLP because they result from fre-quent inconsistencies in spelling: Alef Hamza forms( @/@/ @/@ A/?/A?/A?)
and Ya/Alef-Maqsura (?/?
y/?).
Ifwe consider whether the lemma of the correct wordand its incorrect form are matchable, an additional6.9% can be added to the orthographic variant sum(since all of these cases can share the same lemmas).The rest of the cases, or 59.7% of the words, in-volve complex orthographic errors.
Simple dot mis-placement can only account for 2.4% of all substi-tution errors.
The HR system output does not con-tain any illegal non-words since its vocabulary is re-stricted by its training data and language models.The large proportion of errors involving lemma dif-ferences is consistent with the perception that mostOCR/HR errors create semantically incoherent sen-tences.
This suggests that lemma models can behelpful in identifying such errors.3.2 Problem Zone DefinitionPrior to developing a model for PZD, it is necessaryto define what is considered a ?problem?.
Once adefinition is chosen, gold problem tags can be gen-erated for the training and test data by comparing thehypotheses to their references.3 We decided in thispaper to use a simple binary problem tag: a hypothe-sis word is tagged as "PROB" if it is the result of aninsertion or substitution of a word.
Deleted wordsin a hypothesis, which cannot be tagged themselves,cause their adjacent words to be marked as PROB in-stead.
In this way, a subsequent HR post-processingsystem can be alerted to the possibility of a miss-ing word via its surroundings (hence the idea of aproblem ?zone?).
Any words not marked as PROBare given an "OK" tag (see the PZD row of Table 1).We describe in Section 5.6 some preliminary exper-iments we conducted using more fine-grained tags.4 Experimental Settings4.1 Training and Evaluation DataThe data used in this paper is derived from im-age scans provided by the Linguistic Data Consor-tium (LDC) (Strassel, 2009).
This data consists ofhigh-resolution (600 dpi) handwriting scans of Ara-bic text taken from newswire articles, web logs and3For clarity, we refer to these tags as ?gold?, whereas the cor-rect segment for a given hypothesis set is called the ?reference?.877newsgroups, along with ground truth annotationsand word bounding box information.
The scans in-clude variations in scribe demographic background,writing instrument, paper and writing speed.The BBN Byblos HR system (Natarajan et al,2008; Saleem et al, 2009) is then used to pro-cess these scanned images into sequences of seg-ments (sentence fragments).
The system generatesa ranked N-best list of hypotheses for each segment,where N could be as high as 300.
On average, a seg-ment has 6.87 words (including punctuation).We divide the N-best list data into training, de-velopment (dev) and test sets.4 For training, weconsider two sets of size 2000 and 4000 segments(S) with the 10 top-ranked hypotheses (H) for eachsegment to provide additional variations.5 The ref-erences are also included in the training sets to pro-vide examples of perfect text.
The dev and test setsuse 500 segments with one top-ranked hypothesiseach {H=1}.
We can construct a trivial PZD base-line by assuming all the input words are PROBs;this results in baseline % Precision/Recall/F-scoresof 25.8/100/41.1 and 26.0/100/41.2 for the dev andtest sets, respectively.
Note that in this paper weeschew these baselines in favor of comparison toa non-trivial baseline generated by a simple PZDmodel.4.2 PZD Models and FeaturesThe PZD system relies on a set of SVM classi-fiers trained using morphological and lexical fea-tures.
The SVM classifiers are built using Yamcha(Kudo and Matsumoto, 2003).
The SVMs use aquadratic polynomial kernel.
For the models pre-sented in this paper, the static feature window con-text size is set to +/- 2 words; the previous two (dy-namic) classifications (i.e.
targets) are also used asfeatures.
Experiments with smaller window sizes re-sult in poorer performance, while a larger windowsize (+/- 6 words) yields roughly the same perfor-mance at the expense of an order-of-magnitude in-crease in required training time.
Over 30 different4Naturally, we do not use data that the BBN Byblos HR sys-tem was trained on.5We conducted additional experiments where we varied thenumber of segments and hypotheses and found that the systembenefited from added variety of segments more than hypotheses.We also modified training composition in terms of the ratio ofproblem/non-problem words; this did not help performance.Simple Descriptionword The surface word formnw Normalized word: the word after Alef,Ya and digit normalizationpos The part-of-speech (POS) of the wordlem The lemma of the wordna No-analysis: a binary feature indicat-ing whether the morphological analyzerproduced any analyses for the wordBinned Descriptionnw N-grams Normword 1/2/3-gram probabilitieslem N-grams Lemma 1/2/3-gram probabilitiespos N-grams POS 1/2/3-gram probabilitiesconf Word confidence: the ratio of the num-ber of hypotheses in the N-best list thatcontain the word over the total numberof hypothesesTable 2: PZD model features.
Simple features are useddirectly by the PZD SVM models, whereas Binned fea-tures?
(numerical) values are reduced to a small, labeledcategory set whose labels are used as model features.combinations of features were considered.
Table 2shows the individual feature definitions.In order to obtain the morphological features,all of the training and test data is passed throughMADA 3.0, a software tool for Arabic morpholog-ical analysis disambiguation (Habash and Rambow,2005; Roth et al, 2008; Habash et al, 2010).
Forthese experiments, MADA provides the pos (usingMADA?s native 34-tag set) and the lemma for eachword.
Occasionally MADA will not be able to pro-duce any interpretations (analyses) for a word; sincethis is often a sign that the word is misspelled or un-common, we define a binary na feature to indicatewhen MADA fails to generate analyses.In addition to using the MADA features directly,we also develop a set of nine N-gram models (whereN=1, 2, and 3) for the nw, pos, and lem features de-fined in Table 2.
We train these models using 220Mwords from the Arabic Gigaword 3 corpus (Graff,2007) which had also been run through MADA 3.0to extract the pos and lem information.
The modelsare built using the SRI Language Modeling Toolkit(Stolcke, 2002).
Each word in a hypothesis can thenbe assigned a probability by each of these nine mod-els.
We reduce these probabilities into one of ninebins, with each successive bin representing an orderof magnitude drop in probability (the final bin is re-878served for word N-grams which did not appear inthe models).
The bin labels are used as the SVMfeatures.Finally, we also use a word confidence (conf)feature, which is aimed at measuring the frequencywith which a given word is chosen by the HR systemfor a given segment scan.
The conf is defined hereas the ratio of the number of hypotheses in the N-best list that the word appears in to the total numberof hypotheses.
These numbers are calculated usingthe original N-best hypothesis list, before the data istrimmed to H={1, 10}.
Like the N-grams, this num-ber is binned; in this case there are 11 bins, with 10spread evenly over the [0,1) range, and an extra binfor values of exactly 1 (i.e., when the word appearsin every hypothesis in the set).5 ResultsWe describe next different experiments conductedby varying the features used in the PZD model.
Wepresent the results in terms of F-score only for sim-plicity; we then conduct an error analysis that exam-ines precision and recall.5.1 Effect of Feature Set ChoiceSelecting an appropriate set of features for PZD re-quires extensive testing.
Even when only consider-ing the few features described in Table 2, the param-eter space is quite large.
Rather then exhaustivelytest every possible feature combination, we selec-tively choose feature subsets that can be comparedto gain a sense of the incremental benefit providedby individual features.5.1.1 Simple FeaturesTable 3 illustrates the result of taking a baseline fea-ture set (containing word as the only feature) andadding a single feature from the Simple set to it.
Theresult of combining all the Simple features is also in-dicated.
From this, we see that Simple features, evencollectively, provide only minor improvements.5.1.2 Binned FeaturesTable 4 shows models which include both Simpleand Binned features.
First, Table 4 shows the effectof adding nw N-grams of successively higher ordersto the word baseline.
Here we see that even a sim-ple unigram provides a significant benefit (comparedFeature Set F-score %Impword 43.85 ?word+nw 43.86 ?0word+na 44.78 2.1word+lem 45.85 4.6word+pos 45.91 4.7word+nw+pos+lem+na 46.34 5.7Table 3: PZD F-scores for simple feature combinations.The training set used was {S=2000, H=10} and the mod-els were evaluated on the dev set.
The improvement overthe word baseline case is also indicated.
%Imp is the rel-ative improvement over the first row.Feature Set F-score %Impword 43.85 ?word+nw 1-gram 49.51 12.9word+nw 1-gram+nw 2-gram 59.26 35.2word+nw N-grams 59.33 35.3+pos 58.50 33.4+pos N-grams 57.35 30.8+lem+lem N-grams 59.63 36.0+lem+lem N-grams+na 59.93 36.7+lem+lem N-grams+na+nw 59.77 36.3+lem 60.92 38.9+lem+na 60.47 37.9+lem+lem N-grams 60.44 37.9Table 4: PZD F-scores for models that include Binnedfeatures.
The training set used was {S=2000, H=10} andthe models were evaluated on the dev set.
The improve-ment over the word baseline case is also indicated.
Thelabel "N-grams" following a Binned feature refers to us-ing 1, 2 and 3-grams of that feature.
Indentation marksaccumulative features in model.
The best performing row(with bolded score) is word+nw N-grams+lem.to the improvements gained in Table 3).
The largestimprovement comes with the addition of the bigram(thus introducing context into the model), but the tri-gram provides only a slight improvement above that.This implies that pursuing higher order N-grams willresult in negligible returns.In the next part of Table 4, we see that the sin-gle feature (pos) which provided the highest single-feature benefit in Table 3 does not provide simi-lar improvements under these combinations, and infact seems detrimental.
We also note that usingall the features in one model is outperformed bymore selective choices.
Here, the best performer isthe model which utilizes the word, nw N-grams,879Base Feature Set F-score %Imp+confword 43.85 55.83 27.3+nw N-grams 59.33 61.71 4.0+lem 60.92 62.60 2.8+lem+na 60.47 63.14 4.4+lem+lem N-grams 60.44 62.88 4.0+pos+pos N-grams+na+nw (all system) 59.77 62.44 4.5Table 5: PZD F-scores for models when word confi-dence is added to the feature set.
The training set usedwas {S=2000, H=10} and the models were evaluated onthe dev set.
The improvement generated by includingword confidence is indicated.
The label "N-grams" fol-lowing a Binned feature refers to using 1, 2 and 3-gramsof that feature.
Indentation marks accumulative featuresin model.
%Imp is the relative improvement gained byadding the conf feature.and lem as the only features.
However, the dif-ferences among this model and the other modelsusing lem Table 4 are not statistically significant.The differences between this model and the otherlower performing models are statistically significant(p<0.05).5.1.3 Word ConfidenceThe conf feature deserves special consideration be-cause it is the only feature which draws on informa-tion from across the entire hypothesis set.
In Table 5,we show the effect of adding conf as a feature toseveral base feature sets taken from Table 4.
Exceptfor the baseline case, conf provides a relativelyconsistent benefit.
The large (27.3%) improvementgained by adding conf to the word baseline showsthat conf is a valuable feature, but the smaller im-provements in the other models indicate that the in-formation it provides largely overlaps with the in-formation already present in those models.
The dif-ferences among the last four models (all includinglem) in Table 5 are not statistically significant.
Thedifferences between these four models and the firsttwo are statistically significant (p<0.05).5.2 Effect of Training Data SizeIn order to allow for rapid examination of multi-ple feature combinations, we restricted the size ofthe training set (S) to maintain manageable train-ing times.
With this decision comes the implicit as-S = 2000 S = 4000Feature Set F-score F-score %Impword 43.85 52.08 18.8word+conf 55.83 57.50 3.0word+nw N-grams+lem+conf (best system) 62.60 66.34 6.0+na 63.14 66.21 4.9+lem N-grams 62.88 64.43 2.5all 62.44 65.62 5.1Table 6: PZD F-scores for selected models when thenumber of training segments (S) is doubled.
The trainingset used was {S=2000, H=10} and {S=4000, H=10},and the models were evaluated on the dev set.
The label"N-grams" following a Binned feature refers to using 1, 2and 3-grams of that feature.
Indentation marks accumu-lative features in model.sumption that the results obtained will scale with ad-ditional training data.
We test this assumption bytaking the best-performing feature sets from Table 5and training new models using twice the trainingdata {S=4000}.
The results are shown in Table 6.In each case, the improvements are relatively con-sistent (and on the order of the gains provided by theinclusion of conf as seen in Table 5), indicating thatthe model performance does scale with data size.However, these improvements come with a cost ofa roughly 4-7x increase in training time.
We notethat the value of doubling S is roughly 3-6x timesgreater for the word baseline than the others; how-ever, simply adding conf to the baseline providesan even greater improvement than doubling S. Thedifferences between the final four models in Table 6are not statistically significant.
The differences be-tween these models and the first two models in thetable are statistically significant (p<0.05).
For con-venience, in the next section we refer to the thirdmodel listed in Table 6 as the best system (becauseit has the highest absolute F-score on the large dataset), but readers should recall that these four modelsare roughly equivalent in performance.5.3 Error AnalysisIn this section, we look closely at the performanceof a subset of systems on different types of prob-lem words.
We compare the following model set-tings: for {S=4000} training, we use word, word+ conf, the best system from Table 6 and the model880(a) S=4000 S=2000word wconf best all allPrecision 54.7 59.5 67.1 67.4 62.4Recall 49.7 55.7 65.6 64.0 62.5F-score 52.1 57.5 66.3 65.6 62.4Accuracy 76.4 78.7 82.8 82.7 80.6(b) %Prob word wconf best all allWords 87.2 51.8 57.3 68.5 67.1 64.9Punc.
9.3 39.5 44.7 50.0 46.1 40.8Digits 3.5 24.1 44.8 34.5 34.5 62.1INS 10.6 46.0 49.4 62.1 62.1 55.2DEL 7.9 29.2 20.0 24.6 21.5 27.7SUB 81.4 52.2 60.0 70.0 68.4 66.9Ortho 21.6 63.3 51.4 52.5 53.7 48.6Lemma 5.6 45.7 52.2 63.0 52.2 54.4Semantic 54.2 48.4 64.2 77.7 75.9 75.5Table 7: Error analysis results comparing the perfor-mance of multiple systems over different metrics (a) andword/error types (b).
%Prob shows the distribution ofproblem words into different word types (word, punctua-tion and digit) and error types.
INS, DEL and SUB standfor insertion, deletion and substitution error types, re-spectively.
Ortho stands for orthographic variant.
Lemmastands for ?shared lemma?.
The columns to the rightof the %Prob column show recall percentage for eachword/error type.using all possible features (word, wconf, best andall, respectively); and we also use all trained with{S=2000}.
We consider the performance in termsof precision and recall in addition to F-score ?
seeTable 7 (a).
We also consider the percentage of re-call per error type, such as word/punctuation/digit ordeletion/insertion/substitution and different types ofsubstitution errors ?
see Table 7 (b).
The second col-umn in this table (%Prob) shows the distribution ofgold-tagged problem words into word and error typecategories.Overall, there is no major tradeoff between preci-sion and recall across the different settings; althoughwe can observe the following: (i) adding more train-ing data helps precision more than recall (over threetimes more) ?
compare the last two columns in Ta-ble 7 (a); and (ii) the best setting has a slightly lowerprecision than all features, although a much betterrecall ?
compare columns 4 and 5 in Table 7 (a).The performance of different settings on words isgenerally better than punctuation and that is betterthan digits.
The only exceptions are in the digit cat-egory, which may be explained by that category?ssmall count which makes it prone to large percent-age fluctuations.In terms of error type, the performance on sub-stitutions is better than insertions, which is in turnbetter than deletions, for all systems compared.
Thismakes sense since deletions are rather hard to de-tect and they are marked on possibly correct adja-cent words, which may confuse the classifiers.
Oneinsight for future work is to develop systems fordifferent types of errors.
Considering substitutionsin more detail, we see that surprisingly, the simpleapproach of using the word feature only (withoutconf) correctly recalls a bigger proportion of prob-lems involving orthographic variants than other set-tings.
It seems that the more complex the model,the harder it is to model these cases correctly.
Er-ror types that include semantic variations (differentlemmas) or shared lemmas (but not explained by or-thographic variation), are by contrast much harderfor the simple models.
The more complex models doquite well recalling errors involving semantically in-coherent substitutions (around 77.7% of those cases)and words that share the same lemma but vary in in-flectional features (63% of those cases).
These tworesults are quite a jump from the basic word baseline(around 29% and 18% respectively).The simple addition of data seems to contributemore towards the orthographic variation errors andless towards semantic errors.
The different settingswe use (training size and features) show some de-gree of complementarity in how they identify errors.We try to exploit this fact in Section 5.5 exploringsome simple system combination ideas.5.4 Blind Test SetTable 8 shows the results of applying the same mod-els described in Table 7 to a blind test set of yet un-seen data.
As mentioned in Section 4.1, the trivialbaseline of the test set is comparable to the dev set.However, the test set is harder to tag than the devset; this can be seen in the overall lower F-scores.That said, the relative order of performing featuresis the same as with the dev set, confirming that ourbest model is optimal for test too.
On further study,we noticed that the reason for the test set differenceis that the overlap in word forms between test and881word wconf best allPrecision 37.55 51.48 57.01 55.46Recall 51.73 53.39 61.97 60.44F-score 43.51 52.42 59.39 57.84Accuracy 65.13 74.83 77.99 77.13Table 8: Results on test set of 500 segments with one hy-pothesis each.
The models were trained on the {S=4000,H=10} training set.train is less than dev and train: 63% versus 81%,respectively on {S=4000}.5.5 Preliminary Combination AnalysisIn a preliminarily investigation of the value of com-plementarity across these different systems, we triedtwo simple model combination techniques.
We re-stricted the search to the systems in the error analy-sis (Table 7).First, we considered a sliding voting schemewhere a word is marked as problematic if at leastn systems agreed to that.
Naturally, as n increases,precision increases and recall decreases, provid-ing multiple tradeoff options.
The range spans49.1/83.2/61.8 (% Precision/Recall/F-score) at oneend (n = 1) to 80.4/27.5/41.0 on the other (n = all).The best F-score combination was with n = 2 (anytwo agree) producing 62.8/72.4/67.3, an almost 1%higher than our best system.In a different combination exploration, we ex-haustively sought the best three systems from whichany agreement (2 or 3) can produce an even bettersystem.
The best combination included the wordmodel, the best model (both in {S=4000} training)and the all model (in {S=2000}).
This combinationyields 70.2/64.0/66.9, a lower F-score than the bestgeneral voting approach discussed above, but with adifferent bias towards better precision.These basic exploratory experiments show thatthere is a lot of value in pursuing combinations ofsystems, if not for overall improvement, then at leastto benefit from tradeoffs in precision and recall thatmay be appropriate for different applications.5.6 Preliminary Tag Set ExplorationIn all of the experiments described so far, thePZD models tag words using a binary tag setof PROB/OK.
We may also consider more com-plex tag sets based on problem subtypes, suchas SUB/INS/DEL/OK (where all the problem sub-types are differentiated), SUB/INS/OK (ignoresdeletions), and SUB/OK (ignores deletions and in-sertions).
Care must be taken when comparing thesesystems, because the differences in tag set definitionresults in different baselines.
Therefore we com-pare the % error reduction over the trivial baselineachieved in each case.For an all model trained on the {S=2000, H=10}set, using the PROB/OK tag set results in a 36.3%error reduction over its trivial baseline (using thedev set).
The corresponding SUB/INS/DEL/OK tagset only achieves a 34.8% error reduction.
TheSUB/INS/OK tag set manages a 40.1% error re-duction, however.
The SUB/OK tag set achievesa 38.9% error reduction.
We suspect that the verylow relative number of deletions (7.9% in the devdata) and the awkwardness of a DEL tag indicatinga neighboring deletion (rather than the current word)may be confusing the models, and so ignoring themseems to result in a clearer picture.6 Related WorkCommon OCR/HR post-processing strategies aresimilar to spelling correction solutions involvingdictionary lookup (Kukich, 1992; Jurafsky and Mar-tin, 2000) and morphological restrictions (Domeijet al, 1994; Oflazer, 1996).
Error detection sys-tems using dictionary lookup can sometimes be im-proved by adding entries representing morphologi-cal variations of root words, particularly if the lan-guage involved has a complex morphology (Pal etal., 2000).
Alternatively, morphological informationcan be used to construct supplemental lexicons orlanguage models (Sari and Sellami, 2002; Magdyand Darwish, 2006).In comparison to (Magdy and Darwish, 2006),our paper is about error detection only (done in us-ing discriminative machine learning); whereas theirwork is on error correction (done in a standard gen-erative manner (Kolak and Resnik, 2002)) with noassumptions of some cases being correct or incor-rect.
In essence, their method of detection is thesame as our trivial baseline.
The morphological fea-tures they use are shallow and restricted to breakingup a word into prefix+stem+suffix; whereas we ana-lyze words into their lemmas, abstracting away overa large number of variations.
We also made use of882part-of-speech tags, which they do not use, but sug-gest may help.
In their work, the morphological fea-tures did not help (and even hurt a little), whereas forus, the lemma feature actually helped.
Their hypoth-esis that their large language model (16M words)may be responsible for why the word-based mod-els outperformed stem-based (morphological) mod-els is challenged by the fact that our language modeldata (220M words) is an order of magnitude larger,but we are still able to show benefit for using mor-phology.
We cannot directly compare to their re-sults because of the different training/test sets andtarget (correction vs detection); however, we shouldnote that their starting error rate was quite high (39%on Alef/Ya normalized words), whereas our start-ing error rate is almost half of that (?26% with un-normalized Alef/Yas, which account for almost 5%absolute of the errors).
Perhaps a combination ofthe two kinds of efforts can push the perfomance oncorrection even further by biasing towards problem-atic words and avoiding incorrectly changing correctwords.
Magdy and Darwish (2006) do not report onpercentages of words that they incorrectly modify.7 Conclusions and Future WorkWe presented a study with various settings (linguis-tic and non-linguistic features and learning curve)for automatically detecting problem words in Ara-bic handwriting recognition.
Our best approachachieves a roughly ?15% absolute increase in F-score over a simple baseline.
A detailed error anal-ysis shows that linguistic features, such as lemmamodels, help improve HR-error detection specifi-cally where we expect them to: identifying semanti-cally inconsistent error words.In the future, we plan to continue improving oursystem by considering smarter trainable combina-tion techniques and by separating the training fordifferent types of errors, particularly deletions frominsertions and substitutions.
We would also liketo conduct an extended evaluation comparing othertypes of morphological features, such as roots andstems, directly.
One additional idea is to implementa lemma-confidence feature that examines lemmause in hypotheses across the document.
This couldpotentially provide valuable semantic information atthe document level.We also plan to integrate our system with a systemfor producing correction hypotheses.
We also willconsider different uses for the basic system setupwe developed to identify other types of text errors,such as spelling errors or code-switching betweenlanguages and dialects.AcknowledgmentsWe would like to thank Premkumar Natarajan, RohitPrasad, Matin Kamali, Shirin Saleem, Katrin Kirch-hoff, and Andreas Stolcke.
This work was fundedunder DARPA project number HR0011-08-C-0004.ReferencesFadi Biadsy, Jihad El-Sana, and Nizar Habash.
2006.Online Arabic handwriting recognition using Hid-den Markov Models.
In The 10th InternationalWorkshop on Frontiers in Handwriting Recognition(IWFHR?10), La Baule, France.Kareem Darwish and Douglas W. Oard.
2002.
Term Se-lection for Searching Printed Arabic.
In SIGIR ?02:Proceedings of the 25th annual international ACM SI-GIR conference on Research and development in in-formation retrieval, pages 261?268, New York, NY,USA.
ACM.Rickard Domeij, Joachim Hollman, and Viggo Kann.1994.
Detection of spelling errors in Swedish not us-ing a word list en clair.
J. Quantitative Linguistics,1:1?195.David Graff.
2007.
Arabic Gigaword 3, LDC CatalogNo.
: LDC2003T40.
Linguistic Data Consortium, Uni-versity of Pennsylvania.Nizar Habash and Owen Rambow.
2005.
Arabic Tok-enization, Part-of-Speech Tagging and MorphologicalDisambiguation in One Fell Swoop.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL?05), pages 573?580, AnnArbor, Michigan, June.
Association for ComputationalLinguistics.Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.2007.
On Arabic Transliteration.
In A. van den Boschand A. Soudi, editors, Arabic Computational Mor-phology: Knowledge-based and Empirical Methods.Springer.Nizar Habash, Owen Rambow, and Ryan Roth.
2010.MADA+TOKAN Manual.
Technical Report CCLS-10-01, Center for Computational Learning Systems(CCLS), Columbia University.Nizar Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Morgan & Claypool Publish-ers.883Mohamed Ben Halima and Adel M. Alimi.
2009.
Amulti-agent system for recognizing printed Arabicwords.
In Khalid Choukri and Bente Maegaard, edi-tors, Proceedings of the Second International Confer-ence on Arabic Language Resources and Tools, Cairo,Egypt, April.
The MEDAR Consortium.Daniel Jurafsky and James H. Martin.
2000.
Speechand Language Processing.
Prentice Hall, New Jersey,USA.Okan Kolak and Philip Resnik.
2002.
OCR error cor-rection using a noisy channel model.
In Proceedingsof the second international conference on Human Lan-guage Technology Research.Taku Kudo and Yuji Matsumoto.
2003.
Fast Methods forKernel-Based Text Analysis.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics (ACL?03), pages 24?31, Sapporo,Japan, July.
Association for Computational Linguis-tics.Karen Kukich.
1992.
Techniques for AutomaticallyCorrecting Words in Text.
ACM Computing Surveys,24(4).Leah S. Larkey, Lisa Ballesteros, and Margaret E.Connell, 2007.
Arabic Computational Morphology:Knowledge-based and Empirical Methods, chapterLight Stemming for Arabic Information Retrieval.Springer Netherlands, Kluwer/Springer edition.Zhidong Lu, Issam Bazzi, Andras Kornai, John Makhoul,Premkumar Natarajan, and Richard Schwartz.
1999.A Robust, Language-Independent OCR System.
In the27th AIPR Workshop: Advances in Computer AssistedRecognition, SPIE.Walid Magdy and Kareem Darwish.
2006.
Arabic OCRError Correction Using Character Segment Correction,Language Modeling, and Shallow Morphology.
InProceedings of 2006 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP 2006),pages 408?414, Sydney, Austrailia.Volker M?rgner and Haikal El Abed.
2009.
ArabicWord and Text Recognition - Current Developments.In Khalid Choukri and Bente Maegaard, editors, Pro-ceedings of the Second International Conference onArabic Language Resources and Tools, Cairo, Egypt,April.
The MEDAR Consortium.Mohsen Moftah, Waleed Fakhr, Sherif Abdou, andMohsen Rashwan.
2009.
Stem-based Arabic lan-guage models experiments.
In Khalid Choukri andBente Maegaard, editors, Proceedings of the SecondInternational Conference on Arabic Language Re-sources and Tools, Cairo, Egypt, April.
The MEDARConsortium.Prem Natarajan, Shirin Saleem, Rohit Prasad, EhryMacRostie, and Krishna Subramanian, 2008.
Arabicand Chinese Handwriting Recognition, volume 4768of Lecture Notes in Computer Science, pages 231?250.Springer, Berlin, Germany.Kemal Oflazer.
1996.
Error-tolerant finite-state recog-nition with applications to morphological analysisand spelling correction.
Computational Linguistics,22:73?90.U.
Pal, P. K. Kundu, and B.
B. Chaudhuri.
2000.
OCR er-ror correction of an inflectional Indian language usingmorphological parsing.
J.
Information Sci.
and Eng.,16:903?922.Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,and Cynthia Rudin.
2008.
Arabic Morphological Tag-ging, Diacritization, and Lemmatization Using Lex-eme Models and Feature Ranking.
In Proceedings ofACL-08: HLT, Short Papers, pages 117?120, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Shirin Saleem, Huaigu Cao, Krishna Subramanian, MarinKamali, Rohit Prasad, and Prem Natarajan.
2009.Improvements in BBN?s HMM-based Offline Hand-writing Recognition System.
In Khalid Choukri andBente Maegaard, editors, 10th International Confer-ence on Document Analysis and Recognition (ICDAR),Barcelona, Spain, July.Toufik Sari and Mokhtar Sellami.
2002.
MOrpho-LEXical analysis for correcting OCR-generated Ara-bic words (MOLEX).
In The 8th InternationalWorkshop on Frontiers in Handwriting Recognition(IWFHR?02), Niagara-on-the-Lake, Canada.Andreas Stolcke.
2002.
SRILM - an Extensible Lan-guage Modeling Toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing(ICSLP), volume 2, pages 901?904, Denver, CO.Stephanie Strassel.
2009.
Linguistic Resources forArabic Handwriting Recognition.
In Khalid Choukriand Bente Maegaard, editors, Proceedings of the Sec-ond International Conference on Arabic Language Re-sources and Tools, Cairo, Egypt, April.
The MEDARConsortium.884
