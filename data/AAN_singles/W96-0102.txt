MBT: A Memory-Based Part of Speech Tagger-GeneratorWalter Daelemans, Jakub ZavrelComputat iona l  Linguistics and AITi lburg UniversityP.O.
Box 90153, NL-5000 LE Ti lburg{walter.
daelemans, zavrel}~kub, nlPeter Berck, Steven GillisCenter for Dutch Language and SpeechUniversity of AntwerpUniversiteitsplein 1, B-2610 Wilr i jk{peter.
berck, steven, gillis}@uia, ua.
ac.
beAbstractWe introduce a memory-based approach to part of speech tagging.
Memory-basedlearning is a form of supervised learning based on similarity-based reasoning.
The partof speech tag of a word in a particular context is extrapolated from the most similarcases held in memory.
Supervised learning approaches are useful when a tagged corpusis available as an example of the desired output of the tagger.
Based on such a corpus,the tagger-generator automatically builds a tagger which is able to tag new text thesame way, diminishing development time for the construction of a tagger considerably.Memory-based tagging shares this advantage with other statistical or machine learningapproaches.
Additional advantages specific to a memory-based approach include (i) therelatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii)explanation capabilities, (iv) flexible integration of information in case representations,(v) its non-parametric nature, (vi) reasonably good results on unknown words withoutmorphological nalysis, and (vii) fast learning and tagging.
In this paper we show thata large-scale application of the memory-based approach is feasible: we obtain a taggingaccuracy that is on a par with that of known statistical approaches, and with attractivespace and time complexity properties when using IGTree, a tree-based formalism forindexing and searching huge case bases.
The use of IGTree has as additional advantagethat optimal context size for disambiguation is dynamically computed.1 In t roduct ionPart of Speech (POS) tagging is a process in which syntactic ategories are assigned towords.
It can be seen as a mapping from sentences to strings of tags.Automatic tagging is useful for a number of applications: as a preprocessing stageto parsing, in information retrieval, in text to speech systems, in corpus linguistics, etc.The two factors determining the syntactic ategory of a word are its lexical probability(e.g.
without context, man is more probably a noun than a verb), and its contextualprobability (e.g.
after a pronoun, man is more probably a verb than a noun, as in theyman the boats).
Several approaches have been proposed to construct automatic taggers.Most work on statistical methods has used n-gram models or Hidden Markov Model-basedtaggers (e.g.
Church, 1988; DeRose, 1988; Cutting et al 1992; Merialdo, 1994, etc.).
In14these approaches, a tag sequence is chosen for a sentence that maximizes the product oflexical and contextual probabilities as estimated from a tagged corpus.In rule-based approaches, words are assigned a tag based on a set of rules and alexicon.
These rules can either be hand-crafted (Garside et al, 1987; Klein & Simmons,1963; Green & Rubin, 1971), or learned, as in Hindle (1989) or the transformation-basederror-driven approach of Brill (1992).In a memory-based approach, a set of cases is kept in memory.
Each case consists of aword (or a lexical representation for the word) with preceding and following context, andthe corresponding category for that word in that context.
A new sentence is tagged byselecting for each word in the sentence and its context he most similar case(s) in memory,and extrapolating the category of the word from these 'nearest neighbors'.
A memory-based approach as features of both learning rule-based taggers (each case can be regardedas a very specific rule, the similarity based reasoning as a form of conflict resolution andrule selection mechanism) and of stochastic taggers: it is fundamentally a form of k-nearestneighbors (k-nn) modeling, a well-known non-parametric statistical pattern recognitiontechnique.
The approach in its basic form is computationally expensive, however; eachnew word in context hat has to be tagged, has to be compared to each pattern keptin memory.
In this paper we show that a heuristic case base compression formalism(Daelemans et al, 1996), makes the memory-based approach computationally attractive.2 Memory-Based  LearningMemory-based Learning is a form of supervised, inductive learning from examples.
Ex-amples are represented as a vector of feature values with an associated category label.During training, a set of examples (the training set) is presented in an incremental fash-ion to the classifier, and added to memory.
During testing, a set of previously unseenfeature-value patterns (the test set) is presented to the system.
For each test pattern, itsdistance to all examples in memory is computed, and the category of the least distantinstance(s) is used as the predicted category for the test pattern.
The approach is basedon the assumption that reasoning is based on direct reuse of stored experiences ratherthan on the application of knowledge (such as rules or decision trees) abstracted fromexperience.In AI, the concept has appeared in several disciplines (from computer vision to robotics),using terminology such as similarity-based, example-based, memory-based, exemplar-based, case-based, analogical, lazy, nearest-neighbour, and instance-based (Stanfill andWaltz, 1986; Kolodner, 1993; Aha et al 1991; Salzberg, 1990).
Ideas about this type ofanalogical reasoning can be found also in non-mainstream linguistics and pyscholinguistics(Skousen, 1989; Derwing ~ Skousen, 1989; Chandler, 1992; Scha, 1992).
In computationallinguistics (apart from incidental computational work of the linguists referred to earlier),the general approach as only recently gained some popularity: e.g., Cardie (1994, syn-tactic and semantic disambiguation); Daelemans (1995, an overview of work in the earlynineties on memory-based computational phonology and morphology); Jones (1996, anoverview of example-based machine translation research); Federici and Pirrelli (1996).2.1 Similarity MetricPerformance of a memory-based system (accuracy on the test set) crucially depends onthe distance metric (or similarity metric) used.
The most straightforward distance metricwould be the one in equation (1), where X and Y are the patterns to be compared, and~i(x~, y/) is the distance between the values of the i-th feature in a pattern with n features.15A(X, Y) = ~ ~(xi, yi) (1)i=1Distance between two values is measured using equation (2), an overlap metric, forsymbolic features (we will have no numeric features in the tagging application).5(xi, Yi) = 0 i f  xi = Yi, else 1 (2)We will refer to this approach as IB1 (Aha et al, 1991).
We extended the algorithmdescribed there in the following way: in case a pattern is associated with more than onecategory in the training set (i.e.
the pattern is ambiguous), the distribution of patternsover the different categories i kept, and the most frequently occurring category is selectedwhen the ambiguous pattern is used to extrapolate from.In this distance metric, all features describing an example are interpreted as beingequally important in solving the classification problem, but this is not necessarily thecase.
In tagging, the focus word to be assigned a category is obviously more relevant hanany of the words in its context.
We therefore weigh each feature with its information gain;a number expressing the average amount of reduction of training set information entropywhen knowing the value of the feature (Daelemans & van de Bosch, 1992, Quinlan, 1993;Hunt et al 1966) (Equation 3).
We will call this algorithm IB-IG.Y) = (3)i :13 IGTreesMemory-based learning is an expensive algorithm: of each test item, all feature valuesmust be compared to the corresponding feature values of all training items.
Withoutoptimisation, it has an asymptotic retrieval complexity of O(NF)  (where N is the numberof items in memory, and F the number of features).
The same asymptotic omplexityis of course found for memory storage in this approach.
We use IGTrees (Daelemans etal.
1996) to compress the memory.
IGTree is a heuristic approximation of the IB-IGalgorithm.3 .1  The  IGTree  A lgor i thmsIGTree combines two algorithms: one for compressing a case base into a trees, and one forretrieving classification information from these trees.
During the construction of IGTreedecision trees, cases are stored as paths of connected nodes.
All nodes contain a test(based on one of the features) and a class label (representing the default class at thatnode).
Nodes are connected via arcs denoting the outcomes for the test (feature values).A feature relevance ordering technique (in this case information gain, see Section 2.1) isused to determine the order in which features are used as tests in the tree.
This orderis fixed in advance, so the maximal depth of the tree is always equal to the number offeatures, and at the same level of the tree, all nodes have the same test (they are aninstance of oblivious decision trees; cf.
Langley ~ Sage, 1994).
The reasoning behind thisreorganisation (which is in fact a compression) is that when the computation of featurerelevance points to one feature clearly being the most important in classification, searchcan be restricted to matching a test case to those stored cases that have the same featurevalue at that feature.
Besides restricting search to those memory cases that match onlyon this feature, the case memory can be optimised by further restricting search to the16|1  I IProcedure BUILD-IG-TI:tEE:Input:?
A training set T of cases with their classes (start value: a full case base),?
an information-gain-ordered list of features (tests) Fi...Fn (start value: F1...Fn).Output: A (sub)tree.1.
If T is unambiguous (all cases in T have the same class c), create a leaf node with class label c.2.
Else if i = (n + 1), create a leaf node with as label the class occurring most frequently in T.3.
Otherwise, until i = n (the number of features)?
Select the first feature (test) Fi in Fi...Fn, and construct a new node N for feature Fi, and asdefault class c (the class occurring most frequently in T).?
Partition T into subsets TI...Tm according to the values vl...vm which occur for Fi in T (caseswith the same value for this feature in the same subset).?
For each je{1, ..., m}: BUILD-IG-TREE (Tj, Fi+i...F,~),connect the root of this subtree to N and label the arc with vj.Figure 1: Algor i thm for building IGTrees ( 'BU ILD- IG-TREE ' ) .second most important feature, followed by the third most important feature, etc.
Aconsiderable compression is obtained as similar cases share partial paths.Instead of converting the case base to a tree in which all cases are fully represented aspaths, storing all feature values, we compress the tree even more by restricting the pathsto those input feature values that disambiguate he classification from all other cases inthe training material.
The idea is that it is not necessary to fully store a case as a pathwhen only a few feature values of the case make its classification unique.
This implies thatfeature values that do not contribute to the disambiguation f the case classification (i.e.,the values of the features with lower feature relevance values than the the lowest valueof the disambiguating features) are not stored in the tree.
In our tagging application,this means that only context feature values that actually contribute to disambiguationare used in the construction of the tree.Leaf nodes contain the unique class label corresponding to a path in the tree.
Non-terminal nodes contain information about the most probable or default classification giventhe path thus far, according to the bookkeeping information on class occurrences main-tained by the tree construction algorithm.
This extra information is essential when usingthe tree for classification.
Finding the classification of a new case involves traversing thetree (i.e., matching all feature values of the test case with arcs in the order of the overallfeature information gain), and either retrieving a classification when a leaf is reached, orusing the default classification on the last matching non-terminal node if a feature-valuematch fails.A final compression is obtained by pruning the derived tree.
All leaf-node daughtersof a mother node that have the same class as that node are removed from the tree, astheir class information does not contradict the default class information already presentat the mother node.
Again, this compression does not affect IGTree's generalisationperformance.The recursive algorithms for tree construction (except he final pruning) and retrievalare given in Figures 1 and 2.
For a detailed discussion, see Daelemans et al (1996).17Procedure SEARCH-IG-TI : tEE:Input:?
The root node N of a subtree (start value: top node of a complete IGTree),?
an unlabeled case I with information-gain-ordered feature values fi...fn (start value: fl...f,~).Output: A class label.1.
If N is a leaf node, output default class c associated with this node.2.
Otherwise, if test Fi of the current node does not originate an arc labeled with ffi, output defaultclass c associated with N.3.
Otherwise,?
new node M is the end node of the arc originating from N with as label fi.?
SEARCH-IG-TREE (M, f~+l...f,~)Figure 2: Algorithm for searching IGTrees ('SEARCH-IG-TREE').3.2 IGTree ComplexityThe asymptotic omplexity of IGTree (i.e, in the worst case) is extremely favorable.Complexity of searching a query pattern in the tree is proportional to F * log(V), whereF is the number of features (equal to the maximal depth of the tree), and V is the averagenumber of values per feature (i.e., the average branching factor in the tree).
In IB1, searchcomplexity is O(N * F) (with N the number of stored cases).
Retrieval by search in thetree is independent from the number of training cases, and therefore specially useful forlarge case bases.
Storage requirements are proportional to N (compare O(N * F) forIB1).
Finally, the cost of building the tree on the basis of a set of cases is proportional toN * log(V) ?
F in the worst case (compare O(N) for training in IB1).In practice, for our part-of-speech tagging experiments, IGTree retrieval is 100 to 200times faster than normal memory-based retrieval, and uses over 95% less memory.4 Arch i tec ture  of  the  TaggerThe architecture takes the form of a tagger generator, given a corpus tagged with thedesired tag set, a POS tagger is generated which maps the words of new text to tagsin this tag set according to the same systematicity.
The construction of a POS taggerfor a specific corpus is achieved in the following way.
Given an annotated corpus, threedatastructures are automatically extracted: a lexicon, a case base for known words (wordsoccurring in the lexicon), and a case base for unknown words.
Case Bases are indexedusing IGTree.
During tagging, each word in the text to be tagged is looked up in thelexicon.
If it is found, its lexical representation is retrieved and its context is determined,and the resulting pattern is looked up in the known words case base.
When a word isnot found in the lexicon, its lexical representation is computed on the basis of its form,its context is determined, and the resulting pattern is looked up in the unknown wordscase base.
In each case, output is a best guess of the category for the word in its currentcontext.
In the remainder of this section, we will describe each step in more detail.
Westart from a training set of tagged sentences T.184.1  Lex icon  Const ruct ionA lexicon is extracted from T by computing for each word in T the number of times itoccurs with each category.
E.g.
when using the first 2 million words of the Wall StreetJournal corpus 1 as T, the word once would get the lexical definition RB:  330; IN: 77,i.e.
once was tagged 330 times as an adverb, and 77 times as a preposition/subordinatingconjunction.
~Using these lexical definitions, a new, possibly ambiguous, tag is produced for eachword type.
E.g.
once would get a new tag, representing the category of words whichcan be both adverbs and prepositions/conjunctions (RB-IN).
Frequency order is takeninto account in this process: if there would be words which, like once, can be RB or IN,but more frequently IN than RB (e.g.
the word below), then a different tag (IN-RB) isassigned to these words.
The original tag set, consisting of 44 morphosyntactic tags, wasexpanded this way to 419 (possibly ambiguous) tags.
In the WSJ example, the resultinglexicon contains 57962 word types, 7464 (13%) of which are ambiguous.
On the sametraining set, 76% of word tokens are ambiguous.When tagging a new sentence, words are looked up in the lexicon.
Depending onwhether or not they can be found there, a case representation is constructed for them,and they are retrieved from either the known words case base or the unknown words casebase.4 .2  Known WordsA windowing approach (Sejnowski & Rosenberg, 1987) was used to represent the taggingtask as a classification problem.
A case consists of information about a focus word tobe tagged, its left and right context, and an associated category (tag) valid for the focusword in that context.There are several types of information which can be stored in the case base for eachword, ranging from the words themselves to intricate lexical representations.
In the pre-liminary experiments described in this paper, we limited this information to the possiblyambiguous tags of words (retrieved from the lexicon) for the focus word and its contextto the right, and the disambiguated tags of words for the left context (as the result ofearlier tagging decisions).
Table 1 is a sample of the case base for the first sentence of thecorpus (Pierre Vinken, 61 years old, will join the board as a nonexecutive director nov.29) when using this case representation.
The final column shows the target category; thedisambiguated tag for the focus word.
We will refer to this case representation as ddfat(d for disambiguated, f for focus, a for ambiguous, and t for target).
The informationgain values are given as well.A search among a selection of different context sizes suggested d fat  as a suitablecase representation for tagging known words.
An interesting property of memory-basedlearning is that case representations can be easily extended with different sources of in-formation if available (e.g.
feedback from a parser in which the tagger operates, semantictypes, the words themselves, lexical representations of words obtained from a differentsource than the corpus, etc.).
The information gain feature relevance ordering techniqueachieves a delicate relevance weighting of different information sources when they arefused in a single case representation.
The window size used by the algorithm will alsodynamically change depending on the information present in the context for the disam-biguation of a particular focus symbol (see Schfitze et al, 1994, and Pereira et al, 19951ACL Data Collection Initiative CD-ROM 1, September 1991.2We disregarded a category associated with a word when less than 10% of the word tokens were taggedwith that category.
This way, noise in the training material is filtered out.
The value for this parameter willhave to be adapted for other training sets, and was chosen here to maximise generalization accuracy (accuracyon tagging unseen text).19Table 1: Case representat ion and informat ion gain pat tern  for known words.word case representationd d f aIG .06 .22 .82 .23PierreVinken;1yearsold= ---- np np= np np ,cd np np ,cd nns np ,cd nns jj-npcd nns jj-np ,npnpcdnnsJJfor similar approaches).4 .3  Unknown WordsIf a word is not present in the lexicon, its ambiguous category cannot be retrieved.
In thatcase, a category can be guessed only on the basis of the form or the context of the word.Again, we take advantage of the data fusion capabilities of a memory-based approach bycombining these two sources of information in the case representation, and having theinformation gain feature relevance weighting technique figure out their relative relevance(see Schmid, 1994; Samuelsson, 1994 for similar solutions).In most taggers, some form of morphological nalysis is performed on unknown words,in an attempt o relate the unknown word to a known combination of known morphemes,thereby allowing its association with one or more possible categories.
After determin-ing this ambiguous category, the word is disambiguated using context knowledge, thesame way as known words.
Morphological analysis presupposes the availability of highlylanguage-specific resources uch as a morpheme lexicon, spelling rules, morphologicalrules, and heuristics to prioritise possible analyses of a word according to their plausi-bility.
This is a serious knowledge ngineering bottleneck when the goal is to develop alanguage and annotation-independent tagger generator.In our memory-based approach, we provide morphological information (especiallyabout suffixes) indirectly to the tagger by encoding the three last letters of the wordas separate features in the case representation.
The first letter is encoded as well becauseit contains information about prefix and capitalization of the word.
Context informationis added to the case representation i  a similar way as with known words.
It turned outthat in combination with the 'morphological' features, a context of one disambiguated tagof the word to the left of the unknown word and one ambiguous category of the word tothe right, gives good results.
We will call this case representation pdasss t :  3 three suffixletters (s), one prefix letter (p), one left disambiguated context words (d), and one am-biguous right context word (a).
As the chance of an unknown word being a function wordis small, and cases representing function words may interfere with correct classificationof open-class words, only open-class words are used during construction of the unknownwords case base.Table 2 shows part of the case base for unknown words.3These parameters (optimal context size and number of suffix features) were again optimised for general-ization accuracy.20Table 2: Case representat ion and informat ion gain pat tern  for unknown words.word case representationp d a s s s tIG .21 .21 .14 .15 .20 .32PierreVinken61yearsoldP = np r r e npV np , k e n np6 nns = 6 1 cdy cd jj-np a r s nnsO nns  , o 1 d jj4.4  Cont ro lFigure 3 shows the architecture of the tagger-generator: a tagger is produced by extractinga lexicon and two case-bases from the tagged example corpus.
During tagging, the controlis the following: words are looked up in the lexicon and separated into known and unknownwords.
They are retrieved from the known words case base and the unknown words casebase, respectively.
In both cases, context is used, in the case of unknown words, the firstand three last letters of the word are used instead of the ambiguous tag for the focusword.
As far as disambiguated tags for left context words are used, these are of coursenot obtained by retrieval from the lexicon (which provides ambiguous categories), but byusing the previous decisions of the tagger.TAGGER GENERATION TAGGINGTagged Corpus LEXICONword -> aKNOWN WORDSCASE BASEddfa - > tUNKNOWN WORDS' CASE BASEpdasss ->  tTAGGERNew Textb Tagged TextFigure 3: Architecture of the tagger-generator:  flow of control.4 .5  IGTrees  fo r  Tagg ingAs explained earlier, both case bases are implemented as IGTrees.
For the known wordscase base, paths in the tree represent variable size context widths.
The first feature(the expansion of the root node of the tree) is the focus word, then context features areadded as further expansions of the tree until the context disambiguates the focus wordcompletely.
Further expansion is halted at that point.
In some cases, short context sizes(corresponding to bigrams, e.g.)
are sufficient o disambiguate a focus word, in other cases,more context is needed.
IGTrees provide an elegant way of automatic determination of21optimal context size.
In the unknown words case base, the trie representation providesan automatic integration of information about the form and the context of a focus wordnot encountered before.
In general, the top levels of the tree represent the morphologicalinformation (the three suffix letter features and the prefix letter), while the deeper levelscontribute contextual disambiguation.5 ExperimentsIn this section, we report first results on our memory-based tagging approach.
In a firstset of experiments, we compared our IGTree implementation of memory-based learningto more traditional implementations of the approach.
In further experiments we studiedthe performance of our system on predicting the category of both known and unknownwords.Exper imenta l  Set -upThe experimental methodology was taken from Machine Learning practice (e.g.
Weiss& Kulikowski, 1991): independent training and test sets were selected from the origi-nal corpus, the system was trained on the training set, and the generalization accuracy(percentage of correct category assignments) was computed on the independent test set.Storage and time requirements were computed as well.
Where possible, we used a 10-foldcross-validation approach.
In this experimental method, a data set is partitioned tentimes into 90% training material, and 10% testing material.
Average accuracy provides areliable estimate of the generalization accuracy.5 .1  Exper iment  1: Compar i son  o f  A lgor i thmsOur goal is to adhere to the concept of memory-based learning with full memory whileat the same time keeping memory and processing speed within attractive bounds.
Tothis end, we applied the IGTree formalism to the task.
In order to prove that IGTree is asuitable candidate for practical memory-based tagging, we compared three memory-basedlearning algorithms: (i) IB1, a slight extension (to cope with symbolic values and ambigu-ous training items) of the well-known k-nn algorithm in statistical pattern recognition (seeAha et al, 1991), (ii) IBI-IG, an extension of IB1 which uses feature relevance weighting(described in Section 2), and (iii) IGTree, a memory- and processing time saving heuris-tic implementation of IBi- IG (see Section 3).
Table 3 lists the results in generalizationaccuracy, storage requirements and speed for the three algorithms using a ddfat  pattern,a 100,000 word training set, and a 10,000 word test set.
In this experiment, accuracy wastested on known words only.Table 3: Compar ison of three memory-based learning techniques.Algorithm Accuracy Time Memory (Kb)IB1 92.5 0:43:34 977IBI-IG 96.0 0:49:45 977i IGTree 96.0 0:00:29 35The IGTree version turns out to be better or equally good in terms of generalizationaccuracy, but also is more than 100 times faster for tagging of new words 4, and compresses4In training, i.e.
building the case base, IB1 and IBi-IG (4 seconds) are faster than IGTree (26 seconds)because the latter has to build a tree instead of just storing the patterns.22the original case base to 4% of the size of the original case base.
This experiment showsthat for this problem, we can use IGTree as a time and memory saving approximationof memory-based learning (IB-IG version), without loss in generalization accuracy.
Thetime and speed advantage of IGTree grows with larger training sets.5.2 Exper iment 2: Learning CurveA ten-fold cross-validation experiment on the first two million words of the WSJ corpusshows an average generalization performance of IGTree (on known words only) of 96.3%.We did 10-fold cross-validation experiments for several sizes of datasets (in steps of 100,000memory items), revealing the learning curve in Figure 4.
Training set size is on" the X-axis,generalization performance as measured in a 10-fold cross-validation experiment is on theY-axis.
the 'error' range indicate averages plus and minus one standard eviation on each10-fold cross-validation experiment.
50o)t~k~96.496.29695.895.695.495.29594.894.6Part of Speech Tagging Learning CurveI I I II I I I500 1000 1500 2000Training size (xlO00)Figure 4: Learning curve for tagging.Already at small data set sizes, performance is relatively high.
With increasinglylarger data sets, the performance becomes more stable (witness the error ranges).
Itshould be noted that in this experiment, we assumed correctly disambiguated tags in theleft context.
In practice, when using our tagger, this is of course not the case because thedisambiguated tags in the left context of the current word to be tagged are the result ofa previous decision of the tagger, which may be a mistake.
To test the influence of thiseffect we performed a third experiment.5 .3  Exper iment  3: Overa l l  AccuracyWe performed the complete tagger generation process on a 2 million words training set(lexicon construction and known and unknown words case-base construction), and testedon 200,000 test words.
Performance on known words, unknown words, and total are givenin Table 4.
In this experiment, numbers were not stored in the known words case base;they are looked up in the unknown words case base.~We are not convinced that variation in the results of the experiments in a 10-fold-cv set-up is statisticallymeaningful (the 10 experiments are not independent), but follow common practice here.23Table 4: Accuracy of IGTree tagging on known and unknown wordsAccuracy PercentageKnown 96.7 94.5Unknown 90.6 5.5Total 96.4 100.06 Re la ted  ResearchA case-based approach, similar to our memory-based approach, was also proposed byCardie (1993a, 1994) for sentence analysis in limited domains (not only POS tagging butalso semantic tagging and structural disambiguation).
We will discuss only the reportedPOS tagging results here.
Using a fairly complex case representation based on output fromthe CIRCUS conceptual sentence analyzer (22 local context features describing syntacticand semantic information about a five-word window centered on the word to be tagged,including the words themselves, and 11 global context features providing informationabout the major constituents parsed already), and with a tag set of 18 tags (7 open-class,11 closed class), she reports a 95% tagging accuracy.
A decision-tree l arning approachto feature selection is used in this experiment (Cardie, 1993b, 1994) to discard irrelevantfeatures.
Results are based on experiments with 120 randomly chosen sentences fromthe TIPSTER JV corpus (representing 2056 cases).
Cardie (p.c.)
reports 89.1% correcttagging for unknown words.
Percentage unknown words was 20.6% of the test words,and overall tagging accuracy (known and unknown) 95%.
Notice that her algorithmgives no initial preference to training cases that match the test word during its initialcase retrieval.
On the other hand, after retrieving the top k cases, the algorithm doesprefer those cases that match the test word when making its final predictions.
So, it'sunderstandable that the algorithm is doing better on words that it's seen during trainingas opposed to unknown words.In our memory-based approach, feature weighting (rather than feature selection) fordetermining the relevance of features is integrated more smoothly with the similaritymetric, and our results are based on experiments with a larger corpus (3 million cases).Our case representation is (at this point) simpler: only the (ambiguous) tags, not thewords themselves or any other information are used.
The most important improvement isthe use of IGTree to index and search the case base, solving the computational complexityproblems a case-based approach would run into when using large case bases.An approach based on k-nn methods (such as memory-based and case-based methods)is a statistical pproach, but it uses a different kind of statistics than Markov model-basedapproaches.
K-nn is a non-parametric technique; it assumes no fixed type of distributionof the data.
The most important advantages compared to current stochastic approachesare that (i) few training items (a small tagged corpus) are needed for relatively goodperformance, (ii) the approach is incremental: adding new cases does not require anyrecomputation f probabilities, and (iii) it provides explanation capabilities, and (iv) itrequires no additional smoothing techniques to avoid zero-probabilities; the IGTree takescare of that.Compared to hand-crafted rule-based approaches, our approach provides a solutionto the knowledge-acquisition and reusability bottlenecks, and to robustness and cover-age problems (similar advantages motivated Markov model-based statistical approaches).Compared to learning rule-based approaches such as the one by Brill (1992), a k-nn ap-proach provides a uniform approach for all disambiguation tasks, more flexibility in theengineering ofcase representations, and a more elegant approach to handling of unknownwords (see e.g.
Cardie 1994).247 Conclus ionWe have shown that a memory-based approach to large-scale tagging is feasible bothin terms of accuracy (comparable to other statistical approaches), and also in terms ofcomputational efficiency (time and space requirements) when using IGTree to compressand index the case base.
The approach combines ome of the best features of learnedrule-based and statistical systems (small training corpora needed, incremental learning,understandable and explainable behavior of the system).
More specifically, memory-basedtagging with IGTrees has the following advantages.?
Accurate generalization from small tagged corpora.
Already at small corpus size(300-400 K tagged words), performance is good.
These corpus sizes can be easilyhandled by our system.?
Incremental learning.
New 'cases' (e.g.
interactively corrected output of the tagger)can be incrementally added to the case bases, continually improving the performanceof the overall system.?
Explanation capabilities.
To explain the classification behavior of the system, a pathin the IGTree (with associated efaults) can be provided as an explanation, as wellas nearest neighbors from which the decision was extrapolated.?
Flexible integration of information sources.
The feature weighting method takescare of the optimal fusing of different sources of information (e.g.
word form andcontext), automatically.?
Automatic selection of optimal context.
The IGTree mechanism (when applied tothe known words case base) automatically decides on the optimal context size fordisambiguation of focus words.?
Non-parametric estimation.
The IGTree formalism provides automatic, nonparametricestimation of classifications for low-frequency contexts (it is similar in this respectto backed-off training), but avoids non-optimal estimation due to false intuitions ornon-convergence of the gradient-descent procedure used in some versions of backed-off training.?
Reasonably good results on unknown words without morphological nalysis.
Onthe WSJ corpus, unknown words can be predicted (using context and word forminformation) for more than 90%.?
Fast learning and tagging.
Due to the favorable complexity properties of IGTrees(lookup time in IGTrees is independent on number of cases), both tagger generationand tagging are extremely fast.
Tagging speed in our current implementation isabout 1000 words per second.We have barely begun to optimise the approach: a more intelligent similarity metricwould also take into account he differences in similarity between different values of thesame feature.
E.g.
the similarity between the tags rb-in-nn and rb-in should be bigger thanthe similarity between rb-in and vb-nn.
Apart from linguistic engineering refinements ofthe similarity metric, we are currently experimenting with statistical measures to computesuch more fine-grained similarities (e.g.
Stanfill & Waltz, 1986, Cost & Salzberg, 1994).AcknowledgementsResearch of the first author was done while he was a visiting scholar at NIAS (NetherlandsInstitute for Advanced Studies) in Wassenaar.
Thanks to Antal van den Bosch, TonWeijters, and Gert Durieux for discussions about tagging, IGTree, and machine learningof natural anguage.25ReferencesAha, D. W., Kibler, D., & Albert, M. (1991).
'Instance-based learning algorithms'.Machine Learning, 7, 37-66.Brill, E. (1992) 'A simple rule-based part-of-speech tagger'.
Proceedings Third ACLApplied, Trento, Italy, 152-155.Cardie, C. (1993a).
'A case-based approach to knowledge acquisition for domain-specificsentence analysis'.
In AAAL93, 798-803.Cardie, C. (1993b).
'Using Decision Trees to Improve Case-Based Learning'.
In Pro-ceedings of the Tenth International Conference on Machine Learning, 25-32.Cardie, C. (1994).
'Domain-Specific Knowledge Acquisition for Conceptual SentenceAnalysis'.
Ph.D. Thesis, University of Massachusetts, Amherst, MA.Chandler, S. (1992).
'Are rules and modules really necessary for explaining language?
'Journal of Psycholinguistic research, 22(6): 593-606.Church, K. (1988).
'A stochastic parts program and noun phrase parser for unrestrictedtext'.
Proceedings Second A CL Applied NLP, Austin, Texas, 136-143.Cost, S. and Salzberg, S. (1993).
'A weighted nearest neighbour algorithm for learningwith symbolic features.'
Machine Learning, 10, 57-78.Cutting, D., Kupiec, J., Pederson, J., Sibun, P. (1992).
A practical part of speech tagger.Proceedings Third A CL Applied NLP, Trento, Italy, 133-140.Daelemans, W. (1995).
'Memory-based lexical acquisition and processing.'
In Steffens,P., editor, Machine Translation and the Lexicon, Lecture Notes in Artificial Intelli-gence 898.
Berlin: Springer, 85-98.Daelemans, W., Van den Bosch, A.
(1992).
'Generalisation performance of backprop-agation learning on a syllabification task.'
In M. Drossaers & A. Nijholt (Eds.
),TWLT3: Connectionism and Natural Language Processing.
Enschede: Twente Uni-versity, 27-38.Daelemans, W., Van den Bosch, A., Weijters, T. (1996).
'IGTree: Using Trees forCompression and Classification in Lazy Learning Algorithms.'
In Aha, D.
(ed.).
AIReview Special Issue on Lazy Learning, forthcoming.DeRose, S. (1988).
'Grammatical category disambiguation by statistical optimization.
'Computational Linguistics 14, 31-39.Derwing, B. L. and Skousen, R. (1989).
'Real Time Morphology: Symbolic Rules orAnalogical Networks'.
Berkeley Linguistic Society 15: 48-62.Federici S. and V. Pirelli.
(1996).
'Analogy, Computation and Linguistic Theory.'
InJones, D.
(ed.)
New Methods in Language Processing.
London: UCL Press, forth-coming.Garside, R., Leech, G. and Sampson, G. (1987).
The computational nalysis of English:A corpus-based approach, London: Longman, 1987.Greene, B.B.
and l~ubin, G.M.
(1971).
Automatic Grammatical Tagging of English.Providence RI: Department of Linguistics, Brown University.Hindle, Donald.
(1989).
'Acquiring disambiguation rules from text.'
In Proceedings,27th Annual Meeting of the Association for Computational Linguistics, Vancouver,BC.Hunt, E., J. Matin, P. Stone.
(1966).
Experiments in Induction.
New York: AcademicPress.Jones, D. Analogical Natural Language Processing.
London: UCL Press, 1996.26Klein S. and Simmons, R. (1963).
'A grammatical pproach to grammatical coding ofEnglish words.'
JACM 10, 334-347.Kolodner, J.
(1993).
Case-Based Reasoning.
San Mateo: Morgan Kaufmann.Langley, P. and Sage, S. (1994).
'Oblivious decision trees and abstract cases.'
InD.
W. Aha (Ed.
), Case-Based Reasoning: Papers from the 1994 Workshop (Techni-cal Report WS-94-01).
Menlo Park, CA: AAAI Press.Merialdo, B.
(1994).
'Tagging English Text with a Probabilistic Model.'
ComputationalLinguistics 20 (2), 155-172.Pereira, F., Y.
Singer, N. Tishby.
(1995).
'Beyond Word N-grams.'
Proceedings ThirdWorkshop on Very Large Corpora, MIT, Cambridge Mass., 95-106.Quinlan, J.
(1993).
C4.5: Programs for Machine Learning.
San Mateo, CA: MorganKaufmann.Salzberg, S. (1990) 'A nearest hyperrectangle learning method'.
Machine Learning 6,251-276.Samuelsson, C. (1994) 'Morphological Tagging Based Entirely on Bayesian Inference.'
InProceedings of the 9th Nordic Conference on Computational Linguistics, StockholmUniversity, Sweden, 1994.Scha, R. (1992) 'Virtuele Grammatica's en Creatieve Algoritmen.'
Gramma/TTT 1 (1),57-77.Schmid, H. (1994) 'Part-of-speech tagging with neural networks.'
In Proceedings ofCOLING, Kyoto, Japan.Schfitze, H., and Y.
Singer.
(1994) 'Part-of-speech Tagging Using a Variable ContextMarkov Model' Proceedings ofACL 1994, Las Cruces, New Mexico.Skousen, R. (1989).
Analogical Modeling of Language.
Dordrecht: Kluwer.Sejnowski, T. J., Rosenberg, C. S. (1987).
Parallel networks that learn to pronounceEnglish text.
Complex Systems, 1, 145-168.Stanfill, C. and Waltz, D. (1986).
'Toward memory-based reasoning.'
Communicationsof the ACM, 29, 1212-1228.Weiss, S. and Kulikowski, C. (1991).
Computer systems that learn.
San-Mateo: MorganKaufmann.27
