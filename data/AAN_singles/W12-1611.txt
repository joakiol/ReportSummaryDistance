Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 84?93,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsReinforcement Learning of Question-Answering Dialogue Policiesfor Virtual Museum GuidesTeruhisa Misu1?, Kallirroi Georgila2, Anton Leuski2, David Traum21National Institute of Information and Communications Technology (NICT), Kyoto, Japan2USC Institute for Creative Technologies, Playa Vista, CA, USAteruhisa.misu@nict.go.jp, {kgeorgila,leuski,traum}@ict.usc.eduAbstractWe use Reinforcement Learning (RL) to learnquestion-answering dialogue policies for areal-world application.
We analyze a corpusof interactions of museum visitors with twovirtual characters that serve as guides at theMuseum of Science in Boston, in order tobuild a realistic model of user behavior wheninteracting with these characters.
A simulateduser is built based on this model and usedfor learning the dialogue policy of the virtualcharacters using RL.
Our learned policy out-performs two baselines (including the originaldialogue policy that was used for collectingthe corpus) in a simulation setting.1 IntroductionIn the last 10 years Reinforcement Learning (RL)has attracted much attention in the dialogue commu-nity, to the extent that we can now consider RL as thestate-of-the-art in statistical dialogue management.RL is used in the framework of Markov DecisionProcesses (MDPs) or Partially Observable MarkovDecision Processes (POMDPs).
In this paradigmdialogue moves transition between dialogue statesand rewards are given at the end of a successful dia-logue.
The goal of RL is to learn a dialogue policy,i.e.
the optimal action that the system should take ateach possible dialogue state.
Typically rewards de-pend on the domain and can include factors such astask completion, dialogue length, and user satisfac-tion.
Traditional RL algorithms require on the order?
This work was done when the first author was a visitingresearcher at USC/ICT.of thousands of dialogues to achieve good perfor-mance.
Because it is very difficult to collect such alarge number of dialogues with real users, instead,simulated users (SUs), i.e.
models that simulate thebehavior of real users, are employed (Georgila et al,2006).
Through the interaction between the systemand the SUs thousands of dialogues can be gener-ated and used for learning.
A good SU should beable to replicate the behavior of a real user in thesame dialogue context (Ai and Litman, 2008).Most research in RL for dialogue managementhas been done in the framework of slot-filling appli-cations (Georgila et al, 2010; Thomson and Young,2010), largely ignoring other types of dialogue.
Inthis paper we focus on the problem of learning di-alogue policies for question-answering characters.With question-answering systems (or characters),the natural language understanding task is to retrievethe best response to a user initiative, and the maindialogue policy decision is whether to provide thisbest response or some other kind of move (e.g.
a re-quest for repair, clarification, or topic change), whenthe best answer does not seem to be good enough.Note that often in the literature the term question-answering is used for slot-filling dialogue systemsas well, in the sense that the user asks some ques-tions, for example, about restaurants in a particulararea, and the system answers by providing a list ofoptions, for example, restaurants.
We use the term?question-answering?
for systems where user ques-tions can be independent of one another (follow-up questions are possible though) and do not havethe objective of reducing the search space and re-trieving results from a database of e.g.
restaurants,flights, etc.
Thus examples of question-answering84characters can be virtual interviewees (that can an-swer questions, e.g.
about an incident), virtual scien-tists (that can answer general science-related ques-tions), and so forth.For our experiments we use a corpus (Aggarwalet al, 2012) of interactions of real users with twovirtual characters, the Twins, that serve as guides atthe Museum of Science in Boston (Swartout et al,2010).
The role of these virtual characters is to en-tertain and educate the museum visitors.
They cananswer queries about themselves and their technol-ogy, generally about science, as well as questionsrelated to the exhibits of the museum.
An exampleinteraction between a museum visitor and the Twinsis shown in Figure 1.
The dialogue policy of theTwins was arbitrarily hand-crafted (see section 7 fordetails) and many other policies are possible (includ-ing Baseline 2, presented in section 7, and takingmore advantage of question topics and context).
Wepropose to use RL for optimizing the system?s re-sponse generation.
This is a real-world applicationfor which RL appears to be an appropriate method.Although there are similarities between question-answering and slot-filling dialogues there are also anumber of differences, such as the reward functionand the behavior of the users.
As discussed later indetail, in question-answering the users have a num-ber of questions that they are planning to ask (stockof queries), which can be increased or decreased de-pending not only on whether they received the in-formation that they wanted but also on how satisfiedthey are with the interaction.
The system has to planahead in order to maximize the number of success-ful responses that it provides to user queries.
At thesame time it needs to avoid providing incorrect orincoherent responses so that the user does not giveup the interaction.One of the challenges of our task is to define anappropriate reward function.
Unlike slot-filling dia-logues, it is not clear what makes an interaction witha question-answering system successful.
A secondchallenge is that in a museum setting it is not clearwhat constitutes a dialogue session.
Often two ormore users alternate in asking questions, which fur-ther complicates the problem of defining a good re-ward function.
A third challenge is that the domainis not well defined, i.e.
users do not know in advancewhat the system is capable of (what kind of ques-tions the characters can answer).
Moreover, thereUser: What are your names?
(ASR: what areyour names)Ada: My name?s Ada.Grace: And I?m Grace.
We?re your Virtual Mu-seum Guides.
With your help, we can suggest ex-hibits that will get you thinking!
Or answer ques-tions about things you may have seen here.Ada: What do you want to learn about?User: Artificial intelligence.
(ASR: isartificial intelligence)Grace: One example of AI, or Artificial Intelli-gence, is 20Q, an online computer activity here atComputer Place that asks you questions to guesswhat you?re thinking.Ada: I wish we?d been programmed to do that.Nah.
.
.
on second thought, I prefer just answeringyour questions.Grace: That takes AI too.Figure 1: Example dialogue between the Twins virtualcharacters and a museum visitor.are many cases of ?junk?
user questions (e.g.
?areyou stupid??)
or even user prompts in languagesother than English (e.g.
?hola?
).We first analyze our corpus in order to build a re-alistic model of user behavior when interacting withthe virtual characters.
A SU is built based on thismodel and used for learning the dialogue policy ofthe virtual characters using RL.
Then we compareour learned policy with two baselines, one of whichis the dialogue policy of the original system that wasused for collecting our corpus and that is currentlyinstalled at the Museum of Science in Boston.
Ourlearned policy outperforms both baselines in a sim-ulation setting.To our knowledge this is the first study that usesRL for learning this type of question-answering dia-logue policy.
Furthermore, unlike most studies thatuse data collected by having paid subjects interactwith the system, we use data collected from realusers, in our case museum visitors.1 We also com-pare our learned dialogue policy with the dialoguepolicy of the original system that is currently in-stalled at the Museum of Science in Boston.The structure of the paper is as follows.
In sec-1Note that the CMU ?Let?s Go!?
corpus is another case ofusing real user data for learning dialogue policies for the SpokenDialogue Challenge.85tion 2 we present related work.
Section 3 provides abrief introduction to RL and section 4 describes ourcorpus.
Then in section 5 we explain how we builtour SU from the corpus, and in section 6 we describeour learning methodology.
Section 7 presents ourevaluation results.
Finally section 8 presents somediscussion and ideas for future work together withour conclusion.2 Related WorkTo date, RL has mainly been used for learning di-alogue policies for slot-filling applications such asrestaurant recommendations (Jurc??
?c?ek et al, 2012),sightseeing recommendations (Misu et al, 2010),appointment scheduling (Georgila et al, 2010), etc.,largely ignoring other types of dialogue.
Recentlythere have been some experiments on applying RLto the more difficult problem of learning negotia-tion policies (Heeman, 2009; Georgila and Traum,2011a; Georgila and Traum, 2011b).
Also, RL hasbeen applied to tutoring domains (Tetreault and Lit-man, 2008; Chi et al, 2011).There has been a lot of work on developingquestion-answering systems with dialogue capabil-ities, e.g.
(Jo?nsson et al, 2004; op den Akker et al,2005; Varges et al, 2009).
Most of these systems aredesigned for information extraction from structuredor unstructured databases in closed or open domains.One could think of them as adding dialogue capa-bilities to standard question-answering systems suchas the ones used in the TREC question-answeringtrack (Voorhees, 2001).
Other work has focused ona different type of question-answering dialogue, i.e.question-answering dialogues that follow the formof an interview and that can be used, for example,for training purposes (Leuski et al, 2006; Gandhe etal., 2009).
But none of these systems uses RL.To our knowledge no one has used RL for learningpolicies for question-answering systems as definedin section 1.
Note that Rieser and Lemon (2009)used RL for question-answering, but in their case,question-answering refers to asking for informationabout songs and artists in an mp3 database, whichis very much like a slot-filling task, i.e.
the systemhas to fill a number of slots (e.g.
name of band, etc.
)in order to query a database of songs and presentthe right information to the user.
As discussed insection 1 our task is rather different.3 Reinforcement LearningA dialogue policy is a function from contexts to(possibly probabilistic) decisions that the dialoguesystem will make in those contexts.
ReinforcementLearning (RL) is a machine learning technique usedto learn the policy of the system.
For an RL-baseddialogue system the objective is to maximize the re-ward it gets during an interaction.
RL is used in theframework of Markov Decision Processes (MDPs)or Partially Observable Markov Decision Processes(POMDPs).In this paper we follow a POMDP-based ap-proach.
A POMDP is defined as a tuple (S, A, P , R,O, Z, ?, b0) where S is the set of states (representingdifferent contexts) which the system may be in (thesystem?s world),A is the set of actions of the system,P : S ?
A ?
P (S, A) is the set of transition prob-abilities between states after taking an action, R : S?
A ?< is the reward function, O is a set of obser-vations that the system can receive about the world,Z is a set of observation probabilities Z : S ?
A?
Z(S, A), and ?
a discount factor weighting long-term rewards.
At any given time step i the worldis in some unobserved state si ?
S. Because si isnot known exactly, we keep a distribution over statescalled a belief state b, thus b(si) is the probability ofbeing in state si, with initial belief state b0.
Whenthe system performs an action ?i ?
A based on b,following a policy pi : S ?
A, it receives a rewardri(si, ?i) ?
< and transitions to state si+1 accord-ing to P (si+1|si, ?i) ?
P .
The system then receivesan observation oi+1 according to P (oi+1|si+1, ?i).The quality of the policy pi followed by the agent ismeasured by the expected future reward also calledQ-function, Qpi : S ?
A ?
<.There are several algorithms for learning the opti-mal dialogue policy and we use Natural Actor Critic(NAC) (Peters and Schaal, 2008), which adopts anatural policy gradient method for policy optimiza-tion, also used by (Thomson and Young, 2010;Jurc??
?c?ek et al, 2012).
Policy gradient methods donot directly update the value of state S orQ-function(expected future reward).
Instead, the policy pi (orparameter?, see below) is directly updated so as toincrease the reward of dialogue episodes generatedby the previous policy.A system action asys is sampled based on the fol-lowing soft-max (Boltzmann) policy:86pi(asys = k|?)
= Pr(asys = k|?,?
)= exp(?Ii=1 ?i ?
?ki)?Jj=1 exp(?Ii=1 ?i ?
?ji)Here, ?
= (?1, ?2, .
.
.
, ?I) is a basis func-tion, which is a vector function of the belief state.?
= (?11, ?12, .
.
.
?1I , .
.
.
, ?JI ) consists of J (# ac-tions) ?
I (# features) parameters.
The parameter?ji works as a weight for the i-th feature of the ac-tion j and determines the likelihood that the action jis selected.
?
is the target of optimization by RL.During training, RL algorithms require thousandsof interactions between the system and the userto achieve good performance.
For this reason weneed to build a simulated user (SU) (Georgila et al,2006), that will behave similarly to a real user, andwill interact with the policy for thousands of itera-tions to generate data in order to explore the searchspace and thus facilitate learning.Topic Example user question/promptintroduction Hello.personal Who are you named after?school Where do you go to school?technology What is artificial intelligence?interfaces What is a virtual human?exhibition What can I do at Robot Park?Table 1: Topics of user questions/prompts.4 The Twins CorpusAs mentioned in section 1 the Twins corpus (Aggar-wal et al, 2012) was collected at the Museum of Sci-ence in Boston (Swartout et al, 2010).
The Twinscan answer a number of user questions/prompts inseveral topics, i.e.
about themselves and their tech-nology, about science in general, and about exhibitsin the museum.
We have divided these topics in sixcategories shown in Table 1 together with an exam-ple for each category.An example interaction between a museum vis-itor and the Twins is shown in Figure 1.
We canalso see the output of the speech recognizer.
In thepart of the corpus that we use for our experimentautomatic speech recognition (ASR) was performedby Otosense, an ASR engine developed by the USCSAIL lab.
Natural language understanding and di-alogue management are both performed as a singletask by the NPCEditor (Leuski and Traum, 2010),a text classification system that classifies the user?squery to a system?s answer using cross-language in-formation retrieval techniques.
When the systemfails to understand the user?s query it can prompt herto do one of the following:?
rephrase her query (from now on referred toas off-topic response 1, OT1), e.g.
?pleaserephrase your question?;?
prompt the user to ask a particular question thatthe system knows that it can handle (from nowon referred to as off-topic response 2, OT2),e.g.
?you may ask us about our hobbies?;?
cease the dialogue and check out the ?behindthe scenes?
exhibit which explains how the vir-tual characters work (from now on referred toas off-topic response 3, OT3).The Twins corpus contains about 200,000 spokenutterances from museum visitors (primarily chil-dren) and members of staff or volunteers.
For thepurposes of this paper we used 1,178 dialogue ses-sions (11,074 pairs of user and system utterances)collected during March to May 2011.
This subsetof the corpus contains manual transcriptions of userqueries, system responses, and correct responses touser queries (the responses that the system shouldgive when ASR is perfect).5 User Simulation ModelIn order to build a model of user behavior we per-form an analysis of the corpus.
One of our chal-lenges is that the boundaries between dialogue ses-sions are hard to define, i.e.
it is very hard to auto-matically calculate whether the same or a new userspeaks to the system, unless complex voice iden-tification techniques are employed.
We make thereasonable assumption that a new dialogue sessionstarts when there are no questions to the system fora time interval greater than 120 sec.From each session we extract 30 features.
A fulllist is shown in Table 7 in the Appendix.
Our goalis to measure the contribution of each feature tothe user?s decision with respect to two issues: (1)whether the user will cease the dialogue or not, and(2) what kind of query the user will make next, based87on what has happened in the dialogue so far.
To dothat we use the Chi-squared test, which is commonlyused for feature selection.So to measure the contribution of each feature towhether the user will cease the dialogue or not, wegive a binary label to each user query in our corpus,i.e.
1 when the query is the last user query in the di-alogue session and 0 otherwise.
Then we calculatethe contribution of each feature for estimating thislabel.
In Table 8, column 1, in the Appendix, we cansee the 10 features that contribute the most to pre-dicting whether the user will cease the dialogue.
Aswe can see the dominant features are not whetherthe system correctly responded to the user?s query,but mostly features based on the dialogue history(e.g.
the number of the system?s off-topic responsesso far) and user type information.
Indeed, a furtheranalysis of the corpus showed that children tend tohave longer dialogue sessions than adults.Our next step is the estimation of the contributionof each feature for predicting the user?s next query.The label we predict here is the topic of the user?sutterance (personal, exhibition, etc., see Table 1).We can see the 10 most predictive features in Ta-ble 8, column 2, in the Appendix.
The contributionof the most recent user?s utterance (previous topiccategory) is larger than that of dialogue history fea-tures.
This tendency is the same when we ignore re-peated user queries, e.g.
when the system makes anerror and the user rephrases her query (see Table 8,column 3, in the Appendix).
The user type is impor-tant for predicting the next user query.
In Figure 2we can see the percentages of user queries per usertype and topic.Based on the above analysis we build a simulateduser (SU).
The SU simulates the following:?
User type (child, male, female): a child useris sampled with a probability of 51.1%, a malewith 31.1%, and a female with 17.8%.
Theseprobabilities are estimated from the corpus.?
Number of questions the user is planning toask (stock of queries): We assume here thatthe user is planning to ask a number of ques-tions.
This number may increase or decrease.For example, it can increase when the systemprompts the user to ask about a particular topic(OT2 prompt), and it may decrease when theuser decides to cease the dialogue immediately.Figure 2: Percentages of user queries per user type andtopic.The number of questions is sampled from auser type dependent Zipf distribution (strictlyspeaking the continuous version of the distri-bution; Parato distribution) the parameter ofwhich is estimated from the corpus using themaximum likelihood criterion.
We chose Zipfbecause it is a long-tail distribution that fits ourdata (users are not expected to ask a large num-ber of questions).
According to this distributiona child user is more likely to have a larger stockof queries than a male or female adult.?
User?s reaction: The user has to decide onone of the following.
Go to the next topic(Go-on); cease the dialogue if there are nomore questions in the stock of queries (Out-of-stock); rephrase the previous query (Rephrase);abandon the dialogue (Give-up) regardless ofthe remaining questions in the stock; gener-ate a query based on a system recommenda-tion, OT2 prompt (Refill).
We calculate theuser type dependent probability for these ac-tions from the corpus.
But the problem hereis that it is not possible to distinguish be-tween the case in which the user asked all thequestions in the stock of queries (i.e.
all thequestions she intended to ask) and left, fromthe case in which she gave up and abandonedthe dialogue.
We estimate the percentage of?Give-up?
as the difference between the ratio of?Cease?
after an incorrect response and the ra-88tio of ?Cease?
after a correct response, assum-ing a similar percentage of ?Out-of-stock?
forboth correct and incorrect responses.
Likewise,the difference in ?Go-on?
for OT2 and other re-sponses is attributed to ?Refill?.
The probabil-ity of ?Rephrase?
is estimated from the corpus.For example the probability that a child willrephrase after an OT1 system prompt is 54%,after an erroneous system prompt 38%, etc.?
Topic for next user query (e.g.
introduction,personal, etc.
): The SU selects a new topicbased on user type dependent topic transitionbigram probabilities estimated from the corpus.?
User utterance: The SU selects a user utter-ance from the corpus that matches the currentuser type and topic.
We have split the corpusin groups of user utterances based on user typeand topic and we sample accordingly.?
Utterance timing: We simulate utterance tim-ing (duration of pause between system utter-ance and next user query) per user type anduser change.
The utterance timing is sampledbased on a Gaussian distribution the parametersof which are set based on the corpus statistics.For example, the average duration of a sessionuntil the user changes is 62.7 sec with a stan-dard deviation of 71.2 sec.6 Learning Question-Answering PoliciesOur goal is to use RL in order to optimize the sys-tem?s response generation.
As we saw in the previ-ous section the SU generates a user utterance fromour corpus.
We do not currently use ASR error sim-ulation but instead a real ASR engine.
So the au-dio file that corresponds to the selected user utter-ance is forwarded to 3 ASR systems, with child,male, and female acoustic models (AMs) respec-tively.
Then these recognition results are forwardedto the NPCEditor that produces an N-best list of pos-sible system responses (retrieval results).
That is,as mentioned in section 4, the NPCEditor classifieseach ASR result to a system answer using cross-language information retrieval techniques.
The pol-icy can choose one of the NPCEditor retrieval re-sults or reject them and instead present one of thethree off-topic prompts (OT1, OT2, or OT3).
So thesystem has 10 possible actions to choose between:?
use the response with the best or the secondbest score retrieved from the NPCEditor basedon a child AM (2 actions);?
use the response with the best or the secondbest score retrieved from the NPCEditor basedon a male AM (2 actions);?
use the response with the best or the secondbest score retrieved from the NPCEditor basedon a female AM (2 actions);?
use the response with the best of the 6 afore-mentioned scores of the NPCEditor;?
use off-topic prompt OT1;?
use off-topic prompt OT2;?
use off-topic prompt OT3.We use the following features to optimize our di-alogue policy (see section 3).
We use the 6 retrievalscores of the NPCEditor (the 2 best scores for eachuser type ASR result), the previous system action,the ASR confidence scores, the voting scores (calcu-lated by adding the scores of the results that agree),the system?s belief on the user type and user change,and the system?s belief on the user?s previous topic.So we need to learn a POMDP-based policy usingthese 42 features.Unlike slot-filling dialogues, defining the rewardfunction is not a simple task (e.g.
reward the systemfor filled and confirmed slots).
So in order to definethe reward function and thus measure the quality ofthe dialogue we set up a questionnaire.
We asked5 people to rate 10 dialogues in a 5-Likert scale.Each dialogue session included 5 question-answerpairs.
Then we used regression analysis to set thereward for each of the question-answer pair cate-gories shown in Table 2.
So for example, respondingcorrectly to an in-domain user question is rewarded(+23.2) whereas providing an erroneous response toa junk question, i.e.
treating junk questions as if theywere in-domain questions, is penalized (-14.7).One limitation of this reward function (Rewardfunction 1) is that it does not take into accountwhether the user has previously experienced an off-topic system prompt.
To account for that we defineReward function 2.
Here we consider the numberof off-topic responses in the two most recent systemprompts.
Reward function 2 is shown in Table 3.89QA Pair Rewardin-domain ?
correct 23.2in-domain ?
error -12.2in-domain ?
OT1 -5.4in-domain ?
OT2 -8.4in-domain ?
OT3 -9.6junk question ?
error -14.7junk question ?
OT1 4.8junk question ?
OT2 10.2junk question ?
OT3 6.1give up -16.9Table 2: Reward function 1.QA Pair Rewardin-domain ?
correct 16.9in-domain ?
error -2.0in-domain ?
OT1 13.9in-domain ?
OT1(2) 7.3in-domain ?
OT2 -7.9in-domain ?
OT2(2) 4.2in-domain ?
OT3 -15.8in-domain ?
OT3(2) -8.3junk question ?
error -4.6junk question ?
OT1 4.1junk question ?
OT1(2) 4.1junk question ?
OT2 43.4junk question ?
OT2(2) -33.1junk question ?
OT3 3.1junk question ?
OT3(2) 6.1give up -19.5Table 3: Reward function 2.As we can see, providing an OT2 as the first off-topic response is a poor action (-7.9); it is preferableto ask the user to rephrase her question (OT1) as afirst attempt to recover from the error (+13.9).
Onthe other hand, providing an OT2 prompt, after anoff-topic prompt has occured in the previous systemprompt, is a reasonable action (+4.2).7 EvaluationWe compare our learned policy with two baselines.The first baseline, Baseline 1, is the dialogue pol-icy that is used by our system that is currently in-stalled at the Museum of Science in Boston.
Base-line 1 selects the best ASR result (i.e.
the resultwith the highest confidence score) out of the resultswith the 3 different AMs (child, male, and female),and forwards this result to the NPCEditor to retrievethe system?s response.
If the NPCEditor score ishigher than an emprically set pre-defined threshold(see (Leuski and Traum, 2010) for details), then thesystem presents the retrieved response, otherwise itpresents an off-topic prompt.
The system presentsthese off-topic prompts in a fixed order.
First, OT1,then OT2, and then OT3.We also have Baseline 2, which forwards all 3ASR results to the NPCEditor (using child, male,and female AMs).
Then the NPCEditor retrieves 3results, one for each one of the 3 ASR results, andselects the retrieved result with the highest score.Again if this score is higher than a threshold, the sys-tem will present this result, otherwise it will presentan off-topic prompt.Each policy interacts with the SU for 10,000 di-alogue sessions and we calculate the average accu-mulated reward for each dialogue.
In Tables 4 and 5we can see our results for Reward functions 1 and 2respectively.
In both cases the learned policy outper-forms both baselines.
For both reward functions themost predictive feature is the ASR confidence scorewhen combined with the NPCEditor?s retrieval scoreand the previous system action.
Also, for both re-ward functions the second best feature is ?voting?when combined with the retrieval score and the pre-vious system action.In Table 6 we can see how often the learned pol-icy, which is based on Reward function 1 using allfeatures, selects each one of the 10 system actions(200,000 system turns in total).Policy Avg RewardBaseline 1 24.76 (19.29)Baseline 2 51.63 (49.84)Learned Policy - FeaturesRetrieval score+ system action (*) 46.74(*) + ASR confidence score 61.59(*) + User type probability 47.28(*) + Estimated previous topic 47.87(*) + Voting 59.94All features 60.93Table 4: Results with reward function 1.
The values inparentheses for Baselines 1 and 2 are the rewards whenthe NPCEditor does not use the pre-defined threshold.90Policy Avg RewardBaseline 1 39.40 (38.51)Baseline 2 55.45 (54.49)Learned Policy - FeaturesRetrieval score+ system action (*) 49.15(*) + ASR confidence score 69.51(*) + User type probability 50.15(*) + Estimated previous topic 49.84(*) + Voting 69.06All features 73.59Table 5: Results with reward function 2.
The values inparentheses for Baselines 1 and 2 are the rewards whenthe NPCEditor does not use the pre-defined threshold.System Action FrequencyChild + 1st best score 10.33%Child + 2nd best score 2.70%Male + 1st best score 13.72%Male + 2nd best score 1.03%Female + 1st best score 39.73%Female + 2nd best score 0.79%Best of scores 1-6 2.38%OT1 11.01%OT2 6.86%OT3 11.45%Table 6: Frequency of the system actions of the learnedpolicy that is based on Reward function 1 using all fea-tures.8 Discussion and ConclusionWe showed that RL is a promising technique forlearning question-answering policies.
Currently weuse the same SU for both training and testing thepolicies.
One could argue that this favors the learnedpolicy over the baselines.
Because our SU is basedon general corpus statistics (probability that the useris child or male or female, number of questions theuser is planning to ask, probability of moving to thenext topic or ceasing the dialogue, utterance timingstatistics) rather than sequential information we be-lieve that this is acceptable.
We only use sequentialinformation when we calculate the next topic thatthe user will choose.
That is, due to the way theSU is built and its randomness, we believe that it isvery unlikely that the same patterns that were gener-ated during training will be generated during testing.Thus we do not anticipate that our results would bedifferent if for testing we used a SU trained on a dif-ferent part of the corpus, or that the learned policy isfavored over the baselines.
However, this is some-thing to verify experimentally in future work.For future work we would also like to do the fol-lowing.
First of all, currently we are in the process ofanalyzing user satisfaction questionnaires from mu-seum visitors in order to define a better reward func-tion.
Second, we would like to use voice identifi-cation techniques to automatically estimate from thecorpus the statistics of having more than one useror alternating users in the same session.
Third, andmost important, we would like to incorporate thelearned policy into the system that is currently in-stalled in the museum and evaluate it with real users.Fourth, currently our SU is based on only some ofour findings from the analysis of the corpus.
We in-tend to build a more complex and hopefully morerealistic SU based on our full corpus analysis.
Fi-nally, we will also experiment with learning policiesdirectly from the data (Li et al, 2009).To conclude, we analyzed a corpus of interactionsof museum visitors with two virtual characters thatserve as guides at the Museum of Science in Boston,in order to build a realistic model of user behaviorwhen interacting with these characters.
Based onthis analysis, we built a SU and used it for learningthe dialogue policy of the virtual characters usingRL.
We compared our learned policy with two base-lines, one of which was the dialogue policy of theoriginal system that was used for collecting the cor-pus and that is currently installed at the Museum ofScience in Boston.
Our learned policy outperformedboth baselines which shows that RL is a promisingtechnique for learning question-answering dialoguepolicies.AcknowledgmentsThis work was funded by the NSF grant #1117313.The Twins corpus collection was supported by theNSF grant #0813541.ReferencesPriti Aggarwal, Ron Artstein, Jillian Gerten, AthanasiosKatsamanis, Shrikanth Narayanan, Angela Nazarian,and David Traum.
2012.
The Twins corpus of mu-91seum visitor questions.
In Proc.
of the LanguageResources and Evaluation Conference (LREC), pages2355?2361, Istanbul, Turkey.Hua Ai and Diane Litman.
2008.
Assessing dialog sys-tem user simulation evaluation measures using humanjudges.
In Proc.
of the Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies (ACL-HLT), pages 622?629, Columbus,OH, USA.Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jor-dan.
2011.
Empirically evaluating the applicationof reinforcement learning to the induction of effectiveand adaptive pedagogical strategies.
User Modelingand User-Adapted Interaction, 21(1-2):137?180.Sudeep Gandhe, Nicolle Whitman, David Traum, andRon Artstein.
2009.
An integrated authoring toolfor tactical questioning dialogue systems.
In Proc.
ofthe IJCAI Workshop on Knowledge and Reasoning inPractical Dialogue Systems, Pasadena, CA, USA.Kallirroi Georgila and David Traum.
2011a.
Learn-ing culture-specific dialogue models from non culture-specific data.
In Proc.
of HCI International, LectureNotes in Computer Science Vol.
6766, pages 440?449,Orlando, FL, USA.Kallirroi Georgila and David Traum.
2011b.
Reinforce-ment learning of argumentation dialogue policies innegotiation.
In Proc.
of Interspeech, pages 2073?2076, Florence, Italy.Kallirroi Georgila, James Henderson, and Oliver Lemon.2006.
User simulation for spoken dialogue systems:Learning and evaluation.
In Proc.
of Interspeech,pages 1065?1068, Pittsburgh, PA, USA.Kallirroi Georgila, Maria K. Wolters, and Johanna D.Moore.
2010.
Learning dialogue strategies from olderand younger simulated users.
In Proc.
of the AnnualSIGdial Meeting on Discourse and Dialogue (SIG-dial), pages 103?106, Tokyo, Japan.Peter A. Heeman.
2009.
Representing the reinforcementlearning state in a negotiation dialogue.
In Proc.
of theIEEE Automatic Speech Recognition and Understand-ing Workshop (ASRU), Merano, Italy.Arne Jo?nsson, Frida Ande?n, Lars Degerstedt, AnnikaFlycht-Eriksson, Magnus Merkel, and Sara Norberg.2004.
Experiences from combining dialogue systemdevelopment with information access techniques.
InNew Directions in Question Answering, Mark T. May-bury (Ed), pages 153?164.
AAAI/MIT Press.Filip Jurc??
?c?ek, Blaise Thomson, and Steve Young.
2012.Reinforcement learning for parameter estimation instatistical spoken dialogue systems.
Computer Speechand Language, 26(3):168?192.Anton Leuski and David Traum.
2010.
Practical lan-guage processing for virtual humans.
In Proc.
of the22nd Annual Conference on Innovative Applicationsof Artificial Intelligence (IAAI), Atlanta, GA, USA.Anton Leuski, Ronakkumar Patel, David Traum, andBrandon Kennedy.
2006.
Building effective questionanswering characters.
In Proc.
of the Annual SIGdialMeeting on Discourse and Dialogue (SIGdial), pages18?27, Sydney, Australia.Lihong Li, Jason D. Williams, and Suhrid Balakrishnan.2009.
Reinforcement learning for dialog managementusing least-squares policy iteration and fast feature se-lection.
In Proc.
of Interspeech, pages 2475?2478,Brighton, United Kingdom.Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake, ChioriHori, Hideki Kashioka, Hisashi Kawai, and SatoshiNakamura.
2010.
Modeling spoken decision makingdialogue and optimization of its dialogue strategy.
InProc.
of the Annual SIGdial Meeting on Discourse andDialogue (SIGdial), pages 221?224, Tokyo, Japan.Rieks op den Akker, Harry Bunt, Simon Keizer, andBoris van Schooten.
2005.
From question answeringto spoken dialogue: Towards an information search as-sistant for interactive multimodal information extrac-tion.
In Proc.
of Interspeech, pages 2793?2796, Lis-bon, Portugal.Jan Peters and Stefan Schaal.
2008.
Natural actor-critic.Neurocomputing, 71(7-9):1180?1190.Verena Rieser and Oliver Lemon.
2009.
Does this listcontain what you were searching for?
Learning adap-tive dialogue strategies for interactive question an-swering.
Natural Language Engineering, 15(1):55?72.William Swartout, David Traum, Ron Artstein, DanNoren, Paul Debevec, Kerry Bronnenkant, JoshWilliams, Anton Leuski, Shrikanth Narayanan, DianePiepol, Chad Lane, Jacquelyn Morie, Priti Aggarwal,Matt Liewer, Jen-Yuan Chiang, Jillian Gerten, SelinaChu, and Kyle White.
2010.
Ada and Grace: Towardrealistic and engaging virtual museum guides.
In Proc.of the International Conference on Intelligent VirtualAgents (IVA), pages 286?300, Philadelphia, PA, USA.Joel R. Tetreault and Diane J. Litman.
2008.
A reinforce-ment learning approach to evaluating state representa-tions in spoken dialogue systems.
Speech Communi-cation, 50(8-9):683?696.Blaise Thomson and Steve Young.
2010.
Bayesian up-date of dialogue state: A POMDP framework for spo-ken dialogue systems.
Computer Speech and Lan-guage, 24(4):562?588.Sebastian Varges, Fuliang Weng, and Heather Pon-Barry.2009.
Interactive question answering and constraintrelexation in spoken dialogue systems.
Natural Lan-guage Engineering, 15(1):9?30.EllenM.
Voorhees.
2001.
The TREC question answeringtrack.
Natural Language Engineering, 7(4):361?378.92AppendixFeatures Featuresaverage ASR accuracy of user queries if system correctly answered current user query# user queries if system responded with off-topic promptto current user query# correct system responses # times user repeated current query# incorrect system responses # successive incorrect system responses# off-topic system prompts # successive off-topic system prompts% correct system responses # user queries for topic ?introduction?% incorrect system responses # user queries for topic ?personal?user type (child, male, female) # user queries for topic ?school?if user asks example query 1 # user queries for topic ?technology?if user asks example query 2 # user queries for topic ?interfaces?if user asks example query 3 # user queries for topic ?exhibition?if user asks example query 4 # user queries for other topicsif system correctly responds to example query 1 if system correctly responds to example query 3if system correctly responds to example query 2 if system correctly responds to example query 4# junk user queries previous topic categoryTable 7: List of features used in predicting when the user will cease a session (Cease Dialogue), what the user will saynext (Say Next 1), and what the user will say next after removing repeated user queries (Say Next 2).
Example query1 is ?who are you named after??
; example query 2 is ?are you a computer??
; example query 3 is ?what do you like todo for fun??
; example query 4 is ?what is artificial intelligence?
?.Cease Dialogue Say Next 1 Say Next 2average ASR accuracy of previous topic category previous topic categoryuser queriesuser type (child, male, female) # user queries for topic ?personal?
# junk user queries# off-topic system prompts # user queries # successive incorrect systemresponses# successive off-topic system # junk user queries if system correctly answeredprompts current user query# incorrect system responses % correct system responses user type (child, male, female)# user queries % incorrect system responses % incorrect system responses# junk user queries # incorrect system responses % correct system responses# user queries for other topics # user queries for other topics # incorrect system responsesif system responded with off-topic # correct system responses # off-topic system promptsprompt to current user query% correct system responses user type (child, male, female) # user queriesTable 8: List of the 10 most dominant features (in order of importance) in predicting when the user will cease a session(Cease Dialogue), what the user will say next (Say Next 1), and what the user will say next after removing repeateduser queries (Say Next 2).93
