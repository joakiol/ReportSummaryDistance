Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 529?536,Sydney, July 2006. c?2006 Association for Computational LinguisticsDistortion Models For Statistical Machine TranslationYaser Al-Onaizan and Kishore PapineniIBM T.J. Watson Research Center1101 Kitchawan RoadYorktown Heights, NY 10598, USA{onaizan, papineni}@us.ibm.comAbstractIn this paper, we argue that n-gram lan-guage models are not sufficient to addressword reordering required for Machine Trans-lation.
We propose a new distortion modelthat can be used with existing phrase-basedSMT decoders to address those n-gram lan-guage model limitations.
We present empiricalresults in Arabic to English Machine Transla-tion that show statistically significant improve-ments when our proposed model is used.
Wealso propose a novel metric to measure wordorder similarity (or difference) between anypair of languages based on word alignments.1 IntroductionA language model is a statistical model that givesa probability distribution over possible sequences ofwords.
It computes the probability of producing a givenword w1 given all the words that precede it in the sen-tence.
An n-gram language model is an n-th orderMarkov model where the probability of generating agiven word depends only on the last n ?
1 words im-mediately preceding it and is given by the followingequation:P (wk1 ) = P (w1)P (w2|w1) ?
?
?
P (wn|wn?11 ) (1)where k >= n.N -gram language models have been successfullyused in Automatic Speech Recognition (ASR) as wasfirst proposed by (Bahl et al, 1983).
They play an im-portant role in selecting among several candidate wordrealization of a given acoustic signal.
N -gram lan-guage models have also been used in Statistical Ma-chine Translation (SMT) as proposed by (Brown et al,1990; Brown et al, 1993).
The run-time search pro-cedure used to find the most likely translation (or tran-scription in the case of Speech Recognition) is typicallyreferred to as decoding.There is a fundamental difference between decodingfor machine translation and decoding for speech recog-nition.
When decoding a speech signal, words are gen-erated in the same order in which their correspondingacoustic signal is consumed.
However, that is not nec-essarily the case in MT due to the fact that differentlanguages have different word order requirements.
Forexample, in Spanish and Arabic adjectives are mainlynoun post-modifiers, whereas in English adjectives arenoun pre-modifiers.
Therefore, when translating be-tween Spanish and English, words must usually be re-ordered.Existing statistical machine translation decodershave mostly relied on language models to select theproper word order among many possible choices whentranslating between two languages.
In this paper, weargue that a language model is not sufficient to ade-quately address this issue, especially when translatingbetween languages that have very different word ordersas suggested by our experimental results in Section 5.We propose a new distortion model that can be usedas an additional component in SMT decoders.
Thisnew model leads to significant improvements in MTquality as measured by BLEU (Papineni et al, 2002).The experimental results we report in this paper are forArabic-English machine translation of news stories.We also present a novel method for measuring wordorder similarity (or differences) between any given pairof languages based on word alignments as described inSection 3.The rest of this paper is organized as follows.
Sec-tion 2 presents a review of related work.
In Section 3we propose a method for measuring the distortion be-tween any given pair of languages.
In Section 4, wepresent our proposed distortion model.
In Section 5,we present some empirical results that show the utilityof our distortion model for statistical machine trans-lation systems.
Then, we conclude this paper with adiscussion in Section 6.2 Related WorkDifferent languages have different word order require-ments.
SMT decoders attempt to generate translationsin the proper word order by attempting many possible529word reorderings during the translation process.
Tryingall possible word reordering is an NP-Complete prob-lem as shown in (Knight, 1999), which makes search-ing for the optimal solution among all possible permu-tations computationally intractable.
Therefore, SMTdecoders typically limit the number of permutationsconsidered for efficiency reasons by placing reorder-ing restrictions.
Reordering restrictions for word-basedSMT decoders were introduced by (Berger et al, 1996)and (Wu, 1996).
(Berger et al, 1996) allow only re-ordering of at most n words at any given time.
(Wu,1996) propose using contiguity restrictions on the re-ordering.
For a comparison and a more detailed discus-sion of the two approaches see (Zens and Ney, 2003).A different approach to allow for a limited reorder-ing is to reorder the input sentence such that the sourceand the target sentences have similar word order andthen proceed to monotonically decode the reorderedsource sentence.Monotone decoding translates words in the same or-der they appear in the source language.
Hence, theinput and output sentences have the same word order.Monotone decoding is very efficient since the optimaldecoding can be found in polynomial time.
(Tillmannet al, 1997) proposed a DP-based monotone search al-gorithm for SMT.
Their proposed solution to addressthe necessary word reordering is to rewrite the inputsentence such that it has a similar word order to the de-sired target sentence.
The paper suggests that reorder-ing the input reduces the translation error rate.
How-ever, it does not provide a methodology on how to per-form this reordering.
(Xia and McCord, 2004) propose a method to auto-matically acquire rewrite patterns that can be appliedto any given input sentence so that the rewritten sourceand target sentences have similar word order.
Theserewrite patterns are automatically extracted by pars-ing the source and target sides of the training parallelcorpus.
Their approach show a statistically-significantimprovement over a phrase-based monotone decoder.Their experiments also suggest that allowing the de-coder to consider some word order permutations inaddition to the rewrite patterns already applied to thesource sentence actually decreases the BLEU score.Rewriting the input sentence whether using syntacticrules or heuristics makes hard decisions that can notbe undone by the decoder.
Hence, reordering is betterhandled during the search algorithm and as part of theoptimization function.Phrase-based monotone decoding does not directlyaddress word order issues.
Indirectly, however, thephrase dictionary1 in phrase-based decoders typicallycaptures local reorderings that were seen in the trainingdata.
However, it fails to generalize to word reorder-ings that were never seen in the training data.
For ex-ample, a phrase-based decoder might translate the Ara-1Also referred to in the literature as the set of blocks orclumps.bic phrase AlwlAyAt AlmtHdp2 correctly into Englishas the United States if it was seen in its training data,was aligned correctly, and was added to the phrase dic-tionary.
However, if the phrase Almmlkp AlmtHdp isnot in the phrase dictionary, it will not be translatedcorrectly by a monotone phrase decoder even if the in-dividual units of the phrase Almmlkp and AlmtHdp, andtheir translations (Kingdom and United, respectively)are in the phrase dictionary since that would requireswapping the order of the two words.
(Och et al, 1999; Tillmann and Ney, 2003) relaxthe monotonicity restriction in their phrase-based de-coder by allowing a restricted set of word reorderings.For their translation task, word reordering is done onlyfor words belonging to the verb group.
The context inwhich they report their results is a Speech-to-Speechtranslation from German to English.
(Yamada and Knight, 2002) propose a syntax-baseddecoder that restrict word reordering based on reorder-ing operations on syntactic parse-trees of the inputsentence.
They reported results that are better thanword-based IBM4-like decoder.
However, their de-coder is outperformed by phrase-based decoders suchas (Koehn, 2004), (Och et al, 1999), and (Tillmann andNey, 2003) .
Phrase-based SMT decoders mostly relyon the language model to select among possible wordorder choices.
However, in our experiments we showthat the language model is not reliable enough to makethe choices that lead to a better MT quality.
This obser-vation is also reported by (Xia and McCord, 2004).Weargue that the distortion model we propose leads to abetter translation as measured by BLEU.Distortion models were first proposed by (Brown etal., 1993) in the so-called IBM Models.
IBM Mod-els 2 and 3 define the distortion parameters in terms ofthe word positions in the sentence pair, not the actualwords at those positions.
Distortion probability is alsoconditioned on the source and target sentence lengths.These models do not generalize well since their param-eters are tied to absolute word position within sentenceswhich tend to be different for the same words acrosssentences.
IBM Models 4 and 5 alleviate this limita-tion by replacing absolute word positions with relativepositions.
The latter models define the distortion pa-rameters for a cept (one or more words).
This modelsphrasal movement better since words tend to move inblocks and not independently.
The distortion is con-ditioned on classes of the aligned source and targetwords.
The entire source and target vocabularies arereduced to a small number of classes (e.g., 50) for thepurpose of estimating those parameters.Similarly, (Koehn et al, 2003) propose a relative dis-tortion model to be used with a phrase decoder.
Themodel is defined in terms of the difference between theposition of the current phrase and the position of theprevious phrase in the source sentence.
It does not con-2Arabic text appears throughout this paper in Tim Buck-walter?s Romanization.530Arabic Ezp1 AbrAhym2 ystqbl3 ms&wlA4 AqtSAdyA5 sEwdyA6 fy7 bgdAd8English Izzet1 Ibrahim2 Meets3 Saudi4 Trade5 official6 in7 Baghdad8Word Alignment (Ezp1,Izzet1) (AbrAhym2,Ibrahim2) (ystqbl3,Meets3) ( ms&wlA4,official6)(AqtSAdyA5,Trade5) (sEwdyA6,Saudi4) (fy7,in7) (bgdAd8,Baghdad8)Reordered English Izzet1 Ibrahim2 Meets3 official6 Trade5 Saudi4 in7 Baghdad8Table 1: Alignment-based word reordering.
The indices are not part of the sentence pair, they are only used toillustrate word positions in the sentence.
The indices in the reordered English denote word position in the originalEnglish order.sider the words in those positions.The distortion model we propose assigns a proba-bility distribution over possible relative jumps condi-tioned on source words.
Conditioning on the sourcewords allows for a much more fine-grained model.
Forinstance, words that tend to act as modifers (e.g., adjec-tives) would have a different distribution than verbs ornouns.
Our model?s parameters are directly estimatedfrom word alignments as we will further explain in Sec-tion 4.
We will also show how to generalize this worddistortion model to a phrase-based model.
(Och et al, 2004; Tillman, 2004) proposeorientation-based distortion models lexicalized on thephrase level.
There are two important distinctions be-tween their models and ours.
First, they lexicalize theirmodel on the phrases, which have many more param-eters and hence would require much more data to esti-mate reliably.
Second, their models consider only thedirection (i.e., orientation) and not the relative jump.We are not aware of any work on measuring wordorder differences between a given language pair in thecontext of statistical machine translation.3 Measuring Word Order SimilarityBetween Two LanguageIn this section, we propose a simple, novel method formeasuring word order similarity (or differences) be-tween any given language pair.
This method is basedon word-alignments and the BLEU metric.We assume that we have word-alignments for a setof sentence pairs.
We first reorder words in the targetsentence (e.g., English when translating from Arabicto English) according to the order in which they arealigned to the source words as shown in Table 1.
Ifa target word is not aligned, then, we assume that itis aligned to the same source word that the precedingaligned target word is aligned to.Once the reordered target (here English) sentencesare generated, we measure the distortion between thelanguage pair by computing the BLEU3 score betweenthe original target and reordered target, treating theoriginal target as the reference.Table 2 shows these scores for Arabic-English and3the BLEU scores reported throughout this paper are forcase-sensitive BLEU.
The number of references used is alsoreported (e.g., BLEUr1n4c: r1 means 1 reference, n4 meansupto 4-gram are considred, c means case sensitive).Chinese-English.
The word alignments we use are bothannotated manually by human annotators.
The Arabic-English test set is the NIST MT Evaluation 2003 testset.
It contains 663 segments (i.e., sentences).
TheArabic side consists of 16,652 tokens and the Englishconsists of 19,908 tokens.
The Chinese-English test setcontains 260 segments.
The Chinese side is word seg-mented and consists of 4,319 tokens and the Englishconsists of 5,525 tokens.As suggested by the BLEU scores reported in Ta-ble 2, Arabic-English has more word order differencesthan Chinese-English.
The difference in n-gPrec is big-ger for smaller values of n, which suggests that Arabic-English has more local word order differences than inChinese-English.4 Proposed Distortion ModelThe distortion model we are proposing consists of threecomponents: outbound, inbound, and pair distortion.Intuitively our distortion models attempt to capture theorder in which source words need to be translated.
Forinstance, the outbound distortion component attemptsto capture what is typically translated immediately afterthe word that has just been translated.
Do we tend totranslate words that precede it or succeed it?
Whichword position to translate next?Our distortion parameters are directly estimatedfrom word alignments by simple counting over align-ment links in the training data.
Any aligner such as(Al-Onaizan et al, 1999) or (Vogel et al, 1996) canbe used to obtain word alignments.
For the resultsreported in this paper word alignments were obtainedusing a maximum-posterior word aligner4 described in(Ge, 2004).We will illustrate the components of our model witha partial word alignment.
Let us assume that oursource sentence5 is (f10, f250, f300)6, and our targetsentence is (e410, e20), and their word alignment isa = ((f10, e410), (f300, e20)).
Word Alignment a can4We also estimated distortion parameters using a Maxi-mum Entropy aligner and the differences were negligible.5In practice, we add special symbols at the start and end ofthe source and target sentences, we also assume that the startsymbols in the source and target are aligned, and similarlyfor the end symbols.
Those special symbols are omitted inour example for ease of presentation.6The indices here represent source and target vocabularyids.531N-gram Precision Arabic-English Chinese-English1-gPrec 1 12-gPrec 0.6192 0.73783-gPrec 0.4547 0.53824-gPrec 0.3535 0.39905-gPrec 0.2878 0.30756-gPrec 0.2378 0.24067-gPrec 0.1977 0.19308-gPrec 0.1653 0.16149-gPrec 0.1380 0.1416BLEUr1n4c 0.3152 0.334095% Confidence ?
0.0180 0.0370Table 2: Word order similarity for two language pairs: Arabic-English and Chinese-English.
n-gPrec is the n-gramprecision as defined in BLEU.be rewritten as a1 = 1 and a2 = 3 (i.e., the second tar-get word is aligned to the third source word).
From thispartial alignment we increase the counts for the follow-ing outbound, inbound, and pair distortions: Po(?
=+2|f10), Pi(?
= +2|f300).
and Pp(?
= +2|f10, f300).Formally, our distortion model components are de-fined as follows:Outbound Distortion:Po(?|fi) =C(?|fi)?kC(?k |fi)(2)where fi is a foreign word (i.e., Arabic in our case),?
is the step size, and C(?|fi) is the observed count ofthis parameter over all word alignments in the trainingdata.
The value for ?, in theory, ranges from ?max to+max (where max is the maximum source sentencelength observed), but in practice only a small numberof those step sizes are observed in the training data,and hence, have non-zero value).Inbound Distortion:Pi(?|fj) =C(?|fj)?kC(?k|fj)(3)Pairwise Distortion:Pp(?|fi, fj) =C(?|fi, fj)?kC(?k|fi, fj)(4)In order to use these probability distributions in ourdecoder, they are then turned into costs.
The outbounddistortion cost is defined as:Co(?|fi) = log {?Po(?|fi) + (1 ?
?)Ps(?)}
(5)where Ps(?)
is a smoothing distribution 7 and ?
is alinear-mixture parameter 8.7The smoothing we use is a geometrically decreasing dis-tribution as the step size increases.8For the experiments reported here we use ?
= 0.1,which is set empirically.The inbound and pair costs (Ci(?|fi) andCp(?|fi, fj)) can be defined in a similar fashion.So far, our distortion cost is defined in terms ofwords, not phrases.
Therefore, we need to general-ize the distortion cost in order to use it in a phrase-based decoder.
This generalization is defined in termsof the internal word alignment within phrases (we usedthe Viterbi word alignment).
We illustrate this withan example: Suppose the last position translated in thesource sentence so far is n and we are to cover a sourcephrase p=wlAyp wA$nTn that begins at position m inthe source sentence.
Also, suppose that our phrase dic-tionary provided the translation Washington State, withinternal word alignment a = (a1 = 2, a2 = 1) (i.e.,a=(<Washington,wA$nTn>,<State,wlAyp>), then theoutbound phrase cost is defined as:Co(p, n, m, a) =Co(?
= (m ?
n)|fn)+l?1?i=1Co(?
= (ai+1 ?
ai) |fai)(6)where l is the length of the target phrase, a is theinternal word alignment, fn is source word at positionn (in the sentence), and fai is the source word that isaligned to the i-th word in the target side of the phrase(not the sentence).The inbound and pair distortion costs (i..e,Ci(p, n, m, a) and Cp(p, n, m, a)) can be definedin a similar fashion.The above distortion costs are used in conjunctionwith other cost components used in our decoder.
Theultimate word order choice made is influenced by boththe language model cost as well as the distortion cost.5 Experimental ResultsThe phrase-based decoder we use is inspired by the de-coder described in (Tillmann and Ney, 2003) and sim-ilar to that described in (Koehn, 2004).
It is a multi-stack, multi-beam search decoder with n stacks (wheren is the length of the source sentence being decoded)532s 0 1 1 1 1 1 2 2 2 2w 0 4 6 8 10 12 4 6 8 10BLEUr1n4c 0.5617 0.6507 0.6443 0.6430 0.6461 0.6456 0.6831 0.6706 0.6609 0.65962 3 3 3 3 3 4 4 4 4 412 4 6 8 10 12 4 6 8 10 120.6626 0.6919 0.6751 0.6580 0.6505 0.6490 0.6851 0.6592 0.6317 0.6237 0.6081Table 3: BLEU scores for the word order restoration task.
The BLEU scores reported here are with 1 reference.The input is the reordered English in the reference.
The 95% Confidence ?
ranges from 0.011 to 0.016and a beam associated with each stack as describedin (Al-Onaizan, 2005).
The search is done in n timesteps.
In time step i, only hypotheses that cover ex-actly i source words are extended.
The beam searchalgorithm attempts to find the translation (i.e., hypoth-esis that covers all source words) with the minimumcost as in (Tillmann and Ney, 2003) and (Koehn, 2004).
The distortion cost is added to the log-linear mixtureof the hypothesis extension in a fashion similar to thelanguage model cost.A hypothesis covers a subset of the source words.The final translation is a hypothesis that covers allsource words and has the minimum cost among all pos-sible 9 hypotheses that cover all source words.
A hy-pothesis h is extended by matching the phrase dictio-nary against source word sequences in the input sen-tence that are not covered in h. The cost of the newhypothesis C(hnew) = C(h) + C(e), where C(e) isthe cost of this extension.
The main components ofthe cost of extension e can be defined by the followingequation:C(e) = ?1CLM (e) + ?2CTM (e) + ?3CD(e)where CLM (e) is the language model cost, CTM (e)is the translation model cost, and CD(e) is the distor-tion cost.
The extension cost depends on the hypothesisbeing extended, the phrase being used in the extension,and the source word positions being covered.The word reorderings that are explored by the searchalgorithm are controlled by two parameters s and w asdescribed in (Tillmann and Ney, 2003).
The first pa-rameter s denotes the number of source words that aretemporarily skipped (i.e., temporarily left uncovered)during the search to cover a source word to the right ofthe skipped words.
The second parameter is the win-dow width w, which is defined as the distance (in num-ber of source words) between the left-most uncoveredsource word and the right-most covered source word.To illustrate these restrictions, let us assume theinput sentence consists of the following sequence(f1, f2, f3, f4).
For s=1 and w=2, the permissi-ble permutations are (f1, f2, f3, f4), (f2, f1, f3, f4),9Exploring all possible hypothesis with all possible wordpermutations is computationally intractable.
Therefore, thesearch algorithm gives an approximation to the optimal so-lution.
All possible hypotheses refers to all hypotheses thatwere explored by the decoder.
(f2, f3, f1, f4), (f1, f3, f2, f4),(f1, f3, f4, f2), and(f1, f2, f4, f3).5.1 Experimental SetupThe experiments reported in this section are in the con-text of SMT from Arabic into English.
The trainingdata is a 500K sentence-pairs subsample of the 2005Large Track Arabic-English Data for NIST MT Evalu-ation.The language model used is an interpolated trigrammodel described in (Bahl et al, 1983).
The languagemodel is trained on the LDC English GigaWord Cor-pus.The test set used in the experiments in this sectionis the 2003 NIST MT Evaluation test set (which is notpart of the training data).5.2 Reordering with Perfect TranslationsIn the experiments in this section, we show the util-ity of a trigram language model in restoring the correctword order for English.
The task is a simplified transla-tion task, where the input is reordered English (Englishwritten in Arabic word order) and the output is Englishin the correct order.
The source sentence is a reorderedEnglish sentence in the same manner we described inSection 3.
The objective of the decoder is to recoverthe correct English order.We use the same phrase-based decoder we use forour SMT experiments, except that only the languagemodel cost is used here.
Also, the phrase dictionaryused is a one-to-one function that maps every Englishword in our vocabulary to itself.
The language modelwe use for the experiments reported here is the sameas the one used for other experiments reported in thispaper.The results in Table 3 illustrate how the languagemodel performs reasonably well for local reorderings(e.g., for s = 3 and w = 4), but its perfromance de-teriorates as we relax the reordering restrictions by in-creasing the reordering window size (w).Table 4 shows some examples of original English,English in Arabic order, and the decoder output for twodifferent sets of reordering parameters.5.3 SMT ExperimentsThe phrases in the phrase dictionary we use inthe experiments reported here are a combination533Eng Ar Opposition Iraqi Prepares for Meeting mid - January in KurdistanOrig.
Eng.
Iraqi Opposition Prepares for mid - January Meeting in KurdistanOutput1 Iraqi Opposition Meeting Prepares for mid - January in KurdistanOutput2 Opposition Meeting Prepares for Iraqi Kurdistan in mid - JanuaryEng Ar Head of Congress National Iraqi Visits Kurdistan IraqiOrig.
Eng.
Head of Iraqi National Congress Visits Iraqi KurdistanOutput1 Head of Iraqi National Congress Visits Iraqi KurdistanOutput2 Head Visits Iraqi National Congress of Iraqi KurdistanEng Ar House White Confirms Presence of Tape New Bin LadenOrig.
Eng.
White House Confirms Presence of New Bin Laden TapeOutput1 White House Confirms Presence of Bin Laden Tape NewOutput2 White House of Bin Laden Tape Confirms Presence NewTable 4: Examples of reordering with perfect translations.
The examples show English in Arabic order (Eng Ar.
),English in its original order (Orig.
Eng.)
and decoding with two different parameter settings.
Output1 is decodingwith (s=3,w=4).
Output2 is decoding with (s=4,w=12).
The sentence lengths of the examples presented here aremuch shorter than the average in our test set (?
28.5).s w Distortion Used?
BLEUr4n4c0 0 NO 0.44681 8 NO 0.43461 8 YES 0.47152 8 NO 0.43092 8 YES 0.47753 8 NO 0.42833 8 YES 0.47924 8 NO 0.41044 8 YES 0.4782Table 5: BLEU scores for the Arabic-English machine translation task.
The 95% Confidence ?
ranges from 0.0158to 0.0176. s is the number of words temporarily skipped, and w is the word permutation window size.of phrases automatically extracted from maximum-posterior alignments and maximum entropy align-ments.
Only phrases that conform to the so-called con-sistent alignment restrictions (Och et al, 1999) are ex-tracted.Table 5 shows BLEU scores for our SMT decoderwith different parameter settings for skip s, windowwidth w, with and without our distortion model.
TheBLEU scores reported in this table are based on 4 refer-ence translations.
The language model, phrase dictio-nary, and other decoder tuning parameters remain thesame in all experiments reported in this table.Table 5 clearly shows that as we open the search andconsider wider range of word reorderings, the BLEUscore decreases in the absence of our distortion modelwhen we rely solely on the language model.
Wrongreorderings look attractive to the decoder via the lan-guage model which suggests that we need a richermodel with more parameter.
In the absence of richermodels such as the proposed distortion model, our re-sults suggest that it is best to decode monotonically andonly allow local reorderings that are captured in ourphrase dictionary.However, when the distortion model is used, we seestatistically significant increases in the BLEU score aswe consider more word reorderings.
The best BLEUscore achieved when using the distortion model is0.4792 , compared to a best BLEU score of 0.4468when the distortion model is not used.Our results on the 2004 and 2005 NIST MT Evalua-tion test sets using the distortion model are 0.4497 and0.464610, respectively.Table 6 shows some Arabic-English translation ex-amples using our decoder with and without the distor-tion model.6 Conclusion and Future WorkWe presented a new distortion model that can be in-tegrated with existing phrase-based SMT decoders.The proposed model shows statistically significant im-provement over a state-of-the-art phrase-based SMTdecoder.
We also showed that n-gram language mod-10The MT05 BLEU score is the from the official NISTevaluation.
The MT04 BLEU score is only our second runon MT04.534Input (Ar) kwryA Al$mAlyp mstEdp llsmAH lwA$nTn bAltHqq mn AnhA lA tSnE AslHp nwwypRef.
(En) North Korea Prepared to allow Washington to check it is not Manufacturing NuclearWeaponsOut1 North Korea to Verify Washington That It Was Not Prepared to Make Nuclear WeaponsOut2 North Korea Is Willing to Allow Washington to Verify It Does Not Make Nuclear WeaponsInput (Ar) wAkd AldblwmAsy An ?AnsHAb (kwryA Al$mAlyp mn AlmEAhdp) ybd> AEtbArA mnAlywm?.Ref.
(En) The diplomat confirmed that ?North Korea?s withdrawal from the treaty starts as of today.
?Out1 The diplomat said that ?
the withdrawal of the Treaty (start) North Korea as of today.
?Out2 The diplomat said that the ?
withdrawal of (North Korea of the treaty) will start as oftoday ?.Input (Ar) snrfE *lk AmAm Almjls Aldstwry?.Ref.
(En) We will bring this before the Constitutional Assembly.
?Out1 The Constitutional Council to lift it.
?Out2 This lift before the Constitutional Council ?.Input (Ar) wAkd AlbrAdEy An mjls AlAmn ?ytfhm?
An 27 kAnwn AlvAny/ynAyr lys mhlp nhA}yp.Ref.
(En) Baradei stressed that the Security Council ?appreciates?
that January 27 is not a finalultimatum.Out1 Elbaradei said that the Security Council ?
understand ?
that is not a final period January 27.Out2 Elbaradei said that the Security Council ?
understand ?
that 27 January is not a final period.Table 6: Selected examples of our Arabic-English SMT output.
The English is one of the human reference trans-lations.
Output 1 is decoding without the distortion model and (s=4, w=8), which corresponds to 0.4104 BLEUscore.
Output 2 is decoding with the distortion model and (s=3, w=8), which corresponds to 0.4792 BLEU score.The sentences presented here are much shorter than the average in our test set.
The average length of the arabicsentence in the MT03 test set is ?
24.7.els are not sufficient to model word movement in trans-lation.
Our proposed distortion model addresses thisweakness of the n-gram language model.We also propose a novel metric to measure word or-der similarity (or differences) between any pair of lan-guages based on word alignments.
Our metric showsthat Chinese-English have a closer word order thanArabic-English.Our proposed distortion model relies solely on wordalignments and is conditioned on the source words.The majority of word movement in translation ismainly due to syntactic differences between the sourceand target language.
For example, Arabic is verb-initialfor the most part.
So, when translating into English,one needs to move the verb after the subject, which isoften a long compounded phrase.
Therefore, we wouldlike to incorporate syntactic or part-of-speech informa-tion in our distortion model.AcknowledgmentThis work was partially supported by DARPA GALEprogram under contract number HR0011-06-2-0001.
Itwas also partially supported by DARPA TIDES pro-gram monitored by SPAWAR under contract numberN66001-99-2-8916.ReferencesYaser Al-Onaizan, Jan Curin, Michael Jahr, KevinKnight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah Smith, and DavidYarowsky.
1999.
Statistical Machine Translation:Final Report, Johns Hopkins University SummerWorkshop (WS 99) on Language Engineering, Cen-ter for Language and Speech Processing, Baltimore,MD.Yaser Al-Onaizan.
2005.
IBM Arabic-to-English MTSubmission.
Presentation given at DARPA/TIDESNIST MT Evaluation workshop.Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer.1983.
A Maximum Likelihood Approach to Con-tinuous Speech Recognition.
IEEE Transactions onPattern Analysis and Machine Intelligence, PAMI-5(2):179?190.Adam L. Berger, Peter F. Brown, Stephen A. DellaPietra, Vincent J. Della Pietra, Andrew S. Kehler,and Robert L. Mercer.
1996.
Language Transla-tion Apparatus and Method of Using Context-BasedTranslation Models.
United States Patent, PatentNumber 5510981, April.Peter F Brown, John Cocke, Stephen A Della Pietra,Vincent J Della Pietra, Frederick Jelinek, John DLafferty, Robert L Mercer, and Paul S Roossin.1990.
A Statistical Approach to Machine Transla-tion.
Computational Linguistics, 16(2):79?85.535Peter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.The Mathematics of Statistical Machine Translation:Parameter Estimation.
Computational Linguistics,19(2):263?311.Niyu Ge.
2004.
Improvements in Word Alignments.Presentation given at DARPA/TIDES NIST MT Eval-uation workshop.Kevin Knight.
1999.
Decoding Complexity in Word-Replacement Translation Models.
ComputationalLinguistics, 25(4):607?615.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Marti Hearstand Mari Ostendorf, editors, HLT-NAACL 2003:Main Proceedings, pages 127?133, Edmonton, Al-berta, Canada, May 27 ?
June 1.
Association forComputational Linguistics.Philipp Koehn.
2004.
Pharaoh: a Beam Search De-coder for Phrase-Based Statistical Machine Trans-lation Models.
In Proceedings of the 6th Con-ference of the Association for Machine Translationin the Americas, pages 115?124, Washington DC,September-October.
The Association for MachineTranslation in the Americas (AMTA).Franz Josef Och, Christoph Tillmann, and HermannNey.
1999.
Improved Alignment Models for Statis-tical Machine Translation.
In Joint Conf.
of Empir-ical Methods in Natural Language Processing andVery Large Corpora, pages 20?28, College Park,Maryland.Franz Josef Och, Daniel Gildea, Sanjeev Khudan-pur, Anoop Sarkar, Kenji Yamada, Alex Fraser,Shankar Kumar, Libin Shen, David Smith, Kather-ine Eng, Viren Jain, Zhen Jin, and Dragomir Radev.2004.
A Smorgasbord of Features for StatisticalMachine Translation.
In Daniel Marcu Susan Du-mais and Salim Roukos, editors, HLT-NAACL 2004:Main Proceedings, pages 161?168, Boston, Mas-sachusetts, USA, May 2 - May 7.
Association forComputational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of machine translation.
In 40th AnnualMeeting of the Association for Computational Lin-guistics (ACL 02), pages 311?318, Philadelphia, PA,July.Christoph Tillman.
2004.
A unigram orienta-tion model for statistical machine translation.
InDaniel Marcu Susan Dumais and Salim Roukos, ed-itors, HLT-NAACL 2004: Short Papers, pages 101?104, Boston, Massachusetts, USA, May 2 - May 7.Association for Computational Linguistics.Christoph Tillmann and Hermann Ney.
2003.
WordRe-ordering and a DP Beam Search Algorithm forStatistical Machine Translation.
Computational Lin-guistics, 29(1):97?133.Christoph Tillmann, Stephan Vogel, Hermann Ney, andAlex Zubiaga.
1997.
A DP-Based Search UsingMonotone Alignments in Statistical Translation.
InProceedings of the 35th Annual Meeting of the Asso-ciation for Computational Linguistics and 8th Con-ference of the European Chapter of the Associa-tion for Computational Linguistics, pages 289?296,Madrid.
Association for Computational Linguistics.Stefan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-BasedWord Alignment in Statisti-cal Machine Translation.
In Proc.
of the 16thInt.
Conf.
on Computational Linguistics (COLING1996), pages 836?841, Copenhagen, Denmark, Au-gust.Dekai Wu.
1996.
A Polynomial-Time Algorithm forStatistical Machine Translation.
In Proc.
of the 34thAnnual Conf.
of the Association for ComputationalLinguistics (ACL 96), pages 152?158, Santa Cruz,CA, June.Fei Xia and Michael McCord.
2004.
Improving aStatistical MT System with Automatically LearnedRewrite Patterns.
In Proc.
of the 20th InternationalConference on Computational Linguistics (COLING2004), Geneva, Switzerland.Kenji Yamada and Kevin Knight.
2002.
A Decoder forSyntax-based Statistical MT.
In Proc.
of the 40thAnnual Conf.
of the Association for ComputationalLinguistics (ACL 02), pages 303?310, Philadelphia,PA, July.Richard Zens and Hermann Ney.
2003.
A Compar-ative Study on Reordering Constraints in StatisticalMachine Translation.
In Erhard Hinrichs and DanRoth, editors, Proceedings of the 41st Annual Meet-ing of the Association for Computational Linguistics,pages 144?151, Sapporo, Japan.536
