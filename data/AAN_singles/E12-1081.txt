Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 798?807,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsLearning the Fine-Grained Information Status of Discourse EntitiesAltaf Rahman and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{altaf,vince}@hlt.utdallas.eduAbstractWhile information status (IS) plays a cru-cial role in discourse processing, there haveonly been a handful of attempts to automat-ically determine the IS of discourse entities.We examine a related but more challengingtask, fine-grained IS determination, whichinvolves classifying a discourse entity asone of 16 IS subtypes.
We investigate theuse of rich knowledge sources for this taskin combination with a rule-based approachand a learning-based approach.
In experi-ments with a set of Switchboard dialogues,the learning-based approach achieves an ac-curacy of 78.7%, outperforming the rule-based approach by 21.3%.1 IntroductionA linguistic notion central to discourse processingis information status (IS).
It describes the extentto which a discourse entity, which is typically re-ferred to by noun phrases (NPs) in a dialogue, isavailable to the hearer.
Different definitions of IShave been proposed over the years.
In this paper,we adopt Nissim et als (2004) proposal, since itis primarily built upon Prince?s (1992) and Eck-ert and Strube?s (2001) well-known definitions,and is empirically shown by Nissim et alto yieldan annotation scheme for IS in dialogue that hasgood reproducibility.1Specifically, Nissim et al(2004) adopt a three-way classification scheme for IS, defining a dis-course entity as (1) old to the hearer if it is knownto the hearer and has previously been referred to inthe dialogue; (2) new if it is unknown to her and1It is worth noting that several IS annotation schemeshave been proposed more recently.
See Go?tze et al(2007)and Riester et al(2010) for details.has not been previously referred to; and (3) me-diated (henceforth med) if it is newly mentionedin the dialogue but she can infer its identity froma previously-mentioned entity.
To capture finer-grained distinctions for IS, Nissim et alallow anold or med entity to have a subtype, which subcat-egorizes an old or med entity.
For instance, a medentity has the subtype set if the NP that refers toit is in a set-subset relation with its antecedent.IS plays a crucial role in discourse processing:it provides an indication of how a discourse modelshould be updated as a dialogue is processed in-crementally.
Its importance can be reflected inpart in the amount of attention it has received intheoretical linguistics over the years (e.g., Halli-day (1976), Prince (1981), Hajic?ova?
(1984), Vall-duv??
(1992), Steedman (2000)), and in part in thebenefits it can potentially bring to NLP applica-tions.
One task that could benefit from knowledgeof IS is identity coreference: since new entities bydefinition have not been previously referred to, anNP marked as new does not need to be resolved,thereby improving the precision of a coreferenceresolver.
Knowledge of fine-grained or subcat-egorized IS is valuable for other NLP tasks.
Forinstance, an NP marked as set signifies that it is ina set-subset relation with its antecedent, therebyproviding important clues for bridging anaphoraresolution (e.g., Gasperin and Briscoe (2008)).Despite the potential usefulness of IS in NLPtasks, there has been little work on learningthe IS of discourse entities.
To investigate theplausibility of learning IS, Nissim et al(2004)annotate a set of Switchboard dialogues withsuch information2 , and subsequently present a2These and other linguistic annotations on the Switch-board dialogues were later released by the LDC as part of theNXT corpus, which is described in Calhoun et al(2010).798rule-based approach and a learning-based ap-proach to acquiring such knowledge (Nissim,2006).
More recently, we have improved Nissim?slearning-based approach by augmenting her fea-ture set, which comprises seven string-matchingand grammatical features, with lexical and syn-tactic features (Rahman and Ng, 2011; hence-forth R&N).
Despite the improvements, the per-formance on new entities remains poor: an F-score of 46.5% was achieved.Our goal in this paper is to investigate fine-grained IS determination, the task of classifyinga discourse entity as one of the 16 IS subtypesdefined by Nissim et al(2004).3 Owing in partto the increase in the number of categories, fine-grained IS determination is arguably a more chal-lenging task than the 3-class IS determination taskthat Nissim and R&N investigated.
To our knowl-edge, this is the first empirical investigation of au-tomated fine-grained IS determination.We propose a knowledge-rich approach to fine-grained IS determination.
Our proposal is moti-vated in part by Nissim?s and R&N?s poor per-formance on new entities, which we hypothesizecan be attributed to their sole reliance on shallowknowledge sources.
In light of this hypothesis,our approach employs semantic and world knowl-edge extracted from manually and automaticallyconstructed knowledge bases, as well as corefer-ence information.
The relevance of coreference toIS determination can be seen from the definitionof IS: a new entity is not coreferential with anypreviously-mentioned entity, whereas an old en-tity may.
While our use of coreference informa-tion for IS determination and our earlier claim thatIS annotation would be useful for coreference res-olution may seem to have created a chicken-and-egg problem, they do not: since coreference reso-lution and IS determination can benefit from eachother, it may be possible to formulate an approachwhere the two tasks can mutually bootstrap.We investigate rule-based and learning-basedapproaches to fine-grained IS determination.
Inthe rule-based approach, we manually composerules to combine the aforementioned knowledgesources.
While we could employ the same knowl-edge sources in the learning-based approach, wechose to encode, among other knowledge sources,3One of these 16 classes is the new type, for which nosubtype is defined.
For ease of exposition, we will refer tothe new type as one of the 16 subtypes to be predicted.the hand-written rules and their predictions di-rectly as features for the learner.
In an evalua-tion on 147 Switchboard dialogues, our learning-based approach to fine-grained IS determina-tion achieves an accuracy of 78.7%, substan-tially outperforming the rule-based approach by21.3%.
Equally importantly, when employingthese linguistically rich features to learn Nissim?s3-class IS determination task, the resulting classi-fier achieves an accuracy of 91.7%, surpassing theclassifier trained on R&N?s state-of-the-art fea-ture set by 8.8% in absolute accuracy.
Improve-ments on the new class are particularly substan-tial: its F-score rises from 46.7% to 87.2%.2 IS Types and Subtypes: An OverviewIn Nissim et als (2004) IS classification scheme,an NP can be assigned one of three main types(old, med, new) and one of 16 subtypes.
Belowwe will illustrate their definitions with examples,most of which are taken from Nissim (2003) orNissim et als (2004) dataset (see Section 3).Old.
An NP is marked is old if (i) it is corefer-ential with an entity introduced earlier, (ii) it is ageneric pronoun, or (iii) it is a personal pronounreferring to the dialogue participants.
Six sub-types are defined for old entities: identity, event,general, generic, ident generic, and relative.
InExample 1, my is marked as old with subtypeidentity, since it is coreferent with I.
(1) I was angry that he destroyed my tent.However, if the markable has a verb phrase (VP)rather than an NP as its antecedent, it will bemarked as old/event, as can be seen in Example2, where the antecedent of That is the VP put myphone number on the form.
(2) They ask me to put my phone numberon the form.
That I think is not needed.Other NPs marked as old include (i) relativepronouns, which have the subtype relative; (ii)personal pronouns referring to the dialogue par-ticipants, which have the subtype general, and(iii) generic pronouns, which have the subtypegeneric.
The pronoun you in Example 3 is an in-stance of a generic pronoun.
(3) I think to correct the judicial system,you have to get the lawyer out of it.Note, however, that in a coreference chain ofgeneric pronouns, every element of the chain is799assigned the subtype ident generic instead.Mediated.
An NP is marked as med if the en-tity it refers to has not been previously introducedin the dialogue, but can be inferred from already-mentioned entities or is generally known to thehearer.
Nine subtypes are available for med en-tities: general, bound, part, situation, event, set,poss, func value, and aggregation.General is assigned to med entities that aregenerally known, such as the Earth, China, andmost proper names.
Bound is reserved for boundpronouns, an instance of which is shown in Ex-ample 4, where its is bound to the variable of theuniversally quantified NP, Every cat.
(4) Every cat ate its dinner.Poss is assigned to NPs involved in intra-phrasalpossessive relations, including prenominal geni-tives (i.e., X?s Y) and postnominal genitives (i.e.,Y of X).
Specifically, Y will be marked as poss ifX is old or med; otherwise, Y will be new.
For ex-ample, in cases like a friend?s boat where a friendis new, boat is marked as new.Four subtypes, namely part, situation, event,and set, are used to identify instances of bridg-ing (i.e., entities that are inferrable from a relatedentity mentioned earlier in the dialogue).
As anexample, consider the following sentences:(5a) He passed by the door of Jan?s houseand saw that the door was painted red.
(5b) He passed by Jan?s house and saw thatthe door was painted red.In Example 5a, by the time the hearer processesthe second occurrence of the door, she has alreadyhad a mental entity corresponding to the door (af-ter processing the first occurrence).
As a result,the second occurrence of the door refers to anold entity.
In Example 5b, on the other hand, thehearer is not assumed to have any mental repre-sentation of the door in question, but she can in-fer that the door she saw was part of Jan?s house.Hence, this occurrence of the door should bemarked as med with subtype part, as it is involvedin a part-whole relation with its antecedent.If an NP is involved in a set-subset relation withits antecedent, it inherits the med subtype set.This applies to the NP the house payment in Ex-ample 6, whose antecedent is our monthly budget.
(6) What we try to do to stick to ourmonthly budget is we pretty much havethe house payment.If an NP is part of a situation set up by apreviously-mentioned entity, it is assigned thesubtype situation, as exemplified by the NP a fewhorses in the sentence below, which is involved inthe situation set up by John?s ranch.
(7) Mary went to John?s ranch and saw thatthere were only a few horses.Similar to old entities, an NP marked as med maybe related to a previously mentioned VP.
In thiscase, the NP will receive the subtype event, as ex-emplified by the NP the bus in the sentence below,which is triggered by the VP traveling in Miami.
(8) We were traveling in Miami, and thebus was very full.If an NP refers to a value of a previously men-tioned function, such as the NP 30 degrees in Ex-ample 9, which is related to the temperature, thenit is assigned the subtype func value.
(9) The temperature rose to 30 degrees.Finally, the subtype aggregation is assigned to co-ordinated NPs if at least one of the NPs involvedis not new.
However, if all NPs in the coordinatedphrase are new, the phrase should be marked asnew.
For instance, the NP My son and I in Exam-ple 10 should be marked as med/aggregation.
(10) I have a son ... My son and I like toplay chess after dinner.New.
An entity is new if it has not been intro-duced in the dialogue and the hearer cannot inferit from previously mentioned entities.
No subtypeis defined for new entities.There are cases where more than one IS valueis appropriate for a given NP.
For instance, giventwo occurrences of China in a dialogue, the sec-ond occurrence can be labeled as old/identity (be-cause it is coreferential with an earlier NP) ormed/general (because it is a generally knownentity).
To break ties, Nissim (2003) define aprecedence relation on the IS subtypes, whichyields a total ordering on the subtypes.
Sinceall the old subtypes are ordered before their medcounterparts in this relation, the second occur-rence of China in our example will be labeled asold/identity.
Owing to space limitations, we referthe reader to Nissim (2003) for details.3 DatasetWe employ Nissim et als (2004) dataset, whichcomprises 147 Switchboard dialogues.
We parti-800tion them into a training set (117 dialogues) and atest set (30 dialogues).
A total of 58,835 NPs areannotated with IS types and subtypes.4 The distri-butions of NPs over the IS subtypes in the trainingset and the test set are shown in Table 1.Train (%) Test (%)old/identity 10236 (20.1) 1258 (15.8)old/event 1943 (3.8) 290 (3.6)old/general 8216 (16.2) 1129 (14.2)old/generic 2432 (4.8) 427 (5.4)old/ident generic 1730 (3.4) 404 (5.1)old/relative 1241 (2.4) 193 (2.4)med/general 2640 (5.2) 325 (4.1)med/bound 529 (1.0) 74 (0.9)med/part 885 (1.7) 120 (1.5)med/situation 1109 (2.2) 244 (3.1)med/event 351 (0.7) 67 (0.8)med/set 10282 (20.2) 1771 (22.3)med/poss 1318 (2.6) 220 (2.8)med/func value 224 (0.4) 31 (0.4)med/aggregation 580 (1.1) 117 (1.5)new 7158 (14.1) 1293 (16.2)total 50874 (100) 7961 (100)Table 1: Distributions of NPs over IS subtypes.
Thecorresponding percentages are parenthesized.4 Rule-Based ApproachIn this section, we describe our rule-based ap-proach to fine-grained IS determination, where wemanually design rules for assigning IS subtypes toNPs based on the subtype definitions in Section 2,Nissim?s (2003) IS annotation guidelines, and ourinspection of the IS annotations in the trainingset.
The motivations behind having a rule-basedapproach are two-fold.
First, it can serve as abaseline for fine-grained IS determination.
Sec-ond, it can provide insight into how the availableknowledge sources can be combined into predic-tion rules, which can potentially serve as ?sophis-ticated?
features for a learning-based approach.As shown in Table 2, our ruleset is composed of18 rules, which should be applied to an NP in theorder in which they are listed.
Rules 1?7 handlethe assignment of old subtypes to NPs.
For in-stance, Rule 1 identifies instances of old/general,which comprises the personal pronouns referring4Not all NPs have an IS type/subtype.
For instance, apleonastic ?it?
does not refer to any real-world entity andtherefore does not have any IS, and so are nouns such as?course?
in ?of course?, ?accident?
in ?by accident?, etc.to the dialogue participants.
Note that this andseveral other rules rely on coreference informa-tion, which we obtain from two sources: (1)chains generated automatically using the Stan-ford Deterministic Coreference Resolution Sys-tem (Lee et al 2011)5, and (2) manually iden-tified coreference chains taken directly from theannotated Switchboard dialogues.
Reporting re-sults using these two ways of obtaining chains fa-cilitates the comparison of the IS determinationresults that we can realistically obtain using ex-isting coreference technologies against those thatwe could obtain if we further improved exist-ing coreference resolvers.
Note that both sourcesprovide identity coreference chains.
Specifically,the gold chains were annotated for NPs belong-ing to old/identity and old/ident generic.
Hence,these chains can be used to distinguish betweenold/general NPs and old/ident generic NPs, be-cause the former are not part of a chain whereasthe latter are.
However, they cannot be usedto distinguish between old/general entities andold/generic entities, since neither of them belongsto any chains.
As a result, when gold chains areused, Rule 1 will classify all occurrences of ?you?that are not part of a chain as old/general, regard-less of whether the pronoun is generic.
While thegold chains alone can distinguish old/general andold/ident generic NPs, the Stanford chains can-not distinguish any of the old subtypes in the ab-sence of other knowledge sources, since it gener-ates chains for all old NPs regardless of their sub-types.
This implies that Rule 1 and several otherrules are only a very crude approximation of thedefinition of the corresponding IS subtypes.The rules for the remaining old subtypes can beinterpreted similarly.
A few points deserve men-tion.
First, many rules depend on the string ofthe NP under consideration (e.g., ?they?
in Rule 2and ?whatever?
in Rule 4).
The decision of whichstrings are chosen is based primarily on our in-spection of the training data.
Hence, these rulesare partly data-driven.
Second, these rules shouldbe applied in the order in which they are shown.For instance, though not explicitly stated, Rule 3is only applicable to the non-anaphoric ?you?
and?they?
pronouns, since Rule 2 has already coveredtheir anaphoric counterparts.
Finally, Rule 7 usesnon-anaphoricity as a test of old/event NPs.
The5The Stanford resolver is available from http://nlp.stanford.edu/software/corenlp.shtml.8011.
if the NP is ?I?
or ?you?
and it is not part of a coreference chain, thensubtype := old/general2.
if the NP is ?you?
or ?they?
and it is anaphoric, thensubtype := old/ident generic3.
if the NP is ?you?
or ?they?, thensubtype := old/generic4.
if the NP is ?whatever?
or an indefinite pronoun prefixed by ?some?
or ?any?
(e.g., ?somebody?
), thensubtype := old/generic5.
if the NP is an anaphoric pronoun other than ?that?, or its string is identical to that of a preceding NP, thensubtype := old/ident6.
if the NP is ?that?
and it is coreferential with the immediately preceding word, thensubtype := old/relative7.
if the NP is ?it?, ?this?
or ?that?, and it is not anaphoric, thensubtype := old/event8.
if the NP is pronominal and is not anaphoric, thensubtype := med/bound9.
if the NP contains ?and?
or ?or?, thensubtype := med/aggregation10.
if the NP is a multi-word phrase that (1) begins with ?so much?, ?something?, ?somebody?, ?someone?,?anything?, ?one?, or ?different?, or (2) has ?another?, ?anyone?, ?other?, ?such?, ?that?, ?of?
or ?type?as neither its first nor last word, or (3) its head noun is also the head noun of a preceding NP, thensubtype := med/set11.
if the NP contains a word that is a hyponym of the word ?value?
in WordNet, thensubtype := med/func value12.
if the NP is involved in a part-whole relation with a preceding NP based on information extracted fromReVerb?s output, thensubtype := med/part13.
if the NP is of the form ?X?s Y?
or ?poss-pro Y?, where X and Y are NPs and poss-pro is a possessivepronoun, thensubtype := med/poss14.
if the NP fills an argument of a FrameNet frame set up by a preceding NP or verb, thensubtype := med/situation15.
if the head of the NP and one of the preceding verbs in the same sentence share the same WordNethypernym which is not in synsets that appear one of the top five levels of the noun/verb hierarchy, thensubtype := med/event16.
if the NP is a named entity (NE) or starts with ?the?, thensubtype := med/general17.
if the NP appears in the training set, thensubtype := its most frequent IS subtype in the training set18.
subtype := newTable 2: Hand-crafted rules for assigning IS subtypes to NPs.reason is that these NPs have VP antecedents, butboth the gold chains and the Stanford chains arecomputed over NPs only.Rules 8?16 concern med subtypes.
Apart fromRule 8 (med/bound), Rule 9 (med/aggregation),and Rule 11 (med/func value), which are arguablycrude approximations of the definitions of thecorresponding subtypes, the med rules are morecomplicated than their old counterparts, in partbecause of their reliance on the extraction of so-phisticated knowledge.
Below we describe the ex-traction process and the motivation behind them.Rule 10 concerns med/set.
The words andphrases listed in the rule, which are derived manu-ally from the training data, provide suggestive ev-idence that the NP under consideration is a subsetor a specific portion of an entity or concept men-tioned earlier in the dialogue.
Examples include?another bedroom?, ?different color?, ?somebodyelse?, ?any place?, ?one of them?, and ?most othercities?.
Condition 3 of the rule, which checkswhether the head noun of the NP has been men-tioned previously, is a good test for identity coref-erence, but since all the old entities have suppos-802edly been identified by the preceding rules, it be-comes a reasonable test for set-subset relations.For convenience, we identify part-whole rela-tions in Rule 12 based on the output produced byReVerb (Fader et al 2011), an open informationextraction system.6 The output contains, amongother things, relation instances, each of which isrepresented as a triple, <A,rel,B>, where rel isa relation, and A and B are its arguments.
To pre-process the output, we first identify all the triplesthat are instances of the part-whole relation us-ing regular expressions.
Next, we create clustersof relation arguments, such that each pair of ar-guments in a cluster has a part-whole relation.This is easy: since part-whole is a transitive rela-tion (i.e., <A,part,B> and <B,part,C> implies<A,part,C>), we cluster the arguments by takingthe transitive closure of these relation instances.Then, given an NP NPi in the test set, we assignmed/part to it if there is a preceding NP NPj suchthat the two NPs are in the same argument cluster.In Rule 14, we use FrameNet (Baker et al1998) to determine whether med/situation shouldbe assigned to an NP, NPi.
Specifically, we checkwhether it fills an argument of a frame set up bya preceding NP, NPj , or verb.
To exemplify, letus assume that NPj is ?capital punishment?.
Wesearch for ?punishment?
in FrameNet to accessthe appropriate frame, which in this case is ?re-wards and punishments?.
This frame contains alist of arguments together with examples.
If NPi isone of these arguments, we assign med/situationto NPi, since it is involved in a situation (describedby a frame) that is set up by a preceding NP/verb.In Rule 15, we use WordNet (Fellbaum, 1998)to determine whether med/event should be as-signed to an NP, NPi, by checking whether NPi isrelated to an event, which is typically describedby a verb.
Specifically, we use WordNet to checkwhether there exists a verb, v, preceding NPi suchthat v and NPi have the same hypernym.
If so, weassign NPi the subtype med/event.
Note that weensure that the hypernym they share does not ap-pear in the top five levels of the WordNet nounand verb hierarchies, since we want them to berelated via a concept that is not overly general.Rule 16 identifies instances of med/general.The majority of its members are generally-known6We use ReVerb ClueWeb09 Extractions 1.1, whichis available from http://reverb.cs.washington.edu/reverb_clueweb_tuples-1.1.txt.gz.entities, whose identification is difficult as it re-quires world knowledge.
Consequently, we applythis rule only after all other med rules are applied.As we can see, the rule assigns med/general toNPs that are named entities (NEs) and definite de-scriptions (specifically those NPs that start with?the?).
The reason is simple.
Most NEs are gener-ally known.
Definite descriptions are typically notnew, so it seems reasonable to assign med/generalto them given that the remaining (i.e., unlabeled)NPs are presumably either new and med/general.Before Rule 18, which assigns an NP to the newclass by default, we have a ?memorization?
rulethat checks whether the NP under considerationappears in the training set (Rule 17).
If so, weassign to it its most frequent subtype based on itsoccurrences in the training set.
In essence, thisheuristic rule can help classify some of the NPsthat are somehow ?missed?
by the first 16 rules.The ordering of these rules has a direct impacton performance of the ruleset, so a natural ques-tion is: what criteria did we use to order the rules?We order them in such a way that they respect thetotal ordering on the subtypes imposed by Nis-sim?s (2003) preference relation (see Section 3),except that we give med/general a lower prioritythan Nissim due to the difficulty involved in iden-tifying generally known entities, as noted above.5 Learning-Based ApproachIn this section, we describe our learning-based ap-proach to fine-grained IS determination.
Sincewe aim to automatically label an NP with its ISsubtype, we create one training/test instance fromeach hand-annotated NP in the training/test set.Each instance is represented using five types offeatures, as described below.Unigrams (119704).
We create one binary fea-ture for each unigram appearing in the trainingset.
Its value indicates the presence or absenceof the unigram in the NP under consideration.Markables (209751).
We create one binary fea-ture for each markable (i.e., an NP having an ISsubtype) appearing in the training set.
Its value is1 if and only if the markable has the same stringas the NP under consideration.Markable predictions (17).
We create 17 bi-nary features, 16 of which correspond to the 16IS subtypes and the remaining one corresponds toa ?dummy subtype?.
Specifically, if the NP un-803der consideration appears in the training set, weuse Rule 17 in our hand-crafted ruleset to deter-mine the IS subtype it is most frequently associ-ated with in the training set, and then set the valueof the feature corresponding to this IS subtype to1.
If the NP does not appear in the training set, weset the value of the dummy subtype feature to 1.Rule conditions (17).
As mentioned before, wecan create features based on the hand-crafted rulesin Section 4.
To describe these features, let us in-troduce some notation.
Let Rule i be denoted byAi ??
Bi, where Ai is the condition that mustbe satisfied before the rule can be applied and Biis the IS subtype predicted by the rule.
We couldcreate one binary feature from each Ai, and set itsvalue to 1 if Ai is satisfied by the NP under con-sideration.
These features, however, fail to cap-ture a crucial aspect of the ruleset: the ordering ofthe rules.
For instance, Rule i should be appliedonly if the conditions of the first i?1 rules are notsatisfied by the NP, but such ordering is not en-coded in these features.
To address this problem,we capture rule ordering information by definingbinary feature fi as ?A1??A2?
.
.
.
?Ai?1?Ai,where 1 ?
i ?
16.
In addition, we define a fea-ture, f18, for the default rule (Rule 18) in a simi-lar fashion, but since it does not have any condi-tion, we simply define f18 as ?A1 ?
.
.
.
?
?A16.The value of a feature in this feature group is 1if and only if the NP under consideration satis-fies the condition defined by the feature.
Note thatwe did not create any features from Rule 17 here,since we have already generated ?markables?
and?markable prediction?
features for it.Rule predictions (17).
None of the features fi?sdefined above makes use of the predictions of ourhand-crafted rules (i.e., the Bi?s).
To make useof these predictions, we define 17 binary features,one for each Bi, where i = 1, .
.
.
, 16, 18.
Specif-ically, the value of the feature corresponding toBi is 1 if and only if fi is 1, where fi is a ?rulecondition?
feature as defined above.Since IS subtype determination is a 16-classclassification problem, we train a multi-classSVM classifier on the training instances usingSVMmulticlass (Tsochantaridis et al 2004), anduse it to make predictions on the test instances.77For all the experiments involving SVMmulticlass, weset C, the regularization parameter, to 500,000, since pre-liminary experiments indicate that preferring generalization6 EvaluationNext, we evaluate the rule-based approach andthe learning-based approach to determining the ISsubtype of each hand-annotated NP in the test set.Classification results.
Table 3 shows the resultsof the two approaches.
Specifically, row 1 showstheir accuracy, which is defined as the percent-age of correctly classified instances.
For eachapproach, we present results that are generatedbased on gold coreference chains as well as auto-matic chains computed by the Stanford resolver.As we can see, the rule-based approachachieves accuracies of 66.0% (gold coreference)and 57.4% (Stanford coreference), whereas thelearning-based approach achieves accuracies of86.4% (gold) and 78.7% (Stanford).
In otherwords, the gold coreference results are better thanthe Stanford coreference results, and the learning-based results are better than the rule-based results.While perhaps neither of these results are surpris-ing, we are pleasantly surprised by the extent towhich the learned classifier outperforms the hand-crafted rules: accuracies increase by 20.4% and21.3% when gold coreference and Stanford coref-erence are used, respectively.
In other words, ma-chine learning has ?transformed?
a ruleset thatachieves mediocre performance into a system thatachieves relatively high performance.These results also suggest that coreferenceplays a crucial role in IS subtype determination:accuracies could increase by up to 7.7?8.6% ifwe solely improved coreference resolution perfor-mance.
This is perhaps not surprising: IS andcoreference can mutually benefit from each other.To gain additional insight into the task, we alsoshow in rows 2?17 of Table 3 the performanceon each of the 16 subtypes, expressed in terms ofrecall (R), precision (P), and F-score (F).
A fewpoints deserve mention.
First, in comparison tothe rule-based approach, the learning-based ap-proach achieves considerably better performanceon almost all classes.
One that is of particular in-terest is the new class.
As we can see in row 17,its F-score rises by about 30 points.
These gainsare accompanied by a simultaneous rise in recalland precision.
In particular, recall increases byabout 40 points.
Now, recall from the introduc-to overfitting (by setting C to a small value) tends to yieldpoorer classification performance.
The remaining learningparameters are set to their default values.804Rule-Based Approach Learning-Based ApproachGold Coreference Stanford Coreference Gold Coreference Stanford Coreference1 Accuracy 66.0 57.4 86.4 78.7IS Subtype R P F R P F R P F R P F2 old/ident 77.5 78.2 77.8 66.1 52.7 58.7 82.8 85.2 84.0 75.8 64.2 69.53 old/event 98.6 50.4 66.7 71.3 43.2 53.8 98.3 87.9 92.8 2.4 31.8 4.54 old/general 81.9 82.7 82.3 72.3 83.6 77.6 97.7 93.7 95.6 87.8 92.7 90.25 old/generic 55.9 55.2 55.5 39.2 39.8 39.5 76.1 87.3 81.3 39.9 85.9 54.56 old/ident generic 48.7 77.7 59.9 27.2 51.8 35.7 57.1 87.5 69.1 47.2 44.8 46.07 old/relative 55.0 69.2 61.3 55.1 63.4 59.0 98.0 63.0 76.7 99.0 37.5 54.48 med/general 29.9 19.8 23.8 29.5 19.6 23.6 91.2 87.7 89.4 84.0 72.2 77.79 med/bound 56.4 20.5 30.1 56.4 20.5 30.1 25.7 65.5 36.9 2.7 40.0 5.110 med/part 19.5 100.0 32.7 19.5 100.0 32.7 73.2 96.8 83.3 73.2 96.8 83.311 med/situation 28.7 100.0 44.6 28.7 100.0 44.6 68.4 95.4 79.7 68.0 97.7 80.212 med/event 10.5 100.0 18.9 10.5 100.0 18.9 46.3 100.0 63.3 46.3 100.0 63.313 med/set 82.9 61.8 70.8 78.0 59.4 67.4 90.4 87.8 89.1 88.4 86.0 87.214 med/poss 52.9 86.0 65.6 52.9 86.0 65.6 93.2 92.4 92.8 90.5 97.6 93.915 med/func value 81.3 74.3 77.6 81.3 74.3 77.6 88.1 85.9 87.0 88.1 85.9 87.016 med/aggregation 57.4 44.0 49.9 57.4 43.6 49.6 85.2 72.9 78.6 83.8 93.9 88.617 new 50.4 65.7 57.0 50.3 65.1 56.7 90.3 84.6 87.4 90.4 83.6 86.9Table 3: IS subtype accuracies and F-scores.
In each row, the strongest result, as well as those that are statisticallyindistinguishable from it according to the paired t-test (p < 0.05), are boldfaced.tion that previous attempts on 3-class IS determi-nation by Nissim and R&N have achieved poorperformance on the new class.
We hypothesizethat the use of shallow features in their approacheswere responsible for the poor performance theyobserved, and that using our knowledge-rich fea-ture set could improve its performance.
We willtest this hypothesis at the end of this section.Other subtypes that are worth discussingare med/aggregation, med/func value, andmed/poss.
Recall that the rules we designed forthese classes were only crude approximations, or,perhaps more precisely, simplified versions of thedefinitions of the corresponding subtypes.
Forinstance, to determine whether an NP belongs tomed/aggregation, we simply look for occurrencesof ?and?
and ?or?
(Rule 9), whereas its definitionrequires that not all of the NPs in the coordinatedphrase are new.
Despite the over-simplicityof these rules, machine learning has enabledthe available features to be combined in such away that high performance is achieved for theseclasses (see rows 14?16).Also worth examining are those classes forwhich the hand-crafted rules rely on sophisti-cated knowledge sources.
They include med/part,which relies on ReVerb; med/situation, which re-lies on FrameNet; and med/event, which relies onWordNet.
As we can see from the rule-based re-sults (rows 10?12), these knowledge sources haveyielded rules that achieved perfect precision butlow recall: 19.5% for part, 28.7% for situation,and 10.5 for event.
Nevertheless, the learningalgorithm has again discovered a profitable wayto combine the available features, enabling the F-scores of these classes to increase by 35.1?50.6%.While most classes are improved by machinelearning, the same is not true for old/event andmed/bound, whose F-scores are 4.5% (row 3) and5.1% (row 9), respectively, when Stanford coref-erence is employed.
This is perhaps not surpris-ing.
Recall that the multi-class SVM classifierwas trained to maximize classification accuracy.Hence, if it encounters a class that is both difficultto learn and is under-represented, it may as wellaim to achieve good performance on the easier-to-learn, well-represented classes at the expenseof these hard-to-learn, under-represented classes.Feature analysis.
In an attempt to gain addi-tional insight into the performance contributionof each of the five types of features used in thelearning-based approach, we conduct feature ab-lation experiments.
Results are shown in Table 4,where each row shows the accuracy of the classi-fier trained on all types of features except for theone shown in that row.
For easy reference, theaccuracy of the classifier trained on all types offeatures is shown in row 1 of the table.
Accordingto the paired t-test (p < 0.05), performance dropssignificantly whichever feature type is removed.This suggests that all five feature types are con-tributing positively to overall accuracy.
Also, themarkables features are the least important in thepresence of other feature groups, whereas mark-805Feature Type Gold Coref Stanford CorefAll features 86.4 78.7?rule predictions 77.5 70.0?markable predictions 72.4 64.7?rule conditions 81.1 71.0?unigrams 74.4 58.6?markables 83.2 75.5Table 4: Accuracies of feature ablation experiments.Feature Type Gold Coref Stanford Corefrule predictions 49.1 45.2markable predictions 39.7 39.7rule conditions 58.1 28.9unigrams 56.8 56.8markables 10.4 10.4Table 5: Accuracies of classifiers for each feature type.able predictions and unigrams are the two mostimportant feature groups.To get a better idea of the utility of each featuretype, we conduct another experiment in which wetrain five classifiers, each of which employs ex-actly one type of features.
The accuracies of theseclassifiers are shown in Table 5.
As we can see,the markables features have the smallest contribu-tion, whereas unigrams have the largest contribu-tion.
Somewhat interesting are the results of theclassifiers trained on the rule conditions: the rulesare far more effective when gold coreference isused.
This can be attributed to the fact that thedesign of the rules was based in part on the defini-tions of the subtypes, which assume the availabil-ity of perfect coreference information.Knowledge source analysis.
To gain some in-sight into the extent to which a knowledge sourceor a rule contributes to the overall performance ofthe rule-based approach, we conduct ablation ex-periments: in each experiment, we measure theperformance of the ruleset after removing a par-ticular rule or knowledge source from it.
Specifi-cally, rows 2?4 of Table 6 show the accuracies ofthe ruleset after removing the memorization rule(Rule 17), the rule that uses ReVerb?s output (Rule12), and the cue words used in Rules 4 and 10,respectively.
For easy reference, the accuracy ofthe original ruleset is shown in row 1 of the ta-ble.
According to the paired t-test (p < 0.05),performance drops significantly in all three abla-tion experiments.
This suggests that the memo-rization rule, ReVerb, and the cue words all con-tribute positively to the accuracy of the ruleset.Feature Type Gold Coref Stanford CorefAll rules 66.0 57.4?memorization 62.6 52.0?ReVerb 64.2 56.6?cue words 63.8 54.0Table 6: Accuracies of the simplified ruleset.R&N?s Features Our FeaturesIS Type R P F R P Fold 93.5 95.8 94.6 93.8 96.4 95.1med 89.3 71.2 79.2 93.3 86.0 89.5new 34.6 71.7 46.7 82.4 72.7 87.2Accuracy 82.9 91.7Table 7: Accuracies on IS types.IS type results.
We hypothesized earlier thatthe poor performance reported by Nissim andR&N on identifying new entities in their 3-classIS classification experiments (i.e., classifying anNP as old, med, or new) could be attributed totheir sole reliance on lexico-syntactic features.
Totest this hypothesis, we (1) train a 3-class classi-fier using the five types of features we employedin our learning-based approach, computing thefeatures based on the Stanford coreference chains;and (2) compare its results against those obtainedvia the lexico-syntactic approach in R&N on ourtest set.
Results of these experiments, which areshown in Table 7, substantiate our hypothesis:when we replace R&N?s features with ours, accu-racy rises from 82.9% to 91.7%.
These gains canbe attributed to large improvements in identifyingnew and med entities, for which F-scores increaseby about 40 points and 10 points, respectively.7 ConclusionsWe have examined the fine-grained IS determi-nation task.
Experiments on a set of Switch-board dialogues show that our learning-based ap-proach, which uses features that include hand-crafted rules and their predictions, outperforms itsrule-based counterpart by more than 20%, achiev-ing an overall accuracy of 78.7% when relying onautomatically computed coreference information.In addition, we have achieved state-of-the-art re-sults on the 3-class IS determination task, in partdue to our reliance on richer knowledge sourcesin comparison to prior work.
To our knowledge,there has been little work on automatic IS subtypedetermination.
We hope that our work can stimu-late further research on this task.806AcknowledgmentsWe thank the three anonymous reviewers for theirdetailed and insightful comments on an earlierdraft of the paper.
This work was supportedin part by NSF Grants IIS-0812261 and IIS-1147644.ReferencesCollin F. Baker, Charles J. Fillmore, and John B.Lowe.
1998.
The Berkeley FrameNet project.In Proceedings of the 36th Annual Meeting of theAssociation for Computational Linguistics and the17th International Conference on ComputationalLinguistics, Volume 1, pages 86?90.Sasha Calhoun, Jean Carletta, Jason Brenier, NeilMayo, Dan Jurafsky, Mark Steedman, and DavidBeaver.
2010.
The NXT-format Switchboard cor-pus: A rich resource for investigating the syntax, se-mantics, pragmatics and prosody of dialogue.
Lan-guage Resources and Evaluation, 44(4):387?419.Miriam Eckert and Michael Strube.
2001.
Dialogueacts, synchronising units and anaphora resolution.Journal of Semantics, 17(1):51?89.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 1535?1545.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, MA.Caroline Gasperin and Ted Briscoe.
2008.
Statisti-cal anaphora resolution in biomedical texts.
In Pro-ceedings of the 22nd International Conference onComputational Linguistics, pages 257?264.Michael Go?tze, Thomas Weskott, Cornelia En-driss, Ines Fiedler, Stefan Hinterwimmer, SvetlanaPetrova, Anne Schwarz, Stavros Skopeteas, andRuben Stoel.
2007.
Information structure.
InWorking Papers of the SFB632, InterdisciplinaryStudies on Information Structure (ISIS).
Potsdam:Universita?tsverlag Potsdam.Eva Hajic?ova?.
1984.
Topic and focus.
In Contri-butions to Functional Syntax, Semantics, and Lan-guage Comprehension (LLSEE 16), pages 189?202.John Benjamins, Amsterdam.Michael A. K. Halliday.
1976.
Notes on transitiv-ity and theme in English.
Journal of Linguistics,3(2):199?244.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2011.
Stanford?s multi-pass sieve corefer-ence resolution system at the CoNLL-2011 sharedtask.
In Proceedings of the Fifteenth Confer-ence on Computational Natural Language Learn-ing: Shared Task, pages 28?34.Malvina Nissim, Shipra Dingare, Jean Carletta, andMark Steedman.
2004.
An annotation scheme forinformation status in dialogue.
In Proceedings ofthe 4th International Conference on Language Re-sources and Evaluation, pages 1023?1026.Malvina Nissim.
2003.
Annotation schemefor information status in dialogue.
Availablefrom http://www.stanford.edu/class/cs224u/guidelines-infostatus.pdf.Malvina Nissim.
2006.
Learning information status ofdiscourse entities.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 94?102.Ellen F. Prince.
1981.
Toward a taxonomy of given-new information.
In P. Cole, editor, Radical Prag-matics, pages 223?255.
New York, N.Y.: AcademicPress.Ellen F. Prince.
1992.
The ZPG letter: Subjects,definiteness, and information-status.
In DiscourseDescription: Diverse Analysis of a Fund RaisingText, pages 295?325.
John Benjamins, Philadel-phia/Amsterdam.Altaf Rahman and Vincent Ng.
2011.
Learning theinformation status of noun phrases in spoken dia-logues.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 1069?1080.Arndt Riester, David Lorenz, and Nina Seemann.2010.
A recursive annotation scheme for referentialinformation status.
In Proceedings of the SeventhInternational Conference on Language Resourcesand Evaluation, pages 717?722.Mark Steedman.
2000.
The Syntactic Process.
TheMIT Press, Cambridge, MA.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun.
2004.
Support vec-tor machine learning for interdependent and struc-tured output spaces.
In Proceedings of the 21stInternational Conference on Machine Learning,pages 104?112.Enric Vallduv??.
1992.
The Informational Component.Garland, New York.807
