Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 75?78,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsProcessing Informal, Romanized Pakistani Text MessagesAnn Irvine and Jonathan Weese and Chris Callison-BurchCenter for Language and Speech ProcessingJohns Hopkins UniversityAbstractRegardless of language, the standard characterset for text messages (SMS) and many othersocial media platforms is the Roman alphabet.There are romanization conventions for somecharacter sets, but they are used inconsistentlyin informal text, such as SMS.
In this work, weconvert informal, romanized Urdu messagesinto the native Arabic script and normalizenon-standard SMS language.
Doing so pre-pares the messages for existing downstreamprocessing tools, such as machine translation,which are typically trained on well-formed,native script text.
Our model combines infor-mation at the word and character levels, al-lowing it to handle out-of-vocabulary items.Compared with a baseline deterministic ap-proach, our system reduces both word andcharacter error rate by over 50%.1 IntroductionThere are many reasons why systematically process-ing informal text, such as Twitter posts or text mes-sages, could be useful.
For example, during the Jan-uary 2010 earthquake in Haiti, volunteers translatedCreole text messages that survivors sent to Englishspeaking relief workers.
Machine translation (MT)could supplement or replace such crowdsourcing ef-forts in the future.
However, working with SMS datapresents several challenges.
First, messages mayhave non-standard spellings and abbreviations (?textspeak?
), which we need to normalize into standardlanguage.
Second, many languages that are typicallywritten in a non-Roman script use a romanized ver-sion for SMS, which we need to deromanize.
Nor-malizing and deromanizing SMS messages wouldallow us to use existing MT engines, which are typ-ically trained on well-formed sentences written intheir native-script, in order to translate the messages.With this work, we use and release a corpus of1 million (4, 195 annotated) anonymized text mes-sages sent in Pakistan1.
We deromanize and normal-ize messages written in Urdu, although the generalapproach is language-independent.
Using Mechan-ical Turk (MTurk), we collect normalized Arabicscript annotations of romanized messages in order toboth train and evaluate a Hidden Markov Model thatautomates the conversion.
Our model drasticallyoutperforms our baseline deterministic approach andits performance is comparable to the agreement be-tween annotators.2 Related WorkThere is a strong thread of research dedicated to nor-malizing Twitter and SMS informal English (Sproatet al, 2001).
Choudhury et al (2007) use a super-vised English SMS dataset and build a character-level HMM to normalize individual tokens.
Aw etal.
(2006) model the same task using a statistical MTsystem, making the output context-sensitive at thecost of including a character-level analysis.
Morerecently, Han and Baldwin (2011) use unsupervisedmethods to build a pipeline that identifies ill-formedEnglish SMS word tokens and builds a dictionaryof their most likely normalized forms.
Beaufort etal.
(2010) use a large amount of training data to su-pervise an FST-based French SMS normalizer.
Liand Yarowsky (2008) present methods that take ad-vantage of monolingual distributional similarities toidentify the full form of abbreviated Chinese words.One challenge in working with SMS data is that pub-lic data is sparse (Chen and Kan, 2011).
Translit-eration is well-studied (Knight and Graehl, 1997;Haizhou et al, 2004; Li et al, 2010) and is usuallyviewed as a subproblem of MT.With this work, we release a corpus of SMS mes-sages and attempt to normalize Urdu SMS texts.
Do-ing so involves the same challenges as normalizingEnglish SMS texts and has the added complexitythat we must also deromanize, a process similar tothe transliteration task.1See http://www.cs.jhu.edu/?anni/papers/urduSMS/ for details about obtaining the corpus.75!"#$#%&'()*++&$*!
,#-./(0&1&%($&#2(13(4&5&5('3$6(7$4&(1*(8&"1&#(13("1#(1*9(:1&'3(+1&2&+1(8&"1('39();<=>?@=!"#$%&#%'!
()*&!+',-./#$01#2.$!
!"#$!!"#$!%&!'#(!)%*!+!#,-*!%&!'(")*+!%&!'&,!%&!+!%./!
0#1#2!-*+!%*!3$%4056!7)#$54#2.$!
86')'!#)'!9.&!:'.:4';!5''/5!'<')9.$'!05!5=&*90$%>!.?!5=&*9!0=5!%..
*!Figure 1: Example of SMS with MTurk annotations3 Data and AnnotationOur Pakistani SMS dataset was provided by theTransnational Crisis Project, and it includes 1 mil-lion (724,999 unique) text messages that were sentin Pakistan just prior to the devastating July 2010floods.
The messages have been stripped of allmetadata including sender, receiver, and timestamp.Messages are written in several languages, thoughmost are in Urdu, English, or a combination of thetwo.
Regardless of language, all messages are com-posed in the Roman alphabet.
The dataset contains348,701 word types, 49.5% of which are singletons.We posted subsets of the SMS data to MTurk toperform language identification, followed by dero-manization and normalization on Urdu messages.In the deromanization and normalization task, weasked MTurk workers to convert all romanizedwords into script Urdu and use full, non-abbreviatedword forms.
We applied standard techniques foreliminating noise in the annotation set (Callison-Burch and Dredze, 2010) and limited annotators tothose in Pakistan.
We also asked annotators to in-dicate if a message contained private, sensitive, oroffensive material, and we removed such messagesfrom our dataset.We gathered deromanization and normalizationMTurk annotations for 4,195 messages.
In all ex-periments, we use 3,695 of our annotated SMS textsfor training and 500 for testing.
We found that 18%of word tokens and 44% of word types in the testdata do not appear in the training data.
An exampleof a fully annotated SMS is shown in Figure 1.Figure 2 shows that, in general, productive MTurkannotators also tend to produce high quality annota-tions, as measured by an additional round of MTurkannotations which asked workers to choose the bestannotation among the three we gathered.
The rawaverage annotator agreements as measured by char-acter and word level edit distance are 40.5 and 66.9,respectively.
However, the average edit distances0 100 200 300 4000.10.20.30.40.50.60.7Number of MTurk HITs completedPercentofannotationsvotedbestGood PerformanceMediocre PerformancePoor PerformanceFigure 2: Productivity vs. percent of annotations votedbest among three deromanizations gathered on MTurk.!
"#$%&' ()*+,$-+.
),/'01#2342,"56' 7,89$/:''!
"#' ;#+*'0<6';+#+*'0<6';+#*'0=6';+#2*'0>6' 8#+"2''$%&'#'' ;),/+'0?6';,'/+'0@6';,/+'0<6';),'/+'0=6' A:$":''()"*)+'' B4/#+'0>6'!
"#$0>6'B)/+#)'0>6'B4/#),'0>6' )&:2#'%2)%92'','-'' ;:4/:'0<=6';$'0>6';:4/'0>6' :+%%5'''./0'' C48,)'0>6'8+,C)'0>6' D#2E5''123$4'' F+&2$,'0G6'F+&2,'0G6'F++&2$,'0@6'F+&+$,'0@6'F&2$,'0<6' ":$&H":+&'Figure 3: Urdu words romanized in multiple ways.
TheUrdu word for ?2?
is pronounced approximately ?du.
?between ?good?
MTurk workers (at least 50% of aworker?s messages are voted best) and the deroman-ization which was voted best (when the two are dif-ferent) are 25.1 (character) and 53.7 (word).We used an automatic aligner to align the wordsin each Arabic script annotation to words in the orig-inal romanized message.
The alignments show anaverage fertility of 1.04 script words per romanizedword.
Almost all alignments are one-to-one andmonotonic.
Since there is no reordering, the align-ment is a simplified case of word alignment in MT.Using the aligned dataset, we examine how Urduwords are romanized.
The average entropy for non-singleton script word tokens is 1.49 bits.
This meansit is common for script words to be romanized inmultiple ways (4.2 romanizations per script word onaverage).
Figure 3 shows some examples.4 Deromanization and NormalizationIn order to deromanize and normalize Urdu SMStexts in a single step, we use a Hidden MarkovModel (HMM), shown in Figure 4.
To estimate theprobability that one native-script word follows an-76????
???
??????
?walo kia soratehalFigure 4: Illustration of HMM with an example fromSMS data.
English translation: ?What?s the situation?
?other, we use a bigram language model (LM) withadd-1 smoothing (Lidstone, 1920) and compare twosources of LM training data.We use two sources of data to estimate the prob-ability of a romanized word given a script word:(1) a dictionary of candidates generated from auto-matically aligned training data, (2) a character-basedtransliteration model (Irvine et al, 2010).If r is a romanized word and u is a script Urduword, the dictionary-based distribution, pDICT(r|u),is given by relative frequency estimations over thealigned training data, and the transliteration-baseddistribution, pTRANS(r|u), is defined by the transliter-ation model scores.
We define the model?s emissionprobability distribution as the linear interpolation ofthese two distributions:pe(r|u) = (1?
?
)pDICT(r|u) + ?pTRANS(r|u)When ?
= 0, the model uses only the dictionary,and when ?
= 1 only the transliterations.Intuitively, we want the dictionary-based model tomemorize patterns like abbreviations in the trainingdata and then let the transliterator take over when aromanized word is out-of-vocabulary (OOV).5 Results and discussionIn the eight experiments summarized in Table 1, wevary the following: (1) whether we estimate HMMemissions from the dictionary, the transliterator, orboth (i.e., we vary ?
), (2) language model trainingdata, and (3) transliteration model training data.Our baseline uses an Urdu-extension of the Buck-walter Arabic deterministic transliteration map.Even our worst-performing configuration outper-forms this baseline by a large margin, and the bestconfiguration has a performance comparable to theagreement among good MTurk workers.LM Translit ?
CER WER1 News ?
Dict 41.5 63.32 SMS ?
Dict 38.2 57.13 SMS Eng Translit 33.4 76.24 SMS SMS Translit 33.3 74.15 News SMS Both 29.0 58.16 News Eng Both 28.4 57.27 SMS SMS Both 25.0 50.18 SMS Eng Both 24.4 49.5Baseline: Buckwalter Determ.
64.6 99.9Good MTurk Annotator Agreement 25.1 53.7Table 1: Deromanization and normalization results on500 SMS test set.
Evaluation is by character (CER) andword error rate (WER); lower scores are better.
?LM?indicates the data used to estimate the language modelprobabilities: News refers to Urdu news corpus and SMSto deromanized side of our SMS training data.
?Translit?column refers to the training data that was used to trainthe transliterator: SMS; SMS training data; Eng; English-Urdu transliterations.
?
refers to the data used to estimateemissions: transliterations, dictionary entries, or both.Unsurprisingly, using the dictionary only (Exper-iments 1-2) performs better than using translitera-tions only (Experiments 3-4) in terms of word errorrate, and the opposite is true in terms of charactererror rate.
Using both the dictionary derived fromthe SMS training data and the transliterator (Experi-ments 5?8) outperforms using only one or the other(1?4).
This confirms our intuition that using translit-eration to account for OOVs in combination withword-level learning from the training data is a goodstrategy2.We compare results using two language modeltraining corpora: (1) the Urdu script side of ourSMS MTurk data, and (2) the Urdu side of an Urdu-English parallel corpus,3 which contains news-domain text.
We see that using the SMS MTurk data(7?8) outperforms the news text (5?6).
This is due tothe fact that the news text is out of domain with re-spect to the content of SMS texts.
In future work, weplan to mine Urdu script blog and chat data, whichmay be closer in domain to the SMS texts, providingbetter language modeling probabilities.2We experimented with different ?
values on held out dataand found its value did not impact system performance signifi-cantly unless it was set to 0 or 1, ignoring the transliterations ordictionary.
We set ?
= 0.5 for the rest of the experiments.3LDC2006E11077Training Freq.
bins Length Diff.
binsBin CER WER Bin CER WER100+ 9.8 14.8 0 23.5 43.310?99 15.2 22.1 1, 2 29.1 48.71?9 27.5 37.2 -1, -2 42.3 70.10 73.5 96.6 ?3 100.3 100.0?-3 66.4 87.3Table 2: Results on tokens in the test set, binned by train-ing frequency or difference in character length with theirreference.
Length differences are number of charactersin romanized token minus the number of characters in itsderomanization.
?
= 0.5 for all.We compare using a transliterator trained on ro-manized/deromanized word pairs extracted from theSMS text training data with a transliterator trainedon English words paired with their Urdu translitera-tions and find that performance is nearly equivalent.The former dataset is noisy, small, and in-domainwhile the latter is clean, large, and out-of-domain.We expect that the SMS word pairs based translit-erator would outperform the English-Urdu trainedtransliterator given more, cleaner data.To understand in more detail when our systemdoes well and when it does not, we performed ad-ditional experiments on the token level.
That is, in-stead of deromanizing and normalizing entire SMSmessages, we take a close look at the kinds of ro-manized word tokens that the system gets right andwrong.
We bin test set word tokens by their frequen-cies in the training data and by the difference be-tween their length (in characters) and the length oftheir reference deromanization.
Results are given inTable 2.
Not surprisingly, the system performs betteron tokens that it has seen many times in the trainingdata than on tokens it has never seen.
It does notperform perfectly on high frequency items becausethe entropy of many romanized word types is high.The system also performs best on romanized wordtypes that have a similar length to their deromanizedforms.
This suggests that the system is more suc-cessful at the deromanization task than the normal-ization task, where lengths are more likely to varysubstantially due to SMS abbreviations.6 SummaryWe have defined a new task: deromanizing and nor-malizing SMS messages written in non-native Ro-man script.
We have introduced a unique new anno-tated dataset that allows exploration of informal textfor a low resource language.ReferencesAiTi Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.
Aphrase-based statistical model for SMS text normaliza-tion.
In Proceedings of COLING/ACL.Richard Beaufort, Sophie Roekhaut, Louise-Ame?lieCougnon, and Ce?drick Fairon.
2010.
A hybridrule/model-based finite-state framework for normaliz-ing SMS messages.
In Proceedings of ACL.Chris Callison-Burch and Mark Dredze.
2010.
Creatingspeech and language data with Amazon?s MechanicalTurk.
In NAACL-NLT Workshop on Creating Speechand Language Data With Mechanical Turk.Tao Chen and Min-Yen Kan. 2011.
Creating a live, pub-lic short message service corpus: The NUS SMS cor-pus.
Computation and Language, abs/1112.2468.Monojit Choudhury, Vijit Jain Rahul Saraf, AnimeshMukherjee, Sudeshna Sarkar, and Anupam Basu.2007.
Investigation and modeling of the structure oftexting language.
In International Journal on Docu-ment Analysis and Recognition.Li Haizhou, Zhang Min, and Su Jian.
2004.
A jointsource-channel model for machine transliteration.
InProceedings of ACL.Bo Han and Timothy Baldwin.
2011.
Lexical normalisa-tion of short text messages: Makn sens a #twitter.
InProceedings of ACL.Ann Irvine, Chris Callison-Burch, and Alexandre Kle-mentiev.
2010.
Transliterating from all languages.
InProceedings of the Association for Machine Transla-tion in the America, AMTA ?10.Kevin Knight and Jonathan Graehl.
1997.
Machinetransliteration.
In Proceedings of ACL.Zhifei Li and David Yarowsky.
2008.
Unsupervisedtranslation induction for Chinese abbreviations usingmonolingual corpora.
In Proceedings of ACL/HLT.Haizhou Li, A Kumaran, Min Zhang, and Vladimir Per-vouchine.
2010.
Report of NEWS 2010 translitera-tion generation shared task.
In Proceedings of the ACLNamed Entities WorkShop.George James Lidstone.
1920.
Note on the general caseof the Bayes-Laplace formula for inductive or a poste-riori probabilities.
Transactions of the Faculty of Ac-tuaries, 8:182?192.Richard Sproat, Alan W. Black, Stanley F. Chen, ShankarKumar, Mari Ostendorf, and Christopher Richards.2001.
Normalization of non-standard words.
Com-puter Speech & Language, pages 287?333.78
