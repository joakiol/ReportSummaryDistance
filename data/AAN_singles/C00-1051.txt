Committee-based Decision Makingin Probabilistic Partial ParsingINU I  Takashi*  and INUI  Kentaro  *t?
Department  of Artificial Intelligence, Kyushu Inst itute of Technology?
PRESTO,  Japan Science and Technology Corporat ion{ t_ inu i ,  inu i}@plut  o. a i .
kyutech ,  ac.
jpAbst ractThis paper explores two directions ibr thenext step beyond the state of the art of statis-tical parsing: probabilistic partial parsing andcommittee-based decision making.
Probabilis-tic partial parsing is a probabilistic extension ofthe existing notion of partial parsing~ which en-ables fine-grained arbitrary choice on the trade-off between accuracy and coverage.
Committee-based decision making is to combine the out-puts from different systems to make a betterdecision.
While varions committee-based tech-niques for NLP have recently been investigated,they would need to be fln'ther extended so asto be applicable to probabilistic partial pars-ing.
Aiming at this coupling, this paper givesa general fl'amework to committee-based deci-sion making, which consists of a set of weight-ing flmctions and a combination function, anddiscusses how it can be coupled with probabilis-tic partial parsing.
Our ext)eriments have so farbeen producing promising results.1 In t roduct ionThere have been a number of attempts to usestatistical techniques to improve parsing perfor-mance.
While this goal has been achieved to acertain degree given the increasing availabilityof large tree banks, the remaining room tbr theimprovement appears to be getting saturatedas long as only statistical techniques are takeninto account.
This paper explores two directionstbr the next step beyond the state of the art ofstatistical parsing: probabilistic partial parsingand committee-based decision making.Probabilistic partial parsing is a probabilisticextension of the existing notion of partial pars-ing ( e.g.
(Jensen et al, 1993)) where a parserselects as its output only a part of the parse treethat are probabilistically highly reliable.
Thisdecision-making scheme enables a fine-grainedarbitrary choice on the trade-off between ac-curacy and coverage.
Such trade-oil is impor-tant since there are various applications that re-quire reasonably high accuracy even sacrificingcoverage.
A typical example is the t)araI)hras-ing task embedded in summarization, sentencesimplification (e.g.
(Carroll et al, 1998)), etc.Enabling such trade-off" choice will make state-o f  the-art parsers of wider application.
Partialparsing has also been proven useflll ibr boot-strapping leanfing.One may suspect hat the realization of par-tial parsing is a trivial matter in probabilisticparsing just because a probabilistic parser in-herently has the notion of "reliability" and thushas the trade-off:' between accuracy and cover-age.
However, there has so far been surpris-ingly little research focusing on this matter andahnost no work that evaluates statistical parsersaccording to their coverage-accuracy (or recall-precision) curves.
Taking the significance ofpartial parsing into account, therefi)re in thispaper, we evaluate parsing perfbrmance accord-ing tO coverage-accuracy cnrves.Committee-based decision making is to con>bine the outputs from several difl'erent systems(e.g.
parsers) to make a better decision.
Re-cently, there have been various attempts to at)-ply committee-based techniques to NLP taskssuch as POS tagging (Halteren et al, 1998;Brill et al, 1998), parsing (Henderson andBrill, 1999), word sense disambiguation (Peder-sen, 2000), machine translation (lh'ederking andNirenburg, 1994), and speech recognition (Fis-cus, 1997).
Those works empirically demon-strated that combining different systems oftenachieved significant improvelnents over the pre-vious best system.In order to couple those committee-based348schemes with t)robat)ilistic t)artial parsing, how-ever, Olle would still need to make a fllrther ex-tension.
Ainling at this coupling, ill this t)at)er,we consider a general framework of (:ommil, tee-based decision making that consists of ~ setof weighting flmctions mid a combination flmc-tion, and (lis('uss how that Kalnework enal)lesthe coupling with t)robal)ilistic t)artial t)m:sing.To denionstr~te how it works, we ret)ort the re-sults of our t)arsing exl)eriments on a Japanesetree bank.2 Probabilistic partial parsing2.1 Dependency  probab i l i tyIn this t)at)er, we consider the task of (le(:id-ing the det)endency structure of a Jat);mese in-put sentence.
Note that, while we restrict ore:discussion to analysis of Jat)anese senl;(;nc(;s inthis t)~l)er, what we present l)elow should alsot)e strnightfi?rwardly ?xt)plical)h~ to more wide-ranged tasks such as English det)endency anal-ysis just  like the t)roblem setting considered t)yCollins (1996).Givell ;m inl)ut sentence ,s as a sequence, ofB'unset,su-t)hrases (BPs) J, lq b2 .
.
.
lh~, our taskis to i(tent, i\[y their inter-BP del)endency struc-t , ,e  n = l,j)l,: = ',,,}, where(tenot;es that bi (let)on(Is on (or modities) bj.Let us consider a dependency p'roba, bility (I)P):P('r(bi, bj)l.s'), a t)rol)al)ility l;lu~t 'r(bi, b:j) hohtsin a Given senl:ence s: Vi.
E j  P(','(51, t , j ) l .4 = a.2.2  Es t imat ion  o f  DPsSome of the state-of:the-art 1)rol)at)ilis(;ic bm-guage inodels such as the l)ottomu t) modelsP(l~,l.,.)
propos,,d by Collins (1:)96) and Fujioet al (1998) directly est imate DPs tbr :~ givenint)ut , whereas other models su('h as PCFO-t)ased tel)down generation mod(;ls P(H,,,s) donot, (Charnink, 1997; Collins, 1997; Shir~fi et ~rl.,1998).
If the latter type of mod(,'ls were total lyexchlded fronl any committee, our commit;tee-based framework would not work well in I)rac-lice.
Fortm:ately, how(:ver, even tbr such amodel, one can still est imate l)l?s in the follow-ing way if the rood(;1 provides the n-best del)en-1A bunsctsu phrase (BP) is a chunk of words (-on-sist;ing of a content word (noun, verl), adjective, etc.
)accoml)mfied by sonic flmctional word(s) (i)arti(:le, mlx-iliary, etc.).
A .lai)anes(' sentc'nce can 1)c analyzed as asequence of BPs, which constitutes an inter-BP deI)en-dency structuredency structure candidates cout)led with prot)-abilistic scores.Let Ri be the i-th best del)endency st;ruct;ure(i = 1 , .
.
.
,  'n) of ;~ given input ,s' according to agiven model, and h;t ~H l)e a set; of H,i.
Then,,.,u, l,e csl;ima|;ed by the followingai)l)l"OXilnation equation:./)F?
7~H P(',(b,z, (1)where P'R.u is the probal)ilit;y mass of H, E 7~Lr,and prn.
is the probabi l i ty mass of R ~ ~Hthat suppori;s 'r(bi, bj).
Tile approximation er-ror c is given 1)y c < l;r~--1%, where l),p,, is 1;t2(;- -  l~p~ 'prol)abilil;y mass of all the dependency struc-ture candidates for s (see (Peele, 1993) for thel?roof).
This means that the al)t)roximation er-ror is negligil)le if P'R,, is sut\[iciently close to1),R, which holds for a reasonably small mlmt)er'n in lnOSt cases in practical statistical parsing.2.3 Coverage-accuracy  curvesWe then conside, r the task of selecting depen-dency relations whose est imated probabi l i ty ishigher I:han a (:e|:i;ain l;hreshoht o- (0 < a < 1).When (r is set 1;o be higher (closer to 1.0), t;heaccuracy is cxt)ected to become higher, whilethe coverage is ext)ecl;ed to become lowe,:, andvi(:e versm Here, (;over~ge C* and a,(;ctlra(;y Aare defined as follows:# of the.
decided relationsC# of nil the re, lations in I;\]le t;est so,}i2 )/~# of the COl'rectly decided relati?n~3~vJ A# of the decided relationsMoving the threshohl cr from 1.0 down to-ward 0.0, one (:an obtain a coverage-a(:cura(:y(:urve (C-A curve).
In 1)rol)al)ilistic t)artial pars-ing, we ewflunte the t)erforman('e of a model~mcording to its C-A curve.
A few examt)lesare shown in Figure 1, which were obtainedin our ext)erim(mt (see Section 4).
Ot)viously,Figure 1 shows that model A outt)erformed theor, her two.
To summarize a C-A cIlrve, we usethe l l -t)oint average of accuracy (l l-t)oint at:-curacy, hereafl;er), where the eleven points m'eC = 0.5, 0 .55 , .
.
.
,  1.0.
The accuracy of totalparsing correst)onds to the accuracy of the t)ointin a C-A curve where C = 1.0.
We call it total~ccuracy to distinguish it from l\]- l)oint at:el>racy.
Not;('.
that two models with equal achieve-349!A0.950.90.850 .80 0 .2  0 .4  0 .6  0 .8  1coverageFigure 1: C-A curvesmeuts in total accuracy may be different in l l -point accuracy.
In fact, we found such cases inour experiments reported below.
Plotting C-Acurves enable us to make a more fine-grainedperfbrmance valuation of a model.3 Committee-based probabilis-tic partial parsingWe consider a general scheme of comnfittee-based probabilistic partial parsing as illustratedin Figure 2.
Here we assume that each connnit-tee member M~ (k = 1 , .
.
.
,  m) provides a DPmatrix PM~(r(bi, bj)ls ) (bi, bj E s) tbr each in-put 8.
Those matrices are called inlmt matrices,and are give:: to the committee as its input.A committee consists of a set of weightingfunctions and a combination flmction.
The roleassigned to weighting flmctions is to standardizeinput matrices.
The weighting function associ-ated with model Mk transforms an input ma-trix given by MI~ to a weight matrix WaG- Themajority flmction then combines all the givenweight matrices to produce an output matrix O,which represents the final decision of the con>mittee.
One can consider various options forboth flmctions.3.1 Weight ing  funct ionsWe have so far considered the following threeoptions.S imple  The simplest option is to do nothing:~a~ = PA~ (,.
(b~, bj)l~) (4) ijo Mk where wij is the ( i , j )  element of I/VMk.Normal  A bare DP may not be a precise es-timation of the actual accuracy.
One can seethis by plotting probability-accuracy curves (P-A curves) as shown in Figure 3.
Figure 3 showsthat model A tends to overestimate DPs, while/O}lll l l l \[ttct ~ based  dec is ion  nlakil l l~ii " i - -  =" ,,models  it lpl lt  i weight matrices i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
:matrices"tV l : :  "~Vt~iglt|iJI g \]"u n ?1\]OllCF: Ct3mhinat lan I :mlc l \ [ .nFigure 2: Committee-based probabilistic partialparsing0,9;,~0,8=0.70.6C0.50,5 0.6 0.7 0.8 0.9dependency  probabi l i tyFigure 3: P-A curvesmodel C tends to underestimate DPs.
Thislneans that if A and C give different answerswith the same DP, C's answer is more likelyto be correct.
Thus, it is :sot necessarily agood strategy to simply use give:: bare DPs inweighted majority.
To avoid this problem, weconsider the tbllowing weighting flmction:w~J k =@lkAM~(PMk(','(bi, b:i)l.s)) (5)where AMk (P) is the function that returns theexpected accuracy of Mk's vote with its depen-Mk dency probability p, and oz i is a normalizationfactor.
Such a function can be trained by plot-ting a P-A curve fbr training data.
Note thattraining data should be shared by all the com-mittee members.
In practice, tbr training a P-Acurve, some smoothing technique should be ap-plied to avoid overfitting.C lass The standardization process in theabove option Normal  can also be seen as aneffort for reducing the averaged cross entropyof the model on test, data.
Since P-A curvestend to defi~,r not only between different mod-els but also between different problem classes,if one incorporates ome problem classificationinto (5), the averaged cross entropy is expected350to be reduced fllrther:'w~  =/~';'~AM.%(i)M~(r(b~,bj)l,s)) (6)where AMkcl, i (P) is the P-A curve of model Mkonly tbr the problems of class Cb~ in trainingdata, and flMk is a normalization factor.
For iprobleln classification, syntactie/lexieal featuresof bi may be useful.3.2  Combin ing  funct ionsFor combination flmctions, we have so far con-sidered only simple weighted voting, which av-erages the given weight matrices:1;I,Mk 1 v-" Mk?
= - -  2_, ~'J'U (7) ?iJ 'm,h=lwhere o.i.f/~:_ is the (i, j) element of O.Note that the committee-based partial pars-ing frmnework t)resented here can be see, n asa generalization of the 1)reviously proposedvoting-based techniques in the following re-spects:(a) A committee a(:(:epts probabilistically para-meterized votes as its intmt.
(d) A committee ac(:el)ts multil)le voting (i.e.
it;allow a comnfittee menfl)er to vote not onlyto the 1)est-scored calMi(late trot also to allother potential candidates).
((:) A.
(:ommittee 1)rovides a metals tbr standard-izing original votes.
(b) A committee outl)uts a 1)rot)abilisti(" distri-bution representing a tinal decision, whichconstitutes a C-A curve.For examt)le, none of simple voting techniquesfor word class tagging t)roposed 1)y van Hal-teren et al (1998) does not accepts multiplevoting.
Henderson and Brill (1999) examinedconstituent voting and naive Bayes classifi(:a-lion for parsing, ol)taining positive results ibreach.
Simple constituent voting, however, doesnot accept parametric votes.
While Naive Bayesseems to partly accept l)arametric multit)le vot-ing, it; does not consider either sl;andardizationor coverage/accuracy trade-off.4 Exper iments4.1  Set t ingsWe conducted eXl)erinmnts using the tbllow-ing tive statistical parsers:Table 1: The total / l l-t)oint accuracy achieved1)y each individual modeltotal 11-pointA 0.8974 0.9607B 0.8551 0.9281C 0.8586 0.9291D 0.8470 0.9266E 0.7885 0.8567?
KANA (Ehara, 1998): a bottom-up modelbased oll maxinmm entropy estimation,Since dependency score matrices given byKANA have no probabilistic semantics, wenormalized them tbr each row using a cer-tain function manually tuned for this parser.?
CI\]AGAKE (Fujio et al, 1998): an exten-sion of the bottom-up model proposed byCollins (Collins, 1996).?
Kanaymna's parser (Kanayama et al, 1999):a l)o|,tom-up model coupled with an HPSG.?
Shirai's parser (Shirai et al, 1998): a top-down model incorporating lexical collocationstatistics.
Equation (1) was used tbr estimat-ing DPs.?
Peach Pie Parser (Uchilnoto et al, 1999):a bottom-up model based on maximum en-tropy estimation.Note that these models were developed flfilyindependently of ea('h other, and have siglfifi-Calltly different (:haracters (Ii)r a comparison oftheir performance, see %tble 1).
In what Jbl-lows, these models are referred to anonymously.For the source of the training/test set, weused the Kyoto corpus (ver.2.0) (Kurohashi etal., 1.997), which is a collection of Japanesenewspaper articles mmotated in terms of wordboundaries, POS tags, BP boundaries, andinter-BP dependency relations.
The corpusoriginally contained 19,956 sentences.
To makethe training/test sets~ we tirst removed all thesentences that were rejected by any of the abovefive parsers (3,146 sentences).
For the remain-ing 16,810 sentences, we next checked the con-sistency of the BP boundaries given by theparsers since they had slightly different crite-ria tbr BP segmentation fl'om each other.
Inthis process, we tried to recover as many in-consistent boundaries as possible.
For example,we tbund there were quite a few cases wherea parser recoglfized a certain word sequence msa single BP, whereas ome other parser recog-nized the same sequence as two BPs.
In such3510.965\[ASimple DNormal  \ [ \ ]C lass0.960.9550.95A : 0.9607 \ \na ?o ~ ~00.975 \[e egFigure 4: l l -po int  accuracy: A included0.96 ', \[.~Normal mClass0.950.940.930.929291 f5 g a0Figure 5: l 1-point accuracy: B /C  includeda case, we regarded that  sequence as a singleBP under a certain condition.
An a result;, weobtained 13,990 sentences that  can be acceptedby all the parsers with all the BP boundariesconsistent 2 We used thin set tbr training andevaluation.For cloned tests, we used 11,192 sentences(66,536 BPs a) for both  training and tests.For open tests, we conducted five-fold cross-val idation on the whole sentence set.2In the BP concatenation process described here,quite a few trivial dependency relations between eigl,-boring BPs were removed from the test set.
This madeour test set slightly more difficult tlmn what it shouldhave 1)cert.3This is the total nmnber of BPs excluding the right-most two BPs for each sentence.
Since, in Jal)anese, aBP ahvays depends on a BP following it, the right-mostBP of a sentence does not (lei)(tnd on any other BP, andthe second right-most BP ahvays depends on the right-most BP.
Therefore, they were not seen as subjects ofevahmtion.0.97DSimple  \[21Normal mClass0.9650.970.960.955Figure 6: l l -po in t  accuracy: +KNPFor the classification of problems, we man-ually established the following twelve (:lasses,each of which is defined in terms of a certainnlol:phological pat tern  of depending BPs:1.. nonfinal BP wit, h a case marker "'wa (topic)"2. nominal BP with a case marker "no (POS)"3. nominal BP with a case marker "ga (NOM)"4. nominal BP with a case marker % (ACC)"5. nonlinal BP with a case marker "hi (DAT)"6. nominal BP with a case marker "de (LOC/.
.
.
)"nominal BP (residue)adnominal verbal BPverbal BP (residue)adverbadjectiveresidue4.2 Resu l ts  and d iscuss ionTable 1 shown the to ta l / l l -po in t  accuracy ofeach individual model.
The  performance of eachmodel widely ranged from 0.96 down to 0.86in l l -po int  accuracy.
Remember  that  A is theopt imal  model, and there are two second-bestmodels, B and C, which are closely comparable.In what tbllows, we use these achievements msthe baseline for evaluating the error reduct ionachieved by organizing a committee.The pertbrmanee of various committees isshown in Figure 4 and 5.
Our pr imary inter-est here is whether  the weighting functions pre-sented above effectively contr ibute to error re-duction.
According to those two figures, al-though the contr ibut ion of the f lmction Nor -ma l  were nor very visible, the flmction C lassconsistently improved the accuracy.
These re-sults can be a good evidence tbr the impor tantrole of weighting f lmctions in combining parsers.7.8.9.1().11.12.3520.960.940.92 t ~ t I I ,  \[iJFigure 7: Single voting vs.
Multiple votingWhile we manually tmill: the 1)roblem classiti('a-l;ion in our ext)erimen|;, autom;~I;ic (:lassiticationte.chniques will also 1)e obviously worth consid-ering.We l;hen e.on(tucted another exl)e, rime.nI; to ex-amine the, et\['e(-l;s of muli;it)le voting.
One (:ansl;raighi;forwardly sinn|late a single-voting com-nlil;tee by ret)lacing wij in equal;ion (7) with w~.
igiven by:, { wi.i (if' j = m'g m~xk 'wit~)=_ 0 (o|;he.wise) (S)The resull;s are showll in Figure 7, whichcorot)ares l;he original multi-voting committeesand l;he sinmlai;e(t single-voi:ing (:olmnil;l;ees.Clearly, in our se|;tings, multil)le voting signif-icanl;ly oul;pertbrmed single vol;ing 1)arti(:ul~rlywhen t;he size of a ('ommii;tee is small.The nexl; issues are whel;her ~ (:Omlnil;te,(', al-ways oul;perform its indivi(tmd memt)ers, mtd ifnot;, what should be (-onsidered in organizing acommii;i;ee.
Figure 4 and 5 show |;hal; COllllllil;-tees nol; ilmlu(ling t;he ot)timal model A achievedextensive imt)rovemenl;s, whereas the merit oforganizing COlmnitl;ees including A is not veryvisible.
This can be t)arl, ly attrilml;ed to thefa.ct that the corot)el;once of the, individual mem-l)ers widely diversed, and A signiti(:md;ly OUtl)er-forms the ol:her models.Given l,he good error reduct;ion achievedby commit, tees containing comt)ar~ble meml)erssueh ~s BC, BD a, nd B@I), however, it should t)ereasonable 1;o eXl)ect thai; a (:omlnil,l,e,e includ-ing A would achieve a significant imt)rovement; ifanol;her nearly ol)t;ilnal model was also incorl)o-0.8v0,70.fi0.5 0.6 |1,7 {).g 0.9dependency probabilityFigure 8: P-A curves: +KNPrated.
To empirically prove this assmnpl;ion, we,conduct;ed anot;her experiment, where we addanother parser KNP (Kurohashi el; al., 1 !
)94:) 1;oeach commil;|;ee that apt)ears in Figure 4.
KNI?is much closer to lnodel A in l;ol;al accuracyt;han t;he other models (0.8725 in tol;al accu-racy).
However, il; does not provide.
DP rea-l;rices since it is designed in a rule-l)ased fash-ion the current; version of KNP 1)rovides onlythe t)esl;-t)referrext parse t;ree for ea(:h inl)Ul; sen-tence without ~my scoring annotation.
We l;huslet KNP 1;o simply vol;e its l;ol;al aeem:aey.
Timresults art; shown in lqgure 6.
This time all l;hecommil;tees achieved significant improvemenl;s,wil;h |;he m~ximum e, rror re(hu:|;ion rate up l;o'3~%.As suggested 1)y |;he.
re, suits of t;his exl)erimenl;with KNP, our scheme Mlows a rule-based 11011-t)~r;m,el:ric p~rse.r t;o pb~y in a eommil;l;e.e pre-serving it;s ~d)ilit:y t;o oui;t)ul; t)aralnel;rie I)P ma-(;ri(:es.
To 1)ush (;he ~u'gumen(; fl,rl;her, SUl)pose;~ 1)lausil)le sil;ual;ion where we have ;m Ol)l;imall)ut non-1)arametrie rule-based parser and sev-eral suboptimal si;atistical parsers.
In su('h ~case, our commil;teeA)ased scheme may t)e ablel;o organize a commi|,tee that can 1)rovide l)Plnatri(:es while preserving the original tol;al ac-curacy of the rule-b~sed parser.
To set this, weconducted another small experiment, where, wecombined KNP with each of C and D, 1)oth ofwhi(:h are less compe.tent than KNP.
The result-ing (:ommil;l;ees successflflly t)rovided reasonal)leP-A curves as shown in Figure 8, while evenfurther lint)roving the original |;ol;al at:curacy ofKNP (0.8725 to 0.8868 tbr CF and 0.8860 forDF).
Furthermore, t;he COmlnittees also gainedthe 11-point accuracy over C and D (0.9291 to3530.9600 tbr CF and 0.9266 to 0.9561 for DF).These.
results suggest hat our committee-basedscheme does work even if the most competentmember of a committee is rule-based and thusnon-parametric.5 ConclusionThis paper presented a general committee-based frmnework that can be coupled with prob-abilistic partial parsing.
In this framework, acommittee accepts parametric multiple votes,and then standardizes them, and finally pro-vides a probabilistic distribution.
We presenteda general method for producing probabilisticmultiple votes (i.e.
DP matrices), which al-lows most of the existing probabilistic modelsfor parsing to join a committee.
Our experi-ments revealed that (a) if more than two compa-rably competent models are available, it is likelyto be worthwhile to combine them, (b) bothmultit)le voting and vote standardization effec-tively work in committee-based partial parsing,(c) our scheme also allows a non-parametricrule-based parser to make a good contribution.While our experiments have so far been produc-ing promising results, there seems to be muchroom left for investigation and improvement.AcknowledgmentsWe would like to express our special thanks toall the creators of the parsers used here for en-abling ~fll of this research by providing us theirsystems.
We would also like to thank the re-viewers tbr their suggestive comments.ReferencesBrill, E. and J. Wu.
Classifier Combination for ha-proved Lexical Disambiguation.
In Proc.
of the17th COLING, pp.191-195, 1998.Carroll, J. ,G. Minnen, Y. Cmming, S. Devlin andJ.
Tait.
Practical Simplification of English News-paper Text to Assist Aphasic Readers.
In Prvc.
ofAAAI-98 Workshop on Integrating Artificial In-telligence and Assistive Technology,1998.Charniak, E. Statistical parsing with a context-free grammar and word statistics.
In Prvc.
of theAAAI, pp.598 603, 1997.Collins, M. J.
A new statistical parser based on bi-grmn lexical dependencies.
In Proc.
of the 3~thACL, pp.184-191, 1996.Collins, M. J.
Three generative, lexicalised modelsfor statistical parsing.
In Proc.
of the 35th A CL,pp.16-23, 1997.Ehara, T. Estinlating the consistency of Japanesedependency relations based on the maximam en-trot)y modeling.
Ill Proc.
of the/~th Annual Meet-ing of The Association of Natural Language Pro-cessing, 1)t).382-385, 1998.
(In Japanese)Fiscus, J. G. A post-processing system to yield re-duced word error rates: Recognizer output votingerror reduction (ROVER).
In EuroSpccch, 1997.Fk'ederking, R. and S. Nirenburg.
Three heads arebetter titan one.
In Proc.
of the dth ANLP, 1994.Fujio, M. and Y. Matsmnoto.
Japmmse dependencystructure analysis based on lexicalized statistics.In Proc.
of the 3rd EMNLP, t)I).87-96, 1998.Henderson, J. C. and E. Brill.
Exploiting Diver-sity in Natural Language Processing: CombiningParsers.
In Proc.
of the 1999 Joint SIGDAT Con-fcrcncc on EMNLP and I/LC, pt).187--194.Jensen, K., G. E. Heidorn, and S. D. Richardson,editors, natural anguage processing: The PLNLPAppTvach.
Kluwer Academic Publishers, 1993.Kanayama, H., K. Torisawa, Y. Mitsuisi, andJ.
Tsujii.
Statistical Dependency Analysis withan HPSG-based Japanese Grainmar.
In Proc.
ofthe NLPRS, pp.138-143, 1999.Kurohashi, S. and M. Nagao.
Building a Jat)aneseparsed corpus while lint)roving tile parsing system.In Proc.
of NLPRS, pp.151-156, 1997.Kurohashi, S. and M. Nagao.
KN Parser : JapaneseDependency/Case Structure Analyzer.
in Proc.
ofTh.e httcrnational Worksh.op on Sharablc NaturalLang'aagc Rcso'arccs, pp.48-55, 1994.Poole, D. Average-case analysis of a search algo-rithm fl)r estimating prior and 1)ostcrior probabil-ities in Bayesian etworks with extreme 1)rot)abil-ities, thc i3th LICAL pp.606 612, 1993.Pedersen, T. A Simple AI)l)roach to Building En-sembles of Naive Bayesian Classifiers for WordSense Dismnbiguation In Proc.
of the NAACL,pp.63-69, 2000.Shirai, K., K. hmi, T. Tokunaga and H. TanakaAn empirical evaluation on statistical 1)arsingof Japanese sentences using a lexical associationstatistics, thc 3rd EMNLP, pp.80-87, 1998.Uchimoto, K., S. Sekine, and H. Isahara.
Japanesedependency structure analysis based on maxi-mum entopy models.
In Proc.
of thc 13th EACL,pp.196-203, 1999.van Halteren, H., J. Zavrel, and W. Daelemans.
hn-t)roving data driven wordclass tagging 1)y systemcombination.
In Proc.
of the 17th COLING, 1998.354
