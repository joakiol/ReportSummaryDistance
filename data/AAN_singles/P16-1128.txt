Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1351?1360,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsLearning Word Meta-EmbeddingsWenpeng Yin and Hinrich Sch?utzeCenter for Information and Language ProcessingLMU Munich, Germanywenpeng@cis.lmu.deAbstractWord embeddings ?
distributed represen-tations of words ?
in deep learning arebeneficial for many tasks in NLP.
How-ever, different embedding sets vary greatlyin quality and characteristics of the cap-tured information.
Instead of relying ona more advanced algorithm for embed-ding learning, this paper proposes an en-semble approach of combining differentpublic embedding sets with the aim oflearning metaembeddings.
Experimentson word similarity and analogy tasks andon part-of-speech tagging show better per-formance of metaembeddings compared toindividual embedding sets.
One advan-tage of metaembeddings is the increasedvocabulary coverage.
We release ourmetaembeddings publicly at http://cistern.cis.lmu.de/meta-emb.1 IntroductionRecently, deep neural network (NN) models haveachieved remarkable results in NLP (Collobert andWeston, 2008; Sutskever et al, 2014; Yin andSch?utze, 2015).
One reason for these resultsare word embeddings, compact distributed wordrepresentations learned in an unsupervised mannerfrom large corpora (Bengio et al, 2003; Mnih andHinton, 2009; Mikolov et al, 2013a; Penningtonet al, 2014).Some prior work has studied differences in per-formance of different embedding sets.
For exam-ple, Chen et al (2013) show that the embeddingsets HLBL (Mnih and Hinton, 2009), SENNA(Collobert and Weston, 2008), Turian (Turian etal., 2010) and Huang (Huang et al, 2012) havegreat variance in quality and characteristics of thesemantics captured.
Hill et al (2014; 2015a) showthat embeddings learned by NN machine transla-tion models can outperform three representativemonolingual embedding sets: word2vec (Mikolovet al, 2013b), GloVe (Pennington et al, 2014) andCW (Collobert and Weston, 2008).
Bansal et al(2014) find that Brown clustering, SENNA, CW,Huang and word2vec yield significant gains fordependency parsing.
Moreover, using these repre-sentations together achieved the best results, sug-gesting their complementarity.
These prior stud-ies motivate us to explore an ensemble approach.Each embedding set is trained by a different NNon a different corpus, hence can be treated as adistinct description of words.
We want to lever-age this diversity to learn better-performing wordembeddings.
Our expectation is that the ensemblecontains more information than each componentembedding set.The ensemble approach has two benefits.
First,enhancement of the representations: metaembed-dings perform better than the individual embed-ding sets.
Second, coverage: metaembeddingscover more words than the individual embeddingsets.
The first three ensemble methods we intro-duce are CONC, SVD and 1TON and they directlyonly have the benefit of enhancement.
They learnmetaembeddings on the overlapping vocabulary ofthe embedding sets.
CONC concatenates the vec-tors of a word from the different embedding sets.SVD performs dimension reduction on this con-catenation.
1TON assumes that a metaembeddingfor the word exists, e.g., it can be a randomlyinitialized vector in the beginning, and uses thismetaembedding to predict representations of theword in the individual embedding sets by projec-tions ?
the resulting fine-tuned metaembedding isexpected to contain knowledge from all individualembedding sets.To also address the objective of increased cov-erage of the vocabulary, we introduce 1TON+,1351a modification of 1TON that learns metaembed-dings for all words in the vocabulary union in onestep.
Let an out-of-vocabulary (OOV) word wof embedding set ES be a word that is not cov-ered by ES (i.e., ES does not contain an embed-ding for w).11TON+first randomly initializes theembeddings for OOVs and the metaembeddings,then uses a prediction setup similar to 1TON toupdate metaembeddings as well as OOV embed-dings.
Thus, 1TON+simultaneously achieves twogoals: learning metaembeddings and extendingthe vocabulary (for both metaembeddings and in-vidual embedding sets).An alternative method that increases cover-age is MUTUALLEARNING.
MUTUALLEARNINGlearns the embedding for a word that is an OOV inembedding set from its embeddings in other em-bedding sets.
We will use MUTUALLEARNINGto increase coverage for CONC, SVD and 1TON,so that these three methods (when used togetherwith MUTUALLEARNING) have the advantagesof both performance enhancement and increasedcoverage.In summary, metaembeddings have two benefitscompared to individual embedding sets: enhance-ment of performance and improved coverage ofthe vocabulary.
Below, we demonstrate this ex-perimentally for three tasks: word similarity, wordanalogy and POS tagging.If we simply view metaembeddings as a way ofcoming up with better embeddings, then the alter-native is to develop a single embedding learningalgorithm that produces better embeddings.
Someimprovements proposed before have the disadvan-tage of increasing the training time of embeddinglearning substantially; e.g., the NNLM presentedin (Bengio et al, 2003) is an order of magnitudeless efficient than an algorithm like word2vec and,more generally, replacing a linear objective func-tion with a nonlinear objective function increasestraining time.
Similarly, fine-tuning the hyperpa-rameters of the embedding learning algorithm iscomplex and time consuming.
In terms of cover-age, one might argue that we can retrain an ex-isting algorithm like word2vec on a bigger cor-pus.
However, that needs much longer trainingtime than our simple ensemble approaches whichachieve coverage as well as enhancement with lesseffort.
In many cases, it is not possible to retrain1We do not consider words in this paper that are not cov-ered by any of the individual embedding sets.
OOV refers toa word that is covered by a proper subset of ESs.using a different algorithm because the corpus isnot publicly available.
But even if these obsta-cles could be overcome, it is unlikely that thereever will be a single ?best?
embedding learn-ing algorithm.
So the current situation of multi-ple embedding sets with different properties be-ing available is likely to persist for the forseeablefuture.
Metaembedding learning is a simple andefficient way of taking advantage of this diver-sity.
As we will show below they combine severalcomplementary embedding sets and the resultingmetaembeddings are stronger than each individualset.2 Related WorkRelated work has focused on improving perfor-mance on specific tasks by using several embed-ding sets simultaneously.
To our knowledge, thereis no work that aims to learn generally usefulmetaembeddings from individual embedding sets.Tsuboi (2014) incorporates word2vec andGloVe embeddings into a POS tagging system andfinds that using these two embedding sets togetheris better than using them individually.
Similarly,Turian et al (2010) find that using Brown clus-ters, CW embeddings and HLBL embeddings forName Entity Recognition and chunking tasks to-gether gives better performance than using theserepresentations individually.Luo et al (2014) adapt CBOW (Mikolov etal., 2013a) to train word embeddings on differ-ent datasets ?
a Wikipedia corpus, search click-through data and user query data ?
for web searchranking and for word similarity.
They show thatusing these embeddings together gives stronger re-sults than using them individually.Both (Yin and Sch?utze, 2015) and (Zhang etal., 2016) try to incorporate multiple embeddingsets into channels of convolutional neural networksystem for sentence classification tasks.
The bet-ter performance also hints the complementarity ofcomponent embedding sets, however, such kind ofincorporation brings large numbers of training pa-rameters.In sum, these papers show that using multipleembedding sets is beneficial.
However, they ei-ther use embedding sets trained on the same cor-pus (Turian et al, 2010) or enhance embeddingsets by more training data, not by innovative learn-ing algorithms (Luo et al, 2014), or make thesystem architectures more complicated (Yin and1352Vocab Size Dim Training DataHLBL (Mnih and Hinton, 2009) 246,122 100 Reuters English newswire August 1996-August 1997Huang (Huang et al, 2012) 100,232 50 April 2010 snapshot of WikipediaGlove (Pennington et al, 2014) 1,193,514 300 42 billion tokens of web data, from Common CrawlCW (Collobert and Weston, 2008) 268,810 200 Reuters English newswire August 1996-August 1997word2vec (Mikolov et al, 2013b) 929,022 300 About 100 billion tokens from Google NewsTable 1: Embedding Sets (Dim: dimensionality of word embeddings).Sch?utze, 2015; Zhang et al, 2016).
In our work,we can leverage any publicly available embed-ding set learned by any learning algorithm.
Ourmetaembeddings (i) do not require access to re-sources such as large computing infrastructures orproprietary corpora; (ii) are derived by fast andsimple ensemble learning from existing embed-ding sets; and (iii) have much lower dimensional-ity than a simple concatentation, greatly reducingthe number of parameters in any system that usesthem.An alternative to learning metaembeddingsfrom embeddings is the MVLSA method thatlearns powerful embeddings directly from multi-ple data sources (Rastogi et al, 2015).
Rastogi etal.
(2015) combine a large number of data sourcesand also run two experiments on the embeddingsets Glove and word2vec.
In contrast, our fo-cus is on metaembeddings, i.e., embeddings thatare exclusively based on embeddings.
The ad-vantages of metaembeddings are that they outper-form individual embeddings in our experiments,that few computational resources are needed, thatno access to the original data is required andthat embeddings learned by new powerful (includ-ing nonlinear) embedding learning algorithms inthe future can be immediately taken advantage ofwithout any changes being necessary to our basicframework.
In future work, we hope to compareMVLSA and metaembeddings in effectiveness (Isusing the original corpus better than using embed-dings in some cases?)
and efficiency (Is usingSGD or SVD more efficient and in what circum-stances?
).3 Experimental Embedding SetsIn this work, we use five released embedding sets.
(i) HLBL.
Hierarchical log-bilinear (Mnih andHinton, 2009) embeddings released by Turian etal.
(2010);2246,122 word embeddings, 100 di-mensions; training corpus: RCV1 corpus (ReutersEnglish newswire, August 1996 ?
August 1997).2metaoptimize.com/projects/wordreprs(ii) Huang.3Huang et al (2012) incorporateglobal context to deal with challenges raised bywords with multiple meanings; 100,232 word em-beddings, 50 dimensions; training corpus: April2010 snapshot of Wikipedia.
(iii) GloVe4(Pen-nington et al, 2014).
1,193,514 word embed-dings, 300 dimensions; training corpus: 42 billiontokens of web data, from Common Crawl.
(iv)CW (Collobert and Weston, 2008).
Released byTurian et al (2010);5268,810 word embeddings,200 dimensions; training corpus: same as HLBL.
(v) word2vec (Mikolov et al, 2013b) CBOW;6929,022 word embeddings (we discard phraseembeddings), 300 dimensions; training corpus:Google News (about 100 billion words).
Table 1gives a summary of the five embedding sets.The intersection of the five vocabularies has size35,965, the union has size 2,788,636.4 Ensemble MethodsThis section introduces the four ensemble meth-ods: CONC, SVD, 1TON and 1TON+.4.1 CONC: ConcatenationIn CONC, the metaembedding of w is the con-catenation of five embeddings, one each from thefive embedding sets.
For GloVe, we perform L2normalization for each dimension across the vo-cabulary as recommended by the GloVe authors.Then each embedding of each embedding set isL2-normalized.
This ensures that each embeddingset contributes equally (a value between -1 and 1)when we compute similarity via dot product.We would like to make use of prior knowl-edge and give more weight to well performing em-bedding sets.
In this work, we give GloVe andword2vec weight i > 1 and weight 1 to the otherthree embedding sets.
We use MC30 (Miller andCharles, 1991) as dev set, since all embedding setsfully cover it.
We set i = 8, the value in Figure 13ai.stanford.edu/?ehhuang4nlp.stanford.edu/projects/glove5metaoptimize.com/projects/wordreprs6code.google.com/p/Word2Vec1353where performance reaches a plateau.
After L2normalization, GloVe and word2vec embeddingsare multiplied by i and remaining embedding setsare left unchanged.The dimensionality of CONC metaembeddingsis k = 100+50+300+200+300 = 950.
We alsotried equal weighting, but the results were muchworse, hence we skip reporting it.
It neverthelessgives us insight that simple concatenation, withoutstudying the difference among embedding sets, isunlikely to achieve enhancement.
The main disad-vantage of simple concatenation is that word em-beddings are commonly used to initialize wordsin DNN systems; thus, the high-dimensionality ofconcatenated embeddings causes a great increasein training parameters.4.2 SVD: Singular Value DecompositionWe do SVD on above weighted concatenation vec-tors of dimension k = 950.Given a set of CONC representations for nwords, each of dimensionality k, we compute anSVD decomposition C = USVTof the corre-sponding n?k matrix C. We then use Ud, the firstd dimensions of U , as the SVD metaembeddingsof the n words.
We apply L2-normalization toembeddings; similarities of SVD vectors are com-puted as dot products.d denotes the dimensionality of metaembed-dings in SVD, 1TON and 1TON+.
We use d =200 throughout and investigate the impact of d be-low.4.3 1TONFigure 2 depicts the simple neural network we em-ploy to learn metaembeddings in 1TON.
Whitei2 4 6 8 10 12 14 16 18 20performance0.650.70.750.80.850.9MC30Figure 1: Performance vs.
Weight scalar irectangles denote known embeddings.
The targetto learn is the metaembedding (shown as shadedrectangle).
Metaembeddings are initialized ran-domly.Figure 2: 1toNLet c be the number of embedding sets underconsideration, V1, V2, .
.
.
, Vi, .
.
.
, Vctheir vocab-ularies and V?= ?ci=1Vithe intersection, usedas training set.
Let V?denote the metaembeddingspace.
We define a projection f?ifrom space V?tospace Vi(i = 1, 2, .
.
.
, c) as follows:w?i= M?iw?
(1)where M?i?
Rdi?d, w??
Rdis the metaembed-ding of word w in space V?and w?i?
Rdiis theprojected (or learned) representation of word w inspace Vi.
The training objective is as follows:E =c?i=1ki?
(?w?V?|w?i?wi|2+ l2?
|M?i|2) (2)In Equation 2, kiis the weight scalar of the ithem-bedding set, determined in Section 4.1, i.e, ki= 8for GloVe and word2vec embedding sets, other-wise ki= 1; l2is the weight of L2 normalization.The principle of 1TON is that we treat each in-dividual embedding as a projection of the metaem-bedding, similar to principal component analysis.An embedding is a description of the word basedon the corpus and the model that were used to cre-ate it.
The metaembedding tries to recover a morecomprehensive description of the word when it istrained to predict the individual descriptions.1TON can also be understood as a sentencemodeling process, similar to DBOW (Le andMikolov, 2014).
The embedding of each word ina sentence s is a partial description of s. DBOWcombines all partial descriptions to form a com-prehensive description of s. DBOW initializes thesentence representation randomly, then uses thisrepresentation to predict the representations of in-dividual words.
The sentence representation of scorresponds to the metaembedding in 1TON; andthe representations of the words in s correspond tothe five embeddings for a word in 1TON.13544.4 1TON+Recall that an OOV (with respect to embedding setES) is defined as a word unknown in ES.
1TON+is an extension of 1TON that learns embeddingsfor OOVs; thus, it does not have the limitation thatit can only be run on overlapping vocabulary.Figure 3: 1toN+Figure 3 depicts 1TON+.
In contrast to Figure2, we assume that the current word is an OOV inembedding sets 3 and 5.
Hence, in the new learn-ing task, embeddings 1, 2, 4 are known, and em-beddings 3 and 5 and the metaembedding are tar-gets to learn.We initialize all OOV representations andmetaembeddings randomly and use the same map-ping formula as for 1TON to connect a metaem-bedding with the individual embeddings.
Bothmetaembedding and initialized OOV embeddingsare updated during training.Each embedding set contains information aboutonly a part of the overall vocabulary.
However, itcan predict what the remaining part should looklike by comparing words it knows with the infor-mation other embedding sets provide about thesewords.
Thus, 1TON+learns a model of the de-pendencies between the individual embedding setsand can use these dependencies to infer what theembedding of an OOV should look like.CONC, SVD and 1TON compute metaembed-dings only for the intersection vocabulary.
1TON+computes metaembeddings for the union of all in-dividual vocabularies, thus greatly increasing thecoverage of individual embedding sets.5 MUTUALLEARNINGMUTUALLEARNING is a method that extendsCONC, SVD and 1TON such that they have in-creased coverage of the vocabulary.
With MU-TUALLEARNING, all four ensemble methods ?CONC, SVD, 1TON and 1TON+?
have the ben-efits of both performance enhancement and in-creased coverage and we can use criteria like per-formance, compactness and efficiency of trainingbs lr l21TON 200 0.005 5?
10?4MUTUALLEARNING (ml) 200 0.01 5?
10?81TON+2000 0.005 5?
10?4Table 2: Hyperparameters.
bs: batch size; lr:learning rate; l2: L2 weight.to select the best ensemble method for a particularapplication.MUTUALLEARNING is applied to learn OOVembeddings for all c embedding sets; however,for ease of exposition, let us assume we want tocompute embeddings for OOVs for embedding setj only, based on known embeddings in the otherc?
1 embedding sets, with indexes i ?
{1 .
.
.
j ?1, j + 1 .
.
.
c}.
We do this by learning c?
1 map-pings fij, each a projection from embedding setEito embedding set Ej.Similar to Section 4.3, we train mapping fijon the intersection Vi?
Vjof the vocabulariescovered by the two embedding sets.
Formally,w?j= fij(wi) = Mijwiwhere Mij?
Rdj?di,wi?
Rdidenotes the representation of word win space Viand w?jis the projected metaembed-ding of word w in space Vj.
Training loss has thesame form as Equation 2 except that there is no??ci=1ki?
term.
A total of c ?
1 projections fijare trained to learn OOV embeddings for embed-ding set j.Let w be a word unknown in the vocabulary Vjof embedding set j, but known in V1, V2, .
.
.
, Vk.To compute an embedding for w in Vj, we firstcompute the k projections f1j(w1), f2j(w2), .
.
.,fkj(wk) from the source spaces V1, V2, .
.
.
, Vktothe target space Vj.
Then, the element-wise aver-age of f1j(w1), f2j(w2), .
.
., fkj(wk) is treatedas the representation of w in Vj.
Our motivation isthat ?
assuming there is a true representation of win Vjand assuming the projections were learnedwell ?
we would expect all the projected vectorsto be close to the true representation.
Also, eachsource space contributes potentially complemen-tary information.
Hence averaging them is a bal-ance of knowledge from all source spaces.6 ExperimentsWe train NNs by back-propagation with AdaGrad(Duchi et al, 2011) and mini-batches.
Table 2gives hyperparameters.We report results on three tasks: word similar-ity, word analogy and POS tagging.1355Model SL999 WS353 MC30 MEN RW sem.
syn.
tot.ind-full1 HLBL 22.1 (1) 35.7 (3) 41.5 (0) 30.7 (128) 19.1 (892) 27.1 (423) 22.8 (198) 24.72 Huang 9.7 (3) 61.7 (18) 65.9 (0) 30.1 (0) 6.4 (982) 8.4 (1016) 11.9 (326) 10.43 GloVe 45.3 (0) 75.4 (18) 83.6 (0) 81.6 (0) 48.7 (21) 81.4 (0) 70.1 (0) 75.24 CW 15.6 (1) 28.4 (3) 21.7 (0) 25.7 (129) 15.3 (896) 17.4 (423) 5.0 (198) 10.55 W2V 44.2 (0) 69.8 (0) 78.9 (0) 78.2 (54) 53.4 (209) 77.1 (0) 74.4 (0) 75.6ind-overlap6 HLBL 22.3 (3) 34.8 (21) 41.5 (0) 30.4 (188) 22.2 (1212) 13.8 (8486) 15.4 (1859) 15.47 Huang 9.7 (3) 62.0 (21) 65.9 (0) 30.7 (188) 3.9 (1212) 27.9 (8486) 9.9 (1859) 10.78 GloVe 45.0 (3) 75.5 (21) 83.6 (0) 81.4 (188) 59.1 (1212) 91.1 (8486) 68.2 (1859) 69.29 CW 16.0 (3) 30.8 (21) 21.7 (0) 24.7 (188) 17.4 (1212) 11.2 (8486) 2.3 (1859) 2.710 W2V 44.1 (3) 69.3 (21) 78.9 (0) 77.9 (188) 61.5 (1212) 89.3 (8486) 72.6 (1859) 73.3discard11 CONC (-HLBL) 46.0 (3) 76.5 (21) 86.3 (0) 82.2 (188) 63.0 (1211) 93.2 (8486) 74.0 (1859) 74.812 CONC (-Huang) 46.1 (3) 76.5 (21) 86.3 (0) 82.2 (188) 62.9 (1212) 93.2 (8486) 74.0 (1859) 74.813 CONC (-GloVe) 44.0 (3) 69.4 (21) 79.1 (0) 77.9 (188) 61.5 (1212) 89.3 (8486) 72.7 (1859) 73.414 CONC (-CW) 46.0 (3) 76.5 (21) 86.6 (0) 82.2 (188) 62.9 (1212) 93.2 (8486) 73.9 (1859) 74.715 CONC (-W2V) 45.0 (3) 75.5 (21) 83.6 (0) 81.6 (188) 59.1 (1212) 90.9 (8486) 68.3 (1859) 69.216 SVD (-HLBL) 48.5 (3) 76.1 (21) 85.6 (0) 82.5 (188) 61.5 (1211) 90.6 (8486) 69.5 (1859) 70.417 SVD (-Huang) 48.8 (3) 76.5 (21) 85.4 (0) 83.0 (188) 61.7 (1212) 91.4 (8486) 69.8 (1859) 70.718 SVD (-GloVe) 46.2 (3) 66.9 (21) 81.6 (0) 78.8 (188) 59.1 (1212) 88.8 (8486) 67.3 (1859) 68.219 SVD (-CW) 48.5 (3) 76.1 (21) 85.7 (0) 82.5 (188) 61.5 (1212) 90.6 (8486) 69.5 (1859) 70.420 SVD (-W2V) 49.4 (3) 79.0 (21) 87.3 (0) 83.1 (188) 59.1 (1212) 90.3 (8486) 66.0 (1859) 67.121 1TON (-HLBL) 46.3 (3) 75.8 (21) 83.0 (0) 82.1 (188) 60.5 (1211) 91.9 (8486) 75.9 (1859) 76.522 1TON (-Huang) 46.5 (3) 75.8 (21) 82.3 (0) 82.4 (188) 60.5 (1212) 93.5 (8486) 76.3 (1859) 77.023 1TON (-GloVe) 43.4 (3) 67.5 (21) 75.6 (0) 76.1 (188) 57.3 (1212) 89.0 (8486) 73.8 (1859) 74.524 1TON (-CW) 47.4 (3) 76.5 (21) 84.8 (0) 82.9 (188) 62.3 (1212) 91.4 (8486) 73.1 (1859) 73.825 1TON (-W2V) 46.3 (3) 76.2 (21) 80.0 (0) 81.5 (188) 56.8 (1212) 92.2 (8486) 72.2 (1859) 73.026 1TON+(-HLBL) 46.1 (3) 75.8 (21) 85.5 (0) 82.1 (188) 62.3 (1211) 92.2 (8486) 76.2 (1859) 76.927 1TON+(-Huang) 46.2 (3) 76.1 (21) 86.3 (0) 82.4 (188) 62.2 (1212) 93.8 (8486) 76.1 (1859) 76.828 1TON+(-GloVe) 45.3 (3) 71.2 (21) 80.0 (0) 78.8 (188) 62.5 (1212) 90.0 (8486) 73.3 (1859) 74.029 1TON+(-CW) 46.9 (3) 78.1 (21) 85.5 (0) 82.5 (188) 62.7 (1212) 91.8 (8486) 73.3 (1859) 74.130 1TON+(-W2V) 45.8 (3) 76.2 (21) 84.4 (0) 81.3 (188) 60.9 (1212) 92.4 (8486) 72.4 (1859) 73.2ensemble31 CONC 46.0 (3) 76.5 (21) 86.3 (0) 82.2 (188) 62.9 (1212) 93.2 (8486) 74.0 (1859) 74.832 SVD 48.5 (3) 76.0 (21) 85.7 (0) 82.5 (188) 61.5 (1212) 90.6 (8486) 69.5 (1859) 70.433 1TON 46.4 (3) 74.5 (21) 80.7 (0) 81.6 (188) 60.1 (1212) 91.9 (8486) 76.1 (1859) 76.834 1TON+46.3 (3) 75.3 (21) 85.2 (0) 80.8 (188) 61.6 (1212) 92.5 (8486) 76.3 (1859) 77.035 state-of-the-art 68.5 81.0 ?
?
?
?
?
?Table 3: Results on five word similarity tasks (Spearman correlation metric) and analogical reasoning(accuracy).
The number of OOVs is given in parentheses for each result.
?ind-full/ind-overlap?
: indi-vidual embedding sets with respective full/overlapping vocabulary; ?ensemble?
: ensemble results usingall five embedding sets; ?discard?
: one of the five embedding sets is removed.
If a result is betterthan all methods in ?ind-overlap?, then it is bolded.
Significant improvement over the best baselinein ?ind-overlap?
is underlined (online toolkit from http://vassarstats.net/index.html forSpearman correlation metric, test of equal proportions for accuracy, p < .05).RW(21) semantic syntactic totalRND AVG ml 1TON+RND AVG ml 1TON+RND AVG ml 1TON+RND AVG ml 1TON+indHLBL 7.4 6.9 17.3 17.5 26.3 26.4 26.3 26.4 22.4 22.4 22.7 22.9 24.1 24.2 24.4 24.5Huang 4.4 4.3 6.4 6.4 1.2 2.7 21.8 22.0 7.7 4.1 10.9 11.4 4.8 3.3 15.8 16.2CW 7.1 10.6 17.3 17.7 17.2 17.2 16.7 18.4 4.9 5.0 5.0 5.5 10.5 10.5 10.3 11.4ensembleCONC 14.2 16.5 48.3 ?
4.6 18.0 88.1 ?
62.4 15.1 74.9 ?
36.2 16.3 81.0 ?SVD 12.4 15.7 47.9 ?
4.1 17.5 87.3 ?
54.3 13.6 70.1 ?
31.5 15.4 77.9 ?1TON 16.7 11.7 48.5 ?
4.2 17.6 88.2 ?
60.0 15.0 76.8 ?
34.7 16.1 82.0 ?1TON+?
?
?
48.8 ?
?
?
88.4 ?
?
?
76.3 ?
?
?
81.1Table 4: Comparison of effectiveness of four methods for learning OOV embeddings.
RND: randominitialization.
AVG: average of embeddings of known words.
ml: MUTUALLEARNING.
RW(21) meansthere are still 21 OOVs for the vocabulary union.13566.1 Word Similarity and Analogy TasksWe evaluate on SimLex-999 (Hill et al, 2015b),WordSim353 (Finkelstein et al, 2001), MEN(Bruni et al, 2014) and RW (Luong et al, 2013).For completeness, we also show results for MC30,the validation set.The word analogy task proposed in (Mikolov etal., 2013b) consists of questions like, ?a is to b asc is to ??.
The dataset contains 19,544 such ques-tions, divided into a semantic subset of size 8869and a syntactic subset of size 10,675.
Accuracy isreported.We also collect the state-of-the-art report foreach task.
SimLex-999: (Wieting et al, 2015),WS353: (Halawi et al, 2012).
Not all state-of-the-art results are included in Table 3.
One reasonis that a fair comparison is only possible on theshared vocabulary, so methods without releasedembeddings cannot be included.
In addition, someprior systems can possibly generate better per-formance, but those literature reported lower re-sults than ours because different hyperparametersetup, such as smaller dimensionality of word em-beddings or different evaluation metric.
In anycase, our main contribution is to present ensem-ble frameworks which show that a combination ofcomplementary embedding sets produces better-performing metaembeddings.Table 3 reports results on similarity and anal-ogy.
Numbers in parentheses are the sizes ofwords in the datasets that are uncovered by inter-section vocabulary.
We do not consider them forfair comparison.
Block ?ind-full?
(1-5) lists theperformance of individual embedding sets on thefull vocabulary.
Results on lines 6-34 are for theintersection vocabulary of the five embedding sets:?ind-overlap?
contains the performance of individ-ual embedding sets, ?ensemble?
the performanceof our four ensemble methods and ?discard?
theperformance when one component set is removed.The four ensemble approaches are very promis-ing (31-34).
For CONC, discarding HLBL, Huangor CW does not hurt performance: CONC (31),CONC(-HLBL) (11), CONC(-Huang) (12) andCONC(-CW) (14) beat each individual embeddingset (6-10) in all tasks.
GloVe contributes most inSimLex-999, WS353, MC30 and MEN; word2veccontributes most in RW and word analogy tasks.SVD (32) reduces the dimensionality of CONCfrom 950 to 200, but still gains performance inSimLex-999 and MEN.
GloVe contributes most inSVD (larger losses on line 18 vs. lines 16-17, 19-20).
Other embeddings contribute inconsistently.1TON performs well only on word analogy, butit gains great improvement when discarding CWembeddings (24).
1TON+performs better than1TON: it has stronger results when considering allembedding sets, and can still outperform individ-ual embedding sets while discarding HLBL (26),Huang (27) or CW (29).These results demonstrate that ensemble meth-ods using multiple embedding sets producestronger embeddings.
However, it does not meanthe more embedding sets the better.
Whether anembedding set helps, depends on the complemen-tarity of the sets and on the task.CONC, the simplest ensemble, has robust per-formance.
However, size-950 embeddings as inputmeans a lot of parameters to tune for DNNs.
Theother three methods (SVD, 1TON, 1TON+) havethe advantage of smaller dimensionality.
SVD re-duces CONC?s dimensionality dramatically andstill is competitive, especially on word similar-ity.
1TON is competitive on analogy, but weakon word similarity.
1TON+performs consistentlystrongly on word similarity and analogy.Table 3 uses the metaembeddings of intersec-tion vocabulary, hence it shows directly the qual-ity enhancement by our ensemble approaches; thisenhancement is not due to bigger coverage.System comparison of learning OOV embed-dings.
In Table 4, we extend the vocabularies ofeach individual embedding set (?ind?
block) andour ensemble approaches (?ensemble?
block) tothe vocabulary union, reporting results on RW andanalogy ?
these tasks contain the most OOVs.
Asboth word2vec and GloVe have full coverage onanalogy, we do not rereport them in this table.
Thissubtask is specific to ?coverage?
property.
Appar-ently, our mutual learning and 1TON+can coverthe union vocabulary, which is bigger than each in-dividual embedding sets.
But the more importantissue is that we should keep or even improve theembedding quality, compared with their originalembeddings in certain component sets.For each embedding set, we can compute therepresentation of an OOV (i) as a randomly initial-ized vector (RND); (ii) as the average of embed-dings of all known words (AVG); (iii) by MUTU-ALLEARNING (ml) and (iv) by 1TON+.
1TON+learns OOV embeddings for individual embeddingsets and metaembeddings simultaneously, and it135750 100 150 200 250 300 350 400 450 5005560657075808590Dimension of svdPerformance (%)WC353MCRGSCWSRW(a) Performance vs. d of SVDDimension of O2M50 100 150 200 250 300 350 400 450 500Performance (%)5055606570758085WC353MCRGSCWSRW(b) Performance vs. d of 1TONDimension of O2M+50 100 150 200 250 300 350 400 450 500Performance (%)505560657075808590WC353MCRGSCWSRW(c) Performance vs. d of 1TON+Figure 4: Influence of dimensionalitynewsgroups reviews weblogs answers emails wsjALL OOV ALL OOV ALL OOV ALL OOV ALL OOV ALL OOVbaselinesTnT 88.66 54.73 90.40 56.75 93.33 74.17 88.55 48.32 88.14 58.09 95.76 88.30Stanford 89.11 56.02 91.43 58.66 94.15 77.13 88.92 49.30 88.68 58.42 96.83 90.25SVMTool 89.14 53.82 91.30 54.20 94.21 76.44 88.96 47.25 88.64 56.37 96.63 87.96C&P 89.51 57.23 91.58 59.67 94.41 78.46 89.08 48.46 88.74 58.62 96.78 88.65FLORS 90.86 66.42 92.95 75.29 94.71 83.64 90.30 62.15 89.44 62.61 96.59 90.37+indivFLORS+HLBL 90.01 62.64 92.54 74.19 94.19 79.55 90.25 62.06 89.33 62.32 96.53 91.03FLORS+Huang 90.68 68.53 92.86 77.88 94.71 84.66 90.62 65.04 89.62 64.46 96.65 91.69FLORS+GloVe 90.99 70.64 92.84 78.19 94.69 86.16 90.54 65.16 89.75 65.61 96.65 92.03FLORS+CW 90.37 69.31 92.56 77.65 94.62 84.82 90.23 64.97 89.32 65.75 96.58 91.36FLORS+W2V 90.72 72.74 92.50 77.65 94.75 86.69 90.26 64.91 89.19 63.75 96.40 91.03+metaFLORS+CONC 91.87 72.64 92.92 78.34 95.37 86.69 90.69 65.77 89.94 66.90 97.31 92.69FLORS+SVD 90.98 70.94 92.47 77.88 94.50 86.49 90.75 64.85 89.88 65.99 96.42 90.36FLORS+1TON 91.53 72.84 93.58 78.19 95.65 87.62 91.36 65.36 90.31 66.48 97.66 92.86FLORS+1TON+91.52 72.34 93.14 78.32 95.65 87.29 90.77 65.28 89.93 66.72 97.14 92.55Table 5: POS tagging results on six target domains.
?baselines?
lists representative systems for this task,including FLORS.
?+indiv / +meta?
: FLORS with individual embedding set / metaembeddings.
Boldmeans higher than ?baselines?
and ?+indiv?.would not make sense to replace these OOV em-beddings computed by 1TON+with embeddingscomputed by ?RND/AVG/ml?.
Hence, we do notreport ?RND/AVG/ml?
results for 1TON+.Table 4 shows four interesting aspects.
(i) MU-TUALLEARNING helps much if an embedding sethas lots of OOVs in certain task; e.g., MUTUAL-LEARNING is much better than AVG and RNDon RW, and outperforms RND considerably forCONC, SVD and 1TON on analogy.
However,it cannot make big difference for HLBL/CW onanalogy, probably because these two embeddingsets have much fewer OOVs, in which case AVGand RND work well enough.
(ii) AVG producesbad results for CONC, SVD and 1TON on anal-ogy, especially in the syntactic subtask.
We noticethat those systems have large numbers of OOVs inword analogy task.
If for analogy ?a is to b as c isto d?, all four of a, b, c, d are OOVs, then they arerepresented with the same average vector.
Hence,similarity between b?
a+ c and each OOV is 1.0.In this case, it is almost impossible to predict thecorrect answer d. Unfortunately, methods CONC,SVD and 1TON have many OOVs, resulting in thelow numbers in Table 4.
(iii) MUTUALLEARN-ING learns very effective embeddings for OOVs.CONC-ml, 1TON-ml and SVD-ml all get better re-sults than word2vec and GloVe on analogy (e.g.,for semantic analogy: 88.1, 87.3, 88.2 vs. 81.4for GloVe).
Considering further their bigger vo-cabulary, these ensemble methods are very strongrepresentation learning algorithms.
(iv) The per-formance of 1TON+for learning embeddings forOOVs is competitive with MUTUALLEARNING.For HLBL/Huang/CW, 1TON+performs slightlybetter than MUTUALLEARNING in all four met-1358rics.
Comparing 1TON-ml with 1TON+, 1TON+is better than ?ml?
on RW and semantic task, whileperforming worse on syntactic task.Figure 4 shows the influence of dimensionalityd for SVD, 1TON and 1TON+.
Peak performancefor different data sets and methods is reached ford ?
[100, 500].
There are no big differences inthe averages across data sets and methods for highenough d, roughly in the interval [150, 500].
Insummary, as long as d is chosen to be large enough(e.g., ?
150), performance is robust.6.2 Domain Adaptation for POS TaggingIn this section, we test the quality of those individ-ual embedding embedding sets and our metaem-beddings in a Part-of-Speech (POS) tagging task.For POS tagging, we add word embeddings intoFLORS7(Schnabel and Sch?utze, 2014) which isthe state-of-the-art POS tagger for unsuperviseddomain adaptation.FLORS tagger.
It treats POS tagging as awindow-based (as opposed to sequence classifica-tion), multilabel classification problem using LIB-LINEAR,8a linear SVM.
A word?s representationconsists of four feature vectors: one each for itssuffix, its shape and its left and right distributionalneighbors.
Suffix and shape features are standardfeatures used in the literature; our use of them inFLORS is exactly as described in (Schnabel andSch?utze, 2014).Let f(w) be the concatenation of the two distri-butional and suffix and shape vectors of word w.Then FLORS represents token vias follows:f(vi?2)?
f(vi?1)?
f(vi)?
f(vi+1)?
f(vi+2)where?
is vector concatenation.
Thus, token viistagged based on a 5-word window.FLORS is trained on sections 2-21 of WallStreet Journal (WSJ) and evaluate on the devel-opment sets of six different target domains: fiveSANCL (Petrov and McDonald, 2012) domains ?newsgroups, weblogs, reviews, answers, emails ?and sections 22-23 of WSJ for in-domain testing.Original FLORS mainly depends on distribu-tional features.
We insert word?s embedding asthe fifth feature vector.
All embedding sets (exceptfor 1TON+) are extended to the union vocabularyby MUTUALLEARNING.
We test if this additionalfeature can help this task.Table 5 gives results for some representa-7cistern.cis.lmu.de/flors (Yin et al, 2015)8liblinear.bwaldvogel.de (Fan et al, 2008)tive systems (?baselines?
), FLORS with individ-ual embedding sets (?+indiv?)
and FLORS withmetaembeddings (?+meta?).
Following conclu-sions can be drawn.
(i) Not all individual embed-ding sets are beneficial in this task; e.g., HLBLembeddings make FLORS perform worse in 11out of 12 cases.
(ii) However, in most cases,embeddings improve system performance, whichis consistent with prior work on using embed-dings for this type of task (Xiao and Guo, 2013;Yang and Eisenstein, 2014; Tsuboi, 2014).
(iii)Metaembeddings generally help more than the in-dividual embedding sets, except for SVD (whichonly performs better in 3 out of 12 cases).7 ConclusionThis work presented four ensemble methods forlearning metaembeddings from multiple embed-ding sets: CONC, SVD, 1TON and 1TON+.Experiments on word similarity and analogyand POS tagging show the high quality of themetaembeddings; e.g., they outperform GloVeand word2vec on analogy.
The ensemble meth-ods have the added advantage of increasing vo-cabulary coverage.
We make our metaem-beddings available at http://cistern.cis.lmu.de/meta-emb.AcknowledgmentsWe gratefully acknowledge the support ofDeutsche Forschungsgemeinschaft (DFG): grantSCHU 2246/8-2.ReferencesMohit Bansal, Kevin Gimpel, and Karen Livescu.2014.
Tailoring continuous word representations fordependency parsing.
In Proceedings of ACL, pages809?815.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
JMLR, 3:1137?1155.Elia Bruni, Nam-Khanh Tran, and Marco Baroni.2014.
Multimodal distributional semantics.
JAIR,49(1-47).Yanqing Chen, Bryan Perozzi, Rami Al-Rfou, andSteven Skiena.
2013.
The expressive power of wordembeddings.
In ICML Workshop on Deep Learningfor Audio, Speech, and Language Processing.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of ICML, pages 160?167.1359John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
JMLR, 12:2121?2159.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLIN-EAR: A library for large linear classification.
JMLR,9:1871?1874.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2001.
Placing search in context: Theconcept revisited.
In Proceedings of WWW, pages406?414.Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, andYehuda Koren.
2012.
Large-scale learning ofword relatedness with constraints.
In Proceedingsof KDD, pages 1406?1414.Felix Hill, KyungHyun Cho, Sebastien Jean, ColineDevin, and Yoshua Bengio.
2014.
Not all neuralembeddings are born equal.
In NIPS Workshop onLearning Semantics.Felix Hill, Kyunghyun Cho, Sebastien Jean, ColineDevin, and Yoshua Bengio.
2015a.
Embeddingword similarity with neural machine translation.
InProceedings of ICLR Workshop.Felix Hill, Roi Reichart, and Anna Korhonen.
2015b.Simlex-999: Evaluating semantic models with (gen-uine) similarity estimation.
Computational Linguis-tics, pages 665?695.Eric H Huang, Richard Socher, Christopher D Man-ning, and Andrew Y Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proceedings of ACL, pages 873?882.Quoc V Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
In Pro-ceedings of ICML, pages 1188?1196.Yong Luo, Jian Tang, Jun Yan, Chao Xu, and ZhengChen.
2014.
Pre-trained multi-view word embed-ding using two-side neural network.
In Proceedingsof AAAI, pages 1982?1988.Minh-Thang Luong, Richard Socher, and Christo-pher D Manning.
2013.
Better word representa-tions with recursive neural networks for morphol-ogy.
In Proceedings of CoNLL, volume 104, pages104?113.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word repre-sentations in vector space.
In Proceedings of ICLRWorkshop.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In Proceedings of NIPS, pages 3111?3119.George A Miller and Walter G Charles.
1991.
Contex-tual correlates of semantic similarity.
Language andcognitive processes, 6(1):1?28.Andriy Mnih and Geoffrey E Hinton.
2009.
A scalablehierarchical distributed language model.
In Pro-ceedings of NIPS, pages 1081?1088.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors for wordrepresentation.
Proceedings of EMNLP, 12:1532?1543.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 shared task on parsing the web.
In Pro-ceedings of SANCL, volume 59.Pushpendre Rastogi, Benjamin Van Durme, and RamanArora.
2015.
Multiview LSA: Representation learn-ing via generalized CCA.
In Proceedings of NAACL,pages 556?566.Tobias Schnabel and Hinrich Sch?utze.
2014.
FLORS:Fast and simple domain adaptation for part-of-speech tagging.
TACL, 2:15?26.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural net-works.
In Proceedings of NIPS, pages 3104?3112.Yuta Tsuboi.
2014.
Neural networks leverage corpus-wide information for part-of-speech tagging.
In Pro-ceedings of EMNLP, pages 938?950.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings ofACL, pages 384?394.John Wieting, Mohit Bansal, Kevin Gimpel, and KarenLivescu.
2015.
From paraphrase database tocompositional paraphrase model and back.
TACL,3:345?358.Min Xiao and Yuhong Guo.
2013.
Domain adaptationfor sequence labeling tasks with a probabilistic lan-guage adaptation model.
In Proceedings of ICML,pages 293?301.Yi Yang and Jacob Eisenstein.
2014.
Unsuperviseddomain adaptation with feature embeddings.
In Pro-ceedings of ICLR Workshop.Wenpeng Yin and Hinrich Sch?utze.
2015.
Multichan-nel variable-size convolution for sentence classifica-tion.
In Proceedings of CoNLL, pages 204?214.Wenpeng Yin, Tobias Schnabel, and Hinrich Sch?utze.2015.
Online updating of word representations forpart-of-speech taggging.
In Proceedings of EMNLP,pages 1329?1334.Ye Zhang, Stephen Roller, and Byron Wallace.
2016.MGNC-CNN: A simple approach to exploiting mul-tiple word embeddings for sentence classification.In Proceedings of NAACL-HLT.1360
