Proceedings of the EACL 2012 Student Research Workshop, pages 74?80,Avignon, France, 26 April 2012. c?2012 Association for Computational LinguisticsMining Co-Occurrence Matrices for SO-PMI Paradigm WordCandidatesAleksander WawerInstitute of Computer Science, Polish Academy of Scienceul.
Jana Kazimierza 501-248 Warszawa, Polandaxw@ipipan.waw.plAbstractThis paper is focused on one aspect of SO-PMI, an unsupervised approach to senti-ment vocabulary acquisition proposed byTurney (Turney and Littman, 2003).
Themethod, originally applied and evaluatedfor English, is often used in bootstrap-ping sentiment lexicons for European lan-guages where no such resources typicallyexist.
In general, SO-PMI values are com-puted from word co-occurrence frequenciesin the neighbourhoods of two small sets ofparadigm words.
The goal of this work isto investigate how lexeme selection affectsthe quality of obtained sentiment estima-tions.
This has been achieved by compar-ing ad hoc random lexeme selection withtwo alternative heuristics, based on clus-tering and SVD decomposition of a wordco-occurrence matrix, demonstrating supe-riority of the latter methods.
The work canbe also interpreted as sensitivity analysis onSO-PMI with regard to paradigm word se-lection.
The experiments were carried outfor Polish.1 IntroductionThis paper seeks to improve one of the main meth-ods of unsupervised lexeme sentiment polarity as-signment.
The method, introduced by (Turneyand Littman, 2003), is described in more detail inSection 2.
It relies on two sets of paradigm words,positive and negative, which determine the polar-ity of unseen words.The method is resource lean and therefore oftenused in languages other than English.
Recent ex-amples include Japanese (Wang and Araki, 2007)and German (Remus et al, 2006).Unfortunately, the selection of paradigm wordsrarely receives sufficient attention and is typicallydone in an ad hoc manner.
One notable exampleof manual paradigm word selection method waspresented in (Read and Carroll, 2009).In this context, an interesting variation of thesemantic orientation?pointwise mutual informa-tion (SO-PMI) algorithm for Japanese was sug-gested by (Wang and Araki, 2007).
Authors, mo-tivated by excessive leaning toward positive opin-ions, proposed to modify the algorithm by intro-ducing balancing factor and detecting neutral ex-pressions.
As will be demonstrated, this problemcan be addressed by proper selection of paradigmpairs.One not entirely realistic, but nevertheless in-teresting theoretical possibility is to pick pairsof opposing adjectives with the highest loadingsidentified in Osgood?s experiments on semanticdifferential (Osgood et al, 1967).
In the exper-iments, respondents were presented with a nounand asked to choose its appropriate position ona scale between two bipolar adjectives (for ex-ample: adequate-inadequate, valuable-worthless,hot-cold).
Factor analysis of the results revealedthree distinctive factors, called Osgood dimen-sions.
The first of the dimensions, often consid-ered synonymous with the notion of sentiment,was called Evaluative because its foundational ad-jective pair (one with the highest loading) is good-bad.The first problem with using adjective pairs asexemplary for word co-occurrence distributionson the basis of their loadings, is the fact that fac-tor loadings as measured by Osgood et al are notnecessarily reflected in word frequency phenom-ena.74The second problem is translation: an adjectivepair, central in English, may not be as stronglyassociated with a dimension (here: Evaluative) inother languages and cultures.The approach we suggest in this paper assumesa latent structure behind word co-occurrence fre-quencies.
The structure may be seen as a mix-ture of latent variables of unknown distributionsthat drives word selection.
Some of the vari-ables are more likely to produce certain types ofhighly evaluative words (words with high senti-ment scores).
We do not attempt to model thestructure in a generative way as in for exam-ple probabilistic latent semantic analysis (PLSA)or latent Dirichlet alocation (LDA).
A gener-ative approximation is not feasible when usingcorpora such as the balanced, 300-million ver-sion of the National Corpus of Polish (NKJP)(Przepi?rkowski et al, 2008; Przepi?rkowski etal., 2012) 1 applied in the experiments describedin the next sections, which does not enable creat-ing a word-document matrix and organizing wordoccurrences by documents or narrowly specifiedtopics.Therefore, we propose different techniques.We begin with a symmetric matrix of word co-occurences and attempt to discover as much ofits structure as possible using two well estab-lished techniques: Singular Value Decomposi-tion and clustering.
The discovered structures arethen used to optimize the selection of words forparadigm sets used in SO-PMI.The paper is organized as follows.
In Section2 we define the SO-PMI measure and briefly for-mulate the problem.
Section 3 describes obtainingthe set of sentiment word candidates, which arethen used to generate a symmetric co-occurencematrix as outlined in Section 4.
Section 5 delin-eates the details of human word scoring, whichserves as a basis for evaluations in 9.
Sections6, 7 and 8 describe three distinct approaches toparadigm sets generation.2 Problem Statement.
SO-PMIWhen creating a sentiment lexicon, the strengthof association between candidate words and eachof the two polar classes (positive and negative,for instance) can be calculated using several mea-1http://www.nkjp.pl/index.php?page=0&lang=1sures.
Perhaps most popular of them, employed inthis experiment after (Turney and Littman, 2003)and (Grefenstette et al, 2006) is Pointwise MutualInformation (PMI).
The Pointwise Mutual Infor-mation (PMI) between two words, w1 and w2, isdefined as:PMI(w1, w2) = log2(p(w1&w2)p(w1)p(w2))where p(w1 & w2) is the probability of co-occurrence of (w1) and (w2).
For the task of as-signing evaluative polarity, it is computed as num-ber of co-occurrences of candidate words witheach of the paradigm positive and negative words,denoted as pw and nw.
Optimal selection of thesetwo sets of words is the subject of this paper.Once the words are known, the semantic ori-entation PMI (SO-PMI) of each candidate word ccan be computed as:SO-PMI(c) ==?pw?PWPMI(c, pw)?
?nw?NWPMI(c, nw)The equation above demonstrates that opti-mization of both word lists, pw and nw, is of cru-cial importance for the performance of SO-PMI.3 Generating Sentiment WordCandidatesThis section describes the acquisition of senti-ment word candidates.
The method we followedcould be substituted by any other technique whichresults in a set of highly sentimental lexemes, pos-sibly of varying unknown polarity and strength.
Asimilar experiment for English has been describedby (Grefenstette et al, 2006).The procedure can be described as follows.
Inthe first step, a set of semi-manually defined lexi-cal patterns is submitted to a search engine to findcandidates for evaluatively charged terms.
Then,the downloaded corpus is analyzed for patterncontinuations ?
lexemes immediately followingpattern matches, which are likely to be candidatesfor sentiment words.
In the last step, candidateterms selected this way are tested for their senti-ment strength and polarity (in other words, howpositive or negative are the conotations).
In origi-nal experiment described in the cited paper, wordswere evaluated using the SO-PMI technique.75The purpose of using extraction patterns is toselect candidates for evaluative words.
In thisexperiment, 112 patterns have been created bygenerating all combinations of elements from twomanually prepared sets2, A and B:?
A: [0] wydawac?
sie?, [1] wydawa?
sie?, [2]wydawa?a sie?, [3] czuc?
sie?, [4] czu?em sie?,[5] czu?am sie?, [6] czu?em, [7] byc?
3?
B: [0] nie dos?c?, [1] niewystarczaja?co, [2]niedostatecznie, [3] za ma?o, [4] prawie, [5]niemal, [6] tak, [7] taki, [8] zbyt, [9] zbyt-nio, [10] za bardzo, [11] przesadnie, [12]nadmiernie, [13] szczeg?lnie 4Each pattern (a combination of A and B) hasbeen wrapped with double quotes (?A B?)
andsubmitted to Google to narrow the results to textswith exact phrases.
The Web crawl yielded 17657web pages, stripped from HTML and other webtags to filter out non-textual content.
Two patternsare grammatically incorrect due to gender dis-agreement, namely wydawa?a sie?
taki and czu?amsie?
taki 5, thus did not generate any results.The corpus of 17657 web pages has been an-alyzed using Spejd6, originally a tool for par-tial parsing and rule-based morphosyntactic dis-ambiguation, adapted in the context of this workfor the purpose of finding pattern continuations.Again, 112 patterns were constructed by gener-ating all combinations of elements from the twosets, A and B above.
Spejd rules were written as?A B *?
where the wildcard can be either an ad-jective or an adverb.Parsing the web pages using the 112 patternsresulted in acquiring 1325 distinct base wordforms (lexemes) recognized by the morphologicanalyser and related dictionaries.
This list is sub-sequently used for generating the co-occurrence2Terms are translations of words listed in (Grefenstette etal., 2006).
Many of the expressions denote either excess ordeficiency, as for example not enough or too much.3English translations (morphosyntactic tags in parenthe-ses): [0] seem to (inf), [1] seemed to (sg,pri,perf,m), [2]seemed to (sg,pri,perf,f), [3] feel (inf), [4] felt (sg,pri,perf,m),[5] felt (sg,pri,perf,f), [7] to be (inf)4items [0-3] are various ways of expressing not enough,items [4-5] almost, items [6-7] such, items [8-12] too much,item [13] especially5seemed(f) so(m) and felt(f) so(m)6http://nlp.ipipan.waw.pl/Spejd/(Przepi?rkowski and Buczyn?ski, 2007)matrix as delineated in the next Section and forselecting paradigm words.4 Word Co-Occurrence MatrixEach word (base form) from the list was soughtin the balanced, 300 million segments7 version ofthe National Corpus of Polish (NKJP).
For eachrow i and column j of the co-occurrence matrixm, its value was computed as follows:mij =fijfifjwhere fij denotes the number of co-occurencesof word i within the window of 20 segments leftand right with word j, fi and fj denote the totalnumbers of occurrences of each word.
The se-lection of a window of 20 follows the choice in(Turney and Littman, 2003).This design has been found optimal after anumber of experiments with the singular value de-composition (SVD) technique described further.Without the denominator part, decompositions areheavily biased by word frequency.
In this defni-tion, the matrix resembles the PMI form in (Tur-ney and Pantel, 2010), however we found that thelogarithm transformation flattens the eigenvaluedistribution and is not really necessary.If the distributions of words i and j are statis-tically independent, then by the defnition of inde-pendence fifj = fij .
The product fifj is what wewould expect for fij , if i occurs in the contexts ofj by the matter of a random chance.
The opposingsituation happens when there exists a relationshipbetween i and j, for instance when both wordsare generated by the same latent topic variable,and we expect fij to be larger than in the case ofindependency.5 Evaluating Word CandidatesIn order to evaluate combinations of paradigmwords, one needs to compare the computed SO-PMI scores against a human made scoring.
Ide-ally, such a scoring should not only inform aboutpolarity (indication whether a word is positive ornegative), but also about association strength (thedegree of positivity or negativity).
Reliable and7A segment usually corresponds to a word.
Segmentsare not longer than orthographic words, but sometimesshorter.
See http://nkjp.pl/poliqarp/help/ense1.html#x2-10001 for a detailed discussion76valid measurement of word associations on a mul-tipoint scale is not easy: the inter rater agreementis likely to decrease with the growing complexityof the scale.Therefore, we decided that each lexeme was in-dependently scored by two humans using a fivepoint scale.
Extreme values denoted very nega-tive or positive words, the central value denotedneutral words and remaining intermediate valueswere interpreted as somehow positive or nega-tive.
Discrepancies between raters were solvedby arithmetic means of conflicting scores ratherthan introducing the third human (often called theGolden Annotator) to select one value of the two.Consequently, the 5-point scale extended to 10points.Human word scores were used in evaluations ofmethods described in forthcoming sections.6 Random SelectionThe baseline method to compare against is to se-lect lexemes in a random fashion.
In order to en-sure highest possible performance of the method,lexemes were selected only from those with atleast one extreme human score (very positive orvery negative) and at least 500 occurrences in thecorpus.
The last condition renders this methodslightly favourable because in the case of SVD, inmany eigenvectors the highly loaded terms werenot as frequent and had to be selected despite rel-ative rarity.7 SVDThe word co-occurrence matrix m (1325x1325)was the subject of singular value decomposition(SVD), a well-known matrix factorization tech-nique which decomposes a matrix A into threematrices:A = U?V Twhere ?
is a matrix whose diagonals are thesingular values of A, U and V are left and righteigenvectors matrices.The usage of SVD decompositions has a longand successful history of applications in extract-ing meaning from word frequencies in word-document matrices, as for example the well es-tablished algorithm of latent semantic indexing(LSI).
More recently, the usability of analyzingthe structure of language via spectral analysisof co-occurrence matrices was demonstrated bystudies such as (Mukherjee et al, 2009).
The fo-cus was on phonology with the intention to dis-cover principles governing consonant inventoriesand quantify their importance.
Our work, as webelieve, is the first to apply SVD in the context ofco-occurrence matrices and SO-PMI.We suspect that the SVD technique can be help-ful by selecting lexemes that represent certainamounts of latent co-occurrence structure.
Fur-thermore, the fact that 20 eigenvalues constitutesapproximately half of the norm of the spectrum(Horn and Johnson, 1990), as on Table 1, suggeststhat there may exist a small number of organiz-ing principles which could be potentially helpfulto improve the selection of lexemes into paradigmsets.c m10 0.728 0.41020 0.797 0.498100 0.924 0.720Table 1: Frobenius norm of the spectrum for 10, 20and 100 first eigenvalues.Table 1 depicts also the problem of frequencybias, stronger in case of 10 and 20 eigenvaluesthan for 100.
The values were computed for twomatrices: c contains only co-occurrence frequen-cies and m is the matrix described in section 4.Figure 1 plots the eigenvalue spectrum restrictedto the first 100 values.
""0 20 40 60 80 100Eigenvalues0.00000.00050.00100.00150.00200.00250.00300.00350.0040NormofthespectrumFigure 1: Eigenvalue distribution (limited to the first100).In order to ?discover?
the principles behind theco-occurrences, we examine eigenvectors associ-77ated with the largest eigenvalues.
Some of thevectors indeed appear to have their interpretationsor at least one could name common properties ofinvolved words.
The meaning of vectors becomesusually apparent after examination of the first fewtop component weights.The list below consits of four eigenvectors, topthree and the eighth one (as ordered accordingto their eigenvalues), along with five terms withhighest absolute weights and interpretations ofeach vector.1 sztuczny (artificial), liryczny (lyrical), upi-orny (ghastly), zrze?dliwy (grouchy), prze-jrzysty (lucid).?
abstract properties one could attribute toan actor or a play.2 instynktowny (instinctive), odlotowo (su-per/cool), ostroz?ny (careful), bolesny(painful), przesadnie (excessively)?
physical and sensual experiences3 wyemancypowac?
(emancipate), opuszczony(abandoned), przeszywac?
(pierce), ws?cibski(inquisitive), jednakowo (alike)?
unpleasant states and behaviours8 g?adki (smooth), kochany (beloved), starac?sie?
(make efforts), niedo?e?z?ny (infirm), in-tymnie (intimately)?
intimacy, caring, emotionsAs it has been noted before, the eigenvectorsof pure co-occurrence matrix c did not deliveranything close in terms of conceivable interpreta-tions.
It is also fairly clear that some of the eigen-vectors, as for example the third one, are more re-lated to sentiment than the others.
This is also evi-dent by examination of average lexeme sentimentof top loaded terms of each vector, not disclosedin the paper.The heuristic of SVD backed selection ofparadigm words maximizes three factors:?
corpus frequency: avoid rare words wherepossible;?
eigenvector component weights: selectwords that contribute the most to a giveneigenvector;?
sentiment polarity: select words with thehighest absolute human scores.8 Affinity PropagationAffinity Propagation (Frey and Dueck, 2007)method was selected because of two distinct ad-vantages for our task.
First is the fact that itclusters data by diffusion in the similarity matrix,therefore does not require finding representationsin Euclidean space.
Second advantage, especiallyover cluster analysis algorithms such as k-means,is that the algorithm automatically sets its numberof clusters and does not depend on initialization.Affinity Propagation clusters data by exchang-ing real-valued messages between data points un-til a high-quality set of exemplars (representativeexamples, lexemes in our case) and correspondingclusters gradually emerges.Interestingly, in each parameter setting the al-gorithm found exactly 156 clusters.
It hints atthe fact that the number of ?latent?
variables be-hind the co-occurrences could indeed be over 100.This is further confirmed by the percentage ofnorm of the spectrum covered by top 100 eigen-values.
""0 20 40 60 80 100 120 140Clusters051015202530FrequencyFigure 2: Histogram of cluster counts.The five most frequent clusters cover only 116words.
We restrict the selection of paradigmwords to the same frequency and polarity condi-tions as in the case of random method.
We pickone paradigm word from each most frequent clus-ter because we assume that it is sufficient to ap-proximate the principle which organizes that clus-ter.
The heuristic is very similar to the one usedin case of SVD.789 EvaluationUsing continous SO-PMI and multi point scalesfor human scoring facilitates formulating theproblem as a regression one, where goodness offit of the estimations can be computed using dif-ferent measures than in the case of classification.This, however, demands a mapping such thatranges of the continuous SO-PMI scale corre-spond to discrete human scores.
We propose tobase such a mapping on dividing the SO-PMIrange into 10 segments {s0, ..., s10} of variouslength, each of which corresponds to one discretehuman value.The choice of values (locations) of specificpoints is a subject of minimization where the errorfunction E over a set of words W is as follows:E =?w?Wdist(sc, se)For each word w, the distance function dist re-turns the number of segments between the correctsegment sc and the estimated segment se usingthe SO-PMI.
We minimize E and find optimumlocations for points separating each segment us-ing Powell?s conjugate direction method, deter-mined the most effective for this task.
Powell?salgorithm is a non-gradient numerical optimiza-tion technique, applicable to a real valued func-tion which does not need not be differentiable(Powell, 1964).10 ResultsTable 2 presents E errors and extreme (min andmax) SO-PMI values computed over two indepen-dent samples of 500 lexemes.
Error columns indi-cated as E denote errors computed either on non-optimized default (def ) or optimized segments(min).
Each combination of paradigm words andeach sample required re-computing optimum val-ues of points dividing the SO-PMI scale into seg-ments.Generally, the randomized selection methodperforms surprisingly well ?
most likely due tothe fact that the frequency and polarity conditionsare the key factors.
In either case, the best re-sult was obtained using the selection of paradigmwords using the heuristic based on svd, closelyfollowed by aff .
In one case, random selectionperformed better than the aff .SO-PMI Esample min max def minS1 r1 -14 29 1226 908r2 -15 23 1131 765r3 -18 8.6 844 710aff -9 25 1057 716svd -13 26 1002 701S2 r1 -18 19 983 812r2 -17 15 910 756r3 -11 20 1016 789aff -13 28 1033 732svd -13 35 1028 724Table 2: SO-PMI ranges and error (E) values on twoindependent random samples of N=500.
3 randomizedselections (r1 ?
r3), Affinity Propagation (aff ) andSVD (svd).The small margin of a victory could be ex-plained by the fact that the size of each set ofparadigm SO-PMI words is limited to five lex-emes.
Consequently, it is very difficult to repre-sent a space of over one hundred latent variables?
because such appears to be the number indicatedby the distribution of eigenvalues in SVD and thenumber of clusters.The ranges of SO-PMI values (in the columnsmin and max) were often non symmetric andleaned towards positive.
This shift did not nec-essarily translate to higher error rates, especiallyafter optimizations.11 Discussion and Future WorkThe methods presented in this article, based on theassumption of latent word co-occurrence struc-tures, performed moderately better than the base-line of random selections.
The result is ambigu-ous because it still requires a more in-depth un-derstanding of underlying mechanims.The work will be continued in several aspects.One is to pre-determine lexeme type before it isactually evaluated against particular members ofparadigm word sets.
This could be acheved us-ing a two-step model consisting of lexeme typeclassification (with regard to over one hundredlatent variables) followed by SO-PMI computa-tion, where the selection of paradigm words is notfixed, as in this paper, but dependens on previ-ously selected latent variables.
Another promis-ing direction is to focus on explanations andword features: how adding or removing particu-79lar words change the SO-PMI, and more impor-tantly, why (in terms of features involved)?
Whatare the features that change SO-PMI in specificdirections?
How to extract them?AcknowledgmentThis research is supported by the POIG.01.01.02-14-013/09 project which is co-financed by the Eu-ropean Union under the European Regional De-velopment FundReferencesBrendan J. Frey and Delbert Dueck.
2007.
Clusteringby passing messages between data points.
Science,315:972?976.Gregory Grefenstette, Yan Qu, David A. Evans, andJames G. Shanahan, 2006.
Validating the Cover-age of Lexical Resources for Affect Analysis and Au-tomatically Classifying New Words along SemanticAxes.
Springer.
Netherlands.Roger A. Horn and Charles R. Johnson.
1990.
MatrixAnalysis.
Cambridge University Press.Animesh Mukherjee, Monojit Choudhury, and RaviKannan.
2009.
Discovering global patterns in lin-guistic networks through spectral analysis: a casestudy of the consonant inventories.
In Proceedingsof the 12th Conference of the European Chapterof the Association for Computational Linguistics,EACL ?09, pages 585?593, Stroudsburg, PA, USA.Association for Computational Linguistics.Charles E. Osgood, George J. Suci, and Percy H. Tan-nenbaum.
1967.
The Measurement of Meaning.University of Illinois Press.M.
J. D. Powell.
1964.
An efficient method for findingthe minimum of a function of several variables with-out calculating derivatives.
The Computer Journal,7(2):155?162, January.Adam Przepi?rkowski and Aleksander Buczyn?ski.2007.spade: Shallow parsing and disambiguation engine.In Proceedings of the 3rd Language & TechnologyConference, Poznan?.Adam Przepi?rkowski, Rafa?
L. G?rski, BarbaraLewandowska-Tomaszczyk, and Marek ?azin?ski.2008.
Towards the national corpus of polish.
InThe proceedings of the 6th Language Resources andEvaluation Conference (LREC 2008), Marrakesh,Morocco.Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa?
L.G?rski, and Barbara Lewandowska-Tomaszczyk,editors.
2012.
Narodowy Korpus Je?zyka Polskiego.Wydawnictwo Naukowe PWN, Warsaw.
Forthcom-ing.J.
Read and J. Carroll.
2009.
Weakly supervisedtechniques for domain-independent sentiment clas-sification.
In Proceedings of the 1st internationalCIKM workshop on Topic-sentiment analysis formass opinion, pages 45?52.
ACM.Robert Remus, Uwe Quasthoff, and Gerhard Heyer.2006.
Sentiws: a publicly available german-language resource for sentiment analysis.
In Pro-ceedings of LREC.Peter Turney and Michael Littman.
2003.
Measuringpraise and criticism: Inference of semantic orienta-tion from association.
ACM Transactions on Infor-mation Systems, 21:315?346.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: vector space models of seman-tics.
J. Artif.
Int.
Res., 37:141?188, January.Guangwei Wang and Kenji Araki.
2007.
Modifyingso-pmi for japanese weblog opinion mining by us-ing a balancing factor and detecting neutral expres-sions.
In Human Language Technologies 2007: TheConference of the North American Chapter of theAssociation for Computational Linguistics; Com-panion Volume, Short Papers, NAACL-Short ?07,pages 189?192, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.80
