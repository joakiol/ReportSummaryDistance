Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 467?474, Vancouver, October 2005. c?2005 Association for Computational LinguisticsBidirectional Inference with the Easiest-First Strategyfor Tagging Sequence DataYoshimasa Tsuruoka12 and Jun?ichi Tsujii2311 CREST, JST (Japan Science and Technology Corporation)Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 Japan2 Department of Computer Science, University of TokyoHongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan3 School of Informatics, University of ManchesterPOBox 88, Sackville St, MANCHESTER M60 1QD, UK{tsuruoka,tsujii}@is.s.u-tokyo.ac.jpAbstractThis paper presents a bidirectional in-ference algorithm for sequence label-ing problems such as part-of-speech tag-ging, named entity recognition and textchunking.
The algorithm can enumerateall possible decomposition structures andfind the highest probability sequence to-gether with the corresponding decomposi-tion structure in polynomial time.
We alsopresent an efficient decoding algorithmbased on the easiest-first strategy, whichgives comparably good performance tofull bidirectional inference with signifi-cantly lower computational cost.
Exper-imental results of part-of-speech taggingand text chunking show that the proposedbidirectional inference methods consis-tently outperform unidirectional inferencemethods and bidirectional MEMMs givecomparable performance to that achievedby state-of-the-art learning algorithms in-cluding kernel support vector machines.1 IntroductionThe task of labeling sequence data such as part-of-speech (POS) tagging, chunking (shallow parsing)and named entity recognition is one of the most im-portant tasks in natural language processing.Conditional random fields (CRFs) (Lafferty et al,2001) have recently attracted much attention be-cause they are free from so-called label bias prob-lems which reportedly degrade the performance ofsequential classification approaches like maximumentropy markov models (MEMMs).Although sequential classification approachescould suffer from label bias problems, they have sev-eral advantages over CRFs.
One is the efficiencyof training.
CRFs need to perform dynamic pro-gramming over the whole sentence in order to com-pute feature expectations in each iteration of numer-ical optimization.
Training, for instance, second-order CRFs using a rich set of features can requireprohibitive computational resources.
Max-marginmethods for structured data share problems of com-putational cost (Altun et al, 2003).Another advantage is that one can employ a vari-ety of machine learning algorithms as the local clas-sifier.
There is huge amount of work about devel-oping classification algorithms that have high gener-alization performance in the machine learning com-munity.
Being able to incorporate such state-of-the-art machine learning algorithms is important.
In-deed, sequential classification approaches with ker-nel support vector machines offer competitive per-formance in POS tagging and chunking (Gimenezand Marquez, 2003; Kudo and Matsumoto, 2001).One obvious way to improve the performance ofsequential classification approaches is to enrich theinformation that the local classifiers can use.
In stan-dard decomposition techniques, the local classifierscannot use the information about future tags (e.g.the right-side tags in left-to-right decoding), whichwould be helpful in predicting the tag of the targetword.
To make use of the information about fu-ture tags, Toutanova et al proposed a tagging algo-rithm based on bidirectional dependency networks467(Toutanova et al, 2003) and achieved the best ac-curacy on POS tagging on the Wall Street Journalcorpus.
As they pointed out in their paper, however,their method potentially suffers from ?collusion?
ef-fects which make the model lock onto conditionallyconsistent but jointly unlikely sequences.
In theirmodeling, the local classifiers can always use the in-formation about future tags, but that could cause adouble-counting effect of tag information.In this paper we propose an alternative way ofmaking use of future tags.
Our inference methodconsiders all possible ways of decomposition andchooses the ?best?
decomposition, so the informa-tion about future tags is used only in appropriatesituations.
We also present a deterministic versionof the inference method and show their effective-ness with experiments of English POS tagging andchunking, using standard evaluation sets.2 Bidirectional InferenceThe task of labeling sequence data is to find the se-quence of tags t1...tn that maximizes the followingprobability given the observation o = o1...onP (t1...tn|o).
(1)Observations are typically words and their lexicalfeatures in the task of POS tagging.
Sequential clas-sification approaches decompose the probability asfollows,P (t1...tn|o) =n?i=1p(ti|t1...ti?1o).
(2)This is the left-to-right decomposition.
If wemake a first-order markov assumption, the equationbecomesP (t1...tn|o) =n?i=1p(ti|ti?1o).
(3)Then we can employ a probabilistic classifiertrained with the preceding tag and observations inorder to obtain p(ti|ti?1o) for local classification.
Acommon choice for the local probabilistic classifieris maximum entropy classifiers (Berger et al, 1996).The best tag sequence can be efficiently computedby using a Viterbi decoding algorithm in polynomialtime.t1(a)t2 t3ot1(b)t2 t3t1(c)t2 t3 t1(d)t2 t3oo oFigure 1: Different structures for decomposition.The right-to-left decomposition isP (t1...tn|o) =n?i=1p(ti|ti+1o).
(4)These two ways of decomposition are widely usedin various tagging problems in natural language pro-cessing.
The issue with such decompositions is thatyou have only the information about the preceding(or following) tags when performing local classifi-cation.From the viewpoint of local classification, wewant to give the classifier as much information aspossible because the information about neighboringtags is useful in general.As an example, consider the situation where weare going to annotate a three-word sentence withpart-of-speech tags.
Figure 1 shows the four possi-ble ways of decomposition.
They correspond to thefollowing equations:(a) P (t1...t3|o) = P (t1|o)P (t2|t1o)P (t3|t2o) (5)(b) P (t1...t3|o) = P (t3|o)P (t2|t3o)P (t1|t2o) (6)(c) P (t1...t3|o) = P (t1|o)P (t3|o)P (t2|t3t1o) (7)(d) P (t1...t3|o) = P (t2|o)P (t1|t2o)P (t3|t2o) (8)(a) and (b) are the standard left-to-right and right-to-left decompositions.
Notice that in decomposi-tion (c), the local classifier can use the informationabout the tags on both sides when deciding t2.
If,for example, the second word is difficult to tag (e.g.an unknown word), we might as well take the de-composition structure (c) because the local classifier468can use rich information when deciding the tag ofthe most difficult word.
In general if we have ann-word sentence and adopt a first-order markov as-sumption, we have 2n?1 possible ways of decompo-sition because each of the n ?
1 edges in the cor-responding graph has two directions (left-to-right orright-to-left).Our bidirectional inference method is to considerall possible decomposition structures and choose the?best?
structure and tag sequence.
We will show inthe next section that this is actually possible in poly-nomial time by dynamic programming.As for the training, let us look at the equa-tions of four different decompositions above.
Youcan notice that there are only four types of localconditional probabilities: P (ti|ti?1o), P (ti|ti+1o),P (ti|ti?1ti+1o), and P (ti|o).This means that if we have these four types of lo-cal classifiers, we can consider any decompositionstructures in the decoding stage.
These local classi-fiers can be obtained by training with correspondingneighboring tag information.
Training the first twotypes of classifiers is exactly the same as the train-ing of popular left-to-right and right-to-left sequen-tial classification models respectively.If we take a second-order markov assumption, weneed to train 16 types of local classifiers becauseeach of the four neighboring tags of a classificationtarget has two possibilities of availability.
In gen-eral, if we take a k-th order markov assumption, weneed to train 22k types of local classifies.2.1 Polynomial Time InferenceThis section describes an algorithm to find the de-composition structure and tag sequence that give thehighest probability.
The algorithm for the first-ordercase is an adaptation of the algorithm for decodingthe best sequence on a bidirectional dependency net-work introduced by (Toutanova et al, 2003), whichoriginates from the Viterbi decoding algorithm forsecond-order markov models.Figure 2 shows a polynomial time decoding al-gorithm for our bidirectional inference.
It enumer-ates all possible decomposition structures and tagsequences by recursive function calls, and finds thehighest probability sequence.
Polynomial time isachieved by caching.
Note that for each local clas-sification, the function chooses the appropriate localfunction bestScore(){return bestScoreSub(n+2, ?end, end, end?, ?L,L?
);}function bestScoreSub(i+1, ?ti?1, ti, ti+1?, ?di?1, di?
){// memorizationif (cached(i+1, ?ti?1, ti, ti+1?, ?di?1, di?
))return cache(i+1, ?ti?1, ti, ti+1?, ?di?1, di?
);// left boundary caseif (i = -1)if (?ti?1, ti, ti+1?
= ?start, start, start?)
return 1;else return 0;// recursive caseP = localClassification(i, ?ti?1, ti, ti+1?, ?di?1, di?
);return maxdi?2 maxti?2 P?bestScoreSub(i, ?ti?2, ti?1, ti?, ?di?2, di?1?
);}function localClassification(i, ?ti?1, ti, ti+1?, ?di?1, di?
){if (di?1 = L & di = L) return P (ti|ti+1, o);if (di?1 = L & di = R) return P (ti|o);if (di?1 = R & di = L) return P (ti|ti?1ti+1, o);if (di?1 = R & di = R) return P (ti|ti?1, o);}Figure 2: Pseudo-code for bidirectional inferencefor the first-order conditional markov models.
di isthe direction of the edge between ti and ti+1.classifier by taking into account the directions of theadjacent edges of the classification target.The second-order case is similar but slightly morecomplex.
Figure 3 shows the algorithm.
The recur-sive function needs to consider the directions of thefour adjacent edges of the classification target, andmaintain the directions of the two neighboring edgesto enumerate all possible edge directions.
In addi-tion, the algorithm rules out cycles in the structure.2.2 Decoding with the Easiest-First StrategyWe presented a polynomial time decoding algorithmin the previous section.
However, polynomial time isnot low enough in practice.
Indeed, even the Viterbidecoding of second-order markov models for POStagging is not practical unless some pruning methodis involved.
The computational cost of the bidirec-tional decoding algorithm presented in the previoussection is, of course, larger than that because it enu-merates all possible directions of the edges on top ofthe enumeration of possible tag sequences.In this section we present a greedy version of thedecoding method for bidirectional inference, which469function bestScore(){return bestScoreSub(n+3, ?end, end, end, end, end?, ?L,L, L, L?, ?L,L?
);}function bestScoreSub(i+2, ?ti?2, ti?1, ti, ti+1ti+2?, ?d?i?1, di?1, di, d?i+1?, ?di?2, d?i?
){// to avoid cyclesif (di?1 = di & di != d?i) return 0;// memorizationif (cached(i+2, ?ti?2, ti?1, ti, ti+1ti+2?, ?d?i?1, di?1, di, d?i+1?, ?di?2, d?i?
)return cache(i+2, ?ti?2, ti?1, ti, ti+1ti+2?, ?d?i?1, di?1, di, d?i+1?, ?di?2, d?i?
);// left boundary caseif (i = -2)if (?ti?2, ti?1, ti, ti+1, ti+2?
= ?start, start, start, start, start?)
return 1;else return 0;// recursive caseP = localClassification(i, ?ti?2, ti?1, ti, ti+1, ti+2?, ?d?i?1, di?1, di, d?i+1?
);return maxd?i?2maxdi?3 maxti?3 P?
bestScoreSub(i+1, ?ti?3, ti?2, ti?1, titi+1?, ?d?i?2, di?2, di?1, d?i?, ?di?3, d?i?1?
);}Figure 3: Pseudo-code for bidirectional inference for the second-order conditional markov models.
di is thedirection of the edge between ti and ti+1.
d?i is the direction of the edge between ti?1 and ti+1.
We omit thelocalClassification function because it is the obvious extension of that for the first-order case.is extremely simple and significantly more efficientthan full bidirectional decoding.Instead of enumerating all possible decomposi-tion structures, the algorithm determines the struc-ture by adopting the easiest-first strategy.
The wholedecoding algorithm is given below.1.
Find the ?easiest?
word to tag.2.
Tag the word.3.
Go back to 1. until all the words are tagged.We assume in this paper that the ?easiest?
wordto tag is the word for which the classifier outputsthe highest probability.
In finding the easiest word,we use the appropriate local classifier according tothe availability of the neighboring tags.
Therefore,in the first iteration, we always use the local classi-fiers trained with no contextual tag information (i.e.
(P (ti|o)).
Then, for example, if t3 has been taggedin the first iteration in a three-word sentence, we useP (t2|t3o) to compute the probability for tagging t2in the second iteration (as in Figure 1 (b)).A naive implementation of this algorithm requiresO(n2) invocations of local classifiers, where n is thenumber of the words in the sentence, because weneed to update the probabilities over the words ateach iteration.
However, a k-th order Markov as-sumption obviously allows us to skip most of theprobability updates, resulting in O(kn) invocationsof local classifiers.
This enables us to build a veryefficient tagger.3 Maximum Entropy ClassifierFor local classifiers, we used a maximum entropymodel which is a common choice for incorporatingvarious types of features for classification problemsin natural language processing (Berger et al, 1996).Regularization is important in maximum entropymodeling to avoid overfitting to the training data.For this purpose, we use the maximum entropymodeling with inequality constraints (Kazama andTsujii, 2003).
The model gives equally good per-formance as the maximum entropy modeling withGaussian priors (Chen and Rosenfeld, 1999), andthe size of the resulting model is much smaller thanthat of Gaussian priors because most of the param-eters become zero.
This characteristic enables usto easily handle the model data and carry out quickdecoding, which is convenient when we repetitivelyperform experiments.
This modeling has one param-eter to tune, which is called the width factor.
Wetuned this parameter using the development data ineach type of experiments.470Current word wi & tiPrevious word wi?1 & tiNext word wi+1 & tiBigram features wi?1, wi & tiwi, wi+1 & tiPrevious tag ti?1 & tiTag two back ti?2 & tiNext tag ti+1 & tiTag two ahead ti+2 & tiTag Bigrams ti?2, ti?1 & titi?1, ti+1 & titi+1, ti+2 & tiTag Trigrams ti?2, ti?1, ti+1 & titi?1, ti+1, ti+2 & tiTag 4-grams ti?2, ti?1, ti+1, ti+2 & tiTag/Word ti?1, wi & ticombination ti+1, wi & titi?1, ti+1, wi & tiPrefix features prefixes of wi & ti(up to length 10)Suffix features suffixes of wi & ti(up to length 10)Lexical features whether wi has a hyphen & tiwhether wi has a number & tiwhether wi has a capital letter & tiwhether wi is all capital & tiTable 1: Feature templates used in POS tagging ex-periments.
Tags are parts-of-speech.
Tag featuresare not necessarily used in all the models.
For ex-ample, ?next tag?
features cannot be used in left-to-right models.4 ExperimentsTo evaluate the bidirectional inference methods pre-sented in the previous sections, we ran experimentson POS tagging and text chunking with standard En-glish data sets.Although achieving the best accuracy is not theprimary purpose of this paper, we explored usefulfeature sets and parameter setting by using develop-ment data in order to make the experiments realistic.4.1 Part-of-speech tagging experimentsWe split the Penn Treebank corpus (Marcus et al,1994) into training, development and test sets as in(Collins, 2002).
Sections 0-18 are used as the train-ing set.
Sections 19-21 are the development set, andsections 22-24 are used as the test set.
All the ex-periments were carried out on the development set,except for the final accuracy report using the bestsetting.For features, we basically adopted the feature setMethod Accuracy Speed(%) (tokens/sec)Left-to-right (Viterbi) 96.92 844Right-to-left (Viterbi) 96.89 902Dependency Networks 97.06 1,446Easiest-last 96.58 2,360Easiest-first 97.13 2,461Full bidirectional 97.12 34Table 2: POS tagging accuracy and speed on the de-velopment set.Method Accuracy (%)Dep.
Networks (Toutanova et al, 2003) 97.24Perceptron (Collins, 2002) 97.11SVM (Gimenez and Marquez, 2003) 97.05HMM (Brants, 2000) 96.48Easiest-first 97.10Full Bidirectional 97.15Table 3: POS tagging accuracy on the test set (Sec-tions 22-24 of the WSJ, 5462 sentences).provided by (Toutanova et al, 2003) except for com-plex features such as crude company-name detectionfeatures because they are specific to the Penn Tree-bank and we could not find the exact implementationdetails.
Table 1 lists the feature templates used in ourexperiments.We tested the proposed bidirectional methods,conventional unidirectional methods and the bidirec-tional dependency network proposed by Toutanova(Toutanova et al, 2003) for comparison.
1.
Allthe models are second-order.
Table 2 shows theaccuracy and tagging speed on the developmentdata 2.
Bidirectional inference methods clearly out-performed unidirectional methods.
Note that theeasiest-first decoding method achieves equally goodperformance as full bidirectional inference.
Table 2also shows that the easiest-last strategy, where weselect and tag the most difficult word at each itera-tion, is clearly a bad strategy.An example of easiest-first decoding is given be-low:1For dependency network and full bidirectional decoding,we conducted pruning because the computational cost was toolarge to perform exhaustive search.
We pruned a tag candidate ifthe zero-th order probability of the candidate P (ti|o) was lowerthan one hundredth of the zero-th order probability of the mostlikely tag at the token.2Tagging speed was measured on a server with an AMDOpteron 2.4GHz CPU.471The/DT/4 company/NN/7 had/VBD/11sought/VBN/14 increases/NNS/13 total-ing/VBG/12 $/$/2 80.3/CD/5 million/CD/8,/,/1 or/CC/6 22/CD/9 %/NN/10 ././3Each token represents Word/PoS/DecodingOrder.Typically, punctuations and articles are tagged first.Verbs are usually tagged in later stages because theirtags are likely to be ambiguous.We applied our bidirectional inference methodsto the test data.
The results are shown in Table 3.The table also summarizes the accuracies achievedby several other research efforts.
The best accuracyis 97.24% achieved by bidirectional dependency net-works (Toutanova et al, 2003) with a richer set offeatures that are carefully designed for the corpus.
Aperceptron algorithm gives 97.11% (Collins, 2002).Gimenez and Marquez achieve 97.05% with supportvector machines (SVMs).
This result indicates thatbidirectional inference with maximum entropy mod-eling can achieve comparable performance to otherstate-of-the-art POS tagging methods.4.2 Chunking ExperimentsThe task of chunking is to find non-recursive phrasesin a sentence.
For example, a chunker segments thesentence ?He reckons the current account deficit willnarrow to only 1.8 billion in September?
into the fol-lowing,[NP He] [VP reckons] [NP the current accountdeficit] [VP will narrow] [PP to] [NP only 1.8 bil-lion] [PP in] [NP September] .We can regard chunking as a tagging task by con-verting chunks into tags on tokens.
There are severalways of representing text chunks (Sang and Veen-stra, 1999).
We tested the Start/End representationin addition to the popular IOB2 representation sincelocal classifiers can have fine-grained informationon the neighboring tags in the Start/End represen-tation.For training and testing, we used the data set pro-vided for the CoNLL-2000 shared task.
The trainingset consists of section 15-18 of the WSJ corpus, andthe test set is section 20.
In addition, we made thedevelopment set from section 21 3.We basically adopted the feature set provided in3We used the Perl script provided onhttp://ilk.kub.nl/?
sabine/chunklink/Current word wi & tiPrevious word wi?1 & tiWord two back wi?2 & tiNext word wi+1 & tiWord two ahead wi+2 & tiBigram features wi?2, wi?1 & tiwi?1, wi & tiwi, wi+1 & tiwi+1, wi+2 & tiCurrent POS pi & tiPrevious POS pi?1 & tiPOS two back pi?2 & tiNext POS pi+1 & tiPOS two ahead pi+2 & tiBigram POS features pi?2, pi?1 & tipi?1, pi & tipi, pi+1 & tipi+1, pi+2 & tiTrigram POS features pi?2, pi?1, pi & tipi?1, pi, pi+1 & tipi, pi+1, pi+2 & tiPrevious tag ti?1 & tiTag two back ti?2 & tiNext tag ti+1 & tiTag two ahead ti+2 & tiBigram tag features ti?2, ti?1 & titi?1, ti+1 & titi+1, ti+2 & tiTable 4: Feature templates used in chunking experi-ments.
(Collins, 2002) and used POS-trigrams as well.
Ta-ble 4 lists the features used in chunking experiments.Table 5 shows the results on the development set.Again, bidirectional methods exhibit better perfor-mance than unidirectional methods.
The differenceis bigger with the Start/End representation.
Depen-dency networks did not work well for this chunkingtask, especially with the Start/End representation.We applied the best model on the developmentset in each chunk representation type to the testdata.
Table 6 summarizes the performance on thetest set.
Our bidirectional methods achieved F-scores of 93.63 and 93.70, which are better than thebest F-score (93.48) of the CoNLL-2000 shared task(Sang and Buchholz, 2000) and comparable to thoseachieved by other state-of-the-art methods.5 DiscussionThere are some reports that one can improve theperformance of unidirectional models by combiningoutputs of multiple taggers.
Shen et al (2003) re-ported a 4.9% error reduction of supertagging by472Representation Method Order Recall Precision F-score Speed (tokens/sec)IOB2 Left-to-right 1 93.17 93.05 93.11 1,7752 93.13 92.90 93.01 989Right-to-left 1 92.92 92.82 92.87 1,6352 92.92 92.74 92.87 927Dependency Networks 1 92.71 92.91 92.81 2,5342 92.61 92.95 92.78 1,893Easiest-first 1 93.17 93.04 93.11 2,4412 93.35 93.32 93.33 1,248Full Bidirectional 1 93.29 93.14 93.21 7122 93.26 93.12 93.19 48Start/End Left-to-right 1 92.98 92.69 92.83 8612 92.96 92.67 92.81 439Right-to-left 1 92.92 92.83 92.87 8872 92.89 92.74 92.82 451Dependency Networks 1 87.10 89.56 88.32 1,8942 87.16 89.44 88.28 331Easiest-first 1 93.33 92.95 93.14 1,9502 93.31 92.95 93.13 1,016Full Bidirectional 1 93.52 93.26 93.39 3922 93.44 93.20 93.32 4Table 5: Chunking F-scores on the development set.Method Recall Precision F-scoreSVM (Kudoh and Matsumoto, 2000) 93.51 93.45 93.48SVM voting (Kudo and Matsumoto, 2001) 93.92 93.89 93.91Regularized Winnow (with basic features) (Zhang et al, 2002) 93.60 93.54 93.57Perceptron (Carreras and Marquez, 2003) 93.29 94.19 93.74Easiest-first (IOB2, second-order) 93.59 93.68 93.63Full Bidirectional (Start/End, first-order) 93.70 93.65 93.70Table 6: Chunking F-scores on the test set (Section 20 of the WSJ, 2012 sentences).pairwise voting between left-to-right and right-to-left taggers.
Kudo et al (2001) attained performanceimprovement in chunking by conducting weightedvoting of multiple SVMs trained with distinct chunkrepresentations.
The biggest difference between ourapproach and such voting methods is that the lo-cal classifier in our bidirectional inference methodscan have rich information for decision.
Also, vot-ing methods generally need many tagging processesto be run on a sentence, which makes it difficult tobuild a fast tagger.Our algorithm can be seen as an ensemble classi-fier by which we choose the highest probability oneamong the different taggers with all possible decom-position structures.
Although choosing the highestprobability one is seemingly natural and one of thesimplest ways for combining the outputs of differenttaggers, one could use a different method (e.g.
sum-ming the probabilities over the outputs which sharethe same label sequence).
Investigating the methodsfor combination should be an interesting direction offuture work.As for the computational cost for training, ourmethods require us to train 22n types of classifierswhen we adopt an nth order markov assumption.
Inmany cases a second-order model is sufficient be-cause further increase of n has little impact on per-formance.
Thus the training typically takes four or16 times as much time as it would take for training asingle unidirectional tagger, which looks somewhatexpensive.
However, because each type of classi-fier can be trained independently, the training canbe performed completely in parallel and run withthe same amount of memory as that for training asingle classifier.
This advantage contrasts with thecase for CRFs which requires substantial amount ofmemory and computational cost if one tries to incor-porate higher-order features about tag sequences.Tagging speed is another important factor inbuilding a practical tagger for large-scale text min-473ing.
Our inference algorithm with the easiest-firststrategy needs no Viterbi decoding unlike MEMMsand CRFs, and makes it possible to perform very fasttagging with high precision.6 ConclusionWe have presented a bidirectional inference algo-rithm for sequence labeling problems such as POStagging, named entity recognition and text chunk-ing.
The algorithm can enumerate all possible de-composition structures and find the highest prob-ability sequence together with the correspondingdecomposition structure in polynomial time.
Wehave also presented an efficient bidirectional infer-ence algorithm based on the easiest-first strategy,which gives comparable performance to full bidi-rectional inference with significantly lower compu-tational cost.Experimental results of POS tagging and textchunking show that the proposed bidirectional in-ference methods consistently outperform unidi-rectional inference methods and our bidirectionalMEMMs give comparable performance to thatachieved by state-of-the-art learning algorithms in-cluding kernel support vector machines.A natural extension of this work is to replacethe maximum entropy modeling, which was used asthe local classifiers, with other machine learning al-gorithms.
Support vector machines with appropri-ate kernels is a good candidate because they havegood generalization performance as a single classi-fier.
Although SVMs do not output probabilities, theeasiest-first method would be easily applied by con-sidering the margins output by SVMs as the confi-dence of local classification.ReferencesYasemin Altun, Ioannis Tsochantaridis, and ThomasHofmann.
2003.
Hidden markov support vector ma-chines.
In Proceedings of ICML 2003, pages 3?10.Adam L. Berger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Computational Lin-guistics, 22(1):39?71.Thorsten Brants.
2000.
TnT ?
a statistical part-of-speechtagger.
In Proceedings of the 6th Applied NLP Con-ference (ANLP).Xavier Carreras and Lluis Marquez.
2003.
Phrase recog-nition by filtering and ranking with perceptrons.
InProceedings of RANLP-2003.Stanley F. Chen and Ronald Rosenfeld.
1999.
A gaus-sian prior for smoothing maximum entropy models.Technical Report CMUCS -99-108, Carnegie MellonUniversity.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP 2002, pages 1?8.Jesus Gimenez and Lluis Marquez.
2003.
Fast and accu-rate part-of-speech tagging: The SVM approach revis-ited.
In Proceedings of RANLP 2003, pages 158?165.Jun?ichi Kazama and Jun?ichi Tsujii.
2003.
Evaluationand extension of maximum entropy models with in-equality constraints.
In Proceedings of EMNLP 2003.Taku Kudo and Yuji Matsumoto.
2001.
Chunking withsupport vector machines.
In Proceedings of NAACL2001.Taku Kudoh and Yuji Matsumoto.
2000.
Use of supportvector learning for chunk identification.
In Proceed-ings of CoNLL-2000, pages 142?144.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of ICML 2001, pages 282?289.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics, 19(2):313?330.Erik F. Tjong Kim Sang and Sabine Buchholz.
2000.
In-troduction to the conll-2000 shared task: Chunking.In Proceedings of CoNLL-2000 and LLL-2000, pages127?132.Erik F. Tjong Kim Sang and Jorn Veenstra.
1999.
Rep-resenting text chunks.
In Proceedings of EACL 1999,pages 173?179.Libin Shen and Aravind K. Joshi.
2003.
A SNoW basedSupertagger with Application to NP Chunking.
InProceedings of ACL 2003, pages 505?512.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of HLT-NAACL 2003, pages 252?259.Tong Zhang, Fred Damereau, and David Johnson.
2002.Text chunking based on a generalization of winnow.Journal of Machine Learning Research, 2:615?638.474
