Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2195?2204,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsTemporal Anchoring of Events for the TimeBank CorpusNils Reimers?
?, Nazanin Dehghani??
?, Iryna Gurevych??
?Ubiquitous Knowledge Processing Lab (UKP-TUDA)Department of Computer Science, Technische Universit?at Darmstadt?
Research Training Group AIPHESTechnische Universit?at Darmstadthttp://www.ukp.tu-darmstadt.deAbstractToday?s extraction of temporal informa-tion for events heavily depends on an-notated temporal links.
These so calledTLINKs capture the relation between pairsof event mentions and time expressions.One problem is that the number of possibleTLINKs grows quadratic with the num-ber of event mentions, therefore most an-notation studies concentrate on links formentions in the same or in adjacent sen-tences.
However, as our annotation studyshows, this restriction results for 58% ofthe event mentions in a less precise infor-mation when the event took place.This paper proposes a new annotationscheme to anchor events in time.
Not onlyis the annotation effort much lower as itscales linear with the number of events, italso gives a more precise anchoring whenthe events have happened as the completedocument can be taken into account.
Us-ing this scheme, we annotated a subset ofthe TimeBank Corpus and compare our re-sults to other annotation schemes.
Addi-tionally, we present some baseline exper-iments to automatically anchor events intime.
Our annotation scheme, the auto-mated system and the annotated corpus arepublicly available.11 IntroductionIn automatic text analysis, it is often important toprecisely know when an event occurred.
A user?Guest researcher from the School of Electrical andComputer Engineering, University of Tehran.1https://www.ukp.tu-darmstadt.de/data/timeline-generation/temporal-anchoring-of-events/might be interested in retrieving news articles thatdiscuss certain events which happened in a giventime period, for example articles discussing carbombings in the 1990s.
The user might not onlybe interested in articles from that time period, butalso in more recent articles that cover events fromthat period.
Knowing when an event happened isalso essential for time aware summarization, au-tomated timeline generation as well as automaticknowledge base creation.
In many cases, timeplays a crucial role for facts stored in a knowledgebase, for example for the facts when a person wasborn or died.
Also, some facts are only true for acertain time period, like being the president of acountry.
Event extraction can be used to automat-ically infer many facts for knowledge bases, how-ever, to be useful, it is crucial that the date whenthe event happened can precisely be extracted.The TimeBank Corpus (Pustejovsky et al,2003) is a widely used corpus using the TimeMLspecifications (Saur??
et al, 2004) for the anno-tations of event mentions and temporal expres-sions.
In order to anchor events in time, the Time-Bank Corpus uses the concept of temporal links(TLINKs) that were introduced by Setzer (2001).A TLINK states the temporal relation between twoevents or an event and a time expression.
For ex-ample, an event could happen before, simultane-ous, or after a certain expression of time.
TheTimeBank Corpus served as dataset for the sharedtasks TempEval-1, 2 and 3 (Verhagen et al, 2007;Verhagen et al, 2010; UzZaman et al, 2013).In this paper we describe a new approach to an-chor every event in time.
Instead of using tem-poral links between events and temporal expres-sions, we consider the event time as an argumentof the event mention.
The annotators are askedto write down the date when an event happenedin a normalized format for every event mention.The annotation effort is for this reason identical2195to the number of event mentions, i.e.
for a doc-ument with 200 event mentions, the annotatorsmust perform 200 annotations.
When annotatingthe event mentions, the annotators are asked totake the complete document into account.
Section3 presents our annotation scheme, and section 4gives details about the conducted annotation study.The number of possible TLINKs scalesquadratic with the number of events and temporalexpressions.
Some documents of the TimeBankCorpus contain more than 200 events and tem-poral expressions, resulting in more than 20.000possible TLINKs.
Hand-labeling all links isextremely time-consuming and even when usingtransitive closures and computational support, it isnot feasible to annotate all possible TLINKs for alarger set of documents.
Therefore, all annotationstudies limited the number of TLINKs to annotate.For example, in the original TimeBank Corpus,only links that are salient were annotated.
WhichTLINKs are salient is fairly vague and results ina comparably low reported inter-annotator agree-ment.
Furthermore, around 62% of all events donot have any attached TLINK, i.e.
for most ofthe events in the original TimeBank Corpus, notemporal statement can be made.In contrast to the sparse annotation of TLINKsused in the TimeBank Corpus, the TimeBank-Dense Corpus (Cassidy et al, 2014) used a denseannotation and all temporal links for events andtime expressions in the same sentence and in di-rectly succeeding sentences were annotated.
For asubset of 36 documents with 1729 events and 289time expressions, they annotated 12,715 temporallinks, which is around 6.3 links per event and timeexpression.
Besides the large effort needed for adense annotation, a major downside is the limita-tion that events and time expressions must be inthe same or in adjacent sentences.
Our annota-tion study showed that in 58.72% of the cases themost informative temporal expression is more thanone sentence apart from the event mention.
Foraround 25% of the events, the most informativetemporal expression is even five or more sentencesaway.
Limiting the TLINKs to pairs that are atmost one sentence apart poses the risk that impor-tant TLINKs are not annotated and consequentlycannot be learned by automated systems.A further drawback of TLINKs is that it canbe difficult or even impossible to encode tempo-ral information that originates from different partsin the text.
Given the sentence:December 30th, 2015 - During NewYear?s Eve, it is traditionally very busyin the center of Brussels and peoplegather for the fireworks display.
But theupcoming [display]Eventwas canceledtoday due to terror alerts.For a human it is simple to infer the date for theevent display.
But it is not possible to encode thisknowledge using TLINKs, as the date is not ex-plicitly mentioned in the text.To make our annotations comparable tothe dense TLINK annotation scheme of theTimeBank-Dense Corpus (Cassidy et al, 2014),we annotated the same documents and comparethe results in section 5.
For 385 out of 872 events(44.14%), our annotation scheme results in a moreprecise value on which date an event happened.Section 6 presents a baseline system to extractevent times.
For a subset of events, it achieves anF1-score of 49.01% while human agreement forthese events is 80.50%.2 Previous Annotation WorkThe majority of corpora on events uses sparsetemporal links (TLINKs) to enable anchoring ofevents in time.
The original TimeBank (Puste-jovsky et al, 2003) only annotated salient tem-poral relations.
The subsequent TempEval com-petitions (Verhagen et al, 2007; Verhagen et al,2010; UzZaman et al, 2013) are based on theoriginal TimeBank annotations, but tried to im-prove the coverage and added some further tem-poral links for mentions in the same sentence.
TheMEANtime corpus (van Erp et al, 2015) applieda sparse annotation and only temporal links be-tween events and temporal expressions in the sameand in succeeding sentences were annotated.
TheMEANtime corpus distinguishes between mainevent mentions and subordinated event mentions,and the focus for TLINKs was on main events.More dense annotations were applied by Bram-sen et al (2006), Kolomiyets et al (2012), Do etal.
(2012) and by Cassidy et al (2014).
WhileBramsen et al, Kolomiyets et al, and Do et alonly annotated some temporal links, Cassidy et alannotated all Event-Event, Event-Time, and Time-Time pairs in the same sentence as well as in di-rectly succeeding sentences leading to the densestannotation for the TimeBank Corpus.2196A drawback of the previous annotation worksis the limitation that only links between expres-sions in the same or in succeeding sentences areannotated.
In case the important temporal expres-sion, that defines when the event occurred, is morethan one sentence away, the TLINK will not be an-notated.
Consequently, retrieving the informationwhen the event occurred is not possible.
Increas-ing this window size would result in a significantlyincreased annotation effort as the number of linksgrows quadratic with the number of expressions.Our annotation is the first for the TimeBankCorpus that does not try to annotate the quadraticgrowing number of temporal links.
Instead, weconsider the event time as an argument of the indi-vidual event mention and it is annotated directlyby the annotators.
This reduces the annotationeffort by 85% in comparison to the TimeBank-Dense Corpus.
This allows an annotator to anno-tate significant more documents in the same time.Also, all temporal information, independent whereit is mentioned in the document, can be taken intoaccount resulting in a much more precise anchor-ing of events in time, as section 5 shows.3 Event Time Annotation SchemeThe annotation guidelines for the TimeBank Cor-pus (Saur??
et al, 2004) define an event as a coverterm for situations that happen or occur.
Eventscan be punctual or last for a period of time.
Theyalso consider as events those predicates describingstates or circumstances in which something holdstrue.
For the TimeBank Corpus, the smallest ex-tent of text (usually a single word) that expressesthe occurrence of an event is annotated.The aspectual type of the annotated events inthe TimeBank Corpus can be distinguished intoachievement events, accomplishment events, andstates (Pustejovsky, 1991).
An achievement is anevent that results into an instantaneous change ofsome sort.
Examples of achievement events are tofind, to be born, or to die.
Accomplishment eventsalso result into a change of some sort, however,the change spans over a longer time period.
Ex-amples are to build something or to walk some-where.
States on the other hand do not describea change of some sort, but that something holdstrue for some time, for example, being sick or tolove someone.
The aspectual type of an event doesnot only depend on the event itself, but also on thecontext in which the event is expressed.Our annotation scheme was created with thegoal of being able to create a knowledge basefrom the extracted events in combination withtheir event times.
Punctual events are a single doton the time axis while events that last for a pe-riod of time have a begin and an end point.
It canbe difficult to distinguish between punctual eventsand events with a short duration.
Furthermore, thedocuments typically do not report precise startingand ending times for events, hence we decided todistinguish between events that happened at a Sin-gle Day and Multi-Day Events that span over mul-tiple days.
We used days as the smallest granu-larity for the annotation as none of the annotatedarticles contained any information on the hour, theminute or the second when the event happened.
Incase a corpus contains this information, the anno-tation scheme could be extended to include this in-formation as well.For Single Day Events, the event time is writ-ten in the format YYYY-MM-DD.
For Multi-DayEvents, the annotator annotates the begin point andthe end point of the event.
In case no statement canbe made on when an event happened, the eventwill be annotated with the label not applicable.This applies only to 0.67% of the annotated eventsin the TimeBank Corpus which is mainly due toannotation errors in the TimeBank Corpus.He was sent into space on May 26,1980.
He spent six days aboard theSalyut 6 spacecraft.The first event in this text, sent, will be anno-tated with the event time 1980-05-26.
The secondevent, spent, is a Multi-Day Event and is anno-tated with the event time beginPoint=1980-05-26and endPoint=1980-06-01.In case the exact event time is not stated inthe document, the annotators are asked to narrowdown the possible event time as precisely as possi-ble.
For this purpose, they can annotate the eventtime with after YYYY-MM-DD and before YYYY-MM-DD.In 1996 he was appointed military at-tache at the Hungarian embassy inWashington.
[...] McBride was part of aseven-member crew aboard the OrbiterChallenger in October 1984The event appointed is annotated after 1996-01-01 before 1996-12-31 as the event must have hap-pened sometime in 1996.
The Multi-Day Event2197part is annotated with beginPoint=after 1984-10-01 before 1984-10-31 and endPoint=after 1984-10-01 before 1984-10-31.To speed up the annotation process, annota-tors were allowed to write YYYY-MM-xx to ex-press that something happened sometime withinthe specified month and YYYY-xx-xx to express thatthe event happened sometime during the specifiedyear.
Annotators were also allowed to annotateevents that happened at the Document CreationTime with the label DCT.The proposed annotation scheme requires thatevent mentions are already annotated.
For our an-notation study we used the event mentions thatwere already defined in the TimeBank Corpus.
Incontrast to the annotation of TLINKs, temporalexpressions must not be annotated in the corpus.4 Annotation StudyThe annotation study was performed on the samesubset of documents as used by the TimeBank-Dense Corpus (Cassidy et al, 2014) with theevent mentions that are present in the TempEval-3dataset (UzZaman et al, 2013).
Cassidy et al se-lected 36 random documents from the TimeBankCorpus (Pustejovsky et al, 2003).
These 36 doc-uments include a total of 1498 annotated events.This allows to compare our annotations to thoseof the TimeBank-Dense Corpus (see section 5).Each document has been independently anno-tated by two annotators according to the annota-tion scheme introduced above.
We used the freelyavailable WebAnno (Yimam et al, 2013).
Tospeed up the annotation process, the existent tem-poral expressions that are defined in the TimeBankCorpus were highlighted.
These temporal expres-sions are in principle not required to perform ourannotations, but the highlighting of them helps todetermine the event time.
Figure 1 depicts a sam-ple annotation made by WebAnno.
The two anno-tators were trained on 15 documents distinct fromthe 36 documents annotated for the study.
Dur-ing the training stage, the annotators discussed thedecisions they have made with each other.After both annotators completed the annotationtask, the two annotations were curated by one per-son to derive one final annotation.
The curator ex-amined the events where the annotators disagreedand decided on the final annotation.
The final an-notation might be a merge of the two provided an-notations.Figure 1: Sample Annotation made with We-bAnno.
The violet annotations are existing an-notations of temporal expressions from the Time-Bank Corpus.
The span for the beige annotations,the event mentions, come also from the TimeBankCorpus.
Our annotators added the value for theevent time for those beige annotations.4.1 Inter-Annotator-AgreementWe use Krippendorff?s ?
(Krippendorff, 2004)with the nominal metric to compute the Inter-Annotator-Agreement (IAA).
The nominal metricconsiders all distinct labels equally distant fromone another, i.e.
partial agreement is not measured.The annotators must therefore completely agree.Using this metric, the Krippendorff?s ?
for the36 annotated documents is ?
= 0.617.
Cassidyet al (2014) reported a Kappa agreement between0.56?0.64 for their annotation of TLINKs.
Com-paring these numbers is difficult, as the annota-tion tasks were different.
According to Landis andKoch (1977), these numbers lie on the border of amoderate and a substantial level of agreement.4.2 Disagreement AnalysisIn 648 out of 1498 annotated events, the anno-tators disagreed on the event time.
In 42.3% ofthe disagreements, the annotators disagreed onwhether the event mention is a Single Day Eventor a Multi-Day Event.
Such disagreement occurswhen it is unclear from the text whether the eventlasted for one or for several days.
For example,an article reported on a meeting and due to a lackof precise temporal information in the document,one annotator assumed that the meeting lasted forone day, the other that it lasted for several days.A different source for the disagreement has beenthe annotation of states.
They can either be anno-tated with the date where the text gives evidencethat they hold true, or they can be annotated as aMulti-Day Event that begins before that date andends after that date.Different annotations for Multi-Day Events ac-count for 231 out of the 648 disagreements(35.6%).
In this category, the annotators disagreed2198on the begin point in 110 cases (47.6%), on theend point in 57 cases (24.7%) and on the begin aswell as on the end point in 64 cases (27.7%).
TheKrippendorff?s ?
for all begin point annotations is0.629 and for all end point annotations it is 0.737.A disagreement on Single Day Events was ob-served for 143 event mentions and accounts for22.1% of the disagreements.
The observed agree-ment for Single Day Events is 80.5% or ?
=0.799.
Most disagreements for Single Day Eventswere whether the event occurred on the same dateas the document was written or if it occurred be-fore the document was written.4.3 Measuring Partial AgreementOne issue of the strict nominal metric is that itdoes not take partial agreement into account.
Inseveral cases, the two annotators agreed in prin-ciple on the event time, but might have labeledit slightly differently.
One annotator might havetaken more clues from the text into account to nar-row down when an event has happened.
One an-notator for example, has annotated an event withthe label after 1998-08-01 before 1998-08-31.
Thesecond annotator has taken an additional textualclue into account, which was that the event musthave happened in the first half of August 1998 andannotated it as after 1998-08-01 before 1998-08-15.
Even though both annotators agree in princi-ple, when using the nominal metric it would beconsidered as a distinct annotation.To measure this effect, we created a relaxedmetric to measure mutual exclusivity:dME(a, b) ={1 if a and b are mutual exclusive0 elseThe metric measures whether two annotationscan be satisfied at the same time.
Given the eventhappened on August 5th, 1998, then the two an-notations after 1998-08-01 before 1998-08-31 andafter 1998-08-01 before 1998-08-15 would bothbe satisfied.
In contrast, the two annotations after1998-02-01 and before 1997-12-31 can never besatisfied at the same time and are therefore mutualexclusive.Out of the 648 disagreements, 71 annotationswere mutually exclusive.
Computing the Krippen-dorff?s ?
with the above metric yields a value of?ME= 0.912.4.4 Annotation StatisticsTable 1 gives an overview of the assigned labels.Around 58.21% of the events are either instanta-neous events or their duration is at most one day.41.12% of the events are Multi-Day Events thattake place over multiple days.
While for SingleDay Events there is a precise date for 55.73% ofthe events, the fraction is much lower for Multi-Day Events.
In this category, only in 19.81% ofthe cases the begin point is precisely mentioned inthe article and only in 15.75% of the cases, the endpoint is precisely mentioned.The most prominent label for Single Day Eventsis the Document Creation Time (DCT).
48.28% ofSingle Day Events happened on the day the arti-cle was created, 33.49% of these events happenedat least one day before the DCT and 17.43% ofthe mentions refer to future events.
This distribu-tion shows, that the news articles and TV broad-cast transcripts from the TimeBank Corpus mainlyreport on events that happened on the same day.For Multi-Day Events, the distribution looksdifferent.
In 76.46% of the cases, the event startedin the past, and in 65.10% of the cases, it is stillongoing.4.5 Most Informative Temporal ExpressionNot all temporal expressions in a text are of thesame relevance for an event.
In fact, in manycases only a single temporal expression is of im-portance, which is the expression stating when theevent occurred.
Our annotations allow us to de-termine most informative temporal expression foran event.
We define the most informative tem-poral expression as the expression that has beenused by the annotator to determine the event time.We checked for all annotations whether the eventdate can be found as a temporal expression in thedocument and computed the distance to the closestone with a matching value.
The distance is mea-sured as the number of sentences.
421 out of 1498events happened on the Document Creation Timeand were excluded from this computation.
TheDocument Creation Time is provided as additionalmetadata in the TimeBank Corpus, and it is oftennot explicitly mentioned in the document text.Figure 2 shows the distance between the mostinformative temporal expression and the eventmention.
In 23.68% of the cases, the time ex-pression is in the same sentence, and in 17.59%of the cases, the time expression is either in the2199# Events %Single Day Events 872 58.21%with precise date 486 55.73%after + before 145 16.63%after 124 14.22%before 117 13.42%past events 292 33.49%events at DCT 421 48.28%future events 152 17.43%Multi-Day Events 616 41.12%precise begin point 122 19.81%precise end point 97 15.75%begins in the past 471 76.46%begins on the DCT 38 6.17%begins in the future 105 17.05%ends in the past 179 29.06%ends on the DCT 26 4.22%ends in the future 401 65.10%Not applicable 10 0.67%Table 1: Statistic on the annotated event times.Single Day Events happen on a single day, Multi-Day Events take place over multiple days.
Theevent time can either be precise or the annota-tors used before and after to narrow down theevent time, e.g.
the event has happened in a cer-tain month and year.
DCT = Document CreationTime.next or in the previous sentence.
It follows that in58.72%, of the cases the most informative time ex-pression cannot be found in the same or in the pre-ceding or succeeding sentence.
This is importantto note, as previous shared tasks like TempEval-1,-2, and -3 (Verhagen et al, 2007; Verhagen etal., 2010; UzZaman et al, 2013) and previous an-notation studies like the TimeBank-Dense Corpus(Cassidy et al, 2014) only considered the relationbetween event mentions and temporal expressionsin the same and in adjacent sentences.
However,for the majority of events, the most informativetemporal expression is not in the same or in thepreceding / succeeding sentence.For 7.31% of the annotated events, no matchingtemporal expression was found in the document.Those were mainly events where the event timewas inferred by the annotators from multiple tem-poral expressions in the document.
An exampleis that the year of the event was mentioned in thebeginning of the document and the month of theevent was mentioned in a later part of the docu-ment.?
?5?4?3?2?1 0 1 2 3 4?5MS00.10.20.3Distance (# sentences)Distribution of DistancesFigure 2: Distribution of distances in sentencesbetween the event mention and the most infor-mative temporal expression.
For 58.72% of theevent mentions, the most informative time expres-sion is not in the same or in the previous/next sen-tence.
For 7.3% of the mentions, the time expres-sion originates from multiple sources (MS).5 Comparison of Annotation SchemesDepending on the application scenario and the textdomain, the use of TLINKs or the proposed an-notation scheme may be advantageous.
TLINKshave the capability to capture the temporal order ofevents, even when temporal expressions are com-pletely absent in a document, which is often thecase for novels.
The proposed annotation schemehas the advantage that temporal information, inde-pendent where and in which form it is mentionedin the document, can be taken into account.
How-ever, the proposed scheme requires that the eventscan be anchored on a time axis, which is easy fornews articles and encyclopedic text but hard fornovels and narratives.In this section, we evaluate the application sce-nario of temporal knowledge base population andtime-aware information retrieval.
For temporalknowledge base population, it is important to de-rive the date for facts and events as precisely aspossible (Surdeanu, 2013).
Those facts can ei-ther be instantaneous, e.g.
a person died, or theycan last for a longer time like a military conflict.2200Similar requirements are given for time-aware in-formation retrieval, where it can be important toknow at which point in time something occurred(Kanhabua and N?rv?ag, 2012).We use the TimeBank-Dense Corpus (Cassidyet al, 2014) with its TLINKs annotations andcompare those to our event time annotations.
TheTimeBank-Dense Corpus annotated all TLINKsbetween Event-Event, Event-Time, and Time-Time pairs in the same sentence and betweensucceeding sentences as well as all Event-DCTand Time-DCT pairs.
Six different link typeswere defined: BEFORE, AFTER, INCLUDES,IS INCLUDED, SIMULTANOUS, and VAGUE,where VAGUE encodes that the annotators wherenot able to make a statement on the temporal rela-tion of the pair.We studied how well the event time is capturedby the dense TLINK annotation.
We used transi-tive closure rules as described by Chambers et al(2014) to deduct also TLINKs for pairs that werenot annotated.
For example, when event1hap-pened before event2and event2happened beforedate1, we can infer that event1happened beforedate1.
Using this transitivity allows to infer re-lations for pairs that are more than one sentenceapart.
For all annotated events, we evaluated allTLINKs, including the TLINKs inferred from thetransitivity rules, and derived the event time as pre-cisely as possible.
We then computed how precisethe inferred event times are in comparison to ourannotations.
Preciseness is measured in the num-ber of days.
An event that is annotated with 1998-02-13 has the preciseness of 1 day.
If the inferredevent time from the TLINKs is after 1998-02-01and before 1998-02-15, then the preciseness is 15days.
A more precise anchoring is preferred.The TimeBank-Dense Corpus does not have alink type to mark that an event has started or endedat a certain time point.
This makes the TLINKannotation impractical for the durative events thatspan over multiple days.
According to our annota-tion study, 41.12% of the events in the TimeBankCorpus last for longer time periods.
For these41.12%, it cannot be inferred from when to whenthe events lasted.In 487 out of the 872 Single Day Events(55.85%), the TLINKs give a result with the sameprecision as our annotations.
For 198 events(22.71%), our annotation is more precise, i.e.
thetime window where the event might have hap-pened is smaller.
For 187 events (21.44%), noevent time could be inferred from the TLINKs.This is due to the fact that there was no link to anytemporal expression even when transitivity wastaken into account.For the 487 events where the TLINKs resultedin an event time as precise as our annotation, thevast majority of them were events that happenedat the Document Creation Time.
As depicted inTable 1, 421 events happened at DCT.
For thoseevents the precise date can directly be derivedfrom the annotated link between each event men-tion and the DCT.
For all other events that didnot happen at the Document Creation Time, theTLINKs result for the most cases in a less preciseanchoring in time and for around a fifth of thesecases in no temporal anchoring at all while we doanchor them.We can conclude, that even a dense TLINK an-notation gives suboptimal information on whenevents have happened, and due to the restrictionthat TLINKs are only annotated in the same andin adjacent sentences, a lot of relevant temporalinformation gets lost.6 Automated Event Time ExtractionIn this section, we present a baseline system forautomatic event time extraction.
The system usestemporal relations in which the event is involvedand anchors the event to the most precise time.
Forthis purpose, we have defined a two-step processto determine the events?
time.
Given a set of docu-ments in which the events and time expressions arealready annotated, the system first obtains a set ofpossible times for each of the events.
Second, themost precise time is selected or generated for eachevent.For the first step, we use the multi-pass ar-chitecture introduced by Chambers et al (2014)that was trained and evaluated on the TimeBank-Dense Corpus (Cassidy et al, 2014).
Cham-bers et al describe multiple rules and machinelearning based classifiers to extract relations be-tween events and temporal expressions.
This ar-chitecture extracts temporal relations of the typeBEFORE, AFTER, INCLUDES, IS INCLUDED,and SIMULTANOUS.
The classifiers are combinedinto a precision-ranked cascade of sieves.
The ar-chitecture presented by Chambers et al does notproduce temporal information that an event hasstarted or ended at a certain time point and can2201therefore only be used for Single Day Events.We use these sieves to add the value of the tem-poral expression and the corresponding relation toa set of possible times for each event.
In fact, foreach event we generate a set of <relation,time> tuples in which the event is involved.Police confirmed Friday that the bodyfound along a highwayFor example, the one sieve adds[IS INCLUDED, Friday1998?02?13] and asecond sieve adds [BEFORE, DCT1998?02?14] tothe set of possible event times for the confirmedevent.Applying the sequence of the sieves will ob-tain all various temporal links for each event.In the next step, if the event has a relationof type SIMULTANEOUS, IS INCLUDED orINCLUDES, the system sets the event time to thevalue of the time expression.
If the event has arelation of type BEFORE and/or AFTER, the sys-tem narrows down the event time as precisely aspossible.
If the sieve determines the relation typeas VAGUE, the set of possible event times remainsunchanged.Algorithm 1 demonstrates how the event time isselected or generated from a set of possible times.Algorithm 1 Automatic Event Time Extraction1: function EVENTTIME(times)2: if times is empty then3: return ?Not Available?
.
the event has no non-vague relation4: end if5: min before time = DATE.MAX VALUE6: max after time = DATE.MIN VALUE7: for [relation, time] in times do8: if relation is SIMULTANEOUS or IS INCLUDED or INCLUDES then9: return time10: else if relation is BEFORE and time < min before time then11: min before time = time12: else if relation is AFTER and time > max after time then13: max after time = time14: end if15: end for16: event time = AFTER + max after time + BEFORE + min before time17: return event time18: end functionApplying the proposed method on theTimeBank-Dense Corpus, we obtained somevalue for the event time for 593 of 872 (68%) Sin-gle Day Events.
For 359 events (41%), the systemgenerates the event time with the same precisionas our annotations.
Table 2 gives statistics of theautomatically obtained event times.To evaluate the output of the proposed system,we evaluated how precise the automatically ob-tained event times are in comparison with our an-notations.
Table 3 shows for 41% of events, theproposed system generates the same event timeSingle Day Events # Events %with precise date 260 29.82%after + before 16 1.84%after 99 11.35%before 218 25%not available 279 31.99%Table 2: Statistics on the automatically obtainedevent times for events happened on a single day.The obtained event time can either be precise orthe system used before and after to narrow downthe event time.
For 279 events, the system cannotinfer any event time.as our annotations.
For 21% events our annota-tion is more precise, i.e.
the time window wherethe event might have happened is smaller.
For 47events (5.38%), the system infers an event timethat is in conflict with the human annotation, forexample a disagreement if an event happened be-fore or after DCT.
Considering event times thathave the same preciseness as our annotations astrue positives, the precision of the proposed sys-tem is 60.54% and the recall is 41.17% for SingleDay Events.
As presented in section 4, human an-notators agree in 80.50% of the cases on the labelfor Single Day Events.
The less precise and non-inferred event times are mainly due to the fact thattemporal expressions, that are more than one sen-tence apart, are not taken into account by the sievearchitecture.Obtained event time # Events %same as human annotation 359 41.17%less precise 187 21.44%conflicting annotations 47 5.38%cannot infer event time 279 31.99%Precision 60.54%Recall 41.17%F1-Score 49.01%Human F1-Score 80.50%Table 3: Evaluation results of proposed system incomparison with our annotations.In this work we focused on the automated an-choring of Single Day Events and presented abaseline system that relies on the work of Cham-bers et al (2014).
The F1-score with 49.01%is in comparison to the human score of 80.50%comparatively low.
However, only in 5.38% ofthe cases, the automatically inferred event timeis plain wrong.
In the most cases, no event timecould be inferred (31.99%) or it was less precise2202than the human annotation (21.44%).Extending the described approach to Multi-Day-Events is not straight forward.
TheTimeBank-Dense Corpus and consequently thesystem by Chambers et al does not include aTLINK type to note that an event has started orended at a certain date, hence, extracting the beginpoint and end point for Multi-Day-Events is notpossible.
A fundamental adaption of the systemby Chambers et al would be required.In contrast to Single Day Events, extracting theevent time for Multi-Day Events requires more ad-vanced logic.
The start date of the event must bebefore the end date of the event.
The relation toevents that are included in the Multi-Day Eventsmust be checked to avoid inconsistencies.
The de-velopment of an automated system for Multi-DayEvents is subject of our ongoing work.7 ConclusionWe presented a new annotation scheme for anchor-ing events in time and annotated a subset of theTimeBank Corpus (Pustejovsky et al, 2003) usingthis annotation scheme.
The annotation guidelinesas well as the annotated corpus are publicly avail-able.2In the performed annotation study, the Krip-pendorff?s ?
inter-annotator agreement was con-siderably high at ?
= 0.617.
The largest disagree-ment resulted from events in which it was not ex-plicitly mentioned when the event happened.
Us-ing a more relaxed measure for Krippendorff?s ?which only assigns a distance to mutual exclusiveannotations, the agreement changed to ?ME=0.912.
We can conclude that after little training,annotators are able to perform the annotation witha high agreement.The effort for annotating TLINKs on the otherhand scales quadratic with the number of eventsand temporal expressions.
This imposes the oftenused restriction that only temporal links betweenevents and temporal expressions in the same orin succeeding sentences are annotated.
Even withthis restriction, the annotation effort is quite sig-nificant, as on average 6.3 links per mention mustbe annotated.
As Figure 2 depicts, in more than58.72% of the cases the most informative temporalexpression is more than one sentence apart fromthe event mention.
As a consequence, inferring2https://www.ukp.tu-darmstadt.de/data/timeline-generation/temporal-anchoring-of-events/from TLINKs when an event happened is less pre-cise as temporal information that is more than onesentence away can often not be taken into account.For the 872 Single Day Events, the correct eventtime could be inferred from the TLINKs only in487 cases.
For 187 Single Day Events, no eventtime at all could be inferred, as no temporal ex-pression was within the one sentence window ofthat event.A drawback of the proposed scheme is the lackof temporal ordering of events beyond the small-est unit of granularity, which was in our case oneday.
The scheme is suitable to note that severalevents occurred at the same date, but their orderon that date cannot be encoded.
In case the tem-poral ordering is important for the application sce-nario, the annotation scheme could be extendedand TLINKs could be annotated for events that fallon the same date.
Another option is to increasethe granularity, but this requires that the informa-tion in the documents also allow this more preciseanchoring.AcknowledgementThis work has been supported by the German Re-search Foundation as part of the Research TrainingGroup Adaptive Preparation of Information fromHeterogeneous Sources (AIPHES) under grantNo.
GRK 1994/1 and by the Volkswagen Founda-tion as part of the Lichtenberg-Professorship Pro-gram under grant No.
I/82806.
Additional supportwas provided by the German Federal Ministry ofEducation and Research (BMBF) as a part of theSoftware Campus program under the promotionalreference 01-S12054.ReferencesPhilip Bramsen, Pawan Deshpande, Yoong Keok Lee,and Regina Barzilay.
2006.
Inducing TemporalGraphs.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?06, pages 189?198, Stroudsburg, PA,USA.
Association for Computational Linguistics.Taylor Cassidy, Bill McDowell, Nathanael Chambers,and Steven Bethard.
2014.
An Annotation Frame-work for Dense Event Ordering.
In Proceedingsof the 52nd Annual Meeting of the Association forComputational Linguistics (Volume 2: Short Pa-pers), pages 501?506, Baltimore, Maryland, USA.Association for Computational Linguistics.Nathanael Chambers, Taylor Cassidy, Bill McDowell,and Steven Bethard.
2014.
Dense Event Ordering2203with a Multi-Pass Architecture.
Transactions of theAssociation for Computational Linguistics, 2:273?284.Quang Xuan Do, Wei Lu, and Dan Roth.
2012.
JointInference for Event Timeline Construction.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, EMNLP-CoNLL ?12, pages 677?687, Stroudsburg, PA, USA.Association for Computational Linguistics.Nattiya Kanhabua and Kjetil N?rv?ag.
2012.
Learningto Rank Search Results for Time-sensitive Queries.In Proceedings of the 21st ACM International Con-ference on Information and Knowledge Manage-ment, CIKM ?12, pages 2463?2466, New York, NY,USA.
ACM.Oleksandr Kolomiyets, Steven Bethard, and Marie-Francine Moens.
2012.
Extracting Narrative Time-lines As Temporal Dependency Structures.
In Pro-ceedings of the 50th Annual Meeting of the Associ-ation for Computational Linguistics: Long Papers -Volume 1, ACL ?12, pages 88?97, Stroudsburg, PA,USA.
Association for Computational Linguistics.Klaus Krippendorff.
2004.
Content Analysis: AnIntroduction to Its Methodology (second edition).Sage Publications.J.
Richard Landis and Gary G. Koch.
1977.
TheMeasurement of Observer Agreement for Categor-ical Data.
Biometrics, 33(1):159?174.James Pustejovsky, Patrick Hanks, Roser Sauri, A. See,Robert Gaizauskas, Andrea Setzer, Dragomir Radev,Beth Sundheim, D. Day, Lisa Ferro, and MarciaLazo.
2003.
The TIMEBANK Corpus.
In Pro-ceedings of Corpus Linguistics 2003, pages 647?656, Lancaster, UK.James Pustejovsky.
1991.
The Syntax of Event Struc-ture.
Cognition 41 (1991), pages 47?81.Roser Saur?
?, Jessica Littman, Robert Gaizauskas, An-drea Setzer, and James Pustejovsky.
2004.
TimeMLAnnotation Guidelines, Version 1.2.1.Andrea Setzer.
2001.
Temporal Information inNewswire Articles: An Annotation Scheme and Cor-pus Study.
Ph.D. thesis, University of Sheffield,Sheffield, UK.Mihai Surdeanu.
2013.
Overview of the TAC 2013Knowledge Base Population Evaluation: EnglishSlot Filling and Temporal Slot Filling.
In Proceed-ings of the TAC-KBP 2013 Workshop, Gaithersburg,Maryland, USA.Naushad UzZaman, Hector Llorens, Leon Derczynski,Marc Verhagen, James F. Allen, and James Puste-jovsky.
2013.
SemEval-2013 Task 1: TempEval-3:Evaluating Time Expressions, Events, and TemporalRelations.
In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation (SemEval 2013),pages 1?9, Atlanta, Gerogia, USA.Marieke van Erp, Piek Vossen, Rodrigo Agerri, Anne-Lyse Minard, Manuela Speranza, Ruben Urizar,Egoitz Laparra, Itziar Aldabe, and German Rigau.2015.
Annotated Data, version 2.
Technical report,Amsterdam, Netherlands.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
SemEval-2007 Task 15: TempEval TemporalRelation Identification.
In Proceedings of the 4th In-ternational Workshop on Semantic Evaluations, Se-mEval ?07, pages 75?80, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Marc Verhagen, Roser Saur?
?, Tommaso Caselli, andJames Pustejovsky.
2010.
SemEval-2010 Task 13:TempEval-2.
In Proceedings of the 5th Interna-tional Workshop on Semantic Evaluation, SemEval?10, pages 57?62, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Seid Muhie Yimam, Iryna Gurevych, RichardEckart de Castilho, and Chris Biemann.
2013.WebAnno: A Flexible, Web-based and VisuallySupported System for Distributed Annotations.In Proceedings of the 51st Annual Meeting ofthe Association for Computational Linguistics:System Demonstrations, pages 1?6, Sofia, Bulgaria,August.
Association for Computational Linguistics.2204
