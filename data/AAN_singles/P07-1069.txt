Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 544?551,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsGenerating a Table-of-ContentsS.R.K.
Branavan, Pawan Deshpande and Regina BarzilayMassachusetts Institute of Technology{branavan, pawand, regina}@csail.mit.eduAbstractThis paper presents a method for the auto-matic generation of a table-of-contents.
Thistype of summary could serve as an effec-tive navigation tool for accessing informa-tion in long texts, such as books.
To gen-erate a coherent table-of-contents, we needto capture both global dependencies acrossdifferent titles in the table and local con-straints within sections.
Our algorithm ef-fectively handles these complex dependen-cies by factoring the model into local andglobal components, and incrementally con-structing the model?s output.
The results ofautomatic evaluation and manual assessmentconfirm the benefits of this design: our sys-tem is consistently ranked higher than non-hierarchical baselines.1 IntroductionCurrent research in summarization focuses on pro-cessing short articles, primarily in the news domain.While in practice the existing summarization meth-ods are not limited to this material, they are notuniversal: texts in many domains and genres can-not be summarized using these techniques.
A par-ticularly significant challenge is the summarizationof longer texts, such as books.
The requirementfor high compression rates and the increased needfor the preservation of contextual dependencies be-tween summary sentences places summarization ofsuch texts beyond the scope of current methods.In this paper, we investigate the automatic gener-ation of tables-of-contents, a type of indicative sum-mary particularly suited for accessing information inlong texts.
A typical table-of-contents lists topicsdescribed in the source text and provides informa-tion about their location in the text.
The hierarchicalorganization of information in the table further re-fines information access by specifying the relationsbetween different topics and providing rich contex-tual information during browsing.
Commonly foundin books, tables-of-contents can also facilitate accessto other types of texts.
For instance, this type ofsummary could serve as an effective navigation toolfor understanding a long, unstructured transcript foran academic lecture or a meeting.Given a text, our goal is to generate a tree whereina node represents a segment of text and a title thatsummarizes its content.
This process involves twotasks: the hierarchical segmentation of the text, andthe generation of informative titles for each segment.The first task can be addressed by using the hier-archical structure readily available in the text (e.g.,chapters, sections and subsections) or by employ-ing existing topic segmentation algorithms (Hearst,1994).
In this paper, we take the former approach.As for the second task, a naive approach would be toemploy existing methods of title generation to eachsegment, and combine the results into a tree struc-ture.However, the latter approach cannot guaranteethat the generated table-of-contents forms a coher-ent representation of the entire text.
Since titles ofdifferent segments are generated in isolation, someof the generated titles may be repetitive.
Even non-repetitive titles may not provide sufficient informa-tion to discriminate between the content of one seg-544Scientific computingRemarkable recursive algorithm for multiplying matricesDivide and conquer algorithm designMaking a recursive algorithmSolving systems of linear equationsComputing an LUP decompositionForward and back substitutionSymmetric positive definite matrices and least squares approximationFigure 1: A fragment of a table-of-contents generated by our method.ment and another.
Therefore, it is essential to gen-erate an entire table-of-contents tree in a concertedfashion.This paper presents a hierarchical discriminativeapproach for table-of-contents generation.
Figure 1shows a fragment of a table-of-contents automat-ically generated by this algorithm.
Our methodhas two important points of departure from exist-ing techniques.
First, we introduce a structured dis-criminative model for table-of-contents generationthat accounts for a wide range of phrase-based andcollocational features.
The flexibility of this modelresults in improved summary quality.
Second, ourmodel captures both global dependencies across dif-ferent titles in the tree and local dependencies withinsections.
We decompose the model into local andglobal components that handle different classes ofdependencies.
We further reduce the search spacethrough incremental construction of the model?s out-put by considering only the promising parts of thedecision space.We apply our method to process a 1,180 page al-gorithms textbook.
To assess the contribution of ourhierarchical model, we compare our method withstate-of-the-art methods that generate each segmenttitle independently.1 The results of automatic eval-uation and manual assessment of title quality showthat the output of our system is consistently rankedhigher than that of non-hierarchical baselines.2 Related WorkAlthough most current research in summarizationfocuses on newspaper articles, a number of ap-proaches have been developed for processing longertexts.
Most of these approaches are tailored to a par-1The code and feature vector data forour model and the baselines are available athttp://people.csail.mit.edu/branavan/code/toc.ticular domain, such as medical literature or scien-tific articles.
By making strong assumptions aboutthe input structure and the desired format of the out-put, these methods achieve a high compression ratewhile preserving summary coherence.
For instance,Teufel and Moens (2002) summarize scientific arti-cles by selecting rhetorical elements that are com-monly present in scientific abstracts.
Elhadad andMcKeown (2001) generate summaries of medical ar-ticles by following a certain structural template incontent selection and realization.Our work, however, is closer to domain-independent methods for summarizing long texts.Typically, these approaches employ topic segmen-tation to identify a list of topics described in adocument, and then produce a summary for eachpart (Boguraev and Neff, 2000; Angheluta et al,2002).
In contrast to our method, these approachesperform either sentence or phrase extraction, ratherthan summary generation.
Moreover, extraction foreach segment is performed in isolation, and globalconstraints on the summary are not enforced.Finally, our work is also related to research on ti-tle generation (Banko et al, 2000; Jin and Haupt-mann, 2001; Dorr et al, 2003).
Since work in thisarea focuses on generating titles for one article at atime (e.g., newspaper reports), the issue of hierarchi-cal generation, which is unique to our task, does notarise.
However, this is not the only novel aspect ofthe proposed approach.
Our model learns title gener-ation in a fully discriminative framework, in contrastto the commonly used noisy-channel model.
Thus,instead of independently modeling the selection andgrammaticality constraints, we learn both types offeatures in a single framework.
This joint trainingregime supports greater flexibility in modeling fea-ture interaction.5453 Problem FormulationWe formalize the problem of table-of-contents gen-eration as a supervised learning task where the goalis to map a tree of text segments S to a tree of titlesT .
A segment may correspond to a chapter, sectionor subsection.Since the focus of our work is on the generationaspect of table-of-contents construction, we assumethat the hierarchical segmentation of a text is pro-vided in the input.
This division can either be au-tomatically computed using one of the many avail-able text segmentation algorithms (Hearst, 1994), orit can be based on demarcations already present inthe input (e.g., paragraph markers).During training, the algorithm is provided with aset of pairs (Si, T i) for i = 1, .
.
.
, p, where Si isthe ith tree of text segments, and T i is the table-of-contents for that tree.
During testing, the algorithmgenerates tables-of-contents for unseen trees of textsegments.We also assume that during testing the desiredtitle length is provided as a parameter to the algo-rithm.4 AlgorithmTo generate a coherent table-of-contents, we needto take into account multiple constraints: the titlesshould be grammatical, they should adequately rep-resent the content of their segments, and the table-of-contents as a whole should clearly convey the re-lations between the segments.
Taking a discrimina-tive approach for modeling this task would allow usto achieve this goal: we can easily integrate a rangeof constraints in a flexible manner.
Since the num-ber of possible labels (i.e., tables-of-contents) is pro-hibitively large and the labels themselves exhibit arich internal structure, we employ a structured dis-criminative model that can easily handle complexdependencies.
Our solution relies on two orthogo-nal strategies to balance the tractability and the rich-ness of the model.
First, we factor the model intolocal and global components.
Second, we incremen-tally construct the output of each component usinga search-based discriminative algorithm.
Both ofthese strategies have the effect of intelligently prun-ing the decision space.Our model factorization is driven by the differenttypes of dependencies which are captured by the twocomponents.
The first model is local: for each seg-ment, it generates a list of candidate titles ranked bytheir individual likelihoods.
This model focuses ongrammaticality and word selection constraints, but itdoes not consider relations among different titles inthe table-of-contents.
These latter dependencies arecaptured in the global model that constructs a table-of-contents by selecting titles for each segment fromthe available candidates.
Even after this factoriza-tion, the decision space for each model is large: forthe local model, it is exponential in the length of thesegment title, and for the global model it is exponen-tial in the size of the tree.Therefore, we construct the output for each ofthese models incrementally using beam search.
Thealgorithm maintains the most promising partial out-put structures, which are extended at every itera-tion.
The model incorporates this decoding pro-cedure into the training process, thereby learningmodel parameters best suited for the specific decod-ing algorithm.
Similar models have been success-fully applied in the past to other tasks including pars-ing (Collins and Roark, 2004), chunking (Daume?and Marcu, 2005), and machine translation (Cowanet al, 2006).4.1 Model StructureThe model takes as input a tree of text segments S.Each segment s ?
S and its title z are representedas a local feature vector ?loc(s, z).
Each compo-nent of this vector stores a numerical value.
Thisfeature vector can track any feature of the segment stogether with its title z.
For instance, the ith compo-nent of this vector may indicate whether the bigram(z[j]z[j+ 1]) occurs in s, where z[j] is the jth wordin z:(?loc(s, z))i ={1 if (z[j]z[j + 1]) ?
s0 otherwiseIn addition, our model captures dependenciesamong multiple titles that appear in the same table-of-contents.
We represent a tree of segments Spaired with titles T with the global feature vector?glob(S, T ).
The components here are also numer-ical features.
For example, the ith component of thevector may indicate whether a title is repeated in thetable-of-contents T :546(?glob(S, T ))i ={1 repeated title0 otherwiseOur model constructs a table-of-contents in twobasic steps:Step One The goal of this step is to generate alist of k candidate titles for each segment s ?
S.To do so, for each possible title z, the model mapsthe feature vector ?loc(s, z) to a real number.
Thismapping can take the form of a linear model,?loc(s, z) ?
?locwhere ?loc is the local parameter vector.Since the number of possible titles is exponen-tial, we cannot consider all of them.
Instead, weprune the decision space by incrementally construct-ing promising titles.
At each iteration j, the algo-rithm maintains a beam Q of the top k partially gen-erated titles of length j.
During iteration j + 1, anew set of candidates is grown by appending a wordfrom s to the right of each member of the beam Q.We then sort the entries in Q: z1, z2, .
.
.
such that?loc(s, zi) ?
?loc ?
?loc(s, zi+1) ?
?loc, ?i.
Only thetop k candidates are retained, forming the beam forthe next iteration.
This process continues until a titleof the desired length is generated.
Finally, the list ofk candidates is returned.Step Two Given a set of candidate titlesz1, z2, .
.
.
, zk for each segment s ?
S, our goal isto construct a table-of-contents T by selecting themost appropriate title from each segment?s candi-date list.
To do so, our model computes a score forthe pair (S, T ) based on the global feature vector?glob(S, T ):?glob(S, T ) ?
?globwhere ?glob is the global parameter vector.As with the local model (step one), the num-ber of possible tables-of-contents is too large to beconsidered exhaustively.
Therefore, we incremen-tally construct a table-of-contents by traversing thetree of segments in a pre-order walk (i.e., the or-der in which segments appear in the text).
In thiscase, the beam contains partially generated tables-of-contents, which are expanded by one segment ti-tle at a time.
To further reduce the search space,during decoding only the top five candidate titles fora segment are given to the global model.4.2 Training the ModelTraining for Step One We now describe how thelocal parameter vector ?loc is estimated from train-ing data.
We are given a set of training examples(si, yi) for i = 1, .
.
.
, l, where si is the ith text seg-ment, and yi is the title of this segment.This linear model is learned using a variant ofthe incremental perceptron algorithm (Collins andRoark, 2004; Daume?
and Marcu, 2005).
This on-line algorithm traverses the training set multipletimes, updating the parameter vector ?loc after eachtraining example in case of mis-predictions.
The al-gorithm encourages a setting of the parameter vector?loc that assigns the highest score to the feature vec-tor associated with the correct title.The pseudo-code of the algorithm is shown in Fig-ure 2.
Given a text segment s and the correspondingtitle y, the training algorithm maintains a beam Qcontaining the top k partial titles of length j. Thebeam is updated on each iteration using the func-tions GROW and PRUNE.
For every word in seg-ment s and for every partial title in Q, GROW cre-ates a new title by appending this word to the title.PRUNE retains only the top ranked candidates basedon the scoring function ?loc(s, z) ??loc.
If y[1 .
.
.
j](i.e., the prefix of y of length j) is not in the modi-fied beam Q, then ?loc is updated2 as shown in line4 of the pseudo-code in Figure 2.
In addition, Q isreplaced with a beam containing only y[1 .
.
.
j] (line5).
This process is performed |y| times.
We repeatthis process for all training examples over 50 train-ing iterations.
3Training for Step Two To train the global param-eter vector ?glob, we are given training examples(Si, T i) for i = 1, .
.
.
, p, where Si is the ith tree oftext segments, and T i is the table-of-contents for thattree.
However, we cannot directly use these tables-of-contents for training our global model: since thismodel selects one of the candidate titles zi1, .
.
.
, zikreturned by the local model, the true title of the seg-ment may not be among these candidates.
There-fore, to determine a new target title for the segment,we need to identify the title in the set of candidates2If the word in the jth position of y does not occur in s, thenthe parameter update is not performed.3For decoding, ?loc is averaged over the training iterationsas in Collins and Roark (2004).547s ?
segment text.y ?
segment title.y[1 .
.
.
j] ?
prefix of y of length j.Q ?
beam containing partial titles.1.
for j = 1 .
.
.
|y|2.
Q = PRUNE(GROW(s,Q))3. if y[1 .
.
.
j] /?
Q4.
?loc = ?loc + ?loc(s, y[1 .
.
.
j])??z?Q?loc(s,z)|Q|5.
Q = {y[1 .
.
.
j]}Figure 2: The training algorithm for the local model.that is closest to the true title.We employ the L1 distance measure to comparethe content word overlap between two titles.4 Foreach input (S, T ), and each segment s ?
S, we iden-tify the segment title closest in the L1 measure to thetrue title y5:z?
= arg miniL1(zi, y)Once all the training targets in the corpus havebeen identified through this procedure, the globallinear model ?glob(S, T ) ?
?glob is learned using thesame perceptron algorithm as in step one.
Ratherthan maintaining the beam of partially generated ti-tles, the beam Q holds partially generated tables-of-contents.
Also, the loop in line 1 of Figure 2 iteratesover segment titles rather than words.
The globalmodel is trained over 200 iterations.5 FeaturesLocal Features Our local model aims to generatetitles which adequately represent the meaning of thesegment and are grammatical.
Selection and contex-tual preferences are encoded in the local features.The features that capture selection constraints arespecified at the word level, and contextual featuresare expressed at the word sequence level.The selection features capture the position of theword, its TF*IDF, and part-of-speech information.In addition, they also record whether the word oc-curs in the body of neighboring segments.
We also4This measure is close to ROUGE-1 which in addition con-siders the overlap in auxiliary words.5In the case of ties, one of the titles is picked arbitrarily.Segment has the same title as its siblingSegment has the same title as its parentTwo adjacent sibling titles have the same headTwo adjacent sibling titles start with the same wordRank given to the title by the local modelTable 1: Examples of global features.generate conjunctive features by combining featuresof different types.The contextual features record the bigram and tri-gram language model scores, both for words and forpart-of-speech tags.
The trigram scores are aver-aged over the title.
The language models are trainedusing the SRILM toolkit.
Another type of contex-tual feature models the collocational properties ofnoun phrases in the title.
This feature aims to elim-inate generic phrases, such as ?the following sec-tion?
from the generated titles.6 To achieve this ef-fect, for each noun phrase in the title, we measurethe ratio of their frequency in the segment to theirfrequency in the corpus.Global Features Our global model describes theinteraction between different titles in the tree (SeeTable 1).
These interactions are encoded in threetypes of global features.
The first type of globalfeature indicates whether titles in the tree are re-dundant at various levels of the tree structure.
Thesecond type of feature encourages parallel construc-tions within the same tree.
For instance, titles of ad-joining segments may be verbalized as noun phraseswith the same head (e.g., ?Bubble sort algorithm?,?Merge sort algorithm?).
We capture this propertyby comparing words that appear in certain positionsin adjacent sibling titles.
Finally, our global modelalso uses the rank of the title provided by the localmodel.
This feature enables the global model to ac-count for the preferences of the local model in thetitle selection process.6 Evaluation Set-UpData We apply our method to an undergraduate al-gorithms textbook.
For detailed statistics on the datasee Table 2.
We split its table-of-contents into a set6Unfortunately, we could not use more sophisticated syntac-tic features due to the low accuracy of statistical parsers on ourcorpus.548Number of Titles 540Number of Trees 39Tree Depth 4Number of Words 269,650Avg.
Title Length 3.64Avg.
Branching 3.29Avg.
Title Duplicates 21Table 2: Statistics on the corpus used in the experi-ments.of independent subtrees.
Given a table-of-contentsof depth n with a root branching factor of r, we gen-erate r subtrees, with a depth of at most n ?
1.
Werandomly select 80% of these trees for training, andthe rest are used for testing.
In our experiments, weuse ten different randomizations to compensate forthe small number of available trees.Admittedly, this method of generating trainingand testing data omits some dependencies at thelevel of the table-of-contents as a whole.
However,the subtrees used in our experiments still exhibita sufficiently deep hierarchical structure, rich withcontextual dependencies.Baselines As an alternative to our hierarchical dis-criminative method, we consider three baselines thatbuild a table-of-contents by generating a title foreach segment individually, without taking into ac-count the tree structure, and one hierarchical gener-ative baseline.
The first method generates a title for asegment by selecting the noun phrase from that seg-ment with the highest TF*IDF.
This simple methodis commonly used to generate keywords for brows-ing applications in information retrieval, and hasbeen shown to be effective for summarizing techni-cal content (Wacholder et al, 2001).The second baseline is based on the noisy-channelgenerative (flat generative, FG) model proposed byBanko et al, (2000).
Similar to our local model,this method captures both selection and grammati-cal constraints.
However, these constraints are mod-eled separately, and then combined in a generativeframework.We use our local model (Flat Discriminativemodel, FD) as the third baseline.
Like the secondbaseline, this model omits global dependencies, andonly focuses on features that capture relations withinindividual segments.In the hierarchical generative (HG) baseline werun our global model on the ranked list of titles pro-duced for each section by the noisy-channel genera-tive model.The last three baselines and our algorithm are pro-vided with the title length as a parameter.
In ourexperiments, the algorithms use the reference titlelength.Experimental Design: Comparison with refer-ence tables-of-contents Reference based evalu-ation is commonly used to assess the quality ofmachine-generated headlines (Wang et al, 2005).We compare our system?s output with the table-of-contents from the textbook using ROUGE metrics.We employ a publicly available software package,7with all the parameters set to default values.Experimental Design: Human assessment Thejudges were each given 30 segments randomly se-lected from a set of 359 test segments.
For each testsegment, the judges were presented with its text, and3 alternative titles consisting of the reference andthe titles produced by the hierarchical discriminativemodel, and the best performing baseline.
In addi-tion, the judges had access to all of the segments inthe book.
A total of 498 titles for 166 unique seg-ments were ranked.
The system identities were hid-den from the judges, and the titles were presented inrandom order.
The judges ranked the titles based onhow well they represent the content of the segment.Titles were ranked equal if they were judged to beequally representative of the segment.Six people participated in this experiment.
All theparticipants were graduate students in computer sci-ence who had taken the algorithms class in the pastand were reasonably familiar with the material.7 ResultsFigure 3 shows fragments of the tables-of-contentsgenerated by our method and the four baselinesalong with the reference counterpart.
These extractsillustrate three general phenomena that we observedin the test corpus.
First, the titles produced by key-word extraction exhibit a high degree of redundancy.In fact, 40% of the titles produced by this method arerepeated more than once in the table-of-contents.
In7http://www.isi.edu/licensed-sw/see/rouge/549Reference:hash tablesdirect address tableshash tablescollision resolution by chaininganalysis of hashing with chainingopen addressinglinear probingquadratic probingdouble hashingFlat Generative:linked listworst case timewasted spaceworst case running timeto show that there aredynamic setoccupied slotquadratic functiondouble hashingFlat Discriminative:dictionary operationsuniverse of keyscomputer memoryelement in the listhash table with load factorhash tablehash functionhash functiondouble hashingKeyword Extraction:hash tabledynamic sethash functionworst caseexpected numberhash tablehash functionhash tabledouble hashingHierarchical Generative:dictionary operationsworst case timewasted spaceworst case running timeto show that there arecollision resolutionlinear timequadratic functiondouble hashingHierarchical Discriminative:dictionary operationsdirect address tablecomputer memoryworst case running timehash table with load factoraddress tablehash functionquadratic probingdouble hashingFigure 3: Fragments of tables-of-contents generated by our method and the four baselines along with thecorresponding reference.Rouge-1 Rouge-L Rouge-W Full MatchHD 0.256 0.249 0.216 13.5FD 0.241 0.234 0.203 13.1HG 0.139 0.133 0.117 5.8FG 0.094 0.090 0.079 4.1Keyword 0.168 0.168 0.157 6.3Table 3: Title quality as compared to the referencefor the hierarchical discriminative (HD), flat dis-criminative (FD), hierarchical generative (HG), flatgenerative (FG) and Keyword models.
The improve-ment given by HD over FD in all three Rouge mea-sures is significant at p ?
0.03 based on the Signtest.better worse equalHD vs. FD 68 32 49Reference vs. HD 115 13 22Reference vs. FD 123 7 20Table 4: Overall pairwise comparisons of the rank-ings given by the judges.
The improvement in ti-tle quality given by HD over FD is significant atp ?
0.0002 based on the Sign test.contrast, our method yields 5.5% of the titles as du-plicates, as compared to 9% in the reference table-of-contents.8Second, the fragments show that the two discrim-inative models ?
Flat and Hierarchical ?
have anumber of common titles.
However, adding globaldependencies to rerank titles generated by the localmodel changes 30% of the titles in the test set.Comparison with reference tables-of-contentsTable 3 shows the average ROUGE scores overthe ten randomizations for the five automatic meth-ods.
The hierarchical discriminative method consis-tently outperforms the four baselines according toall ROUGE metrics.At the same time, these results also show that onlya small ratio of the automatically generated titlesare identical to the reference ones.
In some cases,the machine-generated titles are very close in mean-ing to the reference, but are verbalized differently.Examples include pairs such as (?Minimum Span-ning Trees?, ?Spanning Tree Problem?)
and (?Wal-lace Tree?, ?Multiplication Circuit?
).9 While mea-sures like ROUGE can capture the similarity in thefirst pair, they cannot identify semantic proximity8Titles such as ?Analysis?
and ?Chapter Outline?
are re-peated multiple times in the text.9A Wallace Tree is a circuit that multiplies two integers.550between the titles in the second pair.
Therefore,we supplement the results of this experiment witha manual assessment of title quality as described be-low.Human assessment We analyze the human rat-ings by considering pairwise comparisons betweenthe models.
Given two models, A and B, three out-comes are possible: A is better than B, B is bet-ter than A, or they are of equal quality.
The re-sults of the comparison are summarized in Table 4.These results indicate that using hierarchical infor-mation yields statistically significant improvement(at p ?
0.0002 based on the Sign test) over a flatcounterpart.8 Conclusion and Future WorkThis paper presents a method for the automatic gen-eration of a table-of-contents.
The key strength ofour method lies in its ability to track dependenciesbetween generation decisions across different levelsof the tree structure.
The results of automatic evalu-ation and manual assessment confirm the benefits ofjoint tree learning: our system is consistently rankedhigher than non-hierarchical baselines.We also plan to expand our method for the taskof slide generation.
Like tables-of-contents, slidebullets are organized in a hierarchical fashion andare written in relatively short phrases.
From thelanguage viewpoint, however, slides exhibit morevariability and complexity than a typical table-of-contents.
To address this challenge, we will exploremore powerful generation methods that take into ac-count syntactic information.AcknowledgmentsThe authors acknowledge the support of the Na-tional Science Foundation (CAREER grant IIS-0448168 and grant IIS-0415865).
We would alsolike to acknowledge the many people who took partin human evaluations.
Thanks to Michael Collins,Benjamin Snyder, Igor Malioutov, Jacob Eisenstein,Luke Zettlemoyer, Terry Koo, Erdong Chen, Zo-ran Dzunic and the anonymous reviewers for helpfulcomments and suggestions.
Any opinions, findings,conclusions or recommendations expressed aboveare those of the authors and do not necessarily re-flect the views of the NSF.ReferencesRoxana Angheluta, Rik De Busser, and Marie-FrancineMoens.
2002.
The use of topic segmentation for auto-matic summarization.
In Proceedings of the ACL-2002Workshop on Automatic Summarization.Michele Banko, Vibhu O. Mittal, and Michael J. Wit-brock.
2000.
Headline generation based on statisticaltranslation.
In Proceedings of the ACL, pages 318?325.Branimir Boguraev and Mary S. Neff.
2000.
Discoursesegmentation in aid of document summarization.
InProceedings of the 33rd Hawaii International Confer-ence on System Sciences, pages 3004?3014.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceedingsof the ACL, pages 111?118.Brooke Cowan, Ivona Kucerova, and Michael Collins.2006.
A discriminative model for tree-to-tree trans-lation.
In Proceedings of the EMNLP, pages 232?241.Hal Daume?
and Daniel Marcu.
2005.
Learning as searchoptimization: Approximate large margin methods forstructured prediction.
In Proceedings of the ICML,pages 169?176.Bonnie Dorr, David Zajic, and Richard Schwartz.
2003.Hedge trimmer: a parse-and-trim approach to headlinegeneration.
In Proceedings of the HLT-NAACL 03 onText summarization workshop, pages 1?8.Noemie Elhadad and Kathleen R. McKeown.
2001.
To-wards generating patient specific summaries of med-ical articles.
In Proceedings of NAACL Workshop onAutomatic Summarization, pages 31?39.Marti Hearst.
1994.
Multi-paragraph segmentation ofexpository text.
In Proceedings of the ACL, pages 9?16.Rong Jin and Alexander G. Hauptmann.
2001.
Auto-matic title generation for spoken broadcast news.
InProceedings of the HLT, pages 1?3.Simone Teufel and Marc Moens.
2002.
Summariz-ing scientific articles: Experiments with relevanceand rhetorical status.
Computational Linguistics,28(4):409?445.Nina Wacholder, David K. Evans, and Judith Klavans.2001.
Automatic identification and organization of in-dex terms for interactive browsing.
In JCDL, pages126?134.R.
Wang, J. Dunnion, and J. Carthy.
2005.
Machinelearning approach to augmenting news headline gen-eration.
In Proceedings of the IJCNLP.551
