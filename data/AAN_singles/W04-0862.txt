The Swarthmore College SENSEVAL3 SystemRichard Wicentowski, Emily Thomforde and Adrian PackelComputer Science DepartmentSwarthmore CollegeSwarthmore, Pennsylvania, 19081, USA{richardw,ethomfo1,packel}@cs.swarthmore.eduAbstractThis paper presents the Swarthmore College word-sense disambiguation system which was designedfor the 2004 SENSEVAL3 competition.
Our systemparticipated in five tasks: the lexical sample tasks inBasque, Catalan, Italian, Romanian, and Spanish.For each task, a suite of supervised algorithms werecombined using voting to form the final system.1 IntroductionThe Swarthmore College system consisted of threesupervised classifiers which were used to performlexical ambiguity resolution in five languages.
Anearest-neighbor clustering classifier, a na?
?ve Bayesclassifier, and a decision list classifier were eachtrained on several permutations of the extracted fea-ture set, then the answers were joined using voting.The training data was limited to the labeled dataprovided by the organizers; no outside or unlabeleddata was used.The systems presented in this paper were devel-oped by undergraduates as part of a class project atSwarthmore College.2 FeaturesEach of the supervised algorithms made use of thesame set of features, extracted from only the labeleddata provided to us by the task organizers.
We usedno unlabeled data.
We used the tagged and lemma-tized data to extract the following features, whichwere the only features used in our system:?
Bag-of-words and bag-of-lemmas?
Bigrams and trigrams of words, lemmas, part-of-speech, and case (Basque-only) around thetarget word?
Topic or code (Basque, Catalan and Spanish)In order to prevent individual features from dom-inating any individual system, we used up to eightpermutations of the above mentioned features (de-pending on the language) for each of our classifiers.Catalan and Spanish provided fine-grained part-of-speech tags which we felt would lead to sparsedata problems.
To reduce this problem, for somefeature sets we made the part-of-speech tags morecoarse by simplifying the tags to include only thefirst or first two letters of the tag.3 SystemsThe following systems were used to complete theBasque, Catalan, Italian and Romanian lexical sam-ple tasks.
The Spanish lexical sample task was com-pleted before the other four tasks were begun andused only a subset of the systems presented below.Full details on the systems and methods used for theSpanish lexical sample task can be found in Sec-tion 7.3.See Section 4 for details on the classifier com-bination, and Section 5.2 for information about ouruse of bagging.3.1 Cosine-based ClusteringThe first system developed was a nearest-neighborclustering method which used the cosine similarityof feature vectors as the distance metric.
A centroidwas created for each attested sense in the trainingdata, and each test sample was assigned to a clusterbased on its similarity to the existing centroid.
Cen-troids were not recalculated after each added test in-stance.3.2 Na?
?ve BayesThe second system used was a na?
?ve Bayes classifierwhere the similarity between an instance, I , and asense class, Sj , is defined as:Sim(I, Sj) = P (I, Sj) = P (Sj)P (I|Sj)We then choose the sense class, Sj , which max-imized the similarity function above, making stan-dard independence assumptions.3.3 Decision ListThe final system was a decision list classifier thatfound the log-likelihoods of the correspondence be-Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemstween features and senses, using plus-one smooth-ing (Yarowsky, 1994).
The features were orderedfrom most to least indicative to form the decisionlist.
A separate decision list was constructed foreach set of lexical samples in the training data.
Foreach test instance, the first matching feature foundin the associated decision list was used to determinethe classification of the instance.
Instances whichfailed to match any rule in the decision list werelabeled with the most frequent sense, as calculatedfrom the training data.4 Classifier CombinationDue to time constraints, we were unable to getcross-validation results for all of the systems we cre-ated, and therefore all of the final classifier combi-nation was done using simple majority vote, break-ing ties arbitrarily.
To reach a consensus vote, wecombined the multiple decision list systems, whichhad been run on each of the different subsets of ex-tracted features, into a single system.
We then didthe same for the clustering system and the na?
?veBayes system, yielding a total of three new sys-tems.
These three systems were then voted togetherto form the final system.
The two-tiered voting wasperformed to ensure equal voting in the case of ourjoint work (Wicentowski et al, 2004) where the fivesystems that needed to be combined were run ondifferent numbers of feature subsets.4.1 Combination ErrorsThere were two mistakes we made when voting oursystems together.
We caught one mistake after thesubmission deadline but before notification of re-sults; the other we realized only while evaluatingour systems after receiving our results.
For this rea-son, there are three sets of results that we will reporthere:?
Official results: The results from our submis-sion to SENSEVAL3.?
Unofficial results: Includes a bug-fix found be-fore notification of competition results.?
Tie-breaking results: Includes a bug-fix foundafter notification of results.In doing the evaluation of our system for this pa-per, we will use the unofficial results1.
Becauseof the nature of the bug-fix, evaluating our systembased on the official results will yield less informa-tive results than an evaluation of results after fixing1As mentioned previously, Spanish is a special case and wewill report only our official results.the error.
Since these unofficial results were ob-tained before notification of results from the com-petition organizers, we believe this to be a fair com-parison.4.1.1 Over-weighting part-of-speech n-gramsThe bug which yielded our unofficial results oc-curred when we combined the multiple decision listsystems into a single decision list system (and sim-ilarly for the multiple clustering and na?
?ve Bayessystems).
As discussed in Section 2, we experi-mented with forming partial labels for the part-of-speech tags to reduce the sparse-data issues: usingthe full part-of-speech tag, using only the first let-ter of the tag, and using the first two letters of thetag.
However, in the final combination, we endedup including all three methods in the voting, in-stead of including only one.
Obviously, these threeclassifiers, based solely on part-of-speech n-gramsaround the target word, had a high rate of agree-ment and were therefore over-weighted in the finalvoting.
Our systems underperformed where theyshould have, with the surprising exception of Cata-lan, which performed better with the mistake thanwithout it.
Table 1 compares our official results withour unofficial results.Language official unofficial changeBasque 64.6% 66.6% 2.0%Catalan 79.7% 79.5% (-0.2%)Italian 46.5% 49.6% 3.1%Romanian 70.1% 71.3% 1.2%Spanish 79.5% ?
?Table 1: Final results, officially and unofficially,from making a bug-fix before notification of results,but after the submission deadline.4.1.2 Voting without weightingOur classifier combination used a non-informedmethod for breaking ties: whichever sense hadthe first hash code (as determined by Perl?s hashfunction) was chosen.
Our inability to completecross-validation experiments led us to not favor anyone classifier over another.
Performance wouldhave been improved by using an ad-hoc weightingscheme which took into account the following intu-itions:?
Initial experiments indicated that the instancesof the classifiers with access to the full set offeatures would outperform the instances run-ning on limited subsets of the features.?
Empirical evidence suggested that the deci-sion list classifier was the best, the clusteringmethod a strong second, and the na?
?ve Bayesmethod a distant third.In fairness, we did not discover this mistake un-til we were preparing this paper, only after receiv-ing notification of our results.
While we report ourrevised results, we make no further comparisonsbased on these results.
In addition, we ran no ex-tra experiments to determine the weighting schemelisted below, we simply used our intuition based onour earlier experimentation as noted above.
Theseintuitions were not always correct, as indicated inTable 5 and Table 6.Using very simple ad-hoc weights which back upthese intuitions, we changed our classifier combina-tion system to break ties according to the followingscheme: In the first tier of voting, we fractionallyincreased the weight given to the classifiers run onthe full-feature set: instead of each system receiv-ing exactly one vote, we gave those systems an extra110 th of a vote.
In the second tier of voting, we madethe same fractional increase to the weight given tothe decision list classifier.
Use of this tie breakingscheme increases our results impressively, as shownbelow in Table 2.Language official tie-breaking net gainBasque 64.6% 68.2% 3.6%Catalan 79.7% 81.0% 1.3%Italian 46.5% 52.4% 5.9%Romanian 70.1% 73.2% 3.1%Table 2: Using simple tie-breakers in voting.
Thesecond column also includes the bug fix describedin ?4.1.1.
Note that the tie-breaking error was foundafter notification of our final results.5 Additional features5.1 Collocational SensesIn the Basque and Romanian tasks, senses couldbe labeled either as numbered alternatives or asa collocational sense.
For example, the Basqueword astun could be labeled with the collocationalsense pisu astun.From the SENSEVAL2 English lexical-sampletask, we found there were 175 words labeled with acollocational sense.
A lemmatized form of the col-location was found in 96.6% of these when consid-ering a ?2-word window around the target.
To takeadvantage of this expected behavior in Basque andRomanian, we labeled a target word with a colloca-tional sense if we found the lemmatized collocationin a ?2-word window.
In Romanian, many collo-cations contained prepositions or other stop-words;therefore, we labeled a target word with the colloca-tional sense only if a non-stop-word from the collo-cation was found in the ?2-word window.
Overall,as shown in Table 3, this decision proved to be rea-sonably effective.Language Correct Answered PrecisionRomanian 161 190 84.7%Basque 70 79 88.6%Table 3: Precision on likely collocational senses.Complementary to this issue, a sampling of thesame English data indicated that if a target word waspart of a previously seen collocation, it was highlyunlikely that this word would not be tagged withthe collocational sense.
Therefore, we expectedit would be advantageous if we could remove thecollocational senses from the training data to pre-vent target words which were not part of colloca-tions from being tagged as such.
Based on cross-validated results, we found that this was worthwhilefor Basque, but not for Romanian, where there weremany examples of a target word being tagged ascollocational sense without the collocation beingpresent.5.2 BaggingFor the decision list and clustering systems, we usedbagging (Breiman, 1996) to train on five randomlysampled instances of the training data which werecombined using a simple majority vote.
We limitedourselves to five samples due to time limitations im-posed by the competition.
We found a consistent,but minimal, improvement for each of the four tasksdue to our use of bagging, as shown below in Ta-ble 4.Language no bagging bagging net gainBasque 66.0% 66.6% 0.6%Catalan 79.4% 79.5% 0.1%Italian 48.6% 49.6% 1.0%Romanian 70.9% 71.3% 0.4%Table 4: Overall impact of using bagging.6 EvaluationAs previously discussed, we used a combination ofthree supervised classifiers, each run on a differentsubset of the features.
Here we report the perfor-mance of each of the individual classifiers, as wellas the features we found to be most indicative of thecorrect sense.6.1 Indicative featuresAs discussed in Section 2, we did not use any exter-nal data for the lexical sample tasks, but we did tryto use all of the features that were available in thetraining and test sets.
In order to show the effective-ness of each of the features we used, we present thefollowing sample taken from running our decisionlist system in Basque, Catalan, Italian and Roma-nian using only one feature at a time.Basque Feature Catalan55.8% mfs baseline 55.0%64.6% all features 80.6%52.9% case n-grams -54.2% simplified pos n-grams 74.9%59.2% topic/code tag 70.5%54.1% part-of-speech n-grams 77.5%61.7% docsrc tag 69.7%61.4% bag of words 78.7%61.3% bag of ?forms?
79.7%62.6% bag of lemmas 78.7%65.0% word n-grams 81.7%66.1% lemma n-grams 81.7%Italian Feature Romanian27.7% mfs baseline 58.4%50.3% all features 70.9%38.4% simplified pos n-grams 63.7%38.6% part-of-speech n-grams 64.2%41.0% bag of words 64.7%41.1% bag of ?forms?
-41.1% bag of lemmas 64.8%44.4% word n-grams 70.0%46.5% lemma n-grams 69.4%Table 5: Accuracy of the decision list system usingeach of the available features individually.
All ofthe above features, except ?docsrc?
were used in thefinal system.
The features are ordered from least tomost informative across the four languages.With the exception of Romanian, the bigrams andtrigrams comprised of the lemmas were the most in-formative single feature for the decision list system.Surprisingly, in both Catalan and Basque, the de-cision list system trained only on lemma n-gramsoutperformed decision list system which used all ofthe features.Because the lemmas were so important, we sus-pect that omitting them in future data sets will favorthose systems which can incorporate accurate lem-matizers.
Since real world applications will requiresuch lemmatizers, we are in favor of omitting thesein future competitions.6.2 ClassifiersAs shown in Table 6, the decision list systemwas the best single system; however, the nearest-neighbor clustering system outperformed decisionlists in Basque.
Each of the supervised systems iscompared against the baseline most-frequent-senseclassifier (as computed from the training data).Language MFS NB NNC DLBasque 55.8% 60.4% 66.0% 64.6%Catalan 55.0% 71.3% 77.5% 80.6%Italian 27.7% 42.1% 44.9% 50.3%Romanian 58.4% 62.8% 67.9% 70.9%Table 6: Accuracies for each of the classifiers: MostFrequent Sense, Na?
?ve Bayes, Nearest-NeighborClustering, and Decision Lists.7 Task-specific Details7.1 BasqueThe Basque data contained the largest number ofavailable features, but in places, the features wereincomplete (case markers) or required additionalsteps to extract.
Most notably, though lemmas wereprovided, the target word was not indicated in ei-ther the training or test data; therefore, we per-formed some simple pre-processing of the Basquedata to isolate the target lemma in the training andtest data As is shown in Table 5, these lemma n-grams around the target word were the most indica-tive features for our decision list system.7.2 RomanianThe Romanian data also provided a large numberof available features, however some pre-processingwas necessary to change the format of the suppliedpart-of-speech tagged data into the format suppliedby the other tasks.7.3 SpanishWe were required to submit our results for the Span-ish lexical sample task before we had completedwriting our system, so the submission includes onlytwo classifiers, a na?
?ve Bayes classifier and a deci-sion list classifier.
We ran our decision list on sevenpermutations of the feature set, and the na?
?ve Bayeson two permutations, for a total of nine systems.These nine systems were joined using a majority-voting scheme.
Relative performance on this task isexpected to be below that of other tasks.8 Collaborative WorkThis paper refers only to the entries completedexclusively by the Swarthmore College team anddiscusses the entries submitted under the label?Swat?.
The ?Swat-HK?
and ?Swat HK-Bo?
en-tries were submitted by Swarthmore College in col-laboration with a joint team from Hong Kong Poly-technic University and Hong Kong University ofScience and Technology.
For these entries, Swarth-more College provided the data, with all of the fea-tures described, to the Hong Kong team.
Their teamthen sent us back two sets of results: the output oftheir maximum entropy system and their boostingsystem.
These two results were then combined withthe three systems written by Swarthmore College.Details on this joint effort can be found in (Wicen-towski et al, 2004).In addition, the decision list system describedhere was used in the Semantic Role Labeling tasksubmitted by (Ngai et al, 2004).9 AcknowledgmentsThe authors thank the following Swarthmore Col-lege students for their assistance and guidance: BenMitchell ?05, Charles Bell ?06, Lisa Spitalewitz ?06,and Michael Stone ?07.
Their efforts as part of theFall 2003 ?Information Retrieval and Natural Lan-guage Processing?
class laid the foundation for oursuccessful entry into the SENSEVAL3 competition.In addition, the authors express their gratitude toGrace Ngai, Dekai Wu, and all the members of theirjoint team, for asking us to participate in their Se-mantic Role Labeling system.Finally, the authors thank the organizers, espe-cially Rada Mihalcea, for their support of our par-ticipation.ReferencesL.
Breiman.
1996.
Bagging predictors.
MachineLearning, 24:123?140.G.
Ngai, D. Wu, M. Carpuat, C.S.
Wang, andC.Y.
Wang.
2004.
Semantic Role Label-ing with Boosting, SVMs, Maximum Entropy,SNOW, and Decision Lists.
In Proceedings ofSENSEVAL-3, Barcelona.R.
Wicentowski, G. Ngai, E. Thomforde, A. Packel,D.
Wu, and M. Carpuat.
2004.
Joining forcesto resolve lexical ambiguity: East meets Westin Barcelona.
In Proceedings of SENSEVAL-3,Barcelona.D.
Yarowsky.
1994.
Decision lists for lexical am-biguity resolution: Application to accent restora-tion in spanish and french.
In Proceedings of the32nd Annual Meeting of the Association for Com-putational Linguistics, pages 88?95.
