Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 153?160, Vancouver, October 2005. c?2005 Association for Computational LinguisticsDiscretization Based Learning Approach to Information RetrievalDmitri Roussinov Weiguo FanDepartment of Information SystemsW.P.
Carey School of BusinessDepartment of Information Systems andComputer ScienceArizona State University Virginia Polytechnic Institute and StateUniversityTempe, AZ, 85287 Blacksburg, VA 24061dmitri.roussinov@asu.edu wfan@vt.eduAbstractWe approached the problem as learninghow to order documents by estimatedrelevance with respect to a user query.
Oursupport vector machines based classifierlearns from the relevance judgmentsavailable with the standard test collectionsand generalizes to new, previously unseenqueries.
For this, we have designed arepresentation scheme, which is based onthe  discrete representation of the local (lw)and global (gw) weighting functions, thusis capable of reproducing and enhancingthe properties of such popular rankingfunctions as tf.idf, BM25 or those based onlanguage models.
Our tests with thestandard test collections have demonstratedthe capability of our approach to achievethe performance of the best known scoringfunctions solely from the labeled examplesand without taking advantage of knowingthose functions or their importantproperties or parameters.1.
IntroductionOur work is motivated by the objective to bringcloser numerous achievements in the domains ofmachine learning and classification to the classicaltask of ad-hoc information retrieval (IR), which isordering documents by the estimated degree ofrelevance to a given query.
Although used withstriking success for text categorization,classification-based approaches (e.g.
those based onsupport vector machines, Joachims, 2001 ) have beenrelatively abandoned when trying to improve ad hocretrieval in favor of empirical (e.g.
vector space,Salton & McGill, 1983) or generative (e.g.
languagemodels; Zhai & Lafferty 2001; Song & Croft; 1999),which produce a ranking function that gives eachdocument a score, rather than trying to learn aclassifier that would help to discriminate betweenrelevant and irrelevant documents and order themaccordingly.
A generative model needs to makeassumptions that the query and document words aresampled from the same underlying distributions andthat the distributions have certain forms, which entailspecific smoothing techniques (e.g.
popularDirichlet-prior).
A discriminative (classifier-based)model, on the other side, does not need to make anyassumptions about the forms of the underlyingdistributions or the criteria for the relevance butinstead, learns to predict to which class a certainpattern (document) belongs to based on the labeledtraining examples.
Thus, an important advantage of adiscriminative approach for the information retrievaltask, is its ability to explicitly utilize the relevancejudgments existing with standard test collections inorder to train the IR algorithms and possibly enhanceretrieval accuracy for the new (unseen) queries.Cohen, Shapire and Singer (1999) noted thedifferences between ordering and classification andpresented a two-stage model to learn ordering.
Thefirst stage learns a classifier for preference relationsbetween objects using any suitable learningmechanism (e.g.
support vector machines; Vapnik,1998).
The second stage converts preferencerelations into a rank order.
Although the conversionmay be NP complete in a general case, theypresented efficient approximations.
We limited ourfirst study reported here to linear classifiers, in whichconversion can be performed by simple orderingaccording to the score of each document.
However,approaching the problem as ?learning how to orderthings?
allowed us to design our sampling andtraining mechanisms in a novel and, we believe,more powerful way.Our classifier learns how to compare every pair ofdocuments with respect to a given query, based onthe relevance indicating features that the documentsmay have.
As it is commonly done in informationretrieval, the features are derived from the wordoverlap between the query and documents.According to Nallapati (2004), the earliestformulation of the classic IR problem as aclassification (discrimination) problem wassuggested by Robertson and Sparck Jones (1976),however performed well only when the relevancejudgments were available for the same query but notgeneralizing well to new queries.
Fuhr and Buckley(1991) used polynomial regression to estimate thecoefficients in a linear scoring function combiningsuch well-known features as a weighted termfrequency, document length and query length.
Theytested their ?description-oriented?
approach on thestandard small-scale collections (Cranfield, NPL,INSPEC, CISI, CACM) to achieve the relativechange in the average precision ranging from -17%153to + 33% depending on the collection tested and theimplementation parameters.
Gey (1994) appliedlogistic regression in a similar setting with thefollowing results: Cranfield +12%, CACM +7.9%,CISI -4.4%, however he did not test them on new(unseen by the algorithm) queries, hypothesizing thatsplitting documents into training and testingcollections would not be possible since ?a largenumber of queries is necessary in order to train for adecent logistic regression approach to documentretrieval.?
Instead, he applied a regression trained onCranfield to CISI collection but with a negativeeffect.Recently, the approaches based on learning havereported several important breakthroughs.
Fan et al(2004) applied genetic programming in order to learnhow to combine various terms into the optimalranking function that outperformed the popularOkapi formula on robust retrieval test collection.Nallapati (2004) made a strong argument in favor ofdiscriminative models and trained an SVM-basedclassifier to combine 6 different components (terms)from the popular ranking functions (such as tf.idfand language models) to achieve better than thelanguage model performance in 2 out of 16 test cases(figure 3 in Nallapati, 2004), not statisticallydistinguishable in 8 cases and only 80% of the bestperformance in 6 cases.
There have been studiesusing past relevance judgements to optimizeretrieval.
For example, Joachims (2002) appliedSupport Vector Machines to learn linear rankingfunction from user click-throughs while interfacingwith a search engine.In this study, we have developed a representationscheme, which is based on the discretization of theglobal (corpus statistics) and local (documentstatistics) weighting of term overlaps betweenqueries and documents.
We have empirically shownthat this representation is flexible enough to learn theproperties of the popular ranking functions: tf.idf,BM25 and the language models.
The majordifference of our work from Fan et al (2004) orNallapati (2004) or works on fusion (e.g.
Vogt &Cottrell, 1999) is that we did not try to combineseveral known ranking functions (or their separateterms) into one, but rather we learn the weightingfunctions directly through discretization.Discretization allows representing a continuousfunction by a set of values at certain points.
Thesevalues are learned by a machine learning techniqueto optimize certain criteria, e.g.
average precision.Another important motivation behind usingdiscretization was to design a representation withhigh dimensionality of features in order to combineour representation scheme with Support VectorMachines (SVM) (Vapnik, 1998), which are knownto work well with a large number of features.
SVMcontains a large class of neural nets, radial marginseparation (RBF) nets, and polynomial classifiers asspecial cases.
They have been delivering superiorperformance in classification tasks in generaldomains, e.g.
in face recognition (Hearst, 1998), andin text categorization (Joachims, 2001).Another important distinction of this work from theprior research is that we train our classifier not topredict the absolute relevance of a document d withrespect to a query q, but rather to predict which ofthe two documents d1, d2 is more relevant to thequery q.
The motivation for this distinction was thatall the popular evaluation metrics in informationretrieval (e.g.
average precision) are based ondocument ranking rather than classificationaccuracy.
This affected our specially designedsampling procedure which we empiricallydiscovered to be crucial for successful learning.We have also empirically established that ourcombination of the representation scheme, learningmechanism and sampling allows learning from thepast relevance judgments in order to successfullygeneralize to the new (unseen) queries.
When therepresentation was created without any knowledge ofthe top ranking functions and their parameters, ourapproach reached the known top performance solelythrough the learning process.
When ourrepresentation was taking advantage of functions thatare known to perform well and their parameters, theresulting combination was able to slightly exceed thetop performance on large test collections.
The nextsection formalizes our Discretization Based Learning(DBL) approach to Information Retrieval, followedby empirical results and conclusions.2.
Formalization Of Our Approach2.1 Query and Document RepresentationWe limit our representation to the so called lw.gwclass: ?
?=q t)()),,(( d)R(q, tGddttfL ,where L, local weighting, is the function of thenumber of occurrences of the term in the documenttf, possibly combined with the other documentstatistics, e.g.
word length.
G(t), global weighting,can be any collection level statistic of the term.
Forexample, in the classical tf.idf formula L(tf, d) = tf /|d|, where tf is the number of occurrences of the termt in the document, |d| is the length of the documentvector and G(t) = log (N / df(t)), where df(t) is thetotal number of documents in the collection that haveterm t and N is the total number of documents.Without loss of generality it may also be extended tohandle a number of occurrences of the term in thequery, but we omit it here in our formalization forsimplicity.
Lw.gw class includes the BM25 Okapiranking function which performs well on TRECcollections (Robertson et al, 1996).
It can be shownthat many of the recently introduced languagemodels fall into that category as well, specifically thebest performing in TREC ad hoc tests Dirichletsmoothing, Jelinek Mercer smoothing, and AbsoluteDiscounting approaches can be represented that way(see equation 6 and table I in Zhai & Lafferty, 2001).An lw.gw representation of Jelinek Mercersmoothing was used in Nallapati (2004).
It has beenknown for a long time that the shapes of the globaland local weighting functions can dramatically affectthe precision in standard test collections because it in154fact determines the difference between such formulasas tf.idf, bm25 and language models.
However, weare not aware of any attempts to learn those shapesdirectly from the labeled examples, which weperformed in this study.2.2 Intuition behind the discretization-basedlearningThe intuition behind discretization approach is torepresent a function by values at the finite number ofpoints.
Then, the optimal shape of the function canbe learned by using one of the machine learningtechniques.
Our discretization based learning (DBL)approach to information retrieval learns howimportant each class of an occurrence of a queryterm in a document.
For example, in some very?primitive?
DBL approach, we can define twoclasses:  Class S (?strong?
), containing all multipleoccurrences of a rare query term (e.g.?discretization?)
in a document and Class W(?weak?
), containing all single occurrences of afrequent term (e.g.
?information?).
Then, themachine learning technique should discover that theoccurrences of Class S are much stronger indicatorsof relevance than the occurrences of Class W. In theDBL implementation presented in this paper, eachoccurrence of a query term is assigned to a class(called bin) based on the term document frequencyin the collection (df) and the number of occurrenceswithin the document (tf).
The bin determines theweight of the contribution of each occurrence of thequery term in the ranking score.
Thus, the relevancescore is just the weighted sum of the numbers ofoccurrences within each bin.
The other way oflooking at it is that the score is produced by a linearclassifier, where the total number of occurrenceswithin each bin serves as the feature value.
Bylearning the optimal weights, a linear classifiereffectively learns the optimal shapes of the global(gw) and local (lw) weighting functions.
By learningthe discrimination properties of each bin, ratherthan separate word terms, DBL method allowsgeneralization to new queries.2.3 Discretizing global weightingWe discretized the shape of the G(t) function byassigning each term to its global weighting bin g,which is an integer number in the [1, |B|] range, |B|is the total number of global weighting bins.
Theassignment of the term t to its global weighting bing(t) is performed on the log linear scale according tothe document frequency df of the term:)}log(N)(df(t)) log - (1|B{|  g(t) =    (1)where N is the total number of documents, {.}
standsfor rounding down to the nearest integer.
Thelogarithmic scale allows more even term distributionamong bins than simple linear assignment, which isdesirable for more efficient learning.
It is motivatedby a typical histogram of df(t) distribution, whichlooks much more uniform in a logarithmic scale.
It isimportant to note that it does not have anything to dowith the log function in the classical idf weightingand that the formula for g(t) does not produce anyweights but only assigns each term occurrence to aspecific bin based on the term document frequency.The weights are later trained and effectively defineany shape of global weighting, including such simplefunctions tried in the prior heuristic explorations aslogarithm, square root, reciprocal and others.2.4 Discretizing local weightingSimilarly to the global weighting, we assigned eachoccurrence of a term to its local weighting bin l, butthis time by simply capping tf at the total number oflocal weighting bins |L|:l (tf(t, d), d) = min( tf (t, d), |L|) ) (1a)Let?s note that this particular representation does notreally need rounding since tf is already a positiveinteger.
However, in a more general case, tf can benormalized by document length (as is done in BM25and language models) and thus local weightingwould become a continuous function.
It is importantto note that our discrete representation does notignore the occurrences above |L| but simply treatsthem the same way as tf = |L|.
The intuition behindcapping is that increasing tf above certain value (|L|)would not typically indicate the higher relevance ofthe document.
Typically, a certain number ofoccurrences is enough to indicate the presence of therelevant passage.
Please note again that this binassignment does not assign any heuristic weights tothe term occurrences.2.5 Final discretized ranking functionThe bin assignments based on tf and df specified insections 2.3 and 2.4 are straightforward and do notinvolve any significant ?feature engineering.?
Eachoccurrence of a query term in a documentcorresponds to a local/global bin combination (g, l).Each (g,l) combination determines a feature in avector representing a document-query pair f(d, q)and is denoted below as f( d, q) [g , l] .
Thedimensionality of the feature space is  |L| x |B|.
E.g.for 8 local weighting bins and 10 global weightingbins we would deal with the vector size of 80.
Afeature vector f(d, q) represents each document dwith respect to query q.
The value of each feature inthe vector is just the number of the term occurrencesassigned to the pair of bins (g, l):f ( d, q) [g , l]  =  ?==?
ldtlgtg ),(,)( q,t1  (2)Since our features capture local (tf) and global (df)term occurrence information, in order to represent aranking function, we can simply use the dot productbetween the feature vector and the vector of learnedoptimal weights w:R(q, d) = w * f ( d, q).Ideally, the learning mechanism should assign higherweights to the more important bin combinations (e.g.multiple occurrence of a rare term) and low weightsto the less important combinations (e.g.
singleoccurrence of a common term).
The exact learnedvalues determine the optimal shape of global andlocal weighting.155We still can make the representation more powerfulby considering the learned weights w[g, l] not thereplacements but rather the adjustments to someother chosen global G (t) and local L (t, d) weightingfunctions:f ( d, q) [g , l] = ?==?
lddttflgtgtGdtL)),,((,)( q,t)(),(       (2a)We define the specific choice of global G() and localL() weighting functions as starting ranking function(SRF).
When all the bin weights w[g, l] are set to 1,our ranking function is the same as its SRF.
Thelearning process finds the optimal values for w[g, l]for the collection of training queries and theirrelevance judgments, thus adjusting the importantshapes of the global and local weighting to achievebetter accuracy.
SRF can be chosen from one of theknown to perform well ranking functions (e.g.
tf.idfor BM25 or based on language models) to takeadvantage of the fact that those formulas and theiroptimal parameters on the standard test collectionsare known for the researchers.
Alternatively, we canset SRF to the constant value (e.g.
1 in formula 2),thus not taking advantage of any of the priorempirical investigations and to see if our frameworkis able to learn reasonable (or even top-notch)performance purely from labeled examples.
Below,we describe our experiments with each approach.Since the score is linear with respect to the featurevalues, we can train the weights w as a linearclassifier that predicts the preference relationbetween pairs of documents with respect to the givenquery.
Document d1 is more likely to be relevant(has a higher score) than document d2 iff  f(d1, q) *w > f(d1, q) * w. An important advantage of using alinear classifier is that rank ordering of documentsaccording to the learned pairwise preferences can besimply performed by ordering according to the linearscore.
Please refer to Cohen et al (1999) for theordering algorithms in a more general non linearcase.We chose support vector machines (SVM) fortraining the classifier weights w[g, l] since they areknown to work well with large numbers of features,ranging in our experiments from 8 to 512, dependingon the number of bins.
For our empirical tests, weused the SVMLight package freely available foracademic research from Joachims (2001).
Wepreserved the default parameters coming withversion V6.01.
Although SVMLight package allowslearning ranking, we opted for training it as aclassifier to retain more control over sampling,which we found crucial for successful learning, asdescribed in the section below.2.6 SamplingSince we were training a classifier to predictpreference relations, but not the absolute value ofrelevance, we trained on the differences betweenfeature vectors.
Thus, for each selected (sampled)pair of documents (dr, di ), such that dr is a relevantdocument and di is irrelevant, the classifier waspresented with a positive example created from thevector of differences of features fp = f(q, dr) ?
f(q,di), and also with the negative example as theinverse of it:   fn= f(q, di) ?
f(q, dr).
This approachalso balances positive and negative examples.We also informally experimented with training onabsolute relevance judgments, similar to the priorwork mentioned in the Introduction but obtainedmuch worse results.
We explain it by the fact thatrelative judgments (pairwise comparisons) are moregeneralizable to new queries than absolutejudgments (relevant/irrelevant).
This may explainprior difficulties with applying discriminativeapproaches mentioned in our Introduction.Since presenting all pairs to the training mechanismwould be overwhelming, we performed pseudo-random sampling of documents by the followingintuitive consideration.
Since it is more efficient topresent the classifier with the pairs from thedocuments that are likely to more strongly affect theperformance metric (average precision), we first pre-ordered the retrieved documents by any of thereasonably well-performing scoring function (e.g.tf.idf) and limited the sample of documents to the top1000.
Then, for each query, each known relevantdocument dr from that subset was selected and?paired?
with a certain number of randomly selectedirrelevant documents.
This number was linearlydecreasing with the position of the relevantdocument in the pre-order.
Thus, the higher thedocument was positioned in the pre-order, the moretimes it was selected for pairing (training).
Thisplaced more emphasis at correctly classifying themore important document pairs in the averageprecision computation.
Again, without the correctemphasis during sampling the obtained results weremuch weaker.
However, the choice of the rankingfunction to perform pre-order was found to be notimportant: virtually the same results were obtainedusing tf.idf or bm25 or language models.3.
Empirical Evaluation3.1 Empirical setupWe used the TREC, Disks 1 and 2, collections to testour framework.
We used topics 101-150 for trainingand 151-200 for testing and vice-versa.
For indexing,we used the Lemur package (Kraaij et al, 2003),with the default set of parameters, and no stop wordremoval or stemming.
Although those procedures aregenerally beneficial for accuracy, it is also knownthat they do not significantly interfere with testingvarious ranking functions and thus are omitted inmany studies to allow easier replication.We used only topic titles for queries, as it iscommonly done in experiments, e.g.
in Nallapati(2004).
We used the most popular average (non-interpolated) precision as our performance metric,computed by the script included with the Lemurtoolkit (later verified by trec_eval).
Thecharacteristics of the collection after indexing areshown in Table 1.
We also reproduced results similarto the reported below on the Disk 3 collection and156topics 101-150, but did not include them in thispaper due to size limitations.CollectionNumber of documentsNumber of termsNumber of unique termsAverage doc.
lengthTopicsTREC Disks 1 and 2741,863325,059,876697,610438101-200Table 1.
The characteristics of the test collection:TREC Disks 1,23.2 The baselineIn this study, we were interested exclusively in theimprovements due to learning, thus still stayingwithin the ?bag of words?
paradigm.
Although manyenhancements can be easily combined within ourframework, we limited our search for the baselineperformance to ?bag of words?
techniques to avoidunfair comparison.
We used the results reported inNallapati (2004) as guidance and verified that thebest performing language model on this testcollection was the one based on the Dirichletsmoothing with ?
= 1900.
Our average precision waslower (0.205 vs. 0.256), most likely due to thedifferent indexing parameters, stemming or using adifferent stopword list.
By experimenting with theother ranking functions and their parameters, wenoticed that the implementation of BM25, availablein Lemur, provided almost identical performance(0.204).
Its ranking function isBM25 (tf, df) =  tf / (tf + K* (1 ?
b + b * |d| / |d|a) *log ( N /  (df + .5)), where |d| is the document wordlength and |d|a is its average across all documents.The optimal parameter values were close to thedefault K = 1.0 and b = .5.
We noticed that the queryterm frequency components could be ignoredwithout any noticeable loss of precision.
This may bebecause the TREC topic titles are short and thewords are very rarely repeated in the queries.
Sincethe difference between this ranking function and theoptimal from the available language models wasnegligible we selected the former as both ourbaseline and also as the starting ranking function(SRF) in our experiments.
For simplicity, we call itsimply BM25 throughout our paper.3.3 Discretization accuracyBefore testing the learning mechanism, we verifiedthat the loss due to discretization is minimal and thusthe approach is capable of capturing global and localweighting.
For this, we discretized our baselineBM25 formula replacing each score contribution ofthe occurrence of a term G(t)L(t,d) = BM25(t, d)with its average across all other occurrences withinthe same bin combination [g, l], which is determinedby the formulas 1 and 1a.
We discovered that for the|B| x |L| = 8 x 8 configuration, the loss in averageprecision did not exceed 2% (relatively).
Thisdemonstrates that the G(t)L(t,d) ranking functionscan be discretized (replaced by values at certainpoints) at this level of granularity without losingmuch accuracy.
We also verified that the weightsw[g, l] can affect the performance significantly:when we set them to random numbers in the [0,1]range, the performance dropped by 50% relatively tothe baseline.3.4 Ability to achieve top performance fromscratchFirst, we were curious to see if our framework canlearn reasonable performance without takingadvantage of our knowledge of the top rankingfunctions and their parameters.
For this, we set ourstarting ranking function (SRF) to a constant value,thus using only the minimum out of the empiricalknowledge and theoretical models developed byinformation retrieval researchers during severaldecades: specifically only the fact that relevance canbe predicted by tf and dfTable 2 shows performance for the 16 x 8combination of bins.
It can be  seen that ourapproach has reached 90-100% of the topperformance (baseline) solely through the learningprocess.
The original performance is the oneobtained by assigning all the classifier weights to 1.It can be seen that the topics 151-200 are moreamenable for the technique that is why they showbetter recovery when used as a test set even when thetraining set 101-150 recovers only 90%.
In order toevaluate if more training data can help, we also rantests using 90 topics for training and the remaining10 for testing.
We ran 10 tests each time using 10different sequential topics for testing and averagedour results.
In this case, the averaged performancewas completely restored to the baseline level withthe mean difference in precision across test queries+0.5% and 1% standard deviation of the mean.Figure 1.
Learning local weighting for variousTesting:101-150151-200Training: Original Learned Baseline Original Learned Baseline101-150 .119  .165 .174 .135 .180 .204151-200 .119 .175 .174 .135 .206 .204Table 2.
Learning without any knowledge of ranking functions.
16 x 8 bin design.157numbers of bins.
Learning on 101-150 and testing on151-200.Figure 2.
Learning global weighting for variousnumbers of bins.
Learning on 101-150 and testing on151-200.We believe this is a remarkable result consideringthe difficulties that the prior learning basedapproaches had with the classical informationretrieval task.
We attribute our success to bothhigher flexibility and generalizability of our discreterepresentation.
We also varied the number of bins toevaluate the effect of granularity of representation.Figures 1 and 2 demonstrate that 8 bins suffice forboth global and local weighting.
Higher numbers didnot result in noticeable improvements.When the same set was used for training and testingthe result obviously overestimates the learningcapability of the framework.
However, it also givesthe upper bound of performance of a discretizedgw.lw combination assuming that the loss due todiscretization is negligible which can be easilyattained by using sufficiently large number of bins.Thus, the results indicate that gw.lw, which includespractically all the popular ?bag of words?
rankingformulas such as tf.idf, BM25 or language models,has almost reached its upper limit and other classesof representations and ranking formulas need to beexplored to attempt greater improvements.Figure 2.
Learning global weighting for variousnumbers of bins.
Learning on 101-150 and testing on151-200.3.5 Ability to surpass top performanceIn order to test whether our approach can exceed thebaseline performance we set BM25 to be our startingranking function (SRF).
Thus, in this case:G(t) = log ( N /  (df + .5))  (6)L(tf, d) = tf / (tf + K  * (1 ?
b + b * |d| / |d|a)Table 3 shows performance for the 8 by 8 bin design.Although the improvement is relatively small (2-3%)it is still statistically significant at the level of alpha< 0.1, when the paired t-test was performed.
Thevalue in ?% change?
column shows the mean %improvement across all the queries and its standarddeviation.
It may differ from the % change of themean performance since there is wide variability inthe performance across queries but smallervariability in the improvement.We believe even such a small improvement isremarkable considering the amount of attention theresearches have paid to optimizing the rankingfunctions for this specific data set which has beenavailable for more than seven years.
A number ofrecent studies reported comparable improvements onthe same test collection by using more elaboratemodeling or richer representations.
Of course theimprovement due to the techniques such as thosebased on n-grams, document structures, naturallanguage processing or query expansion can possiblyachieve even better results.
However in this study wedeliberately limited our focus to the ?bags of words.
?3.6 Shape of optimal local weightingFigure 3 shows the optimal shape of the localweighting function L(tf) learned on entire set of 100topics and plotted against their counterparts ofBM25(t, d) = tf / (tf + 1) and tf.idf(t, d) = tf forcomparison.
For plotting purposes, we assumed thatthe document length was equal to its average.
Thevalues were linearly scaled to meet at the  tf = 8point.
It is easy to observe that the behavior of theoptimal function is much closer to BM25 than totf.idf, which explains the good performance of theformer on this test set.Figure 3.
Learned optimal shape of local weighting.3.7 Shape of optimal global weightingFigure 4 shows the optimal shape of the globalweighting function G(t) learned on the entire set of100 topics with |B| = 32 plotted in logarithmic scaleagainst the popular idf weighting used in both tf.idfand BM25.
The lower end of the X-axis (log10 df <2) corresponds to very infrequent terms, so thelearned weights may not be very informative since1 2 3 4 5 6 7 8tfGw(tf)TF-IDFDBL (Learned)BM25Testing:101-150 151-200Training: Learned Baseline % change Learned Baseline101-150 .180 .174 +2.3 (+/- 0.9) .208 .204 +2.3 (+/- 1.0)151-200 .179 .174 +1.8 (+/- 1.0) .210 .204 +3.2 (+/- 1.3)Table 3.
Surpassing the baseline performance.
8 x 8 bin design.158the classifier encounters fewer occurrences of themand their impact on the overall accuracy is small.
Inthe mid range (5,000 ?
10,000), the optimal weightsare higher than idf, which indicates that the latter hasan overly steep shape to discount high frequencyterms.
A more detailed interpretation of the optimalshape may require further investigation.Figure 4.
Learned optimal shape of global weightingG(t).4.
ConclusionsWe explored learning how to rank documents withrespect to a given query using linear Support VectorMachines and discretization-based representation.Our approach represents a family of discriminativeapproaches, currently studied much less thanheuristic (tf.idf, bm25) or generative approaches(language models).
Our experiments indicate thatlearning from relevant judgments available with thestandard test collections and generalizing to newqueries is not only feasible but can be a source ofimprovement.
When tested with a popular standardcollection, our approach achieved the performance ofthe best well-known techniques (BM25 and languagemodels), which have been developed as a result ofextensive past experiments and elaborate theoreticalmodeling.
When combined with the best performingranking functions, our approach added a small (2-3%), but statistically significant, improvement.Although practical significance of this study may belimited at the moment since it does not demonstrate adramatic increase in retrieval performance in largetest collections, we believe our findings haveimportant theoretical contributions since theyindicate that the power of discriminative approach iscomparable to the best known analytical or heuristicapporaches.
This work also lays the foundation forextending the discriminative approach to ?richer?representations, such as those using word n-grams,grammatical relations between words, and thestructure of documents.Our results also indicate that gw.lw family, whichincludes practically all the popular ?bag of words?ranking formulas such as tf.idf, BM25 or languagemodels, has almost reached its upper limit and otherclasses of representations and ranking formulas needto be explored in order to accomplish significantperformance break-troughs.Of course, using only few test cases (topics sets andcollections) is a limitation of this current study,which we are going to address in our future research.We view our approach as a complement, rather thancompetitive, to the analytical approaches such aslanguage models.
Our approach can be also used asan explorative tool in order to identify importantrelevance-indicating features, which can be latermodeled analytically.
We believe that our work andthe ones referred in this paper may bring many of theachievements made in a more general area ofclassification and machine learning closer to the taskof rank ordered information retrieval, thus makingretrieval engines more helpful in reducing theinformation overload and meeting people?s needs.5.
AcknowledgementWeiguo Fan's work is supported by NSF under thegrant number ITR0325579.ReferencesBartell, B., Cottrell, G., and Belew, R.(1994).Optimizing Parameters in a Ranked RetrievalSystem Using Multi-Query RelevanceFeedback.
Symposium on Document Analysisand Information Retrieval (SDAIR).Chengxiang Zhai and John Lafferty (2001).
Astudy of smoothing methods for languagemodels applied to Ad Hoc information retrieval.Proceedings of the Conference on Research andDevelopment in Information Retrieval (SIGIR),pp.
334 ?
342, 2001.Cohen, W., Shapire, R., and Singer, Y.
(1999).Learning to order things.
Journal of ArtificialIntelligence Research, 10, 243-270, 1999.Dougherty, J., Kohavi, R., & Sahami, M. (1995).Supervised and unsupervised discretization ofcontinuous features.
Proceedings of the TwelfthInternational Conference on Machine Learning(pp.
194--202).
Tahoe City, CA: MorganKaufmann.Fan, W., Luo, M., Wang, L., Xi, W., and Fox, A.(2004).
Tuning Before Feedback: CombiningRanking Discovery and Blind Feedback forRobust Retrieval.
Proceedings of theConference on Research and Development inInformation Retrieval (SIGIR), 2004.Fuhr, N. and C. Buckley (1991).
A probabilisticlearning approach for document indexing.
ACMTransactions on Information Systems, 9, 223?248.Fuhr, N. and C. Buckley (1991).
A probabilisticlearning approach for document indexing.
ACMTransactions on Information Systems, 9, 223?248.Gey, F. C. (1994).
Inferring probability ofrelevance using the method of logisticregression.
In Proceedings of the 17th ACM1 2 3 4 5 6 7log10 (df)Gw(t)IDFLearned159Conference on Research and Development inInformation Retrieval (SIGIR?94), pp.
222?231.Hearst, M.  (1998).
Support Vector Machines.IEEE Intelligent Systems Magazine, Trends andControversies, Marti Hearst, ed., 13(4),July/August 1998.Hun-Nan Hsu, Hung-Ju Huang and Tzu-TsungWong (2000).
Why Discretization Works forNaive Bayesian Classifiers, In Proceedings ofthe 17th International Conference on MachineLearning (ICML-2000), Stanford, CA, USA.Page 399-406.Joachims, T. (2001).
A Statistical Learning Modelof Text Classification with Support VectorMachines.
Proceedings of the Conference onResearch and Development in InformationRetrieval (SIGIR), 2001.Joachims, T. (2002).
Optimizing Search EnginesUsing Clickthrough Data, Proceedings of theACM Conference on Knowledge Discovery andData Mining (KDD), ACM, 2002.Kraaij, W., Westerveld T. and Hiemstra, D.(2003)., The Lemur Toolkit for LanguageModeling and Information Retrieval,http://www-2.cs.cmu.edu/~lemurNallapati, R. (2004).
Discriminative models forinformation retrieval.
Proceedings of theConference on Research and Development inInformation Retrieval (SIGIR), 2004, pp.
64-71.Robertson S. E. and Sparck Jones, K. (1976).Relevance weighting of search terms, Journal ofAmerican Society for Information Sciences,27(3), pp.
129-146, 1976.Robertson, S. E., Walker, S., Jones S., Hancock-Beaulieu M.M., and Gatford, M.
(1996)., Okapiat TREC-4, in D. K. Harman, editor,Proceedings of the Fourth Text RetrievalConference, pp.
73?97.
NIST SpecialPublication 500-236, 1996.Salton, G. and McGill, M.J. (1983).
Introduction toModern Information Retrieval.
New York.McGraw-Hill.Song, F.  and W.B.
Croft.
(1999) A generallanguage model for information retrieval.
InProceedings of Eighth International Conferenceon Information and Knowledge Management(CIKM?99).Vapnik, V. N. (1998).. Statistical Learning Theory.John Wiley and Sons Inc., New York, 1998.Vogt, C.,  G. Cottrell, G. (1999).
Fusion Via aLinear Combination of Scores.
InformationRetrieval, 1(3), pp.
151?173.160
