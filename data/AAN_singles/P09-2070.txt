Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 277?280,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPParsing Speech Repair without Specialized Grammar Symbols?Tim MillerUniversity of Minnesotatmill@cs.umn.eduLuan NguyenUniversity of Minnesotalnguyen@cs.umn.eduWilliam SchulerUniversity of Minnesotaschuler@cs.umn.eduAbstractThis paper describes a parsing model forspeech with repairs that makes a clear sep-aration between linguistically meaningfulsymbols in the grammar and operationsspecific to speech repair in the operation ofthe parser.
This system builds a model ofhow unfinished constituents in speech re-pairs are likely to finish, and finishes themprobabilistically with placeholder struc-ture.
These modified repair constituentsand the restarted replacement constituentare then recognized together in the sameway that two coordinated phrases of thesame type are recognized.1 IntroductionSpeech repair is a phenomenon in spontaneousspoken language in which a speaker decides tointerrupt the flow of speech, replace some of theutterance (the ?reparandum?
), and continues on(with the ?alteration?)
in a way that makes thewhole sentence as transcribed grammatical onlyif the reparandum is ignored.
As Ferreira et al(2004) note, speech repairs1 are the most disrup-tive type of disfluency, as they seem to requirethat a listener first incrementally build up syntac-tic and semantic structure, then subsequently re-move it and rebuild when the repair is made.
Thisdifficulty combines with their frequent occurrenceto make speech repair a pressing problem for ma-chine recognition of spontaneous speech.This paper introduces a model for dealing withone part of this problem, constructing a syntac-tic analysis based on a transcript of spontaneousspoken language.
The model introduced here dif-fers from other models attempting to solve the?This research was supported by NSF CAREER award0447685.
The views expressed are not necessarily endorsedby the sponsors .1Ferreira et al use the term ?revisions?.same problem, by completely separating the fluentgrammar from the operations of the parser.
Thegrammar thus has no representation of disfluencyor speech repair, such as the ?EDITED?
categoryused to represent a reparandum in the Switchboardcorpus, as such categories are seemingly at oddswith the typical nature of a linguistic constituent.Rather, the approach presented here uses agrammar that explicitly represents incompleteconstituents being processed, and repair is rep-resented by rules which allow incomplete con-stituents to be prematurely merged with existingstructure.
While this model is interesting for itselegance in representation, there is also reasonto hypothesize improved performance, since thisprocessing model requires no additional grammarsymbols, and only one additional operation to ac-count for speech repair, and thus makes better useof limited data resources.2 BackgroundPrevious work on parsing of speech with repairshas shown that syntactic cues can be used to in-crease accuracy of detection of reparanda, whichcan increase overall parsing accuracy.
The firstsource of structure used to recognize repair is whatLevelt (1983) called the ?Well-formedness Rule.
?This rule essentially states that a speech repair actslike a conjunction; that is, the reparandum and thealteration must be of the same syntactic category.Of course, the reparandum is often unfinished, sothe Well-formedness Rule allows for the reparan-dum category to be inferred.This source of structure has been used by tworelated approaches, that of Hale et al (2006) andMiller (2009).
Hale and colleagues exploit thisstructure by adding contextual information to thestandard reparandum label ?EDITED?.
In theirterminology, daughter annotation takes the (pos-sibly unfinished) constituent label of the reparan-dum and appends it to the EDITED label.
This277allows a learned probabilistic context-free gram-mar to represent the likelihood of a reparandum ofa certain type being a sibling with a finished con-stituent of the same type.Miller?s approach exploited the same source ofstructure, but changed the representation to usea REPAIRED label for alterations instead of anEDITED label for reparanda.
The rationale forthat change is the fact that a speech repair does notreally begin until the interruption point, at whichpoint the alteration is started and the reparandumis retroactively labelled as such.
Thus, the argu-ment goes, no special syntactic rules or symbolsshould be necessary until the alteration begins.3 Model Description3.1 Right-corner transformThis work first uses a right-corner transform,which turns right-branching structure into left-branching structure, using category labels that usea ?slash?
notation ?/?
to represent an incompleteconstituent of type ?
?looking for?
a constituentof type ?
in order to complete itself.This transform first requires that trees be bina-rized.
This binarization is done in a similar way toJohnson (1998) and Klein and Manning (2003).Rewrite rules for the right-corner transform areas follows, first flattening right-branching struc-ture:2A1?1A2?2A3a3?A1A1/A2?1A2/A3?2A3a3A1?1A2A2/A3?2.
.
.?A1A1/A2?1A2/A3?2.
.
.then replacing it with left-branching structure:A1A1/A2:?1A2/A3?2?3.
.
.
?A1A1/A3A1/ A2:?1?2?3.
.
.One problem with this notation is the represen-tation given to unfinished constituents, as seen inFigures 1 and 2.
The standard representation of2Here, all Aidenote nonterminal symbols, and ?idenotesubtrees; the notationA1:?0indicates a subtree ?0with labelA1; and all rewrites are applied recursively, from leaves toroot.S.
.
.
EDITEDPPINasNP-UNFDTaPPINasNPNPDTaNNwesternerPP-LOCINinNPNNPindia.
.
.Figure 1: Section of interest of a standard phrasestructure tree containing speech repair with unfin-ished noun phrase (NP).PPPP/NPPP/PPPP/NPPP/PPEDITEDPPEDITEDPP/NP-UNFINasNP-UNFDTaINasNPNP/NNDTaNNwesternerINinNPindiaFigure 2: Right-corner transformed version of thefragment above.
This tree requires several specialsymbols to represent the reparandum that startsthis fragment.an unfinished constituent in the Switchboard cor-pus is to append the -UNF label to the lowest un-finished constituent (see Figure 1).
Since one goalof this work is separation of linguistic knowledgefrom language processing mechanisms, the -UNFtag should not be an explicit part of the gram-mar.
In theory, the incomplete category notationinduced by the right-corner transform is perfectlysuited to this purpose.
For instance, the categoryNP-UNF is a stand in category for several incom-plete constituents, for example NP/NN, NP/NNS,etc.
However, since the sub-trees with -UNF la-bels in the original corpus are by definition unfin-ished, the label to the right of the slash (NN inthis case) is not defined.
As a result, transformedtrees with unfinished structure have the represen-tation of Figure 2, which gives away the positivebenefits of the right-corner transform in represent-ing repair by propagating a special repair symbol(EDITED) through the grammar.3.2 Approximating unfinished constituentsIt is possible to represent -UNF categories as stan-dard unfinished constituents, and account for un-finished constituents by having the parser prema-278turely end the processing of a given constituent.However, in the example given above, this wouldrequire predicting ahead of time that the NP-UNFwas only missing a common noun ?
NN (for ex-ample).
This problem is addressed in this workby probabilistically filling in placeholder final cat-egories of unfinished constituents in the standardphrase structure trees, before applying the right-corner transform.In order to fill in the placeholder with realisticitems, phrase completions are learned from cor-pus statistics.
First, this algorithm identifies anunfinished constituent to be finished as well as itsexisting children (in the continuing example, NP-UNF with child labelled DT).
Next, the corpus issearched for fluent subtrees with matching root la-bels and child labels (NP and DT), and a distri-bution is computed of the actual completions ofthose subtrees.
In the model used in this work,the most common completions are NN, NNS, andNNP.
The original NP-UNF subtree is then given aplaceholder completion by sampling from the dis-tribution of completions computed above.After this addition is complete, the UNF andEDITED labels are removed from the reparandumsubtree, and if a restarted constituent of the sametype is a sibling of the reparandum (e.g.
anotherNP), the two subtrees are made siblings under anew subtree with the same category label (NP).See Figure 3 for a simple visual example of howthis works.S.
.
.
EDITEDPPINasNPDTaNNeliPPINasNPNPDTaNNwesternerPP-LOCINinNPNNPindia.
.
.Figure 3: Same tree as in Figure 1, with the un-finished noun phrase now given a placeholder NNcompletion (both bolded).Next, these trees are modified using the right-corner transform as shown in Figure 4.
This treestill contains placeholder words that will not bein the text stream of an observed input sentence.Thus, in the final step of the preprocessing algo-rithm, the finished category label and the place-holder right child are removed where found in aright-corner tree.
This results in a right-cornertransformed tree in which a unary child or rightPPPP/NNPPP/PPPP/NPPP/PPPPPP/NNPP/NPINasDTaNNeliINasNPNP/NNDTaNNwesternerINinNNPindiaFigure 4: Right-corner transformed tree withplaceholder finished phrase.PPPP/NNPPP/PPPP/NPPP/PPPP/NNPP/NPINasDTaINasNPNP/NNDTaNNwesternerINinNNPindiaFigure 5: Final right-corner transformed state af-ter excising placeholder completions to unfinishedconstituents.
The bolded label indicates the signalof an unfinished category reparandum.child subtree having an unfinished constituent type(a slash category, e.g.
PP/NN in Figure 5) at itsroot represents a reparandum with an unfinishedcategory.
The tree then represents and processesthe rest of the repair in the same way as a coordi-nation.4 EvaluationThis model was evaluated on the Switchboard cor-pus (Godfrey et al, 1992) of conversational tele-phone speech between two human interlocuters.The input to this system is the gold standardword transcriptions, segmented into individual ut-terances.
For comparison to other similar systems,the system was given the gold standard part ofspeech for each input word as well.
The standardtrain/test breakdown was used, with sections 2 and3 used for training, and subsections 0 and 1 of sec-tion 4 used for testing.
Several sentences from theend of section 4 were used during development.For training, the data set was first standardizedby removing punctuation, empty categories, ty-pos, all categories representing repair structure,279and partial words ?
anything that would be diffi-cult or impossible to obtain reliably with a speechrecognizer.The two metrics used here are the standard Par-seval F-measure, and Edit-finding F. The first takesthe F-score of labeled precision and recall of thenon-terminals in a hypothesized tree relative to thegold standard tree.
The second measure markswords in the gold standard as edited if they aredominated by a node labeled EDITED, and mea-sures the F-score of the hypothesized edited wordsrelative to the gold standard.System Configuration Parseval-F Edited-FBaseline CYK 71.05 18.03Hale et al 68.48 37.94Plain RC Trees 69.07 30.89Elided RC Trees 67.91 24.80Merged RC Trees 68.88 27.63Table 1: ResultsResults of the testing can be seen in Ta-ble 1.
The first line (?Baseline CYK?)
indi-cates the results using a standard probabilisticCYK parser, trained on the standardized inputtrees.
The following two lines are results from re-implementations of the systems from Hale et al(2006) andMiller (2009).
The line marked ?Elidedtrees?
gives current results.
Surprisingly, this re-sult proves to be lower than the previous results.Two observations in the output of the parser onthe development set gave hints as to the reasonsfor this performance loss.First, repairs using the slash categories (for un-finished reparanda) were rare (relative to finishedreparanda).
This led to the suspicion that therewas a state-splitting phenomenon, where cate-gories previously lumped together as EDITED-NPwere divided into several unfinished categories(NP/NN, NP/NNS, etc.).
To test this suspicion, an-other experiment was performed where all unarychild and right child subtrees with unfinished cat-egory labels X/Y were replaced with EDITED-X.This result is shown in line five of Table 1.
Thisresult improves on the elided version, and sug-gests that the state-splitting effect is most likelyone cause of decreased performance.The second effect in the parser output was thepresence of several very long reparanda (morethan ten words), which are highly unlikely in nor-mal speech.
This phenomenon does not occurin the ?Plain RC Trees?
condition.
One explana-tion for this effect is that plain RC trees use theEDITED label in each rule of the reparandum (seeFigure 2 for a short real-world example).
Thisessentially creates a reparandum rule set, mak-ing expansion of a reparandum difficult due to thelikelihood of a long chain eventually requiring areparandum rule that was not found in the train-ing data, or was not learned correctly in the muchsmaller set of reparandum-specific training data.5 Conclusion and Future WorkIn conclusion, this paper has presented a newmodel for speech containing repairs that enforcesa clean separation between linguistic categoriesand parsing operations.
Performance was belowexpectations, but analysis of the interesting rea-sons for these results suggests future directions.
Amodel which explicitly represents the distance thata speaker backtracks when making a repair wouldprevent the parser from hypothesizing the unlikelyreparanda of great length.ReferencesFernanda Ferreira, Ellen F. Lau, and Karl G.D. Bai-ley.
2004.
Disfluencies, language comprehension,and Tree Adjoining Grammars.
Cognitive Science,28:721?749.John J. Godfrey, Edward C. Holliman, and Jane Mc-Daniel.
1992.
Switchboard: Telephone speech cor-pus for research and development.
In Proc.
ICASSP,pages 517?520.John Hale, Izhak Shafran, Lisa Yung, Bonnie Dorr,Mary Harper, Anna Krasnyanskaya, Matthew Lease,Yang Liu, Brian Roark, Matthew Snover, and RobinStewart.
2006.
PCFGs with syntactic and prosodicindicators of speech repairs.
In Proceedings of the45th Annual Conference of the Association for Com-putational Linguistics (COLING-ACL).Mark Johnson.
1998.
PCFG models of linguistic treerepresentation.
Computational Linguistics, 24:613?632.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, pages 423?430.Willem J.M.
Levelt.
1983.
Monitoring and self-repairin speech.
Cognition, 14:41?104.Tim Miller.
2009.
Improved syntactic models for pars-ing speech with repairs.
In Proceedings of the NorthAmerican Association for Computational Linguis-tics, Boulder, CO.280
