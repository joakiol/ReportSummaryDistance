DICTIONARY CONSTRUCTION BY DOMAIN EXPERTSEllen Riloff and Wendy G. LehnertDepartment of Computer ScienceUniversity of MassachusettsAmherst MA 01003Sites participating in the recent message understandingconferences have increasingly focused their research ondeveloping methods for automated knowledge acquisitionand tools for human-assisted knowledge ngineering.However, it is important to remember that the ultimateusers of these tools will be domain experts, not naturallanguage processing researchers.
Domain experls haveextensive knowledge about he task and the domain, butwill have little or no background in linguistics or textprocessing.
Tools that assume familiarity withcomputational linguistics will be of limited use inpractical development scenarios.To investigate practical dictionary construction, weconducted an experiment with government analysts.
Wewanted to demonstrate hat domain experts with nobackground in text processing could successfully use theAutoSlog dictionary construction tool \[Riloff 1993\].
Wecompared the dictionaries constructed by the governmentanalysts with a dictionary constructed by a UMassresearcher.
The results of the experiment suggest thatdomain experts can successfully use AutoSlog with onlyminimal training and achieve performance levelscomparable toNLP researchers.AutoSlog is a system that automatically constructs adictionary for information extraction tasks.
Given atraining corpus, AutoSlog proposes domain-specificconcept node definitions that CIRCUS \[Lehnert 1991\]uses to extract information from text.
However, many ofthe definitions proposed by AutoSlog should not beretained in the permanent dictionary because they areuseless or too risky.
We therefore rely on a human-in-the-loop to manually skim the definitions proposed byAutoSlog and separate the good ones from the bad ones.Figure 1 shows a snapshot of the AutoSlog interface usedto review potential dictionary entries.Two government analysts agreed to be the subjects of ourexperiment.
Both analysts had generated templates for thejoint ventures domain, so they were experts with the EJVdomain and the template-filling task.
Neither analyst hadany background in linguistics or text processing and hadno previous experience with our system.
Before theybegan using the AutoSlog interface, we gave them a 1.5hour tutorial to explain how AutoSlog works and how touse the interface.
The tutorial included some examples tohighlight important issues and general decision-makingadvice.
Finally, we gave each analyst a set of 1575concept node definitions to review.
These includeddefinitions to extract 8 types of information: jv-enfities,facilities, person names, product/service d scriptions,ownership ercentages, total revenue amounts, revenuerate amounts, and ownership capitalization amounts.We did not give the analysts all of the concept nodedefinitions proposed by AutoSlog for the EJV domain.AutoSlog actually proposed 3167 concept nodedefinitions, but the analysts were only available for twodays and we did not expect hem to be able to review3167 definitions in this limited time frame.
So we createdan "abridged" version of the dictionary by eliminating iv-entity and product/service patterns that appeared onlyinfrequently in the corpus.
1 The resulting "abridged"dictionary contained 1575 concept node definitions.We compared the analysts' dictionaries with the dictionarygenerated by UMass for the final Tipster evaluation.However, the official UMass dictionary was based on thecomplete set of 3167 definitions originally proposed byAutoSlog as well as definitions that were spawned byAutoSlog's optional generalization modules.
We did notuse the generalization modules in this experiment, due totime constraints.
To create a comparable UMassdictionary, we removed all of the "generalized"definitions from the UMass dictionary as well as thedefinitions that were not among the 1575 given to theanalysts.
The resulting UMass dictionary was a muchsmaller subset of the official UMass dictionary.Analyst A took approximately 12.0 hours and Analyst Btook approximately 10.6 hours to filter their respectivedictionaries.
Figure 2 shows the number of definitionsthat each analyst kept, separated by types.
Forcomparison's sake, we also show the breakdown for thesmaller UMass dictionary.IWhile processing the training corpus, AutoSlog keepstzack of the number of times that it proposes each definition(it may propose a definition more than once if the samepattern appears multiple times in the corpus).
We removedall jv-entity definitions that were proposed <2 times and allproduct/service definitions that were proposed <3 times.
Weeliminated jv-entity and product/service d finitions onlybecause the sheer number of these definitions overwhelmedthe other types.257Proposed CNe Accepted CNsSUBJECT VERB AND DO CONTROL~,~,.
I J PP NOUN STAKE INSUBJECT VERB AND DO EORMINUiml ISUBJECT VERB AND DO HOLD $PEI~SUBJECT VERB AND DO LHUNCHE ~.~.~.;.=.
;~;"SUBJECT VERB AND DO PUHSOIN~.,U, IRejected CNsPP NOUN ASIA WITH-- JAPAN STORAGE BATIERY C0.
ANNOUNCED IT HAS TEAMED UP WITH A LEADING FRENCHBAq'rERY MAKER, SAFT S.A., TO SET UP A JOINT VENqWJRE IN JAPAN TO MARKET SMALLBATTERIES.%JV-ENTITY-NAME-PP-ACTIVE -VERB -TEAMED -UP-WITH%Pattern: "TEA~ED UP WITH <entity>"Trigger: TEAMED (VERB)Doc ID: "0016"Filler: (SAFT S=A=)Entire-NP" "None"I ACCEPT \]i REJECT \]I POSTPONE \]~"1 C"gi~-I{-g~Fffl r~3  JU-PARENT ~COMPRNY JU-CHILD PERSON PARENT GOVERNMENTJU ~NONE 'NONEFigure h The AutoSlog Interface ToolRn-nnnerellzn \]CN Typeentityfacilityownership-percentpeFsonrrod_ser# proposed byAutoSlog# keptfOMass)# kept(Analyst A)688 311 357# kept(Analyst B)42380 20 16 55174 91 117 91243 119 149 52316 76 152 44revenue-rate 19 14 12 16revenue-total 30 22 15 2625 14 13 22 total-capitalizationTOTAL 667 1575 831 729Figure 2: Comparative Dictionary SizesWe compared the dictionaries constructed by the analystswith the UMass dictionary in the following manner.
Wetook the official UMass/I-Iughes ystem, removed theofficial UMass dictionary, and replaced it with a newdictionary (the smaller UMass dictionary or an analysts'dictionary).
One complication is that the UMass/Hughessystem includes two modules, TFG and MayTag, that usethe concept node dictionary during training.
In a cleanexperimental design, we should ideally retrain thesecomponents for each new dictionary.
We did retrain thetemplate generator (TFG), but we did not retrain MayTag.We expect hat this should not have a significant impacton the relative performances of the dictionaries, but weare not certain of its exact impact.
Finally, we scored258each new version of the UMass/Hughes system on theTips3 test set.
Figure 3 shows the results for eachdictionary.The F-measures (P&R) were extremely close across  all 3dictionaries.
In fact, both analysts' dictionaries achievedslightly higher F-measures than the UMass dictionary.The error rates (ERR) for all three dictionaries wereidentical.
But we do see some variation in the recall andprecision scores.
We also see variations when we scorethe three parts of Tips3 separately (see Figure 4).In general, the analysts' dictionaries achieved slightlyhigher recall but lower precision than the UMassdictionary.
We hypothesize that this is because theUMass researcher was not very familiar with the corpusand was therefore somewhat conservative about keepingdefinitions.
The analysts were much more familiar withthe corpus and were probably more willing to keepdefinitions for patterns that hey had seen before.
There isusually a trade-off involved in making these decisions: aliberal strategy will often result in higher ecall but lowerprecision whereas a conservative strategy may result inlower recall but higher precision.frequently triggered by a given test set.
If the threedictionaries were in agreement on that subset of thedictionary that is most heavily used, those definitionscould dominate overall system performance.
Somedictionary definitions are more important than others.To summarize, this experiment suggests that domainexperts can successfully use AutoSlog to build domain-specific dictionaries for information extraction.
With only1.5.
hours of training, two domain experts constructeddictionaries that achieved performance omparable toadictionary constructed by a UMass researcher.
Althoughthis was only one small experiment, the results lendcredibility to the claim that domain experts can buildeffective dictionaries for inf(m'nation extraction.BIBLIOGRAPHYLehnert, W. (1991).
Symbolic/Subsymbolic SentenceAnalysis: Exploiting the Best of Two Worlds.
Advancesin Connectionist and Neural Computation Theory.
Vol.I.
(ed: J. Pollack and J. Barnden) Ablex Publishing,Norwood, New Jersey.
pp.
135-164.It is interesting to note that even though there was greatvariation across the individual dictionaries ( ee Figure 2),the resulting scores were very similar.
This may bebecause some definitions can contribute adisproportionate amount of performance if they areRiloff, E. "Automatically Constructing a Dictionary forInformation Extraction Tasks".
Proceedings of theEleventh National Conference on Artificial Intelligence.1993.
pp.
811-816.TIPS3 RecallUMass/HughesAnalyst AAnalyst BPrecision , i P&R ERR18 51 27.06 83i ,19 47 27.39 8320 47 27.89 83Figure 3: Comparative Scores for Tips3TIPS3/Partl Recall Precision P&R ERR= 27.04 83 UMass/HughesAnalyst AAnalyst B18 5120 48 28.00 8222 47 29.69 81TIPS3/Part2UMass/HughesAnalyst AAnalyst BRecall17Precision52P&R26.03ERR8418 48 25.92 8420 47 27.75 83TIPS3/Part3 Recall Precision P&R ERR20 50 28.12 82 UMass/I-IughesAnalyst AAnalyst B20 46 27.96 8217 48 25.25 84Figure 4: Comparative Scores for Partl, Part2, and Part3259
