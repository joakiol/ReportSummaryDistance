INDEXING AND EXPLOITING A DISCOURSE HISTORYTO GENERATE CONTEXT-SENSITIVE EXPLANATIONSJohanna D. Moore*Department  o f  Computer  Science, and LRDCUnivers i ty  o f  PittsburghPittsburgh, PA 15260ABSTRACTA slriking difference between the interactions that students have withhuman tutors and those they have with computer-based instructionsystems i that human tutors frequently refer to their own previousexplanations.
Based on a study of human-human instructional inter-actions, we are categorizing the uses of previous discourse and aredeveloping a computational model of this behavior.
In this paper, Idescribe the slrategies we have implemented for identifying relevantprior explanations, and the mechanisms that enable our text plannerto exploit he information stored in its discourse history in order toomit information that has previously been communicated, to pointout similarities and differences between entities and situations, andto mark re-explanations i  circumstances where they are deemedappropriate.1.
IntroductionTo reap the benefits of natural language interaction, user inter-faces must be endowed with the properties that make humannatural anguage interaction so effective.
One such prop-exty is that human speakers freely exploit all aspects of themutually known context, including the previous discourse.Computer-generated utterances that do not draw on the previ-ous discourse seem awkward, unnatural, or even incoherent.The effect of the discourse history is especially important ininstructional pplications because xplanation is essentiallyincremental nd interactive.
To provide missing informa-tion in a way that facilitates understanding and learning, thesystem must have the capability to relate new informationeffectively to recently conveyed material, and to avoid re-peating old material that would distract the student from whatis new.
Strategies for using the discourse history in generatingutterances are therefore crucial for building computer systemsintended to engage in instructional dialogues with their users.The goal of our work is to produce a computational modelof the effects of discourse context on explanations in instruc-tional dialogues, and to implement this model in an intelli-gent explanation facility.
Based on a study of human-humaninstructional dialogues, we are developing a taxonomy thatclassifies the types of contextual effects that occur in our dataaccording to the explanatory functions they serve \[1\].
Thus*The reseat~h described in this paper was upported bythe Office of NavalResearch, Cognitive and Neural Sciences Division, and the National ScienceFoundation, Reseerch Initiation Award.far, we have focused on four categories from our taxonomy?
explicit reference to a previous explanation (or portionthereof) in order to point out similarities (differences)between the material currently being explained and ma-terial presented inearfier explanation(s),?
omission of previously explained material to avoid dis-tracting student from what is new,?
explicit marking of repeated material to distinguish itfrom new material (e.g., "As I said before,.
.
.
")?
elaboration of previous material in the form of general-izations, more detail, or justifications.
1Building on previous work \[2, 3\] we have implemented anex-planation facifity that maintains a discourse history and usesit in planning subsequent explanations.
We are using this ex-planation facility as part of two intelligent systems.
The firstis a patient education system intended to provide patients withinformation about heir disease, possible therapies, and med-ications \[4, 5\].
The second is an intelligent coached practiceenvironment for training avionics technicians to troubleshootcomplex electronic equipment \[6\].In order to generate texts that exploit previous discourse, asystem must have the following capabilities:1.
It must understand its own previous explanations.2.
It must be able to findprior explanations (or parts thereof)that are relevant to generating the current explanation, inan e.~cient manner.3.
It must have strategies for exploiting the relevant priortexts in pedagogically useful ways when generating thecurrent explanation.In this paper, I describe how we have realized these threerequirements in the two instructional systems.2.
BackgroundTo achieve the first requirement, our explanation system usesan extended version of the text planner developed by Moore1This category breaks up into number ofsubcategories in our taxonomy.165and Paris \[2\].
Briefly, the text planner works in the follow-ing way.
When the user provides input to the system, thequery analyzer interprets the question and forms a commu-nicative goal representing the system's intended effect on thehearer's mental state, e.g., "achieve the state where the hearerbelieves that action A is suboptimal" or "achieve the statewhere the hearer knows about he side effects of drug X.
"A linear planner synthesizes responses to achieve these goalsusing a library of explanation operators that map communica-five goa~s to linguistic resources (speech acts and rhetoricalstrategies) for achieving them.
In general, there may be manyoperators available to achieve a given goal, and the plannerhas a set of selection heuristics for choosing among them.Planning is complete when all goals have been refined intospeech acts, such as reFORM and RECOMMEND.In this system, a text plan represents he effect hat each partof the text is intended to have on the hearer's mental state,the linguistic strategies that were used to achieve these ffects,and how the complete t xt achieves the overall communicativegoal.
When a text plan is complete, the system presents theexplanation tothe user, retaining the plan that produced it ina discourse history.In previous work, I showed how a system could support alimited range of dialogue capabilities using the informationrecorded in its discourse history \[3\].
In particular, I devisedinterpretation a d recovery heuristics that examine the textplan that produced the immediately preceding explanationin order to interpret and respond to the follow-up questions"Why?"
and "Huh?'.
In ~,~d_ition, the system is able toavoid producing the same answer to a question asked asecondtime by searching the discourse history to determine if thecommtmicative goal corresponding to the question was everposted before.
If so, the system notes which strategy wasused in the previous case and employs recovery heuristics tochoose an alternative swategy.The work reported in this paper is aimed at augmenting theways in which the information recorded in the discourse his-tory affects each new explanation as it is planned.
In general,previous dialogue should potentially influence the answer toevery subsequent question, not just expficit follow-up ques-tions, such as "Why?"
and "Huh?
", or questions that areliterally asked twice.
Supporting this capability requires rec-o~nlzing when prior explanations are relevant and how theyshould affect he current response.3.
ExamplesExamples of the types of contextual effects we are interestedin appear in Figures 1 and 2.
The dialogue in Figure I istaken from our corpus of human-human written instructionaldialogues in the SHERLOCK domain.
SHERLOCK is an in-telligent training system that teaches avionics technicians totroubleshoot complex electronic equipment.
It is built withinthe "learning by doing" paradiEm; in which students olveproblems with minimal tutor interaction and then review theirtroubleshooting behavior in a post-problem reflective follow-up session (SFU) where the tutor eplays each student actionand provides a critique (here the tutor marks each action as"good" (<+>) or as "could be improved" (<->)).
To collectprotocols for study, the system was used to replay each stepduring RFU, but the human tutor provided the assessment andanswered any questions the student posed.In Figure 1, the student performs three actions that are assessednegatively for reasons that are related.
Testing pin 28 is badfor one of the same reasons as testing pin 38, and testing pin36 is bad for precisely the same reason as testing pin 28.
Inthe figure, italics are used to highlight what we categorize ascontextual effects on the explanations given.
For example,when explaining why testing pin 28 is bad (turn 6), the tutorrefers back to one of the reasons given in the explanation iturn 3, and reiterates the fact that the main data inputs arehighly suspect (signalled by "As explained before").
In turn6, the tutor offers an elaboration that introduces the notions ofmain and secondary d~m control signals and justifies why themain data signal should be tested first.
Later, when explainingwhy testing pin 36 is bad in turn 9, the tutor refers backto the explanation he gave for testing pin 28 and states ageneralization explaining why all three of these actions areconsidered suboptimal.
The tutor expects the student to beable to make use of the explanations given in turn 6 (andtherefore turn 3) by indicating that it is relevant to the currentsituation ("for the same reasons given..." serves this purpose).Accordingly, the tutor does not repeat the detailed explanationof why the main control data signals should be tested first.By generating the explanation i turn 9 in such a way that it'meshes' with the previous two, not only does the tutor correctstudent's error, but he forces the student to consider how thethree situations are similar.
Pointing out this s'tmilarity mayfacilitate the student in forming the domain generalization a drecognizing how the three instances fit this generalization.Figure 2 shows an actual dialogue with our patient educationsystem, and is based upon phenomena we observed in nat-urally occurring interviews with physicians.
The responsesshown in this example are automatically generated, and typifythe type of interaction a patient can have with our system.
Pa-tients construct questions via a direct manip~lation interfacethat allows users to combine menu election and highlightingportions of previous explanations on the screen.
This inter-face extends our previous work \[7\].
Note that although theuser asks exactly the same type of question in turns 1 and3, the system's answers are quite different.
This is becausethe answer in turn 4 (R4) is affected by the context creotpd inturns 1-3.
For e~ample, the first sentence of R4 points out thatElavil is used for the same type of therapy as Inderal.
Further166TUTORSTUDENTTUTORTUTORSTUDENTTUTORTUTORSTUDENTTUTOR<->You tested pin 38 of the A1A3AI5.Why is testing pin 38 not considered a good step?For two reasons.
First, because you did not completely test he inputs needed to set relay B24 (you tested thelow input but not the high input).
These two inputs are critical for the TO test step and are highly suspect whena relay appears to be reset.
Secondly, the data input at pin 38 has no effect on the path that is being createdbetween pins 57 & 58 and the output at 11 & 12.
Relay B26 could be set or reset- it just doesn't matter.
: Short discussion about how to determine whether a relay interferes with the signalpath.<-> You tested pin 28 of the A1A3A15.Why is testing pin 28 not a good step?As explained before, the main data inputs to relay B24 are highly suspect at this time.
You have tested the lowinput o B24 but not the high one.
Since it b more likely that there may be a problem with the main data signalsto relay B24 than with the secondary do.to signals to relays B25-B30, the main data signal should be tested flrst.Testing pin 28 is not a bad step, it is just more likely (on the average) that he main data signals are causing theproblem.
: One exchange and one action later.<->You tested pin 36 of the A1A3A15.Don't I need to test pin 36?You might have to, but for the same reasons given when you tested pin 28, it b generally more e.~icient to testthe main control data sisnals flrst, and then test he secondary control data sisnals if necessary.Figure 1: Human-Human Advisory Interaction Displaying Contextual Effects\[1\]\[2\]\[3\]\[4\]\[5\]\[6\]ff\]\[8\]\[9\]note that in R4 ,the system does not explain what prophylac-tic treatment means because it has done so previously in R2,i.e, the system omits information that has been presented inaprevious explanation.
Finally, in the penultimate s ntence ofR4, Elavil's contraindications and side effects are contrastedwith those of Inderal.4.
F ind ing  Re levant  P r io r  Exp lanat ionsIn order to produce the types ofbehavior exemplified above, asystem must be able to determine which prior explanation(s)should be referred to when constructing an explanation.
Al-though it is convenient for expository purposes to think ofthe tasks of finding relevant prior explanations and exploitingthem in the construction of the current explanation as con-ceptuaUy distinct, they may be interleaved in the actual textgeneration process, as discussed in the next section.In our systems, the discourse history is a simple stack.
Ex-planation plans are large, complex structures and they willaccumulate rapidly as the dialogue progresses.
Exhaustivelysearching the discourse history for relevant prior explanationsis computationally prohibitive.
Therefore, we require index-ing strategies that allow the system to find possibly relevantprior explanations in an efficient manner.
We have foundthat two factors influence the identification of relevant priorexplanations:?
what was conveyed in prior explanations, i.e., the re-lationship of the domain content to be conveyed to thedomain contem presented inprior explanation(s)?
why a previous explanation was given, i.e., the relation-ship between the communicative goal(s) of the currentand prior explanation(s)Therefore, to facilitate the search for relevant prior expla-nations, we index each segment of an explanation by thecommunicative goal(s) the text is intended to achieve, andthe domain entities and situations that are discussed in thatsegment}Let us consider how these indices can be used to facilitate theidentification fa relevant prior explanation for generating thetutor's response in turn 6 of Figure 1.
When the student askswhy testing pin 28 is not a good step (turn 5), the system formsthe communicative goal: (BEL H (~v~OPTIMAL-STEP(MEAS-TEST (PIN28 OF AiA3AI5) ) ) ).
One of Mra~-gies for answering this question looks to see if there is aprevious tudent action, ?A, that is similar to the test of pin28 and that was involved in a previous explanation segmentsatisfying the goal (BEL H (S~TBOPTIH~J.,-STEP ?A) ).
Us-ing an algorithm described below, the system determines thatin this case, the action of testing pin 38 is similar to thetest of pin 28 and there is an explanation (turn 3) satisfy-ing the communicative goal (BEL H (SUBOPTIMAL-STEP2Following COrosz and $idner \[8\], in our model intentions are the basicdeterminer of segmentation, andtherefore each communicative goal indicatesa segment boundary.167USERSYSTEMUSERSYSTEMDescribe Inderal.Inderal is a drug that is used for prophylactic Ireatment of migraine.
Drugs for prophylactic treatment areintended to reduce or prevent further migraine attacks.
These are drugs that you must ake every day, whetheror not you have a migraine.
In general, prophylactic ~eatment is suitable for patients with frequent migraines.Inderal may not be suitable for you if you have diabetes, problems with your heart, or a severe pulmonarydisease such as asthma.
Some side effects of Inderal are lowered blood pressure, fatigue, and depression.Describe Elavil.Like lnderal, Elavil is used for prophylactic treatment ofmigraine.
~ Elavil may not be suitable for you if youhave irregular heartbeat or if you experience dizziness when you stand up.
Elavil is beuer than lnderalforpatients who experience depression, because Elavil is actually an anti-depressant.
However, Elavil has otherpossible side effects including dry mouth, difficulty urinating, and weight gain.Figure 2: Actual Dialogue with Patient Education System\[1\]\[2\]\[3\]\[4\](HEAS-TEST (PIN38 OF A1A3A15) ) ) ).
The explanationstrategy points out the similarity between the prior and currentexplanation, generating the text "As explained before, .
.
."
inturn 6.Other strategies cover cases in which an identical commu-nicative goal was attempted before, or the action itself or asimilar action was discussed but in service of a different com-municative goal.
These strategies use the two types of indicesto quickly determine if there are prior explanations that satisfythe constraints on their applicability.
I provide xamples ffi'omthe patient education domain in the Section.Determining Relationships between Domain EntitiesIn the patient education system, domain knowledge is repre-sented in LOOM \[9\], a term-subsumpfion la guage.
Therefore,domain entities and relationships between them are well de-fined and determined simply by queries written in LOOM'Squery language.In the Sherlock system, much of the knowledge used in trou-bleshooting and assessing student's actions is represented pro-cedurally, and therefore other techniques for computing rela-tionships between domain entities are needed.
In rdro inter-actions, the most commoniy asked question is to justify thetutor's assessment ofa step (32% of all questions asked uringRF~), and 27 % of the answers to such questions involve refer-ences to previously assessed actions.
Therefore, an efficientalgorithm for computing similarity of student actions was con-sidered essential for producing the types of context-sensitiveexplanations that are required in this domain.
To compute sim-ilarity between actions, the system uses a technique sd~ptedfrom Ashley's work in case-based legal reasoning \[10\].
Thisalgorithm; developed by James Rosenblum, makes use of aset of facets that SHERLOCK employs to evaluate ach stu-dent action.
These facets were derived from a cognitive taskanalysis aimed at identifying the factors that expert avionicstutors use in assessing student's troubleshooting actions \[11\].Associated with each facet is an indication of whether thatfacet contributes to a good (+), bad ( - ) ,  or neuWal (n) eval-uation in the current problem-solving context.
The system'srepresentation f a student action includes the list of facetscharacterizing the action.Treating each student action as a "case", the algorithm builds asimilarity DA G representing apartial ordering of actions basedon the similarity of each action to a given action.
The systemcan compute overall similarity, or similarity with respect oa certain class of facets (% - ,  or n).
For example, whenanswering a question about why the current action receiveda negative assessment, the similarity DAG is built so that itindicates imilarity of previous actions to the current actionwith respect to the - facets.
The root of the DAG representsthe current action and the facets that apply to it.
Each node inthe graph represents a set of actions that share the same set offacets.
The more facets that a node has in common with thecurrent action (the root node), the closer it will be to the rootnode.Initial results using this algorithm are quite promising.
Thealgorithm is both efficient (complexity O(n 2) where n is thenumber of student actions) and accurate.
In a corpus of 8student-tutor protocols involving 154 student actions and 30requests to justify the tutor's assessment of the student's ac-tion, the human tutor produced 8 responses that explicitlypointed out similarity(ies) toaction(s) whose assessment hadpreviously been explained.
These 8 responses involved 11similar actions in total.
In all 8 situations the algorithm cor-rectly selected as most similar the same actions used in thetutor's explanations.
In3 cases the algorithm suggested a sim-ilarity not used by the tutor.
However, when presented withthese similarities, our expert tutor judged them as correct andstated that explanations that explicitly pointed out these sim-ilarities would have been pedagogically useful.
In all othercases in which the human tutor did not make reference toa previous explanation as part of an answer, our algorithmreported that no prior action was similar.168NAME: Opl~ .
.CT :  (KNOW-ABOUT H ?d))CONSTRAINTS: (AND (ISA 7d DRUG)(USE 7d ?t))NUCLEUS: (BEL H (USE ?d ?t))SATmJIrES:(((BEL H (SOMEREF (contraindication ?d))) *required*)((BEL H (SOMEREF (other-use ?d))) *optional*)((BEL H (SOMEREF (side-effect ?d))) *required*)((BEL H (SOMEREF (warning ?d))) *optional*))NAME: Op3EFFECT: (BEL H (Tr 7argl 7arg2))CONSTRAINTS: (IN-DH (BEL H (?r ?x ?arg2)))NUCLEUS: (BEL H (SAME-AS (Tr ?x ?arg2)(Tr ?argl ?arg2)))SATIn J IrES: nilNAME: Op6EFFECT: (BEL H 7p)CONSTRAINTS: nilNU~S:  (INTORM H ?p)SAT~ J.rIEs: nflFigure 3: Sample Plan Operators from Patient Education System5.
Exp lo i t ing  P r io r  Exp lanat ionswith the capability to identify relevant prior discourse, oursystems are able to exploit his information when planningexplanations using three mechanisms: plan operators thatimplement context-sensitive strategies, domain-independentplanning heuristics (e.g., prefer operators that refer to previousexplanations), and plan modification rules that alter a planbased on information from the discourse history (e.g., if anoptional communicative goal has already been achieved, on'tplan text to achieve it).We now consider how the patient education system can pro-duce the behavior illustrated in the sample dialogue in Fig-ure 2.
When the user asks the system to 'Describe In-deral' (turn 1), the system posts the goal (WOW-ABOUT HINDERAL).
The planner searches its operator library to findan operator capable of achieving this goal, and finds Oplshown in Figure 3.
This operator encodes a strategy for de-scribing a drug derived from our analysis of transcripts ofdoctor-patient i eractions and interviews with physicians.To determine whether this operator can be used in the currentsituation, the planner checks its constraints.
If a constraintpredicate includes only bound variables, then the planner ver-ifies the constraint against the knowledge base.
For example,the first constraint inOpl ( ISA ?d  DRUG) checks the domainknowledge to verify that INDERAL is of type DRUG.
Alterna-tively, if a constraint predicate contains variables that are notyet bound, the planner searches the system's knowledge basesfor acceptable bindings for such variables.
For example, tocheck the constraint (USE ?d ?t) where ?d is bound toINDERAL, but ?
1: is not bound, the planner searches the med-ical knowledge base and finds that the variable ?
1= can bebound to PROPHYLACTIC-MIGRAINE-TREATMENT.
There-fore, all the constraints on Opl are verified, and the operator ischosen.
To expand the operator, the planner posts the subgoalappearing inthe nucleus 3 field of the operator, (BEL H (USEINDERAL PROPHYLACTIC-MIGRAINE-TREATMENT)), and3The terms nuc/eus and sate///te come from Rhetorical Structure Theory(RST).
For more details about RST, see \[12\].then the subgoals appearing in the satellite.
Expanding thesatellites of Opl posts up to four additional subgoals.The planner must then find operators for achieving each ofthe subgoals.
To achieve the first subgoal, (REL H (USEINDERAL PROPHYLACTIC-MIGRAINE-TREATMENT) ), theplanner uses Op6 which encodes the simple strategy: to makethe hearer believe any proposition ?p, simply inform her of?p.
Speech acts, e.g., INFORM and m~comww.~rv, a ethe prim-itives of our text planning system.
When a subgual has beenrefined to a speech act, the system constructs a functionaldescription (FD) for the speech act.
When text planning iscomplete, these FDs are passed to the FUF sentence generator\[13\] which produces the actual English text.In the process of building an FD, new text planning oals maybe posted as side effects.
This occurs because it is only whenbuilding FDs that he planner considers how concepts will berealized in text.
To provide informative and understandableexplanations, the system uses the plan modification heuristic:"Post optional suhgoals to explain unfamiliar terms intro-duced in explanation".
During the process of building FDs,this heuristic auses the system to check its user model to seeif each term that will be mentioned inthe text is known to theuser.
In wansforming (INFORM H (USE INDERAL PRO-PHYLACTIC-MIGRAINE-TREATMENT) ), the interface notesthat he user does not already know the concept PROPHYI~C-TIC-MIGRAINE-TREATMENT, therefore it posts a subgoal todescribe this term, as shown in the system's utterance inturn2 of the sample dialogue.The rest of the explanation i  turn 2 results from expanding theremaining satellite subgoals in a similar manner.
The user thenasks the system to describe Elavil (turn 3).
Opl is again cho-sen, however, this time the planner finds two appficable oper-ators for achieving the subgoal (INFORM H (USE ELAVILPROPHYLACTIC-MIGRAINE-TREATMENT) ), namely Op3and Op6.
Note that the constraint of Op3 (IN-DH (BELH (?r ?x ?arg2))) (where ?r is bound to USE and?arg2 to PROPHYLACTIC-MIGRAINE-TREATMENT) is sat-isfied by binding ?x to INDERAL because the system169achieved the goal (BEL H (USE INDERAL PROPHYI.aAC-TIC-MIGRAINE-TREATHFENT) ) ) in its previous explanation.The system can determine this efficiently using the indicesdescribed in the previous section.The system has a selection heuristic that guides it to preferoperators that refer to previous explanations, and thus Op3is chosen to achieve the current goal.
Refining this operatorleads the system to generate the text "Like Inderal, Elavilis used for ...".
Another context-sensitive operator applieswhen the system expands the subgoal (gEL H (OTHZR-USEF~n~VIL DEPRZSSZON) }, and leads to the text "Elavil is bet-ter than Inderal ... ".
In addition, note that the system didnot explain the term PROPHYI.aACTIC-HIGRAINE-TREAT-MF~rr when describing Elavil.
This is because when thesystem attempts to determine whether this term is knownto the user, it finds that the term was explained in theprevious text (i.e., the goal (KNOW-ABOUT H PROPHYI.~C-TIC-MIGRAINE-TREATMENT) appears in a previous textplan), and therefore it does not re-explain this term.Thus we see that, by checking for the existence of certain com-municative goals in the discourse history, context-sensitiveplan operators, plan selection heuristics, and plan modifi-cation rules enable the system to generate context-sensitiveresponses.6.
Re la ted  WorkComputational linguists have investigated how the contextprovided by the previous discourse should affect the gener-ation of referring expressions, including pronominalizationdecisions (e.g., \[14, 15\]).
Others have studied how a moreextensive discourse history could affect other aspects of theresponse.
Swattout's XPLAIN system can suggest simpleanalogies with previous explanations and omit portions ofa causal chain that have been presented in an earlier expla-nation.
However, this is the only type of contextual effectimplemented in XPLAIN, and it was done so using an ad hoctechnique to provide this one effect.
We are attempting toprovide a more general approach.McKeown carried out a preliminary analysis of how previousdiscourse might affect a system's response to users' requeststo describe an object or compare two objects \[16\].
She foundthat by simply maintaining a list of the questions that had beenasked, it was possible to avoid certain types of rrepetition.
Shefurther found that if the system were to keep track of the exactinformation that was provided previously, it could create atextthat contrasts or parallels an earlier one.
While McKeown'sanalysis was fairly detailed, no discourse history was main-rained in the implementation, and none of the suggestions forhow responses could be altered, if such a history existed, wereactually implemented or tested.
We are devising a way forexplanation strategies to make use of the information storedin a discourse history, and axe implementing these strategies.Finally, our work bears some resemblance to work in planadaptation \[17\].
Systems using plan adaptation often use ceztechniques to index a library of previously synthesized plans.However, plan ~d,_ptation is concerned with indexing plans sothat they can be retrieved and mused, perhaps with modifica-tion, in later situations.
Our emphasis i not on reusing plans,hut rather on exploiting prior plans as one of many knowledgesources affecting explanation generation.Re ferences1.
J.
A. Rosenhlum and J. D. Moore.
A field guide m contextualeffects in instructional dialogues.
Technical report, Universityof Pittsburgh, Computer Science Department, forthcoming.2.
J. D. Moore and C. L. Paris.
Planning text for advisory dia-logues.
In Proc.
of the 27th Annual Meeting of the ACL, pp.203-211, 1989.3.
J.D.
Moore.
A Reactive Approach to Explanation i Expert andAdvice-Giving Systems.
PhD thesis, University of California,Los Angeles, 1989.4.
B. G. Buchanan, J.D.
Moore, D. E. Forsythe, G. E. Banks, andS.
Ohlsson.
Involving patients in health care: Using medicalinformatics for explanation i the clinical setting.
In Proc.
ofthe 8ymposium onComputer Applicatioas inMedical Care, pp.510-514.
McGraw-Hill Inc., 1992.5.
G. Carenini and J. D. Moore.
Generating explanations incon-text.
Proceedings of the International Workshop on IntelligentUser Interfaces, pp.
175-182,1993.
ACM Press.6.
A. Lesgold, S. Lajoie, M. Bunzo, and G. Eggun.
Sherlock: Acoached practice nvironment for an electronics troubleshoot-ing job.
In Computer A~sisted lnstruction and lntelligent Tutor-ing Systems: Shared Goals and Complementary Approaches,pp.
201-238.
LEA HiUsdale, New Jersey, 1992.7.
J. D. Moore and W. R. Swarwut.
Poinling: A way towardexpianation dialogue.
In Proc.
of AAAI-90, pp.
457-464,1990.8.
B. J. Grosz and C. L. Sidner.
A~ntion, intention, and theswucture of discourse.
ComputationalLinguiatics, 12(3):175-204, 1986.9.
R. MacGregor and M. H. Burstein.
Using a description clas-sifter to enhance knowledge representation.
IEEE Expert,6(3):41-4, 1991.10.
K. D. Ashley.
Modeling Legal Argument: Reasoning withCases andHypotheticals.
MIT Press, Cambridge, MA, 1990.11.
R. Pokomy and S. Gort.
The evaluation of a real-worid in-swuctional system: Using technical experts asmters.
Technicalreport, Armstrong Laboratories, Brooks Air Force Base, 1990.12.
W. C. Mann and S. A. Thompson.
Rhetorical Structure The-ory: Towards a functional theory of text organization.
TEXT,8(3):243-281, 1988.13.
M. Elhadad.
FUF: the universal unifier user manual version5.0, October 1991.14.
R. Granville.
Controlling lexical substitution in computer textgeneration.
In Prac.
of COLING, pp.
381-384, 1984.15.
R. Dale.
Cooking up referring expressions.
In Proc.
ofthe 27thAnnual Meeting of the ACL, pp.
68-75, 1989.16.
K.R.
McKeown.
Text Generation: Using Discourse Strategiesand Focus Conswaints to Generate Natural Language Text.Cambridge University Press, Cambridge, England, 1985.17.
R. Alterman.
Adaptive planning.
In Stuart Shapiro, editor, TheEncyclopedia of Artificial Intelligence, pp.
5-15.
Wiley, NewYork, 1992.170
