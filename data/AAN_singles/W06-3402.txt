Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 8?14,New York City, New York, June 2006. c?2006 Association for Computational LinguisticsOff-Topic Detection in Conversational Telephone SpeechRobin Stewart and Andrea DanylukDepartment of Computer ScienceWilliams CollegeWilliamstown, MA 01267{06rss 2, andrea}@cs.williams.eduYang LiuDepartment of Computer ScienceUT DallasRichardson, TX 75080yangl@hlt.utdallas.eduAbstractIn a context where information retrieval isextended to spoken ?documents?
includ-ing conversations, it will be important toprovide users with the ability to seek in-formational content, rather than sociallymotivated small talk that appears in manyconversational sources.
In this paper wepresent a preliminary study aimed at auto-matically identifying ?irrelevance?
in thedomain of telephone conversations.
Weapply a standard machine learning algo-rithm to build a classifier that detects off-topic sections with better-than-chance ac-curacy and that begins to provide insightinto the relative importance of features foridentifying utterances as on topic or not.1 IntroductionThere is a growing need to index, search, summa-rize and otherwise process the increasing amount ofavailable broadcast news, broadcast conversations,meetings, class lectures, and telephone conversa-tions.
While it is clear that users have wide ranginggoals in the context of information retrieval, we as-sume that some will seek only credible informationabout a specific topic and will not be interested in thesocially-motivated utterances which appear through-out most conversational sources.
For these users,a search for information about weather should notreturn conversations containing small talk such as?Nice weather we?ve been having.
?In this paper we investigate one approach for auto-matically identifying ?irrelevance?
in the domain oftelephone conversations.
Our initial data consist ofconversations in which each utterance is labeled asbeing on topic or not.
We apply inductive classifierlearning algorithms to identify useful features andbuild classifiers to automatically label utterances.We begin in Section 2 by hypothesizing featuresthat might be useful for the identification of irrel-evant regions, as indicated by research on the lin-guistics of conversational speech and, in particular,small talk.
Next we present our data and discuss ourannotation methodology.
We follow this with a de-scription of the complete set of features and machinelearning algorithms investigated.
Section 6 presentsour results, including a comparison of the learnedclassifiers and an analysis of the relative utility ofvarious features.2 Linguistics of Conversational SpeechCheepen (Cheepen, 1988) posits that speakers havetwo primary goals in conversation: interactionalgoals in which interpersonal motives such as socialrank and trust are primary; and transactional goalswhich focus on communicating useful informationor getting a job done.
In a context where conversa-tions are indexed and searched for information, weassume in this paper that users will be interestedin the communicated information, rather than theway in which participants interact.
Therefore, weassume that utterances with primarily transactionalpurposes will be most important, while interactionalutterances can be ignored.Greetings and partings are the most predictable8type of interactional speech.
They consistently ap-pear at the beginning and end of conversations andfollow a fairly formulaic pattern of content (Laver,1981).
Thus we hypothesize that: Utterances nearthe beginning or end of conversations are less likelyto be relevant.Cheepen also defines speech-in-action regionsto be segments of conversation that are related tothe present physical world or the activity of chat-ting, e.g.
?What lovely weather.?
or ?It is so niceto see you.?
Since these regions mainly involveparticipants identifying their shared social situation,they are not likely to contain transactional content.Further, since speech-in-action segments are distin-guished by their focus on the present, we hypoth-esize that: Utterances with present tense verbs areless likely to be relevant.Finally, small talk that is not intended to demar-cate social hierarchy tends to be abbreviated, e.g.
?Nice day?
(Laver, 1981).
From this we hypothe-size that: Utterances lacking common helper wordssuch as ?it?, ?there?, and forms of ?to be?
are lesslikely to be relevant.3 Related WorkThree areas of related work in natural language pro-cessing have been particularly informative for ourresearch.First, speech act theory states that with each ut-terance, a conversant is committing an action, suchas questioning, critiquing, or stating a fact.
This isquite similar to the notion of transactional and inter-actional goals.
However, speech acts are generallyfocused on the lower level of breaking apart utter-ances and understanding their purpose, whereas weare concerned here with a coarser-grained notion ofrelevance.
Work closer to ours is that of Bates etal.
(Bates et al, 2005), who define meeting acts forrecorded meetings.
Of their tags, commentary ismost similar to our notion of relevance.Second, there has been research on generatingsmall talk in order to establish rapport between anautomatic system and human user (Bickmore andCassell, 2000).
Our work complements this by po-tentially detecting off-topic speech from the humanuser as an indication that the system should also re-spond with interactional language.Label UtteranceS 2: [LAUGH] Hi.S 2: How nice to meet you.S 1: It is nice to meet you too.M 2: We have a wonderful topic.M 1: Yeah.M 1: It?s not too bad.
[LAUGH]T 2: Oh, I ?
I am one hundred percent infavor of, uh, computers in the classroom.T 2: I think they?re a marvelous tool,educational tool.Table 1: A conversation fragment with annotations:(S)mall Talk, (M)etaconversation, and On-(T)opic.The two speakers are identified as ?1?
and ?2?.Third, off-topic detection can be viewed as a seg-mentation of conversation into relevant and irrele-vant parts.
Thus our work has many similarities totopic segmentation systems, which incorporate cuewords that indicate an abrupt change in topic (e.g.
?so anyway...?
), as well as long term variations inword occurrence statistics (Hearst, 1997; Reynar,1999; Beeferman et al, 1999, e.g.).
Our approachuses previous and subsequent sentences to approxi-mate these ideas, but might benefit from a more ex-plicitly segmentation-based strategy.4 DataIn our work we use human-transcribed conversa-tions from the Fisher data (LDC, 2004).
In each con-versation, participants have been given a topic to dis-cuss for ten minutes.
Despite this, participants oftentalk about subjects that are not at all related to the as-signed topic.
Therefore, a convenient way to defineirrelevance in conversations in this domain is seg-ments which do not contribute to understanding theassigned topic.
This very natural definition makesthe domain a good one for initial study; however,the idea can be readily extended to other domains.For example, broadcast debates, class lectures, andmeetings usually have specific topics of discussion.The primary transactional goal of participants inthe telephone conversations is to discuss the as-signed topic.
Since this goal directly involves theact of discussion itself, it is not surprising that par-ticipants often talk about the current conversation or9the choice of topic.
There are enough such segmentsthat we assign them a special region type: Metacon-versation.
The purely irrelevant segments we callSmall Talk, and the remaining segments are definedas On-Topic.
We define utterances as segments ofspeech that are delineated by periods and/or speakerchanges.
An annotated excerpt is shown in Table 1.For the experiments described in this paper, weselected 20 conversations: 4 from each of the topics?computers in education?, ?bioterrorism?, ?terror-ism?, ?pets?, and ?censorship?.
These topics werechosen randomly from the 40 topics in the Fishercorpus, with the constraint that we wanted to includetopics that could be a part of normal small talk (suchas ?pets?)
as well as topics which seem farther re-moved from small talk (such as ?censorship?
).Our selected data set consists of slightly morethan 5,000 utterances.
We had 2-3 human annota-tors label the utterances in each conversation, choos-ing from the 3 labels Metaconversation, Small Talk,and On-Topic.
On average, pairs of annotatorsagreed with each other on 86% of utterances.
Themain source of annotator disagreement was betweenSmall Talk and On-Topic regions; in most cases thisresulted from differences in opinion of when exactlythe conversation had drifted too far from the topic tobe relevant.For the 14% of utterances with mismatched la-bels, we chose the label that would be ?safest?
in theinformation retrieval context where small talk mightget discarded.
If any of the annotators thought agiven utterance was On-Topic, we kept it On-Topic.If there was a disagreement between Metaconver-sation and Small Talk, we used Metaconversation.Thus, a Small Talk label was only placed if all anno-tators agreed on it.5 Experimental Setup5.1 FeaturesAs indicated in Section 1, we apply machine learn-ing algorithms to utterances extracted from tele-phone conversations in order to learn classifiers forSmall Talk, Metaconversation, and On-Topic.
Werepresent utterances as feature vectors, basing ourselection of features on both linguistic insights andearlier text classification work.
As described in Sec-tion 2, work on the linguistics of conversationalSmall Talk Metaconv.
On-Topichi topic ,.
i ?
?s it youyeah this that?
dollars thehello so andoh is know?m what ain was wouldnmy about tobut talk likename for hishow me theywe okay oftexas do ?tthere phone hewell ah uhfrom times umare really puthere one justTable 2: The top 20 tokens for distinguishing eachcategory, as ranked by the feature quality measure(Lewis and Gale, 1994).speech (Cheepen, 1988; Laver, 1981) implies thatthe following features might be indicative of smalltalk: (1) position in the conversation, (2) the use ofpresent-tense verbs, and (3) a lack of common helperwords such as ?it?, ?there?, and forms of ?to be?.To model the effect of proximity to the beginningof the conversation, we attach to each utterance afeature that describes its approximate position in theconversation.
We do not include a feature for prox-imity to the end of the conversation because our tran-scriptions include only the first ten minutes of eachrecorded conversation.In order to include features describing verb tense,we use Brill?s part-of-speech tagger (Brill, 1992) .Each part of speech (POS) is taken to be a feature,whose value is a count of the number of occurrencesin the given utterance.To account for the words, we use a bag of wordsmodel with counts for each word.
We normalizewords from the human transcripts by converting ev-erything to lower case and tokenizing contractions10Features Valuesn word tokens for each word, # occurrencesstandard POS tags as in Penn Treebank for each tag, # occurrencesline number in conversation 0-4, 5-9, 10-19, 20-49, >49utterance type statement, question, fragmentutterance length (number of words) 1, 2, ..., 20, >20number of laughs laugh countn word tokens in previous 5 utterances for each word, total # occurrences in 5 previoustags from POS tagger, previous 5 for each tag, total # occurrences in 5 previousnumber of words, previous 5 total from 5 previousnumber of laughs, previous 5 total from 5 previousn word tokens, subsequent 5 utterances for each word, total # occ in 5 subsequenttags from POS tagger, subsequent 5 for each tag, total # occurrences in 5 subsequentnumber of words, subsequent 5 total from 5 subsequentnumber of laughs, subsequent 5 total from 5 subsequentTable 3: Summary of features that describe each utterance.and punctuation.
We rank the utility of words ac-cording to the feature quality measure presented in(Lewis and Gale, 1994) because it was devised forthe task of classifying similarly short fragments oftext (news headlines), rather than long documents.We then consider the top n tokens as features, vary-ing the number in different experiments.
Table 2shows the most useful tokens for distinguishing be-tween the three categories according to this metric.Additionally, we include as features the utterancetype (statement, question, or fragment), number ofwords in the utterance, and number of laughs in theutterance.Because utterances are long enough to classify in-dividually but too short to classify reliably, we notonly consider features of the current utterance, butalso those of previous and subsequent utterances.More specifically, summed features are calculatedfor the five preceding utterances and for the five sub-sequent utterances.
The number five was chosen em-pirically.It is important to note that there is some overlapin features.
For instance, the token ???
can be ex-tracted as one of the n word tokens by Lewis andGale?s feature quality measure; it is also tagged bythe POS tagger; and it is indicative of the utterancetype, which is encoded as a separate feature as well.However, redundant features do not make up a sig-nificant percentage of the overall feature set.Finally, we note that the conversation topic is nottaken to be a feature, as we cannot assume that con-versations in general will have such labels.
Thecomplete list of features, along with their possiblevalues, is summarized in Table 3.5.2 ExperimentsWe applied several classifier learning algorithms toour data: Naive Bayes, Support Vector Machines(SVMs), 1-nearest neighbor, and the C4.5 decisiontree learning algorithm.
We used the implementa-tions in the Weka package of machine learning al-gorithms (Witten and Frank, 2005), running the al-gorithms with default settings.
In each case, we per-formed 4-fold cross-validation, training on sets con-sisting of three of the conversations in each topic(15 conversations total) and testing on sets of the re-maining 1 from each topic (5 total).
Average train-ing set size was approximately 3800 utterances, ofwhich about 700 were Small Talk and 350 Metacon-versation.
The average test set size was 1270.6 Results6.1 Performance of a Learned ClassifierWe evaluated the results of our experiments ac-cording to three criteria: accuracy, error cost, andplausibility of the annotations produced.
In all11Algorithm % Accuracy Cohen?s KappaSVM 76.6 0.44C4.5 68.8 0.26k-NN 64.1 0.20Naive Bayes 58.9 0.27Table 4: Classification accuracy and Cohen?s Kappastatistic for each of the machine learning algorithmswe tried, using all features at the 100-words level.25 50 75 100 125 150 175747678808284number of words used as features0 200accuracy(%)7286 Inter-annotator agreementBaselineLine numbers onlyall featuresPart of speech tags onlyFigure 1: Classification results using SVMs withvarying numbers of words.cases our best results were obtained with the SVM.When evaluated on accuracy, the SVM models werethe only ones that exceeded a baseline accuracy of72.8%, which is the average percentage of On-Topicutterances in our data set.
Table 4 displays the nu-merical results using each of the machine learningalgorithms.Figure 1 shows the average accuracy obtainedwith an SVM classifier using all features describedin Section 5.1 except part-of-speech features (forreasons discussed below), and varying the numberof words considered.
While the best results were ob-tained at the 100-words level, all classifiers demon-strated significant improvement in accuracy over thebaseline.
The average standard deviation over the 4cross-validation runs of the results shown is 6 per-centage points.From a practical perspective, accuracy alone isS M T <?
classified as55% 7% 38% Small Talk21% 37% 42% Metaconv.8% 3% 89% On TopicTable 5: Confusion matrix for 100-word SVM clas-sifier.not an appropriate metric for evaluating our re-sults.
If the goal is to eliminate Small Talk regionsfrom conversations, mislabeling On-Topic regionsas Small Talk potentially results in the eliminationof useful material.
Table 5 shows a confusion ma-trix for an SVM classifier trained on a data set at the100-word level.
We can see that the classifier is con-servative, identifying 55% of the Small Talk, but in-correctly labeling On-Topic utterances as Small Talkonly 8% of the time.Finally, we analyzed (by hand) the test data anno-tated by the classifiers.
We found that, in general,the SVM classifiers annotated the conversations in amanner similar to the human annotators, transition-ing from one label to another relatively infrequentlyas illustrated in Table 1.
This is in contrast to the1-nearest neighbor classifiers, which tended to an-notate in a far more ?jumpy?
style.6.2 Relative Utility of FeaturesSeveral of the features we used to describe ourtraining and test examples were selected due to theclaims of researchers such as Laver and Cheepen.We were interested in determining the relative con-tributions of these various linguistically-motivatedfeatures to our learned classifiers.
Figure 1 and Table6 report some of our findings.
Using proximity to thebeginning of the conversation (?line numbers?)
as asole feature, the SVM classifier achieved an accu-racy of 75.6%.
This clearly verifies the hypothesisthat utterances near the beginning of the conversa-tion have different properties than those that follow.On the contrary, when we used only POS tagsto train the SVM classifier, it achieved an accuracythat falls exactly at the baseline.
Moreover, remov-ing POS features from the SVM classifier improvedresults (Table 6).
This may indicate that detect-ing off-topic categories will require focusing on thewords rather than the grammar of utterances.
On12Condition Accuracy KappaAll features 76.6 0.44No word features 75.0 0.19No line numbers 76.9 0.44No POS features 77.8 0.46No utterance type, length, 76.9 0.45or # laughsNo previous/next info 76.3 0.21Only word features 77.9 0.46Only line numbers 75.6 0.16Only POS features 72.8 0.00Only utterance type, length, 74.1 0.09and # laughsTable 6: Percent accuracy and Cohen?s Kappa statis-tic for the SVM at the 100-words level when featureswere left out or put in individually.the other hand, part of speech information is im-plicit in the words (for example, an occurrence of?are?
also indicates a present tense verb), so perhapslabeling POS tags does not add any new informa-tion.
It is also possible that some other detectionapproach and/or richer syntactic information (suchas parse trees) would be beneficial.Finally, the words with the highest feature qual-ity measure (Table 2) clearly refute most of the thirdlinguistic prediction.
Helper words like ?it?, ?there?,and ?the?
appear roughly evenly in each region type.Moreover, all of the verbs in the top 20 Small Talklist are forms of ?to be?
(some of them contractedas in ?I?m?
), while no ?to be?
words appear in thelist for On-Topic.
This is further evidence that dif-ferentiating off-topic speech depends deeply on themeaning of the words rather than on some more eas-ily extracted feature.7 Future WorkThere are many ways in which we plan to expandupon this preliminary study.
We are currently in theprocess of annotating more data and including ad-ditional conversation topics.
Other future work in-cludes:?
applying topic segmentation approaches to ourdata and comparing the results to those we haveobtained so far;?
investigating alternate approaches for detectingSmall Talk regions, such as smoothing with aHidden Markov Model;?
using semi-supervised and active learning tech-niques to better utilize the large amount of un-labeled data;?
running the experiments with automaticallygenerated (speech recognized) transcriptions,rather than the human-generated transcriptionsthat we have used to date.
Our expectation isthat such transcripts will contain more noiseand thus pose new challenges;?
including prosodic information in the featureset.AcknowledgementsThe authors would like to thank Mary Harper, BrianRoark, Jeremy Kahn, Rebecca Bates, and Joe Cruzfor providing invaluable advice and data.
We wouldalso like to thank the student volunteers at Williamswho helped annotate the conversation transcripts,as well as the 2005 Johns Hopkins CLSP summerworkshop, where this research idea was formulated.ReferencesRebecca Bates, Patrick Menning, Elizabeth Willingham,and Chad Kuyper.
2005.
Meeting Acts: A LabelingSystem for Group Interaction in Meetings.
August.Doug Beeferman, Adam Berger, and John Lafferty.1999.
Statistical Models for Text Segmentation.
Ma-chine Learning.Timothy Bickmore and Justine Cassell.
2000.
Howabout this weather?
: Social Dialogue with EmbodiedConversational Agents.
AAAI Fall Symposium on So-cially Intelligent Agents.Eric Brill.
1992.
A simple rule-based part of speech tag-ger.
Proc.
of the Third Conference on Applied NLP.Christine Cheepen.
1988.
The Predictability of InformalConversation.
Pinter Publishers, London.Marti A. Hearst.
1997.
TextTiling: Segmenting Text intoMultiparagraph Subtopics Passages.
ComputationalLinguistics, 23(1):33?64.John Laver, 1981.
Conversational routine, chapter Lin-guistic routines and politeness in greeting and parting,pages 289?304.
Mouton, The Hague.13LDC.
2004.
Fisher english training speech part 1, tran-scripts.
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2004T19.David D. Lewis and William A. Gale.
1994.
A sequentialalgorithm for training text classifiers.
Proc.
of SIGIR.Jeffrey C. Reynar.
1999.
Statistical models for topic seg-mentation.
Proceedings of the 37th Annual Meetingof the ACL.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques, SecondEdition.
Morgan Kaufmann, San Francisco.14
