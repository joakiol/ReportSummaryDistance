Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 598?606,Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational LinguisticsApplying the semantics of negation to SMT through n-best list re-rankingFederico FancelluCentre for Global Intelligent ContentSchool of Computer Science and StatisticsTrinity College Dublinffancellu@cngl.ieBonnie WebberSchool of InformaticsUniversity of EdinburghEdinburgh, UK, EH8 9ABbonnie@inf.ed.ac.ukAbstractAlthough the performance of SMT sys-tems has improved over a range of differ-ent linguistic phenomena, negation has notyet received adequate treatment.Previous works have considered the prob-lem of translating negative data as one ofdata sparsity (Wetzel and Bond (2012)) orof structural differences between sourceand target language with respect to theplacement of negation (Collins et al.(2005)).
This work starts instead from thequestions ofwhat is meant by negation andwhat makes a good translation of negation.These questions have led us to explore theuse of semantics of negation in SMT ?specifically, identifying core semantic el-ements of negation (cue, event and scope)in a source-side dependency parse and re-ranking hypotheses on the n-best list pro-duced after decoding according to the ex-tent to which an hypothesis realises theseelements.The method shows considerable improve-ment over the baseline as measured byBLEU scores and Stanford?s entailment-based MT evaluation metric (Pad?
et al.
(2009)).1 IntroductionTranslating negation is a task that involves morethan the correct rendering of a negation marker inthe target sentence.
For instance, translating Italydid not defeat France in 1909 differs from trans-lating Italy defeated France in 1909, or Francedid not defeat Italy in 1909, or Italy did not con-quer France in 1909.
These examples show thattranslating negation also involves placing in theright position the semantic arguments as well asthe event directly negated.
Moreover, if the sourcesentence was uttered in response to the statement Ithink Italy defeated France in 1911, where the fo-cus is the temporal argument in 1911, one can seethat the system should not lose track of the focusof negation when producing the hypothesis trans-lation.Although negation must be appropriately ren-dered to ensure correct representation of the se-mantics of the source sentence in the machine out-put, only some of the efforts to improve the transla-tion of negation-bearing sentences in SMT addressthe problem.Wetzel and Bond (2012) considered negation asa problem of data sparsity and so attempted to en-rich the training data with negative paraphrasesof positive sentences.
Collins et al.
(2005) andLi et al.
(2009) both addressed differences in theplacement of negation in source and target texts,by re-ordering negative elements in the source sen-tence to better resemble their position in the corre-sponding target text.
Although these approachesshow improvement over the baseline, neither con-siders negation as a linguistic phenomenon withspecific characteristics.This we do in the work presented here: We iden-tify the elements of negation that an MT systemhas to reproduce and then devise a strategy to en-sure that they are output correctly.
These elementswe take to be the cue, event and scope of nega-tion1.
Unlike previous works, we first validatethe hypothesis that if the top-ranked translation inthe n-best list does not replicate elements of nega-tion from the source, there may be a more accuratetranslation after decoding, somewhere else on then-best list.
If the hypothesis is false, then problemsin the translation of negation lie elsewhere.1Due to its ambiguity and the fact that it is already in-cluded in the scope, we have ignored the focus of negation.That does not mean it may not be important to correctly re-produce the focus; there might be cases where, although notfully-capturing the scope, we want to translate correctly thepart that is directly negated or emphasised.598We use dependency parsing as a basis for N-best list re-ranking.
Dependencies between lexicalelements appear to encode all elements of nega-tion, offering a robust and easily-applicable wayto extract negation-related information from a sen-tence.
We carry out our exploration of N-best listre-ranking in two steps:?
First, an oracle translation is computed bothto assess the validity of the approach and tounderstand the maximal extent to which itcould possibly enhance performance.
An or-acle translation is obtained by performing n-best list re-ranking using reference transla-tions as a gold-standard.To avoid the problem in Chinese-English Hi-erarchical Phrase-Based (HPB) translation ofloss and/or misplacement of negation-relatedelements when hierarchical phrases are built,Chinese source sentences are first broken intosub-clauses Yang and Xue (2012), then trans-lated and finally ?stitched?
back together forevaluation.?
Standard n-best list re-ranking is then per-formed using only source-side information.Hypotheses are re-ranked according to thedegree of similarity between the negation-related elements in the hypotheses and thosein the source sentence.
Here the correspon-dence between source and target text is es-tablished through lexical translation probabil-ities output after training.Results of this method show that n-best list rerank-ing does lead to a significant improvement inBLEU score.
However, BLEU says nothing aboutsemantics, so we also evaluate the method usingStanford?s entailment basedMTmetrics Pad?
et al.
(2009), and also show improvement here.
In thefinal section of the paper, we note the value of de-veloping a custom metric that actually assesses thecomponents of negation.2 Related worksNegation has been a widely discussed topic out-side the field of SMT, with recent works focusedmainly on automatic detection of negation.
Blancoand Moldovan (2011) have established the distri-bution of negative cues and the syntactic structuresin which they appear in the WSJ section of thePenn Treebank, as a basis for automatically de-tecting scope and focus of negation using simpleheuristics.Machine-learning has been used by systemsparticipating in the *SEM 2012 shared task onautomatically detecting the scope and focus ofnegation.
Those systems with the best F1 mea-sures (Chowdhury and Mahbub (2012), Read et al.
(2012) and Lapponi et al.
(2012) all use a mix-ture of SVM (Support Vector Machines) and CRF.Their performance improves significantly whensyntactic features are also considered.
In partic-ular, Lapponi et al.
(2012) use features extractedfrom a dependency parse to guide their system todetect the correct scope boundary.In translation, only few efforts have focussed onthe problem of translating negation.
Wetzel andBond (2012) treat it as resulting from data spar-sity.
To remedy this, they enrich their Japanese-to-English training set with negative paraphrasesof positive sentences, where negation is insertedas a ?handle?
to the main verb after a sentence isparsed using MSR (Minimal Recursion SemanticsCopestake et al.
(2005)).
Results show that BLEUscore improves on a test sub-set containing onlynegative sentences when extra negative data is ap-pended to the original training data and the lan-guage model is enriched as well.
However, systemperformance deteriorates on both the original testset and on positive sentences.
Moreover, generat-ing paraphrases with negation expressed only onthe main verb does not allow to fully capture thevarious ways negation can be expressed.Other works considered negation in the frame-work of clause restructuring.
Collins et al.
(2005)pre-process the German source to resemble thestructure of English while Li et al.
(2009) triedto swap the order of the words in a Chinese sen-tence to resemble Korean.
Rosa (2013) takes apost-processing approach to negation in English-Czech translation, ?fixing?
common errors such asthe loss of a negation cue by either generating themorphologically negative form of the relevant verb(if the verb has such a form) or prefixing the verbwith the negative prefix ne.
Despite the improve-ments, these approaches do not really address whatis special about negation.3 Decomposing negationCorrectly translating negation involves more thanplacing a negative marker in the right position.
Wefollow Blanco andMoldovan (2011) in decompos-ing negation into three main components:599?
a negation cue, including negative markers,affixes and all the words or multiwords unitsthat inherently express negation.?
a negation event, i.e.
the event that is directlynegated.
Events can be either verbs (e.g.
?I donot go to the cinema) or adjectives (e.g.
?Heis not clever?).?
a negation scope, i.e.
the part of the state-ment whose meaning is negated (Blanco andMoldovan, 2011, 229).
The scope contains allthose words that, if negated, would make thestatement true.
We follow here the guidelinesfor annotating negative data released duringthe *SEM 2012 Shared Task Morante et al.
(2011) for a more detailed understanding onwhat to consider part of the negation scope.In addition to these three components, formal se-manticists identify a negation focus, i.e.
the partof the scope that is directly negated or more em-phasized.
Focus is the most difficult part to detectsince it is the most ambiguous.
In the sentence ?hedoes not want to go to school by car?
the speakeremphasized the fact that ?he does not want to goto school by car?
or that ?he does not want to goto school by car?
(but he wants to go somewhereelse) or that ?he does not want to go to school bycar?
(but by other means of transportation).Translating negation is therefore a matter of en-suring that the cue is present, that its attachment tothe corresponding event follows language-specificrules and that all the elements included in the scopeare placed in the right order.
Correctly reproduc-ing the focus is left for future works.4 Methodology4.1 N-best list re-rankingN-best list re-ranking is used in SMT to deal withsentence-level phenomena whose locality goes be-yond n-grams or single hierarchical rules.
It in-volves re-ranking the list of target-language hy-potheses produced by decoding, using additionalfeatures extracted from the source sentence.
In thecase of negation, N-best list re-ranking allows us toassess whether a system is able to correctly trans-late the elements of negation, while failing to placethe best hypothesis on these grounds at the top ofthe n-best list.The current work follows the same approach asother n-best list re-rankers (Och et al.
(2004); Spe-cia et al.
(2008); Apidianaki et al.
(2012)) but usingnegation as the additional feature.
Negation is heredefined as the degree of overlap of cue, event andscope between the hypothesis translation and thesource sentence.Following Hasan et al.
(2007), we use an n-bestlist of 10000 sentences but we do not initially tunethe negation feature using MERT or interpolate itwith other features.
This is because in order to as-sess the degree of overlap between the scope inthe source and the hypothesis sentence, a n-grambased score is used which conveys the same in-formation as that of the language model score inthe log-linear model.
Moreover, our re-ranking ex-ploits lexical translation probabilities, thereby re-sembling a simple translation model.4.2 Extract negation using dependencyparsingThe degree of overlap between the source sentenceand the hypothesis translation is measured in termsof the overlap between their negation cue, eventand scope.
These must therefore be correctly ex-tracted.
Dependency parsing provides an efficientway to do so, with several advantages:?
Dependency parsing encodes the notions ofcue and event as the dependant and the headrespectively of a ?neg?
relation.
Scope canbe approximated through recursive retrievalof all the descendants of the verb-event.
Thefollowing example shows how these elementsare extracted from the dependency parse:Peter andconjconjMary did not buynsubjpunctobjauxnega blue cardetamod.nsubj(buy-6, and-2) , conj(and-2, Peter-1), conj (and-2, Mary-3),aux(buy-6, did-4), neg(buy-6, not-5), root(ROOT-0, buy-6),det(car-9, a-7), amod(car-9, blue-8) , dobj(buy-6, car-9)The ?neg?
dependency relation conveys boththe negation cue (not-5) and the negationevent (buy-6) of the sentence ?Peter andMarydid not buy a blue car?.
An approximate scopecan be recovered by following the path fromthe event (included) to the terminal nodes andcollecting all the lexical elements along theway.Also in the case of a sentence containinga subordinate clause, dependency parsing is600able to correctly capture the latter as part ofthe scope given that the relative pronoun de-pends directly on the event of the main clause.On the other hand, recursion from the negatedevent excludes coordinate clauses that are notconsidered part of the scope, given that theevent is a dependant of the connective.One problem with this method is that it isunable to capture the entire scope when thehead is nominal.
For instance, ?no reasonswere given?, the ?neg?
dependency holds be-tween no and reasons but it needs to climbthe hierarchy further to get to the verbal headgiven.
The same holds for negation on ob-ject nominals.
We leave this to future work(along with affix-conveyed negation), need-ing to show first that the current approach isa good one.?
A dependency parser can be developed forany language for which a Treebank or Prop-bank is available for training.
This extendsthe range of source languages to which the ap-proach can be applied.4.3 Computing an oracle translationIn order to test the validity of the method and toassess its maximum contribution, we first use itwith an oracle translation in which n-best list re-ranking relies on a comparison with negation cue,event and scope extracted from the reference trans-lation(s), here assumed to correctly contain all el-ements of negation.Each hypothesis on the n-best list is assignedan overlap score with these reference-translation-derived elements, and the hypothesis with thehighest score is re-ranked at the top and used forevaluation.The overlap score is obtained by summing upthree sub-scores: (i) the cue overlap score mea-sures how many cues in reference are repre-sented in the hypothesis, normalised by the num-ber of cues in the reference; (ii) the event over-lap score measures how many events in the refer-ence are represented in the hypothesis, normalisedby the number of the events in the reference;and (iii) the scope overlap score is a weighted n-gram overlap between hypothesis scope and ref-erence scope, with higher weight for higher-ordern-grams.
Given less-than-perfect machine out-put, breaking down the score into subscores al-lows us to consider different degrees of correct-ness in translating negation.
When multiple ref-erence translations are available, the hypothesis ismatched with each, and only the best score takeninto consideration.4.4 Re-ranking using lexical translationprobabilitiesAfter the oracle translation is computed, tradi-tional n-best list re-ranking is performed relyingon source side information only.
We then bridgethe gap between source and target language usinglexical translation probabilities to render source-side cue, event, scope into the target language.
Re-ranking involves three separate steps:?
The source sentence is parsed and dependen-cies extracted.
Since the present work tack-les Chinese-to-English translation, we had toenhance the representation of negative depen-dencies in the Chinese source, where only theadverb ?
bu4 is flagged as ?neg?
dependant.To do this, we follow the same intuition usedto isolate negation-bearing sentences in thetest set (see section 5).?
To obtain a rough translation of cue, event andscope in the target language, the top ten lex-ical translation probabilities for each lexicalitem, available in the lexical translations (inorder of probability) table output after train-ing, are considered.?
Hypotheses in the n-best list are re-scored tak-ing into consideration the information above.Scoring cue and event is straightforward; thewords for the cue and the event are assignedthe lexical probability of being the translationof the cue and the event in the Chinese sen-tence by looking up the lexical translation ta-ble.
If the cue or the event do not figure astranslations of the negation cue and event inthe Chinese sentence, a score of 0 is assignedto them.The scope is instead scored by loopingthrough the words in the hypothesis; for eachsuch hypothesis word, the process identifieswhich source-side scopeword it is most likelyto be the translation of.
If no scope can be re-trieved, a score of 0 is given for scope match-ing.
For each word the best translation proba-bility is taken into account and these are thensummed together to score how likely is the601scope in the hypothesis to be the translationof the scope in the source.5 SystemA hierarchical phrase based model (HPBM) wastrained on a corpus of 625000 (?
18.200.000 to-kens) length-ratio filtered sentences.
56949 sen-tences (?
9.11%) of the Chinese side and 48941sentences (?
7.83%) of the English side of thetraining set were found to include at least oneinstance of negation.
2500 sentences were in-stead included in the dev set to tune the log-linearmodel using the MERT algorithm.
3725 sentencesfrom the Multiple-Translation Chinese Corpus 2.0(LDC2002T01) were used as test set.
The testset comes divided into four sub-sets; in this paperthese sub-sets are referred as test set 1 to 4.
Thesource side was tokenized using the Stanford Chi-neseWord Segmenter (Tseng et al.
(2005)) and en-coded in ?utf-8?
so to serve as input to the system.In order to focus on the problem of translating neg-ative data, the 563 sentence pairs containing nega-tion were extracted from the original test set.
Thistest set constitutes the true baseline improvementswill be measured upon.
Reducing the numberof test sentences also eases the computation loadwhen involved in dependency parsing on 10.000sentences in each n-best list.
Negated sentencewere isolated by means of both regular expres-sions and dependency parsing; this is because, aspointed out above, the Chinese side does not flagall negative dependencies as such.26 Results6.1 BaselineBLEU scores for the baseline systems are given inTable 1, where the negative subset is compared tothe original (all sentences) and only positive sen-tences conditions.Table 2 shows instead the result for the nega-tive baseline across three different metrics.
Alongwith the BLEU scores, we also took into consider-ation an entailment-based MT evaluation metric,2While the English dependency parser is able to iden-tify almost all negative markers and their dependencies, theChinese dependency parser here deployed (the Stanford Chi-nese Dependency parser) only captures sentences contain-ing the adverb ?bu4.
For this reason, we exploited thelist of negation adverbs included in the Chinese Propbank(LDC2005T23) documentation and look for each of them viaregular expressions.
Moreover we also looked for words con-taining ?
as component since they are most likely to carrynegative meaning (e.g.
?
?, ?not-long?
).the RTE score3 Pad?
et al.
(2009).
The RTE scoreassesses to what extent the hypothesis entails thereference translation across a wide variety of se-mantic and syntactic features.
Another reason wechose this metric is because it contains a feature forpolarity as well as features to check the degree ofsimilarity between the syntactic tree and the depen-dencies between hypothesis and reference transla-tion, the latter being what we used to recover theelements of negation.
We expect this metric to givea further insight on the quality of the machine out-put.Baseline results are in line with the results ofWetzel and Bond (2012), where there is a drop inBLEU scores between positive and negative sen-tences, and between the overall test set and the onecontaining negative data only.When analysing the results from the baseline, wenoticed that words were being deleted or movedinappropriately when the hierarchy of phrases wasbeing built.
This might be detrimental to the trans-lation of negation since elements might end up out-side the correct negation scope.
The following ex-ample illustrates this problem.
(1) Source : ?
?
?
?
??
??
??
?????????????????????
?
?
?
?
?
?
?:::?:::?:::?::?:::?::?:::?::?:::?::?:???????????????
?Baseline : Investment in fixed assetsinvestment in the three years ,????
?yuan , floor is not high , ?
the formerborder city , road , and communicationconditions have not been completed ,will not change .Due to unrestricted rule application, mainlyguided by the language model, the underlinedclauses containing negation on the source sidehave been deleted.
Moreover, the polarity of thelast clause, positive in the source, is changed intonegative in the target translation, most probablybecause a negative cue is moved from somewhereelse in the sentence.In order to solve these two problems, we exploitthe syntactic feature of the Chinese language ofgrouping clauses into a single sentence.
We fol-low the intuition of Yang and Xue (2012) in using3The entailment-based MT metric also outputs anRTE+MT score, where the RTE score is interpolated with tra-ditional MT metrics (e.g.
BLEU, NIST, TER, METEOR).602Test set Original Positive Negative Orig.
?
Neg.
Pos.
?
Neg.Test 1 32.92 32.95 29.64 - 3.28 - 3.31Test 2 25.88 26.21 24.31 - 1.57 - 1.9Test 3 19.00 19.78 16.11 - 2.89 - 3.67Test 4 28.64 29.71 27.14 - 1.5 - 2.57Average 26.61 27.16 24.3 -2.31 - 2.96Table 1: BLEU scores for the baseline system.
The difference in BLEU scores between the positive, theoriginal and the negative conditions is also reported.Test set BLEU RTE RTE+MTTest 1 29.64 0.22 0.837Test 2 24.31 0.307 0.732Test 3 16.11 -0.603 -0.095Test 4 27.14 -0.25 0.33Average 24.3 -0.08 0.451Table 2: BLEU, RTE and RTE+MT scores for the baseline system as tested on the sub-set only containingnegative sentences.commas to guide the segmentation of a sentenceinto constituent sub-clauses.
Moreover, we alsouse other syntactic clues to segment the test sen-tences, including quotes in direct quotation, to re-duce the size of the test sentences.The constituent sub-clauses are then translated sin-gularly and ?stitched?
back together into the origi-nal sentence for evaluation.6.2 Re-ranking resultsTable 3 and 4 shows the performance of the systemwhen n-best list re-ranking is performed.
Table 3shows the results for the oracle translation, whileTable 4 the results for actual n-best list re-ranking.Two conditions are here compared: a short con-dition where test sentences are chunked into con-stituent sub-clauses prior to translation and a orig-inal (orig.)
condition where no chunking is per-formed.Results shows considerable improvements overthe baseline when re-ranking is performed ?an average BLEU score improvement of 1.75points.
As hypothesised, we get further improve-ment when Chinese source sentences are translatedthrough their constituent sub-clauses ?
an aver-age BLEU score improvement of 3.07 points.
Asimilar improvement is shown in Table 5 wherethe original test sets comprising both positive andnegative sentences are considered.
This proves thevalidity of n-best list re-ranking using syntactic de-pendencies as a method to improve the quality ofthe translation of negative data.
The following ex-ample shows the improvement in detail:(2) Source : ??
????
?
??
?
????
???
??
?
??
??
,????
??
??
?
??
??
?
??
???
??
??
?
??
?
??Ex.
reference : When asked about in-flation, he said : ?The overall inflationrate in the Euro area still exhibits a downtrend.
At present, there is no sign to showeconomic development in the medium termwill create risks of price instability?.Baseline : on the inflation he saidthe euro dropped overall mediumterm economic development will inno signs of inflation risks .Oracle : on inflation , saidthe euro dropped overallthere is no signs of economic developmentin the medium term prices will not risks .Source-only re-ranking : on inflation ,said the euro dropped overall there is nosigns of economic development in themedium term will price risks .In (2) the baseline translation shows the problemsmentioned earlier, where movement leaves nega-tion with the wrong scope, changing the overallmeaning of the sentence.
Decomposing sentencesinto constituent clauses and then re-ranking thetranslations permits negation to retain its correctscope so that the meaning is the same as the refer-ence sentence.7 ConclusionWe have presented an approach to translating neg-ative sentences that is based on the semantics ofnegation and applying it to n-best list re-ranking.603BLEU RTE RTE+MT1Baseline 29.64 0.22 0.837Orig.
33.73 (+4.09) 0.64 (+0.42) 1.396 (+0.559)Short 35.39 (+5.75) 0.74 (+ 0.52) 1.508 (+ 0.671)2Baseline 24.31 0.307 0.732Orig.
27.43 (+3.12) 0.457 (+0.15) 1.12 (+0.388)Short 27.29 (+3.18) 0.6 (+ 0.293) 1.175 (+ 0.443)3Baseline 16.11 -0.603 -0.095Orig.
17.97 (+1.86) 0.356 (+ 0.959) 0.958 (+ 1.053)Short 18.19 (+2.08) 0.243 (+ 0.84) 0.78 (+ 0.875)4Baseline 27.14 -0.25 0.33Orig.
31.97 (+ 4.83) 0.42 (+ 0.67) 1.024 (+ 0.694)Short 32.50 (+ 5.36) 0.57 (+ 0.82) 1.36 (+ 1.03)Avg.
Baseline 24.3 - 0.08 0.45Orig.
27.78 (+ 3.48) 0.47 (+ 0.55) 1.12 (+ 0.67)Short 29.09 (+ 4.79) 0.52 (+ 0.60) 1.23 (+ 0.78)Table 3: BLEU, RTE and RTE+MT scores for the oracle translation.
The test sets evaluated are markedfrom 1 to 4.
Improvement over the baseline is reported.BLEU RTE RTE+MT1Baseline 29.64 0.22 0.837Orig.
31.96 (+ 2.32) 0.62 (+ 0.4) 1.382 (+ 0.545)Short 34.20 (+ 4.56) 0.68 (+0.46) 1.452 (+ 0.615)2Baseline 24.31 0.307 0.732Orig.
26.65 (+2.34) 0.48 (+ 0.173) 1.159 (+ 0.427)Short 26.94 (+ 2.63) 0.49 (+0.183) 1.172 (+ 0.44)3Baseline 16.11 -0.603 -0.095Orig.
17.20 (+ 1.09) 0.35 (+ 0.953) 0.935 (+ 1.03)Short 17.41 (+ 1.3) 0.226 (+0.829) 0.87 (+ 0.965)4Baseline 27.14 -0.25 0.33Orig.
28.42 (+ 1.28) 0.302 (+ 0.552) 1.01 (+ 0.68)Short 30.96 (+ 3.82) 0.55 (+ 0.8) 1.36 (+ 1.03)Avg.
Baseline 24.3 -0.08 0.45Orig.
26.05 (+ 1.75) 0.438 (+ 0.518) 1.12 (+ 0.669)Short 27.37 (+ 3.07) 0.51 (+ 0.59) 1.21 (+ 0.759)Table 4: BLEU, RTE and RTE+MT scores for the sentences re-ranked using source side informationonly.
Improvement over the baseline is reported.Dependency parsing and lexical translations arehere considered as easily applicable methods toextract and translate negation related informationacross different language pairs.
Improvementsacross different automatic evaluationmetrics showthat the above method is useful when translatingnegative data.
In particular, the entailment-basedRTE metric is here used as an alternative to theBLEU score given the semantic and syntactic fea-tures assessed, polarity included.
Given the pos-itive results, one can conclude that the problemis neither one of data sparsity nor syntactic mis-match.We have also demonstrated that when dealingwith sentences containing multiple sub-clauses,translating the constituent sub-clauses separatelyand then stitching them back together before eval-uation avoids the loss or excessive movement ofnegation during decoding.
This was evident in thecase of Chinese and HPBMs but there is no reasonwhy this does not hold also for other languages.8 Future worksGiven the validity of the present approach, futureworks should be focused in extending it to differ-ent language pairs.
Also, it would be useful to re-search more in detail into language typology andtry to devise a method which is language indepen-dent.Although leading to an overall improvement, n-best list re-ranking does not always guarantee aperfect translation.
It is therefore useful in the fu-ture to investigate ways of always ensuring that then-best list contains a good translation of negationby, for instance, enriching the hypotheses list withparaphrases.
Post-editing rules can also be consid-ered to further correct the final output.Finally, although we can show considerable im-provement with respect to both n-gram overlap604BLEU RTE RTE+MT1Baseline 32.92 -0.49 -0.073Orig.
33.54 (+ 0.62) -0.38 (+ 0.11) 0.046 (+ 0.119)Short 34.02 (+ 1.1) -0.33 (+ 0.16) 0.057 (+ 0.13)2Baseline 25.88 -2.173 -1.726Orig.
26.3 (+ 0.42) -1.851 (+ 0.322) -1.376 (+ 0.35)Short 26.42 (+ 0.54) -1.80 (+ 0.373) -1.339 (+ 0.387)3Baseline 19.00 -0.897 -0.644Orig.
19.20 (+ 0.20) -0.731 (+ 0.166) -0.474 (+ 0.17)Short 19.23 (+ 0.23) -0.743 (+ 0.154) -0.488 (+ 0.156)4Baseline 28.64 -3.43 -3.16Orig.
29.56 (+ 0.92) -3.01 (+ 0.42) -2.72 (+ 0.44)Short 29.95 (+ 1.31) -2.94 (+ 0.49) -2.67 (+ 0.49)Avg.
Baseline 26.61 -1.747 -1.4Orig.
27.15(+ 0.54) -1.488 (+ 0.259) -1.131 (+ 0.269)Short 27.41(+ 0.8) -1.453 (+ 0.294) -1.11 (+ 0.29)Table 5: BLEU, RTE and RTE+MT scores for the the original test set, containing both positive and neg-ative sentences re-ranked using source side information only.
Improvement over the baseline is reported.with the reference translation (BLEU score) andoverall semantic similarity, it remains to be de-termined the extent to which the machine outputcaptures elements of negation present in the ref-erence translation and on which system improve-ment depends.
A more targeted metric is needed,that can effectively determine the extent to whichcue, event and scope are captured in hypothesistranslation as compared to the reference gold stan-dard.
That is the subject of current and futurework (Fancellu (2013)), which should implementthe new customized metric to include measures ofprecision, recall and a F1 measure.ReferencesApidianaki, M., Wisniewski, G., Sokolov, A.,Max, A., and Yvon, F. (2012).
Wsd for n-bestreranking and local language modeling in smt.In Proceedings of the Sixth Workshop on Syntax,Semantics and Structure in Statistical Transla-tion, pages 1?9.
Association for ComputationalLinguistics.Blanco, E. and Moldovan, D. I.
(2011).
Some is-sues on detecting negation from text.
In FLAIRSConference.Chowdhury, M. and Mahbub, F. (2012).
Fbk: Ex-ploiting phrasal and contextual clues for nega-tion scope detection.
In Proceedings of theFirst Joint Conference on Lexical and Computa-tional Semantics-Volume 1: Proceedings of themain conference and the shared task, and Vol-ume 2: Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation, pages 340?346.
Association for Computational Linguistics.Collins, M., Koehn, P., and Ku?erov?, I.
(2005).Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd annualmeeting on association for computational lin-guistics, pages 531?540.
Association for Com-putational Linguistics.Copestake, A., Flickinger, D., Pollard, C., and Sag,I.
A.
(2005).
Minimal recursion semantics: Anintroduction.
Research on Language and Com-putation, 3(2-3):281?332.Fancellu, F. (2013).
Improving the performanceof chinese-to-english hierarchical phrase basedmodels (hpbm) on negative data using n-best listre-ranking.
Master?s thesis, School of Informat-ics - University of Edinburgh.Hasan, S., Zens, R., and Ney, H. (2007).
Are verylarge n-best lists useful for smt?
In HumanLanguage Technologies 2007: The Conferenceof the North American Chapter of the Associ-ation for Computational Linguistics; Compan-ion Volume, Short Papers, pages 57?60.
Asso-ciation for Computational Linguistics.Lapponi, E., Velldal, E., ?vrelid, L., and Read, J.(2012).
Uio 2: sequence-labeling negation us-ing dependency features.
In Proceedings of theFirst Joint Conference on Lexical and Computa-tional Semantics-Volume 1: Proceedings of themain conference and the shared task, and Vol-ume 2: Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation, pages 319?327.
Association for Computational Linguistics.Li, J.-J., Kim, J., Kim, D.-I., and Lee, J.-H. (2009).Chinese syntactic reordering for adequate gen-eration of korean verbal phrases in chinese-to-605korean smt.
In Proceedings of the Fourth Work-shop on Statistical Machine Translation, pages190?196.
Association for Computational Lin-guistics.Morante, R., Schrauwen, S., and Daelemans, W.(2011).
Annotation of negation cues and theirscope: Guidelines v1.
Technical report, 0.
Tech-nical report, University of Antwerp.
CLIPS:Computational Linguistics & Psycholinguisticstechnical report series.Och, F. J., Gildea, D., Khudanpur, S., Sarkar, A.,Yamada, K., Fraser, A., Kumar, S., Shen, L.,Smith, D., Eng, K., et al.
(2004).
A smorgasbordof features for statistical machine translation.
InHLT-NAACL, pages 161?168.Pad?, S., Galley, M., Jurafsky, D., and Manning,C.
D. (2009).
Textual entailment features formachine translation evaluation.
In Proceedingsof the Fourth Workshop on Statistical MachineTranslation, pages 37?41.
Association for Com-putational Linguistics.Read, J., Velldal, E., ?vrelid, L., and Oepen, S.(2012).
Uio 1: Constituent-based discrimina-tive ranking for negation resolution.
InProceed-ings of the First Joint Conference on Lexicaland Computational Semantics-Volume 1: Pro-ceedings of the main conference and the sharedtask, and Volume 2: Proceedings of the Sixth In-ternational Workshop on Semantic Evaluation,pages 310?318.
Association for ComputationalLinguistics.Rosa, R. (2013).
Automatic post-editing of phrase-basedmachine translation outputs.
Master?s the-sis, Institute of Formal and Applied Linguistics,Charles University, Prague.Specia, L., Sankaran, B., and Nunes, M. d. G. V.(2008).
N-best reranking for the efficient inte-gration of word sense disambiguation and statis-tical machine translation.
InComputational Lin-guistics and Intelligent Text Processing, pages399?410.
Springer.Tseng, H., Chang, P., Andrew, G., Jurafsky, D.,and Manning, C. (2005).
A conditional randomfield word segmenter for sighan bakeoff 2005.In Proceedings of the Fourth SIGHAN Work-shop on Chinese Language Processing, volume171.
Jeju Island, Korea.Wetzel, D. and Bond, F. (2012).
Enriching parallelcorpora for statistical machine translation withsemantic negation rephrasing.
In Proceedingsof the Sixth Workshop on Syntax, Semantics andStructure in Statistical Translation, pages 20?29.
Association for Computational Linguistics.Yang, Y. and Xue, N. (2012).
Chinese commadisambiguation for discourse analysis.
In Pro-ceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics: LongPapers-Volume 1, pages 786?794.
Associationfor Computational Linguistics.606
