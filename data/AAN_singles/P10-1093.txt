Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 907?916,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsCompositional Matrix-Space Models of LanguageSebastian RudolphKarlsruhe Institute of TechnologyKarlsruhe, Germanyrudolph@kit.eduEugenie GiesbrechtFZI Forschungszentrum InformatikKarlsuhe, Germanygiesbrecht@fzi.deAbstractWe propose CMSMs, a novel type ofgeneric compositional models for syntac-tic and semantic aspects of natural lan-guage, based on matrix multiplication.
Weargue for the structural and cognitive plau-sibility of this model and show that it isable to cover and combine various com-mon compositional NLP approaches rang-ing from statistical word space models tosymbolic grammar formalisms.1 IntroductionIn computational linguistics and information re-trieval, Vector Space Models (Salton et al, 1975)and its variations ?
such as Word Space Models(Sch?tze, 1993), Hyperspace Analogue to Lan-guage (Lund and Burgess, 1996), or Latent Se-mantic Analysis (Deerwester et al, 1990) ?
havebecome a mainstream paradigm for text represen-tation.
Vector Space Models (VSMs) have beenempirically justified by results from cognitive sci-ence (G?rdenfors, 2000).
They embody the distri-butional hypothesis of meaning (Firth, 1957), ac-cording to which the meaning of words is definedby contexts in which they (co-)occur.
Dependingon the specific model employed, these contextscan be either local (the co-occurring words), orglobal (a sentence or a paragraph or the whole doc-ument).
Indeed, VSMs proved to perform well in anumber of tasks requiring computation of seman-tic relatedness between words, such as synonymyidentification (Landauer and Dumais, 1997), auto-matic thesaurus construction (Grefenstette, 1994),semantic priming, and word sense disambiguation(Pad?
and Lapata, 2007).Until recently, little attention has been paidto the task of modeling more complex concep-tual structures with such models, which consti-tutes a crucial barrier for semantic vector modelson the way to model language (Widdows, 2008).An emerging area of research receiving more andmore attention among the advocates of distribu-tional models addresses the methods, algorithms,and evaluation strategies for representing compo-sitional aspects of language within a VSM frame-work.
This requires novel modeling paradigms,as most VSMs have been predominantly usedfor meaning representation of single words andthe key problem of common bag-of-words-basedVSMs is that word order information and therebythe structure of the language is lost.There are approaches under way to work outa combined framework for meaning representa-tion using both the advantages of symbolic anddistributional methods.
Clark and Pulman (2007)suggest a conceptual model which unites sym-bolic and distributional representations by meansof traversing the parse tree of a sentence and ap-plying a tensor product for combining vectors ofthe meanings of words with the vectors of theirroles.
The model is further elaborated by Clark etal.
(2008).To overcome the aforementioned difficultieswith VSMs and work towards a tight integra-tion of symbolic and distributional approaches,we propose a Compositional Matrix-Space Model(CMSM) which employs matrices instead of vec-tors and makes use of matrix multiplication as theone and only composition operation.The paper is structured as follows: We start byproviding the necessary basic notions in linear al-gebra in Section 2.
In Section 3, we give a for-mal account of the concept of compositionality,introduce our model, and argue for the plausibil-ity of CMSMs in the light of structural and cogni-tive considerations.
Section 4 shows how commonVSM approaches to compositionality can be cap-tured by CMSMs while Section 5 illustrates thecapabilities of our model to likewise cover sym-bolic approaches.
In Section 6, we demonstrate907how several CMSMs can be combined into onemodel.
We provide an overview of related workin Section 7 before we conclude and point out av-enues for further research in Section 8.2 PreliminariesIn this section, we recap some aspects of linearalgebra to the extent needed for our considerationsabout CMSMs.
For a more thorough treatise werefer the reader to a linear algebra textbook (suchas Strang (1993)).Vectors.
Given a natural number n, an n-dimensional vector v over the reals can be seenas a list (or tuple) containing n real numbersr1, .
.
.
, rn ?
R, written v = (r1 r2 ?
?
?
rn).Vectors will be denoted by lowercase bold fontletters and we will use the notation v(i) to referto the ith entry of vector v. As usual, we writeRn to denote the set of all n-dimensional vectorswith real entries.
Vectors can be added entry-wise, i.e., (r1 ?
?
?
rn) + (r?1 ?
?
?
r?n) = (r1 +r?1 ?
?
?
rn +r?n).
Likewise, the entry-wise prod-uct (also known as Hadamard product) is definedby (r1 ?
?
?
rn)  (r?1 ?
?
?
r?n) = (r1 ?r?1 ?
?
?
rn ?r?n).Matrices.
Given two real numbers n, m, an n?mmatrix over the reals is an array of real numberswith n rows and m columns.
We will use capitalletters to denote matrices and, given a matrix Mwe will write M(i, j) to refer to the entry in the ithrow and the jth column:M =??????????????????????????????????
?M(1, 1) M(1, 2) ?
?
?
M(1, j) ?
?
?
M(1,m)M(2, 1) M(2, 2).........M(i, 1) M(i, j).........M(n, 1) M(1, 2) ?
?
?
?
?
?
?
?
?
M(n,m)??????????????????????????????????
?The set of all n ?
m matrices with real num-ber entries is denoted by Rn?m.
Obviously, m-dimensional vectors can be seen as 1 ?
m matri-ces.
A matrix can be transposed by exchangingcolumns and rows: given the n ?
m matrix M, itstransposed version MT is a m ?
n matrix definedby MT (i, j) = M( j, i).Linear Mappings.
Beyond being merely array-like data structures, matrices correspond to certaintype of functions, so-called linear mappings, hav-ing vectors as in- and output.
More precisely, ann ?
m matrix M applied to an m-dimensional vec-tor v yields an n-dimensional vector v?
(written:vM = v?)
according tov?
(i) =m?j=1v( j) ?
M(i, j)Linear mappings can be concatenated, givingrise to the notion of standard matrix multiplica-tion: we write M1M2 to denote the matrix thatcorresponds to the linear mapping defined by ap-plying first M1 and then M2.
Formally, the matrixproduct of the n?
l matrix M1 and the l?m matrixM2 is an n ?
m matrix M = M1M2 defined byM(i, j) =l?k=1M1(i, k) ?
M2(k, j)Note that the matrix product is associative (i.e.,(M1M2)M3 = M1(M2M3) always holds, thusparentheses can be omitted) but not commutative(M1M2 = M2M1 does not hold in general, i.e., theorder matters).Permutations.
Given a natural number n, a per-mutation on {1 .
.
.
n} is a bijection (i.e., a map-ping that is one-to-one and onto) ?
: {1 .
.
.
n} ?
{1 .
.
.
n}.
A permutation can be seen as a ?reorder-ing scheme?
on a list with n elements: the elementat position i will get the new position ?
(i) in thereordered list.
Likewise, a permutation can be ap-plied to a vector resulting in a rearrangement ofthe entries.
We write ?n to denote the permutationcorresponding to the n-fold application of ?
and?
?1 to denote the permutation that ?undoes?
?.Given a permutation ?, the corresponding per-mutation matrix M?
is defined byM?
(i, j) ={1 if ?
( j) = i,0 otherwise.Then, obviously permuting a vector accordingto ?
can be expressed in terms of matrix multipli-cation as well as we obtain for any vector v ?
Rn:?
(v) = vM?Likewise, iterated application (?n) and the in-verses ?
?n carry over naturally to the correspond-ing notions in matrices.9083 Compositionality and MatricesThe underlying principle of compositional seman-tics is that the meaning of a sentence (or a wordphrase) can be derived from the meaning of itsconstituent tokens by applying a composition op-eration.
More formally, the underlying idea canbe described as follows: given a mapping [[ ? ]]
:?
?
S from a set of tokens (words) ?
into somesemantical space S (the elements of which we willsimply call ?meanings?
), we find a semantic com-position operation ./: S?
?
S mapping sequencesof meanings to meanings such that the meaning ofa sequence of tokens ?1?2 .
.
.
?n can be obtainedby applying ./ to the sequence [[?1]][[?2]] .
.
.
[[?n]].This situation qualifies [[ ? ]]
as a homomorphismbetween (?
?, ?)
and (S, ./) and can be displayed asfollows:?1[[ ?
]]concatenation ?
''?2[[ ?
]]((?
?
?
?n[[ ?
]]))?1?2 .
.
.
?n[[ ?
]][[?1]]composition ./66[[?2]] 55?
?
?
[[?n]] 55[[?1?2 .
.
.
?n]]A great variety of linguistic models are sub-sumed by this general idea ranging from purelysymbolic approaches (like type systems and cate-gorial grammars) to rather statistical models (likevector space and word space models).
At the firstglance, the underlying encodings of word seman-tics as well as the composition operations differsignificantly.
However, we argue that a great vari-ety of them can be incorporated ?
and even freelyinter-combined ?
into a unified model where thesemantics of simple tokens and complex phrasesis expressed by matrices and the composition op-eration is standard matrix multiplication.More precisely, in Compositional Marix-SpaceModels, we have S = Rn?n, i.e.
the semanticalspace consists of quadratic matrices, and the com-position operator ./ coincides with matrix multi-plication as introduced in Section 2.
In the follow-ing, we will provide diverse arguments illustratingthat CMSMs are intuitive and natural.3.1 Algebraic Plausibility ?Structural Operation PropertiesMost linear-algebra-based operations that havebeen proposed to model composition in languagemodels are associative and commutative.
Thereby,they realize a multiset (or bag-of-words) seman-tics that makes them insensitive to structural dif-ferences of phrases conveyed through word order.While associativity seems somewhat acceptableand could be defended by pointing to the stream-like, sequential nature of language, commutativityseems way less justifiable, arguably.As mentioned before, matrix multiplication isassociative but non-commutative, whence we pro-pose it as more adequate for modeling composi-tional semantics of language.3.2 Neurological Plausibility ?Progression of Mental StatesFrom a very abstract and simplified perspective,CMSMs can also be justified neurologically.Suppose the mental state of a person at one spe-cific moment in time can be encoded by a vector vof numerical values; one might, e.g., think of thelevel of excitation of neurons.
Then, an externalstimulus or signal, such as a perceived word, willresult in a change of the mental state.
Thus, theexternal stimulus can be seen as a function beingapplied to v yielding as result the vector v?
thatcorresponds to the persons mental state after re-ceiving the signal.
Therefore, it seems sensible toassociate with every signal (in our case: token ?)
arespective function (a linear mapping, representedby a matrix M = [[?]]
that maps mental states tomental states (i.e.
vectors v to vectors v?
= vM).Consequently, the subsequent reception of in-puts ?, ??
associated to matrices M and M?will transform a mental vector v into the vector(vM)M?
which by associativity equals v(MM?
).Therefore, MM?
represents the mental state tran-sition triggered by the signal sequence ???.
Nat-urally, this consideration carries over to sequencesof arbitrary length.
This way, abstracting fromspecific initial mental state vectors, our semanticspace S can be seen as a function space of mentaltransformations represented by matrices, wherebymatrix multiplication realizes subsequent execu-tion of those transformations triggered by the in-put token sequence.9093.3 Psychological Plausibility ?Operations on Working MemoryA structurally very similar argument can be pro-vided on another cognitive explanatory level.There have been extensive studies about humanlanguage processing justifying the hypothesis ofa working memory (Baddeley, 2003).
The men-tal state vector can be seen as representation of aperson?s working memory which gets transformedby external input.
Note that matrices can per-form standard memory operations such as storing,deleting, copying etc.
For instance, the matrixMcopy(k,l) defined byMcopy(k,l)(i, j) ={1 if i = j , l or i = k, j = l,0 otherwise.applied to a vector v, will copy its kth entry to thelth position.
This mechanism of storage and inser-tion can, e.g., be used to simulate simple forms ofanaphora resolution.4 CMSMs Encode Vector Space ModelsIn VSMs numerous vector operations have beenused to model composition (Widdows, 2008),some of the more advanced ones being related toquantum mechanics.
We show how these com-mon composition operators can be modeled byCMSMs.1 Given a vector composition operation./: Rn?Rn ?
Rn, we provide a surjective function?./ : Rn ?
Rn??n?
that translates the vector rep-resentation into a matrix representation in a waysuch that for all v1, .
.
.
vk ?
Rn holdsv1 ./ .
.
.
./ vk = ?
?1./ (?./(v1) .
.
.
?./(vk))where ?./(vi)?./(v j) denotes matrix multiplicationof the matrices assigned to vi and v j.4.1 Vector AdditionAs a simple basic model for semantic composi-tion, vector addition has been proposed.
Thereby,tokens ?
get assigned (usually high-dimensional)vectors v?
and to obtain a representation of themeaning of a phrase or a sentence w = ?1 .
.
.
?k,the vector sum of the vectors associated to the con-stituent tokens is calculated: vw =?ki=1 v?i .1In our investigations we will focus on VSM composi-tion operations which preserve the format (i.e.
which yield avector of the same dimensionality), as our notion of composi-tionality requires models that allow for iterated composition.In particular, this rules out dot product and tensor product.However the convolution product can be seen as a condensedversion of the tensor product.This kind of composition operation is subsumedby CMSMs; suppose in the original model, a token?
gets assigned the vector v?, then by defining?+(v?)
=??????????????
?1 ?
?
?
0 0.... .
....0 1 0v?
1???????????????
(mapping n-dimensional vectors to (n+1)?
(n+1)matrices), we obtain for a phrase w = ?1 .
.
.
?k?
?1+ (?+(v?1) .
.
.
?+(v?k )) = v?1 + .
.
.
+ v?k = vw.Proof.
By induction on k. For k = 1, we havevw = v?
= ?
?1+ (?+(v?1)).
For k > 1, we have?
?1+ (?+(v?1) .
.
.
?+(v?k?1)?+(v?k ))= ?
?1+ (?+(?
?1+ (?+(v?1) .
.
.
?+(v?k?1)))?+(v?k ))i.h.= ?
?1+ (?+(?k?1i=1 v?i)?+(v?k ))=??1+???????????????????????????
?1 ?
?
?
0 0.... .
....0 1 0?k?1i=1 v?i (1)?
?
?
?k?1i=1 v?i (n) 1???????????????????????????
?1 ?
?
?
0 0.... .
....0 1 0v?k (1)?
?
?
v?k (n) 1????????????????????????????=??1+?????????????
?1 ?
?
?
0 0.... .
....0 1 0?ki=1v?i (1)?
?
?
?ki=1v?i (n) 1?????????????
?=k?i=1v?iq.e.d.24.2 Component-wise MultiplicationOn the other hand, the Hadamard product (alsocalled entry-wise product, denoted by ) has beenproposed as an alternative way of semanticallycomposing token vectors.By using a different encoding into matrices,CMSMs can simulate this type of composition op-eration as well.
By letting?
(v?)
=?????????????????v?
(1) 0 ?
?
?
00 v?(2)....
.
.
00 ?
?
?
0 v?(n)????????????????
?,we obtain an n?n matrix representation for which?
?1 (?
(v?1) .
.
.
?
(v?k )) = v?1  .
.
.
v?k = vw.4.3 Holographic Reduced RepresentationsHolographic reduced representations as intro-duced by Plate (1995) can be seen as a refinement2The proofs for the respective correspondences for  and~ as well as the permutation-based approach in the followingsections are structurally analog, hence, we will omit them forspace reasons.910of convolution products with the benefit of pre-serving dimensionality: given two vectors v1, v2 ?Rn, their circular convolution product v1 ~ v2 isagain an n-dimensional vector v3 defined byv3(i + 1) =n?1?k=0v1(k + 1) ?
v2((i ?
k mod n) + 1)for 0 ?
i ?
n?1.
Now let ?~(v) be the n?n matrixM withM(i, j) = v(( j ?
i mod n) + 1).In the 3-dimensional case, this would result in?~(v(1) v(2) v(3)) =?????????
?v(1) v(2) v(3)v(3) v(1) v(2)v(2) v(3) v(1)?????????
?Then, it can be readily checked that?
?1~ (?~(v?1) .
.
.
?~(v?k )) = v?1 ~ .
.
.
~ v?k = vw.4.4 Permutation-based ApproachesSahlgren et al (2008) use permutations on vec-tors to account for word order.
In this approach,given a token ?m occurring in a sentence w =?1 .
.
.
?k with predefined ?uncontextualized?
vec-tors v?1 .
.
.
v?k , we compute the contextualizedvector vw,m for ?m byvw,m = ?1?m(v?1) + .
.
.
+ ?k?m(v?k ),which can be equivalently transformed into?1?m(v?1 + ?(.
.
.
+ ?
(v?k?1 + (?
(v?k ))) .
.
.
)).Note that the approach is still token-centered, i.e.,a vector representation of a token is endowed withcontextual representations of surrounding tokens.Nevertheless, this setting can be transferred to aCMSM setting by recording the position of the fo-cused token as an additional parameter.
Now, byassigning every v?
the matrix??(v?)
=???????????????0M?...0v?
1??????????????
?we observe that forMw,m := (M??)m?1??
(v?1) .
.
.
??
(v?k )we haveMw,m =??????????????
?0Mk?m?...0vw,m 1??????????????
?,whence ??1?((M??)m?1??
(v?1) .
.
.
??
(v?k ))= vw,m.5 CMSMs Encode Symbolic ApproachesNow we will elaborate on symbolic approaches tolanguage, i.e., discrete grammar formalisms, andshow how they can conveniently be embedded intoCMSMs.
This might come as a surprise, as the ap-parent likeness of CMSMs to vector-space modelsmay suggest incompatibility to discrete settings.5.1 Group TheoryGroup theory and grammar formalisms based ongroups and pre-groups play an important rolein computational linguistics (Dymetman, 1998;Lambek, 1958).
From the perspective of our com-positionality framework, those approaches employa group (or pre-group) (G, ?)
as semantical space Swhere the group operation (often written as multi-plication) is used as composition operation ./.According Cayley?s Theorem (Cayley, 1854),every group G is isomorphic to a permutationgroup on some set S .
Hence, assuming finite-ness of G and consequently S , we can encodegroup-based grammar formalisms into CMSMs ina straightforward way by using permutation matri-ces of size |S | ?
|S |.5.2 Regular LanguagesRegular languages constitute a basic type of lan-guages characterized by a symbolic formalism.We will show how to select the assignment [[ ?
]]for a CMSM such that the matrix associated to atoken sequence exhibits whether this sequence be-longs to a given regular language, that is if it isaccepted by a given finite state automaton.
Asusual (cf.
e.g., Hopcroft and Ullman (1979)) wedefine a nondeterministic finite automaton A =(Q,?,?,QI,QF) with Q = {q0, .
.
.
, qn?1} being theset of states, ?
the input alphabet, ?
?
Q??
?Q thetransition relation, and QI and QF being the sets ofinitial and final states, respectively.911Then we assign to every token ?
?
?
the n ?
nmatrix [[?]]
= M withM(i, j) ={1 if (qi, ?, q j) ?
?,0 otherwise.Hence essentially, the matrix M encodes all statetransitions which can be caused by the input ?.Likewise, for a word w = ?1 .
.
.
?k ?
?
?, thematrix Mw := [[?1]] .
.
.
[[?k]] will encode all statetransitions mediated by w. Finally, if we definevectors vI and vF byvI(i) ={1 if qi ?
QI,0 otherwise,vF(i) ={1 if qi ?
QF,0 otherwise,then we find that w is accepted by A exactly ifvIMwvTF ?
1.5.3 The General Case: Matrix GrammarsMotivated by the above findings, we now define ageneral notion of matrix grammars as follows:Definition 1 Let ?
be an alphabet.
A matrixgrammar M of degree n is defined as the pair?
[[ ?
]], AC?
where [[ ? ]]
is a mapping from ?
to n?nmatrices and AC = {?v?1, v1, r1?, .
.
.
, ?v?m, vm, rm?
}with v?1, v1, .
.
.
, v?m, vm ?
Rn and r1, .
.
.
, rm ?
Ris a finite set of acceptance conditions.
The lan-guage generated by M (denoted by L(M)) con-tains a token sequence ?1 .
.
.
?k ?
??
exactly ifv?i[[?1]] .
.
.
[[?k]]vTi ?
ri for all i ?
{1, .
.
.
,m}.
Wewill call a language L matricible if L = L(M) forsome matrix grammarM.Then, the following proposition is a direct con-sequence from the preceding section.Proposition 1 Regular languages are matricible.However, as demonstrated by the subsequentexamples, also many non-regular and even non-context-free languages are matricible, hinting atthe expressivity of our grammar model.Example 1 We defineM?
[[ ?
]], AC?
with?
= {a, b, c} [[a]] =??????????????
?3 0 0 00 1 0 00 0 3 00 0 0 1???????????????
[[b]] =??????????????
?3 0 0 00 1 0 00 1 3 01 0 0 1???????????????
[[c]] =??????????????
?3 0 0 00 1 0 00 2 3 02 0 0 1??????????????
?AC = { ?
(0 0 1 1), (1 ?1 0 0), 0?,?
(0 0 1 1), (?1 1 0 0), 0?
}Then L(M) contains exactly all palindromes from{a, b, c}?, i.e., the words d1d2 .
.
.
dn?1dn for whichd1d2 .
.
.
dn?1dn = dndn?1 .
.
.
d2d1.Example 2 We defineM = ?
[[ ?
]], AC?
with?
= {a, b, c} [[a]]=?????????????????
?1 0 0 0 0 00 0 0 0 0 00 0 0 0 0 00 0 0 2 0 00 0 0 0 1 00 0 0 0 0 1??????????????????[[b]]=?????????????????
?0 1 0 0 0 00 1 0 0 0 00 0 0 0 0 00 0 0 1 0 00 0 0 0 2 00 0 0 0 0 1??????????????????[[c]]=?????????????????
?0 0 0 0 0 00 0 1 0 0 00 0 1 0 0 00 0 0 1 0 00 0 0 0 1 00 0 0 0 0 2?????????????????
?AC = { ?
(1 0 0 0 0 0), (0 0 1 0 0 0), 1?,?
(0 0 0 1 1 0), (0 0 0 1 ?1 0), 0?,?
(0 0 0 0 1 1), (0 0 0 0 1 ?1), 0?,?
(0 0 0 1 1 0), (0 0 0 ?1 0 1), 0?
}Then L(M) is the (non-context-free) language{ambmcm | m > 0}.The following properties of matrix grammarsand matricible language are straightforward.Proposition 2 All languages characterized by aset of linear equations on the letter counts are ma-tricible.Proof.
Suppose ?
= {a1, .
.
.
an}.
Given a word w,let xi denote the number of occurrences of ai in w.A linear equation on the letter counts has the formk1x1 + .
.
.
+ knxn = k(k, k1, .
.
.
, kn ?
R)Now define [[ai]] = ?+(ei), where ei is the ithunit vector, i.e.
it contains a 1 at he ith position and0 in all other positions.
Then, it is easy to see thatw will be mapped to M = ?+(x1 ?
?
?
xn).
Dueto the fact that en+1M = (x1 ?
?
?
xn 1) we canenforce the above linear equation by defining theacceptance conditionsAC = { ?en+1, (k1 .
.
.
kn ?
k), 0?,?
?en+1, (k1 .
.
.
kn ?
k), 0?
}.q.e.d.Proposition 3 The intersection of two matriciblelanguages is again a matricible language.Proof.
This is a direct consequence of the con-siderations in Section 6 together with the observa-tion, that the new set of acceptance conditions istrivially obtained from the old ones with adapteddimensionalities.
q.e.d.912Note that the fact that the language {ambmcm |m > 0} is matricible, as demonstrated in Ex-ample 2 is a straightforward consequence of thePropositions 1, 2, and 3, since the language inquestion can be described as the intersection ofthe regular language a+b+c+ with the languagecharacterized by the equations xa ?
xb = 0 andxb ?
xc = 0.
We proceed by giving another ac-count of the expressivity of matrix grammars byshowing undecidability of the emptiness problem.Proposition 4 The problem whether there is aword which is accepted by a given matrix gram-mar is undecidable.Proof.
The undecidable Post correspondenceproblem (Post, 1946) is described as follows:given two lists of words u1, .
.
.
, un and v1, .
.
.
, vnover some alphabet ?
?, is there a sequence of num-bers h1, .
.
.
, hm (1 ?
h j ?
n) such that uh1 .
.
.
uhm =vh1 .
.
.
vhm?We now reduce this problem to the emptinessproblem of a matrix grammar.
W.l.o.g., let ??
={a1, .
.
.
, ak}.
We define a bijection # from ???
to Nby#(an1an2 .
.
.
anl) =l?i=1(ni ?
1) ?
k(l?i)Note that this is indeed a bijection and that forw1,w2 ?
??
?, we have#(w1w2) = #(w1) ?
k|w2 | + #(w2).Now, we defineM as follows:?
= {b1, .
.
.
bn} [[bi]] =?????????
?k|ui | 0 00 k|vi | 0#(ui) #(vi) 1?????????
?AC = { ?
(0 0 1), (1 ?
1 0), 0?,?
(0 0 1), (?1 1 0), 0?
}Using the above fact about # and a simple induc-tion on m, we find that[[ah1]] .
.
.
[[ahm]] =?????????
?k|uh1...uhm | 0 00 k|vh1...vhm | 0#(uh1 .
.
.uhm) #(vh1 .
.
.vhm) 1?????????
?Evaluating the two acceptance conditions, wefind them satisfied exactly if #(uh1 .
.
.
uhm) =#(vh1 .
.
.
vhm).
Since # is a bijection, this is thecase if and only if uh1 .
.
.
uhm = vh1 .
.
.
vhm .
There-foreM accepts bh1 .
.
.
bhm exactly if the sequenceh1, .
.
.
, hm is a solution to the given Post Corre-spondence Problem.
Consequently, the questionwhether such a solution exists is equivalent tothe question whether the language L(M) is non-empty.
q.e.d.These results demonstrate that matrix grammarscover a wide range of formal languages.
Never-theless some important questions remain open andneed to be clarified next:Are all context-free languages matricible?
Weconjecture that this is not the case.3 Note that thisquestion is directly related to the question whetherLambek calculus can be modeled by matrix gram-mars.Are matricible languages closed under concatena-tion?
That is: given two arbitrary matricible lan-guages L1, L2, is the language L = {w1w2 | w1 ?L1,w2 ?
L2} again matricible?
Being a propertycommon to all language types from the Chomskyhierarchy, answering this question is surprisinglynon-trivial for matrix grammars.In case of a negative answer to one of the abovequestions it might be worthwhile to introduce anextended notion of context grammars to accom-modate those desirable properties.
For example,allowing for some nondeterminism by associatingseveral matrices to one token would ensure closureunder concatenation.How do the theoretical properties of matrix gram-mars depend on the underlying algebraic struc-ture?
Remember that we considered matrices con-taining real numbers as entries.
In general, ma-trices can be defined on top of any mathemati-cal structure that is (at least) a semiring (Golan,1992).
Examples for semirings are the naturalnumbers, boolean algebras, or polynomials withnatural number coefficients.
Therefore, it wouldbe interesting to investigate the influence of thechoice of the underlying semiring on the prop-erties of the matrix grammars ?
possibly non-standard structures turn out to be more appropri-ate for capturing certain compositional languageproperties.6 Combination of Different ApproachesAnother central advantage of the proposed matrix-based models for word meaning is that severalmatrix models can be easily combined into one.3For instance, we have not been able to find a matrixgrammar that recognizes the language of all well-formedparenthesis expressions.913Again assume a sequence w = ?1 .
.
.
?k oftokens with associated matrices [[?1]], .
.
.
, [[?k]]according to one specific model and matrices([?1]), .
.
.
, ([?k]) according to another.Then we can combine the two models into one{[ ? ]}
by assigning to ?i the matrix{[?i]} =??????????????????????????????
?0 ?
?
?
0[[?i]].... .
.0 00 ?
?
?
0.... .
.
([?i])0 0??????????????????????????????
?By doing so, we obtain the correspondence{[?1]} .
.
.
{[?k]} =??????????????????????????????
?0 ?
?
?
0[[?1]] .
.
.
[[?k]].... .
.0 00 ?
?
?
0.... .
.
([?1]) .
.
.
([?k])0 0??????????????????????????????
?In other words, the semantic compositions belong-ing to two CMSMs can be executed ?in parallel.
?Mark that by providing non-zero entries for the up-per right and lower left matrix part, informationexchange between the two models can be easilyrealized.7 Related WorkWe are not the first to suggest an extension ofclassical VSMs to matrices.
Distributional mod-els based on matrices or even higher-dimensionalarrays have been proposed in information retrieval(Gao et al, 2004; Antonellis and Gallopoulos,2006).
However, to the best of our knowledge, theapproach of realizing compositionality via matrixmultiplication seems to be entirely original.Among the early attempts to provide more com-pelling combinatory functions to capture word or-der information and the non-commutativity of lin-guistic compositional operation in VSMs is thework of Kintsch (2001) who is using a more so-phisticated addition function to model predicate-argument structures in VSMs.Mitchell and Lapata (2008) formulate seman-tic composition as a function m = f (w1,w2,R,K)where R is a relation between w1 and w2 and Kis additional knowledge.
They evaluate the modelwith a number of addition and multiplication op-erations for vector combination on a sentence sim-ilarity task proposed by Kintsch (2001).
Widdows(2008) proposes a number of more advanced vec-tor operations well-known from quantum mechan-ics, such as tensor product and convolution, tomodel composition in vector spaces.
He showsthe ability of VSMs to reflect the relational andphrasal meanings on a simplified analogy task.Giesbrecht (2009) evaluates four vector compo-sition operations (+, , tensor product, convolu-tion) on the task of identifying multi-word units.The evaluation results of the three studies are notconclusive in terms of which vector operation per-forms best; the different outcomes might be at-tributed to the underlying word space models; e.g.,the models of Widdows (2008) and Giesbrecht(2009) feature dimensionality reduction while thatof Mitchell and Lapata (2008) does not.
In thelight of these findings, our CMSMs provide thebenefit of just one composition operation that isable to mimic all the others as well as combina-tions thereof.8 Conclusion and Future WorkWe have introduced a generic model for compo-sitionality in language where matrices are associ-ated with tokens and the matrix representation of atoken sequence is obtained by iterated matrix mul-tiplication.
We have given algebraic, neurological,and psychological plausibility indications in favorof this choice.
We have shown that the proposedmodel is expressive enough to cover and combinea variety of distributional and symbolic aspects ofnatural language.
This nourishes the hope that ma-trix models can serve as a kind of lingua franca forcompositional models.This having said, some crucial questions remainbefore CMSMs can be applied in practice:How to acquire CMSMs for large token sets andspecific purposes?
We have shown the valueand expressivity of CMSMs by providing care-fully hand-crafted encodings.
In practical cases,however, the number of token-to-matrix assign-ments will be too large for this manual approach.Therefore, methods to (semi-)automatically ac-quire those assignments from available data are re-quired.
To this end, machine learning techniquesneed to be investigated with respect to their ap-plicability to this task.
Presumably, hybrid ap-proaches have to be considered, where parts of914the matrix representation are learned whereas oth-ers are stipulated in advance guided by externalsources (such as lexical information).In this setting, data sparsity may be overcomethrough tensor methods: given a set T of tokenstogether with the matrix assignment [[]] : T ?Rn?n, this datastructure can be conceived as a 3-dimensional array (also known as tensor) of sizen?n?|T |wherein the single token-matrices can befound as slices.
Then tensor decomposition tech-niques can be applied in order to find a compactrepresentation, reduce noise, and cluster togethersimilar tokens (Tucker, 1966; Rendle et al, 2009).First evaluation results employing this approach tothe task of free associations are reported by Gies-brecht (2010).How does linearity limit the applicability ofCMSMs?
In Section 3, we justified our model bytaking the perspective of tokens being functionswhich realize mental state transitions.
Yet, us-ing matrices to represent those functions restrictsthem to linear mappings.
Although this restric-tion brings about benefits in terms of computabil-ity and theoretical accessibility, the limitations in-troduced by this assumption need to be investi-gated.
Clearly, certain linguistic effects (like a-posteriori disambiguation) cannot be modeled vialinear mappings.
Instead, we might need somein-between application of simple nonlinear func-tions in the spirit of quantum-collapsing of a "su-perposed" mental state (such as the winner takesit all, survival of the top-k vector entries, and soforth).
Thus, another avenue of further research isto generalize from the linear approach.AcknowledgementsThis work was supported by the German ResearchFoundation (DFG) under the Multipla project(grant 38457858) as well as by the German Fed-eral Ministry of Economics (BMWi) under theproject Theseus (number 01MQ07019).References[Antonellis and Gallopoulos2006] Ioannis Antonellisand Efstratios Gallopoulos.
2006.
Exploringterm-document matrices from matrix models in textmining.
CoRR, abs/cs/0602076.
[Baddeley2003] Alan D. Baddeley.
2003.
Workingmemory and language: An overview.
Journal ofCommunication Disorder, 36:198?208.
[Cayley1854] Arthur Cayley.
1854.
On the theory ofgroups as depending on the symbolic equation ?n =1.
Philos.
Magazine, 7:40?47.
[Clark and Pulman2007] Stephen Clark and StephenPulman.
2007.
Combining symbolic and distribu-tional models of meaning.
In Proceedings of theAAAI Spring Symposium on Quantum Interaction,Stanford, CA, 2007, pages 52?55.
[Clark et al2008] Stephen Clark, Bob Coecke, andMehrnoosh Sadrzadeh.
2008.
A compositional dis-tributional model of meaning.
In Proceedings ofthe Second Symposium on Quantum Interaction (QI-2008), pages 133?140.
[Deerwester et al1990] Scott Deerwester, Susan T. Du-mais, George W. Furnas, Thomas K. Landauer, andRichard Harshman.
1990.
Indexing by latent se-mantic analysis.
Journal of the American Societyfor Information Science, 41:391?407.
[Dymetman1998] Marc Dymetman.
1998.
Group the-ory and computational linguistics.
J. of Logic, Lang.and Inf., 7(4):461?497.
[Firth1957] John R. Firth.
1957.
A synopsis of linguis-tic theory 1930-55.
Studies in linguistic analysis,pages 1?32.
[Gao et al2004] Kai Gao, Yongcheng Wang, and ZhiqiWang.
2004.
An efficient relevant evaluation modelin information retrieval and its application.
In CIT?04: Proceedings of the The Fourth InternationalConference on Computer and Information Technol-ogy, pages 845?850.
IEEE Computer Society.
[G?rdenfors2000] Peter G?rdenfors.
2000.
Concep-tual Spaces: The Geometry of Thought.
MIT Press,Cambridge, MA, USA.
[Giesbrecht2009] Eugenie Giesbrecht.
2009.
In searchof semantic compositionality in vector spaces.
InSebastian Rudolph, Frithjof Dau, and Sergei O.Kuznetsov, editors, ICCS, volume 5662 of Lec-ture Notes in Computer Science, pages 173?184.Springer.
[Giesbrecht2010] Eugenie Giesbrecht.
2010.
Towardsa matrix-based distributional model of meaning.
InProceedings of Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, Student Research Workshop.
ACL.
[Golan1992] Jonathan S. Golan.
1992.
The theory ofsemirings with applications in mathematics and the-oretical computer science.
Addison-Wesley Long-man Ltd.[Grefenstette1994] Gregory Grefenstette.
1994.
Ex-plorations in Automatic Thesaurus Discovery.Springer.915[Hopcroft and Ullman1979] John E. Hopcroft and Jef-frey D. Ullman.
1979.
Introduction to AutomataTheory, Languages and Computation.
Addison-Wesley.
[Kintsch2001] Walter Kintsch.
2001.
Predication.Cognitive Science, 25:173?202.
[Lambek1958] Joachim Lambek.
1958.
The mathe-matics of sentence structure.
The American Math-ematical Monthly, 65(3):154?170.
[Landauer and Dumais1997] Thomas K. Landauer andSusan T. Dumais.
1997.
Solution to Plato?s prob-lem: The latent semantic analysis theory of acqui-sition, induction and representation of knowledge.Psychological Review, (104).
[Lund and Burgess1996] Kevin Lund and Curt Burgess.1996.
Producing high-dimensional semantic spacesfrom lexical co-occurrence.
Behavior ResearchMethods, Instrumentation, and Computers, 28:203?208.
[Mitchell and Lapata2008] Jeff Mitchell and MirellaLapata.
2008.
Vector-based models of seman-tic composition.
In Proceedings of ACL-08: HLT,pages 236?244.
ACL.[Pad?
and Lapata2007] Sebastian Pad?
and Mirella La-pata.
2007.
Dependency-based construction of se-mantic space models.
Computational Linguistics,33(2):161?199.
[Plate1995] Tony Plate.
1995.
Holographic reducedrepresentations.
IEEE Transactions on Neural Net-works, 6(3):623?641.
[Post1946] Emil L. Post.
1946.
A variant of a recur-sively unsolvable problem.
Bulletin of the AmericanMathematical Society, 52:264?268.
[Rendle et al2009] Steffen Rendle, Leandro BalbyMarinho, Alexandros Nanopoulos, and LarsSchmidt-Thieme.
2009.
Learning optimal rankingwith tensor factorization for tag recommendation.In John F. Elder IV, Fran?oise Fogelman-Souli?,Peter A. Flach, and Mohammed Javeed Zaki,editors, KDD, pages 727?736.
ACM.
[Sahlgren et al2008] Magnus Sahlgren, Anders Holst,and Pentti Kanerva.
2008.
Permutations as a meansto encode order in word space.
In Proc.
CogSci?08,pages 1300?1305.
[Salton et al1975] Gerard Salton, Anita Wong, andChung-Shu Yang.
1975.
A vector space model forautomatic indexing.
Commun.
ACM, 18(11):613?620.
[Sch?tze1993] Hinrich Sch?tze.
1993.
Word space.In Lee C. Giles, Stephen J. Hanson, and Jack D.Cowan, editors, Advances in Neural InformationProcessing Systems 5, pages 895?902.
Morgan-Kaufmann.
[Strang1993] Gilbert Strang.
1993.
Introduction toLinear Algebra.
Wellesley-Cambridge Press.
[Tucker1966] Ledyard R. Tucker.
1966.
Some math-ematical notes on three-mode factor analysis.
Psy-chometrika, 31(3).
[Widdows2008] Dominic Widdows.
2008.
Semanticvector products: some initial investigations.
In Pro-ceedings of the Second AAAI Symposium on Quan-tum Interaction.916
