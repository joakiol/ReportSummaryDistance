ADAPTIVE LANGUAGE MODELING USINGTHE MAXIMUM ENTROPY PRINCIPLERaymond Lau, Ronald Rosenfel~, Salim RoukosIBM Research DivisionThomas J. Watson Research CenterYorktown Heights, NY 10598ABSTRACTWe describe our ongoing efforts at adaptive statistical language mod-eling.
Central to our approach is the Maximum Entropy (ME) Prin-ciple, allowing us to combine vidence from multiple sources, uchas long-distance triggers and conventional short.distance trigrams.Given consistent s atistical evidence, a unique ME solution is guar-anteed to exist, and an iterative algorithm exists which is guaranteedto converge to it.
Among the advantages of this approach are itssimplicity, its generality, and its incremental nature.
Among itsdisadvantages are its computational requirements.
We describe asuccession of ME models, culminating in our current MaximumLikelihood / Maximum Entropy (ML/ME) model.
Preliminary re-sults with the latter show a 27% perplexity reduction as compared toa conventional trigram model.1.
STATE OF THE ARTUntil recently, the most successful language model (givenenough training data) was the trigram \[1\], where the proba-bility of a word is estimated based solely on the two wordspreceding it.
The trigram model is simple yet powerful \[2\].However, since it does not use anything but the very immedi-ate history, it is incapable of adapting to the style or topic ofthe document, and is therefore considered a static model.In contrast, a dynamic or adaptive model is one that changesits estimates as a result of "seeing" some of the text.
Anadaptive model may, for example, rely on the history of thecurrent document in estimating the probability of a word.Adaptive models are superior to static ones in that they areable to improve their performance after seeing some of thedata.
This is particularly useful in two situations.
First, whena large heterogeneous language source is composed of smaller,more homogeneous segments, uch as newspaper a ticles.
Anadaptive model trained on the heterogeneous source will beable to hone in on the particular "sublanguage" used in each ofthe articles.
Secondly, when a model trained on data from onedomain is used in another domain.
Again, an adaptive modelwill be able to adjust o the new language, thus improving itsperformance.The most successful adaptive LM to date is described in \[3\].
Acache of the last few hundred words is maintained, and is used*This work is now continued by Ron Rosenfeld at Carnegie MellonUniversity.to derive a "cache trigrarn".
The latter is then interpolatedwith the static trigram.
This results in a 23% reduction inperplexity, and a 5%-24% reduction in the error rate of aspeech recognizer.In what follows, we describe our efforts at improving ouradaptive statistical language models by capitalizing on theinformation present in the document history.2.
TRIGGER-BASED MODELINGTo extract information from the document history, we proposethe idea of a trigger pair as the basic information bearingelement.
If a word sequence A is significantly correlated withanother word sequence B, then (A---, B) is considered a "triggerpair", with A being the trigger and B the triggered sequence.When A occurs in the document, it triggers B, causing itsprobability estimate to change.Before attempting todesign atrigger-based model, one shouldstudy what long distance factors have significant effects onword probabilities.
Obviously, some information about P(B)can be gained simply by knowing that A had occurred.
Butexactly how much?
And can we gain significantly more byconsidering how recently A occurred, or how many times?We have studied these issues using the a Wail Street Journalcorpus of 38 million words.
Some illustrations are given infigs.
1 and 2.
As can be expected, different rigger pairs givedifferent answers, and hence should be modeled ifferently.More detailed modeling should be used when the expectedreturn is higher.Once we determined the phenomena tobe modeled, one mainissue still needs to be addressed.
Given the part of the docu-ment processed so far (h), and a word w considered for the nextposition, there are many different estimates of P(wlh).
Theseestimates are derived from the various triggers of w, from thestatic trigram model, and possibly from other sources, howdo we combine them all to form one optimal estimate?
Wepropose asolution to this problem in the next section.108r( s ~  )N N N N N N N!
2 3 ~10 I1-~ 26-50 51-100 101-200 201-500 501+Figure 1: Probability of 'SHARES' as a function of the dis-tance from the last occurrence of 'STOCK' in the same doc-ument.
The middle horizontal line is the unconditional prob-ability.
The top (bottom) line is the probability of 'SHARES'given that 'STOCK' occurred (did not occur) before in thedocument.3.
MAXIMUM ENTROPY SOLUTIONSUsing several different probability estimates to arrive at onecombined estimate is a general problem that arises in manytasks.
We use the maximum entropy (ME) principle (\[4, 5\]),which can be summarized as follows:1.
Reformulate the different estimates as constraints on theexpectation of various functions, to be satisfied by thetarget (combined) estimate.2.
Among all probabilitydistributionsthat satisfy these con-straints, choose the one that has the highest entropy.P(WlNTI~)WINTER I SUMMER)P(Wl/CrER)0 I 2 3 44- c( SUMME/UFigure 2: Probability of 'WINTER' as a function of the num-ber of times 'SUMMER' occurred before it in the same doc-ument.
Horizontal lines are as in fig.
1.In the next 3 sections, we describe asuccession of models wedeveloped, all based on the ME principle.
We then expand onthe last model, describe possible future extensions to it, andreport current results.
More details can be found in \[6, 7\].4.
MODEL I: EARLY  ATTEMPTSAssume we have identified for each word w in a vocabulary,V, a set of nw trigger words tw~ t~.
.
.
t~,,; we further assumethat we have the relative frequency of observing a triggerword, t, occurring somewhere in the history, h, (in our casewe have used a history length, K, of either 25, 50, 200, or1000 words) and the word w just occurs after the history fromsome training text; denote the observed relative frequency ofa trigger and a word w byc(t E h and w immediatelyf ollows h) d(t, w) =Nwhere c(.)
is the count in the training data.
We use {t, w}to indicate the event hat trigger t occurred in the history andword w occurs next; the term long-distance bigram has beenused for this event.Assume we have a joint distribution p(h, w) of the history ofK words and the next word w. We require this joint modelto assign to the events {t, w} a probability that matches theobserved relative frequencies.
Assuming we have R suchconstraints we find a model that has Maximum Entropy:p*(h, w) = arg max - Ep(h ,  w) lgp(h, w)h,wsubject o the R trigger constraints;:p(t, w) = E p(h, w) = d(t, w)h:tEhWe also include the case that none of the triggers of word woccur in the history (we denote this event by {to, w}.)
UsingLagrange multipliers, one can easily show that the MaximumEntropy model is given by:p(h, w) = H ItWtt:tE hi.e., the joint probability is the product of lh(W) factors onefactor for each trigger t,~ of word w that occurs in the historyh (or one factor if none of the triggers occur.)
The MaximumEntropy joint distribution over a space of \]VI K?l is given byR parameters, one for each constraint.
In our case, we used amaximum of 20 triggers per word for a 20k vocabulary withan average of 10 resulting in 200,000 constraints.!
we also imposed unigram constraints to match the unigram distributionof  the vocabulary109?
The log 4.1. l tow to determine the factors?One can use the "Brown" algorithm to determine the set of fac-tors.
At each iteration, one updates the factor of one constraintand as long as one cycles through all constraints repeatedlythe factors will converge to the optimal value.
At the i-thiteration, assume we are updating the factor that correspondsto the {t, w}-constraint.
Then the update is given by:d( t, w)P~'  = P~llm(t,W)where the model predicted value m(t, w) is given by:m(t, w) = E P?lt(h' w) (I)h:tE hwhere pOll uses the old factor values.Using the ME joint model, we define a conditional unigraramodel by:p* (h, w)p(wlh) = Ewp, (h ,w )This is a "time-varying" unigram model where the previous Kwords determine the relative probability that w would occurnext.
The perplexity of the resulting model was about 2000much higher than the perplexity of a static unigram model.In particular, the model underestimated the probability of thefrequent words.
To ease that problem we disallowed anytriggers for the most frequent L words.
We experimentedwith L ranging from 100 to 500 words.
The resulting modelwas better though its perplexity was still about 1100 which is43% higher than the static unigram perplexity of 772.
Onereason that we conjecture was that the ME model gives arather high probability for histories that are quite unlikely inreality and the trigger constraints are matched using thoseunrealistic histories.
We tried an ad hoc computation wherethe summation over the histories in Equation 1 was weighedby a crude estimate, w(h), of the probability of the history i.e.we usedm(t, w) = E w(h)P?ll(h, w)h:tEhThe resulting model had a much lower perplexity of 559,about 27% lower than the static unigram model on a test setof (1927 words).
This ad hoc computation indicates that weneed to model the histories more realistically.
The model wepropose in the next section is derived from the viewpoint thatME indicates that R factors define a conditional model thatcaptures the"Iong-distance" bigram constraints and that usingthis parametric form with Max imum Likelihood estimationmay ailow us to concentrate on typical histories that occur inthe data.5.
MODEL H: ML OF CONDIT IONAL METhe ME viewpoint results in a conditional model that belongsto the exponential family with K parameters when K con-straints are contemplated.
We can use Maximum Likelihoodestimation to estimate the K factors of the model.likelihood of a training set is given by:N-1L = E lgp(wt+l Ih,)t=oN- 1 Hiet~(w,.i) ~iwhere lh(w) is the set of triggers for word w that occur in h.The convexity of the log likelihood guarantees that any hillclimbing method will converge to the global optimum.
Thegradient can be shown to be:oOlz---~L = -L-(d(t, w) - E p(wlhl)\[~wt h:t6hone can use the gradient o iteratively re-estimate he factorsby:new_ oil 1 P~ - Pwt + T~(d(  t, w) - m'(t, w))lawtwhere the model predicted value m'(t, w) for a constraint is:m'(t, w) = ~ P(wlh))h:tEhThe training data is used to estimate the gradient given thecurrent estimate of the factors.
The size of the gradient stepcan be optimized by a line search on a small amount of trainingdata.Given the "time-varying" unigram estimate, we use the meth-ods of \[8\] to obtain a bigram LM whose unigram matches thetime-varying unigram using a window of the most recent Lwords.6.
CURRENT MODEL:  ML/MEFor estimating a probability function P(x), each constraint iis associated with a constraint function f i(x) and a desiredexpectation ci.
The constraint is then written as:def E Epf  i = P(x)ffi(x) = Ci .
(2)xGiven consistent constraints, a unique ME solutions is guar-anteed to exist, and to be of the form:P(x) = H Pif'(x) ' (3)iwhere the pi's are some unknown constants, to be found.Probability functions of the form (3) are called log-linear,and the family of functions defined by holding thefi's fixedand varying the pi's is called an exponential family.To search the exponential family defined by (3) for the ~i'sthat will make P(x) satisfy all the constraints, an iterativealgorithm, "Generalized Iterative Scaling", exists, which isguaranteed toconverge to the solution (\[9\]).1106.1.
Formulating Triggers as ConstraintsTo reformulate a trigger pair A---, B as a constraint, define theconstraint functionf~..~ as:1 ifAEh, w=Bfa--~(h, w) = 0 otherwise (4)Set c,~-.,n to R~..~\],  the empirical expectation ffA-,B (ie itsexpectation i the training data).
Now impose on the desiredprobability estimate P(h, w) the constraint:Ep \[fA--~t~\] = E \[f~--.B\] (5)6.2.
Estimating Conditionals:The ML/ME SolutionGeneralized Iterative Scaling can be used to find the MEestimate of a simple (non-conditional) probability distributionover some event space.
But in our case, we need to estimateconditional probabilities of the form P(wlh).
How should thisbe done more efficiently than in the previous models?An elegant solution was proposed by \[10\].
Let P(h, w) be thedesired probability estimate, and let P(h, w) be the empiricaldistribution of the training data.
Letfi(h, w) be any constraintfunction, and let cl be its desired expectation.
Equation 5 canbe rewritten as:E P(h).
E P(wlh) .fi(h, w) = ci (6)h wWe now modify the constraint to be:PCh).
~ PCwlh) .f iCh, w) = ci (7)h wOne possible interpretation f this modification is as follows.Instead of constraining the expectation offi(h, w) with regardto P(h, w), we constrain its expectation with regard to a dif-ferent probability distribution, say Q(h, w), whose conditionalQ(wlh) is the same as that of P, but whose marginal Q(h) isthe same as that of P. To better understand the effect of thischange, define H as the set of all possible histories h, anddefine Hi, as the partition of H induced byfi.
Then the modi-fication is equivalent to assuming that, for every constralntfi,P(Hfj) = P(Hf,).
Since typically H/., is a very small set, theassumption is reasonable.The unique ME solution that satisfies equations like (7) or(6) can be shown to also be the Maximum Likelihood (ML)solution, namely that function which, among the exponentialfamily defined by the constraints, has the maximum likelihoodof generating the data.
The identity of the ML and ME so-lutions, apart from being aesthetically pleasing, is extremelyuseful when estimating the conditional P(wlh).
It means thathillclimbing methods can be used in conjunction with Gen-eralized Iterative Scaling to speed up the search.
Since thelikelihood objective function is convex, hillclimbing will notget stuck in local minima.6.3.
Incorporating the trigram modelWe combine the trigger based model with the currently beststatic model, the N-Gram, by reformulating the latter to fitinto the ML/ME paradigm.
The usual unigram, bigram andtrigram ML estimates are replaced by unigram, bigrarn andtrigrarn constraints conveying the same information.
Specifi-cally, the constraint function for the unigram wl is:1 ifw = wlfw,(h,w)= 0 otherwise (8)and its associated constraint is:P(wlh rw,(h, w)=  fw, (h, w)h w(9)Similarly, the constraint function for the bigram Wl, w2 is1 i fhendsin wl and w= w2 (10)fwt,~(h, w) = 0 otherwiseand its associated constraint isP(h) ~ P(wlh)f w,,~(h, w) = Ef w,,w2(h, w).h w(11)and similarly for higher-order ngrarns.The computational bottleneck of the Generalized IterativeScaling algorithm is in constraints which, for typical historiesh, are non-zero for a large number of w's.
This means that bi-gram constraints are more expensive than trigram constraints.Implicit computation can be used for unigram constraints.Therefore, the time cost of bigram and trigger constraintsdominates the total time cost of the algorithm.7.
ME: PROS AND CONSThe ME principle and the Generalized Iterative Scaling algo-rithm have several important advantages:..The ME principle is simple and intuitively appealing.
Itimposes all of the constituent constraints, but assumesnothing else.
For the special case of constraints derivedfrom marginal probabilities, it is equivalent to assuminga lack of higher-order interactions \[11\].ME is extremely general.
Any probability estimate ofany subset of the event space can be used, including es-timates that were not derived from the data or that are111inconsistent with it.
The distance dependence and countdependence illustrated in figs.
1 and 2 can be readilyaccommodated.
Many other knowledge sources, includ-ing higher-order ffects, can be incorporated.
Note thatconstraints need not be independent of nor uncorrelatedwith each other.3.
The information captured by existing language modelscan be absorbed into the ML/ME model.
We have shownhow this is done for the conventional N-gram model.Later on we will show, how it can be done for the cachemodel of \[3\].4.
Generalized Iterative Scaling lends itself to incrementaladaptation.
New constraints can be added at any time.Old constraints can be maintained or else allowed torelax.5.
A unique ME solution is guaranteed to exist for con-sistent constraints.
The Generalized Iterative Scalingalgorithm is guaranteed toconverge to it.This approach also has the following weaknesses:1.
Generalized Iterative Scaling is computationally very ex-pensive.
When the complete system is trained on theentire 50 million words of Wall Street Journal data, it isexpected to require many thousands of MIPS-hours torun to completion.2.
While the algorithm is guaranteed to converge, we donot have a theoretical bound on its convergence rate.3.
It is sometimes useful to impose constraints hat are notsatisfied by the training data.
For example, we maychoose to use Good-Tmqng discounting \[12\], or else theconstraints may be derived from other data, or be ex-ternally imposed.
Under these circumstances, the con-straints may no longer be consistent, and the theoreticalresults guaranteeing existence, uniqueness and conver-gence may not hold.8.
INCORPORATING THECACHE MODELIt seems that the power of the cache model, described in sec-tion 1, comes from the "bursty" nature of language.
Namely,infrequent words tend to occur in "bursts", and once a wordoccurred in a document, its probability of recurrence is sig-nificantly elevated.Of course, this phenomena can be captured by a trigger pairof the form A ~ A, which we call a "self trigger".
Wehave done exactly that in \[13\].
We found that self triggers areresponsible for a disproportionatelylarge partof the reductionin perplexity.
Furthermore, self triggers proved particularlyrobust: when tested in new domains, they maintained thecorrelations found in the training databetter than the"regular"triggers did.Thus self triggers are particularly important, and should bemodeled separately and in more detail.
The trigger modelwe currently use does not distinguish between one or moreoccurrences of a given word in the history, whereas the cachemodel does.
For self-triggers, the additional information canbe significant (see fig.
3).P( DEFAULT )0 I 2 3 4 5+ 12(DI~'AUL~)Figure 3: Behavior of a self-trigger: Probability of 'DE-FAULT' as a function of the number of times it already oc-curred in the document.
The horizontal line is the uncondi-tional probability.We plan to model self triggers in more detail.
We will considerexplicit modeling of frequency of occurrence, distance fromlast occurrence, and other factors.
All of these aspects caneasily be formulated as constraints and incorporated into theME formalism.9.
RESULTSThe ML/ME model described above was trained on 5 mil-lion words of Wail Street Journal text, using DARPA's of-ficial "200" vocabulary of some 20,000 words.
A conven-tionai trigram model was used as a baseline.
The constraintsused by the ML/ME model were: 18,400 unigram constraints,240,000 bigram constraints, and 414,000 trigram constraints.One experiment was run with 36,000 trigger constraints (best3 triggers for each word), and another with 65,000 trigger con-straints (best 6 triggers per word).
All models were trainedon the same data, and evaluated on 325,000 words on in-dependent data.
The Maximum Entropy models were alsointerpolated with the conventional trigram, using yet unseendata for interpolation.
Results are summarized in table 1.112Test-set % improvementmodel Perplexity over baselinetrigrarn 173 - -ML/ME-top3 134 23%+trigram 129 25%MI_/ME-top6 130 25%127 27% +trigramTable 1: Improvement of Maximum Likelihood / MaximumEntropy model over a conventional trigram model.
Trainingis on 5 million words of WSJ text.
Vocabulary is 20,000words.The trigger constraints used in this run were selected verycrudely, and their number was not optimized.
We believemuch more improvement can be achieved.
Special modelingof self triggers has not been implemented yet.
Similarly, weexpect it to yield further improvement.10.
ACKNOWLEDGEMENTSWe are grateful to Peter Brown, Stephen Della Pietra, Vin-cent Della Pietra and Bob Mercer for many suggestions anddiscussions.Research by Ron Rosenfeld was sponsored by the DefenseAdvanced Research Projects Agency and monitored by theSpace and Naval Warfare Systems Command under ContractN00039-91-C-0158, ARPA Order No.
7239.
The views andconclusions contained in this document are those of the au-thors and should not be interpreted as representing the officialpolicies, either expressed or implied, of the U.S. Government.References1.
Bahl, L., Jelinek, F., Mercer, R.L., "A Statistical Approach toContinuous Speech Recognition," IEEE Trans.
on PAMI, 1983.2.
Jelinek, E, "Up From Trigrams!"
Eurospeech 1991.3.
Jellnek, F., Merialdo, B., Roukos, S., and SU'auss, M., "ADynamic Language Model for Speech Recognition."
Proceed-ings of the Speech and Natural Language DARPA Workshop,pp.293-295, Feb. 1991.4.
Jaines, E. T., "Information Theory and Statistical Mechanics."Phys.
Rev.
106, pp.
620-630, 1957.5.
Kullback, S., Information Theory in Statistics.
Wiley, NewYork, 1959.\[6, 7\].6.
Rosenfeld, R., "Adaptive Statistical Language Modeling:a Maximum Entropy Approach," Ph.D. Thesis Proposal,Carnegie MeUon Universit~ September 1992.7.
Lau, R., Rosenfeld, R., Roukos, S., "Trigger-Based LanguageModels: a Maximum Entropy Approach," Proceedings ofICASSP-93, April 1993.8.
Della Pietra, S., Della Pielra, V., Mercer, R. L., Roukos, S.,"Adaptive Language Modeling Using Minimum DiscriminantEstimation," Proceedings of lCASSP-92, pp.
1-633-636, SanFrancisco, March 1992.9.
Darroch, J.N.
and Ratcliff, D., "Generalized Iterative Scalingfor Log-Linear Models", The Annals of Mathematical Statis-tics, Vol.
43, pp 1470-1480, 1972.10.
Brown, P., DcHa Pietra, S., Della Pietra, V., Mercer, R., Nadas,A., and Roukos, S., "Maximum Entropy Methods and TheirApplications to Maximum Likelihood Parameter Estimation ofConditional Exponential Models," A forthcoming IBM techni-cal report.11, Good, I. J., "Maximum Entropy for Hypothesis Formulation,Especially for Multidimensional Contingency Tables."
Annalsof Mathematical Statistics, Vol.
34, pp.
911-934, 1963.12.
Good, I. J., "The Population Frequencies of Species and theEstimation of Population Parameters."
Biometrika, Vol.
40, no.3, 4, pp.
237-264, 1953.13.
Rosenfeld, R., and Huang, X. D., "Improvements in StochasticLanguage Modeling."
Proceedings of the Speech and NaturalLanguage DARPA Workshop, Feb. 1992.113
