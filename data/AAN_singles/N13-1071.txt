Proceedings of NAACL-HLT 2013, pages 627?633,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsThe Life and Death of Discourse Entities: Identifying Singleton MentionsMarta RecasensLinguistics DepartmentStanford UniversityStanford, CA 94305recasens@google.comMarie-Catherine de MarneffeLinguistics DepartmentThe Ohio State UniversityColumbus, OH 43210mcdm@ling.osu.eduChristopher PottsLinguistics DepartmentStanford UniversityStanford, CA 94305cgpotts@stanford.eduAbstractA discourse typically involves numerous en-tities, but few are mentioned more than once.Distinguishing discourse entities that die outafter just one mention (singletons) from thosethat lead longer lives (coreferent) would ben-efit NLP applications such as coreference res-olution, protagonist identification, topic mod-eling, and discourse coherence.
We build a lo-gistic regression model for predicting the sin-gleton/coreferent distinction, drawing on lin-guistic insights about how discourse entitylifespans are affected by syntactic and seman-tic features.
The model is effective in its ownright (78% accuracy), and incorporating it intoa state-of-the-art coreference resolution sys-tem yields a significant improvement.1 IntroductionNot all discourse entities are created equal.
Somelead long lives and appear in a variety of discoursecontexts (coreferent), whereas others never escapetheir birthplaces, dying out after just one mention(singletons).
The ability to make this distinctionbased on properties of the NPs used to identify thesereferents (mentions) would benefit not only corefer-ence resolution, but also topic analysis, textual en-tailment, and discourse coherence.The existing literature provides numerous gen-eralizations relevant to answering the question ofwhether a given discourse entity will be singletonor coreferent.
These involve the internal syntax andmorphology of the target NP (Prince, 1981a; Prince,1981b; Wang et al 2006), the grammatical functionand discourse role of that NP (Chafe, 1976; Hobbs,1979; Walker et al 1997; Beaver, 2004), and the in-teraction of all of those features with semantic oper-ators like negation, modals, and attitude predicates(Karttunen, 1973; Karttunen, 1976; Kamp, 1981;Heim, 1982; Heim, 1992; Roberts, 1990; Groe-nendijk and Stokhof, 1991; Bittner, 2001).The first step in our analysis is to bring theseinsights together into a single logistic regressionmodel ?
the lifespan model ?
and assess theirpredictive power on real data.
We show that thefeatures generally behave as the existing literatureleads us to expect, and that the model itself is highlyeffective at predicting whether a given mention issingleton or coreferent.
We then provide an initialassessment of the engineering value of making thesingleton/coreferent distinction by incorporating ourlifespan model into the Stanford coreference resolu-tion system (Lee et al 2011).
This addition resultsin a significant improvement on the CoNLL-2012Shared Task data, across the MUC, B3, CEAF, andCoNLL scoring algorithms.2 DataAll the data used throughout the paper come fromthe CoNLL-2012 Shared Task (Pradhan et al2012), which included the 1.6M English words fromOntoNotes v5.0 (Hovy et al 2006) that have beenannotated with different layers of annotation (coref-erence, parse trees, etc.).
We used the training, de-velopment (dev), and test splits as defined in theshared task (Table 1).
Since the OntoNotes corefer-ence annotations do not contain singleton mentions,we automatically marked as singletons all the NPs627MENTIONSDataset Docs Tokens Coreferent SingletonsTraining 2,802 1.3M 152,828 192,248Dev 343 160K 18,815 24,170Test 348 170K 19,392 24,921Table 1: CoNLL-2012 Shared Task data statistics.
Weadded singletons (NPs not annotated as coreferent).not annotated as coreferent.
Thus, our singletons in-clude non-referential NPs but not verbal mentions.3 Predicting lifespansOur lifespan model makes a binary distinction be-tween discourse referents that are not part of a coref-erence chain (singletons) and items that are part ofone (coreferent).
The distribution of lifespans in ourdata (Figure 1) suggests that this is a natural divi-sion.
The propensity of singletons also highlightsthe relevance of detecting singletons for a coref-erence system.
We fit a binary logistic regressionmodel in R (R Core Team, 2012) on the trainingdata, coding singletons as ?0?
and coreferent men-tions as ?1?.
Throughout the following tables of co-efficient estimates, positive values favor coreferentsand negative ones favor singletons.
We turn now todescribing and motivating the features of this model.Singleton 2 3 4 5 6-10 11-15 16-20 >2005K15K25KFigure 1: Distribution of lifespans in the dev set.
Single-tons account for 56% of the data.Internal morphosyntax of the mention Table 2summarizes the features from our model that con-cern the internal morphology and syntactic structureof the mention.
Many are common in coreferencesystems (Recasens and Hovy, 2009), but our modelhighlights their influence on lifespans.
The pictureis expected on the taxonomy of given and new de-fined by Prince (1981b) and assumed throughout dy-namic semantics (Kamp, 1981; Heim, 1982): pro-nouns depend on anaphoric connections to previousmentions for disambiguation and thus are very likelyto be coreferent.
This is corroborated by the pos-itive coefficient estimate for ?Type = pronoun?
inTable 2.
Few quantified phrases easily participatein discourse anaphora (Partee, 1987; Wang et al2006), accounting for the association between quan-tifiers and singletons (negative coefficient estimatefor ?Quantifier = quantified?
in Table 2).
The onesurprise is the negative coefficient for indefinites.
Intheories stretching back to Karttunen (1976), indef-inites function primarily to establish new discourseentities, and should be able to participate in coref-erence chains, but here the association with suchchains is negative.
However, interactions explainthis fact (see Table 4 and our discussion of it).The person, number, and animacy values suggestthat singular animates are excellent coreferent NPs,a previous finding of Centering Theory (Grosz et al1995; Walker et al 1998) and of cross-linguisticwork on obviative case-marking (Aissen, 1997).Our model also includes named-entity features forall of the eighteen OntoNotes entity-types (omittedfrom Table 2 for space and clarity reasons).
As arule, they behave like ?Type = proper noun?
in asso-ciating with coreferents.
The exceptions are ORDI-NAL, PERCENT, and QUANTITY, which seem intu-itively unlikely to participate in coreference chains.Estimate P-valueType = pronoun 1.21 < 0.001Type = proper noun 1.88 < 0.001Animacy = inanimate ?1.36 < 0.001Animacy = unknown ?0.38 < 0.001Person = 1 1.05 < 0.001Person = 2 0.13 < 0.001Person = 3 1.62 < 0.001Number = singular 0.61 < 0.001Number = unknown 0.17 < 0.001Quantifier = indefinite ?1.49 < 0.001Quantifier = quantified ?1.23 < 0.001Number of modifiers ?0.39 < 0.001Table 2: Internal morphosyntactic features.Grammatical role of the mention Synthesizingmuch work in Centering Theory and informationstructuring, we conclude that coreferent mentionsare likely to appear as core verbal arguments andwill favor sentence-initial (topic-tracking) positions(Ward and Birner, 2004).
The coefficient estimates628Estimate P-valueSentence Position = end ?0.22 < 0.001Sentence Position = first 0.04 0.07Sentence Position = last ?0.31 < 0.001Sentence Position = middle ?0.11 < 0.001Relation = noun argument 0.56 < 0.001Relation = other ?0.67 < 0.001Relation = root ?0.61 < 0.001Relation = subject 0.65 < 0.001Relation = verb argument 0.32 < 0.001In coordination ?0.48 < 0.001Table 3: Grammatical role features.in Table 3 corroborate these conclusions.
To de-fine the ?Relation?
and ?In coordination?
features, weused the Stanford dependencies (de Marneffe et al2006) on the gold constituents.Semantic environment of the mention Table 4highlights the complex interactions between dis-course anaphora and semantic operators.
Theseinteractions have been a focus of logical seman-tics since Karttunen (1976), whose guiding obser-vation is semantic: an indefinite interpreted insidethe scope of a negation, modal, or attitude predicateis generally unavailable for anaphoric reference out-side of the scope of that operator, as in Kim didn?tunderstand [an exam question]i.
#Iti was too hard.Of course, such discourses cohere if the indefiniteis interpreted as taking wide scope (?there is a ques-tion Kim didn?t understand?).
Such readings are of-ten disfavored, but they become more salient whenmodifiers like certain are included (Schwarzschild,2002) or when the determiner is sensitive to the po-larity or intensionality of its environment (Baker,1970; Ladusaw, 1980; van der Wouden, 1997; Is-rael, 1996; Israel, 2001; Giannakidou, 1999).
Sub-sequent research identified many other factors thatfurther extend or restrict the anaphoric potential ofan indefinite (Roberts, 1996).We do not have direct access to semantic scope,but we expect syntactic scope to correlate stronglywith semantic scope, so we used dependency rep-resentations to define features capturing syntacticscope for negation, modal auxiliaries, and a broadrange of attitude predicates.
These features tend tobias in favor of singletons because they so radicallyrestrict the possibilities for intersentential anaphora.Interacting these features with those for the inter-nal syntax of mentions is also informative.
Sinceproper names and pronouns are not scope-taking,they are largely unaffected by the environment fea-tures, whereas indefinites emerge as even more re-stricted, just as Karttunen and others would predict.Attitude predicates seem initially anomalous,though.
They share the relevant semantic proper-ties with negation and modals, and yet they seemto facilitate coreference.
Here, the findings of deMarneffe et al(2012) seem informative.
Those au-thors find that, in texts of the sort we are studying,attitude predicates are used predominantly to markthe source of information that is effectively asserteddespite being embedded (Rooryck, 2001; Simons,2007).
That is, though X said p does not semanti-cally entail p, it is often interpreted as a commitmentto p, which correspondingly elevates mentions in pto main-clause status (Harris and Potts, 2009).Estimate P-valuePresence of negation ?0.18 < 0.001Presence of modality ?0.22 < 0.001Under an attitude verb 0.03 0.01AttitudeVerb * (Type = pronoun) 0.29 < 0.001AttitudeVerb * (Type = proper noun) 0.14 < 0.001Modal * (Type = pronoun) 0.12 0.04Modal * (Type = proper noun) 0.35 < 0.001Negation * (Type = pronoun) 1.07 < 0.001Negation * (Type = proper noun) 0.30 < 0.001Negation * (Quantifier = indefinite) ?0.37 < 0.001Negation * (Quantifier = quantified) ?0.36 0.23Negation * (Number of modifiers) 0.11 < 0.001Table 4: Semantic environment features and interactions.Results The model successfully learns to teasesingletons and coreferent mentions apart.
Table 5summarizes its performance on the dev set.
TheSTANDARD model uses 0.5 as the decision bound-ary, with 78% accuracy.
The CONFIDENT modelpredicts singleton if Pr < .2 and coreferent if Pr > .8,which increases precision (P) at a cost to recall (R).STANDARD CONFIDENTPrediction R P F1 R P F1Singleton 82.3 79.2 80.7 50.5 89.6 64.6Coreferent 72.2 76.1 74.1 41.3 86.8 55.9Table 5: Recall, precision, and F1 for the lifespan model.629MUC B3 CEAF-?3 CEAF-?4 CoNLLSystem R P F1 R P F1 R / P / F1 R P F1 F1Baseline 66.64* 64.72 65.67 68.05* 71.58 69.77* 58.31 45.49 47.55* 46.50 60.65w/ Lifespan 66.08 67.33* 66.70* 66.40 73.14* 69.61 58.83* 47.77* 46.38 47.07* 61.13*Table 6: Performance on the test set according to the official CoNLL-2012 scorer.
Scores are on automatically pre-dicted mentions.
Stars indicate a statistically significant difference (paired Mann-Whitney U-test, p < 0.05).B3 CEAF-?3 CoNLLSystem R P F1 R P F1 F1Baseline 58.53* 71.58 64.40 63.71* 58.31 60.89 58.86w/ Lifespan 58.14 73.14* 64.78* 63.38 58.83* 61.02 59.52*Table 7: B3, CEAF-?3 and CoNLL measures on the test set according to a modified CoNLL-2012 scorer that followsCai and Strube (2010).
Scores are on automatically predicted mentions.4 Application to coreference resolutionTo assess the usefulness of the lifespan model in anNLP application, we incorporate it into the Stanfordcoreference resolution system (Lee et al 2011),which we take as our baseline.
This was the highest-scoring system in the CoNLL-2011 Shared Task,and was also part of the highest-scoring system inthe CoNLL-2012 Shared Task (Fernandes et al2012).
It is a rule-based system that includes a to-tal of ten rules (or ?sieves?)
for entity coreference,such as exact string match and pronominal resolu-tion.
The sieves are applied from highest to lowestprecision, each rule adding coreference links.Incorporating the lifespan model The lifespanmodel can improve coreference resolution in twodifferent ways: (i) mentions classified as singletonsshould not be considered as either antecedents orcoreferent, and (ii) mentions classified as coreferentshould be linked with another mention(s).
By suc-cessfully predicting singletons (i), we can enhancethe system?s precision; by successfully predictingcoreferent mentions (ii), we can improve the sys-tem?s recall.
Here we focus on (i) and use the lifes-pan model for detecting singletons.
This decisionis motivated by two factors.
First, given the largenumber of singletons (Figure 1), we are more likelyto see a gain in performance from discarding sin-gletons.
Second, the multi-sieve nature of the Stan-ford coreference system does not make it straightfor-ward to decide which antecedent a mention shouldbe linked to even if we know that it is coreferent.We leave the incorporation of coreferent predictionsfor future work.To integrate the singleton model into the Stanfordcoreference system, we let a sieve consider whethera pair of mentions is coreferent only if neither ofthe two mentions are classified as singletons by ourCONFIDENT model.
Experiments on the dev setshowed that the model often made wrong predic-tions for NEs.
We do not trust the model for NEmentions.
Performance on coreference (on the devset) was higher with the CONFIDENT model thanwith the STANDARD model.Results and discussion To evaluate the corefer-ence system with and without the lifespan model, weused the English dev and test sets from the CoNLL-2012 Shared Task, presented in Section 2.
Althoughthe CoNLL shared task evaluated systems on onlymulti-mention (i.e., non-singleton) entities, by stop-ping singletons from being linked to multi-mentionentities, we expected the lifespan model to increasethe system?s precision.
Our evaluation uses fiveof the measures given by the CoNLL-2012 scorer:MUC (Vilain et al 1995), B3 (Bagga and Baldwin,1998), CEAF-?3 and CEAF-?4 (Luo, 2005), and theCoNLL official score (Denis and Baldridge, 2009).We do not include BLANC (Recasens and Hovy,2011) because it assumes gold mentions and so isnot suited for the scenario considered in this paper,which uses automatically predicted mentions.Table 6 summarizes the test set performance.
Allthe scores are on automatically predicted mentions.We use gold POS, parse trees, and NEs.
The base-630line is the Stanford system, and ?w/ Lifespan?
is thesame system extended with our lifespan model todiscard singletons, as explained above.As expected, the lifespan model increases preci-sion but decreases recall.
Overall, however, we ob-tain a significant improvement of 0.5?1 points in theF1 score of MUC, CEAF-?3, CEAF-?4 and CoNLL.The drop in B3 traces to a bug in the CoNLL scorer?simplementation of Cai and Strube (2010)?s algo-rithm for aligning gold and automatically predictedmentions, which affects the computation of B3 andCEAF-?3.1 Table 7 presents the results after mod-ifying the CoNLL-2012 scorer to compute B3 andCEAF-?3 according to Cai and Strube (2010).2 Wedo see an improvement in the precision and F1scores of B3, and the overall CoNLL score remainssignificant.
The CEAF-?3 F1 score is no longer sig-nificant, but is still in the expected direction.5 ConclusionWe built a model to predict the lifespan of discoursereferents, teasing apart singletons from coreferentmentions.
The model validates existing linguisticinsights and performs well in its own right.
Thisalone has ramifications for tracking topics, identify-ing protagonists, and modeling coreference and dis-course coherence.
We applied the lifespan model tocoreference resolution, showing how to incorporateit effectively into a state-of-the-art rule-based coref-erence system.
We expect similar improvementswith machine-learning-based coreference systems,where incorporating all the power of the lifespanmodel would be easier.Our lifespan model has been integrated into thelatest version of the Stanford coreference resolutionsystem.31At present, if the system links two mentions that do notexist in the gold standard, the scorer adds two singletons to thegold standard.
This results in a higher B3 F1 score (when itshould be lower) because recall increases instead of staying thesame (precision goes up).2In the modified scorer, twinless predicted mentions areadded to the gold standard to compute precision but not to com-pute recall.3http://nlp.stanford.edu/software/dcoref.shtmlAcknowledgmentsWe thank Emili Sapena for modifying the CoNLL-2012 scorer to follow Cai and Strube (2010).This research was supported in part by ONRgrant No.
N00014-10-1-0109 and ARO grantNo.
W911NF-07-1-0216.
The first author was sup-ported by a Beatriu de Pino?s postdoctoral schol-arship (2010 BP-A 00149) from Generalitat deCatalunya.ReferencesJudith Aissen.
1997.
On the syntax of obviation.
Lan-guage, 73(4):705?750.Amit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In Proceedings ofthe LREC 1998 Workshop on Linguistic Coreference,pages 563?566.C.
L. Baker.
1970.
Double negatives.
Linguistic Inquiry,1(2):169?186.David Beaver.
2004.
The optimization of discourseanaphora.
Linguistics and Philosophy, 27(1):3?56.Maria Bittner.
2001.
Surface composition as bridging.Journal of Semantics, 18(2):127?177.Jie Cai and Michael Strube.
2010.
Evaluation metricsfor end-to-end coreference resolution systems.
In Pro-ceedings of SIGDIAL 2010, pages 28?36.Wallace L. Chafe.
1976.
Givenness, Contrastiveness,Definiteness, Subjects, Topics, and Point of View.
InCharles N. Li, editor, Subject and Topic, pages 25?55.Academic Press, New York.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of LREC 2006.Marie-Catherine de Marneffe, Christopher D. Manning,and Christopher Potts.
2012.
Did it happen?The pragmatic complexity of veridicality assessment.Computational Linguistics, 38(2):301?333.Pascal Denis and Jason Baldridge.
2009.
Global jointmodels for coreference resolution and named entityclassification.
Procesamiento del Lenguaje Natural,42:87?96.Eraldo Fernandes, C?
?cero dos Santos, and Ruy Milidiu?.2012.
Latent structure perceptron with feature induc-tion for unrestricted coreference resolution.
In Pro-ceedings of CoNLL-2012: Shared Task, pages 41?48.Anastasia Giannakidou.
1999.
Affective dependencies.Linguistics and Philosophy, 22(4):367?421.Jeroen Groenendijk and Martin Stokhof.
1991.
Dynamicpredicate logic.
Linguistics and Philosophy, 14(1):39?100.631Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225.Jesse A. Harris and Christopher Potts.
2009.Perspective-shifting with appositives and expressives.Linguistics and Philosophy, 32(6):523?552.Irene Heim.
1982.
The Semantics of Definite and Indefi-nite Noun Phrases.
Ph.D. thesis, UMass Amherst.Irene Heim.
1992.
Presupposition projection and thesemantics of attitude verbs.
Journal of Semantics,9(2):183?221.Jerry R. Hobbs.
1979.
Coherence and coreference.
Cog-nitive Science, 3(1):67?90.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:The 90% solution.
In Proceedings of HLT-NAACL 2006, pages 57?60.Michael Israel.
1996.
Polarity sensitivity as lexical se-mantics.
Linguistics and Philosophy, 19(6):619?666.Michael Israel.
2001.
Minimizers, maximizers, and therhetoric of scalar reasoning.
Journal of Semantics,18(4):297?331.Hans Kamp.
1981.
A theory of truth and discourserepresentation.
In Jeroen Groenendijk, Theo M. V.Janssen, and Martin Stockhof, editors, Formal Meth-ods in the Study of Language, pages 277?322.
Mathe-matical Centre, Amsterdam.Lauri Karttunen.
1973.
Presuppositions and compoundsentences.
Linguistic Inquiry, 4(2):169?193.Lauri Karttunen.
1976.
Discourse referents.
In James D.McCawley, editor, Syntax and Semantics, volume 7:Notes from the Linguistic Underground, pages 363?385.
Academic Press, New York.William A. Ladusaw.
1980.
On the notion ?affective?in the analysis of negative polarity items.
Journal ofLinguistic Research, 1(1):1?16.Heeyoung Lee, Yves Peirsman, Angel Chang, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2011.Stanford?s multi-pass sieve coreference resolution sys-tem at the CoNLL-2011 Shared Task.
In Proceedingsof CoNLL-2011: Shared Task, pages 28?34.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proceedings of HLT-EMNLP 2005,pages 25?32.Barbara H. Partee.
1987.
Noun phrase interpretationand type-shifting principles.
In Jeroen Groenendijk,Dick de Jong, and Martin Stokhof, editors, Studies inDiscourse Representation Theory and the Theory ofGeneralized Quantifiers, pages 115?143.
Foris Publi-cations, Dordrecht.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 Shared Task: Modeling multilingual unrestrictedcoreference in OntoNotes.
In Proceedings of EMNLPand CoNLL-2012: Shared Task, pages 1?40.Ellen Prince.
1981a.
On the inferencing of indefi-nite ?this?
NPs.
In Bonnie Lynn Webber, Ivan Sag,and Aravind Joshi, editors, Elements of Discourse Un-derstanding, pages 231?250.
Cambridge UniversityPress, Cambridge.Ellen F. Prince.
1981b.
Toward a taxonomy of given?new information.
In Peter Cole, editor, Radical Prag-matics, pages 223?255.
Academic Press, New York.R Core Team, 2012.
R: A Language and Environmentfor Statistical Computing.
R Foundation for StatisticalComputing, Vienna, Austria.Marta Recasens and Eduard Hovy.
2009.
A deeperlook into features for coreference resolution.
In SobhaLalitha Devi, Anto?nio Branco, and Ruslan Mitkov, ed-itors, Anaphora Processing and Applications, volume5847 of Lecture Notes in Computer Science, pages 29?42.
Springer.Marta Recasens and Eduard Hovy.
2011.
BLANC: Im-plementing the Rand index for coreference evaluation.Natural Language Engineering, 17(4):485?510.Craige Roberts.
1990.
Modal Subordination, Anaphora,and Distributivity.
Garland, New York.Craige Roberts.
1996.
Anaphora in intensional contexts.In Shalom Lappin, editor, The Handbook of Contem-porary Semantic Theory, pages 215?246.
BlackwellPublishers, Oxford.Johan Rooryck.
2001.
Evidentiality, Part II.
Glot Inter-national, 5(5):161?168.Roger Schwarzschild.
2002.
Singleton indefinites.
Jour-nal of Semantics, 19(3):289?314.Mandy Simons.
2007.
Observations on embeddingverbs, evidentiality, and presupposition.
Lingua,117(6):1034?1056.Ton van der Wouden.
1997.
Negative Contexts: Col-location, Polarity and Multiple Negation.
Routledge,London and New York.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceedingsof MUC-6, pages 45?52.Marilyn A. Walker, Aravind K. Joshi, and Ellen F. Prince,editors.
1997.
Centering in Discourse.
Oxford Uni-versity Press.Marilyn A. Walker, Aravind K. Joshi, and Ellen F. Prince.1998.
Centering in naturally-occurring discourse: Anoverview.
In Marilyn A. Walker, Aravind K. Joshi,and Ellen F. Prince, editors, Centering Theory in Dis-course, pages 1?28, Oxford.
Clarendon Press.Linton Wang, Eric McCready, and Nicholas Asher.
2006.Information dependency in quantificational subordina-tion.
In Klaus von Heusinger and Ken Turner, editors,632Where Semantics Meets Pragmatics, pages 267?304.Elsevier Science, Amsterdam.Gregory Ward and Betty Birner.
2004.
Informationstructure and non-canonical syntax.
In Laurence R.Horn and Gregory Ward, editors, The Handbook ofPragmatics, pages 153?174.
Blackwell, Oxford.633
