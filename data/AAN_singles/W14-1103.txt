Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 11?20,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsThe impact of near domain transfer on biomedical named entityrecognitionNigel Collier?European Bioinformatics InstituteHinxton, Cambridge, UK, andNational Institute of Informatics, Tokyo, JapanFerdinand PasterUniversity of Applied SciencesUpper AustriaHagenberg Campus, AustriaMai-vu TranUniversity of Engineering and Technology - VNUHanoi, VietnamAbstractCurrent research in fully supervisedbiomedical named entity recognition(bioNER) is often conducted in a settingof low sample sizes.
Whilst experi-mental results show strong performancein-domain it has been recognised thatquality suffers when models are applied toheterogeneous text collections.
Howeverthe causal factors have until now beenuncertain.
In this paper we describe a con-trolled experiment into near domain biasfor two Medline corpora on hereditarydiseases.
Five strategies are employedfor mitigating the impact of near domaintransference including simple transfer-ence, pooling, stacking, class re-labelingand feature augmentation.
We measuretheir effect on f-score performance againstan in domain baseline.
Stacking andfeature augmentation mitigate f-score lossbut do not necessarily result in superiorperformance except for selected classes.Simple pooling of data across domainsfailed to exploit size effects for mostclasses.
We conclude that we can expectlower performance and higher annotationcosts if we do not adequately compensatefor the distributional dissimilarities ofdomains during learning.1 IntroductionModel and feature selection are important exper-imental tasks in supervised machine learning forsuggesting approaches that will generalise well onreal world data.
Research in biomedical named en-tity recognition (bioNER) often displays two fea-tures: (1) small samples of labeled data, and (2)an implicit assumption that the future data will be?collier@ebi.ac.ukdrawn from a similar distribution to the labeleddata and hence that minimising expected predic-tion error on held out data will minimise actual fu-ture loss.
Since expert labeling is time consumingand expensive, labeled data sets tend to be rela-tively small, e.g.
(Kim et al., 2003; Tanabe et al.,2005; Pyysalo et al., 2007), in the region of a fewhundred or thousand Medline abstracts.
Despitethe danger of intrinsic idiosyncracies such corporaare often used to demonstrate putative predictionerror across the heterogeneous collection of 22million Medline abstracts.
Once this assumptionis made explicit it is of interest to both researchersand users that the implications and limitations ofsuch experimental settings are explored.Cross domain studies have indicated an ad-vantage for mechanisms that compensate for do-main bias.
For fully supervised learning, whichis the scenario we explore here, recent methodsinclude: feature augmentation (Daum?e III, 2007;Arnold et al., 2008; McClosky et al., 2010), in-stance weighting (Jiang and Zhai, 2007; Fosteret al., 2010), schema harmonisation (Wang et al.,2010) and semi-supervised/lightly supervised ap-proaches (Sagae and Tsujii, 2007; Liu et al., 2011;Pan et al., 2013).
More generally there is a widebody of work in transfer learning (also known asdomain adaptation) that tries to handle discrep-ancies between training and testing distributions(Pan and Yang, 2010).As an illustration of near domain bias considerthe list of high frequency named entities in Ta-ble 1 drawn from two sub-domains in the researchliterature of hereditary diseases.
A domain ex-pert in hereditary diseases would have no diffi-culty in dividing them into two non-overlappingsets corresponding to the two near domains withone term t5patients shared by both: {t1,t6,t8,t9}and {t2,t3,t4,t7,t10}.Previous studies have shown what happenswhen you radically change the domain and/or the11t1rheumatoid t6human leukocytearthritis antigent2lupus t7coronary hearterythematosus diseaset3leopard syndrome t8type 1 diabetest4Omapatrilat t9T1Dt5patients t10hypertensionTable 1: High frequency entities in the hereditorydisease literature for auto-immune and cardio-vascular diseases.annotation schema, e.g.
from newswire to Med-line or Web pages.
But what happens when theannotation schema, the annotator and the primarydomain stay the same?
Although the notion ofdomain is difficult to formalise in the context ofresearch literature, this study explores the con-dition where the variable factor is a shift to anear domain of literature as defined by biocura-tors and illustrated in the previous example.
Ourcontribution to biomedical named entity recogni-tion (bioNER) is in five areas:1.
We compare four data combination strate-gies for mitigating the impact of near domaintransference and measure their effect on f-score performance against an in domain base-line.2.
We provide additional evidence for the effec-tiveness of (Daum?e III, 2007)?s frustratinglysimple strategy which provides both generaland domain-specific features; in effect a jointlearning model.3.
Expectedly, but not trivially, we show thata general loss of f-score occurs on bioNERwhen transfering to near domains.
This lossis not uniform across all classes.
We provideclass-by-class drill down analysis to the un-derlying causal factors which make some en-tities more robust to near domain transferencein biomedicine than others.4.
Our results challenge the notion that pool-ing small corpora, even when guideline dif-ferences are reconciled, leads to improvedf-score performance (Wang et al., 2010;Wagholikar et al., 2013).5.
In addition to the usual biomedical entitytypes we introduce the class of phenotypeswhich are valued as indicators of genetic mal-function and characteristic of diseases.
Thephenotype class incorporates a complex de-pendency between classes, notably anatomi-cal entities and genes.This paper is organised as follows: Section 2describes related work in cross domain transferfor biomedical NER, Section 3 discusses our ap-proach including the two data sets used in our ex-periments, CRF model, feature choices and evalu-ation framework.
In Section 4 we outline our ex-perimental design.
Finally in Section 5 we com-pare the performance of six data selection strate-gies that try to maximise f-score performance ondomain entity classes in the target corpus.2 Related workIt is surprising that there exists, to the best of ourknowledge, no controlled study that has shed lighton the issue of near domain transfer for bioNERin a straightforward manner.
The closest approachto our investigation in the biomedical domain is(Wang et al., 2009).
Wang et al.
explore potentialsources of incompatibility across major bioNERcorpora with different annotation schema (GENIA- 2000 Medline abstracts, GENETAG - approx-imately 20,000 Medline sentences and AIMed -225 Medline abstracts).
They focus exclusivelyon protein name recognition and observe a drop inperformance of 12% f-score when combining datafrom different corpora.
Various reasons are putforwards such as differences in entity boundaryconventions, the scope of the entity class defini-tions, distributional properties of the entity classesand the degree of overlap between corpora.A follow up study by the authors (Wang etal., 2010) looked at increasing compatibility be-tween the GENIA and GENETAG corpora by re-organising the annotation schema to unify pro-tein, DNA and RNA NER under a new label GGP(Gene and Gene Product).
However the best per-formance from the coarse grained annotations stilldo not improve on the intra-corpus data.In earlier work, (Tsai et al., 2006) looked atschema differences between the JNLPBA corpusof 2000 Medline abstracts (Kim et al., 2004) andthe BioCreative corpus of 15,000 Medline sen-tences (Yeh et al., 2005) and tried to harmonisematching criteria.
They demonstrated that relax-ing the boundary matching criteria was helpful inmaximising the cross-domain performance.12In the clinical domain (Wagholikar et al.,2013), explore the effect of harmonising annota-tion guidelines on the 2010 i2b2 challenge withMayo Clinic Rochester (MCR) electronic patientrecords.
They concluded that the effectiveness ofpooling - i.e.
merging of corpora by ensuring acommon format and harmonised semantics - is de-pendent on several factors including compatibilitybetween the annotation schema and differences insize.
Again they noticed that simple pooling re-sulted in a loss of f-score, 12% for MCR and 4%for i2b2.
They concluded that the asymmetry waslikely due to size effects of the corpora, i.e.
MCRbeing smaller suffered a greater loss due to theclassifier being biased towards i2b2.Due to the formulation of these studies and theirlimited scope it has previously been difficult to un-derstand the precise causual factors affecting per-formance.
Our study sheds light on the expectedlevel of loss under different combination strategiesand more importantly highlights the non-uniformnature of that loss.3 ApproachWe assume two small labeled data sets DS=ds1..dsnand DT= dt1..dtm.
dsi= ?xi?
X, yi?Y ?
is drawn from an unknown distribution Psand represents the source document examples.Similarly,dti= ?xi?
X, yi?
Y ?
is also drawnfrom an unknown distribution Ptand representsthe target document examples.
We assume thatDShas N examples and DThas M exampleswhere N ?
M .
xirepresents a covariate or fea-ture vector and yiis a target or label that can takemultiple discrete values.
We have a learning al-gorithm that learns a function h : X ?
Y withminimal loss on the portion of DTused for test-ing.
Any combination of DSand DTwhich arenot used in testing can be used to learn h. Our taskis to explore various strategies for data selectionand re-factoring labels/features in order to max-imise held out performance.3.1 DataIn this paper we aim to empirically test domaintransferrence for bioNER under the condition thatthe test and training data are relatively small anddrawn from near domains, i.e.
from studies ondifferent types of heritable diseases.
To do thiswe selected Medline abstracts from PubMed thatwere cited by biocuration experts in the canon-ical database on heritable diseases, the OnlineMendelian Inheritance of Man (OMIM) (Hamoshet al., 2005).
We selected auto-immune diseasesand cardio-vascular diseases for our two corporawhich we denote as C1 and C2 respectively.
Bycomparing performance of a single model, a singleannotator and a single annotation scheme with arange of sampling techniques we hope to quantifythe effects of domain transferrence in isolation.The target classes for the entities are as follows:ANA Anatomical structures in the body.
e.g.liver, heart.CHE A chemical or drug.
e.g.
pristane, his-tamine, S-nitrosoglutathione.DIS Diseases.
e.g.
end stage renal disease, mitralvalve prolapse.GGP Genes and gene products.
e.g.
KLKB1gene, highly penetrant recessive major gene.PHE Phenotype entities describing observableand measurable characteristic of an organism.e.g.
cardiovascular abnormalities, abundantragged-red fibers, elevated IgE levels.ORG A living organism.
e.g.first-degree rela-tives, mice.The two corpora were annotated by a singleexperienced annotator who had participated inthe GENIA entity and event corpus annotation.We developed detailed guidelines for single spannone-nested entities before conducting a trainingand feedback session.
Feedback was conductedover two weeks by email and direct meetings withthe annotator and then annotation took approxi-mately two months.
The characteristics of the twocorpora are shown in Table 2.
Because annotationwas carried out by only one person we do not pro-vide inter-annotator scores.Importantly, we note four points at this stage:(1) We incorporate a new named entity type, phe-notype, which is aligned with investigations intoheritable diseases.
Semantically it is interestingbecause phenotypes annotated in the auto-immuneliterature pertain more often to sub-cellular pro-cesses and those in the cardiovascular domain per-tain more often to cells, tissues and organs; (2)It can be seen that two NE classes fall well be-low 500 instances - what we might arbitarily con-sider the necessary level of support for high lev-els of performance.
These are ANA and CHE;13C1 C2 a bAbstracts 110 80 - -Tokens 27,421 26,578 - -Av.
length 32.57 29.93 - -ANA 194 195 0.33 0.26(138) (133)CHE 44 147 0.08 0.07(33) (75)DIS 892 955 0.39 0.27(282) (442)GGP 1663 754 0.41 0.45(928) (511)ORG 799 770 0.56 0.67(429) (323)PHE 507 1430 0.52 0.33(423) (1113)Table 2: Characteristics of the C1 auto-immuneand C2 cardiovascular corpora: number of ab-stracts, number of tokens, average sentence length,frequency of each entity type.
Figures in parenthe-ses represent counts after removing duplication.
a:probability that a word in an entity class X in C1is also a word in entity class X in C2.
b: probabil-ity that a word in an entity class X in C2 is also aword in entity class X in C1(3) We calculated from Table 2 the average num-ber of mentions for each entity form by class andnoted that this is relatively stable across corpora,except for DIS which has less variation in C2 thanC1 and CHE which has more variation in C2 thanC1.
When combining evidence from both cor-pora the approximate order of type/token ratio arePHE < ANA < CHE,GGP < ORG < DISindicating that on average PHE entities have thegreatest variation.
Average entity lengths in to-kens (not shown) indicate that PHE are signifi-cantly longer than other entity mentions; and (4)We calculated the probability that a word token inan entity class from one corpus would appear inan instance of the same entity class in the othercorpus, reported as columns a and b.
Although theprobability of an exact match in instances betweenentities in the two corpora is generally quite low(below 20% - data not shown) there appears to besignificant vocabulary overlap in most classes ex-cept for chemicals.3.2 Conditional Random FieldsAs in (Finkel and Manning, 2009) we apply ourapproach to a linear chain conditional random field(CRF) model (Lafferty et al., 2001; McCallumand Wei, 2003; Settles, 2004; Doan et al., 2012)using the Mallet toolkit1with default parameters.CRFs have been shown consistently to be amongthe highest performing bioNER learners.
The dataselection strategies employed here though are neu-tral and could have been applied to any other fullysupervised learner model.3.3 FeaturesWe made use of a wide range of features, bothconventional features such as word or part ofspeech, as well as gazetteers derived from ex-ternal classification schemes that have been handcrafted by experts.
These are shown in Ta-ble 3.
Previous studies such as (Ratinov andRoth, 2009) have noted that domain gazetteerfeatures play a critical role in aiding classifi-cation.
In order to show realistic model be-haviour consistent with state-of-the-art techniqueswe have included gazetteers derived from: the Hu-man Phenotype Ontology (HPO: 15,800 terms),the Mammalian Phenotype Ontology (MP: 23,700terms), the Phenotypic Attribute and Trait On-tology (PATO: 2,200 synonyms), the BrendaTissue Ontology (BTO: 9,600 synonyms), theFoundation Model of Anatomy (FMA: 120,000terms), National Library of Medicine gene list(NLM: 9 million terms), UMLS disease terms(UMLS: 275,000 terms), Jochem chemical terms(JOCHEM: 320,000 terms).The feature set is quite large and therefore thereis a danger that the learner will be hindered.
Forfeature selection, we conducted baseline test runsunder the same experimental conditions as thosereported here using a grid search on features F1to F11 and found that f-score performance wasuniformly lower when removing any feature (datanot shown but available as supplementary materialfrom the first author).In order to characterise the contribution eachfeature is making in label prediction we wanted toprovide a measure of similarity between the fea-ture and the class label probability distributions.Here we use the Gain Ratio (GR) to estimate intra-corpus class prediction performance by each fea-ture.
GR was used as a splitting function in C4.51http://mallet.cs.umass.edu/14(Quinlan, 1993) and is defined asGR(C,F ) = IG(C,F )/H(F ) (1)where C represents a class label and F repre-sents a feature type.
IG is information gain anddefined as,IG(C,F ) = H(C)?H(C|X) (2)H is entropy and defined for feature types as,H(F ) = ?n?i=1p(fi)log2(p(fi)) (3)for n feature types fi?
F .
Further informa-tion can be found in (Quinlan, 1993).
GR is usedin C4.5 in preference to IG because of its abilityto normalise for the biases in IG.
Generally thisresults in GR having greater predictive accuracythan IR since it takes into account the number offeature values.
Note that GR is undefined whenthe denominator is zero.Several points emerge from looking at GR andIG values in Table 3:?
C1 (auto-immune) and C2 (cardio-vascular)have about the same information gain con-tribution from most features but C1 seemsto benefit more from GENIA named entitytagging, Human Phenotype Ontology (HPO),Foundation Model of Anatomy (FMA) andGene Ontology (GO) terms whereas C2 ben-efits more from the UMLS diseases andChEBI terms.?
GO, containing terms about genetic pro-cesses, has a higher GR in C1 than C2.
Thissupports what we already expected - thatauto-immune diseases contain a higher pro-portion of information about genetic processphenotypes than cardiovascular.?
The GENIA POS tags seem to provide aslightly higher GR in C2 than in C1.?
Despite its large size, UMLS has a smallerGR on both corpora compared to some otherresources like HPO or GO or MA.
This is de-spite its high IG value.3.4 EvaluationTraditional re-sampling using k-fold cross valida-tion (k-CV) divides the n labelled documents intok disjoint subsets of approximately equal size des-ignated as Difor i = 1, .., k. The NER learneris trained successively on k ?
1 folds from D andtested on a held out fold over k iterations.
In or-der to preserve independence between contexts intraining and held out data we assume here that theunit of division is the document, i.e.
a single Med-line abstract.
Estimated prediction error is calcu-lated based on the learner?s labels on the k heldout folds.
Whilst k-CV is known to be nearly un-biased it is a highly variable estimator.
Severalstudies have looked at k-CV for small sample sets.For example, (Braga-Neto and Dougherty, 2004)found on classifier experiments for small microar-ray samples (20 <= n <= 120) that whilst k-CV showed low bias they suffered from excessivevariance compared to bootstrap or resubstitutionestimators.One cause of variance has been identified aswithin-block and between-block training errorsarising from the disproportionate effects of a sin-gle abstract appearing in the training set of manyfolds.
In order to reduce this effect Monte Carlocross validation was used (also called CV with rep-etition).
100 iterations were used to randomly re-order the documents in the corpora before 10-foldCV sampling was run (cv10r100).
Sampling ofdocuments is done without replacement so that theindependence between training and testing sets aremaintained.
Stratification was not applied.
Mi-cro averaged f-scores for labeling accuracy werecalculated based on the 1000 test folds for eachmodel.
Evaluation was done in both directions(training and testing) for each corpus C1 and C2to show any asymmetrical effects.
To minimse thetime taken for each experiment a cluster computerwas used with 48 nodes.The matching criteria we employ is the exactmatch - i.e.
the span of the system labeling andthe held out data labels should be exactly the same.Although this is not a necessary criteria for someapplications such as database curation we used ithere as it is widely applied in shared evaluationsand shows the clearest effects of modeling choice.We evaluate using the named entity precision,recall and F-score calculated using the CoNLL2003 Perl script.
This was calculated as,f ?
score =(2?
precision?
recall)(precision + recall)(4)where,15Feature IG(C1, Fi) GR(C1, Fi) IG(C2, Fi) GR(C2, Fi)F1Word 1.17 0.13 1.20 0.13F2Lemma 1.15 0.13 1.18 0.13F3POS tag 0.36 0.09 1.18 0.13F4Chunk tag 0.22 0.12 0.26 0.10F5GENIA NEa0.20 0.35 0.14 0.27F6Orthography 0.15 0.08 0.16 0.08F7Domain prefix 0.11 0.11 0.11 0.10F8Domain suffix 0.08 0.11 0.08 0.11F9Word length 0.13 0.05 0.16 0.06F10Parenthesis 0.04 0.20 0.04 0.23F11Abbreviation 0.08 0.22 0.06 0.24F12HPOb0.07 0.41 0.09 0.33F13MPc0.03 0.33 0.06 0.33F14PATOd0.01 0.03 0.02 0.04F15BTOe0.03 0.32 0.03 0.29F16FMAf0.05 0.28 0.05 0.23F17MAg0.02 0.31 0.02 0.29F18PROh0.02 0.12 0.03 0.15F19ChEBIi0.01 0.15 0.03 0.20F20JOCHEMj0.01 0.15 0.01 0.14F21NCBIk0.01 0.14 0.01 0.14F22UMLSldisease 0.01 0.14 0.03 0.24F23NCBI gene 0.02 0.18 0.02 0.19F24GOm0.13 0.38 0.05 0.28F25UMLSn0.48 0.12 0.52 0.11F2645CLUSTERSo0.50 0.10 0.47 0.10Table 3: Features used in the experiments.aThe GENIA named entity tagger (Kim et al., 2003),b(Robinson et al., 2008),c(Smith et al., 2004),d(Gkoutos et al., 2005),e(Gremse et al., 2011),f(RosseandMejino, 2003),g(Hayamizu et al., 2005),h(Natale et al., 2011) ,i(Degtyarenko et al., 2008),j(Hettneet al., 2009),k(Federhen, 2012),l(Lindberg et al., 1993),m(Gene Ontology Consortium, 2000),n133 cat-egories from the UMLS,o45 cluster classes derived by Richard Socher and Christoph Manning PubMedavailable at http://nlp.stanford.edu/software/bionlp2011-distsim-clusters-v1.tar.gzprecision = TP/(TP + FP ) (5)and,recall = TP/(TP + FN) (6)A true positive (TP) is a gold standard NEtagged by the system as an NE.
A true negative(TN) is a gold standard none-NE tagged by thesystem as a none-NE.
A false positive (FP) is agold standard none-NE tagged by the system asan NE.
Evaluation is based on correctly markedwhole entities rather than tokens.4 Experimental designIn this section we present the experimental condi-tions we used, starting with a description of themodels which we designate M1 to M6 and de-scribe below.
All methods made use of 100 iter-ations of Monte Carlo 10-fold cross validation.M1: IN DOMAIN We trained and tested on onlythe data for the source domain.
This methodsforms our baseline and represents the stan-dard experimental setting.M2: OUT DOMAIN We trained on the sourcedomain and tested on the target domain.
Thismethod shows expected loss on near domaintransferrence and represents the standard op-erational setting for users.16M3: MIX-IN We trained on 100% of the sourcedomain and unified this with 90% of thefolded in target domain data, leaving 10%for testing.
This method reflects the poolingtechnique typically employed in corpus con-struction for bioNER.M4: STACK We trained a CRF model on 100%of the source domain and stacked it withanother CRF trained on 90% of the foldedin target domain data.
Stacking employs ameta-classifier and is a popular method forconstructing high performance ensembles ofclassifiers (Ekbal and Saha, 2013).
In thiscase we collected the output labels from thesource domain-trained CRF on target sen-tences and added them as features for the tar-get domain trained CRF.M5: BINARY CLASS We re-labeled the com-plex class PHE as PHE-C1 in C1 and PHE-C2 in C2 and repeated M3.
Afterwards werecombined PHE-C1 and PHE-C2 into PHE.M6: FRUSTRATINGLY SIMPLE We fol-lowed the feature augmentation approach of(Daum?e III, 2007).
This method effectivelyprovides a joint learning model on C1 andC2 by splitting each feature into three parts:one for sharing cross domain values and onefor each domain specific value.
We evaluatedusing the same regime as M3.5 Experimental results and discussionIn Table 4 we show f-score performance from nearbiomedical domains with our six strategies.
Thissection now tries to draw together an interpretationfor the performance trends that we see and to drilldown to some of the causal factors.Held out tests performed in-domain (M1) onboth corpora C1 and C2 indicate a relatively highlevel of performance, conservatively in line withstate-of-the-art estimates.
The broad trend in per-formance is for entity classes with more instancesto out perform others with lower numbers.
Theclass which most obviously breaks this trend isthe complex entity type of PHE.
To understandthis consider that PHE is defined as an observableproperty on an organism and as such tends to beformed from a quality such as malformed that de-scribes a structural entity such as valve.
To seecloser what is happening we looked at the confu-sion matrices for M1 on both corpora.
For bothC1 and C2 we observed that a substantial pro-portion of words inside PHE sequences were con-fused with GGP, DIS or ANA entities.
Similarlya high proportion of words inside ANA sequenceswere confused with PHE entities.
This indicatesthat dependencies within complex biomedical en-tities like PHE might better be modeled explicitlyusing tree-structures in a manner similar to eventsrather than using n-gram relations.In the M2 out of domain experiments we seea generally severe loss of f-score performanceacross most classes.
Training on C2 and testingon C1 results in a 19.1% loss (F1 69.9 to 50.8)and training on C1 and testing on C2 results ina 11.9% loss overall (F1 58.5 to 46.6).
The re-sults agree with Wang et al.
?s experience on het-erogeneous Medline corpora and extend the upperlimit on all-class loss due to domain transferrenceto 19%.
The only NE class where we see a sym-metric benefit from pooling entities in M3 is forORG (F1 68.4 to 72.2, F1 73.2 to 77.4).
Intrigu-ingly the data from Tables 2 and 4 hint at a correla-tion between the success of M3 pooling for ORGand broad cross-domain compatibility on the vo-cabulary (over 50% of ORG vocabulary is sharedacross corpora).
However this is not supportedin the low sharing case for CHE where we seeincreased performance from pooling (F1 31.3 to38.7) when the target is C2 but decreased perfor-mance when the target is C1 (F1 29.5 to 20.0).When we look at the pooling method (M3) andcompare to the in-domain method (M1) no obvi-ous size effect occurs for the number of entitiesin each class.
To see this we can examine entityclasses with an imbalanced number of instancesin C1 and C2 such as CHE, GGP and PHE.
Con-sider the following three cases: (1) Adding 147instances of CHE from C2 to 44 instances fromC1 is associated with CHE performance droppingfrom M1:29.5 to M3:20.0 when tested on C1; (2)Similarly adding 1430 instances of PHE from C2to 507 instances from C1 is associated with PHEperformance dropping from 46.0 in M1 to 39.7 inM3 when tested on C1; (3) But adding 1663 in-stances of GGP from C1 to 754 from C2 is asso-ciated with GGP rising from 57.2 in M1 to 61.1 inM3.
If simply pooling more entities was impor-tant to improved f-score we would expect to see aclearer pattern of improvement but we do not.The overall pooling loss for all classes on M3is within 3% in both directions and within the17Model Target ANA CHE DIS GGP PHE ORG ALLM1 C1 57.1 29.5 80.4 74.0 46.0 68.4 69.9M2 C1 34.3 26.9 57.7 55.6 26.9 64.0 50.8M3 C1 50.8 20.0 77.9 71.7 39.7 72.2 67.3M4 C1 56.3 17.4 79.0 74.1 44.1 70.8 69.8M5 C1 56.7 29.6 77.3 72.7 41.5 72.8 68.3M6 C1 57.1 27.7 79.0 73.4 44.9 69.9 69.5M1 C2 37.2 31.3 72.9 57.2 46.5 73.2 58.5M2 C2 21.2 20.2 57.0 52.3 24.4 68.5 46.6M3 C2 36.8 38.7 72.3 61.1 44.0 77.4 59.7M4 C2 34.8 34.4 72.5 57.5 45.9 74.7 58.5M5 C2 34.1 41.6 73.6 58.9 43.2 78.5 59.6M6 C2 39.9 35.0 73.3 56.4 46.6 75.0 59.1Table 4: Named entity recognition f-scores using Methods 1 to 6.
All methods were tested using 100iterations of Monte Carlo 10-fold cross validation.
Figures in bold show best in class scores.
Figures initalics show scores above the M1 baseline.bounds observed by (Wang et al., 2009) and(Wagholikar et al., 2013) for their pooling of het-erogeneous Medline corpora.
Except for the ORGclass which we higlighted above, we might cau-tiously quantify the loss of pooled entity mentionsas being in the range up to 9.5% for CHE but moretypically below 4%.
The majority of the differ-ences they observed - which are not present in ourdata - are most likely due to concept definition dif-ferences and annotation conventions.In contrast to our expectations the M4 experi-ments showed very mild benefits for stacking andthese were mixed across entity types.
M4 testson C2 showed no general improvement but someimprovement in CHE and ORG.
M4 tests on C1resulted again in no overall improvement exceptfor some gain for ORG, supporting our hypothesisthat there is greater compatibility in ORG acrossdomains.The M5 approach of splitting the PHE labels forthe two corpora resulted in a noticable improve-ment over M3 on the C1 test but unfortunately thiswas not sustained when testing on C2.It is striking that in the M6 experiments the fea-ture augmentation method only just meets the in-domain f-score on C1 and mildly exceeds it on C2.One explanation is that the corpora are so smallthat a richer feature set has only marginal effectson performance.
Table 3 certainly indicates thatmany of the features have low predictive capac-ity (gain ratio values below 0.1) in an intra-corpussetting but this is not the case for others such asGENIA NE tags or HPO gazzetteer terms.Overall when we average the f-scores acrossmodels for C1 and C2 we see that there is amarginal benefit to the M1, M4 and M6 strategiesover M3 and M5 with M2 suffering the greatestloss in performance.6 ConclusionIn this paper we have provided evidence that trans-ference even to closely related domains in biomed-ical NER incurs a severe loss in f-score.
Wehave demonstrated empirically that strategies thatmake use of multi-domain corpora such as stack-ing learners and feature augmentation mitigate theaccuracy loss but do not necessarily result in supe-rior performance except for selected classes suchas organisms where there appears to be broadterminology consensus.
Simple pooling of dataacross domains failed to exploit size effects espe-cially for the complex class of phenotypes.
Thelist of strategies employed has not been exhaus-tive and it is possible that others such as featurehierarchies (Arnold et al., 2008) might yield betterresults.BioNER is complicated by various factors suchas descriptive names, polysemous terms, conjuc-tions, nested constructions and a high quantity ofabbreviations.
We have shown that performance isalso held back by not considering document levelproperties related to domain such as topicality.
Wecan expect lower performance and higher annota-tion costs if we do not adequately allow for the dis-tributional dissimilarities of domains during learn-ing, even in closely related topical settings.18AcknowledgmentsThe authors gratefully acknowledge the manyhelpful comments from the anonymous review-ers of this paper.
Nigel Collier?s research issupported by the European Commission throughthe Marie Curie International Incoming Fellow-ship (IIF) programme (Project: Phenominer, Ref:301806).ReferencesA.
Arnold, N. Nallapati, and W. Cohen.
2008.
Exploit-ing feature hierarchy for transfer learning in namedentity recognition.
In Annual meeting of the Asso-ciation for Computational Linguistics (ACL 2008),pages 245?253.U.
Braga-Neto and E. Dougherty.
2004.
Is cross-validation valid for small-sample microarray classi-fication?
Bioinformatics, 20(3):374?380.H.
Daum?e III.
2007.
Frustratingly easy domain adap-tation.
In Annual meeting of the Association forComputational Linguistics (ACL 2007), pages 256?263.K.
Degtyarenko, P. de Matos, M. Ennis, J. Hastings,M.
Zbinden, A. McNaught, R. Alc?antara, M. Dar-sow, M. Guedj, and M. Ashburner.
2008.
ChEBI:a database and ontology for chemical entities of bi-ological interest.
Nucleic acids research, 36(suppl1):D344?D350.S.
Doan, N. Collier, H. Xu, P. Duy, and T. Phuong.2012.
Recognition of medication information fromdischarge summaries using ensembles of classifiers.BMC Medical Informatics and Decision Making,12(1):36.A.
Ekbal and S. Saha.
2013.
Stacked ensemble cou-pled with feature selection for biomedical entity ex-traction.
Knowledge-Based Systems.S.
Federhen.
2012.
The NCBI taxonomy database.Nucleic acids research, 40(D1):D136?D143.J.
Finkel and C. Manning.
2009.
Hierarchical bayesiandomain adaptation.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 602?610.G.
Foster, C. Goutte, and R. Kuhn.
2010.
Discrim-inative instance weighting for domain adaptation instatistical machine translation.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2010), pages 451?459.Gene Ontology Consortium.
2000.
Gene ontology:tool for the unification of biology.
Nature Genetics,25:19?29.G.
Gkoutos, E. Green, A. Mallon, J. Hancock, andD.
Davidson.
2005.
Using ontologies to describemouse phenotypes.
Genome Biology, 6:R8.M.
Gremse, A. Chang, I. Schomburg, A. Grote,M.
Scheer, C. Ebeling, and D. Schomburg.
2011.The BRENDA tissue ontology (BTO): the firstall-integrating ontology of all organisms for en-zyme sources.
Nucleic Acids Research, 39(suppl1):D507?D513.A.
Hamosh, A. F. Scott, J. S. Amberger, and C. A. Boc-chini.
2005.
Online mendelian inheritance of man(OMIM), a knowledgebase of human genes and ge-netic disorders.
Nucleic Acids Research, 33(suppl1):D514?D517.T.
Hayamizu, M. Mangan, J. Corradi, J. Kadin,M.
Ringwald, et al.
2005.
The adult mouse anatom-ical dictionary: a tool for annotating and integratingdata.
Genome Biol, 6(3):R29.K.
Hettne, R. Stierum, M. Schuemie, P. Hendriksen,B.
Schijvenaars, E. van Mulligen, J. Kleinjans, andJ.
Kors.
2009.
A dictionary to identify smallmolecules and drugs in free text.
Bioinformatics,25(22):2983?2991.J.
Jiang and C. Zhai.
2007.
Instance weighting fordomain adaptation in NLP.
In Annual meeting ofthe Association for Computational Linguistics (ACL2007), volume 2007, page 22.J.
D. Kim, T. Ohta, Y. Tateishi, and J. Tsujii.
2003.GENIA corpus - a semantically annotated corpus forbio-textmining.
Bioinformatics, 19(Suppl.1):180?182.J.
Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-lier.
2004.
Introduction to the bio-entity recog-nition task at JNLPBA.
In N. Collier, P. Ruch,and A. Nazarenko, editors, Proceedings of the In-ternational Joint Workshop on Natural LanguageProcessing in Biomedicine and its Applications(JNLPBA), Geneva, Switzerland, pages 70?75, Au-gust 28?29.
held in conjunction with COL-ING?2004.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: probabilistic models for seg-menting and labeling sequence data.
In Proceed-ings of the Eighteenth International Conference onMachine Learning, Massachusetts, USA, pages 282?289, June 28th ?
July 1st.Donald A.B.
Lindberg, L. Humphreys, Betsy, andT.
McCray, Alexa.
1993.
The unified medical lan-guage system.
Methods of Information in Medicine,32:281?291.Xiaohua Liu, Shaodian Zhang, Furu Wei, and MingZhou.
2011.
Recognizing named entities in tweets.In Annual meeting of the Association for Computa-tional Linguistics (ACL 2011), pages 359?367.19A.
McCallum and L. Wei.
2003.
Early results fornamed entity recognition with conditional randomfields, feature induction and web-enhanced lexicons.In Proc.
Seventh Conference on Natural languagelearning at HLT-NAACL 2003 - Volume 4, CONLL?03, pages 188?191.D.McClosky, E. Charniak, andM.
Johnson.
2010.
Au-tomatic domain adaptation for parsing.
In HumanLanguage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics, pages 28?36.Association for Computational Linguistics.D.
Natale, C. Arighi, W. Barker, J. Blake, C. Bult,M.
Caudy, H. Drabkin, P. DEustachio, A. Evsikov,H.
Huang, et al.
2011.
The protein ontology: astructured representation of protein forms and com-plexes.
Nucleic acids research, 39(suppl 1):D539?D545.S.
Pan and Q. Yang.
2010.
A survey on transfer learn-ing.
Knowledge and Data Engineering, IEEE Trans-actions on, 22(10):1345?1359.S.
Pan, Z. Toh, and J. Su.
2013.
Transfer joint em-bedding for cross-domain named entity recognition.ACM Transactions on Information Systems (TOIS),31(2):7.S.
Pyysalo, F. Ginter, J. Heimonen, J. Bj?orne,J.
Boberg, J. J?arvinen, and T. Salakoski.
2007.Bioinfer: a corpus for information extraction in thebiomedical domain.
BMC bioinformatics, 8(1):50.J.
Quinlan.
1993.
C4.
5: programs for machine learn-ing, volume 1.
Morgan kaufmann.L.
Ratinov and D. Roth.
2009.
Design challenges andmisconceptions in named entity recognition.
In Pro-ceedings of the Thirteenth Conference on Computa-tional Natural Language Learning (CoNLL), pages147?155.P.
N. Robinson, S. Kohler, S. Bauer, D. Seelow,D.
Horn, and S. Mundlos.
2008.
The human pheno-type ontology: a tool for annotating and analyzinghuman hereditary disease.
The American Journal ofHuman Genetics, 83(5):610?615.C.
Rosse and J. L. V. Mejino.
2003.
A reference on-tology for bioinformatics: the Foundational Modelof Anatomy.
Journal of Biomedical Informatics,36(6):478?500, December.
PMID: 14759820.K.
Sagae and J. Tsujii.
2007.
Dependency parsingand domain adaptation with lr models and parser en-sembles.
In Conference on Empirical Methods inNatural Language Processing Conference on Com-putational Natural Language Learning (EMNLP-CoNLL), volume 2007, pages 1044?1050.B.
Settles.
2004.
Biomedical named entity recognitionusing conditional random fields.
In Proceedings ofthe International Joint Workshop on Natural Lan-guage Processing in Biomedicine and its Applica-tions (JNLPBA) at COLING?2004, Geneva, Switzer-land, pages 104?107, August 28?29.C.
L. Smith, C. W. Goldsmith, and J. T. Eppig.
2004.The mammalian phenotype ontology as a tool for an-notating, analyzing and comparing phenotypic infor-mation.
Genome Biology, 6:R7.L.
Tanabe, N. Xie, L. H. Thom, W. Matten, and W. J.Wilbur.
2005.
GENETAG: a tagged corpus forgene/protein named entity recognition.
BMC Bioin-formatics, 6(Suppl 1):S3.R.
Tsai, S. Wu, W. Chou, Y. Lin, D. He, J. Hsiang,T.
Sung, and W. Hsu.
2006.
Various criteria in theevaluation of biomedical named entity recognition.BMC bioinformatics, 7(1):92.K.
Wagholikar, M. Torii, S. Jonnalagadda, H. Liu, et al.2013.
Pooling annotated corpora for clinical con-cept extraction.
J. Biomedical Semantics, 4:3.Y.
Wang, J. Kim, R. S?tre, S. Pyysalo, and J. Tsujii.2009.
Investigating heterogeneous protein annota-tions toward cross-corpora utilization.
BMC bioin-formatics, 10(1):403.Y.
Wang, J. Kim, R. S?tre, S Pyysalo, T. Ohta, andJ.
Tsujii.
2010.
Improving the inter-corpora com-patibility for protein annotations.
Journal of bioin-formatics and computational biology, 8(05):901?916.A.
Yeh, A. Morgan, M. Colosimo, and L. Hirschman.2005.
Biocreative task 1a: gene mention findingevaluation.
BMC bioinformatics, 6(Suppl 1):S2.20
