Word-Sense Disambiguation Using Decomposable ModelsRebecca Bruce  and Janyce  WiebeComput ing  Research LabandDepar tment  of Computer  ScienceNew Mexico State UniversityLas Cruces, NM 88003rbruce@cs.nmsu.edu, wiebe@cs.nmsu.eduAbst rac tMost probabilistic classifiers used for word-sense disam-biguation have either been based on only one contextualfeature or have used a model that is simply assumedto characterize the interdependencies among multiplecontextual features.
In this paper, a different approachto formulating a probabilistic model is presented alongwith a case study of the performance of models pro-duced in this manner for the disambiguation f the nouninterest.
We describe a method for formulating proba-bilistic models that use multiple contextual features forword-sense disambiguation, without requiring untestedassumptions regarding the form of the model.
Usingthis approach, the joint distribution of all variables isdescribed by only the most systematic variable inter-actions, thereby limiting the number of parameters tobe estimated, supporting computational efficiency, andproviding an understanding of the data.In t roduct ionThis paper presents a method for constructing prob-abilistic classifiers for word-sense disambiguation thatoffers advantages over previous approaches.
Most pre-vious efforts have not attempted tosystematically iden-tify the interdependencies among contextual features(such as collocations) that can be used to classify themeaning of an ambiguous word.
Many researchers haveperformed isambiguation the basis of only a singlefeature, while others who do consider multiple contex-tual features assume that all contextual features areeither conditionally independent given the sense of theword or fully independent.
Of course, all contextual fea-tures could be treated as interdependent, but, if thereare several features, uch a model could have too manyparameters to estimate in practice.We present a method for formulating probabilisticmodels that describe the relationships among all vari-ables in terms of only the most important interdepen-dencies, that is, models of a certain class that are goodapproximations to the joint distribution of contextualfeatures and word meanings.
This class is the set of de-composable models: models that can be expressed as aproduct of marginal distributions, where each marginalis composed of interdependent variables.
The test usedto evaluate a model gives preference to those that havethe fewest number of interdependencies, thereby select-ing models expressing only the most systematic variableinteractions.To summarize the method, one first identifies infor-mative contextual features (where "informative" is awell-defined notion, discussed in Section 2).
Then, outof all possible decomposable models characterizing in-terdependency relationships among the selected vari-ables, those that are found to produce good approxima-tions to the data are identified (using the test mentionedabove) and one of those models is used to perform dis-ambiguation.
Thus, we are able to use multiple contex-tual features without he need for untested assumptionsregarding the form of the model.
Further, approximat-ing the joint distribution of all variables with a modelidentifying only the most important systematic interac-tions among variables limits the number of parametersto be estimated, supports computational efficiency, andprovides an understanding of the data.
The biggest lim-itation associated with this method is the need for largeamounts of sense-tagged data.
Because asymptotic dis-tributions of the test statistics are used, the validity ofthe results obtained using this approach are compro-mised when it is applied to sparse data (this point isdiscussed further in Section 2).To test the method of model selection presented inthis paper, a case study of the disambiguation f thenoun interest was performed.
Interest was selected be-cause it has been shown in previous tudies to be a dif-ficult word to disambiguate.
We selected as the set ofsense tags all non-idiomatic noun senses of interest de-fined in the electronic version of Longman's Dictionaryof Contemporary English (LDOCE) (\[23\]).
Using themodels produced in this study, we are able to assign anLDOCE sense tag to every usage of interest in a held-out test set with 78% accuracy.
Although it is difficultto compare our results to those reported for previousdisambiguation experiments, aswill be discussed later,we feel these results are encouraging.The remainder of the paper is organized as follows.Section 2 provides a more complete definition of the139methodology used for formulating decomposable mod-els and Section 3 describes the details of the case studyperformed to test the approach.
The results of the dis-ambiguation case study are discussed and contrastedwith similar efforts in Sections 4 and 5.
Section 6 is theconclusion.Decomposab le  Mode lsIn this Section, we address the problem of findingthe models that generate good approximations to agiven discrete probability distribution, as selected fromamong the class of decomposable models.
Decomposablemodels are a subclass of log-linear models and, as such,can be used to characterize and study the structureof data (\[2\]), that is, the interactions among variablesas evidenced by the frequency with which the valuesof the variables co-occur.
Given a data sample of ob-jects, where each object is described by d discrete vari-ables, let x=(zz,  z2 , .
.
.
,  zq) be a q-dimensional vectorof counts, where each zi is the frequency with which oneof the possible combinations of the values of the d vari-ables occurs in the data sample (and the frequencies ofall such possible combinations are included in x).
Thelog-linear model expresses the logarithm of E\[x\] (themean of x) as a linear sum of the contributions of the"effects" of the variables and the interactions amongthe variables.Assume that a random sample consisting of N inde-pendent and identical tridls (i.e., all trials are describedby the same probability density function) is drawn froma discrete d-variate distribution.
In such a situation, theoutcome of each trial must be an event corresponding toa particular combination of the values of the d variables.Let Pi be the probability that the ith event (i.e., the i thpossible combination of the values of all variables) oc-curs on any trial and let zi be the number of timesthat the i th event occurs in the random sample.
Then(zt, x2, .
.
.
,  zq) has a multinomiM distribution with pa-rameters N and P l , .
.
.
,  Pq- For a given sample size, N,the likelihood of selecting any particular andom sam-ple is defined once the population parameters, that is,the Pi'S or, equivalently, the E\[xi\]'s (where E\[zi\] is themean frequency of event i), are known.
Log-linear mod-els express the value of the logarithm of each E\[~:i\] or p;as a linear sum of a smaller (i.e., less than q) number ofnew population parameters that characterize the effectsof individual variables and their interactions.The theory of log-linear models specifies the suffi-cient s lat is l ics (functions of x) for estimating the ef-fects of each variable and of each interaction amongvariables on E\[x\].
The sufficient statistics are the sam-ple counts from the highest-order marginals composedof only interdependent variables.
These statistics arethe maximum likelihood estimates of the mean valuesof the corresponding marginals distributions.
Consider,for example, a random sample taken from a popula-tion in which four contextual features are used to char-acterize each occurrence of an ambiguous word.
Thesufficient statistics for the model describing contextualfeatures one and two as independent but all other vari-ables as interdependent are, for all i, j, k, m, n (in thisand all subsequent equations, f is an abbreviation forfeature) :t~\[count(f2 = j, f3 = k, f4 = m,  tag = n)\] =E Xfx=i,f2=j,f3=k,f4=m,tag=niandl~\[count(fl = i, f3 = k, f4 = m,  tag = n)\] =E Xfa=i,f2=j,f3=k,f4=rn,tag=nJWithin the class of decomposable models, the maxi-mum likelihood estimate for E\[x\] reduces to the productof the sufficient statistics divided by the sample countsdefined in the marginals composed of the common el-ements in the sufficient statistics.
As such, decompos-able models are models that can be expressed as a prod-uct of marginals, 1 where each marginal consists of onlyinterdependent variables.Returning to our previous example, the maximumlikelihood estimate for E\[x\] is, for all i , j ,  k, m,  n:E\[z11=i,l~=j,13=k,1,=m,t~g=n \] =\]~\[count(fl = i, f3 = k, f4 = m,  tag -- n)\] ?\]~\[count(f2 = j, f3 = k, f4 = m,  tag = n)\] --\]~\[count(/a = k, f4 = m,  tag = n)\]Expressing the population parameters as probabil-ities instead of expected counts, the equation abovecan be rewritten as follows, where the sample marginalrelative frequencies are the maximum likelihood esti-mates of the population marginal probabilities.
For alli , j , k ,m,n :P ( f t  = i, f2 = j,  f3 = k, f4 = m,  tag -- n) == i = A = m,  tag  = n)  ?P( f2  = j I f3 = k, f4 = m,  tag= n) ?P( f3  : k, f4 = m,  tag = n)The degree to which the data is approximated by amodel is called the fit of the model.
In this work, thelikelihood ratio statistic, G 2, is used as the measure ofthe goodness-of-fit of a model.
It is distributed asymp-totically as X z with degrees of freedom corresponding tothe number of interactions (and/or variables) omittedfrom (unconstrained in) the model.
Accessing the fit1The marginal distributions can be represented in termsof counts or relative frequencies, depending on whether theparameters are expressed as expected frequencies or proba-bilities, respectively.140of a model in terms of the significance of its G 2 statis-tic gives preference to models with the fewest numberof interdependencies, thereby assuring the selection ofa model specifying only the most systematic variableinteractions.Within the framework described above, the processof model selection becomes one of hypothesis testing,where each pattern of dependencies among variablesexpressible in terms of a decomposable model is pos-tulated as a hypothetical model and its fit to the datais evaluated.
The "best fitting" models are identified,in the sense that the significance of their reference X2values are large, and, from among this set, a conceptu-ally appealing model is chosen.
The exhaustive searchof decomposable models can be conducted as describedin \[12\].What we have just described is a method for approx-imating the joint distribution of all variables with amodel containing only the most important systematicinteractions among variables.
This approach to modelformulation limits the number of parameters to be esti-mated, supports computational efficiency, and providesan understanding of the data.
The single biggest limita-tion remaining in this day of large memory, high speedcomputers results from reliance on asymptotic theoryto describe the distribution of the maximum likelihoodestimates and the likelihood ratio statistic.
The effectof this reliance is felt most acutely when working withlarge sparse multinomials, which is exactly when thisapproach to model construction is most needed.
Whenthe data is sparse, the usual asymptotic properties ofthe distribution of the likelihood ratio statistic and themaximum likelihood estimates may not hold.
In suchcases, the fit of the model will appear to be too good,indicating that the model is in fact over constrained forthe data available.
In this work, we have limited our-selves to considering only those models with sufficientstatistics that are not sparse, where the significance ofthe reference X 2 is not unreasonable; most such modelshave sufficient statistics that are lower-order marginaldistributions.
In the future, we will investigate othergoodness-of-fit tests (\[18\], \[1\], \[22\]) that are perhapsmore appropriate for sparse data.The ExperimentUnlike several previous approaches to word sense disam-biguation (\[29\], \[5\], \[7\], \[10\]), nothing in this approachlimits the selection of sense tags to a particular num-ber or type of meaning distinctions.
In this study, ourgoal was to address a non-trivial case of ambiguity, butone that would allow some comparison of results withprevious work.
As a result of these considerations, theword interest was chosen as a test case, and the sixnon-idiomatic noun senses of interest defined in LDOCEwere selected as the tag set.
The only restriction lim-iting the choice of corpus is the need for large amountsof on-line data.
Due to availability, the Penn TreebankWall Street Journal corpus was selected.In total, 2,476 usages 2 of interest as a noun 3 wereautomatically extracted from the corpus and manuallyassigned sense tags corresponding to the LDOCE defi-nitions.During tagging, 107 usages were removed from thedata set due to the authors' inability to classify themin terms of the set of LDOCE senses.
Of the rejectedusages, 43 are metonymic, and the rest are hybridmeanings pecific to the domain, such as public interestgroup.Because our sense distinctions are not merely be-tween two or three clearly defined core senses of a word,the task of hand-tagging the tokens of interest requiredsubtle judgments, a point that has also been observedby other researchers disambiguating with respect o thefull set of LDOCE senses (\[6\], \[28\]).
Although this un-doubtedly degraded the accuracy of the manually as-signed sense tags (and thus the accuracy of the studyas well), this problem seems unavoidable when makingsemantic distinctions beyond clearly defined core sensesof a word (\[17\], \[11\], \[14\], \[15\]).Of the 2,369 sentences containing the sense-taggedusages of interest, 600 were randomly selected and setaside to serve as the test set.
The distribution of sensetags in the data set is presented in Table 1.We now turn to the selection of individually infor-mative contextual features.
In our approach to disam-biguation, a contextual feature is judged to be informa-tive (i.e., correlated with the sense tag of the ambiguousword) if the model for independence between that fea-ture and the sense tag is judged to have an extremelypoor fit using the test described in Section 2.
The worsethe fit, the more informative the feature is judged to be(similar to the approach suggested in \[9\]).Only features whose values can be automatically de-termined were considered, and preference was given tofeatures that intuitively are not specific to interest (butsee the discussion of collocational features below).
Anadditional criterion was that the features not have toomany possible values, in order to curtail sparsity in theresulting data matrix.We considered three different ypes of contextual fea-tures: morphological, collocation-specific, and class-based, with part-of-speech (POS) categories serving asthe word classes.
Within these classes, we choose anumber of specific features, each of which was judged tobe informative as described above.
We used one mor-phological feature: a dichotomous variable indicatingthe presence or absence of the plural form.
The valuesof the class-based variables are a set of twenty-five POStags formed, with one exception, from the first letter ofthe tags used in the Penn Treebank corpus.
Two dif-ferent sets of class-based variables were selected.
The2For sentences with more than one usage, the tool usedto automatically extract he test data ignored all but one ofthem.
Thus, some usages were missed.3The Penn Treebank corpus comes complete with POStags.141first set contained only the POS tags of the word imme-diately preceding and the word immediately succeedingthe ambiguous word, while the second set was extendedto include the POS tags of the two immediately preced-ing and two succeeding words.A limited number of collocation-specific variableswere selected, where the term collocation is used looselyto refer to a specific spelling form occurring in the samesentence as the ambiguous word.
All of our colloea-tional variables are dichotomous, indicating the pres-ence or absence of the associated spelling form.
Whilecollocation-specific variables are, by definition, specificto the word being disambiguated, the procedure usedto select them is general.
The search for collocation-specific variables was limited to the 400 most frequentspelling forms in a data sample composed of sentencescontaining interest.
Out of these 400, the five spellingforms found to be the most informative using the testdescribed above were selected as the collocational vari-ables.It is not enough to know that each of the featuresdescribed above is highly correlated with the meaningof the ambiguous word.
In order to use the features inconcert o perform disambiguation, a model describingthe interactions among them is needed.
Since we hadno reason to prefer, a priori, one form of model over an-other, all models describing possible interactions amongthe features were generated, and a model with good fitwas selected.
Models were generated and tested as de-scribed in Section 2.Resu l tsBoth the form and the performance of the model se-lected for each set of variables is presented in Table 2.Performance is measured in terms of the total percent-age of the test set tagged correctly by a classifier usingthe specified model.
This measure combines both pre-cision and recall.
Portions of the test set that are notcovered by the estimates of the parameters made fromthe training set are not tagged and, therefore, countedas wrong.The form of the model describes the interactionsamong the variables by expressing the joint distributionof the values of all contextual features and sense tags asa product of conditionally independent marginals, witheach marginal being composed of non-independent vari-ables.
Models of this form describe a markov field (\[8\],\[21\]) that can be represented graphically as is shownin Figure 1 for Model 4 of Table 2.
In both Figures 1and 2, each of the variables hort, in, pursue, rate(s),percent (i.e., the sign '%') is the presence or absence ofthat spelling form.
Each of the variables rlpos, r2pos,llpos, and 12pos is the POS tag of the word 1 or 2 po-sitions to the left (/) or right (r).
The variable endingis whether interest is in the singular or plural, and thevariable tag is the sense tag assigned to interest.The graphical representation f Model 4 is such thatthere is a one-to-one correspondence b tween the nodesof the graph and the sets of conditionally independentvariables in the model.
The semantics of the graphtopology is that all variables that are not directly con-nected in the graph are conditionally independent giventhe values of the variables mapping to the connectingnodes.
For example, if node a separates node b fromnode c in the graphical representation f a markov field,then the variables mapping to node b are conditionallyindependent of the variables mapping to node c giventhe values of the variables mapping to node a.
In thecase of Model 4, Figure 1 graphically depicts the factthat the value of the morphological variable ending isconditionally independent of the values of all other con-textual features given the sense tag of the ambiguousword.~ E  L1POS'.I IFigure 1L2POSThe Markov field depicted in Figure 1 is representedby an undirected graph because conditional indepen-dence is a symmetric relationship.
But decomposablemodels can also be characterized by directed graphs andinterpreted according to the semantics of a Bayesiannetwork (\[21\]; also described as "recursive causal mod-els" in \[27\] and \[16\]).
In a Bayesian network, the no-tions of causation and influence replace the notion ofconditional independence in a Markov field.
The par-ents of a variable (or set of variables) V are those vari-ables judged to be the direct causes or to have directinfluence on the value of V; V is called a "response"to those causes or influences.
The Bayesian networkrepresentation of a decomposable model embodies anexplicit ordering of the n variables in the model suchthat variable i may be considered a response to someor all of variables {i + 1 , .
.
.
,  n}, but is not thought ofas a response to any one of the variables {1 .
.
.
.
, i - 1}.In all models presented in this paper, the sense tag ofthe ambiguous word causes or influences the values ofall other variables in the model.
The Bayesian etworkrepresentation f Model 4 is presented in Figure 2.
InModel 4, the variables in and percent are treated as in-fluencing the values of rate, short, and pursue in orderto achieve an ordering of variables as described above.142\ [ ~ ~  LIPOS, L2POSFigure 2Compar i son  to  P rev ious  WorkMany researchers have avoided characterizing the inter-actions among multiple contextual features by consider-ing only one feature in determining the sense of an am-biguous word.
Techniques for identifying the optimumfeature to use in disambiguating a word are presentedin \[7\], \[30\] and \[5\].
Other works consider multiple con-textual features in performing disambiguation withoutformally characterizing the relationships among the fea-tures.
The majority of these efforts (\[13\], \[31\]) weighteach feature in predicting the sense of an ambiguousword in accordance with frequency information, with-out considering the extent to which the features co-occur with one another.
Gale, Church and Yarowsky(\[10\]) and Yarowsky (\[29\]) formally characterize the in-teractions that they consider in their model, but theysimply assume that their model fits the data.Other researchers have proposed approaches to sys-tematically combining information from multiple con-textual features in determining the sense of an ambigu-ous word.
Schutze (\[26\]) derived contextual featuresfrom a singular value decomposition f a matrix of letterfour-gram co-occurrence frequencies, thereby assuringthe independence of all features.
Unfortunately, inter-preting a contextual feature that is a weighted combina-tion of letter four-grams is difficult.
Further, the clus-tering procedure used to assign word meaning based onthese features is such that the resulting sense clustersdo not have known statistical properties.
This makes itimpossible to generalize the results to other data sets.Black (\[3\]) used decision trees (\[4\]) to define the re-lationships among a number of pre-specified contextualfeatures, which he called "contextual categories", andthe sense tags of an ambiguous word.
The tree construc-tion process used by Black partitions the data accordingto the values of one contextual feature before consider-ing the values of the next, thereby treating all featuresincorporated in the tree as interdependent.
The methodpresented here for using information from multiple con-textual features is more flexible and makes better useof a small data set by eliminating the need to treat allfeatures as interdependent.The work that bears the closest resemblance to thework presented here is the maximum entropy approachto developing language models (\[24\], \[25\], \[19\] and \[20\]).Although this approach as not been applied to word-sense disambiguation, there is a strong similarity be-tween that method of model formulation and our own.A maximum entropy model for multivariate data is thelikelihood function with the highest entropy that satis-fies a pre-defined set of linear constraints on the under-lying probability estimates.
The constraints describeinteractions among variables by specifying the expectedfrequency with which the values of the constrained vari-ables co-occur.
When the expected frequencies speci-fied in the constraints are linear combinations of theobserved frequencies in the training data, the resultingmaximum entropy model is equivalent to a maximumlikelihood model, which is the type of model used here.To date, in the area of natural anguage processing,the principles underlying the formulation of maximumentropy models have been used only to estimate theparameters ofa model.
Although the method escribedin this paper for finding a good approximation to thejoint distribution of a set of discrete variables makesuse of maximum likelihood models, the scope of thetechnique we are describing extends beyond parameterestimation to include selecting the form of the modelthat approximates the joint distribution.Several of the studies mentioned in this Section haveused interest as a test case, and all of them (with the ex-ception of Schutze \[26\]) considered four possible mean-ings for that word.
In order to facilitate comparisonof our work with previous tudies, we re-estimated theparameters of our best model and tested it using datacontaining only the four LDOCE senses correspondingto those used by others (usages not tagged as being oneof these four senses were removed from both the testand training data sets).
The results of the modified ex-periment along with a summary of the published resultsof previous tudies are presented in Table 3.While it is true that all of the studies reported inTable 3 used four senses of interest, it is not clear thatany of the other experimental parameters were held con-stant in all studies.
Therefore, this comparison is onlysuggestive.
In order to facilitate more meaningful com-parisons in the future, we are donating the data used inthis experiment to the Consortium for Lexical Research(ftp site: clr.nmsu.edu) where it will be available to allinterested parties.Conc lus ions  and  Future  Work?
In this paper, we presented a method for formulatingprobabilistic models that use multiple contextual fea-tures for word-sense disambiguation without requiringuntested assumptions regarding the form of the model.In this approach, the joint distribution of all variablesis described by only the most systematic variable in-teractions, thereby limiting the number of parametersto be estimated, supporting computational efficiency,and providing an understanding of the data.
Further,different types of variables, such as class-based andcollocation-specific ones, can be used in combination143with one another.
We also presented the results of astudy testing this approach.
The results suggest hatthe models produced in this study perform as well asor better than previous efforts on a difficult test case.We are investigating several extensions to this work.In order to reasonably consider doing large-scale word-sense disambiguation, it is necessary to eliminate theneed for large amounts of manually sense-tagged data.In the future, we hope to develop a parametric modelor models applicable to a wide range of content wordsand to estimate the parameters of those models fromuntagged ata.
To those ends, we are currently investi-gating a means of obtaining maximum likelihood esti-mates of the parameters of decomposable models fromuntagged ata.
The procedure we are using is a vari-ant of the EM algorithm that is specific to models ofthe form produced in this study.
Preliminary resultsare mixed, with performance being reasonably good onmodels with low-order marginals (e.g., 63% of the testset was tagged correctly with Model 1 using parame-ters estimated in this manner) but poorer on modelswith higher-order marginals, such as Model 4.
Work isneeded to identify and constrain the parameters thatcannot be estimated from the available data and to de-termine the amount of data needed for this procedure.We also hope to integrate probabilistic disambigua-tion models, of the type described in this paper, with aconstraint-based knowledge base such as WordNet.
Inthe past, there have been two types of approaches toword sense disambiguation: 1) a probabilistic approachsuch as that described here which bases the choice ofsense tag on the observed joint distribution of the tagsand contextual features, and 2) a symbolic knowledgebased approach that postulates some kind of relationalor constraint structure among the words to be tagged.We hope to combine these methodologies and therebyderive the benefits of both.
Our approach to combiningthese two paradigms hinges on the network representa-tions of our probabilistic models as described in Section4 and will make use of the methods presented in \[21\].AcknowledgementsThe authors would like to thank Gerald Rogers for shar-ing his expertise in statistics, Ted Dunning for adviceand support on software development, and the membersof the NLP group in the CRL for helpful discussions.Re ferences\[1\] Baglivo, J., Olivier, D., and Pagano, M. (1992).Methods for Exact Goodness-of-Fit Tests.
Jour-nal of the American Statistical Association, Vol.87, No.
418, June 1992.\[2\] Bishop, Y. M.; Fienberg, S.; and Holland, P(1975).
Discrete Multivariate Analysis: Theoryand Practice.
Cambridge: The MIT Press.\[3\] Black, Ezra (1988).
An Experiment in Compu-tational Discrimination of English Word Senses.IBM Journal of Research and Development, Vol.32, No.
2, pp.
185-194.\[4\] Breiman, L., Friedman, J., Olshen, R., and Stone,C.
(1984).
Classification and Regression Trees.Monterey, CA: Wadsworth & Brooks/Cole Ad-vanced Books & Software.\[5\] Brown, P., Della Pietra, S., Della Pietra, V.,and Mercer, R. (1991).
Word Sense Disambigua-tion Using Statistical Methods.
Proceedings of the29th Annual Meeting of the Association for Com-putational Linguistics (A CL-91), pp.
264-304.\[6\] Cowie, J., Guthrie, J., and Guthrie, L. (1992).Lexical Disambiguation and Simulating Anneal-ing.
Proceedings of the 15th International Con-ference on Computational Linguistics (COLING-92).
pp 359-365.\[7\] Dagan, I., Itai, A., and Schwall, U.
(1991).
TwoLanguages Are More Informative Than One.
Pro-ceedings of the 29th Annual Meeting of the Asso-ciation for Computational Linguistics (A CL-9I),pp.
130-137.\[8\] Darroch, J., Lauritzen, S., and Speed, T. (1980).Markov Fields and Log-Linear Interaction Modelsfor Contingency Tables.
The Annals of Statistics,Vol.
8, No.
3, pp.
522-539.\[9\] Dunning, Ted (1993).
Accurate Methods for theStatistics of Surprise and Coincidence.
Computa-tional Linguistics, Vol.
19, No.
1, pp.61-74.\[10\] Gale, W., Church, K., and Yarowsky, D. (1992a).A Method for Disambiguating Word Senses in aLarge Corpus.
A T84T Bell Laboratories StatisticalResearch Report No.
104.\[11\] Gale, W., Church, K. and Yarowsky, D. (1992b).Estimating Upper and Lower Bounds on thePerformance of Word-Sense Disambiguation Pro-grams.
Proceedings of the 30th Annual Meeting ofthe A CL, 1992.\[12\] Havranek, Womas (1984).
A Procedure for ModelSearch in Multidimensional Contingency Tables.Biometrics, 40, pp.95-100.\[13\] Hearst, Marti (1991).
Toward Noun HomonymDisambiguation--Using Local Context in LargeText Corpora.
Proceedings of the Seventh AnnualConference of the UW Centre for the New OEDand Text Research Using Corpora, pp.
1-22.\[14\] Jorgensen, Julia (1990).
The Psychological Real-ity of Word Senses.
Journal of PsycholinguisticResearch, Vol 19, pp.
167-190.\[15\] Kelly, E and P. Stone (1979).
Computer Recog-nition of English Word Senses, Vol.
3 of NorthHolland Linguistics Series, Amsterdam: North-Holland.\[16\] Kiiveri, H., Speed, T., and Carlin, J.
(1984).
Re-cursive Causal Models.
Journal Austral.
Math.Soc.
(Series A), 36, pp.
30-52.144\[17\] Kilgarriff, Adam (1993).
Dictionary Word SenseDistinctions: An Enquiry Into Their Nature.Computers and the Humanities, 26, pp.365-387.\[18\] Koehler, K. (1986).
Goodness-of-Fit Tests forLog-Linear Models in Sparse Contingency Tables.Journal of the American Statistical Association,Vol.
81, No.
394, June 1986.\[19\] Lau, R., Rosenfeld, R., and Roukos, S. (1993a).Trigger-Based Language Models: a MaximumEntropy Approach.
Proceedings of ICASSP-93.April 1993.\[20\] Lau, R., Rosenfeld, R., and Roukos, S. (1993b).Adaptive Language Modeling Using the Max-imum Entropy Principle.
Proc.
ARPA HumanLanguage Technology Workshop.
March 1993.\[21\] Pearl, Judea (1988).
Probabilistic Reasoning InIntelligent Systems: Networks of Plausible Infer-ence.
San Marco, Ca.
: Morgan Kaufmann.\[22\] Pederson, S. and Johnson, M. (1990).
EstimatingModel Discrepancy.
Technometrics, Vol.
32, No.3, pp.
305-314.\[23\] Procter, Paul et al (1978).
Longman Dictionaryof Contemporary English.\[24\] Ratnaparkhi, h. and Roukos, S. (1994).
h Maxi-mum Entropy Model for Prepositional Phrase At-tachment.
Proc.
ARPA Human Language Tech-nology Workshop.
March 1994.\[25\] Rosenfeld, R. (1994).
h Hybrid Approach toAdaptive Statistical Language Modeling.
Proc.ARPA Human Language Technology Workshop.March 1994.\[26\] Schutze, Hinrich (1992).
Word Space.
In S.J.Hanson, J.D.
Cowan, and C.L.
Giles (Eds.
), Ad-vances in Neural Information Processing Systems5, San Mateo, Ca.
: Morgan Kaufmann.\[27\] Wermuth, N. and Lauritzen, S. (1983).
Graphi-cal and recursive models for contingency tables.Biometrika, Vol.
70, No.
3, pp.
537-52.\[28\] Wilks, Y., Fass, D., Guo, C., McDonald, J.,Plate, T., and Slator, B.
(1990).
Providing Ma-chine Tractable Dictionary Tools.
Computers andTranslation 2.
Also to appear in Theoreticaland Computational Issues in Lexicai Semantics(TCILS).
Edited by James Pustejovsky.
Cam-bridge, MA.
: MIT Press.\[29\] Yarowsky, David (1992).
Word-Sense Disam-biguating Using Statistical Models of Roget'sCategories Trained on Large Corpora.
Proceed-ings of the 15th International Conference onComputational Linguistics (COLING-92).\[30\] Yarowsky, David (1993).
One Sense Per Colloca-tion.
Proceedings of the Speech and Natural Lan-guage ARPA Workshop, March 1993, Princeton,NJ.\[31\] Zernik, Uri (1990).
Tagging Word Senses In Cor-pus: The Needle in the Haystack Revisited.
Tech-nical Report 90CRD198, GE Research and Devel-opment Center.145LDOCE Sense Representation Representation Representationin Total Sample in Training Sample in Test Samplesense 1: 361"readiness to give attention" (15%)sense 2: 11"quality of causing attention to be given" (<1%)sense 3: 66"activity, subject, etc., which one (3%)gives time and attention to"271 90(15% / (15%)9 2(<1%) (<1%)50 16(3%) (3%)sense 4:"advantage, advancement, or favor"sense 5:'% share (in a company, business, etc.
)"sense 6:"money paid for the use of money"178 130 48(8%) (7%).
(8%)500 378 122(21%) (21%) (20%)1253 931 322(53%) (53%) (54%)Table 1: Distribution of sense tags.Model PercentCorrect1 P(rlpos, ilpos, ending,tag) = 73%P(rlposltag ) ?
P(ilposltag )?
P(endingltag ) ?
P(tag)2 P(rlpos, r2pos, llpos, 12pos, ending, tag) = 76%P(rlpos, r2posltag ) ?
P(llpos, 12posltag)?
P(endingltag) ?P(tag)3 P(percent,pursue, short, in, rate,tag) = 61%"P(shortlpercent, in tag)x P(rate\[percent, i , tag)xP(pursuelPercent , in, tag)?
P(percent, inltag) ?
P( tag)4 P(percent, pursue, short, in, rate, rlpos, r2pos, llpos, 12pos, ending, tag) = 78%P( short\[percent, i , tag) ?
P(ratelpercent, in, tag) ?
P(pursuelpercent, in tag)?P(percent, inltag)?
P(rlpos, r2posltag ) ?
P(ilpos, 12posltag)?
P(endingltag) ?P(tag)Table 2: The form and performance on the test data of the model found for each set of variables.
Each of thevariables hort, in, pursue, rate(s), percent (i.e., the sign '%') is the presence or absence of that spelling form.
Eachof the variables rlpos, r2pos, ilpos, and 12pos is the POS tag of the word 1 or 2 positions to the left (/) or right (r).The variable ending is whether interest is in the singular or plural, and the variable fag is the sense tag assigned tointerest.Model PercentCorrectBlack (1988) 72%Zernik (1990) 70%Yarowsky (1992) 72%Bruce & Wiebemodel 4 using only four senses 79%Table 3: Comparison to previous results.146
