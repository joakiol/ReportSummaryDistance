Proceedings of NAACL-HLT 2015, pages 96?100,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsOnline Readability and Text Complexity Analysis with TextEvaluatorDiane Napolitano, Kathleen M. Sheehan, and Robert MundkowskyEducational Testing Service660 Rosedale Road, 12RPrinceton, NJ 08541, USA{dnapolitano,ksheehan,rmundkowsky}@ets.orgAbstractWe have developed the TextEvaluator systemfor providing text complexity and CommonCore-aligned readability information.
De-tailed text complexity information is providedby eight component scores, presented in sucha way as to aid in the user?s understanding ofthe overall readability metric, which is pro-vided as a holistic score on a scale of 100to 2000.
The user may select a targeted USgrade level and receive additional analysis rel-ative to it.
This and other capabilities are ac-cessible via a feature-rich front-end, located athttp://texteval-pilot.ets.org/TextEvaluator/.1 IntroductionWritten communication is only effective to the ex-tent that it can be understood by its intended audi-ence.
A metric of readability, along with detailed in-formation about aspects of the text which contributeits complexity, can be an indispensable aid to anycontent developer, teacher, or even reader.
ETS?sTextEvaluator1stands apart from similar systems(e.g.
: Coh-Metrix (Graesser et al, 2011), ReadingMaturity (Landauer, 2011), ATOS (Milone, 2008),Lexile (Stenner et al, 2006), and, perhaps most fa-mously, Flesch-Kincaid (Kincaid et al, 1975)) inthat it not only provides a single, holistic score ofoverall complexity, but also additional complexityinformation in the form of eight contributing compo-nents.
The other systems known to us only provideone of these types of analysis.
In addition to this,1TextEvaluator was previously known as SourceRater.TextEvaluator will also provide the user with infor-mation on how its overall score and each of its com-ponent scores correspond to ideal values relative toa user-specified targeted grade level.
All of this in-formation is aligned with the current set of US gradeschool (K?12) text complexity standards outlined bythe Common Core (CCSSI, 2010).TextEvaluator?s overall complexity scores arehighly correlated with human grade level classifica-tions, as shown in (Nelson et al, 2012).
This studycompared six systems as part of the Gates Founda-tion?s Race to the Top initiative.
Of these systems,the overall complexity scores computed by Text-Evaluator were shown to have the highest Spear-man rank correlation (0.756) between human gradelevel classifications on a set of 168 Common Coreexemplar texts.
TextEvaluator differs from thesesystems in that the computation of its overall com-plexity score relies on its set of eight componentscores, each of which is a linear combination of fourto ten fine-grained features.
Most of these featuresare derived from information provided by part-of-speech tags and syntactic parses, unlike many com-peting systems which tend to only incorporate twoor three basic features, such as average sentencelength and average word frequency.
Also unlikeother systems, TextEvaluator differentiates betweenthe two primary genres proposed by the CommonCore: Informational texts, and their more challeng-ing counter-parts, Literary texts.
Internally, Text-Evaluator makes use of either a model of Informa-tional or Literary text complexity, in order to pro-duce its final, overall score of complexity.In this paper, we provide an overview96of how one can obtain and interpret Text-Evaluator analyses received via the web.We provide a pilot version of our system athttp://texteval-pilot.ets.org/TextEvaluator/.Additional information on the overall complexityscore and the component scores, as well as validityinformation, can be found in Sheehan et al (2014)and Sheehan et al (2013).
Much of this informationis also provided on the website?s About TextEvalu-ator page.1.1 LimitationsAt this time, text submitted to TextEvaluator foranalysis must be plain ASCII or UTF-8 text, free ofimages and tables.
Many of TextEvaluator?s featuresmake use of paragraphs, so it is recommended that atleast one hard return is inserted between each para-graph.
Manual word-wrapping will be corrected,and bulleted or numbered lists will be converted intoone sentence per item.TextEvaluator was designed for short reading pas-sages, such as news articles or short stories, the sortof material one might expect to see during an examor classroom assignment.
We are currently investi-gating its use with longer (greater than 5-6 pages inlength) texts.
It is currently not suitable for poetry,plays, or texts that contain fewer than than 2-3 sen-tences.TextEvaluator was designed for use with materi-als that are publication-ready.
Student assignments,such as essays, and transcripts of free-responsemonologues or dialogues, are not appropriate forTextEvaluator.
TextEvaluator simply may not beable to analyze such transcripts or noisy text suchas casual, online exchanges, due to its reliance on asyntactic parser (?).
If the input contains at least onesentence that the parser cannot find a valid parse for,TextEvaluator cannot proceed with the analysis andwill reject the text.At this time, there is no programmatic API toTextEvaluator that is available to the public.
How-ever, batch-mode processing may be possible bycontacting ETS via the information provided on thewebsite.2 Submitting a Text for AnalysisUpon visiting the TextEvaluator website, the user isasked to provide up to two pieces of information: avalid e-mail address and a client code.
We first val-idate the provided e-mail address by sending an e-mail containing a link to the page described in Sec-tion 3.2Then, rather than have the user wait for theirresults to be computed, TextEvaluator will notify theuser via e-mail when their results are ready.An e-mail address is mandatory but a client codeis not; specifying a valid client code gives the useraccess to some additional analyses.
A client codecan be obtained by purchasing a license for commer-cial use from ETS.
However, this paper will focusprimarily on the version of the website that is freelyaccessible for research and academic use.Research and academic use is limited to texts thatare 1600 words or less in length, and it is the re-sponsibility of the user to truncate their texts.
Witha client code, the length of the text is not con-strained.
The user may either upload a plain textfile or copy and paste such text into the larger inputbox.
The user is then encouraged to provide a ti-tle for their submission, should they potentially haveseveral TextEvaluator analyses on their screen at onetime.TextEvaluator will provide an additional set ofanalyses relative to a specified targeted grade levelwhich ranges from US grades 2 to 12.
At this time,the user is required to select a targeted grade.
If aclient code was entered, the user will be able to se-lect additional targeted grades on the page contain-ing their results.3 The Results PageThe user will receive a link to their results via e-mailas soon as TextEvaluator has completed its analy-sis.
Without the use of a client code, this web pagewill look similar to the one presented in Figure 1.Above the ?Summary?
tab, one can see the optionaltitle they provided, or a title provided by TextEvalu-ator, along with two large boxes.
The leftmost onewill state whether or not the overall complexity ofthe submitted text is above, within, or below the ex-pected range of complexity for your targeted grade2This initial step is slated to be removed in a future versionof the website.97Figure 1: The results page one will see without the use of a client code.
In this example, a targeted grade level of 4was selected.level.
This information is also presented towards thebottom of the Summary tab, and will be explainedlater in this section.
The rightmost box displays thetext?s overall complexity score on a scale of 100 (ap-propriate for first-grade students) to 2000 (appropri-ate for high-proficiency college graduates).
As withthe above/within/below analysis, this information isalso presented towards the bottom of the Summarytab.The box in the lefthand column of the Summarytab provides information regarding the contents ofyour text as discovered by TextEvaluator.
If any ofthis information appears incorrect to the user, theyare encouraged to reformat their text and submit itagain.
We also provide the Flesch-Kincaid gradelevel (Kincaid et al, 1975) of the text, should theuser be interested in comparing their Common Core-aligned complexity score to one aligned to a previ-ous standard.TextEvaluator?s analysis of the submitted text canbe found in a table in the righthand column of thepage.
The scores of the eight components are pre-sented, each on a scale of 0 to 100, with informationregarding whether or not a higher value for that com-ponent leads to a more complex or less complex text.This information is communicated via the arrowsin the second column of this table.
Each compo-nent score is the scaled result of a Principal Compo-nents Analysis which combines at least four but nomore than ten distinct features per score.
Providedis a brief description of each component; however,for more information, the reader is again referred toSheehan et al (2014), Sheehan et al (2013), and thewebsite?s About TextEvaluator page.3.1 Sentence StructureCurrently, the only component in this category, Syn-tactic Complexity, encapsulates all information re-garding how complex the sentences are within thesubmitted text.
It relies on information from syntac-tic parse trees3(Manning et al 2014) and part-of-speech tags (Toutanova et al, 2003), as well as basicmeasures such as the number of extremely long sen-tences and the size of the longest paragraph.4As de-3As provided by Stanford?s shift-reduce parser, version3.5.1: http://nlp.stanford.edu/software/srparser.shtml4We make use of both a syntactic parser and a tagger in orderto differentiate between possessives and the contractive form of?is?.
??s?
forms tagged as POS by the tagger are re-attached to98scribed in section 1.1, should the parser fail to find avalid parse for any sentence in the text, TextEvalu-ator will be unable to calculate the features nec-essary to compute the text?s Syntactic Complexityscore, and thus, unable to compute its overall com-plexity score.3.2 Vocabulary DifficultyThis category?s components measure the amount of:?
Academic Vocabulary, words that are morecharacteristic of academic writing than that offiction or conversation;?
Rare words, as determined by consulting twodifferent word frequency indices and encap-sulated in the Word Unfamiliarity component;and?
Concreteness, which describes the abstractnessor difficulty one might have imagining thewords within the text.The two word frequency indices were createdfrom one containing more than 17 million word to-kens focused on primary school (K?12) reading ma-terials, and one containing more than 400 millionword tokens spanning primary and post-graduateschool.
Features in the Concreteness component arebased on values of the perceived concreteness andimageability of each content word in the text as pro-vided by the MRC Psycholinguistic Database (Colt-heart, 1981).3.3 Connections Across IdeasThe components within this category indicate theamount of difficulty the reader may have followingthe concepts presented throughout the text.
LexicalCohesion combines several features which are com-puted based on the number of overlapping lemmasbetween pairs of sentences in each paragraph.
In-teractive/Conversational Style is concerned with theuse of verbs, contractions, and casual, spoken-stylediscourse, common to Literary texts.
By compar-ison, the Level of Argumentation component pro-vides a measurement of more formal, argumenta-tive discourse, much more common in Informa-tional texts.
This component encapsulates wordsthe preceding noun and this modified tag structure is providedas input to the parser.and phrases that are commonly found in argumen-tative discourse, such as subordinating concessivephrases (?although?, ?however?, ?on the contrary?
),synthetic negations (?nor?, ?neither?
), ?negative?adverbs (?seldom?, ?hardly?, ?barely?
), and causalconjunctive phrases (?as a result?, ?for this reason?,?under the circumstances?
).3.4 OrganizationThis category also only has one component, the De-gree of Narrativity.
This component differs from In-teractive/Conversational Style in that it makes use ofthe number of words found within quotation marks,referential pronouns, and past-tense verbs, all ofwhich are primary features of any written narrative.3.5 The Overall Complexity ScoreThe determination of TextEvaluator?s overall com-plexity score is genre-dependent, relying on the ideathat some features of text complexity will func-tion differently for Informational and Literary texts.Thus, TextEvaluator will actually compute a differ-ent overall complexity score for each genre, eachtrained as a linear regression model of the compo-nent scores.
Should the text actually be a combina-tion of the two, a weighted average of the two scoresis presented as the overall complexity score.
Thedecision of which score to present to the user as thefinal, overall complexity score is determined by cal-culating the probability that the text is Informational.If that value is within a certain range, the text is saidto be Informational, Literary, or Mixed.
Regardlessof the text?s genre, the complexity score?s relativityto the targeted grade level is determined the sameway.The notion of a text being above, within, or belowthe expected range of complexity relative to the tar-geted grade level is described further by the presen-tation of these ranges in Table 1.
A text is ?above?the allowable range of complexity if, for the cho-sen targeted grade, its complexity score is greaterthan the Max value, ?below?
if it is less than theMin value, or ?within?
if it is equal to, or between,the Min and Max values.
The method used to estab-lish this alignment between TextEvaluator complex-ity scores and the Common Core text complexityscale is described in (Sheehan, 2015).
There, threeevaluations of the proposed ranges are presented:99Target GL Min Max2 100 5253 310 5904 405 6555 480 7206 550 7907 615 8608 685 9409 750 102510 820 112511 890 124512 970 1360Table 1: The TextEvaluator/Common Core alignment ta-ble, showing the expected ranges of complexity relativeto a targeted grade level.
Although complexity scorescan be as high as 2000, only the ones presented in theranges here have been externally validated by the Com-mon Core (Sheehan, 2015).one based on the 168 exemplar texts listed in Ap-pendix B of (CCSSI, 2010); one based on a set often texts intended for readers who are career-ready;and one based on a set of 59 texts selected from text-books assigned in typical college courses.
In eachcase, results confirmed that TextEvaluator?s classifi-cations of texts being above, within, or below eachrange of complexity are aligned with classificationsprovided by reading experts.At this stage, users who provided a client codeat the start of their analysis will be able to selectand see analysis for a different targeted grade level.They will also receive an additional piece of infor-mation in the form of color-coding on each compo-nent score, relative to the selected targeted grade.Each score will be highlighted in red, yellow, orgreen, should the value for that component be eithertoo high, a bit too high, or within or below the idealrange for a text in the same genre as the input textand at that targeted grade.4 ConclusionIn this paper we have presented TextEvaluator, atool capable of analyzing almost any written text,for which it provides in-depth information into thetext?s readability and complexity.
This informationis further summarized with a holistic score with botha high correlation to human judgement (Nelson etal., 2012) and external validity.
(Sheehan, 2015) Itis these characteristics that lead us to believe thatTextEvaluator is a useful tool for educators, content-developers, researchers, and readers alike.ReferencesColtheart, M. 1981.
The MRC psycholinguistic database.
TheQuarterly Journal of Experimental Psychology Section A,33(4): 497 ?
505.Common Core State Standards Initiative.
(2010, June).
Com-mon Core State Standards for English language arts and lit-eracy in history/social studies, science and technical sub-jects.
Washington, DC: CCSSO and National Governors As-sociation.Graesser, A.C., McNamara, D.S, and Kulikowich, J.M.
2011.Coh-Metrix: Providing multilevel analyses of text character-istics.
Educational Researcher, 40(5): 223 ?
234.Kincaid, J.P., Fishburne, R.P., Rogers, R.L., and Chissom, B.S.1975.
Derivation of new readability formulas (automatedreadability index, Fog count and Flesch reading ease for-mula) for Navy enlisted personnel.
(Research Branch ReportNo.
8-75), NavalAir Station, Memphis, TN.Landauer, T. 2011.
Pearson?s text complexity measure.
WhitePaper, Pearson.Manning, C.D., Surdeanu, M., Bauer, J, Finkel, J, Bethard, S.J.,and McClosky, D. 2014.
The Stanford CoreNLP NaturalLanguage Processing Toolkit.
In Proceedings of 52nd An-nual Meeting of the Association for Computational Linguis-tics: System Demonstrations, Baltimore, MD.Milone, M. 2008.
The development of ATOS: The Renaissancereadability formula.
Wisconsin Rapids, WI: RenaissanceLearning.Nelson, J., Perfetti, C., Liben, D. and Liben, M. 2012.
Mea-sures of text difficulty: Testing their predictive value forgrade levels and student performance.
Technical Report,Washington, DC: Council of Chief State School Officers.Sheehan, K.M.
2015.
Aligning TextEvaluator scores with theaccelerated text complexity guidelines specified in the Com-mon Core State Standards.
ETS Research Report.
Princeton,NJ: Educational Testing Service.Sheehan, K.M, Flor, M., and Napolitano, D. 2013.
A two-stageapproach for generating unbiased estimates of text complex-ity.
In Proceedings of the 2nd Workshop on Natural Lan-guage Processing for Improving Textual Accessibility, At-lanta, GA.Sheehan, K.M, Kostin, I., Napolitano, D., and Flor, M. 2014.The TextEvaluator Tool: Helping teachers and test develop-ers select texts for use in instruction and assessment.
TheElementary School Journal, 115(2): 184?209.Stenner, A.J., Burdick, H., Sanford, E., and Burdick, D. 2006.How accurate are Lexile text measures?
Journal of AppliedMeasurement, 7(3): 307 ?
322.Toutanova, K, Klein, D., and Manning, C. 2003.
Feature-RichPart-of-Speech Tagging with a Cyclic Dependency Network.In Proceedings of the North American Association for Com-putational Linguistics, Edmonton, AB, Canada.100
