PROGRESSIVE-SEARCH ALGORITHMS FOR LARGE-VOCABULARY SPEECHRECOGNIT IONHy MurveitJohn ButzbergerVassilios DigalakisMitch WeintraubSRI  Internat ionalABSTRACTWe describe a technique we call Progressive Searchwhich is useful for developing and implementing speechrecognition systems with high computational requirements.
Thescheme iteratively uses more and more complex recognitionschemes, where each iteration constrains the search space of thenext.
An algorithm, the Forward-Backward Word-LifeAlgorithm, is described.
It can generate a word lattice in aprogressive search that would be used as a language modelembedded in a succeeding recognition pass to reducecomputation requirements.
We show that speed-ups of more thanan order of magnitude are achievable with only minor costs inaccuracy.1.
INTRODUCTIONMany advanced speech recognition techniques cannot bedeveloped or used in practical speech recognition systemsbecause of their extreme computational requirements.
Simplerspeech recognition techniques can be used to recognize speech inreasonable time, but they compromise word recognitionaccuracy.
In this paper we aim to improve the speed/accuracytrade-off in speeeh recognition systems using progressive s archtechniques.We define progressive s arch techniques as those whichcan be used to efficiently implement other, computationallyburdensome t chniques.
They use results of a simple and fastspeech recognition technique to constrain the search space of afollowing more accurate but slower unning technique.
This maybe done iteratively---each progressive search pass uses aprevious pass' constraints o run more ettieiently, and providesmore constraints for subsequent passes.We will refer to the faster speech recognition techniquesas "earlier-pass techniques", and the slower more accuratetechniques as "advanced techniques."
Constraining the costlyadvanced techniques in this way can make them run significantlyfaster without significant loss in accuracy.The key notions in progressive s arch techniques are:1.
An early-pass peech recognition phase builds alattice, which contains all the likely recognition unitstrings (e.g.
word sequences) given the techniquesused in that recognition pass.2.
A subsequent pass uses this lattice as a grammar thatconstrains the search space of an advanced technique(e.g., only the word sequences contained in a wordlattice of pass p would be considered inpass p+l).Allowing a sufficient breadth of lattice entries shouldallow later passes to recover the correct word sequence, whileruling out very unlikely sequences, thus achieving high accuracyand high speed speech recognition.2.
PRIOR ARTThere are three important categories of techniques thataim to solve problems imilar to the ones the progressive s archtechniques target.2.1.
Fast-Match TechniquesFast-match techniques\[l\] are similar to progressivesearch in that a coarse match is used to constrain a moreadvanced computationally burdensome algorithm.
The fastmatch, however, simply uses the local speech signal to constrainthe costly advanced technique.
Since the advanced techniquesmay take advantage of non-local data, the accuracy of a fast-match is limited and will ultimately limit the overall technique'sperformance.
Techniques uch as progressive search can bnngmore global knowledge to bear when generating constraints, and,thus, more effectively speed up the cosily techniques whileretaining more of their accuracy.2.2.
N-Best Recognition TechniquesN-best techniques\[2\] are also similar to progressivesearch in that a coarse match is used to constrain a morecomputationaUy costly technique.
In this case.
the coarsemateher is a complete (simple) speech recognition system.
Theoutput of the N-best system is a list of the top N most likelysentence hypotheses, which can then be evaluated with theslower but more accurate techniques.Progressive search is a generalization of N-best--theearlier-pass technique produces a graph, instead of a list of N-best sentences.
This generalization is crucial because N-best isonly eomputationally effective for N in the order of tens orhundreds.
A progressive search word graph can effectivelyaccount for orders of magnitude more sentence hypotheses.
Bylimiting the advanced techniques to just searching the few top Nsentences, N-best is destined to limit the effectiveness of theadvanced techniques and, consequently, the overall system's87accuracy.
Furthermore, it does not make much sense to use N-best in an iterative fashion as it does with progressive s arches.2.3.
Word LatticesThis technique is the most similar to progressive s arch.In I~ath approaches, an initial-pass recognition system cangenerate a lattice of word hypotheses.
Subsequent passes cansearclh through the lattice to find the best recognition hypothesis.It should be noted that, although we refer to lattices as wordlattices, they could be used at other linguistic level, such as thephoneme, syllable, e.t.c.In the traditional word-lattice approach, the word latticeis viewed as a scored graph of possible segmentations of theinput speech.
The lattice contains information such as theacoustic match between the input speech and the lattice word, aswell as segmentation information.The progressive search lattice is not viewed as a scoredgraph of possible segmentations of the input speech.
Rather, thelattice is simply viewed as a word-transition grammar whichconstrains subsequent recognition passes.
Temporal and scoringinformation is intentionally left out of the progressive searchlattice.This is a critical difference.
In the traditional word-latticeapproach, many segmentations of the input speech which couldnot be generated (or scored well) by the earlier-pass algorithmswill be eliminated for consideration before the advancedalgorithms are used.
With progressive-search techniques, thesesegmentations are implicit in the grammar and can be recoveredby the advanced techniques in subsequent recognition passes.3.
Building Progressive Search LatticesThe basic step of a progressive search system is using aspeech recognition algorithm to make a lattice which will beused as a grammar for a more advanced speech recognitionalgorithm.
This section discusses how these lattices may begenerated.
We focus on generating word lattices, though thesesame algorithms are easily extended to other levels.3.1.
The Word-Life AlgorithmWe implemented the following algorithm to generate aword-lattice as a by-product of the beam search used inrecognizing a sentence with the DECCIPHER TM system\[4-7\].1.
For each frame, insert into the table Active(W, t) allwords W active for each time t. Similarly constructtables End(W, t) and Transitions(W~, W 2, t) for allwords ending at time t, and for all word-to-wordtransition at time t.2.
Create a table containing the word-lives used in thesentence, WordLives(W, T~tan, Tend).
A word-life forword W is defined as a maximum-length interval(frame Tstar t to Ten d) during which some phone inword W is active.
That is,W E Active (W,  t), Tstar t~ t ~ Ten d3.
Remove word-lives from the table if the word neverended between T, tan and Te~, that is, removeWordLives(W, Tsta, ~, Tend) if there is time t betweenTstar t and Te,ut where End(W, 0 is true.4.
Create a finite-state graph whose nodes correspondto word-lives, whose arcs correspond to word-lifetransitions stored in the Transitions table.
This finitestate graph, augmented by language modelprobabilities, can be used as a grammar for asubsequent recognition pass in the progressivesearch.This algorithm can be efficiently implemented, even forlarge vocabulary recognition systems.
That is, the extra workrequired to build the "word-life lattice" is minimal compared tothe work required to recognize the large vocabulary with a early-pass speech recognition algorithm.This algorithm develops a grammar which contains allwhole-word hypotheses the early-pass speech recognitionalgorithm considered.
If a word hypothesis was active and theword was processed by the recognition system until the wordfinished (was not pruned before transitioning to another word),then this word will be generated as a lattice node.
Therefore, thesize of the lattice is directly controlled by the recognitionseareh's beam width.This algorithm, unfortunately, does not scale downwell--it has the property that small attices may not contain thebest recognition hypotheses.
This is because one must use smallbeam widths to generate small lattices.
However, a small beamwidth will likely generate pruning errors.Because of this deficiency, we have developed theForward/Backward Word-Life Algorithm described below.3.2.
Extending the Word-Life Algorithm UsingForward And Backward Recognition PassesWe wish to generate word lattices that scale downgracefully.
That is, they should have the property that when alattice is reduced in size, the most likely hypotheses remain andthe less likely ones are removed.
As was discussed, this is not theease if lattices are sealed down by reducing the beam searchwidth.The forward-backward word-life algorithm achieves thisscaling property.
In this new scheme, described below, the size ofthe lattice is controlled by the LatticeThresh parameter.1.
A standard beam search recognition pass is doneusing the early-pass peech recognition algorithm.
(None of the lattice building steps from Section 3.1are taken in this forward pass).2.
During this forward pass, whenever a transitionleaving word W is within the beam-search, we recordthat probability in ForwardProbability(W, frame).3.
We store the probability of the best scoringhypothesis from the forward pass, Pbest, andcompute a pruning valuePprune = Pbest I LatticeThresh.884.
We then recognize the same sentence over againusing the same models, but the recognition algorithmis run backwards 1.5.
The lattice building algorithm described in Section3.1 is used in this backward pass with the followingexception.
During the backward pass, wheneverthere is a transition between words W/and Wj at timet, we compute the overall hypothesis probability Phypas the product of ForwardProbability(Wj,t-1), thelanguage model probability P(H~IWj), and theBackward pass probability that W i ended at time t(i.e.
the probability of starting word W i at time t andfinishing the sentence).
If Phyp < Pprune, then thebackward transition between Wi and Wj at time t isblocked.Step 5 above implements a backwards pass pruningalgorithm.
This both greatly reduces the time required by thebackwards pass, and adjusts the size of the resultant lattice.4.
Progressive Search LatticesWe have experimented with generating word latticeswhere the early-pass recognition technique is a simple version ofthe DECIPHER TM speech recognition system, a 4-feature,discrete density HMM trained to recognize a 5,000 vocabularytaken from DARPA's WSJ speech corpus.
The test set is adifficult 20-sentence subset of one of the development sets.We define the number of errors in a single path p in alattice, Errors(p), to be the number of insertions, deletions, andsubstitutions found when comparing the words inp to a referencestring.
We define the number of errors in a word lattice to be theminimum of Errors(p) for all paths p in the word lattice.The following tables how the effect adjusting the beamwidth and LatticeThresh as on the lattice error rate and on thelattice size (the number of nodes and arcs in the word lattice).The grammar used by the has approximately 10,000 nodes and1,000,000 arcs.
The the simple recognition system had a 1-bestword error-rate ranging from 27% (beam width le-52) to 30%(beam width le-30).Table 1: Effect Of Pruning On Lattice SizeBeam Width le-30Lattice # %word nodes ares Thresh errors errorle-5 60 278 43 10.57le-9 94 541 34 8.35le-14 105 1016 30 7.37le-18 196 1770 29 7.13le-32 323 5480 23 5.65le-45 372 : 8626 23 5.65irff 380 9283 23 5.65LatticeThreshle-5le-9le-14le-18le-23le-32LatticeThreshle-14le-18le-23LatticeThreshle-14le-18le-23LatticeThreshle-14le-18le-23LatticeThreshle-14le-18Beam Width le-34#nodes arcserrors64 299 28105 613 20141 1219 16260 2335 15354 3993 15537 9540 15Beam Width le-38#nodes arcs errors186 1338 14301 2674 13444 4903 12Beam Width le-42#nodes ares errors197 1407 13335 2926 11520 5582 10Beam Width le-46#nodes arcs errors201 1436 13351 3045!
10/562 5946 \] 10Beam Width le-52#nodes arcs errors216 1582 12381 3368 9%worderror6.884.913.933.693.693.69%worderror3.443.192.95%wderror3.192.702.46%worder ror3.192.462.46%worder ror2.952.21The two order of magnitude r duction i  lattice size hasa significant impact on HMM decoding time.
Table 2 shows theper-sentence omputation time required for the above test setwhen cemputed using a Spare2 computer, for both the originalgrammar, and word lattice grammars generated using aLatticeThresh of le-23.1.
Using backwards recognition the sentence is processedfrom last frame to first frame with all transitions reversed.89Table 2: Lattice Computation ReductionsForward pass LatticeBeam Width recognition recognitiontime (sees) time (sees)le-30 167 10le-34 281 16le-38 450 24le-46 906 57le-52 1749 655.
Applications of Progressive Search SchemesProgressive search schemes can be used in the same wayN-best schemes are currently used.
The two primary applicationswe've had at SKI are:5.1.
Reducing the time required to performspeech recognition experimentsAt SRI, we've been experimenting with large-vocabulary tied-mixture speech recognition systems.
Using astandard ecoding approach, and average decoding times forrecognizing speech with a 5,000-word bigram language modelwere 46 times real time.
Using lattices generated with beamwidths of le-38 and a LatticeThresh of le-18 we were able todecode in 5.6 times real time).
Further, there was no difference inrecognition accuracy between the original and the lattice-basedsystem.5.2.
Implementing recognition schemes thatcannot be implemented with a standardapproach.We have implemented a trigram language model on our5,000-word recognition system.
This would not be feasible usingstandard decoding techniques.
Typically, continuous-speechtrigram language models are implemented ither with fastmatchtechnology or, more recently, with N-best schemes.
However, ithas been observed at BBN that using an N-best scheme (N=100)to implement a trigram language model for a 20,000 wordcontinuous peech recognition system may have significantlyreduced the potential gain from the language model.
That is,about half of the time, correct hypotheses that would have hadbetter (trigram) recognition scores than the other top-100sentences were not included in the top 100 sentences generatedby a bigram-based recognition system\[8\].We have implemented trigram-based language modelsusing word-lattices, expanding the finite-state network asappropriate ounambiguously represent contexts for all trigrams.We observed that the number of lattice nodes increased by afactor of 2-3 and the number of lattice arcs increased by a factorof approximately 4 (using lattices generated with beam widths ofle-38 and a LatticeThresh of le-18).
The resulting decodingtimes increased approximately by 50% when using trigramlattices instead of bigram lattices.ACKNOWLEDGEMENTSWe gratefully acknowledge support for this work fromDARPA through Office of Naval Research Contract N00014-92-C-0154.
The Government has certain rights in this material.
Anyopinions, findings, and conclusions or recommendationsexpressed in this material are those of the authors and do notnecessarily reflect he views of the government funding agencies.REFERENCES1.
Bahl, L.R., de Souza, P.V., Gopalakrishnan, P.S., Nahamoo,D., and M. Picheny, "A Fast Match for Continuous SpeechRecognition Using Allophonic Models," 19921EEE ICASSP,pp.
1-17-21.2.
Schwartz, R., Austin, S., Kubala, E, Makhoul, J., Nguyen,L., Placeway, P., and G. Zavaliagkos, "New uses for the N-Best Sentence Hypotheses Within the BYBLOS SpeechRecognition System", 1992 IEEE ICASSP, pp.
I-1-4.3.
Chow, Y.L., and S. Roukos, "Speech Understanding Using aUnification Grammar", 1989 IEEE 1CASSP, pp.
727-7304.
H. Murveit, J. Butzberger, and M. Weintraub, "Performanceof SRI's DECIPHER Speech Recognition System onDARPA's CSR Task," 1992 DARPA Speech and NaturalLanguage Workshop Proceedings, pp 410-4145.
Murveit, H., J. Butzberger, and M. Weintraub, "ReducedChannel Dependence for Speech Recognition," 1992DARPA Speech and Natural Language WorkshopProceedings, pp.
280-284.6.
H. Murveit, J. Butzberger, and M. Weintraub, "SpeechRecognition in SRI's Resource Management and ATISSystems," 1991 DARPA Speech and Natural LanguageWorkshop, pp.
94-100.7.
Cohen, M., H. Murveit, J. Bernstein, P. Price, and M.Weintraub, "The DECIPHER TM Speech RecognitionSystem," 1990 IEEE ICASSP, pp.
77-80.8.
Schwartz, R., BBN Systems and Technologies, Cambridge/VIA, Personal Communication90
