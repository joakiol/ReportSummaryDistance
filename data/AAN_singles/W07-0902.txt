Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 9?16,Prague, 28 June 2007. c?2007 Association for Computational LinguisticsViterbi Based Alignment between Text Images and their Transcripts?Alejandro H. Toselli, Vero?nica Romero and Enrique VidalInstitut Tecnolo`gic d?Informa`ticaUniversitat Polite`cnica de Vale`nciaCam??
de Vera s/n46071 - Vale`ncia, Spain[ahector,vromero,evidal]@iti.upv.esAbstractAn alignment method based on the Viterbialgorithm is proposed to find mappings be-tween word images of a given handwrit-ten document and their respective (ASCII)words on its transcription.
The approachtakes advantage of the underlying segmen-tation made by Viterbi decoding in hand-written text recognition based on HiddenMarkov Models (HMMs).
Two HMMsmodelling schemes are evaluated: one using78-HMMs (one HMM per character class)and other using a unique HMM to model allthe characters and another to model blankspaces.
According to various metrics usedto measure the quality of the alignments, en-couraging results are obtained.1 IntroductionRecently, many on-line digital libraries have beenpublishing large quantities of digitized ancient hand-written documents, which allows the general pub-lic to access this kind of cultural heritage resources.This is a new, comfortable way of consulting andquerying this material.
The Biblioteca ValencianaDigital (BiValDi)1 is an example of one such digitallibrary, which provides an interesting collection ofhandwritten documents.
?This work has been supported by the EC (FEDER), theSpanish MEC under grant TIN2006-15694-C02-01, and by theConseller?
?a d?Empresa, Universitat i Cie`ncia - Generalitat Va-lenciana under contract GV06/252.1http://bv2.gva.esSeveral of these handwritten documents includeboth, the handwritten material and its proper tran-scription (in ASCII format).
This fact has moti-vated the development of methodologies to alignthese documents and their transcripts; i.e.
to gen-erate a mapping between each word image on a doc-ument page with its respective ASCII word on itstranscript.
This word by word alignment would al-low users to easily find the place of a word in themanuscript when reading the corresponding tran-script.
For example, one could display both thehandwritten page and the transcript and wheneverthe mouse is held over a word in the transcript, thecorresponding word in the handwritten image wouldbe outlined using a box.
In a similar way, wheneverthe mouse is held over a word in the handwritten im-age, the corresponding word in the transcript wouldbe highlighted (see figure 1).
This kind of alignmentcan help paleography experts to quickly locate im-age text while reading a transcript, with useful ap-plications to editing, indexing, etc.
In the oppositedirection, the alignment can also be useful for peopletrying to read the image text directly, when arrivingto complex or damaged parts of the document.Creating such alignments is challenging since thetranscript is an ASCII text file while the manuscriptpage is an image.
Some recent works address thisproblem by relying on a previous explicit image-processing based word pre-segmentation of the pageimage, before attempting the transcription align-ments.
For example, in (Kornfield et al, 2004),the set of previously segmented word images andtheir corresponding transcriptions are transformedinto two different times series, which are aligned9Figure 1: Screen-shot of the alignment prototype interface displaying an outlined word (using a box) in themanuscript (left) and the corresponding highlighted word in the transcript (right).using dynamic time warping (DTW).
In this samedirection, (Huang and Srihari, 2006), in addition tothe word pre-segmentation, attempt a (rough) recog-nition of the word images.
The resulting word stringis then aligned with the transcription using dynamicprogramming.The alignment method presented here (hencefor-ward called Viterbi alignment), relies on the Viterbidecoding approach to handwritten text recogni-tion (HTR) based on Hidden Markov Models(HMMs) (Bazzi et al, 1999; Toselli et al, 2004).These techniques are based on methods originallyintroduced for speech recognition (Jelinek, 1998).In such HTR systems, the alignment is actually abyproduct of the proper recognition process, i.e.
animplicit segmentation of each text image line is ob-tained where each segment successively correspondsto one recognized word.
In our case, word recogni-tion is not actually needed, as we do already havethe correct transcription.
Therefore, to obtain thesegmentations for the given word sequences, the so-called ?forced-recognition?
approach is employed(see section 2.2).
This idea has been previously ex-plored in (Zimmermann and Bunke, 2002).Alignments can be computed line by line in caseswhere the beginning and end positions of lines areknown or, in a more general case, for whole pages.We show line-by-line results on a set of 53 pagesfrom the ?Cristo-Salvador?
handwritten document(see section 5.2).
To evaluate the quality of the ob-tained alignments, two metrics were used which giveinformation at different alignment levels: one mea-sures the accuracy of alignment mark placementsand the other measures the amount of erroneous as-100.30.7 0.80.20.90.10.80.20.70.3Figure 2: Example of 5-states HMM modeling (feature vectors sequences) of instances of the character ?a?within the Spanish word ?cuarenta?
(forty).
The states are shared among all instances of characters of thesame class.
The zones modelled by each state show graphically subsequences of feature vectors (see detailsin the magnifying-glass view) compounded by stacking the normalized grey level and its both derivativesfeatures.signments produced between word images and tran-scriptions (see section 4).The remainder of this paper is organized as fol-lows.
First, the alignment framework is introducedand formalized in section 2.
Then, an implementedprototype is described in section 3.
The alignmentevaluation metrics are presented in section 4.
Theexperiments and results are commented in section 5.Finally, some conclusions are drawn in section 6.2 HMM-based HTR and Viterbi alignmentHMM-based handwritten text recognition is brieflyoutlined in this section, followed by a more detailedpresentation of the Viterbi alignment approach.2.1 HMM HTR BasicsThe traditional handwritten text recognition problemcan be formulated as the problem of finding a mostlikely word sequence w?
= ?w1, w2, .
.
.
, wn?, fora given handwritten sentence (or line) image rep-resented by a feature vector sequence x = xp1 =?x1, x2, .
.
.
, xp?, that is:w?
= arg maxwPr(w|x)= arg maxwPr(x|w) ?
Pr(w) (1)where Pr(x|w) is usually approximated byconcatenated character Hidden Markov Models(HMMs) (Jelinek, 1998; Bazzi et al, 1999),whereas Pr(w) is approximated typically by ann-gram word language model (Jelinek, 1998).Thus, each character class is modeled by a con-tinuous density left-to-right HMM, characterized bya set of states and a Gaussian mixture per state.
TheGaussian mixture serves as a probabilistic law tomodel the emission of feature vectors by each HMMstate.
Figure 2 shows an example of how a HMMmodels a feature vector sequence corresponding to11b0 b3 b4 b5 b6 bn=7x1w1 w3 w4 w5 w6 xpwn=7b1 b2w2Figure 3: Example of segmented text line image along with its resulting deslanted and size-normalizedimage.
Moreover, the alignment marks (b0 .
.
.
b8) which delimit each of the words (including word-spaces)over the text image feature vectors sequence x.character ?a?.
The process to obtain feature vectorsequences from text images as well as the training ofHMMs are explained in section 3.HMMs as well as n-grams models can be rep-resented by stochastic finite state networks (SFN),which are integrated into a single global SFN by re-placing each word character of the n-gram model bythe corresponding HMM.
The search involved in theequation (1) to decode the input feature vectors se-quence x into the more likely output word sequencew?, is performed over this global SFN.
This searchproblem is adequately solved by the Viterbi algo-rithm (Jelinek, 1998).2.2 Viterbi AlignmentAs a byproduct of the Viterbi solution to (1), thefeature vectors subsequences of x aligned with eachof the recognized words w1, w2, .
.
.
, wn can be ob-tained.
These implicit subsequences can be visual-ized into the equation (1) as follows:w?
= arg maxw?bPr(x,b|w) ?
Pr(w) (2)where b is an alignment; that is, an ordered se-quence of n+1 marks ?b0, b1, .
.
.
, bn?, used to de-marcate the subsequences belonging to each recog-nized word.
The marks b0 and bn always point outto the first and last components of x (see figure 3).Now, approximating the sum in (2) by the domi-nant term:w?
?
arg maxwmaxbPr(x,b|w) ?
Pr(w) (3)where b?
is the optimal alignment.
In our case,we are not really interested in proper text recogni-tion because the transcription is known beforehand.Let w?
be the given transcription.
Now, Pr(w) inequation 3 is zero for all w except w?, for whichPr(w?)
= 1.
Therefore,b?
= arg maxbPr(x,b|w?)
(4)which can be expanded to,b?
= arg maxbPr(x, b1|w?
)Pr(x, b2|b1, w?)
.
.
.. .
.
P r(x, bn|b1b2 .
.
.
bn?1, w?
)(5)Assuming independence of each bi mark fromb1b2 .
.
.
bi?1 and assuming that each subsequencexbibi?1 depends only of w?i, equation (5) can be rewrit-ten as,b?
= arg maxbPr(xb1b0 |w?1) .
.
.
P r(xbnbn?1 |w?n) (6)This simpler Viterbi search problem is known as?forced recognition?.3 Overview of the Alignment PrototypeThe implementation of the alignment prototype in-volved four different parts: document image prepro-cessing, line image feature extraction, HMMs train-ing and alignment map generation.12Document image preprocessing encompasses thefollowing steps: first, skew correction is carried outon each document page image; then backgroundremoval and noise reduction is performed by ap-plying a bi-dimensional median filter (Kavalliera-tou and Stamatatos, 2006) on the whole page im-age.
Next, a text line extraction process based onlocal minimums of the horizontal projection profileof page image, divides the page into separate lineimages (Marti and Bunke, 2001).
In addition con-nected components has been used to solve the situ-ations where local minimum values are greater thanzero, making impossible to obtain a clear text lineseparation.
Finally, slant correction and non-linearsize normalization are applied (Toselli et al, 2004;Romero et al, 2006) on each extracted line image.An example of extracted text line image is shownin the top panel of figure 3, along with the result-ing deslanted and size-normalized image.
Note hownon-linear normalization leads to reduced sizes ofascenders and descenders, as well as to a thiner un-derline of the word ?ciudadanos?.As our alignment prototype is based on Hid-den Markov Models (HMMs), each preprocessedline image is represented as a sequence of featurevectors.
To do this, the feature extraction mod-ule applies a grid to divide line image into N ?M squared cells.
In this work, N = 40 is cho-sen empirically (using the corpus described furtheron) and M must satisfy the condition M/N =original image aspect ratio.
From each cell, threefeatures are calculated: normalized gray level, hor-izontal gray level derivative and vertical gray levelderivative.
The way these three features are deter-mined is described in (Toselli et al, 2004).
Columnsof cells or frames are processed from left to rightand a feature vector is constructed for each frameby stacking the three features computed in its con-stituent cells.Hence, at the end of this process, a sequence ofM 120-dimensional feature vectors (40 normalizedgray-level components, 40 horizontal and 40 verticalderivatives components) is obtained.
An example offeature vectors sequence, representing an image ofthe Spanish word ?cuarenta?
(forty) is shown in fig-ure 2.As it was explained in section 2.1, characters aremodeled by continuous density left-to-right HMMswith 6 states and 64 Gaussian mixture componentsper state.
This topology (number of HMM states andGaussian densities per state) was determined by tun-ing empirically the system on the corpus describedin section 5.1.
Once a HMM ?topology?
has beenadopted, the model parameters can be easily trainedfrom images of continuously handwritten text (with-out any kind of segmentation) accompanied by thetranscription of these images into the correspond-ing sequence of characters.
This training process iscarried out using a well known instance of the EMalgorithm called forward-backward or Baum-Welchre-estimation (Jelinek, 1998).The last phase in the alignment process is the gen-eration of the mapping proper by means of Viterbi?forced recognition?, as discussed in section 2.2.4 Alignment Evaluation MetricsTwo kinds of measures have been adopted to evalu-ate the quality of alignments.
On the one hand, theaverage value and standard deviation (henceforwardcalled MEAN-STD) of the absolute differences be-tween the system-proposed word alignment marksand their corresponding (correct) references.
Thisgives us an idea of the geometrical accuracy of thealignments obtained.
On the other hand, the align-ment error rate (AER), which measures the amountof erroneous assignments produced between wordimages and transcriptions.Given a reference mark sequence r =?r0, r1, .
.
.
, rn?
along with an associated to-kens sequence w = ?w1, w2, .
.
.
, wn?, and asegmentation marks sequence b = ?b0, b1, .
.
.
, bn?
(with r0 =b0 ?
rn =bn), we define the MEAN-STDand AER metrics as follows:MEAN-STD: The average value and standard devi-ation of absolute differences between reference andproposed alignment marks, are given by:?
=?n?1i=1 din ?
1 ?
=?
?n?1i=1 (di ?
?
)2n ?
1 (7)where di = |ri ?
bi|.13w1 w3 w4 w5 w6 wn=7w2r0 r3 r4 r5 r6 r7x1 xpr1 r2b7b1 b2 b3 b4 b6b5b0m7m5m3m1Figure 4: Example of AER computation.
In this case N = 4 (only no word-space are considered:w1, w3, w5, w7) and w5 is erroneously aligned with the subsequence xb6b5 (m5 /?
(b4, b5)).
The resultingAER is 25%.AER: Defined as:AER(%) =100N?j:wj 6=bejej ={0 bj?1 <mj <bj1 otherwise(8)where b stands for the blank-space token, N < n isthe number of real words (i.e., tokens which are notb, and mj = (rj?1 + rj)/2.A good alignment will have a ?
value close to 0and small ?.
Thus, MEAN-STD gives us an idea ofhow accurate are the automatically computed align-ment marks.
On the other hand, AER assesses align-ments at a higher level; that is, it measures mis-matches between word-images and ASCII transcrip-tions (tokens), excluding word-space tokens.
This isillustrated in figure 4, where the AERwould be 25%.5 ExperimentsIn order to test the effectiveness of the presentedalignment approach, different experiments were car-ried out.
The corpus used, as well as the experimentscarried out and the obtained results, are reported inthe following subsections.5.1 Corpus descriptionThe corpus was compiled from the legacy handwrit-ing document identified as Cristo-Salvador, whichwas kindly provided by the Biblioteca ValencianaDigital (BIVALDI).
It is composed of 53 text pageimages, scanned at 300dpi and written by only onewriter.
Some of these page images are shown in thefigure 5.As has been explained in section 3, the page im-ages have been preprocessed and divided into lines,resulting in a data-set of 1,172 text line images.In this phase, around 4% of the automatically ex-tracted line-separation marks were manually cor-rected.
The transcriptions corresponding to each lineimage are also available, containing 10,911 runningwords with a vocabulary of 3,408 different words.To test the quality of the computed alignments, 12pages were randomly chosen from the whole corpuspages to be used as references.
For these pages thetrue locations of alignment marks were set manually.Table 1 summarized the basic statistics of this cor-pus and its reference pages.Number of: References Total Lexiconpages 12 53 ?text lines 312 1,172 ?words 2,955 10,911 3,408characters 16,893 62,159 78Table 1: Basic statistics of the database5.2 Experiments and ResultsAs mentioned above, experiments were carried outcomputing the alignments line-by-line.
Two differ-ent HMM modeling schemes were employed.
Thefirst one models each of the 78 character classes us-ing a different HMM per class.
The second schemeuses 2 HMMs, one to model all the 77 no-blankcharacter classes, and the other to model only theblank ?character?
class.
The HMM topology wasidentical for all HMMs in both schemes: left-to-right with 6 states and 64 Gaussian mixture com-14Figure 5: Examples page images of the corpus ?Cristo-Salvador?, which show backgrounds of big variationsand uneven illumination, spots due to the humidity, marks resulting from the ink that goes through the paper(called bleed-through), etc.ponents per state.As has been explained in section 4, two differentmeasures have been adopted to evaluate the qualityof the obtained alignments: the MEAN-STD and theAER.
Table 2 shows the different alignment evalu-ation results obtained for the different schemes ofHMM modeling.78-HMMs 2-HMMsAER (%) 7.20 25.98?
(mm) 1.15 2.95?
(mm) 3.90 6.56Table 2: Alignment evaluation results 78-HMMsand 2-HMMs.From the results we can see that using the 78HMMs scheme the best AER is obtained (7.20%).Moreover, the relative low values of ?
and ?
(in mil-limeters) show that the quality of the obtained align-ments (marks) is quite acceptable, that is they arevery close to their respective references.
This is il-lustrated on the left histogram of figure 6.The two typical alignment errors are known asover-segmentation and under-segmentation respec-tively.
The over-segmentation error is when oneword image is separated into two or more fragments.The under-segmentation error occurs when two ormore images are grouped together and returned asone word.
Figure 7 shows some of them.6 Remarks and ConclusionsGiven a manuscript and its transcription, we proposean alignment method to map every word image onthe manuscript with its respective ASCII word onthe transcript.
This method takes advantage of theimplicit alignment made by Viterbi decoding usedin text recognition with HMMs.The results reported in the last section should beconsidered preliminary.Current work is under way to apply this align-ment approach to the whole pages, which representsa more general case where the most corpora do nothave transcriptions set at line level.ReferencesI.
Bazzi, R. Schwartz, and J. Makhoul.
1999.
An Om-nifont Open-Vocabulary OCR System for English and150246810120 1 2 3 4 5 6Frequency(%)|Segi ?
Refi| (mm)mean01234560 1 2 3 4 5 6Frequency(%)|Segi ?
Refi| (mm)meanFigure 6: |ri ?
bi| distribution histograms for 78-HMMs (left) and 2-HMMs (right) modelling schemes.Figure 7: Word alignment for 6 lines of a particularly noisy part of the corpus.
The four last words on thesecond line as well as the last line illustrate some of over-segmentation and under-segmentation error types.Arabic.
IEEE Trans.
on PAMI, 21(6):495?504.Chen Huang and Sargur N. Srihari.
2006.
Mapping Tran-scripts to Handwritten Text.
In Suvisoft Ltd., editor,Tenth International Workshop on Frontiers in Hand-writing Recognition, pages 15?20, La Baule, France,October.F.
Jelinek.
1998.
Statistical Methods for Speech Recog-nition.
MIT Press.Ergina Kavallieratou and Efstathios Stamatatos.
2006.Improving the quality of degraded document images.In DIAL ?06: Proceedings of the Second InternationalConference on Document Image Analysis for Libraries(DIAL?06), pages 340?349, Washington, DC, USA.IEEE Computer Society.E.
M. Kornfield, R. Manmatha, and J. Allan.
2004.
TextAlignment with Handwritten Documents.
In First In-ternational Workshop on Document Image Analysisfor Libraries (DIAL), pages 195?209, Palo Alto, CA,USA, January.U.-V. Marti and H. Bunke.
2001.
Using a Statistical Lan-guage Model to improve the preformance of an HMM-Based Cursive Handwriting Recognition System.
Int.Journal of Pattern Recognition and Artificial In telli-gence, 15(1):65?90.V.
Romero, M. Pastor, A. H. Toselli, and E. Vidal.
2006.Criteria for handwritten off-line text size normaliza-tion.
In Procc.
of The Sixth IASTED internationalConference on Visualization, Imaging, and Image Pro-cessing (VIIP 06), Palma de Mallorca, Spain, August.A.
H. Toselli, A. Juan, D. Keysers, J. Gonzlez, I. Sal-vador, H. Ney, E. Vidal, and F. Casacuberta.
2004.Integrated Handwriting Recognition and Interpretationusing Finite-State Models.
Int.
Journal of PatternRecognition and Artificial Intelligence, 18(4):519?539, June.M.
Zimmermann and H. Bunke.
2002.
Automatic Seg-mentation of the IAM Off-Line Database for Hand-written English Text.
In ICPR ?02: Proceedings ofthe 16 th International Conference on Pattern Recog-nition (ICPR?02) Volume 4, page 40035, Washington,DC, USA.
IEEE Computer Society.16
