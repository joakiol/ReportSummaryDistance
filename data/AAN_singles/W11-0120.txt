Measuring the semantic relatedness between words and imagesChee Wee Leong and Rada MihalceaDepartment of Computer Science and EngineeringUniversity of North Texascheeweeleong@my.unt.edu, rada@cs.unt.eduAbstractMeasures of similarity have traditionally focused on computing the semantic relatedness betweenpairs of words and texts.
In this paper, we construct an evaluation framework to quantify cross-modalsemantic relationships that exist between arbitrary pairs of words and images.
We study the effec-tiveness of a corpus-based approach to automatically derive the semantic relatedness between wordsand images, and perform empirical evaluations by measuring its correlation with human annotators.1 IntroductionTraditionally, a large body of research in natural language processing has focused on formalizing wordmeanings.
Several resources developed to date (e.g., WordNet (Miller, 1995)) have enabled a systematicencoding of the semantics of words and exemplify their usage in different linguistic frameworks.
As aresult of this formalization, computing semantic relatedness between words has been possible and hasbeen used in applications such as information extraction and retrieval, query reformulation, word sensedisambiguation, plagiarism detection and textual entailment.In contrast, while research has shown that the human cognitive system is sensitive to visual informa-tion and incorporating a dual linguistic-and-pictorial representation of information can actually enhanceknowledge acquisition (Potter and Faulconer, 1975), the meaning of an image in isolation is not well-defined and it is mostly task-specific.
A given image, for instance, may be simultaneously labeled by aset of words using an automatic image annotation algorithm, or classified under a different set of seman-tic tags in the image classification task, or simply draw its meaning from a few representative regionsfollowing image segmentation performed in an object localization framework.Given that word meanings can be acquired and disambiguated using dictionaries, we can perhapsexpress the meaning of an image in terms of the words that can be suitably used to describe it.
Specif-ically, we are interested to bridge the semantic gap (Smeulders et al, 2000) between words and imagesby exploring ways to harvest the information extracted from visual data in a general framework.
While alarge body of work has focused on measuring the semantic similarity of words (e.g., (Miller and Charles,1998)), or the similarity between images based on image content (e.g., (Goldberger et al, 2003)), veryfew researchers have considered the measure of semantic relatedness1 between words and images.But, how exactly is an image related to a given word?
In reality, quantification of such a cross-modal semantic relation is impossible without supplying it with a proper definition.
Our work seeks toaddress this challenge by constructing a standard evaluation framework to derive a semantic relatednessmetric for arbitrary pairs of words and images.
In our work, we explore methods to build a representa-tion model consisting of a joint semantic space of images and words by combining techniques widelyadopted in computer vision and natural language processing, and we evaluate the hypothesis that we canautomatically derive a semantic relatedness score using this joint semantic space.Importantly, we acknowledge that it is significantly harder to decode the semantics of an image, as itsinterpretation relies on a subjective and perceptual understanding of its visual components (Biederman,1In our paper, we are concerned with semantic relatedness, which is a more general concept than semantic similarity.Similarity is concerned with entities related by virtues of their likeness, e.g., bank-trust company, but dissimilar entities mayalso be related, e.g., hot-cold.
A full treatment of the topic can be found in Budanitsky and Hirst (2005).1851987).
Despite this challenge, we believe this is a worthy research direction, as many important problemscan benefit from the association of image content in relation to word meanings, such as automatic imageannotation, image retrieval and classification (e.g., (Leong et al, 2010)) as well as tasks in the domainsof of text-to-image synthesis, image harvesting and augmentative and alternative communication.2 Related WorkDespite the large amount of work in computing semantic relatedness between words or similarity be-tween images, there are only a few studies in the literature that associate the meaning of words andpictures in a joint semantic space.
The work most similar to ours was done by Westerveld (2000), whoemployed LSA to combine textual words with simple visual features extracted from news images usingcolors and textures.
Although it was concluded that such a joint textual-visual representation model waspromising for image retrieval, no intensive evaluation was performed on datasets on a large scale, ordatasets other than the news domain.
Similarly, Hare et al (2008) compared different methods such asLSA and probabilistic LSA to construct joint semantic spaces in order to study their effects on automaticimage annotation and semantic image retrieval, but their evaluation was restricted exclusively to theCorel dataset, which is somewhat idealistic and not reflective of the challenges presented by real-world,noisy images.Another related line of work by Barnard and Forsyth (2001) used a generative hierarchical modelto learn the associative semantics of words and images for improving information retrieval tasks.
Theirapproach was supervised and evaluated again only on the Corel dataset.More recently, Feng and Lapata (2010) showed that it is possible to combine visual representationsof word meanings into a joint bimodal representation constructed by using latent topics.
While theirwork focused on unifying meanings from visual and textual data via supervised techniques, no effortwas made to compare the semantic relatedness between arbitrary pairs of word and image.3 Bag of Visual CodewordsInspired by the bag-of-words approach employed in information retrieval, the ?bag of visual codewords?is a similar technique used mainly for scene classification (Yang et al, 2007).
Starting with an imagecollection, visual features are first extracted as data points from each image, characterizing its appear-ance.
By projecting data points from all the images into a common space and grouping them into a largenumber of clusters such that similar data points are assigned to the same cluster, we can treat each clusteras a ?visual codeword?
and express every image in the collection as a ?bag of visual codewords?.
Thisrepresentation enables the application of methods used in text retrieval to tasks in image processing andcomputer vision.Typically, the type of visual features selected can be global ?
suitable for representation in all images,or local ?
specific to a given image type and task requirement.
Global features are often described using acontinuous feature space, such as color histogram in three different color spaces (RGB, HSV and LAB),or textures using Gabor and Haar wavelets (Makadia et al, 2008).
In comparison, local features such askey points (Fei-Fei and Perona, 2005) are often distinct across different objects or scenes.
Regardless ofthe features used, visual codeword generation involves the following three important phases.1.
Feature Detection: The image is divided into partitions of varying degrees of granularity fromwhich features can be extracted and represented.
Typically, we can employ normalized cuts todivide an image into irregular regions, or apply uniform segmentation to break it into smallerbut fixed grids, or simply locate information-rich local patches on the image using interest pointdetectors.2.
Feature Description: A descriptor is selected to represent the features that are being extractedfrom the image.
Typically, feature descriptors (global or local) are represented as numerical vec-tors, with each vector describing the feature extracted in each region.
This way, an image isrepresented by a set of vectors from its constituent regions.186Figure 1: An illustration of the process of generating ?Bag of Visual Codewords?3.
Visual Codeword Generation: Clustering methods are applied to group vectors into clusters,where the center of each cluster is defined as a visual codeword, and the entire collection of clustersdefines the visual vocabulary for that image collection.
Each image region or patch abstracted infeature detection is now represented by the visual codeword mapped from its corresponding featurevector.The process of visual codeword generation is illustrated in Figure 1.
Fei-Fei and Perona (2005) hasshown that, unlike most previous work on object or scene classification that focused on adopting globalfeatures, local features are in fact extremely powerful cues.
In our work, we use the Scale-InvariantFeature Transform (SIFT) introduced by Lowe (2004) to describe distinctive local features of an imagein the feature description phase.
SIFT descriptors are selected for their invariance to image scale, rotation,differences in 3D viewpoints, addition of noise, and change in illumination.
They are also robust acrossaffine distortions.4 Semantic Vector ModelsThe underlying idea behind semantic vector models is that concepts can be represented as points in amathematical space, and this representation is learned from a collection of documents such that conceptsrelated in their meanings are near to one another in that space.
In the past, semantic vector modelshave been widely adopted by natural language processing researchers for tasks ranging from informationretrieval and lexical acquisition, to word sense disambiguation and document segmentation.
Severalvariants have been proposed, including the original vector space model (Salton et al, 1997) and theLatent Semantic Analysis (Landauer and Dumais, 1997).
Generally, vector models are attractive becausethey can be constructed using unsupervised methods of distributional corpus analysis and assume littlelanguage-specific requirements as long as texts can be reliably tokenized.
Furthermore, various studies(Kanerva, 1998) have shown that by using collaborative, distributive memory units to represent semanticvectors, a closer correspondence to human cognition can be achieved.While vector-space models typically require nontrivial algebraic machinery, reducing dimensions isoften key to uncover the hidden (latent) features of the terms distribution in the corpus, and to circumventthe sparseness issue.
There are a number of methods that have been developed to reduce dimensions ?see e.g., Widdows and Ferraro (2008) for an overview.
Here, we briefly describe one commonly used187technique, namely the Latent Semantic Analysis (LSA), noted for its effectiveness in previous works forreducing dimensions.In LSA, term co-occurrences in a corpus are captured by means of a dimensionality reduction op-erated by a Singular Value Decomposition (SVD) on the term-by-document matrix T representing thecorpus.
SVD is a well-known operation in linear algebra, which can be applied to any rectangular matrixin order to find correlations among its rows and columns.
SVD decomposes the term-by-document ma-trix T into three matrices T = U?kVT where ?k is the diagonal k ?
k matrix containing the singular kvalues of T, ?1 ?
?2 ?
... ?
?k and U and V are column-orthogonal matrices.
When the three matricesare multiplied together the original term-by-document matrix is re-composed.
Typically we can choosek?
k obtaining the approximation T ' U?k?VT .5 Semantic Relatedness between Words and ImagesAlthough the bag of visual codewords has been extensively used in image classification and retrievaltasks, and vector-space models are well explored in natural language processing, there has been littleconnection between the two streams of research.
Specifically, to our knowledge, there is no research workthat combines the two techniques to model multimodal meaning relatedness.
Since we are exploring newgrounds, it is important to clarify what we mean by computing the semantic relatedness between a wordand an image, and how the nature of this task impacts our hypothesis.
The assumptions below arenecessary to validate our findings:1.
Computing semantic relatedness between a word and an image involves comparing the conceptsinvoked by the word and the salient objects in the image as well as their interaction.
This goesbeyond simply identifying the presence or absence of specific objects indicated by a given word.For instance, we expect a degree of relatedness between an image showing a soccer ball and theword ?jersey,?
since both invoke concepts like {sports, soccer, teamwork} and so on.2.
The semantics of an image is dependent on the focus, size and position of distinct objects identi-fied through image segmentation.
During labeling, we expect this segmentation to be performedimplicitly by the annotators.
Although it is possible to focus one?s attention on specific objects viabounding boxes, we are interested to harvest the meaning of an image using a holistic approach.3.
In the case of measuring the relatedness of a word that has multiple senses with a given image,humans are naturally inclined to choose the sense that provides the highest relatedness inside thepair.
For example, an image of a river bank expectedly calls upon the ?river bank?
sense of theword ?bank?
(and not ?financial bank?
or other alternative word senses).4.
A degree of semantic relatedness can exist between any arbitrary word and image, on a scaleranging from being totally unrelated to perfectly synonymous with each other.
This is triviallytrue, as the same property holds when measuring similarity between words and texts.Next, we evaluate our hypothesis that we can measure the relatedness between a word and an imageempirically, using a parallel corpus of words and images as our dataset.5.1 ImageNetWe use the ImageNet database (Deng et al, 2009), which is a large-scale ontology of images devel-oped for advancing content-based image search algorithms, and serving as a benchmarking standard forvarious image processing and computer vision tasks.
ImageNet exploits the hierarchical structure ofWordNet by attaching relevant images to each synonym set (known as ?synset?
), hence providing picto-rial illustrations of the concept associated with the synset.
On average, each synset contains 500-1000images that are carefully audited through a stringent quality control mechanism.Compared to other image databases with keyword annotations, we believe that ImageNet is suitablefor evaluating our hypothesis for three reasons.
First, by leveraging on reliable keyword annotations inWordNet (i.e., words in the synset and their gloss naturally serve as annotations for the correspondingimages), we can effectively circumvent the propagation of errors caused by unreliable annotations, andconsequently hope to reach more conclusive results for this study.
Second, unlike other image databases,188ImageNet consists of millions of images, and it is a growing resource with more images added on aregular basis.
This aligns with our long-term goal of building a large-scale joint semantic space of imagesand words.
Finally, third, although we can search for relevant images using keywords in ImageNet,2there is currently no method to query it in the reverse direction.
Given a test image, we must searchthrough millions of images in the database to find the most similar image and its corresponding synset.A joint semantic model can hopefully augment this shortcoming by allowing queries to be made in bothdirections.
Figure 2 shows an example of a synset and the corresponding images in ImageNet.
(a)(b)Joint Semantic Space of Words and ImagesSynsets 167Images 230,864Words 1144Nouns 783Verbs 140Adjectives 221Image:Words ratio 202:1Figure 2: (a) A subset of images associated with a node in ImageNet.
The WordNet synset illustratedhere is {Dog, domestic dog, Canis familiaris} with the gloss: A member of the genus Canis (probablydescended from the common wolf) that has been domesticated by man since prehistoric times; occurs inmany breeds; ?the dog barked all night?
(b) A table showing statistical information on our joint semanticspace model5.2 DatasetFor our experiments, we randomly select 167 synsets3 from ImageNet, covering a wide range of conceptssuch as plants, mammals, fish, tools, vehicles etc.
We perform a simple pre-processing step using TreeTagger (Schmid, 1994) and extract only the nouns.
Multiwords are explicitly recognized as collocationsor named entities in the synset.
Not considering part-of-speech distinctions, the vocabulary for synsetwords is 352.
The vocabulary for gloss words is 777.
The shared vocabulary between them is 251.There are a total of 230,864 images associated with the 167 synsets, with an average of 1383 imagesper synset.
We randomly select an image for each synset, thus obtaining a set of 167 test images intotal.
The technique explained in Section 3 is used to generate visual codewords for each image in thisdataset.4 Each image is first pre-processed to have a maximum side length of 300 pixels.
Next, SIFTdescriptors are obtained by densely sampling the image on 20x20 overlapping patches spaced 10 pixelsapart.
K-means clustering is applied on a random subset of 10 million SIFT descriptors to derive a visualvocabulary of 1,000 codewords.
Each descriptor is then quantized into a visual codeword by assigning itto the nearest cluster.To create the gold-standard relatedness annotation, for each test image, six nouns are randomly se-lected from its associated synset and gloss words, and six other nouns are again randomly selected fromthe shared vocabulary words.5 In all, we have 167 x 12 = 2004 word-image pairs as our test dataset.
Sim-ilar to previous word similarity evaluations (Miller and Charles, 1998), we ask human annotators to rateeach pair on a scale of 0 to 10 to indicate their degree of semantic relatedness using the evaluation frame-work outlined below, with 0 being totally unrelated and 10 being perfectly synonymous with each other.To ensure quality ratings, for each word-image pair we used 15 annotators from Amazon Mechanical2http://www.image-net.org/3Not all synsets in ImageNet are annotated with images.
We obtain our dataset from the Spring 2010 version of ImageNetbuilt around Wordnet 3.0.4For our experiments, we obtained the visual codewords computed a priori from ImageNet.
Test images are not used toconstruct the model512 data points are generally considered sufficient for reliable correlation measures (Vania Kovic, p.c.
).189Synset {sunflower, helianthus} Synset {oxygen-mask} Synset {submarine , pigboat ,sub , U-boat}Gloss any plant of the genusHelianthus having large flowerheads with dark disk florets andshowy yellow raysGloss a breathing device thatis placed over the mouth andnose; supplies oxygen from anattached storage tankGloss a submersible warshipusually armed with torpedoesRelatedness Scores Relatedness Scores Relatedness Scorescolor (5.13) dog (0.53) basketball (0.20) central (1.53) africa (0.80) brass (1.73)floret (6.53) flower (9.67) device (5.47) family (0.80) door (1.67) good (2.40)freshwater (2.40) hair (1.00) iron-tree (0.47) mouth (5.13) pacific (2.40) pigboat (6.47)garden (6.60) head (3.80) oxygen-mask (7.73) tank (4.47) sub (8.20) submarine (9.67)plant (8.47) ray (3.67) storage (3.07) supply (5.20) tail (0.93) torpedo (7.60)sunflower (9.80) reed (2.27) nose (6.20) time (1.13) u-boat (7.47) warship (8.73)Table 1: A sample of test images with their synset words and glosses : The number in parenthesis rep-resents the numerical association of the word with the image (0-10).
Human annotations reveal differentdegree of semantic relatedness between the image and words in the synset or gloss.Turk.6 Finally, the average of all 15 annotations for each word-image pair is taken as its gold-standardrelatedness score7.
Note that only the pairs of images and words are provided to the annotators, and nottheir synsets and gloss definitions.The set of standard criteria underlying the cross-modal similarity evaluation framework shown hereis inspired by the semantic relations defined in Wordnet.
These criteria were provided to the humanannotators, to help them decide whether a word and an image are related to each other.1.
Instance of itself: Does the image contain an entity that is represented by the word itself (e.g.
animage of ?Obama?
vs the word ?Obama?)
?2.
Member-of Relation: Does the image contain an entity that is a member of the class suggestedby the word or vice versa (e.g.
an image of an ?apple?
vs the word ?fruits?)
?3.
Part-of Relation: Does the image contain an entity that is a part of a larger entity represented bythe word or vice versa (e.g.
an image of a ?tree?
vs the word ?forest?)
?4.
Semantically Related: Do both the word and the image suggest concepts that are related (e.g.
animage of troops at war vs the word ?peace?)
?5.
Semantically Close: Do both the word and the image suggest concepts that are not only relatedbut also close in meaning?
(e.g.
an image of troops at war vs the word ?gun?)
?Criterion (1) basically tests for synonym relation.
Criteria (2) and (3) are modeled after the hyponym-hypernym and meronym-holonym relations in WordNet, which are prevalent among nouns.
Note thatnone of the criteria is preemptive over the others.
Rather, we provide these criteria as guidelines ina subjective evaluation framework, similar to the word semantic similarity task in Miller and Charles(1998).
Importantly, criterion (4) models dissimilar but related concepts, or any other relation that indi-cates frequent association, while criterion (5) serves to provide additional distinction for pairs of wordsand images on a higher level of relatedness toward similarity.
In Table 1, we show sample images fromour test dataset, along with the annotations provided by the human annotators.6We only allowed annotators with an approval rating of 97% or higher.
Here, we expect some variance in the degree ofrelatedness between the candidate words and images, hence annotations marked with all 10s or 0s are discarded due to lack ofdistinctions in similarity relatedness7Annotation guidelines and dataset can be downloaded at http://lit.csci.unt.edu/index.php/Downloads1905.3 ExperimentsFollowing Erk and McCarthy (2009), who argued that word meanings are graded over their senses, webelieve that the meaning of an image is not limited to a set of ?best fitting?
tags, but rather it exists asa distribution over arbitrary words with varying degrees of association.
Specifically, the focus of ourexperiments is to investigate the correlation between automatic measures of such relatedness scores withrespect to human judgments.To construct the joint semantic space of words and images, we use the SVD described in Section 4to reduce the number of dimensions.
To build each model, we use the 167 synsets from ImageNet andtheir associated images (minus the held out test data), hence accounting for 167 latent dimensions.
Wefirst represent the synsets as a collection of documents D, each document containing visual codewordsused to describe their associated images as well as textual words extracted from their gloss and synsetwords.
Thus, computing a cross-modal relatedness distance amounts to comparing the cosine similarityof vectors representing an image to the vector representing a word in the term-document vector space.Note that, unlike textual words, an image is represented by multiple visual codewords.
Prior to computingthe actual cosine distance, we perform a weighted addition of vectors representing each visual codewordfor that image.To illustrate, consider a single document di, representing the synset ?snail,?
which consists of {cw0,cw555, cw23, cw124, cw876, snail, freshwater, mollusk, spiral, shell}, where cwX represents a particularvisual codeword indexed from 0-9998, and the textual words are nouns extracted from the associatedsynset and gloss.
Given a test image I , it can be expressed as a bag of visual codewords {cw1 , ... , cwk}.We first represent each visual codeword in I as a vector of length |D| using term-frequency inverse-document-frequency (tf idf ) weighting, e.g., cwk=<0.4*d1, 0.2*d2, ... , 0.9*dm>, where m=167, andperform an addition of k such vectors to form a final vector vi.
To measure the semantic relatednessbetween image I and a word w, e.g., ?snail,?
we simply compute the cosine similarity between vi andvw, where vw is also a vector of length |D| calculated using tf idf .This paper seeks answers to the following questions.
First, what is the relation between the discrim-inability of the visual codewords and their ability to capture semantic relatedness between a word and animage, as compared to the gold-standard annotation by humans?
Second, given the unbalanced datasetof images and words, can we use a relatively small number of visual codewords to derive such semanticrelatedness measures reliably?
Third, what is the efficiency of an unsupervised vector semantic model inmeasuring such relatedness, and is it applicable to large datasets?Analogous to text-retrieval methods, we measure the discriminability of the visual codewords usingtwo weighting factors.
The first is term-frequency (tf), which measures the number of times a codewordappears in all images for a particular synset, while the second, image-term-frequency (itf), captures thenumber of images using the codeword in a synset.
For the two weighting schemes, we apply normal-ization by using the total number of codewords for a synset (for tf weighting) and the total number ofimages in a synset (for itf weighting).We are interested to quantify the relatedness for pairs of words and images under two scenarios.
Byranking the 12 words associated with an image in reverse order of their relatedness to the image, wecan determine the ability of our models to identify the most related words for a given image (image-centered).
In the second scenario, we measure the relatedness of words and images regardless of thesynset they belong to, thus evaluating the ability of our methods to capture the relatedness between anyword and any image.
This allows us to capture the correlation in an (arbitrary-image) scenario.
For theevaluations, we use the Spearman?s Rank correlation.To place our results in perspective, we implemented two baselines and an upper bound for each ofthe two scenarios above.
The Random baseline randomly assigns ratings to each word-image pair on thesame 0 to 10 scale, and then measures the correlation to the human gold-standard.
The Vector-Based (VB)method is a stronger baseline aimed to study the correlation performance in the absence of dimensionalityreduction.
As an upper bound, the Inter-Human-Agreement (IHA) measures the correlation of the ratingby each annotator against the average of the ratings of the rest of the annotators, averaged over the 167synsets (for the image-centered scenario) and over the 2004 word-image pairs (for the arbitrary-imagescenario).8For simplicity, we only show the top 5 visual codewords191Spearman?s Rank Coefficient (image-centered)Top K codewords 100 200 300 400 500 600 700 800 900 1000LSA tf 0.228 0.325 0.273 0.242 0.185 0.181 0.107 0.043 -0.018 0.000LSA tf (norm) 0.233 0.339 0.293 0.254 0.202 0.180 0.124 0.047 -0.012 0.000LSA tf*itf 0.268 0.317 0.256 0.248 0.219 0.166 0.081 -0.004 -0.037 0.000LSA tf*itf (norm) 0.252 0.327 0.257 0.246 0.211 0.153 0.097 0.002 -0.042 0.000VB tf 0.243 0.168 0.101 0.055 -0.021 -0.084 -0.157 -0.210 -0.236 -0.332VB tf (norm) 0.240 0.181 0.110 0.062 -0.010 -0.082 -0.152 -0.204 -0.235 -0.332VB tf*itf 0.262 0.181 0.107 0.065 -0.019 -0.081 -0.156 -0.211 -0.241 -0.332VB tf*itf (norm) 0.257 0.180 0.116 0.068 -0.014 -0.079 -0.150 -0.250 -0.237 -0.332Random 0.001 0.018 0.016 -0.008 0.008 0.005 -0.001 0.014 -0.035 0.012IHA 0.687Spearman?s Rank Coefficient (arbitrary-image)Top K codewords 100 200 300 400 500 600 700 800 900 1000LSA tf 0.236 0.341 0.291 0.249 0.208 0.183 0.106 0.033 -0.039 0.000LSA tf (norm) 0.230 0.353 0.301 0.271 0.220 0.186 0.115 0.032 -0.029 0.000LSA tf*itf 0.291 0.332 0.289 0.262 0.235 0.172 0.092 0.008 -0.041 0.000LSA tf*itf (norm) 0.277 0.345 0.292 0.269 0.234 0.164 0.098 0.015 -0.046 0.000VB tf 0.272 0.195 0.119 0.059 -0.012 -0.088 -0.164 -0.218 -0.240 -0.339VB tf (norm) 0.277 0.207 0.130 0.069 -0.003 -0.083 -0.160 -0.215 -0.242 -0.339VB tf*itf 0.287 0.206 0.127 0.062 -0.008 -0.085 -0.161 -0.214 -0.241 -0.339VB tf*itf (norm) 0.286 0.212 0.132 0.071 -0.005 -0.081 -0.158 -0.214 -0.241 -0.339Random -0.024 -0.014 0.015 -0.015 -0.004 -0.014 0.024 -0.009 -0.007 0.007IHA 0.764Table 2: Correlation of automatically generated scores with human annotations on cross-modal semanticrelatedness, as performed on the ImageNet test dataset of 2004 pairs of word and image.
Correlationfigures scoring the highest within a weighting scheme are marked in bold, while those scoring the highestacross weighting schemes and within a visual vocabulary size are underlined.6 DiscussionOur experimental results are shown in Table 2.
A somewhat surprising observation is the consistency ofcorrelation figures between the two scenarios.
In both scenarios, a representative set of 200 visual code-words is sufficient to consistently score the highest correlation ratings across the 8 weighting schemes.Intuitively, based on the experimental results, automatically choosing the top 10% or 20% of the visualcodewords seems to suffice and gives optimal correlation figures, but requires further justification.
Con-versely, the relatively simple weighting scheme using tf (normalized) produces the highest correlation insix visual codeword sizes (K=200,300,400,700,800,900) for the image-centered scenario, as well as inanother six visual codeword sizes (K=200,300,400,600,700,900) for the arbitrary-image scenario.
Un-like stopwords in text retrieval accounting for most of the highest tf scores, visual codewords weightedby the same scheme tf and a similar tf (normalized) scheme seem to be the most discriminative.
Thecorrelation for including the entire visual vocabulary set (1000) produces identical results for all vector-based and LSA weighting schemes, as images across synsets are now encoded by the same set of visualcodewords without discrimination between them.Dimensionality reduction using SVD gains an advantage over the vector-based method for both sce-narios, with the highest correlation rating in LSA (200 visual codeword, tf(norm)) achieving 0.077 pointsbetter than the corresponding highest correlation in Vector-based (100 visual codeword, tf*itf ) for theimage-centered scenario, representing a 29.3% improvement.
Similarly, in the arbitrary-image scenario,the increase in correlation from 0.287 (VB tf*itf at 100 visual codeword) to 0.353 (LSA tf(norm) at200 visual codeword) underlines a gain of approximately 23.0%.
Overall, the arbitrary-image scenarioalso scores consistently higher than the image-centered scenario under similar experimental conditions.For instance, for the top 200 visual words, the same weighting schemes produce consistently lowercorrelation figures for the image-centered scenario.
This is also true for the Inter-Human-Agreementscore, which is higher in the arbitrary-image scenario (0.764) compared to the image-centered scenario(0.687).
Note that for all the experiments, the semantic relatedness scores generated from the semanticvector space are significantly more correlated with the human gold-standard than the random baselines.192(a) (b)Figure 3: (a) Correlation performance, and (b) Classification accuracy, as more data is added to constructthe semantic space model.To investigate the effectiveness of the model when scaling up to large datasets, we employ the bestcombination of weighting scheme and vocabulary size shown in Table 2, i.e., a visual vocabulary sizeof 200 and tf (normalized) weighting for LSA, and vocabulary size of 100 and tf*itf weighting for thevector-based model, and incrementally construct models ranging from 167 synsets to 800 synsets (allrandomly selected from ImageNet).
We then measure the correlation of relatedness scores generatedusing the same test dataset with respect to human annotations.
The dataset was randomly selected to in-crease by approximately five times, from a total of 230,864 images with 878 words to a total of 1,014,528images with 3887 words.
Furthermore, for each unseen test image taken from Synset Si and the associ-ated 12 candidate words, we evaluate the ability of the model to identify which of the candidate wordsactually appear in the gloss or the synset of Si, in a task we term as word classification.
Here, the topsix words are predictably classified as those appearing in Si while the last six are classified as outsideof Si , after all 12 words are ranked in reverse order of their relatedness to the test image.
We measurethe accuracy of the word classification task using TP+TN2004 , where TP is the number of words correctlyclassified as synset or gloss words, and TN is the number of words correctly classified as outside ofsynset or gloss, both summed over the 2004 pairs of words and images.As shown in Figure 3, when a small number of synsets (33) was added to the original semantic space,correlation with human ratings increased steeply to around 0.45 and higher for LSA in both scenarios,while the vector-based method suffers a slight decrease in correlation ratings from 0.262 to 0.251 (image-centered) and from 0.287 to 0.278 (arbitrary-image).
As more images and words are added, correlationfor the vector-based model continues to decrease markedly.
Comparatively, LSA is less sensitive to datascaling, as correlation figures for both scenarios decreases slightly but stays within a 0.40 to 0.45 range.Additionally, we infer that LSA is consistently more effective than the vector-based model in the wordsclassification task (as also seen in Figure 3).
Even with more data added to the semantic space, wordclassification accuracy stays consistently at 0.7 for LSA, while it drops to 0.535 for the vector-basedmodel at a synset size of 800.7 ConclusionIn this paper, we provided a proof of concept in quantifying the semantic relatedness between words andimages through the use of visual codewords and textual words in constructing a joint semantic vectorspace.
Our experiments showed that the relatedness scores have a positive correlation to human gold-standards, as measured using a standard evaluation framework.We believe many aspects of this work can be explored further.
For instance, other visual codewordattributes, such as pixel coordinates, can be employed in a structured vector space along with the existingmodel for improving vector similarity measures.
To improve textual words coverage, a potentially effec-193tive way would be to create mappings from WordNet synsets to Wikipedia entries, where the conceptsrepresented by the synsets are discussed in detail.
We also plan to study the applicability of the jointsemantic representation model to tasks such as automatic image annotation and image classification.AcknowledgmentsThis material is based in part upon work supported by the National Science Foundation CAREER award#0747340 and IIS award #1018613.
Any opinions, findings, and conclusions or recommendations ex-pressed in this material are those of the authors and do not necessarily reflect the views of the NationalScience Foundation.ReferencesBarnard, K. and D. Forsyth (2001).
Learning the semantics of words and pictures.
In Proceedings of InternationalConference on Computer Vision.Biederman, I.
(1987).
Recognition-by-components: A theory of human image understanding.
In PsychologicalReview, Volume 94, pp.
115?147.Budanitsky, A. and G. Hirst (2005).
Evaluating wordnet-based measures of lexical semantic relatedness.
InComputational Linguistics, Volume 32.Deng, J., W. Dong, R. Socher, L.-J.
Li, K. Li, and L. Fei-Fei (2009).
ImageNet: A Large-Scale Hierarchical ImageDatabase.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Erk, K. and D. McCarthy (2009).
Graded word sense assignment.
In Proceedings of Empirical Methods in NaturalLanguage Processing.Fei-Fei, L. and P. Perona (2005).
A bayesian hierarchical model for learning natural scene categories.
In Proceed-ings of the IEEE Conference on Computer Vision and Pattern Recognition.Feng, Y. and M. Lapata (2010).
Visual information in semantic representation.
In Proceedings of the AnnualConference of the North American Chapter of the ACL.Goldberger, J., S. Gordon, and H. Greenspan (2003).
An efficient image similarity measure based on approxima-tions of kl-divergence between two gaussian mixtures.
In Proceedings of IEEE International Conference onComputer Vision.Hare, J. S., S. Samangooei, P. H. Lewis, and M. S. Nixon (2008).
Investigating the performance of auto-annotationand semantic retrieval using semantic spaces.
In Proceedings of the international conference on content-basedimage and video retrieval.Kanerva, P. (1998).
Sparse distributed memory.
In MIT Press.Landauer, T. and S. Dumais (1997).
A solution to platos problem: The latent semantic analysis theory of acquisi-tion.
In Psychological Review, Volume 104, pp.
211?240.Leong, C. W., R. Mihalcea, and S. Hassan (2010).
Text mining for automatic image tagging.
In Proceedings of theInternational Conference on Computational Linguistics.Lowe, D. (2004).
Distinctive image features from scale-invariant keypoints.
In International Journal of ComputerVision.Makadia, A., V. Pavlovic, and S. Kumar (2008).
A new baseline for image annotation.
In Proceedings of EuropeanConference on Computer Vision.Miller, G. (1995).
Wordnet: A lexical database for english.
In Communications of the ACM, Volume 38, pp.
39?41.Miller, G. and W. Charles (1998).
Contextual correlates of semantic similarity.
Language and Cognitive Pro-cesses 6(1).Potter, M. C. and B.
A. Faulconer (1975).
Time to understand pictures and words.
In Nature, Volume 253, pp.437?438.Salton, G., A. Wong, and C. Yang (1997).
A vector space model for automatic indexing.
In Readings in InformationRetrieval, pp.
273?280.
San Francisco, CA: Morgan Kaufmann Publishers.Schmid, H. (1994).
Probabilistic part-of-speech tagging using decision trees.
In Proceedings of the InternationalConference on New Methods in Language Processing.Smeulders, A. W., M. Worring, S. Santini, A. Gupta, and R. Jain (2000).
Content-based image retrieval at theend of the early years.
In IEEE Transactions on Pattern Analysis and Machine Intelligence, Volume 22, pp.1349?1380.Westerveld, T. (2000).
Image retrieval: Context versus context.
In Content-Based Multimedia Information Access.Widdows, D. and K. Ferraro (2008).
Semantic vectors: a scalable open source package and online technology man-agement application.
In Proceedings of the Sixth International Language Resources and Evaluation (LREC?08).Yang, J., Y.-G. Jiang, A. G. Hauptmann, and C.-W. Ngo (2007).
Evaluating bag-of-visual-words representationsin scene classification.
In ACM Multimedia Information Retrieval Workshop.194
