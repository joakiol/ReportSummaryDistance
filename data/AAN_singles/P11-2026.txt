Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 147?152,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsFrom Bilingual Dictionaries to Interlingual Document RepresentationsJagadeesh JagarlamudiUniversity of MarylandCollege Park, USAjags@umiacs.umd.eduHal Daume?
IIIUniversity of MarylandCollege Park, USAhal@umiacs.umd.eduRaghavendra UdupaMicrosoft Research IndiaBangalore, Indiaraghavu@microsoft.comAbstractMapping documents into an interlingual rep-resentation can help bridge the language bar-rier of a cross-lingual corpus.
Previous ap-proaches use aligned documents as trainingdata to learn an interlingual representation,making them sensitive to the domain of thetraining data.
In this paper, we learn an in-terlingual representation in an unsupervisedmanner using only a bilingual dictionary.
Wefirst use the bilingual dictionary to find candi-date document alignments and then use themto find an interlingual representation.
Sincethe candidate alignments are noisy, we de-velop a robust learning algorithm to learnthe interlingual representation.
We show thatbilingual dictionaries generalize to differentdomains better: our approach gives better per-formance than either a word by word transla-tion method or Canonical Correlation Analy-sis (CCA) trained on a different domain.1 IntroductionThe growth of text corpora in different languagesposes an inherent problem of aligning documentsacross languages.
Obtaining an explicit alignment,or a different way of bridging the language barrier,is an important step in many natural language pro-cessing (NLP) applications such as: document re-trieval (Gale and Church, 1991; Rapp, 1999; Balles-teros and Croft, 1996; Munteanu and Marcu, 2005;Vu et al, 2009), Transliteration Mining (Klementievand Roth, 2006; Hermjakob et al, 2008; Udupa etal., 2009; Ravi and Knight, 2009) and MultilingualWeb Search (Gao et al, 2008; Gao et al, 2009).Aligning documents from different languages arisesin all the above mentioned problems.
In this pa-per, we address this problem by mapping documentsinto a common subspace (interlingual representa-tion)1.
This common subspace generalizes the no-tion of vector space model for cross-lingual applica-tions (Turney and Pantel, 2010).There are two major approaches for solving thedocument alignment problem, depending on theavailable resources.
The first approach, whichis widely used in the Cross-lingual InformationRetrieval (CLIR) literature, uses bilingual dictio-naries to translate documents from one language(source) into another (target) language (Ballesterosand Croft, 1996; Pirkola et al, 2001).
Then stan-dard measures such as cosine similarity are used toidentify target language documents that are close tothe translated document.
The second approach is touse training data of aligned document pairs to find acommon subspace such that the aligned documentpairs are maximally correlated (Susan T. Dumais,1996; Vinokourov et al, 2003; Mimno et al, 2009;Platt et al, 2010; Haghighi et al, 2008) .Both kinds of approaches have their own strengthsand weaknesses.
Dictionary based approaches treatsource documents independently, i.e., each sourcelanguage document is translated independently ofother documents.
Moreover, after translation, the re-lationship of a given source document with the restof the source documents is ignored.
On the otherhand, supervised approaches use all the source andtarget language documents to infer an interlingual1We use the phrases ?common subspace?
and ?interlingualrepresentation?
interchangeably.147representation, but their strong dependency on thetraining data prevents them from generalizing wellto test documents from a different domain.In this paper, we propose a technique that com-bines the advantages of both these approaches.
At abroad level, our approach uses bilingual dictionariesto identify initial noisy document alignments (Sec.2.1) and then uses these noisy alignments as train-ing data to learn a common subspace.
Since thealignments are noisy, we need a learning algorithmthat is robust to the errors in the training data.
It isknown that techniques like CCA overfit the trainingdata (Rai and Daume?
III, 2009).
So, we start with anunsupervised approach such as Kernelized Sorting(Quadrianto et al, 2009) and develop a supervisedvariant of it (Sec.
2.2).
Our supervised variant learnsto modify the within language document similaritiesaccording to the given alignments.
Since the origi-nal algorithm is unsupervised, we hope that its su-pervised variant is tolerant to errors in the candidatealignments.
The primary advantage of our method isthat, it does not use any training data and thus gen-eralizes to test documents from different domains.And unlike the dictionary based approaches, we useall the documents in computing the common sub-space and thus achieve better accuracies comparedto the approaches which translate documents in iso-lation.There are two main contributions of this work.First, we propose a discriminative technique to learnan interlingual representation using only a bilingualdictionary.
Second, we develop a supervised variantof Kernelized Sorting algorithm (Quadrianto et al,2009) which learns to modify within language doc-ument similarities according to a given alignment.2 ApproachGiven a cross-lingual corpus, with an underlying un-known document alignment, we propose a techniqueto recover the hidden alignment.
This is achievedby mapping documents into an interlingual repre-sentation.
Our approach involves two stages.
In thefirst stage, we use a bilingual dictionary to find ini-tial candidate noisy document alignments.
The sec-ond stage uses a robust learning algorithm to learn acommon subspace from the noisy alignments iden-tified in the first step.
Subsequently, we project allthe documents into the common subspace and usemaximal matching to recover the hidden alignment.During this stage, we also learn mappings from thedocument spaces onto the common subspace.
Thesemappings can be used to convert any new documentinto the interlingual representation.
We describeeach of these two steps in detail in the following twosub sections (Sec.
2.1 and Sec.
2.2).2.1 Noisy Document AlignmentsTranslating documents from one language into an-other language and finding the nearest neighboursgives potential alignments.
Unfortunately, the re-sulting alignments may differ depending on the di-rection of the translation owing to the asymmetryof bilingual dictionaries and the nearest neighbourproperty.
In order to overcome this asymmetry, wefirst turn the documents in both languages into bagof translation pairs representation.We follow the feature representation used in Ja-garlamudi and Daume?
III (2010) and Boyd-Graberand Blei (2009).
Each translation pair of the bilin-gual dictionary (also referred as a dictionary en-try) is treated as a new feature.
Given a docu-ment, every word is replaced with the set of bilin-gual dictionary entries that it participates in.
IfD represents the TFIDF weighted term ?
docu-ment matrix and T is a binary matrix matrix of sizeno of dictionary entries?
vocab size, then convert-ing documents into a bag of dictionary entries isgiven by the linear operation X(t) ?
TD.2After converting the documents into bag of dic-tionary entries representation, we form a bipartitegraph with the documents of each language as aseparate set of nodes.
The edge weight Wij be-tween a pair of documents x(t)i and y(t)j (in sourceand target language respectively) is computed as theEuclidean distance between those documents in thedictionary space.
Let piij indicate the likeliness ofa source document x(t)i is aligned to a target doc-ument y(t)j .
We want each document to align to atleast one document from other language.
Moreover,we want to encourage similar documents to alignto each other.
We can formulate this objective andthe constraints as the following minimum cost flow2Superscript (t) indicates that the data is in the form of bagof dictionary entries148problem (Ravindra et al, 1993):argminpim,n?i,j=1Wijpiij (1)?i?jpiij = 1 ; ?j?ipiij = 1?i, j 0 ?
piij ?
Cwhere C is some user chosen constant, m and nare the number of documents in source and targetlanguages respectively.
Without the last constraint(piij ?
C) this optimization problem always gives anintegral solution and reduces to a maximum match-ing problem (Jonker and Volgenant, 1987).
Sincethis solution may not be accurate, we allow many-to-many mapping by setting the constant C to a valueless than one.
In our experiments (Sec.
3), wefound that setting C to a value less than 1 gave bet-ter performance analogous to the better performanceof soft Expectation Maximization (EM) comparedto hard-EM.
The optimal solution of Eq.
1 can befound efficiently using linear programming (Ravin-dra et al, 1993).2.2 Supervised Kernelized SortingKernelized Sorting is an unsupervised technique toalign objects of different types, such as English andSpanish documents (Quadrianto et al, 2009; Ja-garalmudi et al, 2010).
The main advantage of thismethod is that it only uses the intra-language doc-ument similarities to identify the alignments acrosslanguages.
In this section, we describe a supervisedvariant of Kernelized Sorting which takes a set ofcandidate alignments and learns to modify the intra-language document similarities to respect the givenalignment.
Since Kernelized Sorting does not relyon the inter-lingual document similarities at all, wehope that its supervised version is robust to noisyalignments.Let X and Y be the TFIDF weighted term ?document matrices in both the languages and letKx and Ky be their linear dot product kernel ma-trices, i.e.
, Kx = XTX and Ky = Y TY .Let ?
?
{0, 1}m?n denote the permutation matrixwhich captures the alignment between documents ofdifferent languages, i.e.
piij = 1 indicates docu-ments xi and yj are aligned.
Then Kernelized Sort-ing formulates ?
as the solution of the following op-timization problem (Gretton et al, 2005):argmax?tr(Kx?Ky?T ) (2)= argmax?tr(XTX ?
Y TY ?T ) (3)In our supervised version of Kernelized Sorting,we fix the permutation matrix (to say ??)
and mod-ify the kernel matrices Kx and Ky so that the ob-jective function is maximized for the given permu-tation.
Specifically, we find a mapping for each lan-guage, such that when the documents are projectedinto their common subspaces they are more likely torespect the alignment given by ??.
Subsequently, thetest documents are also projected into the commonsubspace and we return the nearest neighbors as thealigned pairs.Let U and V be the mappings for the required sub-space in both the languages, then we want to solvethe following optimization problem:argmaxU,Vtr(XTUUTX ??
Y TV V TY ?
?T )s.t.
UTU = I & V TV = I (4)where I is an identity matrix of appropriate size.
Forbrevity, let Cxy denote the cross-covariance matrix(i.e.
Cxy = X?
?Y T ) then the above objective func-tion becomes:argmaxU,Vtr(UUTCxyV V TCTxy)s.t.
UTU = I & V TV = I (5)We have used the cyclic property of the trace func-tion while rewriting Eq.
4 to Eq.
5.
We use alterna-tive maximization to solve for the unknowns.
FixingV (to say V0), rewriting the objective function usingthe cyclic property of the trace function, forming theLagrangian and setting its derivative to zero resultsin the following solution:CxyV0V T0 CTxy U = ?u U (6)For the initial iteration, we can substitute V0V T0 asidentity matrix which leaves the kernel matrix un-changed.
Similarly, fixing U (to U0) and solving theoptimization problem for V results:CTxyU0UT0 Cxy V = ?v V (7)149In the special case where both V0V T0 and U0UT0are identity matrices, the above equations reduce toCxyCTxy U = ?u U and CTxyCxy V = ?v V .
Inthis particular case, we can simultaneously solve forboth U and V using Singular Value Decomposition(SVD) as:USV T = Cxy (8)So for the first iteration, we do the SVD of the cross-covariance matrix and get the mappings.
For thesubsequent iterations, we use the mappings found bythe previous iteration, as U0 and V0, and solve Eqs.6 and 7 alternatively.2.3 SummaryIn this section, we describe our procedure to recoverdocument alignments.
We first convert documentsinto bag of dictionary entries representation (Sec.2.1).
Then we solve the optimization problem in Eq.1 to get the initial candidate alignments.
We use theLEMON3 graph library to solve the min-cost flowproblem.
This step gives us the piij values for everycross-lingual document pair.
We use them to forma relaxed permutation matrix (??)
which is, subse-quently, used to find the mappings (U and V ) forthe documents of both the languages (i.e.
solv-ing Eq.
8).
We use these mappings to project bothsource and target language documents into the com-mon subspace and then solve the bipartite matchingproblem to recover the alignment.3 ExperimentsFor evaluation, we choose 2500 aligned docu-ment pairs from Wikipedia in English-Spanish andEnglish-German language pairs.
For both the datasets, we consider only words that occurred morethan once in at least five documents.
Of the wordsthat meet the frequency criterion, we choose themost frequent 2000 words for English-Spanish dataset.
But, because of the compound word phe-nomenon of German, we retain all the frequentwords for English-German data set.
Subsequentlywe convert the documents into TFIDF weighted vec-tors.
The bilingual dictionaries for both the lan-guage pairs are generated by running Giza++ (Ochand Ney, 2003) on the Europarl data (Koehn, 2005).3https://lemon.cs.elte.hu/trac/lemonEn ?
Es En ?
DeWord-by-Word 0.597 0.564CCA (?
= 0.3) 0.627 0.485CCA (?
= 0.5) 0.628 0.486CCA (?
= 0.8) 0.637 0.487OPCA 0.688 0.530Ours (C = 0.6) 0.67 0.604Ours (C = 1.0) 0.658 0.590Table 1: Accuracy of different approaches on theWikipedia documents in English-Spanish and English-German language pairs.
For CCA, we regularize thewithin language covariance matrices as (1??
)XXT+?Iand the regularization parameter ?
value is also shown.We follow the process described in Sec.
2.3 to re-cover the document alignment for our method.We compare our approach with a dictionary basedapproach, such as word-by-word translation, andsupervised approaches, such as CCA (Vinokourovet al, 2003; Hotelling, 1936) and OPCA (Plattet al, 2010).
Word-by-word translation and ourapproach use bilingual dictionary while CCA andOPCA use a training corpus of aligned documents.Since the bilingual dictionary is learnt from Eu-roparl data set, for a fair comparison, we train su-pervised approaches on 3000 document pairs fromEuroparl data set.
To prevent CCA from overfittingto the training domain, we regularize it heavily.
ForOPCA, we use a regularization parameter of 0.1 assuggested by Platt et al (2010).
For all the systems,we construct a bipartite graph between the docu-ments of different languages, with edge weight be-ing the cross-lingual similarity given by the respec-tive method and then find maximal matching (Jonkerand Volgenant, 1987).
We report the accuracy of therecovered alignment.Table 1 shows accuracies of different methods onboth Spanish and German data sets.
For comparisonpurposes, we trained and tested CCA on documentsfrom same domain (Wikipedia).
It achieves 75% and62% accuracies for the two data sets respectivelybut, as expected, it performed poorly when trainedon Europarl articles.
On the English-German dataset, a simple word-by-word translation performedbetter than CCA and OPCA.
For both the languagepairs, our model performed better than word-by-word translation method and competitively with the150supervised approaches.
Note that our method doesnot use any training data.We also experimented with few values of the pa-rameter C for the min-cost flow problem (Eq.
1).As noted previously, setting C = 1 will reduce theproblem into a linear assignment problem.
Fromthe results, we see that solving a relaxed version ofthe problem gives better accuracies but the improve-ments are marginal (especially for English-German).4 DiscussionFor both language pairs, the accuracy of the firststage of our approach (Sec.
2.1) is almost same asthat of word-by-word translation system.
Thus, theimproved performance of our system compared toword-by-word translation shows the effectiveness ofthe supervised Kernelized sorting.The solution of our supervised Kernelized sorting(Eq.
8) resembles Latent Semantic Indexing (Deer-wester, 1988).
Except, we use a cross-covariancematrix instead of a term ?
document matrix.
Effi-cient algorithms exist for solving SVD on arbitrarilylarge matrices, which makes our approach scalableto large data sets (Warmuth and Kuzmin, 2006).
Af-ter solving Eq.
8, the mappings U and V can beimproved by iteratively solving the Eqs.
6 and 7 re-spectively.
But it leads the mappings to fit the noisyalignments exactly, so in this paper we stop aftersolving the SVD problem.The extension of our approach to the situationwith different number of documents on each side isstraight forward.
The only thing that changes is theway we compute alignment after finding the projec-tion directions.
In this case, the input to the bipar-tite matching problem is modified by adding dummydocuments to the language that has fewer documentsand assigning a very high score to edges that connectto the dummy documents.5 ConclusionIn this paper we have presented an approach to re-cover document alignments from a comparable cor-pora using a bilingual dictionary.
First, we use thebilingual dictionary to find a set of candidate noisyalignments.
These noisy alignments are then fed intosupervised Kernelized Sorting, which learns to mod-ify within language document similarities to respectthe given alignments.Our approach exploits two complimentary infor-mation sources to recover a better alignment.
Thefirst step uses cross-lingual cues available in theform of a bilingual dictionary and the latter stepexploits document structure captured in terms ofwithin language document similarities.
Experimen-tal results show that our approach performs betterthan dictionary based approaches such as a word-by-word translation and is also competitive with su-pervised approaches like CCA and OPCA.ReferencesLisa Ballesteros and W. Bruce Croft.
1996.
Dictio-nary methods for cross-lingual information retrieval.In Proceedings of the 7th International Conferenceon Database and Expert Systems Applications, DEXA?96, pages 791?801, London, UK.
Springer-Verlag.Jordan Boyd-Graber and David M. Blei.
2009.
Multilin-gual topic models for unaligned text.
In Uncertaintyin Artificial Intelligence.Scott Deerwester.
1988.
Improving Information Re-trieval with Latent Semantic Indexing.
In Christine L.Borgman and Edward Y. H. Pai, editors, Proceed-ings of the 51st ASIS Annual Meeting (ASIS ?88), vol-ume 25, Atlanta, Georgia, October.
American Societyfor Information Science.William A. Gale and Kenneth W. Church.
1991.
A pro-gram for aligning sentences in bilingual corpora.
InProceedings of the 29th annual meeting on Associ-ation for Computational Linguistics, pages 177?184,Morristown, NJ, USA.
Association for ComputationalLinguistics.Wei Gao, John Blitzer, and Ming Zhou.
2008.
Usingenglish information in non-english web search.
In iN-EWS ?08: Proceeding of the 2nd ACM workshop onImproving non english web searching, pages 17?24,New York, NY, USA.
ACM.Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong.2009.
Exploiting bilingual information to improveweb search.
In Proceedings of Human Language Tech-nologies: The 2009 Conference of the Association forComputational Linguistics, ACL-IJCNLP ?09, pages1075?1083, Morristown, NJ, USA.
ACL.Arthur Gretton, Arthur Gretton, Olivier Bousquet, OlivierBousquet, Er Smola, Bernhard Schlkopf, and Bern-hard Schlkopf.
2005.
Measuring statistical depen-dence with hilbert-schmidt norms.
In Proceedings ofAlgorithmic Learning Theory, pages 63?77.
Springer-Verlag.151Aria Haghighi, Percy Liang, Taylor B. Kirkpatrick, andDan Klein.
2008.
Learning bilingual lexicons frommonolingual corpora.
In Proceedings of ACL-08:HLT, pages 771?779, Columbus, Ohio, June.
Associa-tion for Computational Linguistics.Ulf Hermjakob, Kevin Knight, and Hal Daume?
III.
2008.Name translation in statistical machine translation -learning when to transliterate.
In Proceedings of ACL-08: HLT, pages 389?397, Columbus, Ohio, June.
As-sociation for Computational Linguistics.H.
Hotelling.
1936.
Relation between two sets of vari-ables.
Biometrica, 28:322?377.Jagadeesh Jagaralmudi, Seth Juarez, and Hal Daume?
III.2010.
Kernelized sorting for natural language process-ing.
In Proceedings of AAAI Conference on ArtificialIntelligence.Jagadeesh Jagarlamudi and Hal Daume?
III.
2010.
Ex-tracting multilingual topics from unaligned compara-ble corpora.
In Advances in Information Retrieval,32nd European Conference on IR Research, ECIR,volume 5993, pages 444?456, Milton Keynes, UK.Springer.R.
Jonker and A. Volgenant.
1987.
A shortest augment-ing path algorithm for dense and sparse linear assign-ment problems.
Computing, 38(4):325?340.Alexandre Klementiev and Dan Roth.
2006.
Weaklysupervised named entity transliteration and discoveryfrom multilingual comparable corpora.
In Proceed-ings of the 21st International Conference on Compu-tational Linguistics and the 44th annual meeting of theAssociation for Computational Linguistics, ACL-44,pages 817?824, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT Summit.David Mimno, Hanna M. Wallach, Jason Naradowsky,David A. Smith, and Andrew McCallum.
2009.Polylingual topic models.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing: Volume 2 - Volume 2, EMNLP ?09,pages 880?889, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Comput.
Linguist., 31:477?504, December.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Ari Pirkola, Turid Hedlund, Heikki Keskustalo, andKalervo Jrvelin.
2001.
Dictionary-based cross-language information retrieval: Problems, methods,and research findings.
Information Retrieval, 4:209?230.John C. Platt, Kristina Toutanova, and Wen-tau Yih.2010.
Translingual document representations fromdiscriminative projections.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?10, pages 251?261,Stroudsburg, PA, USA.Novi Quadrianto, Le Song, and Alex J. Smola.
2009.Kernelized sorting.
In D. Koller, D. Schuurmans,Y.
Bengio, and L. Bottou, editors, Advances in NeuralInformation Processing Systems 21, pages 1289?1296.Piyush Rai and Hal Daume?
III.
2009.
Multi-label pre-diction via sparse infinite cca.
In Advances in NeuralInformation Processing Systems, Vancouver, Canada.Reinhard Rapp.
1999.
Automatic identification of wordtranslations from unrelated english and german cor-pora.
In Proceedings of the 37th annual meetingof the Association for Computational Linguistics onComputational Linguistics, ACL ?99, pages 519?526,Stroudsburg, PA, USA.Sujith Ravi and Kevin Knight.
2009.
Learning phonememappings for transliteration without parallel data.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 37?45, Boulder, Colorado, June.K.
Ahuja Ravindra, L. Magnanti Thomas, and B. OrlinJames.
1993.
Network flows: Theory, algorithms, andapplications.Michael L. Littman Susan T. Dumais, Thomas K. Lan-dauer.
1996.
Automatic cross-linguistic informationretrieval using latent semantic indexing.
In WorkingNotes of the Workshop on Cross-Linguistic Informa-tion Retrieval, SIGIR, pages 16?23, Zurich, Switzer-land.
ACM.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.J.
Artif.
Intell.
Res.
(JAIR), 37:141?188.Raghavendra Udupa, K. Saravanan, A. Kumaran, and Ja-gadeesh Jagarlamudi.
2009.
Mint: A method for ef-fective and scalable mining of named entity transliter-ations from large comparable corpora.
In EACL, pages799?807.
The Association for Computer Linguistics.Alexei Vinokourov, John Shawe-taylor, and Nello Cris-tianini.
2003.
Inferring a semantic representationof text via cross-language correlation analysis.
InAdvances in Neural Information Processing Systems,pages 1473?1480, Cambridge, MA.
MIT Press.Thuy Vu, AiTi Aw, and Min Zhang.
2009.
Feature-basedmethod for document alignment in comparable newscorpora.
In EACL, pages 843?851.Manfred K. Warmuth and Dima Kuzmin.
2006.
Ran-domized pca algorithms with regret bounds that arelogarithmic in the dimension.
In Neural InformationProcessing Systems, pages 1481?1488.152
