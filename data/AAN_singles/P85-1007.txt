Speech Acts and RationalityPhilip R. CohenArtificial Intelligence CenterSRI InternationalandCenter for the Study of Language and InformationStanford UniversityHector J. LevesqueDepartment of Computer ScienceUniversity of Toronto"1 Abst rac tThis pallet derives the ha.sis of a theory, of communication froma formal theov,.'
of rational interaction.
The major result is a<h, mon~t fallen t hat.
i lh,c,t ionary acts need not I)e primitive, and.ee, I uot he reco~'nized..\s a t,'st case.
we derive Searle's con-dit ions on reqt,est in~ from pri,ciples of ralionality coupled witha ~;ric~,an theory of iml~erativ,.s.
The theory is shown to dis-tingui.~h insincere or nonserious imperatives from tr~le requests.\['~xlensions to indirect .~peech acts.
and ramifications for naturallanguage ~ystcms are also brieily discussed.2 Introduction'\]'he tlnifyin~ tilt'me of m,wh c-trent pragmatics antl discoursere~earrh is that the c.herence .
f  dialogue is to he folnnd in tileiuleraclinn of the cottver~alll'~' 1~61rI.I.
Thal is, a speaker is re-garded a~s planning his ,lllcrance,~ re achieve his goals, whichn,ay involve in{h..lwing a hean'r by the ,,se of comm,micativeor "speech" acts.
(-)u receiving an lltler~tnce realizing such anaction, the hearer altempls Io infer the ~peaker's goal(s) anti toqndeffland how the 11llerat|rv fnrthcrs them.
The hearer thenadopts new goals (e.~.. to re-pond to a reqllest, to clarify the pre-vious ~peaker'~ lllll'r~ince or ~.
:f,al) and plan~ his r~wn utterancesto acl,ie:'e those.
:\ cotl,cel'?~alion enslle~, IThis view of language a.~ p.rposefid art ion has pervaded ('om-putational I,inzui-~ics re~carch, and ha.~ re~,lted in numerousprotoCyl~e systems \[I, 2, 3..',.
9.
25, 27\].
llowever, the formalfoundations underlying 01n... %v~l.ems haw" heen unspecified or.nder~peril'ied.
In this ,.late ,~\[' affairs, one cannot characterizewhat a ~,y.
',tem .~llould ih~ independently from what it does.This paper hl,gins to rectify this sit-ation by presenting afl~rmalizalinn of rational interaction.
~pon which is erected tilehe~itmin~'- r,f a theory of rein m~miralion attd ~peech acts.
Inter-;wtion is d~.riv~,d fr~,m prmcil~h,.~ of rational action for indivi,h,ala~enas.
~.. well as lwinciph's - \ [  helief and goal adoption amonga~enls.
The h~sis of a theory nf purposefi,l communication thus"F, ll~,w ,,I' th~ Canadian lr, sti~,~t~- f~)r A,'.wanc~d R-search.~This re~,.areh was mad-  W,~sdde ;n part hy a gilt from ~he Systems Dew.l-opm~.n~ \["~.md:~ti,,n.
and in part t,y suFport fr-m ti~e r)efens~ AdvancedR~.se~rrh \['roje.rts .Ag,ncy un.h'r C,~n~ra.ct Nf~I)t)3D.8.I-K-0078 wilh the.N~v:~| \['~lec~ronic Systems C,,mm~nd.
The views and om?lusions eon-tain~'d in thls document ~re ~hos~" of the ~uthor~ and should not be inter-preted ;~ representa, tive of the.
omci~.| policies, ~ither expre~ed or implied,oi" the Defense ~dvanced Research Projects Agency or the United States(Jovernment.
Mu~h nf this rrsearrh was done when the second a.uthorwa~ employed at the Falrehild ( '~m,r~ and Instrument Corp.emerges as a consequence of principles of action.2.1 Speech Act  TheorySpeech act theory was originally conceived a~s part of action the-ory.
Many of Austin's \[.l\] insights about the nature of ~peechacts, felicity conditions, anti modes of lath,re apply equally wellto non-communicative actions.
Searle \[2G\] repeatedly mentionslhat many of the conditions he attributes to variol,s illocution-ary acls (such as requests anti qm,stions) apply more ~e:.,rallyto non-communicative action.
\]lowever, re~earcher~ have ~rad-ually lost ~ight of their roots.
In recent work \[3~ I illoc,ltior,a~"acts are formalized, antl a logic is proposed, in which propertie~of IA's (e.g., "preparatory conditions" and "mode~ of achieve-ment ' )  are primitively st ip.
laled, rather than derived front moreh~ic principles of action.
We helieve this approach misses sig-nificant generalities.
"\['hm paper ~hows how to derive propertiesof i l locutionary acts from principh,s of rationality, .pdat ing theformalism of \[10J.Work in Artificial Intelligence provided the first forntalgro.nding of speech act theory in terms of plannin~ and planrerog~nitmn, cldminalin~ in Perra.h and \lh.n'~ \[:2:~ I I I .
.
.
ry ofindirect speech acts.
Xhwh ~,I" o~0r re~earch i~.
in.~lfir~'d I,~ lhr i ranalyses, llowe~er, one major ingredien!
~I" their the.ry r:m beshown to he redundant in01 i l locutionary acts.
All do.
in-ferential power nf the recolfnition of their dloc~itionary acts wa.salready available in other "operators'.
Nevertheless, the natu-ral langlnage systems based on this approach \[I. ,-3\] always hadto recognize which i l locutionary act was performed in order torespond to a tnser's utterance.
Since the il locutionary acts wereunnecessary for achieving their ell'errs, so too wa.~ their re~'n~ni-tion.The stance that i l locutionary arts are not primitive, and neednot he re;og'nize(l, is a lih..ratmg one.
()nee taken, it l)ecomesapparent that many of the (lifl~cuhies in applying ~l),,ech acttheory to discourse, or to computer systems, stem from takingthese acts too seriously - i.e., too primitively.3 Form of the argumentWe show that illocutionary acts need not be primitive hy de-riving Searle's conditions on requesting from an independently-motivated theory of action.
The realm of communicative actionis entered following Grice \[13i - -  by postulating a correlationbetween the ,ntterance of a sentence with a certain syntactic fea-ture (e.g., its dominant clause is an imperative) and a complex49propositional att itude expressing the speaker's goal.
This atti-tude becomes true as a result of uttering a sentence with thatfeature.
Because of certain general principles governing beliefsand goals, other causal consequences of the speaker's having theexpressed goal can be derived.
Such derivations will be "summa-rized" as lemmas of the form "If (conditions) are true, then anyaction making (antecedent) rue also makes (consequent) rue \ ]These lemmas will be used to characterize illocutionary acts.though they are not themselves acts.
For example, the lemmacalled REQUEST will characterize a derivation that shows howa heater's knowing that the speaker has certain goals can causethe hearer to act.
The conditions licensing that chain will be col-lected in the REQUEST lemma, and will be shown to subsumethose stipulated by Searle \[261 as felicity conditions.
However,they have been derived here from first principles, and withoutthe need for a primitive action of requesting.The benefits of this approach become clearer as other illocu-tionary arts are derived.
We have derived a characterizationof the speech act of informing, and have used it in derivingthe speech act of questioning.
The latter derivation also allowsus to disting~tish real questions from teacher/student questions,and rhetorical questions.
However.
for brevity, the discussion ofthe.,e speech acts has been omitted.Indirect speech acts can be handled within the framework.although, again, we cannot present the analyses here.
Briefly,axioms similar to those of Perrauh and Allen {22\] can be sup-plied enabling one to reason that an agent has a goal that q,~iven that he also has a goal p. When the p's and q's are them-selves goals of the hearer (i.e.. the speaker is trying to get thehearer to do something), then we can derive a set of lemmas fori,, l irect requests.
Many of these indirect request lemmas corre-spond to what have been called %herr-circuited" implicatures.which, it was suggested \[211 underlie the processing of utterancesof the form "Can you do X?'
.
"Do you know y?
", etc.
l ,emmaformation and lemma application thus provide a familiar modelof-herr-circuit ing.
Furthermore.
this approach shows how oneran use general purpose reasoning in concert with convention-alized b~rms (e.g., how one can reason that "Can you reach thesalt" is a request to pass the salt), a problem that has plagnwdmost theories of speech acts.The plan for the paper is to construct a formalism based ona theory of action that is sufficient for characterizing a request.Most of the work is in the theory of action, as it should be.4 The Formal ismTo achieve these goals we need a carefl:lly worked out (thoughperhaps, incomplete) theory of rational action and interaction."!
'he theory wil~ be expressed in a logic whose mndet theory isba.,ed (loosely) on a possible-worlds semantics.
We shall proposea logic with four primary modal operators -- BELief, BMB,~,f)AL.
and AFTER.
W~th these, we shall characterize whatagents need to know to perform actions that art, intended toachieve their ~oals.
The .zgents do so with Ihe knowledge thatother agents operate similarly.
Thus, agents have beliefs about.
'her '~ gcals, and they have goals to influence others' beliefsand goals.
The integration of these operators follows that ofMoore {20l, who analyzes how an agent's knowledge affects andis affected by his actions, by meshing a possible-worlds modelof knowledge with a situation calculus model of action \[18\].
Byadding GOAL,  we can begin to talk about an agent's plans,which can include his plans to influence the beliefs and goals ofothers.Intuitively, a model for these operators includes courses ofevents (i.e., sequences of primitive acts) " that characterize whathas happened.
Courses of events (O.B.e.
's) are paths through atree of possible future primitive acts, and after any primitive acthas occurred, one can recover the course of events that led upto it.
C.o.e.
's can also be related to one another via accessiblityrelations that partake in the semantics of BEL  and GOAL.
Fur-ther details of this semantics must await our forthcoming paper\[17\].As a general strategy, the formalism will be too strong.
First,we have the usual consequential closure problems that plaguepossible-worlds models for belief.
These, however, will be ac-cepted for the time being.
Second, the formalism will describeagents as satisfying certain properties that might generally hetrue, but for which there might be exceptions.
Perhaps a processof non-monotonic reasoning could smooth over the exceptions,but we will not attempt o specify such reasoning here.
Instead,we assemble a set of basic principles and examine their conse-quences for speech act use.
Third, we are willing to live with thedifficulties of the situation calculus model of action - e.g., thelack of a way to capture tnse parallelism, and the frame prob-lem.
Finally.
the formalism should be regarded as a de,~eriptionor specification Bran agent, rather than one that any agent couldor should use.Our approach will be to ground a theory of communication ia theory of rational interaction, itself suppor ted  by a theory, ofrational action, which is finally grounded in mental states.
Ac-cordingly, we first need to describe the_behavior of BEL, BMB.GOAL and AFTER.
Then, these operators will be combinedto describe how agents' goals and plans influence their actions.Then.
we characterize how having beliefs about the beliefs andgoals of othe~ can affect one's own beliefs and goals.
Finally,we characterize a request.To be more spe~iflc, here are the primitives that will be used,with a minimal explanation.4,1 P r imi t ivesAssume p, q, .
.
.
are schema variables ranging over wffs, anda, b ?
?
are schematic variables ranging over acts.
Then thefollowing are wlfs.4.1.1 tVffs~p{p v q}(AFTEI'~.
a p} - p is true in all courses of events that obt,-,in fromact a's happening';, (if a denotes a halting act).(DONI:'.
a) - The event denoted by a has just happened.
(AGTa x) - Agent x i s theon ly  agent of act aa ~ b -- Art a I)r~cedes act b in the current course of events.3 z p ,~here p contains a free occurrence of variable z.x-~.yTrue.
False(BEL x p) - p foUows from X'S beliefs.
{~OAL x p) -- p fotlotps from x's goals.
{BMB x y p} .- p/~llows from x's beliefs about what is mutuallybelieved by x and y.:P'w chls paper, the only events that will be considered &re primitive acts.3Th&t is.
p is true in ~.11 c.o e.'s resulting from concatenating the currentc .o .e ,  with the c.o.e, denoted by a.504.1.2 Act ion  Format ionIf a, b, c, d range over sequences of primitive acts, and p is awff.
then the following are complex act descriptions:a:b - -  sequential actiona \[ b - -  non-deterministic hoice (a or b) actionp?
- -  action of positively testing p.def (IF p a b) - -  conditional action = (p?
:a)  1 (~pT;b) ,  as in dy-namic logic.
(UNTIL  p a) - -  iterative action d*~ (~p:a) ' ;~p?
(again, as indynamic logic).The recta-symbol "1-' will prefix formulas that are theorems,i.e.. that are derivable.
Properties of the formal system that willbe assumed to hold will be termed Propositions.
Propositionswill be both formulas that should always be valid, for our forth-coming ~emantics, and rules of inference that should be sound.No attempt o prove or validate these propositions here, but wedo so in It 7\].4 .2  P roper t ies  o f  Ac tsWe adop!
,In' ,Isual axioms characterizing how complex actionsbehave .mh ' r  AFTER,  a.s treated in a dynamic logic (e.g., \[20\])namely,Propos i t ion  t Propert*es o /complez  aet~ --~(AFTER(AFTER(AFTERAFTER atttlties:Propos i t ionP ropos i t ionP ropos i t ionP ropos i t lonP ropos i t iona:b p) --- (AFTER a (AFTER b p)).a\]b p) -= (AFTER a p) ^ (AFTER b p).p't q) -= p ^ q.DONE will have ~he fol lowing addit ional  proper.2 V act (AFTER act (DONE x act)) 4$ Va \[{DONE (AFTER a p)?
:a) ~ p\]4 \[lb.
~D,q then(DONE ~?
:a) :~ (DONE ,')?
;a),5 p -= {DONE p?
}6 (DONE \[(p 3 q) ^ p\]?}
.~ (DONE q?
)Our t reatment of acts requires that we deal somehow with the"frame problem" \[18\].
That is, we must characterize not onlywhat changes as a resuh of doing an action, but also what doesnot change.
To approach this problem, the following notationwill he convenient:Def in i t ion  t (PRESERVES a p) d.f P ~ (AFTER a p)Of co.rse, all theorems are preserved.Temporal concepts are introduced wil l  DONE (for past hap-penings) and <> (read "eventually'}.
To say that p was true at~(,me point in the past, we use 3a (DONE p?:a).
<> is to heregarded in the "branching time* sense \[I 1\], and will be definedmore rigorously in !17\].
Essentially, OP  is true iff for all infiniteextensions of any course of events there is a finite prefix satis-fying p. OP and O~p are jointly satisfiable.
Since OP  starts"now ", the following property is also true,* (AFTER t (DONE t ) ) ,  where t is term denoting a primitive act(or a sequence of primitive actsl, is ant always true since aft ;~t '~aychange the values of terms (e.g., an election changes the value of the term(PRESIDENT U.S.))Propos i t ion  7 t- p 30PAlso, we have the following rule of inference:P ropos i t ion  8 I / I -  a ~ fl then O(a  v p) ~ O(3  v p)4 .3  The  At t i tudesNeither BEL ,  BMB.
nor GOAL characterize what an agentactively believes, mutually believes (with someone lse), or hasas a goal, but rather what is imph'cit in his beliefs, mutual be-liefs, and goals, s That is, these operators characterize whatthe world would be like if the agent's beliefs and mutlml beliefswere true, and if his goals were made true.
Importantly.
wedo not inch,de an operator  for wanting, since desire~ m,ed nothe consistent.
We ass.me that once an agent has sorted o~lthis possibly inconsistent desires in deciding what he wishes toachieve, the worhls he will he str iv ing for are consisteal.
~'on-versely recognit ion of an agent's plans n,'ed not, com, ider thatagent's possibly inconsistent desires.
F,zrthermore.
there is al~ono explicit operator  for intending.
If an agent intends to bringabout p, the agent is usually regarded as also being able to bringabout p. By using GOAL,  we will be able to reason about theend state the agent is aiming at separately from our reasoningabout Iris ability to achieve that state.For simplicity, we assume the usual Hintikka axiom schematafor BEL \[I,SI, and we introduce KNOW by definition:Def in i t ion  2 (KNOW x p) ~f p ^ (BEL x p)4.3.1 Mutua l  Be l ie fHuman communicat ion depends crucial ly on what is mutual lybelieved \[I, 6, 7, 9, 22, 23, 2.1\].
We do not use the standarddefinitions, but employ (nMB y x p), which stands for y's beliefthat it is mutually believed between y and x that p. (BMB yx p} is true iff (BEL y \[p A (BMD x y p)\]).
~ BMB has thefol lowing properties:Propos i t ion  9 (BMB y x pAq) =- (BMB y x p) A(DMB y x q)Propos i t ion  10 (BMB y x pDq) 3( (BMD y x p) 3 (BMB y x q))Propos i t ion  11 1/I-,~ 3 # then~-(BMB y x ~) :3 (BMB y x J)Also, we characterize mutual knowledge as:Def in i t ion  3 (MK x y p)d.=f P ^ (BMB x y p) ^(BMD y x p)r5For an exploration of the issues involved in explicit vs. implicit belief, seei lel.SNotice that (BMB y x p) $ (BMB x y p).~This definition is not entirely correct, but is adequate for presentpurposes.514.3.2 Goa lsFor GOAL, we have the following properties:P ropos i t ion  12 {GOAL x {GOAL x p)) ~ (GOAL x p)If an agent thinks he has a goal, then he does.P ropos i t ion  13 {BEL x {GOAL x p}} - {GOAL x p}Propos i t ion  14 {GOAL x p} ^  {GOAL x p~q){GOAL x q)8The following two derived rules are also useful:P ropos i t ion  15 I f  i" o D ~ then~'(GOAL x a) D (GOAL x ~)P ropos i t ion  t0  I lk- a A ;1 D "7 thenI-{BMB y x (GOAL x ~)) ^  (BMB y x {GOAL x ~)} :~(BMB y x {GOAL x "~))More properties of GOAL follow.4.4 At t i tudes  and  Rat iona l  Ac t ionNext.
we must characterize how beliefs, goals, and actions arerelated.
"the interaction of BEL anti AFTER will be patternedafter Moore's analysis \['20l.
In particular, we have:P ropos i t ion  IT v x. act (AGT a x) D(AFTER act (KNOW x (DONE act)))Agents know what they have done.
Moreover, they think certaineffects of their own actions are achieved:P ropos i t ion  18 (BEL x {RESULT  x a p)) 3(RESULT x a (BEL x p)).
tvheredef Def in i t ion  4 (RESULT x a p) = (AFTER a p) ^(AGT a x)The major addition we have made is GOAL.
which interactstightly with the other operators.We will say a rational agent only adopts goals that are achiev-able, and accepts as "desirable" those states of the world thatare inevitable.
To characterize inevitabiJities, we haveDef in i t ion  5 (ALWAYS p) 4.~ Va (AFTER a p)This says that no matter  what happens, p is true.
Clearly, wewantP ropos i t ion  19 lf~-r~ then ~- (BEL x (ALWAYS ,~))That is, theorems are believed to be always true.Another property we want is that no sequence of primitiveacts is forever ruled out from happening.P ropos i t ion  20 ~" Va (ACT a) ~ ~(ALWAYS ~(DONE a)),where (ACT a) ~f ~(AFTER a - - (DONE a))One important variant of ALWAYS is (ALWAYS x p) (rel-ative to an agent), which indicates that no matter what thataqent does, p is true.
The definition of this version is:d~f Def in i t ion  6 (ALWAYS x p) = Va {RESULT x a p)A u:~eful instance of ALWAYS Is (ALWAYS pDq) ill which nomatter what happens, p still implies q.
We can now distinguishbetween p :~ q's being logically valid, its being true in all coursesof events, and its merely being true after some event happens.SNotice that it pDq is true (or even believed} but (GOAL x pDq) is nottrue, we should not reach this conclusion since some act could make itlaise.4.4.1 Goals  and Inevi tabi l i t iesWhat an agent believes to be inevitable is a goal (he acceptswhat he cannot change).P ropos i t ion  21 (BEL x {ALWAYS p)) ~ (GOAL x p)and conversely (almost), agents do not adopt goals that theybelieve to be impossible to achieve - -P ropos i t ion  22 No fut i l i ty - -  (GOAL x p)~(BEL x (ALWAYS ~p))This gives the following useful lemma:Lemma I Inevitable Consequences(GOAL x p) A (BEL x (ALWAYS p~q )) D (GOAL x q)Proof: By Proposition 21, if an agent believes pDq is alwaystrue, he has it as a goal.
Hence by Proposition 14, q followsfrom his goals,This lemma states that if one's goal is ac.o.e, in which p holds,and if one thinks that no matter what happens, pDq, then one'sgoal is a c.o.e, in which q holds.
Two aspects of this propertyare crucially important  o its plausibility.
First, one must keepin mind the "follows from* interpretation of our propositionalattitudes.
Second, the key aspect of the connection betweenp and q is that no one can achieve p without achieving q. Ifsomeone could do so, then q need not be true in a c.o.e, thatsatisfies the agent's goals.Now, we have the following as a lemma that will be used inthe speech act derivations:Lemma 2 Shared Recoqnition(BMB y x {GOAL x p)} A(BMB y x (BEL x (ALWAYS p~q)) )  3(BMB y x (GOAL x q))The proof is a straightforward application of Lemma I andPropositions 9 and 10.4.4.2 Pers i s tent  goalsIn this formalism, we are attempting to capture a number ofproperties of what might be called "intention" without postu-lating a primitive concept for "intend".
Instead, we will combineacts, beqiefs, goals, and a notion of commitment built out of moreprimitive notions.To capture ,me grade of commitment han an agent mighthave towards his goals, we define a persistent goal.
P -GOAL,to be one that the agent will not give up until he thinks it hasbeen an:(stied, or until he thinks he cannot achieve it.Now, in order to state constraints on c.o.e.
's we define:d* f  Def in i t ion  T (PREREQ x p q) =Vc (RESULT x ?
q) ~ 3 a (a ~ c) A (RESULT  x a p}This definition states that p is a prerequisite for x's achieving qif all ways for x to bring about q result in a course of events inwhich p has been true.
Now, we are ready for persistent goals:52dlt Def in i t ion  8 (P -GOAL x p) =(GOAL x p) ^\ [PREREQ x ( (BEL  x p) v{BEL  x (ALWAYS x ~p) ) )~(GOAL x p)lPersistent goals are ones the agent will replan to achieve if hisearlier attempts to achieve it fail to do so.
Our definition doesnot say that an agent must give up his goal when he thinks it issatisfied, since goals of maintenance are allowed.
All this says isthat somewhere along the way to giving up the persistent goal,the agent had to think it was true (or belie~,e it was impossiblefor him to achieve).Though an agent may be persistent, he may be foolishly sobeca,se he ha.~ no competence to achieve his goals.
We charac-terize competence below.4.4.3 CompetenceI'e.ple are ~omet imes experls in certain fiehts, as well as in theirown bodily movements.
For example, a competent electricianwill form correct plans to achieve world states in which "elec-trical" .-tares of affairs obtain.
Most aduhs are competent inachievimz worhl states in which their teeth are brushed, etc.We will say an agent is COMPETENT with respect to p if,whenever he thinks p will tnJe after some action happens, he iscorrect:def Def in i t ion  9 (COMPETENT x p} =Va (BEL x (AFTER x p)) 2) (AFTER a p}One property of competence we will want is:Proposition 23 Vx.
a (AGT x a)(ALWAYS (COMPETENT x (DONE x a))), whereDefinltlon I0 (DONE x a) a---'f (DONE a) .
'~ (AGT a x)That is.
any person is always competent to do the acts ofwhich he is the agent.
~ Of course, he is not always competentto achieve any particular effect.Finally.
~iven all these properties we are ready to describerational agents.4 .5  Rat iona l  Agentsi~elow are properties of ideally rational agents who adopt per-~i.~tent gnals.First.
a~ents are carefuh they do not knowingly and deliber-ately make their persistent goals impossible for them achieve.P ropos i t ion  24 (DONE x act) 2) {DONE x p?
;act), wherep %'J (P -GOAL x q) ~ ~(DEL x (AFTER act(ALWAYS x ~p))) v~(COAL x (DONE x act)) l0in other words, no deliberately shooting onessetf in the foot.Now, agents are cautious in adopting" persistent goats, sincethey must eventually come to some decision about their feasi-bility.
We require an agent to either come up with a "plan ~ toSl}ecause of Proposition 2. all Proposition 23 says is that if a competentagen,, believes his own pr imi t iw  act halts, it will.~nNotice *hat tt is eruciad that p be true in ~he sane world in which theagent does act, hence the use ,if "p?
;aet*.achieve them - -  a belief of some act (or act sequence) that itachieves the persistent goal - -  or to believe he cannot bring thegoal about.
That is, agents do not adopt persistent goals theycould never give up.
The next Proposition will characterize thisproperty of P-GOAL.But, even with a correct plan and a persistent goal.
thereis still the possibility that the competent agent never executesthe plan in the right circumstances - -  some other agent haschanged the circumstances, thereby making the plan incorrect.\[f the agent is competent,  hen if he formulates another plan.
itwill be correct for the new circumstances.
But again, the worldcould change out from under him.
Now, just as with operatingsystems, we want to say that the world is "fair" - the agent willeventually get a chance to execl,te his plans.
This property isalso characterized in the following Proposition:P ropos i t ion  25 fa,r  EzecuHon - -  The agent u,dl prentuallyform a plan and ezeeute *t. believing it achieves his persistentgoal in e,rcumstanees he believes to be appropriate for its sucees.~.V x (P -GOAL x q) 2)0 \ [3  act '  (DONE x p?
;act')\]  v\[BEL x (ALWAYS x ~ql\[},where p 4=*?
(nEL  x (RESULT x act '  q))We now give a crucial theorem:Theorem I Consequences of a pers,stent goal -- If .~omeonehas a pers*stent goal of bringing about p,  and brmgm 9 ~l~ut  p isusffhin his area of competence, then eventually either p becomestrue or he wall believe there is nothing that can be done to achiet, eP?
y (P .GOAL y p) A (ALWAYS (COMPETENT y p)) D(> (p v (BEL y (ALWAYS y ~p}))Proof sketch:Since the agent has a persistent goal.
he eventually will eitherfind and execute a plan.
or will believe there is nothing he cando to achieve the goal.
Since he is competent with respect to p,the plans he forms will be correct.
Since his plan act' is correct,and since any other plans he forms for bringing about p are alsocorrect, and since the world is "fair', eventually either the agt,ntexecutes his correct plan, making p true, or the agent comes tobelieve he cannot achieve p. A more rigorous proof can be foundin the Appendix.This theorem is a major cornerstone of the formalism, tellingus when we can conclude  p ,  given a plan and a ~oal.
and isused throughout he speech act analyses.
\[f an agent who is notCOMPETENT with respect to p adopts p a.s a persistent goal,we cannot conclude that eventually either p will be true (or theagent will think he cannot bring it about), since the agent couldforever create incorrect plans.
\[f the goal is not persistent, wealso cannot conclude OP  since the agent could give it up withoutachieving it.The use of ~ opens the formalism to McDermott's "LittleNell* paradox \[19l.
tt In our context, the problem arises asfollows: First, since an agent has a persistent goal to achieve p,~lLittle Nell is tied to the railroad tracks, and will be muhed by the neXttrain.
Dudley Doright is planning to save her.
McDermott claims that,according to various A\[ theories of planning, he never will, even thoughhe always knows just what to do.53and we assume here he is always competent with respect o p,~p is true.
But, when p is of the form Oq (eg., <>(SAVEDLITTLE-NELL)), <><>q is true, so <>q is true ~ well.
Let usassume the agent knows all this.
Hence, by the definition ofP-GOAL, one might expect the agent to give up his persistentgoal that <>q, since it is already satisfied!On the other hand, it would appear that Proposition 25 issufficient o prevent he agent from giving up his goal too soon,since it states that the agent with a persistent goal must act onit, and, moreover, the definition of P-GOAL does not require theagent to give up his goal immediately.
For persistent goals toachieve <>q.
within someone's cope of competence, one mightthink the agent need "only" maintain <>q as a goal, and thenthe other properties of rationality force the agent to perform aprimitive act.Unfortunately, the properties given so far do not yet rule outLittle Nell's being mashed, and for two reasons.
First, NILdenotes a primitive act -- the empty sequence, llence, doing itwould satisfy Proposition 25, but the agent never does anythingsubstantive.
Second, doing anything that does not affect q alsosatisfies Proposition 25, since after doing the unrelated act, <>qis still true.
We need to say that the agent eventually acts on q!To do so, we have the following property:P ropos i t ion  26 (P-GOAL y Oq) 3O\[ (P-GOAL y q) v(rtgL y (ALWAYS y ~q)) \ ] ,That is.
eventually the agent will have the persistent goal thatq, and by Proposif ion 25. will act on it.
If he eventually comes tobelieve he cannot bring about q, he eventually comes to believehe cannot bring about eventually q as well, allowing him to giveup his persistent goal that eventually q.4.6 Rat iona l  In teract ionThis ends our discussion of single agents.
We now need to char-acterize rational interaction sufficiently to handle a simple re-qt,?st.
First, we ,.liscuss cooperative agents, and then the effectsof uttering sentences.4.6.1 Proper t ies  of Cooperat ive  AgentsWe describe agents as sincere, helpful, and more knowledgeablethan others about the t~lth of some ~tate of affairs.
Essentially,O.,,~e concepts capture (quite ~iml)li,qic) constraints on influegc-ing ~omeone clse's beliefs and goals, and on adopting the beliefsand goal~ of someone lse ~ one'~ own.
More refined versionsare certainly desirable.
Ultimately.
we expect such properties ofcooperative agents, a.s embedded in a theory of rational inter-action, to provide a formal description of the kinds of conver-sational behavior ~rice \[1-t\[ describes with his "conversationalm;Lxims".First, we will say an agcnt i~ S INCERE with respect to p ifwhenever his goal is to get someone else to belietpe p, his goal isin fact to get that person to knom p.dec Definit ion t l  (SINCERE x p) =(GOAL x (laEL y p)) D (GOAL x (KNOW y p))An agent is HELPFUL to another if he adopts as his ownpersistent goal another agent's goal that he eventually do some-thing (provided that potential goal does not conflict with hisown I.Defini t ion 12 (HELPFUL x y) a,?='Ca (BEL x (GOAL y (}(DONE y a))) ^~(GOAL x ~(DONE x a)) D(P -GOAL x (DONE x a))Agent x thinks agent y is more EXPERT about the true of pthan x if he always adopts x's beliefs about p as his own.def Def in i t ion 13 (EXPERT y x p) :(BEL x (BEL y p) )  :3 (BEL x p)4.0.2 Ut ter ing  Sentences  with  Cer ta in  aFeatures"Finally, we need to describe the effects of uttering sentences withcertain "features" \[141, such an mood.
In particular, we needto characterize the results of uttering imperative, interrogative,and declarative sentences t: Our descriptions of these effectswill be similar to Grices's \[131 and to Perrauh and Allen's {22\]%urface speech acts'.
Many times, these sentence forms are notused literally to perform the corresponding speech acts (requests,questions, and assertions).The following is used to characterize uttering an  imperative:P ropos i t ion  27 Imperatives:V x y (MK x y (ATTEND y x}) 3(RESULT x \[IN4PER x y "do y act" 1(laMB y x(GOAL x(BEL y(GOAL x(P-GOAL y (DONE y act))))))))The ac: !
IMPER speaker hearer 'p\] stands for "make p t r~w"Proposition 27 states that if it is mutually known that y is at-tending to x, is then tile result of uttering an imperative to yto make it the case that y has done action act is that y thinksit is mutitally believed that the speaker*s goal is that y shouldthink his goal is foe y to form the persistent goal of doing act.We also need to a~sert hat IMPER preserves incerity aboutthe speak,'r's coals and helpfulness.
These restrictions c,~uld beloosened, but maintaining them is simpler.P ropos i t ion  28 {PRESERVES \ [ IMPER x y "do y act ' \ ](BMB y x (S INCERE y (GOAL y p))))Propos i t ion  29 (PRESERVES \ [ IMPER x y "rio y ;Jet'\](HELPFUL y xt)All t ',ricean "feature'-based theories of communication needto acco,mt for cases in which a speaker uses an utterance with afeat'tre, but does not have the attitudes (e.g.. beliefs, and goals)'2llowever, #e can only present the analysis of imperatives here.tall it is not mutually known that y is attending, for example, if the speakeri~ not speaking to an ~udience, then we do not say what the result ofuttering an imperative is.54usually attr ibuted to someone uttering sentences with that fea-ture.
Thus, the attribution of the attitudes needs to be context-dependent.
Specifically, proposition 28 needs to be weak enoughto prevent nonserious utterances uch as "go jump in the lake ~from being automatically interpreted as requests even thoughthe utterance is an imperative.
On the other hand, the formulamust be strong enough that requests are derivable.5 Der iv ing a S imple  RequestIn making a request, the speaker is trying to get the hearer to doan act.
We will show how the speaker's uttering an imperativeto do the act leads to its eventually being done.
What we needto prove is this:Theorem 2 Result o\[ an Imperative --(DONE \[(MK x y (ATTEND y x)) ^(BMB y x(S INCERE x(GOAL x(P -GOAL y (DONE y act) ) ) ) )^(HELPFUL  y x)l?
;l IMPER x y "do y ac t ' \ ] )  :3O(DONE y act)We will give the major steps of the proof in Fi~lre I, andpoint In their justifications.
The full-fled~'ed proofs are h'ft toIhe ,,nergetic reader.
All formula.s preceded by a * are supposedt,, be Irue just prior to performing the IMPER,  are preserved byil.
an,I thus are implicitly conjoined to formulas 2 - 9.
By theirplacement in the proof, we indicate where they are necessary formaking t he deductions.E~entially.
the proof proceeds as follows:If it is mutually known that y is attending to x. and y thinks iti~ mutually believed Ihat Ihe e-conditions hohl.
then x's ,llteringan imlwrative to y to do some action results in formula (2).
Sinceh i~ mutually believed x is sincere about his goals, then (:~) it ismiltually believed his goal tndy is that y form a persistent goalto ,Io the act.
Since everyone is always competent to do acts ofwhich they are the agent.
(.1) it is mutltally believed that the actwill eventually be done, or y will think it is forever impossibleto do.
But since no halting act is forever impossible to do, itis (.3) mutually believed that x's goal is that y eventually do it.Ih, nee, 16) y thinks x's ~oa\] is that y eventually do the act.
Now,~ince y is helpfillly disposed towards x, and has no objections Iodoing the act.
17) y takes it on as a persistent goal.
Since heis alwa.w competent about doing his own arts, 18) eventually it~ill I , .
, Ione or he will think it impossible to do.
Again.
since itis n(,I f~)rever impossible.
(3) he v, ill eventually do it.W,.
have shown how the p,.rforming of an imperative to doan act leads to the act's evemually being done.
We wish tocreate a number of lemmas from this proof (and others like it)to characterize iilocutionary acts.8 P lans  and Summar ies6.
t PlansA plan for agent "x" to achieve some goal "q" is an action term~a" and two sequences of wits ".no', ~P l " .
.
.
.
"pt," and "q0","qz", .
.
.
~qk" where "qk" is ~q" and satisfyingI .
I- (BEL x (poApt  A .
.
.Ap~}(RESULTxa  qoaptA .
.
.APk  )))2. h (BEL x (ALWAYS (p~a Ch-t) D q,))) i=l ,e. .
.
.kIn other words, given a state where "x" believes the "pi ~, hewill believe that if he does ~a" then "q0" will hold and moreover.given that the act preserves pi, and he believes his making "qi-i ~true in the presence ofp i  will also make "qi* tale.
Consequently,a plan is a special kind of proof thatI- (BEL x ((Po^.-.
A Pk) ~ (RESULT x a q)))and therefore, since(BEL x p) D (BEL x (BEL x p))and(BEL x (p ~ q)) D ((BEL x p) D (BEL x q)).
are axioms ofbelief, a plan is a proof thath (BEL x (p. A .
.
.^ p~)) ~ (BEL x (RESULT x a 'l))Among tile corollaries to a plan are}- (BEL x ( (Po a .
.
.
^ p,) ~ (RESULT x a q , ) ) )  i=\[  .
.
.
.
kand}- (BEL  x ( (p," a .
.
.
a Pi) ~ (ALWAYS q~-i D q i ) ) )i :  1 .
.
.
.
k \]=l" .
.
.
.
kThere are two main points to be made about the~e corollaries.First of all, since they are theorems, the implications can betaken to be believed by the agent "x" in every, state.
In thissense, these wits express general methods believed to achievecertain effects provided the assumptions are satisfied.
The sec-ond point is that these corollaries are in precisely the form thatis required in a plan and therefore can be used as justification fora step in a filture plan in much the same way a lemma becomesa single step in the proof of a theorem.6.2  Summar iesWe therefore propose a notation for describing many ~t,.p~ ofa plan as a single summarizing operator.
A 3ummary consistsof a name, a list of free variables, a distingafished free variablecalled the agent of the summary (who will always be list,,d tirst),an Effect which is a wff, a optional Body which is either anaction or a wff and finally, an optional Gate which is a wff.
Theunderstanding here is that summaries are associated with agentand for an agent "x" to have summary "u".
then there are threecases depending on the body of "u' :I.
If the Bodyof  "u" is a wff, then(BEL x (ALWAYS (Gate ^  Bod~) ~ {Gate ^  Effect})) Is2.
If the Body of "u" is an action term, thenI- (BEL  x (Gate ~ (RESULT agent Bod~ (Gate A Effect}))):60f course, many actions change the truth of their preconditions.
H~ndllngsuch actions and preconditions i$ straightforward.55I.2.GivenP27, P3, P4, I3.
P l l ,  P12, 2(DONE \[(MK x y (ATTEND y x)) A( .condit ions) \ ]?
;l IMPER x y "do y act*\])(BMB y x (GOAL x (BEL y (GOAL x(P-GOAL y (DONE y act))))))  A*(BMB y x (S INCERE x(GOAL x (P-GOAL y (DONE y act)))))(BMB y x (GOAL x (P-GOAL y (DONE y act))))  ^*(BMB y x (ALWAYS(COMPETENT y (DONE y act)))}(BMB y x (GOAL x O\[ (DONE y act) v 4.
(BEL y (ALWAYS ~(DONE y aet))) l))  ^  TX, P l f ,  3, (BMB y x ~(ALWAYS ~(DONE y act)))5.
(BMB y x (GOAL x O(DONE y act))) A P160 P20, P8, 46.
(BEL y x (GOAL x O(DONE y act)))  ^  Def.
BMB(HELPFUL y x)T. (P-GOAL y x (DONE y act))  ^  Def.
of HELPFUL,  MP?
(ALWAYS (COMPETENT y (DONE y act)))8.
<>\[(DONE y act) v (BEL y (ALWAYS ~(DONE y act)})l  ^  T1?
~(ALWAYS ~(DONE y act))9.
<>(DONE y act} P20, P8Q.E.D.Figure 1: Proof of Theorem 2 -- An imperative to do an act result~ in its eventually bein 9 done.
14One thing worth noting about summaries i that normally thewiTs used above~" (BEL x (Ga:e D .
.
.
))will follow from the more general wffI- (;ate D .
.
.llowever, this need not be the ca,~e and different agents couldhave different summaries (even with the same name).
Sayingthat an agent has a summary is no more than a convenientway of saying that the agent always believes an implication of acertain kind.7 Summar izat ion  of  a RequestThe following is a summary named REQUEST that capturessteps 2 through steps 5 of the proof of Theorem 2.\ [REQUEST x y act\]:Gate: i t )  (BMB y x (SINCERE x (GOAL x(P-GOAL y (DONE y act)))))  ^(2) (BMB y x (ALWAYS(COMPETENT y (DONE y act))))(3) (BMB y x ~(ALWAYS ~(DONE y act)))Bo~i~.
(BMB y x(GOAL x(BEL y(GOAL x (P-GOAL y {DONE y act))))})Effect: (BMB y x (GOAL x O(DONE y act)))This summary allows us to conclude that any action preserv-ing the Gate and making the Bod!/true makes the Effect true.Conditions (2) and (3) are theorems and hence are always pre-served.
Condition (1) was preserved by assumption.Searle's conditions for requesting are captured by the above.Specifically, his "propositional content" condition, which statesthat one requests a future act, is present as the Effect becauseof Theorem 2.
Searle's first "preparatory" condition -- that thehearer be able to do the requested act, and that the speakerthink so is satisfied by condition (2).
Searle's second prepara*tory condition - -  that it not be obvious that the hearer wasgoing to do the act anyway --  is captured by our conditions onpersistence, which state when an agent can give up a persistentgoal, that is not one of maintenance, when it has been satisfied.Grice's "recognition of intent* condition \[12, 13\] is satisfiedsince the endpoint in the chain (step 9) is a goal.
Hence, thespeaker's goal is to get the hearer to do the act hy means, inpart, of the (mutual) recognition that the speaker's goal is toget the hearer to do it.
Thus, according to Grice, the speakerhas meant,,,, that the hearer should do the act.
Searle's revisedGricean condition, that the hearer should "understand" the lit-eral meaning of the utterance, and what illocutionary act theutterance "counts as* are also satisfied, provided the summaryis mutually known, leT.
1 Nonser ious  RequestsTwo questions now arise.
First, is this not overly complicated?The answer, perhaps surprisingly, is "No'.
By applying thisREQUEST theorem, we can prove that the utterance of an im-perative in the circumstances specified by the Gate results inthe Effect, which is as simple a propositional attitude as anyonewould propose for the effect of uttering an imperative - -  namelythat it is mutually believed that the speaker's goal is that thehearer eventually do the act.
The Bod V need never be considered16~'he further elaboration of this point that it deserves is outside the ~cope.
ot this paper.56unless one of the gating conditions fails.Then, if the Body is rarely needed, when is the "extra" em-bedding (GOAL speaker (BEL hearer ...)} attitude of use?The answer is that these embeddings are essential to preventingnonserious or insincere imperatives from being interpreted un-conditionally as requests.
In demonstrating this, we will showhow Searle's "Sincerity ~condition is captured by our SINCEREpredicate.The formula (SINCERE speaker p) is false when the speakerdoes something to get the hearer to believe he, the speaker, hasthe goal of the bearer's believing p, when he in fact does not havethe goal of the heater's knowing that p Let us see see how thiswould he applied for "Go jump in the lake', uttered idiomati-cally.
Notice that it could be uttered and meant as a request,and we should be able to capture the distinction between seriousand nonserious uses.
In the case of uttering this imperative, thecontent of SINCERE.
p p =( ( :OAL  speaker (P-GOAL hearer(DONE hearer / JUMP- INTO Laker\]))).Assume that it is mutually known/believed that the lake isfrigidly cold (any other conditions leading to -,.
{GOAL x p)would do as well.
e.g., that the hearer is wearing his best suit,or that there is no lake around).
So, by a reasonable axiom ofgoal formation, no one has goals to achieve states of affairs thatare objectionable (assume what is "objectionable" involves aweighing of alternatives).
~o, it is mutually known/believed that~(GOAL speaker (DONE hearer \[ JUMP-INTO Laket\])), andso the speaker does not believe he has such a goal.
l'l Theconsequent to the implication defining SINCERE is false, andbecause tile result of tile imperative is a mutual belief that thespeaker's goal is that the hearer think he has the goal of thebearer's jumping into the lake, the antecedent of the implica-tion is true.
Hence, the speaker is insincere or not serious, anda request interpretation is blocked, isIn the case of there not being a lake around, the speaker's goalcannot be that the hearer form the persistent goal of jumpingin some non-existent lake.
since by the 3/0 Futility property, thehearer will not adopt a goal if it is unachievable, and hence thespeaker will not form his g~al to achieve the unachievable state ofaffairs (that the hearer adopt a goal he cannot achieve).
}tence,since all this is mutually believed, using the same argument, hespeaker must be insincere.8 Nonspecific requestsThe ability conditions for requests are particularly simple, sinceas long as the hearer knows what action the speaker is referringto.
he can always do it.
He cannot, however, always bring aboutsome goal world.
An important variation of requesting is one inwhich the speaker does not specify the act to be performed; hemerely expresses his goal that some p be made true.
This will becaptured by the action l IMPER y 'p\] for ~make p true*.
Here,tTThe speaker's expressed goat is that the hearer form t persistent goldto jump in the lake.
But.
by the /neeitails Coassqasaees l mma, giventhat a c.o.e, satisfying the speaker's goal also hu the heater's eventuallyjumping in (since the hearer knows what to do), the speaker's goal is also?
c.o.e, in which the hearer eventually jumps in.
In the same way, thespeaker's goal would also be that the hearer eventually gets wet.I*11owever, we do not say what else might be derivable.
The speaker's truegoals may have more to do with the manner of his action (e.g., tone ofvoice), than with the content.
All we have done is demoasnurata formallyhow ?
hearer could determine the utterance is not to be talteo ~r, facevalue.in planning this act, the speaker need only believe the hearerthinks it is mutually believed that it is always the case that thehearer will eventually find a plan to bring about p. Ahhough wecannot present he proof that performing an \[ IMPER x y "p\]will make Op true, the following is the illocutionary summaryof that proof: \ [NONSPECIFIC-REQUEST x y p\]:Gate: (BMB y x (S INCERE x (GOAL x (BEL y(GOAL x (P-GOAL y p) ) ) ) ) )  A(BMB y x (ALWAYS (COMPETENT y p)))(BMB y x (ALWAYS~-7 act'  (DONE y q?
;act'),where q ~( (BEL y (RESULT y act' p))))Body:.
(IJMB y x(GOAL x(BEL y(GOAL x (P-GOAL y p)))))Effect: (nMB y x(GOAL x OPt )Since the speaker only asks the hearer to make p true.
theability conditions are that the hearer think it is mutually be-lieved that it is always true that eventually there will be someact such that the hearer believes of it that it achieves p (or hewill believe it is impossible for him to achieve).
The speakerneed not know what act the hearer might choose.9 On summar izat ionJust as mathematicians have the leeway to decide which proofsare useful enough to be named a.s lemmas or theorems, so toodoes the language user.
linguist, computer system, and speechact theoretician have great leeway in deciding which summariesto name and form.
Grounds for making such decisions rangefrom the existence of ilfocutionary verbs in a particular lan-guage, to efficiency.
However.
summaries are flexible - -  theyallow for different languages and different agents to carve upthe same plans differently.
,o Furthermore, a summary formedfor efficiency may not correspond to a verb in the language.Philosophical considerations may enter into how much of aplan to summarize for an illocutionary verb.
For example, mostillocutionary acts are considered successful when the speaker hascommunicated his intentions, not when the intended effect hastaken hold, This acgues for labelling as Effects of summaries in-tended to capture illocutionary acts only formulas that are of theform (BMI3 hearer speaker (GOAL speaker p)), rather thanthose of the form (BMB hearer speaker p) or (BEL hearer p),where p is not a GOAL-dominated formula.
Finally, summariesmay be formed as conversations progress.The same ability to capture varying amounts of a chain ofinference will allow us to deal with muhi-utterance or muhi-agent acts, such as, betting, complying, answering, etc., in whichthere either needs to be more than one act (a successful betr.quires an offer and an acceptance), or one act is defined torequire the presence of another (complying makes sense onlyin the presence of a previous directive).
For example, whereREQUEST captured the chain of inference from step 2 to step5, one called COMPLY could start at 5 and stop at step 9.tSRemember, summaries are actually beliefs of agents, and those beliefsneed oct be shared.57Thus, the notion of characterizing illocutionary acts as lemma-like summaries, i.e., as chains of inference subject to certainconditions, buys us the ability to encapsulate distant inferencesat "one-shot'.9.1 Ramifications for Computational Models ofLanguage UseThe use of these summaries provides a way to prove that variousshort-cuts that a system might take in deriving a speaker's goalsare correct.
Furthermore, the ability to index summaries bytheir Bodies or from the utterance types that could lead to theirapplication (e.g., for utterances of the form "(.
',an you do <X> ~)allows for fast retrieval of a lemma tlmt is likely to result in goalrecognition.
By an appropriate organization of summaries \[5\],a system can attempt to apply the most comprehensive sum-maries first, and if inapplicable, can fall back on less compre-hensive ones, eventuMly relying on first principles of reasoningabout actions.
Thus.
the apparent difficulty of reasoning aboutspeaker-intent can be tamed for tile "short-circuhed ~ cases, butmore general-purpose r asoning can deployed when necessary.IIowever.
the conil)lexities of rea.~oning about others' beliefs andgoals remains.10 Extens ions :  Indi rect ionIndireciion will be modeh'd ill tills framework a.s tile derivation ofpropositions (lUlling with the speaker's goals that are not statedas such by tile initial propositional attitude.
For example, if wecan conchlde from IBMB y x (GOAL x (GOAL y Nil  that(BMB y x (GOAL x (GOAL y 0 q))), where pdoes not entailq, then.
"loosely', we will say an indirect request has been madeby x.
(;iven the properties of O.
(GOAL x p) D (GOAL x <C>P) isa dworcm.
(GOAL x p) an(l ( ( ; ( )At ,  x - l i )  ar~" mutually un-~ati~\[ial)le, hilt (COAL x OP) and (GOAL x O~p)  are jointly~ali~liahh'.
\["(}r examllh ", ( ( ;OAL BILL OHAVE BILL HAM-MERI) ) )  and (GOAL BILL <~(HAVE JOHN HAMMERI ) )could both be part of a description of Bill's plan for John to geta hammer and give it to him.
Such a plan could be triggeredby Bill's merely saying "C, et tile ilammer" in the right circum-stances, such as when Bill is on a ladder plainly holding a nail.
:0 A subsequent paper will demonstrate the conditions underwhich such reasoning is ~ound.I1 Concluding Remarksrhi~ i)alier tia.~ demonstrated tilat all illocutionary acts ne,'dant t),' primitive.
At least some can be derived from more basicpriuciph.s of rational otion, and an account of tile propositionalattitudes affected by the uttering of sentences wittl decl.
'u-ative,interrogative, and imperative moods.
This account satisfies anumber of criteria for a good theory of illocutionary acts.
* Most elements of :he theory are independently motivated.The ~heory of rational action is motivated independentlyfrom any notions of communication.
Similarly, the proper-ties of cooperative agents are also independent of commu-nication.l?Notice thllt molt theoritqt Ot Ipeech gta  would treat the above utteranceu Bed I I direct request.
We do not.The characterization of the result of uttering sentences withcertain syntactic moods is justified by the results we derivefor illocutionary acts.
as well as the results we cannot de-rive (e.g.. we cannot derive a request under conditions ofinsincerity ).Summaries need not correspond to illocutionary verbs in alanguage.
Different languages could capture different partsof the same chain of reasoning, and an agent might haveformed a summary for purposes of efficiency, but that sum-mary need not correspond to any other agent's ummary.The rules of combination of illocutionary acts (character-izing, for example, how mnltiple assertions could consti-tute the performance of a request) are now reduced to nllesfor combining propositional contents and attitudes.
Thus,multi-utterance illocutionary acts can be handled by accu-mulating the speaker's goals expressed in multiple titter-antes, to allow an illocutionary theorem to be applied.Multi-act utterances are also a natural outgrowth of l|liS ap-proach.
There is no rea.~on why one cannot apply mulliplei l locutionary sunlniaries tO tile res0ill of  utlt, r ing a S?'lllen??
'.Those sllmmaries, however, need not ?
'orre~pond Io illoc0ftionary verbs.The theory is naturally extensible to indirection (to lie ar-gued for hi another paper), to other i l loc.tio.ary act, suchu questions, commands, informs, a~sertions, and to tile actof referring \[gl.Finally.
allllougti illocutionary act rerog'nition may h,, ~lricilyunntwcssary, given the complexily of o01r proofs, it is likely tohe loser011.
I'\]~s,.nliallv.
s01etl rec~l~nilhm would ;lillOlill~ to lh(.application (if ill,lc01tl*lnary Sllnlllllries llleort'nl.~ Io di.~cover thespeaker'~ I~(ml(s L12 AcknowledgementsWe wo.ld like to thank Tom Blenko, Ih.rb (:lark, Michul(,eorg,.lr, David I~r~el, Bob Moore, (;(,off .NUli|ierg', Fernan(|o\[)ereira.
flay Penault, .
":,tan Rosenschein, Ivall ~ag, and ,~losheVacdi for valuable dise.ssions.13 References1.
AIh'n..!.
F. A llhin-lla-'~ed atll)roa~'h I(i Sll,,0.ch act rrc.~nh.ion.
"r,.,ctinic:ll I~.,.port 17.1.
Di'p;lrtnit '!
it  of  ('ornpill.
('r ~cil'nce.ll i l ivei~ity ()f 'r,)roiito, January.
ll.)\]'?.).2.
Allen..I.
\[:., Fr i th.
A. M.. <\[" l,il,nan.
I)..I.
ARt ;(iT: TheRochester dialogue system.
Proceedings of the .Vat,..,dConference on Artificial Intelligence, Phtsh,r~h, I),'nn~yl-vanla, 1982.
?I,-70.3.
Appelt, D. Planning Natural Language Utterances to S(itisfyMultiple Goals.
Ph.D. Th.. Stanford University, Stanford,California, December 1981.4.
Austin, J. L .
/ /o l .
to do thinfs ~ith wo,da.
Oxford UniversityPress, London, 1962.585.
Bracbman.
R., Bobrow, R., Cohen, P., Klovatad, J., Wel>-bet, B. L., & Woods, W. A.
Research in natural languageunderstanding.
Technical Report 4274, Bolt Beranek andNewman Inc., August, 1979.6.
Bruce, B. C., & Newman, D. Interacting plans.
CognRieeScience ~, 3, 1978, pp.
195-233.T.
Clark.
H. H., & Marshall, C. Definite reference and mutualknowledge.
In Elements of Discourse Understanding, Aca-demic Press, Joshi, A. K., Sag, !.
A., & Webber, B., Eds.,New York.
1981.8.
Cohen, P. R. The Pragmatics of Referring and the Modalityof Communication.
Computational LinquMtics lO, 2, 198,1,pp.
97-1.16.9.
Cohen, P. R. On Knowin 9 what to Say: Plannin 9 SpeechActs.
Ph.D.
Th., University of Toronto.
Toronto, January1978.
Technical Report No.
118, Dept.
of Computer Sci-ence.lO, ( :ohen, P. R.. & Levesque, II.
J.
Speech Acts and the Rerog-nition of Shared Plans.
Proe.
, \ [  the Third Iliennial Con-ference, Canadian Society for (~omputa!ional Studies of In-telligence, Victoria.
B.
(;., May.
1980, 263-271.I I .
Emerson, E. A., and Ilalpern.
J. Y.
"Sometimes" and "NotNever" Revisited: On Branching versus \[.inear Time.
ACMSympa.~ium on Prinr~ple.~ of t)rt~jrammin9 Lanquaqes, 1983.12.
(;rice.
l{.
I'..\|caning.
Phdo,,ophiral Ret,ietp 66, 1957, pp.377-3,g8.13.
Grice.
II.
1'.
Utterer'.~ Meaning anti Intentions.
t'hilo.~ophi-cal Reriew 63, 2.
1969, pp.
1.17-177.14.
Grice, t|.
P. Logic and conversation.
In t:ole., P. and Margan, J.
\[,., Eds.,Syntaz and Semantics: Speech Acts , Ace.demic Press, New York,1975.15.
Halpern, J. Y., and Moses.
Y. O.
A tluide to the ModalLogics of Knowledge anti Belief.
Pr~.
a/the Ninth Inter.national Joint (;on\[erenre on .4rtl\]ir:al intelligence, J.J( :AI,Los Angeles, ('alif.. Augnst, 1985.Levesque, tlector, J.
A logic of implicit and explicit belief.Proceedings of the National t,'ofl/erence a/ the American As-~ciation for Artificial Intelligence, ~ustin, Texan, 198.1.esque, H. J., & Cohen, P. R. A Simplified I,ogic of In-~eraction.
in preparation18.
McCarthy.
J.. A: Ilayes.
P..1.
~ome Philo~.phical \['rnhlemsfrom the :-;tandpoint of ..\rtifi?ial l.h'lli~,ehce, In 3t?~rhl,eintelh'fence .i American El.~evier, B. Mehzer & D.
Michh'.Eds., New York.
1~;9.I9.
McDermott, D. A temporal ogic for reasoning about pro-cesses and plans.
Cognitive Science ~, 2, 1982, pp.
101-|55.20.
Moore, R. C. Reasoning about Knowledge and Action.Technical Note 191, Artificial Intelligence Center, SR!
In-ternational, October, 1980.21.
Morgan, J. L. Two types o\[ convention in indirect speechacts.
In Syntaz and Semantics, Volume 9: Pragmaties,Academic Press, P. Cole.
Ed., New York, 1978, 261-280.22.
Perrault, C. R.. & Allen, J. F. A Plan-Based Analysis ofIndirect Speech Acts.
American Journal of ComputationalLinguiaticJ 6, 3, 1980, pp.
167-182.25.
Perrauit, C. R., & Cohen, P. R. It's for your own good:A note on inaccurate reference.
In Elements of DiscourseUnderstandin9, Cambridge University Press, Joshi, A., Sag,i., & Webber, B., Eds., Cambridge, Mass., 1981.24.
Schiffer, S. Mctmin 9.
Oxford University Press, London.1972.25.
Schmidt, D. F., Sridharaa, N.S., & Goodson, J. L. Theplan recognition problem: An intersection of artificial intel-ligence ant| psychology.
Artificial Intelligence 10, 1979. pp.45-83.26.
Searle, J. R. Speech acts: ..In essay in the philosophy oflanguage.
(?ambridge University Pre~s, (:ambridge, 1969.27.
Sidner, (',.
1,., Bates, M., lgobrow.
R. ,1., Brachman, R. J.,Cohen, P. R.. Israel, D. J., Wehber.
B.
l,., & Woods, W. A.Research in knowledge representation for natural languageundecstaz,liog.
Annual R,'p~)rt .1785, Bolt, Beranek andNewman Inc., November, 1981.28.
Vanderveken.
I).
A Model-Theoretic Semantics \['or illocu-tionary Force.
Logique et ,4n,dy.~e ~'6, 10::-I0.l, 19~q'l, pp.3,~9-39.~.5913 AppendixProof of Theorem l:First, we need a lemma:Lemma $ Va (DONE x \[(BEL x (AFTER & p)) ^  (COMPETENT x p)\[?
;a) :) pProo/:Lo3,4.5.Q.E.D.Va {DONE x \[(BEL x {AFTER & p)} A (COMPETENT x p)l?
;a) Ass\[BEL x (AFTER x p)) A (COMPETENT x p) D {AFTER a p) Def.
ofCOMPETENT,  MPYa (DONE x {AFTER & p)?
;a} 2, P4p 3, P3Ya {DONE x \[(BEL x (AFTER a p)} A {COMPETENT x pJl?
;a) D p Impl.
lntr.Theorem I.. Vy (P-COAL y p) A (ALWAYS (COMPETENT y p)) D O(p v {BEL y {ALWAYS ~p)))Proo~I,2.3.4.Q.E.D.
{P.GOAL y (DONE y act}} A {ALWAYS {COMPETENT y {DONE y actJ}}O{3a {DONE y \[(BEL y (AFTER a p}}l?
:a} v {BEL y (ALWAYS ~p}}}O{P v {BEL y (ALWAYS ~p}}}\[P-GOAL y (DONE y act}) ^  (ALWAYS (COMPETENT y {DONE y act}J} :)O(P v {BEL y (ALWAYS ~p)))ASS.I, P25, MPL3, P8, :2Impl.
Intr., 360
