Evaluating Content Selection in Summarization: The Pyramid MethodAni Nenkova and Rebecca PassonneauColumbia UniversityComputer Science DepartmentNew York, NY 10027fani,beckyg@cs.columbia.eduAbstractWe present an empirically grounded methodfor evaluating content selection in summariza-tion.
It incorporates the idea that no single bestmodel summary for a collection of documentsexists.
Our method quantifies the relative im-portance of facts to be conveyed.
We argue thatit is reliable, predictive and diagnostic, thus im-proves considerably over the shortcomings ofthe human evaluation method currently used inthe Document Understanding Conference.1 IntroductionEvaluating content selection in summarization has provento be a difficult problem.
Our approach acknowledgesthe fact that no single best model summary exists, andtakes this as a foundation rather than an obstacle.
In ma-chine translation, the rankings from the automatic BLEUmethod (Papineni et al, 2002) have been shown to corre-late well with human evaluation, and it has been widelyused since and has even been adapted for summarization(Lin and Hovy, 2003).
To show that an automatic methodis a reasonable approximation of human judgments, oneneeds to demonstrate that these can be reliably elicited.However, in contrast to translation, where the evaluationcriterion can be defined fairly precisely it is difficult toelicit stable human judgments for summarization (Rathet al, 1961) (Lin and Hovy, 2002).Our approach tailors the evaluation to observed dis-tributions of content over a pool of human summaries,rather than to human judgments of summaries.
Ourmethod involves semantic matching of content units towhich differential weights are assigned based on their fre-quency in a corpus of summaries.
This can lead to morestable, more informative scores, and hence to a meaning-ful content evaluation.
We create a weighted inventory ofSummary Content Units?a pyramid?that is reliable, pre-dictive and diagnostic, and which constitutes a resourcefor investigating alternate realizations of the same mean-ing.
No other evaluation method predicts sets of equallyinformative summaries, identifies semantic differencesbetween more and less highly ranked summaries, or con-stitutes a tool that can be applied directly to further anal-ysis of content selection.In Section 2, we describe the DUC method.
In Sec-tion 3 we present an overview of our method, contrastour scores with other methods, and describe the distribu-tion of scores as pyramids grow in size.
We compare ourapproach with previous work in Section 4.
In Section 5,we present our conclusions and point to our next step, thefeasibility of automating our method.
A more detailedaccount of the work described here, but not including thestudy of distributional properties of pyramid scores, canbe found in (Passonneau and Nenkova, 2003).2 Current Approach: the DocumentUnderstanding Conference2.1 DUCWithin DUC, different types of summarization have beenstudied: the generation of abstracts and extracts of differ-ent lengths, single- and multi-document summaries, andsummaries focused by topic or opinion.
Evaluation in-volves comparison of a peer summary (baseline, or pro-duced by human or system) by comparing its content toa gold standard, or model.
In 2003 they provided fourhuman summaries for each of the 30 multi-document testsets, any one of which could serve as the model, with nocriteria for choosing among possible models.The four human summaries for each of the 2003 docu-ment sets made our study possible.
As described in Sec-tion 3, we used three of these sets, and collected six addi-tional summaries per set, in order to study the distributionof content units across increasingly many summaries.2.2 DUC evaluation procedureThe procedure used for evaluating summaries in DUC isthe following:1.
A human subject reads the entire input set and cre-ates a 100 word summary for it, called a model.2.
The model summary is split into content units,roughly equal to clauses or elementary discourseunits (EDUs).
This step is performed automaticallyusing a tool for EDU annotation developed at ISI.13.
The summary to be evaluated (a peer) is automat-ically split into sentences.
(Thus the content unitsare of different granularity?EDUs for the model,and sentences for the peer).1http://www.isi.edu/licensed-sw/spade/.4.
Then a human judge evaluates the peer against themodel using the following instructions: For eachmodel content unit:(a) Find all peer units that express at least somefacts from the model unit and mark them.
(b) After all such peer units are marked, think aboutthe whole set of marked peer units and answerthe question:(c) ?The marked peer units, taken together, expressabout k% of the meaning expressed by the cur-rent model unit?, where k can be equal to 0, 20,40, 60, 80 and 100.The final score is based on the content unit coverage.In the official DUC results tables, the score for the entiresummary is the average of the scores of all the contentmodel units, thus a number between 0 and 1.
Some par-ticipants use slightly modified versions of the coveragemetric, where the proportion of marked peer units to thenumber of model units is factored in.The selection of units with the same content is facili-tated by the use of the Summary Evaluation Environment(SEE)2 developed at ISI, which displays the model andpeer summary side by side and allows the user to makeselections by using a mouse.2.3 Problems with the DUC evaluationThere are numerous problems with the DUC human eval-uation method.
The use of a single model summary isone of the surprises ?
all research in summarization eval-uation has indicated that no single good model exists.Also, since not much agreement is expected between twosummaries, many model units will have no counterpartin the peer and thus the expected scores will necessarilybe rather low.
Additionally, the task of determining thepercentage overlap between two text units turns out to bedifficult to annotate reliably ?
(Lin and Hovy, 2002) re-port that humans agreed with their own prior judgment inonly 82% of the cases.These methodological anomalies lead to unreliablescores.
Human-written summaries can score as low as0.1 while machine summaries can score as high as 0.5.For each of the 30 test sets, three of the four human-written summaries and the machine summaries werescored against the fourth human model summary: eachhuman was scored on ten summaries.
Figure 1 showsa scatterplot of human scores for all 30 sets, and illus-trates an apparently random relation of summarizers toeach other, and to document sets.
This suggests that theDUC scores cannot be used to distinguish a good humansummarizer from a bad one.
In addition, the DUC methodis not powerful enough to distinguish between systems.2http://www.isi.edu/?cyl/SEE.???????????
???????????????????????????????????????????????????????????????????????
?data$docsetdata$ducscore0 5 10 15 20 25 300.20.40.60.8Figure 1: Scatterplot for DUC 2003 Human Summaries3 The Pyramid ApproachOur analysis of summary content is based on Summa-rization Content Units, or SCUs and we will now pro-ceed to define the concept.
SCUs emerge from annota-tion of a corpus of summaries and are not bigger than aclause.
Rather than attempting to provide a semantic orfunctional characterisation of what an SCU is, our anno-tation procedure defines how to compare summaries tolocate the same or different SCUs.The following example of the emergence of two SCUsis taken from a DUC 2003 test set.
The sentences areindexed by a letter and number combination, the lettershowing which summary the sentence came from and thenumber indicating the position of the sentence within itsrespective summary.A1 In 1998 two Libyans indicted in 1991 for the Locker-bie bombing were still in Libya.B1 Two Libyans were indicted in 1991 for blowing up aPan Am jumbo jet over Lockerbie, Scotland in 1988.C1 Two Libyans, accused by the United States andBritain of bombing a New York bound Pan Am jet overLockerbie, Scotland in 1988, killing 270 people, for 10years were harbored by Libya who claimed the suspectscould not get a fair trail in America or Britain.D2 Two Libyan suspects were indicted in 1991.The annotation starts with identifying similar sen-tences, like the four above, and then proceeds withfiner grained inspection that can lead to identifyingmore tightly related subparts.
We obtain two SCUsfrom the underlined portions of the sentences above.Each SCU has a weight corresponding to the number ofsummaries it appears in; SCU1 has weight=4 and SCU2has weight=33:3The grammatical constituents contributing to an SCU arebracketed and coindexed with the SCU ID.SCU1 (w=4): two Libyans were officially accused of theLockerbie bombingA1 [two Libyans]1 [indicted]1B1 [Two Libyans were indicted]1C1 [Two Libyans,]1 [accused]1D2 [Two Libyan suspects were indicted]1SCU2 (w=3): the indictment of the two Lockerbiesuspects was in 1991A1 [in 1991]2B1 [in 1991]2D2 [in 1991.
]2The remaining parts of the four sentences above end upas contributors to nine different SCUs of different weightand granularity.
Though we look at multidocument sum-maries rather than single document ones, SCU annotationotherwise resembles the annotation of factoids in (Hal-teren and Teufel, 2003); as they do with factoids, we findincreasing numbers of SCUs as the pool of summariesgrows.
For our 100 word summaries, we find about 34-40 distinct SCUs across four summaries; with ten sum-maries this number grows to about 60.
A more completecomparison of the two approaches follows in section 4.An SCU consists of a set of contributors that, in theirsentential contexts, express the same semantic content.An SCU has a unique index, a weight, and a naturallanguage label.
The label, which is subject to revisionthroughout the annotation process, has three functions.First, it frees the annotation process from dependence ona semantic representation language.
Second, it requiresthe annotator to be conscious of a specific meaning sharedby all contributors.
Third, because the contributors to anSCU are taken out of context, the label serves as a re-minder of the full in-context meaning, as in the case ofSCU2 above where the temporal PPs are about a specificevent, the time of the indictment.Our impression from consideration of three SCU in-ventories is that the pattern illustrated here betweenSCU1 and SCU2 is typical; when two SCUs are seman-tically related, the one with the lower weight is semanti-cally dependent on the other.
We have catalogued a vari-ety of such relationships, and note here that we believe itcould prove useful to address semantic interdependenciesamong SCUS in future work that would involve adding anew annotation layer.4 However, in our approach, SCUsare treated as independent annotation values, which hasthe advantage of affording a rigorous analysis of inter-annotator reliability (see following section).
We do notattempt to represent the subsumption or implicational re-4We are currently investigating the possibility of incorporat-ing narrative relations into SCU pyramids in collaboration withcognitive psychologists.W=4W=1W=2W=3W=4W=1W=2W=3Figure 2: Two of six optimal summaries with 4 SCUslations that Halteren and Teufel assign to factoids (Hal-teren and Teufel, 2003).After the annotation procedure is completed, the finalSCUs can be partitioned in a pyramid.
The partition isbased on the weight of the SCU; each tier contains alland only the SCUs with the same weight.
When we useannotations from four summaries, the pyramid will con-tain four tiers.
SCUs of weight 4 are placed in the top tierand SCUs of weight 1 on the bottom, reflecting the factthat fewer SCUs are expressed in all summaries, morein three, and so on.
For the mid-range tiers, neighbor-ing tiers sometimes have the same number of SCUs.
Indescending tiers, SCUs become less important informa-tionally since they emerged from fewer summaries.We use the term ?pyramid of order n?
to refer to a pyra-mid with n tiers.
Given a pyramid of order n, we canpredict the optimal summary content?it should containall the SCUs from the top tier, if length permits, SCUsfrom the next tier and so on.
In short, an SCU fromtier (n ?
1) should not be expressed if all the SCUs intier n have not been expressed.
This characterization ofoptimal content ignores many complicating factors (e.g.,ordering, SCU interdependency).
However, it is predic-tive: among summaries produced by humans, many seemequally good without having identical content.
Figure2, with two SCUs in the uppermost tier and four in thenext, illustrates two of six optimal summaries of size 4(in SCUs) that this pyramid predicts.The score we assign is a ratio of the sum of the weightsof its SCUs to the sum of the weights of an optimal sum-mary with the same number of SCUs.
It ranges from 0to 1, with higher scores indicating that relatively more ofthe content is as highly weighted as possible.The exact formula we use is computed as follows.
Sup-pose the pyramid has n tiers, Ti, with tier Tnon top andT1on the bottom.
The weight of SCUs in tier Tiwill bei.5 Let jTij denote the number of SCUs in tier Ti.
Let Dibe the number of SCUs in the summary that appear in Ti.SCUs in a summary that do not appear in the pyramid areassigned weight zero.
The total SCU weight D is:D =?ni=1i  Di5This weight is not fixed and the method does not dependon the specific weights assigned.
The weight assignment usedis simply the most natural and intuitive one.The optimal content score for a summary with X SCUsis:Max =n?i=j+1i  jTij + j  (X ?n?i=j+1jTij)where j = maxi(n?t=ijTtj  X) (1)In the equation above, j is equal to the index of thelowest tier an optimally informative summary will drawfrom.
This tier is the first one top down such that thesum of its cardinality and the cardinalities of tiers aboveit is greater than or equal to X (summary size in SCUs).For example, if X is less than the cardinality of the mosthighly weighted tier, then j = n and Max is simply Xn(the product of X and the highest weighting factor).Then the pyramid score P is the ratio of D to Max.Because P compares the actual distribution of SCUs toan empirically determined weighting, it provides a directcorrelate of the way human summarizers select informa-tion from source texts.3.1 Reliability and RobustnessWe aimed for an annotation method requiring relativelylittle training, and with sufficient interannotator reliabil-ity to produce a stable pyramid score.
Here we present re-sults indicating good interannotator reliability, and pyra-mid scores that are robust across annotations.SCU annotation involves two types of choices: extract-ing a contributor from a sentence, and assigning it to anSCU.
In a set of four summaries about the Philippine Air-lines (PAL), two coders (C1 and C2; the co-authors) dif-fered on the extent of the following contributor: fC1afterfC2the ground crew union turned down a settlementgC1whichgC2.
Our approach is to separate syntactic from se-mantic agreement, as in (Klavans et al, 2003).
Becauseconstituent structure is not relevant here, we normalize allcontributors before computing reliability.We treat every word in a summary as a coding unit, andthe SCU it was assigned to as the coding value.
We re-quire every surface word to be in exactly one contributor,and every contributor to be in exactly one SCU, thus anSCU annotation constitutes a set of equivalence classes.Computing reliability then becomes identical to compar-ing the equivalence classes constituting a set of corefer-ence annotations.
In (Passonneau, 2004), we report ourmethod for computing reliability for coreference annota-tions, and the use of a distance metric that allows us toweight disagreements.
Applying the same data represen-tation and reliability formula (Krippendorff?s Alpha) asin (Passonneau, 2004), and a distance metric that takesinto account relative SCU size, to the two codings C1and C2 yields ?
= 81.
Values above .67 indicate goodreliability (Krippendorff, 1980).A H C JC1 .97 .87 .83 .82C2 .94 .87 .84 .74Consensus .95 .89 .85 .76Table 1: Pyramid scores across annotations.1 (9) 2 (36) 3 (84) 4 (128) 5 (128) 6 (84) 7 (36) 8 (9) 9 (1)0.30.40.50.60.70.80.91Number of summaries in the pyramid (number of pyramids)Summaryscorearv d30042.bmin d30042.bmax d30042.barv d30042.qmin d30042.qmax d30042.qFigure 3: Min, max and average scores for two sum-maries ?
one better than the other.More important than interannotator reliability is the ro-bustness of the pyramid metric, given different SCU an-notations.
Table 1 gives three sets of pyramid scores forthe same set of four PAL summaries.
The rows of scorescorrespond to the original annotations (C1, C2) and aconsensus.
There is no significant difference in the scoresassigned across the three annotations (between subjectsANOVA=0.11, p=0.90).3.2 Pyramid Scores of Human SummariesHere we use three DUC 2003 summary sets for whichfour human summaries were written.
In order to provideas broad a comparison as possible for the least annotationeffort, we selected the set that received the highest DUCscores (D30042: Lockerbie), and the two that receivedthe lowest (D31041: PAL; D31050: China).
For each set,we collected six new summaries from advanced under-graduate and graduate students with evidence of superiorverbal skills; we gave them the same instructions used byNIST.
This turned out to be a large enough corpus to in-vestigate how many summaries a pyramid needs for scorestability.
Here we first compare pyramid scores of theoriginal summaries with DUC scores.
Then we presentresults demonstrating the need for at least five summariesper pyramid, given this corpus of 100-word summaries.Table 2 compares DUC and pyramid scores for allthree sets.
The first two rows of pyramid scores are fora pyramid of order 3 using a single pyramid with the re-maining three original DUC summaries (n=3) versus anLockerbie (D30042)Method A B C DDUC n.a.
.82 .54 .74Pyramid (n=3) .69 .83 .75 .82Pyramid (Avg.
n=3) .68 .82 .74 .76Pyramid (n=9) .74 .89 .80 .83PAL (D31041)Method A H I JDUC .30 n.a.
.30 .10Pyramid (n=3) .76 .67 .59 .43Pyramid (Avg.
n=3) .46 .50 .52 .57Pyramid (n=9) .52 .56 .60 .63China (D31050)Method C D E FDUC n.a.
.28 .27 .13Pyramid (n=3) .57 .63 .72 .56Pyramid (Avg.
n=3) .64 .61 .72 .58Pyramid (n=9) .69 .67 .78 .63Table 2: Comparison of DUC and Pyramid scores; capitalletters represent distinct human summarizers.average over all order-3 pyramids (Avg.
n=3); the thirdrow of pyramid scores are for the single pyramid of or-der 9 (n=9; note that the 10th summary is the one beingscored).
Compared to the DUC scores, pyramid scoresshow all humans performing reasonably well.
While theLockerbie set summaries are better overall, the differencewith the PAL and China sets scores is less great than withthe DUC method, which accords with our impressionsabout the relative quality of the summaries.
Note thatpyramid scores are higher for larger pyramid inventories,which reflects the greater likelihood that more SCUs inthe summary appear in the pyramid.
For a given orderpyramid, the scores for the average and for a specificpyramid can differ significantly, as, for example, PAL Aand PAL J do (compare rows n=3 and n=9).The pyramid rows labelled ?n=3?
are the most compa-rable to the DUC scores in terms of the available data.For the DUC scores there was always a single model, andno attempt to evaluate the model.Pyramid scores are quantitatively diagnostic in thatthey express what proportion of the content in a summaryis relatively highly weighted, or alternatively, what pro-portion of the highly weighted SCUs appear in a sum-mary.
The pyramid can also serve as a qualitative diag-nostic tool.
To illustrate both points, consider the PAL Asummary; its score in the n=3 row of .76 indicates thatrelatively much of its content is highly weighted.
Thatis, with respect to the original pyramid with only threetiers, it contained a relatively high proportion of the toptier SCUs: 3/4 of the w=3 facts (75%).
When we av-erage over all order-3 pyramids (Avg.
n=3) or use thelargest pyramid (n=9), the PAL A score goes down to .46or .52, respectively.
Given the nine-tier pyramid, PAL Acontains only 1/3 of the SCUs of w6, a much smallerproportion of the most highly weighted ones.
There arefour missing highly weighted SCUs and they express thefollowing facts: to deal with its financial crisis, Pal nego-tiated with Cathay Pacific for help; the negotiations col-lapsed; the collapse resulted in part from PAL?s refusal tocut jobs; and finally, President Estrada brokered an agree-ment to end the shutdown strike.
These facts were in theoriginal order-3 pyramid with relatively lower weights.The score variability of PAL A, along with the changein status of SCUs from having low weights to having highones, demonstrates that to use the pyramid method reli-ably, we need to ask how many summaries are neededto produce rankings across summaries that we can haveconfidence in.
We now turn to this analysis.3.3 Behavior of Scores as Pyramid GrowsHere we address two questions raised by the data fromTable 2, i.e., that scores change as pyramid size increases:1.
How does variability of scores change as pyramidorder increases?2.
At what order pyramid do scores become reliable?To have confidence in relative ranking of summaries bypyramid scores, we need to answer the above questions.It has often been noted that different people write dif-ferent summaries; we observe that with only a few sum-maries in a pyramid, there is insufficient data for thescores associated with a pyramid generated from onecombination of a few summaries to be relatively the sameas those using a different combination of a few sum-maries.
Empirically, we observed that as pyramids growlarger, and the range between higher weight and lowerweight SCUS grows larger, scores stabilize.
This makessense in light of the fact that a score is dominated by thehigher weight SCUS that appear in a summary.
However,we wanted to study more precisely at what point scoresbecome independent of the choice of models that pop-ulate the pyramid.
We conducted three experiments tolocate the point at which scores stabilize across our threedatasets.
Each experiment supports the same conclusion,thus reinforcing the validity of the result.Our first step in investigating score variability was toexamine all pairs of summaries where the difference inscores for an order 9 pyramid was greater than 0.1; therewere 68 such pairs out of 135 total.
All such pairs ex-hibit the same pattern illustrated in Figure 3 for two sum-maries we call ?b?
and ?q?.
The x-axis on the plot showshow many summaries were used in the pyramid and they-axis shows the min, max and average score scores forthe summaries for a given order of pyramid, 6 Of the two,6Note that we connected data points with lines to make thegraph more readable.?b?
has the higher score for the order 9 pyramid, and isperceivably more informative.
Averaging over all order-1 pyramids, the score of ?b?
is higher than ?q?
but someindividual order-1 pyramids might yield a higher scorefor ?q?.
The score variability at order-1 is huge: it canbe as high as 0.5.
With higher order pyramids, scoresstabilize.
Specifically, in our data, if summaries divergeat some point as in Figure 3, where the minimum scorefor the better summary is higher than the maximum scorefor the worse summary, the size of the divergence neverdecreases as pyramid order increases.
For pyramids oforder > 4, the chance that ?b?
and ?q?
reverse rankingapproaches zero.For all pairs of divergent summaries, the relationshipof scores follows the same pattern we see in Figure 3 andthe point of divergence where the scores for one summarybecome consistently higher than those of the othere, wasfound to be stable ?
in all pair instances, if summary Agets higher scores than summary B for all pyramids oforder n, than A gets higher scores for pyramids of order n. We analyzed the score distributions for all 67 pairsof ?divergent?
summaries in order to determine what or-der of pyramid is required to reliably discriminate them.The expected value for the point of divergence of scores,in terms of number of summaries in the pyramid, is 5.5.We take the scores assigned at order 9 pyramids as be-ing a reliable metric on the assumption that the patternwe have observed in our data is a general one, namelythat variance always decreases with increasing orders ofpyramid, and that once divergence of scores occurs, thebetter summary never gets a lower score than the worsefor any model of higher order.We postulate that summaries whose scores differ byless than 0.06 have roughly the same informativeness.The assumption is supported by two facts.
First, this cor-responds to the difference in PAL scores (D31041) wefind when we use a different one of our three PAL an-notations (see Table 1).
Second, the pairs of summarieswhose scores never clearly diverged had scores differingby less than 0.06 at pyramid order 9.Now, for each pair of summaries (sum1, sum2), wecan say whether they are roughly the same when evalu-ated against a pyramid of order n and we will denote thisas jsum1j ==njsum2j, (scores differ by less than 0.06for some pyramid of order n) or different (scores differby more than 0.06 for all pyramids of order n) and wewill use the notation jsum1j <njsum2j if the score forsum2 is higher.When pyramids of lower order are used, the followingerrors can happen, with the associated probabilities:E1: jsum1j ==9jsum2j but jsum1j <njsum2j orjsum1j >njsum2j at some lower order n pyramid.The conditional probability of this type of error isp1= P (jsum1j >njsum2jjjsum1j ==9jsum2j).E2: jsum1j <9jsum2j but at a lower orderjsum1j ==njsum2j.
This error corresponds to?losing ability to discern?, which means one can tol-erate it, as long as the goal is not be able to make finegrained distinctions between the summaries.
Here,p2= P (jsum1j ==njsum2jjjsum1j <9jsum2j).E3: jsum1j <9jsum2j but at lower leveljsum1j >njsum2j Here, p3= P (jsum1j >njsum2jjjsum1j <9jsum2j) + P (jsum1j <njsum2jjsum1j >njsum2j).
This is the mostsevere kind of mistake and ideally it should neverhappen?the two summaries appear with scoresopposite to what they really are.7The probabilities p1, p2and p3can be computed di-rectly by counting how many times the particular erroroccurs for all possible pyramids of order n. By takingeach pyramid that does not contain either of sum1 orsum2 and comparing the scores they are assigned, theprobabilities in Table 3 are obtained.
We computed prob-abilities for pairs of summaries for the same set, thensummed the counts for error occurrence across sets.
Theorder of the pyramid is shown in column n. ?Data points?shows how many pyramids of a given order were exam-ined when computing the probabilities.
The total proba-bility of error p = p1 P (jsum1j ==9jsum2j) + (p2 +p3)  (1 ?
P (jsum1j ==9jsum2j)) is also in Table 3.Table 3 shows that for order-4 pyramids, the errors oftype E3are ruled out.
At order-5 pyramids, the total prob-ability of error drops to 0.1 and is mainly due to error E2,which is the mildest one.Choosing a desirable order of pyramid involves balanc-ing the two desiderata of having less data to annotate andscore stability.
Our data suggest that for this corpus, 4 or5 summaries provide an optimal balance of annotation ef-fort with reliability.
This is reconfirmed by our followinganalysis of ranking stability.n p1 p2 p3 p data points1 0.41 0.23 0.08 0.35 10802 0.27 0.23 0.03 0.26 37803 0.16 0.19 0.01 0.18 75604 0.09 0.17 0.00 0.14 95505 0.05 0.14 0.00 0.10 75606 0.02 0.10 0.00 0.06 37807 0.01 0.06 0.00 0.04 10808 0.00 0.01 0.00 0.01 135Table 3: Probabilities of errors E1, E2, E3 and total prob-ability of error7Note that such an error can happen only for models of orderlower than their point of divergence.In order to study the issue of how the pyramid scoresbehave when several summarizers are compared, not justtwo, for each set we randomly selected 5 peer summariesand constructed pyramids consisting of all possible sub-sets of the remaining five.
We computed the Spearmanrank-correlation coefficient for the ranking of the 5 peersummaries compared to the ranking of the same sum-maries given by the order-9 pyramid.
Spearman coef-ficent rs(Dixon and Massey, 1969) ranges from -1 to1, and the sign of the coefficent shows whether the tworankings are correlated negatively or positively and itsabsolute value shows the strength of the correlation.
Thestatistic rscan be used to test the hypothesis that the twoways to assign scores leading to the respective rankingsare independent.
The null hypothesis can be rejected withone-sided test with level of significance ?
= 0.05, givenour sample size N = 5, if rs 0.85.Since there are multiple pyramids of order n  5, wecomputed the average ranking coefficient, as shown inTable 4.
Again we can see that in order to have a rankingof the summaries that is reasonably close to the rankingsproduces by a pyramid of order n = 9, 4 or more sum-maries should be used.n average rs# pyramids1 0.41 152 0.65 303 0.77 304 0.87 155 1.00 3Table 4: Spearman correlation coefficient average forpyramids of order n  53.4 Rank-correlation with unigram overlap scoresLin and Hovy (2003) have shown that a unigram co-occurrence statistic, computed with stop words ignored,between a summary and a set of models can be used toassign scores for a test suite that highy correlates with thescores assigned by human evaluators at DUC.
We haveillustrated in Figure 1 above that human scores on humansummaries have large variance, and we assume the sameholds for machine summaries, so we believe the approachis built on weak assumptions.
Also, their approach is notdesigned to rank individual summaries.These qualifications aside, we wanted to test whether itis possible to use their approach for assigning scores notfor an entire test suite but on a per set basis.
We computedthe Spearman rank-coefficent rsfor rankings assigned bycomputing unigram overlap and those by pyramid of or-der 9.
For computing the scores, Lin?s original systemwas used, with stop words ignored.
Again 5 summarieswere chosen at random to be evaluated against modelscomposed of the remaining five summaries.
Compositemodels were obtained by concatenating different combi-nations of the initial five summaries.
Thus scores can becomputed using one, two and so on up to five referencesummaries.
Table 5 shows the average values of rsthatwere obtained.# models average rs# model combinations1 0.12 152 0.27 303 0.29 304 0.35 155 0.33 3Table 5: Spearman correlation coefficient average for un-igram overlap score assignmentAs noted above, in order to consider the two scoringmethods as being substitutable, rsshould be bigger than0.85, given our sample size.
Given the figures shown inTable 5, we don?t have reason to believe that unigramscores are correlated with pyramid scores.4 Comparison with previous workThe work closest to ours is (Halteren and Teufel, 2003),and we profited from the lessons they derived from anannotation of 50 summaries of a single 600-word docu-ment into content units that they refer to as factoids.
Theyfound a total of 256 factoids and note that the increase infactoids with the number of summaries seems to follow aZipfian distribution.We identify four important differences between fac-toids and SCUs.
First, an SCU is a set of contributorsthat are largely similar in meaning, thus SCUs differ fromeach other in both meaning and weight (number of con-tributors).
In contrast, factoids are semi-formal expres-sions in a FOPL-style semantics, which are composition-ally interpreted.
We intentionally avoid creating a rep-resentation language for SCU labels; the function of anSCU label is to focus the annotator?s attention on theshared meaning of the contributors.
In contrast to Hal-tern and Teufel, we do not believe it is possible to arriveat the correct representation for a set of summaries; theyrefer to the observation that the factoids arrived at dependon the summaries one starts with as a disadvantage in thatadding a new summary can require adjustments to the setof factoids.
Given the different knowledge and goals ofdifferent summarizers, we believe there can be no cor-rect representation of the semantic content of a text orcollection; a pyramid, however, represents an emergentconsensus as to the most frequently recognized content.In addition to our distinct philosophical views regardingthe utility of a factoid language, we have methodologicalconcerns: the learning curve required to train annotatorswould be high, and interannotator reliability might be dif-ficult to quantify or to achieve.Second, (Halteren and Teufel, 2003) do not make di-rect use of factoid frequency (our weights): to constructa model 100-word summary, they select factoids that oc-cur in at least 30% of summaries, but within the resultingmodel summary, they do not differentiate between moreand less highly weighted factoids.
Third, they annotatesemantic relations among factoids, such as generalizationand implication.
Finally, they report reliability of the an-notation using recall and precision, rather than a reliabil-ity metric that factors in chance agreement.
In (Passon-neau, 2004), we note that high recall/precision does notpreclude low interannotator reliability on a coreferenceannotation task.Radev et al (2003) also exploits relative importance ofinformation.
Evaluation data consists of human relevancejudgments on a scale from 0 to 10 on for all sentences inthe original documents.
Again, information is lost rela-tive to the pyramid method because a unique referencesummary is produced instead of using all the data.
Thereference summary consists of the sentences with highestrelevance judgements that satisfy the compression con-straints.
For multidocument summarization compressionrates are high, so even sentences with the highest rele-vance judgments are potentially not used.Lin and Hovy (2002) and Lin and Hovy (2003) werethe first to systematically point out problems with thelarge scale DUC evaluation and to look to solutions byseeking more robust automatic alternatives.
In their stud-ies they found that multiple model summaries lead tomore stable evaluation results.
We believe a flaw in theirwork is that they calibrate the method to the erratic DUCscores.
When applied to per set ranking of summaries, nocorrelation was seen with pyramid scores.5 ConclusionsThere are many open questions about how to parameter-ize a summary for specific goals, making evaluation initself a significant research question (Jing et al, 1998).Instead of attempting to develop a method to elicit reli-able judgments from humans, we chose to calibrate ourmethod to human summarization behavior.The strengths of pyramid scores are that they are re-liable, predictive, and diagnostic.
The pyramid methodnot only assigns a score to a summary, but also allows theinvestigator to find what important information is miss-ing, and thus can be directly used to target improvementsof the summarizer.
Another diagnostic strength is that itcaptures the relative difficulty of source texts.
This allowsfor a fair comparison of scores across different input sets,which is not the case with the DUC method.We hope to address two drawbacks to our method infuture work.
First, pyramid scores ignore interdependen-cies among content units, including ordering.
However,our SCU annotated summaries and correlated pyramidsprovide a valuable data resource that will allow us to in-vestigate such questions.
Second, creating an initial pyra-mid is laborious so large-scale application of the methodwould require an automated or semi-automated approach.We have started exploring the feasibility of automationand we are collecting additional data sets.ReferencesWilfrid Dixon and Frank Massey.
1969.
Introduction tostatistical analysis.
McGraw-Hill Book Company.Hans Halteren and Simone Teufel.
2003.
Examiningthe consensus between human summaries: initial ex-periments with factoid analysis.
In HLT-NAACL DUCWorkshop.Hongyan Jing, Regina Barzilay, Kathleen McKeown, andMichael Elhadad.
1998.
Summarization evaluationmethods: Experiments and analysis.
In AAAI Sympo-sium on Intelligent Summarization.Judith Klavans, Sam Popper, and Rebecca J. Passonneau.2003.
Tackling the internet glossary glut: Extractionand evaluation of genus phrases.
In SIGIR Workshop:Semantic Web, Toronto.Klaus Krippendorff.
1980.
Content Analysis: An Intro-duction to Its Methodology.
Sage Publications, Bev-erly Hills, CA.Chin-Yew Lin and Eduard Hovy.
2002.
Manual and au-tomatic evaluation of summaries.
In Proceedings ofthe Workshop on Automatic Summarization, post con-ference workshop of ACL 2002.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic eval-uation of summaries using n-gram co-occurance statis-tics.
In Proceedings of HLT-NAACL 2003.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automatic evalu-ation of machine translation.
In ACL.Rebecca J. Passonneau and Ani Nenkova.
2003.
Evaluat-ing content selection in human- or machine-generatedsummaries: The pyramid method.
Technical ReportCUCS-025-03, Columbia University.Rebecca J. Passonneau.
2004.
Computing reliabilityfor coreference annotation.
In Proceedings of the 4thInternational Conference on Language Resources andEvaluation (LREC), Lisbon, Portugal.Dragomir Radev, Simone Teufel, Horacio Saggion, andW.
Lam.
2003.
Evaluation challenges in large-scalemulti-document summarization.
In ACL.G.
J. Rath, A. Resnick, and R. Savage.
1961.
The for-mation of abstracts by the selection of sentences: Part1: sentence selection by man and machines.
AmericanDocumentation, 2(12):139?208.
