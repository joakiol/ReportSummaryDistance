Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 197?206,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsCombining Shallow and Linguistically Motivated Features inNative Language IdentificationSerhiy Bykh Sowmya Vajjala Julia Krivanek Detmar MeurersSeminar fu?r Sprachwissenschaft, Universita?t Tu?bingen{sbykh, sowmya, krivanek, dm}@sfs.uni-tuebingen.deAbstractWe explore a range of features and ensemblesfor the task of Native Language Identificationas part of the NLI Shared Task (Tetreault et al2013).
Starting with recurring word-based n-grams (Bykh and Meurers, 2012), we testeddifferent linguistic abstractions such as part-of-speech, dependencies, and syntactic treesas features for NLI.
We also experimentedwith features encoding morphological proper-ties, the nature of the realizations of particu-lar lemmas, and several measures of complex-ity developed for proficiency and readabil-ity classification (Vajjala and Meurers, 2012).Employing an ensemble classifier incorporat-ing all of our features we achieved an ac-curacy of 82.2% (rank 5) in the closed taskand 83.5% (rank 1) in the open-2 task.
Inthe open-1 task, the word-based recurring n-grams outperformed the ensemble, yielding38.5% (rank 2).
Overall, across all three tasks,our best accuracy of 83.5% for the standardTOEFL11 test set came in second place.1 IntroductionNative Language Identification (NLI) tackles theproblem of determining the native language of anauthor based on a text the author has written in asecond language.
With Tomokiyo and Jones (2001),Jarvis et al(2004), and Koppel et al(2005) as firstpublications on NLI, the research focus in computa-tional linguistics is relatively young.
But with overa dozen new publications in the last two years, it isgaining significant momentum.In Bykh and Meurers (2012), we explored a data-driven approach using recurring n-grams with threelevels of abstraction using parts-of-speech (POS).
Inthe present work, we continue exploring the contri-bution and usefulness of more linguistically moti-vated features in the context of the NLI Shared Task(Tetreault et al 2013), where our approach is in-cluded under the team name ?Tu?bingen?.2 Corpora usedT11: TOEFL11 (Blanchard et al 2013) This is themain corpus of the NLI Shared Task 2013.
It con-sists of essays written by English learners with 11native language (L1) backgrounds (Arabic, Chinese,French, German, Hindi, Italian, Japanese, Korean,Spanish, Telugu, Turkish), and from three differentproficiency levels (low, medium, high).
Each L1 isrepresented by a set of 1100 essays (train: 900, dev:100, test: 100).
The labels for the train and dev setswere given from the start, the labels for the test setwere provided after the results were submitted.ICLE: International Corpus of Learner English(Granger et al 2009) The ICLEv2 corpus consistsof 6085 essays written by English learners of 16 dif-ferent L1 backgrounds.
They are at a similar level ofEnglish proficiency, namely higher intermediate toadvanced and of about the same age.
For the cross-corpus tasks we used the essays for the seven L1s inthe intersection with T11, i.e., Chinese (982 essays),French (311), German (431), Italian (391), Japanese(366), Spanish (248), and Turkish (276).FCE: First Certificate in English Corpus (Yan-nakoudakis et al 2011) The FCE dataset consistsof 1238 scripts produced by learners taking the FirstCertificate in English exam, assessing English at an197upper-intermediate level.
For the cross-corpus tasks,we used the essays by learners of the eight L1s inthe intersection with T11, i.e., Chinese (66 essays),French (145), German (69), Italian (76), Japanese(81), Korean (84), Spanish (198), and Turkish (73).BALC: BUiD (British University in Dubai) ArabLearner Corpus (Randall and Groom, 2009) TheBALC corpus consists of 1865 English learner textswritten by students with an Arabic L1 backgroundfrom the last year of secondary school and the firstyear of university.
The texts were scored and as-signed to six proficiency levels.
For the cross-corpusNLI tasks, we used the data from the levels 3?5amounting to overall 846 texts.
We excluded the twolowest and the highest, sixth level based on pretestswith the full BALC data.ICNALE: International Corpus Network ofAsian Learners of English (Ishikawa, 2011) Theversion of the ICNALE corpus we used consists of5600 essays written by college students in ten coun-tries and areas in Asia as well as by English na-tive speakers.
The learner essays are assigned tofour proficiency levels following the CEFR guide-lines (A2, B1, B2, B2+).
For the cross-corpus tasks,we used the essays written by learners from Korea(600 essays) and from Pakistan (400).1 Without ac-cess to a corpus with Hindi as L1, we decided to la-bel the essays written by Pakistani students as Hindi.Most of the languages spoken in Pakistan, includingthe official language Urdu, belong to the same Indo-Aryan/-Iranian language family as Hindi.
Our mainfocus here was on avoiding overlap with Telugu, theother Indian language in this shared task, which be-longs to the Dravidian language family.TU?TEL-NLI: Tu?bingen Telugu NLI Corpus Wecollected 200 English texts written by Telugu nativespeakers from bilingual (English-Telugu) blogs, lit-erary articles, news and movie review websites.NT11: NON-TOEFL11 We combined the ICLE,FCE, ICNALE, BALC and TU?TEL-NLI sourcesdiscussed above in the NT11 corpus consisting ofoverall 5843 essays for 11 L1s, as shown in Table 1.1We did not include ICNALE data for more L1s to avoidoverrepresentation of already well-represented Asian L1s.CorporaL1 ICLE FCE BALC ICNALE TU?TEL #ARA - - 846 - - 846CHI 982 66 - - - 1048FRE 311 145 - - - 456GER 431 69 - - - 500HIN - - - 400 - 400ITA 391 76 - - - 467JPN 366 81 - - - 447KOR - 84 - 600 - 684SPA 248 198 - - - 446TEL - - - - 200 200TUR 276 73 - - - 349# 3005 792 846 1000 200 5843Table 1: Distribution of essays for the 11 L1s in NT113 FeaturesRecurring word-based n-grams (rc.
word ng.
)Following, Bykh and Meurers (2012), we used allword-based n-grams occurring in at least two textsof the training set.
We focused on recurring uni-grams and bigrams, which in our previous work andin T11 testing with the dev set worked best.
For thelarger T11 train ?
NT11 set, recurring n-grams upto length five were best, but for uniformity we onlyused word-based unigrams and bigrams for all tasks.As in our previous work, we used a binary featurerepresentation encoding the presence or absence ofthe n-gram in a given essay.Recurring OCPOS-based n-grams (rc.
OCPOSng.)
All OCPOS n-grams occurring in at least twotexts of the training set were obtained as describedin Bykh and Meurers (2012).
OCPOS means thatthe open class words (nouns, verbs, adjectives andcardinal numbers) are replaced by the correspondingPOS tags.
For POS tagging we used the OpenNLPtoolkit (http://opennlp.apache.org).In Bykh and Meurers (2012), recurring OCPOSn-grams up to length three performed best.
How-ever, for T11 we found that including four- and five-grams was beneficial.
This confirms our assumptionthat longer n-grams can be sufficiently common tobe useful (Bykh and Meurers, 2012, p. 433).
Thuswe used the recurring OCPOS n-grams up to lengthfive for the experiments in this paper.
We again useda binary feature representation.198Recurring word-based dependencies (rc.
worddep.)
Extending the perspective on recurring piecesof data to other data types, we explored a new fea-ture: recurring word-based dependencies.
A featureof this type consists of a head and all its immediatedependents.
The dependencies were obtained usingthe MATE parser (Bohnet, 2010).
The words in eachn-tuple are recorded in lowercase and listed in the or-der in which they occur in the text; heads thus are notsingled out in this encoding.
For example, the sen-tence John gave Mary an interesting book yields thefollowing two potential features (john, gave, mary,book) and (an, interesting, book).
As with recur-ring n-grams we utilized only features occurring inat least two texts of the training set, and we used abinary feature representation.Recurring function-based dependencies (rc.func.
dep.)
The recurring function-based depen-dencies are a variant of the recurring word-baseddependencies described above, where each depen-dent is represented by its grammatical function.
Theabove example sentence thus yields the two features(sbj, gave, obj, obj) and (nmod, nmod, book).Complexity Given that the proficiency level of alearner was shown to play a role in NLI (Tetreaultet al 2012), we implemented all the text com-plexity features from Vajjala and Meurers (2012),who used measures of learner language complex-ity from SLA research for readability classification.These features consist of lexical richness and syn-tactic complexity measures from SLA research (Lu,2010; 2012) as well as other syntactic parse treeproperties and traditionally used readability formu-lae.
The parse trees were built using the Berke-ley parser (Petrov and Klein, 2007) and the syntac-tic complexity measures were estimated using theTregex package (Levy and Andrew, 2006).In addition, we included morphological and POSfeatures from the CELEX Lexical Database (Baayenet al 1995).
The morphological properties of wordsin CELEX include information about the deriva-tional, inflectional and compositional features ofthe words along with information about their mor-phological origins and complexity.
POS propertiesof the words in CELEX describe the various at-tributes of a word depending on its parts of speech.We included all the non-frequency based and non-word-string attributes from the English MorphologyLemma (EML) and English Syntax Lemma (ESL)files of the CELEX database.
We also defined Ageof Acquisition features based on the psycholinguis-tic database compiled by Kuperman et al(2012).
Fi-nally, we included the ratios of various POS tags tothe total number of words as POS density features,using the POS tags from the Berkeley parser output.Suffix features The use of different derivationaland inflectional suffixes may contain informationregarding the L1 ?
either through L1 transfer, orin terms of what suffixes are taught, e.g., fornominalization.
In a very basic approximation ofmorphological analysis, we used the porter stem-mer implementation of MorphAdorner (http://morphadorner.northwestern.edu).
For eachword in a learner text, we removed the stemit identified from the word, and if a suffix re-mained, we matched it against the Wiktionary list ofEnglish suffixes (http://en.wiktionary.org/wiki/Appendix:Suffixes:English).
For eachvalid suffix thus identified, we defined a binary fea-ture (suffix, bin.)
recording the presence/absenceand a feature counting the number of occurrences(suffix, cnt.)
in a given learner text.Stem-suffix features We also wondered whetherthe subset of morphologically complex unigramsmay be more indicative than considering all uni-grams as features.
As a simple approximation of thisidea, we used the stemmer plus suffix-list approachmentioned above and used all words for which a suf-fix was identified as features, both binary (stemsuf-fix, bin.)
and count-based (stemsuffix, cnt.
).Local trees Based on the syntactic trees assignedby the Berkeley Parser (Petrov and Klein, 2007), weextracted all local trees, i.e., trees of depth one.
Forexample, for the sentence I have a tree, the parseroutput is: (ROOT (S (NP (PRP I)) (VP (VBP have)(NP (DT a) (NN tree))) (.
.)))
for which the localtrees are (S NP VP .
), (NP PRP), (NP DT NN), (VPVBP NP), (ROOT S).
Count-based features are used.Stanford dependencies Tetreault et al(2012) ex-plored the utility of basic dependencies as featuresfor NLI.
In our approach, we extracted all Stanford199dependencies (de Marneffe et al 2006) using thetrees assigned by the Berkeley Parser.
We consid-ered lemmatized typed dependencies (type dep.
lm.
)such as nsubj(work,human) and POS tagged ones(type dep.
POS) such as nsubj(VB,NN) for our fea-tures.
We used count-based features for those typeddependencies.Dependency number (dep.
num.)
We encoded thenumber of dependents realized by a verb lemma,normalized by this lemma?s count.
For example, ifthe lemma take occurred ten times in a document,three times with two dependents and seven timeswith three dependents, we get the features take:2-dependents = 3/10 and take:3-dependents = 7/10.Dependency variability (dep.
var.)
These fea-tures count possible dependent-POS combinationsfor a verb lemma, normalized by this verb lemma?scount.
If in the example above, the lemma takeoccurred three times with two dependents JJ-NN,two times with three dependents JJ-NN-VB, and fivetimes with three dependents NN-NN-VB, we ob-tain take:JJ-NN = 3/10, take:JJ-NN-VB = 2/10, andtake:NN-NN-VB = 5/10.Dependency POS (dep.
POS) These features arederived from the dep.
var.
features and encode howfrequent which kind of category was a dependent fora given verb lemma.
Continuing the example above,take takes dependents of three different categories:JJ, NN and VB.
For each category, we create a fea-ture, the value of which is the category count dividedby the number of dependents of the given lemma,normalized by the lemma?s count in the document.In the example, we obtain take:JJ = (1/2 + 1/3)/10,take:NN = (1/2 + 1/3 + 2/3)/10, and take:VB = (1/3+ 1/3)/10.Lemma realization matrix (lm.
realiz.)
We spec-ified a set of features that is calculated for each dis-tinct lemma and three feature sets generalizing overall lemmas of the same category:1.
Distinct lemma counts of a specific categorynormalized by the total count of this categoryin a document.
For example, if the lemma canis found in a document two times as a verb andfive times as a noun, and the document contains30 verbs and 50 nouns, we obtain the two fea-tures can:VB = 2/30 and can:NN = 5/50.2.
Type-Lemma ratio: lemmas of same categorynormalized by total lemma count3.
Type-Token ratio: tokens of same category nor-malized by total token count4.
Lemma-Token Ratio: lemmas of same categorynormalized by tokens of same categoryProficiency and prompt features Finally, for somesettings in the closed task we also included two nom-inal features to encode the proficiency (low, medium,high) and the prompt (P1?P8) features provided asmeta-data along with the T11 corpus.4 Results4.1 Evaluation SetupWe developed our approach with a focus on theclosed task, training the models on the T11 train setand testing them on the T11 dev set.
For theclosed task, we report the accuracies on the dev setfor all models (single feature type models and en-sembles as introduced in sections 4.2 and 4.3),before presenting the accuracies on the submittedtest set models, which were trained on the T11 train?
dev set.
In addition, for the submitted modelswe report the accuracies obtained via 10-fold cross-validation on the T11 train ?
dev set using the foldsspecification provided by the organizers of the NLIShared Task 2013.The results for the open-1 task are obtained bytraining the models on the NT11 set, and the resultsfor the open-2 task are obtained by training the mod-els on the T11 train ?
dev set ?
NT11 set.
For theopen-1 and open-2 tasks, we report the basic singlefeature type results on the T11 dev set and two setsof results on the T11 test set: the results for the ac-tual submitted systems and the results for the com-plete systems, i.e., including the features used in theclosed task submissions that for the open tasks wereonly computed after the submission deadline (givenour focus on the closed task and finite computationalinfrastructure).
We include the figures for the com-plete systems to allow a proper comparison of theperformance of our models across the tasks.Below we provide a description of the various ac-curacies (%) we report for the different tasks:200?
Acctest: Accuracy on the T11 test set aftertraining the model on:?
closed: T11 train ?
dev set?
open-1: NT11 set?
open-2: T11 train ?
dev set ?
NT11 set?
Accdev: Accuracy on the T11 dev set aftertraining the model on:?
closed: T11 train set?
open-1: NT11 set?
open-2: T11 train set ?
NT11 set?
Acc10train?dev: Accuracy on the T11 train ?
devset obtained via 10-fold cross-validation usingthe data split information provided by the orga-nizers, applicable only for the closed task.In terms of the tools used for classification, weemployed LIBLINEAR (Fan et al 2008) usingL2-regularized logistic regression, LIBSVM (Changand Lin, 2011) using C-SVC with the RBF kerneland WEKA SMO (Platt, 1998; Hall et al 2009) fit-ting logistic models to SVM outputs (the -M option).Which classifier was used where is discussed below.4.2 Single Feature Type Classifier ResultsFirst we evaluated the performance of each fea-ture separately for the closed task by computing theAccdev values.
These results constituted the basisfor the ensembles discussed in section 4.3.
We alsoreport the corresponding results for the open-1 andopen-2 tasks, which were partly obtained after thesystem submission and thus were not used for de-veloping the approach.
As classifier, we generallyused LIBLINEAR, except for complexity and lm.realiz., where SMO performed consistently better.The summary of the single feature type performanceis shown in Table 2.The results reveal some first interesting insightsinto the employed feature sets.
The figures showthat the recurring word-based n-grams (rc.
word ng.
)taken from Bykh and Meurers (2012) are the bestperforming single feature type in our set yielding anAccdev value of 81.3%.
This finding is in line withthe previous research on different data sets showingthat lexical information seems to be highly relevantfor the task of NLI (Brooke and Hirst, 2011; Bykhand Meurers, 2012; Jarvis et al 2012; Jarvis andPaquot, 2012; Tetreault et al 2012).
But also themore abstract linguistic features, such as complexityAccdevFeature type closed open-1 open-21.
rc.
word ng.
81.3 42.0 80.32. rc.
OCPOS ng.
67.6 26.6 64.83. rc.
word dep.
67.7 30.9 69.44. rc.
func.
dep.
62.4 28.2 61.35. complexity 37.6 19.7 36.56. stemsuffix, bin.
50.3 21.4 48.87. stemsuffix, cnt.
48.2 19.3 47.18. suffix, bin.
20.4 9.1 17.59. suffix, cnt.
19.0 13.0 17.710. type dep.
lm.
67.3 25.7 67.511. type dep.
POS 46.6 27.8 27.612. local trees 49.1 26.2 25.713. dep.
num.
39.7 19.6 41.814. dep.
var.
41.5 18.6 40.115. dep.
POS 47.8 21.5 47.416. lm.
realiz.
70.3 30.3 66.9Table 2: Single feature type results on T11 dev setmeasures, local trees, or dependency variation mea-sures seem to contribute relevant information, con-sidering the random baseline of 9% for this task.Having explored the performance of the singlefeature type models, the interesting question was,whether it is possible to obtain a higher accuracythan yielded by the recurring word-based n-gramsby combining multiple feature types into a singlemodel.
We thus investigated different combinations,with a primary focus on the closed task.4.3 Combining Feature TypesWe followed Tetreault et al(2012) in exploring twooptions: On the one hand, we combined the differ-ent feature types directly in a single vector.
On theother hand, we used an ensemble classifier.
The en-semble setup used combines the probability distribu-tions provided by the individual classifier for eachof the incorporated feature type models.
The indi-vidual classifiers were trained as discussed above,and ensembles were trained and tested using LIB-SVM, which in our tests performed better for thispurpose than LIBLINEAR.
To obtain the ensembletraining files, we performed 10-fold cross-validationfor each feature model on the T11 train set (for in-ternal evaluation) and on the T11 train ?
dev set (for201submission) and took the corresponding probabilityestimate distributions.
For the ensemble test files,we took the probability estimate distribution yieldedby each feature model trained on the T11 train setand tested on the T11 dev set (for internal evalua-tion), as well as by each feature model trained onthe T11 train ?
dev set and tested on the T11 test set(for submission).In our tests, the ensemble classifier always outper-formed the single vector combination, which is inline with the findings of Tetreault et al(2012).
Wethus focused on ensemble classification for combin-ing the different feature types.4.4 Closed Task (Main) ResultsWe submitted the predictions for the systems listedin Table 3, which we chose in order to test all fea-ture types together, the best performing single fea-ture type, everything except for the best single fea-ture type, and two subsets, with the latter primarilyincluding more abstract linguistic features.id system description system type1 overall system ensemble2 rc.
word ng.
single model3 #1 minus rc.
word ng.
ensemble4 well performing subset ensemble5 ?linguistic subset?
ensembleTable 3: Submitted systems for all three tasksThe results for the submitted systems are shown inTable 4.
Here and in the following result tables, thesystem ids in the table headers correspond to the idsin Table 3, the best result on the test set is shown inbold, and the symbols have the following meaning:?
x = feature type used?
- = feature type not used?
-* = feature type ready after submissionWe report theAcctest,Accdev andAcc10train?dev ac-curacies introduced in section 4.1.
The Accdev re-sults are consistently better than the Acctest results,highlighting that relying on a single developmentset can be problematic.
The cross-validation resultsare more closely aligned with the ultimate test setperformance.systemsFeature type 1 2 3 4 51. rc.
word ng.
x x - x -2. rc.
OCPOS ng.
x - x x -3. rc.
word dep.
x - x x -4. rc.
func.
dep.
x - x x -5. complexity x - x x x6.
stemsuffix, bin.
x - x x x7.
stemsuffix, cnt.
x - x - x8.
suffix, bin.
x - x x x9.
suffix, cnt.
x - x - x10.
type dep.
lm.
x - x - x11.
type dep.
POS x - x - x12.
local trees x - x - x13.
dep.
num.
x - x x -14. dep.
var.
x - x x -15. dep.
POS x - x x -16. lm.
realiz.
x - x x -proficiency x - x x -prompt x - x x -Acctest 82.2 79.6 81.0 81.5 74.7Accdev 85.4 81.3 83.5 84.9 76.3Acc10train?dev 82.4 78.9 80.7 81.7 74.1Table 4: Results for the closed taskOverall, comparing the results for the differentsystems shows the following main points (with thesystem ids in the discussion shown in parentheses):?
The overall system performed better than anysingle feature type alone (cf.
Tables 2 and 4).The ensemble thus is successful in combiningthe strengths of the different feature types.?
The rc.
word ng.
feature type alone (2) per-formed very well, but the overall system with-out that feature type (3) still outperformed it.Thus apparently the different properties ac-cessed by more elaborate linguistic modellingcontribute some information not provided bythe surface-based n-gram feature.?
A system incorporating a subset of the differ-ent feature types (4) performed still reasonablywell.
Hence, it is conceivable that a subsys-tem consisting of some selected feature typeswould perform equally well (eliminating onlyinformation present in multiple feature types)or even outperform the overall system (by re-moving some noise).
This point will be inves-tigated in detail in our future work.202?
System 5, combining a subset of feature types,where each one incorporates some degreeof linguistic abstraction (in contrast to puresurface-based feature types such as word-basedn-grams), performed at a reasonably high level,supporting the assumption that incorporatingmore linguistic knowledge into the system de-sign has something to contribute.Putting our results into the context of the NLIShared Task 2013, with our best Acctest value of82.2% for closed as the main task, we ranked fifthout of 29 participating teams.
The best result inthe competition, obtained by the team ?Jarvis?, is83.6%.
According to the significance test resultsprovided by the shared task organizers, the differ-ence of 1.4% is not statistically significant (0.124for pairwise comparison using McNemar?s test).4.5 Open-1 Task ResultsThe Accdev values for the single feature type modelsfor the open-1 task were included in Table 2.
Theresults for the test set are presented in Table 5.
Wereport two different Acctest values: the accuracy forthe actual submitted systems (Acctest) and for thecorresponding complete systems (Acctest with ?)
asdiscussed in section 4.1.systemsFeature type 1 2 3 4 51. rc.
word ng.
x x - x -2. rc.
OCPOS ng.
x - x x -3. rc.
word dep.
x - x x -4. rc.
func.
dep.
x - x x -5. complexity x - x x x6.
stemsuffix, bin.
x - x x x7.
stemsuffix, cnt.
x - x - x8.
suffix, bin.
x - x x x9.
suffix, cnt.
x - x - x10.
type dep.
lm.
-?
- -?
- -?11.
type dep.
POS -?
- -?
- -?12.
local trees -?
- -?
- -?13.
dep.
num.
x - x x -14. dep.
var.
x - x x -15. dep.
POS x - x x -16. lm.
realiz.
x - x x -Acctest 36.4 38.5 33.2 37.8 21.2Acctest with ?
37.0 n/a 35.4 n/a 29.9Table 5: Results for the open-1 taskConceptually, the open-1 task is a cross-corpustask, where we used the NT11 data for training andT11 data for testing.
It is more challenging for sev-eral reasons.
First, the models are trained on datathat is likely to be different from the one of thetest set in a number of respects, including possibledifferences in genre, task and topic, or proficiencylevel.
Second, the amount of data we were able toobtain to train our model is far below what was pro-vided for the closed task.
Thus a drop in accuracy isto be expected.Particularly interesting is the fact that our best re-sult for the open-1 task (38.5%) was obtained usingthe rc.
word ng.
feature type alone.
Thus addingthe more abstract features did not improve the accu-racy.
The reason for that may be the smaller train-ing corpus size, the uneven distribution of the textsamong the different L1s in the NT11 corpus, or thementioned potential differences between NT11 andT11 in genre, task and topic, and learner proficiency.Also interesting is the fact that the system combininga subset of feature types outperformed the overallsystem.
This finding supports the assumption men-tioned in section 4.4 that the ensemble classifier canbe optimized by informed, selective model combina-tion instead of combining all available information.To put our results into the context of the NLIShared Task 2013, our best Acctest value of 38.5%for the open-1 task achieved rank two out of threeparticipating teams.
The best accuracy of 56.5% wasobtained by the team ?Toronto?.
While the open-1 task results in general are much lower than theclosed task results, highlighting an important chal-lenge for future NLI work, they nevertheless aremeaningful steps forward considering the randombaseline of 9%.4.6 Open-2 Task ResultsFor the open-2 task we provide the same informationas for open-1.
The Accdev values for the single fea-ture type models are shown in Table 2, and the twoAcctest values, i.e., the accuracy for the actual sub-mitted systems (Acctest) and for the complete sys-tems (Acctest with ?)
can be found in Table 6.For the open-2 task, we put the T11 train ?dev and NT11 sets together to train our models.
Theinteresting question behind this task is, whether it ispossible to improve the accuracy of NLI by adding203systemsFeature type 1 2 3 4 51. rc.
word ng.
x x - x -2. rc.
OCPOS ng.
x - x x -3. rc.
word dep.
-?
- -?
-?
-4. rc.
func.
dep.
x - x x -5. complexity x - x x x6.
stemsuffix, bin.
x - x x x7.
stemsuffix, cnt.
x - x - x8.
suffix, bin.
x - x x x9.
suffix, cnt.
x - x - x10.
type dep.
lm.
-?
- -?
- -?11.
type dep.
POS x - x - x12.
local trees x - x - x13.
dep.
num.
x - x x -14. dep.
var.
x - x x -15. dep.
POS x - x x -16. lm.
realiz.
x - x x -Acctest 83.5 81.0 79.3 82.5 64.8Acctest with ?
84.5 n/a 83.3 82.9 79.8Table 6: Results for the open-2 taskdata from corpora other than the one used for test-ing.
This is far from obvious, especially consideringthe low results obtained for the open-1 task pointingto significant differences between the T11 and theNT11 corpora.Overall, when using all feature types, our resultsfor the open-2 task (84.5%) are better than those weobtained for the closed task (82.2%).
So adding datafrom a different domain improves the results, whichis encouraging since it indicates that something gen-eral about the language used is being learned, not(just) something specific to the T11 corpus.
Essen-tially, the open-2 task also is closest to the real-worldscenario of using whatever resources are available toobtain the best result possible.Putting the results into the context of the NLIShared Task 2013, our best Acctest value of 83.5%(84.5%) is the highest accuracy for the open-2 task,i.e, first rank out of four participating teams.5 ConclusionsWe explored the task of Native Language Identifi-cation using a range of different feature types in thecontext of the NLI Shared Task 2013.
We consid-ered surface features such as recurring word-basedn-grams system as our basis.
We then exploredthe contribution and usefulness of some more elab-orate, linguistically motivated feature types for thegiven task.
Using an ensemble model combiningfeatures based on POS, dependency, parse trees aswell as lemma realization, complexity and suffix in-formation features, we were able to outperform thehigh accuracy achieved by the surface-based recur-ring n-grams features alone.
The exploration oflinguistically-informed features thus is not just ofanalytic interest but can also make a quantitative dif-ference for obtaining state-of-the-art performance.In terms of future work, we have started exploringthe various feature types in depth to better under-stand the causalities and correlations behind the re-sults obtained.
We also intend to explore more com-plex linguistically motivated features further, suchas features based on syntactic alternations as used inKrivanek (2012).
Studying such variation of linguis-tic properties, instead of recording their presence aswe mostly did in this exploration, also stands to pro-vide a more directly interpretable perspective on thefeature space identified as effective for NLI.AcknowledgmentsWe thank Dr. Shin?ichiro Ishikawa and Dr. MickRandall for providing access to the ICNALE corpusand the BALC corpus respectively.
We also thankthe shared task organizers for organizing this inter-esting competition and sharing the TOEFL11 cor-pus.
Our research is partially funded through the Eu-ropean Commission?s 7th Framework Program un-der grant agreement number 238405 (CLARA).ReferencesR.
H. Baayen, R. Piepenbrock, and L. Gulikers.
1995.The CELEX lexical database (cd-rom).
CDROM,http://www.ldc.upenn.edu/Catalog/readme_files/celex.readme.html.Daniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill, and Martin Chodorow.
2013.
TOEFL11: Acorpus of non-native english.
Technical report, Edu-cational Testing Service.Bernd Bohnet.
2010.
Top accuracy and fast dependencyparsing is not a contradiction.
In Proceedings of the24th International Conference on Computational Lin-guistics (COLING), pages 89?97.Julian Brooke and Graeme Hirst.
2011.
Native lan-guage detection with ?cheap?
learner corpora.
In204Learner Corpus Research 2011 (LCR 2011), Louvain-la-Neuve.Serhiy Bykh and Detmar Meurers.
2012.
Native lan-guage identification using recurring n-grams ?
in-vestigating abstraction and domain dependence.
InProceedings of the 24th International Conference onComputational Linguistics (COLING), pages 425?440, Mumbay, India.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher Manning.
2006.
Generating typed depen-dency parses from phrase structure parses.
In Proceed-ings of the 5th International Conference on LanguageResources and Evaluation (LREC-2006), Genoa, Italy,May 24-26.R.E.
Fan, K.W.
Chang, C.J.
Hsieh, X.R.
Wang, and C.J.Lin.
2008.
Liblinear: A library for large linear classi-fication.
The Journal of Machine Learning Research,9:1871?1874.
Software available at http://www.csie.ntu.edu.tw/?cjlin/liblinear.Sylviane Granger, Estelle Dagneaux, Fanny Meunier, andMagali Paquot, 2009. International Corpus of LearnerEnglish, Version 2.
Presses Universitaires de Louvain,Louvain-la-Neuve.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: An update.
InThe SIGKDD Explorations, volume 11, pages 10?18.Shin?ichiro Ishikawa.
2011.
A new horizon in learnercorpus studies: The aim of the ICNALE projects.
InG.
Weir, S. Ishikawa, and K. Poonpon, editors, Cor-pora and language technologies in teaching, learningand research, pages 3?11.
University of StrathclydePublishing, Glasgow, UK.
http://language.sakura.ne.jp/icnale/index.html.Scott Jarvis and Magali Paquot.
2012.
Exploring therole of n-grams in L1-identification.
In Scott Jarvisand Scott A. Crossley, editors, Approaching LanguageTransfer through Text Classification: Explorations inthe Detection-based Approach, pages 71?105.
Multi-lingual Matters.Scott Jarvis, Gabriela Castan?eda-Jime?nez, and RasmusNielsen.
2004.
Investigating L1 lexical transferthrough learners?
wordprints.
Presented at the 2004Second Language Research Forum.
State College,Pennsylvania, USA.Scott Jarvis, Gabriela Castan?eda-Jime?nez, and RasmusNielsen.
2012.
Detecting L2 writers?
L1s on thebasis of their lexical styles.
In Scott Jarvis andScott A. Crossley, editors, Approaching LanguageTransfer through Text Classification: Explorations inthe Detection-based Approach, pages 34?70.
Multilin-gual Matters.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005.Determining an author?s native language by mining atext for errors.
In Proceedings of the eleventh ACMSIGKDD international conference on Knowledge dis-covery in data mining (KDD ?05), pages 624?628,New York.Julia Krivanek.
2012.
Investigating syntactic alternationsas characteristic features of learner language.
Master?sthesis, University of Tu?bingen, April.Victor Kuperman, Hans Stadthagen-Gonzalez, and MarcBrysbaert.
2012.
Age-of-acquisition ratings for30,000 english words.
Behavior Research Methods,44(4):978?990.Roger Levy and Galen Andrew.
2006.
Tregex and tsur-geon: tools for querying and manipulating tree datastructures.
In 5th International Conference on Lan-guage Resources and Evaluation, Genoa, Italy.Xiaofei Lu.
2010.
Automatic analysis of syntacticcomplexity in second language writing.
InternationalJournal of Corpus Linguistics, 15(4):474?496.Xiaofei Lu.
2012.
The relationship of lexical richnessto the quality of ESL learners?
oral narratives.
TheModern Languages Journal.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Human Language Tech-nologies 2007: The Conference of the North AmericanChapter of the Association for Computational Linguis-tics; Proceedings of the Main Conference, pages 404?411, Rochester, New York, April.John C. Platt.
1998.
Sequential minimal optimiza-tion: A fast algorithm for training support vector ma-chines.
Technical Report MSR-TR-98-14, MicrosoftResearch.Mick Randall and Nicholas Groom.
2009.
The BUiDArab learner corpus: a resource for studying the ac-quisition of L2 english spelling.
In Proceedings of theCorpus Linguistics Conference (CL), Liverpool, UK.Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-tin Chodorow.
2012.
Native tongues, lost and found:Resources and empirical evaluations in native lan-guage identification.
In Proceedings of the 24th In-ternational Conference on Computational Linguistics(COLING), pages 2585?2602, Mumbai, India.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013.A report on the first native language identificationshared task.
In Proceedings of the Eighth Workshopon Building Educational Applications Using NLP, At-lanta, GA, USA, June.
Association for ComputationalLinguistics.205Laura Mayfield Tomokiyo and Rosie Jones.
2001.You?re not from round here, are you?
naive bayes de-tection of non-native utterance text.
In Proceedings ofthe 2nd Meeting of the North American Chapter of theAssociation for Computational Linguistics (NAACL),pages 239?246.Sowmya Vajjala and Detmar Meurers.
2012.
On im-proving the accuracy of readability classification us-ing insights from second language acquisition.
In JoelTetreault, Jill Burstein, and Claudial Leacock, editors,Proceedings of the 7th Workshop on Innovative Useof NLP for Building Educational Applications (BEA7)at NAACL-HLT, pages 163?-173, Montre?al, Canada,June.
Association for Computational Linguistics.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automati-cally grading ESOL texts.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies- Volume 1, HLT ?11, pages 180?189, Stroudsburg,PA, USA.
Association for Computational Linguistics.Corpus available from http://ilexir.co.uk/applications/clc-fce-dataset.206
