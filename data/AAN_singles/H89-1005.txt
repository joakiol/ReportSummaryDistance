SPEAKER INDEPENDENT PHONETIC TRANSCRIPTION OF FLUENT SPEECH FORLARGE VOCABULARY SPEECH RECOGNITIONS.
E. LevinsonM.
Y. LibermanA.
LjoljeandL.
G. MillerAT&T Bell LaboratoriesMurray Hill, New Jersey 07974ABSTRACTSpeaker independent phonetic Iranscription of fluent speech is performed using an ergodiccontinuously variable duration hidden Markov model (CVDHMM) to represent the acoustic,phonetic and phonotactic structure of speech.
An important property of the model is that eachof its fifty-one states is uniquely identified with a single phonetic unit.
Thus, for any spokenutterance, a phonetic transcription is obtained from a dynamic programming (DP) procedure forfinding the state sequence of maximum likelihood.
A model has been constructed based on4020 sentences from the TIMIT database.
When tested on 180 different sentences from thisdatabase, phonetic accuracy was observed to be 56% with 9% insertions.
A speaker dependentversion of the model was also constructed.
The transcription algorithm was then combinedwith lexical access and parsing routines to form a complete recognition system.
When testedon sentences from the DARPA resource management task spoken over the local switchedtelephone network, phonetic accuracy of 64% with 8% insertions and word accuracy of 87%with 3% insertions was measured.
This system is presently operating in an on-line mode overthe local switched telephone network in less than ten times real time on an Alliant FX-80.INTRODUCTIONThough rarely explicitly stated, a fundamental ssumption on which many speech recognitionsystems are implicitly based is that speech is literate.
That is, it is a code for communicationhaving a small number of discrete phonetic symbols in its alphabet.
These symbols are,however, merely mental constructs and, as such, are not directly accessible but are, instead,observable only in their highly variable acoustic manifestation.
It is also well-known butequally seldom expressed that a hidden Markov model comprises a finite set of discreteinaccessible states observable only via a set of random processes, one associated with eachhidden state.
When these two simple ideas are juxtaposed, it seems to us inescapable that themost natural representation of speech by a hidden Markov model is one in which thehypothetical phonetic symbols are identified with the hidden states of the Markov chain and thevariability of the measurable acoustic signal is captured by the observable, state-dependentrandom processes.75The mathematical details of just such a model are given in \[6\].
Its application to a small-vocabulary continuous speech recognition system and a large-vocabulary isolated wordrecognition system are described in \[7\] and \[8\], respectively.
Here we present a brief overviewof the use of this approach in large vocabulary continuous speech recognition and somepreliminary results of two experiments performed with it on the TIMIT \[4\] and DARPA \[9\]databases.THE MODELWe have constructed two models, a 51 state model on which the speaker-independent phonetictranscription results are based, and a 43 state model on which the speaker-dependentrecognition of sentences from the DARPA resource management task are founded.
The 51states in the first model correspond to 51 of the phonetic symbols used in the standardtranscriptions of the TIMIT sentences.
The 43 states of the second model are associated withthe 43 symbols used in the pronunciation guide of the Collins English dictionary \[1\].
Thephonetic units are listed in figure 1.
Flap and closure units are not included in the 43 statemodel.STATE TIMIT COLLINS EXPLANATION1 h# pau + silence2 eh e bet3 ao > bought4 aa @ cot5 uw ux U boot6 er R bird7 ay I bite8 ey A bait9 aw W now10 ax & schwa11 ill i bit12 ae a bat13 ah ^ butt14 uh u book15 oy Y boy16 iy E beat17 ow O boat18 axr # diner19 1 1 led20 el " bottle21 r r red22 w w wet23 y y yet24 hh hv h hay25 s s sister26 sh S shoe27 z z zoo7628 zh Z29 ch C30 jh J31 th T32 dh D33 f f34 v v35 m em m36 nen  n37 ng eng N38 nx \[39 p p40 t t41 k k42 pcl 143 tcl 244 kcl 345 dx \]46 b b47  d d48 g g49 bcl 450 dcl 551 gcl 6measurechurchjudgethieftheyfoodvervemomnunsingnasal flappoptotkickp closuret closurek closurealveolar flapbobdadgagb closured closureg closureFigure 1: Phonetic Units and SymbolsBoth models are of the same form, CVDHMM, as described in the reference cited earlier.
Thestate transition matrices define ergodic Markov chains and weakly capture the phonotacticstructure of English.
The acoustic measurements are represented by 26-dimensional Gaussiandensity functions.
The first twelve coordinates are LPC based cepstra; the second twelve,delta-cepstra \[2\], and the last two, log energy and its time-derivative, respectively.
Thetemporal structure of the acoustic signal is reflected in the durational densities which are of thetwo-parameter gamma family.
Because of the presence of the durational densities, self-transitions are forbidden.PARAMETER ESTIMATIONThe parameters for both the 51 and 43 state models were estimated in the same way althoughon different raining data.
In both cases, the state transition matrix was computed from bigramstatistics extracted from the Collins dictionary.
No attempt was made to count bigramsresulting from word junctures.
Also, in both cases, the respective databases were segmented byhand and labeled with respect o the appropriate phonetic alphabet.
Acoustic observations weresorted into sets corresponding to the phonetic symbols.
The necessary parameters, pectralmeans and covariances and durational means and standard eviations, were then calculated foreach set independently.
No parameter optimization was applied to these estimates.77The 51 state speaker-independent model was trained on 4200 sentences of TIMIT data.
Tendifferent sentences were selected from each of 402 different speakers.
The 43 state speaker-dependent model was trained on one reading of the 450 sentences in the TIMIT phoneticallybalanced list by a single male speaker.
These utterances were recorded over the local switchedtelephone network with a conventional telephone handset.At this writing, we have yet to train a speaker-independent model using the DARPA trainingmaterial.
Although we expect o do so, we are concerned about its utility since the phoneticcontexts in this database are rather estrictive compared with those of the TIMIT sentences.PHONETIC TRANSCRIPTIONPhonetic transcription is accomplished by means of a DP technique for finding the statesequence that maximizes the joint likelihood of state, duration and observation sequences.
Thedetails of this algorithm are given in \[7\].
Note that this procedure makes no use of lexical orsyntactic structure.
The algorithm runs in approximately twice real time on an Alliant FX-80.EXPERIMENTAL RESULTS ON TRANSCRIPTIONThe transcription algorithm was tested on 180 sentences from the TIMIT database.
Neither thesentences nor the speakers were used in ihe training.
Transcription accuracy was determinedby computing the Levenshtein distance between the derived transcription and the standardtranscription supplied with the database.
By this measure, the 51 state model yielded aphonetic recognition rate of 56% with a 9% insertion rate.
The 43 state model resulted in a64% recognition rate with an 8% insertion rate on 48 sentences from the DARPA taskcollected from the male speaker on whose speech the model had been trained.The reader should bear in mind that these are the very first experiments performed with thissystem.
We fully expect hat the performance will improve greatly as a result of refinementswe are presently making to the model.
These include accounting for coarticulation, making thedurational densities more faithful and using parameter reestimation techniques.THE SPEECH RECOGNITION SYSTEMThe phonetic transcription algorithm described above is the first stage of a complete speechrecognition system.
The architecture of the system is unchanged from that described in \[8\] butthe details of the lexical access procedure and the parser are utterly different from those givenin the reference.The lexical access procedure is simply that of computing the likelihood of every word in thelexicon over every sub-interval of the observation sequence.
We define the likelihood of aword on an observation sub-sequence to be the joint likelihood of the standard phonetictranscription for that word as given in the lexicon and the phonetic transcription of thatsubsequence provided by the transcription algorithm.
Because the standard transcription eednot have the same length as the one computed for an arbitrary observation sub-sequence, thecalculation is carried out by means of a DP algorithm.
Note that this procedure issynchronized atthe segment rate, not the frame rate.78The parser takes as input, the word lattice constructed by the lexical access procedure and findsthe well-formed sentence of maximum likelihood.
Here, well-formed means with respect o thestrict DARPA resource management task grammar.
This is a finite state grammar having 4767states, 60433 state transitions, 90 final states and a maximum entropy of 4.4 bits/word.
Theparser itself is yet another DP algorithm.
The search it effects is not pruned in any way.The system has been tested in an on-line mode over the switched local telephone network.Under these conditions, we obtained an 87% correct word recognition rate and a 3% insertionrate.
On an Alliant FX-80, a sentence is recognized in less than ten times real time.
A sampleof the recognizer output is shown in figure 2.PHONETIC TRANSCRIPTION: h@riRriEZUzpl>grUDENw^ndTWz&ndSEdED&nd>rTtUsiZ&kObS&nDURATIONS:5 5 7 4 8 8 7 51017 912 4 613 8 8 7 6 9 7 6 6 7 49 1910 3 6 3 11 17 5 12 4 4 5 3 9 6 7 6 7 14 5 8 3 10 1231375LOG LIKELIHOOD = 0.23880190715663E+04POSITION BEGIN END STATE LOG LIKELIHOOD WORD1 49 53 19 0.2250650E+02 ocean2 42 48 394 0.2147619E+02 pacific3 37 41 344 0.1887782E+02 north4 35 36 265 0.1787559E+02 the5 34 34 378 0.1733334E+02 in6 30 33 299 0.1590989E+02 feet7 24 29 926 0.1379514E+02 thousand8 21 23 838 0.1118440E+02 one9 18 20 758 0.1093698E+02 than10 13 17 691 0.9208550E+01 longer11 9 12 623 0.6723166E+01 ships12 6 8 557 0.4362227E+01 any13 3 5 513 0.3019794E+01 there14 1 2 491 0.1371470E+01 areRECOGNIZED SENTENCE: are there any ships longer than one thousand feet in thenorth pacific oceanLOG LIKELIHOOD = 0.22506502151489E+02RECOGNITION TIME = 49.78 CPU-SECONDSFigure 2: Sample of Sentence Recognition ResultsCONCLUSION79We have presented some very early results of experiments on phonetic transcription andrecognition of fluent speech based on a novel use of a hidden Markov model.
While our errorrates are substantially higher than those achieved by more conventional systems \[5,3,10\], webelieve that by improving the acoustic/phonetic model - the only adjustable part of the system -results comparable to those obtained by other investigators can be realized.References\[U\[21\[3\]\[4\]\[5\]\[6\]\[7\]\[8\]\[9\]\[lO\]Hanks, P., ed., Collins Dictionary of the English Language,Collins, London, 1972.Juang, B. H., Rabiner, L. R. and Wilpon, J. G., "On the use ofBandpass Liftering in Speech Recognition", IEEE Trans.
Acoust.Speech and Signal Processing, ASSP-35 (7), pp.
947-954, July, 1987.Kubala, F. et al, "Continuous Speech Recognition Results of theBYBLOS System on the DARPA 1000-Word Resource Management Database",Proc.
ICASSP-88, New York, NY, pp.
291-294, April, 1988.Lamel, L. F., Kassel, R. H. and Seneff, S., "Speech Database Develop-ment: Design and Analysis of the Acoustic-Phonetic Corpus", Proc.
DARPASpeech Recognition Workshop, Palo Alto, CA, pp.
100-109, Feb., 1986.Lee, K. F. and Hon, H. W., "Large Vocabulary Speaker-Independent SpeechRecognition System using HMM", Proc.
ICASSP-88, New York, NY, pp.
123126, April, 1988.Levinson, S. E., "Continuously Variable Duration Hidden Markov Modelsfor Automatic Speech Recognition", Computer Speech and Language 1 (1),pp.
29-45, 1986.Levinson, S. E., "Continuous Speech Recognition by means of Acoustic-Phonetic Classification Obtained from a Hidden Markov Model", Proc.ICASSP-87, Dallas, TX, pp.
93-96, April, 1987.Levinson, S. E., Ljolje, A. and Miller, L. G., "Large Vocabulary SpeechRecognition using a Hidden Markov Model for Acoustic Phonetic Classif-ication", Proc.
ICASSP-88, New York, NY, pp.
505-508, April, 1988.Price, P., Fisher, W., Bernstein, J. and Pallett, D., "The DARPA 1000-Word Resource Management Database for Continuous Speech Recognition",Proc.
ICASSP-88, New York, NY, pp.
651-654, April, 1988.Pieraccini, R., Lee, C. H., Rabiner, L. R. and Wilpon, J. G., "SomePreliminary Results on Speaker Independent Recognition of the DARPAResource Management Task", in this proceedings.80
