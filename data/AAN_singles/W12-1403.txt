JEP-TALN-RECITAL 2012, Atelier ILADI 2012: Interactions Langagi?res pour personnes Ag?es Dans les habitats Intelligents, pages 17?30,Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCPInteractions sonores et vocales dans l?habitatPierrick Milhorat1, Dan Istrate3, J?r?me Boudy2, G?rard Chollet1(1) T?l?com ParisTech, 37-39 rue Dareau, 75014, Paris, France(2) T?l?com SudParis, 9 rue Charles Fourier, 91011 Evry Cedex, France(3) ESIGETEL, 1 Rue du Port de Valvins, 77210 Avon Cedex, Francemilhorat@telecom-paristech.fr, dan.istrate@esigetel.fr,jerome.boudy@it-sudparis.eu, chollet@telecom-paristech.frR?SUM?___________________________________________________________________________________________________________Cet article pr?sente le syst?me de reconnaissance son/parole en continu d?velopp?
et?valu?
dans le cadre du projet  europ?en CompanionAble.
Ce syst?me analyse le fluxsonore en continu, d?tecte et reconna?t des sons de la vie quotidienne et des commandesvocales gr?ce ?
un microphone.
L?architecture et la description de chaque module sontd?taill?es.
Des contraintes ont ?t?
impos?es ?
l?utilisateur et au concepteur, telle que lalimitation du vocabulaire, dans le but d?obtenir des taux de reconnaissance et de rejetacceptables.
Les  premiers  r?sultats  sont  pr?sent?s,  les  essais  finaux sur le  terrain duprojet sont en cours dans une maison intelligente ?
Eindhoven.ABSTRACT__________________________________________________________________________________________________________Acoustic Interaction At HomeThis  paper  describes  a  hands-free  speech/sound  recognition  system  developed  andevaluated in the framework of the European Project CompanionAble.
The system is ableto work continuously on a distant microphone and detect not only vocal commands butalso everyday life sounds.
The proposed architecture and the description of each moduleare outlined.
In order to have good recognition rate some constraints were defined forthe user and the vocabulary was limited.
First results are presented; currently projecttrials are underway.MOTS-CL?S: reconnaissance  vocale,  traitement  du  son,  reconnaissance  des  sons,domotique.KEYWORDS: hands-free speech recognition, sound processing, sound recognition, domotics.171.
IntroductionLe projet europ?en CompanionAble a pour objectif l?int?gration d?un robot compagnondans  une  maison  intelligente  ?
destination  de  seniors  d?pendants.
Le  robot  sertd?interface  entre  les  fonctionnalit?s  de  la  maison  (allumage/extinction  des  lumi?res,ouverture/fermeture  des  rideaux,  lecture/pause  de  la  cha?ne  Hi-Fi...)  ainsi  qued?assistant ?
la vie quotidienne.
A l?aide de capteurs diss?min?s dans l?habitat (capteursinfra-rouges, capteurs d?ouverture de porte...) et de ses propres informations (cam?ra,ultrasons...),  il  assiste  les  r?sidents  suivant  des  sc?narios  pr?d?finis  (entr?e  dans  lamaison,  sortie,  appel  en  visioconf?rence...)  ou  d?finis  par  l?utilisateur  (rappel  del?agenda, alerte de prise de m?dicaments, objets plac?s dans les paniers du robot...).Dans ce contexte, le robot, porteur d?un ?cran tactile, un ?cran interactif install?
dans lacuisine  et  une  tablette  portative  sont  ?quip?s  d?une  application  graphique  identiquepr?sentant toutes les fonctionnalit?s disponibles.L?Institut Mines-T?l?com est en charge de porter l?interaction vers une communicationvocale.
Un ensemble de commandes domotiques d?riv?es d?exp?rimentation pratiques a?t?
?tablis auxquelles le syst?me d?analyse acoustique doit r?agir.L?interaction avec les applications autres telles que l?agenda, les exercices cognitifs ou lecontr?le du robot a ?galement fait l?objet de d?finitions de commandes.
Dans les deuxcas, les commandes ne sont pas uniquement des mots mais des phrases compl?tes.De nombreux travaux ont port?
sur la reconnaissance vocale et les performances dessyst?mes commerciaux actuels d?montrent leur g?n?ralisation ?
venir.
La sp?cificit?
denos travaux inclus dans ce projet r?side dans la r?solution du probl?me de la distance dumicrophone au locuteur.
Celui-ci, unique, se trouve int?gr?
au robot, soit ?
environ unm?tre  du  sol  et  mobile.
La  distance  au  locuteur  et  le  bruit  environnant  sontincontr?lables et variables.
Les travaux de soustraction du bruit ?
l?aide de microphonesenregistrant les sources de bruits ne s?appliquent pas aux conditions variables rencontr?slors des ?tudes d?usage pr?liminaires.La deuxi?me section de cet article d?crit le projet CompanionAble.
Les sections 3 et 4sont consacr?es ?
la reconnaissance des sons.
Vient ensuite la description du syst?me dereconnaissance de la parole utilis?
et adapt?
?
ce contexte, en section 5.
La sections 6pr?sente les ?valuations du syst?me.
Les futurs axes de recherche et conclusions tir?s dece projet seront exprim?s dans la derni?re partie.182.
CompanionAbleCompanionAble est l?acronyme de Integrated Cognitive Assistive & Domotic CompanionRobotic Systems for Ability & Security.C?est  un  projet  financ?
par  la  commission  europ?enne  qui  r?unit  18  partenairesacad?miques et industriels.Les objectifs sont les suivants :-  combiner  les  capacit?s  d?un  robot  ?
compagnon ?
mobile  avec  lesfonctionnalit?s statiques d?un environnement intelligent.- int?grer les donn?es des capteurs de l?habitat ?
celles du robot- cr?er un lien social entre les s?niors et leurs proches et/ou leurs entouragem?dical- am?liorer la qualit?
de vie et l?autonomie des personnes d?pendantesLes partenaires sont localis?s dans 7 pays, ?
savoir : la France, l?Allemagne, l?Espagne,l?Autriche, la Belgique, les Pays-Bas et le Royaume-Unis.L?Institut Mines-T?l?com (anciennement Groupe des Ecoles des T?l?communications) apour r?le l?addition d?une interface vocale, d?un module multimodal de d?tection dessituations  de  d?tresse  et  participe  ?galement  ?
la  localisation  des  personnes  dansl?habitat.
Cet article se concentre sur la partie acoustique des travaux.Actuellement, le projet est entr?
dans une phase d?exp?rimentation pratique en situationr?elles.
Cela  se  d?roulera  ?
Eindhoven  (Pays-Bas)  et  ?
Gits  (Belgique)  un  paneld?utilisateurs potentiels sera amen?
?
tester le syst?me complet.3.
Architecture de traitement du sonLe son est acquis en continu par deux syst?mes parall?les : l?un attribue un type au son(parole/son et type de son) tandis que l?autre transcrit la parole en texte.
La figure 1montre la communication entre les modules sonores qui utilise un protocole TCP/IP.
Lessons reconnus ou les commandes vocales sont transmises au serveur du projet via unprotocole SOAP.
La reconnaissance vocale est filtr?e par le module de reconnaissance dessons pour ?viter les fausses alarmes (faux-positifs).19Figure 1 ?
Architecture de traitement des sons4.
Reconnaissance des sonsLe syst?me de reconnaissance sonore consiste dans notre application en une s?quence dedeux proc?d?s : un module de d?tection bas?
sur unetransform?e en ondelettes et unmodule de reconnaissance hi?rarchique (son/parole et classification des sons) bas?
surdes GMM (Rougui, 2009).Les classes de sons utilis?es lors des essais CompanionAble ont ?t?
apprises avec desenregistrements effectu?s avec un CMT (microphone produit par AKG) (Rougui, 2009)dans la maison SmartHomes ?
Eindhoven.
Actuellement, le syst?me dispose de 5 classesde son : chute d?objets, sonnette, cl?s, toux et applaudissements.
Les classes de son ont?t?
choisies pour la d?tection de situations de d?tresse et d?actions non parl?es utilespour le syst?me domotique.La sortie de la reconnaissance vocale est filtr?e par le module de reconnaissance des sonspour emp?cher une fausse commande associ?e ?
un son non parl?.
Comme les deuxmodules  tournent  en  parall?le,  une  synchronisation  est  requise.
Le  module  sonoreenregistre constamment les trois derni?res d?cisions d?
?tiquetage sons/parole et d?cidede valider  ou rejeter la reconnaissance vocale en fonction de la corr?lation entre lesdeux.Chaque module a ?t?
initialement ?valu?
sur des bases de donn?es.
Les r?sultats de laclassification  des  sons  selon  15  classes  sont  de  80%  de  bonne  reconnaissance  .
Lareconnaissance  parole/son  a  ?t?
et  sera  ?valu?e  dans  la  maison  SmartHomes ;  lespremiers r?sultats ont montr?
un taux avoisinant les 95%5.
Reconnaissance de la paroleConcernant  le  volet  reconnaissance  vocale,  il  se  justifie,  au  sein  du  projet  par  ladifficult?, voire l?incapacit?
d?une grande partie des utilisateurs cibles d?interagir avec unsyst?me informatique par le biais de menus sur des ?crans tactiles.
Les troubles cognitifs20ou des probl?mes de mobilit?
trop importants rendraient le syst?me obsol?te s?il n?
?taitpas dot?
d?une interface vocale ?
distance.
Les commandes interpr?tables portent surl?interaction ?
face au robot ?
combin?
avec l?affichage graphique et sur les interactions ?distance.
Les trois probl?matiques auxquelles nous proposons une solution sont :- la reconnaissance de commandes vocales dans un environnement bruit?
pourlequel les sources de bruit sont inconnues- la reconnaissance de commandes vocales ?
distance variable- la reconnaissance de commandes vocales toujours activeLes  centres d?exp?rimentation bas?s aux Pays-Bas et en Belgique flamande contraignentle projet ?
r?aliser l?interface vocale du robot en hollandais.Julius,  d?velopp?
par  le  Kawahara  lab  de  Tokyo,  a  ?t?
s?lectionn?
comme ?tant  led?codeur le plus appropri?
pour une application basique ?tat de l?art  (Lee, 2008).
Ilpermet une reconnaissance sur un large vocabulaire (60 000 mots) en temps quasi-r?elgr?ce ?
un algorithme ?
deux passes.
Il s ?appuie sur des mod?les de langage N-grams etdes  mod?les  acoustiques  encod?es  sous  forme  de  mod?les  de  Markov  cach?s.
Lamodularit?
de ce moteur de reconnaissance permet de traiter une m?me entr?e (audio)avec plusieurs mod?les (acoustiques et de langage) diff?renci?s selon les besoins.Les  mod?les  de  Markov  cach?s  des  phon?mes  composant  le  mod?le  acoustiquehollandais ont ?t?
appris sur le Corpus Gesproken Nederlands (CGN).
Cela repr?sente800 heures d?enregistrements audio transcrits dans lesquelles presque 9 millions de motssont prononc?s, faisant du CGN le plus grand corpus pour le hollandais contemporain.Les sources sont r?parties entre des sources monolocuteurs et multilocuteurs, prompt?esou spontan?es.Fran?ais Fen?tre HollandaisNon GoodBye Frame NeeHector Main Frame HectorJe reviens tout de suite GoodBye Frame Ik ben zo terug/Ik ga niet weg/Zo terugQuelques jours GoodBye Frame Een paar dagen/Paar dagenEnviron une heure GoodBye Frame Een uurtje/Een uur/UurAffiche lesappelsmanqu?sGreeting Frame Gemiste oproepen/Laat gemiste oproepenAffiche la liste deschoses ?
faire Greeting Frame Taken/Laat taken zien/Start takenTable 1 ?
Exemple de commandes vocales21?tant donn?es les conditions impos?es (toujours actif, distance au microphone variable,vari?t?
des  fonds  sonores,  etc...),  des  solutions  ont  ?t?
propos?es  pour  am?liorer  larobustesse du syst?me.?
L?attention ?Le gestionnaire du dialogue propose un moyen de limiter le nombre de faux-positifs avecl?utilisation  d?un  mot  ?
d?attention ?.
Ce  mot,  quand il  est  d?tect?,  accro?t  le  niveaud?attention  qui  d?cro?t  avec  le  temps.
Un  niveau  d?attention  non  nul  d?clenche  letraitement et l?analyse des donn?es de la reconnaissance.
Par exemple, ?
l?
?tat initial, leniveau d?attention est nulle : le module de reconnaissance vocale est toujours actif ettransmet  ces  r?sultats,  tant  que  le  mot  cl?
n?est  pas  d?tect?,  les  transcriptions  sontignor?es par le gestionnaire de dialogue.
D?s lors que le niveau d?attention est sup?rieur?
0, le dialogue s?engage, et le gestionnaire interpr?te toutes les commandes re?ues.
Tantque le dialogue est soutenu (soit par r?p?tition du mot d?attention, soit par une ?volutiondu dialogue), le niveau d?attention s?accro?t alors que les silences, du point de vue dugestionnaire de dialogue diminuent la variable d?attention qui, si elle atteint sa valeurplancher  (nulle)  coupe  l?interpr?tation.
Le  choix  d?un  mot  d?attention  le  plusdiscriminatoire possible augmente l?efficacit?
d?un tel m?canisme.La classification sons/paroleUn moteur de reconnaissance vocale tel que Julius cherche la s?quence de mots quicorrespond  au  mieux  ?
la  s?quence  de  vecteurs  acoustiques  pr?sent?e  selon  lesprobabilit?s contenues dans la combinaison des mod?les acoustiques et de langage.
Il estpossible de cr?er un mot ?
poubelle ?
qui remplacerait l?ensemble des mots dont le scorede  reconnaissance  serait  trop  faible.
Dans  notre  application,  nous  avons  choisi  ded?coder syst?matiquement les sons de l?habitat.
Ainsi, les bruits qui se distinguent de laparole sont mis en correspondance avec une s?quence de mots erron?e.
La classificationdes  sons  en  deux  cat?gories  (parole/non-parole)  permet  un  filtrage  des  donn?esacoustiques.
Ce filtrage est effectu?
en parall?le du processus de reconnaissance pourconserver les aspects temps r?el inh?rents au projet.L?adaptationLes techniques d?adaptation qui auraient pu ?tre utilis?es ont ?t?
les premi?res ?
?treimpl?ment?es et test?es.
Un mod?le de langage (N-grams) a ?t?
?labor?
sur un corpus de22plus de 57500 phrases d?riv?es d?exp?riences pratiques et de paraphrases.Une comparaison entre deux proc?dures d?adaptation, Maximum A Posteriori (MAP) etMaximum Likelihood Linear Regression (MLLR),  a ?t?
faite (Caon, 2011).Le locuteur est le m?me pour toute l?exp?rience.
Il a ?t?
enregistr?
et les fichiers audiosont  jou?s  par  un  haut-parleur.
Comme  pr?vu,  ?tant  donn?
le  peu  de  donn?ed?adaptation disponibles (10 phrases par locuteur), l'adaptation par MLLR donne doncles  meilleurs  r?sultats.
Sans  adaptation,  60%  des  allocutions  ont  ?t?
correctementretranscrites par Julius.
Ce taux s?
?l?ve ?
70% avec l?adaptation par MAP et 73% avecl?adaptation MLLR.
De fait, MLLR a ?t?
confirm?
comme la technique d?adaptation laplus idoine.La combinaison de mod?les de langageDans une premi?re version de l?application, un N-gram unique, appris sur un corpus de57 658 phrases a ?t?
utilis?.
La voix de chaque utilisateur ?tait adapt?e avant l?utilisationdu syst?me : la reconnaissance est cibl?e pour un utilisateur d?termin?
par l?adaptationpr?alable de sa voix.
Cette premi?re version pr?sentait trop de ?
faux-positifs ?, i.e.
decommandes non d?sir?es lors de tests pratiques.Dans le but d?am?liorer ?
la fois les taux de rejet et de reconnaissance, un filtre, d?crisci-dessous, a ?t?
impl?ment?.Le  gestionnaire  de  dialogue  mod?lise  le  dialogue  comme  un  ensemble  de  fen?tres(M?ller, 2010).
Celles-ci contiennent chacune un graphe du sous-dialogue pour lequel lestransitions sont d?clench?es par l?
?tat de variables internes au robot ou par des actionsde l?utilisateur (commande vocale,  pression de bouton,  excitation de capteur...).
Unefen?tre devient active lorsque l?une de ses conditions suffisantes d?activation est remplie,ce sont les m?me type de variable que celles associ?es aux transitions intra-fen?tre.
Defait, il est possible de construire une hi?rarchie du dialogue.
La fen?tre principale ouracine,  initialement  active,  contient  (uniquement)  tous  les  d?clencheurs  des  sous-dialogues.
Les  fen?tres  contiennent  des  noeuds  terminaux,  ce  qui  permet  une  auto-d?sactivation et un retour ?
la fen?tre principale.23Figure 2 ?
Syst?me de fen?tre du gestionnaire de dialogueLes diff?rentes sous-fen?tres ont ?t?
regroup?es en 8 cat?gories.
A chaque cat?goriecorrespond un ensemble des commandes vocables possibles dans les sous-dialogues quila compose.
Pour chaque cat?gorie, un mod?le de langage a ?t?
appris sur l?ensemble descommandes correspondantes.Un 9?me mod?le est appris sur l?ensemble des commandes vocales qui activent les sous-fen?tres, il est associ?
?
la fen?tre principale.Bien que le module de reconnaissance vocale ne connaisse pas l?
?tat du dialogue (lafen?tre active) puisque la communication est uni-directionnelle pour garantir au mieuxla synchronisation et parce que Julius le permet, 9 instances du d?codeur, param?tris?esavec un mod?le acoustique identique et un mod?le de langage sp?cifique, analyse enparall?le les sons de l?habitat.Cette m?thode permet de favoriser  en grande partie  les  commandes autoris?es  selonl?
?tat  du dialogue,  cependant,  elle aggrave  les  probl?mes de rejet  des  allocutions  endehors de l?application.Le test de similarit?La similarit?
entre deux hypoth?ses est mesur?e en terme de distance de Levenshtein surles mots.
Elle cumule le nombre de substitution, d?ajout et de retrait de mots.
De pluselle  est  ensuite  divis?e  par  la  longueur  des  phrases,  rendant  alors  une  moyenne dunombre de diff?rences par mots.
La valeur de cette variable, relative ?
un seuil, d?finit lavalidation ou le  rejet de l?hypoth?se de commande vocale reconnu par l?instance dud?codeur bas?e sur un vocabulaire restreint.
Ce test permet de :-  confirmer  les  hypoth?ses  correctes :  une  commande reconnue correctement(vrai-positif)  par  un  d?codeur  sp?cialis?
et  reconnue  correctement  par  le24d?codeur  g?n?ral  est  valid?
par  le  test,  l?hypoth?se  fournie  par  le  d?codeursp?cialis?
est transmise.-  rejeter  les hypoth?ses  incorrectes :  une commande reconnue incorrectement(faux-positif)  par  un  d?codeur  sp?cialis?
et  reconnue  correctement  ousimilairement par le d?codeur g?n?ral est rejet?e par le test, l?hypoth?se fourniepar le d?codeur sp?cialis?
est ignor?e.-  corriger  les  hypoth?ses  partiellement  incorrectes :  une commande reconnuecorrectement  par  un  d?codeur  sp?cialis?
et  reconnue  similairement  par  led?codeur  g?n?ral  est  valid?e par le test,  l?hypoth?se fournie par le  d?codeursp?cialis?
est transmise.Le  mod?le  de  langage  g?n?ral  doit,  dans  cette  configuration,  pouvoir  mod?liser  less?quences de mots d?finies dans les mod?les de langage sp?cialis?s.
Il est donc n?cessaired?ajouter les commandes vocales dans le corpus d?apprentissage du mod?le g?n?ral, deplus,  nous  introduisons  un  poids  ?
ces  commandes.
Le  poids  optimal  a  ?t?
d?finiexp?rimentalement comme ?tant 1000, i.e.
les commandes vocales ont ?t?
ajout?es millefois au corpus CGN avant l?apprentissage.Finalement, le test de similarit?
ne s?effectue pas sur une transcription par d?codeur, il a?t?
d?couvert,  exp?rimentalement,  que  l?utilisation  des  n  meilleures  hypoth?sesam?liorait le taux de reconnaissance sans impacter sensiblement le taux de rejet :-  au  vu  de  la  taille  des  mod?les  de  langage  restreints,  seule  la  meilleurehypoth?se est compar?e.- plusieurs hypoth?ses (les 3 meilleures dans notre application) fournies par led?codeur g?n?ral passent le test de similarit?.L?ensemble de ces am?liorations ont ?t?
impl?ment?es, pour certaines directement dansle code de Julius ou du gestionnaire de dialogue.
Elles sont ?valu?es dans la sectionsuivante.25Figure 3 ?
Syst?me de reconnaissance de la parole incluant le test de similarit?6.
?valuationsUne  batterie  de  test  pour  ?prouver  la  robustesse  du  syst?me  a  ?t?
effectu?e,  nouspr?sentons ici les r?sultats les plus probants et significatifs.Pour l?ensemble des ?valuations, un corpus de test a ?t?
pr?alablement enregistr?
aupr?sde  5  r?sidents  hollandais.
Chacun  d?eux  a  enregistr?
58  phrases :  10  phrasesd?adaptation,  20  commandes  de  l?application,  22  allocutions  hors  application  et  6commandes  d?riv?es.
Une  commande  d?riv?e  est  une  commande  compos?e  duvocabulaire de l?application mais dont la grammaire est inexacte.L?installation de l?exp?rience est pr?sent?
sch?matiquement sur la figure 4.
Les s?quencessonores  en  hollandais  sont  produites  par  un  haut-parleur  et  enregistr?es  par  unmicrophone ?
distance variable.
Un second haut-parleur,  plac?
au-dessus du premiersimule des bruits ambiants dans la seconde phase de l?exp?rience.
Le volume sonore deslocuteurs hollandais est ajust?
aux situations r?elles (environ 60 dBA).26Figure 4 ?
Installation pour l?exp?rienceLors d?une premi?re phase, l?addition du test de similarit?, le poids des commandesvocales dans le corpus d?apprentissage du mod?le g?n?ral et le nombre d?hypoth?sespr?sent?es ?
la comparaison par le d?codeur g?n?ral sont ?valu?.Dans le cas de commandes vocales autoris?es par l?application, le d?codeur de base, quiutilise un mod?le unique de langage g?n?ral comprenant les commandes vocales sanspoids associ?, obtient des r?sultats de reconnaissance de 15%.
L?ajout du test desimilarit?
sans modifier ce mod?le de langage g?n?ral porte le taux ?
20% mais laisseappara?tre des faux-positifs.
Le syst?me le plus ?volu?
obtient des performances de 85%de reconnaissance pour un taux de faux-positifs nul.Toutes les allocutions hors de l?application sont rejet?es par le syst?me.En ce qui concerne les commandes d?riv?es, elles sont peu rejet?es car proche descommandes r?elles.Syst?me Taux de reconnaissance Taux de faux-positifsBase + adaptation 15 0Base + adaptation + test de similarit?
(poids descommandes : 1 ; hypoth?ses du d?codeur g?n?ral : 1) 20 10Base + adaptation + test de similarit?
(poids descommandes : 1000 ; hypoth?ses du d?codeur g?n?ral : 1) 55 0Base + adaptation + test de similarit?
(poids descommandes : 1000 ; hypoth?ses du d?codeur g?n?ral : 3) 85 0Table 2 ?
Taux de reconnaissance correcte et de faux-positifs pour les commandes de l?applicationSyst?me Taux de reconnaissance Taux de faux-positifsBase + adaptation 9.09 0Base + adaptation + test de similarit?
(poids descommandes : 1 ; hypoth?ses du d?codeur g?n?ral : 1) 0 0Base + adaptation + test de similarit?
(poids descommandes : 1000 ; hypoth?ses du d?codeur g?n?ral : 1) 0 0Base + adaptation + test de similarit?
(poids descommandes : 1000 ; hypoth?ses du d?codeur g?n?ral : 3) 0 027Table 3 ?
Taux de reconnaissance correcte et de faux-positifs pour des allocutions hors applicationSyst?me Taux de reconnaissance Taux de faux-positifsBase + adaptation 16.67 0Base + adaptation + test de similarit?
(poids descommandes : 1 ; hypoth?ses du d?codeur g?n?ral : 1) 33.33 0Base + adaptation + test de similarit?
(poids descommandes : 1000 ; hypoth?ses du d?codeur g?n?ral : 1) 66.67 0Base + adaptation + test de similarit?
(poids descommandes : 1000 ; hypoth?ses du d?codeur g?n?ral : 3) 66.67 0Table 4 ?
Taux de reconnaissance correcte et de faux-positifs pour des commandes de l?application d?riv?esLa deuxi?me phase de l?exp?rience consistait en un test de r?sistance au bruit.
Un secondhaut-parleur, simulant du bruit ambiant (non-stationnaire) est ajout?
au dispositif.Cela a pour cons?quence de diminuer les performances du syst?me, autant du point devue de la reconnaissance, que de celui du rejet.Syst?me Taux de reconnaissance Taux de faux-positifsMachine ?
laver 74 11Locuteur hollandais 53 11Musique 47 5Foule 42 11Table 5 ?
Taux de reconnaissance correcte et de faux-positifs pour les commandes de l?applicationSyst?me Taux de reconnaissance Taux de faux-positifsMachine ?
laver 0 0Locuteur hollandais 0 0Musique 0 0Foule 0 3.64Table 6 ?
Taux de reconnaissance correcte et de faux-positifs pour des allocutions hors applicationSyst?me Taux de reconnaissance Taux de faux-positifsMachine ?
laver 40 0Locuteur hollandais 60 0Musique 20 0Foule 60 0Table 7 ?
Taux de reconnaissance correcte et de faux-positifs pour des commandes de l?application d?riv?es287.Conclusion et perspectivesLe syst?me pr?sent?
dans cet article propose de compl?ter l?interaction entre un robot etun humain par ajout de commandes vocales dans une maison intelligente.
Le robot esttoujours actif, tout comme doit l?
?tre l?analyse des commandes vocales accessibles ?
toutmoment.
De par ces contraintes, la caract?ristique la plus importante dont on doit tenircompte  est  la  robustesse  d?un  tel  syst?me.
Cela  combine  ?
la  fois  un  taux  dereconnaissance correct et un taux de rejet acceptable.Un ?quilibre entre ces deux aspects doit  ?tre trouv?.
Accepte-t-on de reconna?tre descommandes  erron?es ?
Peut-on  demander  ?
l?utilisateur  de  r?p?ter  plusieurs  fois  lescommandes ?
Pendant les tests  pratiques pr?c?dents,  il  a ?t?
d?montr?
que les faux-positifs perturbaient l?utilisateur et g?n?raient des comportements inattendus.
Pour parer?
ce probl?me, les possibilit?s de commandes ont ?t?
restreintes, cr?ant deux nouveauxobstacles.
Les utilisateurs cibles sont des s?niors qui pourraient avoir des difficult?s ?
serappeler les commandes pr?cises.
De plus, ils pourraient rapidement se d?sint?resser desfonctionnalit?s vocales s?ils  per?oivent  une fiabilit?
faible dans l'ex?cution des  ordresqu?ils ?mettent.Nous avons propos?, dans ce travail,  d?exp?rimenter une combinaison de mod?les delangage associ?e ?
un test de similarit?
pour am?liorer  la pr?cision du syst?me.Un nouveau mod?le de langage g?n?ral a ?t?
construit ?
partir de la base n?erlandaiseCGN, supposons qu?il est capable de reconna?tre n?importe quelle phrases en n?erlandaisou une phrase proche.
Une passe de la reconnaissance utilise un mod?le de langagesp?cifique ?
l?application, voire ?
une partie de l?application.
La similarit?
entre les deuxsorties, i.e.
la distance de Levenshtein entre les deux phrases, agit comme un filtre pourvalider ou rejeter les sorties.Ce  syst?me  plus  ?labor?
a  d?montr?
?tre  plus  robuste,  autorisant  un  taux  dereconnaissance correct ainsi que des cas limit?
de faux-positifs.
Cependant, l?exp?rience amontr?
ses faiblesses dans le traitement et le rejet des allocutions courtes, i.e.
phrasescompos?es d?un seul mot.
L?utilisation du mot-cl?
?
d?attention ?
emp?che la plupart dutemps ce genre de situation de se produire.Courant  avril  et  mai  de  l?ann?e  2012,  des  essais  en  situation  r?elle  auront  lieu  ?Eindhoven  et  Gits.
Des  couples  de  seniors  sont  invit?s  ?
vivre  dans  une  maisonintelligente dans laquelle un robot compagnon interagira avec eux.
Jusqu??
pr?sent, le29test en conditions r?elles le plus significatif eu lieu dans cette m?me maison (Eindhoven)dans  un environnement sonore  stationnaire.
Par stationnaire,  il  est  entendu que despersonnes parlaient dans les pi?ces voisines et que leur voix parvenaient dans la pi?ce detest sans qu??
elles seules elles ne d?clenchent le processus de reconnaissance programm?pour ?tre effectif ?
partir d?un certain niveau sonore per?u.
Un locuteur n?erlandais, quiavait pr?c?demment adapt?
le mod?le acoustique ?
sa voix, prononce d?s lors les 168commandes d?finies ?
ce moment.
Il  ?tait  autoris?
?
prononcer une seconde fois  lescommandes  mal  ou  non  reconnues  au  premier  essai.
Le  taux  de  reconnaissancescorrectes constat?es s?
?l?ve alors ?
89%, constituant le seuil bas pour l?
?valuation del?
?volution du syst?me.
Pour des raisons de respect de la vie priv?e, les essais pratiques ?venir  ne  seront  pas  enregistr?s,  un  protocole  d?
?valuation  devrait  ?tre  ?tabli  pourreporter les r?sultats significatifs et scientifiques.8.
RemerciementsCe travail a ?t?
soutenu par le projet europ?en CompanionAble.
Nous remercions AKG(Vienne) et SmartHome (Eindhoven) pour leur appui.
Nous remercions ?galement DanielCaon et Pierre Sendorek pour leur aide dans les premi?res impl?mentations du syst?mede reconnaissance.9.
R?f?rencesLEE, A.
(2008).
The Julius Book.ROUGUI,  J.  E.,  ISTRATE,  D.  et SOUIDENE,  W. (2009).
Audio  Sound Event Identification fordistress situations and context awareness.
In EMBC2009, September 2-6, Minneapolis,USA, pp.
3501-3504.ROUGUI,  J.  E.,  ISTRATE,  D.,   SOUIDENE,  W.,  OPITZ,  M.  et RIEMANN,  M. (2009).
Audio  basedsurveillance for cognitive assistance using a CMT microphone within socially assistivetechnology.
In EMBC2009, September 2-6, Minneapolis, USA,  pp.2547-2550.CAON,  D., SIMMONET,  T., BOUDY,  J.  et CHOLLET,  G. (2011).
vAssist:  The Virtual Interactiveassistant  for  Daily  Home-care.
In  pHealth  conference,  8th International  Conference  onWearable Nano and Macro Technologies for Personalized Health, Lyon, France.M?LLER, S., SCHROETER, C. et GROSS, H.-M. (2010).
Aspects of user specific dialog adaptationfor an autonomous robot.
In International Scientific Colloquium, Ilmenau, Allemagne.30
