Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43?50,Prague, June 2007. c?2007 Association for Computational LinguisticsTowards the Automatic Extraction of Definitions in Slavic1Adam Przepio?rkowski2?ukasz Dego?rski8Beata Wo?jtowiczInstitute of Computer Science PASOrdona 21, Warsaw, Polandadamp@ipipan.waw.plldegorski@bach.ipipan.waw.plbeataw@bach.ipipan.waw.pl4Kiril Simov5Petya OsenovaInstitute for Parallel Processing BASBonchev St. 25A, Sofia, Bulgariakivs@bultreebank.orgpetya@bultreebank.org3Miroslav Spousta7Vladislav Kubon?Charles UniversityMalostranske?
na?me?st??
25Prague, Czech Republicspousta@ufal.ms.mff.cuni.czvk@ufal.ms.mff.cuni.cz6Lothar LemnitzerUniversity of Tu?bingenWilhelmstr.
19, Tu?bingen, Germanylothar@sfs.uni-tuebingen.deAbstractThis paper presents the results of the prelim-inary experiments in the automatic extrac-tion of definitions (for semi-automatic glos-sary construction) from usually unstructuredor only weakly structured e-learning textsin Bulgarian, Czech and Polish.
The ex-traction is performed by regular grammarsover XML-encoded morphosyntactically-annotated documents.
The results are lessthan satisfying and we claim that the rea-son for that is the intrinsic difficulty of thetask, as measured by the low interannota-tor agreement, which calls for more sophis-ticated deeper linguistic processing, as wellas for the use of machine learning classifica-tion techniques.1 IntroductionThe aim of this paper is to report on the preliminaryresults of a subtask of the European Project Lan-guage Technology for eLearning (http://www.lt4el.eu/) consisting in the identification ofterm definitions in eLearning materials (LearningObjects; henceforth: LOs), where definitions areunderstood pragmatically, as those text fragmentswhich may, after perhaps some minor editing, beput into a glossary.
Such automatically extractedterm definitions are to be presented to the author orthe maintainer of the LO and, thus, significantly fa-cilitate and accelerate the creation of a glossary fora given LO.
From this specification of the task it fol-lows that good recall is much more important thangood precision, as it is easier to reject wrong glos-sary candidates than to browse the LO for term def-initions which were not automatically spotted.The project involves 9 European languages in-cluding 3 Slavic (and, regrettably, no Baltic) lan-guages: one South Slavic, i.e., Bulgarian, and twoWest Slavic, i.e., Czech and Polish.
For all lan-guages, shallow grammars identifying definitionshave been constructed; after mentioning some previ-ous work on Information Extraction (IE) for Slaviclanguages and on extraction of definitions in sec-tion 2, we briefly describe the three Slavic grammarsdeveloped within this project in section 3.
Section 4presents the results of the application of these gram-mars to LOs in respective languages.
These resultsare evaluated in section 5, where main problems, aswell as some possible solutions, are discussed.
Fi-nally, section 6 concludes the paper.432 Related WorkDefinition extraction is an important NLP task,most frequently a subtask of terminology extraction(Pearson, 1996), the automatic creation of glossaries(Klavans and Muresan, 2000; Klavans and Muresan,2001), question answering (Miliaraki and Androut-sopoulos, 2004; Fahmi and Bouma, 2006), learninglexical semantic relations (Malaise?
et al, 2004; Stor-rer and Wellinghoff, 2006) and automatic construc-tion of ontologies (Walter and Pinkal, 2006).
Toolsfor definition extraction are invariably language-specific and involve shallow or deep processing,with most work done for English (Pearson, 1996;Klavans and Muresan, 2000; Klavans and Muresan,2001) and other Germanic languages (Fahmi andBouma, 2006; Storrer and Wellinghoff, 2006; Wal-ter and Pinkal, 2006), as well as French (Malaise?
etal., 2004).
To the best of our knowledge, no previ-ous attempts at definition extraction have been madefor Slavic, with the exception of some work on Bul-garian (Tanev, 2004; Simov and Osenova, 2005).Other work on Slavic information extraction hasbeen carried out mainly for the last 5 years.
Prob-ably the first forum where such work was compre-hensively presented was the International Workshopon Information Extraction for Slavonic and OtherCentral and Eastern European Languages (IESL),RANLP, Borovets, 2003, Bulgaria.
One of the pa-pers presented there, (Droz?dz?yn?ski et al, 2003), dis-cusses shallow SProUT (Becker et al, 2002) gram-mars for Czech, Polish and Lithuanian.
SProUT hassubsequently been extensively used for the informa-tion extraction from Polish medical texts (Piskorskiet al, 2004; Marciniak et al, 2005).13 Shallow Grammars for DefinitionExtractionThe input to the task of definition extraction isXML-encoded morphosyntactically-annotated text,possibly with some keywords already marked by an1SProUT has not been seriously considered for the task athand for two reasons: first, it was decided that only open sourcetools will be used in the current project, if only available, sec-ond, the input format to the current task is morphosyntactically-annotated XML-encoded text, rather than raw text, as normallyexpected by SProUT.
The second obstacle could be removed byconverting input texts to the SProUT-internal XML representa-tion.independent process.
For example, the representa-tion of a Polish sentence starting as Konstruktywizmk?adzie nacisk na (Eng.
?Constructivism puts em-phasis on?)
may be as follows:2<s id="s9"><markedTerm id="mt7" kw="y"><tok base="konstruktywizm" ctag="subst"id="t253"msd="sg:nom:m3">Konstruktywizm</tok></markedTerm><tok base="klasc" ctag="fin" id="t254"msd="sg:ter:imperf">kladzie</tok><tok base="nacisk" ctag="subst" id="t255"msd="sg:acc:m3">nacisk</tok><tok base="na" ctag="prep" id="t256"msd="acc">na</tok>[...]<tok base="."
ctag="interp" id="t273">.</tok></s>For each language, definitions were manuallymarked in two batches of texts: the first batch, con-sulted during the process of grammar development,contained at least 300 definitions, and the secondbatch, held out for evaluation, contained about 150definitions.
All grammars are regular grammars im-plemented with the use of the lxtransduce tool(Tobin, 2005), a component of the LTXML2 toolsetdeveloped at the University of Edinburgh.3 An ex-ample of a simple rule for prepositional phrases isgiven below:<rule name="PP"><seq><query match="tok[@ctag = ?prep?
]"/><ref name="NP1"><with-param name="case" value="??
"/></ref></seq></rule>This rule identifies a sequence whose first elementis a token tagged as a preposition and whose subse-quent elements are identified by a rule called NP1.This latter rule (not shown here for brevity) is a pa-rameterised rule which finds a nominal phrase of agiven case, but the way it is called above ensures thatit will find an NP of any case.2Part of the representation has been replaced by ?[...
]?.3Among the tools considered here were also CLaRK (Simovet al, 2001), ultimately rejected because it currently does notwork in batch mode, and GATE / JAPE (Cunningham et al,2002), not used here because we found GATE?s handling ofpreviously XML-annotated texts rather cumbersome and ill-documented.
Cf.
also fn.
1.44Currently the grammars show varying degrees ofsophistication, with a small Bulgarian grammar (8rules in a 2.5-kilobyte file), a larger Polish grammar(34 rules in a 11KiB file) and a sophisticated Czechgrammar most developed (147 rules in a 28KiBfile).
The patterns defined by these three grammarsare similar, but sufficiently different to defy an at-tempt to write a single parameterised grammar.4 Theremainder of this section briefly describes the gram-mars.3.1 BulgarianThe Bulgarian grammar is manually constructed af-ter examination of the manually annotated defini-tions.
Here is a list of the rule schemata, togetherwith the number and percentage of matching defini-tions:Pattern # %NP is NP 140 34.2NP verb NP 18 29.8NP - NP 21 5.0This is NP 15 3.7It represents NP 4 1.0other patterns 107 26.2Table 1: Bulgarian definition typesIn the second schema above, ?verb?
is a verb ora verb phrase (not necessarily a constituent) whichis one of the following: ??????????????
(to repre-sent), ?????????
(to show), ??????????
(to mean),????????
(to describe), ???
?????????
(to be used),???????????
(to allow), ?????
??????????
???
(to give opportunity), ???
???????
(is called),???????????
(to improve), ???????????
(to ensure),??????
???
(to serve as), ???
????????
(to be under-stood as), ????????????
(to denote), ?????????
(tocontain), ??????????
(to determine), ?????????
(to include), ???
????????
?????
(is defined as),???
????????
???
(is based on).We classify the rules in five types: copula defi-nitions, copula definitions with anaphoric relation,copula definitions with ellipsis of the copula, defi-nitions with a verb phrase, definitions with a verb4Because of this relative language-dependence of definitionpatters, which includes, e.g., idiosyncratic case information,we have not seriously considered re-using rules for other, non-Slavic, languages.phrase and anaphoric relation.
Each of these types ofdefinitions defines an NP (sometimes via anaphoricrelation) by another one.
There are some variationsof the models where some parenthetical expressionsare presented in the definition.The grammar contains several most importantrules for each type.
The different verb patterns areencoded as a lexicon.
For some of the rules, variantswith parenthetical phrases are also encoded.
The restof the grammar is devoted to the recognition of nounphrases and parenthetical phrases.
For parentheti-cal phrases, we have encoded a list of such possiblephrases, extracted on the basis of a bigger corpus.The NP grammar in our view is the crucial grammarfor recognition of the definitions.
Most work nowhas to be invested into developing the more complexand recursive NPs.3.2 CzechThe Czech grammar for definition context extractionis constructed to follow both linguistic intuition andobservation of common patterns in manually anno-tated data.We adapted a grammar5 based mainly on the ob-servation of Czech Wikipedia entries.
Encyclopediadefinitions are usually clear and very well structured,but it is quite difficult to find such well-formed defi-nitions in common texts, including learning objects.The rules were extended using part of our manuallyannotated texts, evaluated and adjusted in several it-erations, based on the observation of the annotateddata.Pattern # %NP is/are NP 52 21.2NP verb NP 45 18.4structural 39 15.9NP (NP) 30 12.2NP -/:/= NP 20 8.2other patterns 59 24.1Table 2: Czech definition typesThere are 21 top level rules, divided into five cate-gories.
Most of the correctly marked definitions fallinto the copula verb (?is/are?)
category.
The sec-5The grammar was originally developed by Nguyen ThuTrang.45ond most successful rule is the one using selectedverbs like ?definuje?
(defines), ?znamen??
(means),?vymezuje?
(delimits), ?pr?edstavuje?
(presents) andseveral others.
The remaining categories make useof the typical patterns of characters (dash, colon,equal sign and brackets) or additional structural in-formation (e.g., HTML tags).3.3 PolishThe Polish grammar rules are divided into three lay-ers.
Similarly to the Czech grammar, each layer onlyrefers to itself or lower layers.
This allows for ex-pressing top level rules in a clear and easily man-ageable way.The top level layer consists of rules representingtypical patterns found in Polish documents:Pattern # %NP (...) are/is NP-INS 40 15.6NP -/: NP 39 15.2NP (are/is) to NP-NOM 27 10.6NP VP-3PERS 25 9.8NP - i.e./or WH-question 11 4.3N ADJ - PPAS 8 3.1NP, i.e./or NP 7 2.7NP-ACC one maydescribe/define as NP-ACC 5 2.0other patterns(not in the grammar) 94 36.7Table 3: Polish definition typesThe middle layer consists of rules catching pat-terns such as ?simple NP in given case, followed bya sequence of non-punctuation elements?
or ?cop-ula?.The bottom layer rules basically only refer toPOS markup in the input files (or other bottom layerrules).4 ResultsAs mentioned above, the testing corpus for each lan-guage consists of about 150 definitions, unseen dur-ing the construction of the grammar.66Obviously, three different corpora had to be used to eval-uate the grammars for the three languages, but the corpora aresimilar in size and character, so any differences in results stemmostly from the differences in the three grammars.The Bulgarian test corpus, containing around76,800 tokens, consists of the third part of theCalimera guidelines (http://www.calimera.org/).
We view this document as appropriate fortesting because it reflects the chosen domain and itcombines definitions from otherwise different sub-domains, such as XML language, Internet usage,etc.
There are 203 manually annotated definitionsin this corpus: 129 definitions contained in one sen-tence, 69 definitions split across 2 sentences, 4 def-initions in 3 sentences and one definition in 4 sen-tences.
Note that the real test part is the set of the129 definitions in one sentence, since the Bulgar-ian grammar does not consider cross-sentence def-initions in any way.Czech data used for evaluation consist of severalchapters of the Calimera guidelines and MicrosoftExcel tutorial.
The tutorial is a typical text usedin e-learning, consisting of five chapters describingsheets, tables, formating, graphs and lists.
The cor-pus consists of over 90,000 tokens and contains 162definitions, out of which 153 are contained in a sin-gle sentence, 6 span 2 sentences, and 3 definitionsspan 3 sentences.Polish test corpus consists of over 83,200 tokenscontaining 157 definitions: 148 definitions are con-tained within one sentence, while 9 span 2 sen-tences.
The corpus is made up of 10 chapters of apopular introduction to and history of computer sci-ence and computer hardware.Each grammar was quantitatively evaluated bycomparing manually annotated files with the samefiles annotated automatically by the grammar.
Afterconsidering various ways of quantitative evaluation,we decided to do the comparison at token level: pre-cision was calculated as the ratio of the number ofthose tokens which were parts of both a manuallymarked definition and an automatically discovereddefinition to the number of all tokens in automati-cally discovered definitions, while recall was takento be the ratio of the number of tokens simultane-ously in both kinds of definitions to the number oftokens in all manually annotated definitions.
Since,for this task, recall is more important than precision,we used the F2-measure for the combined result.77In general, F?
= (1 + ?)
?
(precision ?
recall)/(?
?precision+recall).
Perhaps?
larger than 2 could be used, but itis currently not clear to us what criteria should be assumed when46The results for the three grammars are given inTable 4.
Note that the processing model for Czechprecision recall F2Bulgarian 20.5% 2.2% 3.1Czech 18.3% 40.7% 28.9Polish 14.8% 22.2% 19.0Table 4: Token-based evaluation of shallow gram-marsdiffers from the other two languages, as the inputtext is converted to a flat format, as described in sec-tion 5.3, and grammar rules are sensitive to sentenceboundaries (and may operate over them).5 Evaluation and Possible Improvements5.1 Interannotator AgreementWe calculated Cohen?s kappa statistic (1) for the cur-rent task, where both the relative observed agree-ment among raters Pr(a) and the probability thatagreement is due to chance Pr(e) where calculatedat token level.?
=Pr(a) ?
Pr(e)1 ?
Pr(e)(1)More specifically, we assumed that two annotatorsagree on a token if the token belongs to a definitioneither according to both annotations or according toneither.
In order to estimate the probability of agree-ment due to chance Pr(e), we measured, separatelyfor each annotator, the proportion of tokens found indefinitions to all tokens in text, which resulted in twoprobability estimates p1 and p2, and treated Pr(e) asthe probability that the two annotators agree if theyrandomly, with their own probability, classify a to-ken as belonging to a definition, i.e.
:Pr(e) = p1 ?
p2 + (1 ?
p1) ?
(1 ?
p2) (2)The interannotator agreement (IAA) was mea-sured this way for Czech and Polish, where ?
foreach language ?
the respective test corpus was an-notated by two annotators.
The results are 0.44 forCzech and 0.31 for Polish.
Such results are very lowfor any classification task, and especially low for adeciding on the exact value of ?.
Note that it would not makesense to use recall alone, as it is trivial to write all-acceptinggrammars with 100% recall.binary classification task.
They show that the task ofidentifying definitions in running texts and agreeingon which parts of text count as a definition is intrin-sically very difficult.
They also call for the recon-sideration of the evaluation and IAA measurementmethodology based on token classification.85.2 Evaluation MethodologyTo the best of our knowledge, there is no estab-lished evaluation methodology for the task of def-inition extraction, where definitions may span sev-eral sentences.9 For this reason we evaluated the re-sults again, in a different way: we treated an auto-matically discovered definition as correct, if it over-lapped with a manually annotated definition.
Wecalculated precision as the number of automatic defi-nitions overlapping with manual definitions, dividedby the number of automatic definitions, while re-call ?
as the number of manual definitions overlap-ping automatic definitions, divided by the number ofmanual definitions.10The results for the three grammars, given in Ta-ble 5, are much higher than those in Table 4 above,although still less than satisfactory.precision recall F2Bulgarian 22.5% 8.9% 11.1Czech 22.3% 46% 33.9Polish 23.3% 32% 28.4Table 5: Definition-based evaluation of shallowgrammars5.3 Definitions and Sentence BoundariesRegardless of the inherent difficulties of the task anddifficulties with the evaluation of the results, thereis clear room for improvement; one possible path8A better approximation would be to measure IAA on thebasis of sentence or (as suggested by an anonymous reviewer)NP classification; we intend to pursue this idea in future work.9With the assumption that definitions are no longer thana sentence, usually the task is treated as a classification task,where sentences are classified as definitional or not, and ap-propriate precision and recall measures are applied at sentencelevel.10At this stage definition fragments distributed across a num-ber of different sentences were treated as different definitions,which negatively affects the evaluation of the Bulgarian gram-mar, as the Bulgarian test corpus contains a large number ofmulti-sentence definitions.47to explore concerns multi-sentence definitions.
Asnoted above, for all languages considered here, therewere definitions which were spanning 2 or more sen-tences; this turned out to be a problem especially forBulgarian, were 36% of definitions crossed a sen-tence boundary.11Such multi-sentence definitions are a problem be-cause in the DTD adopted in this project definitionsare subelements of sentences rather than the otherway round.
In case of a multi-sentence definition,for each sentence there is a separate element en-capsulating the part of the definition contained inthis sentence.
Although these are linked via spe-cial attributes and the information that they are partof the same definition can subsequently be recov-ered, it is difficult to construct an lxtransducegrammar which would be able to automatically marksuch multi-sentence definitions: an lxtransducegrammar expects to find a sequence of elements andwrap them in a single larger element.A solution to this technical problem has been im-plemented in the Czech grammar, where first the in-put text is flattened (via an XSLT script), so that,e.g.
:<par id="d1p2"><s id="d1p2s1"><tok id="d1p2s1t1" base="Pavel"ctag="N" msd="NMS1-----A----">Pavel</tok><tok id="d1p2s1t2" base="satrapa"ctag="N" msd="NMS1-----A----">Satrapa</tok></s></par>becomes:<par id="Sd1p2"/><s id="Sd1p2s1"/><tok id="d1p2s1t1" base="Pavel"ctag="N" msd="NMS1-----A----">Pavel</tok><tok id="d1p2s1t2" base="satrapa"ctag="N" msd="NMS1-----A----">Satrapa</tok><s id="Ed1p2s1"/><par id="Ed1p2"/>11An example of a Polish manually annotated multi-sentencedefinition is: .
.
.
opracowano techniki antyspamowe.
Tech-niki te drastycznie zaniz?aja?
wartos?c?
strony albo ja?
banuja?.
.
.(Eng.
?.
.
.
anti-spam techniques were developed.
Such tech-niques drastically lower the value of the page or they ban it.
.
.
?
).The definition is split into two fragments fully contained in re-spective sentences: techniki antyspamowe and Techniki te.
.
.
.No attempt at anaphora resolution is made.This flattened representation is an input to a gram-mar which is sensitive to the empty s and par ele-ments and may discover definitions containing suchelements; in such a case, the postprocessing script,which restores the hierarchical paragraph and sen-tence structure, splits such definitions into smallerelements, fully contained in respective sentences.5.4 Problems Specific to SlavicAt least in case of the two West Slavic languagesconsidered here, the task of writing a definitiongrammar is intrinsically more difficult than for Ger-manic or Romance languages, mainly for the follow-ing two reasons.First, Czech and Polish have very rich nominalinflection with a large number of paradigm-internalsyncretisms.
These syncretisms are a common causeof tagger errors, which percolate to further stages ofprocessing.
Moreover, the number of cases makes itmore difficult to encode patterns like ?NP verb NP?,as different verbs may combine with NPs of differentcase.
In fact, even two different copulas in Polishtake different cases!Second, the relatively free word order increasesthe number of rules that must be encoded, and makesthe grammar writing task more labour-intensive anderror-prone.
The current version of the Polish gram-mar, with 34 rules, is rather basic, and even the 147rules of the Czech grammar do not take into consid-eration all possible patterns of grammar definitions.As Tables 4 and 5 show, there is a positive corre-lation between the grammar size and the value ofF2, and the Bulgarian and Polish grammars certainlyhave room to grow.
Moreover, a path that is wellworth exploring is to drastically increase the num-ber of rules and, hence, the recall, and then deal withprecision via Machine Learning methods (cf.
sec-tion 5.6).5.5 Levels of Linguistic ProcessingThe work reported here has been an excercise indefinition extraction using shallow parsing methods.However, the poor results suggest that this is oneof the tasks that require a much more sophisticatedand deeper approach to language analysis.
In fact,in turns out that virtually all successful attempts atdefinition extraction that we are aware of build onworked-out deep linguistic approaches (Klavans and48Muresan, 2000; Fahmi and Bouma, 2006; Walterand Pinkal, 2006), some of them combining syn-tactic and semantic information (Miliaraki and An-droutsopoulos, 2004; Walter and Pinkal, 2006).Unfortunately, for most Baltic and Slavic lan-guages, such deep parsers are unavailable or havenot yet been extensively tested on real texts.
Oneexception is Czech, where a number of parsers werealready described and evaluated (on the Prague De-pendency Treebank) in (Zeman, 2004, ?
14.2); thebest of these parsers reach 80?85% accuracy.For Polish, apart from a number of linguisticallymotivated toy parsers, there is a possibly wide cov-erage deep parser (Wolin?ski, 2004), but it has not yetbeen evaluated on naturally occurring texts.
The sit-uation is probably most dire for Bulgarian, althoughthere have been attempts at the induction of a depen-dency parser from the BulTreeBank (Marinov andNivre, 2005; Chanev et al, 2006).Nevertheless, if other possible paths of improve-ment suggested in this section do not bring satisfac-tory results, we plan to make an attempt at adaptingthese parsers to the task at hand.5.6 Postprocessing: Machine Learning andKeyword IdentificationVarious approaches to the machine learning treat-ment of the task of classifying sentences or snippetsas definitions or non-definitions can be found, e.g.,in (Miliaraki and Androutsopoulos, 2004; Fahmiand Bouma, 2006) and references therein.
In thecontext of the present work, such methods may beused to postprocess apparent definitions found atearlier processing stages and decide which of themare genuine definitions.
For example, (Fahmi andBouma, 2006) report that a system trained on 2299sentences, including 1366 definition sentences, mayincrease the accuracy of a definition extraction toolfrom 59% to around 90%.12Another possible improvement may consist in,again, aiming at very high recall and then usingan independent keyword detector to mark keywords(and key phrases) in text and classifying as genuinedefinitions those definitions, whose defined term hasbeen marked as a keyword.12The numbers are so high ?probably due to the fact that thecurrent corpus consists of encyclopedic material only?
(Fahmiand Bouma, 2006, fn.
4).Whatever postprocessing technique or combina-tion of techniques proves most efficient, it seems thatthe linguistic processing should aim at high recallrather than high precision, which further justifies theuse of the F2 measure for evaluation.136 ConclusionTo the best of our knowledge, this paper is the firstreport on the task of definition extraction for a num-ber of Slavic languages.
It shows that the task isintrinsically very difficult, which partially explainsthe relatively low results obtained.
It also calls atten-tion to the fact that there is no established evaluationmethodology where possibly multi-sentence defini-tions are involved and suggests what such method-ology could amount to.
Finally, the paper suggestsways of improving the results, which we hope to fol-low and report in the future.ReferencesMarkus Becker et al 2002.
SProUT ?
shallow process-ing with typed feature structures and unification.
InProceedings of the International Conference on NLP(ICON 2002), Mumbai, India.Sharon A. Caraballo.
2001.
Automatic Construction of aHypernym-Labeled Noun Hierarchy from Text.
Ph.
D.dissertation, Brown University.Atanas Chanev, Kiril Simov, Petya Osenova, and Sve-toslav Marinov.
2006.
Dependency conversion andparsing of the BulTreeBank.
In proceedings of theLREC workshop Merging and Layering Linguistic In-formation, Genoa, Italy.Hamish Cunningham, Diana Maynard, KalinaBontcheva, and Valentin Tablan.
2002.
GATE:A framework and graphical development environmentfor robust NLP tools and applications.
In Proceedingsof the 40th Anniversary Meeting of the Association forComputational Linguistics.Witold Droz?dz?yn?ski, Petr Homola, Jakub Piskorski, andVytautas Zinkevic?ius.
2003.
Adapting SProUT toprocessing Baltic and Slavonic languages.
In Infor-mation Extraction for Slavonic and Other Central andEastern European Languages, pp.
18?25.Ismail Fahmi and Gosse Bouma.
2006.
Learning to iden-tify definitions using syntactic features.
In Proceed-ings of the EACL 2006 workshop on Learning Struc-tured Information in Natural Language Applications.13Note that the situation here is different than in the task ofacquiring hyponymic relations from texts, where high-precisionmanual rules (Hearst, 1992) must be augmented with statisticalclustering methods to increase recall (Caraballo, 2001).49Marti Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe Fourteenth International Conference on Computa-tional Linguistics, Nantes, France.Judith L. Klavans and Smaranda Muresan.
2000.DEFINDER: Rule-based methods for the extraction ofmedical terminology and their associated definitionsfrom on-line text.
In Proceedings of the Annual FallSymposium of the American Medical Informatics As-sociation.Judith L. Klavans and Smaranda Muresan.
2001.
Eval-uation of the DEFINDER system for fully automaticglossary construction.
In Proceedings of AMIA Sym-posium 2001.V?ronique Malais?, Pierre Zweigenbaum, and BrunoBachimont.
2004.
Detecting semantic relations be-tween terms in definitions.
In S. Ananadiou andP.
Zweigenbaum, editors, COLING 2004 CompuTerm2004: 3rd International Workshop on ComputationalTerminology, pp.
55?62, Geneva, Switzerland.
COL-ING.Ma?gorzata Marciniak, Agnieszka Mykowiecka, AnnaKups?c?, and Jakub Piskorski.
2005.
Intelligent con-tent extraction from Polish medical texts.
In L. Bolcet al, editors, Intelligent Media Technology for Com-municative Intelligence, Second International Work-shop, IMTCI 2004, Warsaw, Poland, September 13-14,2004, Revised Selected Papers, volume 3490 of Lec-ture Notes in Computer Science, pp.
68?78.
Springer-Verlag.Svetoslav Marinov and Joakim Nivre.
2005.
A data-driven parser for Bulgarian.
In Proceedings of theFourth Workshop on Treebanks and Linguistic Theo-ries, pp.
89?100, Barcelona.Spyridoula Miliaraki and Ion Androutsopoulos.
2004.Learning to identify single-snippet answers to defi-nition questions.
In Proceedings of COLING 2004,pp.
1360?1366, Geneva, Switzerland.
COLING.Jennifer Pearson.
1996.
The expression of defini-tions in specialised texts: a corpus-based analysis.In M. Gellerstam et al, editors, Proceedings of theSeventh Euralex International Congress, pp.
817?824,G?teborg.Jakub Piskorski et al 2004.
Information extraction forPolish using the SProUT platform.
In M. A. K?opoteket al, editors, Intelligent Information Processing andWeb Mining, pp.
227?236.
Springer-Verlag, Berlin.Kiril Simov and Petya Osenova.
2005.
BulQA:Bulgarian-Bulgarian Question Answering at CLEF2005.
In CLEF, pp.
517?526.Kiril Simov et al 2001.
CLaRK ?
an XML-based sys-tem for corpora development.
In P. Rayson et al, edi-tors, Proceedings of the Corpus Linguistics 2001 Con-ference, pp.
558?560, Lancaster.Angelika Storrer and Sandra Wellinghoff.
2006.
Auto-mated detection and annotation of term definitions inGerman text corpora.
In Proceedings of LREC 2006.Hristo Tanev.
2004.
Socrates: A question answeringprototype for Bulgarian.
In Recent Advances in Nat-ural Language Processing III, Selected Papers fromRANLP 2003, pages 377?386.
John Benjamins.Richard Tobin, 2005.
Lxtransduce, a replace-ment for fsgmatch.
University of Edinburgh.http://www.cogsci.ed.ac.uk/~richard/ltxml2/lxtransduce-manual.html.Stephan Walter and Manfred Pinkal.
2006.
Automaticextraction of definitions from German court decisions.In Proceedings of the Workshop on Information Ex-traction Beyond The Document, pp.
20?28, Sydney,Australia.
Association for Computational Linguistics.Marcin Wolin?ski.
2004.
Komputerowa weryfikacja gra-matyki S?widzin?skiego.
Ph.
D. dissertation, ICS PAS,Warsaw.Daniel Zeman.
2004.
Parsing with a Statistical Depen-dency Model.
Ph.
D. dissertation, Charles University,Prague.50
