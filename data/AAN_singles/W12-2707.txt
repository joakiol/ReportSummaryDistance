NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model?
On the Future of Language Modeling for HLT, pages 50?58,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsRevisiting the Case for Explicit Syntactic Information in Language ModelsAriya Rastrow, Sanjeev Khudanpur, Mark DredzeHuman Language Technology Center of Excellence,Center for Language and Speech Processing, Johns Hopkins UniversityBaltimore, MD USA{ariya,khudanpur,mdredze}@jhu.eduAbstractStatistical language models used in deployedsystems for speech recognition, machinetranslation and other human language tech-nologies are almost exclusively n-gram mod-els.
They are regarded as linguistically na?
?ve,but estimating them from any amount of text,large or small, is straightforward.
Further-more, they have doggedly matched or out-performed numerous competing proposals forsyntactically well-motivated models.
This un-usual resilience of n-grams, as well as theirweaknesses, are examined here.
It is demon-strated that n-grams are good word-predictors,even linguistically speaking, in a large major-ity of word-positions, and it is suggested thatto improve over n-grams, one must exploresyntax-aware (or other) language models thatfocus on positions where n-grams are weak.1 IntroductionLanguage models (LM) are crucial components intasks that require the generation of coherent natu-ral language text, such as automatic speech recog-nition (ASR) and machine translation (MT).
Mostlanguage models rely on simple n-gram statisticsand a wide range of smoothing and backoff tech-niques (Chen and Goodman, 1998).
State-of-the-artASR systems use (n ?
1)-gram equivalence classi-fication for the language model (which result in ann-gram language model).While simple and efficient, it is widely believedthat limiting the context to only the (n ?
1) mostrecent words ignores the structure of language, andseveral statistical frameworks have been proposedto incorporate the ?syntactic structure of languageback into language modeling.?
Yet despite consider-able effort on including longer-dependency features,such as syntax (Chelba and Jelinek, 2000; Khudan-pur and Wu, 2000; Collins et al, 2005; Emamiand Jelinek, 2005; Kuo et al, 2009; Filimonov andHarper, 2009), n-gram language models remain thedominant technique in automatic speech recognitionand machine translation (MT) systems.While intuition suggests syntax is important, thecontinued dominance of n-gram models could in-dicate otherwise.
While no one would dispute thatsyntax informs word choice, perhaps sufficient in-formation aggregated across a large corpus is avail-able in the local context for n-gram models to per-form well even without syntax.
To clearly demon-strate the utility of syntactic information and the de-ficiency of n-gram models, we empirically show thatn-gram LMs lose significant predictive power in po-sitions where the syntactic relation spans beyond then-gram context.
This clearly shows a performancegap in n-gram LMs that could be bridged by syntax.As a candidate syntactic LM we consider theStructured Language Model (SLM) (Chelba and Je-linek, 2000), one of the first successful attempts tobuild a statistical language model based on syntac-tic information.
The SLM assigns a joint probabil-ity P (W,T ) to every word sequence W and everypossible binary parse tree T , where T ?s terminalsare words W with part-of-speech (POS) tags, andits internal nodes comprise non-terminal labels andlexical ?heads?
of phrases.
Other approaches in-clude using the exposed headwords in a maximum-entropy based LM (Khudanpur and Wu, 2000), us-50ing exposed headwords from full-sentence parse treein a neural network based LM (Kuo et al, 2009),and the use of syntactic features in discriminativetraining (Rastrow et al, 2011).
We show that thelong-dependencies modeled by SLM, significantlyimproves the predictive power of the LM, speciallyin positions where the syntactic relation is beyondthe reach of regular n-gram models.2 Weaknesses of n-gram LMsConsider the following sentence, which demon-strates why the (n?
1)-gram equivalence classifica-tion of history in n-gram language models may beinsufficient:<s> i asked the vice president forhis endorsement </s>In an n-gram LM, the word for would be modeledbased on a 3-gram or 4-gram history, such as <vicepresident> or <the vice president>.Given the syntactic relation between the prepositionfor and the verb asked (which together make acompound verb), the strongest evidence in the his-tory (and hence the best classification of the history)for word for should be <asked president>,which is beyond the 4-gram LM.
Clearly, thesyntactic relation between a word position and thecorresponding words in the history spans beyondthe limited (n ?
1)-gram equivalence classificationof the history.This is but one of many examples used for moti-vating syntactic features (Chelba and Jelinek, 2000;Kuo et al, 2009) in language modeling.
How-ever, it is legitimate to ask if this deficiency couldbe overcome through sufficient data, that is, accu-rate statistics could somehow be gathered for the n-grams even without including syntactic information.We empirically show that (n?
1)-gram equivalenceclassification of history is not adequate to predictthese cases.
Specifically, n-gram LMs lose predic-tive power in the positions where the headword rela-tion, exposed by the syntactic structure, goes beyond(n?
1) previous words (in the history.
)We postulate the following three hypotheses:Hypothesis 1 There is a substantial difference inthe predictive power of n-gram LMs at positionswithin a sentence where syntactic dependenciesreach further back than the n-gram context versuspositions where syntactic dependencies are local.Hypothesis 2 This difference does not diminish byincreasing training data by an order of magnitude.Hypothesis 3 LMs that specifically target positionswith syntactically distant dependencies will comple-ment or improve over n-gram LMs for these posi-tions.In the following section (Section 3), we present a setof experiments to support the hypotheses 1 and 2.Section 4 introduces a SLM which uses dependencystructures followed by experiments in Section 5.3 Experimental EvidenceIn this section, we explain our experimental evi-dence for supporting the hypotheses stated above.First, Section 3.1 presents our experimental designwhere we use a statistical constituent parser to iden-tify two types of word positions in a test data,namely positions where the headword syntactic re-lation spans beyond recent words in the history andpositions where the headword syntactic relation iswithin the n-gram window.
The performance ofan n-gram LM is measured on both types of posi-tions to show substantial difference in the predictivepower of the LM in those positions.
Section 3.3 de-scribes the results and analysis of our experimentswhich supports our hypotheses.Throughout the rest of the paper, we refer toa position where the headword syntactic relationreaches further back than the n-gram context as asyntactically-distant position and other type of posi-tions is referred to as a syntactically-local position.3.1 DesignOur experimental design is based on the idea ofcomparing the performance of n-gram LMs forsyntactically-distant vs. syntactically-local .
To thisend, we first parse each sentence in the test set us-ing a constituent parser, as illustrated by the exam-ple in Figure 1.
For each word wi in each sentence,we then check if the ?syntactic heads?
of the preced-ing constituents in the parse ofw1, w2, ?
?
?
, wi?1 arewithin an (n?
1) window of wi.
In this manner, wesplit the test data into two disjoint sets, M and N ,51!"!#$%&'!!!!!()&!*"+&!,-&$"'&.(!!!!!!/0-!!!!!!!)"$!&.'0-$&1&.(!232!42!5!67!62!89!
442!
442!42!:4!
232;!
44!442!22!#$%&'!#$%&'!/0-!&.'0-$&1&.(!,-&$"'&.(!
"!Figure 1: Example of a syntactically distant position ina sentence: the exposed headwords preceding for areh.w?2 =asked and h.w?1 = president, while thetwo preceding words are wi?2 = vice and wi?1 =president.as follows,M = {j|positions s.t h.w?1, h.w?2 = wj?1, wj?2}N = {j|positions s.t h.w?1, h.w?2 6= wj?1, wj?2}Here, h?1 and h?2 correspond, respectively, to thetwo previous exposed headwords at position i, basedon the syntactic structure.
Therefore, M corre-sponds to the positions in the test data for which twoprevious exposed heads match exactly the two previ-ous words.
Whereas, N corresponds to the positionwhere at least on of the exposed heads is further backin the history than the two previous words, possiblyboth.To extract the exposed headwords at each posi-tion, we use a constituent parser to obtain the syn-tactic structure of a sentence followed by headwordpercolation procedure to get the headwords of cor-responding syntactic phrases in the parse tree.
Thefollowing method, described in (Kuo et al, 2009),is then used to extract exposed headwords from thehistory of position i from the full-sentence parsetrees:1.
Start at the leaf corresponding to the word posi-tion (wi) and the leaf corresponding to the pre-vious context word (wi?1).2.
From each leaf, go up the tree until the twopaths meet at the lowest common ancestor(LCA).3.
Cut the link between the LCA and the child thatis along the path from the context word wi?1.The head word of the the LCA child, the onethat is cut, is chosen as previous exposed head-word h.w?1.These steps may be illustrated using the parse treeshown in Figure 1.
Let us show the procedure forour example from Section 2.
Figure 1 shows the cor-responding parse tree of our example.
Consideringword position wi=for and wi?1=president andapplying the above procedure, the LCA is the nodeVPasked.
Now, by cutting the link from VPasked toNPpresident the word president is obtained asthe first exposed headword (h.w?1).After the first previous exposed headword hasbeen extracted, the second exposed headword alsocan be obtained using the same procedure, withthe constraint that the node corresponding the sec-ond headword is different from the first (Kuo et al,2009).
More precisely,1.
set k = 22.
Apply the above headword extraction methodbetween wi and wi?k.3.
if the extracted headword has previously beenchosen, set k = k + 1 and go to step (2).4.
Otherwise, return the headword as h.w?2.Continuing with the example of Figure 1, afterpresident is chosen as h.w?1, asked is cho-sen as h.w?2 of position for by applying the pro-cedure above.
Therefore, in this example the po-sition corresponding to word for belongs to theset N as the two extracted exposed headwords(asked,president) are different from the twoprevious context words (vice,president).After identifying sets N andM in our test data,we measure perplexity of n-gram LMs on N , Mand N ?M separately.
That is,PPLN?M = exp[?
?i?N?M log p(wi|W i?1i?n+1)|N ?M|]PPLN = exp[?
?i?Nlog p(wi|W i?1i?n+1)|N |]PPLM = exp[?
?i?Mlog p(wi|W i?1i?n+1)|M|],52where p(wi|wi?1wi?2 ?
?
?wi?n+1) is the condi-tional probability calculated by an n-gram LM atposition i and |.| is the size (in number of words)of the corresponding portion of the test.In addition, to show the performance of n-gramLMs as a function of training data size, we traindifferent n-gram LMs on 10%,20%,?
?
?
,100% of alarge corpus of text and report the PPL numbers us-ing each trained LM with different training data size.For all sizes less than 100%, we select 10 randomsubset of the training corpus of the required size, andreport the average perplexity of 10 n-gram models.This will enable us to observe the improvement ofthe n-gram LMs on as we increase the training datasize.
The idea is to test the hypothesis that not onlyis there significant gap between predictive power ofthe n-gram LMs on setsN andM, but also that thisdifference does not diminish by adding more train-ing data.
In other words, we want to show that theproblem is not due to lack of robust estimation ofthe model parameters but due to the fact that the in-cluded features in the model (n-grams) are not in-formative enough for the positions N .3.2 SetupThe n-gram LMs are built on 400M words fromvarious Broadcast News (BN) data sources includ-ing (Chen et al, 2006): 1996 CSR Hub4 LanguageModel data, EARS BN03 closed captions, GALEPhase 2 Distillation GNG Evaluation Supplemen-tal Multilingual data, Hub4 acoustic model trainingscripts (corresponding to the 300 Hrs), TDT4 closedcaptions, TDT4 newswire, GALE Broadcast Con-versations, and GALE Broadcast News.
All the LMsare trained using modified Kneser-Ney smoothing.To build the LMs, we sample from each source andbuild a source specific LM on the sampled data.
Thefinal LMs are then built by interpolating those LMs.Also, we do not apply any pruning to the trainedLMs, a step that is often necessary for speech recog-nition but not so for perplexity measurement.
Thetest set consists of the NIST rt04 evaluation data set,dev04f evaluation set, and rt03 evaluation set.
Thetest data includes about 70K words.We use the parser of (Huang and Harper, 2009),which achieves state-of-the-art performance onbroadcast news data, to identify the word poisonsthat belong to N and M, as was described in Sec-tion 3.1.
The parser is trained on the Broadcast Newstreebank from Ontonotes (Weischedel et al, 2008)and the WSJ Penn Treebank (Marcus et al, 1993)along with self-training on 1996 Hub4 CSR (Garo-folo et al, 1996) utterances.3.3 AnalysisWe found that |N ||N?M| ?
0.25 in our test data.
Inother words, two previous exposed headwords gobeyond 2-gram history for about 25% of the testdata.!
"#$%#$"#&%#&"#'%%#'%# (%# )%# *%# "%# +%# !%# $%# &%# '%%#,-./01-#223#456#78/9:9:;#</=/#>9?-#456#@AB# @# B#(a)!
"#$%#$"#&%#&"#'%#'"#(%%#(%# )%# *%# +%# "%# !%# $%# &%# '%# (%%#,-./01-#223#456#78/9:9:;#</=/#>9?-#456#@AB# @# B#(b)Figure 2: Reduction in perplexity with increasing trainingdata size on the entire test setN +M, on its syntacticallylocal subset M, and the syntactically distant subset N .The figure shows relative perplexity instead of absoluteperplexity ?
100% being the perplexity for the smallesttraining set size ?
so that (a) 3-gram and (b) 4-gram LMsmay be directly compared.We train 3-gram and 4-gram LMs on10%,20%,?
?
?
,100% of the BN training data,where each 10% increase corresponds to about40M words of training text data.
Figure 2 showsreduction in perplexity with increasing training datasize on the entire test setN+M, on its syntacticallylocal subsetM, and the syntactically distant subsetN .
The figure basically shows relative perplexityinstead of absolute perplexity ?
100% being the53Position Training Data Sizein 40M words 400M wordsTest Set 3-gram 4-gram 3-gram 4-gramM 166 153 126 107N 228 217 191 171N +M 183 170 143 123PPLNPPLM138% 142% 151% 161%Table 1: Perplexity of 3-gram and 4-gram LMs on syntac-tically local (M) and syntactically distant (N ) positionsin the test set for different training data sizes, showing thesustained higher perplexity in distant v/s local positions.perplexity for the smallest training set size ?
so therate of improvement for 3-grams and 4-gram LMscan be compared.
As can be seen from Figure 2,there is a substantial gap between the improvementrate of perplexity in syntactically distant positionscompared to that in syntactically local positions(with 400M woods of training data, this gap is about10% for both 3-gram and 4-gram LMs).
In otherwords, increasing the training data size has muchmore effect on improving the predictive power ofthe model for the positions included inM.
Also, bycomparing Figure 2(a) to 2(b) one can observe thatthe gap is not overcome by increasing the contextlength (using 4-gram features).Also, to better illustrate the performance of the n-gram LMs for different portions of our test data, wereport the absolute values of PPL results in Table 1.It can be seen that there exits a significant differencebetween perplexity of sets N and M and that thedifference gets larger as we increase the training datasize.4 Dependency Language ModelsTo overcome the lack of predictive power of n-gramLMs in syntactically-distant positions, we use theSLM framework to build a long-span LM.
Our hopeis to show not only that long range syntactic depen-dencies improve over n-gram features, but also thatthe improvement is largely due to better predictivepower in the syntactically distant positions N .Syntactic information may be encoded in termsof headwords and headtags of phrases, which maybe extracted from a syntactic analysis of a sen-tence (Chelba and Jelinek, 2000; Kuo et al, 2009),such as a dependency structure.
A dependency ina sentence holds between a dependent (or modifier)word and a head (or governor) word: the dependentdepends on the head.
These relations are encoded ina dependency tree (Figure 3), a directed graph whereeach edge (arc) encodes a head-dependent relation.The specific parser used to obtain the syntacticstructure is not important to our investigation.
Whatis crucial, however, is that the parser proceeds left-to-right, and only hypothesized structures based onw1, .
.
.
, wi?1 are used by the SLM to predict wi.Similarly, the specific features used by the parserare also not important: more noteworthy is that theSLM uses (h.w?3, h.w?2, h.w?1) and their POStags to predict wi.
The question is whether thisyields lower perplexity than predicting wi from(wi?3, wi?2, wi?1).For the sake of completeness, we next describethe parser and SLM in some detail, but either maybe skipped without loss of continuity.The Parser: We use the shift-reduce incremen-tal dependency parser of (Sagae and Tsujii, 2007),which constructs a tree from a transition sequencegoverned by a maximum-entropy classifier.
Shift-reduce parsing places input words into a queue Qand partially built structures are organized by a stackS.
Shift and reduce actions consume the queue andbuild the output parse on the stack.
The classi-fier g assigns probabilities to each action, and theprobability of a state pg(pi) can be computed as theproduct of the probabilities of a sequence of ac-tions that resulted in the state.
The parser thereforeprovides (multiple) syntactic analyses of the historyw1, .
.
.
, wi?1 at each word position wi.The Dependency Language Model: Parser statesat position wi, called history-states, are denoted?
?i = {pi0?i, pi1?i ?
?
?
, piKi?i }, where Ki is the totalnumber of such states.
Given ?
?i, the probabilityassignment for wi is given byp(wi|W?i) =|?
?i|?j=1p(wi|f(pij?i))pg(pij?i|W?i) (1)where, W?i is the word history w1, .
.
.
, wi?1 forwi, pij?i is the jth history-state of position i,pg(pij?i|W?i) is the probability assigned to pij?i by54stepaction stack queuei asked the vice president ...-0asked the vice president ...shift1 ithe vice president for ...shift2 i askedthe vice president for ...left-reduce3 askedifor his endorsement ...shift6 asked the vice presidentifor his endorsement ...left-reduce7 asked the presidenti vice<s>   i   asked   the vice president   for    his  endorsementThursday, March 29, 12for his endorse ent ...left-reduce8 asked presidentivicethefor his endorsement ...right-reduce9 askedivicethepresidentThursday, March 29, 12stepaction stack queuei asked the vice president ...-0asked the vice president ...shift1 ithe vice president for ...shift2 i askedthe vice president for ...left-reduce3 askedifor his endorsement ...shift6 asked the vice presidentifor his endorsement ...left-reduce7 asked the presidenti vice<s>   i   asked   the vice president   for    his  endorsementThursday, March 29, 12Tuesday, April 3, 12Figure 3: Actions of a shift-reduce parser to producethe dependency structure (up to the word president)shown above.the parser, and f(pij?i) denotes an equivalence clas-sification of the parser history-state, capturing fea-tures from pij?i that are useful for predicting wi.We restrict f(pi) to be based on only the heads ofthe partial trees {s0 s1 ?
?
? }
in the stack.
For exam-ple, in Figure 3, one possible parser state for pre-dicting the word for is the entire stack shown afterstep 8, but we restrict f(?)
to depend only on theheadwords asked/VB and president/NNP.Given a choice of f(?
), the parameters of themodel p(wi|f(pij?i)) are estimated to maximize thelog-likelihood of the training data T using theBaum-Welch algorithm (Baum, 1972), and the re-sulting estimate is denoted pML(wi|f(pij?i)).The estimate pML(w|f(?))
must be smoothed tohandle unseen events, which we do using the methodof Jelinek and Mercer (1980).
We use a fine-to-coarse hierarchy of features of the history-state asillustrated in Figure 4.
WithfM (pi?i) ?
fM?1(pi?i) ?
.
.
.
?
f1(pi?i)denoting the set of M increasingly coarser equiv-alence classifications of the history-state pi?i,we linearly interpolate the higher order esti-mates pML(w|fm(pi?i))with lower order estimatespML(w|fm?1(pi?i))aspJM(wi|fm(pi?i))= ?fmpML(wi|fm(pi?i))+(1?
?fm)pJM(wi|fm?1(pi?i)),for 1 ?
m ?
M , where the 0-th order modelpJM(wi|f0(pi?i)) is a uniform distribution.HW+HT :(h.w0h.t0, h.w 1h.t 1, h.w 2h.t 2)(h.w0h.t0)()(h.w0, h.t0, h.w 1, h.t 1, h.t 2)(h.w0, h.t0, h.t 1)(h.t0)Saturday, April 14, 12Figure 4: The hierarchal scheme of fine-to-coarse con-texts used for Jelinek-Mercer smoothing in the SLM.The coefficients ?fm(pi?i) are estimated on a held-out set using the bucketing algorithm suggested byBahl (1983), which ties ?fm(pi?i)?s based on thecount of fm(pi?i)?s in the training data.
We use theexpected count of the features from the last iterationof EM training, since the pi?i are latent states.We perform the bucketing algorithm for each levelf1, f2, ?
?
?
, fM of equivalence classification sepa-rately, and estimate the bucketed ?c(fm) using theBaum-Welch algorithm (Baum, 1972) to maximizethe likelihood of held out data, where the word prob-ability assignment in Eq.
1 is replaced with:p(wi|W?i) =|?i|?j=1pJM(wi|fM (pij?i))pg(pij?i|W?i).The hierarchy shown in Figure 4 is used1 for obtain-ing a smooth estimate pJM(?|?)
at each level.5 SLM ExperimentsWe train a dependency SLM for two different tasks,namely Broadcast News (BN) and Wall Street Jour-nal (WSJ).
Unlike Section 3.2, where we sweptthrough multiple training sets of multiple sizes,1The original SLM hierarchical interpolation scheme is ag-gressive in that it drops both the tag and headword from thehistory.
However, in many cases the headword?s tag alone issufficient, suggesting a more gradual interpolation.
Keeping theheadtag adds more specific information and at the same timeis less sparse.
A similar idea is found, e.g., in the back-off hi-erarchical class n-gram language model (Zitouni, 2007) whereinstead of backing off from the n-gram right to the (n ?
1)-gram a more gradual backoff ?
by considering a hierarchy offine-to-coarse classes for the last word in the history?
is used.55training the SLM is computationally intensive.
Yet,useful insights may be gained from the 40M wordcase.
So we choose the source of text most suitablefor each task, and proceed as follows.5.1 SetupThe following summarizes the setup for eachtask:?
BN setup : EARS BN03 corpus, which hasabout 42M words serves as our training text.We also use rt04 (45K words) as our evaluationdata.
Finally, to interpolate our structured lan-guage models with the baseline 4-gram model,we use rt03+dev04f (about 40K words) data setsto serve as our development set.
The vocabularywe use in BN experiments has about 84K words.?
WSJ setup : The training text consists of about37M words.
We use eval92+eval93 (10Kwords) as our evaluation set and dev93 (9Kwords) serves as our development set for inter-polating SLMs with the baseline 4-gram model.In both cases, we sample about 20K sentences fromthe training text (we exclude them from trainingdata) to serve as our heldout data for applying thebucketing algorithm and estimating ??s.
To applythe dependency parser, all the data sets are firstconverted to Treebank-style tokenization and POS-tagged using the tagger of (Tsuruoka et al, 2011)2.Both the POS-tagger and the shift-reduce depen-dency parser are trained on the Broadcast News tree-bank from Ontonotes (Weischedel et al, 2008) andthe WSJ Penn Treebank (after converting them todependency trees) which consists of about 1.2M to-kens.
Finally, we train a modified kneser-ney 4-gramLM on the tokenized training text to serve as ourbaseline LM, for both experiments.5.2 Results and AnalysisTable 2 shows the perplexity results for BN and WSJexperiments, respectively.
It is evident that the 4-gram baseline for BN is stronger than the 40M caseof Table 1.
Yet, the interpolated SLM significantlyimproves over the 4-gram LM, as it does for WSJ.2To make sure we have a proper LM, the POS-tagger anddependency parser only use features from history to tag a wordposition and produce the dependency structure.
All lookaheadfeatures used in (Tsuruoka et al, 2011) and (Sagae and Tsujii,Language Model Dev EvalBNKneser-Ney 4-gram 165 158SLM 168 159KN+SLM Interpolation 147 142WSJKneser-Ney 4-gram 144 121SLM 149 125KN+SLM Interpolation 132 110Table 2: Test set perplexities for different LMs on the BNand WSJ tasks.Also, to show that, in fact, the syntactic depen-dencies modeled through the SLM parameterizationis enhancing predictive power of the LM in the prob-lematic regions, i.e.
syntactically-distant positions,we calculate the following (log) probability ratio foreach position in the test data,log pKN+SLM(wi|W?i)pKN(wi|W?i), (2)where pKN+SLM is the word probability assign-ment of the interpolated SLM at each position, andpKN(wi) is the probability assigned by the baseline4-gram model.
The quantity above measures the im-provement (or degradation) gained as a result of us-ing the SLM parameterization3.Figures 5(a) and 5(b) illustrate the histogram ofthe above probability ratio for all the word positionsin evaluation data of BN and WSJ tasks, respectively.In these figures the histograms for syntactically-distant and syntactically-local are shown separatelyto measure the effect of the SLM for either of theposition types.
It can be observed in the figuresthat for both tasks the percentage of positions withlog pKN+SLM(wi|W?i)pKN(wi|W?i) around zero is much higher forsyntactically-local (blue bars) than the syntactically-distant (red bars).
To confirm this, we calculatethe average log pKN+SLM(wi|W?i)pKN(wi|W?i) ?this is the aver-age log-likelihood improvement, which is directly2007) are excluded.3If log pKN+SLM(wi|W?i)pKN(wi|W?i) is greater than zero, then the SLMhas a better predictive power for word position wi.
This is ameaningful comparison due to the fact that the probability as-signment using both SLM and n-gram is a proper probability(which sums to one over all words at each position).56?1 ?0.5 0 0.5 1 1.5 2 2.5 3 3.5 40246810121416Probability Ratio (Log)Percentage Positions (%)Syntactically?local positions    (mean=0.1372)Syntactically?distant postions  (mean=0.2351)(a) BN?1 ?0.5 0 0.5 1 1.5 2 2.5 3 3.5 40246810121416182022Probability Ratio (Log)Percentage Positions (%)Syntactically?local positions    (mean=0.0984)Syntactically?distant postions  (mean=0.2124)(b) WSJFigure 5: Probability ratio histogram of SLM to 4-grammodel for (a) BN task (b) WSJ task.related to perplexity improvement?
for each posi-tion type in the figures.Table 3, reports the perplexity performance ofeach LM (baseline 4-gram, SLM and interpolatedSLM) on different positions of the evaluation datafor BN and WSJ tasks.
As it can be observed fromthis table, the use of long-span dependencies in theSLM partially fills the gap between the performanceof the baseline 4-gram LM on syntactically-distantpositionsN versus syntactically-local positionsM.In addition, it can be seen that the SLM by itselffills the gap substantially, however, due to its under-lying parameterization which is based on Jelinek-Mercer smoothing it has a worse performance onregular syntactically-local positions (which accountfor the majority of the positions) compared to theKneser-Ney smoothed LM4.
Therefore, to improvethe overall performance, the interpolated SLM takesadvantage of both the better modeling performanceof Kneser-Ney for syntactically-local positions and4This is merely due to the superior modeling power andbetter smoothing of the Kneser-Ney LM (Chen and Goodman,1998).Test Set 4-gram SLM 4-gram + SLMPosition BNM 146 152 132N 201 182 171N +M 158 159 142PPLNPPLM138% 120% 129%WSJM 114 120 105N 152 141 131N +M 121 125 110PPLNPPLM133% 117% 125%Table 3: Perplexity on the BN and WSJ evaluation sets forthe 4-gram LM, SLM and their interpolation.
The SLMhas lower perplexity than the 4-gram in syntactically dis-tant positions N , and has a smaller discrepancy PPLNPPLMbetween preplexity on the distant and local predictions,complementing the 4-gram model.the better features included in the SLM for improv-ing predictive power on syntactically-distant posi-tions.6 ConclusionThe results of Table 1 and Figure 2 suggest thatpredicting the next word is about 50% more diffi-cult when its syntactic dependence on the historyreaches beyond n-gram range.
They also suggestthat this difficulty does not diminish with increas-ing training data size.
If anything, the relative diffi-culty of word positions with nonlocal dependenciesrelative to those with local dependencies appears toincrease with increasing training data and n-gramorder.
Finally, it appears that language models thatexploit long-distance syntactic dependencies explic-itly at positions where the n-gram is least effectiveare beneficial as complementary models.Tables 2 and 3 demonstrates that a particular,recently-proposed SLM with such properties im-proves a 4-gram LM trained on a large corpus.AcknowledgmentsThanks to Kenji Sagae for sharing his shift-reducedependency parser and the anonymous reviewers forhelpful comments.57ReferencesLR Bahl.
1983.
A maximum likelihood approach tocontinuous speech recognition.
IEEE Transactionson Pattern Analysis and Machine Inteligence (PAMI),5(2):179?190.L.
E. Baum.
1972.
An equality and associated maxi-mization technique in statistical estimation for proba-bilistic functions of Markov processes.
Inequalities,3:1?8.C.
Chelba and F. Jelinek.
2000.
Structured lan-guage modeling.
Computer Speech and Language,14(4):283?332.SF Chen and J Goodman.
1998.
An empirical study ofsmoothing techniques for language modeling.
Techni-cal report, Computer Science Group, Harvard Univer-sity.S.
Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,H.
Soltau, and G. Zweig.
2006.
Advances in speechtranscription at IBM under the DARPA EARS pro-gram.
IEEE Transactions on Audio, Speech and Lan-guage Processing, pages 1596?1608.M Collins, B Roark, and M Saraclar.
2005.
Discrimina-tive syntactic language modeling for speech recogni-tion.
In ACL.Ahmad Emami and Frederick Jelinek.
2005.
A Neu-ral Syntactic Language Model.
Machine learning,60:195?227.Denis Filimonov and Mary Harper.
2009.
A jointlanguage model with fine-grain syntactic tags.
InEMNLP.John Garofolo, Jonathan Fiscus, William Fisher, andDavid Pallett, 1996.
CSR-IV HUB4.
Linguistic DataConsortium, Philadelphia.Zhongqiang Huang and Mary Harper.
2009.
Self-Training PCFG grammars with latent annotationsacross languages.
In EMNLP.Frederick Jelinek and Robert L. Mercer.
1980.
Inter-polated estimation of Markov source parameters fromsparse data.
In Proceedings of the Workshop on Pat-tern Recognition in Practice, pages 381?397.S.
Khudanpur and J. Wu.
2000.
Maximum entropy tech-niques for exploiting syntactic, semantic and colloca-tional dependencies in language modeling.
ComputerSpeech and Language, pages 355?372.H.
K. J. Kuo, L. Mangu, A. Emami, I. Zitouni, andL.
Young-Suk.
2009.
Syntactic features for Arabicspeech recognition.
In Proc.
ASRU.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):330.Ariya Rastrow, Mark Dredze, and Sanjeev Khudanpur.2011.
Efficient discrimnative training of long-spanlanguage models.
In IEEE Workshop on AutomaticSpeech Recognition and Understanding (ASRU).K.
Sagae and J. Tsujii.
2007.
Dependency parsingand domain adaptation with LR models and parser en-sembles.
In Proc.
EMNLP-CoNLL, volume 7, pages1044?1050.Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichiKazama.
2011.
Learning with Lookahead :Can History-Based Models Rival Globally OptimizedModels ?
In Proc.
CoNLL, number June, pages 238?246.Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,Martha Palmer, Nianwen Xue, Mitchell Marcus, AnnTaylor, Craig Greenberg, Eduard Hovy, Robert Belvin,and Ann Houston, 2008.
OntoNotes Release 2.0.
Lin-guistic Data Consortium, Philadelphia.Imed Zitouni.
2007.
Backoff hierarchical class n-gram language models: effectiveness to model unseenevents in speech recognition.
Computer Speech &Language, 21(1):88?104.58
