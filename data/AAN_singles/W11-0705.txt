Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 30?38,Portland, Oregon, 23 June 2011. c?2011 Association for Computational LinguisticsSentiment Analysis of Twitter DataApoorv Agarwal Boyi Xie Ilia Vovsha Owen Rambow Rebecca PassonneauDepartment of Computer ScienceColumbia UniversityNew York, NY 10027 USA{apoorv@cs, xie@cs, iv2121@, rambow@ccls, becky@cs}.columbia.eduAbstractWe examine sentiment analysis on Twitterdata.
The contributions of this paper are: (1)We introduce POS-specific prior polarity fea-tures.
(2) We explore the use of a tree kernel toobviate the need for tedious feature engineer-ing.
The new features (in conjunction withpreviously proposed features) and the tree ker-nel perform approximately at the same level,both outperforming the state-of-the-art base-line.1 IntroductionMicroblogging websites have evolved to become asource of varied kind of information.
This is due tonature of microblogs on which people post real timemessages about their opinions on a variety of topics,discuss current issues, complain, and express posi-tive sentiment for products they use in daily life.
Infact, companies manufacturing such products havestarted to poll these microblogs to get a sense of gen-eral sentiment for their product.
Many times thesecompanies study user reactions and reply to users onmicroblogs.
One challenge is to build technology todetect and summarize an overall sentiment.In this paper, we look at one such popular mi-croblog called Twitter and build models for classify-ing ?tweets?
into positive, negative and neutral senti-ment.
We build models for two classification tasks:a binary task of classifying sentiment into positiveand negative classes and a 3-way task of classi-fying sentiment into positive, negative and neutralclasses.
We experiment with three types of models:unigram model, a feature based model and a treekernel based model.
For the feature based modelwe use some of the features proposed in past liter-ature and propose new features.
For the tree ker-nel based model we design a new tree representa-tion for tweets.
We use a unigram model, previouslyshown to work well for sentiment analysis for Twit-ter data, as our baseline.
Our experiments show thata unigram model is indeed a hard baseline achievingover 20% over the chance baseline for both classifi-cation tasks.
Our feature based model that uses only100 features achieves similar accuracy as the uni-gram model that uses over 10,000 features.
Our treekernel based model outperforms both these modelsby a significant margin.
We also experiment witha combination of models: combining unigrams withour features and combining our features with the treekernel.
Both these combinations outperform the un-igram baseline by over 4% for both classificationtasks.
In this paper, we present extensive featureanalysis of the 100 features we propose.
Our ex-periments show that features that have to do withTwitter-specific features (emoticons, hashtags etc.
)add value to the classifier but only marginally.
Fea-tures that combine prior polarity of words with theirparts-of-speech tags are most important for both theclassification tasks.
Thus, we see that standard nat-ural language processing tools are useful even ina genre which is quite different from the genre onwhich they were trained (newswire).
Furthermore,we also show that the tree kernel model performsroughly as well as the best feature based models,even though it does not require detailed feature en-gineering.We use manually annotated Twitter data for our30experiments.
One advantage of this data, over pre-viously used data-sets, is that the tweets are col-lected in a streaming fashion and therefore representa true sample of actual tweets in terms of languageuse and content.
Our new data set is available toother researchers.
In this paper we also introducetwo resources which are available (contact the firstauthor): 1) a hand annotated dictionary for emoti-cons that maps emoticons to their polarity and 2)an acronym dictionary collected from the web withEnglish translations of over 5000 frequently usedacronyms.The rest of the paper is organized as follows.
Insection 2, we discuss classification tasks like sen-timent analysis on micro-blog data.
In section 3,we give details about the data.
In section 4 we dis-cuss our pre-processing technique and additional re-sources.
In section 5 we present our prior polarityscoring scheme.
In section 6 we present the designof our tree kernel.
In section 7 we give details of ourfeature based approach.
In section 8 we present ourexperiments and discuss the results.
We concludeand give future directions of research in section 9.2 Literature SurveySentiment analysis has been handled as a NaturalLanguage Processing task at many levels of gran-ularity.
Starting from being a document level classi-fication task (Turney, 2002; Pang and Lee, 2004), ithas been handled at the sentence level (Hu and Liu,2004; Kim and Hovy, 2004) and more recently atthe phrase level (Wilson et al, 2005; Agarwal et al,2009).Microblog data like Twitter, on which users postreal time reactions to and opinions about ?every-thing?, poses newer and different challenges.
Someof the early and recent results on sentiment analysisof Twitter data are by Go et al (2009), (Berminghamand Smeaton, 2010) and Pak and Paroubek (2010).Go et al (2009) use distant learning to acquire senti-ment data.
They use tweets ending in positive emoti-cons like ?:)?
?:-)?
as positive and negative emoti-cons like ?:(?
?:-(?
as negative.
They build mod-els using Naive Bayes, MaxEnt and Support Vec-tor Machines (SVM), and they report SVM outper-forms other classifiers.
In terms of feature space,they try a Unigram, Bigram model in conjunctionwith parts-of-speech (POS) features.
They note thatthe unigram model outperforms all other models.Specifically, bigrams and POS features do not help.Pak and Paroubek (2010) collect data following asimilar distant learning paradigm.
They perform adifferent classification task though: subjective ver-sus objective.
For subjective data they collect thetweets ending with emoticons in the same manneras Go et al (2009).
For objective data they crawltwitter accounts of popular newspapers like ?NewYork Times?, ?Washington Posts?
etc.
They re-port that POS and bigrams both help (contrary toresults presented by Go et al (2009)).
Both theseapproaches, however, are primarily based on ngrammodels.
Moreover, the data they use for training andtesting is collected by search queries and is thereforebiased.
In contrast, we present features that achievea significant gain over a unigram baseline.
In addi-tion we explore a different method of data represen-tation and report significant improvement over theunigram models.
Another contribution of this paperis that we report results on manually annotated datathat does not suffer from any known biases.
Ourdata is a random sample of streaming tweets unlikedata collected by using specific queries.
The sizeof our hand-labeled data allows us to perform cross-validation experiments and check for the variance inperformance of the classifier across folds.Another significant effort for sentiment classifica-tion on Twitter data is by Barbosa and Feng (2010).They use polarity predictions from three websites asnoisy labels to train a model and use 1000 manuallylabeled tweets for tuning and another 1000 manu-ally labeled tweets for testing.
They however donot mention how they collect their test data.
Theypropose the use of syntax features of tweets likeretweet, hashtags, link, punctuation and exclamationmarks in conjunction with features like prior polar-ity of words and POS of words.
We extend theirapproach by using real valued prior polarity, and bycombining prior polarity with POS.
Our results showthat the features that enhance the performance of ourclassifiers the most are features that combine priorpolarity of words with their parts of speech.
Thetweet syntax features help but only marginally.Gamon (2004) perform sentiment analysis onfeeadback data from Global Support Services sur-vey.
One aim of their paper is to analyze the role31of linguistic features like POS tags.
They performextensive feature analysis and feature selection anddemonstrate that abstract linguistic analysis featurescontributes to the classifier accuracy.
In this paperwe perform extensive feature analysis and show thatthe use of only 100 abstract linguistic features per-forms as well as a hard unigram baseline.3 Data DescriptionTwitter is a social networking and microbloggingservice that allows users to post real time messages,called tweets.
Tweets are short messages, restrictedto 140 characters in length.
Due to the nature of thismicroblogging service (quick and short messages),people use acronyms, make spelling mistakes, useemoticons and other characters that express specialmeanings.
Following is a brief terminology associ-ated with tweets.
Emoticons: These are facial ex-pressions pictorially represented using punctuationand letters; they express the user?s mood.
Target:Users of Twitter use the ?@?
symbol to refer to otherusers on the microblog.
Referring to other users inthis manner automatically alerts them.
Hashtags:Users usually use hashtags to mark topics.
Thisis primarily done to increase the visibility of theirtweets.We acquire 11,875 manually annotated Twitterdata (tweets) from a commercial source.
They havemade part of their data publicly available.
For infor-mation on how to obtain the data, see Acknowledg-ments section at the end of the paper.
They collectedthe data by archiving the real-time stream.
No lan-guage, location or any other kind of restriction wasmade during the streaming process.
In fact, theircollection consists of tweets in foreign languages.They use Google translate to convert it into Englishbefore the annotation process.
Each tweet is labeledby a human annotator as positive, negative, neutralor junk.
The ?junk?
label means that the tweet can-not be understood by a human annotator.
A man-ual analysis of a random sample of tweets labeledas ?junk?
suggested that many of these tweets werethose that were not translated well using Googletranslate.
We eliminate the tweets with junk la-bel for experiments.
This leaves us with an unbal-anced sample of 8,753 tweets.
We use stratified sam-pling to get a balanced data-set of 5127 tweets (1709tweets each from classes positive, negative and neu-tral).4 Resources and Pre-processing of dataIn this paper we introduce two new resources forpre-processing twitter data: 1) an emoticon dictio-nary and 2) an acronym dictionary.
We preparethe emoticon dictionary by labeling 170 emoticonslisted on Wikipedia1 with their emotional state.
Forexample, ?:)?
is labeled as positive whereas ?:=(?
islabeled as negative.
We assign each emoticon a labelfrom the following set of labels: Extremely-positive,Extremely-negative, Positive, Negative, and Neu-tral.
We compile an acronym dictionary from an on-line resource.2 The dictionary has translations for5,184 acronyms.
For example, lol is translated tolaughing out loud.We pre-process all the tweets as follows: a) re-place all the emoticons with a their sentiment po-larity by looking up the emoticon dictionary, b) re-place all URLs with a tag ||U ||, c) replace targets(e.g.
?@John?)
with tag ||T ||, d) replace all nega-tions (e.g.
not, no, never, n?t, cannot) by tag ?NOT?,and e) replace a sequence of repeated characters bythree characters, for example, convert cooooooooolto coool.
We do not replace the sequence by onlytwo characters since we want to differentiate be-tween the regular usage and emphasized usage of theword.Acronym English expansiongr8, gr8t greatlol laughing out loudrotf rolling on the floorbff best friend foreverTable 1: Example acrynom and their expansion in theacronym dictionary.We present some preliminary statistics about thedata in Table 3.
We use the Stanford tokenizer (Kleinand Manning, 2003) to tokenize the tweets.
We usea stop word dictionary3 to identify stop words.
Allthe other words which are found in WordNet (Fell-baum, 1998) are counted as English words.
We use1http://en.wikipedia.org/wiki/List of emoticons2http://www.noslang.com/3http://www.webconfs.com/stop-words.php32Emoticon Polarity:-) :) :o) :] :3 :c) Positive:D C: Extremely-Positive:-( :( :c :[ NegativeD8 D; D= DX v.v Extremely-Negative: | NeutralTable 2: Part of the dictionary of emoticonsthe standard tagset defined by the Penn Treebank foridentifying punctuation.
We record the occurrenceof three standard twitter tags: emoticons, URLs andtargets.
The remaining tokens are either non Englishwords (like coool, zzz etc.)
or other symbols.Number of tokens 79,152Number of stop words 30,371Number of English words 23,837Number of punctuation marks 9,356Number of capitalized words 4,851Number of twitter tags 3,371Number of exclamation marks 2,228Number of negations 942Number of other tokens 9047Table 3: Statistics about the data used for our experi-ments.In Table 3 we see that 38.3% of the tokens are stopwords, 30.1% of the tokens are found in WordNetand 1.2% tokens are negation words.
11.8% of allthe tokens are punctuation marks excluding excla-mation marks which make up for 2.8% of all tokens.In total, 84.1% of all tokens are tokens that we ex-pect to see in a typical English language text.
Thereare 4.2% tags that are specific to Twitter which in-clude emoticons, target, hastags and ?RT?
(retweet).The remaining 11.7% tokens are either words thatcannot be found in WordNet (like Zzzzz, kewl) orspecial symbols which do not fall in the category ofTwitter tags.5 Prior polarity scoringA number of our features are based on prior po-larity of words.
For obtaining the prior polarity ofwords, we take motivation from work by Agarwalet al (2009).
We use Dictionary of Affect in Lan-guage (DAL) (Whissel, 1989) and extend it usingWordNet.
This dictionary of about 8000 Englishlanguage words assigns every word a pleasantnessscore (?
R) between 1 (Negative) - 3 (Positive).
Wefirst normalize the scores by diving each score mythe scale (which is equal to 3).
We consider wordswith polarity less than 0.5 as negative, higher than0.8 as positive and the rest as neutral.
If a word is notdirectly found in the dictionary, we retrieve all syn-onyms from Wordnet.
We then look for each of thesynonyms in DAL.
If any synonym is found in DAL,we assign the original word the same pleasantnessscore as its synonym.
If none of the synonyms ispresent in DAL, the word is not associated with anyprior polarity.
For the given data we directly foundprior polarity of 81.1% of the words.
We find po-larity of other 7.8% of the words by using WordNet.So we find prior polarity of about 88.9% of Englishlanguage words.6 Design of Tree KernelWe design a tree representation of tweets to combinemany categories of features in one succinct conve-nient representation.
For calculating the similaritybetween two trees we use a Partial Tree (PT) ker-nel first proposed by Moschitti (2006).
A PT ker-nel calculates the similarity between two trees bycomparing all possible sub-trees.
This tree kernelis an instance of a general class of convolution ker-nels.
Convolution Kernels, first introduced by Haus-sler (1999), can be used to compare abstract objects,like strings, instead of feature vectors.
This is be-cause these kernels involve a recursive calculationover the ?parts?
of abstract object.
This calculationis made computationally efficient by using DynamicProgramming techniques.
By considering all possi-ble combinations of fragments, tree kernels captureany possible correlation between features and cate-gories of features.Figure 1 shows an example of the tree structurewe design.
This tree is for a synthesized tweet:@Fernando this isn?t a great day for playing theHARP!
:).
We use the following procedure to con-vert a tweet into a tree representation: Initialize themain tree to be ?ROOT?.
Then tokenize each tweetand for each token: a) if the token is a target, emoti-con, exclamation mark, other punctuation mark, or anegation word, add a leaf node to the ?ROOT?
with33VBGforEWPOSplayingPOSSTOPNN dayPOSEW EWNN CAPS harpEXC ||P||STOPthis||T||greatROOTNOTSTOP EWis JJVBGG forEW POSVBGG f orEVBGforEW VBG ffoff ffrffFigure 1: Tree kernel for a synthesized tweet: ?
@Fernando this isn?t a great day for playing the HARP!
:)?the corresponding tag.
For example, in the tree inFigure 1 we add tag ||T || (target) for ?
@Fernando?,add tag ?NOT?
for the token ?n?t?, add tag ?EXC?for the exclamation mark at the end of the sentenceand add ||P || for the emoticon representing positivemood.
b) if the token is a stop word, we simply addthe subtree ?
(STOP (?stop-word?))?
to ?ROOT?.
Forinstance, we add a subtree corresponding to each ofthe stop words: this, is, and for.
c) if the token isan English language word, we map the word to itspart-of-speech tag, calculate the prior polarity of theword using the procedure described in section 5 andadd the subtree (EW (?POS?
?word?
?prior polarity?
))to the ?ROOT?.
For example, we add the subtree(EW (JJ great POS)) for the word great.
?EW?
refersto English word.
d) For any other token <token>we add subtree ?
(NE (<token>))?
to the ?ROOT?.?NE?
refers to non-English.The PT tree kernel creates all possible subtreesand compares them to each other.
These subtreesinclude subtrees in which non-adjacent branches be-come adjacent by excising other branches, thoughorder is preserved.
In Figure 1, we show some ofthe tree fragments that the PT kernel will attempt tocompare with tree fragments from other trees.
Forexample, given the tree (EW (JJ) (great) (POS)), thePT kernel will use (EW (JJ) (great) (POS)), (EW(great) (POS)), (EW (JJ) (POS)), (EW (JJ) (great)),(EW (JJ)), (EW (great)), (EW (POS)), (EW), (JJ),(great), and (POS).
This means that the PT tree ker-nel attempts to use full information, and also ab-stracts away from specific information (such as thelexical item).
In this manner, it is not necessary tocreate by hand features at all levels of abstraction.7 Our featuresWe propose a set of features listed in Table 4 for ourexperiments.
These are a total of 50 type of features.We calculate these features for the whole tweet andfor the last one-third of the tweet.
In total we get100 additional features.
We refer to these features asSenti-features throughout the paper.Our features can be divided into three broad cat-egories: ones that are primarily counts of variousfeatures and therefore the value of the feature is anatural number ?
N. Second, features whose valueis a real number ?
R. These are primarily featuresthat capture the score retrieved from DAL.
Thirdly,features whose values are boolean ?
B.
These arebag of words, presence of exclamation marks andcapitalized text.
Each of these broad categories isdivided into two subcategories: Polar features andNon-polar features.
We refer to a feature as polarif we calculate its prior polarity either by lookingit up in DAL (extended through WordNet) or in theemoticon dictionary.
All other features which arenot associated with any prior polarity fall in the Non-polar category.
Each of Polar and Non-polar featuresis further subdivided into two categories: POS andOther.
POS refers to features that capture statisticsabout parts-of-speech of words and Other refers toall other types of features.In reference to Table 4, row f1 belongs to the cat-egory Polar POS and refers to the count of numberof positive and negative parts-of-speech (POS) in atweet, rows f2, f3, f4 belongs to the category Po-34lar Other and refers to count of number of negationwords, count of words that have positive and neg-ative prior polarity, count of emoticons per polaritytype, count of hashtags, capitalized words and wordswith exclamation marks associated with words thathave prior polarity, row f5 belongs to the categoryNon-Polar POS and refers to counts of differentparts-of-speech tags, rows f6, f7 belong to the cat-egory Non-Polar Other and refer to count of num-ber of slangs, latin alphabets, and other words with-out polarity.
It also relates to special terms such asthe number of hashtags, URLs, targets and newlines.Row f8 belongs to the category Polar POS and cap-tures the summation of prior polarity scores of wordswith POS of JJ, RB, VB and NN.
Similarly, row f9belongs to the category Polar Other and calculatesthe summation of prior polarity scores of all words,row f10 refers to the category Non-Polar Other andcalculates the percentage of tweet that is capitalized.Finally, row f11 belongs to the category Non-Polar Other and refers to presence of exclamationand presence of capitalized words as features.8 Experiments and ResultsIn this section, we present experiments and resultsfor two classification tasks: 1) Positive versus Nega-tive and 2) Positive versus Negative versus Neutral.For each of the classification tasks we present threemodels, as well as results for two combinations ofthese models:1.
Unigram model (our baseline)2.
Tree kernel model3.
100 Senti-features model4.
Kernel plus Senti-features5.
Unigram plus Senti-featuresFor the unigram plus Senti-features model, wepresent feature analysis to gain insight about whatkinds of features are adding most value to the model.We also present learning curves for each of the mod-els and compare learning abilities of models whenprovided limited data.Experimental-Set-up: For all our experiments weuse Support Vector Machines (SVM) and report av-eraged 5-fold cross-validation test results.
We tunethe C parameter for SVM using an embedded 5-foldcross-validation on the training data of each fold,i.e.
for each fold, we first run 5-fold cross-validationonly on the training data of that fold for differentvalues of C. We pick the setting that yields the bestcross-validation error and use that C for determin-ing test error for that fold.
As usual, the reportedaccuracies is the average over the five folds.8.1 Positive versus NegativeThis is a binary classification task with two classesof sentiment polarity: positive and negative.
We usea balanced data-set of 1709 instances for each classand therefore the chance baseline is 50%.8.1.1 Comparison of modelsWe use a unigram model as our baseline.
Re-searchers report state-of-the-art performance forsentiment analysis on Twitter data using a unigrammodel (Go et al, 2009; Pak and Paroubek, 2010).Table 5 compares the performance of three models:unigram model, feature based model using only 100Senti-features, and the tree kernel model.
We reportmean and standard deviation of 5-fold test accuracy.We observe that the tree kernels outperform the uni-gram and the Senti-features by 2.58% and 2.66% re-spectively.
The 100 Senti-features described in Ta-ble 4 performs as well as the unigram model thatuses about 10,000 features.
We also experiment withcombination of models.
Combining unigrams withSenti-features outperforms the combination of ker-nels with Senti-features by 0.78%.
This is our bestperforming system for the positive versus negativetask, gaining about 4.04% absolute gain over a hardunigram baseline.8.1.2 Feature AnalysisTable 6 presents classifier accuracy and F1-measure when features are added incrementally.
Westart with our baseline unigram model and subse-quently add various sets of features.
First, we addall non-polar features (rows f5, f6, f7, f10, f11 in Ta-ble 4) and observe no improvement in the perfor-mance.
Next, we add all part-of-speech based fea-tures (rows f1, f8) and observe a gain of 3.49% overthe unigram baseline.
We see an additional increasein accuracy by 0.55% when we add other prior po-larity features (rows f2, f3, f4, f9 in Table 4).
From35NPolarPOS # of (+/-) POS (JJ, RB, VB, NN) f1Other # of negation words, positive words, negative words f2# of extremely-pos., extremely-neg., positive, negative emoticons f3# of (+/-) hashtags, capitalized words, exclamation words f4Non-PolarPOS # of JJ, RB, VB, NN f5Other # of slangs, latin alphabets, dictionary words, words f6# of hashtags, URLs, targets, newlines f7RPolarPOS For POS JJ, RB, VB, NN,?prior pol.
scores of words of that POS f8Other?prior polarity scores of all words f9Non-Polar Other percentage of capitalized text f10B Non-Polar Other exclamation, capitalized text f11Table 4: N refers to set of features whose value is a positive integer.
They are primarily count features; for example,count of number of positive adverbs, negative verbs etc.
R refers to features whose value is a real number; for example,sum of the prior polarity scores of words with part-of-speech of adjective/adverb/verb/noun, and sum of prior polarityscores of all words.
B refers to the set of features that have a boolean value; for example, presence of exclamationmarks, presence of capitalized text.Model Avg.
Acc (%) Std.
Dev.
(%)Unigram 71.35 1.95Senti-features 71.27 0.65Kernel 73.93 1.50Unigram +Senti-features75.39 1.29Kernel +Senti-features74.61 1.43Table 5: Average and standard deviation for test accuracyfor the 2-way classification task using different models:Unigram (baseline), tree kernel, Senti-features, unigramplus Senti-features, and tree kernel plus senti-features.these experiments we conclude that the most impor-tant features in Senti-features are those that involveprior polarity of parts-of-speech.
All other featuresplay a marginal role in achieving the best performingsystem.
In fact, we experimented by using unigramswith only prior polarity POS features and achieved aperformance of 75.1%, which is only slightly lowerthan using all Senti-features.In terms of unigram features, we use InformationGain as the attribute evaluation metric to do featureselection.
In Table 7 we present a list of unigramsthat consistently appear as top 15 unigram featuresacross all folds.
Words having positive or negativeprior polarity top the list.
Emoticons also appear asimportant unigrams.
Surprisingly though, the wordfor appeared as a top feature.
A preliminary analy-Features Acc.F1 MeasurePos NegUnigram baseline 71.35 71.13 71.50+ f5, f6, f7, f10, f11 70.1 69.66 70.46+ f1, f8 74.84 74.4 75.2+ f2, f3, f4, f9 75.39 74.81 75.86Table 6: Accuracy and F1-measure for 2-way classifica-tion task using Unigrams and Senti-features.
All fi referto Table 4 and are cumulative.Positive words love, great, good, thanksNegative words hate, shit, hell, tiredEmoticons ||P || (positive emoticon),||N || (negative emoticon)Other for, ||U || (URL)Table 7: List of top unigram features for 2-way task.sis revealed that the word for appears as frequentlyin positive tweets as it does in negative tweets.
How-ever, tweets containing phrases like for you and forme tend to be positive even in the absence of anyother explicit prior polarity words.
Owing to previ-ous research, the URL appearing as a top feature isless surprising because Go et al (2009) report thattweets containing URLs tend to be positive.8.1.3 Learning curveThe learning curve for the 2-way classificationtask is in Figure 2.
The curve shows that when lim-360.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 16264666870727476Percentage of training dataAccuracy(%)UnigramUnigram + Our FeaturesTree KernelFigure 2: Learning curve for two-way classification task.ited data is used the advantages in the performanceof our best performing systems is even more pro-nounced.
This implies that with limited amount oftraining data, simply using unigrams has a criticaldisadvantage, while both tree kernel and unigrammodel with our features exhibit promising perfor-mance.8.2 Positive versus Negative versus NeutralThis is a 3-way classification task with classesof sentiment polarity: positive, negative and neu-tral.
We use a balanced data-set of 1709 instancesfor each class and therefore the chance baseline is33.33%.8.2.1 Comparison of modelsFor this task the unigram model achieves a gainof 23.25% over chance baseline.
Table 8 comparesthe performance of our three models.
We reportmean and standard deviation of 5-fold test accuracy.We observe that the tree kernels outperform the un-igram and the Senti-features model by 4.02% and4.29% absolute, respectively.
We note that this dif-ference is much more pronounced comparing to thetwo way classification task.
Once again, our 100Senti-features perform almost as well as the unigrambaseline which has about 13,000 features.
We alsoexperiment with the combination of models.
Forthis classification task the combination of tree ker-nel with Senti-features outperforms the combinationof unigrams with Senti-features by a small margin.Model Avg.
Acc (%) Std.
Dev.
(%)Unigram 56.58 1.52Senti-features 56.31 0.69Kernel 60.60 1.00Unigram +Senti-features60.50 2.27Kernel +Senti-features60.83 1.09Table 8: Average and standard deviation for test accuracyfor the 3-way classification task using different models:Unigram (baseline), tree kernel, Senti-features, unigramplus Senti-features, and Senti-features plus tree kernels.This is our best performing system for the 3-wayclassification task, gaining 4.25% over the unigrambaseline.The learning curve for the 3-way classificationtask is similar to the curve of the 2-way classifica-tion task, and we omit it.8.2.2 Feature AnalysisTable 9 presents classifier accuracy and F1-measure when features are added incrementally.
Westart with our baseline unigram model and subse-quently add various sets of features.
First, we add allnon-polar features (rows f5, f6, f7, f10 in Table 4)and observe an small improvement in the perfor-mance.
Next, we add all part-of-speech based fea-tures and observe a gain of 3.28% over the unigrambaseline.
We see an additional increase in accuracyby 0.64% when we add other prior polarity features(rows f2, f3, f4, f9 in Table 4).
These results are inline with our observations for the 2-way classifica-tion task.
Once again, the main contribution comesfrom features that involve prior polarity of parts-of-speech.Features Acc.F1 MeasurePos Neu NegUnigram baseline 56.58 56.86 56.58 56.20+f5, f6, f7, f10, f1156.91 55.12 59.84 55+ f1, f8 59.86 58.42 61.04 59.82+ f2, f3, f4, f9 60.50 59.41 60.15 61.86Table 9: Accuracy and F1-measure for 3-way classifica-tion task using unigrams and Senti-features.The top ranked unigram features for the 3-way37classification task are mostly similar to that of the2-way classification task, except several terms withneutral polarity appear to be discriminative features,such as to, have, and so.9 ConclusionWe presented results for sentiment analysis on Twit-ter.
We use previously proposed state-of-the-art un-igram model as our baseline and report an overallgain of over 4% for two classification tasks: a binary,positive versus negative and a 3-way positive versusnegative versus neutral.
We presented a comprehen-sive set of experiments for both these tasks on manu-ally annotated data that is a random sample of streamof tweets.
We investigated two kinds of models:tree kernel and feature based models and demon-strate that both these models outperform the unigrambaseline.
For our feature-based approach, we do fea-ture analysis which reveals that the most importantfeatures are those that combine the prior polarity ofwords and their parts-of-speech tags.
We tentativelyconclude that sentiment analysis for Twitter data isnot that different from sentiment analysis for othergenres.In future work, we will explore even richer lin-guistic analysis, for example, parsing, semanticanalysis and topic modeling.10 AcknowledgmentsAgarwal and Rambow are funded by NSF grantIIS-0713548.
Vovsha is funded by NSF grantIIS-0916200.
We would like to thank NextGenInvent (NGI) Corporation for providing us withthe Twitter data.
Please contact Deepak Mit-tal (deepak.mittal@ngicorportion.com) about ob-taining the data.ReferencesApoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.2009.
Contextual phrase-level polarity analysis usinglexical affect scoring and syntactic n-grams.
Proceed-ings of the 12th Conference of the European Chapterof the ACL (EACL 2009), pages 24?32, March.Luciano Barbosa and Junlan Feng.
2010.
Robust senti-ment detection on twitter from biased and noisy data.Proceedings of the 23rd International Conference onComputational Linguistics: Posters, pages 36?44.Adam Bermingham and Alan Smeaton.
2010.
Classify-ing sentiment in microblogs: is brevity an advantage isbrevity an advantage?
ACM, pages 1833?1836.C.
Fellbaum.
1998.
Wordnet, an electronic lexicaldatabase.
MIT Press.Michael Gamon.
2004.
Sentiment classification on cus-tomer feedback data: noisy data, large feature vectors,and the role of linguistic analysis.
Proceedings of the20th international conference on Computational Lin-guistics.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.Technical report, Stanford.David Haussler.
1999.
Convolution kernels on discretestructures.
Technical report, University of Californiaat Santa Cruz.M Hu and B Liu.
2004.
Mining and summarizing cus-tomer reviews.
KDD.S M Kim and E Hovy.
2004.
Determining the sentimentof opinions.
Coling.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
Proceedings of the 41st Meet-ing of the Association for Computational Linguistics,pages 423?430.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.
InProceedings of the 17th European Conference on Ma-chine Learning.Alexander Pak and Patrick Paroubek.
2010.
Twitter asa corpus for sentiment analysis and opinion mining.Proceedings of LREC.B.
Pang and L. Lee.
2004.
A sentimental education: Sen-timent analysis using subjectivity analysis using sub-jectivity summarization based on minimum cuts.
ACL.P.
Turney.
2002.
Thumbs up or thumbs down?
seman-tic orientation applied to unsupervised classification ofreviews.
ACL.C M Whissel.
1989.
The dictionary of Affect in Lan-guage.
Emotion: theory research and experience,Acad press London.T.
Wilson, J. Wiebe, and P. Hoffman.
2005.
Recognizingcontextual polarity in phrase level sentiment analysis.ACL.38
