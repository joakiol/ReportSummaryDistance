Proceedings of NAACL HLT 2007, Companion Volume, pages 45?48,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsA Filter-Based Approach to Detect End-of-Utterances from Prosody inDialog SystemsOlac Fuentes, David Vera and Thamar SolorioComputer Science DepartmentUniversity of Texas at El PasoEl Paso, TX 79968AbstractWe propose an efficient method to detectend-of-utterances from prosodic information inconversational speech.
Our method is basedon the application of a large set of binary andramp filters to the energy and fundamental fre-quency signals obtained from the speech sig-nal.
These filter responses, which can be com-puted very efficiently, are used as input to alearning algorithm that generates the final de-tector.
Preliminary experiments using data ob-tained from conversations show that an accu-rate classifier can be trained efficiently and thatgood results can be obtained without requiringa speech recognition system.1 IntroductionWhile there have been improvements and a significantnumber of methods introduced into the realm of dialog-based systems, there are aspects of these methods whichcan be further improved upon.
One such aspect is end-of-utterance (EOU) detection, which consists of automat-ically determining when a user has finished his/her turnand is waiting to receive an answer from the system.
Cur-rent dialog-based systems use a simple pause threshold,which commonly results in either unnecessary long wait-ing times or interruptions from the system when the usermakes a pause in the middle of an utterance.
These prob-lems can annoy and discourage users using even simpledialog systems.Most previous methods aimed at improving uponpause thresholds for detecting end-of-utterances usespectral energy measures (Hariharan et al, 2001; Jia andXu, 2002).
Other methods use prosodic features with(Ferrer et al, 2002) and without speech recognition sys-tems (Ferrer et al, 2003) in conjunction with decisiontrees to determine end-of-utterances as quickly as possi-ble.
For this and related problems, the choice of featuresis critical.
Most common is to use a fixed inventory offeatures, chosen based on the linguistics literature andpast experience (Shriberg and Stolcke, 2004).
Recentlywe have experimented with alternative approaches, in-cluding features hand-tailored to specific discriminationproblems (Ward and Al Bayyari, 2006) and random ex-ploration of the feature space (Solorio et al, 2006).
Inthis paper we explore yet another approach, using a largebattery of very simple and easy to evaluate features.In this paper we present a method to improve the ac-curacy that can be obtained in end-of-utterance detectionthat uses prosodic information only, without a speech rec-ognizer.
We adapt and extend a filter-based approachoriginally proposed in computer graphics (Crow, 1984)and later exploited successfully in computer vision (Violaand Jones, 2001) and music retrieval (Ke et al, 2005).Our approach consists of applying simple filters, whichcan be computed in constant time, in order to generateattributes to be used by a learning algorithm.
After theattributes have been generated, we test different learningalgorithms to detect end-of-utterances.
Our results showthat the features yield good results in combination withseveral of the classifiers, with the best result being ob-tained with bagging ensembles of decision trees.2 MethodThe first stage in our system is to extract prosodic infor-mation from the raw audio signal.
Using the audio anal-ysis tool Didi, the log energy and fundamental frequencysignals are extracted from the source sound wave.
Aftercomputing log energy and pitch, we apply a large set offilters in the time domain to the energy and pitch signalsin order to generate attributes suitable for classification.We compute the filter responses for both signals at everytime step using three types of filters, each applied at manydifferent times scales.The first filter type, shown in Figure 1a), is a two-stepbinary filter, split approximately in half.
The first halfof the filter consists of a sequence of 1?s.
The secondhalf consists of -1?s.
The second filter type is a three-stepbinary filter (as shown in Figure 1b)), split in approximatethirds alternating between 1 and -1.
Finally, the third filteris an upward slope ranging from -1 to 1.Although simple, these filters, in particular when theyare applied at multiple scales, can characterize most ofthe prosodic features that are known to be relevant inidentifying dialog phenomena including raises and fallsin pitch and pauses of different lengths.The response of any of these filters over the signal atany time is given by the dot product of the filter and signal455 10 15 20 25 30 35 40?1?0.8?0.6?0.4?0.200.20.40.60.81a) Type I filter5 10 15 20 25 30 35 40 45 50 55 60?1?0.8?0.6?0.4?0.200.20.40.60.81b) Type II filter5 10 15 20 25 30 35 40?1?0.8?0.6?0.4?0.200.20.40.60.81c) Type III filterFigure 1: The three types of filters, the first two beingbinary only having values -1 or 1, and the last having anupward slope from -1 to 1.window of the same length.
Computing this dot productis slow, especially over larger time window sizes.
Thiscost is even greater when many filter responses are takenover the course of the entire signal length.Given the large number of filters and the size of a nor-mal audio signal, the straightforward dot-product-basedcomputation of the filter responses is prohibitively expen-sive.
Fortunately, it is possible to device methods to com-pute these responses efficiently, as explained in the nextsubsection.2.1 Efficient Filter ComputationThis constant time computation of binary filters for two-dimensional signals was first presented by Crow (Crow,1984) in the field of computer graphics and later ap-plied successfully in computer vision (Viola and Jones,2001).
Here we show how that can be adapted to one-dimensional signals and extended to the case of non-binary filters, such as ramps.Let s be the signal corresponding to either the log en-ergy or the fundamental frequency.
Let f be a filter ofsize n (in arbitrary time units) and let k be the time in-stant for which we want to compute the filter response.0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5x 104?3?2.5?2?1.5?1?0.500.511.520 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5x 104?80?60?40?20020406080Figure 2: Energy, with mean subtracted, and its corre-sponding integral signal.The filter response F is given byF (s, f, k) =n?1?i=0sk+i ?
fiThe standard computation of F takes O(n) operations;however, for the special case of binary filters like the onesshown in figures 1a) and 1b), we can compute this re-sponse in constant time with some preprocessing as fol-lows.
Let I be the integral signal, where each element ofI is given byIj =j?i=0siIt can be seen thatk?i=jsj = Ik ?
Ij?1Thus this summation can be computed with two ac-cesses to memory, after pre-computing and storing thevalues of I in an array.
Figure 2 shows an example of asignal (with its mean subtracted) and the correspondingintegral signal.Consider a binary filter f such as the one shown in1a), f = {1n/2,?1n/2}, that is, f consists of n/2 onesfollowed by n/2 negative ones.
Then the filter responseof a signal can then be computed in constant time usingthree references to the integral signal I:F (s, f, k) = 2Ik+n/2?1 ?
Ik?1 ?
Ik+n?1Similarly, the response to a filter like the one shown inFigure 1 b), given by f = {1n/3,?1n/3, 1n/3} can becomputed with four memory references.F (s, f, k) = Ik+n?1?
2Ik+2n/3?1+2Ik+n/3?1?
Ik?146The third filter is an upward ramp ranging from -1 to1.
Whereas the binary filters are simple to calculate us-ing look-up values, and their application to 1-dimensionalsignals is a simple adaptation to the 2-D algorithm, aramp is more difficult and requires separate preprocess-ing for filters of different lengths.
Regardless, it is stillpossible to compute its response in constant time afterpreprocessing.We define a ramp filter of length n as f = {?1, 2n?1 ?1, 4n?1 ?
1, ..., 1?
2n?1 , 1}.
The response to this filter isF (s, f, k) =n?1?i=0sk+i ?
fi=n?1?i=0( 2in?
1 ?
1)sk+i= 2n?
1n?1?i=0isk+i ?n?1?i=0sk+i= 2n?
1n?1?i=0i sk+i ?
(In?1 ?
Ik?1)Let ?n?1i=0 i sk+i be denoted by Ank.
Clearly, if Ankcan be computed in constant time, then F (s, f, k) canalso be computed in constant time.
This can be done, witha little preprocessing, as follows.
Let An0 be computedin the standard (O(n) time) way,An0 =n?1?i=0isiThen to compute values of Ank for k > 0 we firstobserve thatAnk = sk+1 + 2sk+2 + .
.
.+ (n?
1)sk+n?1andAnk+1 = sk+2 + 2sk+3 + .
.
.+ (n?
1)sk+nFrom this, we can see thatAnk+1 = Ank ?
sk+1 ?
sk+2 ?
.
.
.
?sk+n?1 + (n?
1)sk+n= Ank ?k+n?1?i=k+1si + (n?
1)sk+n= Ank ?
(Ik+n?1 ?
Ik) + (n?
1)sk+nThus after pre-computing vectors I and An, whichtakes linear time in the size of the signal, we can computeany filter response in constant time.
However, while wecan derive all binary-filter responses from vector I , com-puting ramp-filter responses requires the pre-computationof a separate An for every filter length n. Nevertheless,this cost is small compared to the cost of computing a dotproduct for every time instant in the input signal.The integral signal representations are computed fromthe two prosodic feature signals, and filter features arecalculated along the timeframe of the signal.
Once the fil-ter responses are obtained, they are used as attributes formachine learning algorithms, which are used to generatethe final classifiers.
The data is then used to train severallearning algorithms, as implemented in Weka (Witten andFrank, 2005).3 Experimental ResultsExperiments were conducted to test the end-of-utterancesystem on pre-recorded conversations, measuring preci-sion and recall of positive classifications.
The conver-sations used contained speech by both male and femaleusers to compare the robustness among different vocalrange frequencies.3.1 DataThe training set of data is derived from about 22 min-utes of conversations, with the audio split such that eachspeaker is on a separate voice channel (see (Hollingsed,2006)).
In 17-minutes worth of these, volunteers wereasked to list eight exits on the east side of El Paso onInterstate 10.
This provided a clear way to measure end-of-utterances as if a system were prompting users for in-put.
This set of conversations contained a large number ofturn-switches, which also simulated voice-portal systemswell.
For most of the time in this set, the same person(a female) is conducting the quiz.
However, the speakerstaking the quiz have distinctly different voices and aremixed in gender.Five minutes of the training set were taken from acasual conversation also containing a male and femalespeaker combination.
The speakers in this conversa-tion are different from the speakers in the other dataset.Adding these data balances the training set, reducing theprobability of the system learning only the specific quizformat used in much of the training data.Didi was used to extract the prosodic features, and thefilter responses were computed for each of the three fil-ter types, in sizes ranging from 200ms to 3 seconds inincrements of 50ms, totaling 342 features per time in-stance.
The class was set to 0 or 1, signaling non-end-of-utterances and a confirmed end-of-utterances, respec-tively.
992 instances were created for the experiments,split equally in two between positive examples of end-of-utterances, and randomly selected negative examples forboth channels in the source audio.47Table 1: Experimental results of using different classifiers andaveraging ten ten-fold-cross-validation evaluations with randomseeds per classifier.Recall Precision F-MeasureDec.Stump 0.623 0.705 0.660Dec.Table 0.768 0.799 0.783C4.5 0.792 0.800 0.796Boost(DS) 0.792 0.820 0.806Bag(REP) 0.850 0.833 0.841Bag(C4.5) 0.786 0.797 0.791All instances used for training were randomly cho-sen.
The positive examples were chosen from human-determined end-of-utterance intervals, which rangedfrom the time instant a valid end-of-utterance wasrecorded to a point either 1.5 seconds after that instant ora start-of-utterance that occurred prior to that time.
Thenegative examples were randomly chosen such that notime instance was chosen prior to the 3-second-mark ofthe audio file used and none was within a marked end-of-utterance interval.3.2 ResultsSix combinations of classifiers were generated using theWeka data mining tool.
Each of these classifier combina-tions was tested using 10-fold cross-validation.
The re-sults reflect the average of ten such cross-validation runs,each using a different random seed.
The final classifiercombinations used are Weka?s implementations of deci-sion stumps, decision tables, C4.5 (Quinlan, 1993) andensembles of decision stumps using boosting and C4.5and reduced error pruning (REP) decision trees (Quinlan,1987) using bagging.The experiments performed yield interesting results.Table 1 shows that, with the exception of decisionstumps, which are perhaps too simple for this task, allclassifiers performed well, which shows that our filtersproduce suitable features for classification.
The best re-sults were obtained using bagging and REP trees, but re-sults for other methods yield similar precision and recall.It is almost certain that better results can be obtainedusing these methods if bleeding across channels in theaudio streams was reduced.
The F0 features do a good jobof filtering out possible mistakes in the system due to theway the frequencies are calculated.
However, bleedingcan still mislead the classifiers into perceiving an end-of-utterance from another speaker.4 Conclusions and Future WorkWe have shown a new filter-based method for detect-ing end-of-utterances in conversation using only basicprosodic information.
We adapted and extended previ-ously described methods for fast computation of filter re-sponses, which allows our system to be trained quicklyand easily permits real-time performance.
Preliminaryexperiments in the task of classifying windows in dialogrecordings as being end-of-utterances or not have yieldedvery promising results using standard classification algo-rithms, with an f-measure of 0.84.Present and future work includes evaluating themethod as a component of a real-time dialog system,where its usefulness at decreasing waiting time can betested.
We are also working on methods for feature se-lection and compression to obtain further speedup, andfinally we are experimenting with larger datasets.Acknowledgement: The authors would like to thankNSF for partially supporting this work under grants IIS-0415150 and 0080940.ReferencesF.
C. Crow.
1984.
Summed-area tables for texture mapping.In Proceedings of the 11th Annual Conference on ComputerGraphics and Interactive Techniques.L.
Ferrer, E. Shriberg, and A. Stolcke.
2002.
Is the speakerdone yet?
Faster and more accurate end-of-utterance detec-tion using prosody.
In Proceedings of ICSLP.L.
Ferrer, E. Shriberg, and A. Stolcke.
2003.
A prosody-basedapproach to end-of-utterance detection that does not requirespeech recognition.
In Proceedings of IEEE ICASSP.R.
Hariharan, J. Hakkinen, and K. Laurila.
2001.
Robust end-of-utterance detection for real-time speech recognition appli-cations.
In Proceedings of IEEE ICASSP.T.
K. Hollingsed.
2006.
Responsive behavior in tutorial spokendialogues.
Master?s thesis, University of Texas at El Paso.C.
Jia and B. Xu.
2002.
An improved entropy-based endpointdetection algorithm.
In Proceedings of ISCSLP.Y.
Ke, D. Hoiem, and R. Sukthankar.
2005.
Computer visionfor music identification.
In Proceedings of IEEE CVPR.J.
R. Quinlan.
1987.
Simplifying decision trees.
InternationalJournal of Man-Machine Studies, 27.J.
R. Quinlan.
1993.
C4.5: Programs for Machine Learning.Morgan Kaufman.E.
Shriberg and A. Stolcke.
2004.
Direct modeling of prosody:An overview of applications in automatic speech processing.In Proceedings of ICSP.T.
Solorio, O. Fuentes, N. Ward, and Y. Al Bayyari.
2006.Prosodic feature generation for back-channel prediction.
InProceedings of Interspeech.P.
Viola and M. Jones.
2001.
Rapid object detection using aboosted cascade of simple features.
In Proceedings of IEEECVPR.N.
Ward and Y. Al Bayyari.
2006.
A case study in the identi-fication of prosodic cues to turn-taking: Back-channeling inArabic.
In Proceedings of Interspeech.I.
H. Witten and E. Frank.
2005.
Data Mining: Practical ma-chine learning tools and techniques.
Morgan Kaufman.48
