Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1086?1095,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPEmpirical Exploitation of Click Data for Task Specific RankingAnlei Dong Yi Chang Shihao Ji Ciya Liao Xin Li Zhaohui ZhengYahoo!
Labs701 First AvenueSunnyvale, CA 94089{anlei,yichang,shihao,ciyaliao,xinli,zhaohui}@yahoo-inc.comAbstractThere have been increasing needs for taskspecific rankings in web search such asrankings for specific query segments likelong queries, time-sensitive queries, navi-gational queries, etc; or rankings for spe-cific domains/contents like answers, blogs,news, etc.
In the spirit of ?divide-and-conquer?, task specific ranking may havepotential advantages over generic rankingsince different tasks have task-specific fea-tures, data distributions, as well as feature-grade correlations.
A critical problem forthe task-specific ranking is training datainsufficiency, which may be solved by us-ing the data extracted from click log.
Thispaper empirically studies how to appro-priately exploit click data to improve rankfunction learning in task-specific ranking.The main contributions are 1) the explo-ration on the utilities of two promising ap-proaches for click pair extraction; 2) theanalysis of the role played by the noiseinformation which inevitably appears inclick data extraction; 3) the appropriatestrategy for combining training data andclick data; 4) the comparison of click datawhich are consistent and inconsistent withbaseline function.1 IntroductionLearning-to-rank approaches (Liu, 2008) havebeen widely applied in commercial search en-gines, in which ranking models are learned usinglabeled documents.
Significant efforts have beenmade in attempt to learn a generic ranking modelwhich can appropriately rank documents for allqueries .
However, web users?
query intentions areextremely heterogeneous, which makes it difficultfor a generic ranking model to achieve best rank-ing results for all queries.
For this reason, therehave been increasing needs for task specific rank-ings in web search such as rankings for specificquery segments like long queries, time-sensitivequeries, navigational queries, etc; or rankingsfor specific domains/contents like answers, blogs,news, etc.
Therefore, a specific ranking task usu-ally correspond to a category of queries; whenthe search engine determines that a query is be-longing to this category, it will call the rankingfunction dedicated to this ranking task.
The mo-tivation of this divide-and-conquer strategy is that,task specific ranking may have potential advan-tages over generic ranking since different taskshave task-specific features, data distributions, aswell as feature-grade correlations.Such a dedicated ranking model can be trainedusing the labeled data belonging to this query cat-egory (which is called dedicated training data).However, the amount of training data dedicatedto a specific ranking task is usually insufficientbecause human labeling is expensive and time-consuming, not to mention there are multiple rank-ing tasks that need to be taken care of.
To dealwith the training data insufficiency problem fortask-specific ranking, we propose to extract click-through data and incorporate it with dedicatedtraining data to learn a dedicated model.In order to incorporate click data to improve theranking for a dedicate query category, it is criticalto fully exploit click information.
We empiricallyexplore the related approaches for the appropriateclick data exploitation in task-specific rank func-tion learning.
Figure 1 illustrates the proceduresand critical components to be studied.1) Click data mining: the purpose is to extractinformative and reliable users?
preference infor-mation from click log.
We employ two promis-ing approaches: one is heuristic rule approach, theother is sequential supervised learning approach.2) Sample selection and combination: with la-beled training data and unlabeled click data, how1086Generic training dataDedicated training dataGBrank algorithmTask-specific ranking modelGeneric click dataDedicated click dataSample selection and combinationClick log Click data mining?
Heuristic-rule-based approach?
Sequential supervised learning approachFigure 1: Framework of incorporating click-through data with training data to improve dedi-cated model for task-specific ranking.to select and combine them so that the sampleshave the best utility for learning?
As the datadistribution for a specific ranking task is differ-ent from the generic data distribution, it is nat-ural to select those labeled training samples andunlabeled click preference pairs which belong tothis query category, so that the data distributionsof training set and testing set are consistent forthis category.
On the other hand, we should keepin mind that: a) non-dedicated data, i.e, the datathat does not belong the specific category, mightalso have similar distribution as the dedicated data.Such distribution similarity makes non-dedicateddata also useful for task-specific rank functionlearning, especially for the scenario that dedicatedtraining samples is insufficient.
b) The quality ofdedicated click data may be not as reliable as hu-man labeled training data.
In other words, thereare some extracted click preference pairs that areinconsistent with human labeling while we regardhuman labeling as correct labeling.3) Rank function learning algorithm: we useGBrank (Zheng et al, 2007) algorithm for rankfunction learning, which has proved to be oneof the most effective up-to-date learning-to-rankalgorithms; furthermore, GBrank algorithm alsotakes preference pairs as inputs, which will be il-lustrated with more details in the paper.2 Related workLearning to rank has been a promising researcharea which continuously improves web search rel-evance (Burges et al, 2005) (Zha et al, 2006)(Cao et al, 2007) (Freund et al, 1998) (Fried-man, 2001) (Joachims, 2002) (Wang and Zhai,2007) (Zheng et al, 2007).
The ranking prob-lem is usually formulated as learning a rankingfunction from preference data.
The basic ideais to minimize the number of contradicted pairsin the training data, and different algorithm castthe preference learning problem from differentpoint of view, for example, RankSVM (Joachims,2002) uses support vector machines; RankBoost(Freund et al, 1998) applies the idea of boost-ing from weak learners; GBrank (Zheng et al,2007) uses gradient boosting with decision tree;RankNet (Burges et al, 2005) uses gradient boost-ing with neural net-work.
In (Zha et al, 2006),query difference is taken into consideration forlearning effective retrieval function, which leadsto a multi-task learning problem using risk mini-mization framework.There are a few related works to apply multi-ple ranking models for different query categories.However, none of them takes click-through infor-mation into consideration.
In (Kang and Kim,2003), queries are categorized into 3 types, infor-mational, navigational and transactional, and dif-ferent models are applied on each query category.a KNN method is proposed to employ differentranking models to handle different types of queries(Geng et al, 2008).
The KNN method is unsuper-vised, and it targets to improve the overall rankinginstead of the rank-ing for a certain query cate-gory.
In addition, the KNN method requires allfeature vector to be the same.Quite a few research papers explore how to ob-tain useful information from click-through data,which could benefit search relevance (Carteretteet al, 2008) (Fox et al, 2005) (Radlinski andJoachims, 2007) (Wang and Zhai, 2007).
The in-formation can be expressed as pair-wise prefer-ences (Chapelle and Zhang, 2009) (Ji et al, 2009)(Radlinski et al, 2008), or represented as rank fea-tures (Agichtein et al, 2006).
Task-specific rank-ing relies on the accuracy of query classification.Query classification or query intention identifica-tion has been extensively studied in (Beitzel et al,2007) (Lee et al, 2005) (Li et al, 2008) (Rose andLevinson, 2004).
How to combine editorial dataand click data is well discussed in (Chen et al,2008) (Zheng et al, 2007).
In addition, how to useclick data to improve ranking are also exploitedin personalized or preference-based search (Coyle1087Table 1: Statistics of click occurrences for heuris-tic rule approach.imp impression, number of occurrence of the tuplecc number of occurrence of the tuple where twodocuments both get clickedncc number of occurrence of the tuple where url1is not clicked but url2is clickedcnc number of occurrence of the tuple where url1is clicked but url2is not clickedncnc number of occurrence of the tuple where url1and url2are not clickedand Smyth, 2007) (Glance, 2001) (R. Jin, 2008).3 Technical approachThis section presents the related approaches inFigure 1.
In Section 4, we will make deeper anal-ysis based on experimental results.3.1 Click data miningWe use two approaches for click data mining,whose outputs are preference pairs.
A preferencepair is defined as a tuple {< xq, yq> |xq?
yq},which means for the query q, the document xqismore relevant than yq.
We need to extract infor-mative and reliable preference pairs which can beused to improve rank function learning.3.1.1 Heuristic rule approachWe use heuristic rules to extract skip-above pairsand skip-next pairs, which are similar to Strategy1 (click > skip above) and Strategy 5 (click > no-click next) proposed in (Joachims et al, 2005).
Toreduce the misleading effect of an individual clickbehavior, click information from different querysessions is aggregated before applying heuristicrules.
For a tuple (q, url1, url2, pos1, pos2) whereq is query, url1and url2are urls representing twodocuments, pos1and pos2are ranking positionsfor the two documents with pos1?
pos2mean-ing url1has higher rank than url2, the statistics forthis tuple are listed in Table 1.Skip-above pair extraction: if ncc is muchlarger than cnc, andccimp,ncncimpis much smallerthan 1, that means, when url1is ranked higher thanurl2in query q, most users click url2but not clickurl1.
In this case, we extract a skip-above pair, i.e.,url2is more relevant than url1.
In order to havehighly accurate skip-above pairs, a set of thresh-Table 2: Skip-above pairs count vs. human judge-ments (e.g., the element in the third row and sec-ond column means we have 40 skip-above pairswith ?excellent?
url1and ?perfect?
url2).
P: per-fect; E: excellent; G: good; F: fair; B: bad.P E G F BP 13 13 12 4 0E 40 44 16 2 2G 27 53 103 29 8F 10 15 43 27 5B 4 4 11 20 14Table 3: Skip-next pairs vs. human judgements(e.g., the element in the third row and second col-umn means we have 10 skip-next pairs with ?ex-cellent?
url1and ?perfect?
url2).
P: perfect; E:excellent; G: good; F: fair; B: bad.P E G F BP 126 343 225 100 35E 10 71 84 37 12G 6 9 116 56 21F 1 5 17 29 14B 1 1 1 2 5olds are applied to only extract the pairs that havehigh impression and ncc is larger enough than cnc.Skip-next pair extraction: if pos1= pos2?
1,cnc is much larger than ncc, andccimp,ncncimpis muchsmaller than 1, that means, in most of cases whenurl2is ranked just below url1in query q, mostusers click url1but not click url2.
In this case, weregard this tuple as a skip-next pair.To test the accuracy of preference pairs, weask editors to judge some randomly selected pairsfrom skip-above pairs and skip-next pairs.
Edi-tors label each query-url pair using five grades ac-cording to relevance: perfect, excellent, good, fair,bad.
Table 2 shows skip-above pair distribution.The diagonal elements have high values, whichare for tied pairs labeled by editors but determinedas skip-above pairs from heuristic rules.
Highervalues appear in the left-bottom triangle than inthe right-top triangle, because there are more skip-above preferences agreed with editors than dis-agreed with editors.
Summing up the tied pairs,agreed and disagreed pairs, 44% skip-above pref-erence judgments agree with editors, 18% skip-above preference judgments disagree with editors,1088and there are 38% skip-above pairs judged as tiepairs by editors.Table 3 shows skip-next pair distribution.
Sum-ming up the tied pairs, agreed and disagreed pairs,70% skip-next preference judgments agree witheditors, 4% skip-next preference judgments dis-agree with editors, and 26% skip-next pairs judgedas tie pairs by editors.Therefore, skip-next pairs have much higheraccuracy than skip-above.
That is because in asearch engine that already has a good rankingfunction, it is much easier to find a correct skip-next pairs which are consistent with the search en-gine than to find a correct skip-above pairs whichare contradictory to the search engine.
Skip-aboveand skip-next preferences provide us two kinds ofusers?s feedbacks which are complementary: skip-above preferences provide us the feedback that theuser?s vote is contradictory to the current ranking,which implies the current relative ranking shouldbe reversed; skip-next preferences shows that theuser?s vote is consistent with the current ranking,which implies the current relative ranking shouldbe maintained with high confidence provided byusers?
vote.3.1.2 Sequential supervised learningThe click modeling by sequential supervisedlearning (SSL) was proposed in (Ji et al, 2009),in which user?s sequential click information isexploited to extract relevance information fromclick-logs.
This approach is reliable because 1)the sequential click information embedded in anaggregation of user clicks provides substantial rel-evance information of the documents displayed inthe search results, and 2) the SSL is supervisedlearning (i.e., human judgments are provided withrelevance labels for the training).The SSL is formulated in the frameworkof global ranking (Qin et al, 2008).
Letx(q)= {x(q)1, x(q)2, .
.
.
, x(q)n} represent the doc-uments retrieved with a query q, and y(q)={y(q)1, y(q)2, .
.
.
, y(q)n} represent the relevance la-bels assigned to the documents.
Here n is thenumber of documents retrieved with q. Withoutloss of generality, we assume that n is fixed andinvariant with respect to different queries.
TheSSL determines to find a function F in the formof y(q)=F (x(q)) that takes all the documents asits inputs, exploiting both local and global infor-mation among the documents, and predict the rel-evance labels of all the document jointly.
Thisis distinct to most of learning to rank methodsthat optimize a ranking model defined on a sin-gle document, i.e., in the form of y(q)i=f(x(q)i),?
i = 1, 2, .
.
.
, n. This formulation of the SSLis important in extracting relevance informationfrom user click data since users?
click decisionsamong different documents displayed in a searchsession tend to rely not only on the relevance judg-ment of a single document, but also on the relativerelevance comparison among the documents dis-played; and the global ranking framework is well-formulated to exploit both local and global infor-mation from an aggregation of user clicks.The SSL aggregates all the user sessions forthe same query into a tuple <query, n-documentlist, and an aggregation of user clicks>.
Fig-ure 2 illustrates the process of feature extrac-tion from an aggregated session, where x(q)={x(q)1, x(q)2, .
.
.
, x(q)n} denotes a sequence of fea-ture vectors extracted from the aggregated ses-sion, with x(q)irepresenting the feature vector ex-tracted for document i.
Specifically, to form fea-ture vector x(q)i, first a feature vector x(q)i,jis ex-tracted from each user j?s click information, andj ?
{1, 2, .
.
.
}, then x(q)iis formed by averagingover x(q)i,j, ?j ?
{1, 2, .
.
.
}, i.e., x(q)iis actually anaggregated feature vector for document i. Table4 lists all the features used in the SSL modeling.Note that some features are statistics independentof temporal information of the clicks, such as ?Po-sition?
and ?Frequency?, while other features re-ply on their surrounding documents and the clicksequences.
We use 90,000 query-url pairs to trainthe SSL model, and 10,000 query-url pairs for bestmodel selection.With the sequential click modeling discussedabove, several sequential supervised algorithms,including the conditional random fields (CRF)(Lafferty et al, 2001), the sliding window methodand the recurrent sliding window method (Diet-terich, 2002), are explored to find a global rankingfunction F .
We omit the details but refer one to(Ji et al, 2009).
The emphasis here is on the im-portance to adapt these algorithms to the rankingproblem.After training, the SSL model can be used topredict the relevance labels of all the documents ina new aggregated session, and thus pair-wise pref-erence data can be extracted, with the score dif-ference representing the confidence of preference1089={}?oouser2doc10odocioodoc2odoc1 user1q?
?Feature Extractionx(q)x1x2xix10??={}y(q)y1y2yiy10?
?Figure 2: An illustration of feature extraction foran aggregated session for SSL approach.
x(q)de-notes an extracted sequence of feature vectors, andy(q)denotes the corresponding label sequence thatis assigned by human judges for training.Table 4: Click features used in SSL model.Position Position of the documentin the result listClickRank Rank of 1st click of doc.
in click seq.Frequency Average number of clicks for this doc.FrequencyRank Rank in the list sorted by num.
of clicksIsNextClicked 1 if next position is clicked, 0 otherwiseIsPreClicked 1 if previous position is clicked,0 otherwiseIsAboveClicked 1 if there is a click above, 0 otherwiseIsBelowClicked 1 if there is a click below, 0 otherwiseClickDuration Time spent on the documentprediction.
For the reason of convenience, we alsocall the preference pairs contradicting with pro-duction ranking as skip-above pairs and those con-sistent with production ranking as skip-next pairs,so that we can analyze these two types of prefer-ence pairs respectively.3.2 Modeling algorithmThe basic idea of GBrank (Zheng et al, 2007)is that if the ordering of a preference pairby the ranking function is contradictory to thispreference, we need to modify the rankingfunction along the direction by swapping thisprefence pair.
Preferences pairs could be gen-erated from labeled data, or could be extractedfrom click data.
For each preference pair <x, y > in the available preference set S ={< xi, yi> |xi?
yi, i = 1, 2, ..., N}, x shouldbe ranked higher than y.
In GBrank algorithm, theproblem of learning ranking functions is to com-pute a ranking function h , so that h matches theset of preference, i.e, h(xi) ?
h(yi) , if x ?
y,i = 1, 2, ..., N as many as possible.
The followingloss function is used to measure the risk of a givenranking function h.R(h) =12N?i=1(max{0, h(yi)?h(xi)+?
})2, (1)where ?
is the margin between the two documentsin the pair.
To minimize the loss function, h(x) hasto be larger than h(y) with the margin ?
, which canbe chosen as constant value, or as dynamic val-ues varying with pairs.
When pair-wise judgmentsare extracted from editors?
labels with differentgrades, pair-wise judgments can include grade dif-ference, which can further be used as margin ?
.The GBrank algorithm is illustrated in Algorithm1, and two parameters need to be determined: theshrinkage factor ?
and the number of iteration.Algorithm 1 GBrank algorithm.Start with an initial guess h0, for m = 1, 2, ...1.
Construct a training set: for each < xi, yi>?S, derive (xi,max{0, hm?1(yi) ?
hm1(xi) +?
}), and(yi,?max{0, hm?1(yi)?
hm1(xi) + ?}).2.
Fit hmby using a base regressor with theabove training set.3.
hm= hm?1+?smhm(x), where smis foundby line search to minimize the object function,?
is shrinkage factor.3.3 Sample selection and combinationWe use a straightforward approach to learn rank-ing model from the combined data, which is illus-trated in Algorithm 2.Algorithm 2 Learn ranking model by combiningeditorial data and click preference pairs.Input:?
Editorial absolute judgement data.?
Preference pairs from click data.1.
Extract preference pairs from labeled datawith absolute judgement.2.
Select and combine preference pairs fromclick data and labeled data.3.
Learn GBrank model from the combinedpreference pairs.Absolute judgement on labeled data contains(query, url) pairs with absolute grade values la-beled by human.
In Step 1, for each query with1090nqquery-url pairs with corresponding grades, {<query, urli, gradei> |i = 1, 2, .
.
.
, nq}, its prefer-ence pairs are extracted as{< query, urli, urlj, gradei?
gradej> |i, j =1, 2, .
.
.
, nq, i 6= j} .When combining human-labeled pairs and clickpreference pairs, we can give use different relativeweights for these two data sources.
The loss func-tion becomesR(h) =wNl?i?Labeled(max{0, h(yi)?
h(xi) + ?})21?
wNc?i?Click(max{0, h(yi)?
h(xi) + ?
})2,(2)where w is used to control the relative weights be-tween labeled training data and click data, Nlisthe number of training data pairs, and Ncis thenumber of click pairs.
The margin ?
can be deter-mined as grade difference for editor pairs, and bea constant parameter for click pairs.Step 2 is critical for the efficacy of the approach.A few factors need to be considered:1) data distribution: for the application of task-specific ranking, our purpose is to improve rankingfor the queries belonging to this category.
An im-portant observation is that the relevance patternsfor the ranking within a specific category mayhave some unique characteristics, which are differ-ent from generic relevance ranking.
Thus, it is rea-sonable to consider only using dedicated labeledtraining data and dedicated click preference datafor training.
The reality is that dedicated trainingdata is usually insufficient, while it is possible thatnon-dedicated data can also help the learning.2) click pair quality: it is inevitable there existsome incorrect pairs in the click preference pairs.Such incorrect pairs may mislead the learning.
Sooverall, can the click preference pairs still help thelearning for task-specific ranking?
By our study,skip-above pairs usually contain more incorrectpairs compared with skip-above pairs.
Does thismean skip-next pairs are always more helpful inimproving learning than skip-above pairs?3) click pair utility: use labeled training data asbaseline, how much complimentary informationcan click pairs bring?
This is determined by themethodology of click data mining approach.While it is possible to achieve some learningimprovement for task-specific ranking by usingclick pairs by a plausible method, we attempt toempirically explore the above interweaving fac-tors for deeper understanding, in order to apply themost appropriate strategy to exploit click data onreal-world applications of task-specific ranking.4 Experiments4.1 Data setQuery category: in the experiments, we use longquery ranking as an example of task-specific rank-ing, because it is commonly known that long queryranking has some unique relevance patterns com-pared with generic ranking.
We define the longqueries as the queries containing at least three to-kens.
The techniques and analysis proposed in thispaper can be applied to other ranking tasks, suchas rankings for specific query segments like time-sensitive queries, navigational queries, or rankingsfor specific domains/contents like answers, blogs,news, as long as the tasks have their own charac-teristics of data distributions and discriminant rankfeatures.Labeled training data: we do experimentsbased on a data set for a commercial search en-gine, for which there are 16,797 query-url pairs(with 1,123 different queries) that have been la-beled by editors.
The proportion of long queriesis about 35% of all queries.
The data distributionof such long queries may be different from gen-eral data distribution, as it will be validated in theexperiments below.The human labeled data is randomly split intotwo sets: training set (8,831 query-url pairs, 589queries), and testing set (7,966 query-url pairs,534 queries).
The training set will be combinedwith click preference pairs for rank function learn-ing, and the testing set will be used to evaluate theefficacy of the ranking function.
In the training set,there are 3,842 long query-url pairs (229 queries).At testing stage, the learned rank functions are ap-plied only to the long queries in the testing data,as our concern in this paper is how to improvetask-specific ranking, i.e., long query ranking inthe experiment.
In the testing data, there are 3,210query-url pairs (193 queries) are long query data,which will be used to test rank functions.Click preference pairs: using the two ap-proaches of heuristic rule approach and sequen-tial supervised approach, we extract click prefencepairs from the click log of the search engine.
Eachapproach yields both skip-next and skip-abovepairs, which are sorted by confidence descendingorder respectively.1091Table 5: Use click data by heuristic rule approach(Data Selection: ?N?
: not use; ?D?
: use dedicateddata; ?G?
: use generic data.
Data Source: ?T?
:training data; ?C?
: click data)(a) skip-next pairsNT DT GTNC n/a 0.7736 0.7813DC 0.7822 0.7906 (1.2%) 0.7997(2.4%)GC 0.7834 0.7908 (1.2%) 0.7950 (1.7%)(b) skip-above pairsNT DT GTNC n/a 0.7736 0.7813DC 0.6649 0.7676 (-1.6%) 0.7748 (-0.8%)GC 0.6792 0.7656 (-2.0%) 0.7989 (2.2%)4.2 Setup and measurementsWe try different sample selection and combinationstrategies to train rank functions using GBrank al-gorithm.
For the labeled training data, we eitheruse generic data or dedicated data.
For the clickpreference pairs, we also try these two options.Furthermore, as more click preference pairs maybring more useful information to help the learn-ing while on the other hand, the more incorrectpairs may be given so that they mislead the learn-ing, we try different amounts of these prefencepairs: 5,000, 10,000, 30,000, 50,000, 70,000 and100,000 pairs.We use NDCG to evaluate ranking model,which is defined asNDCGn= Zn?ni=12r(i)?1log(i+1)where i is the position in the document list, r(i) isthe score of Document i, and Znis a normalizationfactor, which is used to make the NDCG of ideallist be 1.4.3 ResultsTable 5 and 6 show the NDCG5results by usingheuristic rule approach and SSL approach respec-tively.
We do not present NDCG1results due tospace limitation, but NDCG1results have the sim-ilar trends as NDCG5.Baseline by training data: there are two base-line functions by using training data sets 1) usededicated training data (DT), NDCG5on the test-ing set by the rank function is 0.7736; 2) usegeneric training data (GT), NDCG5is 0.7813.
Itis reasonable that using generic training data isTable 6: Use click data by SSL approach (DataSelection: ?N?
: not use; ?D?
: use dedicated data;?G?
: use generic data.
Data Source: ?T?
: trainingdata; ?C?
: click data)(a) skip-next pairsNT DT GTNC n/a 0.7736 0.7813DC 0.7752 0.7933 (1.5%) 0.7936 (1.5%)GC 0.7624 0.7844 (0.4%) 0.7914 (1.2%)(b) skip-above pairsNT DT GTNC n/a 0.7736 0.7813DC 0.6756 0.7636 (-2.2%) 0.7784 (-0.3%)GC 0.6860 0.7717 (-1.2%) 0.7774 (-0.5%)better than only using dedicated training data, be-cause the distributions of non-dedicated data anddedicated data share some similarity.
As the ded-icated training data is insufficient, the adoption ofthe extra non-dedicated data helps the learning.We compare learning results with Baseline 2) (usegeneric training data, the slot of NC + GT in thetables), which is the higher baseline.Baseline by click data: we then study the utili-ties of click preference pairs by using them alonefor training without using labeled training data.In Table 5 and 6, each of the NDCG5results us-ing click preference pairs is the highest NDCG5value over the cases of using different amounts ofpairs (5000, 10,000, 30,000, 50,000, 70,000 and100,000 pairs).
The results regarding the pairsamounts are illustrated in Figure 3, which will helpus to analyze the results more deeply.If we only use click preference pairs for training(the two table slots DC+NT and GC+NT, corre-sponding to using dedicated click preference pairsand generic click pairs respectively), the best caseis using skip-next pairs extracted by heuristic ruleapproach (Table 5 (a) ).
It is not surprising thatskip-next pairs outperform skip-above pairs be-cause there are significantly lower percentage ofincorrect pairs in skip-next pairs compared withskip-above pairs.
It is a little bit surprising thatthe case of DC+NT has no dominant advantageover GC+NT as we expected.
For example, in Ta-ble 5 (a), the NDCG5values (0.7822 and 0.7834)are very close to each other.
However, in Figure3, we find that with the same amount of pairs,when we use 30,000 or fewer pairs, using dedi-10921 2 3 4 5 6 7 8 9 10x 1040.7550.760.7650.770.7750.780.7850.790.7950.8click pairsNDCG5dedicate train + dedicate clickgeneric train + dedicate clickdedicate clickgeneric clickFigure 3: Incorporate different amounts of skip-next pairs by heuristic rule approach with generictraining data.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.760.770.780.790.80.81trainging sample weightNDCG5+ 5,000 dedicate click+ 10,000 dedicate click+ 30,000 dedicate clickFigure 4: The effects of using different combin-ing weights.
Skip-next pairs by heuristic rule ap-proach are combined with generic training data.cated click pairs alone is always better than usinggeneric click pairs alone.
With more click pairsbeing used (> 30, 000), the noise rates becomehigher in the pairs, which makes the distributionfactor less important.Combine training data and click data: wecompare the four table slots, DC+DT, GC+DT,DC+GT, GC+GT, in Table 5 and 6, and there arequite a few interesting observations:1) Skip-next vs. skip-above: overall, incorporat-ing skip-next pairs with training data is better thanincorporating skip-above pairs, due to the reasonthat there are more incorrect pairs in skip-abovepairs, which may mislead the learning.
The onlyexception is the slot GC+GT in Table 5 (b), whoseNDCG5improvement is as high as 2.2%.
We fur-ther track this result, and find that this is the caseby using only 5,000 generic skip-above pairs.
Thenoise rate of these 5,000 pairs is low because theyhave the highest pair extraction confidence values.At the same time, these 5,000 pairs may providegood complementary signals to the generic train-ing data, so that the learning result is good.
How-ever, in general, skip-next pairs have better utilitiesthan skip-above pairs.2) Dedicated training data vs. generic train-ing data: using generic training data is gen-erally better than only using dedicated trainingdata.
If training data is insufficient, the extranon-dedicated data provides useful informationfor relevance pattern learning, and the distribu-tion dissimilarity between dedicated data and non-dedicated data is not the most important factor.3) Dedicated click data vs. generic click data:using dedicated click data is more effective thanusing generic click data.
From Figure 3, we ob-serve that when 30,000 or fewer pairs are incorpo-rated into training data, using dedicate click pairsis always better than using generic click pairs.0 0.5 1 1.5 2 2.50.7840.7860.7880.790.7920.7940.7960.7980.80.802?NDCG5+ 5,000 dedicate click+ 10,000 dedicate click+ 30,000 dedicate clickFigure 5: The effects of using different marginvalues for click preference pairs.
Skip-next pairsby heuristic rule approach are incorporated withgeneric training data.4) Heuristic rule approach vs. SSL approach:the preference pairs extracted by heuristic rule ap-proach have better utilities than those extracted bySSL approach.5) GBrank parameters for combining trainingdata and click pairs: the relative weight w forcombining training data and click pairs in (2) mayalso affect rank function learning.
Figure 4 showsthe effects of using different combining weights,1093for which skip-next pairs by heuristic rule ap-proach are combined with generic training data.We observe that neither over-weighting trainingdata or over-weighting click pairs yields good re-sults while the two data sources are best exploitedat certain weight values when there is good bal-ance between them.
Another concern is the ap-propriate margin value ?
for the click pairs in (2).Figure 5 shows that ?
= 1 consistently yields goodlearning results, which suggests us that click pairprovides good information at ?
= 1.4.4 Discussionswe have defactorized the related approaches forexploiting click data to improve task-specific ranklearning.
The utility of click preference pairs de-pends on the following factors:1) Data distribution: if click pairs have goodquality, we should use dedicated click pairs in-stead of generic click pairs, so that the samplesfor training have similar distribution to the task oftask-specific ranking.2) The amount of dedicated training data: themore dedicated training data, the more reliable thetask-specific rank function is; thus, the less roomfor learning improvement using click data.
For thecase in the experiment that dedicated training is in-sufficient, the non-dedicated training data can alsohelp the learning as non-dedicated training datashare relevance pattern similarity with the dedi-cated data distribution.3) The quality of click pairs: if we can extractlarge amount of high-quality click pairs, the learn-ing improvement will be significant.
For example,as shown in Figure 3, at the early stage with fewerclick pairs (5,000 and 10,000 pairs) being com-bined with training data, the learning improvementis best.
With more click pairs are used, the noiserate in the click pairs becomes higher so that thelearning misleading factor is more important thaninformation complementary factor.
Thus, it is im-portant to improve the reliability of the click pairs.4) The utility of click pairs: by our study, thequality of click pairs extracted by SSL approachis comparable to those extracted by heuristic ruleapproach.
The possible reason that heuristic-rule-based click pairs can bring more benefit is thatthese pairs provide more complementary infor-mation compared with SSL approach.
As themethodologies of these two click data extractionapproaches are totally different, in future we willexplore the concrete reason that causes such utilitydifference.5 ConclusionsBy empirically exploring the related factors inutilizing click-through data to improve dedicatedmodel learning for task-specific ranking, we havebetter understood the principles of using clickpreference pairs appropriately, which is impor-tant for the real-world applications in commer-cial search engines as using click data can sig-nificantly save human labeling costs and makesrank function learning more efficient.
In the casethat dedicated training data is limited, while non-dedicated training data is helpful, using dedicatedskip-next pairs is the most effective way to furtherimprove the learning.
Heuristic rule approach pro-vides more useful click pairs compared with se-quential supervised learning approach.
The qual-ity of click pairs is critical for the efficacy of theapproach.
Therefore, an interesting topic is howto further reduce the inconsistency between skip-above pairs and human labeling so that such datamay also be useful for task-specific ranking.1094ReferencesE.
Agichtein, E. Brill, and S. Dumais.
2006.
Improv-ing web search ranking by incorporating user behav-ior information.
Proc.
of ACM SIGIR Conference.S.
M. Beitzel, E. C. Jensen, A. Chowdhury, andO.
Frieder.
2007.
Varying approaches to topicalweb query classification.
Proceedings of ACM SI-GIR conference.C.
Burges, T. Shaked, E. Renshaw, A. Lazier,M.
Deeds, N. Hamilton, and G. Hullender.
2005.Learning to rank using gradient descent.
Proc.
ofIntl.
Conf.
on Machine Learning.Z.
Cao, T. Qin, T. Liu, M. Tsai, and H. Li.
2007.Learning to rank: From pairwise approach to list-wise.
Proceedings of ICML conference.B.
Carterette, P. N. Bennett, D. M. Chickering, and S. T.Dumais.
2008.
Here or there: preference judgmentsfor relevance.
Proc.
of ECIR.O.
Chapelle and Y. Zhang.
2009.
A dynamic bayesiannetwork click model for web search ranking.
Pro-ceedings of the 18th International World Wide WebConference.K.
Chen, Y. Zhang, Z. Zheng, H. Zha, and G. Sun.2008.
Adapting ranking functions to user prefer-ence.
ICDE Workshops, pages 580?587.M.
Coyle and B. Smyth.
2007.
Supporting intelligentweb search.
ACM Transaction Internet Tech., 7(4).T.
G. Dietterich.
2002.
Machine learning for sequen-tial data: a review.
Lecture Notes in Computer Sci-ence, (2396):15?30.S.
Fox, K. Karnawat, M. Mydland, S. Dumias, andT.
White.
2005.
Evaluating implicit measures toimprove web search.
ACM Trans.
on InformationSystems, 23(2):147?168.Y.
Freund, R. D. Iyer, R. E. Schapire, and Y. Singer.1998.
An efficient boosting algorithm for combin-ing preferences.
Proceedings of International Con-ference on Machine Learning.J.
Friedman.
2001.
Greedy function approximation: agradient boosting machine.
Ann.
Statist., 29:1189?1232.X.
Geng, T. Liu, T. Qin, A. Arnold, H. Li, and H. Shum.2008.
Query dependent ranking with k nearestneighbor.
Proceedings of ACM SIGIR Conference.N.
S. Glance.
2001.
Community search assistant.
In-telligent User Interfaces, pages 91?96.S.
Ji, K. Zhou, C. Liao, Z. Zheng, G. Xue, O. Chapelle,G.
Sun, and H. Zha.
2009.
Global ranking by ex-ploiting user clicks.
In SIGIR?09, Boston, USA, July19-23.T.
Joachims, L. Granka, B. Pan, and G Gay.
2005.Accurately interpreting clickthough data as implicitfeedback.
Proc.
of ACM SIGIR Conference.T.
Joachims.
2002.
Optimizing search engines usingclickthrough data.
In Proceedings of the ACM Con-ference on Knowledge Discovery and Data Mining(KDD).I.
Kang and G. Kim.
2003.
Query type classificationfor web document retrieval.
Proceedings of ACMSIGIR Conference.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In ICML, pages282?289.U.
Lee, Z. Liu, and J. Cho.
2005.
Automatic identifi-cation of user goals in web search.
Proceedings ofInternational Conference on World Wide Web.X.
Li, Y.Y Wang, and A. Acero.
2008.
Learning queryintent from regularized click graphs.
Proceedings ofACM SIGIR Conference.T.
Y Liu.
2008.
Learning to rank for information re-trieval.
SIGIR tutorial.T.
Qin, T. Liu, X. Zhang, D. Wang, and H. Li.
2008.Global ranking using continuous conditional ran-dom fields.
In NIPS.H.
Li R. Jin, H. Valizadegan.
2008.
Ranking re-finement and its application to information retrieval.Proceedings of International Conference on WorldWide Web.F.
Radlinski and T. Joachims.
2007.
Active explorationfor learning rankings from clickthrough data.
Proc.of ACM SIGKDD Conference.F.
Radlinski, M. Kurup, and T. Joachims.
2008.
Howdoes clickthrough data reflect retrieval quality?
Pro-ceedings of ACM CIKM Conference.D.
E. Rose and D. Levinson.
2004.
Understanding usergoals in web search.
Proceedings of InternationalConference on World Wide Web.X.
Wang and C. Zhai.
2007.
Learn from web searchlogs to organize search results.
In Proceedings ofthe 30th ACM SIGIR.H.
Zha, Z. Zheng, H. Fu, and G. Sun.
2006.
Incor-porating query difference for learning retrieval func-tions in world wide web search.
Proceedings of the15th ACM Conference on Information and Knowl-edge Management.Z.
Zheng, H. Zhang, T. Zhang, O. Chapelle, K. Chen,and G. Sun.
2007.
A general boosting method andits application to learning ranking functions for websearch.
NIPS.1095
