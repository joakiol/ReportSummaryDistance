Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 221?224,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsModeling Spoken Decision Making Dialogueand Optimization of its Dialogue StrategyTeruhisa Misu, Komei Sugiura, Kiyonori Ohtake,Chiori Hori, Hideki Kashioka, Hisashi Kawai and Satoshi NakamuraMASTAR Project, NICTKyoto, Japan.teruhisa.misu@nict.go.jpAbstractThis paper presents a spoken dialogue frame-work that helps users in making decisions.Users often do not have a definite goal or cri-teria for selecting from a list of alternatives.Thus the system has to bridge this knowledgegap and also provide the users with an appro-priate alternative together with the reason forthis recommendation through dialogue.
Wepresent a dialogue state model for such deci-sion making dialogue.
To evaluate this model,we implement a trial sightseeing guidance sys-tem and collect dialogue data.
Then, we opti-mize the dialogue strategy based on the statemodel through reinforcement learning with anatural policy gradient approach using a usersimulator trained on the collected dialoguecorpus.1 IntroductionIn many situations where spoken dialogue interfacesare used, information access by the user is not a goal initself, but a means for decision making (Polifroni andWalker, 2008).
For example, in a restaurant retrievalsystem, the user?s goal may not be the extraction ofprice information but to make a decision on candidaterestaurants based on the retrieved information.This work focuses on how to assist a user who isusing the system for his/her decision making, whenhe/she does not have enough knowledge about the tar-get domain.
In such a situation, users are often un-aware of not only what kind of information the sys-tem can provide but also their own preference or fac-tors that they should emphasize.
The system, too, haslittle knowledge about the user, or where his/her inter-ests lie.
Thus, the system has to bridge such gaps bysensing (potential) preferences of the user and recom-mend information that the user would be interested in,considering a trade-off with the length of the dialogue.We propose a model of dialogue state that consid-ers the user?s preferences as well as his/her knowledgeabout the domain changing through a decision makingdialogue.
A user simulator is trained on data collectedwith a trial sightseeing system.
Next, we optimizethe dialogue strategy of the system via reinforcementlearning (RL) with a natural policy gradient approach.2 Spoken decision making dialogueWe assume a situation where a user selects from a givenset of alternatives.
This is highly likely in real worldsituations; for example, the situation wherein a user se-lects one restaurant from a list of candidates presentedChoose the optimal spot1.
CherryBlossoms2.
JapaneseGarden3.
EasyAccessKinkakuji-TempleRyoanji-TempleNanzenji-Temple??????GoalCriteriaAlternatives(choices)????
?p1 p2 p3v11 v12 v13?
?Figure 1: Hierarchy structure for sightseeing guidancedialogueby a car navigation system.
In this work, we deal witha sightseeing planning task where the user determinesthe sightseeing spot to visit, with little prior knowledgeabout the target domain.
The study of (Ohtake et al,2009), which investigated human-human dialogue insuch a task, reported that such consulting usually con-sists of a sequence of information requests from theuser, presentation and elaboration of information aboutcertain spots by the guide followed by the user?s evalu-ation.
We thus focus on these interactions.Several studies have featured decision support sys-tems in the operations research field, and the typicalmethod that has been employed is the Analytic Hierar-chy Process (Saaty, 1980) (AHP).
In AHP, the problemis modeled as a hierarchy that consists of the decisiongoal, the alternatives for achieving it, and the criteriafor evaluating these alternatives.
An example hierarchyusing these criteria is shown in Figure 1.For the user, the problem of making an optimal de-cision can be solved by fixing a weight vector Puser=(p1, p2, .
.
.
, pM) for criteria and local weight matrixVuser= (v11, v12, .
.
.
, v1M, .
.
.
, vNM) for alterna-tives in terms of the criteria.
The optimal alternativeis then identified by selecting the spot k with the maxi-mum priority of?Mm=1pmvkm.
In typical AHP meth-ods, the procedure of fixing these weights is often con-ducted through pairwise comparisons for all the possi-ble combinations of criteria and spots in terms of thecriteria, followed by weight tuning based on the re-sults of these comparisons (Saaty, 1980).
However, thismethodology cannot be directly applied to spoken dia-logue systems.
The information about the spot in termsof the criteria is not known to the users, but is obtainedonly via navigating through the system?s information.In addition, spoken dialogue systems usually handleseveral candidates and criteria, making pairwise com-parison a costly affair.We thus consider a spoken dialogue framework thatestimates the weights for the user?s preference (po-tential preferences) as well as the user?s knowledge221about the domain through interactions of informationretrieval and navigation.3 Decision support system with spokendialogue interfaceThe dialogue system we built has two functions: an-swering users?
information requests and recommend-ing information to them.
When the system is requestedto explain about the spots or their determinants, it ex-plains the sightseeing spots in terms of the requesteddeterminant.
After satisfying the user?s request, thesystem then provides information that would be helpfulin making a decision (e.g., instructing what the systemcan explain, recommending detailed information of thecurrent topic that the user might be interested in, etc.
).Note that the latter is optimized via RL (see Section 4).3.1 Knowledge baseOur back-end DB consists of 15 sightseeing spots as al-ternatives and 10 determinants described for each spot.We select determinants that frequently appear in the di-alogue corpus of (Ohtake et al, 2009) (e.g.
cherry blos-soms, fall foliage).
The spots are annotated in terms ofthese determinants if they apply to them.
The value ofthe evaluation enmis ?1?
when the spot n applies to thedeterminant m and ?0?
when it does not.3.2 System initiative recommendationThe content of the recommendation is determinedbased on one of the following six methods:1.
Recommendation of determinants based on thecurrently focused spot (Method 1)This method is structured on the basis of the user?scurrent focus on a particular spot.
Specifically, thesystem selects several determinants related to thecurrent spot whose evaluation is ?1?
and presentsthem to the user.2.
Recommendation of spots based on the cur-rently focused determinant (Method 2)This method functions on the basis of the focus ona certain specific determinant.3.
Open prompt (Method 3)The system does not make a recommendation, andpresents an open prompt.4.
Listing of determinants 1 (Method 4)This method lists several determinants to the user inascending order from the low level user knowledgeKsys(that the system estimates).
(Ksys, Psys, pmand Pr(pm= 1) are defined and explained in Sec-tion 4.2.)5.
Listing of determinants 2 (Method 5)This method also lists the determinants, but the or-der is based on the user?s high preference Psys(thatthe system estimates).6.
Recommendation of user?s possibly preferredspot (Method 6)The system recommends a spot as well as the de-terminants that the users would be interested inbased on the estimated preference Psys.
The sys-tem selects one spot k with a maximum value of?Mm=1Pr(pm= 1) ?
ek,m.
This idea is basedon collaborative filtering which is often used forrecommender systems (Breese et al, 1998).
Thismethod will be helpful to users if the system suc-cessfully estimates the user?s preference; however,it will be irrelevant if the system does not.We will represent these recommendationsthrough a dialogue act expression, (casys{scsys}),which consists of a communicative act casysand the semantic content scsys.
(For exam-ple Method1{(Spot5), (Det3,Det4,Det5)},Method3{NULL,NULL}, etc.
)4 Optimization of dialogue strategy4.1 Models for simulating a userWe introduce a user model that consists of a tuple ofknowledge vector Kuser, preference vector Puser, andlocal weight matrix Vuser.
In this paper, for simplic-ity, a user?s preference vector or weight for determi-nants Puser= (p1, p2, .
.
.
, pM) is assumed to con-sist of binary parameters.
That is, if the user is in-terested in (or potentially interested in) the determi-nant m and emphasizes it when making a decision,the preference pmis set to ?1?.
Otherwise, it is setto ?0?.
In order to represent a state that the user haspotential preference, we introduce a knowledge param-eter Kuser= (k1, k2, .
.
.
, kM) that shows if the userhas the perception that the system is able to handle orhe/she is interested in the determinants.
kmis set to?1?
if the user knows (or is listed by system?s recom-mendations) that the system can handle determinant mand ?0?
when he/she does not.
For example, the statethat the determinant m is the potential preference of auser (but he/she is unaware of that) is represented by(km= 0, pm= 1).
This idea is in contrast to previousresearch which assumes some fixed goal observable bythe user from the beginning of the dialogue (Schatz-mann et al, 2007).
A user?s local weight vnmfor spotn in terms of determinant m is set to ?1?, when thesystem lets the user know that the evaluation of spots is?1?
through recommendation Methods 1, 2 and 6.We constructed a user simulator that is based onthe statistics calculated through an experiment with thetrial system (Misu et al, 2010) as well as the knowl-edge and preference of the user.
That is, the user?s com-municative act catuserand the semantic content sctuserfor the system?s recommendation atsysare generatedbased on the following equation:Pr(catuser, sctuser|catsys, sctsys,Kuser,Puser)= Pr(catuser|catsys)?Pr(sctuser|Kuser,Puser, catuser, catsys, sctsys)This means that the user?s communicative act causeris sampled based on the conditional probability ofPr(catuser|catsys) in (Misu et al, 2010).
The seman-tic content scuseris selected based on the user?s pref-erence Puserunder current knowledge about the de-terminants Kuser.
That is, the sc is sampled from thedeterminants within the user?s knowledge (km= 1)based on the probability that the user requests the de-terminant of his/her preference/non-preference, whichis also calculated from the dialogue data of the trial sys-tem.4.2 Dialogue state expressionWe defined the state expression of the user in the pre-vious section.
However the problem is that for thesystem, the state (Puser,Kuser,Vuser) is not observ-able, but is only estimated from the interactions withthe user.
Thus, this model is a partially observableMarkov decision process (POMDP) problem.
In or-der to estimate unobservable properties of a POMDP222 Priors of the estimated state:- Knowledge: Ksys= (0.22, 0.01, 0.02, 0.18, .
.
.
)- Preference: Psys= (0.37, 0.19, 0.48, 0.38, .
.
.
)Interactions (observation):- System recommendation:asys= Method1{(Spot5), (Det1, Det3, Det4)}- User query:auser= Accept{(Spot5), (Det3)}Posterior of the estimated state:- Knowledge: Ksys= (1.00, 0.01, 1.00, 1.00, .
.
.
)- Preference: Psys= (0.26, 0.19, 0.65, 0.22, .
.
.
)User?s knowledge acquisition:- Knowledge: Kuser?
{k1= 1, k3= 1, k4= 1}- Local weight: Vuser?
{v51= 1, v53= 1, v54=1} Figure 2: Example of state updateand handle the problem as an MDP, we introducethe system?s inferential user knowledge vector Ksysor probability distribution (estimate value) Ksys=(Pr(k1= 1), P r(k2= 1), .
.
.
, P r(kM= 1)) andthat of preference Psys= (Pr(p1= 1), P r(p2=1), .
.
.
, P r(pM= 1)).The dialogue state DSt+1 or estimated user?s dia-logue state of the step t+1 is assumed to be dependentonly on the previous state DSt, as well as the interac-tions It = (atsys, atuser).The estimated user?s state is represented as a prob-ability distribution and is updated by each interac-tion.
This corresponds to representing the user typesas a probability distribution, whereas the work of (Ko-matani et al, 2005) classifies users to several discreteuser types.
The estimated user?s preference Psysis up-dated when the system observes the interaction It.
Theupdate is conducted based on the following Bayes?
the-orem using the previous state DSt as a prior.Pr(pm= 1|It) =Pr(It|pm=1)Pr(pm=1)Pr(It|pm=1)Pr(pm=1)+Pr(It|(pm=0))Pr(1?Pr(pm=1))Here, Pr(It|pm= 1), P r(It|(pm= 0) to the rightside was obtained from the dialogue corpus of (Misu etal., 2010).
This posterior is then used as a prior in thenext state update using interaction It+1.
An exampleof this update is illustrated in Figure 2.4.3 Reward functionThe reward function that we use is based on the num-ber of agreed attributes between the user preferenceand the decided spot.
Users are assumed to determinethe spot based on their preference Puserunder theirknowledge Kuser(and local weight for spots Vuser)at that time, and select the spot k with the maximumpriority of?mkk?
pk?
vkm.
The reward R is thencalculated based on the improvement in the number ofagreed attributes between the user?s actual (potential)preferences and the decided spot k over the expectedagreement by random spot selection.R =M?m=1pm?
ek,m?1NN?n=1M?m=1pm?
en,mFor example, if the decided spot satisfies three prefer-ences and the average agreement of the agreement byrandom selection is 1.3, then the reward is 1.7.4.4 Optimization by reinforcement learningThe problem of system recommendation generation isoptimized through RL.
The MDP (S, A, R) is definedas follows.
The state parameter S = (s1, s2, .
.
.
, sI) isgenerated by extracting the features of the current dia-logue state DSt.
We use the following 29 features 1.1.
Parameters that indicate the # of interactions fromthe beginning of the dialogue.
This is approximated byfive parameters using triangular functions.
2.
User?sprevious communicative act (1 if at?1user= xi, other-wise 0).
3.
System?s previous communicative act (1 ifat?1sys= yj, otherwise 0).
4.
Sum of the estimated userknowledge about determinants (?Nn=1Pr(kn= 1)).5.
Number of presented spot information.
6.
Expecta-tion of the probability that the user emphasizes the de-terminant in the current state (Pr(kn= 1)?
Pr(pn=1)) (10 parameters).
The action set A consists of thesix recommendation methods shown in subsection 3.2.Reward R is given by the reward function of subsection4.3.A system action asys(casys) is sampled based on thefollowing soft-max (Boltzmann) policy.?
(asys= k|S) = Pr(asys= k|S,?)=exp(?Ii=1si?
?ki)?Jj=1exp(?Ii=1si?
?ji)Here, ?
= (?11, ?12, .
.
.
?1I, .
.
.
, ?JI) consists of J (#actions) ?
I (# features) parameters.
The parameter?jiworks as a weight for the i-th feature of the ac-tion j and determines the likelihood that the action jis selected.
This ?
is the target of optimization by RL.We adopt the Natural Actor Critic (NAC) (Peters andSchaal, 2008), which adopts a natural policy gradientmethod as the policy optimization method.4.5 Experiment by dialogue simulationFor each simulated dialogue session, a simulated user(Puser,Kuser,Vuser) is sampled.
A preference vectorPuserof the user is generated so that he/she has fourpreferences.
As a result, four parameters in Puserare?1?
and the others are ?0?.
This vector is fixed through-out the dialogue episode.
This sampling is conductedbased on the rate proportional to the percentage of userswho emphasize it for making decisions (Misu et al,2010).
The user?s knowledge Kuseris also set basedon the statistics of the ?percentage of users who statedthe determinants before system recommendation?.
Foreach determinant, we sample a random valuable r thatranges from ?0?
to ?1?, and kmis set to ?1?
if r issmaller than the percentage.
All the parameters oflocal weights Vuserare initialized to ?0?, assumingthat users have no prior knowledge about the candi-date spots.
As for system parameters, the estimateduser?s preference Psysand knowledge Ksysare ini-tialized based on the statistics of our trial system (Misuet al, 2010).We assumed that the system does not misunderstandthe user?s action.
Users are assumed to continue a di-alogue session for 20 turns2, and episodes are sampledusing the policy ?
at that time and the user simulator1Note that about half of them are continuous variables andthat the value function cannot be denoted by a lookup table.2In practice, users may make a decision at any point oncethey are satisfied collecting information.
And this is the rea-son why we list the rewards in the early dialogue stage in223Table 1: Comparison of reward with baseline methodsReward (?std)Policy T = 5 T = 10 T = 15 T = 20NAC 0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)B1 0.02 (0.42) 0.13 (0.54) 0.29 (0.59) 0.34 (0.59)B2 0.46 (0.67) 0.68 (0.65) 0.80 (0.61) 0.92 (0.56)Table 2: Comparison of reward with discrete dialoguestate expressionReward (?std)State T = 5 T = 10 T = 15 T = 20PDs 0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)Discrete 0.89 (0.60) 0.97 (0.56) 1.03 (0.54) 1.10 (0.52)Table 3: Effect of estimated preference and knowledgeReward (?std)Policy T = 5 T = 10 T = 15 T = 20Pref+Know0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)Pref only 0.94 (0.57) 0.96 (0.55) 1.02 (0.55) 1.09 (0.53)Know only 0.96 (0.59) 1.00 (0.56) 1.08 (0.53) 1.15 (0.51)No Pref orKnow0.93 (0.57) 0.96 (0.55) 1.02 (0.53) 1.08 (0.52)of subsection 4.1.
In each turn, the system is rewardedusing the reward function of subsection 4.3.
The pol-icy (parameter ?)
is updated using NAC in every 2,000dialogues.4.6 Experimental resultThe policy was fixed at about 30,000 dialogueepisodes.
We analyzed the learned dialogue policy byexamining the value of weight parameter ?.
We com-pared the parameters of the trained policy between ac-tions3.
The weight of the parameters that represent theearly stage of the dialogue was large in Methods 4 and5.
On the other hand, the weight of the parameters thatrepresent the latter stage of the dialogue was large inMethods 2 and 6.
This suggests that in the trained pol-icy, the system first bridges the knowledge gap betweenthe user, estimates the user?s preference, and then, rec-ommends specific information that would be useful tothe user.Next, we compared the trained policy with the fol-lowing baseline methods.1.
No recommendation (B1)The system only provides the requested informa-tion and does not generate any recommendations.2.
Random recommendation (B2)The system randomly chooses a recommendationfrom six methods.The comparison of the average reward between thebaseline methods is listed in Table 1.
Note that the ora-cle average reward that can be obtained only when theuser knows all knowledge about the knowledge base(it requires at least 50 turns) was 1.45.
The reward bythe strategy optimized by NAC was significantly betterthan that of baseline methods (n = 500, p < .01).We then compared the proposed method with thecase where estimated user?s knowledge and preferenceare represented as discrete binary parameters instead ofprobability distributions (PDs).
That is, the estimateduser?s preference pmof determinant m is set to ?1?when the user requested the determinant, otherwise itis ?0?.
The estimated user?s knowledge kmis set tothe following subsections.
In our trial system, the dialoguelength was 16.3 turns with a standard deviation of 7.0 turns.3The parameters can be interpreted as the size of the con-tribution for selecting the action.?1?
when the system lets the user know the determi-nant, otherwise it is ?0?.
Another dialogue strategy wastrained using this dialogue state expression.
This resultis shown in Table 2.
The proposed method that rep-resents the dialogue state as a probability distributionoutperformed (p < .01 (T=15,20)) the method using adiscrete state expression.We also compared the proposed method with thecase where either one of estimated preference orknowledge was used as a feature for dialogue state inorder to carefully investigate the effect of these factors.In the proposed method, expectation of the probabil-ity that the user emphasizes the determinant (Pr(kn=1) ?
Pr(pn= 1)) was used as a feature of dialoguestate.
We evaluated the performance of the cases wherethe estimated knowledge Pr(kn= 1) or estimatedpreference Pr(pn= 1) was used instead of the expec-tation of the probability that the user emphasizes thedeterminant.
We also compared with the case whereno preference/knowledge feature was used.
This resultis shown in Table 3.
We confirmed that significant im-provement (p < .01 (T=15,20)) was obtained by takinginto account the estimated knowledge of the user.5 ConclusionIn this paper, we presented a spoken dialogue frame-work that helps users select an alternative from a list ofalternatives.
We proposed a model of dialogue state forspoken decision making dialogue that considers knowl-edge as well as preference of the user and the system,and its dialogue strategy was trained by RL.
We con-firmed that the learned policy achieved a better recom-mendation strategy over several baseline methods.Although we dealt with a simple recommendationstrategy with a fixed number of recommendation com-ponents, there are many possible extensions to thismodel.
The system is expected to handle a more com-plex planning of natural language generation.
We alsoneed to consider errors in speech recognition and un-derstanding when simulating dialogue.ReferencesJ.
Breese, D. Heckerman, and C. Kadie.
1998.
?empiricalanalysis of predictive algorithms for collaborative filter-ing?.
In ?Proc.
the 14th Annual Conference on Uncer-tainty in Artificial Intelligence?, pages 43?52.K.
Komatani, S. Ueno, T. Kawahara, and H. Okuno.
2005.User Modeling in Spoken Dialogue Systems to GenerateFlexible Guidance.
User Modeling and User-Adapted In-teraction, 15(1):169?183.T.
Misu, K. Ohtake, C. Hori, H. Kashioka, H. Kawai, andS.
Nakamura.
2010.
Construction and Experiment of aSpoken Consulting Dialogue System.
In Proc.
IWSDS.K.
Ohtake, T. Misu, C. Hori, H. Kashioka, and S. Nakamura.2009.
Annotating Dialogue Acts to Construct DialogueSystems for Consulting.
In Proc.
The 7th Workshop onAsian Language Resources, pages 32?39.J.
Peters and S. Schaal.
2008.
Natural Actor-Critic.
Neuro-computing, 71(7-9):1180?1190.J.
Polifroni and M. Walker.
2008.
Intensional Summariesas Cooperative Responses in Dialogue: Automation andEvaluation.
In Proc.
ACL/HLT, pages 479?487.T.
Saaty.
1980.
The Analytic Hierarchy Process: Planning,Priority Setting, Resource Allocation.
Mcgraw-Hill.J.
Schatzmann, B. Thomson, K. Weilhammer, H. Ye, andS.
Young.
2007.
Agenda-based User Simulation forBootstrapping a POMDP Dialogue System.
In Proc.HLT/NAACL.224
