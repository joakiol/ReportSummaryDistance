Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1587?1597,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsAbstractive Multi-Document Summarization via Phrase Selection andMerging?Lidong Bing?Piji Li\Yi Liao\Wai Lam\Weiwei Guo?Rebecca J.
Passonneau?
?Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA USA\Department of Systems Engineering and Engineering Management,The Chinese University of Hong Kong?Yahoo Labs, Sunnyvale, CA, USA?Center for Computational Learning Systems, Columbia University, New York, NY, USA?lbing@cs.cmu.edu,\{pjli, yliao, wlam}@se.cuhk.edu.hk?wguo@yahoo-inc.com,?becky@ccls.columbia.eduAbstractWe propose an abstraction-based multi-document summarization framework thatcan construct new sentences by exploringmore fine-grained syntactic units than sen-tences, namely, noun/verb phrases.
Dif-ferent from existing abstraction-based ap-proaches, our method first constructs apool of concepts and facts represented byphrases from the input documents.
Thennew sentences are generated by selectingand merging informative phrases to max-imize the salience of phrases and mean-while satisfy the sentence constructionconstraints.
We employ integer linear op-timization for conducting phrase selectionand merging simultaneously in order toachieve the global optimal solution for asummary.
Experimental results on thebenchmark data set TAC 2011 show thatour framework outperforms the state-of-the-art models under automated pyramidevaluation metric, and achieves reasonablywell results on manual linguistic qualityevaluation.1 IntroductionExisting multi-document summarization (MDS)methods fall in three categories: extraction-based,compression-based and abstraction-based.
Most?The work described in this paper is substan-tially supported by grants from the Research and De-velopment Grant of Huawei Technologies Co. Ltd(YB2013090068/TH138232) and the Research Grant Coun-cil of the Hong Kong Special Administrative Region, China(Project Codes: 413510 and 14203414).The work was done when Weiwei Guo was in Columbia Uni-versitysummarization systems adopt the extraction-based approach which selects some original sen-tences from the source documents to create a shortsummary (Erkan and Radev, 2004; Wan et al,2007).
However, the restriction that the whole sen-tence should be selected potentially yields someoverlapping information in the summary.
To thisend, some researchers apply compression on theselected sentences by deleting words or phrases(Knight and Marcu, 2000; Lin, 2003; Zajic etal., 2006; Harabagiu and Lacatusu, 2010; Liet al, 2015), which is the compression-basedmethod.
Yet, these compressive summarizationmodels cannot merge facts from different sourcesentences, because all the words in a summarysentence are solely from one source sentence.In fact, previous investigations show thathuman-written summaries are more abstractive,which can be regarded as a result of sentence ag-gregation and fusion (Cheung and Penn, 2013;Jing and McKeown, 2000).
Some works, albeitless popular, have studied abstraction-based ap-proach that can construct a sentence whose frag-ments come from different source sentences.
Oneimportant work developed by Barzilay and McK-eown (2005) employed sentence fusion, followedby (Filippova and Strube, 2008; Filippova, 2010).These works first conduct clustering on sentencesto compute the salience of topical themes.
Then,sentence fusion is applied within each cluster ofrelated sentences to generate a new sentence con-taining common information units of the sen-tences.
The abstractive-based approaches gatherinformation across sentence boundary, and hencehave the potential to cover more content in a moreconcise manner.In this paper, we propose an abstractive MDSframework that can construct new sentences by1587SVPDTROOTNP JJNNAnarmedmanVPVPVBDwalkedPPINVPNPintoDTNNPNNanAmishschool, ,VBDsent DTNNSNPADVPtheboysRB outsideandCCVPVBNPRTtiedRP upCC andVPNshotDTNNSthegirlsNPVP, ,S VPVBGkillingNP CD threeIN ofNP PRPthemPPNP.
.Figure 1: The constituency tree of a sentence from a news document.exploring more fine-grained syntactic units thansentences, namely, noun/verb phrases (NPs/VPs).This idea is based on two observations.
First, themajor constituent phrases loosely correspond tothe concepts and facts.
After reading a set of doc-uments describing the same topic or event, a per-son digests these documents as key concepts andfacts in his/her mind, such as ?an armed man?and ?walked into an Amish school?
from Figure1.
Second, a summary writer re-organizes the keyconcepts and facts to form new sentences for thesummary.
Accordingly, our proposed frameworkhas two major components corresponding to theabove observations.
The first component creates apool of concepts and facts represented by NPs andVPs from the input documents.
A salience scoreis computed for each phrase by exploiting redun-dancy of the document content in a global man-ner.
The second component constructs new sen-tences by selecting and merging phrases based ontheir salience scores, and ensures the validity ofnew sentences using a integer linear optimizationmodel.The contribution of this paper is two folds.
(1)We extract NPs/VPs from constituency trees torepresent key concepts/facts, and merge them toconstruct new sentences, which allows more sum-mary content units (SCUs) (Nenkova and Passon-neau, 2004) to be included in a sentence by break-ing the original sentence boundaries.
(2) The de-signed optimization framework for addressing theproblem is unique and effective.
Our optimiza-tion algorithm simultaneously selects and mergesa set of phrases that maximize the number of cov-ered SCUs in a summary.
Meanwhile, since thebasic unit is phrases, we design compatibility re-lations among NPs and VPs, as well as other op-timization constraints, to ensure that the gener-ated sentences contain correct facts.
Comparedwith the sentence fusion approaches that computesalience scores of sentence clusters, our proposedframework explores a more fine-grained textualunit (i.e., phrases), and maximizes the salience ofselected phrases in a global manner.2 Description of Our FrameworkWe first introduce how to extract NPs and VPsfrom constituency trees, and subsequently calcu-late salience scores for them.
Then we formulatethe sentence generation task as an optimizationproblem, and design constraints.
In the end, weperform several post-processing steps to improvethe order and the readability of the generated sen-tences.2.1 Phrase Salience CalculationThe first component decomposes the sentences indocuments into a set of noun phrases (NPs) de-rived from the subject parts of a constituency treeand a set of verb-object phrases (VPs), represent-ing potential key concepts and key facts, respec-tively.
These phrases will serve as the basic ele-ments for sentence generation.We employ Stanford parser (Klein and Man-ning, 2003) to obtain a constituency tree for eachinput sentence.
After that, we extract NPs and VPsfrom the tree as follows: (1) The NPs and VPs thatare the direct children of the sentence node (repre-1588sented by the S node) are extracted.
(2) VPs (NPs)in a path on which all the nodes are VPs (NPs)are also recursively extracted and regarded as hav-ing the same parent node S. Recursive operationin the second step will only be carried out in twolevels since the phrases in the lower levels maynot be able to convey a complete fact.
Take thetree in Figure 1 as an example, the correspondingsentence is decomposed into phrases ?An armedman?, ?walked into an Amish school, sent the boysoutside and tied up and shot the girls, killing threeof them?, ?walked into an Amish school?, ?sentthe boys outside?, and ?tied up and shot the girls,killing three of them?.1Because of the recursiveoperation, the extracted phrases may have over-laps.
Later, we will show how to avoid such over-lapping in phrase selection.A salience score is calculated for each phrase toindicate its importance.
Different types of saliencecan be incorporated in our framework, such asposition-based method (Yih et al, 2007), statis-tical feature based method (Woodsend and Lap-ata, 2012), concept-based method (Li et al, 2011),etc.
One key characteristic of our approach isthat the considered basic units are phrases insteadof sentences.
Such finer granularity leaves moreroom for better global salience score by poten-tially covering more distinct facts.
In our imple-mentation, we adopt a concept-based weight in-corporating the position information.
The con-cept set is designated to be the union set of un-igrams, bigrams, and named entities in the docu-ments.
We remove stopwords and perform lemma-tization before extracting unigrams and bigrams.The position-based term frequency is used in theconcept weighting scheme.
When counting thefrequency, each occurrence of a concept in an in-put document is weighted with the paragraph po-sition.
The weight larger than 1 is given to theconcept occurrences in the first few paragraphs.Specifically, the weight of the first paragraph isB and the weight decreases as the position of theparagraph increases from the beginning of the doc-1We only consider the recursive operation for a VP withmore than one parallel sub-VPs, such as the highest VP inFigure 1.
The sub-VPs following modal, link or auxiliaryverbs are not extracted as individual VPs.
In addition, wealso extract the clauses functioning as subjects of sentencesas NPs, such as ?that clause?.
Note that we also mention suchclauses as ?noun phrase?
although their syntactic labels couldbe ?SBAR?
or ?S?.ument.
The weighting function is:H(p) ={?p?B if p < ?
(logB/ log ?
)1 otherwise,(1)where p is the position of the paragraph startingfrom 0, from beginning of the document, and ?
isa positive constant and smaller than 1.
Then, thesalience of a phrase is calculated as the summedweights of its concepts.2.2 New Sentence Construction ModelThe construction of new sentences is formulatedas an optimization problem which is able to si-multaneously generate a group of sentences.
Eachnew sentence is composed of one NP and at leastone VP, where the NP and VPs may come fromdifferent source sentences.
In the process of newsentence generation, the compatibility relation be-tween NP and VP and a variety of summarizationrequirements are jointly considered.2.2.1 Compatibility RelationCompatibility relation is designed to indicatewhether an NP and a VP can be used to form anew sentence.
For example, the NP ?Police?
fromanother sentence should not be the subject of theVP ?sent the boys outside?
extracted from Figure1.
We use some heuristics to find compatibility,and then expand the compatibility relation to morephrases by extracting coreference.To find coreference NPs (different mentions forthe same entity), we first conduct coreference res-olution for each document with Stanford corefer-ence resolution package (Lee et al, 2013).
Weadopt those resolution rules that are able to achievehigh quality and address our need for summariza-tion.
In particular, Sieve 1, 2, 3, 4, 5, 9, and 10in the package are used.
A set of clusters are ob-tained and each cluster contains the mentions thatrefer to the same entity in a document.
The clus-ters from different documents in the same topicare merged by matching the named entities.
Aftermerging, the mentions that are not NPs extractedin the phrase extraction step are removed in eachcluster.
Two NPs in the same cluster are deter-mined as alternative of each other.To find alternative VPs, Jaccard Index is em-ployed as the similarity measure.
Specifically,each VP is represented as a set of its concepts andthe index value is calculated for each pair of VPs.If the value is larger than a threshold, the two VPsare determined as alternative of each other.1589We then define an indicator matrix ?|N||V|, inwhich ?
[i, j] = 1 if an NP Niand a VP Vjcomefrom the same node S in the constituency tree, oth-erwise, ?
[i, j] = 0.
Let?Niand?Virepresent the al-ternative phrases of Niand Vias described above.The compatibility matrix?
?|N||V|is defined as fol-lows:??
[p, q] =??????????
?1 if Np??Ni?
?
[i, q] = 11 if Vq??Vj?
?
[p, j] = 11 if ?
[p, q] = 10 otherwise(2)where??
[p, q] = 1 means Npand Vqare compat-ible/permitted for constructing a new sentence.?
?is the final compatibility matrix that we use in theoptimization.
The first case of Equation 2 impliesthat if Npand Niare coreferent, Npcan replaceNiand serve as the subject of Ni?s VP (i.e., Vq).The second case implies that if Vqis very similarto Vj, Vqcan be concatenated to Vj?s NP (i.e.,Np).2.2.2 Phrase-based Content OptimizationThe overall objective function of our optimizationformulation to select NPs and VPs is defined as:max{?i?iSNi?
?i<j?ij(SNi+ SNj)RNij+?i?iSVi?
?i<j?ij(SVi+ SVj)RVij},(3)where ?iand ?iare selection indicators for theNP Niand the VP Vi, respectively.
SNiand SViare the salience scores of Niand Vi.
?ijand ?ijare co-occurrence indicators of pairs (Ni, Nj) and(Vi, Vj).
RNijand RVijare the similarity of pairs(Ni, Nj) and (Vi, Vj).
IfNiandNjare coreferent,RNij= 1.
Otherwise, the similarity is calculatedwith the above Jaccard Index based method.
Thenotations are summarized in Table 1.Specifically, we maximize the salience score ofthe selected NPs and VPs as indicated by the firstand the third terms in Equation 3, and penalize theselection of similar NP pairs and similar VP pairsas indicated by the second and the fourth terms.Meanwhile, the phrase selection is governed by aset of constraints so that the selected phrases cangenerate valid sentences.
The constraints will beexplained in details in Section 2.2.3.One characteristic of our objective function isthat NPs and VPs are treated differently, i.e., thereNotation DescriptionNi, ViNoun phrase i and verb phrase i?i, ?iSelection indicators of Niand Vi?ij, ?ijCo-occurrence indicators of pairs (Ni, Nj) and(Vi, Vj)SNi, SViSalience scores of Niand ViRNij, RVijSimilarity of pair (Ni, Nj) and pair (Vi, Vj)?|N||V|?
[i, j] = 1 if Niand Vjare from the same sen-tence?Ni,?ViThe alternative phrases of Niand Vi??|N||V|??
[i, j] = 1 means Niand Vjare compatible forbeing used to construct a new sentence?
?ijSentence generation indicator for Niand Vjif??
[i, j] = 1Table 1: Notations.are different selection/penalty terms for NP andVP.
Such design enables us to avoid the falsepenalty between an NP and a VP.
For example, thealgorithm produces two sentences: the first sen-tence is ?the gunman shot ...?
with an NP ?thegunman?, and the other sentence has a VP ?con-firmed the gunman died?.
Obviously, we shouldnot penalize the redundancy between them, be-cause mentioning the gunman is necessary in bothsentences.2.2.3 Sentence Generation ConstraintsTo summarize the related sentences in the docu-ments, human writers usually merge the importantfacts in different VPs about the same entity into asingle sentence, and omit the trivial facts.
Also,the same entity is likely to be described by coref-erent NPs.
Therefore, in our approach, only oneNP is selected and employed as the subject of thenewly generated sentence, which is then concate-nated with the merged facts (i.e., VPs).
If the com-patibility entry??
[i, j] for Niand Vjis 1, we de-fine a sentence generation indicator ?
?ijto indicatewhether both Niand Vjare selected to construct anew sentence in the summary.We design the following groups of constraintsto realize our aim of phrase selection and newsentence construction.
The objective function andconstraints are linear, therefore the problem canbe solved by existing Integer Linear Programming(ILP) solvers such as simplex algorithm (Dantzigand Thapa, 1997).NP validity.
To maintain the consistency be-tween the selection indicator ?
and the compati-bility entry??
for NP Ni, we introduce two con-straints as follows:?i, j, ?i?
?
?ij; ?i,?j??ij?
?i.
(4)1590These two constraints work together to ensure thevalid assignment of ?
according to the compatibil-ity entry?
?.VP legality.
Similarly, the following require-ment guarantees the consistency between the se-lection indicator ?
and the compatibility entry?
?for selected VP Vi:?j,?i?
?ij= ?j.
(5)The above two constraints jointly ensure that theselected NPs and VPs are able to form new sum-mary sentences according to the values of sentencegeneration indicators.Not i-within-i.
Two phrases in the samepath of a constituency tree cannot be chosen at thesame time:if ?VkVj, then ?k+ ?j?
1,if ?NkNj, then ?k+ ?j?
1.
(6)For example, ?walked into an Amish school, sentthe boys outside and tied up and shot the girls,killing three of them?
and ?walked into an Amishschool?
cannot be both incorporated in the sum-mary, because of the obvious redundancy.Phrase co-occurrence.
These constraintscontrol the co-occurrence relation of NPs or VPs.For NPs, we introduce three constraints:?ij?
?i?
0, (7)?ij?
?j?
0, (8)?i+ ?j?
?ij?
1.
(9)Constraints 7 to 9 ensure a valid solution of NPselection.
The first two constraints state that if theunits Niand Njco-occur in the summary (i.e.,?ij= 1), then we have to include them individ-ually (i.e., ?i= 1 and ?j= 1).
The third con-straint is the inverse of the first two.
Similarly, theconstraints for VPs are as follows:?ij?
?i?
0, (10)?ij?
?j?
0, (11)?i+ ?j?
?ij?
1.
(12)Sentence number.
In abstractive summariza-tion, we do not prefer to generate many short sen-tences.
This is controlled by:?i?i?
K, (13)where K is the maximum number of sentences.Short sentence avoidance.
We do notselect the VPs from very short sentences because ashort sentence normally cannot convey a completekey fact (Woodsend and Lapata, 2012).if l(S) < M,Vi?
S, then ?i= 0, (14)where M is the threshold of the sentence length.Pronoun avoidance.
We exclude the NPsthat are pronouns from being selected as the sub-ject of the new sentences.
As previously observed(Woodsend and Lapata, 2012), pronouns are nor-mally not used by human summary writers.
It isbecause the summary is short and the narrationrelation of sentences is relatively simple so thatpronouns are not needed.
Moreover, in automaticsummary, pronouns will cause ambiguity in thesummary, especially when the sentence order isautomatically determined.
Therefore, we modelthe constraint as:if Niis pronoun, then ?i= 0.
(15)Length constraint.
The overall length ofthe selected NPs and VPs is no larger than a limitL:?i{l(Ni) ?
?i}+?j{l(Vj) ?
?j} ?
L, (16)where l() is the word-based length of a phrase.2.3 PostprocessingRecall that we require that one NP and at leastone VP compose a sentence.
Thus, we form araw sentence with a selected NP as the subjectfollowed by the corresponding selected VPs thatare indicated by sentence generation indicator ?
?ijhaving the value 1.
The VPs in a summary sen-tence are ordered according to their natural orderif they come from the same document.
Otherwise,they are ordered according to the timestamps ofthe corresponding documents.
After that, if the to-tal length is smaller than L, we add conjunctionssuch as ?and?
and ?then?
to concatenate the VPsfor improving the readability of the newly gener-ated sentences.
The pseudo-timestamp of a sen-tence is defined as the earliest timestamp of itsVPs and the sentences are ordered based on theirpseudo-timestamps.2.4 Relation to Existing MDS ApproachesMany existing extraction-based and compression-based MDS approaches could be regarded as spe-cial cases under our framework: (1) To simulate1591extraction-based summarization, we just need toconstrain that the highest NP and the highest VPfrom the same sentence are selected simultane-ously.
In addition, no NPs and VPs in lower lev-els can be selected.
Thus, the output only con-tains the original sentences of the source docu-ments.
(2) To simulate compression-based sum-marization, we can adapt our framework to con-duct sentence selection and sentence compressionin a joint manner.
Specifically, we only need to re-strict that the NP and VPs of a summary sentencemust come from the same original sentence.3 Experiments3.1 Experimental SetupThe data set of traditional summarization task inText Analysis Conference (TAC) 2011 is used toevaluate the performance of our approach.
Thisdata set is the latest one and it contains 44 topics.Each topic falls into one of 5 predefined event cat-egories and contains 10 related news documents.There are four writers to write model summariesfor each topic.The data set of traditional summarization task inTAC 2010 is employed as the development/tuningdata set.
This data set contains 46 topics from thesame predefined categories.
Each topic also has10 documents and 4 model summaries.Based on the tuning set, the key parameters ofour model are set as follows.
The constants B and?
in the weighting function are set to 6 and 0.5repectively.
The similarity threshold in obtainingthe alternative VPs is 0.75.
We did not observe sig-nificant difference between cosine similarity andJaccard Index.We mainly evaluate the system by pyramid eval-uation.
To gain a comprehensive understanding,we also evaluate by ROUGE evaluation and man-ual linguistic quality evaluation.3.2 Results with Pyramid EvaluationThe pyramid evaluation metric (Nenkova and Pas-sonneau, 2004) involves semantic matching ofsummary content units (SCUs) so as to recognizealternate realizations of the same meaning.
Differ-ent weights are assigned to SCUs based on theirfrequency in model summaries.
A weighted inven-tory of SCUs named a pyramid is created, whichconstitutes a resource for investigating alternaterealizations of the same meaning.
Such propertymakes pyramid method more suitable to evalu-Auto-pyr Auto-pyr Rank inSystem (Th: .6) (Th: .65) TAC 2011Our 0.905 0.793 NA22 0.878 0.775 143 0.875 0.756 217 0.860 0.741 3Table 2: Comparison with the top 3 systems inTAC 2011.ate summaries.
Another widely used evaluationmetric is ROUGE (Lin and Hovy, 2003) and itevaluates summaries from word overlapping per-spective.
Because of the strict string matching, itignores the semantic content units and performsbetter when larger sets of model summaries areavailable.
In contrast to ROUGE, pyramid scor-ing is robust with as few as four model summaries(Nenkova and Passonneau, 2004).
Therefore, inrecent summarization evaluation workshops suchas TAC, the pyramid is used as the major metric.Since manual pyramid evaluation is time-consuming, and the exact evaluation scores arenot reproducible especially when the assessors forour results are different from those of TAC, weemploy the automated version of pyramid pro-posed in (Passonneau et al, 2013).
The automatedpyramid scoring procedure relies on distributionalsemantics to assign SCUs to a target summary.Specifically, all n-grams within sentence boundsare extracted, and converted into 100 dimensionlatent topical vectors via a weighted matrix fac-torization model (Guo and Diab, 2012).
Simi-larly, the contributors and the label of an SCUare transformed into 100 dimensional vector rep-resentations.
An SCU is assigned to a summaryif there exists an n-gram such that the similarityscore between the SCU low dimensional vectorand the n-gram low dimensional vector exceedsa threshold.
Passonneau et al (2013) showedthat the distributional similarity based method pro-duces automated scores that correlate well withmanual pyramid scores, yielding more accuratepyramid scores than string matching based auto-mated methods (Harnly et al, 2005).
In this pa-per, we adopt the same setting as in (Passonneauet al, 2013): a 100 dimension matrix factorizationmodel is learned on a domain independent corpus,which is drawn from sense definitions of WordNetand Wiktionary2, and Brown corpus.
We exper-2http://en.wiktionary.org/1592ROUGE-2 ROUGE-SU4System P R F1 P R F1Our 0.117 0.117 0.117 0.148 0.147 0.14822 0.112 0.114 0.113 0.147 0.150 0.14843 0.132 0.135 0.134 0.162 0.166 0.16417 0.128 0.131 0.129 0.157 0.160 0.159Table 3: Performance under ROUGE metric.iment with 2 threshold values, i.e., 0.6 and 0.65,similar to those used in (Passonneau et al, 2013).The top three systems in TAC 2011 evaluatedwith manual pyramid score were System 22 (Li etal., 2011), 43, and 17 (Ng et al, 2011).
Table 2shows the comparison with them under the auto-mated pyramid evaluation.
Our method achievesthe best results in both thresholds, which meansthat our method is able to find more semantic con-tent units (SCUs) than the state-of-the-art systemin TAC 2011.
In addition, paired t-test (with p <0.01) comparing our model with the best systemin TAC 2011, i.e., System 22, shows that the per-formance of our model is significantly better.
It isworth noting that the three systems used additionalexternal linguistic resources: System 22 used aWikipedia corpus for providing domain knowl-edge, System 17 and 43 defined some category-specific features.
Without any domain adaption,our framework can still achieve encouraging per-formance.We calculate Pearson?s correlation to measurehow well the automatic pyramid approximates themanual pyramid scores for 50 system submissionsin TAC 2011.
The values are 0.91 and 0.93 forthresholds 0.6 and 0.65 respectively.
It demon-strates that the automated pyramid is reliable todifferentiate the performance of different methods.3.3 Results with ROUGE EvaluationAs mentioned above, we favor the pyramid evalua-tion over the ROUGE score because it can measurethe summary quality beyond simply string match-ing.
Here, we also provide ROUGE score for ourreference.
ROUGE-1.5.5 package3is employedwith the same parameters as in TAC.
The resultsare summarized in Table 3.
Our performance isslightly better than System 22, and it is not as goodas System 43 and 17.
The reason is that System 43and 17 used category-specific features and trainedthe feature weights with the category information3http://www.berouge.com/Pages/default.aspxin TAC 2010 data.
These features help them se-lect better category-specific content for the sum-mary.
However, the usability of such features de-pends on the availability of predefined categoriesin the summarization task, as well as the avail-ability of training data with the same predefinedcategories for estimating feature weights.
There-fore, the adaptability of these methods is limited tosome extent.
In contrast, our framework does notdefine any category-specific feature and only usesTAC 2010 data to tune the parameters for generalsummarization purpose.3.4 Linguistic Quality EvaluationThe linguistic quality of summaries is evaluatedusing the five linguistic quality questions on gram-maticality (Q1), non-redundancy (Q2), referentialclarity (Q3), focus (Q4), and coherence (Q5) inDocument Understanding Conferences (DUC).
ALikert scale with five levels is employed with 5 be-ing very good with 1 being very poor.
A summarywas blindly evaluated by three assessors on eachquestion.
System 22 performed better than Sys-tem 43 and 17 in TAC 2011 on the evaluation ofreadability, which is an aggregation of the abovequestions.
Considering the intensive labor force ofmanual assessment, we only conduct comparisonwith System 22.The results are given in Table 4.
On average,the two systems perform very closely.
System 22is an extraction-based method that picks the orig-inal sentences, hence it achieves higher score inQ1 grammaticality, while our approach has somenew sentences with grammar mistakes, which is acommon problem for abstractive methods and de-serves more future research effort.
For Q4 focus,our score is higher than System 22, which revealsthat our summary sentences are relatively more co-hesive.
The score of Q3 referential clarity showsthat the referential relation is basically clear in oursummaries, even when new sentences are automat-ically generated.
In general, ignoring the gram-maticality scores, our system still performs betterthan System 22.
Specifically, the average scoresof our system and System 22 on the last four ques-tions are 3.37 and 3.33 respectively.4 Qualitative Results4.1 Analysis of Summary Sentence TypeThere are three types of sentences in the sum-maries generated by our framework, namely, new1593System Q1 Q2 Q3 Q4 Q5 AVGOur 3.67 3.50 3.90 3.23 2.83 3.4322 4.13 3.50 3.97 2.97 2.87 3.49Table 4: Evaluation of linguistic quality.sentences, compressed sentences, and originalsentences.
A new sentence is constructed by merg-ing the phrases from different original sentences.A compressed sentence is generated by deletingphrases from an original sentence.
An originalsentence in the summary is directly extracted fromthe input documents.The percentage of different types of sentencesin our summaries is calculated.
About 33% of thesummary sentences are newly constructed.
Thisdemonstrates that our framework has good capa-bility of merging phrases from the original sen-tences so as to convey more information in com-pacted summaries.
In addition, about 44% of thesummary sentences are generated by compression.It shows a unique characteristic of our framework:sentence construction and sentence compressionare conducted in a unified model.4.2 Case StudyTable 5 shows the summary of the first topic,i.e., ?Amish Shooting?, by our framework.The summary sentence ID and the sentencetype are given in the form of ?
[summarysentence ID: sentence type]?.
Eachselected phrase and the original sentence IDwhere the phrase originated are given in theform of ?
{selected phrase (originalsentence ID)}?.
There are three compressedsentences with IDs 1, 2, and 4, one new sentencewith ID 3, and two original sentences with IDs 5and 6.The new sentence is constructed from the fol-lowing original sentences in which the extractedNPs and VPs are indicated with colored parenthe-ses:(84): On Monday morning, (NP Charles CarlRoberts IV) (VP (VP entered the West NickelMines Amish School in Lancaster County) and(VP shot 10 girls), (VP killing five)).
(85): (NP Roberts) (VP killed himself as policestormed the building).
(150): (NP Roberts) (VP left what they de-scribed as rambling notes for his family).
[1:C] {An armed man (25)} {walked intoan Amish school (25)} {tied up and shot thegirls, killing three of them.
(25)} [2:C]{A man who laid siege to a one-room Amishschoolhouse (64)} {told his wife shortly be-fore opening fire that he had molested two younggirls who were his relatives decades ago (64)}{was tormented by dreams of molesting again.
(64)} [3:N] {Charles Carl Roberts IV (84)}{killed himself as police stormed the building(85)} {left what they described as ramblingnotes for his family.
(150)} [4:C] {The gun-man (145)} {was not Amish (145)} {had notattended the school.
(145)} [5:O] {The shoot-ings (148)} {occurred about 10:45 a.m.(148)}[6:O] {Police (149)} {could offer no explana-tion for the killings.
(149)}Table 5: The summary of ?Amish Shooting?
topic.The NPs of these sentences are coreferent so thatsome of their VPs are merged and concatenatedwith one NP, i.e., ?Charles Carl Roberts IV?.The summary sentences with IDs 1, 2, and 4are compressions from the following original sen-tences respectively:(25): (NP An armed man) (VP(VP walked intoan Amish school), (VP sent the boys outside)and (VP tied up and shot the girls, killing threeof them)), (NP authorities) (VP said).
(64): (NP(NP A man)who laid siege to aone-room Amish schoolhouse),(VP killing fivegirls),(VP(VP told his wife shortly before open-ing fire that he had molested two young girls whowere his relatives decades ago)and(VP was tor-mented by ?dreams of molesting again?
)),(NPauthorities)(VP said Tue).
(145): According to media reports, (NP thegunman) (VP(VP was not Amish) and (VP hadnot attended the school)).Some uncritical information is excluded fromthe summary sentences, such as ?sent the boysoutside?, ?authorities said?, etc.
In addition, theVP ?killing five girls?
of the original sentencewith ID 64 is also excluded since it has significantredundancy with the summary sentence with ID 1.5 Related WorkExisting multi-document summarization (MDS)works can be classified into three categories:1594extraction-based approaches, compression-basedapproaches, and abstraction-based approaches.Extraction-based approaches are the most stud-ied of the three.
Early studies mainly followed agreedy strategy in sentence selection (C?elikyilmazand Hakkani-T?ur, 2011; Goldstein et al, 2000;Wan et al, 2007).
Each sentence in the docu-ments is firstly assigned a salience score.
Then,sentence selection is performed by greedily select-ing the sentence with the largest salience scoreamong the remaining ones.
The redundancy iscontrolled during the selection by penalizing theremaining ones according to their similarity withthe selected sentences.
An obvious drawback ofsuch greedy strategy is that it is easily trappedin local optima.
Later, unified models are pro-posed to conduct sentence selection and redun-dancy control simultaneously (McDonald, 2007;Filatova and Hatzivassiloglou, 2004; Yih et al,2007; Gillick et al, 2007; Lin and Bilmes, 2010;Lin and Bilmes, 2012; Sipos et al, 2012).
How-ever, extraction-based approaches are unable toevaluate the salience and control the redundancyon the granularity finer than sentences.
Thus, theselected sentences may still contain unimportantor redundant phrases.Compression-based approaches have been in-vestigated to alleviate the above limitation.
Asa natural extension of the extractive method, theearly works adopted a two-step approach (Lin,2003; Zajic et al, 2006; Gillick and Favre, 2009).The first step selects the sentences, and the secondstep removes the unimportant or redundant unitsfrom the sentences.
Recently, integrated modelshave been proposed that jointly conduct sentenceextraction and compression (Martins and Smith,2009; Woodsend and Lapata, 2010; Almeida andMartins, 2013; Berg-Kirkpatrick et al, 2011; Li etal., 2015).
Note that our model also jointly con-ducts phrase selection and phrase merging (newsentence generation).
Nonetheless, compressivemethods are unable to merge the related facts fromdifferent sentences.On the other hand, abstraction-based ap-proaches can generate new sentences based on thefacts from different source sentences.
In additionto the previously mentioned sentence fusion work,new directions have been explored.
Researchersdeveloped an information extraction based ap-proach that extracts information items (Genest andLapalme, 2011) or abstraction schemes (Genestand Lapalme, 2012) as components for generat-ing sentences.
Summary revision was also inves-tigated to improve the quality of automatic sum-mary by rewriting the noun phrases or people ref-erences in the summaries (Nenkova, 2008; Sid-dharthan et al, 2011).
Sentence generation withword graph was applied for summarizing customeropinions and chat conversations (Ganesan et al,2010; Mehdad et al, 2014).Recently, the factors of information certaintyand timeline in MDS task were explored (Ng etal., 2014; Wan and Zhang, 2014; Yan et al, 2011).Researchers also explored some variants of thetypical MDS setting, such as query-chain focusedsummarization that combines aspects of updatesummarization and query-focused summarization(Baumel et al, 2014), and hierarchical summa-rization that scales up MDS to summarize a largeset of documents (Christensen et al, 2014).
Adata-driven method for mining sentence structureson large news archive was proposed and utilizedto summarize unseen news events (Pighin et al,2014).
Moreover, some works (Liu et al, 2012;K?ageb?ack et al, 2014; Denil et al, 2014; Caoet al, 2015) utilized deep learning techniques totackle some summarization tasks.6 Conclusions and Future WorkWe propose an abstractive MDS framework thatconstructs new sentences by exploring more fine-grained syntactic units, namely, noun phrases andverb phrases.
The designed optimization frame-work operates on the summary level so that morecomplementary semantic content units can be in-corporated.
The phrase selection and merging isdone simultaneously to achieve global optimal.Meanwhile, the constructed sentences should sat-isfy the constraints related to summarization re-quirements such as NP/VP compatibility.
Exper-imental results on TAC 2011 summarization dataset show that our framework outperforms the topsystems in TAC 2011 under the pyramid metric.For future work, one aspect is to enhance thegrammar quality of the generated new sentencesand compressed sentences.
Another aspect is toimprove time efficiency of our framework, and itsmajor bottleneck is the time consuming ILP opti-mzation.1595ReferencesMiguel Almeida and Andre Martins.
2013.
Fastand robust compressive summarization with dual de-composition and multi-task learning.
In ACL, pages196?206.Regina Barzilay and Kathleen R. McKeown.
2005.Sentence fusion for multidocument news summa-rization.
Comput.
Linguist., 31(3):297?328.Tal Baumel, Raphael Cohen, and Michael Elhadad.2014.
Query-chain focused summarization.
In ACL,pages 913?922.Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InHLT, pages 481?490.Ziqiang Cao, Furu Wei, Li Dong, Sujian Li, and MingZhou.
2015.
Ranking with recursive neural net-works and its application to multi-document sum-marization.
In AAAI.Asli C?elikyilmaz and Dilek Hakkani-T?ur.
2011.Concept-based classification for multi-documentsummarization.
In ICASSP, pages 5540?5543.Jackie Chi Kit Cheung and Gerald Penn.
2013.
To-wards robust abstractive multi-document summa-rization: A caseframe analysis of centrality and do-main.
In ACL, pages 1233?1242.Janara Christensen, Stephen Soderland, Gagan Bansal,and Mausam.
2014.
Hierarchical summarization:Scaling up multi-document summarization.
In ACL,pages 902?912.George B. Dantzig and Mukund N. Thapa.
1997.
Lin-ear Programming 1: Introduction.
Springer-VerlagNew York, Inc.Misha Denil, Alban Demiraj, Nal Kalchbrenner, PhilBlunsom, and Nando de Freitas.
2014.
Modelling,visualising and summarising documents with a sin-gle convolutional neural network.
arXiv preprintarXiv:1406.3830.G?unes Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based lexical centrality as salience in textsummarization.
J. Artif.
Int.
Res., 22(1):457?479.Elena Filatova and Vasileios Hatzivassiloglou.
2004.A formal model for information selection in multi-sentence text extraction.
In COLING.Katja Filippova and Michael Strube.
2008.
Sen-tence fusion via dependency graph compression.
InEMNLP, pages 177?185.Katja Filippova.
2010.
Multi-sentence compression:Finding shortest paths in word graphs.
In COLING,pages 322?330.Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.2010.
Opinosis: A graph-based approach to abstrac-tive summarization of highly redundant opinions.
InCOLING, pages 340?348.Pierre-Etienne Genest and Guy Lapalme.
2011.Framework for abstractive summarization usingtext-to-text generation.
In MTTG, pages 64?73.Pierre-Etienne Genest and Guy Lapalme.
2012.
Fullyabstractive approach to guided summarization.
InACL, pages 354?358.Dan Gillick and Benoit Favre.
2009.
A scalable globalmodel for summarization.
In Workshop on ILP forNLP, pages 10?18.Dan Gillick, Benoit Favre, and Dilek Hakkani-t?ur.2007.
The icsi summarization system at tac 2008.In Proc.
of Text Understanding Conference.Jade Goldstein, Vibhu Mittal, Jaime Carbonell, andMark Kantrowitz.
2000.
Multi-document summa-rization by sentence extraction.
In NAACL-ANLP-AutoSum, pages 40?48.Weiwei Guo and Mona Diab.
2012.
Modeling sen-tences in the latent space.
In ACL, pages 864?872.Sanda Harabagiu and Finley Lacatusu.
2010.
Us-ing topic themes for multi-document summarization.ACM Trans.
Inf.
Syst., 28(3):13:1?13:47.Aaron Harnly, Ani Nenkova, Rebecca Passonneau, andOwen Rambow.
2005.
Automation of summaryevaluation by the pyramid method.
In RANLP.Hongyan Jing and Kathleen R. McKeown.
2000.
Cutand paste based text summarization.
In NAACL,pages 178?185.Mikael K?ageb?ack, Olof Mogren, Nina Tahmasebi, andDevdatt Dubhashi.
2014.
Extractive summariza-tion using continuous vector space models.
InCVSC@EACL, pages 31?39.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In ACL, pages 423?430.Kevin Knight and Daniel Marcu.
2000.
Statistics-based summarization - step one: Sentence compres-sion.
In AAAI-IAAI, pages 703?710.Heeyoung Lee, Angel Chang, Yves Peirsman,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2013.
Deterministic coreference resolu-tion based on entity-centric, precision-ranked rules.Comput.
Linguist., 39(4):885?916.Huiying Li, Yue Hu, Zeyuan Li, Xiaojun Wan, andJianguo Xiao.
2011.
Pkutm participation intac2011.
In Proceedings of TAC.Piji Li, Lidong Bing, Wai Lam, Hang Li, and Yi Liao.2015.
Reader-aware multi-document summariza-tion via sparse coding.
In IJCAI.Hui Lin and Jeff Bilmes.
2010.
Multi-document sum-marization via budgeted maximization of submodu-lar functions.
In HLT, pages 912?920.1596Hui Lin and Jeff Bilmes.
2012.
Learning mixturesof submodular shells with application to documentsummarization.
In UAI, pages 479?490.Chin-Yew Lin and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In NAACL, pages 71?78.Chin-Yew Lin.
2003.
Improving summarization per-formance by sentence compression: a pilot study.
InProceedings of the sixth international workshop onInformation retrieval with Asian languages-Volume11, pages 1?8.
Association for Computational Lin-guistics.Yan Liu, Sheng-hua Zhong, and Wenjie Li.
2012.Query-oriented multi-document summarization viaunsupervised deep learning.
In AAAI.Andr?e F. T. Martins and Noah A. Smith.
2009.
Sum-marization with a joint model for sentence extrac-tion and compression.
In Workshop on ILP for NLP,pages 1?9.Ryan McDonald.
2007.
A study of global inferencealgorithms in multi-document summarization.
InECIR, pages 557?564.Yashar Mehdad, Giuseppe Carenini, and Raymond T.Ng.
2014.
Abstractive summarization of spokenand written conversations based on phrasal queries.In ACL, pages 1220?1230.Ani Nenkova and Rebecca J. Passonneau.
2004.Evaluating content selection in summarization: Thepyramid method.
In HLT-NAACL, pages 145?152.Ani Nenkova.
2008.
Entity-driven rewrite for multi-document summarization.
In Third InternationalJoint Conference on Natural Language Processing,IJCNLP, pages 118?125.Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min yenKan, and Chew lim Tan.
2011.
Swing: Exploit-ing category-specific information for guided sum-marization.
In Proceedings of TAC.Jun-Ping Ng, Yan Chen, Min-Yen Kan, and ZhoujunLi.
2014.
Exploiting timelines to enhance multi-document summarization.
In ACL, pages 923?933.Rebecca J. Passonneau, Emily Chen, Weiwei Guo, andDolores Perin.
2013.
Automated pyramid scoringof summaries using distributional semantics.
In ACL(2), pages 143?147.Daniele Pighin, Marco Cornolti, Enrique Alfonseca,and Katja Filippova.
2014.
Modelling eventsthrough memory-based, open-ie patterns for abstrac-tive summarization.
In ACL, pages 892?901.Advaith Siddharthan, Ani Nenkova, and KathleenMcKeown.
2011.
Information status distinctionsand referring expressions: An empirical study of ref-erences to people in news summaries.
Comput.
Lin-guist., 37(4):811?842.Ruben Sipos, Pannaga Shivaswamy, and ThorstenJoachims.
2012.
Large-margin learning of submod-ular summarization models.
In EACL, pages 224?233.Xiaojun Wan and Jianmin Zhang.
2014.
Ctsum: Ex-tracting more certain summaries for news articles.In SIGIR, pages 787?796.Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.2007.
Manifold-ranking based topic-focused multi-document summarization.
In IJCAI, pages 2903?2908.Kristian Woodsend and Mirella Lapata.
2010.
Auto-matic generation of story highlights.
In ACL, pages565?574.Kristian Woodsend and Mirella Lapata.
2012.
Mul-tiple aspect summarization using integer linear pro-gramming.
In EMNLP-CoNLL, pages 233?243.Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,Xiaoming Li, and Yan Zhang.
2011.
Evolution-ary timeline summarization: A balanced optimiza-tion framework via iterative substitution.
In SIGIR,pages 745?754.Wen-tau Yih, Joshua Goodman, Lucy Vanderwende,and Hisami Suzuki.
2007.
Multi-document summa-rization by maximizing informative content-words.In IJCAI, pages 1776?1782.David M. Zajic, Bonnie J. Dorr, Jimmy Lin, andRichard Schwartz.
2006.
Sentence compressionas a component of a multi-document summarizationsystem.
In DUC at NLT/NAACL 2006.1597
