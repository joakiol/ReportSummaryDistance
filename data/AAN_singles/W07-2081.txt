Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 370?373,Prague, June 2007. c?2007 Association for Computational LinguisticsUCD-FC: Deducing semantic relations using WordNet senses that occurfrequently in a database of noun-noun compounds ?Fintan J. Costello,School of Computer Science and Informatics, University College Dublin,Belfield, Dublin 6, Ireland.fintan.costello@ucd.ieAbstractThis paper describes a system for classify-ing semantic relations among nominals, asin SemEval task 4.
This system uses acorpus of 2,500 compounds annotated withWordNet senses and covering 139 differentsemantic relations.
Given a set of nomi-nal pairs for training, as provided in the Se-mEval task 4 training data, this system con-structs for each training pair a set of featuresmade up of relations and WordNet sensepairs which occurred with those nominalsin the corpus.
A Naive Bayes learning al-gorithm learns associations between thesefeatures and relation membership categories.The identification of relations among nomi-nals in test items takes place on the basis ofthese associations.1 IntroductionThis paper describes a system for deducing thecorrect semantic relation between a pair of nom-inals in a sentence, as in SemEval task 4 (Girju,Hearst, Nakov, Nastase, Szpakowicz, Turney, &Yuret, 2007).
This system is an adaptation of anexisting system for deducing the correct semanticrelation between the pair of words in a noun-nouncompound.
This compound disambiguation system(named PRO, for Proportional Relation Occurrence;see Costello, Veale, & Dunne, 2006) makes use of?
This research was supported by the FP6 NEST Pro-gramme of the European Commission (ANALOGY: Humansthe Analogy-Making Species: STREP Contr.
No 029088)a corpus of 2,500 compounds annotated with Word-Net senses and covering 139 different semantic re-lations, with each noun and each relation annotatedwith its correct WordNet sense.1 Section 2 of the pa-per will describe the format and structure of this cor-pus, Section 3 will describe the original PRO com-pound disambiguation system, and Section 4 willexplain how the PRO system was adapted to deducethe correct semantic relation between a pair of nom-inals, as in SemEval task 4.
Four different versionsof the adapted system were produced (versions A,B,C and D), either using or not using the WordNet la-bels and the Query labels provided with training andtest items in SemEval task 4.
Section 5 discusses theperformance of these different versions of the sys-tem.
Finally, Section 6 finishes the paper with somediscussion and ideas for future work.2 A Corpus of Annotated CompoundsUsing WordNet (Miller, 1995), version 2.0, a cor-pus of noun-noun compounds was constructed suchthat each compound was annotated with the correctWordNet noun senses for constituent words, the cor-rect semantic relation between those words, and thecorrect WordNet verb sense for that relation, as de-scribed below.2.1 Corpus ProcedureThe compounds used in this corpus were selectedfrom the set of noun-noun compounds defined inWordNet.
Compounds from WordNet were usedbecause each compound had an associated gloss or1A file containing this corpus is available for download fromhttp://inismor.ucd.ie/?fintanc/wordnet compounds370definition explaining the relation between the wordsin that compound (compounds from other sourceswould not have such associated definitions).
Also,using compounds from WordNet guarantees that allconstituent words of those compounds would alsohave entries in WordNet.
An initial list of over40,000 two-word noun-noun compounds was ex-tracted from WordNet 2.0.
From this list a randomsubset was selected.
From that set al compoundsusing scientific latin (e.g.
ocimum basilicum), id-iomatic compounds (e.g.
zero hour), compoundscontaining proper nouns (e.g.
Yangtze river), non-english compounds (e.g.
faux pas), and chemicalterminology (e.g.
carbon dioxide) were excluded.The remaining compounds were placed in randomorder, and a research assistant annotated each withthe WordNet noun senses of the constituent words,the semantic relation between those words, and theWordNet verb sense of that relation.
A web pagewas created for this annotation task, showing the an-notator the compound to be annotated and the Word-Net gloss (meaning) for that compound.
This pagealso showed the annotator the list of WordNet sensesfor the modifier noun and head noun in the com-pound, allowing the annotator to select the correctsense for each word.
After word-sense selection an-other page was presented allowing the annotator toidentify the correct semantic relation for that com-pound and to select the correct WordNet sense forthe verb in that relation.2.2 Corpus ResultsWord sense, relation, and relation sense informationwas gathered for 2,500 compounds.
Relation occur-rence was well distributed across these compounds:there were 139 different relations used in the corpus.Note that in SemEval task 4, the number of relationcategories available was much smaller than the setof relation categories available in our corpus (just 7relation categories in the SemEval task).3 Compound Disambiguation AlgorithmThis section presents the ?Proportional Relation Oc-currence?
(PRO) algorithm which makes use of thecorpus results described above to deduce seman-tic relations for noun-noun compounds.
In Section4 this algorithm is adapted to deduce relations be-Preconditions:The entry for each compound C in corpus D contains:CmodList = sense + hypernym senses for modifier of C;CheadList = sense + hypernym senses for head of C;Crel = semantic relation of C;Input:X = compound for which a relation is required;modList = sense + hypernym senses for modifier of X;headList = sense + hypernym senses for head of X;finalRelationList = ();finalPairList = ();Begin:1 for each modifier sense M ?modList2 for each head sense H ?
headList3 relCount = ();4 matchCount = 0;5 P = (M,H);6 for each compound C ?
corpus D7 if ((M ?
CmodList) and (H ?
CheadList))8 relCount[Crel] = relCount[Crel] + 1;9 matchCount = matchCount + 1;10 for each relation R ?
relCount11 score = relCount[R]/matchCount;12 prevScore = finalRelationList[R];13 if (score > prevScore)14 finalRelationList[R] = score;15 if (score > pairScore)16 finalPairList[P ] = score;17 sort finalRelationList by score ;18 sort finalPairList by score ;19 return (finalRelationList, finalPairList);End.Figure 1: PRO disambiguation algorithm.tween nominals in SemEval task 4.The approach to compound disambiguation takenhere is similar to that taken by for example Kim &Baldwin (2005) and Girju, Moldovan, Tatu, & An-tohe (2005), and works by finding other compoundscontaining words from the same semantic categoriesas the words in the compound to be disambiguated:if a particular relation occurs frequently in thoseother compounds, that relation is probably also thecorrect relation for the compound in question.
Wetake WordNet senses to represent semantic cate-gories.
Once the correct WordNet sense for a wordhas been identified, that word can placed in a set ofnested semantic categories: the category representedby that sense, by the parent sense (or hypernym) ofthat sense, the parent of that parent, and so on up tothe (notional) root sense of WordNet.Figure 1 shows the algorithm in pseudocode.
Thealgorithm uses the corpus of annotated noun-noun371compounds and, to disambiguate a compound, takesas input the correct WordNet sense for the modifierand head words of that compound (if known) plusall hypernyms of those senses.
If modifier and headword senses are not known, the most frequent sensesfor those words are used, plus all hypernyms of thosesenses.
The algorithm pairs each modifier sense withevery head sense.
For each sense-pair, the algorithmgoes through the corpus of compounds and extractsevery compound whose modifier sense (or a hyper-nym of that sense) is equal to the modifier sense inthe current sense-pair, and whose head sense (or ahypernym of that sense) is equal to the head sense inthat pair.
The algorithm counts the number of timeseach relation occurs in that set of compounds, andassigns each relation a Proportional Relation Occur-rence (PRO) score for that pair, equal to the condi-tional probability of relation R given sense-pair S.If the PRO score for relation R in the currentsense-pair is greater than the score obtained for Rwith some other pair, the current score is recordedfor R. If the score for R for the current pair P isgreater than any previous score obtained for P , thatscore is recorded for P .
In this way the algorithmfinds the maximum score for each relation R acrossall sense-pairs, and the maximum score for each pairP across all relations.
The algorithm returns a list ofrelations and of sense-pairs for the compound, bothsorted by score.
The relations and sense-pairs withthe highest scores are those most likely to be correctfor that compound and to be most important for itsrelational meaning.In Costello, Veale and Dunne (2006), this algo-rithm was tested by applying it to the annotated cor-pus using a leave-one-out approach.
These testsshowed a reliable relationship between PRO scoreand accuracy of response.
At a PRO level of 1, thealgorithm return a response (selects a relation) forjust over 900 compounds, and approximately 850 ofthose responses are correct (the algorithm?s preci-sion at this level is 0.92).4 Adapting to the SemEval 4 taskTo apply the PRO algorithm to the training and testsentences in SemEval task 4 first required a mappingfrom the labels used to tag nominals in that task (la-bels e1 and e2) to the modifier and head categoriesused by the PRO algorithm.
To carry out this map-ping the nominal whose label appeared in the firstposition in a relation tag was taken to be the modi-fier for that relation, and that in the second positionwas taken to be the head; for example, with the rela-tion tag CONTAINER-CONTENT(E1,E2) the nomi-nal e1 would be taken to be the modifer and e2 tobe the head.
Given this mapping the PRO algorithmcould be applied to sentences from SemEval task 4,taking modifier and head nominals as input and pro-ducing as output lists of candidate relations and rel-evant sense pairs (sorted by PRO score).The relations produced by the PRO algorithm donot correspond to the 7 relations in SemEval task4.
To make predictions about the 7 SemEval rela-tions, the scored relation lists and sense-pair listsreturned by the PRO algorithm were used as fea-tures for a straightforward Naive Bayes learning al-gorithm, as implemented in the Perl module Algo-rithm::NaiveBayes.
For each sentence in a trainingset in SemEval task 4, the PRO algorithm was ap-plied to produce a list of relations and sense pairsdescribing that sentence.
Each relation and eachsense pair in this list has an associated PRO score,and Naive Bayes was trained on these features of allmembers of the training set, and then applied to testset sentences to produce predictions about each sen-tence?s membership or non-membership in the rela-tion in question.Version A of the system used neither the WordNetsense tags nor the Query labels provided with the 7relation categories used.
Instead of using WordNetsenses for the input words the system simply usedthe first (most frequent) noun senses for those words,and proceeded as described above.
Version B usedWordNet sense tags.
Versions C and D of the systemused either the first WordNet sense or the providedsense tags, coupled with the query terms used in theSemEval task.
An additional module in the systemwas intended to make use of these query terms inrelation classification by comparing the query termof the sentence to be classified with query terms inpositive or negative training examples of that rela-tion, and making a decision based on that compari-son.
Unfortunately, due to an error this query termmodule was not activated in the submitted runs, sothe results from versions C and D are the same asfrom A and B.372Table 1: F-Score results by relation and run.relation A4 B4 C4 D4Cause-Effect 72.1 65.1 72.1 65.1Instrument-Agency 69.8 58.1 69.8 58.1Product-Producer 73.1 73 73.1 73Origin-Entity 43.1 42.3 43.1 42.3Theme-Tool 50 49.2 50 49.2Part-Whole 71.7 75 71.7 75Content-Container 73.8 59.4 73.8 59.4Avg 64.8 60.3 64.8 60.35 SemEval 4 task resultsTable 1 shows the results returned for the PRO sys-tem for training run 4 (using all 140 training items ineach relation) for the four possible runs A, B, C andD.
Due to the error in activating the query term mod-ule, columns C4 and D4 are identical to columns A4and B4.
There are two notable aspects of the resultsin Table 1.
First, the system?s performance was bet-ter for run A4 (that did not useWordNet senses) thanfor B4 (using WordNet senses).
Indeed, the systemcame first out of 6 systems which took part in theA4 run.
This was surprising: it had been expectedthat using the correct WordNet senses for nominalswould improve the system?s performance.
Analy-sis revealed that A4 runs using most frequent Word-Net senses provided more matches with entries inthe compound corpus the B4 run using the correctWordNet senses.
This may explain why the systemgave a better performance for A4 than B4.The second interesting aspect of Table 1 is thevariation of the system?s responses across the dif-ferent relation categories.
For the two relations?Origin-Entity?
and ?Theme-Tool?
the system has anF-score of 50 or less, while for the other five rela-tions the system?s F-score is around 70.
It is not asyet clear why the system performed so poorly forthese relations: further investigation is needed to ex-plain this curious pattern.6 ConclusionsThis paper has described a system for automaticallyseslecting relations between nominals which usesthe PRO algorithm and compound corpus to formfeatures for pairs of nominals (consisting of can-didate relations and sense-pairs co-occurring withthose relations), and uses a Naive Bayes algorithmto learn to identify relations between nominals fromthose features.
The system performs best using themost frequent WordNet senses for those nominals,suggesting that the system may work usefully in de-ducing semantic relations between nominals with-out the need to deduce word senses.
However, thesystem?s performance does not seem particularlyimpressive or suitable for application to real-worldtasks as yet.
The system?s best performance repre-sents an accuracy of 66% across relations: in otherwords, the system gets 1 in three relations wrong inthe SemEval task.There is one very obvious area for improvement inthe system described here.
Currently the system usesa simple Naive Bayes algorithm for learning associ-ations between features and relation categories.
Amore sophisticated approach (using Support VectorMachines, for example) would be likely to improvethe systsem?s performance noticably.
The conver-sion of the system to use some form of SVM shouldnot be difficult.
A more difficult problem, however,is to address the system?s poor performance on somerelations.
This is currently difficult to understand,and represents a serious flaw in the system.
Resolv-ing this problem may reveal some useful aspects ofthe structure of different sorts of semantic relationsbetween nominals.ReferencesF.
J. Costello, T. Veale and S. Dunne.
2006.
Using WordNetto Automatically Deduce Relations between Words in Noun-Noun Compounds.
In Proceedings of the COLING/ACL2006 Main Conference, pp 160?167.R.
Girju, M. Hearst, P. Nakov, V. Nastase, S. Szpakowicz,P.
Turney and D. Yuret.
2007.
Classification of SemanticRelations between Nominals: Dataset for Task 4 in SemEval2007.
4th International Workshop on Semantic Evaluations,June 23-24, Prague, Czech Republic.R.
Girju, D. Moldovan, M. Tatu, and D. Antohe.
2005.
Onthe semantics of noun compounds.
Computer Speech andLanguage 19, 4, 479?496.S.
N. Kim and T. Baldwin.
Automatic Interpretation of NounCompounds using WordNet::Similarity.
2nd InternationlJoint Conference on Natual Language Processing, Korea,2005.G.
Miller.
1995.
WordNet: A lexical database.
Communica-tion of the ACM, 38(11), 39?41.373
