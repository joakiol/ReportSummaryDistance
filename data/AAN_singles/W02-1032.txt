Exploiting Headword Dependency and Predictive Clustering forLanguage ModelingJianfeng GaoMicrosoft Research, AsiaBeijing, 100080, Chinajfgao@microsoft.comHisami SuzukiMicrosoft ResearchRedmond WA 98052, USAhisamis@microsoft.comYang Wen*Department of Computer &Information Sciences ofTsinghua University, China* This work was done while the author was visiting Microsoft Research Asia.AbstractThis paper presents several practical waysof incorporating linguistic structure intolanguage models.
A headword detector isfirst applied to detect the headword of eachphrase in a sentence.
A permuted headwordtrigram model (PHTM) is then generatedfrom the annotated corpus.
Finally, PHTMis extended to a cluster PHTM (C-PHTM)by defining clusters for similar words in thecorpus.
We evaluated the proposed modelson the realistic application of JapaneseKana-Kanji conversion.
Experiments showthat C-PHTM achieves 15% error ratereduction over the word trigram model.
Thisdemonstrates that the use of simple methodssuch as the headword trigram and predictiveclustering can effectively capture longdistance word dependency, andsubstantially outperform a word trigrammodel.1 IntroductionIn spite of its deficiencies, trigram-based languagemodeling still dominates the statistical languagemodeling community, and is widely applied to taskssuch as speech recognition and Asian language textinput (Jelinek, 1990; Gao et al, 2002).Word trigram models are deficient because theycan only capture local dependency relations, takingno advantage of richer linguistic structure.
Manyproposals have been made that try to incorporatelinguistic structure into language models (LMs), butlittle improvement has been achieved so far inrealistic applications because (1) capturing longerdistance word dependency leads to higher-ordern-gram models, where the number of parameters isusually too large to estimate; (2) capturing deeperlinguistic relations in a LM requires a large amountof annotated training corpus and a decoder thatassigns linguistic structure, which are not alwaysavailable.This paper presents several practical ways ofincorporating long distance word dependency andlinguistic structure into LMs.
A headword detectoris first applied to detect the headwords in eachphrase in a sentence.
A permuted headword trigrammodel (PHTM) is then generated from theannotated corpus.
Finally, PHTM is extended to acluster model (C-PHTM), which clusters similarwords in the corpus.Our models are motivated by three assumptionsabout language: (1) Headwords depend on previousheadwords, as well as immediately precedingwords; (2) The order of headwords in a sentence canfreely change in some cases; and (3) Word clustershelp us make a more accurate estimate of theprobability of word strings.
We evaluated theproposed models on the realistic application ofJapanese Kana-Kanji conversion, which convertsphonetic Kana strings into proper Japaneseorthography.
Results show that C-PHTM achieves a15% error rate reduction over the word trigrammodel.
This demonstrates that the use of simplemethods can effectively capture long distance worddependency, and substantially outperform the wordtrigram model.
Although the techniques in thispaper are described in the context of JapaneseKana-Kanji conversion, we believe that they can beextended to other languages and applications.This paper is organized as follows.
Sections 2and 3 describe the techniques of using headwordAssociation for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.
248-256.Proceedings of the Conference on Empirical Methods in Naturaldependency and clustering for language modeling.Section 4 reviews related work.
Section 5introduces the evaluation methodology, and Section6 presents the results of our main experiments.Section 7 concludes our discussion.2 Using Headwords2.1 MotivationJapanese linguists have traditionally distinguishedtwo types of words1, content words (jiritsugo) andfunction words (fuzokugo), along with the notion ofthe bunsetsu (phrase).
Each bunsetsu typicallyconsists of one content word, called a headword inthis paper, and several function words.
Figure 1shows a Japanese example sentence and its Englishtranslation2.[??+?][??+??][??+??][??+?][??+?][??+?
][chiryou+ni]   [sennen+shite]       [zenkai+made][treatment+to][concentration+do][full-recovery+until][juubun+na]      [ryouyou+ni]  [tsutome+ru][enough+ADN] [rest+for]        [try+PRES]'(One) concentrates on the treatment and tries to restenough until full recovery'Figure 1.
A Japanese example sentence withbunsetsu and headword tagsIn Figure 1, we find that some headwords in thesentence are expected to have a strongerdependency relation with their precedingheadwords than with their immediately precedingfunction words.
For example, the three headwords??~??~??
(chiryou 'treatment' ~ sennen'concentrate' ~ zenkai 'full recovery') form a trigramwith very strong semantic dependency.
Therefore,we can hypothesize (in the trigram context) thatheadwords may be conditioned not only by the twoimmediately preceding words, but also by twoprevious headwords.
This is our first assumption.We also note that the order of headwords in asentence is flexible in some sense.
From the1 Or more correctly, morphemes.
Strictly speaking, theLMs discussed in this paper are morpheme-based modelsrather than word-based, but we will not make thisdistinction in this paper.2 Square brackets demarcate the bunsetsu boundary, and+ the morpheme boundary; the underlined words are theheadwords.
ADN indicates an adnominal marker, andPRES indicates a present tense marker.example in Figure 1, we find that if ??~??~??
(chiryou 'treatment' ~ sennen 'concentrate' ~ zenkai'full recovery') is a meaningful trigram, then itspermutations (such as ??~??~??
(zenkai 'fullrecovery' ~ chiryou 'treatment' ~ sennen'concentrate')) should also be meaningful, becauseheadword trigrams tend to capture an order-neutralsemantic dependency.
This reflects a characteristicof Japanese, in which arguments and modifiers of apredicate can freely change their word order, aphenomenon known as "scrambling" in linguisticliterature.
We can then introduce our secondassumption: headwords in a trigram are permutable.Note that the permutation of headwords should beuseful more generally beyond Japanese: forexample, in English, the book Mary bought andMary bought a book can be captured by the sameheadword trigram (Mary ~ bought ~ book) if weallow such permutations.In this subsection, we have stated twoassumptions about the structure of Japanese that canbe exploited for language modeling.
We now turn todiscuss how to incorporate these assumptions inlanguage modeling.2.2 Permuted headword trigram model(PHTM)A trigram model predicts the next word wi byestimating the conditional probability P(wi|wi-2wi-1),assuming that the next word depends only on twopreceding words, wi-2 and wi-1.
The PHTM is asimple extension of the trigram model thatincorporates the dependencies between headwords.If we assume that each word token can uniquely beclassified as a headword or a function word, thePHTM can be considered as a cluster-basedlanguage model with two clusters, headword H andfunction word F. We can then define the conditionalprobability of wi based on its history as the productof the two factors: the probability of the category (Hor F), and the probability of wi given its category.Let hi or fi be the actual headword or function wordin a sentence, and let Hi or Fi be the category of theword wi.
The PHTM can then be formulated as:=?
?
))...(|( 11 ii wwwP   (1)))...(|())...(|( 1111 iiiii HwwwPwwHP ??
???))...(|())...
(|( 1111 iiiii FwwwPwwFP ??
??
?+where ?
is a function that maps the word history(w1?wi-1) onto equivalence classes.P(Hi|?
(w1?wi-1)) and P(Fi|?
(w1?wi-1)) arecategory probabilities, and P(wi|?
(w1?wi-1)Fi) isthe word probability given that the category of wi isfunction word.
For these three probabilities, weused the standard trigram estimate (i.e., ?
(w1?wi-1)= (wi-2wi-1)).
The estimation of headwordprobability is slightly more elaborate, reflecting thetwo assumptions described in Section 2.1:)|(())...(|( 122111 iiiiiii HhhwPHwwwP ???
=?
??
(2)))|()1( 212 iiii HhhwP ??
?+ ?
)|()1( 121 iiii HwwwP ??
?+ ?
.This estimate is an interpolated probability of threeprobabilities: P(wi|hi-2hi-1Hi) and P(wi|hi-1hi-2Hi),which are the headword trigram probability with orwithout permutation, and P(wi|wi-2wi-1Hi), which isthe probability of wi given that it is a headword,where hi-1 and hi-2 denote the two precedingheadwords, and ?1, ?2 ?
[0,1] are the interpolationweights optimized on held-out data.The use of ?1 in Equation (2) is motivated by thefirst assumption described in Section 2.1:headwords are conditioned not only on twoimmediately preceding words, but also on twoprevious headwords.
In practice, we estimated theheadword probability by interpolating theconditional probability based on two previousheadwords P(wi|hi-2hi-1Hi) (and P(wi|hi-1hi-2Hi) withpermutation), and the conditional probability basedon two preceding words P(wi|wi-2wi-1Hi).
If ?1 isaround zero, it indicates that this assumption doesnot hold in real data.
Note that we did not estimatethe conditional probability P(wi|wi-2wi-1hi-2hi-1Hi)directly, because this is in the form of a 5-gram,where the number of parameters are too large toestimate.The use of ?2 in Equation (2) comes from thesecond assumption in Section 2.1: headwordtrigrams are permutable.
This assumption can beformulated as a co-occurrence model for headwordprediction: that is, the probability of a headword isdetermined by the occurrence of other headwordswithin a window.
However, in our experiments, weinstead used an interpolated probability?2?P(wi|hi-2hi-1Hi) + (1?
?2)?P(wi|hi-1hi-2Hi) for tworeasons.
First, co-occurrence models do not predictwords from left to right, and are thus very difficultto interpolate with trigram models for decoding.Second, if we see n-gram models as one extremethat predicts the next word based on a strictlyordered word sequence, co-occurrence models go tothe other extreme of predicting the next word basedon a bag of previous words without taking wordorder into account at all.
We prefer models that liesomewhere between the two extremes, and considerword order in a more flexible way.
In PHTM ofEquation (2), ?2 represents the impact of word orderon headword prediction.
When ?2 = 1 (i.e., theresulting model is a non-permuted headwordtrigram model, referred to as HTM), it indicates thatthe second assumption does not hold in real data.When ?2 is around 0.5, it indicates that a headwordbag model is sufficient.2.3 Model parameter estimationAssume that all conditional probabilities inEquation (1) are estimated using maximumlikelihood estimation (MLE).
Then)|( 12 ??
iii wwwP =)|()|( 1212 iiiiiii HwwwPwwHP ????
, wi: headword?????????
)|()|( 1212 iiiiiii FwwwPwwFP ????
, wi: function wordis a strict equality when each word token is uniquelyclassified as a headword or a function word.
Thiscan be trivially proven as follows.
Let Ci representthe category of wi (Hi or Fi in our case).
We have)|()|( 1212 iiiiiii CwwwPwwCP ????
?)()()()(1212122iiiiiiiiiiiiiCwwPwCwwPwwPCwwP????????
?=)()(1212???
?=iiiiiiwwPwCwwP  (3)Since each word is uniquely assigned to a category,P(Ci|wi)=1, and thus it follows that)|()()( 121212 iiiiiiiiiii wwwCPwwwPwCwwP ??????
?=)|()( 12 iiiii wCPwwwP ?= ??
)( 12 iii wwwP ?
?= .
(4)Substituting Equation (4) into Equation (3), we get)|()|( 1212 iiiiiii CwwwPwwCP ????
?)|()()(121212??????
== iiiiiiii wwwPwwPwwwP .
(5)Now, by separating the estimates of probabilities ofheadwords and function words, Equation (1) can berewritten as:P(wi|?
(w1?wi-1))= (6))|()(|(( 122121 ????
iiiiii hhwPwwHP ??
))|()1( 212 ??
?+ iii hhwP?
)|()1( 121 ??
?+ iii wwwP?wi: headword)|( 12 ??
iii wwwP   ????????
?wi: function wordThere are three probabilities to be estimated inEquation (6): word trigram probabilityP(wi|wi-2wi-1), headword trigram probabilityP(wi|hi-2hi-1) and P(wi|hi-1hi-2) (where wi is aheadword), and category probability P(Hi|wi-2wi-1).In order to deal with the data sparseness problemof MLE, we used a backoff scheme (Katz, 1987) forthe parameter estimation.
The backoff schemerecursively estimates the probability of an unseenn-gram by utilizing (n?1)-gram estimates.
To keepthe model size manageable, we also removed alln-grams with frequency less than 2.In order to classify a word uniquely as H or F,we needed a mapping table where each word in thelexicon corresponds to a category.
The table wasgenerated in the following manner.
We firstassumed that the mapping from part-of-speech(POS) to word category is fixed.
The tag set weused included 1,187 POS tags, of which 102 countas headwords in our experiments.
We then used aPOS-tagger to generate a POS-tagged corpus, fromwhich we generated the mapping table3.
If a wordcould be mapped to both H and F, we chose themore frequent category in the corpus.
Using thismapping table, we achieved a 98.5% accuracy ofheadword detection on the test data we used.Through our experiments, we found thatP(Hi|wi-2wi-1) is a poor estimator of categoryprobability; in fact, the unigram estimate P(Hi)achieved better results in our experiments as shownin Section 6.1.
Therefore, we also used the unigramestimate for word category probability in our3 Since the POS-tagger does not identify phrases, ourimplementation does not identify precisely oneheadword for a phrase, but identify multiple headwordsin the case of compounds.experiments.
The alternative model that uses theunigram estimate is given below:P(wi|?
(w1?wi-1))= (7))|()((( 1221 ??
iiii hhwPHP ??
))|()1( 212 ??
?+ iii hhwP?
)|()1( 121 ??
?+ iii wwwP?wi: headword)|( 12 ??
iii wwwP   ????????
?wi: function wordWe will denote the models using trigram forcategory probability estimation of Equation (6) asT-PHTM, and the models using unigram forcategory probability estimation of Equation (7) asU-PHTM.3 Using Clusters3.1 PrincipleClustering techniques attempt to make use ofsimilarities between words to produce a betterestimate of the probability of word strings(Goodman, 2001).We have mentioned in Section 2.2 that theheadword trigram model can be thought of as acluster-based model with two clusters, theheadword and the function word.
In this section, wedescribe a method of clustering automaticallysimilar words and headwords.
We followed thetechniques described in Goodman (2001) and Gaoet al (2001), and performed experiments usingpredictive clustering along with headword trigrammodels.3.2 Predictive clustering modelConsider a trigram probability P(w3|w1w2), wherew3 is the word to be predicted, called the predictedword, and w1 and w2 are context words used topredict w3, called the conditional words.
Gao et al(2001) presents a thorough comparative study onvarious clustering models for Asian languages,concluding that a model that uses clusters forpredicted words, called the predictive clusteringmodel, performed the best in most cases.Let iw  be the cluster which word wi belongs to.In this study, we performed word clustering forwords and headwords separately.
As a result, wehave the following two predictive clustering models,(8) for words and (9) for headwords:)|()|()|( 121212 iiiiiiiiii wwwwPwwwPwwwP ??????
?=  (8))|()|()|( 121212 iiiiiiiiii whhwPhhwPhhwP ??????
?=wi: headword(9)Substituting Equations (8) and (9) into Equation (7),we get the cluster-based PHTM of Equation (10),referred to as C-PHTM.P(wi|?
(w1?wi-1))= (10))|()|()((( 121221 iiiiiiii whhwPhhwPHP ????
???
))|()|()1( 21212 iiiiiii whhwPhhwP ????
?
?+ ?
)|()|()1( 12121 iiiiiii wwwwPwwwP ????
?
?+ ?wi: headword)|()|( 1212 iiiiiii wwwwPwwwP ????
?????????
?wi: function word3.3 Finding clusters: model estimationIn constructing clustering models, two factors wereconsidered: how to find optimal clusters, and theoptimal number of clusters.The clusters were found automatically byattempting to minimize perplexity (Brown et al,1992).
In particular, for predictive clusteringmodels, we tried to minimize the perplexity of thetraining data of )|()|( 1 iiii wwPwwP ??
.
Letting N bethe size of the training data, we have?=?
?Niiiii wwPwwP11 )|()|(?= ??
?=Ni iiiiiiWPwwPwPwwP1 11)()()()(?=??
?=Ni iiiiiiwPwwPwPwwP111 )()()()(?=??
?=Niiiii wwPwPwP111)|()()(Now,)()(1?iiwPwP is independent of the clustering used.Therefore, in order to select the best clusters, it issufficient to try to maximize ?= ?Ni ii wwP1 1 )|( .The clustering technique we used creates abinary branching tree with words at the leaves.
Bycutting the tree at a certain level, it is possible toachieve a wide variety of different numbers ofclusters.
For instance, if the tree is cut after the sixthlevel, there will be roughly 26=64 clusters.
In ourexperiments, we always tried the numbers ofclusters that are the powers of 2.
This seems toproduce numbers of clusters that are close enoughto optimal.
In Equation (10), the optimal number ofclusters we used was 27.4 Relation to Previous WorkOur LMs are similar to a number of existing ones.One such model was proposed by ATR (Isotani andMatsunaga, 1994), which we will refer to as ATRmodel below.
In ATR model, the probability ofeach word in a sentence is determined by thepreceding content and function word pair.
Isotaniand Matsunaga (1994) reported slightly betterresults over word bigram models for Japanesespeech recognition.
Geutner (1996) interpolated theATR model with word-based trigram models, andreported very limited improvements over wordtrigram models for German speech recognition.One significant difference between the ATRmodel and our own lies in the use of predictiveclustering.
Another difference is that our modelsuse separate probability estimates for headwordsand function words, as shown in Equations (6) and(7).
In contrast, ATR models are conceptually moresimilar to skipping models (Rosenfeld, 1994; Ney etal., 1994; Siu and Ostendorf, 2000), where only oneprobability estimate is applied for both content andfunction words, and the word categories are usedonly for the sake of finding the content and functionword pairs in the context.Another model similar to ours is Jelinek (1990),where the headwords of the two phrasesimmediately preceding the word as well as the lasttwo words were used to compute a wordprobability.
The resulting model is similar to a5-gram model.
A sophisticated interpolationformula had to be used since the number ofparameters is too large for direct estimation.
Ourmodels are easier to learn because they usetrigrams.
They also differ from Jelinek's model inthat they separately estimate the probability forheadwords and function words.A significant number of sophisticated techniquesfor language modeling have recently been proposedin order to capture more linguistic structure from alarger context.
Unfortunately, most of them sufferfrom either high computational cost or difficulty inobtaining enough manually parsed corpora forparameter estimation, which make it difficult toapply them successfully to realistic applications.For example, maximum entropy (ME) models(Rosenfeld, 1994) provide a nice framework forincorporating arbitrary knowledge sources, buttraining and using ME models is computationallyextremely expensive.Another interesting idea that exploits the use oflinguistic structure is structured language modeling(SLM, Chelba and Jelinek, 2000).
SLM uses astatistical parser trained on an annotated corpus inorder to identify the headword of each constituent,which are then used as conditioning words in thetrigram context.
Though SLMs have been shown tosignificantly improve the performance of the LMmeasured in perplexity, they also pose practicalproblems.
First, the performance of SLM iscontingent on the amount and quality ofsyntactically annotated training data, but such datamay not always be available.
Second, SLMs arevery time-intensive, both in their training and use.Charniak (2001) and Roark (2001) also presentlanguage models based on syntactic dependencystructure, which use lexicalized PCFGs that sumover the derivation probabilities.
They both reportimprovements in perplexity over Chelba andJelinek (2000) on the Wall Street Journal section ofthe Penn Treebank data, suggesting that syntacticstructure can be further exploited for languagemodeling.
The kind of linguistic structure used inour models is significantly more modest than thatprovided by parser-based models, yet offerspractical benefits for realistic applications, asshown in the next section.5 Evaluation MethodologyThe most common metric for evaluating a languagemodel is perplexity.
Perplexity can be roughlyinterpreted as the expected branching factor of thetest document when presented to a language model.Perplexity is widely used due to its simplicity andefficiency.
However, the ultimate quality of alanguage model must be measured by its effect onthe specific task to which it is applied, such asspeech recognition.
Lower perplexities usuallyresult in lower error rates, but there are numerouscounterexamples to this in the literature.In this study, we evaluated our language modelson the application of Japanese Kana-Kanjiconversion, which is the standard method ofinputting Japanese text by converting the text ofsyllabary-based Kana string into the appropriatecombination of ideographic Kanji and Kana.
This isa similar problem to speech recognition, except thatit does not include acoustic ambiguity.
Performanceon this task is generally measured in terms of thecharacter error rate (CER), which is the number ofcharacters wrongly converted from the phoneticstring divided by the number of characters in thecorrect transcript.
The role of the language model isto select the word string (in a combination of Kanjiand Kana) with the highest probability among thecandidate strings that match the typed phonetic(Kana) string.
Current products make about 5-10%errors in conversion of real data in a wide variety ofdomains.For our experiments, we used two newspapercorpora: Nikkei and Yomiuri Newspapers.
Bothcorpora have been word-segmented.
We builtlanguage models from a 36-million-word subset ofthe Nikkei Newspaper corpus.
We performedparameter optimization on a 100,000-word subsetof the Yomiuri Newspaper (held-out data).
Wetested our models on another 100,000-word subsetof the Yomiuri Newspaper corpus.
The lexicon weused contains 167,107 entries.In our experiments, we used the so-called?N-best rescoring?
method.
In this method, a list ofhypotheses is generated by the baseline languagemodel (a word trigram model in this study4), whichis then rescored using a more sophisticated LM.Due to the limited number of hypotheses in theN-best list, the second pass may be constrained bythe first pass.
In this study, we used the 100-bestlist.
The ?oracle?
CER (i.e., the CER among thehypotheses with the minimum number of errors) ispresented in Table 1.
This is the upper bound onperformance in our experiments.
The performanceof the conversion using the baseline trigram modelis much better than the state-of-the-art performancecurrently available in the marketplace.
This may bedue to the large amount of training data we used,and to the similarity between the training and thetest data.
We also notice that the ?oracle?
CER is4  For the detailed description of the baseline trigrammodel, see Gao et al (2002).relatively high due to the high out-of-vocabularyrate, which is 1.14%.
Because we have only limitedroom for improvement, the reported results of ourexperiments in this study may be underestimated.Baseline Trigram Oracle of 100-best3.73% 1.51%Table 1.
CER results of baseline and 100-best list6 Results and Discussion6.1 Impact of headword dependency andpredictive clusteringWe applied a series of language models proposed inthis paper to the Japanese Kana-Kanji conversiontask in order to test the effectiveness of ourtechniques.
The results are shown in Table 2.
Thebaseline result was obtained by using aconventional word trigram model.
HTM stands forthe headword trigram model of Equation (6) and (7)without permutation (i.e., ?2=1), while PHTM is themodel with permutation.
The T- and U-prefixesrefer to the models using trigram (Equation (6)) orunigram (Equation (7)) estimate for word categoryprobability.
The C-prefix, as in C-PHTM, refers toPHTM with predictive clustering (Equation (10)).For comparison, we also include in Table 2 theresults of using the predictive clustering modelwithout taking word category into account, referredto as predictive clustering trigram model (PCTM).In PCTM, the probability for all words is estimatedby )|()|( 1212 iiiiiii wwwwPwwwP ????
?
.Model ?1 ?
2 CER CER reductionBaseline ---- ---- 3.73% ----T-HTM 0.2 1 3.54% 5.1%U-HTM  0.2 1 3.41% 8.6%T-PTHM 0.2 0.7 3.53% 5.4%U-PHTM  0.2 0.7 3.34% 10.5%PCTM ---- ---- 3.44% 7.8%C-HTM  0.3 1 3.23% 13.4%C-PHTM  0.3 0.7 3.17% 15.0%Table 2.
Comparison of CER resultsIn Table 2, we find that for both PHTM and HTM,models U-HTM and U-PHTM achieve betterperformance than models T-HTM and T-PHTM.Therefore, only models using unigram for categoryprobability estimation are used for furtherexperiments, including the models with predictiveclustering.By comparing U-HTM with the baseline model,we can see that the headword trigram contributesgreatly to the CER reduction: U-HTMoutperformed the baseline model by about 8.6% inerror rate reduction.
HTM with headwordpermutation (U-PHTM) achieves furtherimprovements of 10.5% CER reduction against thebaseline.
The contribution of predictive clustering isalso very encouraging.
Using predictive clusteringalone (PCTM), we reduced the error rate by 7.8%.What is particularly noteworthy is that thecombination of both techniques leads to even largerimprovements: for both HTM and PHTM,predictive clustering (C-HTM and C-PHTM) bringsconsistent improvements over the models withoutclustering, achieving the CER reduction of 13.4%and 15.0% respectively against the baseline model,or 4.8% and 4.5% against the models withoutclustering.In sum, considering the good performance of ourbaseline system and the upper bound onperformance improvement due to the 100-best listas shown in Table 1, the improvements we obtainedare very promising.
These results demonstrate thatthe simple method of using headword trigrams andpredictive clustering can be used to effectivelyimprove the performance of word trigram models.6.2 Comparsion with other modelsIn this subsection, we present a comparison of ourmodels with some of the previously proposedmodels, including the higher-order n-gram models,skipping models, and the ATR models.Higher-order n-gram models refer to thosen-gram models in which n>3.
Although most of theprevious research showed little improvement,Goodman (2001) showed recently that, with a largeamount of training data and sophisticatedsmoothing techniques, higher-order n-gram modelscould be superior to trigram models.The headword trigram model proposed in thispaper can be thought of as a variation of a higherorder n-gram model, in that the headword trigramscapture longer distance dependencies than trigrammodels.
In order to see how far the dependency goeswithin our headword trigram models, we plotted thedistribution of headword trigrams (y-axis) againstthe n of the word n-gram were it to be captured bythe word n-gram (x-axis) in Figure 2.
For example,given a word sequence w1w2w3w4w5w6, and if w1, w3and w6 are headwords, then the headword trigramP(w6|w3w1) spans the same distance as the word6-gram model.0.0E+005.0E+061.0E+071.5E+072.0E+072.5E+071 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21N of n-gramNumberofwordsFigure 2.
Distribution of headword trigramsagainst the n of word n-gramFrom Figure 2, we can see that approximately95% of the headword trigrams can be captured bythe higher-order n-gram model with the value of nsmaller than 7.
Based on this observation, we builtword n-gram models with the values of n=4, 5 and6.
For all n-gram models, we used the interpolatedmodified absolute discount smoothing method (Gaoet al, 2001), which, in our experiments, achievedthe best performance among the state-of-the-artsmoothing techniques.
Results showed that theperformance of the higher-order word n-grammodels becomes saturated quickly as n grows: thebest performance was achieved by the word 5-grammodel, with the CER of 3.71%.
FollowingGoodman (2001), we suspect that the poorperformance of these models is due to the datasparseness problem.Skipping models are an extension of an n-grammodel in that they predict words based on nconditioning words, except that these conditioningwords may not be adjacent to the predicted word.For instance, instead of computing P(wi|wi-2wi-1), askipping model might compute P(wi|wi-3wi-1) orP(wi|wi-4wi-2).
Goodman (2001) performedexperiments of interpolating various kinds ofhigher-order n-gram skipping models, and obtaineda very limited gain.
Our results confirm his resultsand suggest that simply extending the contextwindow by brute-force can achieve littleimprovement, while the use of even the mostmodest form of structural information such as theidentification of headwords and automaticclustering can help improve the performance.We also compared our models with the trigramversion of the ATR models discussed in Section 4,in which the probability of a word is conditioned bythe preceding content and function word pair.
Weperformed experiments using the ATR models asdescribed in Isotani and Matsunaga (1994).
Theresults show that the CER of the ATR model aloneis much higher than that of the baseline model, butwhen interpolated with a word trigram model, theCER is slightly reduced by 1.6% from 3.73% to3.67%.
These results are consistent with thosereported in previous work.
The difference betweenthe ATR model and our models indicates that thepredictions of headwords and function words canbetter be done separately, as they play differentsemantic and syntactic roles capturing differentdependency structure.6.3 DiscussionIn order to better understand the effect of theheadword trigram, we have manually inspected theactual improvements given by PHTM.
As expected,many of the improvements seem to be due to the useof larger context: for example, the headwordtrigram??
~??
~??
(shouhi 'consume' ~shishutsu 'expense' ~ genshou 'decrease')contributed to the correct conversion of thephonetic string ?????
genshou into ?
?genshou 'decrease' rather than ?
?
genshou'phenomenon' in the context of ??????????
shouhi shishutsu hajimete no genshou  'consumerspending decreases for the first time'.On the other hand, the use of headword trigramsand predictive clustering is not without side effects.The overall gain in CER was 15% as we have seenabove, but a closer inspection of the conversionresults reveals that while C-PHTM corrected theconversion errors of the baseline model in 389sentences (8%), it also introduced new conversionerrors in 201 sentences (4.1%).
Among the newlyintroduced errors, one type of error is particularlyworth noting: these are the errors where thecandidate conversion preferred by the HTM isgrammatically impossible or unlikely.
For example,????????
(beikoku-ni shinkou-dekiru,USA-to invade-can 'can invade USA') wasmisconverted as ????????
(beikoku-nishinkou-dekiru, USA-to new-can), even though ??
shinkou 'invade' is far more likely to be precededby the morpheme ?
ni 'to', and ??
shinkou 'new'practically does not precede ???
dekiru 'can'.The HTM does not take these function words intoaccount, leading to a grammatically impossible orimplausible conversion.
Finding the types of errorsintroduced by particular modeling assumptions inthis manner and addressing them individually willbe the next step for further improvements in theconversion task.7 ConclusionWe proposed and evaluated a new language model,the permuted headword trigram model withclustering (C-PHTM).
We have shown that thesimple model that combines the predictiveclustering with a headword detector can effectivelycapture structure in language.
Experiments showthat the proposed model achieves an encouraging15% CER reduction over a conventional wordtrigram model in a Japanese Kana-Kanji conversionsystem.
We also compared C-PTHM to severalsimilar models, showing that our model has manypractical advantages, and achieves substantiallybetter performance.One issue we did not address in this paper wasthe language model size: the models that use HTMare larger than the baseline model we compared theperformance with.
Though we did not pursue theissue of size reduction in this paper, there are manyknown techniques that effectively reduce the modelsize while minimizing the loss in performance.
Onearea of future work is therefore to reduce the modelsize.
Other areas include the application of theproposed model to a wider variety of test corporaand to related tasks.AcknowledgementsWe would like to thank Ciprian Chelba, Bill Dolan,Joshua Goodman, Changning Huang, Hang Li andYoshiharu Sato for their comments on earlythoughts and drafts of the paper.
We would also liketo thank Hiroaki Kanokogi, Noriko Ishibashi andMiyuki Seki for their help in our experiments.ReferencesBrown, Peter F., Vincent J. Della Pietra, Peter V.deSouza, Jennifer C. Lai, and Robert L. Mercer.
1992.Class-Based N-gram Models of Natural Language.Computational Linguistics, 18-4: 467-479.Charniak, Eugene.
2001.
Immediate-head parsing forlanguage models.
In ACL/EACL 2001, pp.124-131.Chelba, Ciprian and Frederick Jelinek.
2000.
StructuredLanguage Modeling.
Computer Speech and Language,Vol.
14, No.
4. pp 283-332.Gao, Jianfeng, Joshua T. Goodman and Jiangbo Miao.2001.
The use of clustering techniques for languagemodel ?
application to Asian language.
ComputationalLinguistics and Chinese Language Processing.
Vol.
6,No.
1, pp 27-60.Gao, Jianfeng, Joshua Goodman, Mingjing Li andKai-Fu Lee.
2002.
Toward a unified approach tostatistical language modeling for Chinese.
ACMTransactions on Asian Language InformationProcessing, Vol.
1, No.
1, pp 3-33.Geutner, Petra.
1996.
Introducing linguistic constraintsinto statistical language modeling.
In InternationalConference on Spoken Language Processing,Philadelphia, USA.
pp.402-405.Goodman, Joshua T. 2001.
A bit of progress in languagemodeling.
Computer Speech and Language.
October,2001, pp 403-434.Goodman, Joshua T., and Jianfeng Gao.
2000.
Languagemodel size reduction by pruning and clustering.ICSLP-2000, Beijing.Isotani, Ryosuke and Shoichi Matsunaga.
1994.
Astochastic language model for speech recognitionintegrating local and global constraints.
ICASSP-94,pp.
5-8.Jelinek, Frederick.
1990.
Self-organized languagemodeling for speech recognition.
In A. Waibel and K.F.
Lee (eds.
), Readings in Speech Recognition,Morgan-Kaufmann, San Mateo, CA.
pp.
450-506.Katz, S. M. 1987.
Estimation of probabilities from sparsedata for other language component of a speechrecognizer.
IEEE transactions on Acoustics, Speechand Signal Processing, 35(3): 400-401.Ney, Hermann, Ute Essen and Reinhard Kneser.
1994.On structuring probabilistic dependences in stochasticlanguage modeling.
Computer Speech and Language,8: 1-38.Roark, Brian.
2001.
Probabilistic top-down parsing andlanguage modeling.
Computational Linguistics, 17-2:1-28.Rosenfeld, Ronald.
1994.
Adaptive statistical languagemodeling: a maximum entropy approach.
Ph.D. thesis,Carnegie Mellon University.Siu, Manhung and Mari Ostendorf.
2000.
Variablen-grams and extensions for conversational speechlanguage modeling.
IEEE Transactions on Speech andAudio Processing, 8: 63-75.
