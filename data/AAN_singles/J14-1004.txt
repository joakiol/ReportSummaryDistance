Learning Representations for WeaklySupervised Natural LanguageProcessing TasksFei Huang?Temple UniversityArun Ahuja?
?Northwestern UniversityDoug Downey?Northwestern UniversityYi Yang?Northwestern UniversityYuhong Guo?Temple UniversityAlexander Yates?Temple UniversityFinding the right representations for words is critical for building accurate NLP systems whendomain-specific labeled data for the task is scarce.
This article investigates novel techniques forextracting features from n-gram models, Hidden Markov Models, and other statistical languagemodels, including a novel Partial Lattice Markov Random Field model.
Experiments on part-of-speech tagging and information extraction, among other tasks, indicate that features takenfrom statistical language models, in combination with more traditional features, outperformtraditional representations alone, and that graphical model representations outperform n-grammodels, especially on sparse and polysemous words.?
1805 N. Broad St., Wachman Hall 324, Philadelphia, PA 19122, USA.E-mail: {fei.huang,yuhong,yates}@temple.edu.??
2133 Sheridan Road, Evanston, IL, 60208.
E-mail: ahuja@eecs.northwestern.edu.?
2133 Sheridan Road, Evanston, IL, 60208.
E-mail: ddowney@eecs.northwestern.edu.?
2133 Sheridan Road, Evanston, IL, 60208.
E-mail: yya518@eecs.northwestern.edu.Submission received: 13 June 2012; revised submission received: 25 November 2012, accepted for publication:15 January 2013.doi:10.1162/COLI a 00167?
2014 Association for Computational LinguisticsComputational Linguistics Volume 40, Number 11.
IntroductionNLP systems often rely on hand-crafted, carefully engineered sets of features to achievestrong performance.
Thus, a part-of-speech (POS) tagger would traditionally use afeature like, ?the previous token is the?
to help classify a given token as a nounor adjective.
For supervised NLP tasks with sufficient domain-specific training data,these traditional features yield state-of-the-art results.
However, NLP systems are in-creasingly being applied to the Web, scientific domains, personal communications likee-mails and tweets, among many other kinds of linguistic communication.
These textshave very different characteristics from traditional training corpora in NLP.
Evidencefrom POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing(Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan,Ward, and Martin 2007), among other NLP tasks (Daume?
III and Marcu 2006; Chelbaand Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer,Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degradessignificantly when tested on domains different from those used for training.
Collectinglabeled training data for each new target domain is typically prohibitively expensive.In this article, we investigate representations that can be applied to weakly supervisedlearning, that is, learning when domain-specific labeled training data are scarce.A growing body of theoretical and empirical evidence suggests that traditional,manually crafted features for a variety of NLP tasks limit systems?
performance in thisweakly supervised learning for two reasons.
First, feature sparsity prevents systemsfrom generalizing accurately, because many words and features are not observed intraining.
Also because word frequencies are Zipf-distributed, this often means that thereis little relevant training data for a substantial fraction of parameters (Bikel 2004b), espe-cially in new domains (Huang and Yates 2009).
For example, word-type features formthe backbone of most POS-tagging systems, but types like ?gene?
and ?pathway?
showup frequently in biomedical literature, and rarely in newswire text.
Thus, a classifiertrained on newswire data and tested on biomedical data will have seen few trainingexamples related to sentences with features ?gene?
and ?pathway?
(Blitzer, McDonald,and Pereira 2006; Ben-David et al.
2010).Further, because words are polysemous, word-type features prevent systems fromgeneralizing to situations in which words have different meanings.
For instance, theword type ?signaling?
appears primarily as a present participle (VBG) in Wall StreetJournal (WSJ) text, as in, ?Interest rates rose, signaling that .
.
.
?
(Marcus, Marcinkiewicz,and Santorini 1993).
In biomedical text, however, ?signaling?
appears primarily in thephrase ?signaling pathway,?
where it is considered a noun (NN) (PennBioIE 2005); thisphrase never appears in the WSJ portion of the Penn Treebank (Huang and Yates 2010).Our response to the sparsity and polysemy challenges with traditional NLP repre-sentations is to seek new representations that allow systems to generalize to previouslyunseen examples.
That is, we seek representations that permit classifiers to have closeto the same accuracy on examples from other domains as they do on the domain of thetraining data.
Our approach depends on the well-known distributional hypothesis,which states that a word?s meaning is identified with the contexts in which it appears(Harris 1954; Hindle 1990).
Our goal is to develop probabilistic statistical languagemodels that describe the contexts of individual words accurately.
We then constructrepresentations, or mappings from word tokens and types to real-valued vectors,from statistical language models.
Because statistical language models are designed tomodel words?
contexts, the features they produce can be used to combat problemswith polysemy.
And by careful design of the statistical language models, we can limit86Huang et al.
Computational Linguisticsthe number of features that they produce, controlling how sparse those features are intraining data.Our specific contributions are as follows:1.
We show how to generate representations from a variety of languagemodels, including n-gram models, Brown clusters, and Hidden MarkovModels (HMMs).
We also introduce a Partial-Lattice Markov RandomField (PL-MRF), which is a tractable variation of a Factorial HiddenMarkov Model (Ghahramani and Jordan 1997) for language modeling,and we show how to produce representations from it.2.
We quantify the performance of these representations in experimentson POS tagging in a domain adaptation setting, and weakly supervisedinformation extraction (IE).
We show that the graphical models outperformn-gram representations, even when the n-gram models leverage largercorpora for training.
The PL-MRF representation achieves a state-of-the-art93.8% accuracy on a biomedical POS tagging task, which represents a5.5 percentage point absolute improvement over more traditional POStagging representations, a 4.8 percentage point improvement over a taggerusing an n-gram representation, and a 0.7 percentage point improvementover a tagger with an n-gram representation using several orders ofmagnitude more training data.
The HMM representation improvesover the n-gram model by 7 percentage points on our IE task.3.
We analyze how sparsity, polysemy, and differences between domainsaffects the performance of a classifier using different representations.Results indicate that statistical language model representations, andespecially graphical model representations, provide the best featuresfor sparse and polysemous words.The next section describes background material and related work on representationlearning for NLP.
Section 3 presents novel representations based on statistical languagemodels.
Sections 4 and 5 discuss evaluations of the representations, first on sequence-labeling tasks in a domain adaptation setting, and second on a weakly supervised set-expansion task.
Section 6 concludes and outlines directions for future work.2.
Background and Previous Work on Representation Learning2.1 Terminology and NotationIn a traditional machine learning task, the goal is to make predictions on test data usinga hypothesis that is optimized on labeled training data.
In order to do so, practitionerspredefine a set of features and try to estimate classifier parameters from the observedfeatures in the training data.
We call these feature sets representations of the data.Formally, let X be an instance space for a learning problem.
Let Z be the space ofpossible labels for an instance, and let f : X ?
Z be the target function to be learned.A representation is a function R: X ?
Y , for some suitable feature space Y (such as Rd).We refer to dimensions of Y as features, and for an instance x ?
X we refer to valuesfor particular dimensions of R(x) as features of x.
Given a set of training examples, alearning machine?s task is to select a hypothesis h from the hypothesis space H, a subsetof ZR(X ).
Errors by the hypothesis are measured using a loss function L(x, R, f, h) that87Computational Linguistics Volume 40, Number 1measures the cost of the mismatch between the target function f (x) and the hypothesish(R(x)).As an example, the instance set for POS tagging in English is the set of all Englishsentences, and Z is the space of POS sequences containing labels like NN (for noun) andVBG (for present participle).
The target function f is the mapping between sentencesand their correct POS labels.
A traditional representation in NLP converts sentencesinto sequences of vectors, one for each word position.
Each vector contains values forfeatures like, ?+1 if the word at this position ends with -tion, and 0 otherwise.?
Atypical loss function would count the number of words that are tagged differently byf (x) and h(R(x)).2.2 Representation-Learning Problem FormulationMachine learning theory assumes that there is a distribution D over X from whichdata is sampled.
Given a training set S = {(x1, z1), .
.
.
, (xN, zN )} ?
(D(X ),Z )N, a fixedrepresentation R, a hypothesis space H, and a loss function L, a machine learningalgorithm seeks to identify the hypothesis in H that will minimize the expected lossover samples from distribution D:h?
= argminh?HEx?D(X )L(x, R, f, h) (1)The representation-learning paradigm breaks the traditional notion of a fixed rep-resentation R. Instead, we allow a space of possible representations R. The full learningproblem can then be formulated as the task of identifying the best R ?
R and h ?
Hsimultaneously:R?, h?
= argminR?R,h?HEx?D(X )L(x, R, f, h) (2)The representation-learning problem formulation in Equation (2) can in fact bereduced to the general learning formulation in Equation (1) by setting the fixed rep-resentation R to be the identity function, and setting the hypothesis space to be R?Hfrom the representation-learning task.
We introduce the new formulation primarily asa way of changing the perspective on the learning task: most NLP systems considera fixed, manually crafted transformation of the original data to some new space, andinvestigate hypothesis classes over that space.
In the new formulation, systems learnthe transformation to the feature space, and then apply traditional classification orregression algorithms.2.3 Theory on Domain AdaptationWe refer to the distribution D over the instance space X as a domain.
For example,the newswire domain is a distribution over sentences that gives high probability tosentences about governments and current events; the biomedical literature domaingives high probability to sentences about proteins and regulatory pathways.
In domainadaptation, a system observes a set of training examples (R(x), f (x)), where instancesx ?
X are drawn from a source domain DS, to learn a hypothesis for classifying ex-amples drawn from a separate target domain DT.
We assume that large quantities ofunlabeled data are available for the source domain and target domain, and call these88Huang et al.
Computational Linguisticssamples US and UT, respectively.
For any domain D, let R(D) represent the induceddistribution over the feature space Y given by PrR(D)[y] = PrD[{x such that R(x) = y}].Previous work by Ben-David et al.
(2007, 2010) proves theoretical bounds on anopen-domain learning machine?s performance.
Their analysis shows that the choice ofrepresentation is crucial to domain adaptation.
A good choice of representation mustallow a learning machine to achieve low error rates on the source domain.
Just asimportant, however, is that the representation must simultaneously make the sourceand target domains look as similar to one another as possible.
That is, if the labelingfunction f is the same on the source and target domains, then for every h ?
H, we canprovably bound the error of h on the target domain by its error on the source domainplus a measure of the distance between DS and DT:Ex?DTL(x, R, f, h) ?
Ex?DSL(x, R, f, h) + d1(R(DS), R(DT )) (3)where the variation divergence d1 is given byd1(D,D?)
= 2 supB?B|PrD[B] ?
PrD?
[B]| (4)where B is the set of measurable sets under D and D?
(Ben-David et al.
2007, 2010).Crucially, the distance between domains depends on the features in the representa-tion.
The more that features appear with different frequencies in different domains, theworse this bound becomes.
In fact, one lower bound for the d1 distance is the accuracyof the best classifier for predicting whether an unlabeled instance y = R(x) belongs todomain S or T (Ben-David et al.
2010).
Thus, if R provides one set of common features forexamples from S, and another set of common features for examples from T, the domainof an instance becomes easy to predict, meaning the distance between the domainsgrows, and the bound on our classifier?s performance grows worse.In light of Ben-David et al.
?s theoretical findings, traditional representations inNLP are inadequate for domain adaptation because they contribute to the d1 distancebetween domains.
Although many previous studies have shown that lexical featuresallow learning systems to achieve impressively low error rates during training, they alsomake texts from different domains look very dissimilar.
For instance, a feature based onthe word ?bank?
or ?CEO?
may be common in a domain of newswire text, but scarceor nonexistent in, say, biomedical literature.
Ben David et al.
?s theory predicts greatervariance in the error rate of the target domain classifier as the distance grows.At the same time, traditional representations contribute to data sparsity, a lack ofsufficient training data for the relevant parameters of the system.
In traditional super-vised NLP systems, there are parameters for each word type in the data, or perhapseven combinations of word types.
Because vocabularies can be extremely large, thisleads to an explosion in the number of parameters.
As a consequence, for many of theirparameters, supervised NLP systems have zero or only a handful of relevant labeledexamples (Bikel 2004a, 2004b).
No matter how sophisticated the learning technique, itis difficult to estimate parameters without relevant data.
Because vocabularies differacross domains, domain adaptation greatly exacerbates this issue of data sparsity.2.4 Problem Formulation for the Domain Adaptation SettingFormally, we define the task of representation learning for domain adaptation as thefollowing optimization problem: Given a set of unlabeled instances US drawn from the89Computational Linguistics Volume 40, Number 1source domain and unlabeled instances UT from the target domain, as well as a set oflabeled instances LS drawn from the source domain, identify a function R?
from thespace of possible representations R that minimizesR?, h?
= argminR?R,h?H(Ex?DSL(x, R, f, h))+ ?d1(R(DS), R(DT )) (5)where ?
is a free parameter.Note that there is an underlying tension between the two terms of the objec-tive function: The best representation for the source domain would naturally includedomain-specific features, and allow a hypothesis to learn domain-specific patterns.We are aiming, however, for the best general classifier, which happens to be trainedon training data from one domain (or a few domains).
The domain-specific featurescontribute to distance between domains, and to classifier errors on data taken fromdomains not seen in training.
By optimizing for this combined objective function, weallow the optimization method to trade off between features that are best for classifyingsource-domain data and features that allow generalization to new domains.Unlike the representation-learning problem-formulation in Equation (2), Equa-tion (5) does not reduce to the standard machine-learning problem (Equation (1)).
Ina sense, the d1 term acts as a regularizer on R, which also affects H. Representationlearning for domain adaptation is a fundamentally novel learning task.2.5 Tractable Representation Learning: Statistical Language Modelsas RepresentationsFor most hypothesis classes and any interesting space of representations, Equations (2)and (5) are completely intractable to optimize exactly.
Even given a fixed representation,it is intractable to compute the best hypothesis for many hypothesis classes.
And the d1metric is intractable to compute from samples of a distribution, although Ben-Davidet al.
(2007, 2010) propose some tractable bounds.
We view these problem formulationsas high-level goals rather than as computable objectives.As a tractable objective, in this work we describe an investigation into the use ofstatistical language models as a way to represent the meanings of words.
This approachdepends on the well-known distributional hypothesis, which states that a word?smeaning is identified with the contexts in which it appears (Harris 1954; Hindle 1990).From this hypothesis, we can formulate the following testable prediction, which we callthe statistical language model representation hypothesis, or LMRH:To the extent that a model accurately describes a word?s possible contexts, parametersof that model are highly informative descriptors of the word?s meaning, and aretherefore useful as features in NLP tasks like POS tagging, chunking, NER, andinformation extraction.The LMRH says, essentially, that for NLP tasks, we can decouple the task of optimiz-ing a representation from the task of optimizing a hypothesis.
To learn a representation,we can train a statistical language model on unlabeled text, and then use parametersor latent states from the statistical language model to create a representation function.Optimizing a hypothesis then follows the standard learning framework, using therepresentation from the statistical language model.90Huang et al.
Computational LinguisticsThe LMRH is similar to the manifold and cluster assumptions behind other semi-supervised approaches to machine learning, such as Alternating Structure Optimization(ASO) (Ando and Zhang 2005) and Structural Correspondence Learning (SCL) (Blitzer,McDonald, and Pereira 2006).
All three of these techniques use predictors built onunlabeled data as a way to harness the manifold and cluster assumptions.
However,the LMRH is distinct from at least ASO and SCL in important ways.
Both ASO and SCLcreate multiple ?synthetic?
or ?pivot?
prediction tasks using unlabeled data, and findtransformations of the input feature space that perform well on these tasks.
The LMRH,on the other hand, is more specific ?
it asserts that for language problems, if we opti-mize word representations on a single task (the language modeling task), this will leadto strong performance on weakly supervised tasks.
In reported experiments on NLPtasks, both ASO and SCL use certain synthetic predictors that are essentially languagemodeling tasks, such as the task of predicting whether the next token is of word type w.To the extent that these techniques?
performance relies on language-modeling tasks astheir ?synthetic predictors,?
they can be viewed as evidence in support of the LMRH.One significant consequence of the LMRH is that it allows us to leverage well-developed techniques and models from statistical language modeling.
Section 3presents a series of statistical language models that we investigate for learning repre-sentations for NLP.2.6 Previous WorkThere is a long tradition of NLP research on representations, mostly falling into one offour categories: 1) vector space models of meaning based on document-level lexical co-occurrence statistics (Salton and McGill 1983; Sahlgren 2006; Turney and Pantel 2010);2) dimensionality reduction techniques for vector space models (Deerwester et al.
1990;Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, andJordan 2003; Va?yrynen and Honkela 2004, 2005; Va?yrynen, Honkela, and Lindqvist2007); 3) using clusters that are induced from distributional similarity (Brown et al.1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparsefeatures (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al.
2009; Zhaoet al.
2009); and, recently, 4) neural network statistical language models (Bengio 2008;Bengio et al.
2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnihand Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobertand Weston 2008; Bengio et al.
2009).
Our work is a form of distributional clusteringfor representations, but where previous work has used bigram and trigram statistics toform clusters, we build sophisticated models that attempt to capture the context of aword, and hence its similarity to other words, more precisely.
Our experiments showthat the new graphical models provide representations that outperform those fromprevious work on several tasks.Neural network statistical language models have recently achieved state-of-the-artperplexity results (Mnih and Hinton 2009), and representations based on them have im-proved in-domain chunking, NER, and SRL (Weston, Ratle, and Collobert 2008; Turian,Bergstra, and Bengio 2009; Turian, Ratinov, and Bengio 2010).
As far as we are aware,Turian, Ratinov, and Bengio (2010) is the only other work to test a learned representationon a domain adaptation task, and they show improvement on out-of-domain NERwith their neural net representations.
Though promising, the neural network modelsare computationally expensive to train, and these statistical language models workonly on fixed-length histories (n-grams) rather than full observation sequences.
Turian,91Computational Linguistics Volume 40, Number 1Ratinov, and Bengio?s (2010) tests also show that Brown clusters perform as well orbetter than neural net models on all of their chunking and NER tests.
We concentrate onprobabilistic graphical models with discrete latent states instead.
We show that HMM-based and other representations significantly outperform the more commonly usedBrown clustering (Brown et al.
1992) as a representation for domain adaptation settingsof sequence-labeling tasks.Most previous work on domain adaptation has focused on the case where somelabeled data are available in both the source and target domains (Chan and Ng 2006;Daume?
III and Marcu 2006; Blitzer, Dredze, and Pereira 2007; Daume?
III 2007; Jiangand Zhai 2007a, 2007b; Dredze and Crammer 2008; Finkel and Manning 2009; Dredze,Kulesza, and Crammer 2010).
Learning bounds for this domain-adaptation setting areknown (Blitzer et al.
2007; Mansour, Mohri, and Rostamizadeh 2009).
Approaches to thisproblem setting have focused on appropriately weighting examples from the source andtarget domains so that the learning algorithm can balance the greater relevance of thetarget-domain data with the larger source-domain data set.
In some cases, researcherscombine this approach with semi-supervised learning to include unlabeled examplesfrom the target domain as well (Daume?
III, Kumar, and Saha 2010).
These techniquesdo not handle open-domain corpora like the Web, where they require expert input toacquire labels for each new single-domain corpus, and it is difficult to come up witha representative set of labeled training data for each domain.
Our technique requiresonly unlabeled data from each new domain, which is significantly easier and cheaper toacquire.
Where target-domain labeled data is available, however, these techniques canin principle be combined with ours to improve performance, although this has not yetbeen demonstrated empirically.A few researchers have considered the more general case of domain adaptationwithout labeled data in the target domain.
Perhaps the best known is Blitzer, McDonald,and Pereira?s (2006) Structural Correspondence Learning (SCL).
SCL uses ?pivot?
wordscommon to both source and target domains, and trains linear classifiers to predict thesepivot words from their context.
After an SVD reduction of the weight vectors for theselinear classifiers, SCL projects the original features through these weight vectors toobtain new features that are added to the original feature space.
Like SCL, our languagemodeling techniques attempt to predict words from their context, and then use theoutput of these predictions as new features.
Unlike SCL, we attempt to predict all wordsfrom their context, and we rely on traditional probabilistic methods for language mod-eling.
Our best learned representations, which involve significantly different techniquesfrom SCL, especially latent-variable probabilistic models, significantly outperform SCLin POS tagging experiments.Other approaches to domain adaptation without labeled data from the target do-main include Satpal and Sarawagi (2007), who show that by changing the optimizationfunction during conditional random field (CRF) training, they can learn classifiers thatport well to new domains.
Their technique selects feature subsets that minimize thedistance between training text and unlabeled test text, but unlike our techniques, theirscannot learn representations with features that do not appear in the original feature set.In contrast, we learn hidden features through statistical language models.
McClosky,Charniak, and Johnson (2010) use classifiers from multiple source domains and featuresthat describe how much a target document diverges from each source domain to deter-mine an optimal weighting of the source-domain classifiers for parsing the target text.However, it is unclear if this ?source-combination?
technique works well on domainsthat are not mixtures of the various source domains.
Dai et al.
(2007) use KL-divergencebetween domains to directly modify the parameters of their naive Bayes model for a92Huang et al.
Computational Linguisticstext classification task trained purely on the source domain.
These last two techniquesare not representation learning, and are complementary to our techniques.Our representation-learning approach to domain adaptation is an instance ofsemi-supervised learning.
Of the vast number of semi-supervised approaches tosequence labeling in NLP, the most relevant ones here include Suzuki and Isozaki?s(2008) combination of HMMs and CRFs that uses over a billion words of unlabeled textto achieve the current best performance on in-domain chunking, and semi-supervisedapproaches to improving in-domain SRL with large quantities of unlabeled text(Weston, Ratle, and Collobert 2008; Deschacht and Moens 2009; and Fu?rstenau andLapata 2009).
Ando and Zhang?s (2005) semi-supervised sequence labeling techniquehas been tested on a domain adaptation task for POS tagging (Blitzer, McDonald, andPereira 2006); our representation-learning approaches outperform it.
Unlike most semi-supervised techniques, we concentrate on a particularly simple task decomposition: un-supervised learning for new representations, followed by standard supervised learning.In addition to our task decomposition being simple, our learned representations are alsotask-independent, so we can learn the representation once, and then apply it to any task.One of the best-performing representations that we consider for domain adaptationis based on the HMM (Rabiner 1989).
HMMs have of course also been used for super-vised, semi-supervised, and unsupervised POS tagging on a single domain (Banko andMoore 2004; Goldwater and Griffiths 2007).
Recent efforts on improving unsupervisedPOS tagging have focused on incorporating prior knowledge into the POS inductionmodel (Grac?a et al.
2009; Toutanova and Johnson 2007), or on new training techniqueslike contrastive estimation (Smith and Eisner 2005) for alternative sequence models.Despite the fact that completely connected, standard HMMs perform poorly at the POSinduction task (Johnson 2007), we show that they still provide very useful featuresfor a supervised POS tagger.
Experiments in information extraction have previouslyalso shown that HMMs provide informative features for this quite different, semanticprocessing task (Downey, Schoenmackers, and Etzioni 2007; Ahuja and Downey 2010).This article extends our previous work on learning representations for do-main adaptation (Huang and Yates 2009, 2010) by investigating new languagerepresentations?the naive Bayes representation and PL-MRF representation (Huanget al.
2011)?by analyzing results in terms of polysemy, sparsity, and domain diver-gence; by testing on new data sets including a Chinese POS tagging task; and by pro-viding an empirical comparison with Brown clusters as representations.3.
Learning Representations of Distributional SimilarityIn this section, we will introduce several representation learning models.3.1 Traditional POS-Tagging RepresentationsAs an example of our terminology, we begin by describing a representation used intraditional POS taggers (this representation will later form a baseline for our POStagging experiments).
The instance set X is the set of English sentences, and Z is the setof POS tag sequences.
A traditional representation TRAD-R maps a sentence x ?
X to asequence of boolean-valued vectors, one vector per word xi in the sentence.
Dimensionsfor each latent vector include indicators for the word type of xi and various orthographicfeatures.
Table 1 presents the full list of features in TRAD-R. Because our IE task classifiesword types rather than tokens, this baseline is not appropriate for that task.
Herein, we93Computational Linguistics Volume 40, Number 1Table 1Summary of features provided by our representations.
?a1[g(a)] represents a set of booleanfeatures, one for each value of a, where the feature is true iff g(a) is true.
xi represents a token atposition i in sentence x, w represents a word type, Suffixes = {-ing,-ogy,-ed,-s,-ly,-ion,-tion,-ity},k (and k) represents a value for a latent state (set of latent states) in a latent-variable model, y?represents the maximum a posteriori sequence of states y for x, yi is the latent variable for xi, andyi,j is the latent variable for xi at layer j. prefix(y,p) is the p-length prefix of the Brown cluster y.Representation FeaturesTRAD-R ?w1[xi = w]?s?Suffixes1[xi ends with s]1[xi contains a digit]n-GRAM-R ?w?
,w??P(w?ww??
)/P(w)LSA-R ?w,j{v?left(w)}j?w,j{v?right(w)}jNB-R ?k1[y?i = k]HMM-TOKEN-R ?k1[y?i = k]HMM-TYPE-R ?kP(y = k|x = w)I-HMM-TOKEN-R ?j,k1[y?i,j = k]I-HMM-TYPE-R ?j,kP(y.,j = k|x = w)BROWN-TOKEN-R ?j?{?2,?1,0,1,2}?p?
{4,6,10,20} prefix(yi+j, p)BROWN-TYPE-R ?p prefix(y, p)LATTICE-TOKEN-R ?j,k1[y?i,j = k]LATTICE-TYPE-R ?kP(y = k|x = w)describe how we can learn representations R by using a variety of statistical languagemodels, for use in both our IE and POS tagging tasks.
All representations for POStagging inherit the features from TRAD-R; all representations for IE do not.3.2 n-gram Representationsn-gram representations, which we call n-GRAM-R, model a word type w in terms of then-gram contexts in which w appears in a corpus.
Specifically, for word w we generatethe vector P(w?ww??
)/P(w), the conditional probability of observing the word sequencew?
to the left and w??
to the right of w. Each dimension in this vector represents a com-bination of the left and right words.
The experimental section describes the particularcorpora and statistical language modeling methods used for estimating probabilities.Note that these features depend only on the word type w, and so for every token xi = w,n-GRAM-R provides the same set of features regardless of local context.One drawback of n-GRAM-R is that it does not handle sparsity well?the featuresare as sparsely observed as the lexical features in TRAD-R, except that n-GRAM-R fea-tures can be obtained from larger corpora.
As an alternative, we apply latent semanticanalysis (LSA) (Deerwester et al.
1990) to compute a reduced-rank representation.
Forword w, let vright(w) represent the right context vector of w, which in each dimensioncontains the value of P(ww??
)/P(w) for some word w?
?, as observed in the n-grammodel.
Similarly, let vleft(w) be the left context vector of w. We apply LSA to the set94Huang et al.
Computational Linguistics     	 	 Figure 1A graphical representation of the naive Bayes statistical language model.
The B and E are specialdummy words for the beginning and end of the sentence.of right context vectors and the set of left context vectors separately,1 to find reduced-rank versions v?right(w) and v?left(w), where each dimension represents a combinationof several context word types.
We then use each component of v?right(w) and v?left(w)as features.
After experimenting with different choices for the number of dimensions toreduce our vectors to, we choose a value of 10 dimensions as the one that maximizesthe performance of our supervised sequence labelers on held-out data.
We call thismodel LSA-R.3.3 A Context-Dependent Representation Using Naive BayesThe n-GRAM-R and LSA-R representations always produce the same features F for agiven word type w, regardless of the local context of a particular token xi = w. Ourremaining representations are all context-dependent, in the sense that the featuresprovided for token xi depend on the local context around xi.
We begin with a statis-tical language model based on the Naive Bayes model with categorical latent statesS = {1, .
.
.
, K}.
First, we form trigrams from our sentences.
For each trigram, we form aseparate Bayes net in which each token from the trigram is conditionally independentgiven the latent state.
For tokens xi?1, xi, and xi+1, the probability of this trigram givenlatent state Yi = y is given by:P(xi?1, xi, xi+1|yi) = Pleft(xi?1|yi)Pmid(xi|yi)Pright(xi+1|yi) (6)where Pleft, Pmid, and Pright are multinomial distributions conditioned on the latent state.The probability of a whole sentence is then given by the product of the probabilitiesof its trigrams.
Figure 1 shows a graphical representation of this model.
We train ourmodels using standard expectation-maximization (Dempster, Laird, and Rubin 1977)with random initialization of the parameters.Because our factorization of the sentence does not take into account the fact that thetrigrams overlap, the resulting statistical language model is mass-deficient.
Worse still,it is throwing away information from the dependencies among trigrams which mighthelp make better clustering decisions.
Nevertheless, this model closely mirrors manyof the clustering algorithms used in previous approaches to representation learning forsequence labeling (Ushioda 1996; Miller, Guinness, and Zamanian 2004; Koo, Carreras,1 Compare with Dhillon, Foster, and Ungar (2011), who use canonical correlation analysis to find asimultaneous reduction of the left and right context vectors, a significantly more complex undertaking.95Computational Linguistics Volume 40, Number 1and Collins 2008; Lin and Wu 2009; Ratinov and Roth 2009), and therefore serves as animportant benchmark.Given a naive Bayes statistical language model, we construct an NB-R representa-tion that produces |S| boolean features Fs(xi) for each token xi and each possible latentstate s ?
S:Fs(xi) ={true if s = arg maxs?
?SP(xi?1, xi, xi+1|yi = s?
),false otherwise.For a reasonable choice of S (i.e., |S|  |V|), each feature should be observed oftenin a sufficiently large training data set.
Therefore, compared with n-GRAM-R, NB-Rproduces far fewer features.
On the other hand, its features for xi depend not just onthe contexts in which xi has appeared in the statistical language model?s training data,but also on xi?1 and xi+1 in the current sentence.
Furthermore, because the range ofthe features is much more restrictive than real-valued features, it is less prone to datasparsity or variations across domains than real-valued features.3.4 Context-Dependent, Structured Representations: The Hidden Markov ModelIn previous work, we have implemented several representations based on hiddenMarkov models (Rabiner 1989), which we used for both sequential labeling (like POStagging [Huang et al.
2011] and NP chunking [Huang and Yates 2009]) and IE (Downey,Schoenmackers, and Etzioni 2007).
Figure 2 shows a graphical model of an HMM.
AnHMM is a generative probabilistic model that generates each word xi in the corpusconditioned on a latent variable yi.
Each yi in the model takes on integral values from 1to K, and each one is generated by the latent variable for the preceding word, yi?1.
Thejoint distribution for a corpus x = (x1, .
.
.
, xN ) and a set of state vectors y = (y1, .
.
.
, yN )is given by: P(x, y) =?i P(xi|yi)P(yi|yi?1).
Using expectation-maximization (EM)(Dempster, Laird, and Rubin 1977), it is possible to estimate the distributions forP(xi|yi) and P(yi|yi?1) from unlabeled data.We construct two different representations from HMMs, one for sequence-labelingtasks and one for IE.
For sequence labeling, we use the Viterbi algorithm to produce theoptimal setting y?
of the latent states for a given sentence x, or y?
= argmaxyP(x, y).
Weuse the value of y?i as a new feature for xi that represents a cluster of distributionallysimilar words.
For IE, we require features for word types w, rather than tokens xi.Applying Bayes?
rule to the HMM parameters, we compute a distribution P(Y|x = w),where Y is a single latent node, x is a single token, and w is its word type.
We then useeach of the K values for P(Y = k|x = w), where k ranges from 1 to K, as features.
This set     	 	Figure 2The Hidden Markov Model.96Huang et al.
Computational Linguisticsof features represents a ?soft clustering?
of w into K different clusters.
We refer to theserepresentations as HMM-TOKEN-R and HMM-TYPE-R, respectively.We also compare against a multi-layer variation of the HMM from our previouswork (Huang and Yates 2010).
This model trains an ensemble of M independent HMMmodels on the same corpus, initializing each one randomly.
We can then use the Viterbi-optimal decoded latent state of each independent HMM model as a separate feature fora token, or the posterior distribution for P(Y|x = w) from each HMM as a separate setof features for each word type.
We refer to this statistical language model as an I-HMM,and the representations as I-HMM-TOKEN-R and I-HMM-TYPE-R, respectively.Finally, we compare against Brown clusters (Brown et al.
1992) as learned features.Although not traditionally described as such, Brown clustering involves constructingan HMM model in which each word type is restricted to having exactly one latent statethat may generate it.
Brown et al.
describe a greedy agglomerative clustering algorithmfor training this model on unlabeled text.
Following Turian, Ratinov, and Bengio (2010),we use Percy Liang?s implementation of this algorithm for our comparison, and we testruns with 100, 320, 1,000 and 3,200 clusters.
We use features from these clusters identicalto Turian et al.
?s.2 Turian et al.
have shown that Brown clusters match or exceed theperformance of neural network-based statistical language models in domain adaptationexperiments for named-entity recognition, as well as in-domain experiments for NERand chunking.Because HMM-based representations offer a small number of discrete states asfeatures, they have a much greater potential to combat sparsity than do n-gram mod-els.
Furthermore, for token-based representations, these models can potentially handlepolysemy better than n-gram statistical language models by providing different featuresin different contexts.3.5 A Novel Lattice Statistical Language Model RepresentationOur final statistical language model is a novel latent-variable statistical language model,called a Partial Lattice MRF (PL-MRF), with rich latent structure, shown in Figure 3.
Themodel contains a lattice of M ?
N latent states, where N is the number of words in asentence and M is the number of layers in the model.
The dotted and solid lines in thefigure together form a complete lattice of edges between these nodes; the PL-MRF usesonly the solid edges.
Formally, let c = 	N2, where N is the length of the sentence; let idenote a position in the sentence, and let j denote a layer in the lattice.
If i < c and j isodd, or if j is even and i > c, we delete edges between yi,j and yi,j+1 from the completelattice.
The same set of nodes remains, but the partial lattice contains fewer edges andpaths between the nodes.
A central ?trunk?
at i = c connects all layers of the lattice, andbranches from this trunk connect either to the branches in the layer above or the layerbelow (but not both).The result is a model that retains most of the edges of the complete lattice, butunlike the complete lattice, it supports tractable inference.
As M, N ?
?, five out ofevery six edges from the complete lattice appear in the PL-MRF.
However, the PL-MRFmakes the branches conditionally independent from one another, except through thetrunk.
For instance, the left branch between layers 1 and 2 ((y1,1, y1,2) and (y2,1, y2,2)) inFigure 3 are disconnected; similarly, the right branch between layers 2 and 3 ((y4,2, y4,3)and (y5,2, y5,3)) are disconnected, except through the trunk and the observed nodes.
As2 Percy Liang?s implementation is available at http://metaoptimize.com/projects/wordreprs/.97Computational Linguistics Volume 40, Number 1y4,1y3,1y4,2y3,2y4,3y3,3y4,4y3,4y4,5y3,5x1y2,1y1,1x2y2,2y1,2x3y2,3y1,3x4y2,4y1,4x5y2,5y1,5Figure 3The PL-MRF model for a five-word sentence and a four-layer lattice.
Dashed gray edges are partof a complete lattice, but not part of the PL-MRF.a result, excluding the observed nodes, this model has a low tree-width of 2 (excludingobserved nodes), and a variety of efficient dynamic programming and message-passingalgorithms for training and inference can be readily applied (Bodlaender 1988).
Ourinference algorithm passes information from the branches inwards to the trunk, andthen upward along the trunk, in time O(K4MN).
In contrast, a fully connected latticemodel has tree-width = min(M, N), making inference and learning intractable (Sutton,McCallum, and Rohanimanesh 2007), partly because of the difficulty in enumeratingand summing over the exponentially-many configurations y for a given x.We can justify the choice of this model from a linguistic perspective as a way tocapture the multi-dimensional nature of words.
Linguists have long argued that wordshave many different features in a high dimensional space: They can be separatelydescribed by part of speech, gender, number, case, person, tense, voice, aspect, massvs.
count, and a host of semantic categories (agency, animate vs. inanimate, physical vs.abstract, etc.
), to name a few (Sag, Wasow, and Bender 2003).
In the PL-MRF, each layerof nodes is intended to represent some latent dimension of words.We represent the probability distribution for PL-MRFs as log-linear models thatdecompose over cliques in the MRF graph.
Let Cliq(x, y) represent the set of all maximalcliques in the graph of the MRF model for x and y.
Expressing the lattice model in log-linear form, we can write the marginal probability P(x) of a given sentence x as:?y?c?Cliq(x,y) score(c, x, y)?x?,y??c?Cliq(x?,y? )
score(c, x?, y?
)where score(c, x, y) = exp(?c ?
fc(xc, yc)).
Our model includes parameters for transitionsbetween two adjacent latent variables on layer j: ?transi,s,i+1,s?,j for yi,j = s and yi+1,j = s?.
Italso includes observation parameters for latent variables and tokens, as well as for pairsof adjacent latent variables in different layers and their tokens: ?obsi,j,s,w and ?obsi,j,s,j+1,s?,w foryi,j = s, yi,j+1 = s?, and xi = w.98Huang et al.
Computational LinguisticsAs with our HMM models, we create two representations from PL-MRFs, one fortokens and one for types.
For tokens, we decode the model to compute y?, the matrix ofoptimal latent state values for sentence x.
For each layer j and and each possible latentstate value k, we add a boolean feature for token xi that is true iff y?i,j = k. For wordtypes, we compute distributions over the latent state space.
Let y be a column vector oflatent variables for word type w. For a PL-MRF model with M layers of binary variables,there are 2M possible values for y.
Our type representation computes a probabilitydistribution over these 2M possible values, and uses each probability as a feature forw.3 We refer to these two representations as LATTICE-TOKEN-R and LATTICE-TYPE-R,respectively.We train the PL-MRF using contrastive estimation (Smith and Eisner 2005), whichiteratively optimizes the following objective function on a corpus X:?x?Xlog?y?c?Cliq(x,y) score(c, x, y)?x?
?N (x),y??c?Cliq(x?,y? )
score(c, x?, y?
)(7)where N (x), the neighborhood of x, indicates a set of perturbed variations of the originalsentence x. Contrastive estimation seeks to move probability mass away from the per-turbed neighborhood sentences and onto the original sentence.
We use a neighborhoodfunction that includes all sentences which can be obtained from the original sentence byswapping the order of a consecutive pair of words.
Training uses gradient descent overthis non-convex objective function with a standard software package (Liu and Nocedal1989) and converges to a local maximum or saddle point.For tractability, we modify the training procedure to train the PL-MRF one layerat a time.
Let ?i represent the set of parameters relating to features of layer i, and let?
?i represent all other parameters.
We fix ?
?0 = 0, and optimize ?0 using contrastiveestimation.
After convergence, we fix ?
?1, and optimize ?1, and so on.
For training eachlayer, we use a convergence threshold of 10?6 on the objective function in Equation (7),and each layer typically converges in under 100 iterations.4.
Domain Adaptation with Learned RepresentationsWe evaluate the representations described earlier on POS tagging and NP chunkingtasks in a domain adaptation setting.4.1 A Rich Problem Setting for Representation LearningExisting supervised NLP systems are domain-dependent: There is a substantial drop intheir performance when tested on data from a new domain.
Domain adaptation is thetask of overcoming this domain dependence.
The aim is to build an accurate system for3 This representation is only feasible for small numbers of layers, and in our experiments that require typerepresentations, we used M = 10.
For larger values of M, other representations are also possible.
We alsoexperimented with a representation which included only M possible values: For each layer l, we includedP(yl = 0|w) as a feature.
We used the less-compact representation in our experiments because resultswere better.99Computational Linguistics Volume 40, Number 1a target domain by training on labeled examples from a separate source domain.
Thisproblem is sometimes also called transfer learning (Raina et al.
2007).Two of the challenges for NLP representations, sparsity and polysemy, are exacer-bated by domain adaptation.
New domains come with new words and phrases thatappear rarely (or even not at all) in the training domain, thus increasing problemswith data sparsity.
And even for words that do appear commonly in both domains, thecontexts around the words will change from the training domain to the target domain.As a result, domain adaptation adds to the challenge of handling polysemous words,whose meaning depends on context.In short, domain adaptation is a challenging setting for testing NLP representations.We now present several experiments testing our representations against state-of-the-art POS taggers in a variety of domain adaptation settings, showing that the learnedrepresentations surpass the previous state-of-the-art, without requiring any labeled datafrom the target domain.4.2 Experimental Set-upFor domain adaptation, we test our representations on two sequence labeling tasks:POS tagging and chunking.
To incorporate learned representation into our models, wefollow this general procedure, although the details vary by experiment and are given inthe following sections.
First, we collect a set of unannotated text from both the trainingdomain and test domain.
Second, we learn representations on the unannotated text.We then automatically annotate both the training and test data with features from thelearned representation.
Finally, we train a supervised linear-chain CRF model on theannotated training set and apply it to the test set.A linear-chain CRF is a Markov random field (Darroch, Lauritzen, and Speed 1980)in which the latent variables form a path with edges only between consecutive nodes inthe path, and all latent variables are globally conditioned on the observations.
Let X be arandom variable over data sequences, and Z be a random variable over correspondinglabel sequences.
The conditional distribution over the label sequence Z given X has theformp?
(Z = z|X = x) ?
exp??
?i?j?j fj(zi?1, zi, x, i)??
(8)where fj(zi?1, zi, x, i) is a real-valued feature function of the entire observation sequenceand the labels at positions i and i ?
1 in the label sequence, and ?j is a parameter to beestimated from training data.We use an open source CRF software package designed by Sunita Sarawagi to trainand apply our CRF models.4 As is standard, we use two kinds of feature functions:transition and observation.
Transition feature functions indicate, for each pair of labelsl and l?, whether zi = l and zi?1 = l?.
Boolean observation feature functions indicate, foreach label l and each feature f provided by a representation, whether zi = l and xi hasfeature f .
For each label l and each real-valued feature f in representation R, real-valuedobservation feature functions have value f (x) if zi = l, and are zero otherwise.4 Available from http://sourceforge.net/projects/crf/.100Huang et al.
Computational Linguistics4.3 Domain Adaptation for POS TaggingOur first experiment tests the performance of all the representations we introducedearlier on an English POS tagging task, trained on newswire text, to tag biomedical re-search literature.
We follow Blitzer et al.
?s experimental set-up.
The labeled data consistsof the WSJ portion of the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993)as source domain data, and 561 labeled sentences (9,576 tokens) from the biomedicalresearch literature database MEDLINE as target domain data (PennBioIE 2005).
Fully23% of the tokens in the labeled test text are never seen in the WSJ training data.
Theunlabeled data consists of the WSJ text plus 71,306 additional sentences of MEDLINEtext (Blitzer, McDonald, and Pereira 2006).
As a preprocessing step, we replace hapaxlegomena (defined as words that appear once in our unlabeled training data) withthe special symbol *UNKNOWN*, and do the same for words in the labeled test sets thatnever appeared in any of our unlabeled training text.For representations, we tested TRAD-R, n-GRAM-R, LSA-R, NB-R, HMM-TOKEN-R, I-HMM-TOKEN-R (between 2 and 8 layers), and LATTICE-TOKEN-R (8, 12, 16,and 20 layers).
Each latent node in the I-HMMs had 80 possible values, creating808 ?
1015 possible configurations of the eight-layer I-HMM for a single word.
Eachnode in our PL-MRF is binary, creating a much smaller number (220 ?
106) of possibleconfigurations for each word in a 20-layer representation.
To give the n-gram modelthe largest training data set available, we trained it on the Web 1Tgram corpus (Brantsand Franz 2006).
We included the top 500 most common n-grams for each word type,and then used mutual information on the training data to select the top 10,000 mostrelevant n-gram features for all word types, in order to keep the number of featuresmanageable.
We incorporated n-gram features as binary values indicating whether xiappeared with the n-gram or not.
For comparison, we also report on the performance ofBrown clusters (100, 320, 1,000, and 3,200 possible clusters), following Turian, Ratinov,and Bengio (2010).
Finally, we compare against Blitzer, McDonald, and Pereira (2006)SCL technique, described in Section 2.6, and the standard semi-supervised learningalgorithm ASO (Ando and Zhang 2005), whose results on this task were previouslyreported by Blitzer, McDonald, and Pereira (2006).Table 2 shows the results for the best variation of each kind of model?20 layers forthe PL-MRF, 7 layers for the I-HMM, and 3,200 clusters for the Brown clustering.
Allstatistical language model representations outperform the TRAD-R baseline.In nearly all cases, learned representations significantly outperformed TRAD-R. Thebest representation, the 20-layer LATTICE-TOKEN-R, reduces error by 47% (35% onOOV) relative to the baseline TRAD-R, and by 44% (24% on out-of-vocabulary words(OOV)) relative to the benchmark SCL system.
For comparison, this model achieved a96.8% in-domain accuracy on Sections 22?24 of the Penn Treebank, about 0.5 percentagepoint shy of a state-of-the-art in-domain system with more sophisticated supervisedlearning (Shen, Satta, and Joshi 2007).
The BROWN-TOKEN-R representation, whichTurian, Ratinov, and Bengio (2010) demonstrated performed as well or better thana variety of neural network statistical language models as representations, achievedaccuracies between the SCL system and the HMM-TOKEN-R.
The WEB1T-n-GRAM-R,I-HMM-TOKEN-R, and LATTICE-TOKEN-R all performed quite close to one another,but the I-HMM-TOKEN-R and LATTICE-TOKEN-R were trained on many orders ofmagnitude less text.
The LSA-R and NB-R outperformed the TRAD-R baseline butnot the SCL system.
The n-GRAM-R, which was trained on the same text as theother representations except the WEB1T-n-GRAM-R, performed far worse than theWEB1T-n-GRAM-R.101Computational Linguistics Volume 40, Number 1Table 2Learned representations, and especially latent-variable statistical language modelrepresentations, significantly outperform a traditional CRF system on domain adaptation forPOS tagging.
Percent error is shown for all words and out-of-vocabulary (OOV) words.
TheSCL+500bio system was given 500 labeled training sentences from the biomedical domain.1.8% of tokens in the biomedical test set had POS tags like ?HYPHENATED?, which are notpart of the tagset for the training data, and were labeled incorrectly by all systems withoutaccess to labeled data from the biomedical domain.
As a result, an error rate of 1.8 + 3.9 = 5.7serves as a reasonable lower bound for a system that has never seen labeled examples fromthe biomedical domain.Model All words OOV wordsTRAD-R 11.7 32.7n-GRAM-R 11.7 32.2LSA-R 11.6 31.1NB-R 11.6 30.7ASO 11.6 29.1SCL 11.1 28BROWN-TOKEN-R 10.0 25.2HMM-TOKEN-R 9.5 24.8WEB1T-n-GRAM-R 6.9 24.4I-HMM-TOKEN-R 6.7 24LATTICE-TOKEN-R 6.2 21.3SCL+500bio 3.9 ?The amount of unlabeled training data has a significant impact on the performanceof these representations.
This is apparent in the difference between WEB1T-n-GRAM-R and n-GRAM-R, but it is also true for our other representations.
Figure 4 shows theaccuracy of a representative subset of our taggers on words not seen in labeled trainingdata, as we vary the amount of unlabeled training data available to the languageFigure 4Learning curve for representations: target domain accuracy of our taggers on OOV words(not seen in labeled training data), as a function of the number of unlabeled examples givento the language models.102Huang et al.
Computational Linguisticsmodels.
Performance grows steadily for all representations we measured, and noneof the learning curves appears to have peaked.
Furthermore, the margin between themore complex graphical models and the simpler n-gram models grows with increasingamounts of training data.4.3.1 Sparsity and Polysemy.
We expected that statistical language model represen-tations would perform well in part because they provide meaningful features forsparse and polysemous words.
For sparse tokens, these trends are already evidentin the results in Table 2: Models that provide a constrained number of features, likeHMM-based models, tend to outperform models that provide huge numbers of fea-tures (each of which, on average, is only sparsely observed in training data), likeTRAD-R.As for polysemy, HMM models significantly outperform naive Bayes models andthe n-GRAM-R.
The n-GRAM-R?s features do not depend on a token type?s context at all,and the NB-R?s features depend only on the tokens immediately to the right and left ofthe current word.
In contrast, the HMM takes into account all tokens in the surroundingsentence (although the strength of the dependence on more distant words decreasesrapidly).
Thus the performance of the HMM compared with n-GRAM-R and NB-R,as well as the performance of the LATTICE-TOKEN-R compared with the WEB1T-n-GRAM-R, suggests that representations that are sensitive to the context of a wordproduce better features.To test these effects more rigorously, we selected 109 polysemous word types fromour test data, along with 296 non-polysemous word types.
The set of polysemous wordtypes was selected by filtering for words in our labeled data that had at least twoPOS tags that began with distinct letters (e.g., VBZ and NNS).
An initial set of non-polysemous word types was selected by filtering for types that appeared with justone POS tag.
We then manually inspected these initial selections to remove obviouscases of word types that were in fact polysemous within a single part-of-speech, suchas ?bank.?
We further define sparse word types as those that appear five times orfewer in all of our unlabeled data, and we define non-sparse word types as those thatappear at least 50 times in our unlabeled data.
Table 3 shows our POS tagging resultson the tokens of our labeled biomedical data with word types matching these fourcategories.As expected, all of our statistical language models outperform the baseline bya larger margin on polysemous words than on non-polysemous words.
The marginbetween graphical model representations and the WEB1T-n-GRAM-R model also in-creases on polysemous words, except for the NB-R.
The WEB1T-n-GRAM-R uses noneof the local context to decide which features to provide, and the NB-R uses only theimmediate left and right context, so both models ignore most of the context.
In contrast,the remaining graphical models use Viterbi decoding to take into account all tokensin the surrounding sentence, which helps to explain their relative improvement overWEB1T-n-GRAM-R on polysemous words.The same behavior is evident for sparse words, as compared with non-sparsewords: All of the statistical language model representations outperform the baselineby a larger margin on sparse words than not-sparse words, and all of the graphicalmodels perform better relative to the WEB1T-n-GRAM-R on sparse words than not-sparse words.
By reducing the feature space from millions of possible n-gram fea-tures to L categorical features, these models ensure that each of their features willbe observed often in a reasonably sized training data set.
Thus representations based103Computational Linguistics Volume 40, Number 1Table 3Graphical models consistently outperform n-gram models by a larger margin on sparse wordsthan not-sparse words, and by a larger margin on polysemous words than not-polysemouswords.
One exception is the NB-R, which performs worse relative to WEB1T-n-GRAM-R onpolysemous words than non-polysemous words.
For each graphical model representation,we show the difference in performance between that representation and WEB1T-n-GRAM-Rin parentheses.
For each representation, differences in accuracy on polysemous andnon-polysemous subsets were statistically significant at p < 0.01 using a two-tailedFisher?s exact test.
Likewise for performance on sparse vs. non-sparse categories.polysemous not polysemous sparse not sparsetokens 159 4,321 463 12,194TRAD-R 59.5 78.5 52.5 89.6WEB1T-n-GRAM-R 68.2 85.3 61.8 94.0NB-R 64.5 88.7 57.8 89.4(-WEB1T-n-GRAM-R) (?3.7) (+3.4) (?4.0) (?4.6)HMM-TOKEN-R 67.9 83.4 60.2 91.6(-WEB1T-n-GRAM-R) (?0.3) (?1.9) (?1.6) (?2.4)I-HMM-TOKEN-R 75.6 85.2 62.9 94.5(-WEB1T-n-GRAM-R) (+7.4) (?0.1) (+1.1) (+0.5)LATTICE-TOKEN-R 70.5 86.9 65.2 94.6(-WEB1T-n-GRAM-R) (+2.3) (+1.6) (+3.4) (+0.6)on graphical models help address two key issues in building representations for POStagging.4.3.2 Domain Divergence.
Besides sparsity and polysemy, Ben-David et al.
?s (2007, 2010)theoretical analysis of domain adaptation shows that the distance between two domainsunder a representation R of the data is crucial for a good representation.
We test theirpredictions using learned representations.Ben-David et al.
?s (2007, 2010) analysis depends on a particular notion of distance,the d1 divergence, that is computationally intractable to calculate.
For our analysis, weresort instead to two different computationally efficient approximations of this measure.The first uses a more standard notion of distance: the Jensen-Shannon Divergence (dJS),a distance metric for probability distributions:dJS(p||q) = 12?i[pilog( pimi)+ qilog( qimi)]where mi =pi+qi2 .Intuitively, we aim to measure the distance between two domains by measuringwhether features appear more commonly in one domain than in the other.
For instance,the biomedical domain is far from the newswire domain under the TRAD-R repre-sentation because word-based features like protein, gene, and pathway appear far morecommonly in the biomedical domain than the newswire domain.
Likewise, bank andpresident appear far more commonly in newswire text.
Since the d1 distance is relatedto the optimal classifier for distinguishing two domains, it makes sense to measure thedistance by comparing the frequencies of these features: a classifier can easily use theoccurrence of words like bank and protein to accurately predict whether a given sentencebelongs to the newswire or biomedical domain.104Huang et al.
Computational LinguisticsMore formally, let S and T be two domains, and let f be a feature5 in representationR?that is, a dimension of the image space of R. Let V be the set of possible valuesthat f can take on.
Let US be an unlabeled sample drawn from S, and likewise forUT.
We first compute the relative frequencies of the different values of f in R(US) andR(UT ), and then compute dJS between these empirical distributions.
Let pf represent theempirical distribution over V estimated from observations of feature f in R(US), and letqf represent the same distribution estimated from R(UT ).Definition 1JS domain divergence for a feature or df (US, UT ) is the domain divergence betweendomains S and T under feature f from representation R, and is given bydf (US, UT ) = dJS(pf ||qf )For a multidimensional representation, we compute the full domain divergence as aweighted sum over the domain divergences for its features.
Because individual featuresmay vary in their relevance to a sequence-labeling task, we use weights to indicatetheir importance to the overall distance between the domains.
We set the weight wffor feature f proportional to the L1 norm of CRF parameters related to f in the trainedPOS tagger.
That is, let ?
be the CRF parameters for our trained POS tagger, and let?f = {?l,v|l be the state for zi and v be the value for f}.
We set wf =||?f ||1||?||1 .Definition 2JS Domain Divergence or dR(US, UT ), is the distance between domains S and T underrepresentation R, and is given bydR(US, UT ) =?fwf df (US, UT )Blitzer (2008) uses a different notion of domain divergence to approximate the d1divergence, which we also experimented with.
He trains a CRF classifier on exampleslabeled with a tag indicating which domain the example was drawn from.
We refer tothis type of classifier as a domain classifier.
Note that these should not be confusedwith our CRFs used for POS tagging, which take as input examples which are labeledwith POS sequences.
For the domain classifier, we tag every token from the WSJ domainas 0, and every token from the biomedical domain as 1.
Blitzer then uses the accuracyof his domain classifier on a held-out test set as his measure of domain divergence.
Ahigh accuracy for the domain classifier indicates that the representation makes the twodomains easy to separate, and thus high accuracy signifies a high domain divergence.
Tomeasure domain divergence using a domain classifier, we trained our representationson all of the unlabeled data for this task, as before.
We then used 500 randomly sampledsentences from the WSJ domain, and 500 randomly sampled biomedical sentences, andlabeled these with 0 for the WSJ data and 1 for the biomedical data.
We measuredthe error rate of our domain-classifier CRF as the average error rate across folds whenperforming three-fold cross-validation on these 1,000 sentences.5 For simplicity, the definition we provide here works only for discrete features, although it is possible toextend this definition to continuous-valued features.105Computational Linguistics Volume 40, Number 10.880.890.90.910.920.930.940.32 0.37 0.42 0.47TargetDomainTaggingAccuracyDomain Divergence under the RepresentationLATTICE-RI-HMM-RTrad-RNgram-R1 HMM7 HMMs8 layer LATTICE20 layer LATTICEFigure 5Target-domain POS tagging accuracy for a model developed using a representation R correlatesstrongly with lower JS domain divergence between WSJ and biomedical text under eachrepresentation R. The correlation coefficients r2 for the linear regressions drawn in thefigure are both greater than 0.97.Figure 5 plots the accuracies and JS domain divergences for our POS taggers.Figure 6 shows the difference between target-domain error and source-domain erroras a function of JS domain divergence.
Figures 7 and 8 show the same information,except that the x axis plots the accuracy of a domain classifier as the way of mea-suring domain divergence.
These results give empirical support to Ben-David et al.
?s(2007, 2010) theoretical analysis: Smaller domain divergence?whether measured byJS domain divergence or by the accuracy of a domain classifier?correlates stronglywith better target-domain accuracy.
Furthermore, smaller domain divergence correlatesstrongly with a smaller difference in the accuracy of the taggers on the source andtarget domains.Figure 6Smaller JS domain divergence correlates with a smaller difference between target-domain errorand source-domain error.106Huang et al.
Computational Linguistics0.880.890.90.910.920.930.940.950.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98TargetDomainTaggingAccuracyDomain Classification AccuracyTrad RepI-HMMNgramPL-MRF1 HMM7 HMMs20 layer PL-MRF8 layer PL-MRFFigure 7Target-domain tagging accuracy decreases with the accuracy of a CRF domain classifier.Intuitively, this means that training data from a source domain is less helpful for tagging ina target domain when source-domain data is easy to distinguish from target-domain data.Figure 8Better domain classification correlates with a larger difference between target-domain error andsource-domain error.Although both the JS domain divergence and the domain classifier provide onlyapproximations of the d1 metric for domain divergence, they agree very strongly:In both cases, the LATTICE-TOKEN-R representations had the lowest domain diver-gence, followed by the I-HMM-TOKEN-R representations, followed by TRAD-R, withn-GRAM-R somewhere between LATTICE-TOKEN-R and I-HMM-TOKEN-R.
The maindifference between the two metrics appears to be that the JS domain divergence givesa greater domain divergence to the eight-layer LATTICE-TOKEN-R model and then-GRAM-R, placing them past the four- through eight-layer I-HMM-TOKEN-R represen-tations.
The domain classifier places these models closer to the other LATTICE-TOKEN-Rrepresentations, just past the seven-layer I-HMM-TOKEN-R representation.107Computational Linguistics Volume 40, Number 1The domain divergences of all models, using both techniques for measuring diver-gence, remain significantly far from zero, even under the best representation.
As a result,there is ample room to experiment with even less-divergent representations of the twodomains, to see if they might yield ever-increasing target-domain accuracies.
Note thatthis is not simply a matter of adding more layers to the layered models.
The I-HMM-TOKEN-R model performed best with seven layers, and the eight-layer representationhad about the same accuracy and domain divergence as the five-layer model.
Thismay be explained by the fact that the I-HMM layers are trained independently, and soadditional layers may be duplicating other ones, and causing the supervised classifierto overfit.
But it also shows that our current methodology has no built-in techniquefor constraining the domain divergence in our representations?the decrease in domaindivergence from our more sophisticated representations is a coincidental byproduct ofour training methodology, but there is no guarantee that our current mechanisms willcontinue to decrease domain divergence simply by increasing the number of layers.
Animportant consideration for future research is to devise explicit learning mechanismsthat guide representations towards smaller domain divergences.4.4 Domain Adaptation for Noun-Phrase Chunking and Chinese POS TaggingWe test the generality of our representations by using them for other tasks, domains, andlanguages.
Here, we report on further sequence-labeling tasks in a domain adaptationsetting: noun phrase chunking for adaptation from news text to biochemistry journals,and POS tagging in Mandarin for a variety of domains.
In the next section, we describethe use of our representations in a weakly supervised information extraction task.For chunking, the training set consists of the CoNLL 2000 shared task data forsource-domain labeled data (Sections 15?18 of the WSJ portion of the Penn Treebank,labeled with chunk tags) (Tjong, Sang, and Buchholz 2000).
For test data, we usedbiochemistry journal data from the Open American National Corpus6 (OANC).
Oneof the authors manually labeled 198 randomly selected sentences (5,361 tokens) fromthe OANC biochemistry text with noun-phrase chunk information.7 We focus on nounphrase chunks because they are relatively easy to annotate manually, but contain a largevariety of open-class words that vary from domain to domain.
The labeled training setconsists of 8,936 sentences and 211,726 tokens.
Twenty-three percent of chunks in thetest set begin with an OOV word (especially adjective-noun constructions like ?aqueousformation?
and ?angular recess?
), and 29% begin with a word seen at most twice intraining data; we refer to these as OOV chunks and rare chunks.
For our unlabeleddata, we use 15,000 sentences (358,000 tokens; Sections 13?19) of the Penn Treebankand 45,000 sentences (1,083,000 tokens) from the OANC?s biochemistry section.
Wetested TRAD-R (augmented with features for automatically generated POS tags), LSA-R,n-GRAM-R, NB-R, HMM-TOKEN-R, I-HMM-TOKEN-R (7 layers, which performed bestfor POS tagging) and LATTICE-TOKEN-R (20 layers) representations.Figure 9 shows our NP chunking results for this domain adaptation task.
Theperformance improvements for the HMM-based chunkers are impressive: LATTICE-TOKEN-R reduces error by 57% with respect to TRAD-R, and comes close to state-of-the-art results for chunking on newswire text.
The results suggest that this representationallows the CRF to generalize almost as well to out-of-domain text as in-domain text.6 Available from http://www.anc.org/OANC/.7 The labeled data for this experiment are available from the first author?s Web site.108Huang et al.
Computational LinguisticsF1onBiochemistryText0.72 0.740.75 0.760.840.87 0.89 0.86 0.87 0.87 0.880.910.94 0.940.60.650.70.750.80.850.90.951Trad-R Ngram-R LSA-R NB-R HMM-R I-HMM-R LATTICE-ROOV ALLFreq: 0 1 2 allChunks: 284 39 39 1,258R P R P R P R PTRAD-R .74 .70 .85 .87 .79 .86 .86 .87n-GRAM-R .74 .74 .85 .85 .79 .86 .87 .87LSA-R .76 .74 .82 .83 .78 .85 .87 .88NB-R .73 .78 .86 .73 .86 .75 .88 .88HMM-TOKEN-R .80 .89 .92 .88 .92 .90 .91 .90I-HMM-TOKEN-R .90 .86 .92 .95 .87 .97 .95 .92LATTICE-TOKEN-R .92 .85 .94 .95 .87 .97 .95 .93Figure 9On biomedical journal data from the OANC, our best NP chunker outperforms the baselineCRF chunker by 0.17 F1 on chunks that begin with OOV words, and by 0.08 on all chunks.
Thetable shows performance breakdowns (recall and precision) for chunks whose first word hasfrequency 0, 1, and 2 in training data, and the number of chunks in test data that fall into eachof these categories.Improvements are greatest on OOV and rare chunks, where LATTICE-TOKEN-R madeabsolute improvements over TRAD-R by 0.17 and 0.09 F1, respectively.
Improvementsfor the single-layer HMM-TOKEN-R were smaller but still significant: 36% relative re-duction in error overall, and 32% for OOV chunks.The improved performance from our HMM-based chunker caused us to wonderhow well the chunker could work without some of its other features.
We removed alltag features and orthographic features and all features for word types that appear fewerthan 20 times in training.
This chunker still achieves 0.91 F1 on OANC data, and 0.93F1 on WSJ data (Section 20), outperforming the TRAD-R system in both cases.
It hasonly 20% as many features as the baseline chunker, greatly improving its training time.Thus these features are more valuable to the chunker than features from automaticallyproduced tags and features for all but the most common words.For Chinese POS tagging, we use text from the UCLA Corpus of Written Chinese(Tao and Xiao 2007), which is part of the Lancaster Corpus of Mandarin Chinese(LCMC).
The UCLA Corpus consists of 11,192 sentences of word-segmented and POS-tagged text in 13 genres (see Table 4).
We use gold-standard word segmentation labelsfor training and testing.
The LCMC tagset consists of 50 Chinese POS tags.
On average,each genre contains 5,284 word tokens, for a total of 68,695 tokens among all genres.
Weuse the ?news?
genre as our source domain, which we use for training and development109Computational Linguistics Volume 40, Number 1Table 4POS tagging accuracy: the LATTICE-TOKEN-R and other graphical model representationsoutperform TRAD-R and state-of-the-art Chinese POS taggers on all target domains.
For targetdomains, * indicates the performance is statistically significantly better than the Stanford andTRAD-R baselines at p < 0.05, using a two-tailed ?2 test; ** indicates significance at p < 0.01.On the news domain, the Stanford tagger is significantly different from all other systemsusing a two-tailed ?2 test with p < 0.01.Domain Stanford TRAD NGR LSA NB HMM I-H LATlore 88.4 84.0 84.2 85.3 85.3 89.7 89.9 90.1*religion 83.5 79.1 79.4 79.8 80.0 85.2 85.6 85.9*humour 89.0 84.2 84.5 86.2 86.8 89.6 89.6 89.9*general-fic 87.5 84.5 85.0 85.3 85.7 89.4 89.7 89.9*essay 88.4 83.2 83.7 84.0 84.3 89.0 89.1 90.1*mystery 87.4 82.4 83.4 84.3 85.3 90.1 91.1 91.3**romance 87.5 84.2 84.5 85.3 86.1 89.0 89.5 89.8**science-fic 88.6 82.1 82.5 83.0 83.0 87.0 88.3 88.6skills 82.7 77.3 77.7 78.2 78.4 84.9 85.0 85.1**science 86.0 82.0 82.3 82.4 82.4 87.8 87.8 87.9*adventure-fic 82.1 74.3 75.2 76.1 77.8 81.7 82.0 82.2report 91.7 84.2 85.1 85.3 86.1 91.9 91.9 91.9news 98.8** 96.9 92.3 93.4 94.3 94.2 97.0 97.1all but news 87.0 81.2 82.0 82.8 83.6 88.1 88.4 88.8**all domains 88.7 83.2 83.6 84.4 85.5 89.5 89.7 90.0**data.
For test data, we randomly select 20% of every other genre.
For our unlabeleddata, we use all of the ?news?
text, plus the remaining 80% of the texts from the othergenres.
As before, we replace hapax legomena in the unlabeled data with the specialsymbol *UNKNOWN*, and do the same for word types in the labeled test sets that neverappear in our unlabeled training texts.
We compare against a state-of-the-art ChinesePOS tagger for in-domain text, the CRF-based Stanford tagger (Tseng, Jurafsky, andManning 2005).
We obtained the code for this tagger,8 and retrained it on our trainingdata set.The Chinese POS tagging results are shown in Table 4.
The LATTICE-TOKEN-Routperforms the state-of-the-art Stanford tagger on all target domains.
Overall, on allout-of-domain tests, LATTICE-TOKEN-R provides a relative reduction in error of 13.8%compared with the Stanford tagger.
The best performance is on the ?mystery?
domain,where the LATTICE-TOKEN-R model reaches 91.3% accuracy, a 3.9 percentage pointsimprovement over the Stanford tagger.
Its performance on the in-domain ?news?
test setis significantly worse (1.7 percentage points) than the Stanford tagger, suggesting thatthe Stanford tagger relies on domain-dependent features that are helpful for taggingnews, but not for tagging in general.
The LATTICE-TOKEN-R?s accuracy is still signifi-cantly worse on out-of-domain text than in-domain text, but the gap between the two(8.3 percentage points) is better than the gap for the Stanford tagger (11.8 percentagepoints).
We believe that the lower out-of-domain performance of our Chinese POStagger, compared with our English POS tagger and our chunker, was at least in partdue to having far less unlabeled text available for this task.8 Available at http://nlp.stanford.edu/software/tagger.shtml.110Huang et al.
Computational Linguistics5.
Information Extraction ExperimentsIn this section, we evaluate our learned representations on their ability to capturesemantic, rather than syntactic, information.
Specifically, we investigate a set-expansiontask in which we?re given a corpus and a few ?seed?
noun phrases from a semanticcategory (e.g., Superheroes), and our goal is to identify other examples of the categoryin the corpus.
This is a different type of weakly supervised task from the earlier domainadaptation tasks because we are given only a handful of positive examples from a cate-gory, rather than a large sample of positively and negatively labeled training examplesfrom a separate domain.Existing set-expansion techniques utilize the distributional hypothesis: Candidatenoun phrases for a given semantic class are ranked based on how similar their contex-tual distributions are to those of the seeds.
Here, we measure how performance on theset-expansion task varies when we employ different representations for the contextualdistributions.5.1 MethodsThe set-expansion task we address is formalized as follows.
Given a corpus, a set ofseeds from some semantic category C, and a separate set of candidate phrases P, outputa ranking of the phrases in P in decreasing order of likelihood of membership in thesemantic category C.For any given representation R, the set-expansion algorithm we investigate isstraightforward: We rank candidate phrases in increasing order of the distance betweentheir feature vectors and those of the seeds.
The particular distance metrics utilized aredetailed subsequently.Because set expansion is performed at the level of word types rather than to-kens, it requires type-based representations.
We compare HMM-TYPE-R, n-GRAM-R,LATTICE-TYPE-R, and BROWN-TYPE-R in this experiment.
We used a 25-state HMM,and the LATTICE-TYPE-R as described in the previous section.
Following previous set-expansion experiments with n-grams (Ahuja and Downey 2010), we use a trigrammodel with Kneser-Ney smoothing for n-GRAM-R.The distances between the candidate phrases and the seeds for HMM-TYPE-R,n-GRAM-R, and LATTICE-TYPE-R representations are calculated by first creating aprototypical ?seed feature vector?
equal to the mean of the feature vectors for eachof the seeds in the given representation.
Then, we rank candidate phrases in orderof increasing distance between their feature vector and the seed feature vector.
As adistance measure between vectors (in this case, probability distributions), we computethe average of five standard distance measures, including KL and JS divergence, andcosine, Euclidean, and L1 distance.
In experiments, we found that improving uponthis simple averaging was not easy?in fact, tuning a weighted average of the distancemeasures for each representation did not improve results significantly on held-out data.For Brown clusters, we use prefixes of all possible lengths as features.
We definethe similarity between two Brown representation feature vectors to be the number offeatures they share in common (this is equivalent to the length of the longest commonprefix between the two original Brown cluster labels).
The candidate phrases are thenranked in decreasing order of the sum of their similarity scores to each of the seeds.
Weexperimented with normalizing the similarity scores by the longer of the two vectorlengths, and found this to decrease results slightly.
We use unnormalized (integer)similarity scores for Brown clusters in our experiments.111Computational Linguistics Volume 40, Number 15.2 Data SetsWe utilized a set of approximately 100,000 sentences of Web text, joining multi-wordnamed entities in the corpus into single tokens using the Lex algorithm (Downey,Broadhead, and Etzioni 2007).
This process enables each named entity (the focus of theset-expansion experiments) to be treated as a single token, with a single representationvector for comparison.
We developed all word type representations using this corpus.To obtain examples of multiple semantic categories, we utilized selected Wikipedia?listOf?
pages from Pantel et al.
(2009) and augmented these with our own manuallydefined categories, such that each list contained at least ten distinct examples occurringin our corpus.
In all, we had 432 examples across 16 distinct categories such as Coun-tries, Greek Islands, and Police TV Dramas.5.3 ResultsFor each semantic category, we tested five different random selections of five seedexamples, treating the unselected members of the category as positive examples, andall other candidate phrases as negative examples.
We evaluate using the area under theprecision-recall curve (AUC) metric.The results are shown in Table 5.
All representations improve performance overa random baseline, equal to the average AUC over five random orderings for eachcategory, and the graphical models outperform the n-gram representation.
I-HMM-TYPE-R and Brown clustering in the particular case of 1,000 clusters perform best, withHMM-TYPE-R performing nearly as well.
Brown clusters give somewhat lower resultsas the number of clusters varies.As with POS tagging, we expect that language model representations improveperformance on the IE task by providing informative features for sparse word types.However, because the IE task classifies word types rather than tokens, we expect the rep-resentations to provide less benefit for polysemous word types.
To test these hypotheses,we measured how IE performance changed in sparse or polysemous settings.
We identi-fied polysemous categories as those for which fewer than 90% of the category membershad the category as a clear dominant sense (estimated manually); other categories wereconsidered non-polysemous.
Categories whose members had a median number ofoccurrences in the corpus of less than 30 were deemed sparse, and others non-sparse.Table 5I-HMM-TYPE-R outperforms the other methods, improving performance over a randombaseline by twice as much as either n-GRAM-R or LATTICE-TYPE-R.model AUCI-HMM-TYPE-R 0.18HMM-TYPE-R 0.17BROWN-TYPE-R-3200 0.16BROWN-TYPE-R-1000 0.18BROWN-TYPE-R-320 0.15BROWN-TYPE-R-100 0.13LATTICE-TYPE-R 0.11n-GRAM-R baseline 0.10Random baseline 0.10112Huang et al.
Computational LinguisticsTable 6Graphical models as representations for IE consistently perform better relative to n-gram modelson sparse words, but not necessarily polysemous words.polysemous not-polysemous sparse not-sparsetypes 222 210 266 166categs.
12 4 13 3n-GRAM-R 0.07 0.17 0.06 0.25LATTICE-TYPE-R 0.09 0.15 0.1 0.19-n-GRAM-R +0.02 ?0.02 +0.04 ?0.06HMM-TYPE-R 0.14 0.26 0.15 0.32-n-GRAM-R +0.07 +0.09 +0.09 +0.07IE performance on these subsets of the data are shown in Table 6.
Both graphicalmodel representations outperform the n-gram representation more on sparse words, asexpected.
For polysemy, the picture is mixed: The LATTICE-TYPE-R outperformsn-GRAM-R on polysemous categories, whereas HMM-TYPE-R?s performance advan-tage over n-GRAM-R decreases.One surprise on the IE task is that the LATTICE-TYPE-R performs significantly lesswell than the HMM-TYPE-R, whereas the reverse is true on POS tagging.
We suspectthat the difference is due to the issue of classifying types vs. tokens.
Because of theirmore complex structure, PL-MRFs tend to depend more on transition parameters thando HMMs.
Furthermore, our decision to train the PL-MRFs using contrastive estimationwith a neighborhood that swaps consecutive pairs of words also tends to emphasizetransition parameters.
As a result, we believe the posterior distribution over latent statesgiven a word type is more informative in our HMM model than the PL-MRF model.We measured the entropy of these distributions for the two models, and found thatH(PPL-MRF(y|x = w)) = 9.95 bits, compared with H(PHMM(y|x = w)) = 2.74 bits, whichsupports the hypothesis that the drop in the PL-MRF?s performance on IE is due to itsdependence on transition parameters.
Further experiments are warranted to investigatethis issue.5.4 Testing the Language Model Representation Hypothesis in IEThe language model representation hypothesis (Section 2) suggests that all else beingequal, more accurate language models will provide features that lead to better perfor-mance on NLP tasks.
Here, we test this hypothesis on the set expansion IE task.Figures 10 and 11 show how the performance of the HMM-TYPE-R varies with thelanguage modeling accuracy of the underlying HMM.
Language modeling accuracyis measured in terms of perplexity on held-out text.
Here, we use set expansion datasets from previous work (Ahuja and Downey 2010).
The first two are composed ofextractions from the TextRunner information extraction system (Banko et al.
2007) andare denoted as Unary (361 examples) and Binary (265 examples).
The second, Wikipedia(2,264 examples), is a sample of Wikipedia concept names.
We evaluate the performanceof several different trained HMMs with numbers of latent states K ranging from 5 to1,600 (to help illustrate how IE and LM performance varies even when model capacityis fixed, we include three distinct models with K = 100 states trained separately overthe full corpus).
We used a distributed implementation of HMM training and corpus113Computational Linguistics Volume 40, Number 1K = 5K = 10K = 25 K = 50K = 100K = 100K = 100K = 200K = 400Figure 10Information extraction (IE) performance of HMM-TYPE-R as the language modeling accuracy ofthe HMM varies, on TextRunner data sets.
IE accuracy (in terms of area under the precision-recallcurve) tends to increase as language modeling accuracy improves (i.e., perplexity decreases).5 1025 50100100100200 40055252550501002001002008001600 400Figure 11Information extraction (IE) performance of HMM-TYPE-R as the language modeling accuracyof the HMM varies on the Wikipedia data set.
Number labels indicate the number of latentstates K, and performance is shown for three training corpus sizes (the full corpus consists ofapproximately 60 million tokens).
IE accuracy (in terms of area under the precision-recall curve)tends to increase as language modeling accuracy improves (i.e., perplexity decreases).partitioning techniques (Yang, Yates, and Downey 2013) to enable training of our largercapacity HMM models on large data sets.The results provide support for the language model representation hypothesis,showing that IE performance does tend to improve as language model perplexitydecreases.
On the smaller Unary and Binary sets (Figure 10), although IE accuracy114Huang et al.
Computational Linguisticsdoes decrease for the lowest-perplexity models, overall language model perplexityexhibits a negative correlation with IE area under the precision-recall curve (the Pearsoncorrelation coefficient is ?0.18 for Unary, and ?0.28 for Binary).
For Wikipedia (Fig-ure 11), the trend is more consistent, with IE performance increasing monotonicallyas perplexity decreases for models trained on the full training corpus (the Pearsoncorrelation coefficient is ?0.90).Figure 11 also illustrates how LM and IE performance changes as the amountof training text varies.
In general, increasing the training corpus size increases IEperformance and decreases perplexity.
Over all data points in the figure, IE perfor-mance correlates most strongly with model perplexity (?0.68 Pearson correlation, ?0.88Spearman correlation), followed by corpus size (0.66, 0.71) and model capacity (?0.05,0.38).
The small negative Pearson correlation between model capacity and IE perfor-mance is primarily due to the model with 1,600 states trained on 4% of the corpus.This model has a large parameter space and sparse training data, and thus suffers fromoverfitting in terms of both model perplexity and IE performance.
If we ignore thisoverfit model, the Pearson correlation between model capacity and IE performance forthe other models in the Figure is 0.24.Our results show that IE based on distributional similarity tends to improve as thequality of the latent variable model used to measure distributional similarity improves.A similar trend was exhibited in our previous work (Ahuja and Downey 2010); here, weextend the previous results to models with more latent states and a larger, more reliabletest set (Wikipedia).
The results suggest that scaling up the training of latent variablemodels to utilize larger training corpora and more latent states may be a promisingdirection for improving IE capabilities.6.
Conclusion and Future WorkOur study of representation learning demonstrates that by using statistical languagemodels to aggregate information across many unannotated examples, it is possible tofind accurate distributional representations that can provide highly informative featuresto weakly supervised sequence labelers and named-entity classifiers.
For both domainadaptation and weakly supervised set expansion, our results indicate that graphicalmodels outperform n-gram models as representations, in part for their greater ability tohandle sparsity and polysemy.
Our IE task provides important evidence to support theLanguage Model Representation Hypothesis, showing that the AUC of the IE systemcorrelates more with language model perplexity than the size of the training data orthe capacity of the language model.
Finally, our sequence labeling experiments provideempirical evidence in support of theoretical work on domain adaptation, showing thattarget-domain tagging accuracy is highly correlated with two different measures ofdomain divergence.Representation learning remains a promising area for finding further improve-ments in various NLP tasks.
The representations we have described are trained inan unsupervised fashion, so a natural extension is to investigate supervised or semi-supervised representation-learning techniques.
As mentioned previously, our currenttechniques have no built-in methods for enforcing that they provide similar features indifferent domains; devising a mechanism that enforces this could allow for less domain-divergent and potentially more accurate representations.
We have considered sequencelabeling, but another promising direction is to apply these techniques to more complexstructured prediction tasks, like parsing or relation extraction.
Our current approachto sequence labeling requires retraining of a CRF for every new domain; incremental115Computational Linguistics Volume 40, Number 1retraining techniques for new domains would speed up the process.
Finally, modelsthat combine our representation learning approach with instance weighting and otherforms of supervised domain adaptation may take better advantage of labeled data intarget domains, when it is available.AcknowledgmentsThis material is based on work supportedby the National Science Foundation undergrant no.
IIS-1065397.ReferencesAhuja, Arun and Doug Downey.
2010.Improved extraction assessment throughbetter language models.
In Proceedings ofthe Annual Meeting of the North AmericanChapter of the Association of ComputationalLinguistics (NAACL-HLT), pages 225?228,Los Angeles, CA.Ando, Rie Kubota and Tong Zhang.
2005.A high-performance semi-supervisedlearning method for text chunking.In Proceedings of the ACL, pages 1?9,Ann Arbor, MI.Banko, Michele, Michael J. Cafarella,Stephen Soderland, Matt Broadhead, andOren Etzioni.
2007.
Open informationextraction from the web.
In Proceedings ofthe IJCAI, pages 2670?2676, Hyderabad.Banko, Michele and Robert C. Moore.2004.
Part of speech tagging in context.In Proceedings of the COLING, pages556?561, Geneva.Ben-David, Shai, John Blitzer, KobyCrammer, Alex Kulesza, Fernando Pereira,and Jenn Wortman.
2010.
A theory oflearning from different domains.
MachineLearning, 79:151?175.Ben-David, Shai, John Blitzer, KobyCrammer, and Fernando Pereira.
2007.Analysis of representations for domainadaptation.
In Advances in NeuralInformation Processing Systems 20,pages 127?144, Vancouver.Bengio, Yoshua.
2008.
Neural net languagemodels.
Scholarpedia, 3(1):3,881.Bengio, Yoshua, Re?jean Ducharme, PascalVincent, and Christian Janvin.
2003.A neural probabilistic language model.Journal of Machine Learning Research,3:1,137?1,155.Bengio, Yoshua, Jerome Louradour,Ronan Collobert, and Jason Weston.2009.
Curriculum learning.
In Proceedingsof the International Conference on MachineLearning (ICML), pages 41?48,Montreal.Bikel, Daniel M. 2004a.
A distributionalanalysis of a lexicalized statisticalparsing model.
In Proceedings of theConference on Empirical Methods in NaturalLanguage Processing, pages 182?189,Barcelona.Bikel, Daniel M. 2004b.
Intricacies of Collins?parsing model.
Computational Linguistics,30(4):479?511.Blei, David M., Andrew Y. Ng, and Michael I.Jordan.
2003.
Latent Dirichlet allocation.Journal of Machine Learning Research,3:993?1,022.Blitzer, John.
2008.
Domain Adaptation ofNatural Language Processing Systems.Ph.D.
thesis, University of Pennsylvania,Philadelphia, PA.Blitzer, John, Koby Crammer, Alex Kulesza,Fernando Pereira, and Jenn Wortman.2007.
Learning bounds for domainadaptation.
In Advances in NeuralInformation Processing Systems,pages 129?136, Vancouver.Blitzer, John, Mark Dredze, and FernandoPereira.
2007.
Biographies, Bollywood,boom-boxes and blenders: Domainadaptation for sentiment classification.In Association for Computational Linguistics(ACL), pages 40?47, Prague.Blitzer, John, Ryan McDonald, andFernando Pereira.
2006.
Domainadaptation with structural correspondencelearning.
In Proceedings of the EMNLP,pages 120?128, Sydney.Bodlaender, Hans L. 1988.
Dynamicprogramming on graphs with boundedtreewidth.
In Proceedings of the 15thInternational Colloquium on Automata,Languages and Programming,pages 105?118, Tampere.Brants, Thorsten and Alex Franz.
2006.Web 1t 5-gram version 1. www.ldc.upenn.edu/catalog/.Brown, Peter F., Vincent J. Della Pietra,Peter V. deSouza, Jenifer C. Lai, andRobert L. Mercer.
1992.
Class-basedn-gram models of natural language.Computational Linguistics, 18:467?479.Candito, Marie and Benoit Crabbe.
2009.Improving generative statistical parsingwith semi-supervised word clustering.In Proceedings of the IWPT, pages 138?141,Paris.116Huang et al.
Computational LinguisticsChan, Yee Seng and Hwee Tou Ng.
2006.Estimating class priors in domainadaptation for word sense disambiguation.In Proceedings of the Association forComputational Linguistics (ACL),pages 89?96, Sydney.Chelba, Ciprian and Alex Acero.
2004.Adaptation of maximum entropyclassifier: Little data can help a lot.In Proceedings of the EMNLP,pages 285?292, Barcelona.Collobert, Robert and Jason Weston.
2008.
Aunified architecture for natural languageprocessing: Deep neural networks withmultitask learning.
In Proceedings of theInternational Conference on Machine Learning(ICML), pages 160?167, Helsinki.Dai, Wenyuan, Gui-Rong Xue, Qiang Yang,and Yong Yu.
2007.
Transferring naiveBayes classifiers for text classification.In Proceedings of the National Conferenceon Artificial Intelligence (AAAI),pages 540?545, Vancouver.Darroch, J. N., S. L. Lauritzen, andT.
P. Speed.
1980.
Markov fields andlog-linear interaction models forcontingency tables.
The Annals ofStatistics, 8(3):522?539.Daume?
III, Hal.
2007.
Frustratingly easydomain adaptation.
In Proceedings of theACL, pages 256?263, Prague.Daume?
III, Hal, Abhishek Kumar, andAvishek Saha.
2010.
Frustratingly easysemi-supervised domain adaptation.In Proceedings of the ACL Workshopon Domain Adaptation (DANLP),pages 53?59, Uppsala.Daume?
III, Hal and Daniel Marcu.
2006.Domain adaptation for statisticalclassifiers.
Journal of Artificial IntelligenceResearch, 26:101?126.Deerwester, Scott, Susan T. Dumais,George W. Furnas, Thomas K. Landauer,and Richard Harshman.
1990.
Indexing bylatent semantic analysis.
Journal of theAmerican Society of Information Science,41(6):391?407.Dempster, Arthur, Nan Laird, and DonaldRubin.
1977.
Likelihood from incompletedata via the EM algorithm.
Journal ofthe Royal Statistical Society, Series B,39(1):1?38.Deschacht, Koen and Marie-Francine Moens.2009.
Semi-supervised semantic rolelabeling using the latent words languagemodel.
In Proceedings of the Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP), pages 21?29,Singapore.Dhillon, Paramveer S., Dean Foster, andLyle Ungar.
2011.
Multi-View Learning ofWord Embeddings via CCA.
In Proceedingsof the Advances in Neural InformationProcessing Systems (NIPS), volume 24,pages 886?874, Granada.Downey, Doug, Matthew Broadhead, andOren Etzioni.
2007.
Locating complexnamed entities in web text.
In Proceedingsof the 20th International Joint Conferenceon Artificial Intelligence (IJCAI 2007),pages 2,733?2,739, Hyderabad.Downey, Doug, Stefan Schoenmackers, andOren Etzioni.
2007.
Sparse informationextraction: Unsupervised language modelsto the rescue.
In Proceedings of the ACL,pages 696?703, Prague.Dredze, Mark and Koby Crammer.
2008.Online methods for multi-domain learningand adaptation.
In Proceedings of EMNLP,pages 689?697, Honolulu, HI.Dredze, Mark, Alex Kulesza, and KobyCrammer.
2010.
Multi-domain learningby confidence weighted parametercombination.
Machine Learning,79:123?149.Finkel, Jenny Rose and Christopher D.Manning.
2009.
Hierarchical Bayesiandomain adaptation.
In Proceedings ofHLT-NAACL, pages 602?610, Boulder, CO.Fu?rstenau, Hagen and Mirella Lapata.
2009.Semi-supervised semantic role labeling.In Proceedings of the 12th Conference of theEuropean Chapter of the ACL, pages 220?228,Athens.Ghahramani, Zoubin and Michael I. Jordan.1997.
Factorial hidden Markov models.Machine Learning, 29(2-3):245?273.Gildea, Daniel.
2001.
Corpus variation andparser performance.
In Conference onEmpirical Methods in Natural LanguageProcessing, pages 167?202, Pittsburgh, PA.Goldwater, Sharon and Thomas L. Griffiths.2007.
A fully Bayesian approach tounsupervised part-of-speech tagging.In Proceedings of the ACL, pages 744?751,Prague.Grac?a, Joa?o V., Kuzman Ganchev, Ben Taskar,and Fernando Pereira.
2009.
Posterior vs.parameter sparsity in latent variablemodels.
In Proceedings of the NeuralInformation Processing Systems Conference(NIPS), pages 664?672, Vancouver.Harris, Z.
1954.
Distributional structure.Word, 10(23):146?162.Hindle, Donald.
1990.
Noun classificationfrom predicage-argument structures.In Proceedings of the ACL, pages 268?275,Pittsburgh, PA.117Computational Linguistics Volume 40, Number 1Honkela, Timo.
1997.
Self-organizingmaps of words for natural languageprocessing applications.
In Proceedingsof the International ICSC Symposium onSoft Computing, pages 401?407, Millet,Alberta.Huang, Fei and Alexander Yates.
2009.Distributional representations forhandling sparsity in supervised sequencelabeling.
In Proceedings of the AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 495?503,Singapore.Huang, Fei and Alexander Yates.
2010.Exploring representation-learningapproaches to domain adaptation.
InProceedings of the ACL 2010 Workshop onDomain Adaptation for Natural LanguageProcessing (DANLP), pages 23?30, Uppsala.Huang, Fei, Alexander Yates, Arun Ahuja,and Doug Downey.
2011.
Languagemodels as representations for weaklysupervised NLP tasks.
In Proceedingsof the Conference on Natural LanguageLearning (CoNLL), pages 125?134,Portland, OR.Jiang, Jing and ChengXiang Zhai.
2007a.Instance weighting for domainadaptation in NLP.
In Proceedingsof ACL, pages 264?271, Prague.Jiang, Jing and ChengXiang Zhai.
2007b.
Atwo-stage approach to domain adaptationfor statistical classifiers.
In Proceedings ofthe Conference on Information and KnowledgeManagement (CIKM), pages 401?410, Lisbon.Johnson, Mark.
2007.
Why doesn?t EM findgood HMM POS-taggers.
In Proceedings ofthe EMNLP, pages 296?305, Prague.Kaski, S. 1998.
Dimensionality reductionby random mapping: Fast similaritycomputation for clustering.
InProceedings of the IJCNN, pages 413?418,Washington, DC.Koo, Terry, Xavier Carreras, and MichaelCollins.
2008.
Simple semi-superviseddependency parsing.
In Proceedings ofthe Annual Meeting of the Association ofComputational Linguistics (ACL),pages 595?603, Columbus, OH.Lin, Dekang and Xiaoyun Wu.
2009.
Phraseclustering for discriminative learning.In Proceedings of the ACL-IJCNLP,pages 1,030?1,038, Singapore.Liu, Dong C. and Jorge Nocedal.
1989.
Onthe limited memory method for large scaleoptimization.
Mathematical Programming B,45(3):503?528.Mansour, Y., M. Mohri, andA.
Rostamizadeh.
2009.
Domainadaptation with multiple sources.In Proceedings of the Advances in NeuralInformation Processing Systems,pages 1,041?1,048, Vancouver.Marcus, Mitchell P., Mary AnnMarcinkiewicz, and Beatrice Santorini.1993.
Building a large annotated corpus ofEnglish: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Martin, Sven, Jorg Liermann, and HermannNey.
1998.
Algorithms for bigram andtrigram word clustering.
SpeechCommunication, 24:19?37.McClosky, David.
2010.
Any Domain Parsing:Automatic Domain Adaptation for Parsing.Ph.D.
thesis, Brown University,Providence, RI.McClosky, David, Eugene Charniak, andMark Johnson.
2010.
Automatic domainadaptation for parsing.
In North AmericanChapter of the Association for ComputationalLinguistics - Human Language Technologies2010 Conference (NAACL-HLT 2010),pages 28?36, Los Angeles, CA.Miller, Scott, Jethran Guinness, andAlex Zamanian.
2004.
Name tagging withword clusters and discriminative training.In Proceedings of the Annual Meeting of theNorth American Chapter of the Association ofComputational Linguistics (HLT-NAACL),pages 337?342, Boston, MA.Mnih, Andriy and Geoffrey Hinton.
2007.Three new graphical models for statisticallanguage modelling.
In Proceedings ofthe 24th International Conference onMachine Learning, pages 641?648,Corvallis, OR.Mnih, Andriy and Geoffrey Hinton.
2009.A scalable hierarchical distributedlanguage model.
In Proceedings of theNeural Information Processing Systems(NIPS), pages 1,081?1,088, Vancouver.Mnih, Andriy, Zhang Yuecheng, andGeoffrey Hinton.
2009.
Improving astatistical language model throughnon-linear prediction.
Neurocomputing,72(7-9):1414?1418.Morin, Frederic and Yoshua Bengio.
2005.Hierarchical probabilistic neural networklanguage model.
In Proceedings of theInternational Workshop on ArtificialIntelligence and Statistics, pages 246?252,Barbados.Pantel, Patrick, Eric Crestan, ArkadyBorkovsky, Ana-Maria Popescu,and Vishnu Vyas.
2009.
Web-scaledistributional similarity and entity setexpansion.
In Proceedings of the EMNLP,pages 938?947, Singapore.118Huang et al.
Computational LinguisticsPennBioIE.
2005.
Mining the bibliomeproject.
http://bioie.ldc.upenn.edu/.Pereira, Fernando, Naftali Tishby, andLillian Lee.
1993.
Distributional clusteringof English words.
In Proceedings of theAnnual Meeting of the Association forComputational Linguistics (ACL),pages 183?190, Columbus, OH.Pradhan, Sameer, Wayne Ward, and James H.Martin.
2007.
Towards robust semantic rolelabeling.
In Proceedings of NAACL-HLT,pages 556?563, Rochester, NY.Rabiner, Lawrence R. 1989.
A tutorial onhidden Markov models and selectedapplications in speech recognition.Proceedings of the IEEE, 77(2):257?285.Raina, Rajat, Alexis Battle, Honglak Lee,Benjamin Packer, and Andrew Y. Ng.2007.
Self-taught learning: Transferlearning from unlabeled data.In Proceedings of the 24th InternationalConference on Machine Learning,pages 759?766, Corvallis, OR.Ratinov, Lev and Dan Roth.
2009.
Designchallenges and misconceptions in namedentity recognition.
In Proceedings of theConference on Natural Language Learning(CoNLL), pages 147?155, Boulder, CO.Ritter, H. and T. Kohonen.
1989.Self-organizing semantic maps.Biological Cybernetics, 61(4):241?254.Sag, Ivan A., Thomas Wasow, and Emily M.Bender.
2003.
Synactic Theory: A FormalIntroduction.
CSLI Publications, Stanford,CA, second edition.Sahlgren, Magnus.
2001.
Vector-basedsemantic analysis: Representing wordmeanings based on random labels.In Proceedings of the Semantic KnowledgeAcquisition and Categorization Workshop,pages 1?12, Helsinki.Sahlgren, Magnus.
2005.
An introductionto random indexing.
In Methods andApplications of Semantic Indexing Workshopat the 7th International Conference onTerminology and Knowledge Engineering(TKE), 87:1?9.Sahlgren, Magnus.
2006.
The word-spacemodel: Using distributional analysis torepresent syntagmatic and paradigmaticrelations between words in high-dimensionalvector spaces.
Ph.D. thesis, StockholmUniversity.Salton, Gerard and Michael J. McGill.
1983.Introduction to Modern Information Retrieval.McGraw-Hill.Satpal, Sandeep and Sunita Sarawagi.2007.
Domain adaptation of conditionalprobability models via feature subsetting.In Proceedings of ECML/PKDD,pages 224?235, Warsaw.Sekine, Satoshi.
1997.
The domaindependence of parsing.
In Proceedings ofApplied Natural Language Processing(ANLP), pages 96?102, Washington, DC.Shen, Libin, Giorgio Satta, and Aravind K.Joshi.
2007.
Guided learning forbidirectional sequence classification.In Proceedings of the ACL, pages 760?767,Prague.Smith, Noah A. and Jason Eisner.
2005.Contrastive estimation: Traininglog-linear models on unlabeled data.In Proceedings of the 43rd Annual Meetingof the Association for ComputationalLinguistics (ACL), pages 354?362,Ann Arbor, MI.Sutton, Charles, Andrew McCallum, andKhashayar Rohanimanesh.
2007.
Dynamicconditional random fields: Factorizedprobabilistic models for labeling andsegmenting sequence data.
Journal ofMachine Learning Research, 8:693?723.Suzuki, Jun and Hideki Isozaki.
2008.Semi-supervised sequential labeling andsegmentation using giga-word scaleunlabeled data.
In Proceedings of theAnnual Meeting of the Association forComputational Linguistics (ACL-HLT),pages 665?673, Columbus, OH.Suzuki, Jun, Hideki Isozaki, Xavier Carreras,and Michael Collins.
2009.
An empiricalstudy of semi-supervised structuredconditional models for dependencyparsing.
In Proceedings of the EMNLP,pages 551?560, Singapore.Tao, Hongyin and Richard Xiao.
2007.The UCLA Chinese corpus.
UCREL.www.lancaster.ac.uk/fass/projects/corpus/UCLA/.Tjong, Erik F., Kim Sang, and SabineBuchholz.
2000.
Introduction to theCoNLL-2000 shared task: Chunking.In Proceedings of the 4th Conferenceon Computational Natural LanguageLearning, pages 127?132, Lisbon.Toutanova, Kristina and Mark Johnson.2007.
A Bayesian LDA-based model forsemi-supervised part-of-speechtagging.
In Proceedings of the NIPS,pages 1,521?1,528, Vancouver.Tseng, Huihsin, Daniel Jurafsky, andChristopher Manning.
2005.Morphological features help POStagging of unknown words acrosslanguage varieties.
In Proceedingsof the Fourth SIGHAN Workshop,pages 32?39, Jeju Island.119Computational Linguistics Volume 40, Number 1Turian, Joseph, James Bergstra, andYoshua Bengio.
2009.
Quadraticfeatures and deep architectures forchunking.
In Proceedings of the NorthAmerican Chapter of the Association forComputational Linguistics - HumanLanguage Technologies (NAACL HLT),pages 245?248, Boulder, CO.Turian, Joseph, Lev Ratinov, and YoshuaBengio.
2010.
Word representations:A simple and general method forsemi-supervised learning.
In Proceedingsof the Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 384?394, Uppsala.Turney, Peter D. and Patrick Pantel.
2010.From frequency to meaning: Vectorspace models of semantics.
Journal ofArtificial Intelligence Research, 37:141?188.Ushioda, Akira.
1996.
Hierarchical clusteringof words.
In Proceedings of the InternationalConference on Computational Linguistics(COLING), pages 1,159?1,162, Copenhagen.Va?yrynen, Jaakko and Timo Honkela.
2004.Word category maps based on emergentfeatures created by ICA.
In Proceedings ofthe STePs 2004 Cognition + CyberneticsSymposium, pages 173?185, Tikkurila.Va?yrynen, Jaakko and Timo Honkela.
2005.Comparison of independent componentanalysis and singular value decompositionin word context analysis.
In Proceedingsof the International and InterdisciplinaryConference on Adaptive KnowledgeRepresentation and Reasoning (AKRR),pages 135?140, Espoo.Va?yrynen, Jaakko, Timo Honkela, andLasse Lindqvist.
2007.
Towards explicitsemantic features using independentcomponent analysis.
In Proceedings of theWorkshop Semantic Content Acquisitionand Representation (SCAR), pages 20?27,Stockholm.Weston, Jason, Frederic Ratle, andRonan Collobert.
2008.
Deep learningvia semi-supervised embedding.In Proceedings of the 25th InternationalConference on Machine Learning,pages 1,168?1,175, Helsinki.Yang, Yi, Alexander Yates, and DougDowney.
2013.
Overcoming the memorybottleneck in distributed trainingof latent variable models of text.In Proceedings of the NAACL-HLT,pages 579?584, Atlanta, GA.Zhao, Hai, Wenliang Chen, Chunyu Kit,and Guodong Zhou.
2009.
Multilingualdependency learning: A huge featureengineering method to semanticdependency parsing.
In Proceedings of theCoNLL 2009 Shared Task, pages 55?60,Boulder, CO.120
