Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 36?44,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsA Connectionist Model of Language-Scene InteractionMarshall R. Mayberry, III Matthew W. Crocker Pia KnoeferleDepartment of Computational LinguisticsSaarland UniversitySaarbru?cken 66041, Germanymartym,crocker,knoferle@coli.uni-sb.deAbstractRecent ?visual worlds?
studies, whereinresearchers study language in context bymonitoring eye-movements in a visualscene during sentence processing, have re-vealed much about the interaction of di-verse information sources and the timecourse of their influence on comprehen-sion.
In this study, five experiments thattrade off scene context with a variety oflinguistic factors are modelled with a Sim-ple Recurrent Network modified to inte-grate a scene representation with the stan-dard incremental input of a sentence.
Theresults show that the model captures thequalitative behavior observed during theexperiments, while retaining the ability todevelop the correct interpretation in theabsence of visual input.1 IntroductionPeople learn language within the context of the sur-rounding world, and use it to refer to objects in thatworld, as well as relationships among those objects(e.g., Gleitman, 1990).
Recent research in the vi-sual worlds paradigm, wherein participants?
gazesin a scene while listening to an utterance are moni-tored, has yielded a number of insights into the timecourse of sentence comprehension.
The careful ma-nipulation of information sources in this experimen-tal setting has begun to reveal important character-istics of comprehension such as incrementality andanticipation.
For example, people?s attention to ob-jects in a scene closely tracks their mention in a spo-ken sentence (Tanenhaus et al, 1995), and worldand linguistic knowledge seem to be factors that fa-cilitate object identification (Altmann and Kamide,1999; Kamide et al, 2003).
More recently, Knoe-ferle et al (2005) have shown that when scenes in-clude depicted events, such visual information helpsto establish important relations between the entities,such as role relations.Models of sentence comprehension to date, how-ever, continue to focus on modelling reading behav-ior.
No model, to our knowledge, attempts to ac-count for the use of immediate (non-linguistic) con-text.
In this paper we present results from two simu-lations using a Simple Recurrent Network (SRN; El-man, 1990) modified to integrate input from a scenewith the characteristic incremental processing ofsuch networks in order to model people?s ability toadaptively use the contextual information in visualscenes to more rapidly interpret and disambiguate asentence.
In the modelling of five visual worlds ex-periments reported here, accurate sentence interpre-tation hinges on proper case-role assignment to sen-tence referents.
In particular, modelling is focussedon the following aspects of sentence processing:?
anticipation of upcoming arguments and theirroles in a sentence?
adaptive use of the visual scene as context for aspoken utterance?
influence of depicted events on developing in-terpretation?
multiple/conflicting information sources andtheir relative importance36Figure 1: Selectional Restrictions.
Gaze fixations dependon whether the hare is the subject or object of the sentence,as well as the thematic role structure of the verb.
These gazefixations reveal that people use linguistic and world knowledgeto anticipate upcoming arguments.2 Simulation 1In the first simulation, we simultaneously modelfour experiments that featured revealing contrastsbetween world knowledge and context.
These fourexperiments show that the human sentence proces-sor is very adept at utilizing all available sources ofinformation to rapidly interpret language.
In partic-ular, information from visual context can readily beintegrated with linguistic and world knowledge todisambiguate argument roles where the informationfrom the auditory stream is insufficient in itself.All experiments were conducted in German, alanguage that allows both subject-verb-object (SVO)and object-verb-subject (OVS) sentence types, sothat word order alone cannot be relied upon to deter-mine role assignments.
Rather, case marking in Ger-man is used to indicate grammatical function suchas subject or object, except in the case of feminineand neuter nouns where the article does not carryany distinguishing marking for the nominative andaccusative cases.2.1 Anticipation depending on stereotypicalityThe first two experiments modelled involved unam-biguous sentences in which case-marking and verbselectional restrictions in the linguistic input (i.e.,linguistic and world knowledge or stereotypicality),together with characters depicted in a visual scene,allowed rapid assignment of the roles played bythose characters in the sentence.Experiment 1: Morphosyntactic and lexicalverb information.
In order to examine the influenceof linguistic knowledge of case-marking, Kamideet al (2003) presented experiment participants witha scene showing, for example, a hare, a cabbage, afox, and a distractor (see Figure 1), together with ei-ther a spoken German SVO sentence (1) or with anOVS sentence (2):(1) Der Hase frisst gleich den Kohl.The harenom eats shortly the cabbageacc.
(2) Den Hasen frisst gleich der Fuchs.The hareacc eats shortly the foxnom.The subject and object case-marking on the article ofthe first noun phrase (NP) together with verb mean-ing and world knowledge allowed anticipation of thecorrect post-verbal referent.
Participants made an-ticipatory eye-movements to the cabbage after hear-ing ?The harenom eats ...?
and to the fox upon en-countering ?The hareacc eats ...?.
Thus, people areable to predict upcoming referents when the utter-ance is unambiguous and linguistic/world knowl-edge restricts the domain of potential referents in ascene.Experiment 2: Verb type information.
To fur-ther investigate the role of verb information, theauthors used the same visual scenes in a follow-up study, but replaced the agent/patient verbs likefrisst (?eats?)
with experiencer/theme verbs like in-teressiert (?interests?).
The agent (experiencer) andpatient (theme) roles from Experiment 1 were inter-changed.
Given the same scene in Figure 1 but thesubject-first sentence (3) or object-first sentence (4),participants showed gaze fixations complementaryto those in the first experiment, confirming that bothsyntactic case information and semantic verb infor-mation are used to predict subsequent referents.
(3) Der Hase interessiert ganz besonders den Fuchs.The harenom interests especially the foxacc.
(4) Den Hasen interessiert ganz besonders der Kohl.The hareacc interests especially the cabbagenom.2.2 Anticipation depending on depicted eventsThe second set of experiments investigated tem-porarily ambiguous German sentences.
Findingsshowed that depicted events?just like world and lin-guistic knowledge in unambiguous sentences?canestablish a scene character?s role as agent or patientin the face of linguistic structural ambiguity.37Figure 2: Depicted Events.
The depiction of actions allowsrole information to be extracted from the scene.
People can usethis information to anticipate upcoming arguments even in theface of ambiguous linguistic input.Experiment 3: Verb-mediated depicted role re-lations.
Knoeferle et al (2005) investigated com-prehension of spoken sentences with local structuraland thematic role ambiguity.
An example of the Ger-man SVO/OVS ambiguity is the SVO sentence (5)versus the OVS sentence (6):(5) Die Princessin malt offensichtlich den Fechter.The princessnom paints obviously the fenceracc.
(6) Die Princessin wa?scht offensichtlich der Pirat.The princessacc washes obviously the piratenom.Together with the auditorily presented sentence ascene was shown in which a princess both paintsa fencer and is washed by a pirate (see Figure 2).Linguistic disambiguation occurred on the secondNP; in the absence of stereotypical verb-argumentrelationships, disambiguation prior to the second NPwas only possible through use of the depicted eventsand their associated depicted role relations.
Whenthe verb identified an action, the depicted role rela-tions disambiguated towards either an SVO agent-patient (5) or OVS patient-agent role (6) relation, asindicated by anticipatory eye-movements to the pa-tient (pirate) or agent (fencer), respectively, for (5)and (6).
This gaze-pattern showed the rapid influ-ence of verb-mediated depicted events on the assign-ment of a thematic role to a temporarily ambiguoussentence-initial noun phrase.Experiment 4: Weak temporal adverb con-straint.
Knoeferle et al also investigated Germanverb-final active/passive constructions.
In both theactive future-tense (7) and the passive sentence (8),the initial subject noun phrase is role-ambiguous,and the auxiliary wird can have a passive or futureinterpretation.
(7) Die Princessin wird sogleich den Pirat washen.The princessnom will right away wash the pirateacc.
(8) Die Princessin wird soeben von dem Fechter gemalt.The princessacc is just now painted by the fencernom.To evoke early linguistic disambiguation, temporaladverbs biased the auxiliary wird toward either thefuture (?will?)
or passive (?is -ed?)
reading.
Sincethe verb was sentence-final, the interplay of sceneand linguistic cues (e.g., temporal adverbs) wererather more subtle.
When the listener heard a future-biased adverb such as sogleich, after the auxiliarywird, he interpreted the initial NP as an agent of a fu-ture construction, as evidenced by anticipatory eye-movements to the patient in the scene.
Conversely,listeners interpreted the passive-biased constructionwith these roles exchanged.2.3 ArchitectureThe Simple Recurrent Network is a type of neu-ral network typically used to process temporal se-quences of patterns such as words in a sentence.A common approach is for the modeller to trainthe network on prespecified targets, such as verbsand their arguments, that represent what the net-work is expected to produce upon completing a sen-tence.
Processing is incremental, with each new in-put word interpreted in the context of the sentenceprocessed so far, represented by a copy of the pre-vious hidden layer serving as additional input to thecurrent hidden layer.
Because these types of asso-ciationist models automatically develop correlationsamong the sentence constituents they are trainedon, they will generally develop expectations aboutthe output even before processing is completed be-cause sufficient information occurs early in the sen-tence to warrant such predictions.
Moreover, duringthe course of processing a sentence these expecta-tions can be overridden with subsequent input, oftenabruptly revising an interpretation in a manner rem-iniscent of how humans seem to process language.Indeed, it is these characteristics of incremental pro-cessing, the automatic development of expectations,seamless integration of multiple sources of informa-tion, and nonmonotonic revision that have endearedneural network models to cognitive researchers.In this study, the four experiments described38hidden layercontext layerevent layerswaescht Prinzessin Pirat PATinput layerwaeschtFigure 3: Scene Integration.
A simple conceptual rep-resentation of the information in a scene, along with com-pressed event information from depicted actions when present,is fed into a standard SRN to model adaptive processing.
Thelinks connecting the depicted characters to the hidden layer areshared, as are the links connecting the event layers to the hiddenlayer.above have been modelled simultaneously using asingle network.
The goal of modelling all experi-mental results by a single architecture required en-hancements to the SRN, the development and pre-sentation of the training data, as well as the trainingregime itself.
These will be described in turn below.In two of the experiments, only three charactersare depicted, representation of which can be propa-gated directly to the network?s hidden layer.
In theother two experiments, the scene featured three char-acters involved in two events (e.g., pirate-washes-princess and princess-paints-fencer, as shown inFigure 3).
The middle character was involved inboth events, either as an agent or a patient (e.g.,princess).
Only one of the events, however, corre-sponded to the spoken linguistic input.The representation of this scene information andits integration into the model?s processing was themain modification to the SRN.
Connections betweenrepresentations for the depicted characters and thehidden layer were provided.
Encoding of the de-picted events, when present, required additionallinks from the characters and depicted actions toevent layers, and links from these event layers to theSRN?s hidden layer.
The network developed repre-sentations for the events in the event layers by com-pressing the scene representations of the involvedcharacters and depicted actions through weights cor-responding to the action, its agent and its patient foreach event.
This event representation was kept sim-ple and only provided conceptual input to the hiddenlayer: who did what to whom was encoded for bothevents, when depicted, but grammatical informationonly came from the linguistic input.
As the focus ofthis study was on whether sentence processing couldadapt to information from the scene when present orfrom stored knowledge, lower-level perceptual pro-cesses such as attention were not modelled.Neural networks will usually encode any correla-tions in the data that help to minimize error.
In orderto prevent the network from encoding regularities inits weights regarding the position of the charactersand events given in the scene (such as, for example,that the central character in the scene correspondsto the first NP in the presented sentence) which arenot relevant to the role-assignment task, one set ofweights was used for all characters, and another setof weights used for both events.
This weight-sharingensured that the network had to access the informa-tion encoded in the event layers, or determine therelevant characters itself, thus improving generaliza-tion.
The representations for the characters and ac-tions were the same for both input (scene and sen-tence) and output.The input assemblies were the scene represen-tations and the current word from the input sen-tence.
The output assemblies were the verb, thefirst and second nouns, and an assembly that indi-cated whether the first noun was the agent or pa-tient of the sentence (token PAT in Figure 3).
Typ-ically, agent and patient assemblies would be fixedin a case-role representation without such a discrim-inator, and the model required to learn to instantiatethem correctly (Miikkulainen, 1997).
However, wefound that the model performed much better whenthe task was recast as having to learn to isolate thenouns in the order in which they are introduced, andseparately mark how those nouns relate to the verb.The input and output assemblies had 100 units each,the event layers contained 200 units each, and thehidden and context layers consisted of 400 units.392.4 Input Data, Training, and ExperimentsWe trained the network to correctly handle sentencesinvolving non-stereotypical events as well as stereo-typical ones, both when visual context was presentand when it was absent.
As over half a billion sen-tence/scene combinations were possible for all of theexperiments, we adopted a grammar-based approachto exhaustively generate sentences and scenes basedon the experimental materials while holding out theactual materials to be used for testing.
In order toaccurately model the first two experiments involv-ing selectional restrictions on verbs, two additionalwords were added to the lexicon for each charac-ter selected by a verb.
For example, in the sentenceDer Hase frisst gleich den Kohl, the nouns Hase1,Hase2, Kohl1, and Kohl2 were used to develop train-ing sentences.
These were meant to represent, forexample, words such as ?rabbit?
and ?jackrabbit?
or?carrot?
and ?lettuce?
in the lexicon that have thesame distributional properties as the original words?hare?
and ?cabbage?.
With these extra tokens thenetwork could learn that Hase, frisst, and Kohl werecorrelated without ever encountering all three wordsin the same training sentence.
The experiments in-volving non-stereotypicality did not pose this con-straint, so training sentences were simply generatedto avoid presenting experimental items.Some standard simplifications to the words havebeen made to facilitate modelling.
For example,multi-word adverbs such as fast immer were treatedas one word through hyphenation so that sentencelength within a given experimental set up is main-tained.
Nominal case markings such as -n in Hasenwere removed to avoid sparse data as these markingsare idiosyncratic, while the case markings on the de-terminers are more informative overall.
More impor-tantly, morphemes such as the infinitive marker -enand past participle ge- were removed, because, forexample, the verb forms malt, malen, and gemalt,would all be treated as unrelated tokens, again con-tributing unnecessarily to the problem with sparsedata.
The result is that one verb form is used, andto perform accurately, the network must rely on itsposition in the sentence (either second or sentence-final), as well as whether the word von occurs toindicate a participial reading rather than infinitival.All 326 words in the lexicon for the first four exper-                                                 									                                                                                               859095100Exp 1 Exp 2 Exp 3 Exp 4PercentageCorrectAdverbNP2Figure 4: Results.
In each of the four experiments modelled,anticipation of the upcoming argument at the adverb is nearlyas accurate as at sentence end.
However, the network has somedifficulty with distinguishing stereotypical arguments.iments were given random representations over thevertices of a 100-dimensional hypercube, which re-sulted in marked improvement over sampling fromwithin the hypercube (Noelle et al, 1997).We trained the network by repeatedly presentingthe model with 1000 randomly generated sentencesfrom each experiment (constituting one epoch) andtesting every 100 epochs against the held-out testmaterials for each of the four experiments.
Sceneswere provided half of the time to provide an un-biased approximation to linguistic experience.
Thenetwork was initialized with weights between -0.01and 0.01.
The learning rate was initially set to 0.05and gradually reduced to 0.002 over the course of15000 epochs.
Ten splits were run on 1.6Ghz PCsand took a little over two weeks to complete.2.5 ResultsFigure 4 reports the percentage of targets at thenetwork?s output layer that the model correctlymatches, both as measured at the adverb and at theend of the sentence.
The model clearly demonstratesthe qualitative behavior observed in all four experi-ments in that it is able to access the information fromthe encoded scene or stereotypicality and combine itwith the incrementally presented sentence to antici-pate forthcoming arguments.For the two experiments (1 and 2) using stereotyp-ical information, the network achieved just over 96%at sentence end, and anticipation accuracy was justover 95% at the adverb.
Analysis shows that the net-work makes errors in token identification, confus-ing words that are within the selectionally restricted40set, such as, for example, Kohl and Kohl2.
Thus,the model has not quite mastered the stereotypicalknowledge, particularly as it relates to the presenceof the scene.For the other two experiments using non-stereotypical characters and depicted events (exper-iments 3 and 4), accuracy was 100% at the end ofthe sentence.
More importantly, the model achievedover 98% early disambiguation on experiment 3,where the sentences were simple, active SVO andOVS.
Early disambiguation on experiment 4 wassomewhat harder because the adverb is the disam-biguating point in the sentence as opposed to theverb in the other three experiments.
As nonlineardynamical systems, neural networks sometimes re-quire an extra step to settle after a decision point isreached due to the attractor dynamics of the weights.On closer inspection of the model?s behavior dur-ing processing, it is apparent that the event layersprovide enough additional information beyond thatencoded in the weights between the characters andthe hidden layer that the model is able to make finerdiscriminations in experiments 3 and 4, enhancingits performance.3 Simulation 2The previous set of experiments examined how peo-ple are able to use either stereotypical knowledge ordepicted information to anticipate forthcoming ar-guments in a sentence.
But how does the humansentence processor handle these information sourceswhen both are present?
Which takes precedencewhen they conflict?
The experiment modelled in thissection was designed to provide some insight intothese questions.Scene vs Stored Knowledge.
Based on the find-ings from the four experiments in Simulation 1,Knoeferle and Crocker (2004b) examined two is-sues.
First, it verified that stored knowledge aboutnon-depicted events and information from depicted,but non-stereotypical, events each enable rapid the-matic interpretation.
An example scene showed awizard, a pilot, and a detective serving food (Fig-ure 5).
When people heard condition 1 (examplesentence 9), the case-marking on the first NP identi-fied the pilot as a patient.
Stereotypical knowledgeidentified the wizard as the only relevant agent, asFigure 5: Scene vs Stored Knowledge.
Experimental resultsshow that people rely on depicted information over stereotypicalknowledge when both are present during sentence processing.indicated by a higher proportion of anticipatory eye-movements to the stereotypical agent (wizard) thanto the detective.
In contrast, when people heard theverb in condition 2 (sentence 10), it uniquely iden-tified the detective as the only food-serving agent,revealed by more inspections to the agent of the de-picted event (detective) than to the wizard.
(9) Den Piloten verzaubert gleich der Zauberer.The pilotacc jinxes shortly the wizardnom.
(10) Den Piloten verko?stigt gleich der Detektiv.The pilotacc serves-food-to shortly the detectivenom.Second, the study determined the relative impor-tance of depicted events and verb-based thematicrole knowledge when the information sources werein competition.
In both conditions 3 & 4 (sentences11 & 12), participants heard an utterance in whichthe verb identified both a stereotypical (detective)and a depicted agent (wizard).
When faced with thisconflict, people preferred to rely on the immediateevent depictions over stereotypical knowledge, andlooked more often at the wizard, the agent in the de-picted event, than at the other, stereotypical agent ofthe spying-action (the detective).
(11) Den Piloten bespitzelt gleich der Detektiv.The pilotacc spies-on shortly the detectivenom.
(12) Den Piloten bespitzelt gleich der Zauberer.The pilotacc spies-on shortly the wizardnom.3.1 Architecture, Data, Training, and ResultsIn simulation 1, we modelled experiments that de-pended on stereotypicality or depicted events, butnot both.
The experiment modelled in simulation2, however, was specifically designed to investigate41how these two information sources interacted.
Ac-cordingly, the network needed to learn to use eitherinformation from the scene or stereotypicality whenavailable, and, moreover, favor the scene when thetwo sources conflicted, as observed in the empiricalresults.
Recall that the network is trained only on thefinal interpretation of a sentence.
Thus, capturingthe observed behavior required manipulation of thefrequencies of the four conditions described aboveduring training.
In order to train the network to de-velop stereotypical agents for verbs, the frequencythat a verb occurs with its stereotypical agent, suchas Detektiv and bespitzelt from example (11) above,had to be greater than for a non-stereotypical agent.However, the frequency should not be so great thatit overrode the influence from the scene.The solution we adopted is motivated by a the-ory of language acquisition that takes into accountthe importance of early linguistic experience in a vi-sual environment (see the General Discussion).
Wefound a small range of ratios of stereotypicality tonon-stereotypicality that permitted the network todevelop an early reliance on information from thescene while it gradually learned the stereotypical as-sociations.
When the ratio was lower than 6:1, thenetwork developed too strong a reliance on stereo-typicality, overriding information from the scene.When the ratio was greater than 15:1, the scenealways took precedence when it was present, butstereotypical knowledge was used when the scenewas not present.
Within this range, however, thenetwork quickly learns to extract information fromthe scene because the scene representation remainsstatic while a sentence is processed incrementally.It is the stereotypical associations, predictably, thattake longer for the network to learn in rough propor-tion to their ratio over non-stereotypical agents.Figure 6 shows the effect this training regime hadover 6000 epochs on the ability of the network to ac-curately anticipate the missing argument in each ofthe four conditions described above when the ratioof non-stereotypical to stereotypical sentences was8:1.
The network quickly learns to use the scene forconditions 2-4 (examples 10-12), where the action inthe linguistic input stream is also depicted, allowingthe network to determine the relevant event and de-duce the missing argument.
(Because conditions 3and 4 are the same up to the second NP, their curves.0.70.750.80.850.90.9510  1000  2000  3000  4000  5000  6000PercentageCond 1Cond 2Cond 3Cond 4Figure 6: Acquisition of Stereotypicality.
Stereotypicalknowledge (condition 1) is acquired much more gradually thaninformation from the scene (conditions 2-4).are, in fact, identical.)
But condition 1 (sentence 9)requires only stereotypical knowledge.
The accu-racy of condition 1 remains close to 75% (correctlyproducing the verb, first NP, and role discriminator,but not the second NP) until around epoch 1200 orso and then gradually improves as the network learnsthe appropriate stereotypical associations.
The con-dition 1 curve asymptotically approaches 100% overthe course of 10,000 epochs.Results from several runs with different trainingparameters (such as learning rate and stereotypical-ity ratio) show that the network does indeed modelthe observed experimental behavior.
The best resultsso far exceed 99% accuracy in correctly anticipatingthe proper roles and 100% accuracy at sentence end.As in simulation 1, the training corpus was gen-erated by exhaustively combining participants andactions for all experimental conditions while hold-ing out all test sentences.
However, we found thatwe were able to use a larger learning rate, 0.1, thanthe 0.05 used in the first simulation.
The 130 wordsin the lexicon were given random binary representa-tions from the vertices of a 100-dimensional hyper-cube as described before.Analysis of the network after successful trainingsuggests why the training regime of holding the ratioof stereotypical to non-stereotypical sentences con-stant works.
Early in training, before stereotypical-ity has been encoded in the network?s weights, pat-terns are developed in the hidden layer as each wordis processed that enable the network to accuratelydecode the words in the output layer.
Once the verbis read in, its hidden layer pattern is available to pro-42duce the correct output representations for both theverb itself and its stereotypical agent.
Not surpris-ingly, the network thus learns to associate the hiddenlayer pattern for the verb with its stereotypical agentpattern in the second NP output slot.
The only con-straint for the network is to ensure that the scene canstill override this stereotypicality when the depictedevent so dictates.4 General Discussion and Future WorkExperiments in the visual worlds paradigm haveclearly reinforced the view of language comprehen-sion as an active, incremental, highly integrativeprocess in which anticipation of upcoming argu-ments plays a crucial role.
Visual context not onlyfacilitates identification of likely referents in a sen-tence, but helps establish relationships between ref-erents and the roles they may fill.
Research thus farhas shown that the human sentence processor seemsto have easy access to whatever information is avail-able, whether it be syntactic, lexical, semantic, or vi-sual, and that it can combine these sources to achieveas complete an interpretation as is possible at anygiven point in comprehending a sentence.The modelling results reported in this paper are animportant step toward the goal of understanding howthe human sentence processor is able to accomplishthese feats.
The SRN provides a natural frameworkfor this research because its operation is premisedon incremental and integrative processing.
Trainedsimply to produce a representation of the completeinterpretation of a sentence as each new word is pro-cessed (on the view that people learn to process lan-guage by reviewing what they hear), the model au-tomatically develops anticipations for upcoming ar-guments that allow it to demonstrate the early dis-ambiguation behavior observed in the visual worldsexperiments modelled here.The simple accuracy results belie the complex-ity of the task in both simulations.
In Simulation1, the network has to demonstrate early disambigua-tion when the scene is present, showing that it canindeed access the proper role and filler from thecompressed representation of the event associatedwith the first NP and verb processed in the linguisticstream.
This task is rendered more difficult becausethe proper event must be extracted from the super-imposition of the two events in the scene, which iswhat is propagated into the model?s hidden layer.
Inaddition, it must also still be able to process all sen-tences correctly when the scene is not present.Simulation 2 is more difficult still.
The experi-ment shows that information from the scene takesprecedence when there is a conflict with stereotypi-cal knowledge; otherwise, each source of knowledgeis used when it is available.
In the training regimeused in this simulation, the dominance of the sceneis established early because it is much more fre-quent than the more particular stereotypical knowl-edge.
As training progresses, stereotypical knowl-edge is gradually learned because it is sufficientlyfrequent for the network to capture the relevant as-sociations.
As the network weights gradually satu-rate, it becomes more difficult to retune them.
Butencoding stereotypical knowledge requires far fewerweight adjustments, so the network is able to learnthat task later during training.Knoeferle and Crocker (2004a,b) suggest that thepreferred reliance of the comprehension system onthe visual context over stored knowledge might bestbe explained by appealing to a boot-strapping ac-count of language acquisition such as that of Gleit-man (1990).
The development of a child?s worldknowledge occurs in a visual environment, whichaccordingly plays a prominent role during languageacquisition.
The fact that the child can draw on twoinformational sources (utterance and scene) enablesit to infer information that it has not yet acquiredfrom what it already knows.
This contextual devel-opment may have shaped both our cognitive archi-tecture (i.e., providing for rapid, seamless integra-tion of scene and linguistic information), and com-prehension mechanisms (e.g., people rapidly availthemselves of information from the immediate scenewhen the utterance identifies it).Connectionist models such as the SRN have beenused to model aspects of cognitive development, in-cluding the timing of emergent behaviors (Elmanet al, 1996), making them highly suitable for sim-ulating developmental stages in child language ac-quisition (e.g., first learning names of objects in theimmediate scene, and later proceeding to the acqui-sition of stereotypical knowledge).
If there are de-velopmental reasons for the preferred reliance of lis-teners on the immediate scene during language com-43prehension, then the finding that modelling that de-velopment provides the most efficient (if not only)way to naturally reproduce the observed experimen-tal behavior promises to offer deeper insight intohow such knowledge is instilled in the brain.Future research will focus on combining all of theexperiments in one model, and expand the range ofsentence types and fillers to which the network isexposed.
The architecture itself is being redesignedto scale up to much more complex linguistic con-structions and have greater coverage while retainingthe cognitively plausible behavior described in thisstudy (Mayberry and Crocker, 2004).5 ConclusionWe have presented a neural network architecture thatsuccessfully models the results of five recent exper-iments designed to study the interaction of visualcontext with sentence processing.
The model showsthat it can adaptively use information from the vi-sual scene such as depicted events, when present,to anticipate roles and fillers as observed in each ofthe experiments, as well as demonstrate traditionalincremental processing when context is absent.
Fur-thermore, more recent results show that training thenetwork in a visual environment, with stereotypicalknowledge gradually learned and reinforced, allowsthe model to negotiate even conflicting informationsources.6 AcknowledgementsThis research was funded by SFB 378 project ?AL-PHA?
to the first two authors and a PhD scholar-ship to the last, all awarded by the German ResearchFoundation (DFG).ReferencesAltmann, G. T. M. and Kamide, Y.
(1999).
Incre-mental interpretation at verbs: Restricting the do-main of subsequent reference.
Cognition, 73:247?264.Elman, J. L. (1990).
Finding structure in time.
Cog-nitive Science, 14:179?211.Elman, J. L., Bates, E. A., Johnson, M. H.,Karmiloff-Smith, A., Parisi, D., and Plunkett, K.(1996).
Rethinking Innateness: A ConnectionistPerspective on Development.
MIT Press, Cam-bridge, MA.Gleitman, L. (1990).
The structural sources of verbmeanings.
Language Acquisition, 1:3?55.Kamide, Y., Scheepers, C., and Altmann, G. T. M.(2003).
Integration of syntactic and seman-tic information in predictive processing: Cross-linguistic evidence from German and English.Journal of Psycholinguistic Research, 32(1):37?55.Knoeferle, P. and Crocker, M. W. (2004a).
The co-ordinated processing of scene and utterance: ev-idence from eye-tracking in depicted events.
InProceedings of International Conference on Cog-nitive Science, Allahabad, India.Knoeferle, P. and Crocker, M. W. (2004b).
Storedknowledge versus depicted events: what guidesauditory sentence comprehension.
In Proceedingsof the 26th Annual Conference of the CognitiveScience Society.
Mahawah, NJ: Erlbaum.
714?719.Knoeferle, P., Crocker, M. W., Scheepers, C., andPickering, M. J.
(2005).
The influence of the im-mediate visual context on incremental thematicrole-assignment: evidence from eye-movementsin depicted events.
Cognition, 95:95?127.Mayberry, M. R. and Crocker, M. W. (2004).
Gen-erating semantic graphs through self-organization.In Proceedings of the AAAI Symposium on Com-positional Connectionism in Cognitive Science,pages 40?49, Washington, D.C.Miikkulainen, R. (1997).
Natural language process-ing with subsymbolic neural networks.
In Browne,A., editor, Neural Network Perspectives on Cogni-tion and Adaptive Robotics, pages 120?139.
Insti-tute of Physics Publishing, Bristol, UK; Philadel-phia, PA.Noelle, D. C., Cottrell, G. W., and Wilms, F. (1997).Extreme attraction: The benefits of corner attrac-tors.
Technical Report CS97-536, Department ofComputer Science and Engineering, UCSD, SanDiego, CA.Tanenhaus, M. K., Spivey-Knowlton, M. J., Eber-hard, K. M., and Sedivy, J. C. (1995).
Integrationof visual and linguistic information in spoken lan-guage comprehension.
Science, 268:1632?1634.44
