Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 76?83,NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsEnhancing commercial grammar-based applications using robustapproaches to speech understandingMatthieu He?bertNetwork ASR R+D, Nuance Communications1500, Universite?, Suite 935, Montre?al, Que?bec, H3A 3T2, Canadahebert@nuance.comAbstractThis paper presents a series of measure-ments of the accuracy of speech under-standing when grammar-based or robustapproaches are used.
The robust ap-proaches considered here are based on sta-tistical language models (SLMs) with theinterpretation being carried out by phrase-spotting or robust parsing methods.
Wepropose a simple process to leverage ex-isting grammars and logged utterancesto upgrade grammar-based applications tobecome more robust to out-of-coverageinputs.
All experiments herein are runon data collected from deployed directeddialog applications and show that SLM-based techniques outperform grammar-based ones without requiring any changein the application logic.1 IntroductionThe bulk of the literature on spoken dialog systemsis based on the simple architecture in which theinput speech is processed by a statistical languagemodel-based recognizer (SLM-based recognizer) toproduce a word string.
This word string is furtherprocessed by a robust parser (Ward, 1990) or callrouter (Gorin et al 1997) to be converted in a se-mantic interpretation.
However, it is striking to seethat a large portion of deployed commercial appli-cations do not follow this architecture and approachthe recognition/interpretation problem by relying onhand-crafted rules (context-free grammars - CFGs).The apparent reasons for this are the up-front costand additional delays of collecting domain-specificutterances to properly train the SLM (not to men-tion semantic tagging needed to train the call router)(Hemphill et al 1990; Knight et al 2001; Gorin etal, 1997).
Choosing to use a grammar-based ap-proach also makes the application predictable andrelatively easy to design.
On the other hand, theseapplications are usually very rigid: the users are al-lowed only a finite set of ways to input their requestsand, by way of consequences, these applications suf-fer from high out-of-grammar (OOG) rates or out-of-coverage rates.A few studies have been published compar-ing grammar-based and SLM-based approaches tospeech understanding.
In (Knight et al 2001),a comparison of grammar-based and robust ap-proaches is presented for a user-initiative home au-tomation application.
The authors concluded thatit was relatively easy to use the corpus collectedduring the course of the application development totrain a SLM which would perform better on out-of-coverage utterances, while degrading the accu-racy on in-coverage utterances.
They also reportedthat the SLM-based system showed slightly lowerword error rate but higher semantic error rate forthe users who know the application?s coverage.
In(Rayner et al 2005), a rigorous test protocol is pre-sented to compare grammar-based and robust ap-proaches in the context of a medical translation sys-tem.
The paper highlights the difficulties to con-struct a clean experimental set-up.
Efforts are spentto control the training set of both approaches to76have them align.
The training sets are defined asthe set of data available to build each system: for agrammar-based system, it might be a series of sam-ple dialogs.
(ten Bosch, 2005) presents experimentscomparing grammar-based and SLM-based systemsfor na?
?ve users and an expert user.
They concludethat the SLM-based system is most effective in re-ducing the error rate for na?
?ve users.
Recently (see(Balakrishna et al 2006)), a process was presentedto automatically build SLMs from a wide varietyof sources (in-service data, thesaurus, WordNet andworld-wide web).
Results on data from commer-cial speech applications presented therein echo ear-lier results (Knight et al 2001) while reducing theeffort to build interpretation rules.Most of the above studies are not based on datacollected on deployed applications.
One of the con-clusions from previous work, based on the measuredfact that in-coverage accuracy of the grammar-basedsystems was far better than the SLM one, was thatas people get more experience with the applications,they will naturally learn its coverage and gravitatetowards it.
While this can be an acceptable optionfor some types of applications (when the user pop-ulation tends to be experienced or captive), it cer-tainly is not a possibility for large-scale commercialapplications that are targeted at the general public.
Afew examples of such applications are public transitschedules and fares information, self-help applica-tions for utilities, banks, telecommunications busi-ness, and etc.
Steering application design and re-search based on in-coverage accuracy is not suitablefor these types of applications because a large frac-tion of the users are na?
?ves and tend to use more nat-ural and unconstrained speech inputs.This paper exploits techniques known since the90?s (SLM with robust parsing, (Ward, 1990)) andapplies them to build robust speech understandinginto existing large scale directed dialog grammar-based applications.
This practical application of(Ward, 1990; Knight et al 2001; Rayner et al 2005;ten Bosch, 2005) is cast as an upgrade problemwhich must obey the following constraints.1.
No change in the application logic and to thevoice user interface (VUI)2.
Roughly similar CPU consumption3.
Leverage existing grammars4.
Leverage existing transcribed utterances5.
Simple process that requires little manual inter-ventionThe first constraint dictates that, for each context,the interpretation engines (from the current and up-graded systems) must return the same semantics (i.e.same set of slots).The rest of this paper is organized as follows.
Thenext Section describes the applications from whichthe data was collected, the experimental set-up andthe accuracy measures used.
Section 3 describeshow the semantic truth is generated.
The main re-sults of the upgrade from grammar-based to SLM-based recognition are presented in Section 4.
Thetarget audience for this paper is composed of appli-cation developers and researchers that are interestedin the robust information extraction from directeddialog speech applications targeted at the generalpublic.2 Applications, corpus and experimentalset-up2.1 Application descriptionsAs mentioned earlier, the data for this study was col-lected on deployed commercial directed dialog ap-plications.
AppA is a self-help application in the in-ternet service provider domain, while AppB is alsoa self-help application in the public transportationdomain.
Both applications are grammar-based di-rected dialogs and receive a daily average of 50kcalls.
We will concentrate on a subset of contexts(dialog states) for each application as described inTable 1.
The mainmenu grammars (each applicationhas its own mainmenu grammar) contain high-leveltargets for the rest of the application and are activeonce the initial prompt has been played.
The com-mand grammar contains universal commands like?help?, ?agent?, etc.
The origin and destinationgrammars contain a list of ?
2500 cities and stateswith the proper prefixes to discriminate origin anddestination.
num type passenger accepts up to ninepassengers of types adults, children, seniors, etc.Finally time is self explanatory.
For each applica-tion, the prompt directs the user to provide a specific77Context Description Active grammars Training Testingsentences uttsAppA MainMenu Main menu mainmenu and 5000 5431for the application commands (350) (642)AppB MainMenu Main menu mainmenu and 5000 4039for the application commands (19) (987)AppB Origin Origin of travel origin, destination 5000 8818and commands (20486) (529)AppB Passenger Number and type num type passenger 1500 2312of passenger and commands (32332) (66)AppB Time Time of departure time and commands 1000 1149(4102) (55)Table 1: Description of studied contexts for each application.
Note that the AppB Origin context contains adestination grammar: this is due to the fact that the same set of grammars was used in the AppB Destinationcontext (not studied here).
?Training?
contains the number of training sentences drawn from the corpus andused to train the SLMs.
As mentioned in Sec.
2.3, in the case of word SLMs, we also use sentences that arecovered by the grammars in each context as backoffs (see Sec.
2).
The number of unique sentences coveredby the grammars is in parenthesis in the ?Training?
column.
The ?Testing?
column contains the number ofutterances in the test set.
The number of those utterances that contain no speech (noise) is in parenthesis.piece of information (directed dialog).
Each gram-mar fills a single slot with that information.
The in-formation contained in the utterance ?two adults andone child?
(AppB Passenger context) would be col-lapsed to fill the num type passenger slot with thevalue ?Adult2 Child1?.
From the application pointof view, each context can fill only a very limited setof slots.
To keep results as synthesized as possible,unless otherwise stated, the results from all studiedcontexts will be presented per application: as suchresults from all contexts in AppB will be pooled to-gether.2.2 Corpus descriptionTable 1 presents the details of the corpus that wehave used for this study.
As mentioned above the en-tire corpora used for this study is drawn from com-mercially deployed systems that are used by the gen-eral public.
The user population reflects realisticusage (expert vs na?
?ve), noise conditions, handsets,etc.
The training utterances do not contain noise ut-terances and is used primarily for SLM training (noacoustic adaptation of the recognition models is per-formed).2.3 Experimental set-up descriptionThe baseline system is the grammar-based system;the recognizer uses, on a per-context basis, the gram-mars listed in Table 1 in parallel.
The SLM systemsstudied all used the same interpretation engine: ro-bust parsing with the grammars listed in Table 1 asrules to fill slots.
Note that this allows the applica-tion logic to stay unchanged since the set of potentialslots returned within any given context is the same asfor the grammar-based systems (see first constraintin Sec.
1).
Adhering to this experimental set-up alsoguarantees that improvements measured in the labwill have a direct impact on the raw accuracy of thedeployed application.We have considered two different SLM-basedsystems in this study: standard SLM (wordSLM)and class-based SLM (classSLM) (Jelinek, 1990;Gillett and Ward, 1998).
In the classSLM systems,the classes are defined as the rules of the interpre-tation engine (i.e.
the grammars active for eachcontext as defined in Table 1).
The SLMs are alltrained on a per-context basis (Xu and Rudnicky,2000; Goel and Gopinath, 2006) as bi-grams withWitten-Bell discounting.
To insure that the word-SLM system covered all sentences that the grammar-based system does, we augmented the training set of7800.20.40.60.810  0.05  0.1  0.15  0.2CA-inFA-totalCA-in/FA-totalgrammar-based - automaticgrammar-based - humanwordSLM - automaticwordSLM - human  0.860.880.90.920.940.960.9810  0.2  0.4  0.6  0.8  1PrecisionRecallRecall/Precisiongrammar-based - automaticgrammar-based - humanwordSLM - automaticwordSLM - humanFigure 1: ROC curves for AppA MainMenu with the automatic or human-generated truth.
In each thegrammar-based and SLM-based systems are compared.the wordSLM (see Table 1) with the list of sentencesthat are covered by the baseline grammar-based sys-tem.
This acts as a backoff in case a word or bi-gram is not found in the training set (not to be con-fused with bi-gram to uni-gram backoffs found instandard SLM training).
This is particularly helpfulwhen a little amount of data is available for trainingthe wordSLM (see Sec.
4.3).2.4 Accuracy measuresThroughout this paper, we will use two sets of mea-sures.
This is motivated by the fact that applica-tion developers are familiar with the concepts of cor-rect/false acceptance at the utterance level.
For in-formation extraction (slot filling) from utterances,these concepts are restrictive because an utterancecan be partly correct or wrong.
In this case we pre-fer a more relevant measure from the information re-trieval field: precision and recall on a per-slot basis.We use the following definitions.?
CA-in = #utts that had ALL slots correct (slotname and value) / #utts that are in-coverage(i.e.
truth has at least a slot filled)?
FA-total = #utts that had at least one erroneousslot (slot name or value) / total #utts?
Precision = #slot correct slots (slot name andvalue) / #slots returned by system?
Recall = #slot correct slots (slot name andvalue) / #slots potential slots (in truth)Since applications use confidence extensively toguide the course of dialogue, it is of limited interestto study forced-choice accuracy (accuracy with norejection).
Hence, we will present receiver operat-ing characteristic (ROC) curves.
The slot confidencemeasure is based on redundancy of a slot/value pairacross the NBest list.
For CA-in and FA-total, theconfidence is the average confidence of all slotspresent in the utterance.
Note that in the case whereeach utterance only fills a single slot, CA-in = Re-call.3 TruthDue to the large amount of data processed (see Table1), semantic tagging by a human may not be avail-able for all contexts (orthographic transcriptions areavailable however).
We need to resort to a more au-tomatic way of generating the truth files while main-taining a strong confidence in our measurements.
Tothis end, we need to ensure that any automatic wayof generating the truth will not bias the results to-wards any of the systems.The automatic truth can be generated by simplyusing the robust parser (see Sec.
2.3) on the or-thographic transcriptions which are fairly cheap toacquire.
This will generate a semantic interpreta-tion for those utterances that contain fragments that79parse rules defined by the interpretation engine.
Thehuman-generated truth is the result of semanticallytagging all utterances that didn?t yield a full parseby one of the rules for the relevant context.Figure 1 presents the ROC curves of human andautomatic truth generation for the grammar-basedand wordSLM systems.
We can see that human se-mantic tagging increases the accuracy substantially,but this increase doesn?t seem to favor one systemover the other.
We are thus led to believe that in ourcase (very few well defined non-overlapping classes)the automatic truth generation is sufficient.
Thiswould not be the case, for example if for a given con-text a time grammar and number were active classes.Then, an utterance like ?seven?
might lead to an er-roneous slot being automatically filled while a hu-man tagger (who would have access to the entire di-alog) would tag it correctly.In our experiments, we will use the hu-man semantically tagged truth when available(AppA MainMenu and AppB Origin).
We havechecked that the conclusions of this paper are notaltered in any way if the automatic semanticallytagged truth had been used for these two contexts.4 Results and analysis4.1 Out-of-coverage analysisContext (#utts) grammar- SLM-basedbasedAppA MainMenu 1252 1086AppB MainMenu 1287 1169AppB Origin 1617 1161AppB Passenger 492 414AppB Time 327 309Table 2: Number of utterances out-of-coverage foreach context.Coverage is a function of the interpretation en-gine.
We can readily analyze the effect of goingfrom a grammar-based interpretation engine (gram-mars in Table 1 are in parallel) to the robust ap-proach (rules from grammars in Table 1 are usedin robust parsing).
This is simply done by runningthe interpretation engine on the orthographic tran-scriptions.
As expected, the coverage increased.
Ta-ble 2 shows the number of utterances that didn?tfire any rule for each of the interpretation engines.These include noise utterances as described in Table1.
If we remove the noise utterances, going fromthe grammar-based interpretation to an SLM-basedone reduces the out-of-coverage by 31%.
This resultis interesting because the data was collected fromdirected-dialog applications which should be heav-ily guiding the users to the grammar-based system?scoverage.4.2 Results with recognizerThe main results of this paper are found in Fig-ure 2.
It presents for grammar-based, wordSLMand classSLM systems the four measurements men-tioned in Sec.2.4 for AppA and AppB.
We havemanaged, with proper Viterbi beam settings, to keepin the increase in CPU (grammar-based system ?SLM-based system) between 0% and 24% relative.We can see that the wordSLM is outperforming theclassSLM.
The SLM-based systems outperform thegrammar-based systems substantially (?
30 ?
50%error rate reduction on most of the confidence do-main).
The only exception to this is the classSLMin AppA: we will come back to this in Sec.
4.4.This can be interpreted as a different conclusion thanthose of (Knight et al 2001; ten Bosch, 2005).
Thediscrepancy can be tied to the fact that the data weare studying comes from a live deployment targetedto the general public.
In this case, we can makethe hypothesis that a large fraction of the popula-tion is composed of na?
?ve users.
As mentioned in(ten Bosch, 2005), SLM-based systems perform bet-ter than grammar-based ones on that cross-section ofthe user population.One might argue that the comparison between thegrammar-based and wordSLM systems is unfair be-cause the wordSLM intrinsically records the a prioriprobability that a user says a specific phrase whilethe grammar-based system studied here didn?t ben-efit from this information.
In Sec.
4.4, we will ad-dress this and show that a priori has a negligible ef-fect in this context.Note that these impressive results are surprisinglyeasy to achieve.
A simple process could be as fol-lows.
An application is developed using grammar-based paradigm.
After a limited deployment or pilotwith real users, a wordSLM is built from transcribed(orthographic) data from the field.
Then the recog-8000.20.40.60.810  0.05  0.1  0.15  0.2  0.25  0.3  0.35CA-inFA-totalCA-in/FA-totalgrammar-based (73ms)wordSLM (74ms)classSLM (108ms)0.70.750.80.850.90.9510  0.2  0.4  0.6  0.8  1PrecisionRecallRecall/Precisiongrammar-based (73ms)wordSLM (74ms)classSLM (108ms)AppA00.20.40.60.810  0.05  0.1  0.15  0.2  0.25  0.3  0.35CA-inFA-totalCA-in/FA-totalgrammar-based (94ms)wordSLM (117ms)classSLM (113ms)0.70.750.80.850.90.9510  0.2  0.4  0.6  0.8  1PrecisionRecallRecall/Precisiongrammar-based (94ms)wordSLM (117ms)classSLM (113ms)AppBFigure 2: ROC curves for AppA (top) and AppB (bottom).
In parenthesis is the average time for therecognition and interpretation.nition and interpretation engines are upgraded.
Thegrammars built in the early stages of developmentcan largely be re-used as interpretation rules.4.3 Amount of training data for SLM trainingFor the remaining Sections, we will use precisionand recall for simplicity.
We will discuss an ex-treme case where only a subset of 250 sentencesfrom the standard training set is used to train theSLM.
We have run experiments with two contexts:AppA MainMenu and AppB Origin.
These con-texts are useful because a) we have the human-generated truth and b) they represent extremes in thecomplexity of grammars (see Section 2).
On onehand, the grammars for AppA MainMenu can covera total of 350 unique sentences while AppB Origincan cover over 20k.
As the amount of trainingdata for the SLMs is reduced from 5000 down to250 sentences, the accuracy for AppA MainMenuis only perceptibly degraded for the wordSLM andclassSLM systems on the entire confidence domain(not shown here).
On the other hand, in the caseof the more complex grammar (class), it is a dif-ferent story which highlights a second regime.
ForAppB Origin, the precision and recall curve is pre-sented on Figure 3.
In the case of classSLM (left),810.70.750.80.850.90.9510  0.2  0.4  0.6  0.8  1PrecisionRecallRecall/Precisiongrammar-basedclassSLM - 5000classSLM - 2500.70.750.80.850.90.9510  0.2  0.4  0.6  0.8  1PrecisionRecallRecall/Precisiongrammar-basedwordSLM - 5000wordSLM - 250wordSLM - 250 - no backoffFigure 3: Precision and recall for the AppB Origin context as the amount of training data for the SLMs isreduced.
On the left, classSLM systems are presented; on the right it is the wordSLM.even with very little training data, the accuracy isfar better than the grammar-based system and onlyslightly degraded by reducing the size of the trainingset.
In the case of wordSLM (right), we can still seethat the accuracy is better than the grammar-basedsystem (refer to ?wordSLM - 250?
on the graph),but the reduction of training data has a much morevisible effect.
If we remove the sentences that weredrawn from the grammar-based system?s coverage(backoff - see Sec.
2.3), we can see that the drop inaccuracy is even more dramatic.4.4 Coverage of interpretation rules and priorsAs seen in Sec.
4.2, the classSLM results for AppAare disappointing.
They, however, shed some lighton two caveats of the robust approach describedhere.
The first caveat is the coverage of the interpre-tation rules.
As described in Sec.
2, the SLM-basedsystems?
training sets and interpretation rules (gram-mars from Table 1) were built in isolation.
This canhave a dramatic effect: after error analysis of theclassSLM system?s results, we noticed a large frac-tion of errors for which the recognized string was aclose (semantically identical) variant of a rule in theinterpretation engine (?cancellations?
vs ?cancella-tion?).
In response, we implemented a simple toolto increase the coverage of the grammars (and hencethe coverage of the interpretation rules) using the listof words seen in the training set.
The criteria for se-lection is based on common stem with a word in thegrammar.The second caveat is based on fact that theclassSLM suffers from a lack of prior informationonce the decoding process enters a specific classsince the grammars (class) do not contain priors.The wordSLM benefits from the full prior informa-tion all along the search.
We have solved this bytraining a small wordSLM within each grammar(class): for each grammar, the training set for thesmall wordSLM is composed of the set of fragmentsfrom all utterances in the main training set that firethat specific rule.
Note that this represents a wayto have the grammar-based and SLM-based systemsshare a common training set (Rayner et al 2005).In Figure 4, we show the effect of increasing thecoverage and adding priors in the grammars.
Thefirst conclusion comes in comparing the grammar-based results with and without increased coverage(enhanced+priors in figure) and priors.
We see thatthe ROC curves are one on top of the other.
The onlydifferences are: a) at low confidence where the en-hanced+priors version shows better precision, andb) the CPU consumption is greatly reduced (73ms?
52ms).
When the enhanced+priors version ofthe grammars (for classes and interpretation rules)is used in the context of the classSLM system, wecan see that there is a huge improvement in the accu-racy: this shows the importance of keeping the SLM820.70.750.80.850.90.9510  0.2  0.4  0.6  0.8  1PrecisionRecallRecall/Precisiongram.-based (73ms)gram.-based - enhanced+priors (52ms)classSLM (108ms)classSLM - enhanced+priors (79ms)Figure 4: ROC curves for AppA showing the ef-fect of increasing the grammar coverage and addingprior information in the grammars.and interpretation rules in-sync.
The final classSLMROC curve (Figure 4) is now comparable with itswordSLM counter-part (Figure 2 upper right graph).5 ConclusionWe have demonstrated in this paper that grammar-based systems for commercially deployed directeddialog applications targeted at the general publiccan be improved substantially by using SLMs withrobust parsing.
This conclusion is different than(Rayner et al 2005) and can be attributed to that factthat the general public is likely composed of a largeportion of na?
?ve users.
We have sketched a very sim-ple process to upgrade an application from using agrammar-based approach to a robust approach whenin-service data and interpretation rules (grammars)are available.
We have also shown that only a verysmall amount of data is necessary to train the SLMs(Knight et al 2001).
Class-based SLMs should befavored in the case where the amount of trainingdata is low while word-based SLMs should be usedwhen enough training data is available.
In the caseof non-overlapping classes, we have demonstratedthe soundness of automatically generated semantictruth.6 AcknowledgementsThe author would like to acknowledge the helpfuldiscussions with M. Fanty, R. Tremblay, R. Lacou-ture and K. Govindarajan during this project.ReferencesW.
Ward.
1990.
The CMU Air Travel Information Ser-vice: Understanding spontaneous speech .
Proc.
of theSpeech and Natural Language Workshop, Hidden Val-ley PA, pp.
127?129.A.L.
Gorin, B.A.
Parker, R.M.
Sachs and J.G.
Wilpon.1997.
How may I help you?.
Speech Communica-tions, 23(1):113?127.C.
Hemphill, J. Godfrey and G. Doddington.
1990.
TheATIS spoken language systems and pilot corpus.
Proc.of the Speech and Natural Language Workshop, Hid-den Valley PA, pp.
96?101.S.
Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-ing and I. Lewin.
2001.
Comparing grammar-basedand robust approaches to speech understanding: a casestudy.
Proc.
of EuroSpeech.M.
Rayner, P. Bouillon, N. Chatzichrisafis, B.A.
Hockey,M.
Santaholma, M. Starlander, H. Isahara, K. Kanzakiand Y. Nakao.
2005.
A methodology for comparinggrammar-based and robust approaches to speech un-derstanding.
Proc.
of EuroSpeech.L.
ten Bosch.
2005.
Improving out-of-coverage lan-guage modelling in a multimodal dialogue system us-ing small training sets.
Proc.
of EuroSpeech.M.
Balakrishna, C. Cerovic, D. Moldovan and E. Cave.2006.
Automatic generation of statistical languagemodels for interactive voice response applications.Proc.
of ICSLP.J.
Gillett and W. Ward.
1998.
A language model com-bining tri-grams and stochastic context-free grammars.Proc.
of ICSLP.F.
Jelinek.
1990.
Readings in speech recognition, Editedby A. Waibel and K.-F. Lee , pp.
450-506.
MorganKaufmann, Los Altos.W.
Xu and A. Rudnicky.
2000.
Language modeling fordialog system.
Proc.
of ICSLP.V.
Goel and R. Gopinath.
2006.
On designing contextsensitive language models for spoken dialog systems.Proc.
of ICSLP.83
