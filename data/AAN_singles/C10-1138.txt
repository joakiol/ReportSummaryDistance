Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1227?1235,Beijing, August 2010Contextual Modeling for Meeting Translation Using Unsupervised WordSense DisambiguationYang MeiDepartment of Electrical EngineeringUniversity of Washingtonyangmei@u.washington.eduKatrin KirchhoffDepartment of Electrical EngineeringUniversity of Washingtonkatrin@ee.washington.eduAbstractIn this paper we investigate the challengesof applying statistical machine translationto meeting conversations, with a particu-lar view towards analyzing the importanceof modeling contextual factors such as thelarger discourse context and topic/domaininformation on translation performance.We describe the collection of a small cor-pus of parallel meeting data, the develop-ment of a statistical machine translationsystem in the absence of genre-matchedtraining data, and we present a quantita-tive analysis of translation errors result-ing from the lack of contextual modelinginherent in standard statistical machinetranslation systems.
Finally, we demon-strate how the largest source of translationerrors (lack of topic/domain knowledge)can be addressed by applying document-level, unsupervised word sense disam-biguation, resulting in performance im-provements over the baseline system.1 IntroductionAlthough statistical machine translation (SMT)has made great progress over the last decade,most SMT research has focused on the transla-tion of structured input data, such as newswiretext or parliamentary proceedings.
Spoken lan-guage translation has mostly concentrated on two-person dialogues, such as travel expressions orpatient-provider interactions in the medical do-main.
Recently, more advanced spoken-languagedata has been addressed, such as speeches (Stu?keret al, 2007), lectures (Waibel and Fu?gen, 2008),and broadcast conversations (Zheng et al, 2008).Problems for machine translation in these genresinclude the nature of spontaneous speech input(e.g.
disfluencies, incomplete sentences, etc.)
andthe lack of high-quality training data.
Data thatmatch the desired type of spoken-language inter-action in topic, domain, and, most importantly, instyle, can only be obtained by transcribing andtranslating conversations, which is a costly andtime-consuming process.
Finally, many spoken-language interactions, especially those involvingmore than two speakers, rely heavily on the par-ticipants?
shared contextual knowledge about thedomain and topic of the discourse, relationshipsbetween speakers, objects in the real-world en-vironment, past interactions, etc.
These are typ-ically not modelled in standard SMT systems.The problem of speech disfluencies has beenaddressed by disfluency removal techniques thatare applied prior to translation (Rao et al, 2007;Wang et al, 2010).
Training data sparsity has beenaddressed by adding data from out-of-domain re-sources (e.g.
(Matusov et al, 2004; Hildebrandtet al, 2005; Wu et al, 2008)), exploiting com-parable rather than parallel corpora (Munteanuand Marcu, 2005), or paraphrasing techniques(Callison-Burch et al, 2006).
The lack of con-textual modeling, by contrast, has so far not beeninvestigated in depth, although it is a generallyrecognized problem in machine translation.
Earlyattempts at modeling contextual information inmachine translation include (Mima et al, 1998),where information about the role, rank and gen-der of speakers and listeners was utilized in atransfer-based spoken-language translation sys-tem for travel dialogs.
In (Kumar et al, 2008)1227statistically predicted dialog acts were used in aphrase-based SMT system for three different di-alog tasks and were shown to improve perfor-mance.
Recently, contextual source-language fea-tures have been incorporated into translation mod-els to predict translation phrases for traveling do-main tasks (Stroppa et al, 2007; Haque et al,2009).
However, we are not aware of any work ad-dressing contextual modeling for statistical trans-lation of spoken meeting-style interactions, notleast due to the lack of a relevant corpus.The first goal of this study is to provide a quan-titative analysis of the impact of the lack of con-textual modeling on translation performance.
Tothis end we have collected a small corpus of par-allel multi-party meeting data.
A baseline SMTsystem was trained for this corpus from freelyavailable data resources, and contextual transla-tion errors were manually analyzed with respectto the type of knowledge sources required to re-solve them.
Our analysis shows that the largesterror category consists of word sense disambigua-tion errors resulting from a lack of topic/domainmodeling.
In the second part of this study wetherefore present a statistical way of incorporat-ing such knowledge by using a graph-based unsu-pervised word sense disambiguation algorithm ata global (i.e.
document) level.
Our evaluation onreal-world meeting data shows that this techniqueimproves the translation performance slightly butconsistently with respect to position-independentword error rate (PER).2 Data2.1 Parallel Conversational DataFor our investigations we used a subset of the AMIcorpus (McCowan, 2005), which is a collection ofmulti-party meetings consisting of approximately100 hours of multimodal data (audio and videorecordings, slide images, data captured from dig-ital whiteboards, etc.)
with a variety of existingannotations (audio transcriptions, topic segmenta-tions, summaries, etc.).
Meetings were recordedin English and fall into two broad types: sce-nario meetings, where participants were asked toact out roles in a pre-defined scenario, and non-scenario meetings where participants were not re-stricted by role assignments.
In the first case, thescenario was a project meeting about the devel-opment of a new TV remote control; participantroles were project manager, industrial designer,marketing expert, etc.
The non-scenario meet-ings are about the move of an academic lab toa new location on campus.
The number of par-ticipants is four.
For our study we selected 10meetings (5 scenario meetings and 5 non-scenariomeetings) and had their audio transcriptions trans-lated into German (our chosen target language) bytwo native speakers each.
Translators were ableto simultaneously read the audio transcription ofthe meeting, view the video, and listen to the au-dio, when creating the translation.
The transla-tion guidelines were designed to obtain transla-tions that match the source text as closely as pos-sible in terms of style ?
for example, translatorswere asked to maintain the same level of collo-quial as opposed to formal language, and to gen-erally ensure that the translation was pragmati-cally adequate.
Obvious errors in the source text(e.g.
errors made by non-native English speak-ers among the meeting participants) were not ren-dered by equivalent errors in the German transla-tion but were corrected prior to translation.
Thefinal translations were reviewed for accuracy andthe data were filtered semi-automatically by elim-inating incomplete sentences, false starts, fillers,repetitions, etc.
Although these would certainlypose problems in a real-world application of spo-ken language translation, the goal of this studyis not to analyze the impact of speech-specificphenomena on translation performance (which, asdiscussed in Section 1, has been addressed be-fore) but to assess the impact of contextual infor-mation such as discourse and knowledge of thereal-world surroundings.
Finally, single-word ut-terances such as yeah, oh, no, sure, etc.
weredownsampled since they are trivial to translate andwere very frequent in the corpus; their inclusionwould therefore bias the development and tuningof the MT system towards these short utterancesat the expense of longer, more informative utter-ances.Table 1 shows the word counts of the trans-lated meetings after the preprocessing steps de-scribed above.
As an indicator of inter-translator1228ID type # utter.
# word S-BLEUES2008a S 224 2327 21.5IB4001 NS 419 3879 24.5IB4002 NS 447 3246 30.5IB4003 NS 476 5118 24.1IB4004 NS 593 5696 26.9IB4005 NS 381 4719 30.4IS1008a S 191 2058 25.8IS1008b S 353 3661 24.1IS1008c S 308 3351 19.6TS3005a S 245 2339 28.1Table 1: Sizes and symmetric BLEU scores fortranslated meetings from the AMI corpus (S = sce-nario meeting, NS = non-scenario meeting).agreement we computed the symmetric BLEU(S-BLEU) scores on the reference translations(i.e.
using one translation as the reference and theother as the hypothesis, then switching them andaveraging the results).
As we can see, scores arefairly low overall, indicating large variation in thetranslations.
This is due to (a) the nature of con-versational speech, and (b) the linguistic proper-ties of the target language.
Conversational datacontain a fair amount of colloquialisms, referen-tial expressions, etc.
that can be translated in a va-riety of ways.
Additionally, German as the targetlanguage permits many variations in word orderthat convey slight differences in emphasis, whichis turn is dependent on the translators?
interpreta-tion of the source sentence.
German also has richinflectional morphology that varies along with thechoice of words and word order (e.g.
verbal mor-phology depends on which subject is chosen).2.2 SMT System Training DataSince transcription and translation of multi-party spoken conversations is extremely time-consuming and costly, it is unlikely that parallelconversational data will ever be produced on a suf-ficiently large scale for a variety of different meet-ing types, topics, and target languages.
In order tomimic this situation we trained an initial English-German SMT system on freely available out-of-domain data resources.
We considered the follow-ing parallel corpora: news text (de-news1, 1.5Mwords), EU parliamentary proceedings (Europarl(Koehn, 2005), 24M words) and EU legal docu-ments (JRC Acquis2, 35M words), as well as twogeneric English-German machine-readable dictio-naries3,4 (672k and 140k entries, respectively).3 Translation SystemsWe trained a standard statistical phrase-basedEnglish-German translation system from the re-sources described above using Moses (Hoang andKoehn, 2008).
Individual language models weretrained for each data source and were then lin-early interpolated with weights optimized on thedevelopment set.
Similarly, individual phrase ta-bles were trained and were then combined into asingle table.
Binary indicator features were addedfor each phrase pair, indicating which data sourceit was extracted from.
Duplicated phrase pairswere merged into a single entry by averaging theirscores (geometric mean) over all duplicated en-tries.
The weights for binary indicator featureswere optimized along with all other standard fea-tures on the development set.
Our previous ex-perience showed that this method worked betterthan the two built-in features in Moses for han-dling multiple translation tables.
We found thatthe JRC corpus obtained very small weights; itwas therefore omitted from further system de-velopment.
Table 2 reports results from six dif-ferent systems: the first (System 1) is a systemthat only uses the parallel corpora but not theexternal dictionaries listed in Section 2.2.
Sys-tem 2 additionally uses the external dictionar-ies.
All systems use two meetings (IB4002 andIS1008b) as a development set for tuning modelparameters and five meetings for testing (IB4003-5,IS1008c,TS3005a).
For comparison we alsotrained a version of the system where a small in-domain data set (meetings ES2008a, IB4001, andIS1008a) was added to the training data (System3).
Finally, we also compared our performanceagainst Google Translate, which is a state-of-the-art statistical MT system with unconstrained ac-1www.iccs.inf.ed.ac.uk/?pkoehn/publications/de-news2http://wt.jrc.it/lt/Acquis/3http://www.dict.cc4http://www-user.tu-chemnitz.de/?fri/ding1229System descriptionDev set Eval setOOV (%) Trans.
Scores OOV (%) Trans.
ScoresEN DE BLEU PER EN DE BLEU PERSystem 1 OOD parallel data only 4.1 17.0 23.8 49.0 6.5 20.5 21.1 49.5System 2 System 1 + dictionaries 1.5 15.9 24.6 47.3 2.8 16.3 21.7 48.4System 3 System 1 + ID parallel data 3.5 13.4 24.7 47.2 5.8 19.7 21.9 48.3System 4 System 2 + ID parallel data 1.2 12.9 25.4 46.1 2.5 15.9 22.0 48.2System 5 System 4 + web data 1.2 12.8 26.0 45.9 2.5 15.8 22.1 48.1System 6 Google Translate ?
?
25.1 49.1 ?
?
23.7 50.8Table 2: System performance using out-of-domain (OOD) parallel data only vs. combination with asmall amount of in-domain (ID) data and generic dictionaries.
For each of the development (DEV)and evaluation (Eval) set, the table displays the percentages of unknown word types (OOV) for English(EN) and German (DE), as well as the translation scores of BLEU (%) and PER.cess to the web as training data (System 6).
Asexpected, translation performance is fairly poorcompared to the performance generally obtainedon more structured genres.
The use of exter-nal dictionaries helps primarily in reducing PERscores while BLEU scores are only improved no-ticeably by adding in-domain data.
System 6shows a more even performance across dev andeval sets than our trained system, which may re-flect some degree of overtuning of our systemsto the relatively small development set (about 7Kwords).
However, the PER scores of System 6 aresignificantly worse compared to our in-house sys-tems.In order to assess the impact of adding web dataspecifically collected to match our meeting corpuswe queried a web portal5 that searches a range ofEnglish-German bilingual web resources and re-turns parallel text in response to queries in eitherEnglish or German.
As queries we used Englishphrases from our development and evaluation setsthat (a) did not already have phrasal translationsin our phrase tables, (b) had a minimum lengthof four words, and (c) occurred at least twice inthe test data.
In those cases where the search en-gine returned results with an exact match on theEnglish side, we word-aligned the resulting paral-lel text (about 600k words) by training the wordalignment together with the news text corpus.
Wethen extracted new phrase pairs (about 3k) fromthe aligned data.
The phrasal scores assigned to5http://www.linguee.comthe new phrase pairs were set to 1; the lexicalscores were computed from a word lexicon trainedover both the baseline data resources and the par-allel web data.
However, results (Row 5 in Ta-ble 2) show that performance hardly improved,indicating the difficulty in finding matching datasources for conversational speech.Table 2 also shows the impact of different dataresources on the percentages of unknown wordtypes (OOV) for both the source and target lan-guages.
The use of external dictionaries gave thelargest reduction of OOV rates (System 1 vs. Sys-tem 2 and System 3 vs. System 4), followed by theuse of in-domain data (System 1 vs. System 3 andSystem 2 vs. System 4).
Since they were retrievedby multi-word query phrases, adding the web datadid not lead to significant reduction on the OOVrates (System 4 vs. System 5).Finally, we also explored a hierarchical phrase-based system as an alternative baseline system.The system was trained using the Joshua toolkit(Li et al, 2009) with the same word alignmentsand language models as were used in the standardphrase-based baseline system (System 4).
Afterextracting the phrasal (rule) tables for each datasource, they were combined into a single phrasal(rule) table using the same combination approachas for the basic phrase-based system.
However,the translation results (BLEU/PER of 24.0/46.6(dev) and 20.8/47.6 (eval), respectively) did notshow any improvement over the basic phrase-based system.12304 Analysis of Baseline Translations:Effect of Contextual InformationThe output from System 5 was analyzed manu-ally in order to assess the importance of model-ing contextual information.
Our goal was not todetermine how translation of meeting style datacan be improved in general ?
better translationscould certainly be generated by better syntacticmodeling, addressing morphological variation inGerman, and generally improving phrasal cover-age, in particular for sentences involving collo-quial expressions.
However, these are fairly gen-eral problems of SMT that have been studied pre-viously.
Instead, our goal was to determine therelative importance of modeling different contex-tual factors, such as discourse-level information orknowledge of the real-world environment, whichhave not been studied extensively.We considered three types of contextual in-formation: discourse coherence information (inparticular anaphoric relations), knowledge of thetopic or domain, and real-world/multimodal infor-mation.
Anaphoric relations affect the translationof referring expressions in cases where the sourceand target languages make different grammaticaldistinctions.
For example, German makes moremorphological distinctions in noun phrases thanEnglish.
In order to correctly translate an expres-sion like ?the red one?
the grammatical featuresof the target language expression for the referentneed to be known.
This is only possible if a suf-ficiently large context is taken into account dur-ing translation and if the reference is resolved cor-rectly.
Knowledge of the topic or domain is rele-vant for correctly translating content words and isclosely related to the problem of word sense dis-ambiguation.
In our current setup, topic/domainknowledge could be particularly helpful becausein-domain training data is lacking and many wordtranslations are obtained from generic dictionar-ies that do not assign probabilities to compet-ing translations.
Finally, knowledge of the real-world environment, such as objects in the room,other speakers present, etc.
determines translationchoices.
If a speaker utters the expression ?thatone?
while pointing to an object, the correct trans-lation might depend on the grammatical featuresError type % (dev) % (eval)Word sense 64.5 68.2Exophora (addressee) 24.3 23.4Anaphora 10.2 7.8Exophora (other) 1.0 0.6Table 3: Relative frequency of different errortypes involving contextual knowledge.
The totalnumber of errors is 715, for 315 sentences.of the linguistic expression for that object; e.g.
inGerman, the translation could be ?die da?, ?derda?
or ?das da?.
Since the participants in ourmeeting corpus use slides and supporting docu-ments we expect to see some effect of such ex-ophoric references to external objects.In order to quantify the influence of contextualinformation we manually analyzed the 1-best out-put of System 5, identified those translation errorsthat require knowledge of the topic/domain, largerdiscourse, or external environment for their res-olution, classified them into different categories,and computed their relative frequencies.
We thencorrected these errors in the translation output tomatch at least one of the human references, in or-der to assess the maximum possible improvementin standard performance scores that could be ob-tained from contextual modeling.
The results areshown in Tables 3 and 4.
We observe that out of allerrors that can be related to the lack of contextualknowledge, word sense confusions are by far themost frequent.
A smaller percentage of errors iscaused by anaphoric expressions.
Contrary to ourexpectations, we did not find a strong impact ofexophoric references; however, there is one cru-cial exception where real-world knowledge doesplay an important role.
This is the correct transla-tion of the addressee you.
In English, this form isused for the second person singular, second per-son plural, and the generic interpretation (as in?one?, or ?people?).
German has three distinctforms for these cases and, additionally, formal andinformal versions of the second-person pronouns.The required formal/informal pronouns can onlybe determined by prior knowledge of the rela-tionships among the meeting participants.
How-ever, the singular-plural-generic distinction canpotentially be resolved by multimodal informa-1231Original CorrectedBLEU (%) PER BLEU (%) PERdev 26.0 45.9 27.5 44.0eval 22.1 48.1 23.3 46.0Table 4: Scores obtained by correcting errors dueto lack of contextual knowledge.tion such as gaze, head turns, body movements,or hand gestures of the current speaker.
Sincethese errors affect mostly single words as opposedto larger phrases, the impact of the corrections onBLEU/PER scores is not large.
However, for prac-tical applications (e.g.
information extraction orhuman browsing of meeting translations) the cor-rect translation of content words and referring ex-pressions would be very important.
In the remain-der of the paper we therefore describe initial ex-periments designed to address the most importantsource of contextual errors, viz.
word sense con-fusions.5 Resolving Word Sense DisambiguationErrorsThe problem of word sense disambiguation(WSD) in MT has received a fair amount ofattention before.
Initial experiments designedat integrating a WSD component into an MTsystem (Carpuat and Wu, 2005) did not meetwith success; however, WSD was subsequentlydemonstrated to be successful in data-matchedconditions (Carpuat and Wu, 2007; Chan et al,2007).
The approach pursued by these latter ap-proaches is to train a supervised word sense clas-sifier on different phrase translation options pro-vided by the phrase table of an initial baseline sys-tem (i.e.
the task is to separate different phrasesenses rather than word senses).
The input fea-tures to the classifier consist of word features ob-tained from the immediate context of the phrasein questions, i.e.
from the same sentence or fromthe two or three preceding sentences.
The classi-fier is usually trained only for those phrases thatare sufficiently frequent in the training data.By contrast, our problem is quite different.First, many of the translation errors caused bychoosing the wrong word sense relate to wordsobtained from an external dictionary that do notoccur in the parallel training data; there is also lit-tle in-domain training data available in general.For these reasons, training a supervised WSDmodule is not an option without collecting addi-tional data.
Second, the relevant information forresolving a word sense distinction is often not lo-cated in the immediately surrounding context butit is either at a more distant location in the dis-course, or it is part of the participants?
backgroundknowledge.
For example, in many meetings theopening remarks refer to slides and an overheadprojector.
It is likely that subsequent mention-ing of slide later on during the conversation alsorefer to overhead slides (rather than e.g.
slide inthe sense of ?playground equipment?
), though thecontextual features that could be used to identifythis word sense are not located in the immedi-ately preceding sentences.
Thus, in contrast to su-pervised, local phrase sense disambiguation em-ployed in previous work, we propose to utilizeunsupervised, global word sense disambiguation,in order to obtain better modeling of the topicand domain knowledge that is implicitly presentin meeting conversations.5.1 Unsupervised Word SenseDisambiguationUnsupervised WSD algorithms have been pro-posed previously (e.g.
(Navigli and Lapata, 2007;Cheng et al, 2009)).
The general idea is to ex-ploit measures of word similarity or relatednessto jointly tag all words in a text with their correctsense.
We adopted the graph-based WSD methodproposed in (Sinha and Mihalcea, 2007), whichrepresents all word senses in a text as nodes in anundirected graph G = (V,E).
Pairs of nodes arelinked by edges weighted by scores indicating thesimilarity or relatedness of the words associatedwith the nodes.
Given such a graph, the likeli-hood of each node is derived by the PageRank al-gorithm (Brin and Page, 1998), which measuresthe relative importance of each node to the entiregraph by considering the amount of ?votes?
thenode receives from its neighboring nodes.
ThePageRank algorithm was originally designed fordirected graphs, but can be easily extended to anundirected graph.
Let PR(vi) denote the PageR-ank score of vi.
The PageRank algorithm itera-1232tively updates this score as follows:PR(vi) = (1 ?
d) + d?
(vi,vj)?EPR(vj)wij?k wkjwhere wij is the similarity weight of the undi-rected edge (vi, vj) and d is a damping factor,which is typically set to 0.85 (Brin and Page,1998).
The outcome of the PageRank algorithmis numerical weighting of each node in the graph.The sense with the highest score for each wordidentifies its most likely word sense.
For ourpurposes, we modified the procedure as follows.Given a document (meeting transcription), we firstidentify all content words in the source document.The graph is then built over all target-languagetranslation candidates, i.e.
each node represents aword translation.
Edges are then established be-tween all pairs of nodes for which a word similar-ity measure can be obtained.5.2 Word Similarity MeasuresWe follow (Zesch et al, 2008a) in computingthe semantic similarity of German words by ex-ploiting the Wikipedia and Wiktionary databases.We use the publicly available toolkits JWPL andJWKTL (Zesch et al, 2008b) to retrieve relevantarticles in Wikipedia and entries in Wiktionary foreach German word ?
these include the first para-graphs of Wikipedia articles entitled by the Ger-man word, the content of Wiktionary entries ofthe word itself as well as of closely related words(hypernyms, hyponyms, synonyms, etc.).
We thenconcatenate all retrieved material for each word toconstruct a pseudo-gloss.
We then lowercase andlemmatize the pseudo-glosses (using the lemma-tizer available in the TextGrid package 6), excludefunction words by applying a simple stop-wordlist, and compute a word similarity measure fora given pair of words by counting the number ofcommon words in their glosses.We need to point out that one drawback in thisapproach is the low coverage of German contentwords in the Wikipedia and Wiktionary databases.Although the English edition contains millionsof entries, the German edition of Wikipedia andWiktionary is much smaller ?
the coverage of allcontent words in our task ranges between 53% and6http://www.textgrid.de/en/beta.html56%, depending on the meeting, which leads tographs with roughly 3K to 5K nodes and 8M to13M edges.
Words that are not covered mostly in-clude rare words, technical terms, and compoundwords.5.3 Experiments and ResultsFor each meeting, the derived PageRank scoreswere converted into a positive valued feature, re-ferred to as the WSD feature, by normalizationand exponentiation:fWSD(wg|we) = exp{PR(wg)?wg?H(we) PR(wg)}where PR(wg) is the PageRank score for the Ger-man word wg and H(we) is the set of all transla-tion candidates for the English word we.
Sincethey are not modeled in the graph-based method,multi-words phrases and words that are not foundin the Wikipedia or Wiktionary databases will re-ceive the default value 1 for their WSD feature.The WSD feature was then integrated into thephrase table to perform translation.
The new sys-tem was optimized as before.It should be emphasized that the standard mea-sures of BLEU and PER give an inadequate im-pression of translation quality, in particular be-cause of the large variation among the referencetranslations, as discussed in Section 4.
In manycases, better word sense disambiguation does notresult in better BLEU scores (since higher grammatches are not affected) or even PER scoresbecause although a feasible translation has beenfound it does not match any words in the refer-ence translations.
The best way of evaluating theeffect of WSD is to obtain human judgments ?however, since translation hypotheses change withevery change to the system, our original error an-notation described in Section 4 cannot be re-used,and time and resource constraints prevented usfrom using manual evaluations at every step dur-ing system development.In order to loosen the restrictions imposed byhaving only two reference translations, we uti-lized a German thesaurus7 to automatically ex-tend the content words in the references with syn-onyms.
This can be seen as an automated way of7http://www.openthesaurus.de1233No WSD With WSDBLEU (%) PER XPER BLEU (%) PER XPERdev 25.4 46.1 43.4 25.4 45.6 42.9eval 22.0 48.2 44.6 22.0 47.9 44.0IB4003 21.4 48.3 44.4 21.4 47.5 43.8IB4004 22.4 48.5 44.4 23.1 48.4 43.9IB4005 25.4 45.9 42.4 25.3 45.6 42.2IS1008c 15.9 52.9 50.0 14.9 52.3 48.6TS3005a 23.1 45.2 41.9 23.2 45.3 41.7Table 5: Performance of systems with and without WSD for dev and eval sets as well as individualmeetings in the eval set.approximating the larger space of feasible trans-lations that could be obtained by producing addi-tional human references.
Note that the thesaurusprovided synonyms for only roughly 50% of allcontent words in the dev and eval set.
For eachof them, on average three synonyms are found inthe thesaurus.
We use these extended referencesto recompute the PER score as an indicator ofcorrect word selection.
All results (BLEU, PERand extended PER (or XPER)) are shown in Table5.
As expected, BLEU is not affected but WSDimproves the PER and XPER slightly but consis-tently.
Note that this is despite the fact that onlyroughly half of all content words received disam-biguation scores.Finally, we provide a concrete example oftranslation improvements, with improved wordshighlighted:Source:on the balconythere?s that terracethere?s no place inside the buildingTranslation, no WSD:auf dem balkones ist das absatzes gibt keinen platz innerhalb des geba?udesTranslation, with WSD:auf dem balkones ist das terrassees gibt keinen platz geba?udeinternReferences:auf dem balkon / auf dem balkonda gibt es die terrasse / da ist die terrassees gibt keinen platz im geba?ude / es gibt keinenplatz innen im geba?ude6 Summary and ConclusionsWe have presented a study on statistical transla-tion of meeting data that makes the following con-tributions: to our knowledge it presents the firstquantitative analysis of contextual factors in thestatistical translation of multi-party spoken meet-ings.
This analysis showed that the largest im-pact could be obtained in the area of word sensedisambiguation using topic and domain knowl-edge, followed by multimodal information to re-solve addressees of you.
Contrary to our ex-pectations, further knowledge of the real-worldenvironment (such as objects in the room) didnot show an effect on translation performance.Second, it demonstrates the application of unsu-pervised, global WSD to SMT, whereas previ-ous work has focused on supervised, local WSD.Third, it explores definitions derived from col-laborative Wiki sources (rather than WordNet orexisting dictionaries) for use in machine transla-tion.
We demonstrated small but consistent im-provements even though word coverage was in-complete.
Future work will be directed at improv-ing word coverage for the WSD algorithm, in-vestigating alternative word similarity measures,and exploring the combination of global and localWSD techniques.AcknowledgmentsThis work was funded by the National Science Foundationunder Grant IIS-0840461 and by a grant from the Univer-sity of Washington?s Provost Office.
Any opinions, findings,and conclusions or recommendations expressed in this mate-rial are those of the authors and do not necessarily reflect theviews of the funding organizations.1234ReferencesS.
Brin and L. Page.
1998.
?The Anatomy of a Large-Scale Hypertextual Web Search Engine?.
Proceed-ings of WWW7.C.
Callison-Burch, P. Koehn and M. Osborne.
2006.?Improved Statistical Machine Translation UsingParaphrases?.
Proceedings of NAACL.M.
Carpuat and D. Wu.
2005.
?Word sense disam-biguation vs. statistical machine translation?.
Pro-ceedings of ACL.M.
Carpuat and D. Wu.
2007.
?Improving statisticalmachine translation using word sense disambigua-tion?.
Proceedings of EMNLP-CoNLL.Y.S.
Chan and H.T.
Ng and D. Chiang 2007.
?Wordsense disambiguation improves statistical machinetranslation?.
Proceedings of ACL.P.
Chen, W. Ding, C. Bowes and D. Brown.
2009.?A fully unsupervised word sense disambiguationmethod using dependency knowledge?.
Proceed-ings of NAACL.E.
Gabrilovich and S. Markovitch.
2007 ?Computingsemantic relatedness usingWikipedia-based explicitsemantic analysis?.
Proceedings of IJCAI.R.
Haque, S.K.
Naskar, Y. Ma and A.Way.
2009.
?Us-ing supertags as source language context in SMT?.Proceedings of EAMT.A.S.
Hildebrandt, M. Eck, S. Vogel and A. Waibel.2005.
?Adaptation of the Translation Model for Sta-tistical Machine Translation using Information Re-trieval?.
Proceedings of EAMT.H.
Hoang and P. Koehn.
2008.
?Design of the Mosesdecoder for statistical machine translation?.
Pro-ceedings of SETQA-NLP.P.
Koehn.
2005.
?Europarl: a parallel corpus for statis-tical machine translation?.
Proceedings of MT Sum-mit.V.
Kumar, R. Sridhar, S. Narayanan and S. Bangalore.2008.
?Enriching spoken language translation withdialog acts?.
Proceedings of HLT.Z.
Li et al.
2009.
?Joshua: An Open Source Toolkitfor Parsing-based Machine Translation?.
Proceed-ings of StatMT.E.
Matusov, M.
Popovic?, R. Zens and H. Ney.
2004.?Statistical Machine Translation of SpontaneousSpeech with Scarce Resources?.
Proceedings ofIWSLT.A.
McCowan.
2005.
?The AMI meeting corpus?,H.
Mima, O. Furuse and H. Iida.
1998.
?ImprovingPerformance of Transfer-Driven Machine Transla-tion with Extra-Linguistic Information from Con-text, Situation and Environment?.
Proceedings ofColing.
Proceedings of the International Confer-ence on Methods and Techniques in Behavioral Re-search.D.S.
Munteanu and D. Marcu.
2005.
?Improvingmachine translation performance by exploiting non-parallel corpora?.
Computational Linguistics.R.
Navigli and M. Lapata.
2007.
?Graph Connectiv-ity Measures for Unsupervised Word Sense Disam-biguation?, Proceedings of IJCAIS.
Rao and I.
Lane and T. Schultz.
2007.
?Improvingspoken language translation by automatic disfluencyremoval?.
Proceedings of MT Summit.
31(4).R.
Sinha and R. Mihalcea.
2007.
?UnsupervisedGraph-based Word Sense Disambiguation UsingMeasures of Word Semantic Similarity?, Proceed-ings of IEEE-ICSCN.
Stroppa, A. Bosch and A.
Way.
2007.
?ExploitingSource Similarity for SMT using Context-InformedFeatures?.
Proceedings of TMI.S.
Stu?ker, C. Fu?gen, F. Kraft and M. Wo?lfel.
2007.?The ISL 2007 English Speech Transcription Sys-tem for European Parliament Speeches?.
Proceed-ings of Interspeech.A.
Waibel and C. Fu?gen.
2008.
?Spoken LanguageTranslation ?
Enabling cross-lingual human-humancommunication?.
Proceedings of Coling).W.
Wang, G. Tur, J. Zheng and N.F.
Ayan.
2010.?Automatic disfluency removal for improving spo-ken language translation?.
Proceedings of ICASSP.IEEE Signal Processing MagazineH.
Wu, H. Wang and C. Zong.
2008.
?Domain adapta-tion for statistical machine translation with domaindictionary and monolingual corpora?,T.
Zesch, C. Mu?ller and Iryna Gurevych.
2008.?Extracting Lexical Semantic Knowledge fromWikipedia and Wiktionary?.
Proceedings of LREC.T.
Zesch, Christof Mu?ler and Iryna Gurevych.
2008.?Using Wiktionary for Computing Semantic Relat-edness?.J.
Zheng, W. Wang and N.F.
Ayan.
2008.
?Devel-opment of SRI?s translation systems for broadcastnews and broadcast conversations?.
Proceedings ofInterspeech.
Proceedings of AAAI.1235
