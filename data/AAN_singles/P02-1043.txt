Generative Models for Statistical Parsing with Combinatory CategorialGrammarJulia Hockenmaier and Mark SteedmanDivision of InformaticsUniversity of EdinburghEdinburgh EH8 9LW, United Kingdomfjulia, steedmang@cogsci.ed.ac.ukAbstractThis paper compares a number of gen-erative probability models for a wide-coverage Combinatory Categorial Gram-mar (CCG) parser.
These models aretrained and tested on a corpus obtained bytranslating the Penn Treebank trees intoCCG normal-form derivations.
Accordingto an evaluation of unlabeled word-worddependencies, our best model achieves aperformance of 89.9%, comparable to thefigures given by Collins (1999) for a lin-guistically less expressive grammar.
Incontrast to Gildea (2001), we find a signif-icant improvement from modeling word-word dependencies.1 IntroductionThe currently best single-model statistical parser(Charniak, 1999) achieves Parseval scores of over89% on the Penn Treebank.
However, the grammarunderlying the Penn Treebank is very permissive,and a parser can do well on the standard Parsevalmeasures without committing itself on certain se-mantically significant decisions, such as predictingnull elements arising from deletion or movement.The potential benefit of wide-coverage parsing withCCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex-traction and coordination.We present a number of models over syntac-tic derivations of Combinatory Categorial Grammar(CCG, see Steedman (2000) and Clark et al (2002),this conference, for introduction), estimated fromand tested on a translation of the Penn Treebankto a corpus of CCG normal-form derivations.
CCGgrammars are characterized by much larger categorysets than standard Penn Treebank grammars, distin-guishing for example between many classes of verbswith different subcategorization frames.
As a re-sult, the categorial lexicon extracted for this purposefrom the training corpus has 1207 categories, com-pared with the 48 POS-tags of the Penn Treebank.On the other hand, grammar rules in CCG are lim-ited to a small number of simple unary and binarycombinatory schemata such as function applicationand composition.
This results in a smaller and lessovergenerating grammar than standard PCFGs (ca.3,000 rules when instantiated with the above cate-gories in sections 02-21, instead of >12,400 in theoriginal Treebank representation (Collins, 1999)).2 Evaluating a CCG parserSince CCG produces unary and binary branchingtrees with a very fine-grained category set, CCGParseval scores cannot be compared with scoresof standard Treebank parsers.
Therefore, we alsoevaluate performance using a dependency evalua-tion reported by Collins (1999), which counts word-word dependencies as determined by local trees andtheir labels.
According to this metric, a local treewith parent node P, head daughter H and non-headdaughter S (and position of S relative to P, ie.
leftor right, which is implicit in CCG categories) de-fines a hP;H;Si dependency between the head wordof S, wS, and the head word of H , wH.
This measureis neutral with respect to the branching factor.
Fur-thermore, as noted by Hockenmaier (2001), it doesnot penalize equivalent analyses of multiple modi-Computational Linguistics (ACL), Philadelphia, July 2002, pp.
335-342.Proceedings of the 40th Annual Meeting of the Association forPierre Vinken ; 61 years old ; will join the board as a nonexecutive director Nov 29N=N N ; N=N N (S[adj]nNP)nNP ; (S[dcl]nNP)=(S[b]nNP) ((S[b]nNP)=PP)=NP NP=N N PP=NP NP=N N=N N ((SnNP)n(SnNP))=N N> > > > >N N N N (SnNP)n(SnNP)>NP NP NP NP< > >NP S[adj]nNP (S[b]nNP)=PP PP>NPnNP S[b]nNP< <NP S[b]nNP>NP S[dcl]nNP<S[dcl]Figure 1: A CCG derivation in our corpusfiers.
In the unlabeled case hi (where it only matterswhether word a is a dependent of word b, not whatthe label of the local tree is which defines this depen-dency), scores can be compared across grammarswith different sets of labels and different kinds oftrees.
In order to compare our performance with theparser of Clark et al (2002), we also evaluate ourbest model according to the dependency evaluationintroduced for that parser.
For further discussion werefer the reader to Clark and Hockenmaier (2002) .3 CCGbank?a CCG treebankCCGbank is a corpus of CCG normal-form deriva-tions obtained by translating the Penn Tree-bank trees using an algorithm described byHockenmaier and Steedman (2002).
Almost alltypes of construction?with the exception of gap-ping and UCP (?Unlike Coordinate Phrases?)
arecovered by the translation procedure, which pro-cesses 98.3% of the sentences in the training corpus(WSJ sections 02-21) and 98.5% of the sentencesin the test corpus (WSJ section 23).
The grammarcontains a set of type-changing rules similar to thelexical rules described in Carpenter (1992).
Figure1 shows a derivation taken from CCGbank.
Cate-gories, such as ((S[b]nNP)=PP)=NP, encode unsat-urated subcat frames.
The complement-adjunct dis-tinction is made explicit; for instance as a nonexec-utive director is marked up as PP-CLR in the Tree-bank, and hence treated as a PP-complement of join,whereas Nov. 29 is marked up as an NP-TMP andtherefore analyzed as VP modifier.
The -CLR tagis not in fact a very reliable indicator of whether aconstituent should be treated as a complement, butthe translation to CCG is automatic and must do thebest it can with the information in the Treebank.The verbal categories in CCGbank carry fea-tures distinguishing declarative verbs (and auxil-iaries) from past participles in past tense, past par-ticiples for passive, bare infinitives and ing-forms.There is a separate level for nouns and noun phrases,but, like the nonterminal NP in the Penn Treebank,noun phrases do not carry any number agreement.The derivations in CCGbank are ?normal-form?
inthe sense that analyses involving the combinatoryrules of type-raising and composition are only usedwhen syntactically necessary.4 Generative models of CCG derivationsExpansion HeadCat NonHeadCatP(exp j : : : ) P(H j : : : ) P(S j : : : )Baseline P P;exp P;exp;H+ Conj P;con jP P;exp;con jP P;exp;H ;con jP+ Grandparent P;GP P;GP;exp P;GP;exp;H+ ?
P#?L;RP P;exp#?L;RP P;exp;H#?L;RPTable 1: The unlexicalized modelsThe models described here are all extensions ofa very simple model which models derivations by atop-down tree-generating process.
This model wasoriginally described in Hockenmaier (2001), whereit was applied to a preliminary version of CCGbank,and its definition is repeated here in the top row ofTable 1.
Given a (parent) node with category P,choose the expansion exp of P, where exp can beleaf (for lexical categories), unary (for unary ex-pansions such as type-raising), left (for binary treeswhere the head daughter is left) or right (binarytrees, head right).
If P is a leaf node, generate itshead word w. Otherwise, generate the category ofits head daughter H .
If P is binary branching, gen-erate the category of its non-head daughter S (acomplement or modifier of H).The model itself includes no prior knowledge spe-cific to CCG other than that it only allows unary andbinary branching trees, and that the sets of nontermi-nals and terminals are not disjoint (hence the need toinclude leaf as a possible expansion, which acts as astop probability).All the experiments reported in this section wereconducted using sections 02-21 of CCGbank astraining corpus, and section 23 as test corpus.
Wereplace all rare words in the training data with theirPOS-tag.
For all experiments reported here and insection 5, the frequency threshold was set to 5.
LikeCollins (1999), we assume that the test data is POS-tagged, and can therefore replace unknown words inthe test data with their POS-tag, which is more ap-propriate for a formalism like CCG with a large setof lexical categories than one generic token for allunknown words.The performance of the baseline model is shownin the top row of table 3.
For six out of the 2379sentences in our test corpus we do not get a parse.1The reason is that a lexicon consisting of the word-category pairs observed in the training corpus doesnot contain all the entries required to parse the testcorpus.
We discuss a simple, but imperfect, solutionto this problem in section 7.5 Extending the baseline modelState-of-the-art statistical parsers use many otherfeatures, or conditioning variables, such as headwords, subcategorization frames, distance measuresand grandparent nodes.
We too can extend thebaseline model described in the previous sectionby including more features.
Like the models ofGoodman (1997), the additional features in ourmodel are generated probabilistically, whereas inthe parser of Collins (1997) distance measures areassumed to be a function of the already generatedstructure and are not generated explicitly.In order to estimate the conditional probabilitiesof our model, we recursively smooth empirical es-timates e?i of specific conditional distributions with(possible smoothed) estimates of less specific distri-butions e?i 1, using linear interpolation:e?i = ?e?i +(1 ?)e?i 1?
is a smoothing weight which depends on the par-ticular distribution.2When defining models, we will indicate a back-off level with a # sign between conditioning vari-ables, eg.
A;B # C # D means that we interpolate?P(::: j A;B;C;D) with ?P(::: j A;B;C), which is an in-terpolation of ?P(::: j A;B;C) and ?P(::: j A;B).1We conjecture that the minor variations in coverage amongthe other models (except Grandparent) are artefacts of the beam.2We compute ?
in the same way as Collins (1999), p. 185.5.1 Adding non-lexical informationThe coordination feature We define a booleanfeature, conj, which is true for constituents whichexpand to coordinations on the head path.S, +conjS=NP, +conjS=NP,  conjS=(SnNP)NPIBM(SnNP)=NPbuysS=NP[c], +conjconjbutS=NP[c],  conjS=(SnNP)NPLotus(SnNP)=NPsellsNPsharesThis feature is generated at the root of the sentencewith P(conj j TOP).
For binary expansions, conjHis generated with P(conjH j H;S;con jP) and conjS isgenerated with P(conjS j S # P;expP;H;conjP).
Ta-ble 1 shows how conj is used as a conditioning vari-able.
This is intended to allow the model to cap-ture the fact that, for a sentence without extraction,a CCG derivation where the subject is type-raisedand composed with the verb is much more likely inright node raising constructions like the above.The impact of the grandparent featureJohnson (1998) showed that a PCFG estimatedfrom a version of the Penn Treebank in whichthe label of a node?s parent is attached to thenode?s own label yields a substantial improvement(LP/LR: from 73.5%/69.7% to 80.0%/79.2%).The inclusion of an additional grandparent featuregives Charniak (1999) a slight improvement in theMaximum Entropy inspired model, but a slightdecrease in performance for an MLE model.
Table3 (Grandparent) shows that a grammar transfor-mation like Johnson?s does yield an improvement,but not as dramatic as in the Treebank-CFG case.At the same time coverage is reduced (which mightnot be the case if this was an additional feature inthe model rather than a change in the representationof the categories).
Both of these results are to beexpected?CCG categories encode more contextualinformation than Treebank labels, in particularabout parents and grandparents; therefore the his-tory feature might be expected to have less impact.Moreover, since our category set is much larger,appending the parent node will lead to an even morefine-grained partitioning of the data, which thenresults in sparse data problems.Distance measures for CCG Our distance mea-sures are related to those proposed by Goodman(1997), which are appropriate for binary trees (un-like those of Collins (1997)).
Every node has a leftdistance measure, ?L, measuring the distance fromthe head word to the left frontier of the constituent.There is a similar right distance measure ?R.
Weimplemented three different ways of measuring dis-tance: ?Adjacency measures string adjacency (0, 1 or2 and more intervening words); ?Verb counts inter-vening verbs (0 or 1 and more); and ?Pct counts in-tervening punctuation marks (0, 1, 2 or 3 and more).These ?s are generated by the model in the follow-ing manner: at the root of the sentence, generate ?Lwith P(?L j TOP), and ?R with P(?R j TOP;?L).Then, for each expansion, if it is a unary expan-sion, ?LH = ?LP and ?RH = ?RP with a probabil-ity of 1.
If it is a binary expansion, only the ?
inthe direction of the sister changes, with a probabilityof P(?LH j ?LPH#P;S) if exp = right, and analo-gously for exp=left.
?LS and ?RS are conditionedon S and the ?
of H and P in the direction of S:P(?LS j S#?RP;?RH) and P(?RS j S;?LS#?RP;?RH).They are then used as further conditioning variablesfor the other distributions as shown in table 1.Table 3 also gives the Parseval and dependencyscores obtained with each of these measures.
?Pcthas the smallest effect.
However, our model doesnot yet contain anything like the hard constraint onpunctuation marks in Collins (1999).5.2 Adding lexical informationGildea (2001) shows that removing the lexical de-pendencies in Model 1 of Collins (1997) (that is,not conditioning on wh when generating ws) de-creases labeled precision and recall by only 0.5%.It can therefore be assumed that the main influenceof lexical head features (words and preterminals) inCollins?
Model 1 is on the structural probabilities.In CCG, by contrast, preterminals are lexical cat-egories, encoding complete subcategorization infor-mation.
They therefore encode more informationabout the expansion of a nonterminal than TreebankPOS-tags and thus are more constraining.Generating a constituent?s lexical category c at itsmaximal projection (ie.
either at the root of the tree,TOP, or when generating a non-head daughter S),and using the lexical category as conditioning vari-able (LexCat) increases performance of the baselinemodel as measured by hP;H;Si by almost 3%.
Inthis model, cS, the lexical category of S depends onthe category S and on the local tree in which S isgenerated.
However, slightly worse performance isobtained for LexCatDep, a model which is identicalto the original LexCat model, except that cS is alsoconditioned on cH , the lexical category of the headnode, which introduces a dependency between thelexical categories.Since there is so much information in the lexicalcategories, one might expect that this would reducethe effect of conditioning the expansion of a con-stituent on its head word w. However, we did find asubstantial effect.
Generating the head word at themaximal projection (HeadWord) increases perfor-mance by a further 2%.
Finally, conditioning wSon wH , hence including word-word dependencies,(HWDep) increases performance even more, by an-other 3.5%, or 8.3% overall.
This is in stark contrastto Gildea?s findings for Collins?
Model 1.We conjecture that the reason why CCG benefitsmore from word-word dependencies than Collins?Model 1 is that CCG allows a cleaner parametriza-tion of these surface dependencies.
In Collins?Model 1, wS is conditioned not only on the localtree hP;H;Si, cH and wH , but also on the distance ?between the head and the modifier to be generated.However, Model 1 does not incorporate the notionof subcategorization frames.
Instead, the distancemeasure was found to yield a good, if imperfect, ap-proximation to subcategorization information.Using our notation, Collins?
Model 1 generates wSwith the following probability:PCollins1(wS j cS;?
;P;H;S;cH;wH) =?1 ?P(wS j cS;?
;P;H;S;cH ;wH)+(1 ?1)?2 ?P(wS j cS;?
;P;H;S;cH)+(1 ?2) ?P(wS j cS)?whereas the CCG dependency model generateswS as follows:PCCGdep(wS j cS;P;H;S;cH ;wH) =?
?P(wS j cS;P;H;S;cH ;wH)+(1 ?)
?P(wS j cS)Since our P, H , S and cH are CCG categories, andhence encode subcategorization information, the lo-cal tree always identifies a specific argument slot.Therefore it is not necessary for us to include a dis-tance measure in the dependency probabilities.Expansion HeadCat NonHeadCat LexCat Head wordP(exp j :::) P(H j :::) P(S j :::) P(cS j :::) P(cTOPj:::) P(wS j :::) P(wTOP j:::)LexCat P;cP P;exp;cP P;exp;H#cP S#H;exp;P P=TOP ?
?LexCatDep P;cP P;exp;cP P;exp;H#cP S#H;exp;P#cP P=TOP ?
?HeadWord P;cP#wP P;exp;cP#wP P;exp;H#cP#wP S#H;exp;P P=TOP cS cPHWDep P;cP#wP P;exp;cP#wP P;exp;H#cP#wP S#H;exp;P P=TOP cS#P;H;S;wP cPHWDep?
P;cP#?L;RP#wP P;exp;cP#?L;RP#wP P;exp;H#?L;RP#cP#wP S#H;exp;P P=TOP cS#P;H;S;wP cPHWDepConj P;cP;conjP#wP P;exp;cP;conjP#wP P;exp;H;conjP#cP#wP S#H;exp;P P=TOP cS#P;H;S;wP cPTable 2: The lexicalized modelsModel NoParse LexCat LP LR BP BR hP;H;Si hSi hi CM on hi 2 CDBaseline 6 87.7 72.8 72.4 78,3 77.9 75.7 81.1 84.3 23.0 51.1Conj 9 87.8 73.8 73.9 79.3 79.3 76.7 82.0 85.1 24.3 53.2Grandparent 91 88.8 77.1 77.6 82.4 82.9 79.9 84.7 87.9 30.9 63.8?Pct 6 88.1 73.7 73.1 79.2 78.6 76.5 81.8 84.9 23.1 53.2?Verb 6 88.0 75.9 75.5 81.6 81.1 76.9 82.3 85.3 25.2 55.1?Adjacency 6 88.6 77.5 77.3 82.9 82.8 78.9 83.8 86.9 24.8 59.6LexCat 9 88.5 75.8 76.0 81.3 81.5 78.6 83.7 86.8 27.4 57.8LexCatDep 9 88.5 75.7 75.9 81.2 81.4 78.4 83.5 86.6 26.3 57.9HeadWord 8 89.6 77.9 78.0 83.0 83.1 80.5 85.2 88.3 30.4 63.0HWDep 8 92.0 81.6 81.9 85.5 85.9 84.0 87.8 90.1 37.9 69.2HWDep?
8 90.9 81.4 81.6 86.1 86.3 83.0 87.0 89.8 35.7 68.7HWDepConj 9 91.8 80.7 81.2 84.8 85.3 83.6 87.5 89.9 36.5 68.6HWDep (+ tagger) 7 91.7 81.4 81.8 85.6 85.9 83.6 87.5 89.9 38.1 69.1Table 3: Performance of the models: LexCat indicates accuracy of the lexical categories; LP, LR, BP andBR (the standard Parseval scores labeled/bracketed precision and recall) are not commensurate with otherTreebank parsers.
hP;H;Si, hSi, and hi are as defined in section 2.
CM on hi is the percentage of sentenceswith complete match on hi, and 2 CD is the percentage of sentences with under 2 ?crossing dependencies?as defined by hi.The hP;H;Si labeled dependencies we report arenot directly comparable with Collins (1999), sinceCCG categories encode subcategorization frames.For instance, if the direct object of a verb has beenrecognized as such, but a PP has been mistaken asa complement (whereas the gold standard says itis an adjunct), the fully labeled dependency eval-uation hP;H;Si will not award a point.
Therefore,we also include in Table 3 a more comparable eval-uation hSi which only takes the correctness of thenon-head category into account.
The reported fig-ures are also deflated by retaining verb features liketensed/untensed.
If this is done (by stripping offall verb features), an improvement of 0.6% on thehP;H;Si score for our best model is obtained.5.3 Combining lexical and non-lexicalinformationWhen incorporating the adjacency distance mea-sure or the coordination feature into the dependencymodel (HWDep?
and HWDepConj), overall per-formance is lower than with the dependency modelalone.
We conjecture that this arises from datasparseness.
It cannot be concluded from these re-sults alone that the lexical dependencies make struc-tural information redundant or superfluous.
Instead,it is quite likely that we are facing an estimationproblem similar to Charniak (1999), who reportsthat the inclusion of the grandparent feature worsensperformance of an MLE model, but improves per-formance if the individual distributions are modelledusing Maximum Entropy.
This intuition is strength-ened by the fact that, on casual inspection of thescores for individual sentences, it is sometimes thecase that the lexicalized models perform worse thanthe unlexicalized models.5.4 The impact of tagging errorsAll of the experiments described above use the POS-tags as given by CCGbank (which are the Treebanktags, with some corrections necessary to acquire cor-rect features on categories).
It is reasonable to as-sume that this input is of higher quality than canbe produced by a POS-tagger.
We therefore ran thedependency model on a test corpus tagged with thePOS-tagger of Ratnaparkhi (1996), which is trainedon the original Penn Treebank (see HWDep (+ tag-ger) in Table 3).
Performance degrades slightly,which is to be expected, since our approach makesso much use of the POS-tag information for un-known words.
However, a POS-tagger trained onCCGbank might yield slightly better results.5.5 Limitations of the current modelUnlike Clark et al (2002), our parser does not al-ways model the dependencies in the logical form.For example, in the interpretation of a coordinatestructure like ?buy and sell shares?, shares will headan object of both buy and sell.
Similarly, in exampleslike ?buy the company that wins?, the relative con-struction makes company depend upon both buy asobject and wins as subject.
As is well known (Ab-ney, 1997), DAG-like dependencies cannot in gen-eral be modeled with a generative approach of thekind taken here3.5.6 Comparison with Clark et al (2002)Clark et al (2002) presents another statistical CCGparser, which is based on a conditional (ratherthan generative) model of the derived depen-dency structure, including non-surface dependen-cies.
The following table compares the two parsersaccording to the evaluation of surface and deepdependencies given in Clark et al (2002).
Weuse Clark et al?s parser to generate these de-pendencies from the output of our parser (seeClark and Hockenmaier (2002)) 4.LP LR UP URClark 81.9% 81.8% 89.1% 90.1%Hockenmaier 83.7% 84.2% 90.5% 91.1%6 Performance on specific constructionsOne of the advantages of CCG is that it provides asimple, surface grammatical analysis of extractionand coordination.
We investigate whether our best3It remains to be seen whether the more restricted reentran-cies of CCG will ultimately support a generative model.4Due to the smaller grammar and lexicon of Clark et al, ourparser can only be evaluated on slightly over 94% of the sen-tences in section 23, whereas the figures for Clark et al (2002)are on 97%.model, HWDep, predicts the correct analyses, usingthe development section 00.Coordination There are two instances of argu-ment cluster coordination (constructions like cost$5,000 in July and $6,000 in August) in the devel-opment corpus.
Of these, HWDep recovers nonecorrectly.
This is a shortcoming in the model, ratherthan in CCG: the relatively high probability both ofthe NP modifier analysis of PPs like in July and ofNP coordination is enough to misdirect the parser.There are 203 instances of verb phrase coordina-tion (S[:]nNP, with [:] any verbal feature) in the de-velopment corpus.
On these, we obtain a labeled re-call and precision of 67.0%/67.3%.
Interestingly, onthe 24 instances of right node raising (coordinationof (S[:]nNP)=NP), our parser achieves higher per-formance, with labeled recall and precision of 79.2%and 73.1%.
Figure 2 gives an example of the outputof our parser on such a sentence.Extraction Long-range dependencies are not cap-tured by the evaluation used here.
However, the ac-curacy for recovering lexical categories for wordswith ?extraction?
categories, such as relative pro-nouns, gives some indication of how well the modeldetects the presence of such dependencies.The most common category for subject relativepronouns, (NPnNP)=(S[dcl]nNP), has been recov-ered with precision and recall of 97.1% (232 out of239) and 94.3% (232/246).Embedded subject extraction requires the speciallexical category ((S[dcl]nNP)=NP)=(S[dcl]nNP)for verbs like think.
On this category, the modelachieves a precision of 100% (5/5) and recall of83.3% (5/6).
The case the parser misanalyzed is dueto lexical coverage: the verb agree occurs in our lex-icon, but not with this category.The most common category for object relativepronouns, (NPnNP)=(S[dcl]=NP), has a recall of76.2% (16 out of 21) and precision of 84.2% (16/19).Free object relatives, NP=(S[dcl]=NP), have arecall of 84.6% (11/13), and precision of 91.7%(11/12).
However, object extraction appears morefrequently as a reduced relative (the man John saw),and there are no lexical categories indicating this ex-traction.
Reduced relative clauses are captured by atype-changing rule NPnNP !
S[dcl]=NP.
This rulewas applied 56 times in the gold standard, and 70S[dcl]NPthe suitS[dcl]nNPS[dcl]nNP(S[dcl]nNP)=NPseeksNPa court order(SnNP)n(SnNP)S[ng]nNP(S[ng]nNP)=PP((S[ng]nNP)=PP)=NPpreventingNPthe guildPPPP=(S[ng]nNP)fromS[ng]nNP(S[ng]nNP)=NP(S[ng]nNP)=NPpunishing(S[ng]nNP)=NP[c]conjor(S[ng]nNP)=NP(S[ng]nNP)=PPretaliatingPP=NPagainstNPMr: TrudeauFigure 2: Right node raising output produced by our parser.
Punishing and retaliating are unknown words.times by the parser, out of which 48 times it corre-sponded to a rule in the gold standard (or 34 times,if the exact bracketing of the S[dcl]=NP is taken intoaccount?this lower figure is due to attachment de-cisions made elsewhere in the tree).These figures are difficult to compare with stan-dard Treebank parsers.
Despite the fact that theoriginal Treebank does contain traces for move-ment, none of the existing parsers try to gener-ate these traces (with the exception of Collins?Model 3, for which he only gives an overall scoreof 96.3%/98.8% P/R for subject extraction and81.4%/59.4% P/R for other cases).
The only ?longrange?
dependency for which Collins gives numbersis subject extraction hSBAR, WHNP, SG, Ri, whichhas labeled precision and recall of 90.56% and90.56%, whereas the CCG model achieves a labeledprecision and recall of 94.3% and 96.5% on the mostfrequency subject extraction dependency hNPnNP,(NPnNP)=(S[dcl]nNP), S[dcl]nNPi, which occurs262 times in the gold standard and was produced256 times by our parser.
However, out of the15 cases of this relation in the gold standard thatour parser did not return, 8 were in fact analyzedas subject extraction of bare infinitivals hNPnNP,(NPnNP)=(S[b]nNP), S[b]nNPi, yielding a com-bined recall of 97.3%.7 Lexical coverageThe most serious problem facing parsers like thepresent one with large category sets is not so muchthe standard problem of unseen words, but rather theproblem of words that have been seen, but not withthe necessary category.For standard Treebank parsers, the latter problemdoes not have much impact, if any, since the PennTreebank tagset is fairly small, and the grammar un-derlying the Treebank is very permissive.
However,for CCG this is a serious problem: the first threerows in table 4 show a significant difference in per-formance for sentences with complete lexical cover-age (?No missing?)
and sentences with missing lex-ical entries (?Missing?
).Using the POS-tags in the corpus, we can estimatethe lexical probabilities P(w j c) using a linear in-terpolation between the relative frequency estimates?P(w j c) and the following approximation:5?Ptags(w j c) = ?
t2tags ?P(w j t) ?P(t j c)We smooth the lexical probabilities as follows:?P(w j c) = ?
?P(w j c)+(1 ?)
?Ptags(w j c)Table 4 shows the performance of the baselinemodel with a frequency cutoff of 5 and 10 for rarewords and with a smoothed and non-smoothed lexi-con.6 This frequency cutoff plays an important rolehere - smoothing with a small cutoff yields worseperformance than not smoothing, whereas smooth-ing with a cutoff of 10 does not have a significantimpact on performance.
Smoothing the lexicon inthis way does make the parser more robust, result-ing in complete coverage of the test set.
However, itdoes not affect overall performance, nor does it alle-viate the problem for sentences with missing lexicalentries for seen words.5We compute ?
in the same way as Collins (1999), p. 185.6Smoothing was only done for categories with a total fre-quency of 100 or more.Baseline, Cutoff = 5 Baseline, Cutoff = 10 HWDep, Cutoff = 10(Missing = 463 sentences) (Missing = 387 sentences) (Missing = 387 sentences)Non-smoothed Smoothed Non-smoothed Smoothed SmoothedParse failures 6 ?
5 ?
?hP;H;Si, All 75.7 73.2 76.2 76.3 83.9hP;H;Si, Missing 66.4 64.2 67.0 67.1 75.1hP;H;Si, No missing 78.5 75.9 78.5 78.6 86.6Table 4: The impact of lexical coverage, using a different cutoff for rare words and smoothing (section 23)8 Conclusion and future workWe have compared a number of generative probabil-ity models of CCG derivations, and shown that ourbest model recovers 89.9% of word-word dependen-cies on section 23 of CCGbank.
On section 00, itrecovers 89.7% of word-word dependencies.
Thesefigures are surprisingly close to the figure of 90.9%reported by Collins (1999) on section 00, given that,in order to allow a direct comparison, we have usedthe same interpolation technique and beam strategyas Collins (1999), which are very unlikely to be aswell-tuned to our kind of grammar.As is to be expected, a statistical model of a CCGextracted from the Treebank is less robust than amodel with an overly permissive grammar such asCollins (1999).
This problem seems to stem mainlyfrom the incomplete coverage of the lexicon.
Wehave shown that smoothing can compensate for en-tirely unknown words.
However, this approach doesnot help on sentences which require previously un-seen entries for known words.
We would expect aless naive approach such as applying morphologi-cal rules to the observed entries, together with bettersmoothing techniques, to yield better results.We have also shown that a statistical model ofCCG benefits from word-word dependencies to amuch greater extent than a less linguistically moti-vated model such as Collins?
Model 1.
This indi-cates to us that, although the task faced by a CCGparser might seem harder prima facie, there areadvantages to using a more linguistically adequategrammar.AcknowledgementsThanks to Stephen Clark, Miles Osborne and theACL-02 referees for comments.
Various parts of theresearch were funded by EPSRC grants GR/M96889and GR/R02450 and an EPSRC studentship.ReferencesSteven Abney.
1997.
Stochastic Attribute-Value Grammars.Computational Linguistics, 23(4).Bob Carpenter.
1992.
Categorial Grammars, Lexical Rules,and the English Predicative.
In R. Levine, ed., FormalGrammar: Theory and Implementation.
OUP.Eugene Charniak.
1999.
A Maximum-Entropy-Inspired Parser.TR CS-99-12, Brown University.David Chiang.
2000.
Statistical Parsing with an Automatically-Extracted Tree Adjoining Grammar 38th ACL, Hong Kong,pp.
456-463.Stephen Clark and Julia Hockenmaier.
2002.
Evaluating aWide-Coverage CCG Parser.
LREC Beyond PARSEVALworkshop, Las Palmas, Spain.Stephen Clark, Julia Hockenmaier, and Mark Steedman.2002.
Building Deep Dependency Structures Using a Wide-Coverage CCG Parser.
40th ACL, Philadelphia.Michael Collins.
1997.
Three Generative Lexicalized Modelsfor Statistical Parsing.
35th ACL, Madrid, pp.
16?23.Michael Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, University ofPennsylvania.Daniel Gildea.
2001.
Corpus Variation and Parser Perfor-mance.
EMNLP, Pittsburgh, PA.Julia Hockenmaier.
2001.
Statistical Parsing for CCG withSimple Generative Models.
Student Workshop, 39th ACL/10th EACL, Toulouse, France, pp.
7?12.Julia Hockenmaier and Mark Steedman 2002.
Acquiring Com-pact Lexicalized Grammars from a Cleaner Treebank.
ThirdLREC, Las Palmas, Spain.Joshua Goodman.
1997.
Probabilistic Feature Grammars.IWPT, Boston.Mark Johnson.
1998.
PCFG Models of Linguistic Tree Repre-sentations.
Computational Linguistics, 24(4).Adwait Ratnaparkhi.
1996.
A Maximum Entropy Part-Of-Speech Tagger.
EMNLP, Philadelphia, pp.
133?142.Mark Steedman.
2000.
The Syntactic Process.
The MIT Press,Cambridge Mass.
