Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2358?2367,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsA Thorough Examination of theCNN/Daily Mail Reading Comprehension TaskDanqi Chen and Jason Bolton and Christopher D. ManningComputer Science Stanford UniversityStanford, CA 94305-9020, USA{danqi,jebolton,manning}@cs.stanford.eduAbstractEnabling a computer to understand a docu-ment so that it can answer comprehensionquestions is a central, yet unsolved goalof NLP.
A key factor impeding its solu-tion by machine learned systems is the lim-ited availability of human-annotated data.Hermann et al (2015) seek to solve thisproblem by creating over a million trainingexamples by pairing CNN and Daily Mailnews articles with their summarized bulletpoints, and show that a neural network canthen be trained to give good performanceon this task.
In this paper, we conduct athorough examination of this new readingcomprehension task.
Our primary aim isto understand what depth of language un-derstanding is required to do well on thistask.
We approach this from one side bydoing a careful hand-analysis of a smallsubset of the problems and from the otherby showing that simple, carefully designedsystems can obtain accuracies of 72.4% and75.8% on these two datasets, exceeding cur-rent state-of-the-art results by over 5% andapproaching what we believe is the ceilingfor performance on this task.11 IntroductionReading comprehension (RC) is the ability to readtext, process it, and understand its meaning.2 Howto endow computers with this capacity has been anelusive challenge and a long-standing goal of Arti-ficial Intelligence (e.g., (Norvig, 1978)).
Genuinereading comprehension involves interpretation of1Our code is available at https://github.com/danqi/rc-cnn-dailymail.2https://en.wikipedia.org/wiki/Reading_comprehensionthe text and making complex inferences.
Humanreading comprehension is often tested by askingquestions that require interpretive understandingof a passage, and the same approach has beensuggested for testing computers (Burges, 2013).In recent years, there have been several strands ofwork which attempt to collect human-labeled datafor this task ?
in the form of document, question andanswer triples ?
and to learn machine learning mod-els directly from it (Richardson et al, 2013; Berantet al, 2014; Wang et al, 2015).
However, thesedatasets consist of only hundreds of documents, asthe labeled examples usually require considerableexpertise and neat design, making the annotationprocess quite expensive.
The subsequent scarcity oflabeled examples prevents us from training power-ful statistical models, such as deep learning models,and would seem to prevent a system from learningcomplex textual reasoning capacities.Recently, researchers at DeepMind (Hermannet al, 2015) had the appealing, original idea ofexploiting the fact that the abundant news articlesof CNN and Daily Mail are accompanied by bulletpoint summaries in order to heuristically createlarge-scale supervised training data for the readingcomprehension task.
Figure 1 gives an example.Their idea is that a bullet point usually summarizesone or several aspects of the article.
If the computerunderstands the content of the article, it should beable to infer the missing entity in the bullet point.This is a clever way of creating supervised datacheaply and holds promise for making progress ontraining RC models; however, it is unclear whatlevel of reading comprehension is actually neededto solve this somewhat artificial task and, indeed,what statistical models that do reasonably well onthis task have actually learned.In this paper, our aim is to provide an in-depthand thoughtful analysis of this dataset and whatlevel of natural language understanding is needed to2358( @entity4 ) if you feel a ripple in the force today , it may be the news that the official @entity6 is getting its first gay character .
according to the sci-fi website @entity9 , the upcoming novel " @entity11 " will feature a capable but flawed @entity13 official named @entity14 who " also happens to be a lesbian . "
the character is the first gay figure in the official @entity6 -- the movies , television shows , comics and books approved by @entity6 franchise owner @entity22 -- according to @entity24 , editor of " @entity6 " books at @entity28 imprint @entity26 .PassageQuestioncharacters in " @placeholder " movies have gradually become more diverseAnswer@entity6Figure 1: An example item from dataset CNN.do well on it.
We demonstrate that simple, carefullydesigned systems can obtain high, state-of-the-artaccuracies of 72.4% and 75.8% on CNN and DailyMail respectively.
We do a careful hand-analysisof a small subset of the problems to provide dataon their difficulty and what kinds of language un-derstanding are needed to be successful and we tryto diagnose what is learned by the systems that wehave built.
We conclude that: (i) this dataset iseasier than previously realized, (ii) straightforward,conventional NLP systems can do much better onit than previously suggested, (iii) the distributedrepresentations of deep learning systems are veryeffective at recognizing paraphrases, (iv) partlybecause of the nature of the questions, current sys-tems much more have the nature of single-sentencerelation extraction systems than larger-discourse-context text understanding systems, (v) the systemsthat we present here are close to the ceiling ofperformance for single-sentence and unambiguouscases of this dataset, and (vi) the prospects for get-ting the final 20% of questions correct appear poor,since most of them involve issues in the data prepa-ration which undermine the chances of answeringthe question (coreference errors or anonymizationof entities making understanding too difficult).2 The Reading Comprehension TaskThe RC datasets introduced in (Hermann et al,2015) are made from articles on the news websitesCNN and Daily Mail, utilizing articles and theirbullet point summaries.3 Figure 1 demonstrates3The datasets are available at https://github.com/deepmind/rc-data.an example4: it consists of a passage p, a questionq and an answer a, where the passage is a newsarticle, the question is a cloze-style task, in whichone of the article?s bullet points has had one entityreplaced by a placeholder, and the answer is thisquestioned entity.
The goal is to infer the missingentity (answer a) from all the possible entities whichappear in the passage.
A news article is usuallyassociated with a few (e.g., 3?5) bullet points andeach of them highlights one aspect of its content.The text has been run through a Google NLPpipeline.
It it tokenized, lowercased, and namedentity recognition and coreference resolution havebeen run.
For each coreference chain containingat least one named entity, all items in the chain arereplaced by an @entitynmarker, for a distinct indexn.
Hermann et al (2015) argue convincingly thatsuch a strategy is necessary to ensure that systemsapproach this task by understanding the passage infront of them, rather than by using world knowledgeor a language model to answer questions withoutneeding to understand the passage.
However, thisalso gives the task a somewhat artificial character.On the one hand, systems are greatly helped byentity recognition and coreference having alreadybeen performed; on the other, they suffer wheneither of these modules fail, as they do (in Figure 1,?the character?
should probably be coreferent with@entity14; clearer examples of failure appear lateron in our data analysis).
Moreover, this inabilityto use world knowledge also makes it much moredifficult for a human to do this task ?
occasionallyit is very difficult or impossible for a human todetermine the correct answer when presented withan item anonymized in this way.The creation of the datasets benefits from thesheer volume of news articles available online, sothey offer a large and realistic testing ground for sta-tistical models.
Table 1 provides some statistics onthe two datasets: there are 380k and 879k trainingexamples forCNN andDailyMail respectively.
Thepassages are around 30 sentences and 800 tokenson average, while each question contains around12?14 tokens.In the following sections, we seek to more deeplyunderstand the nature of this dataset.
We first buildsome straightforward systems in order to get a betteridea of a lower-bound for the performance of currentNLP systems.
Then we turn to data analysis of a4The original article can be found at http://www.cnn.com/2015/03/10/entertainment/feat-star-wars-gay-character/.2359CNN Daily Mail# Train 380,298 879,450# Dev 3,924 64,835# Test 3,198 53,182Passage: avg.
tokens 761.8 813.1Passage: avg.
sentences 32.3 28.9Question: avg.
tokens 12.5 14.3Avg.
# entities 26.2 26.2Table 1: Data statistics of the CNN and DailyMail datasets.
The avg.
tokens and sentences inthe passage, the avg.
tokens in the query, and thenumber of entities are based on statistics from thetraining set, but they are similar on the developmentand test sets.sample of the items to examine their nature and anupper bound on performance.3 Our SystemsIn this section, we describe two systems we im-plemented ?
a conventional entity-centric classifierand an end-to-end neural network.
While Hermannet al (2015) do provide several baselines for per-formance on the RC task, we suspect that theirbaselines are not that strong.
They attempt to usea frame-semantic parser, and we feel that the poorcoverage of that parser undermines the results, andis not representative of what a straightforward NLPsystem ?
based on standard approaches to factoidquestion answering and relation extraction devel-oped over the last 15 years ?
can achieve.
Indeed,their frame-semantic model is markedly inferiorto another baseline they provide, a heuristic worddistance model.
At present just two papers areavailable presenting results on this RC task, bothpresenting neural network approaches: (Hermannet al, 2015) and (Hill et al, 2016).
While the latteris wrapped in the language of end-to-end mem-ory networks, it actually presents a fairly simplewindow-based neural network classifier running onthe CNN data.
Its success again raises questionsabout the true nature and complexity of the RCtask provided by this dataset, which we seek toclarify by building a simple attention-based neuralnet classifier.Given the (passage, question, answer) triple(p, q, a), p = {p1, .
.
.
, pm } and q = {q1, .
.
.
, ql } aresequences of tokens for the passage and questionsentence, with q containing exactly one ?@place-holder?
token.
The goal is to infer the correctentity a ?
p ?
E that the placeholder correspondsto, where E is the set of all abstract entity markers.Note that the correct answer entity must appear inthe passage p.3.1 Entity-Centric ClassifierWefirst build a conventional feature-based classifier,aiming to explore what features are effective for thistask.
This is similar in spirit to (Wang et al, 2015),which at present has very competitive performanceon the MCTest RC dataset (Richardson et al, 2013).The setup of this system is to design a feature vectorf p,q (e) for each candidate entity e, and to learna weight vector ?
such that the correct answer ais expected to rank higher than all other candidateentities:?
?f p,q (a) > ?
?f p,q (e),?e ?
E ?
p \ {a} (1)We employ the following feature templates:1.
Whether entity e occurs in the passage.2.
Whether entity e occurs in the question.3.
The frequency of entity e in the passage.4.
The first position of occurence of entity e in thepassage.5.
n-gram exact match: whether there is an exactmatch between the text surrounding the place-holder and the text surrounding entity e. Wehave features for all combinations of matchingleft and/or right one or two words.6.
Word distance: we align the placeholder witheach occurrence of entity e, and compute theaverage minimum distance of each non-stopquestion word from the entity in the passage.7.
Sentence co-occurrence: whether entity e co-occurs with another entity or verb that appearsin the question, in some sentence of the passage.8.
Dependency parse match: we dependency parseboth the question and all the sentences in thepassage, and extract an indicator feature ofwhether wr??
@placeholder and wr??
e areboth found; similar features are constructed for@placeholderr??
w and er??
w.3.2 End-to-end Neural NetworkOur neural network system is based on the Attentive-Reader model proposed by (Hermann et al, 2015).The framework can be described in the followingthree steps (see Figure 2):2360( @entity4 ) if you feel a ripple in the force today , it may be the news that the official @entity6 is getting its first gay character .
according to the sci-fi website @entity9 , the upcoming novel " @entity11 " will feature a capable but flawed @entity13 official named @entity14 who " also happens to be a lesbian . "
the character is the first gay figure in the official @entity6 -- the movies , television shows , comics and books approved by @entity6 franchise owner @entity22 -- according to @entity24 , editor of " @entity6 " books at @entity28 imprint @entity26 .PassageQuestioncharacters in " @placeholder " movies have gradually become more diverse Answer@entity6?
?
?characters in " @placeholder " movies have gradually become more diverseassageQuestionentity6AnswerFigure 2: Our neural network architecture for the reading comprehension task.Encoding: First, all the words are mapped to d-dimensional vectors via an embedding ma-trix E ?
Rd?|V |; therefore we have p:p1, .
.
.
, pm ?
Rdand q : q1, .
.
.
, ql ?
Rd.Next we use a shallow bi-directional LSTMwith hidden size?h to encode contextual em-beddings ?pi of each word in the passage,?
?h i = LSTM(?
?h i?1, pi ), i = 1, .
.
.
,m?
?h i = LSTM(?
?h i+1, pi ), i = m, .
.
.
, 1and ?pi = concat(?
?h i,?
?h i ) ?
Rh, where h =2?h.
Meanwhile, we use another bi-directionalLSTM to map the question q1, .
.
.
, ql to anembedding q ?
Rh.Attention: In this step, the goal is to compare thequestion embedding and all the contextual em-beddings, and select the pieces of informationthat are relevant to the question.
We computea probability distribution ?
depending on thedegree of relevance between word pi (in itscontext) and the question q and then producean output vector o which is a weighted combi-nation of all contextual embeddings { ?pi }:?i = softmaxi q?Ws ?pi (2)o =?i?i ?pi (3)Ws?
Rh?his used in a bilinear term, whichallows us to compute a similarity betweenq and ?pi more flexibly than with just a dotproduct.Prediction: Using the output vector o, the systemoutputs the most likely answer using:a = argmaxa?p?E W?ao (4)Finally, the system adds a softmax functionon top of W?ao and adopts a negative log-likelihood objective for training.Differences from (Hermann et al, 2015).
Ourmodel basically follows the AttentiveReader.
How-ever, to our surprise, our experiments observednearly 8?10% improvement over the original Atten-tiveReader results on CNN and Daily Mail datasets(discussed in Sec.
4).
Concretely, our model hasthe following differences:?
We use a bilinear term, instead of a tanh layerto compute the relevance (attention) betweenquestion and contextual embeddings.
Theeffectiveness of the simple bilinear attentionfunction has been shown previously for neuralmachine translation by (Luong et al, 2015).?
After obtaining the weighted contextual em-beddings o, we use o for direct prediction.
Incontrast, the original model in (Hermann etal., 2015) combined o and the question em-bedding q via another non-linear layer beforemaking final predictions.
We found that wecould remove this layer without harming per-formance.
We believe it is sufficient for themodel to learn to return the entity to which itmaximally gives attention.?
The original model considers all the wordsfrom the vocabularyV in making predictions.We think this is unnecessary, and only predictamong entities which appear in the passage.Of these changes, only the first seems important;the other two just aim at keeping the model simple.2361Window-based MemN2Ns (Hill et al, 2016).Another recent neural network approach proposedby (Hill et al, 2016) is based on a memory networkarchitecture (Weston et al, 2015).
We think it ishighly similar in spirit.
The biggest difference istheir way of encoding passages: they demonstratethat it is most effective to only use a 5-word contextwindowwhen evaluating a candidate entity and theyuse a positional unigram approach to encode thecontextual embeddings: if a window consists of 5words x1, .
.
.
, x5, then it is encoded as?5i=1 Ei (xi ),resulting in 5 separate embedding matrices to learn.They encode the 5-word window surrounding theplaceholder in a similar way and all other wordsin the question text are ignored.
In addition, theysimply use a dot product to compute the ?relevance?between the question and a contextual embedding.This simple model nevertheless works well, show-ing the extent to which this RC task can be done byvery local context matching.4 Experiments4.1 Training DetailsFor training our conventional classifier, we use theimplementation of LambdaMART (Wu et al, 2010)in the RankLib package.5 We use this rankingalgorithm since our problem is naturally a rankingproblem and forests of boosted decision trees havebeen very successful lately (as seen, e.g., in manyrecent Kaggle competitions).
We do not use all thefeatures of LambdaMART since we are only scoring1/0 loss on the first ranked proposal, rather thanusing an IR-style metric to score ranked results.
Weuse Stanford?s neural network dependency parser(Chen and Manning, 2014) to parse all our docu-ment and question text, and all other features canbe extracted without additional tools.For training our neural networks, we only keepthe most frequent |V | = 50k words (including en-tity and placeholder markers), and map all otherwords to an <unk> token.
We choose word embed-ding size d = 100, and use the 100-dimensional pre-trained GloVe word embeddings (Pennington et al,2014) for initialization.
The attention and output pa-rameters are initialized from a uniform distributionbetween (?0.01, 0.01), and the LSTM weights areinitialized from a Gaussian distribution N (0, 0.1).We use hidden size h = 128 for CNN and 256for Daily Mail.
Optimization is carried out using5https://sourceforge.net/p/lemur/wiki/RankLib/.vanilla stochastic gradient descent (SGD), with afixed learning rate of 0.1.
We sort all the examplesby the length of its passage, and randomly sample amini-batch of size 32 for each update.
We also applydropout with probability 0.2 to the embedding layerand gradient clipping when the norm of gradientsexceeds 10.All of our models are run on a single GPU(GeForce GTX TITAN X), with roughly a runtimeof 6 hours per epoch for CNN, and 15 hours perepoch for Daily Mail.
We run all the models up to30 epochs and select the model that achieves thebest accuracy on the development set.4.2 Main ResultsTable 2 presents our main results.
The conventionalfeature-based classifier obtains 67.9% accuracy onthe CNN test set.
Not only does this significantlyoutperform any of the symbolic approaches reportedin (Hermann et al, 2015), it also outperforms allthe neural network systems from their paper and thebest single-system result reported so far from (Hillet al, 2016).
This suggests that the taskmight not beas difficult as suggested, and a simple feature set cancover many of the cases.
Table 3 presents a featureablation analysis of our entity-centric classifier onthe development portion of the CNN dataset.
Itshows that n-gram match and frequency of entitiesare the two most important classes of features.More dramatically, our single-model neural net-work surpasses the previous results by a large mar-gin (over 5%), pushing up the state-of-the-art ac-curacies to 72.4% and 75.8% respectively.
Due toresource constraints, we have not had a chance toinvestigate ensembles of models, which generallycan bring further gains, as demonstrated in (Hill etal., 2016) and many other papers.Concurrently with our paper, Kadlec et al (2016)and Kobayashi et al (2016) also experiment onthese two datasets and report competitive results.However, our single model not only still outper-forms theirs, but also appears to be structurallysimpler.
All these recent efforts converge to similarnumbers, and we believe that they are approachingthe ceiling performance of this task, as we willindicate in the next section.5 Data AnalysisSo far, we have good results via either of our systems.In this section, we aim to conduct an in-depth analy-sis and answer the following questions: (i) Since the2362ModelCNN Daily MailDev Test Dev TestFrame-semantic model?36.3 40.2 35.5 35.5Word distance model?50.5 50.9 56.4 55.5Deep LSTM Reader?55.0 57.0 63.3 62.2Attentive Reader?61.6 63.0 70.5 69.0Impatient Reader?61.8 63.8 69.0 68.0MemNNs (window memory)?58.0 60.6 N/A N/AMemNNs (window memory + self-sup.
)?63.4 66.8 N/A N/AMemNNs (ensemble)?66.2?69.4?N/A N/AOurs: Classifier 67.1 67.9 69.1 68.3Ours: Neural net 72.4 72.4 76.9 75.8Table 2: Accuracy of all models on the CNN andDaily Mail datasets.
Results marked?are from (Hermannet al, 2015) and results marked?are from (Hill et al, 2016).
Classifier and Neural net denote ourentity-centric classifier and neural network systems respectively.
The numbers marked with?indicate thatthe results are from ensemble models.Features AccuracyFull model 67.1?
whether e is in the passage 67.1?
whether e is in the question 67.0?
frequency of e 63.7?
position of e 65.9?
n-gram match 60.5?
word distance 65.4?
sentence co-occurrence 66.0?
dependency parse match 65.6Table 3: Feature ablation analysis of our entity-centric classifier on the development portion of theCNN dataset.
The numbers denote the accuracyafter we exclude each feature from the full system,so a low number indicates an important feature.dataset was created in an automatic and heuristicway, how many of the questions are trivial to an-swer, and how many are noisy and not answerable?
(ii) What have these models learned?
What are theprospects for further improving them?
To studythis, we randomly sampled 100 examples from thedev portion of the CNN dataset for analysis (seemore details in Appendix A).5.1 Breakdown of the ExamplesAfter carefully analyzing these 100 examples, weroughly classify them into the following categories(if an example satisfies more than one category, weclassify it into the earliest one):Exact match The nearest words around the place-holder are also found in the passage surround-ing an entity marker; the answer is self-evident.Sentence-level paraphrasing The question text isentailed/rephrased by exactly one sentence inthe passage, so the answer can definitely beidentified from that sentence.Partial clue In many cases, even though we cannotfind a complete semantic match between thequestion text and some sentence, we are stillable to infer the answer through partial clues,such as some word/concept overlap.Multiple sentences It requires processing multi-ple sentences to infer the correct answer.Coreference errors It is unavoidable that there aremany coreference errors in the dataset.
Thiscategory includes those examples with criticalcoreference errors for the answer entity or keyentities appearing in the question.
Basicallywe treat this category as ?not answerable?.Ambiguous or very hard This category includesexamples for which we think humans are notable to obtain the correct answer (confidently).Table 5 provides our estimate of the percentagefor each category, and Table 4 presents one represen-tative example from each category.
To our surprise,2363Category Question PassageExactMatchit ?s clear @entity0 is leaning to-ward @placeholder , says an ex-pert who monitors @entity0.
.
.
@entity116 , who follows @entity0 ?s operationsand propaganda closely , recently told @entity3 , it ?sclear @entity0 is leaning toward @entity60 in terms ofdoctrine , ideology and an emphasis on holding territoryafter operations .
.
.
.Para-phrase@placeholder says he under-stands why @entity0 wo n?t playat his tournament.
.
.
@entity0 called me personally to let me know thathe would n?t be playing here at @entity23 , "@entity3said on his @entity21 event ?s website .
.
.
.Partialcluea tv movie based on @entity2 ?sbook @placeholder casts a @en-tity76 actor as @entity5.
.
.
to @entity12 @entity2 professed that his @entity11is not a religious book .
.
.
.Multiplesent.he ?s doing a his - and - her duetall by himself , @entity6 said of@placeholder.
.
.
we got some groundbreaking performances , here too, tonight , @entity6 said .
we got @entity17 , who willbe doing some musical performances .
he ?s doing a his- and - her duet al by himself .
.
.
.Coref.Errorrapper@placeholder " disgusted ," cancels upcoming show for @en-tity280.
.
.
with hip - hop star@entity246 saying on@entity247that he was canceling an upcoming show for the @en-tity249 .
.
.
.
(but @entity249 = @entity280 = SAEs)Hard pilot error and snow were reasonsstated for @placeholder planecrash.
.
.
a small aircraft carrying @entity5 , @entity6 and@entity7 the @entity12 @entity3 crashed a few milesfrom @entity9 , near @entity10 , @entity11 .
.
.
.Table 4: Some representative examples from each category.No.
Category (%)1 Exact match 132 Paraphrasing 413 Partial clue 194 Multiple sentences 25 Coreference errors 86 Ambiguous / hard 17Table 5: An estimate of the breakdown of thedataset into classes, based on the analysis of oursampled 100 examples from the CNN dataset.
?coreference errors?
and ?ambiguous/hard?
casesaccount for 25% of this sample set, based on ourmanual analysis, and this certainly will be a barrierfor training models with an accuracy much above75% (although, of course, a model can sometimesmake a lucky guess).
Additionally, only 2 examplesrequire multiple sentences for inference ?
this isa lower rate than we expected and Hermann et al(2015) suggest.
Therefore, we hypothesize thatin most of the ?answerable?
cases, the goal is toCategory Classifier Neural netExact match 13 (100.0%) 13 (100.0%)Paraphrasing 32 (78.1%) 39 (95.1%)Partial clue 14 (73.7%) 17 (89.5%)Multiple sentences 1 (50.0%) 1 (50.0%)Coreference errors 4 (50.0%) 3 (37.5%)Ambiguous / hard 2 (11.8%) 1 (5.9%)All 66 (66.0%) 74 (74.0%)Table 6: The per-category performance of our twosystems.identify the most relevant (single) sentence, andthen to infer the answer based upon it.5.2 Per-category PerformanceNow, we further analyze the predictions of our twosystems, based on the above categorization.As seen in Table 6, we have the following ob-servations: (i) The exact-match cases are quitesimple and both systems get 100% correct.
(ii) Forthe ambiguous/hard and entity-linking-error cases,2364meeting our expectations, both of the systems per-form poorly.
(iii) The two systems mainly differ inparaphrasing cases, and some of the ?partial clue?cases.
This clearly shows how neural networks arebetter capable of learning semantic matches involv-ing paraphrasing or lexical variation between thetwo sentences.
(iv) We believe that the neural-netsystem already achieves near-optimal performanceon all the single-sentence and unambiguous cases.There does not seem to be much useful headroomfor exploring more sophisticated natural languageunderstanding approaches on this dataset.6 Related TasksWe briefly survey other tasks related to readingcomprehension.MCTest (Richardson et al, 2013) is an open-domain reading comprehension task, in the formof fictional short stories, accompanied by multiple-choice questions.
It was carefully created usingcrowd sourcing, and aims at a 7-year-old readingcomprehension level.On the one hand, this dataset has a high demandon various reasoning capacities: over 50% of thequestions require multiple sentences to answer andalso the questions come in assorted categories (what,why, how, whose, which, etc).
On the other hand,the full dataset has only 660 paragraphs in total(each paragraph is associated with 4 questions),which renders training statistical models (especiallycomplex ones) very difficult.Up to now, the best solutions (Sachan et al,2015; Wang et al, 2015) are still heavily relyingon manually curated syntactic/semantic features,with the aid of additional knowledge (e.g., wordembeddings, lexical/paragraph databases).Children Book Test (Hill et al, 2016) was de-veloped in a similar spirit to the CNN/Daily Maildatasets.
It takes any consecutive 21 sentences froma children?s book ?
the first 20 sentences are usedas the passage, and the goal is to infer a missingword in the 21st sentence (question and answer).The questions are also categorized by the type ofthe missing word: named entity, common noun,preposition or verb.
According to the first study onthis dataset (Hill et al, 2016), a language model(an n-gram model or a recurrent neural network)with local context is sufficient for predicting verbsor prepositions; however, for named entities orcommon nouns, it improves performance to scanthrough the whole paragraph to make predictions.So far, the best published results are reported bywindow-based memory networks.bAbI (Weston et al, 2016) is a collection ofartificial datasets, consisting of 20 different rea-soning types.
It encourages the development ofmodels with the ability to chain reasoning, induc-tion/deduction, etc., so that they can answer a ques-tion like ?The football is in the playground?
afterreading a sequence of sentences ?John is in theplayground; Bob is in the office; John picked up thefootball; Bob went to the kitchen.?
Various types ofmemory networks (Sukhbaatar et al, 2015; Kumaret al, 2016) have been shown effective on thesetasks, and Lee et al (2016) show that vector spacemodels based on extensive problem analysis canobtain near-perfect accuracies on all the categories.Despite these promising results, this dataset is lim-ited to a small vocabulary (only 100?200 words)and simple language variations, so there is still ahuge gap from real-world datasets that we need tofill in.7 ConclusionIn this paper, we carefully examined the recentCNN/Daily Mail reading comprehension task.
Oursystems demonstrated state-of-the-art results, butmore importantly, we performed a careful analysisof the dataset by hand.Overall, we think the CNN/Daily Mail datasetsare valuable datasets, which provide a promisingavenue for training effective statistical models forreading comprehension tasks.
Nevertheless, weargue that: (i) this dataset is still quite noisy due toits method of data creation and coreference errors;(ii) current neural networks have almost reached aperformance ceiling on this dataset; and (iii) therequired reasoning and inference level of this datasetis still quite simple.As future work, we need to consider how we canutilize these datasets (and the models trained uponthem) to help solve more complex RC reasoningtasks (with less annotated data).AcknowledgmentsWe thank the anonymous reviewers for their thought-ful feedback.
Stanford University gratefully ac-knowledges the support of the Defense AdvancedResearch Projects Agency (DARPA) Deep Explo-ration and Filtering of Text (DEFT) Program underAir Force Research Laboratory (AFRL) contractno.
FA8750-13-2-0040.
Any opinions, findings,2365and conclusion or recommendations expressed inthis material are those of the authors and do notnecessarily reflect the view of the DARPA, AFRL,or the US government.ReferencesJonathan Berant, Vivek Srikumar, Pei-Chun Chen, AbbyVander Linden, Brittany Harding, Brad Huang, PeterClark, and Christopher D. Manning.
2014.
Modelingbiological processes for reading comprehension.
InEmpirical Methods in Natural Language Processing(EMNLP), pages 1499?1510.Christopher J.C. Burges.
2013.
Towards the machinecomprehension of text: An essay.
Technical report,Microsoft Research Technical Report MSR-TR-2013-125.Danqi Chen and Christopher Manning.
2014.
A fast andaccurate dependency parser using neural networks.
InEmpirical Methods in Natural Language Processing(EMNLP), pages 740?750.Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,and Phil Blunsom.
2015.
Teaching machines to readand comprehend.
In Advances in Neural InformationProcessing Systems (NIPS), pages 1684?1692.Felix Hill, Antoine Bordes, Sumit Chopra, and JasonWeston.
2016.
The goldilocks principle: Readingchildren?s books with explicit memory representa-tions.
In International Conference on Learning Rep-resentations (ICLR).Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, andJan Kleindienst.
2016.
Text understanding with theattention sum reader network.
In Association forComputational Linguistics (ACL).Sosuke Kobayashi, Ran Tian, Naoaki Okazaki, and Ken-taro Inui.
2016.
Dynamic entity representation withmax-pooling improves machine reading.
In NorthAmerican Association for Computational Linguistics(NAACL).Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer,James Bradbury, Ishaan Gulrajani, Victor Zhong,Romain Paulus, and Richard Socher.
2016.
Ask meanything: Dynamic memory networks for naturallanguage processing.
In International Conference onMachine Learning (ICML).Moontae Lee, Xiaodong He, Wen-tau Yih, JianfengGao, Li Deng, and Paul Smolensky.
2016.
Reasoningin vector space: An exploratory study of questionanswering.
In International Conference on LearningRepresentations (ICLR).Thang Luong, Hieu Pham, and Christopher D. Manning.2015.
Effective approaches to attention-based neuralmachine translation.
In Empirical Methods in NaturalLanguage Processing (EMNLP), pages 1412?1421.Peter Norvig.
1978.
A Unified Theory of Inferencefor Text Understanding.
Ph.D. thesis, University ofCalifornia, Berkeley.Jeffrey Pennington, Richard Socher, and ChristopherManning.
2014.
Glove: Global vectors for wordrepresentation.
In Empirical Methods in NaturalLanguage Processing (EMNLP), pages 1532?1543.Matthew Richardson, Christopher J.C. Burges, and ErinRenshaw.
2013.
MCTest: A challenge dataset forthe open-domain machine comprehension of text.
InEmpirical Methods in Natural Language Processing(EMNLP), pages 193?203.Mrinmaya Sachan, Kumar Dubey, Eric Xing, andMatthew Richardson.
2015.
Learning answer-entailing structures for machine comprehension.
InAssociation for Computational Linguistics and In-ternational Joint Conference on Natural LanguageProcessing (ACL/IJCNLP), pages 239?249.Sainbayar Sukhbaatar, arthur szlam, Jason Weston, andRob Fergus.
2015.
End-to-end memory networks.
InAdvances in Neural Information Processing Systems(NIPS), pages 2431?2439.Hai Wang, Mohit Bansal, Kevin Gimpel, and DavidMcAllester.
2015.
Machine comprehension with syn-tax, frames, and semantics.
In Association for Com-putational Linguistics and International Joint Confer-ence onNatural Language Processing (ACL/IJCNLP),pages 700?706.JasonWeston, Sumit Chopra, and Antoine Bordes.
2015.Memory networks.
In International Conference onLearning Representations (ICLR).Jason Weston, Antoine Bordes, Sumit Chopra, andTomas Mikolov.
2016.
Towards AI-complete ques-tion answering: A set of prerequisite toy tasks.
InInternational Conference on Learning Representa-tions (ICLR).Qiang Wu, Christopher J. Burges, Krysta M. Svore, andJianfeng Gao.
2010.
Adapting boosting for informa-tion retrieval measures.
Information Retrieval, pages254?270.A Samples and Labeled Categories fromthe CNN DatasetFor the analysis in Section 5, we uniformly sampled100 examples from the development set of the CNNdataset.
Table 8 provides a full index list of oursamples and Table 7 presents our labeled categories.2366Category Sample IDsExact match (13) 8, 11, 23, 27, 28, 32, 43, 57, 63, 72, 86, 87, 99Sentence-level paraphrasing (41) 0, 2, 7, 9, 12, 14, 16, 18, 19, 20, 29, 30, 31, 34, 36,37, 39, 41, 42, 44, 47, 48, 52, 54, 58, 64, 65, 66, 69,73, 74, 78, 80, 81, 82, 84, 85, 90, 92, 95, 96Partial clues (19) 4, 17, 21, 24, 35, 38, 45, 53, 55, 56, 61, 62, 75, 83,88, 89, 91, 97, 98Multiple sentences (2) 5, 76Coreference errors (8) 6, 22, 40, 46, 51, 60, 68, 94Ambiguous or very hard (17) 1, 3, 10, 13, 15, 25, 26, 33, 49, 50, 59, 67, 70, 71, 77,79, 93Table 7: Our labeled categories of the 100 samples.ID Filename ID Filename0 ddb1e746f88a22fee654ecde8f018e7586595045.question 1 2bef8ec21b10a3294b1496d9a86f29f0592d2300.question2 38c702812a874f983e9890c32ba832841a327351.question 3 636857045cf266dd69b67b1e53617bed5253dc33.question4 417cbffd5e6275b3c42cb88be222a9f6c7d415f1.question 5 ef96409c707a699e4055a1d0684eecdb6e115c16.question6 b4e157a6a34bf11a03e0b5cd55065c0f39ac8d60.question 7 1d75e7c59978c7c06f3aecaf52bc35b8919eee17.question8 223c8e3aeddc3f65fee1964df17bb72f89b723e4.question 9 13d33b8c86375b0f5fdc856116e91a7355c6fc5a.question10 378fd418b8ec18dff406be07ec225e6bf53659f5.question 11 d8253b7f22662911c19ec4468f81b9db29df1746.question12 80529c792d3a368861b404c1ce4d7ad3c12e552a.question 13 728e7b365e941d814676168c78c9c4f38892a550.question14 3cf6fb2c0d09927a12add82b4a3f248da740d0de.question 15 04b827f84e60659258e19806afe9f8d10b764db1.question16 f0abf359d71f7896abd09ff7b3319c70f2ded81e.question 17 b6696e0f2166a75fcefbe4f28d0ad06e420eef23.question18 881ab3139c34e9d9f29eb11601321a234d096272.question 19 66f5208d62b543ee41accb7a560d63ff40413bac.question20 f83a70d469fa667f0952959346b496fbf3cdb35c.question 21 1853813a80f83a1661dd3f6695559674c749525e.question22 02664d5e3af321afbaf4ee351ba1f24643746451.question 23 20417b5efb836530846ddf677d1bd0bbc831643c.question24 42c25a01801228a863c508f9d9e95399ea5f37a4.question 25 70a3ba822770abcaf64dd131c85ec964d172c312.question26 b6636e525ad58ffdc9a7c18187fb3412660d2cdd.question 27 6147c9f2b3d1cc6fbc57c2137f0356513f49bf46.question28 262b855e2f24e1b2e4e0ba01ace81a1f214d729e.question 29 d7211f4d21f40461bb59954e53360eeb4bb6c664.question30 be813e58ae9387a9fdaf771656c8e1122794e515.question 31 ad39c5217042f36e4c1458e9397b4a588bbf8cf9.question32 9534c3907f1cd917d24a9e4f2afc5b38b82d9fca.question 33 3fbe4bfb721a6e1aa60502089c46240d5c332c05.question34 6efa2d6bad587bde65ca22d10eca83cf0176d84f.question 35 436aa25e28d3a026c4fcd658a852b6a24fc6935e.question36 0c44d6ef109d33543cfbd26c95c9c3f6fe33a995.question 37 8472b859c5a8d18454644d9acdb5edd1db175eb5.question38 fb4dd20e0f464423b6407fd0d21cc4384905cf26.question 39 a192ddbcecf2b00260ae4c7c3c20df4d5ce47a85.question40 f7133f844967483519dbf632e2f3fb90c5625a4c.question 41 29b274958eb057e8f1688f02ef8dbc1c6d06c954.question42 8ea6ad57c1c5eb1950f50ea47231a5b3f32dd639.question 43 1e43f2349b17dac6d1b3143f8c5556e2257be92c.question44 7f11f0b4f6bb9aaa3bdc74bffaed5c869b26be97.question 45 8e6d8d984e51adb5071aad22680419854185eaea.question46 57fc2b7ffcfbd1068fbc33b95d5786e2bff24698.question 47 57b773478955811a8077c98840d85af03e1b4f05.question48 d857700721b5835c3472ba73ef7abfad0c9c499f.question 49 f8eedded53c96e0cb98e2e95623714d2737f29da.question50 4c488f41622ad48977a60c2283910f15a736417e.question 51 39680fd0bff53f2ca02f632eabbc024d698f979e.question52 addd9cebe24c96b4a3c8e9a50cd2a57905b6defb.question 53 50317f7a626e23628e4bfd190e987ad5af7d283e.question54 3f7ac912a75e4ef7a56987bff37440ffa14770c6.question 55 610012ef561027623f4b4e3b8310c1c41dc819cc.question56 d9c2e9bfc71045be2ecd959676016599e4637ed1.question 57 848c068db210e0b255f83c4f8b01d2d421fb9c94.question58 f5c2753703b66d26f43bafe7f157803dc96eedbc.question 59 4f76379f1c7b1d4acc5a4c82ced64af6313698dd.question60 e5bb1c27d07f1591929bf0283075ad1bc1fc0b50.question 61 33b911f9074c80eb18a57f657ad01393582059be.question62 58c4c046654af52a3cb8f6890411a41c0dd0063b.question 63 7b03f730fda1b247e9f124b692e3298859785ef3.question64 ece6f4e047856d5a84811a67ac9780d48044e69a.question 65 35565dc6aecc0f1203842ef13aede0a14a8cf075.question66 ddf3f2b06353fe8a9b50043f926eb3ab318e91b2.question 67 e248e59739c9c013a2b1b7385d881e0f879b341d.question68 e86d3fa2a74625620bcae0003dfbe13416ee29cf.question 69 176bf03c9c19951a8ae5197505a568454a6d4526.question70 ee694cb968ae99aea36f910355bf73da417274c0.question 71 7a666f78590edbaf7c4d73c4ea641c545295a513.question72 91e3cdd46a70d6dfbe917c6241eab907da4b1562.question 73 e54d9bdcb478ecc490608459d3405571979ef3f2.question74 f3737e4de9864f083d6697293be650e02505768c.question 75 1fc7488755d24696a4ed1aabc0a21b8b9755d8c6.question76 fb3eadd07b9f1df1f8a7a6b136ad6d06f4981442.question 77 1406bdad74b3f932342718d5d5d0946a906d73e2.question78 54b6396669bdb2e30715085745d4f98d058269ef.question 79 0a53102673f2bebc36ce74bf71db1b42a0187052.question80 d5eb4f98551d23810bfeb0e5b8a94037bcf58b0d.question 81 370de4ffe0f2f9691e4bd456ff344a6a337e0edf.question82 12f32c770c86083ff21b25de7626505c06440018.question 83 9f6b5cff3ce146e21e323a1462c3eff8fca3d4a0.question84 1c2a14f525fa3802b8da52aebaa9abd2091f9215.question 85 f2416e14d89d40562284ba2d15f7d5cc59c7e602.question86 adcf5881856bcbaf1ad93d06a3c5431f6a0319ba.question 87 097d34b804c4c052591984d51444c4a97a3c41ac.question88 773066c39bb3b593f676caf03f7e7370a8cd2a43.question 89 598cf5ff08ea75dcedda31ac1300e49cdf90893a.question90 b66ebaaefb844f1216fd3d28eb160b08f42cde62.question 91 535a44842decdc23c11bae50d9393b923897187e.question92 e27ca3104a596171940db8501c4868ed2fbc8cea.question 93 bb07799b4193cffa90792f92a8c14d591754a7f3.question94 83ff109c6ccd512abdf317220337b98ef551d94a.question 95 5ede07a1e4ac56a0155d852df0f5bb6bde3cb507.question96 7a2a9a7fbb44b0e51512c61502ce2292170400c1.question 97 9dcdc052682b041cdbf2fadc8e55f1bafc88fe61.question98 0c2e28b7f373f29f3796d29047556766cc1dd709.question 99 2bdf1696bfd2579bb719402e9a6fa99cb8dbf587.questionTable 8: A full index list of our samples.2367
