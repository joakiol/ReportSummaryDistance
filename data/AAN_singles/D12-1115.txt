Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1255?1265, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsResolving ?This-issue?
AnaphoraVarada KolhatkarDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canadavarada@cs.toronto.eduGraeme HirstDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canadagh@cs.toronto.eduAbstractWe annotate and resolve a particularcase of abstract anaphora, namely, this-issue anaphora.
We propose a candidateranking model for this-issue anaphoraresolution that explores different issue-specific and general abstract-anaphorafeatures.
The model is not restrictedto nominal or verbal antecedents; rather,it is able to identify antecedents thatare arbitrary spans of text.
Our re-sults show that (a) the model outperformsthe strong adjacent-sentence baseline;(b) general abstract-anaphora features,as distinguished from issue-specific fea-tures, play a crucial role in this-issueanaphora resolution, suggesting that ourapproach can be generalized for otherNPs such as this problem and this debate;and (c) it is possible to reduce the searchspace in order to improve performance.1 IntroductionAnaphora in which the anaphoric expression refersto an abstract object such as a proposition, a prop-erty, or a fact is known as abstract object anaphora.This is seen in the following examples.
(1) [Be careful what you wish... because wishessometimes come true.
]i [That]i is what theSemiconductor Industry Association, which rep-resents U.S. manufacturers, has been learning.
(from Asher (1993))(2) This prospective study suggested [that oralcarvedilol is more effective than oral meto-prolol in the prevention of AF after on-pumpCABG]i.
It is well tolerated when started beforeand continued after the surgery.
However, furtherprospective studies are needed to clarify [this is-sue]i.
(3) In principle, he said, airlines should be allowed[to sell standing-room-only tickets for adults]i?
as long as [this decision]i was approved bytheir marketing departments.These examples highlight a difficulty not found withnominal anaphora.
First, the anaphors refer to ab-stract concepts that can be expressed with differ-ent syntactic shapes which are usually not nominals.The anaphor That in (1) refers to the proposition inthe previous utterance, whereas the anaphor this is-sue in (2) refers to a clause from the previous text.In (3), the anaphoric expression this decision refersto a verb phrase from the same sentence.
Second,the antecedents do not always have precisely definedboundaries.
In (2), for example, the whole sentencecontaining the marked clause could also be thoughtto be the correct antecedent.
Third, the actual refer-ents are not always the precise textual antecedents.The actual referent in (2), the issue to be clarified,is whether oral carvedilol is more effective than oralmetoprolol in the prevention of AF after on-pumpCABG or not, a variant of the antecedent text.Generally, abstract anaphora, as distinguishedfrom nominal anaphora, is signalled in English bypronouns this, that, and it (Mu?ller, 2008).
But inabstract anaphora, English prefers demonstrativesto personal pronouns and definite articles (Pas-sonneau, 1989; Navarretta, 2011).1 Demonstra-1This is not to say that personal pronouns and definite arti-cles do not occur in abstract anaphora, but they are not common.1255tives can be used in isolation (That in (1)) or withnouns (e.g., this issue in (2)).
The latter followsthe pattern demonstrative {modifier}* noun.
Thedemonstrative acts as a determiner and the noun fol-lowing the demonstrative imposes selectional con-straints for the antecedent, as in examples (2) and(3).
Francis (1994) calls such nouns label nouns,which ?serve to encapsulate or package a stretchof discourse?.
Schmid (2000) refers to them asshell nouns, a metaphoric term which reflects differ-ent functions of these nouns such as encapsulation,pointing, and signalling.Demonstrative nouns, along with pronouns likeboth and either, are referred to as sortal anaphors(Castan?o et al 2002; Lin and Liang, 2004; Toriiand Vijay-Shanker, 2007).
Castan?o et alobservedthat sortal anaphors are prevalent in the biomedi-cal literature.
They noted that among 100 distinctanaphors derived from a corpus of 70 Medline ab-stracts, 60% were sortal anaphors.
But how oftendo demonstrative nouns refer to abstract objects?We observed that from a corpus of 74,000 randomlychosen Medline2 abstracts, of the first 150 most fre-quently occurring distinct demonstrative nouns (fre-quency > 30), 51.3% were abstract, 41.3% wereconcrete, and 7.3% were discourse deictic.
Thisshows that abstract anaphora resolution is an impor-tant component of general anaphora resolution in thebiomedical domain.
However, automatic resolutionof this type of anaphora has not attracted much atten-tion and the previous work for this task is limited.The present work is a step towards resolving ab-stract anaphora in written text.
In particular, wechoose the interesting abstract concept issue anddemonstrate the complexities of resolving this-issueanaphora manually as well as automatically in theMedline domain.
We present our algorithm, results,and error analysis for this-issue anaphora resolution.The abstract concept issue was chosen for the fol-lowing reasons.
First, it occurs frequently in allkinds of text from newspaper articles to novels toscientific articles.
There are 13,489 issue anaphorainstances in the New York Times corpus and 1,116instances in 65,000 Medline abstracts.
Second, it isabstract enough that it can take several syntactic and2http://www.nlm.nih.gov/bsd/pmresources.htmlsemantic forms, which makes the problem interest-ing and non-trivial.
Third, issue referents in scien-tific literature generally lie in the previous sentenceor two, which makes the problem tractable.
Fourth,issues in Medline abstracts are generally associatedwith clinical problems in the medical domain andspell out the motivation of the research presented inthe article.
So extraction of this information wouldbe useful in any biomedical information retrievalsystem.2 Related WorkAnaphora resolution has been extensively studiedin computational linguistics (Hirst, 1981; Mitkov,2002; Poesio et al 2011).
But CL research hasmostly focused on nominal anaphora resolution(e.g., resolving multiple ambiguous mentions of asingle entity representing a person, a location, or anorganization) mainly for two reasons.
First, nominalanaphora is the most frequently occurring anaphorain most domains, and second, there is a substantialamount of annotated data available for this kind ofanaphora.Besides pronominal anaphora, some work hasbeen done on complement anaphora (Modjeska,2003) (e.g., British and other European steelmak-ers).
There is also some research on resolving sor-tal anaphora in the medical domain using domainknowledge (Castan?o et al 2002; Lin and Liang,2004; Torii and Vijay-Shanker, 2007).
But all theseapproaches focus only on the anaphors with nominalantecedents.By contrast, the area of abstract object anaphoraremains relatively unexplored mainly because thestandard anaphora resolution features such as agree-ment and apposition cannot be applied to abstractanaphora resolution.
Asher (1993) built a theoreti-cal framework to resolve abstract anaphora.
He di-vided discourse abstract anaphora into three broadcategories: event anaphora, proposition anaphora,and fact anaphora, and discussed how abstract en-tities can be resolved using discourse representa-tion theory.
Chen et al(2011) focused on a sub-set of event anaphora and resolved event corefer-ence chains in terms of the representative verbs ofthe events from the OntoNotes corpus.
Our task dif-fers from their work as follows.
Chen et almainly1256focus on events and actions and use verbs as a proxyfor the non-nominal antecedents.
But this-issue an-tecedents cannot usually be represented by a verb.Our work is not restricted to a particular syntactictype of the antecedent; rather we provide the flexibil-ity of marking arbitrary spans of text as antecedents.There are also some prominent approaches to ab-stract anaphora resolution in the spoken dialoguedomain (Eckert and Strube, 2000; Byron, 2004;Mu?ller, 2008).
These approaches go beyond nom-inal antecedents; however, they are restricted to spo-ken dialogues in specific domains and need seriousadaptation if one wants to apply them to arbitrarytext.In addition to research on resolution, there isalso some work on effective annotation of abstractanaphora (Strube and Mu?ller, 2003; Botley, 2006;Poesio and Artstein, 2008; Dipper and Zinsmeister,2011).
However, to the best of our knowledge, thereis currently no English corpus annotated for issueanaphora antecedents.3 Data and AnnotationTo create an initial annotated dataset, we collected188 this {modifier}* issue instances along with thesurrounding context from Medline abstracts.3 Fiveinstances were discarded as they had an unrelated(publication related) sense.
Among the remaining183 instances, 132 instances were independently an-notated by two annotators, a domain expert and anon-expert, and the remaining 51 instances were an-notated only by the domain expert.
We use the for-mer instances for training and the latter instances(unseen by the developer) for testing.
The anno-tator?s task was to mark arbitrary text segmentsas antecedents (without concern for their linguistictypes).
To make the task tractable, we assumed thatan antecedent does not span multiple sentences butlies in a single sentence (since we are dealing withsingular this-issue anaphors) and that it is a continu-ous span of text.3Although our dataset is rather small, its size is similar toother available abstract anaphora corpora in English: 154 in-stances in Eckert and Strube (2000), 69 instances in Byron(2003), 462 instances annotated by only one annotator in Botley(2006), and 455 instances restricted to those which have onlynominal or clausal antecedents in Poesio and Artstein (2008).r11 r12 r13 r14 r15r21 r22 r23 r24 r25Annotator 1Annotator 2r16 r17 r18 r19r26 r27 r28 r29 r2,10id2Intersections 1 2 3 4 5 6 7 8 9 10 11 12 13 14id3 id4 id5id1 id2 id3 id4 id5Figure 1: Example of annotated data.
Bold segmentsdenote the marked antecedents for the correspondinganaphor ids.
rh j is the jth section identified by the an-notator h.3.1 Inter-annotator AgreementThis kind of annotation ?
identifying and markingarbitrary units of text that are not necessarily con-stituents ?
requires a non-trivial variant of the usualinter-annotator agreement measures.
We use Krip-pendorff?s reliability coefficient for unitizing (?u)(Krippendorff, 1995) which has not often been usedor described in CL.
In our context, unitizing meansmarking the spans of the text that serve as the an-tecedent for the given anaphors within the given text.The coefficient ?u assumes that the annotated sec-tions do not overlap in a single annotator?s outputand our data satisfies this criterion.4 The generalform of coefficient ?u is:?u = 1?
uDouDe(1)where uDo and uDe are observed and expected dis-agreements respectively.
Both disagreement quanti-ties express the average squared differences betweenthe mismatching pairs of values assigned by anno-tators to given units of analysis.
?u = 1 indicatesperfect reliability and ?u = 0 indicates the absenceof reliability.
When ?u < 0, the disagreement is sys-tematic.
Annotated data with reliability of ?u?
0.80is considered reliable (Krippendorff, 2004).Krippendorff?s ?u is non-trivial, and explaining itin detail would take too much space, but the generalidea, in our context, is as follows.
The annotatorsmark the antecedents corresponding to each anaphorin their respective copies of the text, as shown in Fig-ure 1.
The marked antecedents are mutually exclu-sive sections r; we denote the jth section identified4If antecedents overlap with each other in a single annota-tor?s output (which is a rare event) we construct data that satis-fies the non-overlap criterion by creating different copies of thesame text corresponding to each anaphor instance.1257Antecedent type Distribution Exampleclause 37.9% There is a controversial debate (SBAR whether back school program might improvequality of life in back pain patients).
This study aimed to address this issue.sentence 26.5% (S Reduced serotonin function and abnormalities in the hypothalamic-pituitary-adrenalaxis are thought to play a role in the aetiology of major depression.)
We sought toexamine this issue in the elderly ...mixed 18.2% (S (PP Given these data) (, ,) (NP decreasing HTD to < or = 5 years) (VP may havea detrimental effect on patients with locally advanced prostate cancer) (.
.))
Only arandomized trial will conclusively clarify this issue.nominalization 17.4% As (NP the influence of estrogen alone on breast cancer detection) is not established,we examined this issue in the Women?s Health Initiative trial...Table 1: Antecedent types.
In examples, the antecedent type is in bold and the marked antecedent is in italics.by the annotator h by rh j.
In Figure 1, annotators 1and 2 have reached different conclusions by identi-fying 9 and 10 sections respectively in their copiesof the text.
Annotator 1 has not marked any an-tecedent for the anaphor with id = 1, while annotator2 has marked r21 for the same anaphor.
Both anno-tators have marked exactly the same antecedent forthe anaphor with id = 4.
The difference between twoannotated sections is defined in terms of the squareof the distance between the non-overlapping parts ofthe sections.
The distance is 0 when the sections areunmarked by both annotators or are marked and ex-actly same, and is the summation of the squares ofthe unmatched parts if they are different.
The coeffi-cient is computed using intersections of the markedsections.
In Figure 1, annotators 1 and 2 have a to-tal of 14 intersections.
The observed disagreementuDo is the weighted sum of the differences betweenall mismatching intersections of sections marked bythe annotators, and the expected disagreement is thesummation of all possible differences of pairwisecombinations of all sections of all annotators nor-malized by the length of the text (in terms of thenumber of tokens) and the number of pairwise com-binations of annotators.For our data, the inter-annotator agreement was?u = 0.86 (uDo = 0.81 and uDe = 5.81) despite thefact that the annotators differed in their domain ex-pertise, which suggests that abstract concepts suchas issue can be annotated reliably.3.2 Corpus StatisticsA gold standard corpus was created by resolving thecases where the annotators disagreed.
Among 132training instances, the annotators could not resolve6 instances and we broke the tie by writing to theauthors of the articles and using their response toresolve the disagreement.
In the gold standard cor-pus, 95.5% of the antecedents were in the current orprevious sentence and 99.2% were in the current orprevious two sentences.
Only one antecedent wasfound more than two sentences back and it was sixsentences back.
One instance was a cataphor, butthe antecedent occurred in the same sentence as theanaphor.
This suggests that for an automatic this-issue resolution system, it would be reasonable toconsider only the previous two sentences along withthe sentence containing the anaphor.The distribution of the different linguistic formsthat an antecedent of this-issue can take in our dataset is shown in Table 1.
The majority of antecedentsare clauses or whole sentences.
A number of an-tecedents are noun phrases, but these are gener-ally nominalizations that refer to abstract concepts(e.g., the influence of estrogen alone on breast can-cer detection).
Some antecedents are not even well-defined syntactic constituents5 but are combinationsof several well-defined constituents.
We denote thetype of such antecedents as mixed.
In the corpus,18.2% of the antecedents are of this type, suggest-ing that it is not sufficient to restrict the antecedentsearch space to well-defined syntactic constituents.6In our data, we did not find anaphoric chains forany of the this-issue anaphor instances, which indi-cates that the antecedents of this-issue anaphors are5We refer to every syntactic constituent identified by theparser as a well-defined syntactic constituent.6Indeed, many of mixed type antecedents (nearly three-quarters of them) are the result of parser attachment errors, butmany are not.1258in the reader?s local memory and not in the globalmemory.
This observation supports the THIS-NPshypothesis (Gundel et al 1993; Poesio and Mod-jeska, 2002) that this-NPs are used to refer to enti-ties which are active albeit not in focus, i.e., they arenot the center of the previous utterance.4 Resolution Algorithm4.1 Candidate ExtractionFor correct resolution, the set of extracted candidatesmust contain the correct antecedent in the first place.The problem of candidate extraction is non-trivial inabstract anaphora resolution because the antecedentsare of many different types of syntactic constituentssuch as clauses, sentences, and nominalizations.Drawing on our observation that the mixed type an-tecedents are generally a combination of differentwell-defined syntactic constituents, we extract theset of candidate antecedents as follows.
First, wecreate a set of candidate sentences which containsthe sentence containing the this-issue anaphor andthe two preceding sentences.
Then, we parse everycandidate sentence with the Stanford Parser7.
Ini-tially, the set of candidate constituents contains alist of well-defined syntactic constituents.
We re-quire that the node type of these constituents be inthe set {S, SBAR, NP, SQ, SBARQ, S+V}.
Thisset was empirically derived from our data.
To eachconstituent, there is associated a set of mixed typeconstituents.
These are created by concatenating theoriginal constituent with its sister constituents.
Forexample, in (4), the set of well-defined eligible can-didate constituents is {NP, NP1} and so NP1 PP1 isa mixed type candidate.
(4) NPNP1 PP1 PP2The set of candidate constituents is updated withthe extracted mixed type constituents.
Extractingmixed type candidate constituents not only dealswith mixed type instances as shown in Table 1, butas a side effect it also corrects some attachment er-rors made by the parser.
Finally, the constituents7http://nlp.stanford.edu/software/lex-parser.shtmlhaving a number of leaves (words) less than a thresh-old8 are discarded to give the final set of candidateconstituents.4.2 FeaturesWe explored the effect of including 43 automati-cally extracted features (12 feature classes), whichare summarized in Table 2.
The features can also bebroadly divided into two groups: issue-specific fea-tures and general abstract-anaphora features.
Issue-specific features are based on our common-senseknowledge of the concept of issue and the differentsemantic forms it can take; e.g., controversy (X iscontroversial), hypothesis (It has been hypothesizedX), or lack of knowledge (X is unknown), where Xis the issue.
In our data, we observed certain syn-tactic patterns of issues such as whether X or notand that X and the IP feature class encodes this in-formation.
Other issue-specific features are IVERBand IHEAD.
The feature IVERB checks whetherthe governing verb of the candidate is an issueverb (e.g., speculate, hypothesize, argue, debate),whereas IHEAD checks whether the candidate headin the dependency tree is an issue word (e.g., contro-versy, uncertain, unknown).
The general abstract-anaphora resolution features do not make use ofthe semantic properties of the word issue.
Someof these features are derived empirically from thetraining data (e.g., ST, L, D).
The EL feature is bor-rowed from Mu?ller (2008) and encodes the embed-ding level of the candidate within the candidate sen-tence.
The MC feature tries to capture the idea of theTHIS-NPs hypothesis (Gundel et al 1993; Poesioand Modjeska, 2002) that the antecedents of this-NP anaphors are not the center of the previous utter-ance.
The general abstract-anaphora features in theSR feature class capture the semantic role of the can-didate in the candidate sentence.
We used the IllinoisSemantic Role Labeler9 for SR features.
The gen-eral abstract-anaphora features also contain a fewlexical features (e.g., M, SC).
But these features areindependent of the semantic properties of the wordissue.
The general abstract-anaphora resolution fea-tures also contain dependency-tree features, lexical-8The threshold 5 was empirically derived.
Antecedents inour training data had on average 17 words.9http://cogcomp.cs.illinois.edu/page/software_view/SRL1259ISSUE PATTERN (IP)ISWHETHER 1 iff the candidate follows the pattern SBAR?
(IN whether) (S ...)ISTHAT 1 iff the candidate follows the pattern SBAR?
(IN that) (S ...)ISIF 1 iff the candidate follows the pattern SBAR?
(IN iff) (S ...)ISQUESTION 1 iff the candidate node is SBARQ or SQSYNTACTIC TYPE (ST)ISNP 1 iff the candidate node is of type NPISS 1 iff the candidate node is a sentence nodeISSBAR 1 iff the candidate node is an SBAR nodeISSQ 1 iff the candidate node is an SQ or SBARQ nodeMIXED 1 iff the candidate node is of type mixedEMBEDDING LEVEL (EL) (Mu?ller, 2008)TLEMBEDDING level of embedding of the given candidate in its top clause (the root node of the syntactic tree)ILEMBEDDING level of embedding of the given candidate in its immediate clause (the closest parent of type S or SBAR)MAIN CLAUSE (MC)MCLAUSE 1 iff the candidate is in the main clauseDISTANCE (D)ISSAME 1 iff the candidate is in the same sentence as anaphorSADJA 1 iff the candidate is in the adjacent sentenceISREM 1 iff the candidate occurs 2 or more sentences before the anaphorPOSITION 1 iff the antecedent occurs before anaphorSEMANTIC ROLE LABELLING (SR)IVERB 1 iff the governing verb of the given candidate is an issue verbISA0 1 iff the candidate is the agent of the governing verbISA1 1 iff the candidate is the patient of the governing verbISA2 1 iff the candidate is the instrument of the governing verbISAM 1 iff the candidate plays the role of modifficationISNOR 1 iff the candidate plays no well-defined semantic role in the sentenceDEPENDENCY TREE (DT)IHEAD 1 iff the candidate head in the dependency tree is an issue word (e.g., controversial, unknown)ISSUBJ 1 iff the dependency relation of the candidate to its head is of type nominal, controlling or clausal subjectISOBJ 1 iff the dependency relation of the candidate to its head is of type direct object or preposition objISDEP 1 iff the dependency relation of the candidate to its head is of type dependentISROOT 1 iff the candidate is the root of the dependency treeISPREP 1 iff the dependency relation of the candidate to its head is of type prepositionISCONT 1 iff the dependency relation of the candidate to its head is of type continuationISCOMP 1 iff the dependency relation of the candidate to its head is of type clausal or adjectival complementISSENT 1 iff candidate?s head is the root nodePRESENCE OF MODALS (M)MODAL 1 iff the given candidate contains a modal verbPRESENCE OF SUBORDINATING CONJUNCTION (SC)ISCONT 1 iff the candidate starts with a contrastive subordinating conjunction (e.g., however, but, yet)ISCAUSE 1 iff the candidate starts with a causal subordinating conjunction (e.g., because, as, since)ISCOND 1 iff the candidate starts with a conditional subordinating conjunction (e.g., if, that, whether or not)LEXICAL OVERLAP (LO)TOS normalized ratio of the overlapping words in candidate and the title of the articleAOS normalized ratio of the overlapping words in candidate and the anaphor sentenceDWS proportion of domain-specific words in the candidateCONTEXT (C)ISPPREP 1 iff the preceding word of the candidate is a prepositionISFPREP 1 iff the following word of the candidate is a prepositionISPPUNCT 1 iff the preceding word of the candidate is a punctuationISFPUNCT 1 iff the following word of the candidate is a punctuationLENGTH (L)LEN length of the candidate in wordsTable 2: Feature sets for this-issue resolution.
All features are extracted automatically.1260overlap features, and context features.4.3 Candidate Ranking ModelGiven an anaphor ai and a set of candidateantecedents C = {C1,C2, ...,Ck}, the problem ofanaphora resolution is to choose the best candidateantecedent for ai.
We follow the candidate-rankingmodel proposed by Denis and Baldridge (2008).The advantage of the candidate-ranking model overthe mention-pair model is that it overcomes thestrong independence assumption made in mention-pair models and evaluates how good a candidate isrelative to all other candidates.We train our model as follows.
If the anaphoris a this-issue anaphor, the set C is extracted us-ing the candidate extraction algorithm from Section4.1.
Then a corresponding set of feature vectors,C f = {C f 1,C f 2, ...,C f k}, is created using the featuresin Table 2.
The training instances are created as de-scribed by Soon et al(2001).
Note that the instancecreation is simpler than for general coreference res-olution because of the absence of anaphoric chainsin our data.
For every anaphor ai and eligible can-didates C f = {C f 1,C f 2, ...,C f k}, we create trainingexamples (ai,C f i, label),?C f i ?
C f .
The label is 1if Ci is the true antecedent of the anaphor ai, oth-erwise the label is ?1.
The examples with label 1get the rank of 1, while other examples get the rankof 2.
We use SVMrank (Joachims, 2002) for train-ing the candidate-ranking model.
During testing, thetrained model is used to rank the candidates of eachtest instance of this-issue anaphor.5 EvaluationIn this section we present the evaluation of eachcomponent of our resolution system.5.1 Evaluation of Candidate ExtractionThe set of candidate antecedents extracted by themethod from Section 4.1 contained the correct an-tecedent 92% of the time.
Each anaphor had, onaverage, 23.80 candidates, of which only 5.19 can-didates were nominal type.
The accuracy droppedto 84% when we did not extract mixed type candi-dates.
The error analysis of the 8% of the instanceswhere we failed to extract the correct antecedent re-vealed that most of these errors were parsing errorswhich could not be corrected by our candidate ex-traction method.10 In these cases, the parts of theantecedent had been placed in completely differentbranches of the parse tree.
For example, in (5), thecorrect antecedent is a combination of the NP fromthe S?
V P?
NP?
PP?
NP branch and the PPfrom S?V P?
PP branch.
In such a case, concate-nating sister constituents does not help.
(5) The data from this pilot study (VP (VBP provide)(NP (NP no evidence) (PP (IN for) (NP a dif-ference in hemodynamic effects between pulseHVHF and CPFA))) (PP in patients with sep-tic shock already receiving CRRT)).
A largersample size is needed to adequately explore thisissue.5.2 Evaluation of this-issue ResolutionWe propose two metrics for abstract anaphora eval-uation.
The simplest metric is the percentage of an-tecedents on which the system and the annotatedgold data agree.
We denote this metric as EXACT-M (Exact Match) and compute it as the ratio ofnumber of correctly identified antecedents to the to-tal number of marked antecedents.
This metric isa good indicator of a system?s performance; how-ever, it is a rather strict evaluation because, as wenoted in section 1, issues generally have no preciseboundaries in the text.
So we propose another met-ric called RLL, which is similar to the ROUGE-Lmetric (Lin, 2004) used for the evaluation of auto-matic summarization.
Let the marked antecedentsof the gold corpus for k anaphor instances be G =?g1,g2, ...,gk?
and the system-annotated antecedentsbe A = ?a1,a2, ...,ak?.
Let the number of words inG and A be m and n respectively.
Let LCS(gi,ai)be the the number of words in the longest commonsubsequence of gi and ai.
Then the precision (PRLL)and recall (RRLL) over the whole data set are com-puted as shown in equations (2) and (3).
PRLL isthe total number of word overlaps between the goldand system-annotated antecedents normalized by thenumber of words in system-annotated antecedentsand RRLL is the total number of such word overlapsnormalized by the number of words in the gold an-tecedents.
If the system picks too much text for an-tecedents, RRLL is high but PRLL is low.
The F-score,10Extracting candidate constituents from the dependencytrees did not add any new candidates to the set of candidates.12615-fold Cross-Validation TestPRLL RRLL FRLL EX-M PRLL RRLL FRLL EX-M1 Adjacent sentence 66.47 86.16 74.93 22.93 61.73 87.69 72.46 24.002 Random 50.71 32.84 39.63 8.40 43.75 35.00 38.89 15.693 {IP, D, C, LO, EL, M, MC, L, SC, SR, DT} 79.37 83.66 81.11 59.80 71.89 85.74 78.20 58.824 {IP, D, C, LO, M, MC, L, SC, DT} 78.71 83.86 81.14 59.89 70.64 88.09 78.40 54.905 {IP, D, C, EL, L, SC, SR, DT} 77.95 83.06 80.33 57.41 72.03 84.85 77.92 60.786 {IP, D, EL, MC, L, SR, DT} 80.00 84.75 82.24 59.91 68.88 85.29 76.22 56.867 {IP, D, M, L, SR} 73.42 83.16 77.90 52.31 70.74 91.03 79.61 50.988 {D, C, LO, L, SC, SR, DT} 79.15 85.28 82.04 56.07 67.39 86.32 75.69 52.949 issue-specific features 74.66 45.70 56.57 41.42 64.20 45.88 53.52 41.3810 non-issue features 76.39 79.39 77.82 51.48 71.19 83.24 76.75 58.8211 All 78.22 82.92 80.41 56.75 71.28 83.24 76.80 56.8612 Oracle candidate extractor + row 3 79.63 82.26 80.70 58.32 74.65 87.06 80.38 64.7113 Oracle candidate sentence extractor + row 3 86.67 92.12 89.25 63.72 79.71 91.49 85.20 62.00Table 3: this-issue resolution results with SVMrank.
All means evaluation using all features.
Issue-specific features ={IP, IVERB, IHEAD}.
EX-M is EXACT-M.FRLL, combines these two scores.PRLL =1nk?i=1LCS(gi,ai) (2)RRLL =1mk?i=1LCS(gi,ai) (3)FRLL =2?PRLL?RRLLPRLL +RRLL(4)The lower bound of FRLL is 0, where no true an-tecedent has any common substring with the pre-dicted antecedents and the upper bound is 1, whereall the predicted and true antecedents are exactly thesame.
In our results we represent these scores interms of percentage.There are no implemented systems that resolve is-sue anaphora or abstract anaphora signalled by labelnouns in arbitrary text to use as a comparison.
Sowe compare our results against two baselines: ad-jacent sentence and random.
The adjacent sentencebaseline chooses the previous sentence as the correctantecedent.
This is a high baseline because in ourdata 84.1% of the antecedents lie within the adjacentsentence.
The random baseline chooses a candidatedrawn from a uniform random distribution over theset of candidates.1111Note that our FRLL scores for both baselines are rather highbecause candidates often have considerable overlap with oneanother; hence a wrong choice may still have a high FRLL score.We carried out two sets of systematic experi-ments in which we considered all combinations ofour twelve feature classes.
The first set consists of5-fold cross-validation experiments on our trainingdata.
The second set evaluates how well the modelbuilt on the training data works on the unseen testdata.Table 3 gives results of our system.
The first tworows are the baseline results.
Rows 3 to 8 give re-sults for some of the best performing feature sets.All systems based on our features beat both base-lines on F-scores and EXACT-M.
The empiricallyderived feature sets IP (issue patterns) and D (dis-tance) appeared in almost all best feature set com-binations.
Removing D resulted in a 6 percentagepoints drop in FRLL and a 4 percentage points dropin EXACT-M scores.
Surprisingly, feature set ST(syntactic type) was not included in most of the bestperforming set of feature sets.
The combination ofsyntactic and semantic feature sets {IP, D, EL, MC,L, SR, DT} gave the best FRLL and EXACT-M scoresfor the cross-validation experiments.
For the test-data experiments, the combination of semantic andlexical features {D, C, LO, L, SC, SR, DT} gavethe best FRLL results, whereas syntactic, discourse,and semantic features {IP, D, C, EL, L, SC, SR,DT} gave the best EXACT-M results.
Overall, row3 of the table gives reasonable results for both cross-validation and test-data experiments with no statisti-cally significant difference to the corresponding best1262EXACT-M scores in rows 6 and 5 respectively.12To pinpoint the errors made by our system, wecarried out three experiments.
In the first experi-ment, we examined the contribution of issue-specificfeatures versus non-issue features (rows 9 and 10).Interestingly, when we used only non-issue features,the performance dropped only slightly.
The FRLL re-sults from using only issue-specific features werebelow baseline, suggesting that the more generalfeatures associated with abstract anaphora play acrucial role in resolving this-issue anaphora.In the second experiment, we determined the er-ror caused by the candidate extractor component ofour system.
Row 12 of the table gives the resultwhen an oracle candidate extractor was used to addthe correct antecedent in the set of candidates when-ever our candidate extractor failed.
This did notaffect cross-validation results by much because ofthe rarity of such instances.
However, in the test-data experiment, the EXACT-M improvements thatresulted were statistically significant.
This showsthat our resolution algorithm was able to identify an-tecedents that were arbitrary spans of text.In the last experiment, we examined the effect ofthe reduction of the candidate search space.
We as-sumed an oracle candidate sentence extractor (Row13) which knows the exact candidate sentence inwhich the antecedent lies.
We can see that bothRLL and EXACT-M scores markedly improved inthis setting.
In response to these results, we traineda decision-tree classifier to identify the correct an-tecedent sentence with simple location and lengthfeatures and achieved 95% accuracy in identifyingthe correct candidate sentence.6 Discussion and ConclusionsWe have demonstrated the possibility of resolv-ing complex abstract anaphora, namely, this-issueanaphora having arbitrary antecedents.
The worktakes the annotation work of Botley (2006) and Dip-per and Zinsmeister (2011) to the next level by re-solving this-issue anaphora automatically.
We pro-posed a set of 43 automatically extracted featuresthat can be used for resolving abstract anaphora.12We performed a simple one-tailed, k-fold cross-validatedpaired t-test at significance level p = 0.05 to determine whetherthe difference between the EXACT-M scores of two featureclasses is statistically significant.Our results show that general abstract-anaphoraresolution features (i.e., other than issue-specificfeatures) play a crucial role in resolving this-issueanaphora.
This is encouraging, as it suggests thatthe approach could be generalized for other NPs ?especially NPs having similar semantic constraintssuch as this problem, this decision, and this conflict.The results also show that reduction of searchspace markedly improves the resolution perfor-mance, suggesting that a two-stage process that firstidentifies the broad region of the antecedent and thenpinpoints the exact antecedent might work betterthan the current single-stage approach.
The rationalebehind this two-stage process is twofold.
First, thesearch space of abstract anaphora is large and noisycompared to nominal anaphora.13 And second, it ispossible to reduce the search space and accuratelyidentify the broad region of the antecedents usingsimple features such as the location of the anaphorin the anaphor sentence (e.g., if the anaphor occursat the beginning of the sentence, the antecedent ismost likely present in the previous sentence).We chose scientific articles over general text be-cause in the former domain the actual referents areseldom discourse deictic (i.e., not present in thetext).
In the news domain, for instance, which wehave also examined and are presently annotating, alarge percentage of this-issue antecedents lie out-side the text.
For example, newspaper articles oftenquote sentences of others who talk about the issuesin their own world, as shown in example (6).
(6) As surprising and encouraging to organizers ofthe movement are the Wall Street names addedto their roster.
Prominent among them is PaulSinger, a hedge fund manager who is straightand chairman of the conservative ManhattanInstitute.
He has donated more than $8 millionto various same-sex marriage efforts, in statesincluding California, Maine, New Hampshire,New Jersey, New York and Oregon, much of itsince 2007.?It?s become something that gradually peo-13If we consider all well-defined syntactic constituents of asentence as issue candidates, in our data, a sentence has on av-erage 43.61 candidates.
Combinations of several well-definedsyntactic constituents only add to this number.
Hence if weconsider the antecedent candidates from the previous 2 or 3 sen-tences, the search space can become quite large and noisy.1263ple like myself weren?t afraid to fund, weren?tafraid to speak out on,?
Mr. Singer said in an in-terview.
?I?m somebody who is philosophicallyvery conservative, and on this issue I thoughtthat this really was important on the basis ofliberty and actual family stability.
?In such a case, the antecedent of this issue is notalways in the text of the newspaper article itself, butmust be inferred from the context of the quotationand the world of the speaker quoted.
That said, wedo not use any domain-specific information in ourthis-issue resolution model.
Our features are solelybased on distance, syntactic structure, and semanticand lexical properties of the candidate antecedentswhich could be extracted for text in any domain.Issue anaphora can also be signalled by demon-stratives other than this.
However, for our initialstudy, we chose this issue for two reasons.
First, inour corpus as well as in other general corpora suchas the New York Times corpus, issue occurs muchmore frequently with this than other demonstratives.Second, we did not want to increase the complexityof the problem by including the plural issues.Our approach needs further development to makeit useful.
Our broad goal is to resolve abstractanaphora signalled by label nouns in all kinds oftext.
At present, the major obstacle is that thereis very little annotated data available that could beused to train an abstract anaphora resolution sys-tem.
And the understanding of abstract anaphoraitself is still at an early stage; it would be prema-ture to think about unsupervised approaches.
In thiswork, we studied the narrow problem of resolutionof this-issue anaphora in the medical domain to geta good grasp of the general abstract-anaphora reso-lution problem.A number of extensions are planned for this work.First, we will extend the work to resolve other ab-stract anaphors (e.g., this decision, this problem).Second, we will experiment with a two-stage reso-lution approach.
Third, we would like to explore theeffect of including serious discourse structure fea-tures in our model.
(The feature sets SC and C en-code only shallow discourse information.)
Finally,during annotation, we noted a number of issue pat-terns (e.g., An open question is X, X is under debate);a possible extension is extracting issues and prob-lems from text using these patterns as seed patterns.7 AcknowledgementsWe thank Dr. Brian Budgell from the CanadianMemorial Chiropractic College for annotating ourdata and for helpful discussions.
We also thankthe anonymous reviewers for their detailed and con-structive comments.
This research was financiallysupported by the Natural Sciences and EngineeringResearch Council of Canada and by the Universityof Toronto.ReferencesNicholas Asher.
1993.
Reference to Abstract Objects inDiscourse.
Kluwer Academic Publishers, Dordrecht,Netherlands.Philip Simon Botley.
2006.
Indirect anaphora: Testingthe limits of corpus-based linguistics.
InternationalJournal of Corpus Linguistics, 11(1):73?112.Donna K. Byron.
2003.
Annotation of pronouns andtheir antecedents: A comparison of two domains.Technical Report, University of Rochester.Donna K. Byron.
2004.
Resolving pronominal refer-ence to abstract entities.
Ph.D. thesis, Rochester, NewYork: University of Rochester.Jose?
Castan?o, Jason Zhang, and James Pustejovsky.2002.
Anaphora resolution in biomedical literature.
InProceedings of the International Symposium on Refer-ence Resolution for NLP, Alicante, Spain, June.Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim Tan.2011.
A unified event coreference resolution by inte-grating multiple resolvers.
In Proceedings of 5th Inter-national Joint Conference on Natural Language Pro-cessing, Chiang Mai, Thailand, November.Pascal Denis and Jason Baldridge.
2008.
Specializedmodels and ranking for coreference resolution.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 660?669,Honolulu, Hawaii, October.
Association for Computa-tional Linguistics.Stefanie Dipper and Heike Zinsmeister.
2011.
Annotat-ing abstract anaphora.
Language Resources and Eval-uation, 69:1?16.Miriam Eckert and Michael Strube.
2000.
Dialogue acts,synchronizing units, and anaphora resolution.
Journalof Semantics, 17:51?89.Gill Francis.
1994.
Labelling discourse: an aspectof nominal group lexical cohesion.
In MalcolmCoulthard, editor, Advances in written text analysis,pages 83?101, London.
Routledge.Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski.1993.
Cognitive status and the form of referring ex-1264pressions in discourse.
Language, 69(2):274?307,June.Graeme Hirst.
1981.
Anaphora in Natural Language Un-derstanding: A Survey, volume 119 of Lecture Notesin Computer Science.
Springer.Thorsten Joachims.
2002.
Optimizing search engines us-ing clickthrough data.
In ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining (KDD),pages 133?142.Klaus Krippendorff.
1995.
On the reliability of unitizingcontiguous data.
Sociological Methodology, 25:47?76.Klaus Krippendorff.
2004.
Content Analysis: An In-troduction to Its Methodology.
Sage, Thousand Oaks,CA, second edition.Yu-Hsiang Lin and Tyne Liang.
2004.
Pronominal andsortal anaphora resolution for biomedical literature.
InProceedings of ROCLING XVI: Conference on Com-putational Linguistics and Speech Processing, Taiwan,September.Chin-Yew Lin.
2004.
ROUGE: A package for auto-matic evaluation of summaries.
In Text SummarizationBranches Out: Proceedings of the ACL-04 Workshop,pages 74?81, Barcelona, Spain, July.
Association forComputational Linguistics.Ruslan Mitkov.
2002.
Anaphora Resolution.
Longman.Natalia N. Modjeska.
2003.
Resolving Other-Anaphora.Ph.D.
thesis, School of Informatics, University of Ed-inburgh.Christoph Mu?ller.
2008.
Fully Automatic Resolution ofIt, This and That in Unrestricted Multi-Party Dialog.Ph.D.
thesis, Universita?t Tu?bingen.Costanza Navarretta.
2011.
Antecedent and referenttypes of abstract pronominal anaphora.
In Proceed-ings of the Workshop Beyond Semantics: Corpus-based investigations of pragmatic and discourse phe-nomena, Go?ttingen, Germany, February.Rebecca Passonneau.
1989.
Getting at discourse refer-ents.
In Proceedings of the 27th Annual Meeting ofthe Association for Computational Linguistics, pages51?59, Vancouver, British Columbia, Canada, June.Association for Computational Linguistics.Massimo Poesio and Ron Artstein.
2008.
Anaphoricannotation in the ARRAU corpus.
In Proceedings ofthe Sixth International Conference on Language Re-sources and Evaluation (LREC?08), Marrakech, Mo-rocco, May.Massimo Poesio and Natalia N. Modjeska.
2002.The THIS-NPs hypothesis: A corpus-based investiga-tion.
In Proceedings of the 4th Discourse Anaphoraand Anaphor Resolution Conference (DAARC 2002),pages 157?162, Lisbon, Portugal, September.Massimo Poesio, Simone Ponzetto, and Yannick Versley.2011.
Computational models of anaphora resolution:A survey.
Unpublished.Hans-Jo?rg Schmid.
2000.
English Abstract Nouns AsConceptual Shells: From Corpus to Cognition.
Topicsin English Linguistics.
De Gruyter Mouton, Berlin.Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.2001.
A machine learning approach to coreferenceresolution of noun phrases.
Computational Linguis-tics, 27(4):521?544.Michael Strube and Christoph Mu?ller.
2003.
A machinelearning approach to pronoun resolution in spoken di-alogue.
In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics, pages168?175, Sapporo, Japan, July.
Association for Com-putational Linguistics.Manabu Torii and K. Vijay-Shanker.
2007.
Sortalanaphora resolution in Medline abstracts.
Computa-tional Intelligence, 23(1):15?27.1265
