Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 50?54,Baltimore, Maryland USA, June 26 2014.c?2014 Association for Computational LinguisticsSoftware Requirements: A new Domain for Semantic ParsersMichael Roth?
Themistoklis Diamantopoulos?
Ewan Klein?
Andreas Symeonidis?
?ILCC, School of InformaticsUniversity of Edinburgh{mroth,ewan}@inf.ed.ac.uk?Electrical & Computer Engineering DepartmentAristotle University of Thessalonikithdiaman@issel.ee.auth.grasymeon@eng.auth.grAbstractSoftware requirements are commonlywritten in natural language, making themprone to ambiguity, incompleteness andinconsistency.
By converting require-ments to formal semantic representations,emerging problems can be detected at anearly stage of the development process,thus reducing the number of ensuing errorsand the development costs.
In this paper,we treat the mapping from requirements toformal representations as a semantic pars-ing task.
We describe a novel data set forthis task that involves two contributions:first, we establish an ontology for formallyrepresenting requirements; and second, weintroduce an iterative annotation scheme,in which formal representations are de-rived through step-wise refinements.1 IntroductionDuring the process of software development, de-velopers and customers typically discuss andagree on requirements that specify the function-ality of a system that is being developed.1Suchrequirements play a crucial role in the develop-ment lifecycle, as they form the basis for actualimplementations, corresponding work plans, costestimations and follow-up directives (van Lam-sweerde, 2009).
In general, software requirementscan be expressed in various different ways, includ-ing the use of UML diagrams and storyboards.Most commonly, however, expectations are ex-pressed in natural language (Mich et al., 2004), asshown in Example (1):(1) A user should be able to login to his account.1Although software engineering can also involve non-functional requirements, which describe general quality cri-teria of a system, this paper is only concerned with functionalrequirements, i.e., requirements that specify the behavior of asystem.While requirements expressed in natural lan-guage have the advantage of being intelligible toboth clients and developers, they can of coursealso be ambiguous, vague and incomplete.
Al-though formal languages could be used as an alter-native that eliminates some of these problems, cus-tomers are rarely equipped with the mathematicaland technical expertise for understanding highlyformalised requirements.
To benefit from the ad-vantages of both natural language and formal rep-resentations, we propose to induce the latter au-tomatically from text in a semantic parsing task.Given the software requirement in Example (1),for instance, we would like to construct a represen-tation that explicitly specifies the types of the en-tities involved (e.g., object(account)) and that cap-tures explicit and inferable relationships amongthem (e.g., owns(user, account)).
We expect suchformal representations to be helpful in detectingerrors at an early stage of the development process(e.g., via logical inference and verification tools),thus avoiding the costs of finding and fixing prob-lems at a later and hence more expensive stage(Boehm and Basili, 2001).Given the benefits of formal representations,we believe that software requirements constitutea useful application domain for semantic parsers.Requirement texts naturally occur in the real worldand appropriate data sets can thus be constructedwithout setting up artificial tasks to collect them.Parsing requirements of different software projectsalso poses interesting challenges as texts exhibit aconsiderable amount of lexical variety, while fre-quently also containing more than one relation persentence.2 Related WorkA range of methods have been proposed in previ-ous work to (semi-)automatically process require-ments written in plain, natural language text andmap them to formal representations.
To the best50of our knowledge, Abbott (1983) was the first tointroduce a technique for extracting data types,variables and operators from informal texts de-scribing a problem.
The proposed method fol-lows a simple rule-based setup, in which commonnouns are identified as data types, proper nounsas objects and verbs as operators between them.Booch (1986) described a method of similar com-plexity that extends Abbot?s approach to object-oriented development.
Saeki et al.
(1989) imple-mented a first prototype that automatically con-structs object-oriented models from informal re-quirements.
As proposed by Abbott and Booch,the system is based on automatically extractednouns and verbs.
Although Saeki et al.
found re-sulting object diagrams of reasonable quality, theyconcluded that human intervention was still nec-essary to distinguish between words that are rele-vant for the model and irrelevant nouns and verbs.Nanduri and Rugaber (1995) proposed to furtherautomate object-oriented analysis of requirementtexts by applying a syntactic parser and a set ofpost-processing rules.
In a similar setting, Mich(1996) employed a full NLP pipeline that con-tains a semantic analysis module, thus omitting theneed for additional post-processing rules.
Morerecent approaches include those by Harmain andGaizauskas (2003) and Kof (2004), who relied ona combination of NLP components and human in-teraction.
Whereas most approaches in previouswork aim to derive class diagrams, Ghosh et al.
(2014) proposed a pipeline architecture that con-verts syntactic parses to logical expressions via aset of heuristic post-processing rules.Despite this seemingly long tradition, previ-ous methods for processing software requirementshave tended to depend on domain-specific heuris-tics and knowledge bases or have required addi-tional user intervention.
In contrast, we proposeto utilize annotated data to learn how to performsemantic parsing of requirements automatically.3 Data SetGiven our conviction that mapping natural lan-guage software requirements to formal representa-tions provides an attractive challenge for semanticparsing research, we believe that there is a moregeneral benefit in building a corpus of annotatedrequirements.
One immediate obstacle is that soft-ware requirements can drastically differ in quality,style and granularity.
To cover a range of possible#sentences #tokens #typesstudent projects 270 3130 604industrial prototypes 55 927 286Our dataset (total) 325 4057 765GEOQUERY880 880 6656 279FREE917 917 6769 2035Table 1: Statistics on our requirements collectionand existing semantic parsing data sets.differences, we asked lecturers from several uni-versities to provide requirement documents writ-ten by students.
We received requirement docu-ments on student projects from various domains,including embedded systems, virtual reality andweb applications.2From these documents, we ex-tracted lists of requirements, each of which is ex-pressed within a single sentence.
We addition-ally collected single sentence requirements withinthe S-CASE project, describing industrial proto-types of cloud-based web services.3Table 1 givesan overview of the quantity of requirements col-lected.
We observe that the number of require-ments received for student projects is much higher.The token counts reveal however that require-ments written for industrial prototypes are longeron average (16.6 vs. 11.6 words).
This observa-tion might be related to the fact that students insoftware engineering classes are often providedwith explicit guidelines on how to concisely ex-press requirements in natural language.
As a con-sequence, we also find their requirement texts tobe more regimented and stylised than those writ-ten by senior software engineers.
Examples (2)and (3) show examples of a student-written anddeveloper-written requirement, respectively.
(2) The user must be able to vote on polls.
(3) For each user contact, back-end must performa check to determine whether the contact is aregistered user or not.In comparison to two extant data sets, namelyGeoQuery880 (Tang, 2003) and Free917 (Cai andYates, 2013), we find that our collection is still rel-atively small in terms of example sentences.
The2The majority of collected requirements arefrom a software development course organizedjointly by several European universities, cf.http://www.fer.unizg.hr/rasip/dsd3http://www.scasefp7.eu/51ConceptOperationTypeThingTypeActionEmergenceStatusOwnershipPropertyParticipantObjectActorlevel 1 level 2 level 3Figure 1: Class hierarchy of our conceptual ontol-ogy for modeling software requirements.difference in total number of tokens is not as cru-cial, however, given that sentences in our data setare much longer on average.
We further observethat the token/type ratio in our texts lies some-where between ratios reported in previous work.Based on the observed lexical variety and averagesentence length, we expect our texts to be chal-lenging but not too difficult to parse using existingmethods.4 Modeling Requirements ConceptuallyDifferent representations have been proposed formodeling requirements in previous work: whereasearly work focused on deriving simple class dia-grams, more recent approaches suggest represent-ing requirements via logical forms (cf.
Section 2).In this paper, we propose to model requirementsusing a formal ontology that captures general con-cepts from different application domains.
Our pro-posed ontology covers the same properties as ear-lier work and provides a means to represent re-quirements in logical form.
In practice, such logi-cal forms can be induced by semantic parsers andin subsequent steps be utilized for automatic infer-ence.
The class hierarchy of our ontology is shownin Figure 1.
At the highest level of the class hierar-chy, we distinguish between ?things?
(ThingType)and ?operations?
(OperationType).4.1 ThingTypeWe define the following subclasses of ThingType:?
A Participant is a thing that is involved in anoperation.
We further subdivide Participantsinto Actors, which can be users of a systemor the system itself, and Objects.?
A Property is an attribute of an Object or acharacteristic of an OperationType.4.2 OperationTypeWe further divide operations into the followingsubclasses:?
An Action describes an operation that is per-formed by an Actor on one or several Ob-ject(s).?
A State is an operation that describes the sta-tus of an Actor.?
Ownership is used to model operations thatexpress possession.?
Emergence represent operations that undergopassive transformation.4.3 RelationsIn addition to the class hierarchy, we define a setof relations between classes, which describe andconstrain how different operations and things caninteract with each other.On the level of OperationType, every opera-tion can be assigned one Actor via the relationsHAS ACTOR or HAS OWNER, respectively.
Ob-jects can participate in Actions, States and Owner-ships via the relations ACTS ON, HAS STATE andOWNS, respectively.
Every instance of Opera-tionType and Object can further have an arbitrarynumber of properties assigned to it via the relationHAS PROPERTY.5 Annotation ProcessIn preliminary annotation experiments, we foundthat class diagrams may be too simple to repre-sent requirements conceptually.
Logical forms, onthe other hand, can be difficult to use for anno-tators without sufficient background knowledge.To keep the same level of expressiveness as log-ical forms and the simplicity of object-orientedannotations, we propose a multi-step annotationscheme, in which decisions in one iteration are fur-ther refined in later iterations.By adopting the class hierarchy introduced inSection 4, we can naturally divide each annotationiteration according to a level in the ontology.
Thismeans that in the first iteration, we ask annotators52A user that is logged in to his account must be able to update his password.Actor(user) ?
Action(login) ?
Action(update)?
Object(account) ?
HAS ACTOR(login,user) ?
HAS ACTOR(update,user)?
Object(password) ?
ACTS ON(login,account) ?
ACTS ON(update,password)?
Ownership(o1) ?
Ownership(o2)?
HAS OWNER(o1,user) ?
HAS OWNER(o2,user)?
OWNS(o1,account) ?
OWNS(o2,password)The system must be able to forward and rewind a playing program.Actor(system) ?
Action(forward) ?
Action(rewind)?
Object(program) ?
HAS ACTOR(forward,system) ?
HAS ACTOR(rewind,system)?
ACTS ON(forward,program) ?
ACTS ON(rewind,program)?
Property(playing) ?
HAS PROPERTY(program,playing)Table 2: Example requirements from different domains and logical forms derived from annotations.A user should be ableloginto his accountThingTypeOperationTypeThingTypeParticipantActionParticipantActor ObjectHAS ACTOR ACTS ON(implicit)OwnershipHAS OWNER OWNSFigure 2: Annotation process: instances aremarked in text (dashed), class assignments are re-fined (dotted), and relations are added (solid).to simply mark all instances of ThingType and Op-erationType that are explicitly expressed in a givenrequirement.
We then resolve conflicting annota-tions and present the resulting instances from thefirst level to annotators for the next iteration.
Ineach iteration, we add one layer of sophisticationfrom the class hierarchy, resulting in step-wise re-finements.
In the final iteration, we add relationsbetween instances of concepts, including implicitbut inferable cases.An illustration of the overall annotation process,based on Example (1), is depicted in Figure 2.
Thelast iteration in this example involves the additionof an Ownership instance that is indicated (by thephrase ?his account?)
but not explicitly realized intext.
Although identifying and annotating such in-stances can be more challenging than the previousannotation steps, we can directly populate our on-tology at this stage (e.g., via conversion to RDFtuples) and run verification tools to check whetherthey are consistent with the annotation schema.6 DiscussionThe annotation scheme introduced in Section 4 isdesigned with the goal of covering a wide rangeof different application domains.
Although thismeans that many of the more fine-grained distinc-tions within a domain are not considered here, webelieve that the scheme already provides sufficientinformation for a range of tasks.
By storing pro-cessed requirements in a relational database, forexample, they can be retrieved using structuredqueries and utilized for probabilistic inference.Given the hierarchical structure of our annota-tion process, as defined in Section 5, it is possibleto extend existing annotations with additional lev-els of granularity provided by domain ontologies.As an example, we have defined a domain ontol-ogy for web services, which contains subclassesof Action to further distinguish between the HTTPmethods get, put, post and delete.
Similar exten-sions can be defined for other domains.Regarding the task of semantic parsing itself,we are currently in the process of annotating sev-eral hundreds of instances of requirements (cf.Section 3) following the proposed ontology.
Wewill release an initial version of this data set atthe Semantic Parsing workshop.
The initial re-lease will serve as a basis for training and eval-uating parsers in this domain, for which we arealso planning to collect more examples through-out the year.
We believe that requirements forman interesting domain for the parsing community53as the texts involve a fair amount of variation andchallenging semantic phenomena (such as infer-able relations), while also serving a practical andvaluable purpose.AcknowledgementsParts of this work have been supported by the FP7Collaborative Project S-CASE (Grant AgreementNo 610717), funded by the European Commis-sion.
We thank our project partners for data sup-port and useful discussions on the proposed ontol-ogy.ReferencesRussell J Abbott.
1983.
Program design by informalenglish descriptions.
Communications of the ACM,26(11):882?894.Barry Boehm and Victor R. Basili.
2001.
Softwaredefect reduction top 10 list.
Computer, 34:135?137.Grady Booch.
1986.
Object-oriented develop-ment.
IEEE Transactions on Software Engineering,(2):211?221.Qingqing Cai and Alexander Yates.
2013.
Large-scalesemantic parsing via schema matching and lexiconextension.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 423?433, Sofia,Bulgaria, August.Shalini Ghosh, Daniel Elenius, Wenchao Li, PatrickLincoln, Natarajan Shankar, and Wilfried Steiner.2014.
Automatically extracting requirements spec-ifications from natural language.
arXiv preprintarXiv:1403.3142.H.
M. Harmain and Robert Gaizauskas.
2003.
Cm-builder: A natural language-based case tool forobject-oriented analysis.
Automated Software Engi-neering, 10(2):157?181.Leonid Kof.
2004.
Natural language processing forrequirements engineering: Applicability to large re-quirements documents.
In 19th International Con-ference on Automated Software Engineering, Work-shop Proceedings.Luisa Mich, Franch Mariangela, and Novi InverardiPierluigi.
2004.
Market research for requirementsanalysis using linguistic tools.
Requirements Engi-neering, 9(1):40?56.Luisa Mich. 1996.
NL-OOPS: From natural languageto object oriented requirements using the natural lan-guage processing system LOLITA.
Natural Lan-guage Engineering, 2(2):161?187.Sastry Nanduri and Spencer Rugaber.
1995.
Re-quirements validation via automated natural lan-guage parsing.
In Proceedings of the Twenty-EighthHawaii International Conference on System Sci-ences, volume 3, pages 362?368.Motoshi Saeki, Hisayuki Horai, and Hajime Enomoto.1989.
Software development process from naturallanguage specification.
In Proceedings of the 11thInternational Conference on Software Engineering,pages 64?73.Lappoon R. Tang.
2003.
Integrating Top-down andBottom-up Approaches in Inductive Logic Program-ming: Applications in Natural Language Processingand Relational Data Mining.
Ph.D. thesis, Depart-ment of Computer Sciences, University of Texas,Austin, Texas, USA, August.Axel van Lamsweerde.
2009.
Requirements Engineer-ing: From System Goals to UML Models to SoftwareSpecifications.
Wiley.54
