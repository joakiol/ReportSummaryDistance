Proceedings of the ACL 2010 Conference Short Papers, pages 247?252,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsDon?t ?have a clue?
?Unsupervised co-learning of downward-entailing operatorsCristian Danescu-Niculescu-Mizil and Lillian LeeDepartment of Computer Science, Cornell Universitycristian@cs.cornell.edu, llee@cs.cornell.eduAbstractResearchers in textual entailment havebegun to consider inferences involvingdownward-entailing operators, an inter-esting and important class of lexical itemsthat change the way inferences are made.Recent work proposed a method for learn-ing English downward-entailing operatorsthat requires access to a high-quality col-lection of negative polarity items (NPIs).However, English is one of the very fewlanguages for which such a list exists.
Wepropose the first approach that can be ap-plied to the many languages for whichthere is no pre-existing high-precisiondatabase of NPIs.
As a case study, weapply our method to Romanian and showthat our method yields good results.
Also,we perform a cross-linguistic analysis thatsuggests interesting connections to somefindings in linguistic typology.1 IntroductionCristi: ?Nicio?
... is that adjective you?ve mentioned.Anca: A negative pronominal adjective.Cristi: You mean there are people who analyze thatkind of thing?Anca: The Romanian Academy.Cristi: They?re crazy.
?From the movie Police, adjectiveDownward-entailing operators are an interest-ing and varied class of lexical items that changethe default way of dealing with certain types ofinferences.
They thus play an important role inunderstanding natural language [6, 18?20, etc.
].We explain what downward entailing means byfirst demonstrating the ?default?
behavior, whichis upward entailing.
The word ?observed?
is anexample upward-entailing operator: the statement(i) ?Witnesses observed opium use.
?implies(ii) ?Witnesses observed narcotic use.
?but not vice versa (we write i ?
( 6?)
ii).
Thatis, the truth value is preserved if we replace theargument of an upward-entailing operator by a su-perset (a more general version); in our case, the set?opium use?
was replaced by the superset ?narcoticuse?.Downward-entailing (DE) (also known asdownward monotonic or monotone decreasing)operators violate this default inference rule: withDE operators, reasoning instead goes from ?sets tosubsets?.
An example is the word ?bans?
:?The law bans opium use?6?
(?
)?The law bans narcotic use?.Although DE behavior represents an exception tothe default, DE operators are as a class rather com-mon.
They are also quite diverse in sense andeven part of speech.
Some are simple negations,such as ?not?, but some other English DE opera-tors are ?without?, ?reluctant to?, ?to doubt?, and?to allow?.1 This variety makes them hard to ex-tract automatically.Because DE operators violate the default ?setsto supersets?
inference, identifying them can po-tentially improve performance in many NLP tasks.Perhaps the most obvious such tasks are those in-volving textual entailment, such as question an-swering, information extraction, summarization,and the evaluation of machine translation [4].
Re-searchers are in fact beginning to build textual-entailment systems that can handle inferences in-volving downward-entailing operators other thansimple negations, although these systems almostall rely on small handcrafted lists of DE operators[1?3, 15, 16].2 Other application areas are natural-language generation and human-computer interac-tion, since downward-entailing inferences induce1Some examples showing different constructions for ana-lyzing these operators: ?The defendant does not own a bluecar?
6?
(?)
?The defendant does not own a car?
; ?They arereluctant to tango?
6?
(?)
?They are reluctant to dance?
;?Police doubt Smith threatened Jones?
6?
(?)
?Police doubtSmith threatened Jones or Brown?
; ?You are allowed to useMastercard?
6?
(?)
?You are allowed to use any credit card?.2The exception [2] employs the list automatically derivedby Danescu-Niculescu-Mizil, Lee, and Ducott [5], describedlater.247greater cognitive load than inferences in the oppo-site direction [8].Most NLP systems for the applications men-tioned above have only been deployed for a smallsubset of languages.
A key factor is the lackof relevant resources for other languages.
Whileone approach would be to separately develop amethod to acquire such resources for each lan-guage individually, we instead aim to amelioratethe resource-scarcity problem in the case of DEoperators wholesale: we propose a single unsuper-vised method that can extract DE operators in anylanguage for which raw text corpora exist.Overview of our work Our approach takes theEnglish-centric work of Danescu-Niculescu-Mizilet al [5] ?
DLD09 for short ?
as a starting point,as they present the first and, until now, only al-gorithm for automatically extracting DE operatorsfrom data.
However, our work departs signifi-cantly from DLD09 in the following key respect.DLD09 critically depends on access to a high-quality, carefully curated collection of negativepolarity items (NPIs) ?
lexical items such as?any?, ?ever?, or the idiom ?have a clue?
that tendto occur only in negative environments (see ?2for more details).
DLD09 use NPIs as signals ofthe occurrence of downward-entailing operators.However, almost every language other than En-glish lacks a high-quality accessible NPI list.To circumvent this problem, we introduce aknowledge-lean co-learning approach.
Our al-gorithm is initialized with a very small seed setof NPIs (which we describe how to generate), andthen iterates between (a) discovering a set of DEoperators using a collection of pseudo-NPIs ?
aconcept we introduce ?
and (b) using the newly-acquired DE operators to detect new pseudo-NPIs.Why this isn?t obvious Although the algorith-mic idea sketched above seems quite simple, it isimportant to note that prior experiments in thatdirection have not proved fruitful.
Preliminarywork on learning (German) NPIs using a smalllist of simple known DE operators did not yieldstrong results [14].
Hoeksema [10] discusses whyNPIs might be hard to learn from data.3 We cir-cumvent this problem because we are not inter-ested in learning NPIs per se; rather, for our pur-3In fact, humans can have trouble agreeing on NPI-hood;for instance, Lichte and Soehn [14] mention doubts aboutover half of Ku?rschner [12]?s 344 manually collected GermanNPIs.poses, pseudo-NPIs suffice.
Also, our prelim-inary work determined that one of the most fa-mous co-learning algorithms, hubs and authoritiesor HITS [11], is poorly suited to our problem.4Contributions To begin with, we apply our al-gorithm to produce the first large list of DE opera-tors for a language other than English.
In our casestudy on Romanian (?4), we achieve quite highprecisions at k (for example, iteration achieves aprecision at 30 of 87%).Auxiliary experiments explore the effects of us-ing a large but noisy NPI list, should one be avail-able for the language in question.
Intriguingly, wefind that co-learning new pseudo-NPIs providesbetter results.Finally (?5), we engage in some cross-linguisticanalysis based on the results of applying our al-gorithm to English.
We find that there are somesuggestive connections with findings in linguistictypology.Appendix available A more complete accountof our work and its implications can be found in aversion of this paper containing appendices, avail-able at www.cs.cornell.edu/?cristian/acl2010/.2 DLD09: successes and challengesIn this section, we briefly summarize those aspectsof the DLD09 method that are important to under-standing how our new co-learning method works.DE operators and NPIs Acquiring DE opera-tors is challenging because of the complete lack ofannotated data.
DLD09?s insight was to make useof negative polarity items (NPIs), which are wordsor phrases that tend to occur only in negative con-texts.
The reason they did so is that Ladusaw?s hy-pothesis [7, 13] asserts that NPIs only occur withinthe scope of DE operators.
Figure 1 depicts exam-ples involving the English NPIs ?any?5 and ?havea clue?
(in the idiomatic sense) that illustrate thisrelationship.
Some other English NPIs are ?ever?,?yet?
and ?give a damn?.Thus, NPIs can be treated as clues that a DEoperator might be present (although DE operatorsmay also occur without NPIs).4We explored three different edge-weighting schemesbased on co-occurrence frequencies and seed-set member-ship, but the results were extremely poor; HITS invariablyretrieved very frequent words.5The free-choice sense of ?any?, as in ?I can skim any pa-per in five minutes?, is a known exception.248NPIsDE operators any3 have a clue, idiomatic sensenot or n?t X We do n?t have any apples X We do n?t have a cluedoubt XI doubt they have any apples X I doubt they have a clueno DE operator ?
They have any apples ?
They have a clueFigure 1: Examples consistent with Ladusaw?s hypothesis that NPIs can only occur within the scope ofDE operators.
A X denotes an acceptable sentence; a ?
denotes an unacceptable sentence.DLD09 algorithm Potential DE operators arecollected by extracting those words that appear inan NPI?s context at least once.6 Then, the potentialDE operators x are ranked byf(x) :=fraction of NPI contexts that contain xrelative frequency of x in the corpus,which compares x?s probability of occurrenceconditioned on the appearance of an NPI with itsprobability of occurrence overall.7The method just outlined requires access to alist of NPIs.
DLD09?s system used a subset ofJohn Lawler?s carefully curated and ?moderatelycomplete?
list of English NPIs.8 The resultantrankings of candidate English DE operators werejudged to be of high quality.The challenge in porting to other languages:cluelessness Can the unsupervised approach ofDLD09 be successfully applied to languages otherthan English?
Unfortunately, for most other lan-guages, it does not seem that large, high-qualityNPI lists are available.One might wonder whether one can circumventthe NPI-acquisition problem by simply translatinga known English NPI list into the target language.However, NPI-hood need not be preserved undertranslation [17].
Thus, for most languages, welack the critical clues that DLD09 depends on.3 Getting a clueIn this section, we develop an iterative co-learning algorithm that can extract DE operatorsin the many languages where a high-quality NPI6DLD09 policies: (a) ?NPI context?
was defined as thepart of the sentence to the left of the NPI up to the firstcomma, semi-colon or beginning of sentence; (b) to encour-age the discovery of new DE operators, those sentences con-taining one of a list of 10 well-known DE operators were dis-carded.
For Romanian, we treated only negations (?nu?
and?n-?)
and questions as well-known environments.7DLD09 used an additional distilled score, but we foundthat the distilled score performed worse on Romanian.8http://www-personal.umich.edu/?jlawler/aue/npi.htmldatabase is not available, using Romanian as acase study.3.1 Data and evaluation paradigmWe used Rada Mihalcea?s corpus of?1.45 millionsentences of raw Romanian newswire articles.Note that we cannot evaluate impact on textualinference because, to our knowledge, no publiclyavailable textual-entailment system or evaluationdata for Romanian exists.
We therefore examinethe system outputs directly to determine whetherthe top-ranked items are actually DE operators ornot.
Our evaluation metric is precision at k of agiven system?s ranked list of candidate DE oper-ators; it is not possible to evaluate recall since nolist of Romanian DE operators exists (a problemthat is precisely the motivation for this paper).To evaluate the results, two native Romanianspeakers labeled the system outputs as being?DE?, ?not DE?
or ?Hard (to decide)?.
The la-beling protocol, which was somewhat complexto prevent bias, is described in the externally-available appendices (?7.1).
The complete systemoutput and annotations are publicly available at:http://www.cs.cornell.edu/?cristian/acl2010/.3.2 Generating a seed setEven though, as discussed above, the translationof an NPI need not be an NPI, a preliminary re-view of the literature indicates that in many lan-guages, there is some NPI that can be translatedas ?any?
or related forms like ?anybody?.
Thus,with a small amount of effort, one can form a min-imal NPI seed set for the DLD09 method by us-ing an appropriate target-language translation of?any?.
For Romanian, we used ?vreo?
and ?vreun?,which are the feminine and masculine translationsof English ?any?.3.3 DLD09 using the Romanian seed setWe first check whether DLD09 with the two-item seed set described in ?3.2 performs well onRomanian.
In fact, the results are fairly poor:2490 5 9 10 150510152025303540k=10k=20k=30k=40k=50k=80IterationNumberof DE?operators10 20 30 40 50 60 70 800102030405060708090100kPrecision at k (in %)DEHardFigure 2: Left: Number of DE operators in the top k results returned by the co-learning method at each iteration.Items labeled ?Hard?
are not included.
Iteration 0 corresponds to DLD09 applied to {?vreo?, ?vreun?}.
Curves fork = 60 and 70 omitted for clarity.
Right: Precisions at k for the results of the 9th iteration.
The bar divisions are:DE (blue/darkest/largest) and Hard (red/lighter, sometimes non-existent).for example, the precision at 30 is below 50%.
(See blue/dark bars in figure 3 in the externally-available appendices for detailed results.
)This relatively unsatisfactory performance maybe a consequence of the very small size of the NPIlist employed, and may therefore indicate that itwould be fruitful to investigate automatically ex-tending our list of clues.3.4 Main idea: a co-learning approachOur main insight is that not only can NPIs be usedas clues for finding DE operators, as shown byDLD09, but conversely, DE operators (if known)can potentially be used to discover new NPI-likeclues, which we refer to as pseudo-NPIs (or pNPIsfor short).
By ?NPI-like?
we mean, ?serve as pos-sible indicators of the presence of DE operators,regardless of whether they are actually restrictedto negative contexts, as true NPIs are?.
For exam-ple, in English newswire, the words ?allegation?
or?rumor?
tend to occur mainly in DE contexts, like?
denied ?
or ?
dismissed ?, even though they areclearly not true NPIs (the sentence ?I heard a ru-mor?
is fine).
Given this insight, we approach theproblem using an iterative co-learning paradigmthat integrates the search for new DE operatorswith a search for new pNPIs.First, we describe an algorithm that is the ?re-verse?
of DLD09 (henceforth rDLD), in that it re-trieves and ranks pNPIs assuming a given list ofDE operators.
Potential pNPIs are collected by ex-tracting those words that appear in a DE context(defined here, to avoid the problems of parsing orscope determination, as the part of the sentence tothe right of a DE operator, up to the first comma,semi-colon or end of sentence); these candidates xare then ranked byfr(x) :=fraction of DE contexts that contain xrelative frequency of x in the corpus.Then, our co-learning algorithm consists of theiteration of the following two steps:?
(DE learning) Apply DLD09 using a set Nof pseudo-NPIs to retrieve a list of candidateDE operators ranked by f (defined in Section2).
Let D be the top n candidates in this list.?
(pNPI learning) Apply rDLD using the set Dto retrieve a list of pNPIs ranked by fr; ex-tend N with the top nr pNPIs in this list.
In-crement n.Here, N is initialized with the NPI seed set.
Ateach iteration, we consider the output of the al-gorithm to be the ranked list of DE operators re-trieved in the DE-learning step.
In our experi-ments, we initialized n to 10 and set nr to 1.4 Romanian resultsOur results show that there is indeed favorablesynergy between DE-operator and pNPI retrieval.Figure 2 plots the number of correctly retrievedDE operators in the top k outputs at each iteration.The point at iteration 0 corresponds to a datapointalready discussed above, namely, DLD09 appliedto the two ?any?-translation NPIs.
Clearly, we seegeneral substantial improvement over DLD09, al-though the increases level off in later iterations.250(Determining how to choose the optimal numberof iterations is a subject for future research.
)Additional experiments, described in theexternally-available appendices (?7.2), suggestthat pNPIs can even be more effective clues thana noisy list of NPIs.
(Thus, a larger seed setdoes not necessarily mean better performance.
)pNPIs also have the advantage of being derivableautomatically, and might be worth investigatingfrom a linguistic perspective in their own right.5 Cross-linguistic analysisApplying our algorithm to English: connec-tions to linguistic typology So far, we havemade no assumptions about the language on whichour algorithm is applied.
A valid question is, doesthe quality of the results vary with choice of appli-cation language?
In particular, what happens if werun our algorithm on English?Note that in some sense, this is a perverse ques-tion: the motivation behind our algorithm is thenon-existence of a high-quality list of NPIs forthe language in question, and English is essen-tially the only case that does not fit this descrip-tion.
On the other hand, the fact that DLD09 ap-plied their method for extraction of DE operatorsto English necessitates some form of comparison,for the sake of experimental completeness.We thus ran our algorithm on the EnglishBLLIP newswire corpus with seed set {?any?}
.We observe that, surprisingly, the iterative addi-tion of pNPIs has very little effect: the precisionsat k are good at the beginning and stay about thesame across iterations (for details see figure 5 inin the externally-available appendices).
Thus, onEnglish, co-learning does not hurt performance,which is good news; but unlike in Romanian, itdoes not lead to improvements.Why is English ?any?
seemingly so ?powerful?,in contrast to Romanian, where iterating beyondthe initial ?any?
translations leads to better re-sults?
Interestingly, findings from linguistic typol-ogy may shed some light on this issue.
Haspel-math [9] compares the functions of indefinite pro-nouns in 40 languages.
He shows that English isone of the minority of languages (11 out of 40)9 inwhich there exists an indefinite pronoun series thatoccurs in all (Haspelmath?s) classes of DE con-texts, and thus can constitute a sufficient seed on9English, Ancash Quechua, Basque, Catalan, French,Hindi/Urdu, Irish, Portuguese, Swahili, Swedish, Turkish.its own.
In the other languages (including Roma-nian),10 no indirect pronoun can serve as a suffi-cient seed.
So, we expect our method to be vi-able for all languages; while the iterative discov-ery of pNPIs is not necessary (although neither isit harmful) for the subset of languages for which asufficient seed exists, such as English, it is essen-tial for the languages for which, like Romanian,?any?-equivalents do not suffice.Using translation Another interesting questionis whether directly translating DE operators fromEnglish is an alternative to our method.
First, weemphasize that there exists no complete list of En-glish DE operators (the largest available collec-tion is the one extracted by DLD09).
Second, wedo not know whether DE operators in one lan-guage translate into DE operators in another lan-guage.
Even if that were the case, and we some-how had access to ideal translations of DLD09?slist, there would still be considerable value in us-ing our method: 14 (39%) of our top 36 highest-ranked Romanian DE operators for iteration 9 donot, according to the Romanian-speaking author,have English equivalents appearing on DLD09?s90-item list.
Some examples are: ?abt?inut?
(ab-stained), ?criticat?
(criticized) and ?react?ionat?
(re-acted).
Therefore, a significant fraction of theDE operators derived by our co-learning algorithmwould have been missed by the translation alterna-tive even under ideal conditions.6 ConclusionsWe have introduced the first method for discov-ering downward-entailing operators that is univer-sally applicable.
Previous work on automaticallydetecting DE operators assumed the existence ofa high-quality collection of NPIs, which renders itinapplicable in most languages, where such a re-source does not exist.
We overcome this limita-tion by employing a novel co-learning approach,and demonstrate its effectiveness on Romanian.Also, we introduce the concept of pseudo-NPIs.Auxiliary experiments described in the externally-available appendices show that pNPIs are actuallymore effective seeds than a noisy ?true?
NPI list.Finally, we noted some cross-linguistic differ-ences in performance, and found an interestingconnection between these differences and Haspel-math?s [9] characterization of cross-linguistic vari-ation in the occurrence of indefinite pronouns.10Examples: Chinese, German, Italian, Polish, Serbian.251Acknowledgments We thank Tudor Marian forserving as an annotator, Rada Mihalcea for ac-cess to the Romanian newswire corpus, and ClaireCardie, Yejin Choi, Effi Georgala, Mark Liber-man, Myle Ott, Joa?o Paula Muchado, Stephen Pur-pura, Mark Yatskar, Ainur Yessenalina, and theanonymous reviewers for their helpful comments.Supported by NSF grant IIS-0910664.References[1] Roy Bar-Haim, Jonathan Berant, Ido Da-gan, Iddo Greental, Shachar Mirkin, EyalShnarch, and Idan Szpektor.
Efficient seman-tic deduction and approximate matching overcompact parse forests.
In Proceedings of theText Analysis Conference (TAC), 2008.
[2] Eric Breck.
A simple system for detectingnon-entailment.
In Proceedings of the TextAnalysis Conference (TAC), 2009.
[3] Christos Christodoulopoulos.
Creating a nat-ural logic inference system with combinatorycategorial grammar.
Master?s thesis, Univer-sity of Edinburgh, 2008.
[4] Ido Dagan, Oren Glickman, and BernardoMagnini.
The PASCAL Recognising TextualEntailment challenge.
In Machine Learn-ing Challenges, Evaluating Predictive Un-certainty, Visual Object Classification andRecognizing Textual Entailment, First PAS-CAL Machine Learning Challenges Work-shop, pages 177?190.
Springer, 2006.
[5] Cristian Danescu-Niculescu-Mizil, LillianLee, and Richard Ducott.
Without a ?doubt?
?Unsupervised discovery of downward-entailing operators.
In Proceedings ofNAACL HLT, 2009.
[6] David Dowty.
The role of negative polar-ity and concord marking in natural languagereasoning.
In Mandy Harvey and Lynn San-telmann, editors, Proceedings of SALT IV,pages 114?144, 1994.
[7] Gilles Fauconnier.
Polarity and the scaleprinciple.
In Proceedings of the Chicago Lin-guistic Society (CLS), pages 188?199, 1975.Reprinted in Javier Gutierrez-Rexach (ed.
),Semantics: Critical Concepts in Linguistics,2003.
[8] Bart Geurts and Frans van der Slik.
Mono-tonicity and processing load.
Journal of Se-mantics, 22(1):97?117, 2005.
[9] Martin Haspelmath.
Indefinite Pronouns.Oxford University Press, 2001.
[10] Jack Hoeksema.
Corpus study of negativepolarity items.
IV-V Jornades de corpus lin-guistics 1996-1997, 1997.
http://odur.let.rug.nl/?hoeksema/docs/barcelona.html.
[11] Jon Kleinberg.
Authoritative sources in a hy-perlinked environment.
In Proceedings ofthe 9th ACM-SIAM Symposium on DiscreteAlgorithms (SODA), pages 668?677, 1998.Extended version in Journal of the ACM,46:604?632, 1999.
[12] Wilfried Ku?rschner.
Studien zur Negation imDeutschen.
Narr, 1983.
[13] William A. Ladusaw.
Polarity Sensitivity asInherent Scope Relations.
Garland Press,New York, 1980.
Ph.D. thesis date 1979.
[14] Timm Lichte and Jan-Philipp Soehn.
The re-trieval and classification of Negative Polar-ity Items using statistical profiles.
In SamFeatherston and Wolfgang Sternefeld, edi-tors, Roots: Linguistics in Search of its Ev-idential Base, pages 249?266.
Mouton deGruyter, 2007.
[15] Bill MacCartney and Christopher D. Man-ning.
Modeling semantic containment andexclusion in natural language inference.
InProceedings of COLING, pages 521?528,2008.
[16] Rowan Nairn, Cleo Condoravdi, and LauriKarttunen.
Computing relative polarity fortextual inference.
In Proceedings of In-ference in Computational Semantics (ICoS),2006.
[17] Frank Richter, Janina Rado?, and ManfredSailer.
Negative polarity items: Corpuslinguistics, semantics, and psycholinguis-tics: Day 2: Corpus linguistics.
Tutorialslides: http://www.sfs.uni-tuebingen.de/?fr/esslli/08/byday/day2/day2-part1.pdf, 2008.
[18] V?
?ctor Sa?nchez Valencia.
Studies on naturallogic and categorial grammar.
PhD thesis,University of Amsterdam, 1991.
[19] Johan van Benthem.
Essays in Logical Se-mantics.
Reidel, Dordrecht, 1986.
[20] Ton van der Wouden.
Negative contexts:Collocation, polarity and multiple negation.Routledge, 1997.252
