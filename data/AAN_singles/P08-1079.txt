Proceedings of ACL-08: HLT, pages 692?700,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsUnsupervised Discovery of Generic Relationships Using Pattern Clustersand its Evaluation by Automatically Generated SAT Analogy QuestionsDmitry DavidovICNCHebrew University of Jerusalemdmitry@alice.nc.huji.ac.ilAri RappoportInstitute of Computer ScienceHebrew University of Jerusalemarir@cs.huji.ac.ilAbstractWe present a novel framework for the dis-covery and representation of general semanticrelationships that hold between lexical items.We propose that each such relationship can beidentified with a cluster of patterns that cap-tures this relationship.
We give a fully unsu-pervised algorithm for pattern cluster discov-ery, which searches, clusters and merges high-frequency words-based patterns around ran-domly selected hook words.
Pattern clusterscan be used to extract instances of the corre-sponding relationships.
To assess the qualityof discovered relationships, we use the patternclusters to automatically generate SAT anal-ogy questions.
We also compare to a set ofknown relationships, achieving very good re-sults in both methods.
The evaluation (donein both English and Russian) substantiates thepremise that our pattern clusters indeed reflectrelationships perceived by humans.1 IntroductionSemantic resources can be very useful in many NLPtasks.
Manual construction of such resources is la-bor intensive and susceptible to arbitrary human de-cisions.
In addition, manually constructed semanticdatabases are not easily portable across text domainsor languages.
Hence, there is a need for developingsemantic acquisition algorithms that are as unsuper-vised and language independent as possible.A fundamental type of semantic resource is thatof concepts (represented by sets of lexical items)and their inter-relationships.
While there is rel-atively good agreement as to what concepts areand which concepts should exist in a lexical re-source, identifying types of important lexical rela-tionships is a rather difficult task.
Most establishedresources (e.g., WordNet) represent only the mainand widely accepted relationships such as hyper-nymy and meronymy.
However, there are manyother useful relationships between concepts, such asnoun-modifier and inter-verb relationships.
Identi-fying and representing these explicitly can greatlyassist various tasks and applications.
There are al-ready applications that utilize such knowledge (e.g.,(Tatu and Moldovan, 2005) for textual entailment).One of the leading methods in semantics acqui-sition is based on patterns (see e.g., (Hearst, 1992;Pantel and Pennacchiotti, 2006)).
The standard pro-cess for pattern-based relation extraction is to startwith hand-selected patterns or word pairs express-ing a particular relationship, and iteratively scanthe corpus for co-appearances of word pairs in pat-terns and for patterns that contain known word pairs.This methodology is semi-supervised, requiring pre-specification of the desired relationship or hand-coding initial seed words or patterns.
The methodis quite successful, and examining its results in de-tail shows that concept relationships are often beingmanifested by several different patterns.In this paper, unlike the majority of studies thatuse patterns in order to find instances of given rela-tionships, we use sets of patterns as the definitionsof lexical relationships.
We introduce pattern clus-ters, a novel framework in which each cluster cor-responds to a relationship that can hold between thelexical items that fill its patterns?
slots.
We presenta fully unsupervised algorithm to compute pat-692tern clusters, not requiring any, even implicit, pre-specification of relationship types or word/patternseeds.
Our algorithm does not utilize preprocess-ing such as POS tagging and parsing.
Some patternsmay be present in several clusters, thus indirectly ad-dressing pattern ambiguity.The algorithm is comprised of the followingstages.
First, we randomly select hook words andcreate a context corpus (hook corpus) for each hookword.
Second, we define a meta-pattern using highfrequency words and punctuation.
Third, in eachhook corpus, we use the meta-pattern to discoverconcrete patterns and target words co-appearingwith the hook word.
Fourth, we cluster the patternsin each corpus according to co-appearance of the tar-get words.
Finally, we merge clusters from differenthook corpora to produce the final structure.
We alsopropose a way to label each cluster by word pairsthat represent it best.Since we are dealing with relationships that areunspecified in advance, assessing the quality of theresulting pattern clusters is non-trivial.
Our evalu-ation uses two methods: SAT tests, and compari-son to known relationships.
We used instances ofthe discovered relationships to automatically gener-ate analogy SAT tests in two languages, English andRussian1.
Human subjects answered these and realSAT tests.
English grades were 80% for our test and71% for the real test (83% and 79% for Russian),showing that our relationship definitions indeed re-flect human notions of relationship similarity.
In ad-dition, we show that among our pattern clusters thereare clusters that cover major known noun-compoundand verb-verb relationships.In the present paper we focus on the pattern clus-ter resource itself and how to evaluate its intrinsicquality.
In (Davidov and Rappoport, 2008) we showhow to use the resource for a known task of a to-tally different nature, classification of relationshipsbetween nominals (based on annotated data), obtain-ing superior results over previous work.Section 2 discusses related work, and Section 3presents the pattern clustering and labeling algo-rithm.
Section 4 describes the corpora we used andthe algorithm?s parameters in detail.
Sections 5 and1Turney and Littman (2005) automatically answers SATtests, while our focus is on generating them.6 present SAT and comparison evaluation results.2 Related WorkExtraction of relation information from text is alarge sub-field in NLP.
Major differences betweenpattern approaches include the relationship typessought (including domain restrictions), the degreesof supervision and required preprocessing, and eval-uation method.2.1 Relationship TypesThere is a large body of related work that deals withdiscovery of basic relationship types represented inuseful resources such as WordNet, including hyper-nymy (Hearst, 1992; Pantel et al, 2004; Snowet al, 2006), synonymy (Davidov and Rappoport,2006; Widdows and Dorow, 2002) and meronymy(Berland and Charniak, 1999; Girju et al, 2006).Since named entities are very important in NLP,many studies define and discover relations betweennamed entities (Hasegawa et al, 2004; Hassan etal., 2006).
Work was also done on relations be-tween verbs (Chklovski and Pantel, 2004).
Thereis growing research on relations between nominals(Moldovan et al, 2004; Girju et al, 2007).2.2 Degree of Supervision and PreprocessingWhile numerous studies attempt to discover one ormore pre-specified relationship types, very little pre-vious work has directly attempted the discovery ofwhich main types of generic relationships actuallyexist in an unrestricted domain.
Turney (2006) pro-vided a pattern distance measure that allows a fullyunsupervised measurement of relational similaritybetween two pairs of words; such a measure couldin principle be used by a clustering algorithm in or-der to deduce relationship types, but this was notdiscussed.
Unlike (Turney, 2006), we do not per-form any pattern ranking.
Instead we produce (pos-sibly overlapping) hard clusters, where each patterncluster represents a relationship discovered in thedomain.
Banko et al (2007) and Rosenfeld andFeldman (2007) find relationship instances wherethe relationships are not specified in advance.
Theyaim to find relationship instances rather than iden-tify generic semantic relationships.
Thus, their rep-resentation is very different from ours.
In addition,(Banko et al, 2007) utilize supervised tools such693as a POS tagger and a shallow parser.
Davidov etal.
(2007) proposed a method for unsupervised dis-covery of concept-specific relations.
That work, likeours, relies on pattern clusters.
However, it requiresinitial word seeds and targets the discovery of rela-tionships specific for some given concept, while weattempt to discover and define generic relationshipsthat exist in the entire domain.Studying relationships between tagged named en-tities, (Hasegawa et al, 2004; Hassan et al, 2006)proposed unsupervised clustering methods that as-sign given sets of pairs into several clusters, whereeach cluster corresponds to one of a known set of re-lationship types.
Their classification setting is thusvery different from our unsupervised discovery one.Several recent papers discovered relations on theweb using seed patterns (Pantel et al, 2004), rules(Etzioni et al, 2004), and word pairs (Pasca et al,2006; Alfonseca et al, 2006).
The latter used thenotion of hook which we also use in this paper.Several studies utilize some preprocessing, includ-ing parsing (Hasegawa et al, 2004; Hassan et al,2006) and usage of syntactic (Suchanek et al, 2006)and morphological (Pantel et al, 2004) informa-tion in patterns.
Several algorithms use manually-prepared resources, including WordNet (Moldovanet al, 2004; Costello et al, 2006) and Wikipedia(Strube and Ponzetto, 2006).
In this paper, wedo not utilize any language-specific preprocessingor any other resources, which makes our algorithmrelatively easily portable between languages, as wedemonstrate in our bilingual evaluation.2.3 Evaluation MethodEvaluation for hypernymy and synonymy usuallyuses WordNet (Lin and Pantel, 2002; Widdows andDorow, 2002; Davidov and Rappoport, 2006).
Formore specific lexical relationships like relationshipsbetween verbs (Chklovski and Pantel, 2004), nom-inals (Girju et al, 2004; Girju et al, 2007) ormeronymy subtypes (Berland and Charniak, 1999)there is still little agreement which important rela-tionships should be defined.
Thus, there are morethan a dozen different type hierarchies and tasks pro-posed for noun compounds (and nominals in gen-eral), including (Nastase and Szpakowicz, 2003;Girju et al, 2005; Girju et al, 2007).There are thus two possible ways for a fair eval-uation.
A study can develop its own relationshipdefinitions and dataset, like (Nastase and Szpakow-icz, 2003), thus introducing a possible bias; or itcan accept the definition and dataset prepared byanother work, like (Turney, 2006).
However, thismakes it impossible to work on new relationshiptypes.
Hence, when exploring very specific relation-ship types or very generic, but not widely accepted,types (like verb strength), many researchers resortto manual human-based evaluation (Chklovski andPantel, 2004).
In our case, where relationship typesare not specified in advance, creating an unbiasedbenchmark is very problematic, so we rely on hu-man subjects for relationship evaluation.3 Pattern Clustering AlgorithmOur algorithm first discovers and clusters patterns inwhich a single (?hook?)
word participates, and thenmerges the resulting clusters to form the final struc-ture.
In this section we detail the algorithm.
Thealgorithm utilizes several parameters, whose selec-tion is detailed in Section 4.
We refer to a patterncontained in our clusters (a pattern type) as a ?pat-tern?
and to an occurrence of a pattern in the corpus(a pattern token) as a ?pattern instance?.3.1 Hook Words and Hook CorporaAs a first step, we randomly select a set of hookwords.
Hook words were used in e.g.
(Alfonsecaet al, 2006) for extracting general relations startingfrom given seed word pairs.
Unlike most previouswork, our hook words are not provided in advancebut selected randomly; the goal in those papers isto discover relationships between given word pairs,while we use hook words in order to discover rela-tionships that generally occur in the corpus.Only patterns in which a hook word actually par-ticipates will eventually be discovered.
Hence, inprinciple we should select as many hook words aspossible.
However, words whose frequency is veryhigh are usually ambiguous and are likely to producepatterns that are too noisy, so we do not select wordswith frequency higher than a parameter FC .
In ad-dition, we do not select words whose frequency isbelow a threshold FB , to avoid selection of typosand other noise that frequently appear on the web.We also limit the total number N of hook words.694Our algorithm merges clusters originating from dif-ferent hook words.
Using too many hook words in-creases the chance that some of them belong to anoisy part in the corpus and thus lowers the qualityof our resulting clusters.For each hook word, we now create a hook cor-pus, the set of the contexts in which the word ap-pears.
Each context is a window containing Wwords or punctuation characters before and after thehook word.
We avoid extracting text from clearlyunformatted sentences and our contexts do not crossparagraph boundaries.The size of each hook corpus is much smaller thanthat of the whole corpus, easily fitting into mainmemory; the corpus of a hook word occurring htimes in the corpus contains at most 2hW words.Since most operations are done on each hook corpusseparately, computation is very efficient.Note that such context corpora can in principle beextracted by focused querying on the web, makingthe system dynamically scalable.
It is also possi-ble to restrict selection of hook words to a specificdomain or word type, if we want to discover onlya desired subset of existing relationships.
Thus wecould sample hook words from nouns, verbs, propernames, or names of chemical compounds if we areonly interested in discovering relationships betweenthese.
Selecting hook words randomly allows us toavoid using any language-specific data at this step.3.2 Pattern SpecificationIn order to reduce noise and to make the computa-tion more efficient, we did not consider all contextsof a hook word as pattern candidates, only contextsthat are instances of a specified meta-pattern type.Following (Davidov and Rappoport, 2006), we clas-sified words into high-frequency words (HFWs) andcontent words (CWs).
A word whose frequency ismore (less) than FH (FC) is considered to be a HFW(CW).
Unlike (Davidov and Rappoport, 2006), weconsider all punctuation characters as HFWs.
Ourpatterns have the general form[Prefix] CW1 [Infix] CW2 [Postfix]where Prefix, Infix and Postfix contain only HFWs.To reduce the chance of catching CWi?s that areparts of a multiword expression, we require Prefixand Postfix to have at least one word (HFW), whileInfix is allowed to contain any number of HFWs (butrecall that the total length of a pattern is limited bywindow size).
A pattern example is ?such X as Yand?.
During this stage we only allow single wordsto be in CW slots2.3.3 Discovery of Target WordsFor each of the hook corpora, we now extract allpattern instances where one CW slot contains thehook word and the other CW slot contains someother (?target?)
word.
To avoid the selection of com-mon words as target words, and to avoid targets ap-pearing in pattern instances that are relatively fixedmultiword expressions, we sort all target words ina given hook corpus by pointwise mutual informa-tion between hook and target, and drop patterns ob-tained from pattern instances containing the lowestand highest L percent of target words.3.4 Local Pattern ClusteringWe now have for each hook corpus a set of patterns.All of the corresponding pattern instances share thehook word, and some of them also share a targetword.
We cluster patterns in a two-stage process.First, we group in clusters all patterns whose in-stances share the same target word, and ignore therest.
For each target word we have a single patterncluster.
Second, we merge clusters that share morethan S percent of their patterns.
A pattern can ap-pear in more than a single cluster.
Note that clusterscontain pattern types, obtained through examiningpattern instances.3.5 Global Cluster MergingThe purpose of this stage is to create clusters of pat-terns that express generic relationships rather thanones specific to a single hook word.
In addition,the technique used in this stage reduces noise.
Foreach created cluster we will define core patterns andunconfirmed patterns, which are weighed differentlyduring cluster labeling (see Section 3.6).
We mergeclusters from different hook corpora using the fol-lowing algorithm:1.
Remove all patterns originating from a single hookcorpus.2While for pattern clusters creation we use only single wordsas CWs, later during evaluation we allow multiword expressionsin CW slots of previously acquired patterns.6952.
Mark all patterns of all present clusters as uncon-firmed.3.
While there exists some cluster C1 from corpus DXcontaining only unconfirmed patterns:(a) Select a cluster with a minimal number of pat-terns.
(b) For each corpus D different from DX :i. Scan D for clusters C2 that share at leastS percent of their patterns, and all of theircore patterns, with C1.ii.
Add all patterns of C2 to C1, setting allshared patterns as core and all others asunconfirmed.iii.
Remove cluster C2.
(c) If all of C1?s patterns remain unconfirmed re-move C1.4.
If several clusters have the same set of core patternsmerge them according to rules (i,ii).We start from the smallest clusters because we ex-pect these to be more precise; the best patterns forsemantic acquisition are those that belong to smallclusters, and appear in many different clusters.
Atthe end of this algorithm, we have a set of patternclusters where for each cluster there are two subsets,core patterns and unconfirmed patterns.3.6 Labeling of Pattern ClustersTo label pattern clusters we define a HITS measurethat reflects the affinity of a given word pair to agiven cluster.
For a given word pair (w1, w2) andcluster C with n core patterns Pcore and m uncon-firmed patterns Punconf ,Hits(C, (w1, w2)) =|{p; (w1, w2) appears in p ?
Pcore}| /n+??
|{p; (w1, w2) appears in p ?
Punconf}| /m.In this formula, ?appears in?
means that the wordpair appears in instances of this pattern extractedfrom the original corpus or retrieved from the webduring evaluation (see Section 5.2).
Thus if somepair appears in most of patterns of some cluster itreceives a high HITS value for this cluster.
The top5 pairs for each cluster are selected as its labels.?
?
(0..1) is a parameter that lets us modify therelative weight of core and unconfirmed patterns.4 Corpora and ParametersIn this section we describe our experimental setup,and discuss in detail the effect of each of the algo-rithms?
parameters.4.1 Languages and CorporaThe evaluation was done using corpora in Englishand Russian.
The English corpus (Gabrilovich andMarkovitch, 2005) was obtained through crawlingthe URLs in the Open Directory Project (dmoz.org).It contains about 8.2G words and its size is about68GB of untagged plain text.
The Russian corpuswas collected over the web, comprising a variety ofdomains, including news, web pages, forums, nov-els and scientific papers.
It contains 7.5G words ofsize 55GB untagged plain text.
Aside from remov-ing noise and sentence duplicates, we did not applyany text preprocessing or tagging.4.2 ParametersOur algorithm uses the following parameters: FC ,FH , FB , W , N , L, S and ?.
We used part of theRussian corpus as a development set for determin-ing the parameters.
On our development set we havetested various parameter settings.
A detailed analy-sis of the involved parameters is beyond the scopeof this paper; below we briefly discuss the observedqualitative effects of parameter selection.
Naturally,the parameters are not mutually independent.FC (upper bound for content word frequency inpatterns) influences which words are considered ashook and target words.
More ambiguous words gen-erally have higher frequency.
Since content wordsdetermine the joining of patterns into clusters, themore ambiguous a word is, the noisier the result-ing clusters.
Thus, higher values of FC allow moreambiguous words, increasing cluster recall but alsoincreasing cluster noise, while lower ones increasecluster precision at the expense of recall.FH (lower bound for HFW frequency in patterns)influences the specificity of patterns.
Higher val-ues restrict our patterns to be based upon the fewmost common HFWs (like ?the?, ?of?, ?and?)
andthus yield patterns that are very generic.
Loweringthe values, we obtain increasing amounts of patternclusters for more specific relationships.
The valuewe use for FH is lower than that used for FC , in or-der to allow as HFWs function words of relativelylow frequency (e.g., ?through?
), while allowing ascontent words some frequent words that participatein meaningful relationships (e.g., ?game?).
However,this way we may also introduce more noise.696FB (lower bound for hook words) filters hookwords that do not appear enough times in the cor-pus.
We have found that this parameter is essentialfor removing typos and other words that do not qual-ify as hook words.N (number of hook words) influences relation-ship coverage.
With higher N values we discovermore relationships roughly of the same specificitylevel, but computation becomes less efficient andmore noise is introduced.W (window size) determines the length of the dis-covered patterns.
Lower values are more efficientcomputationally, but values that are too low result indrastic decrease in coverage.
Higher values wouldbe more useful when we allow our algorithm to sup-port multiword expressions as hooks and targets.L (target word mutual information filter) helps inavoiding using as targets common words that areunrelated to hooks, while still catching as targetsfrequent words that are related.
Low L values de-crease pattern precision, allowing patterns like ?giveX please Y more?, where X is the hook (e.g., ?Alex?
)and Y the target (e.g., ?some?).
High values increasepattern precision at the expense of recall.S (minimal overlap for cluster merging) is a clus-ters merge filter.
Higher values cause more strictmerging, producing smaller but more precise clus-ters, while lower values start introducing noise.
Inextreme cases, low values can start a chain reactionof total merging.?
(core vs. unconfirmed weight for HITS labeling)allows lower quality patterns to complement higherquality ones during labeling.
Higher values increaselabel noise, while lower ones effectively ignore un-confirmed patterns during labeling.In our experiments we have used the followingvalues (again, determined using a development set)for these parameters: FC : 1, 000 words per mil-lion (wpm); FH : 100 wpm; FB: 1.2 wpm; N : 500words; W : 5 words; L: 30%; S: 2/3; ?
: 0.1.5 SAT-based EvaluationAs discussed in Section 2, the evaluation of semanticrelationship structures is non-trivial.
The goal of ourevaluation was to assess whether pattern clusters in-deed represent meaningful, precise and different re-lationships.
There are two complementary perspec-tives that a pattern clusters quality assessment needsto address.
The first is the quality (precision/recall)of individual pattern clusters: does each pattern clus-ter capture lexical item pairs of the same semanticrelationship?
does it recognize many pairs of thesame semantic relationship?
The second is the qual-ity of the cluster set as whole: does the pattern clus-ters set alow identification of important known se-mantic relationships?
do several pattern clusters de-scribe the same relationship?Manually examining the resulting pattern clus-ters, we saw that the majority of sampled clusters in-deed clearly express an interesting specific relation-ship.
Examples include familiar hypernymy clusterssuch as3 {?such X as Y?, ?X such as Y?, ?Y and otherX?,} with label (pets, dogs), and much more specificclusters like { ?buy Y accessory for X!
?, ?shipping Yfor X?, ?Y is available for X?, ?Y are available for X?,?Y are available for X systems?, ?Y for X?
}, labeledby (phone, charger).
Some clusters contain overlap-ping patterns, like ?Y for X?, but represent differentrelationships when examined as a whole.We addressed the evaluation questions above us-ing a SAT-like analogy test automatically generatedfrom word pairs captured by our clusters (see belowin this section).
In addition, we tested coverage andoverlap of pattern clusters with a set of 35 known re-lationships, and we compared our patterns to thosefound useful by other algorithms (the next section).Quantitatively, the final number of clusters is 508(470) for English (Russian), and the average clustersize is 5.5 (6.1) pattern types.
55% of the clustershad no overlap with other clusters.5.1 SAT Analogy Choice TestOur main evaluation method, which is also a use-ful application by itself, uses our pattern clusters toautomatically generate SAT analogy questions.
Thequestions were answered by human subjects.We randomly selected 15 clusters.
This allowedus to assess the precision of the whole cluster set aswell as of the internal coherence of separate clus-ters (see below).
For each cluster, we constructeda SAT analogy question in the following manner.The header of the question is a word pair that is oneof the label pairs of the cluster.
The five multiple3For readability, we omit punctuations in Prefix and Postfix.697choice items include: (1) another label of the clus-ter (the ?correct?
answer); (2) three labels of otherclusters among the 15; and (3) a pair constructed byrandomly selecting words from those making up thevarious cluster labels.In our sample there were no word pairs assignedas labels to more than one cluster4.
As a baseline forcomparison, we have mixed these questions with 15real SAT questions taken from English and RussianSAT analogy tests.
In addition, we have also askedour subjects to write down one example pair of thesame relationship for each question in the test.As an example, from one of the 15 clusters wehave randomly selected the label (glass, water).
Thecorrect answer selected from the same cluster was(schoolbag, book).
The three pairs randomly se-lected from the other 14 clusters were (war, death),(request, license) and (mouse, cat).
The pair ran-domly selected from a cluster not among the 15 clus-ters was (milk, drink).
Among the subjects?
propos-als for this question were (closet, clothes) and (wal-let, money).We computed accuracy of SAT answers, and thecorrelation between answers for our questions andthe real ones (Table 1).
Three things are demon-strated about our system when humans are capableof selecting the correct answer.
First, our clustersare internally coherent in the sense of expressing acertain relationship, because people identified thatthe pairs in the question header and in the correctanswer exhibit the same relationship.
Second, ourclusters distinguish between different relationships,because the three pairs not expressing the same rela-tionship as the header were not selected by the evalu-ators.
Third, our cluster labeling algorithm producesresults that are usable by people.The test was performed in both English and Rus-sian, with 10 (6) subjects for English (Russian).The subjects (biology and CS students) were not in-volved with the research, did not see the clusters,and did not receive any special training as prepara-tion.
Inter-subject agreement and Kappa were 0.82,0.72 (0.9, 0.78) for English (Russian).
As reportedin (Turney, 2005), an average high-school SATgrade is 57.
Table 1 shows the final English and Rus-4But note that a pair can certainly obtain a positive HITSvalue for several clusters.Our method Real SAT CorrelationEnglish 80% 71% 0.85Russian 83% 79% 0.88Table 1: Pattern cluster evaluation using automaticallygenerated SAT analogy choice questions.sian grade average for ours and real SAT questions.We can see that for both languages, around 80%of the choices were correct (the random choice base-line is 20%).
Our subjects are university students,so results higher than 57 are expected, as we cansee from real SAT performance.
The differencein grades between the two languages might be at-tributed to the presence of relatively hard and un-common words.
It also may result from the Russiantest being easier because there is less verb-noun am-biguity in Russian.We have observed a high correlation between truegrades and ours, suggesting that our automaticallygenerated test reflects the ability to recognize analo-gies and can be potentially used for automated gen-eration of SAT-like tests.The results show that our pattern clusters indeedmirror a human notion of relationship similarity andrepresent meaningful relationships.
They also showthat as intended, different clusters describe differentrelationships.5.2 Analogy Invention TestTo assess recall of separate pattern clusters, we haveasked subjects to provide (if possible) an additionalpair for each SAT question.
On each such pairwe have automatically extracted a set of pattern in-stances that capture this pair by using automatedweb queries.
Then we calculated the HITS value foreach of the selected pairs and assigned them to clus-ters with highest HITS value.
The numbers of pairsprovided were 81 for English and 43 for Russian.We have estimated precision for this task asmacro-average of percentage of correctly assignedpairs, obtaining 87% for English and 82% for Rus-sian (the random baseline of this 15-class classifi-cation task is 6.7%).
It should be noted howeverthat the human-provided additional relationship ex-amples in this test are not random so it may intro-duce bias.
Nevertheless, these results confirm thatour pattern clusters are able to recognize new in-69830 Noun Compound RelationshipsAvg.
num Overlapof clustersRussian 1.8 0.046English 1.7 0.0595 Verb Verb RelationshipsRussian 1.4 0.01English 1.2 0Table 2: Patterns clusters discovery of known relation-ships.stances of relationships of the same type.6 Evaluation Using Known InformationWe also evaluated our pattern clusters using relevantinformation reported in related work.6.1 Discovery of Known RelationshipsTo estimate recall of our pattern cluster set, weattempted to estimate whether (at least) a subsetof known relationships have corresponding patternclusters.
As a testing subset, we have used 35 re-lationships for both English and Russian.
30 rela-tions are noun compound relationships as proposedin the (Nastase and Szpakowicz, 2003) classifica-tion scheme, and 5 relations are verb-verb relationsproposed by (Chklovski and Pantel, 2004).
Wehave manually created sets of 5 unambiguous sam-ple pairs for each of these 35 relationships.
For eachsuch pair we have assigned the pattern cluster withbest HITS value.The middle column of Table 2 shows the averagenumber of clusters per relationship.
Ideally, if foreach relationship all 5 pairs are assigned to the samecluster, the average would be 1.
In the worst case,when each pair is assigned to a different cluster, theaverage would be 5.
We can see that most of thepairs indeed fall into one or two clusters, success-fully recognizing that similarly related pairs belongto the same cluster.
The column on the right showsthe overlap between different clusters, measured asthe average number of shared pairs in two randomlyselected clusters.
The baseline in this case is essen-tially 5, since there are more than 400 clusters for 5word pairs.
We see a very low overlap between as-signed clusters, which shows that these clusters in-deed separate well between defined relations.6.2 Discovery of Known Pattern SetsWe compared our clusters to lists of patterns re-ported as useful by previous papers.
These listsincluded patterns expressing hypernymy (Hearst,1992; Pantel et al, 2004), meronymy (Berland andCharniak, 1999; Girju et al, 2006), synonymy(Widdows and Dorow, 2002; Davidov and Rap-poport, 2006), and verb strength + verb happens-before (Chklovski and Pantel, 2004).
In all cases,we discovered clusters containing all of the reportedpatterns (including their refinements with domain-specific prefix or postfix) and not containing patternsof competing relationships.7 ConclusionWe have proposed a novel way to define and identifygeneric lexical relationships as clusters of patterns.Each such cluster is set of patterns that can be usedto identify, classify or capture new instances of someunspecified semantic relationship.
We showed howsuch pattern clusters can be obtained automaticallyfrom text corpora without any seeds and without re-lying on manually created databases or language-specific text preprocessing.
In an evaluation basedon an automatically created analogy SAT test weshowed on two languages that pairs produced by ourclusters indeed strongly reflect human notions of re-lation similarity.
We also showed that the obtainedpattern clusters can be used to recognize new ex-amples of the same relationships.
In an additionaltest where we assign labeled pairs to pattern clus-ters, we showed that they provide good coverage forknown noun-noun and verb-verb relationships forboth tested languages.While our algorithm shows good performance,there is still room for improvement.
It utilizes a setof constants that affect precision, recall and the gran-ularity of the extracted cluster set.
It would be ben-eficial to obtain such parameters automatically andto create a multilevel relationship hierarchy insteadof a flat one, thus combining different granularitylevels.
In this study we applied our algorithm to ageneric domain, while the same method can be usedfor more restricted domains, potentially discoveringuseful domain-specific relationships.699ReferencesAlfonseca, E., Ruiz-Casado, M., Okumura, M., Castells,P., 2006.
Towards large-scale non-taxonomic relationextraction: estimating the precision of rote extractors.COLING-ACL ?06 Ontology Learning & PopulationWorkshop.Banko, M., Cafarella, M. J. , Soderland, S., Broadhead,M., and Etzioni, O., 2007.
Open information extrac-tion from the Web.
IJCAI ?07.Berland, M., Charniak, E., 1999.
Finding parts in verylarge corpora.
ACL ?99.Chklovski, T., Pantel, P., 2004.
VerbOcean: mining theweb for fine-grained semantic verb relations.
EMNLP?04.Costello, F., Veale, T. Dunne, S., 2006.
Using Word-Net to automatically deduce relations between wordsin noun-noun compounds.
COLING-ACL ?06.Davidov, D., Rappoport, A., 2006.
Efficient unsuper-vised discovery of word categories using symmetricpatterns and high frequency words.
COLING-ACL?06.Davidov, D., Rappoport, A. and Koppel, M., 2007.
Fullyunsupervised discovery of concept-specific relation-ships by Web mining.
ACL ?07.Davidov, D., Rappoport, A., 2008.
Classification of re-lationships between nominals using pattern clusters.ACL ?08.Etzioni, O., Cafarella, M., Downey, D., Popescu, A.,Shaked, T., Soderland, S., Weld, D., and Yates, A.,2004.
Methods for domain-independent informationextraction from the web: An experimental compari-son.
AAAI 04Gabrilovich, E., Markovitch, S., 2005.
Feature gener-ation for text categorization using world knowledge.IJCAI 2005.Girju, R., Giuglea, A., Olteanu, M., Fortu, O., Bolohan,O., and Moldovan, D., 2004.
Support vector machinesapplied to the classification of semantic relations innominalized noun phrases.
HLT/NAACL Workshop onComputational Lexical Semantics.Girju, R., Moldovan, D., Tatu, M., and Antohe, D., 2005.On the semantics of noun compounds.
ComputerSpeech and Language, 19(4):479-496.Girju, R., Badulescu, A., and Moldovan, D., 2006.
Au-tomatic discovery of part-whole relations.
Computa-tional Linguistics, 32(1).Girju, R., Hearst, M., Nakov, P., Nastase, V., Szpakow-icz, S., Turney, P., and Yuret, D., 2007.
Task 04:Classification of semantic relations between nominalat SemEval 2007.
ACL ?07 SemEval Workshop.Hasegawa, T., Sekine, S., and Grishman, R., 2004.
Dis-covering relations among named entities from largecorpora.
ACL ?04.Hassan, H., Hassan, A. and Emam, O., 2006.
Unsu-pervised information extraction approach using graphmutual reinforcement.
EMNLP ?06.Hearst, M., 1992.
Automatic acquisition of hyponymsfrom large text corpora.
COLING ?92Lin, D., Pantel, P., 2002.
Concept discovery from text.COLING 02.Moldovan, D., Badulescu, A., Tatu, M., Antohe, D.,Girju,R., 2004.
Models for the semantic classification ofnoun phrases.
HLT-NAACL ?04 Workshop on Compu-tational Lexical Semantics.Nastase, V., Szpakowicz, S., 2003.
Exploring noun mod-ifier semantic relations.
IWCS-5.Pantel, P., Pennacchiotti, M., 2006.
Espresso: leveraginggeneric patterns for automatically harvesting semanticrelations.
COLING-ACL 2006.Pantel, P., Ravichandran, D. and Hovy, E.H., 2004.
To-wards terascale knowledge acquisition.
COLING ?04.Pasca, M., Lin, D., Bigham, J., Lifchits A., Jain, A.,2006.
Names and similarities on the web: fact extrac-tion in the fast lane.
COLING-ACL ?06.Rosenfeld, B., Feldman, R., 2007.
Clustering for unsu-pervised relation identification.
CIKM ?07.Snow, R., Jurafsky, D., Ng, A.Y., 2006.
Seman-tic taxonomy induction from heterogeneous evidence.COLING-ACL ?06.Strube, M., Ponzetto, S., 2006.
WikiRelate!
computingsemantic relatedness using Wikipedia.
AAAI ?06.Suchanek, F., Ifrim, G., and Weikum, G., 2006.
LEILA:learning to extract information by linguistic analysis.COLING-ACL ?06 Ontology Learning & PopulationWorkshop.Tatu, M., Moldovan, D., 2005.
A semantic approach torecognizing textual entailment.
HLT/EMNLP 2005.Turney, P., 2005.
Measuring semantic similarity by la-tent relational analysis.
IJCAI ?05.Turney, P., Littman, M., 2005.
Corpus-based learn-ing of analogies and semantic selations.
MachineLearning(60):1?3:251?278.Turney, P., 2006.
Expressing implicit semantic relationswithout supervision.
COLING-ACL ?06.Widdows, D., Dorow, B., 2002.
A graph model for un-supervised lexical acquisition.
COLING ?02.700
