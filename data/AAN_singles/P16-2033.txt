Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 201?206,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsThe Value of Semantic Parse Labeling forKnowledge Base Question AnsweringWen-tau Yih Matthew Richardson Christopher Meek Ming-Wei Chang Jina SuhMicrosoft ResearchRedmond, WA 98052, USA{scottyih,mattri,meek,minchang,jinsuh}@microsoft.comAbstractWe demonstrate the value of collecting se-mantic parse labels for knowledge basequestion answering.
In particular, (1)unlike previous studies on small-scaledatasets, we show that learning from la-beled semantic parses significantly im-proves overall performance, resulting inabsolute 5 point gain compared to learn-ing from answers, (2) we show that with anappropriate user interface, one can obtainsemantic parses with high accuracy and ata cost comparable or lower than obtainingjust answers, and (3) we have created andshared the largest semantic-parse labeleddataset to date in order to advance researchin question answering.1 IntroductionSemantic parsing is the mapping of text to a mean-ing representation.
Early work on learning to buildsemantic parsers made use of datasets of questionsand their associated semantic parses (Zelle andMooney, 1996; Zettlemoyer and Collins, 2005;Wong and Mooney, 2007).
Recent work onsemantic parsing for knowledge base question-answering (KBQA) has called into question thevalue of collecting such semantic parse labels,with most recent KBQA semantic parsing systemsbeing trained using only question-answer pairs in-stead of question-parse pairs.
In fact, there is ev-idence that using only question-answer pairs canyield improved performance as compared with ap-proaches based on semantic parse labels (Liang etal., 2013).
It is also widely believed that collect-ing semantic parse labels can be a ?difficult, timeconsuming task?
(Clarke et al, 2010) even for do-main experts.
Furthermore, recent focus has beenmore on the final task-specific performance of asystem (i.e., did it get the right answer for a ques-tion) as opposed to agreement on intermediate rep-resentations (Berant et al, 2013; Kwiatkowski etal., 2013), which allows for KBQA datasets to bebuilt with only the answers to each question.In this work, we re-examine the value of se-mantic parse labeling and demonstrate that seman-tic parse labels can provide substantial value forknowledge base question-answering.
We focus onthe task of question-answering on Freebase, usingthe WEBQUESTIONS dataset (Berant et al, 2013).Our first contribution is the construction of thelargest semantic parse dataset for KB question-answering to date.
In order to evaluate the costsand benefits of gathering semantic parse labels, wecreated the WEBQUESTIONSSP dataset1, whichcontains semantic parses for the questions fromWEBQUESTIONS that are answerable using Free-base.
In particular, we provide SPARQL queriesfor 4,737 questions.
The remaining 18.5% of theoriginal WEBQUESTIONS questions are labeled as?not answerable?.
This is due to a number offactors including the use of a more stringent as-sessment of ?answerable?, namely that the ques-tion be answerable via SPARQL rather than byreturning or extracting information from textualdescriptions.
Compared to the previous seman-tic parse dataset on Freebase, Free917 (Cai andYates, 2013), our WEBQUESTIONSSP is not onlysubstantially larger, but also provides the semanticparses in SPARQL with standard Freebase entityidentifiers, which are directly executable on Free-base.Our second contribution is a demonstration thatsemantic parses can be collected at low cost.
Weemploy a staged labeling paradigm that enables ef-ficient labeling of semantic parses and improvesthe accuracy, consistency and efficiency of ob-1Available at http://aka.ms/WebQSP.201taining answers.
In fact, in a simple comparisonwith using a web browser to extract answers fromfreebase.com, we show that we can collect se-mantic parse labels at a comparable or even fasterrate than simply collecting answers.Our third contribution is an empirical demon-stration that we can leverage the semantic parselabels to increase the accuracy of a state-of-the-artquestion-answering system.
We use a system thatcurrently achieves state-of-the-art performance onKBQA and show that augmenting its training withsemantic parse labels leads to an absolute 5-pointincrease in average F1.Our work demonstrates that semantic parse la-bels can provide additional value over answer la-bels while, with the right labeling tools, beingcomparable in cost to collect.
Besides accuracygains, semantic parses also have further benefits inyielding answers that are more accurate and con-sistent, as well as being updatable if the knowl-edge base changes (for example, as facts are addedor revised).2 Collecting Semantic ParsesIn order to verify the benefits of having labeledsemantic parses, we completely re-annotated theWEBQUESTIONS dataset (Berant et al, 2013)such that it contains both semantic parses and thederived answers.
We chose to annotate the ques-tions with the full semantic parses in SPARQL,based on the schema and data of the latest and lastversion of Freebase (2015-08-09).Labeling interface Writing SPARQL queriesfor natural language questions using a text editor isobviously not an efficient way to provide semanticparses even for experts.
Therefore, we designed astaged, dialog-like user interface (UI) to improvethe labeling efficiency.
Our UI breaks the po-tentially complicated structured-labeling task intoseparate, but inter-dependent sub-tasks.
Given aquestion, the UI first presents entities detected inthe questions using an entity linking system (Yangand Chang, 2015), and asks the user to pick an en-tity in the question as the topic entity that couldlead to the answers.
The user can also suggest anew entity if none of the candidates returned bythe entity linking system is correct.
Once the en-tity is selected, the system then requests the userto pick the Freebase predicate that represents therelationship between the answers and this topicentity.
Finally, additional filters can be added tofurther constrain the answers.
One key advantageof our UI design is that the annotator only needs tofocus on one particular sub-task during each stage.All of the choices made by the labeler are used toautomatically construct a coherent semantic parse.Note that the user can easily go back and forth toeach of these three stages and change the previouschoices, before pressing the final submit button.Take the question ?who voiced meg on fam-ily guy??
for example.
The labeler will be pre-sented with two entity choices: Meg Griffinand Family Guy, where the former links ?meg?to the character?s entity and the latter links to theTV show.
Depending on the entity selected, legiti-mate Freebase predicates of the selected entity willbe shown, along with the objects (either proper-ties or entities).
Suppose the labeler chooses MegGriffin as the topic entity.
He should then pickactor as the main relationship, meaning the an-swer should be the persons who have played thisrole.
To accurately describe the question, the la-beler should add additional filters like the TV se-ries is Family Guy and the performance type isvoice in the final stage2.The design of our UI is inspired by recent workon semantic parsing that has been applied to theWEBQUESTIONS dataset (Bast and Haussmann,2015; Reddy et al, 2014; Berant and Liang, 2014;Yih et al, 2015), as these approaches use a sim-pler and yet more restricted semantic representa-tion than first-order logic expressions.
Followingthe notion of query graph in (Yih et al, 2015),the semantic parse is anchored to one of the enti-ties in the question as the topic entity and the corecomponent is to represent the relation between theentity and the answer, referred as the inferentialchain.
Constraints, such as properties of the an-swer or additional conditions the relation needsto hold, are captured as well.
Figure 1 shows anexample of these annotated semantic parse com-ponents and the corresponding SPARQL query.While it is clear that our UI does not cover compli-cated, highly compositional questions, most ques-tions in WEBQUESTIONS can be covered3.Labeling process In order to ensure the dataquality, we recruit five annotators who are famil-iar with design of Freebase.
Our goal is to provide2Screenshots are included in the supplementary material.3We manually edited the SPARQL queries for about 3.1%of the questions in WEBQUESTIONS that are not expressibleby our UI.202Family Guyin-tv-program actorMeg Griffin xy0who voiced meg on family guy?Topic Entity:  Meg Griffin (m.035szd)Inf.
Chain:  in-tv-program ?
actorConstraints: (1) y0 ?
series ?
Family Guy (m.019nnl)(2) y0 ?
performance-type ?
Voice (m.02nsjvf)VoicePREFIX ns: <http://rdf.freebase.com/ns/>SELECT ?xWHERE {ns:m.035szd ns:tv.tv_character.appeared_in_tv_program ?y0 .
?y0 ns:tv.regular_tv_appearance.actor ?x ;ns:tv.regular_tv_appearance.series ns:m.019nnl ;ns:tv.regular_tv_appearance.special_performance_typens:m.02nsjvf .
}(a)(b)(c)(d)Figure 1: Example semantic parse of the ques-tion (a) ?who voiced meg on family guy??
Thethree components in (b) record the labels collectedthrough our dialog-like user interface, and can bemapped deterministically to either the correspond-ing query graph (c) or the SPARQL query (d).correct semantic parses for each of the legitimateand unambiguous questions in WEBQUESTIONS.Our labeling instructions (included in the supple-mentary material) follow several key principles.For instance, the annotators should focus on giv-ing the correct semantic parse of a question, basedon the assumption that it will result in correct an-swers if the KB is complete and correct.Among all the 5,810 questions in WEB-QUESTIONS, there are 1,073 questions that the an-notators cannot provide the complete parses to findthe answers, due to issues with the questions orFreebase.
For example, some questions are am-biguous and without clear intent (e.g., ?where didromans go??).
Others are questions that Freebaseis not the appropriate information source (e.g.,?where to watch tv online for free in canada??
).3 Using Semantic ParsesIn order to compare two training paradigms, learn-ing from question-answer pairs and learning fromsemantic parses, we adopt the Staged QueryGraph Generation (STAGG) algorithm (Yih et al,2015), which achieves the highest published an-swer prediction accuracy on the WEBQUESTIONSdataset.
STAGG formulates the output semanticparse in a query graph representation that mimicsthe design of a graph knowledge base.
It searchesover potential query graphs for a question, iter-atively growing the query graph by sequentiallyadding a main topic entity, then adding an in-ferential chain and finally adding a set of con-straints.
During the search process, each candi-date query graph is judged by a scoring functionon how likely the graph is a correct parse, basedon features indicating how each individual com-ponent matches the original question, as well assome properties of the whole query graph.
Exam-ple features include the score output by the entitylinking system, the match score of the inferentialchain to the relation described in the question froma deep neural network model, number of nodesin the candidate query graph, and the number ofmatching words in constraints.
For additional de-tails see (Yih et al, 2015).When question-answer pairs are available, wecreate a set of query graphs connecting entities inthe question to the answers in the training set, asin (Yih et al, 2015).
We score the quality of aquery graph by using the F1score between the an-swer derived from the query graph and the answerin the training set.
These scores are then used in alearning-to-rank approach to predict high-qualityquery graphs.In the case that semantic parses are available,we change the score that we use for evaluatingthe quality of a query graph.
In particular, weassign the query graph score to be zero when-ever the query graph is not a subgraph consis-tent with the semantic parse label and to be theF1score described above otherwise.
The hope isthat by leveraging the semantic parse, we can sig-nificantly reduce the number of incorrect querygraphs used during training.
For instance, thepredicate music.artist.track was incor-rectly predicted as the inferential chain for thequestion ?what are the songs that justin bieberwrite?
?, where a correct parse should use the re-lation music.composer.compositions.4 The Value of Semantic ParsesIn this section, we explore the costs of collect-ing semantic parse labels and the benefits of usingthem.4.1 Benefits of Semantic ParsesLeveraging the new dataset, we study whether asemantic parser learned using full parses instead203Training Signals Prec.
Rec.
Avg.
F1Acc.Answers 67.3 73.1 66.8 58.8Sem.
Parses 70.9 80.3 71.7 63.9Table 1: The results of two different model train-ing settings: answers only vs. semantic parses.of just question-answer pairs can answer questionsmore accurately, using the knowledge base.
Be-low, we describe our basic experimental settingand report the main results.Experimental setting We followed the sametraining/testing splits as in the original WEB-QUESTIONS dataset, but only used questions withcomplete parses and answers for training and eval-uation in our experiments.
In the end, 3,098 ques-tions are used for model training and 1,639 ques-tions are used for evaluation4.
Because there canbe multiple answers to a question, precision, re-call and F1are computed for each individual ques-tion.
The average F1score is reported as the mainevaluation metric.
In addition, we also report thetrue accuracy ?
a question is considered answeredcorrectly only when the predicted answers exactlymatch one of the answer sets.Results Table 1 shows the results of two differ-ent models: learning from question-answer pairsvs.
learning from semantic parses.
With the la-beled parses, the average F1score is 4.9-pointhigher (71.7% vs. 66.8%).
The stricter metric,complete answer set accuracy, also reflects thesame trend, where the accuracy of training withlabeled parses is 5.1% higher than using only theanswers (63.9% vs. 58.8%).While it is expected that training using the anno-tated parses could result in a better model, it is stillinteresting to see the performance gap, especiallywhen the evaluation is on the correctness of the an-swers rather than the parses.
We examined the out-put answers to the questions where the two modelsdiffer.
Although the setting of using answers onlyoften guesses the correct relations connecting thetopic entity and answers, it can be confused by re-lated, but incorrect relations as well.
Similar phe-nomena also occur on constraints, which suggeststhat subtle differences in the meaning are difficult4The average F1score of the original STAGG?s outputto these 1,639 questions is 60.3%, evaluated using WEB-QUESTIONS.
Note that the number is not directly comparableto what we report in Table 1 because many of the labeled an-swers in WEBQUESTIONS are either incorrect or incomplete.Labeling Methods Ans.
Ans.
Sem.
ParsesAnnotator MTurkers Experts ExpertsAvg.
time/Question Unknown 82 sec 21 secLabeling Correctness 66% 92% 94%Table 2: Comparing labeling methods on 50 sampled ques-tions.to catch if the semantic parses are automaticallygenerated using only the answers.4.2 Costs of Semantic ParsesOur labeling process is very different from thatof the original WEBQUESTIONS dataset, wherethe question is paired with answers found on theFreebase Website by Amazon MTurk workers.
Tocompare these two annotation methods, we sam-pled 50 questions and had one expert label themusing two schemes: finding answers using theFreebase Website and labeling the semantic parsesusing our UI.
The time needed, as well as the cor-rectness of the answers are summarized in Table 2.Interestingly, in this study we found that it ac-tually took less time to label these questions withsemantic parses using our UI, than to label withonly answers.
There could be several possible ex-planations.
First, as many questions in this datasetare actually ?simple?
and do not need complicatedcompositional structured semantic parses, our UIcan help make the labeling process very efficient.By ranking the possible linked entities and likelyrelations, the annotators are able to pick the cor-rect component labels fairly easily.
In contrast,simple questions may have many legitimate an-swers.
Enumerating all of the correct answers cantake significantly longer than authoring a semanticparse that computes them.When we compare the annotation quality be-tween labeling semantic parses and answers, wefind that the correctness5of the answers are aboutthe same (92% vs 94%).
In the original WEB-QUESTIONS dataset, only 66% of the answers arecompletely correct.
This is largely due to thelow accuracy (42.9%) of the 14 questions contain-ing multiple answers.
This indicates that to en-sure data quality, more verification is needed whenleveraging crowdsourcing.5 DiscussionUnlike the work of (Liang et al, 2013; Clarke etal., 2010), we demonstrate that semantic parses5We considered a label to be correct only if the de-rived/labeled answer set is completely accurate.204can improve over state-of-the-art knowledge basequestion answering systems.
There are a numberof potential differences that are likely to contributeto this finding.
Unlike previous work, we comparetraining with answers and training with semanticparses while making only minimal changes in astate-of-the-art training algorithm.
This enablesa more direct evaluation of the potential benefitsof using semantic parses.
Second, and perhapsthe more significant difference, is that our evalu-ation is based on Freebase which is significantlylarger than the knowledge bases used in the pre-vious work.
We suspect that the gains providedby semantic parse labels are due a significant re-duction in the number of paths between candidateentities and answers when we limit to semanticallyvalid paths.
However, in domains where the num-ber of potential paths between candidate entitiesand answers is small, the value of collecting se-mantic parse labels might also be small.Semantic parsing labels provide additional ben-efits.
For example, collecting semantic parse la-bels relative to a knowledge base can ensure thatthe answers are more faithful to the knowledgebase and better captures which questions are an-swerable by the knowledge base.
Moreover, bycreating semantic parses using a labeling systembased on the target knowledge base, the correct-ness and completeness of answers can be im-proved.
This is especially true for question thathave large answer sets.
Finally, semantic labelsare more robust to changes in knowledge basefacts because answers can be computed via exe-cution of the semantic representation for the ques-tion.
For instance, the answer to ?Who does ChrisHemsworth have a baby with??
might change ifthe knowledge base is updated with new factsabout children but the semantic parse would notneed to change.Notice that besides being used for the fullsemantic parsing task, our WEBQUESTIONSSPdataset is a good test bed for several important se-mantic tasks as well.
For instance, the topic en-tity annotations are beneficial to training and test-ing entity linking systems.
The core inferentialchains alone are quality annotations for relationextraction and matching.
Specific types of con-straints are useful too.
For example, the temporalsemantic labels are valuable for identifying tem-poral expressions and their time spans.
Becauseour dataset specifically focuses on questions, itcomplements existing datasets in these individualtasks, as they tend to target at normal corpora ofregular sentences.While our labeling interface design was aimedat supporting labeling experts, it would be valu-able to enable crowdsourcing workers to providesemantic parse labels.
One promising approach isto use a more dialog-driven interface using natu-ral language (similar to (He et al, 2015)).
SuchUI design is also crucial for extending our workto handling more complicated questions.
For in-stance, allowing users to traverse longer paths in asequential manner will increase the expressivenessof the output parses, both in the core relation andconstraints.
Displaying a small knowledge graphcentered at the selected entities and relations mayhelp users explore alternative relations more effec-tively as well.AcknowledgmentsWe thank Andrei Aron for the initial design of thelabeling interface.ReferencesHannah Bast and Elmar Haussmann.
2015.
More ac-curate question answering on Freebase.
In Proceed-ings of the 24th ACM International on Conferenceon Information and Knowledge Management, pages1431?1440.
ACM.Jonathan Berant and Percy Liang.
2014.
Seman-tic parsing via paraphrasing.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1415?1425, Baltimore, Maryland, June.
Associationfor Computational Linguistics.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, pages 1533?1544, Seattle, Wash-ington, USA, October.
Association for Computa-tional Linguistics.Qingqing Cai and Alexander Yates.
2013.
Large-scale semantic parsing via schema matching and lex-icon extension.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 423?433,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.James Clarke, Dan Goldwasser, Ming-Wei Chang, andDan Roth.
2010.
Driving semantic parsing from205the world?s response.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning, pages 18?27.
Association for Com-putational Linguistics.Luheng He, Mike Lewis, and Luke Zettlemoyer.
2015.Question-answer driven semantic role labeling: Us-ing natural language to annotate natural language.In Proceedings of the 2015 Conference on Empiri-cal Methods in Natural Language Processing, pages643?653, Lisbon, Portugal, September.
Associationfor Computational Linguistics.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling semantic parsers withon-the-fly ontology matching.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1545?1556, Seattle,Washington, USA, October.
Association for Compu-tational Linguistics.Percy Liang, Michael I Jordan, and Dan Klein.
2013.Learning dependency-based compositional seman-tics.
Computational Linguistics, 39(2):389?446.Siva Reddy, Mirella Lapata, and Mark Steedman.2014.
Large-scale semantic parsing withoutquestion-answer pairs.
Transactions of the Associ-ation for Computational Linguistics, 2:377?392.Yuk Wah Wong and Raymond J Mooney.
2007.Learning synchronous grammars for semantic pars-ing with lambda calculus.
In Annual Meeting of theAssociation for Computational Linguistics (ACL).Yi Yang and Ming-Wei Chang.
2015.
S-MART: Noveltree-based structured learning algorithms applied totweet entity linking.
In Proceedings of the 53rd An-nual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Confer-ence on Natural Language Processing (Volume 1:Long Papers), pages 504?513, Beijing, China, July.Association for Computational Linguistics.Wen-tau Yih, Ming-Wei Chang, Xiaodong He, andJianfeng Gao.
2015.
Semantic parsing via stagedquery graph generation: Question answering withknowledge base.
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Lin-guistics and the 7th International Joint Conferenceon Natural Language Processing (Volume 1: LongPapers), pages 1321?1331, Beijing, China, July.
As-sociation for Computational Linguistics.John Zelle and Raymond Mooney.
1996.
Learningto parse database queries using inductive logic pro-gramming.
In Proceedings of the National Confer-ence on Artificial Intelligence, pages 1050?1055.Luke S Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In Conference on Uncertainty in Arti-ficial Intelligence (UAI).206
