Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222?1231,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPBilingually-Constrained (Monolingual) Shift-Reduce ParsingLiang HuangGoogle Research1350 Charleston Rd.Mountain View, CA 94043, USAlianghuang@google.comliang.huang.sh@gmail.comWenbin Jiang and Qun LiuKey Lab.
of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100190, Chinajiangwenbin@ict.ac.cnAbstractJointly parsing two languages has beenshown to improve accuracies on either orboth sides.
However, its search space ismuch bigger than the monolingual case,forcing existing approaches to employcomplicated modeling and crude approxi-mations.
Here we propose a much simpleralternative, bilingually-constrained mono-lingual parsing, where a source-languageparser learns to exploit reorderings as ad-ditional observation, but not bothering tobuild the target-side tree as well.
We showspecifically how to enhance a shift-reducedependency parser with alignment fea-tures to resolve shift-reduce conflicts.
Ex-periments on the bilingual portion of Chi-nese Treebank show that, with just 3 bilin-gual features, we can improve parsing ac-curacies by 0.6% (absolute) for both En-glish and Chinese over a state-of-the-artbaseline, with negligible (?6%) efficiencyoverhead, thus much faster than biparsing.1 IntroductionAmbiguity resolution is a central task in Natu-ral Language Processing.
Interestingly, not all lan-guages are ambiguous in the same way.
For exam-ple, prepositional phrase (PP) attachment is (no-toriously) ambiguous in English (and related Eu-ropean languages), but is strictly unambiguous inChinese and largely unambiguous Japanese; see(1a) I [ saw Bill ] [ with a telescope ].wo [ yong wangyuanjin] [kandao le Bi?er].
?I used a telescope to see Bill.?
(1b) I saw [ Bill [ with a telescope ] ].wo kandao le [ [ na wangyuanjin ] de Bi?er].
?I saw Bill who had a telescope at hand.
?Figure 1: PP-attachment is unambiguous in Chi-nese, which can help English parsing.Figure 1 for an example.1 It is thus intuitive to usetwo languages for better disambiguation, whichhas been applied not only to this PP-attachmentproblem (Fossum and Knight, 2008; Schwartz etal., 2003), but also to the more fundamental prob-lem of syntactic parsing which subsumes the for-mer as a subproblem.
For example, Smith andSmith (2004) and Burkett and Klein (2008) showthat joint parsing (or reranking) on a bitext im-proves accuracies on either or both sides by lever-aging bilingual constraints, which is very promis-ing for syntax-based machine translation which re-quires (good-quality) parse trees for rule extrac-tion (Galley et al, 2004; Mi and Huang, 2008).However, the search space of joint parsing is in-evitably much bigger than the monolingual case,1Chinese uses word-order to disambiguate the attachment(see below).
By contrast, Japanese resorts to case-markersand the unambiguity is limited: it works for the ?V or N?attachment ambiguities like in Figure 1 (see (Schwartz et al,2003)) but not for the ?N1or N2?
case (Mitch Marcus, p.c.
).1222forcing existing approaches to employ compli-cated modeling and crude approximations.
Jointparsing with a simplest synchronous context-freegrammar (Wu, 1997) is O(n6) as opposed to themonolingual O(n3) time.
To make things worse,languages are non-isomorphic, i.e., there is no 1-to-1 mapping between tree nodes, thus in practiceone has to use more expressive formalisms suchas synchronous tree-substitution grammars (Eis-ner, 2003; Galley et al, 2004).
In fact, rather thanjoint parsing per se, Burkett and Klein (2008) re-sort to separate monolingual parsing and bilingualreranking over k2 tree pairs, which covers a tinyfraction of the whole space (Huang, 2008).We instead propose a much simpler alterna-tive, bilingually-constrained monolingual parsing,where a source-language parser is extended to ex-ploit the reorderings between languages as addi-tional observation, but not bothering to build a treefor the target side simultaneously.
To illustrate theidea, suppose we are parsing the sentence(1) I saw Bill [PP with a telescope ].which has 2 parses based on the attachment of PP:(1a) I [ saw Bill ] [PP with a telescope ].
(1b) I saw [ Bill [PP with a telescope ]].Both are possible, but with a Chinese translationthe choice becomes clear (see Figure 1), becausea Chinese PP always immediately precedes thephrase it is modifying, thus making PP-attachmentstrictly unambiguous.2 We can thus use Chinese tohelp parse English, i.e., whenever we have a PP-attachment ambiguity, we will consult the Chinesetranslation (from a bitext), and based on the align-ment information, decide where to attach the En-glish PP.
On the other hand, English can help Chi-nese parsing as well, for example in deciding thescope of relative clauses which is unambiguous inEnglish but ambiguous in Chinese.This method is much simpler than joint pars-ing because it remains monolingual in the back-bone, with alignment information merely as softevidence, rather than hard constraints since auto-matic word alignment is far from perfect.
It is thus2to be precise, in Fig.
1(b), the English PP is translatedinto a Chinese relative clause, but nevertheless all phrasalmodifiers attach to the immediate right in Mandarin Chinese.straightforward to implement within a monolin-gual parsing algorithm.
In this work we chooseshift-reduce dependency parsing for its simplicityand efficiency.
Specifically, we make the followingcontributions:?
we develop a baseline shift-reduce depen-dency parser using the less popular, but clas-sical, ?arc-standard?
style (Section 2), andachieve similar state-of-the-art performancewith the the dominant but complicated ?arc-eager?
style of Nivre and Scholz (2004);?
we propose bilingual features based on word-alignment information to prefer ?target-sidecontiguity?
in resolving shift-reduce conflicts(Section 3);?
we verify empirically that shift-reduce con-flicts are the major source of errors, and cor-rect shift-reduce decisions strongly correlatewith the above bilingual contiguity condi-tions even with automatic alignments (Sec-tion 5.3);?
finally, with just three bilingual features,we improve dependency parsing accuracyby 0.6% for both English and Chinese overthe state-of-the-art baseline with negligible(?6%) efficiency overhead (Section 5.4).2 Simpler Shift-Reduce DependencyParsing with Three ActionsThe basic idea of classical shift-reduce parsingfrom compiler theory (Aho and Ullman, 1972) isto perform a left-to-right scan of the input sen-tence, and at each step, choose one of the two ac-tions: either shift the current word onto the stack,or reduce the top two (or more) items on the stack,replacing them with their combination.
This ideahas been applied to constituency parsing, for ex-ample in Sagae and Lavie (2006), and we describebelow a simple variant for dependency parsingsimilar to Yamada and Matsumoto (2003) and the?arc-standard?
version of Nivre (2004).2.1 The Three ActionsBasically, we just need to split the reduce ac-tion into two symmetric (sub-)actions, reduceLand reduceR, depending on which one of the two1223stack queue arcsprevious S wi|Q Ashift S|wiQ Aprevious S|st?1|stQ AreduceL S|st Q A ?
{(st, st?1)}reduceR S|st?1 Q A ?
{(st?1, st)}Table 1: Formal description of the three actions.Note that shift requires non-empty queue whilereduce requires at least two elements on the stack.items becomes the head after reduction.
More for-mally, we describe a parser configuration by a tu-ple ?S,Q,A?
where S is the stack, Q is the queueof remaining words of the input, and A is the setof dependency arcs accumulated so far.3 At eachstep, we can choose one of the three actions:1. shift: move the head of (a non-empty) queueQ onto stack S;2. reduceL: combine the top two items on thestack, stand st?1(t ?
2), and replacethem with st(as the head), and add a left arc(st, st?1) to A;3. reduceR: combine the top two items on thestack, stand st?1(t ?
2), and replace themwith st?1(as the head), and add a right arc(st?1, st) to A.These actions are summarized in Table 1.
Theinitial configuration is always ?
?, w1.
.
.
wn, ?
?with empty stack and no arcs, and the final con-figuration is ?wj, ?, A?
where wjis recognized asthe root of the whole sentence, and A encodes aspanning tree rooted at wj.
For a sentence of nwords, there are exactly 2n ?
1 actions: n shiftsand n ?
1 reductions, since every word must bepushed onto stack once, and every word except theroot will eventually be popped in a reduction.
Thetime complexity, as other shift-reduce instances, isclearly O(n).2.2 Example of Shift-Reduce ConflictFigure 2 shows the trace of this paradigm on theexample sentence.
For the first two configurations3a ?configuration?
is sometimes called a ?state?
(Zhangand Clark, 2008), but that term is confusing with the states inshift-reduce LR/LL parsing, which are quite different.0 - I saw Bill with a ...1 shift I saw Bill with a ...2 shift I saw Bill with a ...3 reduceL saw Bill with a ...I4 shift saw Bill with a ...I5a reduceR saw with a ...I Bill5b shift saw Bill with a ...IFigure 2: A trace of 3-action shift-reduce on theexample sentence.
Shaded words are on stack,while gray words have been popped from stack.After step (4), the process can take either (5a)or (5b), which correspond to the two attachments(1a) and (1b) in Figure 1, respectively.
(0) and (1), only shift is possible since there arenot enough items on the stack for reduction.
Atstep (3), we perform a reduceL, making word ?I?a modifier of ?saw?
; after that the stack containsa single word and we have to shift the next word?Bill?
(step 4).
Now we face a shift-reduce con-flict: we can either combine ?saw?
and ?Bill?
ina reduceR action (5a), or shift ?Bill?
(5b).
We willuse features extracted from the configuration to re-solve the conflict.
For example, one such featurecould be a bigram st?
st?1, capturing how likelythese two words are combined; see Table 2 for thecomplete list of feature templates we use in thisbaseline parser.We argue that this kind of shift-reduce conflictsare the major source of parsing errors, since theother type of conflict, reduce-reduce conflict (i.e.,whether left or right) is relatively easier to resolvegiven the part-of-speech information.
For exam-ple, between a noun and an adjective, the formeris much more likely to be the head (and so is averb vs. a preposition or an adverb).
Shift-reduceresolution, however, is more non-local, and ofteninvolves a triple, for example, (saw, Bill, with) fora typical PP-attachment.
On the other hand, if weindeed make a wrong decision, a reduce-reducemistake just flips the head and the modifier, andoften has a more local effect on the shape of thetree, whereas a shift-reduce mistake always leads1224Type FeaturesUnigram stT (st) st?
T (st)st?1T (st?1) st?1?
T (st?1)wiT (wi) wi?
T (wi)Bigram st?
st?1T (st) ?
T (st?1) T (st) ?
T (wi)T (st) ?
st?1?
T (st?1) st?
st?1?
T (st?1) st?
T (st) ?
T (st?1)st?
T (st) ?
st?1st?
T (st) ?
st?1?
T (st?1)Trigram T (st) ?
T (wi) ?
T (wi+1) T (st?1) ?
T (st) ?
T (wi) T (st?2) ?
T (st?1) ?
T (st)st?
T (wi) ?
T (wi+1) T (st?1) ?
st?
T (wi)Modifier T (st?1) ?
T (lc(st?1)) ?
T (st) T (st?1) ?
T (rc(st?1)) ?
T (st) T (st?1) ?
T (st) ?
T (lc(st))T (st?1) ?
T (st) ?
T (rc(st)) T (st?1) ?
T (lc(st?1)) ?
stT (st?1) ?
T (rc(st?1)) ?
stT (st?1) ?
st?
T (lc(st))Table 2: Feature templates of the baseline parser.
st, st?1denote the top and next to top words on thestack; wiand wi+1denote the current and next words on the queue.
T (?)
denotes the POS tag of agiven word, and lc(?)
and rc(?)
represent the leftmost and rightmost child.
Symbol ?
denotes featureconjunction.
Each of these templates is further conjoined with the 3 actions shift, reduceL, and reduceR.to vastly incompatible tree shapes with crossingbrackets (for example, [saw Bill] vs. [Bill with atelescope]).
We will see in Section 5.3 that thisis indeed the case in practice, thus suggesting usto focus on shift-reduce resolution, which we willreturn to with the help of bilingual constraints inSection 3.2.3 Comparison with Arc-EagerThe three action system was originally describedby Yamada and Matsumoto (2003) (although theirmethods require multiple passes over the input),and then appeared as ?arc-standard?
in Nivre(2004), but was argued against in comparison tothe four-action ?arc-eager?
variant.
Most subse-quent works on shift-reduce or ?transition-based?dependency parsing followed ?arc-eager?
(Nivreand Scholz, 2004; Zhang and Clark, 2008), whichnow becomes the dominant style.
But we arguethat ?arc-standard?
is preferable because:1. in the three action ?arc-standard?
system, thestack always contains a list of unrelated sub-trees recognized so far, with no arcs betweenany of them, e.g.
(I?
saw) and (Bill) in step4 of Figure 2), whereas the four action ?arc-eager?
style can have left or right arrows be-tween items on the stack;2. the semantics of the three actions are atomicand disjoint, whereas the semantics of 4 ac-tions are not completely disjoint.
For exam-ple, their Left action assumes an implicit Re-duce of the left item, and their Right ac-tion assumes an implicit Shift.
Furthermore,these two actions have non-trivial precondi-tions which also causes the next problem (seebelow).
We argue that this is rather compli-cated to implement.3.
the ?arc-standard?
scan always succeeds,since at the end we can always reduce withempty queue, whereas the ?arc-eager?
stylesometimes goes into deadends where no ac-tion can perform (prevented by precondi-tions, otherwise the result will not be a well-formed tree).
This becomes parsing failuresin practice (Nivre and Scholz, 2004), leavingmore than one fragments on stack.As we will see in Section 5.1, this simplerarc-standard system performs equally well witha state-of-the-art arc-eager system (Zhang andClark, 2008) on standard English Treebank pars-ing (which is never shown before).
We arguethat all things being equal, this simpler paradigmshould be preferred in practice.
42.4 Beam Search ExtensionWe also enhance deterministic shift-reduce pars-ing with beam search, similar to Zhang and Clark(2008), where k configurations develop in paral-lel.
Pseudocode 1 illustrates the algorithm, wherewe keep an agenda V of the current active con-figurations, and at each step try to extend them byapplying one of the three actions.
We then dumpthe best k new configurations from the buffer back4On the other hand, there are also arguments for ?arc-eager?, e.g., ?incrementality?
; see (Nivre, 2004; Nivre, 2008).1225Pseudocode 1 beam-search shift-reduce parsing.1: Input: POS-tagged word sequence w1.
.
.
wn2: start ?
?
?, w1.
.
.
wn, ??
?
initial config: empty stack,no arcs3: V?
{start} ?
initial agenda4: for step ?
1 .
.
.
2n?
1 do5: BUF?
?
?
buffer for new configs6: for each config in agenda V do7: for act ?
{shift, reduceL, reduceR} do8: if act is applicable to config then9: next ?
apply act to config10: insert next into buffer BUF11: V?
top k configurations of BUF12: Output: the tree of the best config in Vinto the agenda for the next step.
The complexityof this algorithm is O(nk), which subsumes thedeterminstic mode as a special case (k = 1).2.5 Online TrainingTo train the parser we need an ?oracle?
or gold-standard action sequence for gold-standard depen-dency trees.
This oracle turns out to be non-uniquefor the three-action system (also non-unique forthe four-action system), because left dependentsof a head can be reduced either before or after allright dependents are reduced.
For example, in Fig-ure 2, ?I?
is a left dependent of ?saw?, and can inprinciple wait until ?Bill?
and ?with?
are reduced,and then finally combine with ?saw?.
We chooseto use the heuristic of ?shortest stack?
that alwaysprefers reduceL over shift, which has the effect thatall left dependents are first recognized inside-out,followed by all right dependents, also inside-out,which coincides with the head-driven constituencyparsing model of Collins (1999).We use the popular online learning algorithmof structured perceptron with parameter averag-ing (Collins, 2002).
Following Collins and Roark(2004) we also use the ?early-update?
strategy,where an update happens whenever the gold-standard action-sequence falls off the beam, withthe rest of the sequence neglected.
As a specialcase, for the deterministic mode, updates alwaysco-occur with the first mistake made.
The intuitionbehind this strategy is that future mistakes are of-ten caused by previous ones, so with the parser onthe wrong track, future actions become irrelevantfor learning.
See Section 5.3 for more discussions.
(a) I:::::::::saw Bill with a telescope .wo yong wangyuanjin kandao le Bi?er.c(st?1, st) =+; reduce is correct(b) I:::::::::saw Bill with a telescope .wo kandao le na wangyuanjin de Bi?er.c(st?1, st) =?
; reduce is wrong(c) I saw:::::::::::Bill with:::a::::::::::telescope:.wo kandao le na wangyuanjin de Bi?er.cR(st, wi) =+; shift is correct(d) I saw:::::::::Bill with:::a::::::::::telescope:.wo yong wangyuanjin kandao le Bi?er.cR(st, wi) =?
; shift is wrongFigure 3: Bilingual contiguity features c(st?1, st)and cR(st, wi) at step (4) in Fig.
2 (facing a shift-reduce decision).
Bold words are currently onstack while gray ones have been popped.
Here thestack tops are st= Bill, st?1= saw, and the queuehead is wi= with; underlined texts mark the sourceand target spans being considered, and wavy un-derlines mark the allowed spans (Tab.
3).
Red boldalignment links violate contiguity constraints.3 Soft Bilingual Constraints as FeaturesAs suggested in Section 2.2, shift-reduce con-flicts are the central problem we need to addresshere.
Our intuition is, whenever we face a deci-sion whether to combine the stack tops st?1andstor to shift the current word wi, we will consultthe other language, where the word-alignment in-formation would hopefully provide a preference,as in the running example of PP-attachment (seeFigure 1).
We now develop this idea into bilingualcontiguity features.12263.1 A Pro-Reduce Feature c(st?1, st)Informally, if the correct decision is a reduction,then it is likely that the corresponding words ofst?1and ston the target-side should also form acontiguous span.
For example, in Figure 3(a), thesource span of a reduction is [saw .. Bill], whichmaps onto [kandao .
.
.
Bi?er] on the Chinese side.This target span is contiguous, because no wordwithin this span is aligned to a source word out-side of the source span.
In this case we say featurec(st?1, st) =+, which encourages ?reduce?.However, in Figure 3(b), the source span is still[saw .. Bill], but this time maps onto a muchlonger span on the Chinese side.
This target spanis discontiguous, since the Chinese words na andwangyuanjin are alinged to English ?with?
and?telescope?, both of which fall outside of thesource span.
In this case we say feature c(st?1, st)=?, which discourages ?reduce?
.3.2 A Pro-Shift Feature cR(st, wi)Similarly, we can develop another featurecR(st, wi) for the shift action.
In Figure 3(c),when considering shifting ?with?, the sourcespan becomes [Bill .. with] which maps to [na.. Bi?er] on the Chinese side.
This target spanlooks like discontiguous in the above definitionwith wangyuanjin aligned to ?telescope?, but wetolerate this case for the following reasons.
Thereis a crucial difference between shift and reduce:in a shift, we do not know yet the subtree spans(unlike in a reduce we are always combining twowell-formed subtrees).
The only thing we aresure of in a shift action is that stand wiwill becombined before st?1and stare combined (Ahoand Ullman, 1972), so we can tolerate any targetword aligned to source word still in the queue,but do not allow any target word aligned to analready recognized source word.
This explainsthe notational difference between cR(st, wi) andc(st?1, st), where subscript ?R?
means ?rightcontiguity?.As a final example, in Figure 3(d), Chineseword kandao aligns to ?saw?, which is alreadyrecognized, and this violates the right contiguity.So cR(st, wi) =?, suggesting that shift is probablywrong.
To be more precise, Table 3 shows the for-mal definitions of the two features.
We basicallysource target alowedfeature f span sp span tp span apc(st?1, st) [st?1..st] M(sp) [st?1..st]cR(st, wi) [st..wi] M(sp) [st..wn]f = + iff.
M?1(M(sp)) ?
apTable 3: Formal definition of bilingual features.M(?)
is maps a source span to the target language,and M?1(?)
is the reverse operation mapping backto the source language.map a source span sp to its target span M(sp),and check whether its reverse image back onto thesource language M?1(M(sp)) falls inside the al-lowed span ap.
For cR(st, wi), the allowed spanextends to the right end of the sentence.53.3 Variations and ImplementationTo conclude so far, we have got two alignment-based features, c(st?1, st) correlating with reduce,and cR(st, wi) correlating with shift.
In fact, theconjunction of these two features,c(st?1, st) ?
cR(st, wi)is another feature with even stronger discrimina-tion power.
Ifc(st?1, st) ?
cR(st, wi) = + ?
?it is strongly recommending reduce, whilec(st?1, st) ?
cR(st, wi) = ?
?+is a very strong signal for shift.
So in total we gotthree bilingual feature (templates), which in prac-tice amounts to 24 instances (after cross-productwith {?,+} and the three actions).
We show inSection 5.3 that these features do correlate withthe correct shift/reduce actions in practice.The naive implemention of bilingual featurecomputation would be of O(kn2) complexityin the worse case because when combining thelargest spans one has to scan over the whole sen-tence.
We envision the use of a clever datastructurewould reduce the complexity, but leave this to fu-ture work, as the experiments (Table 8) show that5Our definition implies that we only consider faithfulspans to be contiguous (Galley et al, 2004).
Also note thatsource spans include all dependents of stand st?1.1227the parser is only marginally (?6%) slower withthe new bilingual features.
This is because the ex-tra work, with just 3 bilingual features, is not thebottleneck in practice, since the extraction of thevast amount of other features in Table 2 dominatesthe computation.4 Related Work in Grammar InductionBesides those cited in Section 1, there are someother related work on using bilingual constraintsfor grammar induction (rather than parsing).
Forexample, Hwa et al (2005) use simple heuris-tics to project English trees to Spanish and Chi-nese, but get discouraging accuracy results learnedfrom those projected trees.
Following this idea,Ganchev et al (2009) and Smith and Eisner (2009)use constrained EM and parser adaptation tech-niques, respectively, to perform more principledprojection, and both achieve encouraging results.Our work, by constrast, never uses bilingualtree pairs not tree projections, and only uses wordalignment alone to enhance a monolingual gram-mar, which learns to prefer target-side contiguity.5 Experiments5.1 Baseline ParserWe implement our baseline monolingual parser (inC++) based on the shift-reduce algorithm in Sec-tion 2, with feature templates from Table 2.
Weevaluate its performance on the standard Penn En-glish Treebank (PTB) dependency parsing task,i.e., train on sections 02-21 and test on section 23with automatically assigned POS tags (at 97.2%accuracy) using a tagger similar to Collins (2002),and using the headrules of Yamada and Mat-sumoto (2003) for conversion into dependencytrees.
We use section 22 as dev set to deter-mine the optimal number of iterations in per-ceptron training.
Table 4 compares our baselineagainst the state-of-the-art graph-based (McDon-ald et al, 2005) and transition-based (Zhang andClark, 2008) approaches, and confirms that oursystem performs at the same level with those state-of-the-art, and runs extremely fast in the determin-istic mode (k=1), and still quite fast in the beam-search mode (k=16).parser accuracy secs/sentMcDonald et al (2005) 90.7 0.150Zhang and Clark (2008) 91.4 0.195our baseline at k=1 90.2 0.009our baseline at k=16 91.3 0.125Table 4: Baseline parser performance on standardPenn English Treebank dependency parsing task.The speed numbers are not exactly comparablesince they are reported on different machines.Training Dev TestCTB Articles 1-270 301-325 271-300Bilingual Paris 2745 273 290Table 5: Training, dev, and test sets from bilingualChinese Treebank a` la Burkett and Klein (2008).5.2 Bilingual DataThe bilingual data we use is the translated por-tion of the Penn Chinese Treebank (CTB) (Xueet al, 2002), corresponding to articles 1-325 ofPTB, which have English translations with gold-standard parse trees (Bies et al, 2007).
Table 5shows the split of this data into training, devel-opment, and test subsets according to Burkett andKlein (2008).
Note that not all sentence pairs couldbe included, since many of them are not one-to-one aligned at the sentence level.
Our word-alignments are generated from the HMM alignerof Liang et al (2006) trained on approximately1.7M sentence pairs (provided to us by David Bur-kett, p.c.).
This aligner outputs ?soft alignments?,i.e., posterior probabilities for each source-targetword pair.
We use a pruning threshold of 0.535 toremove low-confidence alignment links,6 and usethe remaining links as hard alignments; we leavethe use of alignment probabilities to future work.For simplicity reasons, in the following exper-iments we always supply gold-standard POS tagsas part of the input to the parser.5.3 Testing our HypothesesBefore evaluating our bilingual approach, we needto verify empirically the two assumptions wemade about the parser in Sections 2 and 3:6and also removing notoriously bad links in {the, a, an}?
{de, le} following Fossum and Knight (2008).1228sh ?
re re ?
sh sh-re re-re# 92 98 190 7% 46.7% 49.7% 96.4% 3.6%Table 6: [Hypothesis 1] Error distribution in thebaseline model (k = 1) on English dev set.
?sh ?
re?
means ?should shift, but reduced?.
Shift-reduce conflicts overwhelmingly dominate.1.
(monolingual) shift-reduce conflict is the ma-jor source of errors while reduce-reduce con-flict is a minor issue;2.
(bilingual) the gold-standard decisions ofshift or reduce should correlate with contigu-ities of c(st?1, st), and of cR(st, wi).Hypothesis 1 is verified in Table 6, where wecount all the first mistakes the baseline parsermakes (in the deterministic mode) on the En-glish dev set (273 sentences).
In shift-reduce pars-ing, further mistakes are often caused by previ-ous ones, so only the first mistake in each sen-tence (if there is one) is easily identifiable;7 thisis also the argument for ?early update?
in apply-ing perceptron learning to these incremental pars-ing algorithms (Collins and Roark, 2004) (see alsoSection 2).
Among the 197 first mistakes (other76 sentences have perfect output), the vast ma-jority, 190 of them (96.4%), are shift-reduce er-rors (equally distributed between shift-becomes-reduce and reduce-becomes-shift), and only 7(3.6%) are due to reduce-reduce conflicts.8 Thesestatistics confirm our intuition that shift-reduce de-cisions are much harder to make during parsing,and contribute to the overwhelming majority of er-rors, which is studied in the next hypothesis.Hypothesis 2 is verified in Table 7.
We takethe gold-standard shift-reduce sequence on the En-glish dev set, and classify them into the four cat-egories based on bilingual contiguity features: (a)c(st?1, st), i.e.
whether the top 2 spans on stackis contiguous, and (b) cR(st, wi), i.e.
whether the7to be really precise one can define ?independent mis-takes?
as those not affected by previous ones, i.e., errorsmade after the parser recovers from previous mistakes; butthis is much more involved and we leave it to future work.8Note that shift-reduce errors include those due to thenon-uniqueness of oracle, i.e., between some reduceL andshift.
Currently we are unable to identify ?genuine?
errorsthat would result in an incorrect parse.
See also Section 2.5.c(st?1, st) cR(st, wi) shift reduce+ ?
172 ?
1,209?
+ 1,432 > 805+ + 4,430 ?
3,696?
?
525 ?
576total 6,559 = 6,286Table 7: [Hyp.
2] Correlation of gold-standardshift/reduce decisions with bilingual contiguityconditions (on English dev set).
Note there is al-ways one more shift than reduce in each sentence.stack top is contiguous with the current word wi.According to discussions in Section 3, when (a) iscontiguous and (b) is not, it is a clear signal forreduce (to combine the top two elements on thestack) rather than shift, and is strongly supportedby the data (first line: 1209 reduces vs. 172 shifts);and while when (b) is contiguous and (a) is not,it should suggest shift (combining stand wibe-fore st?1and stare combined) rather than reduce,and is mildly supported by the data (second line:1432 shifts vs. 805 reduces).
When (a) and (b) areboth contiguous or both discontiguous, it shouldbe considered a neutral signal, and is also consis-tent with the data (next two lines).
So to conclude,this bilingual hypothesis is empirically justified.On the other hand, we would like to note thatthese correlations are done with automatic wordalignments (in our case, from the Berkeley aligner)which can be quite noisy.
We suspect (and will fin-ish in the future work) that using manual align-ments would result in a better correlation, thoughfor the main parsing results (see below) we canonly afford automatic alignments in order for ourapproach to be widely applicable to any bitext.5.4 ResultsWe incorporate the three bilingual features (again,with automatic alignments) into the baselineparser, retrain it, and test its performance on theEnglish dev set, with varying beam size.
Table 8shows that bilingual constraints help more withlarger beams, from almost no improvement withthe deterministic mode (k=1) to +0.5% better withthe largest beam (k=16).
This could be explainedby the fact that beam-search is more robust thanthe deterministic mode, where in the latter, if our1229baseline +bilingualk accuracy time (s) accuracy time (s)1 84.58 0.011 84.67 0.0122 85.30 0.025 85.62 0.0284 85.42 0.040 85.81 0.0448 85.50 0.081 85.95 0.08516 85.57 0.158 86.07 0.168Table 8: Effects of beam size k on efficiency andaccuracy (on English dev set).
Time is averageper sentence (in secs).
Bilingual constraints showmore improvement with larger beams, with a frac-tional efficiency overhead over the baseline.English Chinesemonolingual baseline 86.9 85.7+bilingual features 87.5 86.3improvement +0.6 +0.6signficance level p < 0.05 p < 0.08Berkeley parser 86.1 87.9Table 9: Final results of dependency accuracy (%)on the test set (290 sentences, beam size k=16).bilingual features misled the parser into a mistake,there is no chance of getting back, while in theformer multiple configurations are being pursuedin parallel.
In terms of speed, both parsers run pro-portionally slower with larger beams, as the timecomplexity is linear to the beam-size.
Computingthe bilingual features further slows it down, butonly fractionally so (just 1.06 times as slow as thebaseline at k=16), which is appealing in practice.By contrast, Burkett and Klein (2008) reportedtheir approach of ?monolingual k-best parsing fol-lowed by bilingual k2-best reranking?
to be ?3.8times slower?
than monolingual parsing.Our final results on the test set (290 sentences)are summarized in Table 9.
On both Englishand Chinese, the addition of bilingual featuresimproves dependency arc accuracies by +0.6%,which is mildly significant using the Z-test ofCollins et al (2005).
We also compare our resultsagainst the Berkeley parser (Petrov and Klein,2007) as a reference system, with the exact samesetting (i.e., trained on the bilingual data, and test-ing using gold-standard POS tags), and the result-ing trees are converted into dependency via thesame headrules.
We use 5 iterations of split-mergegrammar induction as the 6th iteration overfits thesmall training set.
The result is worse than ourbaseline on English, but better than our bilingualparser on Chinese.
The discrepancy between En-glish and Chinese is probably due to the fact thatour baseline feature templates (Table 2) are engi-neered on English not Chinese.6 Conclusion and Future WorkWe have presented a novel parsing paradigm,bilingually-constrained monolingual parsing,which is much simpler than joint (bi-)parsing, yetstill yields mild improvements in parsing accuracyin our preliminary experiments.
Specifically,we showed a simple method of incorporatingalignment features as soft evidence on top of astate-of-the-art shift-reduce dependency parser,which helped better resolve shift-reduce conflictswith fractional efficiency overhead.The fact that we managed to do this with onlythree alignment features is on one hand encour-aging, but on the other hand leaving the bilingualfeature space largely unexplored.
So we will en-gineer more such features, especially with lexical-ization and soft alignments (Liang et al, 2006),and study the impact of alignment quality on pars-ing improvement.
From a linguistics point of view,we would like to see how linguistics distanceaffects this approach, e.g., we suspect English-French would not help each other as much asEnglish-Chinese do; and it would be very interest-ing to see what types of syntactic ambiguities canbe resolved across different language pairs.
Fur-thermore, we believe this bilingual-monolingualapproach can easily transfer to shift-reduce con-stituency parsing (Sagae and Lavie, 2006).AcknowledgmentsWe thank the anonymous reviewers for pointing tous references about ?arc-standard?.
We also thankAravind Joshi and Mitch Marcus for insights onPP attachment, Joakim Nivre for discussions onarc-eager, Yang Liu for suggestion to look at man-ual alignments, and David A. Smith for sendingus his paper.
The second and third authors weresupported by National Natural Science Foundationof China, Contracts 60603095 and 60736014, and863 State Key Project No.
2006AA010108.1230ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
TheTheory of Parsing, Translation, and Compiling, vol-ume I: Parsing of Series in Automatic Computation.Prentice Hall, Englewood Cliffs, New Jersey.Ann Bies, Martha Palmer, Justin Mott, and ColinWarner.
2007.
English chinese translation treebankv1.0.
LDC2007T02.David Burkett and Dan Klein.
2008.
Two languagesare better than one (for syntactic parsing).
In Pro-ceedings of EMNLP.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceed-ings of ACL.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL, pages 531?540,Ann Arbor, Michigan, June.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedingsof EMNLP.Jason Eisner.
2003.
Learning non-isomorphic treemappings for machine translation.
In Proceedingsof ACL (poster), pages 205?208.Victoria Fossum and Kevin Knight.
2008.
Using bilin-gual chinese-english word alignments to resolve pp-attachment ambiguity in english.
In Proceedings ofAMTA Student Workshop.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of HLT-NAACL, pages 273?280.Kuzman Ganchev, Jennifer Gillenwater, and BenTaskar.
2009.
Dependency grammar inductionvia bitext projection constraints.
In Proceedings ofACL-IJCNLP.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofthe ACL: HLT, Columbus, OH, June.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Natural Language Engineering.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,and Ben Taskar.
2006.
An end-to-end discrimina-tive approach to machine translation.
In Proceed-ings of COLING-ACL, Sydney, Australia, July.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of the 43rd ACL.Haitao Mi and Liang Huang.
2008.
Forest-based trans-lation rule extraction.
In Proceedings of EMNLP,Honolulu, Haiwaii.Joakim Nivre and Mario Scholz.
2004.
Deterministicdependency parsing of english text.
In Proceedingsof COLING, Geneva.Joakim Nivre.
2004.
Incrementality in deterministicdependency parsing.
In Incremental Parsing: Bring-ing Engineering and Cognition Together.
Workshopat ACL-2004, Barcelona.Joakim Nivre.
2008.
Algorithms for deterministic in-cremental dependency parsing.
Computational Lin-guistics, 34(4):513?553.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of HLT-NAACL.Kenji Sagae and Alon Lavie.
2006.
A best-first prob-abilistic shift-reduce parser.
In Proceedings of ACL(poster).Lee Schwartz, Takako Aikawa, and Chris Quirk.
2003.Disambiguation of english pp attachment using mul-tilingual aligned data.
In Proceedings of MT SummitIX.David A. Smith and Jason Eisner.
2009.
Parser adapta-tion and projection with quasi-synchronous features.In Proceedings of EMNLP.David A. Smith and Noah A. Smith.
2004.
Bilingualparsing with factored estimation: Using english toparse korean.
In Proceedings of EMNLP.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404.Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.2002.
Building a large-scale annotated chinese cor-pus.
In Proceedings of COLING.H.
Yamada and Y. Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
InProceedings of IWPT.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-based and transition-based dependency parsing us-ing beam-search.
In Proceedings of EMNLP.1231
