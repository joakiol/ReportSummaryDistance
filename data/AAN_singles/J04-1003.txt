c?
2004 Association for Computational LinguisticsVerb Class Disambiguation UsingInformative PriorsMirella Lapata?
Chris Brew?University of Sheffield Ohio State UniversityLevin?s (1993) study of verb classes is a widely used resource for lexical semantics.
In her frame-work, some verbs, such as give, exhibit no class ambiguity.
But other verbs, such as write, haveseveral alternative classes.
We extend Levin?s inventory to a simple statistical model of verb classambiguity.
Using this model we are able to generate preferences for ambiguous verbs without theuse of a disambiguated corpus.
We additionally show that these preferences are useful as priorsfor a verb sense disambiguator.1.
IntroductionMuch research in lexical semantics has concentrated on the relation between verbs andtheir arguments.
Many scholars hypothesize that the behavior of a verb, particularlywith respect to the expression and interpretation of its arguments, is to a large extentdetermined by its meaning (Talmy 1985; Jackendoff 1983; Goldberg 1995; Levin 1993;Pinker 1989; Green 1974; Gropen et al 1989; Fillmore 1965).
The correspondence be-tween verbal meaning and syntax has been extensively studied in Levin (1993), whichargues that verbs which display the same diathesis alternations?alternations in therealization of their argument structure?can be assumed to share certain meaningcomponents and to form a semantically coherent class.The converse of this assumption is that verb behavior (i.e., participation in diathe-sis alternations) can be used to provide clues about aspects of meaning, which in turncan be exploited to characterize verb senses (referred to as classes in Levin?s [1993] ter-minology).
A major advantage of this approach is that criteria for assigning senses canbe more concrete than is traditionally assumed in lexicographic work (e.g., WordNet ormachine-readable dictionaries) concerned with sense distinctions (Palmer 2000).
As anexample consider sentences (1)?
(4), taken from Levin.
Examples (1) and (2) illustratethe dative and benefactive alternations, respectively.
Dative verbs alternate betweenthe prepositional frame ?NP1 V NP2 to NP3?
(see (1a)) and the double-object frame?NP1 V NP2 NP3?
(see (1b)), whereas benefactive verbs alternate between the double-object frame (see (2a)) and the prepositional frame ?NP1 V NP2 for NP3?
(see (2b)).To decide whether a verb is benefactive or dative it suffices to test the acceptabilityof the for and to frames.
Verbs undergoing the conative alternation can be attestedeither as transitive or as intransitive with a prepositional phrase headed by the wordat.1 The role filled by the object of the transitive variant is shared by the noun phrasecomplement of at in the intransitive variant (see (3)).
This example makes explicit thatclass assignment depends not only on syntactic facts but also on judgments about?
Department of Computer Science, Regent Court, 211 Portobello Street, Sheffield, S1 4DP, UK.
E-mail:mlap@dcs.shef.ac.uk.?
Department of Linguistics, Oxley Hall,1712 Neil Avenue, Columbus, OH.
E-mail: cbrew@ling.ohio-state.edu.1 At is the most likely choice, but for some conative verbs the preposition is instead on or onto.46Computational Linguistics Volume 30, Number 1semantic roles.
Similarly, the possessor object alternation involves a possessor and apossessed attribute that can be manifested either as the verbal object or as the objectof a prepositional phrase headed by for (see (4)).
(1) a.
Bill sold a car to Tom.b.
Bill sold Tom a car.
(2) a. Martha carved the baby a toy.b.
Martha carved a toy for the baby.
(3) a. Paula hit the fence.b.
Paula hit at the fence.
(4) a. I admired his honesty.b.
I admired him for his honesty.Observation of the semantic and syntactic behavior of pay and give reveals thatthey pattern with sell in licensing the dative alternation.
These verbs are all membersof the Give class.
Verbs like make and build behave similarly to carve in licensing thebenefactive alternation and are members of the class of Build verbs.
The verbs beat,kick, and hit undergo the conative alternation; they are all members of the Hit verbclass.
By grouping together verbs that pattern together with respect to diathesis alter-nations, Levin (1993) defines approximately 200 verb classes, which she argues reflectimportant semantic regularities.
These analyses (and many similar ones by Levin andher successors) rely primarily on straightforward syntactic and syntactico-semantic cri-teria.
To adopt this approach is to accept some limitations on the reach of our analyses,since not all semantically interesting differences will have the appropriate reflexes insyntax.
Nevertheless, the emphasis on concretely available observables makes Levin?smethodology a good candidate for automation (Palmer 2000).Therefore, Levin?s (1993) classification has formed the basis for many efforts thataim to acquire lexical semantic information from corpora.
These exploit syntactic cues,or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulteim Walde 2000; Lapata 1999; McCarthy 2000).
Other work has used Levin?s classifi-cation (in conjunction with other lexical resources) to create dictionaries that expressthe systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosen-zweig, and Palmer 1997; Dorr and Jones 1996).
Levin?s inventory of verbs and classeshas been also useful for applications such as machine translation (Dorr 1997; Palmerand Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin2000), and document classification (Klavans and Kan 1998).Although the classification provides a general framework for describing verbalmeaning, it says only which verb meanings are possible, staying silent on the relativelikelihoods of the different meanings.
The inventory captures systematic regularitiesin the meaning of words and phrases but falls short of providing a probabilistic modelof these regularities.
Such a model would be useful in applications that need to resolveambiguity in the presence of multiple and conflicting probabilistic constraints.More precisely, Levin (1993) provides an index of 3,024 verbs for which she lists thesemantic classes and diathesis alternations.
The mapping between verbs and classesis not one to one.
Of the 3,024 verbs which she covers, 784 are listed as having morethan one class.
Even though Levin?s monosemous verbs outnumber her polysemousverbs by a factor of nearly four to one, the total frequency of the former (4,252,715)47Lapata and Brew Verb Class Disambiguation Using Informative PriorsTable 1Polysemous verbs according to Levin.Classes Verbs BNC frequency1 2,239 4,252,7152 536 2,325,9823 173 738,8544 43 395,2125 23 222,7476 7 272,6697 2 26,12310 1 4,427Figure 1Relation between number of classes and alternations.is comparable to the total frequency of the latter (3,986,014).
This means that closeto half of the cases processed by a semantic tagger would manifest some degree ofambiguity.
The frequencies are detailed in Table 1 and were compiled from a lemma-tized version of the British National Corpus (BNC) (Burnard 1995).
Furthermore, asshown in Figure 1, the level of ambiguity increases in tandem with the number ofalternations licensed by a given verb.
Consider, for example, verbs participating inone alternation only: Of these, 90.4% have one semantic class, 8.6% have two classes,0.7% have three classes, and 0.3% have four classes.
In contrast, of the verbs licensingsix different alternations, 14% have one class, 17% have two classes, 12.4% have threeclasses, 53.6% have four classes, 2% have six classes, and 1% have seven classes.
Asambiguity increases, so does the availability and potential utility of information aboutdiathesis alternations.Palmer (2000) and Dang et al (1998) argue that syntactic frames and verb classesare useful for developing principled classifications of verbs.
We go beyond this, show-ing that they can also be of assistance in disambiguation.
Consider, for instance, theverb serve, which is a member of four Levin classes: Give, Fit, Masquerade, andFulfilling.
Each of these classes can in turn license four distinct syntactic frames.48Computational Linguistics Volume 30, Number 1As shown in the examples2 below, in (5a) serve appears ditransitively and belongs tothe semantic class of Give verbs, in (5b) it occurs transitively and is a member of theclass of Fit verbs, in (5c) it takes the predicative complement as minister of the interiorand is a member of the class of Masquerade verbs.
Finally, in sentence (5d) serve isa Fulfilling verb and takes two complements, a noun phrase (an apprenticeship) anda prepositional phrase headed by to (to a still-life photographer).
In the case of verbs likeserve, we can guess their semantic class solely on the basis of the frame with whichthey appear.
(5) a. I?m desperately trying to find a venue for the reception which canserve our guests an authentic Italian meal.
NP1 V NP2 NP3b.
The airline serves 164 destinations in over 75 countries.
NP1 V NP2c.
Jean-Antoine Chaptal was a brilliant chemist and technocrat whoserved Napoleon as minister of the interior from 1800 to 1805.
NP1 V NP2as NP3d.
Before her brief exposure to pop stardom, she served anapprenticeship to a still-life photographer.
NP1 V NP2 to NP3But sometimes we do not have the syntactic information that would provide cuesfor semantic disambiguation.
Consider example (6).
The verb write is a member of threeLevin classes, two of which (Message Transfer, Performance) take the double-object frame.
In this case, we have the choice between theMessage Transfer reading(see (6a)) and the Performance reading (see (6b)).
The same situation arises with theverb toast, which is listed as a Prepare verb and a Judgment verb; both these classeslicense the prepositional frame ?NP1 V NP2 for NP3.?
In sentence (7a) the preferredreading is that of Prepare rather than that of Judgment (see sentence (7b)).
The verbstudy is ambiguous among three classes when attested in the transitive frame: Learn(see example (8a)), Sight (see example (8b)), and Assessment (see example (8c)).
Theverb convey, when attested in the prepositional frame ?NP1 V NP2 to NP3,?
can beambiguous between the Say class (see example (9a)) and the Send class (see exam-ple (9b)).
In order to correctly decide the semantic class for a given ambiguous verb,we would need not only detailed semantic information about the verb?s arguments,but also a considerable amount of world knowledge.
Admittedly, selectional restric-tions are sufficient for distinguishing (7a) from (7b) (one normally heats up inanimateentities and salutes animate ones), but selectional restrictions alone are probably notenough to disambiguate (6a) from (6b), since both letter and screenplay are likely to bedescribed as written material.
Rather, we need fine-grained world knowledge: Bothscripts and letters can be written for someone: only letters can be written to someone.
(6) a.
A solicitor wrote him a letter at the airport.b.
I want you to write me a screenplay called ?The Trip.?
(7) a.
He sat by the fire and toasted a piece of bread for himself.b.
We all toasted Nigel for his recovery.2 Unless otherwise stated, our example sentences were taken (possibly in simplified form) from the BNC.49Lapata and Brew Verb Class Disambiguation Using Informative Priors(8) a. Chapman studied medicine at Cambridge.b.
Romanov studied the old man carefully, looking for some sign that heknew exactly what had been awaiting him at the bank.c.
The alliance will also study the possibility of providing service to otherhigh-volume products, such as IBM and multi-vendor workstations.
(9) a.
By conveying the news to her sister, she would convey by implicationsomething of her own anxiety.b.
The judge signed the committal warrant and the police conveyed Mr.Butler to prison, giving the warrant to the governor.This need for world knowledge (or at least a convenient way of approximatingthis knowledge) is not an isolated phenomenon but manifests itself across a varietyof classes and frames (e.g., double object, transitive, prepositional frame: see exam-ples (6)?(9)).
We have argued that the concreteness of Levin-style verb classes is anadvantage, but this advantage would be compromised if we tried to fold too muchworld knowledge into the classification.
We do not do this.
Instead, Section 5 of thecurrent article describes disambiguation experiments in which our probabilistic Levinclasses are used in tandem with proxies for appropriate world knowledge.It is important to point out that Levin?s (1993) classification is not intended as anexhaustive description of English verbs, their meanings, and their likelihood.
Manyother classifications could have been built using the same principles.
A different group-ing might, for example, have occurred if finer or coarser semantic distinctions weretaken into account (see Merlo and Stevenson [2001] and Dang, Rosenzweig, and Palmer[1997] for alternative classifications) or if the containment of ambiguity was one of theclassification objectives.
As pointed out by Kipper, Dang, and Palmer (2000), Levinclasses exhibit inconsistencies, and verbs are listed in multiple classes, some of whichhave conflicting sets of syntactic frames.
This means that some ambiguities may alsoarise as a result of accidental errors or inconsistencies.
The classification was creatednot with computational uses in mind, but for human readers, so it has not been nec-essary to remedy all the errors and omissions that might cause trouble for machines.Similar issues arise in almost all efforts to make use of preexisting lexical resources forcomputational purposes (Briscoe and Carroll 1997), so none of the above commentsshould be taken as criticisms of Levin?s achievement.The objective of this article is to show how to train and use a probabilistic versionof Levin?s classification in verb sense disambiguation.
We treat errors and inconsis-tencies in the classification as noise.
Although all our tests have used Levin?s classesand the British National Corpus, the method itself depends neither on the details ofLevin?s classification nor on parochial facts about the English language.
Our futurework will include tests on other languages, other classifications, and other corpora.The model developed in this article takes as input a partially parsed corpus andgenerates, for each combination of a verb and its syntactic frame, a probability distri-bution over the available verb classes.
The corpus itself does not have to be labeledwith classes.
This makes it feasible to use large corpora.
Our model is not immediatelyuseful for disambiguation, because it cannot discriminate among different occurrencesof the same verb and frame, but it can (as we show in Section 5) be used as a priorin a full disambiguation system that does take appropriate account of context.
Themodel relies on several gross simplifications; it does not take selectional restrictions,discourse, or pragmatic information into account but is demonstrably superior to sim-pler priors that make no use of subcategorization.50Computational Linguistics Volume 30, Number 1The remainder of this article is organized as follows.
In Section 2 we describe theprobabilistic model and the estimation of the various model parameters.
In Sections 3and 4 we report on the results of two experiments that use the model to derive thedominant class for polysemous verbs.
Sections 5 and 6 discuss our verb class disam-biguation experiments.
We base our results on the BNC, a 100-million-word collectionof samples of written and spoken language from a wide range of sources designedto represent a wide cross-section of current British English, both spoken and writ-ten (Burnard 1995).
We discuss our results in Section 7 and review related work inSection 8.2.
The Prior ModelConsider again the sentences in (6).
Assuming that we more often write something tosomeone rather than for someone, we would like to derive Message Transfer asthe prevalent class for write rather than Performance.
We view the choice of a classfor a polysemous verb in a given frame as maximizing the joint probability P(c, f , v),where v is a verb subcategorizing for the frame f and inhabiting more than one Levinclass c:P(c, f , v) = P(v) ?
P(f |v) ?
P(c|v, f ) (10)Although the terms P(v) and P(f |v) can be estimated from the BNC (P(v) reducesto the number of times a verb is attested in the corpus, and P(f |v) can be obtainedthrough parsing), the estimation of P(c|v, f ) is somewhat problematic, since it relieson the frequency F(c, v, f ).
The latter could be obtained straightforwardly if we hadaccess to a parsed corpus annotated with subcategorization and semantic-class infor-mation.
Lacking such a corpus we will assume that the semantic class determines thesubcategorization patterns of its members independently of their identity (see (11)):P(c|v, f ) ?
P(c|f ) (11)The independence assumption is a simplification of Levin?s (1993) hypothesis that theargument structure of a given verb is a direct reflection of its meaning.
The rationalebehind the approximation in (11) is that since class formation is determined on thebasis of diathesis alternations, it is the differences in subcategorization structure, ratherthan the identity of the individual verbs, that determine class likelihood.
For example,if we know that some verb subcategorizes for the double object and the prepositional?NP1 V NP2 to NP3?
frames, we can guess that it is a member of the Give class orthe Message Transfer class without knowing whether this verb is give, write, or tell.Note that the approximation in (11) assumes that verbs of the same class uniformlysubcategorize (or not) for a given frame.
This is evidently not true for all classes ofverbs.
For example, all Give verbs undergo the dative diathesis alternation, and there-fore we would expect them to be attested in both the double-object and prepositionalframe, but only a subset of Create verbs undergo the benefactive alternation.
Forexample, the verb invent is a Create verb and can be attested only in the benefactiveprepositional frame (I will invent a tool for you versus ?I will invent you a tool; see Levin[1993] for details).
By applying Bayes?
law we write P(c|f ) asP(c|f ) = P(f |c) ?
P(c)P(f )(12)By substituting (12) into (10), we can write P(c, f , v) asP(c, f , v) =P(v) ?
P(f |v) ?
P(f |c) ?
P(c)P(f )(13)51Lapata and Brew Verb Class Disambiguation Using Informative PriorsTable 2Estimation of model parameters.
(a) P?
(v) =F(v)?iF(vi)(b) P?
(f |v) = F(f , v)F(v)(c) P?
(f |c) = F(f , c)F(c)(d) P?
(c) =F(c)?iF(ci)(e) P?
(f ) =F(f )?iF(fi)(f) F(f , c) =?iF(c, f , vi)(g) F(c) =?iF(vi, c) (h) F(v, c) = F(v) ?
P(c|v)It is easy to obtain P(v) from the lemmatized BNC (see (a) in Table 2).
In order to es-timate the probability P(f |v), we need to know how many times a verb is attested witha given frame.
We acquired Levin-compatible subcategorization frames from the BNCafter performing a coarse-grained mapping between Levin?s frame descriptions andsurface syntactic patterns without preserving detailed semantic information about ar-gument structure and thematic roles.
This resulted in 80 frame types that were grosslycompatible with Levin.
We used Gsearch (Corley et al 2001), a tool that facilitates thesearch of arbitrary part-of-speech-tagged corpora for shallow syntactic patterns basedon a user-specified context-free grammar and a syntactic query.
We specified a chunkgrammar for recognizing the verbal complex, NPs, and PPs and used Gsearch to ex-tract tokens matching the frames specified in Levin.
We discarded all frames with afrequency smaller than five, as they were likely to be unreliable given our heuristicapproach.
The frame probability P(f ) (see the denominator in (13) and equation (e)in Table 2) was also estimated on the basis of the Levin-compatible subcategorizationframes that were acquired from the BNC.We cannot read off P(f |c) in (13) directly from the corpus, because the corpusis not annotated with verb classes.
Nevertheless Levin?s (1993) classification recordsthe syntactic frames that are licensed by a given verb class (for example, Give verbslicense the double object and the ?NP1 V NP2 to NP3?
frame) and also the number andtype of classes a given verb exhibits (e.g., write inhabits two classes, Performanceand Message Transfer).
Furthermore, we know how many times a given verb isattested with a certain frame in the corpus, as we have acquired Levin-compatibleframes from the BNC (see (b) in Table 2).
We first explain how we obtain F(f , c), whichwe rewrite as the sum of all occurrences of verbs v that are members of class c andare attested in the corpus with frame f (see (c) and (f) in Table 2).For monosemous verbs the count F(c, f , v) reduces to the number of times theseverbs have been attested in the corpus with a certain frame.
For polysemous verbs,we additionally need to know the class in which they were attested in the corpus.Note that we don?t necessarily need an annotated corpus for class-ambiguous verbswhose classes license distinct frames (see example (5)), provided that we have extractedverb frames relatively accurately.
For genuinely ambiguous verbs (i.e., verbs licensedby classes that take the same frame), given that we don?t have access to a corpusannotated with verb class information, we distribute the frequency of the verb and itsframe evenly across its semantic classes:F(c, f , v) =F(f , v)|classes(v, f )| (14)Here F(f , v) is the co-occurrence frequency of a verb and its frame and |classes(v, f )|is the number of classes verb v is a member of when found with frame f .
The joint52Computational Linguistics Volume 30, Number 1Table 3Estimation of F(c, f , v) and F(v, c).Give F(Give, NPVNPNP, v) F(Give, NPVNPtoNP, v) F(v,Give)feed9824023, 2634give 25, 705 7, 502 126, 894lend 343 648 2, 650rent62101, 0602pass1813256319, 4594serve 8558215, 4574frequency of a class and its frame F(f , c) is then the sum of all verbs that are mem-bers of the class c and are attested with frame f in the corpus (see (f) in Table 2).Table 3 shows the estimation of the frequency F(c, f , v) for six verbs that are mem-bers of the Give class.
Consider for example feed, which is a member of four classes:Give, Gorge, Feeding, and Fit.
Of these classes, only Feeding and Give licensethe double-object and prepositional frames.
This is why the co-occurrence frequencyof feed with these frames is divided by two.
The verb serve inhabits four classes.
Thedouble-object frame is licensed by the Give class, whereas the prepositional frameis additionally licensed by the Fulfilling class, and therefore the co-occurrence fre-quency F(NPVNPtoNP, serve) is equally distributed between these two classes.
Thisis clearly a simplification, since one would expect F(c, f , v) to vary for different verbclasses.
However, note that according to this estimation, F(f , c) will vary across framesreflecting differences in the likelihood of a class being attested with a certain frame.Both terms P(f |c) and P(c) in (13) rely on the class frequency F(c) (see (c) and (d)in Table 2).
We rewrite F(c) as the sum of all verbs attested in the corpus with class c(see (g) in Table 2).
For monosemous verbs the estimate of F(v, c) reduces to the countof the verb in the corpus.
Once again we cannot estimate F(v, c) for polysemous verbsdirectly.
The task would be straightforward if we had a corpus of verbs, each labeledexplicitly with class information.
All we have is the overall frequency of a given verbin the BNC and the number of classes it is a member of according to Levin (1993).Since polysemous verbs can generally be the realization of more than one semanticclass, counts of semantic classes can be constructed by dividing the contribution fromthe verb by the number of classes it belongs to (Resnik 1993; Lauer 1995).
We rewritethe frequency F(v, c) as shown in (h) in Table 2 and approximate P(c|v), the truedistribution of the verb and its classes, as follows:P(c|v) ?
F(v)|classes(v)| (15)Here, F(v) is the number of times the verb v was observed in the corpus and |classes(v)|is the number of classes c it belongs to.
For example, in order to estimate the frequencyof the class Give, we consider all verbs that are listed as members of this class in Levin(1993).
The class contains thirteen verbs, among which six are polysemous.
We willobtain F(Give) by taking into account the verb frequency of the monosemous verbs(|classes(v)| is one in this case) as well as distributing the frequency of the polyse-mous verbs among their classes.
For example, feed inhabits the classes Give, Gorge,53Lapata and Brew Verb Class Disambiguation Using Informative PriorsFeeding, and Fit and occurs in the corpus 3,263 times.
We will increment the countof F(Give) by 3,2634 .
Table 3 illustrates the estimation of F(v, c) for six members of theGive class.
The total frequency of the class is obtained by summing over the individualvalues of F(v, c) (see equation (g) in Table 2).The approach in (15) relies on the simplifying assumption that the frequency ofa verb is distributed evenly across its semantic classes.
This is clearly not true for allverbs.
Consider, for example, the verb rent, which inhabits classes Give (Frank rentedPeter his room) and Get (I rented my flat for my sister).
Intuitively speaking, the Givesense of rent is more frequent than the Get sense, however, this is not taken intoaccount in (15), primarily because we do not know the true distribution of the classesfor rent.
An alternative to (15) is to distribute the verb frequency unequally among verbclasses.
Even though we don?t know how likely classes are in relation to a particularverb, we can approximate how likely classes are in general on the basis of their size(i.e., number of verbs that are members of each class).
So then we can distribute averb?s frequency unequally, according to class size.
This time we approximate P(c|v)(see (h) in Table 2) by P(c|amb class), the probability of class c given the ambiguityclass3 amb class.
The latter represents the set of classes a verb might inhabit:F(v, c) ?
F(v) ?
P(c|amb class) (16)We collapse verbs into ambiguity classes in order to reduce the number of parametersthat must be estimated; we certainly lose information, but the approximation makes iteasier to get reliable estimates from limited data.
We simply approximate P(c|amb class)using a heuristic based on class size:P(c|amb class) ?
|c|?c ?
amb class|c| (17)For each class we recorded the number of its members after discarding verbswhose frequency was less than one per million in the BNC.
This gave us a first ap-proximation of the size of each class.
We then computed, for each polysemous verb,the total size of the classes of which it was a member.
We calculated P(c|amb class) bydividing the former by the latter (see equation (17)).
We obtained the class frequencyF(c) by multiplying P(c|amb class) by the observed frequency of the verb in the BNC(see equation (16)).
As an example, consider again F(Give), which is calculated bysumming over all verbs that are members of this class (see (g) in Table 2).
In orderto add the contribution of the verb feed, we need to distribute its corpus frequencyamong the classes Give, Gorge, Feed, and Fit.
The respective P(c|amb class) valuesfor these classes are 1538 ,838 ,338 , and1238 .
By multiplying these by the frequency of feed inthe BNC (3,263), we obtain the values of F(v, c) given in Table 4.
Only the frequencyF(feed,Give) is relevant for F(Give).The estimation process just described involves at least one gross simplification,since P(c|amb class) is calculated without reference to the identity of the verb in ques-tion.
For any two verbs that fall into the same set of classes, P(c|amb class) will be thesame, even though one or both may be atypical in its distribution across the classes.Furthermore, the estimation tends to favor large classes, again irrespectively of theidentity of the verb in question.
For example, the verb carry has three classes, Carry,3 Our use of ambiguity classes is inspired by a similar use in hidden Markov model?basedpart-of-speech tagging (Kupiec 1992).54Computational Linguistics Volume 30, Number 1Table 4Estimation of F(v, c) for the verb feed.c |c| P(c|amb class) F(v, c)Give 15 .39 1,272.57Gorge 8 .21 685.23Feed 3 .08 261.04Fit 12 .32 1,044.16Table 5Ten most frequent classes using equal distribution of verb frequencies.c F(c)Characterize 601,647.4Get 514,308.0Say 450,444.6Conjecture 390,618.4Future Having 369,229.3Declare 264,923.6Amuse 258,857.9Directed Motion 252,775.6Message Transfer 248,238.7Give 208,884.1Table 6Ten most frequent classes using unequal distribution of verb frequencies.c F(c)Get 453,843.6Say 447,044.2Characterize 404,734.2Conjecture 382,193.8Future Having 370,717.7Declare 285,431.7Directed Motion 255,821.6Pocket 247,392.7Amuse 205,729.4Give 197,828.8Fit, and Cost.
Intuitively speaking, the Carry class is the most frequent (e.g., Smok-ing can impair the blood which carries oxygen to the brain; I carry sugar lumps around with me).However, since the Fit class (e.g., Thameslink presently carries 20,000 passengers daily)is larger than the Carry class, it will be given a higher probability (.45 versus .4).Our estimation scheme is clearly a simplification, but it is an empirical question howmuch it matters.
Tables 5 and 6 show the ten most frequent classes as estimated us-ing (15) and (16).
We explore the contribution of the two estimation schemes for P(c)in Experiments 1 and 2.The probabilities P(f |c) and P(f |v) will be unreliable when the frequencies F(f , v)and F(f , c) are small and will be undefined when the frequencies are zero.
FollowingHindle and Rooth (1993), we smooth the observed frequencies as shown in Table 7.When F(f , v) is zero, the estimate used is proportional to the average F(f ,V)F(V) across55Lapata and Brew Verb Class Disambiguation Using Informative PriorsTable 7Smoothed estimates.
(a) P(f |v) ?F(f , v) + F(f ,V)F(V)F(v) + 1(b) F(f , V) =?iF( f , vi)(c) P(f |c) ?F(f , c) + Ff ,C)F(C)F(c) + 1(d) F(C) =?iF( f , ci)all verbs.
Similarly, when F(f , c) is zero, our estimate is proportional to the averageF(f ,C)F(C) across all classes.
We do not claim that this scheme is perfect, but any deficien-cies it may have are almost certainly masked by the effects of approximations andsimplifications elsewhere in the system.We evaluated the performance of the model on all verbs listed in Levin (1993) thatare polysemous (i.e., members of more than one class) and take frames characteristicof the widely studied dative and benefactive alternations (Pinker 1989; Boguraev andBriscoe 1989; Levin 1993; Goldberg 1995; Briscoe and Copestake 1999) and of the lesswell-known conative and possessor object alternations (see examples (1)?(4)).
All fouralternations seem fairly productive; that is, a large number of verbs undergo thesealternations, according to Levin.
A large number of classes license the frames thatare relevant for these alternations and the verbs that inhabit these classes are likelyto exhibit class ambiguity: 20 classes license the double object frame, 22 license theprepositional frame ?NP1 V NP2 to NP3,?
17 classes license the benefactive ?NP1 VNP2 for NP3?
frame, 118 (out of 200) classes license the transitive frame, and 15 classeslicense the conative ?NP1 V at NP2?
frame.In Experiment 1 we use the model to test the hypothesis that subcategorizationinformation can be used to disambiguate polysemous verbs.
In particular, we concen-trate on verbs like serve (see example (5)) that can be disambiguated solely on thebasis of their frame.
In Experiment 2 we focus on verbs that are genuinely ambiguous;that is, they inhabit a single frame and yet can be members of more than one seman-tic class (e.g., write, study, see examples (6)?(9)).
In this case, we use the probabilisticmodel to assign a probability to each class the verb inhabits.
The class with the highestprobability represents the dominant meaning for a given verb.3.
Experiment 1: Using Subcategorization to Resolve Verb Class Ambiguity3.1 MethodIn this experiment we focused solely on verbs whose meaning can be potentiallydisambiguated by taking into account their subcategorization frame.
A model thatperforms badly on this task cannot be expected to produce any meaningful results forgenuinely ambiguous verbs.We considered 128 verbs with the double-object frame (2.72 average class ambi-guity), 101 verbs with the prepositional frame ?NP1 V NP2 to NP3?
(2.59 averageclass ambiguity), 113 verbs with the frame ?NP1 V NP2 for NP3?
(2.63 average classambiguity), 42 verbs with the frame ?NP1 V at NP3?
(3.05 average class ambiguity),and 39 verbs with the transitive frame (2.28 average class ambiguity).
The task was thefollowing: Given that we know the frame of a given verb, can we predict its semanticclass?
In other words by varying the class c in the term P(c, f , v), we are trying to seewhether the class that maximizes it is the one predicted by the lexical semantics and56Computational Linguistics Volume 30, Number 1Table 8Model accuracy using equal distribution of verb frequencies for the estimation of P(c).Frame Baseline ModelNP1 V NP2 NP3 60.9% 93.8%NP1 V NP to NP3 63.3% 95.0%NP1 V NP for NP3 63.6% 98.2%NP1 V at NP2 2.4% 83.3%NP1 V NP2 43.6% 87.2%Combined 55.8% 93.9%Table 9Model accuracy using unequal distribution of verb frequencies for the estimation of P(c).Frame Baseline ModelNP1 V NP2 NP3 62.5% 93.8%NP1 V NP to NP3 67.3% 95.0%NP1 V NP for NP3 66.4% 98.2%NP1 V at NP2 2.4% 85.7%NP1 V NP2 41.0% 84.6%Combined 56.7% 93.9%the argument structure of the verb in question.
The model?s responses were evaluatedagainst Levin?s (1993) classification.
The model?s performance was considered correctif it agreed with Levin in assigning a verb to an appropriate class given a particularframe.
Recall from Section 2 that we proposed two approaches for the estimation ofthe class probability P(c).
We explore the influence of P(c) by obtaining two sets ofresults corresponding to the two estimation schemes.3.2 ResultsThe model?s accuracy is shown in Tables 8 and 9.
The results in Table 8 were ob-tained using the estimation scheme for P(c) that relies on the even distribution ofthe frequency of a verb across its semantic classes (see equation (15)).
The resultsin Table 9 were obtained using an alternative scheme that distributes verb frequencyunequally among verb classes by taking class size into account (see equation (16)).As mentioned in Section 3.1, the results were based on comparison of the model?sperformance against Levin?s (1993) classification.
We also compared the results to thebaseline of choosing the most likely class P(c) (without taking subcategorization in-formation into account).
The latter was determined on the basis of the approximationsdescribed in Section 2 (see equation (9) in Table 2, as well as equations (15), (16),and (17)).The model achieved an accuracy of 93.9% using either type of estimation for P(c).It also outperformed the baseline by 38.1% (see Table 8) and 37.2% (see Table 9).
Onemight expect an accuracy of 100%, since these verbs can be disambiguated solely on thebasis of their frame.
However, the performance of our model achieves a lesser accuracy,mainly because of the way we estimate the terms P(c) and P(f |c): We overemphasizethe importance of class information without taking into account how individual verbsdistribute across classes.
Furthermore, we rely on frame frequencies acquired from theBNC, using shallow syntactic analysis, which means that the correspondence between57Lapata and Brew Verb Class Disambiguation Using Informative PriorsLevin?s (1993) frames and our acquired frames is not one to one.
Except for the factthat our frames do not preserve much of the linguistic information detailed Levin,the number of frames acquired for a given verb can be a subset or superset of theframes available in Levin.
Note that the two estimation schemes yield comparableperformances.
This is a positive result given the importance of P(c) in the estimationof P(c, f , v).A more demanding task for our probabilistic model will be with genuinely am-biguous verbs (i.e., verbs for which the mapping between meaning and subcatego-rization is not one to one).
Although native speakers may have intuitions about thedominant interpretation for a given verb, this information is entirely absent from Levin(1993) and from the corpus on which our model is trained.
In Experiment 2 we showhow our model can be used to recover this information.4.
Experiment 2: Using Corpus Distributions to Derive Verb Class Preferences4.1 MethodWe evaluated the performance of our model on 67 genuinely ambiguous verbs, thatis, verbs that inhabit a single frame and can be members of more than one seman-tic class (e.g., write).
These verbs are listed in Levin (1993) and undergo the dative,benefactive, conative, and possessor object alternations.
As in Experiment 1, we con-sidered verbs with the double-object frame (3.27 average class ambiguity), verbs withthe frame ?NP1 V NP2 to NP3?
(2.94 average class ambiguity), verbs with the frame?NP1 V NP2 for NP3?
(2.42 average class ambiguity), verbs with the frame ?NP1 Vat NP3?
(2.71 average class ambiguity), and transitive verbs (2.77 average class am-biguity).
The model?s predictions were compared against manually annotated datathat was used only for testing purposes.
The model was trained without access to adisambiguated corpus.
More specifically, corpus tokens characteristic of the verb andframe in question were randomly sampled from the BNC and annotated with classinformation so as to derive the true distribution of the verb?s classes in a particularframe.
We describe the verb selection procedure as follows.Given the restriction that these verbs be semantically ambiguous in a specificsyntactic frame, we could not simply sample from the entire BNC, since this woulddecrease the chances of finding the verb in the frame we are interested in.
Instead,a stratified sample was used: For all class-ambiguous verbs, tokens were randomlysampled from the parsed data used for the acquisition of verb frame frequencies.
Themodel was evaluated on verbs for which a reliable sample could be obtained.
Thismeant that verbs had to have a frame frequency larger than 50.
For verbs exceedingthis threshold 100 tokens were randomly selected and annotated with verb class infor-mation.
For verbs with frame frequency less than 100 and more than 50, no samplingtook place; the entire set of tokens was manually annotated.
This selection procedureresulted in 14 verbs with the double-object frame, 16 verbs with the frame ?NP1 VNP2 to NP3,?
2 verbs with the frame ?NP1 V NP2 for NP3,?
1 verb with the frame?NP1 V at NP3,?
and 80 verbs with the transitive frame.
From the transitive verbswe further randomly selected 34 verbs; these were manually annotated and used forevaluating the model?s performance.4The selected tokens were annotated with class information by two judges, bothlinguistics graduate students.
The classes were taken from Levin (1993) and augmented4 Although the model can yield predictions for any number of verbs, evaluation could not be performedfor all 80 verbs, as to perform such evaluation, our judges would have had to annotate 8,000 corpustokens.58Computational Linguistics Volume 30, Number 1with the class Other, which was reserved for either corpus tokens that had the wrongframe or those for which the classes in question were not applicable.
The judges weregiven annotation guidelines (for each verb) but no prior training (for details on theannotation study see Lapata [2001]).
The annotation provided a gold standard forevaluating the model?s performance and enabled us to test whether humans agreeon the class annotation task.
We measured the judges?
agreement on the annotationtask using the kappa coefficient (Cohen 1960).
In general, the agreement on the classannotation task was good, with kappa values ranging from .66 to 1.00 (the mean kappawas .80, SD = .09).4.2 ResultsWe counted the performance of our model as correct if it agreed with the ?most pre-ferred,?
that is, the most frequent, verb class, as determined in the manually annotatedcorpus sample by taking the average of the responses of both judges.
As an example,consider the verb feed, which in the double-object frame is ambiguous between theclasses Feed and Give.
According to the model, Feed is the most likely class for feed.Out of 100 instances of the verb feed in the double-object frame, 61 were manuallyassigned the Feed class, 32 were assigned the Give class, and 6 were parsing mis-takes (and therefore assigned the class Other).
In this case the model?s outcome isconsidered correct given that the corpus tokens also reveal a preference for the Feed(i.e., the Feed instances outnumber the Give ones).As in Experiment 1, we explored the influence of the parameter P(c) on the model?sperformance by obtaining two sets of results corresponding to the two estimationschemes discussed in Section 2.
The model?s accuracy is shown in Tables 10 and 11.The results in Table 10 were obtained using the estimation scheme for P(c) thatrelies on the even distribution of a verb?s frequency across its semantic classes (seeTable 10Model accuracy using equal distribution of verb frequencies for the estimation of P(c).Frame Baseline ModelNP1 V NP2 NP3 50.0% 78.6%NP1 V NP to NP3 43.8% 68.8%NP1 V NP for NP3 00.0% 100.0%NP1 V at NP2 100.0% 100.0%NP1 V NP2 47.1% 73.5%Combined 46.2% 74.6%Table 11Model accuracy using unequal distribution of verb frequencies for the estimation of P(c).Frame Baseline ModelNP1 V NP2 NP3 50.0% 78.6%NP1 V NP to NP3 43.8% 75.0%NP1 V NP for NP3 00.0% 100.0%NP1 V at NP2 100.0% 100.0%NP1 V NP2 47.1% 67.6%Combined 46.2% 73.1%59Lapata and Brew Verb Class Disambiguation Using Informative PriorsTable 12Semantic preferences for verbs with the double-object frame.Verb Class Kcall?Dub Get Other93 -7.59 3 -8.12 4 .82cook Build Prepare Other28 -11.68 33 -11.50 1 1.00declare?Declare Ref.
Appear.
Other35 -10.51 18 -12.18 5 .89feed?Feed Give Other61 -10.63 32 -12.16 6 .73find?Declare Get Other36 -7.69 47 -7.43 17 .70leave?Get Fulfill F. Have Other6 -7.91 14 -10.40 56 ?7.66 23 .67make?Build Dub Other21 -7.25 66 -6.13 13 .79pass?Give Send Throw Other81 -8.84 0 -8.96 0 ?9.98 19 .93save?Bill Get Other24 -9.74 62 -9.59 14 .74shoot Throw Get Other91 -10.94 0 -9.99 5 1.00take Bring-Take Perform Other15 -7.02 40 -7.38 45 .77write?Msg.
Trans.
Perform Other54 -8.79 19 -9.05 18 .85equation (15)).
The results in Table 11 were obtained using a scheme that distributesverb frequency unequally among verb classes by taking class size into account (seeequation (16)).
As in Experiment 1, the results were compared to a simple baseline thatdefaults to the most likely class without taking verb frame information into account(see equation (g) in Table 2 as well as equations (15), (16), and (17)).The model achieved an accuracy of 74.6% using the estimation scheme of equaldistribution and a accuracy of 73.1% using the estimation scheme of unequal distribu-tion.
The difference between the two estimation schemes is not statistically significant( ?2(67) = 2.17, p = .84).
Table 12 gives the distribution of classes for 12 polysemousverbs taking the double-object frame as obtained from the manual annotation of corpustokens together with interannotator agreement (?).
We also give the (log-transformed)probabilities of these classes as derived by the model.5 The presence of the symbol?indicates that the model?s class preference for a given verb agrees with its distributionin the corpus.
The absence of?indicates disagreement.
For the comparison shown inTable 12, model class preferences were derived using the equal-distribution estimationscheme for P(c) (see equation (15)).As shown in Table 12, the model?s predictions are generally borne out in the corpusdata.
Misclassifications are due mainly to the fact that the model does not take verbclass dependencies into account.
Consider, for example, the verb cook.
According to themodel the most likely class for cook is Build.
Although it may generally be the casethat Build verbs (e.g., make, assemble, build) are more frequent than Prepare verbs5 No probabilities are given for the Other class; this is not a Levin class, however, it was used by theannotators, mainly to indicate parsing errors.60Computational Linguistics Volume 30, Number 1(e.g., bake, roast, boil), the situation is reversed for cook.
The same is true for the verbshoot, which when attested in the double-object frame is more likely to be a Throwverb (Jamie shot Mary a glance) rather than a Get verb (I will shoot you two birds).Notice that our model is not context sensitive; that is, it does not derive classrankings tailored to specific verbs, primarily because this information is not readilyavailable in the corpus, as explained in Section 2.
However, we have effectively built aprior model of the joint distribution of verbs, their classes, and their syntactic framesthat can be useful for disambiguating polysemous verbs in context.
We describe ourclass disambiguation experiments as follows.5.
Class DisambiguationIn the previous sections we focused on deriving a model of the distribution of Levinclasses without relying on annotated data and showed that this model infers the rightclass for genuinely ambiguous verbs 74.6% of the time without taking the local contextof their occurrence into account.
An obvious question is whether this information isuseful for disambiguating tokens rather than types.
In the following we report on adisambiguation experiment that takes advantage of this prior information.Word sense disambiguation is often cast as a problem in supervised learning,where a disambiguator is induced from a corpus of manually sense-tagged text.
Thecontext within which the ambiguous word occurs is typically represented by a setof linguistically motivated features from which a learning algorithm induces a repre-sentative model that performs the disambiguation.
A variety of classifiers have beenemployed for this task (see Mooney [1996] and Ide and Veronis [1998] for overviews),the most popular being decision lists (Yarowsky 1994, 1995) and naive Bayesian classi-fiers (Pedersen 2000; Ng 1997; Pedersen and Bruce 1998; Mooney 1996; Cucerzan andYarowsky 2002).
We employed a naive Bayesian classifier (Duda and Hart 1973) forour experiments, as it is a very convenient framework for incorporating prior knowl-edge and studying its influence on the classification task.
In Section 5.1 we describea basic naive Bayesian classifier and show how it can be extended with informativepriors.
In Section 5.2 we discuss the types of contextual features we use.
We report onour experimental results in Section 6.5.1 Naive Bayes ClassificationA naive Bayesian classifier assumes that all the feature variables representing a prob-lem are conditionally independent, given the value of the classification variable.
Inword sense disambiguation, the features (a1, a2, .
.
.
, an) represent the context surround-ing the ambiguous word, and the classification variable c is the sense (Levin class inour case) of the ambiguous word in this particular context.
Within a naive Bayesapproach, the probability of the class c given its context can be expressed asP(c|ai) =P(c)n?i=1P(ai|c)P(ai)(18)where P(ai|c) is the probability that a test example is of class c given the contextualfeatures ai.
Since the denominator P(ai) is constant for all classes c, the problem reducesto finding the class c with the maximum value for the numerator:P(c|ai) ?
P(c)n?i=1P(ai|c) (19)61Lapata and Brew Verb Class Disambiguation Using Informative PriorsIf we choose the prior P(c) to be uniform (P(c) = 1|c| for all c ?
C), (19) can befurther simplified toP(c|a) ?n?i=1P(ai|c) (20)Assuming a uniform prior, a basic naive Bayesian classifier is as follows:?
c =n?i=1P(ai|c) (21)Note, however, that we developed in the previous section two types of non-uniform prior models.
The first model derives P(c) heuristically from the BNC, ig-noring the identity of the polysemous verb and its subcategorization profile, and thesecond model estimates the class distribution P(c, v, f ) by taking the frame distribu-tion into account.
So, the naive Bayesian classifier in (21) can be extended with anonuniform prior:?
c = P(c)n?i=1P(ai|c) (22)?
c = P(c, v, f )n?i=1P(ai|c, f , v) (23)where P(c) is estimated as shown in (d)?
(g) in Table 2 and P(c, v, f ), the prior foreach class c corresponding to verb v in frame f , is estimated as explained in Sec-tion 2 (see (13)).
As before, ai are the contextual features.
The probabilities P(ai|c)and P(ai|c, f , v) can be estimated from the training data simply by counting the co-occurrence of feature ai with class c (for (22)) or the co-occurrence of ai with classc, verb v, and frame f (for (23)).
For features that have zero counts, we use add-ksmoothing (Johnson 1932), where k is a number less than one.5.2 Feature SpaceAs is common in word sense disambiguation studies, we experimented with two typesof context representations, collocations and co-occurrences.
Co-occurrences simply in-dicate whether a given word occurs within some number of words to the left or rightof an ambiguous word.
In this case the contextual features are binary and representthe presence or absence of a particular word in the current or preceding sentence.
Weused four types of context in our experiments: left context (i.e., words occurring tothe left of the ambiguous word), right context (i.e., words occurring to the right of theambiguous word), the current sentence (i.e., words surrounding the ambiguous word),and the current sentence together with its immediately preceding sentence.
Punctua-tion and capitalization were removed from the windows of context; noncontent wordswere included.
The context words were represented as lemmas or parts of speech.Collocations are words that are frequently adjacent to the word to be disam-biguated.
We considered 12 types of collocations.
Examples of collocations for theverb write are illustrated in Table 13.
The L columns in the table indicate the numberof words to the left of the ambiguous words, and the R columns, the number of wordsto the right.
So for example, the collocation 1L3R represents one word to the left andthree words to the right of the ambiguous word.
Collocations again were representedas lemmas (see Table 13) or parts of speech.62Computational Linguistics Volume 30, Number 1Table 13Features for collocations.L R Example L R Example0 1 write you 1 1 can write you1 0 can write 1 2 can write you a0 2 write you a 2 1 I can write you2 0 I can write 1 3 can write you a story0 3 write you a story 3 1 perhaps I can write you3 0 perhaps I can write 2 4 I can write you a story sunshine6.
Experiment 3: Disambiguating Polysemous Verbs6.1 MethodWe tested the performance of our naive Bayesian classifiers on the 67 genuinely am-biguous verbs on which the prior models were tested.
Recall that these models weretrained without access to a disambiguated corpus, which was used only to determinefor a given verb and its frame its most likely meaning overall (i.e., across the corpus)instead of focusing on the meaning of individual corpus tokens.
The same corpuswas used for the disambiguation of individual tokens, excluding tokens assigned theclass Other.
The naive Bayes classifiers were trained and tested using 10-fold cross-validation on a set of 5,002 examples.
These were representative of the frames ?NP1V NP2,?
?NP1 V NP2 NP3,?
?NP1 V NP2 to NP3,?
and ?NP1 V NP2 for NP3.?
Theframe ?NP1 V at NP2?
was excluded from our disambiguation experiments as it wasrepresented solely by the verb kick (50 instances).In this study we compare a naive Bayesian classifier that relies on a uniformprior (see (20)) against two classifiers that make use of nonuniform prior models:The classifier in (22) effectively uses as prior the baseline model P(c) from Section 2,whereas the classifier in (23) relies on the more informative model P(c, f , v).
As abaseline for the disambiguation task, we simply assign the most common class in thetraining data to every instance in the test data, ignoring context and any form of priorinformation (Pedersen 2001; Gale, Church, and Yarowsky 1992a).
We also report anupper bound on disambiguation performance by measuring how well human judgesagree with one another (percentage agreement) on the class assignment task.
Recallfrom Section 4.1 that our corpus was annotated by two judges with Levin-compatibleverb classes.6.2 ResultsThe results of our class disambiguation experiments are summarized in Figures 2?5.In order to investigate differences among different frames, we show how the naiveBayesian classifiers perform for each frame individually.
Figures 2?5 (x-axis) also re-veal the influence of collocational features of different sizes (see Table 13) on theclassification task.
Panel (b) in the figures presents the classifiers?
accuracy when thecollocational features are encoded as lemmas; in panel (c) of the figures, the contextis represented as parts of speech, whereas in panel (a) of the figures, the context isrepresented by both lemmas and parts of speech.As can be seen in the figures, the naive Bayesian classifier with our informa-tive prior (P(c, f , v), IPrior in Figures 2?5) generally outperforms the baseline prior(P(c), BPrior in Figures 2?5), the uniform prior (UPrior in Figures 2?5), and the base-line (Baseline in Figures 2?5) for all frames.
Good performances are attained with63Lapata and Brew Verb Class Disambiguation Using Informative Priors(a) (b) (c)Figure 2Word sense disambiguation accuracy for ?NP1 V NP2?
frame.
(a) (b) (c)Figure 3Word sense disambiguation accuracy for ?NP1 V NP2 NP3?
frame.
(a) (b) (c)Figure 4Word sense disambiguation accuracy for ?NP1 V NP2 to NP3?
frame.lemmas, parts of speech, and combination of the two.
The naive Bayesian classifier(IPrior) reaches the upper bound (UpBound in Figures 2?5) for the ditransitive frames?NP1 V NP2 NP3,?
?NP1 V NP2 to NP3,?
and ?NP1 V NP2 for NP3.
?The best accuracy (87.8%) for the transitive frame is achieved with the collocationalfeatures 0L2R, 1L2R, and 1L3R (see Figures 2(a)?(c)).
For the double-object frame, thehighest accuracy (90.8%) is obtained with features 0LR1 and 0L3R (see Figures 3(b)and 3(c)).
Similarly, for the ditransitive ?NP1 V NP2 to NP3?
frame, the features 0L3Rand 0L1R yield the best accuracies (88.8%, see Figures 4(a)?(c)).
Finally, for the ?NP1 VNP2 for NP3?
frame, accuracy (94.4%) is generally good for most features when aninformative prior is used.
In fact, neither the uniform prior nor the baseline P(c)outperforms the baseline for this frame.64Computational Linguistics Volume 30, Number 1(a) (b) (c)Figure 5Word sense disambiguation accuracy for ?NP1 V NP2 for NP3?
frame.The context encoding (lemmas versus parts of speech) does not seem to have agreat influence on the disambiguation performance.
Good accuracies are obtained witheither parts of speech or lemmas; combination of the two does not yield better results.The classifier with the informative prior P(c, f , v) outperforms the baseline priorP(c) and the uniform prior also when co-occurrences are used.
However, the co-occurrences never outperform the collocational features, for all four types of context.The classifiers (regardless of the type of prior being used) never beat the baseline forthe frames ?NP1 V NP2?
and ?NP1 V NP2 to NP3?.
Accuracies above the baselineare achieved for the frames ?NP1 V NP2 NP3?
and ?NP1 V NP2 for NP3?
whenan informative prior is used.
Detailed results are summarized in the Appendix.
Co-occurrences and windows of large sizes traditionally work well for topical distinctions(Gale, Church, and Yarowsky 1992b).
Levin classes, however, typically capture differ-ences in argument structure, that is, the types of objects or subjects that verbs selectfor.
Argument structure is approximated by our collocational features.
For example, averb often taking a reflexive pronoun as its object is more likely to be a ReflexiveVerb of Appearance than a verb that never subcategorizes for a reflexive object.There is not enough variability among the wider contexts surrounding a polysemousverb to inform the class-disambiguation task, as the Levin classes often do not crosstopical boundaries.7.
DiscussionIn this article, we have presented a probabilistic model of verb class ambiguity basedon Levin?s (1993) semantic classification.
Our results show that subcategorization in-formation acquired automatically from corpora provides important cues for verb classdisambiguation (Experiment 1).
In the absence of subcategorization cues, corpus-baseddistributions and quantitative approximations of linguistic concepts can be used to de-rive a preference ordering on the set of verbal meanings (Experiment 2).
The semanticpreferences that we have generated can be thought of as default semantic knowledge,to be used in the absence of any explicit contextual or lexical semantic information tothe contrary (see Table 12).
We have also shown that these preferences are useful fordisambiguating polysemous verbs within their local contexts of occurrence (Experi-ment 3).The approach is promising in that it achieves satisfactory results with a simplemodel that has a straightforward interpretation in a Bayesian framework and does not65Lapata and Brew Verb Class Disambiguation Using Informative Priorsrely on the availability of annotated data.
The model?s parameters are estimated usingsimple distributions that can be extracted easily from corpora.
Our model achieved anaccuracy of 93.9% (over a baseline of 56.7%) on the class disambiguation task (Exper-iment 1) and an accuracy of 74.6% (over a baseline of 46.2%) on the task of derivingdominant verb classes (Experiment 2).
Our disambiguation experiments reveal thatthis default semantic knowledge, when incorporated as a prior in a naive Bayes classi-fier, outperforms the uniform prior and the baseline of always defaulting to the mostfrequent class (Experiment 3).
In fact, for three out of the four frames under study,our classifier with the informative prior achieved upper-bound performance.Although our results are promising, it remains to be shown that they generalizeacross frames and alternations.
Four types of alternations were investigated in thisstudy.
However, Levin lists 79 alternations and approximately 200 classes.
Althoughdistributions for different class/frame combinations can easily be derived automati-cally, it remains to be shown that these distributions are useful for all verbs, frames,and classes.
Also note that the models described in the previous sections crucially relyon the acquisition of relatively accurate frames from the corpus.
It is a matter of futurework to examine how the quality of the acquired frames influences the disambiguationtask.
Also, the assumption that the semantic class determines the subcategorizationpatterns of its class members independently of their identity may not be harmless forall classes and frames.Although our original aim was to develop a probabilistic framework that exploitsLevin?s (1993) linguistic classification and the systematic correspondence between syn-tax and semantics, a limitation of the model is that it cannot infer class information forverbs not listed in Levin.
For these verbs, P(c), and hence P(c, f , v), will be zero.
Recentwork in computational linguistics (e.g., Schu?tze 1998) and cognitive psychology (e.g.,Landauer and Dumais 1997) has shown that large corpora implicitly contain semanticinformation, which can be extracted and manipulated in the form of co-occurrencevectors.
One possible approach would be to compute the centroid (geometric mean)of the vectors of all members of a semantic class.
Given an unknown verb (i.e., a verbnot listed in Levin), we can decide its semantic class by comparing its semantic vectorto the centroids of all semantic classes.
For example, we could determine class mem-bership on the basis of the distance to the closest centroid representing a semanticclass (see Patel, Billinaria, and Levy [1998] for a proposal similar in spirit).
Anotherapproach put forward by Dorr and Jones (1996) utilizes WordNet (Miller and Charles1991) to find similarities (via synonymy) between unknown verbs and verbs listed inLevin.
Once we have chosen a class for an unknown verb, we are entitled to assumethat it will share the broad syntactic and semantic properties of that class.8.
Related WorkLevin?s (1993) seminal study on diathesis alternations and verb semantic classes hasrecently influenced work in dictionary creation (Dorr 1997; Dang et al 1998; Dorrand Jones 1996) and notably lexicon acquisition on the basis of the assumption thatverbal meaning can be gleaned from corpora using cues pertaining to syntactic struc-ture (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000).Previous work in word sense disambiguation has not tackled explicitly the ambiguityproblems arising from Levin?s classification, although methods for deriving informa-tive priors in an unsupervised manner have been proposed by Ciaramita and Johnson(2000) and Chao and Dyer (2000) within the context of noun and adjective sense dis-ambiguation, respectively.
In this section we review related work on classification andlexicon acquisition and compare it to our own work.66Computational Linguistics Volume 30, Number 1Dang et al (1998) observe that verbs in Levin?s (1993) database are listed in morethan one class.
The precise meaning of this ambiguity is left open to interpretationin Levin, as it may indicate that the verb has more than one sense or that one sense(i.e., class) is primary and the alternations for this class should take precedence overthe alternations for the other classes for which the verb is listed.
Dang et al augmentLevin?s semantic classes with a set of ?intersective?
classes that are created by groupingtogether sets of existing classes that share a minimum of three overlapping members.Intersective classes are more fine-grained than the original Levin classes and exhibitmore-coherent sets of syntactic frames and associated semantic components.
Dang etal.
further argue that intersective classes are more compatible with WordNet than thebroader Levin classes and thus make it possible to attribute the semantic componentsand associated sets of syntactic frames to specific WordNet senses as well, therebyenriching the WordNet representation and providing explicit criteria for word sensedisambiguation.Most statistical approaches, including ours, treat verbal-meaning assignment as asemantic classification task.
The underlying question is the following: How can corpusinformation be exploited in deriving the semantic class for a given verb?
Despite theunifying theme of using corpora and corpus distributions for the acquisition task, theapproaches differ in the inventory of classes they employ, in the methodology usedfor inferring semantic classes, and in the specific assumptions concerning the verbs tobe classified (e.g., can they be polysemous or not).Merlo and Stevenson (2001) use grammatical features (acquired from corpora) toclassify verbs into three semantic classes: unergative, unaccusative, and object drop.These classes are abstractions of Levin?s (1993) classes and as a result yield a coarserclassification.
For example, object-drop verbs comprise a variety of Levin classes suchas Gesture verbs, Caring verbs, Load verbs, Push-Pull verbs, Meet verbs, So-cial Interaction verbs, andAmuse verbs.
Unergative, unaccusative, and object-dropverbs have identical subcategorization patterns (i.e., they alternate between the tran-sitive and intransitive frame), yet distinct argument structures, and therefore differ inthe thematic roles they assign to their arguments.
For example, when attested in theintransitive frame, the subject of an object-drop verb is an agent, whereas the subjectof an unaccusative verb is a theme.
Under the assumption that differences in thematicrole assignment uniquely identify semantic classes, numeric approximations of argu-ment structure are derived from corpora and used in a machine-learning paradigm toplace verbs in their semantic classes.
The approach is evaluated on 59 verbs manuallyselected from Levin (20 unergatives, 20 object drops, and 19 unaccusatives).
It is as-sumed that these verbs are monosemous, that is, they can be ergative, unergative, orobject drop.
A decision-tree learner achieves an accuracy of 69.8% on the classificationtask over a chance baseline of 34%.Schulte im Walde (2000) uses subcategorization information and selectional re-strictions to cluster verbs into Levin (1993)?compatible semantic classes.
Subcatego-rization frames are induced from the BNC using a robust statistical parser (Carroll andRooth 1998).
The selectional restrictions are acquired using Resnik?s (1993) information-theoretic measure of selectional association, which combines distributional and taxo-nomic information (e.g., WordNet) to formalize how well a predicate associates witha given argument.
Two sets of experiments are run to evaluate the contribution of se-lectional restrictions using two types of clustering algorithms: iterative clustering andlatent-class clustering (see Schulte im Walde [2000] for details).
The approach is evalu-ated on 153 verbs taken from Levin, 53 of which are polysemous (i.e., belong to morethan one class).
The size of the derived clusters is restricted to four verbs and comparedto Levin: Verbs are classified correctly if they are members of a nonsingleton cluster67Lapata and Brew Verb Class Disambiguation Using Informative Priorsthat is a subset of a Levin class.
Polysemous verbs can be assigned to distinct clustersonly using the latent-class clustering method.
The best results achieve a recall of 36%and a precision of 61% (over a baseline of 5%, calculated as the number of randomlycreated clusters that are subsets of a Levin class) using subcategorization informationonly and iterative clustering.
Inclusion of information about selectional restrictionsyields a lower accuracy of 38% (with a recall of 20%), again using iterative clustering.Dorr and Jones (1996) use Levin?s (1993) classification to show that there is a pre-dictable relationship between verbal meaning and syntactic behavior.
They create adatabase of Levin verb classes and the sentences exemplifying them (including bothpositive and negative examples, i.e., examples marked with asterisks).
A parser is usedto extract basic syntactic patterns for each semantic class.
These patterns form the syn-tactic signature of the class.
Dorr and Jones show that 97.9% of the semantic classes canbe identified uniquely by their syntactic signatures.
Grouping verbs (instead of classes)with identical signatures to form a semantic class yields a 6.3% overlap with Levinclasses.
Dorr and Jones?s results are somewhat difficult to interpret, since in practiceinformation about a verb and its syntactic signature is not available, and it is pre-cisely this information that is crucial for classifying verbs into Levin classes.
Schulteim Walde?s study and our own study show that acquisition of syntactic signatures(i.e., subcategorization frames) from corpora is feasible; however, these acquired sig-natures are not necessarily compatible with Levin and in most cases will depart fromthose derived by Dorr and Jones, as negative examples are not available in real corpora.Ciaramita and Johnson (2000) propose an unsupervised Bayesian model for dis-ambiguating verbal objects that uses WordNet?s inventory of senses.
For each verbthe model creates a Bayesian network whose architecture is determined by WordNet?shierarchy and whose parameters are estimated from a list of verb-object pairs found ina corpus.
A common problem for unsupervised models trained on verb-object tuplesis that the objects can belong to more than one semantic class.
The class ambiguityproblem is commonly resolved by considering each observation of an object as evi-dence for each of the classes the word belongs to.
The formalization of the problem interms of Bayesian networks allows the contribution of different senses to be weightedvia explaining away (Pearl 1988): If A is a hyponym of B and C is a hyponym of B,and B is true, then finding that C is true makes A less likely.Prior knowledge about the likelihoods of concepts is hand coded in the networkaccording to the following principles: (1) It is unlikely that any given class will bea priori selected for; (2) if a class is selected, then its hyponyms are also likely to beselected; (3) a word is likely as the object of a verb, if at least one of its classes is selectedfor.
Likely and unlikely here correspond to numbers that sum up to to one.
Ciaramitaand Johnson show that their model outperforms other word sense disambiguationapproaches that do not make use of prior knowledge.Chao and Dyer (2000) propose a method for the disambiguation of polysemousadjectives in adjective-noun combinations that also uses Bayesian networks and Word-Net?s taxonomic information.
Prior knowledge about the likelihood of different sensesor semantic classes is derived heuristically by submitting queries (e.g., great hurricane)to the AltaVista search engine and extrapolating from the number of returned doc-uments the frequency of the adjective-noun pair (see Mihalcea and Moldovan [1998]for details of this technique).
For each polysemous adjective-noun combination, thesynonyms representative of each sense are retrieved from WordNet (e.g., {great, large,big} vs. {great, neat, good}).
Queries are submitted to AltaVista for each synonym-nounpair; the number of documents returned is used then as an estimate of how likelythe different adjective senses are.
Chao and Dyer obtain better results when priorknowledge is factored into their Bayesian network.68Computational Linguistics Volume 30, Number 1Our work focuses on the ambiguity inherently present in Levin?s (1993) classifi-cation.
The problem is ignored by Merlo and Stevenson (2001), who focus only onmonosemous verbs.
Polysemous verbs are included in Schulte im Walde?s (2000) ex-periments: The clustering approach can go so far as to identify more than one classfor a given verb without, however, providing information about its dominant class.We recast Levin?s classification in a statistical framework and show in agreement withMerlo and Stevenson and Schulte im Walde that corpus-based distributions provideimportant information for semantic classification, especially in the case of polysemousverbs whose meaning cannot be easily inferred from the immediate surrounding con-text (i.e., subcategorization).
We additionally show that the derived model is usefulnot only for determining the most likely overall class for a given verb (i.e., across thecorpus), but also for disambiguating polysemous verb tokens in context.Like Schulte im Walde (2000), our approach relies on subcategorization framesextracted from the BNC (although using a different methodology).
We employ Levin?sinventory of semantic classes, arriving at a finer-grained classification than Merlo andStevenson (2001).
In contrast to Schulte im Walde, we do not attempt to discover Levinclasses from corpora; instead, we exploit Levin?s classification and corpus frequenciesin order to derive a distribution of verbs, classes, and their frames that is not known apriori but is approximated using simplifications.
Our approach is not particularly tiedto Levin?s exact classification.
We have presented in this article a general frameworkthat could be extended to related classifications such as the semantic hierarchy pro-posed by Dang et al (1998).
In fact the latter may be more appropriate than Levin?soriginal classification for our disambiguation experiments, as it is based on a tightercorrespondence between syntactic frames and semantic components and contains linksto the WordNet taxonomy.Prior knowledge with regard to the likelihood of polysemous verb classes is ac-quired automatically in an unsupervised manner by combining corpus frequenciesestimated from the BNC and information inherent in Levin.
The models proposed byChao and Dyer (2000) and Ciaramita and Johnson (2000) are not directly applicableto Levin?s classification, as the latter is not a hierarchy (and therefore not a DAG) andcannot be straightforwardly mapped into a Bayesian network.
However, in agreementwith Chao and Dyer and Ciaramita and Johnson, we show that prior knowledge aboutclass preferences improves word sense disambiguation performance.Unlike Schulte im Walde (2000) and Merlo and Stevenson (2001), we ignore infor-mation about the arguments of a given verb in the form of either selectional restric-tions or argument structure while building our prior models.
The latter information is,however, indirectly taken into account in our disambiguation experiments: The verbs?arguments are features for our naive Bayesian classifiers.
Such information can be alsoincorporated into the prior model in the form of conditional probabilities, where theverb is, for example, conditioned on the thematic role of its arguments if this is known(see Gildea and Jurafsky [2000] for a method that automatically labels thematic roles).Unlike Stevenson and Merlo, Schulte im Walde, and Dorr and Jones (1996), we pro-vide a general probabilistic model that assigns a probability to each class of a givenverb by calculating the probability of a complex expression in terms of the probabilityof simpler expressions that compose it.
We further show that this model is useful fordisambiguating polysemous verbs in context.Appendix: Disambiguation Results with Co-occurrencesFigures 6?9 show the performances of our naive Bayesian classifier when co-occurrencesare used as features.
We experimented with four types of context: left context (Left),69Lapata and Brew Verb Class Disambiguation Using Informative Priorsright context (Right), sentential context (Sentence), and the sentence within whichthe ambiguous verb is found together with its immediately preceding sentence(PSentence).
The context was encoded as lemmas or parts of speech.
(a) (b)Figure 6Word sense disambiguation accuracy for ?NP1 V NP2?
frame.
(a) (b)Figure 7Word sense disambiguation accuracy for ?NP1 V NP2 NP3?
frame.70Computational Linguistics Volume 30, Number 1(a) (b)Figure 8Word sense disambiguation accuracy for ?NP1 V to NP2 NP3?
frame.
(a) (b)Figure 9Word sense disambiguation accuracy for ?NP1 V for NP2 NP3?
frame.71Lapata and Brew Verb Class Disambiguation Using Informative PriorsAcknowledgmentsMirella Lapata was supported by ESRCgrant number R000237772.
Thanks to FrankKeller, Alex Lascarides, Katja Markert, PaolaMerlo, Sabine Schulte im Walde, StaceyBailey, Markus Dickinson, Anna Feldman,Anton Rytting, and two anonymousreviewers for valuable comments.ReferencesBoguraev, Branimir K. and Ted Briscoe.
1989.Utilising the LDOCE grammar codes.
InTed Briscoe and Branimir K. Boguraev,editors, Computational Lexicography forNatural Language Processing.
Longman,London, pages 85?116.Briscoe, Ted and John Carroll.
1997.Automatic extraction of subcategorizationfrom corpora.
In Proceedings of the FifthConference on Applied Natural LanguageProcessing, pages 356?363, Washington,DC.Briscoe, Ted and Ann Copestake.
1999.Lexical rules in constraint-based grammar.Computational Linguistics, 25(4):487?526.Burnard, Lou, 1995.
The Users Reference Guidefor the British National Corpus.
BritishNational Corpus Consortium, OxfordUniversity Computing Service.Carroll, Glenn and Mats Rooth.
1998.Valence induction with a head-lexicalizedPCFG.
In Nancy Ide and Atro Voutilainen,editors, Proceedings of the Third Conference onEmpirical Methods in Natural LanguageProcessing, pages 36?45, Granada, Spain.Chao, Gerald and Michael G. Dyer.
2000.Word sense disambiguation of adjectivesusing probabilistic networks.
InProceedings of the 18th InternationalConference on Computational Linguistics,pages 152?158, Saarbru?cken, Germany.Ciaramita, Massimiliano and Mark Johnson.2000.
Explaining away ambiguity:Learning verb selectional restrictions withBayesian networks.
In Proceedings of the18th International Conference onComputational Linguistics, pages 187?193,Saarbru?cken, Germany.Cohen, J.
1960.
A coefficient of agreement fornominal scales.
Educational andPsychological Measurement, 20:37?46.Corley, Steffan, Martin Corley, Frank Keller,Matthew W. Crocker, and Shari Trewin.2001.
Finding syntactic structure inunparsed corpora: The Gsearch corpusquery system.
Computers and theHumanities, 35(2):81?94.Cucerzan, Silviu and David Yarowsky.
2002.Augmented mixture models for lexicaldisambiguation.
In Jan Hajic?
and YujiMatsumoto, editors, Proceedings of theConference on Empirical Methods in NaturalLanguage Processing, pages 33?40,Philadelphia, PA.Dang, Hoa Trang, Karin Kipper, MarthaPalmer, and Joseph Rosenzweig.
1998.Investigating regular sense extensionsbased on intersective Levin classes.
InProceedings of the 17th InternationalConference on Computational Linguistics and36th Annual Meeting of the Association forComputational Linguistics, pages 293?299,Montre?al, Que?bec, Canada.Dang, Hoa Trang, Joseph Rosenzweig, andMartha Palmer.
1997.
Associatingsemantic components with intersectiveLevin classes.
In Proceedings of the FirstAMTA SIG-IL Workshop on Interlinguas,pages 1?8, San Diego, CA.Dorr, Bonnie J.
1997.
Large-scale dictionaryconstruction for foreign language tutoringand interlingual machine translation.Machine Translation, 12(4):371?322.Dorr, Bonnie J. and Doug Jones.
1996.
Roleof word sense disambiguation in lexicalacquisition: Predicting semantics fromsyntactic cues.
In Proceedings of the 16thInternational Conference on ComputationalLinguistics, pages 322?327, Copenhagen,Denmark.Duda, Richard O. and Peter E. Hart.
1973.Pattern Classification and Scene Analysis.Wiley, New York.Fillmore, Charles.
1965.
Indirect ObjectConstructions and the Ordering ofTransformations.
Mouton, The Hague.Gale, William, Kenneth W. Church, andDavid Yarowsky.
1992a.
Estimating upperand lower bounds on the performance ofword-sense disambiguation programs.
InProceedings of the 30th Annual Meeting of theAssociation for Computational Linguistics,pages 249?256, Columbus, OH.Gale, William A., Kenneth W. Church, andDavid Yarowsky.
1992b.
A method fordisambiguating word senses in a largecorpus.
Computers and the Humanities,26(5?6):415?439.Gildea, Daniel and Daniel Jurafsky.
2000.Automatic labelling of semantic roles.
InProceedings of the 38th Annual Meeting of theAssociation for Computational Linguistics,Hong Kong.Goldberg, Adele.
1995.
Constructions.University of Chicago Press, Chicago.Green, Georgia.
1974.
Semantics and SyntacticRegularity.
Indiana University Press,Bloomington.Gropen, Jess, Steven Pinker,Michelle Hollander, Richard M. Goldberg,and Ronald Wilson.
1989.
The learnabilityand acquisition of the dative alternation.72Computational Linguistics Volume 30, Number 1Language, 65(2):203?257.Hindle, Donald and Mats Rooth.
1993.Structural ambiguity and lexical relations.Computational Linguistics, 19(1):103?120.Ide, Nancy and Jean Ve?ronis.
1998.Introduction to the special issue on wordsense disambiguation: The state of the art.Computational Linguistics, 24(1):1?40.Jackendoff, Ray.
1983.
Semantics andCognition.
MIT Press, Cambridge, MA.Johnson, William E. 1932.
Probability: Thedeductive and inductive problems.
Mind,49:409?423.Kipper, Karin, Hoa Trang Dang, and MarthaPalmer.
2000.
Class-based construction of averb lexicon.
In Proceedings of the 17thNational Conference on Artificial Intelligence,pages 691?696, Austin, TX.Klavans, Judith and Min-Yen Kan. 1998.
Roleof verbs in document analysis.
InProceedings of the 17th InternationalConference on Computational Linguistics and36th Annual Meeting of the Association forComputational Linguistics, pages 680?688,Montre?al, Que?bec, Canada.Kupiec, Julian.
1992.
Robust part-of-speechtagging using a hidden Markov model.Computer Speech and Language, 6(3):225?242.Landauer, Thomas K. and Susan T. Dumais.1997.
A solution to Plato?s problem: Thelatent semantic analysis theory ofacquisition, induction and representationof knowledge.
Psychological Review,104(2):211?240.Lapata, Maria.
1999.
Acquiring lexicalgeneralizations from corpora: A case studyfor diathesis alternations.
In Proceedings ofthe 37th Annual Meeting of the Association forComputational Linguistics, pages 397?404,College Park, MD.Lapata, Maria.
2001.
The Acquisition andModeling of Lexical Knowledge: ACorpus-Based Investigation of SystematicPolysemy.
Ph.D. thesis, University ofEdinburgh.Lauer, Mark.
1995.
Designing StatisticalLanguage Learners: Experiments on CompoundNouns.
Ph.D. thesis, Macquarie University,Sydney, Australia.Levin, Beth.
1993.
English Verb Classes andAlternations: A Preliminary Investigation.University of Chicago Press, Chicago.Levow, Gina-Anne, Bonnie Dorr, andDekang Lin.
2000.
Construction ofChinese-English semantic hierarchy forinformation retrieval.
Technical report,University of Maryland, College Park.McCarthy, Diana.
2000.
Using semanticpreferences to identify verbal participationin role switching alternations.
InProceedings of the First North AmericanAnnual Meeting of the Association forComputational Linguistics, pages 256?263,Seattle, WA.Merlo, Paola and Susanne Stevenson.
2001.Automatic verb classification based onstatistical distribution of argumentstructure.
Computational Linguistics,27(3):373?408.Mihalcea, Rada and Dan Moldovan.
1998.Word sense disambiguation based onsemantic density.
In Sanda Harabagiu,editor, Proceedings of COLING/ACLWorkshop on Usage of WordNet in NaturalLanguage Processing, pages 16?22,Montre?al, Que?bec, Canada.Miller, George A. and William G. Charles.1991.
Contextual correlates of semanticsimilarity.
Language and Cognitive Processes,6(1):1?28.Mooney, Raymond J.
1996.
Comparativeexperiments on disambiguating wordsenses: An illustration of the role of biasin machine learning.
In Eric Brill andKenneth Church, editors, Proceedings of theFirst Conference on Empirical Methods inNatural Language Processing, pages 82?91,Philadelphia, PA.Ng, Hwee Tou.
1997.
Exemplar-based wordsense disambiguation: Some recentimprovements.
In Claire Cardie andRalph Weischedel, editors, Proceedings ofthe Second Conference on Empirical Methodsin Natural Language Processing, pages208?216, Providence, RI.Palmer, Martha.
2000.
Consistent criteria forsense distinctions.
Computers and theHumanities, 34(1?2):217?222.Palmer, Martha and Zhibiao Wu.
1995.
Verbsemantics for English-Chinese translation.Machine Translation, 10:59?92.Patel, Malti, John A. Bullinaria, andJoseph P. Levy.
1998.
Extracting semanticrepresentations from large text corpora.
InJohn A. Bullinaria, D. W. Glasspool, andG.
Houghton, editors, Proceedings of theFourth Workshop on Neural Computation andPsychology, pages 199?212.
Springer,Berlin.Pearl, Judea.
1988.
Probabilistic Reasoning inIntelligent Systems.
Morgan Kaufmann,San Mateo, CA.Pedersen, Ted.
2000.
A simple approach tobuilding ensembles of naive Bayesianclassifiers for word sense disambiguation.In Proceedings of the First North AmericanAnnual Meeting of the Association forComputational Linguistics, pages 63?69,Seattle, WA.Pedersen, Ted.
2001.
A decision tree ofbigrams is an accurate predictor of wordsense.
In Proceedings of the Second North73Lapata and Brew Verb Class Disambiguation Using Informative PriorsAmerican Annual Meeting of the Associationfor Computational Linguistics, pages 63?69,Pittsburgh, PA.Pedersen, Ted and Rebecca Bruce.
1998.Knowledge lean word-sensedisambiguation.
In Proceedings of the 17thNational Conference on Artificial Intelligence,pages 800?805, Madison, WI.Pinker, Steven.
1989.
Learnability andCognition: The Acquisition of ArgumentStructure.
MIT Press, Cambridge, MA.Resnik, Philip Stuart.
1993.
Selection andInformation: A Class-Based Approach to LexicalRelationships.
Ph.D. thesis, University ofPennsylvania.Schulte im Walde, Sabine.
2000.
Clusteringverbs semantically according to theiralternation behaviour.
In Proceedings of the18th International Conference onComputational Linguistics, pages 747?753,Saarbru?cken, Germany.Schu?tze, Hinrich.
1998.
Automatic wordsense discrimination.
ComputationalLinguistics, 24(1):97?124.Stede, Manfred.
1998.
A generativeperspective on verb alternations.Computational Linguistics, 24(3):401?430.Talmy, Leonard.
1985.
Lexicalisationpatterns: Semantic structure in lexicalforms.
In Timothy Shopen, editor,Language Typology and Syntactic Description,III: Grammatical Categories and the Lexicon.Cambrige University Press, Cambridge,pages 57?149.Yarowsky, David.
1994.
Decision lists forlexical ambiuguity resolution: Applicationto accent restoration in Spanish andFrench.
In Proceedings of the 32nd AnnualMeeting of the Association for ComputationalLinguistics, pages 88?95, Las Cruces, NM.Yarowsky, David.
1995.
Unsupervised wordsense disambiguation rivaling supervisedmethods.
In Proceedings of the 33rd AnnualMeeting of the Association for ComputationalLinguistics, pages 189?196, Cambridge,MA.
