Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 56?64,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsRobust kaomoji detection in TwitterSteven Bedrick, Russell Beckley, Brian Roark, Richard SproatCenter for Spoken Language Understanding, Oregon Health & Science UniversityPortland, Oregon, USAAbstractIn this paper, we look at the problem of robustdetection of a very productive class of Asianstyle emoticons, known as facemarks or kao-moji.
We demonstrate the frequency and pro-ductivity of these sequences in social mediasuch as Twitter.
Previous approaches to detec-tion and analysis of kaomoji have placed lim-its on the range of phenomena that could bedetected with their method, and have lookedat largely monolingual evaluation sets (e.g.,Japanese blogs).
We find that these emoticonsoccur broadly in many languages, hence ourapproach is language agnostic.
Rather thanrelying on regular expressions over a prede-fined set of likely tokens, we build weightedcontext-free grammars that reward graphicalaffinity and symmetry within whatever sym-bols are used to construct the emoticon.1 IntroductionInformal text genres, such as email, SMS or socialmedia messages, lack some of the modes used inspoken language to communicate affect ?
prosodyor laughter, for example.
Affect can be providedwithin such genres through the use of text format-ting (e.g., capitalization for emphasis) or through theuse of extra-linguistic sequences such as the widelyused smiling, winking ;) emoticon.
These sorts ofvertical face representations via ASCII punctuationsequences are widely used in European languages,but in Asian informal text genres another class ofemoticons is popular, involving a broader symbol setand with a horizontal facial orientation.
These go bythe name of facemarks or kaomoji.
Figure 1 presents|?_?|57606710235369473: interesting use of u338 (diagonal overlay; inthe "combining diacritical marks" section)57807873274683393: RT @adamsbaldwin: THIS!
--> | RT @MelissaTweets"By being afraid to go at ?bama politically, people display a softracism."
~ #Repres ... (note peace sign in the "O" in Obama)57577928942305280: Use of "ake" for "a que" in Spanish57577937330913280: Example of tricky-to-tokenize tweet (irregularspacing): Sigan @TodoBiebs tiene la mejor informacion de Bieber,yes completamente cierta.Ellas son geniales.
#LEGOO :) <357651140510224384: great English abbreviations and use of Unicode:Hehe?
OK u r sexy, hottie ffw me naw, wud ya RT @Hawtbaeby:@enahoanagha Sha?
M sad:-(57581074657718272: You dnt never answer yo phone!57583914226683904: IHate When Ppl Have Attitudes Wit Mee For NoReason.
<---- repetition and also "With"->"Wit", "People"->"Ppl"57850097320460289: Good example of shortening: @China_DollCris ulucky smh I'm going str8 thru !
!57610065699549184 <--- using Cyrillic characters to write inSpanish/portugese...57577987641573376: hashtags used as parts of speech; also note "2"instead of "to"57592260870668288: awareness of spelling issuesFun smileys:(?_?'!
)57746992926949376 (breve with combining lower line, makes a nicetear effect)??-?
( !!!)"?
??#"(????
?
?
?
????)#(?_?
)5759226087066828857689354826547200: ?(?????)?
as well as (!!!
)))))))))?5767862580532019257675270324367360 (???!???
)576174644559912965760316604849766457596757185544192 " ?[?
?
??
]#, (????
)57584430105108480 (??"??
)57745965351837696 (*?m?
*)57745944376119296 (????-????
)57745659142483968 (???
?`)57825858454433792 <-- uses a fun swirly Tamil letter: (?!?!
`)[o_-]57606710235369473: interesting use of u338 (diagonal overlay; inthe "combining diacritical marks" section)57807873274683393: RT @adamsbaldwin: THIS!
--> | RT @MelissaTweets"By being afraid to go at ?bama politically, people display a softracism."
~ #Repres ... (note peace sign in the "O" in Obama)57577928942305280: Use of "ake" for "a que" in Spanish57577937330913280: Example of tricky-to-tokenize tweet (irregularspacing): Sigan @TodoBiebs tiene la mejor informacion de Bieber,yes completamente cierta.Ellas son geniales.
#LEGOO :) <357651140510224384: great English abbreviations and use of Unicode:Hehe?
OK u r sexy, hottie ffw me naw, wud ya RT @Hawtbaeby:@enahoanagha Sha?
M sad:-(57581074657718272: You dnt never answer yo phone!57583914226683904: IHate When Ppl Have Attitudes Wit Mee For NoReason.
<---- repetition and also "With"->"Wit", "People"->"Ppl"57850097320460289: Good example of shortening: @China_DollCris ulucky smh I'm going str8 thru !
!57610065699549184 <--- using Cyrillic characters to write inSpanish/portugese...57577987641573376: hashtags used as parts of speech; also note "2"instead of "to"57592260870668288: awareness of spelling issuesFun smileys:(?_?'!
)57746992926949376 (breve with combining lower line, makes a nicetear effect)??-?
( !!!)"?
??#"(????
?
?
?
????)#(?_?
)5759226087066828857689354826547200: ?(?????)?
as well as (!!!
)))))))))?5767862580532019257675270324367360 (???!???
)576174644559912966031 60484976645967 7185 4192 " ?[?
?
??
]#, (????
)57584430105108480 (??"??
)57745965351837696 (*?m?
*)74594 376119  (????-????
)57745659142483968 (???
?`)57825858454433792 <-- uses a fun swirly Tamil letter: (?!?!
`)\(?v?
)/57606710235369473: interesting use of u338 (diagonal overlay; inthe "combining diacritical marks" section)57807873274683393: RT @adamsbaldwin: THIS!
--> | RT @MelissaTweets"By being afraid to go at ?bama politically, people display a softracism."
~ #Repres ... (note peace sign in the "O" in Obama)57577928942305280: Use of "ake" for "a que" in Spanish57577937330913280: Example of tricky-to-tokenize tweet (irregularspacing): Sigan @TodoBiebs tiene la mejor informacion de Bieber,yes completamente cierta.Ellas son geniales.
#LEGOO :) <357651140510224384: great English abbreviations and use of Unicode:Hehe?
OK u r sexy, hottie ffw me naw, wud ya RT @Hawtbaeby:@enahoanagha Sha?
M sad:-(57581074657718272: You dnt never answer yo phone!57583914226683904: IHate When Ppl Have Attitudes Wit Mee For NoReason.
<---- repetition and also "With"->"Wit", "People"->"Ppl"57850097320460289: Good example of shortening: @China_DollCris ulucky smh I'm going str8 thru !
!57610065699549184 <--- using Cyrillic characters to write inSpanish/portugese...57577987641573376: hashtags used as parts of speech; also note "2"instead of "to"57592260870668288: awareness of spelling issuesFun smileys:(?_?'!
)57746992926949376 (breve with combining lower line, makes a nicetear effect)??-?
( !!!)"?
??#"(????
?
?
?
????)#(?_?
)5759226087066828857689354826547200: ?(?????)?
as well as (!!!
)))))))))?5767862580532019257675270324367360 (???!???
)576174644559912965760316604849766457596757185544192 " ?[?
?
??
]#, (????
)57584430105108480 (??"??
)57745965351837696 (*?m?
*)57745944376119296 (????-????
)57745659142483968 (???
?`)57825858454433792 <-- uses a fun swirly Tamil letter: (?!?!
`)Figure 1: Some representative kaomoji emoticonsseveral examples of these sequences, including bothrelatively common kaomoji as well as more exoticand complex creations.This class of emoticon is far more varied and pro-ductive than the sideways European style emoticons,and even lists of on the order of ten thousand emoti-cons will fail to cover all instances in even a mod-est sized sample of text.
This relative productiv-ity is due to several factors, including the horizon-tal orientation, which allows for more flexibility inconfiguring features both within the face and sur-rounding the face (e.g., arms) than the vertical ori-entation.
Another important factor underlying kao-moji productivity is historical in nature.
kaomojiwere developed and popularized in Japan and otherAsian countries whose scripts have always requiredmultibyte character encodings, and whose users ofelectronic communication systems have significantexperience working with characters beyond thosefound in the standard ASCII set.Linguistic symbols from various scripts can beappropriated into the kaomoji for their resemblenceto facial features, such as a winking eye, and au-thors of kaomoji sometimes use advanced Unicodetechniques to decorate glyphs with elaborate com-binations of diacritic marks.
For example, the kao-56moji in the top righthand corner of Figure 1, includesan Arabic letter, and Thai vowel diacritics.
Accu-rate detection of these tokens ?
and other commonsequences of extra-linguistic symbol sequences ?
isimportant for normalization of social media text fordownstream applications.At the most basic level, the complex and unpre-dictable combinations of characters found withinmany kaomoji (often including punctuation andwhitespace, as well as irregularly-used Unicodecombining characters) can seriously confound sen-tence and word segmentation algorithms that at-tempt to operate on kaomoji-rich text; since segmen-tation is typically the first step in any text process-ing pipeline, issues here can cause a wide varietyof problems downstream.
Accurately removing ornormalizing such sequences before attempting seg-mentation can ensure that existing NLP tools areable to effectively work with and analyze kaomoji-including text.At a higher level, the inclusion of a particularkaomoji in a text represents a conscious decisionon the part of the text?s author, and fully interpret-ing the text necessarily involves a degree of inter-pretation of the kaomoji that they chose to include.European-style emoticons form a relatively closedset and are often fairly straightforward to interpret(both in terms of computational, as well as human,effort); kaomoji, on the other hand, are far more di-verse, and interpretation is rarely simple.In this paper, we present preliminary work ondefining robust models for detecting kaomoji in so-cial media text.
Prior work on detecting and classi-fying these extra-linguistic sequences has relied onthe presence of fixed attested patterns (see discus-sion in Section 2) for detection, and regular expres-sions for segmentation.
While such approaches cancapture the most common kaomoji and simple vari-ants of them, the productive and creative nature ofthe phenomenon results in a non-negligible out-of-vocabulary problem.
In this paper, we approach theproblem by examining a broader class of possiblesequences (see Section 4.2) for symmetry using arobust probabilistic context-free grammar with ruleprobabilities proportional to the symmetry or affin-ity of matched terminal items in the rule.
Our PCFGis robust in the sense that every candidate sequenceis guaranteed to have a valid parse.
We use the re-sulting Viterbi best parse to provide a score to thecandidate sequence ?
reranking our high recall listto achieve, via thresholds, high precision.
In addi-tion, we investigate unsupervised model adaptation,by incorporating Viterbi-best parses from a small setof attested kaomoji scraped from websites; and in-ducing grammars with a larger non-terminal set cor-responding to regions of the face.We present bootstrapping experiments for deriv-ing highly functional, language independent modelsfor detecting kaomoji in text, on multilingual Twit-ter data.
Our approach can be used as part of astand-alone detection model, or as input into semi-automatic kaomoji lexicon development.
Before de-scribing our approach, we will first present priorwork on this class of emoticon.2 Prior WorkNakamura et al (2003) presented a natural languagedialogue system that learned a model for generat-ing kaomoji face marks within Japanese chat.
Theytrained a neural net to produce parts of the emoti-con ?
mouth, eyes, arms and ?optional things?
asobserved in real world data.
They relied on a hand-constructed inventory of observed parts within eachof the above classes, and stitched together predictedparts into a complete kaomoji using simple tem-plates.Tanaka et al (2005) presented a finite-statechunking approach for detecting kaomoji inJapanese on-line bulletin boards using SVMs withsimple features derived from a 7 character window.Training was performed on kaomoji dictionariesfound online.
They achieved precision and recall inthe mid-80s on their test set, which was a significantrecall improvement (17% absolute) and modestprecision improvement (1.5%) over exact matchwithin the dictionaries.
They note certain kinds oferrors, e.g., ?(Thu)?
which demonstrate that theirchunking models are (unsurprisingly) not capturingthe typical symmetry of kaomoji.
In addition, theyperform classification of the kaomoji into 6 roughcategories (happy, sad, angry, etc.
), achieving highperformance (90% accuracy) using a string kernelwithin an SVM classifier.Ptaszynski et al (2010) present work on a largedatabase of kaomoji, which makes use of an analy-57sis of the gestures conveyed by the emoticons andtheir relation to a theory of non-verbal expressions.They created an extensive (approximately 10,000entry) lexicon with 10 emotion classes, and usedthis database as the basis of both emoticon extrac-tion from text and emotion classification.
To detectan emoticon in text, their system (named ?CAO?
)looked for three symbols in a row from a vocabularyof the 455 most frequent symbols in their database.Their approach led to a 2.4% false negative ratewhen evaluated on 1,000 sentences extracted fromJapanese blogs.
Once detected, the system extractsthe emoticon from the string using a gradual relax-ation from exact match to approximate match, withvarious regular expressions depending on specificpartial match criteria.
A similar deterministic al-gorithm based on sequenced relaxation from exactmatch was used to assign affect to the emoticon.Our work focuses on the emoticon detectionstage, and differs from the above systems in a num-ber of ways.
First, while kaomoji were popularizedin Asia, and are most prevalent in Asian languages,they not only found in messages in those languages.In Twitter, which is massively multilingual, we findkaomoji with some frequency in many languages,including European languages such as English andPortuguese, Semitic languages and a range of Asianlanguages.
Our intent is to have a language inde-pendent algorithm that looks for such sequences inany message.
Further, while we make use of onlinedictionaries as development data, we appreciate theproductivity of the phenomenon and do not want torestrict the emoticons that we detect to those con-sisting of pre-observed characters.
Hence we focusinstead on characteristics of kaomoji that have beenignored in the above models: the frequent symmetryof the strings.
We make use of context-free mod-els, built in such a way as to guarantee a parse forany candidate sequence, which permits explorationof a much broader space of potential candidates thanthe prior approaches, using very general models andlimited assumptions about the key components ofthe emoticons.3 DataOur starting resources consisted of a large, multi-lingual corpus of Twitter data as well as a smallercollection of kaomoji scraped from Internet sources.Our Twitter corpus consists of approximately 80million messages collected using Twitter?s ?Stream-ing API?
over a 50-day period from June throughAugust 2011.
The corpus is extremely linguisticallydiverse; human review of a small sample identifiedmessages written in >30 languages.
The messagesthemselves exhibit a wide variety of phenomena, in-cluding substantial use of different types of Inter-net slang and written dialect, as well as numerousforms of non-linguistic content such as emoticonsand ?ASCII art.
?We took a two-pronged approach to developinga set of ?gold-standard?
kaomoji.
Our first ap-proach involved manually ?scraping?
real-world ex-amples from the Internet.
Using a series of hand-written scripts, we harvested 9,193 examples fromseveral human-curated Internet websites devoted tocollecting and exhibiting kaomoji.
Many of theseconsisted of several discrete sub-units, typically in-cluding at least one ?face?
element along with asmall amount of additional content.
For example,consider the following kaomoji, which appeared inthis exact form eight times in our Twitter corpus:?(*???)??+.???????+.??(???*)?
9?(!"?"
?
?)??????#?
8?(??????)???
9?(???`;)????
52???????
5?_(?_? )
1?/T?T)/???????
4????????????????????????
1??????
1?????
426??????
1?(?????)/?????
1??????????????
4??(?????)(?
?_ _)????
10????(???)???????
1????(T-T)?
(^^ )????
2???Uo???oU???
1?????+.(???)(???)?+.?!!
1.
Note that, in this case, the?face?
is followed by a small amount of hiragana,and that the message concludes with a dingbat in theform of a ?heart?
symbol.1Of these 9,193 scraped examples, we observed?3,700 to appear at least once in our corpus ofTwitter messages, and ?2,500 more than twice.The most common kaomoji occurred with frequen-cies in the low hundreds of thousands, although thefrequency with which individual kaomoji appearedroughly followed a power-law distribution, meaningthat there were a small number that occurred withgreat frequency and a much larger number that onlyappeared rarely.From this scraped corpus, we attempted to iden-tify a subset that consisted solely of ?faces?
to serveas a high-precision training set.
After observingthat nearly all of the faces involved a small numberof characters bracketed one of a small set of natu-ral grouping characters (parentheses, ?curly braces,?1Note as well that this kaomoji includes not only a wide va-riety of symbols, but that some of those symbols are themselvesmodified using combining diacritic marks.
This is a commonpractice in modern kaomoji, and one that complicates analysis.58etc.
), we extracted approximately 6,000 substringsmatching a very simple regular expression pattern.This approach missed many kaomoji, and of theexamples that it did detect, many were incom-plete (in that they were missing any extra-bracketedcontent?
arms, ears, whiskers, etc.)
However, thecontents of this ?just faces?
sub-corpus offered de-cent coverage of many of the core kaomoji phenom-ena in a relatively noise-free manner.
As such, wefound it to be useful as ?seed?
data for the grammaradaptation described in section 4.4.In addition to our ?scraped?
kaomoji corpus, weconstructed a smaller corpus of examples drawn di-rectly from our Twitter corpus.
The kaomoji phe-nomenon is complex enough that capturing it in itstotality is difficult.
However, it is possible to capturea subset of kaomoji by looking for regions of per-fect lexical symmetry.
This approach will capturemany of the more regularly-formed and simple kao-moji (for example, ?(-_-)?
), although it will missmany valid kaomoji.
Using this approach, we iden-tied 3,580 symmetrical candidate sequences; mostof these were indeed kaomoji, although there wereseveral false positives (for example, symmetrical se-quences of repeated periods, question marks, etc.
).Using simple regular expressions, we were able toremove 289 such false positives.Interestingly, there was very little overlap be-tween the corpus scraped from the Web and the sym-metry corpus.
A total of 39 kaomoji appeared in ex-actly the same form in both sets.
We noted, however,that the kaomoji harvested from the Web tended tobe longer and more elaborate than those identifiedfrom our Twitter corpus using the symmetry heuris-tic (Mann-Whitney U, p < 0.001), and as previouslydiscussed, the Web kaomoji often contained one ormore face elements.
Thus we expanded our defi-nition of overlap, and counted sequences from thesymmetrical corpus that were substrings of scrapedkaomoji.
Using this criterion, we identified 177 pos-sibly intersecting kaomoji.
The fact that so few indi-vidual examples occurred in both corpora illustratesthe extremely productive nature of the phenomenon.4 Methods4.1 Graphical similarityThe use of particular characters in kaomoji is ul-timately based on their graphical appearance.
ForFigure 2: Ten example character pairs with imperfect(but very high) symmetry identified by our algorithm.Columns are: score, hex code point 1, hex code point2, glyph 1, glyph 2.example, good face delimiters frequently includemated brackets or parentheses, since these elementsnaturally look as if they delimit material.
Further-more, there are many characters which are not tech-nically ?paired,?
but look roughly more-or-less sym-metrical.
For example, the Arabic-Indic digits!???"and!???"
are commonly used as bracketing delimiters, forexample: !???".
These characters can serve both as?arms?
as well as ?ears.
?Besides bracketing, symmetry plays an additionalrole in kaomoji construction.
Glyphs that make good?eyes?
are often round; ?noses?
are often symmet-ric about their central axis.
Therefore a measure ofgraphical similarity between characters is desirable.To that end, we developed a very simple measureof similarity.
From online sources, we downloadeda sample glyph for each code point in the UnicodeBasic Multilingual Plane, and extracted a bitmap foreach.
In comparing two glyphs we first scale themto have the same aspect ratio if necessary, and wethen compute the proportion of shared pixels be-tween them, with a perfect match being 1 and theworst match being 0.
We can thus compute whethertwo glyphs look similar; whether one glyph is a goodmirror image of the other (by comparing glyph Awith the mirror image of glyph B); and whether aglyph is (vertically) symmetric (by computing thesimilarity of the glyph and its vertical mirror image).The method, while clearly simple-minded,nonetheless produces plausible results, as seen inFigure 2, which shows the best 10 candidates formirror image character pairs.
We also calculatethe same score without flipping the image verti-cally, which is also used to score possible symbolmatches, as detailed in Section 4.3.594.2 Candidate extractionWe perform candidate kaomoji extraction via a verysimple hidden Markov model, which segments allstrings of Unicode graphemes into contiguous re-gions that are either primarily linguistic (mainlylanguage encoding symbols2) or primarily non-linguistic (mainly punctuation, or other symbols).Our candidate emoticons, then, are this extensivelist of mainly non-linguistic symbol sequences.
Thisis a high recall approach, returning most sequencesthat contain valid emoticons, but quite low precision,since it includes many other sequences as well (ex-tended runs of punctuation, etc.
).The simple HMM consists of 2 states: call themA (mainly linguistic) and @ (mainly non-linguistic).Since there are two emitted symbol classes (linguis-tic L and non-linguistic N ), each HMM state musthave two emission probabilities, one for its domi-nant symbol class (L in A and N in @) and onefor the other symbol class.
Non-linguistic symbolsoccur quite often in linguistic sequences, as punc-tuation for example.
However, sequences of, say,3 or more in a row are not particularly frequent.Similarly, linguistic symbols occur often in kaomoji,though not often in sequences of, say, 3 or more.Hence, to segment into contiguous sequences of acertain number in a row, the probability of transitionfrom state A to state @ or vice versa must be signif-icantly lower than the probability of emitting one ortwo N from A states or L from @ states.
We thushave an 8 parameter HMM (four transition and fouremission probabilities) that was coarsely parameter-ized to have the above properties, and used it to ex-tract candidate non-linguistic sequences for evalua-tion by our PCFG model.Note that this approach does have the limitationthat it will trim off some linguistic symbols that oc-cur on the periphery of an emoticon.
Future versionsof this part of the system will address this issue byextending the HMM.
For this paper, we made use ofa slightly modified version of this simple HMM forcandidate extraction.
The modifications involved theaddition of a special input state for whitespace andfull-stop punctuation, which helped prevent certainvery common classes of false-positive.2Defined as a character having the Unicode ?letter?
charac-ter property.rule score rule scoreX?
a X b S(a,b) X?
a b S(a,b)X?
a X  X?
X a X?
a ?
X?
X X ?Table 1: Rule schemata for producing PCFG4.3 Baseline grammar inductionWe perform a separate PCFG induction for ev-ery candidate emoticon sequence, based on a smallset of rule templates methods for assigning ruleweights.
By inducing small, example-specificPCFGs, we ensure that every example has a validparse, without growing the grammar to the point thatthe grammar constant would seriously impact parserefficiency.Table 1 shows the rule schemata that we used forthis paper.
The resulting PCFG would have a singlenon-terminal (X) and the variables a and b would beinstantiated with terminal items taken from the can-didate sequence.
Each instantiated rule receives aprobability proportional to the assigned score.
Forthe rules that ?pair?
symbols a and b, a score is as-signed in two ways, call them S1(a, b) and S2(a, b)(they will be defined in a moment).
Then S(a, b) =max(S1(a, b) and S2(a, b)).
If S(a, b) < ?, for somethreshold ?,3 then no rule is generated.
S1 is thegraphical similarity of the first symbol with the verti-cal mirror image of the second symbol, calculated aspresented in Section 4.1.
This will give a high scorefor things like balanced parentheses.
S2 is the graph-ical similarity of the first symbol with the secondsymbol (not vertically flipped), which gives highscores to the same or similar symbols.
This permitsmatches for, say, eyes that are not symmetric due toan orientation of the face, e.g.,!???"(!#!).
The other pa-rameters (, ?
and ?)
are included to allow for, butpenalize, unmatched symbols in the sequence.All possible rules for a given sequence are instan-tiated using these templates, by placing each symbolin the a slot with all subsequent symbols in the b slotand scoring, as well as creating all rules with just aalone for that symbol.
For example, if we are giventhe kaomoji (o o;) specific rules would be createdif the similarity scores were above threshold.
For thesecond symbol ?o?, the algorithm would evaluate the3For this paper, ?
was chosen to be 0.7.60similarity between ?o?
and each of the four symbolsto its right , o, ; and ).The resulting PCFG is normalized by summingthe score for each rule and normalizing by the score.The grammar is then transformed to a weakly equiv-alent CNF by binarizing the ternary rules and in-troducing preterminal non-terminals.
This grammaris then provided to the parser4, which returns theViterbi best parse of the candidate emoticon alongwith its probability.
The score is then converted toan approximate perplexity by dividing the negativelog probability by the number of unique symbols inthe sequence and taking the exponential.4.4 Grammar enhancement and adaptationThe baseline grammar induction approach outlinedin the previous section can be improved in a cou-ple of ways, without sacrificing the robustness of theapproach.
One way is through grammar adaptationbased on automatic parses of attested kaomoji.
Theother is by increasing the number of non-terminalsin the grammar, according to a prior understandingof their typical (canonical) structure.
We shall dis-cuss each in turn.Given a small corpus of attested emoticons (in ourcase, the ?just faces?
sub-corpus described in sec-tion 3), we can apply the parser above to those ex-amples, and extract the Viterbi best parses into anautomatically created treebank.
From that treebank,we extract counts of rule productions and use theserule counts to inform our grammar estimation.
Thebenefit of this approach is that we will obtain addi-tional probability mass for frequently observed con-structions in that corpus, thus preferring commonlyassociated pairs within the grammar.
Of course, thecorpus only has a small fraction of the possible sym-bols that we hope to cover in our robust approach, sowe want to incorporate this information in a way thatdoes not limit the kinds of sequences we can parse.We can accomplish this by using simple Maxi-mum a Posteriori (MAP) adaptation of the grammar(Bacchiani et al, 2006).
In this scenario, we willfirst use our baseline method of grammar induction,using the schemata shown in Table 1.
The scoresderived in that process then serve as prior counts4We used the BUBS parser (Bodenstab et al, 2011).http://code.google.com/p/bubs-parser/for the rules in the grammar, ensuring that all ofthese rules continue to receive probability mass.
Wethen add in the counts for each of the rules from thetreebank.
Many of the rules may have been unob-served in the corpus, in which case they receive noadditional counts; observed rules, however, will re-ceive extra weight proportional to their frequency inthat corpus.
Note that these additional weights canbe scaled according to a given parameter.
After in-corporating these additional counts, the grammar isnormalized and parsing is performed as before.
Ofcourse, this process can be iterated ?
a new auto-matic treebank can be produced based on an adaptedgrammar, and so on.In addition to grammar adaptation, we can en-rich our grammars by increasing the non-terminalsets.
To do this, we created a nested hierarchyof ?regions?
of the emoticons, with constraints re-lated to the canonical composition of the faces,e.g., eyes are inside of faces, noses/mouths betweeneyes, etc.
These non-terminals replace our genericnon-terminal X in the rule schemata.
For the cur-rent paper, we included the following five ?region?non-terminals: ITEM, OUT, FACE, EYES, NM.
Thenon-terminal ITEM is intended as a top-most non-terminal to allow multiple emoticons in a single se-quence, via an ITEM ?
ITEM ITEM production.None of the others non-terminals have repeating pro-ductions of that sort ?
so this replaces the X?
X Xproduction from Table 1.Every production (other than ITEM ?
ITEMITEM) has zero or one non-terminals on the right-hand side.
In our new schemata, non-terminals onthe left-hand side can only have non-terminals on theright-hand side at the same or lower levels.
This en-forces the nesting constraint, i.e., that eyes are insideof the face.
Levels can be omitted however ?
e.g.,eyes but no explicit face delimiter ?
hence we can?skip?
a level using unary projections, e.g., FACE?EYES.
Those will come with a ?skip level?
weight.Categories can also rewrite to the same level (with a?stay level?
weight) or rewrite to the next level af-ter emitting symbols (with a ?move to next level?weight).To encode a preference to move to the next levelrather than to stay at the same level, we assign aweight of 1 to moving to the next level and a weightof 0.5 to staying at the same level.
The ?skip?61rule scoreITEM ?
ITEM ITEM ?ITEM ?
OUT ?OUT ?
a OUT b S(a,b) + 0.5OUT ?
a OUT  + 0.5OUT ?
OUT a  + 0.5OUT ?
a FACE b S(a,b) + 1OUT ?
a FACE  +1OUT ?
FACE a  +1OUT ?
FACE 0.5FACE ?
a FACE b S(a,b) + 0.5FACE ?
a FACE  + 0.5FACE ?
FACE a  + 0.5FACE ?
a EYES b S(a,b) + 1FACE ?
a EYES  +1FACE ?
EYES a  +1FACE ?
EYES 0.1EYES ?
a EYES b S(a,b) + 0.5EYES ?
a EYES  + 0.5EYES ?
EYES a  + 0.5EYES ?
a NM b S(a,b) + 1EYES ?
a NM  +1EYES ?
NM a  +1EYES ?
NM 0.1EYES ?
a b S(a,b) + 1NM ?
a NM NM ?
NM a NM ?
a ?Table 2: Rule schemata for expanded non-terminal setweights depend on the level, e.g., skipping OUTshould be cheap (weight of 0.5), while skippingthe others more expensive (weight of 0.1).
Theseweights are like counts, and are added to the similar-ity counts when deriving the probability of the rule.Finally, there is a rule in the schemata in Table 1 witha pair of symbols and no middle non-terminal.
Thisis most appropriate for eyes, hence will only be gen-erated at that level.
Similarly, the single symbol onthe right-hand side is for the NM (nose/mouth) re-gion.
Table 2 presents our expanded rule schemata.Note that the grammar generated with this ex-panded set of non-terminals is robust, just as the ear-lier grammar is, in that every sequence is guaranteedto have a parse.
Further, it can be adapted using thesame methods presented earlier in this section.5 Experimental ResultsUsing the candidate extraction methodology de-scribed in section 4.2, we extracted 1.6 million dis-tinct candidates from our corpus of 80 million Twit-ter messages (candidates often appeared in multi-ple messages).
These candidates included genuineemoticons, as well as extended strings of punc-tuation and other ?noisy?
chunks of text.
Gen-uine kaomoji were often picked up with someamount of leading or trailing punctuation, for exam-ple: ?..\(??`)/?
; other times, kaomoji beginningwith linguistic characters were truncated: (^?
*)?.We provided these candidates to our parser un-der four different conditions, each one producing1.5 million parse trees: the single non-terminal ap-proach described in section 4.3 or the enhanced mul-tiple non-terminal approach described in section 4.4,both with and without training via the Maximum APosteriori approach described in section 4.4.Using the weighted-inside-score method de-scribed in section 4.3, we produced a ranked listof candidate emoticons from each condition?s out-put.
?Well-scoring?
candidates were ones for whichthe parser was able to construct a low-cost parse.We evaluated our approach in two ways.
The firstway examined precision?
how many of the best-scoring candidate sequences actually contained kao-moji?
Manually reviewing all 1.6 million candidateswas not feasible, so we evaluated this aspect of oursystem?s performance on a small subset of its out-put.
Computational considerations forced us to pro-cess our large corpus in parallel, meaning that our setof 1.6 million candidate kaomoji was already parti-tioned into 160 sets of?10,000 candidates each.
Wemanually reviewed the top 1,000 sorted results fromone of these partitions, and flagged any entries thatdid not contain or consist of a face-like kaomoji.
Theresults of each condition are presented in table 3.The second evaluation approach we will exam-ine looks at how our method compares with thetrigram-based approach described by (Yamada et al,2007) (as described by (Ptaszynski et al, 2010)).We trained both smoothed and unsmoothed lan-guage models 5 on the ?just faces?
sub-corpus usedfor the A Posteriori grammar enhancement, andcomputed perplexity measurements for the sameset ?10,000 candidates used previously.
Table 3presents these results; clearly, a smoothed trigrammodel can achieve good results.
The unsmoothedmodel at first glance seems to have performed verywell; note, however, that only approximately 600(out of nearly 10,000) candidates were ?matched?by the unsmoothed model (i.e., they did not containany OOV symbols and therefore had finite perplex-ity scores), yielding a very small but high-precisionset of emoticons.Looking at precision, the model-based ap-proaches outperformed our grammar approach.
It5Using the OpenGrm ngram language modeling toolkit.62Condition P@1000 MAPSingle Nonterm, Untrained 0.662 0.605Single Nonterm, Trained 0.80 0.945Multiple Nonterm, Untrained 0.795 0.932Multiple Nonterm, Trained 0.885 0.875Unsmoothed 3-gram 0.888 0.985Smoothed 3-gram 0.905 0.956Mixed, Single Nonterm, Untrained 0.662 0.902Mixed, Single Nonterm, Trained 0.804 0.984Mixed, Multiple Nonterm, Untrained 0.789 0.932Mixed, Multiple Nonterm, Trained 0.878 0.977Table 3: Experimental Results.should be noted, however, that the trigram approachwas much less tolerant of certain non-standard for-mulations involving novel characters or irregularformulations ((?!?)?(?o?)?(??o??)and(?!?)?(?o?)?(??o??)
are examples of kao-moji that our grammar-based approach ranked morehighly than did the trigram approach).
The twoapproaches also had different failure profiles.
Thegrammar approach?s false positives tended to besymmetrical sequences of punctuation, whereas thelanguage models?
were more variable.
Were we toreview a larger selection of candidates, we believethat the structure-capturing nature of the grammarapproach would enable it to outperform the moresimplistic approach.We also attempted a hybrid ?mixed?
approach inwhich we used the language models to re-rank thetop 1,000 ?best?
candidates from our parser?s output.This generally resulted in improved performance,and for some conditions the improvement was sub-stantial.
Future work will explore this approach ingreater detail and over larger amounts of data.6 DiscussionWe describe an almost entirely unsupervised ap-proach to detecting kaomoji in irregular, real-worldtext.
In its baseline state, our system is able to ac-curately identify a large number of examples usinga very simple set of templates, and can distinguishkaomoji from other non-linguistic content (punctu-ation, etc.).
Using minimal supervision, we wereable to effect a dramatic increase in our system?sperformance.
Visual comparison of the ?untrained?results with the ?trained?
results was instructive.The untrained systems?
results were very heavily in-fluenced by their template rules?
strong preferencefor visual symmetry.
Many instances of symmet-rical punctuation sequences (e.g., ..?..)
endedup being ranked more highly than even fairly sim-ple kaomoji, and in the absence of other informa-tion, the length of the input strings also played a too-important role in their rankings.The MAP-ehanced systems?
results, on the otherhand, retained their strong preference for symme-try, but were also influenced by the patterns andcharacters present in their training data.
For ex-ample, two of the top-ranked ?false positives?
fromthe enhanced system were the sequences >,< and= =, both of which (while symmetrical) also con-tain characters often seen in kaomoji.
By using morestructurally diverse training data, we expect furtherimprovements in this area.
Also, our system cur-rently relies on a very small number of relativelysimplistic grammar templates; expanding these toencode additional structure may also help.Due to our current scoring mechanism, our parseris biased against certain categories of kaomoji.
Par-ticularly poorly-scored are complex creations suchas (((| ????
??|?
???=??
?| ????
??|?))).
In this example, the large numberof combining characters and lack of obvious nest-ing therein confounded our templates and producedexpensive parse trees.
Future work will involve im-proved handling of such cases, either by modifiedparsing schemes or additional templates.One other area of future work is to match par-ticular kaomoji, or fragments of kaomoji (e.g.
par-ticular eyes), to particular affective states, or otherfeatures of the text.
Some motifs are already wellknown: for example, there is wide use of TT, or thesimilar-looking Korean hangeul vowel yu, to repre-sent crying eyes.
We propose to do this initially bycomputing the association between particular kao-moji and words in the text.
Such associations mayyield more than just information on the likely af-fect associated with a kaomoji.
So, for example,using pointwise mutual information as a measureof association, we found that in our Twitter corpus,(*?_?
*) seems to be highly associated with tweetsabout Korean pop music, *-* with Brazilian post-ings, and with Indonesian postings.
Suchassociations presumably reflect cultural preferences,and could prove useful in identifying the provenanceof a message even if more conventional linguistictechniques fail.63ReferencesMichiel Bacchiani, Michael Riley, Brian Roark, andRichard Sproat.
2006.
MAP adaptation of stochas-tic grammars.
Computer Speech and Language,20(1):41?68.Nathan Bodenstab, Aaron Dunlop, Keith Hall, and BrianRoark.
2011.
Adaptive beam-width prediction for ef-ficient cyk parsing.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics, pages 440?449.Junpei Nakamura, Takeshi Ikeda, Nobuo Inui, andYoshiyuki Kotani.
2003.
Learning face mark for nat-ural language dialogue system.
In Proc.
Conf.
IEEEInt?l Conf.
Natural Language Processing and Knowl-edge Eng, pages 180?185.Michal Ptaszynski, Jacek Maciejewski, Pawel Dybala,Rafal Rzepka, and Kenji Araki.
2010.
Cao: A fullyautomatic emoticon analysis system based on theoryof kinesics.
IEEE Transactions on Affective Comput-ing, 1:46?59.Yuki Tanaka, Hiroya Takamura, and Manabu Okumura.2005.
Extraction and classification of facemarks withkernel methods.
In Proc.
10th Int?l Conf.
IntelligentUser Interfaces.T.
Yamada, S. Tsuchiya, S. Kuroiwa, and F. Ren.
2007.Classification of facemarks using n-gram.
In Inter-national Conference on Natural Language Processingand Knowledge Engineering, pages 322?327.64
