An effective method of using Web based information for Relation ExtractionYong Wai Keong, StanleyInstitute for Inforcomm Research21 Heng Mui Keng Terrace,Singapore 119613wkyong@i2r.a-star.edu.sgSu JianInstitute for Inforcomm Research21 Heng Mui Keng Terrace,Singapore 119613sujian@i2r.a-star.edu.sgAbstractWe propose a method that incorporatesparaphrase information from the Web toboost the performance of a supervised re-lation extraction system.
Contextual infor-mation is extracted from the Web using asemi-supervised process, and summarizedby skip-bigram overlap measures over theentire extract.
This allows the capture of lo-cal contextual information as well as moredistant associations.
We observe a statisti-cally significant boost in relation extractionperformance.We investigate two extensions, thematicclustering and hypernym expansion.
In tan-dem with thematic clustering to reduce noisein our paraphrase extraction, we attempt toincrease the coverage of our search for para-phrases using hypernym expansion.Evaluation of our method on the ACE 2004corpus shows that it out-performs the base-line SVM-based supervised learning algo-rithm across almost all major ACE relationtypes, by a margin of up to 31%.1 Introduction and motivationIn this paper, we shall be primarily dealing withthe sort of relations defined in the NIST?s Auto-matic Content Extraction program, specifically forthe Relation Detection and Characterization (RDC)task (Doddington et al, 2004).
These are links be-tween two entities mentioned in the same sentence,and further restrict our consideration to those rela-tionships clearly supported by evidence in the scopeof the same document.The ACE?s annotators mark all mentions of re-lations where there is a direct syntactic connectionbetween the entities, i.e.
when one entity mentionmodifies another one, or when two entity mentionsare arguments of the same event.
Relations betweenentities that are implied in the text but which do notsatisfy either requirement are considered to be im-plicit, and are marked only once.Our work sits squarely in the realm of work onregular IE done by (Zelenko et al, 2003; Zhou etal., 2005; Chen et al, 2006).
Here, the corpus ofinterest is a well defined set of texts, such as newsarticles, and we have to detect and classify all ap-pearances of relations from a set of given relationtypes in the documents.
In line with assumptionsin the related work, we assert that the differences inthe markup for implicit and explicit relations doesnot significantly affect our performance.Supervised learning methods have proved to besome of the most effective for regular IE.
They dohowever, need large volumes of expensively anno-tated examples to perform robustly.
As a result, eventhe large ACE compilation has deficiencies in thenumber of instances available for some of the rela-tion types.
(Zhang et al, 2005) reports an F-scoreof 50% for categories of intermediate size and only30% for the least common relation types.
It appearsthat there are hard limits on the performance of rela-tion extraction systems as long as they have to relysolely on information in the training set.We were thus inspired to explore how one could350exploit theWeb, the largest raw text collection freelyavailable, for regular IE.
In this paper, we detailthe ways one can fruitfully employ relation spe-cific sentences retrieved from the Web with a semi-supervised labeling approach.
Most importantly weshow how the output from such an approach can becombined with existing knowledge gleaned from su-pervised learning to improve the performance of re-lation extraction significantly.2 Related Work and DifferencesTo our knowledge, there is no previous work that ex-ploits the information from a large raw text corpuslike the Web to improve supervised relation extrac-tion.
In the spirit of the work done by (Shinyamaand Sekine, 2003; Bunescu and Mooney, 2007), weare trying to collect clusters of paraphrases for givenrelation mentions.
Briefly, since the same relationcan be expressed in many ways, the information wemay learn about that relation in any single sentenceis very limited.
The idea then is to alleviate this biasby collecting many paraphrases of the same relationinstance into clusters when we train our system.Shinyama generalizes the expressions using part-of-speech information and dependency tree similar-ity into generic templates.
Bunescu?s work uses arelation kernel on subsquences of words developedin (Bunescu and Mooney, 2005).
We observed thatboth approaches suffer from low recall despite theattempts to generalize the subsequences and tem-plates probably because they rely on local contextonly.Based on our observation, we looked for a way touse our clusters without losing non-local informa-tion about the sentences.
Bag-of-words or unigramrepresentations of our paraphrase clusters are easyto compute, but information about word ordering islost.
Hence, we settled on the use of a skip-bigramrepresentation of our relation clusters instead.Skip-bigrams (Lin and Och, 2004) are pairs ofwords in sentence order allowing for gaps in be-tween.
Longer gaps capture far-flung associationsbetween words, while short gaps of between 1 and 4capture local structure.
In using them, we do not re-strict ourselves to context centered around the entitymentions.
Another advantage of using skip-bigramsis that we can capture some extra-sentential infor-mation since we are no longer restricted by the abil-ity to generate dependency structures within a singlesentence as Shinyama is.Using skip-bigrams, we can assess the similarityof a particular new relation mention instance againstthe relation clusters we collect in training.
We canthen compute a likelihood that we combine with thepredictions of the supervised learning algorithm forfinal classification.Two possible extensions to the basic methodstated above were examined.A central problem with the paraphrase collectionapproach when applied to an open corpus is noise.As pointed out by (Bunescu and Mooney, 2007),even though the same entities co-occur in multiplesentences, they are not necessarily linked by thesame relationship in all of them.
The problem isexacerbated when the open corpus we look at con-tains documents from heterogenous domains.
In-deed, we cannot even assume that the predominantrelation that holds between two entities in the set ofsentences is the relation of interest to us.One means of combating this is suggested by(Bunescu and Mooney, 2007).
They re-weight theimportance of word features in their model to re-duce topic drift.
We try a different solution based onthe thematic clustering of sentences.
Sentences ex-tracted from the raw corpus are mapped to a vectorspace and partitioned into different clusters using thePartitioning Around Medoids algorithm (Kaufmannand Rousseeuw, 1987).
Sentences in the clustersclosest to the original relation mention instance aremore likely to embody the same relationship.
Hencewe retain such clusters, while discarding the rest.
Asthe relation we wish to recover may not be the pre-dominant one, the cluster that is retained is also of-ten not the largest one.Another problem identified by Shinyama is thatthe same entity may itself be referred to in differ-ent ways.
If the form used in the original relationmention is uncommon, then few paraphrases will befound.
For instance, ?
?President Bush??
may be re-ferred to as ??Dubya??
by a writer.
Searching forsentences online with the word ??Dubya??
and theother entity participating in the relation is likely toresult in a collection heavily biased towards the orig-inator of the nickname.
Shinyama?s solution is touse a limited form of co-reference resolution to re-351place these forms with a more general noun phrase.As co-reference resolution is itself an unreliable pro-cess, we suggest the use of hypernym substitutioninstead.In subsequent sections, we will outline the struc-ture of our system, examine the experimental evi-dence of its viability using the ACE program data,and finish with a discussion of the extensions.3 Overall structureOur system is organized very naturally into twomain phases, a learning or training phase, followedby a usage or testing phase.
The learning phase issubdivided into two parallel paths, reflecting the hy-brid nature of our algorithm.Fully supervised learning based on annotationstakes place in tandem with a semi-supervised algo-rithm that captures the paraphrase clusters using en-tity mention instances.
We will combine the mod-els from both the supervised learning and the semi-supervised algorithm using a meta-classifier traineda different subset of the data.3.1 Learning ProcedureOur goal is to acquire as much contextual informa-tion from the available annotations as possible viaour supervised learner and expand on that usingWebbased information found by our semi-supervised al-gorithm.We constructed our fully supervised learner ac-cording to the specifications for the system devel-oped by (Zhou et al, 2005).
It utilizes a featurebased framework with a Support Vector Machine(SVM) classifier (Cortes and Vapnik, 1995).
Wesupport the same set of features as Zhou, namely: lo-cal word position features, entity type, base phrasechunking features, dependency and parse tree fea-tures as well as semantic information like coun-try names and a list of trigger words.
In our cur-rent work, we use Michael Collins?
(Collins, 2003)parser for syntactic information.Sentence boundary detection, chunking, and pars-ing are done as preprocessing steps before we beginour learning.Given a sentence with the relation mention in-stance, the semi-supervised method goes through thefollowing five stages:1.
From the list of entitites marked in the sen-tence, generate all possible pairings as candi-dates.
We pick one of these candidates and pro-ceed to the next step.2.
Gather hypernyms for each entity mention us-ing Wordnet synsets and generate all possiblecombinations of entity pairs from the two sets.3.
Find sentences from the Web that mention bothentities.4.
Cluster the sentences found using k-medoids,filter out noise and retain only the cluster thatthe original relation mention would be assignedto.5.
Collate all clusters by relation type and gener-ate a skip-bigram index for each relation type.We will spend the rest of this section on the detailsof each stage.3.1.1 Extracting entity mentions and gatheringhypernymsThe process starts when we receive a relationmention and the sentence it was found in, for in-stance in the following sentence from the ACE 2004corpus: ?As president of Sotheby?s, she often con-ducted the biggest, the highest profile auctions.
?From the annotation, we find that the twoentities of interest are president and Sotheby?sin the EMP-ORG Executive relation, whichwe could represent as the predicate EMP ?ORG Executive(em1, em2).
Since our aim is tofind as many instances of semantically similar sen-tences as possible, we want to lessen the negativeimpact of quirky spelling or naming in a given sen-tence.
Hence we do a hypernym search in Wordnetto find more general terms for our entities and createlist of similarly related entities (em1?, em2?)
etc.
Ifthe mention is a named entity, we do without the hy-pernym expansion and use coreference information(when available) to find the most common substringamongst the mentions for the same entity.In this example, it might result in the four pairsshow in Table 1.352Table 1: Examples of entity pairs after hypernymexpansionPresident Sotheby?sChief Executive Sotheby?sDecision maker Sotheby?sleader Sotheby?sTable 2: Examples of extracted text from GoogleThe 40 year old former presidenttravels incognitio to Sotheby?sBrooks was named president ofSotheby?s Inc.Subsidiary President at Sotheby?s...Bill Ruprecht, chief executive of Sotheby?s,agreed that September 11 had been...The Duke will also remain leaderof Sotheby?s Germany...3.1.2 Web search(Geleijnse and Korst, 2006) use Google as theirsearch engine for extracting surface patterns fromWeb documents.
We use the same procedure hereto find our paraphrases.
For each pair of arguments,we create a boolean query string em1 ?
em2, andsubmit it to Google.
The query will find documentswhere mentions of em1 are separated from em2 byany number of words.
We restrict the language toEnglish.Google returns a two line extract from the docu-ments that match our boolean query.
The extractsare generally those lines where the key query termsare most densely collocated.
These are parsedand obviously nonsensical sentences are discardedbased on the occurrence of words from a list of stopwords.
If a group of sentences are very similar,we choose a single representative and discard therest.
For every remaining sentence, we normalizethem by removing extraneous HTML tags.
Someexamples of extracts found are listed in Table 2.3.1.3 Cluster, filter and collateIn general, the collection of sentence extracts wehave at the end of the previous stage are likely to beabout a diverse range of topics.
As we are only inter-ested in the subset that is most compatible with thethematic content of our original relation mention, wewill have to filter away unrelated extracts.
For exam-ple, the EMP-ORG Executive relation does not holdin the sentence: ?The 40 year old former presidenttravels incognito to Sotheby?s?.We make use of the K-medoids method by (Kauf-mann and Rousseeuw, 1987).
The terms from allsentences are extracted and their frequency in eachsentence is computed.
Each sentence is nowmappedas a vector of frequencies in the space of the termsthat we observed.
The resulting vectors are storedas a large matrix.
Picking a random partition of thesentences as our starting point, we assign some sen-tences to be the cluster centers, and iteratively refinethe clusters based on a distance minimization heuris-tic.Through some preliminary experiments, we findthat K-medoids based clustering with 5 classes pro-duced the most consistent results.
From a list ofexcerpts, our algorithm culls the sentences that be-longed to the 4 irrelevant clusters and producesthe excerpts which capture the original relationshipbest.Since the quality of the partitions produced bythe algorithm is sensitive to the initial random start,we do this process twice with different configura-tions and take the union of the two clusters as ourfinal result.The best excerpts are stored with accompanyingmeta-data about the originating training relation in-stance in what we call pseudo-documents.
We groupthe pseudo-documents in our database by the rela-tion label that the instance pairs were given.
Thuswe end up with several bags of pseudo-documents,where each bag corresponds to a single relation typeof interest.For computational efficiency, we generate an in-verted hash-index for the pseudo-documents.
Ourskip-bigrams act as the keys and the records are listsof meta-data nodes.
Each node records the sentencethat the bigram is observed in, the relation type ofthat sentence, and the position of the bigram.All we need now is a means of measuring the sim-ilarity of a new relation mention instance with thebags of pseudo-documents to assign a relation labelto it.3533.1.4 Skip-bigramsAs discussed in the introduction, instead of gen-eralizing our bags of documents into patterns or re-lation extraction kernels, we create skip-bigram in-dices.
There are several advantages in doing so.Skip-bigrams are easy to compute relative tothe dependency trees or subsequence kernels usedin (Shinyama and Sekine, 2003) or (Bunescu andMooney, 2005).
Moreover, we can tune the num-ber of gaps allowed to capture long-distance worddependency information, which is not done by theother approaches because it is relatively more expen-sive for them to do so, due to the combinatorial ex-plosion.
In addition, as compared to Bunescu?s sub-sequence approach which needs 4 words to match,bigrams are far less likely to be sparse in the docu-ment space.Since we relied upon skip-bigrams in our queriesto Google, it is only natural that we use it again inassessing the similarity of two pseudo-documents.Each pseudo-document is really an extractive sum-mary of online articles about the same topic, withthe same entities.
The degree of overlap betweentwo pseudo-documents is a good measure of theirthematic overlap.Now that we have a metric, that still leaves thequestion of our matching heuristic open.
Do we au-tomatically assign a test instance the relation labelof the sentence with the highest skip-bigram over-lap?
This naive approach is problematic.
In gen-eral, longer sentences will have more bigrams andhence higher probability of overlapping with othersentences.
We could normalize the bigram overlapscore by the length of the sentences, but here weleave the optimization to a machine learner.Another possible heuristic is to pick the relationlabel whose bag has the highest number of match-ing bigrams with our test instance.
Again, this willbe biased, but now towards bags with larger num-bers of pseudo-documents.
A last possibility is tolook at the total number of sentences that have bi-gram matches, and weight the overlap score higherfor those with more sentence matches.Therefore, instead of designing the heuristic ex-plicitly, we use a validation set to observe the statis-tical correlations of each of the three possible heuris-tics we discussed above.
We train an additionalmodel, using an SVM to choose the weights for eachheuristic automatically.Accordingly, we do the following for each valida-tion instance, V, and its pseudo-document P.For each extracted sentence j in pseudo-document P, we look up the database of pseudo-documents from our training set, and compute theskip-bigram similarity with every single sentence.We have a skip-bigram similarity score for every sin-gle sentence in the database with respect to V. Thescores are collated according to the relation classes.For each relation class we generate three numbers,TopS, Matching docs, and Total.
Using the nota-tion by (Lin and Och, 2004), we denote the skip-bigram overlap between two sentences X and Y asSkip2(X,Y ).
For the ith relation, Ci is the set of allpseudo-documents in our training set of that relationtype.TopS = maxY ?Ci,j?PSkip2(Pj , Y ) (1)Matching =?Y ?Ci?j?PI[Skip2(Pj , Y ) > 0] (2)Total =?Y ?Ci?j?PSkip2(Pj , Y ) (3)The three figures provide a summary of the bestsentence level match, and the overall relation leveloverlap in terms of the number of sentences andnumber of overlaps.
As an illustration, we considerthe case where we have two sentences in our pseudo-document P = P1, P2 and a relation RX .
We com-pute Skip2(P1, Y ) and Skip2(P2, Y ) by looking upthe skip-bigrams in the database for RX and aggre-gating over sentences.
Let?s assume that |RX| = 3and only Skip2(P1, Y1) = 2, Skip2(P1, Y3) = 5,Skip2(P2, Y3) = 4 are non-zero.
Then TopS for in-stance V is 5.
Matching will be 2 since only Y1 andY3 have overlaps with elements of P .
The Total issimply 2 + 5 + 4 = 11.3.2 Combining supervised withsemi-supervised modelsAfter the preceding steps, we have a trained SVMmodel based, and our skip-bigram index from thesemi-supervised procedure.
In this section, we willdescribe a method of combining these into a betterclassifier.354The validation data we left aside earlier is sentthrough our system with the relation labels removed.Each entity pair in this validation set has a corre-sponding pseudo-document and a file with numeri-cal features for the SVM model.An instance V is scored by Zhou?s SVM clas-sifier, which assigns a relation tag, VST to it.In parallel, the skip-bigram assessment results in{TopS,Matching, Total} scores for each of therelation classes.
We treat the tag and numbers as fea-tures for training another SVM, which we shall referto as SVMC .
This is our final meta-classifier forrelation extraction on the tenth of the original dataset aside for testing.
The meta-classifier may also beused for completely new data.4 ExperimentationWe use the ACE corpus provided by the LDC from2004 to train and evaluate our system.
There are674 annotated text documents and 9683 relation in-stances in the set.Starting with a single training set provided bythe user, we split that into three parts: the major-ity (80%) is used for the learning phase, one tenthis used for the validation during construction of thecombined model, and the remaining tenth is used fortesting.
We ran a series of experiments, using five-fold cross-validation.Unlike typical cross-validation, the fifth that weset aside is further sub-divided into two parts as westated before.
Half is used when we construct the hy-brid model merging supervised and semi-supervisedpaths, and the remainder is used for the actual testingand evaluation.We used the 6 main relation types defined in theannotation for the ACE dataset: ART, EMP-ORG,GPE-AFF, OTHER-AFF, PER-SOC, and PHYS.Wecomputed three measures of the effectiveness, therecall (R), precision (P) and F-1 score (F-1).4.1 Comparison against the baselineThe first set of experiments we shall discuss com-pares the overall system against our baseline.
Thebaseline system is implemented as a feature extrac-tion module on top of a set of binary SVM classi-fiers.
A primary classifier is used to separate thecandidates without any relation of interest from therest.
Secondary classifiers for each relation type arethen used to partition the positive candidates by vot-ing.
The performance of our baseline classifier whentested on the ACE2003 dataset is statistically indis-tinguishable from that reported by Zhou et al in(Zhou et al, 2005).Drilling down to the level of individual relationclasses as shown below, we note that the meta-classifier performs better than the baseline on all butone of the relations.
This might be due to the inher-ent ambiguity of the OTHER-AFF class.Relation Ratio System R P F-1ART5.6 Hybrid 0.48 0.73 0.59baseline 0.29 0.43 0.34EMP-ORG40.0 Hybrid 0.78 0.75 0.76baseline 0.67 0.83 0.73GPE-AFF11.7 Hybrid 0.49 0.59 0.53baseline 0.36 0.56 0.45OTHER-AFF3.4 Hybrid 0.14 0.18 0.16baseline 0.18 0.59 0.28PER-SOC9.7 Hybrid 0.63 0.80 0.70baseline 0.32 0.5 0.39PHYS29.4 Hybrid 0.75 0.59 0.66baseline 0.40 0.64 0.49The hybrid system has slightly lower precisionon the two largest relation classes, EMP-ORG andPHYS, but higher recall, resulting in better F-scoreson both types.
Finally, note that on the three inter-mediate sized classes, ART, PER-SOC, and GPE-AFF, the recall and precision were both higher.
Theresults suggest that the Web information does im-prove recall substantially, but affects precision incases where there already is a substantial amount oftraining data.
It confirms our original assertion thatthe hybrid approach works well for mid-sized rela-tion classes, where the amount of training data is notenough for the supervised system to perform opti-mally.We use the T-test to see if our improvements overthe baseline were significant at a 0.05 level.
Tosummarize, recall for the PHYS, PER-SOC, GPE-AFF and EMP-ORG relations was improved signif-icantly.
The difference in precision is significant forthe PER-SOC, and OTHER-AFF classes, while F-1score differences for the PER-SOC and PHYS rela-tion classes at 0.00085, 0.032 respectively were bothsignificant.3554.2 Testing hypernym expansion and clusteringSubsequent experiments were aimed at quantifyingthe contribution of hypernym expansion and the-matic clustering to our hybrid system.
We ran a2 factorial experiment with four of our five folds,where we take the hypernym expansion and cluster-ing as treatments.
Since we have more than one fea-ture being tested, and we wish to observe the relativecontribution of each factor, we used an ANOVA testinstead of T-tests (Montgomery, 2005).Our intuitive justification for hypernym expansionwas that recall would be boosted for relation typeswhere the name entities tend to be overly specificin the training corpus.
Place names, personal namesand the object names are obvious targets.
Indeed, wenoted that the recall did increase in absolute termson average for the ART, PER-SOC and PHYS rela-tion types (about 3% for each), but declined slightly(about 0.5%) for the rest.
Overall however, the sizeof the effects was too small to be statistically sig-nificant.
This suggests that other methods of termgeneralization may be needed to achieve a larger ef-fect.Next, we looked at the contribution of clustering.Our initial experiments showed that k-medoids with5 clusters was able to produce very precise clusters.However, it would be at the expense of some of thepotential gain in recall form Web extracts.Our experiments shows that clustering does in-deed lower the potential recall.
However, the hopedfor improvement in precision was observed only inthe PER-SOC (6%) and GPE-AFF (0.7%) relations.This suggests that the effect of name entities havingmultiple relations is concentrated in the classes ofnamed entities related to Persons and GPEs.
Again,the size of the effects was not statistically significant.A more thorough investigation of clustering tech-niques with different settings for k and differentalgorithms will be needed before we can makestronger statements.5 Discussion and ConclusionWe have presented a hybrid approach to relationextraction that incorporates Web based informationsuccessfully to boost the performance of state-of-the-art supervised feature based systems.
Evaluationon the ACE corpus shows that our skip-bigram basedrelevance measures for finding the right paraphrasein our Web extract database are very effective.While our analysis shows that the addition of clus-tering and hypernym expansion to the skip-bigrambased process is not statistically significant, we haveindications that the effect on recall and precision ispositive for certain relation classes.In future work, we will examine improvements tothe clustering algorithm to reduce the impact on re-call.
We will look at alternative ways of attackingthe problem of name entity generalization and assessthe impact of methods like co-reference resolution inthe same ANOVA framework.AcknowledgmentThis research is supported by a Specific Tar-geted Research Project (STREP) of the EuropeanUnion?s 6th Framework Programme within IST call4, Bootstrapping Of Ontologies and TerminologiesSTtragegic REsearch Project (BOOTStrep).ReferencesRazvan C. Bunescu and Raymond J. Mooney.
2005.Subsequence kernels for relation extraction.
In NIPS.Razvan Bunescu and Raymond Mooney.
2007.
Learningto extract relations from the web using minimal su-pervision.
In Proceedings of the 45th Annual Meetingof the Association of Computational Linguistics, pages576?583, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Jinxiu Chen, Donghong Ji, Chew Lim Tan, and Zheng-Yu Niu.
2006.
Relation extraction using label prop-agation based semi-supervised learning.
In ACL.
TheAssociation for Computer Linguistics.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguis-tics, 29(4):589?637.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
volume 20, pages 273?297.George Doddington, Alexis Mitchell, Mark Przybocki,Lance Ramshaw, Stephanie Strassel, and RalphWeischedel.
2004.
The automatic content extraction(ace) program.
tasks, data and evaluation.G.
Geleijnse and J. Korst.
2006.
Learning ef-fective surface text patterns for information ex-traction.
In Proceedings of the EACL 2006workshop on Adaptive Text Extraction and Min-ing (ATEM 2006), pages 1 ?
8, Trento, Italy,356April.
The Association for Computational Linguistics.http://acl.ldc.upenn.edu/eacl2006/ws06 atem.pdf.L.
Kaufmann and P. J. Rousseeuw.
1987.
Clusteringby means of medoids.
In Y.
Dodge, editor, StatisticalData Analysis based on the L1 Norm, pages 405?416,Amsterdam.
Elsevier/North Holland.Chin Y. Lin and Franz J. Och.
2004.
Orange: amethod for evaluating automatic evaluation metricsfor machine translation.
In Proceedings of Col-ing 2004, pages 501?507, Geneva, Switzerland, AugFebruaryMarch?Aug FebruaryJuly.
COLING.Douglas C. Montgomery.
2005.
Design and analysis ofexperiments.
John Wiley And Sons.Yusuke Shinyama and Satoshi Sekine.
2003.
Paraphraseacquisition for information extraction.
In Kentaro Inuiand Ulf Hermjakob, editors, Proceedings of the Sec-ond International Workshop on Paraphrasing, pages65?71.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for relationextraction.
Journal of Machine Learning Research,3:1083?1106.Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, andChew Lim Tan.
2005.
Discovering relations betweennamed entities from a large raw corpus using treesimilarity-based clustering.
In IJCNLP, pages 378?389.Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.2005.
Exploring various knowledge in relation extrac-tion.
In ACL.
The Association for Computer Linguis-tics.357
