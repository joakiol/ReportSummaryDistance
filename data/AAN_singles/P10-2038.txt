Proceedings of the ACL 2010 Conference Short Papers, pages 205?208,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsSimple semi-supervised training of part-of-speech taggersAnders S?gaardCenter for Language TechnologyUniversity of Copenhagensoegaard@hum.ku.dkAbstractMost attempts to train part-of-speech tag-gers on a mixture of labeled and unlabeleddata have failed.
In this work stackedlearning is used to reduce tagging to aclassification task.
This simplifies semi-supervised training considerably.
Ourprefered semi-supervised method com-bines tri-training (Li and Zhou, 2005) anddisagreement-based co-training.
On theWall Street Journal, we obtain an error re-duction of 4.2% with SVMTool (Gimenezand Marquez, 2004).1 IntroductionSemi-supervised part-of-speech (POS) tagging isrelatively rare, and the main reason seems to bethat results have mostly been negative.
Meri-aldo (1994), in a now famous negative result, at-tempted to improve HMM POS tagging by expec-tation maximization with unlabeled data.
Clarket al (2003) reported positive results with littlelabeled training data but negative results whenthe amount of labeled training data increased; thesame seems to be the case in Wang et al (2007)who use co-training of two diverse POS taggers.Huang et al (2009) present positive results forself-training a simple bigram POS tagger, but re-sults are considerably below state-of-the-art.Recently researchers have explored alternativemethods.
Suzuki and Isozaki (2008) introducea semi-supervised extension of conditional ran-dom fields that combines supervised and unsuper-vised probability models by so-called MDF pa-rameter estimation, which reduces error on WallStreet Journal (WSJ) standard splits by about 7%relative to their supervised baseline.
Spoustovaet al (2009) use a new pool of unlabeled datatagged by an ensemble of state-of-the-art taggersin every training step of an averaged perceptronPOS tagger with 4?5% error reduction.
Finally,S?gaard (2009) stacks a POS tagger on an un-supervised clustering algorithm trained on largeamounts of unlabeled data with mixed results.This work combines a new semi-supervisedlearning method to POS tagging, namely tri-training (Li and Zhou, 2005), with stacking on un-supervised clustering.
It is shown that this methodcan be used to improve a state-of-the-art POS tag-ger, SVMTool (Gimenez and Marquez, 2004).
Fi-nally, we introduce a variant of tri-training calledtri-training with disagreement, which seems toperform equally well, but which imports much lessunlabeled data and is therefore more efficient.2 Tagging as classificationThis section describes our dataset and our inputtagger.
We also describe how stacking is used toreduce POS tagging to a classification task.
Fi-nally, we introduce the supervised learning algo-rithms used in our experiments.2.1 DataWe use the POS-tagged WSJ from the Penn Tree-bank Release 3 (Marcus et al, 1993) with thestandard split: Sect.
0?18 is used for training,Sect.
19?21 for development, and Sect.
22?24 fortesting.
Since we need to train our classifiers onmaterial distinct from the training material for ourinput POS tagger, we save Sect.
19 for training ourclassifiers.
Finally, we use the (untagged) Browncorpus as our unlabeled data.
The number of to-kens we use for training, developing and testingthe classifiers, and the amount of unlabeled dataavailable to it, are thus:tokenstrain 44,472development 103,686test 129,281unlabeled 1,170,811205The amount of unlabeled data available to ourclassifiers is thus a bit more than 25 times theamount of labeled data.2.2 Input taggerIn our experiments we use SVMTool (Gimenezand Marquez, 2004) with model type 4 run incre-mentally in both directions.
SVMTool has an ac-curacy of 97.15% on WSJ Sect.
22-24 with thisparameter setting.
Gimenez and Marquez (2004)report that SVMTool has an accuracy of 97.16%with an optimized parameter setting.2.3 Classifier inputThe way classifiers are constructed in our experi-ments is very simple.
We train SVMTool and anunsupervised tagger, Unsupos (Biemann, 2006),on our training sections and apply them to the de-velopment, test and unlabeled sections.
The re-sults are combined in tables that will be the inputof our classifiers.
Here is an excerpt:1Gold standard SVMTool UnsuposDT DT 17NNP NNP 27NNP NNS 17*NNP NNP 17VBD VBD 26Each row represents a word and lists the goldstandard POS tag, the predicted POS tag and theword cluster selected by Unsupos.
For example,the first word is labeled ?DT?, which SVMToolcorrectly predicts, and it belongs to cluster 17 ofabout 500 word clusters.
The first column is blankin the table for the unlabeled section.Generally, the idea is that a classifier will learnto trust SVMTool in some cases, but that it mayalso learn that if SVMTool predicts a certain tagfor some word cluster the correct label is anothertag.
This way of combining taggers into a singleend classifier can be seen as a form of stacking(Wolpert, 1992).
It has the advantage that it re-duces POS tagging to a classification task.
Thismay simplify semi-supervised learning consider-ably.2.4 Learning algorithmsWe assume some knowledge of supervised learn-ing algorithms.
Most of our experiments are im-plementations of wrapper methods that call off-1The numbers provided by Unsupos refer to clusters; ?
*?marks out-of-vocabulary words.the-shelf implementations of supervised learningalgorithms.
Specifically we have experimentedwith support vector machines (SVMs), decisiontrees, bagging and random forests.
Tri-training,explained below, is a semi-supervised learningmethod which requires large amounts of data.Consequently, we only used very fast learning al-gorithms in the context of tri-training.
On the de-velopment section, decisions trees performed bet-ter than bagging and random forests.
The de-cision tree algorithm is the C4.5 algorithm firstintroduced in Quinlan (1993).
We used SVMswith polynomial kernels of degree 2 to provide astronger stacking-only baseline.3 Tri-trainingThis section first presents the tri-training algo-rithm originally proposed by Li and Zhou (2005)and then considers a novel variant: tri-trainingwith disagreement.Let L denote the labeled data and U the unla-beled data.
Assume that three classifiers c1, c2, c3(same learning algorithm) have been trained onthree bootstrap samples of L. In tri-training, anunlabeled datapoint in U is now labeled for a clas-sifier, say c1, if the other two classifiers agree onits label, i.e.
c2 and c3.
Two classifiers informthe third.
If the two classifiers agree on a label-ing, there is a good chance that they are right.The algorithm stops when the classifiers no longerchange.
The three classifiers are combined by ma-jority voting.
Li and Zhou (2005) show that un-der certain conditions the increase in classificationnoise rate is compensated by the amount of newlylabeled data points.The most important condition is that the threeclassifiers are diverse.
If the three classifiers areidentical, tri-training degenerates to self-training.Diversity is obtained in Li and Zhou (2005) bytraining classifiers on bootstrap samples.
In theirexperiments, they consider classifiers based on theC4.5 algorithm, BP neural networks and naiveBayes classifiers.
The algorithm is sketchedin a simplified form in Figure 1; see Li andZhou (2005) for all the details.Tri-training has to the best of our knowledge notbeen applied to POS tagging before, but it has beenapplied to other NLP classification tasks, incl.
Chi-nese chunking (Chen et al, 2006) and questionclassification (Nguyen et al, 2008).2061: for i ?
{1..3} do2: Si ?
bootstrap sample(L)3: ci ?
train classifier(Si)4: end for5: repeat6: for i ?
{1..3} do7: for x ?
U do8: Li ?
?9: if cj(x) = ck(x)(j, k 6= i) then10: Li ?
Li ?
{(x, cj(x)}11: end if12: end for13: ci ?
train classifier(L ?
Li)14: end for15: until none of ci changes16: apply majority vote over ciFigure 1: Tri-training (Li and Zhou, 2005).3.1 Tri-training with disagreementWe introduce a possible improvement of the tri-training algorithm: If we change lines 9?10 in thealgorithm in Figure 1 with the lines:if cj(x) = ck(x) 6= ci(x)(j, k 6= i) thenLi ?
Li ?
{(x, cj(x)}end iftwo classifiers, say c1 and c2, only label a data-point for the third classifier, c3, if c1 and c2 agreeon its label, but c3 disagrees.
The intuition isthat we only want to strengthen a classifier in itsweak points, and we want to avoid skewing ourlabeled data by easy data points.
Finally, since tri-training with disagreement imports less unlabeleddata, it is much more efficient than tri-training.
Noone has to the best of our knowledge applied tri-training with disagreement to real-life classifica-tion tasks before.4 ResultsOur results are presented in Figure 2.
The stackingresult was obtained by training a SVM on top ofthe predictions of SVMTool and the word clustersof Unsupos.
SVMs performed better than deci-sion trees, bagging and random forests on our de-velopment section, but improvements on test datawere modest.
Tri-training refers to the original al-gorithm sketched in Figure 1 with C4.5 as learn-ing algorithm.
Since tri-training degenerates toself-training if the three classifiers are trained onthe same sample, we used our implementation oftri-training to obtain self-training results and vali-dated our results by a simpler implementation.
Wevaried poolsize to optimize self-training.
Finally,we list results for a technique called co-forests (Liand Zhou, 2007), which is a recent alternative totri-training presented by the same authors, and fortri-training with disagreement (tri-disagr).
The p-values are computed using 10,000 stratified shuf-fles.Tri-training and tri-training with disagreementgave the best results.
Note that since tri-trainingleads to much better results than stacking alone,it is unlabeled data that gives us most of the im-provement, not the stacking itself.
The differ-ence between tri-training and self-training is near-significant (p <0.0150).
It seems that tri-trainingwith disagreement is a competitive technique interms of accuracy.
The main advantage of tri-training with disagreement compared to ordinarytri-training, however, is that it is very efficient.This is reflected by the average number of tokensin Li over the three learners in the worst round oflearning:av.
tokens in Litri-training 1,170,811tri-disagr 173Note also that self-training gave very good re-sults.
Self-training was, again, much slower thantri-training with disagreement since we had totrain on a large pool of unlabeled data (but onlyonce).
Of course this is not a standard self-trainingset-up, but self-training informed by unsupervisedword clusters.4.1 Follow-up experimentsSVMTool is one of the most accurate POS tag-gers available.
This means that the predictionsthat are added to the labeled data are of veryhigh quality.
To test if our semi-supervised learn-ing methods were sensitive to the quality of theinput taggers we repeated the self-training andtri-training experiments with a less competitivePOS tagger, namely the maximum entropy-basedPOS tagger first described in (Ratnaparkhi, 1998)that comes with the maximum entropy library in(Zhang, 2004).
Results are presented as the sec-ond line in Figure 2.
Note that error reduction ismuch lower in this case.207BL stacking tri-tr.
self-tr.
co-forests tri-disagr error red.
p-valueSVMTool 97.15% 97.19% 97.27% 97.26% 97.13% 97.27% 4.21% <0.0001MaxEnt 96.31% - 96.36% 96.36% 96.28% 96.36% 1.36% <0.0001Figure 2: Results on Wall Street Journal Sect.
22-24 with different semi-supervised methods.5 ConclusionThis paper first shows how stacking can be used toreduce POS tagging to a classification task.
Thisreduction seems to enable robust semi-supervisedlearning.
The technique was used to improve theaccuracy of a state-of-the-art POS tagger, namelySVMTool.
Four semi-supervised learning meth-ods were tested, incl.
self-training, tri-training, co-forests and tri-training with disagreement.
Allmethods increased the accuracy of SVMTool sig-nificantly.
Error reduction on Wall Street Jour-nal Sect.
22-24 was 4.2%, which is comparableto related work in the literature, e.g.
Suzuki andIsozaki (2008) (7%) and Spoustova et al (2009)(4?5%).ReferencesChris Biemann.
2006.
Unsupervised part-of-speechtagging employing efficient graph clustering.
InCOLING-ACL Student Session, Sydney, Australia.Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.2006.
Chinese chunking with tri-training learn-ing.
In Computer processing of oriental languages,pages 466?473.
Springer, Berlin, Germany.Stephen Clark, James Curran, and Mike Osborne.2003.
Bootstrapping POS taggers using unlabeleddata.
In CONLL, Edmonton, Canada.Jesus Gimenez and Lluis Marquez.
2004.
SVMTool: ageneral POS tagger generator based on support vec-tor machines.
In LREC, Lisbon, Portugal.Zhongqiang Huang, Vladimir Eidelman, and MaryHarper.
2009.
Improving a simple bigram HMMpart-of-speech tagger by latent annotation and self-training.
In NAACL-HLT, Boulder, CO.Ming Li and Zhi-Hua Zhou.
2005.
Tri-training: ex-ploiting unlabeled data using three classifiers.
IEEETransactions on Knowledge and Data Engineering,17(11):1529?1541.Ming Li and Zhi-Hua Zhou.
2007.
Improve computer-aided diagnosis with machine learning techniquesusing undiagnosed samples.
IEEE Transactions onSystems, Man and Cybernetics, 37(6):1088?1098.Mitchell Marcus, Mary Marcinkiewicz, and BeatriceSantorini.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational Linguistics,20(2):155?171.Tri Nguyen, Le Nguyen, and Akira Shimazu.
2008.Using semi-supervised learning for question classi-fication.
Journal of Natural Language Processing,15:3?21.Ross Quinlan.
1993.
Programs for machine learning.Morgan Kaufmann.Adwait Ratnaparkhi.
1998.
Maximum entropy mod-els for natural language ambiguity resolution.
Ph.D.thesis, University of Pennsylvania.Anders S?gaard.
2009.
Ensemble-based POS taggingof italian.
In IAAI-EVALITA, Reggio Emilia, Italy.Drahomira Spoustova, Jan Hajic, Jan Raab, andMiroslav Spousta.
2009.
Semi-supervised trainingfor the averaged perceptron POS tagger.
In EACL,Athens, Greece.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedsequential labeling and segmentation using giga-word scale unlabeled data.
In ACL, pages 665?673,Columbus, Ohio.Wen Wang, Zhongqiang Huang, and Mary Harper.2007.
Semi-supervised learning for part-of-speechtagging of Mandarin transcribed speech.
In ICASSP,Hawaii.David Wolpert.
1992.
Stacked generalization.
NeuralNetworks, 5:241?259.Le Zhang.
2004.
Maximum entropy modeling toolkitfor Python and C++.
University of Edinburgh.208
