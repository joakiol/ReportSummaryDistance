Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 89?96,Sydney, July 2006. c?2006 Association for Computational LinguisticsUnsupervised Relation Disambiguation Using Spectral ClusteringJinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu11Institute for Infocomm Research 2Department of Computer Science21 Heng Mui Keng Terrace National University of Singapore119613 Singapore 117543 Singapore{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sgAbstractThis paper presents an unsupervised learn-ing approach to disambiguate various rela-tions between name entities by use of vari-ous lexical and syntactic features from thecontexts.
It works by calculating eigen-vectors of an adjacency graph?s Laplacianto recover a submanifold of data from ahigh dimensionality space and then per-forming cluster number estimation on theeigenvectors.
Experiment results on ACEcorpora show that this spectral cluster-ing based approach outperforms the otherclustering methods.1 IntroductionIn this paper, we address the task of relation extrac-tion, which is to find relationships between name en-tities in a given context.
Many methods have beenproposed to deal with this task, including supervisedlearning algorithms (Miller et al, 2000; Zelenko etal., 2002; Culotta and Soresen, 2004; Kambhatla,2004; Zhou et al, 2005), semi-supervised learn-ing algorithms (Brin, 1998; Agichtein and Gravano,2000; Zhang, 2004), and unsupervised learning al-gorithm (Hasegawa et al, 2004).Among these methods, supervised learning is usu-ally more preferred when a large amount of la-beled training data is available.
However, it istime-consuming and labor-intensive to manually taga large amount of training data.
Semi-supervisedlearning methods have been put forward to mini-mize the corpus annotation requirement.
Most ofsemi-supervised methods employ the bootstrappingframework, which only need to pre-define some ini-tial seeds for any particular relation, and then boot-strap from the seeds to acquire the relation.
How-ever, it is often quite difficult to enumerate all classlabels in the initial seeds and decide an ?optimal?number of them.Compared with supervised and semi-supervisedmethods, Hasegawa et al (2004)?s unsupervised ap-proach for relation extraction can overcome the dif-ficulties on requirement of a large amount of labeleddata and enumeration of all class labels.
Hasegawaet al (2004)?s method is to use a hierarchical cluster-ing method to cluster pairs of named entities accord-ing to the similarity of context words intervening be-tween the named entities.
However, the drawback ofhierarchical clustering is that it required providingcluster number by users.
Furthermore, clustering isperformed in original high dimensional space, whichmay induce non-convex clusters hard to identified.This paper presents a novel application of spec-tral clustering technique to unsupervised relation ex-traction problem.
It works by calculating eigenvec-tors of an adjacency graph?s Laplacian to recover asubmanifold of data from a high dimensional space,and then performing cluster number estimation ona transformed space defined by the first few eigen-vectors.
This method may help us find non-convexclusters.
It also does not need to pre-define the num-ber of the context clusters or pre-specify the simi-larity threshold for the clusters as Hasegawa et al(2004)?s method.The rest of this paper is organized as follows.
Sec-tion 2 formulates unsupervised relation extractionand presents how to apply the spectral clustering89technique to resolve the task.
Then section 3 reportsexperiments and results.
Finally we will give a con-clusion about our work in section 4.2 Unsupervised Relation ExtractionProblemAssume that two occurrences of entity pairs withsimilar contexts, are tend to hold the same relationtype.
Thus unsupervised relation extraction prob-lem can be formulated as partitioning collections ofentity pairs into clusters according to the similarityof contexts, with each cluster containing only entitypairs labeled by the same relation type.
And then, ineach cluster, the most representative words are iden-tified from the contexts of entity pairs to induce thelabel of relation type.
Here, we only focus on theclustering subtask and do not address the relationtype labeling subtask.In the next subsections we will describe our pro-posed method for unsupervised relation extraction,which includes: 1) Collect the context vectors inwhich the entity mention pairs co-occur; 2) Clusterthese Context vectors.2.1 Context Vector and Feature DesignLet X = {xi}ni=1 be the set of context vectors of oc-currences of all entity mention pairs, where xi repre-sents the context vector of the i-th occurrence, and nis the total number of occurrences of all entity men-tion pairs.Each occurrence of entity mention pairs can bedenoted as follows:R ?
(Cpre, e1, Cmid, e2, Cpost) (1)where e1 and e2 represents the entity mentions, andCpre,Cmid,and Cpost are the contexts before, be-tween and after the entity mention pairs respectively.We extracted features from e1, e2, Cpre, Cmid,Cpost to construct context vectors, which are com-puted from the parse trees derived from CharniakParser (Charniak, 1999) and the Chunklink script 1written by Sabine Buchholz from Tilburg University.Words: Words in the two entities and three contextwindows.1 Software available at http://ilk.uvt.nl/ sabine/chunklink/Entity Type: the entity type of both entities, whichcan be PERSON, ORGANIZATION, FACIL-ITY, LOCATION and GPE.POS features: Part-Of-Speech tags correspondingto all words in the two entities and three con-text windows.Chunking features: This category of features areextracted from the chunklink representation,which includes:?
Chunk tag information of the two enti-ties and three context windows.
The ?0?tag means that the word is outside of anychunk.
The ?I-XP?
tag means that thisword is inside an XP chunk.
The ?B-XP?by default means that the word is at thebeginning of an XP chunk.?
Grammatical function of the two entitiesand three context windows.
The last wordin each chunk is its head, and the functionof the head is the function of the wholechunk.
?NP-SBJ?
means a NP chunk asthe subject of the sentence.
The otherwords in a chunk that are not the head have?NOFUNC?
as their function.?
IOB-chains of the heads of the two enti-ties.
So-called IOB-chain, noting the syn-tactic categories of all the constituents onthe path from the root node to this leafnode of tree.We combine the above lexical and syntactic fea-tures with their position information in the contextto form the context vector.
Before that, we filter outlow frequency features which appeared only once inthe entire set.2.2 Context ClusteringOnce the context vectors of entity pairs are prepared,we come to the second stage of our method: clusterthese context vectors automatically.In recent years, spectral clustering technique hasreceived more and more attention as a powerful ap-proach to a range of clustering problems.
Amongthe efforts on spectral clustering techniques (Weiss,1999; Kannan et al, 2000; Shi et al, 2000; Ng et al,2001; Zha et al, 2001), we adopt a modified version90Table 1: Context Clustering with Spectral-based Clusteringtechnique.Input: A set of context vectors X = {x1, x2, ..., xn},X ?
<n?d;Output: Clustered data and number of clusters;1.
Construct an affinity matrix by Aij = exp(?
s2ij?2 ) if i 6=j, 0 if i = j.
Here, sij is the similarity between xi andxj calculated by Cosine similarity measure.
and the freedistance parameter ?2 is used to scale the weights;2.
Normalize the affinity matrix A to create the matrix L =D?1/2AD?1/2, where D is a diagonal matrix whose (i,i)element is the sum of A?s ith row;3.
Set q = 2;4.
Compute q eigenvectors of L with greatest eigenvalues.Arrange them in a matrix Y .5.
Perform elongated K-means with q + 1 centers on Y ,initializing the (q + 1)-th mean in the origin;6.
If the q+1-th cluster contains any data points, then theremust be at least an extra cluster; set q = q + 1 and goback to step 4.
Otherwise, algorithm stops and outputsclustered data and number of clusters.
(Sanguinetti et al, 2005) of the algorithm by Ng etal.
(2001) because it can provide us model order se-lection capability.Since we do not know how many relation typesin advance and do not have any labeled relationtraining examples at hand, the problem of modelorder selection arises, i.e.
estimating the ?opti-mal?
number of clusters.
Formally, let k be themodel order, we need to find k in Equation: k =argmaxk{criterion(k)}.
Here, the criterion is de-fined on the result of spectral clustering.Table 1 shows the details of the whole algorithmfor context clustering, which contains two mainstages: 1) Transformation of Clustering Space (Step1-4); 2) Clustering in the transformed space usingElongated K-means algorithm (Step 5-6).2.3 Transformation of Clustering SpaceWe represent each context vector of entity pair as anode in an undirected graph.
Each edge (i,j) in thegraph is assigned a weight that reflects the similaritybetween two context vectors i and j.
Hence, the re-lation extraction task for entity pairs can be definedas a partition of the graph so that entity pairs thatare more similar to each other, e.g.
labeled by thesame relation type, belong to the same cluster.
As arelaxation of such NP-hard discrete graph partition-ing problem, spectral clustering technique computeseigenvalues and eigenvectors of a Laplacian matrixrelated to the given graph, and construct data clus-ters based on such spectral information.Thus the starting point of context clustering is toconstruct an affinity matrix A from the data, whichis an n ?
n matrix encoding the distances betweenthe various points.
The affinity matrix is then nor-malized to form a matrix L by conjugating with thethe diagonal matrix D?1/2 which has as entries thesquare roots of the sum of the rows of A.
This is totake into account the different spread of the variousclusters (points belonging to more rarified clusterswill have lower sums of the corresponding row ofA).
It is straightforward to prove that L is positivedefinite and has eigenvalues smaller or equal to 1,with equality holding in at least one case.Let K be the true number of clusters present inthe dataset.
If K is known beforehand, the first Keigenvectors of L will be computed and arranged ascolumns in a matrix Y .
Each row of Y correspondsto a context vector of entity pair, and the above pro-cess can be considered as transforming the originalcontext vectors in a d-dimensional space to new con-text vectors in the K-dimensional space.
Therefore,the rows of Y will cluster upon mutually orthogonalpoints on the K dimensional sphere,rather than onthe coordinate axes.2.4 The Elongated K-means algorithmAs the step 5 of Table 1 shows, the result of elon-gated K-means algorithm is used to detect whetherthe number of clusters selected q is less than the truenumber K, and allows one to iteratively obtain thenumber of clusters.Consider the case when the number of clusters qis less than the true cluster number K present in thedataset.
In such situation, taking the first q < Keigenvectors, we will be selecting a q-dimensionalsubspace in the clustering space.
As the rows of theK eigenvectors clustered along mutually orthogo-nal vectors, their projections in a lower dimensionalspace will cluster along radial directions.
Therefore,the general picture will be of q clusters elongated inthe radial direction, with possibly some clusters verynear the origin (when the subspace is orthogonal tosome of the discarded eigenvectors).Hence, the K-means algorithm is modified asthe elongated K-means algorithm to downweightdistances along radial directions and penalize dis-91-4 -3 -2 -1 0 1 2 3 4-4-3-2-101234(a)-4 -3 -2 -1 0 1 2 3 4-4-3-2-101234(b)0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08-0.08-0.06-0.04-0.0200.020.040.060.080.1(c)-4 -3 -2 -1 0 1 2 3 4-4-3-2-101234(d)Figure 1: An Example:(a) The Three Circle Dataset.
(b) The clustering result using K-means; (c) Threeelongated clusters in the 2D clustering space usingSpectral clustering: two dominant eigenvectors; (d)The clustering result using Spectral-based clustering(?2=0.05).
(4,?
and + denote examples in differentclusters)tances along transversal directions.
The elongatedK-means algorithm computes the distance of pointx from the center ci as follows:?
If the center is not very near the origin, cTi ci > ?
(?
is aparameter to be fixed by the user), the distances are cal-culated as: edist(x, ci) = (x ?
ci)TM(x ?
ci), whereM = 1?
(Iq ?cicTicTi ci) + ?
cicTicTi ci, ?
is the sharpness param-eter that controls the elongation (the smaller, the moreelongated the clusters) 2.?
If the center is very near the origin,cTi ci < ?, the dis-tances are measured using the Euclidean distance.In each iteration of procedure in Table 1, elon-gated K-means is initialized with q centers corre-sponding to data points in different clusters and onecenter in the origin.
The algorithm then will drag thecenter in the origin towards one of the clusters notaccounted for.
Compute another eigenvector (thusincreasing the dimension of the clustering space toq + 1) and repeat the procedure.
Eventually, whenone reach as many eigenvectors as the number ofclusters present in the data, no points will be as-signed to the center at the origin, leaving the clusterempty.
This is the signal to terminate the algorithm.2.5 An exampleFigure 1 visualized the clustering result of three cir-cle dataset using K-means and Spectral-based clus-tering.
From Figure 1(b), we can see that K-meanscan not separate the non-convex clusters in three cir-cle dataset successfully since it is prone to local min-imal.
For spectral-based clustering, as the algorithmdescribed, initially, we took the two eigenvectors ofL with largest eigenvalues, which gave us a two-dimensional clustering space.
Then to ensure thatthe two centers are initialized in different clusters,one center is set as the point that is the farthest fromthe origin, while the other is set as the point thatsimultaneously farthest the first center and the ori-gin.
Figure 1(c) shows the three elongated clusters inthe 2D clustering space and the corresponding clus-tering result of dataset is visualized in Figure 1(d),which exploits manifold structure (cluster structure)in data.2 In this paper, the sharpness parameter ?
is set to 0.292Table 2: Frequency of Major Relation SubTypes in the ACEtraining and devtest corpus.Type SubType Training DevtestROLE General-Staff 550 149Management 677 122Citizen-Of 127 24Founder 11 5Owner 146 15Affiliate-Partner 111 15Member 460 145Client 67 13Other 15 7PART Part-Of 490 103Subsidiary 85 19Other 2 1AT Located 975 192Based-In 187 64Residence 154 54SOC Other-Professional 195 25Other-Personal 60 10Parent 68 24Spouse 21 4Associate 49 7Other-Relative 23 10Sibling 7 4GrandParent 6 1NEAR Relative-Location 88 323 Experiments and Results3.1 Data SettingOur proposed unsupervised relation extraction isevaluated on ACE 2003 corpus, which contains 519files from sources including broadcast, newswire,and newspaper.
We only deal with intra-sentenceexplicit relations and assumed that all entities havebeen detected beforehand in the EDT sub-task ofACE.
To verify our proposed method, we only col-lect those pairs of entity mentions which have beentagged relation types in the given corpus.
Then therelation type tags were removed to test the unsuper-vised relation disambiguation.
During the evalua-tion procedure, the relation type tags were used asground truth classes.
A break-down of the data by24 relation subtypes is given in Table 2.3.2 Evaluation method for clustering resultWhen assessing the agreement between clusteringresult and manually annotated relation types (groundtruth classes), we would encounter the problem thatthere was no relation type tags for each cluster in ourclustering results.To resolve the problem, we construct a contin-gency table T , where each entry ti,j gives the num-ber of the instances that belong to both the i-th es-timated cluster and j-th ground truth class.
More-over, to ensure that any two clusters do not sharethe same labels of relation types, we adopt a per-mutation procedure to find an one-to-one mappingfunction ?
from the ground truth classes (relationtypes) TC to the estimated clustering result EC.There are at most |TC| clusters which are assignedrelation type tags.
And if the number of the esti-mated clusters is less than the number of the groundtruth clusters, empty clusters should be added so that|EC| = |TC| and the one-to-one mapping can beperformed, which can be formulated as the function:??
= argmax?
?|TC|j=1 t?
(j),j , where ?
(j) is the in-dex of the estimated cluster associated with the j-thclass.Given the result of one-to-one mapping, we adoptPrecision, Recall and F-measure to evaluate theclustering result.3.3 Experimental DesignWe perform our unsupervised relation extraction onthe devtest set of ACE corpus and evaluate the al-gorithm on relation subtype level.
Firstly, we ob-serve the influence of various variables, includingDistance Parameter ?2, Different Features, ContextWindow Size.
Secondly, to verify the effectivenessof our method, we further compare it with other twounsupervised methods.3.3.1 Choice of Distance Parameter ?2We simply search over ?2 and pick the valuethat finds the best aligned set of clusters on thetransformed space.
Here, the scattering criteriontrace(P?1W PB) is used to compare the cluster qual-ity for different value of ?2 3, which measures the ra-tio of between-cluster to within-cluster scatter.
Thehigher the trace(P?1W PB), the higher the clusterquality.In Table 3 and Table 4, with different settings offeature set and context window size, we find out the3 trace(P?1W PB) is trace of a matrix which is the sum ofits diagonal elements.
PW is the within-cluster scatter matrixas: PW =?cj=1?Xi?
?j (Xi ?
mj)(Xi ?
mj)t and PBis the between-cluster scatter matrix as: PB =?cj=1(mj ?m)(mj ?
m)t, where m is the total mean vector and mj isthe mean vector for jth cluster and (Xj ?
mj)t is the matrixtranspose of the column vector (Xj ?mj).93Table 3: Contribution of Different FeaturesFeatures ?2 cluster number trace value Precison Recall F-measureWords 0.021 15 2.369 41.6% 30.2% 34.9%+Entity Type 0.016 18 3.198 40.3% 42.5% 41.5%+POS 0.017 18 3.206 37.8% 46.9% 41.8%+Chunking Infomation 0.015 19 3.900 43.5% 49.4% 46.3%Table 4: Different Context Window Size SettingContext Window Size ?2 cluster number trace value Precision Recall F-measure0 0.016 18 3.576 37.6% 48.1% 42.2%2 0.015 19 3.900 43.5% 49.4% 46.3%5 0.020 21 2.225 29.3% 34.7% 31.7%corresponding value of ?2 and cluster number whichmaximize the trace value in searching for a range ofvalue ?2.3.3.2 Contribution of Different FeaturesAs the previous section presented, we incorporatevarious lexical and syntactic features to extract rela-tion.
To measure the contribution of different fea-tures, we report the performance by gradually in-creasing the feature set, as Table 3 shows.Table 3 shows that all of the four categories of fea-tures contribute to the improvement of performancemore or less.
Firstly,the addition of entity type fea-ture is very useful, which improves F-measure by6.6%.
Secondly, adding POS features can increaseF-measure score but do not improve very much.Thirdly, chunking features also show their great use-fulness with increasing Precision/Recall/F-measureby 5.7%/2.5%/4.5%.We combine all these features to do all other eval-uations in our experiments.3.3.3 Setting of Context Window SizeWe have mentioned in Section 2 that the contextvectors of entity pairs are derived from the contextsbefore, between and after the entity mention pairs.Hence, we have to specify the three context windowsize first.
In this paper, we set the mid-context win-dow as everything between the two entity mentions.For the pre- and post- context windows, we couldhave different choices.
For example, if we specifythe outer context window size as 2, then it means thatthe pre-context (post-context)) includes two wordsbefore (after) the first (second) entity.For comparison of the effect of the outer contextof entity mention pairs, we conducted three differentTable 5: Performance of our proposed method (Spectral-based clustering) compared with other unsupervised methods:((Hasegawa et al, 2004))?s clustering method and K-meansclustering.Precision Recall F-measureHasegawa?s Method1 38.7% 29.8% 33.7%Hasegawa?s Method2 37.9% 36.0% 36.9%Kmeans 34.3% 40.2% 36.8%Our Proposed Method 43.5% 49.4% 46.3%settings of context window size (0, 2, 5) as Table 4shows.
From this table we can find that with the con-text window size setting, 2, the algorithm achievesthe best performance of 43.5%/49.4%/46.3% inPrecision/Recall/F-measure.
With the context win-dow size setting, 5, the performance becomes worsebecause extending the context too much may includemore features, but at the same time, the noise alsoincreases.3.3.4 Comparison with other UnsupervisedmethodsIn (Hasegawa et al, 2004), they preformed un-supervised relation extraction based on hierarchicalclustering and they only used word features betweenentity mention pairs to construct context vectors.
Wereported the clustering results using the same clus-tering strategy as Hasegawa et al (2004) proposed.In Table 5, Hasegawa?s Method1 means the test usedthe word feature as Hasegawa et al (2004) whileHasegawa?s Method2 means the test used the samefeature set as our method.
In both tests, we specifiedthe cluster number as the number of ground truthclasses.We also approached the relation extraction prob-lem using the standard clustering technique, K-94means, where we adopted the same feature set de-fined in our proposed method to cluster the con-text vectors of entity mention pairs and pre-specifiedthe cluster number as the number of ground truthclasses.Table 5 reports the performance of our proposedmethod comparing with the other two unsupervisedmethods.
Table 5 shows our proposed spectral basedmethod clearly outperforms the other two unsuper-vised methods by 12.5% and 9.5% in F-measure re-spectively.
Moreover, the incorporation of variouslexical and syntactic features into Hasegawa et al(2004)?s method2 makes it outperform Hasegawa etal.
(2004)?s method1 which only uses word feature.3.4 DiscussionIn this paper, we have shown that the modified spec-tral clustering technique, with various lexical andsyntactic features derived from the context of entitypairs, performed well on the unsupervised relationextraction problem.
Our experiments show that bythe choice of the distance parameter ?2, we can esti-mate the cluster number which provides the tightestclusters.
We notice that the estimated cluster num-ber is less than the number of ground truth classesin most cases.
The reason for this phenomenon maybe that some relation types can not be easily distin-guished using the context information only.
For ex-ample, the relation subtypes ?Located?, ?Based-In?and ?Residence?
are difficult to disambiguate evenfor human experts to differentiate.The results also show that various lexical andsyntactic features contain useful information for thetask.
Especially, although we did not concern thedependency tree and full parse tree information asother supervised methods (Miller et al, 2000; Cu-lotta and Soresen, 2004; Kambhatla, 2004; Zhou etal., 2005), the incorporation of simple features, suchas words and chunking information, still can providecomplement information for capturing the character-istics of entity pairs.
This perhaps dues to the factthat two entity mentions are close to each other inmost of relations defined in ACE.
Another observa-tion from the result is that extending the outer con-text window of entity mention pairs too much maynot improve the performance since the process mayincorporate more noise information and affect theclustering result.As regards the clustering technique, the spectral-based clustering performs better than direct cluster-ing, K-means.
Since the spectral-based algorithmworks in a transformed space of low dimension-ality, data can be easily clustered so that the al-gorithm can be implemented with better efficiencyand speed.
And the performance using spectral-based clustering can be improved due to the reasonthat spectral-based clustering overcomes the draw-back of K-means (prone to local minima) and mayfind non-convex clusters consistent with human in-tuition.Generally, from the point of view of unsu-pervised resolution for relation extraction, ourapproach already achieves best performance of43.5%/49.4%/46.3% in Precision/Recall/F-measurecompared with other clustering methods.4 Conclusion and Future workIn this paper, we approach unsupervised relation ex-traction problem by using spectral-based clusteringtechnique with diverse lexical and syntactic featuresderived from context.
The advantage of our methodis that it doesn?t need any manually labeled relationinstances, and pre-definition the number of the con-text clusters.
Experiment results on the ACE corpusshow that our method achieves better performancethan other unsupervised methods, i.e.Hasegawa etal.
(2004)?s method and Kmeans-based method.Currently we combine various lexical and syn-tactic features to construct context vectors for clus-tering.
In the future we will further explore othersemantic information to assist the relation extrac-tion problem.
Moreover, instead of cosine similar-ity measure to calculate the distance between con-text vectors, we will try other distributional similar-ity measures to see whether the performance of re-lation extraction can be improved.
In addition, if wecan find an effective unsupervised way to filter outunrelated entity pairs in advance, it would make ourproposed method more practical.ReferencesAgichtein E. and Gravano L.. 2000.
Snowball: Ex-tracting Relations from large Plain-Text Collections,In Proc.
of the 5th ACM International Conference onDigital Libraries (ACMDL?00).95Brin Sergey.
1998.
Extracting patterns and relationsfrom world wide web.
In Proc.
of WebDB Workshop at6th International Conference on Extending DatabaseTechnology (WebDB?98).
pages 172-183.Charniak E.. 1999.
A Maximum-entropy-inspired parser.Technical Report CS-99-12.. Computer Science De-partment, Brown University.Culotta A. and Soresen J.
2004.
Dependency tree kernelsfor relation extraction, In proceedings of 42th AnnualMeeting of the Association for Computational Linguis-tics.
21-26 July 2004.
Barcelona, Spain.Defense Advanced Research Projects Agency.
1995.Proceedings of the Sixth Message Understanding Con-ference (MUC-6) Morgan Kaufmann Publishers, Inc.Hasegawa Takaaki, Sekine Satoshi and Grishman Ralph.2004.
Discovering Relations among Named Enti-ties from Large Corpora, Proceeding of ConferenceACL2004.
Barcelona, Spain.Kambhatla N. 2004.
Combining lexical, syntactic andsemantic features with Maximum Entropy Models forextracting relations, In proceedings of 42th AnnualMeeting of the Association for Computational Linguis-tics.
21-26 July 2004.
Barcelona, Spain.Kannan R., Vempala S., and Vetta A.. 2000.
On cluster-ing: Good,bad and spectral.
In Proceedings of the 41stFoundations of Computer Science.
pages 367-380.Miller S.,Fox H.,Ramshaw L. and Weischedel R. 2000.A novel use of statistical parsing to extract informationfrom text.
In proceedings of 6th Applied Natural Lan-guage Processing Conference.
29 April-4 may 2000,Seattle USA.Ng Andrew.Y, Jordan M., and Weiss Y.. 2001.
On spec-tral clustering: Analysis and an algorithm.
In Pro-ceedings of Advances in Neural Information Process-ing Systems.
pages 849-856.Sanguinetti G., Laidler J. and Lawrence N.. 2005.
Au-tomatic determination of the number of clusters us-ing spectral algorithms.In: IEEE Machine Learningfor Signal Processing.
28-30 Sept 2005, Mystic, Con-necticut, USA.Shi J. and Malik.J.
2000.
Normalized cuts and imagesegmentation.
IEEE Transactions on Pattern Analysisand Machine Intelligence.
22(8):888-905.Weiss Yair.
1999.
Segmentation using eigenvectors: Aunifying view.
ICCV(2).
pp.975-982.Zelenko D., Aone C. and Richardella A.. 2002.
Ker-nel Methods for Relation Extraction, Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP).
Philadelphia.Zha H.,Ding C.,Gu.M,He X.,and Simon H.. 2001.
Spec-tral Relaxation for k-means clustering.
In Neural In-formation Processing Systems (NIPS2001).
pages1057-1064, 2001.Zhang Zhu.
2004.
Weakly-supervised relation classifi-cation for Information Extraction, In proceedings ofACM 13th conference on Information and KnowledgeManagement (CIKM?2004).
8-13 Nov 2004.
Wash-ington D.C.,USA.Zhou GuoDong, Su Jian, Zhang Jie and Zhang min.2005.
Exploring Various Knowledge in Relation Ex-traction, In proceedings of 43th Annual Meeting of theAssociation for Computational Linguistics.
USA.96
