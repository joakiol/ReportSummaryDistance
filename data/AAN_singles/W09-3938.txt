Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 272?275,Queen Mary University of London, September 2009. c?2009 Association for Computational Linguisticsk-Nearest Neighbor Monte-Carlo Control Algorithmfor POMDP-based Dialogue SystemsF.
Lefe`vre?, M.
Gas?ic?, F.
Jurc??
?c?ek, S. Keizer, F. Mairesse, B. Thomson, K. Yu and S. YoungSpoken Dialogue Systems GroupCambridge University Engineering DepartmentTrumpington Street, Cambridge CB2 1PZ, UK{frfl2, mg436, fj228, sk561, farm2, brmt2, ky219, sjy}@eng.cam.ac.ukAbstractIn real-world applications, modelling di-alogue as a POMDP requires the use ofa summary space for the dialogue staterepresentation to ensure tractability.
Sub-optimal estimation of the value func-tion governing the selection of system re-sponses can then be obtained using a grid-based approach on the belief space.
Inthis work, the Monte-Carlo control tech-nique is extended so as to reduce trainingover-fitting and to improve robustness tosemantic noise in the user input.
This tech-nique uses a database of belief vector pro-totypes to choose the optimal system ac-tion.
A locally weighted k-nearest neigh-bor scheme is introduced to smooth the de-cision process by interpolating the valuefunction, resulting in higher user simula-tion performance.1 IntroductionIn the last decade dialogue modelling as a PartiallyObservable Markov Decision Process (POMDP)has been proposed as a convenient way to improvespoken dialogue systems (SDS) trainability, nat-uralness and robustness to input errors (Young etal., 2009).
The POMDP framework models dia-logue flow as a sequence of unobserved dialoguestates following stochastic moves, and provides aprincipled way to model uncertainty.However, to deal with uncertainty, POMDPsmaintain distributions over all possible states.
Butthen training an optimal policy is an NP hardproblem and thus not tractable for any non-trivialapplication.
In recent works this issue is ad-dressed by mapping the dialog state representation?Fabrice Lefe`vre is currently on leave from the Univer-sity of Avignon, France.space (the master space) into a smaller summaryspace (Williams and Young, 2007).
Even thoughoptimal policies remain out of reach, sub-optimalsolutions can be found by means of grid-based al-gorithms.Within the Hidden Information State (HIS)framework (Young et al, 2009), policies are rep-resented by a set of grid points in the summary be-lief space.
Beliefs in master space are first mappedinto summary space and then mapped into a sum-mary action via the dialogue policy.
The resultingsummary action is then mapped back into masterspace and output to the user.Methods which support interpolation betweenpoints are generally required to scale well to largestate spaces (Pineau et al, 2003).
In the currentversion of the HIS framework, the policy choosesthe system action by associating each new beliefpoint with the single, closest, grid point.
In thepresent work, a k-nearest neighbour extension isevaluated in which the policy decision is based ona locally weighted regression over a subset of rep-resentative grid points.
This method thus lies be-tween a strictly grid-based and a point-based valueiteration approach as it interpolates the value func-tion around the queried belief point.
It thus re-duces the policy?s dependency on the belief gridpoint selection and increases robustness to inputnoise.The next section gives an overview of theCUED HIS POMDP dialogue system which weextended for our experiments.
In Section 3, thegrid-based approach to policy optimisation is in-troduced followed by a presentation of the k-nn Monte-Carlo policy optimization in Section 4,along with an evaluation on a simulated user.2722 The CUED Spoken Dialogue System2.1 System ArchitectureThe CUED HIS-based dialogue system pipelinesfive modules: the ATK speech recogniser, anSVM-based semantic tuple classifier, a POMDPdialogue manager, a natural language generator,and an HMM-based speech synthesiser.
Duringan interaction with the system, the user?s speechis first decoded by the recogniser and an N-bestlist of hypotheses is sent to the semantic classifier.In turn the semantic classifier outputs an N-bestlist of user dialogue acts.
A dialogue act is a se-mantic representation of the user action headed bythe user intention (such as inform, request,etc) followed by a list of items (slot-value pairssuch as type=hotel, area=east etc).
TheN-best list of dialogue acts is used by the dialoguemanager to update the dialogue state.
Based onthe state hypotheses and the policy, a machine ac-tion is determined, again in the form of a dialogueact.
The natural language generator translates themachine action into a sentence, finally convertedinto speech by the HMM synthesiser.
The dia-logue system is currently developed for a touristinformation domain (Towninfo).
It is worth not-ing that the dialogue manager does not contain anydomain-specific knowledge.2.2 HIS Dialogue ManagerThe unobserved dialogue state of the HIS dialoguemanager consists of the user goal, the dialogue his-tory and the user action.
The user goal is repre-sented by a partition which is a tree structure builtaccording to the domain ontology.
The nodes inthe partition consist mainly of slots and values.When querying the venue database using the par-tition, a set of matching entities can be produced.The dialogue history consists of the groundingstates of the nodes in the partition, generated us-ing a finite state automaton and the previous userand system action.
A hypothesis in the HIS ap-proach is then a triple combining a partition, a useraction and the respective set of grounding states.The distribution over all hypotheses is maintainedthroughout the dialogue (belief state monitoring).Considering the ontology size for any real-worldproblem, the so-defined state space is too large forany POMDP learning algorithm.
Hence to obtain atractable policy, the state/action space needs to bereduced to a smaller scale summary space.
The setof possible machine dialogue acts is also reducedin summary space.
This is mainly achieved by re-Master SpaceMasters Sppp c  uamypppeus Sers Sppp us SamypppMMMMast er S prcr S pr uSr pSm MaytSummary SpaceMaster Spcu rmymtycumaycry rcsasymtyarFigure 1: Master-summary Space Mapping.moving all act items and leaving only a reduced setof dialogue act types.
When mapping back intomaster space, the necessary items (i.e.
slot-valuepairs) are inferred by inspecting the most likelydialogue state hypotheses.The optimal policy is obtained using reinforce-ment learning in interaction with an agenda basedsimulated user (Schatzmann et al, 2007).
At theend of each dialogue a reward is given to the sys-tem: +20 for a successful completion and -1 foreach turn.
A grid-based optimisation is used to ob-tain the optimal policy (see next section).
At eachturn the belief is mapped to a summary point fromwhich a summary action can be determined.
Thesummary action is then mapped back to a masteraction by adding the relevant information.3 Grid-based Policy OptimisationIn a POMDP, the optimal exact value function canbe found iteratively from the terminal state in aprocess called value iteration.
At each iterationt, policy vectors are generated for all possible ac-tion/observation pairs and their corresponding val-ues are computed in terms of the policy vectorsat step t ?
1.
However, exact optimisation isnot tractable in practice, but approximate solutionscan still provide useful policies.
Representing aPOMDP policy by a grid of representative beliefpoints yields an MDP optimisation problem forwhich many tractable solutions exist, such as theMonte Carlo Control algorithm (Sutton and Barto,1998) used here.In the current HIS system, each summary beliefpoint is a vector consisting of the probabilities ofthe top two hypotheses in master space, two dis-crete status variables summarising the state of the273Algorithm 1 Policy training with k-nn MonteCarlo1: LetQ(b?, a?m) = expected reward on taking action a?m from belief point b?2: LetN(b?, a?m) = number of times action a?m is taken from belief point b?3: Let B be a set of grid-points in belief space, {b?}
any subset of it4: Let piknn : b??
a?m; ?b?
?
B be a policy5: repeat6: t?
07: a?m,0 ?
initial greet action8: b = b0 [= all states in single partition ]Generate dialogue using -greedy policy9: repeat10: t?
t + 111: Get user turn au,t and update belief state b12: b?t ?
SummaryState(b)13: {b?k}knn ?
k-Nearest(b?t,B)14: a?m,t ?
{RandomAction with probability piknn(b?t) otherwise15: record ?b?t, {b?k}knn, a?m,t?, T ?
t16: until dialogue terminates with rewardR from user simulatorScan dialogue and update B,Q andN17: for t = T downto 1 do18: if ?b?i ?
B, |b?t ?
b?i| < ?
then ?
update nearest pt in B19: for all b?k in {b?k}knn do20: w ?
?
(b?t, b?k) ??
weighting function21: Q(b?k, a?m,t)?Q(b?k,a?m,t)?N(b?k,a?m,t)+R?wN(b?k,a?m,t)+w22: N(b?k, a?m,t)?
N(b?k, a?m,t) + w23: end for24: else ?
create new grid point25: add b?t to B26: Q(b?t, a?m,t)?
R,N(b?t, a?m,t)?
127: end if28: R?
?R ?
discount the reward29: end for30: until convergedtop hypothesis and its associated partition, and thetype of the last user act.In order to use such a policy, a simple distancemetric in belief space is used to find the closestgrid point to a given arbitrary belief state:|b?i ?
b?j | =2?d=1?d ??(b?i(d)?
b?j(d))2+5?d=3?d ?
(1?
?
(b?i(d), b?j(d)))(1)where the ?
?s are weights, d ranges over the 2 con-tinuous and 3 discrete components of b?
and ?
(x, y)is 1 iff x = y and 0 otherwise.Associated with each belief point is a functionQ(b?, a?m) which records the expected reward oftaking summary action a?m when in belief state b?.Q is estimated by repeatedly executing dialoguesand recording the sequence of belief point-actionpairs ?b?t, a?m,t?.
At the end of each dialogue, eachQ(b?t, a?m,t) estimate is updated with the actual dis-counted reward.
Dialogues are conducted usingthe current policy pi but to allow exploration of un-visited regions of the state-action space, a randomaction is selected with probability .Once theQ values have been estimated, the pol-icy is found by settingpi(b?)
= argmaxa?mQ(b?, a?m), ?b?
?
B (2)Belief points are generated on demand during thepolicy optimisation process.
Starting from a sin-gle belief point, every time a belief point is en-countered which is sufficiently far from any ex-isting point in the policy grid, it is added to thegrid as a new point.
The inventory of grid pointsis thus growing over time until a predefined maxi-mum number of stored belief vectors is reached.The training schedule adopted in this work iscomparable to the one presented in (Young et al,2009).
Training starts in a noise free environmentusing a small number of grid points and it con-tinues until the performance of the policy asymp-totes.
The resulting policy is then taken as an ini-tial policy for the next stage in which the noiselevel is increased, the set of grid points is ex-panded and the number of iterations is increased.In practice a total of 750 to 1000 grid points havebeen found to be sufficient and the total number ofsimulated dialogues needed for training is around100,000.4 k-nn Monte-Carlo Policy OptimizationIn this work, we use the k nearest neighbor methodto obtain a better estimate of the value function,represented by the belief points?
Q values.
The al-gorithm maintains a set of sample vectors b?
alongwith their Q value vector Q(b?, a).
When a newbelief state b??
is encountered, its Q values are ob-tained by looking up its k-nearest neighbours inthe database, then averaging their Q-values.To obtain good estimates for the value func-tion interpolation, local weights are used basedon the belief point distance.
A Kullback-Leibler(KL) divergence (relative entropy) could be usedas a distance function between the belief points.However, while the KL-divergence between twocontinuous distributions is well defined, this isnot the case for sample sets.
In accordance withthe locally weighted learning theory (Atkeson etal., 1997), a simple weighting scheme based on anearly Euclidean distance (eq.
1) is used to inter-polate the policy over a set of points:piknn(b?)
= argmaxa?m?
{b?k}knnQ(b?k, a?m)?
?
(b?k, b?
)In our experiments, we set the weighting co-efficients with the kernel function ?
(b?1, b?2) =e?|b?1?b?2|2.274Since it can be impossible to construct a fullsystem act from the best summary act, a back-offstrategy is used: an N -best list of summary acts,ranked by their Q values, is scrolled through un-til a feasible summary act is found.
The resultingoverall process of mapping between master andsummary space and back is illustrated in Figure 1.The complete k-nn version policy optimisation al-gorithm is described in Algorithm 1.The user simulator results for semantic errorrates ranging from 0 to 50% with a 5% step areshown in Figure 2 for k ?
{1, 3, 5, 7}, averagedover 3000 dialogues.
The results demonstrate thatthe k-nn policies outperform the baseline 1-nn pol-icy, especially on high noise levels.
While ourinitial expectations are met, increasing k above 3does not improve performances.
This is likely tobe due to the small size of the summary space aswell as the use of discrete dimensions.
Howeverenlarging the summary space and the sample set isconceivable with k-nn time-efficient optimisations(as in (Lefe`vre, 2003)).5 ConclusionIn this paper, an extension to a grid-based pol-icy optimisation technique has been presented andevaluated within the CUED HIS-based dialoguesystem.
The Monte-Carlo control policy optimi-sation algorithm is complemented with a k-nearestneighbour technique to ensure a better generaliza-tion of the trained policy along with an increasedrobustness to noise in the user input.
Preliminaryresults from an evaluation with a simulated userconfirm that the k-nn policies outperform the 1-nnbaseline on high noise, both in terms of successfuldialogue completion and accumulated reward.AcknowledgementsThis research was partly funded by the UK EP-SRC under grant agreement EP/F013930/1 andby the EU FP7 Programme under grant agree-ment 216594 (CLASSIC project: www.classic-project.org).ReferencesC Atkeson, A Moore, and S Schaal.
1997.
Locallyweighted learning.
AI Review, 11:11?73, April.F Lefe`vre.
2003.
Non-parametric probability estima-tion for HMM-based automatic speech recognition.Computer Speech & Language, 17(2-3):113 ?
136.78808284868890929496980  10  20  30  40  50SuccessfulCompletionRateSemantic Error Rate1-nn3-nn5-nn7-nn45678910111213140  10  20  30  40  50AverageRewardSemantic Error Rate1-nn3-nn5-nn7-nnFigure 2: Comparison of the percentage of suc-cessful simulated dialogues and the average re-ward between the k-nn strategies on different errorrates.J Pineau, G Gordon, and S Thrun.
2003.
Point-basedvalue iteration: An anytime algorithm for POMDPs.In Proc IJCAI, pages pp1025?1032, Mexico.J Schatzmann, B Thomson, K Weilhammer, H Ye, andSJ Young.
2007.
Agenda-Based User Simulationfor Bootstrapping a POMDP Dialogue System.
InHLT/NAACL, Rochester, NY.RS Sutton and AG Barto.
1998.
Reinforcement Learn-ing: An Introduction.
MIT Press, Cambridge, Mass.JD Williams and SJ Young.
2007.
Scaling POMDPsfor Spoken Dialog Management.
IEEE Audio,Speech and Language Processing, 15(7):2116?2129.SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatz-mann, B Thomson, and K Yu.
2009.
The hid-den information state model: A practical frame-work for POMDP-based spoken dialogue manage-ment.
Computer Speech & Language, In Press, Un-corrected Proof.275
