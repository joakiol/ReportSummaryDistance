Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 878?888,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLanguage to Code:Learning Semantic Parsers for If-This-Then-That RecipesChris QuirkMicrosoft ResearchRedmond, WA, USAchrisq@microsoft.comRaymond Mooney?UT AustinAustin TX, USAmooney@cs.utexas.eduMichel GalleyMicrosoft ResearchRedmond, WA, USAmgalley@microsoft.comAbstractUsing natural language to write programsis a touchstone problem for computationallinguistics.
We present an approach thatlearns to map natural-language descrip-tions of simple ?if-then?
rules to executablecode.
By training and testing on a large cor-pus of naturally-occurring programs (called?recipes?)
and their natural language de-scriptions, we demonstrate the ability toeffectively map language to code.
Wecompare a number of semantic parsing ap-proaches on the highly noisy training datacollected from ordinary users, and find thatloosely synchronous systems perform best.1 IntroductionThe ability to program computers using natural lan-guage would clearly allow novice users to moreeffectively utilize modern information technology.Work in semantic parsing has explored mappingnatural language to some formal domain-specificprogramming languages such as database queries(Woods, 1977; Zelle and Mooney, 1996; Berant etal., 2013), commands to robots (Kate et al, 2005),operating systems (Branavan et al, 2009), smart-phones (Le et al, 2013), and spreadsheets (Gulwaniand Marron, 2014).
Developing such language-to-code translators has generally required specificdedicated efforts to manually construct parsers orlarge corpora of suitable training examples.An interesting subset of the possible programspace is if-then ?recipes,?
simple rules that allowusers to control many aspects of their digital lifeincluding smart devices.
Automatically parsing?Work performed while visiting Microsoft Research.these recipes represents a step toward complex nat-ural language programming, moving beyond singlecommands toward compositional statements withcontrol flow.Several services, such as Tasker and IFTTT, al-low users to create simple programs with ?triggers?and ?actions.?
For example, one can program theirPhillips Hue light bulbs to flash red and blue whenthe Cubs hit a home run.
A somewhat complicatedGUI allows users to construct these recipes basedon a set of information ?channels.?
These chan-nels represent many types of information.
Weather,news, and financial services have provided constantupdates through web services.
Home automationsensors and controllers such as motion detectors,thermostats, location sensors, garage door openers,etc.
are also available.
Users can then describe therecipes they have constructed in natural languageand publish them.Our goal is to build semantic parsers that al-low users to describe recipes in natural languageand have them automatically mapped to exe-cutable code.
We have collected 114,408 recipe-description pairs from the http://ifttt.com website.Because users often provided short or incompleteEnglish descriptions, the resulting data is extremelynoisy for the task of training a semantic parser.Therefore, we have constructed semantic-parserlearners that utilize and adapt ideas from severalprevious approaches (Kate and Mooney, 2006;Wong and Mooney, 2006) to learn an effective in-terpreter from such noisy training data.
We presentresults on our collected IFTTT corpus demonstrat-ing that our best approach produces more accurateprograms than several competing baselines.
Byexploiting such ?found data?
on the web, seman-tic parsers for natural-language programming canpotentially be developed with minimal effort.8782 BackgroundWe take an approach to semantic parsing thatdirectly exploits the formal grammar of the tar-get meaning representation language, in our caseIFTTT recipes.
Given supervised training data inthe form of natural-language sentences each pairedwith their corresponding IFTTT recipe, we learnto introduce productions from the formal-languagegrammar into the derivation of the target programbased on expressions in the natural-language input.This approach originated with the SILT system(Kate et al, 2005) and was further developed inthe WASP (Wong and Mooney, 2006; Wong andMooney, 2007b) and KRISP (Kate and Mooney,2006) systems.WASP casts semantic parsing as a syntax-basedstatistical machine translation (SMT) task, wherea synchronous context-free grammar (SCFG) (Wu,1997; Chiang, 2005; Galley et al, 2006) is usedto model the translation of natural language into aformal meaning representation.
It uses statisticalmodels developed for syntax-based SMT for lexicallearning and parse disambiguation.
Productions inthe formal-language grammar are used to constructsynchronous rules that simultaneously model thegeneration of the natural language.
WASP was sub-sequently ?inverted?
to use the same synchronousgrammar to generate natural language from the for-mal language (Wong and Mooney, 2007a).KRISP uses classifiers trained using a Support-Vector Machine (SVM) to introduce productionsin the derivation of the formal translation.
Theproductions of the formal-language grammar aretreated like semantic concepts to be recognizedfrom natural-language expressions.
For each pro-duction, an SVM classifier is trained using a stringsubsequence kernel (Lodhi et al, 2002).
Each clas-sifier can then estimate the probability that a givennatural-language substring introduces a productioninto the derivation of the target representation.
Dur-ing semantic parsing, these classifiers are employedto estimate probabilities on different substringsof the sentence to compositionally build the mostprobable meaning representation for the sentence.Unlike WASP whose synchronous grammar needsto be able to directly parse the input, KRISP?s ap-proach to ?soft matching?
productions allows itto produce a parse for any input sentence.
Conse-quently, KRISP was shown to be much more robustto noisy training data than previous approaches tosemantic parsing (Kate and Mooney, 2006).Since our ?found data?
for IFTTT is extremelynoisy, we have taken an approach similar to KRISP;however, we use a probabilistic log-linear text clas-sifier rather than an SVM to recognize productions.This method of assembling well-formed pro-grams guided by a natural language query bearssome resemblance to Keyword Programming (Lit-tle and Miller, 2007).
In that approach, users en-ter natural language queries in the middle of anexisting program; this query drives a search forprograms that are relevant to the query and fitwithin the surrounding program.
However, thefunction used to score derivations is a simple match-ing heuristic relying on the overlap between queryterms and program identifiers.
Our approach usesmachine learning to build a correspondence be-tween queries and recipes based on parallel data.There is also a large body of work applying Com-binatory Categorical Grammars to semantic pars-ing, starting with Zettlemoyer and Collins (2005).Depending on the set of combinators used, this ap-proach can capture more expressive languages thansynchronous context-free MT.
In practice, however,synchronous MT systems have competitive accu-racy scores (Andreas et al, 2013).
Therefore, wehave not yet evaluated CCG on this task.3 If-this-then-that recipesThe recipes considered in this paper are diverse andpowerful despite being simple in structure.
Eachrecipe always contains exactly one trigger and oneaction.
Whenever the conditions of the trigger aresatisfied, the action is performed.
The resultingrecipes can perform tasks such as home automation(?turn on my lights when I arrive home?
), homesecurity (?text me if the door opens?
), organization(?add receipt emails to a spreadsheet?
), and muchmore (?remind me to drink water if I?ve been ata bar for more than two hours?).
Triggers andactions are drawn from a wide range of channelsthat must be activated by each user.
These channelscan represent many entities and services, includingdevices (such as Android devices or WeMo lightswitches) and knowledge sources (such as ESPNor Gmail).
Each channel exposes a set of functionsfor both trigger and action.Several services such as IFTTT, Tasker, andLlama allow users to author if-this-then-thatrecipes.
IFTTT is unique in that it hosts a largeset of recipes along with descriptions and othermetadata.
Users of this site construct recipes usinga GUI interface to select the trigger, action, and the879parameters for both trigger and action.
After therecipe is authored, the user must provide a descrip-tion and optional set of notes for this recipe andpublish the recipe.
Other users can browse and usethese published recipes; if a user particularly likesa recipe, they can mark it as a favorite.As of January 2015, we found 114,408 recipeson http://ifttt.com.
Among the available recipes weencountered a total of 160 channels.
In total, wefound 552 trigger functions from 128 of those chan-nels, and 229 action functions from 99 channels,for a total of 781 functions.
Each recipe includesa number of pieces of information: description1,note, author, number of uses, etc.
99.98% of theentries have a description, and 35% contain a note.Based on availability, we focused primarily on thedescription, though there are cases where the noteis a more explicit representation of program intent.The recipes at http://ifttt.com are represented asHTML forms, with combo boxes, inline maps, andother HTML UI components allowing end usersto select functions and their parameters.
This isconvenient for end users, but difficult for automatedapproaches.
We constructed a formal grammar ofpossible program structures, and from each HTMLform we extracted an abstract syntax tree (AST)conforming to this grammar.
We model this as acontext-free grammar, though this assumption isviolated in some cases.
Consider the program inFigure 1, where some of the parameters used theaction are provided by the trigger.This data could be used in a variety of ways.Recipes could be suggested to users based on theiractivities or interests, for instance, or one couldtrain a natural language generation system to givea readable description of code.In this paper, the paired natural language descrip-tions and abstract syntax trees serve as training datafor semantic parsing.
Given a description, a systemmust produce the AST for an IFTTT recipe.
Wenote in passing that the data was constructed inthe opposite direction: users first implemented therecipe and then provided a description afterwards.Ideal data for our application would instead startwith the description and construct the recipe basedon this description.
Yet the data is unusually largeand diverse, making it interesting training data formapping natural language to code.1The IFTTT site refers to this as ?title?.4 Program synthesis methodsWe consider a number of methods to map the natu-ral language description of a problem into its for-mal program representation.4.1 Program retrievalOne natural baseline is retrieval.
Multiple userscould potentially have similar needs and thereforeauthor similar or even identical programs.
Givena novel description, we can search for the closestdescription in a table of program-description pairs,and return the associated program.
We exploredseveral text-similarity metrics, and found that stringedit distance over the unmodified character se-quence achieved best performance on the devel-opment set.
As the corpus of program-descriptionpairs becomes larger, this baseline should increasein quality and coverage.4.2 Machine TranslationThe downside to retrieval is that it cannot general-ize.
Phrase-based SMT systems(Och et al, 1999;Koehn et al, 2003) can be seen as an incrementalstep beyond retrieval: they segment the trainingdata and attempt to match and assemble those seg-ments at runtime.
If the phrase length is unbounded,retrieval is almost a special case: it could returnwhole programs from the training data when thedescription matches exactly.
In addition, they canfind subprograms that are relevant to portions of theinput, and assemble those subprograms into wholeprograms.As a baseline, we adopt a recent approach (An-dreas et al, 2013) that casts semantic parsing asphrasal translation.
First, the ASTs are convertedinto flat sequences of code tokens using a pre-orderleft-to-right traversal.
The tokens are annotatedwith their arity, which is sufficient to reconstructthe tree given a well formed sequence of tokensusing a simple stack algorithm.
Given this paral-lel corpus of language and code tokens, we traina conventional statistical machine translation sys-tem that is similar in structure and performance toMoses (Koehn et al, 2007).
We gather the k-besttranslations, retaining the first such output that canbe successfully converted into a well-formed pro-gram according to the formal grammar.
Integrationof the well-formedness constraint into decodingwould likely produce better translations, but wouldrequire more modifications to the MT system.Approaches to semantic parsing inspired by ma-chine translation have proven effective when the880(A) CHANNELS(B) FUNCTIONS(C) PARAMETERSIFACTIONGoogle DriveAdd row to spreadsheetDrivefolder pathIFTTT AndroidFormatted row{{OccurredAt}} {{FromNumber}} {{ContactName}}Spreadsheet namemissedTRIGGERAndroid Phone CallAny phone call missedArchive your missed calls from Android to Google DriveFigure 1: Example recipe with description, with nodes corresponding to (a) Channels, (b) Functions, and (c) Parameters indicatedwith specific boxes.
Note how some of the fields in braces, such as OccurredAt, depend on the trigger.data is very parallel.
In the IFTTT dataset, however,the available pairs are not particularly clean.
Wordalignment quality suffers, and production extrac-tion suffers in turn.
Descriptions in this corpus areoften quite telegraphic (e.g., ?Instagram to Face-book?)
or express unnecessary pieces of informa-tion, or are downright unintelligible (?
2Mrl14?
).Approaches that rely heavily on lexicalized infor-mation and assume a one-to-one correspondencebetween source and target (at the phrase, if not theword level) struggle in this setting.4.3 Generation without alignmentAn alternate approach is to treat the source lan-guage as context and a general direction, rather thana hard constraint.
The target derivation can be pro-duced primarily according to the formal grammarwhile guided by features from the source language.For each production in the formal grammar, wecan train a binary classifier intended to predictwhether that production should be present in thederivation.
This classifier uses general features ofthe source sentence.
Note how this allows produc-tions to be inferred based on context: although adescription might never explicitly say that a pro-duction is necessary, the surrounding context mightstrongly imply it.We assign probabilities to derivations by lookingat each production independently.
A derivation ei-ther uses or does not use each production.
For eachproduction used in the derivation, we multiply bythe probability of its inclusion.
Likewise for eachproduction not used in the derivation, we multiplyby one minus the probability of its inclusion.Let G = (V,?, R, S) be the formal grammarwith non-terminals V , terminal vocabulary ?, pro-ductions R and start symbol S. E represents asource sentence, and D, a formal derivation treefor that sentence.
R(D) is the set of productionsin that derivation.
The score of a derivation is thefollowing product:P (D|E) =?r?R(D)P (r|E)?r?R\R(D)P (?r|E)The binary classifiers are log-linear models overfeatures, F , of the input string: P (r|E) ?exp(?>rF (E)).4.3.1 TrainingFor each production, we train a binary classifierpredicting its presence or absence.
Given a train-ing set of parallel descriptions and programs, wecreate |R| binary classifier training sets, one foreach classifier.
We currently use a small set ofsimple features: word unigrams and bigrams, andcharacter trigrams.4.3.2 InferenceWhen presented with a novel utterance, E, our sys-tem must find the best code corresponding to thatutterance.
We use a top-down, left-to-right gener-ation strategy, where each search node contains astack of symbols yet to be expanded and a log prob-ability.
The initial node is ?
[S] , 0?
; and a node iscomplete when its stack of non-terminals is empty.Given a search node with a non-terminal as itsfirst symbol on the stack, we expand with any pro-duction for that symbol, putting its yield onto thestack and updating the node cost to include its881derivation score:?[X,?]
, p?
(X ?
?)
?
R?
[?, ?]
, p+ logP (X ?
?|E)?
?If the first stack item is a terminal, it is scanned:?
[a, ?]
, p?
a ?
??[?]
, p?Using these inference rules, we utilize a simplegreedy approach that only accounts for the produc-tions included in the derivation.
To account for thenegative?r?R\R(D)P (?r|E) factors, we use abeam search, and rerank the n-best final outcomesfrom this search based on the probability of all pro-ductions that are not included.
Partial derivationsare grouped into beams according to the number ofproductions in that derivation.4.4 Loosely synchronous generationThe above method learns distributions over pro-ductions given the input, but treats the sentence asan undifferentiated bag of linguistic features.
Thesyntax of the source sentence is not leveraged at all,nor is any correspondence between the languagesyntax and the program structure used.
Often thepairs are not in sufficient correspondence to sug-gest synchronous approaches, but some loose corre-spondence to maintain at least a notion of coveragecould be helpful.We pursue an approach similar to KRISP (Kateand Mooney, 2006), with several differences.
First,rather than a string kernel SVM, we use a log-linearmodel with character and word n-gram features.2Second, we allow the model to consider both span-internal features and contextual features.This approach explicitly models the correspon-dence between nodes in the code side and tokens inthe language.
Unlike standard MT systems, wordalignment is not used as a hard constraint.
Instead,this phrasal correspondence is induced as part ofmodel training.We define a semantic derivation D of a natu-ral language sentence E as a program AST whereeach production in the AST is augmented with aspan.
The substrings covered by the children ofa production must not overlap, and the substringcovered by the parent must be the concatenationof the substrings covered by the children.
Figure 2shows a sample semantic derivation.2We have a preference for log-linear models given theirrobustness to hyperparameter settings, ease of optimization,and flexible incorporation of features.
An SVM trained withsimilar features should have similar performance, though.IF[1-6]ACTION[1-2]Phone call[1-2]Call my phone[1-2]TRIGGER[3-6]ESPN[3-6]New in-game update[3-6]Chicago Cubs[5-5]1 2 3 4 5 6Call me if the Cubs scoreFigure 2: An example training pair with its semantic deriva-tion.
Note the correspondence between formal language andnatural language denoted with indices and spans.The core components of KRISP are string-kernelclassifiers P (r, i..j|E) denoting the probabilitythat a production r in the AST covers the spanof words i..j in the sentence E. Here, i < j arepositions in the sentence indicating the span oftokens most relevant to this production.
In otherwords, the substring E[i..j] denotes the productionr with probability P (r, i..j|E).
The probability ofa semantic derivation D is defined as follows:P (D|E) =?
(r,i..j)?DP (r, i..j|E)That is, we assume that each production is indepen-dent of all others, and is conditioned only on thestring to which it is aligned.
This can be seen as arefinement of the above production classificationapproach using a notion of correspondence.Rather than using string kernels, we use logis-tic regression classifiers with word unigram, wordbigram, and character trigram features.
UnlikeKRISP, we include features from both inside andoutside the substring.
Consider the production?Phone call?
Call my phone?
with span 1-2 fromFigure 2.
Word unigram features indicate that ?call?and ?me?
are inside the span; the remaining wordsare outside the span.
Word bigram features indicatethat ?call me?
is inside the span, ?me if?
is on theboundary of the span, and all remaining bigramsare outside the span.4.4.1 TrainingThese classifiers are trained in an iterative EM-like manner (Kate and Mooney, 2006).
Startingwith some initial classifiers and a training set ofNL and AST pairs, we search for the most likelyderivation.
If the AST underlying this derivationmatches the gold AST, then this derivation is added882to the set of positive instances.
Otherwise, it isadded to the set of negative instances, and the bestderivation constrained to match the gold standardAST is found and added to the positive instances.Given this revised training data, the classifiers areretrained.
After each pass through the training data,we evaluate the current model on the developmentset.
This procedure is repeated until development-set performance begins to fall.4.4.2 InferenceTo find the most probable derivation according tothe grammar, KRISP uses a variation on Earleyparsing.
This is similar to the inference methodfrom Section 4.3.2, but each item now additionallymaintains a position and a span.
Inference proceedsleft-to-right through the source string.
The naturallanguage may present information in a differentorder than the formal language, so all permutationsof rules are considered during inference.We found this inference procedure to be quiteslow for larger data sets, especially because widebeams were needed to prevent search failure.
Tospeed up inference, we used scores from theposition-independent classifiers as completion-costestimates.The completion-cost estimate for a given sym-bol is defined recursively.
Terminals have a cost ofzero.
Productions have a completion cost of the logprobability of the production given the sentence,plus the completion cost of all non-terminal sym-bols.
The completion cost for a non-terminal isthe max cost of any production rooted in that non-terminal.
Computing this cost requires traversingall productions in the grammar for each sentence.Given a partial hypothesis, we use exact scoresfor the left-corner subtree that has been fully con-structed, and completion estimates for all the sym-bols and productions whose left and right spans arenot yet fully instantiated.5 Experimental EvaluationNext we evaluate the accuracy of these approaches.The 114,408 recipes described in Section 3 werefirst cleaned and tokenized.
We kept only onerecipe per unique description, after mapping to low-ercase and normalizing punctuation.3Finally therecipes were split by author, randomly assigningeach to training, development, or test, to prevent3We found many recipes with the same description, likelycopies of some initial recipe made by different users.
Weselected one representative using a deterministic heuristic.Language CodeRecipes 77,495 77,495Train Tokens 527,368 1,776,010Vocabulary 58,102 140,871Recipes 5,171 5,171Dev Tokens 37,541 110,074Vocabulary 7,741 14,804Recipes 4,294 4,294Test Tokens 28,214 94,367Vocabulary 6,782 13,969Table 1: Statistics of the data after cleaning and separatinginto training, development, and test sets.
In each case, thenumber of recipes, tokens (including punctuation, etc.)
andvocabulary size are included.overfitting to the linguistic style of a particular au-thor.
Table 1 presents summary statistics for theresulting data.Although certain trigger-action pairs occur muchmore often than others, the recipes in this dataare quite diverse.
The top 10 trigger-action pairsaccount for 14% of the recipes; the top 100 accountfor 37%; the top 1000 account for 72%.5.1 MetricsTo evaluate system performance, several differentmeasures are employed.
Ideally a system wouldoutput exactly the correct abstract syntax tree.
Onemeasure is to count the number of exact matches,though almost all methods receive a score of 0.4Alternatively, we can look at the AST as a set ofproductions, computing balanced F-measure.
Thisis a much more forgiving measure, giving partialcredit for partially correct results, though it has thecaveat that all errors are counted equally.Correctly assigning the trigger and action is themost important, especially because some of the pa-rameter values are tailored for particular users.
Forexample, ?turn off my lights when I leave home?requires a ?home?
location, which varies for eachuser.
Therefore, we also measure accuracy at iden-tifying the correct trigger and action, both at thechannel and function level.5.2 Human comparisonOne remaining difficulty is that multiple programsmay be equally correct.
Some descriptions are verydifficult to interpret, even for humans.
Second,4Retrieval gets an exact match 3.7% of the time, likely dueto near-duplicates from copied recipes.883multiple channels may provide similar functional-ity: both Phillips Hue and WeMo channels providethe ability to turn on lights.
Even a well-authoreddescription may not clarify which channel shouldbe used.
Finally, many descriptions are underspec-ified.
For instance, the description ?notify me ifit rains?
does not specify whether the user shouldreceive an Android notification, an iOS notification,an email, or an SMS.
This is difficult to capturewith an automatic metric.To address the prevalence and impact of under-specification and ambiguity in descriptions, weasked humans to perform a very similar task.Human annotators on Amazon Mechanical Turk(?turkers?)
were presented with recipe descriptionsand asked to identify the correct channel and func-tion (but not parameters).
Turkers received carefulinstructions and several sample description-recipepairs, then were asked to specify the best recipe foreach input.
We requested they try their best to findan action and a trigger even when presented withvague or ambiguous descriptions, but they couldtag inputs as ?unintelligible?
if they were unable tomake an educated guess.
Turkers created recipesonly for English descriptions, applying the label?non-English?
otherwise.
Five recipes were gath-ered for each description.
The resulting recipes arenot exactly gold, as they have limited training at thetask.
However, we imposed stringent qualificationrequirements to control the annotation quality.5Our workers were in fair agreement with one an-other and the gold standard, producing high qualityannotation at wages calibrated to local minimumwage.
We measure turker agreement with Krippen-dorff?s ?
(Krippendorff, 1980), which is a statis-tical measure of agreement between any numberof coders.
Unlike Cohen?s ?
(Cohen, 1960), the ?statistic does not require that coders be the same foreach unit of analysis.
This property is particularlydesirable in our case, since turkers generally differacross HITs.
A value of ?
= 1 indicates perfectagreement, while ?
?
0 suggests the absence ofagreement or systematic disagreement.
Agreementmeasures on the Mechanical Turk data are shownin Table 2.
This shows encouraging levels of agree-ment for both the trigger and the action, especiallyconsidering the large number of categories.
Krip-pendorff (1980) advocates a 0.67 cutoff to allow5Turkers must have 95% HIT approval rating and be nativespeakers of English (As an approximation of the latter, werequired Turkers be from the U.S.).
Manual inspection of an-notation on a control set drawn from the training data ensuredthere was no apparent spam.Trigger ActionC C+F C C+F# of categories 128 552 99 229all .592 .492 .596 .532Intelligible English .687 .528 .731 .627Table 2: Annotator agreement as measured by Krippendorff?s?
coefficient (Krippendorff, 1980).
Agreement is measuredon either channel (C) or channel and function (C+F), andon either the full test set (4294 recipes) or its English andintelligible subset (2262 recipes).
?tentative conclusion?
of agreement, and turkers arerelatively close to that level for both trigger andaction channels.
However, it is important to notethat the coding scheme used by turkers is not mutu-ally exclusive, as several triggers and actions (e.g.,?SMS?
vs. ?Android SMS?
actions) accomplishsimilar effects.
Thus, our levels of agreement arelikely to be greater than suggested by measures inthe table.
Finally, we also measured agreement onthe English and intelligible subset of the data, as wefound that confusion between the two labels ?non-English?
and ?unintelligible?
was relatively high.As shown in the table, this substantially increasedlevels of agreement, up to the point where ?
forboth trigger and action channels are above the 0.67cutoff drawing tentative conclusion of agreement.5.3 Systems and baselinesThe retrieval method searches for the closest de-scription in the training data based on characterstring-edit-distance and returns the recipe for thattraining program.
The phrasal method uses phrase-based machine translation to generate candidateoutputs, searching the resulting n-best candidatesfor the first well-formed recipe.
After exploringmultiple word alignment approaches, we foundthat an unsupervised feature-rich method (Berg-Kirkpatrick et al, 2010) worked best, leverag-ing features of string similarity between the de-scription and the code.
We ran MERT on the de-velopment data to tune parameters.
We used aphrasal decoder with performance similar to Moses.The synchronous grammar method, a recreation ofWASP, uses the same word alignment as above,but extracts a synchronous grammar rules fromthe parallel data (Wong and Mooney, 2006).
Theclassifier approach described in Section 4.3 is in-dependent of word alignment.
Finally, the posclassapproach from Section 4.4 derives its own deriva-884tion structure from the data.The human annotations are used to establish themturk human-performance baseline by taking themajority selection of the trigger and action over5 HITs for each description and comparing theresult to the gold standard.
The oracleturk human-performance baseline shows how often at least oneof the turkers agreed with the gold standard.In addition, we evaluated all systems on a sub-set of the test data where at least three human-generated recipes agreed with the gold standard.This subset represents those programs that areeasily reproducible by human workers.
A goodmethod should strive to achieve 100% accuracyon this set, and we should perhaps not be overlyconcerned about the remaining examples wherehumans disagree about the correct interpretation.5.4 Results and discussionTable 3 summarizes the main evaluation results.Most of the measures are in concordance.Interestingly, retrieval outperforms the phrasalMT baseline.
With a sufficiently long phrase limit,phrasal MT approaches retrieval, but with a fewcrucial differences.
First, phrasal requires an exactmatch of some substring of the input to some sub-string of the training data, where retrieval can skipover words.
Second, the phrases are heavily depen-dent on word alignment; we find the word align-ment techniques struggle with the noisy IFTTTdescriptions.
Sync performs similarly to phrasal.The underspecified descriptions challenge assump-tions in synchronous grammars: much of the targetstructure is implied rather than stated.In contrast, the classification method performsquite well.
Some productions may be very likelygiven a prior alone, or may be inferred given otherproductions and the need for a well-formed deriva-tion.
Augmenting this information with positionalinformation as in posclass can help with the attri-bution problem.
Consider the input ?DownloadFacebook Photos you?re tagged in to Dropbox?
:we would like the token ?Facebook?
to invoke onlythe trigger, not the action.
We believe further gainscould come from better modeling of the correspon-dence between derivation and natural language.We find that semantic parsing systems have ac-curacy nearly as high or even higher than turkersin certain conditions.
There are several reasons forthis.
First, many of the channels overlap in func-tionality (Gmail vs. email, or Android SMS vs.SMS); likewise functions may be very closely re-Channel +Func Prod F1(a) All: 4,294 recipesretrieval 28.2 19.3 40.8phrasal 17.3 10.0 34.8sync 16.2 9.5 34.9classifier 46.3 33.0 47.3posclass 47.4 34.5 48.0mturk 33.4 22.6 ?n/a?oracleturk 48.8 37.8 ?n/a?
(b) Omit non-English: 3,741 recipesretrieval 28.9 20.2 41.7phrasal 19.3 11.3 35.3sync 18.1 10.6 35.1classifier 48.8 35.2 48.4posclass 50.0 36.9 49.3mturk 38.4 26.0 ?n/a?oracleturk 56.0 43.5 ?n/a?
(c) Omit non-English & unintelligible: 2,262 recipesretrieval 36.8 25.4 49.0phrasal 27.8 16.4 39.9sync 26.7 15.5 37.6classifier 64.8 47.2 56.5posclass 67.2 50.4 57.7mturk 59.0 41.5 ?n/a?oracleturk 86.2 59.4 ?n/a?
(d) ?3 turkers agree with gold: 758 recipesretrieval 43.3 32.3 56.2phrasal 37.2 23.5 45.5sync 36.5 24.1 42.8classifier 79.3 66.2 65.0posclass 81.4 71.0 66.5mturk 100.0 100.0 ?n/a?oracleturk 100.0 100.0 ?n/a?Table 3: Evaluation results.
The first column measures howoften the channels are selected correctly for both trigger andaction (e.g.
Android Phone Call and Google Drive in Fig-ure 1).
The next column measures how often both the channeland function are correctly selected for both trigger and ac-tion (e.g.
Android Phone Call::Any phone call missed andGoogle Drive::Add row to spreadsheet).
The last columnshows balanced F-measure against the gold tree over all pro-ductions in the proposed derivation, from the root productiondown to the lowest parameter.
We show results on (a) thefull test data; (b) omitting descriptions marked as non-Englishby a majority of the crowdsourced workers; (c) omitting de-scriptions marked as either non-English or unintelligible bythe crowd; and (d) only recipes where at least three of fiveworkers agreed with the gold standard.lated (Post a tweet vs. Post a tweet with an image).All the systems with access to thousands of train-ing pairs are at a strong advantage; they can, for885INPUT Park in garage when snow tomorrow(a) IFTTT Weather : Tomorrow?s forecast calls for =?
SMS : Send me an SMSOUTPUT Weather : Tomorrow?s forecast calls for =?
SMS : Send me an SMSINPUT Suas fotos do instagr.am salvas no dropbox(b) IFTTT Instagram : Any new photo by you =?
Dropbox : Add file from URLOUTPUT Instagram : Any new photo by you =?
Dropbox : Add file from URLINPUT Foursquare check-in archive(c) IFTTT Foursquare : Any new check-in =?
Evernote : Create a noteOUTPUT Foursquare : Any new check-in =?
Google Drive : Add row to spreadsheetINPUT if i post something on blogger it will post it to wordpress(d) IFTTT Blogger : Any new post =?WordPress : Create a postOUTPUT Feed : New feed item =?
Blogger : Create a postINPUT Endless loop!
(e) IFTTT Gmail : New email in inbox from =?
Gmail : Send an emailOUTPUT SMS : Send IFTTT any SMS =?
Philips hue : Turn on color loopTable 4: Example output from the posclass system.
For each input instance, we show the original query, the recipe originallyauthored through IFTTT, and our system output.
Instance (a) demonstrates a case where the correct program is produced eventhough the input is rather tricky.
Even the Portuguese query of (b) is correctly predicted, though keywords help here.
In instance(c), the query is underspecified, and the system predicts that archiving should be done in Google Drive rather than evernote.Instance (d) shows how we sometimes confuse the trigger and action.
Certain queries, such as (e), would require very deepinference: the IFTTT recipe sets up an endless email loop, where our system assembles a strange interpretation based on keywordmatch.instance, more effectively break such ties by learn-ing a prior over which channels are more likely.Turkers, on the other hand, have neither specifictraining at this job nor a background corpus andmore frequently disagree with the gold standard.Second, there are a number of non-English andunintelligible descriptions.
Although the turkerswere asked to skip these sentences, the machine-learning systems may still correctly predict thechannel and action, since the training set alo con-tains non-English and cryptic descriptions.
Forthe cases where humans agree with each other andwith the gold standard, the best automated system(posclass) does fairly well, getting 81% channeland 71% function accuracy.Table 4 has some sample outputs from theposclass system, showing both examples wherethe system is effective and where it struggles tofind the intended interpretation.6 ConclusionsThe primary goal of this paper is to highlighta new application and dataset for semantic pars-ing.
Although if-this-then-that recipes have a lim-ited structure, many potential recipes are possible.This is a small step toward broad program synthe-sis from natural language, but is driven by realuser data for modern hi-tech applications.
To en-courage further exploration, we are releasing theURLs of recipes along with turker annotations athttp://research.microsoft.com/lang2code/.The best performing results came from a looselysynchronous approach.
We believe this is a verypromising direction: most work inspired by pars-ing or machine translation has assumed a strongconnection between the description and the opera-ble semantic representation.
In practical situations,however, many elements of the semantic representa-tion may only be implied by the description, ratherthan explicitly stated.
As we tackle domains withgreater complexity, identifying implied but neces-sary information will be even more important.Underspecified descriptions open up new inter-face possibilities as well.
This paper consideredonly single-turn interactions, where the user de-scribes a request and the system responds with aninterpretation.
An important next step would beto engage the user in an interactive dialogue toconfirm and refine the user?s intent and develop afully-functional correct program.AcknowledgmentsThe authors would like to thank William Dolan andthe anonymous reviewers for their helpful adviceand suggestions.886ReferencesJacob Andreas, Andreas Vlachos, and Stephen Clark.2013.
Semantic parsing as machine translation.
InProceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics (Volume 2:Short Papers), pages 47?52, Sofia, Bulgaria, August.Association for Computational Linguistics.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP-13).Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,John DeNero, and Dan Klein.
2010.
Painless un-supervised learning with features.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 582?590, LosAngeles, California, June.
Association for Computa-tional Linguistics.S.R.K.
Branavan, Harr Chen, Luke S. Zettlemoyer, andRegina Barzilay.
2009.
Reinforcement learning formapping instructions to actions.
In Joint Conferenceof the 47th Annual Meeting of the Association forComputational Linguistics and the 4th InternationalJoint Conference on Natural Language Processingof the Asian Federation of Natural Language Pro-cessing (ACL-IJCNLP), Singapore.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43nd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages263?270, Ann Arbor, MI.J.
Cohen.
1960.
A coefficient of agreement for nomi-nal scales.
Educational and Psychological Measure-ment, 20(1):37 ?
46.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics(COLING/ACL), pages 961?968, Sydney, Australia,July.Sumit Gulwani and Mark Marron.
2014.
Nlyze: Inter-active programming by natural language for spread-sheet data analysis and manipulation.
In SIGMOD.Rohit J. Kate and Raymond J. Mooney.
2006.
Us-ing string-kernels for learning semantic parsers.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 913?920, Sydney, Australia, July.
Associationfor Computational Linguistics.R.
J. Kate, Y. W. Wong, and R. J. Mooney.
2005.Learning to transform natural to formal languages.In Proceedings of the Twentieth National Confer-ence on Artificial Intelligence (AAAI-05), pages1062?1068, Pittsburgh, PA, July.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Vol-ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,USA.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Annual Meeting of the Association for Com-putational Linguistics (ACL), demonstration session,Prague, Czech Republic, June.Klaus Krippendorff.
1980.
Content Analysis: an In-troduction to its Methodology.
Sage Publications,Beverly Hills, CA.Vu Le, Sumit Gulwani, and Zhendong Su.
2013.Smartsynth: Synthesizing smartphone automationscripts from natural language.
In MobiSys.Greg Little and Robert C. Miller.
2007.
Keyword pro-gramming in java.
In Proceedings of the Twenty-second IEEE/ACM International Conference on Au-tomated Software Engineering, ASE ?07, pages 84?93, New York, NY, USA.
ACM.Huma Lodhi, Craig Saunders, John Shawe-Taylor,Nello Cristianini, and Chris Watkins.
2002.
Textclassification using string kernels.
Journal of Ma-chine Learning Research, 2:419?444.Franz Josef Och, Christoph Tillmann, and HermannNey.
1999.
Improved alignment models for statisti-cal machine translation.
In Proc.
of the Conferenceon Empirical Methods in Natural Language Process-ing and Very Large Corpora, pages 20?28, Univer-sity of Maryland, College Park, MD, June.Yuk Wah Wong and Raymond Mooney.
2006.
Learn-ing for semantic parsing with statistical machinetranslation.
In Proceedings of the Human LanguageTechnology Conference of the NAACL, Main Confer-ence, pages 439?446, New York City, USA, June.Association for Computational Linguistics.Yuk Wah Wong and Raymond J. Mooney.
2007a.
Gen-eration by inverting a semantic parser that uses sta-tistical machine translation.
In Proceedings of Hu-man Language Technologies: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL-HLT), pages 172?179, Rochester, NY.887Yuk Wah Wong and Raymond J. Mooney.
2007b.Learning synchronous grammars for semantic pars-ing with lambda calculus.
In Proceedings of the45th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 960?967, Prague,Czech Republic, June.William A.
Woods.
1977.
Lunar rocks in naturalEnglish: Explorations in natural language questionanswering.
In Antonio Zampoli, editor, LinguisticStructures Processing.
Elsevier North-Holland, NewYork.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In Proceedings of the Thirteenth Na-tional Conference on Artificial Intelligence (AAAI-96), pages 1050?1055, Portland, OR, August.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In In Proceedings of the 21st Conferenceon Uncertainty in AI, pages 658?666.888
