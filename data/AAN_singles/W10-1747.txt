Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 315?320,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsThe RWTH System Combination System for WMT 2010Gregor Leusch and Hermann NeyRWTH Aachen UniversityAachen, Germany{leusch,ney}@cs.rwth-aachen.deAbstractRWTH participated in the System Combi-nation task of the Fifth Workshop on Sta-tistical Machine Translation (WMT 2010).For 7 of the 8 language pairs, we com-bine 5 to 13 systems into a single con-sensus translation, using additional n-bestreranking techniques in two of these lan-guage pairs.
Depending on the languagepair, improvements versus the best sin-gle system are in the range of +0.5 and+1.7 on BLEU, and between ?0.4 and?2.3 on TER.
Novel techniques comparedwith RWTH?s submission to WMT 2009include the utilization of n-best rerankingtechniques, a consensus true casing ap-proach, a different tuning algorithm, andthe separate selection of input systemsfor CN construction, primary/skeleton hy-potheses, HypLM, and true casing.1 IntroductionThe RWTH approach to MT system combinationis a refined version of the ROVER approach inASR (Fiscus, 1997), with additional steps to copewith reordering between different hypotheses, andto use true casing information from the input hy-potheses.
The basic concept of the approach hasbeen described by Matusov et al (2006).
Severalimprovements have been added later (Matusov etal., 2008).
This approach includes an enhancedalignment and reordering framework.
In con-trast to existing approaches (Jayaraman and Lavie,2005; Rosti et al, 2007), the context of the wholecorpus rather than a single sentence is consideredin this iterative, unsupervised procedure, yieldinga more reliable alignment.
Majority voting on thegenerated lattice is performed using prior weightsfor each system as well as other statistical mod-els such as a special n-gram language model.
Inaddition to lattice rescoring, n-best list rerankingtechniques can be applied to n best paths of thislattice.
True casing is considered a separate stepin RWTH?s approach, which also takes the inputhypotheses into account.The pipeline, and consequently the descriptionof the pipeline given in this paper, is based on ourpipeline for WMT 2009 (Leusch et al, 2009), withseveral extensions as described.2 System Combination AlgorithmIn this section we present the details of our systemcombination method.
Figure 1 gives an overviewof the system combination architecture describedin this section.
After preprocessing the MT hy-potheses, pairwise alignments between the hy-potheses are calculated.
The hypotheses are thenreordered to match the word order of a selectedprimary or skeleton hypothesis.
From this, wecreate a lattice which we then rescore using sys-tem prior weights and a language model (LM).The single best path in this CN then constitutesthe consensus translation; alternatively the n bestpaths are generated and reranked using additionalstatistical models.
The consensus translation isthen true cased and postprocessed.2.1 Word AlignmentThe proposed alignment approach is a statisticalone.
It takes advantage of multiple translations fora whole corpus to compute a consensus translationfor each sentence in this corpus.
It also takes ad-vantage of the fact that the sentences to be alignedare in the same language.For each of the K source sentences in thetest corpus, we select one of its translationsEn, n = 1, .
.
.
,M, as the primary hypothesis.Then we align the secondary hypotheses Em(m=1, .
.
.
,M ;n 6= m) with En to match the word or-der in En.
Since it is not clear which hypothesisshould be primary, i. e. has the ?best?
word order,we let several or all hypothesis play the role of theprimary translation, and align all pairs of hypothe-ses (En, Em); n 6= m. In this paper, we denotethe number of possible primary hypotheses by N .The word alignment is trained in analogy tothe alignment training procedure in statistical MT.The difference is that the two sentences that haveto be aligned are in the same language.
We use theIBM Model 1 (Brown et al, 1993) and the Hid-den Markov Model (HMM, (Vogel et al, 1996))315alignmentGIZA++- Network generation Weighting&RescoringReordering 200-bestlistHyp 1Hyp k... ConsensusTranslationnbestrescoring(Triplets,LM, ...)Figure 1: The system combination architecture.to estimate the alignment model.The alignment training corpus is created from atest corpus of effectively N ?
(M?1)?K sentencestranslated by the involved MT engines.
Model pa-rameters are trained iteratively using the GIZA++toolkit (Och and Ney, 2003).
The training is per-formed in the directions Em ?
En and En ?Em.
The final alignments are determined usinga cost matrix C for each sentence pair (Em, En).Elements of this matrix are the local costs C(j, i)of aligning a word em,j from Em to a word en,ifrom En.
Following Matusov et al (2004), wecompute these local costs by interpolating thenegated logarithms of the state occupation proba-bilities from the ?source-to-target?
and ?target-to-source?
training of the HMM model.2.2 Word Reordering and ConfusionNetwork GenerationAfter reordering each secondary hypothesis Emand the rows of the corresponding alignment costmatrix, we determine M?1 monotone one-to-onealignments between En as the primary translationand Em,m = 1, .
.
.
,M ;m 6= n. We then con-struct the confusion network.We consider words without a correspondence tothe primary translation (and vice versa) to have anull alignment with the empty word ?, which willbe transformed to an ?-arc in the correspondingconfusion network.The M?1 monotone one-to-one alignments canthen be transformed into a confusion network, asdescribed by Matusov et al (2008).2.3 Voting in the Confusion NetworkInstead of choosing a fixed sentence to define theword order for the consensus translation, we gen-erate confusion networks for N possible hypothe-ses as primary, and unite them into a single lattice.In our experience, this approach is advantageousin terms of translation quality compared to a min-imum Bayes risk primary (Rosti et al, 2007).Weighted majority voting on a single confu-sion network is straightforward and analogous toROVER (Fiscus, 1997).
We sum up the probabil-ities of the arcs which are labeled with the sameword and have the same start state and the sameend state.
This can also be regarded as having abinary system feature in a log-linear model.2.4 Language ModelsThe lattice representing a union of several confu-sion networks can then be directly rescored withan n-gram language model (LM).
A transforma-tion of the lattice is required, since LM history hasto be memorized.We train a trigram LM on the outputs of the sys-tems involved in system combination.
For LMtraining, we take the system hypotheses for thesame test corpus for which the consensus transla-tions are to be produced.
Using this ?adapted?
LMfor lattice rescoring thus gives bonus to n-gramsfrom the original system hypotheses, in most casesfrom the original phrases.
Presumably, many ofthese phrases have a correct word order.
Previousexperimental results show that using this LM inrescoring together with a word penalty notably im-proves translation quality.
This even results in bet-ter translations than using a ?classical?
LM trainedon a monolingual training corpus.
We attributethis to the fact that most of the systems we com-bine already include such general LMs.2.5 Extracting Consensus TranslationsTo generate our consensus translation, we extractthe single-best path from the rescored lattice, us-ing ?classical?
decoding as in MT.
Alternatively,we can extract the n best paths for n-best listrescoring.2.6 n-best-List RerankingIf n-best lists were generated in the previous steps,additional sentence-based features can be calcu-lated on these sentences, and combined in a log-linear way.
These scores can then be used to re-rank the sentences.For the WMT 2010 FR?EN and the DE?ENtask, we generated 200-best lists, and calculatedthe following features:1.
Total score from the lattice rescoring2.
NGram posterior weights on those (Zens andNey, 2006)3.
Word Penalty4.
HypLM trained on a different set of hypothe-ses (FR?EN only)5.
Large fourgram model trained on Gigaword(DE?EN) or Europarl (FR?EN)6.
IBM1 scores and deletion counts based on aword lexicon trained on WMT training data3167.
Discriminative word lexicon score (Mauser etal., 2009)8.
Triplet lexicon score (Hasan et al, 2008)Other features were also calculated, but did notseem to give an improvement on the DEV set.2.7 Consensus True CasingPrevious approaches to achieve true cased outputin system combination operated on true-cased lat-tices, used a separate input-independent true caser,or used a general true-cased LM to differenti-ate between alternative arcs in the lattice, as in(Leusch et al, 2009).
For WMT 2010, we useper-sentence information from the input systemsto determine the consensus case of each outputword.
Lattice generation, rescoring, and rerank-ing are performed on lower-cased input, with alower-cased consensus hypothesis as their result.For each word in this hypothesis, we count howoften each casing variant occurs in the input hy-potheses for this sentence.
We then use the vari-ant with the highest support for the final consen-sus output.
One advantage is that the set of sys-tems used to determine the consensus case doesnot have to be identical to those used for buildingthe lattice: Assuming that each word from the con-sensus hypothesis also occurs in one or several ofthe true casing input hypotheses, we can focus onsystems that show a good true casing performance.3 Tuning3.1 Tuning Weights for Lattice and n-bestRescoringFor lattice rescoring, we need to tune systemweights, LM factor, and word penalty to producegood consensus translations.
The same holds forthe log-linear weights in n-best reranking.For the WMT 2010 Workshop, we selecteda linear combination of BLEU (Papineni et al,2002) and TER (Snover et al, 2006) as optimiza-tion criterion, ??
:= argmax?
{BLEU ?
TER},based on previous experience (Mauser et al,2008).
For more stable results, we use the case-insensitive variants for both measures, despite theexplicit use of case information in the pipeline.System weights were tuned to this criterion us-ing the Downhill Simplex method.
Because weconsidered the number of segments in the tuningset to be too small to allow for a further split intoan actual tuning and a control (dev) part, we wentfor a method closely related to 5-fold cross valida-tion: We randomly split the tuning set into 5 equal-sized parts, and tune parameters on four fifth ofthe set, measuring progress on the remaining fifth.This was repeated for the other four choices for the?dev?
part.
Only settings which reliably showedprogress on these five different versions were usedlater on the test set.
For the actual weights andnumerical parameters to be used on the test set,we calculate the median of the five variants, whichlowered the risk of outliers and overfitting.3.2 System SelectionWith the large numbers of input systems ?
e.g., 17for DE?EN ?
and their large spread in translationquality ?
e.g.
10% abs.
in BLEU ?
not all sys-tems should participate in the system combinationprocess.
For the generation of lattices, we con-sidered several variants of systems, often startingfrom the top, and either replacing some of the sys-tems very similar to others with systems furtherdown the list, or not considering those as primary,adding further systems as additional secondaries.For true casing, and the additional HypLM forFR?EN, we selected a set of 8 to 12 promisingsystems, and ran an exhaustive search on all com-binations of those to optimize the LM perplexityon the dev set (LM) or the true case BLEU/TERscore on a consensus translation (TC).
Further re-search may include a weighted combination here,followed by an optimization of the weights as de-scribed in the previous paragraph.4 Experimental ResultsEach language pair and each direction inWMT 2010 had its own set of systems, so we se-lected and tuned for each direction separately.
Af-ter submission of our system combination outputto WMT 2010, we also calculated scores on thetest set (TEST), to validate our results, and as apreparation for this report.
Note that the scores re-ported for DEV are calculated on the full DEV set,but not on any combination of the one-fifth ?crossvalidation?
subcorpora.4.1 FR?EN and EN?FRFor French?English, we selected a set of eightsystems for the primary submission, and elevensystems for the contrastive system, of which sixserved as skeleton.
Six different systems wereused for an additional HypLM, five for consen-sus true casing.
Table 1 shows the distribution ofthese systems.
We see the results of system com-bination on DEV and TEST (the latter calculatedafter submission) in Table 2.
System combinationitself turns out to have the largest improvement,+0.5 in BLEU and -0.7 in TER on TEST over thebest single system.
n-best reranking improves thisresult even more, by +0.3/-0.3.
The influence oftuning and of TC selection is measurable on DEV,but rather small on TEST.For English?French, 13 systems were used toconstruct the lattice, 5 serving as skeleton.
Fivedifferent systems were used for true casing.
Non-best list reranking was performed here, as pre-liminary experiments did not show any significant317Table 1: Overview of systems used for FR/EN.System FR?EN EN?FRA B A Bcambridge P L C p P pcu-zeman Scmu-statxfer L sdfki Seu Sgeneva Shuicong sjhu P L p S pkoc Slig slimsi P C p S C plium P L C s P C pnrc P C s S prali P L p P C prwth P p P C puedin P L C p P C p?A?
is the primary, ?B?
the contrastive submission.?P?
denotes a system that served as skeleton.?S?
a system that was only aligned to others.?L?
denotes a system used for a larger HypLM-n-best-rescoring.?C?
is a system used for consensus true casing.Table 2: Results for FR?EN.TUNE TESTBLEU TER BLEU TERBest single 27.9 55.4 28.5 54.0Lattice SC 28.4 55.0 29.0 53.3+ tuning 28.8 54.5 29.1 53.3+ CV tuning 28.6 54.7 29.1 53.3+ nbest rerank.
29.0 54.4 29.4 53.0+ sel.
for TC 29.1 54.3 29.3 53.0Contrast.
SC 28.9 54.3 28.8 53.4?SC?
stands for System Combination output.?CV?
denotes the split into five different tuning and valida-tion parts.?sel.
TC?
is the separate selection for consensus true casing.Systems in bold were submitted for WMT 2010.Table 3: Results for EN?FR.TUNE TESTBLEU TER BLEU TERBest single 27.1 55.7 26.5 56.1Primary SC 28.3 55.2 28.2 54.7Contrast.
SC 28.5 54.7 28.1 54.6Table 4: Overview of systems used for DE/EN.System DE?EN EN?DEA B A Bcu-zeman Scmu C Pdfki S pfbk P C p Pjhu pkit P C p P C pkoc S C plimsi P p P C pliu C S C prwth P p P C psfu Suedin P C p P C pumd P puppsala p SFor abbreviations see Table 1.Table 5: Results for DE?EN.TUNE TESTBLEU TER BLEU TERBest single 23.8 59.7 23.5 59.7Lattice SC 24.7 58.5 25.0 57.9+ tuning 25.1 57.6 25.0 57.6+ CV tuning 24.8 58.0 24.9 57.8+ nbest rerank.
25.3 57.6 24.9 57.6+ sel.
for TC 25.5 57.5 24.9 57.6Contrast.
SC 25.2 57.7 24.8 57.7For abbreviations see Table 2.gain in this direction.
As a contrastive submission,we submitted the consensus of 8 systems.
Theseare also listed in Table 1.
The results can be foundin Table 3.
Note that the contrastive system wasnot tuned using the ?cross validation?
approach;as a result, we expected it to be sensitive to over-fitting.
We see improvements around +1.7/-1.4 onTEST.4.2 DE?EN and EN?DEIn the German?English language pair, 17 systemswere available, but incorporating only six of themturned out to deliver optimal results on DEV.
Asshown in Table 4, we used a combination of sevensystems in the contrastive submission.
While aTable 6: Results for EN?DE.TUNE TESTBLEU TER BLEU TERBest single 16.1 66.3 16.4 65.7Primary SC 16.4 64.9 17.0 63.7Contrast.
SC 16.4 64.9 17.3 63.4318Table 7: Overview of systems used for CZ/EN.System CZ?EN EN?CZaalto Pcmu P Ccu-bojar P Pcu-tecto Scu-zeman P S Cdcu Peurotrans Sgoogle P C P Ckoc P Cpc-trans Spotsdam P Csfu Suedin P C P CFor abbreviations see Table 1.No contrastive systems were built for this language pair.Table 8: Results for CZ?EN and EN?CZ.TUNE TESTBLEU TER BLEU TERCZ?ENBest single 21.8 58.4 22.9 57.5Primary SC 22.4 59.1 23.4 57.9EN?CZBest single 17.0 67.1 16.6 66.4Primary SC 16.7 65.4 17.4 63.6different set of five systems was used for consen-sus true casing, it turned out that using the samesix systems for the ?additional?
HypLM as forthe lattice seemed to be optimal in our approach.Table 5 shows the outcome of our experiments:Again, we see that the largest effect on TEST re-sults from system combination as such (+1.5/-1.8).The other steps, in particular tuning and selectionfor TC, seem to help on DEV, but make hardlya difference on TEST.
n-best reranking brings animprovement of -0.2 in TER, but at a minor dete-rioration (-0.1) in BLEU.In the opposite direction, English?German, wecombined all twelve systems, five of them serv-ing as skeleton.
The contrastive submission con-sists of a combination of eight systems.
Six sys-tems were used for true casing.
Again, n-bestlist rescoring did not result in any improvementin preliminary experiments, and was skipped.
Re-sults are shown in Table 6: We see that eventhough both versions perform equally well onDEV (+0.4/-1.4), the contrastive system performsbetter by +0.3/-0.3 on TEST (+0.9/-2.3).4.3 CZ?EN and EN?CZIn both directions involving Czech, the number ofsystems was rather limited, so no additional se-Table 9: Overview of systems used for ES/EN.System EN?ESA Bcambridge P C pdcu P pdfki P C pjhu P C psfu P C puedin P C pupv pupv-nnlm P pTable 10: Results for EN?ES.TUNE TESTBLEU TER BLEU TERES?ENBest single 28.7 53.6 ?
?SC 29.0 53.3 ?
?EN?ESBest single 27.8 55.2 28.7 54.0Primary SC 29.5 52.9 30.0 51.4Contrast.
SC 29.6 52.8 30.1 51.7lection turned out to be necessary, and we did notbuild a contrastive system.
For Czech?English, allsix systems were used; three of them for true cas-ing.
For English?Czech, all eleven systems wereused in building the lattice, six of them also asskeleton.
Five systems were used in the true cas-ing step.
Table 7 lists these systems.
From theresults in Table 8, we see that for CZ?EN, systemcombination gains around +0.5 in BLEU, but atcosts of +0.4 to +0.7 in TER.
For EN?CZ, the re-sults look more positive: While we see only -0.3/-1.7 on DEV, there is a significant improvement of+1.2/-2.8 on TEST.4.4 ES?EN and EN?ESIn the Spanish?English language pair, we did notsee any improvement at all on the direction withEnglish as target in preliminary experiments.
Con-sequently, and given the time constraints, we didnot further investigate on this language pair.
Post-eval experiments revealed that improvements of+0.3/-0.3 are possible, with far off-center weightsfavoring the top three systems.On English?Spanish, where these preliminaryexperiments showed a gain, we used seven out ofthe available ten systems in building the latticefor the primary system, eight for the contrastive.Five of those were uses for consensus true cas-ing.
Table 9 lists these systems.
Table 10 showsthe results on this language pair: For both the pri-mary and the contrastive systems we see improve-319ments of around +1.7/-2.3 on DEV, and +1.3/-2.6on TEST.
Except for the TER on TEST, these twosubmissions differ only by ?0.1 from each other.5 ConclusionsWe have shown that our system combination sys-tem can lead to significant improvements over sin-gle best MT output where a significant number ofcomparably good translations is available on a sin-gle language pair.
n-best reranking can furtherimprove the quality of the consensus translation;results vary though.
While consensus true casingturned out to be very useful despite of its simplic-ity, we were unable to find significant improve-ments on TEST from the selection of a separateset of true casing input systems.AcknowledgmentsThis work was partly realized as part of theQuaero Programme, funded by OSEO, FrenchState agency for innovation.
This work waspartly supported by the Defense Advanced Re-search Projects Agency (DARPA) under ContractNo.
HR0011-06-C-0023.ReferencesP.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: parameter estimation.
Compu-tational Linguistics, 19(2):263?311, June.J.
Fiscus.
1997.
A post-processing system to yield re-duced word error rates: Recognizer output voting er-ror reduction (ROVER).
In IEEE Workshop on Au-tomatic Speech Recognition and Understanding.S.
Hasan, J. Ganitkevitch, H. Ney, and J. Andre?s-Ferrer.2008.
Triplet lexicon models for statistical machinetranslation.
In Conference on Empirical Methods inNatural Language Processing, pages 372?381, Hon-olulu, Hawaii, October.
Association for Computa-tional Linguistics.S.
Jayaraman and A. Lavie.
2005.
Multi-engine ma-chine translation guided by explicit word matching.In Proc.
of the 10th Annual Conf.
of the EuropeanAssociation for Machine Translation (EAMT), pages143?152, Budapest, Hungary, May.G.
Leusch, E. Matusov, and H. Ney.
2009.
TheRWTH system combination system for WMT 2009.In Fourth Workshop on Statistical Machine Transla-tion, pages 56?60, Athens, Greece, March.
Associa-tion for Computational Linguistics.E.
Matusov, R. Zens, and H. Ney.
2004.
Symmetricword alignments for statistical machine translation.In COLING ?04: The 20th Int.
Conf.
on Computa-tional Linguistics, pages 219?225, Geneva, Switzer-land, August.E.
Matusov, N. Ueffing, and H. Ney.
2006.
Computingconsensus translation from multiple machine trans-lation systems using enhanced hypotheses align-ment.
In Conference of the European Chapter of theAssociation for Computational Linguistics (EACL),pages 33?40, Trento, Italy, April.E.
Matusov, G. Leusch, R. E. Banchs, N. Bertoldi,D.
Dechelotte, M. Federico, M. Kolss, Y. S. Lee,J.
B. Marino, M. Paulik, S. Roukos, H. Schwenk,and H. Ney.
2008.
System combination for machinetranslation of spoken and written language.
IEEETransactions on Audio, Speech and Language Pro-cessing, 16(7):1222?1237, September.A.
Mauser, S. Hasan, and H. Ney.
2008.
Automaticevaluation measures for statistical machine transla-tion system optimization.
In International Confer-ence on Language Resources and Evaluation, Mar-rakech, Morocco, May.A.
Mauser, S. Hasan, and H. Ney.
2009.
Extending sta-tistical machine translation with discriminative andtrigger-based lexicon models.
In Conference on Em-pirical Methods in Natural Language Processing,pages 210?217, Singapore, August.F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51, March.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.BLEU: a Method for Automatic Evaluation of Ma-chine Translation.
In Proc.
of the 40th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 311?318, Philadelphia, PA, July.A.
V. Rosti, S. Matsoukas, and R. Schwartz.
2007.Improved word-level system combination for ma-chine translation.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Lin-guistics (ACL), pages 312?319, Prague, Czech Re-public, June.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A Study of Translation ErrorRate with Targeted Human Annotation.
In Proc.
ofthe 7th Conf.
of the Association for Machine Trans-lation in the Americas (AMTA), pages 223?231,Boston, MA, August.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-based word alignment in statistical translation.
InCOLING ?96: The 16th Int.
Conf.
on ComputationalLinguistics, pages 836?841, Copenhagen, Denmark,August.R.
Zens and H. Ney.
2006.
N-gram posterior prob-abilities for statistical machine translation.
In Hu-man Language Technology Conf.
/ North AmericanChapter of the Assoc.
for Computational LinguisticsAnnual Meeting (HLT-NAACL), Workshop on Statis-tical Machine Translation, pages 72?77, New YorkCity, June.320
