Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 742?751,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsGenerating Synthetic Comparable Questionsfor News ArticlesOleg RokhlenkoYahoo!
ResearchHaifa 31905, Israelolegro@yahoo-inc.comIdan SzpektorYahoo!
ResearchHaifa 31905, Israelidan@yahoo-inc.comAbstractWe introduce the novel task of automati-cally generating questions that are relevantto a text but do not appear in it.
One mo-tivating example of its application is forincreasing user engagement around newsarticles by suggesting relevant compara-ble questions, such as ?is Beyonce a bet-ter singer than Madonna?
?, for the userto answer.
We present the first algorithmfor the task, which consists of: (a) of-fline construction of a comparable ques-tion template database; (b) ranking of rel-evant templates to a given article; and (c)instantiation of templates only with enti-ties in the article whose comparison un-der the template?s relation makes sense.We tested the suggestions generated byour algorithm via a Mechanical Turk ex-periment, which showed a significant im-provement over the strongest baseline ofmore than 45% in all metrics.1 IntroductionFor companies whose revenues are mainly ad-based, e.g.
Facebook, Google and Yahoo, increas-ing user engagement is an important goal, leadingto more time spent on site and consequently to in-creased exposure to ads.
Examples for typical en-gaging content include other articles for the user toread, updates from the user?s social neighborhoodand votes or comments on videos, blogs etc.In this paper we propose a new way to increaseuser engagement around news articles, namelysuggesting questions for the user to answer, whichare related to the viewed article.
Our motivationis that there are questions that are ?irresistible?because they are fun, involve emotional reactionand expect simple answers.
These are comparativequestions, such as ?is Beyonce a better singer thanMadonna?
?, ?who is better looking, Brad Pitt orGeorge Clooney?
?, ?who is faster: Superman orFlash??
and ?which camera brand do you prefer:Canon or Nikon??
Furthermore, such questionsare social in nature since users would be inter-ested in reading the opinions of other users, similarto viewing other comments (Schuth et al, 2007).Hence, a user that provided an answer may returnto view other answers, further increasing her en-gagement with the site.One approach for generating comparable ques-tions would be to employ traditional question gen-eration, which syntactically transform assertionsin a given text into questions (Mitkov et al, 2006;Heilman and Smith, 2010; Rus et al, 2010).Sadly, fun and engaging comparative questionsare typically not found within the text of newsarticles.
A different approach would be to findconcrete relevant questions within external col-lections of manually generated comparable ques-tions.
Such collections include Community-basedQuestion Answering (CQA) sites such as Yahoo!Answers and Baidu Zhidao and sites that are spe-cialized in polls, such as Toluna.
However, itis highly unlikely that such sources will containenough relevant questions for any news article dueto typical sparseness issues as well as differencesin interests between askers in CQA sites and newsreporters.
To better address the motivating appli-cation above, we propose the novel task of au-tomatically suggesting comparative questions thatare relevant to a given input news article but do notappear in it.To achieve broad coverage for our task, wepresent an algorithm that generates synthetic con-crete questions from question templates, such as?Who is a better actor: #1 or #2??.
Our algorithmconsists of two parts.
An offline part constructsa database of comparative question templates thatappear in a large question corpus.
For a givennews article, an online part chooses relevant tem-742Figure 1: An example news article from OMG!plates for the article by matching between the ar-ticle content and typical template contexts.
Thealgorithm then instantiates each relevant templatewith two entities that appear in the article.
Yet,for a given template, only some of the entities areplausible slot fillers.
For example, ?Madonna?
isnot a reasonable filler for ?Who is a better dad, #1or #2??.
Thus, our algorithm employs entity filter-ing to exclude candidate instantiations that do notmake sense.To test the performance of our algorithm, weconducted a Mechanical Turk experiment that as-sessed the quality of suggested questions for newsarticles on celebrities.
We compared our algo-rithm to a random baseline and to a partial ver-sion of our algorithm that includes a template rel-evance component but lacks filtering of candidateinstantiations.
The results show that the full al-gorithm provided 45% more correct instantiations,but surprisingly also 46% more relevant sugges-tions compared to the stronger baseline.
These re-sults point at the importance of both picking rel-evant templates and smart instantiation selectionto the quality of generated questions.
In addition,they indicate that user perception of relevance isaffected by the correctness of the question.2 Motivation and Algorithmic OverviewBefore we detail our algorithm, we provide somemotivations and insights to the design choices wetook in our algorithm, which also indicate the dif-ficulties inherent in the task.2.1 MotivationGiven a news article, our algorithm generates a setof comparable questions for the article from ques-tion templates, e.g.
?who is faster #1 or #2?
?.Though the template words typically do not ap-pear in the article, they need to be relevant to it?scontent, that is they should correspond to one ofthe main themes in the article or to one of the pub-Figure 2: A high-level overview of the comparablequestion generation algorithm.
The offline part iscolored dark grey and the online part is coloredlight blue.lic interests of the compared entities.
For example,?who is a better dad #1 or #2??
is relevant to thearticle in Figure 1, while ?who is faster #1 or #2?
?is not relevant.
Therefore, we need to model thetypical contents to which each template is relevant.Looking at the structure of comparable ques-tions, we observed that a specific comparable re-lation, such as ?better dad?
and ?faster?, can usu-ally be combined with named entities in severalsyntactic ways to construct a concrete question.We encode this information in generic compara-ble templates, e.g.
?who is a RE: #1 or #2??
and?is #1 a RE than #2?
?, where RE is a slot for acomparable relation and #1 and #2 are slots for en-tities.
Using the above generic templates, ?Jet Li?and ?Jackie Chan?
can be combined with the com-parable relation ?better fighter?
to generate ?whois a better fighter: Jackie Chan or Jet Li??
and ?isJackie Chan a better fighter than Jet Li??
respec-tively.
Following, our algorithm separately main-tains comparable relations and generic templates.In this paper we constrain ourselves to generatecomparable questions between entities that appearin the article.
Yet, not all entities can be comparedto each other under a specific template, addingsubstantial complexity to the generation of ques-tions.
Looking at Figure 1, the generated question?who is faster, Angelina Jolie or David Beckham?
?makes sense with respect to David Beckham, butnot with respect to Angelina Jolie, since the typi-cal reader is rarely interested in her running skills.Our algorithm thus needs to assess whether an in-stantiation is correct, that is whether the compar-ison between the two entities makes sense underthe specific template.743Further delving into question correctness, theabove example shows the need to assess each en-tity by itself.
However, even if both entities areindependently valid for the template, their com-parison may not make sense.
For example, ?whois better looking: Will Smith or Angelina Jolie?
?doesn?t feel right, even though each entity by itselffits the template.
This is because when comparinglooks, we expect a same sex comparison.2.2 Algorithmic OverviewThe above observations led us to the design of theautomatic generation algorithm depicted in Fig-ure 2.
The algorithm?s offline part constructs, froma large collection of questions, a database of com-parable relations, together with their typical con-texts.
It also extracts generic templates and themapping to the relations that may instantiate them.From this database, we learn: (a) a context profileper template for relevance matching; (b) a singleentity model per template slot that identify validinstantiations; and (c) an entity pair model thatdetects pairs of entities that can be compared to-gether under the template.
In the online part, thesethree models are applied to rank relevant templatesfor a given article and to generate only correctquestions with respect to template instantiation.The next two sections detail the template extrac-tion component and the model training and appli-cation component in our algorithm.3 Comparable Question MiningTo suggest comparable questions our algorithmneeds a database of question templates.
As dis-cussed previously, a good source for mining suchtemplates are CQA sites.
Specifically, in this studywe utilize all questions submitted to Yahoo!
An-swers in 2011 as our corpus.
We next describehow comparable relations and generic comparabletemplates are extracted from this corpus.3.1 Comparable Relation ExtractionAn important observation for the task of compa-rable relation extraction is that many relations arecomplex multiword expressions, and thus their au-tomatic detection is not trivial.
Examples for suchrelations are marked in the questions ?Who is thebest rapper alive, Eminem or Jay-z??
and ?Whois the most beautiful woman in the world, AdrianaLima or Jessica Alba??.
Therefore, we decided toemploy a Conditional Random Fields (CRF) tag-ger (Lafferty et al, 2001) to the task, since CRFwas shown to be state-of-the-art for sequential re-lation extraction (Mooney and Bunescu, 2005; Cu-lotta et al, 2006; Jindal and Liu, 2006).As a pre-processing step for detecting compara-ble relations, our extraction algorithm identifies allthe named entities of interest in our corpus, keep-ing only questions that contain at least two entities.In each of remaining questions, we then substitutethe entity names with the variable slots #i in theorder of their appearance.
For example, ?NnamdiAsomugha vs. Darrelle Revis?
Who is the bettercornerback??
turned into ?#1 vs. #2?
Who is thebetter cornerback??.
This transformation helps usto design a simpler CRF than that of (Jindal andLiu, 2006), since our CRF utilizes the known po-sitions of the target entities in the text.To train the CRF model, the authors manuallytagged all comparable relation words in approx-imately 300 transformed questions in the filteredcorpus.
The local and global features for the CRF,which we induce from each question word, arespecified in Figures 3 and 4 respectively.
Thoughthere are many questions in Yahoo!
Answers con-taining two named entities, e.g.
?Is #1 dating#2?
?, our CRF tagger is trained to detect onlycomparable relations like ?Who is prettier #1 or#2??.
This is due to the labeled training set, whichcontains only this kind of relations, and to our fea-tures, which capture aspects of this specific lin-guistic structure.The trained model was then applied to all otherquestions in the filtered corpus.
This tagging pro-cess resulted in 60,000 identified question relationoccurrences.
From this output we constructed adatabase consisting of all occurring relations; eachrelation is accompanied by its supporting ques-tions, those questions in which the relation occur-rences were found.
To achieve a highly accuratedatabase, we filtered out relations with less than50 supporting questions, ending with 295 relationsin our database1.
The authors conducted a manualevaluation of the CRF tagger performance, whichshowed 80% precision per occurrence.
Yet, ourfiltering above of relations with low support leftus with virtually 100% precision per relation andper occurrence.1We intend to make this database publicly avail-able under Yahoo!
WebscopeTM (http://webscope.sandbox.yahoo.com).2http://nlp.stanford.edu/software/744(a) The word itself(b) Whether the word is capitalized(c) The word?s suffixes of length 1,2, and 3, which helpsin detecting comparative adjectives that ends ?est?
or ?er?
(d) The word?s position in the sentence(e) The word?s Part of speech (POS) tag, based on theStanford POS tagger2(f) The words in a window of ?3 around the current one(g) The adjective before the word, if exists, which helpsdetecting comparative noun phrases, e.g.
?better driver?and ?best singer?
(h) The shortest word distance between the word and oneof the #i variables.
(i) The shortest word distance of the word to one of thefollowing connectives: ?between?, ?out?, ?
:?, ?,?, ??
?Figure 3: CRF local features for each word(a) WH question type of the question, e.g.
what, which,who, where(b) The average word distance between all #i variables inthe question(c) The conjunction tokens appearing between the #i vari-ables, such as or, vs, andFigure 4: CRF global features for each word3.2 Comparable Template ExtractionOur second mining task is to extract generic com-parable templates that appear in our corpus, aswell as identifying which comparable relation caninstantiate which generic template.To this end, we replace each recognized rela-tion sequence with a variable RE in the supportquestions annotated with #i variables.
For exam-ple, ?who is the best rapper alive, #1 or #2??
istransformed to ?who is RE, #1 or #2??.
We nextcount the occurrences of each templatized ques-tion.
While some questions contain many de-tails besides the comparable generic template, oth-ers are simpler and contain only the generic tem-plate.
Through this counting, frequently occurringgeneric templates are revealed, such as ?is #1 aRE than #2??.
We retain only generic templateswhich appeared more than 50 times.Finally, for each comparable relation we markas applicable only generic templates that occur atleast once in the supporting questions of this rela-tion.
For example, the template ?who is RE: #1 or#2??
was found applicable for ?funnier?, and thuscould be used to generate the concrete question?who is funnier: Jennifer Aniston or CourteneyCox??.
On average, each relation was associatedwith 3 generic templates.Algorithm 1 A high level overview of the onlinepart of the question generation algorithmInput: A news articleOutput: A sorted list of comparable questions1: Identify all target named entities (NEs) in the article2: Infer the distribution of LDA topics for the article3: For each comparable relation R in the database, computeits relevance score to be the similarity between the topicdistributions of R and the article4: Rank all the relations according to their relevance scoreand pick the top M as relevant5: for each relevant relation R in the order of relevanceranking do6: Filter out all the target NEs that do not pass the singleentity classifier for R7: Generate all possible NE pairs from the those thatpassed the single classifier8: Filter out all the generated NE pairs that do not passthe entity pair classifier for R9: Pick up the top N pairs with positive classificationscore to be qualified for generation10: Instantiate R with each chosen NE pair via a ran-domly selected generic template11: end for4 Online Question GenerationThe online part of our automatic generation algo-rithm takes as input a news article and generatesconcrete comparable questions for it.
Its high leveldescription is presented in Algorithm 1.
The algo-rithm starts with identifying the comparable rela-tions in our database that are relevant to the arti-cle.
For each relevant relation, we then generateconcrete questions by picking generic templatesthat are applicable for this relation and instantiat-ing them with pairs of named entities appearing inthe article.
Yet, as discussed before, only for someentity pairs the comparison under the specific re-lation makes sense, a quality which we refer to asinstantiation correctness (see Section 2).
To thisend, we utilize two supervised models to filter in-correct instantiations.
We next detail the two as-pects of the online part: ranking relevant relationsand correctly instantiating relations.4.1 Ranking relevant relationsTo assess how relevant a given comparable rela-tion is to an article, we model the relation?s typ-ical context as a distribution over latent semantictopics.
Specifically, we utilize Latent Dirichlet Al-location (LDA) (Blei et al, 2003) to infer latenttopics in texts.To train an LDA model, we constructed for eachcomparable relation a pseudo-document consist-ing of all questions that contain this relation inour corpus (the supporting questions).
We then745trained a model of 200 topics over these pseudo-documents, resulting in a model over a lexicon of107,835 words.
An additional product of the LDAtraining process is a topic distribution for each re-lation?s pseudo-document, which we consider asthe relation?s context profile.
We note that, un-less otherwise specified, different model param-eters were chosen based on a small held out col-lection of articles and questions, manually anno-tated by the authors.
This collection was used tovalidate that the chosen parameter values indeed?make sense?
for the task.Given a news article, a distribution over LDAtopics is inferred from the article?s text using thetrained model.
Then, a cosine similarity betweenthis distribution and the context profile of eachcomparable relation in our database is computedand taken as the relevance score for this relation.Finally, we rank all relations according to their rel-evance score and pick the top M as candidates forinstantiation (M=3 in our experiment).4.2 Correctly instantiating relationsTo generate useful questions from relevant com-parable relations, we need to retain only correctinstantiations of these relations.
To this end, weutilize two complementing types of filters, onefor each entity by itself, and one for pairs, sinceeach filter considers different attributes of the en-tities at hand.
For example, for the relation ?isfaster?, the single entity filter looks for athletes ofall kinds, for whom this comparison is of interestto the reader.
The pair filter, on the other hand, at-tempts to pass only same sex and same professioncomparisons, e.g.
male football players or femalebaseball players for this relation.We next describe the various features we ex-tract for every entity and the supervised modelsthat given this feature vector representation assessthe correctness of an instantiation.4.2.1 Entity FeaturesWe want to represent each entity as a vector of fea-tures that capture different aspects of entity char-acterization.
To this end, we utilize two differentbroad-scale sources of information about namedentities.
The first is DBPedia3, which containsstructured information on entries in Wikipedia,many of them are named entities that appear innews articles.
The second source is the corpus of3http://wiki.dbpedia.org/AboutCQA questions, which in our study was harvestedfrom Yahoo!
Answers (see Section 3).For named entities with a DBPedia en-try, we extract all the DBPedia properties ofclasses subject and type as indicator features.Some example features for Brad Pitt includeActors from Oklahoma, AmericanAtheists, Artistand American film producers.One property that is currently missing from DB-Pedia is gender, a feature that was found to be veryuseful in our experiments.
We automatically in-duce this feature from the Wikipedia abstract ineach DBPedia entry.
Specifically, we construct ahistogram of male and female pronouns: he andhis vs. she and her.
The majority pronoun sex isthen chosen to be the gender of the named entity,or none if the histogram is empty.One way to utilize the CQA question corpuscould be to extract co-occurring words with eachtarget entity as relevant contexts.
Yet, sinceour questions come from Yahoo!
Answers, wedecided to use another attribute of the ques-tions, the category to which the question is as-signed, within a hierarchy of 1,669 categories(e.g.
?Sports>Baseball?
and ?Pets>Dogs?).
Foreach named entity, we construct a histogram ofthe number of questions containing it that are as-signed to each category.
This histogram is normal-ized into a probability distribution with Laplacesmoothing of 0.03, to incorporate the uncertaintythat lies in named entities that appear only veryfew times.
The categories and their probabilitiesare added as features, providing a high level rep-resentation of relevant contexts for the entity.4.2.2 Single entity filteringWe view the task of single entity filtering as a clas-sification task.
To this end, we trained a classifierper relation, constructing a different labeled train-ing set for each relation.
Positive examples are theentities that instantiate this relation in our CQAcorpus.
As negative examples, we take named en-tities that were never seen instantiating the relationin the corpus, but still occurred in some questions.We note that our named entity tagger could recog-nize more than 200,000 named entities, and mostof them are negative for a given relation.For each relation we select negative examplesby sampling uniformly from its negative entity list,assuming that the probability of hitting false neg-atives is low for such a long list.
It is knownthat better classification performance is typically746achieved for a balanced training set (Provost,2000).
In our case, we over sample to help theclassifier explore the large space of negative ex-amples.
Specifically, we sample 2,000 negativeexamples and duplicate the positive set to reacha similar number.We utilize the Support Vector Machines (SVM)implementation of LIBSVM (Chang and Lin,2011) with a linear kernel as our classifier.
Thefeature vector of each named entity was inducedas described in Section 4.2.1.
We split the labeleddataset into 70% training set and 30% validationset.
Feature selection using information gain wasperformed on the training set to throw out non-significant features (Mitchell, 1997).
The averageaccuracy of the single classifiers, measured overthe validation sets, was 91%.4.2.3 Entity pair filteringSimilar to single entity filtering, we view the taskof filtering entity pairs as a classification task,training a separate classifier for each relation.
En-tity pairs that instantiate the given relation in thequestion corpus are considered positive examples.Yet, the space of all the pairs that never instanti-ated the relation is huge, and the set of positiveexamples is relatively much smaller compared tothe situation in the single entity classifier.
In ourstudy, uniform negative example sampling turnedthe training into a trivial task, preventing fromthe classifier to choose an useful discriminativeboundary.
Therefore, we generate negative exam-ples by sampling only from pairs of named enti-ties that both pass the single entity filter for thisrelation.
The risk here is that we may sample falsenegative examples.
Still, this sampling scheme en-abled the classifier to identify better discriminativefeatures.To generate features for a candidate pair, wetake the two feature vectors of the two entitiesand induce families of pair features by compar-ing between the two vectors.
Figure 5 describesthe various features we generate.
We utilize LIB-SVM with an RBF kernel for this task, splittingthe examples into 70% training set and 30% vali-dation set.
We over sampled the positive examplesto reach up to 100 examples.The average accuracy of the pair classifiers onthe validation set was 83%.
For example, namedentities that pass the single entity filtering for?be funny?, include Jay Leno, David Letterman(American TV hosts), Jim Carrey, and Steve Mar-(a) All shared DBPedia indicator features in the two vec-tors: fDBPediaa ?
fDBPediab , indicating them as shared,e.g.
?FilmMaker s?
(b) All DBPedia features that appear only in one of thevectors, termed one-side features: fDBPediaa \ fDBPediaband fDBPediab \ fDBPediaa , indicating them as such, e.g.
?FilmMaker o?
(c) Wikipedia categories that are ancestors of at least twoone-side features that appear in the training set.
For ex-ample, a common ancestor of ?Spanish actors?
and ?Rus-sian actors?
is ?European actors?.
These features providea high level perspective on one-side features(d) The Yahoo!
Answers categories in which both namedentities appear(e) Hellinger distance (Pollard, 2001) between the proba-bility distributions over categories of the two entities(f) Three indicator gender features: whether both namedentities are males, both are females or are differentFigure 5: The entity pair features generated fromtwo single entity feature vectors fa and fbtin (actors).
The pair classifier assigned positivescores only to {Jay Leno, David Letterman} (TVhosts) and {Jim Carrey, Steve Martin} (actors) butnot to other pairings of these entities.5 Evaluation5.1 Experimental SettingsTo evaluate our algorithm?s performance, we de-signed a Mechanical Turk (MTurk) experiment inwhich human annotators assess the quality of thequestions that our algorithm generates for a sam-ple of news articles.
As the source of test arti-cles, we chose the OMG!
website4, which containsnews articles on celebrities.Test articles were selected by first randomlysampling 5,000 news article from those that wereposted on OMG!
in 2011.
We then filtered out ar-ticles that are longer than 4,000 characters, whichwere found to be tiresome for annotators to read,and those that are shorter than 300 characters,which consist mainly of video and photos.
Wewere left with a pool of 1,016 articles from whichwe randomly sampled 100 as the test set.For each test article our algorithm obtained thetop three relevant comparable relations, and foreach relation selected the best instantiation (if ex-ists).
We used two baselines for performance com-parison.
The first random baseline chooses a rela-tion randomly out of all possible relations in thedatabase and then instantiates it with a randompair of entities that appear in the article.
The sec-ond relevance baseline chooses the most relevant4http://www.omg.com/747Relevance CorrectnessRandom baseline 29% 43%Relevance baseline 37% 53%Full algorithm 54% 77%Table 1: Relevance and correctness percentage bytested algorithmrelation to the article based on our algorithm, butstill instantiates it with a random pair.
For each testarticle, we presented to the evaluators the ques-tions generated by the three tested algorithms ina random order to avoid any bias.
We note thatour second baseline enabled us to measure thestand-alone contribution of the LDA-based rele-vance model.
In addition, it enabled us to measurethe relative contribution of the instantiation mod-els on top of relevance model.Each article was evaluated by 10 MTurk work-ers, which were asked to mark for each displayedquestion whether it is relevant and whether it iscorrect (see Section 2 for relevance and correct-ness definitions).
The workers were given pre-cise instructions along with examples before theystarted the test.
A control story was used to filterout dishonest or incapable workers5.5.2 ResultsFor each tested algorithm, we separately countedthe percentage of annotations that marked eachquestion as relevant and the percentage of anno-tations that marked each question as instantiatedcorrectly, denoted relevance score and correctnessscore.
We then averaged these scores over allquestions that were displayed for the test articles.The results are presented in Table 1.
The differ-ences between the full algorithm and the baselinesare statistically significant at p < 0.01 and betweenbaselines the differences are statistically signifi-cant at p < 0.05 using the Wilcoxon double-sidedsigned-ranks test (Wilcoxon, 1945).Our main result is that our full algorithm sub-stantially outperforms the stronger relevance base-line.
It improves the correctness score by 45%,which points at the effectiveness of our two stepfiltering of incorrect instantiations.
It?s perfor-mance is just under 80%, showing high qualityentity pair selection for relations.
Yet, we did notexpect to see an increase of 46% in the relevance5We intend to make the tested articles, the instructionsto annotators and their annotations publicly available underYahoo!
WebscopeTM (http://webscope.sandbox.yahoo.com).metric, since both the full algorithm and the rele-vance baseline use the same relevance componentto rank relations by.
One explanation for this isthat sometimes the instantiation filter eliminatesall possible entity pairs for some relation that isincorrectly considered relevant by the algorithm.Thus, the filtering of entities provides also an ad-ditional filtering perspective on relevance.
In ad-dition, it may be that humans tend to be morepermissive when assessing the relevance of a cor-rectly instantiated question.To illustrate the differences between baselinesand the full algorithm, Table 2 presents an exam-ple article together with the suggested generatedquestions by each algorithm.
The random baselinepicked an irrelevant relation, and while the rele-vance baseline selected a relevant relation, ?a bet-ter president?, it was instantiated incorrectly.
Thefull algorithm, on the other hand, both chose rel-evant relations for all three questions and instan-tiated them correctly.
Especially, the incorrectlyinstantiated relation in the relevance baseline isnow correctly instantiated with plausible presiden-tial candidates.Comparing between baselines, the relevancebaseline beats the random baseline by 28% interms of relevance.
This is not surprising, sincethis was the focus of this baseline.
Yet, it also im-proved correctness by 23% over the random base-line.
This is an unexpected result that indicatesthat when users view relevant relations, they maybe more forgiving in their perception of unreason-able instantiations.For each article, our full algorithm attempts togenerate three questions, one for each of the topthree relevant questions.
It is possible that forsome articles not all three questions will be gen-erated, due to instantiation filtering.
We foundthat for 85% of the articles all three questionswere generated.
For the remaining 15% at leastone question was always generated, and for 13 ofthem two questions were composed.
Furthermore,we found that the relevance and correctness scoreswere not affected by the position of the question.In the case of instantiation correctness, since thebest pair was picked for each relation and thiscomponent is quite accurate, this is somewhat ex-pected.
In the case of relevance, this indicates thatthere are usually several relations in our databasethat are relevant to the article.748Ron Livingston is teaming up with Tom Hanks and HBO again after their successful 2001 collaboration on Band of Broth-ers.
The actor has been cast in HBO?s upcoming film Game Change that centers on the 2008 presidential campaign,Deadline reports.
He joins Ed Harris, Julianne Moore and Woody Harrelson.
The Jay Roach-directed movie follows JohnMcCain (Harris) as he selects Alaska Gov.
Sarah Palin (Moore) as his running mate, throughout the campaign and to theirultimate defeat to Barack Obama.
Livingston will play Mark Wallace, one of the campaign?s senior advisors and the manwho prepped Palin for her debate.
Harrelson will play campaign strategist Steve Schmidt.
.
.Algorithm QuestionRandom baseline Who is a better singer, Sarah Palin or Barack Obama ?Relevance baseline Would Ron Livingston be a better president than Julianne Moore ?Full algorithm Who has the best movies Tom Hanks or Julianne Moore ?Full algorithm Is John Mccain a better leader than Barack Obama ?Full algorithm Would Sarah Palin be a better president than John Mccain ?Table 2: Automatically generated questions by the baselines and the full algorithm to an example article5.3 Error AnalysisTo better understand the performance of our algo-rithm, we looked at some low quality questionsthat were generated, either due to incorrect instan-tiation or due to irrelevance to the article.Starting with relevance, one of the repeatingmistakes was promoting relations that are relatedto a list of named entities in the article, but not toits main theme.
For example, the relation ?who isa better actor?
was incorrectly ranked high for anarticle about Ricky Gervais claiming that he hasbeen asked to host Globes again after he offendedAngelina Jolie, Johnny Depp, Robert DowneyJr.
and Charlie Sheen, among others during lastGlobes ceremony.
The reason for this mistake isthat many named entities appear as frequent termsin LDA topics, and thus mentioning many namesthat belong to a single topic drives LDA to as-sign this topic a high probability.
Yet, unlike othercases, here entity filtering does not help ignoringsuch errors, since the same entities that triggeredthe ranking of the relation are also valid instantia-tions for it.Analyzing incorrect instantiations, many mis-takes are due to mismatches between the two com-pared entities that were too fine grained for our al-gorithm to catch.
For example, ?who?s the betterguitarist: Paul McCartney or Ringo Starr??
wasgenerated since our algorithm failed to identifythat Ringo Starr is a drummer rather than a gui-tarist, though both participants in the relation aremusicians.
In other cases, strong co-occurrence ofthe two celebs in our question corpus convincedthe classifiers that they can be matched.
For ex-ample, ?who is a better dancer Michael Jacksonor Debbie Rowe??
was incorrectly generated,since Debbie Rowe is not a dancer.
Yet, she wasMichael Jackson?s wife and they appear togetherin a lot of questions in our corpus.6 Related WorkTraditionally, question generation focuses on con-verting assertions in a text into question forms(Brown et al, 2005; Mitkov et al, 2006; Myller,2007; Heilman and Smith, 2010; Rus et al, 2010;Agarwal et al, 2011; Olney et al, 2012).
Tothe best of our knowledge, there is no prior workon our task, which is to generate relevant syn-thetic questions whose content, except for the ar-guments, might not appear in the text.Our extraction of comparable relations fallswithin the field of Relation Extraction, in whichCRF is a state-of-the-art method (Mooney andBunescu, 2005; Culotta et al, 2006).
We notethat in the works of Jindal and Liu (2006) andLi et.
al.
(2010) comparative questions are iden-tified as an intermediate step for the task of ex-tracting compared entities, which are unknown intheir setting.
We, on the other hand, detect thecompared entities in a pre-processing step, and ourtarget is the extraction of the comparable relationsgiven known candidate entities.Our algorithm ranks relevant templates basedon the similarity between an article?s content andthe typical context of each relation.
Prior workrank relevant concrete questions to a given in-put question, focusing on strong lexical similari-ties (Jeon et al, 2005; Cai et al, 2011; Hao andAgichtein, 2012).
We, however, do not expect tofind direct lexical similarities between candidaterelations and the article.
Instead, we are interestedin a higher level topical similarity to the input ar-ticle, for which LDA topics were shown to help(Celikyilmaz et al, 2010).Finally, several works present unsupervisedmethods for ranking proper template instantia-749tions, mainly as selectional preferences (Light andGreiff, 2002; Erk, 2007; Ritter et al, 2010).
How-ever, we eventually choose instantiation candi-dates, and thus preferred supervised methods thatenable filtering and not just ranking.
Furthermore,we target a more subtle discrimination betweenentities than prior work, e.g.
between quarter-backs, singers and actors.
Machine learning nat-urally incorporates the many features that capturedifferent aspects of entity characterization.7 ConclusionsWe introduced the novel task of automatically gen-erating synthetic comparable questions that arerelevant to a given news article but do not neces-sarily appear in it.
To this end, we proposed analgorithm that consists of two parts.
The offlinepart identifies comparable relations in a large col-lection of questions.
Its output is a database ofcomparable relations together with a context pro-file for each relation and models that detect cor-rect instantiations of this relation, all learned fromthe question corpus.
In the online part, given anews article, the algorithm identifies relevant com-parable relations based on the similarity betweenthe article content and each relation?s context pro-file.
Then, relevant relations are instantiated onlywith pairs of named entities from the article whosecomparison makes sense by applying the instanti-ation correctness models to candidate pairs.We assessed the performance of our algorithmvia a Mechanical Turk experiment.
A partial ver-sion of our algorithm, without instantiation filter-ing, was our strongest baseline.
The full algorithmoutperformed this baseline by 45% on questioncorrectness, but surprisingly also by 46% on ques-tion relevance.
These results show that our super-vised filtering methods are successful in keepingonly correct pairs, but they also serve as an ad-ditional filtering for relevant relations, on top ofcontext matching.In future work, we want to generate more di-verse and intriguing questions by selecting rele-vant named entities for template instantiation thatdo not appear in the article.
Another directionwould be take a supervised approach, trainingclassifiers over a labeled dataset for filtering irrel-evant templates and incorrect instantiations.
Fi-nally, it would be interesting to see how our algo-rithm performs on other news domains.ReferencesManish Agarwal, Rakshit Shah, and Prashanth Man-nem.
2011.
Automatic question generation usingdiscourse cues.
In Proceedings of the 6th Workshopon Innovative Use of NLP for Building EducationalApplications, IUNLPBEA ?11, pages 1?9, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022, March.Jonathan C. Brown, Gwen A. Frishkoff, and MaxineEskenazi.
2005.
Automatic question generation forvocabulary assessment.
In Proceedings of the con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,HLT ?05, pages 819?826, Stroudsburg, PA, USA.Association for Computational Linguistics.Li Cai, Guangyou Zhou, Kang Liu, and Jun Zhao.2011.
Learning the latent topics for question re-trieval in community qa.
In Proceedings of 5th In-ternational Joint Conference on Natural LanguageProcessing, pages 273?281, Chiang Mai, Thailand,November.
Asian Federation of Natural LanguageProcessing.Asli Celikyilmaz, Dilek Hakkani-Tur, and Gokhan Tur.2010.
Lda based similarity modeling for questionanswering.
In Proceedings of the NAACL HLT 2010Workshop on Semantic Search, SS ?10, pages 1?9,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Chih-Chung Chang and Chih-Jen Lin.
2011.
Libsvm:A library for support vector machines.
ACM TIST,2(3):27.Aron Culotta, Andrew McCallum, and Jonathan Betz.2006.
Integrating probabilistic extraction modelsand data mining to discover relations and patterns intext.
In Proceedings of the main conference on Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association of Computa-tional Linguistics, HLT-NAACL ?06, pages 296?303, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Katrin Erk.
2007.
A simple, similarity-based modelfor selectional preferences.
In Proceedings of the45th Annual Meeting of the Association of Compu-tational Linguistics, pages 216?223, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Tianyong Hao and Eugene Agichtein.
2012.
Findingsimilar questions in collaborative question answer-ing archives: toward bootstrapping-based equivalentpattern learning.
Inf.
Retr., 15(3-4):332?353, June.Michael Heilman and Noah A. Smith.
2010.
Goodquestion!
statistical ranking for question generation.750In Human Language Technologies: The 2010 An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics, HLT?10, pages 609?617, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
2005.Finding similar questions in large question and an-swer archives.
In Proceedings of the 14th ACM in-ternational conference on Information and knowl-edge management, CIKM ?05, pages 84?90, NewYork, NY, USA.
ACM.Nitin Jindal and Bing Liu.
2006.
Mining comparativesentences and relations.
In proceedings of the 21stnational conference on Artificial intelligence - Vol-ume 2, AAAI?06, pages 1331?1336.
AAAI Press.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of ICML.Shasha Li, Chin-Yew Lin, Young-In Song, and Zhou-jun Li.
2010.
Comparable entity mining from com-parative questions.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 650?658.
Associationfor Computational Linguistics.Marc Light and Warren R. Greiff.
2002.
Statisticalmodels for the induction and use of selectional pref-erences.
Cognitive Science, 26(3):269?281.Tom M. Mitchell.
1997.
Machine learning.
McGrawHill series in computer science.
McGraw-Hill.Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis.2006.
A computer-aided environment for gener-ating multiple-choice test items.
Nat.
Lang.
Eng.,12(2):177?194, June.Raymond J. Mooney and Razvan Bunescu.
2005.
Min-ing knowledge from text using information extrac-tion.
SIGKDD Explor.
Newsl., 7(1):3?10, June.Niko Myller.
2007.
Automatic generation of predic-tion questions during program visualization.
Elec-tron.
Notes Theor.
Comput.
Sci., 178:43?49, July.A.M.
Olney, A.C. Graesser, and N.K.
Person.
2012.Question generation from concept maps.
Dialogue& Discourse, 3(2):75?99.D.
Pollard.
2001.
A User?s Guide to Measure Theo-retic Probability.
Cambridge University Press.F.
Provost.
2000.
Machine learning from imbalanceddata sets 101.
Proceedings of the AAAI-2000 Work-shop on Imbalanced Data Sets.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A la-tent dirichlet alocation method for selectional pref-erences.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguistics,ACL ?10, pages 424?434, Stroudsburg, PA, USA.Association for Computational Linguistics.Vasile Rus, Brendan Wyse, Paul Piwek, Mihai C. Lin-tean, Svetlana Stoyanchev, and Cristian Moldovan.2010.
The first question generation shared task eval-uation challenge.
In John D. Kelleher, Brian MacNamee, Ielka van der Sluis, Anja Belz, Albert Gatt,and Alexander Koller, editors, INLG 2010 - Pro-ceedings of the Sixth International Natural Lan-guage Generation Conference, July 7-9, 2010, Trim,Co.
Meath, Ireland.
The Association for ComputerLinguistics.Anne Schuth, Maarten Marx, and Maarten de Rijke.2007.
Extracting the discussion structure in com-ments on news-articles.
In Proceedings of the 9thannual ACM international workshop on Web infor-mation and data management, WIDM ?07, pages97?104, New York, NY, USA.
ACM.Frank Wilcoxon.
1945.
Individual comparisons byranking methods.
Biometrics Bulletin, 1:80?83.751
