Workshop on Monolingual Text-To-Text Generation, pages 84?90,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 84?90,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsParaphrastic Sentence Compression with a Character-based Metric:Tightening without DeletionCourtney Napoles1 and Chris Callison-Burch1 and Juri Ganitkevitch1 andBenjamin Van Durme1,21Department of Computer Science2Human Language Technology Center of ExcellenceJohns Hopkins UniversityAbstractWe present a substitution-only approach tosentence compression which ?tightens?
a sen-tence by reducing its character length.
Replac-ing phrases with shorter paraphrases yieldsparaphrastic compressions as short as 60% ofthe original length.
In support of this task,we introduce a novel technique for re-rankingparaphrases extracted from bilingual corpora.At high compression rates1 paraphrastic com-pressions outperform a state-of-the-art dele-tion model in an oracle experiment.
For fur-ther compression, deleting from oracle para-phrastic compressions preserves more mean-ing than deletion alone.
In either setting, para-phrastic compression shows promise for sur-passing deletion-only methods.1 IntroductionSentence compression is the process of shortening asentence while preserving the most important infor-mation.
Because it was developed in support of ex-tractive summarization (Knight and Marcu, 2000),much of the previous work considers deletion-basedmodels, which extract a subset of words from a longsentence to create a shorter sentence such that mean-ing and grammar are maximally preserved.
Thisframework imposes strict constraints on the task anddoes not accurately model human-written compres-sions, which tend to be abstractive rather than ex-tractive (Marsi et al, 2010).
This is one sense inwhich paraphrastic compression can improve exist-ing compression methodologies.1Compression rate is defined as the compression length overoriginal length, so lower values indicate shorter sentences.We distinguish two non-identical notions of sen-tence compression: making a sentence substantiallyshorter versus ?tightening?
a sentence by remov-ing unnecessary verbiage.
We propose a method totighten sentences with just substitution and no dele-tion operations.
Using paraphrases extracted frombilingual text and re-ranked on monolingual data,our system selects the set of paraphrases that min-imizes the character length of a sentence.While not currently the standard, character-basedlengths have been considered before in compres-sion, and we believe that it is relevant for currentand future applications.
Character lengths have beenused for document summarization (DUC 2004, Overand Yen (2004)), summarizing for mobile devices(Corston-Oliver, 2001), and subtitling (Glickman etal., 2006).
Although in the past strict word limitshave been imposed for various documents, informa-tion transmitted electronically is often limited by thenumber of bytes, which directly relates to the num-ber of characters.
Mobile devices, SMS messages,and microblogging sites such as Twitter are increas-ingly important for quickly spreading information.In this context, it is important to consider character-based constraints.We examine whether paraphrastic compressionallows more information to be conveyed in the samenumber of characters as deletion-only compressions.For example, the length constraint of Twitter posts ortweets is 140 characters, and many article lead sen-tences exceed this limit.
A paraphrase substitutionoracle compresses the sentence in the table below to76% of its original length (162 to 123 characters; thefirst is the original).
The compressed tweet is 14084characters, including spaces 17-character shortenedlink to the original article.2Congressional leaders reached a last-gasp agreementFriday to avert a shutdown of the federal government,after days of haggling and tense hours of brinksman-ship.Congress made a final agreement Fri. to avoid govern-ment shutdown, after days of haggling and tense hoursof brinkmanship.
on.wsj.com/h8N7n1In contrast, using deletion to compress to the samelength may not be as expressive:Congressional leaders reached agreement Friday toavert a shutdown of federal government, after hagglingand tense hours.
on.wsj.com/h8N7n1This work presents a model that makes paraphrasechoices to minimize the character length of a sen-tence.
An oracle paraphrase-substitution experimentshows that human judges rate paraphrastic compres-sions higher than deletion-based compressions.
Toachieve further compression, we shortened the or-acle compressions using a deletion model to yieldcompressions 80% of the original sentence lengthand compared these to compressions generated us-ing just deletions.
Manual evaluation found thatthe oracle-then-deletion compressions to preservemore meaning than deletion-only compressions atuniform compression rates.2 Related workMost of the previous research on sentence compres-sion focuses on deletion using syntactic informa-tion, (e.g., Galley and McKeown (2007), Knightand Marcu (2002), Nomoto (2009), Galanis and An-droutsopoulos (2010), Filippova and Strube (2008),McDonald (2006), Yamangil and Shieber (2010),Cohn and Lapata (2008), Cohn and Lapata (2009),Turner and Charniak (2005)).
Woodsend et al(2010) incorporate paraphrase rules into a deletionmodel.
Previous work in subtitling has made one-word substitutions to decrease character length athigh compression rates (Glickman et al, 2006).More recent approaches in steganography have usedparaphrase substitution to encode information in textbut focus on grammaticality, not meaning preserva-tion (Chang and Clark, 2010).
Zhao et al (2009) ap-plied an adaptable paraphrasing pipeline to sentence2Taken from the main page of http://wsj.com, April 9, 2011.compression, optimizing for F-measure over a man-ually annotated set of gold standard paraphrases.Sentence compression has been considered be-fore in contexts outside of summarization, such asheadline, title, and subtitle generation (Dorr et al,2003; Vandeghinste and Pan, 2004; Marsi et al,2009).
Corston-Oliver (2001) deleted charactersfrom words to shorten the character length of sen-tences.
To our knowledge character-based compres-sion has not been examined before with the surgingpopularity and utility of Twitter.3 Sentence TighteningThe distinction between tightening and compressioncan be illustrated by considering how much spaceneeds to be preserved.
In the case of microblogging,often a sentence has just a few too many charactersand needs to be ?tightened?.
On the other hand, if asentence is much longer than a desired length, moredrastic compression is necessary.
The first subtaskis relevant in any context with strict word or charac-ter limits.
Some sentences may not be compressiblebeyond a certain limit.
For example, we found thatnear 10% of the compressions generated by Clarkeand Lapata (2008) were identical to the original sen-tence.
In situations where the sentence must meeta minimum length, tightening can be used to meetthese requirements.Multi-reference translations provide an instanceof the natural length variation of human-generatedsentences.
These translations represent differentways to express the foreign same sentence, so thereshould be no meaning lost between the different ref-erence translations.
The character-based length ofdifferent translations of a given sentence varies onaverage by 80% when compared to the shortest sen-tence in a set.3 This provides evidence that sen-tences can be tightened to some extent without los-ing any meaning.Through the lens of sentence tightening, we con-sider whether paraphrase substitutions alone canyield compressions competitive with a deletion atthe same length.
A character-based compressionrate is crucial in this framework, as two compres-3This value will vary by collection and with the number ofreferences: for example, the NIST05 Arabic reference set has amean compression rate of 0.92 with 4 references per set.85sions having the same character-based compres-sion rate may have different word-based compres-sion rates.
The advantage of a character-based sub-stitution model is in choosing shorter words whenpossible, freeing space for more content words.
Go-ing by word length alone would exclude the manyparaphrases with fewer characters than the originalphrase and the same number of words (or more).3.1 Paraphrase AcquisitionTo generate paraphrases for use in our experiments,we took the approach described by Bannard andCallison-Burch (2005), which extracts paraphrasesfrom bilingual parallel corpora.
Figure 1 illustratesthe process.
A phrase to be paraphrased, like throwninto jail, is found in a German-English parallel cor-pus.
The corresponding foreign phrase (festgenom-men) is identified using word alignment and phraseextraction techniques from phrase-based statisticalmachine translation (Koehn et al, 2003).
Other oc-currences of the foreign phrase in the parallel corpusmay align to another English phrase like jailed.
Fol-lowing Bannard and Callison-Burch, we treated anyEnglish phrases that share a common foreign phraseas potential paraphrases of each other.As the original phrase occurs several times andaligns with many different foreign phrases, each ofthese may align to a variety of other English para-phrases.
Thus, thrown into jail not only paraphrasesas jailed, but also as arrested, detained, impris-oned, incarcerated, locked up, taken into custody,and thrown into prison .
Moreover, because themethod relies on noisy and potentially inaccurateword alignments, it is prone to generate many badparaphrases, such as maltreated, thrown, cases, cus-tody, arrest, owners, and protection.To rank candidates, Bannard and Callison-Burchdefined the paraphrase probability p(e2|e1) basedon the translation model probabilities p(e|f) andp(f |e) from statistical machine translation.
Follow-ing Callison-Burch (2008), we refine selection by re-quiring both the original phrase and paraphrase tobe of the same syntactic type, which leads to moregrammatical paraphrases.Although many excellent paraphrases are ex-tracted from parallel corpora, many others are un-suitable and the translation score does not alwaysaccurately distinguish the two.
Therefore, we re-Paraphrase Monlingual Bilingualstudy in detail 1.00 0.70scrutinise 0.94 0.08consider 0.90 0.20keep 0.83 0.03learn 0.57 0.10study 0.42 0.07studied 0.28 0.01studying it in detail 0.16 0.05undertook 0.06 0.06Table 1: Candidate paraphrases for study in detail withcorresponding approximate cosine similarity (Monolin-gual) and translation model (Bilingual) scores.ranked our candidates based on monolingual distri-butional similarity, employing the method describedby Van Durme and Lall (2010) to derive approxi-mate cosine similarity scores over feature counts us-ing single token, independent left and right contexts.Features were computed from the web-scale n-gramcollection of Lin et al (2010).
As 5-grams are thehighest order of n-gram in this collection, the al-lowable set of paraphrases have at most four words(which allows at least one word of context).To our knowledge this is the first time such tech-niques have been used in combination in order toderive higher quality paraphrase candidates.
See Ta-ble 1 for an example.The monolingual-filtering technique we describeis by no means limited to paraphrases extracted frombilingual corpora.
It could be applied to other data-driven paraphrasing techniques (see Madnani andDorr (2010) for a survey).
Although it is particularlywell suited to the bilingual extracted corpora, sincethe information that it adds is orthogonal to thatmodel, it would presumably add less to paraphras-ing techniques that already take advantage of mono-lingual distributional similarity (Pereira et al, 1993;Lin and Pantel, 2001; Barzilay and Lee, 2003).In order to evaluate the paraphrase candidatesand scoring techniques, we randomly selected 1,000paraphrase sets where the source phrase was presentin the corpus described in Clarke and Lapata (2008).For each phrase and set of candidate paraphrases, weextracted all of the contexts from the corpus in whichthe source phrase appeared.
Human judges werepresented each sentence with the original phrase andthe same sentences with each paraphrase candidate86... letzteWoche wurden in Irland f?nf Landwirte festgenommen , weil sie verhindern wollten... last week five farmers were thrown into jail in Ireland because they resisted ......Zahlreiche Journalisten sind verschwunden oder wurden festgenommen , gefoltert und get?tet .Quite a few journalists have disappeared or have been imprisoned , tortured and killed .Figure 1: Using a bilingual parallel corpus to extract paraphrases.substituted in.
Each paraphrase substitution wasgraded based on the extent to which it preservedthe meaning and affected the grammaticality of thesentence.
While both the bilingual translation scoreand monolingual cosine similarity positively corre-lated with human judgments, the monolingual scoreproved a stronger predictor of quality in both dimen-sions.
Using Kendall?s tau correlation coefficient,the agreement between the ranking imposed by themonolingual score and human ratings surpassed thatof the original ranking as derived during the bilin-gual extraction, for both meaning and grammar.4 Inour substitution framework, we ignore the transla-tion probabilities and use only the approximate co-sine similarity in the paraphrase decision task.4 Framework for Sentence TighteningOur sentence tightening approach uses a dynamicprogramming strategy to find the combination ofnon-overlapping paraphrases that minimizes a sen-tence?s character length.
The threshold of the mono-lingual score for paraphrases can be varied to widenor narrow the search space, which may be further in-creased by considering any lexical paraphrases notsubject to syntactic constraints.
Sentences with acompression rate as low as 0.6 can be generatedwithout thresholding the paraphrase scores.
Becausethe system can generate multiple paraphrased sen-tences of equal length, we apply two layers of filter-ing to generate a single output.
First we calculate aword-overlap score between the original and candi-date sentences to favor compressions similar to theoriginal sentence; then, from among the sentences4For meaning and grammar respectively, ?
= 0.28 and 0.31for monolingual scores and 0.19 and 0.15 for bilingual scores.with the highest word overlap, we select the com-pression with the best language model score.Higher paraphrase thresholds guarantee more ap-propriate paraphrases but yield longer compressions.Using a cosine-similarity threshold of 0.95, the av-erage compression rate is 0.968, which is consider-ably longer than the compressions using no thresh-old (0.60).
In these experiments we did not syntac-tically constrain paraphrases.
However, we believethat our monolingual refining of paraphrase sets im-proves paraphrase selection and is a reasonable al-ternative to using syntactic constraints.In case judges favor compressions that have highword overlap with the original sentence, we com-pressed the longest sentence from each set of ref-erence translations (Huang et al, 2002) and ran-domly chose a sentence from the set of referencetranslations to use as the standard for comparison.Paraphrastic compressions were generated at cosine-similarity thresholds ranging from 0.60 to 0.95.We implemented a state-of-the-art deletion model(Clarke and Lapata, 2008) to generate deletion-onlycompressions.
We fixed the compression lengthto ?5 characters of the length of each paraphras-tic compression, in order to isolate the compressionquality from the effect of compression rate (Napoleset al, 2011).
Manual evaluation used Amazon?sMechanical Turk with three-way redundancy andpositive and negative controls to filter bad workers.Meaning and grammar judgments were collected us-ing two 5-point scales (5 being the highest score).5 EvaluationThe initial results of our substitution system showroom for improvement in future work (Table 2).
Webelieve this is due to erroneous paraphrase substi-87System Grammar Meaning CompR Cos.Substitution 3.8 3.7 0.97 0.95Deletion 4.1 4.0 0.97 -Substitution 3.4 3.2 0.89 0.85Deletion 4.0 3.8 0.89 -Substitution 3.1 3.0 0.85 0.75Deletion 3.9 3.7 0.85 -Substitution 2.9 2.9 0.82 0.65Deletion 3.8 3.5 0.82 -Table 2: Mean ratings of compressions using just deletionor substitution at different paraphrase thresholds (Cos.).Deletion performed better in all settings.tutions, since phrases with the same syntactic cate-gory and distributional similarity are not necessarilysemantically identical.
Illustrative examples includeWTO for United Nations and east or west for south.Because the quality of the multi-reference transla-tions is not uniformly high, for the following exper-iment we used a dataset of English newspaper arti-cles.To control against these errors and test the viabil-ity of a substitution-only approach, we generated allpossible paraphrase substitutions above a thresholdof 0.80 within a set of 20 randomly chosen sentencesfrom the written corpus of Clarke and Lapata (2008).We solicited humans to make a ternary decision ofwhether a paraphrase was acceptable in the context(good, bad, or not sure).
We applied our model togenerate compressions using only paraphrase substi-tutions on which all three annotators agreed that theparaphrase was good.
The oracle generated com-pressions with an average compression rate of 0.90.On the same set of original sentences, we usedthe deletion model to generate compressions con-strained to ?5 characters of the length of the ora-cle compression.
Next, we examined whether apply-ing the deletion model to paraphrastic compressionswould improve compression quality.
In manual eval-uation along the dimensions of grammar and mean-ing, both the oracle compressions and oracle-plus-deletion compressions outperformed the deletion-only compressions at uniform lengths (Table 3)5.These results suggest that improvements in para-phrase acquisition will make our system competitivewith deletion-only models.5Paraphrastic compressions were rated significantly higherfor meaning, p < 0.05Model Grammar Meaning CompROracle 4.1 4.3 0.90Deletion 4.0 4.1 0.90Gold 4.3 3.8 0.75Oracle+deletion 3.4 3.7 0.80Deletion 3.2 3.4 0.80Table 3: Mean ratings of compressions generated by asubstitution oracle, deletion only, deletion on the oraclecompression, and the gold standard.
Being able to choosethe best paraphrases would enable our substitution modelto outperform the deletion model.6 ConclusionThis work shows promise for the use of only sub-stitution in the task of sentence tightening.
Thereare myriad possible extensions and improvementsto this method, most notably richer features be-yond paraphrase length.
We do not currently usesyntactic information in our paraphrastic compres-sion model because it places limits on the numberof paraphrases available for a sentence and therebylimits the possible compression rate.
The currentmethod for paraphrase extraction does not includecertain types of rewriting, such as passivization, andshould be extended to incorporate even more short-ening paraphrases.
Future work can directly applythese methods to Twitter and extract additional para-phrases and abbreviations from Twitter and/or SMSdata.
Our substitution approach can be improved byapplying more sophisticated techniques to choosingthe best candidate compression, or by framing it asan optimization problem over more than just mini-mal length.
Overall, we find these results to be en-couraging for the possibility of sentence compres-sion without deletion.AcknowledgmentsWe are grateful to John Carroll for helping us obtainthe RASP parser.
This research was partially fundedby the JHU Human Language Technology Center ofExcellence.
This research was funded in part by theNSF under grant IIS-0713448.
The views and find-ings are the authors?
alone.ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of ACL.88Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings of HLT/NAACL.Chris Callison-Burch.
2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of EMNLP.Ching-Yun Chang and Stephen Clark.
2010.
Linguis-tic steganography using automatically generated para-phrases.
In Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages591?599.
Association for Computational Linguistics.James Clarke and Mirella Lapata.
2008.
Global infer-ence for sentence compression: An integer linear pro-gramming approach.
Journal of Artificial IntelligenceResearch, 31:399?429.Trevor Cohn and Mirella Lapata.
2008.
Sentence com-pression beyond word deletion.
In Proceedings ofCOLING.Trevor Cohn and Mirella Lapata.
2009.
Sentence com-pression as tree transduction.
Journal of Artificial In-telligence Research, 34:637?674.Simon Corston-Oliver.
2001.
Text compaction for dis-play on very small screens.
In Proceedings of theNAACL Workshop on Automatic Summarization.Bonnie Dorr, David Zajic, and Richard Schwartz.
2003.Hedge trimmer: A parse-and-trim approach to head-line generation.
In Proceedings of the HLT-NAACLWorkshop on Text summarization Workshop.Katja Filippova and Michael Strube.
2008.
Dependencytree based sentence compression.
In Proceedings ofthe Fifth International Natural Language GenerationConference.
Association for Computational Linguis-tics.Dimitrios Galanis and Ion Androutsopoulos.
2010.
Anextractive supervised two-stage method for sentencecompression.
In Proceedings of NAACL.Michel Galley and Kathleen R. McKeown.
2007.
Lex-icalized Markov grammars for sentence compression.the Proceedings of NAACL/HLT.Oren Glickman, Ido Dagan, Mikaela Keller, Samy Ben-gio, and Walter Daelemans.
2006.
Investigating lexi-cal substitution scoring for subtitle generation.
In Pro-ceedings of the Tenth Conference on ComputationalNatural Language Learning, pages 45?52.
Associa-tion for Computational Linguistics.Shudong Huang, David Graff, and George Doddington.2002.
Multiple-Translation Chinese Corpus.
Linguis-tic Data Consortium.Kevin Knight and Daniel Marcu.
2000.
Statistics-basedsummarization ?
Step one: Sentence compression.
InProceedings of AAAI.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: A probabilistic ap-proach to sentence compression.
Artificial Intelli-gence, 139:91?107.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT/NAACL.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules from text.
Natural Language Engineering,7(3):343?360.Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,David Yarowsky, Shane Bergsma, Kailash Patil, EmilyPitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,and Sushant Narsale.
2010.
New Tools for Web-ScaleN-grams.
In Proceedings of LREC.Nitin Madnani and Bonnie Dorr.
2010.
Generat-ing phrasal and sentential paraphrases: A surveyof data-driven methods.
Computational Linguistics,36(3):341?388.Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and WalterDaelemans.
2009.
Is sentence compression an NLGtask?
In Proceedings of the 12th European Workshopon Natural Language Generation.Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and WalterDaelemans.
2010.
On the limits of sentence com-pression by deletion.
Empirical Methods in NaturalLanguage Generation, pages 45?66.Ryan McDonald.
2006.
Discriminative sentence com-pression with soft syntactic constraints.
In In Proceed-ings of EACL.Courtney Napoles, Benjamin Van Durme, and ChrisCallison-Burch.
2011.
Evaluating sentence compres-sion: Pitfalls and suggested remedies.
In Proceedingsof ACL, Workshop on Monolingual Text-To-Text Gen-eration.Tadashi Nomoto.
2009.
A comparison of model free ver-sus model intensive approaches to sentence compres-sion.
In Proceedings of EMNLP.Paul Over and James Yen.
2004.
An introduction toDUC 2004: Intrinsic evaluation of generic news textsummarization systems.
In Proceedings of DUC 2004Document Understanding Workshop, Boston.Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993.Distributional clustering of English words.
In ACL-93.Jenine Turner and Eugene Charniak.
2005.
Supervisedand unsupervised learning for sentence compression.In Proceedings of ACL.Benjamin Van Durme and Ashwin Lall.
2010.
Onlinegeneration of locality sensitive hash signatures.
InProceedings of ACL, Short Papers.Vincent Vandeghinste and Yi Pan.
2004.
Sentence com-pression for automated subtitling: A hybrid approach.In Proceedings of the ACL workshop on Text Summa-rization.89Kristian Woodsend, Yansong Feng, and Mirella Lapata.2010.
Generation with quasi-synchronous grammar.In Proceedings of EMNLP.Elif Yamangil and Stuart M. Shieber.
2010.
Bayesiansynchronous tree-substitution grammar induction andits application to sentence compression.
In Proceed-ings of ACL.Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li.
2009.Application-driven statistical paraphrase generation.90
