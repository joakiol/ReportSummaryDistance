Discovering Synonyms and Other Related WordsKrister LINDE?N and Jussi PIITULAINENHelsinki University, Department of General Linguistics,P.O.Box 9 (Siltavuorenpenger 20 A),FIN-00014 University of Helsinki,Finland,Krister.Linden@helsinki.fi, Jussi.Piitulainen@helsinki.fiAbstractDiscovering synonyms and other related wordsamong the words in a document collection canbe seen as a clustering problem, where we ex-pect the words in a cluster to be closely relatedto one another.
The intuition is that words oc-curring in similar contexts tend to convey simi-lar meaning.We introduce a way to use translation dictio-naries for several languages to evaluate the rateof synonymy found in the word clusters.
We alsoapply the information radius to calculating sim-ilarities between words using a full dependencysyntactic feature space, and introduce a methodfor similarity recalculation during clustering asa fast approximation of the high-dimensionalfeature space.
Finally, we show that 69-79% ofthe words in the clusters we discover are usefulfor thesaurus construction.1 IntroductionFinding related words among the words in adocument collection can be seen as a clusteringproblem, where we expect the words in a clusterto be closely related to the same sense or to bedistributional substitutes or proxies for one an-other.
A number of language-technology taskscan benefit from such word clusters, e.g.
docu-ment classification applications, language mod-elling, resolving prepositional phrase attach-ment, conjunction scope identification, wordsense disambiguation, word sense separation,automatic thesaurus generation, information re-trieval, anaphor resolution, text simplification,topic identification, spelling correction (Weeds,2003).At present, synonyms and other relatedwords are available in manually constructedontologies, such as synonym dictionaries, the-sauri, translation dictionaries and terminolo-gies.
Manually constructing ontologies is time-consuming even for a single domain.
On theworld-wide web there are documents on manytopics in different languages that could bene-fit from having an ontology.
For many of themsome degree of automation is eventually needed.Humans often infer the meaning of an un-known word from its context.
Lets look at a lesswell-known word like blopping.
We look it up onthe Web.
Some of the hits are: Blopping throughsome of my faves, i.e.
leafing through favouriteweb links, A blop module emits strange elec-tronic blopping noises, i.e.
an electronic sound,The volcano looked like something off the coverof a Tolkien novel - perfectly conical, billowingsmoke and blopping out chunks of bright orangelava, i.e.
spluttering liquid.
At first we findall of them different and perhaps equally im-portant.
When looking at further links, we getan intuition that the first instance is perhapsa spurious creative metonym, whereas the twoothers can be regarded as more or less conven-tional and represent two distinct senses of blop-ping.
However, the meaning of all three seemsto be related to a sound, which is either clickingor spluttering in nature.The intuition is that words occurring in thesame or similar contexts tend to convey similarmeaning.
This is known as the DistributionalHypothesis (Harris, 1968).
There are many ap-proaches to computing semantic similarity be-tween words based on their distribution in a cor-pus.
For a general overview of similarity mea-sures, see (Manning and Schu?tze, 1999), and forsome recent and extensive overviews and evalu-ations of similarity measures for i.a.
automaticthesaurus construction, see (Weeds, 2003; Cur-ran, 2003; Lee, 2001; Dagan et al, 1999).
Theyshow that the information radius and the ?-skew distance are among the best for findingdistributional proxies for words.If we assume that a word w is represented as asum of its contexts and that we can calculate thesimilarities between such word representations,we get a list Lw of words with quantificationsof how similar they are to w. Each similarityCompuTerm 2004  -  3rd International Workshop on Computational Terminology 63list Lw contains a mix of words related to thesenses of the word w.If we wish to identify groups of synonymsand other related words in a list of similarity-rated words, we need to find clusters of simi-lar words that are more similar to one anotherthan they are to other words.
For a review ofgeneral clustering algorithms, see (Jain et al,1999) and for a recent evaluation of clusteringalgorithms for finding word categories, see (Pan-tel, 2003).
(Pantel, 2003) shows that among thestandard algorithms the average-link and the k-means clustering perform the best when tryingto discover meaningful word groups.In order to evaluate the quality of the discov-ered clusters three methods can be used, i.e.measuring the internal coherence of clusters,embedding the clusters in an application, orevaluating against a manually generated answerkey.
The first method is generally used by theclustering algorithms themselves.
The secondmethod is especially relevant for applicationsthat can deal with noisy clusters and avoidsthe need to generate answer keys specific to theword clustering task.
The third method requiresa gold standard such as WordNet or some otherontological resource.
For an overview of eval-uation methodologies for word clustering, see(Weeds, 2003; Curran, 2003; Pantel, 2003).The contribution of this article is four-fold.The first contribution is to apply the informa-tion radius in a full dependency syntactic fea-ture space when calculating the similarities be-tween words.
Previously, only a restricted setof dependency relations has been applied.
Thesecond contribution is a similarity recalcula-tion during clustering, which we introduce as afast approximation of high-dimensional featurespace and study its effect on some standard clus-tering algorithms.
The third contribution is asimple but efficient way to evaluate the synonymcontent of clusters by using translation dictio-naries for several languages.
Finally we showthat 69-79% of the words in the discovered clus-ters are useful for thesaurus construction.The rest of this article is organized as fol-lows.
Section 2 presents the corpus data andthe the feature extraction.
Section 3 introducesthe discovery methodology.
Section 4 presentsthe evaluation methodology.
In Section 5 wepresent the experiments and evaluate the resultsand their significance.
Sections 6 and 7 containthe discussion and conclusion, respectively.2 Corpus DataOur corpus consists of nouns in a sentence con-text.
We used all the nouns (in base form) thatoccurred more than 100 times (in any inflectedform) in a corpus of Finnish newspaper text.The corpus contained 245 000 documents total-ing 48 million words of the Finnish newspaperHelsingin sanomat from 1995?1997.
ExcludingTV and radio listings, there were 196 000 doc-uments with 42 million words.
As corpus datawe selected all the 17 835 nouns occurring morethan 100 times comprising 14 million words ofthe corpus.3 MethodologyFirst we present the types of features we haveextracted from the corpus.
Then we briefly de-scribe the similarity measure which we use inorder to calculate the similarity between thenouns in the corpus data.
We also introducea method for creating derived similarity infor-mation in a low-dimensional space.
Finally wepresent the clustering algorithms which we ap-ply to the similarity information.3.1 Feature extractionThe present experiments aim at discovering thenouns that are most similar in meaning to agiven noun.
The assumption is that words oc-curring in similar syntactic contexts belong tothe same semantic categories (Harris, 1968).
Inorder to determine the similarity of the syntac-tic contexts, we represent a word w as a proba-bility distribution over a set of features a occur-ring in the context of w: P (a|w).
The contextfeatures a are the major class words w?
(nouns,adjectives and verbs) with direct dependencylinks to the word w. The context feature is theword w?
in base form labeled with the depen-dency relation r. For example, the noun mightoccur as an object of a verb and with an adjec-tive modifier; both the verb and the adjectiveincluding their dependency relations are contextfeatures.We used Connexor?s dependency parser FDGfor Finnish (Connexor, 2002) for parsing thecorpus.
A sample of the parser output is shownin Table 1.
Tokens of each sentence are num-bered starting from zero, each token is on itsown line, the token number first, the actualword form second and the base form in the thirdfield.
The fourth field links dependent tokens totheir heads using a grammatical label and theCompuTerm 2004  -  3rd International Workshop on Computational Terminology64# Token Base form Dependency Morphosyntax Gloss01 Toisessa toinen &NH PRON SG INE In the other2 esiteta?a?n esitta?a?
main:>0 &+MV V PASS IND PRES they showed3 videoita video obj:>2 &NH N PL PTV videos4 ja ja cc:>3 &CC CC and5 filmeja?
filmi cc:>3 &NH N PL PTV films6 .
.7 <s> <s> >6Table 1: Sample output from the FDG parser for Finnish (with an English gloss added).number of the head token.
The fifth field con-tains morphosyntactic information.Two tokens, 3 and 5, are labeled as nouns N.The noun video is a direct object to the verbesitta?a?, and the noun filmi is coordinatedwith video, so video gets two feature occur-rences from this sentence:esitta?a?-objcc-filmi.Also, filmi getsvideo-ccas a feature occurrence.
The pronoun toinen isnot a potential feature because of its word classand because it is not linked.
The coordinatingconjunction ja is not a potential feature becauseof its word class.The parsed corpus contained a to-tal of 18 516 609 unambiguous noun oc-currences, 69 314 noun/verb ambigui-ties, 39 104 noun/adjective ambiguities,20 847 noun/adverb ambiguities and 11 739noun/numeral ambiguities, i.e.
the amountof remaining ambiguities was less than 0.8%.When its analyses were underspecified withmore than one morphological analysis re-maining, we took the relatively small risk(p < 0.008) of committing to a noun analysis.As a straightforward weighting of the contextfeatures of a word, we used the number of occur-rences with all the instances of the word.
In ourchoice of similarity formula, the representationof a word w must be a probability distribution.This is formally just a matter of normalizing theweights of the features.
Thus, a word w is rep-resented as w : a 7?
P (a|w), i.e.
the conditionalprobability distribution of all features a giventhe word w, such that?a P (a|w) = 1.Extracting features only from direct depen-dency relations produces few feature occur-rences for each instance of a noun.
This keepsthe number of distinct features tolerable for allbut the most frequent words, and still retainsthe most promising co-occurring words.
As weuse only linear frequency weighting, very fre-quent features tend to get more weight thanthey should.
Additionally, many rare featurescould have been dropped without much loss ofinformation.3.2 Similarity calculationsIn (Weeds, 2003; Lee, 2001), i.a.
the informa-tion radius is applied to finding words that canbe used as proxies or substitutes for one an-other.
Their tests show that the informationradius is among the best for finding such words.Here we briefly recapitulate the details of thesimilarity estimate, which is rather an estimateof dissimilarity.Two words are distributionally similar to theextent that they co-occur with the same words,i.e., to the extent that they share features.
Wedefine the dissimilarity of two words, p and q,asJ(p, q) = (D(p?m) +D(q?m))/2, (1)where D(p?m) =?a p(a)(log2 p(a)?
log2 m(a))and m(a) = (p(a) + q(a))/2 for any feature a.This is the symmetrically weighted case of theJensen?Shannon divergence (Lin, 1991), alsoknown as the information radius or the mean di-vergence to the mean (Dagan et al, 1999).
Forcomplete identity, J(p, p) = 0.
For completelydisjoint feature sets, J(p, q) = 1.
The formula issymmetric but does not satisfy the triangle in-equality.
For speed the estimate may be calcu-lated from the shared features alone (Lee, 1999).After calculating all the pairwise estimates,we retained lists of the 100 most similar nounsfor each of the nouns in the corpus data.
Noother data is used in the similarity calculations.CompuTerm 2004  -  3rd International Workshop on Computational Terminology 653.3 Low-dimensional similaritymeasuresPerforming all the calculations in high-dimensional feature space is time-consuming.Here we introduce a method that can be usedas an approximation in low-dimensional featurespace based on the initial similarity estimates.Assume that we have lists of the words thatare distributionally most similar to a given wordw.
Each list Lw contains 100 words with anestimate of their similarity to w. The words inLw represent a mix of the different meanings ofthe word w. We create a similarity matrix diswfor these words such that disw(p, q) = J(p, q),where p, q ?
Lw.
The similarity matrix disw is asymmetric matrix of the dimensions 101 by 101,as we also include the word w in the matrix.A vector pw = disw(p, .)
in the similaritymatrix disw is regarded as a projection of theword p from a high dimensional feature spaceonto a 101-dimensional space, i.e.
p is projectedonto the 101 most important dimensions of w.The new matrix is not orthogonal, so we ap-ply single-value decomposition (SVD) disw =T S D and use T to rotate the matrix so that thefirst axis runs along the direction of the largestvariation among the word similarity estimates,the second dimension runs along the directionof the second largest variation and so forth.
Af-ter this rotation we can cluster the new vec-tors pw,T = T t pw as low-dimensional represen-tatives of the original high-dimensional featurespace.
Often SVD is used for dimensionality re-duction, but here we use its left singular vectorsonly for rotating the matrix in order to achievenoise reduction during clustering.In the new low-dimensional vector repre-sentation pw,T we apply the cosine distancecosd(pw,T , qw,T ) = 1 ?
cos(pw,T , qw,T ) in orderto calculate the similarity between words.
As acomparison we also tried the squared Euclideandistance eucld(pw,T , qw,T ) = ?pw,T ?
qw,T ?2 be-tween words in the low-dimensional space.
Wefirst normalize the vectors to unit length, whicheffectively makes the squared Euclidean dis-tance the same as two times the cosine distance:?A?B?2 = ?A?2+?B?2?2?A?
?B?
cos(A,B),and when ?A?
= 1 and ?B?
= 1, we have?A?B?2 = 2 (1?
cos(A,B)).3.4 ClusteringWhen we wish to discover the potential sensesof w by clustering, we are currently only inter-ested in the 100 words in Lw with a similarityestimate for w. The other words are deemed tobe too dissimilar to w to be relevant.We cluster the words related to w withstandard algorithms such as complete-link andaverage-link clustering (Manning and Schu?tze,1999).
Complete-link and average-link are hier-archical clustering methods.
We compare themwith flat clustering methods like k-means andself-organizing maps (SOM) (Kohonen, 1997).In k-means the clusters have no ordering.
Thepotential benefit of using SOM with a two-dimensional display compared to k-means isthat related data samples get assigned intonearby clusters as the SOM converges formingcluster areas with related content.We use the MATLAB implementation (TheMathWorks, Inc., 2002) of the algorithms.
Weuse both the original similarity measures indisw and the distance measures cosd and eucld,which we defined on the low-dimensional space.In order to use methods like k-means and SOM,we need to be able to calculate the similar-ity between cluster centroids and words to beclustered each time a centroid is updated.
Wedo this in the low-dimensional space pw,T usingcosd and eucld.For SOM, the MATLAB implementation sup-ported only the squared Euclidean distance.
Itshould be noted that the centroids are not nec-essarily of unit length, so the squared Euclideandistance is different from the cosine distance be-tween the samples and the centroids, when thecentroids are based on more than one sample.Our clustering setup currently produces hardclusters, where each word w in Lw belong toone cluster, as opposed to soft clustering, wherea word may belong to several clusters.
We callthe cluster containing the word w itself the keycluster.4 Evaluation methodologyIn order to evaluate the quality of the clusterswe need a gold standard.
English and a num-ber of other languages have resources such asWordNet (Fellbaum, 1998; Vossen, 2001).
ForFinnish there is no WordNet and there are nolarge on-line synonym dictionaries available.
Infact, our experiment can be seen as a feasibilitystudy for automatically extracting informationthat could be used for building a WordNet forFinnish.
The synsets of WordNet contain syn-onyms, so we can evaluate the feasibility of theclusters for WordNet development by rating theamount of synonyms and related words in theCompuTerm 2004  -  3rd International Workshop on Computational Terminology66Language Target word Back translationEnglish deficit vaje, vajaus, alija?a?ma?
; tilivajausshortfall vaje, alija?a?ma?German Defizit vajaus, vaje, alija?a?ma?
; kassavajaus, tappio; tilivajaus; puutos, puuteUnterbilanz alija?a?ma?, vajaus, vaje, kauppavajeFehlbetrag vajaus, alija?a?ma?, tappio, virhemaksuFrench de?ficit alija?a?ma?, miinus, tilivajaus; vajaus, vaje; tappioTable 2: Translations of the Finnish source word alija?a?ma?
into English, German and French withthe back translations into Finnish.
The shared back translations vaje, vajaus, alija?a?ma?, tilivajausare highlighted.discovered clusters.We note that when translating a word fromthe source language the meaning of the word isrendered in a target language.
Such meaningpreserving relations are available in translationdictionaries.
If we translate into the target lan-guage and back we end up i.a.
with the syn-onyms of the original source language word.
Inaddition, we may also get some spurious wordsthat are related to other meanings of the targetlanguage words.
If we assume that the otherwords represent spurious cases of polysemy orhomonymy in the target language, we can re-duce the impact of these spurious words by con-sidering several target languages and for eachsource word we use only the back-translatedsource words that are common to all the tar-get languages.
We call such a group of words asource word synonym set.
For an example, seeTable 2.In addition to the mechanical rating of thesynonym content we also manually classified thewords of some cluster samples into synonymy,antonymy, hyperonymy, hyponymy, comple-mentarity and other relations.4.1 Evaluation dataIn order to evaluate the clusters we picked arandom sample of 1759 nouns from the corpusdata, which represented approximately 10% ofthe words we had clustered.
For these wordswe extracted the translations in the Finnish-English, Finnish-German and Finnish-FrenchMOT dictionaries (Kielikone, 2004) available inelectronic form.
We then translated each tar-get language word back into Finnish using thesame resources.
The dictionaries are based onextensive hand-made dictionaries.
The choice ofwords may be slightly different in each of them,which means that the words in common for allthe dictionaries after the back translation tendto be only the core synonyms.For evaluation purposes it would be unfair todemand that the clustering generate words intothe clusters that are not in the corpus data, sowe also removed those back translations fromthe source word synonym sets.
Finally, onlysynonym sets that had more than one word re-maining were interesting, i.e.
they containedmore than the original source word.
There were453 of the 1759 test words that met the quali-fications.
The average number of synonyms orback translations for these test words was 3.53including the source word itself.For manual classification we used a sample of50 key clusters from the whole set of clusters andan additional sample of 50 key clusters from thewords qualifying for the mechanical evaluation.4.2 Evaluation methodThe mechanical evaluation was performed bypicking the key cluster produced by a cluster-ing algorithm for each of the test words.
Thekey cluster was the cluster which contained theoriginal source word.
The evaluation was a sim-ple overlap calculation with the gold standardgenerated from the translation dictionaries.
Bycounting the number of cluster words in a sourceword synonym set and dividing by the synonymset size, we get the recall R. By counting thenumber of of source word synonyms in a clus-ter and dividing by the cluster size, we get theprecision P .The manual evaluation was performed inde-pendently by the two authors and an externallinguist.
We then discussed the result in orderto arrive at a common view.5 TestingFirst we did some initial experimenting with apreliminary test sample in order to tune the pa-rameters.
We then clustered the corpus dataand evaluated the clusters against the gold stan-dard, which gave an estimate of the synonymcontent of the clusters.
In addition, we per-formed a manual evaluation of the result of theCompuTerm 2004  -  3rd International Workshop on Computational Terminology 67Clustering method Information radius Cosine distance Euclidean distanceR P R P R PAverage link 47 42 43 41 43 41Complete link 47 40 42 39 42 38K-means - - 43 36 42 36SOM - - - - 41 35Table 3: Cluster synonym content as average recall (R) and precision (P) in per cent (%) with astandard deviation of 2% using different clustering methods and similarity measures.Clustering method Cosine distance Cosine distancew rotated feature space w/o rotationR P R PAverage link 43 41 42 32Complete link 42 39 41 33Table 4: Cluster synonym content as average recall (R) and precision (P) in per cent (%) with astandard deviation of 2% using a denoised and a noisy low-dimensional feature space.best clustering algorithm.5.1 Parameter selectionWe clustered the words in Lw with thecomplete-link and average-link clustering algo-rithms using the disw similarity information.The algorithms form hierarchical cluster treeswhich need to be split into clusters at somelevel.
The inconsistency coefficient c character-izes each link in a cluster tree by comparing itslength with the average size of other links atthe same level of the hierarchy.
The higher thevalue of this coefficient, the less similar the ob-jects connected by the link (The MathWorks,Inc., 2002).
We selected the inconsistency coef-ficient c = 1 by testing on a separate initial testset different from the final evaluation data.Using the cosine distance cosd(pw,T , qw,T )as a similarity measure on the projected androtated representation of the words we clus-tered with the above mentioned standard clus-tering algorithms as well as with the k-means algorithm.
Using the euclidean dis-tance eucld(pw,T , qw,T ) we also produced self-organizing maps (SOM).
For k-means and SOMan initial number of clusters need to be selected.We selected 35 clusters as this was close to theaverage of what the other algorithms produced,which we were comparing with.
For k-meanswe used the best out of 10 iterations and forSOM we trained a 5 ?
7 hexagonal gridtop for10 epochs.
We also tried a considerably longertraining period for SOM but noticed only aninsignificant improvement on the cluster preci-sion.We also tried a number of other algorithms inthe MATLAB package, but they typically pro-duced a result either containing only the worditself or clusters containing more than one fifthof the words in the key cluster.
We deemed suchclustering results a failure on our data withoutneed for formal evaluation.5.2 ExperimentsAfter evaluating against the translation dictio-nary gold standard, the result of the experimentwith complete-link, average-link, k-means andSOM clustering using different similarity mea-sures is shown in Table 3.
The best recall withthe best precision was achieved with the average-link clustering using the information radius onthe original feature space with 47 ?
2% recalland and 42?2% precision.
This produced clus-ters with an average size of 6.05 words.The difference between complete-link andaverage-link clustering is not statistically sig-nificant even if the average-link is slightly bet-ter.
The recall is statistically significantly betterin the original feature space than in the low-dimensional space at the risk level p = 0.05,whereas the precision remains roughly the same.The average-link and complete-link clusteringhave a statistically significantly better precisionthan k-means and SOM, respectively, at the risklevel p < 0.05.
We can also see that there ishardly any difference in practice between theEuclidean distance on normalized word vectorsand the cosine distance despite the fact that thecentroids were not normalized when using thesquared Euclidean distance with k-means.As can be seen from Table 4 the rotation ofthe low-dimensional feature space using SVDhas the effect of increasing precision statisticallysignificantly at the risk level p < 0.005, i.e.
theCompuTerm 2004  -  3rd International Workshop on Computational Terminology68Word alija?a?ma?/deficit maatalous/agriculture tuki/aidSynonymy vaje/deficiency avustus/subsidyvajaus/shortfall apu/helpAntonymy ylija?a?ma?/surplusComplementarity teollisuus/industryvientiteollisuus/export industryelintarviketeollisuus/food industryHyperonymy elinkeinoela?ma?/businesstalousela?ma?/economyHyponymy rahoitus/financingTable 5: Semantic relations of the cluster content of some sample words (English glosses added)Content Dictionary All wordsRelations sample sampleSynonymy 52% 38%Antonymy 1% 1%Complementarity 12% 34%Hyperonymy 2% 4%Hyponymy 1% 3%Other 31% 21%Total 100% 100%Table 6: Manual evaluation of the percentageof different semantic relations in the cluster con-tent in two different samples of 50 clusters each.clusters become less noisy.We then performed a manual evaluation ofthe output of the best clustering algorithm.
Weused one cluster sample from the 453 clustersqualifying for mechanical evaluation and onesample from the whole set of 1753 clusters.
Theresults of the manual evaluation is shown in Ta-ble 6.
The evaluation shows that 69-79% of thematerial in the clusters is relevant for construct-ing a thesaurus.The manual evaluation agrees with the me-chanical evaluation, when the manual evalua-tion found a synonym content of 52%, com-pared to the minimum synonym content of42% found by the mechanical evaluation.
Thismeans that the clusters actually contain afew more synonyms than those conservativelyagreed on by the three translation dictionaries.If we evaluate the sample of key clustersdrawn from all the words in the test sample, weget a synonym content of 38%.
This figure israther low, but can be explained by the fact thatmany of the words were compound nouns thathad no synonyms, which is why the translationdictionaries either did not have them listed orcontained no additional source word synonymsfor them.In Table 5, we see a few sample clusters whosewords we rated during manual evaluation.6 DiscussionThe feature selection and the feature weightingradically influences the outcome of the resultsof any machine learning task.
This has beennoted in several evaluations of supervised ma-chine learning algorithms (Voorhees et al, 1995;Yarowsky and Florian, 2002; Linde?n, 2003).During clustering, i.e.
unsupervised learning,the features extracted from the corpus are theonly information guiding the machine learningin addition to the clustering principle, whichmakes successful feature extraction, good fea-ture weighting and accurate similarity measure-ments crucial for the success of the clustering.The clustering algorithms only exploit and pre-serve the information provided by the featuresand the similarity measure.In (Weeds, 2003; Lee, 2001; Dagan et al,1999), the information radius is applied to findwords that can be used as distributional prox-ies for one another.
They extract features onlyfrom verb relations whereas we use the full rangeof dependency syntactic relations.
One inten-tion of this study was to evaluate whether theselected corpus and the features extracted pro-vide a basis for forming linguistically meaning-ful clusters that are useful in thesaurus con-struction.
The result showed that 69-79% ofthe words found in the key clusters are useful,which is very encouraging.
It turned out thatthe chosen features as such were useful, even ifthe over-all result probably could benefit froma more nuanced feature weighting scheme.
Wedo not yet fully understand how the initial fea-ture weighting affects the outcome of the clus-tering.
Perhaps there are features that wouldcontribute to a more fine-grained clustering ifproperly weighted.Next we intend to identify more than a sin-gle key cluster for each word, which poses addi-CompuTerm 2004  -  3rd International Workshop on Computational Terminology 69tional challenges for the evaluation.
We also aimat evaluating the generated clusters in an infor-mation retrieval setting in order to see if theyimprove performance despite the fact that theycontain more than synonyms.
This would alsoshed some light on exactly how much synonymywe need to aim at in a practical application.7 ConclusionWe have demonstrated that it is feasible to cal-culate similarities between words using a fulldependency syntactic feature space.
We havealso introduced similarity recalculation duringclustering as a fast approximation of the high-dimensional feature space.
In addition we intro-duced a way to use translation dictionaries forevaluating the rate of synonymy found in theword clusters, which is useful for languages thatdo not yet have publicly available thesaurus re-sources like WordNet.
Finally we have shownthat 69-79% of the words in the discovered clus-ters are useful for thesaurus construction.AcknowledgementsThe second author made the Jensen-Shannonsimilarity lists and the corpus processing de-scribed in Sections 2, 3.1 and 3.2, and thefirst author did the rest.
We are gratefulto Lauri Carlson, Kimmo Koskenniemi andMathias Creutz for helpful comments on themanuscript and to Juhani Jokinen for manuallyevaluating the test samples.ReferencesConnexor.
2002.
Machinese phrase tagger.
[http://www.connexor.com/].James Richard Curran.
2003.
From Distribu-tional to Semantic Similarity.
Ph.D. thesis,Institute for Communicating and Collabora-tive Systems, School of Informatics, Univer-sity of Edinburgh.Ido Dagan, Lillian Lee, and Fernando C. N.Pereira.
1999.
Similarity-based models ofword co-occurrence probabilities.
MachineLearning, 34(1?3):43?69.Christiane Fellbaum.
1998.
WordNet, An Elec-tronic Lexical Database.
The MIT Press.Zellig Harris.
1968.
Mathematical structuresof language.
Interscience Tracts in Pure andApplied Mathematics, 21(ix):230 pp.A.
K. Jain, M. N. Murty, and P. J. Flynn.
1999.Data clustering: a review.
ACM ComputingSurveys, 31(3):264?323.Kielikone.
2004.
Dictionary servicemot - dictionaries and terminologies.
[http://www.kielikone.fi/en/].Teuvo Kohonen.
1997.
Self-Organizing Maps(Second Edition), volume 30 of SpringerSeries in Information Sciences.
Springer,Berlin.Lillian Lee.
1999.
Measures of distributionalsimilarity.
In 37th Annual Meeting of theAssociation for Computational Linguistics,pages 25?32.Lillian Lee.
2001.
On the effectiveness of theskew divergence for statistical language anal-ysis.
In Artificial Intelligence and Statistics2001, pages 65?72.Jianhua Lin.
1991.
Divergence measures basedon the Shannon entropy.
IEEE Transactionson Information Theory, 37(1):145?151, Jan-uary.Krister Linde?n.
2003.
Word sense disam-biguation with thessom.
In Proceedings ofthe WSOM?03 ?
Intelligent Systems and In-novational Computing, Kitakuyshu, Japan,September.Christopher D. Manning and Hinrich Schu?tze.1999.
Foundations of Statistical Natural Lan-guage Processing.
The MIT Press, Cam-bridge, Massachusetts.Patrick Andre?
Pantel.
2003.
Clustering byCommittee.
Ph.D. thesis, University of Al-berta, Edmonton, Alberta, Canada.The MathWorks, Inc. 2002.
Matlab withstatistics toolbox and neural network tool-box.
[http://www.mathworks.com/], June18.
Version 6.5.0.180913a Release 13.Ellen M. Voorhees, Claudia Leacock, and Ge-offrey Towell, 1995.
Computational Learn-ing Theory and Natural Language LearningSystems 3: Selecting Good Models, chap-ter Learning context to disambiguate wordsenses, pages 279?305.
MIT Press, Cam-bridge.Piek Vossen.
2001.
Eurowordnet.
[http://www.hum.uva.nl/?ewn/].Julie Elisabeth Weeds.
2003.
Measures and Ap-plications of Lexical Distributional Similarity.Ph.D.
thesis, University of Sussex, Septem-ber.David Yarowsky and Radu Florian.
2002.
Eval-uating sense disambiguation across diverseparameter spaces.
Natural Language Engi-neering, 8(4):293?310, December.CompuTerm 2004  -  3rd International Workshop on Computational Terminology70
