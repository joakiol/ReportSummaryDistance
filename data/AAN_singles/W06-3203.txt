Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 21?30,New York City, USA, June 2006. c?2006 Association for Computational LinguisticsLearning Quantity Insensitive Stress Systems via Local InferenceJeffrey HeinzLinguistics DepartmentUniversity of California, Los AngelesLos Angeles, California 90095jheinz@humnet.ucla.eduAbstractThis paper presents an unsupervised batchlearner for the quantity-insensitive stresssystems described in Gordon (2002).
Un-like previous stress learning models, thelearner presented here is neither cue based(Dresher and Kaye, 1990), nor reliant ona priori Optimality-theoretic constraints(Tesar, 1998).
Instead our learner ex-ploits a property called neighborhood-distinctness, which is shared by all of thetarget patterns.
Some consequences of thisapproach include a natural explanationfor the occurrence of binary and ternaryrhythmic patterns, the lack of higher n-aryrhythms, and the fact that, in these sys-tems, stress always falls within a certainwindow of word edges.1 IntroductionThe central premise of this research is that phonotac-tic patterns are have properties which reflect prop-erties of the learner.
This paper illustrates this ap-proach for quantity-insensitive (QI) stress systems(see below).I present an unsupervised batch learner that cor-rectly learns every one of these languages.
Thelearner succeeds because there is a universal prop-erty of QI stress systems which I refer to asneighborhood-distinctness (to be defined below).This property, which is a structural notion of local-ity, is used by the learning algorithm to successfullyinfer the target pattern from samples.A learner is a function from a set of observationsto a grammar.
An observation is some linguisticsign, in this case a word-sized sequence of stress val-ues.
A grammar is some device that must at least re-spond Yes or No when asked if a linguistic sign is apossible sign for this language (Chomsky and Halle,1968; Halle, 1978).1The remainder of the introduction outlines the ty-pology of the QI stress systems, motivates represent-ing phonotactics with regular languages, and exam-ines properties of the attested patterns.
In ?2, I definethe class of neighborhood-distinct languages.
Thelearning algorithm is presented in two stages.
?3 in-troduces a basic version of the learner the learner,which successfully acquires just under 90% of thetarget patterns.
In ?4, one modification is made tothis learner which consequently succeeds on all tar-get patterns.
?5 discusses predictions made by theselearning algorithms.
The appendix summarizes thetarget patterns and results.1.1 Quantity-Insensitive Stress SystemsStress assignment in QI languages is indifferent tothe weight of a syllable.
For example, Latin isquantity-sensitive (QS) because stress assignmentdepends on the syllable type: if the penultimate syl-lable is heavy (i.e.
has a long vowel or coda) thenit receives stress, but otherwise the antepenult does.The stress systems under consideration here, unlikeLatin, do not distinguish syllable types.1In this respect, this work departs from (or is a special caseof) gradient phonotactic models (Coleman and Pierrehumbert,1997; Frisch et al, 2000; Albright, 2006; Hayes and Wilson,2006)21There are 27 types of QI stress systems found inGordon?s (2002) typology.
Gordon adds six plausi-bly attestable QI systems by considering the behav-ior of all-light-syllabled words from QS systems.These 33 patterns are divided into four kinds: sin-gle, dual, binary and ternary.
Single systems haveone stressed syllable per word, and dual systems upto two.
Binary and ternary systems stress every sec-ond (binary) or third (ternary) syllable.The choice to study QI stress systems was madefor three reasons.
First, they are well studied and thetypology is well established (Hayes, 1995; Gordon,2002).
Secondly, learning of stress systems has beenapproached before (Dresher and Kaye, 1990; Guptaand Touretzky, 1991; Goldsmith, 1994; Tesar, 1998)making it possible to compare learners and results.Third, these patterns have been analyzed with ad-jacency restrictions (e.g.
no clash), as disharmony(e.g.
a primary stress may not be followed by an-other), and with recurrence requirements (e.g.
buildtrochaic feet iteratively from the left).
Thus the pat-terns found in the QI stress systems are represen-tative of other phonotactic domains that the learnershould eventually be extended to.The 33 types are shown in Table 1.
See Gor-don (2002) and Hayes (1995) for details, exam-ples, and original sources.
Note that some patternshave a minimal word condition (Prince, 1980; Mc-Carthy and Prince, 1990; Hayes, 1995), banning ei-ther monosyllables or light monosyllables.
For ex-ample, Cayuvava bans all monosyllables, whereasHopi bans only light monosyllables.
Because thispaper addresses QI stress patterns I abstract awayfrom the internal structure of the syllable.
For con-venience, when stress patterns are explicated in thispaper I assume (stressed) monosyllables are permit-ted.
The learning study, however, includes eachstress pattern both with and without stressed mono-syllables.
Predictions our learner makes with respectto the minimal word condition are given in ?5.2.2We use the (first) language name to exemplify the stresspattern.
The number in parentheses is an index to the lan-guage Gordon?s 2003 appendix.
All stress representations fol-low Gordon?s notation, who uses the metrical grid (Libermanand Prince, 1977; Prince, 1983).
Thus, primary stress is indi-cated by 2, secondary stress by 1, and no stress by 0.1.2 Phonotactics as Regular LanguagesI represent phonotactic descriptions as regular sets,accepted by finite-state machines.
A finite state ma-chine is a 5-tuple (?, Q, q0, F, ?)
where ?
is a finitealphabet, Q is a set of states, q0?
Q is the start state,F ?
Q is a set of final states, and ?
is a set of tran-sitions.
Each transition has an origin and a terminusand is labeled with a symbol of the alphabet; i.e.
atransition is a 3-tuple (o, a, t) where o, t ?
Q anda ?
?.Empirically, it has been observed that mostphonological phenomena are regular (Johnson,1972; Kaplan and Kay, 1981; Kaplan and Kay, 1994;Ellison, 1994; Eisner, 1997; Karttunen, 1998).
Thisis especially true of phonotactics: reduplication andmetathesis, which have higher complexity, are notphonotactic patterns as they involve alternations.3Formally, regular languages are widely studied incomputer science, and their basic properties are wellunderstood (Hopcroft et al, 2001).
Also, a learningliterature exists.
E.g.
the class of regular languagesis not exactly identifiable in the limit (Gold, 1967),but certain subsets of it are (Angluin, 1980; Angluin,1982).
Thus it is becomes possible to ask: Whatsubset of the regular languages delimits the class ofpossible human phonotactics and can properties ofthis class be exploited by a learner?This perspective also connects to finite state mod-els of Optimality Theory (OT) (Prince and Smolen-sky, 1993).
Riggle (2004) shows that if OT con-straints are made finite-state, it is possible to build atransducer that takes any input to a grammatical out-put.
Removing from this transducer the input labelsand hidden structural symbols (such as foot bound-aries) in the output labels yields a phonotactic ac-ceptor for the language, a target for our learner.Consider Pintupi, #26 in Table 1, which exempli-fies a binary stress pattern.
Its phonotactic gram-mar is given in Figure 1.
The hexagon indicates thestart state, and final states are marked by the doubleperimeter.This machine accepts the Pintupi words, but notother words of the same length.
Also, the Pin-tupi grammar accepts an infinite number of words?just like the grammars in Hayes (1995) and Gordon3See Albro (1998; 2005) for restricted extensions to regularlanguages.22Table 1: The Quantity-Insensitive Stress Systems.2Single Systems1.
(1) Chitimacha 20000000 2000000 200000 20000 2000 200 20 22.
(2) Lakota 02000000 0200000 020000 02000 0200 020 02 23.
(3) Hopi (qs) 02000000 0200000 020000 02000 0200 020 20 24.
(4) Macedonian 00000200 0000200 000200 00200 0200 200 20 25.
(5) Nahuatl / Mohawk?
00000020 0000020 000020 00020 0020 020 20 26.
(6) Atayal / Dieguen?o?
00000002 0000002 000002 00002 0002 002 02 2Dual Systems7.
(7f) Quebec French 10000002 1000002 100002 10002 1002 102 12 28.
(9f) Udihe 10000002 1000002 100002 10002 1002 102 02 29.
(10i) Lower Sorbian 20000010 2000010 200010 20010 2010 200 20 210.
(11f) Sanuma 10000020 1000020 100020 10020 1020 020 20 211.
(15f) Georgian 10000200 1000200 100200 10200 0200 200 20 212.
(16i) Walmatjari 20000100 2000100 200100 20100 2010 200 20 2(optional variants) 20000010 2000010 200010 20010Binary Systems13.
(24i) Araucanian 02010101 0201010 020101 02010 0201 020 02 214.
(24f) Creek?
(qs) 01010102 0101020 010102 01020 0102 020 02 215.
(25f) Urubu Kaapor 01010102 1010102 010102 10102 0102 102 02 216.
(26i) Malakmalak 20101010 0201010 201010 02010 2010 020 20 217.
(26f) Cavinen?a?
10101020 0101020 101020 01020 1020 020 20 218.
(27i) Maranungku 20101010 2010101 201010 20101 2010 201 20 219.
(27f) Palestinean Arabic?
(qs) 10101020 1010102 101020 10102 1020 102 20 2Binary Systems with Clash20.
(28i) Central Alaskan Yupik?
01010102 0101012 010102 01012 0102 012 02 221.
(29i) Southern Paiute?
02010110 0201010 020110 02010 0210 020 20 222.
(30i) Gosiute Shoshone 20101011 2010101 201011 20101 2011 201 21 223.
(32f) Biangai 10101020 1101020 101020 11020 1020 120 20 224.
(33f) Tauya 11010102 1010102 110102 10102 1102 102 12 2Binary Systems with Lapse25.
(34f) Piro 10101020 1010020 101020 10020 1020 020 20 226.
(36i) Pintupi / Diyari?
20101010 2010100 201010 20100 2010 200 20 227.
(40f) Indonesian 10101020 1001020 101020 10020 1020 020 20 228.
(42i) Garawa 20101010 2001010 201010 20010 2010 200 20 2Ternary Systems29.
(48i) Ioway-Oto 02001001 0200100 020010 02001 0200 020 02 230.
(49f) Cayuvava?
00100200 0100200 100200 00200 0200 200 20 231.
(67i) Estonian (qs) 20010010 2001010 201010 20010 2010 200 20 2(optional variants) 20101010 2010100 200100 2010020100100 20100102001010032.
(71f) Pacific Yupik (qs) 01001002 0100102 010020 01002 0102 020 02 233.
(72i) Winnebago?
(qs) 00200101 0020010 002001 00201 0020 002 02 2?
Bans monosyllables.?
Bans light monosyllables.230 122041300Figure 1: Stress in Pintupi as a finite state machine(2002), who take the observed forms as instances ofa pattern that extends to longer words.
The learner?stask is to take the Pintupi words in Table 1 and returnthe pattern represented by Figure 1.1.3 Properties of QI Stress PatternsThe deterministic acceptor with the fewest states fora language is called the language?s canonical accep-tor.
Therefore, let us ask what properties the canoni-cal acceptors for the 33 stress types have in commonthat might be exploited by a learner.One property shared by all grammars except Es-tonian is that they have exactly one loop (Estonianhas two).
Though this restriction is nontrivial, it isinsufficient for learning to be guaranteed.4 A secondshared property is slenderness.
A machine is slenderiff it accepts only one word of length n. The only ex-ceptions to this are Walmatjari and Estonian, whichhave free variation in longer words (see Table 1).I focus in this paper on another property which areshared by all machines without exception.
In 29 ofthe canonical acceptors, each state can be uniquelyidentified by its incoming symbol set, its outgo-ing symbol set, and whether it is final or non-final.These items make up the neighborhood of a state,which will be formally defined in the next section.The other four stress systems have non-canonicalacceptors wherein each state can also be uniquelyidentified by its neighborhood.
This property I callneighborhood-distinctness.
Thus, neighborhood-distinctness is a universal property of QI stress sys-tems, and it is this property that the learner will ex-ploit.4The proof is similar to the one used to show the cofinitelanguages are not learnable (Osherson et al, 1986).2 Neighborhood-Distinctness2.1 Neighborhood-Distinct AcceptorsThe neighborhood of a state in an acceptor(?, Q, q0, F, ?)
is defined in (1).
(1) The neighborhood of a state q is triple(f, I,O) where f = 1 iff q ?
F and f = 0otherwise, I = {a | ?o ?
Q, (o, a, q) ?
?
},and O = {a | ?t ?
Q, (q, a, t) ?
?
}Thus the neighborhood of state can be determinedby looking solely at whether or not it is final, the setof symbols labeling the transitions which reach thatstate, and the set of symbols labeling the transitionswhich depart that state.
For example in Figure 2,states p and q have the same neighborhood becausethey are both nonfinal, can both be reached by someelement of {a, b}, and because each state can onlybe exited by observing a member of {c, d}.5qa cdbpacdabFigure 2: Two states with the same neighborhood.Neighborhood-distinct acceptors are defined in(2).
(2) An acceptor is said to be neighborhood-distinct iff no two states have the sameneighborhood.This class of acceptors is finite: there are 22|?|+1neighborhoods, i.e.
types of states.
Since each statein a neighborhood-distinct machine has a uniqueneighborhood, this becomes an upper bound on ma-chine size.65The notion of neighborhood can be generalized to neigh-borhoods of size k, where sets I and O are defined as the in-coming and outgoing paths of length k. However, this paper isonly concerned with neighborhoods of size 1.6For some acceptor, the notion of neighborhood lends it-self to an equivalence relation RNover Q: pRNq iff p and qhave the same neighborhood.
Therefore, RNpartitions Q intoblocks, and neighborhood-distinct machines are those wherethis partition equals the trivial partition.242.2 Neighborhood-Distinct LanguagesThe class of neighborhood-distinct languages is de-fined in (3).
(3) The neighborhood-distinct languages arethose for which there is an acceptor whichis neighborhood-distinct.The neighborhood-distinct languages are a (finite)proper subset of the regular languages over analphabet ?
: all regular languages whose small-est acceptors have more than 22|?|+1 states cannotbe neighborhood-distinct (since at least two stateswould have the same neighborhood).The canonically neighborhood-distinct languagesare defined in (4).
(4) The canonically neighborhood-distinctlanguages are those for which the canonicalacceptor is neighborhood-distinct.The canonically neighborhood-distinct languagesform a proper subset of the neighborhood-distinctlanguages.
For example, the canonical accep-tor shown in Figure 3 of Lower Sorbian (#9 inTable 1) is not neighborhood-distinct (states 2and 3 have the same neighborhood).
However,there is a non-canonical (because non-deterministic)neighborhood-distinct acceptor for this language, asshown in Figure 4.0 1220513060140 10Figure 3: The canonical acceptor for Lower Sorbian.0123242520200001Figure 4: A neighborhood-distinct acceptor forLower Sorbian.Neighborhood-distinctness is a universal propertyof the patterns under consideration.
Additionally, itis a property which a learner can use to induce agrammar from surface forms.3 The Neighborhood LearnerIn this section, I present the basic unsupervisedbatch learner, called the Neighborhood Learner,which learns 29 of the 33 patterns.
In the nextsection, I introduce one modification to this learnerwhich results in perfect accuracy.The basic version of the learner operates in twostages: prefix tree construction and state-merging,cf.
Angluin (1982).
These two stages find smallerdescriptions of the observed data; in particular state-merging may lead to generalization (see below).A prefix tree is constructed as follows.
Set the ini-tial machine M = (?, {q0}, q0, ?, ?)
and the currentstate c = q0.
With each word, each symbol a is con-sidered in order.
If ?t ?
Q, (c, a, t) ?
?
then setc = t. Otherwise, add a new state n to Q and a newarc (c, a, n) to ?.
A new arc is therefore created onevery symbol in the first word.
The last state for aword is added to F .
The process is repeated for eachword.
The prefix tree for Pintupi words from Table1 is shown in Figure 5.0 122031110405110060719080Figure 5: The prefix tree of Pintupi words.The second stage of the learner is state-merging,a process which reduces the number of states in themachine.
A key concept in state merging is thatwhen two states are merged into a single state, theirtransitions are preserved.
Specifically, if states p andq merge, then a merged state pq is added to the ma-chine, and p and q are removed.
For every arc thatleft p (or q) to a state r, there is now an arc from pqgoing to r. Likewise, for every arc from a state r top (or q), there is now an arc from r to pq.The post-merged machine accepts every word thatthe pre-merged machine accepts, and possibly more.For example, if there is a path between two stateswhich become merged, a loop is formed.25What remains to be explained is the criteria thelearner uses to determine whether two states inthe prefix tree merge.
The Neighborhood Learnermerges two states iff they have the same neighbor-hood, guaranteeing that the resulting grammar isneighborhood-distinct.The intuition is that the prefix tree providesa structured representation of the input and hasrecorded information about different environments,which are represented in the tree as states.
Learningis a process which identifies actually different envi-ronments as ?the same??
here states are ?the same?iff their local features, i.e their neighborhoods, arethe same.
For example, suppose states p and q in theprefix tree are both final or both nonfinal, and theyshare the same incoming symbol set and outgoingsymbol set.
In the learner?s eyes they are then ?thesame?, and will be merged.The merging criteria partitions the states of thePintupi prefix tree into five groups.
States 3,5 and7 are merged; states 2,4,6 are merged, and states8,9,10,12 are merged.
Merging of states halts whenno two nodes have the same neighborhood?
thus, theresulting machine is neighborhood-distinct.
The re-sult for Pintupi is shown in Figure 6.0 122-4-603-5-718-9-10-1100Figure 6: The grammar learned for Pintupi.The machine in Figure 6 is equivalent to the onein Figure 1?
they accept exactly the same language.7I.e.
neighborhood merging of the prefix tree in Fig-ure 5 generalizes from the data exactly as desired.3.1 Results of Neighborhood LearningThe Neighborhood Learner successfully learns 29 ofthe 33 language types (see appendix).
These are ex-actly the 29 canonically neighborhood-distinct lan-guages.
This suggests the following claim, whichhas not been proven.87This can be verified by checking to see if the minimizedversions of the two machines are isomorphic.8The proof is made difficult by the fact that the acceptorreturned by the Neighborhood Learner is not necessarily the(5) Conjecture: The Neighborhood Learneridentifies the class of canonicallyneighborhood-distinct languages.In ?4, I discuss why the learner fails where it does,and introduce a modification which results in perfectaccuracy.4 Reversing the Prefix TreeThis section examines the four cases where neigh-borhood learning failed and modifies the learning al-gorithm, resulting in perfect accuracy.
The goal is torestrict generalization because in every case wherelearning failed, the learner overgeneralized by merg-ing more states than it should have.
Thus, the re-sulting grammars recognize multiple words with nsyllables.The dual stress pattern of Lower Sorbian placesstress initially and, in words of four or more sylla-bles, on the penult (see #9 Table 1).
The prefix treebuilt from these words is shown in Figure 7.0 12201513011 12013 14016014015091601007180Figure 7: The prefix tree for Lower Sorbian.Here the Neighborhood Learner fails because itmerges states 2 and 3.
The resulting grammar incor-rectly accepts words of the form 20?.The proposed solution follows from the observa-tion that if the prefix tree were constructed in reverse(reading each word from right to left) then the corre-sponding states in this structure would not have thesame neighborhoods, and thus not be merged.
A re-verse prefix tree is constructed like a forward prefixtree, the only difference being that the order of sym-bols in each word is reversed.
When neighborhoodlearning is applied to this structure and the result-ing machine reversed again, the correct grammar isobtained, shown in Figure 4.How is the learner to know whether to constructthe prefix tree normally or in reverse?
It simply doesboth and intersects the results.
Intersection of twocanonical acceptor.26languages is an operation which returns a languageconsisting of the words common to both.
Similarly,machine intersection returns an acceptor which rec-ognizes just those words that both machines recog-nize.
This strategy is thus conservative: the learnerkeeps only the most robust generalizations, whichare the ones it ?finds?
in both the forward and reverseprefix trees.This new learner is called the Forward BackwardNeighborhood (FBN) Learner and it succeeds withall the patterns (see appendix).Interestingly, the additional languages the FBNLearner can acquire are ones that, under foot-basedanalyses like those in Hayes (1995), require feet tobe built from the right word edge.
For example,Lower Sorbian has a binary trochee aligned to theright word edge; Indonesian iteratively builds binarytrochaic feet from the right word edge; Cayuvava it-eratively builds anapests from the right word edge.Thus structuring the input in reverse appears akin toa footing procedure which proceeds from the rightword boundary.5 Predictions of Neighborhood LearningIn this section, let us examine some of the predic-tions that are made by neighborhood learning.
Inparticular, let us consider the kinds of languages thatthe Neighborhood Learner can and cannot learn andcompare them with the attested typology.5.1 Binary and Ternary Stress PatternsNeighborhood learning suggests an explanation ofthe fact that the stress rhythms found in naturallanguage are binary or ternary and not higher n-ary, and of the fact that stress falls within a three-syllable window of the word edge: perhaps only sys-tems with these properties are learnable.
This is be-cause the neighborhood learner cannot distinguishbetween sequences of the same symbol with lengthgreater than two.As an example, consider the quaternary (andhigher n-ary) stress pattern 2(0001)?
(0|00|000).9 Ifthe learner is exposed to samples from this pattern,it incorrectly generalizes to 2(000?1)?
(0|00|000).9I follow Hopcroft et al(2001) in our notation of regularexpressions with one substitution?
we use | instead of + to in-dicate disjunction.Similarly, neighborhood learning cannot distin-guish a form like 02000 from 020000, so a sys-tem that places stress on the pre-antepenult (e.g.02000, 002000, 0002000) is not learnable.
Withsamples from the pre-antepenultimate language(0?2000|200|20|2), the learner incorrectly general-izes to 0?20?.5.2 Minimal Word ConditionsA subtle prediction made by neighborhood-learningis that a QI stress language with a pattern like theone exemplified by Hopi (shown in Figure 8) cannothave a minimal word condition banning monosylla-bles.
This is because if there were no monosyllablesin this language, then state 4 in Figure 8 would havethe same neighborhood as state 2 (as in Figure 9).042105022300Figure 8: The stress pattern exemplified by Hopi,allowing monosyllables.042105022300Figure 9: The stress pattern exemplified by Hopi,not allowing monosyllables.Since such a grammar recognizes a non-neighborhood-distinct language it cannot be learnedby the Neighborhood Learner.As it happens, Hopi is a QS language which pro-hibits light, but permits heavy, monosyllables.
SinceI have abstracted away from the internal structure ofthe syllable in this paper, this prediction is not dis-confirmed by the known typology: there are in factno QI Hopi-like stress patterns in Gordon?s (2002)typology which ban all monosyllables; i.e there areno QI patterns like the one in Figure 9.Some QI languages do have a minimal word con-dition banning all monosyllables.
To our knowl-edge these are Cavinen?a and Cayuvava (see Ta-ble 1), Mohawk (which places stress on the penult27like Nahuatl), and Diyari, Mohwak, Pitta Pitta andWangkumara (all which assign stress like Pintupi)(Hayes, 1995).
The Forward Backward Neighbor-hood Learner learns all of these patterns successfullyirrespective of whether the patterns (and correspond-ing input samples) permit monosyllables, predictingthat such patterns do not correlate with a prohibitionon monosyllables (see appendix).Other QI languages prohibit light monosyllables.Dieguen?o, for example, places stress finally likeAtayal (see Table 1), but only allows heavy mono-syllables.
This is an issue to attend to in future re-search when trying to extend the learning algorithmto QS patterns, when the syllable type (light/heavy)is included in the representational scheme.5.3 Restrictiveness and Other ApproachesThere are languages that can be learned by neighbor-hood learning that phonologists do not consider tobe natural.
For example, the Neighborhood Learnerlearns a pattern in which words with an odd numberof syllables bear initial stress but words with an evennumber of syllables bear stress on all odd syllables.However, the grammar for this language differs fromall of the attested systems in that it has two loops butis slender (cf.
Estonian which has two loops but isnot slender).
Thus this case suggests a further formalrestriction to the class of possible stress systems.More serious challenges of unattestable, butNeighborhood Learner-able, patterns exist; e.g.21*.
In other words, it does not followfrom neighborhood-distinctness that languages withstress must have stressless syllables.
Nor does thenotion that every word must bear some stress some-where (i.e.
Culminativity?
see Hayes (1995)).However, despite the existence of learnable patho-logical languages, this approach is not unrestricted.The class of languages to be learned is finite?asin the Optimality-theoretic and Principles and Pa-rameters frameworks?and is a proper subset of theregular languages.
Future research will seek addi-tional properties to better approximate the class ofQI stress systems that can be exploited by inductivelearning.This approach offers more insight into QI stresssystems than earlier learning models.
Optimality-theoretic learning models (e.g.
(Tesar, 1998)) andmodels set in the Principles and Parameters frame-work (e.g.
(Dresher and Kaye, 1990)) make no useof any property of the class of patterns to be learnedbeyond its finiteness.
Also, our learner is much sim-pler than these other models, which require a largeset of a priori switches and cues or constraints.6 ConclusionsThis paper presented a batch learner which correctlyinfers the attested QI stress patterns from surfaceforms.
The key to the success of this learner is that ittakes advantage of a universal property of QI stresssystems, neighborhood-distinctness.
This propertyprovides a natural explanation for why stress fallswithin a particular window of the word edge andwhy rhythms are binary and ternary.
It is strik-ing that all of the attested patterns are learned bythis simple approach, suggesting that it will be fruit-ful and revealing when applied to other phonotacticlearning problems.AcknowledgementsI especially thank Bruce Hayes, Ed Stabler, ColinWilson, Kie Zuraw and the anonymous reviewers forinsightful comments and suggestions.
I also thankSarah Churng, Greg Kobele, Katya Pertsova, andSarah VanWagenen for helpful discussion.ReferencesAdam Albright.
2006.
Gradient phonotactic effects: lex-ical?
grammatical?
both?
neither?
Talk handout fromthe 80th Annual LSA Meeting, Albuquerque, NM.Dan Albro.
1998.
Evaluation, implementation, and ex-tension of primitive optimality theory.
Master?s thesis,University of California, Los Angeles.Dan Albro.
2005.
A Large-Scale, LPM-OT Analysis ofMalagasy.
Ph.D. thesis, University of California, LosAngeles.Dana Angluin.
1980.
Finding patterns common to a setof strings.
Journal of Computer and System Sciences,21:46?62.Dana Angluin.
1982.
Inference of reversible languages.Journal for the Association of Computing Machinery,29(3):741?765.Noam Chomsky and Morris Halle.
1968.
The SoundPattern of English.
Harper & Row.28John Coleman and Janet Pierrehumbert.
1997.
Stochas-tic phonological grammars and acceptability.
In Com-puational Phonolgy, pages 49?56.
Somerset, NJ: As-sociation for Computational Linguistics.
Third Meet-ing of the ACL Special Interest Group in Computa-tional Phonology.Elan Dresher and Jonathan Kaye.
1990.
A computa-tional learning model for metrical phonology.
Cogni-tion, 34:137?195.Jason Eisner.
1997.
What constraints should ot allow?Talk handout, Linguistic Society of America, Chicago,January.
Available on the Rutgers Optimality Archive,ROA#204-0797, http://roa.rutgers.edu/.T.M.
Ellison.
1994.
The iterative learning of phonologi-cal constraints.
Computational Linguistics, 20(3).S.
Frisch, N.R.
Large, and D.B.
Pisoni.
2000.
Percep-tion of wordlikeness: Effects of segment probabilityand length on the processing of nonwords.
Journal ofMemory and Language, 42:481?496.E.M.
Gold.
1967.
Language identification in the limit.Information and Control, 10:447?474.John Goldsmith.
1994.
A dynamic computational the-ory of accent systems.
In Jennifer Cole and CharlesKisseberth, editors, Perspectives in Phonology, pages1?28.
Stanford: Center for the Study of Language andInformation.Matthew Gordon.
2002.
A factorial typology ofquantity-insensitive stress.
Natural Language andLinguistic Theory, 20(3):491?552.
Appendices avail-able at http://www.linguistics.ucsb.edu/faculty/gordon/pubs.html.Prahlad Gupta and David Touretzky.
1991.
What a per-ceptron reveals about metrical phonology.
In Proceed-ings of the Thirteenth Annual Conference of the Cog-nitive Science Society, pages 334?339.Morris Halle.
1978.
Knowledge unlearned and untaught:What speakers know about the sounds of their lan-guage.
In Linguistic Theory and Psychological Real-ity.
The MIT Prss.Bruce Hayes and Colin Wilson.
2006.
The ucla phono-tactic learner.
Talk handout from UCLA PhonologySeminar.Bruce Hayes.
1995.
Metrical Stress Theory.
ChicagoUniversity Press.John Hopcroft, Rajeev Motwani, and Jeffrey Ullman.2001.
Introduction to Automata Theory, Languages,and Computation.
Addison-Wesley.C.
Douglas Johnson.
1972.
Formal Aspects of Phonolog-ical Description.
The Hague: Mouton.Ronald Kaplan and Martin Kay.
1981.
Phonologicalrules and finite state transducers.
Paper presented atACL/LSA Conference, New York.Ronald Kaplan and Martin Kay.
1994.
Regular modelsof phonological rule systems.
Computational Linguis-tics, 20(3):331?378.Lauri Karttunen.
1998.
The proper treatment of optimal-ity theory in computational phonology.
Finite-statemethods in natural language processing, pages 1?12.Mark Liberman and Alan Prince.
1977.
On stress andlinguistic rhythm.
Linguistic Inquiry, 8:249?336.John McCarthy and Alan Prince.
1990.
Foot and wordin prosodic morphology.
Natural Language and Lin-guistic Theory, 8:209?283.Daniel Osherson, Scott Weinstein, and Michael Stob.1986.
Systems that Learn.
MIT Press, Cambridge,Massachusetts.Alan Prince and Paul Smolensky.
1993.
Optimal-ity theory: Constraint interaction in generative gram-mar.
Technical Report 2, Rutgers University Centerfor Cognitive Science.Alan Prince.
1980.
A metrical theory for estonian quan-tity.
Linguistic Inquiry, 11:511?562.Alan Prince.
1983.
Relating to the grid.
Linguistic In-quiry, 14(1).Jason Riggle.
2004.
Generation, Recognition, andLearning in Finite State Optimality Theory.
Ph.D. the-sis, University of California, Los Angeles.Bruce Tesar.
1998.
An interative strategy for languagelearning.
Lingua, 104:131?145.Appendix.
Target Grammars and ResultsSee Table 2.
Circled numbers mean the learner iden-tified the pattern.
The ?
mark means the learnerfailed to identify the pattern.
The number inside thecircle indicates which forms were necessary for con-vergence.
Specifically, in the ?With Monosyllables?column, ?n means the learner succeeded learningthe ?With Monosyllables?
pattern with words withone to n syllables.
Likewise, in the ?Without Mono-syllables?
column, ?n means the learner succeededlearning the ?Without Monosyllables?
pattern withwords with two to n syllables.
For example, inthe ?With Monosyllables?
column, ?5 means thatthe learner succeeded only with words with one tofive syllables.
The learners still succeed when givenlonger words.
The number n may be thought of asthe smallest word needed for generalization.29Table 2: Learning ResultsWith Monosyllables Without MonosyllablesLanguage RegExp NHLFBNLRegExp NHLFBNLSingle1.
Chitimacha 20?
?4 ?4 20+ ?4 ?42.
Lakota (2|020?)
?5 ?5 020?
?5 ?53.
Hopi (qs) (2|20|020+) ?5 ?5 (020?|2)0 ?
?4.
Macedonian (2|20|0?200) ?6 ?6 (2|0?20)0 ?
?65.
Nahuatl (2|0?20) ?5 ?5 0?20 ?5 ?56.
Atayal 0?2 ?4 ?4 0+2 ?4 ?4Dual7.
Quebec French (2|10?2) ?5 ?5 10?2 ?5 ?58.
Udihe (2|(10?
)?02) ?5 ?6 (10?
)?02 ?5 ?69.
Lower Sorbian (2|2(0|0+1)0) ?
?6 2(0|0+1)0 ?
?610.
Sanuma (2|20|020|10+20) ?6 ?7 (2|02|10+2)0 ?6 ?711.
Georgian (2|20|0?200|10+200) ?7 ?8 (2|0?20|10+20)0 ?
?812.
Walmatjari (2|20(0?10)?0?)
?
?6 20(0?10)?0?
?
?6Binary13.
Araucanian (2|02(01)?0?)
?6 ?6 02(01)?0?
?6 ?614.
Creek (qs) (2|(01)?020?)
?6 ?6 (01)?020?
?6 ?615.
Urubu Kappor 0?
(10)?2 ?5 ?5 (0|10)(10)?2 ?5 ?516.
Malakmalak (2|0?2(01)?0) ?6 ?6 0?2(01)?0 ?6 ?617.
Cavinen?a (2|0?
(10)?20) ?6 ?6 0?
(10)?20 ?6 ?618.
Maranungku 2(01)?0?
?5 ?5 20(10)?1?
?5 ?519.
Palestinean Arabic (qs) (10)?20?
?5 ?5 (20|(10)+20?)
?5 ?5Binary w/clash20.
Central Alaskan Yupik (0(10)?1?
)?2 ?5 ?5 0(10)?1?2 ?5 ?521.
Southern Paiute (2|(2|02(01) ?
1?
)0) ?7 ?8 (2|02(01) ?
1?
)0 ?7 ?822.
Gosiute Shoshone 2((01)?0?1)?
?5 ?6 2(01)?0?1 ?5 ?623.
Biangai (2|1?
(10)?20) ?7 ?7 1?
(10)?20 ?7 ?724.
Tauya (2|1?
(10)?2) ?6 ?6 1?
(10)?2 ?6 ?6Binary w/lapse25.
Piro (2|(10)?0?20) ?6 ?7 (10)?0?20 ?6 ?726.
Pintupi 2(0(10)?0?)?
?6 ?6 20(10)?0?
?6 ?627.
Indonesian (2|(10)?0?
(10)?20) ?
?8 (10)?0?
(10)?20 ?
?828.
Garawa 2(00?(10)?)?
?6 ?6 200?(10)?
?6 ?6Ternary29.
Ioway Oto (2|02(001)?0?0?)
?7 ?8 02(001)?0?0?
?7 ?830.
Cayuvava (0?0?
(100)?200|20|2) ?
?9 (0?0?
(100)?20|2)0 ?
?931.
Estonian (qs) 20?0?(100|10)?
?6 ?6 200?(100|10)?
?6 ?632.
Pacific Yupik (qs) (2|0(100)?
(20?|102)) ?7 ?7 0(100)?
(20?|102) ?7 ?733.
Winnebago (qs) (2|02|002(001)?0?1?)
?9 ?9 (02|002(001)?0?1?)
?9 ?9NHL : Neighborhood Learner FBNL : Forward Backward Neighborhood Learner30
