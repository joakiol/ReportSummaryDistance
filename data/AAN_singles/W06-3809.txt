Workshop on TextGraphs, at HLT-NAACL 2006, pages 53?60,New York City, June 2006. c?2006 Association for Computational LinguisticsRandom-Walk Term Weightingfor Improved Text ClassificationSamer Hassan and Carmen BaneaDepartment of Computer ScienceUniversity of North TexasDenton, TX 76203samer@unt.edu, carmen@unt.eduAbstractThis paper describes a new approach forestimating term weights in a text classifi-cation task.
The approach uses term co-occurrence as a measure of dependencybetween word features.
A random walkmodel is applied on a graph encodingwords and co-occurrence dependencies,resulting in scores that represent a quan-tification of how a particular word featurecontributes to a given context.
We arguethat by modeling feature weights usingthese scores, as opposed to the traditionalfrequency-based scores, we can achievebetter results in a text classification task.Experiments performed on four standardclassification datasets show that the newrandom-walk based approach outperformsthe traditional term frequency approach tofeature weighting.1 IntroductionTerm frequency has long been adapted as a measureof term significance in a specific context (Robert-son and Jones, 1997).
The logic behind it is that themore a certain term is encountered in a certain con-text, the more it carries or contributes to the mean-ing of the context.
Due to this belief, term frequencyhas been a major factor in estimating the probabilis-tic distribution of features using maximum likeli-hood estimates and hence has been incorporated in abroad spectrum of tasks ranging from feature selec-tion techniques (Yang and Pedersen, 1997; Schutzeet al, 1995) to language models (Bahl et al, 1983).In this paper we introduce a new measure of termweighting, which integrates the locality of a termand its relation to the surrounding context.
Wemodel this local contribution using a co-occurrencerelation in which terms that co-occur in a certaincontext are likely to share between them some oftheir importance (or significance).
Note that in thismodel the relation between a given term and its con-text is not linear, since the context itself consists ofa collection of other terms, which in turn have adependency relation with their own context, whichmight include the original given term.
In order tomodel this recursive relation we use a graph-basedranking algorithm, namely the PageRank random-walk algorithms (Brin and Page, 1998), and its Text-Rank adaption to text processing applications (Mi-halcea and Tarau, 2004).
TextRank takes as in-put a set of textual entities and relations betweenthem, and uses a graph-based ranking algorithm(also known as random walk algorithm) to producea set of scores that represent the accumulated weightor rank for each textual entity in their context.
TheTextRank model was so far evaluated on three nat-ural language processing tasks: document summa-rization, word sense disambiguation, and keywordextraction, and despite being fully unsupervised, ithas been shown to be competitive with other some-time supervised state-of-the-art algorithms.In this paper, we show how TextRank can beused to model the probabilistic distribution of wordfeatures in a document, by making further use ofthe scores produced by the random-walk model.53Through experiments performed on a text classifi-cation task, we show that these random walk scoresoutperform the traditional term frequencies typicallyused to model the feature weights for this task.2 Graph-based Ranking AlgorithmsThe basic idea implemented by an iterative graph-based ranking algorithm is that of ?voting?
or ?rec-ommendation?.
When one vertex links to anotherone, it is basically casting a vote for that other ver-tex.
The higher the number of votes that are castfor a vertex, the higher the importance of the ver-tex.
Moreover, the importance of the vertex castinga vote determines how important the vote itself is,and this information is also taken into account bythe ranking algorithm.
Hence, the score associatedwith a vertex is determined based on the votes thatare cast for it, and the scores of the vertices castingthese votes.While there are several graph-based ranking algo-rithms previously proposed in the literature (Heringset al, 2001), we focus on only one such algorithm,namely PageRank (Brin and Page, 1998), as it waspreviously found successful in a number of applica-tions, including Web link analysis (Brin and Page,1998), social networks (Dom et al, 2003), citationanalysis, and more recently in several text process-ing applications (Mihalcea and Tarau, 2004), (Erkanand Radev, 2004).Given a graph G = (V,E), let In(Va) be theset of vertices that point to vertex Va (predecessors),and let Out(Va) be the set of vertices that vertex Vapoints to (successors).
The PageRank score associ-ated with the vertex Va is then defined using a recur-sive function that integrates the scores of its prede-cessors:S(Va) = (1 ?
d) + d ?
?Vb?In(Va)S(Vb)|Out(Vb)|(1)where d is a parameter that is set between 0 and 11.The score of each vertex is recalculated upon eachiteration based on the new weights that the neighbor-ing vertices have accumulated.
The algorithm termi-nates when the convergence point is reached for allthe vertices, meaning that the error rate for each ver-tex falls below a pre-defined threshold.
Formally,1The typical value for d is 0.85 (Brin and Page, 1998), andthis is the value we are also using in our implementation.for a vertex Vi let Sk(Vi) be the rank or the scoreat iteration k and Sk+1(Vi) be the score at iterationk + 1.
The error rate ER is defined as:ER = Sk+1(Vi) ?
Sk(Vi) (2)This vertex scoring scheme is based on a ran-dom walk model, where a walker takes random stepson the graph G, with the walk being modeled asa Markov process ?
that is, the decision on whatedge to follow is solely based on the vertex wherethe walker is currently located.
Under certain con-ditions, this model converges to a stationary dis-tribution of probabilities, associated with verticesin the graph.
Based on the Ergodic theorem forMarkov chains (Grimmett and Stirzaker, 1989), thealgorithm is guaranteed to converge if the graph isboth aperiodic and irreducible.
The first condition isachieved for any graph that is a non-bipartite graph,while the second condition holds for any stronglyconnected graph ?
property achieved by PageRankthrough the random jumps introduced by the (1?d)factor.
In matrix notation, the PageRank vector ofstationary probabilities is the principal eigenvectorfor the matrix Arow, which is obtained from the ad-jacency matrix A representing the graph, with allrows normalized to sum to 1: (P = ATrowP ).Intuitively, the stationary probability associatedwith a vertex in the graph represents the probabilityof finding the walker at that vertex during the ran-dom walk, and thus it represents the importance ofthe vertex within the graph.
In the context of se-quence data labeling, the random walk is performedon the label graph associated with a sequence ofwords, and thus the resulting stationary distributionof probabilities can be used to decide on the mostprobable set of labels for the given sequence.2.1 TextRankGiven a natural language processing task, the Text-Rank model includes four general steps for theapplication of a graph-based ranking algorithm tograph structures derived from natural language texts:1.
Identify text units that best define the proposedtask and add them as vertices in the graph.2.
Identify relations that connect such test units,and use these relations to draw edges between54vertices in the graph.
Edges can be directed orundirected, weighted or un-weighted.3.
Iterate the graph ranking algorithm to conver-gence.4.
Sort vertices based on their final score.
Use thevalues attached to each vertex for ranking.The strength of this model lies in the global repre-sentation of the context and its ability to model howthe co-occurrence between features might propagateacross the context and affect other distant features.While TextRank has already been applied to sev-eral language processing tasks, we focus here on thekeyword extraction task, since it best relates to ourapproach.
The goal of a keyword extraction tool isto find a set of words or phrases that best describe agiven document.
The co-occurrence relation withina specific window is used to portray the correlationbetween words, which are represented as vertices inthe graph.
Two vertices are connected if their cor-responding lexical units co-occur within a windowof at most N words, where N can be set to anyvalue greater than two.
The TextRank applicationto keyword extraction has also used different syn-tactic filters for vertex selection, including all openclass words, nouns and verbs, nouns and adjectives,and others.
The algorithm was found to provide thebest results using nouns and adjectives with a win-dow size of two.Our approach follows the same main steps as usedin the TextRank keyword extraction application.
Weare however incorporating a larger number of lexicalunits, and we use different window sizes, as we willshow in the following section.3 TextRank for Term WeightingThe goal of the work reported in this paper is tostudy the ranking scores obtained using TextRank,and evaluate their potential usefulness as a new mea-sure of term weighting.To understand how the random-walk weights(rw) might be a good replacement for the traditionalterm frequency weights (tf ), consider the examplein Figure 1.
The example represents a sample doc-ument from the Reuters collection.
A graph is con-structed as follows.
If a term has not been previouslyseen, then a node is added to the graph to representthis term.
A term can only be represented by onenode in the graph.
An undirected edge is drawn be-tween two nodes if they co-occur within a certainwindow size.
This example assumes a window sizeof two, corresponding to two consecutive terms inthe text (e.g.
London is linked to based).London-based sugar operator Kaines Ltd con-firmed it sold two cargoes of white sugar to Indiaout of an estimated overall sales total of four or fivecargoes in which other brokers participated.
Thesugar, for April/May and April/June shipment, wassold at between 214 and 218 dlrs a tonne cif, it said.Figure 1: Sample Reuters documentLondonbasedsugaroperatorKainesconfirmedsoldcargoeswhiteIndiaestimatedsalestotalbrokersparticipatedAprilMayJuneshipmentdlrstonnecifFigure 2: Sample graphTable 1 shows the tf and rw weights, also plottedin Figure 3.
By analyzing the rw weights, we canobserve a non-linear correlation with the tf weights,with an emphasis given to terms surrounding impor-tant key term like e.g.
?sugar?
or ?cargoes.?
Thisspatial locality has resulted in higher ranks for termslike ?operator?
compared to other terms like ?lon-don?2.2All the missing words (e.g.
?Ltd,?
?it?)
that are not shownin the graph are common-words that were eliminated in the pre-processing phase.55Term rw tfsugar 2.248 3sold 1.594 2april 1.407 2cargoes 1.542 2cif 0.600 1sales 0.891 1london 0.546 1tonne 1.059 1shipment 0.829 1based 0.933 1estimated 0.888 1dlrs 0.938 1kaines 0.871 1confirmed 0.859 1total 0.856 1white 0.796 1india 0.846 1operator 0.839 1brokers 0.826 1june 0.801 1participated 0.819 1Table 1: tf & rw scores00.511.522.533.5sugarsoldcargoesapriltonnedlrsbasedsalesestimatedkainesconfirmedtotalindiaoperatorshipmentbrokersparticipatedjunewhite ciflondonFrequencyr.wt.fFigure 3: tf & rw plots4 Experimental SetupTo evaluate our random-walk based approach to fea-ture weighting, we integrate it in a text classificationalgorithm, and evaluate its performance on severalstandard text classification data sets.4.1 Random-Walk Term WeightingStarting with a given document, we determine aranking over the words in the document by using theapproach described in Section 3.First, we tokenize the document for punctuation,special symbols, word abbreviations.
We also re-move the common words, using a list of approx-imately 500 frequently used words as used in theSmart retrieval system 3.Next, the resulting text is processed to extract bothtf and rw weights for each term in the document.Note that we do not apply any syntactic filters, asit was previously done in applications of TextRank.Instead, we consider each word as a potential fea-ture.
To determine tf we simply count the frequen-cies of each word in the document.
To determinerw, all the terms are added as vertices in a graphrepresenting the document.
A co-occurrence scan-ner is then applied to the text to relate the terms thatco-occur within a given window size .
For a giventerm, all the terms that fall in the vicinity of thisterm are considered dependent terms.
This is rep-resented by a set of edges that connect this term toall the other terms in the window.
Experiments areperformed for window sizes of 2, 4, 6, and 8.
Oncethe graph is constructed and the edges are in place,the TextRank algorithm is applied4.
The result of theranking process is a list of all input terms and theircorresponding rw scores.We then calculate tf.idf and rw.idf as follows:tf.idf = tf ?
logNDnwhere ND represent the total number of documentsin the collection and n is the number of documentsin which the target term appeared at least once.Similarly,rw.idf = rw ?
logNDnThese term weights (tf.idf or rw.idf ) are thenused to create a feature vector for each document.The vectors are fed to a traditional text classifica-tion system, using one of the learning algorithms de-scribed below.
The results obtained using tf.idf willact as a baseline in our evaluation.4.2 Text ClassificationText classification is a problem typically formulatedas a machine learning task, where a classifier learnshow to distinguish between categories in a given set3ftp://ftp.cs.cornell.edu/pub/smart.4We use an implementation where the maximum number ofiterations is limited to 100, the damping factor is set to 0.85, andconvergence threshold to 0.0001.
Each graph node is assignedwith an initial weight of 0.25.56using features automatically extracted from a collec-tion of training documents.
There is a large bodyof algorithms previously tested on text classificationproblems, due also to the fact that this task is oneof the testbeds of choice for machine learning algo-rithms.
In the experiments reported here, we com-pare results obtained with four frequently used textclassifiers ?
Rocchio, Na?
?ve Bayes, Nearest Neigh-bor, and Support Vector Machines, selected based ontheir diversity of learning methodologies.Na?
?ve Bayes.
The basic idea in a Na?
?ve Bayestext classifier is to estimate the probability of acategory given a document using joint probabili-ties of words and documents.
Na?
?ve Bayes as-sumes word independence, which means that theconditional probability of a word given a categoryis assumed to be independent of the conditionalprobability of other words given the same category.Despite this simplification, Na?
?ve Bayes classifierswere shown to perform surprisingly well on textclassification (Joachims, 1997), (Schneider, 2004).While there are several versions of Na?
?ve Bayesclassifiers (variations of multinomial and multivari-ate Bernoulli), we use the multinomial model (Mc-Callum and Nigam, 1998), which was shown to bemore effective.Rocchio.
This is an adaptation of the relevancefeedback method developed in information retrieval(Rocchio, 1971).
It uses standard tf.idf weightedvectors to represent documents, and builds a pro-totype vector for each category by summing up thevectors of the training documents in each category.Test documents are then assigned to the categorythat has the closest prototype vector, based on acosine similarity.
Text classification experimentswith different versions of the Rocchio algorithmshowed competitive results on standard benchmarks(Joachims, 1997), (Moschitti, 2003).KNN.
K-Nearest Neighbor is one of the earliest textcategorization approaches (Makoto and Takenobu,1995; Masand et al, 1992).
The algorithm classifiesa test document based on the best class label identi-fied for the nearest K-neighbors in the training doc-uments.
The best class label is chosen by weightingthe class of each similar training document with itssimilarity to the target test document.SVM.
Support Vector Machines (Vapnik, 1995) isa state-of-the-art machine learning approach basedon decision plans.
The algorithm defines the besthyper-plan which separates set of points associatedwith different class labels with a maximum-margin.The unlabeled examples are then classified by de-ciding in which side of the hyper-surface they re-side.
The hyper-plan can be a simple linear plan asfirst proposed by Vapnik, or a non-linear plan suchas e.g.
polynomial, radial, or sigmoid.
In our eval-uation we used the linear kernel since it was provedto be as powerful as the other kernels when tested ontext classification data sets (Yang and Liu, 1999).4.3 Data SetsIn our experiments we use Reuters-21578,WebKB, 20Newsgroups, and LingSpamdatasets.
These datasets are commonly used for textclassification evaluations (Joachims, 1996; Cravenet al, 1998; Androutsopoulos et al, 2000; Mihalceaand Hassan, 2005).Reuter-21578.
This is a publicly available subset ofthe Reuters news, containing about 120 categories.We use the standard ModApte data split (Apte etal., 1994).
The unlabeled documents were discardedand only the documents with one or more class la-bels were used in the classification experiments.WebKB.
This is a data set collected from com-puter science departments of various universities bythe CMU text learning group.
The dataset containsseven class labels which are Project, Student, De-partment, Faculty, Staff, Course, and Other.
TheOther label was removed from the dataset for evalu-ation purposes.
Most of the evaluations in the liter-ature have been performed on only four of the cate-gories (Project, Student, Faculty, and Course) sincethey represent the largest categories.
However, sincewe wanted to see how our system behaves when onlya few training examples were available as e.g.
in theStaff and the Department classes, we performed ourevaluations on two versions of WebKB: one withthe four categories version (WebKB4) and one withthe six categories (WebKB6).20-Newsgroups.
This is a collection of 20,000 mes-sages from 20 different newsgroups, correspondingto different topics or subjects.
Each newsgroup hasabout 1000 message split into 400 test and 600 traindocuments.LingSpam.
This is a spam corpus, consisting ofemail messages organized in 10 collections to al-57low for 10-fold cross validation.
Each collection hasroughly 300 spam and legitimate messages.
Thereare four versions of the corpus standing for bare,stop-word filtered, lemmatized, and stop-word andlemmatized.
We use the bare collection with a stan-dard 10-fold cross validation.4.4 Performance MeasuresTo evaluate the classification system we used the tra-ditional accuracy measure defined as the number ofcorrect predictions divided with the number of eval-uated examples.We also use the correlation coefficient (?)
asa diversity measure to evaluate the dissimilaritybetween the weighting models.
Pairwise diver-sity measures have been traditionally used to mea-sure the statistical independence among ensemble ofclassifiers (Kuncheva and Whitaker, 2003).
Here,we use them to measure the correlation between ourrandom-walk approach and the traditional term fre-quency approach.
The typical setting in which thepairwise diversity measures are used is a set of dif-ferent classifiers which are used to classify the sameset of feature vectors or documents over a givendataset.
In our evaluation we use the same classifierto evaluate two different sets of feature vectors thatare produced by different weighting features: the rwrandom walk weighting, and the tf term frequencyweighting.
Since the two feature vector collectionsare evaluated by one classifier at a time, the resulteddiversity scores will reflect the diversity of the twosystems.Let Di and Dj be two feature weighting modelswith the following contingency table.Dj correct=Y Dj correct=NDi correct=Y a bDi correct=N c dTable 2: Di & Dj Contingency tableThe correlation coefficient (?)
is defined as:?ij =ad ?
bc?
(a + b)(c + d)(a + c)(b + d)5The symbol ?indicates a statistically significant result usingTable 3: Naive Bayes Results5N.B.
tf rw2 rw4 rw6 rw8WebKB4 81.9 81.9 82.8 82.7 81.2WebKB6 71.7 73.0 74.2?
74.4?
73.5Reuter 83.2 82.5 82.9 83.0 82.820NG 81.7 82.0 82.3?
82.3?
82.1?LSpam 99.3 99.4 99.3 99.3 99.3Table 4: Rocchio ResultsROC tf rw2 rw4 rw6 rw8WebKB4 71.9 77.5?
78.6?
80.8?
80.9?WebKB6 58.3 69.6?
72.0?
76.5?
76.2?Reuter 78.2 80.8?
81.1?
81.0?
81.4?20NG 76.2 77.3?
77.1?
77.2?
77.4?LSpam 97.5 97.8 97.8 97.7 97.85 Evaluation and DiscussionTables 3, 4, 5, 6 show the classification results forWebKB4, WebKB6, LingSpam, Reuter, and20Newsgroups respectively.
The rw2, rw4, rw6,and rw8 represent the accuracies achieved usingrandom-walk weighting under window sizes of 2,4, 6, and 8 respectively.
The tf column representsthe results obtained with a term frequency weightingscheme.By examining the results we can see that therw.idf model outperforms the tf.idf model on allthe classifiers and datasets with only one excep-tion in the case of a Na?
?ve Bayes classifier underReuter.
The error reductions range from 3.5% as in{20Newsgroups, NaiveBayes, rw4} to 44% as inthe case of {WebKB6, Rocchio, rw6}.
The systemgives, in its worst performance, a comparable resultto the tf.idf baseline.
The system shows a consis-tent performance with different window sizes, withno clear cut window size that would give the bestresult.
By further analyzing the results using statis-tical paired t-tests we can see that windows of size4 and 6 supply the most significant results across allthe classifiers as well as the datasets.Comparing WebKB4 and WebKB6 fine-grainedresults, we found that both systems failed to pre-dict the class Staff; however the significant improve-a paired t-test, with p < 0.05.
The result is marked by ?
whenp < 0.001.58Table 5: KNN ResultsKNN tf rw2 rw4 rw6 rw8WebKB4 59.2 68.6?
67.0?
64.6?
66.6?WebKB6 55.8 63.7?
55.8 59.9?
61.0?Reuter 73.6 76.9?
78.1?
78.5?
78.5?20NG 70.3 76.1?
76.5?
77.2?
77.8?LSpam 97.5 97.8 97.8 98.1?
97.9Table 6: SVM ResultsSVM tf rw2 rw4 rw6 rw8WebKB4 87.7 87.9 87.9 89?
88.5WebKB6 82.5 84.5?
85.2?
85.2?
84.6?Reuter 83.2 84.5?
84.4?
84.6?
84.1?20NG 95.2 95.5?
95.6?
95.6?
95.4?LSpam 95.6 96.4?
96.4?
96.2?
96.3?ment was over the class Department, in which ourrw model scores an accuracy of 47% compared to4% in using tf.idf .
This indicates how successfulrw.idf model is in cases where there are few train-ing examples.
This could be due to the ability of themodel to extract more realistic and smoother distri-bution of terms as seen in the rw curve plotted inFigure 3, hence reducing the feature bias imposedby the limited number of training examples.Table 7: Naive Bayes Correlation ?N.B.
rw2 rw4 rw6 rw8WebKB4 0.68 0.70 0.70 0.66WebKB6 0.71 0.71 0.71 0.65Reuter 0.86 0.87 0.87 0.8520NG 0.82 0.84 0.83 0.82LSpam 0.89 0.89 0.92 0.92By also examining the diversity of the classifi-cation systems based on rw and tf weighting, asshown in Table 7, 8, 9, 10, we can see an inter-esting property of the system.
The two models aregenerally more diverse and less correlated when us-ing windows of size 6 and 8 than using windows ofsize 2 and 4.
This could be due to the increasingdrift from the feature independence assumption thatis implied by tf.idf .
However increasing the depen-dency is not always desirable as seen in the reportedaccuracies.
We expect that at a certain window sizethe system performance will degrade to tf.idf .
ThisTable 8: Rocchio Correlation ?ROC rw2 rw4 rw6 rw8WebKB4 0.49 0.51 0.53 0.54WebKB6 0.40 0.40 0.41 0.42Reuter 0.75 0.77 0.75 0.7120NG 0.77 0.77 0.77 0.77LSpam 0.82 0.85 0.81 0.78Table 9: KNN Correlation ?KNN rw2 rw4 rw6 rw8WebKB4 0.35 0.32 0.36 0.37WebKB6 0.35 0.35 0.37 0.37Reuter 0.74 0.70 0.68 0.6720NG 0.62 0.64 0.63 0.59LSpam 0.66 0.69 0.63 0.57threshold window size will be equal to the documentsize.
In such a case each term will depend on all theremaining terms resulting in an almost completelyconnected graph.
Consequently, each feature contri-bution to the surrounding will be equal resulting insimilar rw scores to all the features.6 Conclusions and Future WorkBased on results obtained in text classification ex-periments, the TextRank random-walk model toterm weighting was found to achieve error rate re-ductions of 3.5?44% as compared to the traditionalfrequency-based approach.
The evaluation resultshave shown that the system performance varies de-pending on window size, dataset, as well as classi-fier, with the greatest boost in performance recordedfor KNN ,Rocchio, and SVM.
We believe that theseresults support our claim that random-walk modelscan accurately estimate term weights, and can beused as a technique to model the probabilistic dis-tribution of features in a document.The evaluations reported in this paper has shownthat the TextRank model can accurately provide uni-gram probabilities for a sequence of words.
In futurework we will try to extend the TextRank model anduse it to define a formal language model in whichwe can estimate the probability of entire sequencesof words (n-grams).59Table 10: SVM Correlation ?SVM rw2 rw4 rw6 rw8WebKB4 0.73 0.77 0.78 0.82WebKB6 0.73 0.76 0.78 0.80Reuter 0.80 0.83 0.82 0.8220NG 0.80 0.78 0.82 0.83LSpam 0.86 0.88 0.88 0.89ReferencesI.
Androutsopoulos, J. Koutsias, K. V. Chandrinos,G.
Paliouras, and C. D. Spyropoulos.
2000.
An eval-uation of naive bayesian anti-spam filtering.
In Pro-ceedings of the workshop on Machine Learning in theNew Information Age.C.
Apte, F. Damerau, and S. M. Weiss.
1994.
Towardslanguage independent automated learning of text cat-egorisation models.
In Proceedings of the 17th ACMSIGIR Conference on Research and Development inInformation Retrieval.L.
Bahl, F. Jelinek, and R. Mercer.
1983.
A maximumlikelihood approach to continuous speech recognition.IEEE Transactions on Pattern Analysis and MachineIntelligence, 5(2).S.
Brin and L. Page.
1998.
The anatomy of a large-scalehypertextual Web search engine.
Computer Networksand ISDN Systems, 30(1?7).M.
Craven, D. DiPasquo, D. Freitag, A. McCallum,T.
Mitchell, K. Nigam, and S. Slattery.
1998.
Learn-ing to extract symbolic knowledge from the WorldWide Web.
In Proceedings of the 15th Conference ofthe American Association for Artificial Intelligence.B.
Dom, I. Eiron, A. Cozzi, and Y. Shang.
2003.
Graph-based ranking algorithms for e-mail expertise analysis.In Proceedings of the 8th ACM SIGMOD workshop onResearch issues in data mining and knowledge discov-ery, San Diego, California.G.
Erkan and D. Radev.
2004.
Lexpagerank: Prestige inmulti-document text summarization.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, Barcelona, Spain, July.G.
Grimmett and D. Stirzaker.
1989.
Probability andRandom Processes.
Oxford University Press.P.J.
Herings, G. van der Laan, and D. Talman.
2001.Measuring the power of nodes in digraphs.
Technicalreport, Tinbergen Institute.T.
Joachims.
1996.
A probabilistic analysis of the roc-chio algorithm with tf.idf for text categorization.
InProceedings of the 14th International Conference onMachine Learning.T.
Joachims.
1997.
A probabilistic analysis of the Roc-chio algorithm with TFIDF for text categorization.
InProceedings of ICML-97, 14th International Confer-ence on Machine Learning, Nashville, US.L.
Kuncheva and C. Whitaker.
2003.
Measures of diver-sity in classifier ensembles and their relationship withthe ensemble accuracy.
Machine Learning, 51.I.
Makoto and T. Takenobu.
1995.
Cluster-based text cat-egorization: A comparison of category search starte-gies.
In Proceedings of the 18th ACM InternationalConference on Research and Development in Informa-tion Retrieval.B.
Masand, G. Linoff, and D. Waltz.
1992.
Classify-ing news stories using memory based reasoning.
InProceedings of the 15th International Conference onResearch and Development in information Retrieval.A.
McCallum and K. Nigam.
1998.
A comparison ofevent models for Naive Bayes text classification.
InProceedings of AAAI-98 Workshop on Learning forText Categorization.R.
Mihalcea and S. Hassan.
2005.
Using the essence oftexts to improve document classification.
In Proceed-ings of the Conference on Recent Advances in NaturalLanguage Processing (RANLP), Borovetz, Bulgaria.R.
Mihalcea and P. Tarau.
2004.
TextRank ?
bringingorder into texts.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP 2004), Barcelona, Spain.A.
Moschitti.
2003.
A study on optimal paramter tun-ing for Rocchio text classifier.
In Proceedings of theEuropean Conference on Information Retrieval, Pisa,Italy.R.
Robertson and K. Sparck Jones.
1997.
Simple, provenapproaches to text retrieval.
Technical report.J.
Rocchio, 1971.
Relevance feedback in information re-trieval.
Prentice Hall, Ing.
Englewood Cliffs, New Jer-sey.K.
Schneider.
2004.
A new feature selection score formultinomial naive bayes text classification based onkl-divergence.
In The Companion Volume to the Pro-ceedings of 42st Annual Meeting of the Association forComputational Linguistics, Barcelona, Spain, July.H.
Schutze, D. A.
Hull, and J. O. Pedersen.
1995.
Acomparison of classifiers and document representa-tions for the routing problem.
In Proceedings of the18th annual international ACM SIGIR conference onResearch and development in information retrieval,Seattle, Washington.V.
Vapnik.
1995.
The Nature of Statistical Learning The-ory.
Springer, New York.Y.
Yang and X. Liu.
1999.
A reexamination of text cate-gorization methods.
In Proceedings of the 22nd ACMSIGIR Conference on Research and Development inInformation Retrieval.Y.
Yang and J. O. Pedersen.
1997.
A comparative studyon feature selection in text categorization.
In Proceed-ings of the 14th International Conference on MachineLearning, Nashville, US.60
