Mot ivat ions and Methods  tbr Text Simpli f icat ionR.
Chandrasekar*  Chr ist ine Doran B. SrinivasInstitute for Research in l)el)artm<;nt of Deparl;mcnt ofCognitive, Science & (\]cntcr for \[,inguistics (\]Oml)uter $?the Advanced Study of hldia InlbrHlatioll Scienc(;University ot7 Pcnnsylwmia, lqfiladclphia, PA 19104{ra?ckeyc, doran, sr in?
}Ol?nc.
c i s .
upenn, eduAbst ractLottg alld eolni)licated seltteltces prov(: to b(:a. stumbling block for current systems rely-ing on N\[, input.
These systenls stand togaill frolil ntethods that syntacti<:aHy sim-plily su<:h sentences.
'\]b simplify a sen=tence, we nee<t an idea of tit(."
structure ofthe sentence, to identify the <:omponents obe separated out.
Obviously a parser couhlbe used to obtain the complete structure ofthe sentence.
\]\[owever, hill parsing is slowa+nd i)rone to fa.ilure, especially on <:omph!xsentences.
In this l)aper, we consider twoalternatives to fu\]l parsing which could beuse<l for simplification.
The tirst al)l)roachuses a Finite State Grammar (FSG) to pro-dn<:e noun and verb groups while the seconduses a Superta.gging model to i)roduce de-pendency linkages.
We discuss the impact ofthese two input representations on the sim-plification pro(:ess.1 Reasons  fo r  Text  S impl i f i ca t ionl ,ong and <:oml)licatcd sentences prove to be as tuml J ing  block for <'urrent systems which rely onnatural  language input.
'l'lmsc systems stand togain from metho<ls that  preprocess uch sentencesso as to make them simpler.
Consider, for exam-ph;, the following sentence:( l )  7'he embattled Major government survived acrucial 'vole on coal pits closure as itslast-minute concessions curbed the extent of' lbry revolt over an issue that generatedu'ausual heat in the l\]ousc of Commons andbrought the miners to London streets.Such sentences are not uncommon in newswiretexts.
( \ ]ompare this with the mult i -sentence ver-sion which has been manual ly  simplif ied:(2) The embatlled Major governmcnl survived acrucial vote o'u coal pits closure.
Itslast:minute conccssious curbed the cxlenl o\]"*On leave fl'om the National Centre for Soft, wareTechno\]ogy, (lulmohar (?ross Road No.
9, Juhu,Bombay 4:0(/ (149, IndiaTory revolt over the coal-miue issue.
Th.isissue generaled unusual heat in the ltousc ofCommons.
II also brought the miners toLondon streels.If coml>lex text can be made simph'x, sen-ten(-es beconae easier to process, both for In:O-grams and humans.
Wc discuss a simplif ica-tion process which identifies components of a sen-tence that  may be separated out, and transformseach of these into f rec-sta ,d ing s impler sentences.
(\]learly, some mmnees of meaning from the origi-nal text may be lost in the simpli f ication process.Simplit ication is theretbre inappropr iate  for texts(such as legal docunlents) where it is importa.ntnot to lose any nuance.
I|owew;r, one c.~tl\] COil-ceive of several areas of natura l  language process-ing where such simplit ication would be of greatuse.
This is especially true in dolnains uch as Ina-chine translat ion,  which commonly  have a manualpost-processing stage, where semantic  and prag-mat ic  repairs may be <'arried out if ne<;essary.?
Parsing: Syntact ical ly  <:omplex sentence's arclikely to generate a large number of parses,and may cause parsers to fail altogether.
Re-solving ambiguit ies in a t tachment  of con-st i tuents is non-tr ivial .
This ambiguii, y is re-duced for simpler sentences in<'e they involvefewer constituents.
'Fhus s impler sentenceslead to faster parsing and less parse aml)igu-ity.
Once the i>arses for the s impler sentencesare obtained, the subparses can be assembledto form a full parse, or left as is, dependingon the appl icat ion.?
Machine Translat ion (MT): As in the pars-ing case, s impli f ication results in s impler scn-tential  structures and reduced ambiguity.
Asargued in (Chandrasekar,  1994), this conldlead to improvements in the qual ity of ma-chine translat ion.?
In format ion Retrieval: IR systems usual ly re-trieve large segments  of texts of which only apart  n\]ay bc reh~'wml,.
Wit | ,  simplif ied texts,it is possible to extract  Sl>eCific phrases orsimple sentences of relevance in response toqueries.1041?
Summarization: With the overload of infor-mation that people face today, it would bevery helpful to have text summarization toolsthat; reduce large bodies of text to the salientminimum.
Simplification can be used to weedout irrelevant ext with greater precision, andthus aid in summarization.?
Clarity of Text: Assembly/use/maintenancemanuals must be clear and simple to follow.Aircraft companies use a Simplified Englishfor maintenance manuals precisely for thisreason (Wojcik et M., 1993).
However, itis not easy to create text in such an artifi-cially constrained language.
Automatic (orsemi-automatic ) simplification could be usedto ensure that texts adhere to standards.We view simplification as a two stage process.The first stage provides a structural representa-tion for a sentence on which the second stage ap-plies a sequence of rules to identify and extract hecomponents that can be simplified.
One could usea parser to obtain the complete structure of thesentence.
If all the constituents of the sentencealong with the dependency relations are given,simplification is straightforward, ttowever, fullparsing is slow and prone to failure, especially oncomplex sentences.
To overcome the limitations offull parsers, researchers have adopted FSG basedapproaches to parsing (Abney, 1994; Hobbs et al,1992; Grishman, 1995).
These parsers are fastand reasonably robust; they produce sequencesof noun and verb groups without any hierarchicalstructure.
Section 3 discusses an FSG based ap-proach to simplification.
An alternative approachwhich is both fast and yields hierarchical struc-ture is discussed in Section 4.
In Section 5 wecompare the two approaches, and address somegeneral concerns for the simplification task in Sec-tion 6.2 The Basics of SimplificationText simplification uses the f~ct that complextexts typically contains complex syntax, some ofwhich may be particular to specific domain of dis-course, such as newswire texts.
We assume thatthe simplification system will process one sentenceat a time.
Interactions across sentences will not bcconsidered.
Wc also assume that sentences haveto be maximally simplified.2'o simplify sentences, we nced to know wherewe can split them.
We define articulation-pointsto be those points at which sentences may be log-ically split.
Possible articulation points includethe beginnings and ends of phrases, punctuationmarks, subordinating and coordinating conjunc-tions, and relative pronouns.
These articulationpoints are gcneral, and should apply to arbitraryEnglish texts.
These may, however, be augmentedwith domain-specific articulation points.
We canuse these articulation-points to define a set of ruleswhich map froln given sentence patterns to sim-pler sentences patterns.
These rules are repeat -edly applied on each sentence until they do notapply any more.
For example, the sentence (3)with a relative clause can be simplified into twosentences (4).
(3) Talwinder Singh, who masterminded theKanishka crash in 198~, was killed in afierce lwo-honr e~.connter...(4) Talwindcr Singh was killed in a .fiercetwo-hoar cncounler ... Talwinder Siughmasterminded the Kanishka crash in 198~.3 FSG based Simplification(Chandrasekar, 1994) discusses an approach thatuses a FSG for text simplification as part ofa machine aided translation prototype namedVaakya.
In this approach, we consider sentencesto be composed of sequence of word groups, orchunks.
Chunk boundaries are regarded as poten-tial articulation-points.
Chunking allows us to de-fine the syntax of a sentence and the structure ofsimplification rules at a coarser granularity, sincewe need no longer be concerned with the internalstructure of the chunks.In this approach, we first tag each word with itspart-of-speech.
Chunks are then identified nsing aFSG.
Each chunk is a word group consisting of averb phrase or a noun phrase, with some attachedmodifiers.
The noun phrase recognizer also marksthe number (singular/plural) of the phrase.
Theverb phrase recognizer provides some informationon tense, voice and aspect.
Chunks identified bythis mechanism include phrases uch as the extentof Tory ~evolt and have recently bcen finalizcd.The chunked sentences are then simplified usinga set of ordered simplification rules.
The orderi~gof the rules is decided manually, to take care ofmore frequent ransformations first, and to avoidunproductive rule interaction.
An example rulethat simplifies sentences with a relative pronounis shown in (5).
(5) X:tiP, ReXPron Y, Z --* X:tiP Z. X:tiP Y.The rule is interpreted as follows.
If a sentencestarts with a noun phrase (X:tiP), and is followedby a phrase with a relative pronoun, of the \['orm( , l%elPron Y ,) followed by soIne (Z), whereY and Z are arbitrary sequences of words, thenthe sentence may be simplified into two sentences,namely the sequence (X) followed by (Z), and (X)followed by (Y).
The resulting sen\];ences are thenrecursively simplified, to the extent possible.The system has been tested on news text, andperforms well on certain classes of sentences.
See(Chandrasekar and R, amani, 1996) ibr details ofquantitative valuation of the system, includingan evaluation of the acceptability of the resulting1042sentences.
A set of news stories, consist, ing of 224sentences, was simplitied by the prototype system,resulting in 369 simplified sentences.Ilowever, there are certain weMenesses in thissystem, caused mostly by the relatively simplemechanisms used to detect phrases and attach-meats.
Sentences which include long distanceor crossed del)enden('ies, and sentences whichhave malt|ply stacked appositives are not handledllrOl)erly; nor are sentences with atnbiguous or un-ch'.ar attachnwnts.
Some of these prol)\]oms can behandh'd I)y augmenting the ruh' set but what isi'eally require(I is ntorc syntactic firel)ower.4 A Dependency-based modelA second a.I)l)roaeh to simplification is to useri(:her syntactic in\[brmation, in terms of both con-stituency inlbrmation and dependency inf'orma-tion.
We use partial parsing and simple depen-.dency attachment techniques as an alternative tothe FSG I)ased simpliiication.
This ~no(M (theI)SM) is based on a sinq)le dependency tel)r(>sentation provided l)y I,exicalized Tree.
Adjoining(Ira.tmnar (I/FAG) and uses the "SUl>ertaggiug"l;echniques described in (Josh| and Srinivas, 1994).4.1 Br ie f  Ovt;rvlt;w of LTAGsThe primitive elements el LTA(~ formalism are ('.l-( : lnentary  trees.
Elementary trees are of twotypes: initial frees and au,iliary trees.
Initial/;rees are minimal linguistic structures that con-tain no recurs|on, such as sitnph; sentences, N Ps,l)Ps etc.
Auxiliary trees are recursive stru<-turcswhich represent constituents that arc adjuncts tobasic structure (e.g.
relative clauses, sententialadjuncts, a(Iw'.rbials).
For a more R)rmal and (le-taile(I (lescription of l,'l'A(\]s see (Schabes et M.,J988).4.2 SuI)(*xl;aggingTlte elemmttary trees of LTAG localize dependen-(-ies, including hmg distance dependencies, by re-quiring that all and only the dependent elementsbe present within the same tree.
As a result ofthis localization, a lexical item may be (and al-most alwws is) associated with more than one eL-ementary tree, We call these elementary trees su-pcrlags, since they conttdn more information (suchas sul)categorization a d agreement information)than standard part -of  speech tags.
Henc.e, eachword is associated with more than one supertag.At the end of a complete l)arse, each word is asso-ciated with just one supertag (assuming there isno global ambiguity), and the supertags of all thewords in a sentence are combined by sul)stitutionand adjunct|on.As in standard part-of-speech disambiguation,we can use local statistical information in the formof N-gram models based on the distribution of sn-l)ertags ill a LTAG parsed corl)us for disamhigua-tion.
We.
use a trigram model to disambiguate ilesupcrtags o as to assign one SUl)ertag tbr eachword, in a process termed supertagging.
'\['he tri-gram model of supcrtagging is very efficient (inlinear time) and robust (Josh| and Srinivas, \] 994).
'1'o establish the dependency links among thewords of the sentence, we ('xph)it the dei)endencyinformation present in the supertags.
Each su-perl;ag associated with a word allocates lots forthe arguments o1' the word.
These slots have apolarity value re\[lecting their orientation wii;h re-Sl)ect to the anchor o\[' the SUl)ertag.
Also asso-('iated with a supertag is a list of internal nodes(hmluding the root node) thai, appear in the su-pertag.
Using I;his information, a simple algo-rithnt may be used to annotate the sentence withd(,pe.ndency links.4.3 Simpl i f i cat ion  w i th  DeI ) (mden( 'y  l inksTlte output provide(\[ by t, he dellendency analyzernot only contains depen(hmcy links annmg wordsbut also in(lical,cs the constituent strncture as cn-code(I by snpertags.
The constituent informationis used to identify whether a supertag contains aclausal constituent and the dependency links areused to identify the span of the clause.
Thus,embedded clauses can easily be identified and ex-tracte(t, akmg with their arguments.
\])nnctuationcan be used to identify constituents such as appos-itives which can also 1)e sel)arate(I ont.
As withthe finite-state al)l)roach, the resulting segmentsmay 1)e incomplete as indellt'ndetlt clauses.
I\[' thesegments are to I)e reassembh'd, no further pro-cessing need be done on them.l?igm'e 1 shows a rule \[br extracting relative('lauses, in dependency notation.
We tits| iden-tify the relative clause tree (Z), and then extractthe verb which anchors it along with all of its (te-pendents.
The right hand side shows the two re-suiting trees.
The gap in the relative clause (Y)need only be tilled if the clauses are not going tobc reconlbined.
Examples (6) and (7) show a sen-tence belbre and after this rule has applied.X:SY:NP WZ: RelClause=>yZ':S:NP Y:NP W/Figure 1: R,ule for extracting relative clauses(6) .
.
.
an issue \[that generated  unnsnal heatin the IIouse of Commons \] .
.
.
(7) An issne \ [generated  unusnal heat in theIlouse of Commons \].
The issue .
.
.10435 Evaluat ionThe objective of the evaluation is to examine theadvantages of the DSM over the FSG-based modelfor simplification.
In the FSG approach since theinput to the simplifier is a set of noun and verbgroups, the rules for the simplifier have to identifybasic predicate argument relations to ensure thatthe right chunks remain together in the output.The simplifier in the DSM has access to infor-mation about argument structure, which makesit much easier to specify simplification patternsinvolving complete constituents.
Consider exam-pie 8,(8) Th.e creator of Air India, Mr. JRD 7hta,believes that the airline, which celebrated 60years today, could return to its old days ofglory.q'he FSG-based model fails to recognize the rel-ative clause on the embedded subject the airlinein example (8), because Rule 5 looks for matrixsubject NPs.
On the other hand, the DSM cor-rectly identifies the relative clause using the ruleshown in Figure 1, which holds for relative clausesin all positions.Other differences are in the areas of modifier at-tachment and rule generality.
In contrast o the/)SM approach, the FSG output does not have allmodifiers attached, so the bulk of attachment de-cisions must be made by the simplification rules.The FSG approach is forced to enumerate all pos-sible variants of the LHS of each simplificationrule (eg.
Subject versus Object relatives, singularversus plural NPs) whereas in the DSM approach,the rules, encoded in supertags and the associatedconstituent types, are more general.Preliminary results using the DSM model arevery promising.
Using a corpus of newswire data,and only considering relative clause and apposi-tive simplification, we correctly recovered 25 outof 28 relative clauses and i4 of 14 appositives.
Wegenerated 1 spurious relative clause and 2 spuri-ous appositives.
A version of the FSG model onthe same data  recovered 17 relative clauses and 3appositives.6 Discuss ionSimplification can be used for two general (:lassesof tasks.
The first is as a preprocessor to a flfllparser so as to reduce \];he parse ambiguity for theparser.
Tile second class of tasks demands thatthe output of the simplifier be free-standing sen-tences.
Maintaining the coherence of the simpli-fied text raises the fbllowing problems:?
Determining the relative order of the simpli-fied sentences, which impacts the choice ofreferring expressions to be used and the over-all coherence of the text.?
Choosing referring expressions: For instance,when separating relative clauses fi'om thenouns they modify, copying the head nouninto the relative clause is simple, but leadsto quite awkward sounding texts.
IIowever,choosing an appropriate pronoun or choosingbetween definite and indefinite NPs involvesknowledge of complex discourse information.?
Selecting the right tense when creating newsentences presents imilar problems.?
No matter how sophisticated the simplifica-tion heuristics, the subtleties of meaning in-tended by the author may be diluted, if notlost altogether.
For many computer appli-cations, this disadvantage is outweighed bythe advantages of simplification (i.e.
gains ofspeed and/or accuracy), or may be correctedwith the use of human l)ost-processiug.AcknowledgementsThis work is partially supported by NSF grant NSF-STC SBR 8920230, ARPA grant N00014-94 and All.
()grant DAAH04-94-G0426.ReferencesSteven Abney.
1994.
I)epcndency Grammars andContext-Free Grammars.
Manuscript, Universityof Tubingen, March.R.
Chandrasekar and S. Ramani.
1996.
Auto-matic Simplifica.tion of Natural Language Text.M~muscript, National Centre for So\[tware Technol:ogy, Bombay.R.
Chandrasekar.
1994.
A Hybrid Approach to Ma-chine Translation using Man Machine Communica-tion.
Ph.D. thesis, 'Pata Institute of I"undamcnta\]Research/University of Bombay, Bombay.Ralph Grishman.
1995.
Where's the Syntax?
TheNew York University M UC-6 System.
\[n P~occe(gings of the Sixth Message Understanding Confer-ence, Columbia, Maryland.Jerry Hobbs, Doug Appelt, John Bear, \])avid Israel,and W. Mary Tyson.
11992.
FAST(IS: a system, forextracting information from natural anguage text.Technical Report 519, SRI.Aravind K. Joshi and B. Srinivas.
1994.
Disam-biguation of Super Parts of Speech (or Supertags):Almost Parsing.
In Proceedings of the 17 th Inter-national Conference on Computational Linguistics(COLING '94), Kyoto, Japan, August.Yves Schabes, Anne Abeilld, and Aravind K. Joshi.1988.
Parsing strategies with 'lexicMized' gram-mars: Application to Tree Adjoining Grammars.In Proceedings of the 12 th International Co@renccon Computational Linguistics (COL1NG'88), Bu-dapest, Ilungm:y, August.Richard II.
Wojcik, Philip tIarrison, rod John Bremer.1993.
Using bracketed parses to evMuate a grain-mar checking ;rpplication.
In Proceedings of the 31'~tConference of Association of Computational Lin-guistics, Ohio State University, Columbus, Ohio.1044
