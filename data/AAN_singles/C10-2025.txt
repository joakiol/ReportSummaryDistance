Coling 2010: Poster Volume, pages 214?222,Beijing, August 2010Hybrid Decoding: Decoding with Partial Hypotheses Combination overMultiple SMT Systems?Lei Cui?, Dongdong Zhang?, Mu Li?, Ming Zhou?, and Tiejun Zhao?
?School of Computer Science and TechnologyHarbin Institute of Technology{cuilei,tjzhao}@mtlab.hit.edu.cn?Microsoft Research Asia{dozhang,muli,mingzhou}@microsoft.comAbstractIn this paper, we present hybrid decod-ing ?
a novel statistical machine transla-tion (SMT) decoding paradigm using mul-tiple SMT systems.
In our work, in ad-dition to component SMT systems, sys-tem combination method is also employedin generating partial translation hypothe-ses throughout the decoding process, inwhich smaller hypotheses generated byeach component decoder and hypothesescombination are used in the following de-coding steps to generate larger hypothe-ses.
Experimental results on NIST evalu-ation data sets for Chinese-to-English ma-chine translation (MT) task show that ourmethod can not only achieve significantimprovements over individual decoders,but also bring substantial gains comparedwith a state-of-the-art word-level systemcombination method.1 IntroductionIn recent years, system combination for SMT hasbeen known to be quite effective with translationconsensus information built from multiple SMTsystems.
The combination approaches can beclassified into two types.
One is the combinationwith each system?s outputs, which can be seen asfull hypotheses combination.
The other is the par-tial hypotheses (PHS) combination during the de-coding phase.A lot of impressive work has been done to im-prove the performance of the SMT systems by uti-?This work has been done while the first author was vis-iting Microsoft Research Asia.lizing consensus statistics which come from sin-gle system or multiple systems.
For example,Minimum Bayes Risk (MBR) (Kumar and Byrne,2004) decoding over n-best list finds a translationthat has lowest expected loss with all the other hy-potheses, and it shows that improvement over theMaximum a Posteriori (MAP) decoding.
Severalword-based methods (Rosti et al, 2007a; Sim etal., 2007) have also been proposed.
Usually, thesemethods take n-best list from different SMT sys-tems as inputs, and construct a confusion networkfor second-pass decoding.
There are also a lot ofresearch work to advance the confusion networkconstruction by finding better alignment betweenthe skeleton and the other hypotheses (He et al,2008; Ayan et al, 2008).
Typically, all the ap-proaches above only use full hypotheses but haveno access to the PHS information.Moreover, some dedicated efforts have beentried by manipulating PHS between multiple MTsystems.
Collaborative decoding (co-decoding)(Li et al, 2009) leverages translation consensusby exchanging partial translation results and re-ranking both full and partial hypotheses exploredin decoding.
However, no new PHS are generatedcompared to the individual decoding but only theranking is affected.
Liu et al (2009) proposesjoint decoding, a method that integrates multipletranslation models in one decoder.
Although jointdecoding is able to generate new translations com-pared to single decoder, it has to use the PHSexisted in one of its component decoder at eachstep.
Different from their work, we propose anew perspective which leverages outputs from lo-cal word-level combination.
This will potentiallybring much benefit of performance since word-214level combination can produce more promisingPHS.The word-level system combination method isemployed to generate partial translation hypothe-ses in our hybrid decoding framework.
In thissense, full hypotheses word-level combination(FH-Comb) method(Rosti et al, 2007a; Sim et al,2007; He et al, 2008; Ayan et al, 2008) can beconsidered as a special case of hybrid decoding,where their combinations are only performed onthe largest hypotheses.
Similar with FH-Comb,hybrid decoding also uses word alignment infor-mation.
However, challenge exists in hybrid de-coding as word alignment needs to be carefullyconducted through the decoding process.
Obvi-ously, document-level word alignment methodssuch as GIZA++(Och and Ney, 2000) are quitetime consuming and unpractical to be embeddedinto hybrid decoding.
We propose a heuristicmethod that can conduct word alignment of par-tial hypotheses based on word alignment informa-tion of phrase pairs learnt automatically from themodel training process.
In this way, more PHS aregenerated and the search space is enlarged sub-stantially, which brings better translation results.The rest of the paper is organized as follows:Section 2 gives a formal description of hybriddecoding, including framework overview, word-level PHS combination and parameter estimation.We conduct experiments with different settingsand make comparison between our method andbaseline, as well as a state-of-the-art word-levelsystem combination method in Section 3.
Exper-imental results discussion is presented in Section4.
Section 5 concludes the paper.2 Hybrid Decoding2.1 OverviewDifferent system combination methods (Li et al,2009; Liu et al, 2009) offer different frameworksto coordinate multiple SMT decoders.
Hybrid de-coding provides a new scheme to organize mul-tiple decoders to work synchronously.
As thedecoding algorithms may differ in multiple de-coders1, hybrid decoding has some difficulty in1In the SMT area, some decoders use left-right decod-ing to generate the hypothesis and?Pharaoh?
(Koehn et al,integrating different decoding algorithms.
With-out loss of generality, we assume that bottom-upCKY-based decoding is adopted in each individ-ual decoder, which is the same as co-decoding(Li et al, 2009) and joint decoding (Liu et al,2009).
Hybrid decoding collects n-best PHS of asource span2 from multiple decoders, then resultsfrom word-level PHS combination of that span aregiven back to each decoder, mixed with the origi-nal PHS.
After that, we re-rank the hybrid list andcontinue the decoding.
In an example with twodecoders, parts of the whole decoding process areillustrated in Figure 1 and can be summarized asfollows:3-5 6-6 3-4 5-63-6 3-61-2 3-6 1-21-6 1-61-6Decoder1 Decoder2Local decoding layerLocal decoding layerLocal decoding layerLocal combination layerLocal combination layer3-6 3-61-6 1-6Figure 1: Hybrid decoding with two decoders,where the string ?s-e?means the source spanstarts from position s and ends at position e. Theblank rectangles represent the n-best partial trans-lations of each decoder, and the shaded rectan-gles illustrate the n-best local combination out-puts.
The ovals denote bottom-up CKY-based de-coding results.2003) is one of them, while others adopt bottom-up decodingwhich is represented by?Hiero?
(Chiang, 2007).2The word ?span?is used to represent translation unitin CKY-based decoders, which denotes one or more consec-utive words in the source sentence.2151.
Individual decoding.
Each individual de-coder should maintain the n-best PHS ofeach span from the bottom.
After all the in-dividual decoders finish translating the samespan, they feed their own partial translationsinto a public container which can be used forword-level PHS combination, then get backthe partial combination outputs for step 3.2.
Local word-level combination.
After fedwith PHS from multiple decoders, a confu-sion network is built and word-level combi-nation for PHS is conducted.
The obtainednew partial translations are given back toeach individual decoder to continue the de-coding.3.
Mix new PHS with the original ones.
Thespan in each individual decoder will receivethe corresponding new PHS from the localcombination outputs.
The feature space ofthe new PHS is not exactly the same with thatof the original ones.
It has to be mapped insome way then the mixed hypotheses are re-ranked.In the following sub-sections, we first presentthe background of word-level combination forPHS, then introduce hybrid decoding algorithm indetail, as well as the feature definition and param-eter estimation.2.2 Word-Level Combination for PartialHypothesesMost word-level system combination methods arebased on confusion network decoding.
In con-fusion network construction, one hypothesis hasto be selected as the skeleton which determinesthe word order of the combination results.
Otherhypotheses are aligned against the skeleton.
Ei-ther votes or some word confidence scores are as-signed to each word in the network.Most of the research on confusion network con-struction focuses on seeking better word align-ment between the skeleton and the other hypothe-ses.
So far, several word alignment procedures areused for SMT system combination, which mainlyare GIZA++ alignments (Matusov et al, 2006),TER alignments (Sim et al, 2007) and IHMM?
?||| political ||| 0-0???
?||| political and economic ||| 0-0 1-2?
?||| economic ||| 0-0???
?||| economic interests ||| 0-0 1-1??
[X1] ||| political and [X1] ||| 0-0 1-2Figure 2: The example of translation alignmentfrom phrase-table and rule-tablealignments (He et al, 2008).
Similar with generalword-level system combination method, word-level PHS combination also uses word alignmentinformation.
However, in hybrid decoding, it isquite time-consuming and impractical to conductword alignment like GIZA++ for each span.
For-tunately, unit hypotheses word alignment can beobtained from the model training process, whichis shown in Figure 2.
We devise a heuristicapproach for PHS alignment that leverages thetranslation derivations from the sub-phrases.
Thederivation information ultimately comes from thephrase table in phrase-based systems (Koehn etal., 2003; Xiong et al, 2006) or the rule table insyntactic-based systems (Chiang, 2007; Liu et al,2007; Galley et al, 2006).The derivation is built in a phrase-based sys-tem as follows.
For example, we have two phrasetranslations ???
?
||| our ||| 0-0 1-0?and?????
||| economic interests ||| 0-0 1-1?,where string ?m-n?means the mth word in thesource phrase is aligned to the nth word in the tar-get phrase.
When combining the two phrases forgenerating ???
?
??
??
?, we obtainthe translation hypothesis as ?our economic in-terests?and also integrate the alignment fragmentto get ?0-0 1-0 2-1 3-2?.
The case is similar insyntactic-based system for non-terminal substitu-tion, which we will not discuss further here.Next, we introduce the skeleton-to-hypothesisword alignment algorithm in detail.
With thetranslation derivations, the skeleton-to-hypothesis(sk2hy) word alignment can be performed basedon the source-to-skeleton (so2sk) and source-to-hypothesis (so2hy) word alignment as they sharethe same source sentence.
The basic idea is toconstruct the sk2hy word alignment with the min-imum correspondence subsets (MCS).
A MCS isdefined as a triple < SK,HY, SO > where the216SK is the subset of skeleton words, HY is thesubset of the hypothesis words, and SO is theminimum source word set that all target words inboth SK and HY are aligned to.
Figure 3 showsthe algorithm for skeleton-to-hypothesis align-ment.
Most of the pseudo-code is self-explainedexcept for some subroutines, which are listed inTable 1.1: procedure SKEHYPALIGN(so2sk, so2hy)2: repeat3: Fetch out a source word to SO4: SO1 = SO2 = SO5: repeat6: SO=UNION(SO1, SO2)7: SK=GETALIGN(SO, so2sk)8: HY =GETALIGN(SO, so2hy)9: SO1=GETALIGN(SK, so2sk)10: SO2=GETALIGN(HY, so2hy)11: until |SO1| == |SO2| == |SO|12: simmax = ?infinity13: for all sk ?
SK do14: for all hy ?
HY do15: sim =SIM(sk, hy)16: if sim ?
simmax then17: simmax = sim18: skmax = sk19: hymax = hy20: end if21: end for22: end for23: ADDALIGN(skmax, hymax)24: until all the source words are fetched out25: end procedureFigure 3: Algorithm for skeleton-to-hypothesisalignmentSubroutines DescriptionUNION(A,B) the union of set A and set BGETALIGN(S,align) get the words aligned toS based on alignSIM(w1,w2) similarity between w1 and w2,we use edit distance hereADDALIGN(w1,w2) align w1 with w2Table 1: Description for subroutinesDue to the variety of the word order in n-best outputs, skeleton selection becomes essen-tial in confusion network construction.
The sim-plest way is to use the top-1 PHS from any indi-vidual decoder with the best performance undersome criteria.
However, this cannot always leadto better performance on some evaluation met-rics (Rosti et al, 2007a).
An alternative wouldbe MBR method with some loss function such asTER (Snover et al, 2006) or BLEU (Papineni etal., 2002).
We show the experimental results oftwo skeleton selection methods for PHS combina-tion in Section 3.2.3 Hybrid Decoding ModelFor a given source sentence f , any individual de-coder in hybrid decoding finds the best transla-tion e?
among the possible translation hypotheses?
(f) in terms of a ranking function F :e?
= argmaxe??
(f)F(e) (1)Suppose we have n individual decoders.
Theranking function Fn of the nth decoder can bewritten as:Fn(e) =m?i=1?n,ihn,i(f, e) (2)where each hn,i(f, e) is a feature function of thenth decoder, and ?n,i is the corresponding featureweight.
m is the number of features in each de-coder.The final result of hybrid decoder is the top-1 translation from the confusion network, whichis constructed on multiple decoders with the lastlayer?s output of CKY-based decoding.2.4 Hybrid Decoding AlgorithmThe hybrid decoder acts as a control unit whichcontrols the synchronization of multiple individ-ual decoders.
The algorithm is fully demonstratedin Figure 4.
The hybrid decoder pushes the samespan f ji to different decoders and gets back the n-best PHS (lines 2-6).
When the span?s length istoo small, both word alignment and partial com-bination results are not accurate.
We predefine afixed threshold ?
which is used for determining thestart-up of combination (line 7).
When the lengthcondition holds, the n-best PHS of each individual217decoder are stored in container G (lines 8).
Con-fusion network is constructed and new PHS can beextracted from it and are further mixed and sortedwith the original ones (lines 11-15).1: procedure HYBRIDDECODING(fn1 , D)2: for l?
1...n do3: for all i, j s.t.
j ?
i = l do4: G?
?5: for all d ?
D do6: nbest =DECODING(d, i, j)7: if j ?
i ?
?
then8: ADD(G,nbest)9: end if10: end for11: cn =CONNETBUILD(G)12: nbest?
=GETPARHYP(cn)13: for all d ?
D do14: MIXSORT(nbestd, nbest?
)15: end for16: end for17: end for18: end procedureFigure 4: Hybrid decoding algorithm2.5 Hybrid Decoding FeaturesNext we present the PHS word-level combinationfeature functions for hybrid decoding.
Following(Rosti et al, 2007b), four features are utilized tomodel the PHS as:Word Confidence Feature hwc(e)The word confidence feature is computed ashwc(e) =?ni=1 ?iciw, where n is the num-ber of the systems, ?i is the system confi-dence of system i, and ciw is the word confi-dence of word w in system i.Word Penalty Feature hwp(e)Word penalty feature is the number of wordsin the partial hypothesis (PH).Null Penalty Feature hnp(e)For null penalty feature, we mean the numberof NULL links along the PH when extractedfrom the confusion network.Language Model Feature hlm(e)Different from the above three combinationfeatures, which can be obtained during theconfusion network construction or hypothe-ses extraction, the language model featurecannot be summed up on the fly.
Instead,it must be re-computed when building eachnew PH.2.6 Feature Space MappingThe features used in hybrid decoding can be clas-sified into two categories: features for individualdecoders (FID) and features for PHS word-levelcombination (FComb), and they are independent.When mixing the new PHS with the original onesof individual decoders, FComb space has to bemapped to a FID space.
However, several featuresin FID are not defined in FComb, such as sourceto target (S2T) phrase probability, target to source(T2S) phrase probability, S2T lexical probability,T2S lexical probability and other model specificfeatures.
A mapping function H needs to be de-fined as follows:Ffid = H(Ffcomb) (3)where Ffcomb denotes the feature vector fromFComb space, while Ffid is the feature vectorfrom FID space.An easy mapping function is implemented withan intuitive motivation: PHS combination resultsare better than the ones in individual decoder andwe prefer not to disorder the original search space.Thus, the undefined feature values of PHS fromFComb space are assigned by corresponding fea-ture values of the top-1 PH in original decoder.Experiments show that our method is not onlypractical but also quite effective.2.7 Parameter EstimationMinimum Error Rate Training (MERT) (Och,2003) algorithm is adopted to estimate featureweights for hybrid decoding.
As hybrid decodermakes use of PHS from both individual decodersand combination results as a whole, we devisea new feature vector representation.
The featurevectors from FID space and FComb space are sim-ply concatenated to form a longer vector withoutoverlapping.
The weights are tuned simultane-ously in order to reach a relatively global optima.2183 Experiment3.1 Data and MetricWe conducted our experiments on the test dataof NIST 2005 and NIST 2006 Chinese-to-Englishmachine translation tasks.
The NIST 2003 testdata is used as the development data to tune theparameters.
Statistics of the data sets are shown inTable 2.
Translation performances are measuredwith case-insensitive BLEU4 score (Papineni etal., 2002).
Statistical significance test is per-formed using the bootstrap re-sampling methodproposed by Koehn (2004).The bilingual training corpora we used arelisted in Table 3, which contains 498K sentencepairs, 12.1M Chinese words and 13.8M Englishwords after pre-processing.
Word alignment isperformed by GIZA++ (Och and Ney, 2000) inboth directions with the default setting, and theintersect-diag-grow method is used to refine thesymmetric word alignment.Data Set # SentencesNIST 2003(dev) 919NIST 2005(test) 1,082NIST 2006(test) 1,664Table 2: Statistics of test/dev data setsLDC ID DescriptionLDC2003E07 Ch/En Treebank Par CorpusLDC2003E14 FBIS Multilanguage TextsLDC2005T06 Ch News Translation Text Part 1LDC2005T10 Ch/En News Magazine Par TextLDC2005E83 GALE Y1 Q1 Release - TranslationsLDC2006E26 GALE Y1 - En/Ch Par Financial NewsLDC2006E34 GALE Y1 Q2 Release - TranslationsV2.0LDC2006E85 GALE Y1 Q3 Release - TranslationsLDC2006E92 GALE Y1 Q4 Release - TranslationsTable 3: Training corpora for Chinese-EnglishtranslationThe language model used for hybrid decodingand all the baseline systems is a 5-gram modeltrained with the Xinhua portion of LDC EnglishGigaword Version 3.0 plus the English part ofbilingual training data.3.2 ImplementationWe use two baseline systems.
The first one(SYS1) is re-implementation of Hiero, a hi-erarchical phrase-based system (Chiang, 2007)based on Synchronous Context Free Grammar(SCFG).
Phrasal translation rules and hierarchi-cal translation rules with nonterminals are ex-tracted from all the bilingual sentence pairs.The second one (SYS2) is a phrase-based sys-tem (Xiong et al, 2006) based on BracketingTransduction Grammar (Wu, 1997) with a lex-icalized reordering model (Zhang et al, 2007)under maximum entropy framework, where thephrasal translation rules are exactly the samewith that of SYS1.
The lexicalized reorder-ing model is trained using the MaxEnt toolkit(Zhang, 2006) where the training instances areextracted from subset of the training corpora,which contains LDC2003E07, LDC2003E14,LDC2005T06, LDC2005T10.
Both systems usethe bottom-up CKY-based decoding with cube-pruning (Chiang, 2007) and the beam size is setto 10 for decoding efficiency.For hybrid decoder, we set ?
to besentence.length ?
3, meaning that the PHS ofindividual decoders only perform local combi-nation in the last three layers.
The reason whywe adopt this setting is because we find thatstarting local combination on short spans hurtsthe performance badly on test data.
Experimentalresults are shown in the next section.3.3 Translation ResultsWe present the overall results of hybrid decod-ing over two baseline systems on both test sets.We also implement an IHMM-based word-levelsystem combination method (He et al, 2008) tomake comparison with hybrid decoding system,and the n-best candidates used for IHMM-basedword-level system combination is set to 10.
Pa-rameters for all the systems are tuned on NIST2003 test set.
The results are shown in Table 4.In Table 4, we find that the hybrid decoding per-forms significantly better than SYS1 and SY2 onboth test sets.
Besides, compared to IHMM word-level system combination method, hybrid decod-ing also brings substantial gains with 0.63% and0.92% points respectively.219NIST 2005 NIST 2006SYS1 0.3745 0.3346SYS2 0.3699 0.3296IHMM Word-Comb 0.3821?
0.3421?Hybrid 0.3884?+ 0.3513?+Table 4: Hybrid decoding results on test sets,*:significantly better than SYS1 and SYS2 withp<0.01, +:significantly better than IHMM Word-Comb with p<0.01We also try different layers for determiningthe start-up of local word-level PHS combination.Figure 5 gives the intuitive BLEU results.0.320.330.340.350.360.370.380.390.4Figure 5: Performance of hybrid decoding withdifferent start-up settings on NIST 2005 test set,where the ?lastM?
means to conduct local word-level PHS combination in the last M layers fromthe perspective of CKY-based decoding.As shown in Figure 5, the performance dropsdrastically if we start to conduct word-level PHScombination too early.
After considering about ef-ficiency and performance, we determine to do thatin the last three layers.We then investigate the effects on hybrid de-coding with different beam sizes, and compare thetrend with two baseline systems and IHMM-basedword-level system combination method as well.The results are illustrated in Figure 6.From what we see in Figure 6, the performanceof each system is monotonically increasing as thebeam size becomes larger.
Hybrid decoding per-forms consistently better than IHMM Word-Combwhen the beam size is small, and the largest im-provement (+0.63% points) is obtained when thebeam size is set to 10.
However, as the beam size0.3550.360.3650.370.3750.380.3850.390.3950.410 20 50 100SYS1SY S2IHMMHybridFigure 6: Performance of hybrid decoding withdifferent beam sizes on NIST 2005 test setincreases, the performance gap is getting narrow.One intuitive observation is that hybrid decodingperforms slightly worse than IHMM Word-Combwhen the beam size is set to 100.
One possiblereason for this phenomenon is that, the alignmentnoise may be introduced to hybrid decoding sincewe have to generate monolingual alignments withmany poor translation derivations.The confusion network for PHS of each systemcan be built independently.
We would like to eval-uate the performance of single system hybrid de-coding.
Table 5 gives the results on both Hieroand BTG decoders.NIST 2005 NIST 2006SYS1 SYS2 SYS1 SYS2baseline 0.3745 0.3699 0.3346 0.3296self-comb 0.3770 0.3758?
0.3358 0.3355?Table 5: Hybrid decoding for single system,*:significantly better than baseline with p<0.05Table 5 shows that BTG decoder (SYS2) hasmore potential for so-called ?self-boosting?.The self-combination of BTG decoder improvesthe performance substantially over the baseline.However, we did not observe any significant im-provement for Hiero decoder (SYS1).Finally, we examine the impacts of skeleton se-lection for PHS in hybrid decoding.
The results inTable 6 demonstrate that, compared to the top-1selection method, translation performance can beimproved significantly with MBR-based skeletonselection method.
It strongly suggests that choos-ing the skeleton with more consistent word order220will lead to better translation results.NIST 2005 NIST 2006Top-1 0.3817 0.3415MBR 0.3884?
0.3513?Table 6: Skeleton selection in hybrid decoding,*:significantly better than top-1 skeleton selectionmethod with p<0.014 DiscussionSystem combination methods have been widelyused in SMT to improve the performance.
Forexample, in (Rosti et al, 2007a), several combi-nation methods have been proposed to make useof different kinds of consensus information.
In(He et al, 2008), better word alignment method isadopted to advance the word-level system combi-nation.
Our method is different from these meth-ods in the sense that we do not exclusively relyon the n-best full hypotheses from each individualdecoder, but emphasize the importance of word-level combination for PHS.
Thus, it enlarges thesearch space and is more prone to find better trans-lations.
Experimental results have shown the ef-fectiveness of our method.The idea of multiple systems collaborative de-coding (Li et al, 2009) works well on re-rankingthe outputs of each system using n-gram agree-ment statistics.
However, no new translation re-sults are generated compared to individual decod-ing.
Our method takes advantage of confusionnetwork to give PHS which cannot be seen before.Although (Liu et al, 2009) also work on PHS,we explore the cooperation of multiple systemsfrom a new perspective.
They use translationderivations from different decoders jointly as abridge to connect different models.
Different fromtheir work, we devise a heuristic method to ob-tain word alignment information from the deriva-tion of each decoder, which can be embeddedfor word-level PHS combination easily and effi-ciently.5 Conclusion and Future WorkIn this paper, we propose a new SMT decodingframework named hybrid decoding, in which mul-tiple decoders work synchronously to conduct lo-cal decoding and local word-level PHS combina-tion in turn.
We also devise a heuristic method toobtain word alignment information directly fromthe translation derivations, which is both intuitiveand efficient.
Experimental results show that withhybrid decoding the overall performance can beimproved significantly over both the individualbaseline decoder and the state-of-the-art systemcombination method.In the future, we will involve more individualSMT decoders into hybrid decoding.
In addition,we would like to keep on this work in two direc-tions.
On the one hand, start-up threshold of PHScombination will be explored in detail to find itsunderlying impact on hybrid decoding.
On theother hand, we will try to employ a more theoreti-cally sound approach to conduct the feature spacemapping from the feature space of confusion net-work to that of individual decoders.ReferencesAyan, Necip Fazil, Jing Zheng, and Wen Wang.
2008.Improving alignments for better confusion networksfor combining machine translation systems.
In Pro-ceedings of the 22nd International Conference onComputational Linguistics, pages 33-40Chiang, David.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting of the Associ-ation for Computational Linguistics, pages 263-270Chiang, David.
2007.
Hierarchical phrase-basedtranslation.
Computational Linguistics, 33(2):pages 201-228Galley, Michel, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 961-968He, Xiaodong, Mei Yang, Jianfeng Gao, PatrickNguyen, and Robert Moore.
2008.
Indirect-hmm-based hypothesis for combining outputs from ma-chine translation systems.
In Proceedings of the2008 Conference on Empirical Methods in NaturalLanguage Processing, pages 98-107Koehn, Phillip, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceed-221ings of the 2003 Human Language Technology Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics, pages 48-54Koehn, Phillip.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofthe 2004 Conference on Empirical Methods in Nat-ural Language Processing, pages 388-395Kumar, Shankar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine trans-lation.
In Proceedings of the 2004 Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, pages 169-176Li, Mu, Nan Duan, Dongdong Zhang, Chi-Ho Li, andMing Zhou.
2009.
Collaborative decoding: par-tial hypothesis re-ranking using translation consen-sus between decoders.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP, pages 585-592Liu, Yang, Yun Huang, Qun Liu, and Shouxun Lin.2007.
Forest-to-string statistical translation rules.In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages704-711Liu, Yang, Haitao Mi, Yang Feng, and Qun Liu.
2009.Joint decoding with multiple translation models.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP, pages 576-584Matusov, Evgeny, Nicola Ueffing, and Hermann Ney.2006.
Computing consensus translation from mul-tiple machine translation systems using enhancedhypotheses alignment.
In 11th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, pages 33-40Och, Franz Josef.
and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of the38th Annual Meeting of the Association for Compu-tational Linguistics, pages 440-447Och, Franz Josef.
2003.
Minimum Error Rate Train-ing in Statistical Machine Translation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160-167Papineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a method for auto-matic evaluation of machine translation.
In Pro-ceedings of the 40th Annual Meeting of the Associ-ation for Computational Linguistics, pages 311-318Rosti, Antti-Veikko, Necip Fazil Ayan, Bing Xiang,Spyros Matsoukas, Richard Schwartz, and BonnieDorr.
2007.
Combining outputs from multiple ma-chine translation systems.
In Proceedings of the2007 Human Language Technology Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 228-235Rosti, Antti-Veikko, Spyros Matsoukas, and RichardSchwartz.
2007.
Improved word-level system com-bination for machine translation.
In Proceedings ofthe 45th Annual Meeting of the Association of Com-putational Linguistics, pages 312-319Sim, K.C., W. Byrne, M. Gales, H. Sahbi, and P.Woodland.
2007.
Consensus network decodingfor statistical machine translation combinnation.
In32nd IEEE International Conference on Acoustics,Speech, and Signal ProcessingSnover, Matthew, Bonnie Dorr, Richard Schwartz, Lin-nea Micciula, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annota-tion.
In the 7th conference of the Association forMachine Translation in the Americas, pages 223-231Wu, Dekai.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel cor-pora.
Computational Linguistics, 23(3): pages 377-404Xiong, Deyi, Qun Liu, and Shouxun Lin.
2006.Maximum entropy based phrase reordering modelfor statistical machine translation.
In Proceedingsof the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, pages521-528Zhang, Dongdong, Mu Li, Chi-Ho Li, Ming Zhou.2007.
Phrase Reordering Model Integrating Syn-tactic Knowledge for SMT.
In Proceedings of the2007 Joint Conference on Empirical Methods inNatural Language Processing and ComputationalNatural Language Learning, pages 533-540Zhang, Le.
2006.
Maximum entropy model-ing toolkit for python and c++.
available athttp://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html.222
