UNIVERSITY OF SHEFFIELD: DESCRIPTION OF THELaSIE-II SYSTEM AS USED FOR MUC-7K.
Humphreys, R. Gaizauskas, S. Azzam, C. Huyck, B. Mitchell,H.
Cunningham, Y. Wilks1Department of Comp uter ScienceUniversity of SheeldRegent Court, Portobello RoadSheeld S1 4DP UKfkwh,robertg,saliha,huyck,brianm,hamish,yorickg@dcs.shef.ac.ukINTRODUCTIONThe University of Sheeld NLP group took part in MUC-7 using the LaSIE-II system, an evolution ofthe LaSIE (Large Scale Information Extraction) system rst created for participation in MUC-6 [9] and partof a larger research eort into information extraction underway in our group.
LaSIE-II was used to carryout all ve of the MUC-7 tasks and was, in fact, the only system to take part in all of the MUC-7 tasks.While LaSIE-II is signicantly dierent from the earlier version (dierences are detailed below) there areno radical changes in the basic philosophy of the approach.
This could be described as seeking a pragmaticmiddle way in the shallow vs deep analysis debate which has characterised the last several MUCs.
Thatis, while aware that information extraction tasks may not require full text understanding, and hence thatsystems should be optimised to make use of shallow techniques where appropriate, we have not wanted topreclude the application of arbitrarily sophisticated linguistic analysis techniques where these may proveuseful.
The result is an eclectic mixture of techniques including nite state recognition of domain-speciclexical patterns, partial parsing using a restricted context-free grammar, simplied semantic representationof each sentence in the text and a formal representation of the whole discourse from which all of the IE taskresults and the coreference task results are derived.
From our perspective, LaSIE-II should not be viewedas the expression of a theory about how to do IE, but as a laboratory in which ongoing experiments withdierent component NL processing techniques, and most importantly, their interaction are being carriedout.
Seen this way, one of the most important developments in LaSIE-II is its modularised architecture andintegration into the GATE platform (see below) which has enabled us to gain much deeper insights intostrengths and weaknesses of components of the system and the ways in which these interact.OVERVIEWLaSIE-II is a highly modularised system, made up of 9 TIPSTER-compliant modules, pictured in Figure1 as executed interactively through the GATE Graphical Interface.
The system is essentially a pipeline ofmodules each of which processes the entire text before the next is invoked.
The following is a brief descriptionof each of the component modules in the system:Tokenizer Identies token boundaries (as byte osets into the text) and text section boundaries (textheader, text body and any zones to be excluded from processing).1Thanks for additional contributions from Sandy Robertson, Andrea Setzer, George Demetriou, Malcolm Crawford (fsandyr,andrea, demetri, malcg@dcs.shef.ac.uk) and Mette Nelson (mln.id@cbs.dk) from the Copenhagen Business School.Figure 1: LaSIE-II ArchitectureGazetteer Lookup Looks for single and multi-word matches in multiple domain specic full name (loca-tions, organisations, etc.)
and keyword (company designators, person rst names, etc.)
lists, and tagsmatching phrases with appropriate name categories.Sentence Splitter Identies sentence boundaries in the text body.Brill Tagger [4] Assigns one of the 48 Penn TreeBank part-of-speech tags to each token in the text.Tagged Morph Simple morphological analysis to identify the root form and inectional sux for tokenswhich have been tagged as noun, verb, or adjective.buchart Parser Does two pass bottom-up chart parsing, pass one with a special named entity grammar,and pass two with a general phrasal grammar.
A `best parse' is then selected, which may be only apartial parse, and a predicate-argument representation, or quasi-logical form (QLF), of each sentenceis constructed compositionally.Name Matcher Matches variants of named entities across the text.Discourse Interpreter Adds the QLF representation to a semantic net, which encodes the system's domainmodel as a hierarchy of concepts.
Additional information presupposed by the input is also added to themodel, then coreference resolution is performed between new and old instances, and nally informationconsequent upon the input is added, producing an updated discourse model.Template Writer Writes out the ST, TR, and TE results by traversing the discourse model and extractingthe required information.NE and CO results are generated following the Discourse Interpreter by a generic SGML dump utility.LaSIE-II CHANGESRather than duplicating much of the description of the LaSIE system as used for MUC-6 [9], the followingsections describe the major changes between LaSIE, referred to in the following as LaSIE-I, and LaSIE-II.GATE/TIPSTER ArchitectureThe LaSIE-II system was developed using GATE, a General Architecture for Text Engineering [5], Shef-eld's implementation of the TIPSTER architecture specication [11].
GATE manages all the informationabout the texts that is produced by each module, and provides graphical tools for visualising that inform-ation, selecting controlow through dierent module combinations and running the IE system over sets oftexts.
A major strength of the architecture is that it encourages reuse by insulating the various modulesfrom each other by means of a common data management substrate | a \document manager" in TIPSTERterminology.
It also enables reuse of visualisation code: modules producing similar sorts of information (e.g.a PoS tagger and a named entity parser) share the same graphical viewing tool.
GATE provides a con-venient GUI-based environment within which to develop diverse modules, unconstrained by implementationlanguage (LaSIE-II is made up of C, Perl and Prolog modules), with the architecture taking care of the com-mon engineering tasks (e.g.
data storage) that are uninteresting from a language processing point-of-view.Lastly, GATE provides a command-line interface for batch processing, enabling us to run a nightly build,run, score, and report process as we developed the LaSIE-II system.GATE is now in a one-and-a-half release (1.5) that adds Java support, SGML I/O, a manual annotationtool, an annotation comparison tool and improved support for managing collections of documents.
Thisrelease is a half-way house between version 1, which was C++-based, and version 2, which will be Java-based.
Java modules that run under version 1.5 will run unchanged under version 2, while still accessingall the modules and facilities available under version 1.
SGML facilities are much improved, with inputvia the University of Edinburgh's LT NSL toolkit [15] that uses the Sp parser.
The manual annotationtool allows hand-coding of annotations to use as training or test data, and the comparison tool providesa straightforward way to score one annotation set against another.
In cases where a dedicated scorer isavailable, like MUC or Parseval [12], this can be integrated as a module in GATE, as shown in Figure1 with a scorer module for each MUC task.
The output of the MUC scorers can also be read into theGATE database, allowing keys and errors to be displayed using the existing viewers, as shown for the COtask scorer in Figure 2.
Wrapper code for these and other modules is available from our ftp site (seewww.dcs.shef.ac.uk/research/groups/nlp/gate/ for more details, and to download GATE, which isfreely available for research purposes, and comes bundled with a version of our MUC-6 extraction system).Lexical PreprocessingWith the exception of the Gazetteer Lookup stage, modules up to the buchart Parser are relativelyunchanged from LaSIE-I.
Minor changes were required to make use of the structure of the MUC-7 NYTtexts, and to classify SGML and other special symbols.The most noticeable change to the Gazetteer Lookup module is its change in position { from immediatelybefore the Parser in LaSIE-I to immediately after the Tokenizer in LaSIE-II.
This change was made mainlyto improve the accuracy of the Sentence Splitter module, which could previously propose incorrect sentenceboundaries within known NEs, for example \3 p.m. EST", \St.
Louis", etc.
The Splitter has been modiedslightly to treat gazetteer matches as units in the same way as tokens.The move involved decoupling the Lookup stage from the PoS Tagger.
Originally only tokens withparticular tags (nouns, adjectives, determiners, conjunctions, numerals, symbols) were matched against thegazetteer lists, but this restriction has been removed.
The Lookup stage now attempts to match all tokens,and therefore no longer suers from tagging errors.
However, this does introduce a few spurious matches,particularly with capitalised words in sentence initial position, for example the (Swedish) person rst names\Are" and \My".
An additional ltering stage in the Tagger module handles these cases, removing anygazetteer matches for sentence initial tokens not tagged as nouns or adjectives.
This ltering stage alsoattempts to correct some of the tagger's frequent mistagging of capitalised common nouns as proper nounsin document headers, by reference to a list of common English nouns (also used in the Splitter module).Figure 2: MUC CO Scorer output viewers, showing spurious coreferences during debuggingThe Lookup stage has also been modied to allow much simpler integration of new lists of names, eachdening a distinct semantic category.
A top-level conguration le species a set of plain text lists and typeand subtype values (e.g.
organization:company) to be assigned to matches in each list.
The module can nowbe switched between domains simply by specifying alternative conguration les.
For MUC-7 we used 55lists comprising 23,000 entries in total.A further improvement to the Lookup module is a reduction in its case sensitivity.
While initial exper-iments with complete case insensitivity in matching against the lists produced too many spurious matches,some reduction in sensitivity proved useful.
In particular, sequences of all-uppercase tokens in the input arenow matched in the lists in both their original form and also with each token converted to a form where onlythe initial character is uppercase.
This signicantly improves matches in the NYT headers.ParsingAs in LaSIE-I, parsing is still carried out in two stages, each stage using the same parsing mechanismbut a dierent grammar { rst a specialist NE grammar, then a general phrasal grammar.
The parseritself is largely unchanged from MUC-6 { a bottom-up chart parser written in Prolog which processescontext-free grammar rules with associated feature structures expressed as Prolog terms.
A complete chartis generated from which a single parse, quite possibly partial (i.e., a gapped sequence of phrasal subtrees),is selected using a `best parse selection' heuristic when parsing ceases.
Semantic interpretations in the formof predicate-argument representations are built up compositionally from phrasal constituents during parsingand the semantic interpretation of the nal selected analysis becomes the output of the parser and getspassed on to the discourse interpreter.The main changes introduced for MUC-7 are a signicantly enhanced grammar development environment,and a completely rewritten and extended grammar which now reects a substantially dierent philosophy.Figure 3: buchart Parser submodulesGrammar Development The modular facilities of GATE have been exploited to further compartment-alise the grammars into a total of 17 subgrammars which during development can be individually executedthrough the GATE graphical interface (see Figure 3).
After each grammar is run, its results may be viewedusing a tree viewer, and then, if changes are required, the grammar may be edited and rerun without leavingGATE for any recompilation process.The rst ten grammars shown in Figure 3 comprise the NE grammar and when the system is run inproduction mode the rules from these ten grammars are compiled into a single grammar for use in the rstpass of the parser.
The net eect is the same as running them separately since the ordering of the rules inthe compiled single grammar is the same as in the cascaded development version and the best parse selectionheuristic will, other things being equal, select the last of co-spanning analyses.
The same holds for the nextseven subgrammars which form the phrasal grammar and are compiled into a single grammar for the secondpass of the parser in production mode.Division of the grammar into smaller specialist chunks together with the new graphical tools had theexpected benets of allowing more rapid development and verication of subgrammars, and supported con-current grammar development by dierent persons working on dierent subgrammars.The Grammars The ten NE grammars consist of approximately 400 hand-coded rules that make useof part of speech tags, semantic tags added in the gazetteer lookup stage, and if necessary lexical itemsthemselves.
While signicantly rewritten since MUC-6 the basic philosophy here is the same (see [17] fordetails): patterns are detected in the texts and manually added to the grammar.
The enhancements to thegrammar development cycle described above have eased and speeded this process, but otherwise there islittle change.The phrasal grammars have been completely rewritten and compartmentalised, but there has also been asignicant change in the grammar acquisition process.
For MUC-6 the grammar was obtained by extractingthe context-free grammar rules implicit in the bracketing of the Penn Tree Bank [14] and selecting a subsetof them by thresholding on frequency [6].
Features to enable semantic interpretation were then added byhand to the extracted syntactic rules.While this technique allowed us to obtain a grammar quickly, the resulting analyses were poor, andthe eort of manually annotating the rules with features for semantic interpretation substantially reducedthe benets of the `grab-and-run' approach to grammar acquisition.
Given that so much handcrafting wasgoing into the grammar, it seemed that we might as well get the benet of more carefully handcrafting thesyntactic aspects of the rules too.
So, we have eectively rewritten the grammar from the ground up, usinga combination of general principles [16] and iterative renement using the MUC-7 training data.
The resultis a phrasal grammar of about 150 rules.The best parse selection heuristic chooses a single (possibly partial) analysis based on selecting a shortestsequence of maximally spanning non-overlapping edges which are of a semantically interpretable category(NP, VP, PP, S, and RelC).
Where there are several equivalent alternatives, the last one generated isselected.
This approach eliminates any ambiguity detected in parsing and ensures that a single analysis ispassed on to the discourse interpreter.
However, since our grammar does not rely on lexical semantic orsyntactic constraints to any great extent (e.g.
we do not use lexical selectional restrictions or subcategorisationinformation in parsing) it is very weak.
As a consequence, we adopt a very conservative approach withregards to phenomena such as attachment of complements, prepositional phrases and relative clauses, andalso apposition and co-ordination.
We followed a philosophy of only adding those rules which were (almost)certain never to generate errors in analysis { i.e.
a high precision, possibly low recall, approach to grammaticalanalysis.
In theory, when grammatical relations such as complements (e.g.
subjects and objects) are missed,they are meant to be added during discourse interpretation, where lexical-semantic information is available,and to some extent this happens (see below), though we have not developed this part of the system asmuch as we would like.
Exploring the boundaries between syntactic analysis, lexical-semantic analysis, anddiscourse analysis is very much part of ongoing work in LaSIE-II.The net eect using a cascade of grammars, each of which aims to identify a `chunk' of a particularcategory and is conservative with respect to attachment, is something very like the nite state models thathave been advocated by other MUC participants over the past few years [13, 10], as well as others in thelanguage engineering community [1].
In fact, we believe our grammars are now regular and that our chartparser could be replaced by a nite state parser, with substantial increase in speed.
We have not doneso to date due to restricted development resources and because the current grammar development/parsingenvironment is quite habitable.Discourse InterpretationApart from some gazetteer lists, and the corresponding grammar rules, all domain specic knowledge inthe system is concentrated in the Domain Model of the Discourse Interpreter.
As in LaSIE-I, this modelis expressed using a semantic net whose nodes represent `concepts' (classes or instances), with associatedattribute-value structures recording properties and relations of the concept, and whose arcs model a concepthierarchy and support property inheritance (see [7] for further details).The initial domain model for the MUC-7 ST task was constructed directly from the template denition.
Alaunch event concept node was added, together with vehicle and payload nodes, each with a subhierarchylisting the possible vehicle and payload types specied in the template denition:entity(X) ==> object(X) v event(X) v property(X).object(X) ==> artifact(X) v ...event(X) ==> launch_event(X) v ...artifact(X) ==> vehicle(X) vpayload(X).vehicle(X) ==> spacecraft(X) vaircraft(X) vground_vehicle(X) vwater_vehicle(X).spacecraft(X) ==> shuttle(X) vrocket(X).payload(X) ==> satellite(X) vmissile(X) vspace_probe(X) vmaterial(X) vpersonnel(X).Property types were also added for each template slot, launch date, vehicle owner, etc., and theTemplate Writer module was modied to read o and write out instances of the required types with therequired properties.Consequence properties (see below) were then added to hypothesise instances for each slot of a templateentity, given the appropriate textual trigger.
For example, an instance of a launch event in a text causesthe hypothesis of a vehicle, a payload, a date and a launch site related to the event; an instance of avehicle causes the hypothesis of an owner and a manufacturer, etc.
The Discourse Interpreter's generalcoreference mechanism is then used to attempt to resolve hypothesised instances with instances mentioned inthe text.
Running the system in this state, with absolutely no domain specic restrictions on the resolutionof hypotheses, gave an overall performance of 27.66 P&R on the ST training data.
This result was achievedusing only the template denition and no training data to customise the system, and required only a fewhours work.
This baseline customisation of the system to a new IE task is largely mechanical, and we intendto investigate the extent to which initial concept hierarchies and associated properties can be producedautomatically from a template denition.A small set of predened, or static, instances were also added to the Domain Model, encoding certainworld knowledge necessary to complete particular ST task slots.
For example, NASA was predened toallow its use as a default value for the payload owner slot for American astronauts, as required by theST task specications.
Similarly, common spacecraft launch sites were predened, restricting the selectionof launch site values.
Much greater use of this facility could be made to encode other relevant worldknowledge.During processing, the instances and properties from the semantic representation of a text (QLF) pro-duced by the Parser are added to the Domain Model.
The semantic representation of each sentence isFigure 4: Discourse Interpreter submodulesprocessed in the following stages, illustrated by the submodules in the GATE interface shown in Figure 4,gradually specialising the Domain Model to become a Discourse Model, the nal version of which is passedto the Template Writer.Add Semantics Instances from the QLF representation of a sentence are added below their parent classesin the concept hierarchy.
New concept nodes are created dynamically for classes not dened in advance, andare added directly below the object node for instances introduced by nouns, and below the event node forinstances introduced by verbs.
Properties in the input semantics are added to the attribute-value structuresassociated with the instances to which they relate.A new mechanism introduced in LaSIE-II is a word root to concept node mapping, used to establish theparent nodes of new instances.
Previously, concept nodes in the ontology were labelled with English wordroots onto which the QLF semantic representation was mapped directly, forcing subclass hierarchies to beconstructed even for synonymous terms.
The introduction of a word-to-concept table, or dictionary, providesa many-to-one mapping onto the concept nodes in the ontology, allowing synonym sets to be representedstraightforwardly in the table.The word-to-concept mapping also provides the ability to process QLF from non-English languages withthe same Discourse Interpreter and Domain Model.
An experimental Multilingual LaSIE (M-LaSIE) systemhas been constructed to process French, Spanish and English texts, producing templates or natural languagesummaries in each language, using the word-to-concept table for output as well as input.
Further details onthis system can be found in [8].Add Presuppositions Each instance and property added from the current sentence semantics attemptsto inherit any presuppositions, predened in the domain model, from its parent classes.
Presuppositionproperties of concept nodes are used to perform the following functions in the discourse model:1.
Additional Inferences Add inferred information to the current discourse model, in addition to the inputsemantics.
For example, a presupposition of the property name (proper name) is that any instance withthis property must be an instance of the object class, and also have a number property with the valuesingular.
If not already known, this information will be added to the discourse model.Some inferences will be highly domain, or even template, specic, providing particular slot values basedon patterns of semantic relations in the input.2.
Entity Hypothesis Expected, or implicit, instances, can be hypothesised to be resolved by the core-ference mechanism.
Nominalisations of verbs can be identied by presuppositions and lead to thehypothesis of the corresponding event, for example a hypothesised launch event from an instanceof a launch object.
Such events, if they acquire the necessary properties, will be written out as STtemplate entries.
Similarly, instances of indirectly related scenario specic objects, such as mission,can also give rise to the hypothesis of a launch event.3.
Word Sense Disambiguation Identify whether an instance of a particular class in the input is the sameas a known class in the ontology.
For example, the ontology contains fall as a subclass of date,but an instance of fall initially added here may be removed by a presupposition that identies thisinstance as, say, referring to a fall in share prices rather than a date.
Scenario specic senses canalso be caught in this way, for example only fire event instances related to missiles are retained aspotential launch events.4.
Role Classication The ontology contains a hierarchy of person roles, including domain specic rolessuch as astronaut, pilot, etc.
A presupposition acts to reclassify instances of these nodes from thetext as instances of the person node, with a property indicating the role.
This avoids the previousrequirement to specify person roles as subclasses of person to force semantic compatibility for corefer-ence.
Subhierarchies of roles can now be dened, for example job roles and family roles, with crossclassication of instances permitted.
The roles used for the ST task were obtained by identifying theintersection of terms in the ST training data with entries in an electronic dictionary with an animatefeature.5.
Partial Parse Extension Missing or unattached properties will cause the hypothesis, and attemptedresolution, of required instances.
A presupposition of the event node species that all instances belowit should have an lsubj (logical subject) property.
This presupposition will cause the hypothesis of anew instance for each event instance that the parser has failed to attach a subject relation to.
Thegeneral coreference mechanism is then used to attempt to resolve the hypothesis, applying variousrestrictions, also expressed as properties in the domain model, such as requiring subjects to be beforean active verb in the same sentence.
More specic event types can presuppose additional restrictions,for example restricting a hypothesised subject of a crash event to be of type vehicle.
This eectivelyspecies semantic roles, or subcategorisation patterns, for particular event types.
However we currentlyspecify very few such restrictions.The same mechanism can also be used to attempt prepositional attachment, where the parser has lefta phrase unattached.
Phrases can be classied as temporal, locational, etc., using the semantic classesof their heads, and then semantic role information used to identify potential attachment sites, as forverb arguments.
Again, however, we make little use of this facility in the current domain model.Object Coreference The general coreference mechanism takes a set of instances newly added to thediscourse model, and compares each one with the set of instances already in the discourse model.
For objectcoreference, proper names, pronouns, and common nouns are handled separately, rst attempting intra-sentential coreference for each set, and then inter-sentential coreference.
Each new-old pair of instances, if atall compatible, has a similarity score calculated for it, based on the distance between the instances' parentclasses in the concept hierarchy, and the number of shared properties.
The highest scoring pair, for each newinstance, is merged in the discourse model, deleting the instance with the least specic class in the ontology,and combining the properties of both instances.
This mechanism is basically unchanged from the LaSIE-IMUC-6 system, but with the addition of several new features:Coreference1.
Long Distance Coreference In LaSIE-I antecedents for pronouns and bare nouns were sought only inthe current and immediately preceding paragraphs, and no attempt was made to nd an antecedent inearlier paragraphs even if the anaphor almost certainly required one, as in the case of pronouns.
Thishas been extended to search successively earlier paragraphs until an antecedent can be found.
On the30 dry run texts, this extension gave a 2% increase in recall for pronouns, and a 7% increase in recallfor bare nouns, with no signicant change in precision.2.
Copular Constructions Constructions of the type NP1 be NP2 where NP1 should corefer with NP2(`Predicate Nominals' in the CO task denition), e.g.
The F14 \Tomcat" is the Navy's rst-line ghteraircraft, were not dealt with in LaSIE-I due to lack of development time, but they are now consideredby the coreference algorithm.This necessitated reviewing all the coreference rules, to add exceptions for copular constructions.
Forexample, in general an indenite noun phrase such as a president cannot have an antecedent, but thisneeded to be relaxed so as not to apply to copulars.Another important aspect of copular constructions is that they provide information to allow `unknown'words, i.e.
words whose semantic class is not in the ontology, to be classied during processing.
This ispossible when an instance of an unknown class is coreferred in a copular construction with an instanceof a known class.
For example, in Bill is a president, if president is not known as a concept in theontology and Bill is recognised as an instance of the known person node, then a president nodecan be automatically added below person.
Subsequent coreferences can then be more accurate bypreferring or preventing coreference with instances of the newly added class, for example, to preventsubsequent occurrences of it from being resolved with instances of president.3.
Catapora LaSIE-II handles two specic cases of pronouns occurring before their antecedents.
Firstly,pronouns within quotations, such as:\I caught Reggie when he was much younger counting his dad's trophies," McNair said.where I refers ahead to the speaker, McNair.
However, the coreference is currently only possible ifa complete sentence is successfully parsed within the quotation.
Secondly, pronouns within copularconstructions can also refer ahead, as in:This is a mystery.4.
Coordinated NPs A change in the MUC-7 CO task specication was the introduction of certain con-joined NPs as markables.
In the LaSIE-II coreference algorithm, a new instance representing the set ofany coordinated instances is created in the discourse model, which can act as a potential antecedent.The new instance will have a plural attribute and the semantic class will be the lowest common par-ent class of the coordinated instances.
For example, in Bruce and his boys three instances: e1, Bruce,e2, his boys, and the set instance e3, Bruce and his boys, of type person will be represented in thediscourse model.However, coreferences may fail or spurious coreferences be generated if the parser fails to correctlyrecognise the coordinated phrases on which the set instance identication relies.5.
Header Coreference In LaSIE-I only proper noun coreferences was attempted in text headers, but inLaSIE-II both pronoun and bare noun coreferences are also attempted.
However, coreference rules thatapply for normal sentences may fail for header sentences because of incomplete syntactic informationcaused by the telegraphic style and omission of determiners common in headers.
Relaxing some of thecoreference rules for headers gave a noticeable improvement in recall.6.
Generic Nouns A new subclass of bare nouns was introduced, with its own set of specic resolutionrules.
These `generic' nouns occur as NP heads with no modier or other relation which could allowadditional syntactic or semantic information about them to be inferred.The coreference mechanism still has a number of limitations.
One of the most common pronoun errors isrelated to rst and second person pronouns (you, we, I), but there is currently no special treatment for thisclass.
Also, these pronouns typically occur within quotations, the treatment of which is still very limited.We also do not attempt to handle type coercion and metonymy, and so fail to resolve pronouns like they incases such as:The Navy informs me that they have been unable to nd a common thread to these accidents.We also need a more robust proper name matching module, since the coreference mechanism is heavilydependent on this for proper name coreference.
It currently fails to match the organisation National Trans-portation Safety Board with Safety Board, while it does match Latin America with Latin | errors whichcould be easily corrected.Add Consequences The use of consequence rules parallels that of presuppositions, but since they applyafter object coreference, they may refer to information outside that of the current sentence.
If an instancein the current input is resolved with an instance elsewhere in the text it will acquire additional properties,potentially allowing more accurate inferences.The majority of template slot lls are proposed through consequence properties, and thus most of thedomain specic rules are applied at this stage.
Consequences can hypothesise unknown instances in the sameway as presuppositions, but these hypotheses are not restricted to being resolved during the processing ofthe current sentence.
Unresolved hypotheses from consequence rules are retained and resolution retried aftereach subsequent sentence.
As a special case, unresolved hypotheses may be removed by the introductionof new hypotheses of the same type but from a dierent source.
Currently, this removal only takes placefor hypotheses from consequences of launch events, to reect an assumption that instances related to arst launch will not be introduced in a text following the description of a second launch.
If, however, twomentions of an event are subsequently coreferred, all properties, including unresolved hypotheses, will bemerged anyway.Event Coreference The nal stage in the processing of each sentence is to attempt to merge launch eventsintroduced in the current sentence with any others in the discourse model.
This includes both hypothesisedand explicitly mentioned events.
Initially the criterion for merging two events was that all related entities(vehicle, payload, site, date) of each event be equivalent.
However, this proved to be too strict, generatingmany spurious template lls due to failed merges, and the criterion was gradually relaxed until it was re-quired only that the payload of two events be the same.
This makes a strong assumption that a payloadwill never be related to more than one launch event.
Other related entities are not compared, and only theentities related to the earlier of two events are retained in a successful merge.RESULTSThe following table summarises LaSIE-II system performance on all ve tasks.
After the NE evaluationa few modications were made to the initial system (system `A' ) as a result of examining the IE trainingdata which had been the NE formal run data.
In particular, a name list for `astronomical bodies', noneof which were identied during the NE evaluation run, was added when we realised we should have beenmarking these as locations; we also added a list of rocket names to help with identifying artifacts for theTE task.
Thus, the `A' system results for NE are the ocial results; the `B' system (the system run in thesubsequent CO and IE evaluation) results are ocial for the other tasks.
We unocially rescored the `B'system against the (no longer blind) NE evaluation data to see what eect the changes had.Task System Recall Precision P&RNE A 83 89 85.83B 87 94 90.41CO B 56.1 68.8 61.8TE B 75 80 77.17TR B 41 82 54.70ST B 47 42 44.04WALKTHROUGH TEXTSNamed EntityTask System Recall Precision P&RNE Walkthrough A 77 86 81.40NE Overall A 83 89 85.83NE Walkthrough B 88 88 88.41NE Overall B 87 94 90.41Performance on the NE walkthrough text was slightly below the level across the whole NE formal runtest set.
The system missed the organizations Intelsat and Globo, the locations Latin America and Xichang,and the person Llennel Evangelista.
It misclassied Hughes Electronics and MURDOCH SATELLITE aspersons, due to \Hughes" and \Murdoch" being listed as valid person rst names followed by unknownproper names.
We spuriously identied MTV and CNN as organizations, because they were both presentin our company gazetteer and we made no attempt to disambiguate companies from TV channels.
We alsoidentied \March" in the Long March rocket as a date.Some of these errors are easily corrected with a minor gazetteer additions.
Experimentation to this eectlead to the correct classication of Intelsat and Hughes Electronics, and the avoidance of the spurious date\March".
We also then correctly classied Latin America as a location, but the Namematch module thencaused Latin and Latin American to also be classied as locations.The remaining errors can be avoided by using the immediate context of the names, for example Globois qualied by conglomerate, which, if added to our ontology of organization types, would allow Globo to beclassied.
Also, Llennel Evangelista could be classied correctly if the appositional phrase a spokesman forIntelsat were interpreted correctly.CoreferenceTask Recall Precision P&RCO Walkthrough 60.8 64.0 62.3CO Overall 56.1 68.8 61.8The coreference algorithm performed well on all denite NPs, and on the majority of proper names in thewalkthrough text.
Only half of the pronouns are correctly resolved, however, and this is less than the systemwas typically able to resolve on the training data.
The main reasons are explained below.
More generally,the walkthrough recall result is higher than the average recall we obtained with the training and the formalrun data (respectively 56.5% and 56.1%).
The precision result is lower than the average obtained on thetraining and the formal run data (respectively 72.3% and 68.8%).
We concentrate below on the sources oferrors that caused precision to drop.The most common error is when a pronoun corefers to an entity that precedes it and has not beenruled out as a possible antecedent by possessing an incompatible syntactic or semantic property (the basicalgorithm is `eager', in the sense that it will corefer a pronoun with the closest preceding entity which cannotbe ruled out as an antecedent).
This happens for three pronouns in the walkthrough text.
For example inthe following paragraph, its corefers to revenue while the correct antecedent is Galaxy VIII, the main focusof the paragraph.Hughes' Galaxy VIII(I) plan would use one satellite, which the company estimates will cost $230million to build and launch.
Hughes expects Galaxy VIII(I) will bring in $30 million in revenuein its rst year and $58 million each year for the following 11 years, according to lings at theFCC.The same problem occurs with the pronoun they that corefers to airwaves instead of functions, in thefollowing:Those functions are likely to be slowly shifted to another slice of spectrum, while the airwavesthey've historically used are turned over, in part, to satellite services such as the ones planned byGE and GM.This kind of spurious coreference is more likely to happen with words, like revenue or airwaves whichare not recorded in our domain model, as no information is available about such classes to rule out orrestrict coreference.
Both examples suggest that a more complex mechanism is needed to detect whichentity the paragraph is about, i.e.
the focus or center (Galaxy in the rst example, and functions in thesecond) and to constrain the pronouns to corefer with it.
Substituting a focus-based approach based on [2],that provides such a mechanism for pronoun resolution, correctly resolves these three pronouns.
However,such an approach has problems of its own: the result of applying the focus-based algorithm across the wholetext shows a slight improvement for precision, but a somewhat larger drop in recall, due mainly to the factthat the focus-based approach is more restrictive in proposing antecedents for pronouns [3].
At presentwe are carrying out experiments to see how combining the advantages of both focus- and non-focus basedapproaches could lead to in superior results for pronoun resolution.The mechanism which automatically adds nodes to the ontology for words of unknown semantic class,contributes strongly to recall.
All the dynamically added nodes corefer correctly, i.e., the denite nounphrases the allocation, the airwaves, the plan whose head nouns were not known in our ontology are coreferredcorrectly.Coreferring instances introduced by head nouns which are identical or of compatible semantic type iscomplicated when they have dierent qualiers or modiers.
We have been handling these cases carefully inour coreference algorithm, but the walkthrough text provided some examples we had not met in the trainingdata.
Correcting our algorithm for the qualier comparison (to avoid coreferring things like direct-to-homeservice and direct-to-home video satellite service) led to a small improvement: R: 62.0; P: 68.1; P & R: 64.9on the walkthrough text, and R: 56.0; P: 70.2; P & R: 62.3 when run across the whole formal run test set.Another source of error is when the parse extension mechanism (described in the section on DiscourseInterpretation above) proposes the wrong argument for a verb.
Such is the case in the example below, whereit and the allocation do not corefer because the parse extension mechanism, when looking for a missing logicalsubject for use, hypothesises that it is the allocation; the allocation is then ruled out as an antecedent of thepronoun, since the logical subject and logical object of the same verb cannot corefer, except for particularcases such as the copula.Other companies that support the allocation and may use it include Lockheed Martin Corp.'sLoral Space and Communications, International Private Satellite Partners/Orion Atlantic Cap-ital Corp., and Comsat Corp.The system also failed to detect that the pronoun it that occurs in the pleonastic construction It is.
.
.
critical is not coreferential, and tried to corefer it anyway.
Pleonastic constructions are addressed in oursystem; however, in this case the parser was thrown o by the ellipsis.Finally, incomplete noun group recognition of Hughes' Galaxy VIII(I), i.e., not including (I) as a part ofthe name, resulted in identifying I as a pronoun, then coreferring it with Rupert Murdoch.Information ExtractionTexts/Task TE TR STR P P&R R P P&R R P P&RWalkthrough 75 87 80.68 39 77 51.97 42 40 41.18Overall 75 80 77.17 41 82 54.70 47 42 44.04Template Elements TE scores on the walkthrough article were slightly above the average across the testset.
On this article we were largely successful with all slots except for the Entity Descriptor slot where scoreswere 50 % precision and 21 % recall.
We will rst explain the particular items we failed on, and then discusswhy our Entity Descriptor slots were so poor.The system performed relatively poorly on the three artifacts in the walkthrough text.
We did get theLong March 3 artifact, but missed the B, so the name was incorrect.
We identied both of the artifactswithout names { the Intelsat satellites { but incorrectly used Intelsat as the artifact name, and got thedescriptors wrong.On organizations the system was much better, failing on only four (of twenty-three).
We did not coreferNews Corp. and News Corporation, nor did we corefer TCI and Tel-Communications Inc. because the namematcher module did not equate them.
Therefore an extra template element was generated for both TCI andNews Corp. We also did not recognize ING Barings as a company, and so did not print a template for it.Finally, we reported Organizacoes Globo as a city instead of a company.
Since it was not in our Gazetteerwe did not know its type; it was adjacent to a country, so we guessed it was a city.The system also did quite well on persons, failing on only three (of ten).
We did not classify ShayneMcGuire or Virnell Bruce as persons because their rst names did not appear in our Gazetteer.
We failedto get the unnamed company spokesman element, because we had made a conscious decision only to outputtemplates for entities with a name eld.
This was done because our handling of descriptors was so weak.The system also did quite well on locations, failing on only four (of nineteen).
We failed to print Brazilsince we printed Organizacoes Globo as a city with Brazil as its country.
We failed to identify Arlington asa city because it was in the Gazetteer as an organization and thus was attached to Space TransportationAssociation; this led us to mistakenly print Virginia as a province.
We printed U.S. from U.S.-based whichseems correct.
We simply missed French Guyana because it was not in the Gazetteer.The system did quite poorly on Entity Descriptors getting only four right, two incomplete, two wrong,and missing thirteen.
We had incomplete answers with Televisa where we got Mexico instead of Mexico'sbiggest broadcaster.
This was due to our weak handling of the 's in the grammar.
We were also incompletewith Irving Goldstein where the Descriptor proposed was chief executive instead of director general and chiefexecutive of Intelsat.
This was due to our poor handling of conjunction during parsing.The system proposed two spurious Descriptors.
First, we gave Rupert Murdoch the descriptor group,again due to a grammar confusion involving 's.
Second, we gave TCI the descriptor media due to a problemwith name lists.The system also missed thirteen Descriptors altogether.
This was largely due to a combination of a con-servative approach to parsing and a lack of eort on lling the Descriptor eld.
Our basic approach to parsingwas to only make attachments when we were quite sure they were correct.
This meant that many of thecomplex Descriptor elds such as spokesman for the Space Transportation Association of Arlington, Virginiawere never successfully combined during parsing.
We left these attachments to the discourse interpreter, butwe did not have enough time to develop discourse interpretation rules to make these attachments.Improving scores on Entity Descriptors would have improved other aspects of our TE performance.
Wecould not propose Elements based simply on Descriptors because our Descriptors were so poor.
Furthermore,Descriptors can help to classify entities, as in the case of Shayne McGuire, spokesman.Template Relations TR scores on the walkthrough text were slightly below the test set average.
Inthe walkthrough text we correctly identied six of eight location of relations, six of ten employee ofrelations and zero of two product of relations.In no formal run text did the system correctly identify a product of relation.
We had only one discourseinterpreter rule to nd this relation and that relied on discovering a known aircraft manufacturer.
Whilequite restrictive, this rule had led to high precision in the air crash domain of the dry run training data.
Wehad believed that there would be a large degree of overlap between aircraft and rocket manufacturers butwe were incorrect.
Perhaps we would have been more eective on this problem if we had had some trainingdata for Relations in the domain of launch events, but none was made available.Of the other six relations that we missed, three are due to failing to correctly categorize template elements.That is, the TR task fails because the TE task fails.
One employee of relation is missed due to ourconservative parsing strategy; we do not attach for the Space Transportation Association to spokesman ineither the parser or discourse interpreter.
One location of relation is not generated due to our weaknessof 's processing.
The nal location of relation is not generated because we do not completely parseInternational Technology Underwriters and thus do not attach it to the adjacent location.In addition to including a list of rocket manufacturers, two major improvements could be made to improveour TR scores.
The rst is to improve the TE scores, in particular by improving our Descriptors.
Secondlywe could improve our phrasal attachment either in the grammar or in the discourse interpreter.
This wouldenable us to see more relations between entities that are nearby in the text.Scenario Template The walkthrough text is reasonably representative of our overall performance on theST task.
The system proposed four launch events, twice as many as in the key, due to the identication offour separate satellites, each of which was associated with an event which then prevented event coreference.One of the spurious satellites was mentioned as a qualier: an Intelsat satellite launch.
We attemptedto avoid these cases with a rule to prevent any hypothesised entity resolving with noun modier within anominal compound.
This assumes that all entities required for the template will be mentioned as head nounsat some point in the text.
Unfortunately this rule failed altogether due to a bug.The other spurious satellite was caused by a coreference failure.
The coreference mechanism includes astrict rule that indenite noun phrases do not have antecedents, and so no coreference was made between anIntelsat satellite and a satellite built by Loral Corp. of New York for Intelsat, resulting in a spurious launchevent for the second case.Our system's poor performance on descriptors also lead to incorrect payload entities for the satellite,despite identifying the correct instances from the input, e.g.
my satellite as a descriptor for a second Intelsatsatellite.
This was corrected by introducing dierent criteria for substantial descriptors in the ST task thanfor the TE/TR tasks where we only output descriptors for entities that also had names.Correcting the hypothesis resolution rule and the descriptor selection increased both recall and precisionon this text, with a 4.5% increase in P&R.Further errors included the system's failure to identify any FAILED launch events.
All events proposedfor the walkthrough text were SCHEDULED.
We also associated two launch events with the shuttle as avehicle, rather than Long March 3B, and we missed the B from the rocket name.The system's (ocial, not debugged) ST output can be summarised as follows:1.
A civilian TV satellite is to be launched today from China.2.
A civilian TV satellite is to be launched in a year from the US by Long March 3.3.
A civilian TV satellite is to be launched in a year from China by the shuttle.4.
A civilian TV satellite is to be launched in a year from the US by the shuttle.The production of these simple summaries in not yet automated, as in LaSIE-I for MUC-6, but we planto add this facility in the near future.
The construction of a complete discourse model by the system allowsvarious results to be read o easily, not only the MUC output.
For summarisation we therefore have accessto much more information than that contained in the template itself, with the ability to control the level ofdetail.
Such summaries can be used as a debugging tool, especially as a means for non-technical users toprovide error reports.DISCUSSIONBefore closing, two questions are worth considering.
First, how did our MUC-7 results compare with ourMUC-6 results and what does this tell us?
Second, what would we do dierently or next in order to improveon our MUC-7 performance?
These questions are best discussed in relation to each of the tasks.Named EntityOur ocial MUC-6 NE scores were R: 84;P: 94; P & R: 89.06.
Our system failed to process two texts in theMUC-6 evaluation due to a bug totally unrelated to the system's language processing capabilities; scoringdone by the MUC scoring committee with this bug xed yielded R: 89;P: 93; P & R: 91.01 which is a moreaccurate reection of the system's capabilities.
This time our ocal scores were R: 83;P: 89; P & R: 85.83{ a drop of about 5 %.The rst thing to note is that the scores of many of the MUC-6 veterans dropped comparably on NE inMUC-7.
One obvious cause of this was lack of training data in the domain of the nal evaluation.
In ourcase, and in some other cases, one manifestation of this was the failure to recognise astronomical bodies aslocations, since there were none, or very few, astronomical bodies in the NE dry run data.
As noted above,adding this change, and one or two other small changes apparent from examining the NE test data (whichsubsequently became the IE training data), lead to improved scores of R: 87; P: 94; P & R: 90.41.Thus, on balance, a case can be made for claiming our NE scores have remained broadly the sameas in MUC-6.
But, given the further eort that has gone into the system since MUC-6, one would haveexpected scores to have improved.
Why haven't they?
Without further detailed failure analysis we cannotsay precisely.
However, a few remarks can be made.
First, it appears that the NYT texts used for MUC-7 are more heterogeneous in their style, and hence there is more variation in form of NE expressions.Designing a rule set to capture this variation is, consequently, more dicult.
This observation has beensupported anecdotally by variousMUC-7 participants.
Second, the NE task was harder in certain respects.In particular, relative temporal expressions were included in the MUC-7 NE task { both for dates and timesof day.
Some of these expressions were very dicult indeed (e.g.
less than one hour after the AmericanAirlines 757 slammed into the mountain side killing all 147 passengers) and it is not clear that the taskguidelines had completely stabilised for this subtask.What would we do dierently/next to improve NE performance?
Most obviously, since our recall isconsiderably below precision, we need to concentrate on recall.
Our system has a category of `unknown'proper name (any proper name not resolved to one of the MUC categories) and even supercial reviewingof system results show that many proper name expressions falling into this category ought in fact to havebeen assigned one of the MUC NE categories.
Given that the system carries out a relatively deep analysis,it should be possible to use further lexical semantic/conceptual knowledge in the discourse interpreter toresolve some of these cases.2Another area that needs work concerns the determinism of the NE grammars.
The ten NE grammarsused in the MUC-7 system operate in a xed order and the last co-spanning analysis always gets chosen.In some cases, regardless of the order of the grammars, errors will result.
For example, consider the namesJulian Hill and Pearl Harbour.
As presented to our NE grammars both consist of a known person rst namefollowed by a location trigger word.
If our person grammar is run after our location grammar both come outas persons; if the location grammar is run after the person grammar both come out as locations.
Clearly,both could be either locations or persons, though most of us will use world knowledge to make the choice.However, the best solution is to pass on the ambiguity and allow a later module, with more contextualknowledge, to decide.
Controlled propagation of ambiguity into the discourse interpreter is thus a challengefor us.CoreferenceOur MUC-6 ocial coreference scores were: R: 51; P: 71 (P & R: 59.363).
As with NE we missedprocessing two texts and when the results for these were added in our scores were R: 54; P: 70 (P &R:60.97).
For MUC-7 our scores were R: 56.1; P: 68.8; P & R: 61.8.Thus, there has been a very slight overall improvement.
Considerable eort went into ne-tuning thecoreference mechanism and more detailed failure analysis will be necessary to determine why more signicantgains were not achieved and where further advances can be made.
The nal test set was smaller in MUC-7(20 articles, as opposed to 30 in MUC-6) and it may be that the extra coreference phenomena we addressedin our MUC-7 system (see above) simply did not occur in the test set with sucient frequency to make asignicant dierence.
Or it may be that the MUC-7 CO test was signicantly harder in some way that hasnot yet been determined.Obvious areas for work include better handling of quoted speech, ascertaining whether the combinationof a focus mechanism and a semantic compatibility/recency mechanism can yield overall better results, andimproving the underlying grammatical analysis on which the coreference algorithm relies.Information ExtractionTemplate Element For the template element task our ocial MUC-6 scores were: R: 66; P: 74; P & R:69.8 and, with the addition of the two missed articles: R: 68; P: 74; P & R: 70.8.
MUC-7 scores were: R:75; P: 80; P & R: 77.17.Template element is the one MUC-7 task where we appear to have made clear and signicant gains overMUC-6.
Of course, the major redenition of this task since MUC-6 may mean direct comparison is notsensible.
However, it is worth briey considering whether a reasonable story can be told about why ourscores have gone up.2We did attempt to collect information from the training keys about the distribution of NE classes occurring as complementsof specic prepositions (e.g.
what sort of NE's follow the preposition in) and as noun phrases modied by PPs with a specicpreposition and complement type (e.g.
what sort of NE's are modied by phrases of the form in LOCATION).
However, noneof these patterns occurred reliably enough to be used in the nal system.3F-measures were not calculated for the coreference task in MUC-6, but were for MUC-7.
The P & R gures supplied herefor MUC-6 are our calculations using the standard formula.Our NE scores were no better than in MUC-6 and TE is crucially dependent on the ability to identifylocations and organizations.
After analysis it appears that the increase is due to two factors.
First, a changein the TE task denition meant that the location information associated with an organization in MUC-6 {the locale and country slots of the organization template element { was exported into the location oftemplate relation in MUC-7.
Since we did relatively poorly in identifying this information in MUC-6 (relativeto other slots in the organization template), it seems reasonable that our scores on TE would go up when itwas removed from the task.
Second, our entity descriptor scores, while bad in MUC-7, were much worsein MUC-6: in fact these scores improved by a factor of three in MUC-7.
We believe this is attributable toless ambitious, but more reliable phrasal parsing, and to the creation of a more nely tuned set of rules inthe discourse interpreter specically geared to extracting entity descriptors.What would we do dierently/next to improve TE performance?
As noted in the discussion of thewalkthrough text above, the entity descriptor slot is where most eort needs expending, and to improvethis we need more reliable PP-attachment in descriptive phrases (many of our descriptors were fragments ofthe correct descriptor) as well as more work on identifying which phrases are descriptors.
Other weak pointswere low recall on artifacts and low precision on locations.Template Relation The template relation task was new for MUC-7, so there are no MUC-6 scoresto compare with (the information for one of the MUC-7 relations { location of { was captured in thetemplate element for organization in MUC-6, but it is not clear how to compare meaningfully the localeand country slot scores in the MUC-6 organization TE, with the MUC-6 TR location of object and slotscores).As discussed in relation to the walkthrough text above, the major improvements needed for TR arerst to develop appropriate rules for product of relations, since we missed all such relations in the nalevaluation; second, to improve TE scores, since TR is parasitic on TE; and third, as with TE, to enhancegrammatical and semantic analysis to better detect TR's.Scenario Template Our MUC-6 ST scores were: R: 37; P: 73; P & R: 48.96 (the addition of the twomissed articles made no dierence as they contained no scenario events).
In MUC-7 our ST scores were: R:47; P: 42; P & R: 44.04.
Thus, we suered an overall drop in f-measure of nearly ve points and while overrecall went up by ten, our precision, which been the highest ST precision score at MUC-6, went down byover thirty points.We believe that the MUC-7 ST task was signicantly more dicult than the MUC-6 one.
This conclusionappears to be borne out by the overall lower performance on ST in MUC-7 (high f-measure of 50.79 ascompared to 56.4 in MUC-6) and is the result of a number of factors.
First, the MUC-7 texts were longeron average and told more complex stories.
Second, the template for MUC-7 was more complex, consistingof 7 object types with a total of 32 non-optional slots, while the MUC-6 template consisted of 5 objecttypes with 20 non-optional slots.
Third, the MUC-7 template required more ne-grained distinctions to bemade.
For example, no less than ve organizations had to be distinguished: the launch vehicle owner, launchvehicle manufacturer, the payload owner, payload manufacturer and payload recipient.
These are subtlerdistinctions than those required in the MUC-6 management succession task.Our drop in precision was largely due to being unable to appropriately merge multiple references to thesame event, as discussed in reference to the walkthrough article above.
Our belief, though this requiresfurther analysis, is that this is more of a problem in the MUC-7 ST task because the texts tend to refer tothe same scenario event repeatedly more than the MUC-6 texts do.
We also observed a tendency for eventsin the MUC-7 scenario to be expressed more frequently by nominalisation ((rocket) launch, (missile) attack,(shuttle)ight, even (shuttle) mission could all signal a launch event) than the management successionevents had been in MUC-6.
Our attempts to handle these no doubt contributed to recall but had a largenegative impact on precision, as accurately merging multiple nominalisations of the same event is dicult.Without considerably more analysis of the results it is dicult to identify the most promising avenuesto improve LaSIE-II's MUC-7 ST performance.
As ever, more time to implement the scenario would havehelped.
We devoted under ten person days to this task and given its complexity the results are not surprising(we produced no ll at all for several slots, due to lack of time to implement any rules for them).CONCLUSIONFrom our perspective the most encouraging result from MUC-7 was the vindication it supplied for theeort we have put into the GATE architecture over the two years leading up to the evaluation.
The chiefadvantages GATE aorded were: a framework supporting a highly modular approach to language processing which in turn permitted ateam with varying levels of programming skills, areas of expertise, and available time to work eectivelytogether; reusability of interface code, especially results viewers, that allowed rapid creation of useful tools forgaining diagnostic insights.Using GATE we were able to take part in all ve of the MUC-7 tasks without excessive expenditure of eort{ everyone working on MUC-7 had a parallel full time commitment to other projects.
Further, componentsof the MUC-7 system were in active use in systems undergoing simultaneous development for other languageengineering projects, and GATE made managing this complexity straightforward (this contrasts with LaSIE-Iwhich was a monolithic system more or less dedicated to MUC-6).As noted at the Introduction, LaSIE-II does not diverge radically from LaSIE-I in general approach, andas we did not set out to test explicitly any hypothesis about language processing in the evaluation, we donot see MUC-7 as allowing us to draw any strong conclusions conrming or disconrming aspects of thisapproach.
Perhaps the most interesting insights we have gained as a result of participating in MUC-7 arethe following. Simple replacement of a semantic compatibility/recency approach to pronoun resolution with a focus-based approach does more harm than good { perhaps because the focus-based approach relies on moreaccurate/complete syntactic information than is available from a parser designed to perform robustsyntactic analysis on real texts. Using a hand-crafted grammar which aims only to do phrasal analysis up to the point of ambiguity,as opposed to a grammar extracted from the PTB and thresholded on rule frequency (as we did forMUC-6), produces less complete but more accurate syntactic analyses.
How this feeds through to taskperformance is hard to assess.
More work needs to be done to see how and to what extent these partialsyntactic analyses can be extended using conceptual knowledge. Further techniques need to be devised for (semi-) automatically acquiring and rening lexical se-mantic/conceptual knowledge in the domain.MUC-7 was both a harder and a broader test than MUC-6, so we are neither surprised nor dismayedby the lack of striking progress in `bottom line' gures.
The data, task denitions, and scoring softwareproduced for MUC-7 are a rich resource which we intend to mine for deeper insights for the foreseeablefuture.
From these insights further progress is sure to follow.ACKNOWLEDGEMENTSThis work was partly supported by EPSRC grant GR/K25267 (GATE/LaSIE), EC DGXIII grant LE2238-1 (AVENTINUS), GlaxoWellcome plc and Elsevier Science (EMPathIE).REFERENCES[1] S. Abney.
Partial parsing via nite state cascades.
Natural Language Engineering, 2(4):337{344, 1996.
[2] S. Azzam.
Resolving anaphors in embedded sentences.
In Proceedings of the 34th meetings of theAsssociation for Computational Linguistics (ACL'96), Santa Cruz, CA, 1996.
[3] S. Azzam, K. Humphreys, and R. Gaizauskas.
Evaluating a focus-based approach to anaphora resolution.In Proceedings of COLING-ACL'98, pages 74{78, 1998.
[4] E. Brill.
A simple rule-based part-of-speech tagger.
In Proceeding of the Third Conference on AppliedNatural Language Processing, pages 152{155, Trento, Italy, 1992.
[5] H. Cunningham, K. Humphreys, R. Gaizauskas, and Y. Wilks.
Software Infrastructure for NaturalLanguage Processing.
In Proceedings of the Fifth Conference on Applied Natural Language Processing(ANLP-97), pages 237{244, March 1997.
Available as http://xxx.lanl.gov/ps/9702005.
[6] R. Gaizauskas.
Investigations into the grammar underlying the Penn Treebank II.
Research Memor-andum CS-95-25, Department of Computer Science, Univeristy of Sheeld, 1995.
[7] R. Gaizauskas and K. Humphreys.
Using a semantic network for information extraction.
Journal ofNatural Language Engineering, 3(2/3):147{169, 1997.
[8] R. Gaizauskas, K. Humphreys, S. Azzam, and Y. Wilks.
Concepticons vs. lexicons: An architecture formultilingual information extraction.
In M.T.
Pazienza, editor, Proceedings of the Summer School onInformation Extraction (SCIE-97), LNCS/LNAI, pages 28{43.
Springer-Verlag, 1997.
[9] R. Gaizauskas, T. Wakao, K Humphreys, H. Cunningham, and Y. Wilks.
Description of the LaSIEsystem as used for MUC-6.
In Proceedings of the Sixth Message Understanding Conference (MUC-6),pages 207{220.
Morgan Kaufmann, 1995.
[10] R. Grishman.
The NYU system for MUC-6 or where's the syntax.
In Proceedings of the Sixth MessageUnderstanding Conference (MUC-6), pages 167{176.
Morgan Kaufmann, 1995.
[11] R. Grishman.
TIPSTER Architecture Design Document Version 2.3.
Technical report, DARPA, 1997.Available at http://www.tipster.org/.
[12] P. Harrison, S. Abney, E. Black, D. Flickinger, C. Gdaniec, R. Grishman, D. Hindle, R. Ingria, M. Mar-cus, B. Santorini, and T. Strzalkowski.
Evaluating syntax performance of parser/grammars of english.In Proceedings of the Workshop On Evaluating Natural Language Processing Systems.
Association ForComputational Linguistics, 1991.
[13] J.R. Hobbs, D. Appelt, M. Tyson, J.
Bear, and D. Israel.
Description of the FASTUS system as usedfor MUC-4.
In Proceedings of the Fourth Message Understanding Conference MUC-4, pages 268{275.Morgan Kaufmann, 1992.
[14] M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.
Building a large annotated corpus of english: ThePenn treebank.
Computational Linguistics, 19(2):313{330, 1993.
[15] D. McKelvie, C. Brew, and H. Thompson.
Using SGML as a Basis for Data-Intensive NLP.
In Proceed-ings of the fth Conference on Applied Natural Language Processing (ANLP-97), 1997.
[16] R. Quirk and S. Greenbaum.
A University Grammar of English.
Longman, Harlow, Essex, 1973.
[17] T. Wakao, R. Gaizauskas, and Y. Wilks.
Evaluation of an algorithm for the recognition and classicationof proper names.
In Proceedings of the 16th International Conference on Computational Linguistics(COLING96), pages 418{423, Copenhagen, 1996.
