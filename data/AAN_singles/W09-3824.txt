Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 150?161,Paris, October 2009. c?2009 Association for Computational LinguisticsCross Parser Evaluation and Tagset Variation : a French Treebank StudyDjam?
Seddah?, Marie Candito?
and Beno?t Crabb???
Universit?
Paris-SorbonneLALIC & INRIA (ALPAGE)28 rue SerpenteF-75006 Paris ?
France?
Universit?
Paris 7INRIA (ALPAGE)30 rue du Ch?teau des RentiersF-75013 Paris ?
FranceAbstractThis paper presents preliminary investiga-tions on the statistical parsing of French bybringing a complete evaluation on Frenchdata of the main probabilistic lexicalizedand unlexicalized parsers first designedon the Penn Treebank.
We adapted theparsers on the two existing treebanks ofFrench (Abeill?
et al, 2003; Schluter andvan Genabith, 2007).
To our knowledge,mostly all of the results reported here arestate-of-the-art for the constituent parsingof French on every available treebank.
Re-garding the algorithms, the comparisonsshow that lexicalized parsing models areoutperformed by the unlexicalized Berke-ley parser.
Regarding the treebanks, weobserve that, depending on the parsingmodel, a tag set with specific featureshas direct influence over evaluation re-sults.
We show that the adapted lexical-ized parsers do not share the same sensi-tivity towards the amount of lexical ma-terial used for training, thus questioningthe relevance of using only one lexicalizedmodel to study the usefulness of lexical-ization for the parsing of French.1 IntroductionThe development of large scale symbolic gram-mars has long been a lively topic in the FrenchNLP community.
Surprisingly, the acquisition ofprobabilistic grammars aiming at stochastic pars-ing, using either supervised or unsupervised meth-ods, has not attracted much attention despite theavailability of large manually syntactic annotateddata for French.
Nevertheless, the availabilityof the Paris 7 French Treebank (Abeill?
et al,2003), allowed (Dybro-Johansen, 2004) to carryout the extraction of a Tree Adjoining Grammar(Joshi, 1987) and led (Arun and Keller, 2005)to induce the first effective lexicalized parser forFrench.
Yet, as noted by (Schluter and van Gen-abith, 2007), the use of the treebank was ?chal-lenging?.
Indeed, before carrying out successfullyany experiment, the authors had to perform a deeprestructuring of the data to remove errors and in-consistencies.
For the purpose of building a sta-tistical LFG parser, (Schluter and van Genabith,2007; Schluter and van Genabith, 2008) have re-annotated a significant subset of the treebank withtwo underlying goals: (1) designing an annota-tion scheme that matches as closely as possiblethe LFG theory (Kaplan and Bresnan, 1982) and(2) ensuring a more consistent annotation.
On theother hand, (Crabb?
and Candito, 2008) showedthat with a new released and corrected version ofthe treebank1 it was possible to train statisticalparsers from the original set of trees.
This pathhas the advantage of an easier reproducibility andeases verification of reported results.With the problem of the usability of the datasource being solved, the question of finding oneor many accurate language models for parsingFrench raises.
Thus, to answer this question,this paper reports a set of experiments wherefive algorithms, first designed for the purpose ofparsing English, have been adapted to French:a PCFG parser with latent annotation (Petrov etal., 2006), a Stochastic Tree Adjoining Grammarparser (Chiang, 2003), the Charniak?s lexicalizedparser (Charniak, 2000) and the Bikel?s implemen-tation of Collins?
Model 1 and 2 (Collins, 1999)described in (Bikel, 2002).
To ease further com-parisons, we report results on two versions of thetreebank: (1) the last version made available inDecember 2007, hereafter FTB , and describedin (Abeill?
and Barrier, 2004) and the (2) LFGinspired version of (Schluter and van Genabith,2007).The paper is structured as follows : After a briefpresentation of the treebanks, we discuss the use-1This has been made available in December 2007.150fulness of testing different parsing frameworksover two parsing paradigms before introducingour experimental protocol and presenting our re-sults.
Finally, we discuss and compare with re-lated works on cross-language parser adaptation,then we conclude.2 Treebanks for FrenchThis section provides a brief overview to the cor-pora on which we report results: the French Tree-bank (FTB) and the Modified French Treebank(MFT).2.1 The French TreebankTHE FRENCH TREEBANK is the first treebankannotated and manually corrected for French.
Itis the result of a supervised annotation project ofnewspaper articles from Le Monde (Abeill?
andBarrier, 2004).
The corpus is annotated with la-belled constituent trees augmented with morpho-logical annotations and functional annotations ofverbal dependents as shown below :<SENT><NP fct="SUJ"><w cat="D" lemma="le" mph="ms" subcat="def">le</w><w cat="N" lemma="bilan" mph="ms" subcat="C">bilan</w></NP><VN><w cat="ADV" lemma="ne" subcat="neg">n?</w><w cat="V" lemma="?tre" mph="P3s" subcat="">est</w></VN><AdP fct="MOD"><w compound="yes" cat="ADV" lemma="peut-?tre"><w catint="V">peut</w><w catint="PONCT">-</w><w catint="V">?tre</w></w><w cat="ADV" lemma="pas" subcat="neg">pas</w></AdP><AP fct="ATS"><w cat="ADV" lemma="aussi">aussi</w><w cat="A" lemma="sombre" mph="ms" subcat="qual">sombre</w></AP><w cat="PONCT" lemma="."
subcat="S">.</w></SENT>Figure 1: Simplified example of the FTB: ?Le bi-lan n?est peut-?tre pas aussi sombre.?
(i.e.
Theresult is perhaps not as bleak)Though the original release (in 2000) consistsof 20,648 sentences, the subset of 12351 function-ally annotated sentences is known to be more con-sistently annotated and therefore is the one usedin this work.
Its key properties, compared withthe Penn Treebank (hereafter PTB, (Marcus et al,1994)), are the following :Size: The FTB consists of 385,458 tokens and12,351 sentences, that is the third of the PTB.
Italso entails that the average length of a sentenceis 27.48 tokens.
By contrast the average sentencelength in the PTB is 24 tokens.Inflection: French morphology is richer thanEnglish and leads to increased data sparseness is-sues for the purpose of statistical parsing.
Thereare 24,098 types in the FTB, entailing an averageof 16 tokens occurring for each type.A Flat Annotation Scheme: Both the FTBand the PTB are annotated with constituent trees.However, the annotation scheme is flatter in theFTB.
For instance, there are no VPs for finite verbsand only one sentential level for clauses or sen-tences whether or not introduced by a complemen-tizer.
Only verbal nucleus (VN) is annotated andcomprises the verb, its clitics, auxiliaries, adverbsand surrounding negation.While X-bar inspired constituents are supposedto contain all the syntactic information, in the FTBthe shape of the constituents does not necessar-ily express unambiguously the type of dependencyexisting between a head and a dependent appear-ing in the same constituent.
Yet, this is crucial toextract the underlying predicate-argument struc-tures.
This has led to a ?flat?
annotation scheme,completed with functional annotations that informon the type of dependency existing between a verband its dependents.
This was chosen for Frenchto reflect, for instance, the possibility to mix post-verbal modifiers and complements (Figure 2), orto mix post-verbal subject and post-verbal indirectcomplements : a post verbal NP in the FTB cancorrespond to a temporal modifier, (most often) adirect object, or an inverted subject, and all cases,other subcategorized complements may appear.SENTNP-SUJDuneNlettreVNVavaitV?t?Venvoy?eNP-MODDlaNsemaineAderni?rePP-AOBJPauxNPNsalari?s(a) A letter had been sent last week to the employeesSENTNP-SUJDLeNConseilVNVaVnotifi?NP-OBJDsaNd?cisionPP-AOBJP?NPDlaNbanque(b) The Council has notified his decision to the bankFigure 2: Two examples of post-verbal NPs : atemporal modifier (a) and a direct object (b)Compounds: Compounds are explicitly anno-tated and very frequent in the treebank: 14.52% oftokens are part of a compound (see the compoundpeut-?tre ?perhaps?
in Figure 1 ).
They include151digit numbers (written with spaces in French) (e.g.10 000), frozen compounds (eg.
pomme de terre?potato?)
but also named entities or sequenceswhose meaning is compositional but where inser-tion is rare or difficult (e.g.
garde d?enfant ?childcare?).
As noted by (Arun and Keller, 2005), com-pounds in French may exhibit ungrammatical se-quences of tags as in ?
la va vite ?in a hurry?
: Prep+ Det+ finite verb + adverb or can in-clude ?words?
which do not exist outside a com-pound (e.g hui in aujourd?hui ?today?).
Therefore,compounds receive a two-level annotation : con-stituent parts are described in a subordinate levelusing the same POS tagset as the genuine com-pound POS.
This makes it more difficult to extracta proper grammar from the FTB without mergedcompounds2.
This is why, following (Arun andKeller, 2005) and (Schluter and van Genabith,2007), all the treebanks used in this work containcompounds.2.2 The Modified French TreebankTHE MODIFIED FRENCH TREEBANK (MFT) hasbeen derived from the FTB by (Schluter and vanGenabith, 2008) as a basis for a PCFG-based Lexi-cal Functional Grammar induction process (Cahillet al, 2004) for French.
The corpus is a subset of4739 sentences extracted from the original FTB.The MFT further introduces formal differences oftwo kinds with respect to the original FTB: struc-tural and labeling modifications.Regarding structural changes, the main transfor-mations include increased rule stratification (Fig.3), coordination raising (Fig.
5).Moreover, the MFT?s authors introduced newtreatments of linguistic phenomena that were notcovered by their initial source treebank.
Thoseinclude, for example, analysis for ?It?-cleft con-structions.3 Since the MFT was designed for thepurpose of improving the task of grammar induc-tion, the MFT?s authors also refined its tag set bypropagating information (such as mood featuresadded to VN node labels), and added functionalpaths4 to the original function labels.
The modifi-cations introduced in the MFT meet better the for-mal requirements of the LFG architecture set up2Consider the case of the compound peut-?tre ?perhaps?whose POS is ADV, its internal structure (Fig.
1) would leadto a CFG rule of the form ADV ??
V V.3See pages 2-3 of (Schluter and van Genabith, 2007) fordetails.4Inspired by the LFG framework (Dalrymple, 2001).AdPADVencoreADVpasADVtr?sADVbienAdPADVencoreAdPADVpasAdP ADVtr?sAdP ADVbienFTB initial analysis MFT modificationFigure 3: Increased stratification in the MFT : ?en-core pas tr?s bien?
(?still not very well?)XP1..Y..
X1 ..Z.. COORDC XP2XP1COORD-XPXP..Y.. X1 ..Z..C XP2Figure 5: Coordinated structures in the generalcase, for FTB (up) and MFT (down)by (Cahill et al, 2004) and reduce the size of thegrammars extracted from the treebank.
MFT hasalso undergone a phase of error mining and an ex-tensive manual correction.2.3 Coordination in French TreebanksOne of the key differences between the two Frenchtreebanks is the way they treat coordinate struc-tures.
Whereas the FTB represents them with anadjunction of a COORD phrase as a sister or adaughter of the coordinated element, the MFT in-troduces a treatment closer to the one used in thePTB to describe such structures.
As opposed to(Arun and Keller, 2005) who decided to transformthe FTB?s coordinations to match the PTB?s analy-sis, the COORD label is not removed but extendedto include the coordinated label (Fig.
5).In Figure 5, we show the general coordinationstructure in the FTB, and the corresponding mod-ified structure in the MFT.
A more complicatedmodification concerns the case of VP coordina-tions.
(Abeill?
et al, 2003) argue for a flat repre-sentation with no VP-node for French, and this is152SENTVNCLElleVajouteSsubque ...COORDCCetVNVpr?senteNPdouze points de d?saccordSENTNPCLElleCOORD-VPVPVN-finiteV-finiteajouteSsubque ...C-CetVPVN-finiteV-finitepr?senteNPdouze points de d?saccordFigure 4: Two representations of ?VP coordinations?
for the sentence She adds that ... and presentstwelve sticking points: in the FTB (left) and in the MFT (right)particularly justified in some cases of subject-verbinversion.
Nevertheless, VP phrases are used inthe FTB for non-finite VPs only (nodes VPinf andVPpart).
In the MFT, finite VPs were introducedto handle VP coordinations.
In those cases, theFTB annotation scheme keeps a flat structure (Fig-ure 4, left), where the COORD phrase has to be in-terpreted as a coordinate of the VN node; whereasfinite VP nodes are inserted in the MFT (Figure 4,right).2.4 SummaryIn Table 2, we describe the annotation schemes ofthe treebanks and we provide in Table 1 a numericsummary of some relevant different features be-tween these two treebanks.
The reported numberstake into account the base syntactic category labelswithout functions, part-of-speech tags without anymorpho-syntactic information (ie.
no ?gender?
ornumber?
).properties FTB MFT# of sentences 12351 4739Average sent.
length 27.48 28.38Average node branching 2.60 2.11PCFG size (without term.
prod.)
14874 6944# of NT symbols 13 39# of POS tags 15 27Table 1: Treebanks Properties3 Parsing AlgorithmsAlthough Probabilistic Context Free Grammars(PCFG) are a baseline formalism for probabilis-tic parsing, it is well known that they suffer fromtwo problems: (a) The independence assumptionsmade by the model are too strong, and (b) For Nat-ural Language Parsing, they do not take into ac-count lexical probabilities.
To date, most of theresults on statistical parsing have been reportedfor English.
Here we propose to investigate howto apply these techniques to another language ?French ?
by testing two distinct enhancementsFTB MFTPOS tags A ADV C CL D ETI N P P+D P+PROPONCT PREF PROVA A_card ADVADV_int AD-Vne A_int CC CLC_S D D_cardET I N N_cardP P+D PONCTP+PRO_rel PREFPRO PRO_cardPRO_int PRO_relV_finite V_infV_partNT labels AP AdP COORD NPPP SENT Sint SrelSsub VN VPinf VP-partAdP AdP_int APAP_int COORD_XPCOORD_UC CO-ORD_unary NCNP NP_int NP_relPP PP_int PP_relSENT Sint Srel SsubVN_finite VN_infVN_part VP VPinfVPpart VPpart_relTable 2: FTB?s and MFT?s annotation schemesover the bare PCFG model carried out by two classof parser models: an unlexicalized model attempt-ing to overcome problem (a) and 3 different lex-icalized models attempting to overcome PCFG?sproblems (a) and (b)5.3.1 Lexicalized algorithmsThe first class of algorithms used are lexicalizedparsers of (Collins, 1999; Charniak, 2000; Chi-ang, 2003).
The insight underlying the lexical-ized algorithms is to model lexical dependenciesbetween a governor and its dependants in order toimprove attachment choices.Even though it has been proven numerous timesthat lexicalization was useful for parsing the WallStreet Journal corpus (Collins, 1999; Charniak,2000), the question of its relevance for other lan-guages has been raised for German (Dubey andKeller, 2003; K?bler et al, 2006) and for French5Except (Chiang, 2003) which is indeed a TREE IN-SERTION GRAMMAR (Schabes and Waters, 1995) parser butwhich must extract a lexicalized grammar from the set of con-text free rules underlying a treebank.153(Arun and Keller, 2005) where the authors ar-gue that French parsing benefits from lexicaliza-tion but the treebank flatness reduces its impactwhereas (Schluter and van Genabith, 2007) arguethat an improved annotation scheme and an im-proved treebank consistency should help to reacha reasonable state of the art.
As only Collins?
mod-els 1 & 2 have been used for French as instancesof lexicalised parsers, we also report results fromthe history-based generative parser of (Charniak,2000) and the Stochastic Tree Insertion Grammarparser of (Chiang, 2003) as well as (Bikel, 2002)?simplementation of the Collins?
models 1 & 2(Collins, 1999).
Most of the lexicalized parserswe use in this work are well known and since theirreleases, almost ten years ago, their core parsingmodels still provide state-of-the-art performanceon the standard test set for English.6 We insist onthe fact that one of the goals of this work was toevaluate raw performance of well known parsingmodels on French annotated data.
Thus, we havenot considered using more complex parsing archi-tectures that makes use of reranking (Charniak andJohnson, 2005) or self-training (McClosky et al,2006) in order to improve the performance of araw parsing model.
Furthermore, studying and de-signing a set of features for a reranking parser wasbeyond the scope of this work.
However, we diduse some of these models in a non classical way,leading us to explore a Collins?
model 2 variation,named model X, and a Stochastic Tree AdjoiningGrammar (Schabes, 1992; Resnik, 1992) variant7 ,named Spinal Stochastic Tree Insertion Grammars(hereafter SPINAL STIG), which was first used tovalidate the heuristics used by our adaptation ofthe Bikel?s parser to French.
The next two subsec-tions introduce these variations.Collins?
Model 2 variation During the ex-ploratory phase of this work, we found out that aspecific instance of the Collins?
model 2 leads tosignificantly better performance than the canoni-cal model when applied to any of the French Tree-banks.
The difference between those two modelsrelies on the way probabilities associated to so-called ?modifier non terminals?
nodes are handledby the generative model.To explain the difference, let us recall that6Section 23 of the Wall Street Journal section of the PTB.7The formalism actually used in this parser is a con-text free variant of Tree Adjoining Grammar, Tree InsertionGrammars (TIG), first introduced in (Schabes and Waters,1995).a lexicalized PCFG can roughly be describedas a set of stochastic rules of the form:P ?
Ln Ln?1 ..L1 H R1 .. Rm?1 Rmwhere Li, H , Ri and P are all lexicalized nonterminals; P inherits its head from H (Bikel,2004).
The Collins?
model 2 deterministicallylabels some nodes of a rule to be arguments ofa given Head and the remaining nodes are con-sidered to be modifier non terminals (hereafterMNT).In this model, given a left-hand side symbol, thehead and its arguments are first generated and thenthe MNT are generated from the head outward.In Bikel?s implementation of Collins?s model 2(Bikel, 2004), the MNT parameter class is the fol-lowing (for clarity, we omit the verb intervening,subcat and side features which are the same inboth classes) :?
model 2 (canonical) :p(M(t)i|P,H,wh, th,map(Mi?1))Where M(t)i is the POS tag of the ith MNT,P the parent node label, H the head nodelabel, wh the head word and th its POStag.
map(Mi?1) is a mapped version ofthe previously-generated modifier added tothe conditioning context (see below for itsdefinition).map(Mi) =8>><>>:+START+ if i = 0CC if Mi = CC+PUNC+ if Mi =,or Mi =:+OTHER+ otherwise9>>=>>;Whereas in the model we call X 8, the mappingversion of the previously generated non terminal isreplaced by a complete list of all previously gen-erated non terminals.?
Model X :p(M(t)i|P,H,wh, th, (Mi?1, ...,Mi?k))The FTB being flatter than the PTB, one can con-jecture that giving more context to generate MNTwill improve parsing accuracy, whereas clusteringMNT in a X-bar scheme must help to reduce datasparseness.
Note that the Model X, to the best ofour knowledge, is not documented but included inBikel?s parser.8See file NonTerminalModelStructure1.java in Bikel?sparser source code at http://www.cis.upenn.edu/~dbikel/download/dbparser/1.2/install.sh.154The spinal STIG model In the case of the STIGparser implementation, having no access to anargument adjunct table leads it to extract a gram-mar where almost all elementary trees consist ofa suite of unary productions from a lexical anchorto its maximal projection (i.e.
spine9).
Thereforeextracted trees have no substitution node.Moreover, the probability model, being splitbetween lexical anchors and tree templates,allows a very coarse grammar that contains, forexample, only 83 tree templates for one treebankinstantiation, namely the FTB-CC (cf.
section 5).This behavior, although not documented10, isclose to Collins?
model 1, which does not use anyargument adjunct distinction information, and ledto results interesting enough to be integrated asthe ?Chiang Spinal?
model in our parser set.
Itshould be noted that, recently, the use of similarmodels has been independently proposed in(Carreras et al, 2008) with the purpose of gettinga richer parsing model that can use non localfeatures and in (Sangati and Zuidema, 2009) as amean of extracting a Lexicalized Tree SubstitutionGrammar.
In their process, the first extractedgrammar is actually a spinal STIG.3.2 Unlexicalized ParserAs an instance of an unlexicalized parser, the lastalgorithm we use is the Berkeley unlexicalizedparser (BKY) of (Petrov et al, 2006).
This algo-rithm is an evolution of treebank transformationprinciples aimed at reducing PCFG independenceassumptions (Johnson, 1998; Klein and Manning,2003).Treebank transformations may be of two kinds(1) structure transformation and (2) labellingtransformations.
The Berkeley parser concentrateson (2) by recasting the problem of acquiring anoptimal set of non terminal symbols as an semi-supervised learning problem by learning a PCFGwith Latent annotations (PCFG-LA): given an ob-served PCFG induced from the treebank, the latentgrammar is generated by combining every non ter-minal of the observed grammar to a predefined setH of latent symbols.
The parameters of the latentgrammar are estimated from the actual treebank9Not to be confused with the ?spine?
in the Tree AdjunctGrammar (Joshi, 1987) framework which is the path from afoot node to the root node.10We mistakenly ?discovered?
this obvious property dur-ing the preliminary porting phase.trees (or observed trees) using a specific instanci-ation of EM.4 Experimental protocolIn this section, we specify the settings of theparsers for French, the evaluation protocol and thedifferent instantiations of the treebanks we usedfor conducting the experiments.4.1 Parsers settingsHead Propagation table All lexicalized parsersreported in this paper use head propagation tables.Adapting them to the French language requiresto design French specific head propagationrules.
To this end, we used those described by(Dybro-Johansen, 2004) for training a StochasticTree Adjoining Grammar parser on French.
Fromthis set, we built a set of meta-rules that wereautomatically derived to match each treebankannotation scheme.As the Collins Model 2 and the STIG model needto distinguish between argument and adjunctnodes to acquire subcategorization frames prob-abilities, we implemented an argument-adjunctdistinction table that takes advantage of thefunction labels annotated in the treebank.
This isone of the main differences with the experimentsdescribed in (Arun and Keller, 2005) and (Dybro-Johansen, 2004) where the authors had to relyonly on the very flat treebank structure withoutfunction labels, to annotate the arguments of ahead.Morphology and typography adaptation Fol-lowing (Arun and Keller, 2005), we adaptedthe morphological treatment of unknown wordsproposed for French when needed (BKY?s andBIKEL?s parser).
This process clusters unknownwords using typographical and morphological in-formation.
Since all lexicalized parsers containspecific treatments for the PTB typographical con-vention, we automatically converted the originalpunctuation parts of speech to the PTB?s punctua-tion tag set.4.2 Experimental detailsFor the BKY parser, we use the Berkeley imple-mentation, with an initial horizontal markoviza-tion h=0, and 5 split/merge cycles.
For theCOLLINS?
MODEL, we use the standard param-eters set for the model 2, without any argu-155ment adjunct distinction table, as a rough emu-lation of the COLLINS MODEL 1.
The same setof parameters used for COLLINS?
MODEL 2 isused for the MODEL X except for the parameters?Mod{Nonterminal,Word}ModelStructureNumber?
set to1 instead of 2.4.3 ProtocolFor all parsers, we report parsing results with thefollowing experimental protocol: a treebank is di-vided in 3 sections : test (first 10%), development(second 10%) and training (remaining 80%).
TheMFT partition set is the canonical one (3800 sen-tences for training, 509 for the dev set and the last430 for the test set).
We systematically report theresults with compounds merged.
Namely, we pre-process the treebank in order to turn each com-pound into a single token both for training and test.4.4 Evaluation metricsConstituency Evaluation: we use the standardlabeled bracketed PARSEVAL metric for evalua-tion (Black et al, 1991), along with unlabeleddependency evaluation, which is described as amore annotation-neutral metric in (Rehbein andvan Genabith, 2007).
In the remainder of this pa-per, we use PARSEVAL as a shortcut for LabeledBrackets results on sentence of length 40 or less.Dependency Evaluation: unlabeled dependenciesare computed using the (Lin, 1995) algorithm,and the Dybro Johansens?s head propagation rulescited above11.
The unlabeled dependency accu-racy gives the percentage of input words (exclud-ing punctuation) that receive the correct head.
Allreported evaluations in this paper are calculated onsentences of length less than 40 words.4.5 Baseline : Comparison using minimaltagsetsWe compared all parsers on three different in-stances, but still comparable versions, of both theFTB and the MFT.
In order to establish a base-line, the treebanks are converted to a minimal tagset (only the major syntactic categories.)
withoutany other information (no mode propagation as inthe MFT) except for the BIKEL?s parser in Collins?model 2 (resp.
model X) and the STIG parser (i.e.11For this evaluation, the gold constituent trees are con-verted into pseudo-gold dependency trees (that may con-tain errors).
Then parsed constituent trees are convertedinto parsed dependency trees, that are matched against thepseudo-gold trees.STIG-pure) whose models needs function labels toperform.Note that by stripping all information from thenode labels in the treebanks, we do not meanto compare the shape of the treebanks or theirparsability but rather to present an overview ofparser performance on each treebank regardless oftagset optimizations.
However, in each experimentwe observe that the BKY parser significantly out-performs the other parsers in all metrics.As the STIG parser presents non statistically sig-nificant PARSEVAL results differences between itstwo modes (PURE & SPINAL) with a f-score p-value of 0.32, for the remaining of the paper wewill only present results for the STIG?s parser in?spinal?
mode.FTB-min MFT-minCOLLINS MX PARSEVAL 81.65 79.19UNLAB.
DEP 88.48 84.96COLLINS M2 PARSEVAL 80.1 78.38UNLAB.
DEP 87.45 84.57COLLINS M1 PARSEVAL 77.98 76.09UNLAB.
DEP 85.67 82.83CHARNIAK PARSEVAL 82,44 81.34UNLAB.
DEP 88.42 84.90CHIANG-SPINAL PARSEVAL 80.66 80.74UNLAB.
DEP 87.92 85,14BKY PARSEVAL 84,93 83.16UNLAB.
DEP 90.06 87.29CHIANG-PURE PARSEVAL 80.52 79.56UNLAB.
DEP 87,95 85.02Table 3: Labeled F1 scores for unlexicalisedand lexicalised parsers on treebanks with minimaltagsets5 Cross parser evaluation of tagsetvariationIn (Crabb?
and Candito, 2008), the authorsshowed that it was possible to accurately train thePetrov?s parser (Petrov et al, 2006) on the FTB us-ing a more fine grained tag set.
This tagset, namedCC12 annotates the basic non-terminal labels withverbal mood information, and wh-features.
Re-sults were shown to be state of the art with a F1parseval score of 86.42% on less than 40 wordssentences.To summarize, the authors tested the impact oftagset variations over the FTB using constituencymeasures as performance indicators.Knowing that the MFT has been built with PCFG-based LFG parsing performance in mind (Schluter12TREEBANKS+ in (Crabb?
and Candito, 2008).156and van Genabith, 2008) but suffers from a smalltraining size and yet alows surprisingly high pars-ing results (PARSEVAL F-score (<=40) of 79.95% on the MFT gold standard), one would havewished to verify itsperformance with more annotated data.However, some semi-automatic modificationsbrought to the global structure of this treebankcannot be applied, in an automatic and reversibleway, to the FTB.
Anyway, even if we cannot evalu-ate the influence of a treebank structure to another,we can evaluate the influence of one tagset to an-other treebank using handwritten conversion tools.In order to evaluate the relations between tagsetsand parsing accuracy on a given treebank, we ex-tract the optimal tagsets13 from the FTB, the CCtagset and we convert the MFT POS tags to thistagset.
We then do the same for the FTB on whichwe apply the MFT?s optimal tagset (ie.
SCHLU).Before introducing the results of our experiments,we briefly describe these tagsets.1.
min : Preterminals are simply the main cate-gories, and non terminals are the plain labels2.
cc : (Crabb?
and Candito, 2008) best tagset.Preterminals are the main categories, con-catenated with a wh- boolean for A, ADV,PRO, and with the mood for verbs (there are 6moods).
No information is propagated to nonterminal symbols.
This tagset is shown in Ta-ble 4, and described in (Crabb?
and Candito,2008).ADJ ADJWH ADV ADVWH CC CLO CLRCLS CS DET DETWH ET I NC NPP P P+DP+PRO PONCT PREF PRO PROREL PROWHV VIMP VINF VPP VPR VSTable 4: CC tagset3.
schlu : N. Schluter?s tagset (Table 2.Preterminals are the main categories, plusan inf/finite/part verbal distinction, andint/card/rel distinction on N, PRO, ADV, A.These distinctions propagate to non terminalnodes projected by the lexical head.
Non ter-minals for coordinating structures are splitaccording to the type of the coordinatedphrases.Results of these experiments, presented in Table5, show that BKY displays higher performances13W.r.t constituent parsing accuracyin every aspects (constituency and dependency,except for the MFT-SCHLU).
Regardless of theparser type, we note that unlabeled dependencyscores are higher with the SCHLU tagset than withthe CC tagset.
That can be explained by the finestgranularity of the SCHLU based rule set comparedto the other tagset?s rules.
As these rules have allbeen generated from meta description (a generalCOORD label rewrites into COORD_vfinite, CO-ORD_Sint, etc..) their coverage and global accu-racy is higher.
For example the FTB-CC contains18 head rules whereas the FTB-SCHLU contains43 rules.Interestingly, the ranking of lexicalized parsersw.r.t PARSEVAL metrics shows that CHARNIAKhas the highest performance over both treebanktagsets variation even though the MFT?s table (ta-ble 5) exhibits a non statistically significant vari-ation between CHARNIAK and STIG-spinal onPARSEVAL evaluation of the MFT-CC.14One the other hand, unlabeled dependency evalu-ations over lexicalized parsers are different amongtreebanks.
In the case of the FTB, CHARNIAKexhibits the highest F-score ( FTB-CC: 89.7,FTB-SCHLU: 89.67) whereas SPINAL STIG per-forms slightly better on the MFT-SCHLU (MFT-CC: 86,7, MFT-SCHLU: 87.16).
Note that bothtested variations of the Collins?
model 2 displayvery high unlabeled dependency scores with theSCHLU tagset.6 Related WorksAs we said in the introduction, the initial workon the FTB has been carried by (Dybro-Johansen,2004) in order to extract Tree Adjunct Grammarsfrom the treebank.
Although parsing results werenot reported, she experienced the same argumentadjunct distinction problem than (Arun and Keller,2005) due to the treebank flatness and the lack offunctional labels in this version.
This led Arunto modify some node annotations (VNG to distin-guish nodes dominating subcategorized subject cl-itics and so on) and to add bigrams probabilities tothe language model in order to enhance the over-all COLLINS?
MODEL?
performance.
Althoughour treebanks cannot be compared (20.000 sen-tences for Arun?s one vs 12351 for the FTB), wereport his best PARSEVAL results (<=40): 80.65LP, 80.25 LR, 80.45 F1.However, our results are directly comparable with14Precision P-value = 0.1272 and Recall = 0.06.157ParserCollins (MX)Collins (M2)Collins (M1)CharniakChiang (Sp)BkyParseval DependencyMFTCC MFTSCH.
MFTCC MFTSCH.80.2 80.96 85.97 87.9878.56 79.91 84.84 87.4374 78.49 81.31 85.9482.5 82.66 86.45 86.9482.6 81.97 86.7 87.1683.96 82.86 87.41 86.87Parseval DependencyFTBCC FTBSCH.
FTBCC FTBSCH.82.52 82.65 88.96 89.1280.8 79.56 87.94 87.8779.16 78.51 86.66 86.9384.27 83.27 89.7 89.6781.73 81.54 88.85 89.0286.02 84.95 90.48 90.73Table 5: Evaluation Results: MFT-CC vs MFT-SCHLU and FTB-CC vs FTB-SCHLU(Schluter and van Genabith, 2007) whose bestPARSEVAL F-score on raw text is 79.95 and ourbest 82.86 on the MFT-SCHLU.PARSER FTBARUN MFTSCHLUArun (acl05) 80.45 -Arun (this paper) 81.08 -Schluter (pacling07) - 79.95Collins (Mx) 81.5 80,96Collins (M2) 79.36 79,91Collins (M1) 77.82 -Charniak 82.35 82,66Chiang (Sp) 80.94 81,86Bky 84.03 82.86Table 6: Labeled bracket scores on Arun?s FTBversion and on the MFTIn order to favour a ?fair?
comparison betweenour work and (Arun and Keller, 2005), we alsoran their best adaptation of the COLLINS MODEL2 on their treebank version using our own headrules set15 and obtained 81.08% of F1 score (Ta-ble 6).
This shows the important influence of afine grained head rules set and argues in favorof data driven induction of this kind of heuris-tics.
Even though it was established, in (Chiangand Bikel, 2002), that unsupervised induction ofhead rules did not lead to improvement over anextremely hand crafted head rules set, we believethat for resource poor languages, such methodscould lead toward significant improvements overparsing accuracy.
Thus, the new unsupervisedhead rules induction method presented in (Sangatiand Zuidema, 2009) seems very promising for thistopic.However, it would be of interest to see how theArun?s model would perform using the MODEL Xparameter variations.7 DiscussionRegarding the apparent lack of success of a gen-uine COLLINS?
MODEL 2 (in most cases, its per-15Due to the lack of function annotation labels in this tree-bank, (Arun and Keller, 2005)?s argument distinction tablewas used for this experiment.formance is worse than the other parsers w.r.t toconstituent parsing accuracy) when trained on atreebank with annotated function labels, we sus-pect that this is caused by the increased datasparseness added by these annotations.
The samecan be said about the pure STIG model, whose re-sults are only presented on the FTB-MIN becausethe differences between the spinal model and itselfwere too small and most of the time not statisti-cally significant.
In our opinion, there might besimply not enough data to accurately train a pureCOLLINS?
MODEL 2 on the FTB with functionlabels used for clues to discriminate between argu-ment and adjuncts.
Nevertheless, we do not sharethe commonly accepted opinion about the poten-tial lack of success of lexicalized parsers.To the best of our knowledge, most adaptations ofa lexicalized model to a western language havebeen made with Dan Bikel?s implementation ofCOLLINS?
MODELS.16In fact, the adaptations of the CHARNIAK andBKY?s models exhibit similar magnitudes of per-formances for French as for English.
Evidence oflexicalization usefulness is shown through a learn-ing curve (Figure 6) obtained by running some ofour parsers in perfect tagging mode.
This experi-ment was done in the early stages of this work, thegoal was to see how well the parsers would behavewith the same head rules and the same set of pa-rameters.
We only compared the parsers that couldbe used without argument adjunct distinction table(ie.
COLLIN?S MODEL 1, SPINAL STIG, CHAR-NIAK and BKY).For this earlier experiment, our implementationof the COLLINS MODEL 1 actually corresponds tothe MODEL X without an argument adjunct dis-tinction table.
More precisely, the absence of ar-gument nodes, used for the acquisition of subcat-egorization frames features, makes the MODEL Xparsing model consider all the nodes of a rule, ex-16Note that the CHARNIAK?s parser has been adapted forDanish (Zeman and Resnik, 2008) ; the authors report a 80.20F1 score for a specific instance of the Danish Treebank.1582000 4000 6000 8000 1000076788082848688Number of training sentencesLabeledbracketsF?score(<=40)BerkeleyCharniakSpinalTigModel 1 (emulated)Figure 6: Learning Curve experiment results forparsers in perfect tagging modecept the head, as Modifier Non Terminal nodes(MNTs).
Hence, because of the impossibility toextract subcategorization frames, the generationof a MNT depends mainly on the parent headword and on the whole list of previously gener-ated MNTs.
One can suppose that training onsmall treebanks would lead this distribution to besparse, therefore most of the discriminant infor-mation would come from less specific distribu-tions.
Namely the ones conditioned on the headpos tag and on the last previously generated MNTas shown in this model back-off structure (Table7).Back-off level p(M(t)i| ?
?
?
)0 P,H,wh, th, ?Mi?1, ...,Mi?k?1 P,H, th,Mi?12 P,H, fTable 7: MODEL X simplified parameter class forMNTsM(t)i is the POS tag of the ith MNT, P the parent nodelabel, H the head node label, wh the head word, th its POStag, ?Mi?1, ...,Mi?k?
the list of previously generated MNTsand f a flag stating if the current node is the first MNT to begenerated.Interestingly, in the SPINAL STIG model,almost all the extracted trees are spinal and conse-quently are handled by an operation called SisterAdjunction whose probability model for a givenroot node of an elementary tree, also conditionsits generation upon the label of the previouslygenerated tree (Chiang, 2003).
Furthermore,the second component of the Sister Adjunction?sback-off structure (Table 8) is made coarser by theremoving of the lexical anchor of the tree where asister-adjunction is to occur.Studying in depth the respective impact of thesefeatures on the performance of both models isoutside the scope of this paper, nevertheless wenote that their back-off structures are based onsimilar principles: a deletion of the main lexicalinformation and a context limited to the root labelof the previously generated tree (resp.
MNT nodelabel for the MODEL X).
This can explain whythese formally different parsers display almost thesame learning curves (Fig.
6) and more over whythey surprisingly exhibit few sensitivity to theamount of lexical material used for training.Back-off level Psa(?| ?
?
?
)0 ?
?, ?
?, ?
?, i,X1 ?
?, ?
?, i,X2 ?
?, ?
?, iTable 8: SPINAL STIG parameter class for Sister-adjoining tree templates (Chiang, 2003)?
is the tree to be generated on the sister adjunction site(?
?, i) of the tree template ??
, ??
is the lexical anchor of ??
,??
is ??
stripped from its anchor POS tag and X is the rootlabel of the previous tree to sister-adjoin at the site (?
?, i).However, the learning curve also shows that theCHARNIAK?s17 and BKY?s parsers have almostparallel curves whereas this specific COLLIN?SMODEL 1 parser and the SPINAL STIG model havevery similar shape and seem to reach an upperlimit very quickly.18 The last two parsers havingalso very similar back-off models (Chiang, 2003),we wonder (1) if we are not actually comparingthem because of data sparseness issues and (2) ifthe small size of commonly used treebanks doesnot lead the community to consider lexicalizedmodels, via the lone COLLINS?
MODELS, as inap-propriate to parse other languages than Wall StreetJournal English.17As opposed to the other parsers, the Charniak?s parsertagging accuracy did not reach the 100% limit, 98.32% for thelast split.
So the comparison is not really fair but we believethat the visible tendency still stands.18We are of course aware that the curve?s values are alsofunction of the amount of new productions brought by theincreased treebank size.
That should be of course taken intoaccount.159Regarding the remarkable performance of theBKY algorithm, it remains unclear why exactlyit systematically outperforms the other lexicalizedalgorithms.
We can only make a few remarksabout that.
First, the algorithm is totally dis-joint from the linguistic knowledge, that is entirelytaken from the treebank, except for the suffixesused for handling unknown words.
This is not trueof the Collins?
or Charniak?s models, that wereset up with the PTB annotation scheme in mind.Another point concerns the amount of data nec-essary for an accurate learning.
We had the intu-ition that lexicalized algorithms would have ben-efited more than BKY from the training data sizeincrease.
Yet the BKY?s learning curve displays asomewhat faster progression than lexicalized algo-rithms such as the SPINAL STIG and our specificinstance of the COLLINS?
MODEL 1.In our future work, we plan to conductself-training experiments using discriminativererankers on very large French corpora to studythe exact impact of the lexicon on this unlexical-ized algorithm.8 ConclusionBy adapting those parsers to French and carry-ing out extensive evaluation over the main char-acteristics of the treebank at our disposal, weprove indeed that probabilistic parsing was effi-cient enough to provide accurate parsing resultsfor French.
We showed that the BKY model estab-lishes a high performance level on parsing results.Maybe more importantly we emphasized the im-portance of tag set model to get distinct state ofthe art evaluation metrics for FTB parsing, namelythe SCHLU tagset to get more accurate unlabeleddependencies and the CC tagset to get better con-stituency parses.
Finally, we showed that the lexi-calization debate could benefit from the inclusionof more lexicalized parsing models.9 AcknowledgmentsThis work was supported by the ANR Sequoia(ANR-08-EMER-013).
We heartily thank A.Arun, J. van Genabith an N. Schluter for kindlyletting us use our parsers on their treebanks.Thanks to the anonymous reviewers for their com-ments.
All remaining errors are ours.
We thank J.Wagner for his help and we would like to acknowl-edge the Centre for Next Generation Localization(www.cngl.ie) for providing access to one of itshigh-memory nodes.ReferencesAnne Abeill?
and Nicolas Barrier.
2004.
Enrich-ing a french treebank.
In Proceedings of LanguageRessources and Evaluation Conference (LREC), Lis-bon.Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel,2003.
Building a Treebank for French.
Kluwer,Dordrecht.Abhishek Arun and Frank Keller.
2005.
Lexicalizationin crosslinguistic probabilistic parsing: The case offrench.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics, pages 306?313, Ann Arbor, MI.Daniel M. Bikel.
2002.
Design of a multi-lingual,parallel-processing statistical parsing engine.
InProceedings of the second international conferenceon Human Language Technology Research, pages178?182.
Morgan Kaufmann Publishers Inc. SanFrancisco, CA, USA.Daniel M. Bikel.
2004.
Intricacies of Collins?
ParsingModel.
Computational Linguistics, 30(4):479?511.E.
Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A proce-dure for quantitatively comparing the syntactic cov-erage of english grammars.
In Proceedings of theDARPA Speech and Natural Language Workshop,pages 306?311, San Mateo (CA).
Morgan Kaufman.Aoife Cahill, Michael Burke, Ruth O?Donovan, Josefvan Genabith, and Andy Way.
2004.
Long-DistanceDependency Resolution in Automatically AcquiredWide-Coverage PCFG-Based LFG Approximations.In Proceedings of the 42nd Annual Meeting of theAssociation for Computational Linguistics, pages320?327, Barcelona, Spain.Xavier Carreras, Mickael Collins, and Terry Koo.2008.
TAG, dynamic programming, and the percep-tron for efficient, feature-rich parsing.
In Proceed-ings of the Twelfth Conference on ComputationalNatural Language Learning (CoNLL), pages 9?16.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL 2005), Ann Arbor (MI).Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st AnnualMeeting of the North American Chapter of the ACL(NAACL), Seattle.160David Chiang and Daniel M. Bikel.
2002.
Recover-ing latent information in treebanks.
In Proceedingsof COLING?02, 19th International Conference onComputational Linguistics, Taipei, Taiwan, August.David Chiang, 2003.
Statistical Parsing with an Auto-matically Extracted Tree Adjoining Grammar, chap-ter 16, pages 299?316.
CSLI Publications.Michael Collins.
1999.
Head Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia.Benoit Crabb?
and Marie Candito.
2008.
Exp?riencesd?analyse syntaxique statistique du fran?ais.
InActes de la 15?me Conf?rence sur le Traitement Au-tomatique des Langues Naturelles (TALN?08), pages45?54, Avignon, France.Mary Dalrymple.
2001.
Lexical-Functional Grammar,volume 34 of Syntax and Semantics.
San Diego,CA; London.
Academic Press.Amit Dubey and Frank Keller.
2003.
Probabilis-tic parsing for german using sister-head dependen-cies.
In In Proceedings of the 41st Annual Meet-ing of the Association for Computational Linguis-tics, pages 96?103.Ane Dybro-Johansen.
2004.
Extraction automatiquede grammaires ?
partir d?un corpus fran?ais.
Mas-ter?s thesis, Universit?
Paris 7.Mark Johnson.
1998.
PCFG models of linguis-tic tree representations.
Computational Linguistics,24(4):613?632.Aravind K. Joshi.
1987.
Introduction to tree adjoininggrammar.
In A. Manaster-Ramer, editor, The Math-ematics of Language.
J. Benjamins.R.
Kaplan and J. Bresnan.
1982.
Lexical-functionalgrammar: A formal system for grammarical repre-sentation.
In J. Bresnan, editor, The Mental Repre-sentation of Grammatical Relations, pages 173?281.Mass.
: MIT Press.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Asso-ciation for Computational Linguistics Morristown,NJ, USA.Sandra K?bler, Erhard W. Hinrichs, and WolfgangMaier.
2006.
Is it really that difficult to parse ger-man?
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Process-ing, pages 111?119, Sydney, Australia, July.
Asso-ciation for Computational Linguistics.Dekang Lin.
1995.
A dependency-based method forevaluating broad-coverage parsers.
In InternationalJoint Conference on Artificial Intelligence, pages1420?1425, Montreal.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotatedcorpus of english: The penn treebank.
Computa-tional Linguistics, 19(2):313?330.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of the Human Language TechnologyConference of the NAACL, Main Conference, pages152?159, New York City, USA, June.
Associationfor Computational Linguistics.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associ-ation for Computational Linguistics, Sydney, Aus-tralia, July.
Association for Computational Linguis-tics.Ines Rehbein and Josef van Genabith.
2007.
Tree-bank annotation schemes and parser evaluation forgerman.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), Prague.Philip Resnik.
1992.
Probabilistic tree-adjoininggrammars as a framework for statistic natural lan-guage processing.
COLING?92, Nantes, France.F.
Sangati and W. Zuidema.
2009.
Unsupervisedmethods for head assignments.
In Proceedings ofthe 12th Conference of the European Chapter of theACL (EACL 2009), pages 701?709, Athens, Greece.Association for Computational Linguistics.Y.
Schabes and R.C.
Waters.
1995.
Tree InsertionGrammar: Cubic-Time, Parsable Formalism thatLexicalizes Context-Free Grammar without Chang-ing the Trees Produced.
Computational Linguistics,21(4):479?513.Yves Schabes.
1992.
Stochastic Lexicalized Tree Ad-joining Grammars.
In Proceedings of the 14th con-ference on Computational linguistics, pages 425?432, Nantes, France.
Association for ComputationalLinguistics.Natalie Schluter and Josef van Genabith.
2007.Preparing, restructuring, and augmenting a frenchtreebank: Lexicalised parsers or coherent treebanks?In Proceedings of PACLING 07.Natalie Schluter and Josef van Genabith.
2008.Treebank-based acquisition of lfg parsing resourcesfor french.
In European Language Resources As-sociation (ELRA), editor, Proceedings of the SixthInternational Language Resources and Evaluation(LREC?08), Marrakech, Morocco, may.Daniel Zeman and Philip Resnik.
2008.
Cross-language parser adaptation between related lan-guages.
In Proceedings of IJCNLP 2008 Work-shop on NLP for Less Privileged Languages, Haj-dar?b?du, India.161
