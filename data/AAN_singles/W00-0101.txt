Sentences vs.
Phrases: Syntactic Complexity in MultimediaInformation RetrievalSharon Flankemotion, Inc.2600 Park Tower Dr., Ste.
600, Vienna, VA 22180sharon.flank@emotion.cornAbstractIn experiments on a natural languageinformation retrieval system that retrievesimages based on textual captions, we showthat syntactic complexity actually aidsretrieval.
We compare two types ofcaptioned images, those characterized withfull sentences in English, and thosecharacterized by lists of words and phrases.The full-sentence captions show a 15%increase in retrieval accuracy over the word-list captions.
We conclude that the syntacticcomplexity may be of use in fact because itdecreases semantic ambiguity: the word-listcaptions may be syntactically simple, butthey are semantically confusingly complex.1 IntroductionIn this paper, we describe experimentsconducted on an image retrieval system,PictureQuest, which uses text captions tocharacterize images.
The text captions areof two types.
Optimally, they consist of aprose description of the image, generallytwo to three sentences, with perhaps three orfour additional words or phrases thatdescribe emotional or non-literal imagecontent, e.g.Two little girls play with blocks.
Theyounger girl, wearing a blue shirt, laughsand prepares to knock over the tower thatthe older girl has constructed The oldergirl, dressed in a red shirt, winces inanticipation.Siblings, cooperation, rivalrySome of the captions in PictureQuest are notas well-behaved.
They may contain legacydata or data shared with a keyword-retrievalsystem.
They are optimized for exact-matchretrieval, and, as such, consist of lists ofwords or, at best, a few short phrases mixedin with long lists of words.
The same imagemight appear with the following caption:girl, girls, little girl little girls, block,blocks, play, playing, plays, blue, red, shirt,tower, knock; over, construct, construction,siblings, cooperation, rivalryPictureQuest relies on several naturallanguage processing techniques to enhanceretrieval accuracy.
It contains a part-of-speech tagger, morphological nalyzer, nounphrase pattern matcher, semantic expansionbased on WordNet, and special processingfor names and locations.
These have beentuned to perform most effectively on captiontext of the first type, i.e.
sentences.
Thefollowing chart illustrates how theselinguistic processes operate - or fail tooperate - on syntactic units.Tagger dog-N herding-V sheep-Ndog-N,V; herding-N,V; sheep-NMorphology dog herd-ING sheep (same)NP Pattems small child wearing a small, child, wearing, hathat green, swirls (modifiers de-coupled from headgreen swirls nouns)cat jumping into the air:cat-N (7 senses)jumping-V (13 senses)air-N (13 senses)SemanticExpansion(WordNet-based)cat, jumping, aircat-N,V (9 senses)jumping-N,V,Adj (16 senses)air-N,V,Adj (20 senses)Names George Bush, A1 Gore George, Bush, AI, Gore (matches bush, gore)Locations Arlington, Virginia Arlington, Virginia (matches other Arlingtons inNew England other states)New, England (matches England, new)2 Complexity Measures2.1 Competing Complexity MeasuresHow do we determine what syntacticcomplexity is?
Does it relate to depth?Nesting?
Various definitions have beenused in the various research communities:Alzheimer's research, normal and abnormalchild language acquisition, speech andhearing, English teaching, second languageteaching and acquisition, and theoreticallinguistics of various persuasions (see, e.g.,MacDonald 1997; Rosen 1974; Bar-Hillel etal.
1967).
Fortunately, for the purposes ofour investigation, we are dealing with broaddistinctions that would foster agreementeven among those with different definitionsof complexity.
For the captioned ata, inone case, the data are in full sentences.
Theaverage sentence l ngth is approximately tenwords, and the average number of sentencesis between two and three.
In the other case,the data are either in lists of single words, orin lists of single words with a few two-wordor three-word phrases included, but with nosentences whatsoever.
Regardless of theexact measure of syntactic omplexity used,it is clear that sentences are syntacticallymore complex than word lists or even phraselists.2.2 Query ComplexityThe standard query length for Webapplications i between two and three words,and our experience with PictureQuestconfirms that observation.
In comparisonswith other text-based image retrievalapplications, including keyword systems,query complexity is important: one-wordqueries work equally well on keywordsystems and on linguistically-enhancednatural language processing systems.
Thedifference comes with longer queries, and inparticular with syntactic phrases.
(Booleanthree-word queries, e.g.
A and B; A or B, donot show much difference.)
The morecomplex queries (and, in fact, the queriesthat show PictureQuest off to bestadvantage) consist either of a noun phrase orare of the form NP V-ing NP.
The tablebelow summarizes the differences in querycomplexity for natural anguage informationretrieval as compared to keyword-onlyinformation retrieval.2one word, e.g.zlephantBoolean, e.g.
rhino9r rhinocerosNP V-ing NP, e.g.girl leading ahorsenoun phrase, e.g.black woman in awhite hatBoth are equally goodBoth are equally good,assuming they bothrecognize the meaning ofthe Boolean operatorNLIR shows someimprovementNLIR shows majorimprovement; keywordretrieval scramblesmodifiers randomly2.3 Semantic ComplexitySemantic complexity is more difficult toevaluate, but we can make certainobservations.
Leaving noun phrases intactmakes a text more semantically complexthan deconstructing those noun phrases:rubber baby buggy bumpers is moresemantically complex than a simple list ofnouns and attributes, ince there are variousmodification ambiguities in the longerversion that are not present once it has beenreduced to rubber, baby buggy, bumpers (orrubber, baby, buggy, bumpers, for thatmatter).As for the names of people and locations,one could argue that the intact syntacticunits (AI Gore; George Bush; Arlington,Virginia; New England) are semanticallysimpler, since they resolve ambiguity andeliminate the spurious readings gore, bush,Arlington \[Massachusetts\], new England.Nonetheless, we would argue that they aresyntactically more complex when intact.The PictureQuest system uses a WordNet-based semantic net to expand the captiondata.
To some extent, the syntacticmeasures (part-of-speech tagging, nounphrase pattern matching, name and locationidentification) serve to constrain thesemantic expansion, since they eliminatesome possible semantic expansions based onsyntactic factors.
One could interpret heword-list captions, then, not as syntacticallyless complex, but rather as semantically essconstrained, therefore more ambiguous andthus more complex.
This view would,perhaps, restore the more intuitive notionthat complexity should lead to worse ratherthan better esults.3 ExperimentsWhile the sentence captions are syntacticallymore complex, by almost any measure, theycontain more information than the legacyword list captions.
Specifically, the part-of-speech tagger and the noun phrase patternmatcher are essentially useless with theword lists, since they rely on syntacticpatterns that are not present.
We thereforehypothesized that our retrieval accuracywould be lower with the legacy word listcaptions than with the sentence captions.We performed two sets of experiments, onewith legacy word list captions and the otherwith sentence captions.
Fortunately, thecorpus can be easily divided, since it ispossible to select image providers witheither full sentence or word list captions, andlimit the search to those providers.
In orderto ensure that we did not introduce a biasbecause of the quality of captioning for aparticular provider, we aggregated scoresfrom at least hree providers in each test.Because the collection is large and live, andincludes ranked results, we selected amodified version of precision at 20 ratherthan a manual gold standard precision/recalltest.
We chose this evaluation path for thefollowing reasons:?
Ranking image relevance was difficultfor humans?
The collection was large and live, i.e.changing daily?
The modified measure more accuratelyreflected user evaluations-" 3We performed experiments initially withmanual ranking, and found that it wasimpossible to get reliable cross-coderjudgements for ranked results.
That is, wecould get humans to assess whether animage should or should not have beenincluded, but the rankings did not yieldagreement.
Complicating the problem wasthe fact that we had a large collection(400,000+ images), and creating a testsubset meant that most queries wouldgenerate almost no relevant results.
Finally,we wanted to focus more on precision thanon  recall, because our work with users hadmade it clear that precision was far moreimportant in this application.To evaluate precision at 20 for thiscollection, we used the crossing measureintroduced in Flank 1998.
The crossingmeasure (in which any image ranked aboveanother, better-matching image counts as anerror) is both finer-grained and better suitedto a ranking application in which userevaluations are not binary.
We calibratedthe crossing measure (on a subset of thequeries) as follows:Precision at 20 Images forAll Terms53Precision at 5 Images for All 59TermsPrecision at 20 Images for 100Any TermCrossing Measure at 20 91i Images IThat is, we calculated the precision "for allterms" as a binary measure with respect to aquery, and scored an error if any terms in thequery were not matched.
For the "any term"precision measure, we scored an error onlyif the image failed to match any term in thequery in such a way that a user wouldconsider it a partial match.Thus, for example, for an "all terms" match,tall glass of beer succeeded only when theimages howed (and captions mentioned) allthree terms tall, glass, and beer, or theirsynonyms.
For an "any-term" match, tall orglass or beer or a direct synonym wouldneed to be present (but not, say, glasses).
(For two of the test queries, fewer than 20images were retrieved, so the measure is,more precisely, R-precision: precision at thenumber of documents retrieved or at 20 or 5,whichever is less.4 ResultsWe found a statistically significantdifference in retrieval quality between thesyntactically simple word list captions andthe syntactically complex sentence captions.The word list captions cored 74.6% on ourcrossing measure, while the sentencecaptions cored 89.5%.We performed one test comparing one-wordand two-word queries on sentence versusword list captions.
The sentence captionsshowed little difference: 82.7% on the one-word queries, and 80% on the two-wordqueries.
The word-list captions, however,were dramatically worse on two-wordqueries (70.5%) than on one-word queries(89.7%).Overall 74.6% 89.5%1-word 89.7% 82.7%2-word 7015% 80%5 ConclusionOur experiments indicate that, in aninformation retrieval system tuned torecognize and reward matches usingsyntactic information, syntactic omplexityyields better results than syntactically4mixed-up "word salad."
One can interpretthese results from a semantic complexitystandpoint, since the syntactically simplecaptions all include considerably moresemantic ambiguity, unconstrained as theyare from a syntactic standpoint.
Thisobservation leads us to an additionalconclusion about the relationship betweensyntactic and semantic complexity: in thisinstance, at least, the relationship is inverserather than direct.
The word-list captionsare syntactically simple but, as a result,since syntactic factors are not available tolimit ambiguity, semantically more complexthan the same information presented in amore syntactically complex fashion, i.e.
insentences.6 ReferencesBar-Hillel, Y., A. Kasher and E. Shamir 1967.
"Measures of Syntactic Complexity," in MachineTranslation, A.D. Booth, ed.
Amsterdam: North-Holland, pp.
29-50.Flank, Sharon, 1998.
"A Layered Approach to NLP-Based Information Retrieval," in Proceedings ofCOLING-ACL, 36th Annual Meeting of theAssociation for Computational Linguistics, Montreal,Canada, 10-14 August 1998.MacDonald, M.C.
1997.
Language and CognitiveProcesses: Special Issue on Lexical Representationsand Sentence Processing, 12, pp.
121-399.Rosen, B.K.
1974.
"Syntactic Complexity," inInformation and Control 24, pp.
305-335.
