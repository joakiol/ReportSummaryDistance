Hypertext Authoring for Linking Relevant Segments ofRelated Instruction ManualsHi rosh i  Nakagawa and Tatsunor i  Mor i  and  Nobuyuk i  Omor i  and  Jun  OkamuraDepar tment  of  Computer  and  E lec t ron ic  Eng ineer ing ,  Yokohama Nat iona l  Un ivers i tyTok iwada i  79-5, Hodogaya ,  Yokohama,  240-8501, JAPANE- mail :  nakagawa@ n aklab,  dnj .
ynu.
ac.j p, { mor i ,  ohmor i  ,j un  } @forest.
dnj.
ynu .
ac.j pAbst ractRecently manuals of industrial products becomelarge and often consist of separated volumes.
Inreading such individual but related manuals, wemust consider the relation among segments, whichcontain explanations of sequences of operation.
Inthis paper, we propose methods for linking relevantsegments in hypertext authoring of a set of relatedmanuals.
Our method is based on the similaritycalculation between two segments.
Our experimen-tal results show that the proposed method improvesboth recall and precision comparing with the con-ventional t f .
idf based method.1 In t roduct ionIn reading traditional paper based manuals, weshould use their indices and table of contents in or-der to know where the contents we want to know arewritten.
In fact, it is not an easy task especially fornovices.
Recent years, electronic manuals in a formof hypertext like Help of Microsoft Windows becamewidely used.
Unfortunately it is very expensive tomake a hypertext manual by hand especially in caseof a large volume of manual which consists of sev-eral separated volumes.
In a case of such a largemanual, the same topic appears at several places indifferent volumes.
One of them is an introductoryexplanation for a novice.
Another is a precise ex-planation for an advanced user.
It is very useful tojump from one of them to another of them directlyby just clicking a button of mouse in reading a man-ual text on a browser like NetScape.
This type ofaccess is realized by linking them in hypertext for-mat by hypertext authoring.Automatic hypertext authoring has been focusedon in these years, and much work has been done.
Forinstance, Basili et al (1994) use document struc-tures and semantic information by means of naturallanguage processing technique to set hyperlinks onplain texts.The essential point in the research of automatichypertext authoring is the way to find semanticallyrelevant parts where each part is characterized bya number of key words.
Actually it is very similarwith information retrieval, IR henceforth, especiallywith the so called passage retrieval (Salton et al,1993).
J.Green (1996) does hypertext authoring ofnewspaper articles by word's lexical chains which arecalculated using WordNet.
Kurohashi et al (1992)made a hypertext dictionary of the field of infor-mation science.
They use linguistic patterns thatare used for definition of terminology as well as the-saurus based on words' similarity.
Furner-Hines andWillett (1994) experimentally evaluate and comparethe performance of several human hyper linkers.
Ingeneral, however, we have not yet paid enough at-tention to a full-automatic hyper linker system, thatis what we pursue in this paper.The new ideas in our system are the followingpoints:1.
Our target is a multi-volume manual that de-scribes the same hardware or software but is dif-ferent in their granularity of descriptions fromvolume to volume.2.
In our system, hyper links are set not betweenan anchor word and a certain part of text butbetween two segments, where a segment is asmallest formal unit in document, like a sub-subsection of ~TEX if no smaller units likesubsubsubsection are used.3.
We find pairs of relevant segments over twovolumes, for instance, between an introductorymanual for novices and a reference manual foradvanced level users about the same software orhardware.4.
We use not only t f .
idf  based vector space modelbut also words' co-occurrence information tomeasure the similarity between segments.2 S imi la r i ty  Ca lcu la t ionWe need to calculate a semantic similarity betweentwo segments in order to decide whether two of themare linked, automatically.
The most well knownmethod to calculate similarity in IR is a vector spacemodel based on t f  ?
idf value.
As for idf, namelyinverse document frequency, we adopt a segment in-929stead of document in the definition of idf.
The def-inition of idf in our system is the following.of segments in the manualidf(t) = log ~ of segments in which t occurs + 1Then a segment is described as a vector in a vectorspace.
Each dimension of the vector space consistsof each term used in the manual.
A vector's valueof each dimension corresponding to the term t isits t f  ?
idf value.
The similarity of two segments isa cosine of two vectors corresponding to these twosegments respectively.
Actually the cosine measuresimilarity based on t f .
idf is a baseline in evaluationof similarity measures we propose in the rest of thissection.As the first expansion of definition of t f  ?
idf, weuse case information of each noun.
In Japanese, caseinformation is easily identified by the case particlelike ga( nominal marker ), o( accusative marker ),hi( dative marker ) etc.
which are attached just af-ter a noun.
As the second expansion, we use not onlynouns (+ case information) but also verbs becauseverbs give important information about an action auser does in operating a system.
As the third expan-sion, we use co-occurrence information of nouns andverbs in a sentence because combination of nounsand a verb gives us an outline of what the sentencedescribes.
The problem at this moment is the wayto reflect co-occurrence information in t f .
idf basedvector space model.
We investigate two methods forthis, namely,1.
Dimension expansion of vector space, and2.
Modification of t f  value within a segment.In the following, we describe the detail of these twomethods.2.1 D imens ion  Expans ionThis method is adding extra-dimensions into thevector space in order to express co-occurrence in-formation.
It is described more precisely as the fol-lowing procedure.1.
Extracting a case information (case particle inJapanese) from each noun phrase.
Extracting averb from a clause.2.
Suppose be there n noun phrases with a caseparticle in a clause.
Enumerating every combi-nation of 1 to n noun phrases with case particle.12Then we have E nCk combinations.6=13.
Calculating t f  ?
idf for every combination withthe corresponding verb.
And using them as newextra dimensions of the original vector space.For example, suppose a sentence "An end userlearns the programming language."
Then in ad-dition to dimensions corresponding to every nounphrase like "end user", we introduce the new di-mensions corresponding to co-occurrence informa-tion such as:?
(VERB, learn) (NOMNINAL end user) (AC-CUSATIVE programming language)?
(VERB, learn) (NOMNINAL end user)?
(VERB, learn) (ACCUSATIVE programminglanguage)We calculate t f .
idf of each of these combinationsthat is a value of vector corresponding to each ofthese combinations.
The similarity calculation basedon cosine measure is done on this expanded vectorspace.2.2 Mod i f i ca t ion  of  t f  va lueAnother method we propose for reflecting co-occurrence information to similarity is modificationof t f  value within a segment.
(Takaki and Kitani,1996) reports that co-occurrence of word pairs con-tributes to the IR performance for Japanese newspaper articles.In our method, we modify t f  of pairs of co-occurred words that occur in both of two segments,say dA and dB, in the following way.
Suppose that aterm tk, namely noun or verb, occurs f times in thesegment da.
Then the modified tf'(da, tk) is definedas the following formula.tf'(dA, tk) = t f(da, tk)1+ Z E cw(dA,tk,p, tc)teETc(tk,da,dB)P =11"}- E E Cw'(da,tk,p, tc)tcGTc( tk ,dA,dB ) P =1where cw and cw' are scores of importance for co-occurrence of words, tk and t~.
Intuitively, cw andcw' are counter parts of t f .
idf for co-occurrence ofwords and co-occurrence of (noun case-information),respectively, cw is defined by the following formula.cw(dA, tk, p, to)a(dA,~k,p,t~) X ~(tk,t~) X 7(tk,/c) X CM(dA)where c~(da, tk, p, to) is a function expressing hownear tkand t~ occur, p denotes that pth tk's occur-rence in the segment dA, and fl(tk,t?)
is a normal-ized frequency of co-occurrence of ?~ and ?~.
Eachof them is defined as follows.a(dA, tk, p, t~) = d(dA, tk, p) - dist(dA, tk, p, t~)d(dA, tk, p)930rtf(t~,t?
)~( tk , t~) -  atf(tk)where the function dist(da, tk,p, to) is a distancebetween pth t~ within da and tc counted by word.d(da,tk,p) shows the threshold of distance withinwhich two words are regarded as a co-occurrence.Since, in our system, we only focus on co-occurrenceswithin a sentence, a(da,tk,p,t~) is calculated forpairs of word occurrences within a sentence.
As aresult, d(dA,tk,p) is a number of words in a sen-tence we focus on.
atf(tk) is a total number oftk's occurrences within the manual we deal with.rtf(tk,  t~) is a total number of co-occurrences of tkand tc within a sentence.
7(t~, to) is an inverse doc-ument frequency ( in this case "inverse segment fre-quency") of te which co-occurs with tk, and definedas follows.N7(tk, fc) = lOg( d-~c ) )where N is a number of segments in a manual,and dr(to) is a number segments in which tc occurswith tk.M(da) is a length of segment da counted in mor-phological unit, and used to normalize cw.
C is aweight parameter for cw.
Actually we adopt thevalue of C which optimizes 1 lpoint precision as de-scribed later.The other modification factor cw' is defined in al-most the same way as cw is.
The difference betweencw and cw' is the following, cw is calculated foreach noun.
On the other hand, cw' is calculated foreach combination of noun and its case information.Therefore, cw I is calculated for each ( noun, case )like (user, NOMINAL).
In other words, in calcula-tion of cw', only when ( noun-l, case-1 ) and ( noun-2, case-2 ), like (user NOMINAL) and (program AC-CUSATIVE), occur within the same sentence, theyare regarded as a co-occurrence.Now we have defined cw and cw'.
Then back tothe formula which defines t f ' .
In the definition oftf ' ,  Tc(tk, dA, dB) is a set of word which occur inboth of dA and dB.
Therefore cws and cw's aresummed up for all occurrences of tk in dA.
Namelywe add up all cws and cw% whose tc is included inT~(tk, dA, dn) to calculate t f ' .3 Implementation a d ExperimentalResultsOur system has the following inputs and outputs.Input  is an electronic manual text which can bewritten in plain text,I~TEXor HTML)Output  is a hypertext in HTML format.Electronic Manuals manual A manual BWO~as-~red2o~S ~ Ke),word~xtra~ =.
.
.
.
"4 t f  i~\[cutatlon,Slrnllafl~/Calculationbased on Vector Space Mode1\[ Hypeaext Unk GenaratorI OUTPUTHYPERTEXT~ orphological Ana~sSystemmanual A manual BFigure h Overview of our hypertext generatorWe need a browser like NelScape that can displaya text written in HTML.
Our system consists of foursub-systems shown in Figure 1.Keyword  Ext ract ion  Sub-System In this sub-system, a morphological nalyzer segments outthe input text, and extract all nouns and verbsthat are to be keywords.
We use Chasen 1.04b(Matsumoto et al, 1996) as a morphologicalanalyzer for Japanese texts.
Noun and Case-information pairs are also made in this sub-system.
If you use the dimension expansion de-scribed in 2.1, you introduce new dimensionshere.t f -  id f  Ca lcu lat ion  Sub-SystemThis sub-system calculates t f  ?
idf of extractedkeywords by Keyword Extraction Sub-System.S imi lar i ty  Ca lcu la t ion  Sub-System This sub-system calculates the similarity that is repre-sented by cosine of every pair of segments basedon t f  ?
idf values calculated above.
If you usemodifications of t f  values described in 2.2, youcalculated modified tf ,  namely t f '  in this sub-system.Hyper text  Generator  This sub-system trans-lates the given input text into a hypertext inwhich pairs of segments having high similarity,say high cosine value, are linked.
The similarityof those pairs are associated with their links foruser friendly display described in the followingWe show an example of display on a browser inFigure 2.
The display screen is divided into fourparts.
The upper left and upper right parts showa distinct part of manual text respectively.
In thelower left (right) part, the title of segments thatare relevant o the segment displayed on the upperleft (right) part are displayed in descending order of9311FS-Ze FA i r  V~w Go Booka~pa 0pt~orm D~ZU3ry  WJz~:l~ H~p.__v_J- -2_J .-.I mLocat ion :  I Ihtt~ : / /~ .
fo res t ,  dr,,j.
Ynu.
,etc.
5p/+SuxVjum_ch~frame+ htqL~~hat" s ~1 ~t '~ ~ ?
1  I ks t lnat i ?ns l  Net Search I l~op l?
l  So f t , z re  IE JUMAN ~ - -ChaSen 1 .0  ' r ' ~ .
6 r ~l -  - -  J b~R~L~ t~ ~ =k ~ t~.ANSttt l  L ,~ .- t -?
P JUM AN 2~l ;PJ '~ JUM~N 3~) ~.TrF JUMAN 2 .0  7)'+~>JUMAN 3 .0  , r ' , , .
.CT '~:~m.r : .~9~o~8~gn~- i'a.
-~ l 't  L: "~ I, ?
35 l~l.~'~"lt!$ L < I l I~ ' tF  ?, I  I_ ~, :X ,_,Figure 2: The use of this systemsimilarity.
Since these titles are linked to the cor-responding segment text, if we click one of them inthe lower left (right) part, the hyperlinked segment'stext is instantly displayed on the upper right (left)part, and its relevant segments' title are displayedon the lower right (left) part.
By this type of brows-ing along with links displayed on the lower parts,if a user wants to know relevant information aboutwhat she/he is reading on the text displayed on theupper part, a user can easily access the segments inwhich what she/he wants to know might be writtenin high probability.Now we describe the evaluation of our proposedmethods with recall and precision defined as follows.recall = ~ of retrieved pairs of relevant segmentsprec is ion=of pairs of relevant segmentsof retrieved pairs of relevant segmentsII of retrieved pairs of segmentsThe first experiment is done for a large manualof APPGALLARY(Hitachi, 1995) which is 2.5MBlarge.
This manual is divided into two volumes.
Oneis a tutorial manual for novices that contains 65 seg-ments.
The other is a help manual for advancedusers that contains 2479 segments.
If we try to findthe relevant segments between ones in the tutorialmanual and ones in the help manual, the number ofpossible pairs of segments i 161135.
This numberis too big for human to extract all relevant segmentmanually.
Then we investigate highest 200 pairs ofsegments by hand, actually by two students in theengineering department of our university to extractpairs of relevant segments.
The guideline of selectionof pairs of relevant segments i :0.9080.70.60.504030.20 .
t0Precision - - -Reca l l  - -20  40  60  80 100 120 140 t60  180 200R a n k ~Figure 3: Recall and precision of generated hyper-links on large-scale manualsTable 1: Manual combinations and number of rightcorrespondences of segmentspairofm  uals , ,AoB AO+ BO+of all pairs II 1056 896 924of relevant pairs 65 60 471.
Two segments explain the same operation or thesame terminology.2.
One segment explains an abstract concept andthe other explains that concept in concrete op-eration.Figure 3 shows tim recall and precision for num-bers of selected pairs of segments where those pairsare sorted in descending order of cosine similarityvalue using normal t f  ?
idf of all nouns.
Tiffs resultindicates that pairs of relevant segments are concen-trated in high similarity area.
In fact, the pairs ofsegments within top 200 pairs are almost all relevantones.The second experiment is done for threesmall manuals of three models of video cas-sette recorder(MITSUBISHI, 1995c; MITSUBISHI,1995a; MITSUBISHI, 1995b) produced by the samecompany.
We investigate all pairs of segmentsthat appear in the distinct manuals respectively,and extract relevant pairs of segment accordingto the same guideline we did in the first experi-ment by two students of the engineering depart-ment of our university.
The numbers of segmentsare 32 for manual A(MITSUBISHI, 1995c), 33 formanual B(MITSUBISHI, 1995a) and 28 for manualC(MITSUBISHI, 1995b), respectively.
The numberof relevant pairs of segments are shown ill Table 1.We show the 11 points precision averages for thesemethods in Table 2.
Each recall-precision curve,say Keyword, dimension N, cw+cw' tf, and NormalQuery, corresponds to the methods described in theprevious ection.
We describe the more precise defi-nition of each in the following.932Table 2: 11 point average of precision for eachmethod and combinationMethod ACVB A?~C BvvCKeyword 0.678 0.589 0.549cw+cw' tf 0.683 0.625 0.582C 0.1 0.6 1.3dimension N 0.684 0.597 0.556Normal Query 0 .692  0.532 0.395Keyword: Using t f .
idf for all nouns and verbsoccuring in a pair of manuals.
This is the baselinedata.dimension N: Dimension Expansion method de-scribed in section 2.1.
In this experiment, we useonly noun-noun co-occurrences.cw+cw'  tf: Modification of t f  value method de-scribed in section2.2.
In this experiment, we useonly noun-verb co-occurrences.Normal Query: This is the same as Keyword ex-cept that vector values in one manual are all set to0 or 1, and vector values of the other manual aret f  .
id/.In the rest of this section, we consider the resultsshown above point by point.The effect of using t f .
idf information of bothsegmentsWe consider the effect of using t f .
idf of two seg-ments that we calculate similarity.
For comparison,we did the experiment Normal Query where t f .
idfis used as vector value for one segment and 1 or 0is used as vector value for the other segment.
Thisis a typical situation in IR.
In our system, we calcu-late similarity of two segments .already given.
Thatmakes us possible using t f  ?
idf for both segments.As shown in Table 2, Keyword outperforms Nor-mal Query.The effect of using co-occurrence informationThe same types of operation are generally de-scribed in relevant segments.
The same type ofop-eration consists of the same action and equipmentin high probability.
This is why using co-occurrenceinformation in similarity calculation magnifies im-ilarities between relevant segments.
Comparing di-mension expansion and modification of t f ,  the latteroutperforms the former in precision for almost allrecall rates.
Modification of t f  value method alsoshows better esults than dimension expansion i  11point precision average shown in Table 2 for A-Cand B-C manual pairs.
As for normalization factorC of modification of t f  value method, the smallerC becomes, the less t f  value changes and the moresimilar the result becomes with the baseline ase inwhich only t f  is used.
On the contrary, the bigger Cbecomes, the more incorrect pairs get high similar-ity and the precision deteriorates in low recall area.As a result, there is an optimum C value, which weselected experimentally foreach pair of manuals andis shown in Table 2 respectively.4 Conc lus ionsWe proposed two methods for calculating similarityof a pair of segments appearing in distinct manuals.One is Dimension Expansion method, and the otheris Modification of t f  value method.
Both of themimprove the recall and precision in searching pairs ofrelevant segment .This type of calculation of similar-ity between two segments i useful in implementinga user friendly manual browsing system that is alsoproposed and implemented in this research.Re ferencesRoberto Basili, Fabrizio Grisoli, and Maria TeresaPazienza.
1994.
Might a semantic lexicon supporthypertextual uthoring?
In 4th ANLP, pages174-179.David Elhs.
Jonathan Furner-Hines and Peter Wil-lett.
1994.
On the measurement of inter-linkerconsistency and retrieval effectiveness in hyper-text databases.
In SIGIR '94, pages 51-60.Hitachi, 1995.
How to use the APPGALLERY,APPGALLERY On-Line Help.
Hitachi Limited.Stephen J.Green.
1996.
Using lexcal chains to buildhypertext links in newspaper articles.
In Proceed-ings of AAAI  Workshop on Knowledge Discoveryin Databases, Portland, Oregon.S.
Kurohashi, M. Nagao, S. Sato, and M. Murakami.1992.
A method of automatic hypertext construc-tion from an encyclopedic dictionary of a specificfield.
In 3rd ANLP, pages 239-240.Yuji Matsumoto, Osamu Imaichi, Tatsuo Ya-mashita, Akira Kitauchi, and Tomoaki Imamura.1996.
Japanese morphological analysis systemChaSen manual (version 1.0b4).
Nara Institute ofScience and Technology, Nov.MITSUBISHI, 1995a.
MITSUBISHI Video TapeRecorder HV-BZ66 Instruction Manual.MITSUBISHI, 1995b.
MITSUBISHI Video TapeRecorder HV-F93 Instruction Manual.MITSUBISHI, 1995c.
MITSUBISHI Video TapeRecorder HV-FZ62 Instruction Manual.Gerard Salton, J. Allan, and Chris Buckley.
1993.Approaches to passage retrieval in full text infor-mation systems.
In SIGIR '93, pages 49-58.Toru Takaki and Tsuyoshi Kitani.
1996.
Rele-vance ranking of documents using query word co-occurrences (in Japanese).
IPSJ SIG Notes 96-FI-41-8, IPS Japan, April.933
