Proceedings of BioNLP Shared Task 2011 Workshop, pages 7?15,Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational LinguisticsOverview of Genia Event Task in BioNLP Shared Task 2011Jin-Dong KimDatabase Center for Life Science2-11-16 Yayoi, Bunkyo-ku, Tokyojdkim@dbcls.rois.ac.jpYue WangDatabase Center for Life Science2-11-16 Yayoi, Bunkyo-ku, Tokyowang@dbcls.rois.ac.jpToshihisa TakagiUniversity of Tokyo5-1-5 Kashiwa-no-ha, Kashiwa, Chibatt@k.u-tokyo.ac.jpAkinori YonezawaDatabase Center for Life Science2-11-16 Yayoi, Bunkyo-ku, Tokyoyonezawa@dbcls.rois.ac.jpAbstractThe Genia event task, a bio-molecular eventextraction task, is arranged as one of the maintasks of BioNLP Shared Task 2011.
As its sec-ond time to be arranged for community-widefocused efforts, it aimed to measure the ad-vance of the community since 2009, and toevaluate generalization of the technology tofull text papers.
After a 3-month system de-velopment period, 15 teams submitted theirperformance results on test cases.
The re-sults show the community has made a sig-nificant advancement in terms of both perfor-mance improvement and generalization.1 IntroductionThe BioNLP Shared Task (BioNLP-ST, hereafter)is a series of efforts to promote a community-wide collaboration towards fine-grained informa-tion extraction (IE) in biomedical domain.
Thefirst event, BioNLP-ST 2009, introducing a bio-molecular event (bio-event) extraction task to thecommunity, attracted a wide attention, with 42 teamsbeing registered for participation and 24 teams sub-mitting final results (Kim et al, 2009).To establish a community effort, the organizersprovided the task definition, benchmark data, andevaluations, and the participants competed in devel-oping systems to perform the task.
Meanwhile, par-ticipants and organizers communicated to develop abetter setup of evaluation, and some provided theirtools and resources for other participants, making ita collaborative competition.The final results enabled to observe the state-of-the-art performance of the community on the bio-event extraction task, which showed that the auto-matic extraction of simple events - those with unaryarguments, e.g.
gene expression, localization, phos-phorylation - could be achieved at the performancelevel of 70% in F-score, but the extraction of com-plex events, e.g.
binding and regulation, was a lotmore challenging, having achieved 40% of perfor-mance level.After BioNLP-ST 2009, all the resources from theevent were released to the public, to encourage con-tinuous efforts for further advancement.
Since then,several improvements have been reported (Miwa etal., 2010b; Poon and Vanderwende, 2010; Vlachos,2010; Miwa et al, 2010a; Bjo?rne et al, 2010).For example, Miwa et al (Miwa et al, 2010b)reported a significant improvement with bindingevents, achieving 50% of performance level.The task introduced in BioNLP-ST 2009 was re-named to Genia event (GE) task, and was hostedagain in BioNLP-ST 2011, which also hosted fourother IE tasks and three supporting tasks (Kim et al,2011).
As the sole task that was repeated in the twoevents, the GE task was referenced during the devel-opment of other tasks, and took the role of connect-ing the results of the 2009 event to the main tasks of2011.
The GE task in 2011 received final submis-sions from 15 teams.
The results show the commu-nity made a significant progress with the task, andalso show the technology can be generalized to fullpapers at moderate cost of performance.This paper presents the task setup, preparation,and discusses the results.7Event Type Primary Argument Secondary ArgumentGene expression Theme(Protein)Transcription Theme(Protein)Protein catabolism Theme(Protein)Phosphorylation Theme(Protein) Site(Entity)Localization Theme(Protein) AtLoc(Entity), ToLoc(Entity)Binding Theme(Protein)+ Site(Entity)+Regulation Theme(Protein/Event), Cause(Protein/Event) Site(Entity), CSite(Entity)Positive regulation Theme(Protein/Event), Cause(Protein/Event) Site(Entity), CSite(Entity)Negative regulation Theme(Protein/Event), Cause(Protein/Event) Site(Entity), CSite(Entity)Table 1: Event types and their arguments for Genia event task.
The type of each filler entity is specified in parenthesis.Arguments that may be filled more than once per event are marked with ?+?.2 Task DefinitionThe GE task follows the task definition of BioNLP-ST 2009, which is briefly described in this section.For more detail, please refer to (Kim et al, 2009).Table 1 shows the event types to be addressed inthe task.
For each event type, the primary and sec-ondary arguments to be extracted with an event aredefined.
For example, a Phosphorylation event isprimarily extracted with the protein to be phospho-rylated.
As secondary information, the specific siteto be phosphorylated may be extracted.From a computational point of view, the eventtypes represent different levels of complexity.
Whenonly primary arguments are considered, the first fiveevent types in Table 1 are classified as simple eventtypes, requiring only unary arguments.
The Bind-ing and Regulation types are more complex: Bind-ing requires detection of an arbitrary number of ar-guments, and Regulation requires detection of recur-sive event structure.Based on the definition of event types, the entiretask is divided to three sub-tasks addressing eventextraction at different levels of specificity:Task 1.
Core event extraction addresses the ex-traction of typed events together with their pri-mary arguments.Task 2.
Event enrichment addresses the extrac-tion of secondary arguments that further spec-ify the events extracted in Task 1.Task 3.
Negation/Speculation detectionaddresses the detection of negations andspeculations over the extracted events.Task 1 serves as the backbone of the GE task and ismandatory for all participants, while the other twoare optional.The failure of p65 translocation to the nucleus ?Protein Localization Locationtheme ToLocNegatedFigure 1: Event annotation exampleFigure 1 shows an example of event annotation.The event encoded in the text is represented in astandoff-style annotation as follows:T1 Protein 15 18T2 Localization 19 32T3 Entity 40 46E1 Localization:T2 Theme:T1 ToLoc:T1M1 Negation E1The annotation T1 identifies the entity referredto by the string (p65) between the character offsets,15 and 18 to be a Protein.
T2 identifies the string,translocation, to refer to a Localization event.
Enti-ties other than proteins or event type references areclassified into a default class Entity, as in T3.
E1then represents the event defined by the three enti-ties, as defined in Table 1.
Note that for Task 1, theentity, T3, does not need to be identified, and theevent, E1, may be identified without specification ofthe secondary argument, ToLoc:T1:E1?
Localization:T2 Theme:T1Finding the full representation of E1 is the goal ofTask 2.
In the example, the localization event, E1,is negated as expressed in the failure of .
Finding thenegation, M1 is the goal of Task 3.8Training Devel TestItem Abs.
Full Abs.
Full Abs.
FullArticles 800 5 150 5 260 4Words 176146 29583 33827 30305 57256 21791Proteins 9300 2325 2080 2610 3589 1712Events 8615 1695 1795 1455 3193 1294Gene expression 1738 527 356 393 722 280Transcription 576 91 82 76 137 37Protein catabolism 110 0 21 2 14 1Phosphorylation 169 23 47 64 139 50Localization 265 16 53 14 174 17Binding 887 101 249 126 349 153Regulation 961 152 173 123 292 96Positive regulation 2847 538 618 382 987 466Negative regulation 1062 247 196 275 379 194Table 2: Statistics of annotations in training, development, and test sets3 Data preparationThe data sets are prepared in two collections: theabstract and the full text collections.
The abstractcollection includes the same data used for BioNLP-ST 2009, and is meant to be used to measure theprogress of the community.
The full text collectionincludes full papers which are newly annotated, andis meant to be used to measure the generalizationof the technology to full papers.
Table 2 shows thestatistics of the annotations in the GE task data sets.Since the training data from the full text collection isrelatively small despite of the expected rich varietyof expressions in full text, it is expected that ?gener-alization?
of a model from the abstract collection tofull papers would be a key technique to get a reason-able performance.A full paper consists of several sections includ-ing the title, abstract, introduction, results, conclu-sion, methods, and so on.
Different sections wouldbe written with different purposes, which may af-fect the type of information that are found in the sec-tions.
Table 3 shows the distribution of annotationsin different sections.
It indicates that event men-tions, according to the event definition in Table 1, inMethods and Captions are much less frequent thanin the other TIAB, Intro.
and R/D/C sections.
Fig-ure 2 illustrates the different distribution of anno-tated event types in the five sections.
It is notablethat the Methods section (depicted in blue) showsvery different distribution compared to others: whileGene_expressionTranscrip.BindingRegulationPos_regul.Neg_regul.TIAB Intro.
R/D/C Methods CaptionFigure 2: Event distribution in different sectionsRegulation and Positive regulation events are not asfrequent as in other sections, Negative regulation isrelatively much more frequent.
It may agree withan intuition that experimental devices, which will beexplained in Methods sections, often consists of ar-tificial processes that are designed to cause a nega-tive regulatory effect, e.g.
mutation, addition of in-hibitor proteins, etc.
This observation suggests a dif-ferent event annotation scheme, or a different eventextraction strategy would be required for Methodssections.9Full PaperItem Abstract Whole TIAB Intro.
R/D/C Methods CaptionWords 267229 80962 3538 7878 43420 19406 6720Proteins 14969 6580 336 597 3980 916 751(Density: P / W) (5.60%) (8.13%) (9.50%) (7.58%) (9.17%) (4.72%) (11.18%)Events 13603 4436 272 427 3234 198 278(Density: E / W) (5.09%) (5.48%) (7.69%) (5.42%) (7.51%) (1.02%) (4.14%)(Density: E / P) (90.87%) (67.42%) (80.95%) (71.52%) (81.93%) (21.62%) (37.02%)Gene expression 2816 1193 62 98 841 80 112Transcription 795 204 7 7 140 30 20Protein catabolism 145 3 0 0 3 0 0Phosphorylation 355 137 12 12 101 10 2Localization 492 47 3 15 22 7 0Binding 1485 380 16 74 266 6 18Regulation 1426 371 35 30 281 4 21Positive regulation 4452 1385 98 131 1087 15 54Negative regulation 1637 716 39 60 520 46 51Table 3: Statistics of annotations in different sections of text: the Abstract column is of the abstraction collection(1210 titles and abstracts), and the following columns are of full paper collection (14 full papers).
TIAB = title andabstract, Intro.
= introduction and background, R/D/C = results, discussions, and conclusions, Methods = methods,materials, and experimental procedures.
Some minor sections, supporting information, supplementary material, andsynopsis, are ignored.
Density = relative density of annotation (P/W = Protein/Word, E/W = Event/Word, and E/P =Event/Protein).4 ParticipationIn total, 15 teams submitted final results.
All 15teams participated in the mandatory Task 1, fourteams in Task 2, and two teams in Task 3.
Only oneteam, UTurku, completed all the three tasks.Table 4 shows the profile of the teams, except-ing three who chose to remain anonymous.
A briefexamination on the team organization (the Peoplecolumn) suggests the importance of a computer sci-ence background, C and BI, to perform the GE task,which agrees with the same observation made in2009.
It is interpreted as follows: the role of com-puter scientists may be emphasized in part due tothe fact that the task requires complex computationalmodeling, demanding particular efforts in frame-work design and implementation and computationalresources.
The ?09 column suggests that previousexperience in the task may have affected to the per-formance of the teams, especially in a complex tasklike the GE task.Table 5 shows the profile of the systems.
Anotable observation is that four teams developedtheir systems based on the model of UTurku09(Bjo?rne et al, 2009) which was the winning sys-tem of BioNLP-ST 2009.
It may show an influenceof the BioNLP-ST series in the task.
For syntac-tic analyses, the prevailing use of Charniak John-son re-ranking parser (Charniak and Johnson, 2005)using the self-trained biomedical model from Mc-Closky (2008) (McCCJ) which is converted to Stan-ford Dependency (de Marneffe et al, 2006) is no-table, which may also be an influence from the re-sults of BioNLP-ST 2009.
The last two teams,XABioNLP and HCMUS, who did not use syntacticanalyses could not get a performance comparable tothe others, which may suggest the importance of us-ing syntactic analyses for a complex IE task like GEtask.5 Results5.1 Task 1Table 6 shows the final evaluation results of Task 1.For reference, the reported performance of the twosystems, UTurku09 and Miwa10 is listed in thetop.
UTurku09 was the winning system of Task 1in 2009 (Bjo?rne et al, 2009), and Miwa10 wasthe best system reported after BioNLP-ST 2009(Miwa et al, 2010b).
Particularly, the latter made10Team ?09 Task People referenceFAUST?12- 3C (Riedel et al, 2011)UMASS?12- 1C (Riedel and McCallum, 2011)UTurku?123 1BI (Bjrne and Salakoski, 2011)MSR-NLP 1-- 4C (Quirk et al, 2011)ConcordU?1-3 2C (Kilicoglu and Bergler, 2011)UWMadison?1-- 2C (Vlachos and Craven, 2011)Stanford 1-- 3C+1.5L (McClosky et al, 2011)BMI@ASU?12- 3C (Emadzadeh et al, 2011)CCP-BTMG?1-- 3BI (Liu et al, 2011)TM-SCS 1-- 1C (Bui and Sloot, 2011)XABioNLP 1-- 4C (Casillas et al, 2011)HCMUS 1-- 6L (Minh et al, 2011)Table 4: Team profiles: The ?09 column indicates whether at least one team member participated in BioNLP-ST 2009.In People column, C=Computer Scientist, BI=Bioinformatician, B=Biologist, L=LinguistNLP Task Other resourcesTeam Lexical Proc.
Syntactic Proc.
Trig.
Arg.
group Dictionary OtherFAUST SnowBall, CNLP McCCJ+SD Stacking (UMASS + Stanford)UMASS SnowBall, CNLP McCCJ+SD Joint infer., Dual DecompositionUTurku Porter McCCJ+SD SVM SVM SVM S. cuesMSR-NLP Porter McCCJ+SD, Enju SVM MaxEnt rules Coref(Hobbs)ConcordU - McCCJ+SD dic rules rules S./N.
cuesUWMadison Morpha, Porter MCCCJ+SD Joint infer., SEARNStanford Morpha, CNLP McCCJ+SD MaxEnt MSTParser word clustersBMI@ASU Porter, WordNet Stanford+SD SVM SVM - MeSHCCP-BTMG Porter, WordNet Stanford+SD Subgraph IsomorphismTM-SCS Stanford Stanford dic rules rulesXABioNLP KAF - rulesHCMUS OpenNLP - dic, rules rules UIMATable 5: System profiles: SnowBall=SnowBall Stemmer, CNLP=Stanford CoreNLP (tokenization), KAF=Kyoto An-notation Format McCCJ=McClosky-Charniak-Johnson Parser, Stanford=Stanford Parser, SD=Stanford DependencyConversion, S.=Speculation, N.=Negationan impressive improvement with Binding events(44.41%?52.62%).The best performance in Task 1 this time isachieved by the FAUST system, which adopts acombination model of UMass and Stanford.
Itsperformance on the abstract collection, 56.04%,demonstrates a significant improvement of the com-munity in the repeated GE task, when compared toboth UTurku09, 51.95% and Miwa10, 53.29%.The biggest improvement is made to the Regulationevents (40.11%?46.97%) which requires a com-plex modeling for recursive event structure - anevent may become an argument of another event.The second ranked system, UMass, shows the bestperformance on the full paper collection.
It suggeststhat what FAUST obtained from the model combi-nation might be a better optimization to abstracts.The ConcordU system is notable as it is the solerule-based system that is ranked above the average.It shows a performance optimized for precision withrelatively low recall.
The same tendency is roughlyreplicated by other rule-based systems, CCP-BTMG,TM-SCS, XABioNLP, and HCMUS.
It suggests thata rule-based system might not be a good choice if ahigh coverage is desired.
However, the performanceof ConcordU for simple events suggests that a highprecision can be achieved by a rule based systemwith a modest loss of recall.
It might be more truewhen the task is less complex.This time, three teams achieved better results thanMiwa10, which indicates some role of focused ef-forts like BioNLP-ST.
The comparison between the11performance on abstract and full paper collectionsshows that generalization to full papers is feasiblewith very modest loss in performance.5.2 Task 2Tables 7 shows final evaluation results of Task 2.For reference, the reported performance of the task-winning system in 2009, UT+DBCLS09 (Riedel etal., 2009), is shown in the top.
The first and secondranked system, FAUST and UMass, which share asame author with Riedel09, made a significantimprovement over Riedel09 in the abstract col-lection.
UTurku achieved the best performance infinding sites arguments but did not produce locationarguments.
In table 7, the performance of all thesystems in full text collection suggests that findingsecondary arguments in full text is much more chal-lenging.In detail, a significant improvement was made forLocation arguments (36.59%?50.00%).
A furtherbreakdown of the results of site extraction, shownin table 8, shows that finding site arguments forPhosphorylation, Binding and Regulation events areall significantly improved, but in different ways.The extraction of protein sites to be phosphory-lated is approaching a practical level of performance(84.21%), while protein sites to be bound or to beregulated remains challenging to be extracted.5.3 Task 3Table 9 shows final evaluation results of Task 3.For reference, the reported performance of the task-winning system in 2009, Kilicoglu09(Kilicogluand Bergler, 2009), is shown in the top.
Among thetwo teams participated in the task, UTurku showeda better performance in extracting negated events,while ConcordU showed a better performance inextracting speculated events.6 ConclusionsThe Genia event task which was repeated forBioNLP-ST 2009 and 2011 took a role of measur-ing the progress of the community and generaliza-tion IE technology to full papers.
The results from15 teams who made their final submissions to thetask show that a clear advance of the community interms of the performance on a focused domain andalso generalization to full papers.
To our disappoint-ment, however, an effective use of supporting taskresults was not observed, which thus remains as fu-ture work for further improvement.AcknowledgmentsThis work is supported by the ?Integrated DatabaseProject?
funded by the Ministry of Education, Cul-ture, Sports, Science and Technology of Japan.ReferencesJari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,Tapio Pahikkala, and Tapio Salakoski.
2009.
Extract-ing complex biological events with rich graph-basedfeature sets.
In Proceedings of the BioNLP 2009 Work-shop Companion Volume for Shared Task, pages 10?18, Boulder, Colorado, June.
Association for Compu-tational Linguistics.Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsujii,and Tapio Salakoski.
2010.
Complex event extractionat PubMed scale.
Bioinformatics, 26(12):i382?390.Jari Bjrne and Tapio Salakoski.
2011.
Generaliz-ing Biomedical Event Extraction.
In Proceedings ofthe BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.Quoc-Chinh Bui and Peter.
M.A.
Sloot.
2011.
Extractingbiological events from text using simple syntactic pat-terns.
In Proceedings of the BioNLP 2011 WorkshopCompanion Volume for Shared Task, Portland, Oregon,June.
Association for Computational Linguistics.Arantza Casillas, Arantza Daz de Ilarraza, Koldo Go-jenola, Maite Oronoz, and German Rigau.
2011.
Us-ing Kybots for Extracting Events in Biomedical Texts.In Proceedings of the BioNLP 2011 Workshop Com-panion Volume for Shared Task, Portland, Oregon,June.
Association for Computational Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt DiscriminativeReranking.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL?05), pages 173?180.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InProceedings of the Fifth International Conferenceon Language Resources and Evaluation (LREC?06),pages 449?454.Ehsan Emadzadeh, Azadeh Nikfarjam, and GracielaGonzalez.
2011.
Double Layered Learning for Bi-ological Event Extraction from Text.
In Proceedings12of the BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.Halil Kilicoglu and Sabine Bergler.
2009.
Syntactic de-pendency based heuristics for biological event extrac-tion.
In Proceedings of the BioNLP 2009 WorkshopCompanion Volume for Shared Task, pages 119?127,Boulder, Colorado, June.
Association for Computa-tional Linguistics.Halil Kilicoglu and Sabine Bergler.
2011.
Adapting aGeneral Semantic Interpretation Approach to Biolog-ical Event Extraction.
In Proceedings of the BioNLP2011 Workshop Companion Volume for Shared Task,Portland, Oregon, June.
Association for Computa-tional Linguistics.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof BioNLP?09 Shared Task on Event Extraction.In Proceedings of Natural Language Processing inBiomedicine (BioNLP) NAACL 2009 Workshop, pages1?9.Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, RobertBossy, and Jun?ichi Tsujii.
2011.
Overview ofBioNLP Shared Task 2011.
In Proceedings ofthe BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.Haibin Liu, Ravikumar Komandur, and Karin Verspoor.2011.
From graphs to events: A subgraph matchingapproach for information extraction from biomedicaltext.
In Proceedings of the BioNLP 2011 WorkshopCompanion Volume for Shared Task, Portland, Oregon,June.
Association for Computational Linguistics.David McClosky and Eugene Charniak.
2008.
Self-Training for Biomedical Parsing.
In Proceedings ofthe 46th Annual Meeting of the Association for Com-putational Linguistics - Human Language Technolo-gies (ACL-HLT?08), pages 101?104.David McClosky, Mihai Surdeanu, and Christopher Man-ning.
2011.
Event Extraction as Dependency Parsingfor BioNLP 2011.
In Proceedings of the BioNLP 2011Workshop Companion Volume for Shared Task, Port-land, Oregon, June.
Association for ComputationalLinguistics.Quang Le Minh, Son Nguyen Truong, and Quoc Ho Bao.2011.
A pattern approach for Biomedical Event Anno-tation .
In Proceedings of the BioNLP 2011 WorkshopCompanion Volume for Shared Task, Portland, Oregon,June.
Association for Computational Linguistics.Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, andJun?ichi Tsujii.
2010a.
A comparative study of syn-tactic parsers for event extraction.
In Proceedings ofBioNLP?10, pages 37?45.Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichiTsujii.
2010b.
Event extraction with complex eventclassification using rich features.
Journal of Bioinfor-matics and Computational Biology (JBCB), 8(1):131?146, February.Hoifung Poon and Lucy Vanderwende.
2010.
Joint infer-ence for knowledge extraction from biomedical litera-ture.
In Proceedings of NAACL-HLT?10, pages 813?821.Chris Quirk, Pallavi Choudhury, Michael Gamon, andLucy Vanderwend.
2011.
MSR-NLP Entry inBioNLP Shared Task 2011.
In Proceedings ofthe BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.Sebastian Riedel and Andrew McCallum.
2011.
RobustBiomedical Event Extraction with Dual Decomposi-tion and Minimal Domain Adaptation.
In Proceedingsof the BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,and Jun?ichi Tsujii.
2009.
A markov logic approachto bio-molecular event extraction.
In Proceedings ofthe BioNLP 2009 Workshop Companion Volume forShared Task, pages 41?49, Boulder, Colorado, June.Association for Computational Linguistics.Sebastian Riedel, David McClosky, Mihai Surdeanu, An-drew McCallum, and Christopher Manning.
2011.Model Combination for Event Extraction in BioNLP2011.
In Proceedings of the BioNLP 2011 WorkshopCompanion Volume for Shared Task, Portland, Oregon,June.
Association for Computational Linguistics.Andreas Vlachos and Mark Craven.
2011.
BiomedicalEvent Extraction from Abstracts and Full Papers usingSearch-based Structured Prediction.
In Proceedingsof the BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.Andreas Vlachos.
2010.
Two strong baselines for thebionlp 2009 event extraction task.
In Proceedings ofBioNLP?10, pages 1?9.13Team Simple Event Binding Regulation AllUTurku09 A 64.21 / 77.45 / 70.21 40.06 / 49.82 / 44.41 35.63 / 45.87 / 40.11 46.73 / 58.48 / 51.95Miwa10 A 70.44 52.62 40.60 48.62 / 58.96 / 53.29W 68.47 / 80.25 / 73.90 44.20 / 53.71 / 48.49 38.02 / 54.94 / 44.94 49.41 / 64.75 / 56.04FAUST A 66.16 / 81.04 / 72.85 45.53 / 58.09 / 51.05 39.38 / 58.18 / 46.97 50.00 / 67.53 / 57.46F 75.58 / 78.23 / 76.88 40.97 / 44.70 / 42.75 34.99 / 48.24 / 40.56 47.92 / 58.47 / 52.67W 67.01 / 81.40 / 73.50 42.97 / 56.42 / 48.79 37.52 / 52.67 / 43.82 48.49 / 64.08 / 55.20UMass A 64.21 / 80.74 / 71.54 43.52 / 60.89 / 50.76 38.78 / 55.07 / 45.51 48.74 / 65.94 / 56.05F 75.58 / 83.14 / 79.18 41.67 / 47.62 / 44.44 34.72 / 47.51 / 40.12 47.84 / 59.76 / 53.14W 68.22 / 76.47 / 72.11 42.97 / 43.60 / 43.28 38.72 / 47.64 / 42.72 49.56 / 57.65 / 53.30UTurku A 64.97 / 76.72 / 70.36 45.24 / 50.00 / 47.50 40.41 / 49.01 / 44.30 50.06 / 59.48 / 54.37F 78.18 / 75.82 / 76.98 37.50 / 31.76 / 34.39 34.99 / 44.46 / 39.16 48.31 / 53.38 / 50.72W 68.99 / 74.30 / 71.54 42.36 / 40.47 / 41.39 36.64 / 44.08 / 40.02 48.64 / 54.71 / 51.50MSR-NLP A 65.99 / 74.71 / 70.08 43.23 / 44.51 / 43.86 37.14 / 45.38 / 40.85 48.52 / 56.47 / 52.20F 78.18 / 73.24 / 75.63 40.28 / 32.77 / 36.14 35.52 / 41.34 / 38.21 48.94 / 50.77 / 49.84W 59.99 / 85.53 / 70.52 29.33 / 49.66 / 36.88 35.72 / 45.85 / 40.16 43.55 / 59.58 / 50.32ConcordU A 56.51 / 84.56 / 67.75 29.97 / 49.76 / 37.41 36.24 / 47.09 / 40.96 43.09 / 60.37 / 50.28F 70.65 / 88.03 / 78.39 27.78 / 49.38 / 35.56 34.58 / 43.22 / 38.42 44.71 / 57.75 / 50.40W 59.67 / 80.95 / 68.70 29.33 / 49.66 / 36.88 34.10 / 49.46 / 40.37 42.56 / 61.21 / 50.21UWMadison A 54.99 / 79.85 / 65.13 34.87 / 56.81 / 43.21 34.54 / 50.67 / 41.08 42.17 / 62.30 / 50.30F 74.03 / 83.58 / 78.51 15.97 / 29.87 / 20.81 33.11 / 46.87 / 38.81 43.53 / 58.73 / 50.00W 65.79 / 76.83 / 70.88 39.92 / 49.87 / 44.34 27.55 / 48.75 / 35.21 42.36 / 61.08 / 50.03Stanford A 62.61 / 77.57 / 69.29 42.36 / 54.24 / 47.57 28.25 / 49.95 / 36.09 42.55 / 62.69 / 50.69F 75.58 / 75.00 / 75.29 34.03 / 40.16 / 36.84 26.01 / 46.08 / 33.25 41.88 / 57.36 / 48.41W 62.09 / 76.55 / 68.57 27.90 / 44.92 / 34.42 22.30 / 40.26 / 28.70 36.91 / 56.63 / 44.69BMI@ASU A 58.71 / 78.51 / 67.18 26.22 / 47.40 / 33.77 22.99 / 40.47 / 29.32 36.61 / 57.82 / 44.83F 72.47 / 72.09 / 72.28 31.94 / 40.71 / 35.80 20.78 / 39.74 / 27.29 37.65 / 53.93 / 44.34W 53.61 / 75.13 / 62.57 22.61 / 49.12 / 30.96 19.01 / 43.80 / 26.51 31.57 / 58.99 / 41.13CCP-BTMG A 50.93 / 74.50 / 60.50 25.65 / 53.29 / 34.63 19.54 / 43.47 / 26.96 31.87 / 59.02 / 41.39F 61.82 / 76.77 / 68.49 15.28 / 37.29 / 21.67 17.83 / 44.63 / 25.48 30.82 / 58.92 / 40.47W 57.33 / 71.34 / 63.57 34.01 / 44.77 / 38.66 16.39 / 25.37 / 19.91 32.73 / 45.84 / 38.19TM-SCS A 53.65 / 71.66 / 61.36 36.02 / 49.41 / 41.67 18.29 / 27.07 / 21.83 33.36 / 47.09 / 39.06F 68.57 / 70.59 / 69.57 29.17 / 35.00 / 31.82 12.20 / 21.02 / 15.44 31.14 / 42.83 / 36.06W 43.71 / 47.18 / 45.38 05.30 / 50.00 / 09.58 05.79 / 26.94 / 09.54 19.07 / 42.08 / 26.25XABioNLP A 39.76 / 45.90 / 42.61 06.34 / 56.41 / 11.40 04.72 / 23.21 / 07.84 17.91 / 40.74 / 24.89F 55.84 / 50.23 / 52.89 02.78 / 30.77 / 05.10 08.18 / 33.89 / 13.17 21.96 / 45.09 / 29.54W 24.82 / 35.14 / 29.09 04.68 / 12.92 / 06.88 01.63 / 10.40 / 02.81 10.12 / 27.17 / 14.75HCMUS A 22.42 / 37.38 / 28.03 04.61 / 10.46 / 06.40 01.69 / 10.37 / 02.91 09.71 / 27.30 / 14.33F 32.21 / 31.16 / 31.67 04.86 / 28.00 / 08.28 01.47 / 10.48 / 02.59 11.14 / 26.89 / 15.75Table 6: Evaluation results (recall / precision / f-score) of Task 1 in (W)hole data set, (A)bstracts only, and (F)ullpapers only.
Some notable figures are emphasized in bold.14Team Sites (222) Locations (66) All (288)UT+DBCLS09 A 23.08 / 88.24 / 36.59 32.14 / 72.41 / 44.52W 32.88 / 70.87 / 44.92 36.36 / 75.00 / 48.98 33.68 / 71.85 / 45.86FAUST A 43.51 / 71.25 / 54.03 36.92 / 77.42 / 50.00 41.33 / 72.97 / 52.77F 17.58 / 69.57 / 28.07 - 17.39 / 66.67 / 27.59W 31.98 / 71.00 / 44.10 36.36 / 77.42 / 49.48 32.99 / 72.52 / 45.35UMass A 42.75 / 70.00 / 53.08 36.92 / 77.42 / 50.00 40.82 / 72.07 / 52.12F 16.48 / 75.00 / 27.03 - 16.30 / 75.00 / 26.79W 32.88 / 62.93 / 43.20 22.73 / 83.33 / 35.71 30.56 / 65.67 / 41.71BMI@ASU A 37.40 / 67.12 / 48.04 23.08 / 83.33 / 36.14 32.65 / 70.33 / 44.60F 26.37 / 55.81 / 35.82 - 26.09 / 55.81 / 35.56W 40.09 / 65.44 / 49.72 00.00 / 00.00 / 00.00 30.90 / 65.44 / 41.98UTurku A 48.09 / 69.23 / 56.76 00.00 / 00.00 / 00.00 32.14 / 69.23 / 43.90F 28.57 / 57.78 / 38.24 - 28.26 / 57.78 / 37.96Table 7: Evaluation results of Task 2 in (W)hole data set, (A)bstracts only, and (F)ull papers onlyTeam Phospho.
(67) Binding (84) Reg.
(71)Riedel?09 A 71.43 / 71.43 / 71.43 04.76 / 50.00 / 08.70 12.96 / 58.33 / 21.21W 71.64 / 84.21 / 77.42 05.95 / 38.46 / 10.31 28.17 / 60.61 / 38.46FAUST A 71.43 / 81.63 / 76.19 04.76 / 14.29 / 07.14 29.63 / 66.67 / 41.03F 72.73 / 100.0 / 84.21 06.35 / 66.67 / 11.59 23.53 / 44.44 / 30.77W 76.12 / 79.69 / 77.86 04.76 / 36.36 / 08.42 22.54 / 64.00 / 33.33UMass A 76.79 / 76.79 / 76.79 04.76 / 14.29 / 07.14 22.22 / 70.59 / 33.80F 72.73 / 100.0 / 84.21 04.76 / 75.00 / 08.96 23.53 / 50.00 / 32.00W 52.24 / 97.22 / 67.96 20.24 / 53.12 / 29.31 29.58 / 43.75 / 35.29BMI@ASU A 53.57 / 96.77 / 68.97 09.52 / 22.22 / 13.33 31.48 / 51.52 / 39.08F 45.45 / 100.0 / 62.50 23.81 / 65.22 / 34.88 23.53 / 26.67 / 25.00W 76.12 / 91.07 / 82.93 21.43 / 51.43 / 30.25 28.17 / 44.44 / 34.48UTurku A 78.57 / 89.80 / 83.81 09.52 / 18.18 / 12.50 31.48 / 54.84 / 40.00F 63.64 / 100.0 / 77.78 25.40 / 66.67 / 36.78 17.65 / 21.43 / 19.35Table 8: Evaluation results of Site information for different event types in (A)bstractsTeam Negation Speculation AllKilicoglu09 A 14.98 / 50.75 / 23.13 16.83 / 50.72 / 25.27 15.86 / 50.74 / 24.17W 22.87 / 48.85 / 31.15 17.86 / 32.54 / 23.06 20.30 / 39.67 / 26.86UTurku A 22.03 / 49.02 / 30.40 19.23 / 38.46 / 25.64 20.69 / 43.69 / 28.08F 25.76 / 48.28 / 33.59 15.00 / 23.08 / 18.18 19.28 / 30.85 / 23.73W 18.77 / 44.26 / 26.36 21.10 / 38.46 / 27.25 19.97 / 40.89 / 26.83ConcordU A 18.06 / 46.59 / 26.03 23.08 / 40.00 / 29.27 20.46 / 42.79 / 27.68F 21.21 / 38.24 / 27.29 17.00 / 34.69 / 22.82 18.67 / 36.14 / 24.63Table 9: Evaluation results of Task 3 in (W)hole data set, (A)bstracts only, and (F)ull papers only15
