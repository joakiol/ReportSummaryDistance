SESSION I: EVALUATING SPOKEN LANGUAGEJames F. AllenDepartment ofComputer ScienceUniversity of RochesterRochester, NY 14627SYSTEMSThis session concerns the evaluation of spokenlanguage systems.
To understand the issues, it willhelp to briefly review the history of evaluation i  theSpoken Language Systems program (see Figure 1).The existing methods for evaluation evolved from thetechniques used for the speech recognition systems.
Intasks such as resource management, there is a closedvocabulary and the data is read speech.
The mainevaluation criteria is recognition accuracy, i.e., howmany words in the test set are recognized correctly.
Toperform this evaluation, the researchers need a largedatabase of read speech.
Such data is relativelyinexpensive to obtain in sufficient quantities.With the ATIS domain, the task is generalized toquestion answering.
Systems are dealing with an openvocabulary and spontaneous speech and the primarycriteria for evaluating systems i the correctness of theanswer given for each query.
To perform thisevaluation, the researchers need a large database ofspontaneous questions annotated with the appropriateanswer.
Not only is the data collection process morecomplex as one needs pontaneous speech, but an orderof magnitude more data is needed for training andevaluation (since there are many words in everyutterance).The ATIS task has evolved to the stage where a newconcern is handling dialog.
For this, the systems mustdeal with spontaneous natural speech in context.
Theevaluation criteria for such systems i not yet clear, andtwo of the papers in this session put forth some initialexperiments with possible evaluation techniques.Whatever the evaluation technique, however, it is clearthat he researchers now need an even larger database ofdialogs both for training and evaluation.At the present time, the spoken language program is intransition to the last stage described above.
Systems arestarting to try to deal with dialogs, but the existingevaluation techniques are only appropriate for question-answering tasks.
The papers in this session offer aninteresting perspective of the issues involved in makingthis transition.In particular, the papers discussed three crucial issues:?
Where do we get al the data that is needed7?
How are we currently doing (at question-answering)7?
What are appropriate evaluation metrics for dialogsystems?The MADCOW paper describes the data collectioneffort in the last year.
At each stage of development -from speech recognition, to question-answering, todialog systems - there is an order of magnitude increasein the amount of data needed for training andevaluation.
While it was possible in the early stages tohave a single data collection and analysis ite, it wasclear that not enough dialogs could be collected rapidlyenough under the old scheme.
The MADCOW effortinvolves collecting data at all the different SLS sites,and co-ordinating the annotation of the data and itsdistribution.The second paper gives the results of the latest ATISbenchmarks.
Most of the results are straightforward tointerpret and need no further comment here.
But it isimportant to not confuse the full-session evaluationperformed this time with a dialog evaluation.
Since thisis an issue that is easy to misinterpret, and since it laysthe groundwork for the remaining papers in the session,I will discuss this further here.Full session evaluation consists of testing systems onentire dialogs as they occurred in data collection.
Eachutterance isannotated with the correct answer.
But thereis no precise notion of a "correct" answer, because oftenmany answers are possible and equally correct.
Forexample, one answer might give more information thananother because it is relevant: Often, the answer thatcontains the minimal amount of information requestedwould in fact be quite unhelpful.
For example, considera system that answered the question "What are the faresfor flights from Boston to San Francisco?"
by simplylisting the fares without identifying what flights hadwhat fares.
The answer might be "correct" butuncooperative.
On the other hand, we would not wantto allow arbitrary extra information, as then theoptimal scoring strategy would be for systems to listall information about anything that is mentioned in aquery, or even list the entire database, not a helpfulresponse.As a start owards handling this problem, each query isannotated with a minimum an a maximum answer.
Theminimum answer contains just that data that isexplicitly asked for in the query.
Any system answerthat does not contain all of the minimum answer wouldbe incorrect.
The maximum answer, on the other hand,includes all information that could be relevant to thequery.
Any system answer that includes moreinformation than the maximum answer is incorrect.DomainClosed voeab,read speechOpen vocab,spontaneousOpen vocab,spontaneousdialogueEvaluation MeasureRecognitionAccuracyAnswerAccuracy77?Data NeedsManywordsMany sentenceswith answersManydialogsFigure h A mini-history of the SIS evaluationSince the dialogs are transcripts of actual humanperformance, they do occasionally contain utterancesthat are simply not comprehensible, or are off topic.
Itwould not be reasonable for the systems to be able tohandle such utterances.
To account for this, and toobtain information on how systems handle contextdependency, all utterances in the dialogs are classifiedinto one of three classes:?
Class A - queries that are answerable independent ofcontext;?
Class D - queries that require context set by previousutterances;?
Class X - unanswerable queries.While the type of the utterance is provided in thetraining data, it was not revealed on the test data.
Thus,while evaluation results are tabulated using thisclassification, the systems did not have thisinformation available when they were tested.While the above procedure might seem to test dialoghandling capabilities at first glance, this is misleading.It is important to remember that a transcript is just onepossible dialogue between the individuals involved.
Ifwe put the same people back in the same situation withthe same task, they would almost surely have adifferent dialog.
This is because at any stage of thedialog, there are always many possible questions thatcould be asked and many possible ,answers to eachquestion.
Even if the system is restricted to onlyanswering questions and not taking any initiative on itsown, we saw that there are many possible reasonableanswers.
But different answers, while all reasonable,might lead to different continuations in a dialog.
As aresult, a transcript-based valuation of dialog could atbest test a systems ability to track an existing dialog,rather than partake in a dialog fully.To conclude, dialog evaluation cannot be reduced toindividual answer evaluation.
Furthermore, there doesnot seem to be a plausible way to generalize theevaluation techniques based on transcripts.
Think abouthow many dialogs would have to be collected tocharacterize the range of acceptable dialogs for even asimple single task!
One would need a separate dialogfor every possible variation that could occur in anyquestion or  answer.Rather, we need new reliable, objective measures fordialog evaluation.
To be objective means that theresults are reproducible.
So while some proposalsdiscussed in this session use subjective valuations ofjudges to score a dialog, if these judgements can beobtained reliably from different judges, then themeasure is reproducible and thus as objective as anyother measure.
