Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 30?38,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsA Comparison of Latent Variable Models For Conversation AnalysisSourish Chaudhuri and Bhiksha RajLanguage Technologies Institute,School of Computer Science,Carnegie Mellon University,Pittsburgh, PA - 15213.
{sourishc, bhiksha} @ cs.cmu.eduAbstractWith the evolution of online communicationmethods, conversations are increasingly han-dled via email, internet forums and other suchmethods.
In this paper, we attempt to modellexical information in a context sensitive man-ner, encoding our belief that the use of lan-guage depends on the participants in the con-versation.
We model the discourse as a com-bination of the speaker, the addressee andother participants in the conversation as wellas a context specific language model.
In or-der to do this, we introduce a novel methodbased on an HMM with an exponential statespace to capture speaker-addressee context.We also study the performance of topic model-ing frameworks in conversational settings.
Weevaluate the models on the tasks of identify-ing the set of people present in any conver-sation, as well as identifying the speaker forevery utterance in the conversation, and theyshow significant improvement over the base-line models.1 IntroductionIn this paper, we experiment with different methodsof automatically analyzing discourse.
We presentand validate hypotheses on how conversations canbe better analyzed using information about thespeakers, as well as other participants in the con-versation.
We present a novel method of modelingdiscourse using an exponential state Hidden MarkovModel where states are based on speakers and ad-dressees.
We also cast the problem into the populartopic modeling frameworks, and compare the vari-ous approaches.Consider a small group of people that a personknows well.
Given a transcript of a discussion ona topic of mutual interest, that person would likelybe able to identify who is likely to have said what,based on his knowledge of the speakers and their in-clinations on various topics.
We would like to beable to encode similar intelligence into a system thatcould automatically learn about speakers based ontranscripts of prior conversations, and use that infor-mation to analyze new conversations.The scenario we consider in this work is as fol-lows: we have a known set of characters, any subsetof whom could be present in a conversation.
Giventhe transcript of a conversation only, without speakerannotations, we would like to : 1.
Predict the set ofparticipants in the conversation from the character-istics of the entire conversation, and 2.
Identify theindividual speakers at each conversation turn.In order to do this, we model each utterance ina conversation as dependent on the speaker, the ad-dressee and the other people present.
As we shalldescribe, our models encode the belief that peoplespeak/behave differently depending on other partic-ipants in the conversation.
This has a two-fold ben-efit: first, it can help us discover social (or even,professional) relationship structures; second, it canhelp us understand how to respond to different peo-ple, and incorporate that information into automatedconversational agents which can then behave in amore context sensitive manner.
The ability to auto-matically model discourse as context specific in thismanner is also useful for other tasks such as directedadvertising and duplicity detection.In Section 2, we describe relevant related work.30Section 3 describes the dataset for our experiments,Section 4 describes the problem, our use of topicmodels, and the novel HMM based method, whileSection 5 summarizes the results and we concludein Section 6.2 Related WorkThe task of automatically segmenting speech andthen identifying speakers from audio (Reynoldsand Torres-Carrasquillo, 2005) is referred to as di-arization and has been well-studied (Tranter andReynolds, 2006).
More recently, approaches havebeen developed to fuse information from both theaudio and video modalities (Noulas et al, 2011)to improve diarization systems when video informa-tion is available.
In this paper, we attempt to under-stand just how much information is available in thetext alone.
Systems that can work with text only canbe used to improve audio-based systems which canprovide speech recognition output to a text-basedsystem.
They can also be used to work with closedcaption streams, or on human-generated transcrip-tions of meeting recordings.Research on identifying speakers from text or lex-ical information is limited in comparison to workwith audio data.
However, efforts have been madeto use discourse level information to automaticallyidentify speakers to calibrate idiolectal differencesbetween speakers (Doddington, 2001).
(Canseco etal., 2005, ) investigated the use of lexical featuresto automatically diarize (but not actually identify)transcripts to determine if a current speaker contin-ued or a previous speaker spoke or the next speakerspoke.
Lei and Mirghafori (2 007) attempted to in-corporate idiolect based speaker information by us-ing word conditioning of phone N -grams to recog-nize speakers in dialogs with 2 speakers.In our work, the models we use to identify speak-ers are powerful enough to predict the addressee aswell.
In this context, we note that several attemptshave been made recently to automatically identifyaddressees in dialog settings.
These approacheshave used information about the context and con-tent of the utterance, using dialog acts and informa-tion about the speaker?s gaze to aid classifier per-formance (Jovanovic et al, 2006).
Den Akker andTraum (2009) proposed rule-based methods for ad-dressee classification.
Unlike in these works, weattempt to jointly model both the speaker and theaddressee as one of our proposed approaches.
Thisis similar to the approach employed by (Otsuka etal., 2005, ), who proposed a Dynamic Bayesian Net-work model to understand multiparty conversationstructure using non-verbal cues only?
eye gaze, fa-cial expression, gesticulations and posture.3 DataThe data for our experiments consists of fan-sourced transcripts of the episodes of the sitcomF.R.I.E.N.D.S.
The structure of the data is as fol-lows: we have a set of conversations as training data.Each conversation contains a sequence of turns, witheach turn annotated with its speaker.
We do nothave any information about the addressee from thedataset.
We do, however, have implicit informa-tion of the set of speakers within a conversation seg-ment (we make the assumption here that if a char-acter doesn?t speak in a segment, he is not present).Annotator notes appear periodically to indicate thatthe scene changed or that new characters entered thescene or that some characters left the scene.
Wetreat these annotator notes as conversation bound-aries and the segment of turns between two suchboundaries constitutes one conversation instance.The set of characters used for our experiments isfinite.
The 6 primary characters in the sitcom (Chan-dler, Joey, Monica, Phoebe, Rachel and Ross) areretained.
In addition to these 6 primary characters,there are a number of supporting characters who ap-pear occasionally.
We use Other to denote all othercharacters, as the amount of data for a number of thesupporting characters is quite small and would notresult in learning useful patterns regarding their be-havior.
As a result, we treat all of these charactersas one character that can be thought of as a univer-sal supporting character.
Hence, we have a total of7 possible characters.
Any subset of these 7 char-acters could be part of a conversation.
Below is anexample of a pair of conversations from our dataset:[EVENT]Paul: thank you!
thank you so much!Monica: stop!Paul: no, i?m telling you last night was like umm,all my birthdays, both graduations, plus the barn31raising scene in witness.Monica: we?ll talk later.Paul: yeah.
thank you.
[EVENT]Joey: that wasn?t a real date?!
what the hell doyou do on a real date?Monica: shut up, and put my table back.All: okayyy!
[EVENT]The event markers are tags inserted at pre-processing time, to denote transcriber annotationssuch as characters entering or leaving scenes.
Thesequence of turns between two event markers aretreated as one conversation.
Also, note the characterPaul in the first conversation in the example above?
when training the system, the content of Paul?s ut-terances are used to train the model for Other, sincePaul is not one of the primary characters that wetrack.
At test time, the input looks similar to theabove, except that the turns are not annotated byspeaker.The transcripts used in our experiments are seg-mented by speaker turns, so that consecutive turnsare uttered by different speakers.
The entire set of230 episodes was split randomly into training, de-velopment and test splits.
Sequential informationfor the individual conversations were not used.
Eachepisode was further divided into conversations basedon the scene boundaries denoted by the transcribers.For training, overall, we used 195 episodes fromF.R.I.E.N.D.S, with a total of 9,171 conversationsand a total 52,516 turns.
The average length in num-ber of turns for each conversation was 5.73.
Thetest set consisted of a total of 20 episodes with 855conversations and 4,981 turns.
The average lengthof a conversation in the test set was 5.83.
The re-maining 15 episodes were used as development datato tune hyperparameters ?
this set consisted of 529conversations and 2,984 turns in total.
The distribu-tion of the number of utterances by speakers acrossthe training, test and development set are shown inFigure 1.
As one can observe, the distribution is notparticularly skewed for any of the speakers acrossthe splits of the dataset.Figure 1: Distribution of #utterances for each speaker inthe dataset.4 Conversation ModelsPrevious work in analyzing participants in a conver-sation have used meeting data, with a fixed numberof participants.
In our task, the total number of pos-sible participants is finite, but we do not have in-formation on how many of them are present at anyparticular instant.
Thus, our model first attempts todetect the participants in a segment of conversation,and then attempts to attribute speaker turns to indi-viduals.Our model for discourse structure is based on twopremises.
First, we believe that what a person sayswill depend on who he or she is speaking to.
Intu-itively, consider a person trying to make the samepoint to his boss and (at a different time and place)to his friend.
It is likely that he will be more formalwith his boss than his friend.
Second, if the speakeraddresses someone specifically in a group of people,knowing who he addressed would likely help us pre-dict better who would speak next.
We assume thatthe first hypothesis above also holds for groups ofpeople in conversations, where the topics and theirdistribution in discussions (and words that affect thetone of the discussion) depend on the participants.As described earlier, we evaluate our models ontwo tasks.
First, we would like to identify the set ofcharacters present in any conversation.
Given seg-ments of conversation, we attempt to understand thedistribution of topics for specific subsets of charac-ters present in that segment.
To do this, we castthis problem into a topic modeling framework ?
weexperiment with the Author-Topic model (Rosen-32Zvi et al, 2004), described in Section 4.1, for thistask.
We use the Author Topic Model to link the co-occurrence information of characters with the wordsin the conversation.Second, we attempt to attribute speakers to ut-terances, described in Section 4.2.
We introduce anovel approach using an HMM with an exponen-tial state space to model speakers and addressees,described in Section 4.2.1.
We also use the AuthorTopic Model and the Author-Recipient Topic Model(McCallum et al, 2007), described in Section 4.2.2for this task.
The key difference between the HMM-based model and the topic model based approachesis that the former explicitly takes sequence informa-tion into account.4.1 Identifying Character Subset PresenceThe premise behind attempting to model subsets ofcharacters is that the nature of the conversation de-pends on the group of people participating.
For in-stance, it seems intuitively likely that the content ofa conversation between two friends would be differ-ent if they were the only ones present than it wouldbe if their families were also present.
To extend thishypothesis to a general scenario, the content of eachspeaker?s turn depends not only on the speaker, butalso on the person being spoken to as well as theother people present.
To model this, we require amodel that captures the distribution of the text forentire conversation, for each possible subset of char-acters.
In this section, we describe the training of ageneric model for conversations, and use it to pro-duce features for a discriminative classifier.Let there be N characters who could participatein a conversation.
We assume a general scenario,where any subset of these characters may be present.Thus, there are 2N?1 character subsets that are pos-sible.
We can model this as a multi-class classifica-tion problem (we will refer to this as subset model-ing, henceforth).The generative model for this task is as follows:Each conversation segment is associated with a setof utterances, and a set of characters.
For each suchset of characters, we associate a distribution overtopics.
For each word that is present in the seg-ment, we select a topic from the subset-specific topicdistribution, and then we select the word from thattopic.
Figure 2 shows the graphical model for this inplate notation.Figure 2: Graphical representation of the subset model inplate notationIn the plate notation, the observed variables areshaded and the latent variables are unshaded.
Platesencapsulate a set of variables which are repeatedlysampled a fixed number of times, and the number atthe bottom right indicates this fixed number.Sc represents a subset of the characters who werepresent in the conversation segment.
We have Csuch conversations, and each conversation containsNc words.
z represents the latent topic variable, and?
represents the multinomial topic distribution foreach subset of characters (there are 2N such sub-sets).
The multinomial distribution of topics has aprior distribution characterized by ?.
Similarly, ev-ery topic (there are a set of T topics) has a multino-mial distribution ?
over the words in the vocabulary,and ?
has a prior distribution characterized by ?.For every conversation in the training corpus, theset of characters present is known.
The content ofthe conversation is treated as a bag of words.
Fromthe topic distribution for the subset of characterspresent, we sample a topic.
Based on the word dis-tributions for this topic, we sample a word.
Thisprocess is repeated Nc times corresponding to thenumber of words in the conversation.
The entireprocess of generating a conversation is repeated Ctimes, corresponding to the number of conversationsin the training corpus.Depending on the value of N , the number of pos-33sible classes may be very high.
Training a largenumber of models may lead to a data scarcity, es-pecially given the high dimensionality of languagedata.
We therefore slightly modify the model, so thatinstead of topic distributions for each possible sub-set, we have a topic distribution for each character,and the distribution of topics in the conversation isa mixture of the topic distributions for each charac-ter.
This leads us to a graphical model that has beenwell-studied in the past ?
the Author-Topic model(ATM, henceforth) and is shown in Figure 3.Figure 3: Graphical representation of the simplified sub-set model in plate notationThus, given the set of characters present, we sam-ple one of them (x) from a uniform distribution.Then we generate a topic by sampling from the dis-tribution of topics for that speaker.
The rest of theprocess remains the same.We use this model to helpus predict which subset of characters was present ina given conversation.We learn speaker-specific topic distributions us-ing the ATM.
In order to predict characters presentin a test conversation, we train binary SVM (Shawe-Taylor and Cristianini, 2000) classifiers for eachspeaker in the following manner: we compute thedistribution of the speaker-specific topics in eachconversation, and use these as the features of thedata point.
If the speaker was present in the con-versation, the data point corresponding to the con-versation has a class label of +1, else -1.
A linearSVM classifier is trained over the data.
At test time,we compute the distribution of the speaker?s topicsin the conversation, and use the SVM to predict ifthe speaker was present or not.4.2 Identifying Speakers From UtterancesIn this section, we describe our approach to identi-fying speakers from the text of the utterance.
TheATM (as described above) treats all the participantsin the conversation as being potential contributors toeach turn.
However, we can also use the ATM topredict speakers directly.
In this case, we will useeach turn as analogous to a document.
Each suchdocument has only one author and the author topicmodel can be used to learn models for each author.The plate notation for this would look very similarto the one in Figure 2, except that instead of a sub-set of characters being observed, only one would beobserved, and the number of possible topic distribu-tions would be equal to the number of characters.The ATM for this task does not take any contextinformation into account.
In the following subsec-tion, we introduce a novel HMM based approachthat seeks to leverage information from the sequenceof turns.4.2.1 Exponential State Hidden Markov ModelIn this model, we assign a state to each speaker-addressee combination possible.
If our data consistsofN characters, only one of theN characters will bespeaking at any given point.
He/She may be speak-ing to any combination of the remainingN?1 char-acters.
Thus, the number of states in this model isN ?2(N?1).
Note that the addressee is not observeddirectly from the data.The sequence of turns in a conversation is mod-eled by a Hidden Markov Model (Rabiner, 1989).At each time instant, the speaker corresponding tothe state speaks a turn, which is the observed emis-sion, before transitioning to another state at the nexttime instant.
The state at the next time instant is con-strained to have a different speaker.The model is trained using the standard Baum-Welch training.
The emission probabilities are cap-tured by a trigram language model, trained usingthe SRILM toolkit (Stolcke, 2002).
The parametersof the model are initialized as follows: for emis-sion probabilities, we take all the utterances by aspeaker and distributing them uniformly among the34states that have that speaker, since we do not havedirect information about the addressees.
For tran-sition probabilities, we initialize with a bias insteadof uniformly.
Given a conversation, for a state withspeaker A and set of addressees (R, say ?
Notethat R may have multiple characters), we give equalprobabilities of transitioning to all states that haveone of the characters in R as the speaker.
Now, wepick the set of speakers (call it M ) that uttered thenext three turns (essentially, we look ahead in thedata stream to see who the next 3 speakers are whiletraining).
We add a bias to every state with A asthe speaker, and every possible combination of thespeakers in M , to encode the hypothesis that the ad-dressee would be likely to speak pretty soon, if notdirectly after.The large state space in this model makes compu-tation extremely expensive.
However, an examina-tion of the posterior probabilities show that a numberof states are rarely, or never, entered.
We prune awaysuch states after every 5 iterations in the followingmanner ?
we use the current parameters of the modelafter each iteration to identify the speakers of eachturn on the development set.
Decoding of a sequenceof turns at test time is done using the Viterbi algo-rithm.
However, instead of using the best path only,we keep track of the top 10 best paths.
Thus, afteran iteration of training, we test on the developmentdata, and obtain 10 possible sequences of speakersfor each conversation.
Over 5 iterations, we havethe 50 best paths for each conversation.
We thencompute the average number of states entered in allthe decoded paths obtained.
If the average numberof times a state was entered is ?, then any state thatwas entered less than k ?
?
times (k = 0.02, forour experiments), according to the posterior proba-bilities was pruned out.
In order to set the value ofk, the development set was split into 2 halves, withone half being used to compute the average numberof times a state is entered across the 10 best decodesfor data in that half.
For different values of k, accu-racy of speaker identification on the 1-best decodewas computed on the other half of the developmentset, for values of k from 0.005 to 0.1.The optimal state sequence at test time also con-tains information about the addressee.
For the taskswe evaluate, this information is not directly used.However, in other applications, such as those in-volving automated agents, this information could bevaluable in triggering the agent.4.2.2 Author-Recipient Topic ModelThe Author Recipient Topic Model (McCallum etal., 2007) (ARTM, henceforth) was used for discov-ering topics and roles in social networks.
It is builtover the Author-Topic Model discussed previously,with the exception that messages are conditioned onthe sender as well as the receivers.
The graphicalmodel in plate notation is shown in Figure 4.Figure 4: Graphical representation of the Author-Recipient Topic model in plate notationHere, we model each turn as having a set of Ntwords.
Each turn has one speaker S, and a set ofaddressees At.
The generative model works as fol-lows: For each word in a turn, sample an addresseea from the set of addressees.
Topic distributions arenow conditioned over speaker-addressee pairs, in-stead of only the speaker as we saw in the ATM.A topic is now sampled from the speaker-addresseespecific topic distribution.
A word is now sampledfrom this topic using the topic specific word distri-butions.
The parameters ?, ?, and z have the samemeaning as in the ATM described earlier.Note that the set of addressees in our setting isnot explicitly observed.
We know the participants inthe conversation at training time, and we know thespeaker, but we do not know who was addressed.Since we do not have information to make a betterchoice of addressee, we model the entire set of par-35ticipants without the speaker as the set of addressees,in this model.For the task of identifying the speaker who utteredthe turn, we employ an approach, similar to the oneused for ATM.
We train speaker-addressee-specificmodels.
The feature set for this task includes fea-tures not only from the turn itself, but also fromthe context.
Thus, we have the distribution of thetopics in the turn for every speaker-addressee pairwith the right speaker, the speakers of the previ-ous two turns, and the distribution of topics of thespeaker of the current turn over the previous twoturns.
(Thus, while the model does not explicitlymodel sequence, as an HMM does, it utilizes con-text information in its feature space.)
Using thesefeatures, we train a linear SVM to predict whetheror not the speaker uttered the turn.
In this case, wecould potentially have multiple speakers (or none ofthem) predicted to have uttered the same turn.
Inthat case, we choose the speaker with the maximumdistance from the margin.4.3 Baseline ModelsIn this section, we set up simple baseline models toevaluate our performance against.
We describe howwe set up a random baseline, a Naive Bayes baselineand an HMM baseline model.4.3.1 Random BaselineFor the task of identifying the set of charac-ters present in a conversation, the random baselinewould work as follows: it knows that the number ofcharacters present in any conversation lies between1 and N (N = 7, in this case).
(Note that monologues,with only 1 person being present, are possible.
Typ-ically, in our data, they happen at the beginning orend of scenes.)
Thus, it randomly decides if eachof these characters are present or not in any givenconversation.Suppose that the total number of characters are nand r of them are actually present in the conversa-tion.
Let us say the random guess system predictst of the characters to be present.
If we use the uni-form distribution for picking t, then P (t) = 17 , ?t ?
[1, 2, ..., 7], in this case.
For any given t, the proba-bility that we get k correct is given by:P (k|t) =(rk)?
(n?rt?k)(nt) (1)To compute the probability of getting k right, wemarginalize out the number of characters guessed tobe present, t:P (k) =?tP (k, t) =?tP (k|t).P (t) (2)Now we can compute the probability of getting kcorrect by randomly guessing, for all k from 0 to r.Using these, we can compute the expected numberof correct guesses, which turns out to be 0.571.r foran average recall would be 57.1%.For the task of identifying the characters, everyturn could have been uttered by one of the n charac-ters (n = 7, for our case).
Thus, the average accu-racy at identifying turns would be 17 or 14.29%.4.3.2 Naive Bayes ClassifierFor the task of predicting the subset of speakers,we set up a Naive Bayes using words as features.We build up a term-document matrix, with each con-versation treated as a document.
For each charac-ter, we train a binary classifier using the trainingdata- conversations where the character was presentwere marked as a positive instance for that charac-ter, and ones where he was not present were markedas negative instances.
We experimented both withusing priors based on the empirical distribution inthe training data and with using uniform prior (i.e.P (character) = 0.5).
Given a test conversation,we use individual classifiers for each of the charac-ters to determine whether he/she was present or not.For the task of identifying speakers, given an ut-terance, the Naive Bayes classifier is set up as fol-lows: Again, we create term-document matrices foreach of the speakers, where a document is a turn ut-tered by the speaker.
Turns uttered by that speakerare positive instances and those uttered by someoneelse are negative instances.
For each speaker, wecompute the Naive Bayes probability ratio (odds) ofhim uttering the turn and not uttering the turn, in or-der to decide.
If multiple speakers are classified ashaving uttered the turn, or no speaker is classifiedas having uttered the turn, the speaker with the bestodds of having uttered the turn is selected.36System Precision RecallAuthor Topic Model 63.22% 74.71%NB 52.33% 44.19%NB-prior 68.31% 36.25%Random Baseline 28.05% 57.1%Table 1: Results for predicting subset of characterspresent4.3.3 Single Speaker HMMThis model is only used to attribute speakers toturns.
Section 4.2.1 described an HMM modelthat captures speaker-addressee information.
In thesingle- speaker HMM, we have a state for eachspeaker.
Emission probabilities are given by a tri-gram language model that is trained on the speaker?sutterances in the training data.
The transition proba-bilities are initialized as per the empirical transitionsbetween speakers in the data.
This model does notcapture any kind of addressee information.5 ResultsIn this section, we present results of our experi-ments with the models we described earlier, on thetwo tasks, identifying the set of speakers in anygiven conversation and identifying individual speak-ers who uttered each turn in a conversation.For the task of identifying the set of speakers inany given conversation, we evaluate performanceusing precision and recall, which are defined as fol-lows: If the conversation actually contained r char-acters, the system predicted that it contained t char-acters, and got k right, then:Precision =kt;Recall =kr(3)The results are summarized in Table 1.
In the ta-ble, NB-prior indicates that the prior for the binaryclassifier was determined based on the number ofconversations each character appeared in, while NBindicates that the prior was uniform (i.e., for eachcharacter, P (present) = P (absent) = 0.5).
Wefind that the results obtained using the author-topicmodel are significantly better than each of the otherthree models.On average, the number of speakers in each con-versation in the test data was 2.44 (the correspond-System AccuracyESHMM 27.13%Speaker-LM HMM 25.04%ARTM 23.64%Author Topic Model 26.2%NB 23.41%NB-prior 21.39%Random Baseline 14.29%Table 2: Results for predicting speakers of utterancesing number in the training data appears to be some-what higher at 2.65).
Our attempts to restrict the setof characters in a real setting plays a significant rolehere as we shall discuss later.The Naive Bayes classifier with empirical priorson average predicted that there were 1.3 characterspresent per conversation, while the version with uni-form priors predicted 2.2 characters to be present perconversation on average.
The author-topic model,on average, over-estimated the number of charactersat 2.86 characters per conversation.For the task of predicting the speaker, given an ut-terance, we have two kinds of Hidden Markov Mod-els, the Exponential State HMM (ESHMM) and andHMM with emission probabilities based on individ-ual speaker language models (Speaker LM HMM).We also have the topic model based systems- theARTM and the ATM.
Finally, we have the baselinemodels- the Naive Bayes with empirical priors andwith uniform priors, and the random baseline.
Table2 summarizes their performance.
In this case, weonly report accuracy.
Since each turn has only onespeaker, we can constrain each of the models to pro-duce one speaker, in order to calculate the accuracy.The HMM and topic based models all incorporatesequence information in some form.
In the case ofthe HMM based models, state transitions are condi-tioned on the previous speaker.
In the case of thetopic model based systems, the feature vectors con-tain context, although the task is modeled as a dis-criminative classification task.
The ESHMM modelworked the best on this dataset.
With the exceptionof the ATM and the speaker LM HMM (p < 0.10),the improvements obtained by using the ESHMMover all other models were statistically significant(p < 0.05).
Surprisingly, the single speaker LM37HMM and the ATM both outperform the ARTM onthis task.
One of the reasons for this could be thatthe ARTM does not suitably capture what we hopedit would, perhaps because of the fact that the recipi-ents (addressees) are not observed.6 ConclusionIn this paper, we presented a set of latent variablemodel based approaches to analyzing conversationstructure using the text transcript of the conversa-tions only.
The initial set of experiments showpromising improvements over simple baseline meth-ods, though the overall results leave considerableroom for improvement.
Conversations are a dy-namic process, with the content varying significantlywith time, and the use of formulations such as dy-namic topic modeling (Blei and Lafferty, 2006) mayhelp.We believe that the concept of modeling speak-ers and addressees would be a powerful one in mod-eling conversation structure and useful in applica-tions such as those involving automated agents, orin understanding discourse on discussion forums, aswell as understanding development of authority insuch forums.
The state sequences predicted by theESHMM implicitly predict addressees for each turn.This is not directly used in our tasks, but could beuseful for automated agents, in understanding appro-priate moments to take its turn.The dataset used in this case introduced somenoise.
We decided to subsume everyone aside fromthe 6 main characters under the moniker other, in or-der to keep the state space manageable.
In reality, itwas a collection of a few dozen characters, some ofwhom appeared intermittently through the episodes.As a result, the emission model for this state was nota stable one.
The system rarely predicted this class,and had very low accuracy when it did.Further, development of datasets with annotationsspecifying the addressees explicitly would probablyaccelerate development of methods that work wellin such settings.ReferencesAndrew McCallum, Xuerui Wang and Andres Corrada-Emmanuel.
2007.
Topic and Role Discovery in SocialNetworks with Experiments on Enron and AcademicEmail.
In Journal of Artificial Intelligence Research..Andreas Stolcke.
2002.
SRILM an Extensible LanguageModeling Toolkit.
In ICSLP.D.
A. Reynolds and P. Torres-Carrasquillo.
2005.
Ap-proaches and applications of audio diarization.
InProc.
of ICASSP.David M. Blei and John D. Lafferty.
2006.
DynamicTopic Models.
In Proceedings of ICML.George Doddington.
2001.
Speaker Recognition basedon Idiolectal Differences between Speakers.
In Eu-rospeech..S.E.
Tranter and D.A.
Reynolds.
2006.
An overview ofautomatic speaker diarization systems.
In IEEE Trans-actions on Audio, Speech and Language Processing..Athanasios Noulas, Gwenn Englebienne, Ben J.A.
Krse2011.
Multimodal Speaker Diarization.
In IEEETransactions on Pattern Analysis and Machine Intel-ligence..Howard Lei and Nikki Mirghafori.
2007.
Word-Conditioned Phone n-grams For Speaker Recognition.In Proceedings of ICASSPJohn Shawe Taylor and Nello Cristianini.
2000.
Sup-port Vector Machines and other Kernel Based Learn-ing Methods.
Cambridge University Press..Kazuhiro Otsuka, Yoshinao Takemae and Junji Yam-ato.
2005.
A probabilistic inference of multiparty-conversation structure based on Markov-switchingmodels of gaze patterns, head directions, and utter-ances.
In Proceedings of the 7th international con-ference on Multimodal interfaces.L.R.
Rabiner.
1989.
A tutorial on Hidden Markov Mod-els and selected applications in speech recognition.
InProceedings of IEEE.Leonardo Canseco, Lori Lamel and Jean-Luc Gauvain2005.
A Comparative Study using Manual and Auto-matic Transcriptions for Diarization.
In Proceedingsof ASRU.Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers andPadhraic Smyth.
2004.
The Author-Topic Model forAuthors and Documents.
In 20th Conference on Un-certainty in Artificial Intelligence.Natasa Jovanovic, Rieks op den Akker and Anton Nijholt.2006.
Addressee Identification in Face-to-Face Meet-ings In Proc.
of EACL.Rieks op den Akker and David Traum.
2009.
A Compar-ison of Addressee Detection Methods for MultipartyConversations.
In Proc.
of Diaholmia 2009.Thomas Hofmann.
1999.
Probabilistic latent semanticanalysis.
In Proc of UAI..Xavier Anguera, Chuck Wooters and Javier Hernando2005.
Speaker Diarization for Multi-Party Meetingsusing Acoustic Fusion In IEEE Workshop on Auto-matic Speech Recognition and Understanding..38
