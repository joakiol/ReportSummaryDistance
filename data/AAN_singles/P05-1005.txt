Proceedings of the 43rd Annual Meeting of the ACL, pages 34?41,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsLearning Semantic Classes for Word Sense DisambiguationUpali S. Kohomban Wee Sun LeeDepartment of Computer ScienceNational University of SingaporeSingapore, 117584{upalisat,leews}@comp.nus.edu.sgAbstractWord Sense Disambiguation suffers froma long-standing problem of knowledge ac-quisition bottleneck.
Although state of theart supervised systems report good accu-racies for selected words, they have notbeen shown to be promising in terms ofscalability.
In this paper, we present an ap-proach for learning coarser and more gen-eral set of concepts from a sense taggedcorpus, in order to alleviate the knowl-edge acquisition bottleneck.
We show thatthese general concepts can be transformedto fine grained word senses using simpleheuristics, and applying the technique forrecent SENSEVAL data sets shows that ourapproach can yield state of the art perfor-mance.1 IntroductionWord Sense Disambiguation (WSD) is the task ofdetermining the meaning of a word in a given con-text.
This task has a long history in natural languageprocessing, and is considered to be an intermediatetask, success of which is considered to be importantfor other tasks such as Machine Translation, Lan-guage Understanding, and Information Retrieval.Despite a long history of attempts to solve WSDproblem by empirical means, there is not any clearconsensus on what it takes to build a high perfor-mance implementation of WSD.
Algorithms basedon Supervised Learning, in general, show better per-formance compared to unsupervised systems.
Butthey suffer from a serious drawback: the difficultyof acquiring considerable amounts of training data,also known as knowledge acquisition bottleneck.
Inthe typical setting, supervised learning needs train-ing data created for each and every polysemousword; Ng (1997) estimates an effort of 16 person-years for acquiring training data for 3,200 significantwords in English.
Mihalcea and Chklovski (2003)provide a similar estimate of an 80 person-year ef-fort for creating manually labelled training data forabout 20,000 words in a common English dictionary.Two basic approaches have been tried as solu-tions to the lack of training data, namely unsu-pervised systems and semi-supervised bootstrappingtechniques.
Unsupervised systems mostly workon knowledge-based techniques, exploiting senseknowledge encoded in machine-readable dictionaryentries, taxonomical hierarchies such as WORD-NET (Fellbaum, 1998), and so on.
Most of thebootstrapping techniques start from a few ?seed?
la-belled examples, classify some unlabelled instancesusing this knowledge, and iteratively expand theirknowledge using information available within newlylabelled data.
Some others employ hierarchical rel-atives such as hypernyms and hyponyms.In this work, we present another practical alterna-tive: we reduce the WSD problem to a one of findinggeneric semantic class of a given word instance.
Weshow that learning such classes can help relieve theproblem of knowledge acquisition bottleneck.1.1 Learning senses as conceptsAs the semantic classes we propose learning, weuse WORDNET lexicographer file identifiers corre-34sponding to each fine-grained sense.
By learningthese generic classes, we show that we can reusetraining data, without having to rely on specifictraining data for each word.
This can be done be-cause the semantic classes are common to wordsunlike senses; for learning the properties of a givenclass, we can use the data from various words.
Forinstance, the noun crane falls into two semanticclasses ANIMAL and ARTEFACT.
We can expect thewords such as pelican and eagle (in the bird sense)to have similar usage patterns to those of ANIMALsense of crane, and to provide common training ex-amples for that particular class.For learning these classes, we can make use of anytraining example labelled with WORDNET sensesfor supervised WSD, as we describe in section 3.1.Once the classification is done for an instance, theresulting semantic classes can be transformed intofiner grained senses using some heuristical mapping,as we show in the next sub section.
This would notguarantee a perfect conversion because such a map-ping can miss some finer senses, but as we show inwhat follows, this problem in itself does not preventus from attaining good performance in a practicalWSD setting.1.2 Information loss in coarse grained sensesAs an empirical verification of the hypothesis thatwe can still build effective fine-grained sense dis-ambiguators despite the loss of information, we an-alyzed the performance of a hypothetical coarsegrained classifier that can perform at 100% accu-racy.
As the general set of classes, we used WORD-NET unique beginners, of which there are 25 fornouns, and 15 for verbs.To simulate this classifier on SENSEVAL Englishall-words tasks?
data (Edmonds and Cotton, 2001;Snyder and Palmer, 2004), we mapped the fine-grained senses from official answer keys to theirrespective beginners.
There is an information lossin this mapping, because each unique beginner cantypically include more than one sense.
To see howthis ?classifier?
fares in a fine-grained task, we canmap the ?answers?
back to WORDNET fine-grainedsenses by picking up the sense with the lowest sensenumber that falls within each unique beginner.
Inprincipal, this is the most likely sense within theclass, because WORDNET senses are said to be	   Figure 1: Performance of a hypothetical coarse-grained classifier, output mapped to fine-grainedsenses, on SENSEVAL English all-words tasks.ordered in descending order of frequency.
Sincethis sense is not necessarily the same as the origi-nal sense of the instance, the accuracy of the fine-grained answers will be below 100%.Figure 1 shows the performance of this trans-formed fine-grained classifier (CG) for nouns andverbs with SENSEVAL-2 and 3 English all wordstask data (marked as S2 and S3 respectively),along with the baseline WORDNET first sense (BL),and the best-performer classifiers at each SENSE-VAL excercise (CL), SMUaw (Mihalcea, 2002) andGAMBL-AW (Decadt et al, 2004) respectively.There is a considerable difference in terms of im-provement over baseline, between the state-of-the-art systems and the hypothetical optimal coarse-grained system.
This shows us that there is an im-provement in performance that we can attain overthe state-of-the-art, if we can create a classifier foreven a very coarse level of senses, with sufficientlyhigh accuracy.
We believe that the chances for sucha high accuracy in a coarse-grained sense classifieris better, for several reasons:?
previously reported good performance forcoarse grained systems (Yarowsky, 1992)?
better availability of data, due to the possibil-ity of reusing data created for different words.For instance, labelled data for the noun ?crane?is not found in SEMCOR corpus at all, butthere are more than 1000 sample instances forthe concept ANIMAL, and more than 9000 forARTEFACT.35?
higher inter-annotator agreement levels andlower corpus/genre dependencies in train-ing/testing data due to coarser senses.1.3 Overall approachBasically, we assume that we can learn the ?con-cepts?, in terms of WORDNET unique beginners, us-ing a set of data labelled with these concepts, re-gardless of the actual word that is labelled.
Hence,we can use a generic data set that is large enough,where various words provide training examples forthese concepts, instead of relying upon data from theexamples of the same word that is being classified.Unfortunately, simply labelling each instancewith its semantic class and then using standard su-pervised learning algorithms did not work well.
Thisis probably because the effectiveness of the featurepatterns often depend on the actual word being dis-ambiguated and not just its semantic class.
For ex-ample, the phrase ?run the newspaper?
effectivelyindicates that ?newspaper?
belongs to the seman-tic class GROUP.
But ?run the tape?
indicates that?tape?
belongs to the semantic class ARTEFACT.
Thecollocation ?run the?
is effective for indicating theGROUP sense only for ?newspaper?
and closely re-lated words such as ?department?
or ?school?.In this experiment, we use a k-nearest neighborclassifier.
In order to allow training examples ofdifferent words from the same semantic class toeffectively provide information for each other, wemodify the distance between instances in a waythat makes the distance between instances of simi-lar words smaller.
This is described in Section 3.The rest of the paper is organized as follows: Insection 2, we discuss several related work.
We pro-ceed on to a detailed description of our system insection 3, and discuss the empirical results in section4, showing that our representation can yield state ofthe art performance.2 Related WorkUsing generic classes as word senses has beendone several times in WSD, in various contexts.Resnik (1997) described a method to acquire a setof conceptual classes for word senses, employingselectional preferences, based on the idea that cer-tain linguistic predicates constraint the semantic in-terpretation of underlying words into certain classes.The method he proposed could acquire these con-straints from a raw corpus automatically.Classification proposed by Levin (1993) for Eng-lish verbs remains a matter of interest.
Althoughthese classes are based on syntactic properties unlikethose in WORDNET, it has been shown that they canbe used in automatic classifications (Stevenson andMerlo, 2000).
Korhonen (2002) proposed a methodfor mapping WORDNET entries into Levin classes.WSD System presented by Crestan et al (2001)in SENSEVAL-2 classified words into WORD-NET unique beginners.
However, their approachdid not use the fact that the primes are common forwords, and training data can hence be reused.Yarowsky (1992) used Roget?s Thesaurus cate-gories as classes for word senses.
These classes dif-fer from those mentioned above, by the fact that theyare based on topical context rather than syntax orgrammar.3 Basic Design of the SystemThe system consists of three classifiers, built usinglocal context, part of speech and syntax-based rela-tionships respectively, and combined with the most-frequent sense classifier by using weighted major-ity voting.
Our experiments (section 4.3) show thatbuilding separate classifiers from different subsetsof features and combining them works better thanbuilding one classifier by concatenating the featurestogether.For training and testing, we used publicly avail-able data sets, namely SEMCOR corpus (Miller etal., 1993) and SENSEVAL English all-words taskdata.
In order to evaluate the systems performancein vivo, we mapped the outputs of our classifier tothe answers given in the key.
Although we face apenalty here due to the loss of granularity, this ap-proach allows a direct comparison of actual usabilityof our system.3.1 DataAs training corpus, we used Brown-1 and Brown-2 parts of SEMCOR corpus; these parts have all oftheir open-class words tagged with correspondingWORDNET senses.
A part of the training corpus wasset aside as the development corpus.
This part wasselected by randomly selecting a portion of multi-36class words (600 instances for each part of speech)from the training data set.
As labels, the seman-tic class (lexicographic file number) was extractedfrom the sense key of each instance.
Testing datasets from SENSEVAL-2 and SENSEVAL-3 Englishall-words tasks were used as testing corpora.3.2 FeaturesThe feature set we selected was fairly simple; Aswe understood from our initial experiments, wide-window context features and topical context werenot of much use for learning semantic classes froma multi-word training data set.
Instead of general-izing, wider context windows add to noise, as seenfrom validation experiments with held-out data.Following are the features we used:3.2.1 Local contextThis is a window of n words to the left, and nwords to the right, where n ?
{1, 2, 3} is a parame-ter we selected via cross validation.1Punctuation marks were removed and all wordswere converted into lower case.
The feature vec-tor was calculated the same way for both nouns andverbs.
The window did not exceed the boundariesof a sentence; when there were not enough words toeither side of the word within the window, the valueNULL was used to fill the remaining positions.For instance, for the noun ?companion?
in sen-tence (given with POS tags)?Henry/NNP peered/VBD doubtfully/RBat/IN his/PRP$ drinking/NN compan-ion/NN through/IN bleary/JJ ,/, tear-filled/JJ eyes/NNS ./.
?the local context feature vector is [at,his, drinking, through, bleary,tear-filled], for window size n = 3.
Noticethat we did not consider the hyphenated words astwo words, when the data files had them annotatedas a single token.3.2.2 Part of speechThis consists of parts of speech for a window ofn words to both sides of word (excluding the word1Validation results showed that a window of two words toboth sides yields the best performance for both local context andPOS features.
n = 2 is the size we used in actual evaluation.Feature Example ValuenounsSubject - verb [art] represents a culture representVerb - object He sells his [art] sellAdjectival modifiers the ancient [art] of runes ancientPrepositional connectors academy of folk [art] academy ofPost-nominal modifiers the [art] of fishing of fishingverbsSubject - verb He [sells] his art heVerb - object He [sells] his art artInfinitive connector He will [sell] his art heAdverbial modifier He can [paint] well wellWords in split infinitives to boldly [go] boldlyTable 1: Syntactic relations used as features.
Thetarget word is shown inside [brackets]itself), with quotation signs and punctuation marksignored.
For SEMCOR files, existing parts of speechwere used; for SENSEVAL data files, parts of speechfrom the accompanying Penn-Treebank parsed datafiles were aligned with the XML data files.
Thevalue vector is calculated the same way as the lo-cal context, with the same constraint on sentenceboundaries, replacing vacancies with NULL.As an example, for the sentence we used in theprevious example, the part-of-speech vector withcontext size n = 3 for the verb peered is [NULL,NULL, NNP, RB, IN, PRP$].3.2.3 Syntactic relations with the wordThe words that hold several kinds of syntactic re-lations with the word under consideration were se-lected.
We used Link Grammar parser due to Sleatorand Temperley (1991) because of the information-rich parse results produced by it.Sentences in SEMCOR corpus files and the SEN-SEVAL files were parsed with Link parser, and wordswere aligned with links.
A given instance of a wordcan have more than one syntactic features present.Each of these features was considered as a binaryfeature, and a vector of binary values was con-structed, of which each element denoted a uniquefeature found in the test set of the word.Each syntactic pattern feature falls into either oftwo types collocation or relation:Collocation features Collocation features aresuch features that connect the word under consid-eration to another word, with a preposition or an in-finitive in between ?
for instance, the phrase ?artof change-ringing?
for the word art.
For these fea-tures, the feature value consists of two words, whichare connected to the given word either from left or37from right, in a given order.
For the above example,the feature value is [?.of.change-ringing],where ?
denotes the placeholder for word underconsideration.Relational features Relational features representmore direct grammatical relationships, such assubject-verb or noun-adjective, the word under con-sideration has with surrounding words.
Whenencoding the feature value, we specified the re-lation type and the value of the feature in thegiven instance.
For instance, in the phrase ?Henrypeered doubtfully?, the adverbial modifier featurefor the verb ?peered?
is encoded as [adverb-moddoubtfully].A description of the relations for each part ofspeech is given in the table 1.3.3 Classifier and instance weightingThe classifier we used was TiMBL, a memory basedlearner due to Daelemans et al (2003).
One reasonfor this choice was that memory based learning hasshown to perform well in previous word sense dis-ambiguation tasks, including some best performersin SENSEVAL, such as (Hoste et al, 2001; Decadtet al, 2004; Mihalcea and Faruque, 2004).
Anotherreason is that TiMBL supported exemplar weights, anecessary feature for our system for the reasons wedescribe in the next section.One of the salient features of our system is that itdoes not consider every example to be equally im-portant.
Due to the fact that training instances fromdifferent instances can provide confusing examples,as shown in section 1.3, such an approach cannot betrusted to give good performance; we verified thisby our own findings through empirical evaluationsas shown in section 4.2.3.3.1 Weighting instances with similarityWe use a similarity based measure to assignweights to training examples.
In the method we use,these weights are used to adjust the distances be-tween the test instance and the example instances.The distances are adjusted according to the formula?E(X,Y ) =?
(X,Y )ewX + ,where ?E(X,Y ) is the adjusted distance betweeninstance Y and example X , ?
(X,Y ) is the originaldistance, ewX is the exemplar weight of instance X .The small constant  is added to avoid division byzero.There are various schemes used to measure inter-sense similarity.
Our experiments showed that themeasure defined by Jiang and Conrath (1997) (JCn)yields best results.
Results for various weightingschemes are discussed in section 4.2.3.3.2 Instance weighting explainedThe exemplar weights were derived from the fol-lowing method:1. pick a labelled example e, and extract its sensese and semantic class ce.2.
if the class ce is a candidate for the current testword w, i.e.
w has any senses that fall intoce, find out the most frequent sense of w, scew ,within ce.
We define the most frequent sensewithin a class as the sense that has the lowestWORDNET sense number within that class.
Ifnone of the senses of w fall into ce, we ignorethat example.3.
calculate the relatedness measure between seand scew , using whatever the similarity metricbeing considered.
This is the exemplar weightfor example e.In the implementation, we used freely availableWordNet::Similarity package (Pedersen etal., 2004).
23.4 Classifier optimizationA part of SEMCOR corpus was used as a validationset (see section 3.1).
The rest was used as trainingdata in validation phase.
In the preliminary experi-ments, it was seen that the generally recommendedclassifier options yield good enough performance,although variations of switches could improve per-formance slightly in certain cases.
Classifier op-tions were selected by a search over the availableoption space for only three basic classifier parame-ters, namely, number of nearest neighbors, distancemetric and feature weighting scheme.2WordNet::Similarity is a perl package availablefreely under GNU General Public Licence.
http://wn-similarity.sourceforge.net.38Classifier Senseval-2 Senseval-3Baseline 0.617 0.627POS 0.616 0.614Local context 0.627 0.633Synt.
Pat 0.620 0.612Concatenated 0.609 0.611Combined 0.631 0.643Table 2: Results of baseline, individual, and com-bined classifiers: recall measures for nouns andverbs combined.4 ResultsIn what follows, we present the results of our ex-periments in various test cases.3 We combined thethree classifiers and the WORDNET first-sense clas-sifier through simple majority voting.
For evaluatingthe systems with SENSEVAL data sets, we mappedthe outputs of our classifiers to WORDNET sensesby picking the most-frequent sense (the one with thelowest sense number) within each of the class.
Thismapping was used in all tests.
For all evaluations,we used SENSEVAL official scorer.We could use the setting only for nouns and verbs,because the similarity measures we used were notdefined for adjectives or adverbs, due to the fact thathypernyms are not defined for these two parts ofspeech.
So we list the initial results only for nounsand verbs.4.1 Individual classifiers vs. combinationWe evaluated the results of the individual classifiersbefore combination.
Only local context classifiercould outperform the baseline in general, althoughthere is a slight improvement with the syntactic pat-tern classifier on SENSEVAL-2 data.The results are given in the table 2, togetherwith the results of voted combination, and baselineWORDNET first sense.
Classifier shown as ?con-catenated?
is a single classifier trained from all ofthese feature vectors concatenated to make a sin-gle vector.
Concatenating features this way does notseem to improve performance.
Although exact rea-sons for this are not clear, this is consistent with pre-3Note that the experiments and results are reported for SEN-SEVAL data for comparison purposes, and were not involved inparameter optimization, which was done with the developmentsample.Senseval-2 Senseval-3No similarity used 0.608 0.599Resnik 0.540 0.522JCn 0.631 0.643Table 3: Effect of different similarity schemes onrecall, combined results for nouns and verbsSenseval-2 Senseval-3SM 0.631 0.643GW 0.634 0.649LW 0.641 0.650Table 4: Improvement of performance with classifierweighting.
Combined results for nouns and verbswith voting schemes Simple Majority (SM), Globalclassifier weights (GW) and local weights (LW).vious observations (Hoste et al, 2001; Decadt et al,2004) that combining classifiers, each using differ-ent features, can yield good performance.4.2 Effect of similarity measureTable 3 shows the effect of JCn and Resnik simi-larity measures, along with no similarity weighting,for the combined classifier.
It is clear that propersimilarity measure has a major impact on the perfor-mance, with Resnik measure performing worse thanthe baseline.4.3 Optimizing the voting processSeveral voting schemes were tried for combiningclassifiers.
Simple majority voting improves perfor-mance over baseline.
However, previously reportedresults such as (Hoste et al, 2001) and (Decadt et al,2004) have shown that optimizing the voting processhelps improve the results.
We used a variation ofWeighted Majority Algorithm (Littlestone and War-muth, 1994).
The original algorithm was formulatedfor binary classification tasks; however, our use of itfor multi-class case proved to be successful.We used the held-out development data set for ad-justing classifier weights.
Originally, all classifiershave the same weight of 1.
With each test instance,the classifier builds the final output considering theweights.
If this output turns out to be wrong, theclassifiers that contributed to the wrong answer gettheir weights reduced by some factor.
We could ad-39Senseval-2 Senseval-3System 0.777 0.806Baseline 0.756 0.783Table 5: Coarse grained resultsjust the weights locally or globally; In global setting,the weights were adjusted using a random sampleof held-out data, which contained different words.These weights were used for classifying all wordsin the actual test set.
In local setting, each classifierweight setting was optimized for individual wordsthat were present in test sets, by picking up randomsamples of the same word from SEMCOR .4 Table 4shows the improvements with each setting.Coarse grained (at semantic-class level) resultsfor the same system are shown in table 5.
Baselinefigures reported are for the most-frequent class.4.4 Final results on SENSEVAL dataHere, we list the performance of the system with ad-jectives and adverbs added for the ease of compar-ison.
Due to the facts mentioned at the beginningof this section, our system was not applicable forthese parts of speech, and we classified all instancesof these two POS types with their most frequentsense.
We also identified the multi-word phrasesfrom the test documents.
These phrases generallyhave a unique sense in WORDNET ; we markedall of them with their first sense without classify-ing them.
All the multiple-class instances of nounsand verbs were classified and converted to WORD-NET senses by the method described above, with lo-cally optimized classifier voting.The results of the systems are shown in tables 7and 8.
Our system?s results in both cases are listedas Simil-Prime, along with the baseline WORD-NET first sense (including multi-word phrases and?U?
answers), and the two best performers?
resultsreported.5 These results compare favorably with theofficial results reported in both tasks.4Words for which there were no samples in SEMCOR wereclassified using a weight of 1 for all classifiers.5The differences of the baseline figures from the previouslyreported figures are clearly due to different handling of multi-word phrases, hyphenated words, and unknown words in eachsystem.
We observed by analyzing the answer keys that evenbetter baseline figures are technically possible, with better tech-niques to identify these special cases.Senseval-2 Senseval-3Micro Average < 0.0001 < 0.0001Macro Average 0.0073 0.0252Table 6: One tailed paired t-test significance levelsof results: P (T 6 t)System RecallSMUaw (Mihalcea, 2002) 0.690Simil-Prime 0.664Baseline (WORDNET first sense) 0.648CNTS-Antwerp (Hoste et al, 2001) 0.636Table 7: Results for SENSEVAL-2 English all wordsdata for all parts of speech and fine grained scoring.Significance of results To verify the significanceof these results, we used one-tailed paired t-test, us-ing results of baseline WORDNET first sense andour system as pairs.
Tests were done both at micro-average level and macro-average level, (consideringtest data set as a whole and considering per-word av-erage).
Null hypothesis was that there is no signif-icant improvement over the baseline.
Both settingsyield good significance levels, as shown in table 6.5 Conclusion and Future WorkWe analyzed the problem of Knowledge AcquisitionBottleneck in WSD, proposed using a general set ofsemantic classes as a trade-off, and discussed whysuch a system is promising.
Our formulation al-lowed us to use training examples from words dif-ferent from the actual word being classified.
Thismakes the available labelled data reusable for differ-ent words, relieving the above problem.
In order tofacilitate learning, we introduced a technique basedon word sense similarity.The generic classes we learned can be mapped toSystem RecallSimil-Prime 0.661GAMBL-AW-S (Decadt et al, 2004) 0.652SenseLearner (Mihalcea and Faruque, 2004) 0.646Baseline (WORDNET first sense) 0.642Table 8: Results for SENSEVAL-3 English all wordsdata for all parts of speech and fine grained scoring.40finer grained senses with simple heuristics.
Throughempirical findings, we showed that our system canattain state of the art performance, when applied tostandard fine-grained WSD evaluation tasks.In the future, we hope to improve on these results:Instead of using WORDNET unique beginners, usingmore natural semantic classes based on word usagewould possibly improve the accuracy, and findingsuch classes would be a worthwhile area of research.As seen from our results, selecting correct similaritymeasure has an impact on the final outcome.
Wehope to work on similarity measures that are moreapplicable in our task.6 AcknowledgementsAuthors wish to thank the three anonymous review-ers for their helpful suggestions and comments.ReferencesE.
Crestan, M. El-Be`ze, and C. De Loupy.
2001.
Improv-ing wsd with multi-level view of context monitored bysimilarity measure.
In Proceeding of SENSEVAL-2:Second International Workshop on Evaluating WordSense Disambiguation Systems, Toulouse, France.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2003.
TiMBL: Tilburg MemoryBased Learner, version 5.0, reference guide.
Technicalreport, ILK 03-10.Bart Decadt, Ve?ronique Hoste, Walter Daelemans, andAntal Van den Bosch.
2004.
GAMBL, geneticalgorithm optimization of memory-based wsd.
InSenseval-3: Third Intl.
Workshop on the Evaluation ofSystems for the Semantic Analysis of Text.P.
Edmonds and S. Cotton.
2001.
Senseval-2: Overview.In Proc.
of the Second Intl.
Workshop on EvaluatingWord Sense Disambiguation Systems (Senseval-2).C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
The MIT Press, Cambridge, MA.Ve?ronique Hoste, Anne Kool, and Walter Daelmans.2001.
Classifier optimization and combination in Eng-lish all words task.
In Proceeding of SENSEVAL-2:Second International Workshop on Evaluating WordSense Disambiguation Systems.J.
Jiang and D. Conrath.
1997.
Semantic similarity basedon corpus statistics and lexical taxonomy.
In Proceed-ings of International Conference on Research in Com-putational Linguistics.Anna Korhonen.
2002.
Assigning verbs to semanticclasses via wordnet.
In Proceedings of the COLINGWorkshop on Building and Using Semantic Networks.Beth Levin.
1993.
English Verb Classes and Alterna-tions.
University of Chicago Press, Chicago, IL.N Littlestone and M.K.
Warmuth.
1994.
The weightedmajority algorithm.
Information and Computation,108(2):212?261.Rada Mihalcea and Tim Chklovski.
2003.
Open MindWord Expert: Creating large annotated data collec-tions with web users?
help.
In Proceedings of theEACL 2003 Workshop on Linguistically AnnotatedCorpora.Rada Mihalcea and Ehsanul Faruque.
2004.
Sense-learner: Minimally supervised word sense disam-biguation for all words in open text.
In Senseval-3:Third Intl.
Workshop on the Evaluation of Systems forthe Semantic Analysis of Text.Rada Mihalcea.
2002.
Bootstrapping large sense taggedcorpora.
In Proc.
of the 3rd Intl.
Conference on Lan-guages Resources and Evaluations.G.
Miller, C. Leacock, T. Randee, and R. Bunker.
1993.A semantic concordance.
In Proc.
of the 3rd DARPAWorkshop on Human Language Technology.Hwee Tou Ng.
1997.
Getting serious about word sensedisambiguation.
In Proceedings of the ACL SIGLEXWorkshop on Tagging Text with Lexical Semantics:Why, What, and How?, pages 1?7.T.
Pedersen, S. Patwardhan, and J. Michelizzi.
2004.Wordnet::Similarity - Measuring the relatedness ofconcepts.
In Proceedings of the Nineteenth NationalConference on Artificial Intelligence (AAAI-04).P.
Resnik.
1997.
Selectional preference and sense dis-ambiguation.
In Proc.
of ACL Siglex Workshop onTagging Text with Lexical Semantics, Why, What andHow?D.
Sleator and D. Temperley.
1991.
Parsing English witha Link Grammar.
Technical report, Carnegie MellonUniversity Computer Science CMU-CS-91-196.B.
Snyder and M. Palmer.
2004.
The English all-wordstask.
In Senseval-3: Third Intl.
Workshop on the Eval-uation of Systems for the Semantic Analysis of Text.Suzanne Stevenson and Paola Merlo.
2000.
Automaticlexical acquisition based on statistical distributions.
InProc.
of the 17th conf.
on Computational linguistics.David Yarowsky.
1992.
Word-sense disambiguation us-ing statistical models of Roget?s categories trained onlarge corpora.
In Proceedings of COLING-92, pages454?460.41
