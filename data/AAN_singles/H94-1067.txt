MICROPHONE-INDEPENDENT ROBUST SIGNAL PROCESSING USINGPROBABILISTIC OPTIMUM FILTERINGLeonardo Neumeyer and Mitchel WeintraubSRI InternationalSpeech Technology and Research Laboratory333 Ravenswood AvenueMenlo Park, CA 94025ABSTRACTA new mapping algorithm for speech recognition relates the fea-tures of simultaneous recordings of clean and noisy speech.
Themodel is a piecewise nonfinear t ansformation appfied to the noisyspeech feature.
The transformation is a set of multidimensionallinear least-squares filters whose outputs are combined using aconditional Gaussian model.
The algorithm was tested using SRI'sDECIPHER TM speech recognition system \[1-5\].
Experimentalresults how how the mapping is used to reduce recognition errorswhen the training and testing acoustic environments donot match.1.
INTRODUCTIONIn many practical situations an automatic speech recognizer has tooperate in several different but well-defined acoustic environ-ments.
For example, the same recognition task may be imple-mented using different microphones ortransmission channels.
Inthis situation it may not be practical to recollect aspeech corpus totrain the acoustic models of the recognizer.
To alleviate this prob-lem, we propose an algorithm that maps speech features betweentwo acoustic spaces.
The models of the mapping algorithm aretrained using a small database recorded simultaneously in bothenvironments.In the case of steady-state additive homogenous noise, we canderive a MMSE estimate of the clean speech filterbank-log energyfeatures using a model for how the features change in the presenceof this noise \[6-7\].
In these algorithms, the estimated speech spec-trum is a function of the global spectral signal-to-noise ratio(SNR), the instantaneous spectral SNR, and the overall spectralshape of the speech signal.
However, after studying simultaneousrecordings made with two microphones, we befieve that the rela-tionship between the two simultaneous features is nonlinear.
Wetherefore propose to use a piecewise-nonlinear model to relate thetwo feature spaces.1.1.
Related Work on Feature MappingSeveral algorithms in the literature have focused on experimen-tally training a mapping between the noisy features and the cleanfeatures \[8-13\].
The proposed algorithm differs from previousalgorithms in several ways:?
The MMSE estimate of the clean speech features in noise istrained experimentally rather than with a model as in \[6, 7\].?
Several frames are joined together similar to \[13\].?
The conditional PDF is based on a generic noisy feature notnecessarily related to the feature that we are trying to esti-mate.
For example, we could condition the estimate of thecepstral energy on the instantaneous spectral SNR vector.?
Multidimensional least-squares filters are used for the map-ping transformation.
This exploits the correlation of the fea-tures over time and among components of the spectral,features at the same time.?
Linear transformations are combined together without harddecisions.?
All delta parameters are computed after mapping the cep-strum and cepstral energy.?
The mapping parameters are trained using stereo recordingswith two different microphones.
Once trained, the mappingparameters are fixed.?
The algorithm can either map noisy speech features to cleanfeatures during training, or clean features to noisy featuresduring recognition.1.2.
Related Work on AdaptationThe algorithm used to map the incoming features into a morerobust representation has some similarities to work on modeladaptation.
Some of the high-level differences between hiddenMarkov model (HMM) adaptation and the mapping algorithmsproposed in this paper are:?
The mapping algorithm works by primarily correcting shiftsin the mean of the feature set that are correlated withobservable information.
Adapting HMM model parametershas certain degrees of freedom that the mapping algorithmdoes not have- for example the ability to change state vari-ances, and mixture weights.?
Two HMM states that have identical probability distribu-tions and are not tied can have different distributions afteradaptation.
These distributions cannot be differentiated bymapping features.?
The mapping algorithms described in this paper are able toincoiporate many pieces of information that have been tra-336difionaUy difficult o incorporate into HMM models and intoadaptation algorithms.
These include observations that spanacross everal frames and the correlation of the state fea-tures with global characteristics of the speech waveform.These two techniques are not mutually exclusive and can be usedtogether to achieve robust speech recognition performance.
Theboundary between these two techniques can be blurred when themapping algorithm is dependent on the speech recognizer'shypothesis.2.
THE POF ALGORITHMThe mapping algorithm is based on a probabilistic piecewise-non-linear transformation f the acoustic space that we call Probabilis-tic Optimum Filtering (POF).
Let us assume that the recognizer istrained with data recorded with a high-quality close-talking micro-phone (clean speech), and the test data is acquired in a differentacoustic environment (noisy speech).
Our goal is to estimate aclean feature vector ~ given its corresponding noisy featurenYn where n is the frame index.
(A list of symbols is shown inTable 1.)
To estimate the clean vector we vector-quantize the cleanfeature space in I regions using the generalized Lloyd algorithm\[14\].
Each VQ region is assigned amultidimensional transversalfilter (see Figure 1).
The error between the clean vector and theI~,o l A~xc n Fet rFigure 1: Multi-dimensional transversal fi ter for cluster i.estimated vectors produced by the i-th filter is given bye ni = Xn - Xni = Xn - ~i  Yn (1)where e_: is the error associated with region i, W. is the filtercoeffficidh~t matrix, and Yn is the tapped-delay lind of the noisyvectors.
Expanding these matrices we get~ = \[Ai,_p .
.
.A i ,_ l  Ai, oAi, 1 .
.
.A i ,  pb~ (2)n-p  "'" Yn-  1 Yn Yn + 1 "'" Yn + pThe conditional error in each region is defined asN-1  -pE i= E I\[%i112p%'zn) (4)n=pwhere p(gilzn ) is the probability that the clean vector x ibelongs to region gi given an arbitrary conditional noisy featurevector z n .
Note that the conditioning noisy feature can be anyacoustic vector generated from the noisy speech frame.
For exam-ple, it may include an estimate of the SNR, energy, cepstral energy,eepstrum, and so forth.The conditional probability density function p(Znlg i) is modeledas a mixture of I Gaussian distributions.
Each Gaussian distribu-tion models a VQ region.
The parameters of the distributions(mean vectors and covariance matrices) are estimated using thecorresponding z n vectors associated with that region.
The poste-rior probabilities p(gilzn ) are computed using Bayes' theoremand the mixture weights P( gil are estimated using the relativenumber of training clean vectors that are assigned to a given VQfregion.Symbol ~ Dimension Descriptionn 1 frame indexi 1L 1M 1N 1I 1p 1e ?
L?In!x L?Ini LxlnYn Lx lz n Mx 1ix i Mx  IZ.
MxM1W.
(2p+l)L+l x LlYn (2p+l)L+l x 1Aik LxLb i Lx  1R.
(2p+l)L+l x ~auto-correl~l (2p+I)L+l Ir; (2p+I)L+l xL  cross-correlregion indexfeature vector sizeconditioning feature vector sizenumber of training flamesnumber of VQ regionsmaximum filter delayestimation error vectordean feature vectorestimate of clean feature vectornoisy feature vectorconditioning noisy feature vectormean vector of gaussian ieovarianee matrix of gaussian itransversal fi ter coefficient matrixtap input vectormultiplicative tap matrixadditive tap matrixrrelation matrix~rrelation matrixTable 1: List of symbolsTo compute the optimum filters in the mean-squared rror sense,we minimize the conditional error in each VQ region.
The mini-mum mean-squared rror vector is obtained by taking the gradientof E i defined in Eq.
(4) with respect to the filter coefficient matrixand equating all the dements of the gradient matrix to zero.
As aresult, the optimum filter coefficient matrix has the form,W.
= RSlr.
wherel l lN- l -pRi = E Yn~n p(gi Izn )n=p(5)337is a probabilistic nonsingular uto-correlation matrix, andN-1  -pre= E YnxnTp(g ilZ n) (6)n=pis a probabilistic cross-correlation matrix.The algorithm can be completely trained without supervision andrequires no additional information other than the simultaneouswaveforms.The run-time stimate of the clean feature vector can be computedby integrating the outputs of all the filters as follows:1-1 1-1"~n= i=0E~iYnP(gilzn)={~0~iP(gilZn)}Yni= (7)3.
EXPERIMENTSA series of experiments show how the mapping algorithm can beused in a continuous peech recognizer across acoustic environ-ments.
In all of the experiments the recognizer models are trainedwith data recorded with high-quality microphones and digitallysampled at 16,000 Hz.
The analysis frame rate is 100 Hz.The tables below show three types of performance indicators:?
Relative distortion measure.
For a given component ofafeature vector we define the relative distortion between theclean and noisy data as follows:lEE<z-y)2\]d = var (x)Word recognition error.
(8)Error ratio.
The error atio is given by En/E  c whereE is the word recognition error for the test-noisy/train- nclean condition, and E c is the word recognition error ofthe test-clean/train-clean condition.3.1.
Single MicrophoneTo test he POF algorithm on a single target acoustic environmentwe used the DARPA Wall Street Journal database \[15\] on SRI'sDECIPHER TM phonetically tied-mixture speech recognition sys-tem \[2\].
The signal processing consisted of a filterbank-basedfront end that generated six feature streams: cepstrum (cl-c12),cepstral energy (cO), and their first- and second-order derivatives.Cepstral-mean normalization \[16\] was used to equalize the chan-nel.
We used simultaneous recordings of high-quality speech(Sennheiser 414 head-mounted microphone with a noise-cancel-ing element) along with speech recorded by a standard speakerphone (AT&T 720) and transmitted over local telephone lines.
Wewill refer to this stereo data as clean and noisy speech, respec-tively.
The models of the recognizer were trained using 42 maleWSJ0 training talkers (3500 sentences) recorded with a Sen-nheiser microphone.
The models of the mapping algorithm weretrained using 240 development training sentences recorded bythree speakers.
The test set consisted of 100 sentences (notincluded in the training set) recorded by the same three speakers.In this experiment we mapped two of the six features: the cep-strum (cl-c12) and the cepstral energy (cO) separately.
The deriva-tives were computed from the mapped vectors of the cepstralfeatures.
For the conditioning feature we used a 13-dimensionalcepstral vector (c0-c12) modeled with 512 Gaussians with diago-nal ?ovariance matrices.
The results are shown in Table 2.Average RecognitionFilter Coefficients Distortion Error (%) Error RatioNo mappingAi.o=I, biAi.o, biAi_ 1 .... A i .
1 , b iAi _ 2 .... Ai,.
2 , biAi,.3 .... Ai,.3 , biAi _4 .... Ai .
4 , bi0.720.620.570.510.500.490.4927.618.117.017.316.415.916.12.461.621.521.541A61.421.44Table 2: Performance ofthe POF algorithm for different num-ber of filter coefficients.
The number of Oaussian distributions is512 per feature and the conditioning feature is a 13-dimensionalcepstral vector.The baseline xperiment produced aword error rate of 27.6% onthe noisy test set, that is, 2.46 times the error obtained when usingthe clean data channel.
A 34% improvement i  recognition perfor-mance was obtained when using only the additive filter coefficientb i.
(Recognition error goes down to 18.1%.)
The best result(15.9% recognition error) was obtained for the condition p=3, inwhich six neighboring noisy frames are being used to estimate thefeature vector for the current frame.
The correlation between theaverage relative distortion between the six clean and noisy featuresand the recognition error is 0.9.3.2.
ATIS Simultaneous CorpusTo test the performance ofthe POF algorithm on multiple micro-phones we used SRI's stereo-ATIS database.
(See \[1\] for details.
)A corpus of both training and testing speech was collected usingsimultaneous recordings made from subjects wearing a SennheiserHMD 414 microphone and holding a telephone handset.
Thespeech from the telephone handset was transmitted over local tele-phone lines during data collection.
Ten different elephone hand-sets were used.
Ten male speakers were designated as trainingspeakers, and three male speakers were designated as the test set.The training set consisted of 3,000 simultaneous recordings ofSennheiser microphone and telephone speech.
The test set con-sisted of 400 simultaneous recordings of Sennheiser and telephonespeech.
The results obtained with this pilot corpus are shown inTable 3.338Acoustic Model Training Test Set Word Error (%)Training Front-End Sennheiser Telephone Data BandwidthSennheiser Wide 7.8 19.4Sennheiser Telephone 9.0 9.7ITelephone Telephone 10.0 10.3Table 3: Effect of different training and front-end bandwidth ontest set performance.
Results are word error ate on the 400 Sen-tence simultaneous test set.We can see from Table 3 that here is a 15.4% decrease inperfor-mance when using a telephone front end (7.8% increases to 9.0%word error) and testing on Sennheiser data.
This is due to the lossof information in reducing the bandwidth from 100-6400 Hz to300-3300 Hz.
However, when we are using a telephone front end,there is only a 7.8% increase in word error when testing on tele-phone speech compared to testing on Sennheiser speech (9.7%versus 9.0%).
This is a very surprising result, and we had expecteda much bigger performance difference when Sennheiser modelsare tested on telephone speech acoustics.3.3.
Multiple Microphones: Single or Multiple MappingThe POF mapping algorithm can be used in a number of wayswhen the microphone is unknown.
Some of these variations areshown in Table 4.WordExperiment ErrorSingle Mapping Combining All 10 Telephones 9.4in Training DataTrain 10 Mappings, One for Each Telephone; 9.2Run 10 Recognizers inParallel, each using Dif-ferent Mapping; Select Recognizer with HighestProbabilityTop1 9.3 Train 10 Mappings, One for EachTelephone; Run 10 Mappings inParallel and Average Features ofBest N Feature-Streams that HaveHighest LikelihoodTrain 15 Mappings for WSJ Cor-pus; Run 15 Mappings in Paralleland Average Features of Best NFeature-Streams that Have theHighest LikelihoodTop2 9.2Top3 8.98.7 Top49.8Top4Top1Top2 9.6Top3 10.310.7Table 4: Performance on the multiple-telephone handset test setwhen mapping algorithm isused in different ways.The differences between the experimental conditions are small,but the trends are different and depend on the mapping and thecorpus.
These differences depend on the similarities of the differ-ent microphones that are used in training conditions, and the rela-tionship between the training and the testing conditions.When the microphones are all similar (10 telephone mappings),then averaging the features of each mapping helps improve perfor-mance.
When the microphones are very different (e.g., those in theWSJ corpus), averaging the features of each mapping has a mini-mum when averaging two best (likelihood) feature streams.3.4.
Multiple Microphones: Conditioning FeatureThe next experiment varied the conditioning feature.
The condi-tioning feature is the feature vector used to divide the space intodifferent acoustic regions.
In each region of the acoustic space adifferent linear transformation is trained.The mapping approach was fixed: we used a single POF mappingfor multiple telephone handsets.
For this experiment we mappedthe eepstrum vector (cl-c12) and the eepstral energy (cO).
Themaximum delay of the filters was kept fixed atp=2, and the num-ber of Gaussians was 512.
The experimental variable was the fea-ture the estimates were conditioned on.
We tried the followingconditioning features:?
Cepstrum.
Same conditioning feature used in the singlemicrophone experiment (c0-c12).?
Spectral SNR.
This is an estimate of the instantaneous sig-nal-to-noise ratio computed on the log-ffilterbank energydomain.
The vector size is 25.Cepstral SNR.
This feature is generated by applying thediscrete cosine transform (DCT) to the spectral SNR.
Thetransformation reduces the dimensionality of the vector from25 to 12 elements.The results are shown in Table 5.
The baseline result is a 19.4%word error rate.
This result is achieved when the same wide-bandfront end is used for training the models with clean data and forrecognition using telephone data.
When a telephone front end \[1\]is used for training and testing, the error decreases to 9.7%.
Thedisadvantage of using this approach is that he acoustic models ofthe recognizer have to be reestimated.
However, the POF-basedfront end operates on the clean models and results in better perfor-mance.
The eepstral SNR produces the best result (8.7%).
Withthis conditioning feature we combine the effects of noise and spec-tral shape in a compact representation.WordExperiment Error (%) Error RatioWide-band front-endTelephone-bandwidth front-endMapping with cepstrumMapping with spectral SNRMapping with cepstral SNR19.49.79.48.98.72.491.241.201.141.11Table 5: Performance for the multiple-telephone handset testset when varying the conditioning feature.3394.
WSJ  EXPERIMENTAL  RESULTSAnother series of experiments was performed on the WSJ SpeechCorpus \[15\].
We evaluated our system on the 5000-word-recogni-tion closed-vocabulary speaker-independent speech-recognitiontasks: Spoke $5 Unknown Microphone, Spoke $6: Known Micro-phone, and Spoke $7 Noisy Environment.The version of the DECIPHER speaker-independent co inuousspeech :recognition system used for these experiments i  based ona progressive-search strategy \[3\] and continuous-density, genonicHMMs \[2\].
Gender-dependent models are used in all passes.
Gen-der selection uses the models with the higher ecognition likeli-hood.The acoustic models used by the HMM system were trained with37,000 sentences of Sennheiser data from 280 speakers, a set offi-cially designated as the WSJ0+WSJ1 many-speaker baselinetraining.
A 5,000 closed-vocabulary back-off trigram languagemodel provided by M.I.T.
Lincoln Laboratory for the WSJ taskwas used.
Gender-dependent HMM acoustic models were used.The front-end processing extracts one long spectral vector consist-ing of the following six feature components: cepstrum, energy,and their first and second order derivatives.
The dimensionality ofthis feature is 39 (13 * 3) for the wide-bandwidth spectral analysisand 27 (9 * 3) for the telephone-bandwidth spectral analysis.
Thecepstral features are computed from an FFI" filterbank, and subse-quent cepstral-mean normalization on a sentence-by-sentencebasis is performed.Before using wide-bandwidth context-dependent genonie HMMs,a robust estimate of the Sennheiser cepstral parameters is com-puted using POE The robust front-end analysis is designed for anunknown microphone condition.
The POF mapping algorithmestimates are conditioned on the noisy cepstral observations.
Sep-arate mappings are trained for each of the 14 microphones in thebaseline WSJ0+WS/1 si_tr_s stereo training, and one mapping forthe overall ease of single nontelephone mapping.
When the defaultno-transformation zero-mean eepstra re included, this makes atotal of 15 estimated feature streams.
These feature streams arecomputed on each test waveform, and the two feature streams withthe highest likelihoods (using a simplified HMM for scoring thefeatures) are averaged together (Top2).
In all cases the first andsecond elta parameters are computed on these stimated cepstralvalues.Front-End WordBandwidth Signal Processing Test Set Error (%)Wide Standard Sennheiser 5.8Telephone Standard Sennheiser 9.6Telephone Standard Telephone 10.9Wide Robust POF15 Telephone 11.9Cepstral MappingTable 6: Performance on the Aug 1993 WSJ Spoke $6 develop-ment est set for simultaneous Sennheiser/telephone recordingsThe results in Table 6 show that most of the loss in performancebetween recognizing on high-quality Sennheiser recordings andon local telephone speech is due to the loss of information outsidethe telephone bandwidth.
There is an increase in the word-errorrate of 66% when testing on Sennheiser recordings with a wide-bandwidth analysis (5.8%) compared to testing with a telephone-bandwidth analysis (9.6%).The loss in performance when switching from Sennheiser record-ings to telephone recordings i  small in comparison tothe loss ofinformation due to bandwidth restrictions.
There is a 14% increasein the word error rate when testing on the Sennheiser recordings(9.6%) compared to testing on the AT&T telephone recordings(10.9%).4.1.
Official Spoke Results: Unknown MicrophoneThe results in Table 7 show the speech recognition performancewhen the secondary microphone condition is unknown.
In theseexperiments, the robust signal processing front end decreased theword error rate from 17.2 to 13.1%.ExperimentWord ErrorSennheiserSecondaryMicrophoneCompensation Disabled 6.6 17.2Compensation E abled i 6.6 13.1Table 7: Word error ate with and without compensation on bothSennheiser and secondary microphone data4.2.
Official Spoke Results: Known MicrophoneThe results in Table 8 show no significant difference in speech rec-ognition performance between those obtained with an Audio-Teehnica microphone and those obtained with the Sennheisermicrophone.
The robust front-end signal processing has demon-strated for the first time that one can achieve the same performancewith a stand-mounted microphone as with a high-quality close-talking microphone, all when trained on a high-quality speech cor-pus.ExperimentWord ErrorSennheiserSecondaryMicrophoneAudio-Technica Recordings 5.9 6.4Telephone Handset Recordings 7.2 19.1Table 8: Word Error for both Sermheiser and Secondary Micro-phone with Robust Signal Processing Front End4.3.
Official Spoke Results: Noisy EnvironmentThe results in Table 9 show the performance when the recordingsare made in a noisy environment.
The first noisy environment wasa computer room (average background noise level of 58 to 59dBA), and the second noisy environment was a laboratory withmail sorting equipment (average noise level varied from 62 to 68dBA).
The error rates are significantly higher for the Audio-Tech-340nica microphone than for the Sennheiser microphone inthe noisierenvironment.
In the computer room environment, the performancewith the Audio-Technica microphone is almost indistinguishablefrom that of the Sennheiser recording.ExperimentAudio-Techniea Env 1RecordingsEnv 2Telephone Handset Env 1RecordingsEnv 2Word ErrorSennheiserSecondaryMicrophone6.3 8.59.1 17.48.4 29.18.3 28.8Table 9: Word Error for both Sennheiser and Secondary Micro-phone with Robust Signal Processing Front End when Recordedin Two Noisy Environments5, CONCLUSIONSWe have presented a feature-mapping algorithm capable ofexploiting nonlinear relations between two acoustic spaces.
Wehave shown how to improve the performance of the recognizer inthe presence of a noisy signal by using a small database withsimultaneous recordings in the clean and noisy acoustic environ-ments.We have shown that?
There is no significant difference in speech recognition per-formance between those obtained with an Audio-Teehnieamicrophone and those obtained with a Sennheiser micro-phone.
There is no significant performance d gradation i  aquiet environment and only a slight degradation i  low-noise environments (~59 dBA).?
Multidimensional least-squares filters can be successfullyused to exploit he correlation of the features over time andamong components of the spectral features at the same time.These filters can be conditioned on both local and globalspectral information toimprove robust recognition perfor-malice.?
Most of the performance loss in converting wide-bandwidthmodels to telephone speech models is due to the loss ofinformation associated with the telephone bandwidth.?
It is possible to construct acoustic models for telephonespeech using a high-quality speech corpus with only a minorincrease in recognition word error rate.?
A telephone-bandwidth system trained with high-qualityspeech can outperform a system that is trained on telephonespeech even when tested on telephone speech.?
The variability introduced by the telephone handset does notdegrade speech recognition performance.?
Robust signal processing can be designed to maintain speechrecognition performance using wide-bandwidth HMM mod-els with a telephone-bandwidth test set.ACKNOWLEDGMENTSThe authors thank John Butzberger for helping to set up the ATISexperiments, and Vassilios Digalakis for providing enonic HMMmodels and helping with the telephone-bandwidth experiments.This research was supported by a grant, NSF IRI-9014829, fromthe National Science Foundation (NSF).
It was also supported bythe Advanced Research Projects Agency (ARPA) under ContractsONR N00014-93-C-0142 and ONR N00014-92-C-0154.REFERENCES1.
M. Wcintraub and L. Neurneyer, "Constructing Telephone AcousticModels from a High-Quality Speech Corpus," 1994 IEEE ICASSP.2.
V. Digalakis and H. Murveit, "GENONES: Optimizing the Degree ofMixture Tying in a Large Vocabulary Hidden Markov Model BasedSpeech Recognizer," 1994 IEEE ICASSP.3.
H. Murveit, J. Butzberger, V. Digalalds, and M. Weintraub, "Large-Vocabulary Dictation Using SRrs DECIPHER Speech RecognitionSystem: Progressive S arch Techniques," 1993 IV PF ICASSP, pp.11319-H322.4.
H. Murveit, J. Butzberger, and M. Weintraub, "Performance of SRI'sDECIPHER TM Speech Recognition System on DARPA's CSR Task,"1992 DARPA Speech and Natural Language Workshop Proceedings,pp.
410-414.5.
Murveit, H., J. Butzberger, and M. Weintraub, "Reduced ChannelDependence furSpeech Recognition," 1992 DARPA Speech and Nat-ural Language Workshop Proceedings, pp.
280-284.6.
A. Erell and M. Weintranb, "Spectral Estimation for Noise RobustSpeech Recognition," 1989 DARPA SLS Workshop, p. 319-324.7.
A. Erell and M. Weintranb, "Recoguition f Noisy Speech: UsingMinimum-Mean Log-Spectral Distance Estimation," 1990 DARPASLS Workshop, p. 341-345.8.
B.H.
Juang and LR.
Rabiner, "Signal Restoration bySpectral Map-ping," 1987 IEEE ICASSP, pp.
2368-2371.9.
H. Gish, Y.L.
Chow, and J.R. Rohlicek, "Probabilistie V ctor Mappingof Noisy Speech Parameters forHMM Word Spotting," 1990 IEEEICASSP, pp.
117-120.10.
K. Ng, H. Giab, and LR.
Roblicek, "Robust Mapping of Noisy SpeechParameters for HMM Word Spotting," 1992 IV V.F.
ICASSP, pp.
H-109-II- 112.11.
A. Acero, "Acoustical nd Environmental Robustness in AutomaticSpeech Recognition," Ph.D. Thesis, Carnegie-MeLton U iversity, Sep-tember 1990.12.
R.M.
Stem, FJ-I.
Leu, Y. Ohshima, T.M.
Sullivan, and A. Acero,"Multiple Approaches toRobust Speech Recognition," 1992 Interna-tional Conference onSpoken Language Processing, pp.
695-698.13.
A. Nadas, D. Nahamoo, and M. Pieheny, "Adaptive Labeling: Nor-malization ofSpeoch by Adaptive Transformations ba ed on VectorQuantization," 1988 IEEE ICASSP, pp.
521-524.14.
Y. Linde, A. Buzo, and R.M.
Gray, "An Algorithm for Vector Quan-tizer Design," IE17F~ Trans.
Comm., voL 28, pp.
84-95, January 1980.15.
G. Doddington, "CSR Corpus Development," 1992 DARPA SLSWorkshop, p. 36,3-366.16.
S.E Furui, "Copstral Analysis Technique for Automatic Speaker Veri-fication," II~.VF.
Trans.
ASSP, VoL 29, pp.
254-2'72, April 1981.341
