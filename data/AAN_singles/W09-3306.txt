Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 38?41,Suntec, Singapore, 7 August 2009.c?2009 ACL and AFNLPEvaluating a Statistical CCG Parser on WikipediaMatthew Honnibal Joel NothmanSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australia{mhonn,joel,james}@it.usyd.edu.auJames R. CurranAbstractThe vast majority of parser evaluation isconducted on the 1984 Wall Street Journal(WSJ).
In-domain evaluation of this kindis important for system development, butgives little indication about how the parserwill perform on many practical problems.Wikipedia is an interesting domain forparsing that has so far been under-explored.
We present statistical parsing re-sults that for the first time provide infor-mation about what sort of performance auser parsing Wikipedia text can expect.We find that the C&C parser?s standardmodel is 4.3% less accurate on Wikipediatext, but that a simple self-training ex-ercise reduces the gap to 3.8%.
Theself-training also speeds up the parser onnewswire text by 20%.1 IntroductionModern statistical parsers are able to retrieve accu-rate syntactic analyses for sentences that closelymatch the domain of the parser?s training data.Breaking this domain dependence is now oneof the main challenges for increasing the indus-trial viability of statistical parsers.
Substantialprogress has been made in adapting parsers fromnewswire domains to scientific domains, espe-cially for biomedical literature (Nivre et al, 2007).However, there is also substantial interest in pars-ing encyclopedia text, particularly Wikipedia.Wikipedia has become an influential resourcefor NLP for many reasons.
In addition to its va-riety of interesting metadata, it is massive, con-stantly updated, and multilingual.
Wikipedia isnow given its own submission keyword in generalCL conferences, and there are workshops largelycentred around exploiting it and other collabora-tive semantic resources.Despite this interest, there have been few in-vestigations into how accurately existing NLP pro-cessing tools work on Wikipedia text.
If it is foundthat Wikipedia text poses new challenges for ourprocessing tools, then our results will constitutea baseline for future development.
On the otherhand, if we find that models trained on newswiretext perform well, we will have discovered anotherinteresting way Wikipedia text can be exploited.This paper presents the first evaluation of a sta-tistical parser on Wikipedia text.
The only pre-vious published results we are aware of were de-scribed by Ytrest?l et al (2009), who ran theLinGo HPSG parser over Wikipedia, and foundthat the correct parse was in the top 500 returnedparses for 60% of sentences.
This is an interestingresult, but one that gives little indication of howwell a user could expect a parser to actually anno-tate Wikipedia text, or how to go about adjustingone if its performance is inadequate.To investigate this, we randomly selected 200sentences from Wikipedia, and hand-labelled themwith CCG annotation in order to evaluate the C&Cparser (Clark and Curran, 2007).
C&C is the fastestdeep-grammar parser, making it a likely choice forparsing Wikipedia, given its size.Even at the parser?s WSJ speeds, it wouldtake about 18 days to parse the current EnglishWikipedia on a single CPU.
We find that the parseris 54% slower on Wikipedia text, so parsing a fulldump is inconvenient at best.
The parser is only4.3% less accurate, however.We then examine how these figures might beimproved.
We try a simple domain adaptationexperiment, using self-training.
One of our ex-periments, which involves self-training using theSimple English Wikipedia, improves the accuracyof the parser?s standard model on Wikipedia by0.8%.
The bootstrapping also makes the parserfaster.
Parse speeds on newswire text improve20%, and speeds on Wikipedia improve by 34%.38Corpus Sentences Mean lengthWSJ 02-21 39,607 23.5FEW 889,027 (586,724) 22.4 (16.6)SEW 224,251 (187,321) 16.5 (14.1)Table 1: Sentence lengths before (and after) length filter.2 CCG ParsingCombinatory Categorial Grammar (CCG) (Steed-man, 2000) is a linguistically motivated grammarformalism with several advantages for NLP.
LikeHPSG, LFG and LTAG, a CCG parse recovers thesemantic structure of a sentence, including long-range dependencies and complement/adjunct dis-tinctions, providing substantially more informa-tion than skeletal brackets.Clark and Curran (2007) describe how a fast andaccurate CCG parser can be trained from CCGbank(Hockenmaier and Steedman, 2007).
One of thekeys to the system?s success is supertagging (Ban-galore and Joshi, 1999).
Supertagging is the as-signment of lexical categories before parsing.
Theparser is given only tags assigned a high proba-bility, greatly restricting the search space it mustexplore.
We use this system, referred to as C&C,for our parsing experiments.3 Processing Wikipedia DataWe began by processing all articles from theMarch 2009 dump of Simple English Wikipedia(SEW) and the matching Full English Wikipedia(FEW) articles.
SEW is an online encyclopediawritten in basic English.
It has stylistic guidelinesthat instruct contributors to use basic vocabularyand syntax, to improve the articles?
readability.This might make SEW text easier to parse, mak-ing it useful for our self-training experiments.mwlib (PediaPress, 2007) was used to parsethe MediaWiki markup.
We did not expand tem-plates, and retained only paragraph text tokenizedaccording to the WSJ, after it was split into sen-tences using the NLTK (Loper and Bird, 2002) im-plementation of Punkt (Kiss and Strunk, 2006) pa-rameterised on Wikipedia text.
Finally, we dis-carded incorrectly parsed markup and other noise.We also introduced a sentence length filter forthe domain adaptation data (but not the evaluationdata), discarding sentences longer than 25 wordsor shorter than 3 words.
The length filter was usedto gather sentences that would be easier to parse.The effect of this filter is shown in Table 1.4 Self-training MethodologyTo investigate how the parser could be improvedon Wikipedia text, we experimented with semi-supervised learning.
We chose a simple method,self-training.
Unlabelled data is annotated by thesystem, and the predictions are taken as truth andintegrated into the training system.Steedman et al (2003) showed that the selec-tion of sentences for semi-supervised parsing isvery important.
There are two issues: the accu-racy with which the data can be parsed, which de-termines how noisy the new training data will be;and the utility of the examples, which determineshow informative the examples will be.We experimented with a novel source of datato balance these two concerns.
Simple EnglishWikipedia imposes editorial guidelines on thelength and syntactic style authors can use.
Thistext should be easier to parse, lowering the noise,but the syntactic restrictions might mean its exam-ples have lower utility for adapting the parser tothe full English Wikipedia.We train the C&C supertagger and parser (Clarkand Curran, 2007) on sections 02-21 of the WallStreet Journal (WSJ) marked up with CCG annota-tions (Hockenmaier and Steedman, 2007) in thestandard way.
We then parse all of the Sim-ple English Wikipedia remaining after our pre-processing.
We discard the 826 sentences theparser could not find an analysis for, and set aside1,486 randomly selected sentences as a future de-velopment set, leaving a corpus of 185,000 auto-matically parsed sentences (2.6 million words).We retrain the supertagger on a simple concate-nation of the 39,607 WSJ training sentences andthe Wikipedia sentences, and then use it with thenormal-form derivations and hybrid dependenciesmodel distributed with the parser1.We repeated our experiments using text fromthe full English Wikipedia (FEW) for articleswhose names match an article in SEW.
We ran-domly selected a sample of 185,000 sentencesfrom these, to match the size of the SEW corpus.We also performed a set of experiments wherewe re-parsed the corpus using the updated su-pertagger and retrained on output, the logic beingthat the updated model might make fewer errors,producing higher quality training data.
This itera-tive retraining was found to have no effect.1http://svn.ask.it.usyd.edu.au/trac/candc39Model WSJ Section 23 Wiki 200 Wiki 90kP R F speed cov P R F speed cov speed covWSJ derivs 85.51 84.62 85.06 545 99.58 81.20 80.51 80.86 394 99.00 239 98.81SEW derivs 85.06 84.11 84.59 634 99.75 81.96 81.34 81.65 739 99.50 264 99.11FEW derivs 85.24 84.32 84.78 653 99.79 81.94 81.36 81.65 776 99.50 296 99.15WSJ hybrid 86.20 84.80 85.50 481 99.58 81.93 80.51 81.22 372 99.00 221 98.81SEW hybrid 85.80 84.30 85.05 571 99.75 82.16 80.49 81.32 643 99.50 257 99.11FEW hybrid 85.94 84.46 85.19 577 99.79 82.49 81.03 81.75 665 99.50 275 99.15Table 2: Parsing results with automatic POS tags.
SEW and FEW models incorporate self-training.5 Annotating the Wikipedia DataWe manually annotated a Full English Wikipediaevaluation set of 200 sentences.
The sentenceswere sampled at random from the 5000 articlesthat were linked to most often by Wikipedia pages.Articles used for self-training were excluded.The annotation was conducted by one annota-tor.
First, we parsed the sentences using the C&Cparser.
We then manually corrected the supertags,supplied them back to the parser, and correctedthe parses using a GUI.
The interface allowed theannotator to specify bracket constraints until theparser selected the correct analysis.
The annota-tion took about 20 hours in total.We used the CCGbank manual (Hockenmaierand Steedman, 2005) as the guidelines for ourannotation.
There were, however, some system-atic differences from CCGbank, due to the faultynoun phrase bracketing and complement/adjunctdistinctions inherited from the Penn Treebank.6 ResultsThe results in this section refer to precision, re-call and F -Score over labelled CCG dependencies,which are 5-tuples (head, child, category, slot,range).
Speed is reported as words per second, us-ing a single core 2.6 GHz Pentium 4 Xeon.6.1 Out-of-the-Box PerformanceOur experiments were performed using two mod-els provided with v1.02 of the C&C parser.
Thederivs model is calculated using features from theEisner (1996) normal form derivation.
This is themodel C&C recommend for general use, becauseit is simpler and faster to train.
The hybrid modelachieves the best published results for CCG pars-ing (Clark and Curran, 2007), so we also experi-mented with this model.
The models?
performanceis shown in the WSJ rows of Table 2.
We report ac-curacy using automatic POS tags, since we did notcorrect the POS tags in the Wikipedia data.The derivs and hybrid models show a simi-lar drop in performance on Wikipedia, of about4.3%.
Since this is the first accuracy evalua-tion conducted on Wikipedia, it is possible thatWikipedia data is simply harder to parse, possi-bly due to its wider vocabulary.
It is also possiblethat our manual annotation made the task slightlyharder, because we did not reproduce the CCGbanknoun phrase bracketing and complement/adjunctdistinction errors.We also report the parser?s speed and coverageon Wikipedia.
Since these results do not requirelabelled data, we used a sample of 90,000 sen-tences to obtain more reliable figures.
Speeds var-ied enormously between this sample and the 200annotated sentences.
A length comparison revealsthat our manually annotated sentences are slightlyshorter, with a mean of 20 tokens per sentence.Shorter sentences are often easier to parse, so thisissue may have affected our accuracy results, too.The 54% drop in speed on Wikipedia text is ex-plained by the way the supertagger and parser areintegrated.
The supertagger supplies the parserwith a beam of categories.
If parsing fails, thechart is reinitialised with a wider beam and it triesagain.
These failures occur more often when thesupertagger cannot produce a high quality tag se-quence, particularly if the problem is in the tagdictionary, which constrains the supertagger?s se-lections for frequent words.
This is why we fo-cused on the supertagger in our domain adaptationexperiments.6.2 Domain Adaptation ExperimentsThe inclusion of parsed data from Wikipedia ar-ticles in the supertagger?s training data improvesits accuracy on Wikipedia data, with the FEW en-hanced model achieving 89.86% accuracy, com-pared with the original accuracy of 88.77%.
TheSEW enhanced supertagger achieved 89.45% ac-curacy.
The derivs model parser improves in ac-curacy by 0.8%, the hybrid model by 0.5%.40The out-of-domain training data had little im-pact on the models?
accuracy on the WSJ, butdid improve parse speed by 20%, as it did onWikipedia.
The speed increases because the su-pertagger?s beam width is decided by its confi-dence scores, which are more narrowly distributedafter the model has been trained with more data.After self-training, the derivs and hybrid mod-els performed equally accurately.
With no reasonto use the hybrid model, the total speed increase is34%.
With our pre-processing, the full Wikipediadump had close to 1 billion words, so speed is animportant factor.Overall, our simple self-training experimentwas quite successful.
This result may seem sur-prising given that the CoNLL 2007 participantsgenerally failed to use similar resources to adaptdependency parsers to biomedical text (Dredzeet al, 2007).
However, our results confirm Rimelland Clark?s (2009) finding that the C&C parser?sdivision of labour between the supertagger andparser make it easier to adapt to new domains.7 ConclusionWe have presented the first investigation into sta-tistical parsing on Wikipedia data.
The parser?saccuracy dropped 4.3%, suggesting that the sys-tem is still useable out-of-the-box.
The parser isalso 54% slower on Wikipedia text.
Parsing a fullWikipedia dump would therefore take about 52days of CPU time using our 5-year-old architec-ture, which is inconvenient, but manageable overmultiple processors.Using simple domain adaptation techniques,we are able to increase the parser?s accuracy onWikipedia, with the fastest model improving in ac-curacy by 0.8%.
This closed the gap in accuracybetween the two parser models, removing the needto use the slower hybrid model.
This allowed us toachieve an overall speed improvement of 34%.Our results reflect the general trend thatNLP systems perform worse on foreign domains(Gildea, 2001).
Our results also support Rimelland Clark?s (2009) conclusion that because C&Cis highly lexicalised, domain adaptation is largelya process of adapting the supertagger.A particularly promising aspect of these resultsis that the parse speeds on the Wall Street Journalimproved, by 15%.
This improvement came withno loss in accuracy, and suggests that further boot-strapping experiments are likely to be successful.8 AcknowledgementsWe would like to thank Stephen Clark and theanonymous reviewers for their helpful feedback.Joel was supported by a Capital Markets CRCPhD scholarship and a University of Sydney Vice-Chancellor?s Research Scholarship.ReferencesSrinivas Bangalore and Aravind Joshi.
1999.
Supertagging:An approach to almost parsing.
Computational Linguis-tics, 25(2):237?265.Stephen Clark and James R. Curran.
2007.
Wide-coverage ef-ficient statistical parsing with CCG and log-linear models.Computational Linguistics, 33(4):493?552.Mark Dredze, John Blitzer, Partha Pratim Talukdar, KuzmanGanchev, Jo?ao Graca, and Fernando Pereira.
2007.
Frus-tratingly hard domain adaptation for dependency pars-ing.
In Proceedings of the CoNLL Shared Task Sessionof EMNLP-CoNLL 2007, pages 1051?1055.
ACL, Prague,Czech Republic.Jason Eisner.
1996.
Efficient normal-form parsing for Com-binatory Categorial Grammar.
In Proceedings of the Asso-ciation for Computational Linguistics, pages 79?86.
SantaCruz, CA, USA.Daniel Gildea.
2001.
Corpus variation and parser perfor-mance.
In Proceedings of the EMNLP Conference, pages167?202.
Pittsburgh, PA.Julia Hockenmaier and Mark Steedman.
2005.
CCGbankmanual.
Technical Report MS-CIS-05-09, Department ofComputer Science, University of Pennsylvania.Julia Hockenmaier and Mark Steedman.
2007.
CCGbank: acorpus of CCG derivations and dependency structures ex-tracted from the Penn Treebank.
Computational Linguis-tics, 33(3):355?396.Tibor Kiss and Jan Strunk.
2006.
Unsupervised multilingualsentence boundary detection.
Computational Linguistics,32(4):485?525.Edward Loper and Steven Bird.
2002.
NLTK: The naturallanguage toolkit.Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan McDonald,Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007.The CoNLL 2007 shared task on dependency parsing.
InProceedings of the CoNLL Shared Task Session, pages915?932.
Prague, Czech Republic.PediaPress.
2007. mwlib MediaWiki parsing library.http://code.pediapress.com.Laura Rimell and Stephen Clark.
2009.
Porting a lexicalized-grammar parser to the biomedical domain.
Journal ofBiomedical Informatics.
(in press).Mark Steedman.
2000.
The Syntactic Process.
The MITPress, Cambridge, MA.Mark Steedman, Rebecca Hwa, Stephen Clark, Miles Os-borne, Anoop Sarkar, Julia Hockenmaier, Paul Ruhlen,Steven Baker, and Jeremiah Crim.
2003.
Example selec-tion for bootstrapping statistical parsers.
In Proceedingsof HLT-NAACL 2003.
Edmonton, Alberta.Gisle Ytrest?l, Stephan Oepen, and Daniel Flickinger.
2009.Extracting and annotating Wikipedia sub-domains.
InProceedings of the 7th International Workshop on Tree-banks and Linguistic Theories, pages 185?197.
Gronin-gen, Netherlands.41
