Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1648?1659, Dublin, Ireland, August 23-29 2014.Learning to Generate Coherent Summarywith Discriminative Hidden Semi-Markov ModelHitoshi Nishikawa1, Kazuho Arita1, Katsumi Tanaka1,Tsutomu Hirao2, Toshiro Makino1and Yoshihiro Matsuo1Nippon Telegraph and Telephone Corporation11-1 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-0847 Japan22-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan{nishikawa.hitoshi, arita.kazuho, tanaka.katsumihirao.tsutomu, makino.toshiro, matsuo.yoshihiro}@lab.ntt.co.jpAbstractIn this paper we introduce a novel single-document summarization method based on a hiddensemi-Markov model.
This model can naturally model single-document summarization as theoptimization problem of selecting the best sequence from among the sentences in the input doc-ument under the given objective function and knapsack constraint.
This advantage makes itpossible for sentence selection to take the coherence of the summary into account.
In additionour model can also incorporate sentence compression into the summarization process.
To demon-strate the effectiveness of our method, we conduct an experimental evaluation with a large-scalecorpus consisting of 12,748 pairs of a document and its reference.
The results show that ourmethod significantly outperforms the competitive baselines in terms of ROUGE evaluation, andthe linguistic quality of summaries is also improved.
Our method successfully mimicked thereference summaries, about 20 percent of the summaries generated by our method were com-pletely identical to their references.
Moreover, we show that large-scale training samples arequite effective for training a summarizer.1 IntroductionSingle-document summarization is attracting much more attention as a key technology in providingbetter information access in a commercial context.
The Financial Times and CNN have been providingsummaries of articles in their websites to attract users, and Summly, which has been acquired by Yahoo!,provided the service of automatically summarizing articles on the Internet.
Given the cost of manualsummarization, we can greatly improve the information access of Internet users by creating an automaticsummarizer that can approach the summarization quality of humans.To mimic manually-written summaries, one important aspect is coherence (Nenkova and McKeown,2011).
Although coherence has been studied widely in a field of multi-document summarization (Kara-manis et al., 2004; Barzilay and Lapata, 2005; Nishikawa et al., 2010; Christensen et al., 2013), it has notbeen studied enough in the context of single-document summarization.
In this paper, we revisit the prob-lem of coherence and employ it to produce both informative and linguistically high-quality summaries.To obtain such summaries, we introduce a novel summarization method based on a hidden semi-Markov model.
The method has the properties of both the popular single-document summarizationmodel, the knapsack problem, which packs the sentences into the given length and the hidden Markovmodel, which takes summary coherence into account by determining sentence context when selectingsentences.
By leveraging this, we can build a summarizer that naturally achieves coherence.We state the novelty and contributions of this paper as follows:?
We regard single-document summarization as a combinatorial optimization problem modeled by ahidden semi-Markov model and propose an efficient decoding algorithm for the problem.?
We introduce various features related to coherence in a combinatorial formulation.
We extend ahidden semi-Markov model to achieve discrimination, so our method can take advantage of manyfeatures for predicting coherence.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1648?
We show that our large-scale corpus greatly improves the performance of summarization.This paper is organized as follows.
In Section 2, we describe related work.
In Section 3, we detailour proposed model.
We also explain how the parameters in our model are optimized and how sentencesare compressed.
In Section 4, we explain how variants of the original sentences are generated.
InSection 5, we explain the decoding algorithm for our method.
In Section 6, we explain the settings ofour experiments, our corpus, and compared methods.
In Section 7, we show results of the experimentsconducted to evaluate our method.
In Section 8, we conclude this paper.2 Related Work2.1 Single-Document SummarizationBasically, single-document summarization can be done through sentence selection (Nenkova and McK-eown, 2011) .
The document to be summarized is decomposed into a set of sentences and then thesummarizer selects a subset of the sentences as a summary.McDonald (2007) pointed out that single-document summarization can be formulated as a well-knowncombinatorial optimization problem, the knapsack problem.
Given a set of sentences together with theirlengths and values, the summarizer packs them into a summary so that the total value is as large as possi-ble but the total length is less than or equal to a given maximum summary length.
Interestingly, a hiddensemi-Markov model (Yu, 2010) can be regarded as a natural extension of the knapsack problem, we takeadvantage of this property for single-document summarization.
We elaborate the relation between theknapsack problem and the hidden semi-Markov model in Section 3.To generate coherent summaries in single-document summarization, there are two types of ap-proaches1: tree-based approaches (Marcu, 1997; Daume and Marcu, 2002; Hirao et al., 2013) andsequence-based approaches (Barzilay and Lee, 2004; Shen et al., 2007).
The former rely on the treerepresentation of a document based on the Rhetorical Structure Theory (RST) (Mann and Thompson,1988).
Basically, the former approaches (Marcu, 1997; Daume and Marcu, 2002; Hirao et al., 2013) trimthe tree representation of a document by making use of nucleus-satellite relations among sentences.
Theadvantage of RST-based approaches is that they can take advantage of global information about the doc-uments.
However, a drawback is that they depend heavily on the RST parser that is used.
Performanceis remarkably sensitive to the accuracy of RST parsing, and hence we have to build a good RST parser.Instead of making use of the global structure of the document, the sequence-based methods rely on andtake advantage of the local coherence of sentences.
As one advantage over the tree-based approaches,the sequence-based approaches do not require tools as RST parsers, and hence they are more robust.
Forthis reason, this paper focuses on sequence-based approaches.The previous works most closely related to our method are those proposed by Barzilay and Lee (2004)and Shen et al.
(2007).
Barzilay and Lee built a hidden Markov model to capture the content structure ofdocuments and used it to identify the important sentences.
Shen et al.
(2007) extended the HMM-basedapproach to make it discriminative by making use of conditional random fields (Lafferty et al., 2001).Conditional random fields can incorporate various features to identify the importance of a sentence andthey showed its effectiveness.
A shortcoming of these approaches is that their model only classifies sen-tences into two classes, it cannot take account of output length directly.
This deficiency is problematicbecause in practical usage the maximum length of a summary is specified by the user; hence, the sum-marizer should be able to control output length.
In contrast to their method, our approach naturally takesthe maximum summary length into account when summarizing a document.2.2 CoherenceIn the context of multi-document summarization, coherence has been studied widely.
In multi-documentsummarization, sentences are selected from different documents, and hence some way of ordering thesentences is required.
Sentence ordering (Barzilay et al., 2002; Althaus et al., 2004; Karamanis et al.,1As an interesting related work, Clarke and Lapata (2007) compresses documents by making use of Centering Theory(Grosz et al., 1995).
However, in their approach, the desired length of an output summary could not be specified and hence theysaid their method was compression rather than summarization.1649Figure 1: An example of the hidden semi-Markov model.
The system observes a sequence consistingof 10 symbols o1...o10over time t1...t10and transitions between states s1...s3.
Unlike the basic hiddenMarkov model, states can persist for a non-unit length.
In this figure, state s2and state s3persist fornon-unit lengths.
Hence, the system traverses only 6 states despite observing 10 symbols.2004; Okazaki et al., 2004) is a task to order extracted sentences and is closely related to coherence(Lapata, 2003; Barzilay and Lapata, 2005; Nenkova et al., 2010; Pitler et al., 2010; Louis and Nenkova,2012).
Many effective features have been found out to capture coherence and we utilize these features.Some work proposed a model that could jointly taking the content of the summary and its coherenceinto account (Nishikawa et al., 2010; Christensen et al., 2013).
Since extracted sentences in multi-document summarization must be ordered, a task that is NP-hard, they relied on integer linear program-ming (Nishikawa et al., 2010) or a local search strategy (Christensen et al., 2013).
The former can locatethe optimal solution at a heavy computation cost, while the latter runs quickly but there is no guaranteeof locating the optimal solution.
In contrast to their trade-off, our proposed algorithm, based on dynamicprogramming, can locate the optimal solution quickly because the single-document summarization canskip the ordering operation by reproducing the original order of the input sentences.In this paper, we show that coherence also takes an important role in single-document summarization.We model the coherence between adjacent sentences in the summary by leveraging the hidden semi-Markov model, which can naturally capture the coherence between sentences.3 Summarization with Hidden Semi-Markov ModelWe first introduce the knapsack problem, which can naturally model single-document summarization.Next, we explain the hidden semi-Markov model and show its relationship to the knapsack problem.Then, we elaborate our summarization method.3.1 Knapsack ProblemThe knapsack problem is a type of combinatorial optimization problem (Korte and Vygen, 2008).
Givena set of elements, each of which has a score and size, the problem is formulated as the task of findingthe best subset in terms of maximizing the sum of their scores under the size limitation.
As mentionedabove, single-document summarization can be regarded as an instance of the knapsack problem.
Thebest combination of input sentences can be found by calculating the value of each sentence and packingthem into a summary through the dynamic programming knapsack algorithm.3.2 Hidden Semi-Markov ModelThe hidden semi-Markov model (HSMM) is an extension of the hidden Markov model (HMM) (Yu,2010).
In the popular hidden Markov model, each state persists for only one unit length.
For example,if a system observes 10 discrete symbols, it outputs 10 hidden states.
In the HSMM, each state canpersist for some unit lengths through the concept of duration.
For example, if a system observes 10discrete symbols and each state persists for two unit lengths, i.e., their duration is 2, the system outputs5 hidden states.
We show an example in Figure 1.
The system observes a sequence consisting of 10symbols o1...o10over time t1...t10and transitions between states s1...s3.
Unlike the basic HMM, statescan persist for a non-unit length.
In this figure, state s2and state s3persist for a non-unit length.
Hence,the system traverses 6 states even though it observes 10 symbols.
This property has been utilized for1650sequential tagging, such as named entity recognition (Sarawagi and Cohen, 2004), scene text recognition(Weinman et al., 2008) and phonetic recognition (Kim et al., 2011).The hidden semi-Markov model is closely related to the knapsack problem.
The length, K, of theobserved symbols can be regarded as a knapsack constraint.
We can consider that the system tries to packthe states of the model into the observed sequence of symbols by transitioning over the states under theknapsack constraint so as to maximize the likelihood.
Therefore, the hidden semi-Markov can naturallybe used for single-document summarization.
Suppose that the document to be summarized consists of10 sentences and the length of each of them is measured by the number of words.
In this case, the systemtransitions over 10 states corresponding to the 10 sentences until it cannot select any further sentence dueto the given length requirement.
Since each state persists for the length of the corresponding sentence,the remaining length decreases every time the system transitions to a new state.While an HMM is basically a generative model, Collins (2002) extended it to create a discriminativemodel.
An HSMM can also be extended to become discriminative model (Sarawagi and Cohen, 2004).Our discriminative HSMM learns through the application of max-margin training.3.3 FormulationWe consider there are n input sentences s1, s2, ..., sn.
These sentences have lengths ?1, ?2, ..., ?nandweights w1, w2, ..., wn.
We assume that a sentence that has a high weight should be present in the outputsummary.
We also consider each sentence, si, has mivariants si,1, si,2, ..., si,m, each produced by somesort of sentence compression or paraphrase module.
These variants also have lengths ?i,1, ?i,2, ..., ?i,miand weights wi,1, wi,2, ..., wi,mi.
For simplicity, we hereinafter note the original sentences s1, s2, ..., snas s1,0, s2,0, ..., sn,0.
Hence we have original sentence si,0and variants si,1, si,2, ..., si,m.
Let s0,0andsn+1,0be special symbols indicating the beginning of a document and the end of a document, respec-tively.
We define coherence cg,h,i,jas the coherence between sentence sg,hand sentence si,j.
An outputsummary is described as a sequence of input sentences, g. LetG be the entire set of sequences that can beconstructed from the input sentences, i.e., g ?
G. Finally, let K be the maximum length of the summarydesired.
With these notations, our proposed method can be formulated as the following optimizationproblem:g?= argmaxg?G?si,j?sent(g)wi,j+?(sg,h,si,j)?adj(g)cg,h,i,j(1)s.t.?si,j?sent(g)?i,j?
K, (2)where sent(g) and adj(g) indicate a set of sentences in g and a set of adjacent sentences in g, respec-tively.
That is, our model tries to find the best sequence of sentences under the knapsack constraint so asto maximize the sum of weights and sentence coherence.
In contrast to the common knapsack problemwhich cannot take the variants and sentence coherence into account, our method, based on the hiddensemi-Markov model, does so naturally.3.4 Parameter OptimizationHere we elaborate how parameters in the model are optimized to achieve the desired summaries.
Thegoal is to determine the value of wi,jfor all i, j and cg,h,i,jfor all g, h, i, j.
We define wi,jand cg,h,i,jasfollows:wi,j= ww?
fw(si,j) (3)cg,h,i,j= wc?
fc(sg,h, si,j), (4)where fwand fcare dw-dimensional and dc-dimensional feature vectors for sentences and sentence pairs,respectively, andwwandwcare dw-dimensional and dc-dimensional parameter vectors for sentences andsentence pairs, respectively.
The goal of optimization is to determine the values of both vector wwand1651wc, given feature function fwand fc.
For simplicity, let s be a summary, let f = ?fw, fc?
be a (dw+ dc)-dimensional feature function for the whole summary and let w = ?ww,wc?
be a (dw+ dc)-dimensionalweight vector.
The value that the objective function outputs for summary s is w ?
f(s).To optimize the parameter, we employ the Passive-Aggressive algorithm (Crammer, 2006), a widely-used structured learning method.
Since the algorithm offers online learning, it can learn the parameterquickly and is easy to implement.
To learn the parameter so that the output summary is optimized tothe evaluation criteria popular in document summarization research, ROUGE (Lin, 2004), we introduceROUGE as the loss function.
The parameter is estimated by solving the following formula iteratively2:wnew= argminw12||w ?
wold||2(5)s.t.
w ?
f(r) ?
w ?
f(s) ?
loss(s; r),where wnewis the parameter vector after update, woldis the parameter vector before update, r is areference summary, and loss is the loss function.
We define loss as 1 ?
ROUGE(s; r).
Among thevariants of ROUGE, we used ROUGE-1 for the loss function.3.4.1 Sentence FeatureThe features introduced in this section are used to calculate the weights of sentences, wi,j.Term Frequency: Term frequency is a classic feature in document summarization (Luhn, 1958).
Wecalculate the total number of times each content word occurs in the document and then, for each sentence,sum the totals of the content words that appear in the sentence as the value of this feature.Word: We also use the words and parts-of-speech as features.Named Entity: Named entities such as a name of person or organization are important.
We use namedentities and classes as features.Length: The length of a sentence may indicate the information value of its content.
We use the length ofa sentence, measured by character number, as a feature.Position: The position of a sentence is a classically important feature.
We use the position of a sentence,the relative position of a sentence, whether the sentence is the first in the document and whether thesentence is the first in a paragraph, the position of the paragraph in which the sentence is, as features.3.4.2 Coherence FeatureThe features introduced in this section are used to calculate sentence coherence, cg,h,i,h.Lexical Transition: Lapata (2003) showed that the structure of the document can be captured by word-pairs consisting of words of two adjacent sentences.
We use this feature for capturing the links betweentwo sentences3.
We build a set of word pairs where one occurs in a precedent sentence and the otheroccurs in a succeeding one, and use the elements of the set as a feature.Lexical Cohesion: Pitler et al.
(2010) showed that the similarity of two sentences is one of the strongestfeatures for predicting coherence.
We reproduce this feature for generating coherent summaries.
Wecalculate cosine similarity between two sentences and use its value as a feature.Entity Grid: Previous studies showed that Entity Grid (Barzilay and Lapata, 2005) is a strong featurefor predicting coherence (Pitler et al., 2010).
We also employ this feature for summarization.
While theentity vector made from the entity grid was originally defined for whole documents, we build the entityvector for each pair of two sentences because our model is based on the Markovian assumption, andhence the coherence score is defined between two sentences.2As we explain later in Section 5, computation complexity of our algorithm is pseudo-polynomial, and hence the bestsolution of our model can be located quickly.
This is also advantageous in the learning phase because to learn parameters usingstructured learning, the learner has to generate a summary to calculate the loss.
Since our algorithm can quickly find the bestsolution and generate a summary, it can also contribute to shortening the time required for learning.3It is expected that this feature will also contribute to sentence selection.
Barzilay and Elhadad (1997) showed that a closelyrelated word-pair was a good indicator for sentence selection.
This feature captures this property by learning.16520200040006000800010000120001400016000180000  5  10  15  20  25  30The number of sentencesLevenshtein distanceFigure 2: Distribution of Levenshtein distance in thealigned sentences.
Among the 36,413 sentences inthe references, 16,643 were identical (Levenshteindistance is 0) to the aligned sentences in the inputdocuments.0.60.610.620.630.640.650.660.670.680.690.70  2000  4000  6000  8000  10000ROUGE-2The number of training samplesFigure 3: Learning curve of HSMM.4 Generating Sentence VariantsSince our model can take the variants of an original sentence in the input document as in the multi-candidate reduction framework (Zajic et al., 2007), we incorporate sentence compression.We generate a few variants of each original sentence by trimming the dependency tree of the sentence;this simple operation is sufficient for reproducing reference summaries.
By aligning sentences in a refer-ence summary with those in the corresponding input document4, we found that human summaries werequite conservative.
Among the 36,413 sentences in the references, 16,643 were identical to the alignedsentences in the input documents.
Furthermore, most remaining sentences were virtually identical to theoriginal sentences; revisions were minor, and can be reproduced by simple operations.
Few sentencesexhibited paraphrasing or more sophisticated operations.
We plot the distribution of Levenshtein distancein the aligned sentences in Figure 2.
According to this observation, we produce the following types ofvariants by sentence compression:1.
Removing information in parentheses.
Some sentences contain parentheses containing additionalinformation for readers.
The first type of variant deletes text in parentheses.2.
Shortening sentences by trimming their dependency trees.
Basically this method follows the sen-tence trimmer proposed by Nomoto (2008).
While using his method, we keep the predicate and itsobligatory arguments in the sentences to keep the sentences grammatical.
If a predicate is trimmed,its obligatory arguments are also trimmed and vice versa.
Since there are an exponential numberof subtrees in one tree, we use only n-best subtrees by ranking them according to n-gram languagelikelihood and dependency-based language likelihood.
We used the dependency parser proposed byImamura et al (Imamura et al., 2007) to acquire the dependency tree.5 Decoding with Dynamic ProgrammingTo solve Equation 1 under the constraints of Equation 2, we use dynamic programming.
Algorithm1 shows the pseudo code of the decoding algorithm.
Line 1 to Line 7 initializes the variables used inthe algorithm.
Vector x = ?x0, ..., xn+1?
stores which sentence and which variants are included in theoutput summary.
If x3= 2, s3,2is included in the summary.
V , P and S are two-dimensional arrays,each of which is used as a dynamic programming table.
They store the process of dynamic programming.4Alignment proceeds in two steps: first, we calculate the Levenshtein distance between sentences in the document and itsreference, and then we align sentences so as to minimize the distance between them.1653Algorithm 1 Decoding Algorithm: Filling Table1: x = ?x0, ..., xn+1?2: for i = 0 to n + 1 do3: xi= ?14: V [0][i]?
?15: P [0][i]?
?16: S[0][i]?
07: V [0][0] = 08: for k = 1 to K do9: for i = 1 to n do10: V [k][i]?
V [k ?
1][i]11: P [k][i]?
P [k ?
1][i]12: S[k][i]?
S[k ?
1][i]13: for v = 0 to mido14: if ?i,v?
k then15: for h = 0 to i?
1 do16: u = V [k ?
?i,v][h]17: if u ?= ?1 ?
S[k ?
?i,v][h] + wi,v+ ch,u,i,v?
S[k][i] then18: V [k][i]?
v19: P [k][i]?
h20: S[k][i]?
S[k ?
?i,v][h] + wi,v+ ch,u,i,v21: V [K + 1][n + 1]?
022: P [K + 1][n + 1]?
023: S[K + 1][n + 1]?
024: for h = 1 to n do25: u = V [K][h]26: if S[K][h] + ch,u,n+1,0?
S[K + 1][n + 1] then27: P [K + 1][n + 1]?
h28: S[K + 1][n + 1]?
S[K][h] + ch,u,n+1,0Document ReferenceAvg.
# of characters 476.2 142.0Avg.
# of words 298.6 88.3Avg.
# of sentences 9.7 2.9Table 1: The statistics of our corpus.V [k][i] stores which variants are used at time k, i.
If V [k][i] = 0, original sentence si,0is selected attime k, i.
If V [k][i] = ?1, no sentence is selected at time k, i. P [k][i] stores a pointer to the sentenceconnected to the front of the current sentence.
S[k][i] stores the value of the objective function at timek, i.
Line 8 to Line 36 locates the best sequence of sentences based on the following recurrence formula:S[k][i] ={maxh=0...i?1,v=0...mS[k ?
?i,v][h] + wi,v+ ch,V [k?
?i,v][h],i,v(A)S[k ?
1][i] (B),(6)where case A is: ?i,v?
k ?
S[k ?
1][i] ?
S[k ?
?i,v][h] + wi,v+ ch,V [k?
?i,v][h],i,vand case B is:otherwise.
This recurrence formula means that at time k, i the best variant to be selected as can bedetermined at time k ?
?i,v, h. Hence, for all k ?
1...K and i ?
1...n, the algorithm finds the bestsequence of sentences at time k, i.
After Algorithm 1 locates the best sequence of sentences by fillingthe tables, the best sequence can be restored by backtracing along the pointers stored in P .
Finally, thealgorithm outputs x, which stores which sentences and variants are used in the best sequence.
Sincethis algorithm is based on a dynamic programming knapsack algorithm (Korte and Vygen, 2008), it runsin pseudo-polynomial time.
This is a significant advantage over the methods that rely on integer linearprogramming solvers due to their substantial computation cost.6 Experiments6.1 DataWe prepared 12,748 pairs of Japanese newspaper articles and their manually-written reference sum-maries.
This is one of the largest corpus available for single-document summarization research.
Thelength of all references is within 150 characters.
All references in the corpus were written by a specialiststaff in a Japanese newspaper company and the company sold these summaries for commercial purposes.1654We list the statistics of our corpus in Table 1.
As shown, the task is to summarize the document in abouta third of its original length in terms of the number of words.6.2 Evaluation CriteriaROUGE; ROUGE is an automatic evaluation method for automatic summarization proposed by Lin(2004).
We used ROUGE-1 and ROUGE-2 to evaluate the summaries.
Since our document-referencepairs are written in Japanese, we segmented the sentences into words using the Japanese morphologicalanalyzer developed by Fuchi and Takagi (1998).
When calculating the ROUGE score, we used onlycontent words (i.e.
nouns, verbs and adjectives) and so excluded function words as stop words.Linguistic Quality: To evaluate the linguistic quality of the summaries generated by our method, weperformed a manual evaluation according to quality questions proposed by the National Institute ofStandards and Technology (NIST) (2007)5.
We randomly sampled 100 summaries from the outputs ofeach method described below and asked 7 subjects to evaluate the summaries according to the questions.All subjects were Japanese native and none were among the authors.
Since the quality questions byNIST (2007) were designed for multi-document summarization, we used 3 of the 5 NIST questions forsingle-document summarization: grammaticality, referential clarity, and structure/coherence.
We alsoasked the subjects to evaluate overall summary quality.6.3 Compared MethodsWe compared the following 8 methods.Random: Random method selects sentences in the input document randomly.Lead: Lead method is a classic baseline in single-document summarization.
It only extracts the wordsfrom the beginning of the document until the extracted words reach the given length.
We simply extracted150 characters from the beginning of each document.Knapsack: The knapsack problem can be used as a single-document summarization model (McDonald,2007).
In this baseline, the weight of each sentence was calculated based on the average probabilitiesof the words in the sentence (Nenkova and Vanderwende, 2005).
Then, a summary was generated bysolving the knapsack problem.Knapsack with Supervision: Instead of the average word probabilities used in the above baseline, weused only sentence features fwto weigh a sentence.Conditional Random Fields: Conditional random fields can be used to weigh sentences (Shen et al.,2007).
Since CRFs required binary labels in learning, we aligned sentences in an input document withthe sentences in its reference as explained in Section 4.
We used the probabilities of sentences fromCRFs as the weights of the knapsack problem.Hidden Semi-Markov Model: This is our proposed method without variants of the original sentences.It selected sentences only from the set of original sentences.Hidden Semi-Markov Model with Compression: This is our proposed method with variants of theoriginal sentences.
It selected from among the variants and the original ones.Human: In the linguistic quality evaluation, we added references to the summaries generated by theabove methods to show the upper bound.When learning, we did 10-fold cross validation.
In the experiments, statistical significance waschecked by Wilcoxon signed-rank test (Wilcoxon, 1945).
To counteract the problem of multiple com-parisons, we used the Holm-Bonferroni method (Holm, 1979) to adjust the significance level, ?.7 Results and DiscussionWe show the results of our experiment in Table 2 and Table 3.
In this section, first we discuss the resultsof the ROUGE evaluation, and then we discuss the results of the linguistic quality evaluation.In the ROUGE evaluation, all the compared methods except for RANDOM showed good performance.This is because, as shown in Section 4, many references consisted of sentences identical to the original5Some recent studies have tried to predict the readability of the text automatically (Pitler et al., 2010).1655Method R-1 R-2 Idt.RANDOM 0.417 0.291 1.2%LEAD 0.779C,S,U,R0.727C,S,U,R4.4%KP 0.704R0.611R9.3%KP(S) 0.729U,R0.647U,R10.4%CRFs 0.741U,R0.675S,U,R11.3%HSMM 0.769C,S,U,R0.703C,S,U,R15.2%HSMM(C) 0.785C,S,U,R0.722C,S,U,R20.4%Table 2: Results of the ROUGE evaluation.?R-1?
and ?R-2?
correspond to ROUGE-1 andROUGE-2, respectively.
The values in the col-umn of ?Idt.?
are the percentage of summariescompletely-identical to the corresponding refer-ences.
In the table,C,S,U,L,Rindicate statisti-cal significance against CRFs, KP(S), KP, LEAD,RANDOM, respectively.Method Gram.
Ref.
S./C.
OverallLEAD 1.9 3.9 2.5 2.1KP 4.1L3.7 3.4 3.5KP(S) 4.2L3.6 3.5 3.6LCRFs 4.1L3.9 3.7L3.6LHSMM 4.3L4.0 4.1L4.0LHSMM(C) 4.0L3.9 4.0L3.9LHUMAN 4.7L4.5 4.7L4.8LTable 3: Results of the linguistic quality evalua-tion.
The values ranged from 1 (very poor) to 5(very good) (National Institute of Standards andTechnology, 2007).
We show statistical signifi-cance with the same notations as Table 2.ones, and hence the references can be reproduced if important sentences are identified.
Since the com-pression rate in our corpus was relatively light, it made important information easy to identify.
Amongthe compared methods, both LEAD and our proposed method, HSMM(C), achieved the best result.
Therewas no significant difference between LEAD and HSMM(C).
This surprising performance of LEAD wasdue to the ROUGE evaluation.
The words in the document leads were likely to be important, and LEADdrew on this property.
However, as we mentioned later, it sacrificed the linguistic quality to achieve thehigh ROUGE score.
Furthermore, it failed to yield summaries identical to the reference.
In contrast toLEAD, almost 20% of the summaries generated by HSMM(C) were identical to the references.
Thisshows that our method successfully mimicked human assessments.
HSMM followed the best models.There was a statistically significant difference between HSMM(C) and HSMM.
Since some sentences,especially the first sentence in the document, were long and the first sentence was particularly impor-tant to summarize the document, sentence compression yielded a significant improvement.
As shownin Table 2, employing compression greatly improved the percentage of identical summaries.
HSMMsignificantly outperformed all of the baseline extractive methods except LEAD.
While CRFs can takeadvantage of all features used in HSMM, CRFs cannot take the evaluation measure such as ROUGE andthe knapsack constraint into account in learning.
HSMM also significantly outperformed KP(S).
Thisdifference is particularly important, and shows the usefulness of features related to coherence.
WhileKP(S) used only features about sentences, HSMM successfully mimicked the references as it drew onthe features related to coherence.We show the learning curve of HSMM in Figure 3.
We fixed 2,748 pairs for testing, and learnedparameters from 100, 250, 500, 1,000, 2,500, 5,000, 7,500 and 10,000 pairs.
The curve in the figureclearly shows the effectiveness of our large-scale corpus in learning.
It seems that the curve does notsaturate and hence HSMM performance can be improved by more training samples.
As in the resultsrecently shown by Filippova (2013), this result implies that large-scale data is important in the fieldof document summarization as in other fields of computational linguistics.
Past studies in documentsummarization relied on relatively small datasets consisting of a few dozen or at most a few hundredpairs of a document and its reference in learning.
In contrast to the past studies, there are over 10,000pairs in our dataset and the results show its effectiveness.Second, we discuss the result of the linguistic quality evaluation.
Unlike the ROUGE evaluation,HSMM achieved the best result.
As previous studies have pointed out (Nenkova and McKeown, 2011),sentence compression commonly tends to degrade the linguistic quality of a summary while improvingits content.
As shown in Table 3, the grammaticality of HSMM(C) is lower than that of HSMM, but the1656difference is not significant.
Although we could not observe any significant difference between HSMMand other extractive baselines, our proposals, HSMM and HSMM(C), yielded the best result in termsof structure/coherence.
By making use of the features related to coherence, we successfully improvedsummary quality.
In contrast to the surprising performance of LEAD in the ROUGE evaluation, in thelinguistic quality evaluation, LEAD yielded the worst performance.
Since LEAD had to cut the sentenceswhen it reached the given length, it create ungrammatical fragments.Finally, we touch on the balance between the quality of content and linguistic quality.
ComparingTable 2 to 3, we can see the correlation between the quality of content and linguistic quality.
This re-sult is reasonable because we can extract much more information from grammatical and well-organizedsentences.
Although we optimized the parameter to maximize the ROUGE score, it also yielded im-provements in linguistic quality.
This is because the manually-generated reference summaries are ba-sically grammatical and well-organized and the parameter is learnt to mimic them.
However, there isan inherent trade-off between the quality of content and linguistic quality.
For example, under stricterlength limitations, instead of cohesive devices such as conjunctions, which can improve the coherence ofsentences, content words would be preferred for summary inclusion to augment information.
Balancingthem to maximize reader satisfaction is an interesting problem.8 ConclusionsIn this paper we presented a novel single-document summarization method based on the hidden semi-Markov model, which is a natural extension of the knapsack problem.
Our model naturally takes accountof sentence context when identifying important sentences.
This property is particularly important toensure the coherence of output summaries and to produce informative and linguistically high-qualitysummaries.
We also proposed an algorithm based on dynamic programming so the best solution can belocated quickly.
Experiments on a very large-scale single-document summarization corpus showed thatour proposed method significantly outperforms competitive baselines.As future work, we plan to tackle on the summarization task where higher compression is demanded.To generate shorter summaries, we plan to employ more sophisticated approaches, such as paraphrasing.AcknowledgementThe corpus used in this paper is owned by The Mainichi Newspapers Co., Ltd. and is leased to NipponTelegraph and Telephone Corporation.
We sincerely appreciate their consideration.
We also appreciatethe insightful comments from reviewers.
Their comments greatly improved the quality of this paper.ReferencesErnst Althaus, Nikiforos Karamanis, and Alexander Koller.
2004.
Computing locally coherent discourses.
InProceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL), pages 399?406.Regina Barzilay and Michael Elhadad.
1997.
Using lexical chains for text summarization.
In Proceedings of theIntelligent Scalable Text Summarization Workshop (ISTS), pages 10?17.Regina Barzilay and Mirella Lapata.
2005.
Modeling local coherence: an entity-based approach.
In Proceedingsof the 43rd Annual Meeting on Association for Computational Linguistics (ACL), pages 141?148.Regina Barzilay and Lillian Lee.
2004.
Catching the drift: Probabilistic content models, with applications togeneration and summarization.
In HLT-NAACL 2004: Main Proceedings, pages 113?120.Regina Barzilay, Noemie Elhadad, and Kathleen R. McKeown.
2002.
Inferring strategies for sentence ordering inmultidocument news summarization.
Journal of Artificial Intelligence Research, 17:35?55.Janara Christensen, Mausam, Stephen Soderland, and Oren Etzioni.
2013.
Towards coherent multi-documentsummarization.
In Proceedings of the 2013 Conference of the North American Chapter of the Association forComputational Linguistics: Human Language Technologies, pages 1163?1173.James Clarke and Mirella Lapata.
2007.
Modelling compression with discourse constraints.
In Proceedings ofthe 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 1?11.1657Michael Collins.
2002.
Discriminative training methods for hidden markov models: Theory and experiments withperceptron algorithms.
In Proceedings of the 2002 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1?8.Koby Crammer.
2006.
Online passive-aggressive algorithms.
Journal of Machine Learning Research,7(Mar):551?585.Hal Daume, III and Daniel Marcu.
2002.
A noisy-channel model for document compression.
In Proceedings ofthe 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 449?456.Katja Filippova.
2013.
Overcoming the lack of parallel data in sentence compression.
In Proceedings of the 2013Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1481?1491.Takeshi Fuchi and Shinichiro Takagi.
1998.
Japanese morphological analyzer using word co-occurrence: Jtag.In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17thInternational Conference on Computational Linguistics (ACL-COLING), pages 409?413.Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi.
1995.
Centering: a framework for modeling the localcoherence of discourse.
Computational Linguistics, 21(2):203?225.Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino, Norihito Yasuda, and Masaaki Nagata.
2013.
Single-document summarization as a tree knapsack problem.
In Proceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing (EMNLP), pages 1515?1520.Sture Holm.
1979.
A simple sequentially rejective multiple test procedure.
Scandinavian Journal of Statistics,6(2):65?70.Kenji Imamura, Genichiro Kikui, and Norihito Yasuda.
2007.
Japanese dependency parsing using sequential label-ing for semi-spoken language.
In Proceedings of the 45th Annual Meeting of the Association for ComputationalLinguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 225?228.Nikiforos Karamanis, Massimo Poesio, Chris Mellish, and Jon Oberlander.
2004.
Evaluating centering-basedmetrics of coherence.
In Proceedings of the 42nd Meeting of the Association for Computational Linguistics(ACL), pages 391?398.Sungwoong Kim, Sungrack Yun, and Chang D. Yoo.
2011.
Large margin discriminative semi-markov modelfor phonetic recognition.
IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING,7(19):1999?2012.Bernhard Korte and Jens Vygen.
2008.
Combinatorial Optimization.
Springer-Verlag, third edition.John Lafferty, Andrew McCallum, and Fernando C. N. Pereira.
2001.
Conditional random fields: Probabilisticmodels for segmenting and labeling sequence data.
In Proceedings of the 18th International Conference onMachine Learning (ICML), pages 282?289.Mirella Lapata.
2003.
Probabilistic text structuring: Experiments with sentence ordering.
In Proceedings of the41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 545?552.Chin-Yew Lin.
2004.
Rouge: A package for automatic evaluation of summaries.
In Proceedings of ACL WorkshopText Summarization Branches Out, pages 74?81.Annie Louis and Ani Nenkova.
2012.
A coherence model based on syntactic patterns.
In Proceedings of the 2012Conference on Empirical Methods on Natural Language Processing and Computational Natural LanguageLearning (EMNLP-CoNLL).Hans P. Luhn.
1958.
The automatic creation of literature abstracts.
IBM Journal of Research and Development,22(2):159?165.William C. Mann and Sandra A Thompson.
1988.
Rhetorical structure theory: Toward a functional theory of textorganization.
Text, 8(3):243?281.Daniel Marcu.
1997.
From discourse structure to text summaries.
In Proceedings of ACL/EACL 1997 Summariza-tion Workshop, pages 82?88.Ryan McDonald.
2007.
A study of global inference algorithms in multi-document summarization.
In Proceedingsof the 29th European Conference on Information Retrieval (ECIR), pages 557?564.1658National Institute of Standards and Technology.
2007.
The linguistic quality questions.
http://www-nlpir.nist.gov/projects/duc/duc2007/quality-questions.txt.Ani Nenkova and Kathleen McKeown.
2011.
Automatic Summarization.
Now Publishers.Ani Nenkova and Lucy Vanderwende.
2005.
The impact of frequency on summarization.
Technical report,MSR-TR-2005-101.Ani Nenkova, Jieun Chae, Annie Louis, and Emily Pitler.
2010.
Structural features for predicting the linguisticquality of text: Applications to machine translation, automatic summarization and human-authored text.
InEmiel Krahmer and Theunem Mariet, editors, Empirical Methods in Natural Language Generation: Data-oriented Methods and Empirical Evaluation, pages 222?241.
Springer.Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo, and Genichiro Kikui.
2010.
Opinion summarizationwith integer linear programming formulation for sentence extraction and ordering.
In Coling 2010: Posters,pages 910?918.Tadashi Nomoto.
2008.
A generic sentence trimmer with crfs.
In Proceedings of the 46th Annual Conference ofthe Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pages 299?307.Naoaki Okazaki, Yutaka Matsuo, and Mitsuru Ishizuka.
2004.
Improving chronological sentence ordering byprecedence relation.
In Proceedings of the 20th International Conference on Computational Linguistics (Col-ing), pages 750?756.Emily Pitler, Annie Louis, and Ani Nenkova.
2010.
Automatic evaluation of linguistic quality in multi-documentsummarization.
In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics(ACL), pages 544?554.Sunita Sarawagi and William W. Cohen.
2004.
Semi-markov conditional random fields for information extraction.In Advances in Neural Information Processing Systems 17, pages 1185?1192.Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen.
2007.
Document summarization using conditionalrandom fields.
In Proceedings of the 20th international joint conference on Artifical intelligence (IJCAI), pages2862?2867.Jerod J. Weinman, Erik Learned-Miller, and Allen Hanson.
2008.
A discriminative semi-markov model for robustscene text recognition.
In Proceedings of the 19th International Conference on Pattern Recognition (ICPR),pages 1?5.Frank Wilcoxon.
1945.
Individual comparisons by ranking methods.
Biometrics Bulletin, 1(6):80?83.Shun-Zheng Yu.
2010.
Hidden semi-markov models.
Artificial Intelligence, 174(2):215?243.David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and Schwartz Richard.
2007.
Multi-candidate reduction: Sentencecompression as a tool for document summarization tasks.
Information Processing and Management, 43:1549?1570.1659
