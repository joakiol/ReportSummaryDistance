Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 407?411,Sydney, July 2006. c?2006 Association for Computational LinguisticsTowards the Orwellian NightmareSeparation of Business and Personal EmailsSanaz Jabbari, Ben Allison, David Guthrie, Louise GuthrieDepartment of Computer ScienceUniversity of Sheffield211 Portobello St.SheffieldS1 4DP{s.jabbari, b.allison, d.guthrie, l.guthrie}@dcs.shef.ac.ukAbstractThis paper describes the largest scale annotation pro-ject involving the Enron email corpus to date.
Over12,500 emails were classified, by humans, into thecategories ?Business?
and ?Personal?, and then sub-categorised by type within these categories.
The paperquantifies how well humans perform on this task(evaluated by inter-annotator agreement).
It presentsthe problems experienced with the separation of theselanguage types.
As a final section, the paper presentspreliminary results using a machine to perform thisclassification task.1 IntroductionAlmost since it became a global phenomenon, com-puters have been examining and reasoning about ouremail.
For the most part, this intervention has beenwell natured and helpful ?
computers have been try-ing to protect us from attacks of unscrupulous blanketadvertising mail shots.
However, the use of computersfor more nefarious surveillance of email has so farbeen limited.
The sheer volume of email sent meanseven government agencies (who can legally interceptall mail) must either filter email by some pre-conceived notion of what is interesting, or they mustemploy teams of people to manually sift through thevolumes of data.
For example, the NSA has had mas-sive parallel machines filtering e-mail traffic for atleast ten years.The task of developing such automatic filters at re-search institutions has been almost impossible, but forthe opposite reason.
There is no shortage of willingresearchers, but progress has been hampered by thelack of any data ?
one?s email is often hugely private,and the prospect of surrendering it, in its entirety, forresearch purposes is somewhat unsavoury.Recently, a data resource has become available whereexactly this condition (several hundred people?s entireemail archive) has been satisfied ?
the Enron dataset.During the legal investigation of the collapse of En-ron, the FERC (Federal Energy Regulatory Commis-sion) seized the emails of every employee in thatcompany.
As part of the process, the collection ofemails was made public and subsequently preparedfor research use by researchers at Carnegie MelonUniversity (Klimt and Yang, 2004).Such a corpus ofauthentic data, on such a large scale, is unique, and aninvaluable research tool.
It then falls to the prospec-tive researcher to decide which divisions in the lan-guage of email are interesting, which are possible, andhow the new resource might best be used.Businesses which offer employees an email system atwork (and there are few who do not) have alwaysknown that they possess an invaluable resource formonitoring their employees?
work habits.
During the1990s, UK courts decided that that an employee?semail is not private ?
in fact, companies can readthem at will.
However, for exactly the reasons de-scribed above, automatic monitoring has been impos-sible, and few businesses have ever considered it suf-ficiently important to employ staff to monitor theemail use of other staff.
However, in monitoring staffproductivity, few companies would decline the use ofa system which could analyse the email habits of itsemployees, and report the percentage of time whicheach employee was spending engaged in non-workrelated email activities.The first step in understanding how this problemmight be tackled by a computer, and if it is even fea-sible for this to happen, is to have humans perform thetask.
This paper describes the process of having hu-mans annotate a corpus of emails, classifying each asto whether they are business or personal, and thenattempting to classify the type of business or personalmail being considered.A resource has been created to develop a system ableto make these distinctions automatically.
Furthermore,the process of subcategorising types of business andpersonal has allowed invaluable insights into the areas407where confusion can occur, and how these confusionsmight be overcome.The paper presents an evolution of appropriate sub-categories, combined with analysis of performance(measured by inter-annotator agreement) and reasonsfor any alterations.
It addresses previous work donewith the Enron dataset, focusing particularly on thework of Marti Hearst at Berkeley who attempted asmaller-scale annotation project of the Enron corpus,albeit with a different focus.
It concludes by suggest-ing that in the main part (with a few exceptions) thetask is possible for human annotators.
The project hasproduced a set of labeled messages (around 14,000,plus double annotations for approximately 2,500) witharguably sufficiently high business-personal agree-ment that machine learning algorithms will have suf-ficient material to attempt the task automatically.2 Introduction to the CorpusEnron?s email was made public on the Web by FERC(Federal Energy Regulatory Commission), during alegal investigation on Enron Corporation.
The emailscover 92 percent of the staff?s emails, because somemessages have been deleted "as part of a redactioneffort due to requests from affected employees".
Thedataset was comprised of 619,446 messages from 158users in 3,500 folders.
However, it turned out that theraw data set was suffering from various data integrityproblems.
Various attempts were made to clean andprepare the dataset for research purposes.
The datasetused in this project was the March 2, 2004 versionprepared at Carnegie Mellon University, acquiredfrom http://www.cs.cmu.edu/~enron/.
This version ofthe dataset was reduced to 200,399 emails by remov-ing some folders from each user.
Folders like ?discus-sion threads?
and ?all documents?, which were ma-chine generated and contained duplicate emails, wereremoved in this version.There were on average 757 emails per each of the 158users.
However, there are between one and 100,000emails per user.
There are 30,091 threads present in123,091 emails.
The dataset does not include attach-ments.
Invalid email addresses were replaced with?user@enron.com?.
When no recipient was specifiedthe address was replaced with?no_address@enron.com?
(Klimt and Yang, 2005).3 Previous Work with the DatasetThe most relevant piece of work to this paper wasperformed at Berkeley.
Marti Hearst ran a small-scaleannotation project to classify emails in the corpus bytheir type and purpose (Email annotation at Berkely).In total, approximately 1,700 messages were anno-tated by two distinct annotators.
Annotation catego-ries captured four dimensions, but broadly speakingthey reflected the following qualities of the email:coarse genre, the topic of the email if business wasselected, information about any forwarded or includedtext and the emotional tone of the email.
However, thecategories used at the Berkeley project were incom-patible with our requirements for several reasons: thatproject allowed multiple labels to be assigned to eachemail; the categories were not designed to facilitatediscrimination between business and personal emails;distinctions between topic, genre, source and purposewere present in each of the dimensions; and no effortwas made to analyse the inter-annotator agreement(Email annotation at Berkely).User-defined folders are preserved in the Enron data,and some research efforts have used these folders todevelop and evaluate machine-learning algorithms forautomatically sorting emails (Klimt and Yang, 2004).However, as users are often inconsistent in organisingtheir emails, so the training and testing data in thesecases are questionable.
For example, many usershave folders marked ?Personal?, and one might thinkthese could be used as a basis for the characterisationof personal emails.
However, upon closer inspection itbecomes clear that only a tiny percentage of an indi-vidual?s personal emails are in these folders.
Simi-larly, many users have folders containing exclusivelypersonal content, but without any obvious foldername to reveal this.
All of these problems dictate thatfor an effective system to be produced, large-scalemanual annotation will be necessary.Researchers at Queen?s University, Canada (Keila,2005) recently attempted to categorise and identifydeceptive messages in the Enron corpus.
Theirmethod used a hypothesis from deception theory (e.g.,deceptive writing contains cues such as reduced fre-quency of first-person pronouns and increased fre-quency of ?negative emotion?
words) and as to whatconstitutes deceptive language.
Single value decom-position (SVD) was applied to separate the emails,and a manual survey of the results allowed them toconclude that this classification method for detectingdeception in email was promising.Other researchers have attempted to analyse the Enronemails from a network analytic perspective (Deisner,2005).
Their goal was to analyse the flow of commu-nication between employees at times of crisis, anddevelop a characterisation for the state of a communi-cation network in such difficult times, in order toidentify looming crises in other companies from thestate of their communication networks.
They com-pared the network flow of email in October 2000 andOctober 2001.4 Annotation Categories for this ProjectBecause in many cases there is no definite line be-tween business emails and personal emails, it wasdecided to mark emails with finer categories than408Business and Personal.
This subcategorising not onlyhelped us to analyse the different types of emailwithin business and personal emails, but it helped usto find the nature of the disagreements that  occurredlater on, in inter-annotation.
In other words, thisprocess allowed us to observe patterns in disagree-ment.Obviously, the process of deciding categories in anyannotation project is a fraught and contentious one.The process necessarily involves repeated cycles ofcategory design, annotation, inter-annotation, analysisof disagreement, category refinement.
While the proc-ess described above could continue ad infinitum, thesensible project manager must identify were thisprocess is beginning to converge on a set of well-defined but nonetheless intuitive categories, and final-ise them.Likewise, the annotation project described here wentthrough several evolutions of categories, mediated byinput from annotators and other researchers.
The finalcategories chosen were:Business: Core Business, Routine Admin, Inter-Employee Relations, Solicited/soliciting mailing, Im-age.Personal: Close Personal, Forwarded, Auto generatedemails.5 Annotation and Inter-AnnotationBased on the categories above, approximately 12,500emails were single-annotated by a total of four anno-tators.The results showed that around 83% of the emailswere business related, while 17% were personal.
Thecompany received one personal email for every fivebusiness emails.Fig 1: Distribution of Emails in the CorpusBUSINESS83%PERSONAL17%BUSINESSPERSONALA third of the received emails were ?Core Business?and a third were ?Routine Admin?.
All other catego-ries comprised the remaining third of the emails.
Onecould conclude that approximately one third of emailsreceived at Enron were discussions of policy, strategy,legislation, regulations, trading, and other high-levelbusiness matters.
The next third of received emailswere about the peripheral, routine matters of the com-pany.
These are emails related to HR, IT administra-tion, meeting scheduling, etc.
which can be regardedas part of the common infrastructure of any largescale corporation.The rest of the emails were distributed among per-sonal emails, emails to colleagues, company newsletters, and emails received due to subscription.
Thebiggest portion of the last third, are emails receiveddue to subscription, whether the subscription be busi-ness or personal in nature.In any annotation project consistency should bemeasured.
To this end 2,200 emails were double an-notated between four annotators.
As Figure 2 belowshows, for 82% of the emails both annotators agreedthat the email was business email and in 12% of theemails, both agreed on them being personal.
Six per-cent of the emails were disagreed upon.Fig 2: Agreements and Disagreements in Inter-AnnotationDisagreement6%PersonalAgreement12%BusinessAgreement82%Business AgreementPersonal AgreementDisagreementBy analysing the disagreed categories, some patternsof confusion were found.Around one fourth of the confusions were solicitedemails where it was not clear whether the employeewas subscribed to a particular newsletter group for hispersonal interest, private business, or Enron?s busi-ness.
While some subscriptions were clearly personal(e.g.
subscription to latest celebrity news) and somewere clearly business related (e.g.
Daily Energy re-ports), for some it was hard to identify the intention ofthe subscription (e.g.
New York Times).Eighteen percent of the confusions were due to emailsabout travel arrangements, flight and hotel bookingconfirmations, where it was not clear whether the per-sonal was acting in a business or personal role.409Thirteen percent of the disagreements were uponwhether an email is written between two Enron em-ployees as business colleagues or friends.
The emailssuch as ?shall we meet for a coffee at 2:00??
If insuf-ficient information exists in the email, it can be hardto draw the line between a personal relationship and arelationship between colleagues.
The annotators wereadvised to pick the category based on the formality ofthe language used in such emails, and reading be-tween the lines wherever possible.About eight percent of the disagreements were onemails which were about services that Enron providesfor its employees.
For example, the Enron?s runningclub is seeking for runners, and sending an ad to En-ron?s employers.
Or Enron?s employee?s assistanceProgram (EAP), sending an email to all employees,letting them know that in case of finding themselvesin stressful situations they can use some of the ser-vices that Enron provides for them or their families.One theme was encountered in many types of confu-sions: namely, whether to decide an e-mail?s categorybased upon its topic or its form.
For example, shouldan email be categorised because it is scheduling ameeting or because of the subject of the meeting be-ing scheduled?
One might consider this a distinctionby topic or by genre.As the result, final categories were created to reflecttopic as the only dimension to be considered in theannotation.
?Solicited/Soliciting mailing?, ?Solic-ited/Auto generated mailing?
and ?Forwarded?
wereremoved and ?Keeping Current?, ?Soliciting?
wereadded as business categories and ?Personal Mainte-nance?
and ?Personal Circulation?
were added as per-sonal categories.
The inter-annotation agreement wasmeasured for one hundred and fifty emails, annotatedby five annotators.
The results confirmed that thesechanges had a positive effect on the accuracy of anno-tation.6 Preliminary Results of AutomaticClassificationSome preliminary experiments were performed withan automatic classifier to determine the feasibility ofseparating business and personal emails by machine.The classifier used was a probabilistic classifier basedupon the distribution of distinguishing words.
Moreinformation can be found in (Guthrie and Walker,1994).Two categories from the annotation were chosenwhich were considered to typify the broad categories?
these were Core Business (representing business)and Close Personal (representing personal).
The CoreBusiness class contains 4,000 messages (approx900,000 words), while Close Personal contains ap-proximately 1,000 messages (220,000 words).The following table summarises the performance ofthis classifier in terms of Recall, Precision and F-Measure and accuracy:Class Recall Precision F-MeasureAccuracyBusiness 0.99 0.92 0.95 0.99Personal 0.69 0.95 0.80 0.69AVERAGE 0.84 0.94 0.88 0.93Based upon the results of this experiment, one canconclude that automatic methods are also suitable forclassifying emails as to whether they are business orpersonal.
The results indicate that the business cate-gory is well represented by the classifier, and giventhe disproportionate distribution of emails, the classi-fier?s tendency towards the business category is un-derstandable.Given that our inter-annotator agreement statistic tellsus that humans only agree on this task 94% of thetime, preliminary results with 93% accuracy (the sta-tistic which correlates exactly to agreement) of theautomatic method are encouraging.
While more workis necessary to fully evaluate the suitability of thistask for application to a machine, the seeds of a fullyautomated system are sown.7 ConclusionThis paper describes the process of creating an emailcorpus annotated with business or personal labels.
Bymeasuring inter-annotator agreement it shows that thisprocess was successful.
Furthermore, by analysing thedisagreements in the fine categories, it has allowed usto characterise the areas where the business/personaldecisions are difficult.In general, the separation of business and personalmails is a task that humans can perform.
Part of theproject has allowed the identification of the areaswhere humans cannot make this distinction (as dem-onstrated by inter-annotator agreement scores) andone would not expect machines to perform the taskunder these conditions either.
In all other cases, wherethe language is not ambiguous as judged by humanannotators, the challenge has been made to automaticclassifiers to match this performance.Some initial results were reported where machinesattempted exactly this task.
They showed that accu-racy almost as high as human agreement wasachieved by the system.
Further work, using muchlarger sets and incorporating all types of business andpersonal emails, is the next logical step.410Any annotation project will encounter its problems indeciding appropriate categories.
This paper describedthe various stages of evolving these categories to astage where they are both intuitive and logical andalso, produce respectable inter-annotator agreementscores.
The work is still in progress in ensuringmaximal consistency within the data set and refiningthe precise definitions of the categories to avoid pos-sible overlaps.ReferencesBrian Klimt and Yiming Yang.
2004.
Introducing theEnron Email Corpus, Carnegie Mellon University.Brian Klimt and Yiming Yang.
2004.
The Enron Cor-pus: A New Data Set for Email Classification Re-search.
Carnegie Mellon University.Email Annotation at Berkelyhttp://bailando.sims.berkeley.edu/enron_email.htmlJana Diesner and Kathleen M. Karley.
2005.
Explora-tion of Communication Networks from the EnronEmail Corpus, Carnegie Mellon UniversityLouise Guthrie,  Elbert Walker and Joe Guthrie.
1994Document classification by machine: Theory andpractice.
Proc.
of COLING'94Parambir S. Keila and David B. Skillcorn.
2005.
De-tecting Unusual and Deceptive Communication inEmail.
Queen?s University, CA411
