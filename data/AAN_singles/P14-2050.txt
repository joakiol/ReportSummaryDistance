Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 302?308,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsDependency-Based Word EmbeddingsOmer Levy?and Yoav GoldbergComputer Science DepartmentBar-Ilan UniversityRamat-Gan, Israel{omerlevy,yoav.goldberg}@gmail.comAbstractWhile continuous word embeddings aregaining popularity, current models arebased solely on linear contexts.
In thiswork, we generalize the skip-gram modelwith negative sampling introduced byMikolov et al to include arbitrary con-texts.
In particular, we perform exper-iments with dependency-based contexts,and show that they produce markedlydifferent embeddings.
The dependency-based embeddings are less topical and ex-hibit more functional similarity than theoriginal skip-gram embeddings.1 IntroductionWord representation is central to natural languageprocessing.
The default approach of represent-ing words as discrete and distinct symbols is in-sufficient for many tasks, and suffers from poorgeneralization.
For example, the symbolic repre-sentation of the words ?pizza?
and ?hamburger?are completely unrelated: even if we know thatthe word ?pizza?
is a good argument for the verb?eat?, we cannot infer that ?hamburger?
is alsoa good argument.
We thus seek a representationthat captures semantic and syntactic similaritiesbetween words.
A very common paradigm for ac-quiring such representations is based on the distri-butional hypothesis of Harris (1954), stating thatwords in similar contexts have similar meanings.Based on the distributional hypothesis, manymethods of deriving word representations were ex-plored in the NLP community.
On one end of thespectrum, words are grouped into clusters basedon their contexts (Brown et al, 1992; Uszkor-eit and Brants, 2008).
On the other end, words?Supported by the European Community?s SeventhFramework Programme (FP7/2007-2013) under grant agree-ment no.
287923 (EXCITEMENT).are represented as a very high dimensional butsparse vectors in which each entry is a measureof the association between the word and a particu-lar context (see (Turney and Pantel, 2010; Baroniand Lenci, 2010) for a comprehensive survey).In some works, the dimensionality of the sparseword-context vectors is reduced, using techniquessuch as SVD (Bullinaria and Levy, 2007) or LDA(Ritter et al, 2010; S?eaghdha, 2010; Cohen etal., 2012).
Most recently, it has been proposedto represent words as dense vectors that are de-rived by various training methods inspired fromneural-network language modeling (Bengio et al,2003; Collobert and Weston, 2008; Mnih andHinton, 2008; Mikolov et al, 2011; Mikolov etal., 2013b).
These representations, referred to as?neural embeddings?
or ?word embeddings?, havebeen shown to perform well across a variety oftasks (Turian et al, 2010; Collobert et al, 2011;Socher et al, 2011; Al-Rfou et al, 2013).Word embeddings are easy to work with be-cause they enable efficient computation of wordsimilarities through low-dimensional matrix op-erations.
Among the state-of-the-art word-embedding methods is the skip-gram with nega-tive sampling model (SKIPGRAM), introduced byMikolov et al (2013b) and implemented in theword2vec software.1Not only does it produceuseful word representations, but it is also very ef-ficient to train, works in an online fashion, andscales well to huge copora (billions of words) aswell as very large word and context vocabularies.Previous work on neural word embeddings takethe contexts of a word to be its linear context ?words that precede and follow the target word, typ-ically in a window of k tokens to each side.
How-ever, other types of contexts can be explored too.In this work, we generalize the SKIP-GRAM model, and move from linear bag-of-wordscontexts to arbitrary word contexts.
Specifically,1code.google.com/p/word2vec/302following work in sparse vector-space models(Lin, 1998; Pad?o and Lapata, 2007; Baroni andLenci, 2010), we experiment with syntactic con-texts that are derived from automatically produceddependency parse-trees.The different kinds of contexts produce no-ticeably different embeddings, and induce differ-ent word similarities.
In particular, the bag-of-words nature of the contexts in the ?original?SKIPGRAM model yield broad topical similari-ties, while the dependency-based contexts yieldmore functional similarities of a cohyponym na-ture.
This effect is demonstrated using both quali-tative and quantitative analysis (Section 4).The neural word-embeddings are consideredopaque, in the sense that it is hard to assign mean-ings to the dimensions of the induced represen-tation.
In Section 5 we show that the SKIP-GRAM model does allow for some introspectionby querying it for contexts that are ?activated by?
atarget word.
This allows us to peek into the learnedrepresentation and explore the contexts that arefound by the learning process to be most discrim-inative of particular words (or groups of words).To the best of our knowledge, this is the first workto suggest such an analysis of discriminatively-trained word-embedding models.2 The Skip-Gram ModelOur departure point is the skip-gram neural em-bedding model introduced in (Mikolov et al,2013a) trained using the negative-sampling pro-cedure presented in (Mikolov et al, 2013b).
Inthis section we summarize the model and train-ing objective following the derivation presented byGoldberg and Levy (2014), and highlight the easeof incorporating arbitrary contexts in the model.In the skip-gram model, each word w ?
W isassociated with a vector vw?
Rdand similarlyeach context c ?
C is represented as a vectorvc?
Rd, where W is the words vocabulary, Cis the contexts vocabulary, and d is the embed-ding dimensionality.
The entries in the vectorsare latent, and treated as parameters to be learned.Loosely speaking, we seek parameter values (thatis, vector representations for both words and con-texts) such that the dot product vw?
vcassociatedwith ?good?
word-context pairs is maximized.More specifically, the negative-sampling objec-tive assumes a dataset D of observed (w, c) pairsof words w and the contexts c, which appeared ina large body of text.
Consider a word-context pair(w, c).
Did this pair come from the data?
We de-note by p(D = 1|w, c) the probability that (w, c)came from the data, and by p(D = 0|w, c) =1 ?
p(D = 1|w, c) the probability that (w, c) didnot.
The distribution is modeled as:p(D = 1|w, c) =11+e?vw?vcwhere vwand vc(each a d-dimensional vector) arethe model parameters to be learned.
We seek tomaximize the log-probability of the observed pairsbelonging to the data, leading to the objective:argmaxvw,vc?
(w,c)?Dlog11+e?vc?vwThis objective admits a trivial solution in whichp(D = 1|w, c) = 1 for every pair (w, c).
This canbe easily achieved by setting vc= vwand vc?vw=K for all c, w, where K is large enough number.In order to prevent the trivial solution, the ob-jective is extended with (w, c) pairs for whichp(D = 1|w, c) must be low, i.e.
pairs which arenot in the data, by generating the set D?of ran-dom (w, c) pairs (assuming they are all incorrect),yielding the negative-sampling training objective:argmaxvw,vc(?
(w,c)?Dp(D = 1|c, w)?
(w,c)?D?p(D = 0|c, w))which can be rewritten as:argmaxvw,vc(?
(w,c)?Dlog ?(vc?
vw) +?
(w,c)?D?log ?(?vc?
vw))where ?
(x) = 1/(1+ex).
The objective is trainedin an online fashion using stochastic-gradient up-dates over the corpus D ?D?.The negative samples D?can be constructed invarious ways.
We follow the method proposed byMikolov et al: for each (w, c) ?
D we constructn samples (w, c1), .
.
.
, (w, cn), where n is a hy-perparameter and each cjis drawn according to itsunigram distribution raised to the 3/4 power.Optimizing this objective makes observedword-context pairs have similar embeddings,while scattering unobserved pairs.
Intuitively,words that appear in similar contexts should havesimilar embeddings, though we have not yet founda formal proof that SKIPGRAM does indeed max-imize the dot product of similar words.3 Embedding with Arbitrary ContextsIn the SKIPGRAM embedding algorithm, the con-texts of a word w are the words surrounding it303in the text.
The context vocabulary C is thusidentical to the word vocabulary W .
However,this restriction is not required by the model; con-texts need not correspond to words, and the num-ber of context-types can be substantially largerthan the number of word-types.
We generalizeSKIPGRAM by replacing the bag-of-words con-texts with arbitrary contexts.In this paper we experiment with dependency-based syntactic contexts.
Syntactic contexts cap-ture different information than bag-of-word con-texts, as we demonstrate using the sentence ?Aus-tralian scientist discovers star with telescope?.Linear Bag-of-Words Contexts This is thecontext used by word2vec and many other neu-ral embeddings.
Using a window of size k aroundthe target word w, 2k contexts are produced: thek words before and the k words after w. Fork = 2, the contexts of the target word w arew?2, w?1, w+1, w+2.
In our example, the contextsof discovers are Australian, scientist, star, with.2Note that a context window of size 2 may misssome important contexts (telescope is not a con-text of discovers), while including some acciden-tal ones (Australian is a context discovers).
More-over, the contexts are unmarked, resulting in dis-covers being a context of both stars and scientist,which may result in stars and scientists endingup as neighbours in the embedded space.
A win-dow size of 5 is commonly used to capture broadtopical content, whereas smaller windows containmore focused information about the target word.Dependency-Based Contexts An alternative tothe bag-of-words approach is to derive contextsbased on the syntactic relations the word partic-ipates in.
This is facilitated by recent advancesin parsing technology (Goldberg and Nivre, 2012;Goldberg and Nivre, 2013) that allow parsing tosyntactic dependencies with very high speed andnear state-of-the-art accuracy.After parsing each sentence, we derive wordcontexts as follows: for a target word w withmodifiers m1, .
.
.
,mkand a head h, we considerthe contexts (m1, lbl1), .
.
.
, (mk, lblk), (h, lbl?1h),2word2vec?s implementation is slightly more compli-cated.
The software defaults to prune rare words based ontheir frequency, and has an option for sub-sampling the fre-quent words.
These pruning and sub-sampling happen beforethe context extraction, leading to a dynamic window size.
Inaddition, the window size is not fixed to k but is sampleduniformly in the range [1, k] for each word.Australian scientist discovers star with telescopeamodnsubjdobjpreppobjAustralian scientist discovers star telescopeamodnsubjdobjprep withWORD CONTEXTSaustralian scientist/amod?1scientist australian/amod, discovers/nsubj?1discovers scientist/nsubj, star/dobj, telescope/prep withstar discovers/dobj?1telescope discovers/prep with?1Figure 1: Dependency-based context extraction example.Top: preposition relations are collapsed into single arcs,making telescope a direct modifier of discovers.
Bottom: thecontexts extracted for each word in the sentence.where lbl is the type of the dependency relation be-tween the head and the modifier (e.g.
nsubj, dobj,prep with, amod) and lbl?1is used to mark theinverse-relation.
Relations that include a preposi-tion are ?collapsed?
prior to context extraction, bydirectly connecting the head and the object of thepreposition, and subsuming the preposition itselfinto the dependency label.
An example of the de-pendency context extraction is given in Figure 1.Notice that syntactic dependencies are bothmore inclusive and more focused than bag-of-words.
They capture relations to words that arefar apart and thus ?out-of-reach?
with small win-dow bag-of-words (e.g.
the instrument of discoveris telescope/prep with), and also filter out ?coinci-dental?
contexts which are within the window butnot directly related to the target word (e.g.
Aus-tralian is not used as the context for discovers).
Inaddition, the contexts are typed, indicating, for ex-ample, that stars are objects of discovery and sci-entists are subjects.
We thus expect the syntacticcontexts to yield more focused embeddings, cap-turing more functional and less topical similarity.4 Experiments and EvaluationWe experiment with 3 training conditions: BOW5(bag-of-words contexts with k = 5), BOW2(same, with k = 2) and DEPS (dependency-basedsyntactic contexts).
We modified word2vec tosupport arbitrary contexts, and to output the con-text embeddings in addition to the word embed-dings.
For bag-of-words contexts we used theoriginal word2vec implementation, and for syn-tactic contexts, we used our modified version.
Thenegative-sampling parameter (how many negativecontexts to sample for every correct one) was 15.304All embeddings were trained on EnglishWikipedia.
For DEPS, the corpus was taggedwith parts-of-speech using the Stanford tagger(Toutanova et al, 2003) and parsed into labeledStanford dependencies (de Marneffe and Man-ning, 2008) using an implementation of the parserdescribed in (Goldberg and Nivre, 2012).
All to-kens were converted to lowercase, and words andcontexts that appeared less than 100 times werefiltered.
This resulted in a vocabulary of about175,000 words, with over 900,000 distinct syntac-tic contexts.
We report results for 300 dimensionembeddings, though similar trends were also ob-served with 600 dimensions.4.1 Qualitative EvaluationOur first evaluation is qualitative: we manually in-spect the 5 most similar words (by cosine similar-ity) to a given set of target words (Table 1).The first target word, Batman, results in similarsets across the different setups.
This is the case formany target words.
However, other target wordsshow clear differences between embeddings.In Hogwarts - the school of magic from thefictional Harry Potter series - it is evident thatBOW contexts reflect the domain aspect, whereasDEPS yield a list of famous schools, capturingthe semantic type of the target word.
This ob-servation holds for Turing3and many other nounsas well; BOW find words that associate with w,while DEPS find words that behave like w. Turney(2012) described this distinction as domain simi-larity versus functional similarity.The Florida example presents an ontologi-cal difference; bag-of-words contexts generatemeronyms (counties or cities within Florida),while dependency-based contexts provide cohy-ponyms (other US states).
We observed the samebehavior with other geographical locations, partic-ularly with countries (though not all of them).The next two examples demonstrate that simi-larities induced from DEPS share a syntactic func-tion (adjectives and gerunds), while similaritiesbased on BOW are more diverse.
Finally, we ob-serve that while both BOW5 and BOW2 yield top-ical similarities, the larger window size result inmore topicality, as expected.3DEPS generated a list of scientists whose name ends with?ing?.
This is may be a result of occasional POS-taggingerrors.
Still, the embedding does a remarkable job and re-trieves scientists, despite the noisy POS.
The list containsmore mathematicians without ?ing?
further down.Target Word BOW5 BOW2 DEPSbatmannightwing superman supermanaquaman superboy superboycatwoman aquaman supergirlsuperman catwoman catwomanmanhunter batgirl aquamanhogwartsdumbledore evernight sunnydalehallows sunnydale collinwoodhalf-blood garderobe calartsmalfoy blandings greendalesnape collinwood millfieldturingnondeterministic non-deterministic paulingnon-deterministic finite-state hotellingcomputability nondeterministic hetingdeterministic buchi lessingfinite-state primality hammingfloridagainesville fla texasfla alabama louisianajacksonville gainesville georgiatampa tallahassee californialauderdale texas carolinaobject-orientedaspect-oriented aspect-oriented event-drivensmalltalk event-driven domain-specificevent-driven objective-c rule-basedprolog dataflow data-drivendomain-specific 4gl human-centereddancingsinging singing singingdance dance rappingdances dances breakdancingdancers breakdancing mimingtap-dancing clowning buskingTable 1: Target words and their 5 most similar words, as in-duced by different embeddings.We also tried using the subsampling option(Mikolov et al, 2013b) with BOW contexts (notshown).
Since word2vec removes the subsam-pled words from the corpus before creating thewindow contexts, this option effectively increasesthe window size, resulting in greater topicality.4.2 Quantitative EvaluationWe supplement the examples in Table 1 withquantitative evaluation to show that the qualita-tive differences pointed out in the previous sec-tion are indeed widespread.
To that end, we usethe WordSim353 dataset (Finkelstein et al, 2002;Agirre et al, 2009).
This dataset contains pairs ofsimilar words that reflect either relatedness (top-ical similarity) or similarity (functional similar-ity) relations.4We use the embeddings in a re-trieval/ranking setup, where the task is to rank thesimilar pairs in the dataset above the related ones.The pairs are ranked according to cosine sim-ilarities between the embedded words.
We thendraw a recall-precision curve that describes theembedding?s affinity towards one subset (?sim-ilarity?)
over another (?relatedness?).
We ex-pect DEPS?s curve to be higher than BOW2?scurve, which in turn is expected to be higher than4Some word pairs are judged to exhibit both types of sim-ilarity, and were ignored in this experiment.305Figure 2: Recall-precision curve when attempting to rank thesimilar words above the related ones.
(a) is based on theWordSim353 dataset, and (b) on the Chiarello et al dataset.BOW5?s.
The graph in Figure 2a shows this is in-deed the case.
We repeated the experiment with adifferent dataset (Chiarello et al, 1990) that wasused by Turney (2012) to distinguish between do-main and functional similarities.
The results showa similar trend (Figure 2b).
When reversing thetask such that the goal is to rank the related termsabove the similar ones, the results are reversed, asexpected (not shown).55 Model IntrospectionNeural word embeddings are often consideredopaque and uninterpretable, unlike sparse vec-tor space representations in which each dimen-sion corresponds to a particular known context, orLDA models where dimensions correspond to la-tent topics.
While this is true to a large extent, weobserve that SKIPGRAM does allow a non-trivialamount of introspection.
Although we cannot as-sign a meaning to any particular dimension, wecan indeed get a glimpse at the kind of informa-tion being captured by the model, by examiningwhich contexts are ?activated?
by a target word.Recall that the learning procedure is attemptingto maximize the dot product vc?vwfor good (w, c)pairs and minimize it for bad ones.
If we keep thecontext embeddings, we can query the model forthe contexts that are most activated by (have thehighest dot product with) a given target word.
Bydoing so, we can see what the model learned to bea good discriminative context for the word.To demonstrate, we list the 5 most activatedcontexts for our example words with DEPS em-beddings in Table 2.
Interestingly, the most dis-criminative syntactic contexts in these cases are5Additional experiments (not presented in this paper) re-inforce our conclusion.
In particular, we found that DEPSperform dramatically worse than BOW contexts on analogytasks as in (Mikolov et al, 2013c; Levy and Goldberg, 2014).batman hogwarts turingsuperman/conj?1students/prep at?1machine/nn?1spider-man/conj?1educated/prep at?1test/nn?1superman/conj student/prep at?1theorem/poss?1spider-man/conj stay/prep at?1machines/nn?1robin/conj learned/prep at?1tests/nn?1florida object-oriented dancingmarlins/nn?1programming/amod?1dancing/conjbeach/appos?1language/amod?1dancing/conj?1jacksonville/appos?1framework/amod?1singing/conj?1tampa/appos?1interface/amod?1singing/conjflorida/conj?1software/amod?1ballroom/nnTable 2: Words and their top syntactic contexts.not associated with subjects or objects of verbs(or their inverse), but rather with conjunctions, ap-positions, noun-compounds and adjectivial modi-fiers.
Additionally, the collapsed preposition rela-tion is very useful (e.g.
for capturing the schoolaspect of hogwarts).
The presence of many con-junction contexts, such as superman/conj forbatman and singing/conj for dancing, mayexplain the functional similarity observed in Sec-tion 4; conjunctions in natural language tend to en-force their conjuncts to share the same semantictypes and inflections.In the future, we hope that insights from suchmodel introspection will allow us to develop bettercontexts, by focusing on conjunctions and prepo-sitions for example, or by trying to figure out whythe subject and object relations are absent andfinding ways of increasing their contributions.6 ConclusionsWe presented a generalization of the SKIP-GRAM embedding model in which the linear bag-of-words contexts are replaced with arbitrary ones,and experimented with dependency-based con-texts, showing that they produce markedly differ-ent kinds of similarities.
These results are ex-pected, and follow similar findings in the distri-butional semantics literature.
We also demon-strated how the resulting embedding model can bequeried for the discriminative contexts for a givenword, and observed that the learning procedureseems to favor relatively local syntactic contexts,as well as conjunctions and objects of preposition.We hope these insights will facilitate further re-search into improved context modeling and better,possibly task-specific, embedded representations.Our software, allowing for experimentation witharbitrary contexts, together with the embeddingsdescribed in this paper, are available for downloadat the authors?
websites.306ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pasca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and wordnet-based approaches.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 19?27, Boulder, Colorado, June.
Associationfor Computational Linguistics.Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.2013.
Polyglot: Distributed word representationsfor multilingual nlp.
In Proc.
of CoNLL 2013.Marco Baroni and Alessandro Lenci.
2010.
Dis-tributional memory: A general framework forcorpus-based semantics.
Computational Linguis-tics, 36(4):673?721.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.Peter F Brown, Robert L Mercer, Vincent JDella Pietra, and Jenifer C Lai.
1992.
Class-basedn-gram models of natural.
Computational Linguis-tics, 18(4).John A Bullinaria and Joseph P Levy.
2007.
Extractingsemantic representations from word co-occurrencestatistics: A computational study.
Behavior Re-search Methods, 39(3):510?526.Christine Chiarello, Curt Burgess, Lorie Richards, andAlma Pollock.
1990.
Semantic and associativepriming in the cerebral hemispheres: Some wordsdo, some words don?t... sometimes, some places.Brain and Language, 38(1):75?104.Raphael Cohen, Yoav Goldberg, and Michael Elhadad.2012.
Domain adaptation of a dependency parserwith a class-class selectional preference model.
InProceedings of ACL 2012 Student Research Work-shop, pages 43?48, Jeju Island, Korea, July.
Associ-ation for Computational Linguistics.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th International Conference onMachine Learning, pages 160?167.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependencies rep-resentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation, pages 1?8, Manchester, UK, Au-gust.
Coling 2008 Organizing Committee.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing search in context: Theconcept revisited.
ACM Transactions on Informa-tion Systems, 20(1):116?131.Yoav Goldberg and Omer Levy.
2014. word2vecexplained: deriving mikolov et al?s negative-sampling word-embedding method.
arXiv preprintarXiv:1402.3722.Yoav Goldberg and Joakim Nivre.
2012.
A dynamicoracle for the arc-eager system.
In Proc.
of COLING2012.Yoav Goldberg and Joakim Nivre.
2013.
Trainingdeterministic parsers with non-deterministic oracles.Transactions of the association for ComputationalLinguistics, 1.Zellig Harris.
1954.
Distributional structure.
Word,10(23):146?162.Omer Levy and Yoav Goldberg.
2014.
Linguisticregularities in sparse and explicit word representa-tions.
In Proceedings of the Eighteenth Conferenceon Computational Natural Language Learning, Bal-timore, Maryland, USA, June.
Association for Com-putational Linguistics.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Lin-guistics and 17th International Conference on Com-putational Linguistics - Volume 2, ACL ?98, pages768?774, Stroudsburg, PA, USA.
Association forComputational Linguistics.Tomas Mikolov, Stefan Kombrink, Lukas Burget,JH Cernocky, and Sanjeev Khudanpur.
2011.Extensions of recurrent neural network languagemodel.
In Acoustics, Speech and Signal Processing(ICASSP), 2011 IEEE International Conference on,pages 5528?5531.
IEEE.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
CoRR, abs/1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.Corrado, and Jeffrey Dean.
2013b.
Distributed rep-resentations of words and phrases and their com-positionality.
In Advances in Neural InformationProcessing Systems 26: 27th Annual Conference onNeural Information Processing Systems 2013.
Pro-ceedings of a meeting held December 5-8, 2013,Lake Tahoe, Nevada, United States, pages 3111?3119.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013c.
Linguistic regularities in continuous spaceword representations.
In Proceedings of the 2013Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 746?751, Atlanta,Georgia, June.
Association for Computational Lin-guistics.307Andriy Mnih and Geoffrey E Hinton.
2008.
A scal-able hierarchical distributed language model.
In Ad-vances in Neural Information Processing Systems,pages 1081?1088.Sebastian Pad?o and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A la-tent dirichlet alocation method for selectional pref-erences.
In ACL, pages 424?434.Diarmuid?O S?eaghdha.
2010.
Latent variable modelsof selectional preference.
In ACL, pages 435?444.Richard Socher, Jeffrey Pennington, Eric H Huang,Andrew Y Ng, and Christopher D Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 151?161.
Association forComputational Linguistics.Kristina Toutanova, Dan Klein, Chris Manning, andYoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of NAACL.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 384?394.
Association forComputational Linguistics.P.D.
Turney and P. Pantel.
2010.
From frequency tomeaning: Vector space models of semantics.
Jour-nal of Artificial Intelligence Research, 37(1):141?188.Peter D. Turney.
2012.
Domain and function: A dual-space model of semantic relations and compositions.Journal of Artificial Intelligence Research, 44:533?585.Jakob Uszkoreit and Thorsten Brants.
2008.
Dis-tributed word clustering for large scale class-basedlanguage modeling in machine translation.
In Proc.of ACL, pages 755?762.308
