Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 25?30,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsA Robot in the KitchenPeter Wallis,Department of Computer ScienceThe University of SheffieldSheffield, S1 4DP, UKp.wallis@dcs.shef.ac.ukAbstractA technology demonstrator is one thingbut having people use a technology is an-other, and the result reported here is thatpeople often ignore our lovingly craftedhandiwork.
The SERA project - SocialEngagement with Robots and Agents -was set up to look explicitly at what hap-pens when a robot companion is put insomeone?s home.
Even if things workedperfectly, there are times when a compan-ion?s human is simply not engaged.
As aresult we have separated our ?dialog man-ager?
into two parts: the dialog manageritself that determines what to say next, andan ?interaction manager?
that determineswhen to say it.
This paper details the de-sign of this SALT-E architecture.1 IntroductionThe SERA project, funded under FP7-ICT call3, was initially intended to take established tech-nology and put it in people?s homes so we couldrecord what happens.
The core idea was to providedata in order to compare alternate methodologiesfor moving from raw data to the next generationof synthetic companion.
Our primary motivationfor the proposal was the realisation that the se-mantics of language is just one part of languagein use.
Even in apparently task based dialogs, ef-fective repair strategies are essential and, what ismore, highly dependent on social skills.
Althoughthere are many ways of looking at language, doany of them provide the kind of information, andlevel of detail, required to build better conversa-tional agents?The focus has turned out to be on robots ratherthan embodied conversational agents and the robotof choice was a Nabaztag.
The Nabaztag is a com-mercially produced talking head from Violet in theFigure 1: Making an omelette.
In the real world,people ignore our handiwork!
(note Nabaztag earsin the foreground)style of Kismet and the Philips iCat.
It is a stylizedrabbit with expressive ears, a set of multi colourLEDs and is marketed as the world?s first inter-net enabled talking rabbit.
The rabbit connectsto the Violet server via a wireless router and canrun several applications including receiving SMSmessages, weather reports, tai chi, and streamingselected radio or blog sites.The target participant group for the SERA ex-periments was older people with little experienceof the limitations of computers.
As it turns out,our subjects to date all have personal computers athome, but the lack of a keyboard or screen, and therabbit being the only visible ?beige-ware?
meansthe set-up has been seen as sufficiently novel toprovide classic discourse behaviour in spite of itslimitations.The original scenario was to have the rabbit pro-vide classic internet services but our connectionwith the National Health Service (UK) throughone of the participants provided impetus for usto use a health related theme and enabled us torecruit some interesting people through Help the25Aged (Hel, 2010), Aged Concern (Age, 2010) andsimilar organisations.The primary result so far is that the establishedtechnology is seriously wanting.
Our initial in-tention was to put a Nabaztag in people?s homespretty much as it comes out of the box.
The prob-lem is that these robots are intended to be enter-taining rather than useful and the novelty soonwears off.
As Mival et alpoint out (Mival et al,2004) it is quite a challenge to design somethingthat doesn?t ?end up in the back of the cupboardwith the batteries out.?
Indeed these machines areexpected to be on a desk, and to be poked andprodded to make them do things.
For instance, themessaging function of the Nabaztag is certainlyfun and useful, but there are two modes in its stan-dard format: in the first the rabbit gives the mes-sage and assumes you are there.
There is no sens-ing of the environment; the rabbit simply blurts itout.
In the second mode it acts more like a clas-sic answering machine and the user is expected topress a button to prompt a conversation about mes-sages.
Although this might be useful, it is actingexactly like a classic answering machine and wethought we could do significantly better by addinga PIR sensor - a standard home security passiveinfra red sensor that detects movement.
We thusskipped the first version of our set-up and movedstraight to a slightly more pro-active version thatincorporated a PIR sensor to detect if the user waspresent.
This is where the trouble starts, and is theprimary point addressed in this paper.The second piece of wanting technology is ASR?
the automatic speech recognition.
We initiallyconsidered a range of possibilities for the ASR andsettled on Dragon Naturally Speaking, version 10(DNS).
In part this was driven by the fact that otherprojects were using it, and in part because of theDNS reputation.
If we had gone for somethingelse and it didn?t work, well, people would haveasked why we didn?t use DNS.
As it turned out,we could not get DNS to work with our set-upand for the first pass we resorted to yes/no but-tons.
Despite failing to get it working, using DNSwas probably the right decision for exactly the rea-son given above.
For the effort to have any im-pact however, other researchers need to know whathappened and to this end the next section detailsour woes.2 Speech RecognitionSpeech recognition has been seen as ?almostthere?
for twenty years and, from Furbys to in-teractive voice response phone systems, there areinstances where the technology is useful.
Whatis more, there is a body of work that points tothe word recognition rates being less critical thanone might assume (Wallis et al, 2001; Skantze,2007).
We allocated three months of a speechpost-doc to get something working and expectedit to take a week.
We considered several optionsincluding DNS, Loquendo?s VoxNauta which hasa garbage model (see below) the Sphinx-4 sys-tem from CMU which is open source and in Java,the Juicer system (Moore et al, 2006) for whichwe have local expertise, and the ubiquitous HTKToolKit which would certainly have the flexibilityto do what we thought needed doing but would,no doubt, result in something cobbled together andunreliable.
On the plus side we did have a singleuser that we could train but on the minus, we felta head-set microphone was out of the question forthe type of casual interaction we were expecting.From the outset the intention was to use wordspotting in continuous speech rather than attempt-ing to parse the user?s input.
This was primarilymotivated by the observation that successful NLPtechnologies such as chatbots and information ex-traction work that way.
What is more, unlike dic-tating a letter or capturing an academic talk, weexpected our subjects would not talk in full sen-tences, and utterances to be quite short.
A com-mand based system was considered but we did notwant to restrict it to ?Say yes, or no, now?
styledialogs.The approach we took was to use DNS as a largevocabulary continuous speech recognizer and thenrun regex style phrase spotting over the result - aclassic pipeline model.
The architecture was, andremains, an event driven model in which the di-alog manager unloads and loads sets of ?wordsof interest?
into the recognizer at pretty well eachturn.
These sets are of phrases rather than words,and ideally would include the regex equivalent of?.+?
and ???
- that is ?anything said?
and ?nothingsaid?.
The recognizer then reports back wheneversomething of interest occurs in the input, and doesit in a timely manner.The motivation for integrating speech and lan-guage this closely is the belief that the dialog man-ager can have a quite concise view of what the sub-26ject will say next.
What is more, getting it wrongis not critical if (and only if) the dialog managerhas a decent repair strategy.
The first of these be-liefs is discussed further below, and the second isbased on the results such as those in Wallis and inSkantze mentioned above.The result was that we failed to get speechrecognition working for the first iteration - despitethe world leading expertise in the group.
To quotefrom the 12 month project review:The COTS speech recognition did notprove as effective as supposed in the un-structured domestic environment, partlybecause of poor accuracy but also be-cause of unacceptable latency imposedby the language model.
Effective ASRdeployment was further complicated bylack of access to the workings of the un-derlying proprietary recognition engine.... and there is now a wider realisa-tion and acceptance among partners thatASR is not a solved problem.
[sera m12review, 25/03/2010]It turns out that a significant part of the per-formance delivered from dictation systems comesfrom the language model, not from the sound it-self.
The result was firstly that the system wouldwait for more input when the user didn?t produce agrammatical sentence.
This latency was often wellbeyond the point at which the resulting silence istreated by the user as information bearing.
Sec-ondly, when we did grab the available parts of thedecision lattice in order to fix the latency issue,the hypotheses were very poor.
Presumably thisis because the language model was providing evi-dence based on the false assumption that the userwould speak in proper sentences.
Trials are underway to test this.
The take away message is thatdictation systems are not necessarily suited to in-teractive dialog.
We have since heard that thereare ?secret switches?
that those in the know canadjust (Hieronymus, 2009) on DNS but, in retro-spect, if one is forced to use a COTS product onemight be better off using a system such as Vox-Nauta that acknowledges the needs of interactivesystems by including a garbage model.
At leastLoquendo have thought about the problems of in-teractive speech even if there is an apparent per-formance difference as measured in terms of worderror rates.The extent to which ASR relies on the languagemodel encourages us further to believe that atightly coupled dialog manager and speech recog-nition system will prove significantly better thansimply piping data from one module to another.3 Situated agentsIf you use a chatbot, or trial a demo, you neces-sarily attend to the artifact.
Your attention is on it,you want your attention on it, and the trial satisfac-torily ends when you stop attending to it.
Alarmsare designed to demand attention, but what shoulda companion do?
Figure 1 is a typical scene in par-ticipant number one?s kitchen.
She is making anomelette, and has told the rabbit that she is mak-ing an omelette.
Now she is not attending to therabbit and so what should the rabbit do?
In par-ticular, the rabbit can receive SMS style messagesand if one arrives as she is making her omelette,should the rabbit pass it on now or wait until thenext time she talks to it?
There is of course no rightanswer to this but the issue does need to be man-aged.
This is not a problem for a demo in whichthe action is scripted, and it is not an issue for theNabaztag in its commercial form as it only knowswhen a message arrives, and when the user pressesthe button.
With a PIR sensor however the systemknows that someone is there, but are they payingattention?
In the first iteration the system was cob-bled together with a quite linear approach to sys-tem initiative.
The latest version takes a slightlymore sophisticated approach and distinguishes be-tween three states at the top level.
The system is:?
Sleeping ?
not seeing or hearing anything,?
Alert ?
?attending to?
the person,?
Engaged - it is committed a conversationThe most obvious case of engagement is when theperson and the machine are having a conversation- that is Listening and Talking to each other, how-ever even if the conversation is finished, the sys-tem may still want to keep the context of the recentdiscussion.
As an example the system might havefinished its (system initiated) conversation aboutthe day ahead and wait to see if the human wantsto talk about their day before moving back to theAlert state in which the subject would need to gothrough the process of initiating a discussion.These four states, Sleeping, Alert, Talking, orListening (Engaged) are controlled by external27Figure 2: The InteractionManager handles whento say somethng; the DialogManager what to say.events and timers.
The GUI for editing dialog ac-tion frames provides 4 timing values as follows:Pause 1 indicates the end of a turn by the user- it is an opportunity for the systemto say something.Pause 2 indicates the system ought to saysomething, and with nothing to say,it does an encouragement.Pause 3 is the time after which the systemdrops the context of the conversa-tion.Pause 4 is the time at which the system goesto Sleep after the last PIR event.Mapping these pauses into action, at pause 1 thesystem may move from Listening to Talking;pause 2 is the same but with a conversational?filler?.
At pause 3 it moves from Engaged toAlert, and pause 4 from Alert to Sleeping.
ThePIR sensor is the primary means by which thesystem is moved from Sleeping to Alert, andAlert to Engaged (actually Listening) can behuman initiated by calling the system by name -?Hey Furby!?
being used on that classic toy, and?Computer?
being used on the bridge of the StarShip Enterprise.
Alternatively the system mayinitiate a conversation (Alert to Listening again)based on sensor information (for example, in ourcase the house keys being taken off the hook) anincoming message, or a diary event.The SALT(E) interaction manager relates to thedialog manager in that the interaction managerhandles the timing and determines when to saythings while it is left to the dialog manager to de-cide what to say.
The interface can again be de-scribed with a class diagram in which a Dialog-Manager extends the InteractionManager imple-menting the following abstract methods:heardThis(wrdEvent)getWhatToSay():StringnextEncouragement():StringIt is of course trivial to implement an Eliza styleconversation based on heardThis/getWhatToSaywith nextEncouragement taking the role of ?noth-ing matched?
patterns.
In the case of SERA, thedialog manager is a conventional state based sys-tem with states clustered into topics.The interaction manager also provides two othermethods:wakeup()systemInit(WrdEvent1,wrdEvent2)The first moves the system from Sleeping to Alertand initiates the pause 4 timer.
The method sys-temInit(...) calls heardThis() immediately withwrdEvent1 - note the interaction manager stillneeds to call getWhatToSay() before anything issaid.
The second argument is past to heardThis()the next time the system becomes Alert.
Thatis, the next time the user appears and the systemmoves from Sleeping to Alert, or the next time thesystem moves from Engaged to Alert.
wrdEvent1is an urgent message - in our case the message thatthe video recording is on - and wrdEvent2 repre-sents something that can join the queue.4 How language works (version 3)The above has been rather low level but hopefullysufficiently brief, while detailed enough to be re-producible.
But why is this of interest?
Surelythis is simply a technical issue that can be left tothe RAs - a classic case of ?flush pop-rivets?
(Vin-centi, 1990) which might be critical but is surely,well, boring.
This section provides the theoreticalbackground to the claim that managing engage-ment is critical.The classic computer science view of humanlanguage is that it is some form of debased perfectlanguage (Eco, 1995).
In the middle ages perfec-tion was defined in terms of God but to the Modernmind perfection has tended to mean something el-egant, concise and unambiguous, typified by pred-icate calculus.
Attempts to make computers un-derstand language have forced the realisation thathuman languages are primarily driven by conven-tion, highly context sensitive, and rely on the hu-man capability for simile and metaphor.
My latestview is that it is worse than that and that we pretty28much make it up as we go along.
This sectionbriefly introduces a model of language from theApplied Linguistics community and shows howthat model makes managing engagement critical.In 2004 a group of us became interested inthe way people tend to swear at conversationalagents (de Angeli, 2005).
In some work on an ani-mal version of the Turing Test, there is some ratherdramatic footage of a dog attacking an AIBO (Ku-binyi et al, 2003).
The interesting thing is thatthe dog warns the AIBO (twice) before throwingit across the room.
The observation is that dogs,like people, are social animals and that the warn-ing appears to be one mechanism for socializa-tion of the young.
When people abuse chat-bots,are they trying to socialize the machine?
This ofcourse would not be a concious process but rathernormative (Wallis, 2005).
This prompted a searchfor some high level social norm that might explainwhy people swear at computers.
The result of thatsearch was such a rule from the literature on Con-versation Analysis or CA.Paul Seedhouse (Seedhouse, 2004) summarisesthe outcome of the last 50 years of CA researchas with the observation that a conversational part-ner?s utterance will, in the normal case, go seenbut unnoticed.
That is, the utterance is an an-swer to a question, a response to a greeting or,more formally the second pair part of an adja-cency pair.
This is something computers can han-dle.
A common occurrence is however where anutterance goes noticed and accounted for.
In thiscase the conversational partner does not producethe expected response but what they do say makessense.
Indeed the conversational partner will workhard to produce the accounting.
This is Egginsand Slade (Eggins and Slade, 1997) introducingsequential relevance:Consider, for example, the two turns attalk below:A What?s that floating in the wine?B There aren?t any other solutions.You will try very hard to find a way ofinterpreting B?s turn as somehow an an-swer to A?s question, even though thereis no obvious link between them, apartfrom their appearance in sequence.
Per-haps you will have decided that B tooka common solution to a resistant winecork and poked it through into the bottle,and it was floating in the wine.
What-ever explanation you came up with, it isunlikely that you looked at the exampleand simply said ?it doesn?t make sense?,so strong is the implication that adjacentturns relate to each other.Having a machine account for human utterances isof course tricky but conversely the human conver-sational partner will work hard to account for whatare actually machine failings.If you fail to account for your CP?s utterance,then your CP risks sanction.
When your CP is achat-bot, that sanction is likely, it seems, to takethe form of swearing.
As a classic example of amachine?s interaction eliciting sanctions, here isa conversation from the 1996 Lobner Competition:SYSTEM G?day mate.JUDGE G?day.
How?s it going?SYSTEM My mother once said ?How it is go-ing is a mystery.?
She was a bit nuts.JUDGE Uh-huhSYSTEM Are you still learning to speak, myfriend?JUDGE NO, no.
But your anecdote took mea bit by surprise.JUDGE How was your morning?SYSTEM Oh well, that?s a pity!JUDGE I?m sorry; what?s a pity?SYSTEM Apologies accepted.JUDGE I feel compelled to insult you andwe?ve only been talking a minute.SYSTEM We are bloody well talking, aren?twe?JUDGE Oh baby.
We sure are.
So here aresome keywords for you: shit motherbaseball....This was the winning entry from that year butthe point is not the system, but how the humanresponds.There are of course more recent and betterfunded projects and in particular the DARPACommunicator Programme.
The observation isthat those systems did well when the human stuckto answering the system?s questions and the ASRworked - the seen but unnoticed.
When thegrounded knowledge was not as the script design-ers expected however, the system did not have theworld knowledge to understand the user?s repairstrategy.
The systems also failed to hand over con-trol to the user (Wallis, 2008).
The result wassanction and although swearing is rare ?
surpris-29ing when one listens to the conversations ?
usersdid ?not want to use the system on a regular ba-sis?
(Walker, 2002)The mechanism for accounting for can be bothtactical and strategic.
Eliza and Parry were verysuccessful in that user satisfaction was high com-pared to modern day systems.
The mechanism wasstrategic in those systems in that they provide anaccounting for their behaviour ?
in the first casebecause the role of psychologist accounts for theendless stream of personal questions, and in thesecond because being paranoid accounts for thesystem?s odd responses and interests.4.1 So, engagement?Why are we interested in engagement?
Becausein order for the human to ?work very hard to finda way of interpreting [what the machine said]?the human must be committed to the conversation.This commitment needs management, and it is therole of the InteractionManager to do this.
This isnot an issue for a chat bot on a website nor for asystem set up for experiments in a laboratory, butbecomes a significant issue for an interactive arti-fact that is permanently in someone?s kitchen.5 ConclusionsOur aim is to study long term relationships be-tween people and robot companions and the inten-tion is to put Nabaztags in an older person?s homeand see what happens.
This is not as straight-forward as it may first appear as much of our un-derstanding of these systems is based on demon-strators and experimental trials in which attentionis, by the very nature of the trial, directed to theartifact.
We introduce the SALT(E) model whichseparates the dialog manager in to a module thatdetermines what to say, and another that deter-mines when to say it.6 AcknowledgmentsThe research leading to these results has receivedfunding from the European Community?s SeventhFramework Programme [FP7/2007-2013] undergrant agreement no.
231868.
It has also receivedhelp from Loquendo that has generously providedthe text-to-speech system for our robots.References2010.
Aged Concern.
http://www.ageconcern.org.uk.Antonella de Angeli.
2005.
Stupid computer!abuse and social identity.
In Antonella De An-geli, Sheryl Brahnam, and Peter Wallis, edi-tors, Abuse: the darker side of Human-ComputerInteraction (INTERACT ?05), Rome, September.http://www.agentabuse.org/.Umberto Eco.
1995.
The Search for the Perfect Lan-guage (The Making of Europe).
Blackwell Publish-ers, Oxford, UK.Suzanne Eggins and Diana Slade.
1997.
AnalysingCasual Conversation.
Cassell, Wellington House,125 Strand, London.2010.
Help the Aged.
http://www.helptheaged.org.uk.Jim Hieronymus.
2009. personal communication.Eniko?
Kubinyi, A?da?m Miklo?si, Fre?de?ric Kaplan, Ma?rtaGa?csi, o?zsef Topa?l, and Vilmos Csa?nyi.
2003.Social behaviour of dogs encountering AIBO, ananimal-like robot in a neutral and in a feeding sit-uation.
Behavioural Proceses, 65:231?239.Oli Mival, S. Cringean, and D. Benyon.
2004.
Personi-fication technologies: Developing artificial compan-ions for older people.
In CHI Fringe, Austria.Darren Moore, John Dines, Mathew Magimai Doss,Jithendra Vepa, Octavian Cheng, and Thomas Hain.2006.
Juicer: A weighted finite state transducerspeech decoder.
In MLMI-06, Washington DC.Paul Seedhouse.
2004.
The Interactional Architectureof the Language Classroom: A Conversation Analy-sis Perspective.
Blackwell, September.Gabriel Skantze.
2007.
Error Handling in Spoken Di-alogue Systems - Managing Uncertainty, Groundingand Miscommunication.
Ph.D. thesis, Departmentof Speech, Music and Hearing, KTH.Walter G. Vincenti.
1990.
What Engineers know andhow they know it: analytical studies from aeronauti-cal history.
The John Hopkins Press Ltd, London.Marilyn et alWalker.
2002.
DARPA communicatorevaluation: Progress from 2000 to 2001.
In Pro-ceedings of ICSLP 2002, Denver, USA.Peter Wallis, Helen Mitchard, Damian O?Dea, and Jy-otsna Das.
2001.
Dialogue modelling for a con-versational agent.
In Markus Stumptner, Dan Cor-bett, and Mike Brooks, editors, AI2001: Advances inArtificial Intelligence, 14th Australian Joint Confer-ence on Artificial Intelligence, Adelaide, Australia.Springer (LNAI 2256).Peter Wallis.
2005.
Robust normative systems: Whathappens when a normative system fails?
In Sh-eryl Brahnam Antonella De Angeli and Peter Wallis,editors, Abuse: the darker side of human-computerinteraction, Rome, September.Peter Wallis.
2008.
Revisiting the DARPA communi-cator data using Conversation Analysis.
InteractionStudies, 9(3), October.30
