Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 17?25,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsAccurate Language Identification of Twitter MessagesMarco Lui and Timothy BaldwinNICTA VRLDepartment of Computing and Information SystemsUniversity of Melbourne, VIC 3010, Australiamhlui@unimelb.edu.au, tb@ldwin.netAbstractWe present an evaluation of ?off-the-shelf?
language identification systems asapplied to microblog messages from Twit-ter.
A key challenge is the lack of an ad-equate corpus of messages annotated forlanguage that reflects the linguistic diver-sity present on Twitter.
We overcome thisthrough a ?mostly-automated?
approach togathering language-labeled Twitter mes-sages for evaluating language identifica-tion.
We present the method to con-struct this dataset, as well as empiricalresults over existing datasets and off-the-shelf language identifiers.
We also testtechniques that have been proposed in theliterature to boost language identificationperformance over Twitter messages.
Wefind that simple voting over three specificsystems consistently outperforms any spe-cific system, and achieves state-of-the-artaccuracy on the task.1 IntroductionTwitter1has captured the attention of various re-search communities as a potent data source, be-cause of the immediacy of the information pre-sented, the volume and variability of the data con-tained, the potential to analyze networking effectswithin the data, and the ability to (where GPSdata is available) geolocate messages (Krishna-murthy et al., 2008).
Although individual mes-sages range from inane through mundane right upto insane, the aggregate of these messages can leadto profound insights in real-time.
Examples in-clude real-time detection of earthquakes (Sakaki1http://www.twitter.comet al., 2010), analysis of the location and preva-lence of flu epidemics (Lampos et al., 2010; Cu-lotta, 2010), news event detection (Petrovi?c et al.,2010), and prediction of sporting match outcomes(Sinha et al., 2013).Text analysis of social media has quickly be-come one of the ?frontier?
areas of Natural Lan-guage Processing (NLP), with major conferencesopening entire tracks for it in recent years.
Thechallenges in NLP for social media are many,stemming primarily from the ?noisy?
nature of thecontent.
Research indicates that English Twitterin particular is more dissimilar to the kinds of ref-erence corpora used in NLP to date, comparedto other forms of social media such as blogs andcomments (Baldwin et al., 2013).
This has ledto the development of techniques to ?normalize?Twitter messages (Han et al., 2013), as well asTwitter-specific approaches to conventional NLPtasks such as part-of-speech tagging (Gimpel etal., 2011) and information extraction (Bontchevaet al., 2013).
Even so, a precondition of NLPtechniques is that the language of the input datais known, and this has led to interest in ?languageidentification?
(LangID) of Twitter messages.
Re-search has shown that ?off-the-shelf?
LangID sys-tems appear to perform fairly well on Twitter (Luiand Baldwin, 2012), but Twitter-specific systemsseem to perform better (Carter et al., 2013; Trompand Pechenizkiy, 2011; Bergsma et al., 2012;Goldszmidt et al., 2013).Twitter recognizes the utility of language meta-data in enabling new applications, and as of March2013 includes language predictions with resultsfrom its API (Roomann-Kurrik, 2013).
These pre-dictions are not perfect (see Section 3.2), and attime of writing do not cover some languages (e.g.Romanian).
Furthermore, some research groups17have collected a substantial cache of Twitter datafrom before the availability of built-in predictions.Motivated by the need to work with monolingualsubsets of historical data, we investigate the mostpractical means of carrying out LangID of Twittermessages, balancing accuracy with ease of imple-mentation.
In this work, we present an evaluationof ?off-the-shelf?
language identifiers, combinedwith techniques that have been proposed for boost-ing accuracy on Twitter messages.A major challenge that we have had to over-come is the lack of annotated data for evaluation.Bergsma et al.
(2012) point out that in LangIDresearch on microblog messages to date, only asmall number of European languages has beenconsidered.
Baldwin and Lui (2010) showed that,when considering full documents, good perfor-mance on just European languages does not nec-essarily imply equally good performance when alarger set of languages is considered.
This doesnot detract from work to date on European lan-guages (Tromp and Pechenizkiy, 2011; Carter etal., 2013), but rather highlights the need for fur-ther research in LangID for microblog messages.Manual annotation of Twitter messages is achallenging and laborious process.
Furthermore,Twitter is highly multilingual, making it very dif-ficult to obtain annotators for all of the languagesrepresented.
Previous work has attempted tocrowdsource part of this process (Bergsma et al.,2012), but such an approach requires substantialmonetary investment, as well as care in ensuringthe quality of the final annotations.
In this paper,we propose an alternative, ?mostly-automated?approach to gathering language-labeled Twittermessages for evaluating LangID.
A corpus con-structed by direct application of automatic LangIDto Twitter messages would obviously be unsuit-able for evaluating the accuracy of LangID tools.Even with manual post-filtering, the remainingdataset would be biased towards messages thatare easy for automated systems to classify cor-rectly.
The novelty of our approach is to leverageuser identity, allowing us to construct a corpus oflanguage-labeled Twitter messages without usingautomated tools to determine the languages of themessages.
This quality makes the corpus suitablefor use in the evaluation of automated LangID ofTwitter messages.Our main contributions are: (1) we providea manually-labeled dataset of Twitter messages,adding Chinese (zh) and Japanese (ja) to the set ofTwitter messages with human annotation for lan-guage; (2) we provide a second dataset constructedusing a mostly-automated approach, covering 65languages; (3) we detail the method for construct-ing the dataset; (4) we provide a comprehensiveempirical evaluation of the accuracy of off-the-shelf LangID systems on Twitter messages, usingpublished datasets in addition to the new datasetswe have introduced; and (5) we discuss and eval-uate a simple voting-based ensemble for LangID,and find that it outperforms any individual systemto achieve state-of-the-art results.2 BackgroundLangID is the problem of mapping a documentonto the language(s) it is written in.
The best-known technique classifies documents accordingto rank order statistics over character n-gram se-quences between a document and a global lan-guage profile (Cavnar and Trenkle, 1994).
Otherstatistical approaches applied to LangID includeMarkov models over n-gram frequency profiles(Dunning, 1994), dot products of word frequencyvectors (Darnashek, 1995), and string kernelsin support vector machines (Kruengkrai et al.,2005).
In contrast to purely statistical meth-ods, linguistically-motivated models for LangIDhave also been proposed, such as the use of stopword lists (Johnson, 1993), where a document isclassified according to its degree of overlap withlists for different languages.
Other approachesinclude word and part-of-speech (POS) corre-lation (Grefenstette, 1995), cross-language tok-enization (Giguet, 1995) and grammatical-classmodels (Dueire Lins and Gonc?alves, 2004).LangID of short strings has attracted recentinterest from the research community.
Ham-marstrom (2007) describes a method that aug-ments a dictionary with an affix table, and testsit over synthetic data derived from a parallel biblecorpus.
Ceylan and Kim (2009) compare a num-ber of methods for identifying the language ofsearch engine queries of 2 to 3 words.
They de-velop a method which uses a decision tree to in-tegrate outputs from several different LangID ap-proaches.
Vatanen et al.
(2010) focus on mes-sages of 5?21 characters, using n-gram languagemodels over data drawn from UDHR in a naiveBayes classifier.
Carter et al.
(2013) focus specifi-cally on LangID in Twitter messages by augment-18ing standard methods with LangID priors basedon a user?s previous messages and the contentof links embedded in messages, and this is alsothe method used in TwitIE (Bontcheva et al.,2013).
Tromp and Pechenizkiy (2011) presenta method for LangID of short text messages bymeans of a graph structure, extending the stan-dard ?bag?
model of text to include informationabout the relative order of tokens.
Bergsma etal.
(2012) examine LangID for creating language-specific twitter collections, finding that a compres-sive method trained over out-of-domain data fromWikipedia and standard text corpora performedbetter than the off-the-shelf language identifiersthey tested.
Goldszmidt et al.
(2013) proposea method based on rank-order statistics, using abootstrapping process to acquire in-domain train-ing data from unlabeled Twitter messages.
Recentwork has also put some emphasis on word-levelrather than document-level LangID (Yamaguchiand Tanaka-Ishii, 2012; King and Abney, 2013),including research on identifying the language ofeach word in multilingual online communications(Nguyen and Dogruoz, 2013; Ling et al., 2013).In this paper, we focus on monolingual messages,as despite being simpler, LangID of monolingualTwitter messages is far from solved.In Section 1, we discussed some work to dateon LangID on Twitter data.
Some authors have re-leased accompanying datasets: the dataset used byTromp and Pechenizkiy (2011) was made avail-able in its entirety, consisting of 9066 messagesin 6 Western European languages.
Other au-thors have released message identifiers with as-sociated language labels, including Carter et al.
(2013), with 5000 identifiers in 5 Western Euro-pean languages, and Bergsma et al.
(2012), pro-viding 13190 identifiers across 9 languages from3 language families (Arabic, Cyrillic and Devana-gari).
To date, only the dataset of Tromp andPechenizkiy (2011) has been used by other re-searchers (Goldszmidt et al., 2013).
With the kindco-operation of the authors, we have obtained thefull datasets of Carter et al.
(2013) and Bergsmaet al.
(2012), allowing us to present the most ex-tensive empirical evaluation of LangID of Twittermessages to date.
However, the total set of lan-guages covered is still very small.
In Section 2.1,we present our own manually-annotated dataset,adding Chinese (zh) and Japanese (ja) to the lan-guages that have manually-annotated data.English Chinese JapaneseInitial 0.906 0.773 0.989Post-review 0.930 0.916 0.998Table 1: Inter-annotator agreement measured us-ing Fleiss?
kappa (Fleiss, 1971) over annotationsfor TWITTER.2.1 Manual annotation of ZHENJAA manual approach to constructing a LangIDdataset from Twitter data is difficult due to thewide variety of languages present on Twitter ?Bergsma et al.
(2012) report observing 65 lan-guages in a 10M message sample, and Baldwinet al.
(2013) report observing 97 languages in a1M message sample.
While this is encouragingin terms of sourcing data for lower-density lan-guages, the distribution of languages is Zipfian,and the relative proportion of data in most lan-guages is very small.
Manually retrieving all avail-able messages in a language would require a na-tive speaker to view and reject a huge numberof messages in other languages in order to col-lect the small number that are written in the tar-get language.
We initially attempted this, build-ing ZHENJA, a dataset derived from a set of 5000messages randomly sampled from a larger bodyof 622192 messages collected from the Twitterstreaming API over a single 24-hour period in Au-gust 2010.
The messages are a 1% representativesample of the total public messages posted on thatday.
Each of the 5000 selected messages was an-notated by speakers of three languages, English,Japanese and Mandarin Chinese.
For each mes-sage, three annotators were asked if the messagecontained any text in languages which they spoke,as well as if it appeared to contain text in (unspeci-fied) languages which they did not speak.
The lat-ter label was introduced in order to make a distinc-tion between text in languages not spoken by ourannotators (e.g.
Portuguese) and text with no lin-guistic content (e.g.
URLs).
After the initial anno-tation, annotators were asked to review messageswhere there was disagreement, and messages wereassigned labels given by a majority of annotatorspost-review.
Inter-annotator agreement (Table 1)is strong for the task: only 20 out of 5000 mes-sages have less than 80% majority in annotations.In many instances, the disagreement was due tomessages consisting entirely of a short sequenceof hanzi/kanji, which both Chinese and Japanesespeakers recognized as valid (these messages are19excluded from our set of labeled messages).
Outof the 5000 messages, 1953 (39.1%) were labeledas English, 16 were labeled as Chinese (0.3%) and1047 were labeled as Japanese (20.9%), for a totalof 3016 labeled messages.A total of 8 annotators each invested 2?4 hoursin this annotation task, and the final dataset onlycovers 3 languages (which includes the top-2highest-density languages in Twitter).
Obviously,constructing a dataset of language-labeled Twit-ter messages is a labor-intensive process, and thelower density the language, the more expensiveour methodology becomes (as more and more doc-uments need to be looked over to find documentsin the language of interest).
Ideally, we would liketo be able to use some form of automated LangIDto accelerate the process without biasing the datatowards easy-to-classify messages.2.2 A broad-coverage Twitter corpusBased on our discussion so far, our desiderata for aLangID dataset of Twitter messages are as follows:(1) achieve broader coverage of languages than ex-isting datasets; (2) minimize manual annotation;and (3) avoid bias induced by selecting messagesusing LangID.
(2) and (3) may seem to be con-flicting objectives, but we sidestep the problem byfirst identifying monolingual users, then produce adataset by sampling messages by these users froma held-out collection.The overall workflow for constructing a datasetis summarized in Algorithm 1.
For each user weconsider, we divide all their messages into two dis-joint sets.
One set (Mmainu) is used to determinethe language(s) spoken by the user.
If only onelanguage is detected, the user is added to a poolof candidate users (Uaccept).
A fixed number ofusers is sampled for each language (Usample), andfor each sampled user a fixed number of messagesis sampled from the held-out set (Mheldoutu) andadded to the final dataset.
We sample a fixed num-ber of users per language to limit the amount ofdata in the more-frequent languages, and we onlysample a small number of messages per user inorder to avoid biasing the dataset towards the lin-guistic idiosyncrasies of any specific individual.For both sampling steps, if the number of itemsavailable is less than the number required, all theavailable items are returned.Algorithm 1 uses automated LangID to detectthe language of messages in Mmainu(line 8).
TheAlgorithm 1 Procedure for building a TwitterLangID dataset1: U ?
active users2: Laccept,Maccept, Uaccept?
{}, {}, {}3: for each u ?
U do4: Mu?
all messages by user u5: Mmainu,Mheldoutu?
RandomSplit(Mu)6: Lu?
{}7: for each m ?Mmainudo8: lu?
LangID(m)9: if lu6= unknown then10: Lu?
Lu?
{lu}11: end if12: end for13: if len(Lu) = 1 then14: Uaccept?
Uaccept?
{(u, Lu)}15: Laccept?
Laccept?
Lu16: end if17: end for18: for each l ?
Lacceptdo19: Usample?
Sample(Uacceptl,K)20: for each u ?
Usampledo21: Msample?
Sample(Mheldoutu, N)22: Maccept?Maccept?
{(Msample, l)}23: end for24: end for25: return Macceptaccuracy of this identifier is not critical, as anymisclassifications for a monolingual user wouldcause them to be rejected, as they would appearmultilingual.
Hence, the risk of false positives atthe user-level LangID is very low.
However, in-correctly rejecting users reduces the pool of dataavailable for sampling, so a higher-accuracy solu-tion is preferable.
We compared the performanceof 8 off-the-shelf (i.e.
pre-trained) LangID systemsto determine which would be the most suitable forthis role.langid.py (Lui and Baldwin, 2012): an n-gram feature set selected using data from multi-ple sources, combined with a multinomial naiveBayes classifier.CLD2 (McCandless, 2010): the language iden-tifier embedded in the Chrome web browser;2ituses a naive Bayes classifier and script-specific to-kenization strategies.LangDetect (Nakatani, 2010): a naive Bayesclassifier, using a character n-gram based repre-sentation without feature selection, with a set ofnormalization heuristics to improve accuracy.LDIG (Nakatani, 2012): a Twitter-specificLangID tool, which uses a document representa-tion based on tries, combined with normalization2http://www.google.com/chrome20heuristics and Bayesian classification, trained onTwitter data.whatlang (Brown, 2013): a vector-spacemodel with per-feature weighting over charactern-grams.YALI (Majli?s, 2012): computes a per-languagescore using the relative frequency of a set of byten-grams selected by term frequency.TextCat (Scheelen, 2003); an implementationof Cavnar and Trenkle (1994), which uses an ad-hoc rank-order statistic over character n-grams.MSR-LID (Goldszmidt et al., 2013): based onrank-order statistics over character n-grams, andSpearman?s ?
to measure correlation.
Twitter-specific training data is acquired through a boot-strapping approach.
We use the 49-languagemodel provided by the authors, and the best pa-rameters reported in the paper.We investigated the performance of the systemsusing manually-labeled datasets of Twitter mes-sages (Table 2), including the ZHENJA set de-scribed in Section 2.1.3We find that all the sys-tems tested perform well on TROMP, with theexception of TextCat.
CARTER covers a verysimilar set of languages to TROMP, yet all sys-tems consistently perform worse on it.
This sug-gests that TROMP is biased towards messages thatLangID systems are likely to identify correctly(also observed by Goldszmidt et al.
(2013)).
Thisis due in part to the post-processing applied to themessages, but also suggests a bias in how mes-sages were selected.
LDIG is the best performeron TROMP and CARTER, albeit falling slightlyshort of the 99.1% accuracy reported by the author(Nakatani, 2012).
However, it is only trained on17 languages and thus is not able to fully supportBERGSMA and ZHENJA, and so we cannot drawany conclusions on whether the method will gen-eralize well to more languages.
The system thatsupports the most languages by far is whatlang,but as a result its accuracy on Twitter messagessuffers.
Manual analysis suggests this is due toTwitter-specific ?noise?
tipping the model in fa-vor of lower-density languages.
On BERGSMA,LangDetect is the best performer, likely dueto its specific heuristics for distinguishing certainlanguage pairs (Nakatani, 2010), which happen tobe present in the BERGSMA dataset.
Overall, in3We do not limit the comparison to languages supportedby each system as this would bias evaluation towards systemsthat support few languages that are easy to discriminate.their off-the-shelf configuration, only three sys-tems (langid.py, CLD2, LangDetect) per-form consistently well on LangID of Twitter mes-sages.
Even so, the macro-averaged F-Scores ob-served were as low as 83%, indicating that whilstperformance is good, the problem of LangID ofTwitter messages is far from solved.Given that the set of languages covered and ac-curacy varies between systems, we investigated asimple voting-based approach to combining thepredictions.
For each dataset, we considered allcombinations of 3, 5, and 7 systems, combin-ing the predictions using a simple majority vote.The single-best combination for each dataset is re-ported in Table 3.
In all cases, the macro-averagedF-score is improved upon, showing the effective-ness of the voting approach.
Hence, for purposesof LangID in Algorithm 1, we chose to use amajority-vote ensemble of langid.py, CLD2and LangDetect, a combination that generallyperforms well on all datasets.4Where all 3 sys-tems disagree, the message is labeled as unknown,which does not count as a separate language fordetermining if a user is multilingual, mitigatingthe risk of wrongly rejecting a monolingual userdue to misclassifying a particular message.
Thisensemble is hereafter referred to as VOTING.To build our final dataset, we collected all mes-sages by active users from the 1% feed made avail-able by Twitter over the course of 31 days, be-tween 8 January 2012 and 7 February 2012.
Wedeemed users active if they had posted at least5 messages in a single day on at least 7 differ-ent days in the 31-day period we collected datafor.
This gave us a set of approximately 2Musers.
For each user, we partitioned their mes-sages (RandomSplit in Algorithm 1) by selectingone day at random.
All of the messages postedby the user on this day were treated as heldoutdata (Mheldoutu), and the remainder of the user?smessages (Mmainu) were used to determine thelanguage(s) spoken by the user.
The day cho-sen was randomly selected per-user to avoid anybias that may be introduced by messages froma particular day or date.
Of the active users,we identified 85.0% to be monolingual, cover-ing a set of 65 languages.
50.6% of these usersspoke English (en), 14.1% spoke Japanese (ja),and 13.0% spoke Portuguese (pt); this user-level4MSR-LID was excluded due to technical difficulties inapplying it to a large collection of messages because of itsoversized model.21Dataset langid.py CLD2 LangDetect LDIG whatlang YALI TextCat MSR-LIDTROMP 0.983 0.972 0.959 0.986 0.950 0.911 0.814 0.983CARTER 0.917 0.902 0.891 0.943 0.834 0.824 0.510 0.927BERGSMA 0.847 0.911 0.923 0.000 0.719 0.428 0.046 0.546ZHENJA 0.871 0.884 0.831 0.315 0.622 0.877 0.313 0.848Table 2: Macro-averaged F-Score on manually-annotated Twitter datasets.
Italics denotes results wherethe dataset contains languages not supported by the identifier.DatasetSingle Best Voting 3-SystemSystem F-Score Systems F-Score F-ScoreTROMP LDIG 0.986 CLD2, MSR-LID, LDIG 0.992 0.986CARTER LDIG 0.943 MSR-LID, langid.py, LDIG 0.948 0.927BERGSMA LangDetect 0.923 CLD2, LangDetect, langid.py 0.935 0.935ZHENJA CLD2 0.884 CLD2, MSR-LID, LDIG, YALI, langid.py 0.969 0.941Table 3: System combination by majority voting.
All combinations of 3, 5 and 7 systems were con-sidered.
For each dataset, we report the single-best system, the best combination, and F-score of themajority-vote combination of langid.py, CLD2 and LangDetect.language distribution largely mirrors the message-level language distribution reported by Baldwin etal.
(2013) and others.
From this set of users, werandomly selected up to 100 users per language,leaving us with a pool of 26011 held-out mes-sages from 2914 users.
Manual inspection of thesemessages revealed a number of English messagesmislabeled with another language, indicating thateven predominantly monolingual users occasion-ally introduce English into their online commu-nications.
Such messages are generally entirelyEnglish, with code-switching (i.e.
multiple lan-guages in the same message) very rarely observed.In order to eliminate mislabeled messages, we ap-plied all 8 systems to this pool of 26011 messages.Where at least 5 systems agree and the predictedlanguage does not match the user?s language, wediscarded the message.
Where 3 or 4 systemsagree, we manually inspected the messages andeliminated those that were clearly mislabeled (thisis the only manual step in the construction of thisdataset).
Overall, we retained 24220 messages(93.1%).
From these, we sampled up to 5 mes-sages per unique user, producing a final dataset of14178 messages across 65 languages (hereafter re-ferred to as the TWITUSER dataset).3 Evaluating off-the-shelf languageidentifiers on TwitterGiven TWITUSER, our broad-coverage Twittercorpus, we return to the task of examining theperformance of the off-the-shelf LangID systemswe discussed in Section 2.2 (Table 4, left side).In terms of macro-averaged F-Score across thefull set of 65 languages, CLD2 is the single best-performing system.
Unlike langid.py andLangDetect, CLD2 does not always produce aprediction, and instead has an in-built thresholdfor it to output a prediction of ?unknown?.
Thisis reflected in the elevated precision, at the ex-pense of decreased recall and message-level ac-curacy.
Systems like langid.py which alwaysmake a prediction have reduced precision, bal-anced by increased recall and message-level ac-curacy.
As with the manually-annotated datasets,we experimented with a simple voting-based ap-proach to combining multiple classifiers.
Weagain experimented with all possible combina-tions of 3, 5 and 7 classifiers, and found that onTWITUSER, a majority-vote ensemble of CLD2,langid.py and LangDetect attains the bestmacro-averaged F-Score, and also outperformsany individual system on all of the metrics con-sidered.
We note that this is exactly the VOTINGensemble of Section 2.2, validating its choice asLangID(m) in Algorithm 1.3.1 Adapting off-the-shelf LangID to TwitterTromp and Pechenizkiy (2011) propose to removelinks, usernames, hashtags and smilies before at-tempting LangID, as they are Twitter specific.
Weexperimented with applying this cleaning proce-dure to each message body before passing it toour off-the-shelf systems (Table 4, right side).
ForLDIG and MSR-LID, the results are exactly thesame with and without cleaning.
These two sys-tems are specifically targeted at Twitter messages,and thus may include a similar normalization aspart of their processing pipeline.
This also sug-gests that the systems do not leverage this Twitter-22ToolWithout Cleaning With CleaningP R F Acc P R F Acclangid.py 0.767 0.861 0.770 0.842 0.759 0.861 0.766 0.840CLD2 0.852 0.814 0.806 0.775 0.866 0.823 0.820 0.780LangDetect 0.618 0.680 0.626 0.839 0.623 0.687 0.634 0.854LDIG 0.167 0.239 0.189 0.447 0.167 0.239 0.189 0.447whatlang 0.749 0.655 0.663 0.624 0.739 0.667 0.663 0.623YALI 0.441 0.564 0.438 0.710 0.449 0.560 0.443 0.705TextCat 0.327 0.245 0.197 0.257 0.316 0.295 0.230 0.316MSR-LID 0.533 0.609 0.536 0.848 0.533 0.609 0.536 0.848VOTING 0.920 0.876 0.887 0.861 0.919 0.883 0.889 0.868Table 4: Macro-averaged Precision/Recall/F-Score, as well as message-level accuracy for each systemon TWITUSER.
The right side of the table reports results after applying message-level cleaning (Trompand Pechenizkiy, 2011).specific content in making predictions.
Other sys-tems generally show a small improvement withcleaning, except for langid.py.
The VOTINGensemble also benefits from cleaning, due to theimprovement in two of its component classifiers(CLD2 and LangDetect).
This cleaning pro-cedure is trivial to implement, so despite the im-provement being small, it may be worth imple-menting if adapting off-the-shelf language identi-fiers to Twitter messages.Goldszmidt et al.
(2013) suggest bootstrap-ping a Twitter-specific language identifier usingan off-the-shelf language identifier and an unla-beled collection of Twitter messages.
We testedthis approach, using the 3 systems that providetools to generate new models from labeled data(LangDetect, langid.py and TextCat).We constructed bootstrap collections by: (1) us-ing the off-the-shelf tools to directly identifythe language of messages; and (2) using Algo-rithm 1.
Overall, the bootstrapped identifiers arenot better than their off-the-shelf counterparts.For TextCat there is an increase in accuracyusing bootstrapped models, but the accuracy ofTextCat with bootstrapped models is still infe-rior to LangDetect and langid.py in theiroff-the-shelf configuration.
For LangDetect,utilizing bootstrapped models does not always in-crease the accuracy of LangID of Twitter mes-sages.
Where it does help, the bootstrap collec-tions that are effective vary with the target dataset.For langid.py, none of the bootstrapped mod-els outperformed the off-the-shelf model.
Thissuggests that for LangID, the same features thatare predictive of language in other domains areequally applicable to Twitter messages, and thatthe cross-domain feature selection procedure pro-posed utilized by langid.py (Lui and Baldwin,Dataset Period ProportionCARTER Jan ?
Apr 2010 76.4%BERGSMA May 2007 ?
Feb 2012 92.2%TWITUSER Jan ?
Feb 2012 79.7%Table 5: Proportion of messages from each datasetthat were still accessible as of August 2013.2011) is able to identify these features effectively.Bontcheva et al.
(2013) report positive resultsfrom the integration of LangID priors (Carter etal., 2013), but we did not experiment with them,as the calculation of priors is relatively expensivecompared to the other adaptations we have con-sidered, in terms of both run time and developereffort.
Furthermore, there is a number of open is-sues that are likely to affect the effectiveness of thepriors, such as the size and the scope of the mes-sage collection used to determine the prior.
Thisis an interesting avenue of future work but is be-yond the scope of this particular paper.
However,we observe that priors based on user identity (e.gthe ?Blogger?
prior) are likely to be artificially ef-fective on TWITUSER, because the messages havebeen sampled from users that we have identifiedas monolingual.3.2 Twitter API predictionsFor CARTER, BERGSMA and TWITUSER, wehave access to the original identifiers for each mes-sage, which use used to download the messagesvia the Twitter API.5Table 5 reports the propor-tion of each dataset that is still accessible as ofAugust 2013.
For the messages that we were ableto recover, the full response from the API nowincludes language predictions.
We do not reportquantitative results on the accuracy of the TwitterAPI predictions as the Twitter API terms of ser-5http://dev.twitter.com23vice forbid benchmarking (?You will not attempt... to ... use or access the Twitter API ... for ...benchmarking or competitive purposes?).
Further-more, any results would be impossible to replicate:the set of messages that are accessible is likely tocontinue to decrease, and the accuracy of Twitter?spredictions may vary as updates are made to theAPI.Error analysis of the language predictions pro-vided by the Twitter API shows that at the timeof writing, for the languages supported the accu-racy of the Twitter API is not substantially betterthan the best off-the-shelf language identifiers weexamined in this paper.
However, about a quarterof the languages present in TWITUSER are neveroffered as predictions.
This has implications forthe precision of LangID in other languages: onenotable example is poor precision in Italian, dueto some Romanian messages being identified asItalian (no messages are identified as Romanian).This suggests that caution must be taken in tak-ing the language predictions offered by the Twit-ter API as goldstandard.
The accuracy of the pre-dictions is not perfect, and highlights the need forfurther research into improving the scope and ac-curacy of LangID for Twitter messages.4 ConclusionIn this paper, we presented ZHENJA and TWIT-USER, two novel datasets of language-labeledTwitter messages.
ZHENJA is constructed us-ing a conventional manual annotation approach,whereas TWITUSER is constructed using a novelmostly-automated method that leverages useridentity.
Using these new datasets alongsidethree previously-published datasets, we com-pared 8 off-the-shelf LangID systems over Twit-ter messages, and found that a simple major-ity vote across three specific systems (CLD2,langid.py, LangDetect) consistently out-performs any individual system.
We also foundthat removing Twitter-specific content from mes-sages improves the performance of off-the-shelfsystems.
We reported that the predictions providedby the Twitter API are not better than state-of-the-art off-the-shelf systems, and that a number of lan-guages in use on Twitter appear to be unsupportedby the Twitter API, underscoring the need for fur-ther research to broaden the scope and accuracy oflanguage identification from Twitter messages.AcknowledgmentsNICTA is funded by the Australian Governmentas represented by the Department of Broadband,Communications and the Digital Economy andthe Australian Research Council through the ICTCentre of Excellence program.ReferencesTimothy Baldwin and Marco Lui.
2010.
Language identifi-cation: The long and the short of the matter.
In Proceed-ings of Human Language Technologies: The 11th AnnualConference of the North American Chapter of the Associ-ation for Computational Linguistics (NAACL HLT 2010),pages 229?237, Los Angeles, USA.Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKin-lay, and Li Wang.
2013.
How noisy social media text,how diffrnt social media sources?
In Proceedings of the6th International Joint Conference on Natural LanguageProcessing (IJCNLP 2013), Nagoya, Japan.Shane Bergsma, Paul McNamee, Mossaab Bagdouri, ClaytonFink, and Theresa Wilson.
2012.
Language identifica-tion for creating language-specific Twitter collections.
InProceedings the Second Workshop on Language in SocialMedia (LSM2012), pages 65?74, Montr?eal, Canada.Kalina Bontcheva, Leon Derczynski, Adam Funk, Mark A.Greenwood, Diana Maynard, and Niraj Aswani.
2013.TwitIE: An open-source information extraction pipelinefor microblog text.
In Proceedings of Recent Advancesin Natural Language Processing (RANLP 2013), Hissar,Buglaria.Ralf Brown.
2013.
Selecting and weighting n-grams toidentify 1100 languages.
In Proceedings of the 16th in-ternational conference on text, speech and dialogue (TSD2013), Plze?n, Czech Republic.Simon Carter, Wouter Weerkamp, and Manos Tsagkias.2013.
Microblog language identification: Overcomingthe limitations of short, unedited and idiomatic text.
Lan-guage Resources and Evaluation, pages 1?21.William B. Cavnar and John M. Trenkle.
1994.
N-gram-based text categorization.
In Proceedings of the ThirdSymposium on Document Analysis and Information Re-trieval, pages 161?175, Las Vegas, USA.Hakan Ceylan and Yookyung Kim.
2009.
Language iden-tification of search engine queries.
In Proceedings of theJoint Conference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP, pages 1066?1074,Singapore.Aron Culotta.
2010.
Towards detecting influenza epidemicsby analyzing Twitter messages.
In Proceedings of theKDD Workshop on Social Media Analytics.Marc Darnashek.
1995.
Gauging similarity with n-grams:Language-independent categorization of text.
Science,267:843?848.Rafael Dueire Lins and Paulo Gonc?alves.
2004.
Automaticlanguage identification of written texts.
In Proceedings ofthe 2004 ACM Symposium on Applied Computing (SAC2004), pages 1128?1133, Nicosia, Cyprus.Ted Dunning.
1994.
Statistical identification of language.Technical Report MCCS 940-273, Computing ResearchLaboratory, New Mexico State University.24Joseph L. Fleiss.
1971.
Measuring nominal scale agreementamong many raters.
Psychological Bulletin, 76(5):378?382.Emmanuel Giguet.
1995.
Categorisation according to lan-guage: A step toward combining linguistic knowledge andstatistical learning.
In Proceedings of the 4th Interna-tional Workshop on Parsing Technologies (IWPT-1995),Prague, Czech Republic.Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Di-panjan Das, Daniel Mills, Jacob Eisenstein, Michael Heil-man, Dani Yogatama, Jeffrey Flanigan, and Noah A.Smith.
2011.
Part-of-speech tagging for Twitter: An-notation, features, and experiments.
In Proceedings ofthe 49th Annual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies (ACLHLT 2011), pages 42?47, Portland, USA.Moises Goldszmidt, Marc Najork, and Stelios Paparizos.2013.
Boot-strapping language identifiers for short col-loquial postings.
In Proceedings of the European Confer-ence on Machine Learning and Principles and Practice ofKnowledge Discovery in Databases (ECMLPKDD 2013),Prague, Czech Republic.Gregory Grefenstette.
1995.
Comparing two language iden-tification schemes.
In Proceedings of Analisi Statistica deiDati Testuali (JADT), pages 263?268, Rome, Italy.Harald Hammarstrom.
2007.
A Fine-Grained Model forLanguage Identication.
In Proceedings of Improving NonEnglish Web Searching (iNEWS07), pages 14?20.Bo Han, Paul Cook, and Timothy Baldwin.
2013.
Lexicalnormalization for social media text.
ACM Trans.
Intell.Syst.
Technol., 4(1):5:1?5:27, February.Stephen Johnson.
1993.
Solving the problem of languagerecognition.
Technical report, School of Computer Stud-ies, University of Leeds.Ben King and Steven Abney.
2013.
Labeling the languagesof words in mixed-language documents using weakly su-pervised methods.
In Proceedings of the 2013 Confer-ence of the North American Chapter of the Associationfor Computational Linguistics: Human Language Tech-nologies, pages 1110?1119, Atlanta, Georgia.Balachander Krishnamurthy, Phillipa Gill, and Martin Arlitt.2008.
A few chirps about Twitter.
In Proceedings of theFirst Workshop on Online Social Networks (WOSN 2008),pages 19?24, Seattle, USA.Canasai Kruengkrai, Prapass Srichaivattana, Virach Sornlert-lamvanich, and Hitoshi Isahara.
2005.
Language identifi-cation based on string kernels.
In Proceedings of the 5thInternational Symposium on Communications and Infor-mation Technologies (ISCIT-2005), pages 896?899, Bei-jing, China.Vasileios Lampos, Tijl De Bie, and Nello Cristianini.
2010.Flu Detector ?
tracking epidemics on Twitter.
In Pro-ceedings of the European Conference on Machine Learn-ing and Principles and Practice of Knowledge Discov-ery in Databases (ECML PKDD 2010), pages 599?602,Barcelona, Spain.Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and Is-abel Trancoso.
2013.
Microblogs as parallel corpora.
InProceedings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers),pages 176?186, Sofia, Bulgaria.Marco Lui and Timothy Baldwin.
2011.
Cross-domain fea-ture selection for language identification.
In Proceedingsof the 5th International Joint Conference on Natural Lan-guage Processing (IJCNLP 2011), pages 553?561, ChiangMai, Thailand.Marco Lui and Timothy Baldwin.
2012. langid.py: An off-the-shelf language identification tool.
In Proceedings ofthe 50th Annual Meeting of the Association for Computa-tional Linguistics (ACL 2012) Demo Session, pages 25?30, Jeju, Republic of Korea.Martin Majli?s.
2012.
Yet another language identifier.
InProceedings of the Student Research Workshop at the 13thConference of the European Chapter of the Associationfor Computational Linguistics, pages 46?54, Avignon,France.Michael McCandless.
2010.
Accuracy and performanceof google?s compact language detector.
blog post.
avail-able at http://blog.mikemccandless.com/2011/10/accuracy-and-performance-of-googles.html.Shuyo Nakatani.
2010.
Language detection library(slides).
http://www.slideshare.net/shuyo/language-detection-library-for-java.Retrieved on 21/06/2013.Shuyo Nakatani.
2012.
Short text language detec-tion with infinity-gram.
blog post.
available athttp://shuyo.wordpress.com/2012/05/17/short-text-language-detection-with-infinity-gram/.Dong Nguyen and A. Seza Dogruoz.
2013.
Word levellanguage identification in online multilingual communi-cation.
In Proceedings of the 2013 Conference on Em-pirical Methods in Natural Language Processing, pages857?862, Seattle, USA.S.
Petrovi?c, Miles Osborne, and Victor Lavrenko.
2010.Streaming first story detection with application to twitter.In Proceedings of Human Language Technologies: The11th Annual Conference of the North American Chapterof the Association for Computational Linguistics (NAACLHLT 2010), pages 181?189, Los Angeles, USA.Arne Roomann-Kurrik.
2013.
Introducing new meta-data fro tweets.
blog post.
available at https://dev.twitter.com/blog/introducing-new-metadata-for-tweets.Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010.Earthquake shakes Twitter users: real-time event detectionby social sensors.
In Proceedings of the 19th InternationalConference on the World Wide Web (WWW 2010), pages851?860, Raleigh, USA.Frank Scheelen, 2003. libtextcat.
Software avail-able at http://software.wise-guys.nl/libtextcat/.Shiladitya Sinha, Chris Dyer, Kevin Gimpel, and Noah A.Smith.
2013.
Predicting the NFL using Twitter.
InProceedings of the ECML/PKDD Workshop on MachineLearning and Data Mining for Sports Analytics, Prague,Czech Republic.Erik Tromp and Mykola Pechenizkiy.
2011.
Graph-basedn-gram language identification on short texts.
In Proceed-ings of Benelearn 2011, pages 27?35, The Hague, Nether-lands.Tommi Vatanen, Jaakko J. Vayrynen, and Sami Virpioja.2010.
Language identification of short text segmentswith n-gram models.
In Proceedings of the 7th Interna-tional Conference on Language Resources and Evaluation(LREC 2010), pages 3423?3430.Hiroshi Yamaguchi and Kumiko Tanaka-Ishii.
2012.
Textsegmentation by language using minimum descriptionlength.
In Proceedings the 50th Annual Meeting of the As-sociation for Computational Linguistics (Volume 1: LongPapers), pages 969?978, Jeju Island, Korea.25
