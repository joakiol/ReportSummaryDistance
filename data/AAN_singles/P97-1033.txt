Intonational Boundaries,  Speech Repairs andDiscourse Markers: Model ing Spoken DialogPeter A. Heeman and James F. AllenDepartment of Computer ScienceUniversity 9f RochesterRochester NY 14627, USA{heeman,  j ames }~cs .
rochester ,  eduAbst rac tTo understand a speaker's turn of a con-versation, one needs to segment it into in-tonational phrases, clean up any speech re-pairs that might have occurred, and iden-tify discourse markers.
In this paper, weargue that these problems must be resolvedtogether, and that they must be resolvedearly in the processing stream.
We put for-ward a statistical language model that re-solves these problems, does POS tagging,and can be used as the language model ofa speech recognizer.
We find that by ac-counting for the interactions between thesetasks that the performance on each taskimproves, as does POS tagging and per-plexity.1 I n t roduct ionInteractive spoken dialog provides many new chal-lenges for natural anguage understanding systems.One of the most critical challenges is simply de-termining the speaker's intended utterances: bothsegmenting the speaker's turn into utterances anddetermining the intended words in each utterance.Since there is no well-agreed to definition of whatan utterance is, we instead focus on intonationalphrases (Silverman et al, 1992), which end with anacoustically signaled boundary lone.
Even assumingperfect word recognition, the problem of determin-ing the intended words is complicated ue to theoccurrence of speech repairs, which occur where thespeaker goes back and changes (or repeats) some-thing she just said.
The words that are replacedor repeated are no longer part of the intended ut-terance, and so need to be identified.
The follow-ing example, from the Trains corpus (Heeman andAllen, 1995), gives an example of a speech repairwith the words that the speaker intends to be re-placed marked by reparandum, the words that arethe intended replacement marked as alteration, andthe cue phrases and filled pauses that tend to occurin between marked as the editing term.Example  1 (d92a-5.2 ut t34)we'll pick up ~ .
uh the tanker of orangesreparandu "q'ml ~ ?
~ ?
editing term alterationinterruption pointMuch work has been done on both detect-ing boundary tones (e.g.
(Wang and Hirschberg,1992; Wightman and Ostendorf, 1994; Stolcke andShriberg, 1996a; Kompe et al, 1994; Mast et al,1996)) and on speech repair detection and correction(e.g.
(Hindle, 1983; Bear, Dowding, and Shriberg,1992; Nakatani and Hirschberg, 1994; Heeman andAllen, 1994; Stolcke and Shriberg, 1996b)).
Thiswork has focused on one of the issues in isolation ofthe other.
However, these two issues are intertwined.Cues such as the presence of silence, final syllablelengthening, and presence of filled pauses tend tomark both events.
Even the presence of word cor-respondences, a tradition cue for detecting and cor-recting speech repairs, sometimes marks boundarytones as well, as illustrated by the following examplewhere the intonational phrase boundary is markedwith the ToBI symbol %.Example  2 (d93-83.3 utt73)that's all you need % you only need one boxcarIntonational phrases and speech repairs also in-teract with the identification of discourse markers.Discourse markers (Schiffrin, 1987; Hirschberg andLitman, 1993; Byron and Heeman, 1997) are usedto relate new speech to the current discourse state.Lexical items that can function as discourse mark-ers, such as "well" and "okay," are ambiguous as towhether they are being used as discourse markersor not.
The complication is that discourse markerstend to be used to introduce a new utterance, orcan be an utterance all to themselves (such as theacknowledgment "okay" or "alright"), or can be usedas part of the editing term of a speech repair, or tobegin the alteration.
Hence, the problem of identi-fying discourse markers also needs to be addressedwith the segmentation a d speech repair problems.These three phenomena ofspoken dialog, however,cannot be resolved without recourse to syntactic in-formation.
Speech repairs, for example, are often254signaled by syntactic anomalies.
Furthermore, inorder to determine the extent of the reparanduin,one needs to take into account he parallel structurethat typically exists between the reparandum and al-teration, which relies on at identifying the s:?ntacticroles, or part-of-speech (POS) tags, of the words in-volved (Bear, Dowding, and Shriberg, 1992; Heemanand Allen, 1994).
However, speech repairs disruptthe context that is needed to determine the POStags (Hindle, 1983).
Hence, speech repairs, as wellas boundary tones and discourse markers, must beresolved uring syntactic disambiguation.Of course when dealing with spoken dialogue, onecannot forget the initial problem of determining theactual words that the speaker is saying.
Speech rec-ognizers rely on being able to predict the probabil-ity of what word will be said next.
Just as intona-tional phrases and speech repairs disrupt the localcontext hat is needed for syntactic disambiguation,the same holds for predicting what word will comenext.
If a speech repair or intonational phrase oc-curs, this will alter the probability estimate.
Butmore importantly, speech repairs and intonationalphrases have acoustic correlates uch as the pres-ence of silence.
Current speech recognition languagemodels camlot account for the presence of silence,and tend to simply ignore it.
By modeling speech re-pairs and intonational boundaries, we can take intoaccount he acoustic correlates and hence use moreof the available information.From the above discussion, it is clear that we needto model these dialogue phenomena together andvery early on in the speech processing stream, infact, during speech recognition.
Currently, the ap-proaches that work best in speech recognition arestatistical approaches that are able to assign proba-bility estimates for what word will occur next giventhe previous words.
Hence, in this paper, we in-troduce a statistical anguage model that can de-tect speech repairs, boundary tones, and discoursemarkers, and can assign POS tags, and can use thisinformation to better predict what word will occurnext.In the rest of the paper, we first introduce theTrains corpus.
We then introduce a statistical lan-guage model that incorporates POS tagging and theidentification of discourse markers.
We then aug-meat this model with speech repair detection andcorrection and intonational boundary tone detec-tion.
We then present he results of this model onthe Trains corpus and show that it can better ac-count for these discourse vents than can be achievedby modeling them individually.
We also show thatby modeling these two phenomena that we can in-crease our POS tagging performance by 8.6%, andimprove our ability to predict the next word.Dialogs 98Speakers 34Words 58298Turns 6163Discourse Markers 8278Boundary Tones 10947Turn-Internal Boundary Tones 5535Abridged Repairs 423Modification Repairs 1302Fresh Starts 671Editing Terms 1128Table 1: Frequency of Tones, Repairs and EditingTerms in the Trains Corpus2 T ra ins  CorpusAs part of the TRAINS project (Allen et al, 1995),which is a long term research project to build a con-versationally proficient planning assistant, we havecollected a corpus of problem solving dialogs (Hee-man and Allen, 1995).
The dialogs involve two hu-man participants, one who is playing the role of auser and has a certain task to accomplish, and an-other who is playing the role of the system by actingas a planning assistant.
The collection methodologywas designed to make the setting as close to human-computer interaction as possible, but was not a wiz-ard scenario, where one person pretends to be a com-puter.
Rathor, the user knows that he is talking toanother person.The TaAINS corpus consists of about six and halfhours of speech.
Table 1 gives some general statisticsabout the corpus, including the number of dialogs,speakers, words, speaker turns, and occurrences ofdiscourse markers, boundary tones and speech re-pairs.The speech repairs in the Trains corpus have beenhand-annotated.
We have divided the repairs intothree types: fresh starts, modification repairs, andabridged repairs.
1 A fresh start is where the speakerabandons the current utterance and starts again,where the abandonment seems acoustically signaled.Example  3 (d93-12.1 utt30)so it'll take um so you want to do whatreparandum| editing term alterationinterruption pointThe second type of repairs are the modification re-pairs.
These include all other repairs in which thereparandum is not empty.Example 4 (d92a- l .3 utt65)so that will total will take seven hours to do thatreparandumT alterationinterruption point1This classification is similar to that of Hindle (1983)and Levelt (1983).255The third type of repairs are the abridged repairs,which consist solely of an editing term.
Note thatutterance initial filled pauses are not treated asabridged repairs.Example 5 (d93-14.3 utt42)we need to um manage to get the bananas to DansvilleT editing terminterruption pointThere is typically a correspondence betweenthe reparandum and the alteration, and followingBear et al (1992), we annotate this using the la-bels m for word matching and r for word replace-ments (words of the same syntactic ategory).
Eachpair is given a unique index.
Other words in thereparandum and alteration are annotated with anx.
Also, editing terms (filled pauses and clue words)are labeled with et, and the interruption point withip, which will occur before any editing terms asso-ciated with the repair, and after a word fragment,if present.
The interruption point is also marked asto whether the repair is a fresh start, modificationrepair, or abridged repair, in which cases, we useip:ean, ip :mod and ip:abr,  respectively.
The ex-ample below illustrates how a repair is annotated inthis scheme.Example 6 (d93-15.2 utt42)engine two from Elmi(ra)- or engine three from Elmiraml  r2 m3 m4 Tet ml r2 m3 m4ip:mod3 A POS-Based  Language Mode lThe goal of a speech recognizer is to find the se-quence of words l~ that is maximal given the acous-tic signal A.
However, for detecting and correctingspeech repairs, and identifying boundary tones anddiscourse markers, we need to augment he modelso that it incorporates shallow statistical analysis, inthe form of POS tagging.
The POS tagset, based onthe Penn Treebank tagset (Marcus, Santorini, andMarcinkiewicz, 1993), includes special tags for de-noting when a word is being used as a discoursemarker.
In this section, we give an overview of ourbasic language model that incorporates POS tag-ging.
Full details can be found in (Heeman andAllen, 1997; Heeman~ 1997).To add in POS tagging, we change the goal of thespeech recognition process to find the best word andPOS tags given the acoustic signal.
The derivationof the acoustic model and language model is now asfollows.IfVP = argmaxPr(WPIA)W,PPr(A\[WP) Pr(WP):- arg maxWP Pr(A)= argmaxPr(AIWP ) Pr(WP)WYThe first term Pr(AIWP ) is the factor due tothe acoustic model, which we can approximate byPr(A\[W).
The second term Pr(WP) is the factordue to the language model.
We rewrite Pr(WP) asPr(WI,NPI,N), where N is the number of words inthe sequence.
We now rewrite the language modelprobability as follows.Pr( W1,N P1,N )= H Pr(WiPilWl,i-lPl, i-1)i= l ,N= 1-I Pr(WiIWl,i-lPl, i) Pr(PilW~i-lPl'i-1)i= l ,NWe now have two probability distributions that weneed to estimate, which we do using decision trees(Breiman et al, 1984; Bahl et al, 1989).
The de-cision tree algorithm has the advantage that it usesinformation theoretic measures to construct equiva-lence classes of the context in order to cope withsparseness of data.
The decision tree algorithmstarts with all of the training data in a single leafnode.
For each leaf node, it looks for the questionto ask of the context such that splitting the nodeinto two leaf nodes results in the biggest decreasein impurity, where tile impurity measures how welleach leaf predicts the events in the node.
After thetree is grown, a heldout dataset is used to smooththe probabilities of each node with its parent (Bahlet al, 1989).To allow the decision tree to ask about the wordsand POS tags in the context, we cluster the wordsand POS tags using the algorithm of Brown etal.
(1992) into a binary classification tree.
This givesan implicit binary encoding for each word and POStag, thus allowing the decision tree to ask about thewords and POS tags using simple binary questions,such as 'is the third bit of the POS tag encodingequal to one?'
Figure 1 shows a POS classificationtree.
The binary encoding for a POS tag is deter-mined by the sequence of top and bottom edges thatleads from the root node to the node for the POStag.Unlike other work (e.g.
(Black et al, 1992; Mater-man, 1995)), we treat the word identities as a furtherrefinement of the POS tags; thus we build a wordclassification tree for each POS tag.
This has theadvantage of avoiding unnecessary data fragmenta-tion, since the POS tags and word identities are nolonger separate sources of information.
As well, itconstrains the task of building the word classifica-tion trees since the major distinctions are capturedby the POS classification tree.4 Augment ing  the  Mode lJust as we redefined the speech recognition prob-lem so as to account for POS tagging and identify-ing discourse markers, we do the same for modeling256Figure 1: POS Classification Treeboundary tones and speech repairs.
We introducenull tokens between each pair of consecutive wordswi-1 and wi (Heeman and Allen, 1994), which wiltbe tagged as to the occurrence of these events.
Theboundary tone tag T/ indicates if word wi-1 ends anintonational boundary (T~=T), or not (T~=null).For detecting speech repairs, we have the prob-lem that repairs are often accompanied by an edit-ing term, such as "um", "uh", "okay", or "well",and these must be identified as such.
Furthermore,an editing term might be composed of a number ofwords, such as "let's see" or "uh well".
Hence we usetwo tags: an editing term tag Ei and a repair tag Ri.The editing term tag indicates if wi starts an edit-ing term (Ei=Push), if wi continues an editing term(Ei=ET), if wi-~ ends an editing term (Ei=Pop),or otherwise (Ei=null).
The repair tag Ri indicateswhether word wi is the onset of the alteration of afresh start (Ri=C), a modification repair (Ri=M),or an abridged repair (Ri=A), or there is not a re-pair (Ri=null).
Note that for repairs with an edit-ing term, the repair is tagged after the extent of theediting term has been determined.
Below we give anexample showing all non-null tone, editing term andrepair tags.Example 7 (d93-18.1 utt47)it takes one Push you ET know Pop M two hours TIf a modification repair or fresh start occurs,we need to determine the extent (or the onset)of the reparandum, which we refer to as correct-ing the speech repair.
Often, speech repairs havestrong word correspondences between the reparan-we'll pick up a tank of uh the tanker of oranges' I ' t l  .
.
.
.Figure 2: Cross Serial Correspondencesdum and alteration, involving word matches andword replacements.
Hence, knowing the extent ofthe reparandum eans that we can use the reparan-dum to predict the words (and their POS tags) thatmake up the alteration.
For Ri E {Mod, Can}, wedefine Oi to indicate the onset of the reparandum.
2If we are in the midst of processing a repair, weneed to determine if there is a word correspondencefrom the reparandum to the current word wi.
Thetag Li is used to indicate which word in the reparan-dum is licensing the correspondence.
Word cor-respondences tend to exhibit a cross serial depen-dency; in other words if we have a correspondencebetween wj in the reparandum and wk in the alter-ation, any correspondence with a word in the alter-ation after w~ will be to a word that is after wj, as il-lustrated in Figure 2.
This means that if wi involvesa word correspondence, it will most likely be with aword that follows the last word in the reparandumthat has a word correspondence.
Hence, we restrictLI to only those words that are after the last word inthe reparandum that has a correspondence (or fromthe reparandum onset if there is not yet a correspon-dence).
If there is no word correspondence forwi, weset Li to the first word after the last correspondence.The second tag involved in the correspondences isCi, which indicates the type of correspondence b -tween the word indicated by Li and the current wordwi.
We focus on word correspondences that involveeither a word match (Ci=m), a word replacement(Ci=r), where both words are of the same POS tag,or no correspondence (Ci=x).Now that we have defined these six additional tagsfor modeling boundary tones and speech repairs, weredefine the speech recognition problem so that itsgoal is to find the maximal assignment for the wordsas well as the POS, boundary tone, and speech repairtags.WPCLORET = arg max Pr(WCLORET\[A)WPCLOI tETThe result is that we now have eight probability dis-tributions that we need to estimate.Pr (Ti I Wl,i- 1Pl,i-1Cl,i-1Ll, i-101,1-1Rl,i-i El,i-1Tl,i-1 )Pr( EilWl,i- 1Pl,i-1CI,i-1Ll,l-1 01,1-1Rl,i- 1 El,l-1Tl,i)Pr(Ri \[WI,i-1Pl, i-1 el, i -  1 .LI,I- 10l~i-1 RI,I-1 El,iTl,i )Pr (Oi \[ Wl,i-1Pl,i-1Cl,i-1Ll,i-101,1-1Rl,iEl,iTl,i)Pr(Li \[W1,,-1Pl,i-1Cl, i-1Ll, i-101,1Rl,i EI,,TI,i )2Rather than estimate Oi directly, we instead queryeach potential onset to see how likely it is to be the actualonset of the reparandum.257Pr(CiIW~,+-~ PJ,+-~ Ct,+-~ Ll,i Ol,i Rl,i El, i Zl,i )Pr( Pi l Wl,i-1PI, i-1CI,i L I,i 01,i R I,i El,i Tl,i )Pr(W, Pl,i Cl,i L l,i Ol,i Rl,i El,i Zl,i )The context for each of the probability distribu-tions includes all of the previous context.
In princi-pal, we could give all of this context to the decisiontree algorithm and let it decide what informationis relevant in constructing equivalence classes of thecontexts.
However, the amount of training data islimited (as are the learning techniques) and so weneed to encode the context in order to simplify thetask of constructing meaningflfl equivalence classes.We start with the words and their POS tags thatare in the context and for each non-null tone, editingterm (we also skip over E=ET) ,  and repair tag, weinsert it into the appropriate place, just as Kompe etal.
(1994) do for boundary tones in their languagemodel.
Below we give the encoded context for theword "know" from Example 7Example  8 (d93-18.1 u t t47)i t /PRP  takes/VBP one/CD Push you/PRPThe result of this is that the non-null tag values aretreated just as if they were lexical items.
3 Further-more, if an editing term is completed, or the extentof a repair is known, we can also clean up the edit-ing term or reparandum, respectively, in the sameway that Stolcke and Shriberg (1996b) clean up filledpauses, and simple repair patterns.
This means thatwe can then generalize between fluent speech andinstances that have a repair.
For instance, in thetwo examples below, the context for the word "get"and its POS tag will be the same for both, namely"so/CC_D we/PRP  need/VBP to /TO" .Example  9 (d93-11.1 u t t46)so we need to get the three tankersExample  10 (d92a-2.2  u t t6 )so we need to Push um Pop A get a tanker of OJWe also include other features of the context.
Forinstance, we include a variable to indicate if we arecurrently processing an editing term, and whethera non-filled pause editing term was seen.
For es-timating Ri, we include the editing terms as well.For estimating Oi, we include whether the proposedreparandum includes discourse markers, filled pausesthat are not part of an editing term, boundary terms,and whether the proposed reparandum overlaps withany previous repair.5 Si lencesSilence, as well as other acoustic information, canalso give evidence as to whether an intonationalphrase, speech repair, or editing term occurred.
We3Since we treat the non-null tags as lexical items, weassociate a unique POS tag with each value., , , , , ,Fluant - -Tone .
.
.
.Modif ication .
.
.
.Fresh Starl .........Push .
.
.
.
.
.- .
.
.
.
.
.
.
.
.
.
.
.
.
.
, Pop  .
.
.
./ ' \  , .
_ .
.
.
,  ,,,, ...... ".+,,, .
.
.
.
.
: .#'%-.
:,..<+-.< .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t '"  / -  ..........
'.,it}',."
'...." ".. '~ ................................ -::~ .L_ ,  " ............................................ : ..... _ .....0.5 1 1.5 2 2.5 3 3.5Figure 3: Preference for tone, editing term, and re-pair tags given the length of silenceinclude Si, the silence duration between word wi-1and wi, as part of the context for conditioning theprobability distributions for the tone T/, editingterm El, and repair Ri tags.
Due to sparseness ofdata, we make several the independence assumptionsso that we can separate the silence information fromthe rest of the context.
For example, for the tonetag, let Resti represent he rest of the context thatis used to condition T/.
By assuming that Resti andSi are independent, and are independent given T/,we can rewrite Pr(T i IS iRest i )  as follows.. Pr(2qlSi-1)Pr(T~lS~Rest~) = Pr(f i lResh)Pr (T ,  IS ,  ) We can now use P,-(T,) as a factor to modify thetone probability in order to take into account thesilence duration.
In Figure 3, we give the factorsby which we adjust the tag probabilities given theamount of silence.
Again, due to sparse of data,we collapse the values of the tone, editing term andrepair tag into six classes: boundary tones, editingterm pushes, editing term pops, modification repairsand fresh starts (without an editing term).
Fromthe figure, we see that if there is no silence betweenwi-1 and wi, the null interpretation for the tone,repair and editing term tags is preferred.
Since theindependence assumptions that we have to make aretoo strong, we normalize the adjusted tone, editingterm and repair tag probabilities to ensure that theysum to one over all of the values of the tags.6 ExampleTo demonstrate how the model works, consider thefollowing example.Example  11 (d92a-2.1 u t t95)will take a total of um let's see total of s- of 7 hoursreparandum | et reparandum liV iVThe language model considers all possible interpre-tations (at least those that do not get pruned) andassigns a probability to each.
Below, we give theprobabilities for the correct interpretation of the258word "um", given.the correct interpretation of thewords "will take a total of".
For reference, we givea simplified view of the context hat is used for eachprobability.Pr(T6=null\[a total of)=0.98Pr(E6=Pushla total of)=0.28Pr(R~=nultla total of Push)=l.00Pr(P6=UH_FP\[a total of Push)=0.75Pr(Ws=um\[a total of Push UH_FP)=0.33Given the correct interpretation of the previouswords, the probability of the filled pause "urn" alongwith the correct POS tag, boundary tone tag, andrepair tags is 0.0665.Now lets consider predicting the second instanceof "total", which is the first word of the alteration ofthe first repair, whose editing term "urn let's see",which ends with a boundary tone, has just finished.Pr(T10=TlPush let's see)=0.93Pr(E:0=PoPlPush let's see Tone)=0.79Pr(R10=Mla total of Push let's see Pop) = 0.26Pr(O10=totallwill take a total of R10=Mod)=0.07Pr(L10=totalltotal of R10=Mod)=0.94Pr(C10=mlwill take a L10=total/NN) = 0.87 4Pr(P10=NN\]will take a L10=total/NN C10=m)=lPr(W10=total\[will take a NN L10=totai C10---m)=lGiven the correct interpretation of the previouswords, the probability of the word "total" along withthe correct POS tag, boundary tone tag, and repairtags is 0.011.7 Resu l t sTo demonstrate our model, we use a 6-fold crossvalidation procedure, in which we use each sixth ofthe corpus for testing data, and the rest for train-ing data.
We start with the word transcriptions ofthe Trains corpus, thus allowing us to get a clearerindication of the performance of our model withouthaving to take into account he poor performanceof speech recognizers on spontaneous speech.
All si-lence durations are automatically obtained from aword aligner (Ent, 1994).Table 2 shows how POS tagging, discourse markeridentification and perplexity benefit by modeling thespeaker's utterance.
The POS tagging results are re-ported as the percentage ofwords that were assignedthe wrong tag.
The detection of discourse markers isreported using recall and precision.
The recall rateof X is the number of X events that were correctlydetermined by the algorithm over the number of oc-currences of X.
The precision rate is the numberof X events that were correctly determined over thenumber of times that the algorithm guessed X. Theerror rate is the number of X events that the algo-rithm missed plus the number of X events that itincorrectly guessed as occurring over the number ofX events.
The last measure is perplexity, which isBaseModelTonesTones RepairsRepairs CorrectionsCorrections SilencesPOS TaggingError Rate 2.95 2.86 2.69Discourse MarkersRecall 96.60 96.60 97.14Precision 95.76 95.86 96.31Error Rate 7.67 7.56 6.57Perplexity 24.35 23.05 22.45Table 2: POS Tagging and Perplexity ResultsTonesRepairsTones CorrectionsTones Silences SilencesWithin TurnRecall 64.9 70.2 70.5Precision 67.4 68.7 69.4Error Rate 66.5 61.9 60.5All TonesRecall 80.9 83.5 83.9Precision 81.0 81.3 81.8Error Rate 38.0 35.7 34.8Perplexity 24.12 23.78 22.45Table 3: Detecting Intonational Phrasesa way of measuring how well the language model isable to predict the next word.
The perplexity of atest set of N words Wl,g is calculated as follows.1 N 2-~ ~,=1 l?g2 Pr(wdwl, ~-')The second column of Table 2 gives the resultsof the POS-based model, the third column givesthe results of incorporating the detection and cor-rection of speech repairs and detection of intona-tional phrase boundary tones, and the fourth col-umn gives the results of adding in silence informa-tion.
As can be seen, modeling the user's utterancesimproves POS tagging, identification of discoursemarkers, and word perplexity; with the POS er-ror rate decreasing by 3.1% and perplexity by 5.3%.Furthermore, adding in silence information to helpdetect he boundary tones and speech repairs resultsin a further improvement, with the overall POS tag-ging error rate decreasing by 8.6% and reducing per-plexity by 7.8%.
In contrast, a word-based trigrambackoff model (Katz, 1987) built with the CMU sta-tistical language modeling toolkit (Rosenfeld, 1995)achieved a perplexity of 26.13.
Thus our full lan-guage model results in 14.1% reduction in perplex-ity.Table 3 gives the results of detecting intonationalboundaries.
The second column gives the resultsof adding the boundary tone detection to the POSmodel, the third column adds silence information,259RepairsRepairs CorrectionsRepairs Silences SilencesDetectionRecall 67.9 72.7Precision 80.6 77.9Error Rate 48.5 47.9CorrectionRecallPrecisionError RatePerplexity 24.11 23.72TonesRepairsCorrectionsSilences75.7 77.080.8 84.842.4 36.862.4 65.066.6 71.568.9 60.923.04 22.45Table 4: Detecting and Correcting Speech Repairsand the fourth column adds speech repair detectionand correction.
We see that adding in silence infor-mation gives a noticeable improvement in detectingboundary tones.
Furthermore, adding in the speechrepair detection and correction further improves theresults of identifying boundary tones.
Hence to de-tect intonational phrase boundaries in spontaneousspeech, one should also model speech repairs.Table-4 gives the results of detecting and correct-ing speech repairs.
The detection results report thenumber of repairs that were detected, regardless ofwhether the type of repair (e.g.
modification repairversus abridged repair) was properly determined.The second column gives the results of adding speechrepair detection to the POS model.
The third col-umn adds in silence information.
Unlike the case forboundary tones, adding silence does not have muchof an effect.
4 The fourth column adds in speech re-pair correction, and shows that taking into accountthe correction, gives better detection rates (Heeman,Loken-Kim, and Allen, 1996).
The fifth column addsin boundary tone detection, which improves both thedetection and correction of speech repairs.8 Compar i son  to  Other  WorkComparing the performance of this model to oth-ers that have been proposed in the literature is verydifficult, due to differences in corpora, and differentinput assumptions.
However, it is useful to comparethe different echniques that are used.Bear et al (1992) used a simple pattern matchingapproach on ATIS word transcriptions.
They ex-clude all turns that have a repair that just consistsof a filled pause or word fragment.
On this subsetthey obtained a correction recall rate of 43% and aprecision of 50%.Nakatani and Hirschberg (1994) examined howspeech repairs can be detected using a variety ofinformation, including acoustic, presence of word4Silence has a bigger effect on detection and correc-tion if boundary tones are modeled.matchings, and POS tags.
Using these clues theywere able to train a decision tree which achieved arecall rate of 86.1% and a precision of 92.1% on a setof turns in which each turn contained at least onespeech repair.Stolcke and Shriberg (1996b) examined whetherperplexity can be improved by modeling simpletypes of speech repairs in a language model.
Theyfind that doing so actually makes perplexity worse,and they attribute this to not having a linguistic seg-mentation available, which would help in modelingfilled pauses.
We feel that speech repair modelingmust be combined with detecting utterance bound-aries and discourse markers, and should take advan-tage of acoustic information.For detecting boundary tones, the model ofWightman and Ostendorf (1994) achieves a recallrate of 78.1% and a precision of 76.8%.
Their betterperformance is partly attributed to richer (speakerdependent) acoustic modeling, including phonemeduration, energy, and pitch.
However, their modelwas trained and tested on professionally read speech,rather than spontaneous speech.Wang and Hirschberg (1992) did employ sponta-neous speech, namely, the ATIS corpus.
For turn-internal boundary tones, they achieved a recall rateof 38.5% and a precision of 72.9% using a decisiontree approach that combined both textual features,such as POS tags, and syntactic onstituents withintonational features.
One explanation for the differ-ence in performance was that our model was trainedon approximately ten times as much data.
Secondly,their decision trees are used to classify each datapoint independently of the next, whereas we findthe best interpretation over the entire turn, and in-corporate speech repairs.The models of Kompe et al (1994) and Mast etal.
(1996) are the most similar to our model interms of incorporating a language model.
Mast etal.
achieve a recall rate of 85.0% and a precision of53.1% on identifying dialog acts in a German cor-pus.
Their model employs richer acoustic modeling,however, it does not account for other aspects of ut-terance modeling, such as speech repairs.9 Conc lus ionIn this paper, we have shown that the problemsof identifying intonational boundaries and discoursemarkers, and resolving speech repairs can be tack-led by a statistical language model, which uses lo-cal context.
We have also shown that these tasks,along with POS tagging, should be resolved to-gether.
Since our model can give a probability esti-mate for the next word, it can be used as the lan-guage model for a speech recognizer.
In terms ofperplexity, our model gives a 14% improvement overword-based language models.
Part of this improve-ment is due to being able to exploit silence durations,260which tradit ional  word-based language models tendto ignore.
Our next step is to incorporate this modelinto a speech recognizer in order to validate that theimproved perplexity does in fact lead to a betterword recognition rate.10  AcknowledgmentsThis material  is based upon work supported by theNSF under grant IRI-9623665 and by ONR undergrant N00014-95-1-1088.
Final preparat ion of thispaper was done while the first author was visitingCNET, France T~l~com.Re ferencesAllen, J. F., L. Schubert, G. Ferguson, P. Heeman,C.
Hwang, T. Kato, M. Light, N. Martin, B. Miller,M.
Poesio, and D. Traum.
1995.
The Trains project:A case study in building a conversational planningagent.
Journal of Experimental nd Theoretical AI,7:7-48.Bahl, L. R., P. F. Brown, P. V. deSouza, and R. L. Mer-cer.
1989.
A tree-based statistical language modelfor natural naguage speech recognition.
IEEE Trans-actions on Acoustics, Speech, and Signal Processing,36(7):1001-1008.Bear, J., J. Dowding, and E. Shriberg.
1992.
Integratingmultiple knowledge sources for detection and correc-tion of repairs in human-computer dialog.
In Proceed-ings of the 30 th Annual Meeting of the Association forComputational Linguistics, pages 56-63.Black, E., F. Jelinek, J. Lafferty, R. Mercer, andS.
Roukos.
1992.
Decision tree models applied to thelabeling of text with parts-of-speech.
In Proceedings ofthe DARPA Speech and Natural Language Workshop,pages 117-121.
Morgan Kaufmann.Breiman, L., J. H. Friedman, R. A. Olshen, and C. J.Stone.
1984.
Classification and Regression Trees.Monterrey, CA: Wadsworth & Brooks.Brown, P. F., V. J. Della Pietra, P. V. deSouza, J. C.Lai, and R. L. Mercer.
1992.
Class-based n-grammodels of natural anguage.
Computational Linguis-tics, 18(4):467-479.Byron, D. K. and P. A. Heeman.
1997.
Discourse markeruse in task-oriented spoken dialog.
In Proceedings ofthe 5 th European Conference on Speech Communica-tion and Technology (Eurospeech), Rhodes, Greece.Entropic Research Laboratory, Inc., 1994.
Aligner Ref-erence Manual.
Version 1.3.Heeman, P. and J. Allen.
1994.
Detecting and correct-ing speech repairs.
In Proceedings of the 32 th AnnualMeeting of the Association for Computational Linguis-tics, pages 295-302, Las Cruces, New Mexico, June.Heeman, P. A.
1997.
Speech repairs, intonationalboundaries and discourse markers: Modeling speakers'utterances in spoken dialog.
Doctoral dissertation.Heeman, P. A. and J. F. Allen.
1995.
The Trains spo-ken dialog corpus.
CD-ROM, Linguistics Data Con-sortium.Heeman, P. A. and J. F. Allen.
1997.
Incorporating POStagging into language modeling.
In Proceedings of the5 th European Conference on Speech Communicationand Technology (Eurospeech), Rhodes, Greece.Heeman, P. A., K. Loken-Kim, and J. F. Allen.
1996.Combining the detection and correction of speech re-pairs.
In Proceedings of the 4rd International Con-ference on Spoken Language Processing (ICSLP-96),pages 358-361, Philadephia, October.ttindle, D. 1983.
Deterministic parsing of syntactic non-fluencies.
In Proceedings of the 21 st Annual Meeting ofthe Association for Computational Linguistics, pages123-128.Hirschberg, J. and D. Litman.
1993.
Empirical studieson the disambiguation of cue phrases.
ComputationalLinguistics, 19(3):501-530.Katz, S. M. 1987.
Estimation of probabilities fromsparse data for the language model component of aspeech recognizer.
IEEE Transactions on Acoustics,Speech, and Signal Processing, pages 400-401, March.Kompe, R., A. Battiner, A. Kiefling, U. Kilian, H. Nie-mann, E. NSth, and P. Regel-Brietzmann.
1994.
Au-tomatic classification of prosodically marked phraseboundaries in german.
In Proceedings of the Interna-tional Conference on Audio, Speech and Signal Pro-cessing (ICASSP), pages 173-176, Adelaide.Levelt, W. J. M. 1983.
Monitoring and self-repair inspeech.
Cognition, 14:41-104.Magerman, D. M. 1995.
Statistical decision trees folparsing.
In Proceedings of the 33 th Annual Meeting ofthe Association for Computational Linguistics, pages7-14, Cambridge, MA, June.Marcus, M. P., B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of en-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313-330.Mast, M., R. Kompe, S. Harbeck, A. Kieflling, H. Nie-mann, E. NSth, E. G. Schukat-Taiamazzini, andV.
Warnke.
1996.
Dialog act classification with thehelp of prosody.
In Proceedings of the 4rd Inter-national Conference on Spoken Language Processing(ICSLP-96), Philadephia, October.Nakatani, C. H. and J. Hirschberg.
1994.
Acorpus-based study of repair cues in spontaneousspeech.
Journal of the Acoustical Society of America,95(3):1603-1616.Rosenfeld, R. 1995.
The CMU statistical language mod-eling toolkit and its use in the 1994 ARPA CSR evai-uation.
In Proceedings of the ARPA Spoken LanguageSystems Technology Workshop, San Mateo, California,1995.
Morgan Kaufmann.Schiffrin, D. 1987.
Discourse Markers.
New York: Cam-bridge University Press.Silverman, K., M. Beckman, J. Pitrelli, M. Osten-dorf, C. Wightman, P. Price, J. Pierrehumbert, andJ.
Hirschberg.
1992.
ToBI: A standard for labellingEnglish prosody.
In Proceedings of the 2nd Inter-national Conference on Spoken Language Processing(ICSLP-92), pages 867-870.Stolcke, A. and E. Shriberg.
1996a.
Automatic linguisticsegmentation of conversational speech.
In Proceedingsof the 4rd International Conference on Spoken Lan-guage Processing (1CSLP-96), October.Stolcke, A. and E. Shriberg.
1996b.
Statistical languagemodeling for speech disfluencies.
In Proceedings of theInternational Conference on Audio, Speech and SignalProcessing (1CASSP), May.Wang, M. Q. and J. Hirschberg.
1992.
Automatic lassi-fication of intonational phrase boundaries.
ComputerSpeech and Language, 6:175-196.Wightman, C. W. and M. Ostendorf.
1994.
Automaticlabeling of prosodic patterns.
IEEE Transactions onspeech and audio processing, October.261
