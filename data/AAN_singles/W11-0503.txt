Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 16?24,Portland, Oregon, June 23, 2011. c?2011 Association for Computational LinguisticsSummarizing Decisions in Spoken MeetingsLu WangDepartment of Computer ScienceCornell UniversityIthaca, NY 14853luwang@cs.cornell.eduClaire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14853cardie@cs.cornell.eduAbstractThis paper addresses the problem of summa-rizing decisions in spoken meetings: our goalis to produce a concise decision abstract foreach meeting decision.
We explore and com-pare token-level and dialogue act-level au-tomatic summarization methods using bothunsupervised and supervised learning frame-works.
In the supervised summarization set-ting, and given true clusterings of decision-related utterances, we find that token-levelsummaries that employ discourse context canapproach an upper bound for decision ab-stracts derived directly from dialogue acts.In the unsupervised summarization setting,wefind that summaries based on unsupervisedpartitioning of decision-related utterances per-form comparably to those based on partitionsgenerated using supervised techniques (0.22ROUGE-F1 using LDA-based topic modelsvs.
0.23 using SVMs).1 IntroductionMeetings are a common way for people to share in-formation and discuss problems.
And an effectivemeeting always leads to concrete decisions.
As a re-sult, it would be useful to develop automatic meth-ods that summarize not the entire meeting dialogue,but just the important decisions made.
In particular,decision summaries would allow participants to re-view decisions from previous meetings as they pre-pare for an upcoming meeting.
For those who didnot participate in the earlier meetings, decision sum-maries might provide one type of efficient overviewof the meeting contents.
For managers, decisionsummaries could act as a concise record of the ideageneration process.While there has been some previous work insummarizing meetings and conversations, very lit-tle work has focused on decision summarization:Ferna?ndez et al (2008a) and Bui et al (2009) in-vestigate the use of a semantic parser and machinelearning methods for phrase- and token-level deci-sion summarization.
We believe our work is the firstto explore and compare token-level and dialogueact-level approaches ?
using both unsupervised andsupervised learning methods ?
for summarizing de-cisions in meetings.C: Just spinning and not scrolling , I would say .
(1)C: But if you?ve got a [disfmarker] if if you?ve got a flippedthing , effectively it?s something that?s curved on one sideand flat on the other side , but you folded it in half .
(2)D: the case would be rubber and the the buttons , (3)B: I think the spinning wheel is definitely very now .
(1)B: and then make the colour of the main remote [vocal-sound] the colour like vegetable colours , do you know ?
(4)B: I mean I suppose vegetable colours would be orangeand green and some reds and um maybe purple (4)A: but since LCDs seems to be uh a definite yes , (1)A: Flat on the top .
(2)Decision Abstracts (Summary)DECISION 1: The remote will have an LCD and spinningwheel inside.DECISION 2: The case will be flat on top and curved onthe bottom.DECISION 3: The remote control and its buttons will bemade of rubber.DECISION 4: The remote will resemble a vegetable andbe in bright vegetable colors.Table 1: A clip of a meeting from the AMI meeting cor-pus (Carletta et al, 2005).
A, B, C and D refer to distinctspeakers; the numbers in parentheses indicate the asso-ciated meeting decision: DECISION 1, 2, 3 or 4.
Alsoshown is the gold-standard (manual) abstract (summary)for each decision.16Consider the sample dialogue snippet in Table 1,which is part of the AMI meeting corpus (Carletta etal., 2005).
The Table lists only decision-related di-alogue acts (DRDAs) ?
utterances associated withat least one decision made in the meeting.1 The DR-DAs are ordered by time; intervening utterances arenot shown.
DRDAs are important because they con-tain critical information for decision summary con-struction.Table 1 clearly shows some challenges for deci-sion summarization for spoken meetings beyond thedisfluencies, high word error rates, absence of punc-tuation, interruptions and hesitations due to speech.First, different decisions can be discussed more orless concurrently; as a result, the utterances asso-ciated with a single decision are not contiguous inthe dialogue.
In Table 1, the dialogue acts (hence-forth, DAs) concerning DECISION 1, for exam-ple, are interleaved with DAs for other decisions.Second, some decision-related DAs contribute morethan others to the associated decision.
In compos-ing the summary for DECISION 1, for example, wemight safely ignore the first DA for DECISION 1.
Fi-nally, more so than for standard text summarization,purely extract-based summaries are not likely to beeasily interpretable: DRDAs often contain text thatis irrelevant to the decision and many will only beunderstandable if analyzed in the context of the sur-rounding utterances.In this paper, we study methods for decision sum-marization for spoken meetings.
We assume thatall decision-related DAs have been identified andaim to produce a summary for the meeting in theform of concise decision abstracts (see Table 1), onefor each decision made.
In response to the chal-lenges described above, we propose a summariza-tion framework that includes:Clustering of decision-related DAs.
Here we aim topartition the decision-related utterances (DRDAs)according to the decisions each supports.
This stepis similar in spirit to many standard text summariza-tion techniques (Salton et al, 1997) that begin bygrouping sentences according to semantic similar-ity.Summarization at the DA-level.
We select just the im-portant DRDAs in each cluster.
Our goal is to elimi-nate redundant and less informative utterances.
The1These are similar, but not completely equivalent, to the de-cision dialogue acts (DDAs) of Bui et al (2009), Ferna?ndez etal.
(2008a), Frampton et al (2009).
The latter refer to all DAsthat appear in a decision discussion even if they do NOT supportany particular decision.selected DRDAs are then concatenated to form thedecision summary.Optional token-level summarization of the selectedDRDAs.
Methods are employed to capture con-cisely the gist of each decision, discarding anydistracting text.Incorporation of the discourse context as needed.We hypothesize that this will produce moreinterpretable summaries.More specifically, we compare both unsupervised(TFIDF (Salton et al, 1997) and LDA topic mod-eling (Blei et al, 2003)) and (pairwise) supervisedclustering procedures (using SVMs and MaxEnt) forpartitioning DRDAs according to the decision eachsupports.
We also investigate unsupervised methodsand supervised learning for decision summarizationat both the DA and token level, with and without theincorporation of discourse context.
During training,the supervised decision summarizers are told whichDRDAs for each decision are the most informativefor constructing the decision abstract.Our experiments employ the aforementionedAMI meeting corpus: we compare our decisionsummaries to the manually generated decision ab-stracts for each meeting and evaluate performanceusing the ROUGE-1 (Lin and Hovy, 2003) text sum-marization evaluation metric.In the supervised summarization setting, our ex-periments demonstrate that with true clusterings ofdecision-related DAs, token-level summaries thatemploy limited discourse context can approach anupper bound for summaries extracted directly fromDRDAs2 ?
0.4387 ROUGE-F1 vs. 0.5333.
Whenusing system-generated DRDA clusterings, the DA-level summaries always dominate token-level meth-ods in terms of performance.For the unsupervised summarization setting, weinvestigate the use of both unsupervised and su-pervised methods for the initial DRDA clusteringstep.
We find that summaries based on unsupervisedclusterings perform comparably to those generatedusing supervised techniques (0.2214 ROUGE-F1using LDA-based topic models vs. 0.2349 usingSVMs).
As in the supervised summarization setting,we observe that including additional discourse con-text boosts performance only for token-level sum-maries.2The upper bound measures the vocabulary overlap of eachgold-standard decision summary with the complete text of all ofits associated DRDAs.172 Related WorkThere exists much previous research on automatictext summarization using corpus-based, knowledge-based or statistical methods (Mani, 1999; Marcu,2000).
Dialogue summarization methods, how-ever, generally try to account for the special char-acteristics of speech.
Among early work inthis subarea, Zechner (2002) investigates speechsummarization based on maximal marginal rele-vance (MMR) and cross-speaker linking of infor-mation.
Popular supervised methods for summa-rizing speech ?
including maximum entropy, con-ditional random fields (CRFs), and support vectormachines (SVMs) ?
are investigated in Buist et al(2004), Xie et al (2008) and Galley (2006).
Tech-niques for determining semantic similarity are usedfor selecting relevant utterances in Gurevych andStrube (2004).Studies in Banerjee et al (2005) show that de-cisions are considered to be one of the most im-portant outputs of meetings.
And in recent years,there has been much research on detecting decision-related DAs.
Hsueh and Moore (2008), for exam-ple, propose maximum entropy classification tech-niques to identify DRDAs in meetings; Ferna?ndezet al (2008b) develop a model of decision-makingdialogue structure and detect decision DAs based onit; and Frampton et al (2009) implement a real-timedecision detection system.Ferna?ndez et al (2008a) and Bui et al (2009),however, might be the most relevant previous workto ours.
The systems in both papers run an open-domain semantic parser on meeting transcriptionsto produce multiple short fragments, and then em-ploy machine learning methods to select the phrasesor words that comprise the decision summary.
Al-though their task is also decision summarization,their gold-standard summaries consist of manuallyannotated words from the meeting while we judgeperformance using manually constructed decisionabstracts as the gold standard.
The latter are morereadable, but often use a vocabulary different fromthat of the associated decision-related utterances inthe meeting.Our work differs from all of the above in that we(1) incorporate a clustering step to partition DRDAsaccording to the decision each supports; (2) generatedecision summaries at both the DA- and token-level;and (3) investigate the role of discourse context fordecision summarization.In the following sections, we investigate methodsfor clustering DRDAs (Section 3) and generatingDA-level and token-level decision summaries (Sec-tion 4).
In each case, we evaluate the methods usingthe AMI meeting corpus.3 Clustering Decision-Related DialogueActsWe design a preprocessing step that facilitates deci-sion summarization by clustering all of the decision-related dialogue acts according to the decision(s) itsupports.
Because it is not clear how many deci-sions are made in a meeting, we use a hierarchi-cal agglomerative clustering algorithm (rather thantechniques that require a priori knowledge of thenumber of clusters) and choose the proper stoppingconditions.
In particular, we employ average-linkmethods: at each iteration, we merge the two clus-ters with the maximum average pairwise similarityamong their DRDAs.
In the following subsections,we introduce unsupervised and supervised methodsfor measuring the pairwise DRDA similarity.3.1 DRDA Similarity: Unsupervised MethodsWe consider two unsupervised similarity measures?
one based on the TF-IDF score from the Infor-mation Retrieval research community, and a secondbased on Latent Dirichlet Allocation topic models.TF-IDF similarity.
TF-IDF similarity metricshave worked well as a measure of document simi-larity.
As a result, we employ it as one metric formeasuring the similarity of two DRDAs.
Supposethere are L distinct word types in the corpus.
Wetreat each decision-related dialgue act DAi as adocument, and represent it as an L-dimensionalfeature vector??
?FVi = (xi1, xi2, ..., xiL), where xikis word wk?s tf ?
idf score for DAi.
Then the(average-link) similarity of cluster Cm and clusterCn, Sim TFIDF (Cm, Cn), is defined as :1| Cm | ?
| Cn |?DAi?CmDAj?Cn??
?FVi ????FVj???
?FVi ????
?FVj ?LDA topic models.
In recent years, topic modelshave become a popular technique for discovering thelatent structure of ?topics?
or ?concepts?
in a cor-pus.
Here we use the Latent Dirichlet Allocation(LDA) topic models of Blei et al (2003) ?
unsuper-18Featuresnumber of overlapping wordsproportion of the number of overlapping words to the le-ngth of shorter DATF-IDF similaritywhether the DAs are in an adjacency pair (see 4.3)time difference of pairwise DAsrelative dialogue position of pairwise DAswhether the two DAs have the same DA typenumber of overlapping words in the contexts (see 4.2)Table 2: Features for Pairwise Supervised Clusteringvised probabilistic generative models that estimatethe properties of multinomial observations.
In oursetting, LDA-based topic models provide a soft clus-tering of the DRDAs according to the topics theydiscuss.3 To determine the similarity of two DR-DAs, we effectively measure the similarity of theirterm-based topic distributions.To train an LDA-based topic model for our task4,we treat each DRDA as an individual document.After training, each DRDA, DAi, is assigned atopic distribution??
?i according to the learned model.Thus, we can define the similarity of cluster Cm andcluster Cn, Sim LDA(Cm, Cn), as :1| Cm | ?
| Cn |?DAi?CmDAj?Cn??
?i ???
?j3.2 DRDA Similarity: Supervised TechniquesIn addition to unsupervised methods for clusteringDRDAs, we also explore an approach based on Pair-wise Supervised Learning: we develop a classifierthat determines whether or not a pair of DRDAs sup-ports the same decision.
So each training and testexample is a feature vector that is a function of twoDRDAs: for DAi and DAj , the feature vector is??
?FVij = f(DAi, DAj) = {fv1ij , fv2ij , ..., fvkij}.
Ta-ble 2 gives a full list of features that are used.
Be-cause the annotations for the time information anddialogue type of DAs are available from the cor-pus, we employ features including time differenceof pairwise DAs, relative position5 and whether they3We cannot easily associate each topic with a decision be-cause the number of decisions is not known a priori.4Parameter estimation and inference done by GibbsLDA++.5Here is the definition for the relative position of pairwiseDAs.
Suppose there are N DAs in one meeting ordered by time,have the same DA type.We employ Support Vector Machines (SVMs)and Maximum Entropy (MaxEnt) as our learningmethods, because SVMs are shown to be effectivein text categorization (Joachims, 1998) and Max-Ent has been applied in many natural languageprocessing tasks (Berger et al, 1996).
Given an??
?FVij , for SVMs, we utilize the decision value ofwT ???
?FVij + b as the similarity, where w is theweight vector and b is the bias.
For MaxEnt, wemake use of the probability of P (SameDecision |??
?FVij) as the similarity value.3.3 ExperimentsCorpus.
We use the AMI meeting Corpus (Car-letta et al, 2005), a freely available corpus of multi-party meetings that contains a wide range of anno-tations.
The 129 scenario-driven meetings involvefour participants playing different roles on a de-sign team.
A short (usually one-sentence) abstractis included that describes each decision, action, orproblem discussed in the meeting; and each DA islinked to the abstracts it supports.
We use the manu-ally constructed decision abstracts as gold-standardsummaries and assume that all decision-related DAshave been identified (but not linked to the decision(s)it supports).Baselines.
Two clustering baselines are utilizedfor comparison.
One baseline places all decision-related DAs for the meeting into a single partition(ALLINONEGROUP).
The second uses the text seg-mentation software of Choi (2000) to partition thedecision-related DAs (ordered according to time)into several topic-based groups (CHOISEGMENT).Experimental Setup and Evaluation.
Results forpairwise supervised clustering were obtained using3-fold cross-validation.
In the current work, stop-ping conditions for hierarchical agglomerative clus-tering are selected manually: For the TF-IDF andtopic model approaches, we stop when the similar-ity measure reaches 0.035 and 0.015, respectively;For the SVM and MaxEnt versions, we use 0 and0.45, respectively.
We use the Mallet implementa-tion for MaxEnt and the SVMlight implementationof SVMs.Our evaluation metrics include b3 (also called B-cubed) (Bagga and Baldwin, 1998), which is a com-DAi is the ith DA and DAj is positioned at j.
So the relativeposition of DAi and DAj is|i?j|N .19B-cubed Pairwise VOIPRECISION RECALL F1 PRECISION RECALL F1BaselinesAllInOneGroup 0.2854 1.0000 0.4441 0.1823 1.0000 0.3083 2.2279ChoiSegment 0.4235 0.9657 0.5888 0.2390 0.8493 0.3730 1.8061Unsupervised MethodsTFIDF 0.6840 0.6686 0.6762 0.3281 0.3004 0.3137 1.6604LDA topic models 0.8265 0.6432 0.7235 0.4588 0.2980 0.3613 1.4203Pairwise Supervised MethodsSVM 0.7593 0.7466 0.7529 0.5474 0.4821 0.5127 1.2239MaxEnt 0.6999 0.7948 0.7443 0.4858 0.5704 0.5247 1.2726Table 3: Results for Clustering Decision-Related DAs According to the Decision Each Supportsmon measure employed in noun phrase coreferenceresolution research; a pairwise scorer that measurescorrectness for every pair of DRDAs; and a variationof information (VOI) scorer (Meila?, 2007), whichmeasures the difference between the distributions ofthe true clustering and system generated clustering.As space is limited, we refer the readers to the orig-inal papers for more details.
For b3 scorer and pair-wise scorer, higher results represent better perfor-mance; for VOI, lower is better.6Results.
The results in Table 3 show first that allof the proposed clustering methods outperform thebaselines.
Among the unsupervised methods, theLDA topic modeling is preferred to TFIDF.
For thesupervised methods, SVMs and MaxEnt producecomparable results.4 Decision SummarizationIn this section, we turn to decision summarization ?extracting a short description of each decision basedon the decision-related DAs in each cluster.
We in-vestigate options for constructing an extract-basedsummary that consists of a single DRDA and anabstract-based summary comprised of keywords thatdescribe the decision.
For both types of summary,we employ standard techniques from text summa-rization, but also explore the use of dialogue-specificfeatures and the use of discourse context.4.1 DA-Level Summarization Based on Unsu-pervised MethodsWe make use of two unsupervised methods to sum-marize the DRDAs in each ?decision cluster?.
Thefirst method simply returns the longest DRDA in the6The MUC scorer is popular in coreference evaluation, but itis flawed in measuring the singleton clusters which is prevalentin the AMI corpus.
So we do not use it in this work.Lexical Featuresunigram/bigramlength of the DAcontain digits?has overlapping words with next DA?next DA is a positive feedback?Structural Featuresrelative position in the meeting?
(beginning, ending, or else)in an AP?if in an AP, AP typeif in an AP, the other part is decision-related?if in an AP, is the source part or target part?if in an AP and is source part, target is positive feedback?if in an AP and is target part, source is a question?Discourse Featuresrelative position to ?WRAP UP?
or ?RECAP?Other FeaturesDA typespeaker roletopicTable 4: Features Used in DA-Level Summarizationcluster as the summary (LONGEST DA).
The sec-ond approach returns the decision cluster prototype,i.e., the DRDA with the largest TF-IDF similar-ity with the cluster centroid (PROTOTYPE DA).
Al-though important decision-related information maybe spread over multiple DRDAs, both unsupervisedmethods allow us to determine summary qualitywhen summaries are restricted to a single utterance.4.2 DA-Level and Token-Level SummarizationUsing Supervised LearningBecause the AMI corpus contains a decision abstractfor each decision made in the meeting, we can usethis supervisory information to train classifiers thatcan identify informative DRDAs (for DA-level sum-maries) or informative tokens (for token-level sum-maries).20Lexical Featurescurrent token/current token and next tokenlength of the DAis digit?appearing in next DA?next DA is a positive feedback?Structural Featuressee Table 3Grammatical Featurespart-of-speechphrase type (VP/NP/PP)dependency relationsOther Featuresspeaker roletopicTable 5: Features Used in Token-Level SummarizationPREC REC F1True ClusteringsLongest DA 0.3655 0.4077 0.3545Prototype DA 0.3626 0.4140 0.3539System Clusteringsusing LDALongest DA 0.3623 0.1892 0.2214Prototype DA 0.3669 0.1887 0.2212using SVMsLongest DA 0.3719 0.1261 0.1682Prototype DA 0.3816 0.1264 0.1700No ClusteringLongest DA 0.1039 0.1382 0.1080Prototype DA 0.1350 0.1209 0.1138Upper Bound 0.8970 0.4089 0.5333Table 6: Results for ROUGE-1: Decision Summary Gen-eration Using Unsupervised MethodsDialogue Act-based Summarization.
Previousresearch (e.g., Murray et al (2005), Galley(2006), Gurevych and Strube (2004)) has shownthat DRDA-level extractive summarization can beeffective when viewed as a binary classification task.To implement this approach, we assume that theDRDA to be extracted for the summary is the onewith the largest vocabulary overlap with the cluster?sgold-standard decision abstract.
This DA-level sum-marization method has an advantage that the sum-mary maintains good readability without a naturallanguage generation component.Token-based Summarization.
As shown in Table1, some decision-related DAs contain many uselesswords when compared with the gold-standard ab-stracts.
As a result, we propose a method for token-level decision summarization that focuses on iden-tifying critical keywords from the cluster?s DRDAs.We follow the method of Ferna?ndez et al (2008a),but use a larger set of features and different learningmethods.Adding Discourse Context.
For each of the su-pervised DA- and token-based summarization meth-ods, we also investigate the role of the discoursecontext.
Specifically, we augment the DRDA clus-terings with additional (not decision-related) DAsfrom the meeting dialogue: for each decision par-tition, we include the DA with the highest TF-IDFsimilarity with the centroid of the partition.
Wewill investigate the possible effects of this additionalcontext on summary quality.In the next subsection, we describe the featuresused for supervised learning of DA- and token-baseddecision summaries.4.3 Dialogue Cues for Decision SummarizationDifferent from text, dialogues have some notablefeatures that we expect to be useful for finding in-formative, decision-related utterances.
This sectiondescribes some of the dialogue-based features em-ployed in our classifiers.
The full lists of featuresare shown in Table 4 and Table 5.Structural Information: Adjacency Pairs.
AnAdjacency Pair (AP) is an important conversationalanalysis concept; APs are considered the fundamen-tal unit of conversational organization (Schegloffand Sacks, 1973).
In the AMI corpus, an AP pairconsists of a source utterance and a target utterance,produced by different speakers.
The source pre-cedes the target but they are not necessarily adja-cent.
We include features to indicate whether or nottwo DAs are APs indicating QUESTION+ANSWERor POSITIVE FEEDBACK.
For these features, we usethe gold-standard AP annotations.
We also includeone feature that checks membership in a small setof words to decide whether a DA contains positivefeedback (e.g., ?yeah?, ?yes?
).Discourse Information: Review and Closing In-dicator.
Another pragmatic cue for dialogue dis-cussion is terms like ?wrap up?
or ?recap?, indicat-ing that speakers will review the key meeting con-tent.
We include the distance between these indica-tors and DAs as a feature.Grammatical Information: Dependency RelationBetween Words.
For token-level summarization,we make use of the grammatical relationships inthe DAs.
As in Bui et al (2009) and Ferna?ndez21CRFs SVMsPRECISION RECALL F1 PRECISION RECALL F1True ClusteringsDA 0.3922 0.4449 0.3789 0.3661 0.4695 0.3727Token 0.5055 0.2453 0.3033 0.4953 0.3788 0.3963DA+Context 0.3753 0.4372 0.3678 0.3595 0.4449 0.3640Token+Context 0.5682 0.2825 0.3454 0.6213 0.3868 0.4387System Clusteringsusing LDADA 0.3087 0.1663 0.1935 0.3391 0.2097 0.2349Token 0.3379 0.0911 0.1307 0.3760 0.1427 0.1843DA+Context 0.3305 0.1748 0.2041 0.2903 0.1869 0.2068Token+Context 0.4557 0.1198 0.1727 0.4882 0.1486 0.2056using SVMsDA 0.3508 0.1884 0.2197 0.3592 0.2026 0.2348Token 0.2807 0.04968 0.0777 0.3607 0.0885 0.1246DA+Context 0.3583 0.1891 0.2221 0.3418 0.1892 0.2213Token+Context 0.4891 0.0822 0.1288 0.4873 0.0914 0.1393No ClusteringDA 0.08673 0.1957 0.0993 0.0707 0.1979 0.0916Token 0.1906 0.0625 0.0868 0.1890 0.3068 0.2057Table 7: Results for ROUGE-1: Summary Generation Using Supervised Learninget al (2008a), we design features that encode (a)basic predicate-argument structures involving majorphrase types (S, VP, NP, and PP) and (b) additionaltyped dependencies from Marneffe et al (2006).
Weuse the Stanford Parser.5 ExperimentsExperiments based on supervised learning are per-formed using 3-fold cross-validation.
We train twodifferent types of classifiers for identifying infor-mative DAs or tokens: Conditional Random Fields(CRFs) (via Mallet) and Support Vector Machines(SVMs) (via SVMlight).We remove function words from DAs before us-ing them as the input of our systems.
The AMI deci-sion abstracts are the gold-standard summaries.
Weuse the ROUGE (Lin and Hovy, 2003) evaluationmeasure.
ROUGE is a recall-based method that canidentify systems producing succinct and descriptivesummaries.7Results and Analysis.
Results for the unsuper-vised and supervised summarization methods areshown in Tables 6 and 7, respectively.
In the tables,TRUE CLUSTERINGS means that we apply our meth-ods on the gold-standard DRDA clusterings.
SYS-TEM CLUSTERINGS use clusterings obtained fromthe methods introduced in Section 4; we show re-7We use the stemming option of the ROUGE software athttp://berouge.com/.sults only using the best unsupervised (USING LDA)and supervised (USING SVMS) DRDA clusteringtechniques.Both Table 6 and 7 show that some attempt tocluster DRDAs improves the summarization resultsvs.
NO CLUSTERING.
In Table 6, there is no signif-icant difference between the results obtained fromthe LONGEST DA and PROTOTYPE DA for any ex-periment setting.
This is because the longest DA isoften selected as the prototype.
An UPPER BOUNDresult is listed for comparison: for each decisioncluster, this system selects all words from the DR-DAs that are part of the decision abstract (discardingduplicates).Table 7 presents the results for supervised sum-marization.
Rows starting with DA or TOKEN indi-cate results at the DA- or token-level.
The +CON-TEXT rows show results when discourse context isincluded.8 We see that: (1) SVMs have a superior orcomparable summarization performance vs. CRFson every task.
(2) Token-level summaries performbetter than DA-level summaries only using TRUECLUSTERINGS and the SVM-based summarizer.
(3)Discourse context generally improves token-levelsummaries but not DA-level summaries.9 (4) DRDA8In our experiments, we choose the top 20 relevant DAs ascontext.9We do not extract words from the discourse context andexperiments where we tried this were unsuccessful.22clusterings produced by (unsupervised) LDA lead tosummaries that are quite comparable in quality tothose generated from DRDA clusterings producedby SVMs (supervised).
From Table 6, we see thatF1 is 0.2214 when choosing longest DAs from LDA-generated clusterings, which is comparable with theF1s of 0.1935 and 0.2349, attained when employingCRF and SVMs on the same clusterings.The results in Table 7 are achieved by compar-ing abstracts having function words with system-generated summaries without function words.
To re-duce the vocabulary difference as much as possible,we also ran experiments that remove function wordsfrom the gold-standard abstracts, but no significantdifference is observed.10Finally, we considered comparing our systems tothe earlier similar work of (Ferna?ndez et al, 2008a)and (Bui et al, 2009), but found that it wouldbe quite difficult because they employ a differentnotion from DRDAs which is Decision DialogueActs(DDAs).
In addition, they manually annotatewords from their DDAs as the gold-standard sum-mary, guaranteeing that their decision summariesemploy the same vocabulary as the DDAs.
We in-stead use the actual decision abstracts from the AMIcorpus.5.1 Sample Decision SummariesHere we show sample summaries produced usingour methods (Table 8).
We pick one of the clus-terings generated by LDA consisting of four DAswhich support two decisions and take SVMs asthe supervised summarization method.
We removefunction words and special markers like ?[disf-marker]?
from the DAs.The outputs indicate that either the longest DA orprototype DA contains part of the decisions in this?mixed?
cluster.
Adding discourse context refinesthe summaries at both the DA- and token-levels.6 ConclusionIn this work, we explore methods for producing de-cision summaries from spoken meetings at both theDA-level and the token-level.
We show that clus-10Given abstracts without function words, and using the clus-terings generated by LDA and employ CRF on DA- and token-level summarization, we get F1s of 0.1954 and 0.1329, whichis marginally better than the corresponding 0.1935 and 0.1307in Table 7.
Similarly, if SVMs are employed in the same cases,we get F1s of 0.2367 and 0.1861 instead of 0.2349 and 0.1843.All of the other results obtain negligible minor increases in F1.DA (1): um of course , as [disfmarker] we , we?ve alreadytalked about the personal face plates in this meeting , (a)DA (2): and I?d like to stick to that .
(a)DA (3): Well , I guess plastic and coated in rubber .
(b)DA (4): So the actual remote would be hard plastic andthe casings rubber .
(b)Decision (a): Will use personal face plates.Decision (b): Case will be plastic and coated in rubber.Longest DA:talked about personal face plates in meetingPrototype DA:actual remote hard plastic casings rubberDA-level:talked about personal face plates in meeting, like tostick to, guess plastic and coated in rubber,actual remote hard plastic casings rubberToken-level:actual remote plastic casings rubberDA-level and Discourse Context:talked about personal face plates in meeting, guess plasticand coated in rubber, actual remote hard plastic casingsrubberToken-level and Discourse Context:remote plastic rubberTable 8: Sample system outputs by different methods arein the third cell (methods?
names are in bold).
First cellcontains four DAs.
(a) or (b) refers to the decision thatDA supports, which is listed in the second cell.tering DRDAs before identifying informative con-tent to extract can improve summarization quality.We also find that unsupervised clustering of DR-DAs (using LDA-based topic models) can producesummaries of comparable quality to those gener-ated from supervised DRDA clustering.
Token-levelsummarization methods can be boosted by addingdiscourse context and outperform DA-level summa-rization when true DRDA clusterings are available;otherwise, DA-level summarization methods offerbetter performance.Acknowledgments.
This work was supported in partby National Science Foundation Grants IIS-0535099 andIIS-0968450, and by a gift from Google.ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In In The First Interna-tional Conference on Language Resources and Eval-uation Workshop on Linguistics Coreference, pages563?566.Satanjeev Banerjee, Carolyn Penstein Rose?, and Alexan-der I. Rudnicky.
2005.
The necessity of a meet-23ing recording and playback system, and the benefit oftopic-level annotations to meeting browsing.
In IN-TERACT, pages 643?656.Adam L. Berger, Vincent J. Della Pietra, and StephenA.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Comput.
Linguist.,22:39?71, March.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022, March.Trung H. Bui, Matthew Frampton, John Dowding, andStanley Peters.
2009.
Extracting decisions frommulti-party dialogue using directed graphical modelsand semantic similarity.
In Proceedings of the SIG-DIAL 2009 Conference, pages 235?243.Anne Hendrik Buist, Wessel Kraaij, and Stephan Raaij-makers.
2004.
Automatic summarization of meetingdata: A feasibility study.
In in Proc.
Meeting of Com-putational Linguistics in the Netherlands (CLIN.Jean Carletta, Simone Ashby, Sebastien Bourban,Mike Flynn, Thomas Hain, Jaroslav Kadlec, VasilisKaraiskos, Wessel Kraaij, Melissa Kronenthal, Guil-laume Lathoud, Mike Lincoln, Agnes Lisowska, andMccowan Wilfried Post Dennis Reidsma.
2005.
Theami meeting corpus: A pre-announcement.
In In Proc.MLMI, pages 28?39.Freddy Y. Y. Choi.
2000.
Advances in domain inde-pendent linear text segmentation.
In Proceedings ofthe 1st North American chapter of the Association forComputational Linguistics conference, pages 26?33.Raquel Ferna?ndez, Matthew Frampton, John Dowding,Anish Adukuzhiyil, Patrick Ehlen, and Stanley Peters.2008a.
Identifying relevant phrases to summarize de-cisions in spoken meetings.
INTERSPEECH-2008,pages 78?81.Raquel Ferna?ndez, Matthew Frampton, Patrick Ehlen,Matthew Purver, and Stanley Peters.
2008b.
Mod-elling and detecting decisions in multi-party dialogue.In Proceedings of the 9th SIGdial Workshop on Dis-course and Dialogue, pages 156?163.Matthew Frampton, Jia Huang, Trung Huu Bui, and Stan-ley Peters.
2009.
Real-time decision detection inmulti-party dialogue.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 3 - Volume 3, pages 1133?1141.Michel Galley.
2006.
A skip-chain conditional randomfield for ranking meeting utterances by importance.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, pages 364?372.Iryna Gurevych and Michael Strube.
2004.
Semanticsimilarity applied to spoken dialogue summarization.In Proceedings of the 20th international conference onComputational Linguistics.Pei-Yun Hsueh and Johanna D. Moore.
2008.
Automaticdecision detection in meeting speech.
In Proceedingsof the 4th international conference on Machine learn-ing for multimodal interaction, pages 168?179.Thorsten Joachims.
1998.
Text categorization with Sup-port Vector Machines: Learning with many relevantfeatures.
In Claire Ne?dellec and Ce?line Rouveirol,editors, Machine Learning: ECML-98, volume 1398,chapter 19, pages 137?142.
Berlin/Heidelberg.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic evalu-ation of summaries using n-gram co-occurrence statis-tics.
In Proceedings of the 2003 Conference of theNorth American Chapter of the Association for Com-putational Linguistics on Human Language Technol-ogy - Volume 1, pages 71?78.Inderjeet Mani.
1999.
Advances in Automatic Text Sum-marization.
MIT Press, Cambridge, MA, USA.Daniel Marcu.
2000.
The Theory and Practice of Dis-course Parsing and Summarization.
MIT Press, Cam-bridge, MA, USA.M.
Marneffe, B. Maccartney, and C. Manning.
2006.Generating Typed Dependency Parses from PhraseStructure Parses.
In Proceedings of LREC-06, pages449?454.Marina Meila?.
2007.
Comparing clusterings?an infor-mation based distance.
J. Multivar.
Anal., 98:873?895,May.Gabriel Murray, Steve Renals, and Jean Carletta.
2005.Extractive summarization of meeting recordings.
Inin Proceedings of the 9th European Conference onSpeech Communication and Technology, pages 593?596.Gerard Salton, Amit Singhal, Mandar Mitra, and ChrisBuckley.
1997.
Automatic text structuring andsummarization.
Inf.
Process.
Manage., 33:193?207,March.E.
A. Schegloff and H. Sacks.
1973.
Opening up clos-ings.
Semiotica, 8(4):289?327.Shasha Xie, Yang Liu, and Hui Lin.
2008.
Evaluatingthe effectiveness of features and sampling in extractivemeeting summarization.
In in Proc.
of IEEE SpokenLanguage Technology (SLT.Klaus Zechner.
2002.
Automatic summarization ofopen-domain multiparty dialogues in diverse genres.Comput.
Linguist., 28:447?485, December.24
