Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1?10,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsOn Using Very Large Target Vocabulary forNeural Machine TranslationS?ebastien Jean Kyunghyun ChoRoland MemisevicUniversit?e de Montr?ealYoshua BengioUniversit?e de Montr?ealCIFAR Senior FellowAbstractNeural machine translation, a recentlyproposed approach to machine transla-tion based purely on neural networks,has shown promising results compared tothe existing approaches such as phrase-based statistical machine translation.
De-spite its recent success, neural machinetranslation has its limitation in handlinga larger vocabulary, as training complex-ity as well as decoding complexity in-crease proportionally to the number of tar-get words.
In this paper, we proposea method based on importance samplingthat allows us to use a very large target vo-cabulary without increasing training com-plexity.
We show that decoding can beefficiently done even with the model hav-ing a very large target vocabulary by se-lecting only a small subset of the wholetarget vocabulary.
The models trainedby the proposed approach are empiricallyfound to match, and in some cases out-perform, the baseline models with a smallvocabulary as well as the LSTM-basedneural machine translation models.
Fur-thermore, when we use an ensemble ofa few models with very large target vo-cabularies, we achieve performance com-parable to the state of the art (measuredby BLEU) on both the English?Germanand English?French translation tasks ofWMT?14.1 IntroductionNeural machine translation (NMT) is a recentlyintroduced approach to solving machine transla-tion (Kalchbrenner and Blunsom, 2013; Bahdanauet al, 2015; Sutskever et al, 2014).
In neural ma-chine translation, one builds a single neural net-work that reads a source sentence and generatesits translation.
The whole neural network is jointlytrained to maximize the conditional probability ofa correct translation given a source sentence, us-ing the bilingual corpus.
The NMT models haveshown to perform as well as the most widely usedconventional translation systems (Sutskever et al,2014; Bahdanau et al, 2015).Neural machine translation has a number ofadvantages over the existing statistical machinetranslation system, specifically, the phrase-basedsystem (Koehn et al, 2003).
First, NMT requiresa minimal set of domain knowledge.
For instance,all of the models proposed in (Sutskever et al,2014), (Bahdanau et al, 2015) or (Kalchbrennerand Blunsom, 2013) do not assume any linguis-tic property in both source and target sentencesexcept that they are sequences of words.
Sec-ond, the whole system is jointly trained to maxi-mize the translation performance, unlike the exist-ing phrase-based system which consists of manyseparately trained features whose weights are thentuned jointly.
Lastly, the memory footprint of theNMT model is often much smaller than the exist-ing system which relies on maintaining large ta-bles of phrase pairs.Despite these advantages and promising results,there is a major limitation in NMT compared tothe existing phrase-based approach.
That is, thenumber of target words must be limited.
This ismainly because the complexity of training and us-ing an NMT model increases as the number of tar-get words increases.A usual practice is to construct a target vo-cabulary of the K most frequent words (a so-called shortlist), where K is often in the range of30k (Bahdanau et al, 2015) to 80k (Sutskever etal., 2014).
Any word not included in this vocab-ulary is mapped to a special token representingan unknown word [UNK].
This approach workswell when there are only a few unknown wordsin the target sentence, but it has been observed1that the translation performance degrades rapidlyas the number of unknown words increases (Choet al, 2014a; Bahdanau et al, 2015).In this paper, we propose an approximate train-ing algorithm based on (biased) importance sam-pling that allows us to train an NMT model witha much larger target vocabulary.
The proposed al-gorithm effectively keeps the computational com-plexity during training at the level of using onlya small subset of the full vocabulary.
Oncethe model with a very large target vocabulary istrained, one can choose to use either all the targetwords or only a subset of them.We compare the proposed algorithm against thebaseline shortlist-based approach in the tasks ofEnglish?French and English?German transla-tion using the NMT model introduced in (Bah-danau et al, 2015).
The empirical results demon-strate that we can potentially achieve better trans-lation performance using larger vocabularies, andthat our approach does not sacrifice too muchspeed for both training and decoding.
Further-more, we show that the model trained with this al-gorithm gets the best translation performance yetachieved by single NMT models on the WMT?14English?French translation task.2 Neural Machine Translation andLimited Vocabulary ProblemIn this section, we briefly describe an approachto neural machine translation proposed recently in(Bahdanau et al, 2015).
Based on this descrip-tion we explain the issue of limited vocabulariesin neural machine translation.2.1 Neural Machine TranslationNeural machine translation is a recently proposedapproach to machine translation, which uses a sin-gle neural network trained jointly to maximizethe translation performance (Forcada and?Neco,1997; Kalchbrenner and Blunsom, 2013; Cho etal., 2014b; Sutskever et al, 2014; Bahdanau et al,2015).Neural machine translation is often imple-mented as the encoder?decoder network.
The en-coder reads the source sentence x = (x1, .
.
.
, xT)and encodes it into a sequence of hidden statesh = (h1, ?
?
?
, hT):ht= f (xt, ht?1) .
(1)Then, the decoder, another recurrent neural net-work, generates a corresponding translation y =(y1, ?
?
?
, yT?)
based on the encoded sequence ofhidden states h:p(yt| y<t, x) ?
exp {q (yt?1, zt, ct)} , (2)wherezt= g (yt?1, zt?1, ct) , (3)ct= r (zt?1, h1, .
.
.
, hT) , (4)and y<t= (y1, .
.
.
, yt?1).The whole model is jointly trained to maximizethe conditional log-probability of the correct trans-lation given a source sentence with respect to theparameters ?
of the model:?
?= argmax?N?n=1Tn?t=1log p(ynt| yn<t, xn),where (xn, yn) is the n-th training pair of sen-tences, and Tnis the length of the n-th target sen-tence (yn).2.1.1 Detailed DescriptionIn this paper, we use a specific implementation ofneural machine translation that uses an attentionmechanism, as recently proposed in (Bahdanau etal., 2015).In (Bahdanau et al, 2015), the encoder inEq.
(1) is implemented by a bi-directional recur-rent neural network such thatht=[??ht;??ht],where?
?ht= f(xt,??ht+1),?
?ht= f(xt,?
?ht?1).They used a gated recurrent unit for f (see, e.g.,(Cho et al, 2014b)).The decoder, at each time, computes the con-text vector ctas a convex sum of the hidden states(h1, .
.
.
, hT) with the coefficients ?1, .
.
.
, ?Tcomputed by?t=exp {a (ht, zt?1)}?kexp {a (hk, zt?1)}, (5)where a is a feedforward neural network with asingle hidden layer.A new hidden state ztof the decoder in Eq.
(3) iscomputed based on the previous hidden state zt?1,previous generated symbol yt?1and the computed2context vector ct.
The decoder also uses the gatedrecurrent unit, as the encoder does.The probability of the next target word inEq.
(2) is then computed byp(yt| y<t, x) =1Zexp{w>t?
(yt?1, zt, ct) + bt},(6)where ?
is an affine transformation followed bya nonlinear activation, and wtand btare respec-tively the target word vector and the target wordbias.
Z is the normalization constant computed byZ =?k:yk?Vexp{w>k?
(yt?1, zt, ct) + bk}, (7)where V is the set of all the target words.For the detailed description of the implementa-tion, we refer the reader to the appendix of (Bah-danau et al, 2015).2.2 Limited Vocabulary Issue andConventional SolutionsOne of the main difficulties in training this neu-ral machine translation model is the computationalcomplexity involved in computing the target wordprobability (Eq.
(6)).
More specifically, we needto compute the dot product between the feature?
(yt?1, zt, ct) and the word vector wtas manytimes as there are words in a target vocabulary inorder to compute the normalization constant (thedenominator in Eq.
(6)).
This has to be done for,on average, 20?30 words per sentence, which eas-ily becomes prohibitively expensive even with amoderate number of possible target words.
Fur-thermore, the memory requirement grows linearlywith respect to the number of target words.
Thishas been a major hurdle for neural machine trans-lation, compared to the existing non-parametricapproaches such as phrase-based translation sys-tems.Recently proposed neural machine translationmodels, hence, use a shortlist of 30k to 80k mostfrequent words (Bahdanau et al, 2015; Sutskeveret al, 2014).
This makes training more feasible,but comes with a number of problems.
First of all,the performance of the model degrades heavily ifthe translation of a source sentence requires manywords that are not included in the shortlist (Choet al, 2014a).
This also affects the performanceevaluation of the system which is often measuredby BLEU.
Second, the first issue becomes moreproblematic with languages that have a rich set ofwords such as German or other highly inflectedlanguages.There are two model-specific approaches to thisissue of large target vocabulary.
The first approachis to stochastically approximate the target wordprobability.
This has been proposed recently in(Mnih and Kavukcuoglu, 2013; Mikolov et al,2013) based on noise-contrastive estimation (Gut-mann and Hyvarinen, 2010).
In the second ap-proach, the target words are clustered into multi-ple classes, or hierarchical classes, and the targetprobability p(yt|y<t, x) is factorized as a productof the class probability p(ct|y<t, x) and the intra-class word probability p(yt|ct, y<t, x).
This re-duces the number of required dot-products into thesum of the number of classes and the words in aclass.
These approaches mainly aim at reducingthe computational complexity during training, butdo not often result in speed-up when decoding atranslation during test time.1Other than these model-specific approaches,there exist translation-specific approaches.
Atranslation-specific approach exploits the proper-ties of the rare target words.
For instance, Luonget al proposed such an approach for neural ma-chine translation (Luong et al, 2015).
They re-place rare words (the words that are not includedin the shortlist) in both source and target sentencesinto corresponding ?OOVn?
tokens using the wordalignment model.
Once a source sentence is trans-lated, each ?OOVn?
in the translation will be re-placed based on the source word marked by thecorresponding ?OOVn?.It is important to note that the model-specific approaches and the translation-specificapproaches are often complementary and can beused together to further improve the translationperformance and reduce the computational com-plexity.3 Approximate Learning Approach toVery Large Target Vocabulary3.1 DescriptionIn this paper, we propose a model-specific ap-proach that allows us to train a neural machinetranslation model with a very large target vocab-ulary.
With the proposed approach, the compu-1This is due to the fact that the beam search requires theconditional probability of every target word at each time stepregardless of the parametrization of the output probability.3tational complexity of training becomes constantwith respect to the size of the target vocabulary.Furthermore, the proposed approach allows us toefficiently use a fast computing device with lim-ited memory, such as a GPU, to train a neural ma-chine translation model with a much larger targetvocabulary.As mentioned earlier, the computational inef-ficiency of training a neural machine translationmodel arises from the normalization constant inEq.
(6).
In order to avoid the growing complex-ity of computing the normalization constant, wepropose here to use only a small subset V?of thetarget vocabulary at each update.
The proposedapproach is based on the earlier work of (Bengioand S?en?ecal, 2008).Let us consider the gradient of the log-probability of the output in Eq.
(6).
The gradientis composed of a positive and negative part:?
log p(yt| y<t, x) (8)=?E(yt)?
?k:yk?Vp(yk| y<t, x)?E(yk),where we define the energy E asE(yj) = w>j?
(yj?1, zj, cj) + bj.The second, or negative, term of the gradient is inessence the expected gradient of the energy:EP[?E(y)] , (9)where P denotes p(y | y<t, x).The main idea of the proposed approach is toapproximate this expectation, or the negative termof the gradient, by importance sampling with asmall number of samples.
Given a predefined pro-posal distribution Q and a set V?of samples fromQ, we approximate the expectation in Eq.
(9) withEP[?E(y)] ??k:yk?V??k?k?:yk??V??k?
?E(yk),(10)where?k= exp {E(yk)?
logQ(yk)} .
(11)This approach allows us to compute the normal-ization constant during training using only a smallsubset of the target vocabulary, resulting in muchlower computational complexity for each param-eter update.
Intuitively, at each parameter update,we update only the vectors associated with the cor-rect word wtand with the sampled words in V?.Once training is over, we can use the full target vo-cabulary to compute the output probability of eachtarget word.Although the proposed approach naturally ad-dresses the computational complexity, using thisapproach naively does not guarantee that the num-ber of parameters being updated for each sen-tence pair, which includes multiple target words,is bounded nor can be controlled.
This becomesproblematic when training is done, for instance,on a GPU with limited memory.In practice, hence, we partition the training cor-pus and define a subset V?of the target vocabu-lary for each partition prior to training.
Beforetraining begins, we sequentially examine each tar-get sentence in the training corpus and accumulateunique target words until the number of unique tar-get words reaches the predefined threshold ?
.
Theaccumulated vocabulary will be used for this par-tition of the corpus during training.
We repeat thisuntil the end of the training set is reached.
Let usrefer to the subset of target words used for the i-thpartition by V?i.This may be understood as having a separateproposal distribution Qifor each partition of thetraining corpus.
The distribution Qiassigns equalprobability mass to all the target words included inthe subset V?i, and zero probability mass to all theother words, i.e.,Qi(yk) =??
?1|V?i|if yt?
V?i0 otherwise.This choice of proposal distribution cancels outthe correction term ?
logQ(yk) from the impor-tance weight in Eqs.
(10)?
(11), which makes theproposed approach equivalent to approximatingthe exact output probability in Eq.
(6) withp(yt| y<t, x)=exp{w>t?
(yt?1, zt, ct) + bt}?k:yk?V?exp{w>k?
(yt?1, zt, ct) + bk}.It should be noted that this choice of Q makes theestimator biased.The proposed procedure results in speed upagainst usual importance sampling, as it exploitsthe advantage of modern computers in doingmatrix-matrix vs matrix-vector multiplications.43.1.1 Informal Discussion on ConsequenceThe parametrization of the output probability inEq.
(6) can be understood as arranging the vectorsassociated with the target words such that the dotproduct between the most likely, or correct, targetword?s vector and the current hidden state is max-imized.
The exponentiation followed by normal-ization is simply a process in which the dot prod-ucts are converted into proper probabilities.As learning continues, therefore, the vectors ofall the likely target words tend to align with eachother but not with the others.
This is achieved ex-actly by moving the vector of the correct word inthe direction of ?
(yt?1, zt, ct), while pushing allthe other vectors away, which happens when thegradient of the logarithm of the exact output prob-ability in Eq.
(6) is maximized.
Our approximateapproach, instead, moves the word vectors of thecorrect words and of only a subset of sampled tar-get words (those included in V?
).3.2 DecodingOnce the model is trained using the proposed ap-proximation, we can use the full target vocabularywhen decoding a translation given a new sourcesentence.
Although this is advantageous as it al-lows the trained model to utilize the whole vocab-ulary when generating a translation, doing so maybe too computationally expensive, e.g., for real-time applications.Since training puts the target word vectors in thespace so that they align well with the hidden stateof the decoder only when they are likely to be acorrect word, we can use only a subset of candi-date target words during decoding.
This is similarto what we do during training, except that at testtime, we do not have access to a set of correct tar-get words.The most na?
?ve way to select a subset of candi-date target words is to take only the top-K mostfrequent target words, where K can be adjusted tomeet the computational requirement.
This, how-ever, effectively cancels out the whole purpose oftraining a model with a very large target vocabu-lary.
Instead, we can use an existing word align-ment model to align the source and target words inthe training corpus and build a dictionary.
With thedictionary, for each source sentence, we constructa target word set consisting of the K-most fre-quent words (according to the estimated unigramprobability) and, using the dictionary, at most K?likely target words for each source word.
K andK?may be chosen either to meet the computa-tional requirement or to maximize the translationperformance on the development set.
We call asubset constructed in either of these ways a candi-date list.3.3 Source Words for Unknown WordsIn the experiments, we evaluate the proposed ap-proach with the neural machine translation modelcalled RNNsearch (Bahdanau et al, 2015) (seeSec.
2.1.1).
In this model, as a part of decodingprocess, we obtain the alignments between the tar-get words and source locations via the alignmentmodel in Eq.
(5).We can use this feature to infer the source wordto which each target word was most aligned (in-dicated by the largest ?tin Eq.
(5)).
This isespecially useful when the model generated an[UNK] token.
Once a translation is generatedgiven a source sentence, each [UNK] may be re-placed using a translation-specific technique basedon the aligned source word.
For instance, in theexperiment, we try replacing each [UNK] tokenwith the aligned source word or its most likelytranslation determined by another word alignmentmodel.
Other techniques such as transliterationmay also be used to further improve the perfor-mance (Koehn, 2010).4 ExperimentsWe evaluate the proposed approach inEnglish?French and English?German trans-lation tasks.
We trained the neural machinetranslation models using only the bilingual, paral-lel corpora made available as a part of WMT?14.For each pair, the datasets we used are:?
English?French:2?
Common Crawl?
News Commentary?
Gigaword?
Europarl v7?
UN?
English?German:?
Common Crawl?
News Commentary?
Europarl v72The preprocessed data can be found and down-loaded from http://www-lium.univ-lemans.fr/?schwenk/nnmt-shared-task/README.5English-French English-GermanTrain Test Train Test15k 93.5 90.8 88.5 83.830k 96.0 94.6 91.8 87.950k 97.3 96.3 93.7 90.4500k 99.5 99.3 98.4 96.1All 100.0 99.6 100.0 97.3Table 1: Data coverage (in %) on target-side cor-pora for different vocabulary sizes.
?All?
refers toall the tokens in the training set.To ensure fair comparison, the English?Frenchcorpus, which comprises approximately 12 mil-lion sentences, is identical to the one used in(Kalchbrenner and Blunsom, 2013; Bahdanauet al, 2015; Sutskever et al, 2014).
As forEnglish?German, the corpus was preprocessed,in a manner similar to (Peitz et al, 2014; Li et al,2014), in order to remove many poorly translatedsentences.We evaluate the models on the WMT?14 testset (news-test 2014),3while the concatenationof news-test-2012 and news-test-2013 is usedfor model selection (development set).
Table 1presents data coverage w.r.t.
the vocabulary size,on the target side.Unless mentioned otherwise, all reported BLEUscores (Papineni et al, 2002) are computed withthe multi-bleu.perl script4on the cased tokenizedtranslations.4.1 SettingsAs a baseline for English?French translation, weuse the RNNsearch model proposed by (Bah-danau et al, 2015), with 30k source and targetwords.5Another RNNsearch model is trained forEnglish?German translation with 50k source andtarget words.For each language pair, we train another setof RNNsearch models with much larger vocab-ularies of 500k source and target words, usingthe proposed approach.
We call these modelsRNNsearch-LV.
We vary the size of the short-list used during training (?
in Sec.
3.1).
We tried3To compare with previous submissions, we use the fil-tered test sets.4https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl5The authors of (Bahdanau et al, 2015) gave us access totheir trained models.
We chose the best one on the validationset and resumed training.15k and 30k for English?French, and 15k and50k for English?German.
We later report the re-sults for the best performance on the developmentset, with models generally evaluated every twelvehours.
The training speed is approximately thesame as for RNNsearch.
Using a 780 Ti or TitanBlack GPU, we could process 100k mini-batchesof 80 sentences in about 29 and 39 hours respec-tively for ?
= 15k and ?
= 50k.For both language pairs, we also trained newmodels, with ?
= 15k and ?
= 50k, by reshufflingthe dataset at the beginning of each epoch.
Whilethis causes a non-negligible amount of overhead,such a change allows words to be contrasted withdifferent sets of other words each epoch.To stabilize parameters other than the word em-beddings, at the end of the training stage, wefreeze the word embeddings and tune only theother parameters for approximately two more daysafter the peak performance on the development setis observed.
This helped increase BLEU scores onthe development set.We use beam search to generate a translationgiven a source.
During beam search, we keepa set of 12 hypotheses and normalize probabili-ties by the length of the candidate sentences, as in(Cho et al, 2014a).6The candidate list is chosento maximize the performance on the developmentset, for K ?
{15k, 30k, 50k} and K??
{10, 20}.As explained in Sec.
3.2, we test using a bilin-gual dictionary to accelerate decoding and to re-place unknown words in translations.
The bilin-gual dictionary is built using fast align (Dyer etal., 2013).
We use the dictionary only if a wordstarts with a lowercase letter, and otherwise, wecopy the source word directly.
This led to betterperformance on the development sets.Note on ensembles For each language pair, webegan training four models from each of whichtwo points corresponding to the best and second-best performance on the development set were col-lected.
We continued training from each point,while keeping the word embeddings fixed, untilthe best development performance was reached,and took the model at this point as a single modelin an ensemble.
This procedure resulted in a to-tal of eight models from which we averaged thelength-normalized log-probabilities.
Since muchof training had been shared, the composition of6These experimental details differ from (Bahdanau et al,2015).6RNNsearch RNNsearch-LV Google Phrase-based SMTBasic NMT 29.97 (26.58) 32.68 (28.76) 30.6?33.3?37.03?+Candidate List ?
33.36 (29.32) ?+UNK Replace 33.08 (29.08) 34.11 (29.98) 33.1?+Reshuffle (?=50k) ?
34.60 (30.53) ?+Ensemble ?
37.19 (31.98) 37.5?
(a) English?FrenchRNNsearch RNNsearch-LV Phrase-based SMTBasic NMT 16.46 (17.13) 16.95 (17.85)20.67+Candidate List ?
17.46 (18.00)+UNK Replace 18.97 (19.16) 18.89 (19.03)+Reshuffle ?
19.40 (19.37)+Ensemble ?
21.59 (21.06)(b) English?GermanTable 2: The translation performances in BLEU obtained by different models on (a) English?French and(b) English?German translation tasks.
RNNsearch is the model proposed in (Bahdanau et al, 2015),RNNsearch-LV is the RNNsearch trained with the approach proposed in this paper, and Google is theLSTM-based model proposed in (Sutskever et al, 2014).
Unless mentioned otherwise, we report single-model RNNsearch-LV scores using ?
= 30k (English?French) and ?
= 50k (English?German).For the experiments we have run ourselves, we show the scores on the development set as well in thebrackets.
(?)
(Sutskever et al, 2014), (?)
(Luong et al, 2015), (?)
(Durrani et al, 2014), (?)
StandardMoses Setting (Cho et al, 2014b), () (Buck et al, 2014).such ensembles may be sub-optimal.
This is sup-ported by the fact that higher cross-model BLEUscores (Freitag et al, 2014) are observed for mod-els that were partially trained together.4.2 Translation PerformanceIn Table 2, we present the results obtained by thetrained models with very large target vocabular-ies, and alongside them, the previous results re-ported in (Sutskever et al, 2014), (Luong et al,2015), (Buck et al, 2014) and (Durrani et al,2014).
Without translation-specific strategies, wecan clearly see that the RNNsearch-LV outper-forms the baseline RNNsearch.In the case of the English?French task,RNNsearch-LV approached the performance levelof the previous best single neural machine transla-tion (NMT) model, even without any translation-specific techniques (Sec.
3.2?3.3).
With these,however, the RNNsearch-LV outperformed it.
Theperformance of the RNNsearch-LV is also betterthan that of a standard phrase-based translationsystem (Cho et al, 2014b).
Furthermore, by com-bining 8 models, we were able to achieve a trans-lation performance comparable to the state of theart, measured in BLEU.For English?German, the RNNsearch-LV out-performed the baseline before unknown word re-placement, but after doing so, the two systems per-formed similarly.
We could reach higher large-vocabulary single-model performance by reshuf-fling the dataset, but this step could potentiallyalso help the baseline.
In this case, we were ableto surpass the previously reported best translationresult on this task by building an ensemble of 8models.With ?
= 15k, the RNNsearch-LV performanceworsened a little, with best BLEU scores, with-out reshuffling, of 33.76 and 18.59 respectively forEnglish?French and English?German.The English?German ensemble described inthis paper has also been used for the shared trans-lation task of the 10thWorkshop on Statistical Ma-chine Translation (WMT?15), where it was rankedfirst in terms of BLEU score.
The translations bythis ensemble can be found online.74.3 Analysis4.3.1 Decoding SpeedIn Table 3, we present the timing information ofdecoding for different models.
Clearly, decodingfrom RNNsearch-LV with the full target vocab-7http://matrix.statmt.org/matrix/output/1774?run_id=40797CPU?GPU?RNNsearch 0.09 s 0.02 sRNNsearch-LV 0.80 s 0.25 sRNNsearch-LV0.12 s 0.05 s+Candidate listTable 3: The average per-word decoding time.Decoding here does not include parameter load-ing and unknown word replacement.
The baselineuses 30k words.
The candidate list is built withK = 30k and K?= 10.
(?)
i7-4820K (singlethread), (?)
GTX TITAN Blackulary is slowest.
If we use a candidate list fordecoding each translation, the speed of decodingsubstantially improves and becomes close to thebaseline RNNsearch.A potential issue with using a candidate list isthat for each source sentence, we must re-build atarget vocabulary and subsequently replace a partof the parameters, which may easily become time-consuming.
We can address this issue, for in-stance, by building a common candidate list formultiple source sentences.
By doing so, we wereable to match the decoding speed of the baselineRNNsearch model.4.3.2 Decoding Target VocabularyFor English?French (?
= 30k), we evaluate theinfluence of the target vocabulary when translat-ing the test sentences by using the union of a fixedset of 30k common words and (at most) K?likelycandidates for each source word according to thedictionary.
Results are presented in Figure 1.
WithK?= 0 (not shown), the performance of the sys-tem is comparable to the baseline when not replac-ing the unknown words (30.12), but there is not asmuch improvement when doing so (31.14).
As thelarge vocabulary model does not predict [UNK] asmuch during training, it is less likely to generateit when decoding, limiting the effectiveness of thepost-processing step in this case.
With K?= 1,which limits the diversity of allowed uncommonwords, BLEU is not as good as with moderatelylarger K?, which indicates that our models can, tosome degree, correctly choose between rare alter-natives.
If we rather use K = 50k, as we didfor testing based on validation performance, theimprovement over K?= 1 is approximately 0.2BLEU.When validating the choice of K, we found itto be correlated with the value of ?
used during100101102103K?32.833.033.233.433.633.834.034.2BLEUscoreWith UNK replacementWithout UNK replacementFigure 1: Single-model test BLEU scores(English?French) with respect to the number ofdictionary entries K?allowed for each sourceword.training.
For example, on the English?Frenchvalidation set, with ?
= 15k (and K?= 10), theBLEU score is 29.44 with K = 15k, but dropsto 29.19 and 28.84 respectively for K = 30k and50k.
For ?
= 30k, the score increases moder-ately from K = 15k to K = 50k.
A similareffect was observed for English?German and onthe test sets.
As our implementation of importancesampling does not apply the usual correction to thegradient, it seems beneficial for the test vocabular-ies to resemble those used during training.5 ConclusionIn this paper, we proposed a way to extend the sizeof the target vocabulary for neural machine trans-lation.
The proposed approach allows us to traina model with much larger target vocabulary with-out any substantial increase in computational com-plexity.
It is based on the earlier work in (Bengioand S?en?ecal, 2008) which used importance sam-pling to reduce the complexity of computing thenormalization constant of the output word proba-bility in neural language models.On English?French and English?Germantranslation tasks, we observed that the neural ma-chine translation models trained using the pro-posed method performed as well as, or betterthan, those using only limited sets of target words,even when replacing unknown words.
As per-formance of the RNNsearch-LV models increasedwhen only a selected subset of the target vocab-ulary was used during decoding, this makes theproposed learning algorithm more practical.When measured by BLEU, our models showedtranslation performance comparable to the8state-of-the-art translation systems on both theEnglish?French task and English?German task.On the English?French task, a model trainedwith the proposed approach outperformed the bestsingle neural machine translation (NMT) modelfrom (Luong et al, 2015) by approximately 1BLEU point.
The performance of the ensembleof multiple models, despite its relatively lessdiverse composition, is approximately 0.3 BLEUpoints away from the best system (Luong et al,2015).
On the English?German task, the bestperformance of 21.59 BLEU by our model ishigher than that of the previous state of the art(20.67) reported in (Buck et al, 2014).Finally, we release the source code used in ourexperiments to encourage progress in neural ma-chine translation.8AcknowledgmentsThe authors would like to thank the developersof Theano (Bergstra et al, 2010; Bastien et al,2012).
We acknowledge the support of the fol-lowing agencies for research funding and comput-ing support: NSERC, Calcul Qu?ebec, ComputeCanada, the Canada Research Chairs, CIFAR andSamsung.ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In ICLR?2015,arXiv:1409.0473.Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu,James Bergstra, Ian J. Goodfellow, Arnaud Berg-eron, Nicolas Bouchard, and Yoshua Bengio.
2012.Theano: new features and speed improvements.Deep Learning and Unsupervised Feature LearningNIPS 2012 Workshop.Yoshua Bengio and Jean-S?ebastien S?en?ecal.
2008.Adaptive importance sampling to accelerate train-ing of a neural probabilistic language model.
IEEETrans.
Neural Networks, 19(4):713?722.James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,Pascal Lamblin, Razvan Pascanu, Guillaume Des-jardins, Joseph Turian, David Warde-Farley, andYoshua Bengio.
2010.
Theano: a CPU andGPU math expression compiler.
In Proceedingsof the Python for Scientific Computing Conference(SciPy), June.
Oral Presentation.8https://github.com/sebastien-j/LV_groundhogChristian Buck, Kenneth Heafield, and Bas van Ooyen.2014.
N-gram counts and language models from thecommon crawl.
In Proceedings of the Language Re-sources and Evaluation Conference, Reykjav?
?k, Ice-land, May.Kyunghyun Cho, Bart van Merri?enboer, Dzmitry Bah-danau, and Yoshua Bengio.
2014a.
On theproperties of neural machine translation: Encoder?Decoder approaches.
In Eighth Workshop on Syn-tax, Semantics and Structure in Statistical Transla-tion, October.Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Fethi Bougares, Holger Schwenk, and YoshuaBengio.
2014b.
Learning phrase representa-tions using RNN encoder-decoder for statistical ma-chine translation.
In Proceedings of the EmpiricialMethods in Natural Language Processing (EMNLP2014), October.Nadir Durrani, Barry Haddow, Philipp Koehn, andKenneth Heafield.
2014.
Edinburgh?s phrase-basedmachine translation systems for WMT-14.
In Pro-ceedings of the Ninth Workshop on Statistical Ma-chine Translation, pages 97?104.
Association forComputational Linguistics Baltimore, MD, USA.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A simple, fast, and effective reparameter-ization of IBM Model 2.
In Proceedings of the2013 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 644?648, At-lanta, Georgia, June.
Association for ComputationalLinguistics.Mikel L. Forcada and Ram?on P.?Neco.
1997.
Re-cursive hetero-associative memories for translation.In Jos?e Mira, Roberto Moreno-D?
?az, and JoanCabestany, editors, Biological and Artificial Compu-tation: From Neuroscience to Technology, volume1240 of Lecture Notes in Computer Science, pages453?462.
Springer Berlin Heidelberg.Markus Freitag, Stephan Peitz, Joern Wuebker, Her-mann Ney, Matthias Huck, Rico Sennrich, NadirDurrani, Maria Nadejde, Philip Williams, PhilippKoehn, et al 2014.
Eu-bridge MT: Combined ma-chine translation.
In Proc.
of the Workshop on Sta-tistical Machine Translation, pages 105?113.M.
Gutmann and A. Hyvarinen.
2010.
Noise-contrastive estimation: A new estimation principlefor unnormalized statistical models.
In Proceedingsof The Thirteenth International Conference on Arti-ficial Intelligence and Statistics (AISTATS?10).Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentcontinuous translation models.
In Proceedings ofthe ACL Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 1700?1709.
Association for Computational Linguistics.9Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Vol-ume 1, NAACL ?03, pages 48?54.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press, New York, NY, USA,1st edition.Liangyou Li, Xiaofeng Wu, Santiago Cortes Vaillo, JunXie, Andy Way, and Qun Liu.
2014.
The DCU-ICTCAS MT system at WMT 2014 on German-English translation task.
In Proceedings of the NinthWorkshop on Statistical Machine Translation, pages136?141, Baltimore, Maryland, USA, June.
Associ-ation for Computational Linguistics.Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, OriolVinyals, and Wojciech Zaremba.
2015.
Addressingthe rare word problem in neural machine translation.In Proceedings of ACL.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In International Conferenceon Learning Representations: Workshops Track.Andriy Mnih and Koray Kavukcuoglu.
2013.
Learningword embeddings efficiently with noise-contrastiveestimation.
In C.J.C.
Burges, L. Bottou, M. Welling,Z.
Ghahramani, and K.Q.
Weinberger, editors, Ad-vances in Neural Information Processing Systems26, pages 2265?2273.
Curran Associates, Inc.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Stephan Peitz, Joern Wuebker, Markus Freitag, andHermann Ney.
2014.
The RWTH Aachen German-English machine translation system for WMT 2014.In Proceedings of the Ninth Workshop on Statisti-cal Machine Translation, pages 157?162, Baltimore,Maryland, USA, June.
Association for Computa-tional Linguistics.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural net-works.
In NIPS?2014.10
