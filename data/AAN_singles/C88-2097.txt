A Parser based on Connectionist ModelLliroshi NAKAGAWA , Tatsunori MORIl)ept.
Elect:ronics and Computer Engineering, Yokohama National University156 Tokiwadai Hodogaya-ku, Yokohama 240 Japannet-mail-address a36646@ec, cent.
u--tokyo, junetAbstractThis paper proposes a parser based ful ly upon theconneet:i.oni.st modeL(ca\]led "CM parser" hereafter).
Inorder to realize L~ile CM purser, we use Sigma-Pi-Units to implement ~ constraint of grammaticalcategory order or word order, and a copy mechanism ofsuh~parse trees.
Further more, we suppose there existweak suppressive connection lisks between every pairof CM units.
By these suppressive links, our CMparser explains why garden path sentences and/ordeeply nested sentences are hard to recognize.
Our CMparser also explains the preference principles forsyntact:\[cally ambJ guotls sentences.I.
IntroductionIn order to make clear a human parsing mechanismfor natural language sentences, there remain somephenomena that are difficult to be explained by one.integrated principl, e. These phenomena inc ludecognitive diff iculties to recognize garden pathsentences or deeply nested sentences, and preferenceof structurally ambiguous sentences.
All the parsingmechanisms proposed so far, for instances the top-down parsJngs /Pereira \]980/, the left corner parsing/.Johnson-.Laired ~983/, Marcus's parsing model/Marcus\]980/, Shieher's shift~rednce parser /Shleber 1983/,and so on, have not yet sncceeded to explain all ofthese phenomena under one s imple  in tegratedprinciple.
Note that all of them are based on symbolmanJ pu\]atien paradigm.Recently a connectionist model ( cal led CMhereafter ) approach has been noticed in many area ofcogn i t ive  science inc lud ing  hatura\], languagerecognition?
This approach has some advantages thatthe symbol manipulation approaches do not have.
Oneadvantage  is that it is easy to use not onlysyntactic informations but also semantic and/orcontextual informations in a uniform manner /Reilly19~4/.
One fruitful result of this approach is theexplanation about recognition of semantic garden pathsentences like "The astronomer married the star"/Waltz 1985/.
Another advantage is as follows.
Sincethe connectionist model is a parallel system withoutany central, controller I and an activation level ofeach unit and a connectlon strength between units maybe presented as continuous values\] it al ludes muchmore f lexible approaches than symbol manipulationapproaches do.
And we also expect it can simulatesome aspects of human mental processing of sentenceparsing.This paper is concerned with the secondadvantage in parsing.
The paper proposes a CM parserwhich can explain the above mentioned phenomena aspreferences etco in one integrated principle.2.
Parser based on conneetionist modelHere we omit the technical details of the CM/MeCle\].land&Rumelhart 1986/, but we must make clearthat we stand for the so cal led "localist" view inwhich one symbol corresponds to one unit.
Tbereforein our CM parser, syntactical categories like nounphrase are represented by a unit in the CM, and aparse tree is represented as a network in whichsuitable syntactical categories being activated areconnected.
In order to realize a CM parser, we haveto make clear the following two problems:(1)  How to express a word order or a syntacticalcategories order appearing in phrase structurerules.
For example~ in a rule S -~ NP VP, NP mustprecede VP.
(2) How to represent a ease when a parse tree isgenerated by recursive phrase structure rules.Consider rules as follows: S -~ NP VP, NP ---9 NP S and--> Comp S. The same pattern, in this case a patterncorresponding S-~ NP VP, may appear more than once :ina parse tree of one sentence.
In order to representthis case, we need a copy mechanism of a partialparse tree pattern corresponding to the phrasestructure rule in a connection network.
Otherwise wehave to prepare infinite number of copies of apartial parse tree pattern in advance?
Of coarse thispreparation is non-realistic not on computer hardwarebat on human we\]ware.
In Fauty's CM parser mentionedin /MeClelland&Kawamoto 1986/, the length of sentenceis l imited because of the above descr ibedpreparation.2.1 Phrase structure sub-networkConsider the next rule.C ---> A B (3)This rule has at least two meanings.
One is that: thecategory C cons ists  of the category A and thecategory B.
Another is that'\]he category B fol lowsthe category A.
This meaning is concerned directlywith the problem (i).
To represent a case that a wordis coincident with some syntactic category, wemodify (3) as follows.C -9 wordSince this rule is one variant of rule of type(3), westudy about only rules of type (3) hereafter.
We will.explain about a sub-network that corresponds to thephrase structure rule (3).We solve the problem (i) by introducing atrigger link that is presented as .-~-~ in figures.Namely " A ~.t > B"  expresses that B fol lows A. Fromthe viewpoint of the CMp the meaning of this triggerlink is that the unit for category B ( cal led "Bunit" hereafter) can be activated only when the unitfor category A (called "A unit" hereafter) is ful lyactivated.
Due to the trigger link, the A unit musthe activated chronologically faster than the B unit.The trigger link is realized by a Sigma-Pi-Unit/McCle l land & Rumelhart 1986/ that includes amult ip ly operation.
Figure 1 shows a concept ofSigma-Pi-Unit in the CM.F igure 1.
S igma-P i -Un i tIn  F igure  1, B and C are  CM un i ts .
They send  outputswhose va lues  a re  fb  and fc  expressed  as  pos : i t i veva lues  , to the A unit.
These va lues  arecorresponding to the B and C unit's activation levelsrespectively.
WIA is a weight of link from B and Cto A.
The input to the A unit is as follows.WlA*fb*feIf the B unit's activation level:fb=0~ then the Cunit's activation level, does not transmit to the Aunit at all.
\]:n other words, the B(or C) unit~sactivation level is an on-off switch for actJ vationtransmission from the C(or B) unit to the A unit~Using Sigma-Pi-Units,  a sub-network  of phrasestructure rule (3) is represented as shown in Figure2.
The weight WA> B is very small  in this case, butnote that it depends on some semantic information?45~CWA>C WB>CFigure 2o Sub--network of C -> A BThis network wil l  be presented in a simpler formusing a trigger link " A -~-~ B"  hereafter as shownin Figure 3.
A-, B-, and C-connectors' structuresappeared in Figure 3 are explained in Section 2.3?C~conuectorA.
-connector  B -connectornetwork is copied to the programmable sub-networksvia the connection activation system.
In order toimplement a copying mechanism of phrase structurerules in the form of C -~ A B , we use three CIDmechanisms.
They are for bidirectional connectionsbetween the A unit and the C unit, between the B unitand the C unit, and between the A unit and the B unitrespectively.
We omit the further details because ofthe limited paper space.Central network Connection Activation SystemFigure 4.
A simple connection infoprmationdistribution (CID) mechanismFigure 3.
Simpler form of Figure 2's network2.2 Copying sub-networkOur final goal is to make clear a mechanism ofbuilding a parse tree \]for a whole sentence byconnecting sub-networks.
For this purpose, thesimplest method is preparing parse trees of all thepossible sentence structures.
\]in principle thismethod is not possible, because there are infinitenumber of possible sentence structures.
Other methodis preparing a number of copies of a sub-network foreach phrase structure rule in advance.
For example~ten sub~-uetworks of S -~ NP VP, ten sub-networks ofVP -~ V NP, and so on.
When a parser reads asentence, it selects some sub-networks from theseprepared set of sub-networks, and connects them tomake a parse tree of the input sentence.
This methodseems to work well  and solves the above mentionedproblem (2).
Unfortunately this method has a seriousdeficiency as follows.
From the view point oflearning in the CM, all  the weights of connectionlinks of sub-networks are learned by parsing orrecognizing a number of sentences.
It is a plausiblehypothesis that once a human becomes to be able toparse some structure of sentence, he/she ever canparse that structure since then.
In order to explainthis hypothesis, the above mentioned weights learningmust be uniformly done for all copies of sub-networksof the same phrase structure rule.
But this uniformlylearning is too artificial for the human mentallearning processes.A solution avoiding these diff iculties is asfollows.
There is only one central sub-network forone phrase structure rule, and all learning processesare done on it.
In parsing, when a parser needs asub-network of some rule, the parser makes copies ofthe sub-network and connects them into a suitablep lace of a parse tree yet to be constructed.A sub-network copying mechanism is implementedas an application of the connection informationdistribution (CID) mechanism /McCle l land 1986/.F igure 4 is a s imple  example  of copying.
Theprogrammable sub-networks are implemented with theS igma-P i -Uni ts .
There are a lot of yet to beprogra~ned programmable sub-networks, namely blanksub--networks.
When the input comes in, thecorresponding connection pattern of the central2.3 Connecting sub-networksTo generate a parse tree, we need a mechanism ofgenerat ing  connect ion  l inks  dynamica l ly .Unfortunately the CM ham not yet had this mechanism.Instead of this mechanism, we use a connector thatchanges connection dynamically by SJgma-Pi-Un\]ts.There are three kinds of connector, namely A-, B-,and C-connector as shown in Figure 3o We will explainthese connectors' functions in this section?C-connector : If a C unit of a sub-network isact ivated,  the C-connector  sends requests forconnection to A-connectors of blank sub-networks orB-connectors whose sub-network's B unit is the samesyntactical category as the sender sub-network's Cunit's syntact i ca l  category.
More than oneconnections may be established by these requests,however, they suppress each other, and at last theconnection from the most strongly activated B'un:Ltwins.
Even if a C unit is not so strongly activated,the C-connector sends these requests.
Before a humanhas read a whole sentence, or even if he/she readsonly few words, he/she predicts a complete or fairlylarge part of parse tree of possible sentence, Thisis why we adopt this low threshold strategy ofrequests sending.A--connector : When an A-connector receives a requestfor connection from the other sub-network's C-connector, if the A-connector has not yet receivedany other requests for connecting, the A-connectormakes a copy of sub-network whose A unit's syntacticcategory is the same as the syntactic category of Cunit of the sender sub-network.
By this copying, aparse tree grows in bottom-up manner.B-connector : When a B-connector receives a requestfor connection from the other sub-network's C-connector, if the B unit's syntactic category is thesame as the sender sub-network's C unit's syntacticcategory, a connection between the sender's C-connector  and the receiver 's  B-connector  isestab l ished.
If more than one connections areestablisiled, they suppress each other.
Final ly themost strongly activated connection inhibits otherconnect ions.
This suppress ive  or exc lus iveconnect ions  are expressed as \[ X Y \] shown infigures~ \]in this expression, connections between Xand Y are mutually suppressive or exclusive?The above described connectors structure areshown in Figure 5,6 and 7 respectively?455\[?r ONIun i t  C. .
.
.
.
.
.  )
request~-r -  .
~-~~ <--~C'~ ac knowledge4 - ~ "  To A- or B-connectors..... :negative weight linkFigure 5.
C-connector (-~ :unitrequestacknowlodge,tom C connector _ I /9 .Figure 6.
A-connectorTo unit Arequestacknowledge <-From C-connectorrequestacknowledge <---Trigger input from A-v To unit BFigure 7.
B-connector2.4 Parsing on the CM parserTo summarize the above described CM parser, wesketch a parsing process of a sentence '!I eatapples."
Phrase structure rules used in this exampleare as follows.
S -9 N VP and VP -9 V N.Parsin~ ~rocess(I) The CM parser reads "I" , and a unit for categoryN is activated.
(2) The C-connector of the N unit sends a request forconnection to an A-connector of the currently usableblank sub-network.
(3) When an A-connector receives the request, itmakes a copy sub-network of S -9 N VP.
Since the Nunit of the copied sub-network is fully activated,the trigger link from the N unit to the VP unitbecomes active.
(4) Tile CM parser reads "eats", and a unit forcategory V is activated, and a request for connectionis sent from its C-connector to some A~connector.
(5) When an A-connector receives this request, itmakes a copy sub-network of VP -9 V N. Not only the Vunit but also the VP unit is activated.
Of coursethe trigger link from the V unit to the N unit isactivated.
(6) The VP unit sends a request for connection viaits C-connector.
This request is received by the B-connector of the previously copied sub-network forthe phrase structure rule S -~ N VP, because thissub-network's B unit's category is VP, and the sendersub~network's C unit's category is also VP andtriggered as you see at stage (3).
(7) The CM parser reads "apples", and a unit fo~category N is activated, and a request for connectionis seat from its C-connector.
(8) This request is received by the B-connector ofthe copied sub-network at(5).
This activates the Cunit of this sub-network whose category is VP.
This456activation causes that the B unit of the sub-networkof S -9 N VP.
Finally;.
its C unit whose category is Sbecomes ful ly activated, namely the sentence isrecognized and the parse tree is accomplished?The result parse tree is shown in Figure 8.
Forcompact expressions, the A- B- and C-connectors areomitted in the rest of the paper.S / \N t -> VPI IA-con B-conI IC-con C--conI I /\[%V NI IA-con B-conI IC-con C-conI IV Ni I_ ea t____ssFigure 8.
An example parse tree made by the CM parserIntuitionally, our CM parser is a parallel\[ \].eftcorner parser.
Speaking more precisely, owing to usea trigger link which predicts syntactic categoriesof the next incoming word, Our CM parser is regardedas a paral le l  left corner parser with a continuousactivation level for each generated nonterminalsymbolrepresentingsomesyntacticcategory.3.
Control on resource bounded conditionIt is well  known that a human memory systemconsists of at least two levels namely the short termmemory and the long term memory respectively.
Acapacity of short term memory is limited to 7 4~ 2chunks.
In the CM, an implementation of short termmemory has not yet been cleared.
But intuitionally,the sum of al l  units' activation level is bounded.We implement this bound by the almost equivalentmechanism as follows.
Namely there exist weaksuppressive connection links between every pairs ofunits.
Owing to this limitation, even if our CMparser is parallel one, it is impossible in parsingto maintain all possible candidate parse trees.
Sinceour parser is based on the CM, the most promisingparse tree is the most strongly activated one.
Otherparse trees are suppressed by the most promising onethrough the suppresszve or the exclusive connectionsdescribed in Section 2.3.
In the rest of the paper,we propose explanations for control mechanisms of theCM parser especially about parsings of deeply nestedsentences, garden path sentences and preferences ofsyntactically ambiguous sentences,4.
Recognition of deeply nested sentencesOur CM parser can explain why deeply nestedsentences like "The man who the girl who the dogchased liked laughed" are hard to recognize for ushuman.
Figure 9 shows a network being built justafter the CM parser reads "The mall who the girl whothe dog chased".
Here, since the NP 3 unit is stronglyactivated, the VP2/NP unit is strongly predicted andit is the right prediction.
But since the NP 1 unitand the S unit are also activated, the VP 1 unit isalso predicted.
Therefore when the CM parser reads"liked", it is not very easy to select the VP2/NPunit definitely.
As seen in this example, when the CMparser reads a word at the deeply nested level, theremay be a case that more than one units are stronglyactivated and predicted, If they have nearly the sameactivation level, it is not easy to select the rightunit.
Th:~s is one possible explanation why it is bardfor us human to recognize deeply nested sentences, ifthe CM is a plausible model of the human mentalprocess?S2NP2 .
.
.
.
.
.
.
.
~1Det~-k~--~ N Comp---- ~ - - -S/N P! '
I~ln wl\[o / P 3 ~De~t "~- " N Comp~S/NPI I i i \the ~ who NP~VP/NP..... ii I It h__?e ~ chasedFigure 9o A parse tree (connected network)just after "The man who the girl who the dog chased"5o Gard~,n path sentencesIf there are more than one possible syntacticstructures for the input sentence, the CM parsermakes more than one parse tree networks correspondingto them in a parsing process.
If one of them is muchmore strongly activated than others, the parsereasily ~e\]ects it as the right network.
But more thanone networks are often activated to almost the same\]evel.
\[n the case, how to select one of them dependson many factors, for instance a contextual or asemantic inforl,ation?
There is a worse case asfollows_ Assume that a parser reads some words of thesentence, and there are more than one parse trees.One of them has the highest activation level thanothers at that time.
But when the parser reads thenext word, if the highest parse tree turns out to besyntactically impossible, some weakly activated parsetree is forced to be activated to the highest levelsuddenly.
This forced sudden change of the activationlevel may cause us human a diff iculty to recognizethe sentence.
This is an informal explanation forcognitive diff iculty of recognizing garden pathsentencE ,  s .\]n order to explain what parse tree is chosen,we have to recognize which exclusive connection playsthe main role of preference between possible parsetrees.
Without loss of generality, it is sufficientto explain how one of two parse trees is chosen.
Inshort, this choice point is such that an upper partof tree from this point is common to the both trees,and a part of trees that are below this choice pointare different.
Figure i0 shows a network generatedfor a garden path sentence "The cotton clothing ismade of grows in Mississipi."
The wrong parse treeincluding the S~ unit is preferred while our CMparser reads "T~e cotton clothing is made of" ,because in the phrase structure rule ~ -~ S/Np, theconnect\[on link from the S unit to the ~n i t  isweak, and "clothing" is NP.
But when the CM parserreads "i~rows" , the wrong parse tree including the S aunit is rejected syn?actically, and the right butweakly predicted VP.
unit must be connected the VPunit for "grows".
~ybe humans feel cogn i t ivediff iculty at that time.
Note that although our CMparser should do a lot of works to parse a gardenpath sentence~ namely the forced sudden change ofactivation levels , finally it succeeds to parse thegarden path sentence as wel l  as human.
It is a maindifference of performance between our CM parser andShieber's shift reduce parser./ \ \ | rejected/ 2 ,y< /NP Mod~NP~-~- -VP /Np |vPThe cotton clothin~ is made of r f~Figure lO.
The parse tree network just after"The cotton clothing is made of grows"6.
Parsing PreferenceI f  there  are  more than one poss ib le  syntact i cs t ruc tures  for  the input  sentence  a f te r  the ent i resentence  was input ,  one of them is  p re fer red  overothers.
In order to expla in the parsing preferences,some syntact i ca l  preference pr inc ip les  such as Ri_i~Associat ion , Minimal Attachment and so on, have beenproposed so fa r  in  TFord 1982/ e ta l .
But there  aresome problems about  these  pr inc ip les .
The mostimportant problem is  which pr inc ip le  should be usedin parsing the given sentence /Schubert 1984/.
Sinceour parser is based on the CM, the parsingpreferences are uniformly explained using each of theactivation level of the units being the components ofparse tree for the given sentence.
This preferencemechanism with the activation levels is regarded asthe minimal attachment principle for some cases andus the right association principle for other cases.In this section, we wil l  show some examples aboutthis matter.The first example is about the sentence"John bought the book which I had selected for Mary.
"If we adopt the phrase structure rule VP -9 VP PP,the result parse tree of this sentence generated byour CM parser is the one shown in Figure Ii.John bought the book whi:h had selected for Mar LFigure ll.
The example of parse trees of structuralambiguous sentence457There are two promising parse trees for thissentence as shown in Figure ii.
I~ the tree includingthe VP 1 unit is preferred, the PP unit of "for Mary"is strongly cennected to the VPI/NP unit.
If the treeI nc lud ing  the  V} 2 unxt  xs pre fezred ,  Am I I  a \ ] t  .Lsstrongly connected to the VP 2 unit?
Now we examinethe activation levels of these two unit.
The VPl/NPunit .i.s activated direet\].y by the V ulit for "h~'{<'Iselected"?
It is also indirect\].y activated andtriggered by the N |,nit for "I"o On the other hand,the VP 2 unit is i nd i rec t ly  ac t ivaLed  by the V un i tfo r  "hougi~t" and the  NP un i t  fo r  " the  book which.
.
"and so on.
By th i s  compar i son ,  the VP1/NP un i t  i sknown to be more s t : roug ly  ac t ivated  than the VP 2un i t?
There fore  tile PP un i t  fo r  " fo r  Mary" :i.s more, .stroi\]f, l y connected  to the VP\] N / ~) unJ.
t than the.
Vl<2unit, and the parse tree including the VP\] unit ispre fer red .
The resu \ ] t  co inc ides  w\[ th t:fie r ig~l :associat:i.on pr inc ip le  that  i s  l i ke ly  used when humansparse th i s  example sentence?As you see in the example, many cases of theprocess of connecting to the most strongly activatedunit are exp la ined  as the r ight assoc ia t ionprinciple, gut there are other cases in which thecontrol mechanism are not so clear.
(\]onsider the nextexample.
"Johu carried the groceries for Mary.
"Here we phrase structure rules of the Chomsky aornmlform.
For instance, VP .--> VP PP, VP -> V NP, and soon.
The result pazse trees are shown in Figure \]2.?
Notice that the native speakers of English showdefinite preferences for the parse tree includingVP 2.
Now we are required to explain a parsing controlmechanism ~vhi.
ch causes this preference?
If the PPunit for "\]for Mary" Js connected to the NP I unit, theparse tree ieckuding the VP\] unit Js preferred.
Ifthe PP un i t  i s  connected  to the VP 2 un i t ,  the parset ree  inc lud ing  the VP 2 un i t  J s  p re fer red .
The NP\]unit is activated direct\] y by the NP unit for "thegroceries".
On the other hand, the VP 2 urlJt isact ivated  hy both the NP unit for "the grocer:ies" andl:he V unit for "carried" but indirectly.
We can notdetermine which parse tree :\[~ preferred withoutfurther information for instance, the weight of everyconnection \] :ink.
If the weight of the connection linkfrom the VP unit to the VP 2 unit is very heavy, ourparser prefer the parse tree including the VP 2 unit.From the v iewpo int  of phrase  s t ruc ture  ro:l.'es, hyth i s  connect ion  l ink ' s  heavy we ight ,  we can regardthe phrase s t ruc ture  ru les  VP --~ V NP and VP -~ VP PPms on\].y one ru le  VP -~ V NP PP.
Us ing th i s  ru le  inpars ing  min imizes  the resn J tant  number of nodes.
I fwe adopt the minimal attachment principle, the parsetree including the VP 2 unit is preferred.
In short~the minim~d, attachment principle is explained in ourparser's performance.SNP ~/ t t "~ ' -~Iv .
.
.
.
t _ _ \ [Np  NP \]NIP .
.
.
.
.
.
PP ..... ~ ppJohn car r ied  the fa\[9_cerigs fo_zjMarvFigure 12.
The parse trees for "John carried ??"
byVP --> VP PP and VP -} V NP etcoAs you know from these examp\].es, the minima\]attachment principle and the.
right: associationpr inc ip le  are in tegrated in our CM parser bydetermining the appropriate weights of connectionlinks.
This result is completely compatib_\]e the CM',qprinciple that all informatJons are represented asconnection \].ink's weights.7.
Conc \].usionsWe proposed a parser based ful ly on the CM.
'Byintroducing an upper bouud for the sin, of e-ach un:ittsactivation level into this CM parser, we can explainwhy garden path sentences and deeply nested sentencesare  hard to recogn ize?
Our CM parser  can in tegratethe min imal  a t tachment  pr inc i l ) le  and the rightassoc ia t ion  pr inc ip le  in to  one p l ' inc ip le  that  themost: s t rong ly  ac t ivated  unit  i s  se\].ected.
Future workto be s tud ied  i s  to un i fy  semant ic  and contextJuformationa into th i s  CM parser?A c k n owl e d g erie n i:We thank members of the spec ia l  interest groupof a r t i f i c ia l  in te l l igence  so ca l led  "AIUF, O'~ and i)c.EoEash~da t  ETL.
\[Iis e legant  theory encouraged us iostudy about the work of th i s  f ie ld .
The research  wassuppor ted  \]ly the Grant - in -A id  fo r  Spec ia l  P rs jec lResearch  o{ the min is t ry  of educat iou ,se : ience  andcu l ture ,  and the Inamor:i Foundation?ReferencesFord,M.
Bresnan, J .
& Kap lan~ R. i982) ,  "kcompetence--based theory of  syntact i c  c losure" ,  inBresnan, J ?
(ed . )
,  The Menta l  Representat ion  ofGrammat::i.ca\] Relatious~ M\]:T PressJonson-Laired,P.N.
(1983), "Menta l  Mode ls"Cambridge \[Iniversity PressMarcus,M.
(1980)"A Theory of Syntactic RecognJ giollfor Natural Language" MIT PressMcCleil.and,J.L.
(1986), "The Programmable BlackboardModel  o f  Read ing" ,  in  Para l le l  D is t r ibutedProcessing Volo2, The MIT PressMcClelland,J.L2~ Kawamoto,A.H.
(1986), "Mechanismsof Sentence Process ing :Ass ign ing  Roles  toConstituents", in Parallel Distributed ProcessingVol.
2, The MIT PressMcClelland,J.L.
& Rumelhart,D.g.
eLal.
(1986) ,Parallel Distributed Processing Vol.l Vo\].
?2~ TheMIT PressPereira,F.C~N.
& Warren,DoII.D (1980) "Definite ClauseGrmmner for Language Analysis", Artifo Intel\].. 13Reil I y,RoG~Aspects ofCOTING'84(1984) "A Connectionist Model of someAnaphor Resolution"pp.144-149Schuber t , l , .K .
(1984)  UOn Pars Jng  Pre ferences"~COLING~ 84 pp.
24"1-250Shieber,SoMo (1983) "Sentence  d isambiguat io r ,  by ashift-reduce parsing ' " techn\].que , SRI in ternat iona l .Technical Note 281Waltz,D.I,.
& Pollack,J,B.
(\].985) " Massively Para\].ie\]Parsing:A Strongly Interactive Model of NaturalLanguage Interpretation", Cognitive Science 9,pp.51-74458
