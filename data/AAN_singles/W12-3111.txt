Proceedings of the 7th Workshop on Statistical Machine Translation, pages 104?108,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsPRHLT Submission to the WMT12 Quality Estimation TaskJesu?s Gonza?lez Rubio and Alberto Sanchis and Francisco CasacubertaD.
Sistemas Informa?ticos y Computacio?nUniversitat Polite`cnica de Vale`nciaCamino de vera s/n, 46022, Valencia, Spain{jegonzalez,josanna,fcn}@dsic.upv.esAbstractThis is a description of the submissions madeby the pattern recognition and human lan-guage technology group (PRHLT) of the Uni-versitat Polite`cnica de Vale`ncia to the qual-ity estimation task of the seventh workshopon statistical machine translation (WMT12).We focus on two different issues: how to ef-fectively combine subsequence-level featuresinto sentence-level features, and how to selectthe most adequate subset of features.
Resultsshowed that an adequate selection of a subsetof highly discriminative features can improveefficiency and performance of the quality esti-mation system.1 IntroductionQuality estimation (QE) (Ueffing et al, 2003; Blatzet al, 2004; Sanchis et al, 2007; Specia and Farzin-dar, 2010) is a topic of increasing interest in machinetranslation (MT).
It aims at providing a quality indi-cator for unseen translations at various granularitylevels.
Different from MT evaluation, QE do notrely on reference translations and is generally ad-dressed using machine learning techniques to pre-dict quality scores.Our main focus in this article is in the combi-nation of subsequence features into sentence fea-tures, and in the selection of a subset of relevant fea-tures to improve performance and efficiency.
Sec-tion 2 describes the features and the learning algo-rithm used in the experiments.
Section 3 describetwo different approaches implemented to select thebest-performing subset of features.
Section 4 dis-plays the results of the experimentation intended todetermine the optimal setup to train our final sub-mission.
Finally, section 5 summarizes the submis-sion and discusses the results.2 Features and Learning Algorithm2.1 Available Sources of InformationThe WMT12 QE task is carried out on English?Spanish news texts produced by a phrase-based MTsystem.
As training data we are given 1832 trans-lations manually annotated for quality in terms ofpost-editing effort (scores in the range [1, 5]), to-gether with their source sentences, decoding in-formation, reference translations, and post-editedtranslations.
Additional training data can be used,as deemed appropriate.
Any of these informationsources can be used to extract the features, however,test data consists only on source sentence, transla-tion, and search information.
Thus, features wereextracted from the sources of information availablein test data only.
Additionally, we compute someextra features from the WMT12 translation task(WMT12TT) training data.2.2 FeaturesWe extracted a total of 475 features classified intosentence-level and subsequence-level features.
Weconsidered subsequences of sizes one to four.Sentence-level features?
Source and target sentence lengths, and ratio.?
Proportion of dead nodes in the search graph.?
Number of source phrases.?
Number and average size of the translation op-tions under consideration during search.104?
Source and target sentence probability and per-plexities computed by language models of or-der one to five.?
Target sentence probability, probability dividedby sentence length, and perplexities computedby language models of order one to five.
Lan-guage models were trained on the 1000-besttranslations.?
1000-best average sentence length, 1000-bestvocabulary divided by average length, and1000-best vocabulary divided by source sen-tence length.?
Percentage of subsequences (sizes one to four)previously unseen in the source training data.Subsequence-level features?
Frequency of source subsequences in theWMT12TT data.?
IBM Model-1 confidence score for each wordin the translation (Ueffing et al, 2003).?
Subsequence confidence scores computed on1000-best translations as described in (Ueffinget al, 2003; Sanchis et al, 2007).
We usefour subsequence correctness criteria (Levens-thein position, target position, average position,and any position) and three weighting schemes(translation probability, translation rank, andrelative frequencies).?
Subsequence confidence scores computed by asmoothed na?
?ve bayes classifier (Sanchis et al,2007).
We computed a confidence score foreach correctness criteria (Levensthein, target,average and any).
The smoothed classifier wastuned to improve classification error rate on aseparate development set (union of news-testsets for years 2008 to 2011).2.3 Combination of Subsequence-levelFeaturesSince WMT12 focuses on sentence-level QE,subsequence-level features must be combined to ob-tain sentence-level indicators.
We used two differentmethods to combine subsequence features:?
Average value of subsequence-level scores, asdone in (Blatz et al, 2004).?
Percentage of subsequence scores belonging toeach frequency quartile1, as done in (Speciaand Farzindar, 2010).Thus, each subsequence-level feature was repre-sented as five sentence-level features: one averagescore plus four quartile percentages.Both methods aim at summarizing the scores ofthe subsequences in a translations.
The average isa rough indicator that measures the ?middle?
valueof the scores while the percentages of subsequencesbelonging to each quartile are more fine-grained in-dicators that try to capture how spread out the sub-sequence scores are.2.4 Learning AlgorithmWe trained our quality estimation model using animplementation of support vector machines (Vap-nik, 1995) for regression.
Specifically, we usedSVMlight (Joachims, 2002) for regression with a ra-dial basis function kernel with the parameters C, wand ?
optimized.
The optimization was performedby cross-validation using ten random subsamples ofthe training set (1648 samples for training and 184samples for validation).3 Feature SelectionOne of the principal challenges that we had to con-front is the small size of the training data (only1832 samples) in comparison with the large numberof features, 475.
This inadequate amount of train-ing data did not allow for an acceptable training ofthe regression model which yielded instable systemswith poor performance.
We also verified that manyfeatures were highly correlated and were even re-dundant sometimes.
Since the amount of trainingdata is fixed, we tried to improve the robustness ofour regression systems by selecting a subset of rele-vant features.We implemented two different feature selectiontechniques: one based on partial component anal-ysis (PCA), and a greedy selection according to theindividual performance of each feature.3.1 PCA Selection (PS)Principal component analysis (Pearson, 1901)(PCA) is a mathematical procedure that uses an or-1Quartile values were computed on the WMT12TT data.1050.340.360.380.40.420.440.4650  100  150  200  250  300  350  400DeltaAverageNumber of featuresGS PS Baseline(a) Delta average score0.550.560.570.580.590.60.610.620.6350  100  150  200  250  300  350  400MAENumber of featuresGS PS Baseline(b) Mean Average ErrorFigure 1: Delta average score (a) (higher is better) and mean average error (b) (lower is better) as a function of thenumber of features.
Cross-validation results for PCA selection (PS), and greedy selection (GS) methods.thogonal transformation to convert a set of observa-tions of possibly correlated variables into a set ofvalues of linearly uncorrelated variables called prin-cipal components.
This transformation is defined insuch a way that the first principal component hasthe largest possible variance (that is, accounts for asmuch of the variability in the data as possible), andeach succeeding component in turn has the highestvariance possible under the constraint that it be un-correlated with the preceding components.
Strictlyspeaking, PCA does not perform a feature selectionbecause the principal components are linear combi-nations of the individual features.PCA generates sets of features (the principal com-ponents) with almost no correlation.
However, it ig-nores the quality scores to be predicted.
Since wewant to obtain the best-performing subset of fea-tures, there is a mismatch between the selection cri-terion of PCA and the criterion we are interested in.In other words, although the features generated byPCA contain almost no redundancy, they do not nec-essarily have to constitute the best-performing sub-set of features.3.2 Greedy Performance-driven Selection (GS)We also implemented a greedy feature selectionmethod which iteratively creates subsets of increas-ing size with the best-scoring individual features.The score of each feature is given by the perfor-mance of a system trained solely on that feature.
Ata given iteration, we select the K best scoring fea-tures and train a regression system with them.Since we select the features incrementally accord-ing to their individual performance, we expect to ob-tain the subset of features that yield the best perfor-mance.
However, we do not take into account thecorrelations that may exist between the different fea-tures, thus, the final subset is almost sure to containa large number of redundant features.4 Experiments4.1 Assessment MeasuresThe organizers propose two variations of the taskthat will be evaluated separately:Ranking: Participants are required to submit aranking of translations.
This ranking will usedto split the data into n quantiles.
The evalua-tion will be performed in terms of delta averagescore, the average difference over n betweenthe scores of the top quantiles and the overallscore of the corpus.
The Spearman correlationwill be used as tie-breaking metric.Scoring: Participants are required to assign a scorein the range [1, 5] for each translation.
Theevaluation will be performed in terms of meanaverage error (MAE).
Root mean squared error(RMSE) will be used as tie-breaking metric.4.2 Pre-Submission ResultsWe now describe a number of experiments whosegoal is to determine the optimal training setup.106Specifically, we wanted to determine which selec-tion method to use (PCA or greedy) and which fea-tures yield a better system.
As a preliminary step,we extracted all the features described in section 2.The complete training data consisted on 1832 sam-ples each one with 475 features.We trained systems using feature sets of increas-ing size as given by PCA selection (PS) or greedyselection (GS).
The parameters of each system weretuned to optimize each of the evaluation measuresunder consideration.
Performance was measured asthe average of a ten-fold cross-validation experimenton the training data.Figure 1 shows the results obtained for the ex-periments that optimized delta average, and MAE(result optimizing Spearman and RMSE were quitesimilar).
We also display the performance of a sys-tem trained on the baseline features.
We observedthat both selection methods yielded a better perfor-mance than the baseline system.
PS allowed for aquick improvement in performance as more featuresare selected, reaching its best results when select-ing approximately 80 features.
After that, perfor-mance rapidly deteriorate.
Regarding GS, its im-provements in performance were slower in com-parison with PS.
However, GS finally reached thebest scores of the experimentation when selecting?
225 features.
Specifically, the best performancewas reached using the top 222 features for delta av-erage, and using the top 254 features for MAE.According to these results, our submissions weretrained on the best subsets of features as given bythe GS method.
222 features were selected accord-ing to their delta average score for the ranking taskvariation, and 254 according to their MAE value forthe scoring task variation.
Final submissions weretrained on the complete training set.Most of the selected features are sentence-levelfeatures calculated from subsequence-based scores.For instance, among the 222 features of the rank-ing variation of the task, 174 were computed fromsubsequence scores.
Among these 174 features,129 were calculated from confidence scores com-puted on 1000-best translations, 29 from confidencescores computed by a smoothed na?
?ve bayes classi-fier, 11 from the frequencies of the subsequences inthe WMT12TT data, and 5 from IBM Model-1 wordconfidence scores.Participant ID Delta average?
MAE?SDL Language Weaver 0.63 0.61Uppsala U.
0.58 0.64LORIA Institute ?
0.68Trinity College Dublin 0.56 0.68Baseline 0.55 0.69PRHLT 0.55 0.70U.
Edinburgh 0.54 0.68Shanghai Jiao Tong U.
0.53 0.69U.
Wolverhampton/Sheffield 0.51 0.69DFKI 0.46 0.82Dublin City U.
0.44 0.75U.
Polite`cnica Catalunya 0.22 0.84Table 1: Best official evaluation results on each task ofthe different participating teams.
Results for our submis-sions are displayed in bold.
Baseline results in italics.-10-50 510 1520 25301  2  3  4  5  6  7  8  9  10 11 12 13 14 15MeanvalueFeature numberTrain data Test dataFigure 2: Average value (?
std.
deviation) of the first15 features used in our final submissions.
Feature valuesfollow a similar distribution in the training and test data.4.3 Official Evaluation ResultsAfter establishing the optimal training setup, wenow show the official evaluation results for our sub-missions.
Table 1 shows the performance of the var-ious participants in the ranking (delta average) andscoring (MAE) tasks.
Surprisingly our submissionsyielded a slightly worse result than the baseline fea-tures.
However, given the large improvements overthe baseline system obtained in the pre-submissionexperiments, we expected to obtain similar improve-ments over Baseline in test.We considered two possible explanations for thiscounterintuitive result.
First, a possibly divergencebetween the underlying distributions of the trainingand test data.
To investigate this possibility, we stud-107ied the distributions of feature values in the trainingand test data.
Figure 2 displays mean?std.
deviationfor the first 15 features used in our final submissions(similar results are obtained for all the 222 features).We can observe that feature values in training andtest data follow a similar distribution, although testvalues tend to be slightly lower than training values.A second plausible explanation is the smallamount of training data (only 1832 samples).
Lim-ited data favors simpler systems that can train its fewfree parameters more accurately.
This is the case ofthe Baseline system that was trained using only 11features, in comparison with the 222 features usedin our submissions.
Since the training and test dataseem to have been generated following the same un-derlying distribution, we hypothesize that the lim-ited training data is the main explanation for the poortest performance of our submissions.5 Summary and DiscussionWe have presented the submissions of the PRHLTgroup to the WMT12 QE task.
The estimation sys-tems were based on support vector machines for re-gression.
Several features were used to train thesystems in order to predict human-annotated post-editing effort scores.
Our main focus in this articlehave been the combination of subsequence featuresinto sentence features, and the selection of a subsetof relevant features to improve the submitted sys-tems performance.Results of the experiments showed that PCAselection was able to obtain better performancewhen selecting a small number of features whileGS yielded the best-performing systems but us-ing much more features.
Among the selected fea-tures, the larger percentage of them were calculatedfrom subsequence features.
These facts indicatethat the combination of subsequence features yieldssentence-level features with a strong individual per-formance.
However, the high number of features se-lected by GS indicate that these top-scoring featuresare highly correlated.Official evaluation results differ from what weexpected; baseline system performs better thanour submissions while pre-submission experimentsyielded just opposite results.
After discarding a pos-sibly discrepancy between training and test data dis-tributions, and given that smaller models such as thebaseline system can be trained more accurately withlimited data, we concluded that the limited trainingdata is the main explanation for the disparity be-tween our training and test results.A future line of research could be the study ofmethods that allow to select sets of uncorrelated fea-tures, that unlike PCA, also take into account the in-dividual performance of each feature.
Specifically,we plan to study a features selection technique basedon partial least squares regression.AcknowledgmentsThe research leading to these results has receivedfunding from the European Union Seventh Frame-work Programme (FP7/2007-2013) under grantagreement no 287576.
Work also supported bythe EC (FEDER/FSE) and the Spanish MEC underthe MIPRCV ?Consolider Ingenio 2010?
program(CSD2007-00018) and iTrans2 (TIN2009-14511)project and by the Generalitat Valenciana undergrant ALMPR (Prometeo/2009/01).ReferencesJohn Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2004.
Confidence estimation formachine translation.
In M. Rollins, editor, Mental Im-agery.
Yale University Press.Thorsten Joachims.
2002.
SVM light.Karl Pearson.
1901.
On lines and planes of closest fit tosystems of points in space.
Philosophical Magazine,2(11):559?572.Alberto Sanchis, Alfons Juan, and Enrique Vidal.
2007.Estimation of confidence measures for machine trans-lation.
In In Procedings of the MT Summit XI.Springer-Verlag.Lucia Specia and Atefeh Farzindar.
2010.
Estimat-ing machine translation post-editing effort with hter.In AMTA 2010- workshop, Bringing MT to the User:MT Research and the Translation Industry.
The NinthConference of the Association for Machine Transla-tion in the Americas, nov.Nicola Ueffing, Klaus Macherey, and Hermann Ney.2003.
Confidence measures for statistical machinetranslation.
In In Proceedings of the MT Summit IX,pages 394?401.
Springer-Verlag.Vladimir N. Vapnik.
1995.
The nature of statisticallearning theory.
Springer-Verlag New York, Inc., NewYork, NY, USA.108
