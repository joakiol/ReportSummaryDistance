Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 98?108,Baltimore, Maryland USA, 27 June 2014.c?2014 Association for Computational LinguisticsUser Type Classification of Tweets with Implications for EventRecognitionLalindra De Silva and Ellen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112{alnds,riloff}@cs.utah.eduAbstractTwitter has become one of the foremostplatforms for information sharing.
Conse-quently, it is beneficial for the consumersof Twitter to know the origin of a tweet,as it affects how they view and inter-pret this information.
In this paper, weclassify tweets based on their origin, ex-ploiting only the textual content of tweets.Specifically, using a rich, linguistic fea-ture set and a supervised classifier frame-work, we classify tweets into two usertypes - organizations and individual per-sons.
Our user type classifier achieves an89% F1-score for identifying tweets thatoriginate from organizations in Englishand an 87% F1-score for Spanish.
Wealso demonstrate that classifying the usertype of a tweet can improve downstreamevent recognition tasks.
We analyze sev-eral schemes that exploit user type infor-mation to enhance Twitter event recogni-tion and show that substantial improve-ments can be achieved by training separatemodels for different user types.1 IntroductionTwitter has become one of the most widely usedsocial media platforms, with users (as of March2013) posting approximately 400 million tweetsper day (Wickre, 2013).
This public data servesas a potential source for a multitude of informa-tion needs, but the sheer volume of tweets is a bot-tleneck in identifying relevant content (Becker etal., 2011).
De Choudhury et al.
(2012) showedthat the user type of a Twitter account is an impor-tant indicator in sifting through Twitter data.
Theknowledge of a tweet?s origin has potential impli-cations on the nature of the content to an end user(e.g., credibility, location, etc).
Also, certain typesof events are more likely to be reported by individ-ual persons (e.g., local events) whereas organiza-tions generally report events that are of interest toa wider audience.The first part of our research focuses on usertype classification in Twitter.
De Choudhury etal.
(2012) addressed this problem by examiningmeta-information derived from the Twitter API.In contrast, the goal of our work is to classifytweets, based solely on their textual content.
Wehighlight several reasons why this can be advanta-geous.
One reason is that people frequently sharecontent from other sources, but the shared con-tent often appears in their Twitter timeline as ifit was their own.
Consequently, a tweet that wasposted by an individual may have originated froman organization.
Moreover, meta-information cansometimes be infeasible to obtain given the ratelimits1and there are times when profile informa-tion for a user account is unavailable or ambigu-ous (e.g., users often leave their profile informa-tion blank or write vague entries).
Therefore, webelieve there is value in being able to infer thetype of user who authored a tweet based solely onits textual content.
Potentially, our methods foruser type classification based on textual contentcan also be combined with methods that examineuser profile data or other meta-data, since they arecomplementary sources of information.In this paper, we present a classifier that tries todetermine whether a tweet originated from an or-ganization or a person using a rich, linguistically-motivated feature set.
We design features to rec-ognize linguistic characteristics, including senti-ment and emotion expressions, informal languageuse, tweet style, and similarity with news head-lines.
We evaluate our classifier on both Englishand Spanish Twitter data and find that the classifierachieves an 89% F1-score for identifying tweetsthat originate from organizations in English and a1https://dev.twitter.com/docs/rate-limiting/1.1/limits9887% F1-score for Spanish.The second contribution of this paper is todemonstrate that user type classification can im-prove event recognition in Twitter.
We conduct astudy of event recognition for civil unrest eventsand disease outbreak events.
Based on statisticsfrom manually annotated tweets, we found thatorganization-tweets are much more likely to men-tion these events than person-tweets.
We then in-vestigate several approaches to incorporate usertype information into event recognition models.Our best results are produced by training sepa-rate event classifiers for tweets from different usertypes.
We show that user type information con-sistently improves event recognition performancefor both civil unrest events and disease outbreakevents and for both English and Spanish tweets.2 Related WorkOur work is most closely related to that of DeChoudhury et al.
(2012), which proposed methodsto classify Twitter users into three categories: 1)Journalists/media bloggers, 2) Organizations and3) Ordinary Individuals.
They employed featuresderived from social network structure, user ac-tivity and users?
social interaction behaviors, andnamed entities and historical topic distributions intweets.
In contrast, our work classifies isolatedtweets into two different user types, based on theirtextual content.
Consequently, our work can pro-duce different user type labels for different tweetsby the same user, which can help identify sharedcontent not authored by the user.Another body of related work tries to classifyTwitter users along other dimensions such as eth-nicity and political orientation (Pennacchiotti andPopescu, 2011; Cohen and Ruths, 2013).
Genderinference in Twitter has also garnered interest inthe recent past (Ciot et al., 2013; Liu and Ruths,2013; Fink et al., 2012).
Researchers have also fo-cused on user behaviors showcased in Twitter in-cluding the types of messages posted (Naaman etal., 2010), social connections (Wu et al., 2011),user responses to events (Popescu and Pennac-chiotti, 2011) and behaviors related to demograph-ics (Volkova et al., 2013; Mislove et al., 2011; Raoet al., 2010).Event recognition is another area that continuesto attract a lot of interest in social media.
Previ-ous work has investigated event identification andextraction (Jackoway et al., 2011; Becker et al.,2009; Becker et al., 2010; Ritter et al., 2012),event discovery (Benson et al., 2011; Sakaki et al.,2010; Petrovi?c et al., 2010), tracking events overtime (Kim et al., 2012; Sayyadi et al., 2009) andevent retrieval over archived Twitter data (Metzleret al., 2012).
While our work focuses on user typeclassification, we show that the user type of a tweetis an important piece of information that can bebeneficial in event recognition models.3 Twitter User TypesTwitter user types can be analyzed in differentgranularities and across different dimensions.
Wefollow a high-level categorization of user typesinto organizations and individual persons.
Whilewe acknowledge the existence of other user types,such as automated bots, we focus only on the orga-nization and individual person user types for ourresearch.?
Banking Commission Split Over EU BonusCap http://t.co/GSSbmHAWsQ?
Apple likely to introduce smaller, cheaperiPad mini today http://t.co/TuKBHZ3z?
Diet Coke may be the new #2, but U.S. sodamarket is shrinking http://ow.ly/1bSNnhSample Tweets from Organizations?
@john It?s a stress fracture.
Nah, no dancingwas involved!?
My gawd feels like my head?s gonna explode?
Watching The Rainmaker.
It has totallysucked me in :D #notsomuch lolSample Tweets from Individual PersonsFigure 1: Sample tweets from individual personsand organizationsFrom a linguistic point of view, we can ob-serve several distinguishing characteristics be-tween organization- and person-tweets.
As shownin Figure 1, organization-tweets are often char-acterized by headline-like language usage, struc-tured style, a lack of conversation with the au-dience (i.e., few reply-tweets), and hyperlinks tooriginal articles.
In contrast, person-tweets showsignificant language variability including short-hand terms, conversational behavior, slang andprofanity, expressions of emotion, and an overallrelaxed usage of language.993.1 Data Acquisition for User TypesTo create our data sets, we archived tweets (us-ing Twitter Streaming API) for six months, be-ginning February 1st, 2013.
We then used a lan-guage filter (Lui and Baldwin, 2012) to separateout the English and Spanish tweets.
Also, in thedata sets we created (see below), we removed du-plicates, retweets and any tweet with less than 5words.
Given that large-scale human annotationis expensive, we explored several heuristics to re-liably compile a large gold standard collection ofperson- and organization-tweets.3.1.1 Acquiring Person-tweetsTo acquire person-tweets, we devised a personheuristic, focusing on the name and the profile de-scription fields in each user account correspond-ing to a tweet.
We first gathered lists of personnames (first names and surnames), for both En-glish and Spanish, using census data2and onlineresources3.
We also compiled a list of commonorganization terms (e.g., agency, institute, com-pany, etc) in both English and Spanish.The person heuristic labels a tweet as a person-tweet if[no organization term is in the name orthe profile description fields]AND[all the wordsin the name field are person names OR the profiledescription field starts with either ?I?m?
or ?I am?]4.
To assess the accuracy of the person heuris-tic, we also performed a manual annotation task.We employed two annotators and provided themwith guidelines of what constitutes an individualperson?s Twitter account.
We defined an individ-ual person as someone who uses Twitter in theirday-to-day life to post information about his/herdaily activities, update personal status messages,comment about societal issues and/or interact withclose social circles.
The annotators were able tosee the name, profile description, location and urlfields of the Twitter user account and were asked tolabel each account as individual, not individual orundetermined.
To calculate annotator agreementbetween the two annotators, we gave them 100Twitter accounts, corresponding to English tweetscollected using the person heuristic.
The inter-annotator agreement (IAA) was .98 (raw agree-ment) and .97 (G-Index score).
We did not use2http://www.census.gov/genealogy/www/data/1990surnames/names_files.html3http://genealogy.familyeducation.com/browse/origin/spanish4Corresponding terms were used for SpanishCohen?s Kappa (?)
as it is known to underestimateagreement (known as Kappa Paradox) when onecategory dominates.
We then released another 250accounts to each of the annotators, giving us a totalof 600 manually labeled accounts5.In the distribution of labels assigned by the hu-man annotators for these 600 accounts, 91.5% wasconfirmed as belonging to individual persons.
5%was identified as not individual whereas 3.5% waslabeled as undetermined.
These numbers corrob-orate our claim that the person heuristic is a validapproximation for acquiring person-tweets.However, limiting our person-tweets to thosefrom accounts identified with the person heuris-tic could introduce bias (i.e., it may consider onlythe people who provided more complete profileinformation).
To address this issue, we lookedinto additional heuristics that are representativeof individual persons?
Twitter accounts.
We ob-served that applications designed specifically forhand-held devices (e.g., twitter for iphone) are fre-quently used to author tweets and used by individ-ual persons.
Organizations, on the other hand, pri-marily use the Twitter web tool and content man-agement software applications to create, manageand post content to Twitter.To further investigate our observation, we ex-tracted the source information (i.e., the softwareapplications used to author tweets) for a collectionof 1.2 million English tweets from our tweet pool,for a random day, and identified those that wereclearly hand-held device apps and covered at least1% of the tweets.
Table 1 shows the distributionof these hand-held device apps, which together ac-counted for approximately 66% of all tweets.Hand-held Device App % of Tweetstwitter for iphone 37.11twitter for android 16.50twitter for blackberry 5.50twitter for ipad 2.55mobile web (m2) 1.46ios 1.36echofon 1.29ALL 65.77Table 1: Percentage of (English) tweets authoredfrom hand-held device appsTo evaluate our hypothesis that a high percent-age of these tweets are person-tweets, we carriedout another manual annotation task.
We selected5We adjudicated the disagreements in the initial 100 Twit-ter accounts.100100 English Twitter accounts whose tweets weregenerated using one of the above hand-held de-vice apps and asked the two annotators to labelthem using the same guidelines.
For this task, theIAA was .84 (raw agreement) and .76 (G-Indexscore).
As before, we released another 250 ac-counts to each of the annotators.
In these 600 useraccounts, 87.1% was confirmed to be individualpersons.
Only 1% was judged to be clearly notindividual whereas 11.9% was labeled as unde-termined.3.1.2 Acquiring Organization-tweetsDesigning similar heuristics to identifyorganization-tweets proved to be difficult.Organizations describe themselves in numerousways, making it difficult to automatically identifytheir names in user profiles.
Furthermore, organi-zation names often appear in individual persons?accounts because they mention their employers(e.g., I?m a software engineer at Microsoft Corpo-ration).
Therefore, to acquire organization-tweets,we relied on web-based directories of organiza-tions (e.g., www.twellow.com) and gatheredtheir tweets using the Twitter API.
We used 58organization accounts for English tweets and 83accounts for Spanish.3.1.3 Complete Data SetWe created a data set of 200,000 tweets for eachlanguage, consisting of 90% person-tweets and10% organization-tweets.
Among the 180,000person-tweets, 132,000 (66% of 200,000) weretweets whose source was a hand-held deviceapp.
To collect the remaining 48,000 (24%of 200,000) of the person-tweets, we reliedon the person heuristic.
Finally, we gathered20,000 organization-tweets using the web directo-ries mentioned previously.
In doing so, to ensurethat we had a balanced mix of organizations, eachorganization contributed with a maximum of 500tweets.4 User Type ClassificationTo automatically distinguish person-tweets fromorganization-tweets, we trained a supervised clas-sifier using N-gram features, an organizationheuristic, and a linguistic feature set categorizedinto six classes.
For the classification algorithm,we employed a Support Vector Machine (SVM)with a linear kernel, using the LIBSVM package(Chang and Lin, 2011).
For the features that relyon part-of-speech (POS) tags, we used the EnglishTwitter POS tagger by Gimpel et al.
(2011) andanother tagger trained on the CoNLL 2002 sharedtask data for Spanish (Tjong Kim Sang, 2002) us-ing the OpenNLP toolkit (OpenSource, 2010).4.1 N-gram FeaturesWe started off by introducing N-gram features tocapture the words in a tweet.
Specifically, wetrained a supervised classifier using unigram andbigram features encoded with binary values.
In se-lecting the N-gram features, we discarded any N-gram that appears less than five times in the train-ing data.4.2 Organization HeuristicFollowing observations by Messner et al.
(2011),we combined two simple heuristic rules to flagtweets that are likely to be from an organization.The first observation is that ?replies?
(i.e., @usermentions at the beginning of a tweet) are uncom-mon in organization-tweets.
Hence, if a tweet is areply, it is likely to be a person-tweet.
The secondobservation is that organization-tweets frequentlyinclude a web link to external content.Our organization heuristic, therefore, com-bined these two properties.
If the tweet is not areply AND contains a web link, we labeled it asan organization-tweet.
Otherwise, we labeled itas a person-tweet.
In Section 5, we evaluate thisheuristic as a classification rule on its own, andalso incorporate its label as a feature in our classi-fier.4.3 Linguistic FeaturesIn the following sections, we describe our linguis-tic features and the intuitions in designing them.4.3.1 Emotion and SentimentTwitter is a platform where individuals often ex-press emotion.
We detected emotions using fourfeature types: 1) interjections, 2) profanity, 3)emoticons and 4) overall sentiment of the tweet.Interjections, profanity, and emoticons arewidely used by individuals to convey emotion,such as anger, surprise, happiness, etc.
To iden-tify these three feature types, we used a combina-tion of POS tags in the English tagger (which con-tains tags for interjections, emoticons, etc), com-piled lists of interjections and profanity from the101web for both English6and Spanish7and regularexpression patterns for emoticons.We also included sentiment features using thesentiment140 API8(Go et al., 2009).
This APIprovides a sentiment label (positive, negative orneutral) for a tweet corresponding to its overallsentiment.
We expect person-tweets to show morepositive and negative sentiment and organization-tweets to be more neutral.4.3.2 Similarity to News HeadlinesEarlier, we observed that organization-tweets areoften similar to news headlines.
To exploit this ob-servation, we introduced four features using lan-guage models and verb categories.First, we collected 3 million person-tweets, foreach language, using the person heuristic de-scribed in Section 3.1.
Second, we collected an-other 3 million news headlines from each of theEnglish and Spanish Gigaword corpora (Parkeret al., 2009; Mendonca et al., 2009).
Usingthese two data sets, we built unigram and bigramlanguage models (with Laplace smoothing) forperson-tweets and for news headlines.
Given anew tweet, we calculated the probability of thetweet using both the person-tweet and headlinelanguage models.
We defined a binary feature thatindicates which unigram language model (person-tweet model vs. headline model) produced thehighest probability.
A similar feature is definedfor the bigram language models.We also observed that certain verbs are pre-dominantly used in news headlines and are rarelyassociated with colloquial language (therefore, inperson-tweets).
Similarly, we observed verbs thatare much more likely to be used by individual per-sons.
To identify the most discriminating verbs,we ranked verbs appearing more than 5 times inthe collected news headlines and person-tweetsbased on the following probabilities:p(h|verb) =Frequency of verb in headlinesFrequency of verb in all instancesp(pt|verb) =Frequency of verb in person-tweetsFrequency of verb in all instancesThe verbs were sorted by probability and we re-tained two disjoint sets of verbs, 1) the verbs most6http://www.noswearing.com/dictionary7http://nawcom.com/swearing/mexican_spanish.htm8http://help.sentiment140.com/apirepresentative of headlines (i.e., headline verbs),selected by applying a threshold of p(h|verb) >0.8 and 2) verbs most representative of person-tweets (i.e., personal verbs), with a similar thresh-old of p(pt|verb) > 0.8.
We introduced two bi-nary features that look for verbs in the tweet fromthese two learned verb lists.
The top-ranked verbsfor each category are displayed in Table 2.
Thelearned headline verbs tend to be more formaland are often used in business or government con-texts (e.g., revoke, granting, etc) whereas the per-sonal verbs tend to represent personal activities,communications, and emotions (e.g., hate, sleep,etc).
In total, we learned 687 headline verbs and2221 personal verbs for English, and 1924 head-line verbs and 5719 personal verbs for Spanish.Headline verbs: aided, revoke, issued, broaden, tes-tify, leads, postponing, forged, deepen, hijacked, raises,granting, honoring, pledged, departing, suspending, cit-ing, compensate, preserved, weakening, differingPersonal verbs: raining, sleep, hanging, hate, march-ing, teaching, sway, having, risk, lurk, screaming, tag-ging, disturb, baking, exaggerate, pinch, enjoy, shred-ding, force, hide, wreck, saved, cooking, blur, toldTable 2: Top-ranked representative verbs learnedfrom headlines and person-tweets4.3.3 1stand 2ndPerson PronounsPerson-tweets often include self-references, inthe form of first-person pronouns and their vari-ant forms (e.g., possessive, reflexive), whileorganization-tweets rarely contain self-references.Also, organizations often address their audienceusing second-person pronouns in tweets (e.g., Willyou High Five the #Bruins or #Blackhawks?
Signup for a chance to win a trip to the Cup Final:http://t.co/XQP8ZDOINV).
To capture these char-acteristics, we included two binary features thatlook for 1stand 2ndperson pronouns in a tweet.4.3.4 NER FeaturesWe hypothesized that organization-tweets willcarry more named entities and proper nouns.
ForEnglish tweets, we identified Persons, Organiza-tions and Locations using the Named Entity Rec-ognizer (NER) from Ritter et al.
(2011).
ForSpanish tweets, we used NER models trained onCoNLL 2002 shared task data for Spanish.
Thefeatures were encoded as three values, represent-ing the frequency of each entity type in a tweet.102English SpanishP R F1P R F1ULM: Unigram Language Model 71.63 63.18 67.14 66.14 60.43 63.16BLM: Bigram Language Model 81.46 49.17 61.32 80.03 51.08 62.36NGR: SVM with N-grams 86.02 62.76 72.57 85.76 66.56 74.95OrgH: Organization Heuristic 66.87 91.08 77.12 65.32 81.44 72.49NGR + OrgH 82.26 86.82 84.48 83.85 85.17 84.50NGR + OrgH + Linguistic Features 89.01 89.40 89.20?87.59 85.47 86.52?Table 3: User type classification results with Precision (%), Recall (%) and F1-Score (%).
?
denotesstatistical significance at p < 0.01 compared to NGR + OrgH4.3.5 Informal Language FeaturesPerson-tweets often showcase erratic and casualuse of language, whereas organization-tweets tendto have (relatively) more grammatical languageusage.
Hence, we introduced a feature to deter-mine the informality of a tweet.
Specifically, wecheck if a tweet begins with an uppercase letter ornot, and whether sentences are properly separatedwith punctuation.
To accomplish this, we usedregular expression patterns that look for capital-ized characters following punctuation and white-space characters.
We also added a feature to checkif all the letters in the tweet are lowercased.
Use ofelongated words (e.g., cooooooool) for emphasis,is another property of person-tweets and we cap-tured this property by identifying words with threeor more repetitions of the same character.To comply with the 140 character length restric-tion of a tweet, person-tweets often employ ad-hoc short-hand usage of words that omit or replacecharacters with a phonetic substitute (e.g., 2mrw,good n8).
We used lists of common abbreviationsfound in social media9collected from the web anda binary feature was set if a tweet contained an in-stance from these lists.4.3.6 Twitter Stylistic FeaturesOne can also notice structural properties that areprevalent in either user type.
News organiza-tions often append a topic descriptor to the be-ginning of a tweet (e.g., Petraeus affair: Womanwho complained of harassing emails identifiedhttp://t.co/hpyLQYeL).
To encode this behavior,we employed a simple heuristic that looked for asemicolon or a hyphen within the first three wordsof a tweet.
Also, person-tweets employ heavy useof hashtags so we included the frequency of hash-tags in a tweet as a single feature.
We added twomore features in the form of the length of the tweet9http://www.noslang.com/dictionary/full/and the frequency of @user mentions in the tweet.5 Evaluation of User Type ClassificationIn this section, we discuss and evaluate our usertype classifier.
All of the experiments were carriedout using five-fold cross-validation, using data setsdescribed in Section 3.1.
In these experiments, wemaintained the separation of organization-tweetsat a user-account level in order to avoid tweetsfrom one organization appearing in both train andtest sets.5.1 User Type Classifier ResultsWe first evaluated several baseline systems to as-sess the difficulty of the user type classificationtask.
We report precision, recall and F1-score withorganization-tweets as the positive class.To evaluate our hypothesis that organization-tweets are similar to news headlines, we first pre-dicted user types using only the unigram and bi-gram language models described in Section 4.3.2.As shown in Table 3 (ULM & BLM), unigrammodels were capable of discerning organization-tweets with 71% and 66% precision on Englishand Spanish tweets, respectively.
This is sub-stantial performance given that the random chanceof labeling an organization-tweet (i.e., precision)is merely 10%.
The bigram models show ?80% precision whereas the unigram models showhigher recall.As another baseline, we evaluated an SVM clas-sifier that uses only N-gram features.
As Table 3shows, the N-gram classifier (NGR) achieved veryhigh precision (86%) for both English and Spanishtweets.
However, it yielded relatively moderate re-call (63% for English and 67% for Spanish).We then evaluated the organization heuris-tic (OrgH) all by itself.
The heuristic identi-fies two common characteristics of organization-tweets and as expected, it achieved substantial re-call (91% for English and 81% for Spanish) but103English SpanishP R F1P R F1NGR + OrgH 82.26 86.82 84.48 83.85 85.17 84.50+ Emotion and Sentiment Features 86.58 86.41 86.50 85.91 84.19 85.05+ Features Derived from News Headlines 87.83 87.10 87.46 86.68 84.05 85.35+ 1stand 2ndPerson Pronouns 87.88 88.53 88.20 86.61 84.38 85.48+ NER Features 88.05 88.75 88.40 86.71 84.69 85.69+ Informal Language Features 88.39 89.14 88.77 86.89 85.31 86.09+ Twitter Stylistic Features 89.01 89.40 89.20 87.59 85.47 86.52NGR + OrgH + Linguistic Features 89.01 89.40 89.20 87.59 85.47 86.52Table 4: Linguistic feature ablation with Precision (%), Recall (%) and F1-Score (%)with mediocre precision.These results show that the N-gram classifierachieved high precision whereas the organizationheuristic achieved high recall.
To exploit the bestof both worlds, we evaluated another model (NGR+ OrgH) that added the organization heuristic asan additional feature for the N-gram classifier.This system fares better than all the previous mod-els, achieving 82% precision with 87% recall forEnglish and 84% precision with 85% recall forSpanish.Next, we show the benefits obtained fromadding the linguistic feature set.
As the final rowin Table 3 shows, having incorporated all the lin-guistic features, our final system showed an im-provement of 7% precision and 3% recall on En-glish tweets for an overall F1-score gain of approx-imately 5%.
On Spanish tweets, the same incre-ments were 4%, 0.3% and 2%, respectively.
Thisfinal classifier is statistically significantly betterthan the model without linguistic features (NGR +OrgH) for both languages at the p < 0.01 level,analyzed using a paired booststrap test drawing106samples with repetition from test data, as de-scribed in Berg-Kirkpatrick et al.
(2012).5.2 Analysis of Linguistic FeaturesHaving observed that linguistic features improveduser type classification, we evaluated the impactof each type of linguistic feature using an ablationstudy.
Table 4 shows the classifier performancewhen each of the features types was added cumu-latively over the NGR + OrgH baseline.We immediately see a 4% and 2% precisiongain by adding emotion and sentiment features,for English and Spanish, respectively.
Adding fea-tures derived from news headlines, we observethat the classifier fares better, improving precisionfor both languages and improving recall for En-glish.
1stand 2ndperson pronouns improved re-call on English data but had little impact on Span-ish data.
The NER features produced very smallgains in both languages.
The informal languagefeatures increased recall from 84.69% to 85.31%on Spanish tweets.
Finally, the Twitter stylisticfeatures gained 0.7% more precision for both lan-guages.
Overall, the feature types that contributedthe most were the emotion/sentiment features, thenews headline features, and the Twitter stylisticfeatures.6 Twitter Event RecognitionTwitter provides a facility where users can searchfor tweets using keywords.
However, keyword-based queries for events can often lead to myriadirrelevant results due to different senses of key-words (polysemy) and figurative or metaphoricaluse of keywords.
For instance, a Twitter searchfor civil unrest events with a few representativekeywords (e.g., strike, rally, riot, etc.)
can oftenlead to results referring to sports events, such as abowling strike or a tennis rally or where the key-words are used figuratively (e.g., She?s a riot!).
Inthis section, we investigate if the user type of atweet can help cut through such ambiguity.
Specif-ically, we hypothesize that event keywords may beused more consistantly and with less ambiguity inorganization-tweets, and therefore user type infor-mation may be helpful in improving event recog-nition.To explore our hypothesis that the user type caninfluence the event relevance of a tweet, we con-structed a set of experiments using two types ofevents - civil unrest events and disease outbreaks.Civil unrest refers to forms of public disturbancethat affect the order of a society (e.g., strikes,protests, etc.)
whereas a disease outbreak refers toan unusual or widespread occurrence of a disease(e.g., a measles outbreak).104English SpanishCivil Unrest Disease Outbreaks Civil Unrest Disease OutbreaksPerson-tweets 5.27% 9.52% 9.32% 5.00%Organization-tweets 36.54% 39.34% 51.66% 44.06%All-tweets 12.50% 20.07% 14.72% 13.22%Table 6: Percentage of event-relevant tweets in 4000 tweets with keywords for each categoryEnglish Civil Unrest: protest, protested, protesting,riot, rioted, rioting, rally, rallied, rallying, marched,marching, strike, striked, strikingEnglish Disease Outbreaks: outbreak, epidemic, in-fluenza, h1n1, h5n1, pandemic, quarantine, cholera,ebola, flu, malaria, dengue, hepatitis, measlesSpanish Civil Unrest: protesta, protestar, amoti-naron, protestaron, protestaban, protestado, amotinarse,amotinaban, marcha, huelga, amotinando, protestando,amotinadoSpanish Disease Outbreaks: brote, epidemia, in-fluenza, h1n1, h5n1, pandemia, cuarantena, sarampi?on,c?olera, ebola, malaria, dengue, hepatitis, gripeTable 5: Keywords used to query Twitter for twotypes of events in English and Spanish6.1 Data Acquisition for Event RecognitionWe began by collecting tweets that contained atleast one of the keywords listed in Table 5, usingthe Twitter search API, and we set up an annota-tion task using Amazon Mechanical Turk (AMT)annotators.
First, we created guidelines to distin-guish event-relevant tweets from irrelevant tweetsand annotated 300 tweets for each of the four cat-egories (i.e., English Civil Unrest, Spanish CivilUnrest, English Disease Outbreaks and SpanishDisease Outbreaks).We released 200 tweets in each category forannotation to three AMT annotators10.
We usedthese 200 tweets to calculate pair-wise IAA usingCohen?s Kappa (?)
which we report in Table 7.The IAA scores were generally good, rangingfrom 0.67 to 0.89.
Each annotator subsequently la-beled 2000 tweets, yielding a total of 6000 tweetsfor each category.
In each of these 6000 tweet sets,we randomly separated 2000 tweets as tuning dataand 4000 as test data.First, we applied our user type classifier to thesetweets and analyzed the number of true eventtweets for each user type.
Table 6 shows the per-centage of true event tweets in the entire test set,as well as the percentage of event tweets for each10We first released 100 tweets in each category to AMTand enlisted 10 annotators.
After calculating IAA on these100 tweets, we retained 3 annotators who had the highestagreement with our annotations.English SpanishCivil Unrest .89, .88, .77 .74, .74, .67Disease Outbreaks .82, .73, .68 .84, .83, .80Table 7: Pair-wise inter-annotator agreement(IAA) measured using Cohen?s Kappa (?)
on 200tweets among the three AMT annotators for eachevent type in each languageuser type.
Overall, the percentage of true eventtweets in each test set is ?
20%.
This means thatmost of the tweets (> 80%) with event keywordsdo not discuss an event, confirming the unreliabil-ity of using event keywords alone.However, there is a substantial difference inthe density of true event tweets between the twouser types.
Across both civil unrest and dis-ease outbreaks, and for both languages, we seea much higher percentage of organization-tweetswith event keywords mentioning an event thanperson-tweets with event keywords.
Table 6 showsthat, in English civil unrest category, organization-tweets are 7 times more likely (36.54% as opposedto 5.27%) to report an actual event than person-tweets with the same keywords.
In the English dis-ease outbreaks category, organization-tweets are4 times more likely to report an event (39.34%vs.
9.52%).
We notice similar observations in theSpanish tweets too.6.2 Event Recognition ResultsIn this section, we evaluate the impact of user typeinformation by introducing a simple baseline ex-periment for Twitter event recognition followedby several schemes that we devised to incorporateuser type information in more principled ways.First, we trained a supervised classifier to pre-dict the probability of a tweet being event-relevantusing only unigrams and bigrams as features, en-coded with binary values.
This baseline system isagnostic to the user type.
We used the SVM Plattmethod implementation of LIBSVM (Chang andLin, 2011) and carried out experiments using five-fold cross-validation.
As Table 8 shows, this ap-105English SpanishP R F1P R F1Civil Unrest EventsUser type-agnostic classifier 80.97 50.20 61.98 77.51 60.37 67.88User type included as a feature 80.00 50.40 61.84 77.19 61.56 68.50(?p, ?o) optimized for F1-score 60.50 72.60 66.00 64.97 78.57 71.13User type-specific classifier 79.34 63.61 70.61?79.20 81.89 80.52?Disease Outbreak EventsUser type-agnostic classifier 83.15 55.99 66.92 80.49 56.14 66.15User type included as a feature 83.46 55.36 66.57 80.93 59.36 68.48(?p, ?o) optimized for F1-score 75.10 66.58 70.58 68.94 72.58 70.71User type-specific classifier 80.35 66.07 72.51?82.20 74.26 78.03?Table 8: Event recognition results showing Precision (%), Recall (%) and F1-Score (%), for the twoevent types in English and Spanish.
?
denotes statistical significance at p < 0.01 compared to thebaseline (User type-agnostic classifier)proach achieved 62% F1-score in English and 68%F1-score in Spanish, for civil unrest events.
Fordisease outbreak events, the corresponding valueswere 67% and 66%.As our first attempt to incorporate user type in-formation, we added the user type label as one ad-ditional feature.
As shown in Table 8, the addedfeature yielded small gains for Spanish but madelittle difference for English.Given our initial hypothesis (and evidence inTable 6) about events and organization-tweets,we would prefer to be aggressive in labelingorganization-tweets as event-relevant.
One way toaccomplish this with a trained probabilistic classi-fier is to assign different probability thresholds toperson- and organization-tweets.
To acquire theoptimal threshold parameters for person-tweets(?p) and organization-tweets (?o), we performed agrid-based threshold sweep on tuning data and op-timized with respect to F1-scores.
Table 8 showsthat this approach yielded substantial recall gainsfor all four categories and produced the best F1-scores thus far.A more principled approach is to create twocompletely different classifiers, one for each usertype.
Each classifier can then model the vocabu-lary and word associations that are most likely tooccur in tweets of that type.
Using five-fold cross-validation, we train separate models for person-and organization-tweets.
During event recogni-tion, we first apply our user type classifier to atweet and then apply the appropriate event recog-nition model.
As shown in the final rows in Ta-ble 8, this method consistently outperforms theother approaches.
Compared to the best compet-ing method, the user type-specific classifiers pro-duced F1-score gains of 4.6% and 9.4% for En-glish and Spanish civil unrest events, and F1-scoregains of 2% and 7.3% for English and Spanish dis-ease outbreak events.7 ConclusionIn this work, we tackled the problem of classify-ing tweets into two user types, organizations andindividual persons, based on their textual content.We designed a rich set of features that exploitdifferent linguistic aspects of tweet content, anddemonstrated that our classifier achieves F1-scoresof 89% for English and 87% for Spanish.
We alsopresented results showing that organization-tweetswith event keywords have a much higher den-sity of event mentions than person-tweets with thesame keywords and showed the benefits of incor-porating user type information into event recog-nition models.
Our results showed that creatingseparate event recognition classifiers for differentuser types yields substantially better performancethan using a single event recognition model on alltweets.8 AcknowledgmentsThis work was supported by the Intelligence Ad-vanced Research Projects Activity (IARPA) viaDepartment of Interior National Business Cen-ter (DoI/NBC) contract number D12PC00285.The U.S. Government is authorized to reproduceand distribute reprints for Governmental purposesnotwithstanding any copyright annotation thereon.The views and conclusions contained herein arethose of the authors and should not be interpretedas necessarily representing the official policiesor endorsements, either expressed or implied, ofIARPA, DoI/NBC, or the U.S. Government.106ReferencesHila Becker, Mor Naaman, and Luis Gravano.
2009.Event identification in social media.
In WebDB.Hila Becker, Mor Naaman, and Luis Gravano.
2010.Learning similarity metrics for event identificationin social media.
In Proceedings of the Third ACMInternational Conference on Web Search and DataMining, WSDM ?10, pages 291?300, New York,NY, USA.
ACM.H.
Becker, M. Naaman, and L. Gravano.
2011.
Select-ing quality twitter content for events.
In Proceed-ings of the Fifth International AAAI Conference onWeblogs and Social Media (ICWSM11).E.
Benson, A. Haghighi, and R. Barzilay.
2011.
Eventdiscovery in social media feeds.
In The 49th AnnualMeeting of the Association for Computational Lin-guistics, Portland, Oregon, USA.
To appear.Taylor Berg-Kirkpatrick, David Burkett, and DanKlein.
2012.
An empirical investigation of statis-tical significance in nlp.
In Proceedings of the 2012Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning, pages 995?1005.
Associationfor Computational Linguistics.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technol-ogy, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Morgane Ciot, Morgan Sonderegger, and Derek Ruths.2013.
Gender inference of twitter users in non-english contexts.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, Seattle, Wash, pages 18?21.Raviv Cohen and Derek Ruths.
2013.
Classifying po-litical orientation on twitter: Its not easy!
In SeventhInternational AAAI Conference on Weblogs and So-cial Media.M.
De Choudhury, N. Diakopoulos, and M. Naaman.2012.
Unfolding the event landscape on twitter:classification and exploration of user categories.
InProceedings of the ACM 2012 conference on Com-puter Supported Cooperative Work, pages 241?244.ACM.Clayton Fink, Jonathon Kopecky, and MaksymMorawski.
2012.
Inferring gender from the contentof tweets: A region specific example.
In ICWSM.K.
Gimpel, N. Schneider, B. O?Connor, D. Das,D.
Mills, J. Eisenstein, M. Heilman, D. Yogatama,J.
Flanigan, and N.A.
Smith.
2011.
Part-of-speechtagging for twitter: annotation, features, and exper-iments.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies: short papers-Volume 2, pages 42?47.
Association for Computa-tional Linguistics.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.CS224N Project Report, Stanford, pages 1?12.A.
Jackoway, H. Samet, and J. Sankaranarayanan.2011.
Identification of live news events using twit-ter.
In Proceedings of the 3rd ACM SIGSPATIALInternational Workshop on Location-Based SocialNetworks, page 9.
ACM.M.
Kim, L. Xie, and P. Christen.
2012.
Event diffusionpatterns in social media.
In Sixth International AAAIConference on Weblogs and Social Media.Wendy Liu and Derek Ruths.
2013.
Whats in a name?using first names as features for gender inference intwitter.Marco Lui and Timothy Baldwin.
2012. langid.
py:An off-the-shelf language identification tool.
InProceedings of the ACL 2012 System Demonstra-tions, pages 25?30.
Association for ComputationalLinguistics.Angelo Mendonca, David Andrew Graff, DeniseDiPersio, Linguistic Data Consortium, et al.
2009.Spanish gigaword second edition.
Linguistic DataConsortium.M.
Messner, M. Linke, and A. Eford.
2011.
Shov-eling tweets: An analysis of the microbloggingengagement of traditional news organizations.
InInternational Symposium on Online Journalism,UT Austin, available at: http://online.
journalism.utexas.
edu/2011/papers/Messner2011.
pdf (last ac-cessed April 3, 2011).D.
Metzler, C. Cai, and E. Hovy.
2012.
Structuredevent retrieval over microblog archives.
In Proceed-ings of the 2012 Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages646?655.Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-Pekka Onnela, and J Niels Rosenquist.
2011.Understanding the demographics of twitter users.ICWSM, 11:5th.M.
Naaman, J. Boase, and C.H.
Lai.
2010.
Is it re-ally about me?
: message content in social aware-ness streams.
In Proceedings of the 2010 ACM con-ference on Computer supported cooperative work,pages 189?192.
ACM.OpenSource.
2010.
Opennlp: http ://opennlp.sourceforge.net/.Robert Parker, Linguistic Data Consortium, et al.2009.
English gigaword fourth edition.
LinguisticData Consortium.Marco Pennacchiotti and Ana-Maria Popescu.
2011.A machine learning approach to twitter user classifi-cation.107Sa?sa Petrovi?c, Miles Osborne, and Victor Lavrenko.2010.
Streaming first story detection with applica-tion to twitter.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 181?189.
Association for Computa-tional Linguistics.A.M.
Popescu and M. Pennacchiotti.
2011.
Dancingwith the stars, nba games, politics: An exploration oftwitter users response to events.
In Proceedings ofthe International AAAI Conference on Weblogs andSocial Media.Delip Rao, David Yarowsky, Abhishek Shreevats, andManaswi Gupta.
2010.
Classifying latent user at-tributes in twitter.
In Proceedings of the 2nd in-ternational workshop on Search and mining user-generated contents, pages 37?44.
ACM.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: An ex-perimental study.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, EMNLP ?11, pages 1524?1534, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.A.
Ritter, O. Etzioni, S. Clark, et al.
2012.
Opendomain event extraction from twitter.
In Proceed-ings of the 18th ACM SIGKDD international con-ference on Knowledge discovery and data mining,pages 1104?1112.
ACM.T.
Sakaki, M. Okazaki, and Y. Matsuo.
2010.
Earth-quake shakes twitter users: real-time event detectionby social sensors.
In Proceedings of the 19th inter-national conference on World wide web, pages 851?860.
ACM.H.
Sayyadi, M. Hurst, and A. Maykov.
2009.
Eventdetection and tracking in social streams.
In Proceed-ings of International Conference on Weblogs and So-cial Media (ICWSM).Erik F. Tjong Kim Sang.
2002.
Introduction tothe conll-2002 shared task: Language-independentnamed entity recognition.
In Proceedings of the 6thConference on Natural Language Learning - Volume20, COLING-02, pages 1?4, Stroudsburg, PA, USA.Association for Computational Linguistics.Svitlana Volkova, Theresa Wilson, and DavidYarowsky.
2013.
Exploring demographic lan-guage variations to improve multilingual sentimentanalysis in social media.
In Proceedings of the2013 Conference on Empirical Methods on NaturalLanguage Processing.K Wickre.
2013.
Celebrating #twitter7.https://blog.twitter.com/2013/celebrating-twitter7.
Accessed:03/20/2014.S.
Wu, J.M.
Hofman, W.A.
Mason, and D.J.
Watts.2011.
Who says what to whom on twitter.
InProceedings of the 20th international conference onWorld wide web, pages 705?714.
ACM.108
