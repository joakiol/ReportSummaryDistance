Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 369?377,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPDependency Grammar Induction via Bitext Projection ConstraintsKuzman Ganchev and Jennifer Gillenwater and Ben TaskarDepartment of Computer and Information ScienceUniversity of Pennsylvania, Philadelphia PA, USA{kuzman,jengi,taskar}@seas.upenn.eduAbstractBroad-coverage annotated treebanks nec-essary to train parsers do not exist formany resource-poor languages.
The wideavailability of parallel text and accurateparsers in English has opened up the pos-sibility of grammar induction through par-tial transfer across bitext.
We considergenerative and discriminative models fordependency grammar induction that useword-level alignments and a source lan-guage parser (English) to constrain thespace of possible target trees.
Unlikeprevious approaches, our framework doesnot require full projected parses, allowingpartial, approximate transfer through lin-ear expectation constraints on the spaceof distributions over trees.
We considerseveral types of constraints that rangefrom generic dependency conservation tolanguage-specific annotation rules for aux-iliary verb analysis.
We evaluate our ap-proach on Bulgarian and Spanish CoNLLshared task data and show that we con-sistently outperform unsupervised meth-ods and can outperform supervised learn-ing for limited training data.1 IntroductionFor English and a handful of other languages,there are large, well-annotated corpora with a vari-ety of linguistic information ranging from namedentity to discourse structure.
Unfortunately, forthe vast majority of languages very few linguis-tic resources are available.
This situation islikely to persist because of the expense of creat-ing annotated corpora that require linguistic exper-tise (Abeill?, 2003).
On the other hand, parallelcorpora between many resource-poor languagesand resource-rich languages are ample, motivat-ing recent interest in transferring linguistic re-sources from one language to another via paralleltext.
For example, several early works (Yarowskyand Ngai, 2001; Yarowsky et al, 2001; Merloet al, 2002) demonstrate transfer of shallow pro-cessing tools such as part-of-speech taggers andnoun-phrase chunkers by using word-level align-ment models (Brown et al, 1994; Och and Ney,2000).Alshawi et al (2000) and Hwa et al (2005)explore transfer of deeper syntactic structure:dependency grammars.
Dependency and con-stituency grammar formalisms have long coex-isted and competed in linguistics, especially be-yond English (Mel?c?uk, 1988).
Recently, depen-dency parsing has gained popularity as a simpler,computationally more efficient alternative to con-stituency parsing and has spurred several super-vised learning approaches (Eisner, 1996; Yamadaand Matsumoto, 2003a; Nivre and Nilsson, 2005;McDonald et al, 2005) as well as unsupervised in-duction (Klein and Manning, 2004; Smith and Eis-ner, 2006).
Dependency representation has beenused for language modeling, textual entailmentand machine translation (Haghighi et al, 2005;Chelba et al, 1997; Quirk et al, 2005; Shen et al,2008), to name a few tasks.Dependency grammars are arguably more ro-bust to transfer since syntactic relations betweenaligned words of parallel sentences are better con-served in translation than phrase structure (Fox,2002; Hwa et al, 2005).
Nevertheless, sev-eral challenges to accurate training and evalua-tion from aligned bitext remain: (1) partial wordalignment due to non-literal or distant transla-tion; (2) errors in word alignments and source lan-guage parses, (3) grammatical annotation choicesthat differ across languages and linguistic theo-ries (e.g., how to analyze auxiliary verbs, conjunc-tions).In this paper, we present a flexible learning369framework for transferring dependency grammarsvia bitext using the posterior regularization frame-work (Gra?a et al, 2008).
In particular, we ad-dress challenges (1) and (2) by avoiding com-mitment to an entire projected parse tree in thetarget language during training.
Instead, we ex-plore formulations of both generative and discrim-inative probabilistic models where projected syn-tactic relations are constrained to hold approxi-mately and only in expectation.
Finally, we ad-dress challenge (3) by introducing a very smallnumber of language-specific constraints that dis-ambiguate arbitrary annotation choices.We evaluate our approach by transferring froman English parser trained on the Penn treebank toBulgarian and Spanish.
We evaluate our resultson the Bulgarian and Spanish corpora from theCoNLL X shared task.
We see that our transferapproach consistently outperforms unsupervisedmethods and, given just a few (2 to 7) language-specific constraints, performs comparably to a su-pervised parser trained on a very limited corpus(30 - 140 training sentences).2 ApproachAt a high level our approach is illustrated in Fig-ure 1(a).
A parallel corpus is word-level alignedusing an alignment toolkit (Gra?a et al, 2009) andthe source (English) is parsed using a dependencyparser (McDonald et al, 2005).
Figure 1(b) showsan aligned sentence pair example where depen-dencies are perfectly conserved across the align-ment.
An edge from English parent p to child c iscalled conserved if word p aligns to word p?
in thesecond language, c aligns to c?
in the second lan-guage, and p?
is the parent of c?.
Note that we arenot restricting ourselves to one-to-one alignmentshere; p, c, p?, and c?
can all also align to otherwords.
After filtering to identify well-behavedsentences and high confidence projected depen-dencies, we learn a probabilistic parsing model us-ing the posterior regularization framework (Gra?aet al, 2008).
We estimate both generative and dis-criminative models by constraining the posteriordistribution over possible target parses to approxi-mately respect projected dependencies and otherrules which we describe below.
In our experi-ments we evaluate the learned models on depen-dency treebanks (Nivre et al, 2007).Unfortunately the sentence in Figure 1(b) ishighly unusual in its amount of dependency con-servation.
To get a feel for the typical case, weused off-the-shelf parsers (McDonald et al, 2005)for English, Spanish and Bulgarian on two bi-texts (Koehn, 2005; Tiedemann, 2007) and com-pared several measures of dependency conserva-tion.
For the English-Bulgarian corpus, we ob-served that 71.9% of the edges we projected wereedges in the corpus, and we projected on average2.7 edges per sentence (out of 5.3 tokens on aver-age).
For Spanish, we saw conservation of 64.4%and an average of 5.9 projected edges per sentence(out of 11.5 tokens on average).As these numbers illustrate, directly transfer-ring information one dependency edge at a timeis unfortunately error prone for two reasons.
First,parser and word alignment errors cause much ofthe transferred information to be wrong.
We dealwith this problem by constraining groups of edgesrather than a single edge.
For example, in somesentence pair we might find 10 edges that haveboth end points aligned and can be transferred.Rather than requiring our target language parse tocontain each of the 10 edges, we require that theexpected number of edges from this set is at least10?, where ?
is a strength parameter.
This givesthe parser freedom to have some uncertainty aboutwhich edges to include, or alternatively to chooseto exclude some of the transferred edges.A more serious problem for transferring parseinformation across languages are structural differ-ences and grammar annotation choices betweenthe two languages.
For example dealing with aux-iliary verbs and reflexive constructions.
Hwa et al(2005) also note these problems and solve them byintroducing dozens of rules to transform the trans-ferred parse trees.
We discuss these differencesin detail in the experimental section and use ourframework introduce a very small number of rulesto cover the most common structural differences.3 Parsing ModelsWe explored two parsing models: a generativemodel used by several authors for unsupervised in-duction and a discriminative model used for fullysupervised training.The discriminative parser is based on theedge-factored model and features of the MST-Parser (McDonald et al, 2005).
The parsingmodel defines a conditional distribution p?
(z | x)over each projective parse tree z for a particularsentence x, parameterized by a vector ?.
The prob-370(a)(b)Figure 1: (a) Overview of our grammar induction approach via bitext: the source (English) is parsed and word-aligned withtarget; after filtering, projected dependencies define constraints over target parse tree space, providing weak supervision forlearning a target grammar.
(b) An example word-aligned sentence pair with perfectly projected dependencies.ability of any particular parse isp?
(z | x) ??z?ze???
(z,x), (1)where z is a directed edge contained in the parsetree z and ?
is a feature function.
In the fully su-pervised experiments we run for comparison, pa-rameter estimation is performed by stochastic gra-dient ascent on the conditional likelihood func-tion, similar to maximum entropy models or con-ditional random fields.
One needs to be able tocompute expectations of the features ?
(z,x) underthe distribution p?
(z | x).
A version of the inside-outside algorithm (Lee and Choi, 1997) performsthis computation.
Viterbi decoding is done usingEisner?s algorithm (Eisner, 1996).We also used a generative model based on de-pendency model with valence (Klein and Man-ning, 2004).
Under this model, the probability ofa particular parse z and a sentence with part ofspeech tags x is given byp?
(z,x) = proot(r(x)) ?
(2)(?z?zp?stop(zp, zd, vz) pchild(zp, zd, zc))?
(?x?xpstop(x, left, vl) pstop(x, right, vr))where r(x) is the part of speech tag of the rootof the parse tree z, z is an edge from parent zpto child zc in direction zd, either left or right, andvz indicates valency?false if zp has no other chil-dren further from it in direction zd than zc, trueotherwise.
The valencies vr/vl are marked as trueif x has any children on the left/right in z, falseotherwise.4 Posterior RegularizationGra?a et al (2008) introduce an estimation frame-work that incorporates side-information into un-supervised problems in the form of linear con-straints on posterior expectations.
In grammartransfer, our basic constraint is of the form: theexpected proportion of conserved edges in a sen-tence pair is at least ?
(the exact proportion weused was 0.9, which was determined using un-labeled data as described in Section 5).
Specifi-cally, let Cx be the set of directed edges projectedfrom English for a given sentence x, then givena parse z, the proportion of conserved edges isf(x, z) = 1|Cx|?z?z 1(z ?
Cx) and the expectedproportion of conserved edges under distributionp(z | x) isEp[f(x, z)] =1|Cx|?z?Cxp(z | x).The posterior regularization framework (Gra?aet al, 2008) was originally defined for gener-ative unsupervised learning.
The standard ob-jective is to minimize the negative marginallog-likelihood of the data : E?[?
log p?
(x)] =E?[?
log?z p?
(z,x)] over the parameters ?
(weuse E?
to denote expectation over the sample sen-tences x).
We typically also add standard regular-ization term on ?, resulting from a parameter prior?
log p(?)
= R(?
), where p(?)
is Gaussian for theMST-Parser models and Dirichlet for the valencemodel.To introduce supervision into the model, we de-fine a set Qx of distributions over the hidden vari-ables z satisfying the desired posterior constraintsin terms of linear equalities or inequalities on fea-ture expectations (we use inequalities in this pa-per):Qx = {q(z) : E[f(x, z)] ?
b}.371Basic Uni-gram Featuresxi-word, xi-posxi-wordxi-posxj-word, xj-posxj-wordxj-posBasic Bi-gram Featuresxi-word, xi-pos, xj-word, xj-posxi-pos, xj-word, xj-posxi-word, xj-word, xj-posxi-word, xi-pos, xj-posxi-word, xi-pos, xj-wordxi-word, xj-wordxi-pos, xj-posIn Between POS Featuresxi-pos, b-pos, xj-posSurrounding Word POS Featuresxi-pos, xi-pos+1, xj-pos-1, xj-posxi-pos-1, xi-pos, xj-pos-1, xj-posxi-pos, xi-pos+1, xj-pos, xj-pos+1xi-pos-1, xi-pos, xj-pos, xj-pos+1Table 1: Features used by the MSTParser.
For each edge (i, j), xi-word is the parent word and xj-word is the child word,analogously for POS tags.
The +1 and -1 denote preceeding and following tokens in the sentence, while b denotes tokensbetween xi and xj .In this paper, for example, we use the conserved-edge-proportion constraint as defined above.
Themarginal log-likelihood objective is then modi-fied with a penalty for deviation from the de-sired set of distributions, measured by KL-divergence from the set Qx, KL(Qx||p?
(z|x)) =minq?Qx KL(q(z)||p?(z|x)).
The generativelearning objective is to minimize:E?[?
log p?
(x)] +R(?)
+ E?[KL(Qx||p?
(z | x))].For discriminative estimation (Ganchev et al,2008), we do not attempt to model the marginaldistribution of x, so we simply have the two regu-larization terms:R(?)
+ E?[KL(Qx||p?
(z | x))].Note that the idea of regularizing moments is re-lated to generalized expectation criteria algorithmof Mann and McCallum (2007), as we discuss inthe related work section below.
In general, theobjectives above are not convex in ?.
To opti-mize these objectives, we follow an ExpectationMaximization-like scheme.
Recall that standardEM iterates two steps.
An E-step computes a prob-ability distribution over the model?s hidden vari-ables (posterior probabilities) and an M-step thatupdates the model?s parameters based on that dis-tribution.
The posterior-regularized EM algorithmleaves the M-step unchanged, but involves project-ing the posteriors onto a constraint set after theyare computed for each sentence x:argminqKL(q(z) ?
p?(z|x))s.t.
Eq[f(x, z)] ?
b,(3)where p?
(z|x) are the posteriors.
The new poste-riors q(z) are used to compute sufficient statisticsfor this instance and hence to update the model?sparameters in the M-step for either the generativeor discriminative setting.The optimization problem in Equation 3 can beefficiently solved in its dual formulation:argmin??0b>?+log?zp?
(z | x) exp {?
?>f(x, z)}.
(4)Given ?, the primal solution is given by: q(z) =p?
(z | x) exp{?
?>f(x, z)}/Z, where Z is a nor-malization constant.
There is one dual variable perexpectation constraint, and we can optimize themby projected gradient descent, similar to log-linearmodel estimation.
The gradient with respect to ?is given by: b ?
Eq[f(x, z)], so it involves com-puting expectations under the distribution q(z).This remains tractable as long as features factor byedge, f(x, z) =?z?z f(x, z), because that en-sures that q(z) will have the same form as p?
(z |x).
Furthermore, since the constraints are per in-stance, we can use incremental or online versionof EM (Neal and Hinton, 1998), where we updateparameters ?
after posterior-constrained E-step oneach instance x.5 ExperimentsWe conducted experiments on two languages:Bulgarian and Spanish, using each of the pars-ing models.
The Bulgarian experiments transfer aparser from English to Bulgarian, using the Open-Subtitles corpus (Tiedemann, 2007).
The Span-ish experiments transfer from English to Spanishusing the Spanish portion of the Europarl corpus(Koehn, 2005).
For both corpora, we performedword alignments with the open source PostCAT(Gra?a et al, 2009) toolkit.
We used the Tokyotagger (Tsuruoka and Tsujii, 2005) to POS tagthe English tokens, and generated parses usingthe first-order model of McDonald et al (2005)with projective decoding, trained on sections 2-21of the Penn treebank with dependencies extractedusing the head rules of Yamada and Matsumoto(2003b).
For Bulgarian we trained the StanfordPOS tagger (Toutanova et al, 2003) on the Bul-372Discriminative model Generative modelBulgarian Spanish Bulgarian Spanishno rules 2 rules 7 rules no rules 3 rules no rules 2 rules 7 rules no rules 3 rulesBaseline 63.8 72.1 72.6 67.6 69.0 66.5 69.1 71.0 68.2 71.3Post.Reg.
66.9 77.5 78.3 70.6 72.3 67.8 70.7 70.8 69.5 72.8Table 2: Comparison between transferring a single tree of edges and transferring all possible projected edges.
The transfermodels were trained on 10k sentences of length up to 20, all models tested on CoNLL train sentences of up to 10 words.Punctuation was stripped at train time.gtreebank corpus from CoNLL X.
The SpanishEuroparl data was POS tagged with the FreeLinglanguage analyzer (Atserias et al, 2006).
The dis-criminative model used the same features as MST-Parser, summarized in Table 1.In order to evaluate our method, we a baselineinspired by Hwa et al (2005).
The baseline con-structs a full parse tree from the incomplete andpossibly conflicting transferred edges using a sim-ple random process.
We start with no edges andtry to add edges one at a time verifying at eachstep that it is possible to complete the tree.
Wefirst try to add the transferred edges in random or-der, then for each orphan node we try all possibleparents (both in random order).
We then use thisfull labeling as supervision for a parser.
Note thatthis baseline is very similar to the first iteration ofour model, since for a large corpus the differentrandom choices made in different sentences tendto smooth each other out.
We also tried to cre-ate rules for the adoption of orphans, but the sim-ple rules we tried added bias and performed worsethan the baseline we report.
Table 2 shows at-tachment accuracy of our method and the baselinefor both language pairs under several conditions.By attachment accuracy we mean the fraction ofwords assigned the correct parent.
The experimen-tal details are described in this section.
Link-leftbaselines for these corpora are much lower: 33.8%and 27.9% for Bulgarian and Spanish respectively.5.1 PreprocessingPreliminary experiments showed that our wordalignments were not always appropriate for syn-tactic transfer, even when they were correct fortranslation.
For example, the English ?bike/V?could be translated in French as ?aller/V env?lo/N?, where the word ?bike?
would be alignedwith ?v?lo?.
While this captures some of the se-mantic shared information in the two languages,we have no expectation that the noun ?v?lo?will have a similar syntactic behavior to the verb?bike?.
To prevent such false transfer, we filterout alignments between incompatible POS tags.
Inboth language pairs, filtering out noun-verb align-ments gave the biggest improvement.Both corpora also contain sentence fragments,either because of question responses or frag-mented speech in movie subtitles or because ofvoting announcements and similar formulaic sen-tences in the parliamentary proceedings.
We over-come this problem by filtering out sentences thatdo not have a verb as the English root or for whichthe English root is not aligned to a verb in thetarget language.
For the subtitles corpus we alsoremove sentences that end in an ellipsis or con-tain more than one comma.
Finally, following(Klein and Manning, 2004) we strip out punctu-ation from the sentences.
For the discriminativemodel this did not affect results significantly butimproved them slightly in most cases.
We foundthat the generative model gets confused by punctu-ation and tends to predict that periods at the end ofsentences are the parents of words in the sentence.Our basic model uses constraints of the form:the expected proportion of conserved edges in asentence pair is at least ?
= 90%.15.2 No Language-Specific RulesWe call the generic model described above ?no-rules?
to distinguish it from the language-specificconstraints we introduce in the sequel.
The norules columns of Table 2 summarize the perfor-mance in this basic setting.
Discriminative modelsoutperform the generative models in the majorityof cases.
The left panel of Table 3 shows the mostcommon errors by child POS tag, as well as bytrue parent and guessed parent POS tag.Figure 2 shows that the discriminative modelcontinues to improve with more transfer-type data1We chose ?
in the following way: we split the unlabeledparallel text into two portions.
We trained a models with dif-ferent ?
on one portion and ran it on the other portion.
Wechose the model with the highest fraction of conserved con-straints on the second portion.3730.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.680.1110accuracy (%)training datasize (thousands of sentences)our method baselineFigure 2: Learning curve of the discriminative no-rulestransfer model on Bulgarian bitext, testing on CoNLL trainsentences of up to 10 words.Figure 3: A Spanish example where an auxiliary verb dom-inates the main verb.up to at least 40 thousand sentences.5.3 Annotation guidelines and constraintsUsing the straightforward approach outlinedabove is a dramatic improvement over the standardlink-left baseline (and the unsupervised generativemodel as we discuss below), however it doesn?thave any information about the annotation guide-lines used for the testing corpus.
For example, theBulgarian corpus has an unusual treatment of non-finite clauses.
Figure 4 shows an example.
We seethat the ?da?
is the parent of both the verb and itsobject, which is different than the treatment in theEnglish corpus.We propose to deal with these annotation dis-similarities by creating very simple rules.
ForSpanish, we have three rules.
The first rule setsmain verbs to dominate auxiliary verbs.
Specifi-cally, whenever an auxiliary precedes a main verbthe main verb becomes its parent and adopts itschildren; if there is only one main verb it becomesthe root of the sentence; main verbs also becomeFigure 4: An example where transfer fails because ofdifferent handling of reflexives and nonfinite clauses.
Thealignment links provide correct glosses for Bulgarian words.
?Bh?
is a past tense marker while ?se?
is a reflexive marker.parents of pronouns, adverbs, and common nounsthat directly preceed auxiliary verbs.
By adopt-ing children we mean that we change the parentof transferred edges to be the adopting node.
Thesecond Spanish rule states that the first elementof an adjective-noun or noun-adjective pair domi-nates the second; the first element also adopts thechildren of the second element.
The third and fi-nal Spanish rule sets all prepositions to be chil-dren of the first main verb in the sentence, unlessthe preposition is a ?de?
located between two nounphrases.
In this later case, we set the closest nounin the first of the two noun phrases as the preposi-tion?s parent.For Bulgarian the first rule is that ?da?
shoulddominate all words until the next verb and adopttheir noun, preposition, particle and adverb chil-dren.
The second rule is that auxiliary verbsshould dominate main verbs and adopt their chil-dren.
We have a list of 12 Bulgarian auxiliaryverbs.
The ?seven rules?
experiments add rules for5 more words similar to the rule for ?da?, specif-ically ?qe?, ?li?, ?kakvo?, ?ne?, ?za?.
Table 3compares the errors for different linguistic rules.When we train using the ?da?
rule and the rules forauxiliary verbs, the model learns that main verbsattach to auxiliary verbs and that ?da?
dominatesits nonfinite clause.
This causes an improvementin the attachment of verbs, and also drastically re-duces words being attached to verbs instead of par-ticles.
The latter is expected because ?da?
is an-alyzed as a particle in the Bulgarian POS tagset.We see an improvement in root/verb confusionssince ?da?
is sometimes errenously attached to athe following verb rather than being the root of thesentence.The rightmost panel of Table 3 shows similaranalysis when we also use the rules for the fiveother closed-class words.
We see an improvementin attachments in all categories, but no qualitativechange is visible.
The reason for this is probablythat these words are relatively rare, but by encour-aging the model to add an edge, it also rules out in-correct edges that would cross it.
Consequently weare seeing improvements not only directly fromthe constraints we enforce but also indirectly astypes of edges that tend to get ruled out.5.4 Generative parserThe generative model we use is a state of the artmodel for unsupervised parsing and is our only374No Rules Two Rules Seven Ruleschild POS parent POSacc(%) errors errorsV 65.2 2237 T/V 2175N 73.8 1938 V/V 1305P 58.5 1705 N/V 1112R 70.3 961 root/V 555child POS parent POSacc(%) errors errorsN 78.7 1572 N/V 938P 70.2 1224 V/V 734V 84.4 1002 V/N 529R 79.3 670 N/N 376child POS parent POSacc(%) errors errorsN 79.3 1532 N/V 1116P 75.7 998 V/V 560R 69.3 993 V/N 507V 86.2 889 N/N 450Table 3: Top 4 discriminative parser errors by child POS tag and true/guess parent POS tag in the Bulgarian CoNLL train dataof length up to 10.
Training with no language-specific rules (left); two rules (center); and seven rules (right).
POS meanings:V verb, N noun, P pronoun, R preposition, T particle.
Accuracies are by child or parent truth/guess POS tag.0.6 0.65 0.7 0.7520 40 6080 100 120 140accuracy (%)supervised training data sizesupervised no rules two rulesseven rules0.65 0.7 0.75 0.820 40 6080 100 120 140accuracy (%)supervised training data sizesupervised no rules three rules0.65 0.7 0.75 0.820 40 6080 100 120 140accuracy (%)supervised training datasizesupervised no rules two rulesseven rules0.65 0.7 0.75 0.820 40 6080 100 120 140accuracy (%)supervised training datasizesupervised no rules three rulesFigure 5: Comparison to parsers with supervised estimation and transfer.
Top: Generative.
Bottom: Discriminative.
Left:Bulgarian.
Right: Spanish.
The transfer models were trained on 10k sentences all of length at most 20, all models testedon CoNLL train sentences of up to 10 words.
The x-axis shows the number of examples used to train the supervised model.Boxes show first and third quartile, whiskers extend to max and min, with the line passing through the median.
Supervisedexperiments used 30 random samples from CoNLL train.fully unsupervised baseline.
As smoothing we adda very small backoff probability of 4.5 ?
10?5 toeach learned paramter.
Unfortunately, we foundgenerative model performance was disappointingoverall.
The maximum unsupervised accuracy itachieved on the Bulgarian data is 47.6% with ini-tialization from Klein and Manning (2004) andthis result is not stable.
Changing the initializationparameters, training sample, or maximum sen-tence length used for training drastically affectedthe results, even for samples with several thousandsentences.
When we use the transferred informa-tion to constrain the learning, EM stabilizes andachieves much better performance.
Even settingall parameters equal at the outset does not preventthe model from learning the dependency structureof the aligned language.
The top panels in Figure 5show the results in this setting.
We see that perfor-mance is still always below the accuracy achievedby supervised training on 20 annotated sentences.However, the improvement in stability makes thealgorithm much more usable.
As we shall see be-low, the discriminative parser performs even betterthan the generative model.5.5 Discriminative parserWe trained our discriminative parser for 100 iter-ations of online EM with a Gaussian prior vari-ance of 100.
Results for the discriminative parserare shown in the bottom panels of Figure 5.
Thesupervised experiments are given to provide con-text for the accuracies.
For Bulgarian, we see thatwithout any hints about the annotation guidelines,the transfer system performs better than an unsu-375pervised parser, comparable to a supervised parsertrained on 10 sentences.
However, if we spec-ify just the two rules for ?da?
and verb conjuga-tions performance jumps to that of training on 60-70 fully labeled sentences.
If we have just a lit-tle more prior knowledge about how closed-classwords are handled, performance jumps above 140fully labeled sentence equivalent.We observed another desirable property of thediscriminative model.
While the generative modelcan get confused and perform poorly when thetraining data contains very long sentences, the dis-criminative parser does not appear to have thisdrawback.
In fact we observed that as the maxi-mum training sentence length increased, the pars-ing performance also improved.6 Related WorkOur work most closely relates to Hwa et al (2005),who proposed to learn generative dependencygrammars using Collins?
parser (Collins, 1999) byconstructing full target parses via projected de-pendencies and completion/transformation rules.Hwa et al (2005) found that transferring depen-dencies directly was not sufficient to get a parserwith reasonable performance, even when boththe source language parses and the word align-ments are performed by hand.
They adjusted forthis by introducing on the order of one or twodozen language-specific transformation rules tocomplete target parses for unaligned words andto account for diverging annotation rules.
Trans-ferring from English to Spanish in this way, theyachieve 72.1% and transferring to Chinese theyachieve 53.9%.Our learning method is very closely related tothe work of (Mann and McCallum, 2007; Mannand McCallum, 2008) who concurrently devel-oped the idea of using penalties based on pos-terior expectations of features not necessarily inthe model in order to guide learning.
They calltheir method generalized expectation constraintsor alternatively expectation regularization.
In thisvolume (Druck et al, 2009) use this frameworkto train a dependency parser based on constraintsstated as corpus-wide expected values of linguis-tic rules.
The rules select a class of edges (e.g.auxiliary verb to main verb) and require that theexpectation of these be close to some value.
Themain difference between this work and theirs isthe source of the information (a linguistic infor-mant vs. cross-lingual projection).
Also, we de-fine our regularization with respect to inequalityconstraints (the model is not penalized for exceed-ing the required model expectations), while theyrequire moments to be close to an estimated value.We suspect that the two learning methods couldperform comparably when they exploit similar in-formation.7 ConclusionIn this paper, we proposed a novel and effec-tive learning scheme for transferring dependencyparses across bitext.
By enforcing projected de-pendency constraints approximately and in expec-tation, our framework allows robust learning fromnoisy partially supervised target sentences, insteadof committing to entire parses.
We show that dis-criminative training generally outperforms gener-ative approaches even in this very weakly super-vised setting.
By adding easily specified language-specific constraints, our models begin to rivalstrong supervised baselines for small amounts ofdata.
Our framework can handle a wide range ofconstraints and we are currently exploring richersyntactic constraints that involve conservation ofmultiple edge constructions as well as constraintson conservation of surface length of dependen-cies.AcknowledgmentsThis work was partially supported by an Integra-tive Graduate Education and Research Trainee-ship grant from National Science Foundation(NSFIGERT 0504487), by ARO MURI SUB-TLE W911NF-07-1-0216 and by the EuropeanProjects AsIsKnown (FP6-028044) and LTfLL(FP7-212578).ReferencesA.
Abeille?.
2003.
Treebanks: Building and UsingParsed Corpora.
Springer.H.
Alshawi, S. Bangalore, and S. Douglas.
2000.Learning dependency translation models as collec-tions of finite state head transducers.
ComputationalLinguistics, 26(1).J.
Atserias, B. Casas, E. Comelles, M. Gonza?lez,L.
Padro?, and M. Padro?.
2006.
Freeling 1.3: Syn-tactic and semantic services in an open-source nlplibrary.
In Proc.
LREC, Genoa, Italy.376P.
F. Brown, S. Della Pietra, V. J. Della Pietra, and R. L.Mercer.
1994.
The mathematics of statistical ma-chine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311.C.
Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khudan-pur, L. Mangu, H. Printz, E. Ristad, R. Rosenfeld,A.
Stolcke, and D. Wu.
1997.
Structure and perfor-mance of a dependency language model.
In Proc.Eurospeech.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.G.
Druck, G. Mann, and A. McCallum.
2009.
Semi-supervised learning of dependency parsers usinggeneralized expectation criteria.
In Proc.
ACL.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: an exploration.
In Proc.
CoLing.H.
Fox.
2002.
Phrasal cohesion and statistical machinetranslation.
In Proc.
EMNLP, pages 304?311.K.
Ganchev, J. Graca, J. Blitzer, and B. Taskar.2008.
Multi-view learning over structured and non-identical outputs.
In Proc.
UAI.J.
Grac?a, K. Ganchev, and B. Taskar.
2008.
Expec-tation maximization and posterior constraints.
InProc.
NIPS.J.
Grac?a, K. Ganchev, and B. Taskar.
2009.
Post-cat - posterior constrained alignment toolkit.
In TheThird Machine Translation Marathon.A.
Haghighi, A. Ng, and C. Manning.
2005.
Ro-bust textual inference via graph matching.
In Proc.EMNLP.R.
Hwa, P. Resnik, A. Weinberg, C. Cabezas, andO.
Kolak.
2005.
Bootstrapping parsers via syntacticprojection across parallel texts.
Natural LanguageEngineering, 11:11?311.D.
Klein and C. Manning.
2004.
Corpus-based induc-tion of syntactic structure: Models of dependencyand constituency.
In Proc.
of ACL.P.
Koehn.
2005.
Europarl: A parallel corpus for statis-tical machine translation.
In MT Summit.S.
Lee and K. Choi.
1997.
Reestimation and best-first parsing algorithm for probabilistic dependencygrammar.
In In WVLC-5, pages 41?55.G.
Mann and A. McCallum.
2007.
Simple, robust,scalable semi-supervised learning via expectationregularization.
In Proc.
ICML.G.
Mann and A. McCallum.
2008.
Generalized expec-tation criteria for semi-supervised learning of con-ditional random fields.
In Proc.
ACL, pages 870 ?878.R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProc.
ACL, pages 91?98.I.
Mel?c?uk.
1988.
Dependency syntax: theory andpractice.
SUNY.
inci.P.
Merlo, S. Stevenson, V. Tsang, and G. Allaria.
2002.A multilingual paradigm for automatic verb classifi-cation.
In Proc.
ACL.R.
M. Neal and G. E. Hinton.
1998.
A new view of theEM algorithm that justifies incremental, sparse andother variants.
In M. I. Jordan, editor, Learning inGraphical Models, pages 355?368.
Kluwer.J.
Nivre and J. Nilsson.
2005.
Pseudo-projective de-pendency parsing.
In Proc.
ACL.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nils-son, S. Riedel, and D. Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.
In Proc.EMNLP-CoNLL.F.
J. Och and H. Ney.
2000.
Improved statistical align-ment models.
In Proc.
ACL.C.
Quirk, A. Menezes, and C. Cherry.
2005.
De-pendency treelet translation: syntactically informedphrasal smt.
In Proc.
ACL.L.
Shen, J. Xu, and R. Weischedel.
2008.
A newstring-to-dependency machine translation algorithmwith a target dependency language model.
In Proc.of ACL.N.
Smith and J. Eisner.
2006.
Annealing structuralbias in multilingual weighted grammar induction.
InProc.
ACL.J.
Tiedemann.
2007.
Building a multilingual parallelsubtitle corpus.
In Proc.
CLIN.K.
Toutanova, D. Klein, C. Manning, and Y. Singer.2003.
Feature-rich part-of-speech tagging with acyclic dependency network.
In Proc.
HLT-NAACL.Y.
Tsuruoka and J. Tsujii.
2005.
Bidirectional infer-ence with the easiest-first strategy for tagging se-quence data.
In Proc.
HLT/EMNLP.H.
Yamada and Y. Matsumoto.
2003a.
Statistical de-pendency analysis with support vector machines.
InProc.
IWPT, pages 195?206.H.
Yamada and Y. Matsumoto.
2003b.
Statistical de-pendency analysis with support vector machines.
InProc.
IWPT.D.
Yarowsky and G. Ngai.
2001.
Inducing multilin-gual pos taggers and np bracketers via robust pro-jection across aligned corpora.
In Proc.
NAACL.D.
Yarowsky, G. Ngai, and R. Wicentowski.
2001.Inducing multilingual text analysis tools via robustprojection across aligned corpora.
In Proc.
HLT.377
