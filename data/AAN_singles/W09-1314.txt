Proceedings of the Workshop on BioNLP, pages 108?116,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUser-Driven Development of Text Mining Resources for Cancer RiskAssessmentLin Sun, Anna KorhonenUniversity of CambridgeComputer Laboratory15 JJ Thomson AvenueCambridge CB3 0GD, UKls418,alk23@cl.cam.ac.ukIlona Silins, Ulla SteniusInstitute of Environmental MedicineKarolinska InstitutetS-17177, StockholmSwedenilona.silins,ulla.stenius@ki.seAbstractOne of the most neglected areas of biomed-ical Text Mining (TM) is the developmentof systems based on carefully assessed userneeds.
We investigate the needs of an im-portant task yet to be tackled by TM ?
Can-cer Risk Assessment (CRA) ?
and take thefirst step towards the development of TM forthe task: identifying and organizing the sci-entific evidence required for CRA in a taxon-omy.
The taxonomy is based on expert annota-tion of 1297 MEDLINE abstracts.
We reportpromising results with inter-annotator agree-ment tests and automatic classification experi-ments, and a user test which demonstrates thatthe resources we have built are well-defined,accurate, and applicable to a real-world CRAscenario.
We discuss extending and refiningthe taxonomy further via manual and machinelearning approaches, and the subsequent stepsrequired to develop TM for the needs of CRA.1 IntroductionBiomedical Text Mining (TM) has become increas-ingly popular due to the pressing need to provideaccess to the tremendous body of texts availablein biomedical sciences.
Considerable progress hasbeen made in the development of basic resources(e.g.
ontologies, annotated corpora) and techniques(e.g.
Information Retrieval (IR), Information Ex-traction (IE)) in this area, and research has beganto focus on increasingly challenging tasks, e.g.
sum-marization and the discovery of novel information inbiomedical literature (Hunter and Cohen 2006, Ana-niadou et al 2006, Zweigenbaum et al 2007).In recent past, there has been an increasing de-mand for research which is driven by actual userneeds rather than technical developments (Zweigen-baum et al 2007).
Shared tasks (e.g.
BioCreativeand the TREC Genomics track) targeting the work-flow of biomedical researchers have appeared alongwith studies exploring the TM needs of specific tasks(Karamanis et al 2008, Demaine et al 2006).
How-ever, the understanding of user needs is still one ofthe neglected areas of BIO-TM, and further user-centered evaluations and systems grounded in real-life tasks are required to determine which tools andservices are useful (Cohen et al 2008).We investigate the user needs of a challengingtask yet to be tackled by TM but identified as animportant potential application for it (Lewin et al2008): Cancer Risk Assessment (CRA).
Over thepast years, CRA has become increasingly importantas the link between environmental chemicals andcancer has become evident.
It involves examiningpublished evidence to determine the relationship be-tween exposure to a chemical and the likelihood ofdeveloping cancer from that exposure (EPA, 2005).Performed manually by experts in health related in-stitutions worldwide, CRA requires searching, lo-cating and interpreting information in biomedicaljournal articles.
It can be extremely time-consumingbecause the data for a single carcinogen may be scat-tered across thousands of articles.Given the exponentially growing volume ofbiomedical literature and the rapid development ofmolecular biology techniques, the task is now get-ting too challenging to manage via manual means.From the perspective of BIO-TM, CRA is an excel-lent example of real-world task which could greatlybenefit from a dedicated TM tool.
However, the de-velopment of a truly useful tool requires careful in-vestigation of risk assessors needs.108This paper reports our investigation of the userneeds of CRA and the creation of basic TM re-sources for the task.
Expanding on our preliminaryexperiments (Lewin et al 2008), we present a taxon-omy which specifies the scientific evidence neededfor CRA at the level of detail required for TM.
Thetaxonomy is based on expert annotation of a corpusof 1297 MEDLINE abstracts.
We report promis-ing results with inter-annotator agreement tests, au-tomatic classification of corpus data into taxonomyclasses, and a user test in a near real-world CRAscenario which shows that the taxonomy is highlyaccurate and useful for practical CRA.
We discussrefining and extending it further via manual and ma-chine learning approaches, and the subsequent stepsrequired to develop TM for the needs of CRA.2 User Needs of Cancer Risk AssessmentWe interviewed 14 experienced risk assessors work-ing for a number of authorities in Sweden1 askinga range of questions related to different aspects oftheir work.
The risk assessors described the follow-ing steps of CRA: (1) identifying the journal articlesrelevant for CRA of the chemical in question, (2)identifying the scientific evidence in these articleswhich help to determine whether/how the chemicalcauses cancer, (3) classifying and analysing the re-sulting (partly conflicting) evidence to build the tox-icological profile for the chemical, and (4) prepar-ing the risk assessment report.
These steps are con-ducted manually, relying only on standard literaturesearch engines (e.g.
PubMed) and word processors.The average time required for CRA of a singlechemical was reported to be two years when done(as usual) on a part time basis.
Risk assessors wereunanimous about the need to increase productivityto meet the current CRA demand.
They reportedthat locating and classifying the scientific evidencein literature is the most time consuming part of theirwork and that a tool capable of assisting it and ensur-ing that all the potentially relevant evidence is foundwould be particularly helpful.It became clear that a prerequisite for the devel-opment of such a tool would be an extensive spec-ification of the scientific evidence used for CRA.1Institute of Environmental Medicine at Karolinska Insti-tutet, Swedish Chemical Inspectorate, Scientific Committee onOccupational Exposure Limits (EU), Swedish Criteria Group.This evidence ?
which forms the basis of all thesubsequent steps of CRA ?
is described in theguideline documents of major international CRAagencies, e.g.
European Chemicals Agency (ECHA,2008) and the United States Environmental Protec-tion Agency (EPA, 2005).
However, although thesedocuments constitute the main reference material inCRA, they cover the main types of evidence only,do not specify the evidence at the level of detailrequired for comprehensive data gathering, and arenot updated regularly (i.e.
do not incorporate the lat-est developments in biomedical sciences).
The riskassessors admitted that rather than relying on thesedocuments, they rely on their experience and expertknowledge when looking for the evidence.
We de-cided that our starting point should be to composea more adequate specification of the scientific evi-dence needed for CRA.3 Cancer Risk Assessment TaxonomyWe recruited three experienced risk assessors to helpconstruct the resources described in sections below:(i) a representative corpus of CRA literature forparts of hazard identification (i.e.
the assessment ofwhether a chemical is capable of causing cancer),(ii) a tool for expert annotation of the corpus, (iii) anannotated corpus, and (iv) a taxonomy which classi-fies and organizes the scientific evidence discoveredin the corpus.3.1 CRA corpusVarious human, animal (in vivo), cellular (in vitro)and other mechanistic data provide evidence for haz-ard identification and the assessment of the Mode ofAction (MOA) (i.e.
the sequence of key events thatresult in cancer formation, e.g.
mutagenesis and in-creased cell proliferation) in CRA.
The experts se-lected eight chemicals which are (i) well-researchedusing a range of scientific tests and (ii) represent thetwo most frequently used MOAs ?
genotoxic andnon-genotoxic2 .
15 journals were identified whichare used frequently for CRA and jointly provide agood coverage of relevant scientific evidence (e.g.Cancer Research, Chemico-biological Interaction,Mutagenesis, Toxicological Sciences).
From these2Chemicals acting by a genotoxic MOA interact with DNA,while chemicals acting by a nongenotoxic MOA induce cancerwithout interfering directly with DNA.109Figure 1: Screenshot of the annotation tooljournals, all the PubMed abstracts from 1998-2008which include one of the 8 chemicals were down-loaded.
The resulting corpus of 1297 abstracts isdistributed per chemical as shown in Table 1.3.2 Annotation toolRisk assessors typically (i) read each abstract re-trieved by PubMed to determine its relevance forCRA, and (ii) classify each relevant abstract basedon the type of evidence it provides for CRA.
We ex-tended the tool designed for expert annotation of ab-stracts in our earlier work (Lewin et al 2008) so thatimitates this process as closely as possible.The tool provides two types of functionality.
Thefirst enables the experts to classify abstracts as rele-vant, irrelevant or unsure.
The second enables themto annotate such keywords (words or phrases) in ab-stracts and their titles which indicate the scientificevidence relevant for the task.
Keyword annotationwas chosen because the experts found it intuitive, itdid not require linguistic training, and it specifies thescientific evidence more precisely than larger spansof text.Initially a very shallow taxonomy (including onlyhuman, animal, and cellular data) and the two typesof MOA was integrated inside the tool.
This wasgradually extended as the annotation progressed.The tool permits annotating any number of relevantkeywords in the abstracts, attaching them to anyclass in the taxonomy, and classifying the same textin more than one way.
It was implemented inside thefamiliar Mozilla Firefox browser using its extensionfacility.
A screenshot illustrating the tool is providedin Figure 1.3.3 AnnotationGiven a set of initial guidelines agreed by the ex-perts, one of the experts annotated a subset of thecorpus, the other two evaluated the result, disagree-ments were then discussed, and the guidelines wereimproved where needed.
This process (crucial formaintaining quality) was repeated several times.The guidelines described below are the final resultof this work.3.3.1 Relevance annotationAn abstract is classified as (i) relevant when it (orits title) contains evidence relevant for CRA and (ii)irrelevant when it (or its title) contains no evidenceor contains ?negative?
evidence (e.g.
diseases orendpoints unrelated to cancer).
Abstracts containingvague, conflicting or complex evidence (e.g.
stud-ies on chemicals in complex mixtures) or evidencewhose association with cancer is currently unclearwere dealt on case by case basis.
All the potentiallyrelevant abstracts were included for further assess-ment as not to lose data valuable for CRA.The experts annotated the 1297 abstracts in thecorpus.
89.4% were classified as relevant, 10.1% asirrelevant, and 0.5% as unsure.
We used the Kappastatistics (Cohen 1960) to measure inter-annotatoragreement on unseen data which two experts an-notated independently.
208 abstracts were selectedrandomly from the 15 journals and from 16 jour-nals likely to be irrelevant for CRA.
The latter wereincluded to make the task harder as the proportionof relevant abstracts was high in our corpus.
OurKappa result is 0.68 ?
a figure which indicates sub-stantial agreement (Landis and G.Koch 1977).The experts disagreed on 24 (11.5% of the) ab-stracts.
Half of the disagreements are due to oneof the annotators failing to notice relevant evidence.Such cases are likely to decrease when annotatorsgain more experience.
The other half are caused byvague or conflicting evidence.
Many of these couldbe addressed by further development of guidelines.3.3.2 Keyword annotationKeyword annotation focussed on the types of sci-entific evidence experts typically look for in CRA:carcinogenic activity (human, animal, cellular, andother mechanistic data), Mode of Action (MOA)(data for a specific MOA type ?
genotoxic or non-110Chemical Retrieved Relevant1,3-butadiene 195 187phenobarbital 270 240diethylnitrosamine 221 214diethylstilbestrol 145 110benzoapyrene 201 192fumonisin 80 70chloroform 96 84styrene 162 132Total 1297 1164Table 1: Total of abstracts per chemicalgenotoxic), and relevant parts of toxicokinetics (e.g.metabolic activation).
The experts annotated thekeywords which they considered as the most impor-tant and which jointly identify the types of scientificdata offered by the abstract.
They focussed on new(rather than previously published) data on the chem-ical in question.All the 1164 abstracts deemed relevant were an-notated.
A total of 1742 unique keywords wereidentified, both simple nouns and complex nomi-nals / phrases.
Figure 1 shows an example of anannotated abstract where the keyword chromoso-mal aberrations is identified as evidence for geno-toxic MOA.
Since the experts were not required toannotate every relevant keyword, calculating inter-annotator agreement was not meaningful.
However,the keyword annotation was evaluated jointly withtaxonomy classification (the following section).3.4 The taxonomy and the resulting corpusDuring keyword annotation, the initial taxonomywas extended and refined with new classes and classmembers.
The resulting taxonomy relies solely onexpert knowledge.
Experts were merely advisedon the main principles of taxonomy creation: theclasses should be conceptually coherent and their hi-erarchical organization should be in terms of coher-ent sub- and superordinate relations.The taxonomy contains three top level classes:1) Carcinogenic activity (CA), 2) Mode of Action(MOA) and 3) Toxicokinetics (TOX).
1) and 2) areorganized by TYPE-OF relations (leukemia is a typeof carcinogenic evidence) and 3) by PART-OF rela-tions (biodegradation is a part of Metabolism).
Eachtop level class divides into sub-classes.
Figure 2shows CA taxonomy with three keyword examplesper class.
The taxonomy has 48 classes in total; halfof them under CA.
Table 6 shows the total numberof abstracts and keywords per class: 82.4% of theabstracts include keywords for CA, and 50.3% and28.1% for MOA and TOX, respectively.We calculated inter-annotator agreement for as-signing abstracts to taxonomy classes.
For each ofthe 8 chemicals, 10 abstracts were randomly cho-sen from the 15 journals.
The average agreementbetween two annotators is the highest with CA andMOA (78%) and the lowest with TOX (62%).
Theoverall agreement is 76%.
This result is good, par-ticularly considering the high number of classes andthe chance agreement of 1.5%.
The disagreementsare mostly due to one of the experts annotating asmany keywords as possible, and the other one an-notating only the ones that classify each abstract asprecisely as possible.
This was not a serious prob-lem for us, but it demonstrates the importance of de-tailed guidelines.
Also, some of the classes were tooimprecise to yield unique distinctions.
Future workshould focus on refining them further.4 Automatic classificationTo examine whether the classification created by ex-perts provides a good representation of the corpusdata and is machine learnable, we conducted a se-ries of abstract classification experiments.4.1 Methods4.1.1 Feature extractionThe first step of text categorization (TC) is totransform documents into a feature vector represen-tation.
We experimented with two document rep-resentation techniques.
The first one is the sim-ple ?bag of words?
approach (BOW) which consid-ers each word in the document as a separate feature.BOW was evaluated using three methods which haveproved useful in previous TC work: (i) stemming(using the Porter (1980) stemmer) which removesaffixes from words, (ii) the TFIDF weighting (Kib-riya et al 2004), and (iii) stop word removal.The second technique is the recent ?bag of sub-strings?
(BOS) method by (Wang et al 2008) whichconsiders the whole abstract as a string and extractsfrom it all the length p substrings without affix re-moval.
BOS has proved promising in biomedicalTC (Han et al 2006, Wang et al 2008) and un-like a traditional grammatical stemmer, does not re-111Figure 2: Taxonomy of Carcinogenic Activityquire domain tuning for optimal performance.
Be-cause BOS generates substrings with fixed length p,a word shorter than p?2 can get obscured by its con-text3.
For example, ?mice?
would be transformed to?
mice a?, ?
mice b?, .
.
.
, which is less informativethan the original word form.
Therefore, we enrichedBOS features with word forms shorter than p?
2.4.1.2 Feature selectionWe employed two feature selection methods fordimensionality reduction.
The first is InformationGain (IG) which has proved useful in TC (Yangand Pedersen 1997).
Given a feature?s distribu-tion X and class label distribution Y , IG(X) =H(Y ) ?
H(Y |X), H(X) is the entropy of X. Thesecond method fscore optimises the number of fea-tures (N ).
Features are first ranked using the simplefscore criterion (Chen and Lin 2006), and N is se-lected based on the performance of the SVM classi-fier using the N features.4.1.3 ClassificationThree classifiers were used: Naive Multino-mial Bayesian (NMB), Complement Naive Bayesian(CNB) (Rennie and Karger 2003) and Linear Sup-port Vector Machines (L-SVM) (Vapnik 1995).NMB is a widely used classifier in TC (Kib-riya et al 2004).
It selects the class C withthe maximum probability given the document d:argmaxc Pr(C)?w?d Pr(X = w|C).
Pr(C) can3Minus 2 because of space characters.be estimated from the frequency of documents in C .Pr(X = w|C) is estimated as the fraction of tokensin documents of class C that contain w.CNB extends NMB by addressing the problemsit has e.g.
with imbalanced data and weightmagnitude error.
The class c of a documentis: argmaxc[logp(?c)?
?i filogNc?i+?iNc?+?
].
Nc?i is thenumber of times term i occurs in classes other thanc.
?
and ?i are the smoothing parameters.
p(?c) isthe prior distribution of class c.L-SVM is the basic type of SVM which pro-duces a hyperplane that separates two-class sampleswith a maximum margin.
It handles high dimen-sional data efficiently, and has shown to performwell in TC (Yang and Liu 1999).
Given the dataset X = (x1, y1), .
.
.
, (xn, yn) yi ?
{?1,+1},L-SVM requires a solution w to the following un-constrained optimisation problem: min(12wTw +C?ni=1 max(1 ?
yiwTxi, 0)2.
Cost parameter Cwas estimated within range 22,. .
.
, 25 on trainingdata using cross validation.
The C of the posi-tive class was weighted by class population ratior = negative populationpositive population .4.1.4 EvaluationWe used the standard measures of recall (R), pre-cision (P) and F measure (F) for evaluation.
Theseare defined as follows:R = TPTP+FN P = TPTP+FP F = 2?R?PR+POur random baseline is P+N+P+ .112P+/N : positive/negative population TP: truth positive; FN: false negative, FP: false positive4.2 Experimental evaluation4.2.1 DataOur data was the expert annotated CRA corpus.4.2.2 Document preprocessingWe first evaluated the BOW preprocessing tech-nique with and without the use of (i) the Porter(1980) stemmer, (ii) TFIDF, (iii) stop word removal,and (iv) their combinations.
The evaluation wasdone in the context of the binary relevance classifica-tion of abstracts (not in the context of the main tax-onomic classification task to avoid overfitting pre-processing techniques to the taxonomy).
Only (iii)improved all the classifiers and was thus adoptedfor the main experiments.
The poor performanceof (i) demonstrates that a standard stemmer is notoptimal for our data.
As highlighted by (Han et al2006, Wang et al 2008), semantically related bio-logical terms sharing the same stem are not alwaysreducible to the stem form.4.2.3 Feature selectionWe evaluated the feature selection methods ontwo taxonomy classes: the most balanced class ?An-imal study?
(positive/negative 1:1.4) and an imbal-anced class ?Adducts?
(positive/negative 1:6.5).
IGwas used for the fixed N setting and fscore for thedynamic N setting.
Each combination of classifiers(NMB/CNB/SVM), document representations (BOW,BOS) and settings for N (dynamic, .
.
.
, 83098) wasevaluated.
The results show that the dynamic settingyields consistent improvement on all the setups (al-though the impact on SVM?s is not big).
Also theoptimal N varies by the data and the classifier.
Thus,we used the dynamic feature selection in the taxo-nomic classification.4.2.4 Taxonomic classificationExperimental setup We ran two sets of experi-ments on the corpus, using 1) BOW and 2) BOS forfeature extraction.
Without feature selection, BOWhad c. 9000 features and BOS c. 83000.
Featureswere selected using fscore.
For each class withmore than 20 abstracts (37 in total)4, three ?one4The classes with less than 20 abstracts may have less than2 positive abstracts in each fold of 10 fold CV, which is notMethod Feature Set P R FNMB BOW 0.59 0.75 0.66NMB BOS 0.62 0.82 0.70CNB BOW 0.52 0.74 0.60CNB BOS 0.57 0.76 0.64SVM BOW 0.68 0.76 0.71SVM BOS 0.71 0.77 0.74Table 2: Performance of classifiers with BOS/BOWClass Method P R FCA NMB 0.94 0.89 0.91CA CNB 0.92 0.94 0.93CA SVM 0.93 0.93 0.93MOA NMB 0.88 0.81 0.84MOA CNB 0.84 0.82 0.83MOA SVM 0.92 0.80 0.86TOX NMB 0.66 0.83 0.74TOX CNB 0.70 0.80 0.75TOX SVM 0.76 0.79 0.78Table 3: Result for the top level classesagainst other?
classifiers (NMB, CNB and L-SVM)were trained and tested using 10-fold cross valida-tion.Results Table 2 shows the average performancefor the whole taxonomy.
The performance of BOSis better than that of BOW according to all the threemeasures.
On average, BOS outperforms BOW by4% in P and F, and 3% in R. SVM yields the bestoverall P and F (0.71 and 0.74) with BOS.
Surpris-ingly, NMB outperforms CNB with all the settings.NMB yields the best overall R with BOS (0.82) butits P is notably lower than that of SVM.Table 3 shows the average P, R and F for the toplevel classes using the best performing feature setBOS with the three classifiers.
CA has the best F(0.93).
Its positive population is the highest (posi-tive/negative: 5:1).
TOX with a lower positive pop-ulation (1:2.6) has still good F (0.78).
R and P arebalanced with an average difference of 0.06.Table 4 shows the distribution of F across thetaxonomy.
There is a clear correlation betweenrepresentative for the class population.No.
of abstracts(f) Classes F Randomf > 300 9 0.80 0.38100 < f ?
300 12 0.73 0.1320 < f ?
100 16 0.68 0.04Table 4: Mean F and random baseline for taxonomicclasses in three frequency ranges.113frequency and performance: the average F de-creases with descending frequency range, revealingincreased classification difficulty.
Classes with morethan 300 abstracts have the highest average F (0.80with standard deviation (SD) 0.08).
Classes with20-100 abstracts have the average F 0.68 (SD 0.11),which is lower but still fairly good.
No class has Flower than 0.46, which is much higher than the av-erage random baseline of 0.11.5 User TestA user test was carried out to examine the practicalusefulness of the automatic classification in a nearreal-world scenario.
The L-SVM+BOS classifier wasapplied to the PubMed abstract data (from 1998-2008) of five unseen chemicals representing geno-toxic (geno) and non-genotoxic (non) MOAs (seetable 5).
The results were displayed to two expertsin a friendly web interface.
The experts were in-vited to imagine that they have submitted a query toa system, the system has returned the classificationof relevant abstracts for each chemical, and the taskis to judge whether it is correct.
The top 500 BOSfeatures per class were shown to aid the judgement.Results were evaluated using precision (P) (re-call could not be calculated as not all of the positivepolulation was known).
Table 5 shows the averageP for chemicals and top level classes.
The resultsare impressive: the only chemical with P lower than0.90 is polychlorinated biphenyls (PCB).
As PCBhas a well-known neuro-behavioural effect, the dataincludes many abstracts irrelevant for CRA.
Mostother errors are due to the lack of training data forlow frequency classes.
For example, the CRA cor-pus had only 27 abstracts in ?DNA repair (damage)?class, while the new corpus has many abstracts onDNA damage some of which are irrelevant for CRA.The experts found the tool easy to use and feltthat if such a tool was available to support real-worldCRA, it could significantly increase their productiv-ity and also lead to more consistent and thoroughCRA.
Such a wide range of scientific evidence is dif-ficult to gather via manual means, and chemical car-cinogenesis is such a complex process that even themost experienced risk assessor is incapable of mem-orizing the full range of relevant evidence withoutthe support of a thorough specification / taxonomy.Name MOA ?
PAflatoxin B1 geno 189 0.95Benzene geno 461 0.99PCB non 761 0.89Tamoxifen non 382 0.96TCDD non 641 0.96Class PCA 0.94MOA 0.95TOX 0.99Table 5: Chemicals and the results of the user test6 Conclusion and Future WorkThe results of our inter-annotator agreement tests,automatic classification experiments and the usertest demonstrate that the taxonomy created by riskassessors is accurate, well-defined, and can be use-ful in a real-world CRA scenario.
This is particu-larly encouraging considering that the taxonomy isbased on biomedical annotation.
As highlighted by(Kim et al 2008), expert annotation is more chal-lenging and prone to inter-annotator disagreementthan better-constrained linguistic annotation.
Webelieve that we obtained promising results becausewe worked in collaboration with risk assessors anddeveloped technology which imitates their currentpractices as closely as possible.Most related work focuses on binary classifica-tion, e.g.
BioCreative II had a subtask (Krallingeret al 2008) on the relevance classification of ab-stracts for protein interactions.
The few worksthat have attempted multi-classification include e.g.that of Aphinyanaphongs et al (2005) who appliedNMB, SVM and AdaBoost to classify abstracts ofinternal medicine into four categories, and that ofHan et al (2006) who used BOS and NMB/L-SVM toclassify abstracts in five categories of protein post-translational modifications.In the future, we plan to refine the taxonomy fur-ther by careful analysis of keyword types found inthe data and the taxonomic relationships defined byexperts.
This will help to transform the taxonomyinto a better-developed knowledge resource.
Wealso need to extend the taxonomy.
Although ourresults show that the current taxonomy provides agood basis for the classification of CRA literature,it is not comprehensive: more data is required espe-cially for low frequency classes, and the taxonomyneeds to be extended to cover more specific MOAtypes (e.g.
further subtypes of non-genotoxic chem-icals).The taxonomy can be extended by manual annota-114Change in F ?
Classes Abstracts of class20-100 100 - 200 200 - 1100?F > 1% 16 (43%) 75% 33% 8%|?F | ?
1% 15 (41%) 6% 44% 75%?F < ?1% 6 (16%) 19% 33% 17%Table 6: F gain(?F ) of MeSH compared to BOSClass ?
FCarcinogenic activity 1068 92.8Human study/epidemiology 190 77.7Animal study 629 80.2Cell experiments 319 78.5Study on microorganisms 44 85.2Mode of Action 653 85.5Genotoxic 421 89.1Nongenotoxic 324 76.3Toxicokinetics 356 77.7Absorption, .
.
.
,excretion 113 69.8Metabolism 268 76.4Toxicokinetic modeling 31 84.6Table 7: ?
abstracts and F of level 1,2 classes.tion, supplementing it with additional information inknowledge resources and/or by automatic methods.One knowledge resource potentially useful is theMedical Subject Headings (MeSH) taxonomy (Nel-son et al 2002) which classifies PubMed abstractsaccording to manually defined terms.
We performeda small experiment to investigate the usefulness ofMeSH for supplementing our current classification.MeSH terms were first retrieved for each abstract us-ing EFetch (NCBI 2005) and then appended to theBOS feature vector.
Best features were then selectedusing fscore and classified using L-SVM.
The fig-ures in table 6 show that the results improved sig-nificantly for 43% of the low frequency classes.
Al-though this demonstrates the potential usefulness ofadditional resources, given the rapidly evolving na-ture of CRA data, the best approach long term isto develop technology for automatic updating of thetaxonomy from literature.
Given the basic resourceswe have constructed, the development of such tech-nology is now realistic and can be done using unsu-pervised or semi-supervised machine learning tech-niques, e.g.
(Cohen and Hersh 2005, Blaschko andGretton 2009).The automatic classification could be improvedby the use of more sophisticated features extractedusing NLP tools that have been tuned for biomedi-cal texts, such as parsers, e.g.
(Tsuruoka et al 2005),and named entity recognizers, e.g.
(Corbett et al2007), and exploiting resources such as the BioLex-ion (Sasaki et al 2008).Our long term goal is to develop a TM toolspecifically designed for CRA.
Some tools have re-cently been built to assist other critical activities ofbiomedicine (e.g.
literature curation for genetics).A few of them have been evaluated for their practi-cal usefulness in a real-world scenario (Karamaniset al 2008, Demaine et al 2006).
Such tools andevaluations act as an important proof of concept forbiomedical TM and help to develop technology forthe needs of practical applications.According to the interviews we conducted (Sec-tion 2), a tool capable of identifying, ranking andclassifying articles based on the evidence they con-tain, displaying the results to experts, and assistingalso in subsequent steps of CRA would be particu-larly welcome.
Such a tool, if developed in closecollaboration with users, could significantly increasethe productivity of CRA and enable risk assessorsto concentrate on what they are best at: the expertjudgement.Acknowledgements Our work was funded by theRoyal Society (UK), the Medical Research Council(G0601766) (UK) and the Swedish Council for WorkingLife and Social Research (Sweden).
LS was supportedby a Dorothy Hodgkin Postgraduate Award (UK).
Wewould like to thank Ian Lewin for his assistance at theearly stages of this work and for providing the first ver-sion of the annotation tool.
We are also grateful to JohanHogberg for supporting the annotation and the taxonomyconstruction work.ReferencesSophia Ananiadou, Douglas B. Kell, and Jun ichi Tsujii.Text mining and its potential applications in systemsbiology.
Trends in Biotechnology, 24(12), 2006.Y.
Aphinyanaphongs, I. Tsamardinos, A. Statnikov,D.
Hardin, and C.F.
Aliferis.
Text categorizationmodels for high-quality article retrieval in internalmedicine.
JAMIA, 12(2), 2005.Matthew Blaschko and Arthur Gretton.
Learning tax-onomies by dependence maximization.
In 22rd NIPS,2009.Yi-Wei Chen and Chih-Jen Lin.
Combining SVMs withvarious feature selection strategies.
In Feature extrac-tion, foundations and applications.
2006.Aaron M. Cohen and William R. Hersh.
A survey of115current work in biomedical text mining.
Briefings inBioinformatics, 6(1), 2005.Jacob Cohen.
A coefficient of agreement for nominalscales.
Educ.
Psychol.
Meas., 20(1), 1960.K.
Bretonnel Cohen, Hong Yu, Philip E. Bourne, andLynette Hirschman.
Translating biology:text miningtools that work.
In PSB, 2008.Peter Corbett, Colin Batchelor, and Simone Teufel.
An-notation of chemical named entities.
In Proceedings ofthe ACL, 2007.Jeffrey Demaine, Joel Martin, Lynn Wei, and Berryde Bruijn.
Litminer: integration of library serviceswithin a bio-informatics application.
Biomedical Dig-ital Libraries, 3(1), 2006.ECHA, 2008.
Guidance on Information Requirementsand Chemical Safety Assessment.
European ChemicalsAgency, 2008.Bo Han, Zoran Obradovic, Zhang zhi Hu, Cathy H. Wu,and Slobodan Vucetic.
Substring selection for biomed-ical document classification.
Bioinformatics, 22, 2006.Lawrence Hunter and K. Bretonnel Cohen.
Biomedicallanguage processing: What?s beyond pubmed?
MolCell, 21(5), 2006.N.
Karamanis, R. Seal, I. Lewin, P. McQuilton, A. Vla-chos, C. Gasperin, R. Drysdale, and T. Briscoe.
Nat-ural language processing in aid of flybase curators.BMC Bioinformatics, 9(1), 2008.Ashraf M. Kibriya, Eibe Frank, Bernhard Pfahringer, andGeoffrey Holmes.
Multinomial naive bayes for textcategorization revisited.
In Australian Conference onAI, volume 3339, 2004.Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
Cor-pus annotation for mining biomedical events from lter-ature.
BMC Bioinformatics, 9, 2008.Martin Krallinger, Florian Leitner, Carlos Rodriguez-Penagos, and Alfonso Valencia.
Overview of theprotein-protein interaction annotation extraction taskof biocreative ii.
Genome Biology, 2008.J.Richard Landis and Gary G.Koch.
The measurement ofobserver agreement for categorical data.
Biometrics,33(1), 1977.Ian Lewin, Ilona Silins, Anna Korhonen, Johan Hogberg,and Ulla Stenius.
A new challenge for text mining:Cancer risk assessment.
In Proceedings of the ISMBBioLINK Special Interest Group on Text Data Mining.,2008.NCBI.
Efetch entrez utility, 2005.
URLhttp://www.ncbi.nlm.nih.gov/entrez/query/static/efetch_help.html.Sturart J. Nelson, Tammy Powell, and Besty L.Humphreys.
The Unified Medical Language System(UMLS) Project.
In Encyclopedia of Library and In-formation Science, pages 369?378.
Marcel Dekker,2002.M.
F. Porter.
An algorithm for suffix stripping.
Program,14(3):130?137, 1980.Jason D. M. Rennie and David Karger.
Tackling the poorassumptions of naive bayes text classifiers.
In In Pro-ceedings of the 20th ICML, 2003.Y.
Sasaki, S. Montemagni, P. Pezik, D. Rebholz-Schuhmann, J. McNaught, and S. Ananiadou.
BioLex-icon: A Lexical Resource for the Biology Domain.2008.Y.
Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,S.
Ananiadou, and J. Tsujii.
Developing a Robust Part-of-Speech Tagger for Biomedical Text.
3746, 2005.EPA, 2005.
Guidelines for carcinogen risk as-sessment.
U.S. Environmental Protection Agency,2005.
URL http://www.epa.gov/iris/cancer032505.pdf.Vladimir N. Vapnik.
The nature of statistical learningtheory.
New York, NY, USA, 1995.Hongning Wang, Minlie Huang, Shilin Ding, and Xi-aoyan Zhu.
Exploiting and integrating rich featuresfor biological literature classification.
BMC Bioinfor-matics, 9(Suppl 3), 2008.Yiming Yang and Xin Liu.
A re-examination of text cate-gorization methods.
In Proceedings of the 22nd SIGIR,New York, NY, USA, 1999.Yiming Yang and Jan O. Pedersen.
A comparative studyon feature selection in text categorization.
1997.Pierre Zweigenbaum, Dina Demner-Fushman, Hong Yu,and Kevin B. Cohen.
Frontiers of biomedical text min-ing: current progress.
Brief Bioinform, 8(5), 2007.116
