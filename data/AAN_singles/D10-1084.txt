Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 862?871,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsClassifying Dialogue Acts in One-on-one Live ChatsSu Nam Kim,?
Lawrence Cavedon?
and Timothy Baldwin??
Dept of Computer Science and Software Engineering, University of Melbourne?
School of Computer Science and IT, RMIT Universitysunamkim@gmail.com, lcavedon@gmail.com, tb@ldwin.netAbstractWe explore the task of automatically classify-ing dialogue acts in 1-on-1 online chat forums,an increasingly popular means of providingcustomer service.
In particular, we investi-gate the effectiveness of various features andmachine learners for this task.
While a sim-ple bag-of-words approach provides a solidbaseline, we find that adding information fromdialogue structure and inter-utterance depen-dency provides some increase in performance;learners that account for sequential dependen-cies (CRFs) show the best performance.
Wereport our results from testing using a corpusof chat dialogues derived from online shop-ping customer-feedback data.1 IntroductionRecently, live chats have received attention due tothe growing popularity of chat services and the in-creasing body of applications.
For example, largeorganizations are increasingly providing support orinformation services through live chat.
One advan-tage of chat-based customer service over conven-tional telephone-based customer service is that itbecomes possible to semi-automate aspects of theinteraction (e.g.
conventional openings or cannedresponses to standard questions) without the cus-tomer being aware of it taking place, something thatis not possible with speech-based dialogue systems(as synthesised speech is still easily distinguishablefrom natural speech).
Potentially huge savings canbe made by organisations providing customer helpservices if we can increase the degree of automationof live chat.Given the increasing impact of live chat services,there is surprisingly little published computationallinguistic research on the topic.
There has been sub-stantially more work done on dialogue and dialoguecorpora, mostly in spoken dialogue (e.g.
Stolcke etal.
(2000)) but also multimodal dialogue systems inapplication areas such as telephone support service(Bangalore et al, 2006) and tutoring systems (Lit-man and Silliman, 2004).
Spoken dialogue analysisintroduces many complications related to the errorinherent in current speech recognition technologies.As an instance of written dialogue, an advantage oflive chats is that recognition errors are not such an is-sue, although the nature of language used in chat istypically ill-formed and turn-taking is complicatedby the semi-asynchronous nature of the interaction(e.g.
Werry (1996)).In this paper, we investigate the task of automaticclassification of dialogue acts in 1-on-1 live chats,focusing on ?information delivery?
chats since theseare proving increasingly popular as part of enter-prise customer-service solutions.
Our main chal-lenge is to develop effective features and classifiersfor classifying aspects of 1-on-1 live chat.
Much ofthe work on analysing dialogue acts in spoken di-alogues has relied on non-lexical features, such asprosody and acoustic features (Stolcke et al, 2000;Julia and Iftekharuddin, 2008; Sridhar et al, 2009),which are not available for written dialogues.
Pre-vious dialogue-act detection for chat systems hasused bags-of-words (hereafter, BoW) as featuresfor dialogue-act detection; this simple approachhas shown some promise (e.g.
Bangalore et al(2006), Louwerse and Crossley (2006) and Ivanovic(2008)).
Other features such as keywords/ontologies(Purver et al, 2005; Forsyth, 2007) and lexical cues(Ang et al, 2005) have also been used for dialogueact classification.862In this paper, we first re-examine BoW featuresfor dialogue act classification.
As a baseline, weuse the work of Ivanovic (2008), which explored 1-grams and 2-grams with Boolean values in 1-on-1live chats in the MSN Online Shopping domain (thisdataset is described in Section 5).
Although thiswork achieved reasonably high performance (up toa micro-averaged F-score of around 80%), we be-lieve that there is still room for improvement usingBoW only.
We extend this work by using ideas fromrelated research such as text categorization (Deboleand Sebastiani, 2003), and explore variants of BoWbased on analysis of live chats, along with featureweighting.
Finally, our main aim is to explore newfeatures based on dialogue structure and dependen-cies between utterances1 that can enhance the use ofBoW for dialogue act classification.
Our hypothesisis that, for task-oriented 1-on-1 live chats, the struc-ture and interactions among utterances are useful inpredicting future dialogue acts: for example, conver-sations typically start with a greeting, and questionsand answers typically appear as adjacency pairs ina conversation.
Therefore, we propose new featuresbased on structural and dependency information de-rived from utterances (Sections 4.2 and 4.3).2 Related WorkWhile there has been significant work on classify-ing dialogue acts, the bulk of this has been for spo-ken dialogue.
Most such work has considered: (1)defining taxonomies of dialogue acts; (2) discover-ing useful features for the classification task; and (3)experimenting with different machine learning tech-niques.
We focus here on (2) and (3); we return to(1) in Section 3.For classifying dialogue acts in spoken dialogue,various features such as dialogue cues, speech char-acteristics, and n-grams have been proposed.
Forexample, Samuel et al (1998) utilized the charac-teristics of spoken dialogues and examined speakerdirection, punctuation marks, cue phrases and n-grams for classifying spoken dialogues.
Jurafsky etal.
(1998) used prosodic, lexical and syntactic fea-tures for spoken dialogue classification.
More re-cently, Julia and Iftekharuddin (2008) and Sridhar et1An utterance is the smallest unit to deliver a participant?smessage(s) in a turn.al.
(2009) achieved high performance using acous-tic and prosodic features.
Louwerse and Cross-ley (2006), on the other hand, used various n-gramfeatures?which could be adapted to both spokenand written dialogue?and tested them using theMap Task Corpus (Anderson et al, 1991).
Extend-ing the discourse model used in previous work, Ban-galore et al (2006) used n-grams from the previous1?3 utterances in order to classify dialogue acts forthe target utterance.There has been substantially less effort on clas-sifying dialogue acts in written dialogue: Wu et al(2002) and Forsyth (2007) have used keyword-basedapproaches for classifying online chats; Ivanovic(2008) tested the use of n-gram features for 1-on-1live chats with MSN Online Shopping assistants.Various machine learning techniques have beeninvestigated for the dialogue classification task.Samuel et al (1998) used transformation-basedlearning to classify spoken dialogues, incorporat-ing Monte Carlo sampling for training efficiency.Stolcke et al (2000) used Hidden Markov Mod-els (HMMs) to account for the structure of spo-ken dialogues, while Wu et al (2002) also usedtransformation- and rule-based approaches plusHMMs for written dialogues.
Other researchershave used Bayesian based approaches, such asnaive Bayes (e.g.
(Grau et al, 2004; Forsyth,2007; Ivanovic, 2008)) and Bayesian networks (e.g.
(Keizer, 2001; Forsyth, 2007)).
Maximum entropy(e.g.
(Ivanovic, 2008)), support vector machines(e.g.
(Ivanovic, 2008)), and hidden Markov models(e.g.
(Bui, 2003)) have also all been applied to auto-matic dialogue act classification.3 Dialogue ActsA number of dialogue act taxonomies have been pro-posed, designed mainly for spoken dialogue.
Manyof these use the Dialogue Act Markup in SeveralLayers (DAMSL) scheme (Allen and Core, 1997).DAMSL was originally applied to the TRAINS cor-pus of (transcribed) spoken task-oriented dialogues,but various adaptations of it have since been pro-posed for specific types of dialogue.
The Switch-board corpus (Godfrey et al, 1992) defines 42 typesof dialogue acts from human-to-human telephoneconversations.
The HCRCMap Task corpus (Ander-863son et al, 1991) defines a set of 128 dialogue acts tomodel task-based spoken conversations.For casual online chat dialogues, Wu et al (2002)define 15 dialogue act tags based on previously-defined dialogue act sets (Samuel et al, 1998;Shriberg et al, 1998; Jurafsky et al, 1998; Stolckeet al, 2000).
Forsyth (2007) defines 15 dialogue actsfor casual online conversations, based on 16 conver-sations with 10,567 utterances.
Ivanovic (2008) pro-poses 12 dialogue acts based on DAMSL for 1-on-1online customer service chats.Ivanovic?s set of dialogue acts for chat dia-logues has significant overlap with the dialogue actsets of Wu et al (2002) and Forsyth (2007) (e.g.GREETING, EMOTION/EXPRESSION, STATEMENT,QUESTION).
In our work, we re-use the set of dia-logue acts proposed in Ivanovic (2008), due to ourtargeting the same task of 1-on-1 IM chats, and in-deed experimenting over the same dataset.
The def-initions of the dialogue acts are provided in Table 1,along with examples.4 Feature SelectionIn this section, we describe our initial dialogue-actclassification experiments using simple BoW fea-tures, and then introduce two groups of new fea-tures based on structural information and dependen-cies between utterances.4.1 Bag-of-Wordsn-gram-based BoW features are simple yet effec-tive for identifying similarities between two utter-ances, and have been used widely in previous workon dialogue act classification for online chat di-alogues (Louwerse and Crossley, 2006; Ivanovic,2008).
However, chats containing large amounts ofnoise such as typos and emoticons pose a greaterchallenge for simple BoW approaches.
On the otherhand, keyword-based features (Forsyth, 2007) haveachieved high performance; however, keyword-based approaches are more domain-dependent.
Inthis work, we chose to start with a BoW approachbased on our observation that commercial live chatservices contain relatively less noise; in particular,the commercial agent tends to use well-formed, for-mulaic prose.Previously, Ivanovic (2008) explored Boolean 1-gram and 2-gram features to classify MSN OnlineShopping live chats, where a user requests assis-tance in purchasing an item, in response to which thecommercial agent asks the customer questions andmakes suggestions.
Ivanovic (2008) achieved solidperformance over this data (around 80% F-score).While 1-grams performed well (as live chat utter-ances are generally shorter than, e.g., sentences innews articles), we expect 2- and 3-grams are neededto detect formulaic expressions, such as No problemand You are welcome.
We would also expect a pos-itive effect from combining n-grams due to increas-ing the coverage of feature words.
We thus test 1-,2- and 3-grams individually, as well as the combi-nation of 1- and 2-grams together (i.e.
1+2-grams)and 1-, 2- and 3-grams (i.e.
1+2+3-grams); this re-sults in five BoW sets.
Also, unlike Ivanovic (2008),we test both raw words and lemmas; we expect theuse of lemmas to perform better than raw words asour data is less noisy.
As the feature weight, in addi-tion to simple Boolean, we also experiment with TF,TF?IDF and Information Gain (IG).4.2 Structural InformationOur motivation for using structural information asa feature is that the location of an utterance can bea strong predictor of the dialogue act.
That is, dia-logues are sequenced, comprising turns (i.e.
a givenuser is sending text), each of which is made up ofone or more messages (i.e.
strings sent by the user).Structured classification methods which make use ofthis sequential information have been applied to re-lated tasks such as tagging semantic labels of keysentences in biomedical domains (Chung, 2009) andpost labels in web forums (Kim et al, 2010).Based on the nature of live chats, we observed thatthe utterance position in the chat, as well as in a turn,plays an important role when identifying its dialogueact.
For example, an utterance such as Hello will oc-cur at the beginning of a chat while an utterance suchas Have a nice day will typically appear at the end.The position of utterances in a turn can also helpidentify the dialogue act; i.e.
when there are severalutterances in a turn, utterances are related to eachother, and thus examining the previous utterances inthe same turn can help correctly predict the targetutterance.
For example, the greeting (Welcome to ..)and question (How may I help you?)
could occur in864Dialogue Act, Definition and ExamplesCONVENTIONAL CLOSING: Various ways of ending a conversation e.g.
Bye ByeCONVENTIONAL OPENING: Greeting and other ways of starting a conversation e.g.
Hello CustomerDOWNPLAYER: A backwards-linking label often used after THANKS to down play the contributione.g.
You are welcome, my pleasureEXPRESSIVE: An acknowledgement of a previous utterance or an indication of the speaker?s mood.e.g.
haha, : ?)
wowNO ANSWER: A backward-linking label in the form of a negative response to a YESNO-QUESTION e.g.
no, nopeOPEN QUESTION: A question that cannot be answered with only a yes or no.
The answer is usuallysome form of explanation or statement.
e.g.
how do I use the international version?REQUEST: Used to express a speaker?s desire that the learner do something ?
either performing some actionor simply waiting.
e.g.
Please let me know how I can assist you on MSN Shopping today.RESPONSE ACK: A backward-linking acknowledgement of the previous utterance.
Used to confirmthat the previous utterance was received/accepted.
e.g.
SureSTATEMENT: Used for assertions that may state a belief or commit the speaker to doing somethinge.g.
I am sending you the page which will pop up in a new window on your screen.THANKS: Conventional thanks e.g.
Thank you for contacting us.YES ANSWER: A backward-linking label in the form of an affirmative response to a YESNO-QUESTION e.g.
yes, yeahYESNO QUESTION: A closed question which can be answered in the affirmative or negative.e.g.
Did you receive the page, Customer?Table 1: The set of dialogue acts used in this research, taken from Ivanovic (2008)the same turn.
We also noticed that identifying theutterance author can help classify the dialogue act(previously used in Ivanovic (2008)).Based on these observations, we tested the follow-ing four structural features:?
Author information,?
Relative position in the chat,?
Author + Relative position,?
Author + Turn-relative position among utter-ances in a given turn.We illustrate our structural features in Table 2,which shows an example of a 1-on-1 live chat.
Theparticipants are the agent (A) and customer (C); Uxxindicates an utterance (U) with ID number xx.
Thisconversation has 42 utterances in total.
The relativeposition is calculated by dividing the utterance num-ber by the total number of utterances in the dialogue;the turn-relative position is calculated by dividingthe utterance position by the number of utterancesin that turn.
For example, for utterance 4 (U4), therelative position is 442 , while its turn-relative positionis 23 since U4 is the second utterance among U3,4,5that the customer makes in a single turn.4.3 Utterance DependencyIn recent work, Kim et al (2010) demonstrated theimportance of dependencies between post labels inweb forums.
The authors introduced series of fea-tures based on structural dependencies among posts.They used relative position, author information andautomatically predicted labels from previous post(s)as dependency features for assigning a semantic la-bel to the current target post.Similarly, by examining our chat corpus, we ob-served significant dependencies between utterances.First, 1-on-1 (i.e.
agent-to-user) dialogues often con-tain dependencies between adjacent utterances bydifferent authors.
For example, in Table 2, when theagent asks Is that correct?, the expected responsefrom the user is a Yes or No.
Another example isthat when the agent makes a greeting, such as Havea nice day, then the customer will typically respondwith a greeting or closing remark, and not a Yes orNo.
Second, the flow of dialogues is in general co-hesive, unless the topic of utterances changes dra-matically (e.g.
U5: Are you still there?, U22: brbin 1 min in Table 2).
Third, we observed that be-tween utterances made by the same author (eitheragent or user), the target utterance relies on previousutterances made by the same author, especially when865ID UtteranceA:U1 Hello Customer, welcome to MSN Shopping.A:U2 My name is Krishna and I am youronline Shopping assistant today.C:U3 Hello!C:U4 I?m trying to find a sports watch.C:U5 are you still there?A:U6 I understand that you are looking for sportswatch.A:U7 Is that correct?C:U8 yes, that is correct...C:U22 brb in 1 minC:U23 Thank you for waiting..A:U37 Thank you for allowing us to assistyou regarding wrist watch.A:U38 I hope you found our session today helpful.A:U39 If you have any additional questions oryou need additional information,please log in again to chat with us.We are available 24 hours a day, 7 days aweek for your help.A:U40 Thank you for contacting MSN Shopping.A:U41 Have a nice day!
Good Bye and Take Care.C:U42 You too.Table 2: An example of a 1-on-1 live chat, with turn andutterance structurethe agent and user repeatedly question and answer.With these observations, we checked the likelihoodof dialogue act pairings between two adjacent utter-ances, as well as between two adjacent utterancesmade by the same author.
Overall, we found strongco-occurrence (as measured by number of occur-rences of labels across adjacency pairs) between cer-tain pairs of dialogue acts (e.g.
(YESNO QUESTION?YES ANSWER/NO ANSWER) and (REQUEST?YES ANSWER)).
STATEMENT, on the otherhand, can associate with most other dialogue acts.Based on this, we designed the following five ut-terance dependency features; by combining these,we obtain 31 feature sets.1.
Dependency of utterances regardless of author(a) Dialogue act of previous utterance(b) Accumulated dialogue act(s) of previousutterances(c) Accumulated dialogue acts of previous ut-terances in a given turn2.
Dependency of utterances made by a single au-thor(a) Dialogue act of previous utteranceby same author; a dialogue act can be inthe same turn or in the previous turn(b) Accumulated dialogue acts of previousutterances by same author; dialogue actscan be in the same turn or in the previousturnTo capture utterance dependency, Bangalore et al(2006) previously used n-gram BoW features fromthe previous 1?3 utterances.
In contrast, instead ofusing utterances which indirectly encode dialogueacts, we directly use the dialogue act classifications,as done in Stolcke et al (2000).
The motivation isthat, due to the high performance of simple BoWfeatures, using dialogue acts directly would cap-ture the dependency better than indirect informationfrom utterances, despite introducing some noise.
Wedo not build a probabilistic model of dialogue tran-sitions the way Stolcke et al (2000) does, but followan approach similar to that used in Kim et al (2010)in using predicted dialogue act(s) labels learned inprevious step(s) as a feature.5 Experiment SetupAs stated earlier, we use the data set from Ivanovic(2008) for our experiments; it contains 1-on-1 livechats from an information delivery task.
This datasetcontains 8 live chats, including 542 manually-segmented utterances.
The maximum and minimumnumber of utterances in a dialogue are 84 and 42,respectively; the maximum number of utterances ina turn is 14.
The live chats were manually taggedwith the 12 dialogue acts described in Section 3.The utterance distribution over the dialogue acts isdescribed in Table 3.For our experiments, we calculated TF, TF?IDFand IG (Information Gain) over the utterances,which were optionally lemmatized with the morphtool (Minnen et al, 2000).
We then built a dialogueact classifier using three different machine learn-ers: SVM-HMM (Joachims, 1998),2 naive Bayes2http://www.cs.cornell.edu/People/tj/svm light/svm hmm.html866Dialogue Act Utterance numberCONVENTIONAL CLOSING 15CONVENTIONAL OPENING 12DOWNPLAYER 15EXPRESSIVE 5NO ANSWER 12OPEN QUESTION 17REQUEST 28RESPONSE ACK 27STATEMENT 198THANKS 79YES ANSWER 35YESNO QUESTION 99Table 3: Dialogue act distribution in the corpusIndex Learner Ours IvanovicFeature Acc.
Feature Acc.Word SVM 1+2+3/B .790 1/B .751NB 1/B .673 1/B .673CRF 1/IG .839 1/B .825Lemma SVM 1+2+3/IG .777 N/A N/ANB 1/B .672 N/A N/ACRF 1/B .862 N/A N/ATable 4: Best accuracy achieved by the different learn-ers over different feature sets and weighting methods (1= 1-gram; 1+2+3 = 1/2/3-grams; B = Boolean; IG = in-formation gain)from the WEKA machine learning toolkit (Wit-ten and Frank, 2005), and Conditional RandomFields (CRF) using CRF++.3 Note that we choseto test CRF and SVM-HMM as previous work (e.g.
(Samuel et al, 1998; Stolcke et al, 2000; Chung,2009)) has shown the effectiveness of structuredclassification models on sequential dependencies.Thus, we expect similar effects with CRF and SVM-HMM.
Finally, we ran 8-fold cross-validation usingthe feature sets described above (partitioning acrossthe 8 sessions).
All results are presented in termsof classification accuracy.
The accuracy of a zero-R(i.e.
majority vote) baseline is 0.36.6 Evaluation6.1 Testing Bag-of-Words FeaturesTable 4 shows the best accuracy achieved by the dif-ferent learners, in combination with BoW represen-3http://crfpp.sourceforge.net/n-gram Boolean TF TF?IDF IG1 .731 .511 .517 .7662 .603 .530 .601 .6143 .474 .463 .472 .4821+2 .756 .511 .522 .7771+2+3 .773 .511 .528 .777Table 5: Accuracy of different feature representations andweighting methods for SVM-HMMtations and feature weighting methods.
Note that theCRF learner ran using 1-grams only, as CRF++ doesnot accept large numbers of features.
As a bench-mark, we also tested the method in Ivanovic (2008)and present the best performance over words (ratherthan lemmas).
Overall, we found using just 1-gramsproduced the best performance for all learners, al-though SVM achieved the best performance whenusing all three n-gram orders (i.e.
1+2+3).
Since theutterances are very short, 2-grams or 3-grams aloneare too sparse to be effective.
Among the featureweighting methods, Boolean and IG achieved higheraccuracy than TF and TF?IDF.
Likewise, due to theshort utterances, simple Boolean values were oftenthe most effective.
However, as IG was computedusing the training data, it also achieved high perfor-mance.
When comparing the learners, we found thatCRF produced the best performance, due to its abil-ity to capture inter-utterance dependencies.
Finally,we confirmed that using lemmas results in higher ac-curacy.Table 5 shows the accuracy over all feature sets;for brevity, we show this for SVM only since thepattern is similar across all learners.6.2 Using Structural InformationIn this section, we describe experiments using struc-tural information?i.e.
author and/or position?withBoWs.
As with the base BoW technique, we used1-gram lemmas with Boolean values, based on theresults from Section 6.1.
Table 6 shows the results:Pos indicates the relative position of an utterance inthe whole dialogue, Author means author informa-tion, and Posturn indicates the relative position ofthe utterance in a turn.
All methods outperformedthe baseline; methods that surpassed the results forthe simple BoW method (for the given learner) at a867Feature LearnersCRF SVM NBBoW .862 .731 .672BoW+Author .860 .655 .649BoW+Pos .862 .721 .655BoW+Posabsolute .863 .631 .524BoW+Author+Pos .875 .700 .642BoW+Author+Posturn .871 .651 .631Table 6: Accuracy with structural informationlevel of statistical significance (based on randomisedestimation, p < 0.05) are boldfaced.Overall, using CRFs with Author and Position in-formation produced better performance than usingBoW alone.
Clearly, the ability of CRFs to nativelyoptimise over structural dependencies provides anadvantage over other learners.Relative position cannot of course be measureddirectly in an actual online application; hence Ta-ble 6 also includes the use of ?absolute position?
asa feature.
We see that, for CRF, the absolute posi-tion feature shows an insignificant drop in accuracyas compared to the use of relative position.
(How-ever, we do see a significant drop in performancewhen using this feature with SVM and NB.
)6.3 Using Utterance DependencyWe next combined the inter-utterance dependencyfeatures with the BoW features.
Since we use thedialogue acts directly in utterance dependency, wefirst experimented using gold-standard dialogue actlabels.
We also tested using the dialogue acts whichwere automatically learned in previous steps.Table 7 shows performance using both the gold-standard and learned dialogue acts.
The differ-ent features listed are as follows: LabelList/L in-dicates those corresponding to all utterances ina dialogue preceding the target utterance; Label-Prev/P indicates a dialogue act from a previousutterance; LabelAuthor/A indicates a dialogue actfrom a previous utterance by the same author;and LabelPrevt/LabelAuthort indicates the previ-ous utterance(s) and previously same-authored ut-terance(s) in a turn, respectively.
Since the accuracyfor SVM and NB using learned labels is similar tothat using gold standard labels, for brevity we reportFeatures Dialogue ActsGoldstandard LearnedCRF HMM NB CRFBoW .862 .731 .672 .862BoW+LabelList(L) .795 .435 .225 .803BoW+LabelPrev(P) .875 .661 .364 .876BoW+LabelAuthor(A) .865 .633 .559 .865BoW+LabelPrevt(Pt) .873 .603 .557 .873BoW+LabelAuthort(At) .862 .587 .535 .851BoW+L+P .804 .428 .227 .808BoW+L+A .799 .404 .225 .804BoW+L+Pt .803 .413 .229 .804BoW+L+At .808 .408 .216 .801BoW+P+A .873 .631 .517 .869BoW+P+Pt .878 .579 .539 .875BoW+P+At .871 .603 .519 .867BoW+A+Pt .847 .594 .519 .849BoW+A+At .869 .594 .530 .871BoW+Pt+At .871 .592 .519 .867BoW+L+P+A .812 .419 .231 .804BoW+L+P+Pt .816 .423 .229 .812BoW+L+P+At .808 .397 .225 .806BoW+L+A+Pt .810 .388 .225 .810BoW+L+A+At .812 .415 .216 .801BoW+L+Pt+At .810 .375 .205 .816BoW+P+A+Pt .875 .602 .522 .876BoW+P+A+At .862 .609 .511 .864BoW+P+Pt+At .873 .594 .515 .867BoW+A+Pt+At .865 .594 .517 .864BoW+L+P+A+Pt .817 .410 .231 .810BoW+L+P+A+At .814 .411 .223 .810BoW+L+P+Pt+At .816 .382 .205 .806BoW+L+A+Pt+At .812 .406 .203 .808BoW+P+A+Pt+At .865 .583 .513 .865BoW+L+P+A+Pt+At .816 .399 .205 .803Table 7: Accuracy for the different learners with depen-dency featuresthe performance for CRF using learned labels only.Results that exceed the BoW accuracy at a level ofstatistical significance (p < 0.05) are boldfaced.Utterance dependency features worked well incombination with CRF only.
Individually, Prev andPrevt (i.e.
BoW+P+Pt) helped to achieve higher ac-curacies, and the Author feature was also benefi-cial.
However, List decreased the performance, asthe flow of dialogues can change, and when a largerhistory of dialogue acts is included, it tends to in-troduce noise.
Comparing use of gold-standard andlearned dialogue acts, the reduction in accuracy wasnot statistically significant, indicating that we can868Feature CRF SVM NBC+LabelList .9557 .4613 .2565C+LabelPrev .9649 .6365 .5720C+LabelAuthor .9686 .6310 .5424C+LabelPrevt .9686 .5738 .5738C+LabelAuthort .9561 .6125 .5332Table 8: Accuracy with Structural and Dependency Infor-mation: C means lemmatized Unigram+Position+Authorachieve high performance on dialogue act classifi-cation even with interactively-learned dialogue acts.We believe this demonstrates the robustness of theproposed techniques.Finally, we tested the combination of featuresfrom structural and dependency information.
Thatis, we used a base feature (unigrams with Booleanvalue), relative position, author information, com-bined with each of the different dependency features?
LabelList, LabelPrev, LabelAuthor, LabelPrevtand LabelAuthort.Table 8 shows the performance when using thesecombinations, for each dependency feature.
As wewould expect, CRFs performed well with the com-bined features since CRFs can incorporate the struc-tural and dependency information; the achieved thehighest accuracy of 96.86%.6.4 Error Analysis and Future WorkFinally, we analyzed the errors ofthe best-performing feature set (i.e.BoW+Position+Author+LabelAuthor).
In Ta-ble 9, we present a confusion matrix of errors,for CONVENTIONAL CLOSING (Cl), CON-VENTIONAL OPENING (Op), DOWNPLAYER(Dp), EXPRESSIVE (Ex), NO ANSWER (No),OPEN QUESTION (Qu), REQUEST (Rq), RE-SPONSE ACK (Ack), STATEMENT (St), THANKS(Ta), YES ANSWER (Yes), and YESNO QUESTION(YN).
Rows indicate the correct dialogue acts andcolumns indicate misclassified dialogue acts.Looking over the data, STATEMENT is a commonsource of misclassification, as it is the majority classin the data.
In particularly, a large number of RE-QUEST and RESPONSE ACK utterances were taggedas STATEMENT.
We did not include punctuationsuch as question marks in our feature sets; includ-ing this would likely improve results further.In future work, we plan to investigate methods forautomatically cleansing the data to remove typos,and taking account of temporal gaps that can some-times arise in online chats (e.g.
in Table 2, there isa time gap between C:U22 brb in 1 min and C:U23Thank you for waiting).7 ConclusionWe have explored an automated approach for classi-fying dialogue acts in 1-on-1 live chats in the shop-ping domain, using bag-of-words (BoW), structuralinformation and utterance dependency features.
Wefound that the BoW features perform remarkablywell, with slight improvements when using lemmasrather than words.
Including structural and inter-utterance dependency information further improvedperformance.
Of the learners we experimented with,CRFs performed best, due to their ability to nativelycapture sequential dialogue act dependencies.AcknowledgementsThis research was supported in part by funding fromMicrosoft Research Asia.ReferencesJ.Allen and M.Core.
Draft of DAMSL: Dialog ActMarkup in Several Layers.
The Multiparty Dis-course Group.
University of Rochester, Rochester,USA.
1997.A.
Anderson, M. Bader, E. Bard, E. Boyle G.M.
Do-herty, S. Garrod, S. Isard, J. Kowtko, J. McAllister,J.
Miller, C. Sotillo, H.S.
Thompson, R. and Weinert.The HCRC Map Task Corpus.
Language and Speech.1991, 34, pp.
351?366.J.
Ang, Y. Liu and E. Shriberg.
Automatic Dialog ActSegmentation and Classification in Multiparty Meet-ings.
IEEE International Conference on Acoustics,Speech, and Signal Processing.
2005, pp, 1061?1064.S.
Bangalore, G. Di Fabbrizio and A. Stent.
Learningthe Structure of Task-Driven Human-Human Dialogs.Proceedings of the 21st COLING and 44th ACL.
2006,pp.
201?208.H.
H. Bui.
A general model for online probabilistic planrecognition.
IJCAI.
2003, pp.
1309?1318.G.Y Chung.
Sentence retrieval for abstracts of random-ized controlled trials.
BMC Medical Informatics andDecision Making.
2009, 9(10), pp.
1?13.F.
Debole and F. Sebastiani.
Supervised term weightingfor automated text categorization.
18th ACM Sympo-sium on Applied Computing.
2003, pp.
784?788.869Cl Op Dp Ex No Qu Rq Ack St Ta Yes YNOp 0 0 0 0 0 0 0 0 0 0 0 2Cl 0 0 0 0 0 0 0 0 1 1 0 0Dp 0 0 0 0 0 0 0 0 0 0 0 0Ex 0 0 0 0 0 0 0 0 0 0 0 0No 0 0 0 0 0 0 0 0 0 0 0 0Qu 0 0 0 0 0 0 0 0 0 0 0 0Rq 0 0 0 0 0 0 0 0 3 0 0 0Ack 0 0 1 0 1 0 0 0 5 0 0 0St 0 0 0 0 0 0 1 0 0 0 0 0Ta 1 0 0 0 0 0 0 0 0 0 0 0Yes 0 0 0 0 0 0 0 0 0 0 0 0YN 0 1 0 0 0 0 0 0 0 0 0 0Table 9: Confusion matrix for errors from the CRF with BoW+Position+Author+LabelAuthor (rows = correct clas-sification; columns = misclassification; CONVENTIONAL CLOSING = Cl; CONVENTIONAL OPENING = Op; DOWN-PLAYER = Dp; EXPRESSIVE = Ex; NO ANSWER = No; OPEN QUESTION = Qu; REQUEST = Rq; RESPONSE ACK= Ack; STATEMENT = St; THANKS = Ta; YES ANSWER = Yes; and YESNO QUESTION = YN)E. N. Forsyth.
Improving Automated Lexical and Dis-course Analysis of Online Chat Dialog.
Master?s the-sis.
Naval Postgraduate School, 2007.J.
Godfrey and E. Holliman and J. McDaniel.
SWITCH-BOARD: Telephone speech corpus for research anddevelopment.
Proceedings of IEEE InternationalConference on Acoustics, Speech, and Signal Process-ing.
1992, pp.
517?520.S.
Grau, E. Sanchis, M. Jose and D. Vilar.
Dialogue actclassification using a Bayesian approach.
Proceedingsof the 9th Conference on Speech and Computer.
2004.P.
A. Heeman and J. Allen.
The Trains 93 Dialogues.Trains Technical Note 94-2.
Computer Science Dept.,University of Rochester, March 1995.T.
Joachims.
Text categorization with support vector ma-chines: Learning with many relevant features.
Pro-ceedings of European Conference on Machine Learn-ing.
1998, pp.
137?142.M.
Johnston, S. Bangalore, G. Vasireddy, A. Stent,P.
Ehlen, M. Walker, S. Whittaker and P. Maloor.MATCH: An Architecture for Multimodal DialogueSystems.
Proceedings of 40th ACL.
2002, pp.
376?383.F.
N. Julia and K. M. Iftekharuddin.
Dialog Act clas-sification using acoustic and discourse information ofMapTask Data.
Proceedings of the International JointConference on Neural Networks.
2008, pp.
1472?1479.D.
Jurafsky, E. Shriberg, B Fox and T. Curl.
Lexical,Prosodic, and Syntactic Cues for Dialog Acts.
Pro-ceedings of ACL/COLING-98 Workshop on DiscourseRelations and Discourse Markers.
1998, pp.
114?120.E.
Ivanovic.
Automatic instant messaging dialogue us-ing statistical models and dialogue acts.
Master?s The-sis.
The University of Melbourne.
2008.S.
Keizer.
A Bayesian Approach to Dialogue Act Clas-sification.
5th Workshop on Formal Semantics andPragmatics of Dialogue.
2001, pp.
210?218.S.N.
Kim and L. Wang and T. Baldwin.
Tagging andLinking Web Forum Posts.
Fourteenth Conference onComputational Natural Language Learning.
2010.J.
Lafferty, A. McCallum and F. Pereira.
Conditionalrandom fields: Probabilistic models for segmentingand labeling sequence data.
Proceedings of ICML.2001, pp.
282?289.D.
J. Litman and S. Silliman.
ITSPOKE: An IntelligentTutoring Spoken Dialogue SYstem.
Proceedings ofthe HLT/NAACL.
2004.M.
M. Louwerse and S. Crossley.
Dialog Act Classifica-tion Using N -Gram Algorithms.
FLAIRS Conference,2006, pp.
758?763.G.
Minnen, J. Carroll and D. Pearce.
Applied morpho-logical processing of English Natural Language Engi-neering 2000, 7(3), pp.
77?80.M.
Purver, J. Niekrasz and S. Peters.
Ontology-BasedDiscourse Understanding for a Persistent Meeting As-sistant.
Proc.
CHI 2005 Workshop on The VirtualityContinuum Revisited.
2005.K.
Samuel, Sandra Carberry and K. Vijay-Shanker.
Dia-logue Act Tagging with Transformation-Based Learn-ing.
Proceedings of COLING/ACL 1998.
1998, pp.1150-1156.E.
Shriberg, R. Bates, P. Taylor, A. Stolcke, D. Jurafsky,K.
Ries, N. Coccaro, R. Martin, M. Meteer and C. Van870Ess-Dykema.
Can Prosody Aid the Automatic Clas-sification of Dialog Acts in Conversational Speech?.Language and Speech.
1998, 41(3-4), pp.
439?487.V.
R. Sridhar, S. Bangalore and S. Narayanan.
Combin-ing lexical, syntactic and prosodic cues for improvedonline dialog act tagging.
Computer Speech and Lan-guage.
2009, 23(4), pp.
407?422.A.
Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,D.
Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykemaand M. Meteer.
Dialogue Act Modeling for AutomaticTagging and Recognition of Conversational Speech.Computational Linguistics.
2000, 26(3), pp.
339?373.A.
Stolcke and E. Shriberg.
Markovian Combination ofLanguage and Prosodic Models for better Speech Un-derstanding and Recognition .
Invited talk at the IEEEWorkshop on Speech Recognition and Understanding,Madonna di Campiglio, Italy, December 2001 2001,C.
C. Werry.
Linguistic and interactional features of In-ternet Relay Chat.
In S. C. Herring (ed.).
Computer-Mediated Communication.
Benjamins, 1996.I.
Witten and E. Frank.
Data Mining: Practical MachineLearning Tools and Techniques.
Morgan Kaufmann,2005.T.
Wu, F. M. Khan, T. A. Fisher, L. A. Shuler and W. M.Pottenger.
Posting act tagging using transformation-based learning.
Proceedings of the Workshop on Foun-dations of Data Mining and Discovery, IEEE Interna-tional Conference on Data Mining.
2002.871
