Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 124?127,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPModeling Machine Transliteration as a Phrase Based Statistical MachineTranslation ProblemTaraka Rama, Karthik GaliLanguage Technologies Research Centre,IIIT, Hyderabad, India.
{taraka,karthikg}@students.iiit.ac.inAbstractIn this paper we use the popular phrase-based SMT techniques for the task ofmachine transliteration, for English-Hindilanguage pair.
Minimum error rate train-ing has been used to learn the modelweights.
We have achieved an accuracy of46.3% on the test set.
Our results showthese techniques can be successfully usedfor the task of machine transliteration.1 IntroductionTransliteration can be defined as the task of tran-scribing the words from a source script to a tar-get script (Surana and Singh, 2008).
Translitera-tion systems find wide applications in Cross Lin-gual Information Retrieval Systems (CLIR) andMachine Translation (MT) systems.
The systemsalso find use in sentence aligners and word align-ers (Aswani and Gaizauskas, 2005).
Transcribingthe words from one language to another languagewithout the use of a bilingual lexicon is a chal-lenging task as the output word produced in tar-get language should be such that it is acceptableto the readers of the target language.
The dif-ficulty arises due to the huge number of Out OfVocabulary (OOV) words which are continuouslyadded into the language.
These OOV words in-clude named entities, technical words, borrowedwords and loan words.In this paper we present a technique for translit-erating named entities from English to Hindi us-ing a small set of training and development data.The paper is organised as follows.
A survey of theprevious work is presented in the next subsection.Section 2 describes the problem modeling whichwe have adopted from (Rama et al, 2009) whichthey use for L2P task.
Section 3 describes howthe parameters are tuned for optimal performance.A brief description of the data sets is provided inSection 4.
Section 5 has the results which we haveobtained for the test data.
Finally we concludewith a summary of the methods and a analysis ofthe errors.1.1 Previous WorkSurana and Singh (2008) propose a transliterationsystem in which they use two different ways oftransliterating the named entities based on theirorigin.
A word is classified into two classes eitherIndian or foreign using character based n-grams.They report their results on Telugu and Hindidata sets.
Sherif and Kondrak (2007) propose ahybrid approach in which they use the Viterbi-based monotone search algorithm for searchingthe possible candidate transliterations.
Using theapproach given in (Ristad et al, 1998) the sub-string translations are learnt.
They integrate theword-based unigram model based on (Knight andGraehl, 1998; Al-Onaizan and Knight, 2002) withthe above model for improving the quality oftransliterations.Malik (2006) tries to solve a special case oftransliteration for Punjabi in which they con-vert from Shahmukhi (Arabic script) to Guru-mukhi using a set of transliteration rules.
AbdulJaleel (2003) show that, in the domain of informa-tion retrieval, the cross language retrieval perfor-mance was reduced by 50% when the name enti-ties were not transliterated.2 Problem ModelingAssume that given a word, represented as a se-quence of letters of the source language s = sJ1 =s1...sj ...sJ , needs to be transcribed as a sequenceof letters in the target language, represented as t= tI1 = t1...ti...tI .
The problem of finding the besttarget language letter sequence among the translit-erated candidates can be represented as:124tbest = argmaxt{Pr (t | s)} (1)We model the transliteration problem based onthe noisy channel model.
Reformulating the aboveequation using Bayes Rule:tbest = argmaxtp (s | t) p (s) (2)This formulation allows for a target languageletters?
n-gram model p (t) and a transcriptionmodel p (s | t).
Given a sequence of letters s, theargmax function is a search function to output thebest target letter sequence.From the above equation, the best target se-quence is obtained based on the product of theprobabilities of transcription model and the prob-abilities of a language model and their respectiveweights.
The method for obtaining the transcrip-tion probabilities is described briefly in the nextsection.
Determining the best weights is necessaryfor obtaining the right target language sequence.The estimation of the models?
weights can be donein the following manner.The posterior probability Pr (t | s) can also bedirectly modeled using a log-linear model.
Inthis model, we have a set of M feature func-tions hm(t, s),m = 1...M .
For each featurefunction there exists a weight or model parameter?m,m = 1...M .
Thus the posterior probabilitybecomes:Pr (t | s) = p?M1 (t | s) (3)=exp[?Mm=1?mhm(t, s)]?t?I1exp[?Mm=1?mhm(t?I1, s)] (4)with the denominator, a normalization factor thatcan be ignored in the maximization process.The above modeling entails finding the suit-able model parameters or weights which reflect theproperties of our task.
We adopt the criterion fol-lowed in (Och, 2003) for optimising the parame-ters of the model.
The details of the solution andproof for the convergence are given in Och (2003).The models?
weights, used for the transliterationtask, are obtained from this training.All the above tools are available as a part of pub-licly available MOSES (Koehn et al, 2007) toolkit.
Hence we used the tool kit for our experi-ments.3 Tuning the parametersThe source language to target language lettersare aligned using GIZA++ (Och and Ney, 2003).Every letter is treated as a single word for theGIZA++ input.
The alignments are then used tolearn the phrase transliteration probabilities whichare estimated using the scoring function givenin (Koehn et al, 2003).The parameters which have a major influenceon the performance of a phrase-based SMT modelare the alignment heuristics, the maximum phraselength (MPR) and the order of the languagemodel (Koehn et al, 2003).
In the context oftransliteration, phrase means a sequence of let-ters(of source and target language) mapped to eachother with some probability (i.e., the hypothesis)and stored in a phrase table.
The maximum phraselength corresponds to the maximum number of let-ters that a hypothesis can contain.
Higher phraselength corresponds a larger phrase table during de-coding.We have conducted experiments to see whichcombination gives the best output.
We initiallytrained the model with various parameters on thetraining data and tested for various values of theabove parameters.
We varied the maximum phraselength from 2 to 7.
The language model wastrained using SRILM toolkit (Stolcke, 2002).
Wevaried the order of language model from 2 to 8.We also traversed the alignment heuristics spec-trum, from the parsimonious intersect at one endof the spectrum through grow, grow-diag, grow-diag-final, grow-diag-final-and and srctotrg to themost lenient union at the other end.We observed that the best results were obtainedwhen the language model was trained on 7-gramand the alignment heuristic was grow-diag-final.No significant improvement was observed in theresults when the value of MPR was greater than 7.We have done post-processing and taken care suchthat the alignments are always monotonic and noletter was left unlinked.4 Data SetsWe have used the data sets provided by organis-ers of the NEWS 2009 Machine TransliterationShared Task (Kumaran and Kellner, 2007).
Priorto the release of the test data only the training dataand development data was available.
The trainingdata and development data consisted of a parallelcorpus having entries in both English and Hindi.125The training data and development data had 9975entries and 974 entries respectively.
We used thetraining data given as a part of the shared taskfor generating the phrase table and the languagemodel.
For tuning the parameters mentioned in theprevious section, we used the development data.From the training and development data wehave observed that the words can be roughly di-vided into following categories, Persian, European(primarily English), Indian, Arabic words, basedon their origin.
The test data consisted of 1000 en-tries.
We proceeded to experiment with the test setonce the set was released.5 Experiments and ResultsThe parameters described in Section 3 were theinitial settings of the system.
The system wastuned on the development set, as described inSection 2, for obtaining the appropriate modelweights.
The system tuned on the developmentdata was used to test it against the test data set.We have obtained the following model weights.The other features available in the translation sys-tem such as word penalty, phrase penalty donotaccount in the transliteration task and hence werenot included.language model = 0.099translation model = 0.122Prior to the release of the test data, we tested thesystem without tuning on development data.
Thedefault model weights were used to test our sys-tem on the development data.
In the next step themodel weights were obtained by tuning the sys-tem.
Although the system allows for a distortionmodel, allowing for phrase movements, we did notuse the distortion model as distortion is meaning-less in the domain of transliteration.
The followingmeasures such as Word Accuracy (ACC), Mean F-Score, Mean Reciprocal Rank (MRR), MAPref ,MAP10, MAPsys were used to evaluate our sys-tem performance.
A detailed description of eachmeasure is available in (Li et al, 2009).Measure ResultACC 0.463Mean F-Score 0.876MRR 0.573MAPref 0.454MAP10 0.201MAPsys 0.201Table 1: Evaluation of Various Measures on TestData6 ConclusionIn this paper we show that we can use the pop-ular phrase based SMT systems successfully forthe task of transliteration.
The publicly availabletool GIZA++ was used to align the letters.
Thenthe phrases were extracted and counted and storedin phrase tables.
The weights were estimated us-ing minimum error rate training as described ear-lier using development data.
Then beam-searchbased decoder was used to transliterate the Englishwords into Hindi.
After the release of the refer-ence corpora we examined the error results andobserved that majority of the errors resulted in thecase of the foreign origin words.
We provide someexamples of the foreign origin words which weretransliterated erroneously.Figure 1: Error Transliterations of Some ForeignOrigin WordsReferencesN.
AbdulJaleel and L.S.
Larkey.
2003.
Statisticaltransliteration for english-arabic cross language in-formation retrieval.Y.
Al-Onaizan and K. Knight.
2002.
Machine translit-eration of names in Arabic text.
In Proceedings ofthe ACL-02 workshop on Computational approachesto semitic languages, pages 1?13.
Association forComputational Linguistics Morristown, NJ, USA.N.
Aswani and R. Gaizauskas.
2005.
A hybrid ap-proach to align sentences and words in English-Hindi parallel corpora.
Building and Using Paral-lel Texts: Data-Driven Machine Translation and Be-yond, page 57.K.
Knight and J. Graehl.
1998.
Machine translitera-tion.
Computational Linguistics, 24(4):599?612.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proceedings of the 2003Conference of the NAACL:HLT-Volume 1, pages 48?54.
ACL Morristown, NJ, USA.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, et al 2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In ACL, volume 45, page 2.126A.
Kumaran and T. Kellner.
2007.
A generic frame-work for machine transliteration.
In Proceedingsof the 30th annual international ACM SIGIR con-ference on Research and development in informa-tion retrieval, pages 721?722.
ACM New York, NY,USA.H.
Li, A. Kumaran, M. Zhang, and V. Pervouch-ine.
2009.
Whitepaper of NEWS 2009 MachineTransliteration Shared Task.
In Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS2009).
ACL, Singapore, 2009.M.G.A.
Malik.
2006.
Punjabi machine transliteration.In Proceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, pages 1137?1144.
Association for Compu-tational Linguistics Morristown, NJ, USA.F.J.
Och and H. Ney.
2003.
A Systematic Comparisonof Various Statistical Alignment Models.
Computa-tional Linguistics, 29(1):19?51.F.J.
Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proceedings of the 41stAnnual Meeting on ACL-Volume 1, pages 160?167.ACL, Morristown, NJ, USA.T.
Rama, A.K.
Singh, and S. Kolachina.
2009.
Model-ing letter to phoneme conversion as a phrase basedstatistical machine translation problem with mini-mum error rate training.
In The NAACL Student Re-search Workshop, Boulder, Colorado.ES Ristad, PN Yianilos, M.T.
Inc, and NJ Princeton.1998.
Learning string-edit distance.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,20(5):522?532.T.
Sherif and G. Kondrak.
2007.
Substring-based transliteration.
In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LIN-GUISTICS, volume 45, page 944.A.
Stolcke.
2002.
Srilm ?
an extensible language mod-eling toolkit.H.
Surana and A.K.
Singh.
2008.
A more discern-ing and adaptable multilingual transliteration mech-anism for indian languages.
In Proceedings ofthe Third International Joint Conference on NaturalLanguage Processing.127
