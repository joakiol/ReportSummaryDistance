CommandTalk:  A Spoken-Language Interfacefor Battlefield SimulationsR.
C.  Moore ,  J .
Dowding ,  H .
Brat t ,  J .
M .
Gawron ,  Y .
Gor fu ,  and  A.  CheyerSRI In ternat iona l333 Ravenswood Ave.Menlo Park,  CA 94025{bmoore, dowding, harry, gawron, gorfu, cheyer}@ai.sri.comAbst ractCommandTalk is a spoken-language inter-face to battlefield simulations that allowsthe use of ordinary spoken English to cre-ate forces and control measures, assignmissions to forces, modify missions dur-ing execution, and control simulation sys-tem functions.
CommandTalk combines anumber of separate components integratedthrough the use of the Open Agent Ar-chitecture, including the Nuance speechrecognition system, the Gemini natural-language parsing and interpretation sys-tem, a contextual-interpretation m dhle, a"push-to-talk" agent, the ModSAF battle-field simulator, and "Start-It" (a graph-ical processing-spawning a ent).
Com-mandTalk is installed at a number of Gov-ernment and contractor sites, includingNRaD and the Marine Corps Air GroundCombat Center.
It is currently beingextended to provide exercise-time controlof all simulated U.S. forces in DARPA'sSTOW 97 demonstration.1 Overv iewCommandTalk is a spoken-language interface to syn-thetic forces in entity-based battlefield simulations.The principal goal of CommandTalk is to let com-manders interact with simulated forces by voice in amanner as similar as possible to the way they waythey would command actual forces.
CommandTalkcurrently interfaces to the ModSAF battlefield sim-ulator and allows the use of ordinary English com-mands to?
Create forces and control measures (points andlines)?
Assign missions to forces?
Modify missions during execution?
Control ModSAF system functions, such as themap displayAs an example, the following sequence of com-mands can be used to initialize a simple simulationin ModSAF and begin its execution:Create an M1 platoon designated Charlie 45.Put Checkpoint 1 at 937 965.Create a point called Checkpoint 2 at 930960.Objective Alpha is 92 96.Charlie 4 5, at my command, advance in acolumn to Checkpoint 1.Next, proceed to Checkpoint 2.Then assault Objective Alpha.Charlie 4 5, move out.With the simulation under way, the user can exer-cise direct control over the simulated forces by givingcommands such as the following for immediate xe-cution:Charlie 4 5, speed up.Change formation to echelon right.Get in a line.Withdraw to Checkpoint 2.Examples of voice commands for controlling Mod-SAF system functions include the following:Show contour lines.Center on M1 platoon.Zoom in closer.Pan west 500 meters.Center north of Checkpoint 2.CommandTalk was initially developed for Leath-erNet, a simulation and training system for the Ma-rine Corps developed under direction of the NavalCommand, Control and Ocean Surveillance Cen-ter, RDT&E Division (NRaD).
In addition to Com-mandTalk, LeatherNet includes?
MCSF, a version of ModSAF customized for theMarine Corps?
CommandVu, a synthetic, data-enhanced nvi-ronment with 3-D representation f MCSF be-haviors and display of commander decision aids?
Terrain Evaluation Module (TEM), a system forline-of-sight and weapons coverage analysisLeatherNet is intended to be used both as a train-ing system for the Marine Corps and as the MarineCorps component of DARPA's Synthetic Theater ofWar (STOW) program.
LeatherNet is currently in-stalled at the Marine Corps Air Ground CombatCenter (MCAGCC), at Twentynine Palms, Califor-nia.A single CommandTalk interacts directly withonly one ModSAF process.
ModSAF, however, cre-ates distributed simulations that can include mul-tiple graphical user interface (GUI) processes andmultiple simulator processes, plus other applica-tions such as CommandVu, communicating over anetwork through Distributed Interactive Simulation(DIS) and Persistent Object (PO) protocols.
Thisarchitecture l ts CommandTalk interact indirectlywith all these components.
Thus, a user can controla simulation using CommandTalk while viewing itin 3-D via CommandVu, without having to be awareof the ModSAF processes that mediate between thespoken commands and their results as seen in the3-D display.2 Arch i tec tureCommandTalk combines a number of separate com-ponents, developed independently, some of which areimplemented in C and others in Prolog.
These com-ponents are integrated through the use of the OpenAgent Architecture (OAA) (Cohen et al, 1994).OAA makes use of a facilitator agent hat plans andcoordinates interactions among agents during dis-tributed computation.
Other processes are encapsu-lated as agents that register with the facilitator thetypes of messages they can respond to.
An agentposts a message in an Interagent CommunicationLanguage (ICL) to the facilitator, which dispatchesthe message to the agents that have registered theirability to handle messages of that type.
This medi-ated communication makes it possible to "hot-swap"2or restart individual agents without restarting thewhole system.
The ICL communications mechanismis built on top of TCP/IP, so an OAA-based systemcan be distributed across both local- and wide-areanetworks based on Internet echnology.
OAA alsoprovides an agent library to simplify turning inde-pendent components into agents.
The agent librarysupplies common functionality to agents in multiplelanguages for multiple platforms, managing networkcommunication, ICL parsing, trigger and monitorhandling, and distributed message primitives.CommandTalk is implemented as a set of agentscommunicating as described above.
The principalagents used in CommandTalk are?
Speech recognition?
Natural anguage?
Contextual interpretation?
Push to talk?
ModSAF?
Start-It2.1 Speech Recognit ionThe speech recognition (SR) agent consists of a thinagent layer on top of the Nuance (formerly Corona)speech recognition system.
Nuance is a commercialspeech recognition product based on technology de-veloped by SRI International.
The recognizer listenson the audio port of the computer on which it is run-ning, and produces its best hypothesis as to whatstring of words was spoken.
The SR agent acceptsmessages that tell it to start and stop listening andto change grammars, and generates messages that ithas stopped listening and messages containing thehypothesized word string.The Nuance recognizer is customized in two waysfor use in CommandTalk.
First, we have replacedthe narrow-band (8-bit, 8-kHz sampled) acousticmodels included with the Nuance recognizer and de-signed for telephone applications, with wide-band(16-bit, 16-kHz sampled) acoustic models that takeadvantage of the higher-quality audio available oncomputer workstations.
Second, any practical ap-plication of speech recognition technology requires avocabulary and grammar tailored to the particularapplication, since for high accuracy the recognizermust be restricted as to what sequences of words itwill consider.
To produce the recognition vocabularyand grammar for CommandTalk, we have imple-mented an algorithm that extracts these from the vo-cabulary and grammar specifications for the natural-language component of CommandTalk.
This easesdevelopment by automatically keeping the languagethat can be recognized and the language that can beparsed in sync; that is, it guarantees that every wordstring that can be parsed by the natural-languagecomponent is a potential recognition hypothesis, andvice versa.
This module that generates the recog-nition grammar for CommandTalk is described inSection 3.2.2 Natura l  LanguageThe natural-language (NL) agent consists of a thinagent layer on top of Gemini (Dowding et al, 1993,1994), a natural-language parsing and semantic in-terpretation system based on unification grammar.
"Unification grammar" means that grammatical cat-egories incorporate features that can be assignedvalues; so that when grammatical category expres-sions are matched in the course of parsing or se-mantic interpretation, the information contained inthe features is combined, and if the feature valuesare incompatible the match fails.
Gemini appliesa set of syntactic and semantic grammar ules toa word string using a bottom-up parser to gener-ate a logical form, a structured representation f thecontext-independent meaning of the string.
The NLagent accepts messages containing word strings tobe parsed and interpreted, and generates messagescontaining logical forms or, if no meaning represen-tation can be found, error messages to be displayedto the user.Gemini is a research system that has been devel-oped over several years, and includes an extensivegrammar of general English.
For CommandTalk,however, we have developed an application-specificgrammar, which gives us a number of advantages.First, because it does not include rules for En-glish expressions not relevant o the application, thegrammar uns faster and finds few grammatical m-biguities.
Second, because the semantic rules aretailored to the application, the logical forms theygenerate require less subsequent processing to pro-duce commands to the application system.
Finally,by restricting the form of the CommandTalk gram-mar, we are able to automatically extract he gram-mar that guides the speech recognizer.The Nuance recognizer, like all other practical rec-ognizers, requires a grammar that defines a finite-state language model.
The Gemini grammar formal-ism, on the other hand, is able to define grammars ofmuch greater computational complexity.
For Com-mandTalk, extraction of the recognition grammar ismade possible by restricting the Gemini syntacticrules to a finite-state backbone with finitely valuedfeatures.
It should be noted that, although we are3not using the full power of the Gemini grammar for-malism, we still gain considerable benefit from Gem-ini because the feature constraints let us write thegrammar much more compactly, Gemini's morphol-ogy component simplifies maintaining the vocabu-lary, and Gemini's unification-based semantic ruleslet us specify the translation from word strings intological forms easily and systematically.2.3 Contextua l  In terpretat ionThe contextual-interpretation (CI) agent accepts alogical form from the NL agent, and produces oneor more commands to ModSAF.
Since a logical formencodes only information that is directly expressedin the utterance, the CI agent often must applycontextual information to produce a complete inter-pretation.
Sources of this information can includelinguistic context, situational context, and defaults.Since ModSAF itself is the source of situational in-formation about the simulation, the interaction be-tween the CI agent and ModSAF is not a simpleone-direction pipeline.
Often, there will be a seriesof queries to ModSAF about the current state of thesimulation before the ModSAF command or com-mands that represent the final interpretation of anutterance are produced.Some of the problems which must be solved by theCI agent are?
Noun phrase resolution?
Predicate resolution?
Temporal resolution?
Vagueness resolution2.3.1 Noun Phrase  Reso lu t ionA noun phrase denoting an object in the simula-tion must be resolved to the unique ModSAF identi-fier for that object.
"M1 platoon," "tank platoon,"or "Charlie 4 5" could all refer to the same entity inthe simulation.
To keep the CI informed about theobjects in the simulation and their properties, theModSAF agent notifies the CI agent whenever anobject is created, modified, or destroyed.
Since theCI agent is immediately notified whenever the usercreates an object through the ModSAF GUI, theCI can note the salience of such objects, and makethem available for pronominal reference (just as ob-jects created by speech are), leading to smootherinteroperation between speech and the GUI.2.3.2 P red icate  reso lu t ionWhile users employ generic verbs like move, at-tack, and assault to give verbal commands, the cor-responding ModSAF tasks often differ depending onthe units involved.
The ModSAF movement askfor a tank platoon is different from the one for aninfantry platoon or the one for a tank company.
Sim-ilarly, the parameter value indicating a column for-mation for tanks is different from the one indicatinga column formation for infantry, and the parame-ter that controls the speed of vehicles has a differ-ent name than the one that controls the speed ofinfantry.
All these differences need to be taken intoaccount when generating the ModSAF command forsomething like "Advance in a column to Checkpoint1 at 10 kph," depending on what type of unit is be-ing given the command.2.3.3 Tempora l  resolutionThe CI agent needs to determine when a com-mand is given to a unit should be carried out.
Thecommand may be part of a mission to be carriedout later, or it may be an order to be carried outimmediately.
If the latter, it may be a permanentchange to the current mission, or merely a tempo-rary interruption of the current task in the mission,which should be resumed when the interrupting taskis completed.
The CI agent decides these questionsbased on a combination of phrasing and context.Sometimes, explicit indicators may be given as towhen the command is to be carried out, such as aspecific time, or after a given duration of time haselapsed, or on the commander's order.2.3.4 Vagueness resolutionSometimes a verbal command oes not include allthe information required by the simulation.
The CIagent attempts to fill in this missing information byusing a combination of linguistic and situational con-text, plus defaults.
For instance, if no unit is explic-itly addressed by a command, it is assumed thatthe addressee is the unit to whom the last verbalcommand was given.
The ModSAF "occupy posi-tion" and "attack by fire" tasks require that a linebe given as a battle position, but users often givejust a point location for the position of the unit.
Insuch cases, the CI agent calls ModSAF to constructa line through the point, and uses that line for thebattle position.2.4 Push  to Ta lkThe push-to-talk (PTT) agent manages the interac-tions with the user.
It provides a long narrow win-dow running across the top of the screen--the onlyvisible indication that a ModSAF is CommandTalk-enabled.
This window contains a microphone iconthat indicates the state of CommandTalk (ready, lis-tening, or busy), an area for the most recent rec-ognized string to be printed, and an area for text4messages from the system to appear (confirmationmessages and error messages).This agent provides two mechanisms for the userto initiate a spoken command.
A push-to-talk but-ton attached to the serial port of the computer canbe pushed down to signal the computer to start lis-tening and released to indicate that the utteranceis finished (push-and-hold-to-talk).
The second op-tion is to click on the microphone icon with the leftmouse button to signal the computer to start listen-ing (click-to-talk).
With click-to-talk, the systemlistens for speech until a sufficiently long pause isdetected.
The length of time to wait is a parameterthat can be set in the recognizer.
The push-and-hold method generally seems more satisfactory for anumber of reasons: Push-and-hold leads to faster re-sponse because the system does not have to wait tohear whether the user is done speaking, click-to-talktends to cut off users who pause in the middle of anutterance to figure out what to say next, and push-and-hold seems natural to military users because itworks like a tactical field radio.The PTT  agent issues messages to the SR agentto start and stop listening.
It accepts messages fromthe SR agent containing the words that were recog-nized, messages that the user has stopped speaking(for click-to-talk), and messages, from any agent,that contain confirmation or error messages to bedisplayed to the user.2.5 ModSAFThe ModSAF agent consists of a thin layer on topof ModSAF.
It sends messages that keep the CIagent informed of the current state of the simulationand executes commands that it receives from the CIagent.
Generally, these commands access functionsthat are also available using the GUI, but not always.For example, it is possible with CommandTalk to tellModSAF to center its map display on a point that isnot currently visible.
This cannot be done with theGUI, because there is no way to select a point that isnot currently displayed on the map.
The set of mes-sages that the ModSAF agent responds to is definedby the ModSAF Agent Layer Language (MALL).2.6 Start-ItStart-It is a graphical processing-spawning a entthat helps control the large number of processesthat make up the CommandTalk system.
It pro-vides a mouse-and-menu interface to configure andstart other processes.
While it is particularly use-ful for starting agent processes, it can also be usedto start nonagent processes such as additional Mod-SAF simulators and interfaces, CommandVu, andthe LeatherNet sound server.include the following:Features of Start-It?
It makes it easy to assign processes to machinesdistributed over a network.?
It reports process tatus (not running, initializ-ing, running, or dead).?
It makes it easy to set command line argumentsand maintain consistent command line argu-ments across processes.?
The Start-It configuration is data-driven, so itis easy to add processes and command line ar-guments, or change default values.?
An automatic restart feature keeps agents run-ning in case of machine failure or process death.3 Gemini - to-Nuance GrammarCompilerThe SR agent requires a grammar to tell the rec-ognizer what sequences of words are possible in aparticular application, and the NL agent requires agrammar to specify the translation of word stringsinto logical forms.
For optimal performance, thesetwo grammars hould, as nearly as possible, ac-cept exactly the same word sequences.
In gen-eral, we would like the recognizer to accept all wordsequences that can be interpreted, and any over-generation by the recognition grammar increases thelikelihood of recognition errors without providingany additional functionality.
In order to keep thesetwo grammars ynchronized, we have implementeda compiler that derives the recognition grammar au-tomatically from the NL grammar.To derive a recognition grammar with coverageequivalent to the NL grammar, we must restrict theform of the NL grammar.
Like virtually all practicalspeech recognizers, the Nuance recognizer requires afinite-state grammar, while the Gemini parser ac-cepts grammars that have a context-free backbone,plus unification-based feature constraints that giveGemini grammars the power of an arbitrary Turingmachine.
To make it possible to derive an equiv-alent finite-state grammar, we restrict the Geminigrammars used as input to our Gemini-to-Nuancecompiler as follows:?
All features in the Gemini grammar that arecompiled into the recognition grammar must al-low only a finite number of values.
This meansthat no feature values are structures that cangrow arbitrarily large.5?
The Gemini grammar must not contain any in-direct recursion.
That  is, no rule subsets are al-lowed with patterns uch as A --+ BC,  C --+ AD.?
Immediately recursive rules are allowed, butonly if the recursive category is leftmost orrightmost in the list of daughters, so that thereis no form of center embedding.
That is, A --+AB and A -~ CA are allowed (even simultane-ously), but not A --+ CAB.There are many possible formats for specifying afinite-state grammar, and the one used by the Nu-ance recognition system specifies a single definitionfor each atomic nonterminal symbol as a regular ex-pression over vocabulary words and other nontermi-nals, such that there is no direct or indirect recursionin the set of definitions.
To transform a restrictedGemini grammar into this format, we first trans-form the Gemini rules over categories with featureconstraints into rules over atomic symbols, and wethen transform these rules into a set of definitions interms of regular expressions.3.1 Generating Atomic CategoriesGiven the restriction that all features must allowonly a finite number of values, it would be trivial totransform all unification rules into rules over atomiccategories by generating all possible full feature in-stantiations of every rule, and making up an atomicname for each combination of category and featurevalues that occur in these fully-instantiated rules.This would, however, increase the total number ofrules to a size that would be too large to deal with.We therefore instantiate the rules in a more carefulway that avoids unnecessarily instantiating featuresand prunes out useless rules.The set of atomic categories i defined by consid-ering, for each daughter category of each rule, allinstantiations of just the subset of features on thedaughter that are constrained by the rule.
Thus,if there is a rule that does not constrain a featureon a particular daughter category, an atomic cate-gory will be created for that daughter that is under-specified for the value of that feature.
A prime ex-ample of this in the CommandTalk grammar is therulecoordinate_hums : \[\] -+digit:f\] digit:f\] digit:f\] digit:f\]which says that a set of coordinate numbers can be asequence of four digits.
In the CommandTalk gram-mar the digit category has features (singular vs. plu-ral, zero vs. nonzero, etc.)
that would generate atleast 60 combinations if all instantiations were con-sidered.
So, if we naively generated all possible com-plete instantiations of this rule, we would get at least604 rules.
Even worse, we need other rules to per-mit up to eight digits to form a set of coordinatenumbers, which would give rise to 60 s rules.
Sincethe original rule, however, puts no constraints onany of the features of the digit category, by gener-ating an atomic category that is under-specified forall features, we only need a single rule in the derivedgrammar.From the set of atomic categories defined in thisway, we generate all rules consistent with the origi-nal Gemini rules, except hat for daughters that haveunconstrained features, we use only the correspond-ing under-specified categories.
We then iterativelyremove all rules that cannot participate in a com-plete parse of an utterance, either because they con-tain daughter categories that cannot be expandedinto any sequence of words, given the particular lex-icon we have, or because they have a mother cate-gory that cannot be reached from the top categoryof the grammar.3.2 Compiling Rules to RegularExpressionsOnce we have transformed the Gemini unificationgrammar into an equivalent grammar over atomicnonterminals, we then rewrite the grammar as a setof definitions of the nonterminals as regular expres-sions.
For the nonterminals that have no recursiverules, we simply collect all the rules with the sameleft-hand side and create a single rule by forming thedisjunction of all the right-hand sides.
For example,if the only rules for the nonterminal A areA -* BCA-*DEthen the regular expression defining A would be\[(BC)(DE)\].
In the Nuance regular expression o-tation, "( )" indicates a sequence and "\[ \]" indicatesa set of disjunctive alternatives.For nonterminals with recursive rules, we elimi-nate the recursion by introducing regular expressionsusing the Kleene star operator.
For each recursivenonterminal A, we divide the rules defining A intoright-recursive, left-recursive, and nonrecursive sub-sets.
For the right-recursive subset, we form the dis-junction of the expressions that occur to the left ofA.
That is, for the rulesA -* BAA-*CAwe generate \[BC\].
Call this expression LEFT-A.
Forthe left-recursive subset, we form the disjunction ofthe expressions that occur to the right of A, whichwe may call RIGHT-A.
Finally, we form the disjunc-tion of all the right-hand sides of the nonrecursiverules, which we may call NON-REC-A.
The com-plete regular expression defining A is then(*LEFT-A NON-REC-A *RIGHT-A)In the Nuance regular expression notation, theKleene star operator "*" precedes the iterated ex-pression, rather than following it as in most nota-tions for regular expressions.
Thus, .X  means thata sequence of zero or more instances of Z may occur.As an example, suppose the rules defining the non-terminal A areA -* BAA -* CDAA-*  EA -* FGA -* AHThe corresponding regular expression defining Awould be(*\[B(CD)\] \[E(FG)\] *H)This completes the transformation of a Geminigrammar with finitely-valued categories and a finite-state backbone into a Nuance regular expressiongrammar.
However, as one final optimization, welook for special cases where we can use the "Kleeneplus" operator, which indicates one or more in-stances of an expression in sequence, and which ishandled more efficiently by the Nuance recognizerthan equivalent expressions using Kleene star.
Wesimply look for sequences" of the form (*X X) or(X *X), and replace them with +X.4 Deve lopment  H is toryWork on CommandTalk began with SRI's initialreceipt of MCSF on February 16, 1995.
Thefirst demonstration of spoken commands to simu-lated forces in MCSF was given three weeks lateron March 7; an initial version the CommandTalkprototype was installed at the Marine Corps AirGround Combat Center (MCAGCC) on May 1; anda demonstration f CommandTalk was given to Gen-eral Palm, the Commanding Officer of MCAGCC,on May 16.Enhanced versions of the system were demon-strated at DARPA's Software and Intelligent Sys-tems Symposium in August 1995, and evaluated inthe STOW ED-1 milestone test in October 1995.
Inthe evaluation of ED-1 performance, CommandTalk6was given the highest grade of any Marine Corps por-tion of the exercise.
In addition to these milestones,CommandTalk has been included in demonstrationsof LeatherNet to numerous VIPs including GeneralC.
C. Krulak, Commandant of the Marine Corps;General J. H. Binford Peay, Commander in Chief USCentral Command; Secretary of the Navy J. H. Dal-ton; and Secretary of Defense William Perry.CommandTalk is currently being extended to pro-vide exercise-time control of all simulated U.S. forcesin DARPA's STOW 97 Advanced Concept Technol-ogy Demonstration.5 AvailabilityCommandTalk executables for Sun SPARC/SunOSand SGI MIPS/IRIX platforms are available atno cost to US Government users under RestrictedRights.
Contractors may obtain CommandTalk inexecutable form exclusively for use on US Gov-ernment projects under license from SRI.
Dis-tribution of CommandTalk for Government pur-poses is handled by NRaD (POC: Brenda Gill-crist, bwgill@nosc.mil).
Other inquiries about Com-mandTalk, including licensing, should be directed toSRI (POC: Robert Moore, bmoore@ai.sri.com).6 AcknowledgementsThis work was supported by the Defense AdvancedResearch Projects Agency under Contract N66001-94-C-6046 with the Naval Command, Control, andOcean Surveillance Center.
Approved for Public Re-lease - Distribution UnlimitedReferencesCohen, P. R., A. J. Cheyer, M. Wang, and S. C. Baeg(1994) "An Open Agent Architecture," in Work-ing Notes, AAAI Spring Symposium Series, Soft-ware Agents, Stanford, California, pp.
1-8.Dowding, J., J. M. Gawron, D. Appelt, J. Bear,L.
Cherny, R. Moore, and D. Moran (1993) "Gem-ini: A Natural Language System for Spoken-Language Understanding," in Proceedings 31stAnnual Meeting of the Association for Computa-tional Linguistics, Columbus, Ohio, pp.
54-61.Dowding, J., R. Moore, F. Andry, and D. Moran(1994) "Interleaving Syntax and Semantics in anEfficient Bottom-Up Parser," in Proceedings 32ndAnnual Meeting of the Association for Compu-tational Linguistics, Las Cruces, New Mexico,pp.
110-116.
