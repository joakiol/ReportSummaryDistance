USING A SEMANTIC CONCORDANCE FOR SENSE IDENTIFICATIONGeorge A. Miller, Martin Chodorow*, Shari Landes, Claudia Leacock, and Robert G. ThomasCognit ive Science LaboratoryPr inceton Univers i tyPr inceton, NJ 08542ABSTRACTThis paper proposes benchmarks for systems of automatic senseidentification.
A textual corpus in which open-class words hadbeen tagged both syntactically and semantically was used toexplore three statistical strategies for sense identification: a guess-ing heuristic, a most-frequent heuristic, and a co-occurrence h uris-tic.
When no information about sense-frequencies was available,the guessing heuristic using the numbers of alternative senses inWordNet was correct 45% of the time.
When statistics for sense-frequancies were derived from a semantic concordance, theassumption that each word is used in its most frequently occurringsense was correct 69% of the time; when that figure was calculatedfor polysemous words alone, it dropped to 58%.
And when a co-occur~nce heuristic took advantage of prior occurrences of wordstogether in the same sentences, little improvement was observed.The semantic concordance is still too small to estimate the potentiallimits of a co-occurrence h uristic.1.
INTRODUCTIONIt is generally recognized that systems for automatic seineidentification should be evaluated against a null hypothesis.
Gale,Church, and Yarowsky \[1\] suggest hat the appropriate basis forcomparison would be a system that assumes that each word is beingused in its most frequently occurring sere.
They review the litera-ture on how well word-disambiguation programs perform; as alower bound, they estimate that the most frequent sense ofpolysemous words would be correct 75% of the time, and they pro-pose that any sense-identification system that does not give thecorrect sense of polysemous words more than 75% of the timewould not be worth serious consideration.The value of setting such a lower bound is obvious.
However,Gale" Church, and Yarowsky \[I\] do not make clear how they deter-mined what the most frequently occurring senses are.
In theabsence of such information, a case can be made that the lowerbound should be given by the proportion of monosemous words inthe textual corpus.Although most words in a dictionary have only a single sense" it isthe polysemons words that occur most frequently in speech andwriting.
This is true even when we ignore the small set of highlypelysemous closed-class words (pronouns, prepositions, auxiliaryverbs, etc.)
that play such an important structural role.
For exam-pie, 82.3% of the opon-class words in WordNet \[2\] aremonosemous, but only 27.2% of the open-class words in a sampleof 103 passages from the Brown Corpus \[3\] were monosemous.
* Hunter College and Graduate School of the City Univendty ofNew Ytz~kThat is to say, 27% of the time no decision would be needed, butfor the remaining 73% of the open-class words, the response wouldhave to be "don't know."
This is probably the lowest lower boundanyone would propose, although if the highly pelysemous, very fre-quently used closed-class words were included, it would be evenlower.A better performance figure would result, of course, if, instead ofresponding "don't know," the system were to guess.
What is thepercentage correct hat you could expect to obtain by guessing72.
THE GUESSING HEURIST ICA guessing strategy presumes the existence of a standard list ofwords and their senses, but it does not assume any knowledge ofthe relative frequencies of different senses of polysemous words.We adopted the lexical database WordNet \[2\] as a convenient on-line list of open-class words and their senses.
Whenever a word isancountered that has more than one sense in WordNet, a systemwith no other information could do no better than to select a senseat random.The guessing heuristic that we evaluated was defined as follows: onencountering a noun (other than a proper noun), verb, adjective, oradverb in the test material, ook it up in WordNet.
If the word ismonosemous (has a single sense in WordNet), assign that sense toit.
If the word is polysemous (has more than one sense in Word-Net), choose asense at random with a probability of l/n, where n isthe number of different senses of that word.This guess.ing heuristic was then used with the sample of 103 pas-sages from the Brown Corpus.
Given the distribution ofopen-classwords in those passages and the number of senses of each word inWordNet, estimating the probability of a correct senseidentification is a straightforward calculation.
The result was that45.0% of the 101,284 guesses would be correct.
When the percentcorrect was calculated for just the 76,067 polysemous word tokens,it was 26.8%.3.
THE MOST-FREQUENT HEURIST ICData on sense frequencies do exist.
During the 1930s, Lorge \[4\]hired students at Columbia University to count how often each ofthe senses in the Oxford English Dictionary occurred in some4,500,000 running words of prose taken from magazines of the day.These and other word counts were used by Thomdike in writing theThorndike-Barnhart Junior Dictionary \[5\], adictionary for childrenthat first appeared in 1935 and that was widely used in the publicschools for many years.
Not only was Thorndike able to limit hisdictionary, to words in common use, but he was also able to listsenses in the order of their frequency, thus insuring that the senses240he included would be the ones that children were most likely toencounter in their eading.
The Lorge-Thomdike data, however, donot seem to he available today in a computer-readable form.More recently, the editors of Collins COBUILD Dictionary of theEnglish Language \[6\] made use of the 20,000,000-word COBUILDcorpus of written English to insure that the most commonly usedwords were included.
Entries in this dictionary are organized insuch a way that, whenever possible, the first sense of a polysemousword is both common and central to the meaning of the word.Again, however, sense-frequencies do not seem to be generallyavailable in a computer-readable form.At the ARPA Human Language Technology Workshop in March1993, Miller, Leacock, Tengi, and Bunker \[7\] described a semanticconcordance that combines passages from the Brown Corpus \[3\]with the WordNet lexical d~t_~base \[2\] in such a way that everyopen-class word in the text (every noun, verb, adjective, or adverb)carries both a syntactic tag and a semantic tag pointing to theappropriate sense of that word in WordNet.
The version of thissemantic oncordance that existed in August 1993, incorporating103 of the 500 passages in the Brown Corpus, was made publiclyavailable, along with version 1.4 of WordNet o which the passageswere tagged.
1 Passages in the Brown Corpus are approximately2,000 words long, and average approximately 1,000 open-classwords each.
Although this sample is much smaller than one wouldlike, this semantic concordance does provide a basis for estimatingsense frequencies for open-class words broken down by part ofspeech (word/pus).
For example, there are seven senses of theword "board" as a noun Cooard/nl, board/n2' .
.
.
.
board/h7), andfour senses as a verb (boardNl, hoard/v2" .
.
.
.
board/v4); the fre-quencies of all eleven senses in the semantic oncordance an hetabulated separately todetermine the most frequent board/n and themost frequent hoard/v.The fact that the words that occur most frequently in standardEnglish tend to be the words that are most polysemous creates abad news, good news situation.
The bad news is that most of thecontent words in textual corpora require disambiguation.
The goodnews is that polysemous words occur frequently enough that statist-ical estimates are possible on the basis of relatively small samples.It is possible., therefore, to pose the question: on the basis of theavailable sample, how often would the most frequent sense becorrect?
A larger semantic concordance would undoubtedly yield amore precise lower bound, but at least an approximate estimate canbe obtained.The most-frequent heuristic was defined as follows: on encounter-ing a noun, verb, adjective, or adverb in the test material, ook it upin WordNet.
If the word is monosamous, assign that sense to it.
Ifthe syntactically tagged word (word/pos) has more than one sensein WordNet, consult the semantic concordance to determine whichsense occurred most often in that corpus and assign that sense to it;if there is a fie, select one of the equally frequent senses at random.If the word is polysemous but does not occur in the semantic con-cordance, choose a sense at random with a probability of 1/rh wheren is the number of different senses of that word in WordNet.In short, when there are dam indicating the most frequent sense of apolyseanous word, use it; otherwise, guess.i via anonymous ftpfn:an chrity.pdnccton.odu.3.1 A Pre l iminary  Exper imentIn order to obtain a preliminary estimate of the accuracy of themost-frequent heuristic, a new passage from the Brown Corpus(passage P7, an excerpt from a novel that was classified by Francisand Ku~era \[3\] as "Imaginative Prose: Romance and Love Story")was semantically tagged to use as the test material.
The ~ainingmaterial was the 103 other passages from the Brown Corpus (notincluding P7) that made up the semantic concordance.
The seman-tic tags assigned by a human reader were then compared, one wordat a time, with the sense assigned by the most-frequent heuristic.For this particular passage, only 62.5% of the open-class wordswere correctly tagged by the most-frequent heuristic.
This estimateis generous, however, since 24% of the open-class words weremonosemous.
When the average is taken solely over polysemouswords, the most frequent sense was right only 50.8% of the time.These results were lower than expected, so we asked whether pas-sage P7 might be unusual in some way.
For example, the sentenceswere relatively short and there were fewer monosemous words thanin an average passage in the training material.
However, an inspec-tion of these data did not reveal any trend as a function of sentencelength; short sentences were no harder than long ones.
And thelower frequency of monosemous words is consistent with the non-technical nature of the passage; there is no obvious reason why thatshould influence the results for polysemous words.
Without com-parable data for other passages, there is no way to know whetherthese results for F7 are representative or not.3.2 A Larger  SampleRather than tag other new passages to use as test material, wedecided to use passages that were already tagged semantically.That is to say, any tagged passage in the semantic concordance anbe made to serve as a test passage by simply eliminating it from thetraining material.
For example, in order to use passage X as a testpassage, we can delete it from the semantic oncordance; then,using this diminished training material, the most-frequent heuristicis evaluated for passage X.
Next, X is restored, Y is deleted, andthe procedure repeats.
Since there are 103 tagged passages in thesemantic concordance, this produces 103 data points in addition tothe one we already have for PT.Using this procedure, the average number of correct senseidentifications produced by the most-frequent heuristic is 66.9%(standard eviation, o = 3.7%) when all of the open-class words,both monosemous and polysemous, are included.
Whan onlypolysemous words are considered, the average drops to 56.4% (o =4.3%).
This larger sample shows that the results obtained from thepreliminary experiment with passage P7 were indeed low, morethan a standard eviation below the mean.The scores obtained when the most-frequent heuristic is applied tothese 2,000-word passages appear to be normally distributed.Cumulative distributions of the scores for all 104 passages areshown in Figure 1.
Separate distributions are shown for all open-class words (both monosemous and polysemous) and for thepolysemous open-class words alone.No doubt some of this variation is attributable to differences ingenre between passages.
Table 1 lists the 15 categories of prosesampled by Francis and Ku~,era \[5\], along with the number of pas-sages of each type in the semantic oncordance and the average241Cure No ofPassagesB100-80-60-40- -20- -0~I4OI I50 6OPercent CorrectAllwardsI I70 80Fig.
1.
Cumulative distributions ofpercent correct when the most-frequent heuristic is applied to 104 passages from the BrownCorpus.percentage correct according to the most-frequent heuristic.
Thepassages of "Informative Prose" (A through J) tend to give lowerscores than the passages of "Imaginative Prose" (K through R),suggesting that fiction writers are slightly more likely to use wordsin their commonest enses.
But the differences are small.Table 1Mean percent correct for genresrecognized by Francis and Ku~era.Genre N All Words PolysemousA.
Press: Reportage 7 69 60B.
Press: Editorial 2 63 51C.
Press: Reviews 3 64 54D.
Religion 4 62 52E.
Skills and Hobbies 6 63 53F.
Popular/.,ore 4 66 54G.
Belles Let~es 3 64 52H.
Miscellaneous (reports) 1 62 50J.
Learned (science) 33 66 55K.
General Fiction 29 69 59L.
Detective Fiction 2 68 58M.
Science Fiction 2 68 57N.
Western Fiction 1 68 59P.
Romance and Love Story 2 67 55R.
Humor 5 69 583.3 Ef fects  o f  Guess ingAs the most-frequent heuristic is defined above, when apolysemous open-class word is encountered in the test material thathas not occurred anywhere in the training material, a random guessat its sere  is used.
Such cases, which lower the average scores,are a necessary but unfortunate consequence of the relatively smallsample of tagged text that is available; with a large sample weshould have sense frequencies for all of the polysemous words.However, we can get some idea of how significent this effect is bysimply omitting all instances of guessing, i.e., by basing the percen-tage correct only on those words for which there are data availablein the training material.When guesses are dropped out, an improvement of approximately2% is obtained.
That is to say, the mean for all substantive wordsincreases fxom 66.9% to 69.0% (a = 3.8%), and the mean forpolysemous words alone increases from 56.4% to 58.2% (o =4.5%).We take these values to be our current best estimates of the perfor-manca of a most-frequent heuristic when a large database is avail-able.
Stated differently: any sense identification system that doesno better than 69% (or 58% for polysemous words) is no improve-ment over a most-frequent heuristic.4.
THE CO-OCCURRENCE HEURIST IC  1The criterion of correcmess in these studies is agreement with thejudgment of a human reader, so it should be insu-uctive toconsiderhow readers do it.
A reader's judgments are made on the basis ofwhole phrases or sentences; enses of co-occurring words areallowed to determine one another and are identified together.
Thegeneral rule is that only senses that suit all of the words in a sen-tence can co-oeeur; not only does word W. constrain the sense of 1 another word W in the same sentence, but W also constrains the 2 sense of W.. ~2hat is what is meant when we say that context1guides a reader in determining the senses of individual words.Given the importance of co-occurring senses, therefore, we under-took to determine whether, on the basis of the available data, co-occurrences could be exploited for sense identificatiorLIn addition to information about he most frequent senses, a seman-tic concordance also contains information about senses that tend tooccur together in the same sentences.
It is possible to compile asemantic o-oocurrence matrix: a matrix showing how often thesenses of each word co-occor in sentences in the semantic concor-dance.
For example, if the test sentence is "The horses and menwere saved," we search the semantic co-occurrence malxix for co-occurrences of horse/n and man/n, horse/n and save/v, and man/nand save/v.
This search reveals that the fifth sense of the nounhorse, horse/nS, co-occurred twice in the same sentence withman/n2 and four times with man/n6, but neither horse/n or man/nco-occurred in the same sentence with save/v.
If we then take themost frequent of the two co-occurring senses of manht, we selectman/n2.
But no co-occurrence information isprovided as to whichone of the 7 senses of save/v should be chosen; for save/v it isnecessary toresort o the most frequent sense, as described above.The co-occurrence h uristic was defined as follows.
First, compilea semantic o-occurrence matrix.
That is to say, for every word-sense in the semantic oncordance, compile a list of all the otherword-senses that co-occur with h in any sentence.
Then, onencountering a noun, verb, adjective, or adverb in the test material,look it up in WordNet.
If the word is monosemous, assign thatsense to it.
If the word has more than one sense in WordNet, con-suit the semantic co-occurrence matrix to determine what senses ofthe word co-occur in the training material with other words in thetest sentence.
If only one sense of the polysemous word co-occurrsin the training material with other words in the test sentence, assignthat sense to it.
If more than one sense of the polysemous wordco-occurs in the training material with other words in test sentence,select from among the co-occunLng senses the sense that is mostfrequent in the training material; break ties by a random choice.
Ifthe polysemous word does not co-occur in the training materialwith other words in the test sentence, select he sense that is most242frequent in the training material; break ties by a random choice.And if the polysemous word does not occur at all in the trainingmaterial, choose a sense at random with a probability of I/n.In short, where there are data indicating co-occurrences of senses ofpolysemous words, use them; if not, use the most-frequent heuris-tic; otherwise, guess.When this co-occurrence h uristic was applied to the 104 seraanti-cal\]y tagged passages, the results were almost identical to those forthe most-frequent heuristic.
Means using the co-occurrence h uris-tic were perhaps a half percent lower than those obtained with themost-frequent heuristic.
And when the effects of guessing wereremoved, an improvement of approximately 2% was obtained, asbefore.
This similarity can be attributed to the limited size of thesemantic oncordance: no co-occurrence data were available for28% of the polysemous words, so the most-frequent heuristic hadto he used; moreover, those words for which co-occurrence datawere available tended to occur in their most frequent senses.On the basis of results obtained with the available sample ofsemantically tagged text, therefore, there is nothing to be gained byusing the more complex co-occurrence h uristic.
Since context isso important in sense identification, however, we concluded thatour semantic oncordance is still too small to estimate the potentiallimits of a co-occurrence h uristic.5.
SUMMARY AND CONCLUSIONSThe considerable improvement that results from having knowledgeof sense frequencies i apparent from the results summarized inTable 2, where the guessing heuristic is contrasted with the most-frequent and co-occurrence h uristics (with guessing removed).Table 2Percent correct sense identifications for open-class wordswithout and with information on sense frequencies.Monosemous Polysemous Heuristic and Polysemous onlyGuessing 45.0 26.8Most frequent 69.0 58.2Co-occuzrence 68.6 57.7The similarity of the results obtained with the most-frequent andthe co-occurrence heuristics is attributable to the fact that whenco-occurrence data were indeterminate or lacking, the most-frequent heuristic was the default.
With a large semantic oncor-dance, we would expect he co-occurrence h uristic to do better--itshould be able to capture the topical context which, in other work\[8\], we have found to give scores as high as 70-75% forpolysemous words.How representative are the percentages in Table 2?
Obviously,they are specific to the Brown Corpus; in a restricted omain ofdiscourse, polysemous words would not be used in such a widevariety of ways and a most-frequent heuristic would be correct farmore frequently.
The percentages in Table 2 are "broadlyrepresentative of current edited American English" \[3\].
They arealso, of course, specific to WordNet.
If WordNet did not draw somany sense distinctions, all of these statistical heuristics would becorrect more often.
But WordNet does not draw impossibly finesense distinctions.
Dictionaries differ widely in the number ofsense distinctions they draw; pocket dictionaries offer few and una-bridged dictionaries offer many alternative senses.
WordNet issomewhere in the middle; it provides about the same semanticgranularity as a good desk dictionary.
Anything coarser could nothave been used to tag passages from the Brown Corpus.Finally, can these heuristics provide anything more than bench-marks?
Can they play a role in a system that does an acceptablejob of sense identification?
It should be noted that none of theseheuristics takes into account the local context.
Even the co-occurrence heuristic is indifferent o word order; imposing word-order constraints would have made sparse data sparser still.
Localcontext--say, ?
2 or 3 words--should contain sufficient informa-tion to identify the intended sense of most polysemous words.Given a system capable of exploiting local context, statisticalheuristics might still provide a default, as Yarowsky \[9\] suggests;something to fall back on when local identification fails.
Underthose conditions, these statistical heuristics could indeed provide afloor on which more intelligent systems could build.ACKNOWLEDGMENTSThis work has been sponsored in part by Grant No.
N00014-91-J-1634 from the Advanced Research Projects Agency, Informationand Technology Office, and the Office of Naval Research, and inpart by grants from the James S. McDonnell Foundation, from ThePew Charitable Trusts, from the Linguistic Data Consortium, andfrom Sun Microsystems.
We are indebted to Henry Ku~era nd W.Nelson Francis for permission to use the Brown Corpus in ourresearch.
And we are indebted for assistance and advice to Ross T.Bunker, Chfisfiane Fellbaum, Benjamin Johnson-Laird, KatherineMiller, Randee Tengi, Pamela Wakefield, and Scott Wayland.REFERENCESI.
Gale, W., Church, K. W., and Yarowsky, D. (1992) Estimatingupper and lower beunds on the performance of word-sensedisambiguation programs.
Proceedings ofthe 30th AnnualMeeting of the Association for ComputationaI Linguistics, pp.249-256.2.
Miller, G. A., Ed.
(1990) Five Papers on WordNet.
Interna-tional Journal of Lexicology, 3, No.
4.
(Revised, March1993)3.
Francis, W. N., and Ku~era, H. (1982) Frequency Analysis ofEnglish Usage: Lexicon and Grammar.
Boston, MA:Houghton Mifflin.4.
Lorge, I.
(1937) The English semantic ount.
Teachers Col-lege Record, 39, 65-77.5.
Thomdike, E. L., and Barnhart, C. L., Eds.
(1935) Thorndike-Barnhart Junior Dictionary.
Glenview, IL: Scott Foresmam6.
Collins COBUILD English Language Dictionary.
(1987) Lon-don: Collins.7.
Miller, G. A., Leacock, C., Tengi, R., and Bunker, R. (1993) Asemantic oncordance.
Proceedings ofa Human LanguageTechnology Workshop, p. 303-308.8.
Leacock, C., Towel\], G., and Voorhees, E. (1993) Corpus-based statistical sense resolution.
Proceedings ofa HumanLanguage Technology Workshop, p. 260-265.9.
Yarowsky, D. (1993) One sense per collocation.
Proceedingsof a Human Language Technology Workshop, p. 266-271.243
