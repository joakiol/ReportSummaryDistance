The problem of ontology alignment on the web: a first reportDavide Fossati and Gabriele Ghidoni and Barbara Di Eugenioand Isabel Cruz and Huiyong Xiao and Rajen SubbaComputer ScienceUniversity of IllinoisChicago, IL, USAdfossa1@uic.edu, red.one.999@virgilio.it, bdieugen@cs.uic.eduifc@cs.uic.edu, hxiao2@uic.edu, rsubba@cs.uic.eduAbstractThis paper presents a general architec-ture and four algorithms that use Natu-ral Language Processing for automatic on-tology matching.
The proposed approachis purely instance based, i.e., only theinstance documents associated with thenodes of ontologies are taken into account.The four algorithms have been evaluatedusing real world test data, taken from theGoogle and LookSmart online directories.The results show that NLP techniques ap-plied to instance documents help the sys-tem achieve higher performance.1 IntroductionMany fundamental issues about the viability andexploitation of the web as a linguistic corpus havenot been tackled yet.
The web is a massive reposi-tory of text and multimedia data.
However, there isnot a systematic way of classifying and retrievingthese documents.
Computational Linguists are ofcourse not the only ones looking at these issues;research on the Semantic Web focuses on pro-viding a semantic description of all the resourceson the web, resulting into a mesh of informationlinked up in such a way as to be easily process-able by machines, on a global scale.
You can thinkof it as being an efficient way of representing dataon the World Wide Web, or as a globally linkeddatabase.1 The way the vision of the SemanticWeb will be achieved, is by describing each doc-ument using languages such as RDF Schema andOWL, which are capable of explicitly expressingthe meaning of terms in vocabularies and the rela-tionships between those terms.1http://infomesh.net/2001/swintro/The issue we are focusing on in this paper isthat these languages are used to define ontologiesas well.
If ultimately a single ontology were usedto describe all the documents on the web, sys-tems would be able to exchange information in atransparent way for the end user.
The availabilityof such a standard ontology would be extremelyhelpful to NLP as well, e.g., it would make it fareasier to retrieve all documents on a certain topic.However, until this vision becomes a reality, a plu-rality of ontologies are being used to describe doc-uments and their content.
The task of automaticontology alignment ormatching (Hughes and Ash-pole, 2005) then needs to be addressed.The task of ontology matching has been typi-cally carried out manually or semi-automatically,for example through the use of graphical user in-terfaces (Noy and Musen, 2000).
Previous workhas been done to provide automated support to thistime consuming task (Rahm and Bernstein, 2001;Cruz and Rajendran, 2003; Doan et al, 2003;Cruz et al, 2004; Subba and Masud, 2004).
Thevarious methods can be classified into two maincategories: schema based and instance based.Schema based approaches try to infer the seman-tic mappings by exploiting information related tothe structure of the ontologies to be matched, liketheir topological properties, the labels or descrip-tion of their nodes, and structural constraints de-fined on the schemas of the ontologies.
Thesemethods do not take into account the actual dataclassified by the ontologies.
On the other hand,instance based approaches look at the informationcontained in the instances of each element of theschema.
These methods try to infer the relation-ships between the nodes of the ontologies fromthe analysis of their instances.
Finally, hybridapproaches combine schema and instance based51methods into integrated systems.Neither instance level information, nor NLPtechniques have been extensively explored in pre-vious work on ontology matching.
For exam-ple, (Agirre et al, 2000) exploits documents (in-stances) on the WWW to enrich WordNet (Milleret al, 1990), i.e., to compute ?concept signatures,?collection of words that significantly distinguishone sense from another, however, not directly forontology matching.
(Liu et al, 2005) uses doc-uments retrieved via queries augmented with, forexample, synonyms that WordNet provides to im-prove the accuracy of the queries themselves, butnot for ontology matching.
NLP techniques suchas POS tagging, or parsing, have been used forontology matching, but on the names and defini-tions in the ontology itself, for example, in (Hovy,2002), hence with a schema based methodology.In this paper, we describe the results we ob-tained when using some simple but effective NLPmethods to align web ontologies, using an instancebased approach.
As we will see, our results showthat more sophisticated methods do not necessar-ily lead to better results.2 General architectureThe instance based approach we propose usesNLP techniques to compute matching scoresbased on the documents classified under the nodesof ontologies.
There is no assumption on the struc-tural properties of the ontologies to be compared:they can be any kind of graph representable inOWL.
The instance documents are assumed to betext documents (plain text or HTML).The matching process starts from a pair of on-tologies to be aligned.
The two ontologies aretraversed and, for each node having at least oneinstance, the system computes a signature basedon the instance documents.
Then, the signaturesassociated to the nodes of the two ontologies arecompared pairwise, and a similarity score for eachpair is generated.
This score could then be usedto estimate the likelihood of a match between apair of nodes, under the assumption that the se-mantics of a node corresponds to the semantics ofthe instance documents classified under that node.Figure 1 shows the architecture of our system.The two main issues to be addressed are (1)the representation of signatures and (2) the def-inition of a suitable comparison metric betweensignatures.
For a long time, the Information Re-Ontologiesdescription(OWL)Instancedocuments(HTML orplain text) NodeSignaturesCreation SignaturesComparison SimilarityScoresFileSystem WordNetFigure 1: Ontology aligment architectureHTML tagsremoval Tokenizationpunctuationremoval Lowercaseconversion Tokengroupingand countingFigure 2: Baseline signature creationtrieval community has succesfully adopted a ?bagof words?
approach to effectively represent andcompare text documents.
We start from there todefine a general signature structure and a metric tocompare signatures.A signature is defined as a function S : K ?R+, mapping a finite set of keys (which can becomplex objects) to positive real values.
With asignature of that form, we can use the cosine sim-ilarity metric to score the similarity between twosignatures:simil(S1, S2) =?p S1(kp)S2(kp)?
?i S1(ki)2 ??
?j S2(kj)2kp ?
K1 ?K2, ki ?
K1, kj ?
K2The cosine similarity formula produces a valuein the range [0, 1].
The meaning of that value de-pends on the algorithm used to build the signa-ture.
In particular, there is no predefined thresh-old that can be used to discriminate matches fromnon-matches.
However, such a threshold could becomputed a-posteriori from a statistical analysis ofexperimental results.2.1 Signature generation algorithmsFor our experiments, we defined and implementedfour algorithms to generate signatures.
The fouralgorithms make use of text and language process-ing techniques of increasing complexity.2.1.1 Algorithm 1: Baseline signatureThe baseline algorithm performs a very simplesequence of text processing, schematically repre-sented in Figure 2.52HTML tagsremoval Tokenizationpunctuationremoval LowercaseconversionNoungroupingand countingPOS taggingFigure 3: Noun signature creationHTML tags are first removed from the in-stance documents.
Then, the texts are tokenizedand punctuation is removed.
Everything is thenconverted to lowercase.
Finally, the tokens aregrouped and counted.
The final signature has theform of a mapping table token ?
frequency.The main problem we expected with thismethod is the presence of a lot of noise.
In fact,many ?irrelevant?
words, like determiners, prepo-sitions, and so on, are added to the final signature.2.1.2 Algorithm 2: Noun signatureTo cope with the problem of excessive noise,people in IR often use fixed lists of stop wordsto be removed from the texts.
Instead, we intro-duced a syntax based filter in our chain of pro-cessing.
The main assuption is that nouns are thewords that carry most of the meaning for our kindof document comparison.
Thus, we introduceda part-of-speech tagger right after the tokeniza-tion module (Figure 3).
The results of the taggerare used to discard everything but nouns from theinput documents.
The part-of-speech tagger weused ?QTAG 3.1 (Tufis and Mason, 1998), readilyavailable on the web as a Java library?
is a HiddenMarkov Model based statistical tagger.The problems we expected with this approachare related to the high specialization of words innatural language.
Different nouns can bear simi-lar meaning, but our system would treat them as ifthey were completely unrelated words.
For exam-ple, the words ?apple?
and ?orange?
are semanti-cally closer than ?apple?
and ?chair,?
but a purelysyntactic approach would not make any differencebetween these two pairs.
Also, the current methoddoes not include morphological processing, so dif-ferent inflections of the same word, such as ?ap-ple?
and ?apples,?
are treated as distinct words.In further experiments, we also consideredverbs, another syntactic category of words bearinga lot of semantics in natural language.
We com-puted signatures with verbs only, and with verbsand nouns together.
In both cases, however, theHTML tagsremoval Tokenizationpunctuationremoval LowercaseconversionSynsetsgroupingand countingPOS taggingNoun lookupon WordNet SynsetshierarchicalexpansionNoungroupingand countingFigure 4: WordNet signature creationperformance of the system was worse.
Thus, wewill not consider verbs in the rest of the paper.2.1.3 Algorithm 3: WordNet signatureTo address the limitations stated above, we usedthe WordNet lexical resource (Miller et al, 1990).WordNet is a dictionary where words are linkedtogether by semantic relationships.
In Word-Net, words are grouped into synsets, i.e., sets ofsynonyms.
Each synset can have links to othersynsets.
These links represent semantic relation-ships like hypernymy, hyponymy, and so on.In our approach, after the extraction of nounsand their grouping, each noun is looked up onWordNet (Figure 4).
The synsets to which thenoun belongs are added to the final signature inplace of the noun itself.
The signature can alsobe enriched with the hypernyms of these synsets,up to a specified level.
The final signature has theform of a mapping synset ?
value, where value isa weighted sum of all the synsets found.Two important parameters of this method arerelated to the hypernym expansion process men-tioned above.
The first parameter is the maximumlevel of hypernyms to be added to the signature(hypernym level).
A hypernym level value of 0would make the algorithm add only the synsets ofa word, without any hypernym, to the signature.
Avalue of 1 would cause the algorithm to add alsotheir parents in the hypernym hierarchy to the sig-nature.
With higher values, all the ancestors up tothe specified level are added.
The second parame-ter, hypernym factor, specifies the damping of theweight of the hypernyms in the expansion process.Our algorithm exponentially dampens the hyper-nyms, i.e., the weigth of a hypernym decreases ex-ponentially as its level increases.
The hypernymfactor is the base of the exponential function.In general, a noun can have more than onesense, e.g., ?apple?
can be either a fruit or a tree.This is reflected in WordNet by the fact that anoun can belong to multiple synsets.
With thecurrent approach, the system cannot decide which53HTML tagsremoval Tokenizationpunctuationremoval LowercaseconversionSynsetsgroupingand countingPOS taggingNoun lookupon WordNet SynsetshierarchicalexpansionWord sensedisambigua-tionFigure 5: Disambiguated signature creationsense is the most appropriate, so all the sensesof a word are added to the final signature, witha weight inversely proportional to the number ofpossible senses of that word.
This fact poten-tially introduces semantic noise in the signature,because many irrelevant senses might be added tothe signature itself.Another limitation is that a portion of the nounsin the source texts cannot be located in WordNet(see Figure 6).
Thus, we also tried a variation (al-gorithm 3+2) that falls back on to the bare lexi-cal form of a noun if it cannot be found in Word-Net.
This variation, however, resulted in a slightdecrease of performance.2.1.4 Algorithm 4: Disambiguated signatureThe problem of having multiple senses for eachword calls for the adoption of word sense dis-ambiguation techniques.
Thus, we implementeda word sense disambiguator algorithm, and weinserted it into the signature generation pipeline(Figure 5).
For each noun in the input documents,the disambiguator takes into account a specifiednumber of context words, i.e., nouns precedingand/or following the target word.
The algorithmcomputes a measure of the semantic distance be-tween the possible senses of the target word andthe senses of each of its context words, pair-wise.
A sense for the target word is chosen suchthat the total distance to its context is minimized.The semantic distance between two synsets is de-fined here as the minimum number of hops inthe WordNet hypernym hierarchy connecting thetwo synsets.
This definition allows for a rela-tively straightforward computation of the seman-tic distance using WordNet.
Other more sophisti-cated definitions of semantic distance can be foundin (Patwardhan et al, 2003).
The word sensedisambiguation algorithm we implemented is cer-tainly simpler than others proposed in the litera-ture, but we used it to see whether a method that isrelatively simple to implement could still help.The overall parameters for this signature cre-ation algorithm are the same as the WordNet sig-nature algorithm, plus two additional parametersfor the word sense disambiguator: left contextlength and right context length.
They represent re-spectively how many nouns before and after thetarget should be taken into account by the dis-ambiguator.
If those two parameters are both setto zero, then no context is provided, and the firstpossible sense is chosen.
Notice that even in thiscase the behaviour of this signature generation al-gorithm is different from the previous one.
Ina WordNet signature, every possible sense for aword is inserted, whereas in a WordNet disam-biguated signature only one sense is added.3 Experimental settingAll the algorithms described in the previous sec-tion have been fully implemented in a coherentand extensible framework using the Java program-ming language, and evaluation experiments havebeen run.
This section describes how the experi-ments have been conducted.3.1 Test dataThe evaluation of ontology matching approachesis usually made difficult by the scarceness of testontologies readily available in the community.This problem is even worse for instance based ap-proaches, because the test ontologies need also tobe ?filled?
with instance documents.
Also, wewanted to test our algorithms with ?real world?data, rather than toy examples.We were able to collect suitable test data start-ing from the ontologies published by the OntologyAlignment Evaluation Initiative 2005 (Euzenat etal., 2005).
A section of their data contained anOWL representation of fragments of the Google,Yahoo, and LookSmart web directories.
We ?re-verse engineered?
some of this fragments, in or-der to reconstruct two consistent trees, one rep-resenting part of the Google directory structure,the other representing part of the LookSmart hi-erarchy.
The leaf nodes of these trees were filledwith instances downloaded from the web pagesclassified by the appropriate directories.
With thismethod, we were able to fill 7 nodes of each ontol-ogy with 10 documents per node, for a total of 140documents.
Each document came from a distinctweb page, so there was no overlap in the data to becompared.
A graphical representation of our twotest ontologies, source and target, is shown in Fig-54ure 6.
The darker outlined nodes are those filledwith instance documents.
For the sake of readabil-ity, the names of the nodes corresponding to realmatches are the same.
Of course, this informa-tion is not used by our algorithms, which adopt apurely instance based approach.
Figure 6 also re-ports the size of the instance documents associatedto each node: total number of words, noun tokens,nouns, and nouns covered by WordNet.3.2 ParametersThe experiments have been run with several com-binations of the relevant parameters: number ofinstance documents per node (5 or 10), algorithm(1 to 4), extracted parts of speech (nouns, verbs, orboth), hypernym level (an integer value equal orgreater than zero), hypernym factor (a real num-ber), and context length (an integer number equalor greater than zero).
Not all of the parameters areapplicable to every algorithm.
The total number ofruns was 90.4 ResultsEach run of the system with our test ontologiesproduced a set of 49 values, representing thematching score of every pair of nodes containinginstances across the two ontologies.
Selected ex-amples of these results are shown in Tables 1, 2,3, and 4.
In the experiments shown in those ta-bles, 10 instance documents for each node wereused to compute the signatures.
Nodes that ac-tually match (identified by the same label, e.g.,?Canada?
and ?Canada?)
should show high sim-ilarity scores, whereas nodes that do not match(e.g., ?Canada?
and ?Dendrochronology?
), shouldhave low scores.
Better algorithms would havehigher scores for matching nodes, and lower scorefor non-matching ones.
Notice that the two nodes?Egypt?
and ?Pyramid Theories,?
although intu-itively related, have documents that take differentperspectives on the subject.
So, the algorithmscorrectly identify the nodes as being different.Looking at the results in this form makes it dif-ficult to precisely assess the quality of the algo-rithms.
To do so, a statistical analysis has to beperformed.
For each table of results, let us parti-tion the scores in two distinct sets:A = {simil(nodei, nodej) | real match = true}B = {simil(nodei, nodej) | real match = false}Target nodeCanadaCanada 0.95 0.89 0.89 0.91 0.87 0.86 0.920.90 0.97 0.91 0.90 0.88 0.87 0.92Egypt 0.86 0.89 0.91 0.87 0.86 0.88 0.90Megaliths 0.90 0.91 0.99 0.93 0.95 0.94 0.93Museums 0.89 0.88 0.90 0.93 0.88 0.87 0.900.88 0.88 0.95 0.91 0.99 0.93 0.910.87 0.87 0.86 0.88 0.82 0.82 0.96Source node Dendro chronology Megaliths Museums Nazca Lines Pyramid Theories United KingdomDendro chronologyNazca LinesUnited KingdomTable 1: Results ?
Baseline signature algorithmTarget nodeCanadaCanada 0.67 0.20 0.14 0.35 0.08 0.08 0.410.22 0.80 0.15 0.22 0.09 0.09 0.25Egypt 0.13 0.23 0.26 0.22 0.17 0.24 0.25Megaliths 0.28 0.20 0.85 0.37 0.22 0.27 0.33Museums 0.30 0.19 0.18 0.58 0.08 0.14 0.270.13 0.12 0.26 0.18 0.96 0.14 0.170.42 0.20 0.17 0.26 0.09 0.11 0.80Source node Dendro chronology Megaliths Museums Nazca Lines Pyramid Theories United KingdomDendro chronologyNazca LinesUnited KingdomTable 2: Results ?
Noun signature algorithmTarget nodeCanadaCanada 0.79 0.19 0.19 0.38 0.15 0.06 0.560.26 0.83 0.18 0.20 0.16 0.07 0.24Egypt 0.17 0.24 0.32 0.21 0.31 0.30 0.27Megaliths 0.39 0.21 0.81 0.41 0.40 0.25 0.42Museums 0.31 0.14 0.17 0.70 0.11 0.11 0.260.24 0.20 0.42 0.29 0.91 0.21 0.290.56 0.17 0.22 0.25 0.15 0.08 0.84Source node Dendro chronology Megaliths Museums Nazca Lines Pyramid Theories United KingdomDendro chronologyNazca LinesUnited KingdomTable 3: Results ?
WordNet signature algorithm(hypernym level=0)55TopGoogleScienceSocial_SciencesArchaeologyEgyptAlternativeMegaliths South_AmericaNazca_LinesMethodologyDendrochronologyMuseums PublicationsEuropeUnitedKingdomOrganizationsNorthAmericaCanadaTopLookSmartScience_&_HealthSocial_ScienceArchaeologyPyramid_TheoriesTopicsMegaliths Nazca_LinesScienceDendrochronologyMuseums PublicationsUnitedKingdomAssociationsCanadaSeven_Wonders_of_the_WorldGyza_PyramidsSource Target44307; 16001;3957; 2220 15447; 4400;1660; 1428 18754; 5574; 1725; 15767362; 2377; 953; 8232872; 949;499; 4419620; 3257; 1233; 1001 3972; 1355;603; 541 3523; 1270;617; 55523039; 7926; 1762; 145113705; 3958;1484; 13036171; 2333;943; 84410721; 3280; 1099; 9887841; 2486; 869; 76917196; 5529;1792; 1486Figure 6: Ontologies used in the experiments.
The numbers below the leaves indicate the size of instancedocuments: # of words; # of noun tokens; # of nouns; # of nouns in WordNetTarget nodeCanadaCanada 0.68 0.18 0.13 0.33 0.12 0.05 0.440.23 0.79 0.15 0.20 0.14 0.07 0.23Egypt 0.15 0.23 0.28 0.22 0.27 0.31 0.27Megaliths 0.30 0.18 0.84 0.37 0.34 0.27 0.33Museums 0.29 0.16 0.15 0.60 0.11 0.10 0.240.20 0.17 0.38 0.26 0.89 0.21 0.260.45 0.17 0.18 0.24 0.15 0.08 0.80Source node Dendro chronology Megaliths Museums Nazca Lines Pyramid Theories United KingdomDendro chronologyNazca LinesUnited KingdomTable 4: Results ?
Disambiguated signature al-gorithm (hypernym level=0, left context=1, rightcontext=1)With our test data, we would have 6 values inset A and 43 values in set B.
Then, let us com-pute average and standard deviation of the valuesincluded in each set.
The average of A representsthe expected score that the system would assignto a match; likewise, the average of B is the ex-pected score of a non-match.
We define the fol-lowing measure to compare the performance ofour matching algorithms, inspired by ?effect size?from (VanLehn et al, 2005):discrimination size =avg(A) ?
avg(B)stdev(A) + stdev(B)Higher discrimination values mean that thescores assigned to matches and non-matches aremore ?far away,?
making it possible to use thosescores to make more reliable decisions about thematching degree of pairs of nodes.Table 5 shows the values of discrimination size(last column) out of selected results from our ex-periments.
The algorithm used is reported in thefirst column, and the values of the other relevantparameters are indicated in other columns.
We canmake the following observations.?
Algorithms 2, 3, and 4 generally outperformthe baseline (algorithm 1).?
Algorithm 2 (Noun signature), which stilluses a fairly simple and purely syntacticaltechnique, shows a substantial improvement.Algorithm 3 (WordNet signature), which in-troduces some additional level of semantics,has even better performance.?
In algorithms 3 and 4, hypernym expansionlooks detrimental to performance.
In fact, thebest results are obtained with hypernym levelequal to zero (no hypernym expansion).?
The word sense disambiguator implementedin algorithm 4 does not help.
Even thoughdisambiguating with some limited context(1 word before and 1 word after) providesslightly better results than choosing the firstavailable sense for a word (context lengthequal to zero), the overall results are worsethan adding all the possible senses to the sig-nature (algorithm 3).?
Using only 5 documents per node signifi-cantly degrades the performance of all the al-gorithms (see the last 5 lines of the table).5 Conclusions and future workThe results of our experiments point out severalresearch questions and directions for future work,56Alg Docs POS Hyp lev Hyp fac L cont R cont Avg (A) Stdev (A) Avg (B) Stdev (B) Discrimination size1 10 0.96 0.02 0.89 0.03 1.372 10 noun 0.78 0.13 0.21 0.09 2.552 10 verb 0.64 0.20 0.31 0.11 1.042 10 nn+vb 0.77 0.14 0.21 0.09 2.483 10 noun 0 0.81 0.07 0.25 0.12 3.083 10 noun 1 1 0.85 0.07 0.41 0.12 2.353 10 noun 1 2 0.84 0.07 0.34 0.12 2.643 10 noun 1 3 0.83 0.07 0.31 0.12 2.803 10 noun 2 1 0.90 0.06 0.62 0.11 1.643 10 noun 2 2 0.86 0.07 0.45 0.12 2.183 10 noun 2 3 0.84 0.07 0.36 0.12 2.563 10 noun 3 1 0.95 0.04 0.78 0.08 1.443 10 noun 3 2 0.88 0.07 0.52 0.12 1.913 10 noun 3 3 0.85 0.07 0.38 0.12 2.453+2 10 noun 0 0 0.80 0.09 0.21 0.11 2.943+2 10 noun 1 2 0.83 0.08 0.30 0.11 2.733+2 10 noun 2 2 0.85 0.08 0.39 0.11 2.404 10 noun 0 0 0 0.80 0.12 0.24 0.10 2.644 10 noun 0 1 1 0.77 0.11 0.22 0.10 2.674 10 noun 0 2 2 0.77 0.11 0.23 0.10 2.594 10 noun 1 2 0 0 0.82 0.10 0.29 0.10 2.564 10 noun 1 2 1 1 0.80 0.10 0.34 0.10 2.274 10 noun 1 2 2 2 0.80 0.10 0.35 0.10 2.221 5 noun 0.93 0.05 0.86 0.04 0.882 5 noun 0.66 0.23 0.17 0.08 1.613 5 noun 0 0.70 0.17 0.21 0.11 1.764 5 noun 0 0 0 0.69 0.21 0.20 0.09 1.634 5 noun 0 1 1 0.64 0.21 0.18 0.08 1.58Table 5: Results ?
Discrimination sizesome more specific and some more general.
Asregards the more specific issues,?
Algorithm 2 does not perform morphologicalprocessing, whereas Algorithm 3 does.
Howmuch of the improved effectiveness of Algo-rithm 3 is due to this fact?
To answer thisquestion, Algorithm 2 could be enhanced toinclude a morphological processor.?
The effectiveness of Algorithms 3 and 4 maybe hindered by the fact that many words arenot yet included in theWordNet database (seeFigure 6).
Falling back on to Algorithm 2proved not to be a solution.
The impact of theincompleteness of the lexical resource shouldbe investigated and assessed more precisely.Another venue of research may be to exploitdifferent thesauri, such as the ones automati-cally derived as in (Curran andMoens, 2002).?
The performance of Algorithm 4 might beimproved by using more sophisticated wordsense disambiguation methods.
It would alsobe interesting to explore the application ofthe unsupervised method described in (Mc-Carthy et al, 2004).As regards our long term plans, first, structuralproperties of the ontologies could potentially beexploited for the computation of node signatures.This kind of enhancement would make our systemmove from a purely instance based approach to acombined hybrid approach based on schema andinstances.More fundamentally, we need to address thelack of appropriate, domain specific resources thatcan support the training of algorithms and modelsappropriate for the task at hand.
WordNet is a verygeneral lexicon that does not support domain spe-cific vocabulary, such as that used in geosciencesor in medicine or simply that contained in a sub-ontology that users may define according to theirinterests.
Of course, we do not want to developby hand domain specific resources that we have tochange each time a new domain arises.The crucial research issue is how to exploit ex-tremely scarce resources to build efficient and ef-fective models.
The issue of scarce resourcesmakes it impossible to use methods that are suc-cesful at discriminating documents based on thewords they contain but that need large corporafor training, for example Latent Semantic Anal-ysis (Landauer et al, 1998).
The experiments de-scribed in this paper could be seen as providing57a bootstrapped model (Riloff and Jones, 1999; Ngand Cardie, 2003)?in ML, bootstrapping requiresto seed the classifier with a small number of wellchosen target examples.
We could develop a webspider, based on the work described on this paper,to automatically retrieve larger amounts of train-ing and test data, that in turn could be processedwith more sophisticated NLP techniques.AcknowledgementsThis work was partially supported by NSF AwardsIIS?0133123, IIS?0326284, IIS?0513553, andONR Grant N00014?00?1?0640.ReferencesEneko Agirre, Olatz Ansa, Eduard Hovy, and DavidMartinez.
2000.
Enriching very large ontologiesusing the WWW.
In ECAI Workshop on OntologyLearning, Berlin, August.Isabel F. Cruz and Afsheen Rajendran.
2003.
Ex-ploring a new approach to the alignment of ontolo-gies.
In Workshop on Semantic Web Technologiesfor Searching and Retrieving Scientific Data, in co-operation with the International Semantic Web Con-ference.Isabel F. Cruz, William Sunna, and Anjli Chaudhry.2004.
Semi-automatic ontology alignment forgeospatial data integration.
GIScience, pages 51?66.James Curran and Marc Moens.
2002.
Improve-ments in automatic thesaurus extraction.
In Work-shop on Unsupervised Lexical Acquisition, pages59?67, Philadelphia, PA, USA.AnHai Doan, Jayant Madhavan, Robin Dhamankar, Pe-dro Domingos, and Alon Halevy.
2003.
Learning tomatch ontologies on the semantic web.
VLDB Jour-nal, 12(4):303?319.Je?ro?me Euzenat, Heiner Stuckenschmidt, andMikalai Yatskevich.
2005.
Introductionto the ontology alignment evaluation 2005.http://oaei.inrialpes.fr/2005/results/oaei2005.pdf.Eduard Hovy.
2002.
Comparing sets of semantic rela-tions in ontology.
In R. Green, C. A. Bean, and S. H.Myaeng, editors, Semantics of Relationships: An In-terdisciplinary Perspective, pages 91?110.
Kluwer.T.
C. Hughes and B. C. Ashpole.
2005.
The semanticsof ontology alignment.
Draft Paper, Lockheed Mar-tin Advanced Technology Laboratories, Cherry Hill,NJ.
http://www.atl.lmco.com/projects/ontology/ pa-pers/ SOA.pdf.Thomas K. Landauer, Peter W. Foltz, and Darrell La-ham.
1998.
Introduction to Latent Semantic Analy-sis.
Discourse Processes, 25:259?284.Shuang Liu, Clement Yu, and Weiyi Meng.
2005.Word sense disambiguation in queries.
In ACMConference on Information and Knowledge Man-agement (CIKM2005), Bremen, Germany.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word senses inuntagged text.
In 42nd Annual Meeting of the As-sociation for Computational Linguistics, Barcelona,Spain.G.
A. Miller, R. Beckwith, C. Fellbaum, D. Gross, andK.
J. Miller.
1990.
Introduction to wordnet: an on-line lexical database.
International Journal of Lexi-cography, 3 (4):235?244.Vincent Ng and Claire Cardie.
2003.
Bootstrappingcoreference classifiers with multiple machine learn-ing algorithms.
In The 2003 Conference on Em-pirical Methods in Natural Language Processing(EMNLP-2003).Natalya Fridman Noy and Mark A. Musen.
2000.Prompt: Algorithm and tool for automated ontologymerging and alignment.
In National Conference onArtificial Intelligence (AAAI).Siddharth Patwardhan, Satanjeev Banerjee, and TedPedersen.
2003.
Using semantic relatedness forword sense disambiguation.
In Fourth InternationalConference on Intelligent Text Processing and Com-putational Linguistics (CiCLING-03), Mexico City.Erhard Rahm and Philip A. Bernstein.
2001.
A sur-vey of approaches to automatic schema matching.VLDB Journal, 10(4):334?350.Ellen Riloff and Rosie Jones.
1999.
Learning dic-tionaries for information extraction by multi-levelbootstrapping.
In AAAI-99, Sixteenth National Con-ference on Artificial Intelligence.Rajen Subba and Sadia Masud.
2004.
Automatic gen-eration of a thesaurus using wordnet as a means tomap concepts.
Tech report, University of Illinois atChicago.Dan Tufis and Oliver Mason.
1998.
Tagging romaniantexts: a case study for qtag, a language independentprobabilistic tagger.
In First International Confer-ence on Language Resources & Evaluation (LREC),pages 589?596, Granada, Spain.Kurt VanLehn, Collin Lynch, Kay Schulze, JoelShapiro, Robert Shelby, Linwood Taylor, DonaldTreacy, Anders Weinstein, and Mary Wintersgill.2005.
The andes physics tutoring system: Five yearsof evaluations.
In 12th International Conference onArtificial Intelligence in Education, Amsterdam.58
