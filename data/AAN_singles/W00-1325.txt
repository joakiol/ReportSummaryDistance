Statistical Filtering and Subcategorization Frame AcquisitionAnna Korhonen and  Genev ieve  Gor re l lComputer Laboratory, University of CambridgePembroke Street, Cambridge CB2 3QG, UKalk23@cl, cam.
ac.
uk, genevieve, gorrel l@netdecis ions,  co. ukDiana  McCar thySchool of Cognitive and Computing SciencesUniversity of Sussex, Brighton, BN1 9QH, UKdianam@cogs, usx.
ac.
ukAbst rac tResearch "into the automatic acquisition ofsubcategorization frames (SCFS) from corporais starting to produce large-scale computa-tional lexicons which include valuable fre-quency information.
However, the accuracyof the resulting lexicons shows room for im-provement.
One significant source of errorlies in the statistical filtering used by some re-searchers to remove noise from automaticallyacquired subcategorization frames.
In this pa-per, we compare three different approaches tofiltering out spurious hypotheses.
Two hy-pothesis tests perform poorly, compared tofiltering frames on the basis of relative fre-quency.
We discuss reasons for this and con-sider directions for future research.1 In t roduct ionSubcategorization information is vital for suc-cessful parsing, however, manual develop-ment of large subcategorized lexicons hasproved difficult because predicates change be-haviour between sublanguages, domains andover time.
Additionally, manually devel-oped sucategorization lexicons do not providethe relative frequency of different SCFs for agiven predicate, ssential in a probabilistic ap-proach.Over the past years acquiring subcatego-rization dictionaries from textual corpora hasbecome increasingly popular.
The differentapproaches (e.g.
Brent, !991, 1993; Ushiodaet al, 1993; Briscoe and Carroll, 1997; Man-ning, 1993; Carroll and Rooth, 1998; Gahl,1998; Lapata, 1999; Sarkar and Zeman, 2000)vary largely according to the methods usedand the number of SCFS being extracted.
Re-gardless of this, there is a ceiling on the perfor-mance of these systems at around 80% tokenrecall 1zWhere token recall is the percentage .ofSCF to-kens in a sample of manually analysed text that wereThe approaches to extracting SCF informa-tion from corpora have frequently employedstatistical methods for filtering (e.g.
Brent,1993; Manning 1993; Briscoe and Carroll,1997; Lapata, 1999).
This has been done toremove the noise that arises when dealing withnaturally occurring data, and from mistakesmade by the SCF acquisition system, for ex-ample, parser errors.Filtering is usually done with a hypothe-sis test, and frequently with a variation ofthe binomial filter introduced by Brent (1991,1993).
Hypothesis testing is performed by for-mulating a null hypothesis, (H0), which is as-sumed true unless there is evidence to the con-trary.
If there is evidence to the contrary,H0 is rejected and the alternative hypothe-sis (H1) is accepted.
In SCF acquisition, H0 isthat there is no association between aparticu-lar verb (verbj) and a SCF (SCFi), meanwhileH1 is that there is such an association.
ForSCF acquisition, the test is one-tailed since H1states the direction of the association, a pos-itive correlation between verbj and scfi.
Wecompare the expected probability of scfi oc-curring with verbj if H0 is true, to the ob-served probability of co-occurrence obtainedfrom the corpus data.
If the observed proba-bility is greater than the expected probabilitywe reject Ho and accept H1, and if not, weretain H0.Despite the popularity of this method, ithas been reported as problematic.
Accord-ing to one account (Briscoe and Carroll, 1997)the majority of errors arise because of the sta-tistical filtering process, which is reported tobe particularly unreliable for low frequencySCFs (Brent, 1993; Briscoe and Carroll, 1997;Manning, 1993; Manning and Schiitze, 1999).Lapata (1999) reported that a threshold onthe relative frequencies produced slightly bet-ter results than those achieved with a Brent-correctly acquired by the system.199style binomial filter when establishing SCFs fordiathesis alternation detection.
Lapata deter-mined thresholds for each SCF using the fre-quency of the SCF in COMLEX Syntax dictio-nary (Grishman et al, 1994).Adopting the SCF acquisition system ofBriscoe and Carroll, we have experimentedwith an alternative hypothesis test, the bi-nomial log-likelihood ratio (LLR) test (Dun-ning, 1993).
Sarkar and Zeman (2000) havealso used this test when filtering SCFs auto-matically acquired for Czech.
This test hasbeen recommended for use in NLP since itdoes not assume a normal distribution, whichinvalidates many other parametric tests foruse with natural language phenomena.
LLRcan be used in a form (-2logA) which isX 2 distributed.
Moreover, this asymptote isappropriate at quite low frequencies, whichmakes the hypothesis test particularly usefulwhen dealing with natural anguage phenom-ena, where low frequency events are common-place.A problem with using hypothesis testing forfiltering automatically acquired SCFs is ob-taining a good estimation of the expected oc-currence of scfi with verbj.
This is oftenperformed using the unconditional distribu-tion, that is the probability distribution overall SCFS, regardless of the verb.
It is as-sumed that verbj must occur with scfi sig-nificantly more than is expected given thisestimate.
Our paper addresses the problemthat the conditional distribution, dependenton the verb, and unconditional distributionare rarely correlated.
Therefore statistical fil-ters which assume such correlation for H0 willbe susceptible to error,In this paper, we compare the results ofthe Brent style binomial filter of Briscoe andCarroll and the LLR filter to a simple methodwhich uses a threshold on the relative frequen-cies of the verb and SCF combinations.
Wedo this within the framework of the Briscoeand Carroll SCF acquisition system, which isdescribed in section 2.1.
The details of thetwo statistical filters are described in section2.2, along with the details of the threshold ap-plied to the relative frequencies output fromthe SCF acquisition system.
The details of theexperimental evaluation are supplied in sec-tion 3.
We discuss our findings in section 3.3and conclude with directions for future work(section 4).2 Method2.1 F ramework  for SCF  Acquisit ionBriscoe and Carroll's (1997) verbal acquisitionsystem distinguishes 163 SCFs and returns rel-ative frequencies for each SCF found for a givenpredicate.
The SCFs are a superset of classesfound in the Alvey NL Tools (ANLT) dictio-nary, Boguraev et al (1987) and the COML~XSyntax dictionary, Grishman et al (1994).They incorporate information about controlof predicative arguments, as well as alterna-tions such as extraposition and particle move-ment.
The system employs a shallow parser toobtain the subcategorization information.
Po-tential SCF entries are filtered before the finalSCF lexicon is produced.
The filter is the onlycomponent of this system which we experi-ment with here.
The three filtering methodswhich we compare are described below.2.2 Fi l ter ing Methods2.2.1 B inomia l  Hypothes is  TestBriscoe and Carroll (1997) used a binomialhypothesis test (BHT) to filter the acquiredSCFs.
They applied BHT as follows.
The sys-tem recorded the total number of sets of SCFcues (n) found for a given predicate, and thenumber of these sets for a given SCF (ra).
Thesystem estimated the error probability (pe)that a cue for a SCF (scfi) occurred with averb which did not take scfi.
pe was esti-mated in two stages, as shown in equation 1.Firstly, the number of verbs which are mem-bers of the target SCF in the ANLT dictionarywere extracted.
This number was convertedto a probability of class membership by divid-ing by the total number of verbs in the dic-tionary.
The complement of this probabilityprovided an estimate for the probability of averb not taking scfi.
Secondly, this proba-bility was multiplied by an estimate for theprobability of observing the cue for scfi.
Thiswas estimated using the number of cues for iextracted from the Susanue corpus (Sampson,1995), divided by the total number of cues.pe = (1  - Iverbsl    i  cZass il I eSlc e l, for il (1)The probability of an event with probability phappening exactly rn times out of n attemptsis given by the following binomial distribution:20On~ P(m,n,p) = m!
(n-  m)!
pro(1 _p)n-m (2)The probability of the event happening m ormore times is:= (3)k=rnFinally, P(m+, n,p e) is the probabil ity thatm or more occurrences of cues for scfi will oc-cur with a verb which is not a member ofscfi,given n occurrences of that verb.
A thresholdon this probability, P(m+,n, pe), was set atless than or equal to 0.05.
This yielded a 95%or better confidence that a high enough pro-portion of cues for scfi have been observed forthe verb to be legitimately assigned scfi.Other approaches which use a binomial fil-ter differ in respect of the calculation of theerror probability.
Brent (1993) estimated theerror probabilities for each SCF experimen-tally from the behaviour of his SCF extrac-tor, which detected simple morpho-syntacticcues in the corpus data.
Manning (1993) in-Creased the number of available cues at the ex-pense of the reliability of these cues.
To main-tain high levels of accuracy, Manning appliedhigher bounds on the error probabilities forcertain cues.
These bounds were determinedexperimentally.
A similar approach was takenby Briscoe, Carroll and Korhonen (1997) in amodification to the Briscoe and Carroll sys-tem.
The overall performance was increasedby changing the estimates of pe according tothe performance of the system for the targetSCF.
In the work described here, we use theoriginal BHT proposed by Briscoe and Carroll.2.2.2 The  B inomia l  Log L ike l ihoodRat io  as a S ta t i s t i ca l  F i l te rDunning (1993) demonstrates the benefits ofthe LLR statistic, compared to Pearson's chi-squared, on the task of ranking bigram data.The binomial log-likelihood ratio test issimple to calculate.
For each verb and SCFcombination four counts are required.
Theseare the number of times that:1. the target verb occurs with the target SCF(kl)2. the target verb occurs with any other SCF(nl - kl)3. any other verb occurs with the target SCF(k2)4. any other verb occurs with any other SCF- k2)The statistic -21ogA is calculated as follows:-log-likelihood =where2\[logL(pl, kl, nl )+logL(p2, k2, n2)-logL(p, kl, nl)-logL(p, k2, n2) \] (4)logL(p, n, k) = k x logp + (n - k) x log(1 -p )andkl k2 kl + k2P l=- - ,  P2------ ,  P - -  nl n2 nl -4- n2The LLR statistic provides a score that re-flects the difference in (i) the number of bitsit takes to describe the observed data, usingpl = p(SCFIverb ) and p2 = p(SCFl-~verb ),and (ii) the number of bits it takes to de-scribe the expected ata using the probabilityp = p(scFlany verb).The LLR statistic detects differences be-tween pl  and p2.
The difference couldpotentially be in either direction, but we areinterested in LLRS where p l  > p2, i.e.
wherethere is a positive association between the SCFand the verb.
For these cases, we comparedthe value of -2logA to the threshold valueobtained from Pearson's Chi-Squared table,to see if it was significant at the 95% level 2.2.2.3 Us ing  a Thresho ld  on theRe la t ive  Frequenc ies  as aBase l ineIn order to examine the baseline performanceof this system without employing any notionof the significance of the observations, weused a threshold on relative frequencies.
Thiswas done by extracting the SCFS, and rank-ing them in the order of the probability oftheir occurrence with the verb.
The probabil-ities were estimated using a maximum likeli-hood estimate (MLE) from the observed rela-tive frequencies.
A threshold, determined em-pirically, was applied to these probability esti-mates to filter out the low probability entriesfor each verb.
....2See (Gorrell, 1999) for details of this" method.2013 Eva luat ion3.1 MethodTo evaluate the different approaches, we tooka sample of 10 million words of the BNC cor-pus (Leech, 1992).
We extracted all sentencescontaining an occurrence of one of fourteenverbs 3.
The verbs were chosen at random,subject to the constraint that they exhibitedmultiple complementation patterns.
After theextraction process, we retained 3000 citations,on average, for each verb.
The sentences con-taining these verbs were processed by the SCFacquisition system, and then we applied thethree filtering methods described above.
Wealso obtained results for a baseline withoutany filtering.The results were evaluated against a man-ual analysis of corpus data 4.
This was ob-tained by analysing up to a maximum of 300occurrences for each of the 14 test verbs inLOB (Garside et al, 1987), Susanne and SEC(Taylor and Knowles, 1988) corpora.
Follow-ing Briscoe and Carroll (1997), we calculatedprecision (percentage of SCFS acquired whichwere also exemplified in the manual analysis)and recall (percentage of the SCFs exemplifiedin the manual analysis which were acquiredautomatically).
We also combined precisionand recall into a single measure of overall per-formance using the F measure (MA.nniug andSchiitze, 1999).F = 2.precis ion.
recall (5)precision + recall3.2 Resu l tsTable 1 gives the raw results for the 14 verbsusing each method.
It shows the number oftrue positives (TP), .false positives (FP), and.false negatives (FN), as determined accord-ing to the manual analysis.
The results forhigh frequency SCFs (above 0.01 relative fre-quency), medium frequency (between 0.001and 0.01) and low frequency (below 0.001)SCFs are listed respectively in the second,3These verbs were ask, begin, believe, cause, expect,find, give, help, like, move, produce, provide, seem,swing.4The importance of the manual analysis is outlinedin Briscoe and Carroll (1997).
We use the same man-ual analysis as Briscoe and Carroll, Le.
one from theSusanne, LOB, and SEC corpora.
A manual analysis ofthe BNC data might produce better results.
However,since the BNC is a heterogeneous corpus we felt it wasreasonable to test the data on a different corpus, whichis also heterogeneous.third and fourth columns, and the final col-umn includes the total results for all frequencyranges.Table 2 shows precision and recall for the 14verbs and the F measure, which combines pre-cision and recall.
We also provide the baselineresults, if all SCFs were accepted.From the results given in tables 1 and 2, theMLE approach outperformed both hypothesistests.
For both BHT and LLR there was anincrease in FNs at high frequencies, and anincrease in FPs at medium and low frequen-cies, when compared to MLE.
The number oferrors was typically larger for LLR than BHT.The hypothesis tests reduced the number ofFNS at medium and low frequencies, however,this was countered by the substantial increasein FPs that they gave.
While BHT nearly al-ways acquired the three most frequent SCFs ofverbs correctly, LLR tended to reject these.While the high number of FNS can be ex-plained by reports which have shown LLR tobe over-conservative (Ribas, 1995; Pedersen,1996), the high number of FPs is surprising.Although theoretically, the strength of LLRlies in its suitability for low frequency data,the results displayed in table 1 do not suggestthat the method performs better than BHT onlow frequency frames.MLE thresholding produced better resultsthan the two statistical tests used.
Preci-sion improved considerably, showing that theclasses occurring in the data with the high-est frequency are often correct.
Although MLEthresholding clearly makes no attempt to solvethe sparse data problem, it performs betterthan BHT or LLR overall.
MLE is not adept atfinding low frequency SCFS, however, the othermethods are problematic in that they wronglyaccept more than they correctly reject.
Thebaseline, of accepting all SCFS, obtained a highrecall at the expense of precision.3.3 D iscuss ionOur results indicate that MLE outperformsboth hypothesis tests.
There are two explana-tions for this, and these are jointly responsiblefor the results.Firstly, the SCF distribution is zipfian, asare many distributions concerned with nat-ural language (Manning and Schiitze, 1999).Figure 1 shows the conditional distributionfor the verb find.
This ~mf~ltered SCF prob-ability distribution was obtained from 20 Mwords of BNC data output from the SCF sys-202High FreqTP FP  FNBHT 75 29 23LLR 66 30 32MLE 92 31 6Med ium Freq Low FreqTP FP  FN TP FP  I FN11 37 31 4 23 159 52 33 2 23 170 0 42 0 0 19TotalsTP FP I FNm90 89 6977 105 8292 31 67Table 1: Raw results for 14 test verbs~r31ff.t: Precision % Recall % F measureBHT 50.3 56.6 53.3LLR 42.3 48.4 45.1MLE 74.8 57.8 65.2baseline 24.3 83.5 37.6Table 2: Precision, Recall, and F measure0.10.01&0.0010.0001.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
!.oI , r i i , , i , l  , , i i i i ,10 100rank0.10.01o.oo~0.01~10  4\, , , , t , , r  , i , , i , , ,110 100rankFigure 1: Hypothesised SCF distribution forfindtern.
The unconditional distribution obtainedfrom the observed istribution of SCFs in the20 M words of BNC is shown in figure 2.
Thefigures show SCF rank on the X-axis versusSCF frequency on the Y-axis, using logarith-mic scales.
The line indicates the closest Zipf-like power law fit to the data.Secondly, the hypothesis tests make thefalse assumption (H0) that the unconditionaland conditional distributions are correlated.The fact that a significant improvement inperformance is made by correcting the priorprobabilities according to the performance ofthe system (Briscoe, Carroll and Korhonen,Figure 2: Hypothesised unconditional SCF dis-tribution1997) suggests the discrepancy between theunconditional and the conditional distribu-tions.We examined the correlation between themanual analysis for the 14 verbs, and theunconditional distribution of verb types overall SCFs estimated from the ANLT using theSpearman Rank Correlation Coefficient.
Theresults included in table 3 show that only amoderate correlation was found averaged overall verb types.Both LLR and BHT work by comparing theobserved value of p(scfi\[verbj) to that ex-pected by chance.
They both use the observed203\[ Verb Rank  Correlationask 0.10begin 0.83believe 0.77cause 0.19expectfind0.450.33give 0.06help 0.43like 0.56move 0.53produce 0.95provide 0.65seem 0.16swingAverage0.500.47Table 3: Rank correlation between the condi-tional SCF distributions of the test verbs andthe unconditional distributionvalue for p(sc.filverbj) from the system's out-put, and they both use an estimate for the un-conditional probability distribution (p(scfi))for estimating the expected probability.
Theydiffer in the way that the estimate for the un-conditional probability is obtained, and theway that it is used in hypothesis testing.For  BHT, the null hypothesis i that the ob-served value ofp(scfiIverbj) arose by chance,because of noise in the data.
We estimate theprobability that the value observed could havearisen by chance using p(m+,  n,pe), pe is cal-culated using:?
the SCF acquisition system's raw (until-tered) estimate for the unconditional dis-tribution, which is obtained from the Su-sanne corpus and?
the ANLT estimate of the unconditionaldistribution of a verb not taking scf~,across all SCFsFor LLR, both the conditional (pl) and un-conditional distributions (p2) are estimatedfrom the BNC data.
The unconditional proba-bility distribution uses the occurrence of scfiwith any verb other than our target.The binomial tests look at one point in theSCF distribution at a time, for a given verb.The expected value is determined using theunconditional distribution, on the assumptionthat if the null hypothesis true then this dis-tribution will correlate with the conditionaldistribution.
However, this is rarely the case.Moreover, because of the zipfian nature ofthe distributions, the frequency differences atany point can be substantial.
In these exper-iments, we used one-tailed tests because wewere looking for cases where there was a pos-itive association between the SCF and verb,however, in a two-tailed test the null hypoth-esis would rarely be accepted, because of thesubstantial differences in the conditional andunconditional distributions.A large number of false negatives occurredfor high frequency SCFs because the probabil-ity we compared them to was too high.
Thisprobability was estimated from the combina-tion of many verbs genuinely occurring withthe frame in question, rather than from an es-timate of background noise from verbs whichdid not occur with the frame.
We did not usean estimate from verbs which do not take theSCF, since this would require a priori knowl-edge about the phenomena that we were en-deavouring to acquire automatically.
For LLRthe unconditional probability estimate (p2)was high, simply because this SCF was a com-mon one, rather than because the data wasparticularly noisy.
For BHT, R e was likewisetoo high as the SCF was also common in theSusanne data.
The ANLT estimate went some-way to compensating for this, thus we ob-tained fewer false negatives with BHT thanLLR.A large number of false positives occurredfor low frequency SCFs because the estimatefor p(scf) was low.
This estimate was morereadily exceeded by the conditional estimate.For BHT false positives arose because of thelow estimate of p(scf) (from Susanne) andbecause the estimate of p(-,SCF) from ANLTdid not compensate enough for this.
For LLR,there was no mean~ to compensate for the factthat p2 was lower than pl .In contrast, MLE did not compare two dis-tributions.
Simply rejecting the low frequencydata produced better results overall by avoid-ing the false positives with the low frequencydata, and the false negatives with the highfrequency data.4 Conc lus ionThis paper explored three possibilities for fil-tering out the SCF entries produced by a SCFacquisition system.
These were (i) a versionof Brent's binomial filter, commonly used forthis purpose, (ii) the binomial og-likelihood204ratio test, recommended for use with low fre-quency data and (iii) a simple method usinga threshold on the MLEs of  the SCFS outputfrom the system.
Surprisingly, the simple MLEthresholding method worked best.
The BHTand LLR both produced an astounding mlm-ber of FPs, particularly at low frequencies.Further work on handling low frequencydata in SCF acquisition is warranted.
A non-parametric statistical test, such as Fisher's ex-act test, recommended by Pedersen (1996),might improve on the results obtained usingparametric tests.
However, it seems from ourexperiments hat it would be better to avoidhypothesis tests that make use of the uncon-ditional distribution.One possibility is to put more effort into theestimation of pe, and to avoid use of the un-conditional distribution for this.
In some re-cent experiments, we tried optimising the es-timates for pe depending on the performanceof the system for the target SCF, using themethod proposed by Briscoe, Carroll and Ko-rhonen (1997).
The estimates of pe were ob-tained from a training set separate to the held-out BNC data used for testing.
Results usingthe new estimates for pe gave an improvementof 10% precision and 6% recall, compared tothe BHT results reported here.
Nevertheless,the precision result was 14% worse for preci-sion than MLE, though there was a 4% im-provement in recall, making the overall per-formance 3.9 worse than MLE according to theF measure.
Lapata (1999) also reported thata simple relative frequency cut off producedslightly better esults than a Brent style BHT.If MLE thresholding persistently achievesbetter results, it would be worth investi-gating ways of handling the low frequencydata, such as smoothing, for integration withthis method.
However, more sophisticatedsmoothing methods, which back-off to an un-Conditional distribution, will also suffer fromthe lack of correlation between conditionaland unconditional SCF distributions.
Any sta-tistical test would work better at low frequen-cies than the MLE, since this simply disregardsall low frequency SCFs.
In our experiments, ffwe had used MLE only for the high frequencydata, and BHT for medium and low, then over-all we would have had 54% precision and 67%recall.
It certainly seems worth employing hy-pothesis tests which do not rely on the un-conditional distribution for the low frequencySCFS.5 AcknowledgementsWe thank Ted Briscoe for many helpful dis-cussions and suggestions concerning this work.We also acknowledge Yuval Krymolowski foruseful comments on this paper.Re ferencesBoguraev, B., Briscoe, E., Carroll, J., Carter,D.
and Grover, C. 1987.
The derivation of agrammatically-indexed lexicon from the Long-man Dictionary of Contemporary English.
InProceedings of the 25th Annual Meeting ofthe Association for Computational Linguis-tics, Stanford, CA.
193-200.Brent, M. 1991.
Automatic acquisition ofsubcategorization frames from untagged text.In Proceedings of the 29th Annual Meetingof the Association for Computational Linguis-tics, Berkeley, CA.
209-214.Brent, M. 1993.
From gra.mmar to lexicon:unsupervised learning of lexical syntax.
Com-putational Linguistics 19.3: 243-262.Briscoe, E.J.
and J. Carroll 1997.
Automaticextraction of subcategorization from corpora.In Proceedings of the 5th ACL Conf.
on Ap-plied Nat.
Lg.
Proc., Washington, DC.
356-363.Briscoe, E., Carroll, J. and Korhonen, A.1997.
Automatic extraction of subcategoriza-tion frames from corpora - a framework and3 experiments.
'97 Sparkle WP5 Deliverable,available in http://www.ilc.pi.cnr.it/.Carroll, G. and Rooth, M. 1998.
Valenceinduction with a head-lexicalized PCFG.
InProceedings of the 3rd Conference on Empir-ical Methods in Natural Language Processing,Granada, Spain.Dunning, T. 1993.
Accurate methods for theStatistics of Surprise and Coincidence.
Com-putational Linguistics 19.1: 61-74.Gahl, S. 1998.
Automatic extraction of sub-corpora based on subcategorization framesfrom a part-of-speech tagged corpus.
In Pro-ceedings of the COLING-A CL'98, Montreal,Canada.Garside, R., Leech, G. and Sampson, G. 1987.The computational nalysis of English: Acorpus-based approach.
Longman, London.Gorrell, G. 1999.
Acquiring Subcategorisationfrom Textual Corpora.
MPhil dissertation,University of Cambridge, UK.205Grishman, R., Macleod, C. and Meyers, A.1994.
Comlex syntax: building a computa-tional lexicon.
In Proceedings of the Interna-tional Conference on Computational Linguis-tics, COLING-94, Kyoto, Japan.
268-272.Lapata, M. 1999.
Acquiring lexical gener-alizations from corpora: A case study fordiathesis alternations.
In Proceedings of the37th Annual Meeting of the Association forComputational Linguistics, Maryland.
397-404.Leech, G. 1992.
100 million words of English:the British National Corpus.
Language Re-search 28(1): 1-13.Manning, C. 1993.
Automatic acquisition ofa large subcategorization dictionary from cor-pora.
In Proceedings of the 31st Annual Meet-ing of the Association .for Computational Lin-guistics, Columbus, Ohio.
235-242.Manning, C. and Schiitze, H. 1999.
Founda-tions of Statistical Natural Language Process-ing.
MIT Press, Cambridge MA.Pedersen, T. 1996.
Fishing for Exactness.
InProceedings of the South-Central SAS UsersGroup Conference SCSUG-96, Austin, Texas.Ribas, F. 1995.
On Acquiring Appropriate Se-lectional Restrictions from Corpora Using aSemantic Taxonomy.
Ph.D thesis, Universityof Catalonia.Sampson, G. 1995.
English for the computer.Oxford University Press, Oxford UK.Sarkar, A. and Zeman, D. 2000.
Auto-matic Extraction of Subcategorization Framesfor Czech.
In Proceedings of the Inter-national Conference on Computational Lin-guistics, COLING-O0, Saarbrucken, Germany.691-697.Taylor, L. and Knowles, G. 1988.
Manualof information to accompany the SEC cor-pus: the machine-readable corpus of spokenEnglish.
University of Lancaster, UK, Ms.Ushioda, A., Evans, D., Gibson, T. andWaibel, A.
1993.
The automatic acquisition offrequencies of verb subcategorization framesfrom tagged corpora.
In Boguraev, B. andPustejovsky, J. eds.
SIGLEX A CL Workshopon the Acquisition of Lexieal Knowledge .fromText.
Columbus, Ohio: 95-106.206i
