Using Masks, Suffix Array-based Data Structures and MultidimensionalArrays to Compute Positional Ngram Statistics from CorporaAlexandre Gil*Computer Science DepartmentNew University of LisbonCaparica, Portugalagil@pt.ibm.comGa?l DiasCentre of MathematicsBeira Interior UniversityCovilh?, Portugalddg@di.ubi.pt* The authors want to thank Professor Jos?
Gabriel Pereira Lopes from the New University of Lisbon for his advices.AbstractThis paper describes an implementation tocompute positional ngram statistics (i.e.
Fre-quency and Mutual Expectation) based onmasks, suffix array-based data structures andmultidimensional arrays.
Positional ngramsare ordered sequences of words that representcontinuous or discontinuous substrings of acorpus.
In particular, the positional ngrammodel has shown successful results for the ex-traction of discontinuous collocations fromlarge corpora.
However, its computation isheavy.
For instance, 4.299.742 positionalngrams (n=1..7) can be generated from a100.000-word size corpus in a seven-wordsize window context.
In comparison, only700.000 ngrams would be computed for theclassical ngram model.
It is clear that huge ef-forts need to be made to process positionalngram statistics in reasonable time and space.Our solution shows O(h(F) N log N) timecomplexity where N is the corpus size andh(F) a function of the window context.1 IntroductionMany models have been proposed to evaluate word de-pendencies.
One of the most successful statistical mod-els is certainly the ngram model (Jelinek, 1990).However, in order to overcome its conceptual rigidity,T.
Kuhn et al (1994) have defined the polygram modelthat estimates the probability of an ngram by interpolat-ing the relative frequencies of all its kgrams (k ?
n).Another way to account for variable length dependen-cies is the n-multigram model designed by Deligne andBimbot (1995).All these models have in common the fact that theyneed to compute continuous string frequencies.
Thistask can be colossal when gigabytes of data need to beprocessed.
Indeed, Yamamoto and Church (2000) showthat there exist N(N+1)/2 substrings in a N-size corpus.That is the reason why low order ngrams have beencommonly used in Natural Language Processing appli-cations.In the specific field of multiword unit extraction, Dias(2002) has introduced the positional ngram model thathas evidenced successful results for the extraction ofdiscontinuous collocations from large corpora.
Unlikelyprevious models, positional ngrams are ordered se-quences of tokens that represent continuous or discon-tinuous substrings of a corpus computed in a (2.F+1)-word size window context (F represents the context interms of words on the right and on the left of any wordin the corpus).
As a consequence, the number of gener-ated substrings rapidly explodes and reaches astronomicfigures.
Dias (2002) shows that ?
(Equation 1) posi-tional ngrams can be computed for an N-size corpus in a(2.F+1)-size window context.
( ) ????????
++??=?
?
?
?+= = =???
?1.23 1 11111.2FkFiFjikjij CCFFNEquation 1: Number of positional ngramsIn order to illustrate this equation, 4.299.742 positionalngrams (n=1..7) would be generated from a 100.000-word size corpus in a seven-word size window context.In comparison, only 700.000 ngrams would be com-puted for the classical ngram model.
It is clear that hugeefforts need to be made to process positional ngramstatistics in reasonable time and space.In this paper, we describe an implementation that com-putes the Frequency and the Mutual Expectation (Diaset al 1999) of any positional ngram with time complex-ity O(h(F) N log N).
The global architecture is based onthe definition of masks that allow virtually representingany positional ngram in the corpus.
Thus, we follow theVirtual Corpus approach introduced by Kit and Wilks(1998) and apply a suffix-array-like method, coupled tothe Multikey Quicksort algorithm (Bentley and Sedge-wick, 1997), to compute positional ngram frequencies.Finally, a multidimensional array is built to easilyprocess the Mutual Expectation, an association measurefor collocation extraction.The evaluation of our C++ implementation has beenrealized over the CETEMP?blico2 corpus and showssatisfactory results.
For example, it takes 8.59 minutesto compute both frequency and Mutual Expectation fora 1.092.7233-word corpus on an Intel Pentium III 900MHz Personal Computer for a seven-word size windowcontext.This article is divided into four sections: (1) we explainthe basic principles of positional ngrams and the maskrepresentation to build the Virtual Corpus; (2) we pre-sent the suffix-array-based data structure that allowscounting occurrences of positional ngrams; (3) we showhow a multidimensional array eases the efficient com-putation of the Mutual Expectation; (4) we present re-sults over different size sub-corpora of theCETEMP?blico corpus.2 Positional NgramsIn the specific field of multiword unit extraction, Dias(2002) has introduced the positional ngram model thathas evidenced successful results for the extraction ofdiscontinuous collocations from large corpora.2.1 PrinciplesThe original idea of the positional ngram model comesfrom the lexicographic evidence that most lexical rela-tions associate words separated by at most five otherwords (Sinclair, 1974).
As a consequence, lexical rela-tions such as collocations can be continuous or discon-tinuous sequences of words in a context of at mosteleven words (i.e.
5 words to the left of a pivot word, 52 The CETEMP?blico is a 180 million-word corpus of Portuguese.
Itcan be obtained at http://www.ldc.upenn.edu/.3 This represents 46.986.831 positional ngrams.words to the right of the same pivot word and the pivotword itself).
In general terms, a collocation can be de-fined as a specific4 continuous or discontinuous se-quence of words in a (2.F+1)-word size window context(i.e.
F words to the left of a pivot word, F words to theright of the same pivot word and the pivot word itself).This situation is illustrated in Figure 1 for the colloca-tion Ngram Statistics that fits in the window context.Figure 1: 2.F-word size window contextThus, as computation is involved, we need to process allpossible substrings (continuous or discontinuous) that fitinside the window context and contain the pivot word.Any of these substrings is called a positional ngram.
Forinstance, [Ngram Statistics] is a positional ngram as is thediscontinuous sequence [Ngram ___ from] where the gaprepresented by the underline stands for any word occur-ring between Ngram and from (in this case, Statistics).More examples are given in Table 1.Positional 2grams Positional 3grams[Ngram Statistics] [Ngram Statistics from][Ngram ___ from] [Ngram Statistics ___ Large][Ngram ___ ___ Large] [Ngram ___ from Large][to ___ Ngram] [to ___ Ngram ___ from]Table 1: Possible positional ngramsIn order to compute all the positional ngrams of a cor-pus, we need to take into account all the words as possi-ble pivot words.A   B   C   D   E   F   G   H   I    J    K   L   M   N   ....  X   Y  Z ....A   B   C   D   E   F   G   H   I    J    K   L   M   N   ....  X   Y  Z ....A   B   C   D   E   F   G   H   I    J    K   L   M   N   ....  X   Y  Z ....A   B   C   D   E   F   G   H   I    J    K   L   M   N   ....  X   Y  Z ....A   B   C   D   E   F   G   H   I    J    K   L   M   N   ....  X   Y  Z ............Figure 2: One-window context for F=3A simple way would be to shift the two-window contextto the right so that each word would sequentially beprocessed.
However, this would inevitably lead to du-plications of positional ngrams.
Instead, we propose a4 As specific, we intend a sequence that fits the definition of colloca-tion given by Dias (2002): ?A collocation is a recurrent sequence ofwords that co-occur together more than expected by chance in a givendomain?.Virtual   Approach to Deriving   Ngram Statistics from Large   ScalepivotF=3 F=3one-window context that shifts to the right along thecorpus as illustrated in Figure 2.
It is clear that the sizeof the new window should be 2.F+1.This new representation implies new restrictions.
Whileall combinations of words were valid positional ngramsin the two-window context, this is not true for a one-window context.
Indeed, two restrictions must be ob-served.Restriction 1: Any substring, in order to be valid, mustcontain the first word of the window context.Restriction 2: For any continuous or discontinuous sub-string in the window context, by shifting the substringfrom left to right, excluding gaps and words on the rightand inserting gaps on the left, so that there always existsa word in the central position cpos (Equation 2) of thewindow, there should be at least one shift that containsall the words of the substring in the context window.121.2 +??????
+= FcposEquation 2: Central position of the windowFor example, from the first case of Figure 2, the discon-tinuous sequence [A B _ _ E _ G] is not a positionalngram although it is a possible substring as it does notfollow the second restriction.
Indeed, whenever we tryto align the sequence to the central position, at least oneword is lost as shown in Table 2:PossibleshiftCentralwordDisappearingwords[_ _ A B _ _ E] B G[_ _ _ A B _ _] A E, GTable 2: Shifting SubstringsIn contrast, the sequence [A _ C _ E F _] is a positionalngram as the shift [_ A _ C _ E F], with C in the centralposition, includes all the words of the substring.Basically, the first restriction aims at avoiding duplica-tions and the second restriction simply guarantees thatno substring that would not be computed in a two-window context is processed.2.2 Virtual RepresentationThe representation of positional ngrams is an essentialstep towards efficient computation.
For that, purpose,we propose a reference representation rather than anexplicit structure of each positional ngram.
The idea isto adapt the suffix representation (Manber and Myers,1990) to the positional ngram case.Following the suffix representation, any continuouscorpus substring is virtually represented by a single po-sition of the corpus as illustrated in Figure 3.
In fact, thesubstring is the sequence of words that goes from theword referred by the position till the end of the corpus.Figure 3: Suffix Representation5Unfortunately, the suffix representation can not directlybe extended to the specific case of positional ngrams.One main reason aims at this situation: a positionalngram may represent a discontinuous sequence ofwords.
In order to overcome this situation, we propose arepresentation of positional ngrams based on masks.As we saw in the previous section, the computation ofall the positional ngrams is a repetitive process.
Foreach word in the corpus, there exists an algorithmicpattern that identifies all the possible positional ngramsin a 2.F+1-word size window context.
So, what we needis a way to represent this pattern in an elegant and effi-cient way.One way is to use a set of masks that identify all thevalid sequences of words in a given window context.Thus, each mask is nothing more than a sequence of 1and 0 (where 1 stands for a word and 0 for a gap) thatrepresents a specific positional ngram in the windowcontext.
An example is illustrated in Figure 4.Figure 4: Masks5 The $ symbol stands for the end of the corpus.1A2B3C4A5B6B7C8A9$corpusA B B C A $A B C A B B C A $A $B B C A $B C A B B C A $B C A $C A B B C A $C A $$ 987654321substringsF = 31 2 3 4 5 6 7 8 9 10corpus A B C D E F G H I Jmask 1 0 0 1 1 1 0ngram A _ _ D E F _X X X?Computing all the masks is an easy and quick process.In our implementation, the generation of masks is donerecursively and is negligible in terms of space and time.In table 3, we give the number of masks h(F) for differ-ent values of F.F h(F)1 42 113 434 1715 683Table 3: Number of masksIn order to identify each mask and to prepare the refer-ence representation of positional ngrams, an array ofmasks is finally built as in Figure 5.Figure 5: Masks ArrayFrom these structures, the virtual representation of anypositional ngram is straightforward.
Indeed, any posi-tional ngram can be identified by a position in the cor-pus and a given mask.
Taking into account that a corpusis a set of documents, any positional ngram can be rep-resented by the tuple {{iddoc, posdoc}, idmask} where iddocstands for the document id of the corpus, posdoc for agiven position in the document and idmask for a specificmask.
An example is illustrated in Figure 6.Figure 6: Virtual RepresentationAs we will see in the following section, this referencerepresentation will allow us to follow the Virtual Cor-pus approach introduced by Kit and Wilks (1998) tocompute ngram frequencies.3 Computing FrequencyWith the Virtual Corpus approach, counting continuoussubstrings can easily and efficiently be achieved.
Aftersorting the suffix-array data structure presented in Fig-ure 3, the count of an ngram consisting of any n wordsin the corpus is simply the count of the number of adja-cent indices that take the n words as prefix.
We illus-trate the Virtual Corpus approach in Figure 6.2gram Freq 3gram Freq[A B] 2 [A B B] 1[B B] 1 [B C A] 2Figure 6: Virtual Corpus ApproachCounting positional ngrams can be computed exactly inthe same way.
The suffix-array structure is sorted usinglexicographic ordering for each mask in the array ofmasks.
After sorting, the count of a positional ngram inthe corpus is simply the count of adjacent indices thatstand for the same sequence.
We illustrate the VirtualCorpus approach for positional ngrams in Figure 7.Figure 7: Virtual Corpus for positional ngramsmask..4 1 0 0 1 0 1 15 1 0 0 1 1 0 06 1 0 0 1 1 0 17 1 0 0 1 1 1 08 1 0 0 1 1 1 19 1 0 1 0 0 0 010 1 0 1 0 0 1 0..F=3posdoc 0 1 2 3 4 5 6 7 8 9 10 11 12corpus 0 A B C D  E F G H I  J K L M...masks..4 1 0 0 1 0 1 15 1 0 0 1 1 0 06 1 0 0 1 1 0 17 1 0 0 1 1 1 08 1 0 0 1 1 1 19 1 0 1 0 0 0 010 1 0 1 0 0 1 0..{ {0,2} , 7 } = [ C _ _ F G H _ ]9736258141 2B3C4A5B6B7C8A9$corpusB B C A $A B C A B B C A $A $B B C A $B C A B B C A $B C A $C A B B C A $C A $$AA375621948corpus1 2B3C4A5B6B7C8A9A Amasks...4 1 0 0 1 0 1 15 1 0 0 1 1 0 06 1 0 0 1 1 0 17 1 0 1 1 1 08 1 0 0 1 1 1 19 1 0 1 0 0 0 010 1 0 1 0 0 1 0...010 11B 12 C 13 A 14 B 15 B 16 C 17A 18$AA _ A _ _ _ _A _ B _ _ _ _A _ B _ _ _ _A _ C _ _ _ _B _ A _ _ _ _B _ A _ _ _ _B _ C _ _ _ _C _ A _ _ _ _?C _ B _ _ _ _?
_ ?
_ _ _ _The efficiency of the counting mainly resides in the useof an adapted sort algorithm.
Kit and Wilks (1998) pro-pose to use a bucket-radixsort although they acknowl-edge that the classical quicksort performs faster forlarge-vocabulary corpora.
Around the same perspective,Yamamoto and Church (2000) use the Manber andMyers?s algorithm (1990), an elegant radixsort-basedalgorithm that takes at most O(N log N) time and showsimproved results when long repeated substrings arecommon in the corpus.For the specific case of positional ngrams, we have cho-sen to implement the Multikey Quicksort algorithm(Bentley and Sedgewick, 1997) that can be seen as amixture of the Ternary-Split Quicksort (Bentley andMcIlroy, 1993) and the MSD6 radixsort (Anderson andNilsson, 1998).The algorithm processes as follows: (1) the array ofstring is partitioned into three parts based on the firstsymbol of each string.
In order to process the split apivot element is chosen just as in the classical quicksortgiving rise to: one part with elements smaller than thepivot, one part with elements equal to the pivot and onepart with elements larger than the pivot; (2) the smallerand the larger parts are recursively processed in exactlythe same manner as the whole array; (3) the equal part isalso sorted recursively but with partitioning startingfrom the second symbol of each string; (4) the processgoes on recursively: each time an equal part is beingprocessed, the considered position in each string ismoved forward by one symbol.In Figure 8, we propose an illustration of the MultikeyQuicksort taken from the paper (Bentley and Sedge-wick, 1997).
The pivot is chosen using the medianmethod.Figure 8: Sorting 12 two-letter words.6 MSD stands for Most Significant Digit.Different reasons have lead to use the Multikey Quick-sort algorithm.
First, it performs independently from thevocabulary size.
Second, it shows O(N log N) timecomplexity in our specific case.
Third, Anderson andNilsson (1998) show that it performs better than theMSD radixsort and proves comparable results to thenewly introduced Forward radixsort.Counting frequencies is just a preliminary step towardscollocation extraction.
The following step attaches anassociation measure to each positional ngram thatevaluates the interdependency between words inside agiven sequence.
In the positional ngram model, Dias etal.
(1999) propose the Mutual Expectation measure.4 Computing Mutual Expectation4.1 PrinciplesThe Mutual Expectation evaluates the degree of rigiditythat links together all the words contained in a posi-tional ngram (?n, n ?
2) based on the concept of Nor-malized Expectation and relative frequency.Normalized ExpectationThe basic idea of the Normalized Expectation is toevaluate the cost, in terms of cohesiveness, of the loss ofone word in a positional ngram.
Thus, the NormalizedExpectation measure is defined in Equation 3 where thefunction k(.)
returns the frequency of any positionalngram7.
[ ]( )[ ]( )[ ]( ) ????????
???????????
?+=?=nikknkNE2n1n^i^1i1 11n2n i 2i2 22n1ni1i1 11n1ni1i1 11u p ... u  p ... wpup ... up ... up1u p ... u ...p upu p ... u ...p upEquation 3: Normalized ExpectationFor that purpose, any positional ngram is defined alge-braically as a vector of words [p11 u1 p12 u2 ?
p1n un]where ui stands for any word in the positional ngramand p1i represents the distance that separates words u1and ui8.
Thus, the positional ngram [A _ C D E _ _] wouldbe rewritten as [0 A +2 C +3 D +4 E] and its NormalizedExpectation would be given by Equation 4.7 The "^" corresponds to a convention used in Algebra that consists inwriting a "^" on the top of the omitted term of a given successionindexed from 1 to n.8 By statement, any pii is equal to zero.ib osn teya he sttonf rUnsorted arraySorted array as  at   be    by   he       in    is       it           of          on       or          toas   is  be   by   on       in    at       it          of        he       or         to[ ]( ) [ ]( )[ ]( )[ ]( )[ ]( )[ ]( ) ?????????????
?++++++++++++++ =+++E 2 D 1 C0E 4 D 3A 0E 4 C 2A 0D 3 C 2A 041E 4 D 3 C 2A 0E 4 D 3 C 2A 0kkkkkNEwhich is equivalent to[ ]( ) [ ]( )[ ]( )[ ]( )[ ]( )[ ]( ) ?????????????
?+++=_ _ _ _ E D C_ _ E D _ _A_ _ E _ C _A_ _ _ D C _A41_ _ E D C _A _ _ E D C _AkkkkkNEEquation 4: Normalized Expectation exampleMutual ExpectationOne effective criterion for multiword lexical unit identi-fication is frequency.
From this assumption, Dias et al(1999) pose that between two positional ngrams withthe same Normalized Expectation, the most frequentpositional ngram is more likely to be a collocation.
So,the Mutual Expectation of any positional ngram is de-fined in Equation 5 based on its Normalized Expectationand its relative frequency.
[ ]( )[ ]( ) [ ]( )n1ni1i1 11n1ni1i1 11n1ni1i1 11u p ... u ...p upu p ... u ...p upu p ... u ...p upNEpME?=Equation 5: Mutual ExpectationIn order to compute the Mutual Expectation of any posi-tional ngram, it is necessary to build a data structure thatallows rapid and efficient search over the space of allpositional ngrams.
For that purpose, we propose a mul-tidimensional array structure called Matrix9.4.2 MatrixThe attentive reader will have noticed that the denomi-nator of the Normalized Expectation formula is the av-erage frequency of all the positional (n-1)gramsincluded in a given positional ngram.
These specificpositional ngrams are called positional sub-ngrams oforder n-110.
So, in order to compute the NormalizedExpectation and a fortiori the Mutual Expectation, it isnecessary to access efficiently to the sub-ngrams fre-quencies.
This operation is done through the Matrix.9 The Matrix also speeds up the extraction process that applies theGenLocalMaxs algorithm (Ga?l Dias, 2002).
We do not present thisalgorithm due to lack of space.10 In order to ease the reading, we will use the term sub-ngrams todenote positional sub-ngrams of order n-1.However, to understand the Matrix itself, we first needto show how the sub-ngrams of any positional ngramcan be represented.Representing sub-ngramsA sub-ngram is obtained by extracting one word at atime from its related positional ngram as shown in Fig-ure 9.Figure 9: Sub-ngramsBy representing a sub-ngram, we mean calculating itsvirtual representation that identifies its related substring.The previous figure shows that representing the firstthree sub-ngrams of the positional ngram {{0,0},14} isstraightforward as they all contain the first word of thewindow context.
The only difficulty is to know themask they are associated to.
Knowing this, the first threesub-ngrams would respectively be represented as:{{0,0},15}, {{0,0},16}, {{0,0},13}.For the last sub-ngram, the situation is different.
Thefirst word of the window context is omitted.
As a con-sequence, in order to calculate its virtual representation,we need to know the position of the first word of thesubstring as well as its corresponding mask.
In this case,the position in the document of the positional sub-ngramis simply the position of its related positional ngramplus the distance that separates the first word of thewindow context from the first word of the substring.
Wecall delta this distance.
The obvious representation ofthe fourth sub-ngram is then {{0,2},18} where the positionis calculated as 0+(delta=2)=2.In order to represent the sub-ngrams of any positionalngram, all we need is to keep track of the masks related0 1 2 3 4 5 6 7 8 9corpusA B C D E F G H I J ...ngram   {{0,0},14} A _ C D E _sub-ngram 1 A _ C D _ _ _sub-ngram 2 A _ C _ E _ _sub-ngram 3 A _ _ D E _ _delta=2_13 1 0 0 1 1 0 014 1 0 1 1 1 0 0sub-ngram 4 _ _ C D E _ _masksdocpos ......01 0 1 1 0 0 01 0 1 0 1 0 01 0 1 0 0 0 01 1 1 0 0 0 015161718to the mask of the positional ngram and the respectivedeltas.
Thus, it is clear that for each mask, there exists aset of pairs {idmask, delta} that allows identifying all thesub-ngrams of any given positional ngram.
Each pair iscalled a submask and is associated to its upper mask11 asillustrated in Figure 10.Figure 10: SubmasksNow that all necessary virtual representations are well-established, in order to calculate the Mutual Expecta-tion, we need to build a structure that allows efficientlyaccessing any positional ngram frequency.
This is theobjective of the Matrix, a 2-dimension array structure.2-dimension Array StructureSearching for specific positional ngrams in a huge sam-ple space can be overwhelming.
To overcome this com-putation problem, two solutions are possible: (1) keepthe suffix array-based data structure and design opti-mized search algorithms or (2) design a new data struc-ture to ease the searching process.
We chose the secondsolution as our complete system heavily depends onsearching through the entire space of positionalngrams12 and, as a consequence, we hardly believe thatimproved results may be reached following the secondsolution.This new structure is a 2-dimension array where linesstand for the masks ids and the columns for the posi-tions in the corpus.
Thus, each cell of the 2-dimensionarray represents a given positional ngram as shown inFigure 11.
This structure is called the Matrix.The frequency of each positional ngram can easily berepresented by all its positions in the corpus.
Indeed, agiven positional ngram is a substring that can appear indifferent positions of the corpus being the count of thesepositions its frequency.
From the previous suffix array-11 The upper mask is the mask from which the submasks are calcu-lated.
While upper masks represent positional ngrams, submasksrepresent sub-ngrams.12 In fact, this choice mainly has to do with the extraction process andthe application of the GenLocalMaxs algorithm.based data structure, calculating all these positions isstraightforward.Calculating the Mutual Expectation is also straightfor-ward and fast as accessing to any positional ngram canbe done in O(1) time complexity.
We will illustrate thisreality in the next section.Figure 11: The MatrixThe illustration of our architecture is now complete.
Wenow need to test our assumptions.
For that purpose, wepresent results of our implementation over theCETEMP?blico corpus.5 ExperimentsWe have conducted a number of experiments of ourC++ implementation on the CETEMP?blico Portuguesecorpus to derive positional ngram statistics (Frequencyand Mutual Expectation).
The experiments have beenrealized on an Intel Pentium 900 MHz PC with 390MBof RAM.
From the original corpus, we have randomlydefined 5 different size sub-corpora that we present inTable 4.corpus 01 02 03 04 05Size inMb 0.7 3.1 5.3 6.7 8.8# ofwords 114.373 506.259 864.790 1.092.723 1.435.930# ofngrams13 4.917.781 21.768.879 37.185.712 46.986.831 61.744.732Table 4: Sub-corporaFor each sub-corpus we have calculated the executiontime of different stages of the process: (1) the tokeniza-tion that transforms the corpus into a set of integers; (2)the preparation of the mask structure and the construc-tion of the suffix-array data structure; (3) the sorting ofthe suffix-array data structure and the creation of theMatrix; (4) the calculation of the ME.
The results aregiven in Table 5.13 The window context of the experiment is F=3.2.F+1maskmaskmasksubmasksidmask deltaMEmaskPosN..................posM?corpus 01 02 03 04 05Tokeniz.
0:00:01 0:00:04 0:00:08 0:00:09 0:00:17Masks/Suffix 0:00:04 0:00:14 0:00:25 0:00:31 0:00:40Matrix 0:00:35 0:03:23 0:06:16 0:08:11 0:11:12ME 0:00:00 0:00:03 0:00:06 0:00:08 0:00:10total 0:00:40 0:03:44 0:06:55 0:08:59 0:12:19Table 5: Execution Time in (hh:mm:ss)The results clearly show that the construction of theMatrix and the sort operation over the suffix-array datastructure are the most time consuming procedures.
Onthe contrary, the computation of the Mutual Expectationis quick due to the direct access to sub-ngrams frequen-cies enabled by the Matrix.
In order to understand theevolution of the results, we present, in Figure 12, agraphical representation of the results.Pentium III, 900 MHz, 390 MB0:00:000:01:260:02:530:04:190:05:460:07:120:08:380:10:050:11:310:12:58114373 506259 864790 1092723 1435930# of words in the corpusExecutionTime(hh:mm:ss)Figure 12: Evolution of execution timeThe graphical representation illustrates a linear timecomplexity.
In fact, Alexandre Gil (2002) has provedthat, mainly due to the implementation of the MultikeyQuicksort algorithm, our implementation evidences atime complexity of O(h(F) N log N) where N is the sizeof the corpus and h(F) a function of the window con-text.6 ConclusionIn this paper, we have described an implementation tocompute positional ngram statistics based on masks,suffix array-based data structure and multidimensionalarrays.
Our C++ solution shows that it takes 8.59 min-utes to compute both frequency and Mutual Expectationfor a 1.092.723-word corpus on an Intel Pentium III 900MHz for a seven-word size window context.
In fact, ourarchitecture evidences O(h(F) N log N) time complex-ity.
To some extent, this work proposes a response tothe conclusion of (Kit and Wilks, 1998) that claims that?[?]
a utility for extracting discontinuous co-occurrences of corpus tokens, of any distance from eachother, can be implemented based on this program [TheVirtual Corpus Approach]?.ReferencesAlexandre Gil.
2002.
Extrac?
?o eficiente de padr?es textuaisutilizando algoritmos e estruturas de dados avan?adas.Master Thesis, New University of Lisbon, Portugal.Arne Anderson and Stefan Nilsson.
1998.
ImplementingRadixsort.
ACM Journal of Experimental Algorithmics,Vol.
3. citeseer.nj.nec.com/79696.htmlChunyu Kit and Yorick Wilks.
1998.
The Virtual Approach toDeriving Ngram Statistics from Large Scale Corpora.
In-ternational Conference on Chinese Information ProcessingConference, Beijing, China, 223-229. cite-seer.nj.nec.com/kit98virtual.html.Ga?l Dias, Sylvie Guillor?, and Jos?
Lopes.
1999.
LanguageIndependent Automatic Acquisition of Rigid MultiwordUnits from Unrestricted Text corpora.
Traitement Automa-tique des Langues Naturelles, Institut d?Etudes Scientifi-ques, Carg?se, France, 333-339.www.di.ubi.pt/~ddg/publications/taln1999.ps.gzGa?l Dias 2002.
Extraction Automatique d?Associations Lexi-cales ?
partir de Corpora.
PhD Thesis.
New University ofLisbon (Portugal) and University of Orl?ans (France).www.di.ubi.pt/~ddg/publications/thesis.pdf.gzJohn Sinclair.
1974.
English Lexical Collocations: A study incomputational linguistics.
Singapore, reprinted as chapter 2of Foley, J.
A.
(ed).
1996, John Sinclair on Lexis and Lexi-cography, Uni Press.Jon Bentley and Robert Sedgewick.
1997.
Fast Algorithms forSorting and Searching Strings.
8th Annual ACM-SIAMSymposium on Discrete Algorithms, New Orl?ans.
cite-seer.nj.nec.com/bentley97fast.html.Jon Bentley and Douglas McIlroy.
1993.
Engineering a sortfunction.
Software - Practice and Experience, 23(11):1249-1265.Mikio Yamamoto and Kenneth Church.
2000.
Using SuffixArrays to Compute Term Frequency and Document Fre-quency for All Substrings in a corpus.
Association forComputational Linguistics, 27(1):1-30.www.research.att.com/~kwc/CL_suffix_array.pdfSabine Deligne and Fr?d?ric Bimbot.
1995.
Language Model-ling by Variable Length Sequences: Theoretical Formula-tion and Evaluation of Multigrams.
ICASSP-95.
Detroit,Michigan, 1:169-172. cite-seer.nj.nec.com/deligne95language.htmlT.
Kuhn, H. Nieman, E.G.
Schukat-Talamazzini.
1994.
Er-godic Hidden Markov Models and Polygrams for LanguageModelling.
ICASSP-94, 1:357-360. cite-seer.nj.nec.com/kuhn94ergodic.htmlUdi Manber and Gene Myers.
1990.
Suffix-arrays: A newmethod for on-line string searches.
First Annual ACM-SIAM Symposium on Discrete Algorithms.
319-327.www.cs.arizona.edu/people/udi/suffix.ps
