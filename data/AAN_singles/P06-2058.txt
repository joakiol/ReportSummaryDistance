Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 444?451,Sydney, July 2006. c?2006 Association for Computational LinguisticsObfuscating Document Stylometry to Preserve Author AnonymityGary Kacmarcik         Michael GamonNatural Language Processing GroupMicrosoft ResearchRedmond, WA  USA{garykac,mgamon}@microsoft.comAbstractThis paper explores techniques for reduc-ing the effectiveness of standard author-ship attribution techniques so that an au-thor A can preserve anonymity for a par-ticular document D. We discuss featureselection and adjustment and show howthis information can be fed back to theauthor to create a new document D?
forwhich the calculated attribution movesaway from A.
Since it can be labor inten-sive to adjust the document in this fash-ion, we attempt to quantify the amount ofeffort required to produce the ano-nymized document and introduce twolevels of anonymization: shallow anddeep.
In our test set, we show that shal-low anonymization can be achieved bymaking 14 changes per 1000 words toreduce the likelihood of identifying A asthe author by an average of more than83%.
For deep anonymization, we adaptthe unmasking work of Koppel andSchler to provide feedback that allowsthe author to choose the level of ano-nymization.1 IntroductionAuthorship identification has been a long stand-ing topic in the field of stylometry, the analysisof literary style (Holmes 1998).
Issues of style,genre, and authorship are an interesting sub-areaof text categorization.
In authorship detection itis not the topic of a text but rather the stylisticproperties that are of interest.
The writing styleof a particular author can be identified by analyz-ing the form of the writing, rather than the con-tent.
The analysis of style therefore needs to ab-stract away from the content and focus on thecontent-independent form of the linguistic ex-pressions in a text.Advances in authorship attribution have raisedconcerns about whether or not authors can trulymaintain their anonymity (Rao and Rohatgi2000).
While there are clearly many reasons forwanting to unmask an anonymous author, nota-bly law enforcement and historical scholarship,there are also many legitimate reasons for an au-thor to wish to remain anonymous, chief amongthem the desire to avoid retribution from an em-ployer or government agency.
Beyond the issueof personal privacy, the public good is oftenserved by whistle-blowers who expose wrong-doing in corporations and governments.
The lossof an expectation of privacy can result in a chill-ing effect where individuals are too afraid todraw attention to a problem, because they fearbeing discovered and punished for their actions.It is for this reason that we set out to investi-gate the feasibility of creating a tool to supportanonymizing a particular document, given theassumption that the author is willing to expend areasonable amount of effort in the process.
Moregenerally, we sought to investigate the sensitivityof current attribution techniques to manipulation.For our experiments, we chose a standard dataset, the Federalist Papers, since the variety ofpublished results allows us to simulate author-ship attribution ?attacks?
on the obfuscated docu-ment.
This is important since there is no clearconsensus as to which features should be usedfor authorship attribution.2 Document ObfuscationOur approach to document obfuscation is toidentify the features that a typical authorship at-tribution technique will use as markers and thenadjust the frequencies of these terms to renderthem less effective on the target document.444While it is obvious that one can affect the attri-bution result by adjusting feature values, wewere concerned with:?
How easy is it to identify and present therequired changes to the author??
How resilient are the current authorshipdetection techniques to obfuscation??
How much work is involved for the au-thor in the obfuscation process?The only related work that we are aware of is(Rao and Rohatgi 2000) who identify the prob-lem and suggest (somewhat facetiously, theyadmit) using a round-trip machine translation(MT) process (e.g., English ?
French ?
Eng-lish) to obscure any traces of the original au-thor?s style.
They note that the current quality ofMT would be problematic, but this approachmight serve as a useful starting point for some-one who wants to scramble the words a bit be-fore hand-correcting egregious errors (takingcare not to re-introduce their style).2.1 The Federalist PapersOne of the standard document sets used in au-thorship attribution is the Federalist Papers, acollection of 85 documents initially publishedanonymously, but now known to have been writ-ten by 3 authors: Alexander Hamilton, JohnMadison and John Jay.
Due to illness, Jay onlywrote 5 of the papers, and most of the remainingpapers are of established authorship (Hamilton =51; Madison = 14; and 3 of joint authorship be-tween Hamilton and Madison).
The 12 remainingpapers are disputed between Hamilton and Madi-son.
In this work we limit ourselves to the 65known single-author papers and the 12 disputedpapers.While we refer to these 12 test documents as?disputed?, it is generally agreed (since the workof Mosteller and Wallace (1964)) that all of thedisputed papers were authored by Madison.
Inour model, we accept that Madison is the authorof these papers and adopt the fiction that he isinterested in obscuring his role in their creation.2.2 Problem StatementA more formal problem statement is as follows:We assume that an author A (in our case, Madi-son) has created a document D that needs to beanonymized.
The author self-selects a set K of Nauthors (where A ?
K) that some future agent(the ?attacker?
following the convention used incryptography) will attempt to select between.The goal is to use authorship attribution tech-niques to create a new document D?
based on Dbut with features that identify A as the authorsuppressed.3 Document PreparationBefore we can begin with the process of obfus-cating the author style in D, we need to gather atraining corpus and normalize all of the docu-ments.3.1 Training CorpusWhile the training corpus for our example istrivially obtained, authors wishing to anonymizetheir documents would need to gather their owncorpus specific for their use.The first step is to identify the set of authors K(including A) that could have possibly written thedocument.
This can be a set of co-workers or aset of authors who have published on the topic.Once the authors have been selected, a suitablecorpus for each author needs to be gathered.
Thiscan be emails or newsgroup postings or otherdocuments.
In our experiments, we did not in-clude D in the corpus for A, although it does notseem unreasonable to do so.For our example of the Federalist Papers, K isknown to be {Hamilton, Madison} and it is al-ready neatly divided into separate documents ofcomparable length.3.2 Document CleanupTraditional authorship attribution techniques relyprimarily on associating idiosyncratic formatting,language usage and spelling (misspellings, typos,or region-specific spelling) with each author inthe study.
Rao and Rohatgi (2000) and Koppeland Schler (2003) both report that these wordsserve as powerful discriminators for author attri-bution.
Thus, an important part of any obfusca-tion effort is to identify these idiosyncratic usagepatterns and normalize them in the text.Koppel and Schler (2003) also note that manyof these patterns can be identified using the basicspelling and grammar checking tools available inmost word processing applications.
Correctingthe issues identified by these tools is an easy firststep in ensuring the document conforms to con-ventional norms.
This is especially important forwork that will not be reviewed or edited sincethese idiosyncrasies are more likely to go unno-ticed.445However, there are distinctive usage patternsthat are not simple grammar or spelling errorsthat also need to be identified.
A well-knownexample of this is the usage of while/whilst bythe authors of the Federalist Papers.Hamilton Madison Disputedwhile 36 0 0whilst 1 12 9Table 1  : Occurrence counts of ?while?
and ?whilst?in the Federalist Papers (excluding documents au-thored by Jay and those which were jointly authored).In the disputed papers, ?whilst?
occurs in 6 ofthe documents (9 times total) and ?while?
occursin none.
To properly anonymize the disputeddocuments, ?whilst?
would need to be eliminatedor normalized.This is similar to the problem with idiosyn-cratic spelling in that there are two ways to applythis information.
The first is to simply correct theterm to conform to the norms as defined by theauthors in K. The second approach is to incorpo-rate characteristic forms associated with a par-ticular author.
While both approaches can serveto reduce the author?s stylometric fingerprint, thelatter approach carries the risk of attempted styleforgery and if applied indiscriminately may alsoprovide clues that the document has been ano-nymized (if strong characteristics of multipleauthors can be detected).For our experiments, we opted to leave thesemarkers in place to see how they were handledby the system.
We did, however, need to normal-ize the paragraph formatting, remove all capitali-zation and convert all footnote references to usesquare brackets (which are otherwise unused inthe corpus).3.3 TokenizationTo tokenize the documents, we separated se-quences of letters using spaces, newlines and thefollowing punctuation marks: .,()-:;`'?![].
Nostemming or morphological analysis was per-formed.
This process resulted in 8674 uniquetokens for the 65 documents in the training set.4 Feature SelectionThe process of feature selection is one of themost crucial aspects of authorship attribution.
Byfar the most common approach is to make use ofthe frequencies of common function words thatare content neutral, but practitioners have alsomade use of other features such as letter metrics(e.g., bi-grams), word and sentence length met-rics, word tags and parser rewrite rules.
For thiswork, we opted to limit our study to word fre-quencies since these features are generally ac-knowledged to be effective for authorship attri-bution and are transparent, which allows the au-thor to easily incorporate the information fordocument modification purposes.We wanted to avoid depending on an initiallist of candidate features since there is no guaran-tee that the attackers will limit themselves to anyof the commonly used lists.
Avoiding these listsmakes this work more readily useful for non-English texts (although morphology or stemmingmay be required).We desire two things from our feature selec-tion process beyond the actual features.
First, weneed a ranking of the features so that the authorcan focus efforts on the most important features.The second requirement is that we need a thresh-old value so that the author knows how much thefeature frequency needs to be adjusted.To rank and threshold the features, we useddecision trees (DTs) and made use of the readilyavailable WinMine toolkit (Chickering 2002).DTs produced by WinMine for continuously val-ued features such as frequencies are useful sinceeach node in the tree provides the requiredthreshold value.
For term-ranking, we created aDecision Tree Root (DTR) ranking metric to or-der the terms based on how discriminating theyare.
DTR Rank is computed by creating a seriesof DTs where we remove the root feature, i.e.
themost discriminating feature, before creating thenext DT.
In this fashion we create a rankingbased on the order in which the DT algorithmdetermined that the term was most discrimina-tory.
The DTR ranking algorithm is as follows:1) Start with a set of features2) Build DT and record root feature3) Remove root feature from list of features4) Repeat from step 2It is worth noting that the entire DT need notbe calculated since only the root is of interest.The off-the-shelf DT toolkit could be replacedwith a custom implementation1 that returned onlythe root (also known as a decision stump).
Since1Many DT learners are information-gain based, butthe WinMine toolkit uses a Bayesian scoring criteriondescribed in Chickering et al (1997) with normal-Wishart parameter priors used for continuously val-ued features.446our work is exploratory, we did not pursue op-timizations along these lines.For our first set of experiments, we appliedDTR ranking starting with all of the features(8674 tokens from the training set) and repeateduntil the DT was unable to create a tree that per-formed better than the baseline of p(Hamilton) =78.46%.
In this fashion, we obtained an orderedlist of 2477 terms, the top 10 of which are shownin Table 2, along with the threshold and bias.The threshold value is read directly from the DTroot node and the bias (which indicates whetherwe desire the feature value to be above or belowthe threshold) is determined by selecting thebranch of the DT which has the highest ratio ofnon-A to A documents.Initially, this list looks promising, especiallysince known discriminating words like ?upon?and ?whilst?
are the top two ranked terms.
How-ever, when we applied the changes to our base-line attribution model (described in detail in theEvaluation section), we discovered that while itperformed well on some test documents, otherswere left relatively unscathed.
This is shown inFigure 1 which graphs the confidence in assign-ing the authorship to Madison for each disputeddocument as each feature is adjusted.
We expectthe confidence to start high on the left side andmove downward as more features are adjusted.After adjusting all of the identified features, halfof the documents were still assigned to Madison(i.e., confidence > 0.50).Choosing just the high-frequency terms wasalso problematic since most of them were notconsidered to be discriminating by DTR ranking(see Table 3).
The lack of DTR rank not onlymeans that these are poor discriminators, but italso means that we do not have a threshold valueto drive the feature adjustment process.Token DTR Frequency Token DTR Frequencythe,ofto.andinabethat-595-39-185119515--0.0942270.0689370.0633790.0384040.0279770.0254080.0238380.0214460.0201390.014823itiswhichasby;thiswouldhaveor----5857575477--0.0134040.0118730.0109330.0088110.0086140.0077730.0077010.0071490.0068730.006459Table 3  : Top 20 terms sorted by frequency.We next combined the DTR and the term fre-quency approaches by computing DTR one theset of features whose frequency exceeds a speci-fied threshold for any one of the authors.
Select-ing a frequency of 0.001 produces a list of 35terms, the first 14 of which are shown in Table 4.Token Frequency Threshold ?
49upononpowerstheretomen;bylessinatthoseandany0.0025030.0044290.0014850.0027070.0384040.0011760.0077730.0086140.0011760.0238380.0029900.0026150.0254080.002930> 0.003111< 0.004312< 0.002012< 0.002911> 0.039071> 0.001531< 0.007644< 0.008110< 0.001384> 0.023574> 0.003083> 0.002742< 0.025207> 0.003005+6-90+3+7+10-2-1+60+4-1+2Table 4  : Top 14 DTR(0.001) ranked items.
The lastcolumn is the number of changes required to achievethe threshold frequency for document #49.Results for this list were much more promisingand are shown in Figure 2.
The confidence ofattributing authorship to Madison is reduced byan average of 84.42% (?
= 12.51%) and all of thedocuments are now correctly misclassified asbeing written by Hamilton.Token DTR Threshold Occurrence #49uponwhilstonpowerstherefewkindconsequentlywishedalthough12345678910> 0.003111< 0.000516< 0.004312< 0.002012> 0.002911< 0.000699> 0.001001< 0.000513> 0.000434< 0.0004700 ?
61 ?
016 ?
72 ?
22 ?
51 ?
20 ?
21 ?
01 ?
00 ?
0Table 2  : Top 10 DTR Rank ordered terms with thresholdand corresponding occurrence count (original document ?obfuscated version) for one of the disputed documents(#49).0.000.250.500.751.00uponwhilst onpowersthere few kindconsequentlywishedalthoughFigure 1 : Confidence in assigning disputed papers toMadison graphed as each feature is adjusted.
Each line cor-responds to one of the 12 disputed documents.
Features areordered by DTR Rank and the attribution model is SVM30.Values above 0.5 are assigned to Madison and those below0.5 are assigned to Hamilton.4470.000.250.500.751.00upon onpowersthere tomen by ; less in atthose andanyFigure 2 : Confidence in assigning disputed papers toMadison graphed as each feature is adjusted.
Feature orderis DTR(0.001) and the attribution model is SVM30.5 EvaluationEvaluating the effectiveness of any authorshipobfuscation approach is made difficult by thefact that it is crucially dependent on the author-ship detection method that is being utilized.
Anadvantage of using the Federalist Papers as thetest data set is that there are numerous papersdocumenting various methods that researchershave used to identify the authors of the disputedpapers.However, because of differences in the exactdata set2 and machine learning algorithm used, itis not reasonable to create an exact and completeimplementation of each system.
For our experi-ments, we used only the standard Federalist Pa-pers documents and tested each feature set usinglinear-kernel SVMs, which have been shown tobe effective in text categorization (Joachims1998).
To train our SVMs we used a sequentialminimal optimization (SMO) implementationdescribed in (Platt 1999).The SVM feature sets that we used for theevaluation are summarized in Table 5.For the early experiments described in theprevious section we used SVM30, which incor-porates the final set of 30 terms that Mosteller &Wallace used for their study.
As noted earlier,they made use of a different data set than we did,so we did expect to see some differences in theresults.
The baseline model (plotted as the left-most column of points in Figure 1 and Figure 2)assigned all of the disputed papers to Madisonexcept one3.2Mosteller & Wallace and some others augmented theFederalist Papers with additional document samples(5 Hamilton and 36 Madison), but this has not beendone universally by all researchers.3Document #55.
However, this is not inconsistentwith Mosteller &Wallace?s results: ?Madison is ex-tremely likely [?]
to have written all the disputedSVM70 (Mosteller & Wallace1964)70 common functionwords.4SVM30 (Mosteller & Wallace1964)Final 30 terms.5SVM11 (Tweedie, Singh &Holmes 1996)on, upon, there, any,an, every, his, from,may, can, doSVM08 (Holmes & Forsyth1995)upon, both, on, there,whilst, kind, by,consequentlySVM03 (Bosch & Smith 1998) upon, our, areTable 5  : Summary of feature words used in other Federal-ist Papers studies.5.1 Feature ModificationRather than applying the suggested modificationsto the original documents and regenerating thedocument feature vectors from scratch each time,we simplified the evaluation process by adjustingthe feature vector directly and ignoring the im-pact of the edits on the overall document prob-abilities.
The combination of insertions and dele-tions results in the total number of words in thedocument being increased by an average of 19.58words (?
= 7.79), which is less than 0.5% of thedocument size.
We considered this value to besmall enough that we could safely ignore its im-pact.Modifying the feature vector directly also al-lows us to consider each feature in isolation,without concern for how they might interact witheach other (e.g.
converting whilst?while or re-writing an entire sentence).
It also allows us toavoid the problem of introducing rewrites intothe document with our distinctive stylometricsignature instead of a hypothetical Madison re-write.5.2 ExperimentsWe built SVMs for each feature set listed inTable 5 and applied the obfuscation techniquedescribed above by adjusting the values in thefeature vector by increments of the single-wordprobability for each document.
The results thatwe obtained were the same as observed with ourtest model ?
all of the models were coerced toprefer Hamilton for each of the disputed docu-ments.Federalists [?]
with the possible exception of No.
55.For No.
55 our evidence is relatively weak [?].?
(Mosteller & Wallace 1964) p.263.4ibid p.38.5ibid p.66.4480.000.250.500.751.00upon onpowersthere tomen by ; less in atthose andanyFigure 3 : Confidence in assigning disputed papers toMadison graphed as each feature is adjusted.
Feature orderis DTR(0.001) and the attribution model is SVM70.Figure 3 shows the graph for SVM70, themodel that was most resilient to our obfuscationtechniques.
The results for all models are sum-marized in Table 6.
The overall reductionachieved across all models is 86.86%.% Reduction ?SVM70 74.66% 12.97%SVM30 84.42% 12.51%SVM11 82.65% 10.99%SVM08 93.54% 4.44%SVM03 99.01% 0.74%Table 6  : Percent reduction in the confidenceof assigning the disputed papers to Madisonfor each of the tested feature sets.Of particular note in the results are those forSVM03, which proved to be the most fragilemodel because of its low dimension.
If we con-sider this case an outlier and remove it fromstudy, our overall reduction becomes 83.82%.5.3 Feature ChangesAs stated earlier, an important aspect of any ob-fuscation approach is the number of changes re-quired to effect the mis-attribution.
Table 7summarizes the absolute number of changes(both insertions and deletions) and also expressesthis value related to the original document size.The average number of changes required per1000 words in the document is 14.2.
While it isdifficult to evaluate how much effort would berequired to make each of these individualchanges, this value seems to be within the rangethat a motivated person could reasonably under-take.More detailed summaries of the number offeature changes required for single document(#49) are given in Table 2 and Table 4.By calculating the overall number of changesrequired, we implicitly consider insertions anddeletions to be equally weighted.
However, whiledeletion sites in the document are easy to identify,Document Changes Doc Size Changes/100049 42 3849 10.950 46 2364 19.551 67 4039 16.652 52 3913 13.353 62 4592 13.554 53 4246 12.555 52 4310 12.156 59 3316 17.857 60 4610 13.058 54 4398 12.362 78 5048 15.563 91 6429 14.2Table 7  : Changes required per documentproposing insertion sites can be more problem-atic.
We do not address this difference in thispaper, although it is clear that more investigationis required in this area.6 Deep ObfuscationThe techniques described above result in whatwe term shallow obfuscation since they focus ona small number of features and are only useful asa defense against standard attribution attacks.More advanced attribution techniques, such asthat described in (Koppel and Schler 2004) lookdeeper into the author?s stylometric profile andcan identify documents that have been obfus-cated in this manner.Koppel and Schler introduce an approach theyterm ?unmasking?
which involves training a se-ries of SVM classifiers where the most stronglyweighted features are removed after each itera-tion.
Their hypothesis is that two texts from dif-ferent authors will result in a steady and rela-tively slow decline of classification accuracy asfeatures are being removed.
In contrast, two textsfrom the same author will produce a relativelyfast decline in accuracy.
According to the authors,a slow decline indicates deep and fundamentalstylistic differences in style - beyond the ?obvi-ous?
differences in the usage of a few frequentwords.
A fast decline indicates that there is anunderlying similarity once the impact of a fewsuperficial distinguishing markers has been re-moved.We repeated their experiments using 3-foldcross-validation to compare Hamilton and Madi-son with each other and the original (D) and ob-fuscated (D?)
documents.
The small number ofdocuments required that we train the SVM usingthe 50 most frequent words.
Using a larger poolof feature words resulted in unstable models, es-pecially when comparing Madison (14 docu-ments) with D and D?
(12 documents).
The re-sults of this comparison are shown in Figure 4.4490.30000.40000.50000.60000.70000.80000.90001.0000HvDHvD'HvMMvDMvD'Figure 4 : Unmasking the obfuscated document.
The y-axisplots the accuracy of a classifier trained to distinguish be-tween two authors; the x-axis plots each iteration of theunmasking process.
The top three lines compare Hamilton(H) versus Madison (M), the original document (D) and theobfuscated document (D?).
The bottom line is M vs. D andthe middle line is M vs. D?.In this graph, the comparison of Hamilton andthe modified document (MvD?)
exhibits thecharacteristic curve described by Koppel andSchler, which indicates that the original authorcan still be detected.
However, the curve hasbeen raised above the curve for the originaldocument which suggests that our approach doeshelp insulate against attacks that identify deepstylometric features.Modifying additional features continues thistrend and raises the curve further.
Figure 5 sum-marizes this difference by plotting the differencebetween the accuracy of the HvD?
and MvD?curves for documents at different levels of fea-ture modification.
An ideal curve in this graphwould be one that hugged the x-axis since thiswould indicate that it was as difficult to train aclassifier to distinguish between M and D?
as it isto distinguish between H and D?.
In this graph,the ?0?
curve corresponds to the original docu-ment, and the ?14?
curve to the modified docu-ment shown in Figure 4.
The ?35?
curve uses allof the DTR(0.001) features.This graph demonstrates that using DTR rank-ing to drive feature adjustment can producedocuments that are increasingly harder to detectas being written by the author.
While it is unsur-prising that a deep level of obfuscation is notachieved when only a minimal number of fea-tures are modified, this graph can be used tomeasure progress so that the author can deter-mine enough features have been modified toachieve the desired level of anonymization.Equally unsurprising is that this increased ano-nymization comes at an additional cost, summa-rized in Table 8.Num Features Changes/10007 9.914 14.221 18.328 22.535 25.1Table 8  : Relationship between numberof features modified and correspondingchanges required per 1000 words.While in this work we limited ourselves to the35 DTR(0.001) features, further document modi-fication can be driven by lowering the DTRprobability threshold to identify additional termsin an orderly fashion.7 ConclusionIn this paper, we have shown that the standardapproaches to authorship attribution can be con-founded by directing the author to selectivelyedit the test document.
We have proposed a tech-nique to automatically identify distinctive fea-tures and their frequency thresholds.
By using alist of features that are both frequent and highlyranked according to this automatic technique, theamount of effort required to achieve reasonableauthorship obfuscation seems to be well withinthe realm of a motivated author.
While we makeno claim that this is an easy task, and we makethe assumption that the author has undertakenbasic preventative measures (like spellcheckingand grammar checking), it does not seem to bean onerous task for a motivated individual.It not surprising that we can change the out-come by adjusting the values of features used inauthorship detection.
Our contribution, however,is that many of the important features can be de-termined by simultaneously considering term-frequency and DTR rank, and that this processresults in a set of features and threshold valuesthat are transparent and easy to control.-0.10000.00000.10000.20000.30000.40000.500001435Figure 5 : Overall impact of feature modification for dif-ferent levels of obfuscation.
The y-axis plots the accuracydelta between the HvD' and MvD' curves; the x-axis plotseach iteration of the unmasking process.
The legend indi-cates the number of features modified for each curve.450Given this result, it is not unreasonable to ex-pect that a tool could be created to provide feed-back to an author who desires to publish a docu-ment anonymously.
A sophisticated paraphrasetool could theoretically use the function wordchange information to suggest rewrites thatworked toward the desired term frequency in thedocument.For our experiments, we used a simplifiedmodel of the document rewrite process by evalu-ating the impact of each term modification inisolation.
However, modifying the document toincrease or decrease the frequency of a term willnecessarily impact the frequencies of other termsand thus affect the document's stylometric signa-ture.
Further experimentation is clearly needed inthis area needs to address the impact of this in-terdependency.One limitation to this approach is that it ap-plies primarily to authors that have a reasonably-sized corpus readily available (or easily created).However, for situations where a large corpus isnot available, automated authorship attributiontechniques are likely to be less effective (andthus obfuscation is less necessary) since thenumber of possible features can easily exceed thenumber of available documents.
An interestingexperiment would be to explore how this ap-proach applies to different types of corpora likeemail messages.We also recognize that these techniques couldbe used to attempt to imitate another author?sstyle.
We do not address this issue other than tosay that our thresholding approach is intended topush feature values just barely across the thresh-old away from A rather than to mimic any oneparticular author.Finally, in these results, there is a message forthose involved in authorship attribution: simpleSVMs and low-dimensional models (likeSVM03) may appear to work well, but are farless resilient to obfuscation attempts than Koppeland Schler?s unmasking approach.
Creating clas-sifiers with the minimum number of featuresproduces a model that is brittle and more suscep-tible to even simplistic obfuscation attempts.8 AcknowledgementsThanks are in order to the reviewers of earlierdrafts of this document, notably Chris Brockettand our anonymous reviewers.
In addition, MaxChickering provided useful information regard-ing his implementation of DTs in the WinMinetoolkit.ReferencesR.
A. Bosch and J.
A. Smith.
1998.
Separating Hy-perplanes and the Authorship of the Federalist Pa-pers.
American Mathematical Monthly, Vol.
105#7 pp.
601-608.D.
M. Chickering, D. Heckerman and C. Meek.
1997.A Bayesian Approach to Learning Bayesian Net-works with Local Structure.
In Proceedings of theThirteenth Conference on Uncertainty in ArtificialIntelligence (UAI97 Providence, RI), pp.
80-89.D.
M. Chickering.
2002.
The WinMine Toolkit.Technical Report MSR-TR-2002-103.D.
I. Holmes and R. S. Forsyth.
1995.
The FederalistRevisited: New Directions in Authorship Attribu-tion.
Literary and Linguistic Computing 10(2),pp.111-127.D.
I. Holmes.
1998.
The Evolution of Stylometry inHumanities Scholarship.
Literary and LinguisticComputing 13(3), pp.111-117.T.
Joachims.
1998.
Text Categorization with SupportVector Machines: Learning with many RelevantFeatures.
In Proceedings of the 10th EuropeanConference on Machine Learning, pp.137-142.M.
Koppel and J. Schler.
2003.
Exploiting StylisticIdiosyncrasies for Authorship Attribution.
In Pro-ceedings of IJCAI'03 Workshop on ComputationalApproaches to Style Analysis and Synthesis (Aca-pulco, Mexico).
pp.69-72.M.
Koppel and J. Schler, 2004.
Authorship Verifica-tion as a One-Class Classification Problem.
In Pro-ceedings of the Twenty-First International Confer-ence on Machine Learning (ICML 04 Banff, Al-berta, Canada), pp.489-495.F.
Mosteller and D. L. Wallace.
1964.
Inference andDisputed Authorship: The Federalist.
Addison-Wesley (Reading, Massachusetts, USA).J.
Platt.
1999.
Fast Training of SVMs Using Sequen-tial Minimal Optimization.
In B. Sch?lkopf, C.Burges and A. Smola (eds.)
Advances in KernelMethods: Support Vector Learning.
MIT Press(Cambridge, MA, USA), pp.185-208.J.
R. Rao and P. Rohatgi.
2000.
Can PseudonymityReally Guarantee Privacy?, In Proceedings of the9th USENIX Security Symposium (Denver, Colo-rado, USA), pp.85-96.F.
J. Tweedie, S. Singh and D. I. Holmes.
1996.
Neu-ral Network Applications in Stylometry: The Fed-eralist Papers.
In Computers and the Humanities30(1), pp.1-10.451
