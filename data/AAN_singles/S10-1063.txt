Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 284?291,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsTIPSem (English and Spanish):Evaluating CRFs and Semantic Roles in TempEval-2Hector Llorens, Estela Saquete, Borja NavarroUniversity of AlicanteAlicante, Spain{hllorens,stela,borja}@dlsi.ua.esAbstractThis paper presents TIPSem, a system toextract temporal information from naturallanguage texts for English and Spanish.TIPSem, learns CRF models from trainingdata.
Although the used features includedifferent language analysis levels, the ap-proach is focused on semantic informa-tion.
For Spanish, TIPSem achieved thebest F1 score in all the tasks.
For English,it obtained the best F1 in tasks B (events)and D (event-dct links); and was amongthe best systems in the rest.1 IntroductionThe automatic treatment of time expressions,events and their relations over natural languagetext consists of making temporal elements ex-plicit through a system that identifies and anno-tates them following a standard scheme.
This in-formation is crucial for other natural language pro-cessing (NLP) areas, such as summarization orquestion answering.
The relevance of temporal in-formation has been reflected in specialized confer-ences (Schilder et al, 2007) and evaluation forums(Verhagen et al, 2007).We present a system to tackle the six differenttasks related to multilingual temporal informationtreatment proposed in TempEval-2.
Particularly,in this evaluation exercise, TimeML (Pustejovskyet al, 2003) is adopted as temporal annotationscheme.
In this manner, the tasks require partic-ipating systems to automatically annotate differ-ent TimeML elements.
Firstly, task A consistsof determining the extent of time expressions asdefined by the TimeML TIMEX3 tag, as well asthe attributes ?type?
and ?value?.
Secondly, taskB addresses the recognition and classification ofevents as defined by TimeML EVENT tag.
Fi-nally, tasks C to F comprise the categorization ofdifferent temporal links (TimeML LINKs).
Figure1 illustrates the TimeML elements in a sentence.Figure 1: TimeML exampleIn the context of TempEval-2, we tackle alltasks for English and Spanish with a data-drivensystem.
This consists of CRF models inferredfrom lexical, syntactic and semantic informationof given training data.Our main approach, TIPSem (TemporalInformation Processing based on Semantic in-formation), is focused on semantic roles andsemantic networks.
Furthermore, we presenta secondary approach, TIPSem-B (TIPSem-Baseline), which contrary to the former does notconsider semantic information.The main objectives of this paper are (1) evalu-ating the performance of TIPSem comparing it toother participating systems and (2) measuring thecontribution of semantic information to differentTempEval-2 tasks though the comparison betweenour systems: TIPSem and TIPSem-B.This paper is structured as follows.
Our ap-proach to address the TempEval-2 tasks is moti-vated in Section 2 and described in Section 3.
Theresults obtained in the evaluation are shown andanalyzed in Section 4.
Finally, conclusions aredrawn in Section 5.2 Approach motivationThe next two subsections describe the two maincharacteristics of our approach, CRFs and seman-tic roles, and the reasons why we think they couldbe useful to tackle TimeML annotation.2842.1 CRF probabilistic modelConditional Random Fields is a popular and effi-cient ML technique for supervised sequence label-ing (Lafferty et al, 2001).
In the recognition prob-lem raised by TempEval-2 tasks A and B, assumeX is a random variable over data sequences to belabeled, and Y is a random variable over the corre-sponding label sequences, being all Y components(Yi) members of a finite label alphabet ?.
X mightrange over the sentences and Y range over possi-ble annotations of those sentences, with ?
the setof possible event IOB21 labels.
The following ex-ample illustrates the problem.
(1) X YThat ?
B-TIMEX3was ?
B-EVENTanother ?
?
= I-TIMEX3bad ?
I-EVENTweek ?
OThen, CRFs construct a conditional model frompaired observations and label sequences: p(Y |X).To extend the problem to classification, X is re-placed with elements to be classified and ?
is re-placed with the possible classes, for instance, intask A X = {TIMEX3 instances in text} and?
= {DATE, DURATION, SET, TIME}.From our point of view, CRFs are well suitedto address TempEval-2 tasks.
Firstly, TimeMLelements depend on structural properties of sen-tences.
Not only the word sequence, but mor-phological, syntactic and semantic structure is re-lated with them.
Secondly, some TIMEX3 andEVENT elements are denoted by sequences ofwords, therefore the CRFs are very appropriate.2.2 Semantic rolesSemantic role labeling (SRL) has achieved impor-tant results in the last years (Gildea and Jurafsky,2002; Moreda et al, 2007).
For each predicate in asentence, semantic roles identify all constituents,determining their arguments (agent, patient, etc.
)and their adjuncts (locative, temporal, etc.).
Fig-ure 2 illustrates a semantic role labeled sentence.Figure 2: Semantic roles exampleSemantic roles provide structural relations ofthe predicates in which TimeML elements may1IOB2 format: (B)egin, (I)nside, and (O)utsideparticipate.
Beyond syntactic relations expressedby means of the different types of phrases, seman-tic roles give further information about semanticrelations between the arguments of a predicate.Due to the fact that roles represent high level in-formation, they are more independent from wordtokens.
Hence, roles may aid in learning moregeneral models that could improve the results ofapproaches focused on lower level information.3 Our approach: TIPSemAs defined in previous section, this paper pro-poses CRF as learning method to infer models toface the TempEval-2 tasks.
Specifically, CRF++toolkit2 was used for training and testing our ap-proach.
The learning process was done usingthe parameters: CRF-L2 algorithm and hyper-parameter C=1.In order to set out the approach architecture andselect the features for learning, we divided thetasks proposed in the evaluation exercise into fourgroups: recognition, classification, normalizationand link-categorization.
Each group represents akind of problem to be resolved.
Recognition prob-lem is present in TIMEX3 and EVENT bounding(tasks A and B).
Classification problem appears inTIMEX3 type and EVENT class attributes (tasksA and B).
Normalization arises in TIMEX3 valueattribute (task A).
And link-categorization is ap-plied to different kind of link relations (tasks C toF).
Each group uses a particular feature set to learnan annotation model.
The features of these sets aregrouped in two subsets.
On the one hand, generalfeatures, which are widely used in different NLPfields and represent lower language analysis lev-els.
On the other hand, semantic features, whichare a novelty in the task and our main focus.TIPSem system uses all the features definedabove.
However, to measure the influence of se-mantic information in temporal information treat-ment, TIPSem-B system was implemented ex-cluding the semantic features.3.1 RecognitionIn recognition, the features are obtained at tokenlevel, that is to say, each token has its own set offeatures.Regarding each language analysis level, thegeneral features used to train our CRF model are:2http://crfpp.sourceforge.net/285?
Morphological: The lemma and part-of-speech (PoS) context, in a 5-window (-2,+2),was employed due to the good results itachieved in other NLP tasks.
Tokenization,PoS and lemmatization were obtained usingTreeTagger (Schmid, 1994) for English, andwere got from AnCora (Taule?
et al, 2008) forSpanish.?
Syntactic: Different TimeML elements arecontained in particular types of phrases.
Thisfeature tries to capture this fact by consider-ing phrase level syntactic information.
Thesyntactic tree was obtained using Charniakparser (Charniak and Johnson, 2005) for En-glish, and AnCora for Spanish.?
Polarity, tense and aspect: These were ob-tained using PoS and a set of handcraftedrules (e.g., will+verb ?
future).The semantic level features used to enhance thetraining framework of the CRF model are:?
Role: For each token, we considered therole regarding the verb the token depends on.To get semantic roles, CCG SRL tool (Pun-yakanok et al, 2004) was used for English,and AnCora for Spanish.?
Governing verb: The verb to which the cur-rent token holds a particular role.
This maydistinguish tokens appearing under the influ-ence of different verbs.?
Role+verb combination: The previous twofeatures were combined to capture the rela-tion between them.
This introduces addi-tional information by distinguishing roles de-pending on different verbs.
The importanceof this falls especially on the numbered roles(A0, A1, etc.)
meaning different things whendepending on different verbs.?
Role configuration: This feature is onlypresent in verb tokens heading a sentence orsub-sentence.
This consists of the set of rolesdepending on the verb.
This may be particu-larly useful for distinguish different sentencesettings.?
Lexical semantics: WordNet (Fellbaum,1998) top ontology classes have been widelyused to represent word meaning at ontologi-cal level, and demonstrated its worth in manytasks.
TIPSem uses the top four classesfor each word.
For Spanish, EuroWordNet(Vossen, 1998) was used.In this manner, given a list of tokens and its fea-tures, the trained recognition model will assign toeach token one of the valid labels.
For instance,in the case of TIMEX3 recognition: B-TIMEX3,I-TIMEX3 or O.3.2 ClassificationClassification features, used to get TIMEX3 typesand EVENT classes, are basically the same as theones used for recognition.
However, the maindifference is that the features are not obtainedat token level but at TIMEX3 or EVENT level.This implies that the word context is set to theextent of each element (TIMEX3 and EVENT),as well as all the features have as many valuesas tokens comprises the element (e.g., element-tokens=?next Monday?, PoS-feature=?JJ+NNP?
).Hence, following this description, the classifica-tion models will assign to each element one of thevalid classes.
For example, in the case of TIMEX3typing: DATE, DURATION, SET or TIME.3.3 NormalizationAs in classification the features are obtained atTIMEX3 level.
Furthermore, word-spelled num-bers contained in the TIMEX3 extent are trans-lated to their numerical value (e.g., ?three days??
?3 days?
).Normalization process consists of two mainsteps: (1) obtain the normalization type and (2)apply the corresponding normalization rules.The first step applies a CRF model that uses thesame features as the previous two plus TIMEX3pattern.
This new feature consists of the tokenscomprised by the TIMEX3 but replacing num-bers by NUM, temporal units, such as years ordays, by TUNIT, months by MONTH, and week-days by WEEKDAY.
In other words, ?next Mon-day?
would result in ?next WEEKDAY?
and ?June1999?
in ?MONTH NUM?.
Once the model istrained, for each new TIMEX3 it assigns a normal-ization type.
We define seven normalization types:Period, ISO, ISO set, ISO function, present ref,past ref and future ref.The second step uses as input the output of thefirst one.
Each normalization type has its own nor-malization rules.286?
Period: Apply rules to convert period-likeTIMEX3 (?3 days?)
into P NUM TUNITnormalized period (?P3D?).?
ISO: Apply rules to convert any-format ex-plicit date or time into a valid ISO 8601 stan-dard date.?
ISO set: Apply rules to get a valid ISO-like set from a TIMEX3 set (?monthly?
?XXXX-XX).?
ISO function: This is the most complextype.
The system applies different functionsto get a valid ISO date or time in a valid gran-ularity from DCT3 dates.
Here, time direc-tion indicators like ?next?
or ?previous?, aswell as verbal tenses are used.?
Present ref, past ref and future ref: theseare already normalized.3.4 Link-categorizationEach one of link-related tasks (C to F) has its ownlink-categorization features.
Nevertheless, all linktypes share some of them.?
Task C: For categorizing the relation be-tween an EVENT and a TIMEX3, the systemtakes into account the following features:?
Heading preposition if the event or theTIMEX3 are contained by a preposi-tional phrase as in ?before the meeting?,where ?meeting?
is the event and ?be-fore?
the heading preposition.?
Syntactic relation of the event and theTIMEX3 in the sentence.
This featuremay be evaluated as: same sentence,same subsentence or same phrase.?
Time position.
If the event is not di-rectly linked with the relation TIMEX3but related to another TIMEX3, the timeposition represents whether the eventis before, overlap or after the relationTIMEX3.?
Interval.
This feature is 0 unless thereappears some interval indicator tokennear the TIMEX3.
This is useful toidentify overlap-and-after and overlap-and-before categories.?
TIMEX3 type.3Date Creation Time?
Semantic roles if the event or theTIMEX3 are contained by a tempo-ral subordination (labeled with tempo-ral role), for example, in ?after he lefthome?, ?left?
is the event and ?after?
thesubordinating element (role feature).?
Task D: To determine the relationship be-tween an event and the DCT, TIPSem usesthe same features as in task C except in-terval.
In addition, all the features relatedto TIMEX3 are now related to the closerTIMEX3 (if exists) in the event sentence.
Inthis manner, the time position is calculated bycomparing DCT and that TIMEX3.?
Task E: Relations between two main eventsare categorized using only four features: thetense and aspect of the two events, the syn-tactic relation between them, and the time po-sition, calculated using the closer TIMEX3 toeach event.?
Task F: For categorizing subordinatedevents, TIPSem uses the subordinatingelement of temporal roles containing eachevent (if present), the heading preposition ofa prepositional phrases containing each event(if present), as well as the tense and aspect.To illustrate the system architecture, Figure 3summarizes the strategies that TIPSem followsto tackle the tasks proposed in the TempEval-2framework.Figure 3: TIPSem architecture2874 EvaluationThe test corpus consists of 17K words for Englishand 10K words for Spanish, in which approxi-mately a half part correspond to tasks A and B,and the other half to tasks C, D, E and F. The per-formance is measured using precision, recall andF?=1metrics.
A scoring script is provided.
Thiscounts correct instances at token level for tasks Aand B, and at temporal link level for the rest.Next subsections show the results obtained byTIPSem system in each one of the TempEval-2tasks for English (EN) and Spanish (ES).
More-over, a final subsection illustrates the F?=1resultsin three comparative graphs.
In tasks A and B, pre-cision, recall and F?=1are given.
In tasks C to E,links tasks precision, recall and F?=1are the samebecause our system does not consider the NONEvalue.
Hence, only F?=1is given.
Tasks E andF were not considered for Spanish in TempEval-2 evaluation and thus Spanish is excluded fromthose subsections.For each task, scores in which our system ob-tained the first place in the evaluation exercise arein bold.
Furthermore, in all cases the best scoreobtained by participating systems is reported.
Fi-nally, the influence of semantic information interms of improvement is indicated and analyzedthrough the comparison with TIPSem-B system,which exclude the features related with semantics.4.1 Task A: TIMEX3Table 1 shows the results obtained by our ap-proaches in TIMEX3 recognition, typing and ISO8601 normalization (value).System lang Prec.
Rec.
F?=1type valueTIPSem EN 0.92 0.80 0.85 0.92 0.65TIPSem ES 0.95 0.87 0.91 0.91 0.78TIPSem-B EN 0.88 0.60 0.71 0.88 0.59TIPSem-B ES 0.97 0.81 0.88 0.99 0.75Table 1: Task A - English and SpanishAs shown in results, TIPSem obtains the best re-sults for Spanish in all measures except for ?value?attribute, in which the best system obtained a 0.83.Another system obtained the same recall (0.87)but a lower precision (0.90), and thus a F?=1of(0.88) below TIPSem score (0.91).
For English,our main approach obtained the best precision.However, another system obtained the best recall(0.91).
The best F?=1was 0.86.
Regarding typeattribute, TIPSem obtained values closer to bestsystem (0.98).
Finally, in normalization, whichis the only attribute that is not annotated by apurely data-driven process, best system surpassedTIPSem in 0.20.These results indicate that CRFs represent anappropriate ML technique to learn models for an-notating TIMEX3.
Furthermore, they show thatnormalization process used by TIPSem could beimproved using other techniques.Specifically, the usage of semantic informationimproved the capability of learned models to gen-eralize rules.
For instance in time expressions, ifan unseen instance is contained by a temporal roleis a clear candidate to be a time expression.
Hence,they improve system recall (33% EN, 7% ES).4.2 Task B: EVENTTable 2 shows the results obtained by our ap-proaches in recognizing and classifying events.System lang Prec.
Recall F?=1classTIPSem EN 0.81 0.86 0.83 0.79TIPSem ES 0.90 0.86 0.88 0.66TIPSem-B EN 0.83 0.81 0.82 0.79TIPSem-B ES 0.92 0.85 0.88 0.66Table 2: Task B - English and SpanishIn this tasks, TIPSem obtained the best re-sults in TempEval-2 for Spanish and English inboth recognition and classification.
Although forEnglish another system achieved the best recall(0.88), it obtained a lower precision (0.55); andthus a 0.68 F?=1.
This indicates that our approachobtains the best F?=1(0.83) with a well-balancedprecision and recall.Again, the usage of semantic information im-proves the capability of learned models to gen-eralize, which improves the recall (6% EN, 1%ES).
For events, the improvement is lower than forTIMEX3 because, contrary to TIMEX3, they arenot clearly defined by specific roles.
In this case,features like role configuration, semantic classes,or role-governing verb are more useful.Other attributes present in events such as polar-ity, mood and tense obtained values of about 90%.However, to get the values for these attributes thesystem applies a set of handcrafted rules and thenthe results are not relevant for our approach.4.3 Task C: LINKS - Events and TIMEXsTable 3 shows the results obtained by our ap-proaches in categorizing EVENT-TIMEX3 links.288System lang F?=1TIPSem EN 0.55TIPSem ES 0.81TIPSem-B EN 0.54TIPSem-B ES 0.81Table 3: Task C - English and SpanishTIPSem was the only system participating inthis task for Spanish.
Nevertheless, 0.81 is a highscore comparing it to English best score (0.63).Our system, for English, is 8 points below topscored system.In this task, the application of semantic roles in-troduced an improvement of 2% in F?=1.4.4 Task D: LINKS - Events and DCTsTable 4 shows the results obtained by our ap-proaches in categorizing events with respect to thecreation time of a document.System lang F?=1TIPSem EN 0.82TIPSem ES 0.59TIPSem-B EN 0.81TIPSem-B ES 0.59Table 4: Task D - English and SpanishTask D is successfully covered by TIPSem ob-taining the best results in the evaluation.It seems that the relation of events with doc-ument creation time strongly depends on tenseand aspect, as well as the event position in timewith respect to DCT when defined by neighboringTIMEX3.Furthermore, the learned CRF models take ad-vantage of using temporal semantic roles informa-tion.
Specifically, the usefulness of semantic rolesin this task was quantified to 2%.4.5 Task E: LINKS - Main eventsTable 5 shows the results obtained by our ap-proaches in categorizing main events relations intext.System lang F?=1TIPSem EN 0.55TIPSem-B EN 0.55Table 5: Task E - EnglishIn this task, our system obtains the secondplace.
However, the top scored achieved a 0.56.Again, the tense and aspect features, as well asthe events position in time resulted useful to tacklethis task.
In this case, semantic roles informationis not used so TIPSem and TIPSem-B are equiva-lent.4.6 Task F: LINKS - Subordinated eventsTable 6 shows the results obtained by our ap-proaches in categorizing events relations with theevents they syntactically govern.System lang F?=1TIPSem EN 0.59TIPSem-B EN 0.60Table 6: Task F - EnglishCategorizing subordinated events TIPSem ob-tained the second place.
Best score was 0.66.
Inthis task, the application of roles did not help anddecreased the score in one point.
The cause maybe that for this task roles are not relevant but noisy.In this case, some extra information extending se-mantic roles is needed to turn them into a usefulfeature.4.7 Comparative graphsThis subsection presents the TIPSem F?=1re-sults in three graphs.
Figure 4 illustrates the re-sults for English indicating the higher and lowerscores achieved by TempEval-2 participating sys-tems.
Figure 5 shows the same for Spanish but,due to the fact that TIPSem was the only partici-pant in tasks B, C and D, the graph includes En-glish min.
and max.
scores as indirect assessment.Finally, Figure 6, compares the TIPSem results forEnglish and Spanish.Figure 4: English F?=1comparativeFigure 4 shows how TIPSem achieved, in gen-eral, a high performance for English.289Figure 5: Spanish F?=1indirect assessmentFor Spanish we can only report indirect assess-ment comparing the results to English scores.
Itcan be seen that the quality of the results is similarfor tasks A and B, but seems to be inverted in tasksC and D.Figure 6: TIPSem EN - ES F?=1comparativeFinally, in this graph comparing TIPSem re-sults, we observe that our approach achieved simi-lar performance for both languages in tasks A andB.
This indicates that for this tasks, the approach isvalid for both languages.
However, as in the previ-ous graph, it seems that for English TIPSem per-forms worse in task C and better in task D whilefor Spanish it does right the opposite.The train and test corpora were reviewed to an-alyze this fact.
On the one hand, the reason forthe high performance in task C for Spanish wasthe high amount of ?overlap?
instances in bothcorpora.
This trained the CRF model for catego-rizing event-timex links as ?overlap?
in most ofcases.
On the other hand, the cause of the Spanishlow performance in task D is ?vague?
links.
Thefeatures defined in TIPSem cannot distinguish be-tween ?overlap?
and ?vague?.
Due to the fact that?vague?
links are quite popular in Spanish test set,the results decreased.
This did not affect to En-glish results because of the spareness of ?vague?links.5 Conclusions and Further WorkThis paper presented a system for automaticallytreating temporal information of natural languagetexts as required in the TempEval-2 evaluation ex-ercise, in particular, following TimeML specifica-tions.Our system, TIPSem, is a data-driven approachand consists of different CRF models learned us-ing semantic information as main feature.
CRFswere used taking into account that data-driven ap-proaches have obtained good results in many NLPtasks, and due to their appropriateness in sequencelabeling problems and problems in which struc-tural properties are relevant, as those proposed inTempEval-2.
Furthermore, the models were en-hanced using semantic information.
Roles havebeen applied in other NLP fields with successfulresults, but never employed before for this pur-pose.
With these two main characteristics, we de-signed a complete learning environment selecting,in addition to roles, different language analysislevel properties as features to train the models.The results obtained for English and Spanishin the evaluation exercise were satisfactory andwell-balanced between precision and recall.
ForSpanish, TIPSem achieved the best F?=1in alltasks.
For English, it obtained the best F?=1inevent recognition and classification (task B), andevent and document creation time links catego-rization (task D).
Furthermore, in general, all theresults of TIPSem were very competitive and wereamong the top scored systems.
This verifies thatour approach is appropriate to address TempEval-2 tasks.Regarding multilinguality, the approach wasproven to be valid for different languages (Englishand Spanish).
This was also verified for Catalanlanguage by earlier versions of TIPSem (Llorenset al, 2009).
In fact, the data-driven part of thesystem could be considered language independentbecause it has been applied to different languagesand could be applied to other languages withoutadaptation, provided that there are tools availableto get the morphosyntactic and semantic informa-tion required by the approach.
It has to be high-290lighted that to apply TIPSem-B only morphosyn-tactic information is required.
Only the normaliza-tion of time expressions is a language dependentprocess in our system and requires the construc-tion of a set of rules for each target language.The contribution of semantic information totemporal information treatment was more signif-icant in recall and the improvement was concen-trated in tasks A and B (approx.
12% recall im-provement).
Although, TIPSem-B achieved lowerresults they are high enough to confirm that thatmost of temporal elements strongly depends onlexical and morphosyntactic information.The main errors and difficulties of our approachin this evaluation exercise are related to TIMEX3normalization (value attribute).
A pure ML ap-proach for solving this problem is not trivial, atleast, using our approach philosophy.
The treat-ment of normalization functions is an inherentlycomplex task and requires many training data to beautomatically learned.
This required us to includein the system some handcrafted rules to enable thesystem for this task.As further work we propose improving theTIMEX3 normalization by replacing handcraftednormalization rules with machine learned onesby combining statistic techniques and multilingualtemporal knowledge resources (ontologies).
Fur-thermore, link-categorization will be analyzed inmore detail in order to include more features to im-prove the models.
Finally, the suggested languageindependence of the approach will be tested usingTempEval-2 available data for other languages.AcknowledgmentsThis paper has been supported by the SpanishGovernment, projects TIN-2006-15265-C06-01, TIN-2009-13391-C04-01 and PROMETEO/2009/119, whereHector Llorens is funded under a FPI grant (BES-2007-16256).ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In 43rd Annual Meeting of the ACL.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database (Language, Speech, and Commu-nication).
MIT Press.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of the 18th ICML,pages 282?289.
Morgan Kaufmann.Hector Llorens, Borja Navarro, and Estela Saquete.2009.
Deteccio?n de Expresiones TemporalesTimeML en Catala?n mediante Roles Sema?nticos yRedes Sema?nticas.
In Procesamiento del LenguajeNatural (SEPLN), number 43, pages 13?21.Paloma Moreda, Borja Navarro, and Manuel Palomar.2007.
Corpus-based semantic role approach in in-formation retrieval.
Data Knowledge Engineering,61(3):467?483.Vasin Punyakanok, Dan Roth, W. Yih, D. Zimak, andY.
Tu.
2004.
Semantic role labeling via generalizedinference over classifiers.
In HLT-NAACL (CoNLL),pages 130?133.
ACL.James Pustejovsky, Jose?
M. Castan?o, Robert Ingria,Roser Saur?
?, Robert Gaizauskas, Andrea Setzer, andGraham Katz.
2003.
TimeML: Robust Specifica-tion of Event and Temporal Expressions in Text.
InIWCS-5.Frank Schilder, Graham Katz, and James Pustejovsky.2007.
Annotating, Extracting and Reasoning AboutTime and Events (Dagstuhl 2005), volume 4795 ofLNCS.
Springer.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing, pages 44?49.Mariona Taule?, M. Antonia Mart?
?, and Marta Recasens.2008.
AnCora: Multilevel Annotated Corpora forCatalan and Spanish.
In ELRA, editor, LREC, Mar-rakech, Morocco.Marc Verhagen, Robert Gaizauskas, Mark Hepple,Frank Schilder, Graham Katz, and James Puste-jovsky.
2007.
Semeval-2007 task 15: Tempevaltemporal relation identification.
In Proceedings ofthe 4th International Workshop on Semantic Evalu-ations, pages 75?80, Prague.
ACL.Piek Vossen.
1998.
EuroWordNet: a multilingualdatabase with lexical semantic networks.
KluwerAcademic Publishers, MA, USA.291
