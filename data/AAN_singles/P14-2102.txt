Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 625?630,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsStochastic Contextual Edit Distance and Probabilistic FSTsRyan Cotterell and Nanyun Peng and Jason EisnerDepartment of Computer Science, Johns Hopkins University{ryan.cotterell,npeng1,jason}@cs.jhu.eduAbstractString similarity is most often measuredby weighted or unweighted edit distanced(x, y).
Ristad and Yianilos (1998) de-fined stochastic edit distance?a probabil-ity distribution p(y | x) whose parame-ters can be trained from data.
We general-ize this so that the probability of choosingeach edit operation can depend on contex-tual features.
We show how to constructand train a probabilistic finite-state trans-ducer that computes our stochastic con-textual edit distance.
To illustrate the im-provement from conditioning on context,we model typos found in social media text.1 IntroductionMany problems in natural language processingcan be viewed as stochastically mapping onestring to another: e.g., transliteration, pronuncia-tion modeling, phonology, morphology, spellingcorrection, and text normalization.
Ristad andYianilos (1998) describe how to train the param-eters of a stochastic editing process that movesthrough the input string x from left to right, trans-forming it into the output string y.
In this paper wegeneralize this process so that the edit probabilitiesare conditioned on input and output context.We further show how to model the conditionaldistribution p(y | x) as a probabilistic finite-statetransducer (PFST), which can be easily combinedwith other transducers or grammars for particu-lar applications.
We contrast our probabilistictransducers with the more general framework ofweighted finite-state transducers (WFST), explain-ing why our restriction provides computational ad-vantages when reasoning about unknown strings.Constructing the finite-state transducer is tricky,so we give the explicit construction for use by oth-ers.
We describe how to train its parameters whenthe contextual edit probabilities are given by a log-linear model.
We provide a library for trainingboth PFSTs and WFSTs that works with OpenFST(Allauzen et al, 2007), and we illustrate its usewith simple experiments on typos, which demon-strate the benefit of context.2 Stochastic Contextual Edit DistanceOur goal is to define a family of probability distri-butions p?
(y | x), where x ?
?
?xand y ?
?
?yareinput and output strings over finite alphabets ?xand ?y, and ?
is a parameter vector.Let xidenote the ithcharacter of x.
If i < 1 ori > |x|, then xiis the distinguished symbol BOSor EOS (?beginning/end of string?).
Let xi:jdenotethe (j ?
i)-character substring xi+1xi+2?
?
?xj.Consider a stochastic edit process that reads in-put string x while writing output string y. Havingread the prefix x0:iand written the prefix y0:j, theprocess must stochastically choose one of the fol-lowing 2|?y|+ 1 edit operations:?
DELETE: Read xi+1but write nothing.?
INSERT(t) for some t ?
?y: Write yj+1= twithout reading anything.?
SUBST(t) for some t ?
?y: Read xi+1andwrite yj+1= t. Note that the traditionalCOPY operation is obtained as SUBST(xi+1).In the special case where xi+1= EOS, the choicesare instead INSERT(t) and HALT (where the lattermay be viewed as copying the EOS symbol).The probability of each edit operation dependson ?
and is conditioned on the left input contextC1= x(i?N1):i, the right input context C2=xi:(i+N2), and the left output context C3=y(j?N3):j, where the constants N1, N2, N3?
0specify the model?s context window sizes.1Notethat the probability cannot be conditioned on rightoutput context because those characters have notyet been chosen.
Ordinary stochastic edit dis-tance (Ristad and Yianilos, 1998) is simply thecase (N1, N2, N3) = (0, 1, 0), while Bouchard-C?ot?e et al (2007) used roughly (1, 2, 0).Now p?
(y | x) is the probability that this pro-cess will write y as it reads a given x.
This is thetotal probability (given x) of all latent edit oper-ation sequences that write y.
In general there areexponentially many such sequences, each imply-ing a different alignment of y to x.1IfN2= 0, so that we do not condition on xi+1, we muststill condition on whether xi+1= EOS (a single bit).
Wegloss over special handling for N2= 0; but it is in our code.625This model is reminiscent of conditional mod-els in MT that perform stepwise generation of onestring or structure from another?e.g., string align-ment models with contextual features (Cherry andLin, 2003; Liu et al, 2005; Dyer et al, 2013), ortree transducers (Knight and Graehl, 2005).3 Probabilistic FSTsWe will construct a probabilistic finite-statetransducer (PFST) that compactly models p?
(y |x) for all (x, y) pairs.2Then various computa-tions with this distribution can be reduced to stan-dard finite-state computations that efficiently em-ploy dynamic programming over the structure ofthe PFST, and the PFST can be easily combinedwith other finite-state distributions and functions(Mohri, 1997; Eisner, 2001).A PFST is a two-tape generalization of the well-known nondeterministic finite-state acceptor.
Itis a finite directed multigraph where each arc islabeled with an input in ?x?
{}, an output in?y?
{}, and a probability in [0, 1].
( is the emptystring.)
Each state (i.e., vertex) has a halt proba-bility in [0, 1], and there is a single initial state qI.Each path from qIto a final state qFhas?
an input string x, given by the concatenationof its arcs?
input labels;?
an output string y, given similarly;?
a probability, given by the product of its arcs?probabilities and the halt probability of qF.We define p(y | x) as the total probability of allpaths having input x and output y.
In our applica-tion, a PFST path corresponds to an edit sequencethat reads x and writes y.
The path?s probability isthe probability of that edit sequence given x.We must take care to ensure that for any x ?
?
?x,the total probability of all paths accepting x is 1,so that p?
(y | x) is truly a conditional probabilitydistribution.
This is guaranteed by the followingsufficient conditions (we omit the proof for space),which do not seem to appear in previous literature:?
For each state q and each symbol b ?
?x, thearcs from q with input label b or  must havetotal probability of 1.
(These are the availablechoices if the next input character is x.
)2Several authors have given recipes for finite-state trans-ducers that perform a single contextual edit operation (Kaplanand Kay, 1994; Mohri and Sproat, 1996; Gerdemann and vanNoord, 1999).
Such ?rewrite rules?
can be individually moreexpressive than our simple edit operations of section 2; but itis unclear how to train a cascade of them to model p(y | x).?
For each state q, the halt action and the arcsfrom q with input label  must have totalprobability of 1.
(These are the availablechoices if there is no next input character.)?
Every state q must be co-accessible, i.e., theremust be a path of probability > 0 from q tosome qF.
(Otherwise, the PFST could losesome probability mass to infinite paths.
Thecanonical case of this involves an loop q ?
qwith input label  and probability 1.
)We take the first two conditions to be part of thedefinition of a PFST.
The final condition requiresour PFST to be ?tight?
in the same sense as aPCFG (Chi and Geman, 1998), although the tight-ness conditions for a PCFG are more complex.In section 7, we discuss the costs and benefits ofPFSTs relative to other options.4 The Contextual Edit PFSTWe now define a PFST topology that conciselycaptures the contextual edit process of section 2.We are given the alphabets ?x,?yand the contextwindow sizes N1, N2, N3?
0.For each possible context triple C =(C1, C2, C3) as defined in section 2, we constructan edit state qCwhose outgoing arcs correspondto the possible edit operations in that context.One might expect that the SUBST(t) edit oper-ation that reads s = xi+1and writes t = yj+1would correspond to an arc with s, t as its inputand output labels.
However, we give a more effi-cient design where in the course of reaching qC,the PFST has already read s and indeed the en-tire right input context C2= xi:(i+N2).
So ourPFST?s input and output actions are ?out of sync?
:its read head is N2characters ahead of its writehead.
When the edit process of section 2 has readx0:iand written y0:j, our PFST implementationwill actually have read x0:(i+N2)and written y0:j.This design eliminates the need for nondeter-ministic guessing (of the right context xi:(i+N2)) todetermine the edit probability.
The PFST?s state isfully determined by the characters that it has readand written so far.
This makes left-to-right com-position in section 5 efficient.A fragment of our construction is illustrated inFigure 1.
An edit state qChas the following out-going edit arcs, each of which corresponds to anedit operation that replaces some s ?
?x?
{}with some t ?
?y?
{}:626b     cy     _a     bcz       _a      bx      _a     bax       _?:z/?:?/?:y/?
:x /insertzinsert xsubstituteyforbdeletebb      cx      _readcc:?/1p(INSERT(x) | (a,bc,x) )p(INSERT(z)|(a,bc,x))p(SUBST(y)|(a,bc,x))p(DELETE(b)|(a,bc,x))cFigure 1: A fragment of a PFST withN1= 1, N2= 2, N3=1.
Edit states are shaded.
A state qCis drawn with left andright input contexts C1, C2in the left and right upper quad-rants, and left output context C3in the left lower quadrant.Each arc is labeled with input:output / probability.?
A single arc with probability p(DELETE | C)(here s = (C2)1, t = )?
For each t ?
?y, an arc with probabilityp(INSERT(t) | C) (here s = )?
For each t ?
?y, an arc with probabilityp(SUBST(t) | C) (here s = (C2)1)Each edit arc is labeled with input  (because shas already been read) and output t. The arc leadsfrom qCto qC?, a state that moves s and t intothe left contexts: C?1= suffix(C1s,N1), C?2=suffix(C2, N2?
|s|), C?3= suffix(C3t,N3).Section 2 mentions that the end of x requiresspecial handling.
An edit state qCwhose C2=EOSN2only has outgoing INSERT(t) arcs, and hasa halt probability of p(HALT | C).
The halt proba-bility at all other states is 0.We must also build some non-edit states of theform qCwhere |C2| < N2.
Such a state does nothave the full N2characters of lookahead that areneeded to determine the conditional probability ofan edit.
Its outgoing arcs deterministically reada new character into the right input context.
Foreach s ?
?x, we have an arc of probability 1 fromqCto qC?where C?= (C1, C2s, C3), labeled withinput s and output .
Following such arcs from qCwill reach an edit state after N2?
|C2| steps.The initial state qIwith I = (BOSN1, , BOSN3)is a non-edit state.
Other non-edit states are con-structed only when they are reachable from an-other state.
In particular, a DELETE or SUBST arcalways transitions to a non-edit state, since it con-sumes one of the lookahead characters.5 Computational ComplexityWe summarize some useful facts without proof.For fixed alphabets ?xand ?y, our finalPFST, T , has O(|?x|N1+N2|?y|N3) states andO(|?x|N1+N2|?y|N3+1) arcs.
Composing this Twith deterministic FSAs takes time linear in thesize of the result, using a left-to-right, on-the-flyimplementation of the composition operator ?.Given strings x and y, we can compute p?
(y |x) as the total probability of all paths in x ?
T ?
y.This acyclic weighted FST has O(|x| ?
|y|) statesand arcs.
It takes onlyO(|x| ?
|y|) time to constructit and sum up its paths by dynamic programming,just as in other edit distance algorithms.Given only x, taking the output language ofx ?
T yields the full distribution p?
(y | x)as a cyclic PFSA with O(|x| ?
?N3y) states andO(|x| ?
?N3+1y) arcs.
Finding its most probablepath (i.e., most probable aligned y) takes timeO(|arcs| log |states|), while computing every arc?sexpected number of traversals under p(y | x) takestime O(|arcs| ?
|states|).3p?
(y | x) may be used as a noisy channelmodel.
Given a language model p(x) repre-sented as a PFSA X , X ?
T gives p(x, y) for allx, y.
In the case of an n-gram language modelwith n ?
N1+ N2, this composition is effi-cient: it merely reweights the arcs of T .
Weuse Bayes?
Theorem to reconstruct x from ob-served y: X ?
T ?
y gives p(x, y) (proportionalto p(x | y)) for each x.
This weighted FSA hasO(?N1+N2x?
|y|) states and arcs.6 Parameterization and TrainingWhile the parameters ?
could be trained via var-ious objective functions, it is particularly effi-cient to compute the gradient of conditional log-likelihood,?klog p?
(yk| xk), given a sampleof pairs (xk, yk).
This is a non-convex objectivefunction because of the latent x-to-y alignments:we do not observe which path transduced xkto yk.Recall from section 5 that these possible paths arerepresented by the small weighted FSA xk?T ?yk.Now, a path?s probability is defined by multiply-ing the contextual probabilities of edit operationse.
As suggested by Berg-Kirkpatrick et al (2010),we model these steps using a conditional log-linear model, p?
(e | C)def=1ZCexp(?
?~f(C, e)).3Speedups: In both runtimes, a factor of |x| can be elimi-nated from |states| by first decomposing x ?T into its O(|x|)strongly connected components.
And the |states| factor in thesecond runtime is unnecessary in practice, as just the first fewiterations of conjugate gradient are enough to achieve goodapproximate convergence when solving the sparse linear sys-tem that defines the forward probabilities in the cyclic PFSA.627To increase log p?
(yk| xk), we must raise theprobability of the edits e that were used to trans-duce xkto yk, relative to competing edits from thesame contexts C. This means raising ?
?
f(C, e)and/or lowering ZC.
Thus, log p?
(yk| xk) de-pends only on the probabilities of edit arcs in Tthat appear in xk?
T ?
yk, and the competing editarcs from the same edit states qC.The gradient?
?log p?
(yk| xk) takes the form?C,ec(C, e)[~f(C, e)??e?p?
(e?| C)~f(C, e?
)]where c(C, e) is the expected number of times thate was chosen in context C given (xk, yk).
(Thatcan be found by the forward-backward algorithmon xk?T ?
yk.)
So the gradient adds up the differ-ences between observed and expected feature vec-tors at contexts C, where contexts are weighted byhow many times they were likely encountered.In practice, it is efficient to hold the countsc(C, e) constant over several gradient steps, sincethis amortizes the work of computing them.
Thiscan be viewed as a generalized EM algorithm thatimputes the hidden paths (giving c) at the ?E?
stepand improves their probability at the ?M?
step.Algorithm 1 provides the training pseudocode.Algorithm 1 Training a PFST T?by EM.1: while not converged do2: reset al counts to 0 .
begin the ?E step?3: for k ?
1 to K do .
loop over training data4: M = xk?
T??
yk.
small acyclic WFST5: ~?
= FORWARD-ALGORITHM(M )6:~?
= BACKWARD-ALGORITHM(M )7: for arc A ?M , from state q ?
q?do8: if A was derived from an arc in T?representing edit e, from edit state qC, then9: c(C, e) += ?q?
prob(A) ?
?q?/?qI10: ?
?
L-BFGS(?, EVAL, max iters=5) .
the ?M step?11: function EVAL(?)
.
objective function & its gradient12: F ?
0;?F ?
013: for context C such that (?e)c(C, e) > 0 do14: count?
0; expected?
0; ZC?
015: for possible edits e in context C do16: F += c(C, e) ?
(?
?~f(C, e))17: ?F += c(C, e) ?~f(C, e)18: count += c(C, e)19: expected += exp(?
?~f(C, e)) ?~f(C, e)20: ZC+= exp(?
?~f(C, e))21: F -= count ?
logZC;?F -= count ?expected/ZC22: return (F,?F )7 PFSTs versus WFSTsOur PFST model of p(y | x) enforces a normal-ized probability distribution at each state.
Drop-ping this requirement gives a weighted FST(WFST), whose path weightsw(x, y) can be glob-ally normalized (divided by a constant Zx) to ob-tain probabilities p(y | x).
WFST models of con-textual edits were studied by Dreyer et al (2008).PFSTs and WFSTs are respectively related toMEMMs (McCallum et al, 2000) and CRFs (Laf-ferty et al, 2001).
They gain added power fromhidden states and  transitions (although to permita finite-state encoding, they condition on x in amore restricted way than MEMMs and CRFs).WFSTs are likely to beat PFSTs as linguisticmodels,4just as CRFs beat MEMMs (Klein andManning, 2002).
A WFST?s advantage is that theprobability of an edit can be indirectly affected bythe weights of other edits at a distance.
Also, onecould construct WFSTs where an edit?s weight di-rectly considers local right output context C4.So why are we interested in PFSTs?
Becausethey do not require computing a separate normal-izing contant Zxfor every x.
This makes it com-putationally tractable to use them in settings wherex is uncertain because it is unobserved, partiallyobserved (e.g., lacks syllable boundaries), or nois-ily observed.
E.g., at the end of section 5, X rep-resented an uncertain x.
So unlike WFSTs, PFSTsare usable as the conditional distributions in noisychannel models, channel cascades, and Bayesiannetworks.
In future we plan to measure their mod-eling disadvantage and attempt to mitigate it.PFSTs are also more efficient to train under con-ditional likelihood.
It is faster to compute the gra-dient (and fewer steps seem to be required in prac-tice), since we only have to raise the probabilitiesof arcs in xk?
T ?
ykrelative to competing arcsin xk?
T .
We visit at most |xk| ?
|yk| ?
|?y| arcs.By contrast, training a WFST must raise the prob-ability of the paths in xk?
T ?
ykrelative to theinfinitely many competing paths in xk?
T .
Thisrequires summing around cycles in xk?T , and re-quires visiting all of its |xk| ?
|?y|N3+1arcs.8 ExperimentsTo demonstrate the utility of contextual edit trans-ducers, we examine spelling errors in social me-dia data.
Models of spelling errors are useful ina variety of settings including spelling correctionitself and phylogenetic models of string variation4WFSTs can also use a simpler topology (Dreyer et al,2008) while retaining determinism, since edits can be scored?in retrospect?
after they have passed into the left context.628-8-7-6-5-42000 4000 6000# Training ExamplesMean Log-LikelihoodBackoffFALSETRUETopologyT010T020T110T111234562000 4000 6000# Training ExamplesMean ExpectedEdit DistanceBackoffFALSETRUETopologyT010T020T110T111Figure 2: (a) Mean log p(y | x) for held-out test examples.
(b) Mean expected edit distance (similarly).
(Mays et al, 1991; Church and Gale, 1991; Ku-kich, 1992; Andrews et al, 2014).To eliminate experimental confounds, we useno dictionary or language model as one would inpractice, but directly evaluate our ability to modelp(correct | misspelled).
Consider (xk, yk) =(feeel, feel).
Our model defines p(y | xk) for all y.Our training objective (section 6) tries to make thislarge for y = yk.
A contextual edit model learnshere that e 7?
 is more likely in the context of ee.We report on test data how much probabilitymass lands on the true yk.
We also report howmuch mass lands ?near?
yk, by measuring the ex-pected edit distance of the predicted y to the truth.Expected edit distance is defined as?yp?
(y |xk)d(y, yk) where d(y, yk) is the Levenshtein dis-tance between two strings.
It can be computed us-ing standard finite-state algorithms (Mohri, 2003).8.1 DataWe use an annotated corpus (Aramaki, 2010) of50000 misspelled words x from tweets along withtheir corrections y.
All examples have d(x, y) = 1though we do not exploit this fact.
We randomlyselected 6000 training pairs and 100 test pairs.
Weregularized the objective by adding ?
?||?||22, wherefor each training condition, we chose ?
by coarsegrid search to maximize the conditional likelihoodof 100 additional development pairs.8.2 Context Windows and Edit FeaturesWe considered four different settings for the con-text window sizes (N1, N2, N3): (0,1,0)=stochas-tic edit distance, (1,1,0), (0,2,0), and (1,1,1).Our log-linear edit model (section 6) includesa dedicated indicator feature for each contextualedit (C, e), allowing us to fit any conditional dis-tribution p(e | C).
In our ?backoff?
setting, each(C, e) also has 13 binary backoff features that itshares with other (C?, e?).
So we have a total of 14feature templates, which generate over a millionfeatures in our largest model.
The shared featureslet us learn that certain properties of a contextualedit tend to raise or lower its probability (and theregularizer encourages such generalization).Each contextual edit (C, e) can be character-ized as a 5-tuple (s, t, C1, C?2, C3): it replacess ?
?x?
{} with t ?
?y?
{} when s falls be-tween C1and C?2(so C2= sC?2) and t is precededby C3.
Then each of the 14 features of (C, e) in-dicates that a particular subset of this 5-tuple has aparticular value.
The subset alays includes s, t,or both.
It never includes C1or C?2without s, andnever includes C3without t.8.3 ResultsFigures 2a and 2b show the learning curves.
Wesee that both metrics improve with more trainingdata; with more context; and with backoff.
Withbackoff, all of the contextual edit models substan-tially beat ordinary stochastic edit distance, andtheir advantage grows with training size.9 ConclusionWe have presented a trainable, featurizable modelof contextual edit distance.
Our main contribu-tion is an efficient encoding of such a model asa tight PFST?that is, a WFST that is guaranteedto directly define conditional string probabilitieswithout need for further normalization.
We are re-leasing OpenFST-compatible code that can trainboth PFSTs and WFSTs (Cotterell and Renduch-intala, 2014).
We formally defined PFSTs, de-scribed their speed advantage at training time, andnoted that they are crucial in settings where the in-put string is unknown.
In future, we plan to deployour PFSTs in such settings.629ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
OpenFst: Ageneral and efficient weighted finite-state transducerlibrary.
In Implementation and Application of Au-tomata, pages 11?23.
Springer.Nicholas Andrews, Jason Eisner, and Mark Dredze.2014.
Robust entity clustering via phylogenetic in-ference.
In Proceedings of ACL.Eiji Aramaki.
2010.
Typo corpus.
Available at http://luululu.com/tweet/#cr, January.Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,John DeNero, and Dan Klein.
2010.
Painless un-supervised learning with features.
In Proceedings ofHLT-NAACL, pages 582?590.Alexandre Bouchard-C?ot?e, Percy Liang, Thomas L.Griffiths, and Dan Klein.
2007.
A probabilistic ap-proach to language change.
In NIPS.Colin Cherry and Dekang Lin.
2003.
A probabilitymodel to improve word alignment.
In Proceedingsof ACL, pages 88?95.Zhiyi Chi and Stuart Geman.
1998.
Estimationof probabilistic context-free grammars.
Computa-tional Linguistics, 24(2):299?305.Kenneth W. Church and William A. Gale.
1991.
Prob-ability scoring for spelling correction.
Statistics andComputing, 1(2):93?103.Ryan Cotterell and Adithya Renduchintala.
2014.brezel: A library for training FSTs.
Technical re-port, Johns Hopkins University.Markus Dreyer, Jason R. Smith, and Jason Eisner.2008.
Latent-variable modeling of string transduc-tions with finite-state methods.
In Proceedings ofEMNLP, EMNLP ?08, pages 1080?1089.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A simple, fast, and effective reparameteriza-tion of IBM Model 2.
In Proceedings of NAACL-HLT, pages 644?648.Jason Eisner.
2001.
Expectation semirings: FlexibleEM for learning finite-state transducers.
In Proceed-ings of the ESSLLI Workshop on Finite-State Meth-ods in NLP.Dale Gerdemann and Gertjan van Noord.
1999.
Trans-ducers from rewrite rules with backreferences.
InProceedings of EACL.Ronald M. Kaplan and Martin Kay.
1994.
Regu-lar models of phonological rule systems.
Compu-tational Linguistics, 20(3):331?378.Dan Klein and Christopher D. Manning.
2002.
Condi-tional structure versus conditional estimation in NLPmodels.
In Proceedings of EMNLP, pages 9?16.Kevin Knight and Jonathan Graehl.
2005.
Anoverview of probabilistic tree transducers for naturallanguage processing.
In Proc.
of the Sixth Interna-tional Conference on Intelligent Text Processing andComputational Linguistics (CICLing).Karen Kukich.
1992.
Techniques for automaticallycorrecting words in text.
ACM Computing Surveys(CSUR), 24(4):377?439.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the InternationalConference on Machine Learning.Yang Liu, Qun Liu, and Shouxun Lin.
2005.
Log-linear models for word alignment.
In Proceedingsof ACL, pages 459?466.Eric Mays, Fred J. Damerau, and Robert L. Mercer.1991.
Context based spelling correction.
Informa-tion Processing & Management, 27(5):517?522.Andrew McCallum, Dayne Freitag, and FernandoPereira.
2000.
Maximum entropy Markov mod-els for information extraction and segmentation.
InProceedings of ICML.Mehryar Mohri and Richard Sproat.
1996.
An efficientcompiler for weighted rewrite rules.
In Proceedingsof ACL, pages 231?238.Mehryar Mohri.
1997.
Finite-state transducers in lan-guage and speech processing.
Computational Lin-guistics, 23(2):269?311.Mehryar Mohri.
2003.
Edit-distance of weighted au-tomata: General definitions and algorithms.
Inter-national Journal of Foundations of Computer Sci-ence, 14(06):957?982.Eric Sven Ristad and Peter N. Yianilos.
1998.
Learn-ing string edit distance.
IEEE Transactions on Pat-tern Analysis and Machine Intelligence, 20(5):522?532.630
