Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 575?584,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsTwo Decades of Unsupervised POS induction: How far have we come?Christos ChristodoulopoulosSchool of InformaticsUniversity of Edinburghchristos.c@ed.ac.ukSharon GoldwaterSchool of InformaticsUniversity of Edinburghsgwater@inf.ed.ac.ukMark SteedmanSchool of InformaticsUniversity of Edinburghsteedman@inf.ed.ac.ukAbstractPart-of-speech (POS) induction is one of themost popular tasks in research on unsuper-vised NLP.
Many different methods have beenproposed, yet comparisons are difficult tomake since there is little consensus on eval-uation framework, and many papers evalu-ate against only one or two competitor sys-tems.
Here we evaluate seven different POSinduction systems spanning nearly 20 years ofwork, using a variety of measures.
We showthat some of the oldest (and simplest) systemsstand up surprisingly well against more recentapproaches.
Since most of these systems weredeveloped and tested using data from the WSJcorpus, we compare their generalization abil-ities by testing on both WSJ and the multi-lingual Multext-East corpus.
Finally, we in-troduce the idea of evaluating systems basedon their ability to produce cluster prototypesthat are useful as input to a prototype-drivenlearner.
In most cases, the prototype-drivenlearner outperforms the unsupervised systemused to initialize it, yielding state-of-the-artresults on WSJ and improvements on non-English corpora.1 IntroductionIn recent years, unsupervised learning has becomea hot area in NLP, in large part due to the use ofsophisticated machine learning approaches whichpromise to deliver better results than more tradi-tional methods.
Often the new approaches are testedusing part-of-speech (POS) tagging as an exampleapplication, and usually they are shown to performbetter than one or another comparison system.
How-ever, it is difficult to draw overall conclusions aboutthe relative performance of unsupervised POS tag-ging systems because of differences in evaluationmeasures, and the fact that no paper includes di-rect comparisons against more than a few other sys-tems.
In this paper, we attempt to remedy thatsituation by providing a comprehensive evaluationof seven different POS induction systems spanningnearly 20 years of research.
We focus specificallyon POS induction systems, where no prior knowl-edge is available, in contrast to POS disambigua-tion systems (Merialdo, 1994; Toutanova and John-son, 2007; Naseem et al, 2009; Ravi and Knight,2009; Smith and Eisner, 2005), which use a dic-tionary to provide possible tags for some or all ofthe words in the corpus, or prototype-driven sys-tems (Haghighi and Klein, 2006), which use a smallset of prototypes for each tag class, but no dictio-nary.
Our motivation stems from another part of ourown research, in which we are trying to use NLPsystems on over 50 low-density languages (some ofthem dead) where both tagged corpora and languagespeakers are mostly unavailable.
We therefore de-sire to use these systems straight out of the box andto know how well we can expect them to work.One difficulty in evaluating POS induction sys-tems is that there is no straightforward way to mapthe clusters found by the algorithm onto the goldstandard tags; moreover, some systems are designedto induce the number of clusters as well as theircontents, so the number of found clusters may notmatch either the gold standard or that of another sys-tem.
Nevertheless, most recent papers have usedmapping-based performance measures (either one-to-one or many-to-one accuracy).
Here, we arguethat the entropy-based V-Measure (Rosenberg and575Hirschberg, 2007) is more useful in many cases, be-ing more stable across different numbers of foundand true clusters, and avoiding several of the prob-lems with another commonly used entropy-basedmeasure, Variation of Information (Meila?, 2003).Using V-Measure along with several other evalu-ation measures, we compare the performance of thedifferent induction systems on bothWSJ (the data onwhich most systems were developed and tested) andMultext East, a corpus of parallel texts in eight dif-ferent languages.
We find that for virtually all mea-sures and datasets, older systems using relativelysimple models and algorithms (Brown et al, 1992;Clark, 2003) work as well or better than systemsusing newer and often far more sophisticated andtime-consuming machine learning methods (Gold-water and Griffiths, 2007; Johnson, 2007; Graca etal., 2009; Berg-Kirkpatrick et al, 2010).
Thus, al-though these newer methods have introduced po-tentially useful machine learning techniques, theyshould not be assumed to provide the best perfor-mance for unsupervised POS induction.In addition to our review and comparison, we in-troduce a new way to both evaluate and potentiallyimprove a POS induction system.
Our method isbased on the prototype-driven learning system ofHaghighi and Klein (2006), which achieves verygood performance by using a hand-selected list ofprototypes for each syntactic cluster.
We instead usethe existing POS induction systems to induce proto-types automatically, and evaluate the systems basedon the quality of their prototypes.
We find that theoldest system tested (Brown et al, 1992) producesthe best prototypes, and that using these prototypesas input to Haghighi and Klein?s system yields state-of-the-art performance on WSJ and improvementson seven of the eight non-English corpora.2 POS Induction SystemsWe describe each system only briefly; for details,see the respective papers, cited below.
Each systemoutputs a set of syntactic clusters C; except wherenoted, the target number of clusters |C| must bespecified as an input parameter.
Since we are in-terested in out-of-the-box performance, we use thedefault parameter settings for each system, exceptfor |C|, which is varied in some of our experiments.The systems are as follows:1[brown]: Class-based n-grams (Brown et al,1992).
This is the oldest and one of the simplest sys-tems we tested.
It uses a bigram model where eachword type is assigned to a latent class (a hard assign-ment), and the probability of the corpus w1 .
.
.
wnis computed as P (w1|c1)?ni=2 P (wi|ci)P (ci|ci?1),where ci is the class of wi.
The goal is to opti-mize the probability of the corpus under this model.The authors use an approximate search procedure:greedy agglomerative hierarchical clustering fol-lowed by a step in which individual word types areconsidered for movement to a different class if thisimproves the corpus probability.
[clark]: Class-based n-grams with morphology(Clark, 2003).
This system uses a similar modelto the previous one, and also clusters word types(rather than tokens, as the rest of the systems do).The main differences between the systems are thatclark uses a slightly different approximate searchprocedure, and that he augments the probabilisticmodel with a prior that prefers clusterings wheremorphologically similar words are clustered to-gether.
The morphology component is implementedas a single-order letter HMM.
[cw]: Chinese Whispers graph clustering (Bie-mann, 2006).
Unlike the other systems we consider,this one induces the value of |C| rather than takingit as an input parameter.2 The system uses a graphclustering algorithm called Chinese Whispers that isbased on contextual similarity.
The algorithm worksin two stages.
The first clusters the most frequent10,000 words (target words) based on their contextstatistics, with contexts formed from the most fre-quent 150-250 words (feature words) that appear ei-1Implementations were obtained from:brown: http://www.cs.berkeley.edu/?pliang/software/brown-cluster-1.2.zip (Percy Liang),clark: http://www.cs.rhul.ac.uk/home/alexc/pos2.tar.gz (Alex Clark),cw: http://wortschatz.uni-leipzig.de/%7Ecbiemann/software/jUnsupos1.0.zip (Chris Biemann),bhmm, vbhmm, pr, feat: by request from the authors of therespective papers.2Another recent model that induces |C| is the Infinite HMM(Van Gael et al, 2009).
Unfortunately, we were unable to ob-tain code for the IHMM in time to include it in our analysis.Van Gael et al (2009) report results of around 59% V-Measureon WSJ, with 194 induced clusters, which is not as good as thebest system scores in Section 4.576ther to the left or right of a target word.
The secondstage deals with medium and low frequency wordsand uses pairwise similarity scores calculated by thenumber of shared neighbors between two words ina 4-word context window.
The final clustering isa combination of the clusters obtained in the twostages.
While the number of target words, featurewords, and window size are in principle parametersof the algorithm, they are hard-coded in the imple-mentation we used and we did not change them.
[bhmm]: Bayesian HMM with Gibbs sampling(Goldwater and Griffiths, 2007).
This system isbased on a standard HMM for POS tagging.
It dif-fers from the standard model by placing Dirichletpriors over the multinomial parameters defining thestate-state and state-emission distributions, and usesa collapsed Gibbs sampler to infer the hidden tags.The Dirichlet hyperparameters ?
(which controls thesparsity of the transition probabilities) and ?
(whichcontrols the sparsity of the emission probabilities)can be fixed or inferred.
We used a bigram versionof this model with hyperparameter inference.
[vbhmm]: Bayesian HMM with variationalBayes (Johnson, 2007).
This system uses thesame bigram model as bhmm, but uses variationalBayesian EM for inference.
We fixed the ?
and ?parameters to 0.1, values that appeared to be reason-able based on Johnson (2007), and which were alsoused by Graca et al (2009).
[pr]: Sparsity posterior-regularization HMM(Graca et al, 2009).
The Bayesian approaches de-scribed above encourage sparse state-state and state-emission distributions only indirectly through theDirichlet priors.
This system, while utilizing thesame bigram HMM, encourages sparsity directlyby constraining the posterior distributions using theposterior regularization framework (Ganchev et al,2009).
A parameter ?
controls the strengths of theconstraints (default = 25).
Following Graca et al(2009), we set ?
= ?
= 0.1.
[feat]: Feature-based HMM (Berg-Kirkpatricket al, 2010).
This system uses a model that has thestructure of a standard HMM, but assumes that thestate-state and state-emission distributions are logis-tic, rather than multinomial.
The logistic distribu-tions allow the model to incorporate local featuresof the sort often used in discriminative models.
Thedefault features are morphological, such as charactertrigrams and capitalization.3 Evaluation MeasuresOne difficulty in comparing POS induction meth-ods is in finding an appropriate evaluation measure.Many different measures have been proposed overthe years, but there is still no consensus on which isbest.
In addition, some measures with supposed the-oretical advantages, such as Variation of Information(VI) (Meila?, 2003) have had little empirical analy-sis.
Our goal in this section is to determine whichof these measures is most sensible for evaluatingthe systems presented above.
We first describe eachmeasure before presenting empirical results.
Exceptfor VI, all measures range from 0 to 1, with higherscores indicating better performance.
[many-to-1]: Many-to-one mapping accuracy(also known as cluster purity) maps each cluster tothe gold standard tag that is most common for thewords in that cluster (henceforth, the preferred tag),and then computes the proportion of words taggedcorrectly.
More than one cluster may be mapped tothe same gold standard tag.
This is the most com-monly used metric across the literature as it is in-tuitive and creates a meaningful POS sequence outof the cluster identifiers.
However, it tends to yieldhigher scores as |C| increases, making comparisonsdifficult when |C| can vary.
[crossval]: Cross-validation accuracy (Gao andJohnson, 2008) is intended to address the problemwith many-to-one accuracy which is that assigningeach word to its own class yields a perfect score.
Inthis measure, the first half of the corpus is used toobtain the many-to-one mapping of clusters to tags,and this mapping is used to compute the accuracy ofthe clustering on the second half of the corpus.
[1-to-1]: One-to-one mapping accuracy(Haghighi and Klein, 2006) constrains the mappingfrom clusters to tags, so that at most one cluster canbe mapped to any tag.
The mapping is performedgreedily.
In general, as the number of clustersincreases, fewer clusters will be mapped to theirpreferred tag and scores will decrease (especiallyif the number of clusters is larger than the numberof tags, so that some clusters are unassigned andreceive zero credit).
Again, this makes it difficult to577compare solutions with different values of |C|.
[vi]: Variation of Information (Meila?, 2003) isan information-theoretic measure that regards thesystem output C and the gold standard tags T as twoseparate clusterings, and evaluates the amount of in-formation lost in going from C to T and the amountof information gained, i.e., the sum of the condi-tional entropy of each clustering conditioned on theother.
More formally, V I(C, T ) = H(T |C) +H(C|T ) = H(C)+H(T )?
2I(C, T ), where H(.
)is the entropy function and I(.)
is the mutual infor-mation.
VI and other entropy-based measures havebeen argued to be superior to accuracy-based mea-sures such as those above, because they considernot only the majority tag in each cluster, but alsowhether the remainder of the cluster is more or lesshomogeneous.
Unlike the other measures we con-sider, lower scores are better (since VI measures thedifference between clusterings in bits).
[vm]: V-Measure (Rosenberg and Hirschberg,2007) is another entropy-based measure that is de-signed to be analogous to F-measure, in that it is de-fined as the weighted harmonic mean of two values,homogeneity (h, the precision analogue) and com-pleteness (c, the recall analogue):h = 1?
H(T |C)H(T )(1)c = 1?
H(C|T )H(C)(2)VM = (1 + ?
)hc(?h) + c(3)As with F-measure, ?
is normally set to 1.
[vmb]: V-beta is an extension to V-Measure, pro-posed by (Vlachos et al, 2009).
They noted thatV-Measure favors clusterings where the number ofclusters |C| is larger than the number of POS tags|T |.
To address this issue the parameter ?
in equa-tion 3 is set to |C|/|T | in order adjust the balancebetween homogeneity and completeness.
[s-fscore]: Substitutable F-score (Frank et al,2009).
One potential issue with all of the above mea-sures is that they require a gold standard tagging tocompute.
This is normally available during develop-ment of a system, but if the system is deployed on anovel language a gold standard may not be available.In addition, there is the question of whether the goldstandard itself is ?correct?.
Recently, Frank et al(2009) proposed this novel evaluation measure thatrequires no gold standard, instead using the conceptof substitutability to evaluate performance.
Insteadof comparing the system?s clusters C to gold stan-dard clusters T , they are compared to a set of clus-ters S created from substitutable frames, i.e., clus-ters of words that occur in the same syntactic en-vironment.
Ideally a substitutable frame would becreated by sentences differing in only one word (e.g.
?I want the blue ball.?
and ?I want the red ball.?
)and the resulting cluster would contain the wordsthat change (e.g.
[blue, red]).
However since it isalmost impossible to find these types of sentencesin real-world corpora, the authors use frames cre-ated by two words appearing in the corpus with ex-actly one word between (e.g.
the ?- ball).
Once thesubstitutable clusters have been created, they can beused to calculate the Precision (SP ), Recall (SR)and F-score (SF ) of the system?s clustering:SP =?s?S?c?C |s ?
c|(|s ?
c| ?
1)?c?C |c|(|c| ?
1)(4)SR =?s?S?c?C |s ?
c|(|s ?
c| ?
1)?s?S |s|(|s| ?
1)(5)SF = 2 ?
SP ?
SRSP + SR(6)3.1 Empirical resultsWe mentioned a few strengths and weaknesses ofeach evaluation method above; in this section wepresent some empirical results to expand on theseclaims.
First, we examine the effects of varying |C|on the behavior of the evaluation measures, whilekeeping the number of gold standard tags the same(|T | = 45).
Results were obtained by training andevaluating each system on the full WSJ portion ofthe Penn Treebank corpus (Marcus et al, 1993).
Fig-ure 1 shows the results from the Brown system for|C| ranging from 1 to 200; the same trends were ob-served for all other systems.3 In addition, Table 1provides results for the two extremes of |C| = 1 (allwords assigned to the same cluster) and |C| equal tothe size of the corpus (a single word per cluster), as3The results reported in this paper are only a fractionof the total from our experiments; given the number ofparameters, models and measures tested, we obtained over15000 results.
The full set of results can be found athttp://homepages.inf.ed.ac.uk/s0787820/pos/.578Figure 1: Scores for all evaluation measures as a function of the number of clusters returned [model:brown, corpus:wsj,|C|:{1-200}, |T |:45].
The right-hand y-axis shows VI scores (lower is better); the left-hand y-axis shows percentagescores for all other measures.
The vertical line indicates |T |.
Many-to-1 is invisible as it tracks crossval so closely.measure super random all singlemany-to-1 97.85 13.97 13.97 100crossval 97.59 13.98 13.98 01-to-1 97.86 2.42 13.97 0.01vi 0.35 9.81 4.33 15.82vm 95.98 0.02 0 35.42vmb 95.98 0 0 99.99s-fscore 7.53 0.50 0 0Table 1: Baseline scores for the different evaluation mea-sures on the WSJ corpus.
For all measures except VIhigher is better.well as two other baselines (a supervised tagging4and a random clustering with |C| = 45).These empirical results confirm that certain mea-sures favor solutions with many clusters, while oth-ers prefer fewer clusters.
As expected, many-to-1correlates positively with |C|, rising to almost 85%with |C| = 200 and reaching 100% when the num-ber of clusters is maximal (i.e., single).
Recall thatcrossval was proposed as a possible solution to thisproblem, and it does solve the extreme case of sin-gle, yielding 0% accuracy rather than 100%.
How-ever, it patterns just like many-to-1 for up to 200clusters, suggesting that there is very little difference4We used the Stanford Tagger trained on the WSJ corpus:http://nlp.stanford.edu/software/tagger.shtml.between the two for any reasonable number of clus-ters, and we should be wary of using either one when|C| may vary.In contrast to these measures are 1-to-1 and vi: forthe most part, they yield worse performance (lower1-to-1, higher vi) as |C| increases.
However, in thiscase the trend is not monotonic: there is an initialimprovement in performance before the decrease be-gins.
One might hope that the peak in performancewould occur when the number of clusters is approx-imately equal to the number of gold standard tags;however, the best performance for both 1-to-1 andvi occurs with approximately 25-30 clusters, manyfewer than the gold standard 45.Next we consider vm and vmb.
Interestingly, al-though vmbwas proposed as a way to correct for thesupposed tendency of vm to increase with increas-ing |C|, we find that vm is actually more stable thanvmb over different values of |C|.
Thus, if the goalis to compare systems producing different numbersof clusters (especially important for systems that in-duce the number of clusters), then vm seems moreappropriate than any of the above measures, whichare more standard in the literature.Finally, we analyze the behavior of the gold-standard-independent measure, s-fscore.
On thepositive side, this measure assigns scores of 0 to the579two extreme cases of all and single, and is relativelystable across different values of |C| after an initialincrease.
It assigns a lower score to the supervisedsystem than to brown, indicating that words in thesupervised clusters (which are very close to the goldstandard) are actually less substitutable than wordsin the unsupervised clusters.
This is probably due tothe fact that the gold standard encodes ?pure?
syn-tactic classes, while substitutability also depends onsemantic characteristics (which tend to be picked upby unsupervised clustering systems as well).
An-other potential problem with this measure is that ithas a very small dynamic range ?
while scores ashigh as 1 are theoretically possible, in practice theywill never be achieved, and we see that the actualrange of scores observed are all under 20%.We conclude that there is probably no single eval-uation measure that is best for all purposes.
If a goldstandard is available, then many-to-1 and 1-to-1 arethe most intuitive measures, but should not be usedwhen |C| is variable, and do not account for differ-ences in the errors made.
While vi has been popularas an entropy-based alternative to address the latterproblem, its scores are not easy to interpret (being ona scale of bits) and it still has the problem of incom-parability across different |C|.
Overall, vm seems tobe the best general-purpose measure that combinesan entropy-based score with an intuitive 0-1 scaleand stability over a wide range of |C|.4 System comparisonHaving provided some intuitions about the behav-ior of different evaluation methods, we move on toevaluating the various systems presented in Section2.
We first present results for the same WSJ cor-pus used above.
However, because most of the sys-tems were initially developed on this corpus, andoften evaluated only on it, there is a question ofwhether their methods and/or hyperparameters areoverly specific to the domain or to the English lan-guage.
This is a particularly pertinent question sincea primary argument in favor of unsupervised sys-tems is that they are easier to port to a new languageor domain than supervised systems.
To address thisquestion, we evaluate all the systems as well on themultilingual Multext East corpus (Erjavec, 2004),without changing any of the parameter settings.
|C|was set to 45 for all of the experiments reported inthis section.
Based on our assessment of evaluationFigure 2: Performance of the different systems on WSJ,using three different measures [|C|:45, |T |:45]system runtimebrown?10 min.clark?40 min.cw?10 min.bhmm?4 hrs.vbhmm?10 hrs.pr?10 hrs.
*feat?40 hrs.
*Table 2: Runtimes for the different systems on WSJ[|C|:45].
*pr and feat have multithreading implemen-tations and ran on 16 cores.measures above, we report VM scores as the mostreliable measure across different systems and clus-ter set sizes; to facilitate comparisons with previouspapers, we also report many-to-one and one-to-oneaccuracy.4.1 Results on WSJFigure 2 presents results for all seven systems, withapproximate runtimes shown in Table 2.
While thesealgorithms have not necessarily been optimized forspeed, there is a fairly clear distinction between theolder type-clustering models (brown, clark) and thegraph-based algorithm (cw) on the one hand, andthe newer machine-learning approaches (bhmm,vbhmm, pr, feat) on the other, with the former be-ing much faster to run.
Despite their faster run-times and less sophisticated methods, however, thesesystems perform surprisingly well in comparison tothe latter group.
Even the oldest and perhaps sim-plest method (brown) outperforms the two BHMMsand posterior regularization on all measures.
Only580Figure 3: VM scores for the different systems on EnglishMultext-East and WSJ-S corpora [|C|:45, |T |:{14,17}]Figure 4: VM scores for the different systems on the eightMultext-East corpora [|C|:45, |T |:14]the very latest approach (feat) rivals clark, show-ing slightly better performance on two of the threemeasures (clark: 71.2, 53.8, 65.5 on many-to-one,one-to-one, VM; feat: 73.9, 53.3, 67.7).
The cwsystem returns a total of 568 clusters on this data set,so the many-to-one and one-to-one measures are notstrictly comparable to the other systems; on VM thissystem achieves middling performance.We note that the two best-performing systems,clark and feat, are also the only two to use mor-phological information.
Since the clustering algo-rithms used by brown and clark are quite similar,the difference in performance between the two canprobably be attributed to the extra information pro-vided by the morphology.
This supports the (unsur-prising) conclusion that incorporating morphologi-cal features is generally helpful for POS induction.4.2 Results on other corporaWe now examine whether either the relative or ab-solute performance of the different systems holds upwhen tested on a variety of different languages.
Forthese experiments, we used the 1984 portion of theMultext-East corpus (?
7k sentences), which containsparallel translations of Orwell?s 1984 in 8 differentlanguages: Bulgarian[bg], Czech[cs], Estonian[et],Hungarian[hu], Romanian[ro], Slovene[sl], Ser-bian[sr] and English[en].
We also included a 7ksentence version of the WSJ corpus [wsj-s] to helpdifferentiate effects of corpus size from those of do-main/language.
For the WSJ corpora we experi-mented with two standardly used tagsets: the orig-inal PTB 45-tag gold standard and a coarser set of17 tags previously used by several researchers work-ing on unsupervised POS tagging (Smith and Eis-ner, 2005; Goldwater and Griffiths, 2007; Johnson,2007).
For theMultext-East corpus only a coarse 14-tag tagset was available.5 Finally, to facilitate directcomparisons of genre while controlling for the sizeof both the corpus and the tag set, we also created afurther collapsed 13-tag set for WSJ.6Figure 3 illustrates the abilities of the differentsystems to generalize across different genres of En-glish text.
Comparing the results for the Multext-East English corpus and the small WSJ corpus with13 tags (i.e., controlling as much as possible for cor-pus size and number of gold standard tags), we seethat despite being developed on WSJ, the systemsactually perform better on Multext-East.
This is en-couraging, since it suggests that the methods andhyperparameters of the algorithms are not stronglytied to WSJ.
It also suggests that Multext-East is insome sense an easier corpus than WSJ.
Indeed, thedistribution of vocabulary items supports this view:the 100 most frequent words account for 48% ofthe WSJ corpus, but 57% of the 1984 novel.
It isalso worth pointing out that, although previous re-searchers have reduced the 45-tag WSJ set to 17 tagsin order to create an easier task for unsupervisedlearning (and to decrease training time), reducingthe tag set further to 13 tags actually decreases per-formance, since some distinctions found by the sys-5Out of the 14 tags only 11 are shared across all languages.For details c.f.
Appendix B in (Naseem et al, 2009).6We tried to make the meanings of the tags as similar aspossible between the two corpora; we had to create 13 ratherthan 14 WSJ tags for this reason.
Our 13-tag set can be foundat http://homepages.inf.ed.ac.uk/s0787820/pos/.581tems (e.g., between different types of punctuation)are collapsed in the gold standard.Figure 4 gives the results of the different systemson the various languages.7 Not surprisingly, all thealgorithms perform best on English, often by a widemargin, suggesting that they are indeed tuned bet-ter towards English syntax and/or morphology.
Onemight expect that the two systems with morpho-logical features (clark and feat) would show lessdifference between English and some of the otherlanguages (all of which have complex morphology)than the other systems.
However, although clarkand feat (along with Brown) are the best perform-ing systems overall, they don?t show any particularbenefit for the morphologically complex languages.8One difference between the Multext-East resultsand the WSJ results is that on Multext-East, clarkclearly outperforms all the other systems.
This istrue for both the English and non-English corpora,despite the similar performance of clark and featon (English) WSJ.
This suggests that feat benefitsmore from the larger corpus size of WSJ.
For theother languages clarkmay be benefiting from some-what more general morphological features; feat cur-rently contains suffix features but no prefix features(although these could be added).Overall, our experiments on multiple languagessupport our earlier claim that many of the newerPOS induction systems are not as successful as theolder methods.
Moreover, these experiments under-score the importance of testing unsupervised sys-tems on multiple languages and domains, since boththe absolute and relative performance of systemsmay change on different data sets.
Ideally, some ofthe corpora should be held out as unseen test dataif an effective argument is to be made regarding thelanguage- or domain-generality of the system.5 Learning from induced prototypesWe now introduce a final novel method of evaluat-ing POS induction systems and potentially improv-ing their performance as well.
Our idea is based7Some results are missing because not all of the corporawere successfully processed by all of the systems.8It can be argued that lemmatization would have given a sig-nificant gain to the performance of the systems in these lan-guages.
Although lemmatization information was included inthe corpus we chose not to use it, maintaining the fully unsu-pervised nature of this task.on the prototype-driven learning model of Haghighiand Klein (2006).
This model is unsupervised, butrequires as input a handful of prototypes (canonicalexamples) for each word class.
The system uses alog-linear model with features that include the pro-totype lists as well as morphological features (thesame ones used in feat).
Using the most frequentwords in each gold standard class as prototypes, theauthors report 80.5% accuracy (both many-to-oneand one-to-one) on WSJ, considerably higher thanany of the induction systems seen here.
This raisestwo questions: If we wish to induce prototypes with-out a tagged corpus or language-specific knowledge,which induction system will provide the best pro-totypes (i.e., most similar to the gold standard pro-totypes)?
And, can we use the induced prototypesas input to the prototype-driven model (h&k) toachieve better performance than the system the pro-totypes were extracted from?To explore these questions, we implemented asimple heuristic method for inducing prototypesfrom the output C of a POS induction system byselecting a few frequent words in each cluster thatare the most similar to other words in the cluster andalso the most dissimilar to the words in other clus-ters.
For each cluster ci ?
C, we retain as candi-date prototypes the words whose frequency in ci isat least 90% as high as the word with the highest fre-quency (in ci).
This yields about 20-30 candidatesfrom each cluster.
For each of these, we computeits average similarity S to the other candidates in itscluster, and the average dissimilarity D to the candi-dates in other clusters.
Similarity is computed usingthe method described by Haghighi and Klein (2006),which uses SVD on word context vectors and cosinesimilarity.
Dissimilarity between a pair of words iscomputed as one minus the similarity.
Finally wecompute the average M = 0.5(S + D), sort thewords by their M scores, and keep as prototypesthe top ten words with M > 0.25 ?
maxci(M).The cutoff threshold results in some clusters havingless than ten prototypes, which is appropriate sincesome gold standard categories have very few mem-bers (e.g., punctuation, determiners).Using this method, we first tested the variousbase+proto systems on the WSJ corpus.
Resultsin Table 3 show that the brown system producesthe best prototypes.
Although not as good asusing prototypes from the gold standard (h&k),582system many-to-1 1-to-1 vmbrown 76.1(8.3) 60.7(10.6) 68.8(5.8)clark 74.5(3.3) 62.1(8.3) 68.6(3.0)bhmm 71.8(8.6) 56.5(15.0) 65.7(9.5)vbhmm 68.1(17.9) 67.2(20.7) 67.5(18.3)pr 71.6(9.2) 60.2(17.0) 67.2(12.4)feat 69.8(-4.1) 52.0(-1.3) 63.1(-4.6)h&k 80.2 80.2 75.2Table 3: Scores on WSJ for our prototype-based POS in-duction system, with prototypes extracted from each ofthe existing systems [|C|:45,|T |:45].
Numbers in paren-theses are the improvement over the same system withoutusing the prototype step.
Scores in bold indicate the bestperformance (improvement) in each column.
h&k usesgold standard prototypes.corpus brown clarkwsj 68.8(5.8) 68.5(3.0)wsj-s 62.3(2.7) 67.5(3.6)en 58.5(1.6) 57.9(-3.3)bg 53.7(2.3) 50.2(-7.1)cs 49.9(5.0) 48.0(-4.0)et 45.8(4.9) 44.4(-1.9)hu 45.8(0.1) 47.0(-5.7)ro 53.2(0.8) 52.7(-3.3)sl 51.2(2.9) 51.7(-4.6)sr 48.0(2.8) 46.4(-4.9)Table 4: VM scores for brown+proto and clark+protoon all corpora.
Numbers in parentheses indicate improve-ment over the base systems.brown+proto yields a large improvement overbrown, and the highest performance of any systemtested so far.
In fact, the brown+proto scores are, toour knowledge, the best reported results for an un-supervised POS induction system on WSJ.Next, we evaluated the two best-performing+proto systems on Multext-East, as shown in Ta-ble 4.
We see that brown again yields the bestprototypes, and again yields improvements whenused as brown+proto (although the improvementsare not as large as those on WSJ).
Interestingly,clark+proto actually performs worse than clark onthe multilingual data, showing that although inducedprototypes can in principle improve the performanceof a system, not all systems will benefit in all situ-ations.
This suggests a need for additional investi-gation to determine what properties of an existinginduction system allow it to produce useful proto-types with the current method and/or to develop aspecialized system specifically targeted towards in-ducing useful prototypes.6 ConclusionIn this paper, we have attempted to provide a morecomprehensive review and comparison of evaluationmeasures and systems for POS induction than hasbeen done before.
We pointed out that most of thecommonly used evaluation measures are sensitive tothe number of induced clusters, and suggested thatV-measure (which is less sensitive) should be usedas an alternative or in conjunction with the standardmeasures.
With regard to the systems themselves,we found that many of the newer approaches actu-ally perform worse than older methods that are bothsimpler and faster.
The newer systems have intro-duced potentially important machine learning tools,but are not necessarily better suited to the POS in-duction task specifically.Since portability is a distinguishing feature for un-supervised models, we have stressed the importanceof testing the systems on corpora that were not usedin their development, and especially on different lan-guages.
We found that on non-English languages,Clark?s (2003) system performed best.Finally, we introduced the idea of evaluating in-duction systems based on their ability to produceuseful cluster prototypes.
We found that the old-est system (Brown et al, 1992) yielded the bestprototypes, and that using these prototypes gavestate-of-the-art performance on WSJ, as well as im-provements on nearly all of the non-English corpora.These promising results suggest a new direction forfuture research: improving POS induction by de-veloping methods targeted towards extracting betterprototypes, rather than focusing on improving clus-tering of the entire data set.AcknowledgmentsWe thank Mark Johnson, Kuzman Ganchev, andTaylor Berg-Kirkpatrick for providing the imple-mentations of their models, as well as StellaFrank, Tom Kwiatkowski, Luke Zettlemoyer and theanonymous reviewers for their comments and sug-gestions.
This work was supported by an EPSRCgraduate Fellowship, and by ERCAdvanced Fellow-ship 249520 GRAMPLUS.583ReferencesTaylor Berg-Kirkpatrick, Alexandre B.
Co?te?, John DeN-ero, and Dan Klein.
2010.
Painless unsupervisedlearning with features.
In Proceedings of NAACL2010, pages 582?590, Los Angeles, California, June.Chris Biemann.
2006.
Unsupervised part-of-speech tag-ging employing efficient graph clustering.
In Proceed-ings of COLING ACL 2006, pages 7?12, Morristown,NJ, USA.Peter F. Brown, Vincent J. Della Pietra, Peter V. Desouza,Jennifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proceedings of EACL 2003, pages 59?66,Morristown, NJ, USA.Tomaz?
Erjavec.
2004.
MULTEXT-East Version 3:Multilingual Morphosyntactic Specifications, Lexi-cons and Corpora.
In Fourth International Conferenceon Language Resources and Evaluation, LREC?04,page In print, Paris.
ELRA.Stella Frank, Sharon Goldwater, and Frank Keller.
2009.Evaluating models of syntactic category acquisitionwithout using a gold standard.
In Proceedings ofCogSci09, July.Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, andBen Taskar.
2009.
Posterior regularization for struc-tured latent variable models.
Technical report, Univer-sity of Pennsylvania.Jianfeng Gao and Mark Johnson.
2008.
A comparison ofbayesian estimators for unsupervised hidden markovmodel pos taggers.
In Proceedings of EMNLP 2008,pages 344?352, Morristown, NJ, USA.Sharon Goldwater and Tom Griffiths.
2007.
A fullybayesian approach to unsupervised part-of-speech tag-ging.
In Proceedings of ACL 2007, pages 744?751,Prague, Czech Republic, June.Joao Graca, Kuzman Ganchev, Ben Taskar, and FernandoPereira.
2009.
Posterior vs parameter sparsity in latentvariable models.
In Y. Bengio, D. Schuurmans, J. Laf-ferty, C. K. I. Williams, and A. Culotta, editors, Ad-vances in Neural Information Processing Systems 22,pages 664?672.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofNAACL 2006, pages 320?327, Morristown, NJ, USA.Mark Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers?
In Proceedings of EMNLP-CoNLL2007, pages 296?305, Prague, Czech Republic, June.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: the Penn Treebank.
Computational Linguistics,19(2):331?330.Marina Meila?.
2003.
Comparing clusterings by the vari-ation of information.
In Learning Theory and KernelMachines, pages 173?187.B.
Merialdo.
1994.
Tagging English text with a proba-bilistic model.
Computational Linguistics, 20(2):155?172.Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, andRegina Barzilay.
2009.
Multilingual part-of-speechtagging: Two unsupervised approaches.
Journal of Ar-tificial Intelligence Research, 36:341?385.Sujith Ravi and Kevin Knight.
2009.
Minimized mod-els for unsupervised part-of-speech tagging.
In Pro-ceedings of ACL-IJCNLP 2009, pages 504?512, Sun-tec, Singapore, August.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clus-ter evaluation measure.
In Proceedings of EMNLP-CoNLL 2007, pages 410?420.Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: training log-linear models on unlabeled data.In Proceedings of ACL 2005, pages 354?362, Morris-town, NJ, USA.K.
Toutanova and M. Johnson.
2007.
A Bayesian LDA-based model for semi-supervised part-of-speech tag-ging.
In Proceedings of NIPS 2007.Jurgen Van Gael, Andreas Vlachos, and Zoubin Ghahra-mani.
2009.
The infinite HMM for unsupervised PoStagging.
In Proceedings of EMLNP 2009, pages 678?687, Singapore, August.Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-mani.
2009.
Unsupervised and constrained dirichletprocess mixture models for verb clustering.
In Pro-ceedings of GEMS 2009, pages 74?82, Morristown,NJ, USA.584
