Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 699?703,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsImproving Dependency Parsing with Semantic ClassesEneko Agirre*, Kepa Bengoetxea*, Koldo Gojenola*, Joakim Nivre+* Department of Computer Languages and Systems, University of the Basque CountryUPV/EHU+ Department of Linguistics and Philosophy, Uppsala University{e.agirre, kepa.bengoetxea, koldo.gojenola}@ehu.es joakim.nivre@lingfil.uu.seAbstractThis paper presents the introduction ofWordNet semantic classes in a dependencyparser, obtaining improvements on the fullPenn Treebank for the first time.
We trieddifferent combinations of some basic se-mantic classes and word sense disambigua-tion algorithms.
Our experiments show thatselecting the adequate combination of se-mantic features on development data is keyfor success.
Given the basic nature of thesemantic classes and word sense disam-biguation algorithms used, we think there isample room for future improvements.1 IntroductionUsing semantic information to improve parsingperformance has been an interesting research ave-nue since the early days of NLP, and several re-search works have tried to test the intuition thatsemantics should help parsing, as can be exempli-fied by the classical PP attachment experiments(Ratnaparkhi, 1994).
Although there have beensome significant results (see Section 2), this issuecontinues to be elusive.
In principle, dependencyparsing offers good prospects for experimentingwith word-to-word-semantic relationships.We present a set of experiments using semanticclasses in dependency parsing of the Penn Tree-bank (PTB).
We extend the tests made in Agirre etal.
(2008), who used different types of semanticinformation, obtaining significant improvements intwo constituency parsers, showing how semanticinformation helps in constituency parsing.As our baseline parser, we use MaltParser(Nivre, 2006).
We will evaluate the parser on boththe full PTB (Marcus et al 1993) and on a sense-annotated subset of the Brown Corpus portion ofPTB, in order to investigate the upper bound per-formance of the models given gold-standard senseinformation, as in Agirre et al (2008).2 Related WorkAgirre et al (2008) trained two state-of-the-art sta-tistical parsers (Charniak, 2000; Bikel, 2004) onsemantically-enriched input, where content wordshad been substituted with their semantic classes.This was done trying to overcome the limitationsof lexicalized approaches to parsing (Magerman,1995; Collins, 1996; Charniak, 1997; Collins,2003), where related words, like scissors and knifecannot be generalized.
This simple method allowedincorporating lexical semantic information into theparser.
They tested the parsers in both a full pars-ing and a PP attachment context.
The experimentsshowed that semantic classes gave significant im-provement relative to the baseline, demonstratingthat a simplistic approach to incorporating lexicalsemantics into a parser significantly improves itsperformance.
This work presented the first resultsover both WordNet and the Penn Treebank to showthat semantic processing helps parsing.Collins (2000) tested a combined parsing/wordsense disambiguation model based in WordNetwhich did not obtain improvements in parsing.Koo et al (2008) presented a semisupervisedmethod for training dependency parsers, usingword clusters derived from a large unannotatedcorpus as features.
They demonstrate the effective-ness of the approach in a series of dependencyparsing experiments on PTB and the Prague De-pendency Treebank, showing that the cluster-basedfeatures yield substantial gains in performanceacross a wide range of conditions.
Suzuki et al(2009) also experiment with the same methodcombined with semi-supervised learning.699Ciaramita and Attardi (2007) show that addingsemantic features extracted by a named entity tag-ger (such as PERSON or MONEY) improves theaccuracy of a dependency parser, yielding a 5.8%relative error reduction on the full PTB.Candito and Seddah (2010) performed experi-ments in statistical parsing of French, where termi-nal forms were replaced by more general symbols,particularly clusters of words obtained throughunsupervised clustering.
The results showed thatword clusters had a positive effect.Regarding dependency parsing of the EnglishPTB, currently Koo and Collins (2010) and Zhangand Nivre (2011) hold the best results, with 93.0and 92.9 unlabeled attachment score, respectively.Both works used the Penn2Malt constituency-to-dependency converter, while we will make use ofPennConverter (Johansson and Nugues, 2007).Apart from these, there have been other attemptsto make use of semantic information in differentframeworks and languages, as in (Hektoen 1997;Xiong et al 2005; Fujita et al 2007).3 Experimental FrameworkIn this section we will briefly describe the data-driven parser used for the experiments (subsection3.1), followed by the PTB-based datasets (subsec-tion 3.2).
Finally, we will describe the types of se-mantic representation used in the experiments.3.1 MaltParserMaltParser (Nivre et al 2006) is a trainable de-pendency parser that has been successfully appliedto typologically different languages and treebanks.We will use one of its standard versions (version1.4).
The parser obtains deterministically a de-pendency tree in linear-time in a single pass overthe input using two main data structures: a stack ofpartially analyzed items and the remaining inputsequence.
To determine the best action at eachstep, the parser uses history-based feature modelsand SVM classifiers.
One of the main reasons forusing MaltParser for our experiments is that it eas-ily allows the introduction of semantic informa-tion, adding new features, and incorporating themin the training model.3.2 DatasetWe used two different datasets: the full PTB andthe Semcor/PTB intersection (Agirre et al 2008).The full PTB allows for comparison with the state-of-the-art, and we followed the usual train-testsplit.
The Semcor/PTB intersection contains bothgold-standard sense and parse tree annotations, andallows to set an upper bound of the relative impactof a given semantic representation on parsing.
Weuse the same train-test split of Agirre et al (2008),with a total of 8,669 sentences containing 151,928words partitioned into 3 sets: 80% training, 10%development and 10% test data.
This dataset isavailable on request to the research community.We will evaluate the parser via Labeled Attach-ment Score (LAS).
We will use Bikel?s random-ized parsing evaluation comparator to test thestatistical significance of the results using wordsense information, relative to the respective base-line parser using only standard features.We used PennConverter (Johansson andNugues, 2007) to convert constituent trees in thePenn Treebank annotation style into dependencytrees.
Although in general the results from parsingPennconverter?s output are lower than with otherconversions, Johansson and Nugues (2007) claimthat this conversion is better suited for semanticprocessing, with a richer structure and a more fine-grained set of dependency labels.
For the experi-ments, we used the best configuration for Englishat the CoNLL 2007 Shared Task on DependencyParsing (Nivre et al, 2007) as our baseline.3.3 Semantic representation and disambigua-tion methodsWe will experiment with the range of semanticrepresentations used in Agirre et al (2008), all ofwhich are based on WordNet 2.1.
Words in Word-Net (Fellbaum, 1998) are organized into sets ofsynonyms, called synsets (SS).
Each synset in turnbelongs to a unique semantic file (SF).
There are atotal of 45 SFs (1 for adverbs, 3 for adjectives, 15for verbs, and 26 for nouns), based on syntacticand semantic categories.
For example, noun se-mantic files (SF_N) differentiate nouns denotingacts or actions, and nouns denoting animals,among others.
We experiment with both full syn-sets and SFs as instances of fine-grained andcoarse-grained semantic representation, respec-tively.
As an example of the difference in thesetwo representations, knife in its tool sense is in theEDGE TOOL USED AS A CUTTINGINSTRUMENT singleton synset, and also in theARTIFACT SF along with thousands of other700words including cutter.
Note that these are the twoextremes of semantic granularity in WordNet.As a hybrid representation, we also tested the ef-fect of merging words with their corresponding SF(e.g.
knife+ARTIFACT).
This is a form of seman-tic specialization rather than generalization, andallows the parser to discriminate between the dif-ferent senses of each word, but not generalizeacross words.
For each of these three semantic rep-resentations, we experimented with using each of:(1) all open-class POSs (nouns, verbs, adjectivesand adverbs), (2) nouns only, and (3) verbs only.There are thus a total of 9 combinations of repre-sentation type and target POS: SS (synset), SS_N(noun synsets), SS_V (verb synsets), SF (semanticfile), SF_N (noun semantic files), SF_V (verb se-mantic files), WSF (wordform+SF), WSF_N(wordform+SF for nouns) and WSF_V (for verbs).For a given semantic representation, we needsome form of WSD to determine the semantics ofeach token occurrence of a target word.
We ex-perimented with three options: a) gold-standard(GOLD) annotations from SemCor, which givesthe upper bound performance of the semantic rep-resentation, b) first Sense (1ST), where all tokeninstances of a given word are tagged with theirmost frequent sense in WordNet, and c) automaticSense Ranking (ASR) which uses the sense re-turned by an unsupervised system based on an in-dependent corpus (McCarthy et al 2004).
For thefull Penn Treebank experiments, we only had ac-cess to the first sense, taken from Wordnet 1.7.4 ResultsIn the following two subsections, we will first pre-sent the results in the SemCor/PTB intersection,with the option of using gold, 1st sense and auto-matic sense information (subsection 4.1) and thenext subsection (4.2) will show the results on thefull PTB, using 1st sense information.
All resultsare shown as labelled attachment score (LAS).4.1 Semcor/PTB (GOLD/1ST/ASR)We conducted a series of experiments testing:?
Each individual semantic feature, whichgives 9 possibilities, also testing differentlearning configurations for each one.?
Combinations of semantic features, for in-stance, SF+SS_N+WSF would combine thesemantic file with noun synsets and word-form+semantic file.Although there were hundreds of combinations,we took the best combination of semantic featureson the development set for the final test.
For thatreason, the table only presents 10 results for eachdisambiguation method, 9 for the individual fea-tures and one for the best combination.Table 1 presents the results obtained for each ofthe disambiguation methods (gold standard senseinformation, 1st sense, and automatic sense rank-ing) and individual semantic feature.
In all casesexcept two, the use of semantic classes is benefi-System            LASBaseline  81.10SS 81.18 +0.08SS_N 81.40 +0.30SS_V *81.58 +0.48SF **82.05 +0.95SF_N81.51 +0.41SF_V 81.51 +0.41WSF 81.51 +0.41WSF_N 81.43 +0.33WSF_V *81.51 +0.41GoldSF+SF_N+SF_V+SS+WSF_N *81.74 +0.64SS 81.30 +0.20SS_N *81.56 +0.46SS_V *81.49 +0.39SF 81.00 -0.10SF_N80.97 -0.13SF_V **81.66 +0.56WSF 81.32 +0.22WSF_N *81.62 +0.52WSF_V **81.72 +0.62ASRSF_V+SS_V 81.41 +0.31SS 81.40 +0.30SS_N 81.39 +0.29SS_V *81.48 +0.38SF *81.59 +0.49SF_N81.38 +0.28SF_V *81.52 +0.42WSF *81.57 +0.46WSF_N 81.40 +0.30WSF_V 81.42 +0.321STSF+SS_V+WSF_N **81.92 +0.81Table 1.
Evaluation results on the test set for theSemcor-Penn intersection.
Individual semanticfeatures and best combination.
(**: statistically significant, p < 0.005; *: p < 0.05)701cial albeit small.
Regarding individual features, theSF feature using GOLD senses gives the best im-provement.
However, GOLD does not seem toclearly improve over 1ST and ASR on the rest ofthe features.
Comparing the automatically obtainedclasses, 1ST and ASR, there is no evident clueabout one of them being superior to the other.Regarding the best combination as selected inthe training data, each WSD method yields a dif-ferent combination, with best results for 1ST.
Theimprovement is statistically significant for both1ST and GOLD.
In general, the results in Table 1do not show any winning feature across all WSDalgorithms.
The best results are obtained when us-ing the first sense heuristic, but the difference isnot statistically significant.
This shows that perfectWSD is not needed to obtain improvements, but italso shows that we reached the upperbound of ourgeneralization and learning method.4.2 Penn Treebank and 1st senseWe only had 1st sense information available forthe full PTB.
We tested MaltParser on the bestconfiguration obtained for the reduced Sem-cor/PTB on the full treebank, taking sections 2-21for training and section 23 for the final test.
Table2 presents the results, showing that several of theindividual features and the best combination givesignificant improvements.
To our knowledge, thisis the first time that WordNet semantic classes helpto obtain improvements on the full Penn Treebank.It is interesting to mention that, although notshown on the tables, using lemmatization to assignsemantic classes to wordforms gave a slight in-crease for all the tests (0.1 absolute point approxi-mately), as it helped to avoid data sparseness.
Weapplied Schmid?s (1994) TreeTagger.
This can beseen as an argument in favour of performing mor-phological analysis, an aspect that is many timesneglected when processing morphologically poorlanguages as English.We also did some preliminary experiments us-ing Koo et al?s (2008) word clusters, both inde-pendently and also combined with the WordNet-based features, without noticeable improvements.5 ConclusionsWe tested the inclusion of several types of seman-tic information, in the form of WordNet semanticclasses in a dependency parser, showing that:?
Semantic information gives an improvementon a transition-based deterministic depend-ency parsing.?
Feature combinations give an improvementover using a single feature.
Agirre et al(2008) used a simple method of substitutingwordforms with semantic information,which only allowed using a single semanticfeature.
MaltParser allows the combinationof several semantic features together withother features such as wordform, lemma orpart of speech.
Although tables 1 and 2 onlyshow the best combination for each type ofsemantic information, this can be appreci-ated on GOLD and 1ST in Table 1.
Due tospace reasons, we only have showed the bestcombination, but we can say that in generalcombining features gives significant in-creases over using a single semantic feature.?
The present work presents a statistically sig-nificant improvement for the full treebankusing WordNet-based semantic informationfor the first time.
Our results extend those ofAgirre et al (2008), which showed im-provements on a subset of the PTB.Given the basic nature of the semantic classesand WSD algorithms, we think there is room forfuture improvements, incorporating new kinds ofsemantic information, such as WordNet base con-cepts, Wikipedia concepts, or similarity measures.System            LASBaseline  86.27SS *86.53 +0.26SS_N 86.33 +0.06SS_V *86.48 +0.21SF **86.63 +0.36SF_N*86.56 +0.29SF_V 86.34 +0.07WSF *86.50 +0.23WSF_N 86.25 -0.02WSF_V *86.51 +0.241STSF+SS_V+WSF_N *86.60 +0.33Table 1.
Evaluation results (LAS) on the testset for the full PTB.
Individual features andbest combination.
(**: statistically, p < 0.005; *: p < 0.05)702ReferencesEneko Agirre, Timothy Baldwin, and David Martinez.2008.
Improving parsing and PP attachment perform-ance with sense information.
In Proceedings of ACL-08: HLT, pages 317?325, Columbus, Ohio.Daniel M. Bikel.
2004.
Intricacies of Collins?
parsingmodel.
Computational Linguistics, 30(4):479?511.Candito, M. and D. Seddah.
2010.
Parsing word clus-ters.
In Proceedings of the NAACL HLT 2010 FirstWorkshop on Statistical Parsing of Morphologically-Rich Language, Los Angeles, USA.M.
Ciaramita and G. Attardi.
2007.
Dependency Parsingwith Second-Order Feature Maps and Annotated Se-mantic Information, In Proceedings of the 10th In-ternational Conference on Parsing Technology.Eugene Charniak.
1997.
Statistical parsing with a con-text-free grammar and word statistics.
In Proc.
of the15th Annual Conference on Artificial Intelligence(AAAI-97), pages 598?603, Stanford, USA.Eugene Charniak.
2000.
A maximum entropy-basedparser.
In Proc.
of the 1st Annual Meeting of theNorth American Chapter of Association for Compu-tational Linguistics (NAACL2000), Seattle, USA.Michael J. Collins.
1996.
A new statistical parser basedon lexical dependencies.
In Proc.
of the 34th AnnualMeeting of the ACL, pages 184?91, USA.Michael Collins.
2000.
A Statistical Model for Parsingand Word-Sense Disambiguation.
Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing and Very Large Corpora.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguis-tics, 29(4):589?637.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge.Sanae Fujita, Francis Bond, Stephan Oepen, and Taka-aki Tanaka.
2007.
Exploiting semantic informationfor HPSG parse selection.
In Proc.
of the ACL 2007Workshop on Deep Linguistic Processing.Richard Johansson and Pierre Nugues.
2007.
ExtendedConstituent-to-dependency Conversion for English.In Proceedings of NODALIDA 2007, Tartu, Estonia.Erik Hektoen.
1997.
Probabilistic parse selection basedon semantic cooccurrences.
In Proc.
of the 5th Inter-national Workshop on Parsing Technologies.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Pro-ceedings of ACL-08, pages 595?603, USA.Terry Koo, and Michael Collins.
2008.
Efficient Third-order Dependency Parsers.
In Proceedings of ACL-2010, pages 1?11, Uppsala, Sweden.Shari Landes, Claudia Leacock, and Randee I. Tengi.1998.
Building semantic concordances.
In ChristianeFellbaum, editor, WordNet: An Electronic LexicalDatabase.
MIT Press, Cambridge, USA.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proc.
of the 33rd AnnualMeeting of the ACL, pages 276?83, USA.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn treebank.
ComputationalLinguistics, 19(2):313?30.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant senses inuntagged text.
In Proc.
of the 42nd Annual Meetingof the ACL, pages 280?7, Barcelona, Spain.Joakim Nivre.
2006.
Inductive Dependency Parsing.Text, Speech and Language Technology series,Springer.
2006, XI, ISBN: 978-1-4020-4888-3.Joakim Nivre, Johan Hall, Sandra K?bler, RyanMcDonald, Jens Nilsson,  Sebastian Riedel andDeniz Yuret.
2007b.
The CoNLL 2007 Shared Taskon Dependency Parsing.
Proceedings of EMNLP-CoNLL.
Prague, Czech Republic.Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.1994.
A maximum entropy model for prepositionalphrase attachment.
In HLT ?94: Proceedings of theWorkshop on Human Language Technology, USA.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In Proceedings of In-ternational Conference on New Methods in Lan-guage Processing.
September 1994Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Mi-chael Collins.
2009.
An Empirical Study of Semi-supervised Structured Conditional Models for De-pendency Parsing.
In Proceedings of EMNLP, pages551?560.
Association for Computational Linguistics.Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, andYueliang Qian.
2005.
Parsing the Penn ChineseTreebank with semantic knowledge.
In Proc.
of the2nd International Joint Conference on Natural Lan-guage Processing (IJCNLP-05), Korea.Yue Zhang, and Joakim Nivre.
2011.
Transition-BasedParsing with Rich Non-Local Features.
In Proceed-ings of the 49th Annual Meeting of the Associationfor Computational Linguistics.703
