Parallel Distributed Grammar Engineering for Practical ApplicationsStephan Oepen?, Emily M.
Bender?, Uli Callmeier?, Dan Flickinger?
?, Melanie Siegel?
?CSLI Stanford ?YY Technologies ?DFKI GmbHStanford (CA) Mountain View (CA) Saarbru?cken (Germany)???oebenderdan???
@csli.stanford.edu{ucdan}@yy.com siegel@dfki.deAbstractBased on a detailed case study of paral-lel grammar development distributed acrosstwo sites, we review some of the require-ments for regression testing in grammar en-gineering, summarize our approach to sys-tematic competence and performance profil-ing, and discuss our experience with gram-mar development for a commercial applica-tion.
If possible, the workshop presentationwill be organized around a software demon-stration.1 BackgroundThe production of large-scale constraint-basedgrammars and suitable processing environments isa labour- and time-intensive process that, maybe,has become somewhat of a growth industry overthe past few years, as companies explore productsthat incorporate grammar-based language process-ing.
Many broad-coverage grammars have beendeveloped over several years, sometimes decades,typically coordinated by a single grammarian whowould often draw on additional contributors (e.g.the three HPSG implementations developed as partof the VerbMobil effort, see Flickinger, Copes-take, & Sag, 2000, Mu?ller & Kasper, 2000, andSiegel, 2000; or the LFG implementations devel-oped within the ParGram consortium, Butt, King,Nin?o, & Segond, 1999).More recently, we also find genuinely sharedand distributed development of broad-coveragegrammars, and we will use one such initiative as anexample?viz.
an open-source HPSG implementa-tion for Japanese jointly developed between DFKISaarbru?cken (Germany) and YY Technologies(Mountain View, CA)?to demonstrate the techno-logical and methodological challenges present indistributed grammar and system engineering.2 Parallel Distributed GrammarDevelopment?A Case StudyThe Japanese grammar builds on earlier work per-formed jointly between DFKI and the Computa-tional Linguistics Department at Saarland Univer-sity (Germany) within VerbMobil; much like forthe German VerbMobil grammar, two people werecontributing to the grammar in parallel, one build-ing out syntactic analyses, the other charged withintegrating semantic composition into the syntax.This relatively strict separation of responsibilitiesmostly enabled grammarians to serialize incre-mental development of the resource: the syntacti-cian would supply a grammar with extended cov-erage to the semanticist and, at the onset of the fol-lowing iteration, start subsequent work on syntaxfrom the revised grammar.In the DFKI ?
YY cooperation the situation wasquite different.
Over a period of eight months,both partners had a grammarian working on syn-tax and semantics simultaneously on a day-to-day basis; both grammarians were submittingchanges to a joint, version-controlled source repos-itory and usually would start the work day by re-trieving the most recent revisions.
At the sametime, product building and the development ofso-called ?domain libraries?
(structured collectionsof knowledge about a specific domain that is in-stantiated from semantic representations deliveredfrom grammatical analysis) at YY already incorpo-rated the grammar and depended on it for actual,customer-specific contracts.
Due to a continuousdemand for improvements in coverage and analy-sis accuracy, the grammar used in the main productline would be updated from the current develop-ment version about once or twice a week.
Parallelto work on the Japanese grammar (and simultane-ous work on grammars for English and Spanish),both the grammar development environment (theopen-source LKB system; Copestake, 2002) andthe HPSG run-time component powering the YYlinguistic analysis engine (the open-source PETparser; Callmeier, 2002) continued to evolve, asdid the YY-proprietary mapping of meaning repre-sentations extracted from the HPSG grammars intodomain knowledge?all central parts of a complexsystem of interacting components and constraints.As has been argued before (see, for exam-ple, Oepen & Flickinger, 1998), the nature of alarge-scale constraint-based grammar and the sub-tle interactions of lexical and constructional con-straints make it virtually impossible to predict howa change in one part of the grammar affects over-all system behaviour.
A relatively minor repair inone lexical class, numeral adjectives as in ?threebooks were ordered?
for instance, will have the po-tential of breaking the interaction of that class withthe construction deriving named (numeric) entitiesfrom a numeral (e.g.
as in ?three is my favouritenumber?)
or the partitive construction (e.g.
as in?three have arrived already?).
A ripple effect ofa single change can thus corrupt the semanticsproduced for any of these cases and in the con-sequence cause failure or incorrect behaviour inthe back-end system.
In addition to these qual-ity assurance requirements on grammatical cover-age and correctness, the YY application (like mostapplications for grammar-based linguistic analy-sis) utilizes a set of hand-constructed parse rank-ing heuristics that enables the parser to operatein best-first search mode and to return only onereading, i.e.
the analysis that is ranked best by theheuristic component.
The parse ranking machin-ery builds on preferences that are associated withindividual or classes of lexical items and construc-tions.
The set of preferences is maintained in par-allel to the grammar, in a sense providing a layerof performance-oriented annotations over the basicbuilding blocks of the core competence grammar.Without discussing the details of the parse rankingapproach, it creates an additional element of un-certainty in assessing grammar changes: since thepreference for a specific analysis results implic-itly from a series of local preferences (of lexicalitems and constructions contributing to the com-plete derivation), introducing additional elements(i.e.
new local or global ambiguity) into the searchspace and subjecting them to the partial orderingcan quickly skew the overall result.Summing up, the grammar and application engi-neering example presented here illustrates a num-ber of highly typical requirements on the engi-neering environment.
First, all grammarians andsystem engineers participating in the developmentprocess need to keep frequent, detailed, and accu-rate records of a large number of relevant parame-ters, including but not limited to grammatical cov-erage, correctness of syntactic analyses and cor-responding semantic forms, parse selection accu-racy, and overall system performance.
Second, asmodifications to the system as a whole are madedaily?and sometimes several times each day?alldevelopers must be able to assess the impact ofrecent changes and track their effects on all rele-vant parameters; gathering the data and analyzingit must be simple, fast, and automated as much aspossible.
Third, not all modifications (to the gram-mar or underlying software) will result in ?mono-tonic?
or backwards-compatible effects.
A changein the treatment of optional nominal complements,for example, may affect virtually all derivationtrees and render a comparison of results at thislevel uninformative.
At the same time, a primarilysyntactic change of this nature will not cause an ef-fect in associated meaning representations, so thata semantic equivalence test over analyses shouldbe expected to yield an exact match to earlier re-sults.
Hence, the machinery for representation andcomparison of relevant parameters needs to facil-itate user-level specification of informative testsand evolution criteria.
Finally, the metrics used intracking grammar development cannot be isolatedfrom measurements of system resource consump-tion and overall performance (specific propertiesof a grammar may trigger idiosyncrasies or soft-ware bugs in a particular version of the process-ing system); therefore, and to enable exchange ofreference points and comparability of experiments,grammarians and system developers alike shoulduse the same, homogenuous set of relevant param-eters.3 Integrated Competence andPerformance ProfilingThe integrated competence and performance pro-filing methodology and associated engineeringplatform, dubbed [incr tsdb()] (Oepen & Callmeier,2000)1 and reviewed in the remainder of this sec-1See ?http://www.coli.uni-sb.de/itsdb/?for the (draft) [incr tsdb()] user manual, pronunciation rules,and instructions on obtaining and installing the package.tion, was designed to meet al of the requirementsidentified in the DFKI ?
YY case study.
Generallyspeaking, the [incr tsdb()] environment is an in-tegrated package for diagnostics, evaluation, andbenchmarking in practical grammar and systemengineering.
The toolkit implements an approachto grammar development and system optimizationthat builds on precise empirical data and system-atic experimentation, as it has been advocated by,among others, Erbach & Uszkoreit (1990), Erbach(1991), and Carroll (1994).
[incr tsdb()] has beenintegrated with, as of June 2002, nine differentconstraint-based grammar development and pars-ing systems (including both environments in use atYY, i.e.
the LKB and PET), thus providing a pre-standard reference point for a relatively large (andgrowing) community of NLP developers.
The [incrtsdb()] environment builds on the following com-ponents and modules:?
test and reference data stored with annota-tions in a structured database; annotationscan range from minimal information (uniquetest item identifier, item origin, length et al)to fine-grained linguistic classifications (e.g.regarding grammaticality and linguistic phe-nomena presented in an item), as they are rep-resented in the TSNLP test suites, for example(Oepen, Netter, & Klein, 1997);?
tools to browse the available data, identifysuitable subsets and feed them through theanalysis component of processing systemslike the LKB and PET, LiLFeS (Makino,Yoshida, Torisawa, & Tsujii, 1998), TRALE(Penn, 2000), PAGE (Uszkoreit et al, 1994),and others;?
the ability to gather a multitude of precise andfine-grained (grammar) competence and (sys-tem) performance measures?like the num-ber of readings obtained per test item, varioustime and memory usage statistics, ambigu-ity and non-determinism metrics, and salientproperties of the result structures?and storethem in a uniform, platform-independent dataformat as a competence and performance pro-file; and?
graphical facilities to inspect the resultingprofiles, analyze system competence (i.e.grammatical coverage and overgeneration)and performance (e.g.
cpu time and memoryusage, parser search space, constraint solver'&$%Parser 3Parser 2Parser 1Grammar 3Grammar 2Grammar 1TestSet 3TestSet 2TestSet 1ParallelVirtualMachineC and Lisp APIRelationalDBMS Batch ControlStatistics UserInterfaceANSI C Common-Lisp Tcl/TkFigure 1: Rough sketch of [incr tsdb()] architec-ture: the core engine comprises the database man-agement, batch control and statistics component,and the user interface.workload, and others) at variable granulari-ties, aggregate, correlate, and visualize thedata, and compare among profiles obtainedfrom previous grammar or system versions orother processing environments.As it is depicted in Figure 1, the [incr tsdb()]architecture can be broken down into three majorparts: (i) the underlying database management sys-tem (DBMS), (ii) the batch control and statisticskernel (providing a C and Lisp application pro-gram interface to client systems that can be dis-tributed across the network), and (iii) the graphi-cal user interface (GUI).
Although, historically, theDBMS was developed independently and the ker-nel can be operated without the GUI, the full func-tionality of the integrated competence and perfor-mance laboratory?as demonstrated below?onlyemerges from the combination of all three com-ponents.
Likewise, the flexibility of a clearly de-fined API to client systems and its ability to par-allelize batch processing and distribute test runsacross the network have greatly contributed to thesuccess of the package.
The following paragraphsreview some of the fundamental aspects in moredetail, sketch essential functionality, and commenton how they have been exploited in the DFKI ?
YYcooperation.Abstraction over Processors The [incr tsdb()]environment, by virtue of its generalized pro-file format, abstracts over specific processing en-vironments.
While grammar engineers in theDFKI ?
YY collaboration regularly use both theLKB (primarily for interactive development) andPET (mostly for batch testing and the assessmentof results obtained in the YY production envi-ronment), usage of the [incr tsdb()] profile anal-ysis routines in most aspects hides the specificsof the token processor used in obtaining a profile.Both platforms interprete the same typed featurestructure formalism, load the same set of gram-mar source files, and (unless malfunctioning) pro-duce equivalent results.
Using [incr tsdb()], gram-marians can obtain summary views of grammati-cal coverage and overgeneration, inspect relevantsubsets of the available data, break down analysisviews according to various aggregation schemes,and zoom in on specific aggregates or individualtest items as appropriate.
Moreover, processingresults obtained from the (far more efficient) PETparser (that has no visualization or debugging sup-port built in), once recorded as an [incr tsdb()] pro-file, can be used in conjunction with the LKB (con-tingent on the use of identical grammars), therebyfacilitating graphical inspection of parse trees andsemantic formulae.Parallelization of Test Runs The [incr tsdb()] ar-chitecture (see Figure 1) separates the batch con-trol and statistics kernel from what is referred toas client processors (i.e.
parsing systems like theLKB or PET) through an application program inter-face (API) and the Parallel Virtual Machine (PVM;Geist, Bequelin, Dongarra, Manchek, & Sun-deram, 1994) message-passing protocol layer.
Theuse of PVM?in connection with task scheduling,error recovery, and roll-over facilities in the [incrtsdb()] kernel?enables developers to transparentlyparallelize and distribute execution of batch pro-cessing.
At YY, grammarians had a cluster of net-worked Linux compute servers configured as a sin-gle PVM instance, so that execution of a test run?using the efficient PET run-time engine?could becompleted as a matter of a few seconds.
The com-bination of near-instantaneous profile creation and[incr tsdb()] facilities for quick, semi-automated as-sessment of relevant changes (see below) enableddevelopers to pursue a strongly empiricist style ofgrammar engineering, assessing changes and theireffects on actual system behavior in small incre-ments (often many times per hour).Structured Comparison One of the facilitiesthat has proven particularly useful in the dis-tributed grammar engineering setup outlined inSection 2 above is the flexible comparison of com-petence and performance profiles.
The [incr tsdb()]package eases comparison of results on a per-item basis, using an approach similar to Un?xdiff(1), but generalized for structured data sets.By selection of a set of parameters for intersec-tion (and optionally a comparison predicate), theuser interface allows browsing the subset of testitems (and associated results) that fail to matchin the selected properties.
One dimension thatgrammarians found especially useful in intersect-ing profiles is on the number of readings assignedper item?detecting where coverage was lost oradded?and on derivation trees (bracketed struc-tures labeled with rule names and identifiers of lex-ical items) associated with each parser analysis?assessing where analyses have changed.
Addition-ally, using a user-supplied equivalence predicate,the same technique was regularly used at YY totrack the evolution of meaning representations (asthey form the interface from linguistic analysis intothe back-end knowledge processing engine), bothfor all readings and the analysis ranked best by theparse selection heuristics.Zooming and Interactive Debugging Inanalysing a new competence and performanceprofile, grammarians typically start from summaryviews (overall grammatical coverage, say), thensingle out relevant (or suspicious) subsets ofprofile data, and often end up zooming in tothe level of individual test items.
For most [incrtsdb()] analysis views the ?success?
criteria can bevaried according to user decisions: in assessinggrammatical coverage, for example, the scoringfunction can refer to virtually arbitrary profileelements?ranging from the most basic coveragemeasure (assigning at least one reading) to morerefined or application-specific metrics, the produc-tion of a well-formed meaning representation, say.Although the general approach allows output an-notations on the test data (full or partial constituentstructure descriptions, for example), developers sofar have found the incremental, semi-automatedcomparison against earlier results a more adequatemeans of regression testing.
It would appearthat, especially in an application-driven andtightly scheduled engineering situation like theDFKI ?
YY partnership, the pace of evolutionand general lack of locality in changes (see theexamples discussed in Section 2) precludes theconstruction of a static, ?gold-standard?
target forcomparison.
Instead, the structured comparisonfacilities of [incr tsdb()] enable developers toincrementally approximate target results and, even12-sep-2001 (13:24 h) ?
14-feb-2002 (17:14 h)404550556065707580859095Grammatical Coverage (Per Cent)(generated by [incr tsdb()] at 29-jun-2002 (20:49 h))??
????
??????????????????????????
??
?????????
?
?????????
?????????????
??????
?banking???
?trading?12-sep-2001 (13:24 h) ?
14-feb-2002 (17:14 h)0102030405060708090Ambiguity (Average Number of Analyses)(generated by [incr tsdb()] at 29-jun-2002 (20:59 h))???
?
????
??
?
????
??
?
???????
???????
???????
?banking???
?trading?Figure 2: Evolution of grammatical coverage and average ambiguity (number of readings per test item) overa five-month period; ?banking?
and ?trading?
are two data sets (of some 700 and 400 sentences, respectively)of domain data.in a highly dynamic environment where grammarand processing environment evolve in parallel,track changes and identify regression with greatconfidence.4 Looking Back?Quantifying EvolutionOver time, the [incr tsdb()] profile storage accu-mulates precise data on the grammar developmentprocess.
Figure 2 summarizes two aspects ofgrammatical evolution compiled over a five-monthperiod (and representing some 130 profiles thatgrammarians put aside for future reference): gram-matical coverage over two representative samplesof customer data?one for an on-line banking ap-plication, the other from an electronic stock trad-ing domain?is contrasted with the developmentof global ambiguity (i.e.
the average number ofanalyses assinged to each test item).
As shouldbe expected, grammatical coverage on both datasets increases significantly as grammar develop-ment focuses on these domains (?banking?
for thefirst three months, ?trading?
from there on).
Whilethe collection of available profiles, apparently, in-cludes a number of data points corresponding to?failed?
experiments (fairly dramatic losses in cov-erage), the larger picture shows mostly monotonicimprovement in coverage.
As a control experi-ment, the coverage graph includes another datapoint for the ?banking?
data towards the end of thereporting period.
Two months of focussed devel-opment on the ?trading?
domain have not nega-tively affected grammatical coverage on the dataset used earlier.
Corresponding to the (desirable)increase in coverage, the graph on the right of Fig-ure 2 depicts the evolution of grammatical ambi-guity.
As hand-built linguistic grammars put greatemphasis on the precision of grammatical analy-sis and the exclusion of ungrammatical input, theoverall average of readings assigned to each sen-tence varies around relatively small numbers.
Forthe moderately complex email data2 the grammaroften assigns less than ten analyses, rarely morethan a few dozens.
However, not surprisinglythe addition of grammatical coverage comes witha sharp increase in ambiguity (which may indi-cate overgeneration): the graphs in Figure 2 clearlyshow that, once coverage on the ?trading?
data wasabove eighty per cent, grammarians shifted theirengineering focus on ?tightening?
the grammar, i.e.the elimination of spurious ambiguity and overgen-eration (see Siegel & Bender, 2002, for details onthe grammar).Another view on grammar evolution is pre-sented in Figure 3, depicting the ?size?
of theJapanese grammar over the same five-month de-velopment cycle.
Although measuring the size of2Quantifying input complexity for Japanese is a non-trivial task, as the count of the number of input words woulddepend on the approach to string segmentation used in a spe-cific system (the fairly aggressive tokenizer of ChaSen, Asa-hara & Matsumoto, 2000, in our case); to avoid potential forconfusion, we report input complexity in the (overtly system-specific) number of lexical items stipulated by the grammarinstead: around 50 and 80, on average, for the ?banking?
and?trading?
data sets, respectively (as of February 2002).12-sep-2001 (13:24 h) ?
14-feb-2002 (17:14 h)8800900092009400960098001000010200889092949698100102104106Grammar Size(generated by [incr tsdb()] at 30-jun-2002 (16:09 h))??????????????????????????????
?????????????
??
?  ??
types?
rulesFigure 3: Evolution of grammar size (in the num-bers of types, plotted against the left axis, andgrammar rules, plotted against the right axis) overa five-month period.computational grammars is a difficult challenge,for the HPSG framework two metrics suggest them-selves: the number of types (i.e.
the size of thegrammatical ontology) and the number of gram-mar rules (i.e.
the inventory of construction types).As would be expected, both numbers increasemore or less monotonically over the reporting pe-riod, where the shift of focus from the ?banking?into the ?trading?
domain is marked with a sharpincrease in (primarily lexical) types.
Contrastedto the significant gains in grammatical coverage(a relative improvement of more than seventy percent on the ?banking?
data), the increase in gram-mar size is moderate, though: around fifteen andtwenty per cent in the number of types and rules,respectively.5 ConclusionsAt YY and cooperating partners (primarily DFKISaarbru?cken and CSLI Stanford), grammarians(for all languages) as well as developers of both thegrammar development tools and of the productionsystem all used the competence and performanceprofiling environment as part of their daily engi-neering toolbox.
The combination of [incr tsdb()]facilities to parallelize test run processing and abreak-through in client system efficiency (usingthe PET parser; Callmeier, 2002) has created an ex-perimental development environment where gram-marians can obtain near-instantaneous feedback onthe effects of changes they explore.For the Japanese grammar specifically, thegrammar developers at both ends would typicallyspend the first ten to twenty minutes of the day ob-taining fresh profiles for a number of shared testsets and diagnostic corpora, thereby assessing themost recent set of changes through empirical anal-ysis of their effects.
In conjunction with a certainrigor in documentation and communication, it wasthe ability of both partners to regularly, quickly,and semi-automatically monitor the evolution ofthe joint resource with great confidence that hasenabled truly parallel development of a single,shared HPSG grammar across continents.
Withina relatively short time, the partners succeededin adapting an existing grammar to a new genre(email rather than spoken language) and domain(customer service requests rather than appointmentscheduling), greatly extending grammatical cov-erage (from initially around forty to above ninetyper cent on representative customer corpora), andincorporating the grammar-based analysis engineinto a commercial product.
And even though inFebruary 2002, for business reasons, YY decidedto reorganize grammar development for Japanese,the distributed, parallel grammar development ef-fort positively demonstrates that methodologicaland technological advances in constraint-basedgrammar engineering have enabled commercialdevelopment and deployment of broad-coverageHPSG implementations, a paradigm that until re-cently was often believed to still lack the maturityfor real-world applications.AcknowledgementsThe DFKI ?
YY partnership involved a large groupof people at both sites.
We would like to thankKirk Oatman, co-founder of YY and first CEO,and Hans Uszkoreit, Scientific Director at DFKI,for their initiative and whole-hearted support tothe project; it takes vision for both corporate andacademic types to jointly develop an open-sourceresource.
Atsuko Shimada (from Saarland Uni-versity), as part of a two-month internship at YY,has greatly contributed to the preparation of repre-sentative data samples, development of robust pre-processing rules, and extensions to lexical cover-age.
Our colleague and friend Asahara-san (of theNara Advanced Institute of Technology, Japan),co-developer of the open-source ChaSen tokenizerand morphological analyzer for Japanese, was in-strumental in the integration of ChaSen into theYY product and also helped a lot in adapting and(sometimes) fixing tokenization and morphology.ReferencesAsahara, M., & Matsumoto, Y.
(2000).
Extendedmodels and tools for high-performance part-of-speech tagger.
In Proceedings of the 18th In-ternational Conference on Computational Lin-guistics (pp.
21 ?
27).
Saarbru?cken, Germany.Butt, M., King, T. H., Nin?o, M.-E., & Segond, F.(1999).
A grammar writer?s cookbook.
Stan-ford, CA: CSLI Publications.Callmeier, U.
(2002).
Preprocessing and encodingtechniques in PET.
In S. Oepen, D. Flickinger,J.
Tsujii, & H. Uszkoreit (Eds.
), Collabora-tive language engineering.
A case study in ef-ficient grammar-based processing.
Stanford,CA: CSLI Publications.
(forthcoming)Carroll, J.
(1994).
Relating complexity to practi-cal performance in parsing with wide-coverageunification grammars.
In Proceedings of the32nd Meeting of the Association for Computa-tional Linguistics (pp.
287 ?
294).
Las Cruces,NM.Copestake, A.
(2002).
Implementing typed fea-ture structure grammars.
Stanford, CA: CSLIPublications.Erbach, G. (1991).
An environment for exper-imenting with parsing strategies.
In J. My-lopoulos & R. Reiter (Eds.
), Proceedings ofIJCAI 1991 (pp.
931 ?
937).
San Mateo, CA:Morgan Kaufmann Publishers.Erbach, G., & Uszkoreit, H. (1990).
Grammarengineering.
Problems and prospects (CLAUSReport # 1).
Saarbru?cken, Germany: Compu-tational Linguistics, Saarland University.Flickinger, D., Copestake, A., & Sag, I.
A.
(2000).HPSG analysis of English.
In W.
Wahlster(Ed.
), Verbmobil.
Foundations of speech-to-speech translation (Artificial Intelligence ed.,pp.
321 ?
330).
Berlin, Germany: Springer.Geist, A., Bequelin, A., Dongarra, J., Manchek, W.J.
R., & Sunderam, V.
(Eds.).
(1994).
PVM ?parallel virtual machine.
A users?
guide and tu-torial for networked parallel computing.
Cam-bridge, MA: The MIT Press.Makino, T., Yoshida, M., Torisawa, K., & Tsu-jii, J.
(1998).
LiLFeS ?
towards a practicalHPSG parser.
In Proceedings of the 17th In-ternational Conference on Computational Lin-guistics and the 36th Annual Meeting of theAssociation for Computational Linguistics (pp.807 ?
11).
Montreal, Canada.Mu?ller, S., & Kasper, W. (2000).
HPSG analy-sis of German.
In W. Wahlster (Ed.
), Verbmo-bil.
Foundations of speech-to-speech transla-tion (Artificial Intelligence ed., pp.
238 ?
253).Berlin, Germany: Springer.Oepen, S., & Callmeier, U.
(2000).
Measure formeasure: Parser cross-fertilization.
Towardsincreased component comparability and ex-change.
In Proceedings of the 6th InternationalWorkshop on Parsing Technologies (pp.
183 ?194).
Trento, Italy.Oepen, S., & Flickinger, D. P. (1998).
Towardssystematic grammar profiling.
Test suite tech-nology ten years after.
Journal of ComputerSpeech and Language, 12 (4) (Special Issue onEvaluation), 411 ?
436.Oepen, S., Netter, K., & Klein, J.
(1997).
TSNLP?
Test Suites for Natural Language Process-ing.
In J. Nerbonne (Ed.
), Linguistic Databases(pp.
13 ?
36).
Stanford, CA: CSLI Publica-tions.Penn, G. (2000).
Applying constraint handlingrules to HPSG.
In Proceedings of the first in-ternational conference on computational logic(pp.
51 ?
68).
London, UK.Siegel, M. (2000).
HPSG analysis of Japanese.
InW.
Wahlster (Ed.
), Verbmobil.
Foundations ofspeech-to-speech translation (Artificial Intelli-gence ed., pp.
265 ?
280).
Berlin, Germany:Springer.Siegel, M., & Bender, E. M. (2002).
Efficientdeep processing of japanese.
In Proceedingsof the 19th International Conference on Com-putational Linguistics.
Taipei, Taiwan.Uszkoreit, H., Backofen, R., Busemann, S., Di-agne, A. K., Hinkelman, E. A., Kasper, W.,Kiefer, B., Krieger, H.-U., Netter, K., Neu-mann, G., Oepen, S., & Spackman, S. P.(1994).
DISCO ?
an HPSG-based NLPsystem and its application for appointmentscheduling.
In Proceedings of the 15th Interna-tional Conference on Computational Linguis-tics.
Kyoto, Japan.
