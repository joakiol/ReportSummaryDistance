Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 36?43,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsCapturing the stars: predicting ratings for service and product reviewsNarendra Gupta, Giuseppe Di Fabbrizio and Patrick HaffnerAT&T Labs - Research, Inc.Florham Park, NJ 07932 - USA{ngupta,pino,haffner}@research.att.comAbstractBloggers, professional reviewers, and con-sumers continuously create opinion?rich webreviews about products and services, with theresult that textual reviews are now abundant onthe web and often convey a useful overall rat-ing (number of stars).
However, an overall rat-ing cannot express the multiple or conflictingopinions that might be contained in the text,or explicitly rate the different aspects of theevaluated entity.
This work addresses the taskof automatically predicting ratings, for givenaspects of a textual review, by assigning a nu-merical score to each evaluated aspect in thereviews.
We handle this task as both a re-gression and a classification modeling prob-lem and explore several combinations of syn-tactic and semantic features.
Our results sug-gest that classification techniques perform bet-ter than ranking modeling when handling eval-uative text.1 IntroductionAn abundance of service and products reviews aretoday available on the Web.
Bloggers, professionalreviewers, and consumers continuously contributeto this rich content both by providing text reviewsand often by assigning useful overall ratings (num-ber of stars) to their overall experience.
However,the overall rating that usually accompanies onlinereviews cannot express the multiple or conflictingopinions that might be contained in the text, or ex-plicitly rate the different aspects of the evaluatedentity.
For example, a restaurant might receive anoverall great evaluation, while the service mightbe rated below average due to slow and discourte-ous wait staff.
Pinpointing opinions in documents,and the entities being referenced, would provide afiner?grained sentiment analysis and a solid foun-dation to automatically summarize evaluative text,but such a task becomes even more challengingwhen applied to a generic domain and with unsu-pervised methods.
Some significant contributionsby Hu and Liu (2004), Popescu and Etzioni (2005),and Carenini et al (2006) illustrate different tech-niques to find and measure opinion orientation intext documents.
Other work in sentiment analysis(often referred as opinion mining) has explored sev-eral facets of the problem, ranging from predictingbinary ratings (e.g., thumbs up/down) (Turney, 2002;Pang et al, 2002; Dave et al, 2003; Yu and Hatzivas-siloglou, 2003; Pang and Lee, 2004; Yi and Niblack,2005; Carenini et al, 2006), to more detailed opin-ion analysis methods predicting multi?scale ratings(e.g., number of stars) (Pang and Lee, 2005; Sny-der and Barzilay, 2007; Shimada and Endo, 2008;Okanohara and Tsujii, 2005).This paper focuses on multi?scale multi?aspectrating prediction for textual reviews.
As mentionedbefore, textual reviews are abundant, but when try-ing to make a buy decision on a specific productor service, getting sufficient and reliable informa-tion can be a daunting and time consuming task.On one hand, a single overall rating does not pro-vide enough information and could be unreliable, ifnot supported over a large number of independentreviews/ratings.
From another standpoint, readingthrough a large number of textual reviews in orderto infer the aspect ratings could be quite time con-36suming, and, at the same time, the outcome of theevaluation could be biased by the reader?s interpre-tation.
In this work, instead of a single overall rat-ing, we propose to provide ratings for multiple as-pects of the product/service.
For example, in thecase of restaurant reviews, we consider ratings forfive aspects: food, atmosphere, value, service andoverall experience.
In Lu et al (2009) such aspectratings are called rated aspect summaries, in Shi-mada and Endo (2008) they have been referred to asseeing stars and in Snyder and Barzilay (2007) theyare referred to as multi?aspect ranking.
We use su-pervised learning methods to train predictive modelsand use a specific decoding method to optimize theaspect rating assignment to a review.In the rest of this paper, we overview the previouswork in this research area in Section 2.
We describethe corpus used in the experiments in Section 3.
InSection 4 we present various learning algorithms weexperimented with.
Section 5 explains our experi-mental setup, while in Section 6 we provide analy-sis of our experimental results.
Section 7 presentsdetails of modeling and exploiting interdependenceamong aspect ratings to boost the predictive perfor-mance.
Finally, we describe the future work in Sec-tion 8 and report the concluding remarks in Section9.2 Related workPrevious work in sentiment analysis (Turney, 2002;Pang et al, 2002; Dave et al, 2003; Yu and Hatzivas-siloglou, 2003; Pang and Lee, 2004; Yi and Niblack,2005; Carenini et al, 2006) used different informa-tion extraction and supervised classification meth-ods to detect document opinion polarity (positive vs.negative).By conducting a limited experiment with two sub-jects, Pang and Lee (2005) demonstrated that hu-mans can discern more grades of positive or neg-ative judgments by accurately detecting small dif-ferences in rating scores by just looking at reviewtext.
In a five?star schema, for instance, the subjectswere able to perfectly distinguish rating differencesof three notches or 1.5 stars and correctly perceivedifferences of one star with an average of 83% accu-racy.
This insight confirms that a five?star scale im-proves the evaluative information and is perceivedwith the right discriminative strength by the users.Pang and Lee applied supervised and semi?supervised classification techniques, in addition tolinear, -insensitive SVM regression methods, topredict the overall ratings of movie reviews in threeand four?class star rating schemes.
In the booksreview domain, Okanohara and Tsujii (2005) showa similar approach with comparable results.
Boththese contributions consider only overall ratings,which could be sufficient to describe sentiment formovie and book reviews.
Two recent endeavors,Snyder and Barzilay (2007) for the restaurants do-main, and Shimada and Endo (2008) for videogames reviews, exploit multi?aspect, multiple rat-ing modeling.
Snyder and Barzilay (2007) assumeinter?dependencies among the aspect ratings andcapture the relationship between the ratings via theagreement relation.
The agreement relation de-scribes the likelihood that the user will express thesame rating for all the rated aspects.
Interestingly,Snyder and Barzilay (2007) show that modeling as-pect rating dependencies helps to reduce the rankloss by keeping in consideration the contributions ofthe opinion strength of the single aspects referredto in the review.
They incorporated informationabout the aspect rating dependencies in a regressionmodel and minimized the loss (overall grief ) dur-ing decoding.
Shimada and Endo (2008) exploitsa more traditional supervised machine learning ap-proach where features such as word unigrams andfrequency counts are used to train classification andregression models.
As detailed in Section 4, our ap-proach is similar to (Snyder and Barzilay, 2007) interms of review domain and algorithms, but we im-prove on their performances by optimizing classifi-cation predictions.3 Reviews corpusLabeled data containing textual reviews and aspectratings are rarely available.
For this work, reviewswere mined from the we8there.com websitesaround the end of 2008. we8there.com is oneof the few websites, where, besides textual reviews,numerical ratings for different aspects of restaurantsare also provided.
Aspects used for rating on thissite are: food, service, atmosphere, value and over-all experience.
Ratings are given on a scale from 137to 5; for example, reviewers posting opinions wereasked to rank their overall experience by the follow-ing prompt: ?On a scale of 1 (poor) to 5 (excel-lent), please rate your dining experience?, and thenenter a textual description by the prompt: ?Pleasedescribe your experience (30 words minimum)?.
Atthe time of mining, this site had reviews of about3,800 restaurants with an average of two reviewsper restaurant containing around eight sentences perreview.
A more detailed description is reported inTable 1.
Table 2 shows review ratings distributionover the aspects.
Rating distributions are evidentlyskewed toward high ratings with 70% or more re-views appraised as excellent (rank 5) or above aver-age (rank 4).Restaurants 3,866Reviewers 4,660Reviews 6,823Average reviews per restaurant 1.76Number of sentences 58,031Average sentences per review 8.51Table 1: Restaurant review corpusRating 1 2 3 4 5Atmosphere 6.96 7.81 14.36 23.70 47.18Food 8.24 6.72 9.86 18.53 56.65Value 9.37 7.57 13.61 23.27 46.18Service 11.83 6.12 11.91 22.00 48.14Overall 10.48 8.19 10.17 20.47 50.69Table 2: Restaurant review ratings distribution per aspect4 Learning algorithmsIn this section we review machine learning ap-proaches that can predict ordinal ratings from textualdata.
The goal is ordinal regression, which differsfrom traditional numeric regression because the tar-gets belong to a discrete space, but also differs fromclassification as one wants to minimize the rank lossrather than the classification error.
The rank loss isthe average difference between actual and predictedratings and is defined asRankLoss =1NN?i(|rai ?
rpi |)where rai and rpi are actual and predicted ratingsrespectively for the instance i, and N is the numberof considered reviews.
There are several possibleapproaches to such a regression problem.1.
The most obvious approach is numeric regres-sion.
It is implemented with a neural networktrained using the back?propagation algorithm.2.
Ordinal regression can also be implementedwith multiple thresholds (r ?
1 thresholds areused to split r ranks).
This is implementedwith a Perceptron based ranking model calledPRank (Crammer and Singer, 2001).3.
Since rating aspects with values 1, 2, 3, 4 and5 is an ordinal regression problem it can alsobe interpreted as a classification problem, withone class per possible rank.
In this interpreta-tion, ordering information is not directly usedto help classification.
Our implementation usesbinary one-vs-all Maximum Entropy (MaxEnt)classifiers.
We will see that this very simpleapproach can be extended to handle aspect in-terdependency, as presented in section 7.In order to provide us with a broad range of ratingprediction strategies, we experimented with a nu-merical regression technique viz.
neural network, anordinal regression technique viz.
PRank algorithm,and a classification technique viz.
MaxEnt classi-fiers.
Their implementations are straightforward andthe run?time highly efficient.
After selecting a strat-egy from the previous list, one could consider moreadvanced algorithms described in Section 8.5 Experimental setupTo predict aspect ratings of restaurants from theirtextual reviews we used the reviews mined from thewe8there.com website to train different regres-sion and classification models as outlined in Sec-tion 4.
In each of our experiments, we randomlypartitioned the data into 90% for training and 10%for testing.
This ensures that the distributions intraining and test data are identical.
All the resultsquoted in this paper are averages of 10?fold cross?validation over 6,823 review examples.
We con-ducted repeatedly the same experiment on 10 differ-ent training/test partitions and computed the averagerank loss over all the test partitions.38Figure 1 illustrates the training process whereeach aspect is described by a separate predictivemodel.Figure 1: Predictive model trainingWe introduce the following notation that will behelpful in further discussion.
There are m aspects.For our data m is 5.
Each aspect can have an inte-ger rating from 1 to k. Once again, for our data kis 5.
Each review text document t can have ratingsr, which is a vector of m integers ranging 1 to k(bold faced letters indicate vectors).
Using the train-ing data (t1, r1)..(ti, ri)..(tn, rn) we train m ratingpredictors Rj(ti), one for each aspect j.
Given textti predictor Rj outputs the most likely rating l forthe aspect j.
In these experiments, we treated aspectrating predictors as independent of each other.
Foreach rated aspect, predictor models were trained in-dependently and were used independently to predictratings for each aspect.5.1 Feature SelectionWe experimented with different combinations offeatures, including word unigrams, bigrams, wordchunks, and parts?of?speech (POS) chunks.
The as-sumption is that bag?of?unigrams capture the ba-sic word statistic and that bigrams take into accountsome limited word context.
POS chunks and wordchunks discriminate the use of words in the con-text (e.g., a simple form word sense disambigua-tion) and, at the same time, aggregate co?occurringwords (e.g., collocations), such as saute?ed onions,buffalo burger, etc.Most of the web?based reviews do not usuallyprovide fine?grained aspect ratings of products orservices, however, they often give an overall ratingevaluation.
We therefore also experimented with theoverall rating as an input feature to predict the morespecific aspect ratings.
Results of our experimentsare shown in Table 3.Aspects Uni- Bi- Word Word Unigram gram Chunks Chunks gramPOS OverallChunks RatingAtmosphere 0.740 0.763 0.789 0.783 0.527Food 0.567 0.571 0.596 0.588 0.311Value 0.703 0.725 0.751 0.743 0.406Service 0.627 0.640 0.651 0.653 0.377overall 0.548 0.559 0.577 0.583Average 0.637 0.652 0.673 0.670 0.405Table 3: Average ranking losses using MaxEnt classifierwith different feature setsReview sentences<s>Poor service made the lunch unpleasant.</s><s>The staff was unapologetic about their mistakes theyjust didn?t seem to care.</s><s>For example the buffalo burger I ordered with sauteedonions and fries initially was served without either.</s><s> The waitress said she?d bring out the onions but hadI waited for them before eating the burger the meat wouldhave been cold.</s><s>Other examples of the poor service were that thewaitress forgot to bring out my soup when she brought outmy friend?s salad and we had to repeatedly ask to get ourwater glasses refilled.</s><s> When asked how our meal was I did politely mention mydissatisfaction with the service but the staff person?sresponse was silence not even a simple I m sorry.</s><s>I won?t return.
</s>Word Chunkspoor service made lunch unpleasantstaff unapologetic mistakes n?t careexample buffalo burger ordered sauteed onions fries servedwaitress said bring onions waited eating burger meat coldother examples poor service waitress forgot bringsoup brought friend salad repeatedly ask to get waterglasses refilledasked meal politely mention dissatisfaction servicestaff person response silence not simple sorryn?t returnParts-of-speech ChunksNNP NN VBD NN JJNN JJ NNS RB VBNN NN NN VBD NN NNS NNS VBNNN VBD VB NNS VBD VBG NN NN JJJJ NNS JJ NN NN NN VB NN VBD NN NN RB VB TO VB NN VBZ VBNVBD NN RB VB NN NN NN NN NN NN RB JJ JJRB VBTable 4: Example of reviews and extracted word chunksUnigram and bigram features refer to unigramwords and bigram words occurring more than 3times in the training corpus.
Word chunks are ob-tained by only processing Noun (NP), Verb (VP) andAdjective (ADJP) phrases in the review text.
We re-moved modals and auxiliary verbs form VPs, pro-nouns from NPs and we broke the chunks containingconjunctions.
Table 4 shows an example of extractedword and parts?of?speech chunks from review text.As can be seen, word chunks largely keep the infor-mation bearing chunks phrases and remove the rest.Parts?of?speech chunks are simply parts?of?speech39of word chunks.In spite of richness of word and parts-of-speech,chunks models using word unigrams perform thebest.
We can attribute this to the data sparseness,never?the?less, this results is in line with the find-ings in Pang et al (2002).
Last column of Table 3clearly shows that use of overall rating as input fea-ture significantly improves the performance.
Clearlythis validates the intuition that aspect ratings arehighly co?related with overall ratings.For the remaining experiments, we used only theunigram words as features of the review text.
Sinceoverall ratings given by reviewers may contain theirbiases and since they may not always be available,we did not use them as input features.
Our hopeis that even though we train the predictors using re-viewers provided aspect ratings, learned models willbe able to predict aspect ratings that depend only onthe review text and not on reviewer?s biases.5.2 ResultsTable 5 shows the results of our evaluation.
Eachrow in this table reports average rank loss of fourdifferent models for each aspect.
The baseline rankloss is computed by setting the predicted rank for alltest examples to 5, as it is the most frequently occur-ring rank in the training data (see also Table 2).
Asshown in Table 5, the average baseline rank loss isgreater than one.
The third column shows the resultsfrom the neural network?based numeric regression.The fourth column corresponds to the Perceptron?based PRank algorithm.
The MaxEnt classificationresults appear in the last column.
For these results,we also detail the standard deviation over the 10cross?validation trials.Aspects Base- Back- Percep- MaxEntline Prop.
tronAtmosphere 1.036 0.772 0.930 0.740 ?
0.022Food 0.912 0.618 0.739 0.567?
0.033Value 1.114 0.740 0.867 0.703?
0.028Service 1.116 0.708 0.851 0.627?
0.033Overall 1.077 0.602 0.756 0.548?
0.026Average 1.053 0.694 0.833 0.637?
0.020Table 5: Average ranking losses using different predictivemodels6 AnalysisAs can be seen in table Table 5, Atmosphere andValue are the worst performers.
This is caused bythe missing textual support for these aspects in thetraining data.
Using manual examination of smallnumber of examples, we found that only 62% ofuser given ratings have supporting text for ratingsof these aspects in the reviews.For example, in Figure 2 the first review clearlyexpresses opinions about food, service and atmo-sphere (under appall of cigarette smoke), but there isno evidence about value which is ranked three, twonotches above the other aspects.
Similarly, the sec-ond review is all about food without any referenceto service rated two notches above the other aspects,or atmosphere or value.Because of this reason, we do not expect any pre-dictive model to do much better than 62% accuracy.Manual examination of a small number of examplesalso showed that 55% of ratings predicted by Max-Ent models are supported by the review text.
This is89% of 62% (a rough upper bound) and can be con-sidered satisfactory given small data set and differ-ences among reviewers rating preference.
One wayto boost the predictive performance would be to firstdetermine if there is a textual support for an aspectrating, and use only the supported aspect ratings fortraining and evaluation of the models.
This however,will require labeled data that we tried to avoid in thiswork.Figure 2: Example of ratings with partial support in thetext reviewTo our surprise, MaxEnt classification, although itminimizes a classification error, performs best even40when evaluated using rank loss.
As can be noticed,the performance difference over the second best ap-proach (back?propagation) usually exceeds the stan-dard deviation.MaxEnt results are also comparable to those pre-sented in Snyder and Barzilay (2007) using theGood Grief algorithm.
Snyder and Barzilay (2007)also used data from the we8there.com website.While we are using the same data source, notethe following differences: (i) Snyder and Barzilay(2007) used only 4,488 reviews as opposed to the6,823 reviews used in our work; (ii) our results areaveraged over a 10 fold cross validation.
As shownwith the baseline results reported in Table 6, the im-pact on performance that can be attributed to thesedifferences is small.
The most significant number,which should minimize the impact of data discrep-ancy, is the improvement over baseline (labeled as?gain over baseline?
in Table 6).
In that respect,our MaxEnt classification?based approach outper-forms Good Grief for every aspect.
Note also that,while we trained 5 independent predictors (one foreach aspect) using only word unigrams as features,the Good Grief algorithm additionally modeled theagreements among aspect ratings and used the pres-ence/absence of opposing polarity words in reviewsas additional features.Our results Snyder and Barzilay(2007)Aspects Base- Max Gain Base- Good Gainline Ent.
over line Grief overBase- Base-line lineAtmosphere 1.039 0.740 0.299 1.044 0.774 0.270Food 0.912 0.567 0.344 0.848 0.534 0.314Value 1.114 0.703 0.411 1.030 0.644 0.386Service 1.116 0.627 0.489 1.056 0.622 0.434Overall 1.077 0.548 0.529 1.028 0.632 0.396Table 6: Comparison of rank loss obtained from MaxEntclassification and those reported in Snyder and Barzilay(2007)7 Modeling interdependence among aspectratingsInspired by these observations, we also trained Max-Ent classifiers to predict pair?wise absolute differ-ences in aspect ratings.
Since the difference in rat-ings of any two aspects can only be 0,1,2,3 or 4,there are 5 classes to predict.
For each test exam-ple, MaxEnt classifiers output the posterior proba-bility to observe a class given an input example.
Inour approach, we use these probabilities to computethe best joint assignment of ratings to all aspects.More specifically, in our modified algorithm we use2 types of classifiers.?
Rating predictors - Given the text ti, our clas-sifiers Rj(ti) output vectors pi consisting ofprobabilities pil for text ti having a rating l forthe aspect j.?
Difference predictors - These correspond toclassifiers Dj,k(ti) which output vectors pij,k .Elements of these vectors are the probabilitiesthat the difference between ratings of aspects jand k is 0,1,2,3 and 4, respectively.
While jranges from 1 to m, k ranges from 1 to j ?
1.Thus, we trained a total of m(m ?
1)/2 = 10difference predictors.To predict aspect ratings for a given review textti we use both rating predictors and difference pre-dictors and generate output probabilities.
We thenselect the most likely values of ri for text ti that sat-isfies the probabilistic constraints generated by thepredictors.
More specifically:ri = argmaxr?Rm?j=1log(pirj ) +m?j=1j?k=1log(pij,k|rj?rk|)R is the set of all possible ratings assignments toall aspects.
In our case it contains 55 (3,125) tuples.tuples in our case.
Like Snyder and Barzilay (2007),we also experimented with additional features in-dicating presence of positive and negative polaritywords in the review text.
Besides unigrams in thereview text, we also used 3 features: the counts ofpositive and negative polarity words and their dif-ferences.
Polarity labels are obtained from a dictio-nary of about 700 words.
This dictionary was cre-ated by first collecting words used as adjectives in acorpus of un?related review text.
We then retainedonly those words in the dictionary that, in a contextfree manner generally conveyed positive or negativeevaluation of any object, event or situation.
Some41examples of negative words are awful, bad, bor-ing, crude, disappointing, horrible, worst, worth-less, yucky and some examples of positive wordsare amazing, beautiful, delightful, good, impecca-ble, lovable, marvelous, pleasant, recommendable,sophisticated, superb, wonderful, wow.
Table 7 firstshows gains obtained from using difference predic-tors, and then gains from using polarity word fea-tures in addition to these difference predictors.Aspects MaxEnt + Difference + Polaritypredictor featuresAtmosphere 0.740 0.718 0.707Food 0.567 0.552 0.547Value 0.703 0.695 0.685Service 0.627 0.627 0.617Overall 0.548 0.547 0.528Average 0.637 0.628 0.617Table 7: Improved rank loss obtained by using differencepredictors and polarity word features8 Future WorkWe have presented 3 algorithms chosen for theirsimplicity of implementation and run time effi-ciency.
The results suggest that our classification?based approach performs better than numeric or or-dinal regression approaches.
Our next step is to ver-ify these results with the more advanced algorithmsoutlined below.1.
For many numeric regression problems,(boosted) classification trees have shown goodperformance.2.
Several multi?threshold implementations ofSupport Vector Ordinal Regression are com-pared in Chu and Keerthi (2005).
While theyare more principled than the Perceptron?basedPRank, their implementation is significantlymore complex.
A simpler approach that per-forms regression using a single classifier ex-tracts extended examples from the original ex-amples (Li and Lin, 2007).3.
Among classification?based approaches,nested binary classifiers have been pro-posed (Frank and Hall, 2001) to take intoaccount the ordering information, but theprediction procedure based on classifier scoredifference is ad?hoc.9 ConclusionsTextual reviews for different products and servicesare abundant.
Still, when trying to make a buy deci-sion, getting sufficient and reliable information canbe a daunting task.
In this work, instead of a sin-gle overall rating we focus on providing ratings formultiple aspects of the product/service.
Since mosttextual reviews are rarely accompanied by multipleaspect ratings, such ratings must be deduced frompredictive models.
Several authors in the past havestudied this problem using both classification and re-gression models.
In this work we show that eventhough the aspect rating problem seems like a re-gression problem, maximum entropy classificationmodels perform the best.
Results also show a stronginter?dependence in the way users rate different as-pects.AcknowledgmentsWe thank Remi Zajac and his team for their support.ReferencesCarenini, Giuseppe, Raymond T. Ng, and AdamPauls.
2006.
Interactive multimedia summaries ofevaluative text.
In Proceedings of Intelligent UserInterfaces (IUI).
ACM Press, pages 124?131.Chu, Wei and S. Sathiya Keerthi.
2005.
New ap-proaches to support vector ordinal regression.
InProceedings of the 22nd International Conferenceon Machine Learning.
Bonn, Germany, pages145?152.Crammer, Koby and Yoram Singer.
2001.
Prank-ing with ranking.
In Advances in Neural Infor-mation Processing Systems 14.
MIT Press, pages641?647.Dave, Kushal, Steve Lawrence, and David M. Pen-nock.
2003.
Mining the peanut gallery: Opinionextraction and semantic classification of productreviews.
In WWW ?03: Proceedings of the 12thInternational Conference on World Wide Web.ACM, New York, NY, USA, pages 519?528.Frank, Eibe and Mark Hall.
2001.
A simple ap-proach to ordinal classification.
In Proceedings42of the Twelfth European Conference on MachineLearning.
Springer-Verlag, Berlin, pages 145?156.Hu, Minqing and Bing Liu.
2004.
Mining and sum-marizing customer reviews.
In KDD ?04: Pro-ceedings of the 10th ACM SIGKDD InternationalConference on Knowledge Discovery and DataMining.
ACM, New York, NY, USA, pages 168?177.Li, Ling and Hsuan-Tien Lin.
2007.
Ordinal re-gression by extended binary classification.
InB.
Scho?lkopf, J. C. Platt, and T. Hofmann, edi-tors, Advances in Neural Information ProcessingSystems 19.
MIT Press, pages 865?872.Lu, Yue, ChengXiang Zhai, and Neel Sundaresan.2009.
Rated aspect summarization of short com-ments.
In WWW ?09: Proceedings of the 18thInternational Conference on World Wide Web.ACM, New York, NY, USA, pages 131?140.Okanohara, Daisuke and Jun-ichi Tsujii.
2005.
As-signing polarity scores to reviews using machinelearning techniques.
In Robert Dale, Kam-FaiWong, Jian Su, and Oi Yee Kwong, editors, IJC-NLP.
Springer, volume 3651 of Lecture Notes inComputer Science, pages 314?325.Pang, Bo and Lillian Lee.
2004.
A sentimentaleducation: Sentiment analysis using subjectivitysummarization based on minimum cuts.
In Pro-ceedings of the Association for ComputationalLinguistics (ACL).
pages 271?278.Pang, Bo and Lillian Lee.
2005.
Seeing stars: Ex-ploiting class relationships for sentiment catego-rization with respect to rating scales.
In Proceed-ings of the Association for Computational Lin-guistics (ACL).
pages 115?124.Pang, Bo, Lillian Lee, and ShivakumarVaithyanathan.
2002.
Thumbs up?
Senti-ment classification using machine learningtechniques.
In Proceedings of the Conferenceon Empirical Methods in Natural LanguageProcessing (EMNLP).
pages 79?86.Popescu, Ana-Maria and Oren Etzioni.
2005.
Ex-tracting product features and opinions from re-views.
In Proceedings of the Human LanguageTechnology Conference and the Conference onEmpirical Methods in Natural Language Process-ing (HLT/EMNLP).Shimada, Kazutaka and Tsutomu Endo.
2008.
See-ing several stars: A rating inference task for a doc-ument containing several evaluation criteria.
InAdvances in Knowledge Discovery and Data Min-ing, 12th Pacific-Asia Conference, PAKDD 2008.Springer, Osaka, Japan, volume 5012 of LectureNotes in Computer Science, pages 1006?1014.Snyder, Benjamin and Regina Barzilay.
2007.
Mul-tiple aspect ranking using the Good Grief algo-rithm.
In Proceedings of the Joint Human Lan-guage Technology/North American Chapter of theACL Conference (HLT-NAACL).
pages 300?307.Turney, Peter.
2002.
Thumbs up or thumbsdown?
Semantic orientation applied to unsuper-vised classification of reviews.
In Proceedingsof the Association for Computational Linguistics(ACL).
pages 417?424.Yi, Jeonghee and Wayne Niblack.
2005.
Senti-ment mining in WebFountain.
In Proceedings ofthe International Conference on Data Engineer-ing (ICDE).Yu, Hong and Vasileios Hatzivassiloglou.
2003.
To-wards answering opinion questions: Separatingfacts from opinions and identifying the polarity ofopinion sentences.
In Proceedings of the Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP).43
