MATCHKiosk: A Multimodal Interactive City GuideMichael JohnstonAT&T Research180 Park AvenueFlorham Park, NJ 07932johnston@research.att.comSrinivas BangaloreAT&T Research180 Park AvenueFlorham Park, NJ 07932srini@research.att.comAbstractMultimodal interfaces provide more flexible andcompelling interaction and can enable public infor-mation kiosks to support more complex tasks fora broader community of users.
MATCHKiosk isa multimodal interactive city guide which providesusers with the freedom to interact using speech,pen, touch or multimodal inputs.
The system re-sponds by generating multimodal presentations thatsynchronize synthetic speech with a life-like virtualagent and dynamically generated graphics.1 IntroductionSince the introduction of automated teller machinesin the late 1970s, public kiosks have been intro-duced to provide users with automated access toa broad range of information, assistance, and ser-vices.
These include self check-in at airports, ticketmachines in railway and bus stations, directions andmaps in car rental offices, interactive tourist and vis-itor guides in tourist offices and museums, and morerecently, automated check-out in retail stores.
Themajority of these systems provide a rigid structuredgraphical interface and user input by only touch orkeypad, and as a result can only support a smallnumber of simple tasks.
As automated kiosks be-come more commonplace and have to support morecomplex tasks for a broader community of users,they will need to provide a more flexible and com-pelling user interface.One major motivation for developing multimodalinterfaces for mobile devices is the lack of a key-board or mouse (Oviatt and Cohen, 2000; Johnstonand Bangalore, 2000).
This limitation is also true ofmany different kinds of public information kioskswhere security, hygiene, or space concerns make aphysical keyboard or mouse impractical.
Also, mo-bile users interacting with kiosks are often encum-bered with briefcases, phones, or other equipment,leaving only one hand free for interaction.
Kiosksoften provide a touchscreen for input, opening upthe possibility of an onscreen keyboard, but thesecan be awkward to use and occupy a considerableamount of screen real estate, generally leading to amore moded and cumbersome graphical interface.A number of experimental systems have inves-tigated adding speech input to interactive graphi-cal kiosks (Raisamo, 1998; Gustafson et al, 1999;Narayanan et al, 2000; Lamel et al, 2002).
Otherwork has investigated adding both speech and ges-ture input (using computer vision) in an interactivekiosk (Wahlster, 2003; Cassell et al, 2002).We describe MATCHKiosk, (Multimodal AccessTo City Help Kiosk) an interactive public infor-mation kiosk with a multimodal interface whichprovides users with the flexibility to provide in-put using speech, handwriting, touch, or compositemultimodal commands combining multiple differ-ent modes.
The system responds to the user by gen-erating multimodal presentations which combinespoken output, a life-like graphical talking head,and dynamic graphical displays.
MATCHKioskprovides an interactive city guide for New Yorkand Washington D.C., including information aboutrestaurants and directions on the subway or metro.It develops on our previous work on a multimodalcity guide on a mobile tablet (MATCH) (Johnstonet al, 2001; Johnston et al, 2002b; Johnston et al,2002a).
The system has been deployed for testingand data collection in an AT&T facility in Wash-ington, D.C. where it provides visitors with infor-mation about places to eat, points of interest, andgetting around on the DC Metro.2 The MATCHKioskThe MATCHKiosk runs on a Windows PC mountedin a rugged cabinet (Figure 1).
It has a touch screenwhich supports both touch and pen input, and alsocontains a printer, whose output emerges from a slotbelow the screen.
The cabinet alo contains speak-ers and an array microphone is mounted above thescreen.
There are three main components to thegraphical user interface (Figure 2).
On the right,there is a panel with a dynamic map display, aclick-to-speak button, and a window for feedbackon speech recognition.
As the user interacts withthe system the map display dynamically pans andzooms and the locations of restaurants and otherpoints of interest, graphical callouts with informa-tion, and subway route segments are displayed.
InFigure 1: Kiosk Hardwarethe top left there is a photo-realistic virtual agent(Cosatto and Graf, 2000), synthesized by concate-nating and blending image samples.
Below theagent, there is a panel with large buttons which en-able easy access to help and common functions.
Thebuttons presented are context sensitive and changeover the course of interaction.Figure 2: Kiosk InterfaceThe basic functions of the system are to enableusers to locate restaurants and other points of inter-est based on attributes such as price, location, andfood type, to request information about them suchas phone numbers, addresses, and reviews, and toprovide directions on the subway or metro betweenlocations.
There are also commands for panning andzooming the map.
The system provides users witha high degree of flexibility in the inputs they usein accessing these functions.
For example, whenlooking for restaurants the user can employ speeche.g.
find me moderately priced italian restaurantsin Alexandria, a multimodal combination of speechand pen, e.g.
moderate italian restaurants in thisarea and circling Alexandria on the map, or solelypen, e.g.
user writes moderate italian and alexan-dria.
Similarly, when requesting directions they canuse speech, e.g.
How do I get to the Smithsonian?,multimodal, e.g.
How do I get from here to here?and circling or touching two locations on the map,or pen, e.g.
in Figure 2 the user has circled a loca-tion on the map and handwritten the word route.System output consists of coordinated presenta-tions combining synthetic speech with graphical ac-tions on the map.
For example, when showing asubway route, as the virtual agent speaks each in-struction in turn, the map display zooms and showsthe corresponding route segment graphically.
Thekiosk system also has a print capability.
When aroute has been presented, one of the context sensi-tive buttons changes to Print Directions.
When thisis pressed the system generates an XHTML doc-ument containing a map with step by step textualdirections and this is sent to the printer using anXHTML-print capability.If the system has low confidence in a user in-put, based on the ASR or pen recognition score,it requests confirmation from the user.
The usercan confirm using speech, pen, or by touching ona checkmark or cross mark which appear in the bot-tom right of the screen.
Context-sensitive graphi-cal widgets are also used for resolving ambiguityand vagueness in the user inputs.
For example, ifthe user asks for the Smithsonian Museum a smallmenu appears in the bottom right of the map en-abling them to select between the different museumsites.
If the user asks to see restaurants near a partic-ular location, e.g.
show restaurants near the whitehouse, a graphical slider appears enabling the userto fine tune just how near.The system also features a context-sensitive mul-timodal help mechanism (Hastie et al, 2002) whichprovides assistance to users in the context of theircurrent task, without redirecting them to separatehelp system.
The help system is triggered by spokenor written requests for help, by touching the helpbuttons on the left, or when the user has made sev-eral unsuccessful inputs.
The type of help is chosenbased on the current dialog state and the state of thevisual interface.
If more than one type of help is ap-plicable a graphical menu appears.
Help messagesconsist of multimodal presentations combining spo-ken output with ink drawn on the display by the sys-tem.
For example, if the user has just requested tosee restaurants and they are now clearly visible onthe display, the system will provide help on gettinginformation about them.3 Multimodal Kiosk ArchitectureThe underlying architecture of MATCHKiosk con-sists of a series of re-usable components whichcommunicate using XML messages sent over sock-ets through a facilitator (MCUBE) (Figure 3).
Usersinteract with the system through the Multimodal UIdisplayed on the touchscreen.
Their speech andink are processed by speech recognition (ASR) andhandwriting/gesture recognition (GESTURE, HWRECO) components respectively.
These recogni-tion processes result in lattices of potential wordsand gestures/handwriting.
These are then com-bined and assigned a meaning representation using amultimodal language processing architecture basedon finite-state techniques (MMFST) (Johnston andBangalore, 2000; Johnston et al, 2002b).
This pro-vides as output a lattice encoding all of the potentialmeaning representations assigned to the user inputs.This lattice is flattened to an N-best list and passedto a multimodal dialog manager (MDM) (Johnstonet al, 2002b) which re-ranks them in accordancewith the current dialogue state.
If additional infor-mation or confirmation is required, the MDM usesthe virtual agent to enter into a short informationgathering dialogue with the user.
Once a commandor query is complete, it is passed to the multimodalgeneration component (MMGEN), which builds amultimodal score indicating a coordinated sequenceof graphical actions and TTS prompts.
This scoreis passed back to the Multimodal UI.
The Multi-modal UI passes prompts to a visual text-to-speechcomponent (Cosatto and Graf, 2000) which com-municates with the AT&T Natural Voices TTS en-gine (Beutnagel et al, 1999) in order to coordinatethe lip movements of the virtual agent with syntheticspeech output.
As prompts are realized the Multi-modal UI receives notifications and presents coordi-nated graphical actions.
The subway route server isan application server which identifies the best routebetween any two locations.Figure 3: Multimodal Kiosk Architecture4 Discussion and Related WorkA number of design issues arose in the developmentof the kiosk, many of which highlight differencesbetween multimodal interfaces for kiosks and thosefor mobile systems.Array Microphone While on a mobile device aclose-talking headset or on-device microphone canbe used, we found that a single microphone had verypoor performance on the kiosk.
Users stand in dif-ferent positions with respect to the display and theremay be more than one person standing in front.
Toovercome this problem we mounted an array micro-phone above the touchscreen which tracks the loca-tion of the talker.Robust Recognition and Understanding is par-ticularly important for kiosks since they have somany first-time users.
We utilize the techniquesfor robust language modelling and multimodalunderstanding described in Bangalore and John-ston (2004).Social Interaction For mobile multimodal inter-faces, even those with graphical embodiment, wefound there to be little or no need to support so-cial greetings and small talk.
However, for a publickiosk which different unknown users will approachthose capabilities are important.
We added basicsupport for social interaction to the language under-standing and dialog components.
The system is ableto respond to inputs such as Hello, How are you?,Would you like to join us for lunch?
and so on.Context-sensitive GUI Compared to mobile sys-tems, on palmtops, phones, and tablets, kiosks canoffer more screen real estate for graphical interac-tion.
This allowed for large easy to read buttonsfor accessing help and other functions.
The sys-tem alters these as the dialog progresses.
These but-tons enable the system to support a kind of mixed-initiative in multimodal interaction where the usercan take initiative in the spoken and handwrittenmodes while the system is also able to providea more system-oriented initiative in the graphicalmode.Printing Kiosks can make use of printed outputas a modality.
One of the issues that arises is thatit is frequently the case that printed outputs such asdirections should take a very different style and for-mat from onscreen presentations.In previous work, a number of different multi-modal kiosk systems supporting different sets ofinput and output modalities have been developed.The Touch-N-Speak kiosk (Raisamo, 1998) com-bines spoken language input with a touchscreen.The August system (Gustafson et al, 1999) is a mul-timodal dialog system mounted in a public kiosk.It supported spoken input from users and multi-modal output with a talking head, text to speech,and two graphical displays.
The system was de-ployed in a cultural center in Stockholm, enablingcollection of realistic data from the general public.SmartKom-Public (Wahlster, 2003) is an interactivepublic information kiosk that supports multimodalinput through speech, hand gestures, and facial ex-pressions.
The system uses a number of camerasand a video projector for the display.
The MASKkiosk (Lamel et al, 2002) , developed by LIMSI andthe French national railway (SNCF), provides railtickets and information using a speech and touch in-terface.
The mVPQ kiosk system (Narayanan et al,2000) provides access to corporate directory infor-mation and call completion.
Users can provide in-put by either speech or touching options presentedon a graphical display.
MACK, the Media LabAutonomous Conversational Kiosk, (Cassell et al,2002) provides information about groups and indi-viduals at the MIT Media Lab.
Users interact us-ing speech and gestures on a paper map that sits be-tween the user and an embodied agent.In contrast to August and mVPQ, MATCHKiosksupports composite multimodal input combiningspeech with pen drawings and touch.
TheSmartKom-Public kiosk supports composite input,but differs in that it uses free hand gesture for point-ing while MATCH utilizes pen input and touch.August, SmartKom-Public, and MATCHKiosk allemploy graphical embodiments.
SmartKom usesan animated character, August a model-based talk-ing head, and MATCHKiosk a sample-based video-realistic talking head.
MACK uses articulatedgraphical embodiment with ability to gesture.
InTouch-N-Speak a number of different techniquesusing time and pressure are examined for enablingselection of areas on a map using touch input.
InMATCHKiosk, this issue does not arise since areascan be selected precisely by drawing with the pen.5 ConclusionWe have presented a multimodal public informa-tion kiosk, MATCHKiosk, which supports complexunstructured tasks such as browsing for restaurantsand subway directions.
Users have the flexibility tointeract using speech, pen/touch, or multimodal in-puts.
The system responds with multimodal presen-tations which coordinate synthetic speech, a virtualagent, graphical displays, and system use of elec-tronic ink.Acknowledgements Thanks to Eric Cosatto,Hans Peter Graf, and Joern Ostermann for their helpwith integrating the talking head.
Thanks also toPatrick Ehlen, Amanda Stent, Helen Hastie, GunaVasireddy, Mazin Rahim, Candy Kamm, MarilynWalker, Steve Whittaker, and Preetam Maloor fortheir contributions to the MATCH project.
Thanksto Paul Burke for his assistance with XHTML-print.ReferencesS.
Bangalore and M. Johnston.
2004.
BalancingData-driven and Rule-based Approaches in theContext of a Multimodal Conversational System.In Proceedings of HLT-NAACL, Boston, MA.M.
Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou,and A. Syrdal.
1999.
The AT&T Next-Generation TTS.
In In Joint Meeting of ASA;EAA and DAGA.J.
Cassell, T. Stocky, T. Bickmore, Y. Gao,Y.
Nakano, K. Ryokai, D. Tversky, C. Vaucelle,and H. Vilhjalmsson.
2002.
MACK: Media labautonomous conversational kiosk.
In Proceed-ings of IMAGINA02, Monte Carlo.E.
Cosatto and H. P. Graf.
2000.
Photo-realisticTalking-heads from Image Samples.
IEEE Trans-actions on Multimedia, 2(3):152?163.J.
Gustafson, N. Lindberg, and M. Lundeberg.1999.
The August spoken dialogue system.
InProceedings of Eurospeech 99, pages 1151?1154.H.
Hastie, M. Johnston, and P. Ehlen.
2002.Context-sensitive Help for Multimodal Dialogue.In Proceedings of the 4th IEEE InternationalConference on Multimodal Interfaces, pages 93?98, Pittsburgh, PA.M.
Johnston and S. Bangalore.
2000.
Finite-state Multimodal Parsing and Understanding.
InProceedings of COLING 2000, pages 369?375,Saarbru?cken, Germany.M.
Johnston, S. Bangalore, and G. Vasireddy.
2001.MATCH: Multimodal Access To City Help.
InWorkshop on Automatic Speech Recognition andUnderstanding, Madonna di Campiglio, Italy.M.
Johnston, S. Bangalore, A. Stent, G. Vasireddy,and P. Ehlen.
2002a.
Multimodal Language Pro-cessing for Mobile Information Access.
In Pro-ceedings of ICSLP 2002, pages 2237?2240.M.
Johnston, S. Bangalore, G. Vasireddy, A. Stent,P.
Ehlen, M. Walker, S. Whittaker, and P. Mal-oor.
2002b.
MATCH: An Architecture for Mul-timodal Dialog Systems.
In Proceedings of ACL-02, pages 376?383.L.
Lamel, S. Bennacef, J. L. Gauvain, H. Dartigues,and J. N. Temem.
2002.
User Evaluation ofthe MASK Kiosk.
Speech Communication, 38(1-2):131?139.S.
Narayanan, G. DiFabbrizio, C. Kamm,J.
Hubbell, B. Buntschuh, P. Ruscitti, andJ.
Wright.
2000.
Effects of Dialog Initiative andMulti-modal Presentation Strategies on LargeDirectory Information Access.
In Proceedings ofICSLP 2000, pages 636?639.S.
Oviatt and P. Cohen.
2000.
Multimodal Inter-faces That Process What Comes Naturally.
Com-munications of the ACM, 43(3):45?53.R.
Raisamo.
1998.
A Multimodal User Interfacefor Public Information Kiosks.
In Proceedings ofPUI Workshop, San Francisco.W.
Wahlster.
2003.
SmartKom: Symmetric Multi-modality in an Adaptive and Reusable DialogueShell.
In R. Krahl and D. Gunther, editors, Pro-ceedings of the Human Computer Interaction Sta-tus Conference 2003, pages 47?62.
