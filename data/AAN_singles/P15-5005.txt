Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 16?18,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsMatrix and Tensor Factorization Methods forNatural Language ProcessingGuillaume Bouchard?Jason Naradowsky#Sebastian Riedel#Tim Rockt?aschel#and Andreas Vlachos#?Xerox Research Centre Europeguillaume.bouchard@xerox.com#Computer Science DepartmentUniversity College London{j.narad, s.riedel, t.rocktaschel, a.vlachos}@cs.ucl.ac.uk1 Tutorial ObjectivesTensor and matrix factorization methods have at-tracted a lot of attention recently thanks to theirsuccessful applications to information extraction,knowledge base population, lexical semantics anddependency parsing.
In the first part, we will firstcover the basics of matrix and tensor factorizationtheory and optimization, and then proceed to moreadvanced topics involving convex surrogates andalternative losses.
In the second part we will dis-cuss recent NLP applications of these methods andshow the connections with other popular methodssuch as transductive learning, topic models andneural networks.
The aim of this tutorial is topresent in detail applied factorization methods, aswell as to introduce more recently proposed meth-ods that are likely to be useful to NLP applications.2 Tutorial Overview2.1 Matrix/Tensor Factorization BasicsIn this part, we first remind essential results onbilinear forms, spectral representations of matri-ces and low-rank approximation theorems, whichare often omitted in undergraduate linear algebracourses.
This includes the link between eigen-value decomposition and singular value decompo-sition and the trace-norm (a.k.a.
nuclear norm) asa convex surrogate of the low-rank constraint onoptimization problems.
Then, an overview of themost efficient algorithms to solve low-rank con-strained problems is made, from the power itera-tion method, the Lanczos algorithm and the im-plicitly restarted Arnoldi method that is imple-mented in the LAPACK library (Anderson et al.,1999).
We show how to interpret low-rank modelsas probabilistic models (Bishop, 1999) and howwe can extend SVD algorithms that can factor-ize non-standard matrices (i.e.
with non-Gaussiannoise and missing data) using gradient descent, re-weighted SVD or Frank-Wolfe algorithms.
Wethen show that combining different convex objec-tives can be a powerful tool, and we illustrate it byderiving the robust PCA algorithm by adding anL1penalty term in the objective function (Cand`esand Recht, 2009).
Furthermore, we introduceBayesian Personalized Ranking (BPR) for matrixand tensor factorization which deals with implicitfeedback in ranking tasks (Rendle et al., 2009).
Fi-nally, will introduce the collective matrix factor-ization model (Singh and Gordon, 2008) and ten-sor extensions (Nickel et al., 2011) for relationallearning.2.2 Applications in NLPIn this part we will discuss recent work apply-ing matrix/tensor factorization methods in the con-text of NLP.
We will review the Universal Schemaparadigm for knowledge base construction (Riedelet al., 2013) which relies on matrix factoriza-tion and BPR, as well as recent extensions ofthe RESCAL tensor factorization (Nickel et al.,2011) approach and methods of injecting logicinto the embeddings learned (Rockt?aschel et al.,2015).
These applications will motivate the con-nections between matrix factorization and trans-ductive learning (Goldberg et al., 2010), as wellas tensor factorization and multi-task learning(Romera-Paredes et al., 2013).
Furthermore, wewill review work on applying matrix and tensorfactorization to sparsity reduction in syntactic de-pendency parsing (Lei et al., 2014) and word rep-resentation learning (Pennington et al., 2014).
Inaddition, we will discuss the connections betweenmatrix factorization, latent semantic analysis andtopic modeling (Stevens et al., 2012).163 StructurePart I: Matrix/Tensor Factorization Basics (90minutes)?
Matrix factorization basics (40 min): bilin-ear forms, spectral representations, low rankapproximations theorems, optimization withstochastic gradient descent, losses?
Tensor factorization basics (20 minutes):representations,notation decompositions(Tucker etc.)?
Advanced topics (30 minutes): convex sur-rogates, L1 regularization, alternative losses(ranking loss, logistic loss)Break (15 minutes)Part II: Applications in NLP (75 minutes)?
Information extraction, knowledge base pop-ulation with connections to transductivelearning and multitask learning (35 minutes)?
Lexical semantics with connections to neuralnetworks, latent semantic analysis and topicmodels (30 minutes)?
Structured prediction (10 minutes)4 About the SpeakersGuillaume Bouchard is a senior researcher instatistics and machine learning at Xerox, focusingon statistical learning using low-rank model forlarge relational databases.
His research includestext understanding, user modeling, and social me-dia analytics.
The theoretical part of his work isrelated to the efficient algorithms to compute highdimensional integrals, essential to deal with un-certainty (missing and noisy data, latent variablemodels, Bayesian inference).
The main applica-tion areas of his work includes the design of vir-tual conversational agents, link prediction (predic-tive algorithms for relational data), social mediamonitoring and transportation analytics.
His webpage is available at www.xrce.xerox.com/people/bouchard.Jason Naradowsky is a postdoc at the MachineReading group at UCL.
Having previously ob-tained a PhD at UMass Amherst under the supervi-sion of David Smith and Mark Johnson, his currentresearch aims to improve natural language under-standing by performing task-specific training ofword representations and parsing models.
He isalso interested in semi-supervised learning, jointinference, and semantic parsing.
His web page isavailable at http://narad.github.io/.Sebastian Riedel is a senior lecturer at Univer-sity College London and an Allen DistinguishedInvestigator, leading the Machine Reading Lab.Before, he was a postdoc and research scientistwith Andrew McCallum at UMass Amherst, a re-searcher at Tokyo University and DBCLS withTsujii Junichi, and a PhD student with Ewan Kleinat the University of Edinburgh.
He is interestedin teaching machines how to read and works atthe intersection of Natural Language Processing(NLP) and Machine Learning, investigating vari-ous stages of the NLP pipeline, in particular thosethat require structured prediction, as well as fullyprobabilistic architectures of end-to-end readingand reasoning systems.
Recently he became inter-ested in new ways to represent textual knowledgeusing low-rank embeddings and how to reasonwith such representations.
His web page is avail-able at http://www.riedelcastro.org/.Tim Rockt?aschel is a PhD student in Sebas-tian Riedel?s Machine Reading group at Univer-sity College London.
Before that he worked asresearch assistant in the Knowledge Managementin Bioinformatics group at Humboldt-Universit?atzu Berlin, where he also obtained his Diplomain Computer Science.
He is broadly interestedin representation learning (e.g.
matrix/tensor fac-torization, deep learning) for NLP and automatedknowledge base completion, and how these meth-ods can take advantage of symbolic backgroundknowledge.
His webpage is available at http://rockt.github.io/.Andreas Vlachos is postdoc at the MachineReading group at UCL working with SebastianRiedel on automated fact-checking using low-rank factorization methods.
Before that he wasa postdoc at the Natural Language and Infor-mation Processing group at the University ofCambridge and at the University of Wisconsin-Madison.
He is broadly interested in natural lan-guage understanding (e.g.
information extraction,semantic parsing) and in machine learning ap-proaches that would help us towards this goal.He has also worked on active learning, cluster-ing and biomedical text mining.
His web pageis available at http://sites.google.com/site/andreasvlachos/.17References[Anderson et al.1999] Edward Anderson, Zhaojun Bai,Christian Bischof, Susan Blackford, James Demmel,Jack Dongarra, Jeremy Du Croz, Anne Greenbaum,S Hammerling, Alan McKenney, et al.
1999.
LA-PACK Users?
guide, volume 9.
SIAM.
[Bishop1999] Christopher M Bishop.
1999.
BayesianPCA.
In Advances in Neural Information Process-ing Systems, pages 382?388.
[Cand`es and Recht2009] Emmanuel J Cand`es and Ben-jamin Recht.
2009.
Exact matrix completion viaconvex optimization.
Foundations of Computationalmathematics, 9(6):717?772.
[Goldberg et al.2010] Andrew Goldberg, Ben Recht,Junming Xu, Robert Nowak, and Xiaojin Zhu.2010.
Transduction with matrix completion: Threebirds with one stone.
In Advances in Neural Infor-mation Processing Systems 23, pages 757?765.
[Lei et al.2014] Tao Lei, Yu Xin, Yuan Zhang, ReginaBarzilay, and Tommi Jaakkola.
2014.
Low-ranktensors for scoring dependency structures.
In Pro-ceedings of the 52nd Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1381?1391.
[Nickel et al.2011] Maximilian Nickel, Volker Tresp,and Hans-Peter Kriegel.
2011.
A three-way modelfor collective learning on multi-relational data.
InProceedings of the 28th International Conference onMachine Learning (ICML-11), pages 809?816.
[Pennington et al.2014] Jeffrey Pennington, RichardSocher, and Christopher D. Manning.
2014.
Glove:Global vectors for word representation.
In Proceed-ings of the 2014 Conference on Empirical Methodsin Natural Language Processing.
[Rendle et al.2009] Steffen Rendle, Christoph Freuden-thaler, Zeno Gantner, and Lars Schmidt-Thieme.2009.
Bpr: Bayesian personalized ranking from im-plicit feedback.
In Proceedings of the Twenty-FifthConference on Uncertainty in Artificial Intelligence,pages 452?461.
[Riedel et al.2013] Sebastian Riedel, Limin Yao, Ben-jamin M. Marlin, and Andrew McCallum.
2013.Relation extraction with matrix factorization anduniversal schemas.
In Proceedings of the 2013 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, Atlanta, GA.[Rockt?aschel et al.2015] Tim Rockt?aschel, SameerSingh, and Sebastian Riedel.
2015.
InjectingLogical Background Knowledge into Embeddingsfor Relation Extraction .
In Proceedings of the2015 Human Language Technology Conference ofthe North American Chapter of the Association ofComputational Linguistics.
[Romera-Paredes et al.2013] Bernardino Romera-Paredes, Hane Aung, Nadia Bianchi-Berthouze,and Massimiliano Pontil.
2013.
Multilinearmultitask learning.
In Proceedings of the 30thInternational Conference on Machine Learning,pages 1444?1452.
[Singh and Gordon2008] Ajit P. Singh and Geoffrey J.Gordon.
2008.
Relational learning via collectivematrix factorization.
In Proceedings of the 14thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 650?658.
[Stevens et al.2012] Keith Stevens, Philip Kegelmeyer,David Andrzejewski, and David Buttler.
2012.
Ex-ploring topic coherence over many models and manytopics.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 952?961.18
