Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1004?1013,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsDiscriminative Deep Random Walk for Network ClassificationJuzheng Li, Jun Zhu, Bo ZhangDept.
of Comp.
Sci.
& Tech., State Key Lab of Intell.
Tech.
& Sys.Tsinghua University, Beijing, 100084, Chinalijuzheng09@gmail.com; {dcszj,dcszb}@tsinghua.edu.cnAbstractDeep Random Walk (DeepWalk) can learna latent space representation for describ-ing the topological structure of a network.However, for relational network classifi-cation, DeepWalk can be suboptimal asit lacks a mechanism to optimize the ob-jective of the target task.
In this paper,we present Discriminative Deep RandomWalk (DDRW), a novel method for re-lational network classification.
By solv-ing a joint optimization problem, DDRWcan learn the latent space representationsthat well capture the topological struc-ture and meanwhile are discriminative forthe network classification task.
Our ex-perimental results on several real socialnetworks demonstrate that DDRW signif-icantly outperforms DeepWalk on multi-label network classification tasks, whileretaining the topological structure in thelatent space.
DDRW is stable and con-sistently outperforms the baseline meth-ods by various percentages of labeled data.DDRW is also an online method that isscalable and can be naturally parallelized.1 IntroductionCategorization is an important task in natural lan-guage processing, especially with the growingscale of documents in the Internet.
As the doc-uments are often not isolated, a large amount ofthe linguistic materials present a network structuresuch as citation, hyperlink and social networks.The large size of networks calls for scalable ma-chine learning methods to analyze such data.
Re-cent efforts have been made in developing statis-tical models for various network analysis tasks,such as network classification (Neville and Jensen,2000), content recommendation (Fouss et al,2007), link prediction (Adamic and Adar, 2003),and anomaly detection (Savage et al, 2014).
Onecommon challenge of statistical network models isto deal with the sparsity of networks, which mayprevent a model from generalizing well.One effective strategy to deal with networksparsity is to learn a latent space representationfor the entities in a network (Hoff et al, 2002;Zhu, 2012; Tang and Liu, 2011; Tang et al, 2015).Among various approaches, DeepWalk (Perozzi etal., 2014) is a recent method that embeds all theentities into a continuous vector space using deeplearning methods.
DeepWalk captures entity fea-tures like neighborhood similarity and representsthem by Euclidean distances (See Figure 1(b)).Furthermore, since entities that have closer rela-tionships are more likely to share the same hobbiesor belong to the same groups, such an embeddingby DeepWalk can be useful for network classifica-tion, where the topological information is exploredto encourage a globally consistent labeling.Although DeepWalk is effective on learningembeddings of the topological structure, whendealing with a network classification task, it lacksa mechanism to optimize the objective of the tar-get task and thus often leads to suboptimal embed-dings.
In particular, for our focus of relational net-work classification, we would like the embeddingsto be both representing the topological structure ofthe network actors and discriminative in predictingthe class labels of actors.To address the above issues, we present Dis-criminative Deep Random Walk (DDRW) for re-lational network classification.
DDRW extendsDeepWalk by jointly optimizing the classificationobjective and the objective of embedding entitiesin a latent space that maintains the topologicalstructure.
Under this joint learning framework,DDRM manages to learn the latent representations1004(a) Karate Graph?1.2 ?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.80.911.11.21.31.41.5(b) DeepWalk Embedding?1.5 ?1 ?0.5 0 0.5 11.11.21.31.41.51.6(c) DDRW EmbeddingFigure 1: Different experimental results of embedding a network into a two dimensional real space.
Weuse Karate Graph (Macskassy and Provost, 1977) for this example.
Four different colors stand for theclasses of the vertices.
In (b), vertices which have stronger relations in the network are more likely to becloser in the embedding latent space.
While in (c), besides the above-mentioned property, DDRW makesvertices in different classes more separated.that are strongly associated with the class labels(See Figure 1(c)), making it easy to find a separat-ing boundary between the classes, and the actorsthat are connected in the original network are stillclose to each other in the latent social space.
Thisidea of combining task-specific and representationobjectives has been widely explored in other re-gions such as MedLDA (Zhu et al, 2012) and Su-pervised Dictionary Learning (Mairal et al, 2009).Technically, to capture the topological struc-ture, we follow the similar idea of Deep-Walk by running truncated random walks onthe original network to extract sequences of ac-tors, and then building a language model (i.e.,Word2Vec (Mikolov et al, 2013b)) to project theactors into a latent space.
To incorporate the super-vising signal in network classification, we build aclassifier based on the latent space representations.By sharing the same latent social space, the twoobjectives are strongly coupled and the latent so-cial space is guided by both the network topologyand class labels.
DDRW optimizes the joint objec-tive by using stochastic gradient descent, which isscalable and embarrassingly parallizable.We evaluate the performance on several real-world social networks, including BlogCatalog,Flickr and YouTube.
Our results demonstrate thatDDRW significantly boosts the classification ac-curacy of DeepWalk in multi-label network clas-sification tasks, while still retaining the topolog-ical structure in the learnt latent social space.We also show that DDRW is stable and consis-tently outperforms the baseline methods by var-ious percentages of labeled data.
Although thenetworks we use only bring topological informa-tion for clarity, DDRW is flexible to consider addi-tional attributes (if any) of vertices.
For example,DDRW can be naturally extended to classify docu-ments/webpages, which are often represented as anetwork (e.g., citation/hyperlink network), by con-joining with a word2vec component to embed thedocuments/webpages into the same latent space,similar as previous work on extending DeepWalkto incorporate attributes (Yang et al, 2015).2 Problem DefinitionWe consider the network classification problem,which classifies entities from a given network intoone or more categories from a set Y .
Let G =(V,E, Y ) denote a network, where V is the set ofvertices, representing the entities of the network;E ?
(V ?
V ) is the set of edges, representing therelations between the entities; and Y ?
R|V |?|Y|denotes the labels of entities.
We also consider YUas a set of unknown labels in the same graph G.The target of the classification task is to learn amodel from labeled data and generate a label setYPto be the prediction of YU.
The difference be-tween YPand YUindicates the classification qual-ity.When classifying elements X ?
Rn, traditionalmachine learning methods learn a weight matrixH to minimize the difference between YP=F(X,H) and YU, where F is any known fixedfunction.
In network aspect, we will be ableto utilize well-developed machine learning meth-ods if adequate information of G is embeddedinto a corresponding form as X .
By this mo-tivation, relational learning (Getoor and Taskar,2007; Neville and Jensen, 2000) methods are pop-1005ularly employed.
In network classification, theinternal structure of a network is resolved to ex-tract the neighboring features of the entities (Mac-skassy and Provost, 2007; Wang and Sukthankar,2013).
Accordingly, the core problem is how todescribe the irregular networks within formal fea-ture spaces.
A variety of approaches have beenproposed with the purpose of finding effectivestatistical information through the network (Gal-lagher and Eliassi-Rad, 2008; Henderson et al,2011; Tang and Liu, 2011).DeepWalk (Perozzi et al, 2014) is an outstand-ing method for network embedding, which usestruncated random walks to capture the explicitstructure of the network and applies languagemodels to learn the latent relationships betweenthe actors.
When applied to the network classifica-tion task, DeepWalk first learnsX which describesthe topological structure of G and then learns asubsequent classifier H .
One obvious shortcom-ing of this two-step procedure is that the embed-ding step is unaware of the target class label in-formation and likely to learn embeddings that aresuboptimal for classification.We present Discriminative Deep Random Walk(DDRW) to enhance the effect of DeepWalk bylearning X ?
R|V |?dand H ?
Rd?|Y|jointly.By using topological and label information ofa certain network simultaneously, we will showthat DDRW improves the classification accuracysignificantly compared with most recent relatedmethods.
Furthermore, we will also show that theembedded result X produced by DDRW is able toretain the structure of G well.3 Discriminative Deep Random WalkIn this section, we present the details of Discrimi-native Deep Random Walk (DDRW).
DDRW hasboth embedding and classification objectives.
Weoptimize the two objectives jointly to learn latentrepresentations that are strongly associated withthe class labels in the latent space.
We use stochas-tic gradient descent (Mikolov et al, 1991) as ouroptimization method.3.1 Embedding ObjectiveLet ?
= (?1,?2, .
.
.
,?|V |) denote the embeddedvectors in the latent space, and ?
denote the topo-logical structure of the graph.
The embedding ob-jective can be described as an optimization prob-4 9 1812 311516?Wi       :     ?4  16  18  3  5 ?
Wi+1  : ?16  12  11  5  9  18?
?Figure 2: A part of Random Walk process in anundirected graph.
Every time an adjacent vertexis chosen randomly (no matter visited or not) asthe arrows indicate, until reaching the maximumlength s.lem as follows:min?
Lr(?,?
), (1)where Lrindicates the difference between the em-bedded representations ?
and original topologi-cal structure ?.
For this objective, we use trun-cated random walks to capture the topologicalstructure of the graph and the language modelWord2Vec (Mikolov et al, 2013b) to learn the la-tent representations.
Below, we explain each inturn.3.1.1 Random WalkRandom Walk has been used in different regionsin network analysis to capture the topologicalstructure of graphs (Fouss et al, 2007; Ander-sen et al, 2006).
As the name suggests, RandomWalk chooses a certain vertex in the graph for thefirst step and then randomly migrates through theedges.
Truncated random walk defines a maxi-mum length s for all walk streams.In our implementation, we shuffle the wholevertices V in the graph for ?
times to build thesample set W .
After each time of shuffling, wetake the permutation list of vertices as the startingpoints of walks.
Every time a walk stream startsat one element in order, randomly chooses an ad-jacent vertex to move, and ends when this streamreaches s vertices.
By this procedure we get totally1006?
|V | samples (i.e.
walk streams) from the graph.Thus our sample set W ?
R?
|V |?sis obtained asthe training materials.3.1.2 Word2VecExisting work has shown that both the vertices intruncated random walks and the words in text arti-cles follow similar power-law distributions in fre-quency, and then the idea of reshaping a socialnetwork into a form of corpus is very straight-forward (Perozzi et al, 2014).
Corresponding tolinguistic analysis region, the objective is to findan embedding for a corpus to show the latent sig-nificances between the words.
Words which havecloser meanings are more likely to be embeddedinto near positions.
Word2Vec (Mikolov et al,2013b) is an appropriate tool for this problem.
Weuse the Skip-gram (Mikolov et al, 2013a) strat-egy in Word2Vec, which uses the central word ina sliding window with radius R to predict otherwords in the window and make local optimiza-tions.
Specifically, let ?
= rw(?)
denote thefull walk streams obtained from truncated randomwalks in Section 3.1.1.
Then by Skip-gram we canget the objective functionLr(?,?)
=???i=11ss?t=1?
?R?j?R,j 6=0log p(?i,t+j|?i,j).
(2)The standard Skip-gram method definesp(?i,t+j|?i,j) in Eq.
(2) as follows:p(?O|?I) =exp(?T?O??
?I)?|V |i=1exp(?Ti??
?I), (3)where?
?iand ?iare the input and output represen-tations of the ith vertex, respectively.One shortcoming of the standard form is thatthe summation in Eq.
(3) is very inefficient.
Toreduce the time consumption, we use the Hierar-chical Softmax (Mnih and Hinton, 2009; Morinand Bengio, 2005) which is included in Word2Vecpackages?.
In Hierarchical Softmax, the Huffmanbinary tree is employed as an alternative represen-tation for the vocabulary.
The gradient descentstep will be faster thanks to the Huffman tree struc-ture which allows a reduction of output units nec-essarily evaluated.
?https://code.google.com/archive/p/word2vec/3.2 Classification ObjectiveLet y = (y1,y2, .
.
.
,y|V |) denote the labels, and?
denote the subsequent classifier.
The classifica-tion objective can be described as an optimizationproblem:min?,?Lc(?,?,y).
(4)In DDRW, we use existing classifiers anddo not attempt to extend them.
AlthoughSVMmulticalss(Crammer and Singer, 2002) oftenshows good performance in multi-class tasks em-pirically, we choose the classifier being referredto as L2-regularized and L2-loss Support VectorClassification (Fan et al, 2008) to keep pace withthe baseline methods to be mentioned in Section4.In L2-regularized and L2-loss SVC, the lossfunction isLc(?,?,y)=C|V |?i=1(?(1?
yi?T?i))2+12?T?,(5)where C is the regularization parameter, ?
(x) =x if x > 0 and ?
(x) = 0 otherwise.
Eq.
(5) isfor binary classification problems, and is extendedto multi-class problems following the one-against-rest strategy (Fan et al, 2008).3.3 Joint LearningThe main target of our method is to classify theunlabeled vertices in the given network.
Weachieve this target with the help of intermediateembeddings which latently represent the networkstructure.
We simultaneously optimize two ob-jectives in Section 3.1 and 3.2.
Specifically, letL(?,?,?,y) = ?Lr(?,?)
+ Lc(?,?,y), where?
is a key parameter that balances the weights ofthe two objectives.
We solve the joint optimizationproblem:min?,?L(?,?,?,y).
(6)We use stochastic gradient descent (Mikolov etal., 1991) to solve the optimization problem inEq.(6).
In each gradient descent step, we have?
?
?
?
??L?
?= ?
?
?(??Lr??+?Lc??),?
?
?
?
??L?
?= ?
?
??Lc?
?,(7)where ?
is the learning rate for stochastic gradientdescent.
In our implementation, ?
is initially set to10070.025 and linearly decreased with the steps, sameas the default setting of Word2Vec.
The deriva-tives in Eq.
(7) are estimated by local slopes.In Eq.
(7), the latent representations adjust them-selves according to both topological information(?Lr/??)
and label information (?Lc/??).
Thisprocess intuitively makes vertices in the sameclass closer and those in different classes farther,and this is also proved by experiments (See Fig-ure 1).
Thus by joint learning, DDRW can learnthe latent space representations that well capturethe topological structure and meanwhile are dis-criminative for the network classification task.We take each sample Wifrom walk streams Wto estimate the local derivatives of the loss func-tion for a descent step.
Stochastic gradient descentenables DDRW to be an online algorithm, and thusour method is easy to be parallelized.
Besides, avertex may repeatedly appear for numerous timesin W produced by random walks.
This repeat issuperfluous for classifiers and there is a consider-able possibility to arise overfitting.
Inspired fromDropOut (Hinton et al, 2012) ideas, we randomlyignore the label information to control the opti-mization process in an equilibrium state.4 Experimental SetupIn this section we present an overview of thedatasets and baseline methods which we will com-pare with in the experiments.4.1 DatasetsWe use three popular social networks, which areexactly same with those used in some of the base-line methods.
Table 1 summarizes the statistics ofthe data.?
BlogCatalog: a network of social relation-ships provided by blog authors.
The labelsof this graph are the topics specified by theuploading users.?
Flickr: a network of the contacts betweenusers of the Flickr photo sharing website.The labels of this graph represent the interestsof users towards certain categories of photos.?
YouTube: a network between users of theYoutube video sharing website.
The labelsstand for the groups of the users interested indifferent types of videos.Dataset BlogCatalog Flickr YouTubeActors |V | 10,312 80,513 1,138,499Links |E| 333,983 5,899,882 2,990,443Labels |Y| 29 195 47Sparsity 6.3?
10-31.8?
10-34.6?
10-6Max Degree 3,992 5,706 28,754Average Degree 65 146 5Table 1: Statistics of the three networks.
Sparsityindicates the ratio of the actual links and links in acomplete graph.4.2 Baseline MethodsWe evaluate our proposed method by comparing itwith some significantly related methods.?
LINE (Tang et al, 2015)?
: This methodtakes the edges of a graph as samples totrain the first-order and second-order prox-imity seprately and integrate the results asan embedding of the graph.
This methodcan handle both graphs with unweighted andweighted and is especially efficient in largenetworks.?
DeepWalk (Perozzi et al, 2014): Thismethod employs language models to learnlatent relations between the vertices in thegraph.
The basic assumption is that the closertwo vertices are in the embedding space, thedeeper relationships they have and there ishigher possibility that they are in the samecategories.?
SpectralClustering (Tang and Liu, 2011):This method finds out that graph cuts are use-ful for the classification task.
This idea isimplemented by finding the eigenvectors ofa normalized graph Laplacian of the originalgraph.?
EdgeCluster (Tang and Liu, 2009b): Thismethod uses k-means clustering algorithm tosegment the edges of the graph into pieces.Then it runs iterations on the small clusters tofind the internal relationships separately.
Thecore idea is to scale time-consuming workinto tractable sizes.?
Majority: This baseline method simplychooses the most frequent labels.
It does notuse any structural information of the graph.
?Although LINE also uses networks from Flickr andYouTube in its experiments, the networks are different fromthis paper.1008As the datasets are not only multi-class butalso multi-label, we usually need a thresholdingmethod to test the results.
But literature gives anegative opinion of arbitrarily choosing threshold-ing methods because of the considerably differentperformances.
To avoid this, we assume that thenumber of the labels is already known in all thetest processes.5 ExperimentsIn this section, we present the experimental resultsand analysis on both network classification and la-tent space learning.
We thoroughly evaluate theperformance on the three networks and analyze thesensitivity to key parameters.5.1 Classification TaskWe first represent the results on multi-class clas-sification and compare with the baseline methods.To have a direct and fair comparison, we use thesame data sets, experiment procedures and test-ing points as in the reports of our relevant base-lines (Perozzi et al, 2014; Tang and Liu, 2011;Tang and Liu, 2009b).
The training set of a spec-ified graph consists of the vertices, the edges andthe labels of a certain percentage of labeled ver-tices.
The testing set consists of the rest of the la-bels.
We employ Macro-F1and Micro-F1(Yang,1999) as our measurements.
Micro-F1computesF1score globally while Macro-F1caculates F1score locally and then average them globally.
Allthe results reported are averaged from 10 repeatedprocesses.5.1.1 BlogCatalogBlogCatalog is the smallest dataset among thethree.
In BlogCatalog we vary the percentage oflabeled data from 10% to 90%.
Our results arepresented in Table 2.
We can see that DDRWperforms consistently better than all the baselineson both Macro-F1and Micro-F1with the increas-ing percentage of labeled data.
When comparedwith DeepWalk, DDRW obtains larger improve-ment when the percentage of labeled nodes is high.This improvement demonstrates the significanceof DDRW on learning discriminative latent em-beddings that are good for classification tasks.5.1.2 FlickrFlickr is a larger dataset with quite a number ofclasses.
In this experiment we vary the percentageof labeled data from 1% to 10%.
Our results arepresented in Table 3.
We can see that DDRW stillperforms better than the baselines significantly onboth Macro-F1and Micro-F1, and the results areconsistent with what in BlogCatalog.5.1.3 YouTubeYouTube is an even larger dataset with fewerclasses than Flickr.
In YouTube we vary the per-centage of labeled data from 1% to 10%.
Our re-sults are presented in Table 4.
In YouTube, LINEshows its strength in large sparse networks, proba-bly because the larger scale of samples reduces thediscrepancy from actual distributions.
But from ageneral view, DDRW still performs better at mostof the test points thanks to the latent representa-tions when links are not sufficient.5.2 Parameter SensitivityWe now present an analysis of the sensitivity withrespect to several important parameters.
We mea-sure our method with changing parameters to eval-uate its stability.
Despite the parameters which areunilateral to classification performance, the twomain bidirectional parameters are ?
and the di-mension d of embedding space in different per-centages of labeled data.
We use BlogCatalog andFlickr networks for the experiments, and fix pa-rameters of random walks (?
= 30, s = 40, R =10).
We do not represent the effects of changingparameters of random walks because results usu-ally show unilateral relationships with them.5.2.1 Effect of ?The key parameter ?
in our algorithm adjusts theweights of two objectives (Section 3.3).
We rep-resent the effect of changing ?
in Figure 3(a) and3(b).
We fix d = 128 in these experiments.
Al-though rapid gliding can be observed on eithersides, there are still sufficient value range whereDDRW keeps the good performance.
These ex-periments also show that ?
is not very sensitivetowards the percentage of labeled data.5.2.2 Effect of DimensionalityWe represent the effect of changing dimension dof the embedding space in Figure 3(c) and 3(d).We fix ?
= 1.0 in these experiments.
There is de-cline when the dimension is high, but this decreaseis not very sharp.
Besides, when the dimension ishigh, the percentage of labeled data has more ef-fect on the performance.1009Labeled Nodes 10% 20% 30% 40% 50% 60% 70% 80% 90%Micro-F1(%)DDRW 37.13 39.31 41.08 41.76 42.64 43.17 43.80 44.11 44.79LINE 35.42 37.89 39.71 40.62 41.46 42.09 42.55 43.26 43.68DeepWalk 36.00 38.20 39.60 40.30 41.00 41.30 41.50 41.50 42.00SpecClust 31.06 34.95 37.27 38.93 39.97 40.99 41.66 42.42 42.62EdgeClust 27.94 30.76 31.85 32.99 34.12 35.00 34.63 35.99 36.29Majority 16.51 16.66 16.61 16.70 16.91 16.99 16.92 16.49 17.26Macro-F1(%)DDRW 21.69 24.33 26.28 27.78 28.76 29.53 30.47 31.40 32.04LINE 20.98 23.44 24.91 26.06 27.19 27.89 28.43 29.10 29.45DeepWalk 21.30 23.80 25.30 26.30 27.30 27.60 27.90 28.20 28.90SpecClust 19.14 23.57 25.97 27.46 28.31 29.46 30.13 31.38 31.78EdgeClust 16.16 19.16 20.48 22.00 23.00 23.64 23.82 24.61 24.92Majority 2.52 2.55 2.52 2.58 2.58 2.63 2.61 2.48 2.62Table 2: Multi-class classification results in BlogCatalog.Labeled Nodes 1% 2% 3% 4% 5% 6% 7% 8% 9% 10%Micro-F1(%)DDRW 33.61 35.20 36.72 37.43 38.31 38.89 39.33 39.64 39.85 40.02LINE 31.65 33.98 35.46 36.63 37.53 38.20 38.47 38.74 39.07 39.25DeepWalk 32.40 34.60 35.90 36.70 37.20 37.70 38.10 38.30 38.50 38.70SpecClust 27.43 30.11 31.63 32.69 33.31 33.95 34.46 34.81 35.14 35.41EdgeClust 25.75 28.53 29.14 30.31 30.85 31.53 31.75 31.76 32.19 32.84Majority 16.34 16.31 16.34 16.46 16.65 16.44 16.38 16.62 16.67 16.71Macro-F1(%)DDRW 14.49 17.81 20.05 21.40 22.91 23.84 25.12 25.79 26.28 26.43LINE 13.69 17.77 19.88 21.07 22.36 23.62 24.78 25.11 25.69 25.90DeepWalk 14.00 17.30 19.60 21.10 22.10 22.90 23.60 24.10 24.60 25.00SpecClust 13.84 17.49 19.44 20.75 21.60 22.36 23.01 23.36 23.82 24.05EdgeClust 10.52 14.10 15.91 16.72 18.01 18.54 19.54 20.18 20.78 20.85Majority 0.45 0.44 0.45 0.46 0.47 0.44 0.45 0.47 0.47 0.47Table 3: Multi-class classification results in Flickr.Labeled Nodes 1% 2% 3% 4% 5% 6% 7% 8% 9% 10%Micro-F1(%)DDRW 38.18 39.46 40.17 41.09 41.76 42.31 42.80 43.29 43.81 44.12LINE 38.06 39.36 40.30 41.14 41.58 41.93 42.22 42.67 43.09 43.55DeepWalk 37.95 39.28 40.08 40.78 41.32 41.72 42.12 42.48 42.78 43.05SpecClust 26.61 35.16 37.28 38.35 38.90 39.51 40.02 40.49 40.86 41.13EdgeClust 23.90 31.68 35.53 36.76 37.81 38.63 38.94 39.46 39.92 40.07Majority 24.90 24.84 25.25 25.23 25.22 25.33 25.31 25.34 25.38 25.38Macro-F1(%)DDRW 29.35 32.07 33.56 34.41 34.89 35.38 35.80 36.15 36.36 36.72LINE 27.36 31.08 32.51 33.39 34.26 34.81 35.27 35.52 35.95 36.14DeepWalk 29.22 31.83 33.06 33.90 34.35 34.66 34.96 35.22 35.42 35.67SpecClust 24.62 29.33 31.30 32.48 33.24 33.89 34.15 34.47 34.77 34.98EdgeClust 19.48 25.01 28.15 29.17 29.82 30.65 30.75 31.23 31.45 31.54Majority 6.12 5.86 6.21 6.10 6.07 6.19 6.17 6.16 6.18 6.19Table 4: Multi-class classification results in YouTube.101010?2 10?1 100 101 1020.150.20.250.30.350.40.45?Micro F10.10.20.50.9(a) BlogCatalog, ?10?2 10?1 100 101 1020.150.20.250.30.350.40.45?Micro F10.10.20.50.9(b) Flickr, ?10?2 10?1 100 101 1020.250.30.350.40.45dMicro F10.10.20.50.9(c) BlogCatalog, d10?2 10?1 100 101 1020.250.30.350.40.45dMicro F10.10.20.50.9(d) Flickr, dFigure 3: Parameter Sensitivity in BlogCatalog and FlickrK 1 5 10 20 50DDRW(10%) 91.3 71.0 58.3 44.3 31.2DDRW(50%) 90.9 69.8 62.0 44.7 30.7DDRW(90%) 90.2 72.8 59.7 43.4 31.1DeepWalk 91.2 73.2 59.8 46.5 31.2Random 0.7 0.7 0.7 0.6 0.6Table 5: Adjacency Predict Accuracy(%) in Blog-Catalog.5.3 Representation EfficiencyFinally, we examine the quality of the latent em-beddings of entities discovered by DDRW.
Fornetwork data, our major expectation is that the em-bedded social space should maintain the topologi-cal structure of the network.
A visualization of thetopological structure in a social space is showed inFigure 1.
Besides, we examine the neighborhoodstructure of the vertices.
Specifically, we checkthe top-K nearest vertices for each vertex in theembedded social space and calculate how many ofthe vertex pairs have edges between them in theobserved network.
We call this Adjacency Pre-dict Accuracy.
Table 5 shows the results, whereDDRW with different percentages of labeled data,DeepWalk and Random are compared in BlogCat-alog dataset.
The baseline method Random mapsall the vertices equably randomly into a fixed-sizespace.
The experiments show that although Deep-Walk outperforms on the whole, the performanceof DDRW is approximate.
DDRW is proved toinherit some important properties in latent repre-sentations of the network.6 Related WorkRelational classification (Geman and Geman,1984; Neville and Jensen, 2000; Getoor andTaskar, 2007) is a class of methods which in-volve the data item relation links during classi-fication.
A number of researchers have studieddifferent methods for network relational learning.
(Macskassy and Provost, 2003) present a simpleweighted vote relational neighborhood classifier.
(Xu et al, 2008) leverage the nonparametric infi-nite hidden relational model to analyze social net-works.
(Neville and Jensen, 2005) propose a la-tent group model for relational data, which dis-covers and exploits the hidden structures respon-sible for the observed autocorrelation among classlabels.
(Tang and Liu, 2009a) propose the latentsocial dimensions which are represented as con-tinuous values and allow each node to involve atdifferent dimensions in a flexible manner.
(Gal-lagher et al, 2008) propose a method that learnsparsely labeled network data by adding ghostedges between neighbor vertices, and (Lin and Co-hen, 2010) by using PageRank.
(Wang and Suk-thankar, 2013) extend the conventional relationalclassification to consider more additional features.
(Gallagher and Eliassi-Rad, 2008) propose a com-plimentary approach to within-network classifica-tion based on the use of label-independent fea-tures.
(Henderson et al, 2011) propose a re-gional feature generating method and demonstratethe usage of the regional feature in within-networkand across-network classification.
(Tang and Liu,2009b) propose an edge-centric clustering schemeto extract sparse social dimensions for collectivebehavior prediction.
(Tang and Liu, 2011) proposethe concept of social dimensions to represent thelatent affiliations of the entities.
(Vishwanathanet al, 2010) propose Graph Kernels to use rela-tional data during classification process and (Kanget al, 2012) propose a faster approximated methodof Graph Kernels.7 ConclusionThis paper presents Discriminative Deep RandomWalk (DDRW), a novel approach for relationalmulti-class classification on social networks.
Bysimultaneously optimizing embedding and classi-fication objectives, DDRW gains significantly bet-ter performances in network classification tasks1011than baseline methods.
Experiments on differ-ent real-world datasets represent adequate stabil-ity of DDRW.
Furthermore, the representationsproduced by DDRW is both an intermediate vari-able and a by-product.
Same as other embeddingmethods like DeepWalk, DDRW can provide well-formed inputs for statistical analyses other thanclassification tasks.
DDRW is also naturally anonline algorithm and thus easy to parallel.The future work has two main directions.
Oneis semi-supervised learning.
The low proportionof labeled vertices is a good platform for semi-supervised learning.
Although DDRW has alreadycombined supervised and unsupervised learningtogether, better performance can be expected afterintroducing well-developed methods.
The otherdirection is to promote the random walk step.
Lit-erature has represented the good combination ofrandom walk and language models, but this com-bination may be unsatisfactory for classification.It would be great if a better form of random walkis found.AcknowledgmentsThe work was supported by the National Ba-sic Research Program (973 Program) of China(No.
2013CB329403), National NSF of China(Nos.
61322308, 61332007), the Youngth Top-notch Talent Support Program, Tsinghua TNListLab Big Data Initiative, and Tsinghua InitiativeScientific Research Program (No.
20141080934).ReferencesLada A. Adamic and Eytan Adar.
2003.
Friends andneighbors on the web.
Social Networks, 25:211?230.Reid Andersen, Fan R. K. Chung, and Kevin J. Lang.2006.
Local graph partitioning using pagerank vec-tors.
In Foundations of Computer Science, pages476?486.Koby Crammer and Yoram Singer.
2002.
On the algo-rithmic implementation of multiclass kernel-basedvector machines.
Journal of Machine Learning Re-search, 2:265?292.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.Franc?ois Fouss, Alain Pirotte, Jean-Michel Renders,and Marco Saerens.
2007.
Random-walk compu-tation of similarities between nodes of a graph withapplication to collaborative recommendation.
IEEETransactions on Knowledge and Data Engineering,19:355?369.Brian Gallagher and Tina Eliassi-Rad.
2008.
Lever-aging label-independent features for classification insparsely labeled networks: An empirical study.
InProceedings of the Second International Conferenceon Advances in Social Network Mining and Analy-sis, pages 1?19.Brian Gallagher, Hanghang Tong, Tina Eliassi-Rad,and Christos Faloutsos.
2008.
Using ghost edges forclassification in sparsely labeled networks.
In Pro-ceedings of the 14th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing, pages 256?264.Stuart Geman and Donald Geman.
1984.
Stochas-tic relaxation, gibbs distributions, and the bayesianrestoration of images.
IEEE Trans.
Pattern Anal.Mach.
Intell., 6:721?741.Lise Getoor and Ben Taskar.
2007.
Introduction tostatistical relational learning.
The MIT Press.Keith Henderson, Brian Gallagher, Lei Li, LemanAkoglu, Tina Eliassi-Rad, Hanghang Tong, andChristos Faloutsos.
2011.
It?s who you know: graphmining using recursive structural features.
In Pro-ceedings of the 17th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing, pages 663?671.Geoffrey E. Hinton, Nitish Srivastava, AlexKrizhevsky, Ilya Sutskever, and Ruslan Salakhut-dinov.
2012.
Improving neural networks bypreventing co-adaptation of feature detectors.CoRR, abs/1207.0580.Peter D. Hoff, Adrian E. Raftery, and Mark S. Hand-cock.
2002.
Latent space approaches to social net-work analysis.
Journal of the American StatisticalAssociation, 97:1090?1098.U.
Kang, Hanghang Tong, and Jimeng Sun.
2012.
Fastrandom walk graph kernel.
In SDM, pages 828?838.Frank Lin and William W. Cohen.
2010.
Semi-supervised classification of network data using veryfew labels.
In Proceedings of the 2010 InternationalConference on Advances in Social Networks Analy-sis and Mining, pages 192?199.Sofus A. Macskassy and Foster J. Provost.
1977.
Aninformation flow model for conflict and fission insmall groups.
Journal of Anthropological Research,33:452?473.Sofus A. Macskassy and Foster Provost.
2003.
A sim-ple relational classifier.
In Proceedings of the Multi-Relational Data Mining Workshop at the Ninth ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining.1012Sofus A. Macskassy and Foster J. Provost.
2007.
Clas-sification in networked data: A toolkit and a univari-ate case study.
Journal of Machine Learning Re-search, 8:935?983.Julien Mairal, Jean Ponce, Guillermo Sapiro, AndrewZisserman, and Francis R. Bach.
2009.
Superviseddictionary learning.
In Advances in Neural Informa-tion Processing Systems, pages 1033?1040.Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.Corrado, and Jeffrey Dean.
1991.
Stochastic gradi-ent learning in neural networks.
In Proceedings ofNeuro-N?
?mes 91.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013a.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.Corrado, and Jeffrey Dean.
2013b.
Distributed rep-resentations of words and phrases and their compo-sitionality.
In Advances in Neural Information Pro-cessing Systems, pages 3111?3119.Andriy Mnih and Geoffrey E. Hinton.
2009.
A scal-able hierarchical distributed language model.
In Ad-vances in Neural Information Processing Systems,pages 1081?1088.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InProceedings of the International Workshop on Arti-ficial Intelligence and Statistics, pages 246?252.Jennifer Neville and David Jensen.
2000.
Iterativeclassification in relational data.
In Proceedings ofAAAI-2000 Workshop on Learning Statistical Mod-els from Relational Data, pages 13?20.Jennifer Neville and David Jensen.
2005.
Leveragingrelational autocorrelation with latent group models.In Proceedings of the 4th International Workshop onMulti-relational Mining, pages 49?55.Bryan Perozzi, Rami Al-Rfou, and Steven Skiena.2014.
DeepWalk: online learning of social represen-tations.
In Proceedings of the 20th ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining, pages 701?710.David Savage, Xiuzhen Zhang, Xinghuo Yu,Pauline Lienhua Chou, and Qingmai Wang.2014.
Anomaly detection in online social networks.Social Networks, 39:62?70.Lei Tang and Huan Liu.
2009a.
Relational learning vialatent social dimensions.
In Proceedings of the 15thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 817?826.Lei Tang and Huan Liu.
2009b.
Scalable learning ofcollective behavior based on sparse social dimen-sions.
In Proceedings of the 18th ACM Conferenceon Information and Knowledge Management, pages1107?1116.Lei Tang and Huan Liu.
2011.
Leveraging social me-dia networks for classification.
Data Mining andKnowledge Discovery, 23:447?478.Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, JunYan, and Qiaozhu Mei.
2015.
LINE: Large-scaleinformation network embedding.
In Proceedings ofthe 24th International Conference on World WideWeb, pages 1067?1077.S.
V. N. Vishwanathan, Nicol N. Schraudolph, RisiKondor, and Karsten M. Borgwardt.
2010.
Graphkernels.
Journal of Machine Learning Research,11:1201?1242.Xi Wang and Gita Sukthankar.
2013.
Multi-label re-lational neighbor classification using social contextfeatures.
In Proceedings of the 19th ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining, pages 464?472.Zhao Xu, Volker Tresp, Shipeng Yu, and Kai Yu.
2008.Nonparametric relational learning for social networkanalysis.
In the 2nd SNA-KDD Workshop on SocialNetwork Mining and Analysis.Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun,and Edward Y. Chang.
2015.
Network representa-tion learning with rich text information.
In Proceed-ings of the 24th International Joint Conference onArtificial Intelligence, pages 2111?2117.Yiming Yang.
1999.
An evaluation of statistical ap-proaches to text categorization.
Information Re-trieval, 1:69?90.Jun Zhu, Amr Ahmed, and Eric P. Xing.
2012.MedLDA: maximum margin supervised topic mod-els.
The Journal of Machine Learning Research,13:2237?2278.Jun Zhu.
2012.
Max-margin nonparametric latentfeature models for link prediction.
In Proceedingsof the 29th International Conference on MachineLearning, pages 719?726.1013
