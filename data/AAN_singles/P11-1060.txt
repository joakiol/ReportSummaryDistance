Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 590?599,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLearning Dependency-Based Compositional SemanticsPercy LiangUC Berkeleypliang@cs.berkeley.eduMichael I. JordanUC Berkeleyjordan@cs.berkeley.eduDan KleinUC Berkeleyklein@cs.berkeley.eduAbstractCompositional question answering begins bymapping questions to logical forms, but train-ing a semantic parser to perform this mappingtypically requires the costly annotation of thetarget logical forms.
In this paper, we learnto map questions to answers via latent log-ical forms, which are induced automaticallyfrom question-answer pairs.
In tackling thischallenging learning problem, we introduce anew semantic representation which highlightsa parallel between dependency syntax and effi-cient evaluation of logical forms.
On two stan-dard semantic parsing benchmarks (GEO andJOBS), our system obtains the highest pub-lished accuracies, despite requiring no anno-tated logical forms.1 IntroductionWhat is the total population of the ten largest cap-itals in the US?
Answering these types of complexquestions compositionally involves first mapping thequestions into logical forms (semantic parsing).
Su-pervised semantic parsers (Zelle and Mooney, 1996;Tang and Mooney, 2001; Ge and Mooney, 2005;Zettlemoyer and Collins, 2005; Kate and Mooney,2007; Zettlemoyer and Collins, 2007; Wong andMooney, 2007; Kwiatkowski et al, 2010) rely onmanual annotation of logical forms, which is expen-sive.
On the other hand, existing unsupervised se-mantic parsers (Poon and Domingos, 2009) do nothandle deeper linguistic phenomena such as quan-tification, negation, and superlatives.As in Clarke et al (2010), we obviate the needfor annotated logical forms by considering the end-to-end problem of mapping questions to answers.However, we still model the logical form (now as alatent variable) to capture the complexities of lan-guage.
Figure 1 shows our probabilistic model:(parameters) (world)?
wx z y(question) (logical form) (answer)state with thelargest area x111cargmaxareastate??
Alaskaz ?
p?
(z | x)y = JzKwSemantic Parsing EvaluationFigure 1: Our probabilistic model: a question x ismapped to a latent logical form z, which is then evaluatedwith respect to a world w (database of facts), producingan answer y.
We represent logical forms z as labeledtrees, induced automatically from (x, y) pairs.We want to induce latent logical forms z (and pa-rameters ?)
given only question-answer pairs (x, y),which is much cheaper to obtain than (x, z) pairs.The core problem that arises in this setting is pro-gram induction: finding a logical form z (over anexponentially large space of possibilities) that pro-duces the target answer y.
Unlike standard semanticparsing, our end goal is only to generate the correcty, so we are free to choose the representation for z.Which one should we use?The dominant paradigm in compositional se-mantics is Montague semantics, which constructslambda calculus forms in a bottom-up manner.
CCGis one instantiation (Steedman, 2000), which is usedby many semantic parsers, e.g., Zettlemoyer andCollins (2005).
However, the logical forms therecan become quite complex, and in the context ofprogram induction, this would lead to an unwieldysearch space.
At the same time, representations suchas FunQL (Kate et al, 2005), which was used in590Clarke et al (2010), are simpler but lack the full ex-pressive power of lambda calculus.The main technical contribution of this work isa new semantic representation, dependency-basedcompositional semantics (DCS), which is both sim-ple and expressive (Section 2).
The logical forms inthis framework are trees, which is desirable for tworeasons: (i) they parallel syntactic dependency trees,which facilitates parsing and learning; and (ii) eval-uating them to obtain the answer is computationallyefficient.We trained our model using an EM-like algorithm(Section 3) on two benchmarks, GEO and JOBS(Section 4).
Our system outperforms all existingsystems despite using no annotated logical forms.2 Semantic RepresentationWe first present a basic version (Section 2.1) ofdependency-based compositional semantics (DCS),which captures the core idea of using trees to rep-resent formal semantics.
We then introduce the fullversion (Section 2.2), which handles linguistic phe-nomena such as quantification, where syntactic andsemantic scope diverge.We start with some definitions, using US geogra-phy as an example domain.
Let V be the set of allvalues, which includes primitives (e.g., 3, CA ?
V)as well as sets and tuples formed from other values(e.g., 3, {3, 4, 7}, (CA, {5}) ?
V).
Let P be a setof predicates (e.g., state, count ?
P), which arejust symbols.A world w is mapping from each predicate p ?P to a set of tuples; for example, w(state) ={(CA), (OR), .
.
.
}.
Conceptually, a world is a rela-tional database where each predicate is a relation(possibly infinite).
Define a special predicate ?
withw(?)
= V .
We represent functions by a set of input-output pairs, e.g., w(count) = {(S, n) : n = |S|}.As another example, w(average) = {(S, x?)
:x?
= |S1|?1?x?S1 S(x)}, where a set of pairs Sis treated as a set-valued function S(x) = {y :(x, y) ?
S} with domain S1 = {x : (x, y) ?
S}.The logical forms in DCS are called DCS trees,where nodes are labeled with predicates, and edgesare labeled with relations.
Formally:Definition 1 (DCS trees) Let Z be the set of DCStrees, where each z ?
Z consists of (i) a predicateRelationsRjj?
(join) E (extract)?
(aggregate) Q (quantify)Xi (execute) C (compare)Table 1: Possible relations appearing on the edges of aDCS tree.
Here, j, j?
?
{1, 2, .
.
. }
and i ?
{1, 2, .
.
.
}?.z.p ?
P and (ii) a sequence of edges z.e1, .
.
.
, z.em,each edge e consisting of a relation e.r ?
R (seeTable 1) and a child tree e.c ?
Z .We write a DCS tree z as ?p; r1 : c1; .
.
.
; rm : cm?.Figure 2(a) shows an example of a DCS tree.
Al-though a DCS tree is a logical form, note that it lookslike a syntactic dependency tree with predicates inplace of words.
It is this transparency between syn-tax and semantics provided by DCS which leads toa simple and streamlined compositional semanticssuitable for program induction.2.1 Basic VersionThe basic version of DCS restrictsR to join and ag-gregate relations (see Table 1).
Let us start by con-sidering a DCS tree z with only join relations.
Sucha z defines a constraint satisfaction problem (CSP)with nodes as variables.
The CSP has two types ofconstraints: (i) x ?
w(p) for each node x labeledwith predicate p ?
P; and (ii) xj = yj?
(the j-thcomponent of x must equal the j?-th component ofy) for each edge (x, y) labeled with jj?
?
R.A solution to the CSP is an assignment of nodesto values that satisfies all the constraints.
We say avalue v is consistent for a node x if there exists asolution that assigns v to x.
The denotation JzKw (zevaluated on w) is the set of consistent values of theroot node (see Figure 2 for an example).Computation We can compute the denotationJzKw of a DCS tree z by exploiting dynamic pro-gramming on trees (Dechter, 2003).
The recurrenceis as follows:J?p; j1j?1 :c1; ?
?
?
;jmj?m:cm?Kw(1)= w(p) ?m?i=1{v : vji = tj?i , t ?
JciKw}.At each node, we compute the set of tuples v consis-tent with the predicate at that node (v ?
w(p)), and591Example: major city in Californiaz = ?city; 11 :?major?
; 11 :?loc; 21 :?CA??
?1111major21CAloccity ?c?m?`?s .city(c) ?
major(m)?loc(`) ?
CA(s)?c1 = m1 ?
c1 = `1 ?
`2 = s1(a) DCS tree (b) Lambda calculus formula(c) Denotation: JzKw = {SF, LA, .
.
.
}Figure 2: (a) An example of a DCS tree (written in boththe mathematical and graphical notation).
Each node islabeled with a predicate, and each edge is labeled with arelation.
(b) A DCS tree z with only join relations en-codes a constraint satisfaction problem.
(c) The denota-tion of z is the set of consistent values for the root node.for each child i, the ji-th component of v must equalthe j?i-th component of some t in the child?s deno-tation (t ?
JciKw).
This algorithm is linear in thenumber of nodes times the size of the denotations.1Now the dual importance of trees in DCS is clear:We have seen that trees parallel syntactic depen-dency structure, which will facilitate parsing.
Inaddition, trees enable efficient computation, therebyestablishing a new connection between dependencysyntax and efficient semantic evaluation.Aggregate relation DCS trees that only use joinrelations can represent arbitrarily complex compo-sitional structures, but they cannot capture higher-order phenomena in language.
For example, con-sider the phrase number of major cities, and supposethat number corresponds to the count predicate.It is impossible to represent the semantics of thisphrase with just a CSP, so we introduce a new ag-gregate relation, notated ?.
Consider a tree ??
:c?,whose root is connected to a child c via ?.
If the de-notation of c is a set of values s, the parent?s denota-tion is then a singleton set containing s.
Formally:J??
:c?Kw = {JcKw}.
(2)Figure 3(a) shows the DCS tree for our runningexample.
The denotation of the middle node is {s},1Infinite denotations (such as J<Kw) are represented as im-plicit sets on which we can perform membership queries.
Theintersection of two sets can be performed as long as at least oneof the sets is finite.number ofmajor cities1211?11majorcity??count?
?average population ofmajor cities1211?1111majorcitypopulation??average??
(a) Counting (b) AveragingFigure 3: Examples of DCS trees that use the aggregaterelation (?)
to (a) compute the cardinality of a set and (b)take the average over a set.where s is all major cities.
Having instantiated s asa value, everything above this node is an ordinaryCSP: s constrains the count node, which in turnsconstrains the root node to |s|.A DCS tree that contains only join and aggre-gate relations can be viewed as a collection of tree-structured CSPs connected via aggregate relations.The tree structure still enables us to compute deno-tations efficiently based on (1) and (2).2.2 Full VersionThe basic version of DCS described thus far han-dles a core subset of language.
But consider Fig-ure 4: (a) is headed by borders, but states needsto be extracted; in (b), the quantifier no is syntacti-cally dominated by the head verb borders but needsto take wider scope.
We now present the full ver-sion of DCS which handles this type of divergencebetween syntactic and semantic scope.The key idea that allows us to give semantically-scoped denotations to syntactically-scoped trees isas follows: We mark a node low in the tree with amark relation (one of E, Q, or C).
Then higher up inthe tree, we invoke it with an execute relation Xi tocreate the desired semantic scope.2This mark-execute construct acts non-locally, soto maintain compositionality, we must augment the2Our mark-execute construct is analogous to Montague?squantifying in, Cooper storage, and Carpenter?s scoping con-structor (Carpenter, 1998).592California borders which states?x12 111CAe??stateborder??
Alaska borders no states.x12 111AKqnostateborder??
Some river traverses every city.x122 111qsomeriverqeverycitytraverse?
?x212 111qsomeriverqeverycitytraverse??
(narrow) (wide)city traversed by no riversx121 2e??
11qnorivertraversecity??
(a) Extraction (e) (b) Quantification (q) (c) Quantifier ambiguity (q,q) (d) Quantification (q,e)state borderingthe most statesx121 1e??
21cargmaxstateborderstate?
?state borderingmore states than Texasx121 1e??
21c31TXmorestateborderstate?
?state borderingthe largest state1121x121 1e??cargmaxsizestate?
?borderstatex121 1e??
2111cargmaxsizestateborderstate??
(absolute) (relative)Every state?slargest city is major.x1x21 11121qeverystateloccargmaxsizecitymajor??
(e) Superlative (c) (f) Comparative (c) (g) Superlative ambiguity (c) (h) Quantification+Superlative (q,c)Figure 4: Example DCS trees for utterances in which syntactic and semantic scope diverge.
These trees reflect thesyntactic structure, which facilitates parsing, but importantly, these trees also precisely encode the correct semanticscope.
The main mechanism is using a mark relation (E, Q, or C) low in the tree paired with an execute relation (Xi)higher up at the desired semantic point.denotation d = JzKw to include any informationabout the marked nodes in z that can be accessedby an execute relation later on.
In the basic ver-sion, d was simply the consistent assignments to theroot.
Now d contains the consistent joint assign-ments to the active nodes (which include the rootand all marked nodes), as well as information storedabout each marked node.
Think of d as consistingof n columns, one for each active node according toa pre-order traversal of z.
Column 1 always corre-sponds to the root node.
Formally, a denotation isdefined as follows (see Figure 5 for an example):Definition 2 (Denotations) Let D be the set of de-notations, where each d ?
D consists of?
a set of arrays d.A, where each array a =[a1, .
.
.
, an] ?
d.A is a sequence of n tuples(ai ?
V?
); and?
a list of n stores d.?
= (d.?1, .
.
.
, d.?n),where each store ?
contains a mark relation?.r ?
{E, Q, C, ?
}, a base denotation ?.b ?D?{?
}, and a child denotation ?.c ?
D?{?
}.We write d as ?
?A; (r1, b1, c1); .
.
.
; (rn, bn, cn)??.
Weuse d{ri = x} to mean d with d.ri = d.?i.r = x(similar definitions apply for d{?i = x}, d{bi = x},and d{ci = x}).The denotation of a DCS tree can now be definedrecursively:J?p?Kw = ??
{[v] : v ?
w(p)}; ??
?, (3)J?p; e; jj?
:c?Kw= Jp; eKw ./j,j?
JcKw, (4)J?p; e; ?
:c?Kw = Jp; eKw ./?,?
?
(JcKw) , (5)J?p; e; Xi :c?Kw = Jp; eKw ./?,?
Xi(JcKw), (6)J?p; e; E :c?Kw = M(Jp; eKw, E, c), (7)J?p; e; C :c?Kw = M(Jp; eKw, C, c), (8)J?p; Q :c; e?Kw = M(Jp; eKw, Q, c).
(9)593112111cargmaxsizestateborderstateJ?Kwcolumn 1 column 2A:(OK)(NM)(NV)?
?
?(TX,2.7e5)(TX,2.7e5)(CA,1.6e5)?
?
?r: ?
cb: ?
J?size?Kwc: ?
J?argmax?KwDCS tree DenotationFigure 5: Example of the denotation for a DCS tree witha compare relation C. This denotation has two columns,one for each active node?the root node state and themarked node size.The base case is defined in (3): if z is a sin-gle node with predicate p, then the denotation of zhas one column with the tuples w(p) and an emptystore.
The other six cases handle different edge re-lations.
These definitions depend on several opera-tions (./j,j?
,?,Xi,M) which we will define shortly,but let us first get some intuition.Let z be a DCS tree.
If the last child c of z?sroot is a join ( jj?
), aggregate (?
), or execute (Xi) re-lation ((4)?
(6)), then we simply recurse on z with cremoved and join it with some transformation (iden-tity, ?, or Xi) of c?s denotation.
If the last (or first)child is connected via a mark relation E, C (or Q),then we strip off that child and put the appropriateinformation in the store by invoking M.We now define the operations ./j,j?
,?,Xi,M.Some helpful notation: For a sequence v =(v1, .
.
.
, vn) and indices i = (i1, .
.
.
, ik), let vi =(vi1 , .
.
.
, vik) be the projection of v onto i; we writev?i to mean v[1,...,n]\i.
Extending this notation todenotations, let ??A;???
[i] = ??
{ai : a ?
A};?i?
?.Let d[??]
= d[?i], where i are the columns withempty stores.
For example, for d in Figure 5, d[1]keeps column 1, d[??]
keeps column 2, and d[2,?2]swaps the two columns.Join The join of two denotations d and d?
with re-spect to components j and j?
(?
means all compo-nents) is formed by concatenating all arrays a of dwith all compatible arrays a?
of d?, where compat-ibility means a1j = a?1j?
.
The stores are also con-catenated (?+??).
Non-initial columns with emptystores are projected away by applying ?[1,??].
Thefull definition of join is as follows:??A;???
./j,j?
??A?;????
= ??A??;?
+ ????[1,??],A??
= {a + a?
: a ?
A,a?
?
A?, a1j = a?1j?}.
(10)Aggregate The aggregate operation takes a deno-tation and forms a set out of the tuples in the firstcolumn for each setting of the rest of the columns:?
(??A;???)
= ??A?
?A??;???
(11)A?
= {[S(a), a2, .
.
.
, an] : a ?
A}S(a) = {a?1 : [a?1, a2, .
.
.
, an] ?
A}A??
= {[?, a2, .
.
.
, an] : ?
?a1,a ?
A,?2 ?
i ?
n, [ai] ?
d.bi[1].A}.2.2.1 Mark and ExecuteNow we turn to the mark (M) and execute (Xi)operations, which handles the divergence betweensyntactic and semantic scope.
In some sense, this isthe technical core of DCS.
Marking is simple: Whena node (e.g., size in Figure 5) is marked (e.g., withrelation C), we simply put the relation r, current de-notation d and child c?s denotation into the store ofcolumn 1:M(d, r, c) = d{r1 = r, b1 = d, c1 = JcKw}.
(12)The execute operation Xi(d) processes columnsi in reverse order.
It suffices to define Xi(d) for asingle column i.
There are three cases:Extraction (d.ri = E) In the basic version, thedenotation of a tree was always the set of con-sistent values of the root node.
Extraction al-lows us to return the set of consistent values of amarked non-root node.
Formally, extraction sim-ply moves the i-th column to the front: Xi(d) =d[i,?
(i, ?
)]{?1 = ?}.
For example, in Figure 4(a),before execution, the denotation of the DCS treeis ??
{[(CA, OR), (OR)], .
.
.
}; ?
; (E, J?state?Kw, ?)??
;after applying X1, we have ??
{[(OR)], .
.
.
}; ??
?.Generalized Quantification (d.ri = Q) Gener-alized quantifiers are predicates on two sets, a re-strictor A and a nuclear scope B.
For example,w(no) = {(A,B) : A ?
B = ?}
and w(most) ={(A,B) : |A ?B| > 12 |A|}.In a DCS tree, the quantifier appears as thechild of a Q relation, and the restrictor is the par-ent (see Figure 4(b) for an example).
This in-formation is retrieved from the store when the594quantifier in column i is executed.
In particu-lar, the restrictor is A = ?
(d.bi) and the nu-clear scope is B = ?
(d[i,?
(i, ?)]).
We thenapply d.ci to these two sets (technically, denota-tions) and project away the first column: Xi(d) =((d.ci ./1,1 A) ./2,1 B) [?1].For the example in Figure 4(b), the de-notation of the DCS tree before execution is???
; ?
; (Q, J?state?Kw, J?no?Kw)??.
The restrictorset (A) is the set of all states, and the nuclear scope(B) is the empty set.
Since (A,B) exists in no, thefinal denotation, which projects away the actual pair,is ??
{[ ]}??
(our representation of true).Figure 4(c) shows an example with two interact-ing quantifiers.
The quantifier scope ambiguity isresolved by the choice of execute relation; X12 givesthe narrow reading and X21 gives the wide reading.Figure 4(d) shows how extraction and quantificationwork together.Comparatives and Superlatives (d.ri = C) Tocompare entities, we use a set S of (x, y) pairs,where x is an entity and y is a number.
For su-perlatives, the argmax predicate denotes pairs ofsets and the set?s largest element(s): w(argmax) ={(S, x?)
: x?
?
argmaxx?S1 maxS(x)}.
For com-paratives, w(more) contains triples (S, x, y), wherex is ?more than?
y as measured by S; formally:w(more) = {(S, x, y) : maxS(x) > maxS(y)}.In a superlative/comparative construction, theroot x of the DCS tree is the entity to be compared,the child c of a C relation is the comparative or su-perlative, and its parent p contains the informationused for comparison (see Figure 4(e) for an exam-ple).
If d is the denotation of the root, its i-th columncontains this information.
There are two cases: (i) ifthe i-th column of d contains pairs (e.g., size inFigure 5), then let d?
= J??
?Kw ./1,2 d[i,?i], whichreads out the second components of these pairs; (ii)otherwise (e.g., state in Figure 4(e)), let d?
=J??
?Kw ./1,2 J?count?Kw ./1,1 ?
(d[i,?i]), whichcounts the number of things (e.g., states) that occurwith each value of the root x.
Given d?, we constructa denotation S by concatenating (+i) the second andfirst columns of d?
(S = ?
(+2,1 (d?
{?2 = ?
})))and apply the superlative/comparative: Xi(d) =(J??
?Kw ./1,2 (d.ci ./1,1 S)){?1 = d.?1}.Figure 4(f) shows that comparatives are handledusing the exact same machinery as superlatives.
Fig-ure 4(g) shows that we can naturally account forsuperlative ambiguity based on where the scope-determining execute relation is placed.3 Semantic ParsingWe now turn to the task of mapping natural languageutterances to DCS trees.
Our first question is: givenan utterance x, what trees z ?
Z are permissible?
Todefine the search space, we first assume a fixed setof lexical triggers L. Each trigger is a pair (x, p),where x is a sequence of words (usually one) and pis a predicate (e.g., x = California and p = CA).We use L(x) to denote the set of predicates p trig-gered by x ((x, p) ?
L).
Let L() be the set oftrace predicates, which can be introduced withoutan overt lexical trigger.Given an utterance x = (x1, .
.
.
, xn), we defineZL(x) ?
Z , the set of permissible DCS trees forx.
The basic approach is reminiscent of projectivelabeled dependency parsing: For each span i..j, webuild a set of trees Ci,j and set ZL(x) = C0,n.
Eachset Ci,j is constructed recursively by combining thetrees of its subspans Ci,k and Ck?,j for each pair ofsplit points k, k?
(words between k and k?
are ig-nored).
These combinations are then augmented viaa functionA and filtered via a functionF , to be spec-ified later.
Formally, Ci,j is defined recursively asfollows:Ci,j = F(A(L(xi+1..j) ?
?i?k?k?<ja?Ci,kb?Ck?,jT1(a, b)))).
(13)In (13), L(xi+1..j) is the set of predicates triggeredby the phrase under span i..j (the base case), andTd(a, b) = ~Td(a, b) ?
~T d(b, a), which returns allways of combining trees a and b where b is a de-scendant of a (~Td) or vice-versa ( ~T d).
The former isdefined recursively as follows: ~T0(a, b) = ?, and~Td(a, b) =?r?Rp?L(){?a; r :b?}
?
~Td?1(a, ?p; r :b?
).The latter ( ~T k) is defined similarly.
Essentially,~Td(a, b) allows us to insert up to d trace predi-cates between the roots of a and b.
This is use-ful for modeling relations in noun compounds (e.g.,595California cities), and it also allows us to underspec-ify L. In particular, our L will not include verbs orprepositions; rather, we rely on the predicates corre-sponding to those words to be triggered by traces.The augmentation function A takes a set of treesand optionally attaches E and Xi relations to theroot (e.g., A(?city?)
= {?city?
, ?city; E :??
}).The filtering function F rules out improperly-typedtrees such as ?city; 00 :?state??.
To further reducethe search space, F imposes a few additional con-straints, e.g., limiting the number of marked nodesto 2 and only allowing trace predicates between ar-ity 1 predicates.Model We now present our discriminative se-mantic parsing model, which places a log-lineardistribution over z ?
ZL(x) given an utter-ance x.
Formally, p?
(z | x) ?
e?
(x,z)>?,where ?
and ?
(x, z) are parameter and feature vec-tors, respectively.
As a running example, con-sider x = city that is in California and z =?city; 11 :?loc;21 :?CA??
?, where city triggers cityand California triggers CA.To define the features, we technically need toaugment each tree z ?
ZL(x) with alignmentinformation?namely, for each predicate in z, thespan in x (if any) that triggered it.
This extra infor-mation is already generated from the recursive defi-nition in (13).The feature vector ?
(x, z) is defined by sums offive simple indicator feature templates: (F1) a wordtriggers a predicate (e.g., [city, city]); (F2) a wordis under a relation (e.g., [that, 11]); (F3) a word is un-der a trace predicate (e.g., [in, loc]); (F4) two pred-icates are linked via a relation in the left or rightdirection (e.g., [city, 11, loc, RIGHT]); and (F5) apredicate has a child relation (e.g., [city, 11]).Learning Given a training dataset D con-taining (x, y) pairs, we define the regu-larized marginal log-likelihood objectiveO(?)
=?
(x,y)?D log p?
(JzKw = y | x, z ?ZL(x)) ?
???
?22, which sums over all DCS trees zthat evaluate to the target answer y.Our model is arc-factored, so we can sum over allDCS trees in ZL(x) using dynamic programming.However, in order to learn, we need to sum over{z ?
ZL(x) : JzKw = y}, and unfortunately, theadditional constraint JzKw = y does not factorize.We therefore resort to beam search.
Specifically, wetruncate each Ci,j to a maximum of K candidatessorted by decreasing score based on parameters ?.Let Z?L,?
(x) be this approximation of ZL(x).Our learning algorithm alternates between (i) us-ing the current parameters ?
to generate the K-bestset Z?L,?
(x) for each training example x, and (ii)optimizing the parameters to put probability masson the correct trees in these sets; sets contain-ing no correct answers are skipped.
Formally, letO?
(?, ??)
be the objective function O(?)
with ZL(x)replaced with Z?L,??(x).
We optimize O?
(?, ??)
bysetting ?
(0) = ~0 and iteratively solving ?
(t+1) =argmax?
O?
(?, ?
(t)) using L-BFGS until t = T .
In allexperiments, we set ?
= 0.01, T = 5, andK = 100.After training, given a new utterance x, our systemoutputs the most likely y, summing out the latentlogical form z: argmaxy p?
(T )(y | x, z ?
Z?L,?
(T )).4 ExperimentsWe tested our system on two standard datasets, GEOand JOBS.
In each dataset, each sentence x is an-notated with a Prolog logical form, which we useonly to evaluate and get an answer y.
This evalua-tion is done with respect to a world w. Recall thata world w maps each predicate p ?
P to a set oftuples w(p).
There are three types of predicates inP: generic (e.g., argmax), data (e.g., city), andvalue (e.g., CA).
GEO has 48 non-value predicatesand JOBS has 26.
For GEO, w is the standard USgeography database that comes with the dataset.
ForJOBS, if we use the standard Jobs database, close tohalf the y?s are empty, which makes it uninteresting.We therefore generated a random Jobs database in-stead as follows: we created 100 job IDs.
For eachdata predicate p (e.g., language), we add each pos-sible tuple (e.g., (job37, Java)) to w(p) indepen-dently with probability 0.8.We used the same training-test splits as Zettle-moyer and Collins (2005) (600+280 for GEO and500+140 for JOBS).
During development, we fur-ther held out a random 30% of the training sets forvalidation.Our lexical triggers L include the following: (i)predicates for a small set of ?
20 function words(e.g., (most, argmax)), (ii) (x, x) for each value596System AccuracyClarke et al (2010) w/answers 73.2Clarke et al (2010) w/logical forms 80.4Our system (DCS with L) 78.9Our system (DCS with L+) 87.2Table 2: Results on GEO with 250 training and 250test examples.
Our results are averaged over 10 random250+250 splits taken from our 600 training examples.
Ofthe three systems that do not use logical forms, our twosystems yield significant improvements.
Our better sys-tem even outperforms the system that uses logical forms.predicate x in w (e.g., (Boston, Boston)), and(iii) predicates for each POS tag in {JJ, NN, NNS}(e.g., (JJ, size), (JJ, area), etc.
).3 Predicatescorresponding to verbs and prepositions (e.g.,traverse) are not included as overt lexical trig-gers, but rather in the trace predicates L().We also define an augmented lexicon L+ whichincludes a prototype word x for each predicate ap-pearing in (iii) above (e.g., (large, size)), whichcancels the predicates triggered by x?s POS tag.
ForGEO, there are 22 prototype words; for JOBS, thereare 5.
Specifying these triggers requires minimaldomain-specific supervision.Results We first compare our system with Clarkeet al (2010) (henceforth, SEMRESP), which alsolearns a semantic parser from question-answer pairs.Table 2 shows that our system using lexical triggersL (henceforth, DCS) outperforms SEMRESP (78.9%over 73.2%).
In fact, although neither DCS norSEMRESP uses logical forms, DCS uses even less su-pervision than SEMRESP.
SEMRESP requires a lex-icon of 1.42 words per non-value predicate, Word-Net features, and syntactic parse trees; DCS requiresonly words for the domain-independent predicates(overall, around 0.5 words per non-value predicate),POS tags, and very simple indicator features.
Infact, DCS performs comparably to even the versionof SEMRESP trained using logical forms.
If we addprototype triggers (use L+), the resulting system(DCS+) outperforms both versions of SEMRESP bya significant margin (87.2% over 73.2% and 80.4%).3We used the Berkeley Parser (Petrov et al, 2006) to per-form POS tagging.
The triggers L(x) for a word x thus includeL(t) where t is the POS tag of x.System GEO JOBSTang and Mooney (2001) 79.4 79.8Wong and Mooney (2007) 86.6 ?Zettlemoyer and Collins (2005) 79.3 79.3Zettlemoyer and Collins (2007) 81.6 ?Kwiatkowski et al (2010) 88.2 ?Kwiatkowski et al (2010) 88.9 ?Our system (DCS with L) 88.6 91.4Our system (DCS with L+) 91.1 95.0Table 3: Accuracy (recall) of systems on the two bench-marks.
The systems are divided into three groups.
Group1 uses 10-fold cross-validation; groups 2 and 3 use the in-dependent test set.
Groups 1 and 2 measure accuracy oflogical form; group 3 measures accuracy of the answer;but there is very small difference between the two as seenfrom the Kwiatkowski et al (2010) numbers.
Our bestsystem improves substantially over past work, despite us-ing no logical forms as training data.Next, we compared our systems (DCS and DCS+)with the state-of-the-art semantic parsers on the fulldataset for both GEO and JOBS (see Table 3).
Allother systems require logical forms as training data,whereas ours does not.
Table 3 shows that even DCS,which does not use prototypes, is comparable to thebest previous system (Kwiatkowski et al, 2010), andby adding a few prototypes, DCS+ offers a decisiveedge (91.1% over 88.9% on GEO).
Rather than us-ing lexical triggers, several of the other systems useIBM word alignment models to produce an initialword-predicate mapping.
This option is not avail-able to us since we do not have annotated logicalforms, so we must instead rely on lexical triggersto define the search space.
Note that having lexicaltriggers is a much weaker requirement than havinga CCG lexicon, and far easier to obtain than logicalforms.Intuitions How is our system learning?
Initially,the weights are zero, so the beam search is essen-tially unguided.
We find that only for a small frac-tion of training examples do the K-best sets containany trees yielding the correct answer (29% for DCSon GEO).
However, training on just these exam-ples is enough to improve the parameters, and this29% increases to 66% and then to 95% over the nextfew iterations.
This bootstrapping behavior occursnaturally: The ?easy?
examples are processed first,where easy is defined by the ability of the current597model to generate the correct answer using any tree.Our system learns lexical associations betweenwords and predicates.
For example, area (by virtueof being a noun) triggers many predicates: city,state, area, etc.
Inspecting the final parameters(DCS on GEO), we find that the feature [area, area]has a much higher weight than [area, city].
Tracepredicates can be inserted anywhere, but the fea-tures favor some insertions depending on the wordspresent (for example, [in, loc] has high weight).The errors that the system makes stem from mul-tiple sources, including errors in the POS tags (e.g.,states is sometimes tagged as a verb, which triggersno predicates), confusion of Washington state withWashington D.C., learning the wrong lexical asso-ciations due to data sparsity, and having an insuffi-ciently large K.5 DiscussionA major focus of this work is on our semantic rep-resentation, DCS, which offers a new perspectiveon compositional semantics.
To contrast, considerCCG (Steedman, 2000), in which semantic pars-ing is driven from the lexicon.
The lexicon en-codes information about how each word can used incontext; for example, the lexical entry for bordersis S\NP/NP : ?y.
?x.border(x, y), which meansborders looks right for the first argument and leftfor the second.
These rules are often too stringent,and for complex utterances, especially in free word-order languages, either disharmonic combinators areemployed (Zettlemoyer and Collins, 2007) or wordsare given multiple lexical entries (Kwiatkowski etal., 2010).In DCS, we start with lexical triggers, which aremore basic than CCG lexical entries.
A trigger forborders specifies only that border can be used, butnot how.
The combination rules are encoded in thefeatures as soft preferences.
This yields a morefactorized and flexible representation that is easierto search through and parametrize using features.It also allows us to easily add new lexical triggerswithout becoming mired in the semantic formalism.Quantifiers and superlatives significantly compli-cate scoping in lambda calculus, and often type rais-ing needs to be employed.
In DCS, the mark-executeconstruct provides a flexible framework for dealingwith scope variation.
Think of DCS as a higher-levelprogramming language tailored to natural language,which results in programs (DCS trees) which aremuch simpler than the logically-equivalent lambdacalculus formulae.The idea of using CSPs to represent semantics isinspired by Discourse Representation Theory (DRT)(Kamp and Reyle, 1993; Kamp et al, 2005), wherevariables are discourse referents.
The restriction totrees is similar to economical DRT (Bos, 2009).The other major focus of this work is programinduction?inferring logical forms from their deno-tations.
There has been a fair amount of past work onthis topic: Liang et al (2010) induces combinatorylogic programs in a non-linguistic setting.
Eisen-stein et al (2009) induces conjunctive formulae anduses them as features in another learning problem.Piantadosi et al (2008) induces first-order formu-lae using CCG in a small domain assuming observedlexical semantics.
The closest work to ours is Clarkeet al (2010), which we discussed earlier.The integration of natural language with denota-tions computed against a world (grounding) is be-coming increasingly popular.
Feedback from theworld has been used to guide both syntactic parsing(Schuler, 2003) and semantic parsing (Popescu etal., 2003; Clarke et al, 2010).
Past work has also fo-cused on aligning text to a world (Liang et al, 2009),using text in reinforcement learning (Branavan et al,2009; Branavan et al, 2010), and many others.
Ourwork pushes the grounded language agenda towardsdeeper representations of language?think groundedcompositional semantics.6 ConclusionWe built a system that interprets natural languageutterances much more accurately than existing sys-tems, despite using no annotated logical forms.
Oursystem is based on a new semantic representation,DCS, which offers a simple and expressive alter-native to lambda calculus.
Free from the burdenof annotating logical forms, we hope to use ourtechniques in developing even more accurate andbroader-coverage language understanding systems.Acknowledgments We thank Luke Zettlemoyerand Tom Kwiatkowski for providing us with dataand answering questions.598ReferencesJ.
Bos.
2009.
A controlled fragment of DRT.
In Work-shop on Controlled Natural Language, pages 1?5.S.
Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay.2009.
Reinforcement learning for mapping instruc-tions to actions.
In Association for Computational Lin-guistics and International Joint Conference on NaturalLanguage Processing (ACL-IJCNLP), Singapore.
As-sociation for Computational Linguistics.S.
Branavan, L. Zettlemoyer, and R. Barzilay.
2010.Reading between the lines: Learning to map high-levelinstructions to commands.
In Association for Compu-tational Linguistics (ACL).
Association for Computa-tional Linguistics.B.
Carpenter.
1998.
Type-Logical Semantics.
MIT Press.J.
Clarke, D. Goldwasser, M. Chang, and D. Roth.2010.
Driving semantic parsing from the world?s re-sponse.
In Computational Natural Language Learn-ing (CoNLL).R.
Dechter.
2003.
Constraint Processing.
Morgan Kauf-mann.J.
Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.2009.
Reading to learn: Constructing features fromsemantic abstracts.
In Empirical Methods in NaturalLanguage Processing (EMNLP), Singapore.R.
Ge and R. J. Mooney.
2005.
A statistical semanticparser that integrates syntax and semantics.
In Compu-tational Natural Language Learning (CoNLL), pages9?16, Ann Arbor, Michigan.H.
Kamp and U. Reyle.
1993.
From Discourse to Logic:An Introduction to the Model-theoretic Semantics ofNatural Language, Formal Logic and Discourse Rep-resentation Theory.
Kluwer, Dordrecht.H.
Kamp, J. v. Genabith, and U. Reyle.
2005.
Discourserepresentation theory.
In Handbook of PhilosophicalLogic.R.
J. Kate and R. J. Mooney.
2007.
Learning lan-guage semantics from ambiguous supervision.
In As-sociation for the Advancement of Artificial Intelligence(AAAI), pages 895?900, Cambridge, MA.
MIT Press.R.
J. Kate, Y. W. Wong, and R. J. Mooney.
2005.Learning to transform natural to formal languages.
InAssociation for the Advancement of Artificial Intel-ligence (AAAI), pages 1062?1068, Cambridge, MA.MIT Press.T.
Kwiatkowski, L. Zettlemoyer, S. Goldwater, andM.
Steedman.
2010.
Inducing probabilistic CCGgrammars from logical form with higher-order unifi-cation.
In Empirical Methods in Natural LanguageProcessing (EMNLP).P.
Liang, M. I. Jordan, and D. Klein.
2009.
Learning se-mantic correspondences with less supervision.
In As-sociation for Computational Linguistics and Interna-tional Joint Conference on Natural Language Process-ing (ACL-IJCNLP), Singapore.
Association for Com-putational Linguistics.P.
Liang, M. I. Jordan, and D. Klein.
2010.
Learningprograms: A hierarchical Bayesian approach.
In In-ternational Conference on Machine Learning (ICML).Omnipress.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In International Conference on Computa-tional Linguistics and Association for ComputationalLinguistics (COLING/ACL), pages 433?440.
Associa-tion for Computational Linguistics.S.
T. Piantadosi, N. D. Goodman, B.
A. Ellis, and J. B.Tenenbaum.
2008.
A Bayesian model of the acquisi-tion of compositional semantics.
In Proceedings of theThirtieth Annual Conference of the Cognitive ScienceSociety.H.
Poon and P. Domingos.
2009.
Unsupervised semanticparsing.
In Empirical Methods in Natural LanguageProcessing (EMNLP), Singapore.A.
Popescu, O. Etzioni, and H. Kautz.
2003.
Towardsa theory of natural language interfaces to databases.In International Conference on Intelligent User Inter-faces (IUI).W.
Schuler.
2003.
Using model-theoretic semantic inter-pretation to guide statistical parsing and word recog-nition in a spoken language interface.
In Associationfor Computational Linguistics (ACL).
Association forComputational Linguistics.M.
Steedman.
2000.
The Syntactic Process.
MIT Press.L.
R. Tang and R. J. Mooney.
2001.
Using multipleclause constructors in inductive logic programming forsemantic parsing.
In European Conference on Ma-chine Learning, pages 466?477.Y.
W. Wong and R. J. Mooney.
2007.
Learning syn-chronous grammars for semantic parsing with lambdacalculus.
In Association for Computational Linguis-tics (ACL), pages 960?967, Prague, Czech Republic.Association for Computational Linguistics.M.
Zelle and R. J. Mooney.
1996.
Learning to parsedatabase queries using inductive logic proramming.
InAssociation for the Advancement of Artificial Intelli-gence (AAAI), Cambridge, MA.
MIT Press.L.
S. Zettlemoyer and M. Collins.
2005.
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In Uncer-tainty in Artificial Intelligence (UAI), pages 658?666.L.
S. Zettlemoyer and M. Collins.
2007.
Online learn-ing of relaxed CCG grammars for parsing to logicalform.
In Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP/CoNLL), pages 678?687.599
