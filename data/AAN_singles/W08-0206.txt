Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 45?53,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsThe evolution of a statistical NLP courseFei XiaUniversity of WashingtonDepartment of LinguisticsBox 354340Seattle, WA 98195-4340fxia@u.washington.eduAbstractThis paper describes the evolution of a statis-tical NLP course, which I have been teachingevery year for the past three years.
The pa-per will focus on major changes made to thecourse (including the course design, assign-ments, and the use of discussion board) andhighlight the lessons learned from this experi-ence.1 IntroductionIn the past two decades, there has been tremendousprogress in natural language processing (NLP) andNLP techniques have been applied to various real-world applications such as internet/intranet searchand information extraction.
Consequently, therehas been an increasing demand from the industryfor people with special training in NLP.
To meetthe demand, the University of Washington recentlylaunched a new Professional Masters Program inComputational Linguistics (CLMA).
To earn themaster?s degree, students must take nine courses andcomplete a final project.
The detail of the programcan be found in (Bender et al, 2008).One of the required courses is LING572 (Ad-vanced statistical methods in NLP), a course thatI have been teaching every year for the past threeyears.
During the process and especially in Year 3, Ihave made many changes to course content, assign-ments, and the usage of discussion board.
In thispaper, I will describe the evolution of the course andhighlight the lessons learned from this experience.2 BackgroundLING572 is part of the four-course NLP core se-quence in the CLMA program.
The other threeare LING570 (Shallow Processing Techniques forNLP), LING571 (Deep Processing Techniques forNLP), and LING573 (NLP Systems and Applica-tions).
LING570 and LING571 are organized byNLP tasks (e.g., language model, POS tagging,Named-entity tagging, chunking for LING570, andparsing, semantics and discourse for LING571);LING572 is organized by machine learning meth-ods; LING573 is the place where students use theknowledge learned in LING570-572 to build NLPsystems for some real applications (e.g., questionanswering and information extraction).The prerequisites for LING572 are (1) at leastone college-level course in probability and statistics,(2) strong programming skills, and (3) LING570.The quarter is ten weeks long, with two 80-minutesessions per week.
The class size is relativelysmall, with ten to twenty students.
Most studentsin LING572 are from the CLMA program and aretaking LING571 and other NLP courses at the sametime.
About a half of the students are from computerscience background, and the remaining are from lin-guistics or other humanity background.3 Course contentIt would be impossible to cover all the major ma-chine learning (ML) algorithms in one quarter;therefore, one of the decisions that I made from thevery beginning is that the course would focus onmajor classification algorithms and spend only oneweek showing how these algorithms can be appliedto sequence labeling problems.
I believe that once45students have a solid grasp of these algorithms, itwould be easy for them to learn algorithms for otherkinds of learning problems (e.g., regression, cluster-ing, and ranking).The next question is what classification algo-rithms should be included in the syllabus.
Table 1shows the topics covered each year and the num-ber of sessions spent on each topic.
The topics canbe roughly divided into three units: (1) supervisedlearning, (2) semi- and unsupervised learning, and(3) other related topics.3.1 Year 1The teaching plan for Year 1 turned out to be tooambitious.
For instance, six supervised algorithmswere covered in four weeks (i.e., 8 sessions) and foursemi-/unsupervised algorithms were covered in 2.5weeks.
Such a tight schedule did not leave sufficienttime for students to digest all the important conceptsand equations.3.2 Year 2In Year 2, I reduced the amount of time spent on Unit(2).
For instance, I spent only one session discussingthe main ideas in the EM algorithm, without goingthrough the details of the mathematic deduction andspecial cases of the algorithm such as the backward-forward algorithm and the inside-outside algorithm.Other changes were made to other units, as shown inthe second column of Table 1.3.3 Year 3In the first two years, my lecturing style was simi-lar to the tutorials given at major NLP conferences(e.g., ACL) in that I covered a lot of material in ashort period of time and expected students to digestall the details after class.
This approach did not workvery well because our students came from very di-verse backgrounds (e.g., linguistics, literature, com-puter science) and many of them were not famil-iar with mathematic concepts (e.g., Lagrangian, dualproblem, quadratic programming, hill climbing) thatare commonly used in machine learning.
Most ofthe students were also new to NLP, taking onlyone quarter of NLP-related courses before takingLING572.Based on this observation, I made major changesto the syllabus in Year 3: I reduced the lecture ma-Table 1: Content changes over the years (-: topics notcovered, *: topics moved to LING570 in the previousquarter, ?
: topics moved to an NLP seminar)Y1 Y2 Y3(1) Supervised learning:kNN - 1 1Decision Tree 1 1 1Decision List 1 1 -Naive Bayes - 1 2Maximum entropy (MaxEnt) 2 2 4Support vector machine (SVM) - - 4Transformation-based learning (TBL) 2 1 1Bagging 1 1 -Boosting 1 2 -subtotal 8 10 13(2) Semi-/unsupervised learning:Semisupervised 2 1 1?Unsupervised 3 1 -?subtotal 5 2 1(3) Other topics:Introduction 1 1 1Information theory - - 1Feature selection - 1 1System combination 1 - -Relation between FSA and HMM 1 - -*Multiclass?
binary - 1 -*Beam search - 1 -*Student presentation 1 1 -Recap, summary 3 2 3subtotal 7 7 6Total 20 19 20terial and spent more time on discussion and illus-trative examples.
For example, on average 1.25 ses-sions were spent on a supervised algorithm in Year2 and that number increased to 2.17 sessions in Year3.
I also added one session on information theory,which provides theoretic foundation for many learn-ing methods.
Because of this change, some impor-tant topics had to be cut, as shown in the last col-umn of Table 1.
Fortunately, I was able to incorpo-rate some of the removed topics to two other courses(i.e., LING570 and a seminar on semi- and unsuper-vised learning) that I was teaching in the same year.The feedback from students indicated that the newcourse plan for Year 3 was more effective than theones for the previous two years.Another change I made was that I divided theteaching material into three types: (1) the essentialknowledge that students should fully understand, (2)46more advanced topics that students should be awareof but do not have to understand all the details, and(3) related topics that are not covered in class but areavailable on additional slides for people who wantto learn more by themselves.
Taking MaxEnt as anexample, Type (1) includes the maximum entropyprinciple, the modeling, GIS training, and decod-ing; Type (2) includes regularization and the math-ematic proof that shows the relation between maxi-mum likelihood and maximum entropy as providedin (Ratnaparkhi, 1997), and Type (3) includes L-BFGS training and the similarity between SVM andMaxEnt with regularization (Klein, 2007).
Makingthis distinction helps students focus on the most es-sential part of the algorithms and at the same timeprovides additional material for more advanced stu-dents.4 Reading materialOne challenge of teaching a statistic NLP course isthe lack of good textbooks on the subject; as a result,most of the reading material come from conferenceand journal papers.
The problem is that many of thealgorithms covered in class were originally proposedin non-NLP fields such as machine learning and ap-plied mathematics, and the original papers are oftenheavy in mathematical proofs and rarely refer to theNLP tasks that our students are familiar with.
On theother hand, NLP papers that apply these algorithmsto NLP tasks often assume that the readers are al-ready familiar with the algorithms and consequentlydo not explain the algorithms in detail.Because it is hard to find a suitable paper to coverall the theoretic and application aspects of a learningalgorithm, I chose several papers for each algorithmand specified the sections that the students shouldfocus on.
For instance, for Maximum Entropy, Ipicked (Berger et al, 1996; Ratnaparkhi, 1997) forthe basic theory, (Ratnaparkhi, 1996) for an appli-cation (POS tagging in this case), and (Klein andManning, 2003) for more advanced topics such asoptimization and smoothing.For the more sophisticated learning methods (e.g.,MaxEnt and SVM), it is very important for studentsto read the assigned papers beforehand.
However,some students choose not to do so for various rea-sons; meanwhile, other students might spend toomuch time trying to understand everything in the pa-pers.
To address this problem, in Year 3 I added fivereading assignments, one for each of the followingtopics: information theory, Naive Bayes, MaxEnt,SVM, and TBL.
Each assignment consists of simplequestions such as the one in Appendix A. Studentswere asked to turn in their answers to the questionsbefore class.
Although the assignments were verysimple, the effect was obvious as students started toask in-depth questions even before the topics werecovered in class.5 Written and programming assignmentsIn addition to the reading assignments mentionedabove, students also have weekly assignments.
Forthe sake of clarity, we divide the latter into twotypes, written and programming assignments, de-pending on whether programming is required.
Sig-nificant changes have been made to both types, asexplained below.5.1 Year 1In Year 1, there were three written and six pro-gramming assignments.
The written assignmentswere mainly on mathematic proof, and one exampleis given in Appendix B.
The programming assign-ments asked students to use the following existingpackages to build NLP systems.1.
Carmel, a finite state transducer package writ-ten by Jonathan Graehl at USC/ISI.2.
fnTBL (Ngai and Florian, 2001), an efficientimplementation of TBL created by Ngai andFlorian at JHU.3.
A MaxEnt toolkit writtenby Le Zhang, available athttp://homepages.inf.ed.ac.uk/s0450736.To complete the assignments, the students neededto study some functions in the source code to under-stand exactly how the learning algorithms were im-plemented in the packages.
They would then writepre- and post-processing tools, create data files, andbuild an end-to-end system for a particular NLPtask.
They would then present their work in classand write a final paper to report their findings.475.2 Year 2In Year 2 I made two major changes to the assign-ments.
First, I reduced the number of written as-signments on theoretic proof.
While such assign-ments strengthen students?
mathematic capability,they were very challenging to many students, espe-cially the ones who lacked mathematics and statis-tics training.
The assignments were also not as ef-fective as programming assignments in understand-ing the basic concepts for the learning algorithms.The second major change was to replace the threepackages mentioned above with Mallet (McCallum,2002), a well-known package in the NLP field.
Mal-let is a well-designed package that contains almostall the learning methods covered in the course suchas Naive Bayes, decision tree, MaxEnt, boosting,and bagging; once the training and test data wereput into the Mallet data format, it was easy to run allthese methods and compared the results.For the programming assignments, in additionto reading certain Mallet functions to understandhow the learning methods were implemented, thestudents were also asked to extend the packagein various ways.
For instance, the package in-cludes a text-user-interface (TUI) class called Vec-tors2Classify.java, which produces a classifier fromthe training data, uses the classifier to classify thetest data, compares the results with gold standard,and outputs accuracy and the confusion matrix.
Inone assignment, students were asked to first sepa-rate the code for training and testing, and then addthe beam search to the latter module so that the newcode would work for sequence labeling tasks.While using Mallet as a black box is straight-forward, extending it with additional functionalityis much more difficult.
Because the package didnot have a detailed document that explained howits main classes should be used, I spent more thana week going through hundreds of classes in Mal-let and wrote a 11-page guide based on my find-ings.
While the guide was very helpful, many stu-dents still struggled with the assignments, especiallythe ones who were not used to navigating throughother people?s code and/or who were not familiarwith Java, the language that Mallet was written in.5.3 Year 3To address these problems, in Year 3, we changedthe focus of the assignments: instead of studying andextending Mallet code, students would create theirown package from scratch and use Mallet only asa reference.
For instance, in one assignment, stu-dents would implement the two Naive Bayes mod-els as described in (McCallum and Nigam, 1998)and compare the classification results with the re-sults produced by the Mallet Naive Bayes learner.
Inthe beam search assignment, students?
code wouldinclude modules that read in the model produced byMallet and calculate P (y | x) for a test instancex and a class label y.
Because the code no longerneeded to call Mallet functions, students were free touse whatever language they were comfortable withand could treat Mallet as a black box.The complete assignments are shown in AppendixC.
In summary, students implemented six learners(Naive Bayes, kNN, Decision Tree, MaxEnt, SVM,TBL),1 beam search, and the code for feature selec-tion.
All the coding was completed in eight weeksin total, and the students could choose to either workalone or work with a teammate.6 Implementation issuesAll the programming assignments in Year 3, exceptthe one for MaxEnt, were due in a week, and stu-dents were expected to spend between 10 and 20hours on each assignment.
While the workload wasdemanding, about 90% of students completed the as-signments successfully.
Several factors contribute tothe success:?
All the learners were evaluated on the sameclassification task.2 The input and output dataformat were very similar across different learn-ers; as a result, the code that handled input andoutput could be reused, and the classificationresults of different learners could be compareddirectly.1For SVM, students implemented only the decoder, not thetrainer, and they would test their decoder with the models pro-duced by libSVM (Chang and Lin, 2001).2The task is a simplified version of the classic 20-newsgrouptext classification task, with only three out of the 20 classes be-ing used.
The training data and the test data consist of 900 and100 examples from each class, respectively.48?
I restricted the scope of the assignments so thatthey were doable in a week.
For instance, thecomplexity of a TBL learner highly depends onthe form of the transformations and the type oflearning problem.
In the TBL assignment, thelearner was used to handle classification prob-lems and the transformation had the form if afeature is present in an instance, change theclass label from A to B.
Implementing sucha learner was much easier than implementinga learner (e.g., fnTBL) that use more com-plex transformations to handle sequence label-ing problems.?
Efficiency is an important issue, and there areoften differences between algorithms on pa-per and the code that implements the algo-rithms.
To identify those differences and po-tential pitfalls that students could run into, Icompleted all the assignments myself at leasta week before the assignments were due, andshared some of my findings in class.
I also toldstudents the kind of results to be expected, andencouraged students to discuss the results andimplementation tricks on the discussion board.Implementing machine learning algorithms is of-ten an art, as there are many ways to improve effi-ciency.
Two examples are given below.
While suchtricks are well-known to NLP researchers, they areoften new to students and going through them inclass can help students to speed up their code sig-nificantly.The trainer for TBLAs described in (Brill, 1995), a TBL trainer picksone transformation in each iteration, applies it to thetraining data, and repeats the process until no moregood transformations can be found.
To choose thebest transformation, a naive approach would enu-merate all the possible transformations, for eachtransformation go through the data once to calculatethe net gain, and choose the transformation with thehighest net gain.
This approach is very inefficient asthe data have to be scanned through multiple times.33Let Nf be the number of features and Nc be the numberof classes in a classification task, the number of transformationsin the form we specified above is O(NfN2c ), which means thatthe learner has to go through the data O(NfN2c ) times.A much better implementation would be to gothrough the training data only once, and for each fea-ture in each training instance, update the net gainsof the corresponding transformations accordingly.4Students were also encouraged to read (Ngai andFlorian, 2001), which proposed another efficient im-plementation of TBL.The decoder for Naive BayesIn the multi-variate Bernoulli event model forthe text classification task (McCallum and Nigam,1998), at the test time the class for a document d ischosen according to Eq (1).
If we calculate P (d|c)according to Eq (2), as given in the paper, we haveto go through all the features in the feature set F .However, as shown in Eq (3) and (4), the first prod-uct in Eq (3), denoted as Z(c) in Eq (4), is a constantwith respect to d and can be calculated beforehandand stored with each c. Therefore, to classify d, weonly need to go through the features that are presentin d. Implementing Eq (4) instead of Eq (2) reducesrunning time tremendously.5c?
= arg maxcP (c)P (d|c) (1)P (d|c) =?f 6?d(1?
P (f |c))?f?dP (f |c) (2)=?f?F(1?
P (f |c))?f?dP (f |c)1?
P (f |c)(3)= Z(c)?f?dP (f |c)1?
P (f |c)(4)7 Discussion boardA discussion board is one of the most effective ve-hicles for outside-class communication and collab-oration, as anyone in the class can start a new con-versation, read recent posts, or reply to other peo-4For each feature t in each training instance x, if x?s currentlabel yc is different from the true label y, there would be onlyone transformation whose net gain would be affected by thisfeature in this instance, and the transformation is if t is present,change class label from yc to y.
If yc is the same as y, therewould be Nc ?
1 transformations whose net gain would be af-fected, where Nc is the number of classes.5This trick was actually pointed out by a student in my class.49ple?s posts.6 Furthermore, some students feel morecomfortable posting to a discussion board than rais-ing the questions in class or emailing the instructor.Therefore, I provided a discussion board each timeLING572 was offered and the board was linked tothe course website.
However, the board was not usedas much as I had hoped in the first two years.In Year 3, I took a more pro-active approach:first, I reminded students several times that emailsto me should be reserved only for confidential ques-tions and all the non-confidential questions shouldbe posted to the discussion board.
They should alsocheck the discussion board at least daily and theywere encouraged to reply to their classmates?
ques-tions if they knew the answers.
Second, if a stu-dent emailed me any questions that should go to thediscussion board, I would copy the questions to theboard and ask the sender to find my answers there.Third, I checked the board several times per day andmost of the questions raised there were answeredwithin an hour if not sooner.As a result, there was a significant increase of theusage of the board, as shown in Table 2.
For in-stance, compared to Year 2, the average number ofposts per student in Year 3 more than quadrupled,and at the same time the number of emails I re-ceived from the students was cut by 65%.
More im-portantly, more than a half of the questions postedto the board were answered by other students, in-dicating the board indeed encouraged collaborationamong students.A lesson I learned from this experience is that thesuccess of a discussion board relies on active partic-ipation by its members, and strong promotion by theinstructor is essential in helping students take advan-tage of this form of communication.8 Course evaluationStudents were asked to evaluate the course at theend of the quarter using standard evaluation forms.The results are shown in Table 3.7 For (1)-(11), stu-dents were asked to answer the questions with a 6-6The software we used is called GoPost.
It is one of theWeb-based communication and collaboration applications de-veloped by the Center for Teaching, Learning, and Technologyat the University of Washington.7The complete form has thirty questions, the most relevantones are listed in the table.Table 2: The usage of the course discussion boardY1 Y2 Y3# of students 15 16 11# of conversations 13 47 116Total # of posts 42 149 589# of posts by the instructor 7 21 158# of posts by students 35 128 431Ave # of post/student 2.3 8 39.2point scale: 0 being Very Poor and 5 being Excel-lent.
The question for (12) is ?on average how manyhours per week have you spent on this course??
; Thequestion for (13) is ?among the total average hoursspent on the course, how many do you consider werevaluable in advancing your education??
The valuesfor (1)-(13) in the table are the average of the re-sponses.
The last row, Challenge and EngagementIndex (CEI), is a score computed from several itemson the evaluation form, and reported as a decile rankranging from 0 (lowest) to 9 (highest).
It reflectshow challenging students found the course and howengaged they were in it.The table shows that the overall evaluation in Year2 was worse than the one in Year 1, despite mucheffort put into improving course design.
The mainproblem in Year 2 was the programming assign-ments, as discussed in Section 5.2 and indicated inRow (11): many students found the task of extend-ing Mallet overwhelming, especially since some ofthem had never used Java and debugged a large pre-existing package before.
As a result, they spentmuch time on learning Java and trying to figure outhow Mallet code worked, and they felt that was notthe best way to learn the subjects (cf.
the big gapbetween the values for Row (12) and (13)).Based on the feedback from the first two years,in Year 3 I made a major overhaul to the course, asdiscussed in Sections 3-7 and summarized here:?
The lecture material was cut substantially; forinstance, the average number of slides used in asession was reduced from over 60 in Year 2 tobelow 30 in Year 3.
The saved time was spenton class discussion and going through exam-ples on the whiteboard.?
Reading assignments were introduced to helpstudents focus on the most relevant part of the50Table 3: Student evaluation of instruction (For Item (1)-(11), the scale is between 0 and 5: 0 is Very Poor and 5 isExcellent; The scale for Item (14) is between 0 and 9, 9 being most challenging)Y1 Y2 Y3Number of respondents 14 15 11(1) The course as a whole 3.9 3.8 5.0(2) The course content 4.0 3.7 4.9(3) Course organization 4.0 3.8 4.8(4) Explanations by instructor 3.7 3.5 4.6(5) Student confidence in instructor?s knowledge 4.5 4.5 4.9(6) Instructor?s enthusiasm 4.5 4.6 4.9(7) Encouragement given students to express themselves 3.9 4.3 4.6(8) Instructor?s interest in whether students learned 4.5 4.0 4.8(9) Amount you learned in the course 3.8 3.3 4.8(10) Relevance and usefulness of course content 4.3 3.9 4.9(11) Reasonableness of assigned work 3.8 2.0 3.8(12) Average number of hours/week spent on the course 7.9 21.9 19.8(13) How many were valuable in advancing your education 6.2 14.5 17.2(14) Challenge and engagement index (CEI) 7 9 9reading material.?
Instead of extending Mallet, students wereasked to create their own packages from scratchand many implementation issues were ad-dressed in class and in the discussion board.?
Discussion board was highly promoted to en-courage outside-class discussion and collabora-tion, and its usage was increased dramatically.As shown in the last column of the table, thenew strategies worked very well and the feedbackfrom the students was very positive.
Interestingly,although the amount of time spent on the coursein Y2 and Y3 was about the same, the students inY3 felt the assigned work was more reasonable thanthe students in Y2.
This highlights the importanceof choosing appropriate assignments based on stu-dents?
background.
Also, while the lecture materialwas cut substantially over the years, students in Y3felt that they learned more than the students in Y1and Y2, implying that it is more beneficial to covera small number of learning methods in depth than tohurry through a large number of topics.9 ConclusionTeaching LING572 has been a great learning expe-rience, and significant changes have been made tocourse content, assignments, and the like.
Here aresome lessons learned from this experience:?
A common pitfall for course design is beingover-ambitious with the course plan.
Whatmatters the most is not how much material iscovered in class, but how much students actu-ally digest.?
When using journal/conference papers as read-ing material, it is often better to select multi-ple papers and specify the sections in the pa-pers that are most relevant.
Giving readingassignments would encourage students to readpapers before class and provide guidelines aswhat questions they should focus on.?
Adding new functionality to an existing pack-age is often difficult if the package is very com-plex and not well-documented.
Therefore, thiskind of assignments should be avoided if pos-sible.
In contrast, students often learn morefrom implementing the methods from scratchthan from reading other people?s source code.?
Implementing ML methods is an art, and point-ing out various tricks and potential obstaclesbeforehand would help students tremendously.With careful design of the assignments and in-class/outside-class discussion of implementa-51tion issues, it is possible to implement multiplelearning methods in a short period of time.?
Discussion board is a great venue for studentsto share ideas, but it will be successful only ifstudents actively participate.
The instructor canplay an important role in promoting the usageof the board.Many of the lessons above are not specific toLING572, and I have made similar changes to othercourses that I am teaching.
So far, the feedback fromthe students have been very positive.
Compared tothe first two years, in Year 3, students were muchmore active both in and outside class; they weremuch more satisfied with the assignments; many stu-dents said that they really appreciated all the imple-mentation tips and felt that they had a much betterunderstanding of the algorithms after implementingthem.
Furthermore, several students expressed inter-est in pursuing a Ph.D. degree in NLP.In the future, I plan to replace some of the earlyML algorithms (e.g., kNN, Decision Tree, TBL)with more recent ones (e.g., conditional randomfield, Bayesian approach).
This adjustment has to bedone with special care, because the early algorithms,albeit quite simple, often provide the foundation forunderstanding more sophisticated algorithms.
I willalso fine tune the assignments to make them moremanageable for students with less CS/math training.A Reading assignment exampleThe following is the reading assignment for MaxEntin Year 3.
(Q1) Let P(X=i) be the probability of getting an iwhen rolling a dice.
What is P (X) accord-ing to the maximum entropy principle under thefollowing condition?
(a) P(X=1) + P(X=2) = 0.5(b) P(X=1) + P(X=2) = 0.5 and P(X=6) = 0.2(Q2) In the text classification task, |V | is the num-ber of features, |C| is the number of classes.How many feature functions are there?
(Q3) How to calculate the empirical expectation ofa feature function?B Written assignment exampleThe following is part of a written assignment forBoosting in Year 1: In the basic AdaBoost algo-rithm, let ht be the hypothesis created at time t,Dt(i) be the weight of the i-th training instance,and t be the training error rate of ht.
Let the hy-pothesis weight ?t be 12 ln1?
?t?tand the normaliza-tion factor Zt be?iDt(i)e??tyiht(xi).
Prove thatZt = 2??t(1?
?t) for any t.C Programming assignment examplesIn Year 3, there were seven programming assign-ments, as summarized below:Hw1: Implement the two Naive Bayes models asdescribed in (McCallum and Nigam, 1998).Hw2: Implement a decision tree learner, assumingall features are binary and using informationgain as the split function.Hw3: Implement a kNN learner using cosine andEuclidean distance as similarity/dissimilaritymeasures.
Implement one of feature selectionmethods covered in class, and test the effect offeature selection on kNN.Hw4: Implement a MaxEnt learner.
For training,use General Iterative scaling (GIS).Hw5: Run the svm-train command in the libSVMpackage (Chang and Lin, 2001) to create aSVM model from the training data.
Write a de-coder that classifies test data with the model.Hw6: Implement beam search and reduplicate thePOS tagger described in (Ratnaparkhi, 1996).Hw7: Implement a TBL learner for the text clas-sification task, where a transformation has theform if a feature is present in a document,change the class label from A to B.For Hw6, students compared their POS tag-ging results with the ones reported in (Ratnaparkhi,1996).
For all the other assignments, students testedtheir learners on a text classification task and com-pare the results with the ones produced by pre-existing packages such as Mallet and libSVM.Each assignment was due in a week except forHw4 and Hw6, which were due in 1.5 weeks.
Stu-dents could choose to work alone or work with ateammate.52ReferencesEmily Bender, Fei Xia, and Erik Bansleben.
2008.Building a flexible, collaborative, intensive master?sprogram in computational linguistics.
In Proceedingsof the Third ACL Workshop on Effective Tools andMethodologies for Teaching NLP and CL, Columbus,Ohio, June.Adam L. Berger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Computational Lin-guistics, 22(1), March.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
Computational Lin-guistics, 21(4):543?565.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM:a library for support vector machines.
Available athttp://www.csie.ntu.edu.tw/ cjlin/libsvm.Dan Klein and Christopher Manning.
2003.
Maxentmodel, conditional estimation, and optimization.
ACL2003 tutorial.Dan Klein.
2007.
Introduction to Classification: Like-lihoods, Margins, Features, and Kernels.
Tutorial atNAACL-2007.Andrew McCallum and Kamal Nigam.
1998.
A compar-ison of event models for naive bayes text classification.In In AAAI/ICML-98 Workshop on Learning for TextCategorization.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Grace Ngai and Radu Florian.
2001.
Transformation-based learning in the fast lane.
In Proceedings ofNorth American ACL (NAACL-2001), pages 40?47,June.Adwait Ratnaparkhi.
1996.
A Maximum Entropy Modelfor Part-of-speech Tagging.
In Proc.
of Joint SIGDATConference on Empirical Methods in Natural Lan-guage Processing and Very Large Corpora (EMNLP-1996), Philadelphia.Adwait Ratnaparkhi.
1997.
A simple introduction tomaximum entropy models for natural language pro-cessing.
Technical Report Technical Report 97-08, In-stitute for Research in Cognitive Science, Universityof Pennsylvania.53
