Cross-lingual Slot Filling from Comparable CorporaMatthew Snover, Xiang Li, Wen-Pin Lin, Zheng Chen, Suzanne Tamang,Mingmin Ge, Adam Lee, Qi Li, Hao Li, Sam Anzaroot, Heng JiComputer Science DepartmentQueens College and Graduate CenterCity University of New YorkNew York, NY 11367, USAmsnover@qc.cuny.edu, hengji@cs.qc.cuny.eduAbstractThis paper introduces a new task ofcrosslingual slot filling which aims to dis-cover attributes for entity queries fromcrosslingual comparable corpora and thenpresent answers in a desired language.
It isa very challenging task which suffers fromboth information extraction and machinetranslation errors.
In this paper we ana-lyze the types of errors produced by fivedifferent baseline approaches, and presenta novel supervised rescoring based valida-tion approach to incorporate global evi-dence from very large bilingual compara-ble corpora.
Without using any additionallabeled data this new approach obtained38.5% relative improvement in Precisionand 86.7% relative improvement in Recallover several state-of-the-art approaches.The ultimate system outperformed mono-lingual slot filling pipelines built on muchlarger monolingual corpora.1 IntroductionThe slot filling task at NIST TAC KnowledgeBase Population (KBP) track (Ji et al, 2010)is a relatively new and popular task with thegoal of automatically building profiles of enti-ties from large amounts of unstructured data,and using these profiles to populate an existingknowledge base.
These profiles consist of nu-merous slots such as ?title?, ?parents?
for per-sons and ?top-employees?
for organizations.
Avariety of approaches have been proposed to ad-dress both tasks with considerable success; nev-ertheless, all of the KBP tasks so far have beenlimited to monolingual processing.
However, asthe shrinking fraction of the world?s Web pagesare written in English, many slot fills can onlybe discovered from comparable documents inforeign languages.
By comparable corpora wemean texts that are about similar topics, butare not in general translations of each other.These corpora are naturally available, for ex-ample, many news agencies release multi-lingualnews articles on the same day.
In this paper wepropose a new and more challenging crosslin-gual slot filling task, to find information for anyEnglish query from crosslingual comparable cor-pora, and then present its profile in English.We developed complementary baseline ap-proaches which combine two difficult problems:information extraction (IE) and machine trans-lation (MT).
In this paper we conduct detailederror analysis to understand how we can exploitcomparable corpora to construct more completeand accurate profiles.Many correct answers extracted from ourbaselines will be reported multiple times in anyexternal large collection of comparable docu-ments.
We can thus take advantage of such in-formation redundancy to rescore candidate an-swers.
To choose the best answers we consultlarge comparable corpora and corresponding IEresults.
We prefer those answers which fre-quently appear together with the query in cer-tain IE contexts, including co-occurring names,coreference links, relations and events.
For ex-ample, we prefer ?South Korea?
instead of ?NewYork Stock Exchange?
as the ?per:employee of ?answer for ?Roh Moo-hyun?
using global ev-idence from employment relation extraction.Such global knowledge from comparable corpora110Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 110?119,49th Annual Meeting of the Association for Computational Linguistics,Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguisticsprovides substantial improvement over each in-dividual baseline system and even state-of-the-art monolingual slot filling systems.
Comparedto previous methods of exploiting comparablecorpora, our approach is novel in multiple as-pects because it exploits knowledge from: (1)both local and global statistics; (2) both lan-guages; and (3) both shallow and deep analysis.2 Related WorkSudo et al (2004) found that for a crosslin-gual single-document IE task, source languageextraction and fact translation performed no-tably better than machine translation and tar-get language extraction.
We observed the sameresults.
In addition we also demonstrate thatthese two approaches are complementary andcan be used to boost each other?s results in astatistical rescoring model with global evidencefrom large comparable corpora.Hakkani-Tur et al (2007) described a filteringmechanism using two crosslingual IE systemsfor improving crosslingual document retrieval.Many previous validation methods for crosslin-gual QA, such as those organized by Cross Lan-guage Evaluation Forum (Vallin et al, 2005), fo-cused on local information which involves onlythe query and answer (e.g.
(Kwork and Deng,2006)), keyword translation (e.g.
(Mitamura etal., 2006)) and surface patterns (e.g.
(Soubbotinand Soubbotin, 2001)).
Some global valida-tion approaches considered information redun-dancy based on shallow statistics including co-occurrence, density score and mutual informa-tion (Clarke et al, 2001; Magnini et al, 2001;Lee et al, 2008), deeper knowledge from depen-dency parsing (e.g.
(Shen et al, 2006)) or logicreasoning (e.g.
(Harabagiu et al, 2005)).
How-ever, all of these approaches made limited effortsat disambiguating entities in queries and limiteduse of fact extraction in answer search and vali-dation.Several recent IE studies have stressed thebenefits of using information redundancy onestimating the correctness of the IE out-put (Downey et al, 2005; Yangarber, 2006;Patwardhan and Riloff, 2009; Ji and Grish-man, 2008).
Some recent research used com-parable corpora to re-score name translitera-tions (Sproat et al, 2006; Klementiev and Roth,2006) or mine new word translations (Fung andYee, 1998; Rapp, 1999; Shao and Ng, 2004; Taoand Zhai, 2005; Hassan et al, 2007; Udupa etal., 2009; Ji, 2009).
To the best of our knowl-edge, this is the first work on mining facts fromcomparable corpora for answer validation in anew crosslingual entity profiling task.3 Experimental Setup3.1 Task DefinitionThe goal of the KBP slot filling task is to extractfacts from a large source corpus regarding cer-tain attributes (?slots?)
of an entity, which maybe a person or organization, and use these factsto augment an existing knowledge base (KB).Along with each slot answer, the system mustprovide the ID of a document which supportsthe correctness of this answer.
KBP 2010 (Ji etal., 2010) defines 26 types of attributes for per-sons (such as the age, birthplace, spouse, chil-dren, job title, and employing organization) and16 types of attributes for organizations (suchas the top employees, the founder, the yearfounded, the headquarters location, and the sub-sidiaries).The new problem we define in this paper is anextension of this task to a crosslingual paradigm.Given a query in a target language t and a col-lection of documents in a source language s,a system must extract slot answers about thequery and present the answers in t. In this pa-per we examine a specific setting of s=Chineseand t=English.To score crosslingual slot filling, we pool allthe system responses and group equivalent an-swers into equivalence classes.
Each system re-sponse is rated as correct, wrong, inexact or re-dundant.
Given these judgments, we calculatethe precision, recall and F-measure of each sys-tem, crediting only correct answers.3.2 Data and Query SelectionWe use the comparable corpora of EnglishTDT5 (278,358 documents) and Chinese TDT5111(56,424 documents) as our source collection.For query selection, we collected all the en-tities from the entire source collection andcounted their frequencies.
We then selected 50informative entities (25 persons and 25 organiza-tions) which were located in the middle range offrequency counts.
Among the 25 person queries,half are Chinese-specific names, and half arenon-Chinese names.
The 25 organizations fol-low a representative distribution according tothe entity subtypes defined in NIST AutomaticContent Extraction (ACE) program1.3.3 Baseline Pipelines3.3.1 OverviewWe employ the following two types of base-line crosslingual slot filling pipelines to processChinese documents.
Figure 1 and Table 1 showsthe five system pipelines we have used to con-duct our experiments.Type A Translate Chinese texts into English,and apply English slot filling systems to thetranslations.Type B Translate English queries into Chinese,apply Chinese slot filling systems to Chinesetexts, and translate answers back to English.MachineTranslationEnglishTextsChineseTextsEnglish Candidate AnswersEnglishQueryEnglish Slot FillingAnswerTranslation  Pattern MatchingSupervisedClassificationChinese Slot FillingSupervisedClassificationChineseQueryQueryTranslationFigure 1: Overview of Baseline Crosslingual Slot Fill-ing Pipelines1http://www.itl.nist.gov/iad/mig/tests/ace/Pipeline Label Components Data(1) English Supervised Classification Mono-lingual (2) English Pattern MatchingEnglishTDT5(3)MT+EnglishSupervisedClassification Type A(4)MT+EnglishPattern Matching Cross-lingualTypeB(5)Query Translation+Chinese SupervisedClassification+Answer TranslationChineseTDT5Table 1: Monolingual and Crosslingual Baseline SlotFilling Pipelines3.3.2 Monolingual Slot FillingWe applied a state-of-the-art bilingual slotfilling system (Chen et al, 2010) to processbilingual comparable corpora.
This baselinesystem includes a supervised ACE IE pipelineand a bottom-up pattern matching pipeline.The IE pipeline includes relation extraction andevent extraction based on maximum entropymodels that incorporate diverse lexical, syntac-tic, semantic and ontological knowledge.
Theextracted ACE relations and events are thenmapped to KBP slot fills.
In pattern matching,we extract and rank patterns based on a dis-tant supervision approach (Mintz et al, 2009)that uses entity-attribute pairs from WikipediaInfoboxes and Freebase (Bollacker et al, 2008).We set a low threshold to include more answercandidates, and then a series of filtering stepsto refine and improve the overall pipeline re-sults.
The filtering steps include removing an-swers which have inappropriate entity types orhave inappropriate dependency paths to the en-tities.3.3.3 Document and Name TranslationWe use a statistical, phrase-based MT sys-tem (Zens and Ney, 2004) to translate Chinesedocuments into English for Type A Approaches.The best translation is computed by using aweighted log-linear combination of various sta-tistical models: an n-gram language model, aphrase translation model and a word-based lex-112icon model.
The latter two models are used insource-to-target and target-to-source directions.The model scaling factors are optimized with re-spect to the BLEU score similar to (Och, 2003).The training data includes 200 million runningwords in each language.
The total languagemodel training data consists of about 600 mil-lion running words.We applied various name mining approachesfrom comparable corpora and parallel corpora,as described in (Ji et al, 2009) to extract andtranslate names in queries and answers in TypeB approaches.
The accuracy of name translationis about 88%.
For those names not covered bythese pairs, we relied on Google Translate 2 toobtain results.4 Analysis of Baseline PipelinesIn this section we analyze the coverage (Sec-tion 4.1) and precision (Section 4.2) results ofthe baseline pipelines.
We then illustrate thepotential for global validation from comparablecorpora through a series of examples.4.1 Coverage Analysis: TowardInformation FusionTable 2 summarizes the Precision (P), Recall(R) and F-measure (F) of baseline pipelines andthe union of their individual results.Table 2: Baseline Pipeline ResultsSystem P R F(1) 0.08 0.54 0.15(2) 0.02 0.35 0.03 Mono-lingual Union of(1)+(2)0.03 0.69 0.05(3) 0.04 0.04 0.04(4) 0.03 0.25 0.05Union of(3)+(4) 0.03 0.26 0.05(5) 0.04 0.46 0.08Cross-lingualUnion of(3)+(4)+(5) 0.03 0.56 0.05ComparableCorporaUnion of(1)+(2)+(3)+(4)+(5)0.02 1 0.042http://translate.google.com/Although crosslingual pipelines used a muchsmaller corpus than monolingual pipelines, theyextracted comparable number of correct answers(66 vs. 81) with a slightly better precision.In fact, the crosslingual pipeline (5) performseven better than monolingual pipeline (2), es-pecially on the employment slots.
In particu-lar, 96.35% of the correct answers for Chinese-specific person queries (e.g.
?Tang Jiaxuan?
)were extracted from Chinese data.
Even forthose facts discovered from English data, theyare about quite general slots such as ?title?
and?employee of ?.
In contrast, Chinese data coversmore diverse biographical slots such as ?familymembers?
and ?schools attended?.Compared to the union of Type A approaches(pipelines (3)+(4)), Pipeline (5) returned manymore correct answers with higher precision.
Themain reason is that Type A approaches sufferfrom MT errors.
For example, MT mistakenlytranslated the query name ?Celine Dion?
into?Clinton?
and thus English slot filling compo-nents failed to identify any answers.
One canhypothesize that slot filling on MT output canbe improved by re-training extraction compo-nents directly from MT output.
However, ourexperiments of learning patterns from MT out-put showed negative impact, mainly becauseMT errors were too diverse to generalize.
Inother cases even though slot filling produced cor-rect results, MT still failed to translate the an-swer names correctly.
For example, English slotfilling successfully found a potential answer for?org:founded by?
of the query ?Microsoft?
fromthe following MT output: ?The third largest ofthe Microsoft common founder Alan Doss , aged50, and net assets of US 22 billion.?
; however,the answer string ?Paul Allen?
was mistakenlytranslated into ?Alan Doss?.
MT is not so cru-cial for ?per:title?
slot because it does not requiretranslation of contexts.To summarize, 59% of the missing errors weredue to text, query or answer translation errorsand 20% were due to slot filling errors.
Never-theless, the union of (3)+(4)+(5) still containmore correct answers.
These baseline pipelineswere developed from a diverse set of algorithms,and typically showed strengths in specific slots.113In general we can conclude that monolin-gual and crosslingual pipelines are complemen-tary.
Combining the responses from all baselinepipelines, we can get similar number of correctanswers compared to one single human annota-tor.4.2 Precision Analysis: Toward GlobalValidationThe spurious errors from baseline crosslingualslot filling pipelines reveal both the shortcom-ings of the MT system and extraction acrosslanguages.
Table 3 shows the distribution ofspurious errors.Pipeline Spurious Errors DistributionContent Translation+ Extraction85%Query Translation 13%Type AAnswer Translation 2%Word Segmentation 34%Relation Extraction 33%Coreference 17%Semantic Type 13%Type BSlot Type 3%Table 3: Distribution of Spurious ErrorsTable 3 indicates a majority (85%) of spuriouserrors from Type A pipelines were due to ap-plying monolingual slot filling methods to MToutput which preserves Chinese structure.As demonstrated in previous work (e.g.
(Par-ton and McKeown, 2010; Ji et al, 2009)),we also found that many (14.6%) errors werecaused by the low quality of name translationfor queries and answers.For example, ?????/McGinty?
was mis-takenly translated into the query name ?KimJong-il?, which led to many incorrect answerssuch as ?The British Royal joint military re-search institute?
for ?per:employee of ?.In contrast, the spurious errors from Type Bpipelines were more diverse.
Chinese IE com-ponents severely suffered from word segmen-tation errors (34%), which were then directlypropagated into Chinese document retrieval andslot filling.
Many segmentation errors occurredwith out-of-vocabulary names, especially per-son names and nested organization names.
Forexample, the name ???
?/Yao Mingbao?
wasmistakenly segmented into two words ???/YaoMing?
and ?
?/bao?, and thus the document wasmistakenly retrieved for the query ?Yao Ming?.In many cases (33%) Chinese relation andevent extraction components failed to cap-ture Chinese-specific structures due to the lim-ited size of training corpora.
For example,from the context ???????????????
?/Xiao Wan-chang, who were invited to be-come the economics consultant for Chen Shui-bian?, Chinese slot filling system mistakenly ex-tracted ?consultant?
as a ?per:title?
answer forthe query ?Chen Shui-bian?
using a commonpattern ?<query><title>?.13% of errors were caused due to invalid se-mantic types for certain slots.
For example,many metaphoric titles such as ?tough guy?don?t match the definition of ?per:title?
in theannotation guideline ?employment or member-ship position?.5 Global ValidationBased on the above motivations we propose toincorporate global evidence from a very largecollection of comparable documents to refinelocal decisions.
The central idea is to over-generate candidate answers from multiple weakbaselines to ensure high upper-bound of recall,and then conduct effective global validation tofilter spurious errors while keeping good answersin order to enhance precision.5.1 Supervised RescoringIdeally, we want to choose a validation modelwhich can pick out important features in a con-text wider than that used by baseline pipelines.Merging individual systems to form the union ofanswers can be effective, but Table 2 shows thatsimple union of all pipelines produced worse F-measure than the best pipeline.In this paper we exploit the rerankingparadigm, commonly used in information re-trieval, to conduct global validation.
By model-ing the empirical distribution of labeled trainingdata, statistical models are used to identify the114strengths and weaknesses (e.g.
high and low pre-cision slots) of individual systems, and rescoreanswers accordingly.
Specially, we develop asupervised Maximum Entropy (MaxEnt) basedmodel to rescore the answers from the pipelines,selecting only the highest-scoring answers.The rescorer was trained (using cross-validation) on varying subsets of the features.The threshold at which an answer is deemed tobe true is chosen to maximize the F-Measure onthe training set.5.2 Validation FeaturesTable 4 describes the validation features used forrescoring, where q is the query, q?
the Chinesetranslation of q, t the slot type, a the candidateanswer, a?
the Chinese form of a, s the contextsentence and d is the context document support-ing a.The feature set benefits from multiple dimen-sions of crosslingual slot filling.
These featureswere applied to both languages wherever anno-tation resources were available.In the KBP slot filling task, slots are of-ten dependent on each other, so we can im-prove the results by improving the ?coherence?of the story (i.e.
consistency among all gener-ated answers - query profiles).
We use featuref2 to check whether the same answer was gen-erated for conflicting slots, such as per:parentsand per:children.Compared to traditional QA tasks, slot fill-ing is a more fine-grained task in which differ-ent slots are expected to obtain semanticallydifferent answers.
Therefore, we explored se-mantic constraints in both local and global con-texts.
For example, we utilized bilingual namegazetteers from ACE training corpora, Googlen-grams (Ji and Lin, 2009) and the geonameswebsite 3 to encode features f6, f8 and f9; Theorg:top members/employees slot requires a sys-tem to distinguish whether a person member/employee is in the top position, thus we encodedf10 for this purpose.The knowledge used in our baseline pipelinesis relatively static ?
it is not updated during the3http://www.geonames.org/statistics/extraction process.
Achieving high performancefor cross-lingual slot filling requires that we takea broader view, one that looks outside a sin-gle document or a single language in order toexploit global knowledge.
Fortunately, as moreand more large crosslingual comparable corporaare available, we can take advantage of informa-tion redundancy to validate answers.
The basicintuition is that if a candidate answer a is cor-rect, it should appear together with the queryq repeatedly, in different documents, or even incertain coreference links, relations and events.For example, ?David Kelly - scientist?, and?????
?/Shintaro Ishihara - ?
?/governor?pairs appear frequently in ?title?
coreferencelinks in both English and Chinese corpora;?Elizabeth II?
is very often involved in an ?em-ployment?
relation with ?United Kingdom?
inEnglish corpora.
On the other hand, some in-correct answers with high global statistics can befiltered out using these constraints.
For exam-ple, although the query ???
?/Tang Jiaxuan?appears frequently together with the candidateper:title answer ??
?/personnel?, it is linked byfew coreference links; in contrast, it?s coreferen-tial with the correct title answer ????
?/StateCouncil member?
much more frequently.We processed cross-lingual comparable cor-pora to extract coreference links, relations andevents among mentions (names, nominals andtime expressions etc.)
and stored them in anexternal knowledge base.
Any pair of <q, a>is then compared to the entries in this knowl-edge base.
We used 157,708 documents fromChinese TDT5 and Gigaword to count Chineseglobal statistics, and 7,148,446 documents fromDARPA GALE MT training corpora to countEnglish global statistics, as shown in featuresf12 and f13.
Fact based global features f14, f15,f16 and f17, were calculated from 49,359 Chi-nese and 280,513 English documents (annotatedby the bilingual IE system in Section 3.3.2.6 ExperimentsIn this section, we examine the overall perfor-mance of this method.
We then discuss theusefulness of the individual sets of features.
In115CharacteristicsScope Depth LanguageDescriptionf1: frequency of <q, a, t> that appears in all baseline outputs Global(Cross-system)ShallowEnglish f2: number of conflicting slot types in which answer a appears in all baselineoutputsf3: conjunction of t and whether a is a year answer Shallow Englishf4: conjunction of t and whether a includes numbers or lettersf5: conjunction of place t and whether a is a country namef6: conjunction of per:origin t and whether a is a nationalityf7: if t=per:title, whether a is an acceptable titlef8: if t requires a name answer, whether a is a nameLocalDeepEnglishf9: whether a has appropriate semantic typef10: conjunction of org:top_members/employees and whether there is a high-leveltitle in sGlobal(Within-Document)Deep Englishf11: conjunction of alternative name and whether a is an acronym of qChinese f12: conditional probability of q/q' and a/a' appear in the same document Shallow(Statistics) English f13: conditional probability  of q/q' and a/a' appear in the same sentenceBoth f14:  co-occurrence of q/q' and a/a'  appear in coreference linksEnglish f15: co-occurrence of q/q' and a/a'  appear in relation/event linksEnglish f16: conditional probability of q/q' and a/a' appear in relation/event linksGlobal(Cross-documentincomparablecorpora)Deep(Fact-based)English f17: mutual information of q/q' and a/a' appear in relation/event linksTable 4: Validation Features for Crosslingual Slot Fillingthe following results, the baseline features arealways used in addition to any other features.6.1 Overall PerformanceBecause of the data scarcity, ten-fold cross-validation, across queries, was used to trainand test the system.
Quantitative results aftercombining answers from multiple pipelines areshown in Table 5.
We used two basic features,one is the slot type and the other is the entitytype of the query (i.e.
person or organization).This basic feature set is already successful in im-proving the precision of the pipelines, althoughthis results in a number of correct answers be-ing discarded as well.
By adding the additionalvalidation features described previously, boththe f-score and precision of the models are im-proved.
In the case of the cross-lingual pipelines(3+4+5) the number of correct answers chosenis almost doubled while increasing the precisionof the output.6.2 Impact of Global ValidationA comparison of the benefits of global versus lo-cal features are shown in Table 6, both of whichdramatically improve scores over the baselinefeatures.
The global features are universallyPipelines F P RBasic Features1+2 0.31 0.31 0.303+4+5 0.26 0.39 0.201+2+3+4+5 0.27 0.29 0.25Full Features1+2 0.37 0.30 0.463+4+5 0.36 0.35 0.371+2+3+4+5 0.31 0.28 0.35Table 5: Using Basic Features to Filter Answersmore beneficial than the local features, althoughthe local features generate results with higherprecision at the expense of the number of correctanswers returned.
The global features are espe-cially useful for pipelines 3+4+5, where the per-formance using just these features reaches thoseof using all other features ?
this does not holdtrue for the monolingual pipelines however.6.3 Impact of Fact-driven DeepKnowledgeThe varying benefit of fact-driven cross-document features and statistical cross-document features are shown in Table 7.116Pipelines F P RLocal Features1+2 0.34 0.35 0.333+4+5 0.29 0.40 0.221+2+3+4+5 0.27 0.32 0.24Global Features1+2 0.35 0.30 0.423+4+5 0.37 0.36 0.381+2+3+4+5 0.33 0.29 0.38Table 6: The Benefit of Global versus Local FeaturesWhile both feature sets are beneficial, themonolingual pipelines (1+2) benefit morefrom statistical features while the cross-lingualpipelines (3+4+7) benefit slightly more fromthe fact-based features.
Despite this bias, theoverall results when the features are used inall pipelines are very close with the fact-basedfeatures being slightly more useful overall.Pipelines F P RFact-Based Features1+2 0.33 0.27 0.423+4+5 0.35 0.43 0.291+2+3+4+5 0.30 0.27 0.34Statistical Features1+2 0.37 0.34 0.403+4+5 0.34 0.35 0.331+2+3+4+5 0.29 0.25 0.34Table 7: Fact vs. Statistical Cross-Doc FeaturesTranslation features were only beneficial topipelines 3, 4, and 5, and provided a slight in-crease in precision from 0.39 to 0.42, but pro-vided no noticeable benefit when used in con-junction with results from pipelines 1 and 2.This is because the answers where translationfeatures would be most useful were already be-ing selected by pipelines 1 and 2 using the base-line features.6.4 DiscussionThe use of any re-scoring, even with baselinefeatures, provides large gains over the union ofthe baseline pipelines, removing large numberof incorrect answers.
The use of more sophis-ticated features provided substantial gains overthe baseline features.
In particular, global fea-tures proved very effective.
Further feature en-gineering to address the remaining errors andthe dropped correct answer would likely provideincreasing gains in performance.In addition, two human annotators, indepen-dently, conducted the same task on the samedata, with a second pass of adjudication.
The F-scores of inter-annotator agreement were 52.0%for the first pass and 73.2% for the second pass.This indicates that slot filling remains a chal-lenging task for both systems and human anno-tators?only one monolingual system exceeded30% F-score in the KBP2010 evaluation.7 Conclusion and Future WorkCrosslingual slot filling is a challenging taskdue to limited performance in two separate ar-eas: information extraction and machine trans-lation.
Various methods of combining tech-niques from these two areas provided weak yetcomplementary baseline pipelines.
We proposedan effective approach to integrate these base-lines and enhance their performance using widerand deeper knowledge from comparable cor-pora.
The final system based on cross-lingualcomparable corpora outperformed monolingualpipelines on much larger monolingual corpora.The intuition behind our approach is thatover-generation of candidate answers from weakbaselines provides a potentially strong recallupper-bound.
The remaining enhancement be-comes simpler: filtering errors.
Our experimentsalso suggest that our rescoring models tend toover-fit due to small amount of training data.Manual annotation and assessment are quitecostly, motivating future work in active learningand semi-supervised learning methods.
In addi-tion, we plan to apply our results as feedback toimprove MT performance on facts using queryand answer-driven language model adaptation.We have demonstrated our approach on English-Chinese pair, but the framework is language-independent; ultimately we would like to extendthe task to extracting information from morelanguages.117AcknowledgmentsThis work was supported by the U.S. NSF CAREERAward under Grant IIS-0953149 and PSC-CUNYResearch Program.
Any opinions, findings, and con-clusions or recommendations expressed in this mate-rial are those of the author(s) and do not necessarilyreflect the views of the National Science Foundation.ReferencesK.
Bollacker, R. Cook, and P. Tufts.
2008.
Free-base: A shared database of structured general hu-man knowledge.
In Proc.
National Conference onArtificial Intelligence.Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li,Marissa Passantino, and Heng Ji.
2010.
Top-down and bottom-up: A combined approach toslot filling.
Lecture Notes in Computer Science,6458:300?309, December.C.
L. A. Clarke, G. V. Cormack, and T.R.
Lynam.2001.
Exploiting redundancy in question answer-ing.
In Proc.
SIGIR2001.Doug Downey, Oren Etzioni, and Stephen Soderland.2005.
A Probabilistic Model of Redundancy inInformation Extraction.
In Proc.
IJCAI 2005.Pascale Fung and Lo Yuen Yee.
1998.
An ir ap-proach for translating new words from nonparalleland comparable texts.
In COLING-ACL.Dilek Hakkani-Tur, Heng Ji, and Ralph Grishman.2007.
Using information extraction to improvecross-lingual document retrieval.
In Proc.
RANLPworkshop on Multi-source, Multilingual Informa-tion Extraction and Summarization.S.
Harabagiu, D. Moldovan, C. Clark, M. Bowden,A.
Hickl, and P. Wang.
2005.
Employing twoquestion answering systems in trec 2005.
In Proc.TREC2005.Ahmed Hassan, Haytham Fahmy, and Hany Has-san.
2007.
Improving named entity translationby exploiting comparable and parallel corpora.
InRANLP.Heng Ji and Ralph Grishman.
2008.
Refining EventExtraction through Cross-Document Inference.
InProc.
of ACL-08: HLT, pages 254?262.Heng Ji and Dekang Lin.
2009.
Gender and animacyknowledge discovery from web-scale n-grams forunsupervised person mention detection.
In Proc.PACLIC2009.Heng Ji, Ralph Grishman, Dayne Freitag, MatthiasBlume, John Wang, Shahram Khadivi, RichardZens, and Hermann Ney.
2009.
Name translationfor distillation.
Handbook of Natural LanguageProcessing and Machine Translation: DARPAGlobal Autonomous Language Exploitation.Heng Ji, Ralph Grishman, Hoa Trang Dang, andKira Griffitt.
2010.
An overview of the tac2010knowledge base population track.
In Proc.TAC2010.Heng Ji.
2009.
Mining name translations from com-parable corpora by creating bilingual informationnetworks.
In ACL-IJCNLP 2009 workshop onBuilding and Using Comparable Corpora (BUCC2009): from Parallel to Non-parallel Corpora.Alexandre Klementiev and Dan Roth.
2006.
Namedentity transliteration and discovery from multilin-gual comparable corpora.
In HLT-NAACL 2006.K.-L. Kwork and P. P. Deng.
2006.
Chinesequestion-answering: Comparing monolingual withenglish-chinese cross-lingual results.
In Asia In-formation Retrieval Symposium.Cheng-Wei Lee, Yi-Hsun Lee, and Wen-Lian Hsu.2008.
Exploring shallow answer ranking featuresin cross-lingual and monolingual factoid questionanswering.
Computational Linguistics and Chi-nese Language Processing, 13:1?26, March.B.
Magnini, M. Negri, R. Prevete, and H. Tanev.2001.
Is it the right answer?
: Exploiting webredundancy for answer validation.
In Proc.ACL2001.Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In ACL-IJCNLP2009.Teruko Mitamura, Mengqiu Wang, Hideki Shima,and Frank Lin.
2006.
Keyword translation accu-racy and cross-lingual question answering in chi-nese and japanese.
In EACL 2006 Workshop onMLQA.F.
J. Och.
2003.
Minimum error rate training instatistical machine translaton.
In Proc.ACL2003.Kristen Parton and Kathleen McKeown.
2010.
Mterror detection for cross-lingual question answer-ing.
Proc.
COLING2010.Siddharth Patwardhan and Ellen Riloff.
2009.
AUnified Model of Phrasal and Sentential Evidencefor Information Extraction.
In Proc.
EMNLP2009.Reinhard Rapp.
1999.
Automatic identification ofword translations from unrelated english and ger-man corpora.
In ACL 1999.Li Shao and Hwee Tou Ng.
2004.
Mining new wordtranslations from comparable corpora.
In COL-ING2004.D.
Shen, G. Saarbruechen, and D. Klakow.
2006.Exploring correlation of dependency relationpaths for answer extraction.
In Proc.
ACL2006.118M.
M. Soubbotin and S. M. Soubbotin.
2001.
Pat-terns of potential answer expressions as clues tothe right answers.
In Proc.
TREC2001.Richard Sproat, Tao Tao, and ChengXiang Zhai.2006.
Named entity transliteration with compa-rable corpora.
In ACL 2006.Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.2004.
Cross-lingual information extraction evalu-ation.
In Proc.
COLING2004.Tao Tao and Chengxiang Zhai.
2005.
Mining com-parable bilingual text corpora for cross-languageinformation integration.
In Proc.
KDD2005.Raghavendra Udupa, K. Saravanan, A. Kumaran,and Jagadeesh Jagarlamudi.
2009.
Mint: Amethod for effective and scalable mining of namedentity transliterations from large comparable cor-pora.
In EACL2009.Alessandro Vallin, Bernardo Magnini, Danilo Gi-ampiccolo, Lili Aunimo, Christelle Ayache, PetyaOsenova, Anselmo Peas, Maaren de Rijke, BogdanSacaleanu, Diana Santos, and Richard Sutcliffe.2005.
Overview of the clef 2005 multilingual ques-tion answer track.
In Proc.
CLEF2005.Roman Yangarber.
2006.
Verification of Facts acrossDocument Boundaries.
In Proc.
InternationalWorkshop on Intelligent Information Access.Richard Zens and Hermann Ney.
2004.
Improve-ments in phrase-based statistical machine transla-tion.
In Proc.
HLT/NAACL 2004.119
