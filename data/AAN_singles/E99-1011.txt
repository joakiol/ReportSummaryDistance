Proceedings of EACL '99The T IPSTER SUMMAC Text  Summar izat ion  Eva luat ionInderjeet ManiDavid HouseGary KleinLynet te  H i rschman*The  MITRE Corporat ion11493 Sunset  Hil ls Rd.Reston,  VA 22090USATherese  F i rminDepar tment  of Defense9800 Savage Rd.Ft.
Meade,  MD 20755USABeth SundheimSPAWAR Systems CenterCode D4420853140 Gatchell Rd.San Diego, CA 92152USAAbstractThe TIPSTER Text SummarizationEvaluation (SUMMAC) has establisheddefinitively that automatic text summa-rization is very effective in relevance as-sessment tasks.
Summaries as short as17% of full text length sped up decision-making by almost a factor of 2 with nostatistically significant degradation i F-score accuracy.
SUMMAC has also in-troduced a new intrinsic method for au-tomated evaluation of informative sum-maries.1 In t roduct ionIn May 1998, the U.S. government completedthe TIPSTER Text Summarization Evaluation(SUMMAC), which was the first large-scale,developer-independent valuation of automatictext summarization systems.
The goals of theSUMMAC evaluation were to judge individualsummarization systems in terms of their useful-ness in specific summarization tasks and to gaina better understanding of the issues involved inbuilding and evaluating such systems.1.1 Text  SummarizationText summarization is the process of distilling themost important information from a set of sourcesto produce an abridged version for particular usersand tasks (Maybury 1995).
Since abridgment iscrucial, an important parameter to summariza-tion is the level of compression (ratio of summarylength to source length) desired.
Summaries canbe used to indicate what topics are addressed inthe source text, and thus can be used to alert theuser as to source content (the indicative function).In addition, summaries can also be used to standin place of the source (the informative function).202 Burlington Rd.,' Bedford, MA 01730They can even offer a critique of the source (theevaluative function) (Sparck-Jones 1998).
Often,summaries are tailored to a reader's interests andexpertise, yielding topic-relatedsummaries, or elsethey can be aimed at a broad readership com-munity, as in the case of generic summaries.
Itis also useful to distinguish between summarieswhich are extracts of source material, and thosewhich are abstracts containing new text generatedby the summarizer.1.2 Summarization Evaluation MethodsMethods for evaluating text summarization canbe broadly classified into two categories.The first, an intrinsic (or normative) evalua-tion, judges the quality of the summary directlybased on analysis in terms of some set of norms.This can involve user judgments of fluency of thesummary (Minel et al 1997), (Brandow et al1994), coverage of stipulated "key/essential ideas"in the source (Paice 1990), (Brandow et al 1994),or similarity to an "ideal" summary, e.g., (Ed-mundson 1969), (Kupiec et al 1995).The problem with matching a system summaryagainst an ideal summary is that the ideal sum-mary is hard to establish.
There can be a largenumber of generic and topic-related abstracts thatcould summarize a given document.
Also, therehave been several reports of low inter-annotatoragreement on sentence xtracts, e.g., (Rath et al1961), (Salton et al 1997), although judges mayagree more on the most important sentences toinclude (Jing et al 1998).The second category, an extrinsic evaluation,judges the quality of the summarization based onhow it affects the completion of some other task.There have been a number of extrinsic evalua-tions, including question-answering and compre-hension tasks, e.g., (Morris et al 1992), as weltas tasks which measure the impact of summariza-tion on determining the relevance of a documentto a topic (Mani and Bloedorn 1997), (Jing et al77Proceedings of EACL '991998), (Tombros et al 1998), (Brandow et al1994).1.3 Par t i c ipant  TechnologiesSixteen systems participated in the SUMMACEvaluation: Carnegie Group Inc. and Carnegie-Mellon University (CGI/CMU), Cornell Univer-sity and SablR Research, Inc. (Cornell/SabIR),GE Research and Development (GE), NewMexico State University (NMSU), the Univer-sity of Pennsylvania (Penn), the University ofSouthern California-Information Sciences Insti-tute (ISI), Lexis-Nexis (LN), the University ofSurrey (Surrey), IBM Thomas J. Watson Re-search (IBM), TextWise LLC, SRA International,British Telecommunications (BT), Intelligent Al-gorithms (IA), the Center for Intelligent Infor-mation Retrieval at the University of Massachus-setts (UMass), the Russian Center for InformationResearch (CIR), and the National Taiwan Uni-versity (NTU).
Table 1 offers a high-level sum-mary of the features used by the different par-ticipants.
Most participants confined their sum-maries to extracts of passages from the sourcetext; TextWise, however, extracted combinationsof passages, phrases, named entities, and subjectfields.
Two participants modified the extractedtext: Penn replaced pronouns with coreferentialnoun phrases, and Penn and NMSU both short-ened sentences by dropping constituents.2 SUMMAC Summar izat ion  TasksIn order to address the goals of the evaluation,two main extrinsic evaluation tasks were defined,based on activities typically carried out by infor-mation analysts in the U.S. Government.
In theadhoc task, the focus was on indicative summarieswhich were tailored to a particular topic.
Thistask relates to the real-world activity of an analystconducting full-text searches using an IR systemto quickly determine the relevance of a retrieveddocument.
Given a document (which could be asummary or a full-text source - the subject wasnot told which), and a topic description, the hu-man subject was asked to determine whether thedocument was relevant to the topic.
The accuracyof the subject's relevance assessment decision wasmeasured in terms of "ground-truth" judgmentsof the full-text source relevance, which were sepa-rately obtained from the Text Retrieval (TREC)(Harman and Voorhees 1996) conferences.
Thus,an indicative summary would be "accurate" if itaccurately reflected the relevance or irrelevance ofthe corresponding source.In the categorization task, the evaluation soughtto find out whether a generic summary could ef-fectively present enough information to allow ananalyst o quickly and correctly categorize a doc-ument.
Here the topic was not known to thesummarization system.
Given a document, whichcould be a generic summary or a full-text source(the subject was not told which), the human sub-ject would choose a single category out of five cat-egories (each of which had an associated topic de-scription) to which the document was relevant, orelse choose "none of the above".The final task, a question-answering task, wasintended to support an information analyst writ-ing a report.
This involved an intrinsic evaluationwhere a topic-related summary for a documentwas evaluated in terms of its "informativeness",namely, the degree to which it contained answersfound in the source document o a set of topic-related questions.3 Data  Se lec t ionIn the adhoc task, 20 topics were selected.
Foreach topic, a 50-document subset was created fromthe top 200 ranked documents retrieved by a stan-dard IR system.
For the categorization task, only10 topics were selected, with 100 documents usedper topic.
For both tasks, the subsets were con-structed such that 25%-75% of the documentswere relevant to the topic, with full-text docu-ments being 2000-20,000 bytes (300-2700 words)long, so that they were long enough to be worthsummarizing but short enough to be read withinthe time-frame of the experiment.The documents were all newspaper sources, thevast majority of which were news stories, butwhich also included sundry material such as lettersto the editor.
Reliance on TREC data for docu-ments and topics, and internal criteria for length,relevance, and non-overlap among test sets, re-sulted in the evaluation focusing mostly on shortnewswire texts.
We recognize that larger-sizedtexts from a wider range of genres might challengethe summarizers to a greater extent.In each task, participants ubmitted two sum-maries: a fixed-length (S1) summary limited to10% of the length of the source, and a summarywhich was not limited in length ($2).4 Exper imenta l  Hypotheses  andMethodIn meeting the evaluation goals, the main questionto be answered was whether summarization savedtime in relevance assessment, without impairingaccuracy.78Proceedings of EACL '99Part ic ipant  t f  loc disc corefBT + + +CGI/CMU + +CIR + +Cornell/SabIR +GE + + + +IA +IBM + +ISI + +LN +NMSU + + +NTU + + +Penn - + +SRA + + +Surrey + + -TextWise + +UMass +co-occ syn++++++- ++- ++ ++ ++Table 1: Participant Summarization Features.
tf: term frequency; loc: location; disc:discourse ( .g., useof discourse model); coref: coreference; co-occ: co-occurrence; syn: synonyms.Ground TruthRe levant  is TrueIrrelevant is TrueRelevantTPFPI rrelevantFNTable 2: Adhoc Task Contingency Table.TP=true positive, FP = false positive, TN= truenegative, FN=false negative.Ground Truth Subject 's  JudgmentX Y NoneXisTrue  TP FN FNNone is True FP FP TNTable 3: Categorization Task Contingency Table.X and Y are distinct categories other than None-of-the- above, represented as None.The first test was a summarizat ion conditiontest: to determine whether subjects' relevance as-sessment performance in terms of time and accu-racy was affected by different conditions: full-text(F), fixed-length summaries (S1), variable-lengthsummaries ($2), and baseline summaries (B).
Thelatter were comprised of the first 10% of the bodyof the source text.The second test was a participant technologytest: to compare the performance of different par-ticipants' systems.The third test was a consistency test: to deter-mine how much agreement there was between sub-jects' relevance decisions based on showing themonly full-text versions of the documents from themain adhoc and categorization tasks.
In the ad-hoc and categorization tasks, the 1000 documentsassigned to a subject for each task were allocatedamong F, B, S1, and $2 conditions through ran-dom selection without replacement (20 F, 20 B,480 S1, and 480 $21).
For the consistency tasks,each subject was assigned full-text versions of thesame 1000 documents.
In all tasks, the presenta-tion order was varied among subjects.
The evalu-ation used 51 professional information analysts assubjects, each of whom took approximately 16-20 hours.
The main adhoc task used 21 sub-jects, the main categorization 24 subjects; theconsistency adhoc task had 14 subjects, the con-sistency categorization 7 subjects (some subjectsfrom the main task also did a different consistencytask).
The subjects were told they were work-ing with documents that included summaries, andthat their goal, on being presented with a topic-document pair, was to examine ach document todetermine if it was relevant o the topic.
The con-tingency tables for the adhoc and categorizationtasks are shown in Tables 2 and 3.We used the following aggregate accuracy met-rics:Precision = TP / (TP  + FP)  (1)Recall = TP / (TP  + FN)  (2)Fscore = 2 ?
Precision ?
Recall/( Precision + Recall)(3)5 Results: Adhoc andCategorization Tasks5.1 Per fo rmance  by  Cond i t ionIn the adhoc task, summaries at compressions aslow as 17% of full text length were not significantly~This distribution assures ufficient statistical sen-sitivity for expected effect sizes for both the sum-marization condition and the participant echnologytests.79Proceedings of EACL '99Condi t ion  Time Time SD F-score TP  FP FN TNF 58.89 56.86 .67 .38 .08 .26 .28$2 33.12 36.19 .64 .35 .08 .28 .28$1 19.75 26.96 .53 .27 .07 .35 .31B 23.15 21.82 .42 .18 .05 .41 .35P R.83 .22.80 .23.79 .19.81 .12Table 4: Adhoc Time and Accuracy by Condition.
TP, FP, FN, TN are expressed as percentage oftotals observed in all four categories.
All time differences are significant except between B and S1(HSD=9.8).
All F-score differences are significant, except between F (Full-Text) and $2 (HSD=.10).Precision (P) differences aren't significant.
All Recall (R) differences between conditions are significant,except between F and $2 (HSD=.12).
"SD" = standard eviation.Condit ion Time"F 43.11"$2 43.15S1 25.48B 27.36Time SD F-score52.84 .5042.16 .5029.81 .4330.35 .03TP  FP  FN TN P R24.3 13.3 28.5 33.9 .63 .4519.3 10.5 36.9 33.3 .68 .4227.1 10.7 30.9 31.3 .68 .347.5 11.9 52.5 28.1 .04 .02Table 5: Categorization Time and Accuracy by Condition.
Here TP, FP, FN, TN are expressed aspercentage of totals in all four categories.
All time differences are significant except between F and$2, and between B and S1 (HSD=15.6).Only the F-score of B is significantly less than the others(HSD=.09).
Precision (P) and Recall (R) of B is significantly less than the others: HSD(Precision)--.11;HSD(Recall)-.11.different in accuracy from full text (Table 4), whilespeeding up decision-making by almost a factor of2 (33.12 seconds per decision average time for $2compared to 58.89 for F in 4).
Tukey's HonestlySignificant Difference test (HSD) is used to com-pare multiple differences 2 .In the categorization task, the F-score on full-text was only .5, suggesting the task was veryhard.
Here summaries at 10% of the full-textlength were not significantly different in accuracyfrom full-text (Table 5) while reducing decisiontime by 40% compared to full text (25.48 secondsfor $1 compared to 43.11 for F in 5).
The verylow F-scores for the Bs can be explained by abug which resulted in the same 20 relatively less-effective B summaries being offered to each sub-ject.
However, in this task, summaries longer than10% of the full text, while not significantly differ-ent in accuracy from full-text, did not take lesstime than full-text.
In both tasks, the main ac-curacy losses in summarization came from FNs,not FPs, indicating the summaries were missingtopic-relevant information from the source,5.2 Per fo rmance  by Par t i c ipantIn the adhoc task, the systems were all very closein accuracy for both summary types (Table 6).Three groups of systems were evident in the ad-hoc $2 F-score accuracy data, as shown in Table 8.Interestingly, the Group I systems both used only2The significance level a < .05 throughout this pa-per, unless noted otherwise.GroupGroup IGroup IIMembersCGI/CMU, Comell/SablRGE, LN, NMSU, NTU,Penn, SRA, TextWise, UMassGroup III ISI "Table 8: Adhoc Accuracy: Participant Groups tbr$2 summaries.
Groups I and III are significantlydifferent in F-score (albeit with a small effect size).Accuracy differences within groups and betweenGroup II and the others are not significant.Adhoc: F--Score vs. 3qrne by Party f~r Best--Lermj~ Sun~,=des0.740.70 i0.660.620.580.540.500.48I5GE +peru= ?
LN?U Mass= I$1-- NMSU--NTUSRA.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
i ' "  .
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
J .
.
.
.
.
.
.
.
.
f .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
*20 24 28 \]2 ~ 40 4AA*JST IREFigure 1: Adhoc F-score versus Time by Partic-ipant (variable-length summaries).
HSD(F-score)is 0.13.
HSD(Time) = 12.88.
Decisions basedon summaries from GE, Penn, and TextWise aresignificantly faster than based on SRA and Cor-nell/SabIR.term frequency and co-occurrence (Table 1), in80Proceedings of EACL '99.\]m-~mPCGI/CMU .82CorneU/SabIR .78GE .78LN .78Penn .81UMass .80NMSU .8OTextWise .81SRA .82NTU .8OISI .8O$2R F-score.66 .72.67 .70.60 .67.58 .65.57 .65.54 .63.54 .63.51 .61.49 .60.49 .59.46 .56SlP R F-score.76 .52 .60.79 .47 .56.77 .45 .55.81 .45 .55.76 .45 .53.81 .47 .56.8O .4O .52.79 .41 .52.79 .37 .48.82 .34 .46.82 .36 .47Table 6: Adhoc Accuracy by Participant.
For variable-length: Precision (P) differences aren't signifi-cant; CGI/CMU and Cornell/SabIR are significantly different from SRA, NTU, and ISI in Recall (R)(HSD=0.17) and from ISI in F-score (HSD=0.13).
For fixed-length, no significant differences on any ofthe measures.PCIR .71IBM .68NMSU .69Surrey .69Penn .70ISI .71IA .69BT .63NTU .66SRA .65LN .68Cornell/SablR .66GE .69CGI/CMU .74S2R F-score P.47 .54 .68.47 .51 .63.46 .51 .69.43 .51 .69 .31.42 .50 .66 .29.42 .49 .71 .35.42 .49 .67 .33.43 .48 .70 .33.41 .48 .68 .33.42 .48 .73 .37.41 .47 .68 .37.40 .47 .62 .36.40 .47 .69 .33.39 .47 .69 .33S1I~.
F-score.35 .43.37 .44.34 .43.39.38.44.41.41.43.45.45.42.42.42Table 7: Categorization Accuracy by Participant.
No significant differences on any of the measures.Adhoc: F--Score w. "r'rne by Party for Ftxed--Length Summaries0.740.700 .860 .~ +CGIICMU0.~ U I,/~..~IN+++ ?
ComeJ I SablR0 ,54  Tex~k?
GE .Peru- -  NMSU0.50 '  ISI _SPATU_  -0"46u .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
.
= .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
,16 2O 24 29 3~ ~ 4O 44R~TZHEFigure 2: Adhoc F-score versus Time by Partici-pant (fixed-length summaries).
No significant dif-ferences in F-score, or in Time.particular, exploiting similarity computations be-tween text passages.
For the $2 summaries (Fig-ure 1), the Group I systems (average compression25% for CGI/CMU and 30% for Cornell/SabIR)were not the fastest in terms of human decisiontime; in terms of both accuracy and time, Text-Wise, GE and Penn (equivalent in accuracy) werethe closest in terms of Cartesian distance from theideal performance.
For S1 summaries (Figure 2),the accuracy and time differences aren't signifi-cant.
Finally, clustering the systems based on de-gree of overlap between the sets of sentences theyextracted for summaries judged TP resulted inCGI/CMU, GE, LN, UMass, and Cornell/SabIRclustering together on both S1 and $2 summaries.It is striking that this cluster, shown with the '%"icon in Figures 1 and 2, corresponds to the sys-tems with the highest F-scores, all of whom, withthe exception of GE, used similar features in anal-ysis (Table 1).In the categorization task, by contrast, the 14participating systems 3 had no significant differ-ences in F-score accuracy whatsoever (Table 7,3Note that some participants participated in onlyone of the two tasks.81Proceedings of EACL '99Categ: F--Scorn vs. Time by Party for Best--Length Surrv~ariesR ~'F.,F @0. f~;:; ?
CIR0.53 ii ?
Peru IBM I ?NMSULA O i l l s  I ?
'~i 6E ?~  eT" s~oJ .7  ~ ?
C6~ I ILN ?
C, omel / S~IR0.440.4~.0.
'38'i .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
p .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
J .
.
.
.
.
.
.
.
.
J21 ~ 29 \ ]3  ~ 41 45 4~ 53 25 :3  9 57~T INEFigure 3: Categorization P-score versus Timeby Participant (variable-length summaries).
F-scores are not significantly different.
HSD(Time)= 17.23.
GE is significantly faster than SRA andSurrey.
The latter two are also significantly slowerthan Penn, ISI, LN, NTU, IA, and CGI/CMU.0,56 :o.~0.500.470.440,410.38 ~'21Categ: F--Score vs .
T ime by Party for F~ed--Length ~JrnmariesCIR IBM| LNN I I.IIi ?
?L l .
/ l l  ?
C~I  / S~IRB~ I CGI/CMU.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.25 ~ 33 37 41 ~,5 49 ~ 57~T IHEFigure 4: Categorization F-score versus Time byParticipant (fixed-length summaries).
F-scoresare not significantly different, and neither are timedifferences.Figures 3 and 4).
In this task, in the absenceof a topic, the statistical salience systems whichperformed relatively more accurately in the ad-hoc task had no advantage over the others, and sotheir, performance more closely resemble that ofother systems.
Instead, the systems more often re-lied on inclusion of the first sentence of the source- a useful strategy for newswire (Brandow et al1994): the generic (categorization) summaries hada higher percentage of selections of first sentencesfrom the source than the adhoc summaries (35% ofS1 and 41% of $2 for categorization, compared to21% S1 and 32% $2 for adhoc).
We may surmisethat in this task, where performance on full-textwas hard to begin with, the systems were al~l find-ing the categorization task equally hard, with noparticular technique for producing generic sum-maries standing out.5.3 Agreement  between SubjectsAs indicated in Table 9, the unanimous agreementof just 16.6% and 19.5% in the adhoc and cat-egorization tasks respectively is low: the agree-ment data has Kappa (Carletta et al 1997) of.38 for adhoc and .29 for categorization 4.
The ad-hoc pairwise and 3-way agreement (i.e., agreementbetween groups of 3 subjects) is consistent with a3-subject "dry-run" adhoc consistency task car-ried out earlier.
However, it is much lower thanreported in 3-subject adhoc experiments in TREC(Harman and Voorhees 1996).
One possible xpla-nation is that in contrast o our subjects, TRECsubjects had years of experience in this task.
It isalso possible that our mix of documents had fewerobviously relevant or obviously irrelevant docu-ments than TREC.
However, as (Voorhees 1998)has shown in her TREC study, system perfor-mance rankings can remain relatively stable evenwith lack of agreement in relevance judgments.Further, (Voorhees 1998) found, when only rel-evant documents were considered (and measuringagreement by intersection over union), 44.7% pair-wise agreement and 30.1% 3-way agreement with3 subjects, which is comparable to our scores onthis latter measure (52.9% pairwise, 36.9% 3-wayon adhoc, 45.9% pairwise, 29.7% 3-way on cate-gorization).6 Question-answering (Q&=A) taskIn this task, the summarization system, given adocument and a topic, needed to produce an in-formative, topic-related summary that containedthe answers found in that document o a set oftopic-related questions.
These questions covered"obligatory" information that had to be providedin any document judged relevant o the topic.
Forexample, for a topic concerning prison overcrowd-ing, a topic-related question would be "What isthe name of each correction facility where the re-ported overcrowding exists?
"6.1 Exper imenta l  DesignThe topics we chose were a subset of the 20 adhocTREC topics selected.
For each topic, 30 rele-vant documents from the adhoc task corpus werechosen as the source texts for topic-related sum-marization.
The principal tasks of each evaluator(one evaluator per topic, 3 in all) were to preparethe questions and answer keys and to score the4Dropping two outlier assessors in the categoriza-tion task - the fastest and the slowest - resulted in thepairwise and three-way agreement going up to 69.3%and 54.0% respectively, making the agreement com-parable with the adhoc task.82Proceedings of EACL '99PairwiseAdhoc 69.1Categorization 56.4Adhoc Dry-Run 72.7TREC 88.03-way Al l  7 All 1453.7 NA 16.650.6 19.5 NA59.1 NA NA71.7 NA NATable 9: Percentage of decisions ubjects agreed on when viewing full-text (consistency tasks).system summaries.
To construct he answer key,each evaluator marked off any passages in the textthat provided an answer to a question (exampleshown in Table 10).The summaries generated by the participants(who were given the topics and the documentsto be summarized, but not the questions) werescored against the answer key.
The evaluatorsused a common set of guidelines for writing ques-tions, creating answer keys, and scoring sum-maries that were intended to minimize variabilityacross evaluators in the methods used s.Eight of the adhoc participants also submittedsummaries for the Q&A evaluation.
Thirty sum-maries per topic were scored against the answerkeys.6 .2  Scor ingEach summary was compared manually to the an-swer key for a given document.
If  a summary con-tained a passage that was tagged in the answerkey as the only available answer to a question,the summary was judged Correct for that ques-tion as long as the summary provided sufficientcontext for the passage; if there was insufficientcontext, the summary was judged Partially Cor-rect.
If needed context was totally lacking or wasmisleading, or if the summary did not contain theexpected passage at all, the summary was judgedMissing for that question.
In the case where (a)the answer key contained multiple tagged passagesas answer(s) to a single question and (b) the sum-mary did not contain all of those passages, asses-sors applied additional scoring criteria to deter-mine the amount of credit to assign.Two accuracy metrics were defined, ARL (An-swer Recall Lenient) and ARS (Answer RecallStrict):ARL = (nl + (.5 * n2))/n3 (4)ARS = nl/n3 (5)where nl is the number of Correct answers in thesummary, n2 is the number of Partially Correctanswers in the summary, and n3 is the number ofquestions answered in the key.
A third measure,SWe also had each of the evaluators score a portionof each others' test data; the scores across evaluatorswere very similar, with one exception.ARA (Answer Recall Average), was defined as theaverage of ARL and ARS.6.3 Resu l tsFigure 5 shows a plot of the ARA against com-pression.
The "model" summaries were sentence-extraction summaries created by the evaluatorsfrom the answer keys but not used to evaluatethe summaries.
For the machine-generated sum-maries, the highest ARA was associated with theleast reduction (35-40% compression).
The sys-tems which were in Group I in accuracy on theadhoc task, CGI /CMU and Cornell/SabIR, wereat the top of the ARA ordering of systems ontopics 257 and 271.
The participants' human-evaluated ARA scores were strongly correlatedwith scores computed by a program from Cor-nell/SabIR which measured overlap between sum-maries and answers in the key (Pearson r > .97,a < 0.0001).
The Q&A evaluation is thereforepromising as a new method for automated evalu-ation of informative summaries.7 Conc lus ionsSUMMAC has established efinitively in a large-scale evaluation that automatic text summariza-tion is very effective in relevance assessment tasks.Summaries at relatively low compression rates(summaries as short as 17% of source length foradhoc, 10% for categorization) allowed for rele-vance assessment almost as accurate as with full-text (5% degradation in F-score for adhoc and14% degradation for categorization, both degra-dations not being statistically significant), whilereducing decision-making time by 40% (catego-rization) and 50% (adhoc).
Analysis of feed-back forms filled in after each decision indicatedthat the intelligibility of present-day machine-generated summaries is high, due to use of sen-tence extraction and coherence "smoothing" 6.The task of topic-related summarization, whenlimited to passage xtraction, can be character-ized as a passage ranking problem, and as suchlends itself very well to information retrieval tech-SOn the adhoc task, 99% of F were judged "intel-ligible", as were 93% $2, 96% B, 83% S1; similar datafor categorization.83Proceedings of EACL '9967II~ m*!9.3..................... ?0-92;'1 25~S0 ~  zT  A2T I ,l~271"9 0='?
21r,8M~700.0 D.l Q.2 0.3 03, D.$Compr*nlOn~CG I~CllU I GEC?re~ ImaDIR iN~UP l teFigure 5: ARA versus Compression by Participant.
"Modsumms" are model summaries.T i t le  : Computer SecurityDescr ip t ion  : Identify instances of illegal entry into sensitivecomputer networks by nonauthorized personnel.Narrat ive : Illegal entry into sensitive computer networksis a serious and potentially menacing problem.
Both 'hackers' andforeign agents have been known to acquire unauthorized entry intovarious networks.
Items relative this subject would include but notbe limited to instances of illegally entering networks containinginformation of a sensitive nature to specific countries, such asdefense or technology information, international banking, etc.
Itemsof a personal nature (e.g.
credit card fraud, changing of collegetest scores) should not be considered relevant.Quest ions1)Who is the known or suspected hacker accessing a sensitive computer or computer network?2) How is the hacking accomplished or putatively achieved?..3) Who is the apparent arget of the hacker?4) What did the hacker accomplish once the violation occurred?What was the purpose in performing the violation?5) What is the time period over which the breakins were occurring?As a federal grand jury decides whether he should be prosecuted, <Ql>a graduatestudent</Ql> linked to a ~virus'' that disrupted computers nationwide <Q5>lastmonth</~5>has been teaching his lawyer about the technical subject and turning downoffers for his life story ..... No charges have been filed against <Ql>Morris</Ql>,who reportedly told friends that he designed the virus that temporarily clogged about<q3>6,000 university and military computers</Q3> <Q2>linked to the Pentagon's Arpanetnetwork</Q2> ......Table 10: Q&:A Topic 258, topic-related questions, and part of a relevant source document showinganswer key annotations.84Proceedings ofEACL '99niques.
Summarizers that performed most accu-rately in the adhoc task used statistical passagesimilarity and passage ranking methods commonin information retrieval.
Overall, the most accu-rate systems in this task used similar features andhad similar sentence xtraction behavior.However, for the generic summaries in the cat-egorization task (which was hard even for hu-mans with full-text), in the absence of a topic, thesummarization methods in use by these systemswere indistinguishable in accuracy.
Whether thissuggests an inherent limitation to summarizationmethods which produce xtracts of the source, asopposed to generating abstracts, remains to beseen.In future, text summarization evaluations willbenefit greatly from the availability of test setscovering a wider variety of genres, and includingmuch longer documents.
The extrinsic and in-trinsic evaluations reported here are also relevantto the evaluation of other NLP technologies wherethere may be many potentially acceptable outputs(e.g., machine translation, text generation, speechsynthesis).AcknowledgmentsThe authors wish to thank Eric Bloedorn, JohnBurger, Mike Chrzanowski, Barbara Gates, GlennIwerks, Leo Obrst, Sara Shelton, and Sandra Wag-ner, as well as 51 experimental subjects.
We arealso grateful to the Linguistic Data Consortiumfor making the TREC documents available to us,and to the National Institute of Standards andTechnology for providing TREC data and the ini-tial version of the ASSESS tool.Re ferencesBrandow, R., K. Mitze, and L. Rau.
1994.
Auto-matic ondensation f electronic publications bysentence selection.
Information Processing andManagement, 31(5).Carletta, J., A. Isard, S. Isard, J. C. Jowtko, G.Doherty-Sneddon, and A. H. Anderson.
1997.The Reliability of a Dialogue Structure CodingScheme.
Computational Linguistics, 23, 1, 13-32.Edmundson, H.P.
1969.
New methods in auto-matic abstracting.
The Association for Comput-ing Machinery, 16(2).Harman, D.K.
and E.M. Voorhees.
1996.
The fifthtext retrieval conference (trec-5).
National In-stitute of Standards and Technology NIST SP500-238.Jing, H., R. Barzilay, K. McKeown, and M. E1-hadad.
1998.
Summarization evaluation meth-ods: Experiments and analysis, in WorkingNotes of the AAAI Spring Symposium on Intel-ligent Text Summarization, Spring 1998, Tech-nical Report, AAAI, 1998.Kupiec, J. Pedersen, and F. Chen.
1995.
A train-able document summarizer.
Proceedings of the18th ACM SIGIR Conference (SIGIR'95).Mani, I. and E. Bloedorn.
1997.
Multi-documentSummarization by Graph Search and Merging.Proceedings of the Fourteenth National Con-ference on Artificial Intelligence (AAAI-97),Providence, RI, July 27-31, 1997, 622-628.Maybury, M. 1995.
Generating Summaries fromEvent Data.
Information Processing and Man-agement, 31,5, 735-751.Minel, J-L., S. Nugier, and G. Pint.
1997.
How toappreciate the quality of automatic text sum-marization.
In Mani, I. and Maybury, M., eds.,Proceedings of the A CL/EA CL '97 Workshop onIntelligent Scalable Text Summarization.Morris, A., G. Kasper, and D. Adams.
1992.The Effects and Limitations of Automatic TextCondensing on Reading Comprehension Perfor-mance.
Information Systems Research, 3(1).Paice, C. 1990.
Constructing literature abstractsby computer: Techniques and prospects.
Infor-mation Processing and Management, 26(1).Rath, G.J., A. Resnick, and T.R.
Savage.
1961.The formation of abstracts by the selection ofsentences.
American Documentation, 12(2).Salton, G., A. Singhal, M. Mitra, and C. Buckley.1997.
Automatic Text Structuring and Summa-rization.
Information Processing and Manage-ment, 33(2).Sparck-Jones, K. 1998.
Summarizing: Where arewe now?
where should we go?
Mani, I.and Maybury, M., eds., Proceedings of theACL/EACL'97 Workshop on Intelligent Scal-able Text Summarization.Tombros, A., and M. Sanderson.
1998.
Advan-tages of query biased summaries in informationretrieval, in Proceedings of the 21st A CM SIGIRConference (SIGIR'98), 2-10.Voorhees, Ellen M. 1998.
Variations in RelevanceJudgments and the Measurement of RetrievalEffectiveness.
In Proceedings of the 21st An-nual International ACM SIGIR Conference onResearch and Development in Information Re-trieval (SIGIR-98), Melbourne, Australia.
315-323.85
