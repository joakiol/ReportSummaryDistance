Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1169?1180,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsDetecting Disagreement in Conversations using Pseudo-MonologicRhetorical StructureKelsey Allen Giuseppe Carenini Raymond T. NgDepartment of Computer Science, University of British ColumbiaVancouver, BC, V6T 1Z4, Canada{kelseyra, carenini, rng}@cs.ubc.caAbstractCasual online forums such as Reddit,Slashdot and Digg, are continuing to in-crease in popularity as a means of com-munication.
Detecting disagreement inthis domain is a considerable challenge.Many topics are unique to the conversa-tion on the forum, and the appearanceof disagreement may be much more sub-tle than on political blogs or social me-dia sites such as twitter.
In this analy-sis we present a crowd-sourced annotatedcorpus for topic level disagreement detec-tion in Slashdot, showing that disagree-ment detection in this domain is difficulteven for humans.
We then proceed toshow that a new set of features determinedfrom the rhetorical structure of the con-versation significantly improves the per-formance on disagreement detection overa baseline consisting of unigram/bigramfeatures, discourse markers, structural fea-tures and meta-post features.1 IntroductionHow does disagreement arise in conversation?
Be-ing able to detect agreement and disagreement hasa range of applications.
For an online educator,dissent over a newly introduced topic may alertthe teacher to fundamental misconceptions aboutthe material.
For a business, understanding dis-putes over features of a product may be helpful infuture design iterations.
By better understandinghow debate arises and propagates in a conversa-tion, we may also gain insight into how authors?opinions on a topic can be influenced over time.The long term goal of our research is to laythe foundations for understanding argumentativestructure in conversations, which could be appliedto NLP tasks such as summarization, informationretrieval, and text visualization.
Argumentativestructure theory has been thoroughly studied inboth psychology and rhetoric, with negation anddiscourse markers, as well as hedging and dis-preferred responses, being known to be indicativeof argument (Horn, 1989; Brown and Levinson,1987).
As a starting point, in this paper we focuson the detection of disagreement in casual con-versations between users.
This requires a gener-alized approach that can accurately identify dis-agreement in topics ranging from something asmundane as whether GPS stands for galactic po-sitioning system or global positioning system, tomore ideological debates about distrust in science.Motivated by the widespread consensus in bothcomputational and theoretical linguistics on theutility of discourse markers for signalling prag-matic functions such as disagreement and personalopinions (Webber and Prasad, 2008; Abbott etal., 2011; J. E. Fox-Tree, 2010), we introduce anew set of features based on the Discourse Tree(DT) of a conversational text.
Discourse Treeswere formalized by Mann and Thompson (1988)as part of their Rhetorical Structure Theory (RST)to represent the structure of discourse.
Althoughthis theory is for monologic discourse, we proposeto treat conversational dialogue as a collection oflinked monologues, and subsequently build a rela-tion graph describing both rhetorical connectionswithin user posts, as well as between differentusers.
Features obtained from this graph offer sig-nificant improvements on disagreement detectionover a baseline consisting of meta-post features,lexical features, discourse markers and conversa-tional features.
Not only do these features improvedisagreement detection, but the discovered impor-tance of relations known to be theoretically rele-vant to disagreement detection, such as COMPAR-ISON (Horn, 1989), suggest that this approach maybe capturing the essential aspects of the conversa-tional argumentative structure.1169As a second contribution of this work, we pro-vide a new dataset consisting of 95 topics anno-tated for disagreement.
Unlike the publicly avail-able ARGUE corpus based on the online debateforum 4forums.com (Abbott et al., 2011), our cor-pus is based on Slashdot, which is a general pur-pose forum not targeted to debates.
Therefore, weexpect that detecting disagreement may be a moredifficult task in our new corpus, as certain topics(like discussing GPS systems) may be targeted to-wards objective information sharing without anyparticipants expressing opinions or stances.
Be-cause of this, our corpus represents an excellenttestbed to examine methods for more subtle dis-agreement detection, as well as the major differ-ences between news-style and argument-style dia-logue.2 Related WorkIn the past decade, there have been a number ofcomputational approaches developed for the taskof disagreement and controversy detection, partic-ularly in synchronous conversations such as meet-ings (Somasundaran et al., 2007; Raaijmakers etal., 2008) and in monologic corpora such as newscollections (Awadallah et al., 2012) and reviews(Popescu et al., 2005; Mukherjee and Liu, 2012).In the domain of synchronous conversations,prosodic features such as duration, speech rate andpause have been used for spoken dialogue (Wanget al., 2011; Galley et al., 2004).
Galley et al.
(2004) found that local features, such as lexicaland structural features, as well as global contex-tual features, were particularly useful for identify-ing agreement/disagreement in the ICSI meetingcorpus.
Germesin and Wilson (2009) also showedaccuracies of 98% in detecting agreement in theAMI corpus using lexical, subjectivity and dia-logue act features.
However, they note that theirsystem could not classify disagreement accuratelydue to the small number of training examples inthis category.
Somasundaran et al.
additionallyshow that dialogue act features complement lexi-cal features in the AMI meeting corpus (Somasun-daran et al., 2007).
These observations are takeninto account with our feature choices, and we usecontextual, discourse and lexical features in ouranalysis.In the monologic domain, Conrad et al.
(2012)recently found rhetorical relations to be useful forargument labelling and detection in articles on thetopic of healthcare.
Additionally, discourse mark-ers and sentiment features have been found to as-sist with disagreement detection in collections ofnews documents on a particular topic, as well asreviews (Choi et al., 2010; Awadallah et al., 2012;Popescu et al., 2005).In the asynchronous domain, there has been re-cent work in disagreement detection, especially asit pertains to stance identification.
Content basedfeatures, including sentiment, duration, and dis-course markers have been used for this task (Yinet al., 2012; Somasundaran and Wiebe, 2009;Somasundaran and Wiebe, 2010).
The structureof a conversation has also been used, althoughthese approaches have focused on simple rules fordisagreement identification (Murakami and Ray-mond, 2010), or have assumed that adjacent postsalways disagree (Agrawal et al., 2003).
More re-cent work has focused on identifying users?
atti-tudes towards each other (Hassan et al., 2010), in-fluential users and posts (Nguyen et al., 2014), aswell as identifying subgroups of users who shareviewpoints (Abu-Jbara et al., 2010).
In Slashdot,the h-index of a discussion has been used to rankarticles according to controversiality, although noquantitative evaluation of this approach has beengiven, and, unlike in our analysis, they did notconsider any other features (Gomez et al., 2008).Content based features such as polarity and co-sine similarity have also been used to study influ-ence, controversy and opinion changes on micro-blogging sites such as Twitter (Lin et al., 2013;Popescu and Pennacchiotti, 2010).The simplified task of detecting disagreementbetween just two users (either a question/responsepair (Abbott et al., 2011) or two adjacent para-graphs (Misra and Walker, 2013)) has also been re-cently approached on the ARGUE corpus.
Abbottet al.
(2011) use discourse markers, generalizeddependency features, punctuation and structuralfeatures, while Misra and Walker (2013) focus onn-grams indicative of denial, hedging and agree-ment, as well as cue words and punctuation.
Mostsimilar to our work is that by Mishne and Glance(2006).
They performed a general analysis of we-blog comments, using punctuation, quotes, lexi-con counts, subjectivity, polarity and referrals todetect disputative and non-disputative comments.Referrals and questions, as well as polarity mea-sures in the first section of the post, were found tobe most useful.
However, their analysis did not1170Type Num P/A S/P W/P Num Authors W/S TBP TT TP LengthDisagreement - C 19.00 1.02 3.21 65.86 15.84 20.47 4.60 50.90 16.21 49.11Disagreement - NC 27.00 1.00 3.08 59.80 14.07 19.33 3.89 42.29 14.11 42.26No disagreement - NC 28.00 1.03 2.85 57.25 10.29 19.94 6.83 50.12 10.50 28.00No disagreement - C 21.00 1.00 3.69 69.66 6.29 20.22 6.14 18.22 6.29 19.81Table 1: Characteristics of the four categories determined from the crowd-sourced annotation.
All values except for the numberof topics in the category are given as the average score per topic across all topics in that category.
Key: C and NC: Confident(score ?
0.75) and Not confident (score < 0.75), Num: Number of topics in category, P/A: Posts per author, S/P: Sentencesper post, W/P: Words per post, Num Authors: Number of authors, W/S: Words per sentence, TBP: Time between posts(minutes), TT: Total time in minutes, TP: Total posts, and Length: Length of topic in sentencestake into account many features that have beensubsequently shown to be relevant, such as dis-course markers and conversational structure, andwas hampered by a severe imbalance in the testset (with very few disputative comments).Our method takes advantage of insights frommany of these previous studies, focusing on dis-cussion thread structure as well as text based fea-tures to form our basic feature set.
It is unlikeMishne and Glance?s work in that we incorporateseveral new features, have a balanced testing andtraining set, and only use comments from one typeof online blog.
Furthermore, it is a very differenttask from those so far performed on the ARGUEcorpus, as we consider topics discussed by morethan two users.
We aim to compare our features tothose found to be previously useful in these relatedtasks, and expect similar feature sets to be usefulfor the task of disagreement detection in this newcorpus.3 CorpusThe corpus stems from the online forum Slash-dot.1Slashdot is a casual internet forum, includ-ing sections for users to ask questions, post arti-cles, and review books and games.
For the task ofdisagreement detection, we focus our analysis onthe section of the site where users can post arti-cles, and then comment either on the article or re-spond to other users?
posts.
This results in a tree-like dialogue structure for which the posted arti-cle is the root, and branches correspond to threadsof comments.
Each comment has a timestamp atthe minute resolution as well as author information(although it is possible to post on the forum anony-mously).
Additionally, other users can give differ-ent posts scores (in the range -1 to 5) as well as cat-egorizing posts under ?funny?, ?interesting?, ?in-formative?, ?insightful?, ?flamebait?, ?off topic?,or ?troll?.
This user moderation, as well as the1www.slashdot.orgformalized reply-to structure between comments,makes Slashdot attractive over other internet fo-rums as it allows for high-quality and structuredconversations.In a previous study, Joty et al.
(2013) selected20 articles and their associated comments to be an-notated for topic segmentation boundaries and la-bels by an expert Slashdot contributor.
They de-fine a topic as a subset of the utterances in a con-versation, while a topic label describes what thegiven topic is about (e.g., Physics in videogames).Of the 98 annotated topics from their dataset, wefiltered out those with only one contributing user,for a total of 95 topics.
Next, we developed aHuman Intelligence Task (HIT) using the crowd-sourcing platform Crowdflower.2The objective ofthis task was to both develop a corpus for testingour disagreement detection system, as well as toinvestigate how easily human annotators can de-tect disagreement in casual online forums.
Fortraining, users were shown 3 sample topics, la-belling them as containing disagreement or not.
Ineach round, annotators were shown 5 topics, witha set of radio buttons for participants to choose?Yes?, ?No?, or ?Not sure?
in response to askingwhether or not the users in the conversation dis-agree on the topic.
In order to limit the number ofspam responses, users were shown test questions,which consisted of topics where there was obvi-ous disagreement, as well as topics where therewas obviously no disagreement (either agreement,or more news-style information sharing).
We re-quired that users correctly identify 4 of these testtopics before they were allowed to continue withthe task.
Users were also shown test questionsthroughout the task, which, if answered incor-rectly, would reduce the amount of money they re-ceived for the task, and ultimately disqualify them.For each topic, five different judgements wereobtained.
We consider the trust of each partici-2www.Crowdflower.com1171(a) Discourse tree (b) Relation graphFigure 1: Discourse tree (left) with extracted relation graph (right) for a sample conversation involving three users with threedifferent posts P1, P2 and P3.
N1, N2 and N3 are the corresponding nodes in the relation graph.pant as the fraction of test questions which theyanswered correctly.
Then, each topic is assigneda score according to a weighted average of the re-sponses, with the weight being the trust of eachparticipant:score = A?users(testcorrecttesttotal)useri?
(0, 0.5, 1)(1)where 0, 0.5 and 1 represent the answers ?No?,?Not sure?
and ?Yes?
to the question of disagree-ment existence, and A is a normalization factor.If the score is less than 0.5, its confidence wouldbe 1?score towards ?No disagreement?, whereasgreater than 0.5 would be a confidence of scoretowards ?Disagreement?.
The average confidencescore across all topics was 0.73.
Our corpus con-sists of 49 topics without disagreement and 46 top-ics with disagreement.
Interestingly, 22 topics hadconfidence scores below 55%, which suggests thatsubtle disagreement detection is a subjective anddifficult task.
Further statistics for the developedcorpus are given in Table 1.4 Features for Disagreement DetectionThe features we use in our experiments combineinformation from conversational structure, rhetor-ical relations, sentiment features, n-gram models,Slashdot meta-features, structural features, andlexicon features.4.1 Rhetorical Relation GraphsDiscourse markers have been found to aid inargument and disagreement detection, and fortasks such as stance identification (Abbott et al.,2011; Misra and Walker, 2013; Somasundaranand Wiebe, 2009).
We aim to improve over dis-course markers by capturing the underlying dis-course structure of the conversation in terms ofdiscourse relations.In Rhetorical Structure Theory, Discourse treesare a hierarchical representation of documentstructure for monologues (Mann and Thompson,1988).
At the lowest level, Elementary DiscourseUnits (EDUs) are connected by discourse rela-tions (such as ELABORATION and COMPARISON),which in turn form larger discourse units that arealso linked by these relations.
Computational sys-tems (discourse parsers) have been recently devel-oped to automatically generate a discourse tree fora given monologue (Joty et al., 2013).
Althoughtheoretically the rhetorical relations expected indialogues are different from those in monologues(Stent and Allen, 2000), no sophisticated compu-tational tools exist yet for detecting these relationsreliably.
The core idea of this work is that someuseful (although noisy) information about the dis-course structure of a conversation can be obtainedby applying state-of-the-art document level dis-course parsing to parts of the conversation.More specifically, posts on a particular topicare concatenated according to their temporal order.This pseudo-monologic document is then fed to apublicly available document level discourse parser(Joty et al., 2013).
A discourse tree such as thatseen in Figure 1a is output by the parser.
Then,we extract the novel relation graph (Figure 1b)from the discourse tree.
In this graph, each node(N1, N2, N3) corresponds to a post (P1, P2, P3)and links aim to capture the argumentative struc-ture.
There are three cases when a link is addedbetween two nodes in the relation graph.
Firstly,links existing between two posts directly, such asthe COMPARISON relation between P2 and P3, areadded between the corresponding nodes in the re-1172lation graph (N2 and N3).
Secondly, links existingbetween fractions of posts in the discourse tree areadded to the relation graph (e.g.
if (S2,P2) wasconnected to (S1,P3) directly, N2 and N3 wouldhave an additional link with that label).
Finally,when posts are connected through internal nodes(such as P1 and I1 in Figure 1a), a labelled link isadded for each post in the internal node to the re-lation graph (N1->N2 and N1->N3 in Figure 1b).This relation graph allows for the extractionof many features that may reflect argumenta-tive structure, such as the number of connec-tions, the frequency of each rhetorical relationin the graph per post (diff per post), and thefrequency as a percentage of all rhetorical rela-tions (diff percentage).
For example, COMPAR-ISON relations are known to indicate disagree-ment (Horn, 1989), so we expect higher fre-quencies of this relation if the conversation con-tains argument.
Features from the discourse treesuch as the average depth of each rhetorical re-lation are also added to reflect the cohesivenessof conversation.
Moreover, features combiningthe graph and tree representations, such as the ra-tio of the frequency of a rhetorical relation occur-ring between different posts to the average depth(CONTRAST between different postsAverage depth of CONTRAST), called avg ratioare implemented.
These reflect the hypothesisthat relations connecting larger chunks of text (orwhole posts) may be more important than thoseconnecting sentences or only partial posts.Finally, the sub-trees corresponding to individ-ual posts are used to extract the average frequencyof rhetorical relations within a post (same perpost) and the average frequency of a rhetorical re-lation with respect to other rhetorical relations inthe post (same percentage).
A measure of howoften a rhetorical relation connects different userscompared to how often it connects discourse unitsin the same post (same to diff ), is also added.These capture the intuition that certain rhetoricalrelations such as CAUSE, EVIDENCE and EXPLA-NATION are expected to appear more within a postif users are trying to support their perspective inan argument.
In total, there are 18 (rhetoricalrelations)?7 (avg ratio, avg depth, same per post,same percentage, diff percentage, diff per post,same to diff ) + 1 (number of connections) = 127features.4.2 Discourse MarkersMotivated by previous work, we include a fre-quency count of 17 discourse markers which werefound to be the most common across the ARGUEcorpus (Abbott et al., 2011).
Furthermore, we hy-pothesize that individual discourse markers mighthave low frequency counts in the text.
Therefore,we also include an aggregated count of all 17 dis-course markers in each fifth of the posts in a topic(e.g.
the count of all 17 discourse markers in thefirst fifth of every post in the topic).
Altogether,there are 5 aggregated discourse marker featuresin addition to the 17 frequency count features.4.3 Sentiment FeaturesSentiment polarity features have been shown to beuseful in argument detection (Mishne and Glance,2006).
For this work, we use four sentimentscoring categories: the variance, average score,number of negative sentences, and controversialityscore (Carenini and Cheung, 2008) of sentencesin a post.
These are determined using SoCAL(Taboada et al., 2011), which gives each sentencea polarity score and has been shown to work wellon user-generated content.Overall, we have two main classes of sentimentfeatures.
The first type splits all the posts in a topicinto 4 sections corresponding to the sentences ineach quarter of the post.
The sentiment scores de-scribed above are then applied to each section ofthe posts (e.g.
one feature is the number of neg-ative sentences in the first quarter of each post).As a separate feature, we also include the scoreson just the first sentence, as Mishne and Glance(2006) previously found this to be beneficial.
Thisgives a total of 4?5 = 20 features.
We refer to thisset as ?sentiment?.Motivated by the rhetorical features, our sec-ond main class of sentiment features aims to iden-tify ?more important?
posts for argument detec-tion by applying the four categories of sentimentscores to only those posts connected by each ofour 18 rhetorical relations.
This is done for bothposts with an inner rhetorical connection (iden-tified by the sub-tree for that post), as well asfor posts connected by a rhetorical relation inthe relation graph.
This results in a total of (4sentiment categories)?
(2 (different + same postconnections))?
(18 rhetorical relations) = 144 fea-tures.
This set is referred to as ?RhetSent?.11734.4 Fragment Quotation GraphsAs previously noted in (Murakami and Raymond,2010; Gomez et al., 2008), the structure of dis-cussion threads can aid in disagreement detec-tion.
In online, threaded conversations, the stan-dard approach to extracting conversational struc-ture is through reply-to relations usually presentin online forums.
However, if users strongly dis-agree on a topic (or sub-topic), they may chooseto quote a specific paragraph (defined as a frag-ment) of a previous post in their reply.
Being ableto determine which specific fragments are linkedby relations may then be useful for more targetedcontent-based features, helping to reduce noise.
Inorder to address this, we use the Fragment Quo-tation Graph (FQG), an approach previously de-veloped by Carenini et al.
(2007) for dialogueact modelling and topic segmentation (Joty et al.,2011; Joty et al., 2013).For our analysis, the FQG is found over theentire Slashdot article.
We then select the sub-graph corresponding to those fragments in the tar-get topic.
From the fragment quotation sub-graph,we are then able to extract features for disagree-ment detection such as the number of connections,total number of fragments, and the average pathlength between nodes which we hypothesize tobe useful.
We additionally extract the h-index(Gomez et al., 2008) and average branching ratioper fragment of the topic from the simpler reply-toconversational structure.
In total, there are 8 FQGfeatures.4.5 N-gram modelsAs noted previously (Somasundaran and Wiebe,2010; Thomas et al., 2006), it is often difficult tooutperform a unigram/bigram model in the task ofdisagreement and argument detection.
In this anal-ysis, because of the very small number of sam-ples, we do not consider dependency or part-of-speech features, but do make a comparison witha filtered unigram/bigram model.
In the filtering,we remove stop words and any words that occur infewer than three topics.
This helps to prevent topicspecific words from being selected, and limits thenumber of possible matches slightly.
Additionally,we use a lexicon of bias-words (Recasens et al.,2013) to extract a bias-word frequency score overall posts in the topic as a separate feature.4.6 Structural FeaturesLength features have been well documented inthe literature to provide useful information aboutwhether or not arguments exist, especially in on-line conversations that may be more informativethan subjective (Biyani et al., 2014; Yin et al.,2012).
In this work, length features include thelength of the post in sentences, the average num-ber of words per sentence, the average number ofsentences per post, the number of contributing au-thors, the rate of posting, and the total amount oftime of the conversation.
This results in a total of9 features.4.7 PunctuationLike many other features already described, fre-quency counts of ???,?!?,???,?
?, and ?.?
are found foreach fifth of the post (the first fifth, second fifth,etc.).
These counts are then aggregated across allposts for a total of 5?5 = 25 features.4.8 ReferralsReferrals have been found to help with the detec-tion of disagreement (Mishne and Glance, 2006),especially with respect to other authors.
Sincethere are no direct referrals to previous authorsin this corpus, references to variations of ?you?,?they?, ?us?, ?I?, and ?he/she?
in each fifth of thepost are included instead, for a total of 5?5 = 25features.4.9 Meta-Post FeaturesSlashdot allows users to rank others?
posts with theequivalent of a ?like?
button, changing the ?score?of the post (to a maximum of 5).
They are alsoencouraged to tag posts as either ?Interesting?,?Informative?, ?Insightful?, ?Flamebait?, ?Troll?,?Off-topic?
or ?Funny?.
Frequency counts ofthese tags as a percentage of the total number ofcomments are included as features, as well as theoverall fraction of posts which were tagged withany category.
The average score across the topic,as well as the number of comments with a scoreof 4 or 5, are also added.
These are expected tobe informative features, since controversial topicsmay encourage more up and down-voting on spe-cific posts, and generally more user involvement.This results in 9 meta-post features.1174Feature Set Random Forest SVMP R F1 A ROC-AUC P R F1 A ROC-AUCN-grams 0.71 0.57 0.63 0.67 0.69 0.52 0.60 0.56 0.53 0.53Basic 0.69 0.67 0.68 0.69 0.73 0.62 0.62 0.62 0.63 0.67Basic+N-grams 0.73 0.66 0.69 0.70 0.73 0.57 0.65 0.60 0.59 0.61Basic+FQG 0.69 0.66 0.67 0.69 0.71 0.64 0.63 0.63 0.65 0.70Basic+Sentiment 0.68 0.65 0.66 0.68 0.73 0.61 0.59 0.60 0.62 0.67Basic+RhetStruct 0.71 0.70 0.70 0.71 0.73 0.73 0.70 0.71 0.73 0.78Basic+RhetStruct+FQG 0.69 0.69 0.69 0.70 0.73 0.74 0.74 0.74 0.75 0.80Basic+RhetAll 0.72 0.73 0.73 0.73 0.75 0.76 0.76 0.76 0.77 0.79RhetStructOnly 0.69 0.72 0.71 0.71 0.72 0.76 0.72 0.74 0.75 0.79RhetAllOnly 0.69 0.74 0.71 0.71 0.73 0.75 0.72 0.73 0.75 0.78All 0.71 0.72 0.71 0.72 0.74 0.74 0.77 0.75 0.76 0.77Table 2: Basic: Meta-post, all structural, bias words, discourse markers, referrals, punctuation RhetAll: Structural and sen-timent based rhetorical features All: Basic, all rhetorical, sentiment and FQG features.
The N-gram models include unigramsand bi-grams.
All feature sets in the bottom part of the table include rhetorical reatures.5 ExperimentsExperiments were all performed using the Wekamachine learning toolkit.
Two different types ofexperiments were conducted - one using all an-notated topics in a binary classification of con-taining disagreement or not, and one using onlythose topics with confidence scores greater than0.75 (corresponding to the more certain cases).
Allresults were obtained by performing 10 fold cross-validation on a balanced test set.
Additionally, in-fold cross-validation was performed to determinethe optimal number of features to use for each fea-ture set.
Since this is done in-fold, a paired t-testis still a valid comparison of different feature setsto determine significant differences in F-score andaccuracy.5.1 ClassifiersTwo classifiers were used for this task: RandomForest and SVM.
Random Forest was selected be-cause of its ability to avoid over-fitting data despitelarge numbers of features for relatively few sam-ples.
For all runs, 100 trees were generated in theRandom Forest, with the number of features to usebeing determined by in-fold optimization on the F-score.
For the SVM classifier, we use the normal-ized poly-vector kernel with a maximum exponentof 2 (the lowest possible), and a C parameter of1.0 (Weka?s default value).
This was chosen toavoid over-fitting our data.
We additionally usea supervised in-fold feature selection algorithm,Chi-Squared, to limit over-fitting in the SVM.
Thenumber of features to be used is also optimized us-ing in-fold cross-validation on the F-score.
Boththe SVM classifier and the Random Forest classi-fier were tested on the same training/testing foldpairs, with a total of 10 iterations.6 ResultsThe results of the experiments are shown in Ta-bles 2 and 3.
In order to compare to previous anal-yses, unigram and bigram features are shown, aswell as a combination of the basic features withthe n-grams.
When performing the experiments,we noticed that the n-gram features were hurtingthe performance of the classifiers when includedwith most of our other feature sets (or not chang-ing results significantly), and therefore those re-sults are not shown here.
As seen in the table,Feature Set Random Forest SVMP R F1 A ROC-AUC P R F1 A ROC-AUCN-grams 0.70 0.70 0.70 0.71 0.77 0.63 0.70 0.66 0.63 0.66Basic 0.74 0.69 0.72 0.72 0.77 0.73 0.70 0.72 0.71 0.78Basic+FQG 0.72 0.67 0.69 0.69 0.76 0.73 0.63 0.68 0.69 0.76Basic+Sentiment 0.71 0.65 0.68 0.68 0.76 0.73 0.67 0.70 0.70 0.76Basic+RhetStruct 0.79 0.75 0.77 0.77 0.78 0.79 0.67 0.72 0.74 0.79Basic+RhetStruct+FQG 0.76 0.71 0.73 0.73 0.77 0.74 0.64 0.69 0.70 0.78Basic+RhetAll 0.77 0.75 0.76 0.76 0.78 0.72 0.69 0.71 0.70 0.76RhetStructOnly 0.75 0.71 0.73 0.73 0.75 0.76 0.63 0.69 0.70 0.76RhetAllOnly 0.73 0.76 0.74 0.73 0.76 0.67 0.62 0.65 0.65 0.67All 0.73 0.69 0.71 0.70 0.76 0.71 0.70 0.70 0.69 0.74Table 3: Precision, recall, F1, accuracy and ROC-AUC scores for the simpler task of identifying the cases deemed ?highconfidence?
in the crowd-sourcing task.1175the best performing feature sets are those that in-clude rhetorical features under the SVM+?2clas-sifier.
In fact, these feature sets perform signifi-cantly better than a unigram/bigram baseline ac-cording to a paired t-test between the best clas-sifiers for each set (p < 0.0001).
The inclusionof rhetorical structure also significantly outper-forms the ?basic?
and ?basic+N-grams?
featurebaselines (which includes discourse markers, re-ferrals, punctuation, bias word counts and struc-tural features) with respect to both the F-scoreand accuracy (p < 0.02 for all feature sets withrhetorical features).
Overall, the feature sets ?Ba-sic+RhetAll?
and ?All?
under the SVM+?2clas-sifier perform best.
This performance is also bet-ter than previously reported results for the ARGUECorpus (Abbott et al., 2011), even though the ba-sic and unigram/bigram features perform similarlyto that reported in previous analyses.As an additional check, we also conduct exper-iments on the ?high confidence?
data (those topicswith a confidence score greater than 0.75).
Theseresults are shown in Table 3.
Clearly the basicfeatures perform better on this subset of the sam-ples, although the addition of rhetorical structurestill provides significant improvement (p< 0.001).Overall, this suggests that the rhetorical, sentimentand FQG features help more when the disagree-ment is more subtle.7 Analysis and DiscussionIn order to examine the quality of our features,we report on the rhetorical features selected, andshow that these are reasonable and in many cases,theoretically motivated.
Furthermore, we checkwhether the commonly selected features in eachof our feature categories are similar to those foundto be useful over the ARGUE corpus, as well aswithin other argument detection tasks in online fo-rums.The rhetorical features that are consistently se-lected are very well motivated in the context of ar-gument detection.
From the rhetorical structuralfeatures, we find COMPARISON relation featuresto be most commonly selected across all rhetoricalfeature sets.
Other highly ranked features includethe proportion of JOINT relations linking differentauthors, EXPLANATION relations between differ-ent authors, and the average depth of ELABORA-TION relations.The COMPARISON relations are expected to in-Structural Length of topic in sentences, to-tal number of authors, quotes infirst sentence, quotes in secondsentence, questions in first sen-tence, questions in second sen-tence, referrals to you and theyin first half of postMeta Number of comments with la-bels, number of comments la-belled ?Flamebait?, number ofcomments with scores of 4 or 5FQG Number of connections, Num-ber of fragments, Maximumnumber of links from one nodeRhetStruct COMPARISON (same to diff, diffper post, diff percentage, avgratio, same per post, same per-centage), EXPLANATION (avgratio, diff per post), JOINT(diff percentage), ELABORA-TION (average depth)Discourse Markers Aggregated first sentence, Ag-gregated middle, ?and?, ?oh?,?but?
frequency countsN-grams ?don?t ?
?, ?plus?, ?private?,?anti?, ?hey?, ?present?, ?mak-ing?, ?developers?RhetSent ELABORATION (variance insame post), ATTRIBUTION(variance in same post),CONTRAST (range in samepost)Sentiment Range of sentiment scores infirst sentence of all posts, rangeof sentiment scores over allpostsTable 4: Features found to be commonly selected over dif-ferent iterations of the classifiersdicate disagreement as motivated by theoreticalconsiderations (Horn, 1989).
The importance ofother rhetorical relation features can also be ex-plained by examining conversations in which theyappear.
In particular, EXPLANATION relations of-ten link authors who share viewpoints in a debate,especially when one author is trying to support theclaim of another.
The JOINT relations are also verywell motivated.
In the extreme case, conversationswith a very high number of JOINT relations be-tween different users are usually news based.
Thehigh proportion of these relations indicates thatmany users have added information to the conver-sation about a specific item, such as adding newsuggested videogame features to an ongoing list.Fewer JOINT relations seem to indicate disagree-ment, especially when found in conjunction withCOMPARISON relations between different users.This appears to generally indicate that users aretaking sides in a debate, and commenting specifi-cally on evidence which supports their viewpoint.1176The average depth of ELABORATION relationsreveals how deep the perceived connections arebetween users in a conversation over time.
DeeperELABORATION connections seem to indicate thatthe conversation is more cohesive.
Alone, thisdoes not signify disagreement/agreement but doesseem to signify argument-style over news-style di-alogues.
This is particularly helpful for differ-entiating between articles with many COMPARI-SON relations, as COMPARISON may be presentin both news-style dialogues (e.g.
comparing ci-tation styles) as well as argument style dialogues(e.g.
arguing over which of two operating systemsis superior).For the combined sentiment and rhetorical rela-tions, range and variance in ELABORATION, CON-TRAST and ATTRIBUTION within the same postare found to be the most informative features.
Ad-ditionally, neither ATTRIBUTION nor CONTRASTare useful features when only their structural in-formation is considered.
In the case of ATTRI-BUTION, we hypothesize that the added sentimentscore within the post differentiates between a neu-tral attribution (which would not signify disagree-ment) and a negative or positive attribution (whichmay signify disagreement).
For CONTRAST, theadded sentiment helps to distinguish between re-sponses such as ?We will be trying the Microsoftsoftware.
We won?t, however, be able to test theApple equivalent.?
and ?We will be trying the Mi-crosoft software.
We won?t, however, be trying theinferior Apple equivalent.?
where the second ex-ample more likely signals, or even provokes, dis-agreement.Outside of the rhetorical features, the discoursemarkers which are found to be the most useful inour experiments agree with those found in the AR-GUE corpus (Abbott et al., 2011).
Namely, ?oh?,?but?, ?because?
and ?and?
are discovered to be themost informative features.
We also find the aggre-gated discourse marker frequency count in the firstpart of each post to be useful.Previous analysis on Slashdot as a social net-work (Gomez et al., 2008) suggests that the h-index of the conversation is relevant for detectingcontroversy in a posted article.
We include theh-index as part of the Fragment Quotation Graphfeature set, but surprisingly do not find this to bea useful feature.
This may be due to our corpusinvolving relatively shallow conversational trees -the maximum h-index across all topics is three.Comparing to Mishne and Glance?s work, wealso find quotations, questions and sentimentrange near the beginning of a post to be very in-formative features.
These are often selected acrossall feature sets which include the ?basic?
set.The topics most often misclassified across allfeature sets are those with relatively few sen-tences.
In these cases, the rhetorical structureis not very well defined, and there is much lesscontent available for detecting quotes, punctuationand referrals.
Additionally, the feature sets whichonly use rhetorical and sentiment features consis-tently misclassify the same set of conversations(those that have lower quality discourse trees withfew connections).
When combined with the ?ba-sic?
feature set, these errors are mitigated, and thetopics which the ?basic?
features miss are pickedup by the rhetorical features.
This leads to the bestoverall accuracy and F-score.7.1 Discourse ParserA major source of error in detecting disagreementarises because of inaccuracies in our discourseparser.
In particular, document-level discourseparsing is a challenging task, with relatively fewparsers available at the time of this analysis (Jotyet al., 2013; Hernault et al., 2010).
We chose touse the discourse parser developed by Joty et al.which both identifies elementary discourse units ina text, and then builds a document-level discoursetree using Conditional Random Fields.
Becausetheir approach uses an optimal parsing algorithmas opposed to a greedy parsing algorithm, they areable to achieve much higher accuracies in rela-tion and structure identification than other avail-able parsers.
Here, results from their parser on thestandard RST-DT dataset are presented since thereis no currently available dialogic corpora to com-pare to.RST-DT InstructionalMetrics Joty HILDA Human JotySpan 83.84 74.68 88.70 81.88Nuclearity 68.90 58.99 77.72 63.13Relation 55.87 44.32 65.75 43.60Table 5: Joty et al.
document-level parser accuracy of theparser used in this paper.
The parser was originally testedon two corpora: RST-DT and Instructional.
HILDA was thestate-of-the-art parser at that time.
Span and Nuclearity met-rics assess the quality of the structure of the resulting tree,while the Relation metric assesses the quality of the relationlabels.Examining the relation labels confusion matrix1177Figure 2: Confusion matrix for relation labels on RST-DT.The X-axis represents predicted relations, while the Y-axiscorresponds to true values.
The relations are Topic-Change(T-C), Topic-Comment (T-CM), Textual Organization (T-O), Manner-Means (M-M), Comparison (CMP), Evaluation(EV), Summary (SU), Condition (CND), Enablement (EN),Cause (CA), Temporal (TE), Explanation (EX), Background(BA), Contrast (CO), Joint (JO), Same-Unit (S-U), Attribu-tion (AT) and Elaboration (EL).for the discourse parser in Figure 2, some of thechosen rhetorical features make even more sense.In particular, the confusion of ELABORATION andEXPLANATION may account for the perceived im-portance of ELABORATION relations in the analy-sis.
Likewise, CAUSE (which may be present whenusers attribute positive or negative qualities to anentity, signalling disagreement) is often confusedwith JOINT and ELABORATION which were oftenpicked as important features by our classifiers.8 Conclusions and Future WorkIn this paper, we have described a new set of fea-tures for detecting disagreement in online blog fo-rums.
By treating a written conversation as a seriesof linked monologues, we can apply a documentlevel discourse parser to extract a discourse treefor the conversation.
We then aggregate this infor-mation in a relation graph, which allows us to cap-ture post-level rhetorical relations between users.Combining this approach with sentiment featuresshows significantly improved performance in bothaccuracy and F-score over a baseline consistingof structural and lexical features as well as re-ferral counts, punctuation, and discourse markers.In building our new crowd-sourced corpus fromSlashdot, we have also shown the challenges ofdetecting subtle disagreement in a dataset that con-tains a significant number of news-style discus-sions.In future work, we will improve sentiment fea-tures by considering methods to detect opinion-topic pairs in conversation, similar to Somasun-daran and Wiebe (2009).
Additionally, we willincorporate generalized dependency and POS fea-tures (Abbott et al., 2011), which were not usedin this analysis due to the very small number oftraining samples in our dataset.
The fragment quo-tation graph features did not perform as well as weexpected, and in future work we would like to in-vestigate this further.
Furthermore, we will alsoexplore how to create a discourse tree from thethread structure of a conversation (instead of fromits temporal structure), and verify whether this im-proves the accuracy of the relation graphs, espe-cially when the temporal structure is not represen-tative of the reply-to relationships.Finally, we plan to apply our novel feature set toother corpora (e.g., ARGUE) in order to study theutility of these features across genres and with re-spect to the accuracy of the discourse parser.
Thismay provide insights into where discourse parserscan be most effectively used, as well as how tomodify parsers to better capture rhetorical rela-tions between participants in conversation.ReferencesRob Abbott, Marilyn Walker, Pranav Anand, JeanE.
Fox Tree, Robeson Bowmani and Joseph King.2011.
How can you say such things?!?
: Recogniz-ing Disagreement in Informal Political Argument.
InProceedings of LSM, pages 2-11.Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, andDragomir Radev.
2012.
Subgroup detection in ide-ological discussions.
In Proceedings of ACL, pages399-409.Rakesh Agrawal, Sridhar Rajagopalan, RamakrishnanSrikant, and Yirong Xu.
2003.
Mining newsgroupsusing networks arising from social behavior.
In Pro-ceedings of WWW, pages 529-535.Rawia Awadallah, Maya Ramanath, Gerhard Weikum.2012.
Harmony and Dissonance: Organizing thePeople?s Voices on Political Controversies.
In Pro-ceedings of WSDM, pages 523-532.Prakhar Biyani, Sumit Bhatia, Cornelia Caragea,Prasenjit Mitra.
2014.
Using non-lexical features foridentifying factual and opinionative threads in onlineforums.
In Knowledge-Based Systems, in press.Penelope Brown and Stephen Levinson.
1987.
Polite-ness: Some universals in language usage.
Cam-bridge University Press.1178Giuseppe Carenini and Jackie Cheung.
2008.
Extrac-tive vs. NLG-based Abstractive Summarization ofEvaluative Text: The Effect of Corpus Controver-siality.
In Proceedings of INLG, pages 33-41.Giuseppe Carenini, Raymond Ng, Xiaodong Zhou.2007.
Summarizing Email Conversations with ClueWords.
In Proceedings of WWW, pages 91-100.Yoonjung Choi, Yuchul Jung, and Sung-Hyon Myaeng.2010.
Identifying controversial issues and their sub-topics in news articles.
In Proceedings of PAISI,pages 140-153.Alexander Conrad, Janyce Wiebe and RebeccaHwa.
2012.
Recognizing Arguing Subjectivity andArgument Tags.
In ACL Workshop on Extra-Propositional Aspects of Meaning, pages 80-88.Jean E. Fox Tree.
2010.
Discourse markers acrossspeakers and settings.
Language and LinguisticsCompass, 3(1):1-113.Michel Galley, Kathleen McKeown, Julia Hirschberg,and Elizabeth Shriberg.
2004.
Identifying agreementand disagreement in conversational speech: Use ofbayesian networks to model pragmatic dependen-cies.
In Proceedings of ACL, pages 669-es.Sebastian Germesin and Theresa Wilson.
2009.
Agree-ment Detection in Multiparty Conversation.
In Pro-ceedings of International Conference on MultimodalInterfaces pages 7-14.Vicenc Gomez, Andreas Kaltenbrunner and VicenteLopez.
2008.
Statistical Analysis of the Social Net-work and Discussion Threads in Slashdot.
In Pro-ceedings of WWW, pages 645-654.Ahmed Hassan, Vahed Qazvinian, and DragomirRadev.
2010.
What?s with the attitude?
: identifyingsentences with attitude in online discussions.
In Pro-ceedings of EMNLP, pages 1245-1255.Hugo Hernault, Helmut Prendinger, David A. duVerleand Mitsuru Ishizuka.
2010.
HILDA: A DiscourseParser Using Support Vector Machine Classification.Dialogue and Discourse, 1(3):1-33.Laurence R. Horn.
1989.
A natural history of negation.Chicago University Press.Shafiq Joty, Giuseppe Carenini, and Chin-Yew Lin.2011.
Unsupervised modeling of dialog acts in asyn-chronous conversations.
In Proceedings of IJCAI,pages 1807-1813.Shafiq Joty, Giuseppe Carenini, Raymond Ng andYashar Mehdad.
2013.
Combining Intra- and Multi-sentential Rhetorical Parsing for Document-levelDiscourse Analysis.
In Proceedings of ACL.Shafiq Joty, Giuseppe Carenini and Raymond Ng.2013.
Topic Segmentation and Labeling in Asyn-chronous Conversations.
Journal of AI Research,47:521-573.Ching-Sheng Lin, Samira Shaikh, Jennifer Stromer-Galley, Jennifer Crowley, Tomek Strzalkowski,Veena Ravishankar.
2013.
Topical Positioning: ANew Method for Predicting Opinion Changes inConversation.
In Proceedings of LASM, pages 41-48.William C. Mann and Sandra A. Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243-281.Gilad Mishne and Natalie Glance.
2006.
Leave a reply:An analysis of weblog comments.
In Proceedings ofWWW.Amita Misra and Marilyn Walker.
2013.
Topic Inde-pendent Identification of Agreement and Disagree-ment in Social Media Dialogue.
In Proceedings ofSIGDIAL, pages 41-50.Arjun Mukherjee and Bing Liu.
2012.
Modeling reviewcomments.
In Proceedings of ACL, pages 320-329.Akiko Murakami and Rudy Raymond.
2010.
Supportor Oppose?
Classifying Positions in Online De-bates from Reply Activities and Opinion Expres-sions.
In Proceedings of the International Confer-ence on Computational Linguistics, pages 869-875.Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,Deborah Cai, Jennifer Midberry, Yuanxin Wang.2014.
Modeling topic control to detect influencein conversations using nonparametric topic models.Machine Learning 95:381-421.Ana-Maria Popescu and Oren Etzioni.
2005.
Extract-ing Product Features and Opinions from Reviews.In Proceedings of HLT/EMNLP, pages 339-346.Ana-Maria Popescu and Marco Pennacchiotti.
2010.Detecting controversial events from twitter.
In Pro-ceedings of CIKM, pages 1873-1876.Stephan Raaijmakers, Khiet Truong, Theresa Wilson.2008.
Multimodal subjectivity analysis of multipartyconversation.
In Proceedings of EMNLP, pages 466-474.Marta Recasens, Cristian Danescu-Niculescu-Mizil,and Dan Jurafsky.
2013.
Linguistic Models for Ana-lyzing and Detecting Biased Language.
In Proceed-ings of ACL, pages 16501659.Swapna Somasundaran, Josef Ruppenhofer, JanyceWiebe.
2007.
Detecting Arguing and Sentiment inMeetings.
In Proceedings of SIGDIAL Workshop onDiscourse and Dialogue.Swapna Somasundaran and Janyce Wiebe.
2009.
Rec-ognizing stances in online debates.
In Proceedingsof ACL, pages 226-234.Swapna Somasundaran and Janyce Wiebe.
2010.
Rec-ognizing stances in ideological on-line debates.
InProceedings of NAACL, Workshop on Computa-tional Approaches to Analysis and Generation ofEmotion in Text, pages 116124.1179Amanda Stent and James Allen.
2000.
Annotating Ar-gumentation Acts in Spoken Dialog.
Technical Re-port.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, Manfred Stede.
2011.
Lexicon-basedmethods for sentiment analysis.
Journal of Compu-tational Linguistics, 37(2):267-307.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition fromCongressional floor-debate transcripts.
In Proceed-ings of EMNLP, pages 327-335.Wen Wang, Sibel Yaman, Kristen Precoda, ColleenRichey, and Geoffrey Raymond.
2011.
Detection ofagreement and disagreement in broadcast conversa-tions.
In Proceedings of ACL, pages 374-378.Bonnie Webber and Rashmi Prasad.
2008.
Sentence-initial discourse connectives, discourse structure andsemantics.
In Proceedings of the Workshop on For-mal and Experimental Approaches to Discourse Par-ticles and Modal Adverbs.Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris.2012.
Unifying local and global agreement and dis-agreement classification in online debates.
In Pro-ceedings of Computational Approaches to Subjectiv-ity and Sentiment Analysis, pages 61-69.1180
