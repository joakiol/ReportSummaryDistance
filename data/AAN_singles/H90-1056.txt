Poor Estimates of Contextare Worse than NoneWilliam A. GaleKenneth W. ChurchAT&T Bell LaboratoriesMurray Hill, N.J. 07974AbstractIt is difficult to estimate the probability of a word'scontext because of sparse data problems.
If appropriatecare is taken, we find that it is possible to make usefulestimates of contextual probabilities that improve perfor-mance in a spelling correction application.
In contrast,less careful estimates are found to be useless.
Specifi-cally, we will show that the Good-Turing method makesthe use of contextual information practical for a spellingcorrector, while attempts to use the maximum likelihoodestimator (MLE) or expected Idcellhood estimator (ELE)fail.
Spelling correction was selected as an applicationdomain because it is analogous to many important recogni-tion applications based on a noisy channel model (such asspeech recognition), though somewhat simpler and there-fore possibly more amenable to detailed statisticalanalysis.BackgroundStatistical language models were quite popular in thePrevious work [5] led to the spelling correction pro-gram, correct.
In the course of that work, we observedthat human judges were reluctant to decide between alter-native candidate corrections given only as much informa-tion as was available to the program, the typo and the can-didate corrections.
We also observed that the judges feltmuch more confident when they could see a line or two ofcontext around the typo.
This suggests that there is con-siderable information in the context.However, it is difficult to measure contextual probabili-ties.
Suppose, for example, that we consider just the pre-vious word, I.
(A more adequate model would need tolook at considerably more than just the previous word, buteven this radically over-simplified model illustrdtes theproblem.)
Then we need to measure the conditional pro-babilities, Pr(llw), for all w and 1 in the vocabulary V.The problem is that v2 is generally much larger than thesize of the corpus, N. V is at least lo5, so V' is at least10".
The largest currently available corpora are about10' (100 million words).
Thus, we have at least 100times more parameters than data.
In fact, the problem ismuch worse because the data are not uniformly distri-buted.
'1950s, but faded 'ather sudded~ when ChOmsk~ Correct is reviewed in the next as it provides [I] argued quite successfully that statistics should not play the framework which this study is done.
The set-a role in his competence model.
With the recent availabil-also estimation techniques, Nles for iry of large text corpora (of 100 milson words or more), bi.ing sources of evidence, and evaluation pro-there has been a resurgence of interest in empiricalcedures.methods, especially in recognition applications such asspeech recognition (e.g., [2] ), but also in many other areasof natural language research including machine translation Correct ( P I  ).
The sheer size of the available corpus data is The takes a of misspelled words largely responsible for the revival of these techniques.
(typos) as input (as might be produced by the Unix@ spell Nevertheless, there is never enough data, and conse- program), and outputs a set of candidate corrections for quently, it is important to study the statistical estimationeach typo, along with a probability.
These probability issues very carefully.
Specifically, we will show that thescores distinguish correct from other spelling correction Good-Turing (GT) method[4] for estimating bigram proba- programs, that output a (long) list of candidiate bilities makes the use of contextual information ~racticalrin our spelling corrector application, while attempts to usethe maximum likelihood estimator (MLE) or expected 2.
One might think that the sparse data problem could be solved by collecting larger corpora, but ironically, the problem only likelihood estimator (ELE) f d .
gets worse as we look at more data.
The vocabulary is notfixed both N and V grow as we look at more data.
The rate1.
We would like to acknowledge Mark Kernighan's work on of growth is still a matter of debate, but the evidence clearlycorrecf, which laid the groundwork for this study of context shows that V > 0(f i ) ,  and therefore, the sparse data prob-modelling.
We thank Jill Burstein for help with the judging.
lems only get worse as we look at more and more data.corrections, many of which are often extremely implausi-ble.Here is some sample output:negotations negotiationsnotcampaigning 1 ??
?The entry ???
indicates that no correction was found.progessionThe first stage of correct finds candidate corrections, c,that differ from the typo t by a single insertion, deletion,substitution or reversal.
For example, given the inputtypo, acress, the first stage generates candidate correctionsin the table below.
Thus, the correction actress could betransformed by the noisy channel into the typo acress byreplacing the t with nothing, @, at position 2.
(The sym-bols @ and # represent nulls in the typo and correction,respectively.
The transformations are named from thepoint of view of the correction, not the typo.)
This unusu-ally d~fficult example was selected to illustrate the fourtransformations; most typos have just a few possiblecorrections, and there is rarely more than one plausiblecorrection.progression (94%) procession(4%) profession (2%)Typo Correction Transformationacress actress @ t 2 deletionacress cress a # 0 insertionacress caress ac ca 0 reversalacress access r c 2 substitutionacres across e o 3 substitutionacress acres s # 4 insertionacress acres s # 5 insertionEach candidate correction is scored by the Bayesiancombination rule Pr(c) ~ r ( t l  c), and then normalized bythe sum of the scores for all proposed candidates.
Caremust be taken in estimating the prior because of sparsedata problems.
It is possible (and even likely) that a pro-posed correction might not have appeared in the trainingset.
Some methods of estimating the prior would produceundesirable results in this case.
For example, the max-imum likelihood estimate (MLE) would estimatePr(c) = 0, and consequately, many candidate correctionswould be rejected just because they did not happen toappear in the training set.
We will encounter even moresevere forms of the sparse data problem when we considercontext.frequency r*, where r*  is a function of r. Once r*  hasbeen determined, then p is estimated as p = r*lN*.N* = C r* N, where N, is the frequency of frequencyr, assuring that the estimated probabilities add to one.
Themaximum likelihood estimator (MLE) sets r*  = r. TheMLE estimate is particularly poor when r = 0, since thetrue probabilities are almost certainly greater than 0.Following Box and Tiao [6] , we can assume an unin-formative prior and reach a posterior distribution for p.Using the expectation of this distribution amounts to usingr* = r + .5.
We call this the expected likelihood estimate(ELE).
This method is often used in practice because it iseasy to implement, though it does have some seriousweaknesses.
The third method is the minimax (MM)method [7] , which sets r* = r + .
5 f i .
Its derivation isbased on a risk analysis; it minimizes the maximum qua-dratic loss.
The fourth method is the Good-Turing (GT)method [4] , which sets r*  = ( r + l )  N,+,IN,.
Unlikethe MLE, all three other methods assign nonzero probabili-ties, even when r = 0.
This is probably a desirable pro-perty.We use the ELE for the probabilities of single words asthey are frequent enough not to require elaborate treat-ment.
The channel probabilities, Pr( t  1 c), are computedfrom four confusion matrices: (1) del[x,y], the number oftimes that the characters xy (in the correct word) weretyped as x in the training set, (2), add[x,y], the number oftimes that x was typed as xy, (3) sub[x,y], the number oftimes that y was typed as x, and (4) rev[x,y], the numberof times that xy was typed as yx.
Probabilities areestimated from these matrices by using chars[x,y] andchars[x], the number of times that xy and x appeared inthe training set, respectively, as the total number of obser-vations appropriate to some cell of a matrix.
The proba-bilities are estimated using the Good-Turing method [4] ,with the cells of the matrices as the types.Returning to the acress example, the seven proposedtransformations are scored by multipling the prior proba-bility (which is proportial to 0.5 + column 4 in the tablebelow) and the channel probability (column 5) to form araw score (column 3), which are normalized to produceprobabilities (column 2).
The final results is: acres (45%),actress (37%), across (18%), access (O%), caress (O%),cress (0%).
This example is very hard; in fact, the secondchoice is probably right, as can be seen from the context:... was called a "stellar and versatile acress whose combi-nation of sass and ghmour has defined her ....
The pro-gram would need a much better prior model in order tohandle this case.
The next section shows how the contextcan be used to take advantage of the fact that that actressis considerably more plausible than acres as an antecedentWe will consider four estimation methods for dealingwith the sparse data problems.
All of these methodsattempt to estimate a set of probabilities, p, from observedfrequencies, r. It is assumed that the observed frequenciesare generated by a binomial process with N total observa-tions.
The estimation methods generate an adjustedfor whose.c % Raw freq(c) Pr(t Ic)actress 37% .I57 1343 55./470,000cress 0% .OW 0 46./32,000,000caress 0% .OW 4 .95/580,000access 0% .000 2280 .98/4,700,000across 18% .077 8436 93.110,000,000acres 21% .092 2879 417./13,000,000acres 23% .098 2879 205./6,000,000Many typos such as absorbant have just one candidatecorrection, but others such as adusted are more difficultand have multiple corrections.
(For the purposes of thisexperiment, a typo is defined to be a lowercase wordrejected by the UnixO spell program.)
The table belowshows examples of typos with candidate corrections sortedby their scores.
The second column shows the number oftypos in a seven month sample of the AP newswire, bro-ken out by the number of candidate corrections.
Forexample, there were 1562 typos with exactly two correc-tions proposed by correct.
Most typos have relatively fewcandidate corrections.
There is a general trend for fewerchoices, though the 0-choice case is special.# Freq Typo Corrections0 3937 adrnininistration1 6993 absorbant2 1562 adusted3 639 ambitios4 367 compatabili?y5 221 afte6 157 dialy7 94 poice8 82 piots9 77 spashabsorbentadjusted dustedambitious ambitions ambitioncompatibility compactabilitycomparability computabilityafter fate aft ate antedaily diary dials dialdimly dillypolice price voice poisepice ponce poirepilots pivots riots plots pitspots pints pioussplash smash slash spasm stashswash sash pash spasWe decided to look at the 2-candidate case in moredetail in order to test how often the top scoring candidateagreed with a panel of three judges.
The judges weregiven 564 triples (e.g., absurb, absorb, absurd) and a con-cordance line (e.g., ...$?
nancial community.
"It is absurband probably obscene for any person so engaged ,to...).The first word of the triple was a spell reject, followed bytwo candidates in alphabetical order.
The judges weregiven a 5-way forced choice.
They could circle any oneof the three words, if they thought that was what theauthor had intended.
In addition, they could say "other"if they thought that some other word was intended, or "?
"if they were not sure what was intended.
We decided toconsider only those cases where at least two judges circledone of the two candidate corrections, and they agreed witheach other.
This left only 329 triples, mainly because thethe judges often circled the first word, indicating that theythought it had been incorrectly rejected by spell.The following table shows that correct agrees with themajority of the judges in 87% of the 329 cases of interest.In order to help calibrate this result, three inferior methodsare also evaluated.
The channel-only method ignores theprior probability.
The prior-only method ignores the chan-nel probability.
Finally, the neither method ignores bothprobabilities and selects the first candidate in all cases.
Asthe following table shows, correct is significantly betterthan the three alternative methods.
The table also evalu-ates the three judges.
Judges were only scored on triplesfor which they selected one of the proposed altematives,and for which the other two judges agreed on one of theproposed altematives.
A triple was scored "correct" forone judge if that judge agreed with the other two and"incorrect" if that judge disagreed with the other two.The table shows that the judges significantly out-performcorrect, indicating that there is room for improvement.MethodcorrectContextDiscrimination %2861329 87 * 1.9channel-onlyprior-onl ychanceJudge 1Judge 2Judge 3As previously noted, the judges were extremely reluc-tant to cast a vote without more information than correctuses, and they were much more comfortable when theycould see a concordance line or two.
This suggests thatcontextual clues might help improve performance.
How-ever, it is important to estimate the context carefully; wehave found that poor measures of context are worse thannone.2631329 80 f 2.22471329 75 f 2.41721329 52 f 2.827 11273 99 * 0.527 11275 99 f 0.72711281 96 f 1.1In this work, we use a simple n-gram model of context,based on just the word to the left of the typo, I, and theword to the right of the typo, r. Although n-grammethods are much too simple (compared with much moresophisticated methods used in A1 and natural languageprocessing), even these simple methods illustrate the prob-lem that poor estimates of contextual probabilities areworse than none.
The same estimation issues are probablyeven more critical when the simple n-gram models of con-text are replaced by more sophisticated A1 models.The variables 1 and r are introduced into the Baysianscoring function by changing the formula fromPr(c) Pr(t1c) to P r ( c ) ~ r ( t , r c )  which can beapproximated as Pr(c)Pr(t lc)Pr( l lc)Pr(r lc) ,  underappropriate independence assumptions.
The issue, then, ishow to estimate the two new factors: Pr(l lc) and Pr(rlc).We have four proposals: MLE, ELE, MM and GT.
Let usconsider one way of using the ELE method first.
It isstraightforward and similar to our best method, but hope-lessly wrong.Pr(llc ) = Pr(lc)Pr(c)(freq( lc ) + O.5 )/ d l(freq(c)+O.5)/d2freq(Ic)+0.5o?
:freq(c)+0.5where dl = N + V2/2 and d2 = N + V/2.
We can ignorethe constant d2/dl and use the proportion to score candi-date corrections.
Similarly, we use the relationPr(rlc ) o~ (freq(cr)+O.5)/(freq(c)+0.5) for the rightcontext.
When these estimates for Pr(l lc) and Pr(rlc)are substituted in the formula,Pr(c)Pr(t \ [c)Pr( l lc)Pr(r lc) ,  we have:Pr(t\[c) (freq(lc)+0.5) (freq(cr)+0.5)(freq(c)+0.5)E/EThis new formula produces the desired results for theacress example, as illustrated in the following table.
(Thecolumn labeled raw is 106 times the formula E/E, as onlyproportionalities matter.)
Note that actress is now pre-fered over acres mostly because actress whose is morecommon than acres whose (8 to 0).
Presumably thedifference in frequencies reflects the fact that actress is abetter antecedent of whose.
Note also though, that cress isnow considered a plausible rival because of errors intro-duced by the ELE method.
The high score of cress is dueto the fact that it was not observed in the corpus, andtherefore the ELE estimates Pr ( l l c )= Pr ( r l c )= 1,which is clearly biased high.c % Raw freq(c) Pr(tlc ) freq(Ic) freq(cr)actress 69% 1.85 1343 555470,000 2 8cress 27% .719 0 46./32,000,000 0 0caress 3% .091 4 .95/580,000 0 0access  0% .000 2280 .98/4,700,000 2 0across 0% .011  8436 93./10,000,000 0 20acres 0% .003 2879 417./13,000,000 0 0acres 0% .003 2879 205./6,000,000 0 0We will considerThe method justfive methods for estimating Pr(llc).described is called the E\[E method,286because both Pr(lc) and Pr(c) are estimated with the ELEmethod.
The M/E method uses the MLE estimate forPr(lc) and the ELE estimate for Pr(c).
The E methodtakes Pr(lc) proportional to the ELE estimate(freq(lc)+0.5), but the denominator is adjusted so thatXPr(l lc) = 1.
The MM method adjusts the minimaxcsuggestion in \[7\] in the same way.
The G/E method usesthe enhanced Good-Turing (GT) method for Pr(Ic) andthe ELE estimate for Pr(c).Pr(l lc ) _ Pr(Ic) _ freq(lc)+0.5P(c) freq(c)+0.5E/EPr(l lc ) = Pr(lc) _ freq(lc) MiEP(c) freq(c)+0.5Pr(l lc ) _ freq(tc)+0.5freq(c)+V/2EPr(l lc ) : freq(lc)+O.5 f~req(c)freq(c) + O.
5 V~freq(c)MMgr+l ( r+ l )Nrer( l lc)  = G/Efreq(c)+0.5The first two methods are useless, as shown by the perfor-mance of the context alone:Poor Estimates of Context Offer Little or No Helpchance M/E E/Ewrong 164.5 15 169uninformative 0 136 4right 164.5 178 156The other three are better.
The performance of G/E issignificantly better than the other four.Better Estimates of Context ExistE MM G/Ewrong 62 59 45uninformative 0 0 4right 267 270 280For the Good-Turing estimates, we use an enhancedversion of the Good-Turing estimator.
The basic estimatoris applied to subgroups of the bigrams.
The subgroupshave similar values of Npxpy, where Px and py  are theprobabilities for the individual words.
The grouping vari-able is the expected frequency of the bigrarn if the wordsoccurred independently.
Its use is discussed in detail by\[8\] It results in about 1400 significantly different estimatesfor bigrams not seen in the training text, and in about 150different estimates for words seen once.When combined with the prior and channel, G/E is theonly one of the five estimation methods that improves ig-nificantly 3 on the performance of correct.
The followingtable shows correct in column 1, followed by the twodisastrous measures M/E and E/E, then the two uselessmeasures E and MM, and finally the one useful measureG/E.Context is Useless Unless Carefully Measureddisastrous useless usefulwronguselessright%?~nocontext43028686.91.9+M/E +E/Econtext context11 61136 0182 26855.3 81.52.7 2.1+E +MMcontext context39 400 0290 28988.1 87.81.8 1.8+G/Econtext34029589.71.7ConclusionsWe have studied the problem of incorporating contextinto a spelling correction program, and found that the esti-mation issues need to be addressed very carefully.
Poorestimates of context are useless.
It is better to ignore con-text than to model it badly.
Fortunately, there are goodmethods uch as G/E that provide a significant improve-ment in performance.
However, even the G/E methoddoes not achieve human performance, indicating that thereis considerable room for improvement.
One way toimprove performance might be to add more interestingsources of knowledge than simple n-gram models, e.g.,semantic networks, thesaurus relations, morphologicaldecomposition, parse trees.
Alternatively, one might trymore sophisticated statistical approaches.
For example, wehave only considered the simplest Baysian combinationrules.
One might try to fit a log linear model, as one ofmany possibilities.
In short, it should be taken as a chal-lenge to researchers in computational linguistics and statis-tics to find ways to improve performance to be more com-petitive with human judges.Jelinek, F., Mercer, R., and Pietra, P., "A StatisticalApproach to French/English Translation," m Proceed-ings RIA088 Conference on User-oriented Content-based Text and Image Handling, RIAO, Cambridge,Massachusetts (March 21-24, 1988).4.
Good, I. J., "The population frequencies of species andthe estimation of population parameters," Biometrika40 pp.
237-264 (1953).5.
Kernighan, M. D, Church, K. W., and Gale, W. A., "ASpelling Corrector Based on Error Frequencies," inProceedings of the Thirteenth International Conferenceon Computational Linguistics, (1990).6.
Box, G. E. P. and Tiao, G. C., Bayesian Inference inStatistical Analysis, Addison-Wesley, Reading, Mas-sachusetts (1973).7.
Steinhaus, H., "The problem of estimation," Annals ofMathematical Statistics 28 pp.
633-648 (1957).8.
Church, K. W. and Gale, W. A., "Enhanced Good-Turing and Cat-Cal: Two New Methods for EstimatingProbabilities of English Bigrams," Computer, Speech,and Language, (1991).References1.
Chomsky, N., Syntactic Structures, Mouton & Co, TheHague (1957).2.
Nadas, A., "Estimation of probabilities m the languagemodel of the IBM speech recognition system," IEEETransactions on Acoustics, Speech, and Signal Process-ing ASSP-32 pp.
859-861 (1984).3.
Brown, P., Cocke, J., Della Pietra, S., Della Pietra, V.,The GT method changes the program's preference in 25 ofthe 329 cases; 17 of the changes are right and 8 of them arewrong.
The probability of 17 or more right out of 25, assum-ing equal probability of two alternatives, is .04.
Thus, weconclude that the improvement is significant.287
