Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 55?63,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsKSC-PaL: A Peer Learning Agent that Encourages Students to take theInitiative?Cynthia Kersey and Barbara Di EugenioDepartment of Computer ScienceUniversity of Illinois at ChicagoChicago, IL 60607 USAckerse2@uic.edubdieugen@cs.uic.eduPamela Jordan and Sandra KatzLearning Research and Development CenterUniversity of PittsburghPittsburgh, PA 15260 USApjordan+@pitt.edukatz+@pitt.eduAbstractWe present an innovative application of dis-course processing concepts to educationaltechnology.
In our corpus analysis of peerlearning dialogues, we found that initiativeand initiative shifts are indicative of learn-ing, and of learning-conducive episodes.
Weare incorporating this finding in KSC-PaL, thepeer learning agent we have been developing.KSC-PaL will promote learning by encourag-ing shifts in task initiative.1 IntroductionCollaboration in dialogue has long been researchedin computational linguistics (Chu-Carroll and Car-berry, 1998; Constantino-Gonza?lez and Suthers,2000; Jordan and Di Eugenio, 1997; Lochbaum andSidner, 1990; Soller, 2004; Vizca?
?no, 2005), how-ever, the study of peer learning from a computa-tional perspective is still in the early stages.
Thisis an important area of study because peer learninghas been shown to be an effective mode of learn-ing, potentially for all of the participants (Cohen etal., 1982; Brown and Palincsar, 1989; Birtz et al,1989; Rekrut, 1992).
Additionally, while there hasbeen a focus on using natural language for intelli-gent tutoring systems (Evens et al, 1997; Graesseret al, 2004; VanLehn et al, 2002), peer to peer in-teractions are notably different from those of expert-novice pairings, especially with respect to the rich-ness of the problem-solving deliberations and ne-gotiations.
Using natural language in collaborative?This work is funded by NSF grants 0536968 and 0536959.learning could have a profound impact on the wayin which educational applications engage students inlearning.Previous research has suggested several mecha-nisms that explain why peer learning is effective forall participants.
Among them are: self-directed ex-plaining(Chi et al, 1994), other-directed explaining(Ploetzner et al, 1999; Roscoe and Chi, 2007) andKnowledge Co-construction ?
KCC for short (Haus-mann et al, 2004).
KCC episodes are defined asportions of the dialogue in which students are jointlyconstructing a shared meaning of a concept requiredfor problem solving.
This last mechanism is themost interesting from a peer learning perspective be-cause it is a truly collaborative construct and also be-cause it is consistent with the widely accepted con-structivist view of learning.Since KCC is a high-level concept that is not eas-ily recognized by an artificial agent we collectedpeer learning interactions from students and stud-ied them to identify features that might be useful inidentifying KCC.
We found that linguistically basedinitiative shifts seem to capture the notion of col-laborative construction.
A more thorough analysisfound a strong relationship between KCC and initia-tive shifts and moderate correlations between initia-tive shifts and learning.The results of this analysis are being incorporatedinto KSC-PaL, an artificial agent that can collaboratewith a human student via natural-language dialogueand actions within a graphical workspace.
KSC-PaLhas been developed in the last two years.
Dialogue-wise, its core is TuTalk (Jordan et al, 2007), a dia-logue management system that supports natural lan-55guage dialogue in educational applications.
As wewill describe, we have already developed its userinterface and its student model and have extendedTuTalk?s planner to provide KSC-PaL with the abil-ity to induce initiative shifts.
For the version ofKSCPal we will present in this paper, we wanted tofocus on the question of whether this style of inter-action helps learning; and we were concerned thatits limitations in disambiguating the student?s inputcould impact this interaction.
Hence, this round ofexperiments employs a human ?helper?
that is givena list of concepts the input may match, and choosesthe most appropriate one.The work presented in this paper is part of a largerresearch program: we analyze different paradigms ?tutoring dialogues and peer-learning dialogues?
inthe same basic domain, devise computational mod-els for both, and implement them in two separateSW systems, an ITS and the peer-learning systemwe present here.
For our work on the tutoring dia-logue corpus and the ITS please see (Fossati et al,accepted for publication 2009).Our domain in both cases is problem solving inbasic data structure and algorithms, which is part offoundations of Computer Science.
While in recentyears, interest in CS in the US has dropped dramat-ically, CS is of enormous strategic interest, and isprojected to foster vast job growth in the next fewyears (AA.
VV., 2006).
We believe that by support-ing CS education in its core we can have the largestimpact on reversing the trend of students?
disinter-est.
Our belief is grounded in the observation thatthe rate of attrition is highest at the earliest phasesof undergraduate CS curricula.
This is due in partto students?
difficulty with mastering basic concepts(Katz et al, 2003), which require a deep understand-ing of static structures and the dynamic proceduresused to manipulate them (AA.
VV., 2001).
Theseconcepts also require the ability to move seamlesslyamong multiple representations, such as text, pic-tures, pseudo-code, and real code in a specific pro-gramming language.Surprisingly, few educational SW systems ad-dress CS topics, e.g.
teaching a specific program-ming language like LISP (Corbett and Anderson,1990) or database concepts (Mitrovic?
et al, 2004).Additionally, basically they are all ITSs, where therelationship between the system and the studentis one of ?subordination?.
Only two or three ofthese ITSs address foundations, including: Autotu-tor (Graesser et al, 2004) addresses basic literacy,but not data structures or algorithms; ADIS (Waren-dorf and Tan, 1997) tutors on basic data structures,but its emphasis is on visualization, and it appears tohave been more of a proof of concept than a work-ing system; ProPL (Lane and VanLehn, 2003) helpsnovices design their programs, by stressing problemsolving and design skills.In this paper, we will first discuss the collectionand analysis of peer learning interactions.
Then, wediscuss the design of our peer agent, and how it isguided by the results of our analysis.
We concludeby briefly describing the user experiments we areabout to undertake, and whose preliminary resultswill be available at the time of the workshop.2 Data collectionWe have collected peer learning interactions from 15pairs of students solving problems in the domain ofcomputer science data structures.
Students were re-cruited from introductory courses on data structuresand algorithms.
Each problem involved one of threetypes of data structures: linked-lists, stacks and bi-nary search trees.
Each problem was either a debug-ging problem where the students were asked to worktogether to identify errors in the code or an explana-tion problems in which the students jointly createdan explanation of a segment of code.The students interacted using a computer me-diated interface1 where they could communicatevia text-based chat, drawing and making changesto code (see Figure 1).
The graphical workspace(drawing and coding areas) was shared such thatchanges made by one student were propagated tohis/her partner?s workspace.
Access to this graph-ical workspace was controlled so that only one stu-dent was allowed to draw or make changes to codeat any point in time.Each pair was presented with a total of 5 prob-lems, although not all pairs completed all prob-lems due to time limitations.
The interactions foreach pair were subdivided into separate dialogues1Using text to communicate versus face-to-face interactionsshould be comfortable for most students given the prevalenceof communication methods such as text messaging and instantmessengers.56Figure 1: The data collection / KSC-PaL interfacefor each problem.
Thus, we collected a corpus con-sisting of a total of 73 dialogues.In addition to collecting problem solving data,we also presented each student with a pre-test priorto problem solving and an identical post-test at theconclusion of problem solving in order to measurelearning gains.
A paired t-test of pre- and post-testscores showed that students did learn during collab-orative problem solving (t(30)=2.83; p=0.007).
Theinteractions produced an average normalized learn-ing gain of 17.5 (possible total points are 50).3 Analysis of Peer Learning InteractionsNext, we undertook an extensive analysis of the cor-pus of peer learning interactions in order to deter-mine the behaviors with which to endow KSC-PaL.3.1 Initiative: AnnotationGiven the definition of KCC, it appeared to us thatthe concept of initiative from discourse and dialogueprocessing should play a role: intuitively, if the stu-dents are jointly contructing a concept, the initiativecannot reside only with one, otherwise the partnerwould just be passive.
Hence, we annotated the dia-logues for both KCC and initiative.The KCC annotation involved coding the dia-logues for KCC episodes.
These are defined as aseries of utterances and graphical actions in whichstudents are jointly constructing a shared meaning ofa concept required for problem solving (Hausmannet al, 2004).
Using this definition, an outside anno-tator and one of the authors coded 30 dialogues (ap-proximately 46% of the corpus) for KCC episodes.This entailed marking the beginning utterance andthe end utterance of such episodes, under the as-sumption that all intervening utterances do belong tothe same KCC episode (otherwise the coder wouldmark an earlier end for the episode).
The result-ing intercoder reliability, measured with the Kappastatistic(Carletta, 1996), is considered excellent (?
=0.80).Our annotation of initiative was two fold.
Sincethere is disagreement in the computational lin-guistics community as to the precise definition of57initiative(Chu-Carroll and Carberry, 1998; Jordanand Di Eugenio, 1997), we annotated the dialoguesfor both dialogue initiative, which tracks who isleading the conversation and determining the cur-rent conversational focus, and task initiative, whichtracks the lead in problem solving.For dialogue initiative annotation, we used thewell-known utterance-based rules for allocation ofcontrol from (Walker and Whittaker, 1990).
Inthis scheme, each utterance is tagged with one offour dialogue acts (assertion, command, question orprompt) and control is then allocated based on a setof rules.
The dialogue act annotation was done au-tomatically, by marking turns that end in a questionmark as questions, those that start with a verb ascommands, prompts from a list of commonly usedprompts (e.g.
ok, yeah) and the remaining turns asassertions.
To verify that the automatic annotationwas good, we manually annotated a sizable portionof the dialogues with those four dialogue acts.
Wethen compared the automatic annotation against thehuman gold standard, and we found an excellent ac-curacy: it ranged from 86% for assertions and ques-tions, to 97% for prompts, to 100% for commands.Once the dialogue acts had been automatically an-notated, two coders, one of the authors and an out-side annotator, coded 24 dialogues (1449 utterances,approximately 45% of the corpus) for dialogue ini-tiative, by using the four control rules from (Walkerand Whittaker, 1990):1.
Assertion: Control is allocated to the speakerunless it is a response to a question.2.
Command: Control is allocated to the speaker.3.
Question: Control is allocated to the speaker,unless it is a response to a question or a com-mand.4.
Prompt: Control is allocated to the hearer.The resulting intercoder reliability on dialogue ini-tiative was 0.77, a quite acceptable level of agree-ment.
We then experimented with automatically an-notating dialogue initiative according to those con-trol rules.
Since the accuracy against the gold stan-dard was 82%, the remaining 55% of the corpus wasalso automatically annotated for dialogue initiative,using those four control rules.As concerns task initiative, we define it as any ac-tion by a participant to either achieve a goal directly,decompose a goal or reformulate a goal (Guinn,1998; Chu-Carroll and Brown, 1998).
Actions inour domain that show task initiative include:?
Explaining what a section of code does.?
Identifying that a section of code as correct orincorrect.?
Suggesting a correction to a section of code?
Making a correction to a section of code priorto discussion with the other participant.The same two coders annotated for task initiativethe same portion of the corpus already annotated fordialogue initiative.
The resulting intercoder reliabil-ity for task initiative is 0.68, which is high enoughto support tentative conclusions.
The outside coderthen manually coded the remaining 55% of the cor-pus for task initiative.3.2 KCC, initiative and learningIn analyzing the annotated dialogues, we used mul-tiple linear regression to identify correlations of theannotated features and post-test score.
We used pre-test score as a covariate because of its significantpositive correlations with post-test score.
Due tovariations in student ability in the different problemtypes, our analysis focused only on a portion of thecollected interactions.
In the tree problem there wasa wide variation in experience level of the studentswhich would inhibit KCC.
In the stack problem, thestudents had a better understanding of stacks priorto problem solving and spent less time in discussionand problem solving.
Thus, our analysis focusedonly on the linked-list problems.We started by analyzing the relationship betweenKCC and learning.
As a measurement of KCC weused KCC actions which is the number of utter-ances and graphical actions that occur during KCCepisodes.
This analysis showed that KCC does havea positive correlation with learning in our corpus.
InTable 1, the first row shows the benefit for the dyadoverall by correlating the mean post-test score withthe mean pre-test score and the dyad?s KCC actions.The second row shows the benefit for individuals by58correlating individual post-test scores with individ-ual pre-test scores and the dyad?s KCC actions.
Thedifference in the strength of these correlations sug-gests that members of the dyads are not benefittingequally from KCC.
If the subjects are divided intotwo groups, those with a pre-test score below themean score ( n=14) and those with a pre-test scoreabove the mean score ( n=16) , it can be seen thatthose with a low pre-test score benefit more fromthe KCC episodes than do those with a high pre-testscore (rows 3 and 4 in Table 1).KCC actions predict ?
R2 pMean post-test score 0.43 0.14 0.02Individual post-test score 0.33 0.08 0.03Individual post-test score 0.61 0.37 0.03(low pre-test subjects)Individual post-test score 0.33 0.09 ns(high pre-test subjects)Table 1: KCC Actions as Predictor of Post-test ScoreNext, we explored the relationship between learn-ing and the number of times initiative shifted be-tween the students.
Intuitively, we assumed that fre-quent shifts of initiative would reflect students work-ing together to solve the problem.
We found therewas a significant correlation between post-test score(after removing the effects of pre-test scores) and thenumber of shifts in dialogue initiative and the num-ber of shifts in task initiative (see Table 2).
Thisanalysis excluded two dyads whose problem solvingcollaboration had gone awry.Predictor of Post-test ?
R2 pDialogue initiative shifts 0.45 0.20 0.00Task initiative shifts 0.42 0.20 0.01Table 2: Initiative Predictors of Post-test ScoreWe then computed a second measure of KCC thatis meant to reflect the density of the KCC episodes.KCC initiative shifts is the number of task initiativeshifts that occur during KCC episodes.
Many taskinitiative shifts reflect more active KCC.Table 3 uses KCC initiative shifts as the measureof co-construction.
It shows similar results to ta-ble 1, where KCC actions was used.
Note that whenthe outlier dyads were removed the correlation withlearning is much stronger for the low pre-test scoresubjects when KCC initiative shifts are used as themeasure of KCC (R2 = 0.45, p = 0.02) than whenKCC actions are used.KCC initiative shifts predict ?
R2 pMean post-test score 0.46 0.15 0.01Individual post-test score 0.35 0.09 0.02Individual post-test score 0.67 0.45 0.02(low pre-test subjects)Individual post-test score 0.10 0.01 ns(high pre-test subjects)Table 3: KCC Initiative Shifts Predictors of Post-testScoreLastly we investigated the hypothesis that KCCepisodes involve frequent shifts in initiative, as bothparticipants are actively participating in problemsolving.
To test this hypothesis, we calculatedthe average initiative shifts per line during KCCepisodes and the average initiative shifts per lineduring problem solving outside of KCC episodes foreach dyad.
A paired t-test was then used to verifythat there is a difference between the two groups.The t-test showed no significant difference in aver-age dialogue initiative shifts in KCC episodes com-pared with non-KCC problem solving.
However,there is a significant difference between average taskinitiative shifts in KCC episodes compared with therest of the dialogue ( t(57) = 3.32, p = 0.0016).
Theeffect difference between the two groups (effect size= 0.65 ) shows that there is a meaningful increase inthe number of task initiative shifts in KCC episodescompared with problem solving activity outside ofthe KCC episodes.3.3 Indicators of task initiative shiftsSince our results show that task initiative shifts areconducive to learning, we want to endow our soft-ware agent with the ability to encourage a shift ininitiative from the agent to the student, when thestudent is overly passive.
The question is, what arenatural indicators in dialogue that the partner shouldtake the initiative?
We explored two different meth-ods for encouraging initiative shifts.
One is that stu-dent uncertainty may lead to a shift in initiative.
Theother consists of cues for initiative shifts identified59in related literature(Chu-Carroll and Brown, 1998;Walker and Whittaker, 1990).Intuitively, uncertainty by a peer might lead to hispartner taking the initiative.
One possible identi-fier of student uncertainty is hedging.
To validatethis hypothesis, we annotated utterances in the cor-pus with hedging categories as identified in (Bhattet al, 2004).
Using these categories we were unableto reliably annotate for hedging.
But, after collaps-ing the categories into a single binary value of hedg-ing/not hedging we arrived at an acceptable agree-ment (?
= 0.71).Another identifier of uncertainty is a student?s re-quest for feedback from his partner.
When uncertainof his contribution, a student may request an evalua-tion from his peer.
So, we annotated utterances with?request for feedback?
and were able to arrive at anexcellent level of intercoder reliability (?
= 0.82).
(Chu-Carroll and Brown, 1998) identifies cuesthat may contribute to the shift of task and dialogueinitiative.
Since task initiative shifts appear to iden-tify KCC episodes, we chose to explore the follow-ing cues that potentially result in the shift of taskinitiative.?
Give up task.
These are utterances wherethe student explicitly gives up the task usingphrases like ?Any other ideas??.?
Pause.
A pause may suggest that the speakerhas nothing more to say in the current turn andintends to give up his initiative.?
Prompts.
A prompt is an utterance that has nopropositional content.?
Invalid statements.
These are incorrect state-ments made by a student.Using hedging, request for feedback and initia-tive cues, we were able to identify 283 shifts in taskinitiative or approximately 67% of all task initiativeshifts in the corpus.
The remaining shifts were likelyan explicit take over of initiative without a precedingpredictor.Since we found several possible ways to predictand encourage initiative shifts, the next step was toidentify which of these predictors more often re-sulted in an initiative shift; and, for which predic-tors the resulting initiative shift more often led to anincrease in the student?s knowledge level.
Table 4shows the percentage of instances of each predictorthat resulted in an initiative shift.Percent of instances thatCue/Identifier led to initiative shiftHedge 23.94%Request feedback 21.88%Give-up task 20.00%Pause 25.27%Prompt 29.29%Invalid statement 38.64%Table 4: Cues for Shifts in InitiativeAlong with the likelihood of a predictor leadingto an initiative shift, we also examined the impactof a shift of task initiative on a student?s level ofknowledge, measured using knowledge score, cal-culated on the basis of the student model (see Sec-tion 4).
This is an important characteristic since wewant to encourage initiative shifts in an effort to in-crease learning.
First, we analyzed initiative shiftsto determine if they resulted in an increase in knowl-edge score.
We found that in our corpus, an initiativeshift leads to an increase in a student?s knowledgelevel in 37.0% of task initiative shifts, a decreasein knowledge level in 5.2% of shifts and unchangedin 57.8% of shifts.
Even though over one-half ofthe time knowledge scores were not impacted, inonly a small minority of instances did a shift havea negative impact on a student?s level of knowledge.Therefore, we more closely examined the predictorsto see which more frequently led to an increase instudent knowledge.
The results of that analysis isshow in table 5.Percent of shifts wherePredictor knowledge level increasedHedge 23.52%Request feedback 17.65%Give-up task 0.00%Prompt 32.93%Pause 14.22%Invalid statement 23.53%Table 5: Task Initiative Shifts/Knowledge Level Change604 KSC-PaL, a software peerOur peer-learning agent, KSC-PaL, has at its corethe TuTalk System(Jordan et al, 2007), a dialoguemanagement system that supports natural languagedialogue in educational applications.
Since TuTalkdoes not include an interface or a student model, wedeveloped both in previous years.
We also needed toextend the TuTalk planner to recognize and promoteinitiative shifts.The user interface is structured similarly to theone used in data collection(see Figure 1).
How-ever, we added additional features to allow a stu-dent to effectively communicate with the KSC-PaL.First, all drawing and coding actions of the studentare interpreted and passed to the agent as a naturallanguage utterance.
Graphical actions are matchedto a set of known actions and when a student sig-nals that he/she has finished drawing or coding ei-ther by ceding control of the graphical workspace orby starting to communicate through typed text, theinterface will attempt to match what the student hasdrawn or coded with its database of known graphi-cal actions.
These graphical actions include not onlycorrect ones but also anticipated misconceptions thatwere collected from the data collection interactions.The second enhancement to the interface is a spellcorrector for ?chat slang?.
We found in the corpus,that students often used abbreviations that are com-mon to text messaging.
These abbreviations are notrecognized by the English language spell correctorin the TuTalk system, so a chat slang interpretationmodule was added.KSC-PaL requires a student model to track thecurrent state of problem solving as well as esti-mate the student?s knowledge of concepts involvedin solving the problem in order to guide its behav-ior.
Our student model incorporates problem solu-tion graphs (Conati et al, 2002).
Solution graphsare Bayesian networks where each node representseither an action required to solve the problem, aconcept required as part of problem solving or ananticipated misconception.
A user?s utterances andactions are then matched to these nodes.
A knowl-edge score can be calculated at any point in time bytaking a sum of the probabilities of all nodes in thegraph, except the misconception nodes.
The sum ofthe probabilities of the misconception nodes are sub-tracted from the total to arrive at a knowledge score.This score is then normalized by dividing it by themaximum possible knowledge score for the solutiongraph.4.1 KSC-PaL and initiativeSince our corpus study showed that the level of taskinitiative can be used to identify when KCC andpotentially learning is occurring, we have endowedKSC-PaL with behaviors to manipulate shifts in taskinitiative in order to encourage KCC and learning.This required three enhancements: first, the abilityto recognize the initiative holder in each utteranceor action; second, the ability to encourage the shiftof initiative from the agent to the student; and three,extending the TuTalk planner so that it can processtask initiative shifts.As concerns the first step, that the agent recog-nize the initiative holder in each utterance or action,we resorted to machine learning.
Using the WekaToolkit(Witten and Frank, 2005), we explored var-ious machine learning algorithms and feature setsthat could reliably identify the holder of task initia-tive.
We found that the relevant features of an ac-tion in the graphical workspace were substantiallydifferent from those of a natural language utterance.Therefore, we trained and tested separate classifiersfor each type of student action.
After examining awide variety of machine learning algorithms we se-lected the following two classifiers: (1) K* (Clearyand Trigg, 1995), a clustering algorithm, for clas-sifying natural language utterances which correctlyclassified 71.7699% of utterance and (2) JRip (Co-hen, 1995), a rule-based algorithm, for classifyingdrawing and coding actions which correctly classi-fied 86.971% of the instances.As concerns the second step, encouraging initia-tive shifts so that the student assumes the task initia-tive, we use the results of our analysis of the indica-tors of task initiative shifts from Section 3.3.
KSC-PaL will use prompts, request feedback and makeinvalid statements in order to encourage initiativeshifts and promote learning.Finally, we augmented the TuTalk planner so thatit selects scripts to manage task initiative shifts.
Twofactors will determine whether a script that encour-ages initiative shifts will be selected: the currentlevel of initiative shifts and the change in the stu-61dent?s knowledge score.
Task initiative shifts will betracked using the classifier described above.
Scriptswill be selected to encourage initiative shifts whenthe average level of initiative shifts is less than themean initiative shifts in KCC episodes (calculatedfrom the corpus data) and the student?s knowledgelevel has not increased since the last time a scriptselection was requested.
The scripts are based onthe analysis of methods for encouraging initiativeshifts described above.
Specifically, KSC-PaL willencourage initiative shifts by responding to studentinput using prompts, requesting feedback from thestudent and encouraging student criticism by inten-tionally making errors in problem solving.We are now poised to run user experiments.
Wewill run subjects in two conditions with KSC-PaL:in the first condition (control), KSC-PaL will not en-courage task initiative shifts and act more as a tutor;in the second condition, KSC-PaL will encouragetask initiative shifts as we just discussed.
One finalnote: because we do not want our experiments to beaffected by the inability of the agent to interpret anutterance, given current NLU technology, the inter-face will ?incorporate?
a human interpreter.
The in-terpreter will receive student utterances along with alist of possible matching concepts from TuTalk.
Theinterpreter will select the most likely matching con-cept, thus assisting TuTalk in natural language in-terpretation.
Note that the interpreter has a limited,predetermined sets of choices, corresponding to theconcepts TuTalk knows about.
In this way, his / herintervention is circumscribed.5 ConclusionsAfter an extensive analysis of peer-learning interac-tions, we have found that task initiative shifts canbe used to determine when students are engagedin knowledge co-construction.
We have embed-ded this finding in a peer-learning agent, KSC-PaL,that varies its behavior to encourage initiative shiftsand knowledge co-construction in order to promotelearning.
We are poised to run our user experiments,and we will have preliminary results available by theworkshop time.ReferencesAA.
VV.
2001.
Computer Science, Final Report, TheJoint Task Force on Computing Curricula.
IEEE Com-puter Society and Association for Computing Machin-ery, IEEE Computer Society.AA.
VV.
2006.
US bureau of labor statisticshttp://www.bls.gov/oco/oco20016.htm.Khelan Bhatt, Martha Evens, and Shlomo Argamon.2004.
Hedged responses and expressions of affect inhuman/human and human computer tutorial interac-tions.
In Proceedings Cognitive Science.M.
W. Birtz, J. Dixon, and T. F. McLaughlin.
1989.
Theeffects of peer tutoring on mathematics performance:A recent review.
B. C. Journal of Special Education,13(1):17?33.A.
L. Brown and A. S. Palincsar, 1989.
Guided, cooper-ative learning and individual knowledge acquisition,pages 307?226.
Lawrence Erlbaum Associates, Hills-dale, NJ.Jean Carletta.
1996.
Assessing agreement on classifi-cation tasks: the kappa statistic.
Comput.
Linguist.,22(2):249?254.M.T.H.
Chi, N. De Leeuw, M.H.
Chiu, and C. LaVancher.1994.
Eliciting self-explanations improves under-standing.
Cognitive Science, 18(3):439?477.Jennifer Chu-Carroll and Michael K. Brown.
1998.
Anevidential model for tracking initiative in collabora-tive dialogue interactions.
User Modeling and User-Adapted Interaction, 8(3?4):215?253, September.Jennifer Chu-Carroll and Sandra Carberry.
1998.
Col-laborative response generation in planning dialogues.Computational Linguistics, 24(3):355?400.John G. Cleary and Leonard E. Trigg.
1995.
K*: Aninstance-based learner using an entropic distance mea-sure.
In Proc.
of the 12th International Conference onMachine Learning, pages 108?114.P.A.
Cohen, J.A.
Kulik, and C.C.
Kulik.
1982.
Educa-tion outcomes of tutoring: A meta-analysis of findings.American Education Research Journal, 19(2):237?248.William W. Cohen.
1995.
Fast effective rule induction.In Machine Learning: Proceedings of the Twelve In-ternational Conference.Cristina Conati, Abigail Gertner, and Kurt Vanlehn.2002.
Using bayesian networks to manage uncer-tainty in student modeling.
User Modeling and User-Adapted Interaction, 12(4):371?417.Mar?
?a de los Angeles Constantino-Gonza?lez andDaniel D. Suthers.
2000.
A coached collaborativelearning environment for entity-relationship modeling.Intelligent Tutoring Systems, pages 324?333.Albert T. Corbett and John R. Anderson.
1990.
The ef-fect of feedback control on learning to program withthe LISP tutor.
In Proceedings of the Twelfth AnnualConference of the Cognitive Science Society, pages796?803.62Martha W. Evens, Ru-Charn Chang, Yoon Hee Lee,Leem Seop Shim, Chong Woo Woo, Yuemei Zhang,Joel A. Michael, and Allen A. Rovick.
1997.
Circsim-tutor: an intelligent tutoring system using natural lan-guage dialogue.
In Proceedings of the fifth conferenceon Applied natural language processing, pages 13?14,San Francisco, CA, USA.
Morgan Kaufmann Publish-ers Inc.Davide Fossati, Barbara Di Eugenio, Christopher Brown,Stellan Ohlsson, David Cosejo, and Lin Chen.
ac-cepted for publication, 2009.
Supporting ComputerScience curriculum: Exploring and learning linkedlists with iList.
EEE Transactions on Learning Tech-nologies, Special Issue on Real-World Applications ofIntelligent Tutoring Systems.Arthur C. Graesser, Shulan Lu, George Tanner Jackson,Heather Hite Mitchell, Mathew Ventura, Andrew Ol-ney, and Max M. Louwerse.
2004.
Autotutor: A tutorwith dialogue in natural language.
Behavior ResearchMethods, Instruments, & Computers, 36:180?192(13),May.Curry I. Guinn.
1998.
An analysis of initiative selectionin collaborative task-oriented discourse.
User Model-ing and User-Adapted Interaction, 8(3-4):255?314.Robert G.M.
Hausmann, Michelene T.H.
Chi, and Mar-guerite Roy.
2004.
Learning from collaborative prob-lem solving: An analysis of three hypothesized mech-anisms.
In K.D Forbus, D. Gentner, and T. Regier, edi-tors, 26th Annual Conference of the Cognitive ScienceSociety, pages 547?552, Mahwah, NJ.Pamela W. Jordan and Barbara Di Eugenio.
1997.
Con-trol and initiative in collaborative problem solving di-alogues.
In Working Notes of the AAAI Spring Sympo-sium on Computational Models for Mixed Initiative,pages 81?84, Menlo Park, CA.Pamela W Jordan, Brian Hall, Michael A. Ringenberg,Yui Cue, and Carolyn Penstein Rose?.
2007.
Tools forauthoring a dialogue agent that participates in learningstudies.
In Artificial Intelligence in Education, AIED2007, pages 43?50.S.
Katz, J. Aronis, D. Allbritton, C. Wilson, and M.L.Soffa.
2003.
Gender and race in predicting achieve-ment in computer science.
Technology and SocietyMagazine, IEEE, 22(3):20?27.H.
Chad Lane and Kurt VanLehn.
2003.
Coached pro-gram planning: dialogue-based support for novice pro-gram design.
SIGCSE Bull., 35(1):148?152.Karen E. Lochbaum and Candace L Sidner.
1990.
Mod-els of plans to support communication: An initial re-port.
In Proceedings of the Eighth National Confer-ence on Artificial Intelligence, pages 485?490.
AAAIPress.A.
Mitrovic?, P. Suraweera, B. Martin, and A. Weeras-inghe.
2004.
DB-Suite: Experiences with Three In-telligent, Web-Based Database Tutors.
Journal of In-teractive Learning Research, 15(4):409?433.R.
Ploetzner, P. Dillenbourg, M. Preier, and D. Traum.1999.
Learning by explaining to oneself and to others.Collaborative learning: Cognitive and computationalapproaches, pages 103?121.M.
D. Rekrut.
1992.
Teaching to learn: Cross-age tutor-ing to enhance strategy instruction.
American Educa-tion Research Association.Rod D. Roscoe and Michelene T. H. Chi.
2007.Understanding tutor learning: Knowledge-buildingand knowledge-telling in peer tutors?
explanationsand questions.
Review of Educational Research,77(4):534?574.Amy Soller.
2004.
Computational modeling and analysisof knowledge sharing in collaborative distance learn-ing.
User Modeling and User-Adapted Interaction,Volume 14(4):351?381, January.Kurt VanLehn, Pamela W. Jordan, Carolyn PensteinRose?, Dumisizwe Bhembe, Michael Bo?ttner, AndyGaydos, Maxim Makatchev, Umarani Pappuswamy,Michael A. Ringenberg, Antonio Roque, StephanieSiler, and Ramesh Srivastava.
2002.
The architec-ture of why2-atlas: A coach for qualitative physics es-say writing.
In ITS ?02: Proceedings of the 6th Inter-national Conference on Intelligent Tutoring Systems,pages 158?167, London, UK.
Springer-Verlag.Aurora Vizca??no.
2005.
A simulated student can im-prove collaborative learning.
International Journal ofArtificial Intelligence in Education, 15(1):3?40.Marilyn Walker and Steve Whittaker.
1990.
Mixed ini-tiative in dialogue: an investigation into discourse seg-mentation.
In Proceedings of the 28th annual meetingon Association for Computational Linguistics, pages70?78, Morristown, NJ, USA.
Association for Com-putational Linguistics.Kai Warendorf and Colin Tan.
1997.
Adis-an animateddata structure intelligent tutoring system or putting aninteractive tutor on the www.
In Intelligent Educa-tional Systems on the World Wide Web (Workshop Pro-ceedings), at the Eight International Conference onArtficial Intellignece in Education.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical machine learning tools and techniques.
MorganKaufmann, San Francisco.63
