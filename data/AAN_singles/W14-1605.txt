Proceedings of the Eighteenth Conference on Computational Language Learning, pages 39?48,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsLearning to Rank Answer Candidatesfor Automatic Resolution of Crossword PuzzlesGianni BarlacchiUniversity of Trento38123 Povo (TN), Italygianni.barlacchi@gmail.comMassimo Nicosia and Alessandro MoschittiQatar Computing Research Institute5825 Doha, Qatarm.nicosia@gmail.com, amoschitti@qf.org.qaAbstractIn this paper, we study the impact of rela-tional and syntactic representations for aninteresting and challenging task: the au-tomatic resolution of crossword puzzles.Automatic solvers are typically based ontwo answer retrieval modules: (i) a websearch engine, e.g., Google, Bing, etc.
and(ii) a database (DB) system for access-ing previously resolved crossword puz-zles.
We show that learning to rank modelsbased on relational syntactic structures de-fined between the clues and the answer canimprove both modules above.
In particu-lar, our approach accesses the DB usinga search engine and reranks its output bymodeling paraphrasing.
This improves onthe MRR of previous system up to 53% inranking answer candidates and greatly im-pacts on the resolution accuracy of cross-word puzzles up to 15%.1 IntroductionCrossword puzzles (CPs) are probably the mostpopular language games played around the world.It is very challenging for human intelligence as itrequires high level of general knowledge, logicalthinking, intuition and the ability to deal with am-biguities and puns.
CPs normally have the formof a square or rectangular grid of white and blackshaded squares.
The white squares on the borderof the grid or adjacent to the black ones are associ-ated with clues.
The goal of the game is to fill thesequences of white squares with words answeringthe clues.There have been many attempts to build auto-matic CP solving systems, which have also par-ticipated in competitions such as The AmericanCrossword Puzzle Tournament (ACPT).
This isthe oldest and largest CP tournament for cross-word experts held in the United States.
The goalof such systems is to outperform human playersin solving crosswords more accurately and in lesstime.Automatic CP solvers have been mainly tar-geted by the artificial intelligence (AI) community,who has mostly focused on AI techniques for fill-ing the puzzle grid, given a set of answer candi-dates for each clue.
The basic idea is to optimizethe overall probability of correctly filling the entiregrid by exploiting the likelihood of each candidateanswer, fulfilling at the same time the grid con-straints.
After several failures in approaching thehuman expert performance, it has become clearthat designing more accurate solvers would nothave provided a winning system.
In contrast, thePrecision and Recall of the answer candidates areobviously a key factor: a very high value for bothof them would enable the solver to quickly find thecorrect solution.This basically suggests that, similarly to theJeopardy!
challenge case (Ferrucci et al., 2010b),the solution relies on Question Answering (QA)research.
However, although some CP clues arerather similar to standard questions, as for ex-ample, in the clue/answer pair:What keeps acamera rolling?
: dolly, some specific differenceshold: (i) clues can be in interrogative form or not,e.g.,Capital of USA: Washington; (ii) they cancontain riddles or be deliberately ambiguous andmisleading (e.g.,It?s green at first: orange);(iii) the exact length of the answer keyword isknown in advance; and (vi) the confidence in theanswers is an extremely important input for the CPsolver.In this paper, we study methods for improvingthe quality of automatic extraction of answer can-didate lists for automatic CP resolution.
For thispurpose, we designed learning to rank models forreordering the answers produced with two differ-ent techniques typically used in CP systems: (i)searching the Web with clue representations, e.g.,39exploiting Bing search engine1; and (ii) queryingthe DB of previously resolved CP clues, e.g., usingstandard SQL techniques.We rerank the text snippets returned by Bing bymeans of SVM preference ranking (Herbrich et al.,2000) for improving the first technique.
One in-teresting contribution is that our model exploits asyntactic representation of clues to improve Websearch.
More in detail, we use structural kernels(e.g., see (Moschitti, 2006; Moschitti, 2008)) inSVMs applied to our syntactic representation ofpairs, formed by clues with their candidate snip-pets.
Regarding the DB approach, we provide acompletely novel solution by substituting it andthe SQL function with a search engine for retriev-ing clues similar to the target one.
Then, we rerankthe retrieved clues by applying SVMs and struc-tural kernels to the syntactic representation of cluepairs.
This way, SVMs learn to choose the bestcandidate among similar clues that are available inthe DB.
The syntactic representation captures clueparaphrasing properties.In order to carry out our study, we created twodifferent corpora, one for each task: (i) a snip-pets reranking dataset and (ii) a clue similaritydataset.
The first includes 21,000 clues, each asso-ciated with 150 candidate snippets whereas the lat-ter comprises 794,190 clues.
These datasets con-stitute interesting resources that we made availableto the research community2.We compare our methods with one of the bestsystems for automatic CP resolution, WebCrow(Ernandes et al., 2005).
Such system does usethe two approaches mentioned before.
Regardingsnippet reranking, our structural models improveon the basic approach of WebCrow based on Bingby more than 4 absolute percent points in MRR,for a relative improvement of 23%.
Concerningthe similar clues retrieval, our methods improveon the one used by WebCrow, based on DBs, by25% absolute, i.e., about 53% of error reductionwhereas the answer accuracy at first position im-proves up to 70%.Given such promising results, we used our cluereranking method in WebCrow, and obtained anaverage improvement of 15% in resolving com-plete CPs.
This demonstrates that advanced QAmethods such as those based on syntactic struc-tures and learning to rank methods can help to win1https://www.bing.com/2http://projects.disi.unitn.it/iKernels/projects/webcrow/the CP resolution challenge.In the reminder of this paper, Sec.
2 introducesthe automatic CP resolution task in the contextof the related work, Sec.
3 introduces WebCrow,Sec.
4 illustrates our models for snippets rerank-ing and similar clue retrieval using kernel meth-ods, syntactic structures, and traditional featurevectors, Sec.
5 describes our experiments, and fi-nally, Sec.
6 derives the conclusions.2 Related WorkProverb (Littman et al., 2002) was the first sys-tem for the automatic resolution of CPs.
It in-cludes several modules for generating lists of can-didate answers.
These lists are merged and used tosolve a Probabilistic-Constraint Satisfaction Prob-lem.
Proverb relies on a very large crossworddatabase as well as several expert modules, each ofthem mainly based on domain-specific databases(e.g., movies, writers and geography).
In addition,it employs generic-word list generators and clue-specific modules to find solutions for particularkinds of clues likeTel (4): aviv.
Proverb?smodules use many knowledge sources: databasesof clues, encyclopedias and Web documents.
Dur-ing the 1998 ACPT, Proverb placed 109th out of251 contestants.WebCrow (Ernandes et al., 2005) is based onProverb.
It incorporates additional knowledgesources, provides a solver for the Italian languageand improves the clues retrieval model from DB.In particular, it enables partial matching to re-trieve clues that do not perfectly overlap with thequery.
WebCrow carries out basic linguistic anal-ysis such as Part-Of-Speech tagging and lemma-tization.
It takes advantage of semantic relationscontained in WordNet, dictionaries and gazetteers.Its Web module is constituted by a search en-gine, which can retrieve text snippets or docu-ments related to the clue.
Answer candidatesand their confidence scores are generated fromthis content.
WebCrow uses a WA* algorithm(Pohl, 1970) for Probabilistic-Constraint Satisfac-tion Problems, adapted for CP resolution.
Thesolver fills the grid entries for which no solutionwas found by the previous modules.
It tries com-binations of letters that satisfy the crossword con-straints, where the letters are derived from wordsfound in dictionaries or in the generated candidatelists.
WebCrow participated in international com-petitions with good results.40Figure 1: Overview of WebCrow?s architecture.Dr.
Fill (Ginsberg, 2011) targets the crosswordfilling task with a Weighted-Constraint Satisfac-tion Problem.
Constraint violations are weightedand can be tolerated.
It heavily relies on hugedatabases of clues.
It was placed 92nd out of morethan 600 opponents in the 2013 ACPT.Specifically for QA using syntactic structures,a referring work for our research is the IBM Wat-son system (Ferrucci et al., 2010a).
This is an ad-vanced QA pipeline based on deep linguistic pro-cessing and semantic resources.
It demonstratedthat automatic methods can be more accurate thanhuman experts in answering complex questions.More traditional studies on passage reranking,exploiting structural information, were carried outin (Katz and Lin, 2003), whereas other meth-ods explored soft matching (i.e., lexical similarity)based on answer and named entity types (Aktolgaet al., 2011).
(Radlinski and Joachims, 2006; Jeonet al., 2005) applied question and answer classi-fiers for passage reranking.
In this context, sev-eral approaches focused on reranking the answersto definition/description questions, e.g., (Shen andLapata, 2007; Moschitti et al., 2007; Surdeanu etal., 2008; Severyn and Moschitti, 2012; Severynet al., 2013b).3 WebCrow ArchitectureOur research focuses on the generation of accurateanswer candidate lists, which, when used in a CPresolution systems, can improve the overall solu-tion accuracy.
Therefore, the quality of our mod-ules can be assessed by testing them within suchsystems.
For this purpose, we selected WebCrowas it is rather modular, accurate and it was kindlymade available by the authors.
Its architecture isillustrated in Figure 1.The solving process is divided in two phases:in the first phase, the coordinator module forwardsthe clues of an input CP to a set of modules forthe generation of several candidate answer lists.Each module returns a list of possible solutionsfor each clue.
Such individual clue lists are thenmerged by a specific Merger component, whichuses list confidence values and the probabilities ofcorrectness of each candidate in the lists.
Eventu-ally, a single list of candidate-probability pairs isgenerated for each input clue.
During the secondphase WebCrow fills the crossword grid by solvinga constraint-satisfaction problem.
WebCrow se-lects a single answer from each candidate mergedlist, trying to satisfy the imposed constraints.
Thegoal of this phase is to find an admissible solutionmaximizing the number of correct inserted words.In this paper, we focus on two essential modulesof WebCrow: the Web and the DB modules, de-scribed in the next sections.3.1 WebSearch Module (WSM)WSM carries out four different tasks: (i) the re-trieval of useful text snippets (TS) and web docu-ments, (ii) the extraction of the answer candidatesfrom such text, (iii) the scoring/filtering of the can-didates, and (iv) the estimation of the list confi-dence.
The retrieval of TS is performed by theBing search engine by simply providing it the cluethrough its APIs.
Then, the latter again are usedto access the retrieved TS.
The word list gener-ator extracts possible candidate answers from TSor Web documents by picking the terms (also mul-tiwords) of the correct length.
The generated listsare merged and sorted using the candidate confi-dence computed by two filters: the statistical filterand the morphological filter.
The score associatedwith each candidate word w is given by the fol-lowing heuristic formula:p(w,C) = k(scoresf(w,C)?
scoremf(w,C)),where (i) C is the target clue, (ii) k is aconstant tuned on a validation set such that?ni=0p(wn, C) = 1, (iii) scoresf(w,C) is com-puted using statistical information extracted fromthe text, e.g., the classical TF?IDF, and (iv)scoremf(w,C) is computed using morphologicalfeatures of w.41SREL-NP REL-NP VP NPREL-NNP REL-POS TO VB CC RB TO VB NNhamlet 's to be or not to be addresseeSNP VP REL-NP REL-NP VP VP NP VP PP NP ADVP PP NP PP NPDT RBS JJ NN VBZ REL-NNP REL-POS TO VB CC RB TO VB DT NN VBZVBN IN DT NN RB TO PRP TO DT NNthe most obvious example be hamlet 's to be or not to be the monologue be address by a character either to himself to the audienceFigure 2: Shallow syntactic trees of clue (upper) and snippet (lower) and their relational links.3.2 Database module (CWDB)The knowledge about previous CPs is essential forsolving new ones.
Indeed, clues often repeat indifferent CPs, thus the availability of a large DBof clue-answer pairs allows for easily finding theanswers to previously used clues.
In order to ex-ploit the database of clue-answer pairs, WebCrowuses three different modules:CWDB-EXACT, which simply checks for anexact matching between the target clue and thosein the DB.
The score of the match is computedusing the number of occurrences of the matchedclue.CWDB-PARTIAL, which employs MySQL?spartial matching function, query expansion andpositional term distances to compute clue-similarity scores, along with the Full-Text searchfunctions.CWDB-DICTIO, which simply returns the fulllist of words of correct length, ranked by theirnumber of occurrences in the initial list.We improve WSM and CWDB by applyinglearning-to-rank algorithms based on SVMs andtree kernels applied to structural representations.We describe our models in detail in the next sec-tion.4 Learning to rank with kernelsThe basic architecture of our reranking frameworkis relatively simple: it uses a standard preferencekernel reranking approach (e.g., see (Shen andJoshi, 2005; Moschitti et al., 2006)).
The struc-tural kernel reranking framework is a specializa-tion of the one we proposed in (Severyn and Mos-chitti, 2012; Severyn et al., 2013b; Severyn et al.,2013a).
However, to tackle the novelty of the task,especially for clue DB retrieval, we modeled inno-vative kernels.
In the following, we first describethe general framework and then we instantiate itfor the two reranking tasks studied in this paper.4.1 Kernel frameworkThe framework takes a textual query and retrievesa list of related text candidates using a search en-gine (applied to the Web or a DB), according tosome similarity criteria.
Then, the query and can-didates are processed by an NLP pipeline.
Thepipeline is based on the UIMA framework (Fer-rucci and Lally, 2004) and contains many textanalysis components.
The latter used for our spe-cific tasks are: the tokenizer3, sentence detector1,lemmatizer1, part-of-speech (POS) tagger1, chun-ker4and stopword marker5.The annotations produced by such processorsare used by additional components to producestructural models representing clues and TS.
Thestructure component converts the text fragmentsinto trees.
We use both trees and feature vectorsto represent pairs of clues and TS, which are em-ployed to train kernel-based rerankers for reorder-ing the candidate lists provided by a search engine.Since the syntactic parsing accuracy can impactthe quality of our structure and thus the accuracyof our learning to rank algorithms, we preferredto use shallow syntactic trees over full syntacticrepresentations.
In the next section, we first de-scribe the structures we used in our kernels, thenthe tree kernels used as building blocks for ourmodels.
Finally, we show the reranking modelsfor both tasks, TS and clue reranking.3http://nlp.stanford.edu/software/corenlp.shtml4http://cogcomp.cs.illinois.edu/page/software_view/135Based on a standard stoplist.42Rank Clue Answer1 Kind of support for a computer user tech2 Kind of computer connection wifi3 Computer connection port4 Comb users bees5 Traveling bag gripTable 1: Clue ranking for the query: Kind of con-nection for traveling computer users (wifi)4.2 Relational shallow tree representationThe structures we adopt are similar to those de-fined in (Severyn et al., 2013b).
They are essen-tially shallow syntactic trees built from POS tagsgrouped into chunks.
Each clue and its answercandidate (either a TS or clue) are encoded intoa tree having word lemmas at the leaves and POStags as pre-terminals.
The higher tree level orga-nizes POS tags into chunks.
For example, the up-per tree of Figure 2, shows a shallow tree for theclue: Hamlet?s ?To be, or not to be?
addressee,whereas the lower tree represents a retrieved TScontaining the answer, himself : The most obviousexample is Hamlet?s ?To be or not to be ... themonologue is addressed by a character either tohimself or to the audience.Additionally, we use a special REL tag to linkthe clue/snippet trees above such that structural re-lations will be captured by tree fragments.
Thelinks are established as follows: words from aclue and a snippet sharing a lemma get their par-ents (POS tags) and grandparents, i.e., chunk la-bels, marked by a prepending REL tag.
We buildsuch structural representations for both snippetand similar clue reranking tasks.4.3 Tree kernelsWe briefly report the different types of kernels(see, e.g., (Moschitti, 2006) for more details).Syntactic Tree Kernel (STK), also known as asubset tree kernel (Collins and Duffy, 2002), mapsobjects in the space of all possible tree fragmentsconstrained by the rule that the sibling nodes fromtheir parents cannot be separated.
In other words,substructures are composed by atomic buildingblocks corresponding to nodes along with all oftheir direct children.
These, in case of a syntac-tic parse tree, are complete production rules of theassociated parser grammar.STKbextends STK by allowing leaf nodes to bepart of the feature space.
Leaf in syntactic trees arewords, from this the subscript b (bag-of-words).Subtree Kernel (SbtK) is one of the simplest treekernels as it only generates complete subtrees, i.e.,tree fragments that, given any arbitrary startingnode, necessarily include all its descendants.Partial Tree Kernel (PTK) (Moschitti, 2006) canbe effectively applied to both constituency and de-pendency parse trees.
It generates all possibleconnected tree fragments, e.g., sibling nodes canalso be separated and be part of different tree frag-ments.
In other words, a fragment is any possibletree path, from whose nodes other tree paths candepart.
Thus, it can generate a very rich featurespace resulting in higher generalization ability.4.4 Snippet rerankingThe task of snippet reranking consists in reorder-ing the list of snippets retrieved from the searchengine such that those containing the correct an-swer can be pushed at the top of the list.
For thispurpose, we transform the target clue in a searchquery and retrieve candidate text snippets.
In ourtraining set, these candidate text snippets are con-sidered as positive examples if they contain the an-swer to the target clue.We rerank snippets using preference rerankingapproach (see, e.g., (Shen and Joshi, 2005)).
Thismeans that two snippets are compared to derivewhich one is the best, i.e., which snippet containsthe answer with higher probability.
Since we aimat using kernel methods, we apply the followingpreference kernel:PK(?s1, s2?, ?s?1, s?2?)
= K(s1, s?1)++K(s2, s?2)?K(s1, s?2)?K(s2, s?1),where srand s?rrefer to two sets of candidatesassociated with two rankings and K is a kernelapplied to pairs of candidates.
We represent thelatter as pairs of clue and snippet trees.
More for-mally, given two candidates, si= ?si(c), si(s)?and s?i= ?s?i(c), s?i(s)?, whose members are theclue and snippet trees, we defineK(si, s?i) = TK(si(c), s?i(c))+TK(si(s), s?i(s)),where TK can be any tree kernel function, e.g.,STK or PTK.
Finally, it should be noted that, toadd traditional feature vectors to the reranker, it isenough to add the product (~xs1?~xs2) ?
(~xs?1?~xs?2)to the structural kernel PK, where ~xsis the featurevector associated with the snippet s.43SREL-NP PP REL-NP PP REL-NPREL-NNP IN REL-NN IN VBG REL-NN NNSkind of connection for travel computer userSREL-NP PP REL-NPREL-NNP IN REL-NN REL-NNkind of computer connectionFigure 3: Two similar clues leading to the same answer.4.5 Similar clue rerankingWebCrow creates answer lists by retrieving cluesfrom the DB of previously solved crosswords.
Itsimply uses the classical SQL operator and full-text search.
We instead verified the hypothesisthat a search engine could achieve a better re-sult.
Thus we opted for indexing the DB cluesand their answers with the open source search en-gine Lucene (McCandless et al., 2010), using thestate-of-the-art BM25 retrieval model.
This alonesignificantly improved the quality of the retrievedclue list, which could be further refined by apply-ing reranking.
The latter consists in (i) retrievinga list of similar clues using a search engine and(ii) moving those more similar, which more prob-ably contain the same answer to the clue query,at the top.
For example, Table 1 shows the firstfive clues, retrieved for a query built from the clue:Kind of connection for traveling computer users.The search engine retrieves the wrong clue, Kindof connection for traveling computer users, at thetop since it overlaps more with the query.To solve these kinds of problems by also en-hancing the generalization power of our rerankingalgorithm, we use a structural representationsimilar to the one for TS that we illustrated inthe previous section.
The main difference withthe previous models is that the reranking pair isonly constituted by clues.
For example, Fig.
3shows the representation of the pairs constitutedby the query clue and the correct clue rankedin the second position (see Table 1).
The rela-tional arrows suggest a syntactic transformationfrom connection for*computer tocomputer connection, which can be usedby the reranker to prefer the correct clue tothe wrong one.
Note that such transformationcorresponds to the pair of tree fragments: [S[REL-NP[REL-NN]][PP][NP[VBG][REL-NN]]]?
[S [REL-NP[REL-NN][REL-NN]]], where thenode pairs, ?REL-NN,REL-NN?
define the argumentsof the syntactic transformation.
Such fragmentscan be generated by PTK, which can thus be usedfor learning clue paraphrasing.To build the reranking training set, we usedthe training clues for querying the search engine,which draws candidates from the indexed clues.We stress the fact that this set of clues is disjointfrom the clues in the training and test sets.
Thus,identical clues are not present across sets.
At clas-sification time, the new clue is used as a searchquery.
Similar candidate clues are retrieved andused to form pairs.4.6 Feature VectorsIn addition to structural representations, we alsoused features for capturing the degrees of similar-ity between clues within a pair.DKPro Similarity.
We used similarity featuresfrom a top performing system in the SemanticTextual Similarity (STS) task, namely DKProfrom the UKP Lab (B?ar et al., 2013).
Thesefeatures were effective in predicting the degreeof similarity between two sentences.
DKPro in-cludes the following syntactic similarity metrics,operating on string sequences, and more advancedsemantic similarities:?
Longest common substring measure (Gusfield,1997).
It determines the length of the longestsubstring shared by two text segments.?
Longest common subsequence measure (Allisonand Dix, 1986).
It extends the notion of substringsto word subsequences by applying word insertionsor deletions to the original input text pairs.?
Running-Karp-Rabin Greedy String Tiling(Wise, 1996).
It provides a similarity between twosentences by counting the number of shuffles intheir subparts.?
Resnik similarity (Resnik, 1995).
The WordNethypernymy hierarchy is used to compute a mea-sure of semantic relatedness between conceptsexpressed in the text.
The aggregation algorithmby Mihalcea et al.
(Mihalcea et al., 2006) isapplied to extend the measure from words tosentences.?
Explicit Semantic Analysis (ESA) similarity44(Gabrilovich and Markovitch, 2007).
It representsdocuments as weighted vectors of conceptslearned from Wikipedia, WordNet and Wik-tionary.?
Lexical Substitution (Biemann, 2013).
A super-vised word sense disambiguation system is usedto substitute a wide selection of high-frequencyEnglish nouns with generalizations.
Resnikand ESA features are then computed on thetransformed text.New features.
Hereafter, we describe new fea-tures that we designed for CP reranking tasks.?
Feasible Candidate.
It is a binary feature sig-naling the presence or absence of words with thesame length of the clue answer (only used for snip-pet reranking).?
Term overlap features.
They compute the co-sine similarity of text pairs encoded into sets of n-grams extracted from different text features: sur-face forms of words, lemmas and POS-tags.
Theyare computed keeping and removing stop-words.They complement DKPro features.?
Kernel similarities.
These are computed using(i) string kernels applied to sentences, or PTK ap-plied to structural representations with and with-out embedded relational information (REL).
Thissimilarity is computed between the members of a?clue, snippet?
or a ?clue, clue?
pair.5 ExperimentsOur experiments aim at demonstrating the effec-tiveness of our models on two different tasks: (i)Snippet Reranking and (ii) Similar Clue Retrieval(SCR).
Additionally, we measured the impact ofour best model for SCR in the WebCrow systemby comparing with it.
Our referring database ofclues is composed by 1,158,202 clues, which be-long to eight different crossword editors (down-loaded from the Web6).
We use the latter to createone dataset for snippet reranking and one datasetfor clues retrieval.5.1 Experimental SetupTo train our models, we adopted SVM-light-TK7,which enables the use of structural kernels (Mos-chitti, 2006) in SVM-light (Joachims, 2002), withdefault parameters.
We applied a polynomial ker-nel of degree 3 to the explicit feature vectors,6http://www.crosswordgiant.com7http://disi.unitn.it/moschitti/Tree-Kernel.htmModel MAP MRR AvgRec REC@1 REC@5Bing 16.00 18.09 69.00 12.50 24.80V 18.00 19.88 76.00 14.20 26.10SbtK 17.00 19.6 75.00 13.80 26.40STK 18.00 20.44 76.00 15.10 27.00STKb18.00 20.68 76.00 15.30 27.40PTK 19.00 21.65 77.00 16.10 28.70V+SbtK 20.00 22.39 80.00 17.20 29.10V+STK 19.00 20.82 78.00 14.90 27.90STKb19.00 21.20 79.00 15.60 28.40V+PTK 19.00 21.68 79.00 16.00 29.40V+DK 18.00 20.48 77.00 14.60 26.80V+DK+SbtK 20.00 22.29 80.00 16.90 28.70V+DK+STK 19.00 21.47 79.00 15.50 28.30V+DK+STKb19.00 21.58 79.00 15.4 28.60V+DK+PTK 20.00 22.24 80.00 16.80 29.30Table 2: Snippet rerankingModel MAP MRR AvgRec REC@1 REC@5MB25 69.00 73.78 80.00 62.11 81.23WebCrow - 53.22 58.00 39.60 62.85SbtK 52.00 54.72 69.00 36.50 64.05STK 63.00 68.21 77.00 54.57 76.11STKb63.00 67.68 77.00 53.85 75.63PTK 65.00 70.12 78.00 57.39 77.65V+SbtK 68.00 73.26 80.00 60.95 81.28V+STK 71.00 76.01 82.00 64.58 83.95V+STKb70.00 75.68 82.00 63.95 83.77V+PTK 71.00 76.67 82.00 65.67 84.07V+DK 71.00 76.76 81.00 65.55 84.29V+DK+SbtK 72.00 76.91 82.00 65.87 84.51V+DK+STK 73.00 78.37 84.00 67.83 85.87V+DK+STKb73.00 78.29 84.00 67.71 85.77V+DK+PTK 73.00 78.13 83.00 67.39 85.75Table 3: Reranking of similar clues.as we believe feature combinations can be valu-able.
To measure the impact of the rerankers aswell as the baselines, we used well known met-rics for assessing the accuracy of QA and re-trieval systems, i.e.
: Recall at rank 1 (R@1 and5), Mean Reciprocal Rank (MRR), Mean AveragePrecision (MAP), the average Recall (AvgRec).R@k is the percentage of questions with a cor-rect answer ranked at the first position.
MRR iscomputed as follows: MRR =1|Q|?|Q|q=11rank(q),where rank(q) is the position of the first correct an-swer in the candidate list.
For a set of queries Q,MAP is the mean over the average precision scoresfor each query:1Q?Qq=1AveP (q).
AvgRec andall the measures are evaluated on the first 10 re-trieved snippets/clues.
For training and testing thereranker, only the first 10 snippets/clues retrievedby the search engine are used.5.2 Snippet RerankingThe retrieval from the Web is affected by a sig-nificant query processing delay, which prevents us45to use entire documents.
Thus, we only consid-ered the text from Bing snippets.
Moreover, sinceour reranking approach does not include the treat-ment of special clues such as anagrams or linguis-tic games, e.g., fill-in-the blank clues, we have ex-cluded them by our dataset.
We crawled the lat-ter from the Web.
We converted each clue into aquery and downloaded the first 10 snippets as re-sult of a Bing query.
In order to reduce noise fromthe data, we created a black list containing URLsthat must not be considered in the download phase,e.g., crossword websites.
The training set is com-posed by 20,000 clues while the test set comprises1,000 clues.We implemented and compared many modelsfor reranking the correct snippets higher, i.e., con-taining the answer to the clue.
The compared sys-tems are listed on the first column of Table 2,where: V is the approach using the vector onlyconstituted by the new feature set (see Sec.
4.6);DK is the model using the features made availableby DKPro; the systems ending in TK are describedin Sec.
4.3; and the plus operator indicates modelsobtained by summing the related kernels.Depending on the target measure they suggestslightly different findings.
Hereafter, we commenton MRR as it is the most interesting from a rank-ing viewpoint.
We note that: (i) Bing is improvedby the reranker based on the new feature vector by2 absolute points; (ii) DK+V improves on V byjust half point; (iii) PTK provides the highest re-sult among individual systems; (iv) combinationsimprove on the individual systems; and (v) over-all, our reranking improves on the ranking of para-graphs of Bing by 4 points in MRR and 5 pointsin accuracy on the first candidate (REC@1), cor-responding to about 20% and 50% of relative im-provement and error reduction, respectively.5.3 Similar clue retrievalWe compiled a crossword database of 794,190unique pairs of clue-answer.
Using the clues con-tained in this set, we created three different sets:training and test sets and the database of clues.The database of clues can be indexed for retriev-ing similar clues.
It contains 700,000 unique clue-answer pairs.
The training set contains 39,504clues whose answer may be found in database.
Us-ing the same approach, we created a test set con-taining 5,060 clues that (i) are not in the trainingset and (ii) have at least an answer in the database.Model MRR REC@1 REC@5 REC@10WebCrow 41.00 33.00 51.00 58.00Our Model 46.00 39.00 56.00 59.00Table 4: Performance on the word list candidatesaveraged over the clues of 10 entire CPsModel %Correct words %Correct lettersWebCrow 34.45 49.72Our Model 39.69 54.30Table 5: Performance given in terms of correctwords and letters averaged on the 10 CPsWe experimented with all models, as in theprevious section, trained for the similar clue re-trieval task.
However, since WebCrow includes adatabase module, in Tab.
3, we have an extra rowindicating its accuracy.
We note that: (i) BM25shows a very accurate MRR, 73.78%.
It largelyimproves on WebCrow by about 20.5 absolute per-cent points, demonstrating the superiority of an IRapproach over DB methods.
(ii) All TK types donot improve alone on BM25, this happens sincethey do not exploit the initial rank provided byBM25.
(iii) All the feature vector and TK combi-nations achieve high MRR, up to 4.5 absolute per-cent points of improvement over BM25 and thus25 points more than WebCrow, corresponding to53% of error reduction.
Finally, (iv) the relativeimprovement on REC@1 is up to 71% (28.23%absolute).
This high result is promising in the lightof improving WebCrow for the end task of solvingcomplete CPs.5.4 Impact on WebCrowIn these experiments, we used our rerankingmodel of similar clues (more specifically, theV+DK+STK model) using 10 complete CPs (fora total of 760 clues) from the New York Timesand Washington Post.
This way, we could mea-sure the impact of our model on the complete taskcarried out by WebCrow.
More specifically, wegive our reranked list of answers to WebCrow inplace of the list it would have extracted with theCWDB module.
It should be noted that to evalu-ate the impact of our list, we disabled WebCrowaccess to other lists, e.g., dictionaries.
This meansthat the absolute resolution accuracy of WebCrowusing our and its own lists can be higher (see (Er-nandes et al., 2008) for more details).46The first result that we derive is the accuracyof the answer list produced from the new data,i.e., constituted by the 10 entire CPs.
The resultsare reported in Tab.
4.
We note that the improve-ment of our model is lower than before as a non-negligible percentage of clues are not solved us-ing the clue DB.
However, when we compute theaccuracy in solving the complete CPs, the impactis still remarkable as reported by Tab.
5.
Indeed,the results show that when the lists reordered byour reranker are used by WebCrow, the latter im-proves by more than 5 absolute percent points inboth word and character accuracy.6 ConclusionsIn this paper, we improve automatic CP resolutionby modeling two innovative reranking tasks for:(i) CP answer list derived from Web search and(ii) CP clue retrieval from clue DBs.Our rankers are based on SVMs and structuralkernels, where the latter are applied to robust shal-low syntactic structures.
Our model applied toclue reranking is very interesting as it allows usto learn clue paraphrasing by exploiting relationalsyntactic structures representing pairs of clues.For our study, we created two different corporafor Snippet Reranking Dataset and Clue SimilarityDataset on which we tested our methods.
The lat-ter improve on the lists generated by WebCrow by25 absolute percent points in MRR (about 53% ofrelative improvement).
When such improved listsare used in WebCrow, its resolution accuracy in-creases by 15%, demonstrating that there is a largeroom for improvement in automatic CP resolution.In the future, we would like to add more seman-tic information to our rerankers and include an an-swer extraction component in the pipeline.AcknowledgmentsWe are deeply in debt with Marco Gori andMarco Ernandes for making available WebCrow,for helping us with their system and for the usefultechnical discussion regarding research directions.This research has been partially supported by theEC?s Seventh Framework Programme (FP7/2007-2013) under the grants #288024: LIMOSINE?
Linguistically Motivated Semantic aggregationengiNes.
Many thanks to the anonymous review-ers for their valuable work.ReferencesElif Aktolga, James Allan, and David A. Smith.
2011.Passage reranking for question answering using syn-tactic structures and answer types.
In ECIR.L Allison and T I Dix.
1986.
A bit-string longest-common-subsequence algorithm.
Inf.
Process.
Lett.,23(6):305?310, December.Daniel B?ar, Torsten Zesch, and Iryna Gurevych.
2013.Dkpro similarity: An open source framework fortext similarity.
In Proceedings of the 51st An-nual Meeting of the Association for ComputationalLinguistics (System Demonstrations) (ACL 2013),pages 121?126, Stroudsburg, PA, USA, August.
As-sociation for Computational Linguistics.Chris Biemann.
2013.
Creating a system for lexi-cal substitutions from scratch using crowdsourcing.Lang.
Resour.
Eval., 47(1):97?122, March.Michael Collins and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: Kernels overdiscrete structures, and the voted perceptron.
In Pro-ceedings of the 40th Annual Meeting on Associationfor Computational Linguistics, ACL ?02, pages 263?270, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Marco Ernandes, Giovanni Angelini, and Marco Gori.2005.
Webcrow: A web-based system for crosswordsolving.
In In Proc.
of AAAI 05, pages 1412?1417.Menlo Park, Calif., AAAI Press.Marco Ernandes, Giovanni Angelini, and Marco Gori.2008.
A web-based agent challenges human expertson crosswords.
AI Magazine, 29(1).David Ferrucci and Adam Lally.
2004.
Uima: Anarchitectural approach to unstructured informationprocessing in the corporate research environment.Nat.
Lang.
Eng., 10(3-4):327?348, September.David Ferrucci, Eric Brown, Jennifer Chu-Carroll,James Fan, David Gondek, Aditya Kalyanpur, AdamLally, J. William Murdock, Eric Nyberg, JohnPrager, Nico Schlaefer, and Chris Welty.
2010a.Building watson: An overview of the deepqaproject.
AI Magazine, 31(3).David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyan-pur, Adam Lally, J. William Murdock, Eric Nyberg,John M. Prager, Nico Schlaefer, and Christopher A.Welty.
2010b.
Building watson: An overview of thedeepqa project.
AI Magazine, 31(3):59?79.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In Proceedings ofthe 20th International Joint Conference on ArtificalIntelligence, IJCAI?07, pages 1606?1611, San Fran-cisco, CA, USA.
Morgan Kaufmann Publishers Inc.47Matthew L. Ginsberg.
2011.
Dr.fill: Crosswords andan implemented solver for singly weighted csps.
J.Artif.
Int.
Res., 42(1):851?886, September.Dan Gusfield.
1997.
Algorithms on Strings, Trees, andSequences: Computer Science and ComputationalBiology.
Cambridge University Press, New York,NY, USA.R Herbrich, T Graepel, and K Obermayer.
2000.
Largemargin rank boundaries for ordinal regression.
InA.J.
Smola, P.L.
Bartlett, B. Sch?olkopf, and D. Schu-urmans, editors, Advances in Large Margin Classi-fiers, pages 115?132, Cambridge, MA.
MIT Press.Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
2005.Finding similar questions in large question and an-swer archives.
In CIKM.Thorsten Joachims.
2002.
Optimizing search en-gines using clickthrough data.
In Proceedings of theEighth ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, KDD ?02,pages 133?142, New York, NY, USA.
ACM.Boris Katz and Jimmy Lin.
2003.
Selectively using re-lations to improve precision in question answering.Michael L. Littman, Greg A. Keim, and Noam Shazeer.2002.
A probabilistic approach to solving crosswordpuzzles.
Artificial Intelligence, 134(12):23 ?
55.Michael McCandless, Erik Hatcher, and Otis Gospod-netic.
2010.
Lucene in Action, Second Edition:Covers Apache Lucene 3.0.
Manning PublicationsCo., Greenwich, CT, USA.Rada Mihalcea, Courtney Corley, and Carlo Strappa-rava.
2006.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
In Proceedingsof the 21st National Conference on Artificial Intelli-gence - Volume 1, AAAI?06, pages 775?780.
AAAIPress.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2006.
Semantic role labeling via tree kerneljoint inference.
In Proceedings of CoNLL-X, NewYork City.Alessandro Moschitti, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
Exploit-ing syntactic and shallow semantic kernels for ques-tion/answer classification.
In ACL.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In ECML, pages 318?329.Alessandro Moschitti.
2008.
Kernel methods, syntaxand semantics for relational text categorization.
InCIKM.Ira Pohl.
1970.
Heuristic search viewed as path findingin a graph.
Artificial Intelligence, 1(34):193 ?
204.Filip Radlinski and Thorsten Joachims.
2006.
Querychains: Learning to rank from implicit feedback.CoRR.Philip Resnik.
1995.
Using information content toevaluate semantic similarity in a taxonomy.
InProceedings of the 14th International Joint Confer-ence on Artificial Intelligence - Volume 1, IJCAI?95,pages 448?453, San Francisco, CA, USA.
MorganKaufmann Publishers Inc.Aliaksei Severyn and Alessandro Moschitti.
2012.Structural relationships for large-scale learning ofanswer re-ranking.
In Proceedings of the 35th in-ternational ACM SIGIR conference on Research anddevelopment in information retrieval (SIGIR), pages741?750.
ACM.Aliaksei Severyn, Massimo Nicosia, and AlessandroMoschitti.
2013a.
Building structures from clas-sifiers for passage reranking.
In CIKM, pages 969?978.Aliaksei Severyn, Massimo Nicosia, and AlessandroMoschitti.
2013b.
Learning adaptable patterns forpassage reranking.
In Proceedings of the Seven-teenth Conference on Computational Natural Lan-guage Learning, pages 75?83, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Libin Shen and Aravind K. Joshi.
2005.
Rankingand reranking with perceptron.
Machine Learning,60(1-3):73?96.D.
Shen and M. Lapata.
2007.
Using semantic roles toimprove question answering.
In EMNLP-CoNLL.M.
Surdeanu, M. Ciaramita, and H. Zaragoza.
2008.Learning to rank answers on large online QA collec-tions.
In Proceedings of ACL-HLT.Michael J.
Wise.
1996.
Yap3: Improved detectionof similarities in computer program and other texts.In Proceedings of the Twenty-seventh SIGCSE Tech-nical Symposium on Computer Science Education,SIGCSE ?96, pages 130?134, New York, NY, USA.ACM.48
