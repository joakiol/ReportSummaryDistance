Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 15?22,Sydney, July 2006. c?2006 Association for Computational LinguisticsA System for Summarizing and Visualizing Arguments in SubjectiveDocuments: Toward Supporting Decision MakingAtsushi FujiiGraduate School of Library,Information and Media StudiesUniversity of Tsukuba1-2 Kasuga, Tsukuba, 305-8550, Japanfujii@slis.tsukuba.ac.jpTetsuya IshikawaThe Historiographical InstituteThe University of Tokyo3-1 Hongo 7-chome, Bunkyo-kuTokyo, 133-0033, Japanishikawa@hi.u-tokyo.ac.jpAbstractOn the World Wide Web, the volume ofsubjective information, such as opinionsand reviews, has been increasing rapidly.The trends and rules latent in a large set ofsubjective descriptions can potentially beuseful for decision-making purposes.
Inthis paper, we propose a method for sum-marizing subjective descriptions, specifi-cally opinions in Japanese.
We visual-ize the pro and con arguments for a targettopic, such as ?Should Japan introduce thesummertime system??
Users can summa-rize the arguments about the topic in orderto choose a more reasonable standpoint fordecision making.
We evaluate our system,called ?OpinionReader?, experimentally.1 IntroductionOn the World Wide Web, users can easily dissem-inate information irrespective of their own spe-cialty.
Thus, natural language information on theWeb is not restricted to objective and authorizedinformation, such as news stories and technicalpublications.
The volume of subjective informa-tion, such as opinions and reviews, has also beenincreasing rapidly.Although a single subjective description byan anonymous author is not always reliable, thetrends and rules latent in a large set of subjectivedescriptions can potentially be useful for decision-making purposes.In one scenario, a user may read customer re-views before choosing a product.
In another sce-nario, a user may assess the pros and cons of a po-litical issue before determining their own attitudeon the issue.The decision making in the above scenarios isperformed according to the following processes:(1) collecting documents related to a specifictopic from the Web;(2) extracting subjective descriptions from thedocuments;(3) classifying the subjective descriptions ac-cording to their polarity, such as posi-tive/negative or pro/con;(4) organizing (e.g., summarizing and/or visual-izing) the classified descriptions so that userscan view important points selectively;(5) making the decision.Because it is expensive to perform all of the aboveprocesses manually, a number of automatic meth-ods have been explored.
Specifically, a large num-ber of methods have been proposed to facilitateprocesses (2) and (3).In this paper, we focus on process (4), and pro-pose a method for summarizing subjective infor-mation, specifically opinions in Japanese.
Ourmethod visualizes the pro and con arguments fora target topic, such as ?Should Japan introduce thesummertime system?
?By process (4), users can summarize the argu-ments about the topic in order to choose a morereasonable standpoint on it.
Consequently, oursystem supports decision making by users.However, process (5) is beyond the scope of thispaper, and remains an intellectual activity for hu-man beings.We describe and demonstrate our prototype sys-tem, called ?OpinionReader?.
We also evaluatethe components of our system experimentally.Section 2 surveys previous research on the pro-cessing of subjective information.
Section 3 pro-vides an overview of OpinionReader, and Sec-15tion 4 describes the methodologies of its compo-nents.
Section 5 describes the experiments anddiscusses the results obtained.2 Related WorkFor process (1) in Section 1, existing search en-gines can be used to search the Web for documentsrelated to a specific topic.
However, not all re-trieved documents include subjective descriptionsfor the topic.A solution to this problem is to automaticallyidentify diaries and blogs (Nanno et al, 2004),which usually include opinionated subjective de-scriptions.For process (2), existing methods aim to dis-tinguish between subjective and objective descrip-tions in texts (Kim and Hovy, 2004; Pang and Lee,2004; Riloff and Wiebe, 2003).For process (3), machine-learning methods areusually used to classify subjective descriptionsinto bipolar categories (Dave et al, 2003; Beinekeet al, 2004; Hu and Liu, 2004; Pang and Lee,2004) or multipoint scale categories (Kim andHovy, 2004; Pang and Lee, 2005).For process (4), which is the subject of this pa-per, Ku et al (2005) selected documents that in-clude a large number of positive or negative sen-tences about a target topic, and used their head-lines as a summary of the topic.
This is the appli-cation of an existing extraction-based summariza-tion method to subjective descriptions.Hu and Liu (2004) summarized customer re-views of a product such as a digital camera.
Theirsummarization method extracts nouns and nounphrases as features of the target product, (e.g.,?picture?
for a digital camera), and lists positiveand negative reviews on a feature-by-feature basis.The extracted features are sorted according tothe frequency with which each feature appears inthe reviews.
This method allows users to browsethe reviews in terms of important features of thetarget product.Liu et al (2005) enhanced the above method toallow users to compare different products within aspecific category, on a feature-by-feature basis.3 Overview of OpinionReaderFigure 1 depicts the process flow in Opinion-Reader.
The input is a set of subjective descrip-tions for a specific topic, classified according totheir polarity.
We assume that processes (1)?
(3) inSection 1 are completed, either manually or auto-matically, prior to the use of our system.
It is of-ten the case that users post their opinions and statetheir standpoints, as exemplified by the websitesused in our experiments (see Section 5).While our primarily target is a set of opinionsfor a debatable issue classified into pros and cons,a set of customer reviews for a product, classifiedas positive or negative, can also be submitted.extracting points at issuearranging points at issueranking opinionsopinions about a topicpros consFigure 1: Process flow in OpinionReader.Our purpose is to visualize the pro and con ar-guments about a target topic, so that a user can de-termine which standpoint is the more reasonable.We extract ?points at issue?
from the opinionsand arrange them in a two-dimensional space.
Wealso rank the opinions that include each point atissue according to their importance, so that a usercan selectively read representative opinions on apoint-by-point basis.The output is presented via a graphical inter-face as shown in Figure 2, which is an exampleoutput for the topic ?privatization of hospitals byjoint-stock companies?.
The opinions used for thisexample are extracted from the website for ?BSdebate?1.
This interface is accessible via existingWeb browsers.In Figure 2, the x and y axes correspond tothe polarity and importance respectively, and eachoval denotes an extracted point at issue, such as?information disclosure?, ?health insurance?, or?medical corporation?.Users can easily see which points at issue aremost important from each standpoint.
Points atissue that are important and closely related to oneparticular standpoint are usually the most useful inusers?
decision making.By clicking on an oval in Figure 2, users canread representative opinions corresponding to that1http://www.nhk.or.jp/bsdebate/16point at issue.
In Figure 3, two opinions that in-clude ?information disclosure?
are presented.
Theopinions on the right and left sides are selectedfrom the pros and cons, respectively.
While thepros support information disclosure, the cons in-sist that they have not recognized its necessity.As a result, users can browse the pro and conarguments about the topic in detail.
However, forsome points at issue, only opinions from a singlestandpoint are presented, because the other sidehas no argument about that point.Given the above functions, users can easilysummarize the main points and how they are usedin arguing about the topic in support of one stand-point or the other.If subjective descriptions are classified intomore than two categories with a single axis, wecan incorporate these descriptions into our systemby reclassifying them into just two categories.
Fig-ure 4 is an example of summarizing reviews with amultipoint scale rating.
We used reviews with five-point star rating for the movie ?Star Wars: EpisodeIII?2.
We reclassified reviews with 1?3 stars ascons, and reviews with 4?5 stars as pros.In Figure 4, the points at issue are typicalwords used in the movie reviews (e.g.
?story?
),the names of characters (e.g.
?Anakin?, ?Obi-Wan?, and ?Palpatine?
), concepts related to StarWars (e.g.
?battle scene?
and ?Dark Side?
), andcomparisons with other movies (e.g., ?War of theWorlds?
).Existing methods for summarizing opin-ions (Hu and Liu, 2004; Liu et al, 2005).
extractthe features of a product, which corresponds tothe points at issue in our system, and arrange themalong a single dimension representing the impor-tance of features.
The reviews corresponding toeach feature are not ranked.However, in our system, features are arranged toshow how the feature relates to each polarity.
Theopinions addressing a feature are ranked accordingto their importance.
We target both opinions andreviews, as shown in Figures 2 and 4, respectively.4 Methodology4.1 Extracting Points at IssueIn a preliminary investigation of political opin-ions on the Web, we identified that points at issuecan be different language units: words, phrases,2http://moviessearch.yahoo.co.jp/detail?ty=mv&id=321602sentences, and combinations of sentences.
Wecurrently target nouns, noun phrases, and verbphrases, whereas existing summarization meth-ods (Hu and Liu, 2004; Liu et al, 2005) extractonly nouns and noun phrases.Because Japanese sentences lack lexical seg-mentation, we first use ChaSen3 to perform a mor-phological analysis of each input sentence.
As aresult, we can identify the words in the input andtheir parts of speech.To extract nouns and noun phrases, we usehandcrafted rules that rely on the word and part-of-speech information.
We extract words and wordsequences that match these rules.
To standard-ize among the different noun phrases that describethe same content, we paraphrase specific types ofnoun phrases.To extract verb phrases, we analyze the syntac-tic dependency structure of each input sentence,by using CaboCha4.
We then use handcrafted rulesto extract verb phrases comprising a noun and averb from the dependency structure.It is desirable that the case of a noun (i.e., post-positional particles) and the modality of a verb(i.e., auxiliaries) are maintained.
However, if wewere to allow variations of case and modality, verbphrases related to almost the same meaning wouldbe regarded as different points at issue and thus theoutput of our system would contain redundancy.Therefore, for the sake of conciseness, we cur-rently discard postpositional particles and auxil-iaries in verb phrases.4.2 Arranging Points at IssueIn our system, the points at issue extracted asdescribed in Section 4.1 are arranged in a two-dimensional space, as shown in Figure 2.
The x-axis corresponds to the polarity of the points at is-sue, that is the degree to which a point is relatedto each standpoint.
The y-axis corresponds to theimportance of the points at issue.For a point at issue A, which can be a noun,noun phrase, or verb phrase, the x-coordinate, xA,is calculated by Equation (1):xA = P (pro|A)?
P (con|A) (1)P (S|A), in which S denotes either the pro or constandpoint, is the probability that an opinion ran-domly selected from a set of opinions addressing3http://chasen.naist.jp/hiki/ChaSen/4http://cl.aist-nara.ac.jp/?taku-ku/software/cabocha/17JGCNVJKPUWTCPEGKPHQTOCVKQPFKUENQUWTGOGFKECNEQTRQTCVKQPRTQHKVEQP RTQKORTQXGOGPVEQUOGVKEUWTIGT[EWUVQOGTPGGFUOGFKECNVTGCVOGPVKPHQTOCVKQPFigure 2: Example of visualizing points at issue for ?privatization of hospitals by joint-stock companies?.1RKPKQPUQH2TQ1RKPKQPQH%QPFigure 3: Example of presenting representative opinions for ?information disclosure?.1DK9CP#PCMKP2CNRCVKPGUVQT[EQP RTQ DCVVNGUEGPG&CTM5KFG9CTQHVJG9QTNFUFigure 4: Example of summarizing reviews with multipoint scale rating for ?Star Wars: Episode III?.18A supports S. We calculate P (S|A) as the num-ber of opinions that are classified into S and thatinclude A, divided by the number of opinions thatinclude A.xA ranges from ?1 to 1.
A is classified into oneof the following three categories depending on thevalue of xA:?
if A appears in the pros more frequently thanin the cons, xA is a positive number,?
if A appears in the pros and cons equally of-ten, xA is zero,?
if A appears in the cons more frequently thanin the pros, xA is a negative number.The calculation of the y-coordinate of A, yA de-pends on which of the above categories applies toA.
If A appears in standpoint S more frequentlythan in its opposite, we define yA as the probabil-ity that a point at issue randomly selected from theopinions classified into S is A.We calculate yA as the frequency of A in theopinions classified into S, divided by the total fre-quencies of points at issue in the opinions classi-fied into S. Thus, yA ranges from 0 to 1.However, if A appears in the pros and consequally often, we use the average of the values ofyA for both standpoints.General words, which are usually high fre-quency words, tend to have high values for yA.Therefore, we discard the words whose yA isabove a predefined threshold.
We empirically setthe threshold at 0.02.Table 1 shows example points at issue for thetopic ?privatization of hospitals by joint-stockcompanies?
and their values of xA and yA.
In Ta-ble 1, points at issue, which have been translatedinto English, are classified into the three categories(i.e., pro, neutral, and con) according to xA andare sorted according to yA in descending order, foreach category.In Table 1, ?improvement?
is the most impor-tant in the pro category, and ?medical corporation?is the most important in the con category.
In thepro category, many people expect that the qual-ity of medical treatment will be improved if joint-stock companies make inroads into the medical in-dustry.
However, in the con category, many peopleare concerned about the future of existing medicalcorporations.Table 1: Examples of points at issue and their co-ordinates for ?privatization of hospitals by joint-stock companies?.Point at issue xA yAimprovement 0.33 9.2?10?3information disclosure 0.33 7.9?10?3health insurance 0.60 5.3?10?3customer needs 0.50 3.9?10?3cosmetic surgery 0.00 2.6?10?3medical corporation ?0.69 4.4?10?3medical institution ?0.64 3.6?10?3medical cost ?0.60 3.2?10?3profit seeking ?0.78 3.2?10?34.3 Ranking OpinionsGiven a set of opinions from which a point at is-sue has been extracted, our purpose now is to rankthe opinions in order of importance.
We assumethat representative opinions contain many contentwords that occur frequently in the opinion set.
Inour case, content words are nouns, verbs, and ad-jectives identified by morphological analysis.We calculate the score of a content word w,s(w), as the frequency of w in the opinion set.
Wecalculate the importance of an opinion by the sumof s(w) for the words in the opinion.
However,we normalize the importance of the opinion by thenumber of words in the opinion because long opin-ions usually include many words.5 Experiments5.1 MethodThe effectiveness of our system should be evalu-ated from different perspectives.
First, the effec-tiveness of each component of our system shouldbe evaluated.
Second, the effectiveness of the sys-tem as a whole should be evaluated.
In this secondevaluation, the evaluation measure is the extent towhich the decisions of users can be made correctlyand efficiently.As a first step in our research, in this paperwe perform only the first evaluation and evaluatethe effectiveness of the methods described in Sec-tion 4.
We used the following Japanese websitesas the source of opinions, in which pros and consare posted for specific topics.
(a) BS debate5(b) ewoman65http://www.nhk.or.jp/bsdebate/6http://www.ewoman.co.jp/19(c) Official website of the prime minister ofJapan and his cabinet7(d) Yomiuri online8For evaluation purposes, we collected the pros andcons for five topics.
Table 2 shows the five top-ics, the number of opinions, and the sources.
Fortopic #4, we used the opinions collected from twosources to increase the number of opinions.In Table 2, the background of topic #5 shouldperhaps be explained.
When using escalators, itis often customary for passengers to stand on oneside (either left or right) to allow other passen-gers to walk past them.
However, some peopleinsist that walking on escalators, which are mov-ing stairs, is dangerous.Graduate students, none of who was an authorof this paper, served as assessors, and producedreference data.
The output of a method under eval-uation was compared with the reference data.For each topic, two assessors were assigned toenhance the degree of objectivity of the results.
Fi-nal results were obtained by averaging the resultsover the assessors and the topics.5.2 Evaluation of Extracting Points at IssueFor each topic used in the experiments, the asses-sors read the opinions from both standpoints andextracted the points at issue.
We defined the pointat issue as the grounds for an argument.
We did notrestrict the form of the points at issue.
Thus, theassessors were allowed to extract any continuouslanguage units, such as words, phrases, sentences,and paragraphs, as points at issue.Because our method is intended to extractpoints at issue exhaustively and accurately, weused recall and precision as evaluation measuresfor the extraction.Recall is the ratio of the number of correct an-swers extracted automatically to the total numberof correct answers.
Precision is the ratio of thenumber of correct answers extracted automaticallyto the total number of points at issue extracted au-tomatically.Table 3 shows the results for each topic, inwhich ?System?
denotes the number of points atissue extracted automatically.
In Table 3, ?C?,?R?, and ?P?
denote the number of correct an-swers, recall, and precision, respectively, on anassessor-by-assessor basis.7http://www.kantei.go.jp/8http://www.yomiuri.co.jp/komachi/forum/Looking at Table 3, we see that the resultscan vary depending on the topic and the assessor.However, recall and precision were approximately50% and 4%, respectively, on average.The ratio of agreement between assessors waslow.
When we used the points at issue extractedby one assessor as correct answers and evaluatedthe effectiveness of the other assessor in the ex-traction, the recall and precision ranged from 10%to 20% depending on the topic.
To increase the ra-tio of agreement between assessors, the instructionfor assessors needs to be revised for future work.This was mainly because the viewpoint for a tar-get topic and the language units to be extractedwere different, depending on the assessor.
Be-cause our automatic method extracted points at is-sue exhaustively, the recall was high and the pre-cision was low, irrespective of the assessor.The ratios of noun phrases (including nouns)and verb phrases to the number of manually ex-tracted points at issue were 78.5% and 2.0%, re-spectively.
Although the ratio for verb phrasesis relatively low, extracting both noun and verbphrases is meaningful.The recalls of our method for noun phrases andverb phrases were 60.0% and 44.3%, respectively.Errors were mainly due to noun phrases that werenot modeled in our method, such as noun phrasesthat include a relative clause.5.3 Evaluation of Arranging Points at IssueAs explained in Section 4.2, in our system thepoints at issue are arranged in a two-dimensionalspace.
The x and y axes correspond to the polarityand the importance of points at issue, respectively.Because it is difficult for the assessors to judgethe correctness of coordinate values in the two-dimensional space, we evaluated the effectivenessof arranging points at issue indirectly.First, we evaluated the effectiveness of the cal-culation for the y-axis.
We sorted the points at is-sue, which were extracted automatically (see Sec-tion 5.2), according to their importance.
We eval-uated the trade-off between recall and precisionby varying the threshold of yA.
We discarded thepoints at issue whose yA is below the threshold.Note that while this threshold was used to de-termine the lower bound of yA, the threshold ex-plained in Section 4.2 (i.e., 0.02) was used to de-termine the upper bound of yA and was used con-sistently irrespective of the lower bound threshold.20Table 2: Topics used for experiments.#OpinionsTopic ID Topic Pro Con Source#1 principle of result in private companies 57 29 (a)#2 privatization of hospitals by joint-stock companies 27 44 (a)#3 the summertime system in Japan 14 17 (b)#4 privatization of postal services 28 20 (b), (c)#5 one side walk on an escalator 29 42 (d)Table 3: Recall and precision of extracting points at issue (C: # of correct answers, R: recall (%), P:precision (%)).Assessor A Assessor BTopic ID System C R P C R P#1 1968 194 58.2 5.7 101 44.6 2.3#2 1864 66 50.0 1.8 194 60.8 6.3#3 508 43 48.8 4.1 43 60.5 5.1#4 949 77 64.9 5.3 96 36.5 3.7#5 711 91 30.0 3.8 75 18.7 2.0Table 4 shows the results, in which the precisionwas improved to 50% by increasing the threshold.In Figure 2, users can change the threshold of im-portance by using the panel on the right side tocontrol the number of points at issue presented inthe interface.
As a result, users can choose appro-priate points at issue precisely.Second, we evaluated the effectiveness of thecalculation for the x-axis.
We evaluated the effec-tiveness of our method in a binary classification.For each point at issue extracted by an assessor,the assessor judged which of the two standpointsthe point supports.If a point at issue whose x-coordinate calculatedby our method is positive (or negative), it was clas-sified as pro (or con) automatically.
We did not usethe points at issue whose x-coordinate was zero forevaluation purposes.Table 5 shows the results.
While the number oftarget points at issue was different depending onthe topic and the assessor, the difference in classi-fication accuracy was marginal.For each topic, we averaged the accuracy deter-mined by each assessor and averaged the accura-cies over the topic, which gave 95.6%.
Overall,our method performs the binary classification forpoints at issue with a high accuracy.Errors were mainly due to opinions that in-cluded arguments for both standpoints.
For exam-ple, a person supporting a standpoint might sug-gest that he/she would support the other side un-der a specific condition.
Points at issue classifiedincorrectly had usually been extracted from suchcontradictory opinions.5.4 Evaluation of Ranking OpinionsTo evaluate the effectiveness of our method inranking opinions on a point-by-point basis, weused a method that sorts the opinions randomlyas a control.
We compared the accuracy of ourmethod and that of the control.
The accuracy isthe ratio of the number of correct answers to thenumber of opinions presented by the method un-der evaluation.For each point at issue extracted by an assessor,the assessor assigned the opinions to one of thefollowing degrees:?
A: the opinion argues about the point at issueand is represented,?
B: the opinion argues about the point at issuebut is not represented,?
C: the opinion includes the point at issue butdoes not argue about it.We varied the number of top opinions presentedby changing the threshold for the rank of opinions.Table 6 shows the results, in which N denotesthe number of top opinions presented.
The column?Answer?
refers to two cases: the case in whichonly the opinions assigned to ?A?
were regardedas correct answers, and the case in which the opin-ions assigned to ?A?
or ?B?
were regarded as cor-rect answers.
In either case, our method outper-formed the control in ranking accuracy.Although the accuracy of our method for ?A?opinions was low, the accuracy for ?A?
and ?B?21Table 4: Trade-off between recall and precision in extracting points at issue.Threshold 0 0.002 0.004 0.006 0.008 0.010Recall 0.48 0.17 0.11 0.04 0.03 0.02Precision 0.04 0.14 0.21 0.31 0.33 0.50Table 5: Accuracy for classifying points at issue.Assessor A Assessor BTopic ID #Points Accuracy (%) #Points Accuracy (%)#1 113 98.2 45 97.7#2 33 91.0 118 94.1#3 21 95.2 26 100#4 50 92.0 35 91.4#5 27 96.3 14 100Table 6: Accuracy of ranking opinions.Answer Method N = 1 N = 2 N = 3A Random 19% 28% 19%Ours 38% 32% 23%A+B Random 81% 83% 75%Ours 87% 87% 83%opinions was high.
This suggests that our methodis effective in distinguishing opinions that argueabout a specific point and opinions that include thepoint but do not argue about it.6 ConclusionIn aiming to support users?
decision making, wehave proposed a method for summarizing and vi-sualizing the pro and con arguments about a topic.Our prototype system, called ?OpinionReader?,extracts points at issue from the opinions for bothpro and con standpoints, arranges the points in atwo-dimensional space, and allows users to readimportant opinions on a point-by-point basis.
Wehave experimentally evaluated the effectiveness ofthe components of our system.Future work will include evaluating our systemas a whole, and summarizing opinions that changeover time.ReferencesPhilip Beineke, Trevor Hastie, and ShivakumarVaithyanathan.
2004.
The sentimental factor: Im-proving review classification via human-providedinformation.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Lin-guistics, pages 264?271.Kushal Dave, Steve Lawrence, and David M. Pennock.2003.
Mining the peanut gallery: Opinion extractionand semantic classification of product reviews.
InProceedings of the 12th International World WideWeb Conference.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the TenthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 168?177.Soo-Min Kim and Eduard Hovy.
2004.
Determin-ing the sentiment of opinions.
In Proceedings ofthe 20th International Conference on ComputationalLinguistics, pages 1367?1373.Lun-Wei Ku, Li-Ying Lee, Tung-Ho Wu, and Hsin-HsiChen.
2005.
Major topic detection and its appli-cation to opinion summarization.
In Proceedings ofthe 28th Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 627?628.Bing Liu, Minqing Hu, and Junsheng Cheng.
2005.Opinion observer: Analyzing and comparing opin-ions on the Web.
In Proceedings of the 14th Interna-tional World Wide Web Conference, pages 324?351.Tomoyuki Nanno, Toshiaki Fujiki, Yasuhiro Suzuki,and Manabu Okumura.
2004.
Automatically col-lecting, monitoring, and mining Japanese weblogs.In The 13th International World Wide Web Confer-ence, pages 320?321.
(poster session).Bo Pang and Lillian Lee.
2004.
A sentimental edu-cation: Sentiment analysis using subjectivity sum-marization based on minimum cuts.
In Proceedingsof the 42nd Annual Meeting of the Association forComputational Linguistics, pages 264?271.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploit-ing class relationships for sentiment categorizationwith respect to rating scales.
In Proceedings of the43rd Annual Meeting of the Association for Compu-tational Linguistics, pages 115?124.Ellen Riloff and Janyce Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.
In Proceed-ings of the 2003 Conference on Empirical Methodsin Natural Language Processing, pages 105?112.22
