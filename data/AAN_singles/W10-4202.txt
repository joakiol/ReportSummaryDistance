A Discourse-Aware Graph-Based Content-Selection FrameworkSeniz Demir Sandra Carberry Kathleen F. McCoyDepartment of Computer ScienceUniversity of DelawareNewark, DE 19716{demir,carberry,mccoy}@cis.udel.eduAbstractThis paper presents an easy-to-adapt,discourse-aware framework that can beutilized as the content selection compo-nent of a generation system whose goal isto deliver descriptive texts in several turns.Our framework involves a novel use of agraph-based ranking algorithm, to itera-tively determine what content to convey toa given request while taking into accountvarious considerations such as capturing apriori importance of information, convey-ing related information, avoiding redun-dancy, and incorporating the effects of dis-course history.
We illustrate and evaluatethis framework in an accessibility systemfor sight-impaired individuals.1 IntroductionContent selection is the task responsible for deter-mining what to convey in the output of a gener-ation system at the current exchange (Reiter andDale, 1997).
This very domain dependent taskis extremely important from the perspective ofusers (Sripada et al, 2001) who have been ob-served to be tolerant of realization problems aslong as the appropriate content is expressed.
TheNLG community has proposed various contentselection approaches since early systems (Mooreand Paris, 1993; McKeown, 1985) which placedemphasis on text structure and adapted planningtechniques or schemas to meet discourse goals.This paper proposes a domain-independentframework which can be incorporated as a contentselection component in a system whose goal is todeliver descriptive or explanatory texts, such as theILEX (O?Donnell et al, 2001), KNIGHT (Lesterand Porter, 1997), and POLIBOX (Chiarcos andStede, 2004) systems.
At the core of our frame-work lies a novel use of a graph-based ranking al-gorithm, which exploits discourse related consid-erations in determining what content to convey inresponse to a request for information.
This frame-work provides the ability to generate successivehistory-aware texts and the flexibility to generatedifferent texts with different parameter settings.One discourse consideration is the tenet that thepropositions selected for inclusion in a text shouldbe in some way related to one another.
Thus,the selection process should be influenced by therelevance of information to what has already beenselected for inclusion.
Moreover, we argue thatif the information given in a proposition can bededuced from the information provided by anyother proposition in the text, this would introduceredundancy and should be avoided.Many systems (such as MATCH (Walker et al,2004) and GEA (Carenini and Moore, 2006)) con-tain a user model which is employed to adapt con-tent selection to the user?s preferences (Reiter andDale, 1997).
Our framework provides a facilityto model a stereotypical user by incorporating thea priori importance of propositions.
This facilitycan also be used to capture the preferences of aparticular user.In a dialogue system, utterances that are gen-erated without exploiting the previous discourseseem awkward and unnatural (Moore, 1993).
Ourframework takes the previous discourse into ac-count so as to omit recently communicated propo-sitions and to determine when repetition of a pre-viously communicated proposition is appropriate.To our knowledge, our work is the first effortutilizing a graph-based ranking algorithm for con-tent selection, while taking into account what in-formation preferably should and shouldn?t be con-veyed together, the a priori importance of infor-mation, and the discourse history.
Our frameworkis a domain-independent methodology containingdomain-dependent features that must be instanti-ated when applying the methodology to a domain.Section 2 describes our domain-independentmethodology for determining the content of a re-sponse.
Section 3 illustrates its application in anaccessibility system for sight-impaired individualsand shows the generation flexibility provided bythis framework.
Finally, Section 4 discusses theresults of user studies conducted to evaluate theeffectiveness of our methodology.2 A Graph-based Content SelectionFrameworkOur domain-independent framework can be ap-plied to any domain where there is a set of proposi-tions that might be conveyed and where a bottom-up strategy for content selection is appropriate.
Itis particularly useful when the set of propositionsshould be delivered a little at a time.
For exam-ple, the ILEX system (O?Donnell et al, 2001) usesmultiple descriptions to convey the available infor-mation about a museum artifact, since the lengthof the text that can be displayed on a page is lim-ited.
In order to use our framework, an applicationdeveloper should identify the set of propositionsthat might be conveyed in the domain, specify therelations between these propositions, and option-ally assess a priori importance of the propositions.Our framework uses a weighted undirectedgraph (relation graph), where the propositionsare captured as vertices of the graph and theedges represent relations between these proposi-tions.
While the number and kinds of relationsrepresented is up to the developer, the frame-work does require the use of one specific rela-tion (Redundancy Relation) that is generalizableto any descriptive domain.
Redundancy Relationmust be specified between two propositions if theyprovide similar kinds of information or the infor-mation provided by one of the propositions canbe deduced from the information provided by theother.
For example, consider applying the frame-work to the ILEX domain.
Since the propositionthat ?this jewelry is produced by a single crafts-man?
can be deduced from the proposition that?this jewelry is made by a British designer?, thesepropositions should be connected with a Redun-dancy Relation in the relation graph.There is at most one edge between any two ver-tices and the weight of that edge represents howimportant it is to convey the corresponding propo-sitions in the same text (which we refer to asthe strength of the relation between these proposi-tions).
For example, suppose that once a museumartifact is introduced in ILEX, it is more impor-tant to convey its design style in the same descrip-tion as opposed to where it is produced.
In thiscase, the weight of the edge between the proposi-tions introducing the artifact and its style shouldbe higher than the weight of the edge between thepropositions introducing the artifact and its pro-duction place.The framework incorporates a stereotyp-ical user model via an additional vertex(priority vertex) in the relation graph.
Thepriority vertex is connected to all other verticesin the graph.
The weight of the edge betweena vertex and the priority vertex represents the apriori importance of that vertex, which in turnspecifies the importance of the correspondingproposition.
For example, suppose that in theILEX domain an artifact has two features thatare connected to the proposition introducing theartifact by the ?feature-of?
relation.
The a prioriimportance of one of these features over theother can be specified by giving a higher weightto the edge connecting this proposition to thepriority vertex than is given to the edge betweenthe other feature and the priority vertex.
Thiscaptures a priori importance and makes it morelikely that the important feature will be includedin the artifact?s description.2.1 Our Ranking AlgorithmWith this graph-based setting, the most importantthing to say is the proposition which is most cen-tral.
Several centrality algorithms have been pro-posed in the literature (Freeman, 1979; Navigliand Lapata, 2007) for calculating the importancescores of vertices in a graph.
The well-knownPageRank centrality (Brin and Page, 1998) calcu-lates the importance of a vertex by taking into ac-count the importance of all other vertices and therelation of vertices to one another.
This metric hasbeen applied to various tasks such as word sensedisambiguation (Sinha and Mihalcea, 2007) andtext summarization (Erkan and Radev, 2004).
Weadopted the weighted PageRank metric (Sinha andMihalcea, 2007) for our framework and thereforecompute the importance score of a vertex (Vx) as:PR(V x) = (1?
d) + d ??
(V x,V y)?Ewyx?wyz(V z ,V y)?EPR(V y)where wxy is the weight associated with the edgebetween vertices (Vx) and (Vy), E is the set of alledges, and d is the damping factor, set to 0.85,which is its usual setting.Once the propositions in a domain are capturedin a relation graph with weights assigned to theedges between them, the straightforward way ofidentifying the propositions to be conveyed in thegenerated text would be to calculate the impor-tance of each vertex via the formula above andthen select the k vertices with the highest scores.However, this straightforward application wouldfail to address the discourse issues cited earlier.Thus we select propositions incrementally, wherewith each proposition selected, weights in thegraph are adjusted causing related propositions tobe highlighted and redundant information to be re-pelled.
Because our responses are delivered overseveral turns, we also adjust weights between re-sponses to reflect that discourse situation.Our algorithm, shown in Figure 1, is run eachtime a response text is to be generated.
For eachnew response, the algorithm begins by adjustingthe importance of the priority vertex (making ithigh) and clearing the list of selected propositions.Step 2 is the heart of the algorithm for generating asingle response.
It incrementally selects proposi-tions to include in the current response, and ad-justs weights to reflect what has been selected.In particular, in order to select a proposition, im-portance scores are computed using the weightedPageRank metric for all vertices corresponding topropositions that have not yet been selected for in-clusion in this response (Step 2-a), and only theproposition that receives the highest score is se-lected (Step 2-b).
Then, adjustments are made toachieve four goals toward taking discourse infor-mation into account (Steps 2-c thru 2-g) before thePageRank algorithm is run again to select the nextproposition.
Steps 3 and 4 adjust weights to reflectthe completed response and to prepare for gener-ating the next response.Our first goal is to reflect the a priori impor-tance of propositions in the selection process.
Forthis purpose, we always assign the highest (orone of the highest) importance scores to the pri-ority vertex among the other vertices (Steps 1 and2-g).
This will make the priority vertex as influen-tial as any other neighbor of a vertex when calcu-lating its importance.Our second goal is to select propositions that arerelevant to previously selected propositions, or interms of the graph-based notation, to attract theselection of vertices that are connected to the se-lected vertices.
To achieve this, we increase theimportance of the vertices corresponding to se-lected propositions so that the propositions relatedto them have a higher probability of being chosenas the next proposition to include (Step 2-g).Our third goal is to avoid selecting propositionsthat preferably shouldn?t be communicated withpreviously selected propositions if other relatedpropositions are available.
To accomplish this, weintroduce the term repellers to refer to the kindsof relations between propositions that are dispre-ferred over other relations once one of the propo-sitions is selected for inclusion.
Once a proposi-tion is selected, we penalize the weights on theedges between the corresponding vertex and othervertices that are connected by a repeller (Step 2-d).
We don?t provide any general repellers in theframework, but rather this is left for the developerfamiliar with the domain; any number (zero ormore) and kinds of relations could be identified asrepellers for a particular application domain.
Forexample, suppose that in the ILEX domain, someartifacts (such as necklaces) have as features botha set of design characteristics and the person whofound the artifact.
Once the artifact is introduced,it becomes more important to present the designcharacteristics rather than the person who foundthat artifact.
This preference might be captured byclassifying the relation connecting the propositionconveying the person who found it to the proposi-tion introducing the artifact as arepeller.Our fourth goal is to avoid redundancy by dis-couraging the selection of propositions connectedby a Redundancy Relation to previously selectedpropositions.
Once a proposition is selected, weidentify the vertices (redundant to selected ver-tices) which are connected to the selected ver-tex by the Redundancy Relation (Step 2-e).
Foreach redundant to selected vertex, we penalize theweights on the edges of the vertex except the edgeconnected to the priority vertex (Step 2-f) andhence decrease the probability of that vertex beingchosen for inclusion in the same response.We have so far described how the content of asingle response is constructed in our framework.To capture a situation where the system is engagedin a dialogue with the user and must generate addi-tional responses for each subsequent user request,we need to ensure that discourse flows naturally.Thus, the ranking algorithm must take the previ-Figure 1: Our Ranking Algorithm for Content Selection.ous discourse into account in order to identify andpreferably select propositions that have not beenconveyed before and to determine when repetitionof a previously communicated proposition is ap-propriate.
So once a proposition is included in aresponse, we have to reduce its ability to competefor inclusion in subsequent responses.
Thus once aproposition is conveyed in a response, the weightof the edge connecting the corresponding vertexto the priority vertex is reduced (Step 2-c in Fig-ure 1).
Once a response is completed, we penal-ize the weights of the edges of each vertex thathas been selected for inclusion in the current re-sponse via a penalty factor (if they aren?t alreadyadjusted) (Step 3 in Figure 1).
We use the samepenalty factor (which is used in Step 2-d in Fig-ure 1) on each edge so that all edges connected toa selected vertex are penalized equally.
However,it isn?t enough just to penalize the edges of the ver-tices corresponding to the communicated proposi-tions.
Even after the penalties are applied, a propo-sition that has just been communicated might re-ceive a higher importance score than an uncommu-nicated proposition1.
In order to allow all propo-sitions to become important enough to be said atsome point, the algorithm increases the weightsof the edges of all other vertices in the graph ifthey haven?t already been decreased (Step 4 in Fig-ure 1), thereby increasing their ability to competein subsequent responses.
In the current implemen-tation, the weight of an edge is increased via aboost factor after a response if it is not connectedto a proposition included in that response.
The1We observed that it might happen if a vertex is connectedonly to the priority vertex.boost factor ensures that all propositions will even-tually become important enough for inclusion.3 Application in a Particular DomainThis section illustrates the application of ourframework to a particular domain and how ourframework facilitates flexible content selection.Our example is content selection in the SIGHTsystem (Elzer et al, 2007), whose goal is to pro-vide visually impaired users with the knowledgethat one would gain from viewing informationgraphics (such as bar charts) that appear in popu-lar media.
In the current implementation, SIGHTconstructs a brief initial summary (Demir et al,2008) that conveys the primary message of a barchart along with its salient features.
We enhancedthe current SIGHT system to respond to user?sfollow-up requests for more information about thegraphic, where the request does not specify thekind of information that is desired.The first step in using our framework is deter-mining the set of propositions that might be con-veyed in this domain.
In our earlier work (Demiret al, 2008), we identified a set of propositionsthat capture information that could be determinedby looking at a bar chart, and for each messagetype defined in SIGHT, specified a subset of thesepropositions that are related to this message type.In our example, we use these propositions as can-didates for inclusion in follow-up responses.
Fig-ure 2 presents a portion of the relation graph,where some of the identified propositions are rep-resented as vertices.The second step is optionally assessing the apriori importance of each proposition.
In userFigure 2: Subgraph of the Relation graph for Increasing and Decreasing Trend Message Types.studies (Demir et al, 2008), we asked subjects toclassify the propositions given for a message typeinto one of three classes according to their impor-tance for inclusion in the initial summary: essen-tial, possible, and not important.
We leveragethis information as the a priori importance of ver-tices in our graph representation.
We define threepriority classes.
For the propositions that were notselected as essential by any participant, we clas-sify the edges connecting these propositions to thepriority vertex into Possible class.
For the propo-sitions which were selected as essential by a singleparticipant, we classify the edges connecting themto the priority vertex into Important class.
Theedges of the remaining propositions are classifiedinto Highly Important class.
In this example in-stantiation, we assigned different numeric scoresto these classes where Highly Important and Pos-sible received the highest and lowest scores re-spectively.The third step requires specifying the relationsbetween every pair of related propositions and de-termining the weights associated with these re-lations in the relation graph.
First, we identi-fied propositions which we decided should beconnected by the Redundancy Relation (such asthe propositions conveying ?the overall amount ofchange in the trend?
and ?the range of the trend?
).Next, we had to determine other relations and as-sign relative weights.
Instead of defining a uniquerelation for each related pair, we defined three re-lation classes, and assigned the relations betweenrelated propositions to one of these classes:?
Period Relation: expresses a relation be-tween two propositions that span the sametime period?
Entity Relation: expresses a relation be-tween two propositions if the entities in-volved in the propositions overlap?
Contrast Relation: expresses a relation be-tween two propositions if the informationprovided by one of the propositions contrastswith the information provided by the otherWe determined that it was very common inthis domain to deliver contrasting propositions to-gether (similar to other domains (Marcu, 1998))and therefore we assigned the highest score to theContrast Relation class.
For local focusing pur-poses, it is desirable that propositions involvingcommon entities be delivered in the same responseand thus the Entity Relation class was given thesecond highest score.
On the other hand, twopropositions which only share the same period arenot very related and conveying such propositionsin the same response could cause the text to appear?choppy?.
We thus identified the Period Relationclass as a repeller and assigned the second low-est score to relations in that class.
Since we don?twant redundancy in the generated text, the lowestscore was assigned to the Redundancy Relationclass.
The next section shows how associatingparticular weights with the priority and relationclasses changes the behavior of the framework.In the domain of graphics, a collection of de-scriptions of the targeted kind which would facil-itate a learning based model isn?t available.
How-ever, the accessibility of a corpus in a new domainwould allow the identification of the propositionsalong with their relations to each other and the de-termination of what weighting scheme and adjust-ment policy will produce the corpus within reason-able bounds.3.1 Generating Flexible ResponsesThe behavior of our framework is dependent on anumber of design parameters such as the weightsassociated with various relations, the identificationof repellers, the a priori importance of informa-tion (if applicable), and the extent to which con-veying redundant information should be avoided.The framework allows the application developerto adjust these factors resulting in the selection ofdifferent content and the generation of different re-sponses.
For instance, in a very straightforwardsetting where the same numeric score is assignedto all relations, the a priori importance of infor-mation would be the major determining factor inthe selection process.
In this section, we will il-lustrate our framework?s behavior in SIGHT withthree different scenarios.
In each case, the user isassumed to post two consecutive requests for ad-ditional information about the graphic in Figure 3after receiving its initial summary.In our first scenario (which we refer to as ?base-setting?
), the following values have been given tovarious design parameters that must be specified inorder to run the ranking algorithm.
1) The weightsof the relations are set to the numeric scores shownin the text labelled Edges at the bottom (right side)of Figure 2.
2) The stopping criteria which speci-fies the number of propositions selected for inclu-sion in a follow-up response (Step 2 in Figure 1)is set to four.
3) The amount of decrease in theweight of the edge between the priority vertex andthe vertex selected for inclusion (Step 2-c in Fig-ure 1) is set to that edge?s original weight.
Thus,in our example, the weight of that edge is set to 0once a proposition has been selected for inclusion.4) The penalty and the redundancy penalty factorswhich are used to penalize the edges of a selectedvertex and the vertices redundant to the selectedvertex (Steps 2-d and 3, and 2-f in Figure 1) areset to the quotient of the highest numeric scoreinitially assigned to a relation class divided by thelowest numeric score initially assigned to a rela-tion class.
A penalized score for a relation classis computed by dividing its initial score by thepenalty factor.
The edges of a vertex are penalizedby assigning the penalized scores to these edgesbased on the relations that they represent.
This set-ting guarantees that the weight of an edge whichrepresents the strongest relation cannot be penal-ized to be lower than the score initially assignedto the weakest relation.
5) The boost factor whichis used to favor the selection of previously uncon-veyed propositions for inclusion in subsequent re-sponses (Step 4 in Figure 1) is set to the squareroot of the penalty factor.
Thus, the weights ofthe edges connected to vertices of previously com-municated propositions are restored to their initialscores slowly.Since in our example, the initial summary hasalready been presented, we treat the propositionsconveyed in that summary (P1 and P5 in Figure 2)as if they had been conveyed in a follow-up re-sponse and penalize the edges of their correspond-ing vertices (Steps 2-c and 3 in Figure 1).
Thus,before we invoke the algorithm to construct thefirst follow-up response, the weights of edges ofthe graph are as shown in Figure 2-A.
Within thisbase-setting, SIGHT generates the set of follow-upresponses shown in Figure 3A.In our first scenario (base-setting), we assumedthat the user is capable of making mathematicaldeductions such as inferring ?the overall amountof change in the trend?
from ?the range of thetrend?
; thus we identified such propositions assharing a Redundancy Relation.
Young read-ers (such as fourth graders) might not find thesepropositions as redundant because they are lack-ing in mathematical skills.
In our second sce-nario, we address this issue by setting the re-dundancy penalty factor to 1 (Step 2-f in Fig-ure 1) and thus eliminate the penalty on the Re-dundancy Relation.
Now, for the same graphic,SIGHT generates, in turn, the second alternativeset of responses shown in Figure 3B.
The re-sponses for the two scenarios differ in the secondfollow-up response.
In the first scenario, a descrip-tion of the smallest drop was included.
However,in the second scenario, this proposition is replacedwith the overall amount of change in the trend.This proposition was excluded in the first sce-nario because the redundancy penalty factor madeit drop in importance.Our third scenario shows how altering theweights assigned to relations may change the re-sponses.
Consider a situation where the Con-trast Relation is given even higher importance bydoubling its score; this might occur in a univer-sity course domain where courses on the samegeneral topic are contrasted.
SIGHT would thengenerate the third alternative set of follow-up re-sponses shown in Figure 3C.
The algorithm ismore strongly forced to group propositions thatFigure 3: Initial Summary and Follow-up Responses.are in a contrast relation (shown in bold), whichchanges the ranking of these propositions.4 EvaluationTo determine whether our framework selects ap-propriate content within the context of an applica-tion, and to assess the contribution of the discourserelated considerations to the selected content andtheir impact on readers?
satisfaction, we conductedtwo user studies.
In both studies, the partici-pants were told that the initial summary shouldinclude the most important information about thegraphic and that the remaining pieces of informa-tion should be conveyed via follow-up responses.The participants were also told that the informa-tion in the first response should be more importantthan the information in subsequent responses.Our goal in the first study was to evaluate theeffectiveness of our framework (base-setting) indetermining the content of follow-up responses inSIGHT.
To our knowledge, no one else has gener-ated high-level descriptions of information graph-ics, and therefore evaluation using implementa-tions of existing content selection modules in thedomain of graphics as a baseline is not feasible.Thus, we evaluated our framework by comparingthe content that it selects for inclusion in a follow-up response for a particular graphic with the con-tent chosen by human subjects for the same re-sponse.
Twenty one university students partici-pated in the first study and each participant waspresented with the same four graphics.
For eachgraphic, the participants were first presented withits initial summary and the set of propositions (18different propositions) that were used to constructthe relation graph in our framework.
The partic-ipants were then asked to select the four propo-sitions that they thought were most important toconvey in the first follow-up response.For each graphic, we ranked the propositionswith respect to the number of times that they wereselected by the participants and determined the po-sition of each proposition selected by our frame-work for inclusion in the first follow-up responsewith respect to this ranking.
The propositions se-lected by our framework were ranked by the par-ticipants as the 1st, 2nd, 3rd, and 5th in the firstgraphic, as the 1st, 3rd, 4th, and 5th in the sec-ond graphic, as the 1st, 2nd, 3rd, and 6th in thethird graphic, and as the 2nd, 3rd, 4th, and 6thin the fourth graphic.
Thus for every graph, threeof the four propositions selected by our frame-work were also in the top four highly-rated propo-sitions selected by the participants.
Therefore,this study demonstrated that our content selectionframework selects the most important informationfor inclusion in a response at the current exchange.We argued that simply running PageRank to se-lect the highly-rated propositions is likely to leadto text that does not cohere because it may con-tain unrelated or redundant propositions, or failto communicate related propositions.
Thus, ourapproach iteratively runs PageRank and includesdiscourse related factors in order to allow whathas been selected to influence the future selectionsand consequently improve text coherence.
To ver-ify this argument, we conducted a second studywith four graphics and two different sets of follow-up responses (each consisting of two consecutiveresponses) generated for each graphic.
We con-structed the first set of responses (baseline) byrunning PageRank to completion and selecting thetop eight highly-rated propositions, where the topfour propositions form the first response.
The con-tent of the second set of responses was identifiedby our approach.
Twelve university students (whodid not participate in the first study) were pre-sented with these four graphics along with theirinitial summaries.
Each participant was also pre-sented with the set of responses generated by ourapproach in two graphics and the set of responsesgenerated by the baseline in other cases; the par-ticipants were unaware of how the follow-up re-sponses were generated.
Overall, each set of re-sponses was presented to six participants.We asked the participants to evaluate the setof responses in terms of their quality in convey-ing additional information (from 1 to 5 with 5 be-ing the best).
We also asked each participant tochoose which set of responses (from among thefour sets of responses presented to them) best pro-vides further information about the correspond-ing graphic.
The participants gave the set of re-sponses generated by our approach an average rat-ing of 4.33.
The average participant rating forthe set of responses generated by the baseline was3.96.
In addition, the lowest score given to theset of responses generated by our approach was3, whereas the lowest score that the baseline re-ceived was 2.
We also observed that the set of re-sponses generated by our approach was selectedas the best set by eight of the twelve participants.Three of the remaining four participants selectedthe set of responses generated by the baseline asbest (although they gave the same score to a setof responses generated by our approach).
In thesecases, the participants emphasized the wordingof the responses as the reason for their selection.Thus this study demonstrated that the inclusion ofdiscourse related factors in our approach, in addi-tion to the use of PageRank (which utilizes the apriori importance of the propositions and their re-lations to each other), contributes to text coherenceand improves readers?
satisfaction.5 ConclusionThis paper has presented our implementeddomain-independent content selection framework,which contains domain-dependent features thatmust be instantiated when applying it to a particu-lar domain.
To our knowledge, our work is the firstto select appropriate content by using an incre-mental graph-based ranking algorithm that takesinto account the tendency for some information toseem related or redundant to other information, thea priori importance of information, and what hasalready been said in the previous discourse.
Al-though our framework requires a knowledge engi-neering phase to port it to a new domain, it handlesdiscourse issues without requiring that the devel-oper write code to address them.
We have demon-strated how our framework was incorporated inan accessibility system whose goal is the genera-tion of texts to describe information graphics.
Theevaluation studies of our framework within thataccessibility system show its effectiveness in de-termining the content of follow-up responses.6 AcknowledgementsThe authors would like to thank Debra Yarringtonand the members of the NLP-AI Lab at UD fortheir help throughout the evaluation of this work.This material is based upon work supported by theNational Institute on Disability and RehabilitationResearch under Grant No.
H133G080047.ReferencesS.
Brin and L. Page.
1998.
The anatomy of a large-scale hypertextual Web search engine.
ComputerNetworks and ISDN Systems, 30(1-7):107?117.G.
Carenini and J. Moore.
2006.
Generating and eval-uating evaluative arguments.
Artificial Intelligence,170(11):925?452.C.
Chiarcos and M. Stede.
2004.
Salience-Driven TextPlanning.
In Proc.
of INLG?04.S.
Demir, S. Carberry, and K. F. McCoy.
2008.
Gener-ating Textual Summaries of Bar Charts.
In Proc.
ofINLG?08.S.
Elzer, E. Schwartz, S. Carberry, D. Chester,S.
Demir, and P. Wu.
2007.
A browser extensionfor providing visually impaired users access to thecontent of bar charts on the web.
In Proc.
of WE-BIST?2007.G.
Erkan and D. Radev.
2004.
LexRank: Graph-based Lexical Centrality as Salience in Text Summa-rization.
Journal of Artificial Intelligence Research,22:457?479.L.
C. Freeman.
1979.
Centrality in Social Networks: I.Conceptual Clarification.
Social Networks, 1:215?239.J.
Lester and B. Porter.
1997.
Developing and empir-ically evaluating robust explanation generators: theKNIGHT experiments.
Computational Linguistics,23(1):65?101.D.
Marcu.
1998.
The rhetorical parsing, summariza-tion, and generation of natural language texts.
PhD.Thesis, Department of Computer Science, Universityof Toronto.K.
McKeown.
1985.
Discourse strategies for gener-ating natural-language text.
Artificial Intelligence,27(1):1?41.J.
Moore and C. Paris.
1993.
Planning text for advisorydialogues: capturing intentional and rhetorical infor-mation.
Computational Linguistics, 19(4):651?694.J.
Moore.
1993.
Indexing and exploiting a discoursehistory to generate context-sensitive explanations.In Proc.
of HLT?93, 165?170.R.
Navigli and M. Lapata.
2007.
Graph Connectiv-ity Measures for Unsupervised Word Sense Disam-biguation.
In Proc.
of IJCAI?07, 1683?1688.M.
O?Donnell, C. Mellish, J. Oberlander, and A. Knott.2001.
ILEX: an architecture for a dynamic hypertextgeneration system.
In Natural Language Engineer-ing, 7(3):225?250.E.
Reiter and R. Dale.
1997.
Building applied naturallanguage generation systems.
In Natural LanguageEngineering, 3(1):57?87.R.
Sinha and R. Mihalcea.
2007.
Unsupervised Graph-based Word Sense Disambiguation Using Measuresof Word Semantic Similarity.
In Proc.
of ICSC?07.S.
Sripada, E. Reiter, J.
Hunter, and J. Yu.
2001.
ATwo-Stage Model for Content Determination.
InProc.
of ENLGW?01.M.
Walker, S.J.
Whittaker, A. Stent, P. Maloor,J.
Moore, M. Johnston, and G. Vasireddy.
2004.Generation and evaluation of user tailored responsesin multimodal dialogue.
In Cognitive Science,28(5):811?840.
